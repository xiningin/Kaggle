{"cell_type":{"6d5d24f9":"code","ba319c18":"code","e76b8b5d":"code","de04415a":"code","c506302e":"code","53233b3d":"code","ce3a5e4d":"code","8cb74968":"code","d8f09453":"code","155bccd6":"code","be6d9a11":"code","69d90014":"code","caef5e33":"code","16414b06":"code","42c8de10":"code","50cda4f9":"code","07cfa1d1":"code","c213b738":"code","29bd9ace":"code","ac4cfa7f":"code","4050f4fe":"code","0a0e38c7":"code","b2b09eb8":"code","1f01236b":"code","bcd667f8":"code","712629e0":"code","4bc1c35d":"code","6058be68":"code","ea12ee81":"code","7969c370":"code","d4984de9":"code","652cc396":"code","fd807c32":"code","9f05f002":"markdown","d637d840":"markdown","6fd5f47e":"markdown","fc38be86":"markdown","499f5bf3":"markdown","36bf6a10":"markdown","81bbbc91":"markdown","32368c91":"markdown","fc82298b":"markdown","8b1145cf":"markdown","b7e9334b":"markdown","cb5b04c3":"markdown","f2143c3c":"markdown","48c2a89c":"markdown","e488a767":"markdown","d8114e55":"markdown","9c9c5138":"markdown","68309a5d":"markdown","2e1364d9":"markdown"},"source":{"6d5d24f9":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","ba319c18":"import warnings\nwarnings.filterwarnings('ignore')","e76b8b5d":"import seaborn as sns\nsns.set()","de04415a":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c506302e":"train_set = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_set = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","53233b3d":"combine = [train_set, test_set]","ce3a5e4d":"for dataset in combine:\n    print(dataset.info())\n    print('--------------')","8cb74968":"for data in combine:\n    print(data.isna().sum())\n    print('----------------')\n","d8f09453":"combine[0].describe()","155bccd6":"combine[0].describe(include=['O'])","be6d9a11":"fig, ax = plt.subplots(figsize=(20,10))\nsns.heatmap(combine[0].corr(), vmin=-1, vmax=1,square=True,annot=True, cmap='BrBG')","69d90014":"axis=sns.barplot(x ='Pclass' , y = 'Survived' , data= combine[0])\naxis.set_title('Pclass  v.s  Survived')\naxis.set_xlabel('Pclass')\naxis.set_ylabel('Survived Percentage')","caef5e33":"axis=sns.barplot(x ='Sex' , y = 'Survived' , data= combine[0])\naxis.set_title('Gender  v.s  Survived')\naxis.set_xlabel('Gender')\naxis.set_ylabel('Survived Percentage')","16414b06":"axis=sns.barplot(x ='Embarked' , y = 'Survived' , data= combine[0])\naxis.set_title('Twon  v.s  Survived')\naxis.set_xticklabels(['Southampton (S)' , 'Cherbourg (C)','Queenstown (Q)'])\naxis.set_xlabel('Town Name ')\naxis.set_ylabel('Survived Percentage')","42c8de10":"sns.distplot(combine[0][combine[0][\"Survived\"] ==  0]['Age']  ,label='Not Survived', color = '#e74c3c')\nsns.distplot(combine[0][combine[0][\"Survived\"] ==  1]['Age'] , label='Survived' ,color = '#2ecc71')\n\nplt.legend()\nplt.title('Distribtion of Survived with Age')","50cda4f9":"bins = [ 0, 16, 32, 48,64, np.inf]\nlabels = [' 0-16', ' 16-32', ' 32-48', ' 48-64', ' 64-']\n\nage = pd.cut(combine[0][\"Age\"], bins, labels = labels)\n\naxis=sns.barplot(age , y=combine[0][\"Survived\"] )\naxis.set_title('Age  v.s  Survived')\naxis\naxis.set_xlabel('Age')\naxis.set_ylabel('Survived Percentage')\n_=axis.set_xticklabels(labels,rotation  = 30)\n","07cfa1d1":"combine[0][combine[0][\"Survived\"] == 0]['Age']","c213b738":"from sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer","29bd9ace":"class prepare_data(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, add_title = True, custom_fill_age = True, handel_company = True):\n        self.add_title = add_title\n        self.custom_fill_age = custom_fill_age\n        self.handel_company = handel_company\n    def fit(self, X, y=None):\n        return self \n    \n    \n    def transform(self, X):\n        \n        #--------------------------------------------------------------------------------------------------------------\n        # Converting a categorical feature\n        X['Sex'] = X['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n        #fill uncomplete catogeral data\n        freq_port = X.Embarked.dropna().mode()[0]\n        X['Embarked'] = X['Embarked'].fillna(freq_port)\n        X['Embarked'] = X['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n        \n        if self.add_title:\n            X['Title'] = X.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n            # We can replace many titles with a more common name or classify them as Rare.\n            X['Title'] = X['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n            X['Title'] = X['Title'].replace('Mlle', 'Miss')\n            X['Title'] = X['Title'].replace('Ms', 'Miss')\n            X['Title'] = X['Title'].replace('Mme', 'Mrs')\n            # We can convert the categorical titles to ordinal.\n            title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n            X['Title'] = X['Title'].map(title_mapping)\n            X['Title'] = X['Title'].fillna(0)\n\n        #--------------------------------------------------------------------------------------------------------------\n        #--------------------------------------------------------------------------------------------------------------\n        \n        # fill uncomplete numerical data\n        X['Fare'].fillna(X['Fare'].dropna().median(), inplace=True)\n        \n        # scale the fare feature\n        X.loc[ X['Fare'] <= 7.91, 'Fare'] = 0\n        X.loc[(X['Fare'] > 7.91) & (X['Fare'] <= 14.454), 'Fare'] = 1\n        X.loc[(X['Fare'] > 14.454) & (X['Fare'] <= 31), 'Fare']   = 2\n        X.loc[ X['Fare'] > 31, 'Fare'] = 3\n        X['Fare'] = X['Fare'].astype(int)\n        \n        \n        if self.custom_fill_age:\n            #  way of guessing missing values is to use other correlated features (Age, Gender, and Pclass).\n            guess_ages = np.zeros((2,3))\n            \n            X['Age'] = X.groupby(['Sex' , 'Pclass'])['Age'].apply(lambda x : x.fillna(x.median()))\n\n            X['Age'] = X['Age'].astype(int)\n            \n            # replace Age with ordinals based on bands.\n            X.loc[ X['Age'] <= 16, 'Age'] = 0\n            X.loc[(X['Age'] > 16) & (X['Age'] <= 32), 'Age'] = 1\n            X.loc[(X['Age'] > 32) & (X['Age'] <= 48), 'Age'] = 2\n            X.loc[(X['Age'] > 48) & (X['Age'] <= 64), 'Age'] = 3\n            X.loc[ X['Age'] > 64, 'Age'] = 4\n            \n          \n        if self.handel_company:\n            X['FamilySize'] = X['SibSp'] + X['Parch'] + 1\n            X['IsAlone'] = 0\n            X.loc[X['FamilySize'] == 1, 'IsAlone'] = 1\n                    \n                    \n        #-------------------------------------------------------------------------------------------------------------- \n\n        X = X.drop(columns = ['Cabin','Ticket','Name', 'PassengerId'])\n        #impute any remaning missing values\n        imuter = SimpleImputer(strategy = 'most_frequent')\n        X = imuter.fit_transform(X)\n        return X","ac4cfa7f":"train_X = train_set.drop(columns = ['Survived'])\ntrain_y = train_set['Survived'].copy()","4050f4fe":"cols = [col for col in train_X.columns]\nfull_pipeline = ColumnTransformer([\n        (\"prepare\", prepare_data(),cols)\n    ])","0a0e38c7":"train_X_pre = full_pipeline.fit_transform(train_X)\ntest_X_pre = full_pipeline.fit_transform(test_set)","b2b09eb8":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.model_selection import cross_val_score\nfrom yellowbrick.model_selection import learning_curve","1f01236b":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","bcd667f8":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [400,500,1000],\n              'learning_rate': [0.05, 0.01],\n              'min_samples_leaf': [100],\n              'max_features': [ 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(train_X_pre, train_y)\n\nGBC_best = gsGBC.best_estimator_\n\ngsGBC.best_score_","712629e0":"print(learning_curve(GBC_best, train_X_pre, train_y, cv=10, scoring='accuracy'))","4bc1c35d":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [3],\n              \"min_samples_split\": [ 3],\n              \"min_samples_leaf\": [7, 10],\n              \"bootstrap\": [True , False],\n              \"n_estimators\" :[500,1000, 1500],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(train_X_pre, train_y)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","6058be68":"print(learning_curve(RFC_best, train_X_pre, train_y, cv=5, scoring='accuracy'))","ea12ee81":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001,0.05 ,0.01],\n                  'C': [100,200,300]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(train_X_pre, train_y)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","7969c370":"print(learning_curve(SVMC_best, train_X_pre, train_y, cv=5, scoring='accuracy'))","d4984de9":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('svc', SVMC_best), ('gbc',GBC_best)], voting='soft', n_jobs=4)\nvotingC.fit(train_X_pre, train_y)\ncross_val_score(votingC, train_X_pre, train_y, cv=5, scoring=\"accuracy\").mean()","652cc396":"y_predict = votingC.predict(test_X_pre)","fd807c32":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = test_set['PassengerId']\nsubmission_df['Survived'] = y_predict\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head()","9f05f002":"**Observations:**\n- Some features have uncomplete features.\n\n**Decisions:**  we can either drop the column or empute it depinding on the column importance:\n- We may want to complete Age feature as it is definitely correlated to survival.\n- We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n- Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.","d637d840":"### correlation between features\u00b6\n","6fd5f47e":"### Datatype checking","fc38be86":"**Observations:**\n- Infants (Age <=5) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n\n**Decisions:**\n- We should consider Age in our model training.\n- Complete the Age feature for null values.\n-  We should band age groups.\n","499f5bf3":"**Observations:** \n- We confirm the observation during problem definition that Sex=female had very high survival rate at 74%.\n\n**Decisions:**\n-  the feature sounds an important feature","36bf6a10":"**Observations:** \n- Some features wide range of values like Fare and age comparing to other features.\n\n**Decisions:**\n- we can normalize the data if the model is sensative to scaling of the data.\n- PassengerId may be dropped from training dataset as it does not contribute to survival.\n- We may also want to create a Fare range feature if it helps our analysis.","81bbbc91":"### Creating a transformer of the feautures\u00b6\n","32368c91":"- We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board as there is a relation between theme.\n- their is a relation between the fare and the Pclass","fc82298b":"# descovering the data","8b1145cf":"### Completness of the data checking","b7e9334b":"### Analyze by pivoting features","cb5b04c3":"# Model selection, hyperparameter tuning and cross validation ","f2143c3c":"### Scaling and data distripution checking","48c2a89c":"\n# Importing the data\n","e488a767":"- **Observations:** Some features like sex and Embarked are catogiral features\n\n- **Decisions:**  we can convert features which contain strings to numerical values. This is required by most model algorithms.","d8114e55":"**Observations:** \n- Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n- Ticket feature has high ratio (22%) of duplicate values (unique=681).\n\n**Decisions:**\n- Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n- Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped or converted to more usefull way by extracting information from it like nicknames.","9c9c5138":"# Creating a transformation pipline for the data","68309a5d":"**Observations:** \n- We observe significant correlation (>0.5) among Pclass=1 and Survived.\n\n**Decisions:**\n-  the feature sounds an important feature","2e1364d9":"**Observations:** \n- We confirm the observation during problem definition that most of people from 'Cherbourg (C) are survived , most of people from Southampton (S) and Queenstown unsurvived\n\n**Decisions:**\n-  the feature sounds an important feature"}}