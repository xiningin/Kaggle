{"cell_type":{"2e8d97b3":"code","de5aa59b":"code","877f7058":"code","33c8f511":"code","6a84398f":"code","6f607c0d":"code","a849b022":"code","6ff3dee2":"code","ad85b209":"code","3dd2d9b1":"code","ad02c1b0":"code","78fe05c2":"code","3d47469b":"code","5b44a834":"code","07d9a1e9":"code","a5ef61cd":"code","dceed7ab":"code","d0057f15":"code","62309afe":"code","0a23cd21":"code","86982d09":"code","16ea4138":"code","443fe62c":"code","af0fe7d7":"code","36bc5d0c":"code","155ff794":"code","da2e02b7":"code","47922fa8":"code","26d35199":"code","5e5e96ad":"markdown","fa9fc8fd":"markdown","8c4359b8":"markdown","2cc503fb":"markdown","b1c3fa6c":"markdown","fb54d1b4":"markdown","5461356c":"markdown","9940edac":"markdown","4d0d371b":"markdown"},"source":{"2e8d97b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport torch\nimport cv2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchvision\nfrom PIL import Image","de5aa59b":"!rm -r plates","877f7058":"#Unzip jpg files\n!unzip -o \/kaggle\/input\/platesv2\/plates.zip","33c8f511":"cwd = os.getcwd()\npath = os.path.join(cwd,'plates')\nprint(path)","6a84398f":"!mkdir -p plates\/valid\/cleaned\n!mkdir -p plates\/valid\/dirty","6f607c0d":"!shuf -n 10 -e \/kaggle\/working\/plates\/train\/cleaned\/* | xargs -i mv {} \/kaggle\/working\/plates\/valid\/cleaned\n!shuf -n 10 -e \/kaggle\/working\/plates\/train\/dirty\/* | xargs -i mv {} \/kaggle\/working\/plates\/valid\/dirty","a849b022":"print(os.listdir(os.path.join(cwd,'plates','valid','cleaned')))\nprint(os.listdir(os.path.join(cwd,'plates','valid','dirty')))","6ff3dee2":"transforms1 = transforms.Compose([\n    torchvision.transforms.RandomPerspective(distortion_scale=0.3, p=0.3, interpolation=3),\n    transforms.RandomRotation(20, resample=False, expand=False, center=None),\n    transforms.CenterCrop(224),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize((0, 0, 0), (1, 1, 1)),\n])\n","ad85b209":"class Dataloader(Dataset):\n    \n    def __init__(self,root_dir,transforms=None):\n        \n        self.root_dir = root_dir\n        self.classes = self._find_classes(self.root_dir)\n        self.samples = self.make_samples(self.root_dir,self.classes)\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self,idx):\n        \n        if ('train' in self.root_dir)|('valid' in self.root_dir):\n            \n            image = cv2.imread(self.samples[idx][0])\n            image = cv2.resize(image,(224,224))\n            target = self.samples[idx][1]\n            if self.transforms:\n                image = Image.fromarray(image)\n                image = self.transforms(image)\n            \n            \n            return image,target\n        \n        else:\n            image = cv2.imread(self.samples[idx])\n            image = cv2.resize(image,(224,224))\n            return image\n        \n    \n    def _find_classes(self,dir):\n        \n        if ('train' in dir)|('valid' in dir):\n            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n            classes_to_idx = {classes[i]: i for i in range(len(classes))}\n            classes_to_idx['dirty'] = 1\n            classes_to_idx['cleaned'] = 0\n            return classes_to_idx\n        else:\n            return None\n\n    def make_samples(self,dir,classes_to_idx):\n        images = []\n        if classes_to_idx != None:\n            for target in sorted(classes_to_idx.keys()):\n        \n                d = os.path.join(dir,target)\n            \n                for root, _, fnames in sorted(os.walk(d,followlinks=True)):\n                    for fname in sorted(fnames):\n                        if '.jpg' in fname:\n                            path = os.path.join(root,fname)\n                            item = (path, classes_to_idx[target])\n                            images.append(item)\n        else:\n            \n            d = dir\n            \n            for root, _, fnames in sorted(os.walk(d,followlinks=True)):\n                for fname in sorted(fnames):\n                    if '.jpg' in fname:\n                            path = os.path.join(root,fname)\n                            item = path\n                            images.append(item)\n            \n        return images\n        ","3dd2d9b1":"os.listdir(path)","ad02c1b0":"train_path = os.path.join(path,'train')\ntrain_dataset = Dataloader(train_path)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=0)","78fe05c2":"a = next(iter(train_dataloader))\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(1, 6, figsize=(25, 25))\nfor i in range(6):\n    axs[i].imshow(a[0][i].numpy())\n    if a[1][i].numpy() == 1:\n        axs[i].set_title('Dirty')\n    else:\n        axs[i].set_title('Cleaned')","3d47469b":"train_dataset = Dataloader(train_path,transforms=transforms1)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=0)\na = next(iter(train_dataloader))\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(1, 6, figsize=(25, 25))\nfor i in range(6):\n    axs[i].imshow(torch.transpose(a[0][i],0,2).numpy())\n    if a[1][i].numpy() == 1:\n        axs[i].set_title('Dirty')\n    else:\n        axs[i].set_title('Cleaned')","5b44a834":"train_path = os.path.join(path,'train')\ntrain_dataset = Dataloader(train_path,transforms=transforms1)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True, num_workers=0)\nvalid_path = os.path.join(path,'valid')\nvalid_dataset = Dataloader(valid_path,transforms=transforms1)\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=3, shuffle=True, num_workers=0)","07d9a1e9":"def train_model(model, loss, optimizer, scheduler, num_epochs):\n    global config\n    train_losses =  []\n    train_accs =  []\n    val_losses =  []\n    val_accs =  []\n  # Set model to training mode\n    #model.train()\n    for epoch in tqdm(range(num_epochs)):\n        running_loss = 0.\n        running_acc = 0.\n        print('Epoch {}\/{}:'.format(epoch, num_epochs), flush=True)\n\n        scheduler.step()\n        # Iterate over data.\n        for idx,(inputs, labels) in enumerate(train_dataloader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n\n            #preds = model(torch.transpose(inputs, 1, 3).float())\n            preds = model(inputs)#.transpose(inputs, 1, 3).float())\n            loss_value = loss(preds, labels)\n            preds_class = preds.argmax(dim=1)\n\n            loss_value.backward()\n            optimizer.step()\n\n            # statistics\n            running_loss += loss_value.item()\n            running_acc += (preds_class == labels.data).float().mean()\n            \n        train_loss = running_loss \/ len(train_dataloader)\n        train_acc = running_acc \/ len(train_dataloader)\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n\n        with torch.no_grad():\n            #model.eval()\n            running_loss = 0.\n            running_acc = 0.\n            for idx,(inputs,labels) in enumerate(valid_dataloader):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                #preds = model(torch.transpose(inputs, 1, 3).float())\n                preds = model(inputs)#.transpose(inputs, 1, 3).float())\n                loss_value = loss(preds, labels)\n                preds_class = preds.argmax(dim=1)\n                \n                running_loss += loss_value.item()\n                running_acc += (preds_class == labels.data).float().mean()\n\n        val_loss = running_loss \/ len(valid_dataloader)\n        val_acc = running_acc \/ len(valid_dataloader)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        print('train Loss: {:.4f} train Acc: {:.4f} val Loss: {:.4f} val Acc: {:.4f}'.format( train_loss, train_acc,val_loss,val_acc), flush=True)\n    return model, train_losses,val_losses ,train_accs, val_accs","a5ef61cd":"import torch.nn.functional as F","dceed7ab":"from torchvision import transforms, models\nclass PlatesNet(torch.nn.Module):\n    def __init__(self):\n        super(PlatesNet, self).__init__()\n        self.resnet = models.resnet152(pretrained=True, progress=False)\n\n        # Disable grad for all conv layers\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n\n        self.resnet.fc = torch.nn.Linear(self.resnet.fc.in_features, self.resnet.fc.in_features \/\/ 2)\n        self.act = torch.nn.Sigmoid()\n        self.fc = torch.nn.Linear(self.resnet.fc.in_features \/\/ 2, self.resnet.fc.in_features \/\/ 4)\n        self.act1 = torch.nn.Sigmoid()\n        self.fc1 = torch.nn.Linear(self.resnet.fc.in_features \/\/ 4, 2)\n    \n    def forward(self, X):\n        X = self.resnet(X)\n        X = self.act(X)\n        X = self.fc(X)\n        X = self.act1(X)\n        X = self.fc1(X)\n        \n        return X","d0057f15":"model = PlatesNet()","62309afe":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)","0a23cd21":"from tqdm import tqdm_notebook as tqdm","86982d09":"model, train_loss,val_loss, train_acc, val_acc = train_model(model, loss, optimizer, scheduler, num_epochs=200)","16ea4138":"#import matplotlib.pyplot as plt\nfig, axs = plt.subplots(2, 2, figsize=(15, 5))\naxs[0,0].plot(train_acc)\naxs[0,0].set_title('Train Accuracy')\naxs[0,1].plot(train_loss)\naxs[0,1].set_title('Train Loss')\naxs[1,0].plot(val_acc)\naxs[1,0].set_title('Val Accuracy')\naxs[1,1].plot(val_loss)\naxs[1,1].set_title('Val Loss')","443fe62c":"test_path = os.path.join(path,'test')\ntest_dataset = Dataloader(test_path)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=6, shuffle=True, num_workers=0)","af0fe7d7":"predictions = []\n#model.eval()\nwith torch.no_grad():\n    for idx,(inputs) in tqdm(enumerate(test_dataloader),total=int(len(test_dataset)\/6)):\n        pred = model(torch.transpose(inputs, 1, 3).cuda().float())\n        predictions.append(np.argmax(pred.detach().cpu().numpy(),axis=1))\n\ndel model\npredictions = np.hstack(predictions)","36bc5d0c":"predictions","155ff794":"submission = pd.read_csv('\/kaggle\/input\/platesv2\/sample_submission.csv')\nsubmission.label = predictions\ndecoder = {1:'dirty',0:'cleaned'}\nsubmission.label = submission.label.map(decoder)","da2e02b7":"submission","47922fa8":"submission.to_csv('submission.csv', index=False)","26d35199":"!rm -r plates","5e5e96ad":"#### I think the format is slightly changed(to zip file) after I join the playground, so we have to unzip image files first.","fa9fc8fd":"# Train\nI cited training model from this kernel https:\/\/www.kaggle.com\/heyhey7\/baseline-in-pytorch by Roman Bezborodov.\nIf you need to study in a concrete way, then you can go to this kernel and can try it ;-)\nI erase some parts for berevity. since the number of train images is such a small, I don't think it is needed. <- It is needed","8c4359b8":"#### I found commit error occured when the files unzipped are not removed, so I will remove it below","2cc503fb":"# Prediction","b1c3fa6c":"# Check Images","fb54d1b4":"#### Before Augmentation","5461356c":"# Submission","9940edac":"# Dataloader","4d0d371b":"#### After Augmentation"}}