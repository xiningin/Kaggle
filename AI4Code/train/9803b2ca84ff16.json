{"cell_type":{"243b1e2a":"code","c0616f3b":"code","fc7c18ef":"code","2f28e617":"code","de1b9606":"code","e2a3ed27":"code","64db0efe":"code","50bee9f2":"code","64af6159":"code","dce9078b":"code","7ef54513":"code","12347af6":"code","931776ac":"code","0762d13a":"code","47b25880":"code","71fe28bf":"code","251577b0":"code","4c306c8c":"code","0eabe261":"code","d884fd11":"code","dab5406d":"code","b16fe404":"code","41a1ba2e":"code","c074e8a3":"code","2ce8bba3":"code","579d1dd5":"code","dbf566d2":"code","955d06c0":"code","fb8a787b":"code","fd2ecab3":"code","fd396038":"code","6c34c55c":"code","cf94c956":"code","ba38e662":"code","e6c3a906":"code","d31853a7":"code","425ec2db":"code","34d06166":"code","5a64132e":"code","b5ba504f":"code","f580ff6d":"code","61a18bbf":"code","3bedd174":"markdown","ad07d523":"markdown","8ffa60e9":"markdown","9055ec82":"markdown","ba137a71":"markdown","2de1812d":"markdown","31683a5d":"markdown","81a5a3d9":"markdown","e237a153":"markdown","cd3941e7":"markdown","2d4b568c":"markdown","60fabd2d":"markdown","dad87d56":"markdown","50a2a2c2":"markdown","f22d27e9":"markdown","6b697360":"markdown","5d86796f":"markdown","71ebe0f4":"markdown","e5b79194":"markdown","8d5cc739":"markdown","57d0524e":"markdown","1f89b2c6":"markdown","9344421f":"markdown","d34e6598":"markdown"},"source":{"243b1e2a":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndf = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\ndf[:3]","c0616f3b":"df[:3]","fc7c18ef":"df = df[['title','abstract','authors']]","2f28e617":"df.isnull().sum()","de1b9606":"df.dropna(inplace=True)","e2a3ed27":"df.isnull().sum()","64db0efe":"df[df.duplicated()].__len__()","50bee9f2":"df.drop_duplicates(inplace=True)\ndf[df.duplicated()].__len__()","64af6159":"df[:3]","dce9078b":"import tensorflow as tf\nimport tensorflow_hub as hub\nclass UniversalSenteneceEncoder:\n\n    def __init__(self, encoder='universal-sentence-encoder', version='4'):\n        self.version = version\n        self.encoder = encoder\n        self.embd = hub.load(f\"https:\/\/tfhub.dev\/google\/{encoder}\/{version}\")\n\n    def embed(self, sentences):\n        return self.embd(sentences)\n\n    def squized(self, sentences):\n        return np.array(self.embd(tf.squeeze(tf.cast(sentences, tf.string))))","7ef54513":"encoder = UniversalSenteneceEncoder(encoder='universal-sentence-encoder',version='4')","12347af6":"%%time\ndf = df[:5000]\ndf['title_sent_vects'] = encoder.squized(df.title.values).tolist()\n# df['abstract_sent_vects'] = encoder.embed(df.abstract.values)","931776ac":"import plotly.graph_objects as go\n\nsents = 50\nlabels = df[:sents].title.values\nfeatures = df[:sents].title_sent_vects.values.tolist()\n\nfig = go.Figure(data=go.Heatmap(\n                    z=np.inner(features, features),\n                    x=labels,\n                    y=labels,\n                    colorscale='Viridis',\n                    ))\n\nfig.update_layout(\n    margin=dict(l=40, r=40, t=40, b=40),\n    height=1000,\n    xaxis=dict(\n        autorange=True,\n        showgrid=False,\n        ticks='',\n        showticklabels=False\n    ),\n    yaxis=dict(\n        autorange=True,\n        showgrid=False,\n        ticks='',\n        showticklabels=False\n    )\n)\n\nfig.show()","0762d13a":"%%time\ndf = df[:5000]\ndf['abstract_sent_vects'] = encoder.squized(df.abstract.values).tolist()\n# df['abstract_sent_vects'] = encoder.embed(df.abstract.values)","47b25880":"import plotly.graph_objects as go\n\nsents = 30\nlabels = df[:sents].abstract.values\nfeatures = df[:sents].abstract_sent_vects.values.tolist()\n\nfig = go.Figure(data=go.Heatmap(\n                    z=np.inner(features, features),\n                    x=labels,\n                    y=labels,\n                    colorscale='Viridis',\n                    ))\n\nfig.update_layout(\n    margin=dict(l=140, r=140, t=140, b=140),\n    height=1000,\n    xaxis=dict(\n        autorange=True,\n        showgrid=False,\n        ticks='',\n        showticklabels=False\n    ),\n    yaxis=dict(\n        autorange=True,\n        showgrid=False,\n        ticks='',\n        showticklabels=False\n    ),\n    hoverlabel = dict(namelength = -1)\n)\n\nfig.show()","71fe28bf":"from sklearn.cluster import KMeans\nimport matplotlib.cm as cm\nfrom sklearn.decomposition import PCA\nfrom datetime import datetime\nimport plotly.express as px","251577b0":"%%time\nn_clusters = 10\nvectors = df.title_sent_vects.values.tolist()\nkmeans = KMeans(n_clusters = n_clusters, init = 'k-means++', random_state = 0)\nkmean_indices = kmeans.fit_predict(vectors)\n\npca = PCA(n_components=512)\nscatter_plot_points = pca.fit_transform(vectors)\n\ntmp = pd.DataFrame({\n    'Feature space for the 1st feature': scatter_plot_points[:,0],\n    'Feature space for the 2nd feature': scatter_plot_points[:,1],\n    'labels': kmean_indices,\n    'title': df.title.values.tolist()[:vectors.__len__()]\n})\n\nfig = px.scatter(tmp, x='Feature space for the 1st feature', y='Feature space for the 2nd feature', color='labels',\n                 size='labels', hover_data=['title'])\nfig.update_layout(\n    margin=dict(l=20, r=20, t=20, b=20),\n    height=1000\n)\n\nfig.show()","4c306c8c":"%%time\nn_clusters = 10\nvectors = df.abstract_sent_vects.values.tolist()\nkmeans = KMeans(n_clusters = n_clusters, init = 'k-means++', random_state = 0)\nkmean_indices = kmeans.fit_predict(vectors)\n\npca = PCA(n_components=512)\nscatter_plot_points = pca.fit_transform(vectors)\n\ntmp = pd.DataFrame({\n    'Feature space for the 1st feature': scatter_plot_points[:,0],\n    'Feature space for the 2nd feature': scatter_plot_points[:,1],\n    'labels': kmean_indices,\n    'title': df.abstract.values.tolist()[:vectors.__len__()]\n})\n\nfig = px.scatter(tmp, x='Feature space for the 1st feature', y='Feature space for the 2nd feature', color='labels',\n                 size='labels', hover_data=['title'])\nfig.update_layout(\n    margin=dict(l=20, r=20, t=20, b=20),\n    height=1000\n)\n\nfig.show()","0eabe261":"import networkx as nx","d884fd11":"from sklearn.metrics.pairwise import cosine_similarity","dab5406d":"%%time \nsdf = df.copy()\nsimilarity_matrix = cosine_similarity(sdf.title_sent_vects.values.tolist())","b16fe404":"simdf = pd.DataFrame(\n    similarity_matrix,\n    columns = sdf.title.values.tolist(),\n    index = sdf.title.values.tolist()\n)","41a1ba2e":"long_form = simdf.unstack()\n# rename columns and turn into a dataframe\nlong_form.index.rename(['t1', 't2'], inplace=True)\nlong_form = long_form.to_frame('sim').reset_index()","c074e8a3":"long_form = long_form[long_form.t1 != long_form.t2]\nlong_form[:3]","2ce8bba3":"!pip install python-igraph","579d1dd5":"import igraph as ig","dbf566d2":"%%time\nlng = long_form[long_form.sim > 0.75] \ntuples = [tuple(x) for x in lng.values]\nGm = ig.Graph.TupleList(tuples, edge_attrs = ['sim'])\nlayt=Gm.layout('kk', dim=3)","955d06c0":"Xn=[layt[k][0] for k in range(layt.__len__())]# x-coordinates of nodes\nYn=[layt[k][1] for k in range(layt.__len__())]# y-coordinates\nZn=[layt[k][2] for k in range(layt.__len__())]# z-coordinates\nXe=[]\nYe=[]\nZe=[]\nfor e in Gm.get_edgelist():\n    Xe+=[layt[e[0]][0],layt[e[1]][0], None]# x-coordinates of edge ends\n    Ye+=[layt[e[0]][1],layt[e[1]][1], None]\n    Ze+=[layt[e[0]][2],layt[e[1]][2], None]","fb8a787b":"import plotly.graph_objs as go","fd2ecab3":"trace1 = go.Scatter3d(x = Xe,\n  y = Ye,\n  z = Ze,\n  mode = 'lines',\n  line = dict(color = 'rgb(0,0,0)', width = 1),\n  hoverinfo = 'none'\n)\n\ntrace2 = go.Scatter3d(x = Xn,\n  y = Yn,\n  z = Zn,\n  mode = 'markers',\n  name = 'articles',\n  marker = dict(symbol = 'circle',\n    size = 6, \n    # color = group,\n    colorscale = 'Viridis',\n    line = dict(color = 'rgb(50,50,50)', width = 0.5)\n  ),\n  text = lng.t1.values.tolist(),\n  hoverinfo = 'text'\n)\n\naxis = dict(showbackground = False,\n  showline = False,\n  zeroline = False,\n  showgrid = False,\n  showticklabels = False,\n  title = ''\n)\n\nlayout = go.Layout(\n  title = \"Network of similarity between CORD-19 Articles(3D visualization)\",\n  width = 1500,\n  height = 1500,\n  showlegend = False,\n  scene = dict(\n    xaxis = dict(axis),\n    yaxis = dict(axis),\n    zaxis = dict(axis),\n  ),\n  margin = dict(\n    t = 100,\n    l = 20,\n    r = 20\n  ),\n\n)","fd396038":"fig=go.Figure(data=[trace1,trace2], layout=layout)\n\nfig.show()","6c34c55c":"sim_weight = 0.75\ngdf = long_form[long_form.sim > sim_weight]","cf94c956":"plt.figure(figsize=(50,50))\npd_graph = nx.Graph()\npd_graph = nx.from_pandas_edgelist(gdf, 't1', 't2')\npos = nx.spring_layout(pd_graph)\nnx.draw_networkx(pd_graph,pos,with_labels=True,font_size=10, node_size = 30)","ba38e662":"betCent = nx.betweenness_centrality(pd_graph, normalized=True, endpoints=True)\nnode_color = [20000.0 * pd_graph.degree(v) for v in pd_graph]\nnode_size =  [v * 10000 for v in betCent.values()]\nplt.figure(figsize=(35,35))\nnx.draw_networkx(pd_graph, pos=pos, with_labels=True,\n                 font_size=5,\n                 node_color=node_color,\n                 node_size=node_size )","e6c3a906":"l=list(nx.connected_components(pd_graph))\n\nL=[dict.fromkeys(y,x) for x, y in enumerate(l)]\n\nd=[{'articles':k , 'groups':v }for d in L for k, v in d.items()]","d31853a7":"gcd = pd.DataFrame.from_dict(d)","425ec2db":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt') \nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.tokenize import RegexpTokenizer\n\ntok = RegexpTokenizer(r'\\w+')\nstop_words = set(stopwords.words('english')) \ndef clean(string):\n    return \" \".join([w for w in word_tokenize(\" \".join(tok.tokenize(string))) if not w in stop_words])\n\ngcd.articles = gcd.articles.apply(lambda x: clean(x))","34d06166":"gcd.__len__(),gcd.__len__() \/ df.__len__()","5a64132e":"from wordcloud import WordCloud","b5ba504f":"%%time\nclouds = dict()\n\nbig_groups = pd.DataFrame({\n    'counts':gcd.groups.value_counts()\n    }).sort_values(by='counts',ascending=False)[:9].index.values.tolist()\n\nfor group in big_groups:\n    text = gcd[gcd.groups == group].articles.values\n    wordcloud = WordCloud(width=1000, height=1000).generate(str(text))\n    clouds[group] = wordcloud","f580ff6d":"def plot_figures(figures, nrows = 1, ncols=1):\n    \"\"\"Plot a dictionary of figures.\n\n    Parameters\n    ----------\n    figures : <title, figure> dictionary\n    ncols : number of columns of subplots wanted in the display\n    nrows : number of rows of subplots wanted in the figure\n    \"\"\"\n\n    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows,figsize=(20,20))\n    for ind,title in zip(range(len(figures)), figures):\n        axeslist.ravel()[ind].imshow(figures[title], cmap=plt.jet())\n        axeslist.ravel()[ind].set_title(f'Most Freqent words for the group {title+1}')\n        axeslist.ravel()[ind].set_axis_off()\n    plt.tight_layout() # optional","61a18bbf":"plot_figures(clouds, 3, 3)\nplt.show()","3bedd174":"Here we prepare some simple clustering functions to be used later on","ad07d523":"## Abstract Features Correlation","8ffa60e9":"# CORD-19 Clustering Articles and Papers related to Coronavirus","9055ec82":"Make a copy of our dataframe and create the similarity matrix for the extracted title vectors","ba137a71":"We create our graph from our dataframe","2de1812d":"## Graphs","31683a5d":"## Tensorflow Upgrade\n","81a5a3d9":"Now let's try to find communities in our graph","e237a153":"### Abstract Level Clustering","cd3941e7":"We've got our 'clustered' dataframe of articles, however since we filtered the data to take just the most similar articles, we've ended up havin left just 660 from the 5k data","2d4b568c":"#### Preparing the data for graphs","60fabd2d":"## Title Feature correlation","dad87d56":"## Text features extraction","50a2a2c2":"#### Finding Communities","f22d27e9":"#### Creating word clouds for grooups","6b697360":"Let's unstack them to add them easier into our graph","5d86796f":"So far we are interested just in title,abstract and maybe authors\n\n\n\n*   First we get rid of rows with missing values\n*   Then we get rid of the duplicated values\n\n","71ebe0f4":"## PCA & Clustering","e5b79194":"Since this process is quite consuming we will slice the dataset","8d5cc739":"### Now let's get our groups","57d0524e":"Next step, we filter them nodes with higher similarity","1f89b2c6":"#### Plotly Graph","9344421f":"### Title Level Clustering","d34e6598":"Add them into a dataframe, similiar to what a heatmap looks likes"}}