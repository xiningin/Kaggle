{"cell_type":{"874c5c7d":"code","78aa99f0":"code","07014066":"code","297f6a4d":"code","1eb55025":"code","4f8fc3ab":"code","36641443":"code","a56a4879":"code","53dada72":"code","8ee5f3a3":"code","aa4b5781":"code","442440c0":"code","e64122d9":"code","57e5f812":"code","c7257162":"code","1ed99a18":"code","87fb2497":"code","22d1e199":"code","7362759f":"code","d4d34251":"code","774ca729":"code","fdeaa745":"code","a89acbed":"code","0a8bbfe6":"code","f8b94538":"code","b6829be0":"code","03d5117a":"code","3ccb41ee":"code","94ea43f1":"code","2bb00ba3":"code","1f91f20c":"code","31f99d64":"code","3b565ed2":"code","4f3df773":"code","9c0c834f":"code","3ccd4d58":"code","9d3d84bb":"code","8edc1ea3":"code","7c44425a":"code","63197cd9":"code","fae1f1e4":"code","4c43b1f8":"code","79733167":"code","e6946b58":"code","1094d4e8":"code","52621627":"code","3322fc7e":"code","8d8736f9":"code","2cfb5d57":"code","c980dc43":"code","0f571434":"code","3d9f0460":"code","51313672":"code","fcd21d32":"code","2f7a51e5":"code","dce038b9":"code","efe789b8":"markdown","bc2f657c":"markdown","be9ff987":"markdown","8ae85291":"markdown","9406681e":"markdown","d98f7e98":"markdown","dfec52bc":"markdown","58a96c17":"markdown","34876871":"markdown","00f5f2a7":"markdown","115b8031":"markdown","bcd95a6b":"markdown","397cd34b":"markdown","e0e7df5b":"markdown","35f63267":"markdown","2b77264b":"markdown","b7f09e39":"markdown","30547dbd":"markdown","ada95033":"markdown","308bb59b":"markdown","d08002b0":"markdown","faae0fbf":"markdown","4ba01e23":"markdown","bf955df0":"markdown","1e9546b6":"markdown","a54b2631":"markdown","ee52e63d":"markdown","c6ce2826":"markdown","0d9ffc68":"markdown","4ff08fa0":"markdown","27207486":"markdown","9c0716d1":"markdown","9723ad8c":"markdown","cc3d6188":"markdown","3e6128ba":"markdown","43a1c926":"markdown","64725a55":"markdown","fbc6f920":"markdown","c3057812":"markdown","c8725396":"markdown","0d050ce0":"markdown","917fd291":"markdown","2d8796f3":"markdown","e8b4789a":"markdown","fe09aad7":"markdown","f6bc3fe9":"markdown","d390e061":"markdown","bcd4afc3":"markdown","d2c1f634":"markdown","df97800e":"markdown","86a5a81f":"markdown","d3c1fad9":"markdown","c106ddde":"markdown","7b373430":"markdown","d1f8966a":"markdown","4a0121d9":"markdown","98b11832":"markdown","6b4042a8":"markdown","4598d035":"markdown","0fd2ffe8":"markdown","90284a52":"markdown","5816b870":"markdown","21e32513":"markdown","11351a03":"markdown","0335f57b":"markdown","d1f90163":"markdown","8cf4dc77":"markdown","e8879985":"markdown","5bc9643f":"markdown","36b3b817":"markdown","bb5839c8":"markdown","7627abab":"markdown","166095b1":"markdown","c07e9c39":"markdown","13e47e41":"markdown","9a8c1f02":"markdown","5fe003ac":"markdown","153f0d20":"markdown"},"source":{"874c5c7d":"# @title Tutorial slides\n\n# @markdown These are the slides for the videos in the tutorial\n\n# @markdown If you want to locally download the slides, click [here](https:\/\/osf.io\/3zn9w\/download)\nfrom IPython.display import IFrame\nIFrame(src=f\"https:\/\/mfr.ca-1.osf.io\/render?url=https:\/\/osf.io\/3zn9w\/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)","78aa99f0":"# @title Install dependencies\n!pip install coloredlogs --quiet\n\n!pip install git+https:\/\/github.com\/NeuromatchAcademy\/evaltools --quiet\nfrom evaltools.airtable import AirtableForm\n\n# generate airtable form\natform = AirtableForm('appn7VdPRseSoMXEG','W3D3_T1','https:\/\/portal.neuromatchacademy.org\/api\/redirect\/to\/2baacd95-3fb5-4399-bf95-bbe5de255d2b')","07014066":"# Imports\nimport os\nimport math\nimport time\nimport torch\nimport random\nimport logging\nimport coloredlogs\n\nimport numpy as np\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom tqdm.notebook import tqdm\nfrom pickle import Unpickler\n\nlog = logging.getLogger(__name__)\ncoloredlogs.install(level='INFO')  # Change this to DEBUG to see more info.","297f6a4d":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# for DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","1eb55025":"# @title Set device (GPU or CPU). Execute `set_device()`\n# especially if torch modules used.\n\n# inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","4f8fc3ab":"SEED = 2021\nset_seed(seed=SEED)\nDEVICE = set_device()","36641443":"# @title Download the modules\n\n# @markdown Run this cell!\n\n# @markdown Download from OSF. Original repo: https:\/\/github.com\/raymondchua\/nma_rl_games.git\n\nimport os, io, sys, shutil, zipfile\nfrom urllib.request import urlopen\n\n# download from github repo directly\n#!git clone git:\/\/github.com\/raymondchua\/nma_rl_games.git --quiet\nREPO_PATH = 'nma_rl_games'\n\nif os.path.exists(REPO_PATH):\n  download_string = \"Redownloading\"\n  shutil.rmtree(REPO_PATH)\nelse:\n  download_string = \"Downloading\"\n\nzipurl = 'https:\/\/osf.io\/kf4p9\/download'\nprint(f\"{download_string} and unzipping the file... Please wait.\")\nwith urlopen(zipurl) as zipresp:\n  with zipfile.ZipFile(io.BytesIO(zipresp.read())) as zfile:\n    zfile.extractall()\nprint(\"Download completed.\")\n\nprint(f\"Add the {REPO_PATH} in the path and import the modules.\")\n# add the repo in the path\nsys.path.append('nma_rl_games\/alpha-zero')\n\n# @markdown Import modules designed for use in this notebook\nimport Arena\n\nfrom utils import *\nfrom Game import Game\nfrom MCTS import MCTS\nfrom NeuralNet import NeuralNet\n\nfrom othello.OthelloPlayers import *\nfrom othello.OthelloLogic import Board # our board object\nfrom othello.OthelloGame import OthelloGame # our game object\nfrom othello.pytorch.NNet import NNetWrapper as NNet","a56a4879":"args = dotdict({\n    'numIters': 1,            # in training setting this was 1000 and num of episodes=100\n    'numEps': 1,              # Number of complete self-play games to simulate during a new iteration.\n    'tempThreshold': 15,      # To control exploration and exploitation\n    'updateThreshold': 0.6,   # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n    'maxlenOfQueue': 200,     # Number of game examples to train the neural networks.\n    'numMCTSSims': 15,        # Number of games moves for MCTS to simulate.\n    'arenaCompare': 10,       # Number of games to play during arena play to determine if new net will be accepted.\n    'cpuct': 1,\n    'maxDepth':5,             # Maximum number of rollouts\n    'numMCsims': 5,           # Number of monte carlo simulations\n    'mc_topk': 3,             # top k actions for monte carlo rollout\n\n    'checkpoint': '.\/temp\/',\n    'load_model': False,\n    'load_folder_file': ('\/dev\/models\/8x100x50','best.pth.tar'),\n    'numItersForTrainExamplesHistory': 20,\n\n    # define neural network arguments\n    'lr': 0.001,               # lr: learning rate\n    'dropout': 0.3,\n    'epochs': 10,\n    'batch_size': 64,\n    'device': DEVICE,\n    'num_channels': 512,\n})","53dada72":"# @title Video 0: Introduction\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Yh411B7EP\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"5kQ-xGbjlJo\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 0: Introduction')\n\ndisplay(out)","8ee5f3a3":"# @title Video 1: A game loop for RL\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Wy4y1V7bt\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"aH2Hs8f6KrQ\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 1: A game loop for RL')\n\ndisplay(out)","aa4b5781":"class Test(object):\n    def __init__(self,n):\n        self.n = n\n    \n    @staticmethod\n    def sqrt(self, n):\n        return sqrt*sqrt\n    ","442440c0":"class OthelloGame(Game):\n  square_content = {\n      -1: \"X\",\n      +0: \"-\",\n      +1: \"O\"\n      }\n\n  @staticmethod\n  def getSquarePiece(piece):\n    return OthelloGame.square_content[piece]\n\n  def __init__(self, n):\n    self.n = n\n\n  def getInitBoard(self):\n    # return initial board (numpy board)\n    b = Board(self.n)\n    return np.array(b.pieces)\n\n  def getBoardSize(self):\n    # (a,b) tuple\n    return (self.n, self.n)\n\n  def getActionSize(self):\n    # return number of actions, n is the board size and +1 is for no-op action\n    return self.n*self.n + 1\n\n  def getNextState(self, board, player, action):\n    # if player takes action on board, return next (board,player)\n    # action must be a valid move\n    if action == self.n*self.n:\n      return (board, -player)\n    b = Board(self.n)\n    b.pieces = np.copy(board)\n    move = (int(action\/self.n), action%self.n)\n    b.execute_move(move, player)\n    return (b.pieces, -player)\n\n  def getValidMoves(self, board, player):\n    # return a fixed size binary vector\n    valids = [0]*self.getActionSize()\n    b = Board(self.n)\n    b.pieces = np.copy(board)\n    legalMoves =  b.get_legal_moves(player)\n    if len(legalMoves)==0:\n      valids[-1]=1\n      return np.array(valids)\n    for x, y in legalMoves:\n      valids[self.n*x+y]=1\n    return np.array(valids)\n\n  def getGameEnded(self, board, player):\n    # return 0 if not ended, 1 if player 1 won, -1 if player 1 lost\n    # player = 1\n    b = Board(self.n)\n    b.pieces = np.copy(board)\n    if b.has_legal_moves(player):\n      return 0\n    if b.has_legal_moves(-player):\n      return 0\n    if b.countDiff(player) > 0:\n      return 1\n    return -1\n\n  def getCanonicalForm(self, board, player):\n    # return state if player==1, else return -state if player==-1\n    return player*board\n\n  def getSymmetries(self, board, pi):\n    # mirror, rotational\n    assert(len(pi) == self.n**2+1)  # 1 for pass\n    pi_board = np.reshape(pi[:-1], (self.n, self.n))\n    l = []\n\n    for i in range(1, 5):\n      for j in [True, False]:\n        newB = np.rot90(board, i)\n        newPi = np.rot90(pi_board, i)\n        if j:\n          newB = np.fliplr(newB)\n          newPi = np.fliplr(newPi)\n        l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n    return l\n\n  def stringRepresentation(self, board):\n    return board.tobytes()\n\n  def stringRepresentationReadable(self, board):\n    board_s = \"\".join(self.square_content[square] for row in board for square in row)\n    return board_s\n\n  def getScore(self, board, player):\n    b = Board(self.n)\n    b.pieces = np.copy(board)\n    return b.countDiff(player)\n\n  @staticmethod\n  def display(board):\n    n = board.shape[0]\n    print(\"   \", end=\"\")\n    for y in range(n):\n      print(y, end=\" \")\n    print(\"\")\n    print(\"-----------------------\")\n    for y in range(n):\n      print(y, \"|\", end=\"\")    # print the row #\n      for x in range(n):\n        piece = board[y][x]    # get the piece to print\n        print(OthelloGame.square_content[piece], end=\" \")\n      print(\"|\")\n    print(\"-----------------------\")","e64122d9":"class RandomPlayer():\n  def __init__(self, game):\n    self.game = game\n\n  def play(self, board):\n\n    #################################################\n    ## TODO for students: ##\n    ## 1. Please compute the valid moves using getValidMoves(). ##\n    ## 2. Compute the probability over actions.##\n    ## 3. Pick a random action based on the probability computed above.##\n    # Fill out function and remove ##\n    # raise NotImplementedError(\"Implement the random player\")\n    #################################################\n\n    valids = self.game.getValidMoves(board = board,player = 1) # board and player\n    prob = valids\/np.sum(valids) # probability of valid options\n    a = np.random.choice(self.game.getActionSize(), p = prob) # random choices form probabilities vector\n\n    return a\n\n\n# add event to airtable\natform.add_event('Coding Exercise 1.1: Implement a random player')","57e5f812":"# Display the board\nset_seed(seed=SEED)\ngame = OthelloGame(6)\nboard = game.getInitBoard()\ngame.display(board)","c7257162":"# observe the game board size\nprint(f'Board size = {game.getBoardSize()}')\n\n# observe the action size\nprint(f'Action size = {game.getActionSize()}') #  6*6 + 1 (1 is for no-option) ","1ed99a18":"# define the random player\nplayer1 = RandomPlayer(game).play  # player 1 is a random player\nplayer2 = RandomPlayer(game).play  # player 2 is a random player\n\n# define number of games\nnum_games = 20\n\n# start the competition\nset_seed(seed=SEED)\narena = Arena.Arena(player1, player2 , game, display=None)  # to see the steps of the competition set \"display=OthelloGame.display\"\nresult = arena.playGames(num_games, verbose=False)  # return  ( number of games won by player1, num of games won by player2, num of games won by nobody)\nprint(f\"\\n\\n{result}\")","87fb2497":"print(f\"Number of games won by player1 = {result[0]}, \"\n      f\"Number of games won by player2 = {result[1]} out of {num_games} games\")\nwin_rate_player1 = result[0]\/num_games\nprint(f\"\\nWin rate for player1 over 20 games: {round(win_rate_player1*100, 1)}%\")","22d1e199":"# @title Video 2: Train a value function\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1pg411j7f7\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"f9lZq0WQJFg\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 2: Train a value function')\n\ndisplay(out)","7362759f":"def loadTrainExamples(folder, filename):\n  trainExamplesHistory = []\n  modelFile = os.path.join(folder, filename)\n  examplesFile = modelFile + \".examples\"\n  if not os.path.isfile(examplesFile):\n    print(f'File \"{examplesFile}\" with trainExamples not found!')\n    r = input(\"Continue? [y|n]\")\n    if r != \"y\":\n      sys.exit()\n  else:\n    print(\"File with train examples found. Loading it...\")\n    with open(examplesFile, \"rb\") as f:\n      trainExamplesHistory = Unpickler(f).load()\n    print('Loading done!')\n    # examples based on the model were already collected (loaded)\n    return trainExamplesHistory","d4d34251":"path = \"nma_rl_games\/alpha-zero\/pretrained_models\/data\/\"\nloaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')","774ca729":"class OthelloNNet(nn.Module):\n  def __init__(self, game, args):\n    # game params\n    self.board_x, self.board_y = game.getBoardSize()\n    self.action_size = game.getActionSize()\n    self.args = args\n\n    super(OthelloNNet, self).__init__()\n    self.conv1 = nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1)\n    self.conv2 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1,\n                           padding=1)\n    self.conv3 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n    self.conv4 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n\n    self.bn1 = nn.BatchNorm2d(args.num_channels)\n    self.bn2 = nn.BatchNorm2d(args.num_channels)\n    self.bn3 = nn.BatchNorm2d(args.num_channels)\n    self.bn4 = nn.BatchNorm2d(args.num_channels)\n\n    self.fc1 = nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024)\n    self.fc_bn1 = nn.BatchNorm1d(1024)\n\n    self.fc2 = nn.Linear(1024, 512)\n    self.fc_bn2 = nn.BatchNorm1d(512)\n\n    self.fc3 = nn.Linear(512, self.action_size)\n\n    self.fc4 = nn.Linear(512, 1)\n\n  def forward(self, s):\n    # s: batch_size x board_x x board_y\n    s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n    s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n    s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n    s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n    s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n    s = s.view(-1, self.args.num_channels * (self.board_x - 4) * (self.board_y - 4))\n\n    s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n    s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n\n    pi = self.fc3(s)  # batch_size x action_size\n    v = self.fc4(s)   # batch_size x 1\n    #################################################\n    ## TODO for students: Please compute a probability distribution over 'pi' using log softmax (for numerical stability)\n    # Fill out function and remove\n    # raise NotImplementedError(\"Calculate the probability distribution and the value\")\n    #################################################\n    # return a probability distribution over actions at the current state and the value of the current state.\n    return torch.nn.functional.log_softmax(pi, dim=1), torch.tanh(v)\n\n\n# add event to airtable\natform.add_event('Coding Exercise 2.2: Implement the NN OthelloNNet for Othello')","fdeaa745":"class ValueNetwork(NeuralNet):\n  def __init__(self, game):\n    self.nnet = OthelloNNet(game, args)\n    self.board_x, self.board_y = game.getBoardSize()\n    self.action_size = game.getActionSize()\n\n    self.nnet.to(args.device)\n\n  def train(self, games):\n    \"\"\"\n    examples: list of examples, each example is of form (board, pi, v)\n    \"\"\"\n    optimizer = optim.Adam(self.nnet.parameters())\n    for examples in games:\n      for epoch in range(args.epochs):\n        print('EPOCH ::: ' + str(epoch + 1))\n        self.nnet.train()\n        v_losses = []   # to store the losses per epoch\n        batch_count = int(len(examples) \/ args.batch_size)  # len(examples)=200, batch-size=64, batch_count=3\n        t = tqdm(range(batch_count), desc='Training Value Network')\n        for _ in t:\n          sample_ids = np.random.randint(len(examples), size=args.batch_size)  # read the ground truth information from MCTS simulation using the loaded examples\n          boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))  # length of boards, pis, vis = 64\n          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n          target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n\n          # predict\n          boards, target_vs = boards.contiguous().to(args.device), target_vs.contiguous().to(args.device)\n\n          #################################################\n          ## TODO for students:\n          ## 1. Compute the value predicted by OthelloNNet() ##\n          ## 2. First implement the loss_v() function below and then use it to update the value loss. ##\n          # Fill out function and remove\n          # raise NotImplementedError(\"Compute the output\")\n          #################################################\n          # compute output\n          _, out_v = self.nnet()\n          l_v = self.loss_v(target = target_vs, output = out_v)  # total loss\n\n          # record loss\n          v_losses.append(l_v.item())\n          t.set_postfix(Loss_v=l_v.item())\n\n          # compute gradient and do SGD step\n          optimizer.zero_grad()\n          l_v.backward()\n          optimizer.step()\n\n  def predict(self, board):\n    \"\"\"\n    board: np array with board\n    \"\"\"\n    # timing\n    start = time.time()\n\n    # preparing input\n    board = torch.FloatTensor(board.astype(np.float64))\n    board = board.contiguous().to(args.device)\n    board = board.view(1, self.board_x, self.board_y)\n    self.nnet.eval()\n    with torch.no_grad():\n        _, v = self.nnet(board)\n    return v.data.cpu().numpy()[0]\n\n  def loss_v(self, targets, outputs):\n    #################################################\n    ## TODO for students: Please compute Mean squared error and return as output. ##\n    # Fill out function and remove\n    raise NotImplementedError(\"Calculate the loss\")\n    #################################################\n    # Mean squared error (MSE)\n    return torch.F.mse_loss(targets, outputs)\n\n  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n    filepath = os.path.join(folder, filename)\n    if not os.path.exists(folder):\n      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n      os.mkdir(folder)\n    else:\n      print(\"Checkpoint Directory exists! \")\n    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n    print(\"Model saved! \")\n\n  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n    # https:\/\/github.com\/pytorch\/examples\/blob\/master\/imagenet\/main.py#L98\n    filepath = os.path.join(folder, filename)\n    if not os.path.exists(filepath):\n      raise (\"No model in path {}\".format(filepath))\n\n    checkpoint = torch.load(filepath, map_location=args.device)\n    self.nnet.load_state_dict(checkpoint['state_dict'])\n\n\n# add event to airtable\natform.add_event('Coding Exercise 2.3: Implement the ValueNetwork')","a89acbed":"if not os.listdir('nma_rl_games\/alpha-zero\/pretrained_models\/models\/'):\n  set_seed(seed=SEED)\n  game = OthelloGame(6)\n  vnet = ValueNetwork(game)\n  vnet.train(loaded_games)","0a8bbfe6":"# @title Video 3: Play games using a value function\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Ug411j7ig\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"tvmzVHPBKKs\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 3: Play games using a value function')\n\ndisplay(out)","f8b94538":"model_save_name = 'ValueNetwork.pth.tar'\npath = \"nma_rl_games\/alpha-zero\/pretrained_models\/models\/\"\nset_seed(seed=SEED)\ngame = OthelloGame(6)\nvnet = ValueNetwork(game)\nvnet.load_checkpoint(folder=path, filename=model_save_name)","b6829be0":"class ValueBasedPlayer():\n  def __init__(self, game, vnet):\n    self.game = game\n    self.vnet = vnet\n\n  def play(self, board):\n    valids = self.game.getValidMoves(board, 1)\n    candidates = []\n    max_num_actions = 4\n    va = np.where(valids)[0]\n    va_list = va.tolist()\n    random.shuffle(va_list)\n    #################################################\n    ## TODO for students: In the first part, please return the next board state using getNextState(), then predict\n    ## the value of next state using value network, and finally add the value and action as a tuple to the candidate list.\n    ## Note that you need to reverse the sign of the value. In zero-sum games the players flip every turn. In detail, we train\n    ## a value function to think about the game from one player's (either black or white) perspective. In order to use the same\n    ## value function to estimate how good the position is for the other player, we need to take the negative of the output of\n    ## the function. E.g., if the value function is trained for white's perspective and says that white is likely to win the game\n    ## from the current state with an output of 0.75, this similarly means that it would suggest that black is very unlikely (-0.75)\n    ## to win the game from the current state.##\n    # Fill out function and remove\n    # raise NotImplementedError(\"Implement the value-based player\")\n    #################################################\n    for action in va_list:\n      nextBoard, _ = self.game.getNextState(board =board, player=1, action=action ) # get nextBoard\n      value = self.vnet.predict(nextBoard)\n      candidates += [(-value, action)] # this is a list \n    \n\n      if len(candidates) == max_num_actions:\n        break\n\n    candidates.sort()\n\n    return candidates[0][1]\n\n\n# add event to airtable\natform.add_event('Coding Exercise 3: Value-based player')\n\n# playing games between a value-based player and a random player\nset_seed(seed=SEED)\nnum_games = 20\nplayer1 = ValueBasedPlayer(game, vnet).play\nplayer2 = RandomPlayer(game).play\narena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n## Uncomment the code below to check your code!\nresult = arena.playGames(num_games, verbose=False)\nprint(f\"\\n\\n{result}\")","03d5117a":"print(f\"Number of games won by player1 = {result[0]}, \"\n      f\"Number of games won by player2 = {result[1]}, out of {num_games} games\")\nwin_rate_player1 = result[0]\/num_games # result[0] is the number of times that player 1 wins\nprint(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","3ccb41ee":"# @title Video 4: Train a policy network\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1hQ4y127GJ\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"vj9gKNJ19D8\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 4: Train a policy network')\n\ndisplay(out)","94ea43f1":"class PolicyNetwork(NeuralNet):\n  def __init__(self, game):\n    self.nnet = OthelloNNet(game, args)\n    self.board_x, self.board_y = game.getBoardSize()\n    self.action_size = game.getActionSize()\n\n    self.nnet.to(args.device)\n\n  def train(self, games):\n    \"\"\"\n    examples: list of examples, each example is of form (board, pi, v)\n    \"\"\"\n    optimizer = optim.Adam(self.nnet.parameters())\n\n    for examples in games:\n      for epoch in range(args.epochs):\n        print('EPOCH ::: ' + str(epoch + 1))\n        self.nnet.train()\n        pi_losses = []\n\n        batch_count = int(len(examples) \/ args.batch_size)\n\n        t = tqdm(range(batch_count), desc='Training Policy Network')\n        for _ in t:\n          sample_ids = np.random.randint(len(examples), size=args.batch_size)\n          boards, pis, _ = list(zip(*[examples[i] for i in sample_ids]))\n          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n          target_pis = torch.FloatTensor(np.array(pis))\n\n          # predict\n          boards, target_pis = boards.contiguous().to(args.device), target_pis.contiguous().to(args.device)\n\n          #################################################\n          ## TODO for students: ##\n          ## 1. Compute the policy (pi) predicted by OthelloNNet() ##\n          ## 2. Implement the loss_pi() function below and then use it to update the policy loss. ##\n          # Fill out function and remove\n          # raise NotImplementedError(\"Compute the output\")\n          #################################################\n          # compute output\n          out_pi, _ = self.nnet.forward(s= boards)\n          l_pi = self.loss_pi(targets = target_pis, outputs = out_pi)\n\n          # record loss\n          pi_losses.append(l_pi.item())\n          t.set_postfix(Loss_pi=l_pi.item())\n\n          # compute gradient and do SGD step\n          optimizer.zero_grad()\n          l_pi.backward()\n          optimizer.step()\n\n  def predict(self, board):\n    \"\"\"\n    board: np array with board\n    \"\"\"\n    # timing\n    start = time.time()\n\n    # preparing input\n    board = torch.FloatTensor(board.astype(np.float64))\n    board = board.contiguous().to(args.device)\n    board = board.view(1, self.board_x, self.board_y)\n    self.nnet.eval()\n    with torch.no_grad():\n      pi,_ = self.nnet(board)\n    return torch.exp(pi).data.cpu().numpy()[0]\n\n  def loss_pi(self, targets, outputs):\n    #################################################\n    ## TODO for students: To implement the loss function, please compute and return the negative log likelihood of targets.\n    ## For more information, here is a reference that connects the expression to the neg-log-prob: https:\/\/gombru.github.io\/2018\/05\/23\/cross_entropy_loss\/\n    # Fill out function and remove\n    # raise NotImplementedError(\"Compute the loss\")\n    #################################################\n    return torch.nn.F.nll_loss(targets, outputs)\n\n  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n    filepath = os.path.join(folder, filename)\n    if not os.path.exists(folder):\n      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n      os.mkdir(folder)\n    else:\n      print(\"Checkpoint Directory exists! \")\n    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n    print(\"Model saved! \")\n\n  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n    # https:\/\/github.com\/pytorch\/examples\/blob\/master\/imagenet\/main.py#L98\n    filepath = os.path.join(folder, filename)\n    if not os.path.exists(filepath):\n      raise (\"No model in path {}\".format(filepath))\n\n    checkpoint = torch.load(filepath, map_location=args.device)\n    self.nnet.load_state_dict(checkpoint['state_dict'])\n\n\n# add event to airtable\natform.add_event('Coding Exercise 4: Implement PolicyNetwork')","2bb00ba3":"if not os.listdir('nma_rl_games\/alpha-zero\/pretrained_models\/models\/'):\n  set_seed(seed=SEED)\n  game = OthelloGame(6)\n  pnet = PolicyNetwork(game)\n  pnet.train(loaded_games)","1f91f20c":"# @title Video 5: Play games using a policy network\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1aq4y1S7o4\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"yHtVqT2Nstk\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 5: Play games using a policy network')\n\ndisplay(out)","31f99d64":"model_save_name = 'PolicyNetwork.pth.tar'\npath = \"nma_rl_games\/alpha-zero\/pretrained_models\/models\/\"\nset_seed(seed=SEED)\ngame = OthelloGame(6)\npnet = PolicyNetwork(game)\npnet.load_checkpoint(folder=path, filename=model_save_name)","3b565ed2":"class PolicyBasedPlayer():\n  def __init__(self, game, pnet, greedy=True):\n    self.game = game\n    self.pnet = pnet\n    self.greedy = greedy\n\n  def play(self, board):\n    valids = self.game.getValidMoves(board, 1) # index\n    #################################################\n    ## TODO for students:  ##\n    ## 1. Compute the action probabilities using policy network pnet()\n    ## 2. Mask invalid moves using valids variable and the action probabilites computed above.\n    ## 3. Compute the sum over valid actions and store them in sum_vap.\n    # Fill out function and remove\n    # raise NotImplementedError(\"Define the play\")\n    #################################################\n    action_probs = self.pnet.predict(board) # actions\n    # vap = np.ma.masked_array(action_probs, mask=np.logical_not(valids))\n    vap = action_probs*valids # masking valid moves\n    sum_vap = np.sum(vap)\n\n    if sum_vap > 0:\n      vap \/= sum_vap  # renormalize\n    else:\n      # if all valid moves were masked we make all valid moves equally probable\n      print(\"All valid moves were masked, doing a workaround.\")\n      vap = vap + valids\n      vap \/= np.sum(vap)\n\n    if self.greedy:\n      # greedy policy player\n      a = np.where(vap == np.max(vap))[0][0]\n    else:\n      # sample-based policy player\n      a = np.random.choice(self.game.getActionSize(), p=vap)\n\n    return a\n\n\n# add event to airtable\natform.add_event('Coding Exercise 5: Implement the PolicyBasedPlayer')\n\n# playing games\nset_seed(seed=SEED)\nnum_games = 20\nplayer1 = PolicyBasedPlayer(game, pnet, greedy=True).play\nplayer2 = RandomPlayer(game).play\narena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n## Uncomment below to test!\nresult = arena.playGames(num_games, verbose=False)\nprint(f\"\\n\\n{result}\")\nwin_rate_player1 = result[0] \/ num_games\nprint(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","4f3df773":"set_seed(seed=SEED)\nnum_games = 20\ngame = OthelloGame(6)\nplayer1 = PolicyBasedPlayer(game, pnet, greedy=False).play\nplayer2 = RandomPlayer(game).play\narena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\nresult = arena.playGames(num_games, verbose=False)\nprint(f\"\\n\\n{result}\")","9c0c834f":"win_rate_player1 = result[0]\/num_games\nprint(f\"Win rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","3ccd4d58":"set_seed(seed=SEED)\nnum_games = 20\ngame = OthelloGame(6)\nplayer1 = PolicyBasedPlayer(game, pnet).play\nplayer2 = ValueBasedPlayer(game, vnet).play\narena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\nresult = arena.playGames(num_games, verbose=False)\nprint(f\"\\n\\n{result}\")","9d3d84bb":"win_rate_player1 = result[0]\/num_games\nprint(f\"Win rate for player 1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","8edc1ea3":"set_seed(seed=SEED)\nnum_games = 20\ngame = OthelloGame(6)\nplayer1 = PolicyBasedPlayer(game, pnet).play # greedy player\nplayer2 = PolicyBasedPlayer(game, pnet, greedy=False).play # sample-based player\narena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\nresult = arena.playGames(num_games, verbose=False)\nprint(f\"\\n\\n{result}\")","7c44425a":"win_rate_player1 = result[0]\/num_games\nprint(f\"Win rate for player 1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","63197cd9":"# @title Video 6: Play using Monte-Carlo rollouts\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Rb4y1U7BW\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"DtCWDIlSo18\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 6: Play using Monte-Carlo rollouts')\n\ndisplay(out)","fae1f1e4":"class MonteCarlo():\n  def __init__(self, game, nnet, args):\n    self.game = game\n    self.nnet = nnet\n    self.args = args\n\n    self.Ps = {}  # stores initial policy (returned by neural net)\n    self.Es = {}  # stores game.getGameEnded ended for board s\n\n  # call this rollout\n  def simulate(self, canonicalBoard):\n    \"\"\"\n    This function performs one monte carlo rollout\n    \"\"\"\n\n    s = self.game.stringRepresentation(canonicalBoard)\n    init_start_state = s\n    temp_v = 0\n    isfirstAction = None\n\n    for i in range(self.args.maxDepth): # maxDepth\n\n      if s not in self.Es:\n        self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n      if self.Es[s] != 0:\n        # terminal state\n        temp_v= -self.Es[s]\n        break\n\n      self.Ps[s], v = self.nnet.predict(canonicalBoard)\n      valids = self.game.getValidMoves(canonicalBoard, 1)\n      self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n      sum_Ps_s = np.sum(self.Ps[s])\n\n      if sum_Ps_s > 0:\n        self.Ps[s] \/= sum_Ps_s  # renormalize\n      else:\n        # if all valid moves were masked make all valid moves equally probable\n        # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n        # If you have got dozens or hundreds of these messages you should pay attention to your NNet and\/or training process.\n        log.error(\"All valid moves were masked, doing a workaround.\")\n        self.Ps[s] = self.Ps[s] + valids\n        self.Ps[s] \/= np.sum(self.Ps[s])\n\n      #################################################\n      ## TODO for students: Take a random action.\n      ## 1. Take the random action.\n      ## 2. Find the next state and the next player from the environment.\n      ## 3. Get the canonical form of the next state.\n      # Fill out function and remove\n      # raise NotImplementedError(\"Take the action, find the next state\")\n      #################################################\n      a = np.random.choice(self.game.getActionSize(), p = self.Ps[s])\n      next_s, next_player = self.game.getNextState(board = canonicalBoard, player =1, action = a)\n      next_s = self.game.getCanonicalForm(board = next_s, player = next_player)\n\n      s = self.game.stringRepresentation(next_s)\n      temp_v = v\n\n    return temp_v\n\n\n# add event to airtable\natform.add_event('Coding Exercise 6: MonteCarlo')","4c43b1f8":"# @title Video 7: Play with planning\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1bh411B7S4\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"plmFzAy3H5s\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 7: Play with planning')\n\ndisplay(out)","79733167":"# Load MC model from the repository\nmc_model_save_name = 'MC.pth.tar'\npath = \"nma_rl_games\/alpha-zero\/pretrained_models\/models\/\"","e6946b58":"class MonteCarloBasedPlayer():\n  def __init__(self, game, nnet, args):\n    self.game = game\n    self.nnet = nnet\n    self.args = args\n    #################################################\n    ## TODO for students: Instantiate the Monte Carlo class.\n    # Fill out function and remove\n    # raise NotImplementedError(\"Use Monte Carlo!\")\n    #################################################\n    self.mc = MonteCarlo(game = game, nnet = nnet, args = args)\n    self.K = self.args.mc_topk\n\n  def play(self, canonicalBoard):\n    self.qsa = []\n    s = self.game.stringRepresentation(canonicalBoard)\n    Ps, v = self.nnet.predict(canonicalBoard)\n    valids = self.game.getValidMoves(canonicalBoard, 1)\n    Ps = Ps * valids  # masking invalid moves\n    sum_Ps_s = np.sum(Ps)\n\n    if sum_Ps_s > 0:\n      Ps \/= sum_Ps_s  # renormalize\n    else:\n      # if all valid moves were masked make all valid moves equally probable\n      # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n      # If you have got dozens or hundreds of these messages you should pay attention to your NNet and\/or training process.\n      log = logging.getLogger(__name__)\n      log.error(\"All valid moves were masked, doing a workaround.\")\n      Ps = Ps + valids\n      Ps \/= np.sum(Ps)\n\n    num_valid_actions = np.shape(np.nonzero(Ps))[1]\n\n    if num_valid_actions < self.K:\n      top_k_actions = np.argpartition(Ps,-num_valid_actions)[-num_valid_actions:]\n    else:\n      top_k_actions = np.argpartition(Ps,-self.K)[-self.K:]  # to get actions that belongs to top k prob\n    #################################################\n    ## TODO for students:\n    ## 1. For each action in the top-k actions\n    ## 2. Get the next state using getNextState() function. You can find the implementation of this function in Section 1 in OthelloGame() class.\n    ## 3. Get the canonical form of the getNextState().\n    # Fill out function and remove\n    # raise NotImplementedError(\"Loop for the top actions\")\n    #################################################\n    for action in top_k_actions:\n      next_s, next_player = self.game.getNextState(board=canonicalBoard, player=1, action=action)\n      next_s = self.game.getCanonicalForm(board = next_s, player = next_player)\n\n      values = []\n\n      # do some rollouts\n      for rollout in range(self.args.numMCsims):\n        value = self.mc.simulate(canonicalBoard)\n        values.append(value)\n\n      # average out values\n      avg_value = np.mean(values)\n      self.qsa.append((avg_value, action))\n\n    self.qsa.sort(key=lambda a: a[0])\n    self.qsa.reverse()\n    best_action = self.qsa[0][1]\n    return best_action\n\n  def getActionProb(self, canonicalBoard, temp=1):\n    if self.game.getGameEnded(canonicalBoard, 1) != 0:\n      return np.zeros((self.game.getActionSize()))\n\n    else:\n      action_probs = np.zeros((self.game.getActionSize()))\n      best_action = self.play(canonicalBoard)\n      action_probs[best_action] = 1\n\n    return action_probs\n\n\n# add event to airtable\natform.add_event('Coding Exercise 7: MonteCarlo siumulations')\n\nset_seed(seed=SEED)\ngame = OthelloGame(6)\n# Run the resulting player versus the random player\nrp = RandomPlayer(game).play\nnum_games = 20  # Feel free to change this number\n\nn1 = NNet(game)  # nNet players\nn1.load_checkpoint(folder=path, filename=mc_model_save_name)\nargs1 = dotdict({'numMCsims': 10, 'maxRollouts':5, 'maxDepth':5, 'mc_topk': 3})\n\n## Uncomment below to check Monte Carlo agent!\nprint('\\n******MC player versus random player******')\nmc1 = MonteCarloBasedPlayer(game, n1, args1)\nn1p = lambda x: np.argmax(mc1.getActionProb(x))\narena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\nMC_result = arena.playGames(num_games, verbose=False)\nprint(f\"\\n\\n{MC_result}\")\nprint(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n       f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\nwin_rate_player1 = MC_result[0]\/num_games\nprint(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","1094d4e8":"print('\\n******MC player versus value-based player******')\nset_seed(seed=SEED)\nvp = ValueBasedPlayer(game, vnet).play # value-based player\narena = Arena.Arena(n1p, vp, game, display=OthelloGame.display)\nMC_result = arena.playGames(num_games, verbose=False)\nprint(f\"\\n\\n{MC_result}\")\nprint(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\nwin_rate_player1 = MC_result[0]\/num_games\nprint(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","52621627":"print('\\n******MC player versus policy-based player******')\nset_seed(seed=SEED)\npp = PolicyBasedPlayer(game, pnet).play # policy player\narena = Arena.Arena(n1p, pp, game, display=OthelloGame.display)\nMC_result = arena.playGames(num_games, verbose=False)\nprint(f\"\\n\\n{MC_result}\")\nprint(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\nwin_rate_player1 = MC_result[0]\/num_games\nprint(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","3322fc7e":"# @title Video 8: Unstoppable opponents\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1WA411w7mw\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"q7181lvoNpM\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 8: Unstoppable opponents')\n\ndisplay(out)","8d8736f9":"# @title Video 9: Outro\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1a64y1s7Sh\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"uQ26iIUzmtw\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 9: Outro')\n\ndisplay(out)","2cfb5d57":"# @title Airtable Submission Link\nfrom IPython import display as IPydisplay\nIPydisplay.HTML(\n   f\"\"\"\n <div>\n   <a href= \"{atform.url()}\" target=\"_blank\">\n   <img src=\"https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/static\/SurveyButton.png?raw=1\"\n alt=\"button link end of day Survey\" style=\"width:410px\"><\/a>\n   <\/div>\"\"\" )","c980dc43":"# @title Video 10: Plan with MCTS\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1yQ4y127Sr\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"Hhw6Ed0Zmco\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 10: Plan with MCTS')\n\ndisplay(out)","0f571434":"class MCTS():\n  \"\"\"\n  This class handles the MCTS tree.\n  \"\"\"\n\n  def __init__(self, game, nnet, args):\n    self.game = game\n    self.nnet = nnet\n    self.args = args\n    self.Qsa = {}    # stores Q values for s,a (as defined in the paper)\n    self.Nsa = {}    # stores #times edge s,a was visited\n    self.Ns = {}     # stores #times board s was visited\n    self.Ps = {}     # stores initial policy (returned by neural net)\n    self.Es = {}     # stores game.getGameEnded ended for board s\n    self.Vs = {}     # stores game.getValidMoves for board s\n\n  def search(self, canonicalBoard):\n    \"\"\"\n    This function performs one iteration of MCTS. It is recursively called\n    till a leaf node is found. The action chosen at each node is one that\n    has the maximum upper confidence bound as in the paper.\n\n    Once a leaf node is found, the neural network is called to return an\n    initial policy P and a value v for the state. This value is propagated\n    up the search path. In case the leaf node is a terminal state, the\n    outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n    updated.\n\n    NOTE: the return values are the negative of the value of the current\n    state. This is done since v is in [-1,1] and if v is the value of a\n    state for the current player, then its value is -v for the other player.\n\n    Returns:\n        v: the negative of the value of the current canonicalBoard\n    \"\"\"\n    s = self.game.stringRepresentation(canonicalBoard)\n\n    if s not in self.Es:\n      self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n    if self.Es[s] != 0:\n      # terminal node\n      return -self.Es[s]\n\n    if s not in self.Ps:\n      # leaf node\n      self.Ps[s], v = self.nnet.predict(canonicalBoard)\n      valids = self.game.getValidMoves(canonicalBoard, 1)\n      self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n      sum_Ps_s = np.sum(self.Ps[s])\n      if sum_Ps_s > 0:\n        self.Ps[s] \/= sum_Ps_s  # renormalize\n      else:\n        # if all valid moves were masked make all valid moves equally probable\n        # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n        # If you have got dozens or hundreds of these messages you should pay attention to your NNet and\/or training process.\n        log = logging.getLogger(__name__)\n        log.error(\"All valid moves were masked, doing a workaround.\")\n        self.Ps[s] = self.Ps[s] + valids\n        self.Ps[s] \/= np.sum(self.Ps[s])\n\n      self.Vs[s] = valids\n      self.Ns[s] = 0\n\n      return -v\n\n    valids = self.Vs[s]\n    cur_best = -float('inf')\n    best_act = -1\n\n    #################################################\n    ## TODO for students:\n    ## Implement the highest upper confidence bound depending whether we observed the state-action pair which is stored in self.Qsa[(s, a)]. You can find the formula in the slide 52 in video 8 above.\n    # Fill out function and remove\n    raise NotImplementedError(\"Complete the for loop\")\n    #################################################\n    # pick the action with the highest upper confidence bound\n    for a in range(self.game.getActionSize()):\n      if valids[a]:\n        if (s, a) in self.Qsa:\n          u = ... + ... * ... * math.sqrt(...) \/ (1 + ...)\n        else:\n          u = ... * ... * math.sqrt(... + 1e-8)\n\n        if u > cur_best:\n          cur_best = u\n          best_act = a\n\n    a = best_act\n    next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n    next_s = self.game.getCanonicalForm(next_s, next_player)\n\n    v = self.search(next_s)\n\n    if (s, a) in self.Qsa:\n      self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) \/ (self.Nsa[(s, a)] + 1)\n      self.Nsa[(s, a)] += 1\n\n    else:\n      self.Qsa[(s, a)] = v\n      self.Nsa[(s, a)] = 1\n\n    self.Ns[s] += 1\n    return -v\n\n  def getNsa(self):\n    return self.Nsa","3d9f0460":"# @title Video 11: Play with MCTS\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV13q4y1H7H6\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"1BRXb-igKAU\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 11: Play with MCTS')\n\ndisplay(out)","51313672":"# Load MCTS model from the repository\nmcts_model_save_name = 'MCTS.pth.tar'\npath = \"nma_rl_games\/alpha-zero\/pretrained_models\/models\/\"","fcd21d32":"class MonteCarloTreeSearchBasedPlayer():\n  def __init__(self, game, nnet, args):\n    self.game = game\n    self.nnet = nnet\n    self.args = args\n    self.mcts = MCTS(game, nnet, args)\n\n\n  def play(self, canonicalBoard, temp=1):\n    for i in range(self.args.numMCTSSims):\n\n      #################################################\n      ## TODO for students:\n      #  Run MCTS search function.\n      #  Fill out function and remove\n      raise NotImplementedError(\"Plug the planner\")\n      #################################################\n      ...\n\n    s = self.game.stringRepresentation(canonicalBoard)\n    #################################################\n    ## TODO for students:\n    #  Call the Nsa function from MCTS class and store it in the self.Nsa\n    #  Fill out function and remove\n    raise NotImplementedError(\"Compute Nsa (number of times edge s,a was visited)\")\n    #################################################\n    self.Nsa = ...\n    self.counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n\n    if temp == 0:\n      bestAs = np.array(np.argwhere(self.counts == np.max(self.counts))).flatten()\n      bestA = np.random.choice(bestAs)\n      probs = [0] * len(self.counts)\n      probs[bestA] = 1\n      return probs\n\n    self.counts = [x ** (1. \/ temp) for x in self.counts]\n    self.counts_sum = float(sum(self.counts))\n    probs = [x \/ self.counts_sum for x in self.counts]\n    return np.argmax(probs)\n\n  def getActionProb(self, canonicalBoard, temp=1):\n    action_probs = np.zeros((self.game.getActionSize()))\n    best_action = self.play(canonicalBoard)\n    action_probs[best_action] = 1\n\n    return action_probs\n\n\nset_seed(seed=SEED)\ngame = OthelloGame(6)\nrp = RandomPlayer(game).play  # all players\nnum_games = 20  # games\nn1 = NNet(game)  # nnet players\nn1.load_checkpoint(folder=path, filename=mcts_model_save_name)\nargs1 = dotdict({'numMCTSSims': 50, 'cpuct':1.0})\n\n## Uncomment below to check your agent!\n# print('\\n******MCTS player versus random player******')\n# mcts1 = MonteCarloTreeSearchBasedPlayer(game, n1, args1)\n# n1p = lambda x: np.argmax(mcts1.getActionProb(x, temp=0))\n# arena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\n# MCTS_result = arena.playGames(num_games, verbose=False)\n# print(f\"\\n\\n{MCTS_result}\")\n# print(f\"\\nNumber of games won by player1 = {MCTS_result[0]}, \"\n#       f\"number of games won by player2 = {MCTS_result[1]}, out of {num_games} games\")\n# win_rate_player1 = MCTS_result[0]\/num_games\n# print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","2f7a51e5":"print('\\n******MCTS player versus value-based player******')\nset_seed(seed=SEED)\nvp = ValueBasedPlayer(game, vnet).play  # value-based player\narena = Arena.Arena(n1p, vp, game, display=OthelloGame.display)\nMC_result = arena.playGames(num_games, verbose=False)\nprint(f\"\\n\\n{MC_result}\")\nprint(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\nwin_rate_player1 = MC_result[0]\/num_games\nprint(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","dce038b9":"print('\\n******MCTS player versus policy-based player******')\nset_seed(seed=SEED)\npp = PolicyBasedPlayer(game, pnet).play  # policy-based player\narena = Arena.Arena(n1p, pp, game, display=OthelloGame.display)\nMC_result = arena.playGames(num_games, verbose=False)\nprint(f\"\\n\\n{MC_result}\")\nprint(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\nwin_rate_player1 = MC_result[0]\/num_games\nprint(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")","efe789b8":"### Compare greedy policy based player versus sample-based policy player ","bc2f657c":"## Section 1.4. Compute win rate for the random player (player 1)","be9ff987":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W3D3_ReinforcementLearningForGames\/solutions\/W3D3_Tutorial1_Solution_ef26beca.py)\n\n","8ae85291":"**Goal:** Teach the students how to use the results of an MCTS to play games.\n\n**Exercise:** \n* Plug the MCTS planner into an agent.\n* Play games against other agents.\n* Explore the contributions of prior network, value function, number of simulations \/ time to play, and explore\/exploit parameters.","9406681e":"### Monte-Carlo player against Value-based player","d98f7e98":"The hyperparameters used throughout the notebook.","dfec52bc":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W3D3_ReinforcementLearningForGames\/solutions\/W3D3_Tutorial1_Solution_e774cb2e.py)\n\n","58a96c17":"**Goal:** Teach students to understand the core ideas behind Monte Carlo Tree Search (MCTS).","34876871":"```\nNumber of games won by player1 = 10, number of games won by player2 = 10, out of 20 games\n\nWin rate for player1 over 20 games: 50.0%\n```","00f5f2a7":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W3D3_ReinforcementLearningForGames\/solutions\/W3D3_Tutorial1_Solution_fe4292a6.py)\n\n","115b8031":"```\nNumber of games won by player1 = 11, number of games won by player2 = 9, out of 20 games\n\nWin rate for player1 over 20 games: 55.0%\n```","bcd95a6b":"## Coding Exercise 6: `MonteCarlo`","397cd34b":"**Goal:** Teach students how to use simple Monte Carlo planning to play games.","e0e7df5b":"# Tutorial 1: Learn to play games with RL\n\n**Week 3, Day 3: Reinforcement Learning for Games**\n\n**By Neuromatch Academy**\n\n__Content creators:__ Mandana Samiei, Raymond Chua, Tim Lilicrap, Blake Richards\n\n__Content reviewers:__ Arush Tagade, Lily Cheng, Melvin Selim Atay, Kelson Shilling-Scrivo\n\n__Content editors:__ Melvin Selim Atay, Spiros Chavlis\n\n__Production editors:__ Namrata Bafna, Spiros Chavlis","35f63267":"### Coding Exercise 1.1: Implement a random player","2b77264b":"---\n# Section 3: Use a trained value network to play games\n\n*Time estimate: ~25mins*\n","b7f09e39":"**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n\n<p align='center'><img src='https:\/\/github.com\/NeuromatchAcademy\/widgets\/blob\/master\/sponsors.png?raw=True'\/><\/p>","30547dbd":"---\n# Bonus 1: Plan using Monte Carlo Tree Search (MCTS)\n\n*Time estimate: ~30mins","ada95033":"```\nNumber of games won by player1 = 14, number of games won by player2 = 6, out of 20 games\n\nWin rate for player1 over 20 games: 70.0%\n```","308bb59b":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W3D3_ReinforcementLearningForGames\/solutions\/W3D3_Tutorial1_Solution_7474bcfc.py)\n\n","d08002b0":"## Bonus Coding Exercise 2: Agent that uses an MCTS planner\n\n* Plug the MCTS planner into an agent.\n* Play games against other agents.\n* Explore the contributions of prior network, value function, number of simulations \/ time to play, and explore\/exploit parameters.","faae0fbf":"---\n# Bonus 2: Use MCTS to play games\n\n*Time estimate: ~10mins*\n","4ba01e23":"**Goal**: How to train a policy network via supervised learning \/ behavioural cloning.\n\n**Exercise**:\n* Train a network to predict the next move in an expert dataset by maximizing the log likelihood of the next action.","bf955df0":"---\n# Section 4: Train a policy network from expert game data\n\n*Time estimate: ~20mins*","1e9546b6":"## Section 2.4. Train the value network and observe the MSE loss progress\n\n**Important:** Only run this cell if you do not have access to the pretrained models in the `rl_for_games` repositry.","a54b2631":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W3D3_ReinforcementLearningForGames\/solutions\/W3D3_Tutorial1_Solution_5f1dc256.py)\n\n","ee52e63d":"### MCTS player against Value-based player","c6ce2826":"## Coding Exercise 4: Implement `PolicyNetwork`","0d9ffc68":"## Section 2.1. Load expert data","4ff08fa0":"**Result of pitting a value-based player against a random player**","27207486":"```\nWin rate for player 1 over 20 games: 55.0%\n```","9c0716d1":"### Compare greedy policy based player versus value based player ","9723ad8c":"```\n(11, 9, 0)\n```","cc3d6188":"## Section 2.3. Define the Value network\n During the training the ground truth will be uploaded from the **MCTS simulations** available at 'checkpoint_x.path.tar.examples'.","3e6128ba":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W3D3_ReinforcementLearningForGames\/solutions\/W3D3_Tutorial1_Solution_45ffeae9.py)\n\n","43a1c926":"---\n# Tutorial Objectives\n\nIn this tutotial, you will learn how to implement a game loop and improve the performance of a random player. \n\nThe specific objectives for this tutorial:\n*   Understand the format of two-players games\n*   Learn about value network and policy network\n\nIn the Bonus sections you will learn about Monte Carlo Tree Search (MCTS) and compare its performance to policy-based and value-based players.","64725a55":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W3D3_ReinforcementLearningForGames\/solutions\/W3D3_Tutorial1_Solution_80778c6b.py)\n\n","fbc6f920":"## Coding Exercise 5: Implement the `PolicyBasedPlayer`","c3057812":"```\nNumber of games won by player1 = 20, number of games won by player2 = 0, out of 20 games\n\nWin rate for player1 over 20 games: 100.0%\n```","c8725396":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W3D3_ReinforcementLearningForGames\/solutions\/W3D3_Tutorial1_Solution_83be26c4.py)\n\n","0d050ce0":"---\n# Section 6: Plan using Monte Carlo rollouts\n\n*Time estimate: ~15mins*\n\n","917fd291":"**Goal:** Learn how to train a value function from a dataset of games played by an expert.\n\n**Exercise:** \n\n* Load a dataset of expert generated games.\n* Train a network to minimize MSE for win\/loss predictions given board states sampled throughout the game. This will be done on a very small number of games. We will provide a network trained on a larger dataset.","2d8796f3":"### Coding Exercise 2.3: Implement the `ValueNetwork`","e8b4789a":"## Section 1.3. Create two random agents to play against each other","fe09aad7":"```\nNumber of games won by player1 = 14, Number of games won by player2 = 6, out of 20 games\n\nWin rate for player1 over 20 games: 70.0%\n```","f6bc3fe9":"## Coding Exercise 3: Value-based player","d390e061":"**Goal**: Teach the students the core idea behind using simulated rollouts to understand the future and value actions.\n\n**Exercise**:\n* Build a loop to run Monte Carlo simulations using the policy network.\n* Use this to obtain better estimates of the value of moves.","bcd4afc3":"---\n# Setup","d2c1f634":"---\n# Section 0: Introduction","df97800e":"```\n Win rate for player 1 over 20 games: 50.0%\n```","86a5a81f":"## Section 1.1: Create a random player","d3c1fad9":"<a href=\"https:\/\/colab.research.google.com\/github\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/W3D3_ReinforcementLearningForGames\/student\/W3D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a> &nbsp; <a href=\"https:\/\/kaggle.com\/kernels\/welcome?src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W3D3_ReinforcementLearningForGames\/student\/W3D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https:\/\/kaggle.com\/static\/images\/open-in-kaggle.svg\" alt=\"Open in Kaggle\"\/><\/a>","c106ddde":"```\n(14, 6, 0)\n```","7b373430":"### Monte-Carlo player against Policy-based player","d1f8966a":"## Coding Exercise 7: Monte-Carlo simulations\n\n* Incorporate Monte Carlo simulations into an agent.\n* Run the resulting player versus the random, value-based, and policy-based players.","4a0121d9":"```\nWin rate for player1 over 20 games: 95.0%\n```","98b11832":"```\nNumber of games won by player1 = 11, Number of games won by player2 = 9 out of 20 games\n\nWin rate for player1 over 20 games: 55.0%\n```","6b4042a8":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W3D3_ReinforcementLearningForGames\/solutions\/W3D3_Tutorial1_Solution_7b72acfd.py)\n\n","4598d035":"\n***Goal***: How to setup a game environment with multiple players for reinforcement learning experiments.\n\n***Exercise***: \n\n\n*   Build an agent that plays random moves\n*   Connect with connect 4 game\n*   Generate games including wins and losses","0fd2ffe8":"**Goal**: Learn how to use a value function in order to make a player that works better than a random player.\n\n**Exercise:**\n* Sample random valid moves and use the value function to rank them\n* Choose the best move as the action and play it\nShow that doing so beats the random player\n\n**Hint:** You might need to change the sign of the value based on the player","90284a52":"## Section 2.2. Define the Neural Network Architecture for Othello\n","5816b870":"---\n# Section 2: Train a value function from expert game data\n\n*Time estimate: ~25mins*\n\n","21e32513":"---\n# Summary\n\nIn this tutotial, you have learned how to implement a game loop and improve the performance of a random player. More specifically, you are now able to understand the format of two-players games. We learned about value-based and policy-based players, and we compare them with the MCTS method.","11351a03":"**Goal**: How to use a policy network to play games.\n\n**Exercise:** \n* Use the policy network to give probabilities for the next move.\n* Build a player that takes the move given the maximum probability by the network.\n* Compare this to another player that samples moves according to the probability distribution output by the network.","0335f57b":"## Section 1.2. Initiate the game board\n","d1f90163":"---\n# Section 5: Use a trained policy network to play games\n\nTime estimate: ~20mins\n","8cf4dc77":"### Train the policy network\n\n**Important:** Only run this cell if you do not have access to the pretrained models in the `rl_for_games` repositry.","e8879985":"---\n# Section 8: Ethical aspects\n\n*Time estimate: ~5mins*","5bc9643f":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W3D3_ReinforcementLearningForGames\/solutions\/W3D3_Tutorial1_Solution_d8a53ceb.py)\n\n","36b3b817":"```\nNumber of games won by player1 = 19, num of games won by player2 = 1, out of 20 games\n\nWin rate for player1 over 20 games: 95.0%\n```","bb5839c8":"---\n# Section 7: Use Monte Carlo simulations to play games\n\n*Time estimate: ~20mins*","7627abab":"```\n Win rate for player1 over 20 games: 80.0%\n```","166095b1":"## Bonus Coding Exercise 1: MCTS planner\n\n* Plug together pre-built Selection, Expansion & Backpropagation code to complete an MCTS planner.\n* Deploy the MCTS planner to understand an interesting position, producing value estimates and action counts.","c07e9c39":"```\nNumber of games won by player1 = 10, number of games won by player2 = 10, out of 20 games\n\nWin rate for player1 over 20 games: 50.0%\n```","13e47e41":"### MCTS player against Policy-based player","9a8c1f02":"### Comparing a policy based player versus a random player\n\nThere's often randomness in the results as we are running the players for a low number of games (only 20 games due compute + time costs). So, when students are running the cells they might not get the expected result. To better measure the strength of players you can run more games!","5fe003ac":"### Coding Exercise 2.2: Implement the NN `OthelloNNet` for Othello","153f0d20":"---\n# Section 1: Create a game\/agent loop for RL\n\n*Time estimate: ~15mins*"}}