{"cell_type":{"9b8ba109":"code","09c052d1":"code","e6dd9a7b":"code","a8648840":"code","417abf98":"code","34d8a1a7":"code","8d9586dd":"code","0cf05d0f":"code","ee7ac89b":"code","cbb642da":"code","e4ad09a5":"code","4155dbfa":"code","496110d0":"code","b6faff9f":"code","cf480ec7":"code","74bf00c6":"code","71aada6e":"code","0b8ddfd6":"code","ef55a6dd":"code","14c98866":"code","66eb8238":"code","46bb137c":"code","0217c0bc":"code","c1df7ec1":"code","fd5e1565":"code","c263d650":"code","4efab6da":"code","61adc822":"code","ec0194b4":"code","2f77d491":"code","ad85326d":"code","a9764219":"code","9b95e2e3":"code","c6fc392d":"code","fb2ced5f":"code","397bdb17":"code","7367f731":"code","bb5a093b":"code","4e7d2266":"code","54cf655f":"code","40007a88":"code","0731d8ab":"code","3960c468":"code","9c5c0b7f":"code","93660316":"code","3442dd6b":"code","410b3e31":"code","2e8b915c":"code","7fc194e4":"code","49cce73a":"code","fbf1f6a9":"code","6f01e586":"code","9b923e00":"code","3ba1082f":"code","d75bf2cf":"code","45abee90":"code","7b2b6f45":"code","712ca6c0":"code","00b990ba":"code","0b1a6649":"code","e4320aa4":"code","0bd52eae":"code","727dfa77":"code","e9f4e9a1":"code","490c3679":"code","1f9175ad":"code","809afa1f":"code","149f7668":"code","8e0cb4f8":"code","f92e6765":"code","9f699286":"code","0324f299":"code","8929ded9":"code","4f093e33":"code","60744dc5":"code","7dc06e03":"code","3b72a918":"code","5c9e547c":"code","666006ba":"code","be48a0d2":"markdown","691a7651":"markdown","b0725fe2":"markdown","321c875a":"markdown","9aa53c16":"markdown","6643fd9f":"markdown","8498b02f":"markdown","74039e33":"markdown","0342715a":"markdown","321b2835":"markdown","caa84c0e":"markdown","b726dcef":"markdown","a636a189":"markdown","51576383":"markdown","92d7d622":"markdown","f2d00866":"markdown","56aecd70":"markdown","a96d7697":"markdown","7ce1ab6f":"markdown","7d281bb9":"markdown","02a1e637":"markdown","5f28c7ec":"markdown","830c7091":"markdown","951611db":"markdown","fef845d0":"markdown","cdd0eff6":"markdown","b9b80242":"markdown","f12b9d63":"markdown","7c424f05":"markdown","f2322410":"markdown","14b51587":"markdown","edaa9abe":"markdown","6c1922cb":"markdown","4178c57b":"markdown","1d6d4227":"markdown","d135cef4":"markdown","c85ab73d":"markdown","ae95259a":"markdown","7d2e762d":"markdown","93cf1139":"markdown","f2c18e4a":"markdown","63326a81":"markdown","2f10a480":"markdown","ad79433d":"markdown","5a60b6c0":"markdown","e9a84f7f":"markdown","7b2ea1b7":"markdown","016d7d1a":"markdown","d60f9801":"markdown","7bd35618":"markdown","ee95ee51":"markdown","c5d06db0":"markdown","8766a94a":"markdown","c2b22ae2":"markdown","08d8aef4":"markdown","e0623366":"markdown","313c6b22":"markdown","dd676a95":"markdown","5f97728c":"markdown","b313b915":"markdown","b9a6bbd5":"markdown"},"source":{"9b8ba109":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","09c052d1":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e6dd9a7b":"cancer = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","a8648840":"print(f\"Number of rows: {cancer.shape[0]}\")\nprint(f\"Number of columns: {cancer.shape[1]}\")","417abf98":"pd.set_option('display.max_columns', None)","34d8a1a7":"cancer.head()","8d9586dd":"cancer.tail()","0cf05d0f":"cancer.sample(5)","ee7ac89b":"cancer.info()","cbb642da":"cancer.drop(['id', 'Unnamed: 32'], axis = 1, inplace = True)","e4ad09a5":"cancer.duplicated().any()","4155dbfa":"plt.figure(figsize = (14, 4))\n\nax0 = plt.subplot(1, 2, 1)\nax1 = plt.subplot(1, 2, 2)\n\nproportion = cancer['diagnosis'].value_counts()\/cancer.shape[0]\nmalignant = len(cancer[cancer['diagnosis'] == 'M'])\nbenign = len(cancer[cancer['diagnosis'] == 'B'])\n\nsns.countplot(x = 'diagnosis', data = cancer, \n              palette = ['grey', 'orange'], ax = ax0)\nax0.set_title('Number of Cases by Diagnosis', size = 13)\nax0.set_xlabel(None)\nax0.text(x = 0, y = 100, s = malignant, size = 11, \n         color = 'black', horizontalalignment = 'center')\nax0.text(x = 1, y = 100, s = benign, size = 11,\n        color = 'black', horizontalalignment = 'center')\nax0.yaxis.set_visible(False)\nsns.despine(left = True)\n\nproportion.plot(kind = 'pie', autopct = '%.1f%%', \n                explode = [0, 0.05], shadow = True,\n                labels = ['B', 'M'], colors = ['orange', 'grey'], \n                ax = ax1)\nax1.set_title('Percent of Cases by Diagnosis')\nax1.yaxis.set_visible(False)\n\nplt.subplots_adjust(wspace = 0.5)\nplt.show()","496110d0":"mean = [i for i in cancer.columns if i == 'diagnosis' or i[-1] == 'n']\ncancer_mean = cancer[mean]\n\nse = [i for i in cancer.columns if i == 'diagnosis' or i[-1] == 'e']\ncancer_se = cancer[se]\n\nworst = [i for i in cancer.columns if i == 'diagnosis' or i[-1] == 't']\ncancer_worst = cancer[worst]","b6faff9f":"cancer_mean.iloc[:, 1:].describe()","cf480ec7":"len(cancer[cancer['concavity_mean'] == 0])","74bf00c6":"len(cancer[cancer['concave points_mean'] == 0])","71aada6e":"cancer[(cancer['concavity_mean'] == 0) | \n       (cancer['concave points_mean'] == 0)]","0b8ddfd6":"sns.pairplot(cancer_mean, hue = 'diagnosis', \n             palette = ({'M': 'grey', 'B': 'orange'}))","ef55a6dd":"dimensions = cancer[['diagnosis', 'radius_mean', \n                     'perimeter_mean', 'area_mean', \n                     'compactness_mean']]","14c98866":"fig = plt.figure(figsize= (20, 5))\n\nfor i, j in enumerate(dimensions.columns[1:]):\n    fig.add_subplot(1, 4, i+1)\n    sns.boxplot(x = dimensions['diagnosis'], \n                y = dimensions[j], \n                palette = ['grey', 'orange'])\n    plt.xlabel(None)\n    plt.title(f\"{j.title()} by Diagnosis\")\n    plt.ylabel(None)\n    \n\nplt.subplots_adjust(wspace = 0.3)\nplt.show()","66eb8238":"shape = cancer[['diagnosis','smoothness_mean', \n                'concavity_mean', 'concave points_mean', \n                'symmetry_mean']]","46bb137c":"fig = plt.figure(figsize= (20, 5))\n\nfor i, j in enumerate(shape.columns[1:]):\n    fig.add_subplot(1, 4, i+1)\n    sns.boxplot(x = shape['diagnosis'], \n                y = shape[j], \n                palette = ['grey', 'orange'])\n    plt.xlabel(None)\n    plt.title(f\"{j.title()} by Diagnosis\")\n    plt.ylabel(None)\n\nplt.subplots_adjust(wspace = 0.3)\nplt.show()","0217c0bc":"structure = cancer[['diagnosis', 'texture_mean', \n                    'fractal_dimension_mean']]","c1df7ec1":"fig = plt.figure(figsize= (20, 5))\n\nfor i, j in enumerate(structure.columns[1:]):\n    fig.add_subplot(1, 4, i+1)\n    sns.boxplot(x = structure['diagnosis'], \n                y = structure[j], \n                palette = ['grey', 'orange'])\n    plt.xlabel(None)\n    plt.title(f\"{j.title()} by Diagnosis\")\n    plt.ylabel(None)\n\nplt.subplots_adjust(wspace = 0.5)\nplt.show()","fd5e1565":"cancer_se.iloc[:, 1:].describe()","c263d650":"sns.pairplot(cancer_se, hue = 'diagnosis', \n             palette = ({'M': 'grey', 'B': 'orange'}))","4efab6da":"cancer_worst.iloc[:, 1:].describe()","61adc822":"sns.pairplot(cancer_worst, hue = 'diagnosis', \n             palette = ({'M': 'grey', 'B' : 'orange'}))","ec0194b4":"cancer['diagnosis'] = cancer['diagnosis'].map({'M': 1, 'B': 0})","2f77d491":"plt.figure(figsize = (18, 12))\n\nsns.heatmap(cancer.corr(), annot = True)","ad85326d":"drop_col = ['perimeter_mean', 'area_mean', 'concavity_mean', \n            'perimeter_se', 'area_se', 'perimeter_worst', \n            'area_worst', 'radius_worst', 'texture_worst', \n            'concave points_worst']","a9764219":"from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import confusion_matrix, classification_report","9b95e2e3":"X = cancer.drop('diagnosis', axis = 1)\ny = cancer['diagnosis']","c6fc392d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)","fb2ced5f":"std = StandardScaler()","397bdb17":"X_train = std.fit_transform(X_train)\nX_test = std.transform(X_test)","7367f731":"svc = SVC()","bb5a093b":"svc.fit(X_train, y_train)","4e7d2266":"y_train_predict = svc.predict(X_train)","54cf655f":"sns.heatmap(confusion_matrix(y_train, y_train_predict), \n            annot = True)\nplt.title('Confusion matrix SVM (train set)', \n          size = 13)\nplt.ylabel('Real Values', size = 11)\nplt.xlabel('Predicted Values', size = 11)\nplt.show()","40007a88":"print(classification_report(y_train, y_train_predict))","0731d8ab":"cv = RepeatedStratifiedKFold(n_splits = 5, \n                             n_repeats = 10, \n                             random_state = 10)","3960c468":"accuracies = cross_val_score(svc, X = X_train, y = y_train, \n                             scoring = 'accuracy', cv = cv, \n                             n_jobs = -1)","9c5c0b7f":"print(f\"Accuracy:\\nmean: {accuracies.mean():.3f}, std: {accuracies.std():.3f}\")","93660316":"recalls_w = cross_val_score(svc, X = X_train, y = y_train, \n                            scoring = 'recall', cv = cv, \n                            n_jobs = -1)","3442dd6b":"print(f\"Recall:\\nmean: {recalls_w.mean():.3f}, std: {recalls_w.std():.3f}\")","410b3e31":"y_predict = svc.predict(X_test)","2e8b915c":"sns.heatmap(confusion_matrix(y_test, y_predict), \n            annot = True)\nplt.title('Confusion matrix SVM (test set)', \n          size = 13)\nplt.ylabel('Real Values', size = 11)\nplt.xlabel('Predicted Values', size = 11)\nplt.show()","7fc194e4":"print(classification_report(y_test, y_predict))","49cce73a":"drop_col","fbf1f6a9":"cancer1 = cancer.drop(drop_col, axis = 1)\ncancer1.shape","6f01e586":"X1 = cancer1.drop('diagnosis', axis = 1)\ny1 = cancer1['diagnosis']","9b923e00":"X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1,\n                                                        test_size = 0.2, \n                                                        random_state = 10)","3ba1082f":"std1 = StandardScaler()","d75bf2cf":"X1_train = std1.fit_transform(X1_train)\nX1_test = std1.transform(X1_test)","45abee90":"rfecv = RFECV(LogisticRegression())\nsvc1 = SVC()","7b2b6f45":"pipeline = Pipeline(steps = [('Feature Selection', rfecv), \n                             ('Model', svc1)])","712ca6c0":"rfecv.fit(X1_train, y1_train)","00b990ba":"rfecv.n_features_","0b1a6649":"rfecv.support_","e4320aa4":"features1 = pd.DataFrame(rfecv.support_, index = X1.columns, \n                         columns = ['Features'])","0bd52eae":"features = features1[features1['Features'] == True].index\nfeatures","727dfa77":"cv = RepeatedStratifiedKFold(n_splits = 5, \n                             n_repeats = 10, \n                             random_state = 10)","e9f4e9a1":"accuracies_rfe = cross_val_score(pipeline, X = X1_train, \n                                 y = y1_train, scoring = 'accuracy', \n                                 cv = cv, n_jobs = -1)","490c3679":"print(f\"Accuracy:\\nmean: {accuracies_rfe.mean():.3f}, std: {accuracies_rfe.std():.3f}\")","1f9175ad":"recalls_rfe = cross_val_score(pipeline, X = X1_train, \n                              y = y1_train, scoring = 'recall', \n                              cv = cv, n_jobs = -1)","809afa1f":"print(f\"Recall:\\nmean: {recalls_rfe.mean():.3f}, std: {recalls_rfe.std():.3f}\")","149f7668":"X1_train_lr = pd.DataFrame(X1_train)","8e0cb4f8":"X1_train_lr = X1_train_lr.iloc[:, [0, 1, 4, 7, 10, 13, 15, 17, 18]]","f92e6765":"X1_test_lr = pd.DataFrame(X1_test)","9f699286":"X1_test_lr = X1_test_lr.iloc[:, [0, 1, 4, 7, 10, 13, 15, 17, 18]]","0324f299":"svc2 = SVC()","8929ded9":"svc2.fit(X1_train_lr, y1_train)","4f093e33":"X1_train_lr.shape\n","60744dc5":"y1_predict = svc2.predict(X1_test_lr)","7dc06e03":"sns.heatmap(confusion_matrix(y1_test, y1_predict), annot = True)\nplt.title('Confusion matrix SVM-9 (test set)', \n          size = 13)\nplt.ylabel('Real Values', size = 11)\nplt.xlabel('Predicted Values', size = 11)\nplt.show()","3b72a918":"print(classification_report(y1_test, y1_predict))","5c9e547c":"models = pd.DataFrame({'SVM': {'accuracy': accuracies.mean(), 'a_std': accuracies.std(), \n                    'recall': recalls_w.mean(), 'r_std': recalls_w.std()},\n                       \n                      'SVM_9': {'accuracy': accuracies_rfe.mean(), 'a_std': accuracies_rfe.std(),\n                      'recall': recalls_rfe.mean(), 'r_std': recalls_rfe.std()}})","666006ba":"models.T","be48a0d2":"**Change the setting to display all columns.**","691a7651":"**Split the dataset in train(80%), and test(20%) set**","b0725fe2":"Concavity mean and concave point_mean have a min value of 0. Let's explore it a bit more to make sure that is a legit value and not the result of some problem in the dataset (i.e, missing values).","321c875a":"Apart for fractal dimension, \n\n**Descriptive statistics and  pairplot graph for the standard error group of variables.**","9aa53c16":"The model with 9 variables reached an accuracy of 0.99 and a recall of 1.00 on the test set.\nOnce again the way the dataset was split might have helped the performance of the model.","6643fd9f":"The data for the cross validation are similar to the data from the train set. This means that the model can generalize well.\n\nLet's see how it performs on the test set.","8498b02f":"**Get general information about the dataset**","74039e33":"**Compare the results of cross validation of the model with all variables and the model with 9 variables.**","0342715a":"**Dataset summary**\n\n- The dataset contains 1 dependent variable and 30 independent variables:\n\n- The independent variables consist of ten chatacteristic of each nucleus of the cell and are divided in three types of measurements: mean, standard error and worst.\n\n- The dataset contains no null values and no duplicated entries.","321b2835":"**Create a new dataset without the features in drop_col.**","caa84c0e":"Both accuracy and recall of the model trained with  9 variables are into 0.01 distance from the same metrics from the model with all the variables. ","b726dcef":"**Separate the independent variable from the dependent variable**","a636a189":"## Feature selection and second model training","51576383":"## Model Train, Test and Evaluation\n\n**Steps**\n\n- Train, test and evaluate an SVM model using all the variables.\n\n- Feature selection using correlation coefficient and Recursive Feature Elimination with Cross Validation (RFECV).\n\n- Train, test and evaluate the model with the selected features.\n\n- Compare the performance of the two models.\n","92d7d622":"**Recall**","f2d00866":"In this section I will use two approaches to select the most relevant features for the model.\n\n- First I will eliminate the features with an high correlation coefficient. I have already selected the feature to eliminate and stored them in the variable drop_col.\n\n- Use Recursive Feature Elimination with Cross-Validation to select the most relevant features for the model.","56aecd70":"**Plot a pairwise scatter plot of the mean variables by diagnosis status and the distribution of each variable by diagnosis status.**","a96d7697":"Now I have a dataset with 21 variables.\n\n**Separate the independent variables from the dependent variable**","7ce1ab6f":"### Dataset description\n\nVariables Information:\n\n**1) ID number**\n**2) Diagnosis:** M = malignant; B = benign\n\n**3-32)**\n\nTen real-valued features are computed for each cell nucleus:\n\n**a) Radius:** mean of distances from center to points on the perimeter\n\n**b) Texture:** standard deviation of gray-scale values\n\n**c) Perimeter**\n\n**d) Area**\n\n**e) Smoothness:** local variation in radius lengths\n\n**f) Compactness:** perimeter^2 \/ area - 1.0\n\n**g) Concavity:** severity of concave portions of the contour\n\n**h) Concave points:** number of concave portions of the contour\n\n**i) Symmetry**\n\n**j) Fractal dimension:** \"coastline approximation\" - 1\n\nFor each variable we have three types of measurements\n\n**- Mean**\n\n**- Standard Error**\n\n**- Worst** (mean of the three largest values)","7d281bb9":"Also the variables connected with the shape of the nuclei are well se","02a1e637":"RFECV selected 9 features out of the 21 available\n\nLet's see which features contributed to the model.","5f28c7ec":"## Introduction\n\nBreast cancer is the most common cancer related cause of death among women. An early diagnosis a is a key element in determining the success of the treatment. The Wisconsin Diagnostic Breast Cancer data set was created by analysing imagines from a Fine Needle Aspirate (FNA) of breast mass and recording a series of measurements of the nuclei of the cells.\n\nThe goal of this project is to predict if a breast mass is benign or malignant using the available measurements. ","830c7091":"**Create a pipeline for feature selection and model fitting**","951611db":"**Check for duplicated values**","fef845d0":"**Scale the independent variables using Standard Scaler**","cdd0eff6":"## Exploratory Data Analysis (EDA)","b9b80242":"**Fit a new the model to the train set with 9 variables**","f12b9d63":"**Accuracy**","7c424f05":"In this notebook, I used SVM to predict the malignancy of breast masses based on measurements of the nuclei of the cells.\n\nI first trained a model using all the variables available in order to have a baseline. I then used the correlation among features and recursive feature elimination with cross validation to reduce the number of feature ending up using 9 features instead of 30.\n\nReducing the number of features reduces the complexity of the model and increases its interpretability. \n\nThank you for reading.\n\nIf you found the notebook interesting please upvote.\n\n**Feedbacks are really apprecited.**\n","f2322410":"### Data Cleaning","14b51587":"All the measure connected to the dimensions of the nuclei seem to be good predictors of malignancy. ","edaa9abe":"#### Support Vector Machine (SVM)\n\nSupport Vector Machine is an algorithm that can be used for both classification and regression. In a binary classification problem, the goal of an SVM algorithm is to find the line (in a two dimensional space) that better separate the two classes.\n\n![SVM.png](attachment:SVM.png)\n\nIf we look at the figure on the left, there are several lines that can separate the two classes. The line that better separates the two classes is the one that ensures the widest margins at the two sides of the hyperplane (right figure). The datapoints closest to the  hyperplanes in both classes are called support vectors. They help determine the margine and the hyperplane to separate the classes. ","6c1922cb":"**Recursive Feature Elimination Cross Validation (RFECV)**\n\nRecursive Feature Selection trains a model using all the features available and computes the importance of each feature in the model. The least important features are eliminated from the model and the process is repeated until it reaches the selected number of features. To implement RFE we need to select an algorithm and the number of feature we want to use.\n\nBecause I do not know what the optimal number of features might be, I will use Recursive Feature Elimination with Cross Validation. In this case, the algorithm tries different combinations of variables and than selects the combination that returns the best mean score. I will use Logistic Regression as model for feature selection.","4178c57b":"**Create an object for logistic regression and one for SVM**","1d6d4227":"**Desctiptive statistics and pairplot for the worst set of variables**","d135cef4":"**Let's use cross-validation to select the best features**","c85ab73d":"**Nuclei shape**","ae95259a":"**Recall**","7d2e762d":"**Get the number and percentage of benign and malignant samples**","93cf1139":"**Create an SVC object and train fit it to the train set**","f2c18e4a":"**Checking a few rows of the dataset**","63326a81":"The margins between the two classes are not always easily separable with a straight line as in the example above. Sometimes we can have have a situation like the one in the left figure below.\n\n![SVM2.png](attachment:SVM2.png)\n\nTo separate these classes, the algorithm uses the so called kernel trick. The algorithm adds a dimension and now the two classes can be separated by a straight line again along the new dimension.","2f10a480":"I am curious to check its performance on the test set using the 9 feature selected. I will take the train and test set I created before and keep only the 9 features selected by RFECV.","ad79433d":"**Check the structure of the dataset**","5a60b6c0":"**Apply cross validation with the 9 features**","e9a84f7f":"All the samples with concavity_mean and concave point_mean = 0 are benign sample and have the corresponding se and worst measurements = 0. Apart from sample 473, the dimension of the others are into the first quartile. The data above do not suggest errors in data entry or other kind of problems, so I will continue with the analysis.","7b2ea1b7":"**Accuracy**","016d7d1a":"**Scale the independent variables**","d60f9801":"**Import the modules for Machne Learning**","7bd35618":"**Let's see how the model performs on the test set**","ee95ee51":"The benign and malignant class seem to be well separated.\n\nMany variables are strongly correlated. This could create a multicollinearity problem.","c5d06db0":"The column Unnamed: 32 contains only missing values while the id column contains the code of the sample. We can drop both columns from the dataset.\n\nThe dependent variable diagnosis contains categorical data: benign, malignant.\nThe idependent variables contain contiuous data of float type.","8766a94a":"## Conclusion\n\n","c2b22ae2":"About 63% of the entries are benign and about 37% are malignant.\n\nI will separate the dataset in the three groups mean, se and worst and analyze them separately.","08d8aef4":"**Nuclei Dimension**","e0623366":"**Import the libraries and the dataset**","313c6b22":"**Perform cross-validation to evaluate the model**\n\nI will use RepeatedStratifiedKFold to cross validate the model on the train set.\n\nI will use 5 splits and 10 repetitions. \n\n- The dataset will be split in five parts. Four parts will be used to train the model and one part to test it. Each of the five part will in turn be used as test set, so one repetition produces five evaluation scores.\n\n- The steps above will be repeated 10 times for a total of 50 train and test\n\n- Get the average accuracy.\n\n- I am also interested in the recall so I will perform the cross validation process again using recall as scoring metric.","dd676a95":"From the table above it is clear that many variables are strongly correlated to each other, which could cause multicollinearity. \n\n**Multicollinearity**\n\nWhen two independent variables are strongly correlated to each other.\n\n**Why is multicollinearity a problem?**\n\nMulticollinearity affects the interpretability of the model. Consider the linear regression equation below.\n \ny = b0 + b1x1 + b2x2\n                                                    \nLet's assume that x1 and x2 are highly correlated, which means that we can predict the value of one variable given the value of the other one.\n\nThe coefficient b1 measures how much the dependent variable y will vary when x1 increases of 1 unit while x2 remain constant (b2 measures the effect of b2). Because x1 and x2 influence each other, varying x1 will induce a variation in x2 that will also have an effect on y. So with multicollinearity it is impossible to understand the impact of the single variables on the dependent variable.\n\nAlthough multicollinearity affects the interpretability, it does not necessarily affects the performance of a model, so deciding to reduce the multicollinearity problem it depends from our need to know which variables are actually contributing to the final result.\n\nOne of the most common solutions to multicollinearity is to keep only one of the highly correlated variables while dropping the others.\n\n\n\nIn the dataset above, radius, perimeter and area have a correlation of over 0.9, so I will keep radius and drop the others. \nI will also drop concavity_mean, texture_worst and concave points_worst.\n\nI will store the columns to drop in a variable called drop_col.","5f97728c":"### Train and test an SVM model using all the variables","b313b915":"**Change the labels B and M to 0 and 1, respectively.**","b9a6bbd5":"On the test set the model reached an accuracy of 0.97 that is similar to the accuracy obtained on cross validation.\n\nThe recall on the test set is 1.00 compared to 0.94 on cross-validation. Probably the way the dataset was split made it easy for the model to identify the positive class on the test set."}}