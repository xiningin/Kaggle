{"cell_type":{"099beee2":"code","a64aeb89":"code","da811762":"code","e0e71587":"code","c5a72b17":"code","0b1c1883":"code","25f7080e":"code","62817edf":"code","ed0cdee9":"code","5d87ba30":"code","9797b31f":"code","45fbe3a3":"code","182252fe":"code","9b0537b4":"code","369198dc":"code","c768c21d":"code","c7660233":"code","b7aefd1a":"code","20d99898":"code","77de1c1a":"code","6476955b":"code","0f074388":"code","d422ac28":"code","ee302da9":"code","39dd3a02":"code","8a67863a":"code","9a8b7d78":"code","6332d0b5":"code","cffc5dd1":"code","b410f262":"code","52ee87f1":"code","7c76d70a":"code","c2bbfe79":"code","cddcba30":"code","386bf788":"code","8fc2ffef":"code","237b8619":"code","4a85bd4b":"code","21547689":"code","7197abd4":"code","f0dbb4fe":"code","d2a0a398":"code","5e4c6ffe":"code","88913b22":"code","743c2cb1":"code","1691e134":"code","ac3190ad":"code","b53395b5":"code","d4284c2c":"code","f9d63b6a":"markdown","b20387b0":"markdown","1992a4dd":"markdown","4452ec4b":"markdown","23b67779":"markdown","596a5054":"markdown","e10495b9":"markdown","4d5e56e6":"markdown","f8779adb":"markdown","385d4223":"markdown","af5f0c2d":"markdown","3033ae8d":"markdown","1d0153f4":"markdown","b3b2cb2e":"markdown","1f658e9e":"markdown","734e45ad":"markdown","7da01da9":"markdown","40b49259":"markdown","1e52be55":"markdown","f1a28f3f":"markdown","12226ec6":"markdown","551bde77":"markdown","3015252b":"markdown","8dfa423b":"markdown","079c8bc8":"markdown","073d4f81":"markdown","7ecf45ac":"markdown","00124618":"markdown","48907f4e":"markdown","0b59a7a9":"markdown","79aee94f":"markdown","586432d8":"markdown","f57bc545":"markdown","8ab08a0d":"markdown","8ac4e86f":"markdown","d0aeee5c":"markdown","4942867d":"markdown","1d6d3723":"markdown","d8a4bb0d":"markdown","28fc948a":"markdown"},"source":{"099beee2":"# importing libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn.linear_model import LinearRegression\nfrom math import sqrt","a64aeb89":"# Load Dataset\ndf_house = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')","da811762":"# View the dataset\ndf_house.head()","e0e71587":"# Total Rows and Columns\ndf_house.shape","c5a72b17":"# Checking Datatypes\ndf_house.dtypes","0b1c1883":"df_house = df_house.drop(['id','date'],axis =1)\ndf_house.head()","25f7080e":"df_house.isnull().sum().sort_values(ascending = False)","62817edf":"min_ = min(df_house['price'])\nmax_ = max(df_house['price'])\nx = np.linspace(min_,max_,100)\nmean = np.mean(df_house['price'])\nstd = np.std(df_house['price'])\n\n# For Histogram\nplt.hist(df_house['price'], bins=20, density=True, alpha=0.3, color='b')\ny = norm.pdf(x,mean,std)\n\n# For normal curve\nplt.plot(x,y, color='red')\n\n\nplt.show()","ed0cdee9":"df_house['price'].describe()","5d87ba30":"sns.boxplot(df_house['price'])","9797b31f":"correlation_matrix = df_house.corr()\nprint(correlation_matrix)","45fbe3a3":"sns.heatmap(correlation_matrix)","182252fe":"# Price & Sqft Living\ndf_house.plot(x='sqft_living',y='price',style = 'o')\nplt.title('Sqft_Living Vs Price')","9b0537b4":"df_house.boxplot(column = ['price'],by='bedrooms')","369198dc":"df_house.plot(x='lat',y='price',style = 'o')\nplt.title('lat Vs Price')","c768c21d":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df_house, test_size=0.20)","c7660233":"train.shape","b7aefd1a":"test.shape","20d99898":"X_train_simple = train['sqft_living'].values.reshape(-1,1)\nX_test_simple = test['sqft_living'].values.reshape(-1,1)\n\ny_train_simple = train['price'].values.reshape(-1,1)\ny_test_simple = test['price'].values.reshape(-1,1)","77de1c1a":"model_s = LinearRegression()\nmodel_s.fit(X_train_simple,y_train_simple)\n\nprint('Intercept: ', model_s.intercept_)\n\nprint('Sqft_living Coefficient: ', model_s.coef_)","6476955b":"# Making Predictions\npred_simple = model_s.predict(X_test_simple)\npred_simple","0f074388":"# RSS\nRSS_simple = np.sum((y_test_simple - pred_simple)**2)\nprint(\"RSS_simple: \", RSS_simple)","d422ac28":"plt.plot(test['sqft_living'],test['price'],'.',\n        test['sqft_living'], pred_simple,'-')","ee302da9":"sns.residplot('sqft_living','price', data = test, color = 'red')","39dd3a02":"cov = pd.DataFrame.cov(df_house[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'bedrooms', \n                                 'sqft_basement','waterfront','floors']])\nprint(cov)\n\n","8a67863a":"sns.heatmap(cov,fmt='g')\nplt.show()","9a8b7d78":"# Separating Attributes and Labels\nX_train = train[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'bedrooms']].values\nX_test = test[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'bedrooms']].values\ny_train = train['price'].values.reshape(-1,1)\ny_test = test['price'].values.reshape(-1,1)","6332d0b5":"# Fitting Regression Model\nmodel1 = LinearRegression()\nmodel1.fit(X_train,y_train)\n\nprint('Intercept: ', model1.intercept_)\n\nprint('Coefficients: ', model1.coef_)\n\ndf1 = pd.DataFrame(model1.coef_, columns = ['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view',\n                                            'bedrooms'])\nprint(df1)","cffc5dd1":"# Making the predictions od model 1\npred1 = model1.predict(X_test)\npred1","b410f262":"# RSS of model 1\nRSS_1 = np.sum((y_test - pred1)**2)\nprint(\"RSS_1: \", RSS_1)","52ee87f1":"# Separating Attributes and Labels\nX_train = train[['sqft_living', 'grade', 'bathrooms', 'view', 'bedrooms', 'sqft_above', 'sqft_living15','sqft_basement'\n                 ,'lat']].values\nX_test = test[['sqft_living', 'grade', 'bathrooms', 'view', 'bedrooms','sqft_above', 'sqft_living15','sqft_basement'\n              ,'lat']].values\n\ny_train = train['price'].values.reshape(-1,1)\ny_test = test['price'].values.reshape(-1,1)","7c76d70a":"# Fitting a Regression Model 2\nmodel2 = LinearRegression()\nmodel2.fit(X_train,y_train)\n\nprint('Intercept: ', model2.intercept_)\n\nprint('Coefficients: ', model2.coef_)\n\ndf2 = pd.DataFrame(model2.coef_, columns = ['sqft_living', 'grade', 'bathrooms', 'view', 'bedrooms', \n                                           'sqft_above', 'sqft_living15','sqft_basement','lat'])\nprint(df2)","c2bbfe79":"# Making Predictions of Model 2\npred2 = model2.predict(X_test)\npred2","cddcba30":"# RSS of model 2\nRSS_2 = np.sum((y_test - pred2)**2)\nprint(\"RSS_2: \", np.sum((y_test - pred2)**2))","386bf788":"# Separating Attributes and Labels\nX_train = train[['sqft_living', 'grade', 'bathrooms', 'bedrooms','view','lat']].values\nX_test = test[['sqft_living', 'grade', 'bathrooms', 'bedrooms','view','lat']].values\n","8fc2ffef":"# Fiting a Regression Model\nmodel3 = LinearRegression()\nmodel3.fit(X_train,y_train)\n\nprint('Intercept: ', model3.intercept_)\n\nprint('Coefficients: ', model3.coef_)\n\ndf3 = pd.DataFrame(model3.coef_, columns = ['sqft_living', 'grade', 'bathrooms', 'bedrooms', 'view','lat'])\nprint(df3)","237b8619":"# Predicting the value\npred3 = model3.predict(X_test)\npred3","4a85bd4b":"# RSS\nRSS_3 = np.sum((y_test - pred3)**2)\nprint(\"RSS_3: \", np.sum((y_test - pred3)**2))","21547689":"RSS = pd.DataFrame(np.array([[RSS_1,RSS_2,RSS_3]]),columns = ['RSS_1','RSS_2','RSS_3'])\nprint(RSS)","7197abd4":"def get_data(data,features, output):\n    data['constant'] = 1\n    features = ['constant'] + features\n\n    features_new = data[features]\n    feature_matrix = np.asarray(features_new)\n    \n    output_data = data[output]\n    output_array = output_data.to_numpy()\n    return(feature_matrix,output_array)","f0dbb4fe":"def prediction(feature_matrix, weights):\n    predictions = np.dot(feature_matrix,weights)\n    return(predictions)","d2a0a398":"def feature_derivative(errors,feature):\n    derivative = 2*(np.dot(errors,feature))\n    return(derivative)","5e4c6ffe":"def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n    converged = False\n    weights = np.array(initial_weights)\n    while not converged:\n        predictions = prediction(feature_matrix,weights)\n        \n        errors = predictions - output\n        \n        gradient_sum_squares = 0\n        for i in range(len(weights)):\n            derivative = feature_derivative(errors,feature_matrix[:,i])\n            \n            gradient_sum_squares = derivative**2 + gradient_sum_squares\n            \n            # subtract the step size times the derivative from the current weight\n            weights = weights - step_size*(derivative)\n            \n            gradient_magnitude = sqrt(gradient_sum_squares)\n            if gradient_magnitude < tolerance:\n                converged = True\n    return(weights)  ","88913b22":"(feature_matrix,output_array) = get_data(train, ['sqft_living'],'price')\ninitial_weights = np.array([-47000., 1.])\npredictions = prediction(feature_matrix, initial_weights)\nerrors = output_array - predictions\nstep_size = 7e-12\ntolerance = 2.5e7","743c2cb1":"weights = regression_gradient_descent(feature_matrix, output_array, initial_weights, step_size, tolerance)","1691e134":"print(weights)","ac3190ad":"# Using test data to compute RSS\n(feature_matrix_test,output_array_test) = get_data(test, ['sqft_living'],'price')\n","b53395b5":"prediction_test = prediction(feature_matrix_test, weights)\nerrors = output_array_test - prediction_test\nRSS_simple_GD = np.sum((errors)**2)\nprint(\"RSS_simple_GD: \", np.sum((errors)**2))","d4284c2c":"print(\"RSS using Simple Regression: \", RSS_simple)\n","f9d63b6a":"### Predictions\n\nThen, we get the 1D array predictions by multiplying 2D rfeature_matrix with 1D regression weights which is a dot product between the two vectors. We will define the initial weights to be [-47000, 1]","b20387b0":"# Exploratory Data Analysis","1992a4dd":"### Boxplot","4452ec4b":"### Predictions - Simple Regression","23b67779":"### Fitting A Simple Regression Model","596a5054":"### Residual Sum of Squares","e10495b9":"### Covariance\n","4d5e56e6":"### Missing Values","f8779adb":"As we can see, adding or removing variables may result in the Residual Sum of Squares to rise.","385d4223":"As we can see from the above graph, that the variable 'price' is skewed towards the right. ","af5f0c2d":"### Gradient Descent Algorithm\n","3033ae8d":"# Multivariate Analysis","1d0153f4":"The value of the coefficients, for e.g. sqft_living, represents the predicted change in the value of price per unit change in the value of square feet.","b3b2cb2e":"To get an overview of the distribution of the response or dependent variable \"price\", we will plot a normal distribution curve which will tell us the skewness, and spread of the data. ","1f658e9e":"### Residual Sum of Squares","734e45ad":"#### Dropping Variable ","7da01da9":"A better approach would then be to use hill climbing or descent technique for finding the maximum or minimum respectively. Instead of equating the derivative to 0, we move along the curve from one point to another by updating the value or vector of the estimated parameter W (or weights of the given input features). This is done through an iterative process by defining a stepsize and convergence criteria. \n\nFirst, we assume an initial value of our estimated parameters (w,regression coefficients) and compute the derivative of the cost function (RSS) at that point. Then at each iteration the previous value of w is either increased or decreased by the amount based on the derivative as determined by the stepsize (Instead of the derivative, the value is increased or decresed by the amount of stepsize). \n\nIn case of min, if the value of the derivative is -ve, increase the value of w. If the value of the derivative is +ve, decrease the value of w.\n\nLaslty, how do we assess the convergence? The algorithm will not converge until the magnitude of the derivative is less than the tolerance level i.e. threshold 'e', which will be a very small number. The threshold will be set by us and the algorithm will not terminate until the condition is satisfied.\n","40b49259":"The boxplot provides a visual summary of the following items in a data:\n1. Inter Quartile Rande (IQR)\n2. Median\n3. Whiskers\n4. Outliers","1e52be55":"The boxplot shows that there are some suspected outliers in the dataset. ","f1a28f3f":"### Attributes and Labels","12226ec6":"### Model 3\nIn this model, we will remove the variables ('sqft_above', 'sqft_living15','sqft_basement') as they are strongly related to the variable sqft_living.\n\n\n\n\n\n\n","551bde77":"### Plotting line of Best Fit","3015252b":"There are some varibales such as ID and date which are irrelevant in predicting the value of the house. Therefore,before proceeding further we will drop these variables.","8dfa423b":"### Training and Test Dataset\nThe dataset will be split into a training and test dataset. The training dataset will be used to fit the model i.e. it will find the estimated parameters of the model while the test dataset will be used for predicting the value of price. ","079c8bc8":"### Derivative\n\nNow we take the derivative of the cost function. A derivative function is then defined which takes feature and error array and returns a number. ","073d4f81":"### Residual Plot","7ecf45ac":"### Model 2\nIn model 2 we will add the variables  ('sqft_basement', 'lat') as they are correlated with \"price\".","00124618":"### Define Parameters\n\n1. features\n2. output\n3. errors\n4. tolerance\n5. stepsize","48907f4e":"By looking at the value of correlation coefficients from above, we can see identify variables that have a strong (+ve or -ve) relationship with the variable 'price'. These are 'sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'bedrooms', 'sqft_basement'.","0b59a7a9":"The correlation coefficient shows the relationship or association between the dependent variable and independent variable. The value of the coefficient is between -1 and 1. \n\n**Positive Correlation** - Positive Correlation lies between 0 and 1, where a value close to 1 represents a strong positive correlation and a value close to 0 represents a weak positive correlation. \n\n**Negative Correlation** - Negative Correlation lies between -1 and 0, where a value close to -1 represents a strong negative correlation and a value close to 0 represents a weak negative correlation. \n\nHowever, we must always remember that correlation does not imply causation.","79aee94f":"# Introduction\nRegression is one of the most important machine learning techniques. It is mainly used for identifying the relationship between two or more variables and for predicting a continuous variable. \n\nA particular dataset contains a response variable and a set of explanatory variables, where the explanatory variables are said to be the features of the response variable. Regression modeling is used to estimate the relationship between the variables and predict the value of response variable by finding a line of best fit that minimizes the Residual Sum of Squares (RSS) i.e. the difference between the actual value and predicted value. \n\nIn this analysis, we will first build a simple and a multiple regression model using python library scikit learn and then use the Gradient Descent Algorithm, an optimization technique, to find the minimum of the function RSS. \n\nWe must always remember that our results are based on the quantity and quality of the available dataset. ","586432d8":"From above we can see that the RSS of Simple Regression using the optimization technique, Gradient Descent, is much lower than RSS of simple regression using a pyhton library.","f57bc545":"Before moving further with analysis it is important to check if there are any missing values in the dataset. As we can see from the code below, there are no missing values in the dataset. ","8ab08a0d":"# Gradient Descent\n\nIn a simple regression model, we try to estimate the value of the response variable (house price) from a single explanatory varibale which in this case is \"sqft_living\" by fitting a line that best fits our model. \n\nBut how do we measure the quality or performance of our model? \n\nThis is done by defining a cost function such as Residual Sum of Squares (RSS) in terms of our estimated parameters (w0, w1) where w0 is the intercept and w1 is the coefficeint of 'sqft_living'. Therefore, our main goal in fitting a model is to minimize cost function, RSS(w0,w1),  over all possible values of (w0, w1) i.e. search over the space of all possible lines and find the line that minimize the RSS.\n\nGradient Descent is an optimization technique in Machine Learning that allow us to find specific values of w0 and w1 which minimizes our RSS. \n\nTo understand this in more detail, we can try to find the minimum and maximum of the cost function analyticaly. For example, in order to find the minimum of a convex function, we will compute the derivative of that function and equate it to 0. However, this could become computationaly intensive if there are more than 1 variables. \n\n","8ac4e86f":"From the above covariance matrix, we can see that the variables ('sqft_living','sqft_above', 'sqft_living15') have a strong relationship with one another.","d0aeee5c":"## Simple Regression - Gradient Descent\nTo understand Gradient Descent, let's start with simple regression by taking a single input feature 'sqft_living' from the housing dataset.\n\nThe following function takes data, input features, output and returns a feature_matrix which will consist a first column of ones and then the value of input features in the order defined. It also returns an output 'price' of the dataset. \n\n\n### Input and Output","4942867d":"# Multiple Regression Model\n\nWe will build 3 multiple regression models by adding and removing different variables to see their affect on RSS. This will be done by using the built-in library in python i.e. scikit learn.\n\nThe training and testing datasets defined above will be used in this model.\n\n### Model 1\nIn this model, we will take all the variables ('sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'bedrooms') which were highly corelated to the variable 'price' in our model. \n","1d6d3723":"# Simple Regression Model\nIn a simple regression model, only a single explanatory variable is used to predict the value of response variable. In this case, the response variable is 'price' while explanatory variable would be 'sqft_living' as this variable had the highest positive correlation with price.","d8a4bb0d":"# Univariate Analysis","28fc948a":"# Correlation Matrix"}}