{"cell_type":{"990317aa":"code","ebdd98a1":"code","558c2acd":"code","e2478355":"code","43bfa38a":"code","784b25cf":"code","0d1006a6":"code","bf6ea8f0":"code","30740c86":"code","495c7e3a":"code","89934f7e":"code","aa0efd0a":"code","d79e3b86":"code","a05e14f2":"code","5642a04f":"code","2e4fa92d":"code","cd23c980":"code","4681aea3":"code","179e8097":"code","57da4ef9":"code","c4bdc0ea":"code","9de0a26f":"code","2b8f6aed":"code","0e714fe9":"code","f0a9178c":"code","6721c57f":"code","ac03bb7e":"code","84d0d76e":"code","df167801":"code","37ea6aef":"code","1895d4f2":"code","54d0638c":"code","297a94e3":"code","86bed39d":"code","bdc3df0d":"code","bb88e704":"code","7a9982f5":"markdown","d6165f15":"markdown","eb7f8137":"markdown","9aa371d1":"markdown","67e0799a":"markdown","855994a3":"markdown","24de2abc":"markdown","a738a0cf":"markdown","2425e67f":"markdown","79a0bc2d":"markdown","36f62fb2":"markdown","60f57532":"markdown","b152e3ce":"markdown","774b462c":"markdown","fc4250a3":"markdown","8eba3cfc":"markdown"},"source":{"990317aa":"#Elemental Library\nimport pandas as pd \nimport numpy as np \nimport math \n\nimport re \nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport string\n\n#Visualization\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nsns.set_style('whitegrid')\nimport cufflinks as cf\ncf.go_offline()\nfrom IPython.display import display\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ebdd98a1":"path=\"..\/input\/loanimages\/os.png\"\ndisplay(Image.open(path))","558c2acd":"train = pd.read_csv('..\/input\/my-dataset\/credit_train.csv')\nprint(\"----------Technical Information-------------\")\nprint('Data Set Shape = {}'.format(train.shape))\nprint('Data Set Memory Usage = {:.2f} MB'.format(train.memory_usage().sum()\/1024**2))\nprint(\"Data columns type\"\"\\n\"\"{}\".format(train.dtypes.value_counts()))\ntrain.describe()","e2478355":"path=\"..\/input\/loanimages\/welcome.jpg\"\ndisplay(Image.open(path))","43bfa38a":"# General View\nprint('Target Column is = {}'.format(train.columns[2]))\nprint('-------------')\nprint('Variable Target =\\n{}'.format(train['Loan Status'].value_counts().index.to_list()))\nprint('-------------')\nprint('Columns in train are ={}'.format(train.columns.to_list()))\n\n# Missing Values\n\ntrain.isnull().sum()","784b25cf":"print('Train_Data')\ntrain.head(3)\n","0d1006a6":"missing = (train.isnull().sum()\/ len(train))*100\nTotal = missing.drop(missing[missing==0].index).sort_values(ascending= False)\nprint(\"Missing Data in %\\n\")\nprint(Total)","bf6ea8f0":"print(\"This Result shows all the row with Nan Values but Im intersting in those\\n which the entire row is nan\" )\ntrain[train.isna().any(axis=1)]","30740c86":"print(\"For a better Comprenhension I will drop the columns mentioned Above \\nWe can easily identify that the missing Values in Credit Score - Annual Income matches by index\")\ntrain.drop(['Loan ID','Customer ID'], axis = 1 , inplace= True)\n\n# Heat Map\nfig, ax = plt.subplots(figsize=(12,7))\nsns.heatmap(train.isnull(), yticklabels=False, cmap=\"viridis\", cbar=False,ax=ax)\nplt.show()","495c7e3a":"print(train.isnull().sum(axis=1).value_counts())\nprint('\\n')\nprint(\"Look this, numbers of Nan values by row, in other words there are\\n514 rows with 17 Missng Values\\n1 row with 4 Missng Values\\n...etc\")\nprint('\\n')\nprint(\"Total number of columns are 19, even though we can fill them by many method\\nit will be complicated and they represent only 0.5% of the Total Data \")","89934f7e":"# Finding number of Missing values by row \nNan_values_row = train.isnull().sum(axis=1)\n\n# Finding Index NAN values == 17 \nindex_NAN_Greater_17 = Nan_values_row[Nan_values_row==17].index\n\n\nprint(\"Here we have the 514 Row with Nan Values\")\ntrain.iloc[index_NAN_Greater_17][:5]  #Showing them ","aa0efd0a":"train.drop(index_NAN_Greater_17,axis = 0 , inplace = True)\ntrain.reset_index(drop=True)","d79e3b86":"t_object = train.dtypes[train.dtypes == 'object'].index\nt_float = train.dtypes[train.dtypes == 'float'].index\n\n\ndef nan_col (data):\n    for i in data:\n        MV = train[i].isna().sum()\n        if MV > 0:\n            print(i,MV)\n\nprint(\"Here we have Missing Values by Columns - type = object\")\nnan_col(t_object)\nprint('\\n')\nprint(\"Here we have Missing Values by Columns - type = float\")\nnan_col(t_float)\nprint('\\n')\nprint(\"I will go Colum by Column from the Lowest to Greatest\")","a05e14f2":"def corr_heat (frame):   #<---Heat Map\n    correlation = frame.corr()\n    f,ax = plt.subplots(figsize=(15,10))\n    mask = np.triu(correlation)\n    sns.heatmap(correlation, annot=True, mask=mask,ax=ax,cmap='viridis')\n    bottom,top = ax.get_ylim()\n    ax.set_ylim(bottom+ 0.5, top - 0.5)\n\n\n# Heat Map\ncorr_heat(train)\nprint(\"I plotted a Heat map to find any relation among variables to start filling them out\")","5642a04f":"print(\"there is no a variable which has strong relation with Maximum Open Credit, there fore I will search for a person who has similar behaviour and fill the value\")\n\ntrain[train['Maximum Open Credit'].isna()][['Maximum Open Credit','Credit Score','Annual Income','Monthly Debt','Years of Credit History','Number of Open Accounts','Current Credit Balance']]","2e4fa92d":"Value_1 =train[(train['Years of Credit History']==15.3) & (train['Number of Open Accounts'] == 3 )]['Maximum Open Credit'].mean()\nValue_2 = train[(train['Credit Score']>=7030.0) & (train['Number of Open Accounts'] == 9 ) & (train['Years of Credit History'] >= 22 )]['Maximum Open Credit'].mean()\n\nprint(\"Ok 1. Pandas find someone who's Years of Credit History = 15.3,Open Accounts = 3 and then Tell me the Maximun open Credit average\\nHere you are\")\nprint(round(Value_1,1))\nprint('\\n')\nprint(\"Ok 2. Pandas now find someone who's Years of Credit Score => 7030 ,Open Accounts = 9 , Years of Credit History = 9  then Tell me the Maximun open Credit average\\nHere you are\")\nprint(round(Value_2 ,1))\n\n# then Filling by index \n\ntrain.loc[30180,'Maximum Open Credit'] =Value_1\ntrain.loc[98710,'Maximum Open Credit'] =Value_2\n\nprint('\\nFilling  Nan Values ................... Done..!!') ","cd23c980":"print('Tax Liens has strong relation with Number of Credit Problems , this might help us to fill them out')\ntrain.loc[train['Tax Liens'].isna()][['Number of Credit Problems','Annual Income','Tax Liens']]","4681aea3":"print(\"Here something happened , Tax liens is imposed as a guarantee so people have to pay or they lose their houses, If they have had 1 or less Credit Problems We can assume that those 10 people didnt reach that point\")\nprint(\"\\nfill Nan Values in Tex liens with 0\")\ntrain['Tax Liens'].fillna(0,inplace=True)\nprint('\\nFilling  Nan Values ................... Done..!!') ","179e8097":"print(\"Bankruptcies and Number Of credit Problems are related, Im almost sure that we will have the same escenario as in Tax lines\")\ntrain.loc[train['Bankruptcies'].isna()][['Number of Credit Problems','Annual Income','Tax Liens','Bankruptcies']].tail(10)","57da4ef9":"print(\"As I thought No Credit Problems , no Tax liens = No Bankruptcies\")\nprint(\"\\nfill Nan Values in Bankruptciess with 0\")\ntrain['Bankruptcies'].fillna(0,inplace=True)\nprint('\\nFilling  Nan Values ................... Done..!!') ","c4bdc0ea":"train[train['Months since last delinquent'].isna()][['Credit Score','Months since last delinquent','Years of Credit History']]","9de0a26f":"print(\"please Notice that in USA 'Late payments generally wont end up on your credit reports for at least 30 days after you miss the payment. so you have almost 60 day to pay \")\nprint(\"and if you notice the Credit Score is greater that 700,meaning that they have good - Excellent credit score\")\nprint(\"Which made me think of those Nan Values as 0 , I mean 0 month having a late payment\")\nprint(\"\\nfill Nan Values in Months since last delinquent with 0\")\ntrain['Months since last delinquent'].fillna(0,inplace=True)\nprint('\\nFilling  Nan Values ................... Done..!!') ","2b8f6aed":"# finding outliers\ndef Outliers (data,column):\n    mean_ = data[column].mean()\n    Sdev_= data[column].std()\n    Upper_limit= mean_+ (3*Sdev_)\n    lower_limit= mean_- (3*Sdev_) #error \n    out= data[(data[column]>Upper_limit)|(data[column]<lower_limit)].index\n    \n    data.drop(out ,inplace=True)\n    \n\ntrain.reset_index(drop=True)\ntrain.reset_index(drop=True)","0e714fe9":"def find_ranges(data,column):\n    count= data[column].count()\n    max_ = max(data[column])\n    min_ = min(data[column])\n    element = math.trunc(np.sqrt(count))\n    interval =math.trunc(max_\/element)\n    print(\"count=\",count,\"max=\",max_,\"min=\",min_,\"N-elements=\",element,\"Intervals=\",interval)\n    \nfind_ranges(train,'Annual Income')\n\n#Range monthly debt \nlower_limit = np.arange(0,4621560.0,16867)\nupper_limit = np.arange(16866.99,4638427.0,16867)\n\n\n#Replacing Values\nincome = []\nindex = []\n              \nprint(\"\\n-------------Starting --------------\\n\")\nfor i, j in zip(lower_limit,upper_limit):\n    value= round(train[(train['Annual Income'].notna())&(train['Monthly Debt']>=i)&(train['Monthly Debt']<=j)]['Annual Income'].mean(),2)\n    income.append(value)\n    ind = train[(train['Monthly Debt']>=i)&(train['Monthly Debt']<=j)&(train['Annual Income'].isna())]['Annual Income'].index\n    index.append(ind)\n    \n    #print(\"From\",i,\"to\",j,\"Annual Income =\",value,\", Values to be replaced\", ind[:5])\n    \nfor i, j in zip (index,income):\n    train.loc[i,'Annual Income']= j\n\nprint(\"\\nReplacing Multiples Values in 'Annual Income' ..............Done\")\nprint(\"\\nThe following Rows couldnt be replaced because there were not data between the ranges...They will be dropped\")\ntrain[train['Annual Income'].isna()]\n","f0a9178c":"train.drop([11648,68650], inplace=True)","6721c57f":"# Fixing Digits\n\ndef fixing (data,digits=3): \n    if data >0:\n        new_credit_score = int(str(data)[:digits])\n        return new_credit_score\n    else:\n        return data\n    \n#Fixing Digits\nprint(\"\\n------- Before Replacing Values------\\n\")\nprint(\"Credit Score Min =\",min(train['Credit Score']),\"\\nCredit Score Max=\",max(train['Credit Score']),\"\\nCredit Score Average=\",round(np.mean(train['Credit Score']),2))\ntrain['Credit Score'].fillna(720, inplace = True)\ntrain['Credit Score']=train['Credit Score'].apply(lambda x: fixing(x))\nprint(\"\\n------- After Replacing Values------\\n\")\nprint(train['Credit Score'].describe())\n\n","ac03bb7e":"# Extracting numbers\n\ndef extract_numbers (data):\n    if data == str(data):\n        text = [w for w in data if w in string.digits]\n        years = int(''.join(text))\n        return years\n\ntrain['Years in current job']= train['Years in current job'].apply(lambda x: extract_numbers(x))\nprint('\\nFollowing some information from the web the Average time in the same job in USA is between 4.2 and 5.6 years , I will use 5 to fill all missing values ')\ntrain['Years in current job'].fillna(5,inplace=True)\nprint('\\nFilling  Nan Values ................... Done..!!\\n') \nmissing = (train.isnull().sum()\/ len(train))*100\nTotal = missing.drop(missing[missing==0].index).sort_values(ascending= False)\nprint(\"Missing Data in %\\n\")\nprint(Total)\n\ntrain['Years in current job']=train['Years in current job'].astype(int)","84d0d76e":"print('Train_Data')\ntrain.head(3)\ntrain.reset_index(drop=True)","df167801":"path=\"..\/input\/loanimages\/see.jpg\"\ndisplay(Image.open(path))","37ea6aef":"train.drop(train[train['Current Loan Amount']==99999999.0].index, inplace=True)\ntrain.reset_index(drop=True)","1895d4f2":"fig = plt.figure(constrained_layout=True, figsize=(20,10))\ngrid = gridspec.GridSpec(ncols=6, nrows=2, figure=fig)\n\n#bar plot Horizontal\nax1 = fig.add_subplot(grid[0, :2])\nax1.set_title('Loan Status')\nsns.countplot(y='Loan Status',hue ='Term',data=train, ax=ax1,) #Paid no paid\n\n#bar plot Vertical\nax2 = fig.add_subplot(grid[1, :2])\nax2.set_title('Purpose segmented by Fully Paid\/Charged Off')\nbar = sns.barplot(x='Purpose', y='Current Loan Amount', hue = 'Loan Status',data=train, ax = ax2)\nbar.set_xticklabels(bar.get_xticklabels(),  rotation=90, horizontalalignment='right') #fixing the Names\n\n#box plot Credit Score\nax3 = fig.add_subplot(grid[:, 2])\nax3.set_title('Credit Score')\nsns.boxplot(train.loc[:,'Credit Score'], orient='v', ax = ax3)\n\n\n#box plot Monthly payment\nax4 = fig.add_subplot(grid[:,3])\nax4.set_title(\"Amount paid Monthly\")\nsns.boxplot(train['Monthly Debt'], orient='v' ,ax=ax4)\n\n#Displot Distribution\nax5 = fig.add_subplot(grid[0, 4:6])\nax5.set_title(\"Amount borrowed 'Blue= fully Paid, red=Charged Off'\")\n#---> Segmenting fully Paid \/Charge Off\nfull_paid = train[train['Loan Status']==\"Fully Paid\"]\ncharged_off = train[train['Loan Status']==\"Charged Off\"]\nsns.distplot(full_paid['Current Loan Amount'], color = 'Blue' , rug=False, ax=ax5) \nsns.distplot(charged_off['Current Loan Amount'], color = 'Red',rug=False, ax=ax5) \n\n#Displot Distribution\nax6 = fig.add_subplot(grid[1, 4:6])\nax6.set_title(\"Annual Income 'Blue= fully Paid, red=Charged Off'\")\n#---> Segmenting fully Paid \/Charge Off\nshort = train[train['Term']=='Short Term']\nlong = train[train['Term']=='Long Term']\nsns.distplot(short['Current Loan Amount'], color = 'Blue' , ax=ax6) \nsns.distplot(long['Current Loan Amount'], color = 'Green', ax=ax6)\n\nplt.show()\n\nprint(\"The Loan Status chart shows most of the fully payments are short term, if we take a look of the 'current loan amount chart' 'Blue=Short' show that most of the loan are between 0-200K\\nComparing to 'long Term loan'which are from 200K to almost 6000K\")\nprint(\"The Purpose chart show that 'Business Loan, Consildation,buy a house and Improvement' are the Top 4 for reason for loana application,However the purpose could not tell us if someone will pay in full or not\")\nprint(\"\\nMost of the applicants have credit Scores between 675 - 750 which can be Considered Fair~Good\")\nprint(\"\\nThe average payment recieved monthly is almost 200K\")\nprint(\"\\nThe Institution who borrows the money usually borrows arround 0-400K \")","54d0638c":"from sklearn.preprocessing import OneHotEncoder\n\nprint(\"The following Columns have string Values\\n\")\nt_object = train.dtypes[train.dtypes == 'object'].index\nfor i in t_object:\n    a= train[i].nunique()\n    print(i,a)\n\nprint(\"\\n-------------Starting---------------\\nEncoding with pd.get_dummies\\n-------Done..!!\")\nfor i in t_object:\n    econ = pd.get_dummies(train[i], prefix =i, drop_first=True)\n    train = pd.concat([train,econ],axis = 1)\n    train.drop(i,axis=1,inplace=True)\n    \ntrain.head()","297a94e3":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold, f_classif , f_regression, SelectKBest, SelectPercentile\n\n#Defining \nX=train.drop('Loan Status_Fully Paid',axis=1)\ny=train['Loan Status_Fully Paid']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Removing Constant , QuasiConstant\nCF = VarianceThreshold(threshold=0.01)\nCF.fit(X_train)\nX_train_ = CF.transform(X_train)\nX_test_ = CF.transform(X_test)\n\n\nX_data = VarianceThreshold(threshold = 0.01).fit_transform(X)\nprint(\"before Removing Constant and QuasiConstant values the data set has\",X.shape)\nprint(\"after Removing The data set has\",X_data.shape)","86bed39d":"from sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_predict\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\n\n\nMX=MinMaxScaler()\nX_data =MX.fit_transform(X_data)\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.33, random_state=42)\n\n\n# Model initilization\nknn = KNeighborsClassifier()\ntree_gin = DecisionTreeClassifier(criterion = 'gini')\ntree_ent = DecisionTreeClassifier(criterion = 'entropy')\n#svm_lin = svm.SVC(kernel = 'linear')\n#svm_rbf = svm.SVC(kernel = 'rbf')\nlog = LogisticRegression()\nrf = RandomForestClassifier()\nxgb = XGBClassifier()\n\nmodels = [knn,tree_gin,tree_ent,log,rf,xgb]#,svm_rbf,svm_lin]","bdc3df0d":"def model_fit_predict(model,X_train, X_test, y_train, y_test):\n    model = model.fit(X_train,y_train) #fitting\n    y_pred = model.predict(X_test) #predicting\n    model_acurracy=accuracy_score(y_test, y_pred) #Evaluating\n    accuracy.append(model_acurracy)\n    \naccuracy=[]\n\nfor i in models:\n    model_fit_predict(i,X_train, X_test, y_train, y_test)\n    \n#print(accuracy)\n\n#Visulization for the best model \nmodel_eval = pd.DataFrame(accuracy, \n                          index=['knn','tree_gin','tree_ent','log','rf','xgb'],\n                         columns= ['Accuracy'])\nprint('--- Accuracy Scores---')\nmodel_eval.sort_values(by='Accuracy', ascending=False)","bb88e704":"path=\"..\/input\/loanimages\/upvote.jpg\"\ndisplay(Image.open(path))","7a9982f5":"### 2.2 Feature Engineering \n\n1. First let's Find all those rows which has many Missing Values \n2. Let's Drop those Columns that has combination of Letter and Numbers Ex. Loan, CustomerID\n\n### 2.2.1 Exploring, Cleaning and Dropping","d6165f15":"### b) Tax Liens","eb7f8137":"### a) Maximum Open Credit 4","9aa371d1":"### f) Years in current job\n\n- First This columns is a combination of letter and numbers (Strings and int) \n- I will create a fuction to extract numbers only... using the library ***string***","67e0799a":"# 3. Modeling \n- ordinal, Categorical Variable into Numerical Using One Hote Encoder or OnelaborEnconder\n- Find and Remove Constant,Quasi Constant and Correlated Features\n- Machine Learning Algorithms","855994a3":"### e) Annual Income\n\n1. In order to get the Annual income I will first Normalize (Find outliers)\n2. I will use Monthly debt to make ranges to find average Salary\n3. iterate using list and Dict to replace Value by index","24de2abc":"### d) Months since last delinquent","a738a0cf":"### 2.2.3 Column by Column\n\nBefore going foward ***remember*** we have ***12 float - columns and 7 objects - columns***","2425e67f":"### As We can notice:\n1. Many columns have something in common ***The 514 Missing Values***\n2. Credit Score and Annual icome have ***19668 Missing Values*** \n\n### ***Aproach*** if the Missing Values > 80% Total set, then it will be dropped\n1. Go through Each Column, Fill Missing (interpolate, by range, mean, average, etc)","79a0bc2d":"## Greeting\n\n### As Always I'm following the OSEM Methodology..!!! Hopefully you can have some new knowledge from Here..!!","36f62fb2":"### f) Credit Score\n\nSomething is wrong with the Credit Score because it shows that the ***max score is > 1000 and I check on the web and All credit Score no matter the contries goes from 300-579: Poor.\n580-669: Fair. 670-739: Good. 740-799: Very good. 800-850: Excellent.***\n- I believe that They added one more 0 to the values , so instead of 7510 it should be 751\n- There are several way to fill Credit Score , such as ranges by other columns , as we did before, but In my experience credit score is depends on many factors which we dont have here , that is why and please notice *** It is not recommendable*** repalce those nan\/0 values with a number ***between 670-739: Good*** I did this because most of the USA citizen have this Credit Score ","60f57532":"### 3.1 Machine Learning","b152e3ce":"# 2. Scrub Data\n\n### 2.1 General View","774b462c":"# 1. Obtain Data ","fc4250a3":"# 3. Exploring \n- Creating Many Plots to find Intersting Information \n- there are many ouliers in 'Current Loan' so I will drop it ","8eba3cfc":"### c) Bankruptcies"}}