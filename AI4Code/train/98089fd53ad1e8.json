{"cell_type":{"92ced3ba":"code","f4a809e9":"code","b084f2a9":"code","ea3ac62c":"code","d3bbcf59":"code","2eba5710":"code","792e2433":"code","04b909d3":"code","5c7e03ba":"code","77feadb5":"code","2cece807":"code","1e288a62":"code","0f4f8c55":"code","0cd4d038":"code","8c63dfa7":"code","8b99c18d":"code","c4bba1fe":"code","746737c4":"code","995f15ca":"code","7a32cbbd":"code","ab961876":"code","a3f19778":"code","db79afdc":"code","439191d6":"code","c0874058":"code","17c56d28":"code","a6929fd0":"code","ad0d44c5":"code","4aa5cf4f":"code","b0b1b4ed":"code","e7f970a9":"markdown","e8ca720b":"markdown","e4fc3909":"markdown","5e6ef121":"markdown","7c78eaf6":"markdown","47f69d7a":"markdown","8015f3be":"markdown","4ff8bbc7":"markdown","f03e2c4e":"markdown","facd73ce":"markdown","4ca0f8dc":"markdown","b1434ad9":"markdown","a5fba6d5":"markdown","0b42ab85":"markdown","c6f8725f":"markdown","04b96cab":"markdown","4b0d2735":"markdown","24a3329c":"markdown","1671aa39":"markdown","4cfe714b":"markdown","b93159f0":"markdown","99165a16":"markdown","960a5897":"markdown","25a5394f":"markdown","43e53438":"markdown","290da730":"markdown","6aff3680":"markdown","8db6e86e":"markdown","bff2009d":"markdown","445dec93":"markdown","17b4a108":"markdown","5fbcb3b1":"markdown","f5b9150e":"markdown","f27c2c0b":"markdown","e185e50c":"markdown","cac3b75d":"markdown"},"source":{"92ced3ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f4a809e9":"traindf = pd.read_csv(\"..\/input\/train.csv\")","b084f2a9":"traindf.tail(10)","ea3ac62c":"#traindf.describe()\ntraindf.isnull().sum()","d3bbcf59":"traindf.dropna().describe()","2eba5710":"traindf.dropna(axis=1).describe()","792e2433":"traindf['Age'].describe()","04b909d3":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntraindf['Age'] = imp.fit_transform(traindf[['Age']])\n\ntraindf['Age'].describe()\n\n#imr = Imputer(missing_values=np.nan, strategy='mean', axis=1)\n#traindf['Age'] = imr.fit_transform(traindf['Age']).T\n\n#traindf.describe()","5c7e03ba":"traindf[['Fare', 'Age']].describe()","77feadb5":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\ntraindfScaled = mms.fit_transform(traindf[['Fare', 'Age']])\ntraindf[['Fare', 'Age']] = traindfScaled\ntraindf[['Fare', 'Age']].describe()","2cece807":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(traindf['Ticket'].values)\ny","1e288a62":"traindf.head()","0f4f8c55":"traindf = pd.get_dummies(traindf, columns = ['Embarked', 'Sex'])\n#Once encoded let's see. \ntraindf.head()","0cd4d038":"#The most simple one :D\ntraindf['ratioFareClass'] = traindf['Fare']\/traindf['Pclass']\n\ntraindf.head()","8c63dfa7":"features = ['Age', 'Fare', 'Pclass', 'Embarked_C', 'Embarked_S', 'Embarked_Q', 'Sex_female', 'Sex_male', 'ratioFareClass']\ntraindf['Survived'].value_counts()","8b99c18d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(traindf[features],\n                                                    traindf[\"Survived\"],\n                                                    test_size=0.3,\n                                                    stratify=traindf['Survived'])\n\n# test size as a 30%\n# stratify on survived","c4bba1fe":"y_test.value_counts()","746737c4":"traindf[features].head()\n# Features is the final feature list we are going to use. ","995f15ca":"traindf.describe()\n","7a32cbbd":"#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=0, solver='lbfgs',\n                         multi_class='multinomial').fit(X_train, y_train)\n\n\nprint(\"Logistic Regression score (Train): {0:.2}\".format(lr.score(X_train, y_train)))\nprint(\"Logistic Regression score (Test): {0:.2}\".format(lr.score(X_test, y_test)))\n","ab961876":"#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train, y_train)\nprint(\"KNN score (Train): {0:.2}\".format(neigh.score(X_train, y_train)))\nprint(\"KNN score (Test): {0:.2}\".format(neigh.score(X_test, y_test)))\n","a3f19778":"#http:\/\/scikit-learn.org\/stable\/modules\/svm.html\nfrom sklearn import svm\nsvclass = svm.SVC(gamma='scale')\nsvclass.fit(X_train, y_train) \nprint(\"SVM score (Train): {0:.2}\".format(svclass.score(X_train, y_train)))\nprint(\"SVM score (Test): {0:.2}\".format(svclass.score(X_test, y_test)))\n","db79afdc":"#http:\/\/scikit-learn.org\/stable\/modules\/tree.html\nfrom sklearn import tree\ndt = tree.DecisionTreeClassifier()\ndt = dt.fit(X_train, y_train)\nprint(\"Decision Tree score (Train): {0:.2}\".format(dt.score(X_train, y_train)))\nprint(\"Decision Tree score (Test): {0:.2}\".format(dt.score(X_test, y_test)))\n","439191d6":"# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=100,\n                                criterion='gini',\n                                max_depth=5,\n                                min_samples_split=10,\n                                min_samples_leaf=5,\n                                random_state=0)\nX_train.head()\nforest.fit(X_train, y_train)\nprint(\"Random Forest score (Train): {0:.2}\".format(forest.score(X_train, y_train)))\nprint(\"Random Forest score (Test): {0:.2}\".format(forest.score(X_test, y_test)))\n","c0874058":"model = forest","17c56d28":"#WAAAAARNING: not all models have the \"feature_importances_\" functions\nplt.bar(np.arange(len(features)), model.feature_importances_)\nplt.xticks(np.arange(len(features)), features, rotation='vertical', ha='left')\nplt.tight_layout()","a6929fd0":"X_test\n\nmodel = forest\n# This is an example! Also a bad practise :D\n#AGE, SEX, AGE_NAN, PClass, FARE\ntestcase = np.array([[25, 12, 3, 0, 1, 0, 1, 0, 1]])\nprediction = model.predict(testcase)[0]\npproba = model.predict_proba(testcase)[0]\nprint(\"Prediction for test case: %s (perish -> %.2f, surv -> %.2f)\" %\n      ('PERISH' if prediction == 0 else 'SURVIVED!', pproba[0], pproba[1]))\n","ad0d44c5":"# http:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_ols.html\n\n# Code source: Jaques Grobler\n# License: BSD 3 clause\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the diabetes dataset\ndiabetes = datasets.load_diabetes()\n\n# Use only one feature\ndiabetes_X = diabetes.data[:, np.newaxis, 2]\n\n# Split the data into training\/testing sets\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\n\n# Split the targets into training\/testing sets\ndiabetes_y_train = diabetes.target[:-20]\ndiabetes_y_test = diabetes.target[-20:]\n# -----------------------------------------------\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","4aa5cf4f":"#http:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_polynomial_interpolation.html\n# Author: Mathieu Blondel\n#         Jake Vanderplas\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n\ndef f(x):\n    \"\"\" function to approximate by polynomial interpolation\"\"\"\n    return x * np.sin(x)\n\n\n# generate points used to plot\nx_plot = np.linspace(0, 10, 100)\n\n# generate points and keep a subset of them\nx = np.linspace(0, 10, 100)\nrng = np.random.RandomState(0)\nrng.shuffle(x)\nx = np.sort(x[:20])\ny = f(x)\n\n# create matrix versions of these arrays\nX = x[:, np.newaxis]\nX_plot = x_plot[:, np.newaxis]\n\ncolors = ['teal', 'yellowgreen', 'gold']\nlw = 2\nplt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n         label=\"ground truth\")\nplt.scatter(x, y, color='navy', s=30, marker='o', label=\"training points\")\n\nfor count, degree in enumerate([3, 4, 5]):\n    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n    model.fit(X, y)\n    y_plot = model.predict(X_plot)\n    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw,\n             label=\"degree %d\" % degree)\n\nplt.legend(loc='lower left')\n\nplt.show()","b0b1b4ed":"#http:\/\/scikit-learn.org\/stable\/auto_examples\/plot_isotonic_regression.html#sphx-glr-auto-examples-plot-isotonic-regression-py\n# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\n# Data is genereted randomly\nn = 100\nx = np.arange(n)\nrs = check_random_state(0)\ny = rs.randint(-50, 50, size=(n,)) + 50. * np.log1p(np.arange(n))\n\n# #############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\nir = IsotonicRegression()\n\ny_ = ir.fit_transform(x, y)\n\nlr = LinearRegression()\nlr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\n\n# #############################################################################\n# Plot result\n\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = LineCollection(segments, zorder=0)\nlc.set_array(np.ones(len(y)))\nlc.set_linewidths(np.full(n, 0.5))\n\nfig = plt.figure()\nplt.plot(x, y, 'r.', markersize=12)\nplt.plot(x, y_, 'g.-', markersize=12)\nplt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\nplt.gca().add_collection(lc)\nplt.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')\nplt.title('Isotonic regression')\nplt.show()","e7f970a9":"## Some regression examples","e8ca720b":"### Logistic Regression Classifier","e4fc3909":"## Identification of missings\n","5e6ef121":"### Support Vector Machines (Support Vector Classifier)","7c78eaf6":"**RRC**: Surprise! Age feature has nulls. Let's work on that. ","47f69d7a":"## Transformations","8015f3be":"### KNN Classifier","4ff8bbc7":"## Partitioning a dataset into separate training and test sets","f03e2c4e":"## Scaling ordinal features\n","facd73ce":"## Training ","4ca0f8dc":"**RRC**: It seems everything is Ok! so we can start playing with our data training different models. ","b1434ad9":"**RRC**: Check! min and max once scalled. Now, Fare and Age are numbers between 0 and 1. This is nice for a good training. ","a5fba6d5":"## One hot encoding\n\n**RRC**: Onehot encoding is a nice technique to encode variables. But warning! with this code this does not encode missing values. Try to fix that before you use this. \n\nCheck variables like Sex and Embarked. Let's one-hot encode them. ","0b42ab85":"## Record dropping","c6f8725f":"**RRC**: First of all, load our dataset.","04b96cab":"### Random Forest Classifier","4b0d2735":"**RRC**: Target variable is well balanced. Let's split dataset with a test size as 30% stratifying the target class","24a3329c":"**RRC**: Ok, let's fill (impute) null numerical values in Age. Our strategy is using the 'mean'. Naive, but secure. ","1671aa39":"### Ex 2: Polynomial interpolation","4cfe714b":"## Imputer","b93159f0":"**RRC**: just kidding. Plenty of values. But a good example ;D","99165a16":"### Model Evaluation","960a5897":"**RRC**: Uh oh! Wait! 183!? That's too much. Why?\nLet's try just removing those features with nulls. ","25a5394f":"## Encoding of categorical variables (some examples)","43e53438":"**RRC**: Let's normalize that. ","290da730":"**RRC**: What would happen if we try to encode the Ticket column? ","6aff3680":"* Identifying missing values (Typically NaN (Not a Number), null, empty cell in excel spreadsheet, etc.)\n* Eliminating samples or features with missing\n* Imputing missing values","8db6e86e":"**RRC**: And see what we have at the tail","bff2009d":"### Ex 3: Isotonic Regression","445dec93":"**RRC**: Check survived=1 is exactly a 30% of the total on y_test\n\nGood. Let's see our final dataset and some statistics about it. ","17b4a108":"**WARNING**: number 0 is not a missing value. Also, \" \" sometimes is not a missing value. It depends on the context! Big mistake impute 'null' values as 0. **Reminder** see number 0 as any other value that could bias a prediction. ","5fbcb3b1":"**RRC**: Our Fare and Age feautres are completely different. Check Max, quartiles, mins, and distribution are not so homogeneous. Let's fix that. ","f5b9150e":"### Ex 1: Diabetes dataset","f27c2c0b":"## **Preprocessing**","e185e50c":"### Decision tree","cac3b75d":"## Feature dropping"}}