{"cell_type":{"5c416f63":"code","1f0fffbf":"code","898008a1":"code","c9f7f95b":"code","f2e581bf":"code","98941476":"code","0eed1f95":"code","37ea344d":"code","8b2b038e":"code","7d7a1f3e":"code","1bc69011":"code","ce69db15":"code","f0dd846a":"code","c41f5bc4":"code","01019e82":"code","f070399d":"code","a67b6c14":"code","2230886b":"code","203c4346":"code","7c03cfee":"code","b65ab2af":"code","532278b0":"code","d9b97caf":"code","d8e00da8":"code","a607b020":"code","e4cd1aa9":"markdown","e890fa8c":"markdown","60394367":"markdown","670129a8":"markdown"},"source":{"5c416f63":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1f0fffbf":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","898008a1":"train.head(5)","c9f7f95b":"test.head(5)","f2e581bf":"TARGET='target'\nID='ID_code'","98941476":"train[TARGET].hist()","0eed1f95":"def plot_dist(col, train=train, target=TARGET):\n    plt.figure(figsize=(12,8))\n    target_0 = train[train[target]==0].dropna()\n    target_1 = train[train[target]==1].dropna()\n    sns.distplot(target_0[col].values, label='target: 0')\n    sns.distplot(target_1[col].values, color='red', label='target: 1')\n    plt.xlabel(col)\n    plt.legend()\n    plt.show()\n\ndef plot_train_test_dist(col, train=train, test=test):\n    plt.figure(figsize=(12,8))\n    sns.distplot(train[col].values, label='target: 0')\n    sns.distplot(test[col].values, color='red', label='target: 1')\n    plt.xlabel(col)\n    plt.legend()\n    plt.show()","37ea344d":"cols = test.drop(ID,axis=1).columns","8b2b038e":"for col in cols:\n    print(col)\n    plot_dist(col)","7d7a1f3e":"for col in cols:\n    print(col)\n    plot_train_test_dist(col)","1bc69011":"correlations = train.drop(ID, axis=1).corr()","ce69db15":"plt.figure(figsize=(20,20))\nsns.heatmap(correlations)","f0dd846a":"correlations = correlations.sort_values(by=TARGET)\ncorrelations.head(10)[TARGET]","c41f5bc4":"correlations.tail(10)[TARGET]","01019e82":"from sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.naive_bayes import GaussianNB","f070399d":"def display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('Features (avg over folds)')\n    plt.tight_layout()","a67b6c14":"def kfold_train(train, test, num_folds, CLF, stratified = False, debug= False):\n    # Divide in training\/validation and test data\n    train_df, test_df = train.copy(), test.copy()\n    print(\"Start Training. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    gc.collect()\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=47)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in [TARGET, ID]]\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[TARGET])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df[TARGET].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[TARGET].iloc[valid_idx]\n        # LightGBM parameters found by Bayesian optimization\n        clf = CLF['class'](**CLF['params'])\n        if CLF['eval_set']:\n            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n                eval_metric= 'auc', verbose= 1000, early_stopping_rounds= 200)\n            oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n            sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits            \n        elif 'keras' in CLF:\n            clf.fit(CLF['fit_params'])\n            oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n            sub_preds += clf.predict_proba(test_df[feats])[:, 1] \/ folds.n_splits            \n        else:\n            clf.fit(train_x, train_y)\n            oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n            sub_preds += clf.predict_proba(test_df[feats])[:, 1] \/ folds.n_splits\n        if CLF['feature_importance']:\n            fold_importance_df = pd.DataFrame()\n            fold_importance_df[\"feature\"] = feats\n            fold_importance_df[\"importance\"] = clf.feature_importances_\n            fold_importance_df[\"fold\"] = n_fold + 1\n            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df[TARGET], oof_preds))\n    # Write submission file and plot feature importance\n    if not debug:\n        test_df[TARGET] = sub_preds\n        test_df[[ID, TARGET]].to_csv(CLF['submission'], index= False)\n    if CLF['feature_importance']:\n        display_importances(feature_importance_df)\n    return feature_importance_df","2230886b":"CLF = {\n    'class':GaussianNB, 'params':{},'submission':'sub_naive.csv', 'eval_set':None, 'feature_importance':False\n}","203c4346":"kfold_train(train, test, 5, CLF)","7c03cfee":"from sklearn.preprocessing import PolynomialFeatures","b65ab2af":"high_low = list(correlations.head(5).index) + list(correlations.tail(6).index)\nhigh_low.remove('target')\npoly = PolynomialFeatures(2, interaction_only=True)\npoly_train = poly.fit_transform(train[high_low])\npoly_test = poly.fit_transform(test[high_low])\nnew_train = pd.concat([train, pd.DataFrame(poly_train)], axis=1)\nnew_test = pd.concat([test, pd.DataFrame(poly_test)], axis=1)","532278b0":"CLF = {\n    'class':GaussianNB, 'params':{},'submission':'sub_naive_new.csv', 'eval_set':None, 'feature_importance':False\n}\nkfold_train(new_train, new_test, 5, CLF)","d9b97caf":"from sklearn.tree import DecisionTreeClassifier\nCLF = {\n    'class':DecisionTreeClassifier, 'params':{},'submission':'sub_dt.csv', 'eval_set':None, 'feature_importance':True\n}\nfeat_importance = kfold_train(train, test, 5, CLF)","d8e00da8":"from sklearn.tree import DecisionTreeClassifier\nCLF = {\n    'class':DecisionTreeClassifier, 'params':{},'submission':'sub_dt_new.csv', 'eval_set':None, 'feature_importance':True\n}\nfeat_importance = kfold_train(new_train, new_test, 5, CLF)","a607b020":"from lightgbm import LGBMClassifier\nCLF = {\n    'class':LGBMClassifier, 'params':{'n_estimators':10000, 'learning_rate':0.01, 'max_depth':10},'submission':'sub_lgbm.csv', 'eval_set':True, 'feature_importance':True\n}\nfeat_importance = kfold_train(train, test, 5, CLF)","e4cd1aa9":"**Wow its looks like every feature is not corelated**","e890fa8c":"# **Lets Compare KDE for Train vs Test**","60394367":"## **Normal Distribution :\/**\n## **Its Seems The Train and Test Distribution are almost perfectly matched**","670129a8":"# **Lets Compare KDE for each Target**"}}