{"cell_type":{"45019756":"code","ae6bc358":"code","20abcd58":"code","c6db3905":"code","988185b7":"code","33138b66":"code","229a53bd":"code","678b9fbb":"code","9fb01df9":"code","cc9e07b6":"code","4276d681":"code","8394b510":"code","8309d94e":"code","59905fc3":"code","92711485":"code","345c0d26":"code","7779bd0d":"code","7310bcdc":"code","4e6b0531":"code","bb1ed2ac":"code","5c039599":"code","dc221f01":"code","059b71e6":"code","d0426674":"code","23f7a107":"code","35130c39":"code","68241d02":"code","c6fb1e2e":"code","50d09291":"code","f7aaa8ff":"markdown","40ce2ee3":"markdown","65ece9b3":"markdown","ecf33c51":"markdown","4e3c4d4e":"markdown"},"source":{"45019756":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)","ae6bc358":"train = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/sampleSubmission.csv', sep=\",\")","20abcd58":"train.head(10)","c6db3905":"train.loc[train.SentenceId == 2]","988185b7":"print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))","33138b66":"print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))","229a53bd":"print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))","678b9fbb":"text = ' '.join(train.loc[train.SentenceId == 4, 'Phrase'].values)\ntext = [i for i in ngrams(text.split(), 3)]","9fb01df9":"Counter(text).most_common(20)","cc9e07b6":"tokenizer = TweetTokenizer()","4276d681":"vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)\nvectorizer.fit(full_text)\ntrain_vectorized = vectorizer.transform(train['Phrase'])\ntest_vectorized = vectorizer.transform(test['Phrase'])","8394b510":"y = train['Sentiment']","8309d94e":"logreg = LogisticRegression()\novr = OneVsRestClassifier(logreg)","59905fc3":"%%time\novr.fit(train_vectorized, y)","92711485":"scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))","345c0d26":"%%time\nsvc = LinearSVC(dual=False)\nscores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))","7779bd0d":"ovr.fit(train_vectorized, y);\nsvc.fit(train_vectorized, y);","7310bcdc":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","4e6b0531":"tk = Tokenizer(lower = True, filters='')\ntk.fit_on_texts(full_text)","bb1ed2ac":"train_tokenized = tk.texts_to_sequences(train['Phrase'])\ntest_tokenized = tk.texts_to_sequences(test['Phrase'])","5c039599":"max_len = 50\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_test = pad_sequences(test_tokenized, maxlen = max_len)","dc221f01":"embedding_path = \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"","059b71e6":"embed_size = 300\nmax_features = 20000","d0426674":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","23f7a107":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1, 1))","35130c39":"file_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\ndef build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n    \n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(0.1)(Dense(128,activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(0.1)(Dense(64,activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 10, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model","68241d02":"model = build_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.5)","c6fb1e2e":"pred = model.predict(X_test, batch_size = 1024, verbose = 1)","50d09291":"predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n# for blending if necessary.\n#(ovr.predict(test_vectorized) + svc.predict(test_vectorized) + np.round(np.argmax(pred, axis=1)).astype(int)) \/ 3\nsub['Sentiment'] = predictions\nsub.to_csv(\"blend.csv\", index=False)","f7aaa8ff":"Let's see for example most common trigrams for positive phrases","40ce2ee3":"## Deep learning\nAnd now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition.","65ece9b3":"So, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n- using stopwords can be a bad idea, especially when phrases contain one single stopword;\n- puntuation could be important, so it should be used;\n- ngrams are necessary to get the most info from data;\n- using features like word count or sentence length won't be useful;","ecf33c51":"### Thoughts on feature processing and engineering","4e3c4d4e":"Well... I don't think that they are really positive."}}