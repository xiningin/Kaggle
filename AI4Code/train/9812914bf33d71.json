{"cell_type":{"7370215c":"code","2495e633":"code","cf0434ab":"code","b6600fe7":"code","53d8cc47":"code","ec0baf08":"code","239c813f":"code","b2bbfd66":"code","85d9419c":"code","46bf90c9":"code","c9d520bc":"code","77443fb7":"code","9e42694b":"code","4dcebcf8":"code","a57ad154":"code","35d42124":"code","549f8ac7":"code","d4a7efc1":"code","860b7f9b":"code","c4703f5a":"code","f083ddc3":"code","19ebc995":"code","4826471c":"code","ab69e3a1":"code","98dff5e1":"code","8b6622da":"code","9fe6c1c9":"code","697dce6a":"code","f7d3c146":"code","c60daa21":"code","7d12281c":"code","ccad6644":"code","d902e971":"code","96287b4c":"code","feee13c5":"code","546f783b":"code","b3d37f8f":"code","6352552e":"code","b9a2bfae":"code","a492b36e":"code","4ab78b99":"code","bab717d3":"code","ac23b1b5":"code","0be6f0a1":"code","6a7a8866":"code","b97fb2b7":"code","b1058095":"code","7dcde189":"code","cf37d0fe":"code","15292b33":"code","bcd28a0d":"code","76c11e54":"code","04f1ae50":"code","ffedba6f":"code","629ef8ec":"code","383df620":"code","d42066b6":"code","fc00d95b":"code","039a9990":"code","1b3d89d8":"code","89f85c64":"code","33c487b3":"code","1eab599e":"code","6ec528cc":"code","4c8c57ff":"code","912679b8":"code","a3aaa1d4":"code","3acbf842":"code","004a8447":"code","3fe93526":"code","b6bed33a":"code","d93ab232":"code","d5e5c3f9":"code","42907aff":"code","e9b842c5":"code","2dac1fe2":"markdown","97defe0b":"markdown","3a943100":"markdown","1d573d69":"markdown","c720d22f":"markdown","92421dcd":"markdown","3d43f3ff":"markdown","63a2639d":"markdown","b9582ca0":"markdown","27ced603":"markdown","0bdc9e25":"markdown","3dbe13f3":"markdown","37cf439a":"markdown","cd4d1d05":"markdown","ed454590":"markdown","ffca456c":"markdown","11b3c039":"markdown","9e4d1132":"markdown","01131855":"markdown","e3d736b5":"markdown","50c05dce":"markdown"},"source":{"7370215c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score, plot_roc_curve, accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-dark')\n","2495e633":"#import the rookies dataset\nrookies_original = pd.read_excel(\"..\/input\/nba-rookies-stats\/NBA_Rookies_by_Year.xlsx\")\nrookies = rookies_original[rookies_original[\"Year Drafted\"] < 2016]\nrookies.index = range(0, len(rookies.index)) \nrookies.head()","cf0434ab":"rookies","b6600fe7":"#import players dataset\nplayers_all = pd.read_csv(\"..\/input\/nba-players-stats-19802017\/player_df.csv\")\nplayers_all = players_all.drop(players_all.columns[0], axis=1)\nplayers_all.head()","53d8cc47":"#dropping columns with irregularities\nplayers_all = players_all.drop([\"G\",\"OWS\",\"BPM\",\"FG%\",\"2P\",\"FT\",\"DRB\",\"BLK\"], axis=1)\nplayers_all.head()","ec0baf08":"#converting year column to int\nplayers_all = players_all.astype({\"Year\":int})\nplayers_all","239c813f":"#we can disregard rookies drafted after 2013 because the players dataset only goes up to 2017\nrookies = rookies[rookies[\"Year Drafted\"] < 2014]\nrookies","b2bbfd66":"#storing rookie info in a dictionary\nrkeys_list = list(rookies.loc[:, \"Name\"])\nrval_list = list(rookies.loc[:, \"Year Drafted\"])\nrookie_dict = {k:v for k,v in zip(rkeys_list, rval_list)}","85d9419c":"#function that groups active players in a list based on the year\ndef active_players(year):\n    players_year = players_all[players_all[\"Year\"] == year]\n    players_year = list(players_year.loc[:, \"Player\"])\n    players_year = [s.strip('*') for s in players_year]\n    return players_year\n\n#creating a 2D list where one dimension is the year and the other dimension is the active players\nplayers_by_year = [[None]] * 38\ni=0\nyear = 1980\nfor year in range(1980, 2018):\n    players = active_players(year)\n    players_by_year[i] = players\n    i+=1","46bf90c9":"#storing active player info in a dictionary where the key is the year and the value is the active players during that yera#\n\n#keys\nkeys_list = [year for year in range(1980,2018)]\n\n#creating dictionary\nplayers_dict = {k:v for k,v in zip(keys_list, players_by_year)}","c9d520bc":"#creating list of players that spent at least 5 years in the league\nfivyrs = []\nfor player, rookie_year in rookie_dict.items():\n    target_year = rookie_year + 4\n    if player in players_dict[target_year]:\n        fivyrs.append(player)","77443fb7":"#creating the target column by comparing fivyrs to rookie_dict\ntarget_col = [None]*1424\nrookie_names = list(rookies.loc[:, \"Name\"])\ni = 0\nfor rookie in rookie_names:\n    if rookie in fivyrs:\n        target_col[i] = 1\n    else:\n        target_col[i] = 0\n    i+=1\ntarget_col = np.array(target_col)\nprint(target_col)","9e42694b":"#adding the target column to the dataframe\ntarget_col = pd.DataFrame(data=target_col, index=[i for i in range(0,len(rookies.index))], columns=[\"target\"])\nrookies.index = range(0,len(rookies.index))\nrookies[\"target\"] = target_col.loc[:, \"target\"]","4dcebcf8":"rookies","a57ad154":"pd.set_option('display.max_columns', None)\nrookies.head(10)","35d42124":"rookies.tail(10)","549f8ac7":"#Let's find out how many of each class there is\nrookies[\"target\"].value_counts()","d4a7efc1":"#Let's visualize this distribution\nrookies[\"target\"].value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"])","860b7f9b":"#Deleting the name column\nrookies = rookies.drop([\"Name\"], axis=1)\nrookies","c4703f5a":"#General description of data\nrookies.describe()","f083ddc3":"#compare target column with year\nyr_series = pd.Series(rookies.loc[:, \"Year Drafted\"])\ntarget_series = pd.Series(rookies.loc[:, \"target\"])\npd.crosstab(target_series, yr_series)","19ebc995":"#visualizing this info\npd.crosstab(yr_series, target_series).plot(kind=\"bar\", figsize=(10,7), color=[\"salmon\", \"lightblue\"])\nplt.title(\"5yr Survival By Year\")\nplt.ylabel(\"Count\")","4826471c":"rookies.head()","ab69e3a1":"#PTS Distribution\nrookies[\"PTS\"].plot(kind=\"hist\")","98dff5e1":"#MIN Distribution\nrookies[\"MIN\"].plot(kind=\"hist\")","8b6622da":"#FG% Distribution\nrookies[\"FG%\"].plot(kind=\"hist\")","9fe6c1c9":"#3P% Distrbution\nrookies[\"GP\"].plot(kind=\"hist\")","697dce6a":"rookies.head()","f7d3c146":"#Correlaton matrix\ncorr_matrix = rookies.corr()\nfig, ax = plt.subplots(figsize=(16,10))\nax = sns.heatmap(corr_matrix, annot=True, linewidths=0.5, fmt=\".2f\", cmap=\"YlGnBu\")","c60daa21":"#Cleaning the 3P% column\nrookies[\"3P%\"] = rookies[\"3P%\"].map(lambda x:0 if x==\"-\" else x)","7d12281c":"#Creating Matrix of Features\nX = rookies.drop([\"target\"], axis = 1)","ccad6644":"X","d902e971":"#creating target column\ny = rookies.loc[:, \"target\"]\ny","96287b4c":"rookies.dtypes","feee13c5":"# Splitting into training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","546f783b":"X_train","b3d37f8f":"# Models dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(),\n         \"KNN\": KNeighborsClassifier(),\n         \"Random Forest\": RandomForestClassifier(),\n         \"XGBoost\": XGBClassifier()}\n\n#Function that will evaluate the model performance using various metrics\ndef evaluate_pred(y_pred, y_test):\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    metric_dict = {\"accuracy\": round(accuracy, 2), \"precision\": round(precision, 2), \"recall\": round(recall, 2),\n                  \"f1\": round(f1,2)}\n    print(f\"Accuracy: {accuracy*100:.2f}%\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"F1 score: {f1:.2f}\")\n    \n    return metric_dict\n\n# Function that will fit and score the models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    #Dictionary of model scores\n    model_scores = {}\n    \n    #Loop through models\n    for name, model in models.items():\n        clf = model\n        clf.fit(X_train, y_train)\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","6352552e":"model_scores = fit_and_score(models, X_train, X_test, y_train, y_test)\nmodel_scores","b9a2bfae":"# Create hyperparameter options\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n\n# Apply grid search\nlog_clf = GridSearchCV(LogisticRegression(), grid, cv=5, verbose=0)\n\n#Fit\nlog_clf.fit(X_train, y_train)","a492b36e":"#print the best estimator\nlog_clf.best_estimator_","4ab78b99":"#evaluating the performance of the best estimator\nlog_clf1 = LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\nlog_clf1.fit(X_train, y_train)\ny_pred = log_clf1.predict(X_test)\naccuracy_score(y_pred, y_test)","bab717d3":"#negligible increase in accuracy","ac23b1b5":"#Constructing the grid\nparam_test1 = {\n 'n_estimators':range(50,200,10),\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\n\n#Apply grid search\nxg_clf = GridSearchCV(XGBClassifier(), param_test1, cv=5, verbose=0)\nxg_clf.fit(X_train, y_train)","0be6f0a1":"#Print best estimator\nxg_clf.best_estimator_","6a7a8866":"#evaluating the performance of the best estimator\nxg_clf1 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=7,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=120, n_jobs=0, num_parallel_tree=1,\n              objective='binary:logistic', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\nxg_clf1.fit(X_train, y_train)\ny_pred = xg_clf1.predict(X_test)\naccuracy_score(y_pred, y_test)","b97fb2b7":"#decrease in accuracy","b1058095":"#Desired range for k parameter\nk_range = list(range(19, 50))\n\n#Creating grid\nparam_grid = dict(n_neighbors=k_range)\n\n#Applying GridSearchCV\nknn_clf = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, scoring='accuracy')\nknn_clf.fit(X, y)","7dcde189":"#printing best estimator\nknn_clf.best_estimator_","cf37d0fe":"#evaluating the performance of the best estimator\nknn_clf1 = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=37, p=2,\n                     weights='uniform')\nknn_clf1.fit(X_train, y_train)\ny_pred = knn_clf1.predict(X_test)\nprint(accuracy_score(y_pred, y_test))","15292b33":"#6% increase in accuracy achieved","bcd28a0d":"#Creating the grid\nparam_grid = {\n    'n_estimators'      : range(50,200,10),\n    'max_depth'         : [8, 9, 10, 11, 12],\n    'random_state'      : [0],\n    #'max_features': ['auto'],\n    #'criterion' :['gini']\n}\n\n#Applying grid search\ncv_rfc = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv= 10, scoring='accuracy')\ncv_rfc.fit(X_train, y_train)","76c11e54":"#printing best estimator\ncv_rfc.best_estimator_","04f1ae50":"# #evaluating performance of best estimator\nrfc = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=9, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=340,\n                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n                       warm_start=False)\n\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\naccuracy_score(y_pred, y_test)","ffedba6f":"#negligible increase in accuracy","629ef8ec":"#Function that creates visualization for confusion matrix\nsns.set(font_scale=1.0)\n\ndef plot_conf_mat(y_test, y_preds):\n    fig, ax = plt.subplots(figsize=(4, 4))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True,\n                     cbar=False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")","383df620":"#function that calculates classification metrics using cross validation\ncv_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\ndef cv_calculator(cv_metrics, clf, X, y):\n    cv_dict = {}\n    for metric in cv_metrics:\n        cv_dict[metric] = np.mean(cross_val_score(clf, X, y, cv=5, scoring=metric))\n    return cv_dict","d42066b6":"#Plot ROC Curve and calculate AUC for XGB\nplot_roc_curve(xg_clf1, X_test, y_test)","fc00d95b":"#confusion matrix for XGB\ny_pred1 = xg_clf1.predict(X_test)\nplot_conf_mat(y_pred1, y_test)","039a9990":"#cross validated classification metrics for XGB\ncv_dict = cv_calculator(cv_metrics, xg_clf1, X, y)\ncv_dict","1b3d89d8":"#visualize the cv metrics\ncv_metrics1 = pd.DataFrame(cv_dict, index=[\"score\"])\ncv_metrics1.T.plot.bar(title=\"XGB CV Metrics\", legend=False)","89f85c64":"#feature importance XGB\nplt.figure(figsize=(15, 5))\nplt.bar(list(X_train.columns), xg_clf1.feature_importances_, align='edge', width=0.3)\nplt.show()","33c487b3":"#Plot ROC Curve and calculate AUC for Logistic Regression\nplot_roc_curve(log_clf1, X_test, y_test)","1eab599e":"#confusion matrix for Log Reg\ny_pred2 = log_clf1.predict(X_test)\nplot_conf_mat(y_pred2, y_test)","6ec528cc":"#cross validated classification metrics for Log Reg\ncv_dict2 = cv_calculator(cv_metrics, log_clf, X, y)\ncv_dict2","4c8c57ff":"#visualize the cv metrics\ncv_metrics2 = pd.DataFrame(cv_dict2, index=[\"score\"])\ncv_metrics2.T.plot.bar(title=\"Log Reg CV Metrics\", legend=False)","912679b8":"#feature importance log reg#\n\n#Match coefficients to corresponding columns\nfeature_dict = dict(zip(rookies.columns, list(log_clf1.coef_[0])))\n\n#Visualize feature importance\nplt.figure(figsize=(15, 5))\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\", legend=False)","a3aaa1d4":"#Plot ROC Curve and calculate AUC KNeighbors\nplot_roc_curve(knn_clf1, X_test, y_test)","3acbf842":"#confusion matrix for KNN\ny_pred3 = knn_clf1.predict(X_test)\nplot_conf_mat(y_pred3, y_test)","004a8447":"#cross validated classification metrics for KNN\ncv_dict3 = cv_calculator(cv_metrics, knn_clf1, X, y)\ncv_dict3","3fe93526":"#visualize the cv metrics\ncv_metrics3 = pd.DataFrame(cv_dict3, index=[\"score\"])\ncv_metrics3.T.plot.bar(title=\"KNN CV Metrics\", legend=False)","b6bed33a":"#Plot ROC Curve and calculate AUC for Random Forest\nplot_roc_curve(rfc, X_test, y_test)","d93ab232":"#confusion matrix for RFC\ny_pred4 = rfc.predict(X_test)\nplot_conf_mat(y_pred4, y_test)","d5e5c3f9":"#cross validated classification metrics RF\ncv_dict4 = cv_calculator(cv_metrics, rfc, X, y)\ncv_dict4","42907aff":"#visualize the cv metrics\ncv_metrics4 = pd.DataFrame(cv_dict4, index=[\"score\"])\ncv_metrics4.T.plot.bar(title=\"Random Forest CV Metrics\", legend=False)","e9b842c5":"#feature importance for random forest#\n\n#creating feature importance dictionary\nfeatures_dict2 = dict(zip(rookies.columns , rfc.feature_importances_))\n\n#visualizing feature importance\nplt.figure(figsize=(15, 5))\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Random Forest Feature Importance\", legend=False)","2dac1fe2":"# 1. Problem Definition and Data Introduction\n\n## Problem Definition\nThe NBA is the world's premier basketball league and as such, the competition for admission into the league is fierce; only about 1% of NCAA College Basketball players get drafted into the NBA. In order to remain in the league, newly drafted players must continue to prove their worth on the court. This notebook will use various rookie year stats to predict whether or not a player will last five years in the league.\n\n## Data Introduction\nThis project will use data from two datasets: a rookies dataset and an active players dataset. The rookies dataset includes all of the rookies drafted between 1980 and 2015. The active players dataset lists the active players during each seeason from 1980 to 2017. Both datasets were taken from data.world. The active players dataset will be used to create a target column in the rookies dataset, which will then be used to model.\n\n\n## Rookies Data Dictionary\nThe following are the rookie year statistics that will be used to predict whether or not a player lasts 5 years in thee league:\n'Year Drafted', 'GP', 'MIN', 'PTS', 'FGM', 'FGA', 'FG%',\n       '3P Made', '3PA', '3P%', 'FTM', 'FTA', 'FT%', 'OREB', 'DREB', 'REB',\n       'AST', 'STL', 'BLK', 'TOV', 'EFF', 'target'\n      \n1. Year Drafted\n2. GP: games played during rookie season\n3. MIN: average minutes played per game\n4. PTS: average points per game\n5. FGM: average field goals made per game\n6. FGA: average field goals attempted per game\n7. FG%: average field goal percentage\n8. 3P Made: average 3-point field goals made per game\n9. 3PA: average 3-point field goals attempted per game\n10. 3P%: 3-point percentage\n11. FTM: average free throws made per game\n12. FTA: average free throws attempted per game\n13. FT%: free throw percentage\n14. OREB: average offensive rebounds per game\n15. DREB: average defensive rebound per game\n16. REB: average total rebounds per game\n17. AST: average assists per game\n18. BLK: average blocks per game\n19. TOV: average turnovers per game\n20. EFF: a player's efficiency; EFF = (PTS + REB + AST + STL + BLK - Missed FG \u2212 Missed FT - TO) \/ GP","97defe0b":"## Cleaning the Rookies Dataset","3a943100":"# 4. Modelling ","1d573d69":"## Using Machine Learning to Predict the Longevity of NBA Rookies\nThis notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not an NBA rookie will last 5 years in the league based on their rookie year statistics.\n\nHere is a brief outline of the notebook:\n    1. Problem definition and Data Introduction\n    2. Data Wrangling\n    3. Exploratory Data Analysis \n    4. Modelling\n    5. Evaluation\n    6. Conclusion","c720d22f":"# 2. Data Wrangling","92421dcd":"## Random Forest Tuning","3d43f3ff":"With cross-validated accuracies of around 69%, the logistic regression, KNN, and random forest models seem to perform the best. This low accuracy is due in large part to the fact that the training data does not include any information about injuries, which are a critical determinant of a rookie's longevity. Despite this low accuracy, all four models identify a rookie's efficiency as the most important determinant of longevity.","63a2639d":"## KNN Tuning","b9582ca0":"# 3. Exploratory Data Analysis","27ced603":"## XGBoost Evaluation","0bdc9e25":"# 6. Conclusion","3dbe13f3":"## Prepare the tools","37cf439a":"## Cleaning the Active Players Dataset\n","cd4d1d05":"## XGBoost Tuning","ed454590":"## Random Forest Evaluation","ffca456c":"Feature importance is not explicitly defined for the KNN algorithm","11b3c039":"\n# Hyperparameter Tuning with GridSearchCV\nWe will use GridSearchCV to try and improve the performance of these models\n    1. Logistic Regression Tuning\n    2. XGBoost Tuning\n    3. KNN Tuning\n\n## Logistic Regression Tuning","9e4d1132":"## Creating a Target Column (whether or not the rookie lasted 5 years)","01131855":"## Logistic Regression Evaluation","e3d736b5":"# 5. Evaluation\nFor each model we will look at:\n1. ROC curve and AUC score\n2. Confusion matrix\n3. Classification report\n4. Precision\n5. Recall\n6. F1 Score\n7. Feature Importance","50c05dce":"## KNN Evaluation "}}