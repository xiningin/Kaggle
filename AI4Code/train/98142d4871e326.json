{"cell_type":{"25c744d2":"code","795bef5a":"code","09d4207c":"code","a6f15761":"code","b6989c29":"code","5ecb7ec7":"code","bfe2917c":"code","ec5cfd9c":"code","15cd469f":"code","cb327b95":"code","77bf217e":"code","b6156ab5":"code","5acbf43e":"code","dc038b5b":"markdown","84dc53a4":"markdown","fb1d4a15":"markdown"},"source":{"25c744d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","795bef5a":"train_data = pd.read_csv(\"..\/input\/data-science-london-scikit-learn\/train.csv\",header = None)\ntest_data =pd.read_csv(\"..\/input\/data-science-london-scikit-learn\/test.csv\",header = None)\ntrainLabel = pd.read_csv(\"..\/input\/data-science-london-scikit-learn\/trainLabels.csv\",header = None)","09d4207c":"train_data.head()","a6f15761":"train_data.shape , test_data.shape , trainLabel.shape","b6989c29":"from sklearn.model_selection import train_test_split \nx_train, x_test , y_train ,y_test = train_test_split(train_data , trainLabel,test_size=.33 ,random_state=42)\n","5ecb7ec7":"#naive_bayes\nfrom sklearn.metrics import accuracy_score ,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train,y_train.values.ravel())\npredictor = classifier.predict(x_test)\nprint(\"Naive Bauyes\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n##knn\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier()\nclassifier.fit(x_train , y_train.values.ravel())\npredictor=  classifier.predict(x_test)\nprint(\"KNN\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n## Random Forest \n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc_model = RandomForestClassifier(n_estimators = 150 , random_state =42)\nrfc_model.fit(x_train ,y_train.values.ravel())\npredictor = rfc_model.predict(x_test)\n\nprint(\"\u064cRadom Forest\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n##logist regression \n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(solver=\"sag\")\nclassifier.fit(x_train , y_train.values.ravel())\npredictor = classifier.predict(x_test)\n\nprint(\" Logistic Rg\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n\n## SVC\nfrom sklearn.svm import SVC\npredictor = SVC(gamma=\"auto\").fit(x_train , y_train.values.ravel()).predict(x_test)\nprint(\" SVC\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n####Decision tree \nfrom sklearn.tree import DecisionTreeClassifier\npredictor  = DecisionTreeClassifier().fit(x_train,y_train.values.ravel()).predict(x_test)\n\nprint(\" Decision tree\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n\n\n### XGBoost\nfrom xgboost import XGBClassifier\npredictor  = XGBClassifier().fit(x_train,y_train.values.ravel()).predict(x_test)\n\nprint(\" XGBoost\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))","bfe2917c":"from sklearn.preprocessing import StandardScaler ,Normalizer\nscaler = Normalizer()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","ec5cfd9c":"#naive_bayes\nfrom sklearn.metrics import accuracy_score ,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train,y_train.values.ravel())\npredictor = classifier.predict(x_test)\nprint(\"Naive Bauyes\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n##knn\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier()\nclassifier.fit(x_train , y_train.values.ravel())\npredictor=  classifier.predict(x_test)\nprint(\"KNN\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n## Random Forest \n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc_model = RandomForestClassifier(n_estimators = 150 , random_state =42)\nrfc_model.fit(x_train ,y_train.values.ravel())\npredictor = rfc_model.predict(x_test)\n\nprint(\"\u064cRadom Forest\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n##logist regression \n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(solver=\"sag\")\nclassifier.fit(x_train , y_train.values.ravel())\npredictor = classifier.predict(x_test)\n\nprint(\" Logistic Rg\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n\n## SVC\nfrom sklearn.svm import SVC\npredictor = SVC(gamma=\"auto\").fit(x_train , y_train.values.ravel()).predict(x_test)\nprint(\" SVC\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n####Decision tree \nfrom sklearn.tree import DecisionTreeClassifier\npredictor  = DecisionTreeClassifier().fit(x_train,y_train.values.ravel()).predict(x_test)\n\nprint(\" Decision tree\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n\n\n### XGBoost\nfrom xgboost import XGBClassifier\npredictor  = XGBClassifier().fit(x_train,y_train.values.ravel()).predict(x_test)\n\nprint(\" XGBoost\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))","15cd469f":"from sklearn.decomposition import PCA \npca = PCA(n_components=12)\nx_train = pca.fit_transform(x_train)\nx_test = pca.transform(x_test)","cb327b95":"#naive_bayes\nfrom sklearn.metrics import accuracy_score ,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train,y_train.values.ravel())\npredictor = classifier.predict(x_test)\nprint(\"Naive Bauyes\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n##knn\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier()\nclassifier.fit(x_train , y_train.values.ravel())\npredictor=  classifier.predict(x_test)\nprint(\"KNN\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n## Random Forest \n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc_model = RandomForestClassifier(n_estimators = 150 , random_state =42)\nrfc_model.fit(x_train ,y_train.values.ravel())\npredictor = rfc_model.predict(x_test)\n\nprint(\"\u064cRadom Forest\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n##logist regression \n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(solver=\"sag\")\nclassifier.fit(x_train , y_train.values.ravel())\npredictor = classifier.predict(x_test)\n\nprint(\" Logistic Rg\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n\n## SVC\nfrom sklearn.svm import SVC\npredictor = SVC(gamma=\"auto\").fit(x_train , y_train.values.ravel()).predict(x_test)\nprint(\" SVC\" , accuracy_score(y_test,predictor))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n####Decision tree \nfrom sklearn.tree import DecisionTreeClassifier\npredictor  = DecisionTreeClassifier().fit(x_train,y_train.values.ravel()).predict(x_test)\n\nprint(\" Decision tree\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))\n\n\n\n\n### XGBoost\nfrom xgboost import XGBClassifier\npredictor  = XGBClassifier().fit(x_train,y_train.values.ravel()).predict(x_test)\n\nprint(\" XGBoost\" , accuracy_score(predictor,y_test))\nprint(\"confusion matrix\" , confusion_matrix(predictor,y_test))","77bf217e":"# USING THE GAUSSIAN MIXTURE MODEL \nfrom sklearn.mixture import GaussianMixture\nx_all = np.r_[train_data,test_data]\nprint('x_all shape :',x_all.shape)\nlowest_bic = np.infty\n\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n        gmm.fit(x_all)\n        bic.append(gmm.aic(x_all))\n        \n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n            \n            \nbest_gmm.fit(x_all)\ngmm_train = best_gmm.predict_proba(train_data)\ngmm_test = best_gmm.predict_proba(test_data)","b6156ab5":"from sklearn.model_selection import GridSearchCV\n#Random Forest Classifier\nrfc = RandomForestClassifier(random_state=99)\n\n#USING GRID SEARCH\nn_estimators = [10, 50, 100, 200,400]\nmax_depth = [3, 10, 20, 40]\nparam_grid = dict(n_estimators=n_estimators,max_depth=max_depth)\n\ngrid_search_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv = 10,scoring='accuracy',n_jobs=-1).fit(gmm_train, trainLabel.values.ravel())\nrfc_best = grid_search_rfc.best_estimator_\nprint('Random Forest Best Score',grid_search_rfc.best_score_)\nprint('Random Forest Best Parmas',grid_search_rfc.best_params_)\nprint('Random Forest Accuracy',cross_val_score(rfc_best,gmm_train, trainLabel.values.ravel(), cv=10).mean())\n\n#KNN \nknn = KNeighborsClassifier()\n\n#USING GRID SEARCH\nn_neighbors=[3,5,6,7,8,9,10]\nparam_grid = dict(n_neighbors=n_neighbors)\n\ngrid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv = 10, n_jobs=-1,scoring='accuracy').fit(gmm_train,trainLabel.values.ravel())\nknn_best = grid_search_knn.best_estimator_\nprint('KNN Best Score', grid_search_knn.best_score_)\nprint('KNN Best Params',grid_search_knn.best_params_)\nprint('KNN Accuracy',cross_val_score(knn_best,gmm_train, trainLabel.values.ravel(), cv=10).mean())\n\n#SVM\nsvc = SVC()\n\n#USING GRID SEARCH\nparameters = [{'kernel':['linear'],'C':[1,10,100]},\n              {'kernel':['rbf'],'C':[1,10,100],'gamma':[0.05,0.0001,0.01,0.001]}]\ngrid_search_svm = GridSearchCV(estimator=svc, param_grid=parameters, cv = 10, n_jobs=-1,scoring='accuracy').fit(gmm_train, trainLabel.values.ravel())\nsvm_best = grid_search_svm.best_estimator_\nprint('SVM Best Score',grid_search_svm.best_score_)\nprint('SVM Best Params',grid_search_svm.best_params_)\nprint('SVM Accuracy',cross_val_score(svm_best,gmm_train, trainLabel.values.ravel(), cv=10).mean())","5acbf43e":"rfc_best.fit(gmm_train,trainLabel.values.ravel())\npred  = rfc_best.predict(gmm_test)\nrfc_best_pred = pd.DataFrame(pred)\n\nrfc_best_pred.index += 1\n\nrfc_best_pred.columns = ['Solution']\nrfc_best_pred['Id'] = np.arange(1,rfc_best_pred.shape[0]+1)\nrfc_best_pred = rfc_best_pred[['Id', 'Solution']]\n\n\nrfc_best_pred.to_csv('Submission.csv',index=False)","dc038b5b":"# ****pre processing \n","84dc53a4":"the best are knn , random forest and xgboost ","fb1d4a15":"# **classification**"}}