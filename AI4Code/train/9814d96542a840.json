{"cell_type":{"279f5657":"code","442bf16e":"code","f02e9a82":"code","c4c235b7":"code","0a54258a":"code","f145d627":"code","4be5d8f5":"code","174fe799":"code","c8718deb":"code","a09b8e3a":"code","629ca665":"code","cb854c64":"code","68c9c28d":"code","dc2dcea6":"code","0d6fc5e3":"code","b579f16d":"code","c26fb88c":"code","1efe3d6e":"code","bb02adb7":"code","11ae64ee":"code","160dcf4e":"code","1b88ed46":"code","a9733b09":"code","83a138ed":"code","788548a5":"code","0930e3eb":"code","db27dd39":"code","a93f6c0f":"code","278e972f":"code","20c1e3b1":"code","51fca7a8":"markdown","f346dbf7":"markdown","6f0c7297":"markdown","e39fee28":"markdown","28cb5588":"markdown","fd53a091":"markdown","eca00889":"markdown","d6041b07":"markdown","9ae3803e":"markdown","8bbd848f":"markdown"},"source":{"279f5657":"# Main\nimport pandas as pd\nimport numpy as np\n\n# EDA and Data Vizualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot, plot\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\n\n# Feature Engineering\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, scale, RobustScaler, robust_scale, OrdinalEncoder\nfrom scipy.stats import skewtest, skew, shapiro, boxcox\nfrom sklearn.decomposition import PCA\n\n# Machine Learning\nfrom IPython.display import display, Markdown\nfrom sklearn.svm import LinearSVR, SVR\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\nfrom sklearn.base import clone\nfrom tqdm import tqdm\n\nprint('Imported Successfully!')","442bf16e":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf = pd.concat([train,submission]).set_index('Id')\nprint(f'df shape: {df.shape}\\ntrain shape: {train.shape}\\ntest shape: {submission.shape}')","f02e9a82":"print(f'train.csv: \\n{(df.memory_usage(deep=True)\/(10**6)).sort_values(ascending=False)}\\n\\ntest.csv: \\n{(submission.memory_usage(deep=True)\/(10**6)).sort_values(ascending=False)}')","c4c235b7":"eda,_=train_test_split(df, test_size=.2, random_state=42)\neda.shape","0a54258a":"# Creat price categories\nlabels=['Lower','Medium','Higher']\neda.loc[:,'price_cat'] = pd.qcut(eda.loc[:,'SalePrice'], 3, labels=labels)\n\n# Aplly log transformation to target for getting normalized results\neda['SalePrice_log'] = np.log(eda['SalePrice'])\n\n# EDA\ndef _inspect_cat_col(col):\n    _print_missing_values(col)\n    print(f'Count of each category for {col}:\\n')\n    print(eda[col].value_counts())\n    \ndef _print_missing_values(col):\n    print(f'Number of missing values: {eda[col].isnull().sum()}, Percentage: {eda[col].isnull().sum()\/len(eda[col].isnull())}\\n')\n\ndef _draw_pie_chart(col):\n    pd.set_option('mode.chained_assignment', None)\n    labels=['Lower','Medium','Higher']\n    \n\n    fig = make_subplots(rows=1, cols=3, specs=[[{'type':'domain'}, {'type':'domain'},{'type':'domain'}]], subplot_titles = labels)\n    for i,lb in enumerate(labels):\n        eda_price_labeled = eda[eda.price_cat==lb]\n        eda_price_counted = pd.DataFrame(eda_price_labeled.groupby(col)[col].count()).rename(columns={\n            col:'Count'}).reset_index()\n        fig.add_trace(go.Pie(values=eda_price_counted.Count, labels=eda_price_counted[col], name=lb),1,i+1)\n    fig.update_layout(title_text=f'{col} class distribution for each sale price bins')\n    iplot(fig)\n    \ndef _draw_violin_chart(col,price_log=False):\n    price_column_name = 'SalePrice_log' if price_log else 'SalePrice'\n    fig = go.Figure()\n\n    for cat in eda[col].unique():\n        fig.add_trace(go.Violin(x=eda[col][eda[col] == cat],\n                                y=eda[price_column_name][eda[col] == cat],\n                                name=str(cat),\n                                box_visible=True,\n                                meanline_visible=True))\n\n    iplot(fig)\n    \ndef _draw_pairplot(col,log_price=False,log_col=False, reciprocal_col=False):\n    price_column_name = 'SalePrice_log' if log_price else 'SalePrice'\n    if log_col:\n        log_column_name = col + '_log'\n        eda[log_column_name] = np.log(eda[col])\n        col = log_column_name\n    \n    ax = sns.pairplot(eda.loc[:,[col,price_column_name]], height=4, aspect=1.5, kind='reg')\n    ax.fig.suptitle(f'{col} VS {price_column_name}', y=1.05)\n    _print_missing_values(col)\n    print(f\"Correlation: {eda.loc[:,[col,price_column_name]].corr().iloc[0,1]}\")\n    plt.show()\n    \ndef _create_distplot_log(eda,col):\n    fig = ff.create_distplot([eda[col]],[col])\n    fig2 = ff.create_distplot([np.log(eda[col])],[col+'_log'])\n    iplot(fig)\n    print('After log tranformation')\n    iplot(fig2)\n\ndef automated_eda_report(eda):\n    for ind, current_col in enumerate(list(eda)):\n        if current_col in ['SalePrice','SalePrice_log','price_cat']:\n            continue\n        print(f'Current column is {current_col}')\n        print(f'Type of current columns is {label_map[current_col]}')\n        if label_map[current_col] == 'Categorical':\n            _inspect_cat_col(current_col)\n            _draw_pie_chart(current_col)\n            _draw_violin_chart(current_col)\n        elif label_map[current_col] == 'Ordinal Numerical' or label_map[current_col] == 'Ordinal Categorical':\n            print(f'Desriptive Stats Results for {current_col}:\\n')\n            print(eda[current_col].describe())\n            _draw_pairplot(current_col,log_price=True)\n            \n# Feature Engineering\n\ndef _print_done(col, trans):\n    print(f'PERFECTLY DONE! --> {trans} applied on {col}!')\n    \ndef _log_transformation(df_selected, col):\n    df_selected[col+'_trnsfrm'] = np.where(df_selected[col]!=0,\n                                           np.log(df_selected[col]), 0)\n    return df_selected\n\ndef _scale_log_transformation(df_selected, col):\n    df_selected[col+'_trnsfrm'] = scale(np.where(df_selected[col]!=0, \n                                                 np.log(df_selected[col]), 0))\n    return df_selected\n\ndef _scale_transformation(df_selected, col):\n    df_selected[col+'_trnsfrm'] = scale(df_selected[col])\n    return df_selected\n    \ndef _boxcox_transformation(df_selected, col):\n    if np.any(df_selected[col]==0):\n        data=df_selected[col]\n        posdata = data[data > 0]\n        bcdata, lam = boxcox(posdata)\n        if lam <= 0:\n            x = df_selected[col]\n        else:\n            x = np.empty_like(data)\n            x[data > 0] = bcdata\n            x[data == 0] = -1\/lam\n    else:\n        x,_ = boxcox(df_selected[col])\n    df_selected[col+'_trnsfrm'] = x\n    return df_selected\n\ndef _get_transformation_score(df, transformation_fuction, col):\n    df_trns=df.copy()\n    transformation_fuction(df_trns, col)\n    return shapiro(df_trns[col+'_trnsfrm'])[0]\n    \ndef _apply_best_transformation(df,col):\n    df_test = df.copy()\n    base_score = shapiro(df_test[col])[0]\n    \n    log_score = _get_transformation_score(df, _log_transformation, col)\n    scale_log_score = _get_transformation_score(df, _scale_log_transformation, col)\n    scale_score = _get_transformation_score(df, _scale_transformation, col)\n    boxcox_score = _get_transformation_score(df, _boxcox_transformation, col)\n    \n    arr = {'Do nothing':base_score,_log_transformation:log_score,\n           _scale_log_transformation:scale_log_score,\n           _scale_transformation:scale_score,\n           _boxcox_transformation:boxcox_score}\n    \n    # Select best tranformation with higher score\n    best_tranform_func = max(arr, key=arr.get)\n    if best_tranform_func == 'Do nothing':\n        return df\n    else:\n        return best_tranform_func(df, col)\n    \ndef _basic_fill(df_selected, col, cat=False):\n    df_selected[col] = df_selected.loc[:,col].fillna(df_selected.loc[:,col].value_counts().idxmax()) if cat else df_selected.loc[:,col].fillna(df_selected.loc[:,col].median())\n\ndef _custom_encoder(df_selected, col, col_type):\n    _col_df = np.array(df_selected.loc[:,col]).reshape(-1,1)\n    if col_type == 'Categorical':\n        enc = OneHotEncoder(sparse=False)\n    elif col_type == 'Ordinal Categorical':\n        enc = OrdinalEncoder()\n    else:\n        raise Exception(f'Cannot encode, check the {col} column!')\n    _col_enc = enc.fit_transform(_col_df)\n    temp = pd.DataFrame(_col_enc,index=df_selected.index)\n    temp.columns= enc.get_feature_names([col]) if col_type == 'Categorical' else [col+'_ord_trnsfrm']\n    df_selected=pd.concat([df_selected,temp], axis=1)\n    df_selected.drop(col, axis=1, inplace=True)\n    return df_selected\n\ndef automated_feature_engineering(df_or, label_map):\n    df = df_or.copy()\n    for col in list(df):\n        if col=='SalePrice':\n            continue\n        type_of_col = label_map[col]\n        _cat = True if (type_of_col == 'Categorical') or (type_of_col == 'Ordinal Categorical') else False\n        if df[col].isnull().sum() > 0:\n            _basic_fill(df, col, cat=_cat)\n        if _cat:\n            df=_custom_encoder(df, col, type_of_col)\n        elif type_of_col == 'Ordinal Numerical':\n            _apply_best_transformation(df,col)\n        else:\n            raise Exception(f'Unrecognized type of column! Please check the {col} column!')\n    return df","f145d627":"threshold_miss = len(eda)*0.75\nthreshold_distr = len(eda)*0.75\nfor col in eda:\n    if (eda[col].value_counts().iloc[0] >= threshold_distr or eda[col].isnull().sum() >= threshold_miss) and col != 'SalePrice':\n        eda.drop(col,axis=1,inplace=True)\nprint('Preview after dropping')\ndisplay(eda.head())","4be5d8f5":"def next_col(last):\n    print(f'Next column is {[eda.iloc[:,ind+1].name for ind,col in enumerate(list(eda)) if col==last][0]}')","174fe799":"print('All colum names after drop unnecessary ones:\\n')\nprint(list(eda))","c8718deb":"def label_attrs(eda, target_col):\n    col_dict={}\n    for col in list(eda):\n        if col=='price_cat':\n            continue\n        class_name=type(eda[col].iloc[0]).__name__\n        if class_name == 'str':\n            col_dict.update({col: 'Categorical'})\n        elif class_name == 'float64':\n            col_dict.update({col:'Ordinal Numerical'})\n        elif eda[col].nunique() < len(eda)*.02:\n            if np.abs(eda[col].corr(eda[target_col])) > .15:\n                col_dict.update({col:'Ordinal Categorical'})\n            else:\n                col_dict.update({col: 'Categorical'})\n        else:\n            col_dict.update({col:'Ordinal Numerical'})\n    return col_dict\n\nprint('After automated labeling:\\n')\nlabel_map=label_attrs(eda, 'SalePrice')\nprint(label_map)","a09b8e3a":"%%javascript\nIPython.OutputArea.auto_scroll_threshold = 9999;","629ca665":"plt.rcParams.update({'figure.max_open_warning': 0})\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nautomated_eda_report(eda)","cb854c64":"eda_Neighborhood=eda.loc[:,['Neighborhood','price_cat']] # Use eda for getting price_cat column\neda_Neighborhood['count']=1\neda_Neighborhood=eda_Neighborhood.groupby(['Neighborhood','price_cat']).sum()\neda_Neighborhood.fillna(0,inplace=True)\nrows=[]\nfor neighborhood, counts in eda_Neighborhood.unstack().iterrows():\n    flg_dict={\n        'Neighborhood':neighborhood,\n        'Group':counts.idxmax()[1]\n    }\n    rows.append(flg_dict)\nneighborhood_map = pd.DataFrame(rows)\nneighborhood_map.set_index('Neighborhood',inplace=True)\ndisplay(neighborhood_map.head())","68c9c28d":"df_Neighborhood=df.copy()\nfor _,group in neighborhood_map.iterrows():\n    for ind,row in df_Neighborhood.iterrows():\n        if row['Neighborhood'] == group.name:\n            df_Neighborhood.loc[ind,'Neighborhood'] = group.values[0]","dc2dcea6":"# Drop below threshold like we did in EDA\nthreshold_miss = len(df_Neighborhood)*0.75\nthreshold_distr = len(df_Neighborhood)*0.75\ndf_drop = df_Neighborhood.copy()\nfor col in df_drop:\n    if (df_drop[col].value_counts().iloc[0] >= threshold_distr or df_drop[col].isnull().sum() >= threshold_miss) and col != 'SalePrice':\n        df_drop.drop(col,axis=1, inplace=True)\nprint('Completed!\\nPreview after dropping')\ndisplay(df_drop.head())","0d6fc5e3":"import warnings\nwarnings.filterwarnings('ignore')\ndf_preprocessed=automated_feature_engineering(df_drop, label_map)","b579f16d":"submission_transformed=df_preprocessed[df_preprocessed.SalePrice.isnull()]\ndf_before_ml = df_preprocessed[~df_preprocessed.SalePrice.isnull()]\ny=df_before_ml[['SalePrice']]\nX=df_before_ml.drop('SalePrice',axis=1)","c26fb88c":"X=scale(X)\ny=np.log(y)\nX_train,X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=42)","1efe3d6e":"%%time\n\n\nregressors = [\n    LinearSVR(random_state=42),DecisionTreeRegressor(random_state=42),RandomForestRegressor(random_state=42),\n    LinearRegression(),AdaBoostRegressor(random_state=42),\n    AdaBoostRegressor(LinearRegression()),BaggingRegressor(LinearRegression(),n_estimators=50),\n    Ridge(),GradientBoostingRegressor(random_state=42),\n    Lasso(),ElasticNet(),SGDRegressor()\n]\nscores={}\n\ntqdm()\nfor reg in tqdm(regressors):\n    reg.fit(X_train, y_train)\n    preds = reg.predict(X_test)\n    scores.update({\n        reg:np.sqrt(mean_squared_error(y_test,preds))\n    }) \nfor f,sc in scores.items():\n    display(Markdown(f'RMSE score of {f}={sc}'))\nbest_reg = list(scores)[np.argmin(list(scores.values()))]\ndisplay(Markdown(f'**Best regressor is {best_reg}**'))","bb02adb7":"for model_with_cv in [RidgeCV(), LassoCV(random_state=42), ElasticNetCV(random_state=42)]:\n    model_with_cv.fit(X,y)\n    res=f'$r^2$ of {model_with_cv}={model_with_cv.score(X,y)}'\n    display(Markdown(res))","11ae64ee":"gbr = clone(best_reg)\ngbr.fit(X_train, y_train)\ngbr_preds = gbr.predict(X_test)\nr2_score(y_test, gbr_preds)","160dcf4e":"from sklearn.inspection import permutation_importance\nimp = permutation_importance(gbr,X,y, n_repeats=10, random_state=42)","1b88ed46":"importance_df=pd.Series(imp.importances_mean).sort_values(ascending=False).to_frame().rename({0:'importance'},axis=1)\nimportance_df['cumsum']=importance_df.cumsum()\ndisplay(importance_df)","a9733b09":"good_features=list(importance_df.loc[:importance_df['cumsum'].idxmax(),:].index)\nX_feature_extraction = X[:,good_features]\nprint(X_feature_extraction.shape)\nX_train,X_test, y_train, y_test = train_test_split(X_feature_extraction,y,test_size=.2,random_state=42)\ngbr.fit(X_train, y_train)\ngbr_preds = gbr.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, gbr_preds)))","83a138ed":"pca = PCA(n_components='mle')\n\narr_pca=pca.fit_transform(X_feature_extraction)\nX_pca = pd.DataFrame(arr_pca, columns=['PCA'+str(i) for i in range(arr_pca.shape[1])])\n\ngraph_var = pca.explained_variance_ratio_\n\nfig = go.Figure(data=[\n    go.Bar(name='Explained Variance', x=np.arange(len(graph_var)), y=graph_var*100)\n])\nfig.add_trace(go.Scatter(x=np.arange(1,len(graph_var)+1), \n                         y=np.cumsum(graph_var)*100,\n                         mode='lines',\n                         name='Cumulated Explained Variance'))\n# Change the bar mode\nfig.update_layout(barmode='stack')\n\nprint(f'Dataset has total feature of {X_feature_extraction.shape[1]}')\nprint(f'Total explained variance: {np.sum(pca.explained_variance_ratio_)*100}% with n_components={pca.n_components_}')\nfig.show()","788548a5":"gbr = RidgeCV(scoring='neg_root_mean_squared_error').fit(X_pca,y)\nrmse=f'**RMSE score of {gbr}={gbr.score(X_pca,y)}**'\ndisplay(Markdown(rmse))","0930e3eb":"X_train,X_test, y_train, y_test = train_test_split(X_pca,np.log(y), test_size=.2, random_state=42)\ngbr = Ridge(random_state=42)\ngbr.fit(X_train, y_train)\ngbr_preds = gbr.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, gbr_preds))\ndisplay(Markdown(f'**RMSE = {rmse}**'))\nr2=f'**$r^2$ of {gbr}={r2_score(y_test,gbr_preds)}**'\ndisplay(Markdown(r2))","db27dd39":"sns.distplot(np.log(y))","a93f6c0f":"submission_transformed.drop(columns = 'SalePrice', inplace=True)","278e972f":"submission_transformed.index","20c1e3b1":"sub=scale(pca.transform(submission_transformed.iloc[:,good_features]))\nres=gbr.predict(sub)\nprice_predicted=pd.DataFrame(np.exp(np.exp(res)),index=submission_transformed.index).rename({0:'SalePrice'},axis=1)\ndisplay(price_predicted)\nprice_predicted.to_csv('submission.csv')","51fca7a8":"I realized there are more of columns that not distributed equally or has lot of missing values. Maybe I can handle them in one, because they are taking so much times. Let's say if columns has more than 75% missing values or one feature's category is more than 75%, we are going to drop this attribute. ","f346dbf7":"<link rel=\"preconnect\" href=\"https:\/\/fonts.gstatic.com\">\n<link href=\"https:\/\/fonts.googleapis.com\/css2?family=Hachi+Maru+Pop&display=swap\" rel=\"stylesheet\">\n<link href=\"https:\/\/raw.githubusercontent.com\/twilson63\/istyle\/master\/amblin.css\" rel=\"stylesheet\">\n<div style=\"width:100%;text-align:center;font-family: 'Hachi Maru Pop', cursive;\">\n    <h1>House Prices: Advanced Regression Techniques<\/h1>\n<\/div>\n<br><br>\n\nMain objectives are;\n\n1. Frame the problem and look at the big picture.\n2. Get the data.\n3. Explore the data to gain insights.\n4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms.\n5. Explore many different models and shortlist the best ones.\n6. Fine-tune your models and combine them into a great solution.\n7. Present your solution.\n\n# Frame the Problem\n\n*  **Define the objective.**\n\nThe goal is to accurately predict house prices. You will pair your machine learning skills using [Ames Housing dataset](http:\/\/www.amstat.org\/publications\/jse\/v19n3\/decock.pdf).\n\n\n* **What are the current solutions\/workarounds (if any)?**\n\nThe current solution can be seen in [this](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/notebooks).\n\n\n* **How should you frame this problem (supervised\/unsupervised, online\/offline, etc.)?**\n\nSupervised, offline (batch), model-based learning.\n\n\n* **How should performance be measured?**\n\nSubmissions are evaluated on [Root-Mean-Squared-Error (RMSE)](https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation).\n\n\n* **What would be the minimum performance needed to reach the business objective?**\n\nThere is no minimum performance to reach the business objective, however, we can compare our results with [leaderboard](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/leaderboard).\n\n\n* **What are comparable problems? Can you reuse experience or tools?**\n\nSince this is my second competition contribution in Kaggle, I don't have any script to use. However, I can use some techniques from [my first notebook](https:\/\/www.kaggle.com\/onurserbetci\/end-to-end-titanic-project).\n\n# Import Relevant Libraries\n\nNow, we can import necessary libraries.\n","6f0c7297":"As we have the data now, we should create test dataset in order to avoid from data snooping.","e39fee28":"### Neighborhood\nIt provide us the neighborhood of each property.\n* Graphs would not be explanatory for the feature such has lot of category like this. We should try different approach if we don't want to lose information about neighborhood of house. We can categorize all as 3 whether they area expensive area or not.","28cb5588":"We can check how much memory each variables are using in megabytes:","fd53a091":"We should label each feature as categorical or numerical. We'll automate this as much as we can, however it should be applying in under supervision","eca00889":"# Feature Engineering\nFirst, we need to drop unuseful features as we did explanatory data anaylsis part. ","d6041b07":"# Get the Data\n\nWe'll use `pandas` package for reading `.csv` file.","9ae3803e":"# Explore the data (EDA)\n\nWe should get information about each attribute. Decide whether it is useful or not. There is lot of features, we should try to automate processes.","8bbd848f":"### Automated Feature Engineering"}}