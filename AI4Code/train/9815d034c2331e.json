{"cell_type":{"f6e9aa1a":"code","aa7b79a5":"code","71ddc77d":"code","3d35765d":"code","e63c8d6b":"code","1737c98f":"code","f7e2f435":"code","3de9c277":"code","cf7e3b1a":"code","062e63d8":"code","f8415b18":"code","4049519b":"code","3be91b76":"code","f793c425":"code","071fa42b":"code","22a0948b":"code","0a23f0d7":"code","5f411b5e":"code","e832b346":"code","e87faaec":"code","28eb06db":"code","081ccb38":"code","d4177cd6":"code","82a62a12":"code","f0da33ba":"code","cf57adee":"code","1a5dbae9":"code","c2eb2343":"code","df976132":"code","864f774c":"code","74659b7f":"code","96c62514":"code","7ff260b1":"code","4ee53675":"code","f38c2171":"code","0f550708":"code","6c8dc0c9":"code","738f3e98":"code","98d02058":"code","139342a7":"code","306f0c96":"code","a2fb3563":"code","2ff1234b":"code","9a4a887f":"code","6d6dc3ea":"code","1e1248ec":"code","c761d1a5":"code","e7f3549d":"code","82c67195":"code","fbb0f2af":"code","a61355a6":"code","8e076a13":"code","842e7e3b":"code","d07e24f8":"code","39e6b85c":"code","3e102811":"code","b0e59e14":"code","2b424ae3":"code","9b06b7af":"code","a0710166":"code","41884872":"code","cc230051":"code","6cd08a20":"code","f4067ab2":"code","839012fb":"code","72e24609":"code","eec4d220":"code","f884966f":"code","e34e34fe":"code","9c49ac87":"code","6bb969e2":"code","d21d6c83":"code","4f971af9":"code","ab2a8ce8":"code","b4be0f28":"code","be037b31":"code","39633039":"code","f7d0b0f5":"code","ae4fc95d":"code","71677431":"code","9f02249f":"code","bd1b5b16":"code","b67ed38d":"code","63bebab2":"code","ffe85848":"code","27b7af25":"code","b07716c0":"code","200901d4":"code","2d8718f1":"code","93ef04b8":"code","e63d2c6e":"code","b5b78c11":"code","34a2a112":"code","e1d9fb6c":"code","fd7e8b37":"code","fc69f776":"code","9eb1d031":"code","382a208a":"code","ed0b3f7b":"code","bf2df844":"code","513c1cda":"code","0cc8ac34":"code","0d45851e":"code","4915617d":"code","2494ed72":"code","af638d79":"code","7c3cac06":"code","1f0b3d08":"code","aeb3f893":"code","c03c3ff4":"code","d669c940":"code","94b8400a":"code","0812df16":"code","7b388762":"code","ce6989b0":"code","279a331c":"code","a97945ef":"code","c848936a":"code","c0ba520e":"code","93a3c34a":"code","cc6a48ca":"code","2f118c0b":"code","6bca6a0e":"code","59cfb95c":"code","57b0a239":"code","141d4ed9":"code","9fe97252":"code","b10f01bf":"code","edef9b53":"code","8df3c770":"code","6c3f0d37":"code","16ee4b4c":"code","5e14618e":"code","4f8db6dc":"code","2950261d":"code","7301fbac":"code","f81f1677":"code","b08b13d0":"code","f587fea9":"code","aab5abc3":"code","e187322c":"code","2d3c7bb8":"code","9695f401":"code","933b7c39":"code","a246a681":"code","992a9757":"code","14057afd":"code","c468cdeb":"code","1fc10fe5":"code","0a94261b":"code","a2447d09":"code","76014e38":"code","52d76609":"code","e6d1ebe8":"code","1b544fc3":"code","3846ebeb":"code","a2069339":"code","a0f1aac0":"code","70fcfeaf":"markdown","86329e2b":"markdown","11e84d18":"markdown","685a363d":"markdown","7c7496e3":"markdown","45153384":"markdown","e97872bc":"markdown","459534a0":"markdown","d0b3f942":"markdown","e6274136":"markdown","77fcfa06":"markdown","5b07ece3":"markdown","7305d78a":"markdown","ce3b2143":"markdown","5a3cc96f":"markdown","cebacfc8":"markdown","ff1fb858":"markdown","dbea8775":"markdown","f48f2245":"markdown","1a3f16e8":"markdown","71b24efa":"markdown","51e6f169":"markdown","05322966":"markdown","a30ba307":"markdown","285b41c5":"markdown","b5ff291d":"markdown","20412460":"markdown","ec7b6b96":"markdown","55b45be6":"markdown","5b65bd74":"markdown","132b388a":"markdown","5028eedd":"markdown"},"source":{"f6e9aa1a":"import numpy as np\nimport pandas as pd\n\n#for charts\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom wordcloud import WordCloud      #need to install wordcloud package","aa7b79a5":"#for text processing\nimport string\nimport re\nimport nltk\nfrom textblob import TextBlob","71ddc77d":"#for tokenization\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n#for feature selection\nfrom sklearn import decomposition\n\n#for model building\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report","3d35765d":"jobs = pd.read_csv('\/kaggle\/input\/jobposts\/data job posts.csv')","e63c8d6b":"jobs.head().T","1737c98f":"jobs.shape","f7e2f435":"jobs.info()","3de9c277":"#lowercase the column names\njobs.columns = jobs.columns.str.lower()","cf7e3b1a":"jobs.columns","062e63d8":"#removing duplicate jobposts based on title and post\njobs = jobs.drop_duplicates(['jobpost', 'title'])","f8415b18":"jobs.shape","4049519b":"#removing records with null title\n#jobs = jobs[jobs.title.notna()]\n#jobs.shape","3be91b76":"jobs['title'] = jobs['title'].astype('str')","f793c425":"string.punctuation","071fa42b":"#UDF to do basic cleaning of title column to understand type of jobs\ndef clean_data(text):\n    text = text.lower()  # convert all the text into lowercase\n    text = text.strip()  #remove starting and trailing whitespaces\n    #special_chars = re.compile('[@!#$%^&*()<>?\/\\|}{~:;]')\n    #text = re.sub(special_chars,'', text)\n    special_char_reg = '([a-zA-Z0-9]+)' + '[!\"#$%&\\'()*+,-.\/:;<=>?@\\\\^_`{|}~]' + '([a-zA-Z0-9]+)'\n    text = re.sub(special_char_reg, ' ', text)\n    text = re.sub(r'\\s+', ' ', text) #remove all line formattings\n    text = re.sub(r'\\d+', '', text) #remove digits\n    text = ''.join(c for c in text if c not in string.punctuation)   #remove pecial symbols from job titles\n    return text","22a0948b":"a = 'Ful8l-ti9me Community Connections f09:053yy'\nspecial_char_reg = '([a-zA-Z0-9]+)' + '[!\"#$%&\\'()*+,-.\/:;<=>?@\\\\^_`{|}~]' + '([a-zA-Z0-9]+)'\nre.sub(special_char_reg, ' ', a).strip()","0a23f0d7":"jobs.title.head(6)","5f411b5e":"title_df = jobs.title.apply(lambda x : clean_data(x))\ntitle_df.head()","e832b346":"from nltk import WordNetLemmatizer\n#nltk.download('punkt')","e87faaec":"#nltk.download('wordnet')","28eb06db":"def lemma(text):\n    word_list = nltk.word_tokenize(text) #tokenize beofre lemmatization\n    lemma_output = ' '.join(WordNetLemmatizer().lemmatize(word) for word in word_list)\n    return lemma_output","081ccb38":"# Define the sentence to be lemmatized\nsentence = \"public bats outreach and strengthening of a growth\"\nsentence = \"The striped bats are hanging on their feet for best\"\n# Tokenize: Split the sentence into words\nword_list = nltk.word_tokenize(sentence)\nprint(word_list)\n#> ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n\n# Lemmatize list of words and join\nlemmatized_output = ' '.join([WordNetLemmatizer().lemmatize(w) for w in word_list])\nprint(lemmatized_output)","d4177cd6":"##Lematization\nimport spacy\n#neccesary to download the english model using \"python -m spacy download en\"\nnlp = spacy.load('en_core_web_sm')\n# Parse the sentence using the loaded 'en' model object `nlp`\ndoc = nlp(sentence)\n\n# Extract the lemma for each token and join\n\" \".join([token.lemma_ for token in doc])","82a62a12":"title_df_1 = title_df.apply(lambda x : lemma(x))","f0da33ba":"title_df_1.head()","cf57adee":"#Stop words removal\nstop = nltk.corpus.stopwords.words('english')\n#stop.extend(['armenian', 'armenia', 'job', 'title', 'position', 'location', 'responsibilities', 'application',\n#                  'procedures', 'deadline', 'required','qualifications', 'renumeration', 'salary', 'date', 'company', 'llc'])","1a5dbae9":"title_df_1 = title_df_1.apply(lambda x : ' '.join(x for x in x.split() if x not in stop))","c2eb2343":"title_df_1.head()","df976132":"#Tokenization using count vectorizer\ncount_vect = CountVectorizer(ngram_range=(1,1))\ntoken = count_vect.fit_transform(title_df_1)","864f774c":"token","74659b7f":"print(count_vect.get_feature_names())","96c62514":"print('Total number of tokens\/words in all the job titles - ', len(count_vect.get_feature_names()))","7ff260b1":"temp_df =  pd.DataFrame(token.toarray(), columns=count_vect.get_feature_names())\ntemp_df.tail()","4ee53675":"#count the accurence of each token in entire corpus\ncount_df = temp_df.apply(lambda x : x.sum())","f38c2171":"count_df = pd.DataFrame(count_df).reset_index()","0f550708":"count_df.columns = ['Word', 'Count']","6c8dc0c9":"top_jobs = count_df.sort_values(by= 'Count', ascending=False)","738f3e98":"top_jobs[:10]","98d02058":"# plot the WordCloud image to show top 50 type of demanding jobs in armenia     \nwordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(top_jobs[:50].Word))\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","139342a7":"jobs['jobpost'] = jobs['jobpost'].astype('str')","306f0c96":"#UDF to do basic cleaning of title column to understand type of jobs\ndef clean_data(text):\n    text = text.lower()  # convert all the text into lowercase\n    text = text.strip()  #remove starting and trailing whitespaces\n    #special_chars = re.compile('[@!#$%^&*()<>?\/\\|}{~:;]')\n    #text = re.sub(special_chars,'', text)\n    special_char_reg = '([a-zA-Z0-9]+)' + '[!\"#$%&\\'()*+,-.\/:;<=>?@\\\\^_`{|}~]' + '([a-zA-Z0-9]+)'\n    text = re.sub(special_char_reg, ' ', text)\n    text = re.sub(r'\\s+', ' ', text) #remove all line formattings\n    text = re.sub(r'\\d+', '', text) #remove digits\n    text = ''.join(c for c in text if c not in string.punctuation)   #remove pecial symbols from job titles\n    return text","a2fb3563":"jobs.jobpost.head()","2ff1234b":"jobpost_df = jobs.jobpost.apply(lambda x : clean_data(x))","9a4a887f":"jobpost_df.head(10)","6d6dc3ea":"##Lematization\nimport spacy\n#neccesary to download the english model using \"python -m spacy download en\"\n#nlp = spacy.load('en_core_web_sm')\nlemmatized_out = []\ncount = 0\n#for jobpost in jobpost_df:\n#    doc = nlp(jobpost)\n#    x = \" \".join(word.lemma_ for word in doc)\n#    print(count)\n#    count += 1\n#    lemmatized_out.append(x)","1e1248ec":"#lemmatized_out[0]","c761d1a5":"#Stop words removal\nstop = nltk.corpus.stopwords.words('english')\nstop.extend(['armenian', 'armenia', 'job', 'title', 'position', 'location', 'responsibility', 'application',\n             'procedure', 'deadline', 'requirement','qualification', 'renumeration', 'salary', 'date', 'company', 'llc',\n             'person', 'employement', 'post', 'follow', 'resume', 'open', 'about', 'announcement', 'link', 'website',\n             'organization', 'duration'])","e7f3549d":"#jobpost_df_0 = pd.Series(lemmatized_out)","82c67195":"jobpost_df.head(10)","fbb0f2af":"jobpost_df_1 = jobpost_df.apply(lambda x : ' '.join(word for word in x.split() if word not in stop))","a61355a6":"jobpost_df_1.head(10)","8e076a13":"#Now we will create tokens out of this processed data\n\ntfidf_vect = TfidfVectorizer(ngram_range=(1,1), min_df=0.05, max_df=0.95)\ntfidf_vect","842e7e3b":"token_jobpost = tfidf_vect.fit_transform(jobpost_df_1)","d07e24f8":"vocab = tfidf_vect.get_feature_names()\n#print(vocab)","39e6b85c":"token_jobpost","3e102811":"len(tfidf_vect.get_feature_names())","b0e59e14":"token_df = pd.DataFrame(token_jobpost.toarray(), columns=tfidf_vect.get_feature_names())\ntoken_df.head()","2b424ae3":"#Apply LDA technique to understand important job nature and profiles\n\nlda = decomposition.LatentDirichletAllocation(n_components = 5, learning_method = 'online', max_iter = 50, random_state = 3)\nlda.fit_transform(token_jobpost)\ntopics = lda.components_","9b06b7af":"topics","a0710166":"# view the topic models for cluster 0\nn_top_words = 10\ntopic_summaries = []\nfor i, topic_dist in enumerate(topics):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n\ntopic_summaries","41884872":"#vocab = []\n#def fn_token(post):\n#    list_temp = nltk.word_tokenize(post)\n#    vocab.extend(list_temp)\n\n#jobpost_df_1.apply(lambda x : fn_token(x))","cc230051":"#full_vocab = []\n#for word in  vocab:\n#        if word not in full_vocab:\n#            full_vocab.append(word)","6cd08a20":"#print(full_vocab)","f4067ab2":"topic_words_tokens = []\nfor topic in topic_summaries:\n    word_token = nltk.word_tokenize(topic)\n    topic_words_tokens.extend(word_token)\nprint(topic_words_tokens)\n","839012fb":"#use lexical dispersion plot to see the topics use over time\n#Start pylab inline mode, so figures will appear in the notebook\n#%pylab inline\n\n#from nltk.draw.dispersion import dispersion_plot\n\n#dispersion_plot(vocab, topic_words_tokens[:10])","72e24609":"plot_df = pd.concat([jobpost_df_1, jobs.year], axis = 1)\n#plot_df = jobpost_df_2.apply(lambda x : fn() )","eec4d220":"topic_words_tokens[:10]","f884966f":"('topic', '2018')","e34e34fe":"nt = [(topic, year)  for year in plot_df.year  for topic in topic_words_tokens[0:9] ]","9c49ac87":"cfd = nltk.ConditionalFreqDist(nt)","6bb969e2":"#conditional frequency distribution plot to see the use of topics over time\ncfd = nltk.ConditionalFreqDist(\n    (target, year)\n    for year in plot_df.year\n    for a in plot_df.jobpost\n    for w in nltk.word_tokenize(a)    \n    for target in topic_words_tokens[:10]\n    if w.lower().startswith(target))\ncfd.plot()\n\n#    for w in jobpost_df_1.words(year)","d21d6c83":"x = jobs[jobs.jobdescription.isna() == False]","4f971af9":"jobs.shape","ab2a8ce8":"x.shape","b4be0f28":"x.head()","be037b31":"x['jobdescription'] = x['jobdescription'].astype('str')","39633039":"desc_df = x.jobdescription.apply(lambda x : clean_data(x))\ndesc_df.head()","f7d0b0f5":"#LEmmatization\ndesc_df_1 = desc_df.apply(lambda x : lemma(x))","ae4fc95d":"#lemmatized_out[0:6]","71677431":"#desc_df_0 = pd.Series(lemmatized_out)","9f02249f":"desc_df_1.head()","bd1b5b16":"#stop word removal\ndesc_df_1 = desc_df_1.apply(lambda x : ' '.join(x for x in x.split() if x not in stop))","b67ed38d":"desc_df_1.head()","63bebab2":"#Tokenization\ntfidf_vect = TfidfVectorizer(ngram_range=(1,1), min_df = 0.05, max_df=0.95, stop_words='english')\nx_tdm = tfidf_vect.fit_transform(desc_df_1)\n#print(x_tdm)","ffe85848":"df_clust = pd.DataFrame(x_tdm.toarray(), columns=tfidf_vect.get_feature_names())","27b7af25":"df_clust.head()","b07716c0":"from sklearn.cluster import KMeans\nfrom sklearn import metrics","200901d4":"\nmodel = KMeans(n_clusters=5, \n               init='k-means++', \n               max_iter=100, n_init=1,random_state=5)\nkmeans = model.fit(x_tdm)","2d8718f1":"# we create a kmeans model\nkm_3 = KMeans(n_clusters=3,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_4 = KMeans(n_clusters=4,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_5 = KMeans(n_clusters=5,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_6 = KMeans(n_clusters=6,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_7 = KMeans(n_clusters=7,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_8 = KMeans(n_clusters=8,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)\nkm_9 = KMeans(n_clusters=9,init='k-means++', max_iter=100, n_init=1, random_state=5).fit(x_tdm)","93ef04b8":"# save the cluster labels and sort by cluster\nx['cluster_3'] = km_3.labels_\nx['cluster_4'] = km_4.labels_\nx['cluster_5'] = km_5.labels_\nx['cluster_6'] = km_6.labels_\nx['cluster_7'] = km_7.labels_\nx['cluster_8'] = km_8.labels_\nx['cluster_9'] = km_9.labels_","e63d2c6e":"len(tfidf_vect.get_feature_names())","b5b78c11":"vocab = np.array(tfidf_vect.get_feature_names())\nvocab","34a2a112":"cluster_centers = np.array(km_5.cluster_centers_)\ncluster_centers[0].argsort()","e1d9fb6c":"km_3.labels_","fd7e8b37":"x['cluster_3'].value_counts()\/sum(x['cluster_3'].value_counts())","fc69f776":"x['cluster_4'].value_counts()\/sum(x['cluster_4'].value_counts())","9eb1d031":"x['cluster_5'].value_counts()\/sum(x['cluster_5'].value_counts())","382a208a":"x['cluster_6'].value_counts()\/sum(x['cluster_6'].value_counts())","ed0b3f7b":"x['cluster_7'].value_counts()\/sum(x['cluster_7'].value_counts())","bf2df844":"x['cluster_8'].value_counts()\/sum(x['cluster_8'].value_counts())","513c1cda":"from sklearn import  metrics\nmetrics.silhouette_score(x_tdm, labels=km_3.labels_)","0cc8ac34":"scores = []\nscores.append(metrics.silhouette_score(x_tdm, labels=km_3.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_4.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_5.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_6.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_7.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_8.labels_))\nscores.append(metrics.silhouette_score(x_tdm, labels=km_9.labels_))\nscores","0d45851e":"plt.plot(range(3,10), scores)\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.grid('True')","4915617d":"# sorting the cluster centers for 5 clusters\nsorted_vals = [km_5.cluster_centers_[i].argsort() for i in range(0,np.shape(km_5.cluster_centers_)[0])]","2494ed72":"# get top 10 words from that cluster\nwords=set()\nfor i in range(len(km_5.cluster_centers_)):\n    words = set(vocab[sorted_vals[i][-10:]])\n    print(words)","af638d79":"# sorting the cluster centers for 6 clusters\nsorted_vals = [km_6.cluster_centers_[i].argsort() for i in range(0,np.shape(km_6.cluster_centers_)[0])]","7c3cac06":"# get top 10 words from that cluster\nwords=set()\nfor i in range(len(km_6.cluster_centers_)):\n    words = set(vocab[sorted_vals[i][-10:]])\n    print(words)","1f0b3d08":"class_data = jobs[(jobs.title.isna() == False) & (jobs.jobrequirment.isna() == False) & (jobs.requiredqual.isna() == False) &\n                 (jobs.jobdescription.isna() == False) & (jobs.aboutc.isna() == False) & (jobs.company.isna() == False)]","aeb3f893":"class_data.shape","c03c3ff4":"class_data.isna().sum()","d669c940":"#identify Y variable\nclass_data['it'] = class_data.it.apply(lambda x : 0 if (x is False) else 1)\ny=class_data['it']","94b8400a":"y.value_counts()","0812df16":"sns.countplot(y)","7b388762":"class_data = class_data['title'].str.cat(class_data['jobrequirment'], sep =\" \").str.cat(class_data['requiredqual'], sep =\" \").str.cat(\n    class_data['jobdescription'], sep =\" \").str.cat(class_data['aboutc'], sep =\" \").str.cat(class_data['company'], sep =\" \")\nclass_df = class_data","ce6989b0":"class_df.head()","279a331c":"class_df = class_df.apply(lambda x : clean_data(str(x)))\nclass_df.head()","a97945ef":"#Lemmatization\nclass_df_1 = class_df.apply(lambda x : lemma(x))","c848936a":"class_df_1.head()","c0ba520e":"#stop word removal\nclass_df_1 = class_df_1.apply(lambda x : ' '.join(x for x in x.split() if x not in stop))","93a3c34a":"class_df_1.head()","cc6a48ca":"#Tokenization\ntfidf_vect = TfidfVectorizer(ngram_range=(1,1), min_df = 0.05, max_df=0.95, stop_words='english')\nx_tdm = tfidf_vect.fit_transform(class_df_1)\n#print(x_tdm)","2f118c0b":"df_clust = pd.DataFrame(x_tdm.toarray(), columns=tfidf_vect.get_feature_names())","6bca6a0e":"df_clust.head()","59cfb95c":"df_clust.shape","57b0a239":"from sklearn.model_selection import train_test_split\n\ntrain_x, test_x,train_y, test_y = train_test_split(df_clust,y, test_size = 0.2, random_state = 5)\nprint(train_x.shape, test_x.shape)","141d4ed9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","9fe97252":"param_grid = {'n_estimators':[130,150,160,180,200],\n              'max_features':[13,15,17,19]}\n\ngrid_rf = GridSearchCV(estimator= RandomForestClassifier(),\n                      param_grid=param_grid,\n                      cv = 10,\n                      n_jobs=-1, verbose=True)\n\ngrid_rf.fit(train_x,train_y)","b10f01bf":"grid_rf.best_score_","edef9b53":"grid_rf.best_params_","8df3c770":"grid_rf.best_estimator_","6c3f0d37":"#Fit the model\nrf_model = grid_rf.best_estimator_\nrf_model.fit(train_x, train_y)","16ee4b4c":"rf_train_predict = pd.DataFrame({'actual' : train_y,\n                                 'predicted' : rf_model.predict(train_x)})\nrf_train_predict.head()","5e14618e":"rf_test_predict = pd.DataFrame({'actual' : test_y,\n                                 'predicted' : rf_model.predict(test_x)})\nrf_test_predict.head()","4f8db6dc":"#1. Check accuracy score on train and test\n\nprint('Accuracy Score for train dataset : ' , metrics.accuracy_score(rf_train_predict.actual, rf_train_predict.predicted))\nprint('Accuracy Score for test dataset : ' , metrics.accuracy_score(rf_test_predict.actual, rf_test_predict.predicted))","2950261d":"#2. Check roc_auc score on train and test\n\nprint('ROC-AUC Score for train dataset : ' , metrics.roc_auc_score(rf_train_predict.actual, rf_train_predict.predicted))\nprint('ROC-AUC Score for validation dataset : ' , metrics.roc_auc_score(rf_test_predict.actual, rf_test_predict.predicted))","7301fbac":"#3. Create confusion matrix\n#for test\n\nconn_cm_test = metrics.confusion_matrix(rf_test_predict.actual, rf_test_predict.predicted, [1,0])\nsns.heatmap(conn_cm_test, fmt= '.2f', annot=True,  xticklabels=['IT', 'NOT IT'], yticklabels=['IT', 'NOT IT'])","f81f1677":"#4. Create classification report\nprint(metrics.classification_report(rf_test_predict.actual, rf_test_predict.predicted))","b08b13d0":"indices = np.argsort(rf_model.feature_importances_)[::-1]\nfeature_rank = pd.DataFrame(columns = ['rank', 'feature', 'importance'])\nfor f in range(train_x.shape[1]):\n    feature_rank.loc[f] = [f+1,\n                          train_x.columns[indices[f]],\n                          rf_model.feature_importances_[indices[f]]]\nfeature_rank.round(3)","f587fea9":"feature_rank[:17]","aab5abc3":"###Using TF-IDF as cosine similarity","e187322c":"#from sklearn.metrics.pairwise import cosine_similarity","2d3c7bb8":"def get_cosine_sim(doc): \n    vectors = [t for t in get_vectors(doc)]\n    return cosine_similarity(vectors)\n    \ndef get_vectors(doc):\n    text = [t for t in doc]\n    vectorizer = CountVectorizer(text)\n    vectorizer.fit(text)\n    return vectorizer.transform(text).toarray()","9695f401":"from gensim import utils\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim.models import Doc2Vec","933b7c39":"jobpost_df_1.shape","a246a681":"#Pre-processed text of jobpost column\njobpost_df_1.head(10)","992a9757":"title_df_1.shape","14057afd":"#Pre-processed text of title column column\ntitle_df_1.head()","c468cdeb":"df_sim = pd.concat([jobpost_df_1, title_df_1], axis = 1)\ndf_sim.loc[1810]","1fc10fe5":"docs=[]\ndef fn_tag_doc(jobpost, title):\n        docs.append(TaggedDocument(words = jobpost.split(), tags = [title]))     \n\ndf_sim.apply(lambda x : fn_tag_doc(x['jobpost'], x['title']), axis = 1)","0a94261b":"docs[1]","a2447d09":"model_sim = Doc2Vec(docs, dm=0, alpha = 0.025, min_alpha = 0.025, min_count = 0)  # use fixed learning rate","76014e38":"for epoch in range(10):\n    model_sim.train(docs, total_examples= model_sim.corpus_count, epochs=model_sim.epochs)\n    model_sim.alpha -= 0.002  # decrease the learning rate\n    model_sim.min_alpha = model_sim.alpha  # fix the learning rate, no decay","52d76609":"model_sim.docvecs.most_similar(positive=[model_sim.infer_vector('chief financial officer'.split())],topn=10)","e6d1ebe8":"#docs[0].tags","1b544fc3":"#tags_list=[]\n#for i in range(0, df_sim.shape[0]):\n#    c = str(docs[i].tags).replace('[', '')\n#    c = c.replace(']', '')\n#    c = c.replace(\"'\", '')\n#    tags_list.append(c)","3846ebeb":"#tags_list[0]","a2069339":"#sim_list = []\n#for tag in tags_list:\n    #print(model_sim.docvecs.similarity('bcc specialist', tag))\n#    sim_list.append(model_sim.docvecs.similarity('software developer', tag))\n    \n#sim_list[0]","a0f1aac0":"#sim_score_df = pd.concat([pd.Series(jobpost_df_1),pd.Series(tags_list), pd.Series(sim_list)], axis =1)\n#sim_score_df.columns=['jobpost', 'title', 'similarity_score']\n#sim_score_df\n#sim_score_df.sort_values(by = 'similarity_score', ascending = False)","70fcfeaf":"# Business Problem:","86329e2b":"# Desired characteristics and  Skill-sets\nTo understand this we can make clusters using job description column of the data","11e84d18":"### Using word embeddings Doc2Vec","685a363d":"Above analysis shows that in cluster 6 the tokens get repeated and clusters are more similar to each other. That means **cluster 5** is optimal. ","7c7496e3":"#### Prdict the output for train and validation set","45153384":"Our main business objectives are to understand the dynamics of the labour market of Armenia using the online job portal post as a proxy.","e97872bc":"### 1. Silhouette Coefficient(Higher the better)","459534a0":"#### 2. Build a Model to convert each document(jobpost) into vectors to be used to check similarity","d0b3f942":"Either 5-6 , is the optimal solution for our clusters","e6274136":"#### Divide the data into train and test","77fcfa06":"## Building a Random Forest Model","5b07ece3":"#### 3.Check the similarity of a given job title and get top 10 jobposts similar to that job_title","7305d78a":"# Type of demanding jobs in Armenia","ce3b2143":"# Job Nature changing over time","5a3cc96f":"# Import necessary packages","cebacfc8":"### Text pre-processing","ff1fb858":"# Similarity of Jobs","dbea8775":"Now to undertand the most demanding jobs in armenia we can create a **bi\/tri gram DTM** on the job titles and find the most occuring token to be the most demanding job","f48f2245":"Presence of keywords like software, developer, web, design, cs are the important feautures while clasifying any job as **IT or NON-IT**","1a3f16e8":"# IT Job Classification","71b24efa":"7 Cluster seems to be optimal","51e6f169":"### Import data","05322966":"#### Text pre-processing","a30ba307":"### Evaluation clusters","285b41c5":"# Model Evaluation","b5ff291d":"#### Dimension Reduction","20412460":"### Clustering","ec7b6b96":"# Data Cleaning","55b45be6":"#### 1. Create the tags with each post","5b65bd74":"Above shows **top 10 titles** which are similar to the title 'chief financial officer'.","132b388a":"We can use dispersion plot to see how jobs change over time. To do this we need to get the important topics out of the jobpost and then plot their dispersion over time.","5028eedd":"**1. Preprocessing the text data**"}}