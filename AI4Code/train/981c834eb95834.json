{"cell_type":{"52be48c8":"code","49ab98b2":"code","c9e1751e":"code","935bc72e":"code","937ba558":"code","82691c89":"code","4fc84f9d":"code","7143c99b":"code","e4b440d4":"code","5a61ac30":"code","6d1ce491":"code","665da9ce":"code","8af94eb7":"code","ac224401":"code","5c368d6d":"code","e0b8b623":"code","9e084513":"code","d58c2033":"code","086fe7a5":"code","fde2dd91":"code","b1c12672":"code","e2c9590f":"code","439fcf0b":"code","4468f55b":"markdown","110dd502":"markdown","9763a436":"markdown","81127ad9":"markdown","0ea2c83f":"markdown","326dfb99":"markdown","57ec92df":"markdown","a8153831":"markdown","102b06f9":"markdown","74c7e337":"markdown","082e438d":"markdown","832e2d86":"markdown","9ae16ae8":"markdown","c41107a2":"markdown","f7d9a280":"markdown","da94638e":"markdown","bddb78aa":"markdown","935e5eea":"markdown","735be022":"markdown","6817b998":"markdown","e1c38fc6":"markdown","42ac99d9":"markdown","5ad44ba5":"markdown","83260780":"markdown","1177d812":"markdown","de3b99a7":"markdown","0855af21":"markdown","2222f48f":"markdown","4201206a":"markdown","88074b33":"markdown","938bd600":"markdown","0ddc189f":"markdown","d9408371":"markdown","fd283f8d":"markdown","2a7db130":"markdown","2b7c7740":"markdown","65644592":"markdown","3291582a":"markdown"},"source":{"52be48c8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nimport time\n\ninit_notebook_mode(connected=True)   ","49ab98b2":"def sigmoid(X, weight):\n    z = np.dot(X, weight)\n    return 1 \/ (1 + np.exp(-z))","c9e1751e":"def loss(h, y):\n    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()","935bc72e":"def gradient_descent(X, h, y):\n    return np.dot(X.T, (h - y)) \/ y.shape[0]\ndef update_weight_loss(weight, learning_rate, gradient):\n    return weight - learning_rate * gradient","937ba558":"def log_likelihood(x, y, weights):\n    z = np.dot(x, weights)\n    ll = np.sum( y*z - np.log(1 + np.exp(z)) )\n    return ll","82691c89":"def gradient_ascent(X, h, y):\n    return np.dot(X.T, y - h)\ndef update_weight_mle(weight, learning_rate, gradient):\n    return weight + learning_rate * gradient","4fc84f9d":"data = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\nprint(\"Dataset size\")\nprint(\"Rows {} Columns {}\".format(data.shape[0], data.shape[1]))","7143c99b":"print(\"Columns and data types\")\npd.DataFrame(data.dtypes).rename(columns = {0:'dtype'})","e4b440d4":"df = data.copy()","5a61ac30":"churns = [\"Yes\", \"No\"]\nfig = {\n    'data': [\n        {\n            'x': df.loc[(df['Churn']==churn), 'MonthlyCharges'] ,\n            'y': df.loc[(df['Churn']==churn),'tenure'],\n            'name': churn, 'mode': 'markers',\n        } for churn in churns\n    ],\n    'layout': {\n        'title': 'Tenure vs Monthly Charges',\n        'xaxis': {'title': 'Monthly Charges'},\n        'yaxis': {'title': \"Tenure\"}\n    }\n}\n\npy.offline.iplot(fig)","6d1ce491":"figs = []\n\nfor churn in churns:\n    figs.append(\n        go.Box(\n            y = df.loc[(df['Churn']==churn),'tenure'],\n            name = churn\n        )\n    )\nlayout = go.Layout(\n    title = \"Tenure\",\n    xaxis = {\"title\" : \"Churn?\"},\n    yaxis = {\"title\" : \"Tenure\"},\n    width=800,\n    height=500\n)\n\nfig = go.Figure(data=figs, layout=layout)\npy.offline.iplot(fig)","665da9ce":"figs = []\n\nfor churn in churns:\n    figs.append(\n        go.Box(\n            y = df.loc[(df['Churn']==churn),'MonthlyCharges'],\n            name = churn\n        )\n    )\nlayout = go.Layout(\n    title = \"MonthlyCharges\",\n    xaxis = {\"title\" : \"Churn?\"},\n    yaxis = {\"title\" : \"MonthlyCharges\"},\n    width=800,\n    height=500\n)\n\nfig = go.Figure(data=figs, layout=layout)\npy.offline.iplot(fig)","8af94eb7":"_ = df.groupby('Churn').size().reset_index()\n# .sort_values(by='tenure', ascending=True)\n\ndata = [go.Bar(\n    x = _['Churn'].tolist(),\n    y = _[0].tolist(),\n    marker=dict(\n        color=['rgba(255,190,134,1)', 'rgba(142,186,217,1)'])\n)]\nlayout = go.Layout(\n    title = \"Churn distribution\",\n    xaxis = {\"title\" : \"Churn?\"},\n    width=800,\n    height=500\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)","ac224401":"df['class'] = df['Churn'].apply(lambda x : 1 if x == \"Yes\" else 0)\n# features will be saved as X and our target will be saved as y\nX = df[['tenure','MonthlyCharges']].copy()\nX2 = df[['tenure','MonthlyCharges']].copy()\ny = df['class'].copy()","5c368d6d":"start_time = time.time()\n\nnum_iter = 100000\n\nintercept = np.ones((X.shape[0], 1)) \nX = np.concatenate((intercept, X), axis=1)\ntheta = np.zeros(X.shape[1])\n\nfor i in range(num_iter):\n    h = sigmoid(X, theta)\n    gradient = gradient_descent(X, h, y)\n    theta = update_weight_loss(theta, 0.1, gradient)\n    \nprint(\"Training time (Log Reg using Gradient descent):\" + str(time.time() - start_time) + \" seconds\")\nprint(\"Learning rate: {}\\nIteration: {}\".format(0.1, num_iter))","e0b8b623":"result = sigmoid(X, theta)","9e084513":"f = pd.DataFrame(np.around(result, decimals=6)).join(y)\nf['pred'] = f[0].apply(lambda x : 0 if x < 0.5 else 1)\nprint(\"Accuracy (Loss minimization):\")\nf.loc[f['pred']==f['class']].shape[0] \/ f.shape[0] * 100","d58c2033":"start_time = time.time()\nnum_iter = 100000\n\nintercept2 = np.ones((X2.shape[0], 1))\nX2 = np.concatenate((intercept2, X2), axis=1)\ntheta2 = np.zeros(X2.shape[1])\n\nfor i in range(num_iter):\n    h2 = sigmoid(X2, theta2)\n    gradient2 = gradient_ascent(X2, h2, y) #np.dot(X.T, (h - y)) \/ y.size\n    theta2 = update_weight_mle(theta2, 0.1, gradient2)\n    \nprint(\"Training time (Log Reg using MLE):\" + str(time.time() - start_time) + \"seconds\")\nprint(\"Learning rate: {}\\nIteration: {}\".format(0.1, num_iter))","086fe7a5":"result2 = sigmoid(X2, theta2)","fde2dd91":"print(\"Accuracy (Maximum Likelihood Estimation):\")\nf2 = pd.DataFrame(result2).join(y)\nf2.loc[f2[0]==f2['class']].shape[0] \/ f2.shape[0] * 100","b1c12672":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(fit_intercept=True, max_iter=100000)\nclf.fit(df[['tenure','MonthlyCharges']], y)\nprint(\"Training time (sklearn's LogisticRegression module):\" + str(time.time() - start_time) + \" seconds\")\nprint(\"Learning rate: {}\\nIteration: {}\".format(0.1, num_iter))","e2c9590f":"result3 = clf.predict(df[['tenure','MonthlyCharges']])","439fcf0b":"print(\"Accuracy (sklearn's Logistic Regression):\")\nf3 = pd.DataFrame(result3).join(y)\nf3.loc[f3[0]==f3['class']].shape[0] \/ f3.shape[0] * 100","4468f55b":"From here, there are two common ways to approach the optimization of the Logistic Regression. One is through loss minimizing with the use of **gradient descent** and the other is with the use of **Maximum Likelihood Estimation**. I will try to explain these two in the following sections.","110dd502":"<h2>Objectives:<br><\/h2>\n* To learn the theory behind Logistic Regression (Mathematical part, ugh).\n* To be able to implement the Logistic Regression without using built-in Logistic Regression libraries.\n* To be able to predict whether a customer will churn or not.\n","9763a436":"Before we start coding let us first understand or atleast try to understand the things happening at the back-end of Logistic Regression. The aim of this section, **Logistic Regression behind the mask** is to explain the math behind Logistic Regression and to accomplish the first objective of this kernel. To be able to do this we must answer the question, how does a Logistic Regression work? In theory, a Logistic regression takes input and returns an output of probability, a value between 0 and 1. How does a Logistic Regression do that? With the help of a function called a *logistic function* or most commonly known as a *sigmoid*. This sigmoid function is reponsible for *predicting* or classifying a given input.\nLogistic function or sigmoid is defined as:\n![](https:\/\/imgur.com\/Bw5gMJX.jpg)\nWhere:\n* *e* = Euler's number which is **2.71828**.\n* *x0* = the value of the sigmoid's midpoint on the x-axis.\n* *L* = the maximum value.\n* *k* = steepness of the curve.","81127ad9":"<h2>Summary and Conclusion<\/h2>","0ea2c83f":"![](https:\/\/imgur.com\/rBVzJbt.jpg)\nThe weights are updated by substracting the derivative (gradient descent) times the learning rate, as defined below:\n![](https:\/\/imgur.com\/TAIpnwI.jpg)\nWhere:\n* \u03b1 = learning rate (usually 0.1)","326dfb99":"Now let us try maximum likelihood estimation and compute the accuracy","57ec92df":"In python:","a8153831":"Before we start predicting, an important step to do is to convert our **Churn** feature, which is a string, into integer. *Yes* will be converted to 1 and *No* will be converted to 0. We will name this new columns a \"class\".","102b06f9":"The goal is to **minimize the loss**  by means of increasing or decreasing the weights, which is commonly called fitting. Which weights should be bigger and which should be smaller? This can be decided by a function called **Gradient descent**. The Gradient descent is just the derivative of the loss function with respect to its weights. Below links explains how Gradient descent is derived (I'm just too lazy to explain it): <br>\n* [https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/gradient_descent.html#step-by-step](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/gradient_descent.html#step-by-step)\n* [http:\/\/mccormickml.com\/2014\/03\/04\/gradient-descent-derivation\/](http:\/\/mccormickml.com\/2014\/03\/04\/gradient-descent-derivation\/)","74c7e337":"<center><h1>Logistic Regression from scratch using Python<\/h1>By: Jepp Bautista<\/center>","082e438d":"In python:","832e2d86":"That's a lot of columns, to simplify our experiment we will only use 2 features **tenure** and **MonthlyCharges** and the target would be **Churn**  ofcourse. Let us do a simple EDA and visualization on our features and target.","9ae16ae8":"![](https:\/\/imgur.com\/Bw5gMJX.jpg)\n","c41107a2":"<h2>Introduction: What is Logistic Regression?<br><\/h2>\nLogistic regression is a regression analysis that predicts the probability of an outcome that can only have two values (i.e. a dichotomy). A logistic regression produces a logistic curve, which is limited to values between 0 and 1. Logistic regression models the probability that each input belongs to a particular category. For this particular notebook we will try to predict whether a customer will churn using a Logistic Regression.<br><br>\n**Prerequisites:**\n1. Python knowledge\n1. Atleast basic differential calculus \n1. Matrix algebra\n","f7d9a280":"<h4>1. Loss minimizing<\/h4><br>\nWeights (represented by theta in our notation) is a vital part of Logistic Regression and other Machine Learning algorithms and we want to find the best values for them. To start we pick random values and we need a way to measure how well the algorithm performs using those random weights. That measure is computed using the loss function. [[1]](https:\/\/medium.com\/@martinpella\/logistic-regression-from-scratch-in-python-124c5636b8ac) <br><br>\nThe loss function is defined as:\n![](https:\/\/imgur.com\/riDHhZS.jpg)\nWhere:\n* m = the number of samples\n* y = the target class","da94638e":"<h3>EDA: Independent variables<\/h3>","bddb78aa":"In python:","935e5eea":"<h2>Python implementation<\/h2>","735be022":"Insights from the training, prediction and simple evaluation that we've done: <br>\nWe've accomplished our second objective which is to implement a Logistic Regression without the help of built-in libraries (except numpy of course). <br>\nWe've predicted and computed the accuracy of three different models\n1. Log Regression from scratch using loss minimization. \n1. Log Regression from scratch using maximum likelihood estimation.\n1. Log Regression class of sklearn.\n    ","6817b998":"<h2>References:<\/h2><br>\nThis kernel was heavily influenced by the following:\n* https:\/\/medium.com\/@martinpella\/logistic-regression-from-scratch-in-python-124c5636b8ac\n* https:\/\/beckernick.github.io\/logistic-regression-from-scratch\/","e1c38fc6":"Next, let us try using sklearn's LogisticRegression module","42ac99d9":"Now, the gradient of the log likelihood is the derivative of the log likelihood function. The full derivation of the maximum likelihood estimator can be found [here](https:\/\/www.analyticsvidhya.com\/blog\/2015\/10\/basics-logistic-regression\/) (too lazy to explain again).\n![](https:\/\/imgur.com\/Uvo3rPv.jpg)\nThe weights are now updated by adding the derivative (gradient ascent) times the learning rate, as defined below:\n![](https:\/\/imgur.com\/hIB0LQ0.jpg)","5ad44ba5":"<h2>Logistic Regression behind the mask<\/h2>","83260780":"In this notebook I will try to implement a Logistic Regression without relying to Python's easy-to-use [scikit-learn](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) library. This notebook aims to create a Logistic Regression without the help of in-built Logistic Regression libraries to help us fully understand how Logistic Regression works in the background. <br>**Beware: Mathematical mumbo-jumbos are present in this notebook**","1177d812":"<h3>Logistic Regression in action<\/h3>","de3b99a7":"Insights from our simple EDA:<br>\n* We can see a difference between our target classes on tenure as you can see in the first boxplot, which is good because our model (Logistic Regression) may use this to separate the two classes.\n* There is also a slight difference between our target classes on monthly charges as shown in the second boxplot.\n* The barchart above shows a huge imbalance in our target classes, this may affect the prediction of our model. We may have to deal with this later.","0855af21":"Let us now start implementing what we learned from the previous section into python codes. We will use the Telco Customer Churn data ofcourse, by the end of this section we will be able to make predictions using our \"home-made\" Logistic Regression.","2222f48f":"In python code:","4201206a":"So, we've finished covering one of the steps on LR optimization **Loss minimization** with the use of gradient descent. We will now jump to maximum likelihood estimation.","88074b33":"Let us try first loss minimization with gradient descent and calculate the accuracy of our model.","938bd600":"Looks like we've completed our first objective, let's get to coding now.","0ddc189f":"<h3>EDA: Target<\/h3>","d9408371":"Now I think we're done understanding the math behind Logistic Regression, just a recap:<br>\n1. We learned that Logistic Regression can be used for Classification because the output is a number between 0 and 1.\n1. We understood the two common ways of optimizing Logistic Regression, minimizing the loss and the other is maximizing the likelihood.\n1. We learned the difference between Gradient descent and gradient ascent.<br>\n\nIf you want to add more, or if there's something wrong with the things I stated above or you want to share an improvement, please feel free to leave a comment.","fd283f8d":"In this kernel, we've created a logistic regression from scratch. We've learned the computations happening at the back-end of a Logistic Regression. We've transormed these equations and mathematical functions into python codes. We've trained our logistic regression function in two ways: through loss minimizing using gradient descent and maximizing the likelihood using gradient ascent. The Telco Customer Churn dataset was used for training and also evaluation. Below is the result of the evaluation (not dynamic)\n\n<table>\n    <tr>\n        <td>**LR model**<\/td>\n        <td>**training time (7043 records)**<\/td>\n        <td>**training accuracy**<\/td>\n    <\/tr>\n     <tr>\n        <td>Loss function + Gradient descent<\/td>\n        <td>56 seconds<\/td>\n        <td>68.5%<\/td>\n    <\/tr>\n     <tr>\n        <td>MLE + Gradient ascent<\/td>\n        <td>49 seconds<\/td>\n        <td>73.07%<\/td>\n    <\/tr>\n    <tr>\n        <td>sklearn<\/td>\n        <td>49 seconds<\/td>\n        <td>78%<\/td>\n    <\/tr>\n<\/table><br>\nWhile the table shows that MLE + Gradient ascent is better than the other method, we have to consider the number of training iterations we've set as well as other hyperparameters. I randomly chose 100,000 as the number of iteration for this exercise, increasing or decreasing it might change the result, that's yours to find out. Also we've only chosen **tenure** and **monthlyCharges** as our features to simplify things, there might be important features that we need to include in the future to make the algorithm perform better, again that's yours to find out. Despite all of these, our function performed quite well I would say, (LOL) it's not that far out from the accuracy of sklearn, however there are other metrics to consider in comparing these models, that's also yours to find out. <br>\nTo wrap things up let us review our objectives and wether we've accomplished them. The first objective was to understand the theory behind Logistic Regression. We've discussed that in the section **Logistic Regression behind the mask**, and I do hope that we all understood the things I stated there. The second objective was to implement the Logistic Regression without using built-in Logistic Regression libraries, yes we've done that in the section **Logistic Regression in action**, it was trained, and evaluated. In the same section, we have also predicted the churn of the customers in the Telco Customer Churn dataset. <br><br>\nThis logistic regression implementation would probably be never used in production and it is unlikely that it will defeat sklearn's own LogisticRegression module, however the goal of this kernel was to understand intrecately the structure of different algorithms, in this case, Logistic Regression. Stay tuned, for more of this kind of kernels. If you liked this kernel please leave an upvote, thank you.","2a7db130":"<h4>2. Maximum likelihood estimation<\/h4><br>\nOne step to optimize logistic regression is through likelihood estimation, the goal here is to **maximize the likelihood** we can achieve this through Gradient ascent, not to be mistaken from gradient descent. Gradient ascent is the same as gradient descent, except its goal is to maximize a function rather than minimizing it.<br>\nMaximum likelihood:\n![](https:\/\/imgur.com\/VCU0TKj.jpg)\nz is defined above","2b7c7740":"For Logistic Regression however here is the definition of the logistic function:<br>\n![](https:\/\/imgur.com\/903IYoN.jpg)\nWhere:\n* \u0398 = is the weight.","65644592":"**Dataset initialization**","3291582a":"![Foo](https:\/\/imgur.com\/10nqpqw.png)"}}