{"cell_type":{"ef21dae1":"code","2234b6ab":"code","9f25f90f":"code","77496e04":"code","c5e8534a":"code","b44b37cf":"code","e93aab10":"code","feadf51e":"code","d8549d5a":"code","3c65277b":"code","134ba3c5":"code","32a5242f":"code","4b77d68c":"code","015b708a":"code","81da32a0":"code","9f709974":"code","f484d801":"code","8652e05e":"code","e898e741":"code","021b160a":"code","cb3fff3f":"code","548c66cb":"code","0dd11692":"code","38f87b6d":"code","7e640f0f":"code","5ceb6de7":"code","919310a8":"code","d3d3f9e0":"code","4cfdc3b9":"code","384f8f46":"code","ba25d9e0":"markdown","a708d83d":"markdown","8108aea7":"markdown","1dbb2d0e":"markdown"},"source":{"ef21dae1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))","2234b6ab":"TRAIN_PATH = '..\/input\/train.csv'\n\n# Set columns to most suitable type to optimize for memory usage\ntraintypes = {'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\ncols = list(traintypes.keys())","9f25f90f":"with open(TRAIN_PATH) as file:\n    n_rows = len(file.readlines())\n\nprint (f'Exact number of rows: {n_rows}')","77496e04":"chunksize = 5_000_000 # 5 million rows at one go. Or try 10 million\ntotal_chunk = n_rows \/\/ chunksize + 1\nprint(f'Chunk size: {chunksize:,}\\nTotal chunks required: {total_chunk}')","c5e8534a":"df_list = [] # list to hold the batch dataframe\ni=0\n\nfor df_chunk in pd.read_csv(TRAIN_PATH, usecols=cols, dtype=traintypes, chunksize=chunksize):\n    \n    i = i+1\n    # Each chunk is a corresponding dataframe\n    print(f'DataFrame Chunk {i:02d}\/{total_chunk}')\n    \n    # Neat trick from https:\/\/www.kaggle.com\/btyuhas\/bayesian-optimization-with-xgboost\n    # Using parse_dates would be much slower!\n    df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n    df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n    \n    # Can process each chunk of dataframe here\n    # clean_data(), feature_engineer(),fit()\n    \n    # Alternatively, append the chunk to list and merge all\n    df_list.append(df_chunk) ","b44b37cf":"# Merge all dataframes into one dataframe\ntrain_df = pd.concat(df_list)\n\ndel df_list\n\ntrain_df.info()","e93aab10":"display(train_df.tail())","feadf51e":"train_df = pd.read_csv(\"..\/input\/train.csv\", nrows = 1000000)\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","d8549d5a":"# Search for missing values\nprint(train_df.isnull().sum())","3c65277b":"print('Old size: %d' % len(train_df))\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')\nprint('New size: %d' % len(train_df))","134ba3c5":"train_df['fare_amount'].value_counts()","32a5242f":"train_df['fare_amount'].describe()","4b77d68c":"# Delete the negative values\ntrain_df = train_df.drop(train_df[train_df['fare_amount'] < 0].index, axis=0)\ntrain_df.shape","015b708a":"train_df['passenger_count'].describe()","81da32a0":"# Of course it's impossible that 208 passengers are on a single taxy. Let's delete this row\ntrain_df = train_df.drop(train_df[train_df['passenger_count'] == 208].index, axis=0)","9f709974":"# Just curiosity, let's see the correlation map (actually the number of features is small, so nothing very useful here)\nsns.set_style('white')\nsns.set_context(\"paper\",font_scale=2)\ncorr = train_df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11,9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.3, center=0,\n           square=True, linewidths=0.5, cbar_kws={\"shrink\":0.5})","f484d801":"# Convert to datetime, so that we can split them in year, month, date, day and hour\ntrain_df['key'] = pd.to_datetime(train_df['key'])\ntrain_df['pickup_datetime']  = pd.to_datetime(train_df['pickup_datetime'])","8652e05e":"test_df['pickup_datetime']  = pd.to_datetime(test_df['pickup_datetime'])","e898e741":"data = [train_df, test_df]\nfor i in data: \n    i['Year'] = i['pickup_datetime'].dt.year\n    i['Month'] = i['pickup_datetime'].dt.month\n    i['Date'] = i['pickup_datetime'].dt.day\n    i['Day of Week'] = i['pickup_datetime'].dt.dayofweek\n    i['Hour'] = i['pickup_datetime'].dt.hour","021b160a":"train_df.columns","cb3fff3f":"test_df.columns","548c66cb":"train_df = train_df.drop(['key','pickup_datetime'], axis = 1)\ntest_df = test_df.drop(['key', 'pickup_datetime'], axis = 1)","0dd11692":"# Finally, prepare our data for the training phase\nx_train = train_df.iloc[:, train_df.columns != 'fare_amount']\ny_train = train_df['fare_amount'].values\nx_test = test_df","38f87b6d":"train_df.info()","7e640f0f":"'''from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nrf.fit(x_train, y_train)\nrf_predict = rf.predict(x_test)'''","5ceb6de7":"'''submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['fare_amount'] = rf_predict\nsubmission.to_csv('rnd_fst.csv', index=False)\nsubmission.head(20)'''","919310a8":"import lightgbm as lgbm","d3d3f9e0":"lgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse'\n}","4cfdc3b9":"pred_test_y = np.zeros(x_test.shape[0])\n\ntrain_set = lgbm.Dataset(x_train, y_train, silent=True)\n\nmodel = lgbm.train(lgbm_params, train_set = train_set, num_boost_round=300)\n\npred_test_y = model.predict(x_test, num_iteration = model.best_iteration)","384f8f46":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['fare_amount'] = pred_test_y\nsubmission.to_csv('lgbm_submission.csv', index=False)\nsubmission.head(20)","ba25d9e0":"## 2) LGBM","a708d83d":"# Otherwise, let's use only 10 million rows","8108aea7":"## 1) Random Forest (first model I tried) | Score: 3.69","1dbb2d0e":"# If we want to use the all dataset (55 m rows)\nI've got some problems when loading the all dataset (it runs out of time), so I found this interesting method in this kernel: https:\/\/www.kaggle.com\/szelee\/how-to-import-a-csv-file-of-55-million-rows"}}