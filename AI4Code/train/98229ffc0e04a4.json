{"cell_type":{"80a3b7dc":"code","3715c666":"code","30eb9d1e":"code","9c796bf2":"code","f9441327":"code","bf62bd61":"code","da2ad0d1":"code","1dcd26ac":"code","3acd73de":"code","a03e9a18":"markdown","8d331692":"markdown","19428c00":"markdown","eb1d087d":"markdown","e4dfd9d2":"markdown","5b68f43c":"markdown","f2695ef3":"markdown"},"source":{"80a3b7dc":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq","3715c666":"import pandas as pd\nfrom datasets import Dataset\nfrom sklearn.metrics import mean_squared_error\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer","30eb9d1e":"# disable W&B logging as we don't have access to the internet\n%env WANDB_DISABLED=True","9c796bf2":"model_checkpoint = '..\/input\/distillbert-huggingface-model'\nbatch_size = 16\nmax_length = 256","f9441327":"df = pd.read_csv('..\/input\/step-1-create-folds\/train_folds.csv') # https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\ndf = df.rename(columns={'target':'label'}) # HF expects this column name to pick up the target column in trainer\n\ntrain_dataset = Dataset.from_pandas(df[df.kfold != 0].reset_index(drop=True))\nvalid_dataset = Dataset.from_pandas(df[df.kfold == 0].reset_index(drop=True))\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n\ndef tokenize(batch): return tokenizer(batch['excerpt'], padding='max_length', truncation=True, max_length=max_length)\n\ntrain_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\nvalid_dataset = valid_dataset.map(tokenize, batched=True, batch_size=len(valid_dataset))","bf62bd61":"def model_init():\n    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1) # note this is actually a regression model\n\ndef compute_metrics(pred):\n    return {\n        'rmse': mean_squared_error(pred.label_ids, pred.predictions, squared=False),\n    }\n\n# hyperparameter tuning in this notebook: https:\/\/www.kaggle.com\/thedrcat\/commonlit-hf-trainer-hyperparameter-tuning\/\nargs = TrainingArguments(\n    \".\/tmp\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=9.734456575183276e-05,\n    fp16=True,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    seed=2,\n    weight_decay=0.006786875788460002,\n    load_best_model_at_end=True,\n)\n\ntrainer = Trainer(\n    model_init=model_init,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","da2ad0d1":"trainer.train()","1dcd26ac":"test_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_df = test_df.rename(columns={'target':'label'})\n\ntest_dataset = Dataset.from_pandas(test_df)\ntest_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n\ntest_preds = trainer.predict(test_dataset)","3acd73de":"sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsub.target = test_preds[0]\nsub.to_csv('submission.csv', index=False)\nsub.head()","a03e9a18":"## Minimalistic example to solve CommonLit with HuggingFace transformers and datasets\n\nI'd like to write the simplest possible notebook to train and infer a pretrained transformer model on the CommonLit data. To do this, I use HuggingFace transformers along with their trainer, and HuggingFace datasets to preprocess the data. I created an offline package for HF datasets so that you can use it during inference mode. \n\n### Please upvote if you find this helpful :) ","8d331692":"## Model and Training with HF transformers","19428c00":"## Config","eb1d087d":"## Test Inference","e4dfd9d2":"## Submission","5b68f43c":"### Please upvote if you find this helpful :) ","f2695ef3":"## Loading and preprocessing training data with HF datasets"}}