{"cell_type":{"68c8d0a7":"code","05b21301":"code","8d5759b0":"code","052a6a80":"code","e175cb43":"code","153544d8":"code","5acee931":"code","4cb901df":"code","4f4ab250":"code","adf129a6":"code","0a77e49b":"code","06570fd6":"code","e11642bb":"code","895d054a":"code","dc81f2b8":"code","ed3d8728":"code","abe64f3f":"code","2e37c706":"code","6ea5b1bf":"code","83510e51":"code","dd69534e":"code","46b4f720":"code","10f9b016":"code","1e952d8e":"code","467afef1":"code","d9026e4c":"code","8ad84ccc":"code","0bb7bce1":"code","642790f8":"code","2a90f46a":"code","ecc71f2e":"code","7121c5c6":"code","06ce5819":"code","deacc7b3":"code","95eeb004":"code","c5df14e4":"code","a58aa4f4":"code","da453936":"code","0187593f":"code","2099886b":"code","e9d8d511":"code","b29202b3":"code","448f8fe9":"code","e1ffe55f":"code","00378175":"code","7dda2bb8":"code","c77c1246":"code","b6b1279f":"code","cc47db77":"code","d5fee3a2":"code","82700ae1":"code","3d9b9ae1":"code","c52ad379":"code","1dba7746":"code","6c6ab309":"code","69f48948":"code","c33652f0":"code","3f3dd49d":"code","4f7e4361":"code","1d688888":"markdown","80e790dc":"markdown","762c511a":"markdown","4ee5d5b7":"markdown","bbc14d69":"markdown","8fa085c8":"markdown","9018bbce":"markdown","0cd2dd96":"markdown","852cdaf9":"markdown","c400d109":"markdown","cc500122":"markdown","86e8d52a":"markdown","b4da7b4b":"markdown","d51b1773":"markdown","9350dd24":"markdown","64f57d81":"markdown","306e87ca":"markdown","e400e671":"markdown","294594c1":"markdown","f9d60f47":"markdown","ff95dbbb":"markdown","7ab67f01":"markdown","86ce8f62":"markdown","911e66a4":"markdown","25b1f468":"markdown","54e4e860":"markdown","6c09ca5b":"markdown","da3f965b":"markdown","c323b960":"markdown"},"source":{"68c8d0a7":"# Let's download the necessary libraries.\nimport time\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport warnings\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore')","05b21301":"# the function where the data must return the necessary summary information\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","8d5759b0":"# given training set\ntrain = pd.read_csv('..\/input\/demande-forecasting\/train.csv', parse_dates=['date'])\ntrain.head()","052a6a80":"# given training set\ntest = pd.read_csv('..\/input\/demande-forecasting\/test.csv', parse_dates=['date'])\ntest.head()","e175cb43":"# We delete the first unnecessary column.\ntest=test.iloc[:,1:]\ntest.head()","153544d8":"# the format of the output requested from us.\nsample_submission = pd.read_csv('..\/input\/demande-forecasting\/sample_submission.csv')\nsample_submission.head()","5acee931":"# Let's combine the train and test data. \ndf = pd.concat([train, test], sort=False)\ndf.head()","4cb901df":"df[\"date\"].min(), df[\"date\"].max()","4f4ab250":"check_df(train)","adf129a6":"check_df(test)","0a77e49b":"check_df(df)","06570fd6":"# How is the sales distribution?\ndf[\"sales\"].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.90, 0.95, 0.99])","e11642bb":"# How many stores are there?\ndf[[\"store\"]].nunique()","895d054a":"# How many items are there?\ndf[[\"item\"]].nunique()","dc81f2b8":"# Is there an equal number of unique items in each store?\ndf.groupby([\"store\"])[\"item\"].nunique()","ed3d8728":"# Are there equal numbers of sales in each store?\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\"]})","abe64f3f":"# sales statistics in store-item breakdown\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","2e37c706":"def create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday \/\/ 4\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    return df","6ea5b1bf":"df = create_date_features(df)\npd.set_option('display.max_rows',None)","83510e51":"df.head()","dd69534e":"check_df(df)","46b4f720":"df.groupby([\"store\", \"item\", 'year',\"month\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\",'count']}).head(1000)\n","10f9b016":"# We add random noise to get past the overfit and add a generalization to the model.\ndef random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))","1e952d8e":"# We sort the products in the stores in shift store, item, date.\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)\ndf.head()","467afef1":"# let's look at the top 10 observations of the sale:\ndf[\"sales\"].head(10)","d9026e4c":"df[\"sales\"].shift(1).values[0:10]","8ad84ccc":"df[\"sales\"].shift(2).values[0:10]","0bb7bce1":"df[\"sales\"].shift(3).values[0:10]","642790f8":"pd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"lag1\": df[\"sales\"].shift(1).values[0:10],\n              \"lag2\": df[\"sales\"].shift(2).values[0:10],\n              \"lag3\": df[\"sales\"].shift(3).values[0:10],\n              \"lag4\": df[\"sales\"].shift(4).values[0:10]})","2a90f46a":"# Let's look at the sales in the store and item breakdown of our data that we have listed before.\npd.set_option('display.max_rows',500)\ndf.groupby([\"store\", \"item\"])['sales'].head()","ecc71f2e":"df.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(1))","7121c5c6":"# let's combine what we did above in a function.\ndef lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\n\ndf = lag_features(df, [91,98,105,112,119,126,28,364,546])\n\ncheck_df(df)","06ce5819":"# Here takes the average of itself and the previous or 2 previous values. \n# But I want him not to be included in the sale so that there is no bias.\npd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"roll2\": df[\"sales\"].rolling(window=2).mean().values[0:10],\n              \"roll3\": df[\"sales\"].rolling(window=3).mean().values[0:10],\n              \"roll5\": df[\"sales\"].rolling(window=5).mean().values[0:10]})","deacc7b3":"# So if we put shift here, it will take the average of the previous values.\npd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"roll2\": df[\"sales\"].shift(1).rolling(window=2).mean().values[0:10],\n              \"roll3\": df[\"sales\"].shift(1).rolling(window=3).mean().values[0:10],\n              \"roll5\": df[\"sales\"].shift(1).rolling(window=5).mean().values[0:10]})","95eeb004":"# let's combine what we've done into a function. \ndef roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=6, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\n# I have gathered these days in a list below in order to produce a variable by averaging \n# the previous values as well as the following days. While choosing these days, in order \n# to make a better estimation, I made many experiments and tried to catch certain\n# periods in our series.\ndf = roll_mean_features(df, [91,98,105,112,119,126,7,14,21,28,546,553,729,731])\ndf.head()","c5df14e4":"# Let's find out how we can entertain by making an example.\npd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"roll2\": df[\"sales\"].shift(1).rolling(window=2).mean().values[0:10],\n              \"ewm099\": df[\"sales\"].shift(1).ewm(alpha=0.99).mean().values[0:10],\n              \"ewm095\": df[\"sales\"].shift(1).ewm(alpha=0.95).mean().values[0:10],\n              \"ewm07\": df[\"sales\"].shift(1).ewm(alpha=0.7).mean().values[0:10],\n              \"ewm01\": df[\"sales\"].shift(1).ewm(alpha=0.1).mean().values[0:10]})","a58aa4f4":" \ndef ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe","da453936":"alphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91,98,105,112,119,126,7,14,21,28,364]\n\ndf = ewm_features(df, alphas, lags)","0187593f":"df.head()","2099886b":"df = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])","e9d8d511":"# Since I will build a model with lightgbm, I take the logarithm \n# of the sales variable to get the result in a shorter time.\ndf['sales'] = np.log1p(df[\"sales\"].values)","b29202b3":"# MAE: mean absolute error\n# MAPE: mean absolute percentage error\n# SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)\n# I define the measurement parameters requested from me.\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num \/ denom)) \/ n\n    return smape_val\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False","448f8fe9":"\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\n\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\nY_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]\n\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape","e1ffe55f":"# LightGBM parameters\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 15000,\n              'early_stopping_rounds': 200,\n              'nthread': -1}","00378175":"# metric mae: l1, absolute loss, mean_absolute_error, regression_l1\n# l2, square loss, mean_squared_error, mse, regression_l2, regression\n# rmse, root square loss, root_mean_squared_error, l2_root\n# mape, MAPE loss, mean_absolute_percentage_error\n\n# num_leaves: maximum number of leaves on a tree\n# learning_rate: shrinkage_rate, eta\n# feature_fraction: Random subspace feature of rf. \n#                  the number of variables to be considered randomly at each iteration.\n# max_depth: maximum depth\n# num_boost_round: n_estimators, number of boosting iterations. \n\n# early_stopping_rounds: If the metric in the validation set does not \n#                        progress at a certain early_stopping_rounds, that is, if the error \n#                        does not drop, stop modeling.\n#                        It both shortens the train time and prevents overfit.\n# nthread: num_thread, nthread, nthreads, n_jobs","7dda2bb8":"lgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=100)\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))","c77c1246":"def plot_lgb_importances(model, plot=False, num=10):\n\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:30])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n\n\nplot_lgb_importances(model, num=30)\nplot_lgb_importances(model, num=30, plot=True)\n\nlgb.plot_importance(model, max_num_features=30, figsize=(10, 10), importance_type=\"gain\")\nplt.show()","b6b1279f":"train = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]","cc47db77":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}\n ","d5fee3a2":"# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\n\ntest_preds = model.predict(X_test, num_iteration=model.best_iteration)\n","82700ae1":"test_preds","3d9b9ae1":"test.head()","c52ad379":"submission_df = test.loc[:, ['sales']]\n","1dba7746":"submission_df=submission_df.reset_index()","6c6ab309":"submission_df=submission_df.rename(columns = {'index':'id'})\nsubmission_df.head()","69f48948":"submission_df['sales'] = np.expm1(test_preds)","c33652f0":"submission_df['id'] = submission_df.id.astype(int)","3f3dd49d":"submission_df.to_csv('submission_demand.csv', index=False)","4f7e4361":"submission_df.head(20)","1d688888":"<div style=\"display:fill;\n            border-radius: false;\n            border-style: solid;\n            border-color:#000000;\n            border-style: false;\n            border-width: 2px;\n            color:;\n            font-size:10px;\n            font-family: Arial;\n            background-color:;\n            text-align:center;\n            letter-spacing:0.1px;\n            padding: 0.1em;\">\n\n**<h2>The average of previous values also has an effect on sales. Therefore, we add the average of the previous 2, 3 and 5 values to our data as a variable.**","80e790dc":"## TIME SERIES FORECASTING \nThe time series consists of observation values ordered by time. It is one of the applications that finds the most application area in the sector. Today, we will develop a model in which we estimate the sales of the last 3 months with the time series.\n\n![image.png](attachment:8264b3b3-3053-45ba-ac22-e42cc4f700a4.png)","762c511a":"**<h2><left><font color=\"green\">LightGBM Model<\/font>**","4ee5d5b7":"## **<h2><center><font color=\"red\">Model<\/font>**","bbc14d69":"## **<left><font color=\"green\">Business Problem<\/font>**","8fa085c8":"<div style=\"display:fill;\n            border-radius: false;\n            border-style: solid;\n            border-color:#000000;\n            border-style: false;\n            border-width: 2px;\n            color:;\n            font-size:10px;\n            font-family: Arial;\n            background-color:;\n            text-align:center;\n            letter-spacing:0.1px;\n            padding: 0.1em;\">\n\n**<h2>A chain of stores wants a 3-month demand forecast for its 10 different stores and 50 different products.**","9018bbce":"## **<left><font color=\"green\">Data Preparation<\/font>**","0cd2dd96":"**<h2><left><font color=\"green\">Converting sales to log(1+sales)<\/font>**","852cdaf9":"<div style=\"display:fill;\n            border-radius: false;\n            border-style: solid;\n            border-color:#000000;\n            border-style: false;\n            border-width: 2px;\n            color:;\n            font-size:10px;\n            font-family: Arial;\n            background-color:;\n            text-align:center;\n            letter-spacing:0.1px;\n            padding: 0.1em;\">\n\n**<h2>Since it is the closest sales that affected the sales the most, we need \nto shift to see the previous sales that affected it on the same line. shift(1) means shift once.**\n","c400d109":"**<h2><left><font color=\"green\">One-Hot Encoding<\/font>**","cc500122":"**<h2><left><font color=\"green\">Rolling Mean Features<\/font>**","86e8d52a":"<h1><left><font color=\"\">There\u2019s only one thing more precious than our time and that\u2019s who we spend it on.\n    \n\u2013 Leo Christopher \u2013<\/font>","b4da7b4b":"## **<h1><center><font color=\"red\">Final Model<\/font>**","d51b1773":"<div style=\"display:fill;\n            border-radius: false;\n            border-style: solid;\n            border-color:#000000;\n            border-style: false;\n            border-width: 2px;\n            color:;\n            font-size:10px;\n            font-family: Arial;\n            background-color:;\n            text-align:center;\n            letter-spacing:0.1px;\n            padding: 0.1em;\">\n    \n**<h2>Close days affect sales more, distant days affect less, so I add a weight to the variables I created.**","9350dd24":"## **<h><center><font color=\"red\">Exploratory Data Analysis<\/font>**","64f57d81":"**<h2><left><font color=\"green\">Lag\/Shifted Features<\/font>**","306e87ca":"<div style=\"display:fill;\n            border-radius: false;\n            border-style: solid;\n            border-color:#000000;\n            border-style: false;\n            border-width: 2px;\n            color:;\n            font-size:10px;\n            font-family: Arial;\n            background-color:;\n            text-align:center;\n            letter-spacing:0.1px;\n            padding: 0.1em;\">\n    \n**<h2>What is asked of me in the question is to predict the first 3 months of 2018. However, since these three months have no sales values, I need to test the model success first. For this reason, I take the first 4 years of the train set as a train and throw the first 3 months of 2017 as a dataframe called val in order to see my success.**","e400e671":"## **<left><font color=\"green\">Dataset Story<\/font>**","294594c1":"**<h2><left><font color=\"green\">Random Noise<\/font>**","f9d60f47":"## **<h2><center><font color=\"red\">Features Importants<\/font>**","ff95dbbb":"<div style=\"display:fill;\n            border-radius: false;\n            border-style: solid;\n            border-color:#000000;\n            border-style: false;\n            border-width: 2px;\n            color:;\n            font-size:10px;\n            font-family: Arial;\n            background-color:;\n            text-align:center;\n            letter-spacing:0.1px;\n            padding: 0.1em;\">\n\n**<h2>If we look at the totals of this output, we notice that it increases from the 1st month, then reaches the maximum in the 6th and 7th months, and then decreases again.\nWe notice that it increases again in the second year and decreases slightly towards the interval. From here we can see that it has a 1 year period, and a rising trend.**","7ab67f01":"**<h2><left><font color=\"green\">Exponentially Weighted Mean Features<\/font>**","86ce8f62":"**<h2><left><font color=\"green\">Time-Based Validation Sets<\/font>**","911e66a4":"**<h2><left><font color=\"green\">Custom Cost Function<\/font>**","25b1f468":"<div style=\"display:fill;\n            border-radius: false;\n            border-style: solid;\n            border-color:#000000;\n            border-style: false;\n            border-width: 2px;\n            color:;\n            font-size:10px;\n            font-family: Arial;\n            background-color:;\n            text-align:center;\n            letter-spacing:0.1px;\n            padding: 0.1em;\">\n\n**<h2>This dataset is presented to test different time series techniques.\nA store chain's 5-year data includes information on 10 different stores and 50 different products.**","54e4e860":"## **<left><font color=\"green\">Variables<\/font>**\n\n\n- **date**: History of sales data\n- **store**: Store ID\n- **item**: Product ID\n- **sales**: Number of products sold","6c09ca5b":"**<h1><center><font color=\"red\">Demand Forecasting<\/font>**\n    https:\/\/www.kaggle.com\/c\/demand-forecasting-kernels-only","da3f965b":"## **<h><center><font color=\"red\">FEATURE ENGINEERING<\/font>**","c323b960":"**<h2><left><font color=\"green\">Date Features<\/font>**"}}