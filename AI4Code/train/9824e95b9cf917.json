{"cell_type":{"0c087ea4":"code","0d536ce0":"code","774ebac1":"code","8332ceca":"code","b81ae4a7":"code","08f7334a":"code","8ef2a521":"code","05d806a2":"code","57b64ea4":"markdown"},"source":{"0c087ea4":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport optuna\nfrom optuna.pruners import SuccessiveHalvingPruner, MedianPruner\nfrom optuna.integration import XGBoostPruningCallback, LightGBMPruningCallback\nfrom optuna.distributions import *","0d536ce0":"# load data\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\n\nfeatures = [col for col in test.columns if 'f' in col]\nTARGET = 'claim'\n\ntrain['n_missing'] = train[features].isna().sum(axis=1)\n#test['n_missing'] = test[features].isna().sum(axis=1)\n\ntrain['std'] = train[features].std(axis=1)\n#test['std'] = test[features].std(axis=1)\n\nfeatures += ['n_missing', 'std']","774ebac1":"train[features] = train[features].fillna(train[features].mean())\n#test[features] = test[features].fillna(test[features].mean())\n\nscaler = StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\n#test[features] = scaler.transform(test[features])","8332ceca":"y_cv = train.pop(TARGET)\nX_cv = train.drop('id', axis = 1)","b81ae4a7":"TIMEOUT = int(3600*7)\nMODEL_NAME = 'lgb'","08f7334a":"def get_params(trial, model_name):\n    \n    if model_name == 'xgb':\n        trial_params = {\n            'max_depth': trial.suggest_int('max_depth', 2, 16),\n            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3),\n            'n_estimators': trial.suggest_int('n_estimators', 100, 200),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 512),\n            'subsample': trial.suggest_float('subsample', 0.0, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.0, 1.0),\n            'alpha': trial.suggest_int('alpha', 0, 100),\n            'lambda': trial.suggest_int('lambda', 1, 100),\n            'gamma': trial.suggest_float('gamma', 0.0, 0.1)}\n        \n        user_params = {\n            'use_label_encoder': False,\n            'tree_method': 'gpu_hist',\n            'predictor': 'gpu_predictor',\n            'random_state': 0,\n            'verbosity': 0,\n            'n_jobs':4,\n            'objective': 'binary:logistic'}\n        \n    elif model_name == 'lgb':\n        trial_params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 20000),\n            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3),\n            'num_leaves': trial.suggest_int('num_leaves', 20, 3000),\n            'max_depth': trial.suggest_int('max_depth', 3, 16),\n            'min_child_samples': trial.suggest_int('min_child_samples', 200, 10000, 100),\n            'reg_alpha': trial.suggest_int('reg_alpha', 0, 100, step=5),\n            'reg_lambda': trial.suggest_int('reg_lambda', 0, 100, step=5),\n            'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 10.0),\n            'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n            'subsample_freq': trial.suggest_int('subsample_freq', 1, 5),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0)}\n        \n        user_params = {\n            'n_jobs': 4,\n            'device': 'gpu',\n            'objective': 'binary'}\n        \n    elif model_name == 'cb':\n        trial_params = {}\n        \n    else:\n        trial_params = {}\n\n    return trial_params, user_params\n\n\ndef select_model(model_name):\n\n    if model_name == 'xgb':\n        model = XGBClassifier\n        \n    elif model_name == 'lgb':\n        model = LGBMClassifier\n        \n    elif model_name == 'cb':\n        model = CatBoostClassifier\n        \n    else:\n        model = None\n        \n    return model\n\n\ndef generate_fit_params(trial, model_name, X_train, y_train, X_val, y_val):\n    \n    if model_name == 'xgb':        \n        fit_params = {'X': X_train,\n                      'y': y_train,\n                      'eval_set': [(X_val, y_val)],\n                      'eval_metric': 'logloss',\n                      'early_stopping_rounds': 100,\n                      'verbose': 0,\n                      'callbacks': [XGBoostPruningCallback(trial, 'validation_0-logloss')]}\n        \n    elif model_name == 'lgb':\n        fit_params = {'X': X_train,\n                      'y': y_train,\n                      'eval_set': [(X_val, y_val)],\n                      'eval_metric': 'binary_logloss',\n                      'early_stopping_rounds': 100,\n                      'verbose': 0,\n                      'callbacks': [LightGBMPruningCallback(trial, 'binary_logloss')]}\n        \n    elif model_name == 'cb':\n        fit_params = {}\n        \n    else:\n        fit_params = {}\n    \n    return fit_params\n\n\ndef objective(trial, X, y, model_name, cv = None):\n    \n    '''\n    Args:\n        trial: an optuna trial\n        X: input features\n        y: target\n        search_params: optuna distributions\n        user_params: additional params\n        fit_params: passed to fit\n        cv: cross validations strategy\n    \n    '''\n    \n    if cv==None:\n        cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n    \n    trial_params, user_params = get_params(trial, model_name)\n    \n    if user_params!=None:\n        for k,v in user_params.items():\n            trial.set_user_attr(k,v)\n        params = {**trial_params, **user_params}\n    else:\n        params = trial_params\n    \n    cv_scores = []\n    \n    for idx, (train_idx, val_idx) in enumerate(cv.split(X,y)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        model = select_model(model_name)(**params)\n        model.fit(**generate_fit_params(trial = trial,\n                                        model_name = model_name, \n                                        X_train = X_train, \n                                        y_train = y_train, \n                                        X_val = X_val,\n                                        y_val = y_val))\n        \n        probas = model.predict_proba(X_val)\n        cv_scores.append(log_loss(y_val, probas))\n        \n    cv_score = np.mean(cv_scores)\n    \n    return cv_score\n","8ef2a521":"study = optuna.create_study(study_name = MODEL_NAME,\n                            direction='minimize',\n                            storage = 'sqlite:\/\/\/optuna.db',\n                            load_if_exists = True)\n\nstudy.optimize(lambda trial: objective(trial, X_cv, y_cv, model_name = MODEL_NAME),\n               timeout = TIMEOUT)","05d806a2":"best_trial = study.best_trial\nprint (f'Best Trial Number: {best_trial.number}')\nprint (f'Best Trial Score: {best_trial.value}')\nprint ('Best Trial Params:')\nprint ({**best_trial.params, **best_trial.user_attrs})","57b64ea4":"## Find optimal hyperparameters for XGB, LGB using Optuna + GPU"}}