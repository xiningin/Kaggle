{"cell_type":{"e6b9daed":"code","cb05ef3b":"code","49b34e0e":"code","d80a19d4":"code","57dfa509":"code","8f4ec626":"code","564b2e74":"code","f88d1031":"code","315714e9":"code","47b6dcd7":"code","30dc6c31":"code","332dbcab":"code","ce666d93":"code","de70a302":"code","f3878a2e":"code","5daeb9f4":"code","c2610a77":"code","3943889a":"code","e5fcf04f":"code","37fe92c6":"code","1456815a":"code","a551cba3":"code","9874e47b":"code","dfcc63e0":"code","0c92aa3e":"code","eaa8fd13":"code","2ca886e5":"code","d534cebd":"code","35f1d9e3":"code","0a0b15ed":"code","983cd3e1":"code","0cd0caab":"code","05d0208a":"code","c871808e":"code","7427cf6f":"code","3013ac48":"code","8bafada3":"code","15ac2fcd":"code","94aca5e7":"code","faa6a646":"code","7a741201":"code","335cc5dc":"code","62424221":"code","f5a39bcc":"code","ad2ea050":"code","87d89983":"code","d8726962":"code","75c0eada":"code","19ebd24c":"code","52395fc3":"code","598ec021":"code","22d7d6da":"code","3c5e2c6d":"code","ff5a6e39":"code","9e720419":"code","ea4ad744":"code","94e8232e":"code","ef702dcf":"code","828f2248":"code","915bfef9":"code","14b7c706":"markdown","7976d6c3":"markdown","eeda2743":"markdown","93597d22":"markdown","f717f772":"markdown","58dea0c2":"markdown","719cf998":"markdown","72c032ba":"markdown","a326246d":"markdown","5615ab84":"markdown","065ee9a5":"markdown","17b32fd0":"markdown","21437a07":"markdown","9014739e":"markdown","e9bf1e07":"markdown","8f300107":"markdown","1c1b2df5":"markdown","52311ca4":"markdown","4555960e":"markdown","49d09f06":"markdown","84f92840":"markdown","053a5086":"markdown","15d69926":"markdown","40f53ced":"markdown"},"source":{"e6b9daed":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","cb05ef3b":"import os\nos.listdir('..\/input')","49b34e0e":"PATH = 'competitive-data-science-predict-future-sales'\nitems = pd.read_csv('..\/input\/' + PATH + '\/items.csv')\nshops = pd.read_csv('..\/input\/' + PATH + '\/shops.csv')\ncats = pd.read_csv('..\/input\/' + PATH + '\/item_categories.csv')\ntrain = pd.read_csv('..\/input\/' + PATH + '\/sales_train.csv')\n# set index to ID to avoid droping it later\ntest  = pd.read_csv('..\/input\/' + PATH + '\/test.csv').set_index('ID')\n\nprint('------------- train info ------------') ; print(train.info(), '\\n')\nprint('------------- test info ------------') ; print(test.info(), '\\n')\nprint('------------- items info ------------') ; print(items.info(), '\\n')\nprint('------------- shops info ------------') ; print(shops.info(), '\\n')\nprint('------------- categories info ------------') ; print(cats.info(), '\\n')","d80a19d4":"print(train.isna().sum(), '\\n')\nprint(test.isna().sum())","57dfa509":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)\n\nprint(len(train[train.item_cnt_day>999]))\nprint(len(train[train.item_cnt_day>500]))\nprint(len(train[train.item_cnt_day<501]))\n\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1000]","8f4ec626":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 1000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","564b2e74":"train = train[train.item_price > 0].reset_index(drop=True)\ntrain[train.item_cnt_day <= 0].item_cnt_day.unique()\ntrain.loc[train.item_cnt_day < 1, 'item_cnt_day'] = 0","f88d1031":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 11, 'shop_id'] = 10\ntest.loc[test.shop_id == 11, 'shop_id'] = 10\n\ntrain.loc[train.shop_id == 40, 'shop_id'] = 39\ntest.loc[test.shop_id == 40, 'shop_id'] = 39","315714e9":"shops.shop_name.unique()","47b6dcd7":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\n\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops['category'] = shops['shop_name'].str.split(' ').map(lambda x:x[1]).astype(str)\n\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\n\ncategory = ['\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435,', '\u0422\u0426', '\u0422\u0420\u041a', '\u0422\u0420\u0426','\u0443\u043b.', '\u041c\u0430\u0433\u0430\u0437\u0438\u043d', '\u0422\u041a', '\u0441\u043a\u043b\u0430\u0434']\nshops.category = shops.category.apply(lambda x: x if (x in category) else 'etc')\nshops.category.unique()","30dc6c31":"shops.groupby(['category']).sum()","332dbcab":"category = ['\u0422\u0426', '\u0422\u0420\u041a', '\u0422\u0420\u0426', '\u0422\u041a']\nshops.category = shops.category.apply(lambda x: x if (x in category) else 'etc')\nprint('Category Distribution', shops.groupby(['category']).sum())","ce666d93":"shops['shop_city'] = shops.city\nshops['shop_category'] = shops.category\n\nshops['shop_city'] = LabelEncoder().fit_transform(shops['shop_city'])\nshops['shop_category'] = LabelEncoder().fit_transform(shops['shop_category'])\n\nshops = shops[['shop_id','shop_city', 'shop_category']]\nshops.head()","de70a302":"print(len(cats.item_category_name.unique()))\ncats.item_category_name.unique()","f3878a2e":"cats['type_code'] = cats.item_category_name.apply(lambda x: x.split(' ')[0]).astype(str)\ncats.loc[(cats.type_code == '\u0418\u0433\u0440\u043e\u0432\u044b\u0435') | (cats.type_code == '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b'), 'category'] = '\u0418\u0433\u0440\u044b'\ncats.loc[cats.type_code == 'PC', 'category'] = '\u041c\u0443\u0437\u044b\u043a\u0430'\n\ncategory = ['\u0418\u0433\u0440\u044b', '\u041a\u0430\u0440\u0442\u044b', '\u041a\u0438\u043d\u043e', '\u041a\u043d\u0438\u0433\u0438','\u041c\u0443\u0437\u044b\u043a\u0430', '\u041f\u043e\u0434\u0430\u0440\u043a\u0438', '\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b', '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435', '\u0427\u0438\u0441\u0442\u044b\u0435']\n\ncats['type_code'] = cats.type_code.apply(lambda x: x if (x in category) else 'etc')\n\nprint(cats.groupby(['type_code']).sum())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type_code'])\n\ncats['split'] = cats.item_category_name.apply(lambda x: x.split('-'))\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]","5daeb9f4":"import re\nfrom collections import Counter\nfrom operator import itemgetter\n\nitems = pd.read_csv('..\/input\/' + PATH + '\/items.csv')\n\nitems['name_1'], items['name_2'] = items['item_name'].str.split('[', 1).str\nitems['name_1'], items['name_3'] = items['item_name'].str.split('(', 1).str\n\nitems['name_2'] = items['name_2'].str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ').str.lower()\nitems['name_3'] = items['name_3'].str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ').str.lower()\nitems = items.fillna('0')\n\nresult_1 = Counter(' '.join(items['name_2'].values.tolist()).split(' ')).items()\nresult_1 = sorted(result_1, key=itemgetter(1))\nresult_1 = pd.DataFrame(result_1, columns=['feature', 'count'])\nresult_1 = result_1[(result_1['feature'].str.len() > 1) & (result_1['count'] > 200)]\n\nresult_2 = Counter(' '.join(items['name_3'].values.tolist()).split(\" \")).items()\nresult_2 = sorted(result_2, key=itemgetter(1))\nresult_2 = pd.DataFrame(result_2, columns=['feature', 'count'])\nresult_2 = result_2[(result_2['feature'].str.len() > 1) & (result_2['count'] > 200)]\n\nresult = pd.concat([result_1, result_2])\nresult = result.drop_duplicates(subset=['feature']).reset_index(drop=True)\n\nprint('Most common aditional features:', result)\n\ndef name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x\n\nitems['item_name'] = items['item_name'].apply(lambda x: name_correction(x))\nitems.name_2 = items.name_2.apply(lambda x: x[:-1] if x != '0' else '0')","c2610a77":"items['type'] = items.name_2.apply(lambda x: x[0:8] if x.split(' ')[0] == 'xbox' else x.split(' ')[0])\nitems.loc[(items.type == 'x360') | (items.type == 'xbox360'), 'type'] = 'xbox 360'\nitems.loc[items.type == '', 'type'] = 'mac'\nitems.type = items.type.apply(lambda x: x.replace(' ',''))\nitems.loc[(items.type == 'pc') | (items.type == 'p\u0441') | (items.type == '\u0440\u0441'), 'type'] = 'pc'\nitems.loc[(items.type == '\u0440s3'), 'type'] = 'ps3'\n\ngroup_sum = items.groupby('type').sum()\ngroup_sum.loc[group_sum.item_category_id < 200]","3943889a":"drop_list = ['5c5', '5c7', '5f4', '6dv', '6jv', '6l6', 'android', 'hm3', 'j72', 'kf6', 'kf7','kg4',\n            'ps2', 's3v', 's4v'\t,'\u0430\u043d\u0433\u043b', '\u0440\u0443\u0441\u0441\u043a\u0430\u044f', '\u0442\u043e\u043b\u044c\u043a\u043e', '\u0446\u0438\u0444\u0440\u043e']\n\nitems.name_2 = items.type.apply(lambda x: 'etc' if x in drop_list else x)\nitems = items.drop(['type'], axis=1)\nitems.groupby('name_2').sum()","e5fcf04f":"items.head()","37fe92c6":"items['name_2'] = LabelEncoder().fit_transform(items['name_2'])\nitems['name_3'] = LabelEncoder().fit_transform(items['name_3'])\nitems.drop(['item_name', 'name_1'], axis=1, inplace=True)\nitems.head()","1456815a":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","a551cba3":"print(matrix)\ntrain.head()","9874e47b":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","dfcc63e0":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)#.astype(np.float16))\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\ntime.time() - ts","0c92aa3e":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","eaa8fd13":"ts = time.time()\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\ntime.time() - ts","2ca886e5":"ts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['shop_city'] = matrix['shop_city'].astype(np.int8)\nmatrix['shop_category'] = matrix['shop_category'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\ntime.time() - ts","d534cebd":"matrix.head()","35f1d9e3":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","0a0b15ed":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3], 'item_cnt_month')\ntime.time() - ts","983cd3e1":"ts = time.time()\ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","0cd0caab":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","05d0208a":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","c871808e":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","7427cf6f":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","3013ac48":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'itme_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_item_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'itme_id'], how='left')\nmatrix['date_shop_item_avg_item_cnt'] = matrix['date_shop_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_item_avg_item_cnt')\nmatrix.drop(['date_shop_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","8bafada3":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","15ac2fcd":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_city'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","94aca5e7":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","faa6a646":"# ts = time.time()\n# group = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\n# group.columns = [ 'date_type_avg_item_cnt' ]\n# group.reset_index(inplace=True)\n\n# matrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\n# matrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\n# matrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\n# matrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\n# time.time() - ts","7a741201":"# ts = time.time()\n# group = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\n# group.columns = [ 'date_subtype_avg_item_cnt' ]\n# group.reset_index(inplace=True)\n\n# matrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\n# matrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\n# matrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\n# matrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\n# time.time() - ts","335cc5dc":"ts = time.time()\ngroup = matrix.groupby(['date_block_num','shop_id' ,'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id' ,'item_id'], how='left')\nmatrix['date_shop_item_avg_item_cnt'] = matrix['date_shop_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_item_avg_item_cnt')\nmatrix.drop(['date_shop_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","62424221":"ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts","f5a39bcc":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","ad2ea050":"matrix['month'] = matrix['date_block_num'] % 12\n\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","87d89983":"# ts = time.time()\n# cache = {}\n# matrix['item_shop_last_sale'] = -1\n# matrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\n# for idx, row in matrix.iterrows():    \n#     key = str(row.item_id)+' '+str(row.shop_id)\n#     if key not in cache:\n#         if row.item_cnt_month!=0:\n#             cache[key] = row.date_block_num\n#     else:\n#         last_date_block_num = cache[key]\n#         matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n#         cache[key] = row.date_block_num         \n# time.time() - ts","d8726962":"# ts = time.time()\n# cache = {}\n# matrix['item_last_sale'] = -1\n# matrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\n# for idx, row in matrix.iterrows():    \n#     key = row.item_id\n#     if key not in cache:\n#         if row.item_cnt_month!=0:\n#             cache[key] = row.date_block_num\n#     else:\n#         last_date_block_num = cache[key]\n#         if row.date_block_num>last_date_block_num:\n#             matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n#             cache[key] = row.date_block_num         \n# time.time() - ts","75c0eada":"ts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","19ebd24c":"ts = time.time()\nmatrix = matrix[matrix.date_block_num > 3]\ntime.time() - ts","52395fc3":"ts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)\ntime.time() - ts","598ec021":"matrix.info()\n\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","22d7d6da":"matrix.to_pickle('..\/working\/data.pkl')\n\ndel matrix\ngc.collect();","3c5e2c6d":"import os\nimport gc\nimport pickle\nimport time\nimport pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4","ff5a6e39":"data = pd.read_pickle('..\/working\/data.pkl')\ntest  = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')\n\nprint(len(data.columns))\ndata.columns","9e720419":"data = data[[\n   'date_block_num', 'shop_id', 'item_id', 'item_cnt_month', 'shop_city',\n       'shop_category', 'item_category_id', 'name_2', 'name_3', 'type_code',\n       'subtype_code', 'item_cnt_month_lag_1', 'item_cnt_month_lag_2',\n       'item_cnt_month_lag_3', 'date_avg_item_cnt_lag_1',\n       'date_item_avg_item_cnt_lag_1', 'date_item_avg_item_cnt_lag_2',\n       'date_item_avg_item_cnt_lag_3', 'date_shop_avg_item_cnt_lag_1',\n       'date_shop_avg_item_cnt_lag_2', 'date_shop_avg_item_cnt_lag_3',\n       'date_cat_avg_item_cnt_lag_1', 'date_shop_cat_avg_item_cnt_lag_1',\n       'date_shop_subtype_avg_item_cnt_lag_1', 'date_city_avg_item_cnt_lag_1',\n       'date_item_city_avg_item_cnt_lag_1', \n#     'date_type_avg_item_cnt_lag_1',\n#        'date_subtype_avg_item_cnt_lag_1', \n    'date_shop_item_avg_item_cnt_lag_1',\n    'delta_price_lag',\n       'delta_revenue_lag_1', 'month', 'days', \n#     'item_shop_last_sale',\n#        'item_last_sale', \n    'item_shop_first_sale', \n    'item_first_sale'\n]]\n\nlen(data.columns)","ea4ad744":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","94e8232e":"del data\ngc.collect();","ef702dcf":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=10,\n    n_estimators=1000,\n    min_child_weight=0.5, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.1,\n    tree_method='gpu_hist',\n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 20)\n\ntime.time() - ts","828f2248":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","915bfef9":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))","14b7c706":"## Drop Outliers\ncheck NaN values and below zero values <br>\nWe drop outliers which price > 10^5 and sales > 1000","7976d6c3":"## Monthly sales\nTest set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. Hence, for the most of the items in the test set target value should be zero. \nIn the other hand train set contains only pairs which were sold or returned in the past. Tha main idea is to calculate monthly sales and <b>extend it with zero sales<\/b> for each unique pair within the month. This way train data will be similar to test data.","eeda2743":"And then, drop types","93597d22":"Producing lags brings a lot of nulls.","f717f772":"## Category dataset preprocessing","58dea0c2":"## Mean encoded features<br><br>","719cf998":"Let's create type of item_names. First, cleansing results of name_2. <br>\nConcatenate same meaning of types such as, 'x360' & 'xbox360' -> xbox 360 <br>\nThree words 'pc' in below code look like same. But, after label encoding they transformed different values.","72c032ba":"However, some categories have small values. So we reduce categories 9 to 5.<br>\n['\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435,', '\u0422\u0426', '\u0422\u0420\u041a', '\u0422\u0420\u0426','\u0443\u043b.', '\u041c\u0430\u0433\u0430\u0437\u0438\u043d', '\u0422\u041a', '\u0441\u043a\u043b\u0430\u0434', 'etc'] => ['\u0422\u0426', '\u0422\u0420\u041a', '\u0422\u0420\u0426', '\u0422\u041a', 'etc']**","a326246d":"This notebook is advanced version of the [Feature engineering, xgboost](https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost). <br>\nWe descrive our contribution below pipline \n\n#### Pipline\n* load data\n* heal data and remove outliers\n* ## (Update) add extra features using shops\/items\/cats features\n* ## (Update) categorize shops\/items\/cats objects and features\n* create matrix as product of item\/shop pairs within each month in the train set\n* get monthly sales for each item\/shop pair in the train set and merge it to the matrix\n* clip item_cnt_month by (0,20)\n* append test to the matrix, fill 34 month nans with zeros\n* merge shops\/items\/cats to the matrix\n* add target lag features\n* add mean encoded features\n* add price trend features\n* add month & days\n* add months since last sale\/months since first sale features\n* cut first year and drop columns which can not be calculated for the test set\n* ## (Update) select best features\n* set validation strategy 34 test, 33 validation, less than 33 train\n* ##  (Update) fine tuning XGB model and training with GPU\n* fit the model, predict and clip targets for the test set","5615ab84":"<h3> We recommend creating another notebook to build xgb model because of saving ur time :). <br>\n\n## Build XGB Models (* with GPU *) <br>\nIf u want to use 'CPU' mode, eliminate argument tree_method='gpu_hist'. (But we recommend 'GPU' mode)","065ee9a5":"Add month, and days in a month.","17b32fd0":"Aggregate train set by shop\/item pairs to calculate target aggreagates, then <b>clip(0,20)<\/b> target value. This way train target will be similar to the test predictions.<br>\nhttps:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/discussion\/50149#latest-287470\n\n<i>I use floats instead of ints for item_cnt_month to avoid downcasting it after concatination with the test set later. If it would be int16, after concatination with NaN values it becomes int64, but foat16 becomes float16 even with NaNs.<\/i>","21437a07":"We think that category '\u0418\u0433\u0440\u043e\u0432\u044b\u0435 \u043a\u043e\u043d\u0441\u043e\u043b\u0438' and  '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b' are same as '\u0418\u0433\u0440\u044b'. <br>\nSo, we transform the two features to '\u0418\u0433\u0440\u044b'<br>\nAlso, PC - \u0413\u0430\u0440\u043d\u0438\u0442\u0443\u0440\u044b\/\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438 and \nchange to \u041c\u0443\u0437\u044b\u043a\u0430 - \u0413\u0430\u0440\u043d\u0438\u0442\u0443\u0440\u044b\/\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438 <br><br>","9014739e":"Several shops are duplicates of each other (according to its name). Fix train and test set.<br>\nWe add 40 to 39.","e9bf1e07":"## Category dataset preprocessing <br>\nThis code get from [1st place solution - Part 1 - \"Hands on Data\"](https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data). <bt><br>\nWe use features 'name_2' and 'name_3' below code.","8f300107":"There is one item with price below zero. Eliminate outlier <br>\n\nAlso, there are some negative values in item_cnt_day.<br>\n-1.0 : 7252 ,\n-2.0 : 78 ,\n-3.0 : 14 ,\n-4.0 : 3 ,\n-5.0 : 4 ,\n-6.0 : 2 ,\n-9.0 : 1 ,\n-16.0 : 1 ,\n-22.0 : 1 ,<br>\nWe decide to change all of them to 0","1c1b2df5":"## Traget lags\nWe use lag_feature : 3","52311ca4":"## Shops dataset preprocessing","4555960e":"Select best features","49d09f06":"## Final preparations\nBecause of the using 3 as lag value drop first 3 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).","84f92840":"We categorize subtype of shops in ['\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435,' '\u0422\u0426' '\u0422\u0420\u041a' '\u0422\u0420\u0426', '\u0443\u043b.' '\u041c\u0430\u0433\u0430\u0437\u0438\u043d' '\u0422\u041a' '\u0441\u043a\u043b\u0430\u0434' ] <br>\nThen transform other values to 'etc'","053a5086":"## Trend Features","15d69926":"## Shops\/Items\/Cats features","40f53ced":"## Test set\nTo use time tricks append test pairs to the matrix."}}