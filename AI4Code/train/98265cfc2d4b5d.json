{"cell_type":{"6afc2d08":"code","079aa112":"code","53f537d7":"code","3306b8e9":"code","79e882e6":"code","bd645a63":"code","6b476c9a":"code","50720d13":"code","22d2c674":"code","46a25ca7":"code","d3dc8f41":"code","962abc86":"code","23e67ce0":"code","8bb335cd":"code","3be552a5":"code","28105a44":"code","93e54248":"code","e7ad60c9":"code","230eb0da":"markdown","74c8e4f8":"markdown"},"source":{"6afc2d08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib as mpl\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","079aa112":"train_df = pd.read_csv(\"..\/input\/titanicprocesseddata\/data.csv\") \ntrain_df.head()","53f537d7":"train_df.drop(['Unnamed: 0'], axis=1,inplace=True)\ntrain_df.head()","3306b8e9":"test_df = pd.read_csv(\"..\/input\/titanic\/test.csv\") \ntest_df.head()","79e882e6":"# Check for null values in columns\ntest_df.isnull().sum()","bd645a63":"test_df['Age'].fillna((train_df['Age'].median()), inplace=True)\ntest_df['Fare'].fillna((train_df['Fare'].median()), inplace=True)\ntest_df['Cabin_Available'] = np.where(test_df.Cabin.notna(), 1, 0)\n\ntest_df.isnull().sum()","6b476c9a":"feature_names = ['Age', 'Gender_male', 'Gender_female', 'Pclass','SibSp','Parch','Fare','Cabin_Available', 'EmbarkedFrom_C','EmbarkedFrom_Q','EmbarkedFrom_S']\nX = train_df[feature_names]\ny = train_df['Survived']\n\n# one-hot encoding test categorical data retaining the Sex & Embarked columns \ntest_df['Gender'] = test_df['Sex']\ntest_df['EmbarkedFrom'] = test_df['Embarked']\ntest_df = pd.get_dummies(test_df, columns=['Gender','EmbarkedFrom'])\ntest_df.head()\n\nX_test_final = test_df[feature_names]\nX_test_final[0:1]","50720d13":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_test_final = scaler.transform(X_test_final)\nX_test_final[:1]","22d2c674":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'\n     .format(logreg.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'\n     .format(logreg.score(X_test, y_test)))","46a25ca7":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier().fit(X_train, y_train)\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))","d3dc8f41":"# K-Nearest Neighbour (KNN)\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'\n     .format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'\n     .format(knn.score(X_test, y_test)))","962abc86":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\nprint('Accuracy of GNB classifier on training set: {:.2f}'\n     .format(gnb.score(X_train, y_train)))\nprint('Accuracy of GNB classifier on test set: {:.2f}'\n     .format(gnb.score(X_test, y_test)))","23e67ce0":"# SVM\nfrom sklearn.svm import SVC\nsvm = SVC()\nsvm.fit(X_train, y_train)\nprint('Accuracy of SVM classifier on training set: {:.2f}'\n     .format(svm.score(X_train, y_train)))\nprint('Accuracy of SVM classifier on test set: {:.2f}'\n     .format(svm.score(X_test, y_test)))","8bb335cd":"# Random forest.\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_jobs=10, random_state=0)\n\nrf.fit(X_train, y_train)\nprint('Accuracy of Random Forest classifier on training set: {:.2f}'\n     .format(rf.score(X_train, y_train)))\nprint('Accuracy of Random Forest classifier on test set: {:.2f}'\n     .format(rf.score(X_test, y_test)))\n\n# \npred = rf.predict(X_test_final)\npred_ds = test_df[['PassengerId']]\npred_ds['Survived'] = pred\npred_ds.head()\npred_ds.to_csv('submission3.csv', index=False)","3be552a5":"# Ada Boost.\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nab = AdaBoostClassifier(n_estimators=500)\n\nab.fit(X_train, y_train)\nprint('Accuracy of Ada Boost classifier on training set: {:.2f}'\n     .format(ab.score(X_train, y_train)))\nprint('Accuracy of Ada Boost classifier on test set: {:.2f}'\n     .format(ab.score(X_test, y_test)))","28105a44":"# XGBoost\nfrom xgboost import XGBClassifier\nxb = XGBClassifier(n_estimators=500)\nxb.fit(X_train, y_train)\nprint('Accuracy of XG Boost classifier on training set: {:.2f}'\n     .format(xb.score(X_train, y_train)))\nprint('Accuracy of XG Boost classifier on test set: {:.2f}'\n     .format(xb.score(X_test, y_test)))\n\npred = xb.predict(X_test_final)\npred_ds = test_df[['PassengerId']]\npred_ds['Survived'] = pred\npred_ds.head()\npred_ds.to_csv('submission4.csv', index=False)","93e54248":"# Plot Confusion Matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","e7ad60c9":"# Confusion Matrix for KNN\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\npred = knn.predict(X_test)\n\ncnf_matrix = confusion_matrix(y_test, pred)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[0,1],\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[0,1], normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()\n\nprint(cnf_matrix)\nprint(classification_report(y_test, pred))","230eb0da":"<h1>The purpose is this notebook is to try out different models that will run on pre-processed csv file generated & uploaded<\/h1>","74c8e4f8":"<h2> Evaluating Different Models <\/h2>"}}