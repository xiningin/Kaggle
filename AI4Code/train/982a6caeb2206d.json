{"cell_type":{"7961f456":"code","5c644dd6":"code","164f6450":"code","02629429":"code","d1a57a65":"code","465f40ce":"code","9e479250":"code","f2feed38":"code","fb966569":"code","a7a84f2a":"markdown","083e4960":"markdown","758620bb":"markdown","b7dc82f4":"markdown","5cefffb1":"markdown","e20f92c5":"markdown","8863b418":"markdown","e17c9576":"markdown","945288d9":"markdown","c64a5970":"markdown"},"source":{"7961f456":"!pip install kneed\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom kneed import KneeLocator\nfrom sklearn.metrics import silhouette_score\n\nrcParams['figure.figsize'] = 18, 14\n\n","5c644dd6":"data = pd.read_csv('..\/input\/wine-dataset-for-clustering\/wine-clustering.csv')\nprint(data.head())\nprint()\nprint('Shape:')\nprint(data.shape)","164f6450":"print(data.info())\nprint('Number of unique values:')\nprint(data.nunique())","02629429":"f, axes = plt.subplots(4, 3)\nfor column, ax in zip(data.columns, axes.flatten()):\n        sns.boxplot(x=data[column], ax=ax)","d1a57a65":"corr = data.corr()\nsns.heatmap(corr, annot=True)","465f40ce":"pca = PCA()\npca.fit(data)\nnp.set_printoptions(precision=7, suppress=True)\nprint(pca.explained_variance_ratio_)\npca = PCA(n_components=2)\nprint()\ndata = pca.fit_transform(data)\nprint(pca.explained_variance_ratio_)\n\n\n","9e479250":"wcss=[]\nfor i in range(1,12):\n    kmeans = KMeans(i, random_state=0)\n    kmeans.fit(data)\n    wcss_iter = kmeans.inertia_\n    wcss.append(wcss_iter)\n\nnumber_clusters = range(1,12)\nplt.plot(number_clusters,wcss)\nplt.title('The Elbow title')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')","f2feed38":"kl = KneeLocator(range(1, 12), wcss, curve=\"convex\", direction=\"decreasing\")\nprint('Best number of clusters:', kl.elbow)","fb966569":"fig, ax = plt.subplots(2, 2)\nax = np.ravel(ax)\ncluster_number = 3\nfor i in range(4):\n    km = KMeans(n_clusters=cluster_number, random_state=0)\n    labels = km.fit_predict(data)\n    u_labels = np.unique(labels)\n    centroids = km.cluster_centers_\n    for j in u_labels:\n        ax[i].scatter(data[labels == j , 0] , data[labels == j , 1] , label = j)\n    ax[i].legend()\n    ax[i].scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')\n    ax[i].set_title('Silhouette Coefficient:' + str(silhouette_score(data, labels))[:6])\n    cluster_number += 1\n    \n","a7a84f2a":"Best number of clusters is 3. Now we can use one more metric to define cluster number - Silhouette Coefficient. Plot our clusters with their silhouette score. Choose 3,4,5 and 6 clusters to get results.","083e4960":"Another metric shows us that the best number of clusters is 3. In conclusion we see that using only two PCA components and three clusters we get pretty good results in this clustering task.","758620bb":"We did a short research and now let's get down to our topic. As we saw earlier wine dataset is quite small and using PCA is not necessary. But why not to implement this method and look at the results.","b7dc82f4":"### In this notebook I provide simple clustering solution for wine dataset.","5cefffb1":"We see that distributions are almost perfect and we don't need to deal with outliers. There are only numerical columns, that's why let's build corr matrix for our dataset.","e20f92c5":"Then let's find out how many missing values are in the data and number of unique values.","8863b418":"We get our curve and now can make predictions how many clusters are the best suited for our data. Or we can do it automatically.","e17c9576":"One of the most important tasks is choosing number of components in PCA. We fit our model and using varience_ratio to saw, how our components describe data varience. Using only 2 components we can get 99,9% of our dataset varience in this case. Second important task is choosing cluster number in our KMeans algorithm. For this we will use elbow method and get sum of squared distances of samples to their closest cluster center as a metric.","945288d9":"First of all, look at our data.","c64a5970":"Very nice data :) Now it's time to visualize data distribution."}}