{"cell_type":{"4b52f920":"code","2ef85888":"code","587596bf":"code","483ebc88":"code","cd0912fb":"code","fae570b4":"code","3ef14b30":"code","52720ab5":"code","62b175cb":"code","db837b8e":"code","241aaf45":"code","16a80017":"code","860194f7":"code","29a2baee":"code","e51eb6fc":"code","b3c4dd10":"code","f93cd6bc":"code","cc685d6c":"code","66a81549":"code","e51d98b5":"code","02cd9791":"code","4ef9c108":"code","1e3deeb2":"code","8a805b31":"code","7b619dc9":"code","7e85b7c1":"code","6384f48f":"code","4fa71045":"code","0dd0f0a9":"code","552e8857":"code","37ba88c1":"code","2957fb55":"code","549689fe":"code","ac4d046c":"code","e0251515":"markdown","5330dd55":"markdown","d45fe1a8":"markdown","f591fcf6":"markdown","bb605f2d":"markdown","cd811e62":"markdown","d4f41e8d":"markdown","ce26bbf4":"markdown","72b3c2bd":"markdown","f7ce3955":"markdown","e294acb1":"markdown","922ffadd":"markdown","70793fc3":"markdown","3daffed9":"markdown","8319aef6":"markdown","dfe1d8aa":"markdown","d59c81c2":"markdown","1cfae915":"markdown","5f3b9c40":"markdown","6d727272":"markdown","0e72dd67":"markdown","cfdc723d":"markdown","ebab6020":"markdown","af2901fb":"markdown","64852a65":"markdown"},"source":{"4b52f920":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2ef85888":"!pip install ktrain","587596bf":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nimport ktrain\nfrom ktrain import text\n\n\nimport matplotlib as mpl\nfrom cycler import cycler\nplt.style.use('ggplot')","483ebc88":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\ntrain.head(3)","cd0912fb":"train.shape, test.shape","fae570b4":"# check for class distribution\nsns.countplot(train['target'])\n\n# class(0) :- No Disaster\n# class(1) :- Disaster","3ef14b30":"# check no of character in tweet\nplt.style.use('seaborn-dark')\nfig, ax = plt.subplots(1,2,figsize=(10,5))\n\n# for target 0 \nax[0].hist(train[train['target']==0]['text'].str.len(),color='b',bins=20)\nax[0].set_title(\"Not Disaster Tweets len\");\n\n# for target 1\nax[1].hist(train[train['target']==1]['text'].str.len(),color='c',bins=20)\nax[1].set_title(\"Disaster Tweets len\");\n","52720ab5":"# check no of character in tweet\nfig, ax = plt.subplots(1,2,figsize=(10,5))\n\n# for target 0 \nax[0].hist(train[train['target']==0]['text'].str.split().map(lambda x:len(x)),color='b',bins=20)\nax[0].set_title(\"Not Disaster Tweets len\");\n\n# for target 1\nax[1].hist(train[train['target']==1]['text'].str.split().map(lambda x:len(x)),color='c',bins=20)\nax[1].set_title(\"Disaster Tweets len\");\n\nplt.show()\n","62b175cb":"def create_corpus(target):\n    corpus = []\n    for x in train[train['target'] == target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\n# for class 0\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop = sorted(dic.items(),key=lambda x:x[1], reverse=True)[:10]\n\n\nx, y = zip(*top)\n# print(x,y)\nb = pd.DataFrame({\n    'value':x,\n    'count':y\n})\n\n# b.head()\nsns.barplot(x='value',y='count',data=b);\n","db837b8e":"# for class 1\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx,y=zip(*top)\nc = pd.DataFrame({\n    'value':x,\n    'count':y\n})\n\n# b.head()\nsns.barplot(x='value',y='count',data=c)","241aaf45":"# for train\nsns.heatmap(train.isnull());\n\n# location and so many null values","16a80017":"# for test\nsns.heatmap(test.isnull());\n\n# location and so many null values","860194f7":"import nltk \nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","29a2baee":"def clean(text):\n\n    #     remove urls\n    text = re.sub(r'https?:\/\/\\S+|www\\.\\S+', \" \", text)\n\n    #     remove mentions\n    text = re.sub(r'@\\w+',' ',text)\n\n    #     remove hastags\n    text = re.sub(r'#\\w+', ' ', text)\n\n    #     remove digits\n    text = re.sub(r'\\d+', ' ', text)\n\n    #     remove html tags\n    text = re.sub('r<.*?>',' ', text)\n    \n    #remove puct\n    text = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n    \n    #     remove stop words \n    text = text.split()\n    text = \" \".join([word for word in text if not word in stop_words])\n    \n      \n    return text","e51eb6fc":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","b3c4dd10":"train['text'] = train['text'].apply(lambda x:clean(x))\ntrain['text'] = train['text'].apply(lambda x:remove_emoji(x))\n\ntest['text'] = test['text'].apply(lambda x:clean(x))\ntest['text'] = test['text'].apply(lambda x:remove_emoji(x))","f93cd6bc":"train.head()","cc685d6c":"df_train = train.iloc[:,3:].copy()\ndf_test = test.iloc[:,3:].copy()","66a81549":"df_train.head()","e51d98b5":"df_test.head()","02cd9791":"(X_train,y_train),(X_test,y_test),preprocess = text.texts_from_df(train_df=df_train, text_column='text',\n                  label_columns='target',\n                   val_df= df_test,\n                    maxlen=400,\n                   preprocess_mode='bert'\n                  )","4ef9c108":"model = text.text_classifier(name='bert',\n                            train_data = (X_train,y_train),\n                            preproc=preprocess)","1e3deeb2":"# get learning rate\nlearner = ktrain.get_learner(model=model,\n                            train_data = (X_train,y_train),\n                             val_data=(X_test,y_test),\n                             batch_size = 6\n                            )","8a805b31":"learner.lr_find(max_epochs=2)\nlearner.lr_plot()","7b619dc9":"learner.fit_onecycle(lr=2e-5,epochs=2)","7e85b7c1":"learner","6384f48f":"predictor = ktrain.get_predictor(learner.model, preprocess)","4fa71045":"y_list = []","0dd0f0a9":"for i in range(len(df_test['text'])):\n    text = df_test['text'][i]\n    y_pred =  predictor.predict(text)\n    if y_pred == 'target':\n        y_list.append(1)\n    else:\n        y_list.append(0)","552e8857":"len(y_list)\n","37ba88c1":"sns.countplot(y_list)","2957fb55":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","549689fe":"sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","ac4d046c":"sub.head()","e0251515":"- Reading the submission file","5330dd55":"### What is bert\n- BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model developed by Google. It represented one of the major machine learning breakthroughs of the year, as it achieved state-of-the-art results across 11 different Natural Language Processing (NLP) tasks. Moreover, Google open-sourced the code and made pretrained models available for download similar to computer vision models pretrained on ImageNet. For these reasons, there continues to be a great deal of interest in BERT (even as other models slightly overtake it).","d45fe1a8":"### Importing required libraries","f591fcf6":"## check the shape of the data","bb605f2d":"### Apply text cleaning on datasets","cd811e62":"## Most Important Step (Text clearning)\n- Remove Urls\n- Remove Mentions\n- Remove Hastags\n- Remove HTML tags\n- Remove Punctuations\n- Remove Stop Words","d4f41e8d":"## Check for null values","ce26bbf4":"## Data(Text) Cleaning","72b3c2bd":"- Now time for prediction","f7ce3955":"- Finding the best learning rate for the model\n\n#### Note:- It will take a lot of time even on GPU (in my case it took more then 2 hour)","e294acb1":"# If you like the kernal please do a UpVote","922ffadd":"## No of words in tweet","70793fc3":"## No of character in tweet","3daffed9":"### load the dataset check head","8319aef6":"- for text classification we are usign BERT so need to pass name = 'bert'\n- passing our train_data \n- then pass preprocess","dfe1d8aa":"- We will first use the \"texts_from_df\" function to load the data from the data frames\n- then pass train data set along with the text and target columns\n- BERT can handle a maximum length to 512, but we only use 400 to reduce memory and improve speed.\n- We need to process text a specific way for use with BERT. for that use preprocess_mode = 'bert'\n","d59c81c2":"- train the model using train_data, val_data and batch_size","1cfae915":"## What is K-Train\n\nktrain is a lightweight wrapper for the deep learning library TensorFlow Keras (and other libraries) to help build, train, and deploy neural networks and other machine learning models. Inspired by ML framework extensions like fastai and ludwig, ktrain is designed to make deep learning and AI more accessible and easier to apply for both newcomers and experienced practitioners.\n\nknow more about it \n-https:\/\/github.com\/amaiya\/ktrain\n\n","5f3b9c40":"- Because as predict it will going to predict either \n- target or not target so mapping these values with 1 and 0","6d727272":"## Removing Emojis","0e72dd67":"## Data Exploration (EDA)","cfdc723d":"## install \"Ktrain\" using pip","ebab6020":"### By looking at the datset we can get a sense that we only need 2 cloumns (Text and target) \n- So taking only these columns\n\nand making copy","af2901fb":"## What's in this Kernel?\n- Data Exploration\n- Date(Text) Clearning\n- BERT Model using K-train","64852a65":"## Stopwords counts"}}