{"cell_type":{"132c318c":"code","6f796803":"code","1f2dd95e":"code","ce87d57b":"code","216fc7e7":"code","001d652d":"code","4536b09a":"code","49cf1878":"code","7814219f":"code","6dc01afc":"code","92103fe0":"code","70ea0656":"code","6fabb889":"code","42a7a51e":"code","d80e0cba":"code","5f88fbd9":"code","b147638f":"code","ccc98787":"code","9f7f323a":"code","508598ba":"code","cdf4c228":"code","f43052d4":"code","6fdc390f":"code","076f8d0c":"code","47561f25":"code","4a76781c":"code","5ac9e115":"code","bbab73ce":"code","1a50bcf6":"code","bad7ffcf":"code","acd588f3":"code","02db94e0":"code","9f4c5961":"code","be8e7b17":"code","a449eb0f":"code","aa6d9757":"markdown","c60a94d5":"markdown","326576d0":"markdown","aa601692":"markdown","e4b54133":"markdown","d047ceeb":"markdown","9955836d":"markdown","3d21a001":"markdown","e9dae9f0":"markdown","8f1dd1ce":"markdown","b345c0dc":"markdown","4a0fff60":"markdown","b10119ee":"markdown","c08f3e4e":"markdown","1ffb6abb":"markdown","b1a72e7c":"markdown","82c3e7c4":"markdown","c3830835":"markdown","011af9cb":"markdown","95b62e69":"markdown","f62b2dcf":"markdown","153f3e15":"markdown","883659d7":"markdown","c4b9a613":"markdown","2c7d255a":"markdown","84a0a8c5":"markdown","a3a6ef85":"markdown","d696d7e3":"markdown"},"source":{"132c318c":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport random\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as tfl\n\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom tensorflow.keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","6f796803":"from numpy.random import seed\nseed(1)\n\ntf.random.set_seed(2)","1f2dd95e":"train_path = '..\/input\/dogs-vs-cats\/train.zip'\ntest_path = '..\/input\/dogs-vs-cats\/test1.zip'\n\n# save all files to kaggle\/files\/images\ndestination = '\/kaggle\/files\/images'\n\nfrom zipfile import ZipFile as zipper\nwith zipper(train_path, 'r') as zipp:\n    zipp.extractall(destination)\n    \nwith zipper(test_path, 'r') as zipp:\n    zipp.extractall(destination)","ce87d57b":"# create a pandas dataframe\ntrain = pd.DataFrame({'file': os.listdir('\/kaggle\/files\/images\/train')})\n\ncategories = []\nfor i in os.listdir('\/kaggle\/files\/images\/train'):\n    if 'dog' in i:\n        categories.append(1)\n    else:\n        categories.append(0)\n        \ntrain['categories'] = categories","216fc7e7":"train.head()","001d652d":"def example_im(index):\n    '''\n    Function to display an example image.\n    \n    index -- which image in the training set. \n    '''\n    im = plt.imread('\/kaggle\/files\/images\/train\/'+str(train['file'][index]))\n\n    print(type(im))\n    print(im.shape)\n    print(type(im.shape))\n    \n    plt.imshow(im)\n    plt.axis('off')\n    \nexample_im(1)","4536b09a":"train['categories'].value_counts()","49cf1878":"train.shape","7814219f":"train['categories'] = train['categories'].replace({0: 'cat', 1: 'dog'})\ntrain_set, val_set = train_test_split(train, test_size=0.1, random_state = 0)","6dc01afc":"# creating generators\ntrain_gen_scaled = ImageDataGenerator(rescale=1.\/255)\nval_gen_scaled = ImageDataGenerator(rescale=1.\/255)\n\nbatch_size = 64\n\n# generating from data frame \ntrain_generator_scaled = train_gen_scaled.flow_from_dataframe(\n    dataframe = train_set,\n    directory = destination + '\/train\/', # file path format\n    x_col = 'file',\n    y_col = 'categories',\n    class_mode = 'categorical',\n    target_size = (224,224),\n    batch_size = batch_size\n)\n\nvalidation_generator_scaled = val_gen_scaled.flow_from_dataframe(\n    dataframe = val_set,\n    directory = destination + '\/train\/',\n    x_col = 'file',\n    y_col = 'categories',\n    class_mode = 'categorical',\n    target_size = (224,224),\n    batch_size = batch_size,\n    shuffle = False # ensure the validation and prediction are not random\n)\n","92103fe0":"def mini_batch_example_plot(df):\n    '''\n    This function creates 8 examples plots.\n    \n    df - the dataframe containing examples \n    '''\n    # create an another generator for training set\n    example_generator = train_gen_scaled.flow_from_dataframe(\n        dataframe = df,\n        directory = destination + '\/train\/',\n        x_col = 'file',\n        y_col = 'categories',\n        class_mode = 'categorical',\n        target_size = (224,224)\n    )\n    \n    fig, ax  = plt.subplots(2,4,figsize=(12, 12))\n    ax = ax.flatten()\n    \n    for i in range(8):\n        X, Y = next(example_generator)\n        image = X[0]\n        ax[i].imshow(image)\n        ax[i].axis('off')\n    \n    \nmini_batch_example_plot(train_set)","70ea0656":"# import the layers we will be using to construct our model \nfrom tensorflow.keras.layers import (\n    BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n)\n# import data augmentation tools\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n","6fabb889":"# set constants, the input size is set to be the default size\n(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS) = (224,224,3)\nIMAGE_SIZE  = (IMAGE_WIDTH, IMAGE_HEIGHT)\nIMAGE_SHAPE = (224,224,3)","42a7a51e":"def shallow_CNN_Model(image_shape, augmentation = False):\n    '''\n    This creates a shallow CNN model, the structure uses the idea from VGG-16\n    \n    image_shape - shape of input images\n    augmentation - with or without data augmentation, default is False\n    '''\n    # decide whether to add augmentation layers, the augmentation methods include random\n    # flip and random rotate\n    if (augmentation == True):\n        model = tf.keras.Sequential([RandomFlip(\"horizontal\",input_shape = image_shape),\n                   RandomRotation(0.1)])\n    else: \n        model = tf.keras.Sequential()\n\n    # build a model with four main sections\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape = image_shape))\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(128, (3, 3), activation='relu'))\n    model.add(Conv2D(128, (3, 3), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(2, activation='softmax'))\n\n    return model","d80e0cba":"# begin creating a model \nmodel = shallow_CNN_Model(IMAGE_SHAPE)\nmodel_augmented = shallow_CNN_Model(IMAGE_SHAPE,True)\n\n# compile the mode\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_augmented.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","5f88fbd9":"from tensorflow.keras.callbacks import EarlyStopping\n\nearlystop = EarlyStopping(patience=5)","b147638f":"def model_fitting(model, train_generator, val_generator, callbacks, epochs):\n    '''\n    This is a function used to fit generator, with customized input \n    model, callbacks, and epochs. \n    '''\n    return model.fit_generator(\n        train_generator, \n        epochs = epochs,\n        validation_data = val_generator,\n        validation_steps = val_set.shape[0]\/\/64,\n        steps_per_epoch = train_set.shape[0]\/\/64,\n        callbacks = callbacks \n    )\n","ccc98787":"history_epoch50 = model_fitting(model,train_generator_scaled, validation_generator_scaled, [earlystop], epochs = 50)","9f7f323a":"history_epoch50_augmented = model_fitting(model_augmented,train_generator_scaled, validation_generator_scaled, [earlystop], epochs = 50)","508598ba":"\ndef plotting_loss(history, upper_bound,y_tick = (0, 1, 0.1)):\n    '''\n    This is a wrapper function to create a trend displaying the change of loss and accuracy\n    \n    history -  a keras history object containing information during learning \n    upper_bound - max number of epochs displayed\n    y_tick - a tuple specifying y axis start, end and step\n    '''\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 4))\n    \n    # display train\/validation loss against epochs\n    ax1.plot(history.history['loss'], color='b', label=\"Training loss\")\n    ax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n    ax1.set_xticks(np.arange(1, upper_bound, 1))\n    a,b,c = y_tick\n    ax1.set_yticks(np.arange(a,b,c))\n\n    # display train\/validation accuracy against epochs\n    ax2.plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n    ax2.plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n    ax2.set_xticks(np.arange(1, upper_bound, 1))\n\n    legend = plt.legend(loc='best', shadow=True)\n    plt.tight_layout()\n    plt.show()\n","cdf4c228":"plotting_loss(history_epoch50, 30, y_tick = (0,2,0.5))","f43052d4":"plotting_loss(history_epoch50_augmented, 30, y_tick = (0,1.2,0.1))","6fdc390f":"def data_augmenter():\n    '''\n    create a Sequential model composed of 2 data augmentation layers\n    \n    return - tf.keras.Sequential\n    '''\n    ### START CODE HERE\n    data_augmentation = tf.keras.Sequential()\n    data_augmentation.add(RandomFlip('horizontal'))\n    data_augmentation.add(RandomRotation(0.2))\n    ### END CODE HERE\n    \n    return data_augmentation","076f8d0c":"# create two new generators without normalization (1\/255.)\ntrain_gen = ImageDataGenerator()\nval_gen = ImageDataGenerator()\nbatch_size = 64\n\ntrain_generator = train_gen.flow_from_dataframe(\n    dataframe = train_set,\n    directory = destination + '\/train\/', # file path format\n    x_col = 'file',\n    y_col = 'categories',\n    class_mode = 'categorical',\n    target_size = (224,224),\n    batch_size = batch_size\n)\n\n\nvalidation_generator = val_gen.flow_from_dataframe(\n    dataframe = val_set,\n    directory = destination + '\/train\/',\n    x_col = 'file',\n    y_col = 'categories',\n    class_mode = 'categorical',\n    target_size = (224,224),\n    batch_size = batch_size,\n    shuffle = False\n)","47561f25":"input_shape = (224,224, 3)\n\ndef transfer_learning(model,image_shape=input_shape, data_augmentation=data_augmenter()):\n    ''' \n    define a tf.keras model for binary classification out of specified model\n    \n    arguments:\n        image_shape - image width and height\n        data_augmentation - data augmentation function\n\n    returns:\n        tf.keras.model\n    '''\n    if model == 'MobileNetV2':\n        base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n                                                       include_top=False, \n                                                       weights='imagenet') # From imageNet\n        preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n        \n        \n    if model == 'VGG':\n        base_model = tf.keras.applications.VGG16(input_shape=input_shape,\n                                                    weights=\"imagenet\", \n                                                    include_top=False)\n        preprocess_input = tf.keras.applications.vgg16.preprocess_input\n        \n        \n    # freeze the base model by making it non trainable\n    base_model.trainable = False\n\n    # create the input layer (Same as the imageNetv2 input size)\n    inputs = tf.keras.Input(shape = input_shape) \n    \n    # apply data augmentation to the inputs\n    x = data_augmentation(inputs)\n    \n    # data preprocessing using the same weights the model was trained on\n    x = preprocess_input(x) \n    \n    # set training to False to avoid keeping track of statistics in the batch norm layer\n    x = base_model(x, training=False) \n    \n    # add the new Binary classification layers\n    # use global avg pooling to summarize the info in each channel\n    x = tfl.GlobalAveragePooling2D()(x) \n    # include dropout with probability of 0.2 to avoid overfitting\n    x = tfl.Dropout(0.2)(x)\n        \n    # use a prediction layer with one neuron (as a binary classifier only needs one)\n    outputs = tfl.Dense(units = 2)(x)\n\n    new_model = tf.keras.Model(inputs, outputs)\n    \n    return new_model\n","4a76781c":"model_trans_VGG = transfer_learning('VGG',(224,224,3), data_augmenter())\n\nmodel_trans_VGG.compile(\n              optimizer = 'adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel_trans_Mobile = transfer_learning('MobileNetV2',(224,224,3), data_augmenter())\n\nmodel_trans_Mobile.compile(\n                optimizer = 'adam',\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n\nmodel_trans_VGG.summary()","5ac9e115":"model_trans_Mobile.summary()","bbab73ce":"# Begin fitting with previous function `model_fitting`\nhistory_trans_VGG = model_fitting(model_trans_VGG,train_generator, validation_generator, [earlystop], epochs = 20)","1a50bcf6":"history_trans_Mobile = model_fitting(model_trans_Mobile,train_generator, validation_generator, [earlystop], epochs = 20)","bad7ffcf":"plotting_loss(history_trans_VGG, 15, y_tick = (0,0.5,0.05))","acd588f3":"plotting_loss(history_trans_Mobile, 15, y_tick = (0,0.2,0.02))","02db94e0":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef my_confusion_matrix(model):\n    '''\n    This function creates a confusion matrix to visualize result on validation set.\n    \n    model - the model used for prediction\n    '''\n    # generate the prediction made by the model\n    predict = model.predict_generator(validation_generator, \n                                      steps=np.ceil(val_set.shape[0] \/ 64))\n\n    # transform the probability into label 1 and 0\n    val_set['predict'] = np.argmax(predict, axis=-1)\n    labels = dict((v,k) for k,v in train_generator.class_indices.items())\n    val_set['cat\/dog'] = val_set['predict'].map(labels)\n\n    fig, ax = plt.subplots(figsize = (9, 6))\n    \n    # plot confusion matrix\n    cm = confusion_matrix(val_set[\"categories\"], val_set[\"cat\/dog\"])\n    disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"cat\", \"dog\"])\n    disp.plot(cmap = plt.cm.Blues, ax = ax)\n\n    ax.set_title(\"Validation Set\")\n    plt.show()\n","9f4c5961":"my_confusion_matrix(model_augmented)","be8e7b17":"my_confusion_matrix(model_trans_VGG)","a449eb0f":"my_confusion_matrix(model_trans_Mobile)","aa6d9757":"# Dog Cat Classification","c60a94d5":"## 11 - Visualization of results on validation set ","326576d0":"For our data, we have same number of cats and dogs in training data.","aa601692":"### 10.1 - Data augmentation function","e4b54133":"## 1 - Package import ","d047ceeb":"In previous test, the dataset not being augmented performs worse than augmented dataset on validation set, showing a sign of overfitting in training set.  ","9955836d":"### 10.4 - Fitting VGG and MobileNetV2","3d21a001":"## 6 - Customized CNN model construction","e9dae9f0":"The numeric varaible in `categories` columns is mapped to string for later image generation\n\nThe training data is not a particularly small dataset so we can use `10%` of its data as a `cross validation set`, and `random_state` is set to be 0 for reproductivity.","8f1dd1ce":"### 10.2 - Building new layers on VGG and MobileNetV2 ","b345c0dc":"In this notebook I will be using a customized CNN model and VGG16 and MobileNetV2 with transfer learning to perform a classifiation task on cat and dogs. I will compare the accuracy and training time of these three models, and the accuracy of customized CNN model with and without a data augmentation.","4a0fff60":"Using functional API for easier operation on last few layers for transfer learning.","b10119ee":"Set seed for reproductivity","c08f3e4e":"## 4 - Data Preparation ","1ffb6abb":"We can see that the accuracy achieved by these two models with transfer learning do not show an obvious difference; but MobileNetV2 seems to perform a little bit better: the final accuracy reaches 98.72%. However, as expected, the training time of MobileNetV2(approximately 94s per epoch) is mostly less time than a VGG network(approximately 102s per epoch). The customized models, without transfer learning, have weaker performances and longest traning time(approximately 103per epoch). But among those customized models, the model train on augmented data shows a more robust performance on the valitation set. ","b1a72e7c":"## 3 - Visualize an example ","82c3e7c4":"Here we create a function that generates a customized CNN network.","c3830835":"## 8 - Model Fitting","011af9cb":"## 10 - VGG16 and MobileNetV2 with Tansfer learning models ","95b62e69":"We are going to use mini-batch optimization to train our model. Typically a Convolutional neural network has millions or tens of millions of parameters to train, so mini-batch can largely increase our training speed and shorten the iteration time. The `batch_size` is selected to be 64 here. Also the normalization (1.\/255 rescaling) is used for train and validation generator. ","f62b2dcf":"We can create a wrapper function to fit different models.","153f3e15":"The data provided on Kaggle kernel is zip file, so expand the file first.","883659d7":"### 10.3 - compile new VGG and MobileNetV2 models","c4b9a613":"## 9 - Visualization of results for customized models","2c7d255a":"## 7 - Early stopping ","84a0a8c5":"## 5 - Example mini batch","a3a6ef85":"The training will end after seeing no decrease in loss over 5 epoches. It is useful for the prevention of overfitting and save computation cost.","d696d7e3":"## 2 - Data Import"}}