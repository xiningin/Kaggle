{"cell_type":{"683d94cc":"code","72ebd19e":"code","de2b2c5c":"code","032e56f5":"code","5b56646a":"code","946adc22":"code","77368c2b":"code","9742da16":"code","28e62145":"code","3fc5e8f4":"code","53842a41":"code","51dd8dec":"code","f2aa5aff":"code","c3135563":"code","a324244b":"code","25e3b720":"code","3fc0c864":"code","24b1ecc4":"code","907eb9df":"code","cad40898":"code","5d3d0d36":"code","041cf86b":"code","522eef1d":"markdown","326b84f9":"markdown","0ef230a9":"markdown","e9e23574":"markdown","d9226f6f":"markdown","f4abfcb8":"markdown","29b01213":"markdown"},"source":{"683d94cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72ebd19e":"data =pd.read_csv('\/content\/drive\/MyDrive\/Train.csv')","de2b2c5c":"data.head()","032e56f5":"data.describe()","5b56646a":"data.info()","946adc22":"chars_to_remove = [',','$','+','?','!','.','\u00e0']\n# List of column names to clean\ncols_to_clean = ['content']\n\n# Loop for each column\nfor col in cols_to_clean:\n    # Replace each character with an empty string\n    for char in chars_to_remove:\n        data[col] = data[col].str.replace(char, '')","77368c2b":"import nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nnltk.download('stopwords')","9742da16":"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","28e62145":"my_stop_word_list = stopwords.words('arabic') + stopwords.words('english') + stopwords.words('french')","3fc5e8f4":"def clean_message(message):   \n\n  \n  message = message.lower()\n  message = message.split()\n  stemmer = PorterStemmer()\n  message = [stemmer.stem(word) for word in message if word not in set(my_stop_word_list)]\n  message = \" \".join(message)\n  return message","53842a41":"content_training = []\nfor i in range(0, len(data)):\n    msg = clean_message(data.content[i])\n    content_training.append(msg)","51dd8dec":"X=content_training\nY= np.array(data[\"score\"])\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=42)","f2aa5aff":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\noov_tok = \"<OOV>\"","c3135563":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(x_train)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(x_train)\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(x_test)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)","a324244b":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","25e3b720":"\nmodel_cnn = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nmodel_cnn.summary()\nmodel_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","3fc0c864":"num_epochs = 10 # train with 50\nhistory_model_cnn = model_cnn.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))","24b1ecc4":"test=pd.read_csv('\/content\/drive\/MyDrive\/Test.csv')","907eb9df":"content_test = []\nfor i in range(0, len(test)):\n    msg = clean_message(test.content[i])\n    content_test.append(msg)","cad40898":"testing_seq = tokenizer.texts_to_sequences(content_test)\ntest_X_seq = pad_sequences(testing_seq,maxlen=max_length)","5d3d0d36":"Ypredict = model_cnn.predict_classes(test_X_seq, verbose=1)","041cf86b":"submission = pd.DataFrame()\nsubmission['ID'] = test.ID\nsubmission['score'] = Ypredict\nsubmission.to_csv('hackSubmissionCNN.csv', index=False)","522eef1d":"Model definition","326b84f9":"prediction","0ef230a9":"Tockenization","e9e23574":"data split","d9226f6f":"data cleaning","f4abfcb8":"cleaning function","29b01213":"training"}}