{"cell_type":{"744f5267":"code","4fef2af1":"code","107e384a":"code","1e781d4b":"code","1a3a45e9":"code","d14ad882":"code","f13f0ce0":"code","50c33a42":"code","8ae3bfc6":"code","ea5299c4":"code","7fe620c4":"code","2fdb0c31":"code","e85f3686":"code","c9a24327":"code","1e8a9fbf":"code","2c4dc0b7":"code","26f5baef":"code","760e305b":"code","c1edc74d":"code","0db978f8":"code","d6447acb":"code","d3b93cf2":"code","a8092360":"code","ccf40bb4":"code","0b70f9a3":"code","a8be8859":"markdown","459366da":"markdown","3584cdb8":"markdown","08e722a3":"markdown","1956735f":"markdown","2f24e20c":"markdown","730e114c":"markdown","ea9bc16a":"markdown","d6676f3d":"markdown","e1b0c244":"markdown","4570853d":"markdown","add92bce":"markdown","cc298a17":"markdown","777421e2":"markdown","153e29df":"markdown"},"source":{"744f5267":"import os, re, string\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport random\n\n#!pip install pySpellChecker\n#from spellchecker import SpellChecker               # SpellChecker\nfrom tqdm._tqdm_notebook import tqdm_notebook       # Get progress bar when using pandas\ntqdm_notebook.pandas()\n\nseed = 9870\ndef seeder(seed):\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\nprint(os.listdir('..\/input'))","4fef2af1":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","107e384a":"train.head()","1e781d4b":"test.head()","1a3a45e9":"train.describe(include=['O','float','int'])","d14ad882":"test.describe(include=['O','float','int'])","f13f0ce0":"train.info()","50c33a42":"test.info()","8ae3bfc6":"sns.countplot(train.text.duplicated())","ea5299c4":"# Deleting duplicate tweets\n\nduplicate_index = train[train.text.duplicated()].index\ntrain.drop(index = duplicate_index, inplace = True)\ntrain.reset_index(drop = True, inplace = True)","7fe620c4":"shortforms =   {\"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"He had\",\n    \"he'd've\": \"He would have\",\n    \"he'll\": \"He will\",\n    \"he'll've\": \"He will have\",\n    \"he's\": \"He is\",\n    \"how'd\": \"How did\",\n    \"how'd'y\": \"How do you\",\n    \"how'll\": \"How will\",\n    \"how's\": \"How is\",\n    \"i'd\": \"I had\",\n    \"i'd've\": \"I would have\",\n    \"i'll\": \"I will\",\n    \"i'll've\": \"I will have\",\n    \"i'm\": \"I am\",\n    \"i've\": \"I have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"It had\",\n    \"it'd've\": \"It would have\",\n    \"it'll\": \"It will\",\n    \"it'll've\": \"It will have\",\n    \"it's\": \"It is\",\n    \".it's\": \"It is\",\n    \"let's\": \"Let us\",\n    \"ma'am\": \"Madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"She had\",\n    \"she'd've\": \"She would have\",\n    \"she'll\": \"She will\",\n    \"she'll've\": \"She will have\",\n    \"she's\": \"She is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"that'd\": \"that had\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"There had\",\n    \"there'd've\": \"There would have\",\n    \"there's\": \"There has\",\n    \"they'd\": \"They had\",\n    \"they'd've\": \"They would have\",\n    \"they'll\": \"They will\",\n    \"they'll've\": \"They will have\",\n    \"they're\": \"They are\",\n    \"they've\": \"They have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"We had\",\n    \"we'd've\": \"We would have\",\n    \"we'll\": \"We will\",\n    \"we'll've\": \"We will have\",\n    \"we're\": \"We are\",\n    \"we've\": \"We have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"What will\",\n    \"what'll've\": \"What will have\",\n    \"what're\": \"What are\",\n    \"what's\": \"What is\",\n    \"what've\": \"What have\",\n    \"when's\": \"When is\",\n    \"when've\": \"When have\",\n    \"where'd\": \"Where did\",\n    \"where's\": \"Where is\",\n    \"where've\": \"Where have\",\n    \"who'll\": \"Who will\",\n    \"who'll've\": \"Who will have\",\n    \"who's\": \"Who is\",\n    \"who've\": \"Who have\",\n    \"why's\": \"Why is\",\n    \"why've\": \"Why have\",\n    \"will've\": \"ill have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"You all\",\n    \"y'all'd\": \"You all would\",\n    \"y'all'd've\": \"You all would have\",\n    \"y'all're\": \"You all are\",\n    \"y'all've\": \"You all have\",\n    \"you'd\": \"You had\",\n    \"you'd've\": \"You would have\",\n    \"you'll\": \"You will\",\n    \"you'll've\": \"You will have\",\n    \"you're\": \"You are\",\n    \"you've\": \"You have\"\n}","2fdb0c31":"def cleaner(text):\n    text = str(text).lower()                                                    # LowerCase\n    text = re.sub(r'<*?>',' ',text)                                             # Removing HTML tag\n    text = re.sub(r'https?:\/\/\\S+|www\\.\\S+',' ',text)                            # Removing hyperlink related entries\n    text = ' '.join([shortforms[word] \n                     if word in shortforms.keys() else word\n                     for word in text.split()])\n    text = str(text).lower()                                                    # LowerCase\n    #text = str(text).translate(str.maketrans('','',string.punctuation))         # Removing punctuation\n   \n    text = re.sub(r'^\\s','',text)\n    text = re.sub(r'\\s+',' ',text)\n    \n    # Spelling Checker\n    \n    #spell = SpellChecker()\n    #correct = []\n    #wrong_words = spell.unknown(text.split())\n\n    #for w in text.split():\n    #    if w in wrong_words :\n    #        correct.append(spell.correction(w))\n    #    else :\n    #        correct.append(w)\n    \n    return(text)","e85f3686":"%%time\ntrain['cleaner_text'] = train.text.progress_apply(lambda x: cleaner(x))\ntest['cleaner_text'] = test.text.progress_apply(lambda x: cleaner(x))","c9a24327":"from transformers import RobertaTokenizer, TFAutoModel, AutoConfig, TFRobertaMainLayer\n\ncase = 'roberta-base'\n\ntokenizer = RobertaTokenizer.from_pretrained(case)\nconfig = AutoConfig.from_pretrained(case, output_attentions = True, output_hidden_states = True)\nmodel = TFAutoModel.from_pretrained(case, config = config)\nbert = TFRobertaMainLayer(config)","1e8a9fbf":"%%time\nimport tqdm\n\ndef convert2token(all_text):\n    token_id, attention_id = [], []\n    for i, sent in tqdm.tqdm(enumerate(all_text)):\n        token_dict = tokenizer.encode_plus(sent, max_length=60, pad_to_max_length=True, return_attention_mask=True, \n                                           return_tensors='tf', add_special_tokens= True)\n        token_id.append(token_dict['input_ids'])\n        attention_id.append(token_dict['attention_mask'])\n    \n    token_id = np.array(token_id, dtype='int32')\n    attention_id = np.array(attention_id, dtype='int32')\n    return(token_id, attention_id)\n\ntrain_token_id, train_attention_id = convert2token(train.cleaner_text.values)\ntest_token_id, test_attention_id = convert2token(test.cleaner_text.values)","2c4dc0b7":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ndef building_model(need_emb):\n    inp_1 = tf.keras.layers.Input(shape = (60,), name = 'token_id', dtype = 'int32')\n    inp_2 = tf.keras.layers.Input(shape = (60,), name = 'mask_id', dtype = 'int32')\n\n    x1 = tf.keras.layers.Reshape((60,))(inp_1)\n    x2 = tf.keras.layers.Reshape((60,))(inp_2)\n    \n    if need_emb:\n        \n        emb = model(x1, attention_mask = x2)[0]     # Give output in form batch_size * max_len_token * hidden_dim, Using only CLS token hidden dimension        \n        x = tf.keras.layers.Dense(256, activation = 'relu')(emb[:,0,:])\n        x = tf.keras.layers.BatchNormalization() (x)\n        x = tf.keras.layers.Dropout(0.3) (x)\n        x = tf.keras.layers.Dense(32, activation = 'relu') (x)\n        x = tf.keras.layers.BatchNormalization() (x)\n        x = tf.keras.layers.Dropout(0.3) (x)\n\n    \n    else :\n        \n        emb = bert(x1, attention_mask = x2)[0]\n        x = tf.keras.layers.Dropout(0.2) (emb[:,0,:])\n\n\n    out = tf.keras.layers.Dense(1, activation = 'sigmoid') (x)\n\n    Emb_Model = tf.keras.models.Model(inputs = [inp_1, inp_2], outputs = out)\n    callback = ModelCheckpoint(filepath = 'best.hdf5', monitor = 'val_loss', save_best_only = True, verbose = 1)\n    \n    # Further more we need to set Bert layer as it is. We don't want to train Bert layer as we are using it as pretrained layer.\n    # Setting Bert Layer trainable = False\n    \n    if need_emb :\n        \n        for layer in Emb_Model.layers[:5]:\n            layer.trainable = False\n    \n    return(Emb_Model, callback)","26f5baef":"Emb_Model, callback = building_model(need_emb=True)\nEmb_Model.summary()","760e305b":"Emb_Model.compile(metrics=['accuracy'], optimizer=tf.keras.optimizers.Adam(learning_rate = 4e-5), loss='binary_crossentropy')\nEmb_Model.fit([np.reshape(train_token_id, (7503,60)), np.reshape(train_attention_id, (7503,60))],  train.target, epochs=10, \n      batch_size=64, validation_split=0.20, shuffle = True)","c1edc74d":"%%time\nEmb_Model_Answer = Emb_Model.predict([np.reshape(test_token_id, (3263,60)), np.reshape(test_attention_id, (3263,60))])","0db978f8":"Tune_Bert,callback = building_model(need_emb = False)\nTune_Bert.summary()","d6447acb":"Tune_Bert.compile(metrics=['accuracy'], optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy')\nTune_Bert.fit([np.reshape(train_token_id, (7503,60)), np.reshape(train_attention_id, (7503,60))],  train.target, epochs=10, \n      batch_size=64, validation_split=0.20, shuffle = True, callbacks = [callback])","d3b93cf2":"Tune_Bert.load_weights('best.hdf5')\nTune_answer = Tune_Bert.predict([np.reshape(test_token_id, (3263,60)), np.reshape(test_attention_id, (3263,60))])","a8092360":"sample.head(2)","ccf40bb4":"answer_Emb = pd.DataFrame({'id': sample.id, 'target': np.where(Emb_Model_Answer>0.5,1,0).reshape(Emb_Model_Answer.shape[0])})\nanswer_tune = pd.DataFrame({'id': sample.id, 'target': np.where(Tune_answer>0.5,1,0).reshape(Tune_answer.shape[0])})","0b70f9a3":"answer_Emb.to_csv('submission_emb.csv', index = False)\nanswer_tune.to_csv('submission_tune.csv', index = False)","a8be8859":"### Cleaning -: \nWe have to remove all unnecessary symbols, punctuation, tags and many more. ** StopWords should not be removed because transformer learn it while making embedding**","459366da":"Bert model need two tokens : 1. Input Token id and 2. Attention_id. <br>\nToken_id is id corresponding to cleaned input text and attention_id is basically an id which tells model to give attention to corresponding token. For eg. Attention_id will tell model to give more attention to tweet words but ot give attention to paddings","3584cdb8":"The very first part of data preparation is EDA. Exploring data will help us in better understanding and also help's in bring out hidden pattern which we may overlook. So let's start EDA\n#### Checking for duplicate text data entry","08e722a3":"### a) Using Bert model embedding :\n\nHere we will only use embeddings from Bert model and use it as input to another classifier. Here we will be not using Bert's full potential but this might work in some cases","1956735f":"# 3) Bert Model","2f24e20c":"Now importing HuggingFace transformer's Bert Tokenizer, Base-Model","730e114c":"* ## Contents of the Notebook:\n\n### 1) Data Preparation\n\n### 2) Tokenization\n\n### 3) Bert Model\n#### - a) By Using only embedding from Bert\n#### - b) Fine tuning Bert","ea9bc16a":"# 1) Data Preparation","d6676f3d":"# HOW TO TRAIN YOUR BERT MODEL (\ud83e\udd17HuggingFace\ud83e\udd17) ","e1b0c244":"### b) Fine tuning Bert:\nIn this we are going to use all embedding dimension across all tokens and tune it for our task","4570853d":"Now given tweets are clean. We can further move to next step for Tokenization","add92bce":"### Objective : \nObejctive behind this notebook is to give an idea on building a transformer model using HuggingFace transformers. \n\nIn this noteboook you will see **Basic Text Cleaning, Tokenization, various ways to use a pre-trained Bert model.**\n\nI am trying to keep it as **simple** as i can so that newbie can also understand the workflow.\n\n#### If you learn anything useful from this notebook then **Give Upvote :)","cc298a17":"Here we can clearly see that there are couple of tweets which are similar. Let.s delete duplicate text and keep only unique tweets","777421e2":"## EDA","153e29df":"# 2) Tokenization"}}