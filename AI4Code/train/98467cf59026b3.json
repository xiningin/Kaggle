{"cell_type":{"6d4cf08e":"code","a5c9ccad":"code","a0a5983b":"code","d426aa3d":"code","35c71569":"code","602beac6":"code","5178e689":"code","54a56e24":"code","31b42465":"code","672452fe":"code","63103018":"code","d3d400f9":"code","d42e9c4a":"code","4d9fc468":"code","da4856f4":"code","1609200f":"code","eba0f836":"code","7f9bf78f":"code","3a0de872":"code","ae375a85":"code","0b6f4e69":"code","4cad4a12":"code","40bd92d2":"code","a4ce45ab":"code","e1e34291":"code","2831a435":"code","8829d628":"code","1a11dec1":"code","87ab52e2":"code","ae3fd4f8":"code","b72632f5":"code","4793742e":"code","d0eb4b83":"code","48079f49":"code","b389fd6d":"code","656344a5":"code","0fe89720":"code","287cd998":"code","36affe0f":"code","9652ee74":"code","e2a2149f":"code","35cf554e":"code","c1a63ddf":"code","bdde9ac9":"code","91029ffa":"code","89f72e99":"code","7905a765":"code","7b76edc5":"code","489b7364":"code","5762d365":"code","063ca432":"code","fec49de4":"code","ea2b2316":"code","8edde637":"code","3c9e62a2":"code","e94f9cd4":"code","6e2eb5d8":"code","ff68dbc0":"code","e06acaec":"code","efbe8184":"code","0da28199":"code","97a7fa3b":"code","793f7544":"code","5b3d4c2e":"code","cac46ab1":"code","c8a2f2c2":"code","88702fb9":"code","6a763a34":"code","8a689e7f":"code","70286d80":"code","15166f69":"code","618db649":"code","6fdc0c5b":"code","29aa1aa0":"code","b2467ac6":"code","48dfec00":"code","8b40a9f5":"code","a459df48":"code","aff6be51":"code","219f1310":"code","b4ebe414":"code","2a7a33fa":"code","b2bc8ccc":"code","e7c1d7ae":"code","435bc321":"code","f37208be":"code","8db57573":"code","67faf3b8":"code","5047ca8c":"code","6bcfba0d":"code","9cf77048":"code","95645436":"code","efa3984b":"code","16435cd9":"code","fcd5d895":"code","b185a51f":"code","1f72c08d":"code","0a70019f":"code","f4a32567":"code","fa4bb795":"code","3135959c":"code","91880001":"code","4aba7379":"code","35db758a":"code","daf66909":"code","8ac2ecea":"code","ff82b89e":"code","9f06e8cb":"code","a637f464":"code","b61152bd":"code","081a437f":"code","65f3ca07":"markdown","4cd24c08":"markdown","26894db4":"markdown","4e7332a4":"markdown","99cdb0de":"markdown","539ad70d":"markdown","aa6cdbd5":"markdown","1b502d14":"markdown","752abda1":"markdown","0ca1e728":"markdown","0dfdcc02":"markdown","c8f9b241":"markdown","5cd88aed":"markdown","e016c5d6":"markdown","137cca90":"markdown","a2f5480e":"markdown","6e5f217f":"markdown","4c555927":"markdown","3e01c27a":"markdown","100d2001":"markdown","296e3a30":"markdown","c14b82dd":"markdown","d5ec6f6e":"markdown","58c43103":"markdown","c47052a7":"markdown","86bd77bc":"markdown","b3c64d3c":"markdown","350a8f37":"markdown","a5f96df5":"markdown","60c8e90c":"markdown","2df6d291":"markdown","8c847f8c":"markdown"},"source":{"6d4cf08e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import regularizers\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,scale\nfrom math import sqrt\nfrom numpy import hstack\nfrom numpy import vstack\nfrom numpy import asarray\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nfrom scipy.optimize import linprog\n\nfrom catboost import CatBoostRegressor\n\nimport catboost\nimport datetime\nprint(tf.__version__)","a5c9ccad":"train=pd.read_csv('..\/input\/mckinsey-data-hackathon-dataset\/train.csv')\ntest=pd.read_csv('..\/input\/mckinsey-data-hackathon-dataset\/test.csv')","a0a5983b":"train.isnull().sum()","d426aa3d":"train.head()","35c71569":"galaxy=train['galaxy']\ngalaxy.value_counts()","602beac6":"temp=pd.pivot_table(train,index='galaxy',values='y',aggfunc='mean').sort_values(by='y',ascending=False)\ntemp","5178e689":"ax=sns.distplot(temp['y'])\nax.set_title('Mean Index Values of Galaxies')","54a56e24":"temp['y'].describe()","31b42465":"sns.set()\ngalactic_year=train['galactic year']\nfig=plt.figure(figsize=(10,10))\nax=fig.add_subplot(111)\nmarkers_on = list(range(1010000,1015000))\nsns.lineplot(galactic_year,train['y'],ax=ax)","672452fe":"sns.set()\ngalactic_year=train['galactic year']\nfig=plt.figure(figsize=(10,10))\nax=fig.add_subplot(111)\nsns.scatterplot(galactic_year,train['y'],size=train['y'],ax=ax)","63103018":"pd.DataFrame(train.groupby('galactic year')['y'].mean()).plot.bar(figsize=(10,10))","d3d400f9":"def groupby_galaxy_different_features(feature):\n    temp=pd.DataFrame(train.groupby('galaxy')[feature].mean())\n    fig=plt.figure()\n    ax=fig.add_subplot(111)\n    sns.distplot(temp.dropna(axis=0).values,ax=ax)\n    ax.set_title('Mean {} Value across Galaxies'.format(feature))","d42e9c4a":"features=train.columns.tolist()[2:-1]\nfor feature in features:\n    groupby_galaxy_different_features(feature)","4d9fc468":"def missed_galaxies_mean_value():  \n    dic_missed_galaxies={}\n    features=train.columns.tolist()[2:-1]\n    for feature in features:\n        temp=pd.DataFrame(train.groupby('galaxy')[feature].mean())\n        missed_galaxies=temp[(temp[feature].isnull())].index.tolist()\n        dic_missed_galaxies[feature]=missed_galaxies\n    return dic_missed_galaxies","da4856f4":"dic_missed_galaxies=missed_galaxies_mean_value()","1609200f":"def fillna_feature(x):\n    temp=pd.DataFrame(train.groupby('galaxy')[feature].mean()).fillna(-1000)\n    return temp.loc[x['galaxy'],feature]\n\nfor feature in tqdm(features):## Will take some time\n    train[feature].fillna(train[train[feature].isnull()].apply(fillna_feature,axis=1),inplace=True)\n    train.to_csv('updated_train.csv')","eba0f836":"features=train.columns.tolist()[2:-1]\ntrain=pd.read_csv('updated_train.csv',index_col=0)\nfor feature in features:##Now replacing the negative -1000 values these are those rows whose mean was nan because there was no feature value for the specifc galaxy\n    x = train[feature][train[feature] != -1000].mean()\n    train[feature][train[feature] == -1000] = x","7f9bf78f":"train.head()","3a0de872":"train.isnull().sum()","ae375a85":"train.describe().to_csv('features_description.csv')##open xlsx file in excel to see whether herustic has introduced any anomality","0b6f4e69":"corr_map=pd.DataFrame(train.drop(['galaxy'],1).corr()['y']).drop('y',axis=0).sort_values(by='y',ascending=False)\ncorr_map.to_csv('feature_corr_y.csv')## See the excel for features correlation with y\nprint(corr_map)","4cad4a12":"fig=plt.figure(figsize=(10,10))\nax=fig.add_subplot(111)\ncorr_map[:15].plot.bar(ax=ax)\nax.set_title('Top  15 Features Correlated with y')","40bd92d2":"fig=plt.figure(figsize=(10,10))\nax=fig.add_subplot(111)\ncorr_map[-15:].plot.bar(ax=ax)\nax.set_title('Bottom  15 Features Correlated with y')","a4ce45ab":"sns.set(style=\"white\", color_codes=True)\n#     class color:\n#            PURPLE = '\\033[95m'\n#            CYAN = '\\033[96m'\n#            DARKCYAN = '\\033[36m'\n#            BLUE = '\\033[94m'\n#            GREEN = '\\033[92m'\n#            YELLOW = '\\033[93m'\n#            RED = '\\033[91m'\n#            BOLD = '\\033[1m'\n#            UNDERLINE = '\\033[4m'\n#            END = '\\033[0m'\ndef scatterplot_y_feature(feature):\n    corr=corr_map.loc[feature,'y']\n    if 0.50<corr<=1:\n        color='g'\n    elif 0<corr<=0.50:\n        color='y'\n    else:\n        color='r'\n        \n    ax=sns.jointplot(feature, 'y',data=train, color=color,kind=\"reg\").plot_joint(sns.kdeplot)\n    fig=plt.gcf()\n    fig.set_size_inches(10,10)\n    fig.suptitle('JointPlot of y and {}'.format(feature))\n    print('\\033[1m'+'The correlation of {} with y is {:0.2f}'.format(feature,corr))\n\nscatterplot_y_feature('Interstellar Data Net users, total (% of population)')   ","e1e34291":"scatterplot_y_feature('Old age dependency ratio (old age (65 and older) per 100 creatures (ages 15-64))')  ","2831a435":"scatterplot_y_feature('Estimated gross galactic income per capita, female')  ","8829d628":"scatterplot_y_feature('Intergalactic Development Index (IDI), female') ","1a11dec1":"scatterplot_y_feature('Intergalactic Development Index (IDI), male') ","87ab52e2":"scatterplot_y_feature('Expected years of education, male (galactic years)') ","ae3fd4f8":"scatterplot_y_feature('Intergalactic Development Index (IDI)') ","b72632f5":"scatterplot_y_feature('Mean years of education, male (galactic years)') ","4793742e":"scatterplot_y_feature('Education Index') ","d0eb4b83":"scatterplot_y_feature('Expected years of education, female (galactic years)') ","48079f49":"scatterplot_y_feature('Mean years of education, female (galactic years)') ","b389fd6d":"scatterplot_y_feature('Life expectancy at birth, male (galactic years)') ","656344a5":"scatterplot_y_feature('Expected years of education (galactic years)') ","0fe89720":"scatterplot_y_feature('Income Index') ","287cd998":"scatterplot_y_feature('Mean years of education (galactic years)') ","36affe0f":"scatterplot_y_feature('Life expectancy at birth, female (galactic years)') ","9652ee74":"scatterplot_y_feature('Gross galactic product (GGP) per capita') ","e2a2149f":"scatterplot_y_feature('Population with at least some secondary education, male (% ages 25 and older)') ","35cf554e":"scatterplot_y_feature('existence expectancy at birth') ","c1a63ddf":"scatterplot_y_feature('Population with at least some secondary education, female (% ages 25 and older)') ","bdde9ac9":"scatterplot_y_feature('existence expectancy index') ","91029ffa":"scatterplot_y_feature('Domestic credit provided by financial sector (% of GGP)') ","89f72e99":"scatterplot_y_feature('Population with at least some secondary education (% ages 25 and older)') ","7905a765":"scatterplot_y_feature('Employment in services (% of total employment)') ","7b76edc5":"scatterplot_y_feature('Estimated gross galactic income per capita, male') ","489b7364":"scatterplot_y_feature('Gross income per capita') ","5762d365":"scatterplot_y_feature('Population, urban (%)') ","063ca432":"scatterplot_y_feature('Rural population with access to electricity (%)') ","fec49de4":"scatterplot_y_feature('Interstellar phone subscriptions (per 100 people)') ","ea2b2316":"scatterplot_y_feature('Current health expenditure (% of GGP)') ","8edde637":"scatterplot_y_feature('Share of seats in senate (% held by female)') ","3c9e62a2":"scatterplot_y_feature('Gender Development Index (GDI)') ","e94f9cd4":"scatterplot_y_feature('Share of employment in nonagriculture, female (% of total employment in nonagriculture)') ","6e2eb5d8":"scatterplot_y_feature('Exports and imports (% of GGP)') ","ff68dbc0":"scatterplot_y_feature('Outer Galaxies direct investment, net inflows (% of GGP)') ","e06acaec":"scatterplot_y_feature('Adjusted net savings ') ","efbe8184":"scatterplot_y_feature('Gross enrolment ratio, primary (% of primary under-age population)') ","0da28199":"scatterplot_y_feature('Labour force participation rate (% ages 15 and older), female') ","97a7fa3b":"scatterplot_y_feature(features[-41]) ","793f7544":"scatterplot_y_feature('Intergalactic inbound tourists (thousands)') ","5b3d4c2e":"scatterplot_y_feature('galactic year') ### Hahahah Lol Ignore this","cac46ab1":"scatterplot_y_feature('Gross galactic product (GGP), total') ","c8a2f2c2":"scatterplot_y_feature('Jungle area (% of total land area)') ","88702fb9":"scatterplot_y_feature('Unemployment, total (% of labour force)') ","6a763a34":"scatterplot_y_feature('Gross capital formation (% of GGP)') ","8a689e7f":"scatterplot_y_feature('Population, ages 65 and older (millions)') ","70286d80":"scatterplot_y_feature('Gross fixed capital formation (% of GGP)') ","15166f69":"scatterplot_y_feature('Population, total (millions)') ","618db649":"scatterplot_y_feature('Employment to population ratio (% ages 15 and older)') ","6fdc0c5b":"scatterplot_y_feature(features[17]) ","29aa1aa0":"scatterplot_y_feature('Population, under age 5 (millions)') ","b2467ac6":"scatterplot_y_feature('Labour force participation rate (% ages 15 and older)') ","48dfec00":"scatterplot_y_feature('Private galaxy capital flows (% of GGP)') ","8b40a9f5":"scatterplot_y_feature('Total unemployment rate (female to male ratio)') ","a459df48":"scatterplot_y_feature('Youth unemployment rate (female to male ratio)') ","aff6be51":"scatterplot_y_feature('Creature Immunodeficiency Disease prevalence, adult (% ages 15-49), total') ","219f1310":"scatterplot_y_feature('Remittances, inflows (% of GGP)') ","b4ebe414":"scatterplot_y_feature('Labour force participation rate (% ages 15 and older), male') ","2a7a33fa":"scatterplot_y_feature('Natural resource depletion') ","b2bc8ccc":"scatterplot_y_feature('Renewable energy consumption (% of total final energy consumption)') ","e7c1d7ae":"scatterplot_y_feature('Infants lacking immunization, red hot disease (% of one-galactic year-olds)') ","435bc321":"scatterplot_y_feature('Infants lacking immunization, Combination Vaccine (% of one-galactic year-olds)') ","f37208be":"scatterplot_y_feature('Respiratory disease incidence (per 100,000 people)') ","8db57573":"scatterplot_y_feature('Maternal mortality ratio (deaths per 100,000 live births)') ","67faf3b8":"scatterplot_y_feature('Mortality rate, under-five (per 1,000 live births)') ","5047ca8c":"scatterplot_y_feature('Mortality rate, female grown up (per 1,000 people)') ","6bcfba0d":"scatterplot_y_feature('Mortality rate, male grown up (per 1,000 people)') ","9cf77048":"scatterplot_y_feature('Mortality rate, infant (per 1,000 live births)') ","95645436":"scatterplot_y_feature('Employment in agriculture (% of total employment)') ","efa3984b":"scatterplot_y_feature('Vulnerable employment (% of total employment)') ","16435cd9":"scatterplot_y_feature('Adolescent birth rate (births per 1,000 female creatures ages 15-19)') ","fcd5d895":"scatterplot_y_feature('Young age (0-14) dependency ratio (per 100 creatures ages 15-64)') ","b185a51f":"scatterplot_y_feature('Intergalactic Development Index (IDI), Rank') ","1f72c08d":"scatterplot_y_feature('Intergalactic Development Index (IDI), female, Rank') ","0a70019f":"scatterplot_y_feature('Intergalactic Development Index (IDI), male, Rank') ","f4a32567":"scatterplot_y_feature('Gender Inequality Index (GII)') ","fa4bb795":"features_dropped=['galaxy','y']\nfeature_selected=['Gender Inequality Index (GII)','Gender Inequality Index (GII)','Intergalactic Development Index (IDI), male, Rank',\n'Intergalactic Development Index (IDI), male',\n'Renewable energy consumption (% of total final energy consumption)',\n'Interstellar Data Net users, total (% of population)',\n'Intergalactic Development Index (IDI), female, Rank',\n'Old age dependency ratio (old age (65 and older) per 100 creatures (ages 15-64))',\n'galactic year',\n'Intergalactic Development Index (IDI), female',\n'Gross galactic product (GGP) per capita',\n'Share of seats in senate (% held by female)',\n'Estimated gross galactic income per capita, male', \n'Estimated gross galactic income per capita, female',\n'Mean years of education, male (galactic years)',\n'Labour force participation rate (% ages 15 and older), female',\n'Current health expenditure (% of GGP)',\n'Young age (0-14) dependency ratio (per 100 creatures ages 15-64)',\n'Youth unemployment rate (female to male ratio)',\n'Mortality rate, male grown up (per 1,000 people)'\n]\ndef forming_X_normed(features_method,save=False):\n\n    le=LabelEncoder()\n    ss=StandardScaler()\n    galaxy_encoded=le.fit_transform(train['galaxy'])\n    if features_method=='Drop':\n        X=np.array(train.drop(features_dropped,1).values.tolist())\n        X_standard=ss.fit_transform(X)\n    else:\n        X=np.array(train[feature_selected].values.tolist())\n        X_standard=ss.fit_transform(X)\n    y=np.array(train['y'].tolist())\n    X_normed=np.concatenate((X_standard,np.expand_dims(galaxy_encoded,axis=1)),axis=1)\n    if save:\n        joblib.dump(le,'le.pkl')\n        joblib.dump(ss,'ss.pkl')\n    return (X_normed,y)\n","3135959c":"def tsne_analysis_2da_graph():\n    X_normed,_=forming_X_normed(features_dropped)\n    tnse = TSNE(n_components = 2)\n    X_reduced = tnse.fit_transform(X_normed)\n    ax=sns.regplot(X_reduced[:,0],X_reduced[:,1])\n    fig=plt.gcf()\n    fig.set_size_inches((10,10))\n    ax.set_title('Reduced Feature Space Regplot')\n    plt.show()\ntsne_analysis_2da_graph()","91880001":"BATCH_SIZE = 32\nSHUFFLE_BUFFER_SIZE = 100\nl1=1e-4\nl2=1e-2\nEPOCHS=500\nLR=0.01\nDROPOUT=0.30\ndef ff_model():\n    X,y=forming_X_normed('Drop',save=False)\n    shape_data=X.shape\n    X_train,X_val,y_train,y_val = train_test_split(X,y, test_size=0.1)\n    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n    val_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n    train_data = train_data.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n    val_data = val_data.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n    \n    \n\n    ##callbacks\n    log_dir = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n    EarlyStopping= tf.keras.callbacks.EarlyStopping(patience=2)\n    CP=tf.keras.callbacks.ModelCheckpoint(filepath='nnr_model_day_3_two_layers_all_features.h5')\n    \n    ###Main Model Architecture\n    inputs = tf.keras.Input(shape=(shape_data[1]))\n    \n    layer_1 = layers.Dense(1024, activation=\"relu\",kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2))(inputs)\n    layer_1=layers.Dropout(DROPOUT)(layer_1)\n    layer_2 = layers.Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2))(layer_1)\n    \n    output = layers.Dense(1, activation=\"linear\")(layer_2)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=output, name=\"Neural_Network_Regression_Model\")\n    print(model.summary())\n    optimizer=tf.keras.optimizers.Adam(learning_rate=LR)\n    \n    \n    model.compile(optimizer=optimizer,loss='mse',metrics=['mse','mae'])\n    history=model.fit(train_data,validation_data=(val_data),epochs=EPOCHS,callbacks=[tensorboard_callback,EarlyStopping,CP])\n    pd.DataFrame(history.history)[['val_loss']].plot()\nff_model() ","4aba7379":"BATCH_SIZE = 32\nSHUFFLE_BUFFER_SIZE = 100\nl1=1e-4\nl2=1e-2\nEPOCHS=10\nLR=0.001\nDROPOUT=0.3\ndef Recurrent_neural_network():\n    X,y=forming_X_normed('Drop')\n    X=np.expand_dims(X,axis=2)\n    shape_data=X.shape\n    \n    \n    X_train,X_val,y_train,y_val = train_test_split(X,y, test_size=0.1)\n    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n    val_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n    train_data = train_data.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n    val_data = val_data.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n    \n    \n\n    ##callbacks\n    log_dir = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n    EarlyStopping= tf.keras.callbacks.EarlyStopping(patience=2)\n    CP=tf.keras.callbacks.ModelCheckpoint(filepath='recurrent_model_day_3.h5')\n    \n    ###Main Model Architecture\n    inputs = tf.keras.Input(shape=(shape_data[1:]),name='input')\n    conv_1d_layer=layers.Conv1D(filters=16,kernel_size=3,padding='same',name='Conv_1d_layer',activation='relu')(inputs)\n    \n    lstm_layer_1=layers.LSTM(64,recurrent_regularizer=regularizers.l1_l2(l1=l1, l2=l2),dropout=DROPOUT,\n                             recurrent_dropout=DROPOUT,name='lstm_layer_128')(conv_1d_layer)\n    \n    flatten=layers.Flatten(name='flatten')(lstm_layer_1)\n    \n    output = layers.Dense(1, activation=\"linear\",name='output')(flatten)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=output, name=\"RNN_Regression_Model\")\n    print(model.summary())\n    optimizer=tf.keras.optimizers.Adam(learning_rate=LR)\n    \n    \n    model.compile(optimizer=optimizer,loss='mse',metrics=['mse','mae'])\n    history=model.fit(train_data,validation_data=(val_data),epochs=EPOCHS,callbacks=[tensorboard_callback,EarlyStopping,CP])\n    pd.DataFrame(history.history)[['val_loss']].plot()\nRecurrent_neural_network()","35db758a":"def fillna_feature_test(x):\n    temp=pd.DataFrame(test.groupby('galaxy')[feature].mean()).fillna(-1000)\n    return temp.loc[x['galaxy'],feature]\n\nfeatures=test.columns.tolist()[2:]\n\nfor feature in tqdm(features):## Will take some time\n    test[feature].fillna(test[test[feature].isnull()].apply(fillna_feature_test,axis=1),inplace=True)\n    test.to_csv('updated_test.csv')","daf66909":"def testing_pipeline(model_name,types,graph=True): ### Same Pipeline could be extended to other models   \n    \n    ##Step-1 Loading Updated Filled Null Test File\n    test=pd.read_csv('updated_test.csv').drop('drop',1)\n    features=test.columns.tolist()[2:]\n    \n    for feature in features:\n        mean=test[(test[feature]!=-1000)][feature].mean()\n        test[(test[feature]==-1000)][feature]=mean\n        \n    ##Step-2 Label-Encoding and Normalization\n    le=joblib.load('le.pkl')\n    ss=joblib.load('ss.pkl')\n    galaxy_encoded=le.transform(test['galaxy'])\n    X_test=np.array(test.drop(['galaxy'],1).values.tolist())\n    X_normed_test=np.concatenate((ss.transform(X_test),np.expand_dims(galaxy_encoded,axis=1)),axis=1)\n    \n    ###Step-3 Feeding Data into Main model\n    \n    if types=='Catboost':\n        CBR=CatBoostRegressor()\n        model=CBR.load_model(model_name)\n    \n    elif types=='Neural Network':\n        model=tf.keras.models.load_model(model_name)\n    elif types=='RNN':\n        X_normed_test=np.expand_dims(X_normed_test,axis=2)\n        model=tf.keras.models.load_model(model_name)\n        \n    y_test_predict=model.predict(X_normed_test)\n    if graph:\n        ax=sns.distplot(y_test_predict)\n        ax.set_title('Y_predicted Distribution Plot')\n    ### Creating a temp df which have sum of index for galaxies both orginal and predicted\n    temp_1=pd.DataFrame()\n    temp_1['galaxy']=test['galaxy']\n    temp_1['index_predicted']=y_test_predict\n\n    \n    temp_1=pd.DataFrame(temp_1.groupby('galaxy')['index_predicted'].sum())\n    temp_1['galaxy_count_test']=test['galaxy'].value_counts().values.reshape(-1,)\n    \n    \n    temp_2=pd.DataFrame()\n    temp_2['galaxy']=train['galaxy']\n    temp_2['index_orginal']=train['y']\n    temp_2=pd.DataFrame(temp_2.groupby('galaxy')['index_orginal'].sum())\n    \n    \n    \n    temp_2=temp_1.join(temp_2).reset_index()\n    temp_2.to_csv('{}_index_prediction.csv'.format(types))\n    \n    print('This MSE of total index sum of all common galaxies between train and test set, I want to decrease this metric')\n    print(sum((temp_2['index_orginal']-temp_2['index_predicted'])**2)\/len(temp_2))## Very Important Metric to Improve\n            \n    return y_test_predict","8ac2ecea":"def catboost_model():\n    X,y=forming_X_normed(features_dropped)\n    shape_data=X.shape\n    X_train,X_val,y_train,y_val = train_test_split(X,y, test_size=0.1)\n    \n    CatBoost = CatBoostRegressor()\n    CatBoost.fit(X_train, y_train,\n              eval_set=(X_val, y_val), \n              use_best_model=True,\n              plot=True\n             );\n    feature_names=train.columns.tolist()[2:-1]\n    feature_names.append('galaxy')\n    feature_names.insert(0,'galactic year')\n    feature_importance_df = pd.DataFrame(CatBoost.get_feature_importance(), columns=['importance'])\n    feature_importance_df[\"feature\"]=feature_names\n    feature_importance_df.sort_values(by='importance',ascending=False,inplace=True)\n    feature_importance_df.to_csv('Catboost_feature_importance.csv')\n    plt.figure(figsize=(12, 12));\n    sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df);\n    plt.title('CatBoost features importance:');\n    CatBoost.save_model('cbr_model')\ncatboost_model()","ff82b89e":"def combining_prediction():\n    n=3\n    prediction_nn=testing_pipeline('cbr_model',types='Catboost',graph=False)\n    \n    prediction_catboost=np.squeeze(testing_pipeline('nnr_model_day_3_two_layers_all_features.h5',types='Neural Network',graph=False))\n    \n    prediction_rnn=np.squeeze(testing_pipeline('recurrent_model_day_3.h5',types='RNN',graph=False))\n\n    Y_combine=prediction_catboost+prediction_nn+prediction_rnn\n    \n    Y_combine\/=n\n\n    ax=sns.distplot(Y_combine)\n    ax.set_title('Y_predicted Distribution Plot')\n    temp=pd.DataFrame()\n    temp['index_predicted_combined_models']=Y_combine\n    temp['galaxy']=test['galaxy']\n    temp=pd.DataFrame(temp.groupby('galaxy')['index_predicted_combined_models'].sum()).reset_index()\n    temp_2=pd.DataFrame(train.groupby('galaxy')['y'].sum()).reset_index()\n    \n    temp=temp.merge(temp_2)\n    temp.to_csv('Final Predictions Combined.csv')\n    print('The final MSE for combined models is {}'.format(sum(temp['y']-temp['index_predicted_combined_models']**2)\/len(temp)))\n    return Y_combine\n","9f06e8cb":"# create a list of base-models\ndef get_models():\n\tmodels = list()\n\tmodels.append(LinearRegression())\n\tmodels.append(ElasticNet())\n\tmodels.append(DecisionTreeRegressor())\n\tmodels.append(KNeighborsRegressor())\n\tmodels.append(AdaBoostRegressor())\n\tmodels.append(BaggingRegressor(n_estimators=10))\n\tmodels.append(RandomForestRegressor(n_estimators=10))\n\tmodels.append(ExtraTreesRegressor(n_estimators=10))\n\treturn models\n\n# collect out of fold predictions form k-fold cross validation\ndef get_out_of_fold_predictions(X, y, models):\n\tmeta_X, meta_y = list(), list()\n\t# define split of data\n\tkfold = KFold(n_splits=10, shuffle=True)\n\t# enumerate splits\n\tfor train_ix, test_ix in tqdm(kfold.split(X)):\n\t\tfold_yhats = list()\n\t\t# get data\n\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n\t\tmeta_y.extend(test_y)\n\t\t# fit and make predictions with each sub-model\n\t\tfor model in models:\n\t\t\tmodel.fit(train_X, train_y)\n\t\t\tyhat = model.predict(test_X)\n\t\t\t# store columns\n\t\t\tfold_yhats.append(yhat.reshape(len(yhat),1))\n\t\t# store fold yhats as columns\n\t\tmeta_X.append(hstack(fold_yhats))\n\treturn vstack(meta_X), asarray(meta_y)\n\n# fit all base models on the training dataset\ndef fit_base_models(X, y, models):\n    for model in models:\n        model.fit(X, y)\n\n# fit a meta model\ndef fit_meta_model(X, y):\n    model = LinearRegression()\n    model.fit(X, y)\n    return model\n\n# evaluate a list of models on a dataset\ndef evaluate_models(X, y, models):\n\tfor model in models:\n\t\tyhat = model.predict(X)\n\t\tmse = mean_squared_error(y, yhat)\n\t\tprint('%s: RMSE %.3f' % (model.__class__.__name__, sqrt(mse)))\n\n# make predictions with stacked model\ndef super_learner_predictions(X, models, meta_model):\n\tmeta_X = list()\n\tfor model in models:\n\t\tyhat = model.predict(X)\n\t\tmeta_X.append(yhat.reshape(len(yhat),1))\n\tmeta_X = hstack(meta_X)\n\t# predict\n\treturn meta_model.predict(meta_X)\n\nX,y=forming_X_normed('Drop')\nshape_data=X.shape\nX_train,X_val,y_train,y_val = train_test_split(X,y, test_size=0.1)\n# get models\nmodels = get_models()\n# get out of fold predictions\nmeta_X, meta_y = get_out_of_fold_predictions(X_train, y_train, models)\nprint('Meta ', meta_X.shape, meta_y.shape)\n# fit base models\nfit_base_models(X_train, y_train, models)\n# fit the meta model\nmeta_model = fit_meta_model(meta_X, meta_y)\n# evaluate base models\nevaluate_models(X_val, y_val, models)\n# evaluate meta model\nyhat = super_learner_predictions(X_val, models, meta_model)\nprint('Super Learner: RMSE %.3f' % (sqrt(mean_squared_error(y_val, yhat))))","a637f464":"def super_learned_testing_pipeline():\n    test=pd.read_csv('updated_test.csv').drop('drop',1)\n    features=test.columns.tolist()[2:]\n    \n    for feature in features:\n        mean=test[(test[feature]!=-1000)][feature].mean()\n        test[(test[feature]==-1000)][feature]=mean\n        \n    le=joblib.load('le.pkl')\n    ss=joblib.load('ss.pkl')\n    galaxy_encoded=le.transform(test['galaxy'])\n    X_test=np.array(test.drop(['galaxy'],1).values.tolist())\n    X_normed_test=np.concatenate((ss.transform(X_test),np.expand_dims(galaxy_encoded,axis=1)),axis=1)\n    y_test_predict=super_learner_predictions(X_normed_test, models, meta_model)\n    \n    ax=sns.distplot(y_test_predict)\n    ax.set_title('Y_predicted Distribution Plot')\n    \n    temp_1=pd.DataFrame()\n    temp_1['galaxy']=test['galaxy']\n    temp_1['index_predicted']=y_test_predict\n\n    \n    temp_1=pd.DataFrame(temp_1.groupby('galaxy')['index_predicted'].sum())\n    temp_1['galaxy_count_test']=test['galaxy'].value_counts().values.reshape(-1,)\n    \n    \n    temp_2=pd.DataFrame()\n    temp_2['galaxy']=train['galaxy']\n    temp_2['index_orginal']=train['y']\n    temp_2=pd.DataFrame(temp_2.groupby('galaxy')['index_orginal'].sum())\n    \n    \n    \n    temp_2=temp_1.join(temp_2).reset_index()\n    temp_2.to_csv('Super_Learner_Prediction_index.csv')\n    \n    print('This MSE of total index sum of all common galaxies between train and test set, I want to decrease this metric')\n    print(sum((temp_2['index_orginal']-temp_2['index_predicted'])**2)\/len(temp_2))## Very Important Metric to Improve\n\n","b61152bd":"def optimization_problem_task_2():\n    ### Creating Submission file and Simple Energy Allocation Herustic\n    total_energy=50000\n    energy_given_each_galaxy_below_index=5000\n    energy_given_each_galaxy_above_index=100\n    energy_below_limit=0\n    \n    \n    c=[-i for i in range(172)]\n    A=[[i for i in range(172)]]\n    b=[total_energy]\n    \n    #Waiting\n    x_1=(energy_given_each_galaxy_below_index,None)#Andromeda Galaxy (M31)\n    x_2=(0,energy_given_each_galaxy_above_index)#Andromeda I\n    x_3=(0,energy_given_each_galaxy_above_index)#Andromeda II\n    x_4=(0,energy_given_each_galaxy_above_index)#Andromeda III\n    x_5=(0,energy_given_each_galaxy_above_index)#Andromeda IX\n    x_6=(0,energy_given_each_galaxy_above_index)#Andromeda V\n    x_7=(0,energy_given_each_galaxy_above_index)#Andromeda VIII\n    x_8=(0,energy_given_each_galaxy_above_index)#Andromeda X\n    x_9=(0,energy_given_each_galaxy_above_index)#Andromeda XI\n    x_10=(0,energy_given_each_galaxy_above_index)#Andromeda XV\n    x_11=(0,energy_given_each_galaxy_above_index)#Andromeda XVII\n    x_12=(0,energy_given_each_galaxy_above_index)#Andromeda XX\n    x_13=(0,energy_given_each_galaxy_above_index)#Andromeda XXIII\n    x_14=(0,energy_given_each_galaxy_above_index)#Andromeda XXIX\n    x_15=(0,energy_given_each_galaxy_above_index)#Andromeda XXI[57]\n    x_16=(0,energy_given_each_galaxy_above_index)#Andromeda XXV\n    x_17=(0,energy_given_each_galaxy_above_index)#Andromeda XXVI\n    x_18=(0,energy_given_each_galaxy_above_index)#Andromeda XXVIII\n    x_19=(0,energy_given_each_galaxy_above_index)#Antlia 2\n    x_20=(0,energy_given_each_galaxy_above_index)#Antlia B\n    x_21=(0,energy_given_each_galaxy_above_index)#Antlia Dwarf\n    x_22=(energy_given_each_galaxy_below_index,None)#Aquarius Dwarf Galaxy (DDO 210)\n    x_23=(0,energy_given_each_galaxy_above_index)#Aquarius II\n    x_24=(0,energy_given_each_galaxy_above_index)#Barnard's Galaxy (NGC 6822)\n    x_25=(0,energy_given_each_galaxy_above_index)#Bo\u00c3\u00b6tes I-Spelling Mistake\n    x_26=(0,energy_given_each_galaxy_above_index)#Bo\u00c3\u00b6tes II\n    x_27=(0,energy_given_each_galaxy_above_index)#Bo\u00c3\u00b6tes III\n    x_28=(0,energy_given_each_galaxy_above_index)#Bo\u00c3\u00b6tes IV\n    x_29=(0,energy_given_each_galaxy_above_index)#Camelopardalis B\n    x_30=(0,energy_given_each_galaxy_above_index)#Canes Venatici I Dwarf\n    x_31=(0,energy_given_each_galaxy_above_index)#Canes Venatici II Dwarf\n    x_32=(0,energy_given_each_galaxy_above_index)#Carina Dwarf (E206-G220)\n    x_33=(0,energy_given_each_galaxy_above_index)#Carina II\n    x_34=(0,energy_given_each_galaxy_above_index)#Carina III\n    x_35=(0,energy_given_each_galaxy_above_index)#Cas 1 (KK98 19)\n    x_36=(0,energy_given_each_galaxy_above_index)#Cassiopeia Dwarf (Cas dSph, Andromeda VII)\n    x_37=(0,energy_given_each_galaxy_above_index)#Cassiopeia II (Andromeda XXX)\n    x_38=(0,energy_given_each_galaxy_above_index)#Cassiopeia III (Andromeda XXXII)\n    x_39=(0,energy_given_each_galaxy_above_index)#Cetus Dwarf\n    x_40=(0,energy_given_each_galaxy_above_index)#Cetus III\n    x_41=(0,energy_given_each_galaxy_above_index)#Columba I\n    x_42=(0,energy_given_each_galaxy_above_index)#Coma Berenices Dwarf\n    x_43=(0,energy_given_each_galaxy_above_index)#Crater II\n    x_44=(0,energy_given_each_galaxy_above_index)#DDO 99 (UGC 6817)\n    x_45=(0,energy_given_each_galaxy_above_index)#Donatiello I\n    x_46=(0,energy_given_each_galaxy_above_index)#Draco Dwarf (DDO 208)\n    x_47=(0,energy_given_each_galaxy_above_index)#Draco II\n    x_48=(0,energy_given_each_galaxy_above_index)#Dwingeloo 1\n    x_49=(0,energy_given_each_galaxy_above_index)#Dwingeloo 2\n    x_50=(0,energy_given_each_galaxy_above_index)#ESO 274-01[70]\n    x_51=(0,energy_given_each_galaxy_above_index)#ESO 294-010\n    x_52=(0,energy_given_each_galaxy_above_index)#ESO 321-014[70]\n    x_53=(0,energy_given_each_galaxy_above_index)#ESO 325-11\n    x_54=(0,energy_given_each_galaxy_above_index)#ESO 383-087 (ISG 39)\n    x_55=(0,energy_given_each_galaxy_above_index)#ESO 410-G005\n    x_56=(0,energy_given_each_galaxy_above_index)#ESO 540-030 (KDG 2)\n    x_57=(0,energy_given_each_galaxy_above_index)#ESO 540-032\n    x_58=(0,energy_given_each_galaxy_above_index)#Eridanus II\n    x_59=(0,energy_given_each_galaxy_above_index)#FM2000 1\n    x_60=(0,energy_given_each_galaxy_above_index)#Fornax Dwarf (E356-G04)\n    x_61=(0,energy_given_each_galaxy_above_index)#GR 8 (DDO 155)\n    x_62=(0,energy_given_each_galaxy_above_index)#Grus I\n    x_63=(0,energy_given_each_galaxy_above_index)#Grus II\n    x_64=(0,energy_given_each_galaxy_above_index)#HIPASS J1247-77\n    x_65=(0,energy_given_each_galaxy_above_index)#HIZSS 003\n    x_66=(0,energy_given_each_galaxy_above_index)#Holmberg II (DDO 50, UGC 4305)\n    x_67=(0,energy_given_each_galaxy_above_index)#Horologium I\n    x_68=(0,energy_given_each_galaxy_above_index)#Horologium II\n    x_69=(0,energy_given_each_galaxy_above_index)#Hydra II\n    x_70=(0,energy_given_each_galaxy_above_index)#Hydrus I\n    x_71=(0,energy_given_each_galaxy_above_index)#IC 10 (UGC 192)\n    x_72=(0,energy_given_each_galaxy_above_index)#IC 1613 (UGC 668)\n    x_73=(energy_given_each_galaxy_below_index,None)#IC 3104\n    x_74=(0,energy_given_each_galaxy_above_index)#IC 342\n    x_75=(0,energy_given_each_galaxy_above_index)#IC 4662 (ESO 102-14)\n    x_76=(0,energy_given_each_galaxy_above_index)#IC 5152\n    x_77=(0,energy_given_each_galaxy_above_index)#Indus II\n    x_78=(0,energy_given_each_galaxy_above_index)#KK98 35\n    x_79=(0,energy_given_each_galaxy_above_index)#KK98 77\n    x_80=(0,energy_given_each_galaxy_above_index)#KKH 11 (ZOAG G135.74-04.53)\n    x_81=(0,energy_given_each_galaxy_above_index)#KKH 12\n    x_82=(0,energy_given_each_galaxy_above_index)#KKH 37 (Mai 16)\n    x_83=(0,energy_given_each_galaxy_above_index)#KKR 03 (KK98 230)\n    x_84=(0,energy_given_each_galaxy_above_index)#KKR 25\n    x_85=(0,energy_given_each_galaxy_above_index)#KKh 060\n    x_86=(0,energy_given_each_galaxy_above_index)#KKh 086\n    x_87=(0,energy_given_each_galaxy_above_index)#KKh 98\n    x_88=(0,energy_given_each_galaxy_above_index)#KKs 3\n    x_89=(0,energy_given_each_galaxy_above_index)#KUG 1210+301B (KK98 127)\n    x_90=(0,energy_given_each_galaxy_above_index)#Lacerta I (Andromeda XXXI)\n    x_91=(0,energy_given_each_galaxy_above_index)#Large Magellanic Cloud (LMC)\n    x_92=(0,energy_given_each_galaxy_above_index)#Leo A (Leo III, DDO 69)\n    x_93=(energy_given_each_galaxy_below_index,None)#Leo I Dwarf (DDO 74, UGC 5470)\n    x_94=(0,energy_given_each_galaxy_above_index)#Leo II Dwarf (Leo B, DDO 93)\n    x_95=(0,energy_given_each_galaxy_above_index)#Leo IV Dwarf\n    x_96=(0,energy_given_each_galaxy_above_index)#Leo P\n    x_97=(0,energy_given_each_galaxy_above_index)#Leo T Dwarf\n    x_98=(energy_given_each_galaxy_below_index,None)#Leo V Dwarf\n    x_99=(0,energy_given_each_galaxy_above_index)#M110 (NGC 205)\n    x_100=(0,energy_given_each_galaxy_above_index)#M32 (NGC 221)\n    x_101=(0,energy_given_each_galaxy_above_index)#MB 1 (KK98 21)\n    x_102=(0,energy_given_each_galaxy_above_index)#MB 3\n    x_103=(0,energy_given_each_galaxy_above_index)#Maffei 1\n    x_104=(0,energy_given_each_galaxy_above_index)#Maffei 2\n    x_105=(0,energy_given_each_galaxy_above_index)#NGC 147 (DDO 3)\n    x_106=(0,energy_given_each_galaxy_above_index)#NGC 1560\n    x_107=(0,energy_given_each_galaxy_above_index)#NGC 1569 (UGC 3056)\n    x_108=(0,energy_given_each_galaxy_above_index)#NGC 185\n    x_109=(0,energy_given_each_galaxy_above_index)#NGC 2366\n    x_110=(0,energy_given_each_galaxy_above_index)#NGC 2403\n    x_111=(0,energy_given_each_galaxy_above_index)#NGC 247\n    x_112=(0,energy_given_each_galaxy_above_index)#NGC 300\n    x_113=(0,energy_given_each_galaxy_above_index)#NGC 3109\n    x_114=(0,energy_given_each_galaxy_above_index)#NGC 3741\n    x_115=(0,energy_given_each_galaxy_above_index)#NGC 404\n    x_116=(0,energy_given_each_galaxy_above_index)#NGC 4163 (NGC 4167)\n    x_117=(0,energy_given_each_galaxy_above_index)#NGC 4214 (UGC 7278)\n    x_118=(0,energy_given_each_galaxy_above_index)#NGC 5102\n    x_119=(0,energy_given_each_galaxy_above_index)#NGC 5206\n    x_120=(0,energy_given_each_galaxy_above_index)#NGC 5237\n    x_121=(0,energy_given_each_galaxy_above_index)#NGC 55\n    x_122=(0,energy_given_each_galaxy_above_index)#Pegasus Dwarf Irregular (DDO 216)\n    x_123=(0,energy_given_each_galaxy_above_index)#Pegasus Dwarf Sph (And VI)\n    x_124=(0,energy_given_each_galaxy_above_index)#Pegasus III\n    x_125=(0,energy_given_each_galaxy_above_index)#Perseus I (Andromeda XXXIII)\n    x_126=(0,energy_given_each_galaxy_above_index)#Phoenix Dwarf Galaxy (P 6830)\n    x_127=(0,energy_given_each_galaxy_above_index)#Phoenix II\n    x_128=(0,energy_given_each_galaxy_above_index)#Pictor II\n    x_129=(0,energy_given_each_galaxy_above_index)#Pisces Dwarf\n    x_130=(0,energy_given_each_galaxy_above_index)#Pisces I\n    x_131=(0,energy_given_each_galaxy_above_index)#Pisces II\n    x_132=(0,energy_given_each_galaxy_above_index)#Pisces III (Andromeda XIII)\n    x_133=(0,energy_given_each_galaxy_above_index)#Pisces IV (Andromeda XIV)\n    x_134=(0,energy_given_each_galaxy_above_index)#Pisces V (Andromeda XVI)\n    x_135=(0,energy_given_each_galaxy_above_index)#Reticulum II\n    x_136=(0,energy_given_each_galaxy_above_index)#Reticulum III\n    x_137=(0,energy_given_each_galaxy_above_index)#Sagittarius Dwarf Irregular Galaxy (SagDIG)\n    x_138=(0,energy_given_each_galaxy_above_index)#Sagittarius Dwarf Sphr SagDEG\n    x_139=(0,energy_given_each_galaxy_above_index)#Sagittarius II\n    x_140=(0,energy_given_each_galaxy_above_index)#Sculptor Dwarf (E351-G30)\n    x_141=(0,energy_given_each_galaxy_above_index)#Segue 1\n    x_142=(0,energy_given_each_galaxy_above_index)#Segue 2\n    x_143=(0,energy_given_each_galaxy_above_index)#Sextans A (92205, DDO 75)\n    x_144=(0,energy_given_each_galaxy_above_index)#Sextans B (UGC 5373)\n    x_145=(0,energy_given_each_galaxy_above_index)#Sextans Dwarf Sph\n    x_146=(0,energy_given_each_galaxy_above_index)#Small Magellanic Cloud (SMC, NGC 292)\n    x_147=(0,energy_given_each_galaxy_above_index)#Triangulum II\n    x_148=(0,energy_given_each_galaxy_above_index)#Tucana II\n    x_149=(0,energy_given_each_galaxy_above_index)#Tucana III\n    x_150=(0,energy_given_each_galaxy_above_index)#Tucana IV\n    x_151=(0,energy_given_each_galaxy_above_index)#UGC 4483\n    x_152=(0,energy_given_each_galaxy_above_index)#UGC 4879 (VV124)[61]\n    x_153=(0,energy_given_each_galaxy_above_index)#UGC 7577 (DDO 125)\n    x_154=(0,energy_given_each_galaxy_above_index)#UGC 8508 (I Zw 060)\n    x_155=(0,energy_given_each_galaxy_above_index)#UGC 8651 (DDO 181)\n    x_156=(0,energy_given_each_galaxy_above_index)#UGC 8833\n    x_157=(0,energy_given_each_galaxy_above_index)#UGC 9128 (DDO 187)\n    x_158=(0,energy_given_each_galaxy_above_index)#UGC 9240 (DDO 190)\n    x_159=(0,energy_given_each_galaxy_above_index)#UGCA 105\n    x_160=(0,energy_given_each_galaxy_above_index)#UGCA 133 (DDO 44)\n    x_161=(0,energy_given_each_galaxy_above_index)#UGCA 15 (DDO 6)\n    x_162=(0,energy_given_each_galaxy_above_index)#UGCA 276 (DDO 113)\n    x_163=(0,energy_given_each_galaxy_above_index)#UGCA 292\n    x_164=(0,energy_given_each_galaxy_above_index)#UGCA 438 (ESO 407-018)\n    x_165=(0,energy_given_each_galaxy_above_index)#UGCA 86\n    x_166=(0,energy_given_each_galaxy_above_index)#UGCA 92\n    x_167=(0,energy_given_each_galaxy_above_index)#Ursa Major I Dwarf (UMa I dSph)\n    x_168=(0,energy_given_each_galaxy_above_index)#Ursa Major II Dwarf\n    x_169=(0,energy_given_each_galaxy_above_index)#Ursa Minor Dwarf\n    x_170=(0,energy_given_each_galaxy_above_index)#Virgo I\n    x_171=(0,energy_given_each_galaxy_above_index)#Willman 1\n    x_172=(energy_given_each_galaxy_below_index,None)#Wolf-Lundmark-Melotte (WLM, DDO 221)\n    bounds=[x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10,x_11,x_12,x_13,x_14,x_15,\n                                      x_16,x_17,x_18,x_19,x_20,x_21,x_22,x_23,x_24,x_25,x_26,x_27,x_28,x_29,\n                                      x_30,x_31,x_32,x_33,x_34,x_35,x_36,x_37,x_38,x_39,x_40,x_41,x_42,x_43,x_44,\n                                      x_45,x_46,x_47,x_48,x_49,x_50,x_51,x_52,x_53,x_54,x_55,x_56,x_57,x_58,x_59,x_60\n                                      ,x_61,x_62,x_63,x_64,x_65,x_66,x_67,x_68,x_69,x_70,x_71,x_72,x_73,x_74,x_75,\n                                      x_76,x_77,x_78,x_79,x_80,x_81,x_82,x_83,x_84,x_85,x_86,x_87,x_88,x_89,x_90,x_91,x_92,\n                                      x_93,x_94,x_95,x_96,x_97,x_98,x_99,x_100,x_101,x_102,x_103,x_104,x_105,x_106,x_107,\n                                      x_108,x_109,x_110,x_111,x_112,x_113,x_114,x_115,x_116,x_117,x_118,x_119,x_120,x_121,x_122,\n                                      x_123,x_124,x_125,x_126,x_127,x_128,x_129,x_130,x_131,x_132,x_133,x_134,x_135,x_136,\n                                      x_137,x_138,x_139,x_140,x_141,x_142,x_143,x_144,x_145,x_146,x_147,x_148,x_149,x_150,\n                                      x_151,x_152,x_153,x_154,x_155,x_156,x_157,x_158,x_159,x_160,x_161,x_162,x_163,x_164,x_165,\n                                      x_166,x_167,x_168,x_169,x_170,x_171,x_172]\n    print(len(bounds))\n    results=linprog(c, A_eq=A, b_eq=b,bounds=bounds , method='revised simplex')\n    return results\n","081a437f":"def energy_divide(Y_combine):\n    temp_2=pd.DataFrame()\n    \n    temp=pd.read_csv('Final Predictions Combined.csv').iloc[:,1:]\n    temp=temp.sort_values(by='y')\n    counts=pd.DataFrame(test['galaxy'].value_counts()).rename(columns={'galaxy':'count'})\n    \n    temp_2['Predicted']=Y_combine\n    temp_2['Energy_Allocated']=0\n    temp_2['galaxy']=test['galaxy']\n    temp_2.set_index('galaxy',inplace=True)\n    for x in temp.values:\n        galaxy=x[0]\n        index=x[2]\n        if index<0.8:\n            total_number=counts.loc[galaxy,'count']\n            energy_give=100\/total_number\n            temp_2.loc[galaxy,'Energy_Allocated']=energy_give\n        else:\n            total_number=counts.loc[galaxy,'count']\n            energy_give=100\/total_number\n            temp_2.loc[galaxy,'Energy_Allocated']=energy_give\n        \n    temp_2.to_csv('sub_temp.csv')        \n            \n","65f3ca07":"### B-2) Scatterplots Y vs Features-Understanding the Relationship","4cd24c08":"### B-2) Dimensonality Reduction and Understanding the Reduced Feature Space","26894db4":"### galactic year","4e7332a4":"### B-1) Correlation of Features with Y","99cdb0de":"#### Observation:\n<p> These features tend to highly correlate with y,therefore they would be having much predictive power. Moreover, even intutively the features correlation seems justified for example Interstellar Data Net users, total (% of population)(Internet and Development seems justifable) is highest correlated feature with the y.<\/p>","539ad70d":"### Galaxy ","aa6cdbd5":"#### Observation:\n<p>These are the features which are top 15 negatively correlated features with y. It might seem that these features would be that usefull in their prediction power. However, these features are usefull if you try intutively understand their correlation, for example Gender Inequality Index (GII) is highest negatively correlated feature with y which seems acceptable as gender inequality and development are always in opposite direction. Therefore these features even though negatively correlated might still be usefull in their prediction.<\/p>","1b502d14":"## Combining CatBoost+CNN-1D\/LSTM+FNN Predictions","752abda1":"## Super Learner","0ca1e728":"# Task-2-Optimization","0dfdcc02":"#### Observation:\n<p> All of the Distribution plot shows some variance which reflect that mean of all features tend to vary across galaxies. Therefore my herustic might work as it capitilizies on this difference of same feature over different galaxies<\/p>","c8f9b241":"#### Purpose of each plot\n<p> 1-Check how well behave is the feature with interested response variable i.e Linear,Non-Linear relation and correlation visibility<\/p>\n<p> 2-I might not use all of the features for my final model, these graphs would help me to give a kickstart as which features are most important with respect to their predicting power and therefore help me in feature subsampling.<\/p>\n\nEven though these graphs are alot but I have checked them one by one and have ranked the features as follow, the ranking is in terms of correlation, Linear Behaviour(More Linear the Better) and Noise of the feature extreme values-when looking each graph I have given each feature equal importance etc:\n\n\n","5cd88aed":"# E) Ensembling","e016c5d6":"<p> Super Learner procedure can be summarized as follows:\n\n1. Select a k-fold split of the training dataset.\n2. Select m base-models or model configurations.\n3. For each basemodel:\na. Evaluate using k-fold cross-validation.\nb. Store all out-of-fold predictions.\nc. Fit the model on the full training dataset and store.\n4. Fit a meta-model on the out-of-fold predictions.\n5. Evaluate the model on a holdout dataset or use model to make predictions.<\/.P>\n\n<img src='https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/10\/Diagram-Showing-the-Data-Flow-of-the-Super-Learner-Algorithm.png'>","137cca90":"## Step -1","a2f5480e":"#### Observation\nPlotting Y with Galantic year seems to show very interesting pattern. There is major bump in value at the midpoint of 1010000 & 1015000 which means that galantic year at this phase cannot be used to predict the value y. Otherwise, there seems no pattern of either seasonality or trend as you might expect in a timeseries. The Y values is combined for all galaxies prehaps thats reason as it averages our time","6e5f217f":"#### Observation:\nI used scatterplot so that I can exactly pinpoint the coordinate which causes the major bump shown in the previous lineplot, as visible the top-right thick 10 points tend to be extreme and  causing the major bump in the previous line plot.","4c555927":"#### Observation:\n<p> I saved the correlation into excel file so that I could see the correlation of feature more carefully with y our response variable<\/p>","3e01c27a":"<img src='https:\/\/prohack.org\/helloMan\/HeLLO1.png' width=\"100\" height=\"100\">","100d2001":"# B) My Approach-Feature Engineering and Feature Understanding","296e3a30":"# D) CNN-1D\/LSTM Approach","c14b82dd":"### Catboost","d5ec6f6e":"#### Latest-Run-20200601-011240-Graph\n![image.png](attachment:image.png)","58c43103":"# International Data Science Hackathon by \n<img src='https:\/\/www.underconsideration.com\/brandnew\/archives\/mckinsey_logo.png'>","c47052a7":"# C) Making Simple FeedFoward DeepLearning Model","86bd77bc":"# Task 1:Predicting y","b3c64d3c":"#### Observation \nBarplot again showing that year 1012036,1013042,1014049 are the major bump years.","350a8f37":"# A) My Approach-Filling Null Values\n<p1> 1- I will first try to understand how non-null values in this dataset tend to vary with the y index(our response variable).This will help me to understand as how the interested explanatory variable tend to vary or what kind of relationship does it have with our response variable on its own.There are only two non-null variables:1-galactic year 2-galaxy. I will try to understand how these variables as explanatory variable tend to interact independently with our reponse variable y.<\/p1>\n\n<p2>2- Given the dataset, the features which are to be used to predict y have alot of null values therefore we have to come up with an heurstic which can be used to fill the null values. For now, what I will do is that I will take the mean of all features(those which have some null values) across different galaxies. Then I will draw a distribution plot to understand the variation which will help me to guage whether the choosen heurstic is efficent or not. Afterwards, I will fill the null value of a feature with the mean that this feature have with respect to a specific galaxy. Just for calrification an example; lets say feature Mean Population Age(0-14) has a 25 th row with null value and this 25 th row is about the galaxy Tucana Dwarf, then I would calculate the mean of all non-null values of Mean Population Age(0-14) groupedby galaxy Tucana Dwarf and used it to fill the null value of 25 th row for Mean Population Age(0-14)<\/p2>\n\n<p3> 3- However, there are some galaxies which have mean of None with respect to the specific features. This means that for such cases my herustic would fail. Therefore, I took note of all such galaxies and consequently would be filling these null values with the the simple mean of the features irrespective of galaxies. For example let say Mean Adjusted Net Saving have a mean of null across the galaxy Andromeda XII so in this case to fill any null value of  Mean Adjusted Net Saving with galaxy Andromeda XII, I will use the simple mean of Mean Adjusted Net Saving<\/p3>","a5f96df5":"#### Observation\nI wanted to know how non-linear data is in low space as it will help me in deciding the complexity of model. I used tnse to reduce the dimension of data which preseves local neighborhood distances. Seems, data when reduced to two dimension becomes quite non-linear and distributed as visible by large variance of the low-dimension data. Consequently, even though its difficult to reach the proper conclusion but given the hint from this visualisation, its clear that feature in relation to each other seems to be quite non-linear. Therefore a little complex model wont hurt.","60c8e90c":"## Step -2","2df6d291":"## Step-3","8c847f8c":"#### No more Null Values"}}