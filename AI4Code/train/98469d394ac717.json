{"cell_type":{"e72b9c6a":"code","ce5e5dc1":"code","20c71f18":"code","3c798841":"code","52c06418":"code","5dd11ef7":"code","2a71c32f":"code","aea23331":"code","015a9158":"code","2dc2751f":"code","74d6d75f":"code","9b5a3185":"code","39a06e99":"code","5475b2e7":"code","387c8590":"code","e3031b08":"code","d59b4a37":"code","86fb1e7e":"code","3ce70c79":"code","36849be6":"code","6d735fa5":"code","4195d995":"code","040124d5":"code","e9f8075c":"code","a743f752":"code","5c3acb67":"code","e27ffeb1":"markdown","268398c9":"markdown","bfd6ad57":"markdown","73fbb055":"markdown","63d23efd":"markdown","80a15230":"markdown","3a1248bf":"markdown","d92d63da":"markdown","2f757c06":"markdown","ff2a3db3":"markdown","2ab88604":"markdown","434fb812":"markdown","e61c0fbf":"markdown","ce8f6ecc":"markdown","6c961e8a":"markdown","c0d8b0ea":"markdown","cffd2d6d":"markdown","e1712a1d":"markdown","55f70a66":"markdown","2c8e8449":"markdown","aa8d813a":"markdown","1eea6a66":"markdown","e0009345":"markdown","36f00601":"markdown"},"source":{"e72b9c6a":"import seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nimport matplotlib.image as mpimg\nfrom skimage import io, transform\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torchvision.transforms as transforms\nimport torchvision\nfrom torch import optim,nn\nimport os\nfrom PIL import Image,ImageOps\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\nimport shutil\nimport itertools\nimport glob\nimport random\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping\nfrom keras.metrics import Recall\nfrom sklearn.utils import resample,shuffle\nfrom keras.utils.vis_utils import plot_model\n\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator,array_to_img\nimport tensorflow as tf\ntf.random.set_seed(89)\nnp.random.seed(89)\n\nimport warnings\nwarnings.filterwarnings('ignore')","ce5e5dc1":"path = \"..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\"\ncovid_path = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/COVID'\nnormal_path = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/Normal'\npneumonia_path = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/Viral Pneumonia'\nlung_path = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/Lung_Opacity'","20c71f18":"clases = ['COVID', \"Normal\", \"Viral Pneumonia\", \"Lung_Opacity\"]\ndata_dir = \"..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\"\ntrain_data = []\n\nfor id_clase, clase in enumerate(clases):\n    for file in os.listdir(os.path.join(data_dir, clase)):\n        train_data.append(['{}\/{}'.format(clase, file), id_clase, clase])\n        \ntrain = pd.DataFrame(train_data, columns=['File', 'DiseaseID','Disease Type'])\ntrain.head()","3c798841":"IMAGE_SIZE = 100\ndef read_image(filepath):\n    return cv2.imread(os.path.join(data_dir, filepath), cv2.IMREAD_GRAYSCALE)\n\n# Resize image to target size\ndef resize_image(image, image_size):\n    return cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)","52c06418":"train = train.iloc[np.random.permutation(train.index)].reset_index(drop=True)","5dd11ef7":"train['Disease Type'].value_counts()","2a71c32f":"normal = train[train['Disease Type'] == 'Normal']\nlung_opacity = train[train['Disease Type'] == 'Lung_Opacity']\ncovid = train[train['Disease Type'] == 'COVID']\nviral_pneuomonia = train[train['Disease Type'] == 'Viral Pneumonia']\n\nmenor = viral_pneuomonia.shape[0]\nless_normal = resample(normal, replace = True, n_samples = menor)\nless_opacity = resample(lung_opacity, replace = True, n_samples = menor)\nless_covid = resample(covid, replace = True, n_samples = menor)\n\nunder_train = pd.concat([less_normal,less_opacity, less_covid, viral_pneuomonia])\nunder_train['Disease Type'].value_counts()","aea23331":"under_x = np.zeros((under_train.shape[0], IMAGE_SIZE, IMAGE_SIZE))\n# x = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE))\n# for i, file in tqdm(enumerate(train['File'].values)):\n#     image = read_image(file)\n#     if image is not None:\n#         x[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n        \nfor i, file in tqdm(enumerate(under_train['File'].values)):\n    image = read_image(file)\n    if image is not None:\n        under_x[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))","015a9158":"# x = x \/ 255.\nunder_x = under_x \/ 255.\n# y = pd.get_dummies(train['DiseaseID'], prefix=\"Disease\")\nunder_y = pd.get_dummies(under_train['DiseaseID'])\n\n# X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.25, random_state=1)\nunder_X_train, under_X_val, under_y_train, under_y_val = train_test_split(under_x, under_y, test_size=0.25, random_state=1)","2dc2751f":"#X_train.shape","74d6d75f":"# X_train = X_train.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\n# X_val = X_val.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\n\nunder_X_train = under_X_train.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\nunder_X_val = under_X_val.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)","9b5a3185":"#Samples per class\nplt.figure(figsize=(20,8))\nsns.set(style=\"ticks\", font_scale = 1)\nax = sns.countplot(data = train,y='Disease Type',order = train['Disease Type'].value_counts().index,palette=\"crest\")\nsns.despine(top=True, right=True, left=True, bottom=False)\nplt.xticks(rotation=0,fontsize = 12)\nax.set_ylabel('Sample Type - Diagnosis',fontsize = 14,weight = 'bold')\nplt.title('Number of Samples per Class', fontsize = 16,weight = 'bold');","39a06e99":"types = os.listdir(path)\ntypes = [i for i in types if \".\" not in i]\nprint(types)","5475b2e7":"fig,axs = plt.subplots(3,len(types), figsize=(20,10))\n\nfor m,type_name in zip(range(len(types)),types):\n    path_2 = os.path.join(path, type_name)\n    files = os.listdir(path_2)\n    for j in range(3):\n        file = path_2+'\/'+files[j]       \n        axs[j,m].imshow(cv2.cvtColor(io.imread(file), cv2.COLOR_BGR2RGB))\n        axs[j,m].set_title(type_name)        \n        axs[j,m].axis('off')\nplt.show() ","387c8590":"SIZE= 100\nBATCH_SIZE=64\n\nimage_generator = ImageDataGenerator(rescale=1.\/255, validation_split=0.2)\n\ntrain_dataset = image_generator.flow_from_directory(path, target_size=(SIZE,SIZE), subset=\"training\",\n                            batch_size = BATCH_SIZE, shuffle=True, class_mode='categorical', color_mode = \"grayscale\", seed=42)\n\nvalidation_dataset = image_generator.flow_from_directory(path, target_size=(SIZE,SIZE), subset=\"validation\",\n                            batch_size = BATCH_SIZE, shuffle=True, class_mode='categorical',color_mode = \"grayscale\", seed=42)","e3031b08":"# covid_X = np.empty(shape=(SIZE,SIZE,1))\n# lung_X = np.empty(shape=(SIZE,SIZE,1))\n# normal_X = np.empty(shape=(SIZE,SIZE,1))\n# viral_X = np.empty(shape=(SIZE,SIZE,1))\n\n# covid_y = np.zeros(shape=(1,4))\n# lung_y = np.empty(shape=(1,4))\n# normal_y = np.empty(shape=(1,4))\n# viral_y = np.empty(shape=(1,4))\n\n\n# for i in range(train_dataset.__len__()):\n\n#     X, y = train_dataset.next()\n#     if y[0][0] ==1:\n#         np.append(covid_X, X[0], axis=1)\n#         np.append(covid_y, y, axis=0)\n#     elif y[0][1] ==1:\n#         np.append(lung_X, X[0], axis=1)\n#         np.append(lung_y, y, axis=0)\n#     elif y[0][2] ==1:\n#         np.append(normal_X, X[0], axis=1)\n#         np.append(normal_y, y, axis=0)                \n#     elif y[0][3] ==1:\n#         np.append(viral_X, X[0], axis=1)\n#         np.append(viral_y, y, axis=0)           \n\n# for i in range(validation_dataset.__len__()):\n\n#     X, y = validation_dataset.next()\n#     if y[0][0] ==1:\n#         np.append(covid_X, X[0], axis=1)\n#         np.append(covid_y, y, axis=0)\n#     elif y[0][1] ==1:\n#         np.append(lung_X, X[0], axis=1)\n#         np.append(lung_y, y, axis=0)\n#     elif y[0][2] ==1:\n#         np.append(normal_X, X[0], axis=1)\n#         np.append(normal_y, y, axis=0)                \n#     elif y[0][3] ==1:\n#         np.append(viral_X, X[0], axis=1)\n#         np.append(viral_y, y, axis=0)   \n# print(normal_X)\n# print(normal_y)","d59b4a37":"def entrenarModelos(num_redes, modelos, nombres, X_train, y_train, X_val, y_val, epochs=20,):\n    history = [0] * num_redes\n    #Cambia la tasa de aprendizaje en cada epoch, actualizandola a partir del epoch actual y la funci\u00f3n lambda creada.\n    planificador = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x+epochs))\n    earlyStopping = EarlyStopping(\n        monitor=\"val_recall\",\n        min_delta=0.001,\n        patience=20,\n        verbose=0,\n        restore_best_weights=True,)\n    \n    for i in range(num_redes):\n        history[i] = modelos[i].fit(X_train,y_train, batch_size=80, epochs = epochs, validation_data = (X_val,y_val), callbacks=[planificador, earlyStopping], verbose=0)\n        a = history[i].history['val_recall'].index(max(history[i].history['val_recall']))\n        print(\"RED {0}: Epochs={1:d}, Train recall={2:.5f}, Validation recall={3:.5f}, Train acc={2:.5f}, Validation acc={3:.5f}\".format(\n        nombres[i],epochs,history[i].history['recall'][-1],history[i].history['val_recall'][-1], history[i].history['accuracy'][-1],history[i].history['val_accuracy'][-1] ))","86fb1e7e":"from keras.callbacks import LearningRateScheduler, EarlyStopping\nfrom keras.metrics import Recall\n\nredes = 3\nmodelo = [0]*redes\n\nfor i in range(redes):\n    modelo[i] = Sequential()\n    modelo[i].add(Conv2D(16, kernel_size=3, padding='same', activation='relu', input_shape=(100,100,1)))\n    modelo[i].add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\n    modelo[i].add(MaxPool2D(pool_size = 2))\n    modelo[i].add(Dropout(0.35))\n    \n    if i >= 1:\n        modelo[i].add(Conv2D(32, kernel_size=3, padding='same', activation='relu'))\n        modelo[i].add(Conv2D(32, kernel_size=3, padding='same', activation='relu'))\n        modelo[i].add(MaxPool2D(pool_size = 2))\n        modelo[i].add(Dropout(0.35))\n    \n    if i == 2:\n        modelo[i].add(Conv2D(64, kernel_size=3, padding='same', activation='relu'))\n        modelo[i].add(Conv2D(64, kernel_size=3, padding='same', activation='relu'))\n        modelo[i].add(MaxPool2D(pool_size = 2))\n        modelo[i].add(Dropout(0.35))\n        \n    modelo[i].add(Flatten())\n    modelo[i].add(Dense(64, activation='relu'))\n    modelo[i].add(Dropout(0.35))\n    modelo[i].add(Dense(4, activation='sigmoid'))\n    modelo[i].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"Recall\"])\n\nnombres = [\"1-Capa\",\"2-Capas\",\"3-Capas\"]\nentrenarModelos(redes, modelo, nombres, under_X_train, under_y_train, under_X_val, under_y_val, epochs=20)","3ce70c79":"modelo = Sequential()\nmodelo.add(Conv2D(16, kernel_size=3, padding='same', activation='relu', input_shape=(100,100,1)))\nmodelo.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\nmodelo.add(MaxPool2D(pool_size = 2))\nmodelo.add(Dropout(0.35))\n\nmodelo.add(Conv2D(32, kernel_size=3, padding='same', activation='relu'))\nmodelo.add(Conv2D(32, kernel_size=3, padding='same', activation='relu'))\nmodelo.add(MaxPool2D(pool_size = 2))\nmodelo.add(Dropout(0.35))\n\n#La capa flatten transforma los datos en una dimensi\u00f3n para poder ser procesados por las capas densas.\nmodelo.add(Flatten())\nmodelo.add(Dense(64, activation='relu'))\nmodelo.add(Dropout(0.35))\nmodelo.add(Dense(4, activation='sigmoid'))\nmodelo.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"Recall\"])","36849be6":"plot_model(modelo, \n           show_shapes = True, \n           show_layer_names = True, \n           rankdir = 'TB', \n           expand_nested = False, \n           dpi = 60)","6d735fa5":"datagen = ImageDataGenerator(\n                        width_shift_range=0.05, \n                        height_shift_range=0.05, \n                        zoom_range=0.05,\n                        shear_range=0.1) \ndatagen.fit(under_X_train)","4195d995":"BATCH_SIZE = 64\nearlyStopping = EarlyStopping(\n    monitor=\"val_accuracy\",\n    min_delta=0.001,\n    patience=20,\n    verbose=0,\n    restore_best_weights=True,)\n\nhistory = modelo.fit_generator(datagen.flow(under_X_train, under_y_train, batch_size=BATCH_SIZE),\n            steps_per_epoch = under_X_train.shape[0] \/\/ BATCH_SIZE,\n            epochs=35,\n            verbose=1,\n            callbacks=[earlyStopping],\n            validation_data = (under_X_val,under_y_val))","040124d5":"modelo = Sequential()\nmodelo.add(Conv2D(16, kernel_size=3, padding='same', activation='relu', input_shape=(100,100,1)))\nmodelo.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\nmodelo.add(MaxPool2D(pool_size = 2))\nmodelo.add(Dropout(0.35))\n\nmodelo.add(Conv2D(32, kernel_size=3, padding='same', activation='relu'))\nmodelo.add(Conv2D(32, kernel_size=3, padding='same', activation='relu'))\nmodelo.add(MaxPool2D(pool_size = 2))\nmodelo.add(Dropout(0.35))\n\n#La capa flatten transforma los datos en una dimensi\u00f3n para poder ser procesados por las capas densas.\nmodelo.add(Flatten())\nmodelo.add(Dense(64, activation='relu'))\nmodelo.add(Dropout(0.35))\nmodelo.add(Dense(4, activation='sigmoid'))\nmodelo.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"Recall\"])","e9f8075c":"history = modelo.fit_generator(train_dataset,\n            epochs=35,\n            verbose=1,\n            callbacks=[earlyStopping],\n            validation_data = validation_dataset)","a743f752":"def plot_confusion_matrix(cm, classes, normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nclases = [\"COVID\", \"NORMAL\", \"VIRAL\", \"LUNG\"]\n# Predict the values from the validation dataset\nY_pred = modelo.predict(under_X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = [np.argmax(x) for x in under_y_val.values]\nY_true = np.array(Y_true)\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = clases) ","5c3acb67":"errors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = under_X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((100,100)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n    fig.tight_layout()\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","e27ffeb1":"## 3.2 Modelo definitivo","268398c9":"## 1.4 Cargar las im\u00e1genes en blanco y negro\nCargamos las imagenes en blanco y negro y las almacenamos, ya que anteriormente solamente habiamos guardado las rutas de las imagenes","bfd6ad57":"# 3. Definici\u00f3n de la red","73fbb055":"# Detecci\u00f3n Covid,Pneumon\u00eda y Opacidades en Radiograf\u00edas - Pr\u00e1cticas Xeridia\n## Autores: Alejandro P\u00e9rez, Rub\u00e9n Gonz\u00e1lez, Santiago Valbuena\n* **1. Carga y tratamiento de las imagenes**\n    * 1.1 Cargar las rutas y clases\n    * 1.2 Reordenar las im\u00e1genes\n    * 1.3 Undersampling de im\u00e1genes tipo \"normal\"\n    * 1.4 Cargar las im\u00e1genes\n    * 1.5 Divisi\u00f3n conjunto de datos , Separar el Target y Normalizar\n    * 1.6 Formatear dimensiones dataset\n* **2. EDA**\n    * 2.1 Mostrar im\u00e1genes dataset\n* **3. Definici\u00f3n de la red**\n    * 3.1 Elecci\u00f3n de capas\n    * 3.2 Modelo definitivo\n    * 3.3 Estructura de las capas de red\n    * 3.4 Prueba con under-sampling\n    * 3.5 Prueba sin modificar\n* **4. Evaluaci\u00f3n**\n","63d23efd":"## 3.3 Estructura en capas de red","80a15230":"Se tienen 21165 imagenes repartidas en:\n- Covid-19\n- Lung Opacity\n- Normal\n- Viral Pneumonia\n\nTodas estas im\u00e1genes est\u00e1n en PNG y tienen una resoluci\u00f3n de 299x299 pixels.\nVemos que hay una gran diferencia en el n\u00famero de imagenes en las distintas clases, esto puede dar lugar a bias y varianza en nuestro modelo ya que el peso de predecir un tipo de dato u otro en el entrenamiento varia. Para evirtar abordaremos el problema de tres formas distintas.\n* Dejarlo como esta: Probaremos el modelo con los datos sin modificar y comprobaremos los resultados\n* Under-sampling: Reduciremos el n\u00famero de datos de la clase mayoritaria a los de la clase con menos datos para igualar el n\u00famero en todos ellos.\n* Over-sampling: Aumentaremos los datos de las clases minoritarias mediante t\u00e9cnicas de aumento de datos a los de la clase mayoritaria para igualarlos.","3a1248bf":"## 1.3 Undersampling de im\u00e1genes tipo \"normal\"","d92d63da":"La clase 0 se corresponde con COVID, 1 con NORMAL, 2 con Viral Pneumonia, 3 con Lung Opacity","2f757c06":"Hasta ahora hay esta cantidad de im\u00e1genes seg\u00fan su tipo","ff2a3db3":"## 3.1 Elecci\u00f3n de capas\nRealizaremos pruebas con redes de 1 capa, 2 capas y 3 capas. Cada capa formada por una capa convoluci\u00f3n y otra de maxpooling.","2ab88604":"# 2. EDA\nAnalizaremos el problema y veremos ejemplos de imagenes de cada una de las clases.","434fb812":"## 1.6 Formatear dimensiones dataset\nCambiaremos las dimensiones del dataset para que pueda utilizarse en el modelo ya que se necesitan 4 dimensiones. Para ello a\u00f1adiremos una dimension con -1 sin cambiar los datos.","e61c0fbf":"La pandemia del coronavirus ha supuesto un gran problema a nivel global. Una buena t\u00e9cnica de detecci\u00f3n de COVID-19 puede ser crucial para ayudar a los profesionales de la salud.\nEs muy dif\u00edcil distinguir entre casos  distintos de enfermedades virales como neumon\u00edas. ","ce8f6ecc":"Como vemos en los resultados la mejor opci\u00f3n es usar dos capas ya que obtenemos la mejor puntuaci\u00f3n.","6c961e8a":"## 1.2 Reordenar las im\u00e1genes\nReordenamos aleatoriamente las imagenes ya que se encuentran agrupadas por clases ya que se van cargando directorio por directorio.","c0d8b0ea":"## 1.5 Divisi\u00f3n conjunto de datos , Separar el Target y Normalizar\nDividimos el conjunto de datos en validaci\u00f3n y entrenamiento","cffd2d6d":"Observamos que la mayoria de las clases se han predicho correctamente excepto unas pocas clases y donde mas error se concentra es en la predicci\u00f3n de","e1712a1d":"### Distintas formas de cargar los datos no usadas","55f70a66":"## 3.5 Prueba sin modificar","2c8e8449":"## 3.4 Prueba con undersampling","aa8d813a":"Aqui vemos alguno de los fallos que se han predecido mal.","1eea6a66":"# 1. Carga y tratamiento de las imagenes\n## 1.1 Cargar las rutas y clases","e0009345":"# 4 Evaluaci\u00f3n\n\n## Matriz de confusi\u00f3n y errores","36f00601":"## 2.1 Mostrar las imagenes del dataset\nVemos que carpetas tenemos en nuestro dataset"}}