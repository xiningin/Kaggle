{"cell_type":{"44d8f8aa":"code","de867e4e":"code","15bb2229":"code","045ce13f":"code","8e8be920":"code","4f4aec4d":"code","87005b85":"code","ace7c85a":"code","2bda54bf":"code","3dc059d3":"code","5c1319da":"code","bef97190":"code","d303b50a":"code","3759b82a":"code","e4d59f98":"code","2f514a87":"code","0e6c6356":"code","20dfd629":"code","cfedd353":"code","1f800cae":"code","cb455a1e":"code","4d28a484":"code","c090a8dd":"code","06092818":"code","9b0d1cd4":"code","d6b57dd8":"code","dd2825d1":"code","f4f4444a":"code","0e8bc6bc":"code","75fd5265":"code","74645352":"code","d103508a":"code","ca67e0bd":"code","9a2a67c5":"code","8d9dddf5":"code","c80fdda9":"markdown"},"source":{"44d8f8aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","de867e4e":"test_data_sub=pd.read_csv('..\/input\/test.csv')\ntrain_data=pd.read_csv('..\/input\/train.csv')\nsubmission_data=pd.read_csv('..\/input\/sample-submission.csv')","15bb2229":"test_data_sub.head()","045ce13f":"submission_data.head()","8e8be920":"train_data.head()","4f4aec4d":"test_data_sub.head()","87005b85":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","ace7c85a":"appos = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"\n}","2bda54bf":"text_data=train_data['text']","3dc059d3":"from string import ascii_letters\ntext=text_data[3]\nprint(text)\nlower_case = text.lower()\nwords = lower_case.split()\nreformed = [appos[word] if word in appos else word for word in words]\nreformed_test=list()\nfor word in reformed:\n    if word not in stop_words and '@' not in word and '&' not in word :\n        reformed_test.append(word)\nreformed = \" \".join(reformed_test) \npunct_text= \"\".join([ch for ch in reformed if ch in ascii_letters or ch==\" \"])\npunct_text","5c1319da":"from string import punctuation\ndef review_formatting(reviews):\n    all_reviews=list()\n    for text in reviews:\n        lower_case = text.lower()\n        words = lower_case.split()\n        reformed = [appos[word] if word in appos else word for word in words]\n        reformed_test=list()\n        for word in reformed:\n            if word not in stop_words and '@' not in word and '&' not in word:\n                reformed_test.append(word)\n        reformed = \" \".join(reformed_test) \n        punct_text= \"\".join([ch for ch in reformed if ch in ascii_letters or ch==\" \"])\n        all_reviews.append(punct_text)\n    all_text = \" \".join(all_reviews)\n    all_words = all_text.split()\n    return all_reviews, all_words","bef97190":"from collections import Counter \ndef get_dict(all_reviews, all_words):\n# Count all the words using Counter Method\n    count_words = Counter(all_words)\n    total_words=len(all_words)\n    sorted_words=count_words.most_common(total_words)\n    vocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}\n    return vocab_to_int","d303b50a":"def encode_reviews(reviews, vocab_to_int):\n    \"\"\"\n    encode_reviews function will encodes review in to array of numbers\n    \"\"\"   \n    encoded_reviews=list()\n    for review in reviews:\n        review = review.lower()\n        encoded_review=list()\n        for word in review.split():\n            if word not in vocab_to_int.keys():\n                encoded_review.append(0)\n            else:\n                encoded_review.append(vocab_to_int[word])\n        encoded_reviews.append(encoded_review)\n    return encoded_reviews","3759b82a":"all_reviews, all_words=review_formatting(text_data)\nvocab_to_int=get_dict(all_reviews, all_words)\nencoded_reviews=encode_reviews(all_reviews, vocab_to_int)","e4d59f98":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nreview_len=[len(encoded_review) for encoded_review in encoded_reviews]\npd.Series(review_len).hist()\nplt.show()\npd.Series(review_len).describe()","2f514a87":"#we need to remove tweet with zeros length\nupdated_reviews=list()\nupdated_labels=list()\nnull_index_list=list()\nfor i, encoded_review in enumerate(encoded_reviews):\n    if (len(encoded_review)==0):\n        null_index_list.append(i)\n    else:\n        updated_reviews.append(encoded_reviews[i])\n        updated_labels.append(train_data['target'].values[i])\nprint(len(encoded_reviews),len(null_index_list)+len(updated_reviews),len(updated_labels))\n","0e6c6356":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nreview_len=[len(encoded_review) for encoded_review in updated_reviews]\npd.Series(review_len).hist()\nplt.show()\npd.Series(review_len).describe()","20dfd629":"def pad_sequences(encoded_reviews, sequence_length=10):\n    ''' \n    Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n    '''\n    features=np.zeros((len(encoded_reviews), sequence_length), dtype=int)\n    \n    for i, review in enumerate(encoded_reviews):\n        review_len=len(review)\n        if (review_len<=sequence_length):\n            zeros=list(np.zeros(sequence_length-review_len))\n            new=zeros+review\n        else:\n            new=review[:sequence_length]\n        features[i,:]=np.array(new)\n    return features","cfedd353":"features=pad_sequences(updated_reviews, sequence_length=10)","1f800cae":"true_labels=[1 if label==4 else 0 for label in updated_labels]","cb455a1e":"#split_dataset into 80% training , 10% test and 10% Validation Dataset\ntrain_x=np.array(features[:int(0.90*len(features))])\ntrain_y=np.array(true_labels[:int(0.90*len(features))])\nvalid_x=np.array(features[int(0.90*len(features)):])\nvalid_y=np.array(true_labels[int(0.90*len(features)):])\nprint(len(train_y), len(train_x), len(valid_y), len(valid_x))","4d28a484":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#create Tensor Dataset\ntrain_data=TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data=TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n\n#dataloader\nbatch_size=500\ntrain_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)","c090a8dd":"# obtain one batch of training data\ndataiter = iter(valid_loader)\nsample_x, sample_y = dataiter.next()\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)","06092818":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","9b0d1cd4":"import torch.nn as nn\n\nclass SentimentRNN(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(SentimentRNN, self).__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear and sigmoid layers\n        self.fc1 = nn.Linear(hidden_dim, 256)\n        self.fc2 = nn.Linear(256, 32)\n        self.fc3 = nn.Linear(32, output_size)\n        self.sig = nn.Sigmoid()\n        \n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n\n        # embeddings and lstm_out\n        x = x.long()\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n    \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","d6b57dd8":"# Instantiate the model w\/ hyperparams\nvocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 1000\nn_layers = 2\n\nnet = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n\nprint(net)","dd2825d1":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","f4f4444a":"# training params\n\nepochs = 2 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n        if(inputs.shape[0] != batch_size):\n                    continue\n                \n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n        \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n                if(inputs.shape[0] != batch_size):\n                    continue\n                if(train_on_gpu):\n                    inputs, labels = inputs.cuda(), labels.cuda()\n\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","0e8bc6bc":"def preprocess(input_test):\n    all_reviews, all_words=review_formatting(input_test)\n    encoded_reviews=encode_reviews(all_reviews, vocab_to_int)\n    final_text=pad_sequences(encoded_reviews, sequence_length=10)\n    return final_text","75fd5265":"def test_model(input_test):\n    output_list=list()\n    batch_size=50   \n    net.eval()\n    with torch.no_grad():\n        test_review=preprocess(input_test)\n        for review in test_review:\n            # convert to tensor to pass into your model\n            feature_tensor = torch.from_numpy(review).view(1,-1)\n            if(train_on_gpu):\n                feature_tensor= feature_tensor.cuda()\n            batch_size = feature_tensor.size(0)\n            # initialize hidden state\n            h = net.init_hidden(batch_size)\n            # get the output from the model\n            output, h = net(feature_tensor, h)\n            pred = torch.round(output.squeeze()) \n            output_list.append(pred)\n        labels=[int(i.data.cpu().numpy()) for i in output_list]\n        return labels\nlabels=test_model(test_data_sub['text'])","74645352":"test_data_sub.head()","d103508a":"out_labels=[4*i for i in labels]\nlen(out_labels)","ca67e0bd":"len(test_data_sub)","9a2a67c5":"output = pd.DataFrame()\noutput['Id'] = test_data_sub['Id']\noutput['target'] = out_labels\noutput.to_csv('submission.csv', index=False)","8d9dddf5":"output[:10]","c80fdda9":"## My Approach\nSentimental Analysis of twittes doesn't depends on user Name or Date only tweets are relavent for sentimental Analysis so i am going to analyse only tweet not the user or date"}}