{"cell_type":{"e4a16fe5":"code","f50032aa":"code","c1e11ff6":"code","c263b07f":"code","a7ec14d6":"code","f846855f":"code","6b4a82bf":"code","4169c35b":"code","59f297eb":"code","a40ab908":"code","d919edb9":"code","fdc45f25":"code","bf0ef9cb":"code","d61e30b6":"code","802bf6e4":"code","b1dcb159":"code","57e91959":"code","160a7509":"code","f67c0759":"code","b01968e1":"code","1ab4812d":"code","69705ba4":"code","68fe43d5":"code","cbda0272":"code","169034c4":"code","0b211e93":"code","6f421d6c":"code","38181dbf":"code","96801861":"code","6ed284c8":"code","9555c43d":"code","4693cb22":"code","23f08c16":"code","cbbf1aa9":"code","d3446f6c":"code","5262e01f":"code","66b1409c":"code","c4a10186":"code","fe5a399e":"code","39bd769e":"code","40403060":"code","17c7d8de":"code","8fd28f0b":"code","acba81d8":"code","3e3b04fd":"code","6adfbe77":"code","7631a392":"code","c8e3040a":"code","312a59fe":"code","a7db1fbf":"code","b1123f93":"markdown","ffe5ed56":"markdown","0cb7ea1c":"markdown","ede1c6d5":"markdown","ebc9aadd":"markdown","544c5ce2":"markdown","6470393a":"markdown","48607486":"markdown","e52bc2d8":"markdown","00b73a26":"markdown","c217b814":"markdown","db8cd4e4":"markdown","8827a6af":"markdown","c8288215":"markdown","b23accff":"markdown","5f5a0344":"markdown","fa485a0a":"markdown","6038f278":"markdown","281d4e0c":"markdown","d2413246":"markdown","e5e37c5b":"markdown","56ee9e40":"markdown","e24e80be":"markdown","af642e70":"markdown"},"source":{"e4a16fe5":"#Load necessary libraries for code\nimport numpy as np \nimport os\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy.signal #peak analysis \n\n# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport datetime\n\nimport bq_helper\nimport matplotlib\nimport matplotlib.pyplot as plt \nimport seaborn as sns\ncolor = sns.color_palette()\n\nfrom plotly import tools\nfrom mpl_toolkits.basemap import Basemap\nfrom numpy import array\nfrom matplotlib import cm\n\n# set number of rows in print statements\n# helps with online notebooks\npd.options.display.max_rows = 1000","f50032aa":"# create a helper object for this dataset\nusa_names = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\", dataset_name=\"usa_names\")\n\n# query and export data \nquery = \"\"\"SELECT year, gender, name, sum(number) as number FROM `bigquery-public-data.usa_names.usa_1910_current` GROUP BY year, gender, name\"\"\"\nagg_names = usa_names.query_to_pandas_safe(query)\nagg_names.to_csv(\"usa_names.csv\")","c1e11ff6":"#outputing data according to a certain organization\nagg_names.sort_values([\"gender\", \"name\", \"year\"]).reset_index(drop=True, inplace=True)\nagg_names.head()","c263b07f":"#shape and size of dataset\nagg_names.shape","a7ec14d6":"#get full list of years, check if there are incomplete years\nnp.sort(agg_names.year.unique())","f846855f":"# max and min of years in the dataset\nprint(\"year min and max\", agg_names.year.min(), agg_names.year.max())\nall_years = list(range(agg_names.year.min(), agg_names.year.max()))","6b4a82bf":"# check for categories \nagg_names.gender.unique()","4169c35b":"#total female population for all years\n# Surprise gender!? Then no need to filter dataframe for females\ndf_female = agg_names[agg_names.gender == 'F']\ndf_female_pivot = df_female.pivot(index='year', columns='name', values='number')\ndf_female_pivot.fillna(0, inplace=True)\ndf_female_total = df_female_pivot.sum(axis=1)\ndf_female_total_list = list(df_female.name.unique())","59f297eb":"# graphs for post\n#print(sns.__version__) #0.8.1\n#sns.lineplot(data=df_female_pivot.loc[:, \"Tiana\"], palette=\"tab10\", linewidth=2.5)\n\nprincess_names = [\"Tiana\", \"Ariel\", \"Aurora\", \"Elsa\"]\nmovie_years = [2009, 2000, 1959, 2013]\n\nax = df_female_pivot.loc[:, princess_names].plot(\n    title=\"Babies with \\n Disney Princess Names \\n with Movie Release Years \\n (SSA Data)\"\n    , grid=True, legend=True)\nax.set_ylabel(\"Number of Babies\")\n\n#note release of corresponding Disney Movie with Name\nfor pi in range(0, len(princess_names)): \n    plt.plot(movie_years[pi], \n             np.max(df_female_pivot.loc[:, princess_names[pi]]), '+', color='black', marker='o')                 \n","a40ab908":"def plot_peaks(name, x, indexes, algorithm=None, mph=None, mpd=None):\n    \"\"\"Helper function \n    Plot results of the peak dectection.\n    Function from https:\/\/github.com\/MonsieurV\/py-findpeaks\/blob\/master\/tests\/vector.py\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print('matplotlib is not available.')\n        return\n    _, ax = plt.subplots(1, 1, figsize=(8, 4))\n    ax.plot(x, 'b', lw=1)\n    if indexes.size:\n        label = 'peak'\n        label = label + 's' if indexes.size > 1 else label\n        ax.plot(indexes, x[indexes], '+', mfc=None, mec='r', mew=2, ms=8,\n                label='%d %s' % (indexes.size, label))\n        ax.legend(loc='best', framealpha=.5, numpoints=1)\n    ax.set_xlim(-.02*x.size, x.size*1.02-1)\n    ymin, ymax = x[np.isfinite(x)].min(), x[np.isfinite(x)].max()\n    yrange = ymax - ymin if ymax > ymin else 1\n    ax.set_ylim(ymin - 0.1*yrange, ymax + 0.1*yrange)\n    ax.set_xlabel('Data #', fontsize=14)\n    ax.set_ylabel('Amplitude', fontsize=14)\n    ax.set_title('%s %s (mph=%s, mpd=%s)' % (name, algorithm, mph, mpd))\nplt.show()","d919edb9":"def peak_detection_simple(trend):\n    '''\n    simple approach for demonstration\n    compute differences between consecutive time periods and count the number of sign changes\n    returns the number of sign changes, only if it is from positive to negative \n    '''\n    ch = [x1-x0 for x0,x1 in zip(trend,trend[1:]) if x1 != x0]\n    return sum([1 if c < 0 else 0 for c in ch])","fdc45f25":"test_name = \"Clarissa\"\nprint('number of sign changes from positive to negative: {}'.format(peak_detection_simple(df_female_pivot['Clarissa'])))","bf0ef9cb":"def get_peaks(df, name, d, verbose=False):\n    '''\n    Function takes a dataframe, name (string), d (distance of peak)\n    Returns index or years of applicable time frames\n    '''\n    df_filter = df.loc[:, name]\n    tvec = np.array(df_filter) \n    indexes, _ = scipy.signal.find_peaks(tvec, height=float(tvec.mean()), distance=d)\n    if verbose: \n        print('Peaks are: %s' % (indexes))\n        print(tvec[indexes])\n    return indexes","d61e30b6":"ind = get_peaks(df_female_pivot, test_name, d=5)\nprint('Print output of indicies with peaks for {}: {}'.format(test_name, ind))\nc=0\nfor i in range(1, len(ind)): \n    if ind[i] - ind[i-1] <= 12: \n        #print(indexes[i] - indexes[i-1])\n        c+=1\n\n#calc the range of min and max peaks -- important if within 16 years of each other \nprint('Print output of indicies: {}'.format(c))\nprint('''Is difference between last and first years <= 16 years: {}'''.format(ind[-1] - ind[0] <= 16))","802bf6e4":"plot_peaks(test_name, \n    np.array(df_female_pivot.loc[:, test_name]),\n    ind, mph=0, mpd=0, algorithm='scipy.signal.find_peaks'\n)","b1dcb159":"def yoy_calc(s, df):\n    return df.apply(lambda x: (x - x.shift(s)) \/ x, axis=0)\n\nyoy_female = yoy_calc(1, df_female_pivot)\nyoy5_female = yoy_calc(5, df_female_pivot)","57e91959":"#top 500 name within the last 3 years gets filtered out\n\nnow = datetime.datetime.now()\nlast_3_yrs = list(range(now.year - 3,now.year))\ncount_3_yrs = df_female_pivot.loc[last_3_yrs, :]\ndf_3_yrs = pd.DataFrame(count_3_yrs.sum().reset_index())\ndf_3_yrs.columns= ['names', 'count']\ndf_3_yrs.set_index('names', inplace=True)\ndf_3_yrs['rank_3yr'] = df_3_yrs.rank(ascending=False)","160a7509":"#let's sample some of the top 500 names \ntop_500_list = df_3_yrs[df_3_yrs.loc[:, 'rank_3yr'] <= 500].index\n#df_3_yrs[df_3_yrs.loc[:, 'rank_3yr'] <= 500].sort_values('rank_3yr').head(20)\n","f67c0759":"#dataframe of girls name, not including top 500 \ndf_not_top = df_3_yrs[df_3_yrs.loc[:, 'rank_3yr'] > 500]","b01968e1":"#create fresh dataframe with female names in index\ndf_metrics = pd.DataFrame(index=list(df_female_pivot.columns))\nlast_yr = df_female_pivot.index[-1]","1ab4812d":"#np.array(df_female_pivot.loc[:, 'Ema'])\nnp.array(df_female_total)","69705ba4":"for n in df_female_pivot.columns: \n    ipeaks = get_peaks(df_female_pivot, n, d=5)\n    #print(n, ipeaks)\n    \n    if ipeaks.any(): \n    \n        #acceleration of names over x years \n        df_metrics.loc[n, \"acc_last_1_yr\"] = np.mean(yoy_female.loc[(last_yr):, n])\n        df_metrics.loc[n, \"acc_last_2_yr\"] = np.mean(yoy_female.loc[(last_yr-2):, n])\n        df_metrics.loc[n, \"acc_last_5_yr\"] = np.mean(yoy_female.loc[(last_yr-5):, n])\n        df_metrics.loc[n, \"acc_last_10_yr\"] = np.mean(yoy_female.loc[(last_yr-10):, n])\n        df_metrics.loc[n, \"acc_last_15_yr\"] = np.mean(yoy_female.loc[(last_yr-15):, n])\n        df_metrics.loc[n, \"acc_last_20_yr\"] = np.mean(yoy_female.loc[(last_yr-20):, n])\n        df_metrics.loc[n, \"acc_last_25_yr\"] = np.mean(yoy_female.loc[(last_yr-25):, n])\n\n        #acceleration within the last x years\n        df_metrics.loc[n, \"acc_5_yr\"] = np.divide((df_female_pivot.loc[last_yr, n] - df_female_pivot.loc[last_yr -5, n]), df_female_pivot.loc[last_yr -5, n])\n        df_metrics.loc[n, \"acc_10_yr\"] = np.divide((df_female_pivot.loc[last_yr, n] - df_female_pivot.loc[last_yr -10, n]), df_female_pivot.loc[last_yr -10, n])\n        df_metrics.loc[n, \"acc_15_yr\"] = np.divide((df_female_pivot.loc[last_yr, n] - df_female_pivot.loc[last_yr -15, n]), df_female_pivot.loc[last_yr -15, n])\n        df_metrics.loc[n, \"acc_20_yr\"] = np.divide((df_female_pivot.loc[last_yr, n] - df_female_pivot.loc[last_yr -20, n]), df_female_pivot.loc[last_yr -20, n])\n        df_metrics.loc[n, \"acc_20_yr\"] = np.divide((df_female_pivot.loc[last_yr, n] - df_female_pivot.loc[last_yr -25, n]), df_female_pivot.loc[last_yr -25, n])\n\n        #add peaks metrics to the dataframe\n        peak_pop = np.array(df_female_pivot.loc[:, n])[ipeaks]\n        female_pop = np.array(df_female_total)[ipeaks]\n        peaks_since = [len(df_female_pivot.index) - p for p in ipeaks] #how many years since peak? \n\n        df_metrics.loc[n, \"peak_count\"] = len(ipeaks)\n        df_metrics.loc[n, \"peak_recent_years_since\"] = min(peaks_since)\n        df_metrics.loc[n, \"peaks_within_5_years\"] = 1 if ipeaks[-1] - ipeaks[0] < 6 else 0\n\n        #calculate avg, min, max, median population at peaks\n        pop_perc = list(map(lambda x, y: np.divide(x,y), peak_pop,female_pop))\n        df_metrics.loc[n, \"peaks_avg_pop_perc\"] = np.mean(pop_perc)\n        df_metrics.loc[n, \"peaks_median_pop_perc\"] = np.median(pop_perc)\n        df_metrics.loc[n, \"peaks_min_pop_perc\"] = min(pop_perc)\n        df_metrics.loc[n, \"peaks_max_pop_perc\"] = max(pop_perc)\n\n        #indicators for x years since peaks\n        for pp in peaks_since: \n            if pp <= 5: \n                df_metrics.loc[n, \"peak_last_5_years\"] = 1\n            elif pp <= 10: \n                df_metrics.loc[n, \"peak_last_10_years\"] = 1\n            elif pp < 15:\n                df_metrics.loc[n, \"peak_last_15_years\"] = 1\n            elif pp < 20: \n                df_metrics.loc[n, \"peak_last_20_years\"] = 1\n            elif pp < 25: \n                df_metrics.loc[n, \"peak_last_25_years\"] = 1\n\n        #Ever in top y names in the past x years?\n        df_metrics['top_3_yrs'] = 1 if n in top_500_list else 0\n","68fe43d5":"#check outputs\ndf_metrics.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_metrics.fillna(0, inplace=True)\ndf_metrics.head()","cbda0272":"df_metrics.shape","169034c4":"from sklearn.preprocessing import normalize, StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans","0b211e93":"cats = [\"peaks_within_5_years\", \"peak_last_15_years\", \"peak_last_5_years\", \"top_3_yrs\",\n        \"peak_last_10_years\", \"peak_last_25_years\", \"peak_last_20_years\"]\nuse_cols = [d for d in df_metrics.columns if d not in cats]\nx_nums = df_metrics.loc[:, use_cols]\nx_scale_num = StandardScaler(copy=True, with_mean=True, with_std=True).fit_transform(x_nums)\ndf_scale = pd.DataFrame(data=x_scale_num, index=list(df_metrics.index), columns=use_cols)\nx_merge = df_scale.join(df_metrics.loc[:, cats])\nX = np.array(x_merge.as_matrix())","6f421d6c":"def get_cluster_score(data): \n    for n in range(2, 11):\n        kmeans = KMeans(n_clusters=n).fit(data)\n        label = kmeans.labels_\n        sil_coeff = silhouette_score(data, label, metric='euclidean')\n        print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n, sil_coeff))","38181dbf":"get_cluster_score(X)","96801861":"def get_labeled_clusters(c, data, merged,name_ls, desired_name):\n    tmp = KMeans(n_clusters=c).fit(data).labels_\n    k4 = pd.DataFrame(tmp, index=name_ls, columns=[\"labels\"])\n    k4_labels = pd.concat([k4, merged], axis=1)\n    print(k4_labels.loc[:, [\"labels\"]].reset_index().groupby(\"labels\").count())\n    print(desired_name, ' is in ', k4_labels.loc[desired_name, \"labels\"])\n    \n    return (k4_labels, k4_labels.loc[desired_name, \"labels\"])","6ed284c8":"df_r, p_label = get_labeled_clusters(10, X, x_merge, list(df_metrics.index), \"Pauline\")","9555c43d":"#list of names from cluster of desired name\nr = list(df_r[df_r.labels==p_label].index)\nprint(r)\ndf_r[df_r.labels==p_label].to_csv(\"baby_names_metrics_cluster.csv\")","4693cb22":"ax = df_female_pivot.loc[:, r].plot(\n    title=\"Baby Names \\n Similar to 'Pauline' \\n Clustering with Metrics\\n (SSA Data)\"\n    , grid=True, legend=False)\nax.set_ylabel(\"Number of Babies\")\n","23f08c16":"from sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler","cbbf1aa9":"tmp = df_female.pivot(index='name', columns='year', values='number')\ntmp.fillna(0, inplace=True)\ntmp.shape","d3446f6c":"sc = StandardScaler()\nX_pca_name_ls = list(tmp.index)\nX_pca = np.array(sc.fit_transform(tmp))\nX_pca.shape","5262e01f":"pca_check = PCA(n_components=25)\npca_check.fit(X_pca)\nprint(np.cumsum(pca_check.explained_variance_ratio_))\n\nplt.plot(np.cumsum(pca_check.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title('Name Trends PCA')","66b1409c":"pca = PCA(n_components=10)\nX_pca_transform = pca.fit_transform(X_pca)\nX_pca_transform.shape","c4a10186":"get_cluster_score(X_pca_transform)","fe5a399e":"#get_labeled_clusters(4, X_pca_transform, df_female, X_pca_name_ls, \"Pauline\")\ntmp = KMeans(n_clusters=10).fit(X_pca_transform).labels_\ndata_components = pd.DataFrame(X_pca_transform, index=X_pca_name_ls)\ndata_components['labels'] = tmp\ndata_components.head()\nprint(data_components.loc[:, [\"labels\"]].reset_index().groupby(\"labels\").count())","39bd769e":"p_label2 = data_components.loc[\"Pauline\", \"labels\"]\nprint(p_label2)","40403060":"#list of names from cluster 2\nr2 = list(data_components[data_components.labels==p_label2].index)\nprint(r2)\ndata_components[data_components.labels==p_label2].to_csv(\"baby_names_metrics_cluster.csv\")","17c7d8de":"ax = df_female_pivot.loc[:, r2].plot(\n    title=\"Baby Names \\n Similar to 'Pauline' \\n Clustering with PCA\\n (SSA Data)\"\n    , grid=True, legend=False)\nax.set_ylabel(\"Number of Babies\")","8fd28f0b":"# combine X_pca_transform and X for cluster analysis\n# number of clusters \nX_combo = np.concatenate((X,X_pca_transform),axis=1)\nget_cluster_score(X_combo)","acba81d8":"data_combo, p_label3 = get_labeled_clusters(6, X_combo, df_female, X_pca_name_ls, \"Pauline\")","3e3b04fd":"r3 = list(data_components[data_components.labels==p_label3].index)\nprint(r3)\ndata_combo[data_combo.labels==p_label3].to_csv(\"baby_names_combo_cluster.csv\")","6adfbe77":"ax = df_female_pivot.loc[:, r3].plot(\n    title=\"Baby Names \\n Similar to 'Pauline' \\n Clustering with Combined Feaures\\n (SSA Data)\"\n    , grid=True, legend=False)\nax.set_ylabel(\"Number of Babies\")","7631a392":"both = set(r).intersection(set(r2))\nprint(both)\nprint(len(both))","c8e3040a":"set(r).intersection(set(r3))","312a59fe":"print(len(r))\nprint(len(r2))","a7db1fbf":"#import random\n#random.sample(r3, 10)","b1123f93":"What names are in both lists?","ffe5ed56":"Combined PCA and manual feature lists to get results of names","0cb7ea1c":"Run loop to find number of cluster with best silhoutte score ","ede1c6d5":"Test the peak and helper functions above with sample data. Show trends with plotting data. ","ebc9aadd":"#### **Create dataframe with all features discussed above:**","544c5ce2":"When we use PCA components to partition the names. Any number of clusters between 2 to 10 yields about teh same silhouette score. Let's see how the names are grouped by using both 4 and 10 clusters. I prefer 10 clusters if I can get a shorter list! ","6470393a":"(1) Simple Approach to peak detection ","48607486":"### **(6) Kmeans Clustering**##","e52bc2d8":"Select the number of PCA components desired based on what we learned above. We can always choose more than 4 components. Remember less is more! ","00b73a26":"***Principle Component Analysis and Clustering***","c217b814":"### (3) **Summarize Data**\n* Viewing the following will help create a working knowledge of your datase: \n- sample set of data\n- size \/ shape\n- years represented in the dataset with max and min (make sure years are complete) ","db8cd4e4":"The best score isn't always the best way to partition the data since the purpose of this exercise is to narrow down the list of names. Clustering the data with 2 distinct groups will yield the best silhoutte score. The next best score is clustering into 4. ","8827a6af":"Randomly select names from available lists","c8288215":"# **Optimize Data Science Models with Feature Engineering  **\n## Cluster Analysis, Metrics Development, and PCA with Baby Name Trends Data\n\nThis is a companion notebook to blog article (on medium and personal site) with public USA dataset and executeable code. Please view the article for detailed write up and description of the code. The dataset is provided through Google BigQuery. \n\n**Notebook Sections**\n1. Loading Python Libraries \n2. Loading and Converting Data into CSV\n3. Summarize Data (recommend [Salil Gautam's A Very Extensive Exploratory Analysis Notebook]\n(https:\/\/www.kaggle.com\/salil007\/a-very-extensive-exploratory-analysis-usa-names) for visualizations and summary statistics)\n4. Clustering based on metrics (manually created features) \n\n    a. Setting the criteria for baby names & create the features from the dataset\n     - peak detection algorithm features\n     - top 500 list \n     - year over year growth\n     \n    b. Combine features into one dataframe\n    \n    c. Kmeans clustering on dataframe with new features\n    \n    d. Output list of baby names from selected cluster \n    \n5. Clustering based on PCA (automatically created features) \n\n    a. Perform principle component analysis on baby name trends\n    \n    b. Kmeans clustering on dataframe with pca features\n    \n    d. Output list of baby names from selected cluster \n    \n6. Compare lists from (4) and (5)","b23accff":"(4) What are the most recent trend for name? Create feature to calculate year over year growth. ","5f5a0344":"#### ****Define and find peaks in name popularity****","fa485a0a":"Leave Kmeans clustering to partition names into similar groups. Then generate a narrowed list of names from the cluster containting \"Pauline.\"","6038f278":"Show plot with peaks marked in the trend with \"ind\" calculated from sample name","281d4e0c":"### (1) **Loading Python Libraries**","d2413246":"### (2) **Loading and Converting Data into CSV**","e5e37c5b":"Determine the number of components that explain at least 85% of the variance in the data. In supervised learning, we could test the number of components to be modeled with target variable. The plot of PCA cumulative variance explained by number of components shows that 4 components will explain 88% of the variance. ","56ee9e40":"(5) What are the top 500 names in the past 3 years? Get list of top 500 years over the past 3 years. We can choose to filter out these names in the final list or create another boolean feature ","e24e80be":"(2) Peak Detection with Scipy","af642e70":"### (4) **Clustering based on metrics (manually created features) **\nThe purpose of the features created below are written up in the companion article. "}}