{"cell_type":{"102b7c1e":"code","561a1920":"code","5c552e8c":"code","84285b83":"code","9b222356":"code","bf8306a3":"code","824c9f58":"code","461a9863":"code","b6abc2c9":"code","58876e47":"code","d216718e":"code","e8159112":"code","97704a62":"code","e4370803":"code","bb50c3ed":"code","a65fbdba":"code","90105ae5":"code","45608075":"code","4b68be18":"code","2ed8befe":"code","7784418a":"code","e9ab1922":"code","c87de450":"code","04432ee1":"code","7750e73a":"code","e6e0bc70":"code","70523e02":"code","3fcc7787":"code","5e613fdf":"code","52d3b1ee":"code","1aff6314":"code","d3c8757e":"code","7b253a7e":"code","4a05f15c":"code","9511b686":"code","0b939490":"code","73e17356":"code","de2ce99d":"code","823466f6":"code","51986dc4":"code","9682a937":"code","b92e9dd9":"code","54ab1695":"code","b6a6dcd3":"code","24367cf3":"code","857c43e1":"code","2a373049":"code","0f283a9d":"code","ec74b533":"code","ad7a4523":"code","fb680415":"code","9c5af3f6":"code","e6ce71ac":"code","c08ea8d0":"code","b86978e7":"code","9c98e09c":"code","889309dd":"code","0483e8bb":"code","71c622c7":"code","4e6492fd":"code","6ddff9e5":"code","a36c0552":"code","5fab5183":"code","17c71a49":"code","1a01f1de":"code","41de0f69":"code","5251eb28":"code","aaa7d703":"code","f4466567":"markdown","d4440033":"markdown","e0ac7c2e":"markdown","3dc8a70a":"markdown","c8b89b62":"markdown","5bd8297c":"markdown","e18d1aab":"markdown","23bfe046":"markdown","98c32ebf":"markdown","f0d2e9f3":"markdown","e96da271":"markdown","4e15b275":"markdown","273ca6c9":"markdown","9a7261d1":"markdown","06740c50":"markdown","df541ca1":"markdown","c7032e3f":"markdown","3c36a9c5":"markdown","65d81d3e":"markdown","6f003c56":"markdown","73ee1624":"markdown","9e94fa8c":"markdown","96c16cbc":"markdown","17124fbb":"markdown","8205453c":"markdown","39acaa14":"markdown","268b1d5e":"markdown","e9a08a0b":"markdown"},"source":{"102b7c1e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport random\nimport pandas as pd\nimport scipy.stats as stat\nimport seaborn as sns\nimport os\nimport pandas\nimport sklearn\n\nfrom IPython.display import Image\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.cluster import KMeans\n\n# Para ter repetibilidade nos resultados\nrandom_state = 1\n\n# Tratar valores infinitos como np.NaN\npandas.options.mode.use_inf_as_na = True\n\n# IMPORTANTE para tornar figuras interativas\n%matplotlib notebook\n\n# Tamanho padr\u00e3o das figuras\nfigsize=(10,6)\n\n# Verifica\u00e7\u00e3o do local para carga de dados\npath = os.environ['PATH']\n\nif path.startswith('C'):\n    IN_KAGGLE = False\nelse:\n    IN_KAGGLE = True\n    \n\n# Bibliotecas espec\u00edficas do livro Introduction to Machine Learning with Python\n# https:\/\/github.com\/amueller\/introduction_to_ml_with_python\n# pip install mglearn\n\nimport mglearn\n\n\n# Configura\u00e7\u00e3o do n\u00famero de linhas e colunas a serem apresentadas em listagens\npd.set_option('display.max_row', 1000)\n\npd.set_option('display.max_columns', 50)\n","561a1920":"os.listdir('..\/input')","5c552e8c":"# Fun\u00e7\u00e3o de convers\u00e3o de dados copiada de https:\/\/github.com\/shakedzy\/dython\/blob\/master\/dython\/_private.py\n# Autor Shaked Zychlinski\n\ndef convert(data, to):\n    converted = None\n    if to == 'array':\n        if isinstance(data, np.ndarray):\n            converted = data\n        elif isinstance(data, pd.Series):\n            converted = data.values\n        elif isinstance(data, list):\n            converted = np.array(data)\n        elif isinstance(data, pd.DataFrame):\n            converted = data.as_matrix()\n    elif to == 'list':\n        if isinstance(data, list):\n            converted = data\n        elif isinstance(data, pd.Series):\n            converted = data.values.tolist()\n        elif isinstance(data, np.ndarray):\n            converted = data.tolist()\n    elif to == 'dataframe':\n        if isinstance(data, pd.DataFrame):\n            converted = data\n        elif isinstance(data, np.ndarray):\n            converted = pd.DataFrame(data)\n    else:\n        raise ValueError(\"Unknown data conversion: {}\".format(to))\n    if converted is None:\n        raise TypeError('cannot handle data conversion of type: {} to {}'.format(type(data),to))\n    else:\n        return converted","84285b83":"from sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\n\ndef redes_neurais_regressao(X_, Y_, to_scale=True):\n\n    X_ = convert(X_, 'array')\n        \n    Y_ = convert(Y_, 'array')\n    \n    # Transforma Y em array 1-D\n    #Y_ = np.ravel(Y_)\n    \n    if to_scale:\n        # Escala vari\u00e1veis\n        scaler = MinMaxScaler(feature_range=(0, 1))\n\n        X_escale = scaler.fit_transform(X_) \n        Y_escale = scaler.fit_transform(Y_) \n    else:\n        X_escale = X_\n        Y_escale = Y_\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        X_escale, Y_escale, test_size=0.1, random_state=random_state,shuffle =True)\n\n    estimatorNN = MLPRegressor(\n                              learning_rate = 'adaptive',\n                              random_state = random_state,\n                              verbose=False,\n                                max_iter = 200,\n                            hidden_layer_sizes = [100,50,40,30,20,10],   \n                    solver = 'adam',\n                    alpha = 0.0001,\n                    activation = 'relu'\n                            )\n\n    estimatorNN.fit(x_train,y_train)\n    \n    plt.subplots(figsize=figsize)\n    plt.plot(range(len(y_test)), y_test,'ro')\n    plt.plot(range(len(y_test)), estimatorNN.predict(x_test),'b*')\n    \n\n    plt.ylabel('Estimativa')\n    plt.title('Rede - Estimativa (*) X real (o)')\n    plt.grid(True)\n    plt.show()\n    \n    mean_error = mean_absolute_error(y_test, estimatorNN.predict(x_test))\n    print('\\nErro {}'.format(mean_error))\n    \n    mean_s_error = mean_squared_error(y_test, estimatorNN.predict(x_test))\n    print('\\nErro {}'.format(mean_s_error))\n    \n    r2 = r2_score(y_test, estimatorNN.predict(x_test)) \n    print('\\nR2 Score {}'.format(r2))\n    \n    return estimatorNN,r2","9b222356":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\n\ndef redes_neurais_classificacao(X_, Y_, to_scale=True):\n\n    X_ = convert(X_, 'array')\n        \n    Y_ = convert(Y_, 'array')\n    \n    # Transforma Y em array 1-D\n    Y_ = np.ravel(Y_)\n    \n    if to_scale:\n        # Escala vari\u00e1veis\n        scaler = MinMaxScaler(feature_range=(0, 1))\n\n        X_escale = scaler.fit_transform(X_) \n        #Y_escale = scaler.fit_transform(Y_) \n    else:\n        X_escale = X_\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        X_escale, Y_, test_size=0.1, random_state=random_state,shuffle =True)\n\n    estimatorNN = MLPClassifier(\n                              learning_rate = 'adaptive',\n                              random_state = random_state,\n                              verbose=False,\n                                max_iter = 200,\n                            hidden_layer_sizes = [100,50,40,30,20,10],   \n                    solver = 'adam',\n                    alpha = 0.0001,\n                    activation = 'relu'\n                            )\n\n    estimatorNN.fit(x_train,y_train)\n    \n    plt.subplots(figsize=figsize)\n    plt.plot(range(len(y_test)), y_test,'ro')\n    plt.plot(range(len(y_test)), estimatorNN.predict(x_test),'b*')\n    \n\n    plt.ylabel('Estimativa')\n    plt.title('Rede - Estimativa (*) X real (o)')\n    plt.grid(True)\n    plt.show()\n    \n    # TN FP\n    # FN TP\n    confusion = confusion_matrix(y_test, estimatorNN.predict(x_test))\n    print(\"\\nConfusion matrix:\\n{}\".format(confusion))\n    \n    f1 = f1_score(y_test, estimatorNN.predict(x_test), average ='micro')\n    print(\"\\nf1 score: {:.2f}\".format( f1   ))\n    \n    erro = np.sum(np.abs(estimatorNN.predict(x_test)-y_test))\/len(y_test)\n    print('\\nErro {}'.format(erro))\n    \n    \n    print(classification_report(y_test, estimatorNN.predict(x_test),\n        target_names=[\"Falso\", \"Positivo\"]))\n    \n    return estimatorNN,erro","bf8306a3":"from sklearn.tree import DecisionTreeRegressor\n\ndef arvore_regressao(X_, Y_, to_scale=True):\n    \n    X_ = convert(X_, 'array')\n        \n    Y_ = convert(Y_, 'array')\n    \n    # Transforma Y em array 1-D\n    Y_ = np.ravel(Y_)\n    \n    if to_scale:\n        # Escala vari\u00e1veis\n        scaler = MinMaxScaler(feature_range=(0, 1))\n\n        X_escale = scaler.fit_transform(X_) \n        #Y_escale = scaler.fit_transform(Y_) \n    else:\n        X_escale = X_\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        X_escale, Y_, test_size=0.1, random_state=random_state,shuffle =True)\n    \n    estimatorTree = DecisionTreeRegressor(max_depth=5, random_state = random_state)\n    estimatorTree.fit(x_train,y_train)\n    \n    plt.subplots(figsize=figsize)\n    plt.plot(range(len(y_test)), y_test,'ro')\n    plt.plot(range(len(y_test)), estimatorTree.predict(x_test),'b*')\n    \n\n    plt.ylabel('Estimativa')\n    plt.title('\u00c1rvore - Estimativa (*) X real (o)')\n    plt.grid(True)\n    plt.show()\n    \n    print('Import\u00e2ncias {}'.format(estimatorTree.feature_importances_))\n    \n    mean_error = mean_absolute_error(y_test, estimatorTree.predict(x_test))\n    print('\\nErro {}'.format(mean_error))\n    \n    mean_s_error = mean_squared_error(y_test, estimatorTree.predict(x_test))\n    print('\\nErro {}'.format(mean_s_error))\n    \n    r2 = r2_score(y_test, estimatorTree.predict(x_test)) \n    print('\\nR2 Score {}'.format(r2))\n    \n    return estimatorTree,r2\n    \n","824c9f58":"from sklearn.tree import DecisionTreeClassifier\n\ndef arvore_classificacao(X_, Y_, to_scale=True):\n    \n    X_ = convert(X_, 'array')\n        \n    Y_ = convert(Y_, 'array')\n    \n    # Transforma Y em array 1-D\n    Y_ = np.ravel(Y_)\n    \n    if to_scale:\n        # Escala vari\u00e1veis\n        scaler = MinMaxScaler(feature_range=(0, 1))\n\n        X_escale = scaler.fit_transform(X_) \n        #Y_escale = scaler.fit_transform(Y_) \n    else:\n        X_escale = X_\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        X_escale, Y_, test_size=0.1, random_state=random_state,shuffle =True)\n    \n    estimatorTree = DecisionTreeClassifier(max_depth=5, random_state = random_state)\n    estimatorTree.fit(x_train,y_train)\n    \n    plt.subplots(figsize=figsize)\n    plt.plot(range(len(y_test)), y_test,'ro')\n    plt.plot(range(len(y_test)), estimatorTree.predict(x_test),'b*')\n    \n\n    plt.ylabel('Estimativa')\n    plt.title('\u00c1rvore - Estimativa (*) X real (o)')\n    plt.grid(True)\n    plt.show()\n\n    \n    \n    print('Import\u00e2ncias {}'.format(estimatorTree.feature_importances_))\n    \n    confusion = confusion_matrix(y_test, estimatorTree.predict(x_test))\n    print(\"\\nConfusion matrix:\\n{}\".format(confusion))\n    \n    f1 = f1_score(y_test, estimatorTree.predict(x_test), average ='micro')\n    print(\"\\nf1 score: {:.2f}\".format( f1   ))\n    \n    erro = np.sum(np.abs(estimatorTree.predict(x_test)-y_test))\/len(y_test)\n    print('\\nErro {}'.format(erro))\n    \n    \n    print(classification_report(y_test, estimatorTree.predict(x_test),\n        target_names=[\"Falso\", \"Positivo\"]))\n    \n    return estimatorTree,erro\n    \n    ","461a9863":"if IN_KAGGLE:\n    world_happiness = pd.read_csv(\"..\/input\/world-happiness\/2016.csv\")\nelse:\n    world_happiness = pd.read_csv(\"2016.csv\")\n\n# Conjunto completo\nworld_happiness = world_happiness.loc[:,['Country', 'Region', 'Happiness Rank', 'Happiness Score',\n       'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)',\n       'Freedom', 'Trust (Government Corruption)', 'Generosity',\n       'Dystopia Residual']]\n\n\n\n#world_happiness = shuffle(world_happiness).reset_index(drop=True)\n\n# Conjunto resumido para treinamento de modelos\nworld_happiness_resumido = world_happiness.loc[:,[ 'Happiness Score',\n       'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)',\n       'Freedom', 'Trust (Government Corruption)', 'Generosity']]\n\n# Cria vari\u00e1veis para treinamento de modelos\n\ncolunas_fonte = [ \n       'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)',\n       'Freedom', 'Trust (Government Corruption)', 'Generosity'\n]\n\ncolunas_objetivo = [ \n       'Happiness Score'\n]\n\nworld_happiness_resumido_X = world_happiness_resumido.loc[:,colunas_fonte] \nworld_happiness_resumido_Y = world_happiness_resumido.loc[:,colunas_objetivo]\n\n\nworld_happiness.head(35)","b6abc2c9":"if IN_KAGGLE:\n    tips = pd.read_csv('..\/input\/snstips\/tips.csv')\n    if 'Unnamed: 0' in tips.columns:\n        tips.drop(['Unnamed: 0'], inplace=True, axis=1)\nelse:\n    tips = sns.load_dataset('tips')\n\ntips.head()\n","58876e47":"from sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\n\ncancer_data = cancer['data']\n# 1 benigno, 0 maligno\ncancer_target = cancer['target']\ncancer_target_names  = cancer['target_names']\ncancer_feature_names = cancer['feature_names']","d216718e":"cancer_data_DF = pd.DataFrame(cancer_data,columns=cancer_feature_names) \ncancer_data_DF.head()","e8159112":"cancer_target_DF = pd.DataFrame(cancer_target,columns=['target']) \ncancer_target_DF.head()","97704a62":"np.random.seed(random_state)\n\n\nmu, sigma = 1, 1 \ns = np.random.normal(mu, sigma, 1000)\nruido = np.random.rand(1000)*100\ns_transformado = np.power(10+s*10, 2)+ruido\n\ndata_frame_exemplo = pd.DataFrame(data={'col1': s, 'col2': s_transformado, 'col3': s_transformado, 'grupo':[2]*1000})\n\ndata_frame_exemplo.loc[(data_frame_exemplo.col1<1)&(data_frame_exemplo.col2<500),'grupo'] = 1\ndata_frame_exemplo.loc[(data_frame_exemplo.col1>=0)&\n                       (data_frame_exemplo.col2>=500)&\n                       (data_frame_exemplo.col1<=2)&\n                       (data_frame_exemplo.col2<=1000),'grupo'] = 2\ndata_frame_exemplo.loc[(data_frame_exemplo.col1>2)&\n                       (data_frame_exemplo.col2>1000),'grupo'] = 3\n\n\ndata_frame_exemplo.iloc[0,:] = [-1.022201,23.687699,50000,1]\n\ndata_frame_exemplo.describe()\n\n","e4370803":"data_frame_exemplo.head()","bb50c3ed":"data_frame_exemplo.info()","a65fbdba":"\nf, ax = plt.subplots(figsize=figsize)\n_ = pd.plotting.scatter_matrix(data_frame_exemplo, ax=ax)","90105ae5":"data_frame_exemplo.corr()","45608075":"def analise(data_frame, col1, col2):\n    # Plota dados \n    fig = plt.figure(figsize=figsize)\n    plt.scatter(data_frame[col1], \n                data_frame[col2], \n                c=data_frame.grupo, \n                s=data_frame.grupo*100, \n                alpha=0.3,\n                       cmap='viridis')\n    plt.xlabel(col1)\n    plt.ylabel(col2)\n    plt.title('Col X Col')\n    plt.grid(True) \n    plt.show()\n    \n    # Classifica com rede neural\n    estimatorNN,erro = redes_neurais_classificacao(data_frame.loc[:,[col1,col2]],\n                                                   data_frame.loc[:,['grupo']],\n                                                   to_scale=False)\n    # Classifica\u00e7\u00e3o \u00c1rvore\n    estimatorNN,erro = arvore_classificacao(data_frame.loc[:,[col1,col2]],\n                                                   data_frame.loc[:,['grupo']],\n                                                   to_scale=False)\n    # Clusteriza\n    y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(data_frame.loc[:,[col1,col2]])\n    y_pred = y_pred+1\n    fig = plt.figure(figsize=figsize)\n    plt.scatter(data_frame[col1], \n                data_frame[col2], \n                c=y_pred, \n                s=y_pred*100, \n                alpha=0.3,\n                       cmap='viridis')\n    plt.xlabel(col1)\n    plt.ylabel(col2)\n    plt.title('Clusteriza\u00e7\u00e3o')\n    plt.grid(True) \n    plt.show()","4b68be18":"analise(data_frame_exemplo,'col1','col2')","2ed8befe":"analise(data_frame_exemplo,'col1','col3')","7784418a":"standard = preprocessing.StandardScaler()\ndata_frame_exemplo_scaled = standard.fit_transform(data_frame_exemplo.loc[:,['col1','col2','col3']])\ndata_frame_exemplo_scaled = pd.DataFrame(data=data_frame_exemplo_scaled, columns=['col1','col2','col3']) \ndata_frame_exemplo_scaled.loc[:,'grupo'] = data_frame_exemplo.grupo\ndata_frame_exemplo_scaled.describe()","e9ab1922":"data_frame_exemplo_scaled.corr()","c87de450":"analise(data_frame_exemplo_scaled,'col1','col2')","04432ee1":"analise(data_frame_exemplo_scaled,'col1','col3')","7750e73a":"\nstandard = preprocessing.MinMaxScaler()\ndata_frame_exemplo_scaled = standard.fit_transform(data_frame_exemplo.loc[:,['col1','col2','col3']])\ndata_frame_exemplo_scaled = pd.DataFrame(data=data_frame_exemplo_scaled, columns=['col1','col2','col3']) \ndata_frame_exemplo_scaled.loc[:,'grupo'] = data_frame_exemplo.grupo\ndata_frame_exemplo_scaled.describe()\n","e6e0bc70":"data_frame_exemplo_scaled.corr()","70523e02":"analise(data_frame_exemplo_scaled,'col1','col2')","3fcc7787":"analise(data_frame_exemplo_scaled,'col1','col3')","5e613fdf":"\n\nstandard = preprocessing.Normalizer(norm='l2')\ndata_frame_exemplo_scaled = standard.fit_transform(data_frame_exemplo.loc[:,['col1','col2']])\ndata_frame_exemplo_scaled = pd.DataFrame(data=data_frame_exemplo_scaled, columns=['col1','col2']) \ndata_frame_exemplo_scaled.loc[:,'grupo'] = data_frame_exemplo.grupo\ndata_frame_exemplo_scaled.describe()\n\n\n","52d3b1ee":"data_frame_exemplo_scaled.corr()","1aff6314":"data_frame_exemplo_scaled.head()","d3c8757e":"analise(data_frame_exemplo_scaled,'col1','col2')","7b253a7e":"\n\nstandard = preprocessing.Normalizer(norm='l2')\ndata_frame_exemplo_scaled = standard.fit_transform(data_frame_exemplo.loc[:,['col1','col3']])\ndata_frame_exemplo_scaled = pd.DataFrame(data=data_frame_exemplo_scaled, columns=['col1','col3']) \ndata_frame_exemplo_scaled.loc[:,'grupo'] = data_frame_exemplo.grupo\ndata_frame_exemplo_scaled.describe()\n\n\n","4a05f15c":"analise(data_frame_exemplo_scaled,'col1','col3')","9511b686":"from sklearn.preprocessing import QuantileTransformer\n\n\n\nstandard = QuantileTransformer(n_quantiles=10, random_state=random_state)\ndata_frame_exemplo_scaled = standard.fit_transform(data_frame_exemplo.loc[:,['col1','col2','col3']])\ndata_frame_exemplo_scaled = pd.DataFrame(data=data_frame_exemplo_scaled, columns=['col1','col2','col3']) \ndata_frame_exemplo_scaled.loc[:,'grupo'] = data_frame_exemplo.grupo\ndata_frame_exemplo_scaled.describe()\n\n\n","0b939490":"data_frame_exemplo_scaled.corr()","73e17356":"analise(data_frame_exemplo_scaled,'col1','col2')","de2ce99d":"analise(data_frame_exemplo_scaled,'col1','col3')","823466f6":"min_max_scaler = preprocessing.MinMaxScaler()\ncancer_data_DF_scaled = min_max_scaler.fit_transform(cancer_data_DF)\ncancer_data_DF_scaled = pd.DataFrame(cancer_data_DF_scaled,columns=cancer_feature_names) \ncancer_data_DF_scaled.head()","51986dc4":"f, ax = plt.subplots(1, 1, figsize=figsize)\nf.suptitle('Cancer data', fontsize=14)\n\nsns.boxplot(data=cancer_data_DF_scaled,  ax=ax)\n#ax.set_xticklabels(cancer_data_DF.columns)\nax.set_xlabel(\"Atributos\",size = 12,alpha=0.8)\nax.set_ylabel(\"Valores\",size = 12,alpha=0.8)\nplt.xticks(rotation=60)\nplt.tight_layout()","9682a937":"# Correla\u00e7\u00e3o entre dados\n\nf, ax = plt.subplots(figsize=figsize)\ncorr = pd.concat([cancer_data_DF_scaled, cancer_target_DF], axis=1).corr()\nhm = sns.heatmap(corr, annot=True, ax=ax, cmap=\"coolwarm\",fmt='.0f',\n                 linewidths=.05)\nf.subplots_adjust(bottom=0.3)\nt= f.suptitle('Correla\u00e7\u00e3o entre vari\u00e1veis', fontsize=14)","b92e9dd9":"from pandas.plotting import parallel_coordinates\nfig, ax = plt.subplots(1, 1, figsize=figsize)\n\nparallel_coordinates(frame=pd.concat([cancer_data_DF_scaled, cancer_target_DF], axis=1), \n                     class_column='target'\n                     , ax = ax, )\nplt.xticks(rotation=60)\nfig.subplots_adjust(bottom=0.3)","54ab1695":"pd.concat([cancer_data_DF, cancer_target_DF], axis=1).corr()","b6a6dcd3":"pd.concat([cancer_data_DF_scaled, cancer_target_DF], axis=1).corr()","24367cf3":"estimatorNN,erro = redes_neurais_classificacao(cancer_data_DF,\n                                                   cancer_target_DF,\n                                                   to_scale=False)","857c43e1":"estimatorNN,erro = redes_neurais_classificacao(cancer_data_DF_scaled,\n                                                   cancer_target_DF,\n                                                   to_scale=False)\n    ","2a373049":"estimatorNN,erro = arvore_classificacao(cancer_data_DF,\n                                                   cancer_target_DF,\n                                                   to_scale=False)","0f283a9d":"# Classifica\u00e7\u00e3o \u00c1rvore\nestimatorNN,erro = arvore_classificacao(cancer_data_DF_scaled,\n                                                   cancer_target_DF,\n                                                   to_scale=False)","ec74b533":"if IN_KAGGLE:\n    tips = pd.read_csv('..\/input\/snstips\/tips.csv')\n    if 'Unnamed: 0' in tips.columns:\n        tips.drop(['Unnamed: 0'], inplace=True, axis=1)\nelse:\n    tips = sns.load_dataset('tips')\n\ntips.head()","ad7a4523":"# C\u00f3digos de categorias s\u00e3o estabelecidos na ordem alfab\u00e9tica dos valores da coluna\n# Primeiro transformar a coluna em 'category'\n\ntips['sex'] = tips['sex'].astype('category')\ntips['sex_cat'] = tips['sex'].cat.codes\n\ntips.head()","fb680415":"# A coluna original \u00e9 apagada\n# Usar colchetes na defini\u00e7\u00e3o das colunas a serem codificadas\n\ntips = pd.get_dummies(tips, columns=['smoker'], prefix=['s'])\ntips.head()","9c5af3f6":"# as codifica\u00e7\u00f5es n\u00e3o seguem a posi\u00e7\u00e3o dos elementos das categorias, as numera\u00e7\u00f5es s\u00e3o atribu\u00eddas iniciando em 0 na medida em que os valores aparecem\n\nfrom pandas.api.types import CategoricalDtype\n\ncat_type = CategoricalDtype(categories=['Mon','Tue', 'Wed', 'Thur', 'Fri', 'Sat', 'Sun'],  ordered=True)\ntips['day'] = tips['day'].astype(cat_type)\ntips['day_cat'] =tips['day'].cat.codes\n\ntips.tail()","e6ce71ac":"meals = {'breakfast': 1, 'Lunch' : 2, 'Dinner' : 3 }\ntips['time_cat'] = tips.time.replace(meals, inplace=False)\ntips['time_cat'] = tips.time_cat.astype('int')\ntips.head()","c08ea8d0":"#from sklearn.preprocessing import KBinsDiscretizer\n#enc = KBinsDiscretizer(n_bins=10, encode='ordinal')\n#X_binned = enc.fit_transform(tips.tip)\n#X_binned","b86978e7":"'''\nif IN_KAGGLE:\n    tips = pd.read_csv('..\/input\/snstips\/tips.csv')\n    if 'Unnamed: 0' in tips.columns:\n        tips.drop(['Unnamed: 0'], inplace=True, axis=1)\nelse:\n    tips = sns.load_dataset('tips')\n\ntips.head()\n'''\n","9c98e09c":"from sklearn.preprocessing import Binarizer\nX_binned = preprocessing.Binarizer(threshold=1.1).fit_transform(tips.tip.values.reshape(-1, 1))\nX_binned[:5,:]","889309dd":"tips['TOTAL_LOG'] = np.log(tips.total_bill)\ntips['TOTAL_LOG_INT']  = tips.TOTAL_LOG.astype(int, copy=False)\ntips.TOTAL_LOG_INT.value_counts()","0483e8bb":"tips.info()","71c622c7":"tips.describe()","4e6492fd":"#tips","6ddff9e5":"tips.corr()","a36c0552":"from collections import Counter\n\ndef conditional_entropy(x, y):\n\n    \"\"\"\n\n    Calculates the conditional entropy of x given y: S(x|y)\n\n\n\n    Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Conditional_entropy\n\n\n\n    :param x: list \/ NumPy ndarray \/ Pandas DataFrame\n\n        A sequence of measurements\n\n    :param y: list \/ NumPy ndarray \/ Pandas DataFrame\n\n        A sequence of measurements\n\n    :return: float\n\n    \"\"\"\n\n    # entropy of x given y\n\n    y_counter = Counter(y)\n\n    xy_counter = Counter(list(zip(x,y)))\n\n    total_occurrences = sum(y_counter.values())\n\n    entropy = 0.0\n\n    for xy in xy_counter.keys():\n\n        p_xy = xy_counter[xy] \/ total_occurrences\n\n        p_y = y_counter[xy[1]] \/ total_occurrences\n\n        entropy += p_xy * math.log(p_y\/p_xy)\n\n    return entropy\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = stat.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\ndef theils_u(x, y):\n\n    \"\"\"\n\n    Calculates Theil's U statistic (Uncertainty coefficient) for categorical-categorical association.\n\n    This is the uncertainty of x given y: value is on the range of [0,1] - where 0 means y provides no information about\n\n    x, and 1 means y provides full information about x.\n\n    This is an asymmetric coefficient: U(x,y) != U(y,x)\n\n\n\n    Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Uncertainty_coefficient\n\n\n\n    :param x: list \/ NumPy ndarray \/ Pandas DataFrame\n\n        A sequence of categorical measurements\n\n    :param y: list \/ NumPy ndarray \/ Pandas DataFrame\n\n        A sequence of categorical measurements\n\n    :return: float\n\n        in the range of [0,1]\n\n    \"\"\"\n\n    s_xy = conditional_entropy(x,y)\n\n    x_counter = Counter(x)\n\n    total_occurrences = sum(x_counter.values())\n\n    p_x = list(map(lambda n: n\/total_occurrences, x_counter.values()))\n\n    s_x = stat.entropy(p_x)\n\n    if s_x == 0:\n\n        return 1\n\n    else:\n\n        return (s_x - s_xy) \/ s_x\n    \ndef correlation_ratio(categories, measurements):\n\n    \"\"\"\n\n    Calculates the Correlation Ration (sometimes marked by the greek letter Eta) for categorical-continuous association.\n\n    Answers the question - given a continuous value of a measurement, is it possible to know which category is it\n\n    associated with?\n\n    Value is in the range [0,1], where 0 means a category cannot be determined by a continuous measurement, and 1 means\n\n    a category can be determined with absolute certainty.\n\n\n\n    Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Correlation_ratio\n\n\n\n    :param categories: list \/ NumPy ndarray \/ Pandas DataFrame\n\n        A sequence of categorical measurements\n\n    :param measurements: list \/ NumPy ndarray \/ Pandas DataFrame\n\n        A sequence of continuous measurements\n\n    :return: float\n\n        in the range of [0,1]\n\n    \"\"\"\n\n    categories = convert(categories, 'array')\n\n    measurements = convert(measurements, 'array')\n\n    fcat, _ = pd.factorize(categories)\n\n    cat_num = np.max(fcat)+1\n\n    y_avg_array = np.zeros(cat_num)\n\n    n_array = np.zeros(cat_num)\n\n    for i in range(0,cat_num):\n\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n\n        n_array[i] = len(cat_measures)\n\n        y_avg_array[i] = np.average(cat_measures)\n\n    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))\/np.sum(n_array)\n\n    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n\n    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n\n    if numerator == 0:\n\n        eta = 0.0\n\n    else:\n\n        eta = numerator\/denominator\n\n    return eta","5fab5183":"if IN_KAGGLE:\n    tips = pd.read_csv('..\/input\/snstips\/tips.csv')\n    if 'Unnamed: 0' in tips.columns:\n        tips.drop(['Unnamed: 0'], inplace=True, axis=1)\nelse:\n    tips = sns.load_dataset('tips')\n\ntips.head()","17c71a49":"conditional_entropy(tips.sex,tips.tip)","1a01f1de":"cramers_v(tips.sex,tips.tip)","41de0f69":"theils_u(tips.sex,tips.day)","5251eb28":"#correlation_ratio(tips.sex,tips.day)","aaa7d703":"pd.crosstab(tips.sex,tips.day)","f4466567":"# 5- Escalonamento de vari\u00e1veis\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#preprocessing-scaler\n\nEscalonamento \u00e9 uma forma de transforma\u00e7\u00e3o de vari\u00e1veis. O objetivo \u00e9:\n\n- Diminuir as diferen\u00e7as existentes entre atributos por conta de diferen\u00e7as de escala\n\n- Diminuir a influ\u00eancia de outliers\n\n\nEm ambos os casos a transforma\u00e7\u00e3o \u00e9 importante para visualiza\u00e7\u00e3o correta dos dados, c\u00e1lculos estat\u00edsticos e para o correto funcionamento de algoritmos de minera\u00e7\u00e3o de dados.\n\nO escalonamento pode ser necess\u00e1rio quando vari\u00e1veis s\u00e3o medidas em escalas diferentes ou quando as medidas, ainda que na mesma escala, tenham valores muito diferentes. Al\u00e9m disso, h\u00e1 algoritmos que somente aceitam entradas e sa\u00eddas em determinadas faixas de valores.","d4440033":"#### <br>\n<font size=\"8\" color=\"red\">EXERC\u00cdCIO<\/font>\n\nCalcule os coeficientes de correla\u00e7\u00e3o para tips\n","e0ac7c2e":"# Refer\u00eancias\n\nLivros usados como refer\u00eancia:\n\nIntroduction to Machine Learning with Python\n\nPython Data Science Handbook (https:\/\/www.oreilly.com\/library\/view\/python-data-science\/9781491912126\/)\n\nVisualiza\u00e7\u00e3o:\n\nhttps:\/\/python-graph-gallery.com\/\n\nhttp:\/\/www.apnorton.com\/blog\/2016\/12\/19\/Visualizing-Multidimensional-Data-in-Python\/\n\nhttps:\/\/towardsdatascience.com\/the-art-of-effective-visualization-of-multi-dimensional-data-6c7202990c57\n\nhttps:\/\/www.oreilly.com\/library\/view\/python-data-science\/9781491912126\/ch04.html\n\nhttps:\/\/matplotlib.org\/mpl_toolkits\/mplot3d\/tutorial.html","3dc8a70a":"## Discretiza\u00e7\u00e3o","c8b89b62":"Dados sobre tumores (somente informa\u00e7\u00f5es num\u00e9ricas)\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_breast_cancer.html","5bd8297c":"## Treinamento de rede neural para classifica\u00e7\u00e3o\n\nUsada ao longo do caderno para testar efeitos da redu\u00e7\u00e3o de dimensionalidade\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html","e18d1aab":"## Dados de exemplo\n\nWorld happiness report (http:\/\/worldhappiness.report\/).\n\nSomente vari\u00e1veis num\u00e9ricas","23bfe046":"## Categoriza\u00e7\u00e3o personalizada","98c32ebf":"## An\u00e1lise de vari\u00e1veis transformadas","f0d2e9f3":"## Carrega dados para exerc\u00edcio\n","e96da271":"Data set de gorgetas com vari\u00e1veis categ\u00f3ricas","4e15b275":"## MinMax\n\nEscalona valores para o intervalo [0, 1] ou outro passado como par\u00e2metro","273ca6c9":"## Label encoding","9a7261d1":"## Standardization\n\nConverte m\u00e9dia em zero e desvio padr\u00e3o em um","06740c50":"<font size=\"10\" color=\"black\">Transforma\u00e7\u00e3o de vari\u00e1veis<\/font>\n\nEduardo Chaves Ferreira\n\n","df541ca1":"# 2- Carga de dados\n\n\n","c7032e3f":"## Quantile","3c36a9c5":"# 7- Correla\u00e7\u00e3o entre vari\u00e1veis categ\u00f3ricas\n\nhttps:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9","65d81d3e":"## Treinamento de \u00e1rvore de decis\u00e3o para regress\u00e3o\n\nUsada ao longo do caderno para testar efeitos da redu\u00e7\u00e3o de dimensionalidade\n\nhttp:\/\/scikit-learn.org\/stable\/auto_examples\/tree\/plot_tree_regression.html","6f003c56":"# 6- Transforma\u00e7\u00e3o de vari\u00e1veis\n\n","73ee1624":"# 1- Importa\u00e7\u00e3o de bibliotecas e fun\u00e7\u00f5es gerais usadas no caderno","9e94fa8c":"## Treinamento de rede neural para regress\u00e3o\n\nUsada ao longo do caderno para testar efeitos da redu\u00e7\u00e3o de dimensionalidade\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html","96c16cbc":"## Normaliza\u00e7\u00e3o\n\nVetores de mesmo tamanho","17124fbb":"#### <br>\n<font size=\"8\" color=\"red\">EXERC\u00cdCIO<\/font>\n\nEscalone cancer_data (min_max) e refa\u00e7a as seguintes an\u00e1lises:\n\n- Boxplot dos dados\n- Correla\u00e7\u00f5es (heatmap) entre dados num\u00e9ricos de cancer_data\n- Para cancer_data gr\u00e1fico de coordenadas paralelas (class_column = target)\n\nPergunta: o escalonamento influencia os valores de correla\u00e7\u00e3o?\n\n\nUsar redes neurais para classificar dados antes e depois do escalonamento\n\n","8205453c":"## Treinamento de \u00e1rvore de decis\u00e3o para classifica\u00e7\u00e3o\n\nUsada ao longo do caderno para testar efeitos da redu\u00e7\u00e3o de dimensionalidade\n\nhttp:\/\/scikit-learn.org\/stable\/auto_examples\/tree\/plot_tree_Classifier.html","39acaa14":"## Categoriza\u00e7\u00e3o personalizada","268b1d5e":"## O que ser\u00e1 tratado no curso\n\n- Escalonamento de vari\u00e1veis\n\n- Transforma\u00e7\u00e3o de vari\u00e1veis\n\n- Correla\u00e7\u00e3o entre vari\u00e1veis categ\u00f3ricas\n\n\n\n\n","e9a08a0b":"## Dummies"}}