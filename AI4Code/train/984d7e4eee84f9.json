{"cell_type":{"a7448f7a":"code","396e3dde":"code","b73022df":"code","db3ec8ca":"code","6dc7418c":"code","4b75c1cd":"code","0e89ca68":"code","090813b3":"code","b539544a":"code","2cd73eb5":"code","a339afd7":"code","db940bfd":"code","61a9677c":"code","b93bb16d":"code","ede622e4":"code","840a5f98":"code","075f4be7":"code","3a108d6e":"markdown","5c7c09b8":"markdown","d36865c8":"markdown","6b071546":"markdown","ab1d19a4":"markdown","49485032":"markdown","61195e74":"markdown","5c7f8cea":"markdown","24c33828":"markdown","786419a7":"markdown","038fc7d8":"markdown","f8d4b132":"markdown"},"source":{"a7448f7a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","396e3dde":"# Load data\ntrain_data=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_data.head()","b73022df":"import re\n\ndef preprocessing(data):\n    \"\"\"\n    Preprocessing of the Titanic data\n    \n    Parameters:\n        data (DataFrame): Titanic data\n    \"\"\"\n    # Replace NaN values by mean of values if possible\n    data['Cabin']=data['Cabin'].replace({np.NaN:'none'})\n    data['Embarked']=data['Embarked'].replace({np.NaN:'none'})\n    data['Age']=data['Age'].replace({np.NaN:np.mean(data['Age'])})\n    data['Fare']=data['Fare'].replace({np.NaN:np.mean(data['Fare'])})\n    # Names preprocessing \n    data.loc[:,'Name']=data.loc[:,'Name'].str.lower()\n    data.loc[:,'Name']=data.loc[:,'Name'].str.replace('\\W+', ' ') # remove special chars\n    # Splitting data in attributes and labels\n    data_x = data.loc[:, data.columns != 'Survived']\n    data_y = data.loc[:, data.columns == 'Survived']\n    return data_x, data_y","db3ec8ca":"train_data_x, train_data_y = preprocessing(train_data)\ntest_data_x, test_data_y = preprocessing(test_data)\ntrain_data_x.head()","6dc7418c":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef encode(train_x, test_x):\n    \"\"\"\n    Encode the gender and embark labels\n    Count vectorize their names\n    \n    Parameters:\n        train_x (DataFrame): Titanic training data\n        test_x  (DataFrame): Titanic testing data\n    \"\"\"\n    gender_encoder = LabelEncoder()\n    embark_encoder = LabelEncoder()\n    ticket_encoder = LabelEncoder()\n    cabin_encoder = LabelEncoder()\n    \n    ### Labelling\n    ## Gender\n    train_x, test_x = train_x.copy(), test_x.copy()\n    train_x.loc[:,'Sex'] = gender_encoder.fit_transform(train_x.loc[:,'Sex'])\n    test_x.loc[:,'Sex'] = gender_encoder.transform(test_x.loc[:,'Sex'])\n    \n    ## Embark\n    train_x.loc[:,'Embarked'] = embark_encoder.fit_transform(train_x.loc[:,'Embarked'])\n    test_x.loc[:,'Embarked'] = embark_encoder.transform(test_x.loc[:,'Embarked'])\n    \n    ## Ticket (assume that their ticket number is at least 3 numbers long)\n    # Extract the numbers and  add to new column\n    train_ticket_numbers = train_x.loc[:,'Ticket'].str.extract('(\\d{3,})')\n    train_x.insert(7, \"Ticket Nr.\", train_ticket_numbers, True)\n    train_x.loc[:,'Ticket Nr.'] = train_x.loc[:,'Ticket Nr.'].replace(np.nan, '0')\n    # Remove the numbers from Ticket column and fit the encoder\n    train_x.loc[:,'Ticket'] = train_x.loc[:,'Ticket'].str.replace('(\\d{3,})','')\n    \n    \n    # Extract the numbers and add to new column\n    test_ticket_numbers = test_x.loc[:, 'Ticket'].str.extract('(\\d{3,})')\n    test_x.insert(7, \"Ticket Nr.\", test_ticket_numbers, True)\n    test_x.loc[:,'Ticket Nr.'] = test_x.loc[:,'Ticket Nr.'].replace(np.nan, '0')\n    # Remove the numbers from Ticket column and label encode the rest\n    test_x.loc[:, 'Ticket'] = test_x.loc[:,'Ticket'].str.replace('(\\d{3,})', '')\n    \n    # Fit to both\n    ticket_encoder = ticket_encoder.fit(test_x.append(train_x).loc[:,'Ticket'])\n    \n    # Set the transformed values\n    train_x.loc[:,'Ticket'] = ticket_encoder.transform(train_x.loc[:,'Ticket'])\n    test_x.loc[:, 'Ticket'] = ticket_encoder.transform(test_x.loc[:,'Ticket'])\n    \n    ## Cabin\n    # Extract the letters and  add to new column\n    train_cabin_letter = train_x.loc[:, 'Cabin'].str.extract('([A-Z])')\n    train_x.insert(10, \"Cabin Letter\", train_cabin_letter, True)\n    train_x.loc[:,'Cabin Letter'] = train_x.loc[:,'Cabin Letter'].replace({np.NaN:'none'})\n    # Remove the letters from the cabin number\n    train_x.loc[:,'Cabin'] = train_x.loc[:,'Cabin'].str.replace('([A-Z])', '')\n    train_x.loc[:,'Cabin'] = train_x.loc[:,'Cabin'].str.replace('none', '0')\n    train_x.loc[:,'Cabin'] = train_x.loc[:,'Cabin'].str.replace(' ', '')\n    train_x.loc[:,'Cabin'] = train_x.loc[:,'Cabin'].replace(r'^\\s*$', '0', regex=True)\n    \n    # Extract the letters and  add to new column\n    test_cabin_letter = test_x.loc[:, 'Cabin'].str.extract('([A-Z])')\n    test_x.insert(10, \"Cabin Letter\", test_cabin_letter, True)\n    test_x.loc[:,'Cabin Letter'] = test_x.loc[:,'Cabin Letter'].replace({np.NaN:'none'})\n    # Remove the letters from the cabin number\n    test_x.loc[:,'Cabin'] = test_x.loc[:,'Cabin'].str.replace('([A-Z])', '')\n    test_x.loc[:,'Cabin'] = test_x.loc[:,'Cabin'].str.replace('none', '0')\n    test_x.loc[:,'Cabin'] = test_x.loc[:,'Cabin'].str.replace(' ', '')\n    test_x.loc[:,'Cabin'] = train_x.loc[:,'Cabin'].replace(r'^\\s*$', '0', regex=True)\n    \n    # Fit to both\n    cabin_encoder = cabin_encoder.fit(test_x.append(train_x).loc[:,'Cabin Letter'])\n    \n    # Set the transformed values\n    train_x.loc[:,'Cabin Letter'] = cabin_encoder.transform(train_x.loc[:,'Cabin Letter'])\n    test_x.loc[:,'Cabin Letter'] = cabin_encoder.transform(test_x.loc[:,'Cabin Letter'])\n    \n    \n    ### Count vectorizing\n    ## Names\n    vectorizer = CountVectorizer()\n    train_names = vectorizer.fit_transform(train_x['Name'])\n    train_names_vec = pd.DataFrame(train_names.todense(), columns=vectorizer.get_feature_names())\n    train_x = pd.concat([train_x, train_names_vec], axis=1)\n    train_x = train_x.drop(['Name'], axis=1)\n    \n    test_names = vectorizer.transform(test_x['Name'])\n    test_names_vec = pd.DataFrame(test_names.todense(), columns=vectorizer.get_feature_names())\n    test_x = pd.concat([test_x, test_names_vec], axis=1)\n    test_x = test_x.drop(['Name'], axis=1)\n    return train_x, test_x\n    \ntrain_x, test_x = encode(train_data_x, test_data_x)\ntrain_y = train_data_y","4b75c1cd":"from sklearn.naive_bayes import MultinomialNB\n\ndef naiveBayes(train_x, train_y):\n    \"\"\"\n    Fits a Multinomial Naive Bayes classifier to the given data\n    \n    Parameters:\n        train_x (DataFrame): Titanic training data\n        train_y (DataFrame): Titanic training data labels\n    \"\"\"\n    bayes = MultinomialNB(alpha=0.4)\n    bayes = bayes.fit(train_x, train_y['Survived'].to_numpy())\n    return bayes\n\nbayesModelFinal = naiveBayes(train_x, train_y)","0e89ca68":"from sklearn.tree import DecisionTreeClassifier\n\ndef decisionTree(train_x, train_y):\n    \"\"\"\n    Fits a Decision tree classifier to the given data\n    \n    Parameters:\n        train_x (DataFrame): Titanic training data\n        train_y (DataFrame): Titanic training data labels\n    \"\"\"\n    decisionTree = DecisionTreeClassifier()#max_depth= 6, max_features= 800, min_samples_leaf=9)\n    decisionTree.fit(train_x, train_y['Survived'].to_numpy())\n    return decisionTree\n\ndecistionTreeModelFinal = decisionTree(train_x, train_y)","090813b3":"from sklearn.ensemble import RandomForestClassifier\n\ndef randomForest(train_x, train_y):\n    \"\"\"\n    Fits a Random Forest classifier to the given data\n    \n    Parameters:\n        train_x (DataFrame): Titanic training data\n        train_y (DataFrame): Titanic training data labels\n    \"\"\"\n    randomForest = RandomForestClassifier()\n    randomForest = randomForest.fit(train_x, train_y['Survived'].to_numpy())\n    return randomForest\n\nrandomForestModelFinal = randomForest(train_x, train_y)","b539544a":"from sklearn.neural_network import MLPClassifier\n\ndef mlpClassifier(train_x, train_y):\n        mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=100)\n        mlp.fit(train_x, train_y['Survived'].to_numpy())\n        return mlp\n\nmlpModelFinal = mlpClassifier(train_x, train_y)","2cd73eb5":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\n\ndef crossVal(train_x, train_y):\n    # Bayes\n    bayes = MultinomialNB()\n    alpha = np.arange(0.1, 1.1, 0.05)\n    n_features = range(100, train_x.shape[1])\n    param_grid = dict(alpha=alpha\n                      )\n    # No real best params\n    \n    # DecisionTree\n    decisionTree = DecisionTreeClassifier()\n    max_depth = range(0,200)\n    min_samples_leaf = range(0,10)\n    max_features = np.arange(0,train_x.shape[0],100)\n    param_grid = dict(max_depth=max_depth,\n                      min_samples_leaf=min_samples_leaf,\n                      max_features=max_features\n                     )\n    # Best Score:  0.863551672073986\n    # Best Params:  {'max_depth': 6, 'max_features': 800, 'min_samples_leaf': 9}\n    \n    # MLP\n    mlp = MLPClassifier()\n    hidden_layer_sizes=[(10,10,10), (10,10,10,10), (10,10,10,10,10), (10,10,10,10,10,10),\n                        (100,100,100), (100,100,100,100), (100,100,100,100,100), (100,100,100,100,100,100,100)]\n    max_iter=[100, 500, 1000]\n    param_grid = dict(hidden_layer_sizes=hidden_layer_sizes,\n                      max_iter=max_iter\n                     )\n    # Best Score:  0.6310246674729652\n    # Best Params:  {'hidden_layer_sizes': (10, 10, 10, 10, 10, 10), 'max_iter': 500}\n    \n    # Random Forest\n    randomForest = RandomForestClassifier()\n    n_estimators = [10,100,500,1000,1500,2000]\n    max_depth = [None, 1,2,5,10,15,20,50,100]\n    min_samples_leaf = np.arange(0.1,1.1,0.1)\n    param_grid = dict(n_estimators=n_estimators,\n                      max_depth=max_depth,\n                      min_samples_leaf=min_samples_leaf\n                      )\n    # Best Score:  0.8461495797506732\n    # Best Params:  {'max_depth': 10, 'min_samples_leaf': 0.2, 'n_estimators': 2000}\n    \n\n    grid = GridSearchCV(estimator=randomForest,\n                        param_grid=param_grid,\n                        scoring='roc_auc',\n                        verbose=1,\n                        n_jobs=-1,\n                        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n                        )\n    grid_result = grid.fit(train_x, train_y['Survived'].to_numpy())\n    print(grid_result.cv_results_)\n    print('Best Score: ', grid_result.best_score_)\n    print('Best Params: ', grid_result.best_params_)    \n    return\n\n# crossVal(train_x, train_y)","a339afd7":"def split(percentage, train_df_x, train_df_y):\n    \"\"\"\n    Splits the train data into evaluation and training chunks according to the percentage\n    \n    Parameters:\n        percentage   (float): percentage of the dataframe that should be kept for training\n        train_df (DataFrame): the training dataframe that has to be split\n        train_y  (DataFrame): the labels for the dataframe that need to be spit\n    \"\"\"\n    train_x = train_df_x.sample(frac=percentage,replace=False)\n    train_y = train_df_y.sample(frac=percentage,replace=False)\n    eval_x = train_df_x.drop(train_x.index)\n    eval_y = train_df_y.drop(train_y.index)\n    return train_x, train_y, eval_x, eval_y\n\ntrain_x, train_y, eval_x, eval_y = split(0.7, train_x, train_y)","db940bfd":"from sklearn.metrics import classification_report\n\ndef predict(model, test_x, test_y, report=False):\n    \"\"\"\n    Runs a prediction given the model and the test set\n    \"\"\"\n    prediction = model.predict(test_x)\n    if report:\n        print(np.unique(prediction))\n        print(np.unique(test_y))\n        print(classification_report(test_y, prediction, digits=20))\n    else:\n        return prediction\n    \n    ","61a9677c":"print(\"Bayes (Unoptimized):\")\nbayesModel = naiveBayes(train_x, train_y)\npredict(bayesModel, eval_x, eval_y, True)\nprint(\"\\nBayes (Optimized):\")\nbayesModelOptimized = naiveBayes(train_x, train_y)\npredict(bayesModel, eval_x, eval_y, True)","b93bb16d":"print(\"\\nDecisionTree (Unoptimized):\")\ndecisionTreeModel = decisionTree(train_x, train_y)\npredict(decisionTreeModel, eval_x, eval_y, True)\nprint(\"\\nDecisionTree (Optimized):\")\ndecisionTreeOptimized = DecisionTreeClassifier(max_depth= 6, max_features= 10, min_samples_leaf=9)\npredict(decisionTreeOptimized.fit(train_x, train_y['Survived'].to_numpy()), eval_x, eval_y, True)","ede622e4":"print(\"\\nRandomForest (Unoptimized):\")\nrandomForestModel = randomForest(train_x, train_y)\npredict(randomForestModel, eval_x, eval_y, True)\nprint(\"\\nRandomForest (Optimized):\")\nrandomForestModelOptimized = RandomForestClassifier(max_depth=10,  n_estimators=20, random_state=1)\npredict(randomForestModelOptimized.fit(train_x, train_y['Survived'].to_numpy()), eval_x, eval_y, True)","840a5f98":"print(\"\\nMLPClassifier (Unoptimizied):\")\nmlpModel = mlpClassifier(train_x, train_y)\npredict(mlpModel, eval_x, eval_y, True)\nprint(\"\\nMLPClassifier (Optimizied):\")\nmlpModelOptimized = MLPClassifier(hidden_layer_sizes=(10, 10, 10, 10, 10), max_iter=500)\npredict(mlpModelOptimized.fit(train_x, train_y['Survived'].to_numpy()), eval_x, eval_y, True)","075f4be7":"# Load data\ntrain_data=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# Preprocessing\ntrain_data_x, train_data_y = preprocessing(train_data)\ntest_data_x, test_data_y = preprocessing(test_data)\n\n# Encoding\ntrain_x, test_x = encode(train_data_x, test_data_x)\ntrain_y = train_data_y\n\n# Training\nmodel = RandomForestClassifier(max_depth=10,  n_estimators=20, random_state=1)\nmodel = model.fit(train_x, train_y['Survived'].to_numpy())\n\n# Predicting\npred = model.predict(test_x)\n\noutput = pd.DataFrame({'PassengerId': test_x.PassengerId, 'Survived': pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","3a108d6e":"# Evaluating","5c7c09b8":"# Final Calculation\nWe pick the Random Forest Optimized","d36865c8":"# Preprocessing Data","6b071546":"# Loading Data","ab1d19a4":"## Splitting for training and evaluation","49485032":"## Random Forest","61195e74":"## Naive Bayes","5c7f8cea":"# Encoding Data","24c33828":"# Finding the hyperparameters","786419a7":"## Decision Tree","038fc7d8":"## MLP Classifier","f8d4b132":"# Training Models"}}