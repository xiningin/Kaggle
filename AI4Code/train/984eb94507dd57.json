{"cell_type":{"1df35491":"code","b7a671c0":"code","8f88fc44":"code","6abaed6b":"code","fe5436d7":"code","331e9b47":"code","ca47e68f":"code","a0c5ce71":"code","e6b0f0c3":"code","ed3e4024":"code","a146da13":"code","3f1fbe53":"code","96c13369":"code","13939bf7":"code","ec083f2a":"code","2802e982":"code","d5514820":"code","2ab2788c":"markdown","be38cf8c":"markdown","1980545d":"markdown","cf08cca2":"markdown","050ae3ca":"markdown","cf3247e6":"markdown","fffb9f09":"markdown","5834d87b":"markdown","3d4e00b5":"markdown","05818760":"markdown","81f1dfe8":"markdown","9782fb75":"markdown","c89f59c4":"markdown","fb29b984":"markdown","950bb6b1":"markdown","44daadd8":"markdown","5fbf12db":"markdown","1ad06c1b":"markdown","843eee0f":"markdown","1914c9ae":"markdown","bace0e6a":"markdown","12d44d67":"markdown","efb1d30e":"markdown"},"source":{"1df35491":"import numpy as np\nimport pandas as pd \nfrom surprise import Reader, SVD, Dataset, accuracy\nfrom surprise.model_selection import GridSearchCV, train_test_split, cross_validate","b7a671c0":"movie = pd.read_csv('..\/input\/movielens-20m-dataset\/movie.csv')\nrating = pd.read_csv('..\/input\/movielens-20m-dataset\/rating.csv')\ndf = movie.merge(rating, how=\"left\", on=\"movieId\")\ndf.head()","8f88fc44":"movie_ids = [130219, 356, 4422, 541]\nmovies = [\"The Dark Knight (2011)\",\n          \"Cries and Whispers (Viskningar och rop) (1972)\",\n          \"Forrest Gump (1994)\",\n          \"Blade Runner (1982)\"]","6abaed6b":"sample_df = df[df.movieId.isin(movie_ids)]\nsample_df.shape","fe5436d7":"user_movie_df = sample_df.pivot_table(index=[\"userId\"], columns=[\"title\"], values=\"rating\")\nuser_movie_df.head()","331e9b47":"reader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(sample_df[['userId', 'movieId', 'rating']], reader)\ntype(data)","ca47e68f":"trainset, testset = train_test_split(data, test_size=.25)\n\nsvd_model = SVD()\nsvd_model.fit(trainset)","a0c5ce71":"predictions = svd_model.test(testset)\npredictions[0:5]","e6b0f0c3":"accuracy.rmse(predictions)","ed3e4024":"cross_validate(svd_model, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","a146da13":"svd_model.predict(uid=1.0, iid=541, verbose=True)","3f1fbe53":"\nsvd_model.predict(uid=1.0, iid=356, verbose=True)","96c13369":"param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005]}\n\ngs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3, n_jobs=-1, joblib_verbose=True)\n\ngs.fit(data)\n\ngs.best_score['rmse']","13939bf7":"svd_model = SVD(**gs.best_params['rmse'])\n\ndata = data.build_full_trainset()\nsvd_model.fit(data)","ec083f2a":"svd_model.predict(uid=1.0, iid=541, verbose=True)","2802e982":"svd_model.predict(uid=1.0, iid=356, verbose=True)","d5514820":"user_movie_df.head()","2ab2788c":"# Final Model and Prediction","be38cf8c":"[1] Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender systems. Computer, 42(8), 30-37.\n\n[2] https:\/\/medium.com\/sfu-cspmp\/recommendation-systems-collaborative-filtering-using-matrix-factorization-simplified-2118f4ef2cd3\n\n[3] Tak\u00e1cs, G., Pil\u00e1szy, I., N\u00e9meth, B., & Tikk, D. (2008, December). Investigation of various matrix factorization methods for large recommender systems. In 2008 IEEE International Conference on Data Mining Workshops (pp. 553-562). IEEE.","1980545d":"Let's convert the pandas dataframe structure to the format that the surprise library wanted.","cf08cca2":"# For userid 1 let's make a prediction of Blade-Runner(iid = 541).","050ae3ca":"# CONCLUSION","cf3247e6":"# Lets predict Whispers(iid = 356) for userid is 1.","fffb9f09":"# CROSS-VALIDATION","5834d87b":"To be specific, a matrix factorization is turning a matrix into a matrix multiplication. In the case of collaborative filtering, matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two low-dimensional rectangular matrices.","3d4e00b5":"Some of the most successful realizations of latent factor models are based on matrix factorization. In its basic form, matrix factorization characterizes both items and users by vectors of factors inferred from item rating patterns. High correspondence between item and user factors leads to a recommendation. These methods have become popular in recent years by combining good scalability with predictive accuracy. In addition, they offer much flexibility for model- ing various real-life situations.Recommender systems rely on different types of input data, which are often placed in a matrix with one dimension representing users and the other dimension representing items of interest. The most convenient data is high-quality explicit feedback, which includes explicit input by users regarding their interest in products. For example, Netflix collects star ratings for movies, and TiVo users indicate their preferences for TV shows by pressing thumbs-up and thumbs-down buttons. We refer to explicit user feedback as ratings. Usually, explicit feedback com- prises a sparse matrix, since any single user is likely to have rated only a small percentage of possible items. One strength of matrix factorization is that it allows incorporation of additional information. When explicit feedback is not available, recommender systems can infer user preferences using implicit feedback, which indirectly reflects opinion by observing user behavior including pur- chase history, browsing history, search patterns, or even mouse movements. Implicit feedback usually denotes the presence or absence of an event, so it is typically repre- sented by a densely filled matrix.[1]","05818760":"# Modeling","81f1dfe8":"In real-life scenarios, every user wouldn\u2019t have seen all possible movies. So the rating matrix will look more empty - Sparse Matrix. In this case, the model still learns to factorize the sparse matrix, but RMSE will be calculated only for the ratings that are actually present in the matrix. After obtaining the best factors to approximate the ratings we have, we then perform dot product of the factor matrices to fill in the missing entries in the rating matrix. These filled in ratings are then used to provide recommendations to the users.[2]","9782fb75":"The data set we will use in this study is very large and the operations we will do are very costly, so we will select certain movies and do the calculations on them.","c89f59c4":"# Model Tuning","fb29b984":"# MATRIX FACTORIZATION","950bb6b1":"\n![image.png](attachment:349aa35c-ebd5-4efa-b8b0-ebffb86bd092.png)\n![image.png](attachment:ac75c8ac-1bc7-4b7b-9174-8d6254638b64.png)","44daadd8":"Recommender Systems (RS) are just automated ways to recommend something to someone. Such systems are generally used by e-commerce companies, online content services and news websites. It helps reduce users wasting time trying to find something they like. It is also used to increase profitability.","5fbf12db":"SVD is a well-established technique for identifying latent semantic factors in information retrieval. Applying SVD in the collaborative filtering domain requires factoring the user-item rating matrix. This often raises difficulties due to the high portion of missing values caused by sparse- ness in the user-item ratings matrix. Conventional SVD is undefined when knowledge about the matrix is incom- plete. Moreover, carelessly addressing only the relatively few known entries is highly prone to overfitting. [1]","1ad06c1b":"   SVD is a somewhat complex mathematical technique that has many applications, including PCA and RS, as well as three new matrices that factor matrices.\nSimon Funk applied this method as a very clever strategy at the 2006 Netflix competition, showing a matrix as the product of two different matrices and using gradient descent to find the optimal values of the missing weights in the original matrix. Not SVD, but still used that term to describe his technique. **A more appropriate term for what Funk does is Matrix Factorization**.\nBecause of the good results and the fame that followed, people still call this technique SVD because that's what the author calls it.\n\nWith the growing significance of e-commerce, an increas- ing number of web-based merchant and rental services use recommender systems. Some of the major participants of e-commerce web, like Amazon.com and Netflix, successfully apply recommender systems to deliver automatically gener- ated personalized recommendation to their customers. The importance of a good recommender system was recognized by Netflix, which led to the announcement of the Netflix Prize (NP) competition to motivate researchers to improve the accuracy of their recommender system called Cinematch. This competition motivated our present work as well.\n\nThe approach which makes use of only user activities of the past1 (e.g. transaction history or user satisfaction ex- pressed in rating) is termed collaborative filtering (CF). The NP contest focuses on the case when users express their opin- ion of items by means of ratings. In this framework, the user first provides ratings of some items usually on a discrete nu- merical scale, and the system then recommends other items based on ratings similar users have already provided.\nMatrix factorization based techniques have proven to be efficient in recommender systems (see Section 1.1) when pre- dicting user preferences from known user-item ratings. [3]","843eee0f":"# Preparing the Data Set","1914c9ae":"![image.png](attachment:fc515ba2-072b-4402-8d63-42c3770c89cb.png)","bace0e6a":"We create the User vs Item matrix. However, in the case where users and the movies they watch are collected in a matrix, the overall matrix will contain the NaN value. Especially in larger datasets, this is called sparse)data and constitutes an important hurdle to overcome when developing a recommendation engine.\n\nIn the example below, we see a very sparsed matrix, 99% of which is created with missing values. Fortunately, we have alternative ways to circumvent the sparsity problem!","12d44d67":"To test the success of the model, we divide our data set into two as test and train. Then we need to create the SVD model and fit the it our trainset we created before.","efb1d30e":"# REFERENCES"}}