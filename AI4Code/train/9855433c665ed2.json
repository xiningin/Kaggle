{"cell_type":{"ce232867":"code","83cb7f2e":"code","c3958439":"code","547809b0":"code","08558009":"code","2e553b51":"code","5a0a8737":"code","bb878eee":"code","8e740b28":"code","91b7104e":"code","57dd273c":"code","101ce500":"code","eb775356":"code","38621fa0":"code","b0e3a60d":"code","8500f7d4":"code","6b205abf":"code","c257b081":"code","230508bf":"code","62e5e3e5":"code","5939f599":"code","de758cc5":"code","9ea7c92c":"code","a78f72fb":"code","95535881":"markdown","08171f58":"markdown","16fa1911":"markdown","eb43426e":"markdown","c0cd2edf":"markdown"},"source":{"ce232867":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","83cb7f2e":"pre_dataTest = pd.read_csv(\"..\/input\/test.csv\")\npre_dataTrain = pd.read_csv(\"..\/input\/train.csv\")\ndata_sonuc = pd.read_csv(\"..\/input\/gender_submission.csv\")","c3958439":"pre_dataTrain.head()","547809b0":"pre_dataTest.head()","08558009":"pre_dataTrain.info()\nprint(\"_\"*25)\npre_dataTest.info()","2e553b51":"pre_dataTrain.describe()","5a0a8737":"pre_dataTrain.corr()","bb878eee":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(pre_dataTrain.corr(), annot=True, linewidths=0.5, linecolor=\"red\", fmt='.1f', ax=ax)\nplt.show()\n","8e740b28":"g = sns.jointplot(pre_dataTrain.Survived, pre_dataTrain.Pclass, kind=\"kde\", size=7)\nplt.savefig('graph.png')\nplt.show()","91b7104e":"g = sns.jointplot(pre_dataTrain.Survived, pre_dataTrain.Fare, color=\"green\" , kind=\"kde\", size=7)\nplt.savefig('graph.png')\nplt.show()","57dd273c":"pre_dataTrain.dropna(inplace=True)\npre_dataTest.dropna(inplace=True)","101ce500":"# Data drop\ndataTrain = pre_dataTrain.drop([\"PassengerId\",\"Name\",\"Sex\", \"SibSp\", \"Parch\",\"Ticket\",\"Cabin\", \"Embarked\"], axis=1)\ndataTest = pre_dataTest.drop([\"Name\",\"Sex\",\"SibSp\", \"Parch\", \"Ticket\",\"Cabin\", \"Embarked\"], axis=1)","eb775356":"dataTest.info()","38621fa0":"dataTest.head()","b0e3a60d":"dataTrain.info()","8500f7d4":"pre_x_train = dataTrain.drop(\"Survived\", axis=1)\nx_train = (pre_x_train-np.min(pre_x_train))\/(np.max(pre_x_train)-np.min(pre_x_train)).values\ny_train = dataTrain[\"Survived\"]\npre_x_test = dataTest.drop(\"PassengerId\", axis=1)\nx_test = (pre_x_test-np.min(pre_x_test))\/(np.max(pre_x_test)-np.min(pre_x_test)).values\nx_train.shape, y_train.shape, x_test.shape","6b205abf":"#Logistic Regression Classisification\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\ny_head = lr.predict(x_test)\nresult_lr = round(lr.score(x_train, y_train)*100,2)\nresult_lr\nprint(\"Result Survived Predict to Logistic Regression Class: \", lr.score(x_train, y_train))","c257b081":"# KNN Classification\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_head = knn.predict(x_test)\nresult_knn = round(knn.score(x_train, y_train)*100,2)\nresult_knn\nprint(\"Result Survived Predict to KNN Class.: \", knn.score(x_train, y_train))\n","230508bf":"# Support Vector Machine Classification\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=3)\nsvm.fit(x_train, y_train)\ny_head = svm.predict(x_test)\nresult_svm = round(svm.score(x_train, y_train)*100,2)\nresult_svm\nprint(\"Result Survived Predict to SVM Class.: \", svm.score(x_train, y_train))","62e5e3e5":"#Naive Bayes Classification\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\ny_head = nb.predict(x_test)\nresult_nb = round(nb.score(x_train, y_train)*100,2)\nresult_nb\nprint(\"Result Survived Predict to Naive Bayes Class.: \", nb.score(x_train, y_train))","5939f599":"#Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=5)\ndt.fit(x_train, y_train)\ny_head = dt.predict(x_test)\nresult_dt = round(dt.score(x_train, y_train)*100,2)\nresult_dt\nprint(\"Result Survived Predict to Decision Tree Class.: \", dt.score(x_train, y_train))","de758cc5":"# Random Forest Classification and Evaluation Classification Models\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit (x_train, y_train)\ny_head = rf.predict(x_test)\nresult_rf = round(rf.score(x_train, y_train)*100,2)\nresult_rf\nprint(\"Result Survived Predict to Random Forest Class.: \", rf.score(x_train, y_train))","9ea7c92c":"# Evaluation Classification Models\nmodels = pd.DataFrame({\n    'Model' : ['Logistic Regression', 'KNN', 'SVM',\n               'Naive Bayes', 'Decison Tree', 'Random Forest'],\n    'Score' : [result_lr,result_knn,result_svm,\n              result_nb, result_dt, result_rf]\n})\n\nmodels.sort_values(by='Score', ascending=False)","a78f72fb":"submission = pd.DataFrame({\n        \"PassengerId\": dataTest[\"PassengerId\"],\n        \"Survived\": y_head\n    })\nsubmission.to_csv('submission.csv', index=False)","95535881":"**CONCLUSION**\n\nWe can choose rank our evaluation of all the models the best result. While both Random Forest Class and Decision Tree Class result the same, we choose Random Forest Class due to its strcuture.According to this Random Forest Class. model is the best result.","08171f58":"**In that case we can visualization of data.**","16fa1911":"**Classification Algorithms**\n\nI will explain at this kernel the below to classification algorithms with examples.\n\nWe are predict feature of \"survived\" at this kernel \n\n1. Logistic Regression Classification\n2. K-NN (K-Nearest Neighbour) Classification\n3. Support Vector Machine Classification\n4. Naive Bayes Classification\n5. Decision Tree Classification\n6. Random Forest Classification\n7. Evaluation Classification Models","eb43426e":"**Which we can use features?**\n* As we can see the higest correlation with \"survived\" is feature of \"Pclass\". And there is negative correlation between 2 feature.\n* There is a positive correlation between \"survived\" and \"Fare\" with a correlation of 0.25.\n* \"There isn't a significant correlation between other features.\" that we can interpret.","c0cd2edf":"**Model and predict**\n\nWe want classification whether passengers survived. Therefore we will use the following algorithms. \n\n* Logistic Regression Classification\n* KNN or K-Nearest Neighbors Classification\n* Support Vector Machines Classification\n* Naive Bayes Classification\n* Decision Tree Classification\n* Random Forest Classification\n* Evaluation Classification Models\n"}}