{"cell_type":{"cef28336":"code","6ca2d010":"code","95ca3ae4":"code","1fba7e74":"code","26475b58":"code","3173399f":"code","3e3147fb":"code","322ba0ba":"code","317aab7b":"code","5d3dbf3c":"code","5541f340":"code","ce924c8e":"code","e21049b5":"code","1353873d":"code","f3278649":"code","000b5189":"code","b5ed95c1":"code","8c00c34f":"code","65911dbc":"code","3dab688e":"code","e034a11c":"code","3e179d25":"code","d106130d":"code","c12f80ec":"code","d97bb47d":"code","0d682a1d":"code","b7c6712c":"code","887c727f":"code","9c1a8606":"code","8f6c8166":"markdown","49dbe8e7":"markdown","bc4a2ec4":"markdown","e5d2e31c":"markdown","558ef854":"markdown","833b12ea":"markdown","f938809d":"markdown","dc4c5519":"markdown","4d5e03b8":"markdown"},"source":{"cef28336":"import numpy as np\nimport pandas as pd\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n# Visual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing","6ca2d010":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","95ca3ae4":"train.head()","1fba7e74":"y = train.target\nX = train.text","26475b58":"test['text']","3173399f":"X = X.astype('str')\ntest['text'] = test['text'].astype('str')","3e3147fb":"g = sns.countplot(y)","322ba0ba":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","317aab7b":"X = X.apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))","5d3dbf3c":"X.head()","5541f340":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nX = X.apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\nX.head()","ce924c8e":"def remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\nX = X.apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\nX.head()","e21049b5":"# ps = nltk.stem.PorterStemmer()\n\n# def stemmer(text):\n#     words = [ps.stem(w) for w in text]\n#     return words","1353873d":"# X = X.apply(lambda x : stemmer(x))\n# test['text'] = test['text'].apply(lambda x : stemmer(x))\n# X.head()","f3278649":"wnl = nltk.stem.WordNetLemmatizer()\n\ndef lemmatizer(text):\n    words = [wnl.lemmatize(w) for w in text]\n    return words","000b5189":"X = X.apply(lambda x : lemmatizer(x))\ntest['text'] = test['text'].apply(lambda x : lemmatizer(x))\nX.head()","b5ed95c1":"MAX_NB_WORDS = 100000\nmax_seq_len = 30","8c00c34f":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence","65911dbc":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS+1, char_level=False)\ntokenizer.fit_on_texts(pd.concat([X, test['text']]))\nword_seq_train = tokenizer.texts_to_sequences(X)\nword_seq_test = tokenizer.texts_to_sequences(test['text'])\nword_index = tokenizer.word_index\nprint(\"dictionary size: \", len(tokenizer.word_index))\n\n#pad sequences\nword_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\nword_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)","3dab688e":"import codecs\nfrom tqdm import tqdm\n\nft = codecs.open('..\/input\/fasttext-wikinews\/wiki-news-300d-1M.vec', encoding='utf-8')","e034a11c":"embeddings_index = {}\n\nfor line in tqdm(ft):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n    \nft.close()\nprint('found %s word vectors' % len(embeddings_index))","3e179d25":"#training params\nbatch_size = 54 \nnum_epochs = 20\n\n#model parameters\nnum_filters = 40\nembed_dim = 300 \nweight_decay = 1e-4","d106130d":"words_not_found = []\nnb_words = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.zeros((nb_words+1, embed_dim))\nfor word, i in word_index.items():\n    if i >= nb_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","c12f80ec":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \nfrom keras import regularizers\nfrom keras import optimizers\nfrom keras.callbacks import EarlyStopping","d97bb47d":"model = Sequential()\nmodel.add(Embedding(nb_words+1, embed_dim,\n          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel.summary()","0d682a1d":"early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallbacks_list = [early_stopping]","b7c6712c":"#model training\nhist = model.fit(word_seq_train, y, batch_size=batch_size,\n                 epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)","887c727f":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ntarget = model.predict(word_seq_test)\nsubmission['target'] = target.round().astype(int)\nsubmission.to_csv('submission.csv',index=False)","9c1a8606":"submission.head()","8f6c8166":"# Load fasttext embedding","49dbe8e7":"Params","bc4a2ec4":"Submission","e5d2e31c":"# Bag of words","558ef854":"Lemmatization by WordNetLemmatizer","833b12ea":"This function helps us to clean data","f938809d":"Porter stemming","dc4c5519":"# Normalization:  Stemming, Lemmatization","4d5e03b8":"create embedding matrix"}}