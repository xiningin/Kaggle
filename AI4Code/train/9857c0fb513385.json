{"cell_type":{"37018759":"code","f7ff6799":"code","9b0aa70c":"code","5a72ce5d":"code","5acfd272":"code","2818bf0a":"code","f4c9d70c":"code","061fa7a9":"code","6259af5a":"code","1e57116c":"code","eed9e752":"code","2dde328d":"code","5609f4ee":"code","e9fdb744":"code","704e079f":"code","d1c4f5bb":"code","9ab94988":"code","369586a2":"code","4a9672aa":"code","51ac0ede":"code","66a80401":"code","6634ad15":"code","a50c19f4":"code","9f7f7d92":"code","eaeb10b9":"code","c4ab3925":"code","7944025e":"code","db8e0447":"code","b1f2ac14":"code","64e2f5cf":"code","dab19505":"code","36ffbb13":"code","d8821d06":"code","84143a4f":"code","21484168":"code","2ad61df7":"code","8d4228e4":"code","50e116ab":"code","208fda01":"code","7c050690":"code","878884fb":"code","fece3c0f":"code","611e4759":"code","dbe94900":"code","36d44312":"markdown","981c94aa":"markdown","b55dc1a3":"markdown","e04a7a49":"markdown","48fd8b19":"markdown","76702be6":"markdown","c4bd401f":"markdown","c6a9fcc1":"markdown","859228fc":"markdown","72454d51":"markdown","431c0542":"markdown","4548cc12":"markdown","6c1770b2":"markdown","8bde34bd":"markdown","0830ac68":"markdown","489eba85":"markdown","26be58b1":"markdown","25cb6633":"markdown","cafde169":"markdown","e7128196":"markdown","6e974e1d":"markdown","7bafc276":"markdown","e25e530f":"markdown","0cab840b":"markdown","d8d66dcb":"markdown","f70669cb":"markdown","44f55b7c":"markdown","d700c3c9":"markdown","6f0a841f":"markdown"},"source":{"37018759":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(color_codes=True)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb","f7ff6799":"import warnings\nwarnings.filterwarnings('ignore')","9b0aa70c":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_df.head(10)","5a72ce5d":"train_df.shape","5acfd272":"test_df.shape","2818bf0a":"train_df.dtypes","f4c9d70c":"train_df.isna().sum()","061fa7a9":"train_df['Cabin'].nunique()","6259af5a":"train_df['Embarked'].unique()","1e57116c":"train_df['Embarked'].nunique()","eed9e752":"sns.distplot(train_df['Age'].dropna())","2dde328d":"train_df.drop(['Cabin', 'Name', 'Ticket', 'PassengerId'], inplace=True, axis=1)\ntrain_df = train_df[train_df.Embarked.notna()]\ntrain_df.head()","5609f4ee":"passengerList = test_df['PassengerId']\ntest_df.drop(['Cabin', 'Name', 'Ticket', 'PassengerId'], inplace=True, axis=1)\ntest_df.head()","e9fdb744":"X = train_df.loc[:, train_df.columns != 'Survived']\ny = train_df.loc[:, 'Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","704e079f":"imputer = SimpleImputer(strategy='median')\nimputer.fit(X_train['Age'].to_numpy().reshape(-1, 1))\n\nX_train['Age'] = imputer.transform(X_train['Age'].to_numpy().reshape(-1, 1))\nX_test['Age'] = imputer.transform(X_test['Age'].to_numpy().reshape(-1, 1))\n\ntest_df['Age'] = imputer.transform(test_df['Age'].to_numpy().reshape(-1, 1))","d1c4f5bb":"X_train['Age'] = X_train['Age'].round()\nX_test['Age'] = X_test['Age'].round()\n\nX_train['Fare'] = X_train['Fare'].round()\nX_test['Fare'] = X_test['Fare'].round()\n\ntest_df['Age'] = test_df['Age'].round()\ntest_df['Fare'] = test_df['Age'].round()","9ab94988":"X_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\ntest_df = pd.get_dummies(test_df)","369586a2":"gridParameters = {'n_estimators': [1, 5, 10, 50, 100],\n                 'max_depth': [None, 1, 5, 10, 50, 100]}\n\nmodel = RandomForestClassifier()\nclf = GridSearchCV(model, gridParameters)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","4a9672aa":"accuracy_score(y_test, y_pred)","51ac0ede":"#Function to run different classifications algorithms. Returns the clf object of the classifier that gave highest accuracy\n\ndef getBestClassifier(X_train, y_train, X_test, y_test):    \n    classifierList = {\n        'SVM': SVC(),\n        'Neural Network': MLPClassifier(),\n        'Random Forest': RandomForestClassifier()\n    }\n\n    classifierParams = {\n        'SVM': {'C': [0.01, 0.1, 1, 10, 100],\n               'kernel': ['linear', 'rbf', 'sigmoid']},\n        'Neural Network': {'activation': ['identity', 'logistic', 'tanh', 'relu']},\n        'Random Forest': {'n_estimators': [1, 5, 10, 50, 100],\n                     'max_depth': [None, 1, 5, 10, 50, 100]}\n    }\n    \n    fittedClassifiersParam = {}\n    \n    for key, classifier in classifierList.items():\n        clf = GridSearchCV(classifier, classifierParams[key])\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        fittedClassifiersParam[key] = [accuracy_score(y_test, y_pred), clf.best_estimator_]\n        print('Accuracy of {0:20s}: {1}'.format(key, str(accuracy_score(y_test, y_pred))))\n    \n    return fittedClassifiersParam[sorted(fittedClassifiersParam, key = lambda k : fittedClassifiersParam[k][0], reverse=True)[0]]","66a80401":"#Used to plot a confusion matrix\n\ndef confusionMatrix(y_test, y_pred):\n    fig, ax = plt.subplots()\n    mat = confusion_matrix(y_test, y_pred)\n    sns.heatmap(mat, annot = True, fmt='d')\n\n    plt.xlabel(\"True Labels\")\n    plt.ylabel(\"Predicted Labels\")\n    ax.set_ylim([0, 2])","6634ad15":"bestClassifier = getBestClassifier(X_train, y_train, X_test, y_test)","a50c19f4":"bestClassifier","9f7f7d92":"confusionMatrix(y_test, bestClassifier[1].predict(X_test))","eaeb10b9":"if bestClassifier[1].__class__.__name__ == 'MLPClassifier':\n    print(\"Feature Importance not available for the model chosen: \" + str(bestClassifier[1].__class__.__name__) )\nelse:\n    plt.figure(figsize =(12, 6))\n    plt.title(bestClassifier[1].__class__.__name__)\n    sns.barplot(x=X_train.columns, y=bestClassifier[1].feature_importances_)\n    plt.xticks(rotation=45, horizontalalignment='right')","c4ab3925":"X_train['MemCount'] =  X_train['SibSp'] + X_train['Parch']\nX_test['MemCount'] =  X_test['SibSp'] + X_test['Parch']\ntest_df['MemCount'] =  test_df['SibSp'] + test_df['Parch']\n\nX_train.drop(['SibSp', 'Parch'], inplace=True, axis=1)\nX_test.drop(['SibSp', 'Parch'], inplace=True, axis=1)\ntest_df.drop(['SibSp', 'Parch'], inplace=True, axis=1)\n\nX_train['isAlone'] = X_train['MemCount'].apply(lambda x: 1 if x > 0 else 0)\nX_test['isAlone'] = X_test['MemCount'].apply(lambda x: 1 if x > 0 else 0)\ntest_df['isAlone'] = test_df['MemCount'].apply(lambda x: 1 if x > 0 else 0)\n\nX_train['Age'] = pd.cut(X_train['Age'], 4, labels=[1, 2, 3, 4])\nX_test['Age'] = pd.cut(X_test['Age'], 4, labels=[1, 2, 3, 4])\ntest_df['Age'] = pd.cut(test_df['Age'], 4, labels=[1, 2, 3, 4])\n\nX_train['Fare'] = pd.cut(X_train['Fare'], 4, labels=[1, 2, 3, 4])\nX_test['Fare'] = pd.cut(X_test['Fare'], 4, labels=[1, 2, 3, 4])\ntest_df['Fare'] = pd.cut(test_df['Fare'], 4, labels=[1, 2, 3, 4])","7944025e":"X_train['Age'] = X_train['Age'].astype('int')\nX_train['Fare'] = X_train['Fare'].astype('int')\n\nX_test['Age'] = X_test['Age'].astype('int')\nX_test['Fare'] = X_test['Fare'].astype('int')\n\ntest_df['Fare'] = test_df['Fare'].astype('int')\ntest_df['Age'] = test_df['Age'].astype('int')","db8e0447":"bestClassifier = getBestClassifier(X_train, y_train, X_test, y_test)","b1f2ac14":"confusionMatrix(y_test, bestClassifier[1].predict(X_test))","64e2f5cf":"if bestClassifier[1].__class__.__name__ in ['MLPClassifier', 'SVC']:\n    print(\"Feature Importance not available for the model chosen: \" + str(bestClassifier[1].__class__.__name__) )\nelse:\n    plt.figure(figsize =(12, 6))\n    sns.barplot(x=X_train.columns, y=bestClassifier[1].feature_importances_)\n    plt.xticks(rotation=45, horizontalalignment='right')","dab19505":"titleTrainDf = pd.read_csv('..\/input\/titanic\/train.csv')\ntitleTestDf = pd.read_csv('..\/input\/titanic\/test.csv')","36ffbb13":"titleTrainDf = titleTrainDf.filter(['Name'])\ntitleTestDf = titleTestDf.filter(['Name'])","d8821d06":"trainTitleSeries = titleTrainDf['Name'].str.split(\", \").apply(lambda x: x[1]).str.split(\".\").apply(lambda x : x[0])\ntrainTitleSeries = trainTitleSeries.rename('Title')\n\ntestTitleSeries = titleTestDf['Name'].str.split(\", \").apply(lambda x: x[1]).str.split(\".\").apply(lambda x : x[0])\ntestTitleSeries = testTitleSeries.rename('Title')","84143a4f":"trainTitleSeries.value_counts()","21484168":"# Function to map the names to appropriate titles\n\ndef classifyTitles(x):\n    return 'Rare' if x not in ['Mr', 'Miss', 'Mrs', 'Master'] else x","2ad61df7":"trainTitleSeries = trainTitleSeries.apply(classifyTitles)\ntestTitleSeries = testTitleSeries.apply(classifyTitles)","8d4228e4":"X_train = X_train.merge(trainTitleSeries, left_index=True, right_index=True)\nX_test = X_test.merge(trainTitleSeries, left_index=True, right_index=True)\n\ntest_df = test_df.merge(testTitleSeries, left_index=True, right_index=True)","50e116ab":"X_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\ntest_df = pd.get_dummies(test_df)","208fda01":"classifierList = {\n    'SVM': SVC(),\n    'Neural Network': MLPClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'XGBoost': xgb.XGBClassifier(silent=1, verbose_eval=False),\n    'CatBoost': CatBoostClassifier(logging_level='Silent'),\n    'LightGBM': lgb.LGBMClassifier()\n}\n\ngridEstimatorCount = [1, 5, 10, 50, 100]\ngridMaxDepth = [1, 2, 4, 5, 8, 10]\ngridLearningRate = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0]\n\nclassifierParams = {\n    'SVM': {'C': [0.01, 0.1, 1, 10, 100],\n           'kernel': ['linear', 'rbf', 'sigmoid']},\n    'Neural Network': {'activation': ['identity', 'logistic', 'tanh', 'relu']},\n    'Random Forest': {'n_estimators': gridEstimatorCount,\n                 'max_depth': gridMaxDepth},\n    'XGBoost': {'learning_rate': gridLearningRate, \n            'max_depth': gridMaxDepth, \n            'n_estimators': gridEstimatorCount},\n    'CatBoost': {'n_estimators': gridEstimatorCount,\n                'max_depth': gridMaxDepth},\n    'LightGBM': {'n_estimators': gridEstimatorCount,\n                'max_depth': gridMaxDepth}\n}","7c050690":"# Create an ensemble and return the ensemble object\n\ndef createEnsemble(X_train, y_train, X_test=[], y_test=[], classifierList={}, isFullDataset=False):        \n    fittedClassifiers = {}\n    \n    if not classifierList:\n        return\n    \n    for key, classifier in classifierList.items():\n        \n        print(\"Now training: \", key)\n        \n        clf = GridSearchCV(classifier, classifierParams[key], cv=5, n_jobs=-1, scoring='accuracy')\n        clf.fit(X_train, y_train)\n        fittedClassifiers.update({key: clf.best_estimator_})\n\n        if not isFullDataset:\n            y_pred = clf.predict(X_test)\n            print(key + ' has accuracy: ' + str(accuracy_score(y_test, y_pred)))\n    \n    ensemble = VotingClassifier(estimators=[(k, v) for k, v in fittedClassifiers.items()])\n    return ensemble","878884fb":"classifierList = {\n    'Neural Network': MLPClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'LightGBM': lgb.LGBMClassifier()\n}","fece3c0f":"bestClassifier = createEnsemble(X_train, y_train, X_test, y_test, classifierList=classifierList)","611e4759":"finalTrainingDataset = pd.concat([X_train, X_test])\nfinalTargetDataset = pd.concat([y_train, y_test])\n\nensemble = createEnsemble(finalTrainingDataset, finalTargetDataset, classifierList=classifierList, isFullDataset=True)\n\nresults = ensemble.fit(finalTrainingDataset, finalTargetDataset).predict(test_df)","dbe94900":"pd.concat([passengerList, pd.Series(results, name='Survived')], axis=1).to_csv('080620_final_ensemble.csv', index=False)","36d44312":"Compared to the fact that we have 890 rows of data, 150 unique values is very high and might not lead to much of an improvement in our modelling process, let's drop this later","981c94aa":"As we saw in the distribution plot above, *Age* is a skewed feature. To impute skewed features, we usually use the Median of that feature. If it was not skewed, then we would be using the Mean of the feature in most cases.","b55dc1a3":"**Running model on the entire dataset. This will be submitted for prediction**","e04a7a49":"**After performing a grid search, the best model that we have is a Neural Network which is predicting at an accuracy of 77.1%. Neural Network models do not have feature importances and you will have to use different techniques to understand feature importance. However, if you got another classifier with better parameters, you would be able to seee a graph below, showing the importance of all the features in the model**","48fd8b19":"**Creating CSV for submission:**","76702be6":"**Note:** When you run the below cell block, you will get a different accuracy and that's completely alright. Your dataset will be split in a different way and I am not using random state here to get unified results as I would like the reader to execute these and learn how these accuracies differ with the steps we perform","c4bd401f":"To bring back the title column, we have to read the information again and then extract the titles from the *Name* column","c6a9fcc1":"## Table of Contents\n\n1. [Import statements](#Import-statements)\n2. [Exploratory Data Analysis(EDA)](#Exploratory-Data-Analysis(EDA)\n3. [Imputing & Cleaning](#Imputing-&-Cleaning)\n4. [Modelling](#Modelling)\n5. [Concluding Notes](#Concluding-Notes)","859228fc":"### Imputing & Cleaning","72454d51":"Lets create an ensemble with Neural Network, LightGBM and Random Forest","431c0542":"I feel that our data is now in a good position for us to go ahead and create models out of. Lets start by using Random Forest.","4548cc12":"* **classiferList:** Contains list of names algorithms we are going to use and its corresponding objects\n* **classifierParams:** Contains list of names of algorithms we are going to use and its viable parameters. We are going to perform Grid Search on these parameters","6c1770b2":"In this section, we are going to clean the data by removing unnecessary features and imputing values for missing data","8bde34bd":"Lets start out by importing all required libraries","0830ac68":"We are now going to train multiple classification algorithms on the same dataset and then extract the classifier that provided the best accuracy metrics","489eba85":"To try to increase the accuracy further, we are going to bring about the following changes:\n\n* In the feature engineering side, let's bring back the name column and use only the title of the names. Using the name column will give us access to a plethora of new indirect information such as the status of the person\n* In the modelling side, we are going to bring use a few gradient boosted algorithms, namely, XGBoost, Catboost & LightGBM. We are then going to create an ensemble which will comprise of Neural Networks and Gradient Boosted trees","26be58b1":"### Import statements","25cb6633":"Lets disable warnings so that we can keep this notebook clean","cafde169":"**This solution has given me an accuracy of 78% on the validation set. Again, please note that when you run this block, your accuracy will be different due to the above mentioned reason**","e7128196":"The *pd.get_dummies* is used to split out the categorical variables and performs [One-Hot Encoding](https:\/\/hackernoon.com\/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) on the data. ","6e974e1d":"### Concluding Notes\n\n* A few things in the notebook has been left unexplained. The reason for this is that unless, you, as a reader, do not search for this information and read in detail about it, you are not going to have a proper understanding of the concept. I encourage you to google for any concept that you may be unfamiliar with.\n* If you have any suggestions on how this kernel can be improved or if you spot any mistakes, please feel free to point them out in the comments section\n* While you are executing the code blocks, you may sometimes notice that the Accuracy might have decreased when you performed some step. However, this is perfectly normal as there is no guarantee that what you are doing will help you get to the top of the leaderboard. You should strive to continue to try out new things and read more & more kernels to learn new approaches. ","7bafc276":"# Titanic Dataset Kernel","e25e530f":"### Why is this notebook worth your time?\n\nKaggle has a lot of kernels which show the completed product, but couldn't find one which takes you through the journey on how steps such as feature engineering and grid search will be able to help a data scientist along the way. In this notebook, I iterate over the problem and try to figure out solutions along the way, in an attempt to model how someone actually works on a dataset and improves it. The objective of this notebook is not to show you a completed notebook with complete explanations on every single topic and terminology you come across, but rather, to show you how an imperfect solution can slowly be iterated to something that is better than the previous solution and to make you curious about concepts that you perhaps might not have come across before","0cab840b":"The *Embarked* attribute only has 3 unique values, so let's keep this feature in our dataset","d8d66dcb":"### Exploratory Data Analysis(EDA)","f70669cb":"As we can see above, a lot of Titles occur only once in the dataset. Let us group them all into a title called *Rare*","44f55b7c":"This notebook is intended for anyone to get a walkthrough of how they can proceed with the Titanic dataset. I do not explain the logic behind basic Python\/SKlearn code in this notebook. However, the logic behind this is provided at the end of this notebook. You are encouraged to be curious throughout this journey. If you liked this kernel, please upvote it so that it serves as an inspiration for me :) ","d700c3c9":"After doing a bit of feature engineering, you can see that Random Forest is now performing better and giving better predictions.","6f0a841f":"### Modelling"}}