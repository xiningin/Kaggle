{"cell_type":{"80de9a4e":"code","e5a06eba":"code","00a6d9bb":"code","576bdf9f":"code","72155596":"code","4efa42ab":"code","6d0b5b4b":"code","e812662f":"code","d34790dd":"code","16c8c08f":"code","6b796cbf":"code","9321fa26":"code","ae7537ec":"code","e64f5d6e":"code","39e28e88":"code","02bbc3e2":"code","9382836e":"code","81954e53":"code","6ddbbf42":"code","98ebc6b9":"code","74a45e6a":"code","cd4027dc":"code","14330c5d":"code","91e3092d":"code","4d2c6587":"code","bebde142":"code","601b9a78":"code","1c2ccb4c":"code","5e8bd61d":"code","3f3d6754":"code","53707c52":"markdown","02f7116c":"markdown","401ba21f":"markdown","bc2928cb":"markdown","571d22f0":"markdown","ba907f39":"markdown"},"source":{"80de9a4e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e5a06eba":"# Import the dataset for review as a DataFrame\ndf = pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\")","00a6d9bb":"# Review the first 5 observations\ndf.head()","576bdf9f":"# Display information about the DataFrame\ndf.info(memory_usage=\"deep\")","72155596":"# Shape of the dataframe\nprint(df.shape)\n# Find the number of rows within a dataframe\nprint(len(df))\n# Extracting information from the shape tuple\nprint(f'Number of rows: {df.shape[0]} \\nNumber of columns: {df.shape[1]}')","4efa42ab":"# Review the high level summary details for each variable\ndf.describe()","6d0b5b4b":"# Check for the missing values by columns\ndf.isnull().sum()","e812662f":"# Proportion of missing values by column\ndef isnull_prop(df):\n    total_rows = df.shape[0]\n    missing_val_dict = {}\n    for col in df.columns:\n        missing_val_dict[col] = [df[col].isnull().sum(), (df[col].isnull().sum() \/ total_rows)]\n    return missing_val_dict\n\n# Apply the missing value method\nnull_dict = isnull_prop(df)\nprint(null_dict.items())","d34790dd":"# Create a dataframe of the missing value information\ndf_missing = pd.DataFrame.from_dict(null_dict, orient=\"index\", columns=['missing', 'miss_percent'])\ndf_missing","16c8c08f":"# Display missing values using a heatmap to understand if any patterns are present\nplt.figure(figsize=(15,8))\nsns.heatmap(df.isnull());","6b796cbf":"# set the histogram, mean and median\nsns.displot(df[\"ph\"], kde=False)\nplt.axvline(x=df.ph.mean(), linewidth=3, color='g', label=\"mean\", alpha=0.5)\nplt.axvline(x=df.ph.median(), linewidth=3, color='y', label=\"median\", alpha=0.5)\n\n# set title, legends and labels\nplt.xlabel(\"ph\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of ph\", size=14)\nplt.legend([\"mean\", \"median\"]);\n\nprint(f'Mean pH value {df.ph.mean()} \\n Median pH value {df.ph.median()} \\n Min pH value {df.ph.min()} \\n Max pH value {df.ph.max()}')","9321fa26":"# Preprocessing\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.feature_selection import RFE\n\n# Classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\n# Performance metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score","ae7537ec":"# Apply mean value to the missing values\ndf['ph'].fillna(df['ph'].mean(), inplace=True)\ndf['Sulfate'].fillna(df['Sulfate'].mean(), inplace=True)\ndf['Trihalomethanes'].fillna(df['Trihalomethanes'].mean(), inplace=True)\ndf.isnull().sum()","e64f5d6e":"# Separate into X and y variables\nX = df.drop(['Potability'], axis=1)\ny = df['Potability'].values","39e28e88":"# Display the features\nX.head()","02bbc3e2":"# Does scaling the features change the dynamics\nX_scaled = scale(X)\n\n# Print the mean and standard deviation of the unscaled features\nprint(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \nprint(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n\n# Print the mean and standard deviation of the scaled features\nprint(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \nprint(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))","9382836e":"# k-NN classifier\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=2, stratify=y)\n\n# Create a k-NN classifier with 7 neighbors\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))","81954e53":"# Lets understand the performance of the k-NN classifer across a range of clusters\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 12)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","6ddbbf42":"# Setup the pipeline steps\nsteps = [('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier())]\n        \n# Create the pipeline\npipeline = Pipeline(steps)\n\n# Fit the pipeline to the training set\nknn_scaled = pipeline.fit(X_train, y_train)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\n# Compute and print metrics\nprint('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))","98ebc6b9":"# Decision Tree classifier\n# Setup the parameters and distributions to sample\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","74a45e6a":"class ModelBuild():\n    # Constructor\n    def __init__(self, X, y, model=DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=8)):\n        self.X = X\n        self.y = y\n        self.model = model\n    \n    # Method to perform the train test split\n    def _train_test_split(self):\n        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.3, random_state=42)\n        return X_train, X_test, y_train, y_test\n    \n    # Method to set the pipeline\n    def _pipeline(self):\n        steps = [('scaler', StandardScaler()),\n                 ('model_name', self.model)]\n        return Pipeline(steps)\n    \n    # Method to run all steps\n    def model_build(self):\n        if __name__ == \"__main__\":\n            X_train, X_test, y_train, y_test = self._train_test_split()\n            pipeline = self._pipeline()\n            fit = pipeline.fit(X_train, y_train)\n            return print(\"Accuracy: {}\".format(pipeline.score(X_test, y_test)))","cd4027dc":"ModelBuild(X, y).model_build()","14330c5d":"class FeatureSelection(ModelBuild):\n    \n    # Inherit the ModelBuild features\n    def __init__(self, X, y, model=RandomForestClassifier()):\n        super().__init__(X, y, model=RandomForestClassifier())\n        self.X = X\n        self.y = y\n        self.model = model\n    \n    # Method to evaluate list of models\n    def rfe_model(self):\n        model_dict = dict()\n        for i in range(2, len(self.X.columns)):\n            rfe = RFE(estimator=self.model, n_features_to_select=i)\n            model = DecisionTreeClassifier()\n            model_dict[str(i)] = Pipeline(steps=[('rfe', rfe), ('mod', model)])\n        return model_dict\n    \n    # Method to evaluate the models\n    def eval_model(self, model):\n        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=6)\n        scores = cross_val_score(model, self.X, self.y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n        return scores\n    \n    # Lets understand the features being selected\n    def feature_select(self, n_feature):\n        rfe = RFE(estimator=self.model, n_features_to_select=n_feature)\n        rfe.fit(self.X, self.y)\n#         for i in range(X.shape[1]):\n        for i, col in enumerate(X.columns):\n            print('Column: %s, Selected %s, Rank: %.3f' % (col, rfe.support_[i], rfe.ranking_[i]))   \n    \n    # Method to run all steps\n    def feature_selection(self):\n        if __name__ == \"__main__\":\n            models = self.rfe_model()\n            results, names = list(), list()\n            for name, model in models.items():\n                scores = self.eval_model(model)\n                results.append(scores)\n                names.append(name)\n                print(f'{name}, mean_score: {np.mean(scores)}, std_score: {np.std(scores)}')\n                box_plt = plt.boxplot(results, labels=names, showmeans=True)\n            return box_plt","91e3092d":"box = FeatureSelection(X, y, model=DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=8)).feature_selection()\nplt.show()","4d2c6587":"features = FeatureSelection(X, y, model=DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=8)).feature_select(5)","bebde142":"# Lets try a Light GBM\nfrom lightgbm import LGBMClassifier","601b9a78":"# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=2, stratify=y)\n\n# Instantiate the LGBM\nlgbm = LGBMClassifier()\n\n# Fit the classifier to the training data\nlgbm.fit(X_train, y_train)\n\n# Perform prediction\ny_pred = lgbm.predict(X_test)\n\n# Print the accuracy\nprint(lgbm.score(X_test, y_test))\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","1c2ccb4c":"# Lets understand the baseline params\nlgbm.get_params()","5e8bd61d":"# Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('lgbm', LGBMClassifier())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {\n    'lgbm__learning_rate':[0.03, 0.05, 0.1],\n    'lgbm__objective':['binary'],\n    'lgbm__metric':['binary_logloss'],\n    'lgbm__max_depth':[10],\n    'lgbm__n_estimators':[100, 200, 300]\n}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Instantiate the GridSearchCV object\ncv = GridSearchCV(pipeline, parameters, cv=3)\n\n# Fit to the training set\ncv.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = cv.predict(X_test)","3f3d6754":"# Display best score and params\nprint(f'Best score : {cv.best_score_}')\nprint(f'Best params : {cv.best_params_}')\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))","53707c52":"### 1a. Summary statistics","02f7116c":"# What is Potabible water\n\nAt its most basic level, potabible water relates to the safety of water. \n\nMany questions begin to emerge.\n* Are we able to consume all fresh water types?\n* What percentage of the worlds fresh water can be accessed?\n* Has the water table increased as sea levels have rised?","401ba21f":"Do these values of pH relate to actual water or are there a wider range of sources being supplied?\n![pH scale](https:\/\/www.scienceabc.com\/wp-content\/uploads\/2019\/07\/A-pH-scale-on-white-background-illustration-VectorBlueRingMedias.jpg)","bc2928cb":"# Predict Potability","571d22f0":"### 1b. Missing values","ba907f39":"# EDA"}}