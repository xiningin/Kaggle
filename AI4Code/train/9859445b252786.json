{"cell_type":{"6b7935ff":"code","54191cda":"code","7f4402b7":"code","5c47514a":"code","c4688da9":"code","33814945":"code","03d2423b":"code","6386e950":"code","759bb68e":"markdown","15b6e9b5":"markdown","dafcb91b":"markdown","41b5da1b":"markdown","69461ed6":"markdown","ba89a295":"markdown","6c1d21e8":"markdown","1090fb1b":"markdown","6e0122db":"markdown","ff2e1752":"markdown","5f9f49e4":"markdown"},"source":{"6b7935ff":"import numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler","54191cda":"data = [[0, 100,0.01], [50, 1000,0.05], [100, 10000,0.001], [500, 100000,0.005],[1000,1000000,0.0001]]\ndf = pd.DataFrame(data=data,columns=[\"col1\",\"col2\",\"col3\"])\ndf.head()","7f4402b7":"scaler = StandardScaler()\nscaler.fit(data)","5c47514a":"scaler.mean_","c4688da9":"scaler.scale_","33814945":"scaled_data = scaler.transform(data)\nscaled_data","03d2423b":"predicted_scaled_data = scaler.transform([[365, 1234567,0.02]])\npredicted_scaled_data","6386e950":"scaled_df = pd.DataFrame(data=scaled_data,columns=df.columns)\nscaled_df.head()","759bb68e":"# StandardScaler","15b6e9b5":"# make data ","dafcb91b":"# build StandardScaler","41b5da1b":"## get mean data ","69461ed6":"# transform data using StandardScaler","ba89a295":"# import libraries","6c1d21e8":"# to DataFrame ","1090fb1b":"Standardize features by removing the mean and scaling to unit variance.\n\nThe standard score of a sample x is calculated as:\n\nz = (x - u) \/ s\n\nwhere u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.","6e0122db":"Centering and scaling happen independently on each feature by computing\nthe relevant statistics on the samples in the training set. Mean and\nstandard deviation are then stored to be used on later data using\n:meth:`transform`.\n\nStandardization of a dataset is a common requirement for many\nmachine learning estimators: they might behave badly if the\nindividual features do not more or less look like standard normally\ndistributed data (e.g. Gaussian with 0 mean and unit variance).\n\nFor instance many elements used in the objective function of\na learning algorithm (such as the RBF kernel of Support Vector\nMachines or the L1 and L2 regularizers of linear models) assume that\nall features are centered around 0 and have variance in the same\norder. If a feature has a variance that is orders of magnitude larger\nthat others, it might dominate the objective function and make the\nestimator unable to learn from other features correctly as expected.\n\nThis scaler can also be applied to sparse CSR or CSC matrices by passing\n`with_mean=False` to avoid breaking the sparsity structure of the data.","ff2e1752":"# transform unseen data using StandardScaler","5f9f49e4":"## get scale weight by column"}}