{"cell_type":{"5fda99e8":"code","0b54267d":"code","30403d4c":"code","dcc01274":"code","398d3f23":"code","b47e1cd0":"code","7ebd5753":"code","5b683d7d":"code","40c1e228":"code","87908ddc":"code","773783c6":"code","4226732e":"code","e9743776":"code","364a3d60":"code","01f2d5c1":"code","062acd36":"code","a1ebd2c2":"code","8a5434dd":"code","03949fcb":"code","6ad9ea3f":"code","5a122e5f":"code","eef7cdf3":"code","4cccbd2c":"code","afa712b1":"code","fc904e03":"code","0060eef6":"code","5862aa74":"markdown","c8fb4be0":"markdown","d70f73e4":"markdown","3dc627d7":"markdown","42b6cac8":"markdown","c644c745":"markdown","90423811":"markdown","eec67b3a":"markdown","cb52cb65":"markdown","26509ea0":"markdown","0afd39e9":"markdown","0a524677":"markdown","26bb5fdb":"markdown","d1113162":"markdown","846a5dd4":"markdown","23e7fc84":"markdown","84fed3b4":"markdown","7dceb9db":"markdown","9b7e1ec6":"markdown","58b68dcb":"markdown"},"source":{"5fda99e8":"from sklearn import preprocessing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import svm, tree\nimport xgboost\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport pickle\n%matplotlib inline\nimport datetime \nimport time \nimport tensorflow as tf","0b54267d":"# Load the data\ndata  = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-fraud-detection\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-fraud-detection\/test.csv')\nsubmission = test[['ID']]\n","30403d4c":"#display some information about data\ndata.info()\ntest.info()","dcc01274":"# data.isnull() # shows a df with the information whether a data point is null \n# Since True = the data point is missing, while False = the data point is not missing, we can sum them\n# This will give us the total number of missing values feature-wise\ndata.isnull().sum()","398d3f23":"# As we can see there are no missing data\n\n# so let take a look at the spread of each column\n# Visulazing the distibution of the data for every feature\ndata.hist(linewidth=1, histtype='stepfilled', facecolor='g', figsize=(20, 20));","b47e1cd0":"#lets look at the basic description of the data\ndata.describe()\n#test.describe()","7ebd5753":"# looking at the datset, we can infer that the datsset contains two days of traction or the record spans a  48hrs period and as \n# such, I have generated the hour of transaction by assuming that the first seconds of a day is 0 and the last one 86399, Hence:\n\ndata['HourBank'] = ((np.where(data['Time'] > 86399 , data['Time'] - 86399 , data['Time'])) % (24 *3600) \/\/ 3600).astype(int)\ntest['HourBank'] = ((np.where(test['Time'] > 86399 , test['Time'] - 86399 , test['Time'])) % (24 *3600) \/\/ 3600).astype(int)   \n#temptime = np.where(data['Time'] > 86399 , data['Time'] - 86399 , data['Time'])\n#data['HourBank'] = data['HourBank'].astype(int)\n#test['HourBank'] = test['HourBank'].astype(int)\n\n\n","5b683d7d":"data['Class'].groupby(data['Class']).count()","40c1e228":"# First we limit the data frame to where a fraudulent activity was identify\nd = data[data['Class'] == 1]\n\n# group by HourBank, then count of fradulent transaction\nd1 = d[['Class','HourBank']]\n\nd1 = d1.groupby(['HourBank']).count()\nd1.reset_index(level=0, inplace=True)\n\n# group by HourBank, then average Amount of fradulent transaction\nd2 = d[['Amount','HourBank']]\n# group store and dept by average weekly sales\nd2 = d2.groupby(['HourBank']).mean()\nd2.reset_index(level=0, inplace=True)\n\n\n#Lets do a quick plot to visualise the data\nfig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(10,7))\n\nax1.bar(d1.HourBank, d1.Class)\nax1.set_title('Count of Fraudulent transactions by Hour')\nax2.bar(d2.HourBank, d2.Amount)\nax2.set_title('Average Amount Classed as Fraudulent by Hour')\n\nfig.tight_layout()\nplt.show()","87908ddc":"#lets get dummies\nT_dummies = pd.get_dummies(data['HourBank'])\n\n#lets merge it\ndata = pd.concat([data, T_dummies], axis = 1)\n\n# then drop the redundant column\ndata = data.drop(['HourBank','ID','Time'], axis = 1)\n\n\n##### Repeat the same for test data\n\nTt_dummies = pd.get_dummies(test['HourBank'])\na = list(Tt_dummies.columns.values)\nb = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22, 23]\nc = [x for x in b if x not in a]\nTt_dummies = pd.concat([Tt_dummies, pd.DataFrame(columns = c)]).fillna(0)  \n#lets merge it\ntest = pd.concat([test, Tt_dummies], axis = 1)\n# then drop the redundant column\ntest = test.drop(['HourBank','ID','Time'], axis = 1)\n\n\n","773783c6":"# Balance the data based on column class\ng = data.groupby('Class')\ng = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True)))\ng = g.reset_index(drop=True)\n\n###shuffle dataset\ng = g.sample(frac=1).reset_index(drop=True)\n\n","4226732e":"# The inputs are all columns in the csv, except for the first one [:,0]\n# (which is just the arbitrary customer IDs that bear no useful information),\n# and the last one [:,-1] (which is our targets)\nbalanced_inputs = g.drop(['Class'],axis=1)\n# The targets are in the last column. That's how datasets are conventionally organized.\nbalanced_targets = g['Class'].astype(np.int)\nunbalanced_inputs = data.drop(['Class'],axis=1)\n# The targets are in the last column. That's how datasets are conventionally organized.\nunbalanced_targets = data['Class'].astype(np.int)\n\nbalanced_inputs = preprocessing.scale(balanced_inputs)\nscaled_unbalanced_inputs = preprocessing.scale(unbalanced_inputs)\n\ntest_inputsx = preprocessing.scale(test)\n\nshuffled_inputs = balanced_inputs\nshuffled_targets = balanced_targets","e9743776":"# Count the total number of samples\nsamples_count = shuffled_inputs.shape[0]\n\n# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n# Naturally, the numbers are integers.\ntrain_samples_count = int(0.8 * samples_count)\nvalidation_samples_count = int(0.1 * samples_count)\n\n# The 'test' dataset contains all remaining data.\ntest_samples_count = samples_count - train_samples_count - validation_samples_count\n\n# Create variables that record the inputs and targets for training\n# In our shuffled dataset, they are the first \"train_samples_count\" observations\ntrain_inputs = shuffled_inputs[:train_samples_count]\ntrain_targets = shuffled_targets[:train_samples_count]\n\n# Create variables that record the inputs and targets for validation.\n# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\nvalidation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\nvalidation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n\n# Create variables that record the inputs and targets for test.\n# They are everything that is remaining.\ntest_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\ntest_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n\n# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were \n# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, \n# you will get different values, as each time they are shuffled randomly.\n# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n\n# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\nprint(np.sum(train_targets), train_samples_count, np.sum(train_targets) \/ train_samples_count)\nprint(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) \/ validation_samples_count)\nprint(np.sum(test_targets), test_samples_count, np.sum(test_targets) \/ test_samples_count)","364a3d60":"#Unbalanced datset\ntrain_test_split(scaled_unbalanced_inputs, unbalanced_targets)\n# declare 4 variables for the split\nx_train1, x_test1, y_train1, y_test1 = train_test_split(scaled_unbalanced_inputs, unbalanced_targets, #train_size = 0.75, \n                                                                            test_size = 0.25, random_state = 20)\n","01f2d5c1":"#Now, we will create an array of Classifiers and append different classification models to our array\nclassifiers = [] \n\nmod1 = xgboost.XGBClassifier()\nclassifiers.append(mod1)\nmod2 = svm.SVC()\nclassifiers.append(mod2)\nmod3 = RandomForestClassifier()\nclassifiers.append(mod3)\nmod4 = LogisticRegression()\nclassifiers.append(mod4)\nmod5 = KNeighborsClassifier(3)\nclassifiers.append(mod5)\nmod6 = AdaBoostClassifier()\nclassifiers.append(mod6)\nmod7= GaussianNB()\nclassifiers.append(mod7)","062acd36":"#Lets fit the models into anarray\n\nfor clf in classifiers:\n    clf.fit(train_inputs,train_targets)\n    y_pred= clf.predict(test_inputs)\n    y_tr = clf.predict(train_inputs)\n    acc_tr = accuracy_score(train_targets, y_tr)\n    acc = accuracy_score(test_targets, y_pred)\n    mn = type(clf).__name__\n    \n    print(clf)\n    print(\"Accuracy of trainset %s is %s\"%(mn, acc_tr))\n    print(\"Accuracy of testset %s is %s\"%(mn, acc))\n    cm = confusion_matrix(test_targets, y_pred)\n    print(\"Confusion Matrix of testset %s is %s\"%(mn, cm))","a1ebd2c2":"# convert all value into array\nvalidation_inputs = np.array(validation_inputs)\nvalidation_targets = np.array(validation_targets)\ntrain_targets = np.array(train_targets)\ntrain_inputs = np.array(train_inputs)","8a5434dd":"# Set the input and output sizes\ninput_size = 53\noutput_size = 2\n# Use same hidden layer size for both hidden layers. Not a necessity.\nhidden_layer_size = 6\n    \n# define how the model will look like\nmodel = tf.keras.Sequential([\n    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    # the final layer is no different, we just make sure to activate it with softmax\n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])\n\n\n### Choose the optimizer and the loss function\n\n# we define the optimizer we'd like to use, \n# the loss function, \n# and the metrics we are interested in obtaining at each iteration\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n### Training\n# That's where we train the model we have built.\n\n# set the batch size\nbatch_size = 100\n\n# set a maximum number of training epochs\nmax_epochs = 100\n\n# set an early stopping mechanism\n# let's set patience=2, to be a bit tolerant against random validation loss increases\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n\n# fit the model\n# note that this time the train, validation and test data are not iterable\nhistory = model.fit(train_inputs, # train inputs\n          train_targets, # train targets\n          batch_size=batch_size, # batch size\n          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n          # callbacks are functions called by a task when a task is completed\n          # task here is to check if val_loss is increasing\n          callbacks=[early_stopping], # early stopping\n          validation_data=(validation_inputs, validation_targets), # validation data\n          verbose = 2 # making sure we get enough information about the training process\n          )  ","03949fcb":"\n# Plot the train\/validation loss values\nplt.figure(figsize=(15,10))\n_loss = history.history['loss'][1:]\n_val_loss = history.history['val_loss'][1:]\n\ntrain_loss_plot, = plt.plot(range(1, len(_loss)+1), _loss, label='Train Loss')\nval_loss_plot, = plt.plot(range(1, len(_val_loss)+1), _val_loss, label='Validation Loss')\n\n_ = plt.legend(handles=[train_loss_plot, val_loss_plot])","6ad9ea3f":"test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)","5a122e5f":"#first lets convert the inputs to array\ny_train1 = np.array(y_train1)\ny_test1 = np.array(y_test1)","eef7cdf3":"# Set the input and output sizes\ninput_size = 53\noutput_size = 2\n# Use same hidden layer size for both hidden layers. Not a necessity.\nhidden_layer_size = 2\n    \n# define how the model will look like\ntfk = tf.keras.Sequential([\n    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    # the final layer is no different, we just make sure to activate it with softmax\n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])\n\n\n### Choose the optimizer and the loss function\n\n# we define the optimizer we'd like to use, \n# the loss function, \n# and the metrics we are interested in obtaining at each iteration\ntfk.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n### Training\n# That's where we train the model we have built.\n\n# set the batch size\nbatch_size = 100\n\n# set a maximum number of training epochs\nmax_epochs = 100\n\n# set an early stopping mechanism\n# let's set patience=2, to be a bit tolerant against random validation loss increases\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n\n# fit the model\n# note that this time the train, validation and test data are not iterable\nhistory = tfk.fit(x_train1, # train inputs\n          y_train1, # train targets\n          batch_size=batch_size, # batch size\n          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n          # callbacks are functions called by a task when a task is completed\n          # task here is to check if val_loss is increasing\n          callbacks=[early_stopping], # early stopping\n          validation_data=(x_test1, y_test1), # validation data\n          verbose = 2 # making sure we get enough information about the training process\n          )  ","4cccbd2c":"\n# Plot the train\/validation loss values\nplt.figure(figsize=(20,10))\n_loss = history.history['loss'][1:]\n_val_loss = history.history['val_loss'][1:]\n\ntrain_loss_plot, = plt.plot(range(1, len(_loss)+1), _loss, label='Train Loss')\nval_loss_plot, = plt.plot(range(1, len(_val_loss)+1), _val_loss, label='Validation Loss')\n\n_ = plt.legend(handles=[train_loss_plot, val_loss_plot])","afa712b1":"tfbalanced = model.predict_classes(test_inputsx) # Deep learning with balanced data\ntfunbalanced = tfk.predict_classes(test_inputsx)  # Deep Learning with unbalanced data\nlogreg = mod4.predict(test_inputsx)              # Logistic regression with balanced data\nada = mod6.predict(test_inputsx)            # Adaboost with balanced data","fc904e03":"#df = pd.DataFrame(pred, columns=['Prediction'])\nsubmission['tfb'] = tfbalanced          # the AUC score = 54\nsubmission['tfu'] = tfunbalanced        # the AUC score = 0.89\nsubmission['logr'] = logreg             # the AUC score = 57\nsubmission['ada'] = ada                 # the AUC score = 63","0060eef6":"submission.to_csv('submission.csv', index=False)","5862aa74":"### Shuffle Dataset ","c8fb4be0":"## Import the relevant libraries","d70f73e4":"### Context\nAs presented by the 5th 1056Lab Data Analytics Competition\nFraud detection in financial transactions is one of the most important problems in financial companies.\n\nThe original dataset is in Kaggle Datasets.\nThis data is about fraud detection in credit card transactions. The data was made by credit cards in September 2013 by European cardholders. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\nIt contains only numerical input variables which are the result of a PCA transformation. The original features and more background information about the data is not provided.\nThe dataset contains 284,807 instances, 492 instances are fraudulent, the remaining 284,315 instances are genuine.\nTraining Dat and Test Data\nThe training data set is the former 198,365 (70%) instances, 383 represents fraud transactions, 197,982 transactions are genuine.\nThe test data is the later 86,442 (30%) transactions.","3dc627d7":"### Checking the distribution of the targets","42b6cac8":"# Credit Card Fraud Detection","c644c745":"## Make Prediction on Test data\nApply afew of the models to the test data to make preictions.","90423811":"The plots above shows the distribution of fraudulent transaction over the time(hr) of the day","eec67b3a":"### Exploring the distribution of the Class and Amount","cb52cb65":"### Quick modelling using Default parameters","26509ea0":"## Test the model\n\nAfter training on the train data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before.\n","0afd39e9":"## Apply Deep Learning Model(KNN) to hthe unbalanced dataset","0a524677":"The best model seem to be the tfk model,\nwhich is the model with the unbalanced dataset\nThe model generated an AUC score of 0.89277.\nSo this project gives different ways of adressing the task, with balanced and unbalanced data.\nUsing deep learning, KNN, SVC, Adaboost, SGboostClassifier, Logistic Regression, GaussianNB and Random Foret","26bb5fdb":"## My Approach is quite straitforward\n\nThe main issue with this task is the unbalanced data, so i will present two approaches, one with a balanced dataset and the other with the original (unbalanced) dataset.\n1. I will do a bit of exploratory data analysis\n2. Then do some feature engineering\n3. Then Balance the dataset\n3. Apply some ML models with default parameters; models like logistic regression, XGboost, SVC, KNN etc.\n4. I will examine the accuracy\n5. Apply deep learning model(KNN), validate then compare the model results\n6. Then Make some prediction on the test\n","d1113162":"## Feature Engineering","846a5dd4":"### Deep Learning Modelling(KNN)\nOutline, optimizers, loss, early stopping and training","23e7fc84":"### Balance and Standardize the inputs","84fed3b4":"One Hot Encoding of the Hour Banks","7dceb9db":"### Split the unbalanced dataset into train,  and test","9b7e1ec6":"### Load the dataset","58b68dcb":"Convert Time column into bank of hours in a day"}}