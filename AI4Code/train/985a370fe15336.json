{"cell_type":{"f3e8ab6d":"code","63755313":"code","3b4e7514":"code","649711a6":"code","4c82e5fc":"code","7336cbdf":"code","65dff0d7":"code","8ac23481":"code","15158b43":"code","afbe87ea":"code","c5f3ab44":"code","0b2fad86":"code","070dbc84":"code","b2e6d029":"code","4c5f8a4b":"code","34dc109f":"code","d9ebbbf0":"code","a852295a":"code","f04bbe92":"code","a4c490da":"code","4ee77631":"code","ca5e5bdf":"code","208185df":"code","52f926f4":"code","a3d73792":"code","698a83b2":"markdown","73be1d68":"markdown","1927f9c4":"markdown","45a9d4ad":"markdown","e7bdade9":"markdown","3f75ecc8":"markdown","de6db552":"markdown","c42e7576":"markdown","80c5e2a3":"markdown","9c9a3f9f":"markdown","81c8f39a":"markdown","398590d4":"markdown","4925f0ce":"markdown","7d9e4949":"markdown","65709387":"markdown","04a25462":"markdown"},"source":{"f3e8ab6d":"# --- CSS STYLE ---\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"..\/input\/2020-cost-of-living\/alerts.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"<\/style>\")\ncss_styling()","63755313":"# Common Libraries\nimport wandb\nimport os\nimport random\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport string\nimport gc\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\n# Text Manipulation\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\n\n# Transformers\nfrom transformers import BertTokenizer, RobertaTokenizer, AutoTokenizer\nfrom transformers import BertModel, RobertaModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\n\n# Custom colors\nmy_colors = [\"#E4916C\", \"#E36A67\", \"#FFB2C8\", \"#BCE6EF\", \"#1E5656\"]\nsns.palplot(sns.color_palette(my_colors))\n\nclass color:\n    '''S from Start & E from End.'''\n    S = '\\033[1m' + '\\033[93m'\n    E = '\\033[0m'\n    \n# Environment check\nos.environ[\"WANDB_SILENT\"] = \"true\"\nCONFIG = {'competition': 'common-lit', '_wandb_kernel': 'aot'}\npd.set_option('mode.chained_assignment', None)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Secrets \ud83e\udd2b\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\n\n# Set seed\ndef set_seed(n=10):\n    torch.manual_seed(n)\n    random.seed(n)\n    np.random.seed(0)\n    \nset_seed(n=10)","3b4e7514":"! wandb login $secret_value_0","649711a6":"def train_to_device(ids, mask, metadata, target, device):\n    '''Sends dataloader output to device.'''\n    ids = ids.to(device, dtype=torch.long)\n    mask = mask.to(device, dtype=torch.long)\n    metadata = metadata.to(device, dtype=torch.float)\n    target = target.to(device, dtype=torch.float)\n    \n    return ids, mask, metadata, target\n\n\ndef test_to_device(ids, mask, metadata, device):\n    '''Sends dataloader output to device.'''\n    ids = ids.to(device, dtype=torch.long)\n    metadata = metadata.to(device, dtype=torch.float)\n    mask = mask.to(device, dtype=torch.long)\n    \n    return ids, mask, metadata","4c82e5fc":"def clean_paragraph(paragraph, verbose=False):\n    '''Cleans paragraph before tokenization.\n    This step might help or NOT - as the pretrained models we will use are accustomed\n    with a certain type of input.'''\n    \n    # Tokenize & convert to lower case\n    tokens = word_tokenize(paragraph)\n    tokens = [t.lower() for t in tokens]\n\n    # Remove punctuation & non alphabetic characters from each word\n    table = str.maketrans('', '', string.punctuation)\n    tokens = [t.translate(table) for t in tokens]\n    tokens = [t for t in tokens if t.isalpha()]\n\n    # Filter out stopwords\n    stop_words = stopwords.words('english')\n    tokens = [t for t in tokens if not t in stop_words]\n\n    # Lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    tokens_lemm = [lemmatizer.lemmatize(t) for t in tokens]\n\n    if verbose:\n        print(color.BOLD + \"Show difference between original and lemmatized token:\" + color.END)\n        for a, b, in zip(tokens, tokens_lemm):\n            if a != b: print(a, \" | \", b)\n                \n    return \" \".join(tokens_lemm)","7336cbdf":"def create_features(df):\n    '''Creates features based on preprocessed text column.\n    df: the training or testing dataframe.'''\n    \n    # --- Add new features from text ---\n    word_frequencies = pd.read_csv(\"..\/input\/english-word-frequency\/unigram_freq.csv\")\n    # Convert it into a dict (i.e. hashmap)\n    word_frequencies = dict(zip(word_frequencies[\"word\"], word_frequencies[\"count\"]))\n    available_words = set(word_frequencies.keys())\n    # Tokenize full text\n    df[\"split_text\"] = df[\"text\"].apply(lambda x: [word for word in x.split(\" \")])\n    # Get word count for each word\n    df[\"freq_text\"] = df[\"split_text\"].apply(lambda x: [word_frequencies.get(word, 0) for word in x \n                                                        if word in available_words])\n\n\n    # --- Create more features ---\n    # Get sum, mean, std etc. from the text frequencies\n    df[\"freq_sum\"] = df[\"freq_text\"].apply(lambda x: np.sum(x))\n    df[\"freq_mean\"] = df[\"freq_text\"].apply(lambda x: np.mean(x))\n    df[\"freq_std\"] = df[\"freq_text\"].apply(lambda x: np.std(x))\n    df[\"freq_min\"] = df[\"freq_text\"].apply(lambda x: np.min(x))\n    df[\"freq_max\"] = df[\"freq_text\"].apply(lambda x: np.max(x))\n\n    # Get more info from text itself\n    df[\"no_words\"] = df[\"text\"].apply(lambda x: len(x.split(\" \")))\n    df[\"no_words_paragraph\"] = df[\"excerpt\"].apply(lambda x: len(x.split(\" \")))\n\n\n    # --- Scale the Features ---\n    cols = ['freq_sum', 'freq_mean', 'freq_std', 'freq_min', \n            'freq_max', 'no_words', 'no_words_paragraph']\n    X = df[cols]\n    X_scaled = pd.DataFrame(StandardScaler().fit_transform(X))\n    df[cols] = X_scaled\n    \n    return df","65dff0d7":"# Read in training data and preprocess 'excerpt' feature\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ndf[\"text\"] = df[\"excerpt\"].apply(lambda x: clean_paragraph(x))\n\n# Create features\ndf = create_features(df)\n\ndf.head(3)","8ac23481":"# ===== MODEL PARAMETERS =====\n# OR: \"..\/input\/huggingface-roberta\/roberta-large\"\nMODEL_PATH = \"..\/input\/d\/xhlulu\/huggingface-bert\/bert-large-uncased\"\n# OR: \"roberta-large\"\nMODEL_NAME = \"bert-large-uncased\"\n# OR: RobertaTokenizer\nTOKENIZER = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True)\nscaler = GradScaler()    ### to prevent underflow\n\n# Check if we are using either train or test data\nIS_TEST = False\nif len(df) == 7: IS_TEST == True","15158b43":"class LiitDataset(Dataset):\n    \n    def __init__(self, texts, tokenizer, max_len, is_test, targets=None, metadata=None):\n        '''Initiate the arguments of the object.\n        texts: the raw excerpt or preprocessed text from df\n        targets: corresponding targets\n        tokenizer: the tokenizer from transformers library\n        max_len: the size of the tokenizer\n        is_test: wether the data is a test\/validation or train\n        metadata: additional dataframe with features to use for the model'''\n        \n        self.text = texts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_test = is_test\n        self.metadata = metadata\n        \n    def __len__(self):\n        '''Returns the size of the dataset.'''\n        return len(self.text)\n    \n    def __getitem__(self, i):\n        '''This will help during the data_loader operation.\n        i: index(es) to select the data'''\n        \n        # Select the text & tokenize\n        batch_text = str(self.text[i])\n        inputs = self.tokenizer(batch_text,\n                                max_length = self.max_len,\n                                padding = \"max_length\",\n                                truncation = True)\n        \n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        # Select the metadata\n        meta = np.array(self.metadata.iloc[i].values, dtype=np.float32)\n            \n        if self.is_test == True:\n            return {\"input_ids\" : torch.tensor(ids, dtype=torch.long),\n                    \"attention_mask\" : torch.tensor(mask, dtype=torch.long),\n                    \"metadata\" : torch.tensor(meta, dtype=torch.float)}\n        else:\n            target = self.targets[i]\n            return {\"input_ids\" : torch.tensor(ids, dtype=torch.long),\n                    \"attention_mask\" : torch.tensor(mask, dtype=torch.long),\n                    \"metadata\" : torch.tensor(meta, dtype=torch.float),\n                    \"target\" : torch.tensor(target, dtype=torch.float)}","afbe87ea":"# Array of texts & features & targets\nsample_text = df[\"excerpt\"][:6].values\nsample_meta = df.iloc[:6, 9:]\nsample_target = df[\"target\"][:6].values\n\n\n# Instantiate Dataset object\ndataset = LiitDataset(texts=sample_text, targets=sample_target,\n                      tokenizer=TOKENIZER, max_len=5, is_test=IS_TEST, \n                      metadata = sample_meta)\n# The Dataloader\ndataloader = DataLoader(dataset, batch_size=3, shuffle=False)\n\n# Output of the Dataloader\nfor k, data in enumerate(dataloader):\n    ids, mask, meta, target = data.values()\n    print(color.S + f\"Batch: {k}\" + color.E, \"\\n\" +\n          color.S + \"Ids:\" + color.E, ids, \"\\n\" +\n          color.S + \"Mask:\" + color.E, mask, \"\\n\" +\n          color.S + \"Metadata:\" + color.E, meta, \"\\n\" +\n          color.S + \"Target:\" + color.E, target, \"\\n\" +\n          \"=\"*50)","c5f3ab44":"class TRANSFORMERS_MODEL(nn.Module):\n    # TODO: Implement for RoBERTa as well\n    \n    def __init__(self, MODEL_PATH, drop=0.3, roberta=False, no_columns=7, meta_size=500):\n        # If super is not called, an AttributeError will appear\n        super(TRANSFORMERS_MODEL, self).__init__()\n        \n        # Text Model Layer\n        if roberta == True:\n            self.Model = RobertaModel.from_pretrained(MODEL_PATH)\n        else:\n            self.Model = BertModel.from_pretrained(MODEL_PATH)\n        # Metadata Layer\n        self.Metadata = nn.Sequential(nn.Linear(no_columns, meta_size),\n                                      nn.BatchNorm1d(meta_size),\n                                      nn.ReLU(),\n                                      nn.Dropout(p=drop))\n        # Aggregation Layer\n        self.Linear = nn.Linear(1024 + meta_size, 1)\n        \n    def forward(self, input_ids, attention_mask, metadata, prints=False):\n        '''A forward pass of this network.\n        Use `prints=True` if you want to see output shape at each pass.'''\n        \n        if prints: print(\"===============\")\n        _, text = self.Model(input_ids, attention_mask, return_dict=False)\n        if prints:\n            print(color.S+\"Text Out Shape:\"+color.E, text.shape)\n            \n        meta = self.Metadata(metadata)\n        if prints:\n            print(color.S+\"Metadata Out Shape:\"+color.E, meta.shape)\n            \n        text_meta = torch.cat((text, meta), dim=1)\n        out = self.Linear(text_meta)\n        if prints:\n            print(color.S+\"After FNN Shape:\"+color.E, out.shape)\n        \n        return out.view(-1)","0b2fad86":"# Initiate the model\nmodel_example = TRANSFORMERS_MODEL(MODEL_PATH, drop=0.3, roberta=False, no_columns=7, meta_size=500)\nmodel_example.train()  ### training mode: ON\n\n# We'll use the dataset & dataloader from previous example\nfor k, data in enumerate(dataloader):\n    ids, mask, meta, target = data.values()\n    break\n    \nprint(color.S+\"Input data shape:\"+color.E, len(ids), \"paragraphs.\", \"\\n\")\n\nout = model_example(ids, mask, meta, prints=True)","070dbc84":"def custom_optimizer(model, LR, prints=False):\n    '''A custom optimizer for the model parameters.\n    model: the initialized model class\n    lr: learning rate'''\n    \n    # Get model parameters\n    parameters = list(model.named_parameters())\n    if prints:\n        print(color.S+\"Number of Parameters to Optimize:\"+color.E, len(parameters))\n        \n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    \n    # Set weight_decay to start at 0.0 for the no_decay parameters\n    ### and the rest to start from 0.003\n    optimizer_parameters = [\n            {\"params\" : [p for n, p in parameters if not any(nd in n for nd in no_decay)],\n             \"weight_decay\" : 0.003}, \n            {\"params\" : [p for n, p in parameters if any(nd in n for nd in no_decay)],\n             \"weight_decay\" : 0.0}]\n    \n    optimizer = AdamW(optimizer_parameters, lr=LR)\n    if prints: print(color.S+\"Optimizer:\"+color.E, optimizer)\n    \n    return optimizer","b2e6d029":"# Optimizer Example\noptimizer_example = custom_optimizer(model_example, LR=0.0005, prints=True)","4c5f8a4b":"def MSE_loss(output, target):\n    '''Returns the square-root loss between the predicted and actual.'''\n    loss = torch.sqrt(nn.MSELoss()(output, target))\n    \n    return loss","34dc109f":"# Clean environment of all tests we've performed\ndel optimizer_example, out, model_example, dataset, dataloader\ngc.collect()","d9ebbbf0":"def train_model(lit_data, FEATURE, SPLITS, TOKENIZER, MAX_LEN, IS_TEST, \n                TRAIN_BATCH, VALID_BATCH, DROP, LR, EPOCHS, ROBERTA,\n                META_SIZE, FEATURE_NO, MODEL_NAME, MODEL_PATH, N, CONFIG):\n    \n    # Initialize W&B experiment\n    params = dict(model=MODEL_NAME, feature=FEATURE, splits=SPLITS, max_len=MAX_LEN,\n              is_test=IS_TEST, train_batch=TRAIN_BATCH, valid_batch=VALID_BATCH, \n              drop=DROP, lr=LR, epochs=EPOCHS)\n    CONFIG.update(params)\n    run = wandb.init(project='commonlit', name=f\"{MODEL_NAME}_exp_{N}\", config=CONFIG, anonymous=\"allow\")\n    \n    # Remember to turn GPU: ON\n    device = torch.device('cuda:0')\n\n    # We need to create a BIN, because the K-Fold receives only discrete values\n    bins = int(np.floor(1 + np.log2(len(lit_data))))\n    lit_data[\"bins\"] = pd.cut(lit_data[\"target\"], bins=bins, labels=False)\n\n    # K-Fold Validation\n    cv = StratifiedKFold(n_splits=SPLITS)\n    cv_splits = cv.split(X=lit_data, y=lit_data['bins'].values)\n    del lit_data['bins']\n\n\n    # ~~~~~~~~~ Training ... ~~~~~~~~~\n    for fold, (train_i, valid_i) in enumerate(cv_splits):\n\n        print(color.S + f\"========== Fold {fold} ==========\" + color.E, \"\\n\")\n        # Set train + validation datas\n        train_df = lit_data.iloc[train_i, :]\n        valid_df = lit_data.iloc[valid_i, :]\n\n        # Now set the PyTorch Dataset\n        ### IS_TEST here will be for both True when committing\n        ### and False when submitting to competition\n        train_data = LiitDataset(texts=train_df[FEATURE].values, targets=train_df[\"target\"].values,\n                                 tokenizer=TOKENIZER, max_len=MAX_LEN, is_test=IS_TEST, \n                                 metadata=train_df.iloc[:, 9:])\n        valid_data = LiitDataset(texts=valid_df[FEATURE].values, targets=valid_df[\"target\"].values,\n                                 tokenizer=TOKENIZER, max_len=MAX_LEN, is_test=IS_TEST,\n                                 metadata = valid_df.iloc[:, 9:])\n\n        # Dataloaders\n        ### be sure to set shuffle=False for valid_loader !\n        train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH, \n                                  shuffle=True, num_workers=8)\n        valid_loader = DataLoader(valid_data, batch_size=VALID_BATCH, \n                                  shuffle=False, num_workers=8)\n\n\n        # ~~~ Model Setup ~~~\n        model = TRANSFORMERS_MODEL(MODEL_PATH, drop=DROP, roberta=ROBERTA, \n                                   no_columns=FEATURE_NO, meta_size=META_SIZE).to(device)\n        optimizer = custom_optimizer(model, LR)\n        # Compute total number of training steps\n        no = int(len(train_df) \/ TRAIN_BATCH * VALID_BATCH)\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n                                                    num_training_steps=no)\n\n        # ~~~ Epochs Loop ~~~\n        BEST_LOSS = 2\n        for epoch in range(EPOCHS):\n            print(color.S + f\"--- Epoch {epoch} ---\" + color.E)\n\n            # -Train the Model-\n            model.train()   ### Training Mode: ON\n            train_losses = []\n            for k, data in enumerate(train_loader):\n                ids, mask, meta, target = data.values()\n                ids, mask, meta, target = train_to_device(ids, mask, meta, target, device)\n\n                with autocast(): \n                    out = model(ids, mask, meta)\n                    loss = MSE_loss(out, target)\n                    train_losses.append(loss.cpu().detach().numpy().tolist())\n\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()\n                \n            # Log Training Voss into the experiment\n            train_losses = np.mean(train_losses)\n            wandb.log({\"fold\":fold, \"mean_train_loss\": np.float(train_losses)}, step=epoch)\n\n\n            # -Validate the Model-\n            all_targets, all_preds = [], []\n            model.eval()   ### Evaluation Mode: ON\n            # Disable gradients - we aren't optimizing anything now\n            with torch.no_grad():\n                for k, data in enumerate(valid_loader):\n                    ids, mask, meta, target = data.values()\n                    ids, mask, meta, target = train_to_device(ids, mask, meta, target, device)\n\n                    out = model(ids, mask, meta)\n\n                    all_targets.extend(target.cpu().detach().numpy().tolist())\n                    all_preds.extend(out.cpu().detach().numpy().tolist())\n\n                    \n            # -Let's see how the model did in this epoch-\n            epoch_loss = np.sqrt(mean_squared_error(all_targets, all_preds))\n            # Log Validation Loss into the experiment\n            wandb.log({\"fold\":fold, \"validation_loss\": np.float(epoch_loss)}, step=epoch)\n            print(\"Epoch RMSE Loss:\", epoch_loss)\n            \n\n\n            # -Save Model-\n            if epoch_loss < BEST_LOSS:\n                # If loss decreased, then save the model\n                print(\"saving model in fold {} | epoch {} ...\".format(fold, epoch))\n                torch.save(model.state_dict(), f\"{MODEL_NAME}_fold_{fold}_loss{round(epoch_loss, 4)}.pt\")\n                ### TODO: Save model to W&B\n                BEST_LOSS = epoch_loss\n\n\n        print(color.S+\"Best RMSE in this fold: \"+color.E, BEST_LOSS, \"\\n\"*2)\n        wandb.log({\"best_rmse\": np.float(BEST_LOSS)})\n\n        del model, optimizer, scheduler, ids, mask, meta, target\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    wandb.finish()","a852295a":"MODEL_PATH = \"..\/input\/d\/xhlulu\/huggingface-bert\/bert-large-uncased\"\nMODEL_NAME = \"bert-large-uncased\"\nTOKENIZER = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True)\n\nFEATURE = \"text\"        ### feature to use for training\nSPLITS = 2              ### k-fold number of splits\nMAX_LEN = 150           ### length of tokenizer\nTRAIN_BATCH = 32        ### train batch size\nVALID_BATCH = 32        ### test batch size\nDROP = 0.3              ### percentage of neurons to drop\nLR = 0.00005            ### learning rate\nEPOCHS = 3              ### how many times the model is trained\nROBERTA=False           ### wether to use bert or roberta\nN=0                     ### unique identifier for W&B experiment\nMETA_SIZE = 500         ### metadata size of hidden layer\nFEATURE_NO = 7          ### number of features to use\n\n\ntrain_model(df, FEATURE, SPLITS, TOKENIZER, MAX_LEN, IS_TEST, \n            TRAIN_BATCH, VALID_BATCH, DROP, LR, EPOCHS, ROBERTA,\n            META_SIZE, FEATURE_NO, MODEL_NAME, MODEL_PATH, N, CONFIG)","f04bbe92":"MODEL_PATH = \"..\/input\/huggingface-roberta\/roberta-large\"\nMODEL_NAME = \"roberta-large\"\nTOKENIZER = RobertaTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True)","a4c490da":"FEATURE = \"text\"        ### feature to use for training\nSPLITS = 2              ### k-fold number of splits\nMAX_LEN = 150           ### length of tokenizer\nTRAIN_BATCH = 20        ### train batch size\nVALID_BATCH = 32        ### test batch size\nDROP = 0.3              ### percentage of neurons to drop\nLR = 0.00005            ### learning rate\nEPOCHS = 3              ### how many times the model is trained\nROBERTA=True            ### wether to use bert or roberta\nN=0                     ### unique identifier for W&B experiment\nMETA_SIZE = 500         ### metadata size of hidden layer\nFEATURE_NO = 7          ### number of features to use\n\n\ntrain_model(df, FEATURE, SPLITS, TOKENIZER, MAX_LEN, IS_TEST, \n            TRAIN_BATCH, VALID_BATCH, DROP, LR, EPOCHS, ROBERTA,\n            META_SIZE, FEATURE_NO, MODEL_NAME, MODEL_PATH, N, CONFIG)","4ee77631":"# Read in training data and preprocess 'excerpt' feature\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ndf[\"text\"] = df[\"excerpt\"].apply(lambda x: clean_paragraph(x))\n\n# Create features\ndf = create_features(df)","ca5e5bdf":"def predict(df, FEATURE, TOKENIZER, MAX_LEN, IS_TEST, DROP, ROBERTA, \n            META_SIZE, FEATURE_NO, MODEL_PATH, TRAINED_MODEL_PATH):\n    '''Function that makes the inference.'''\n    \n    # Remember to turn GPU: ON\n    device = torch.device('cuda:0')\n\n    # Now set the PyTorch Dataset\n    lit_dataset = LiitDataset(texts=df[FEATURE].values, tokenizer=TOKENIZER, \n                              max_len=MAX_LEN, is_test=IS_TEST, metadata=df.iloc[:, 7:])\n    # Dataloaders\n    lit_loader = DataLoader(lit_dataset, batch_size=32, \n                              shuffle=True, num_workers=8)\n    \n    # Load pretrained model\n    model = TRANSFORMERS_MODEL(MODEL_PATH, drop=DROP, roberta=ROBERTA,\n                               no_columns=FEATURE_NO, meta_size=META_SIZE).to(device)\n    model.load_state_dict(torch.load(TRAINED_MODEL_PATH))\n    model.eval()\n    \n    final_output = []\n    \n    for index, data in enumerate(lit_loader):\n        # We are predicting, no gradients needed\n        with torch.no_grad():\n            ids, mask, meta = data.values()\n            ids, mask, meta = test_to_device(ids, mask, meta, device)\n            out = model(ids, mask, meta)\n            out = out.detach().cpu().numpy().ravel().tolist()\n            final_output.extend(out)\n    \n    # Return predictions\n    torch.cuda.empty_cache()\n    return np.array(final_output)","208185df":"# Be sure you're using the same numbers as for training!\n\nMODEL_NAME = \"bert-large-uncased\"\nMODEL_PATH = \"..\/input\/d\/xhlulu\/huggingface-bert\/bert-large-uncased\"\nTRAINED_MODEL_PATH = \"..\/input\/commonlit-dataset\/bert-large-uncased_fold_1_loss0.6337.pt\"\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL_PATH)\nFEATURE = \"text\"   \nMAX_LEN = 150 \nIS_TEST = True\nDROP = 0.3     \nROBERTA=False\nMETA_SIZE = 500\nFEATURE_NO = 7\n\n\nprediction = predict(df, FEATURE, TOKENIZER, MAX_LEN, IS_TEST, DROP, ROBERTA, \n                     META_SIZE, FEATURE_NO, MODEL_PATH, TRAINED_MODEL_PATH)","52f926f4":"submission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmission.target = prediction\nsubmission.to_csv(\"submission.csv\", index=False)","a3d73792":"submission","698a83b2":"## II. Baseline Model using RoBERTa\n\n<center><video src=\"https:\/\/i.imgur.com\/IiNJuVN.mp4\" width=700 controls><\/center>","73be1d68":"<img src=\"https:\/\/i.imgur.com\/GvOB7o2.png\">\n\n<center><h1>Training: BERT, RoBERTa and smart Experiment Tracking with W&B<\/h1><\/center>\n\n# Introduction\n\nDo you feel like you have **no idea what the heck are all these Tokenizers, what is BERT, how to train the models and ... who is Roberta?**.\n\nYou're not alone. This is my very first hands on NLP competition and let me tell you, it took some time to wrap my head around it. So, if you're a bit lost like I was starting this, hopefully the detailed explanations will bring a little bit of light.\n\n<div class=\"alert simple-alert\">\n\ud83d\udccd A VERY big part of inspiration from this notebook comes from <b>Tanay Mehta's<\/b> notebook <a href=\"https:\/\/www.kaggle.com\/heyytanay\/training-kfolds-pytorch-bert-large-w-o-oom\">[TRAINING & KFOLDS] PyTorch BERT-Large w\/o OOM\ud83c\udfaf<\/a> and <b>Abhishek Thakur's<\/b> notebook <a href=\"https:\/\/www.kaggle.com\/abhishek\/fork-of-fork-of-yum-yum-yum-93f968\">Fork of Fork of yum yum yum 93f968<\/a>. I look to learn something new every competition, and this one was my first interaction with Hugging Face. \ud83e\udd17\n<\/div>\n\n*OK! Let's get started!*\n\n### \u2b07\ufe0f Libraries","1927f9c4":"# 1. Text Preprocessing\n\nBefore we start, let's make the preprocessing. I am using the very same preprocessing I made [in my first notebook here](https:\/\/www.kaggle.com\/andradaolteanu\/i-commonlit-explore-xgbrf-repeatedfold-model).","45a9d4ad":"### What is \"transformers\" package?\n\nIt's a [State-of-the-art Natural Language Processing](https:\/\/pypi.org\/project\/transformers\/) for Jax, PyTorch and TensorFlow - from [huggingface](https:\/\/huggingface.co\/).\n\n\ud83e\udd17 Transformers provides **thousands of pretrained models** to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages.\n\n\ud83e\udd17 Transformers provides APIs to **quickly download and use those pretrained models** on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.\n\n\ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 Jax, **PyTorch** and TensorFlow.\n\n**Why would I use pretrained models?**\n\nBecause some very smart people trained these very intelligent algorithms on thousands of observations. \n\n> \ud83d\udd25 You can use what the algorithm already knows and just adapt it on our personal dataset through **Transfer Learning**. It's like teaching a 3rd grade kid how to read a passage instead of a kid that has never read before. \ud83d\ude0e","e7bdade9":"<img src=\"https:\/\/i.imgur.com\/cUQXtS7.png\">\n\n# \ud83c\udfa8 My Specs\n* \ud83d\udda5 **Z8 G4** Workstation\n* \ud83d\udcbe 2 CPUs & 96GB Memory \n* \ud83c\udfae NVIDIA **Quadro RTX 8000** \n* \ud83c\udfc3\ud83c\udffe\u200d\u2640\ufe0f **RAPIDS** version 0.17 \n* \ud83d\udcbb **Zbook Studio G7** on the go","3f75ecc8":"# 8. How do I do the inference and submit?\n\n> **Note**: As a last step, I would like to leave here **a sample code that you can use to infere into this competition**. It is quite simple, but you will **have to turn OFF the internet**, because we will send the inference through a *notebook*, and the competition doesn't allow the Internet to be turned ON. W&B doesn't work without Internet, so I would recommend taking the following code **copy-paste and creating a special notebook for inference**.\n\n### \ud83d\udd25 Step 1\n\nCopy into a new notebook (GPU: ON & Internet: OFF) the following classes: `LiitDataset()`, `TRANSFORMERS_MODEL()` and the preprocessing function (if any) for `excerpt`.\n\n### \ud83d\udd25 Step 2\n\nNow just copy the following lines of code:","de6db552":"# 3. The Dataset\n\nThe `Dataset` class is an [abstract class representing a dataset](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html) and it should have the following methods:\n\n\ud83d\udd25 `__init__` to initiate our parameters (like the `text` we want to use, the `tokenizer`, size of the `tokenizer` etc.)\n\n\ud83d\udd25 `__len__` so that len(dataset) returns the size of the dataset\n\n\ud83d\udd25 `__getitem__` to support the indexing such that dataset[i] can be used to get *i*th sample","c42e7576":"# 6. Training Function\n\n> \ud83d\udd25 **Note**: for *each fold*, TRAIN = 2,267 observation, VALID = 567 observations.\n\n\ud83e\udd17 **GET LINEAR SCHEDULE WITH WARMUP:**\n\nCreate a schedule with a `learning_rate` that **decreases linearly** from the initial `lr` set in the optimizer to 0, *after a warmup period* during which it **increases linearly** from 0 to the initial `lr` set in the optimizer.\n\n\ud83d\udd25 **AUTOCAST**:\n\n`torch.cuda.amp` provides **convenience methods** for mixed precision, where some operations use the `torch.float32` (float) datatype and other operations use `torch.float16` (half) [source here](https:\/\/pytorch.org\/docs\/stable\/amp.html#autocasting). Instances of `autocast` serve as context managers or decorators that allow regions of your script to run in mixed precision.\n\nTo better visualize what's happening in the cell below, let's see a schema that **sumarises the trainin process in `train_model()`**:\n<center><img src=\"https:\/\/i.imgur.com\/0vQ2dpk.png\" width=1000><\/center>","80c5e2a3":"# 7. Experiments\n\nWe are going to use **[W&B](https:\/\/wandb.ai\/site)** for *experiment tracking*; this way, if we alter the code or erase it in any way and for some reason we **want to go back** to a hyperparameter setup OR in case we **forgot what model performed** the best, we can just **[check the dashboard and find out](https:\/\/wandb.ai\/andrada\/commonlit?workspace=user-)** \ud83d\ude0e.\n\n> \ud83d\udd25 **Note**: you can tweak the parameters to get better results.\n\n## I. Baseline Model using BERT\n\n<center><video src=\"https:\/\/i.imgur.com\/38fo1Jz.mp4\" width=700 controls><\/center>","9c9a3f9f":"# 5. The Optimizer\n\n\ud83d\udd25 **Optimizers** are methods used to **change the attributes of the neural network such as weights and learning rate** to reduce the losses. Optimizers are used to *solve optimization problems* by minimizing the function.\n\n\ud83d\udd25 **WEIGHT DECAY**:\n\n`weight_decay` is a regularization technique by adding a small penalty, usually the L2 norm of the weights to the loss function. It provides an approach to *reduce the overfitting* on the training data and improve the performance of the model on new data.\n\n\ud83e\udd17 **ADAMW**:\n\nThe `AdamW()` optimizer **decouples the weight decay from the optimization step**. This means that the `weight_decay` and `learning_rate` can be optimized *separately*, i.e. changing the learning rate does not change the optimal weight decay.","81c8f39a":"### Loss\n\nNow lets create a loss function that will be used to assess the **difference between the predicted values of the model vs the actuals**.","398590d4":"### \ud83d\udd25 Step 3\n\nAppend predictions to the `submission.csv` dataframe and you're done!","4925f0ce":"# 4. The Pretrained Model\n\n\ud83d\udd25 OK! Now we want to create a **class** that takes the `input` (our paragraphs), processes them through some sort of pretrained model *(BERT, RoBERTa etc.)*, spills a **hidden embedding** that contains *valuable information* about the paragraphs and then finds out the `target`.\n\n\ud83d\udd25 To do that, the `output` from the pretrained model will be passed to a `Dropout()` layer and then to a simple `Linear()` one, which is the final step that **classifies the information into the score**.","7d9e4949":"### How does this class work?\n\nFor me at least it's a bit tricky to visualize what's behind Neural Nets. This is why I usually `print` a lot and create schemas out of them. The example below is for **BERT** output:\n\n<center><img src=\"https:\/\/i.imgur.com\/Kr8jP8X.png\" width=1000><\/center>","65709387":"# 2. Global Model Setup\n\n### Before we start - BERT vs ROBERTA\n\n\ud83d\udd25 **BERT** is a [bi-directional transformer for pre-training](https:\/\/towardsdatascience.com\/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8) over a lot of unlabeled textual data to learn a language representation that can be used to fine-tune for specific machine learning tasks. While BERT outperformed the NLP state-of-the-art on several challenging tasks, its performance improvement could be attributed to the bidirectional transformer, novel pre-training tasks of Masked Language Model and Next Structure Prediction along with a lot of data and Google\u2019s compute power.\n\n\ud83d\udd25 **RoBERTa** Introduced at Facebook, [robustly optimized BERT approach RoBERTa](https:\/\/towardsdatascience.com\/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8), is a retraining of BERT with improved training methodology, 1000% more data and compute power *(BERT on steroids)*.\n\n<center><img src=\"https:\/\/i.imgur.com\/FdpaUw6.png\" width=700><\/center>\n\n\n### What is \"gradScaler()\"?\n\nI saw Tanay Mehta using this and got me wondered [what is it for](https:\/\/pytorch.org\/docs\/stable\/notes\/amp_examples.html)?\n\nIf the forward pass for a particular op has `float16` inputs => the backward pass for that op will produce `float16` gradients. Gradient *values with small magnitudes may not be representable in `float16`* => these values will flush to zero, hence the update for the corresponding parameters will be lost.\n\n\ud83d\udd25 To prevent **underflow**, **`gradScaler()`** multiplies the network\u2019s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. **In other words, gradient values have a larger magnitude, so they don\u2019t flush to zero.**","04a25462":"### \ud83d\udd0e What's the Output of the Dataset?\n\n* `input_ids`: the token indices\n* `attention_mask`: exactly ehat it says - a `0` or `1` array that tells the model which tokens should be attended to and which should not\n\n<center><img src=\"https:\/\/i.imgur.com\/XDnBLcE.png\" width=1000><\/center>"}}