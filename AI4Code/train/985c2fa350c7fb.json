{"cell_type":{"a9c55295":"code","7788aa78":"code","1c55c9c3":"code","32c951ba":"code","d9066e8d":"code","cfc42613":"code","d3e08181":"code","26f5b8e1":"code","236ed59e":"code","61241276":"code","164faea7":"code","f5449d2f":"code","c35acba5":"code","eec77be6":"code","364e6a7f":"code","70744479":"code","33d31578":"code","479a3da2":"code","47a0ea4d":"code","1bd6451f":"code","4a3ce6a6":"code","399ae411":"code","3833ce9d":"code","b0ad274f":"code","73709e90":"code","dbaa7618":"code","a6ed8f4f":"markdown","92e1e346":"markdown"},"source":{"a9c55295":"%matplotlib inline\nfrom functools import lru_cache\nimport os\nimport numpy as np\nimport scipy.io.wavfile\nimport scipy.fftpack\nimport scipy.linalg\nimport matplotlib.pyplot as plt","7788aa78":"PATH = '..\/input\/dtw2'\nCOMMANDS = f'{PATH}\/audio'\nVALIDATION = f'{PATH}\/validation'\nTEST = f'{PATH}\/test'\nSPOKEN_DIGIT = f'{PATH}\/free-spoken-digit-dataset\/recordings'","1c55c9c3":"# Folder 'audio': 10 recordings of each digit at 16Khz\ncommands10x10 = []\nfor root, dirs, files in os.walk(COMMANDS):\n    for name in files:\n        filename = os.path.join(root, name)\n        command = root.split('\/')[-1]\n        speaker = name.split('_')[0]\n        commands10x10.append({\n            'wav': filename,\n            'text': command,\n            'speaker': speaker\n        })\nprint(commands10x10[-1])","32c951ba":"# Folder 'validation': 100 recordings of each digit at 16Khz\ncommands10x100 = []\nfor root, dirs, files in os.walk(VALIDATION):\n    for name in files:\n        filename = os.path.join(root, name)\n        command = root.split('\/')[-1]\n        speaker = name.split('_')[0]\n        commands10x100.append({\n            'wav': filename,\n            'text': command,\n            'speaker': speaker\n        })\nprint(commands10x100[-1])","d9066e8d":"# Folder 'free-spoken-digit-dataset': 4 recordings of each digit for each of the four selected speakers\nfree10x4x4 = []\nSPEAKERS = ['jackson', 'nicolas', 'theo', 'yweweler']\nN_DIGITS = 10\nN_REPETITIONS = 4\nfor digit in range(N_DIGITS):\n    for speaker in SPEAKERS:\n        for repetition in range(N_REPETITIONS):\n            free10x4x4.append({\n                'wav': f'{SPOKEN_DIGIT}\/{digit}_{speaker}_{repetition}.wav',\n                'text': str(digit),\n                'speaker': speaker\n            })\nprint(free10x4x4[-1])","cfc42613":"# Hamming window\n@lru_cache(maxsize=10)\ndef get_window(n, type='hamming'):\n    coefs = np.arange(n)\n    window = 0.54 - 0.46 * np.cos(2 * np.pi * coefs \/ (n - 1))\n    return window\nplt.plot(get_window(512))","d3e08181":"# Preemphasis filter\ndef apply_preemphasis(y, preemCoef=0.97):\n    y[1:] = y[1:] - preemCoef*y[:-1]\n    y[0] *= (1 - preemCoef)\n    return y","26f5b8e1":"def freq_to_mel(freq):\n    return 2595.0 * np.log10(1.0 + freq \/ 700.0)\nplt.plot(freq_to_mel(np.arange(8000)))","236ed59e":"def mel_to_freq(mels):\n    return 700.0 * (np.power(10.0, mels \/ 2595.0) - 1.0)","61241276":"@lru_cache(maxsize=10)\ndef get_filterbank(numfilters, filterLen, lowFreq, highFreq, samplingFreq):\n    minwarpfreq = freq_to_mel(lowFreq)\n    maxwarpfreq = freq_to_mel(highFreq)\n    dwarp = (maxwarpfreq - minwarpfreq) \/ (numfilters + 1)\n    f = mel_to_freq(np.arange(numfilters + 2) * dwarp + minwarpfreq) * (filterLen - 1) * 2.0 \/ samplingFreq\n    i = np.arange(filterLen)[None, :]\n    f = f[:, None]\n    hislope = (i - f[:numfilters]) \/ (f[1:numfilters+1] - f[:numfilters])\n    loslope = (f[2:numfilters+2] - i) \/ (f[2:numfilters+2] - f[1:numfilters+1])\n    H = np.maximum(0, np.minimum(hislope, loslope))\n    return H\nH = get_filterbank(numfilters=20, filterLen=257, lowFreq=0, highFreq=8000, samplingFreq=16000)\nfig = plt.figure(figsize=(20,10))\nfor h in H:\n  plt.plot(h)","164faea7":"def normalized(y, threshold=0):\n    y -= y.mean()\n    stddev = y.std()\n    if stddev > threshold:\n        y \/= stddev\n    return y","f5449d2f":"def mfsc(y, sfr, window_size=0.025, window_stride=0.010, window='hamming', normalize=False, log=True, n_mels=20, preemCoef=0, melfloor=1.0):\n    win_length = int(sfr * window_size)\n    hop_length = int(sfr * window_stride)\n    n_fft = 512\n    lowfreq = 0\n    highfreq = sfr\/2\n    \n    # get window\n    window = get_window(win_length)\n    padded_window = np.pad(window, (0, n_fft - win_length), mode='constant')[:, None]\n    \n    # preemphasis\n    y = apply_preemphasis(y.copy(), preemCoef)\n\n    # scale wave signal\n    y *= 32768\n    \n    # get frames\n    num_frames = 1 + (len(y) - win_length) \/\/ hop_length\n    pad_after = num_frames*hop_length + (n_fft - hop_length) - len(y)\n    if pad_after > 0:\n        y = np.pad(y, (0, pad_after), mode='constant')\n    frames = np.lib.stride_tricks.as_strided(y, shape=(n_fft, num_frames), strides=(y.itemsize, hop_length * y.itemsize), writeable=False)\n    windowed_frames = padded_window * frames\n    D = np.abs(np.fft.rfft(windowed_frames, axis=0))\n\n    # mel filterbank\n    filterbank = get_filterbank(n_mels, n_fft\/2 + 1, lowfreq, highfreq, sfr)\n    mf = np.dot(filterbank, D)\n    mf = np.maximum(melfloor, mf)\n    if log:\n        mf = np.log(mf)\n    if normalize:\n        mf = normalized(mf)\n\n    return mf","c35acba5":"def mfsc2mfcc(S, n_mfcc=12, dct_type=2, norm='ortho', lifter=22, cms=True, cmvn=True):\n    # Discrete Cosine Transform\n    M = scipy.fftpack.dct(S, axis=0, type=dct_type, norm=norm)[:n_mfcc]\n\n    # Ceptral mean subtraction (CMS) \n    if cms or cmvn:\n        M -= M.mean(axis=1, keepdims=True)\n\n    # Ceptral mean and variance normalization (CMVN)\n    if cmvn:\n        M \/= M.std(axis=1, keepdims=True)\n    \n    # Liftering\n    elif lifter > 0:\n        lifter_window = 1 + (lifter \/ 2) * np.sin(np.pi * np.arange(1, 1 + n_mfcc, dtype=M.dtype) \/ lifter)[:, np.newaxis]\n        M *= lifter_window\n\n    return M","eec77be6":"from IPython.display import Audio\nAudio(filename=free10x4x4[0]['wav'], autoplay=True)","364e6a7f":"import librosa\nimport librosa.display\nfig = plt.figure(figsize=(20,15))\nfor i in range(2):\n    wav = free10x4x4[i]['wav']\n    sfr, y = scipy.io.wavfile.read(wav)\n    y = y\/32768\n    print(wav, 'Sampling frequency: ', sfr)\n    fig = plt.subplot(4,2,i+1)\n    plt.plot(y)\n\n    # Liner frequency spectrogram\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n    fig = plt.subplot(4,2,i+3)\n    librosa.display.specshow(D, y_axis='linear', sr=sfr)\n    plt.title('Linear-frequency power spectrogram')\n\n    # Mel-scaled spectrogram (20 bank filters)\n    S = mfsc(y, sfr)\n    fig = plt.subplot(4,2,i+5)\n    librosa.display.specshow(S - S.min())\n    plt.title('Mel-scaled power spectrogram')\n\n    # MFCC(5)\n    M = mfsc2mfcc(S)\n    fig = plt.subplot(4,2,i+7)\n    plt.plot(M[1,:])","70744479":"from numba import jit\n@jit\ndef dtw(x, y, dist='sqeuclidean'):\n  \"\"\"\n  Computes Dynamic Time Warping (DTW) of two sequences.\n  :param array x: N1*M array\n  :param array y: N2*M array\n  :param func dist: distance used as cost measure\n  \"\"\"\n  r, c = len(x), len(y)\n\n  D = np.zeros((r + 1, c + 1))\n  D[0, 1:] = np.inf\n  D[1:, 0] = np.inf\n\n  D[1:, 1:] = scipy.spatial.distance.cdist(x, y, dist)\n\n  for i in range(r):\n    for j in range(c):\n      min_prev = min(D[i, j], D[i+1, j], D[i, j+1])\n      # D[i+1, j+1] = dist(x[i], y[j]) + min_prev\n      D[i+1, j+1] += min_prev\n\n  '''\n    if len(x) == 1:\n        path = zeros(len(y)), range(len(y))\n    elif len(y) == 1:\n        path = range(len(x)), zeros(len(x))\n    else:\n        path = _traceback(D)\n    return D[-1, -1], path\n  '''\n\n  return D[-1, -1]","33d31578":"# Do not show numba warnings\nimport warnings\nfrom numba.core.errors import NumbaWarning\nwarnings.simplefilter(\"ignore\", category=NumbaWarning)","479a3da2":"a = np.array([1,4,4,5,9,3,1,8,8], dtype=np.float32)[:,np.newaxis]\nb = np.array([1,2,3,8,8,3,1,8], dtype=np.float32)[:,np.newaxis]\ndtw(a, b, dist='sqeuclidean')","47a0ea4d":"# Compute MFCC\ndef get_mfcc(dataset, **kwargs):\n    mfccs = []\n    for sample in dataset:\n        sfr, y = scipy.io.wavfile.read(sample['wav'])\n        y = y\/32768\n        S = mfsc(y, sfr, **kwargs)\n        M = mfsc2mfcc(S).T\n        # DM = delta(M)\n        # M = np.hstack((M, DM))\n        mfccs.append(M.astype(np.float32))\n    return mfccs","1bd6451f":"# Word Error Rate (Accuracy)\ndef wer(test_dataset, ref_dataset=None, same_spk=False):\n    # Compute mfcc\n    test_mfcc = get_mfcc(test_dataset)\n    if ref_dataset is None:\n        ref_dataset = test_dataset\n        ref_mfcc = test_mfcc\n    else:\n        ref_mfcc = get_mfcc(ref_dataset)\n        \n    err = 0\n    for i, test in enumerate(test_dataset):\n        mincost = np.inf\n        minref = None\n        for j, ref in enumerate(ref_dataset):\n            if not same_spk and test['speaker'] == ref['speaker']:\n                # Do not compare with refrence recordings of the same speaker\n                continue\n            if test['wav'] != ref['wav']:\n                distance = dtw(test_mfcc[i], ref_mfcc[j])\n                if distance < mincost:\n                    mincost = distance\n                    minref = ref\n        if test['text'] != minref['text']:\n            err += 1\n\n    wer = 100*err\/len(test_dataset)\n    return wer","4a3ce6a6":"# Free Spoken Digit Dataset\nprint(f'WER including reference recordings from the same speaker: {wer(free10x4x4, same_spk=True):.1f}%')","399ae411":"# Google Speech Commands Dataset (small digit subset)\nprint(f'WER using only reference recordings from other speakers: {wer(commands10x100, commands10x10):.1f}%')","3833ce9d":"test_wavs = []\nfor filename in os.listdir(TEST):\n    test_wavs.append({\n        'wav': TEST + '\/' + filename\n    })","b0ad274f":"def test(test_wavs, ref_wavs):\n    pred = []\n    test_mfccs = get_mfcc(test_wavs)\n    ref_mfccs = get_mfcc(ref_wavs)\n    for i in range(len(test_mfccs)):\n        mincost = np.inf\n        jmin = -1\n        for j in range(len(ref_mfccs)):\n            distance = dtw(test_mfccs[i], ref_mfccs[j])\n            if distance < mincost:\n                mincost = distance\n                jmin = j\n        pred.append(ref_wavs[jmin]['text'])\n        if i<10:\n            print(f'{i:3}\/{len(test_mfccs)}: {pred[i]}')\n    return pred","73709e90":"# Use the two labeled 'Speech Commands' datasets (commands10x10 + commands10x100) as reference templates for the test prediction \npred = test(test_wavs, commands10x10 + commands10x100)","dbaa7618":"with open('submission.csv', 'w') as f:\n    print('filename,command', file=f)\n    for entry, command in zip(test_wavs, pred):\n        filename = entry['wav'].split('\/')[-1].split('.')[0]\n        print(f'{filename},{command}', file=f)","a6ed8f4f":"Plot two examples","92e1e346":"Fast DTW version using scipy.spatial.distance.cdist to compute distance and numba (compiled python)"}}