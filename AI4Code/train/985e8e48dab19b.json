{"cell_type":{"cd7173e1":"code","aef05dec":"code","6651f61f":"code","41eb712a":"code","26dbb26c":"code","b2ee854d":"code","f8c91817":"code","5882235d":"code","a06cfcc0":"code","1a78cbba":"code","ee22f6b6":"code","818ef634":"code","513ae589":"code","93efbb0c":"code","4b48b0c4":"markdown"},"source":{"cd7173e1":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom tensorflow.keras.applications import imagenet_utils\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import load_model\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport random as rd","aef05dec":"def process_data(source_dir, subject,image_size,t_split, v_split,rand_seed):\n    # read in data from each class then split into train, test,valid data sets\n    data=[]\n    labels=[]\n    file_names=[]\n    class_list=os.listdir(source_dir) # list of flower types daisy etc\n    class_list.remove('brain_tumor_dataset')\n    print (class_list)\n    leave=True    \n    c_count=-1\n    for c in class_list:\n        c_count=c_count +1\n        c_path=os.path.join(source_dir,c) #path to class dir - ie c:\\temp\\flowers\\daisy\n        partition=c_path.split(r'\/')\n        p_len=len(partition)\n        desc=partition[p_len-2] + '-' + partition[p_len-1]\n        # read in the data for each class \n        f_list=os.listdir(c_path) # path to images in class directory\n        for f in tqdm(f_list, desc=desc, unit='files', leave=leave):\n            index=f.rfind('.')\n            ext=f[index+1:]\n            if ext in ['jpg', 'jpe', 'png', 'jpeg', 'gif']:\n                f_path=os.path.join(c_path, f)                        \n                img=cv2.imread(f_path,1)\n                image_array = Image.fromarray(img , 'RGB')\n                resize_img = image_array.resize((image_size ,image_size))\n                data.append(np.array(resize_img))   #data is a list of image arrays for the entire data set\n                labels.append(c_count) # c_count is the label associated with the array image, \n                file_names.append(f)\n    # at this point all image data is in a list of arrays, labels exist for each image\n    net_split=(t_split + v_split)\/100\n    v_share=t_split\/(t_split + v_split)\n    train_data , x , train_labels , y = train_test_split(data,labels,test_size =net_split,random_state =rand_seed)\n    train_files, a =train_test_split(file_names, test_size=net_split, random_state=rand_seed)\n    val_data , test_data , val_labels , test_labels = train_test_split(x,y,test_size = v_share,random_state = rand_seed)\n    val_files, test_files=train_test_split(a,test_size=v_share, random_state=rand_seed)\n    print_data(train_labels, test_labels, val_labels, class_list)\n    train_data=np.array(train_data)\n    train_labels-np.array(train_labels)\n    train_files=np.array(train_files)\n    test_data=np.array(test_data)\n    test_labels=np.array(test_labels)\n    test_files=np.array(test_files)\n    val_data=np.array(val_data)\n    val_labels=np.array(val_labels)\n    val_files=np.array(val_files)\n    data_set=[train_data, train_labels, test_data, test_labels, val_data, val_labels,test_files, class_list]\n    return data_set    ","6651f61f":"def print_data(train_labels, test_labels, val_labels, class_list):\n    train_list=list(train_labels)\n    test_list=list(test_labels)\n    val_list=list(val_labels)\n    print('{0:9s}Class Name{0:10s}Class No.{0:4s}Train Files{0:7s}Test Files{0:5s}Valid Files'.format(' '))\n    for i in range(0, len(class_list)):\n        c_name=class_list[i]\n        tr_count=train_list.count(i)\n        tf_count=test_list.count(i)\n        v_count=val_list.count(i)\n        print('{0}{1:^25s}{0:5s}{2:3.0f}{0:9s}{3:4.0f}{0:15s}{4:^4.0f}{0:12s}{5:^3.0f}'.format(' ',\n                                                                                               c_name,i,tr_count,\n                                                                                               tf_count,v_count))\n    print('{0:30s} ______________________________________________________'.format(' '))\n    msg='{0:10s}{1:6s}{0:16s}{2:^3.0f}{0:8s}{3:3.0f}{0:15s}{4:3.0f}{0:13s}{5}\\n'\n    print(msg.format(' ', 'Totals',len(class_list),len(train_labels),len(test_labels),len(val_labels)))","41eb712a":"def get_steps(train_data, test_data,val_data,batch_size):\n    length=train_data.shape[0]\n    if length % batch_size==0:\n        tr_steps=int(length\/batch_size)\n    else:\n        tr_steps=int(length\/batch_size) + 1\n    length=val_data.shape[0]\n    if length % batch_size==0:\n        v_steps=int(length\/batch_size)\n    else:\n        v_steps=int(length\/batch_size) + 1\n    length=test_data.shape[0]\n    batches=[int(length\/n) for n in range(1,length+1) if length % n ==0 and length\/n<=80]\n    batches.sort(reverse=True)\n    t_batch_size=batches[0]\n    t_steps=length\/t_batch_size        \n    return [tr_steps,t_steps, v_steps, t_batch_size]","26dbb26c":"def make_model(source_dir,output_dit,class_list, image_size, subject,model_size, rand_seed,lr_factor):\n    size=len(class_list)\n    check_file = os.path.join(output_dir, 'tmp.h5')\n        \n    if model_size=='L':\n        # mobile = keras.applications.mobilenet_v2.MobileNetV2(input_shape=input_shape)\n        mobile = tf.keras.applications.mobilenet.MobileNet()        \n        #remove last 5 layers of model and add dense layer with 128 nodes and the prediction layer with size nodes\n        # where size=number of classes\n        x=mobile.layers[-6].output\n        x=Dense(128, kernel_regularizer = regularizers.l2(l = 0.015), activation='relu')(x)\n        x=Dropout(rate=.5, seed=rand_seed)(x)\n        predictions=Dense (size, activation='softmax')(x)\n        model = Model(inputs=mobile.input, outputs=predictions)\n        for layer in model.layers:\n            layer.trainable=True\n        model.compile(Adam(lr=lr_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        \n    else:\n        if model_size=='M':\n            fm=2\n        else:\n            fm=1\n        model = Sequential()\n        model.add(Conv2D(filters = 4*fm, kernel_size = (3, 3), activation ='relu', padding ='same', name = 'L11',\n                         kernel_regularizer = regularizers.l2(l = 0.015),input_shape = (image_size, image_size, 3)))\n        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), name ='L12'))\n        model.add(BatchNormalization(name = 'L13'))\n        model.add(Conv2D(filters = 8*fm, kernel_size = (3, 3), activation ='relu',\n                         kernel_regularizer = regularizers.l2(l = 0.015), padding ='same', name = 'L21')) \n        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), name ='L22'))\n        model.add(BatchNormalization(name = 'L23'))\n        model.add(Conv2D(filters = 16*fm, kernel_size = (3, 3), activation ='relu',\n                         kernel_regularizer = regularizers.l2(l = 0.015), padding ='same', name ='L31')) \n        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), name ='L32'))\n        model.add(BatchNormalization(name = 'L33'))\n        if fm==2:\n            model.add(Conv2D(filters = 32*fm, kernel_size = (3, 3), activation ='relu',\n                             kernel_regularizer = regularizers.l2(l = 0.015),padding ='same', name ='L41')) \n            model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), name ='L42'))\n            model.add(BatchNormalization(name = 'L43'))\n            model.add(Conv2D(filters = 64*fm, kernel_size = (3, 3), activation ='relu', \n                             kernel_regularizer = regularizers.l2(l = 0.015),padding ='same', name ='L51')) \n            model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), name ='L52'))\n            model.add(BatchNormalization(name = 'L53'))\n            \n        model.add(Flatten())\n        model.add(Dense(256 *fm,kernel_regularizer = regularizers.l2(l = 0.015), activation='relu', name ='Dn1'))\n        model.add(Dropout(rate=.5))\n        model.add(Dense(size, activation = 'softmax', name ='predict'))\n        model.compile(Adam(lr=lr_rate),loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        \n    early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, mode='min', verbose=1)\n    checkpoint = ModelCheckpoint(check_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min', period=1)\n    lrck=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=lr_factor, patience=1,\n                                           verbose=1, mode='min', min_delta=0.000001, cooldown=1, min_lr=1.0e-08)\n    callbacks=[checkpoint,lrck, early_stop]\n    return [model, callbacks]","b2ee854d":"def make_generators(data_sets, batch_size,t_batch_size,rand_seed):\n    #data_set[0]=train data,[1]train labels,[2]=test data,[3]=test labels,[4]=value data,[5]=val labels,[6]=test files\n    train_datagen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                             horizontal_flip=True,\n                             samplewise_center=True,\n                             samplewise_std_normalization=True)\n    train_gen=train_datagen.flow(data_sets[0],data_sets[1], batch_size=batch_size, seed=rand_seed)\n    val_datagen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                             samplewise_center=True,\n                             samplewise_std_normalization=True)\n    val_gen=val_datagen.flow(data_sets[4], data_sets[5], batch_size=batch_size, seed=rand_seed)\n    test_datagen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                             samplewise_center=True,\n                             samplewise_std_normalization=True)\n    test_gen=test_datagen.flow(data_sets[2], data_sets[3], batch_size=t_batch_size, shuffle=False)\n    return [train_gen, test_gen, val_gen]","f8c91817":"def train(model, train_gen, val_gen,tr_steps,v_steps, epochs,callbacks,sub):\n    start=time.time()\n    data = model.fit_generator(generator = train_gen, validation_data= val_gen,\n                       steps_per_epoch=tr_steps, epochs=epochs, \n                       validation_steps=v_steps, callbacks = callbacks)\n    stop=time.time()\n    duration = stop-start\n    hrs=int(duration\/3600)\n    mins=int((duration-hrs*3600)\/60)\n    secs= duration-hrs*3600-mins*60\n    msg='The training cycle took  {0} hours {1} minutes and {2:6.2f} seconds'\n    print(msg.format(hrs, mins,secs))\n    return data\n    ","5882235d":"def tr_plot(tacc,vacc,tloss,vloss):\n    #Plot the training and validation data\n    Epoch_count=len(tloss)\n    Epochs=[]\n    for i in range (0,Epoch_count):\n        Epochs.append(i+1)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    plt.style.use('fivethirtyeight')\n    plt.show()\n ","a06cfcc0":"def display_pred(source_dir,output_dir,pred,t_files,t_labels,class_list,subject, model_size):\n    # t_files are the test files, t_labels are the class label associated with the test file\n    # class_list is a list of classes\n    trials=len(t_files)   # number of predictions made should be same as len(t_files)\n    errors=0\n    prob_list=[]\n    true_class=[]\n    pred_class=[]\n    file_list=[]\n    x_list=[]\n    index_list=[]\n    pr_list=[]\n    error_msg=''\n    for i in range (0,trials):\n        p_c_num=pred[i].argmax()  #the index with the highest prediction value\n        if p_c_num !=t_labels[i]: #if the predicted class is not the same as the test label it is an error\n            errors=errors + 1\n            file_list.append(t_files[i])  # list of file names that are in error\n            true_class.append(class_list[t_labels[i]]) # list classes that have an eror\n            pred_class.append(class_list[p_c_num]) #class the prediction selected\n            prob_list.append(100 *pred[i][p_c_num])# probability of the predicted class\n            add_msg='{0:^24s}{1:5s}{2:^20s}\\n'.format(class_list[t_labels[i]], ' ', t_files[i])\n            error_msg=error_msg + add_msg\n            \n    accuracy=100*(trials-errors)\/trials\n    print('\\n There were {0} errors in {1} trials for an accuracy of {2:7.3f}'.format(errors, trials,accuracy,),flush=True)\n    if errors<=25:\n        msg='{0}{1:^24s}{0:3s}{2:^20s}{0:3s}{3:20s}{0:3s}{4}'\n        print(msg.format(' ', 'File Name', 'True Class', 'Predicted Class', 'Probability'))\n        for i in range(0,errors):\n            msg='{0}{1:^24s}{0:3s}{2:^20s}{0:3s}{3:20s}{0:5s}{4:^6.2f}'\n            print (msg.format(' ',file_list[i], true_class[i], pred_class[i], prob_list[i]))\n    else:\n        print('with {0} errors the full error list will not be printed'.format(errors))    \n    acc='{0:6.2f}'.format(accuracy)\n    if model_size=='L':\n        ms='Large'\n    elif model_size=='M':\n        ms= 'Medium'\n    else:\n        ms= 'Small'\n    header='Classification subject: {0} There were {1} errors in {2} tests for an accuracy of {3} using a {4} model\\n'.format(subject,errors,trials,acc,ms)\n    header= header +'{0:^24s}{1:5s}{2:^20s}\\n'.format('CLASS',' ', 'FILENAME') \n    error_msg=header + error_msg\n    file_name='error list-' + model_size + acc +'.txt'\n    print('\\n file {0} containing the list of errors has been saved to {1}'.format(file_name, output_dir))\n    file_path=os.path.join(output_dir,file_name)\n    f=open(file_path, 'w')\n    f.write(error_msg)\n    f.close()\n    for c in class_list:\n        count=true_class.count(c)\n        x_list.append(count)\n        pr_list.append(c)\n    for i in range(0, len(x_list)):  # only plot classes that have errors\n        if x_list[i]==0:\n            index_list.append(i)\n    for i in sorted(index_list, reverse=True):  # delete classes with no errors\n        del x_list[i]\n        del pr_list[i]      # use pr_list - can't change class_list must keep it fixed\n    fig=plt.figure()\n    fig.set_figheight(len(pr_list)\/4)\n    fig.set_figwidth(6)\n    plt.style.use('fivethirtyeight')\n    for i in range(0, len(pr_list)):\n        c=pr_list[i]\n        x=x_list[i]\n        plt.barh(c, x, )\n        plt.title('Errors by class')\n    plt.show()\n    time.sleep(5.0)\n    \n    return accuracy        ","1a78cbba":"def save_model(output_dir,subject, accuracy,r_model, image_size):\n    # save the model with the  subect-accuracy.h5\n    acc=str(accuracy)[0:5]\n    tempstr=subject + '-' +str(image_size) + '-' + acc + '.h5'\n    model_save_path=os.path.join(output_dir,tempstr)\n    r_model.save(model_save_path)    ","ee22f6b6":"def make_predictions(output_dir, test_gen, t_steps):\n    # the best model was saved as a file need to read it in and load it since it is not available otherwise\n    test_gen.reset()\n    msg='Training has completed, now loading saved best model and processing test set to see how accurate the model is'\n    print (msg,flush=True)\n    model_path=os.path.join(output_dir,'tmp.h5')\n    model=load_model(model_path)                      # load the saved model with lowest validation loss\n    pred=model.predict_generator(test_gen, steps=t_steps,verbose=1) # make predictions on the test set\n    return [pred, model]","818ef634":"def wrapup (source_dir,output_dir,subject, accuracy, r_model,image_size,run_num, model_size):\n    if accuracy >= 95:\n        msg='With an accuracy of {0:5.2f} the results appear satisfactory and the program will terminate'\n        print(msg.format(accuracy),flush=True)\n        return [False, None]\n    elif accuracy >=85 and accuracy < 95:\n        if run_num<2:\n            msg='With an accuracy of {0:5.2f} our results are mediocure, will run 10 more epochs to see if accuracy improves '\n            print (msg.format(accuracy),flush=True)\n            return [True, 10]\n        else:\n            print('Final accuracy of {0} is still mediocure- program terminating'.format(accuracy))\n            if model_size !='L':\n                print('try running again with model_size=L to get a more accurate result')\n            return [False, None]\n    else:\n        if run_num<2:\n            msg='With an accuracy  of {0:5.2f} the results are poor, will run for 15 more epochs to see if accuracy improves'\n            print (msg.format( accuracy),flush=True)\n            msg='if running more epochs does not improve test set accuracy you may want to get more training data '\n            msg=msg + 'or perhaps crop your images so the desired subject takes up most of the image'\n            print (msg, flush=True)\n            return [True, 15]\n        else:\n            print('Final accuracy of {0} is still poor - program is terminating'.format(accuracy))\n            if model_size !='L':\n                print('try running again with model_size=L to get a more accurate result')\n            return [False, None] ","513ae589":"def TF2_classify(source_dir,output_dir,mode,subject, t_split=10, v_split=5, epochs=20,batch_size=80,\n                 lr_rate=.002,lr_factor=.8,image_size=224,rand_seed=128,model_size='L'):\n    model_size=model_size.upper()\n    mode=mode.lower()\n    if model_size=='L':\n        image_size=224              # for the large model image size must be 224\n    data_sets=process_data(source_dir, subject,image_size,t_split, v_split,rand_seed)\n    #data_set[0]=train data,[1]train labels,[2]=test data,[3]=test labels,[4]=value data,[5]=val labels,[6]=test files\n    # data_sets[7]=class_list\n    steps=get_steps(data_sets[0],data_sets[2],data_sets[4],batch_size)\n    # tr_steps=steps[0]  t_steps=steps[1]   v_steps=steps[2] t_batch_size=steps[3]\n    model_data=make_model(source_dir,output_dir,data_sets[7], image_size, subject, model_size,rand_seed,lr_factor)\n    # model=model_data[0]  callbacks=model_data[1]\n    gens=make_generators(data_sets,batch_size,steps[3], rand_seed)\n    # train_gen = gens[0]  test_gen=gens[1]   val_gen=gens[2]\n    run_num=0\n    run=True\n    tacc=[]\n    tloss=[]\n    vacc=[]\n    vloss=[]\n    while run:\n        run_num=run_num +1\n        results=train(model_data[0], gens[0], gens[2],steps[0],steps[2], epochs,model_data[1],subject)\n        tacc_new=results.history['accuracy']\n        tloss_new=results.history['loss']\n        vacc_new =results.history['val_accuracy']\n        vloss_new=results.history['val_loss']\n        for d in tacc_new:  # need to append new data from training to plot all epochs\n            tacc.append(d)\n        for d in tloss_new:\n            tloss.append(d)\n        for d in vacc_new:\n            vacc.append(d)\n        for d in vloss_new:\n            vloss.append(d)\n        tr_plot(tacc,vacc,tloss,vloss)\n        predict=make_predictions(output_dir, gens[1], steps[1],)\n        # pred= predict[0]  r_model=predict[1]\n        accuracy=display_pred(source_dir,output_dir,predict[0],data_sets[6],data_sets[3],data_sets[7], subject, model_size)\n        decide=wrapup(source_dir,output_dir,subject, accuracy, predict[1], image_size,run_num, model_size)\n        run=decide[0]\n        epochs=decide[1] ","93efbb0c":"import os\nd_list=os.listdir('\/kaggle\/input')\nfor d in d_list:\n    d_path=os.path.join('\/kaggle\/input', d)\n    class_list=os.listdir(d_path)\n    print(class_list)\n    for c in class_list:\n        c_path=os.path.join(d_path, c)\n        print(c,c_path)\nsource_dir='\/kaggle\/input\/brain-mri-images-for-brain-tumor-detection'\noutput_dir='\/kaggle\/working'\nsubject='brain tumors'\nt_split=8\nv_split=10\nepochs=30\nbatch_size=40\nlr_rate=.0015\nlr_factor=.8\nimage_size=224\nrand_seed=64\nmodel_size='L'\nmode='all'\n\nTF2_classify(source_dir,output_dir,mode,subject, t_split=t_split, v_split=v_split, epochs=epochs,batch_size=batch_size,\n         lr_rate=lr_rate,lr_factor=lr_factor,image_size=image_size,rand_seed=rand_seed, model_size=model_size)","4b48b0c4":"This is a general purpose image classifier that can be used for most image classification problems.\nNo knowledge of neural networks or Tensorflow is required to use it.\nAn example of use on the Autism data set is shown below \nsource_dir=r'c:\\Temp\\autism' # note use of r for raw and use a  \\ not a\/\noutput_dir=r'c:\\Temp\\autism'\nsubject='autism'\nt_split=5\nv_split=5 \nepochs=30 \nbatch_size=80 \nlr_rate=.0025 \nlr_factor=.8\nimage_size=128 \nrand_seed=256\nmodel_size='L'\nmode='sep'\n\nTF2_classify(source_dir,output_dir,mode,subject, t_split=t_split, v_split=v_split, epochs=epochs,batch_size=batch_size, lr_rate=lr_rate,lr_factor,image_size=image_size,rand_seed=rand_seed, model_size=model_size)\n\nThe program operates in one of two modes, If mode-'all' the training, test and validation files\nare taken from the source_dir and split into train, test and validation files as defined by \nt_split and v_split integer percentages. If model='sep' images are read in from the train, test and \nvalid directories within the source directory. t-split and v_split values are not used in this mode.\nepochs is the number of training epochs. lr_rate is a float defining the learning rate. \nlr_factor is a float. The learning rate is reduced by this factor each time an epoch results in a\nvalidation loss larger than the lowest value thus far achieved. batch size is the number of images processed as a group during training. \nThe program has 3 models. model_size='L' is a large model based on the MobileNet architecture. It is accurate but propcessing time and memory requirements can be large. For this model set batch_size-80. If you get a resource exhaust error reduce its value. If model_size=\"M' the program uses a medium sized model. Execution is fairly fast but it is less accurate. A batch size of 80 works well. If model_size=\"S\" a small model is used. Execution is fast but accuracy is reduced. \nrand_seed sets the seed for the random generators. It's value is arbitrary but when changed will give a different mix of training, test and validation files when mode='all'.\nimage_size is the size that images are converted to for processing. If mode=\"L\" image size is set internally at 224. subject is a string you can use to denote the name of files that are stored in your output_dir at the conclusion of the program. At the conclusion of training, the best saved model is loaded from the output_dir and the test set is processed. Test results are displayed if there are less than 25 errors. The error list is saved to a file in the output_dir.  At the conclusion of the program two files are stored in the output_dir. One of them is the resulting trained model file. It is labelled as subject-image_size-accuracy.h5 , For example autism-224-95.35.h5 means the subject was autism, the image size was 224 X 224 and the accuracy on the test set was 95.35%. This model file can then be used with a prediction program. The other file saved is a text file that can be easily converted into a python dictionary. The key is the class number and the value is the associated class name. It is labelled as subject.txt. This file will also be needed by a prediction program to generate a list of classes. The error list is stored in the output_dir as error-list-M-accuracy.txt where M is the model type.\nAfter the test list is processed the program evaluates the accuray. If it is above 95% the program terminates. If the accuracy is below 95 precent the program will train for 10 more epochs starting from the last epoch. After these 10 epochs complete the new saved model is loaded and the test set is run again. Results are then shown after which the program terminates.\nTo use the program you need a python 3 environment in which you have loaded the modules tensorflow 2.0, numpy, matplot, sklearn, tqdm cv2,random and PIL."}}