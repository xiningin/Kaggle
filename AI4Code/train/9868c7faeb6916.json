{"cell_type":{"16e06d51":"code","ac31e3bb":"code","f3bd6629":"code","dc9a73f0":"code","32f832fd":"code","b5e06295":"code","9fd8805d":"code","fb345d4f":"code","c7d526e3":"code","865cdb47":"code","4ec1f100":"code","69d69668":"code","9923ddc5":"code","e92a4c9c":"code","2cff8594":"code","a8f24beb":"code","a7986f00":"code","ffeafac4":"code","faf2ac6a":"code","7a320ece":"code","8bbef6ff":"code","ba52d0b3":"code","a06a85f3":"code","d0c202b5":"code","1467312f":"code","729a28b6":"code","b962474a":"code","eabf7253":"code","fc6066e7":"code","73c3d89b":"code","ff8ce40a":"code","4103f408":"code","15c3674c":"code","24aee8b3":"code","1eaeb301":"code","5a143913":"code","33cc6ee8":"code","0f1bcee1":"code","88506236":"code","0073bf56":"code","d5a4cbf7":"markdown","1cf0300c":"markdown","35330c43":"markdown","f0e72129":"markdown","182b9192":"markdown","56ea9ee1":"markdown","eaab8f0e":"markdown","4dcdea67":"markdown","ea261e46":"markdown","d9b4f690":"markdown","01f15e57":"markdown","287d2ed7":"markdown","c52eaca9":"markdown","2a1b4e5e":"markdown","7713764c":"markdown","cf769735":"markdown","13108cc6":"markdown","70f5d23b":"markdown","6388218d":"markdown","ca615579":"markdown","39397c06":"markdown","97de5239":"markdown","6702a649":"markdown","b8370eb0":"markdown","0140a5f1":"markdown","31f24514":"markdown","0e15027d":"markdown","be3bf391":"markdown","9bc69c9a":"markdown","25496162":"markdown","aa710cbe":"markdown","7989a8bc":"markdown","bddb8568":"markdown","ebb3fc98":"markdown","66ec005c":"markdown","a0bfe575":"markdown","4f1e75a6":"markdown","e782e753":"markdown","68b6a016":"markdown","4063f113":"markdown","70419f1d":"markdown"},"source":{"16e06d51":"import pandas as pd\nimport numpy as np","ac31e3bb":"data = pd.read_csv(\"..\/input\/kc_house_data.csv\")","f3bd6629":"data.head()","dc9a73f0":"from treeinterpreter import treeinterpreter as ti\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","32f832fd":"rf = RandomForestRegressor()\ndt = DecisionTreeRegressor()","b5e06295":"y = data.iloc[:,2]\nx = data.loc[:, data.columns != 'price']","9fd8805d":"x = x.drop('date',1)\nx = x.drop('id', 1)","fb345d4f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","c7d526e3":"dt.fit(X_train, y_train)","865cdb47":"instances = X_test.loc[[735]]\ninstances","4ec1f100":"prediction, bias, contributions = ti.predict(dt, instances)","69d69668":"ft_list = []\nfor i in range(len(instances)):\n    #print(\"Instance\", i)\n    print(\"Bias (trainset mean)\", bias[i])\n    #print(\"Feature contributions:\")\n    for c, feature in sorted(zip(contributions[i], \n                                 x.columns), \n                             key=lambda x: -abs(x[0])):\n       ft_list.append((feature, round(c, 2)))\n    print(\"-\"*50)","9923ddc5":"labels, values = zip(*ft_list)","e92a4c9c":"ft_list","2cff8594":"import numpy as np                                                               \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 25, 25\n\nxs = np.arange(len(labels)) \n\nsns.barplot(xs, values)\n\n#plt.bar(xs, values, width, align='center')\n\nplt.xticks(xs, labels)\nplt.yticks(values)\n\nplt.show()","a8f24beb":"contributions","a7986f00":"prediction","ffeafac4":"bias","faf2ac6a":"print(bias + np.sum(contributions, axis=1))","7a320ece":"top50x = X_train.head(50)\ntop5x = X_train.head(5)\ntop50y = y_train.head(50)\ntop5y = y_train.head(5)","8bbef6ff":"dt1 = DecisionTreeRegressor()\ndt1.fit(top5x, top5y)","ba52d0b3":"from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \n#from sklearn.tree import export_graphviz\n#import pydotplus\n#dot_data = StringIO()\n#export_graphviz(dt1, out_file=dot_data,  \n#                filled=True, rounded=True,\n #               special_characters=True)\n#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n#Image(graph.create_png())","a06a85f3":"top5x","d0c202b5":"top5y","1467312f":"rf.fit(X_train, y_train)","729a28b6":"rf_prediction, rf_bias, rf_contributions = ti.predict(rf, instances)","b962474a":"rf_ft_list = []\nfor i in range(len(instances)):\n    print(\"Bias (trainset mean)\", rf_bias[i])\n    for c, feature in sorted(zip(rf_contributions[i], \n                                 x.columns), \n                             key=lambda x: -abs(x[0])):\n       rf_ft_list.append((feature, round(c, 2)))\n    print(\"-\"*50)","eabf7253":"rf_labels, rf_values = zip(*rf_ft_list)","fc6066e7":"rf_ft_list","73c3d89b":"import numpy as np                                                               \nimport matplotlib.pyplot as plt\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 25, 25\n\nrf_xs = np.arange(len(rf_labels)) \n\nplt.bar(rf_xs, rf_values, 0.8, align='center')\n\nplt.xticks(rf_xs, rf_labels)\nplt.yticks(rf_values)\n\nplt.show()","ff8ce40a":"rf_contributions","4103f408":"rf_prediction","15c3674c":"rf_bias","24aee8b3":"print(rf_bias + np.sum(rf_contributions, axis=1))","1eaeb301":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(n_estimators=10)","5a143913":"top5xrf = X_train.head(5)\ntop5yrf = y_train.head(5)","33cc6ee8":"rf_model.fit(top5xrf, top5yrf)","0f1bcee1":"estimator = rf_model.estimators_[5]\nestimator1 = rf_model.estimators_[6]","88506236":"from sklearn.externals.six import StringIO  \nfrom IPython.display import Image\n#from sklearn.tree import export_graphviz\n#import pydotplus\n#dot_data1 = StringIO()\n#export_graphviz(estimator, out_file=dot_data1,  \n #               filled=True, rounded=True,\n  #              special_characters=True)\n#graph = pydotplus.graph_from_dot_data(dot_data1.getvalue())  \n#Image(graph.create_png())","0073bf56":"from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \n#from sklearn.tree import export_graphviz\n#import pydotplus\n#dot_data3 = StringIO()\n#export_graphviz(estimator1, out_file=dot_data3,  \n #               filled=True, rounded=True,\n  #              special_characters=True)\n#graph = pydotplus.graph_from_dot_data(dot_data3.getvalue())  \n#Image(graph.create_png())","d5a4cbf7":"### An example of a learned decision tree for regression to help you make your decision is below:","1cf0300c":"### Please upvote if you find the kernel useful and share your thoughts or suggestions","35330c43":"### Extracting only a single tree to visualise","f0e72129":"![](https:\/\/www.saedsayad.com\/images\/Decision_tree_r1.png)","182b9192":"### Again, prediction must equal \" bias + feature(1)contribution + \u2026 + feature(n)contribution \"","56ea9ee1":"### As seen in the plot above, only 2 features have a positive impact in driving the prices higher.\n\n### The feature contributions are sorted by their absolute impact. We can see that in the instance the predicted \n### value is lower than the data set mean, and that latitude has a negative impact, square foot has a high positive impact meaning, higher the sqft. higher the price, which makes sense.","eaab8f0e":"### Therefore, prediction must equal","4dcdea67":"### As seen in the plot above, again only 2 features have a positive impact in driving the prices higher, but this time latitude has a very high negative impact, bringing the predictions much less than the bias(trainset mean).","ea261e46":"###  We start by looking at the decision tree which is the building block of the random forest.","d9b4f690":"### contributions of all features for instance 735 from test set","01f15e57":"***\n**OBJECTIVE:**\n   \n       To Understand Decision trees & random forest predictions using Treeinterpreter package and to understand \"how\" the prediction is arrived at for each observation in a dataset.\n   \n   \n       Note: 1) This package works only with scikit-learn modules.\n             \n             2) To install treeinterpreter using pip do - pip install treeinterpreter . Refer - https:\/\/github.com\/andosa\/treeinterpreter\n***","287d2ed7":"### Gini Index (Not Gini Impurity) - Difference? Check here - https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning#Gini_impurity\n\n\nGini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.\n\n### Steps to Calculate Gini for a split\n\n1) Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).\n\n2) Calculate Gini for split using weighted Gini score of each node of that split","c52eaca9":"### What do the above results mean?","2a1b4e5e":"### But the dataset we are looking at has a continuous output, so how does the tree split? \n\n***\nFor regression trees, they are chosen to minimize either\n    **1) Variance (Reduction in Variance approach)\n      2) MAE (mean absolute error) within all of the subsets.**\n***\n\n#### Sklearn by default uses the variance approach as the splitting criteria for regression.\n\n#### For more, refer - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html\n\n#### Which tree algorithm does scikit-learn use?  CART it is, more on that here - http:\/\/scikit-learn.org\/stable\/modules\/tree.html#tree-algorithms","7713764c":"### What does the random forest prediction tell us ? ","cf769735":"### Again, turning a black box into a white box for a random forest prediction","13108cc6":"### Although one image is not going to solve the issue, looking at an individual decision tree shows us that a random forest is not an unexplainable method, but a sequence of logical questions and answers and every prediction can be trivially presented as a sum of feature contributions, showing how the features lead to a particular prediction.\n\n### This opens up a lot of opportunities in practical machine learning tasks:","70f5d23b":"![NN](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRsBuN7tJaaVkmdDs0lHMwhUh3n24zjrvs_7fJe2CFkTkMNfyT1)","6388218d":"### From decision trees to forest\n\nWe started the kernel with decision trees, so how do we move from a decision tree to a forest? \n\nThis is straightforward, since the prediction of a forest is the average of the predictions of its trees, the prediction is simply the average of the bias terms plus the average contribution of each feature","ca615579":"![](https:\/\/images.pexels.com\/photos\/302804\/pexels-photo-302804.jpeg?auto=compress&cs=tinysrgb&h=750&w=1260)","39397c06":"### How did the random forest regressor arrive at the results? Lets look at the graph for the top 5 rows from train set","97de5239":"![](https:\/\/raw.githubusercontent.com\/rakash\/Scripts\/master\/tree.png)","6702a649":"![](https:\/\/github.com\/rakash\/Scripts\/blob\/master\/tree1.png?raw=true)","b8370eb0":"### Information Theory\n\nInformation theory is a measure to define this degree of disorganization in a system known as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50% \u2013 50%), it has entropy of one.\n\nEntropy can be calculated using formula:-\n                            \n                               Entropy = -plog(base2)p - qlog(base2)q\n                               \nHere p and q is probability of success and failure respectively in that node. Entropy is also used with categorical target variable. It chooses the split which has lowest entropy compared to parent node and other splits. The lesser the entropy, the better it is.\n\n### Steps to calculate entropy for a split:\n\n1) Calculate entropy of parent node\n\n2) Calculate entropy of each individual node of split and calculate weighted average of all sub-nodes available in split.","0140a5f1":"### Now lets go to a model that is an ensemble of decision trees.\n\n### Yes, i am talking about Random forest","31f24514":"### Turning a black box into a white box: decision paths using treeinterpreter","0e15027d":"### Prediction by Decision tree classifier","be3bf391":"### How does it work?\n\n1) Assume number of cases in the training set is N. Then, sample of these N cases is taken at random but with replacement. This sample will be the training set for growing the tree.\n\n2) If there are M input variables, a number m<M is specified such that at each node, m variables are selected at random out of the M. The best split on these m is used to split the node. The value of m is held constant while we grow the forest.\n\n#### The splitting criteria is similar to that of decisiontreeregressor in sklearn. for more parameter details, refer - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html\n\n3) Each tree is grown to the largest extent possible and  there is no pruning.\n\n4) Predict new data by aggregating the predictions of the ntree trees (i.e., majority votes for classification, average for regression).","9bc69c9a":"The random forest has been a burgeoning machine learning technique in the last few years. It is a non-linear tree-based model that often provides accurate results. However, being mostly black box, it is oftentimes hard to interpret and fully understand especially when it comes to explaining the results and rationale behind it to stakeholders in organizations.","25496162":"### Variance (Reduction in Variance approach):\n\nThis algorithm uses the standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split the population:\n\n#### Variance = Sum(X - X-bar)^2 \/ n\n\nAbove X-bar is mean of the values, X is actual and n is number of values.\n\n### Steps to calculate Variance:\n\n1) Calculate variance for each node.\n\n2) Calculate variance for each split as weighted average of each node variance.","aa710cbe":"![](http:\/\/dataaspirant.com\/wp-content\/uploads\/2017\/01\/B03905_05_01-compressor.png)","7989a8bc":"### Bias term","bddb8568":"### Now lets look at those concepts using the county house prices data set, which is a regression problem ","ebb3fc98":"***\nA Decision Tree is a tree (and a type of directed, acyclic graph) in which the nodes represent decisions (a square box), \nrandom transitions (a circular box) or terminal nodes, and the edges or branches are binary (yes\/no, true\/false) \nrepresenting possible paths from one node to another. The specific type of decision tree used for machine learning contains \nno random transitions. To use a decision tree for classification or regression, one grabs a row of data or a set of features \nand starts at the root, and then through each subsequent decision node to the terminal node. The process is very intuitive and\neasy to interpret, which allows trained decision trees to be used for variable selection or more generally, feature engineering.\n***\n***\nFor classification trees, the splits are chosen so as to \n    **1) minimize entropy or\n      2) Gini impurity in the resulting subsets.**\n***\n\n### An example of a learned decision tree for classification to help you make your decision is below:","66ec005c":"![](https:\/\/github.com\/rakash\/Scripts\/blob\/master\/tree2.png?raw=true)","a0bfe575":"### Another tree","4f1e75a6":"### How did the decision tree arrive at the results? Lets look at the graph for the top 5 rows","e782e753":"### How do decision trees work?","68b6a016":"***\n#### The TreeInterpreter library decomposes the predictions as the sum of contributions from each feature i.e.\n\n#### prediction = bias + feature(1)contribution + \u2026 + feature(n)contribution. \n***","4063f113":"#### REFERENCES:\n\n#### 1) https:\/\/github.com\/andosa\/treeinterpreter\n\n#### 2) http:\/\/blog.datadive.net\/interpreting-random-forests\/","70419f1d":"### Now lets look at the feature contributions"}}