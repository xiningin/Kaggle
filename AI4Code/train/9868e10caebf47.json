{"cell_type":{"c33a4708":"code","44cfe49c":"code","14b876db":"code","99709b3a":"code","99f2ea15":"code","df7b924c":"code","678c4063":"code","73bd5dbc":"code","7b7a1093":"code","3d172c13":"code","abd7dce4":"code","e29725f5":"code","ac878533":"code","0c4c6673":"code","ab00eb1b":"code","6251be00":"code","69f493db":"code","7f805af7":"code","e44c2e51":"code","65b757ae":"code","e82d4209":"code","9b94f504":"code","2fe53070":"code","5517ca16":"code","d48aefc8":"code","d6becbf1":"code","b776e6f7":"code","df388d58":"code","29c421ca":"code","62f51fb9":"code","e6aeb838":"code","2333db0b":"code","45da497e":"code","25df86ce":"code","dd6d7808":"code","4a2abb46":"code","7e1d1ac8":"code","bc6d0b88":"code","059a7612":"code","d6088d1d":"code","7d2b003f":"code","a24c8e4c":"markdown","69081a8e":"markdown","20c3bec9":"markdown","324a8a3a":"markdown","e5004d8f":"markdown","ca8b008d":"markdown","970d8f0c":"markdown","71c86701":"markdown"},"source":{"c33a4708":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()","44cfe49c":"df_1 = pd.read_csv('..\/input\/california-salaries-in-data-science\/title_location_company_salary.csv')\ndf_2 = pd.read_csv('..\/input\/california-salaries-in-data-science\/qualifications.csv')\ndf_3 = pd.read_csv('..\/input\/california-salaries-in-data-science\/benefits.csv')","14b876db":"df_1.info()","99709b3a":"df_1.head()","99f2ea15":"df_2.info()","df7b924c":"df_2.head()","678c4063":"df_3.info()","73bd5dbc":"df_3.head()","7b7a1093":"df_1 = df_1.drop('Unnamed: 0', axis=1)\ndf_2 = df_2.drop('Unnamed: 0', axis=1)\ndf_3 = df_3.drop('Unnamed: 0', axis=1)","3d172c13":"for i in df_2.columns:\n    df_2.loc[:,i] = df_2.loc[:,i].astype('Int64')\n    \nfor i in df_3.columns:\n    df_3.loc[:,i] = df_3.loc[:,i].astype('Int64')","abd7dce4":"pd.pivot_table(df_1, 'Salary', index=['Title'], columns=['Levels'])","e29725f5":"plt.figure(figsize=(15, 15))\nsns.kdeplot(data = df_1[df_1.Title=='Data Scientist'], x='Salary', hue='Levels', shade='fill')\nplt.show()","ac878533":"for i in df_1.Title.unique():\n    plt.figure(figsize=(10, 10))\n    sns.histplot(data = df_1[df_1.Title==i], x='Salary')\n    plt.xlabel('{} Salary'.format(i))\n    plt.show()","0c4c6673":"for i in df_1.Title.unique():\n    plt.figure(figsize=(10, 10))\n    plt.title('{}'.format(i))\n    sns.barplot(x = df_1[df_1.Title==i]['Levels'].value_counts().index, y = df_1[df_1.Title==i]['Levels'].value_counts().values)\n    plt.ylabel('Count')\n    plt.show()","ab00eb1b":"sorted_index_descent = df_1.groupby(['Title']).median().sort_values(by= 'Salary',ascending=False).index","6251be00":"plt.figure(figsize=(10, 10))\nsns.boxplot(y=df_1['Title'], x=df_1['Salary'],order=sorted_index_descent)\nplt.ylabel('')\nplt.title('Salaries for Data Science Positions')\nplt.show()","69f493db":"sorted_index_descent = df_1.groupby(['Levels']).median().sort_values(by= 'Salary',ascending=False).index","7f805af7":"plt.figure(figsize=(10, 10))\nsns.boxplot(y=df_1['Levels'], x=df_1['Salary'],order=sorted_index_descent)\nplt.ylabel('')\nplt.title('Salaries for Data Science Levels')\nplt.show()","e44c2e51":"for i in df_1.Title.unique():\n    qual_index = df_2.loc[df_1[df_1.Title == i].index, :].sum(axis=0).sort_values(ascending=False).index\n    qual_counts = df_2.loc[df_1[df_1.Title == i].index, :].sum(axis=0).sort_values(ascending=False).values\n    plt.figure(figsize=(10, 10))\n    sns.barplot(y=qual_index[0:10], x=qual_counts[0:10])\n    plt.title('Top 10 Qualifications: {}'.format(i))\n    plt.xlabel('Count')\n    plt.show()","65b757ae":"for i in df_1.Title.unique():\n    benefits_index = df_3.loc[df_1[df_1.Title == i].index, :].sum(axis=0).sort_values(ascending=False).index\n    benefits_counts = df_3.loc[df_1[df_1.Title == i].index, :].sum(axis=0).sort_values(ascending=False).values\n    plt.figure(figsize=(10, 10))\n    sns.barplot(y=benefits_index[0:10], x=benefits_counts[0:10])\n    plt.title('Top 10 Most Common Benefits: {}'.format(i))\n    plt.xlabel('Count')\n    plt.show()","e82d4209":"for i in df_1.Title.unique():\n    locations_index = df_1[df_1.Title == i]['Location'].value_counts().index\n    locations_counts = df_1[df_1.Title == i]['Location'].value_counts().values\n    plt.figure(figsize=(10, 10))\n    sns.barplot(y=locations_index[0:10], x=locations_counts[0:10])\n    plt.title('Top 10 Most Common Locations: {}'.format(i))\n    plt.xlabel('Count')\n    plt.show()","9b94f504":"for i in df_1.Title.unique():\n    company_index = df_1[df_1.Title == i]['Company'].value_counts().index\n    company_counts = df_1[df_1.Title == i]['Company'].value_counts().values\n    plt.figure(figsize=(10, 10))\n    sns.barplot(y=company_index[0:10], x=company_counts[0:10])\n    plt.title('Top 10 Most Common Companies: {}'.format(i))\n    plt.xlabel('Count')\n    plt.show()","2fe53070":"from sklearn.impute import SimpleImputer\nimp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ndf_1.loc[:,'Location'] = imp_most_frequent.fit_transform(df_1.loc[:,'Location'].values.reshape(-1,1))\n\ndf_1.info()","5517ca16":"knn_impute_salary = df_1[~pd.isna(df_1.Company)].loc[:,['Title', 'Company', 'Location', 'Salary']]\n\nno_na_salary = knn_impute_salary.loc[~pd.isna(knn_impute_salary.Salary),:]\n\nX = no_na_salary.copy()\ny = no_na_salary.pop('Salary')\nX = pd.get_dummies(X)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)\n\n\ndef reg_performance(regressor, model_name):\n    print(model_name)\n    print('Best Score: {} +\/- {}'.format(str(regressor.best_score_),str(regressor.cv_results_['std_test_score'][regressor.best_index_])))\n    print('Best Parameters: ' + str(regressor.best_params_))\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV\nknn = KNeighborsRegressor()\nparam_grid = { 'n_neighbors' : [1,2,3,4,5,7,9,11,13,15],\n               'weights' : ['uniform','distance']}\nreg_knn = GridSearchCV(knn, param_grid = param_grid, cv = 10, scoring='neg_mean_squared_error', n_jobs = -1)\nbest_reg_knn = reg_knn.fit(X_train,y_train)\nreg_performance(best_reg_knn,'kNeighborsRegressor')\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nknn = KNeighborsRegressor(n_neighbors= 2, weights= 'distance')\nknn.fit(X_train,y_train)\npred_knn = knn.predict(X_test)\nprint('KNeighborsRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,pred_knn)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,pred_knn))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,pred_knn)))\nprint('R-squared: {}'.format(r2_score(y_test,pred_knn)))","d48aefc8":"from sklearn.impute import KNNImputer\nknn_impute_salary = pd.get_dummies(knn_impute_salary)\nimputer = KNNImputer(n_neighbors=2, weights='distance')\nresult = pd.DataFrame(imputer.fit_transform(knn_impute_salary))\n\nimputed_salary = result.iloc[:,0]\n\nj = 0\nfor i in df_1[~pd.isna(df_1.Company)].index:\n    df_1.loc[i,'Salary'] = imputed_salary[j]\n    j+=1\n\nimp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\ndf_1.loc[:,['Location','Company']] = imp_most_frequent.fit_transform(df_1.loc[:,['Location','Company']])\n\ndf_1.info()","d6becbf1":"imputer_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ndf_2 = pd.DataFrame(imputer_most_frequent.fit_transform(df_2), columns = df_2.columns)\n\npd.isna(df_2).sum().sum()","b776e6f7":"df_modeling = pd.concat([df_1, df_2], axis=1)\ndf_modeling.shape\ndf_modeling.columns\n\ndf_no_drop_first = pd.DataFrame(pd.get_dummies(df_modeling.copy()))\n\ndf_modeling_drop_first = pd.DataFrame(pd.get_dummies(df_modeling.copy(), drop_first=True))\ndropped_columns = list(set(df_no_drop_first.columns) - set(df_modeling_drop_first.columns))","df388d58":"df_modeling_drop_first.info()","29c421ca":"df_modeling_drop_first.head()","62f51fb9":"dropped_columns","e6aeb838":"for i in df_modeling_drop_first.columns[1:]:\n    df_modeling_drop_first.loc[:,i] = df_modeling_drop_first.loc[:,i].astype('Int64')","2333db0b":"X_drop_first = df_modeling_drop_first.copy()\ny = X_drop_first.pop('Salary')","45da497e":"X_drop_first.shape","25df86ce":"import statsmodels.api as sm\n\nX_sm = sm.add_constant(X_drop_first)\nmodel = sm.OLS(y.astype(float),X_sm.astype(int))\nresults = model.fit()\nresults.summary()","dd6d7808":"from statsmodels.stats.multitest import multipletests\n\np_set = results.pvalues.values[1:]\n\nreject, p_corrected, alphac_sidak, alphac_bonf = multipletests(pvals=p_set,\n                                                               method='bonferroni',\n                                                               alpha=0.05, \n                                                               is_sorted=False,\n                                                               returnsorted=False)\n\nprint('Total number of hypothesis tests:', len(p_set))\nprint('Number of null hypotheses rejected at alpha = 0.05:', sum(results.pvalues.values < 0.05))\nprint('Number of null hypotheses rejected with adj. alpha with Bonferroni method:', sum(reject))\nprint('Adj. alpha value after FWER correction by Bonferroni method:', alphac_bonf)","4a2abb46":"print('Variables with a rejected null hypothesis using Bonferroni method:')\nfor i in results.pvalues.index[1:][reject]:\n    print(i)","7e1d1ac8":"order = results.pvalues.index[1:][reject][pd.DataFrame(results.params.values[1:][reject]).sort_values(by=0, ascending=False).index]","bc6d0b88":"dropped_columns","059a7612":"#modified from: https:\/\/stackoverflow.com\/a\/56780852\ndef show_values_on_bars(axs, h_v=\"v\", space=0.5):\n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() \/ 2\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_height())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                if p.get_x() + p.get_width() > 0:\n                    _x = p.get_x() + p.get_width() + float(space)\n                elif p.get_x() + p.get_width() < 0:\n                    _x = p.get_x() + p.get_width() - float(space)\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_width())\n                if p.get_x() + p.get_width() > 0:\n                    ax.text(_x, _y, value, ha=\"left\")\n                elif p.get_x() + p.get_width() < 0:\n                    ax.text(_x, _y, value, ha=\"right\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)","d6088d1d":"plt.figure(figsize=(20, 20))\nbp = sns.barplot(y=results.pvalues.index[1:][reject],x=results.params.values[1:][reject], order = order)\nbp.set_axisbelow(True)\nbp.yaxis.grid(color='gainsboro', linestyle='solid')\nplt.xlabel('Salary Difference (USD)')\nplt.title('Change in Salary Based on Title, Company, Location, and Qualificiations (Relative to Baselines)')\nshow_values_on_bars(bp, \"h\", 1)","7d2b003f":"company = 0\ntitle = 0\nlocation = 0\nlevels = 0\nqualifications = 0\nfor i in results.pvalues.index[1:][reject]:\n    if 'Company' in i:\n        company += 1\n    elif 'Title' in i:\n        title += 1\n    elif 'Location' in i:\n        location += 1\n    elif 'Levels' in i:\n        levels += 1\n    else:\n        qualifications += 1\n\nprint('Statistically significant predictors count')        \nprint('Company: {}'.format(company))\nprint('Qualifications: {}'.format(qualifications))\nprint('Title: {}'.format(title))\nprint('Location: {}'.format(location))\nprint('Levels: {}'.format(levels))","a24c8e4c":"With 986 predictors, using an alpha value of 0.05 will result in about 49 type I errors. Therefore, use a multiple testing technique to determine a more conservative alpha value to reduce the number of type I errors.","69081a8e":"# 1. Import libraries","20c3bec9":"# 7. OLS analysis","324a8a3a":"# 4. EDA","e5004d8f":"# 5. Imputation by SimpleImputer and KNNImputer","ca8b008d":"# 3. Data cleaning","970d8f0c":"# 6. OLS data preparation","71c86701":"# 2. Load data"}}