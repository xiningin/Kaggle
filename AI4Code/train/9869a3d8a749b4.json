{"cell_type":{"f768010e":"code","0ea13b50":"code","1bed33f9":"code","c1f54814":"code","1ce065fa":"code","a71dd0bd":"code","d56c5b73":"code","715b065b":"code","7e3b5e5a":"code","89ab45ee":"code","cd2a4774":"code","3242d1eb":"code","877a555d":"code","10712e83":"code","33dc2c9c":"code","e1b5ff8d":"code","a6b64937":"code","1e4c002c":"code","beb86028":"code","542cddb6":"markdown","b9fc1fc6":"markdown","4b089a0c":"markdown","13e19908":"markdown","affe75c7":"markdown","bf3cc95e":"markdown","cf0b7781":"markdown","ee57cb50":"markdown","facdab9e":"markdown","4f579843":"markdown","d7d1c240":"markdown","a7b61b64":"markdown"},"source":{"f768010e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0ea13b50":"train_data = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","1bed33f9":"train_data.head()","c1f54814":"train_data.info()","1ce065fa":"x_train = train_data[\"text\"]","a71dd0bd":"y_train = train_data[\"target\"]","d56c5b73":"'''have given needed library, module'''\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nimport re\nimport string\n\ndef process_tweet(tweet):\n\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n            word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean","715b065b":"'''example for created process_tweet() func.'''\ncustom_tweet = \"OMG house burned :( #bad #morning http:\/\/fire.com\"\n\n# print cleaned tweet\nprint(process_tweet(custom_tweet))","7e3b5e5a":"def lookup(freqs, word, label):\n    n = 0  # freqs.get((word, label), 0)\n\n    pair = (word, label)\n    if (pair in freqs):\n        n = freqs[pair]\n\n    return n","89ab45ee":"def count_tweets(result, tweets, ys):\n\n    for y, tweet in zip(ys, tweets):\n        for word in process_tweet(tweet):\n            # define the key, which is the word and label tuple\n            pair = (word, y)\n\n            # if the key exists in the dictionary, increment the count\n            if pair in result:\n                result[pair] += 1\n\n            # else, if the key is new, add it to the dictionary and set the count to 1\n            else:\n                result[pair] = 1\n    \n\n    return result","cd2a4774":"freqs = count_tweets({}, x_train, y_train)","3242d1eb":"def train_naive_bayes(freqs, train_x, train_y):\n    \n    loglikelihood = {}\n    logprior = 0\n\n   \n    # calculate V, the number of unique words in the vocabulary\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab)\n\n    # calculate N_real and N_notreal\n    N_real = N_notreal = 0\n    for pair in freqs.keys():\n        # if the label is real (greater than zero)\n        if pair[1] > 0:\n\n            # Increment the number of real words by the count for this (word, label) pair\n            N_real += freqs[pair]\n\n        # else, the label is negative\n        else:\n\n            # increment the number of not real words by the count for this (word,label) pair\n            N_notreal += freqs[pair]\n\n    # Calculate D, the number of documents\n    D = len(train_x)\n\n    # Calculate D_pos, the number of real disaster\n    D_real = sum(train_y==1)\n\n    # Calculate D_notreal, the number of not real disaster \n    D_notreal = sum(train_y==0)\n\n    # Calculate logprior\n    logprior = np.log(D_real)- np.log(D_notreal)\n\n    # For each word in the vocabulary...\n    for word in vocab:\n        # get the real and not real frequency of the word\n        freq_real = lookup(freqs,word,1)\n        freq_notreal = lookup(freqs,word,0)\n\n        # calculate the probability that each word is real and not real\n        p_w_real = (freq_real+1)\/(N_real+V)\n        p_w_notreal = (freq_notreal+1)\/(N_notreal+V)\n\n        # calculate the log likelihood of the word\n        loglikelihood[word] = np.log(p_w_real)-np.log(p_w_notreal)\n\n\n    return logprior, loglikelihood","877a555d":"logprior, loglikelihood = train_naive_bayes(freqs, x_train, y_train)\nprint(logprior)\nprint(len(loglikelihood))","10712e83":"def naive_bayes_predict(tweet, logprior, loglikelihood):\n   \n    # process the tweet to get a list of words\n    word_l = process_tweet(tweet)\n\n    # initialize probability to zero\n    p = 0\n\n    # add the logprior\n    p += logprior\n\n    for word in word_l:\n\n        # check if the word exists in the loglikelihood dictionary\n        if word in loglikelihood:\n            # add the log likelihood of that word to the probability\n            p += loglikelihood[word]\n\n    return p","33dc2c9c":"my_tweet = \"@sakuma_en If you pretend to feel a certain way the feeling can become genuine all by accident. -Hei...\"\np = naive_bayes_predict(my_tweet, logprior, loglikelihood)\nprint('The expected output is', p)","e1b5ff8d":"def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n\n    accuracy = 0  # return this properly\n\n    y_hats = []\n    for tweet in test_x:\n        # if the prediction is > 0\n        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n            # the predicted class is 1\n            y_hat_i = 1\n        else:\n            # otherwise the predicted class is 0\n            y_hat_i = 0\n\n        # append the predicted class to the list y_hats\n        y_hats.append(y_hat_i)\n\n    # error is the average of the absolute values of the differences between y_hats and test_y\n    error = np.mean(np.absolute(y_hats-test_y))\n\n    # Accuracy is 1 minus the error\n    accuracy = 1-error\n\n    return accuracy","a6b64937":"test_data = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\ny_predicted = []\n\nfor data in test_data[\"text\"]:\n    result = naive_bayes_predict(data, logprior, loglikelihood)\n    if(result>=0):\n        y_predicted.append(1)\n    if(result<0):\n        y_predicted.append(0)\n        \ny_predicted","1e4c002c":"iyd = test_data[\"id\"]\ntrgt = y_predicted\n\ndict = {'id': iyd, 'target': trgt} \ndf = pd.DataFrame(dict) \ndf.head()\n\n","beb86028":"df.to_csv(\"submission.csv\", index=False)","542cddb6":"4-I will implement a lookup() helper function that takes in the freqs dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets. To help train your naive bayes model, you will need to build a dictionary where the keys are a (word, label) tuple and the values are the corresponding frequency.","b9fc1fc6":"2-Missing values and column name that are keyword and location are removed train data, i will do only text analys.","4b089a0c":"#### Prior and Logprior\n\nThe prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n\nThe prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\nWe can take the log of the prior to rescale it, and we'll call this the logprior\n\n$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n\nNote that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n\n$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$","13e19908":"1-readed train data here","affe75c7":"7-Created code given mathematical instructions. this function train data and give us logprior and loglikelihood.","bf3cc95e":"#### Log likelihood\nTo compute the loglikelihood of that very same word, we can implement the following equations:\n\n$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$","cf0b7781":"6-Some information about Naive Bayes:\n\nNaive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n\n#### So how do you train a Naive Bayes classifier?\n- The first part of training a naive bayes classifier is to identify the number of classes that you have.\n- You will create a probability for each class.\n$P(D_{pos})$ is the probability that the document is positive.\n$P(D_{neg})$ is the probability that the document is negative.\nUse the formulas as follows and store the values in a dictionary:\n\n$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n\n$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n\nWhere $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets.","ee57cb50":"9-test accuracy","facdab9e":"8-Now that we have the logprior and loglikelihood, we can test the naive bayes function by making predicting on some tweets!\n\nImplement the `naive_bayes_predict` function to make predictions on tweets.\n* The function takes in the `tweet`, `logprior`, `loglikelihood`.\n* It returns the probability that the tweet belongs to the positive or negative class.\n* For each tweet, sum up loglikelihoods of each word in the tweet.\n* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n\n$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n","4f579843":"3-Process the data\nFor any machine learning project, once you've gathered the data, the first step is to process it to make useful inputs to your model\n- **Remove noise**: You will first want to remove noise from your data -- that is, remove words that don't tell you much about the content. These include all common words like 'I, you, are, is, etc...' that would not give us enough information on the sentiment.\n- We'll also remove stock market tickers, retweet symbols, hyperlinks, and hashtags because they can not tell you a lot of information on the sentiment.\n- You also want to remove all the punctuation from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word, instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" as different words.\n- Finally you want to use stemming to only keep track of one variation of each word. In other words, we'll treat \"motivation\", \"motivated\", and \"motivate\" similarly by grouping them within the same stem of \"motiv-\".\n\nI create function that is name 'process_tweet()'\n","d7d1c240":"5-I create a function count_tweets() that takes a list of tweets as input, cleans all of them, and returns a dictionary.","a7b61b64":"To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n\n- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n\nWe'll use these to compute the positive and negative probability for a specific word using this formula:\n\n$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n\nNotice that we add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https:\/\/en.wikipedia.org\/wiki\/Additive_smoothing) explains more about additive smoothing."}}