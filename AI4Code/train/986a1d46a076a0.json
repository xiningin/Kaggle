{"cell_type":{"18843ec6":"code","b707d799":"code","a0693037":"code","8e7dbf57":"code","583a4429":"code","30209528":"code","57859d51":"code","c78f1640":"code","1768f040":"code","a54153a2":"code","251e1a0d":"code","924ef4b5":"code","079bf65d":"code","47275c0f":"code","5feffe67":"code","19d7d297":"code","0b042ca7":"code","b10038d3":"markdown","b14063ba":"markdown","5a133cda":"markdown","701eb942":"markdown","81be0298":"markdown","bc6b9548":"markdown","5e43befe":"markdown","6975efe5":"markdown","8003ca01":"markdown","c8859a0f":"markdown","0dc44f21":"markdown","5b4004dc":"markdown","eb23ee47":"markdown","84a36bd0":"markdown","8e1a582c":"markdown"},"source":{"18843ec6":"import tensorflow.compat.v1 as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nfrom scipy.io import loadmat\nimport os\nfrom pywt import wavedec\nfrom functools import reduce\nfrom scipy import signal\nfrom scipy.stats import entropy\nfrom scipy.fft import fft, ifft\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom tensorflow import keras as K\nimport matplotlib.pyplot as plt\nimport scipy\nimport tqdm\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold,cross_validate\nfrom tensorflow.keras.layers import Dense, Activation, Flatten,Embedding, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.optimizers import RMSprop\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.models import Sequential, Model, load_model\nimport matplotlib.pyplot as plt;\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.decomposition import PCA\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv1D,Conv2D,Add\nfrom tensorflow.keras.layers import MaxPool1D, MaxPooling2D\nimport seaborn as sns\nimport sklearn","b707d799":"train_data = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')","a0693037":"train_data","8e7dbf57":"test_data","583a4429":"train_data.describe()","30209528":"test_data.describe()","57859d51":"train_data.pop('id')\ntest_data.pop('id')\ny = train_data.pop('claim')","c78f1640":"train_data.isna().sum() , test_data.isna().sum()","1768f040":"features = [x for x in train_data.columns.values if x[0]==\"f\"]\n\ntrain_data['n_missing'] = train_data[features].isna().sum(axis = 1)\ntest_data['n_missing'] = test_data[features].isna().sum(axis = 1)\n\ntrain_data['std'] = train_data[features].std(axis = 1)\ntest_data['std'] = test_data[features].std(axis = 1)\n\ntrain_data['var'] = train_data[features].var(axis = 1)\ntest_data['var'] = test_data[features].var(axis = 1)\n\ntrain_data['mean'] = train_data[features].mean(axis = 1)\ntest_data['mean'] = test_data[features].mean(axis = 1)\n\ntrain_data['median'] = train_data[features].median(axis = 1)\ntest_data['median'] = test_data[features].median(axis = 1)\n\ntrain_data['rms'] = (train_data.iloc[:,1:]**2).sum(1).pow(1\/2)\ntest_data['rms'] = (test_data.iloc[:,1:]**2).sum(1).pow(1\/2)\n\ntrain_data['abs_sum'] = train_data[features].abs().sum(axis = 1)\ntest_data['abs_sum'] = test_data[features].abs().sum(axis = 1)\n\ntrain_data['max'] = train_data[features].max(axis = 1)\ntest_data['max'] = test_data[features].max(axis = 1)\n\ntrain_data['min'] = train_data[features].min(axis = 1)\ntest_data['min'] = test_data[features].min(axis = 1)\n\nfeatures += ['n_missing', 'std', 'var','mean','median','rms','abs_sum','max','min']\n\n","a54153a2":"sc = RobustScaler()\ntrain_data[features] = sc.fit_transform(train_data[features])\ntest_data[features] = sc.transform(test_data[features])","251e1a0d":"fill_value_dict = {\n    'f1': 'Mean', \n    'f2': 'Median', \n    'f3': 'Median', \n    'f4': 'Median', \n    'f5': 'Mode', \n    'f6': 'Mean', \n    'f7': 'Median', \n    'f8': 'Median', \n    'f9': 'Median', \n    'f10': 'Median', \n    'f11': 'Mean', \n    'f12': 'Median', \n    'f13': 'Mean', \n    'f14': 'Median', \n    'f15': 'Mean', \n    'f16': 'Median', \n    'f17': 'Median', \n    'f18': 'Median', \n    'f19': 'Median', \n    'f20': 'Median', \n    'f21': 'Median', \n    'f22': 'Mean', \n    'f23': 'Mode', \n    'f24': 'Median', \n    'f25': 'Median', \n    'f26': 'Median', \n    'f27': 'Median', \n    'f28': 'Median', \n    'f29': 'Mode', \n    'f30': 'Median', \n    'f31': 'Median', \n    'f32': 'Median', \n    'f33': 'Median', \n    'f34': 'Mean', \n    'f35': 'Median', \n    'f36': 'Mean', \n    'f37': 'Median', \n    'f38': 'Median', \n    'f39': 'Median', \n    'f40': 'Mode', \n    'f41': 'Median', \n    'f42': 'Mode', \n    'f43': 'Mean', \n    'f44': 'Median', \n    'f45': 'Median', \n    'f46': 'Mean', \n    'f47': 'Mode', \n    'f48': 'Mean', \n    'f49': 'Mode', \n    'f50': 'Mode', \n    'f51': 'Median', \n    'f52': 'Median', \n    'f53': 'Median', \n    'f54': 'Mean', \n    'f55': 'Mean', \n    'f56': 'Mode', \n    'f57': 'Mean', \n    'f58': 'Median', \n    'f59': 'Median', \n    'f60': 'Median', \n    'f61': 'Median', \n    'f62': 'Median', \n    'f63': 'Median', \n    'f64': 'Median', \n    'f65': 'Mode', \n    'f66': 'Median', \n    'f67': 'Median', \n    'f68': 'Median', \n    'f69': 'Mean', \n    'f70': 'Mode', \n    'f71': 'Median', \n    'f72': 'Median', \n    'f73': 'Median', \n    'f74': 'Mode', \n    'f75': 'Mode', \n    'f76': 'Mean', \n    'f77': 'Mode', \n    'f78': 'Median', \n    'f79': 'Mean', \n    'f80': 'Median', \n    'f81': 'Mode', \n    'f82': 'Median', \n    'f83': 'Mode', \n    'f84': 'Median', \n    'f85': 'Median', \n    'f86': 'Median', \n    'f87': 'Median', \n    'f88': 'Median', \n    'f89': 'Median', \n    'f90': 'Mean', \n    'f91': 'Mode', \n    'f92': 'Median', \n    'f93': 'Median', \n    'f94': 'Median', \n    'f95': 'Median', \n    'f96': 'Median', \n    'f97': 'Mean', \n    'f98': 'Median', \n    'f99': 'Median', \n    'f100': 'Mode', \n    'f101': 'Median', \n    'f102': 'Median', \n    'f103': 'Median', \n    'f104': 'Median', \n    'f105': 'Median', \n    'f106': 'Median', \n    'f107': 'Median', \n    'f108': 'Median', \n    'f109': 'Mode', \n    'f110': 'Median', \n    'f111': 'Median', \n    'f112': 'Median', \n    'f113': 'Mean', \n    'f114': 'Median', \n    'f115': 'Median', \n    'f116': 'Mode', \n    'f117': 'Median', \n    'f118': 'Mean'\n}\n\n\nfor col in (features):\n    if fill_value_dict.get(col)=='Mean':\n        fill_value = train_data[col].mean()\n    elif fill_value_dict.get(col)=='Median':\n        fill_value = train_data[col].median()\n    elif fill_value_dict.get(col)=='Mode':\n        fill_value = train_data[col].mode().iloc[0]\n    \n    train_data[col].fillna(fill_value, inplace=True)\n    test_data[col].fillna(fill_value, inplace=True)","924ef4b5":"#from scipy import stats\n#train_data = train_data[(np.abs(stats.zscore(train_data)) < 3).all(axis=1)]","079bf65d":"train_data","47275c0f":"x = pd.DataFrame(train_data)\ny = pd.Series(y)","5feffe67":"\"\"\"\nimport optuna\nfrom lightgbm import LGBMClassifier\ndef objective(trial, data = x, target = y):\n\n    \n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 1000, 40000),\n        'max_depth': trial.suggest_int('max_depth', 2, 3),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 50, 500),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 200),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'random_state': 228,\n        'metric': 'auc',\n        'device_type': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n    }\n    \n    model = LGBMClassifier(**params)\n    scores = []\n    k = StratifiedKFold(n_splits = 2, random_state = 228, shuffle = True)\n    for i, (trn_idx, val_idx) in enumerate(k.split(x, y)):\n        \n        X_train, X_val = x.iloc[trn_idx], x.iloc[val_idx]\n        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n        model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 300, verbose = False)\n        \n        tr_preds = model.predict_proba(X_train)[:,1]\n        tr_score = sklearn.metrics.roc_auc_score(y_train, tr_preds)\n        \n        val_preds = model.predict_proba(X_val)[:,1]\n        val_score = sklearn.metrics.roc_auc_score(y_val, val_preds)\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i+1} | AUC: {val_score}\")\n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 10)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)\n\n\"\"\"","19d7d297":"from lightgbm import LGBMClassifier\n\nSEED = 228\nparamsLGBM = {'objective': 'binary',\n               'boosting_type': 'gbdt',\n               'num_leaves': 6,\n               'max_depth': 2,\n               'n_estimators': 40000,\n               'reg_alpha': 25.0,\n               'reg_lambda': 76.7,\n               'random_state': SEED,\n               'bagging_seed': SEED, \n               'feature_fraction_seed': SEED,\n               'n_jobs': -1,\n               'subsample': 0.98,\n               'subsample_freq': 1,\n               'colsample_bytree': 0.69,\n               'min_child_samples': 54,\n               'min_child_weight': 256,\n               'learning_rate': 0.2,\n               'metric': 'AUC',\n               'verbosity': -1,\n              }\n\nfolds = StratifiedKFold(n_splits = 5, random_state = SEED, shuffle = True)\ny_pred = np.zeros(len(test_data))\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(x, y)):\n    \n    X_train, X_val = x.iloc[trn_idx], x.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = LGBMClassifier(**paramsLGBM)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 300)\n    \n    y_pred += model.predict_proba(test_data)[:,1] \/ folds.n_splits ","0b042ca7":"submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsubmission['claim'] = y_pred\nsubmission.to_csv('submission.csv',index = False)","b10038d3":"## Reading the train and test datasets.","b14063ba":"1. Feature Engineering: We construct new features with summary statistics like Mean, Variance, Standard Deviation (SD) and additional features like n_missing denoting the number of missing values along each row.\n\n2. Imputation: We use median imputation for filling in the null values. We create a dictionary to decide how to impute each column.\n\n    Mean: normal distribution\n\n    Median: unimodal and skewed \n\n    Mode: all other cases\n\n3. Normalization\/ Standardization: We observe that the values in each column have different scales, hence we perform RobustScaling to avoid effect of outliers. ","5a133cda":"From the count rows in the tables above, we see that columns like \"Id\" are unique, hence wont contribute for our predictions. Hence we remove them.","701eb942":"## Submitting predictions.","81be0298":"## Dividing the dependent and independent variables.","bc6b9548":"## Importing necessary libraries","5e43befe":"#### Please upvote if you liked it :)","6975efe5":"## Hyperparameter tuning of LGBM Classifier.","8003ca01":"## Performing 10 fold cross validation using LGBMClassifier.","c8859a0f":"We see that each column has multiple missing values, hence we use Imputation to fill the null values instead of dropping those rows which would lead to loss of information.\n","0dc44f21":"## Feature engineering, Median imputation and Z-score normalization.","5b4004dc":"#### With this, we end up with a score of 0.81793 on the leaderboard. There is definitely room for improvement.","eb23ee47":"## Removing outliers using Z-score for data points.","84a36bd0":"## Dealing with missing values.","8e1a582c":"## Deleting unnecessary columns."}}