{"cell_type":{"49327032":"code","aae97f83":"code","10bcfe1b":"code","48193bff":"code","7fe0817e":"code","850f240d":"code","7f8bd0d3":"code","bb4eff46":"code","dbdfb4b6":"code","f092cee2":"code","00fe3e3c":"code","f16115e9":"code","455d9026":"code","a55e1fb1":"code","07bd945a":"code","7045697e":"code","67548f40":"code","db158ea2":"code","3fcecdb4":"code","04ddf81d":"code","6728b35b":"code","7f49b9d2":"code","6b5cb558":"code","0f39f1c1":"code","909ce219":"code","d3f7c19a":"code","d66de499":"code","8228e8b6":"code","df351308":"code","d957bca6":"code","e5771ae2":"markdown","a5272de3":"markdown","a3d41f07":"markdown","6035454f":"markdown","012298da":"markdown","52d27e8d":"markdown","825c440a":"markdown","c0b9c013":"markdown","c1f8ab4d":"markdown","90e14919":"markdown","862f72d5":"markdown","a7b39067":"markdown","420be7b9":"markdown","48d1e7e8":"markdown","d318bf00":"markdown","05391518":"markdown","7010f71b":"markdown"},"source":{"49327032":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score,recall_score, roc_auc_score,roc_curve, plot_roc_curve\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', 500)","aae97f83":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","10bcfe1b":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","48193bff":"def check_df(dataframe, head=5, tail=3):\n    print(\"######################## Shape ########################\\n\")\n    print(dataframe.shape)\n    print(\"######################## Types ########################\\n\")\n    print(dataframe.dtypes)\n    print(\"######################## Head ########################\\n\")\n    print(dataframe.head(head))\n    print(\"######################## Tail ########################\\n\")\n    print(dataframe.tail(tail))\n    print(\"######################## NA ########################\\n\")\n    print(dataframe.isnull().sum())\n    print(\"\\n\\n######################## Quantiles ########################\\n\")\n    print(dataframe.describe([0, 0.05, 0.25, 0.50, 0.95, 0.99]).T)\n\ncheck_df(df)","7fe0817e":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    \"\"\"\n\n    Veri setindeki kategorik, numerik ve kategorik fakat kardinal de\u011fi\u015fkenlerin isimlerini verir.\n    Not: Kategorik de\u011fi\u015fkenlerin i\u00e7erisine numerik g\u00f6r\u00fcn\u00fcml\u00fc kategorik de\u011fi\u015fkenler de dahildir.\n\n    Parameters\n    ------\n        dataframe: dataframe\n                De\u011fi\u015fken isimleri al\u0131nmak istenilen dataframe\n        cat_th: int, optional\n                numerik fakat kategorik olan de\u011fi\u015fkenler i\u00e7in s\u0131n\u0131f e\u015fik de\u011feri\n        car_th: int, optional\n                kategorik fakat kardinal de\u011fi\u015fkenler i\u00e7in s\u0131n\u0131f e\u015fik de\u011feri\n\n    Returns\n    ------\n        cat_cols: list\n                Kategorik de\u011fi\u015fken listesi\n        num_cols: list\n                Numerik de\u011fi\u015fken listesi\n        cat_but_car: list\n                Kategorik g\u00f6r\u00fcn\u00fcml\u00fc kardinal de\u011fi\u015fken listesi\n\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n\n\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = toplam de\u011fi\u015fken say\u0131s\u0131\n        num_but_cat cat_cols'un i\u00e7erisinde.\n\n    \"\"\"\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","850f240d":"num_cols = [col for col in num_cols if col != \"PassengerId\"]\ncat_cols = [col for col in cat_cols if col != \"Survived\"]","7f8bd0d3":"# Categoric Columns\n\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\nfor col in cat_cols:\n    cat_summary(df, col)","bb4eff46":"def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","dbdfb4b6":"def check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\nfor col in num_cols:\n    print(col, check_outlier(df, col))","f092cee2":"# Replace the outliers with the low and up limits on Fare Column.\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n    \nreplace_with_thresholds(df, \"Fare\")","00fe3e3c":"# Check the missing values and fill them:\n\ndf.isnull().sum()","f16115e9":"# categoric (\"Embarked\") filled with mode:\ndf = df.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= 10) else x, axis=0)\n\n# numeric (\"Age\") filled with median of age of gender\ndf[\"Age\"] = df[\"Age\"].fillna(df.groupby(\"Sex\")[\"Age\"].transform(\"median\"))","455d9026":"# Creating some new features:\n\n# cabin bool\ndf[\"New_Cabin_Bool\"] = df[\"Cabin\"].notnull().astype('int')\n\n# name count\ndf[\"New_Name_Count\"] = df[\"Name\"].str.len()\n\n# name word count\ndf[\"New_Name_Word_Count\"] = df[\"Name\"].apply(lambda x: len(str(x).split(\" \")))\n\n# name dr\ndf[\"New_Name_Dr\"] = df[\"Name\"].apply(lambda x: len([x for x in x.split() if x.startswith(\"Dr\")]))\n\n# name title\ndf['New_Title'] = df.Name.str.extract(' ([A-Za-z]+)\\. ', expand=False)\n\n# family size\ndf[\"New_Family_Size\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n\n# age_pclass\ndf[\"New_Age_Pclass\"] = df[\"Age\"] * df[\"Pclass\"]\n\n# is alone\ndf.loc[((df['SibSp'] + df['Parch']) > 0), \"New_Is_Alone\"] = \"NO\"\ndf.loc[((df['SibSp'] + df['Parch']) == 0), \"New_Is_Alone\"] = \"YES\"\n\n# age level\ndf.loc[(df['Age'] < 18), 'New_Age_Cat'] = 'young'\ndf.loc[(df['Age'] >= 18) & (df['Age'] < 56), 'New_Age_Cat'] = 'mature'\ndf.loc[(df['Age'] >= 56), 'New_Age_Cat'] = 'senior'\n\n# sex x age\ndf.loc[(df['Sex'] == 'male') & (df['Age'] <= 21), 'New_Sex_Cat'] = 'youngmale'\ndf.loc[(df['Sex'] == 'male') & ((df['Age'] > 21) & (df['Age']) <= 50), 'New_Sex_Cat'] = 'maturemale'\ndf.loc[(df['Sex'] == 'male') & (df['Age'] > 50), 'New_Sex_Cat'] = 'seniormale'\ndf.loc[(df['Sex'] == 'female') & (df['Age'] <= 21), 'New_Sex_Cat'] = 'youngfemale'\ndf.loc[(df['Sex'] == 'female') & ((df['Age'] > 21) & (df['Age']) <= 50), 'New_Sex_Cat'] = 'maturefemale'\ndf.loc[(df['Sex'] == 'female') & (df['Age'] > 50), 'New_Sex_Cat'] = 'seniorfemale'","a55e1fb1":"# Describe the columns again.\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\nnum_cols = [col for col in num_cols if \"PassengerId\" not in col]","07bd945a":"# remove columns that we don't need.\nremove_cols = [\"Ticket\", \"Name\"]\ndf.drop(remove_cols, inplace=True, axis=1)","7045697e":"# checking the rare values and remove them from dataset:\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")","67548f40":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\nnum_cols = [col for col in num_cols if \"PassengerId\" not in col]\nrare_analyser(df, \"Survived\", cat_cols)","db158ea2":"# If the ratio above under 1% and if they more than 1, we can merge them together and call as rare class. \n\ndef rare_encoder(dataframe, rare_perc, cat_cols):\n    rare_columns = [col for col in cat_cols if (dataframe[col].value_counts() \/ len(dataframe) < 0.01).sum() > 1]\n\n    for col in rare_columns:\n        tmp = dataframe[col].value_counts() \/ len(dataframe)\n        rare_labels = tmp[tmp < rare_perc].index\n        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n\n    return dataframe\n\ndf = rare_encoder(df, 0.01, cat_cols)","3fcecdb4":"# Let's check again the dataset with rare analyser:\n\nrare_analyser(df, \"Survived\", cat_cols)","04ddf81d":"# We need an encoding for the categoric columns before the modelling and use 'One Hot Encoding'\n# Take the categoric columns with between 2 and 10 values, so we do also label encoding (binary columns)\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\nohe_cols = [col for col in df.columns if 10 >= df[col].nunique() >= 2 and col not in \"Survived\"]\ndf = one_hot_encoder(df, ohe_cols, True)","6728b35b":"df.shape\n\n# Now, 45 columns we have.","7f49b9d2":"# One more! We can remove the useless columns.\n# If the ratio less than 1%, we can remove it.\n\nuseless_cols = [col for col in df.columns if df[col].nunique() == 2 and\n                (df[col].value_counts() \/ len(df) < 0.01).any(axis=None)]\ndf.drop(useless_cols, axis=1, inplace=True)","6b5cb558":"df.shape\n\n# Just one is gone...","0f39f1c1":"# Okay, okay. Last move.\n\n# standard scaler\nnum_cols = [col for col in num_cols if \"PassengerId\" not in col]\nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\ndf.head()","909ce219":"# Model\n\ny = df[\"Survived\"]\nX = df.drop([\"PassengerId\", \"Survived\", \"Cabin\"], axis=1)\n\nlog_model = LogisticRegression().fit(X, y)\ny_pred = log_model.predict(X)","d3f7c19a":"print(classification_report(y, y_pred))","d66de499":"# Confusion Matrix of dataset:\n\ndef plot_confusion_matrix(y, y_pred):\n    acc = round(accuracy_score(y, y_pred), 2)\n    cm = confusion_matrix(y, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\".0f\")\n    plt.xlabel('y_pred')\n    plt.ylabel('y')\n    plt.title('Accuracy Score: {0}'.format(acc), size=10)\n    plt.show()\n\nplot_confusion_matrix(y, y_pred)","8228e8b6":"y_prob = log_model.predict_proba(X)[:, 1]\nprint(\"AUC Score: \" + str(roc_auc_score(y, y_prob)))","df351308":"# Model Validation with K-Fold Cross Validate\n\n# 80% Train, 20% Test of Dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)\n\n# Train Set (cross validation: 5)\nlog_model = LogisticRegression().fit(X_train, y_train)\ncv_results = cross_validate(log_model,\n                            X, y,\n                            cv=5,\n                            scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"])\n\n# Test Set\ny_pred = log_model.predict(X_test)\n\ny_prob = log_model.predict_proba(X_test)[:, 1]\n\n# Classification report\nprint(\"Test Accuracy: \" + str(cv_results['test_accuracy'].mean()))\nprint(\"############################################\", end=\"\\n\\n\")\nprint(\"Test Precision: \" + str(cv_results['test_precision'].mean()))\nprint(\"############################################\", end=\"\\n\\n\")\nprint(\"Recall: \" + str(cv_results['test_recall'].mean()))\nprint(\"############################################\", end=\"\\n\\n\")\nprint(\"F1 Score: \" + str(cv_results['test_f1'].mean()))\nprint(\"############################################\", end=\"\\n\\n\")","d957bca6":"# ROC Curve and AUC Score of Test Set\n\nplot_roc_curve(log_model, X_test, y_test)\nplt.title('ROC Curve')\nplt.plot([0, 1], [0, 1], 'r--')\nplt.show()\n\nroc_auc_score(y_test, y_prob)","e5771ae2":"<hr style=\"height:2px;border-width:0;color:white;background-color:green\">","a5272de3":"##### AUC Score: AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example.<br> It measures the entire two-dimensional area under the entire ROC curve from (0,0) to (1,1). The best value is 1, the worst value is 0.5 (We will check the graphic later).","a3d41f07":"# Conclusion","6035454f":"# Modelling","012298da":"# Data Preprocessing","52d27e8d":"# Dataset\n\n#### *Dataset: It is Titanic!*\n#### *Port of Registry: Liverpool, UK*\n#### *Route: Southampton to New York City.*\n#### *Description: The Dataset shows us which passengers survived.*<br>\n\n##### *Columns of Dataset:*\n\n##### *1. Survived: if the passenger survived or not (1, 0)*\n##### *2. Pclass: Ticket class*\n##### *3. Sex: Gender of passengers*\n##### *4. Age: Age of passengers*\n##### *5. Sibsp: Sieblings in Titanic*\n##### *6. Parch: Parents\/Children in Titanic*\n##### *7. Embarked: Port of embarkation*\n##### *8. Fare: Ticket price*\n##### *9. Cabin: Number of cabin*","825c440a":"# Confusion Matrix","c0b9c013":"<hr style=\"height:2px;border-width:0;color:white;background-color:green\">","c1f8ab4d":"#### What do these mean? What is F1 Score or Recall or Accuracy?\n#### So, first we should understand the confusion matrix and the scores:<br>\n##### Accuracy: Success rate of the prediction\n##### Precision: It shows how many of those predicted as positive are successful.\n##### Recall: It shows how many of the values we should have predicted positively.\n##### F1 Score: It is a harmonical mean of Precision and Recall.\n\n![confusion matrix.jpg](attachment:342d2690-ac0e-41ad-8040-a8dd33c15564.jpg)","90e14919":"#### Modelling with Logistic Regression\n\n##### *Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist.*","862f72d5":"<hr style=\"height:2px;border-width:0;color:white;background-color:green\">","a7b39067":"<hr style=\"height:2px;border-width:0;color:white;background-color:green\">","420be7b9":"# Model Validation","48d1e7e8":"<hr style=\"height:2px;border-width:0;color:white;background-color:green\">","d318bf00":"# Import Dataset & Libraries","05391518":"<hr style=\"height:2px;border-width:0;color:white;background-color:green\">","7010f71b":"#### *We did a model with all dataset and got the scores.<br> And also we did a model with test(20%), train(80%) split. That means we take a 80% of dataset and we train our model and then with 20% of dataset (our model never see that part of dataset) we test our model.<br> One last thing; with K-Fold cross validation we schuffled and select dataset randomly and we take all of scores as mean.<br> If we compare the scores, first model has more than last one and it is actually predictable, but it shows us that the scores with K-Fold cross validation are more confidence than without cross validation.*"}}