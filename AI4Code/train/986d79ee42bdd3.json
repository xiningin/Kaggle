{"cell_type":{"a44d22e1":"code","ecf5fa0d":"code","efff07f0":"code","3099ae0b":"code","cb314684":"code","e3868f03":"code","1ffe7cfe":"code","eab0ec2d":"code","97217751":"code","001addfb":"code","dcef5a68":"code","f2895cec":"code","1959d454":"code","1aaef0ba":"code","b38073c4":"code","66bd0b5b":"code","4327aa06":"code","30284088":"code","6b4f4025":"code","914bee39":"code","cbb8e00b":"code","e591d90d":"code","20363067":"code","f0624b7b":"code","328947a6":"code","6ed4b29e":"code","b5891b0f":"code","da918ac1":"code","c2533f45":"code","5fb45770":"markdown","3e37ff76":"markdown","603a8e9e":"markdown","086ab4b2":"markdown","df140b47":"markdown","2c81c5de":"markdown","817d5261":"markdown","1de6ef5d":"markdown","afe28e24":"markdown","8d5a5cac":"markdown","07b26c29":"markdown","9788f12e":"markdown","b91d4b3a":"markdown","f7349574":"markdown","176da8a9":"markdown","d563ebd7":"markdown","99cf2809":"markdown","aad39b50":"markdown","9c9b4ff2":"markdown","55ed3d49":"markdown","130734f5":"markdown","c8a08c8d":"markdown","ea00d7e9":"markdown"},"source":{"a44d22e1":"!pip install rank_bm25\n!pip install sentence_transformers\n!pip install yattag","ecf5fa0d":"#Import of Required Libraries\/functions\nimport os\nimport json\nimport scipy\nimport datetime as dt\nfrom sentence_transformers import SentenceTransformer\nimport pickle\nimport numpy as np\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import Word\nimport pandas as pd\nnltk.download('stopwords')\nexclude_list = string.digits + string.punctuation\ntable = str.maketrans(exclude_list, len(exclude_list)*\" \")\nstop = stopwords.words('english')\nenglish_stopwords = list(set(stop))\nimport matplotlib.pyplot as plt\n\nfrom gensim.summarization import summarize\n# For Visualization\nfrom IPython.core.display import HTML\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive, Layout\nimport seaborn as sns\n#Define dataPath to download files\ndataPath = (\"kaggleData\")\ndownloadPath = (\"\/kaggle\/input\/CORD-19-research-challenge\/\")\ncheckfolder = os.path.isdir(dataPath)\n\n# If folder doesn't exist, then create it.\nif not checkfolder:\n    os.makedirs(dataPath)\n\n#Reading Data from JSON files\nfrom os import walk\nrootFolders = next(os.walk(dataPath))[1]","efff07f0":"#Define utilities for data parsing\n\ndef affiliation_parsing(x: dict) -> str:\n    \"\"\"Parse affiliation into string.\"\"\"\n    current = []\n    for key in ['laboratory', 'institution']:\n        if x['affiliation'].get(key):  # could also use try, except\n            current.append(x['affiliation'][key])\n        else:\n            current.append('')\n    for key in ['addrLine', 'settlement', 'region', 'country', 'postCode']:\n        if x['affiliation'].get('location'):\n            if x['affiliation']['location'].get(key):\n                current.append(x['affiliation']['location'][key])\n        else:\n            current.append('')\n    return ', '.join(current)\n\n\ndef cite_parsing(x: list, key: str) -> list:\n    \"\"\"Parse references into lists for delimiting.\"\"\"\n    cites = [i[key] if i else '' for i in x] #test['body_text']]\n    output = []\n    for i in cites:\n        if i:\n            output.append(','.join([j['ref_id'] if j['ref_id'] else '' for j in i]))\n        else:\n            output.append('')\n    return '|'.join(output)\n\n\ndef extract_key(x: list, key:str) -> str:\n    if x:\n        return ['|'.join(i[key] if i[key] else '' for i in x)]\n    return ''\n\nextract_func = lambda x, func: ['|'.join(func(i) for i in x)]\nformat_authors = lambda x: f\"{x['first']} {x['last']}\"\nformat_full_authors = lambda x: f\"{x['first']} {''.join(x['middle'])} {x['last']} {x['suffix']}\"\nformat_abstract = lambda x: \"{}\\n {}\".format(x['section'], x['text'])\nall_keys = lambda x, key: '|'.join(i[key] for i in x.values())","3099ae0b":"%%time\n\n# This section converts all the JSON files to a aggregated dataframe\naggregate_dflist = []\nfor path in ['biorxiv_medrxiv', 'comm_use_subset', 'custom_license', 'noncomm_use_subset']:\n    path\n    for subpath in ['pdf_json','pmc_json']:\n        if not os.path.exists(f'{downloadPath}\/{path}\/{path}\/{subpath}\/') : continue\n        json_files = [file for file in os.listdir(f'{downloadPath}\/{path}\/{path}\/{subpath}\/') if file.endswith('.json')]\n        df_list = []\n                \n        for js in json_files:\n            \n            with open(os.path.join(f'{downloadPath}\/{path}\/{path}\/{subpath}', js)) as json_file:\n                paper = json.load(json_file)\n                \n            paper_df = pd.DataFrame({\n                'paper_id': paper['paper_id'],\n                'title': paper['metadata']['title'],\n                #'authors': extract_func(paper['metadata']['authors'], format_authors),\n                #'full_authors': extract_func(paper['metadata']['authors'], format_full_authors),\n                #'affiliations': extract_func(paper['metadata']['authors'], affiliation_parsing),\n                #'emails': extract_key(paper['metadata']['authors'], 'email'),              \n                'body': extract_func(paper['body_text'], format_abstract),\n                #'body_cite_spans': cite_parsing(paper['body_text'], 'cite_spans'),\n                #'body_ref_spans': cite_parsing(paper['body_text'], 'ref_spans'),\n                #'bib_titles': all_keys(paper['bib_entries'], 'title'),\n                #'ref_captions': all_keys(paper['ref_entries'], 'text'),\n                #'back_matter': extract_key(paper['back_matter'], 'text')\n            })\n            df_list.append(paper_df)\n            del paper_df\n                        \n        if len(df_list) > 0:\n            temp_df = pd.concat(df_list)\n            #temp_df['dataset'] = path+'_'+subpath\n            aggregate_dflist.append(temp_df)\n            del temp_df\n            \naggregate_df = pd.concat(aggregate_dflist)\n","cb314684":"%%time\n#Extract body length is greater than >1200\naggregate_df= aggregate_df[aggregate_df['body'].apply(len) >=1200]\n","e3868f03":"#Merge from Metadata file URL, Journal, Publish time\n\nmetadata_df = pd.read_csv(f'{downloadPath}\/metadata.csv',parse_dates = ['publish_time'])\naggregate_df = pd.merge(aggregate_df, metadata_df[['sha','publish_time','url' ]], left_on = 'paper_id', right_on = 'sha', how = 'left')","1ffe7cfe":"#Drop the Additional column created\naggregate_df.drop(['sha'], axis = 1, inplace=True)","eab0ec2d":"#Fill the empty columns\naggregate_df.fillna({'publish_time':dt.datetime(2020,1,1), 'url':'UNSPECIFIED'}, inplace=True)","97217751":"#aggregate_df.drop(['publish_time','url'], axis = 1, inplace=True)\naggregate_df.count()","001addfb":"#Process functions for BM-25\n\nSEARCH_DISPLAY_COLUMNS = ['paper_id','title', 'body', 'url']\n\ndef preprocess_with_ngrams(docs):\n    # Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n    bigram = Phrases(docs, min_count=5)\n    trigram = Phrases(bigram[docs])\n\n    for idx in range(len(docs)):\n        for token in bigram[docs[idx]]:\n            if '_' in token:\n                # Token is a bigram, add to document.\n                docs[idx].append(token)\n        for token in trigram[docs[idx]]:\n            if '_' in token:\n                # Token is a trigram, add to document.\n                docs[idx].append(token)\n    return docs\n\nclass SearchResults:\n    \n    def __init__(self, \n                 data: pd.DataFrame,\n                 columns = None):\n        self.results = data\n        if columns:\n            self.results = self.results[columns]\n            \n    def __getitem__(self, item):\n        return Paper(self.results.loc[item])\n    \n    def __len__(self):\n        return len(self.results)\n        \n    def _repr_html_(self):\n        return self.results._repr_html_()\n    \n    def getDf(self):\n        return self.results \n    \ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|\u2019|\u201d|\u201c|\\?|%|>|<', '', text)\n    t = re.sub('\/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t\n\ndef clean(text):\n    t = text.lower()\n    t = strip_characters(t)\n    t = str(t).translate(table)\n    return t\n\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not (word.isnumeric() and len(word) is not 4)\n                     and (not word.isnumeric() or word.isalpha())] )\n               )\n\ndef preprocess(text):\n    t = clean(text)    \n    tokens = tokenize(t)\n    \n    return tokens\n\n# BM25 \n\nclass WordTokenIndex:\n    \n    def __init__(self, \n                 corpus: pd.DataFrame, \n                 columns=SEARCH_DISPLAY_COLUMNS):\n        self.corpus = corpus\n        raw_search_str =self.corpus.title.fillna('') +' ' + self.corpus.body.fillna('')\n        #self.corpus['all_text'] = raw_search_str.apply(preprocess).to_frame()\n        self.index = raw_search_str.apply(preprocess).to_frame()\n        self.index.columns = ['terms']\n        self.index.index = self.corpus.index\n        self.columns = columns\n       \n    def search(self, search_string):\n        search_terms = preprocess(search_string)\n        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n        return SearchResults(results, self.columns + ['paper'])\n    \nclass RankBM25Index(WordTokenIndex):\n    \n    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n        super().__init__(corpus, columns)\n        self.bm25 = BM25Okapi(self.index.terms.tolist(),k1=3,b=0.001)\n        \n    def search(self, search_string, n=4):\n        search_terms = preprocess(search_string)\n        doc_scores = self.bm25.get_scores(search_terms)\n        ind = np.argsort(doc_scores)[::-1][:n]\n        results = self.corpus.iloc[ind][self.columns]\n        results['Score'] = doc_scores[ind]\n        results = results[results.Score > 0]\n        return SearchResults(results.reset_index(), self.columns + ['Score'])\n    \ndef show_task(taskTemp,taskId):\n    #print(Task)\n    keywords = taskTemp#tasks[tasks.Task == Task].Keywords.values[0]\n    print(keywords)\n    search_results = bm25_index.search(keywords, n=200)    \n    return search_results","dcef5a68":"task7 = [\" Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs)\",\n              \"Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms\",\n              \"Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues\",\n              \"How states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public\",\n              \"Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy\",\n              \"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity\",\n              \"Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices\",\n              \"Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes\",\n              \"Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling\",\n              \"Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression\",\n              \"Policies and protocols for screening and testing\",\n              \"Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents\",\n              \"Technology roadmap for diagnostics\",\n              \"Barriers to developing and scaling up new diagnostic tests, how future coalition and accelerator models like Coalition for Epidemic Preparedness Innovations could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment\",\n              \"New platforms and technology like CRISPR to improve response times and employ more holistic approaches to COVID-19 and future diseases\",\n              \"Coupling genomics and diagnostic testing on a large scale\",\n              \"Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant\",\n              \"Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional\",\n              \"One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors\"]\n\ntasks={'What do we know about diagnostics and surveillance?': task7}","f2895cec":"#Load the BM-25 Corpus File\n\nbm25CORPUS_PATH='\/kaggle\/input\/bm25pickle\/corpusbm25.pkl'\nif not os.path.exists(bm25CORPUS_PATH):\n    print(\"Caching the corpus for future use...\")\n    with open('corpusbm25.pkl', 'wb') as file:\n        pickle.dump(bm25_index, file)\n    with open(bm25CORPUS_PATH, 'rb') as corpus_pt:\n        bm25corpus = pickle.load(corpus_pt)    \n        \nelse:\n    print(\"Loading the corpus from\", bm25CORPUS_PATH, '...')\n    with open(bm25CORPUS_PATH, 'rb') as corpus_pt:\n        bm25corpus = pickle.load(corpus_pt)","1959d454":"#Create Plot to show no. of articles in corpus related to Task7\nimport collections\ndict1 = {}\nfor task in task7:\n    task_text = ' '.join(preprocess(task)[:8])\n    dict1[task_text] = bm25corpus.search(task,n=aggregate_df.shape[0]).getDf().shape[0]\n    \n\ndict1_sorted = sorted(dict1.items(), key=lambda kv: kv[1])\n\nfig = plt.figure(figsize=(16,16))\n\nheight = [tpl[1] for tpl in dict1_sorted]\nbars = [tpl[0] for tpl in dict1_sorted]\n#bars = [tpl[0] for tpl in task]\ny_pos = np.arange(len(bars))\n \nplt.barh(y_pos, height)\nplt.yticks(y_pos, bars)\n\nplt.title(\"No of Articles from BM25 Algorithm for What do we know about diagnostics and surveillance?\")\nplt.xlabel(\"# Articles\")\nplt.ylabel(\"Tasks\") \n    \nplt.show()","1aaef0ba":"#Load SciBERT Model\nMODEL_PATH='\/kaggle\/input\/scibertnli\/scibert-nli'\nmodel = SentenceTransformer(MODEL_PATH)","b38073c4":"#Corpus Building and mapping index of Paper-ID\ncorpus_dictionary = {}\ncorpus = []\nidx = 0\nfor _, row in aggregate_df.iterrows():\n    if isinstance(row['body'], str):\n        if row['body'] != \"Unknown\":\n            if len(row['body']) > 900:\n                corpus.append(row['body'])\n                corpus_dictionary[idx] = row['paper_id']   \n                idx +=1","66bd0b5b":"#Building\/Loading Corpus\nCORPUS_PATH='\/kaggle\/input\/scibertnli\/corpus.pkl'\nif not os.path.exists(CORPUS_PATH):\n    print(\"Caching the corpus for future use...\")\n    with open('corpus.pkl', 'wb') as file:\n        pickle.dump(corpus, file)\nelse:\n    print(\"Loading the corpus from\", CORPUS_PATH, '...')\n    with open(CORPUS_PATH, 'rb') as corpus_pt:\n        corpus = pickle.load(corpus_pt)","4327aa06":"#Building\/Loading Embeddings\nEMBEDDINGS_FILE='\/kaggle\/input\/scibertembeddings\/scibertnliembeddings.pkl'\nif not os.path.exists(EMBEDDINGS_FILE):\n        print(\"Computing and caching model embeddings for future use...\")\n        embeddings = model.encode(corpus, show_progress_bar=True)\n        with open(EMBEDDINGS_FILE, 'wb') as file:\n            pickle.dump(embeddings, file)\nelse:\n        print(\"Loading model embeddings from\", EMBEDDINGS_FILE, '...')\n        with open(EMBEDDINGS_FILE, 'rb') as file:\n            embeddings = pickle.load(file)","30284088":"#Process Text\ndef clean(text):\n    t = text.lower()\n    t = strip_characters(t)\n    t = str(t).translate(table)\n    return t\n\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not (word.isnumeric() and len(word) is not 4)\n                     and (not word.isnumeric() or word.isalpha())] )\n               )\n\ndef preprocess(text):\n    t = clean(text)    \n    tokens = tokenize(t)   \n    return tokens\n\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|\u2019|\u201d|\u201c|\\?|%|>|<', '', text)\n    t = re.sub('\/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t","6b4f4025":"#Task7 Questions\ntask7 = [\n\"What do we know about diagnostics and surveillance?\", \n\"What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?\",     \n\"What are the sampling methods available to determine asymptomatic disease like use of serosurveys such as convalescent samples?\",\n\"What do we know about early detection of disease like use of screening of neutralizing antibodies such as ELISAs?\",\n\"What is known about efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms\",\n\"What do we know about recruitment, support, and coordination of local expertise and capacity via public, private\u2014commercial issues?\",\n\"What do we know about recruitment, support, and coordination of local expertise and capacity via non-profit, including academic, including legal, ethical, communications, and operational issues?\",\n\"During a pandemic, how to increase coordination between local, public and private?\",\n\"During a pandemic, how to increase coordination between local, non-profit, academic, legal and ethical?\",\n\"How states in US might leverage universities and private laboratories for testing purposes?\",\n\"How states in US might leverage universities and private laboratories for communications to public health officials?\",\n\"How states in US might leverage universities and private laboratories for communications to the public?\",\n\"Development of a point-of-care test like a rapid influenza test recognizing the tradeoffs between speed, accessibility, and accuracy\",\n\"Development of a point-of-care test like a rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy\",\n\"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity\",\n\"Separation of assay development issues from instruments\",\n\"Separation of assay development issues from the role of the private sector to help quickly migrate assays onto those devices\",\n\"Efforts to track the evolution of the virus like genetic drift and avoid locking into specific reagents and surveillance as well as detection schemes\",\n\"Efforts to track the evolution of the virus like mutations and avoid locking into specific reagents and surveillance as well as detection schemes\",\n\"Latency issues and when there is sufficient viral load to detect the pathogen\",\n\"Latency issues and when there is sufficient viral load in understanding of what is needed in terms of biological sampling\",\n\"Latency issues and when there is sufficient viral load in understanding of what is needed in terms of environmental sampling\",\n\"Use of diagnostics such as host response markers like cytokines to detect early disease\",\n\"Use of diagnostics such as host response markers like cytokines to predict severe disease progression\",\n\"Policies and protocols for screening and testing\",\n\"Policies to mitigate the effects on supplies associated with mass testing\",\n\"Policies to mitigate the effects on supplies associated with testing of swabs and reagents\",\n\"Technology roadmap for diagnostics of COVID-19\",\n\"Barriers to developing and scaling up new diagnostic tests\",\n\"Barriers to developing and scaling up future coalition and accelerator models like Coalition for Epidemic Preparedness\",\n\"Barriers to developing and scaling up innovations that could provide critical funding for diagnostics\",\n\"Barriers to developing and scaling up opportunities for a streamlined regulatory environment\",\n\"New platforms and technology like CRISPR to improve response times\",\n\"New platforms and technology like CRISPR to employ more holistic approaches to COVID-19\",\n\"New platforms and technology like CRISPR to employ more holistic approaches to future diseases\",\n\"Coupling genomics and diagnostic testing on a large scale\",\n\"Enhance capabilities for rapid sequencing to target regions of the genome that will allow specificity for a particular variant\",\n\"Enhance capabilities for bioinformatics to target regions of the genome that will allow specificity for a particular variant\",\n\"One Health surveillance of humans and potential sources of future spillover\",\n\"One Health surveillance of humans ongoing exposure to hosts like bats\",\n\"One Health surveillance of humans on transmission hosts like heavily trafficked and farmed wildlife\",\n\"One Health surveillance of humans on transmission hosts like domestic food and companion species\",\n\"One Health surveillance of humans and potential sources of future spillover on transmission hosts that includes environmental, demographic, and occupational risk factors\" \n]\n\ntasks={'What do we know about diagnostics and surveillance?': task7}\n","914bee39":"# Function to find out relavent articles using 3 distance measures. \ndef ask_question_cosine(query, model, corpus, corpus_embed, task, measure, top_k=5):   \n    queries = [query]\n    query_embeds = model.encode(queries, show_progress_bar=False)\n    for query, query_embed in zip(queries, query_embeds):\n        distances = scipy.spatial.distance.cdist([query_embed], corpus_embed, measure)[0]\n        distances = zip(range(len(distances)), distances)\n        distances = sorted(distances, key=lambda x: x[1])\n        \n        results = []\n        \n        for count, (idx, distance) in enumerate(distances[:top_k]):\n            results.append([count + 1, corpus[idx].strip(), task, measure, corpus_dictionary[idx], round(1 - distance, 4)])\n            \n    return results","cbb8e00b":"# Function to create results in a dataframe\ndef get_bert_results(tasks, model, corpus, embeddings, distance_measure):   \n    results = []\n    \n    #distance_measure = ['cosine','chebyshev','canberra']\n    #distance_measure = ['cosine']   \n    for task in tasks:\n        task_text = ' '.join(preprocess(task)[:8])\n        for dist_measure in distance_measure:\n            results.append(ask_question_cosine(task_text, model,corpus,embeddings,task,dist_measure))\n    \n    return results\n\n\ndef get_bert_result_df(df, tasks, model, corpus, embeddings):\n    bertresults_df = pd.DataFrame(columns=['Task', 'Distance_Measure','URL', 'Publish_Date','Title','Paper_Id','Summary' ])\n    #bertresults_df = pd.DataFrame(columns=['Task', 'Distance_Measure','Score','URL', 'Publish_Date', 'Journal', 'Title','paper_id' ])\n   \n    #bert_results = []\n    #article_ids = []\n    distance_measure = ['cosine','cityblock','sqeuclidean']\n    bert_results= get_bert_results(tasks, model, corpus, embeddings, distance_measure)\n    #bert_results= get_bert_results(tasks, model, corpus, embeddings,distance_measure)\n    #article_ids = get_article_ids(df, bert_results)\n    \n    for ber in bert_results:\n        for rel in ber:\n            matched_row = df[df['paper_id']==rel[4]]\n            url = matched_row.url.values[0]\n            publish_time = matched_row.publish_time.values[0]\n            #journal = matched_row.journal.values[0]\n            title = matched_row.title.values[0]\n            bertsummary = summarize(rel[1],ratio=0.02)\n            #score = str(int(round(rel[5]*100))) \n            #score = str(rel[5])\n            task = rel[2]\n            dist = rel[3]\n            #bertresults_df = bertresults_df.append({'Task':task, 'Distance_Measure':dist,'URL':url, 'Publish_Date':publish_time, 'Journal':journal, 'Title':title,'SciBERTSummary':bertsummary, 'paper_id':rel[4]},ignore_index=True)\n            bertresults_df = bertresults_df.append({'Task':task, 'Distance_Measure':dist, 'URL':url, 'Publish_Date':publish_time, 'Title':title,'Paper_Id':rel[4],'Summary':bertsummary},ignore_index=True)\n    \n    return bertresults_df ","e591d90d":"# This function block helps to create a nice HTML format to visualize\n\ndef generate_html_table(df):\n\n    css_style = \"\"\"table.paleBlueRows {\n      font-family: \"Trebuchet MS\", Helvetica, sans-serif;\n      border: 1px solid #FFFFFF;\n      width: 100%;\n      height: 150px;\n      text-align: center;\n      border-collapse: collapse;\n    }\n    table.paleBlueRows td, table.paleBlueRows th {\n      text-align: center;\n      border: 1px solid #FFFFFF;\n      padding: 3px 2px;\n    }\n    table.paleBlueRows tbody td {\n      text-align: center;\n      font-size: 11px;\n    }\n    table.paleBlueRows tr:nth-child(even) {\n      background: #D0E4F5;\n    }\n    table.paleBlueRows thead {\n      background: #0B6FA4;\n      border-bottom: 5px solid #FFFFFF;\n    }\n    table.paleBlueRows thead th {\n      font-size: 17px;\n      font-weight: bold;\n      color: #FFFFFF;\n      border-left: 2px solid #FFFFFF;\n    }\n    table.paleBlueRows thead th:first-child {\n      border-left: none;\n    }\n\n    table.paleBlueRows tfoot {\n      font-size: 14px;\n      font-weight: bold;\n      color: #333333;\n      background: #D0E4F5;\n      border-top: 3px solid #444444;\n    }\n    table.paleBlueRows tfoot td {\n      font-size: 14px;\n    }\n    div.scrollable {width:100%; max-height:150px; overflow:auto; text-align: center;}\n    \"\"\"\n\n    from yattag import Doc, indent\n    doc, tag, text, line = Doc().ttl()\n\n    with tag(\"head\"):\n        with tag(\"style\"):\n            text(css_style)\n\n    with tag('table', klass='paleBlueRows'):\n        with tag(\"tr\"):\n            for col in list(df.columns):\n                with tag(\"th\"):\n                     with tag(\"div\", klass = \"scrollable\"):\n                        text(col)\n        for idx, row in df.iterrows():\n            with tag('tr'):\n                for i in range(len(row)):\n                    with tag('td'):\n                        with tag(\"div\", klass = \"scrollable\"):\n                            try:\n                                if \"http\" in str(row[i]):\n                                    with tag(\"a\", href = str(row[i])):\n                                        text(str(row[i]))\n                                else:\n                                    text(str(row[i]))\n                            except:\n                                print(row[i])\n\n    #display(HTML(doc.getvalue()))\n    return(doc.getvalue())","20363067":"%%time\n#Run the model and generate results\nbert_results_df = get_bert_result_df(aggregate_df, task7, model, corpus, embeddings)","f0624b7b":"#Drop the duplicate articles extracted by all three distance measures.\nbert_results_df.drop_duplicates(subset='Paper_Id', keep=\"first\", inplace=True)\nbert_results_df['Publish_Date'] = bert_results_df['Publish_Date'].dt.date","328947a6":"#Provides top 5 Articles\nbert_results_df.head(5)","6ed4b29e":"# Save the HTML for later viewing\nberthtml = generate_html_table(bert_results_df.sort_values(by=['Publish_Date'], ascending=False))\n#display(HTML(berthtml))\n%store berthtml >results.html","b5891b0f":"#Sort the results in decending order by Publish Data\nbert_results_df.sort_values(by=['Publish_Date'], ascending=False,inplace=True)","da918ac1":"#Interactive way to show the top Journals\ntask_list = list(np.unique(bert_results_df['Task'].tolist()))\ntask_list.insert(0, \"Please select a question\")\n\ncols = [\"Paper_Id\", \"Title\", \"URL\", \"Publish_Date\", \"Summary\"]\n\n@interact\ndef dropdowns(Question = task_list):\n    if Question == \"Please select a question\":\n        pass\n    else:   \n        print (Question)\n        display(HTML(generate_html_table(bert_results_df[bert_results_df['Task'].str.strip() == str(Question).strip()][cols])))","c2533f45":"#Plot to show Distance Measure Mix\nax = sns.countplot(x=\"Distance_Measure\", data = bert_results_df)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 60)","5fb45770":"## License Agreements\n\n#### PSF LICENSE AGREEMENT FOR PYTHON 3.8.2\n1. This LICENSE AGREEMENT is between the Python Software Foundation (\"PSF\"), and\n   the Individual or Organization (\"Licensee\") accessing and otherwise using Python\n   3.8.2 software in source or binary form and its associated documentation.\n\n2. Subject to the terms and conditions of this License Agreement, PSF hereby\n   grants Licensee a nonexclusive, royalty-free, world-wide license to reproduce,\n   analyze, test, perform and\/or display publicly, prepare derivative works,\n   distribute, and otherwise use Python 3.8.2 alone or in any derivative\n   version, provided, however, that PSF's License Agreement and PSF's notice of\n   copyright, i.e., \"Copyright \u00a9 2001-2020 Python Software Foundation; All Rights\n   Reserved\" are retained in Python 3.8.2 alone or in any derivative version\n   prepared by Licensee.\n\n3. In the event Licensee prepares a derivative work that is based on or\n   incorporates Python 3.8.2 or any part thereof, and wants to make the\n   derivative work available to others as provided herein, then Licensee hereby\n   agrees to include in any such work a brief summary of the changes made to Python\n   3.8.2.\n\n4. PSF is making Python 3.8.2 available to Licensee on an \"AS IS\" basis.\n   PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED.  BY WAY OF\n   EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND DISCLAIMS ANY REPRESENTATION OR\n   WARRANTY OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE\n   USE OF PYTHON 3.8.2 WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n\n5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON 3.8.2\n   FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF\n   MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON 3.8.2, OR ANY DERIVATIVE\n   THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n\n6. This License Agreement will automatically terminate upon a material breach of\n   its terms and conditions.\n\n7. Nothing in this License Agreement shall be deemed to create any relationship\n   of agency, partnership, or joint venture between PSF and Licensee.  This License\n   Agreement does not grant permission to use PSF trademarks or trade name in a\n   trademark sense to endorse or promote products or services of Licensee, or any\n   third party.\n\n8. By copying, installing or otherwise using Python 3.8.2, Licensee agrees\n   to be bound by the terms and conditions of this License Agreement.\n \n \n#### The 3-Clause BSD License\n \nNote: This license has also been called the \"New BSD License\" or \"Modified BSD License\". See also the 2-clause BSD License.\n\nCopyright <YEAR> <COPYRIGHT HOLDER>\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#### NumPy license\nCopyright \u00a9 2005-2020, NumPy Developers.\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\n\nNeither the name of the NumPy Developers nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n \n#### The MIT License\n \nCopyright <YEAR> <COPYRIGHT HOLDER>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n  \n#### License agreement for matplotlib versions 1.3.0 and later\n=========================================================\n\n1. This LICENSE AGREEMENT is between the Matplotlib Development Team\n(\"MDT\"), and the Individual or Organization (\"Licensee\") accessing and\notherwise using matplotlib software in source or binary form and its\nassociated documentation.\n\n2. Subject to the terms and conditions of this License Agreement, MDT\nhereby grants Licensee a nonexclusive, royalty-free, world-wide license\nto reproduce, analyze, test, perform and\/or display publicly, prepare\nderivative works, distribute, and otherwise use matplotlib\nalone or in any derivative version, provided, however, that MDT's\nLicense Agreement and MDT's notice of copyright, i.e., \"Copyright (c)\n2012- Matplotlib Development Team; All Rights Reserved\" are retained in\nmatplotlib  alone or in any derivative version prepared by\nLicensee.\n\n3. In the event Licensee prepares a derivative work that is based on or\nincorporates matplotlib or any part thereof, and wants to\nmake the derivative work available to others as provided herein, then\nLicensee hereby agrees to include in any such work a brief summary of\nthe changes made to matplotlib .\n\n4. MDT is making matplotlib available to Licensee on an \"AS\nIS\" basis.  MDT MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\nIMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, MDT MAKES NO AND\nDISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\nFOR ANY PARTICULAR PURPOSE OR THAT THE USE OF MATPLOTLIB\nWILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n\n5. MDT SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF MATPLOTLIB\n FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR\nLOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING\nMATPLOTLIB , OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF\nTHE POSSIBILITY THEREOF.\n\n6. This License Agreement will automatically terminate upon a material\nbreach of its terms and conditions.\n\n7. Nothing in this License Agreement shall be deemed to create any\nrelationship of agency, partnership, or joint venture between MDT and\nLicensee.  This License Agreement does not grant permission to use MDT\ntrademarks or trade name in a trademark sense to endorse or promote\nproducts or services of Licensee, or any third party.\n\n8. By copying, installing or otherwise using matplotlib ,\nLicensee agrees to be bound by the terms and conditions of this License\nAgreement.\n \n#### 3-clause license (\"BSD License 2.0\", \"Revised BSD License\", \"New BSD License\", or \"Modified BSD License\")\nCopyright (c) <year>, <copyright holder>\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and\/or other materials provided with the distribution.\n    * Neither the name of the <organization> nor the\n      names of its contributors may be used to endorse or promote products\n      derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n \n#### Apache License\n\nVersion 2.0, January 2004\n\nhttp:\/\/www.apache.org\/licenses\/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and\nYou must cause any modified files to carry prominent notices stating that You changed the files; and\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and\nIf the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\n\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\n5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and\/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\n#### Software License\nThe Python Imaging Library (PIL) is\n\n    Copyright \u00a9 1997-2011 by Secret Labs AB\n    Copyright \u00a9 1995-2011 by Fredrik Lundh\n\nBy obtaining, using, and\/or copying this software and\/or its associated documentation, you agree that you have read, understood, and will comply with the following terms and conditions:\n\nPermission to use, copy, modify, and distribute this software and its associated documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appears in all copies, and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of Secret Labs AB or the author not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission.\n\nSECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\n#### SciPy license\nCopyright \u00a9 2001, 2002 Enthought, Inc.\nAll rights reserved.\n\nCopyright \u00a9 2003-2019 SciPy Developers.\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\n\nNeither the name of Enthought nor the names of the SciPy Developers may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","3e37ff76":"# Visualize number of articles in our corpus related to TASK7","603a8e9e":"# We have seen in our model building and training differnt distance measures were providing distanct and better results","086ab4b2":"> We used pretrained model 'scibert-nli' which is a fine-tuned version of AllenAI's SciBERT. \n> SciBERT from AllenAI is trained on Corpus with 1.14M papers (Scientific Journals), 3.1B tokens. \n> We use the full text of the papers in our processing to provide best answers.","df140b47":"# Download and Preprocess the dataset from Kaggle","2c81c5de":"![TopicModelingcoherenceScore.PNG](attachment:TopicModelingcoherenceScore.PNG)","817d5261":"# Topic Modeling Coherence Score","1de6ef5d":"<div class=\"alert alert-block alert-info\">\n<font size=\"5\"><b>Introduction<\/b><\/font>\n<br>\n<br>\nScience and technology communities across the world have come together to address issues related to the COVID-19 virus and ensuing pandemic. The CORD-19 research dataset represents the most extensive machine-readable COVID-19 literature collection of over 59,000 scientific articles available for data mining to date. However, the rapid increase in the volume and type of COVID-19 literature makes it difficult for members of the medical community and policy makers to find what they need.\n<br>\n<br>\nThe focus of AI-007 was to create an artificial intelligence model that effectively searches the CORD-19 database and finds research papers answering the Task 7 questions: \u201c*What do we know about diagnostics and surveillance?*\u201d and \u201c*What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?*\u201d\n<br>\n<br>\nThis AI-007 model built on relevant research articles provided by kaggle to help the medical and policy maker communities to find answers on topic related diagnostics and surveillance relating to COVID-19.\n<br>\n<br>\nBy providing the research and tools needed to empower policy makers to lead and control this pandemic more effectively, this AI-007 model contributes to the ongoing COVID-19 response efforts worldwide.\n<\/div>","afe28e24":"![Task%207%20banner.JPG](attachment:Task%207%20banner.JPG)","8d5a5cac":"<div class=\"alert alert-block alert-info\">\n<font size=\"5\"><b>Notebook Contents<\/b><\/font>\n<br>\n<br>\n<ul><li>Our Overall Approach<\/li>\n    <li>Pros and Cons of Our Approach<\/li>\n    <li>Our Solution: Code and Results<\/li>\n    <li>Acknowledgement and Licenses<\/li>\n<\/ul>\n<font size=\"5\"><b>Summary of Approach<\/b><\/font>\n<br>\n<br>\n<ul>\n    <li>We started by preparing the COVID-19 dataset for efficient processing and analysis. This included consolidating data sources, keeping the most relevant columns of data, and then cleansing the data for easier analysis and input to the data models.<\/li>\n    <li>Next, we minimized the number of articles to analyze by eliminating those that were not relevant to the subtasks on which we were focused.<\/li>\n    <li>From there, we identified the dominant topics across the subtasks and used them to visualize the most relevant articles for each sub task.<\/li>\n    <li>Later, we used SciBERT pretrained model with the dataset provided by Kaggle to identify the relavent articles.<\/li>\n    <li>Lastly, we used 3 distance measures to identify the relavent articles.We displayed the responses to the subtasks follows,<\/li>\n        <ul>\n        <li><b>HTML Dropdown List<\/b><\/li>\n        <li><b>HTML File<\/b><\/li>\n        <li><b>Top 5 Articles<\/b><\/li>\n        <li><b>Distance Measure Plot<\/b><\/li>\n        <\/ul>\n<\/ul>\n    \n<font size=\"4\"><b>Prepare the Dataset for Processing<\/b><\/font>\n<br>\n<br>\nAn important technique that transforms the raw data into a format required for modeling, which results in retaining the critical keyword to improve the efficiency of the retrieval process, as well as the accuracy of the results. \n<br>\nActivities involved in this step were:<br>\n<ul>\n    <li>Remove duplicate papers using <b>title<\/b> data attribute<\/li>\n    <li>Remove punctuation, whitespace, stop words (library of words in English that are common words and not relevant to the article)<\/li>\n    <li>Make all words lowercase for exact matching of words<\/li>\n<li>Tokenize words so that each word can be used as an independent entity; each word is a token and additional preprocessing can be done.<\/li>\n    <li>Remove non English articles<\/li>\n<\/ul>\n<font size=\"4\"><b>Train Model<\/b><\/font>\n<br>\n<br>\nWe use a Bag_of_Words retrieval function called Okapi BM25 to select the top 100 articles related to each task query. BM25, abbreviation of Best Matching25, is a probabilistic information retrieval ranking method. It ranks documents based on a relevance score which is the probability of a document being relevant to the input query.\n<br>\n<br>\n<font size=\"5\"><b>Pros and Cons of the Approach<\/b><\/font>\n<ul>\n<br>\n<br>\n<li>We chose to start our approach by using the TF-ID word frequency model to create clusters of articles to find those related to our task 7 subtasks so we could then focus on that cluster of articles.<\/li>\n<li>We quickly saw that no clear clusters were formed. With 59,000 plus articles, there were not enough relationships among the keywords from the articles to form clear clusters.<\/li>\n<li>We quickly changed to Deep Learning algoritms like SciBERT, and  using different distance measure techniques to provide better results. We found, as a next generation SciBERT best-match model, it had more extensive capabilities to process high volumes of text to identify keywords and match them to relevant articles.<\/li>\n<\/ul>\n<\/div>","07b26c29":"#  Removal of Duplicates and extracting matching URL and Publish_Time from Metadata File","9788f12e":"<div class=\"alert alert-block alert-info\">\n<b>Downloading<\/b><br>\nIt uses Kaggle API to download the CORD-19 Dataset.<br>\n    Note: If you are using the Kaggle CLI tool, the tool will look for this token at ~\/.kaggle\/kaggle.json on Linux, OSX, and other UNIX-based operating systems, and at C:\\Users<Windows-username>.kaggle\\kaggle.json on Windows. If the token is not there, an error will be raised. Hence, once you\u2019ve downloaded the token, you should move it from your Downloads folder to this folder.\n<br>    \n<b>PreProcessing<\/b><br>\nAn important technique that transforms the raw data into a format required for modeling, which results in retaining the critical keyword to improve the efficiency of the retrieval process, as well as the accuracy of the results. \n<br>\nActivities involved in this step were:<br> \n&emsp;&emsp;a.\tDiscard non-English articles (less than 5%) <br> \n&emsp;&emsp;b.\tChanging words to lower case (Lower_case) <br> \n&emsp;&emsp;c.\tRemove stop_words <br> \n&emsp;&emsp;d.\tLemmatization to group together the inflected forms of a word so they can be analyzed as a single item <br> \n&emsp;&emsp;e.\tStemming to reduce inflected words to their word stem, base or root form\u2014generally a written word form <br>\n<\/div>","b91d4b3a":"**Ericsson, the world\u2019s leading telecommunications company, cares about doing good. This task was completed as part of our Ericsson for Good program, which allows our 90,000+ employees to contribute to their communities.**\n\n\u00a9 This Notebook has been released under the OSI-approved GNU LGPLv2.1 license; Google BERT is under Apache License 2.0 (https:\/\/github.com\/google-research\/bert\/blob\/master\/LICENSE); Facebook\u2019s fairseq is under MIT license: https:\/\/github.com\/pytorch\/fairseq\/blob\/master\/LICENSE","f7349574":"# BM-25 to Select Relavent Articles","176da8a9":"![TopicModelingScatterPlot.PNG](attachment:TopicModelingScatterPlot.PNG)","d563ebd7":"#  SciBERT Processing\n","99cf2809":"# **PIP Install of requried Libraries**# ","aad39b50":"# *Import the required libraries*","9c9b4ff2":"<font size=\"10\" align=\"center\"><b>Ericsson CORD-19 Challenge: Task7 AI-007 Model<\/b><\/font>","55ed3d49":"![COVIDTask7-min.PNG](attachment:COVIDTask7-min.PNG)","130734f5":"# Topic Modeling Scatter Plot","c8a08c8d":"# Distance Measure Plot","ea00d7e9":"# We had used 3 distance measures to provide best answers. We had found sometimes different distance measures are bringing good articles"}}