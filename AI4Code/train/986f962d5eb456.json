{"cell_type":{"049fc0d7":"code","dd02cc12":"code","735075de":"code","6215201d":"code","0e259a51":"code","00415518":"code","3ffc8b75":"code","a6c1d6e0":"code","91548d19":"code","9e49c2e3":"code","b6470954":"code","026a6e9d":"code","06a3203d":"code","fae9a121":"code","b133320f":"code","73d3d769":"code","63bf6a84":"code","4803fa96":"code","ed05d1f7":"code","7da66bc4":"code","a397ceb0":"code","5ab4341c":"markdown","078bbdec":"markdown","8f7bfb6e":"markdown","cdd3f33d":"markdown","e7340ad8":"markdown","b8c17571":"markdown","6a99b177":"markdown","2885dae6":"markdown","4dc6999e":"markdown","860298a3":"markdown","2cb9c54a":"markdown","0ada37ce":"markdown","f943b9f4":"markdown","33f05e32":"markdown","cefe3bd6":"markdown","d4248471":"markdown","39d9a11b":"markdown","71503bcd":"markdown","e39b677e":"markdown","2bdd7e92":"markdown"},"source":{"049fc0d7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","dd02cc12":"df = pd.read_csv('..\/input\/knn-data\/KNN_Data.csv')","735075de":"df.head() ","6215201d":"# THIS IS GOING TO BE A VERY LARGE PLOT\nsns.pairplot(df,hue='TARGET CLASS',palette='coolwarm')","0e259a51":"from sklearn.preprocessing import StandardScaler","00415518":"scaler = StandardScaler()","3ffc8b75":"scaler.fit(df.drop('TARGET CLASS',axis=1))","a6c1d6e0":"scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))","91548d19":"df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()","9e49c2e3":"from sklearn.model_selection import train_test_split","b6470954":"X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['TARGET CLASS'],\n                                                    test_size=0.30)","026a6e9d":"from sklearn.neighbors import KNeighborsClassifier","06a3203d":"knn = KNeighborsClassifier(n_neighbors=1)","fae9a121":"knn.fit(X_train,y_train)","b133320f":"pred = knn.predict(X_test)","73d3d769":"from sklearn.metrics import classification_report,confusion_matrix","63bf6a84":"print(confusion_matrix(y_test,pred))","4803fa96":"print(classification_report(y_test,pred))","ed05d1f7":"error_rate = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","7da66bc4":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","a397ceb0":"# NOW WITH K=30\nknn = KNeighborsClassifier(n_neighbors=30)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=30')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","5ab4341c":"# Predictions and Evaluations\nLet's evaluate our KNN model!","078bbdec":"# EDA\n\nSince this data is artificial, we'll just do a large pairplot with seaborn.\n\n**Use seaborn on the dataframe to create a pairplot with the hue indicated by the TARGET CLASS column.**","8f7bfb6e":"**Use the .transform() method to transform the features to a scaled version.**","cdd3f33d":"** Create a confusion matrix and classification report.**","e7340ad8":"## Retrain with new K Value\n\n**Retrain your model with the best K value (up to you to decide what you want) and re-do the classification report and the confusion matrix.**","b8c17571":"**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**","6a99b177":"** Fit scaler to the features.**","2885dae6":"# Using KNN\n\n**Import KNeighborsClassifier from scikit learn.**","4dc6999e":"**Now create the following plot using the information from your for loop.**","860298a3":"** Create a StandardScaler() object called scaler.**","2cb9c54a":"Thanks***","0ada37ce":"## Get the Data\n** Read the 'KNN_Project_Data csv file into a dataframe **","f943b9f4":"**Fit this KNN model to the training data.**","33f05e32":"# Choosing a K Value\nLet's go ahead and use the elbow method to pick a good K Value!\n\n** Create a for loop that trains various KNN models with different k values, then keep track of the error_rate for each of these models with a list. Refer to the lecture if you are confused on this step.**","cefe3bd6":"**Use the predict method to predict values using your KNN model and X_test.**","d4248471":"# Train Test Split\n\n**Use train_test_split to split your data into a training set and a testing set.**","39d9a11b":"**Create a KNN model instance with n_neighbors=1**","71503bcd":"# K Nearest Neighbors Project\n\n\n## Import Libraries\n**Import pandas,seaborn, and the usual libraries.**","e39b677e":"**Check the head of the dataframe.**","2bdd7e92":"# Standardize the Variables\n\nTime to standardize the variables.\n\n** Import StandardScaler from Scikit learn.**"}}