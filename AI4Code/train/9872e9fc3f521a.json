{"cell_type":{"a94b0a5c":"code","cb60d105":"code","7b3c03b0":"code","5fcb4724":"code","21581ab4":"code","37352ec5":"code","a3966066":"code","8bd45449":"code","a48ea30a":"code","f1c78a2f":"code","989983e6":"code","2800f730":"code","63d28aea":"code","ccfbe578":"code","d278fd58":"code","f83bb801":"code","e2266bc8":"code","a26202f0":"code","cfc79a5d":"code","3f8dc0d5":"code","db90b4f0":"code","60b6a00c":"code","d7a61679":"code","1ff3d9fb":"code","7f1e9a75":"code","8e6e840e":"code","c85ecd6a":"code","cfe6b553":"code","9d532bda":"code","d1dc8a08":"code","f13edf45":"code","e7092496":"code","fde67fb0":"code","0c859876":"code","b4fd73b0":"code","eaefe8d3":"code","769979fc":"code","d0a79fb4":"code","f6b5be63":"code","c829b2a8":"code","2321d68c":"code","077358b3":"code","3580b047":"code","f2850b3c":"code","eea3617d":"code","d6a7d388":"code","281df5eb":"code","441893be":"markdown","6540d664":"markdown","b3e8d4ea":"markdown","6a934890":"markdown","aac0a05e":"markdown","23f78807":"markdown","75e0474a":"markdown","90c4573b":"markdown","c15cd808":"markdown","98392a72":"markdown","e980cf16":"markdown","301b2fbb":"markdown","477f99de":"markdown"},"source":{"a94b0a5c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","cb60d105":"#data imports\ndata_train = pd.read_csv(\"..\/input\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/test.csv\")\ndata_train.head()","7b3c03b0":"data_train.describe()","5fcb4724":"features_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\ndata_train[\"Soil_Count\"] = data_train[features_soil].apply(sum, axis=1)\ndata_train.head()","21581ab4":"data_train.Soil_Count.describe()","37352ec5":"data_test[features_soil].describe()","a3966066":"data_train[\"Soil_Type\"] = data_train[features_soil].apply(np.argmax, axis=1)\ndata_train.head()","8bd45449":"data_train[\"Soil_Type\"] = data_train[\"Soil_Type\"].apply(lambda x: x.split(\"Soil_Type\")[-1])\ndata_train.head()","a48ea30a":"features_wilderness = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\ndata_train[\"Wilderness_Area\"] = data_train[features_wilderness].apply(sum, axis=1)\ndata_train.Wilderness_Area.describe()","f1c78a2f":"data_train[\"Wilderness_Area\"] = data_train[features_wilderness].apply(np.argmax, axis=1)\ndata_train[\"Wilderness_Area\"] = data_train[\"Wilderness_Area\"].apply(lambda x: x.split(\"Wilderness_Area\")[-1])\ndata_train.Wilderness_Area.head()","989983e6":"sns.countplot(data_train.Cover_Type)\nplt.show()","2800f730":"data_train.columns","63d28aea":"features = ['Elevation', 'Aspect', 'Slope','Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', \"Cover_Type\"]\nsns.heatmap(data=data_train[features].corr(), annot=True, linecolor=\"w\", fmt=\".1\")\nplt.show()","ccfbe578":"fig = plt.figure(figsize=(12,8))\nfor ind, each in enumerate([\"Elevation\", \"Aspect\" , \"Slope\", \"Hillshade_3pm\", \"Hillshade_Noon\", \"Hillshade_9am\"]):\n    plt.subplot(2, 3, ind + 1)\n    sns.distplot(data_train[each])\nplt.show()","d278fd58":"sns.distplot(data_train[\"Hillshade_Noon\"].apply(lambda x: x**4))\nplt.show()","f83bb801":"sns.distplot(data_train[\"Hillshade_9am\"].apply(lambda x: x**4))\nplt.show()","e2266bc8":"sns.distplot(data_train[\"Slope\"].apply(np.sqrt))\nplt.show()","a26202f0":"data_train[\"Aspect_Slope\"] = data_train.Aspect * data_train.Slope\nsns.distplot(data_train[\"Aspect_Slope\"].apply(np.cbrt))\nplt.show()","cfc79a5d":"data_train[\"Elevation_Slope\"] = np.sqrt(data_train.Elevation * data_train.Slope)\nsns.distplot(data_train[\"Elevation_Slope\"])\nplt.show()","3f8dc0d5":"data_train[\"Elevation_Aspect\"] = np.sqrt(data_train.Elevation * data_train.Aspect)\nsns.distplot(data_train[\"Elevation_Aspect\"])\nplt.show()","db90b4f0":"data_train[\"Hillshade_1\"] = (data_train.Hillshade_3pm * data_train.Hillshade_Noon * data_train.Hillshade_9am)\nsns.distplot(data_train[\"Hillshade_1\"])\nplt.show()","60b6a00c":"fig = plt.figure(figsize=(12,8))\nfor ind, each in enumerate(['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways',  'Horizontal_Distance_To_Fire_Points']):\n    plt.subplot(2, 2, ind + 1)\n    sns.distplot(data_train[each])\nplt.show()","d7a61679":"sns.distplot(np.sqrt(data_train[\"Horizontal_Distance_To_Roadways\"]))\nplt.show()","1ff3d9fb":"sns.distplot(np.sqrt(data_train[\"Horizontal_Distance_To_Fire_Points\"]))\nplt.show()","7f1e9a75":"def clear_dataset(dataset):\n    features_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n    features_wilderness = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\n    dataset[\"Soil_Type\"] = dataset[features_soil].apply(np.argmax, axis=1)\n    dataset[\"Soil_Type\"] = dataset[\"Soil_Type\"].apply(lambda x: x.split(\"Soil_Type\")[-1]).astype(int)\n    dataset = dataset.drop([\"Soil_Type15\", \"Soil_Type7\"], axis=1)\n    dataset[\"Wilderness_Area\"] = dataset[features_wilderness].apply(np.argmax, axis=1)\n    dataset[\"Wilderness_Area\"] = dataset[\"Wilderness_Area\"].apply(lambda x: x.split(\"Wilderness_Area\")[-1]).astype(int)\n    #dataset = dataset.drop(features_wilderness, axis=1)\n    dataset[\"Hillshade_1\"] = (dataset.Hillshade_Noon * dataset.Hillshade_9am)\n    dataset[\"Hillshade_1_sqrt\"] = np.sqrt(dataset[\"Hillshade_1\"])\n    dataset[\"Hillshade_2\"] = (dataset.Hillshade_3pm * dataset.Hillshade_9am)\n    dataset[\"Hillshade_2_sqrt\"] = np.sqrt(dataset[\"Hillshade_2\"])\n    dataset[\"Hillshade_3\"] = (dataset.Hillshade_3pm * dataset.Hillshade_Noon * dataset.Hillshade_9am)\n    dataset[\"Hillshade_3_sqrt\"] = np.sqrt(dataset[\"Hillshade_3\"])\n    dataset.Hillshade_1 = dataset.Hillshade_1.astype(float)\n    dataset[\"DistanceToHydrology\"] = np.sqrt(dataset.Horizontal_Distance_To_Hydrology ** 2 + dataset.Vertical_Distance_To_Hydrology ** 2)\n    dataset[\"Horizontal_Distance_To_Roadways_sqrt\"]= np.sqrt(dataset[\"Horizontal_Distance_To_Roadways\"])\n    dataset[\"Horizontal_Distance_To_Fire_Points_sqrt\"]= np.sqrt(dataset[\"Horizontal_Distance_To_Fire_Points\"])\n    dataset[\"Slope_sqrt\"] = np.sqrt(dataset[\"Slope\"])\n    dataset[\"Hillshade_9am_cube\"] = dataset[\"Hillshade_9am\"].apply(lambda x: x**3)\n    dataset[\"Hillshade_Noon_cube\"] = dataset[\"Hillshade_Noon\"].apply(lambda x: x**3)\n    dataset[\"Aspect_Slope_cbrt\"] = np.cbrt(dataset.Aspect * dataset.Slope)\n    dataset[\"Elevation_Slope_sqrt\"] = np.sqrt(dataset.Elevation * dataset.Slope)\n    dataset[\"Elevation_Aspect_sqrt\"] = np.sqrt(dataset.Elevation * dataset.Aspect)\n    dataset[\"Elevation_sqrt\"] = np.sqrt(dataset.Elevation)\n    return dataset","8e6e840e":"data_train = pd.read_csv(\"..\/input\/train.csv\")\nfinal_train = clear_dataset(data_train)","c85ecd6a":"x_data = final_train.drop([\"Cover_Type\", \"Id\"], axis=1)\ny_data = final_train[\"Cover_Type\"]","cfe6b553":"from sklearn.decomposition import PCA, TruncatedSVD\npca = PCA(n_components = 2 )  # whitten = normalize\nx_pca = pca.fit_transform(x_data)\nprint(\"variance ratio: \", pca.explained_variance_ratio_)\nprint(\"sum: \",sum(pca.explained_variance_ratio_))","9d532bda":"df = pd.DataFrame(np.stack([x_pca[:,0], x_pca[:,1], y_data], axis=1), columns=[\"p1\", \"p2\", \"Cover_Type\"])\ncolor = [\"blue\", \"green\", \"purple\", \"yellow\", \"red\", \"orange\", \"cyan\"]\nplt.figure(1,figsize=(9,6))\nfor each in y_data.unique():\n    plt.scatter(df.p1[df.Cover_Type == each],df.p2[df.Cover_Type == each],color = color[each - 1],label = each, alpha=0.5)\nplt.legend()\nplt.show()","d1dc8a08":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.25, random_state=42)","f13edf45":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef get_metrics(y_test, y_predicted):  \n    # true positives \/ (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives \/ (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives\/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1","e7092496":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=19, max_features=11,n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\n\ny_predicted = clf.predict(x_val)","fde67fb0":"accuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","0c859876":"from sklearn.metrics import confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","b4fd73b0":"cm = confusion_matrix(y_val, y_predicted)\nplt.figure(1, figsize=(9,6))\nplot_confusion_matrix(cm, classes = set(y_data.unique()))\nplt.show()","eaefe8d3":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n    '''\n    plot feature importances of a tree-based sklearn estimator\n    \n    Note: X_train and y_train are pandas DataFrames\n    \n    Note: Scikit-plot is a lovely package but I sometimes have issues\n              1. flexibility\/extendibility\n              2. complicated models\/datasets\n          But for many situations Scikit-plot is the way to go\n          see https:\/\/scikit-plot.readthedocs.io\/en\/latest\/Quickstart.html\n    \n    Parameters\n    ----------\n        clf         (sklearn estimator) if not fitted, this routine will fit it\n        \n        X_train     (pandas DataFrame)\n        \n        y_train     (pandas DataFrame)  optional\n                                        required only if clf has not already been fitted \n        \n        top_n       (int)               Plot the top_n most-important features\n                                        Default: 10\n                                        \n        figsize     ((int,int))         The physical size of the plot\n                                        Default: (8,8)\n        \n        print_table (boolean)           If True, print out the table of feature importances\n                                        Default: False\n        \n    Returns\n    -------\n        the pandas dataframe with the features and their importance\n        \n    Author\n    ------\n        George Fisher\n    '''\n    \n    __name__ = \"plot_feature_importances\"\n    \n    import pandas as pd\n    import numpy  as np\n    import matplotlib.pyplot as plt\n    \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n                                    format(clf.__class__.__name__))\n                \n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n            \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp","769979fc":"a = plot_feature_importances(clf, x_train, y_train, top_n=x_train.shape[1], title=clf.__class__.__name__)","d0a79fb4":"from xgboost import XGBClassifier\nclf = XGBClassifier(n_estimators=200, learning_rate=0.3, max_depth=3,n_jobs=-1, seed=42, objective=\"multi:softmax\")\nclf.fit(x_train, y_train)\ny_predicted = clf.predict(x_val)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","f6b5be63":"cm = confusion_matrix(y_val, y_predicted)\nplt.figure(1, figsize=(9,6))\nplot_confusion_matrix(cm, classes = set(y_data.unique()))\nplt.show()","c829b2a8":"a = plot_feature_importances(clf, x_train, y_train, top_n=x_train.shape[1], title=clf.__class__.__name__)","2321d68c":"from lightgbm import LGBMClassifier\nclf = LGBMClassifier(n_estimators=200, learning_rate=0.3, max_depth=3,n_jobs=-1, seed=42, objective=\"multi:softmax\")\nclf.fit(x_train, y_train)\ny_predicted = clf.predict(x_val)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","077358b3":"cm = confusion_matrix(y_val, y_predicted)\nplt.figure(1, figsize=(9,6))\nplot_confusion_matrix(cm, classes = set(y_data.unique()))\nplt.show()","3580b047":"a = plot_feature_importances(clf, x_train, y_train, top_n=x_train.shape[1], title=clf.__class__.__name__)","f2850b3c":"data_test = pd.read_csv(\"..\/input\/test.csv\")\nfinal_test = clear_dataset(data_test)","eea3617d":"clf = RandomForestClassifier(n_estimators=100,max_depth=11, max_features=21,min_samples_leaf=0.001,criterion=\"entropy\",n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\ny_predicted = clf.predict(x_val)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","d6a7d388":"y_predicted = clf.predict(x_train)\naccuracy, precision, recall, f1 = get_metrics(y_train, y_predicted)\nprint(\"train accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","281df5eb":"clf = RandomForestClassifier(n_estimators=100,max_depth=11, max_features=21,min_samples_leaf=0.001,criterion=\"entropy\",n_jobs=-1, random_state=42)\nclf.fit(x_data, y_data)\ntest_preds = clf.predict(final_test.drop([\"Id\"], axis=1))\noutput = pd.DataFrame({'Id': data_test.Id,\n                       'Cover_Type': test_preds})\noutput.to_csv('rf_submission.csv', index=False)","441893be":"## PCA","6540d664":"### Feature Importances","b3e8d4ea":"## Load Data","6a934890":"# Prediction And Submission","aac0a05e":"There is no soil_type7 or soil_type15 in train data this i'm not sure other soil types usefull or not.","23f78807":"## XGBoost","75e0474a":"### Feature \u0130mportances","90c4573b":"### Confusion Matrix","c15cd808":"## Train Test Split","98392a72":"## Random Forest","e980cf16":"### Confusion Matrix","301b2fbb":"### Confusion Matrix","477f99de":"## LighGBM"}}