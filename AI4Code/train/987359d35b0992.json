{"cell_type":{"4b714734":"code","2736dcd0":"code","091ddca3":"code","8a4caa38":"code","36c1b579":"code","045b1527":"code","f27113de":"code","53610001":"code","bfc431b3":"code","04f27502":"code","a849c7df":"code","df21948a":"code","18183997":"code","283102d7":"code","685a38b7":"code","a20559bc":"code","96a65ecd":"code","018c292a":"code","328aa585":"code","1979106c":"code","5f671460":"code","f87bb3a9":"code","399c2ece":"code","1b2724ae":"code","7be7c7f0":"code","da1a9d68":"code","2e6f2121":"code","53caf7ec":"code","0d53cd88":"code","5bc095df":"code","b0374433":"code","7d8813d1":"code","84a2d203":"code","8676abed":"code","07710c62":"code","66c48ee6":"code","f986c2b6":"code","52eadedf":"code","6cd522c1":"code","be6c72df":"code","2e1a1d1c":"code","044750b0":"code","b0b21327":"code","bf4662cd":"code","36157345":"code","4548c8c4":"code","8c8c7ff6":"code","816c8cb6":"code","75a47424":"code","28ffe54d":"code","cf04cbcd":"code","17d99eb6":"code","f32791b4":"code","eb099176":"code","49ff4483":"code","ad484ece":"code","dabddf1e":"code","fa09a7f0":"code","92adc6e6":"markdown","3371e034":"markdown","21d47d1d":"markdown","8090a877":"markdown","7d97a17d":"markdown","8715862c":"markdown","34d798ca":"markdown","5dc9480c":"markdown","b7581d59":"markdown","adee3926":"markdown","8d8a69f6":"markdown","8784d65a":"markdown","13b5769a":"markdown","dc8075a4":"markdown","f62466a4":"markdown","34289052":"markdown","f3123fb4":"markdown","748284d5":"markdown","a644c4dd":"markdown","6dd06f96":"markdown","8f315792":"markdown","920df42e":"markdown","5ccc1f69":"markdown","886db15d":"markdown","11085970":"markdown","a63d4eba":"markdown","d45832f5":"markdown","9d17df13":"markdown","a9f7bb88":"markdown","da8f86d6":"markdown","41f3871e":"markdown","92bce5b2":"markdown","1a30fb86":"markdown","eec8b0c5":"markdown","422192a3":"markdown","f737f803":"markdown","2bf14422":"markdown","d012aa41":"markdown","0187cc20":"markdown"},"source":{"4b714734":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2736dcd0":"train = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')","091ddca3":"train.head()","8a4caa38":"train.shape","36c1b579":"import seaborn as sns\nsns.heatmap(train.isnull())","045b1527":"percent_missing = train.isnull().sum() * 100 \/ len(train)\nmissing_value_df = pd.DataFrame({'column_name': train.columns,\n                                 'percent_missing': percent_missing})","f27113de":"missing_value_df","53610001":"train","bfc431b3":"sns.countplot(train['target'])","04f27502":"sns.countplot(train['relevent_experience'],hue=train['target'])","a849c7df":"sns.countplot(train['enrolled_university'],hue=train['target'])","df21948a":"sns.boxplot(x='target',y='city_development_index',data=train)","18183997":"sns.boxplot(y='training_hours',x='target',data=train)","283102d7":"sns.barplot(y='training_hours',x='target',data=train)","685a38b7":"import matplotlib.pyplot as plt","a20559bc":"plt.figure(figsize=(20,6))\nsns.countplot(train['experience'],hue=train['target'])","96a65ecd":"sns.countplot(train['education_level'],hue=train['target'])","018c292a":"sns.countplot(train['major_discipline'],hue=train['target'])","328aa585":"print('shape before dropping values',train.shape)","1979106c":"train = train.drop(['company_type','company_size','major_discipline','gender'],axis=1)","5f671460":"train = train.dropna()","f87bb3a9":"print('Shape after data cleaning',train.shape)","399c2ece":"train.head()","1b2724ae":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain['relevent_experience'] = le.fit_transform(train['relevent_experience'])","7be7c7f0":"train['enrolled_university'] = le.fit_transform(train['enrolled_university'])\ntrain['education_level'] = le.fit_transform(train['education_level'])\ntrain['experience'] = le.fit_transform(train['experience'])\ntrain['last_new_job'] = le.fit_transform(train['last_new_job'])","da1a9d68":"train.head(2)","2e6f2121":"train_resample = train.copy()","53caf7ec":"train = train.drop(['enrollee_id','city'],axis=1)","0d53cd88":"from scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(train))\n","5bc095df":"threshold = 3\ntrain = train[(z < 3).all(axis=1)]","b0374433":"train.shape","7d8813d1":"#breaking into training and testing dataset\nx = train[['city_development_index', 'relevent_experience', 'enrolled_university',\n       'education_level', 'experience', 'last_new_job', 'training_hours']]\ny = train['target']\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.91,stratify=y)","84a2d203":"from sklearn import preprocessing\n\nx_train = preprocessing.normalize(x_train)\nx_test = preprocessing.normalize(x_test)","8676abed":"from sklearn.linear_model import LogisticRegression\nlinear = LogisticRegression()\nlinear.fit(x_train,y_train)\nprint('Accuracy on test set is : ',linear.score(x_test,y_test))","07710c62":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree.fit(x_train,y_train)\nprint('Accuracy on test set is ', tree.score(x_test,y_test))","66c48ee6":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train,y_train)\nprint('Accuracy on test set is ', svc.score(x_test,y_test))\n","f986c2b6":"from sklearn.ensemble import GradientBoostingClassifier\nboost = GradientBoostingClassifier(n_estimators=200)\nboost.fit(x_train,y_train)\nprint('accuracy on test set : ',boost.score(x_test,y_test))","52eadedf":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier()\nforest.fit(x_train,y_train)\nprint('accuracy on test set : ',forest.score(x_test,y_test))\n","6cd522c1":"svc = SVC(kernel='poly',gamma=5,C=10,degree=5)\nsvc.fit(x_train,y_train)\nsvc.score(x_test,y_test)","be6c72df":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=100)\nknn.fit(x_train,y_train)\nknn.score(x_test,y_test)","2e1a1d1c":"# define the base models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nlevel0 = list()\nlevel0.append(('lr', LogisticRegression()))\nlevel0.append(('knn', KNeighborsClassifier()))\nlevel0.append(('cart', DecisionTreeClassifier()))\nlevel0.append(('svm', SVC()))\nlevel0.append(('forest',RandomForestClassifier()))","044750b0":"#define meta model\nfrom sklearn.ensemble import StackingClassifier\n# define meta learner model\nlevel1 = LogisticRegression()\n# define the stacking ensemble\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)","b0b21327":"#running model\nmodel.fit(x_train,y_train)","bf4662cd":"#Checking Accuracy\nmodel.score(x_test,y_test)","36157345":"train_resample.head()","4548c8c4":"train_resample = train_resample.drop(['enrollee_id','city'],axis=1)","8c8c7ff6":"x = train_resample[['city_development_index', 'relevent_experience', 'enrolled_university',\n       'education_level', 'experience', 'last_new_job', 'training_hours']]\ny = train_resample['target']\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.91,stratify=y)","816c8cb6":"from sklearn.utils import resample\n# concatenate our training data back together\nX = pd.concat([x_train, y_train], axis=1)\n# separate minority and majority classes\nnot_change = X[X['target']==0]\nchange = X[X['target']==1]\n\n# upsample minority\nchange_upsampled = resample(change,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_change), # match number in majority class\n                          random_state=27) # reproducible results\n# combine majority and upsampled minority\nupsampled = pd.concat([not_change, change_upsampled])","75a47424":"upsampled['target'].value_counts()","28ffe54d":"y_train = upsampled['target']\nX_train = upsampled.drop('target', axis=1)","cf04cbcd":"#Logistic Regression\nlinear.fit(X_train,y_train)\nprint('Accuracy on test set after oversampling',linear.score(x_test,y_test))","17d99eb6":"#Boosting\nboost.fit(X_train,y_train)\nprint('Accuracy on test set after oversampling',boost.score(x_test,y_test))","f32791b4":"#Decision Tree\nlinear.fit(X_train,y_train)\nprint('Accuracy on test set after oversampling',tree.score(x_test,y_test))","eb099176":"# still using our separated classes fraud and not_fraud from above\n\n# downsample majority\nnot_change_downsampled = resample(not_change,\n                                replace = False, # sample without replacement\n                                n_samples = len(change), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_change_downsampled, change])","49ff4483":"downsampled['target'].value_counts()","ad484ece":"y_train = downsampled['target']\nX_train = downsampled.drop('target', axis=1)","dabddf1e":"#Logistic Regression\nlinear.fit(X_train,y_train)\nprint('Accuracy on test set after undersampling',linear.score(x_test,y_test))","fa09a7f0":"#BOOSTING\nboost.fit(X_train,y_train)\nprint('Accuracy on test set after undersampling',boost.score(x_test,y_test))","92adc6e6":"KNN","3371e034":"# UNDERSAMPLING","21d47d1d":"Removing all rows which were having so missing values","8090a877":"# REMOVING OUTLIERS","7d97a17d":"Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don\u2019t have a ton of data to work with.","8715862c":"Decision Tree","34d798ca":"Removing all the extra columns : \n\n1.enrolee_id\n\n2. city as it is represented by development index","5dc9480c":"Encoding Categorical value","b7581d59":"# MODEL","adee3926":"> Conclusion\n\n1.Most of the people who are not changing jobs are from city with high development index so basically they are having a comfortable life in respective city are not willing to change jobs\n\n2.People in cities with less development index tend to change their jobs for better life style maybe!","8d8a69f6":"# DATA PREPARING FOR MODEL","8784d65a":"Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.","13b5769a":"> Conclusion\n\n1. Mostly people are STEM\n\n2. In STEM , people tend not to change the job  \n\n3. People not tend to change job in any field \n\n4. There is a good proportion of people relatively in Business Degree that tend to change job","dc8075a4":"as we have seen earlier, this dataset is very uneven , so we would consider both ways to create model , but first with original data","f62466a4":"Conclusion :\n\n1.Most of the person are having no enrollment in any university and those without any enrollment have no interest in changing their job \n\n2.Part Time course enrolled students are very less and they also dont want to change\n\n3.Full time enrolled are not very much also but in proportion they have higher chance of changing the job than others","34289052":"So , probablity is higher that a person will not change job","f3123fb4":"RANDOM FOREST","748284d5":"> Conclusion \n\n1.Most of the candidates has relevant experience \n\n2.Candidates with relevant experience are not looking for a job change\n\n3.Candidates with no relevant experience are more keen to change job","a644c4dd":"There is no much difference between training hours of those who are wishing to change and those who are not , so not much can be deduced from this classification","6dd06f96":"Let us do some resampling on our data as this is imbalanced data","8f315792":"to read more about stacking in ML\nhttps:\/\/www.kaggle.com\/cosmosankur\/stacking-in-machine-learning#Making-Final-Model-For-Stacking","920df42e":"# Understanding DATA","5ccc1f69":"SVC with different Parameters","886db15d":"First I would drop all the rows ","11085970":"> Conclusion : \n\n1.people who have experience less than 1 year have more tendency to change their job while those with more than 20 years of experience have very less tendency of changing jobz\n\n 2.As experience increase , tendency to change the job becomes more and more less","a63d4eba":"THANKS FOR VIEWING THIS NOTEBOOK","d45832f5":"Logistic Regression","9d17df13":"# DATA TRANSFORMATION","a9f7bb88":"made this copy previously to use it now :)","da8f86d6":"BOOSTING","41f3871e":"since sampling has not affected much of our accuracy , original data was giving better accuracy","92bce5b2":"SVM","1a30fb86":"Gender , Major Discipline and Company size and company type have a lot of missing values, rest are good","eec8b0c5":"# Normalizing our DATA","422192a3":"i dropped all those columns which had more than 10 percent of missing data","f737f803":"# 1. OVERSAMPLING","2bf14422":"let us explore different types of values in column","d012aa41":"> conclusion\n\n1.Graduates have very less chance of leaving \n\n2.Regardless of any education level , there is a very less chance of changing job ","0187cc20":"STACKING ALGORITHMS"}}