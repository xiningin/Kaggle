{"cell_type":{"a9584667":"code","f6a6fb0d":"code","6e807652":"code","7def47b3":"code","371082e7":"code","05f49076":"code","648e7633":"code","93d2e3c4":"code","a7488335":"code","99fa5ac3":"code","f0b26ffb":"code","d0773ee4":"code","2e5ac552":"code","b22302b3":"code","e24f30e0":"code","869d1cfb":"code","dbd7522e":"code","8d4e1463":"code","9927e8ac":"code","80739ce7":"code","9d9417b7":"code","7df143d8":"code","9d97934f":"code","ceff3c9f":"markdown","8ef38f24":"markdown","4cf82e11":"markdown","3f090b30":"markdown","058353f9":"markdown","555825b5":"markdown","339356eb":"markdown","a31b88e3":"markdown","67ba9815":"markdown","cb16b66f":"markdown","013a653d":"markdown","38c59962":"markdown","7f0b7ce3":"markdown","cb57cbd6":"markdown","f961a323":"markdown","78c17299":"markdown","9b5a4b84":"markdown","e000f670":"markdown","bcb08ee9":"markdown"},"source":{"a9584667":"import torch\ntorch.manual_seed(0)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\n\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize']=(15,3.4)","f6a6fb0d":"path = '..\/input\/gtzan-musicspeech-collection'","6e807652":"import librosa\n\naudio_datafile = path + '\/music_wav\/bagpipe.wav'\nx, sf = librosa.load(audio_datafile)\nprint(f'shape: {x.shape}\\nsampling freq: {sf}\\ntime duration: len\/sf = {x.shape[0]\/sf} seconds')","7def47b3":"plt.title('plot of a sound sinal from a .wav file')\nplt.plot(x[:1024])","371082e7":"import IPython.display as ipd\nipd.Audio(audio_datafile)","05f49076":"import librosa.display\n#plt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sf)\nplt.title('a music waveplot with librosa.display.waveplot')","648e7633":"speech_file = path + '\/speech_wav\/god.wav'\n\nx, sf = librosa.load(speech_file)\n\nprint(f'shape: {x.shape}\\nsampling freq: {sf}\\ntime duration: len\/sf = {x.shape[0]\/sf} seconds')\nplt.title('plot of a sound sinal from a .wav file')\nplt.plot(x[:1024]);","93d2e3c4":"import IPython.display as ipd\nipd.Audio(speech_file)","a7488335":"import librosa.display\n#plt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sf)\nplt.title('a speech waveplot')","99fa5ac3":"import os\n\nclass_names = ['music', 'speech']\ndata_dir = {'music' : path + '\/music_wav', \n            'speech': path + '\/speech_wav'}\n\nwav_files = {cls_name: [] for cls_name in class_names}\nfor cls_name in class_names:\n    folder = data_dir[cls_name]\n    filelist = os.listdir(folder)\n    for filename in filelist:\n        if filename[-4:] == '.wav':\n            wav_files[cls_name].append(os.path.join(folder, filename))","f0b26ffb":"print([len(wav_files[c]) for c in class_names])\nwav_files['music'][:3]","d0773ee4":"np.random.seed(1)\n\nfile_list = {'train': [], 'val': []}\nfor class_id, c in enumerate(class_names):\n    n_data = len(wav_files[c])\n    rindx = np.random.permutation(n_data)\n    n_validation = int(0.25*n_data)\n    v_indx = rindx[:n_validation]\n    t_indx = rindx[n_validation:]\n    file_list['train'] += [ (wav_files[c][k], class_id) for k in t_indx ] \n    file_list['val'] += [ (wav_files[c][k], class_id) for k in v_indx ] \n#     file_list['']","2e5ac552":"[ len(file_list[tv]) for tv in ['train', 'val'] ]","b22302b3":"import torch\nimport numpy as np\nimport librosa # audio file manipulation\n\nclass MSDataset(torch.utils.data.Dataset):\n    \"\"\" Music\/Speech Classification \n        filelist: [(file_path, class_id)]\n        sample_time: time duration to sample from .wav file\n                     the sample is extracted somewhere in the middle of the whole sequence\n                     similar to data augmentation\n                     \n         Validation dataset: the first segment of the sequence is used.\n                             Another option is to apply several segments and accumulate multiple inferences\n    \"\"\"\n    def __init__(self, filelist, sample_sec=5., is_train=True):\n        self.filelist = filelist\n        self.time_duration = sample_sec\n        self.is_train = is_train\n        \n        _, sf = librosa.load(filelist[0][0])\n        self.sf = sf\n        self.n_features = int(self.time_duration * sf)\n        \n    def __len__(self):\n        return len(self.filelist)\n    \n    def __getitem__(self, i):\n        # 1. load the file\n        # 2. sample a segment of the length from the whole seq\n        # 3. return segment, id\n        audio_file, class_id = self.filelist[i]\n        x, sf = librosa.load(audio_file)\n\n        if self.is_train:\n            k = np.random.randint(low=0, high=x.shape[0]-self.n_features) # choose the start index\n        else:\n            k = 0\n        \n        x = torch.from_numpy(x[k:k+self.n_features]).reshape(1,-1)\n        \n        return x, class_id\n    \n    def load(self, audio_file):\n        return librosa.load(audio_file)","e24f30e0":"ds = MSDataset(file_list['train'], sample_sec=5, is_train=True)\n\nx, label = ds[-1]\n\nprint('dataset length: ', len(ds), x.dtype, x.shape, label)#label.dtype, label.shape)\n\nplt.title(f'Audio segment, randomly sampled. src len: {x.shape} label: {label}');\nplt.plot(x.reshape(-1)); ","869d1cfb":"sample_sec = 2\nbatch_size = 32\n\ndata_loader = {tv: \n                   torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n                                               batch_size=batch_size,\n                                               shuffle=True,\n                                               )\n               for tv in ['train', 'val']}\n#\ndata_loader","dbd7522e":"import torch.nn as nn\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        kernel_size = 17\n        self.Conv = nn.Conv1d(in_channels=in_channels,\n                              out_channels=out_channels,\n                              kernel_size=kernel_size,\n                              stride=3)\n        self.BatchNorm = nn.BatchNorm1d(out_channels)\n        self.ELU = nn.ELU()\n    \n\n    def forward(self, x):\n        x = self.Conv(x)\n        x = self.BatchNorm(x)\n        x = self.ELU(x)\n\n        return x\n    \n    \nclass MyModel(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.Hidden_1 = BasicBlock(in_channels, 100)  \n        self.Hidden_2 = BasicBlock(100, out_channels)\n        self.Pooling = nn.AdaptiveAvgPool1d(1)\n        self.Flatten = nn.Flatten()\n        self.activation = nn.LogSoftmax(dim = 1)\n\n    def forward(self, x):\n        x = self.Hidden_1(x)\n        x = self.Hidden_2(x)\n        x = self.Pooling(x)\n        x = self.Flatten(x)\n        x = self.activation(x)\n        \n        return x","8d4e1463":"n_channels = 1\nn_targets  = 2\n\nmodel = MyModel(n_channels, n_targets)\nmodel","9927e8ac":"def training_loop(n_epochs, optim, model, loss_fn, dl_train, dl_val, hist=None):\n    if hist is not None:\n        pass\n    else:\n        hist = {'tloss': [], 'tacc': [], 'vloss': [], 'vacc': []}\n    best_acc = 0\n    for epoch in range(1, n_epochs+1):\n        tr_loss, tr_acc = 0., 0.\n        n_data = 0\n        for im_batch, label_batch in dl_train: # minibatch\n            im_batch, label_batch = im_batch.to(device), label_batch.to(device)\n            ypred = model(im_batch)\n            loss_train = loss_fn(ypred, label_batch)\n        \n            optim.zero_grad()\n            loss_train.backward()\n            optim.step()\n   \n            # accumulate correct prediction\n            tr_acc  += (torch.argmax(ypred.detach(), dim=1) == label_batch).sum().item() # number of correct predictions\n            tr_loss += loss_train.item() * im_batch.shape[0]\n            n_data  += im_batch.shape[0]\n        #\n        # statistics\n        tr_loss \/= n_data\n        tr_acc  \/= n_data\n        #\n        val_loss, val_acc = performance(model, loss_fn, dl_val)\n        \n        if epoch <= 5 or epoch % 10 == 0 or epoch == n_epochs:\n             print(f'Epoch {epoch}, tloss {tr_loss:.2f} t_acc: {tr_acc:.4f}  vloss {val_loss:.2f}  v_acc: {val_acc:.4f}')\n#         else:\n#             if best_acc < val_acc:\n#                 best_acc = val_acc\n#                 print(' best val accuracy updated: ', best_acc)\n        #\n        # record for history return\n        hist['tloss'].append(tr_loss)\n        hist['vloss'].append(val_loss) \n        hist['tacc'].append(tr_acc)\n        hist['vacc'].append(val_acc)\n        \n    print ('finished training_loop().')\n    return hist\n#\n\ndef performance(model, loss_fn, dataloader):\n    model.eval()\n    with torch.no_grad():\n        loss, acc, n = 0., 0., 0.\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            ypred = model(x)\n            loss += loss_fn(ypred, y).item() * len(y)\n            p = torch.argmax(ypred, dim=1)\n            acc += (p == y).sum().item()\n            n += len(y)\n        #\n    loss \/= n\n    acc \/= n\n    return loss, acc\n#\ndef plot_history(history):\n    fig, axes = plt.subplots(1,2, figsize=(16,6))\n    axes[0].set_title('Loss'); \n    axes[0].plot(history['tloss'], label='train'); axes[0].plot(history['vloss'], label='val')\n    axes[0].legend()\n    max_vacc = max(history['vacc'])\n    axes[1].set_title(f'Acc. vbest: {max_vacc:.2f}')\n    axes[1].plot(history['tacc'], label='train'); axes[1].plot(history['vacc'], label='val')\n    axes[1].legend()\n#","80739ce7":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n# model\nin_channels = 1\nn_targets = 2\n\nmodel = MyModel(in_channels, n_targets).to(device)\n\n# optim\nlearning_rate = 0.1\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n# loss\ncriterion = nn.CrossEntropyLoss().to(device)\n\n# history\nhistory = None","9d9417b7":"history = training_loop(100, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)","7df143d8":"plot_history(history)","9d97934f":"sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv.zip')\nsample_submission[list_classes] = y_test\nsample_submission.to_csv('submission.csv', index=False)","ceff3c9f":"### Sanity Check: Dataloader + Model","8ef38f24":"### Sanity Check","4cf82e11":"## Trainin Loop","3f090b30":"### Dataloader for train\/validation","058353f9":"## First Trial","555825b5":"1. # Music\/Speech Classification \n- with `nn.Conv1d()`\n- audio signal processing\n- neural network for classification\n- data: http:\/\/marsyas.info\/downloads\/datasets.html\n- main ref: https:\/\/www.kdnuggets.com\/2020\/02\/audio-data-analysis-deep-learning-python-part-1.html\n- see also [Speaker Verification](https:\/\/github.com\/HarryVolek\/PyTorch_Speaker_Verification)","339356eb":"### Dataset","a31b88e3":"## Music Speech Classification\n\n- Each training data is 30 seconds long.\n- We will use only N samples (T = N\/sf seconds) randomly chosen from 30 seconds long sequence for training\/testing.\n    - 1 second = sf samples (22050 samples\/sec,  or 44100 samples\/sec)\n- We will use .wav only. The dataset provide data in two formats: .au and .wav.  ","67ba9815":"### Data file list up","cb16b66f":"The experiment was performed with RTX 2070, 8G memory.","013a653d":"### Train\/Validation Split","38c59962":"End.","7f0b7ce3":"## Model, Optimizer & Loss Function","cb57cbd6":"### About the Data\n\n- A similar dataset which was collected for the purposes of music\/speech discrimination. \n- The dataset consists of 120 tracks, each 30 seconds long. \n- Each class (music\/speech) has 60 examples. \n- The tracks are all 22050Hz Mono 16-bit audio files in .wav format.","f961a323":"## Network Model Design","78c17299":"### Train More","9b5a4b84":"### A music sample from dataset","e000f670":"### A speech sample from dataset","bcb08ee9":"----"}}