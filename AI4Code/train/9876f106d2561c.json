{"cell_type":{"8f4c2dd4":"code","e24f1d85":"code","4544cec4":"code","36889c1e":"code","1100ff26":"code","c69c8b62":"code","32306999":"code","35c2a4e8":"code","9f7dcbf3":"code","bc211206":"code","7b8a0e37":"code","80299352":"code","aeb5c9e0":"code","50bbf6bf":"code","0e1a6b7c":"code","8cb853a2":"code","33d3fb19":"code","335cb74a":"code","97621f4b":"code","9022be29":"code","75650bc9":"code","7bf794ae":"code","faffa8ed":"code","162342ed":"code","702dfa22":"code","ffcca241":"code","3056231d":"code","ae7c9735":"code","19cb78ce":"code","2578971c":"code","cb6b958c":"code","1691d4a3":"code","b638eca3":"code","b8dcef9e":"code","09a8206c":"code","82224683":"code","b39e35ae":"code","775677d4":"code","800b95fa":"code","63e07698":"code","1475b73d":"code","c9f51eff":"code","4ca34954":"code","8c59aaa5":"code","118b09aa":"code","1984ff8f":"code","5136880c":"code","43d97fe5":"code","6e25b078":"code","23a5d623":"code","b2eb9a67":"code","2e241a18":"code","e519e6f7":"code","5088ed1b":"code","6c1daff8":"code","71589426":"code","90d9e106":"code","bd7bb9eb":"code","a002d8fe":"code","a96e3ac0":"code","54ae68a6":"code","e7d94ccb":"code","d175f210":"code","516227f1":"code","2ee44de6":"code","5f1ab8dd":"code","568ef785":"code","052a41e5":"code","d8f775e3":"code","bb41e98c":"code","eb3f85c9":"code","2964b004":"markdown","a02d28a1":"markdown","db0fe4ad":"markdown","191fcbb6":"markdown","cdbddda0":"markdown","4eb35525":"markdown","57d9d126":"markdown","627fca61":"markdown","d4008be9":"markdown","265b83b3":"markdown","ac4186b1":"markdown","708405ec":"markdown","a87a98a3":"markdown","d9508f16":"markdown","b5fe66a1":"markdown","7f4a94dc":"markdown","f0bd549c":"markdown","89980aff":"markdown","d3c81d62":"markdown","947344cb":"markdown","b47bc4fe":"markdown","14bd4b17":"markdown","19e8166f":"markdown","5f00949d":"markdown","b94c9160":"markdown","8d7d5426":"markdown"},"source":{"8f4c2dd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom datetime import datetime\nimport time\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e24f1d85":"df = pd.read_csv('..\/input\/hackathon\/task_2-COVID-19-death_cases_per_country_after_frist_death-till_26_June.csv')\ndf.head()","4544cec4":"# Numerical features\nNumerical_feat = [feature for feature in df.columns if df[feature].dtypes != 'O']\nprint('Total numerical features: ', len(Numerical_feat))\nprint('\\nNumerical Features: ', Numerical_feat)","36889c1e":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = df.corr(method='kendall')\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","1100ff26":"# Let's find the null values in data\n\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","c69c8b62":"## Lets Find the realtionship between discrete features and SalePrice\n\n#plt.figure(figsize=(8,6))\n\nfor feature in Numerical_feat:\n    data=df.copy()\n    plt.figure(figsize=(8,6))\n    data.groupby(feature)['deaths_per_million_85_days_after_first_death'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('deaths_per_million_85_days_after_first_death')\n    plt.title(feature)\n    plt.show()","32306999":"df[Numerical_feat].hist(bins=25)\nplt.show()","35c2a4e8":"## let us now examine the relationship between continuous features and SalePrice\n## Before that lets find continous features that donot contain zero values\n\ncontinuous_nozero = [feature for feature in Numerical_feat if 0 not in data[feature].unique() and feature not in ['deaths_per_million_10_days_after_first_death', 'deaths_per_million_15_days_after_first_death']]\n\nfor feature in continuous_nozero:\n    plt.figure(figsize=(8,6))\n    data = df.copy()\n    data[feature] = np.log(data[feature])\n    data['deaths_per_million_85_days_after_first_death'] = np.log(data['deaths_per_million_85_days_after_first_death'])\n    plt.scatter(data[feature], data['deaths_per_million_85_days_after_first_death'])\n    plt.xlabel(feature)\n    plt.ylabel('deaths_per_million_85_days_after_first_death')\n    plt.show()","9f7dcbf3":"## Normality and distribution checking for continous features\nfor feature in continuous_nozero:\n    plt.figure(figsize=(6,6))\n    data = df.copy()\n    sns.distplot(data[feature])\n    plt.show()","bc211206":"# categorical features\ncategorical_feat = [feature for feature in df.columns if df[feature].dtypes=='O']\nprint('Total categorical features: ', len(categorical_feat))\nprint('\\n',categorical_feat)","7b8a0e37":"# lets find unique values in each categorical features\nfor feature in categorical_feat:\n    print('{} has {} categories. They are:'.format(feature,len(df[feature].unique())))\n    print(df[feature].unique())\n    print('\\n')","80299352":"# let us find relationship of categorical with target variable\n\nfor feature in categorical_feat:\n    data=df.copy()\n    data.groupby(feature)['deaths_per_million_85_days_after_first_death'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('')\n    plt.title(feature)\n    plt.show()","aeb5c9e0":"# these are selected features from EDA section\nfeatures = ['deaths_per_million_85_days_after_first_death', 'deaths_per_million_80_days_after_first_death', 'deaths_per_million_75_days_after_first_death', 'deaths_per_million_70_days_after_first_death', 'deaths_per_million_65_days_after_first_death', 'deaths_per_million_60_days_after_first_death', 'deaths_per_million_55_days_after_first_death', 'deaths_per_million_50_days_after_first_death']","50bbf6bf":"# plot bivariate distribution (above given features with saleprice(target feature))\nfor feature in features:\n    if feature!='deaths_per_million_85_days_after_first_death':\n        plt.scatter(df[feature], df['deaths_per_million_85_days_after_first_death'])\n        plt.xlabel(feature)\n        plt.ylabel('deaths_per_million_85_days_after_first_death')\n        plt.show()","0e1a6b7c":"# Lets first handle numerical features with nan value\nnumerical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes!='O']\nnumerical_nan","8cb853a2":"df[numerical_nan].isna().sum()","33d3fb19":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_nan].isnull().sum()","335cb74a":"# categorical features with missing values\ncategorical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes=='O']\nprint(categorical_nan)","97621f4b":"df[categorical_nan].isna().sum()","9022be29":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    df[feature] = df[feature].fillna('None')","75650bc9":"df[categorical_nan].isna().sum()","7bf794ae":"#Deleting outliers for GrLivArea\ndf = df.drop(df[(df['deaths_per_million_80_days_after_first_death']>4000) & (df['deaths_per_million_85_days_after_first_death']<300000)].index)\n\nplt.scatter(df['deaths_per_million_80_days_after_first_death'], df['deaths_per_million_85_days_after_first_death'])\nplt.xlabel('deaths_per_million_80_days_after_first_death')\nplt.ylabel('deaths_per_million_85_days_after_first_death')\nplt.show()","faffa8ed":"#Deleting outliers for GrLivArea\ndf = df.drop(df[(df['deaths_per_million_75_days_after_first_death']>4000) & (df['deaths_per_million_85_days_after_first_death']<300000)].index)\n\nplt.scatter(df['deaths_per_million_75_days_after_first_death'], df['deaths_per_million_85_days_after_first_death'])\nplt.xlabel('deaths_per_million_75_days_after_first_death')\nplt.ylabel('deaths_per_million_85_days_after_first_death')\nplt.show()","162342ed":"#Deleting outliers for GrLivArea\ndf = df.drop(df[(df['deaths_per_million_70_days_after_first_death']>4000) & (df['deaths_per_million_85_days_after_first_death']<300000)].index)\n\nplt.scatter(df['deaths_per_million_70_days_after_first_death'], df['deaths_per_million_85_days_after_first_death'])\nplt.xlabel('deaths_per_million_70_days_after_first_death')\nplt.ylabel('deaths_per_million_85_days_after_first_death')\nplt.show()","702dfa22":"#Deleting outliers for GrLivArea\ndf = df.drop(df[(df['deaths_per_million_65_days_after_first_death']>4000) & (df['deaths_per_million_85_days_after_first_death']<300000)].index)\n\nplt.scatter(df['deaths_per_million_65_days_after_first_death'], df['deaths_per_million_85_days_after_first_death'])\nplt.xlabel('deaths_per_million_65_days_after_first_death')\nplt.ylabel('deaths_per_million_85_days_after_first_death')\nplt.show()","ffcca241":"#Deleting outliers for GrLivArea\ndf = df.drop(df[(df['deaths_per_million_60_days_after_first_death']>4000) & (df['deaths_per_million_85_days_after_first_death']<300000)].index)\n\nplt.scatter(df['deaths_per_million_60_days_after_first_death'], df['deaths_per_million_85_days_after_first_death'])\nplt.xlabel('deaths_per_million_60_days_after_first_death')\nplt.ylabel('deaths_per_million_85_days_after_first_death')\nplt.show()","3056231d":"#Deleting outliers for GrLivArea\ndf = df.drop(df[(df['deaths_per_million_55_days_after_first_death']>4000) & (df['deaths_per_million_85_days_after_first_death']<300000)].index)\n\nplt.scatter(df['deaths_per_million_55_days_after_first_death'], df['deaths_per_million_85_days_after_first_death'])\nplt.xlabel('deaths_per_million_55_days_after_first_death')\nplt.ylabel('deaths_per_million_85_days_after_first_death')\nplt.show()","ae7c9735":"#Deleting outliers for GrLivArea\ndf = df.drop(df[(df['deaths_per_million_50_days_after_first_death']>4000) & (df['deaths_per_million_85_days_after_first_death']<300000)].index)\n\nplt.scatter(df['deaths_per_million_50_days_after_first_death'], df['deaths_per_million_85_days_after_first_death'])\nplt.xlabel('deaths_per_million_50_days_after_first_death')\nplt.ylabel('deaths_per_million_85_days_after_first_death')\nplt.show()","19cb78ce":"# these are selected features from EDA section\nfeatures = ['deaths_per_million_85_days_after_first_death', 'deaths_per_million_80_days_after_first_death', 'deaths_per_million_75_days_after_first_death', 'deaths_per_million_70_days_after_first_death', 'deaths_per_million_65_days_after_first_death', 'deaths_per_million_60_days_after_first_death', 'deaths_per_million_55_days_after_first_death', 'deaths_per_million_50_days_after_first_death']\n\n# selecting continuous features from above\ncontinuous_features = ['deaths_per_million_85_days_after_first_death', 'deaths_per_million_80_days_after_first_death', 'deaths_per_million_75_days_after_first_death', 'deaths_per_million_70_days_after_first_death', 'deaths_per_million_65_days_after_first_death', 'deaths_per_million_60_days_after_first_death', 'deaths_per_million_55_days_after_first_death', 'deaths_per_million_50_days_after_first_death']","2578971c":"#Train = train_df.shape[0]\n#Test = test_df.shape[0]\n#target_feature = train_df.SalePrice.values\n#combined_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n#combined_data.drop(['SalePrice','Id'], axis=1, inplace=True)\n#print(\"all_data size is : {}\".format(combined_data.shape))","cb6b958c":"#Since I have no train, test files, Id, I adapted the code above for just 1 line, so that I could plot the distplot.  \ncombined_data = pd.concat((df, df)).reset_index(drop=True)","1691d4a3":"from scipy.stats import norm\n\n# checking distribution of continuous features(histogram plot)\nfor feature in continuous_features:\n    if feature!='deaths_per_million_85_days_after_first_death':\n        sns.distplot(combined_data[feature], fit=norm)\n        plt.show()\n    else:\n        sns.distplot(df['deaths_per_million_85_days_after_first_death'], fit=norm)\n        plt.show()","b638eca3":"# so let's label encode above ordinal features\nfrom sklearn.preprocessing import LabelEncoder\nfor feature in features:\n    encoder = LabelEncoder()\n    combined_data[feature] = encoder.fit_transform(list(combined_data[feature].values))","b8dcef9e":"# Now lets see label encoded data\ncombined_data[features].head()","09a8206c":"## One hot encoding or getting dummies \n\ndummy_ordinals = pd.get_dummies(features) \ndummy_ordinals.head()","82224683":"# creating dummy variables\n\ncombined_data = pd.get_dummies(combined_data)\nprint(combined_data.shape)","b39e35ae":"combined_data.head()","775677d4":"# let's first see descriptive stat info \ncombined_data.describe()","800b95fa":"## we willtake all features from combined_dummy_data \nfeatures_to_scale = [feature for feature in combined_data]\nprint(len(features_to_scale))","63e07698":"## Now here is where we will scale our data using sklearn module.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ncols = combined_data.columns  # columns of combined_dummy_data\n\nscaler = MinMaxScaler()\ncombined_data = scaler.fit_transform(combined_data[features_to_scale])","1475b73d":"# after scaling combined_data it is now in ndarray datypes\n# so we will create DataFrame from it\ncombined_scaled_data = pd.DataFrame(combined_data, columns=[cols])","c9f51eff":"combined_scaled_data.head() # this is the same combined_dummy_data in scaled form.","4ca34954":"# lets see descriptive stat info \ncombined_scaled_data.describe()","8c59aaa5":"#That's the code. Though we don't have train nor test, then I adapted once more. \n#train_df.shape, test_df.shape, combined_scaled_data.shape, combined_data.shape","118b09aa":"df.shape, df.shape, combined_scaled_data.shape, combined_data.shape","1984ff8f":"# separate train data and test data \ntrain_data = combined_scaled_data.iloc[:504,:]\ntest_data = combined_scaled_data.iloc[504:,:]\n\ntrain_data.shape, test_data.shape","5136880c":"## lets add target feature to train_data\n#train_data['deaths_per_million_85_days_after_first_death']= train_data['deaths_per_million_85_days_after_first_death']  # This saleprice is normalized. Its very impportant","43d97fe5":"train_data = train_data\ntrain_data.head(10)","6e25b078":"test_data = test_data.reset_index()\ntest_data.tail()","23a5d623":"dataset = train_data.copy()  # copy train_data to dataset variable","b2eb9a67":"dataset.head()","2e241a18":"dataset = dataset.dropna()","e519e6f7":"## lets create dependent and target feature vectors\n\nX = dataset.drop(['deaths_per_million_85_days_after_first_death'],axis=1)\nY = dataset[['deaths_per_million_85_days_after_first_death']]\n\nX.shape, Y.shape","5088ed1b":"Y.head()","6c1daff8":"# lets do feature selection here\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\n# define feature selection\nfs = SelectKBest(score_func=f_regression, k=27)\n# apply feature selection\nX_selected = fs.fit_transform(X, Y)\nprint(X_selected.shape)","71589426":"cols = list(range(1,28))\n\n## create dataframe of selected features\n\nselected_feat = pd.DataFrame(data=X_selected,columns=[cols])\nselected_feat.head()","90d9e106":"# perform train_test_split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(selected_feat,Y,test_size=0.3,random_state=0)","bd7bb9eb":"x_train.shape, x_test.shape","a002d8fe":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nlr = LinearRegression()\nlr.fit(x_train,y_train)","a96e3ac0":"y_pred = lr.predict(x_test) # predicting test data\ny_pred[:10]","54ae68a6":"# Evaluating the model\nprint('R squared score',metrics.r2_score(y_test,y_pred))\n\nprint('\\nMean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","e7d94ccb":"# check for underfitting and overfitting\nprint('Train Score: ', lr.score(x_train,y_train))\nprint('Test Score: ', lr.score(x_test,y_test))","d175f210":"## scatter plot of original and predicted target test data\nplt.figure(figsize=(8,6))\nplt.scatter(y_test,y_pred)\nplt.xlabel('y_tes')\nplt.ylabel('y_pred')\nplt.show()","516227f1":"## Lets do error plot\n## to get error in prediction just substract predicted values from original values\n\nerror = list(y_test.values-y_pred)\nplt.figure(figsize=(8,6))\nsns.distplot(error)","2ee44de6":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(n_estimators=100)\nrf_reg.fit(x_train,y_train)","5f1ab8dd":"y_pred = rf_reg.predict(x_test)\nprint(y_pred[:10])","568ef785":"## evaluating the model\n\nprint('R squared error',metrics.r2_score(y_test,y_pred))\n\nprint('\\nMean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","052a41e5":"# check score\nprint('Train Score: ', rf_reg.score(x_train,y_train))\nprint('Test Score: ', rf_reg.score(x_test,y_test))","d8f775e3":"## scatter plot of original and predicted target test data\nplt.figure(figsize=(8,6))\nplt.scatter(y_test,y_pred)\nplt.xlabel('y_tes')\nplt.ylabel('y_pred')\nplt.show()","bb41e98c":"## Lets do error plot\n## to get error in prediction just substract predicted values from original values\n\nerror = list(y_test.values-y_pred)\nplt.figure(figsize=(8,6))\nsns.distplot(error)","eb3f85c9":"# Plot\nsns.set_style('whitegrid')\nfig, (ax1,ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18, 6)\n\nsns.countplot(df['deaths_per_million_50_days_after_first_death'], order=df['deaths_per_million_50_days_after_first_death'].value_counts().index[:20],palette='viridis', ax=ax1)\nsns.countplot(df['deaths_per_million_85_days_after_first_death'], order=df['deaths_per_million_85_days_after_first_death'].value_counts().index[:20],palette='viridis', ax=ax2)\n\nax1.tick_params(axis='x', labelrotation=45)\nax2.tick_params(axis='x', labelrotation=45)\nax1.set_title('deaths_per_million_50_days_after_first_death')\nax2.set_title('deaths_per_million_85_days_after_first_death')\nax2.set(ylim=(0, 100))\n\n\nplt.show()","2964b004":"#Feature Selection","a02d28a1":"LINEAR REGRESSION","db0fe4ad":"#Above, data range differs so much. We need to scale them to same range.","191fcbb6":"#Worldwide differences in COVID-19-related mortality, by Pedro Curi Hallal \n\nPrint version ISSN 1413-8123On-line version ISSN 1678-4561\nCi\u00eanc. sa\u00fade coletiva vol.25  supl.1 Rio de Janeiro June 2020  Epub June 05, 2020\nhttps:\/\/doi.org\/10.1590\/1413-81232020256.1.11112020  \n\n\nMortality statistics due to COVID-19 worldwide are compared, by adjusting for the size of the population and the stage of the pandemic. Data from the European Centre for Disease Control and Prevention, and Our World in Data websites were used. Analyses are based on number of deaths per one million inhabitants.\n\nIn order to account for the stage of the pandemic, the baseline date was defined as the day in which the 10th death was reported. The analyses included 78 countries and territories which reported 10 or more deaths by April 9. On day 10, India had 0.06 deaths per million, Belgium had 30.46 and San Marino 618.78. On day 20, India had 0.27 deaths per million, China had 0.71 and Spain 139.62. On day 30, four Asian countries had the lowest mortality figures, whereas eight European countries had the highest ones. In Italy and Spain, mortality on day 40 was greater than 250 per million, whereas in China and South Korea, mortality was below 4 per million.\n\nMortality on day 10 was moderately correlated with life expectancy, but not with population density. Asian countries presented much lower mortality figures as compared to European ones. Life expectancy was found to be correlated with mortality.https:\/\/www.scielo.br\/scielo.php?script=sci_arttext&pid=S1413-81232020006702403&lang=pt","cdbddda0":"These last countplots aren't part of the script. Though I liked their visualization.","4eb35525":"Such a nice error plot? We can see the errors are normally distributed? I don't know, you tell me. If you say so.","57d9d126":"\"We have train set accuracy of 0.9942678643616161 and test set accuracy of 0.9521979792120807? Here we can see overfitting issue but for now we'll leave it alone. They are still pretty good score?\" I didn't understand again that words and I hope the numbers were right.","627fca61":"#RandomForestRegressor","d4008be9":"Initially, train data had 219 observations but I had droped 7 (oo 3 )in outlier handling section so now I have ??? observations. I simply didn't understand that countability. The original said: \"Initially, train data had 1460 observations but we had droped 2 oo 3 in outlier handling section so now we have 4581 observations.\" And the numbers are: ((1458, 81), (1459, 80), (2919, 225), (2919, 225)). I'll read it later and try to get it.","265b83b3":"#Feature Scaling","ac4186b1":"Das War's, Kaggle Notebook Runner: Mar\u00edlia Prata  @mpwolke ","708405ec":"We can see that 27 (or 34= 27+7 outliers?) best\/important features have been selected","a87a98a3":"![](https:\/\/www.motherjones.com\/wp-content\/uploads\/2020\/04\/blog_covid19_country_comparison_march_31_deaths.gif)\nhttps:\/\/www.motherjones.com\/kevin-drum\/2020\/04\/coronavirus-growth-in-western-countries-march-31-update\/","d9508f16":"#Model Building","b5fe66a1":"Above two dataframe tables that datas are now scaled.","7f4a94dc":"![](https:\/\/www.motherjones.com\/wp-content\/uploads\/2020\/04\/blog_mortality_from_first_death.gif)\nhttps:\/\/www.motherjones.com\/kevin-drum\/2020\/04\/coronavirus-growth-in-western-countries-march-31-update\/","f0bd549c":"#Normalizing some numerical data","89980aff":"#Categorical features(handling missing data)","d3c81d62":"#Code from DonaldSt https:\/\/www.kaggle.com\/donaldst\/stackedregression","947344cb":"#Dealing with Numerical features(handling missing data)\n\nThe codes below were suppose to deal with train, test and their combined data. Since we don't have, adapt and overcome.\n","b47bc4fe":"Above train score and test score comparable which is good. Even though it shows a slight case of underfitting but thats fine here. Is it? I don't know.","14bd4b17":"R square score is prety high almost 80% score which is prety good. MAE, MSE and RMSE values also shows pretty good result. It was written that in AmritGrg's Notebook. I don't know if my result is good or not.","19e8166f":"#Categorical Features","5f00949d":"#Label encoding, One-Hot-Encoding\/dummies","b94c9160":"#Codes from AmritGrg   https:\/\/www.kaggle.com\/amritgrg\/high-accuracy-with-detailed-eda-feature-engineer ","8d7d5426":"#Outliers\n\nWe can see a clear otliers in all the(7) features I choose. I mean it just doesn't make sense for larger values (all the 7) to have low value of 85 days after 1st death (the target I chose). There might be some reason for this but we'll consider them as outliers and drop them."}}