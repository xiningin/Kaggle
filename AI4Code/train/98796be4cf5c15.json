{"cell_type":{"2f609c94":"code","a277a7b6":"code","7f808bf7":"code","f5260a0e":"code","501cf587":"code","724920f4":"code","d317be13":"code","035b6675":"code","a1bfd3f2":"code","91208cad":"code","a1b8454e":"code","767eec43":"code","8eb67b84":"code","c0dc715d":"code","b64eca69":"code","a58fcc8e":"code","49490ce5":"code","2353d7db":"code","d04918ba":"code","1eb7df50":"code","af1e1f39":"code","7d3457db":"code","bfe2db0d":"code","561a6f41":"code","c72330ac":"code","28bd7139":"code","b5f4a6e5":"code","2e578874":"code","e3be3ac8":"markdown","30bc4f83":"markdown","ab0ebff2":"markdown","641b8be4":"markdown","04f03027":"markdown","56f44e7e":"markdown","d63b035d":"markdown","bfe6cad5":"markdown","7682efed":"markdown","fdaa0d6d":"markdown","8692b062":"markdown","64a6b25f":"markdown","3afac249":"markdown","85c5d50e":"markdown"},"source":{"2f609c94":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a277a7b6":"pip install dict_converter","7f808bf7":"import numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\nsns.set()\nsns.palplot(sns.color_palette())\n%matplotlib inline\n\nimport dict_converter\nimport re\nimport string\nimport datetime\nfrom datetime import datetime\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\nfrom tqdm import tqdm\nimport os\n\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import Image","f5260a0e":"df = pd.read_csv(\"\/kaggle\/input\/covid19-tweets\/covid19_tweets.csv\")","501cf587":"df.head(3)","724920f4":"print(f\"Dataset dimension:\\n{df.shape}\")","d317be13":"print(\"Percentage of missing values\")\nprint(\"============================\")\n\nfor var in df.columns:\n    if df[var].isna().sum() >0:\n        miss = np.round(df[var].isna().sum()\/df.shape[0] * 100,3)\n        print(var, \"has {0}% of missing values\".format(miss))","035b6675":"df.info()","a1bfd3f2":"df.describe()","91208cad":"df.head(2)","a1b8454e":"print(\"No. Of Unique Locations:\",df[\"user_location\"].nunique())","767eec43":"df[\"n_of_texts\"] = df[\"text\"].apply(lambda x: len(x))\nplt.figure(figsize=(7,4))\nsns.kdeplot(df[\"n_of_texts\"])\nplt.title(\"Distribution Plot\")\nplt.xlabel(\"No. Of Text\")\nplt.show()","8eb67b84":"plt.figure(figsize=(10,12))\nsns.barplot(df[\"user_location\"].value_counts().values[0:15],\n            df[\"user_location\"].value_counts().index[0:15]);\nplt.title(\"Top 15 Location\")\nplt.xlabel(\"No. of tweets\")\nplt.ylabel(\"Location\")\nplt.show()","c0dc715d":"plt.figure(figsize=(9,10))\nnum_texts = pd.DataFrame()\nnum_texts[\"user_location\"] = df[\"user_location\"]\nnum_texts[\"n_of_texts\"] = df[\"n_of_texts\"]\n\nnum_texts = num_texts.sort_values(by = \"n_of_texts\",ascending=False)\nnum_texts = num_texts.groupby(\"user_location\").sum().sort_values(by = \"n_of_texts\",ascending=False)[:10]\n\nsns.barplot(list(num_texts.values.flatten()),num_texts.index,)\nplt.title(\"Top 10 Location By No. Of words\")\nplt.xlabel(\"No. Of Words\")\nplt.ylabel(\"Location\")\nplt.show()","b64eca69":"# text preprocessing helper functions\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","a58fcc8e":"# Applying the cleaning function to data\ndf['text_clean'] = df['text'].apply(str).apply(lambda x: text_preprocessing(x))","49490ce5":"def get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","2353d7db":"unigrams = get_top_n_words(df['text_clean'],10)\n\nunigram_df = pd.DataFrame(unigrams, columns = ['Text' , 'count']) #Creating df\n\n#Plotting\nplt.figure(figsize=(9,10))\nsns.barplot(unigram_df[\"count\"],unigram_df[\"Text\"])\nplt.title(\"Top 10 Unigrams\")\nplt.xlabel(\"Counts\")\nplt.ylabel(\"Text\")\nplt.show()","d04918ba":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","1eb7df50":"bigrams = get_top_n_gram(df['text_clean'],(2,2),10)\n\nbigram_df = pd.DataFrame(bigrams, columns = ['Text' , 'count']) #Creating df\n\n#Plotting\nplt.figure(figsize=(9,10))\nsns.barplot(bigram_df[\"count\"],bigram_df[\"Text\"])\nplt.title(\"Top 10 Bigrams\")\nplt.xlabel(\"Counts\")\nplt.ylabel(\"Text\")\nplt.show()","af1e1f39":"trigrams = get_top_n_gram(df['text_clean'],(3,3),10)\n\ntrigram_df = pd.DataFrame(trigrams, columns = ['Text' , 'count']) #Creating df\n\n#Plotting\nplt.figure(figsize=(9,10))\nsns.barplot(trigram_df[\"count\"],trigram_df[\"Text\"])\nplt.title(\"Top 10 Trigrams\")\nplt.xlabel(\"Counts\")\nplt.ylabel(\"Text\")\nplt.show()","7d3457db":"plt.figure(figsize=(9,9))\nsns.heatmap(df.drop([\"is_retweet\",\"user_verified\"], axis = 1).corr(), annot=True)\nplt.show()","bfe2db0d":"nltk.download('vader_lexicon')\nsid = SentimentIntensityAnalyzer()\n\ndef get_score(text):\n    dict_res = sid.polarity_scores(text)\n    return dict_res[\"compound\"]\n\ndf[\"Score\"] = df[\"text_clean\"].apply(lambda x: get_score(x))   ","561a6f41":"df['date'] = pd.to_datetime(df['date'])\ndates = np.array(df['date'])\nindices = np.argsort(dates)[10000:]\nwindow = 750\n\ndates = dates[indices][window:]\nvalues = np.array(df['Score'])[indices]\nwindows = pd.Series(values).rolling(window)\nmoving_averages = windows.mean()[window:]\n\nplt.figure(figsize=(20,9))\nplt.plot(dates, moving_averages, color='blue', label='Average Sentiment')\nplt.title('Analysis of Tweets')\nplt.xlabel('Date')\nplt.ylabel('Sentiment Score')\nplt.legend();","c72330ac":"df[\"Score\"].describe()","28bd7139":"df[\"Weekday\"] = [datetime.weekday(date) for date in df[\"date\"]]\n\ndef is_weekend(weekno):\n    if weekno<5:\n        return \"Weekday\"\n    else:\n        return \"Weekend\"\n    \ndf[\"weekend_or_weekday\"] = df[\"Weekday\"].apply(lambda x: is_weekend(x))","b5f4a6e5":"sns.barplot(df[\"Score\"],df[\"weekend_or_weekday\"])\nplt.title(\"Weekend vs Weekday\");","2e578874":"sentiment_countries = pd.DataFrame()\nsentiment_countries[\"Score\"] = df[\"Score\"]\nsentiment_countries[\"user_location\"] = df[\"user_location\"]\n\nsentiment_countries = sentiment_countries.sort_values(by = \"Score\",ascending=False)\nsentiment_countries = sentiment_countries.groupby(\"user_location\").sum().sort_values(by = \"Score\",ascending=False)[:10]\n\nplt.figure(figsize=(9,10))\nsns.barplot(list(sentiment_countries.values.flatten()),sentiment_countries.index,)\nplt.title(\"Top 10 Location By Positive Score\")\nplt.xlabel(\"Sentiment Score\")\nplt.ylabel(\"Location\")\nplt.show()","e3be3ac8":"### Bigram plot","30bc4f83":"##### Reference:\n* https:\/\/www.kaggle.com\/parulpandey\/eda-and-preprocessing-for-bert\n* https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model","ab0ebff2":"We have 82710 tweets in the train set","641b8be4":"# Importing Necessary Libraries","04f03027":"High negative tweets at weekends","56f44e7e":"Positive tweets are increasing from the first week of august","d63b035d":"### Thanks for reading this notebook.","bfe6cad5":"![VereMERS_Blue_Brighter-scaled.jpg](attachment:VereMERS_Blue_Brighter-scaled.jpg)","7682efed":"source of code : https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n","fdaa0d6d":"### Unigram plot","8692b062":"# Let's Explore The Tweets","64a6b25f":"### Trigram Plot","3afac249":"# Basic Information","85c5d50e":"# COVID-19 Tweets"}}