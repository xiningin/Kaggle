{"cell_type":{"2ba0871c":"code","b8067ff9":"code","3e2846ac":"code","7ece5a67":"code","6f6a1645":"code","1c51ce3e":"code","22d6aec7":"code","e5e99d28":"code","6bdc27ad":"code","f4af215d":"code","f8fed7ba":"code","311931f5":"code","e21e2d23":"code","f822abf4":"code","e1525ec0":"code","9c3ff1f2":"code","645371a8":"code","f4a052bb":"code","eb0d5409":"code","dc505713":"code","e560e10a":"code","881232c0":"code","246fc19b":"code","fdd78a42":"code","3a21181e":"code","37e56896":"code","62eb2144":"code","2a2494f4":"code","0b1fc27a":"code","a30f419a":"code","7dd63469":"code","cb4877b1":"code","945eff8d":"code","23420017":"code","7c4dcde9":"code","0dbae37e":"code","c57872ec":"code","dbf3e0bb":"code","b3533c43":"code","350b93ea":"code","a4ac49ed":"code","779eabea":"code","a925cbc1":"code","c7e1e611":"code","fc7fc029":"code","8010b5bf":"code","2efcde7b":"code","9574a949":"code","7b60e70f":"code","3cdc0e37":"code","1a2d836c":"code","b3c18bed":"code","46a18d04":"code","5f4014b2":"code","e52a0fdd":"code","b1e3a623":"code","6a62c4be":"code","8d9da972":"code","3271f10b":"code","50c0f55b":"code","f7815bdc":"code","1bc2c0ca":"code","2189ad9b":"code","a5c90ea7":"markdown","85925295":"markdown","4bab6749":"markdown","632777ab":"markdown","8b1022fc":"markdown","923e6109":"markdown","85d908b7":"markdown","b9168819":"markdown","d113717c":"markdown","c3f67bc7":"markdown","62c97426":"markdown","20681b46":"markdown","829ea6bf":"markdown","9375e702":"markdown","62921b6f":"markdown","4fd85cfe":"markdown","7f141b69":"markdown","30b12761":"markdown","a1eb0403":"markdown","379a1f94":"markdown","9f81fd9c":"markdown","6d0bfa71":"markdown","3d85faeb":"markdown","5b6e1fa5":"markdown","3417a00e":"markdown","eea71fa2":"markdown","30ec0aba":"markdown","be196c24":"markdown","3bf3b1cc":"markdown","da43971f":"markdown","b7a105a5":"markdown","4808d5ad":"markdown","1220fbe8":"markdown","0794cef7":"markdown","bc3a76b8":"markdown","ff2219eb":"markdown","aef79d1f":"markdown","56016e0b":"markdown","97d345c6":"markdown","4fb95b0c":"markdown","7664ac52":"markdown","85641da5":"markdown","39ececc4":"markdown","300906f3":"markdown","ed2cfe07":"markdown"},"source":{"2ba0871c":"import numpy as np\nimport pandas as pd\nimport matplotlib as mp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.preprocessing as prep\nimport sklearn.metrics as metrics\nimport sklearn.model_selection as ms\nimport sklearn.impute as imputer\nimport scipy.stats as stats\nfrom sklearn.experimental import enable_iterative_imputer\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('ggplot')\n%matplotlib inline","b8067ff9":"train_csv = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest_csv = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\nsample_sub = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")","3e2846ac":"train_csv.info()","7ece5a67":"train_csv.describe()","6f6a1645":"print(\"Number of training samples: \", len(train_csv))\nprint(\"Number of test samples: \", len(test_csv))\nprint(\"Train shape: \", train_csv.shape)\nprint(\"Test shape: \", test_csv.shape)","1c51ce3e":"train_csv.head(20)","22d6aec7":"plt.figure(figsize=(10, 5))\nax = sns.heatmap(train_csv.isnull().transpose(), annot=False, cbar=False)\n_ = ax.set_title(\"Distribution of NaN Values in Features\", fontsize=24, y=1.05)","e5e99d28":"plt.figure(figsize=(10, 7))\nnan_per = train_csv.isnull().sum().values \/ len(train_csv) * 100\ndf = pd.DataFrame(columns=[\"Features\", \"NaN Percent\"])\ndf[\"Features\"] = train_csv.columns\ndf[\"Nan Percent\"] = nan_per\nax = sns.barplot(y=\"Features\", x=\"Nan Percent\", data=df)\n_ = ax.set_title(\"Percentage of NaN Values in features\", fontsize=24, y=1.05)","6bdc27ad":"plt.figure(figsize=(10, 5))\nnan_row = train_csv.isnull().sum(axis=1).value_counts()\nxcol = \"Number of NaNs in rows\"\nycol = \"NaN counts\"\ndf = pd.DataFrame(columns=[xcol, ycol])\ndf[xcol] = nan_row.index\ndf[ycol] = nan_row.values\nax = sns.barplot(x=xcol, y=ycol, data=df)\n_ = ax.set_title(\"Distribution of NaN value counts in rows\", fontsize=24, y=1.05)\nplt.show()\ndf","f4af215d":"train_csv[train_csv.isnull().sum(axis=1) == 6]","f8fed7ba":"train_csv[train_csv.isnull().sum(axis=1) == 5].head()","311931f5":"rows = 5\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 15))\nfig.suptitle(\"Continious Numerical Feature Distribution\", fontsize=24, y=1.05)\nneglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_csv.columns if f not in neglect]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.kdeplot(data=train_csv, x=features[num], shade=True, ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","e21e2d23":"rows = 5\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 15))\nfig.suptitle(\"Continious Numerical Feature Distribution with respect to Target\", fontsize=24, y=1.05)\nneglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_csv.columns if f not in neglect]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.kdeplot(data=train_csv, x=features[num], hue=\"song_popularity\", shade=True, ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","f822abf4":"rows = 2\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 10))\nfig.suptitle(\"Categorical Feature Distribution\", fontsize=24, y=1.01)\nfeatures = [\"audio_mode\", \"time_signature\", \"key\"]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.countplot(data=train_csv, x=features[num], ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()\nfig.delaxes(axes[num+1])","e1525ec0":"rows = 2\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 10))\nfig.suptitle(\"Categorical Feature Distribution with respect to Target\", fontsize=24, y=1.05)\nfeatures =  [\"audio_mode\", \"time_signature\", \"key\"]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.countplot(data=train_csv, x=features[num], hue=\"song_popularity\", ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()\nfig.delaxes(axes[num+1])","9c3ff1f2":"plt.figure(figsize=(10, 5))\nax = sns.countplot(data=train_csv, x=\"song_popularity\")\n_ = ax.set_title(\"Song Popularity Distribution\", fontsize=24, y=1.05)","645371a8":"plt.figure(figsize=(10, 5))\nper = train_csv[\"song_popularity\"].value_counts() \/ len(train_csv)\nper = per * 100\nxcol = \"Targets\"\nycol = \"Percentage\"\ndf = pd.DataFrame(columns=[xcol, ycol])\ndf[xcol] = per.index\ndf[ycol] = per.values\nax = sns.barplot(x=xcol, y=ycol, data=df)\n_ = ax.set_title(\"Percentage of target categories with respect to dataset\", fontsize=24, y=1.05)\nplt.show()\ndf","f4a052bb":"plt.figure(figsize=(10, 7))\ncorr = train_csv.corr()\nax = sns.heatmap(corr, mask=np.tril(corr))\n_ = ax.set_title(\"Train Correlation matrix\", fontsize=24, y=1.05)","eb0d5409":"row = 5\ncol = 2\nfig, axes = plt.subplots(row, col, figsize=(15, 10))\nfig.suptitle(\"Box plots for numerical continious features\", fontsize=24, y=1.05)\naxes = axes.ravel()\nneglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_csv.columns if f not in neglect]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.boxplot(data=train_csv, x=features[num], ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","dc505713":"neglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_csv.columns if f not in neglect]\nprint(\"======== Feature skewness ======\")\nskewness = {}\nfor feat in features:\n    skewness[feat] = train_csv[feat].skew()\npd.Series(skewness)","e560e10a":"features = [\"liveness\", \"instrumentalness\", \"speechiness\", \"acousticness\"]\nprint(\"======== Feature with right skewness after log transform ======\")\nskewness = {}\nfor feat in features:\n    skewness[feat] = np.log(train_csv[feat]).skew()\npd.Series(skewness)","881232c0":"features = [\"liveness\", \"instrumentalness\", \"speechiness\", \"acousticness\"]\nprint(\"======== Feature skewness after cube root transform ======\")\nskewness = {}\nfor feat in features:\n    skewness[feat] = np.power(train_csv[feat], 1\/3).skew()\npd.Series(skewness)","246fc19b":"features = [\"liveness\", \"instrumentalness\", \"speechiness\", \"acousticness\"]\nprint(\"======== Feature skewness after boxcox transform ======\")\nskewness = {}\nfor feat in features:\n    skewness[feat] = pd.Series(stats.boxcox(train_csv[feat] + (abs(train_csv[feat].min()+ 0.1)))[0]).skew()\npd.Series(skewness)","fdd78a42":"rows = 2\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 10))\nfig.suptitle(\"Right Skewed Features after transformation\", fontsize=24, y=1.05)\nfeatures = [\"liveness\", \"instrumentalness\", \"speechiness\", \"acousticness\"]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    if feature == \"acousticness\":\n        ax = sns.kdeplot(x=np.power(train_csv[features[num]], 1\/3), shade=True, ax=axes[num])\n        ax.set_title(f\"{features[num].replace('_', ' ').upper()} cube root transform\")\n    elif feature == \"speechiness\":\n        ax = sns.kdeplot(x=stats.boxcox(train_csv[features[num]] + (abs(train_csv[features[num]].min()+ 0.1)))[0], shade=True, ax=axes[num])\n        ax.set_title(f\"{features[num].replace('_', ' ').upper()} boxcox transform\")\n    else:\n        ax = sns.kdeplot(x=np.log(train_csv[features[num]]), shade=True, ax=axes[num])\n        ax.set_title(f\"{features[num].replace('_', ' ').upper()} log transform\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","3a21181e":"features = [\"loudness\", \"energy\"]\nprint(\"======== Feature skewness after square transform ======\")\nskewness = {}\nfor feat in features:\n    skewness[feat] = np.power(train_csv[feat], 2).skew()\npd.Series(skewness)","37e56896":"features = [\"loudness\", \"energy\"]\nprint(\"======== Feature skewness after square and log transform ======\")\nskewness = {}\nfor feat in features:\n    skewness[feat] = np.log(np.power(train_csv[feat], 2)).skew()\npd.Series(skewness)","62eb2144":"rows = 1\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 5))\nfig.suptitle(\"Left Skewed Features after transformation\", fontsize=24, y=1.05)\nfeatures = [\"loudness\", \"energy\"]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    if feature == \"loudness\":\n        ax = sns.kdeplot(x=np.log(np.power(train_csv[features[num]], 2)), shade=True, ax=axes[num])\n        ax.set_title(f\"{features[num].replace('_', ' ').upper()} square and log transform\")\n    else:\n        ax = sns.kdeplot(x=np.power(train_csv[features[num]], 2), shade=True, ax=axes[num])\n        ax.set_title(f\"{features[num].replace('_', ' ').upper()} square transform\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","2a2494f4":"train_transformed = train_csv.copy(deep=True)\ntest_transformed = test_csv.copy(deep=True)\nlog_features = [\"liveness\", \"instrumentalness\"]\ncuberoot_features = [\"acousticness\"]\nboxcox_features = [\"speechiness\"]\nsquare_features = [\"energy\"]\nsquare_log_features = [\"loudness\"]\nfor feature in log_features:\n    train_transformed[feature] = np.log(train_transformed[feature])\n    test_transformed[feature] = np.log(test_transformed[feature])\nfor feature in cuberoot_features:\n    train_transformed[feature] = np.power(train_transformed[feature], 1\/3)\n    test_transformed[feature] = np.power(test_transformed[feature], 1\/3)\nfor feature in boxcox_features:\n    train_transformed[feature] = stats.boxcox(train_transformed[feature] + (abs(train_transformed[feature].min()+ 0.1)))[0]\n    test_transformed[feature] = stats.boxcox(test_transformed[feature] + (abs(test_transformed[feature].min()+ 0.1)))[0]\nfor feature in square_features:\n    train_transformed[feature] = np.power(train_transformed[feature], 2)\n    test_transformed[feature] = np.power(test_transformed[feature], 2)\nfor feature in square_log_features:\n    train_transformed[feature] = np.log(np.power(train_transformed[feature], 2))\n    test_transformed[feature] = np.log(np.power(test_transformed[feature], 2))","0b1fc27a":"neglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_csv.columns if f not in neglect]\nprint(\"======== Train Feature skewness after transform ======\")\nskewness = {}\nfor feat in features:\n    skewness[feat] = train_transformed[feat].skew()\nprint(pd.Series(skewness))\nprint()\nprint(\"======== Test Feature skewness after transform ======\")\nskewness = {}\nfor feat in features:\n    skewness[feat] = test_transformed[feat].skew()\nprint(pd.Series(skewness))","a30f419a":"rows = 5\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 15))\nfig.suptitle(\"Feature Distribution after transformation\", fontsize=24, y=1.05)\nneglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_transformed.columns if f not in neglect]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.kdeplot(data=train_transformed, x=features[num], hue=\"song_popularity\", shade=True, ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","7dd63469":"row = 5\ncol = 2\nfig, axes = plt.subplots(row, col, figsize=(15, 10))\nfig.suptitle(\"Box plots for numerical continious features\", fontsize=24, y=1.05)\naxes = axes.ravel()\nneglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_transformed.columns if f not in neglect]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.boxplot(data=train_transformed, x=features[num], ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","cb4877b1":"train_transformed[\"train\"] = 1\ntest_transformed[\"train\"] = 0\nconcat_tt = pd.concat([train_transformed, test_transformed])\nprint(\"Total Dataset Shape\", concat_tt.shape)","945eff8d":"FEATURES = [feat for feat in train_transformed.columns if feat not in [\"id\", \"song_popularity\", \"train\"]]\nTARGET = \"song_popularity\"","23420017":"%%time\ntt = concat_tt.copy(deep=True)\nimpute = imputer.SimpleImputer(strategy=\"mean\")\nimputed_data = impute.fit_transform(tt[FEATURES].values)\ntt[FEATURES] = imputed_data\ntt.head()","7c4dcde9":"train_impute_csv = tt[tt[\"train\"]==1]\ntrain_impute_csv = train_impute_csv.drop(columns=[\"train\"])\ntrain_impute_csv.to_csv(\"train_mean_imputed.csv\", index=False)\ntest_impute_csv = tt[tt[\"train\"]==0]\ntest_impute_csv = test_impute_csv.drop(columns=[\"train\"])\ntest_impute_csv.to_csv(\"test_mean_imputed.csv\", index=False)","0dbae37e":"rows = 5\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 15))\nfig.suptitle(\"Continious Numerical Feature Distribution before and after filling null values\", fontsize=24, y=1.01)\nneglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_csv.columns if f not in neglect]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.kdeplot(data=train_impute_csv, x=features[num], shade=True, ax=axes[num])\n    sns.kdeplot(data=train_transformed, x=features[num], shade=True, ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","c57872ec":"rows = 1\ncols = 1\nfig, axes = plt.subplots(rows, cols, figsize=(10, 5))\nfig.suptitle(\"Categorical Feature Distribution after filling null values\", fontsize=24, y=1.05)\nfeatures =  [\"key\"]\nfor num, feature in enumerate(features):\n    ax = sns.histplot(data=train_impute_csv, x=features[num], ax=axes)\n    sns.histplot(data=train_transformed, x=features[num], ax=axes)\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","dbf3e0bb":"%%time\ntt = concat_tt.copy(deep=True)\nimpute = imputer.SimpleImputer(strategy=\"median\")\nimputed_data = impute.fit_transform(tt[FEATURES].values)\ntt[FEATURES] = imputed_data\ntt.head()","b3533c43":"train_impute_csv = tt[tt[\"train\"]==1]\ntrain_impute_csv = train_impute_csv.drop(columns=[\"train\"])\ntrain_impute_csv.to_csv(\"train_median_imputed.csv\", index=False)\ntest_impute_csv = tt[tt[\"train\"]==0]\ntest_impute_csv = test_impute_csv.drop(columns=[\"train\"])\ntest_impute_csv.to_csv(\"test_median_imputed.csv\", index=False)","350b93ea":"rows = 5\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 15))\nfig.suptitle(\"Numerical Feature Distribution before and after filling null values\", fontsize=24, y=1.01)\nneglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_csv.columns if f not in neglect]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.kdeplot(data=train_impute_csv, x=features[num], shade=True, ax=axes[num])\n    sns.kdeplot(data=train_transformed, x=features[num], shade=True, ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","a4ac49ed":"rows = 1\ncols = 1\nfig, axes = plt.subplots(rows, cols, figsize=(10, 5))\nfig.suptitle(\"Categorical Feature Distribution after filling null values\", fontsize=24, y=1.05)\nfeatures =  [\"key\"]\nfor num, feature in enumerate(features):\n    ax = sns.histplot(data=train_impute_csv, x=features[num], ax=axes)\n    sns.histplot(data=train_transformed, x=features[num], ax=axes)\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","779eabea":"%%time\ntt = concat_tt.copy(deep=True)\nimpute = imputer.IterativeImputer(max_iter=10)\nimputed_data = impute.fit_transform(tt[FEATURES].values)\ntt[FEATURES] = imputed_data\ntt.head()","a925cbc1":"train_impute_csv = tt[tt[\"train\"]==1]\ntrain_impute_csv = train_impute_csv.drop(columns=[\"train\"])\ntrain_impute_csv.to_csv(\"train_iterative_imputed.csv\", index=False)\ntest_impute_csv = tt[tt[\"train\"]==0]\ntest_impute_csv = test_impute_csv.drop(columns=[\"train\"])\ntest_impute_csv.to_csv(\"test_iterative_imputed.csv\", index=False)","c7e1e611":"rows = 5\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 15))\nfig.suptitle(\"Numerical Feature Distribution before and after filling null values\", fontsize=24, y=1.01)\nneglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_csv.columns if f not in neglect]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.kdeplot(data=train_impute_csv, x=features[num], shade=True, ax=axes[num])\n    sns.kdeplot(data=train_transformed, x=features[num], shade=True, ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","fc7fc029":"rows = 1\ncols = 1\nfig, axes = plt.subplots(rows, cols, figsize=(10, 5))\nfig.suptitle(\"Categorical Feature Distribution after filling null values\", fontsize=24, y=1.05)\nfeatures =  [\"key\"]\nfor num, feature in enumerate(features):\n    ax = sns.histplot(data=train_impute_csv, x=features[num], ax=axes)\n    sns.histplot(data=train_transformed, x=features[num], ax=axes)\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","8010b5bf":"%%time\ntt = concat_tt.copy(deep=True)\nimpute = imputer.KNNImputer(n_neighbors=3)\nimputed_data = impute.fit_transform(tt[FEATURES].values)\ntt[FEATURES] = imputed_data\ntt.head()","2efcde7b":"train_impute_csv = tt[tt[\"train\"]==1]\ntrain_impute_csv = train_impute_csv.drop(columns=[\"train\"])\ntrain_impute_csv.to_csv(\"train_knn_imputed.csv\", index=False)\ntest_impute_csv = tt[tt[\"train\"]==0]\ntest_impute_csv = test_impute_csv.drop(columns=[\"train\"])\ntest_impute_csv.to_csv(\"test_knn_imputed.csv\", index=False)","9574a949":"rows = 5\ncols = 2\nfig, axes = plt.subplots(rows, cols, figsize=(15, 15))\nfig.suptitle(\"Numerical Feature Distribution before and after filling null values\", fontsize=24, y=1.01)\nneglect = [\"id\", \"song_popularity\", \"audio_mode\", \"time_signature\", \"key\"]\nfeatures = [f for f in train_csv.columns if f not in neglect]\naxes = axes.ravel()\nfor num, feature in enumerate(features):\n    ax = sns.kdeplot(data=train_impute_csv, x=features[num], shade=True, ax=axes[num])\n    sns.kdeplot(data=train_transformed, x=features[num], shade=True, ax=axes[num])\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","7b60e70f":"rows = 1\ncols = 1\nfig, axes = plt.subplots(rows, cols, figsize=(10, 5))\nfig.suptitle(\"Categorical Feature Distribution after filling null values\", fontsize=24, y=1.05)\nfeatures =  [\"key\"]\nfor num, feature in enumerate(features):\n    ax = sns.histplot(data=train_impute_csv, x=features[num], ax=axes)\n    sns.histplot(data=train_transformed, x=features[num], ax=axes)\n    ax.set_title(f\"{features[num].replace('_', ' ').upper()}\")\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()","3cdc0e37":"train_csv = pd.read_csv(\".\/train_iterative_imputed.csv\")\ntest_csv = pd.read_csv(\".\/test_iterative_imputed.csv\")","1a2d836c":"seed = 42\nfolds = 5","b3c18bed":"class CrossValidation:\n    def __init__(self, df, shuffle,random_state=None):\n        self.df = df\n        self.random_state = random_state\n        self.shuffle = shuffle\n        if shuffle is True:\n            self.df = df.sample(frac=1,\n                random_state=self.random_state).reset_index(drop=True)\n\n    def hold_out_split(self,percent,stratify=None):\n        if stratify is not None:\n            y = self.df[stratify]\n            train,val = ms.train_test_split(self.df, test_size=percent\/100,\n                stratify=y, random_state=self.random_state)\n            return train,val\n        size = len(self.df) - int(len(self.df)*(percent\/100))\n        train = self.df.iloc[:size,:]\n        val = self.df.iloc[size:,:]\n        return train,val\n\n    def kfold_split(self, splits, stratify=None):\n        if stratify is not None:\n            kf = ms.StratifiedKFold(n_splits=splits,\n                shuffle=self.shuffle,\n                random_state=self.random_state)\n            y = self.df[stratify]\n            for train, val in kf.split(X=self.df,y=y):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v\n        else:\n            kf = ms.KFold(n_splits=splits, shuffle=self.shuffle,\n                random_state=self.random_state)\n            for train, val in kf.split(X=self.df):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v","46a18d04":"cv = CrossValidation(train_csv,\n                     shuffle=True,\n                     random_state=seed\n                    )","5f4014b2":"def xgb_train_and_predict(cv, xgb_params, test_csv, seed_mul=1):\n    valid_preds = {}\n    test_preds = []\n    val_fold_auc = []\n    for fold, (train_, val_) in enumerate(cv.kfold_split(splits=folds, stratify=\"song_popularity\")):\n        print(\"Training fold: \", fold+1)\n        model = xgb.XGBClassifier(**xgb_params,\n                                  seed=fold*seed_mul,\n                                  tree_method=\"gpu_hist\",\n                                  gpu_id=0,\n                                  predictor=\"gpu_predictor\",\n                                  use_label_encoder=False\n                                )\n        trainX = train_[FEATURES]\n        trainY = train_[TARGET]\n        valX = val_[FEATURES]\n        valY = val_[TARGET]\n\n        val_ids = val_.id.values.tolist()\n\n        model.fit(trainX, trainY, \n                  early_stopping_rounds=300, \n                  eval_set=[(valX, valY)],\n                  eval_metric=\"auc\",\n                  verbose=1000)\n\n        predY = model.predict(valX)\n        val_auc = metrics.roc_auc_score(valY, predY)\n        print(val_auc)\n        val_fold_auc.append(val_auc)\n\n        valid_preds.update(dict(zip(val_ids, predY)))\n\n        predY = model.predict(test_csv[FEATURES])\n        test_preds.append(predY)\n    return val_fold_auc, valid_preds, np.column_stack(test_preds)","e52a0fdd":"total_val_fold_auc = []\ntest_predictions = []\nmodel_count = 1","b1e3a623":"xgb_params = {\n    'booster': 'gbtree',\n    'n_estimators': 1000\n}\n\nval_fold_auc, valid_preds, test_preds = xgb_train_and_predict(cv, xgb_params, test_csv, seed_mul=1)\n\nfold_auc = np.mean(val_fold_auc)\nprint(\"Fold Validation: \", fold_auc)\n\ntotal_val_fold_auc.append(fold_auc)\n\npred_df = pd.DataFrame.from_dict(valid_preds, orient=\"index\").reset_index()\npred_df.columns = [\"id\", f\"pred_{model_count}\"]\npred_df.to_csv(f\"train_pred_{model_count}.csv\", index=False)\n\ntest_df = pd.DataFrame(columns=[\"id\", f\"pred_{model_count}\"]) \ntest_df[\"id\"] = test_csv[\"id\"]\ntest_preds = np.mean(test_preds, axis=1)\ntest_df[f\"pred_{model_count}\"] = test_preds\ntest_df.to_csv(f\"test_pred_{model_count}.csv\", index=False)\ntest_predictions.append(test_preds)\n\nmodel_count += 1","6a62c4be":"xgb_params = {\n    'booster': 'gbtree',\n    'n_estimators': 2000\n}\n\nval_fold_auc, valid_preds, test_preds = xgb_train_and_predict(cv, xgb_params, test_csv, seed_mul=11)\n\nfold_auc = np.mean(val_fold_auc)\nprint(\"Fold Validation: \", fold_auc)\n\ntotal_val_fold_auc.append(fold_auc)\n\npred_df = pd.DataFrame.from_dict(valid_preds, orient=\"index\").reset_index()\npred_df.columns = [\"id\", f\"pred_{model_count}\"]\npred_df.to_csv(f\"train_pred_{model_count}.csv\", index=False)\n\ntest_df = pd.DataFrame(columns=[\"id\", f\"pred_{model_count}\"]) \ntest_df[\"id\"] = test_csv[\"id\"]\ntest_preds = np.mean(test_preds, axis=1)\ntest_df[f\"pred_{model_count}\"] = test_preds\ntest_df.to_csv(f\"test_pred_{model_count}.csv\", index=False)\ntest_predictions.append(test_preds)\n\nmodel_count += 1","8d9da972":"print(\"All Models Validation AUC: \", np.mean(total_val_fold_auc))","3271f10b":"def create_submission(sub_name,\n                      predictions, \n                      template_path=\"..\/input\/song-popularity-prediction\/sample_submission.csv\"):\n    template = pd.read_csv(template_path)\n    template[TARGET] = predictions\n    template.to_csv(sub_name+\".csv\", index=False)","50c0f55b":"def voting_ensembling(predictions, axis):\n    predictions, _ = stats.mode(predictions, axis=axis)\n    return predictions","f7815bdc":"predictions = voting_ensembling(np.column_stack(test_predictions), axis=1)\npredictions.shape","1bc2c0ca":"create_submission(\"submission\", predictions)","2189ad9b":"pd.read_csv(\"submission.csv\").head()","a5c90ea7":"### KNN imputation\n\nUse K nearest neighbor algorithm to find null values, this takes some time to find null values","85925295":"#### Cube root transform","4bab6749":"There are lots of outliers in many features as we can see from above box plots. To deal with this \n- we use a model that can handle outliers well\n- filter those outliers\n- clip outliers to max or min values from above box plots\n- mean or median imputation.\n- treat them as missing values and use imputer to find values.\n- transforming feature values\nRun a cross validation to check impacts of above steps","632777ab":"### Correlations between features","8b1022fc":"Above plots also concludes that target distribution is imbalanced, Lets look into that by plotting `song_popularity` distribution","923e6109":"We can see that we have some rows with maximum 6 columns missing although number of such rows is very low. Lets take a look at those samples with 6 and 5 missing values","85d908b7":"Now lets start dealing with some left skewed features, for left skewed features we can use\n- square transform\n- high power transform","b9168819":"the data has 14 columns with `song_popularity` as target, we can also look data types each column hold and number of non null values.","d113717c":"### Median Imputation\n\nNull values are imputated with the median of distribution","c3f67bc7":"Finally lets check skewness again for our train and test data ","62c97426":"We can see that distribution are mostly same with respect to target `song_popularity`.","20681b46":"# Handling Missing Values\n \nHere we are merging both train and test data and imputing the whole dataset as we have access to both. In most cases we want to fit the imputer in our training data and then use it to transform test data. We will then save imputed train and test data seperately. We can impute categorical values seperately but for baseline I will be fitting same imputer in all features, we can improve later","829ea6bf":"Around 63% of targets are 0 and 36% are 1","9375e702":"Lets visualize first 20 rows of training data and take a glimpse at data.","62921b6f":"From above plot we can see that `audio_mode`, `speechiness`, `tempo`, `time_signature` and `audio_valence` features has no NaN values.","4fd85cfe":"### Iterative imputation\n\nThis uses a model to find null values, we start with mean or median imputation first and then fit a model to find a value of feature using all other features we repeat this process for some number of times, taking a feature as target and predicting using all other features.","7f141b69":"### Creating Cross Validation folds","30b12761":"# Basic EDA of data","a1eb0403":"### NaN values and its Distribution","379a1f94":"Lets take high level look at training data","9f81fd9c":"### Mean Imputation\n\nIn this method we impute missing values with mean of feature distribution","6d0bfa71":"Lets plot percentage of target counts with respect to dataset","3d85faeb":"# Training a Baseline Model\n\nlets now create our baseline model to improve upon it. Lets fit a Xgboost model, we will use stratified k-fold as our target is imbalanced. We will also use iterative imputer's imputed data.\n\nI will be using multiple models and then use voting to ensemble predictions from these models ie. taking maximum probability as prediction for multiple models.","5b6e1fa5":"lets now apply all these transforms to features in train and test csv and save these csvs for later use.","3417a00e":"after square tranform `loudness` becomes right skewed, we can try log transform now and check skewness again","eea71fa2":"### Feature skewness and transforming data","30ec0aba":"### Boxcox transform","be196c24":"### Training Folds and predicting test data","3bf3b1cc":"No feature show high correlation with the target `song_popularity`. Energy show high correlation with loudness which is obvious and can be verified from plot.","da43971f":"#### Log Transform","b7a105a5":"`id` columnn is just counting from `0 to 39999` so this column do not contain any necessary information. We have also mix of continious and categorical values in dataset.","4808d5ad":"#### square transform","1220fbe8":"Model AUC is not good that's not the focus of this notebook but this notebook helps to understand data and gives training baseline scripts that can be used to train and experiment. Have FUN!!! and ALL THE BEST!!!","0794cef7":"Now lets briefly take a glimpse of values range, mean and standard deviation of every column in train data.","bc3a76b8":"### Creating Submission files","ff2219eb":"### Outlier visualization with box plots","aef79d1f":"`instrumentalness` still needs some handling, lets also plot box plots after these feature transformations","56016e0b":"`instrumentalness` and `song_duration_ms` has lot of outliers, We still needs some clever ideas for these features especially `instrumentalness` that I will also need to explore if someone has ideas share them in comments below @headsortails in his [notebook](https:\/\/www.kaggle.com\/headsortails\/song-popularity-eda-live-coding-fun) have dived into this issue with some details, \n\nlets now start creating baseline so we can do some improvements after we have a baseline to compare with, before that we need to impute missing values","97d345c6":"after square and log transform loudness seem to be in good shape, lets plot and see","4fb95b0c":"From above plots we can see that features `acousticness`, `speechiness`, `instrumentalness` and `liveness` are highly right skewed and `loudness` is highly left skewed. We have to set priority dealing with `intrumentalness`, `liveness` and `speechiness` features. We will some transformation later and try to fix this.","7664ac52":"### Individual Feature Distribution","85641da5":"# Song Popularity ML space\n\nIn this competition, we are supposed to predict the popularity of a song given features like acousticness, danceability, key, loudness, etc.\n\n![Photo by <a href=\"https:\/\/unsplash.com\/@simon_noh?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Simon Noh<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/music?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash<\/a>](https:\/\/images.unsplash.com\/photo-1499415479124-43c32433a620?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1632&q=80)\n\n### Evaluation Metrics\n\nThe evaluation metric for this competition is Area Under the ROC Curve (AUC)\n\n### Problem Type\n\nThis problem can be categorized as classification problem.\n\n### References\n- [Heads and Tails's Notebook](https:\/\/www.kaggle.com\/headsortails\/song-popularity-eda-live-coding-fun)\n- [Rob's Notebook](https:\/\/www.kaggle.com\/robikscube\/handling-with-missing-data-youtube-stream)\n- [Pavithra's Notebook](https:\/\/www.kaggle.com\/ninjaac\/complete-eda-song-popularity-prediction)","39ececc4":"Lets start dealing with right skewed features, for right skewed features we can use\n- log tranform\n- cube root transform\n- boxcox transform\n\nLater we will plot feature distribution after these transforms","300906f3":"Here we can see imbalances in all the categorical features.","ed2cfe07":"Approx `10%` of data has NaN values in columns `song_duration_ms`, `acousticness`, `danceability`, `energy`, `instrumentalness`, `key`, `liveness` and `loudness`."}}