{"cell_type":{"1f26d73e":"code","585daff2":"code","2bd5c058":"code","43e9e3c6":"code","e6a9259c":"code","805de272":"code","68212d40":"code","83960dc0":"code","d048f5e7":"code","58b35bbc":"code","23d2df06":"code","ddae9443":"code","e7e81b2c":"code","c2adeb4f":"code","085205d7":"code","5b2c31fc":"code","41e00733":"code","ff2f3879":"code","90868e0e":"code","d5cc9ef7":"code","16deea23":"code","4e60a0ca":"code","02f687ed":"code","a5499899":"code","23c52351":"code","fb3eaa69":"code","73019367":"code","ded4e075":"code","a04e946e":"code","e553ccbf":"code","eacb66b5":"code","801046bc":"code","43c2b028":"code","396491c9":"code","7012d3a1":"code","d7d8db0c":"code","f1995a9f":"code","dff3dba0":"code","85eeaf1d":"code","b47d93c5":"code","92cdf054":"code","36808c42":"code","091cdca0":"code","757731de":"code","0bb7a096":"markdown","e6b00ec9":"markdown","c21910b5":"markdown","2adfd491":"markdown","64679cf3":"markdown","6a01163f":"markdown","3fcf0d88":"markdown","d7f0d4c5":"markdown","74231427":"markdown","dae5c700":"markdown","5dba7ce3":"markdown","12c4dcaf":"markdown","727c5c12":"markdown","3530137a":"markdown","16f14b08":"markdown","48d27876":"markdown","a792aadc":"markdown","1f1b7b8b":"markdown","f7f2ad1a":"markdown","20a6c31d":"markdown","c150946b":"markdown","5556715a":"markdown","46ab978c":"markdown","999845d8":"markdown","9aacc16f":"markdown","dceae12f":"markdown","a04e75ec":"markdown"},"source":{"1f26d73e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","585daff2":"os.listdir('..\/input\/lish-moa')","2bd5c058":"train_features = pd.read_csv(\"..\/input\/lish-moa\/train_features.csv\")\ntest_features = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\nsubmission = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")\ntrain_targets_nonscored = pd.read_csv(\"..\/input\/lish-moa\/train_targets_nonscored.csv\")\ntrain_targets_scored = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")","43e9e3c6":"train_features.head()","e6a9259c":"\nprint(f\" The Shape of training set : {train_features.shape}\")\nprint(f\" The number of training examples are : {train_features.shape[0]}\")\nprint(f\" Independent Features availble in training set  : {train_features.shape[-1]}\")","805de272":"test_features.head()","68212d40":"print(f\" The Shape of test set : {test_features.shape}\")\nprint(f\" The number of test examples are : {test_features.shape[0]}\")\nprint(f\" Independent Features availble in test set  : {test_features.shape[-1]}\")","83960dc0":"# The scored Training Targets \ntrain_targets_scored.head()","d048f5e7":"train_targets_nonscored.head()","58b35bbc":"# Lets Checkout the sample Submission File\nsubmission.head()","23d2df06":"print(f\"The format for predicting the data is of shape : {submission.shape}\" )\nprint(f\"The sample Output Data given for {submission.shape[0]} examples\")","ddae9443":"train_features.info()\nprint('______________________________')\ntest_features.info()\nprint('______________________________')\ntrain_targets_scored.info()\nprint('______________________________')\ntrain_targets_nonscored.info()","e7e81b2c":"train_features.cp_type.value_counts(normalize=True)\n","c2adeb4f":"control_group = train_features.loc[train_features.cp_type == 'ctl_vehicle', 'sig_id']\nprint(f\" The control Value has {train_targets_scored.loc[train_targets_scored['sig_id'].isin(control_group)].sum()[1:].sum()} values in Targets\")","085205d7":"train_features.cp_time.value_counts(normalize=True)\n","5b2c31fc":"train_features.cp_dose.value_counts(normalize=True)\n","41e00733":"test_features.cp_type.value_counts(normalize=True)\n","ff2f3879":"import matplotlib.pyplot as plt\nfrom itertools import cycle\npd.set_option('max_columns', 50)\nplt.style.use('seaborn-dark')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n%matplotlib inline","90868e0e":"# Lets just Concat the train and test dataframe for the (CP) features :\ndf = pd.concat([train_features,test_features])\ntrain_features['kf'] = 'train'\ntest_features['kf'] = 'test'","d5cc9ef7":"import seaborn as sns\nsns.countplot(df['cp_time'])","16deea23":"sns.countplot(df['cp_type'])","4e60a0ca":"fig,ax= plt.subplots(1,2)\nsns.countplot(train_features['cp_type'],ax=ax[0])\nsns.countplot(test_features['cp_type'],ax=ax[1])","02f687ed":"sns.countplot(df['cp_dose'])","a5499899":"import plotly.express as px\nds = df.groupby(['cp_type', 'kf'])['sig_id'].count().reset_index()\nds.columns = ['cp_type', 'kf', 'count']\nfig = px.bar(\n    ds, \n    x='cp_type', \n    y=\"count\", \n    color = 'kf',\n    barmode='group',\n    orientation='v', \n    title='cp_type train+test counts', \n    width=700,\n    height=500\n)\nfig.show()\n","23c52351":"ds = df.groupby(['cp_dose', 'kf'])['sig_id'].count().reset_index()\nds.columns = ['cp_dose', 'kf', 'count']\nfig = px.bar(\n    ds, \n    x='cp_dose', \n    y=\"count\", \n    color = 'kf',\n    barmode='group',\n    orientation='v', \n    title='cp_dose train+test counts', \n    width=600,\n    height=500\n)\nfig.show()","fb3eaa69":"data = train_targets_scored.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\ndata.columns = ['row', 'count']\ndata = data.groupby(['count'])['row'].count().reset_index()\nfig = px.pie(\n    data, \n    values=100 * data['row']\/len(train_targets_scored), \n    names=\"count\", \n    title='Activations in targets for every sample Percentage ', \n    width=800, \n    height=500\n)\nfig.show()","73019367":"x = train_targets_scored.drop(['sig_id'], axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\nx.columns = ['column', 'nonzero_records']\n\nfig = px.bar(\n    x.tail(50), \n    x='nonzero_records', \n    y='column', \n    orientation='h', \n    title='Columns with the lowest number of positive samples (top 50)', \n    height=1000, \n    width=800\n)\nfig.show()","ded4e075":"ax = train_targets_scored.drop('sig_id', axis=1) \\\n    .sum() \\\n    .sort_values(ascending=False) \\\n    .head(30) \\\n    .sort_values() \\\n    .plot(kind='barh',\n         figsize=(15, 10),\n          color=next(color_cycle)\n         )\nax.set_title('Top 30  Targets in Train Set', fontsize=20)\nplt.show()","a04e946e":"import multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nwarnings.simplefilter('ignore')\nsns.set()\nimport random\n%matplotlib inline","e553ccbf":"train_features['cp_type'].value_counts","eacb66b5":"'''\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom category_encoders import CountEncoder\n'''","801046bc":"def seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    \nseed_everything(seed=42)\n\nparams = {'num_leaves': 500,\n          'min_child_weight': 0.03,\n          'feature_fraction': 0.3,\n          'bagging_fraction': 0.4,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'binary_logloss',\n          \"verbosity\": 0,\n          'reg_alpha': 0.4,\n          'reg_lambda': 0.6,\n          'random_state': 47\n         }\n","43c2b028":"for feature in ['cp_type', 'cp_dose']:\n    trans = LabelEncoder()\n    trans.fit(list(train_features[feature].astype(str).values) + list(test_features[feature].astype(str).values))\n    train_features[feature] = trans.transform(list(train_features[feature].astype(str).values))\n    test_features[feature] = trans.transform(list(test_features[feature].astype(str).values))","396491c9":"train_features.drop('kf',axis=1,inplace=True)","7012d3a1":"test_features.drop('kf',axis=1,inplace=True)","d7d8db0c":"\nfeatures = [x for x in train_features.columns if x != 'sig_id']\nprint(len(features))","f1995a9f":"targets = [x for x in train_targets_scored.columns if x != 'sig_id']\nprint(f'Total Labels available :{len(targets)}')","dff3dba0":"X=train_features[features]\ntotal_loss = 0","85eeaf1d":"skf = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)","b47d93c5":"print(f\" The number of Models to be created are : {train_targets_scored.shape[-1]-1}\")\nprint(\" This is equal to the number of Labels we have\")","92cdf054":"\nfor model,target in enumerate(targets,1):\n    y = train_targets_scored[target]\n    predictions = np.zeros(test_features.shape[0])\n    oof_preds = np.zeros(X.shape[0])\n    \n    for train_idx, test_idx in skf.split(X, y):\n        train_data = lgb.Dataset(X.iloc[train_idx], label=y.iloc[train_idx])\n        val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n        clf = lgb.train(params, train_data, 10000, valid_sets = [train_data, val_data], verbose_eval=0, early_stopping_rounds=30)\n        oof_preds[test_idx] = clf.predict(X.iloc[test_idx])\n        predictions += clf.predict(test_features[features]) \/ skf.n_splits\n        \n    submission[target] = predictions\n    loss = log_loss(y, oof_preds)\n    total_loss += loss\n    \n    print(f\"Model:{model} ==> Losses:{loss:.4f}\")\n\n    del predictions, oof_preds,  y, loss\n    gc.collect();\n    ","36808c42":"print('Overall mean loss: {:.3f}'.format(total_loss \/ 206))\n","091cdca0":"submission.to_csv('submission.csv', index=False)","757731de":"print(\"Visualizing the predictions made \")\ntemp = pd.read_csv(\".\/submission.csv\")\ntemp.head()","0bb7a096":"# **Data Loading**","e6b00ec9":"The most scored Target in Train : nfkb_inhibitor , proteasome_inhibator","c21910b5":"# Competition General Description\nToday, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target.\n\n\n\nIn this dataset we have information about one of the approaches: sample of human cells is treated with the drug and then scientists analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.\n\n","2adfd491":"* *We could see that in training Set : **872** Features are of Float Dtype , **1** feature of Int Dtype & **3** Categorical Dtype*\n\n* *We could see that in Test Set : **872** Features are of Float Dtype , **1** feature of Int Dtype & **3** Categorical Dtype*\n\n* *We could see that in Scored Target Set : **0** Features are of Float Dtype , **402** feature of Int Dtype & **1 **Categorical Dtype*\n\n* *We could see that in training Set : **0** Features are of Float Dtype ,** 206** feature of Int Dtype &** 1** Categorical Dtype*","64679cf3":"Lets Checkout the Samples with Zero Targets :\n\ncp_type : control perturbations have no MoAs","6a01163f":"We could see biggest number of Postive samples for Category 1","3fcf0d88":"# Preprocessing of Categorical Variables :\n","d7f0d4c5":"We could see that ( cp_time ) time duration for 3 classes (24,28,72) is approximately same","74231427":"# Target to be Predicted :\n\nIn this competition, you will be predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data.","dae5c700":"# Baseline :\n\nLets use LightGBM as our Baseline","5dba7ce3":"# Evaluation Metric Used : Log Loss\n \nFor every sig_id you will be predicting the probability that the sample had a positive response for each target. For N sig_id rows and M targets, you will be making N\u00d7M predictions. Submissions are scored by the log loss:\n\n","12c4dcaf":"![Photo-RawPixel-Pill-Capsules.jpg](attachment:Photo-RawPixel-Pill-Capsules.jpg)","727c5c12":"Lets Check the DataType of Columns in Train and Test Set :","3530137a":"* Thanks a lot for Reading the kernal . Hope you found the Kernal Helpful !!!\n\n* Eager for Suggestions and making improvements !! ","16f14b08":"# Mechanisms of Action (MoA) Prediction","48d27876":"cp_type is skewed with large quantity of trt_cp over small quantity of ctl_vehicle. \n\nSO lets Check this skewed distribution over in train and test set individually .","a792aadc":"So this rows wont help us in the prediction so not using them for training should suffice the problem","1f1b7b8b":"Glancing Through Training and Test sets ","f7f2ad1a":"In this notebook we will explore the data provided for the competition and create a baseline LGBM  model.","20a6c31d":"# Visualization of Categories \n\nThe 3 categorical Variables are :\n\n1. cp_type : indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle)\n\n2. sig_id : Unique Individual ID's\n\n3. cp_dose : Dose(low,high)\n\nThe One Integer Feature is :\n\n1. cp_time : This indicates treatment Duration(24,48,72) hours","c150946b":"Next Boosting Model to Tryout would be XG-Boost !!!","5556715a":"# Cross Validation :\n\nLets use Stratified k-fold","46ab978c":"Columns with Lower number of Positive Samples :","999845d8":"# GPU for GBM\nParams to be Changed to  use GPU to Accelerate the LGBM: \n1.  'device': 'gpu',\n2. 'gpu_platform_id': 0,\n1. 3.   'gpu_device_id': 0","9aacc16f":"We are creating baseline Model using LightGBM and framing our problem as n-binary classification problem. ","dceae12f":"# 1. **CP_Type Feature :**","a04e75ec":"# Configs :"}}