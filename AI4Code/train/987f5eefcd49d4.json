{"cell_type":{"2456e5a5":"code","c61853f9":"code","7dca5f4a":"code","b197ccdb":"code","1e57236b":"code","a0e9c168":"code","6d827945":"code","f0da12ac":"code","11cd489f":"code","d976f697":"code","6cc50c68":"code","8b31df07":"code","081d0264":"code","36f2cf8c":"code","22ba5439":"code","e48663cf":"code","6826b5c5":"code","80385454":"code","0850ad3d":"code","b8ef3988":"code","e3f265ff":"code","b2737995":"code","089e8308":"code","c25d67c4":"code","4ce3e7ed":"code","9f0522af":"code","6acaf007":"code","39978b39":"code","cadbb672":"code","d6d9d70b":"code","8d9e2849":"code","1f3f6eec":"code","f5eca397":"code","3a1398e9":"code","3f8beb08":"code","149c388a":"code","dbb128f2":"code","a25e0b71":"code","8f0d2388":"code","16f3f4d4":"code","137070bf":"code","306595b4":"code","f574ce86":"code","bc78ec0e":"code","001e18b6":"code","d86d5cff":"code","4725214c":"markdown","6af6e1b5":"markdown","576d7a82":"markdown","7eb64a2d":"markdown","7c269618":"markdown","6e310c65":"markdown","420ec7a6":"markdown","74948674":"markdown","78dd4655":"markdown","ad0cc265":"markdown","6cbfc1b2":"markdown","34e34427":"markdown","983e31e1":"markdown","e69c8252":"markdown","23f104d6":"markdown","349de972":"markdown","d2169a0f":"markdown","93eda807":"markdown","e716cfd8":"markdown","2538f022":"markdown","738588ee":"markdown","be926b58":"markdown","f0a427ee":"markdown","aba232e5":"markdown","09ebfc84":"markdown","3f732159":"markdown"},"source":{"2456e5a5":"!pip install IQA_pytorch #For SSIM Score","c61853f9":"!pip install torchsummaryX","7dca5f4a":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision\nfrom torch.optim import *\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom IQA_pytorch import DISTS, utils\nfrom torchsummaryX import summary\n\nimport math\nimport time\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport albumentations\nimport albumentations.pytorch\nfrom matplotlib import pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import font_manager, rc\nfrom IPython import display\nimport random\nimport glob\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nimport warnings\nimport sys\nfrom tqdm import tqdm\nimport pickle\nimport gc\nimport random\nimport urllib.request\n\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Version of Torch : {0}\".format(torch.__version__))\nprint(\"Version of TorchVision : {0}\".format(torchvision.__version__))","b197ccdb":"gc.collect()\ntorch.cuda.empty_cache()","1e57236b":"%matplotlib inline\n\nplt.rcParams['axes.unicode_minus'] = False\nfontpath = \"..\/input\/koreanfont\/NanumBrush.ttf\"\nfontprop = font_manager.FontProperties(fname=fontpath)\n\nplt.rcParams[\"animation.html\"] = \"jshtml\"\nplt.rcParams['figure.dpi'] = 150  \nplt.ioff()","a0e9c168":"# Device\nUSE_CUDA = torch.cuda.is_available()\n\nprint(\"Device : {0}\".format(\"GPU\" if USE_CUDA else \"CPU\"))\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\ncpu_device = torch.device(\"cpu\")\n\nDEBUG = False\n\nRANDOM_SEED = 2004\n\n# Train\nstart_epoch = 0\nall_epochs = 1\nbatch_size = 14\n\nlrG = 0.0002\nlrD = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\n\nL1lambda = 100\nGAMMA = 0.59\n\nTIME_STEP = 4\nTEST_TIME_STEP = 6\nIMAGE_SIZE = 128\n\npatch = (1,256\/\/2**4,256\/\/2**4)\n\n# Path\nDATASET1_PATH = '..\/input\/the-cloudcast-dataset'\n\n# Checkpoint\nUSE_CHECKPOINT = False\n\nOLD_PATH = '..\/input\/checkpoint'\nOLD_GENERATOR_MODEL = os.path.join(OLD_PATH, 'Generator.pth')\nOLD_DISCRIMINATOR_MODEL = os.path.join(OLD_PATH, 'Discriminator.pth')\nOLD_G_LOSS = os.path.join(OLD_PATH, 'gloss.txt')\nOLD_D_LOSS = os.path.join(OLD_PATH, 'dloss.txt')","6d827945":"torch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\n\nprint('Random Seed : {0}'.format(RANDOM_SEED))","f0da12ac":"def log(text):\n    global DEBUG\n    if DEBUG:\n        print(text)","11cd489f":"def torch_tensor_to_plt(img):\n    img = img.detach().numpy()[0]\n    img = np.transpose(img, (1, 2, 0))\n    return img ","d976f697":"def show_video_in_jupyter_nb(width, height, video_url):\n    from IPython.display import HTML\n    return HTML(\"\"\"<video width=\"{}\" height=\"{}\" controls>\n    <source src={} type=\"video\/mp4\">\n    <\/video>\"\"\".format(width, height, video_url))","6cc50c68":"def plt_image_animation(frames, update_func):\n    fig, ax = plt.subplots(figsize=(4,4))\n    plt.axis('off')\n    anim = animation.FuncAnimation(fig, update_func, frames=frames)\n    video = anim.to_html5_video()\n    html = display.HTML(video)\n    display.display(html)\n    plt.close()","8b31df07":"plt_image_animation(15, lambda t : plt.imshow(np.load(join(DATASET1_PATH, '2017M01', '{0}.npy'.format(t))), cmap='gray'))","081d0264":"transformer = transforms.Compose([transforms.ToTensor(),\n                                  torchvision.transforms.Resize(IMAGE_SIZE),\n                                  transforms.Normalize((0.5), (0.5)), #GrayScale\n                                 ])\n","36f2cf8c":"nowpath = \"\"\n\nclass TimeStepImageDataset(Dataset):\n    def __init__(self, date, time_step, transform=None):\n        self.date = date\n        self.time_step = time_step\n        self.transformer = transform\n        self.file = []\n        \n        file_list = glob.glob(join(self.date, '*'))\n        self.file = [file for file in file_list if (file.endswith(\".npy\") and not file.endswith('TIMESTAMPS.npy'))]\n        \n    def __len__(self):\n        return len(self.file)-self.time_step     \n    \n    def transform(self, image):\n        if self.transformer:\n            return self.transformer(image)\n        else :\n            return image\n\n    def __getitem__(self, idx):\n        global nowpath\n        log(join(self.date, str(idx)+'.npy'))\n        X = self.transform(np.load(join(self.date, str(idx)+'.npy')))\n        nowpath = join(self.date, str(idx)+'.npy')\n        Y_list = []\n        for i in range(1, self.time_step+1):\n            Y_list.append(self.transform(np.load(join(self.date, str(idx+i)+'.npy'))).unsqueeze(0))\n        Y = torch.cat(Y_list)       \n        return X, Y","22ba5439":"DATASET1_DIRS = glob.glob(join(DATASET1_PATH, '*'))\n\nrandom.shuffle(DATASET1_DIRS)\n\ntraindatasetlist = []\nfor ind, name in enumerate(DATASET1_DIRS[:20]):\n    traindatasetlist.append(TimeStepImageDataset(name, TIME_STEP, transform=transformer))\ntrain_dataset = torch.utils.data.ConcatDataset(traindatasetlist)\n\ntestdatasetlist = []\nfor ind, name in enumerate(DATASET1_DIRS[20:]):\n    testdatasetlist.append(TimeStepImageDataset(name, TEST_TIME_STEP, transform=transformer))\ntest_dataset = torch.utils.data.ConcatDataset(testdatasetlist)","e48663cf":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataloader_bs1_shuffle = DataLoader(test_dataset, batch_size=1, shuffle=True) \ntest_dataloader_bs1_noshuffle = DataLoader(test_dataset, batch_size=1, shuffle=False) ","6826b5c5":"def ShowDatasetImage(x, y):\n    grid = torchvision.utils.make_grid(y)\n\n    fig = plt.figure(figsize=(2, 2))\n    plt.imshow(torch_tensor_to_plt(x.unsqueeze(0)), cmap='gray')\n    plt.axis('off')\n    plt.title('Input (Now)', fontproperties=fontprop)\n    plt.show()   \n    \n    fig = plt.figure(figsize=(8, 2.5))\n    plt.title('Real Weather Image', fontproperties=fontprop)\n    plt.axis('off')\n    for i in range(1, TIME_STEP+1):\n        ax = fig.add_subplot(1, TIME_STEP, i)\n        ax.axis('off')\n        ax.imshow(torch_tensor_to_plt(y[i-1].unsqueeze(0)), cmap='gray')\n        ax.set_title('after {0} minutes'.format(15*i), fontproperties=fontprop)\n    plt.show()\n\n    del x, y","80385454":"for ind, (x, y) in enumerate(train_dataset):\n    if ind != 0:\n        continue\n    ShowDatasetImage(x, y)\n    break","0850ad3d":"class depthwise_separable_conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n        super(depthwise_separable_conv, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=False)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        out = self.depthwise(x)\n        out = self.pointwise(out)\n        return out","b8ef3988":"class UNetDown(nn.Module):\n    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0, dsconv=True):\n        super().__init__()\n    \n        if dsconv:\n            layers = [depthwise_separable_conv(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n        else :\n            layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n            \n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_channels)),\n\n        layers.append(nn.LeakyReLU(0.2))\n\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.down = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.down(x)\n        return x","e3f265ff":"class UNetUp(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=0.0, use_skip_conv=True):\n        super().__init__()\n\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels,4,2,1,bias=False),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU()\n        ]\n\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.up = nn.Sequential(*layers)\n        self.use_skip_conv = use_skip_conv\n        \n        if use_skip_conv:\n            self.skip_conv = nn.Conv2d(out_channels, out_channels, 1, bias=False) #Skip Convolution\n\n    def forward(self,x,skip):\n        x = self.up(x)\n        if self.use_skip_conv:\n            skip = self.skip_conv(skip)\n        x = torch.cat((x, skip),1)\n        return x","b2737995":"class GeneratorUNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n\n        self.down1 = UNetDown(in_channels, 64, normalize=False, dsconv=False)\n        self.down2 = UNetDown(64,128)                 \n        self.down3 = UNetDown(128,256)               \n        self.down4 = UNetDown(256,512,dropout=0.5) \n        self.down5 = UNetDown(512,512,dropout=0.5)      \n        self.down6 = UNetDown(512,512,dropout=0.5)             \n        #self.down7 = UNetDown(512,512,dropout=0.5)              \n        self.down8 = UNetDown(512,512,normalize=False,dropout=0.5, dsconv=False)\n\n        #self.up1 = UNetUp(512,512,dropout=0.5)\n        self.up2 = UNetUp(1024\/\/2,512,dropout=0.5)\n        self.up3 = UNetUp(1024,512,dropout=0.5)\n        self.up4 = UNetUp(1024,512,dropout=0.5)\n        self.up5 = UNetUp(1024,256)\n        self.up6 = UNetUp(512,128)\n        self.up7 = UNetUp(256,64)\n        self.up8 = nn.Sequential(\n            nn.ConvTranspose2d(128,out_channels,4,stride=2,padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down8(d6)\n        u1 = d7\n        u2 = self.up2(u1,d6)\n        u3 = self.up3(u2,d5)\n        u4 = self.up4(u3,d4)\n        u5 = self.up5(u4,d3)\n        u6 = self.up6(u5,d2)\n        u7 = self.up7(u6,d1)\n        u8 = self.up8(u7)\n        \n        return u8","089e8308":"def _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor \/ 2) \/\/ divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef conv_3x3_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.identity = stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.identity:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, num_classes=1, width_mult=1.):\n        super(Discriminator, self).__init__()\n        # setting of inverted residual blocks\n        self.cfgs = [\n            # t, c, n, s\n            [1,  16, 1, 1],\n            [6,  24, 2, 2],\n            [6,  32, 3, 2],\n            [6,  64, 4, 2],\n            [6,  96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        input_channel = _make_divisible(32 * width_mult, 4 if width_mult == 0.1 else 8)\n        layers = [conv_3x3_bn(1, input_channel, 2)]\n        # building inverted residual blocks\n        block = InvertedResidual\n        for t, c, n, s in self.cfgs:\n            output_channel = _make_divisible(c * width_mult, 4 if width_mult == 0.1 else 8)\n            for i in range(n):\n                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t))\n                input_channel = output_channel\n        self.features = nn.Sequential(*layers)\n        # building last several layers\n        output_channel = _make_divisible(1280 * width_mult, 4 if width_mult == 0.1 else 8) if width_mult > 1.0 else 1280\n        self.conv = conv_1x1_bn(input_channel, output_channel)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Linear(output_channel, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.conv(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        x = self.sigmoid(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. \/ n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()","c25d67c4":"def weights_init(m):\n    classname = m.__class__.__name__\n    if type(m) == nn.Conv2d:\n        m.weight.data.normal_(0.0, 0.02)\n    elif type(m) == nn.BatchNorm2d:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","4ce3e7ed":"Generator = GeneratorUNet().to(device)\nDiscriminator = Discriminator(num_classes=1).to(device) \n\nsummary_g = Generator.apply(weights_init)\nsummary_d = Discriminator.apply(weights_init)","9f0522af":"summary(Generator, torch.rand((batch_size, 1, IMAGE_SIZE, IMAGE_SIZE)).to(device))","6acaf007":"summary(Discriminator, torch.rand((batch_size, 1, IMAGE_SIZE, IMAGE_SIZE)).to(device))","39978b39":"optimizerG = Adam(Generator.parameters(), lr=lrG, betas=(beta1, beta2))\noptimizerD = Adam(Discriminator.parameters(), lr=lrD, betas=(beta1, beta2))","cadbb672":"img_list = []\nG_loss = []\nD_loss = []\n\nFAKE_LABEL = 0.0\nREAL_LABEL = 1.0","d6d9d70b":"l1loss = nn.L1Loss()\nsmoothl1loss = nn.SmoothL1Loss()\nbceloss = nn.BCELoss()","8d9e2849":"def generator_error(netG, netD, sketch, real, real_label, fake_label, gamma=0.0):\n    def G_error(G_output, real, D_output):\n        return l1loss(G_output, real)*L1lambda + bceloss(D_output, real_label)\n    \n    next_input = sketch\n    error = None\n    \n    real_list = []\n    for i in range(TIME_STEP):\n        real_list.append(real[:,i,:,:,:])\n    \n    for ind, y in enumerate(real_list):\n        G_output = netG(next_input)\n        next_input = G_output.clone().detach()\n        D_output = netD(G_output).view(-1)     \n        \n        if ind==0:\n            error = G_error(G_output, y, D_output)\n        else :\n            error += (gamma ** ind) * G_error(G_output, y, D_output)\n            \n        del G_output, D_output\n        gc.collect()\n        torch.cuda.empty_cache()\n            \n    return error","1f3f6eec":"def discriminator_error(netG, netD, sketch, real, real_label, fake_label, avg=True):\n    output_g = netG(sketch)\n    outputs_fake = netD(output_g.detach()).view(-1)    \n    errD = bceloss(outputs_fake, fake_label)\n    \n    #print(fake_label)\n    #print(outputs_fake)\n    \n    del output_g, outputs_fake\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    for i in range(0, TIME_STEP):\n        outputs_real = netD(real[:,i,:,:]).view(-1)\n        if avg:\n            errD += bceloss(outputs_real, real_label)\/TIME_STEP\n        else:\n            errD += bceloss(outputs_real, real_label)\n        del outputs_real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n    return errD","f5eca397":"def apply_checkpoint(use_checkpoint=True):\n    global Generator, Discriminator, optimizerG, optimizerD, G_Loss, D_Loss, start_epoch\n    \n    if os.path.isdir(OLD_PATH) and use_checkpoint:        \n        checkpoint = torch.load(OLD_GENERATOR_MODEL)\n        start_epoch = checkpoint['epoch']\n        Generator.load_state_dict(checkpoint['model_state_dict'])\n        optimizerG.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        checkpoint = torch.load(OLD_DISCRIMINATOR_MODEL)\n        start_epoch = checkpoint['epoch']\n        Discriminator.load_state_dict(checkpoint['model_state_dict'])\n        optimizerD.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        with open(OLD_G_LOSS, 'rb') as f:\n            G_loss = pickle.load(f)\n            \n        with open(OLD_D_LOSS, 'rb') as f:\n            D_loss = pickle.load(f)\n        \n        print('Continue training. (Epoch : {0})'.format(start_epoch))\n    else :\n        print('Begin training newly.')","3a1398e9":"nowepoch = 0\nstrange_error_limit = 10\nstrange_error_num = 0\n\ndef fit(device, num_epochs=1000):\n    global nowepoch\n    iters = 0\n    for epoch in range(start_epoch+1, num_epochs+start_epoch+1):\n        nowepoch = epoch\n        print(\"< EPOCH{0} >\".format(epoch))\n        result = train_one_epoch(device, train_dataloader, Generator, Discriminator, optimizerG, optimizerD, epoch, num_epochs)\n        if not result:\n            return\n    \n\ndef train_one_epoch(device, dataloader, netG, netD, optimizerG, optimizerD, epoch, num_epochs, iters=0):\n    global nowpath, strange_error_num, strange_error_limit\n    with torch.autograd.set_detect_anomaly(True):\n        for i, data in enumerate(dataloader):   \n            start = time.time()\n            sketch, real = data\n            sketch, real = sketch.to(device), real.to(device)\n            \n            b_size = sketch.size(0)\n            real_label = torch.full((b_size,), REAL_LABEL, dtype=torch.float, device=device)\n            fake_label = torch.full((b_size,), FAKE_LABEL, dtype=torch.float, device=device)\n            \n            #Train Discriminator\n            netG.eval()\n            netD.train()\n            netD.zero_grad()\n            \n            errD = discriminator_error(netG, netD, sketch, real, real_label, fake_label)\n            \n            log('Complete calcuating of Discriminator')\n            errD.backward()\n            log('Complete backprogration of Discriminator')\n            optimizerD.step()\n            log('Complete stepping OptimizerD')\n        \n            #Train Generator\n            netG.train()\n            netD.eval()\n            netG.zero_grad()\n            \n            \n            errG = generator_error(netG, netD, sketch, real, real_label, fake_label, gamma=GAMMA)\n            \n            log('Complete calcuating of Generator')\n            errG.backward()\n            log('Complete backprogration of Genereator')\n            optimizerG.step()\n            log('Complete stepping OptimizerG')\n            \n            del b_size, real_label, fake_label, sketch, real\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            #Log\n            if i % 1 == 0:\n                print('[%d\/%d][%d\/%d]\\tLoss_G: %.4f\\tLoss_D: %.4f\\tTime: %.6f'\n                      % (epoch, num_epochs, i, len(dataloader),\n                         errG.item(), errD.item(), time.time() - start))\n                \n            \n\n            G_loss.append(errG.item())\n            D_loss.append(errD.item())\n            \n            del errG, errD\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            iters += 1\n    return True","3f8beb08":"apply_checkpoint(use_checkpoint=USE_CHECKPOINT)","149c388a":"summary = Generator.train()\nsummary = Discriminator.train()\n\nif all_epochs>0:\n    fit(device, num_epochs=all_epochs)\n\nsummary = Generator.eval()\nsummary = Discriminator.eval()","dbb128f2":"plt.figure(figsize=(10,5))\nplt.title('Loss of Generator')\nplt.plot(G_loss,label=\"\")\nplt.xlabel(\"Iter\")\nplt.legend()\nplt.show()","a25e0b71":"plt.figure(figsize=(10,5))\nplt.title('Loss of Discriminator')\nplt.plot(D_loss,label=\"train\")\nplt.xlabel(\"Iter\")\nplt.legend()\nplt.show()","8f0d2388":"def model_predict(model, time, input):\n    if time%15==0 and time!=0:\n        model.eval()\n        num = time\/\/15\n        \n        next_input = input\n        for i in range(num):\n            next_input = model(next_input).clone().detach()\n        return next_input\n    else:\n        raise ValueError('Please set the time to a multiple of 15.')","16f3f4d4":"from IQA_pytorch import SSIM, utils\n\ntoPILImage = transforms.ToPILImage()\nssim_model = SSIM(channels=1)\n\ndef one_time_step_ssim_score(dataloader, model, time_step, num=-1):\n    model.eval()\n    score = 0\n    total = 0\n    for ind, (x, y) in enumerate(test_dataloader_bs1_shuffle):\n        x, y = x.squeeze(0).to(device), y.squeeze(0).to(device)\n        outputG = model_predict(model, time_step*15, x.unsqueeze(0))\n\n        sketch = utils.prepare_image(toPILImage(outputG.squeeze(0))).to(device)\n        real = utils.prepare_image(toPILImage(y[time_step-1])).to(device)\n\n        score += ssim_model(sketch, real, as_loss=False).item()\n        total += 1\n\n        del x, y, outputG, sketch, real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        if num != -1:\n            if ind+1 >= num:\n                break\n            \n    print(\"SSIM Score of the prediction {0} minutes later : {1}\".format(time_step*15, score\/total))\n    return score\/total\n\nfor ind in range(1, TEST_TIME_STEP+1):\n    one_time_step_ssim_score(test_dataloader_bs1_shuffle, Generator, ind, num=2000)","137070bf":"import zipfile\n\ny_nums = 40\niter = 0\n\nai_noseries_ls = []\nreal_noseries_ls = []\n\nstart_ind = 200\n\nfor ind, (x, y) in enumerate(test_dataloader_bs1_shuffle):\n    if ind < start_ind:\n        continue\n        \n    iter += 1\n    \n    x, y = x.to(device), y[0].to(device)\n        \n    outputg = Generator(x).to(cpu_device)\n    \n    outputg = outputg*127.5+127.5\n    realimage = y*127.5+127.5\n\n    cv2.imwrite('.\/AI_NOSERIES_Answer{0}.png'.format(ind+1), torch_tensor_to_plt(outputg)*30)\n    cv2.imwrite('.\/Real_NOSERIES{0}.png'.format(ind+1), torch_tensor_to_plt(realimage.to(cpu_device))*30)\n    \n    ai_noseries_ls.append('.\/AI_NOSERIES_Answer{0}.png'.format(ind+1))\n    real_noseries_ls.append('.\/Real_NOSERIES{0}.png'.format(ind+1))\n    \n    if iter > y_nums:\n        break\n\nwith zipfile.ZipFile(\"ai_noseries.zip\", 'w') as my_zip:\n    for i in ai_noseries_ls:\n        my_zip.write(i)\n    my_zip.close()\n\n\nwith zipfile.ZipFile(\"real_noseries.zip\", 'w') as my_zip:\n    for i in real_noseries_ls:\n        my_zip.write(i)\n    my_zip.close()\n    \nfor file in (ai_noseries_ls + real_noseries_ls):\n    os.remove(file)\n    \nprint('NOSERIES Images are generated.')","306595b4":"import zipfile\n\ny_nums = 40\niter = 0\n\nai_series_ls = []\nreal_series_ls = []\n\nnext_input = None\nstart_ind = 200\n\nfor ind, (x, y) in enumerate(test_dataloader_bs1_noshuffle):\n    if ind < start_ind:\n        continue\n        \n    iter += 1\n    \n    if ind == start_ind:\n        next_input = x.clone().detach().to(device)\n        cv2.imwrite('.\/Input_SERIES.png', torch_tensor_to_plt(next_input.to(cpu_device)*127.5+127.5)*30)\n    \n    x, y = x.to(device), y[0].to(device)\n    \n    outputg_series = Generator(next_input).to(cpu_device)\n    next_input = outputg_series.clone().detach().to(device)\n\n    outputg_series = outputg_series * 127.5 + 127.5\n    realimage = y*127.5+127.5\n\n    cv2.imwrite('.\/AI_SERIES_Answer{0}.png'.format(ind+1), torch_tensor_to_plt(outputg_series)*30)\n    cv2.imwrite('.\/Real_SERIES{0}.png'.format(ind+1), torch_tensor_to_plt(realimage.to(cpu_device))*30)\n    \n    ai_series_ls.append('.\/AI_SERIES_Answer{0}.png'.format(ind+1))\n    real_series_ls.append('.\/Real_SERIES{0}.png'.format(ind+1))\n    \n    if iter > y_nums:\n        break\n\nwith zipfile.ZipFile(\"ai_series.zip\", 'w') as my_zip:\n    for i in ai_series_ls:\n        my_zip.write(i)\n    my_zip.close()\n\nwith zipfile.ZipFile(\"real_series.zip\", 'w') as my_zip:\n    for i in real_series_ls:\n        my_zip.write(i)\n    my_zip.close()\n    \n\n    \nprint('SERIES Images are generated')","f574ce86":"v1 = cv2.VideoWriter('oraclegan_series.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 3, (128, 128))\nfor name in ai_series_ls:\n    img = cv2.imread(name)\n    v1.write(img)\nv1.release()\n\nv2 = cv2.VideoWriter('real_series.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 3, (128, 128))\nfor name in real_series_ls:\n    img = cv2.imread(name)\n    v2.write(img)\nv2.release()\n\nprint('Videos are generated')\nprint('video path : \".\/oraclegan_series.mp4\" and \".\/real_series.mp4\"')","bc78ec0e":"for file in (ai_series_ls + real_series_ls):\n    os.remove(file)","001e18b6":"torch.save({\n            'epoch': nowepoch,\n            'model_state_dict': Generator.state_dict(),\n            'optimizer_state_dict': optimizerG.state_dict(),\n            }, 'Generator.pth')\n\ntorch.save({\n            'epoch': nowepoch,\n            'model_state_dict': Discriminator.state_dict(),\n            'optimizer_state_dict': optimizerD.state_dict(),\n            }, 'Discriminator.pth')","d86d5cff":"with open('.\/gloss.txt', 'wb') as f:\n    pickle.dump(G_loss, f)\nwith open('.\/dloss.txt', 'wb') as f:\n    pickle.dump(D_loss, f)","4725214c":"# **FastOracleGAN**\n--------------\nThis study explores how to make **lighter, faster [OracleGAN](https:\/\/www.kaggle.com\/lapl04\/oraclegan-pix2pix-for-time-series-image)**.\n\nMobileNet v2 is adopted to Discriminator.\nAlso, part of Generator's convolution layers are replaced with Depthwise Separable Convolution.\nConvolution is added to Generator's skips.","6af6e1b5":"# Visualize Data\n|Name of Function|Explanation|\n|-----|-----|\n|torch_tensor_to_plt|Convert torch image to matplotlib image|\n|plt_image_animation|show a video by update_function|","576d7a82":"# Save Checkpoint","7eb64a2d":"# Define Neural Networks and Optimizers\n|Name|Sort|\n|----|----|\n|Generator|UNet|\n|Discriminator|ResNet|\n|Optimizer of Generator|Adam|\n|Optimizer of Disciminator|Adam|","7c269618":"<a id=\"cost_of_discriminator\"><\/a>\n## **Cost Function of Discriminator**\n\n------------------------------------------------\n\n**$$ \\mathbf{Loss_D(x, y) = E_x\\left [ log D(G(x)) \\right ] + \\frac{1}{t} \\sum_{i=1}^{t} E_{y_i}\\left [ log(1-D(G(y_i)))) \\right ]} $$**\n\n$t$ is Time Step.","6e310c65":"# **Key Featues of FastOracleGAN**\n-------------------------------------\n- [Depthwise Separable Convolution](#depthwise_separable_convolution)\n- [MobileNet v2](#mobilenetv2)\n- [Skip Convolution](#skip_convolution)","420ec7a6":"# Train","74948674":"# Install additional libraries\n\nIQA_pytorch is a library which is used to calculate SSIM Score","78dd4655":"# Define Hyperparameters\n|Name of Hyperparameter|Explanation|\n|-----|-----|\n|USE_CUDA|whether to use GPU|\n|DEBUG|whether to print specific logs|\n|RANDOM_SEED|random seed of pytorch, random, numpy|\n|start_epoch|this is used to continuing train from checkpoint|\n|all_epochs|Epochs|\n|batch_size|Batch Size|\n|lrG|the learning rate of Generator|\n|lrD|the learning rate of Discriminator|\n|beta1, beta2|the beta1 and beta2 of Generator and Discriminator|\n|**L1Lambda**|lambda of pix2pix objective function|\n|**GAMMA**|factor similar to discount factor of DQN. (0<$\\gamma$<1) (check cost function of OracleGAN Generator)|\n|**TIME_STEP**|the number of future images which is used to calculate loss (check cost function of OracleGAN Generator)|\n\n","ad0cc265":"# Preprocess Dataset","6cbfc1b2":"# Apply Checkpoint","34e34427":"<a id=\"depthwise_separable_convolution\"><\/a>\n## Depthwise Separable Convolution\n------------------------\nGenerator's second~sixth convolution layers are replaced with Depthwise Separable Convolution.","983e31e1":"# Import Libraries","e69c8252":"# **Key Featues of OracleGAN**\n-------------------------------------\n- [Time Step Image Dataset](#time_step_image_dataset)\n- [Cost Function of Generator](#cost_of_generator)\n- [Cost Function of Discriminator](#cost_of_discriminator)","23f104d6":"# Define Cost Functions","349de972":"# Initiate Weights and Biases","d2169a0f":"# Test\n\n1. Calculate SSIM Score each Time Steps\n2. Generate test predicted images.\n3. Generate video which consist of series predicted images.","93eda807":"<a id=\"mobilenetv2\"><\/a>\n## MobileNet v2\n------------------------\nMobileNet v2 is adopted to Discriminator for better, lighter, and faster.","e716cfd8":"<a id=\"cost_of_generator\"><\/a>\n## **Cost Function of Generator**\n\n------------------------------------------------\n\n**$$ \\mathbf{Loss_G(x, y) = \\sum_{i=1}^{t}\\gamma ^ {i-1}\\times \\left \\{ \\lambda _1 \\times  E_{x,y_i}\\left [ \\left \\| y_i - G^i(x) \\right \\|_1 \\right ] + E_{x}\\left [ log(1-D(G^i(x))) \\right ] \\right \\} } $$**\n\n$t$ is Time Step. $\\gamma$ is discount factor(GAMMA). $\\lambda _1$ is L1Lambda.","2538f022":"<a id=\"time_step_image_dataset\"><\/a>\n## **Time Step Image Dataset**\n\n------------------------------------------------\n\nOracleGAN calculates loss between predicted image and real image not only after 15 minutes but also **after 15\u00d7TimeStep minutes**.\n \nSo, dataset need to have **multiple output** images per **one input** image.","738588ee":"> **< SSIM Score of normal Pix2Pix >**  *(check \"[Pix2Pix (Compared to OracleNet)](https:\/\/www.kaggle.com\/lapl04\/pix2pix-compared-to-oraclenet)\")*\n>\n> |Prediction|SSIM Score|\n> |-------------------|----------------|\n> |prediction 15 minutes later|0.8199273004531861|\n> |prediction 30 minutes later|0.7006081487536431|\n> |prediction 45 minutes later|0.6030721757411956|\n> |prediction 60 minutes later|0.5150686911344529|","be926b58":"# Define Train Function","f0a427ee":"# **Conclusion**\n------------------\n**the time required of 1 iter training with FastOracleGAN(about 7.5s) is reduced more twice times than its OracleGAN(about 3.36s).**","aba232e5":"# **Goal**\n- Predict future weather images using current weather images.","09ebfc84":"> **< SSIM Score of OracleGAN >**  *(check \"[OracleGAN - Pix2Pix for Time Series Image](https:\/\/www.kaggle.com\/lapl04\/oraclegan-pix2pix-for-time-series-image)\")*\n>\n> |Prediction|SSIM Score|\n> |-------------------|----------------|\n> |prediction 15 minutes later|0.8025623009409756|\n> |prediction 30 minutes later|0.7757316670715809|\n> |prediction 45 minutes later|0.7566662636697292|\n> |prediction 60 minutes later|0.7422156925499439|\n> |prediction 75 minutes later|0.7312239380329847|\n> |prediction 90 minutes later|0.720590250596404|","3f732159":"<a id=\"skip_convolution\"><\/a>\n## Skip Convolution\n------------------------\n**1x1 Convolutions are added to Generator's Skip.**\n\nAt first, I tried adding **Attention Blocks**.\nBut, they are too **heavy** for the purpose of FastOracleGAN. Also, I thought the purpose to Generator is different to the original purpose of UNet.\nThe original purpose of UNet is Semantic Segmentation. Preventing to disturb input is important.\nHowever, the purpose of Generator is converting image. I estimated skip which adds original image to decoder layers can **interrupt** rather achieving the original purpose.\n**So I thought it was necessary to set Generator to be more interested in Up-sampling than Skip, instead of leaving it to artificial intelligence learning about which things to be interested in, such as Attention Blocks.** Therefore, I judged most of Generator's Attention Blocks can be substituted with just manipulating Skip with skip convolution."}}