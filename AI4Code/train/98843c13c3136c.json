{"cell_type":{"c7ab3299":"code","ff610fef":"code","0ef9e352":"code","9a85e07d":"code","b6208b53":"code","860c0238":"code","20c44e83":"code","5a63fc7b":"code","a45ddd6e":"code","1ce74e86":"code","92709002":"code","16f25a63":"code","acaa42a5":"code","c15fdd58":"code","c293520e":"code","5696df88":"code","a5fb6230":"code","2cc5c35e":"code","8012cf3e":"code","0cbdb682":"markdown","944bbe7b":"markdown","dbe4cdba":"markdown","84a33d0f":"markdown","677d4057":"markdown","022eba3c":"markdown","8a901a8e":"markdown","ff5592e4":"markdown","1661608b":"markdown","7277b67d":"markdown","0c019af6":"markdown","4e7bf3ea":"markdown"},"source":{"c7ab3299":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ff610fef":"data=pd.read_csv('..\/input\/diabetes.csv')\ndata.head()","0ef9e352":"data.info()","9a85e07d":"data.Outcome.value_counts()","b6208b53":"y=data.Outcome.values","860c0238":"x_data=data.iloc[:,:8]\nx_data.head()","20c44e83":"x=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nx.head()","5a63fc7b":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","a45ddd6e":"def initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n","1ce74e86":"def sigmoid(z):\n    \n    y_head = 1\/(1+ np.exp(-z))\n    return y_head\n\n\n\n","92709002":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      \n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n\n\n","16f25a63":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","acaa42a5":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","c15fdd58":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","c293520e":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 600)  \n","5696df88":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 700) ","a5fb6230":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 800) ","2cc5c35e":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 830) ","8012cf3e":"y1=data.Outcome.values\nx1d=data.iloc[:,:8]\nx1=(x1d-np.min(x1d))\/(np.max(x1d)-np.min(x1d))\nx_train1,x_test1,y_train1,y_test1=train_test_split(x1,y1,test_size=0.2,random_state=42)\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train1,y_train1)\nprint(lr.get_params())\nprint(\"test accuracy {}\".format(lr.score(x_test1,y_test1)))\n","0cbdb682":"    A\u015fa\u011f\u0131daki fonksiyon yard\u0131m\u0131ya weight ve bias de\u011ferlerinin ba\u015flang\u0131\u00e7 de\u011ferini belirliyoruz. Burada modelimize gelecek olan x giri\u015fi say\u0131s\u0131n\u0131 dimension adl\u0131 parametre ile almaktay\u0131z.","944bbe7b":"                Veri setimizin belirli bir b\u00f6l\u00fcm\u00fcn\u00fc e\u011fitim di\u011fer b\u00f6l\u00fcm\u00fcn\u00fc de test veri seti olarak ay\u0131rma i\u015flemini sklearn k\u00fct\u00fcphanesini kullanarak ger\u00e7ekle\u015ftiriyoruz. Burada train_test_split ile verimizin %80'ini e\u011fitim, %20'sini ise test seti olarak kullanaca\u011f\u0131z ve her seferinde bu veri da\u011f\u0131l\u0131m\u0131 se\u00e7ilirken rasgele de\u011ferler se\u00e7iminin \u00f6n\u00fcne ge\u00e7mek i\u00e7in random_state de\u011ferine 42 de\u011ferini verdik.\n                Daha sonra ise verimizi bir YSA \u00f6rne\u011fi gibi modelimize verece\u011fimiz i\u00e7in her \u00f6zelli\u011fe ait de\u011feri verebilmek i\u00e7in matrisin Transpozunu alaca\u011f\u0131z. ","dbe4cdba":"modelimizi uygulad\u0131\u011f\u0131m\u0131zda hangi iterasyonda nas\u0131l sonu\u00e7 ald\u0131\u011f\u0131m\u0131z\u0131 a\u015fa\u011f\u0131da g\u00f6rmekteyiz. G\u00f6r\u00fcld\u00fc\u011f\u00fc gibi belirli bir iterasyondan sonra ayn\u0131 sonu\u00e7lar\u0131 ald\u0131\u011f\u0131m\u0131z g\u00f6r\u00fclmektedir. ","84a33d0f":"\u015eimdi ise karar s\u0131n\u0131f\u0131m\u0131z ve bu karar s\u0131n\u0131f\u0131n\u0131 etkileten \u00f6zelliklerimizi ay\u0131rmaya yani x ve y veri setlerimizi haz\u0131rlama a\u015famas\u0131nday\u0131z. y bizim karar s\u0131n\u0131f\u0131m\u0131z, x ise karar s\u0131n\u0131f\u0131 d\u0131\u015f\u0131nda bulunan b\u00fct\u00fcn veri setini temsil etmekte.","677d4057":"forward_backward_propagation fonksiyonu ile, forward ve backward propagation i\u015flemlerini ger\u00e7ekle\u015ftiriyoruz. Bu fonksiyonu a\u011f\u0131rl\u0131klar\u0131 ve bias de\u011ferlerini g\u00fcncelleme a\u015famas\u0131nda kullanaca\u011f\u0131z. w ve b de\u011ferlerinin optimum de\u011ferlerine ula\u015fana kadar s\u0131ras\u0131yla ileri-geri yay\u0131l\u0131m i\u015flemini ger\u00e7ekle\u015ftirece\u011fiz.","022eba3c":"\u015eimdi s\u0131ra geldi w ve b de\u011ferlerini g\u00fcncellemeye. Burada belirlenen iterasyona g\u00f6re w ve b de\u011ferleri g\u00fcncellenecek ve forward_backward_propagation fonksiyonu kullan\u0131larak loss ve cost de\u011ferlerinin minimumuna ula\u015f\u0131lmaya \u00e7al\u0131\u015f\u0131lacakt\u0131r.","8a901a8e":"Yukar\u0131daki a\u015famalarda tamamen i\u015flemleri normal matematik kullanarak ger\u00e7ekle\u015ftirdik. Fakat sklearn k\u00fct\u00fcphanesinin linear regression classification i\u015flemi i\u00e7in kullanabilece\u011fimiz bir k\u00fct\u00fcphanesi mevcut. Bu i\u015flem i\u00e7in verimizi e\u011fitim ve test veri seti olarak b\u00f6ld\u00fckten sonra do\u011frudan i\u015flemi ger\u00e7ekle\u015ftirebiliriz. G\u00f6r\u00fcld\u00fc\u011f\u00fc \u00fczere bizim yukar\u0131da buldu\u011fumuz sonu\u00e7lara benzer de\u011fer \u00fcretilmi\u015ftir.","ff5592e4":"sigmoid fonksiyonu ile w*x+b i\u015flemleri sonucu gelen de\u011ferin belirli bir aral\u0131kta olmas\u0131n\u0131 sa\u011flayan i\u015flemimizi ger\u00e7ekle\u015ftiriyoruz.","1661608b":"\u015eimdiye kadar yapt\u0131\u011f\u0131m\u0131z i\u015flem modelimizi olu\u015fturmak i\u00e7in e\u011fitme a\u015famas\u0131yd\u0131. Bundan sonra ise olu\u015fturdu\u011fumuz modelin ne kadar verimli bir model oldu\u011funu test a\u015famas\u0131na s\u0131ra geldi. Bunun i\u00e7in daha \u00f6nce ay\u0131rd\u0131\u011f\u0131m\u0131z test veri setimizi kullanarak modelimizin daha \u00f6nce g\u00f6rmedi\u011fi veride nas\u0131l sonu\u00e7 \u00fcretece\u011fini test edece\u011fiz.","7277b67d":"i\u015flemleri yaparken b\u00fcy\u00fck veri de\u011ferlerimizin k\u00fc\u00e7\u00fck veri de\u011ferlerimizden daha fazla bask\u0131n olmamas\u0131 i\u00e7in normalizasyon \u00e7al\u0131\u015fmas\u0131 yap\u0131lmakta. b\u00fct\u00fcn y veri setimizin tamam\u0131na normalizasyon i\u015flemi uyguluyoruz.","0c019af6":"Verimize ait karar s\u0131n\u0131f\u0131 olan Outcome s\u0131n\u0131f\u0131ndaki verilerin da\u011f\u0131l\u0131m\u0131na bak\u0131yoruz. G\u00f6r\u00fcld\u00fc\u011f\u00fc gibi '1' s\u0131n\u0131f\u0131ndan 268 '0' s\u0131n\u0131f\u0131ndan ise 500 adet veri bulunmakta.","4e7bf3ea":"\u00d6ncelikle verimizi ekliyoruz."}}