{"cell_type":{"1655211a":"code","9850b34b":"code","5b7ca6fd":"code","965a418a":"code","492a2531":"code","d7a514e4":"code","9fa35cb2":"code","0fd0ad2e":"code","016d5339":"code","c7c0c6e0":"code","451c3078":"code","d82793e1":"code","87a87c69":"code","842061a9":"code","feaf9c16":"code","cfe651f8":"code","55c9c3a0":"code","76496f92":"code","edcfedff":"code","1b56135d":"code","07541c51":"code","b4691482":"code","4ea3d06b":"code","3dce3929":"code","e3ce64c8":"code","a04d5623":"code","6c333e8e":"code","9082e8ce":"code","0d4aa8c5":"code","b396022a":"code","b2a69178":"code","963870cb":"code","9312043e":"code","bdb7457c":"code","13f9ff02":"code","9d298887":"code","41a37c86":"code","c4f89774":"code","2d937708":"code","cb4996e8":"code","12b925bc":"code","611832d6":"code","1edc8cde":"code","33245831":"code","dc0915cf":"code","f267bf95":"code","ccd49c77":"code","4a9d2225":"code","5865ef6d":"code","1fef0ec5":"code","ddd2094e":"code","72e74282":"code","d09178b8":"code","ee576be0":"code","2be2d3ed":"code","a690a057":"markdown","32cf64fc":"markdown","2ac45bea":"markdown","48acf0d4":"markdown"},"source":{"1655211a":"#Importing the basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\nfrom plotly import tools\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')","9850b34b":"#Reading the dataset \ndf = pd.read_csv('..\/input\/creditcard.csv')\ndf.shape","5b7ca6fd":"#Let's check if there are any null values \ndf.isnull().sum()","965a418a":"#Describe the dataset to get rough idea about the data \ndf.describe()","492a2531":"#Well let's see all the columns in the dataset \ndf.columns","d7a514e4":"#A brief look at the initial rows of the dataset \ndf.head()","9fa35cb2":"#It's good to shuffle the datset \ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)","0fd0ad2e":"#Let's take a look again \ndf.head()","016d5339":"#Non fradulant cases\ndf.Class.value_counts()[0]","c7c0c6e0":"#Fradulant cases \ndf.Class.value_counts()[1]","451c3078":"print('Percentage of correct transactions: {}'.format((df.Class.value_counts()[0]\/df.shape[0])*100))","d82793e1":"print('Percentage of fradulent transactions: {}'.format((df.Class.value_counts()[1]\/df.shape[0])*100))","87a87c69":"#Let's visualize the distribution of the classes (0 means safe and 1 means fraudulent)\nimport seaborn as sns\ncolors = ['green', 'red']\n\nsns.countplot('Class', data=df, palette=colors)\nplt.title('Normal v\/s Fraudulent')","842061a9":"#Now let's map how much a feature affects our class \ncor = df.corr()\nfig = plt.figure(figsize = (12, 9))\n\n#Plotting the heatmap\nsns.heatmap(cor, vmax = 0.7)\nplt.show()","feaf9c16":"cor.shape","cfe651f8":"#This is how much a each feature affects the our class \ncor.iloc[-1,:]","55c9c3a0":"#We need to delet the least and greatest values \n#From above analysis I've selected the following features \n#Note that I've included the class variable also because I intend to create a new dataframe using the new features\nnew_features=['V1','V3','V4','V7','V10','V11','V12','V14','V16','V17','V18','Class']","76496f92":"#Let's plot a heatmap again and see the relationship\ncor = df[new_features].corr()\nfig = plt.figure(figsize = (12, 9))\n\n#Plotting the heatmap\nsns.heatmap(cor, vmax = 0.7)\nplt.show()","edcfedff":"#We see that the class rows and columns are darker and brighter \n#This means that all the variables in our new dataset have a significant affect ","1b56135d":"#Now splitting the dataset into the dependent variable(y) and independent variales(x)\nx=df[new_features].iloc[:,:-1].values\ny=df[new_features].iloc[:,-1].values","07541c51":"#Withoud reducing the features \n#x=df.iloc[:,:-1].values\n#y=df.iloc[:,-1].values\n#Feel free to try using all the features :)","b4691482":"x.shape","4ea3d06b":"y.shape","3dce3929":"x[:5]","e3ce64c8":"y[:5]","a04d5623":"#Spliting the data into train and test sets \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n","6c333e8e":"#Let's see how many safe and fraudulent cases are there in training set \nsafe_train=(y_train==0).sum()\nfraud_train=(y_train==1).sum()\nprint(\"Safe: {} \\nFraud: {}\".format(safe_train,fraud_train))","9082e8ce":"#Let's see how many safe and fraudulent cases are there in test set \nsafe_test=(y_test==0).sum()\nfraud_test=(y_test==1).sum()\nprint(\"Safe: {} \\nFraud: {}\".format(safe_test,fraud_test))","0d4aa8c5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix","b396022a":"#Using Logistic Regression \nclf = LogisticRegression(random_state = 0)\nclf.fit(x_train, y_train)","b2a69178":"#Let's evaluate our model \ny_pred = clf.predict(x_test)\nprint(\"Training Accuracy: \",clf.score(x_train, y_train))\nprint(\"Testing Accuracy: \", clf.score(x_test, y_test))\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test,y_pred)) ","963870cb":"from sklearn.metrics import roc_curve, auc","9312043e":"#Calculating the FPR and TPR\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)","bdb7457c":"#Plotting the curves \nlabel = 'Logistic Regressoin Classifier AUC:' + ' {0:.2f}'.format(roc_auc)\nlabel2 = 'Random Model' \nplt.figure(figsize = (20, 12))\nplt.plot([0,1], [0,1], 'r--', label=label2)\nplt.plot(fpr, tpr, c = 'g', label = label, linewidth = 4)\nplt.xlabel('False Positive Rate', fontsize = 16)\nplt.ylabel('True Positive Rate', fontsize = 16)\nplt.title('Receiver Operating Characteristic', fontsize = 16)\nplt.legend(loc = 'lower right', fontsize = 16)","13f9ff02":"#We see that AUC is 0.78 which is not bad, not great but fair enough","9d298887":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclf.fit(x_train, y_train)","41a37c86":"y_pred = clf.predict(x_test)\nprint(\"Training Accuracy: \",clf.score(x_train, y_train))\nprint(\"Testing Accuracy: \", clf.score(x_test, y_test))\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test,y_pred)) ","c4f89774":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)","2d937708":"label = 'KNN Classifier AUC:' + ' {0:.2f}'.format(roc_auc)\nlabel2 = 'Random Model' \nplt.figure(figsize = (20, 12))\nplt.plot([0,1], [0,1], 'r--', label=label2)\nplt.plot(fpr, tpr, c = 'g', label = label, linewidth = 4)\nplt.xlabel('False Positive Rate', fontsize = 16)\nplt.ylabel('True Positive Rate', fontsize = 16)\nplt.title('Receiver Operating Characteristic', fontsize = 16)\nplt.legend(loc = 'lower right', fontsize = 16)","cb4996e8":"#We see that AUC is 0.90 which is good, with different random state of data you may get different AUC.\n#I once achieved 0.94\n#Though I've kept a constant seed for random state wile shuffling the dataset, feel free to mess with that :D","12b925bc":"(df.Class.value_counts()[1]\/df.Class.value_counts()[0])","611832d6":"#Importing and fitting the Isolation Forest Algorithm \nfrom sklearn.ensemble import IsolationForest\nclf=IsolationForest(contamination=(df.Class.value_counts()[1]\/df.shape[0]), random_state=123,max_features=x.shape[1])\nclf.fit(x)","1edc8cde":"#Predicting the class\ny_pred = clf.predict(x)","33245831":"#Since the algorithm classifies one class as 1 and other as -1\n#Let's see how many classes it predicted as fraudulent \n(y_pred==-1).sum()","dc0915cf":"#Since our class variables are either 0 or 1, so we need to replace the predicted classes as 0 and 1\ny_pred[y_pred == 1] = 0\ny_pred[y_pred == -1] = 1","f267bf95":"#Let's see how good our model performed \nprint(\"Training Accuracy: \",accuracy_score(y, y_pred))\ncm = confusion_matrix(y, y_pred)\nprint(cm)\nprint(classification_report(y,y_pred))","ccd49c77":"fpr, tpr, thresholds = roc_curve(y, y_pred)\nroc_auc = auc(fpr, tpr)","4a9d2225":"label = 'Isolation Forest Classifier AUC:' + ' {0:.2f}'.format(roc_auc)\nlabel2 = 'Random Model' \nplt.figure(figsize = (20, 12))\nplt.plot([0,1], [0,1], 'r--', label=label2)\nplt.plot(fpr, tpr, c = 'g', label = label, linewidth = 4)\nplt.xlabel('False Positive Rate', fontsize = 16)\nplt.ylabel('True Positive Rate', fontsize = 16)\nplt.title('Receiver Operating Characteristic', fontsize = 16)\nplt.legend(loc = 'lower right', fontsize = 16)","5865ef6d":"#We get AUC score of 0.75, that's bad. Seems like this algorithm is not working very well on our dataset ","1fef0ec5":"#Let's try another unsupervised algorithm \nfrom sklearn.neighbors import LocalOutlierFactor\nclf=LocalOutlierFactor(n_neighbors=5,contamination=(df.Class.value_counts()[1]\/df.shape[0]))","ddd2094e":"y_pred = clf.fit_predict(x)\ny_pred[y_pred == 1] = 0\ny_pred[y_pred == -1] = 1","72e74282":"print(\"Training Accuracy: \",accuracy_score(y, y_pred))\ncm = confusion_matrix(y, y_pred)\nprint(cm)\nprint(classification_report(y,y_pred))","d09178b8":"fpr, tpr, thresholds = roc_curve(y, y_pred)\nroc_auc = auc(fpr, tpr)","ee576be0":"label = 'KNN Classifier AUC:' + ' {0:.2f}'.format(roc_auc)\nlabel2 = 'Random Model' \nplt.figure(figsize = (20, 12))\nplt.plot([0,1], [0,1], 'r--', label=label2)\nplt.plot(fpr, tpr, c = 'g', label = label, linewidth = 4)\nplt.xlabel('False Positive Rate', fontsize = 16)\nplt.ylabel('True Positive Rate', fontsize = 16)\nplt.title('Receiver Operating Characteristic', fontsize = 16)\nplt.legend(loc = 'lower right', fontsize = 16)","2be2d3ed":"#WHAT!!!!? That's a garbage model with an AUC of 0.5.\n#Moreover it made 0 true negatives and that's a teriible, terrible model","a690a057":"# Looking at the Accuracy fallacy \n\n## What it is ? \nIf we look at the accuracy on test set, it's nearly 0.99914 i.e. over 99.9% accuracy.\nDoes that mean our model is phenomenal? No!\nBefore that let me tell you what a confusion matrix for binary classification represents \nWe made the matrix between the actual values and predicted values, so the rows represent the actual values and columns respresent the predicted values. Now that you know what rows and colmns are let's understand what each cell represnt hence:\n1. [0][0] -> True positives i.e. how many safe cases are there that our model predicted correctly. Here 71069 are the number of CORRECTLY PREDICTED safe cases. \n2. [0][1] -> False Negatives i.e. how many safe cases are there that our model predicted incorrectly. Here 4 are the number of the MISCLASSIFIED safe cases. Hence 4 safe cases were misclassified as fraud. This is potentially less dangerous as it's better to stop some safe transactions with slightest chance of fraud.\n3. [1][0] -> False Positives i.e. how many fraud cases are there that our model predicted incorrectly. Here 57 are the number of the MISCLASSIFIED fraud cases. Here 57 fraud cases were misclassified as safe. This is very dangerous because we are letting the fraud cases pass through. This can cause huge loss to the organization. \n4. [1][1] -> True negatives i.e. how many fraud cases are there that our model predicted correctly. Here 72 are the number of CORRECTLY PREDICTED fraud cases.\n\n### We can see that despite having an accuracy over 99.9% our model predicted 57 fraud cases incorrectly. This is what I call accuracy fallacy.\nThis usually happens when data is UNEVENLY DISTRIBUTED. From above code we realize that the number of fraud cases in trianing set is just 363 whereas the number of safe ones are 213242. This is very unevenly distributed and will give great accuracy but misclassify the dangerous classes.\n\nTo give you a perspective, let me misclassify all the fraudulent cases.\nLet's predict every case as safe, so our confusion matrix for the test set is as follows:\n\n[[71073         0]\n\n [129           0]]\n \nSo new accuracy = 71073\/(71073 +  129) = 0.9981\nwhich is nearly 99.8% accuracy.\nAgain great accuracy, terrible performance.\n\n## So how we measure model performance?\n\n### The answer is precision, recall, f1 score and AUC-ROC curve.\nLets look at each one of them one by one\n![image.png](attachment:image.png)\n\nImage source : Wikipedia \n### Precision:\nPrecision refers to the percentage of your results which are relevant and is calculated as follows : \nTrue Positives\/(True Positives + Flase Positives)\n\n### Recall:\nRecall refers to the percentage of total relevant results correctly classified by your algorithm and is calculated as follows : True Positives\/(True Positives + False Negatives)\n\n### F1 Score:\nIt is the harmonic mean of precision and recall and is calculated as follows: \n(2 x Precision x Recall)\/(Precision + Recall)\n\n## Hence the greater the F1 score the better it is\nIn the above example we see that f1 score of safe class is 1 but that of fraud is 0.70 and hence the macro average is 0.85 \n\n### AUC-ROC curve (Area Under Curve - Receiver Operating Characteristic Curve):\nThe True Positive Rate (TPR) is plot against False Positive Rate (FPR) for the probabilities of the classifier predictions. Then, the area under the plot is calculated. The greater the area under the curve the better our model is.\n\nTPR is also knowns as recall and hence = True Positives\/(True Positives + False Negatives)\n\nFPR is negation of specificity = 1 - Specificity = 1- True Negatives\/(True Negatives + False Positives) \n\nHence FPR = False Positives\/(True Negatives + False Positives)\n\n","32cf64fc":"## Let's try a supervised anomaly detection algorithm KNN\n\nWe use anomaly detection algorithm to find unusual patterns in the data. Since the data is unbalanced greatly and there must be some unusual patterns in data, let's try K Nearest Neighbours Algorithm ","2ac45bea":"For now it seems that *KNN* is performing quite well for the given data","48acf0d4":"## Now let's try an unsupervised algorithm namely Isolation forest \n\nSince it is unsupervised, we don't need the target feature that is our class variable. Also we can use entire dataset instead of training on one and hence testing on the other\n\nIsolation forest tries to separate each point in the data. Here an anomalous point could be separated in a few steps while normal points which are closer could take significantly more steps to be segregated."}}