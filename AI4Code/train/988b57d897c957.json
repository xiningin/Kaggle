{"cell_type":{"6ef470ae":"code","a19afa6f":"code","519e72ba":"code","867d62cc":"code","8fcdd03a":"code","f938c86b":"code","b59ee836":"code","4ea8315b":"code","3a750534":"code","39e783ba":"code","33b6dfda":"code","cd3db9f5":"code","edb6f2cb":"code","30caffd0":"code","b3ebffe1":"code","d194a358":"code","f8039d31":"code","e343401c":"code","a6a46875":"code","097aa281":"code","8b0987f9":"code","0c32b225":"code","1429e7f1":"code","7d4b828a":"code","e7633e84":"code","f434d175":"code","732e16aa":"code","8918517a":"code","37116da8":"markdown"},"source":{"6ef470ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a19afa6f":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport pandas as pd\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score","519e72ba":"train = pd.read_csv(\"..\/input\/mnist_train.csv\")\ntest = pd.read_csv(\"..\/input\/mnist_test.csv\")","867d62cc":"print(train.shape)\nprint(test.shape)\nprint(train.columns)\nprint(train[:2])\nprint(test[:2])\nprint(train[:10][\"label\"])\nprint(test[:10][\"label\"])","8fcdd03a":"train = np.array(train)\ntest = np.array(test)","f938c86b":"print(train[10:15])\nprint(test[10:15])","b59ee836":"train_x = train[:,1:]\ntrain_y = pd.get_dummies(train[:,0])\ntest_x = test[:,1:]\ntest_y = pd.get_dummies(test[:,0])","4ea8315b":"print(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)\nprint(train_x[10:12])\nprint(train_y[10:12])\nprint(test_x[10:12])\nprint(test_y[10:12])","3a750534":"#NETWORK PARAMETERS\nn_steps = 28\nn_inputs = 28\nn_neurons = 150\nn_outputs = 10","39e783ba":"train_x = train_x.reshape(-1,n_steps,n_inputs)\ntest_x = test_x.reshape(-1,n_steps,n_inputs)","33b6dfda":"print(train_x.shape)\nprint(test_x.shape)\nprint(train_x[0:1])\nprint(test_x[0:1])","cd3db9f5":"train_X , train_y = shuffle(train_x , train_y)\ntest_X , test_y = shuffle(test_x , test_y)","edb6f2cb":"tf.reset_default_graph()","30caffd0":"# Training Parameters\nlearning_rate = 0.001\ntraining_iters = 500\nbatch_size = 150\ndisplay_step = 200","b3ebffe1":"X = tf.placeholder(tf.float32,[None,n_steps,n_inputs])\ny = tf.placeholder(tf.int32,[None,n_outputs])","d194a358":"def RNN(X):\n    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n    outputs , states = tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n    out = tf.layers.dense(states, n_outputs)\n    out = tf.nn.softmax(out)\n    return out\n    ","f8039d31":"prediction = RNN(X)","e343401c":"# Define loss and optimizer\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)","a6a46875":"# correct_pred = tf.nn.in_top_k(prediction, y, 1)","097aa281":"# Evaluate model (with test logits, for dropout to be disabled)\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initialize the variables (i.e. assign their default value)\ninit = tf.global_variables_initializer()","8b0987f9":"sess = tf.Session()\nsess.run(init) \n\nfor i in range(training_iters):\n    for batch in range(len(train_X)\/\/batch_size):\n        batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n        batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]\n        batch_x = batch_x.reshape((-1, n_steps, n_inputs))\n\n        sess.run(train_op, feed_dict={X: batch_x, y: batch_y})\n        loss = sess.run([loss_op], feed_dict={X: batch_x, y: batch_y})\n    predTest = sess.run(prediction , feed_dict={X:test_X})\n    acc_train = accuracy.eval(session=sess, feed_dict={X: batch_x, y: batch_y})\n    acc_test = accuracy.eval(session=sess, feed_dict={X: test_X, y: test_y})\n    print(\"Iter \"+str(i)+\" Out of\",training_iters , \" Loss= \",loss,\"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n        \n        \n            # Calculate batch loss and accuracy\n#         loss = sess.run([loss_op], feed_dict={X: batch_x, y: batch_y})\n    \n#     predTest = sess.run(prediction , feed_dict={X:test_X})\n\n#     p = np.argmax(predTest,1)\n#     t = np.argmax(np.array(test_y),1)\n\n#     acc = accuracy_score(p,t)\n#     print(\"Iter \"+str(i)+\" Out of\",training_iters , \" Loss= \",loss, \"acc=\",acc )\n            \n#     acc = sess.run([accuracy], feed_dict={X: batch_x, y: batch_y})\n        \n#     print(\"Step \" + str(i) + \",        Batch Loss= \",loss, \",       Training Accuracy= \",acc)\n    \nprint(\"Optimization Finished!\")","0c32b225":"while(True):\n    r = np.random.randint(9000)\n    test_img = np.reshape(test_X[r], (28,28))\n    plt.imshow(test_img, cmap=\"gray\")\n    test_pred = sess.run(prediction, feed_dict = {X:[test_X[r]]})\n    print(\"Model : I think it is :    \",np.argmax(test_pred))\n    plt.show()\n    \n    if input(\"Enter n to exit\")=='n':\n        break\nclear_output();","1429e7f1":"wrong = test_X[tf.argmax(prediction, 1)!=tf.argmax(y, 1)]\nwrong.shape","7d4b828a":"a,b,c, d = wrong.shape","e7633e84":"wrong = np.reshape(wrong, (b,c,d))   \nwrong.shape","f434d175":"while(True):\n    r=np.random.randint(b)\n    plt.imshow(wrong[r].reshape((28,28)),cmap=\"gray\")\n    test_pred_1=sess.run(prediction, feed_dict = {X:[wrong[r]]})\n    print(\"Model : I think it is :    \",np.argmax(test_pred_1))\n    plt.show()\n    \n    if input(\"Enter n to exit\")=='n':\n        break\nclear_output();","732e16aa":"p = np.argmax(predTest,1)\nprint(p)\nt = np.argmax(np.array(test_y),1)\nprint(t)\nacc = accuracy_score(p,t)\nprint(acc*100)","8918517a":"print(\"Saving Weights\")\nsaver = tf.train.Saver()\nsaver.save(sess,\"weights_\"+str(i)+\"\/weights.ckpt\")\nprint(\"Weights Saved\")","37116da8":"#### We will treat each image as a sequence of 28 rows of 28 pixels each (since each MNIST image is 28 \u00d7 28 pixels). We will use cells of 150 recurrent neurons, plus a fully connected layer containing 10 neurons (one per class) connected to the output of the last time step, followed by a softmax layer"}}