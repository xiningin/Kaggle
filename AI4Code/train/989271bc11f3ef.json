{"cell_type":{"bb3a64fe":"code","8d054e08":"code","d7126a5e":"code","1c796141":"code","e138f682":"code","fbbb5816":"code","ed9a8daa":"code","24c17d8e":"code","1ff51c7e":"code","56cad734":"code","ebd06435":"code","78e65406":"code","82a3116a":"code","2cc67c2c":"code","2a363cb9":"code","248e678a":"code","8be7d265":"code","604e6810":"code","74069a4c":"code","827174fb":"code","fcc024dd":"code","c3a851c2":"code","fa1b72fd":"code","0500db2f":"code","e1e0f7f3":"markdown","2bc0001f":"markdown","d446e6f6":"markdown","d2afc85e":"markdown","165bff90":"markdown","8c54a98d":"markdown","9b1f84b7":"markdown","908eb1bd":"markdown","cbc2635d":"markdown","f0838615":"markdown","03dff110":"markdown","243d7689":"markdown","6a8a1711":"markdown","a5f5715c":"markdown","0d095f71":"markdown","e527d636":"markdown"},"source":{"bb3a64fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d054e08":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf","d7126a5e":"heart = '..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv'\no2 = '..\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv'\n\ndf_heart = pd.read_csv(heart)\ndf_o2 = pd.read_csv(o2)","1c796141":"df = pd.concat([df_heart, df_o2], axis=1, join='inner')\ndf.head()","e138f682":"df.rename(columns={'98.6':'o2'})","fbbb5816":"df.isna().sum()","ed9a8daa":"df.info()","24c17d8e":"catFeatures = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall']\ni=1\nfig = plt.figure(figsize=(10,8))\nfig.subplots_adjust(hspace=0.5, wspace=1)\nfor catFeature in catFeatures:\n    ax = fig.add_subplot(2, 4, i)\n    sns.countplot(x = df[catFeature])\n    i+=1","1ff51c7e":"plt.figure(figsize=(7,5))\ndf['trtbps'].plot(kind='density')\nplt.title(\"Resting blood pressure \")\nplt.show()\n\nplt.figure(figsize=(7,5))\ndf['chol'].plot(kind='density')\nplt.title(\"Cholestoral\")\nplt.show()\n\nplt.figure(figsize=(7,5))\ndf['thalachh'].plot(kind='density')\nplt.title(\"Maximum heart rate achieved\")\nplt.show()\n\nplt.figure(figsize=(7,5))\ndf['age'].plot(kind='density')\nplt.title(\"Age\")\nplt.show()","56cad734":"cor_mat = df.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(cor_mat, annot=True)\nplt.show()","ebd06435":"X = df.drop('output', axis=1)\ny = df['output']","78e65406":"X.head()","82a3116a":"y.head()","2cc67c2c":"sns.countplot(x=\"output\", data=df)","2a363cb9":"# Split the data\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","248e678a":"# preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nscalar = MinMaxScaler()\nX_train = scalar.fit_transform(X_train)\nX_test = scalar.transform(X_test)","8be7d265":"# model to find optimal learning rate\ntf.random.set_seed(42)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy,\n             optimizer=tf.keras.optimizers.Adam(),\n             metrics=['accuracy'])\n\n# Create a learning rate callback\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch\/20))\n\nhistory = model.fit(X_train, y_train, epochs=100, callbacks=[lr_scheduler], verbose=0)","604e6810":"# Plot the learning rate versus the loss\nlrs = 1e-4 * (10 ** (tf.range(100)\/20))\nplt.figure(figsize=(10,8))\nplt.semilogx(lrs, history.history['loss'])\nplt.xlabel('learning rate')\nplt.ylabel('loss')\nplt.title('Learning rate vs Loss')\nplt.show();","74069a4c":"# model creation\ntf.random.set_seed(42)\n\nmodel_F = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel_F.compile(loss=tf.keras.losses.binary_crossentropy,\n             optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n             metrics=['accuracy'])\n\nhistory_F = model_F.fit(X_train, y_train, epochs=50, verbose=0)","827174fb":"model_F.evaluate(X_test, y_test)","fcc024dd":"pd.DataFrame(history_F.history).plot(xlabel='epochs', figsize=(6,4))","c3a851c2":"# Check model summary\nmodel.summary()","fa1b72fd":"y_pred = tf.round(model_F.predict(X_test))\ny_pred[:10]","0500db2f":"from sklearn.metrics import confusion_matrix\n#Visualize confusion matrix\nplt.figure(figsize = (8, 8))\nsns.heatmap(confusion_matrix(y_test, tf.round(y_pred)), cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15},\n           yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()","e1e0f7f3":"### Spliting the train and test data","2bc0001f":"### Count plot","d446e6f6":"### Normalizing our data","d2afc85e":"Creating model with learning rate callback","165bff90":"### Final model creation","8c54a98d":"### Feature and Label selection","9b1f84b7":"### Plotting loss and learning rate curve to find the optimal learning rate","908eb1bd":"Loss and accuracy curve","cbc2635d":"### Prediction by our model","f0838615":"### Checking for skewness","03dff110":"### Reading the data set","243d7689":"### Confusion matrix","6a8a1711":"We can see that loss is still decreasing with learning rate between 0.001 to 0.01. For the above curve we can see that 0.003 is a good option","a5f5715c":"### Creating our neural network","0d095f71":"### **So, The accuracy by our model on test data is 90.16%**","e527d636":"### Importing Libraries"}}