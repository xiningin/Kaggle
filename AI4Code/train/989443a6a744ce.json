{"cell_type":{"88f4ee55":"code","24401488":"code","49f2a85e":"code","616d75a7":"code","ae2f6b3a":"code","d77ac5b1":"code","5623573f":"code","8628c20a":"code","b5049674":"code","4d130ac7":"code","e19f7e50":"code","be901724":"code","99a63df8":"code","d5cdd6e6":"code","8fc4bceb":"code","02a8aef5":"code","fffb5ff5":"code","459e57c9":"code","c305bc0b":"code","e348cb28":"markdown","41233149":"markdown","1128db28":"markdown","ccb41dc2":"markdown","3d92041c":"markdown","22d397aa":"markdown","dddb60d0":"markdown","ccff84df":"markdown","de6f7f97":"markdown","43349a5b":"markdown","0aee67be":"markdown","b2a1f322":"markdown"},"source":{"88f4ee55":"!pip install transformers\nimport transformers","24401488":"import pandas as pd\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","49f2a85e":"import matplotlib.pyplot as plt\nplt.title('Train Data')\nplt.xlabel('Target Distribution')\nplt.ylabel('Samples')\nplt.hist(train.target)\nplt.show()","616d75a7":"\n# def decontracted(phrase):\n#     # specific\n#     phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n#     phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n#     # general\n#     phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n#     phrase = re.sub(r\"\\'re\", \" are\", phrase)\n#     phrase = re.sub(r\"\\'s\", \" is\", phrase)\n#     phrase = re.sub(r\"\\'d\", \" would\", phrase)\n#     phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n#     phrase = re.sub(r\"\\'t\", \" not\", phrase)\n#     phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n#     phrase = re.sub(r\"\\'m\", \" am\", phrase)\n#     return phrase","ae2f6b3a":"# import spacy\n# import re\n# nlp = spacy.load('en')\n# def preprocessing(text):\n#   text = text.replace('#','')\n#   text = decontracted(text)\n#   text = re.sub('\\S*@\\S*\\s?','',text)\n#   text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n#   token=[]\n#   result=''\n#   text = re.sub('[^A-z]', ' ',text.lower())\n  \n#   text = nlp(text)\n#   for t in text:\n#     if not t.is_stop and len(t)>2:  \n#       token.append(t.lemma_)\n#   result = ' '.join([i for i in token])\n\n#   return result.strip()","d77ac5b1":"# train.text = train.text.apply(lambda x : preprocessing(x))\n# test.text = test.text.apply(lambda x : preprocessing(x))","5623573f":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)","8628c20a":"import numpy as np\nimport tensorflow as tf ","b5049674":"def bert_encode(data,maximum_length) :\n  input_ids = []\n  attention_masks = []\n  \n\n  for i in range(len(data.text)):\n      encoded = tokenizer.encode_plus(\n        \n        data.text[i],\n        add_special_tokens=True,\n        max_length=maximum_length,\n        pad_to_max_length=True,\n        \n        return_attention_mask=True,\n        \n      )\n      \n      input_ids.append(encoded['input_ids'])\n      attention_masks.append(encoded['attention_mask'])\n  return np.array(input_ids),np.array(attention_masks)","4d130ac7":"train_input_ids,train_attention_masks = bert_encode(train,60)\ntest_input_ids,test_attention_masks = bert_encode(test,60)","e19f7e50":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\ndef create_model(bert_model):\n  input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n  attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n  \n  output = bert_model([input_ids,attention_masks])\n  output = output[1]\n  output = tf.keras.layers.Dense(32,activation='relu')(output)\n  output = tf.keras.layers.Dropout(0.2)(output)\n\n  output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n  model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n  model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])\n  return model\n\n","be901724":"from transformers import TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-large-uncased')","99a63df8":"model = create_model(bert_model)\nmodel.summary()","d5cdd6e6":"# filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n# callbacks_list = [checkpoint]\n# add callbacks = callbacks_list to model.fit","8fc4bceb":"history = model.fit([train_input_ids,train_attention_masks],train.target,validation_split=0.2, epochs=2,batch_size=10)","02a8aef5":"result = model.predict([test_input_ids,test_attention_masks])\nresult = np.round(result).astype(int)","fffb5ff5":"result = pd.DataFrame(result)\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\noutput = pd.DataFrame({'id':submission.id,'target':result[0]})\noutput.to_csv('submission.csv',index=False)","459e57c9":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","c305bc0b":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","e348cb28":"**Pre-Processing** (Optional)\n\n1--> Removing Contraction (Decontraction)\n\n2--> Dealing with HashTags\n\n3--> Removing URLs and Email\n\n4--> Removing Stopwords and Lemmatization\n\n","41233149":"**BERT Encoding**\n\nData is encoded according to BERT requirement.There is a very helpful function called encode_plus provided in the Tokenizer class. It can seamlessly perform the following operations:\n\n\n\n*   Tokenize the text\n*   Add special tokens - [CLS] and [SEP]\n\n*   Add special tokens - [CLS] and [SEP]\n*   create token IDs\n\n*   Pad the sentences to a common length\n*   Create attention masks for the above PAD tokens\n","1128db28":"**Preparing Submission File**","ccb41dc2":"**TFBertModel**\n\nThe bare Bert Model transformer outputing raw hidden-states without any specific head on top. \nhttps:\/\/huggingface.co\/transformers\/model_doc\/bert.html#tfbertmodel","3d92041c":"**Testing**","22d397aa":"**HuggingFace Transformers**\n\n\ud83e\udd17 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet\u2026) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.","dddb60d0":"**Input are 2 Numpy array. Let me briefly go over them:**\n\n1) input_ids : list of token ids to be fed to a model\n\n2) attention_masks: list of indices specifying which tokens should be attended to by the model.The input sequences are denoted by 1 and the padded ones by 0. These masks help to differentiate between the two.\n\n**Note** : Token Ids are not necessary as it is used Two Sentence Problem (To differentiate two sentence)\n","ccff84df":"**Creating Custom Model**\n\nBase TFBert Model with Dense layer and sigmoid activation as head. ","de6f7f97":"**Implementing Custom Model**","43349a5b":"**Loading BertTokenizer**\n\nIt is based on WordPiece Approach","0aee67be":"**Callbacks** (Optional)\n\nTo get best model according to maximum validation accuracy.\n\n","b2a1f322":"**Training**"}}