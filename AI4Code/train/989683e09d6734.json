{"cell_type":{"5a9e00f4":"code","06f3655c":"code","a8039df5":"code","02821cb4":"code","36e3ef74":"code","7fb140de":"code","fe4fd6b7":"code","f441229b":"code","47edd4ab":"code","8371f680":"code","0e54a03e":"code","a430003b":"code","5f511ad5":"code","80fb72c8":"code","51e9c5c7":"code","899269fd":"code","d272e792":"code","5c0d2fef":"code","b8649582":"code","875866e6":"code","8f1d865d":"code","2cd7e8d8":"code","b03be044":"code","a6beb54c":"code","c35bf587":"code","54bf8b34":"code","199db4d9":"code","78c2c61b":"code","4a10bc95":"code","326d925c":"code","eaef3e9b":"code","f6dcdaba":"code","b24b66d7":"code","af46ff55":"code","be3fa046":"code","1f124dd6":"code","1dbb16fc":"code","f2ed93a0":"code","cf55c441":"code","3826016a":"code","c3812f1e":"code","eaa3b69f":"code","660089bb":"code","b6b6701b":"code","7edaa6c9":"code","1808240d":"code","160b5e6c":"code","0cf6747c":"markdown","0dd8148d":"markdown","487cc7cc":"markdown","850a65a3":"markdown","0d5a147c":"markdown","9a1a15a8":"markdown","8b0f482c":"markdown","c93b0ff7":"markdown","ad821904":"markdown","3a506d19":"markdown","fd139368":"markdown","42b5331e":"markdown","e8bbf8e7":"markdown","41e3b3c1":"markdown","7740590d":"markdown","9ebe5829":"markdown","3ec1cabb":"markdown","86fb4350":"markdown","0540d850":"markdown","bbf5fc3e":"markdown","a0371e1a":"markdown","9e59982f":"markdown","a0b875dc":"markdown","eba278b4":"markdown"},"source":{"5a9e00f4":"# Import libraries\n\nimport pandas as pd\nimport numpy as np","06f3655c":"# Read the data \n\nX_train = pd.read_csv(\"..\/input\/labeledTrainData.tsv\",quoting = 3, delimiter = \"\\t\", header= 0)\nX_test = pd.read_csv(\"..\/input\/testData.tsv\", quoting = 3, delimiter = \"\\t\", header = 0)","a8039df5":"# Read only the first 600 sentences of the first review.\n\nX_train['review'][0][:600]","02821cb4":"print('Training set dimension:',X_train.shape)\nprint('Test set dimension:',X_test.shape)","36e3ef74":"X_train.head()","7fb140de":"from bs4 import BeautifulSoup\nimport re\nimport nltk","fe4fd6b7":"def prep(review):\n    \n    # Remove HTML tags.\n    review = BeautifulSoup(review,'html.parser').get_text()\n    \n    # Remove non-letters\n    review = re.sub(\"[^a-zA-Z]\", \" \", review)\n    \n    # Lower case\n    review = review.lower()\n    \n    # Tokenize to each word.\n    token = nltk.word_tokenize(review)\n    \n    # Stemming\n    review = [nltk.stem.SnowballStemmer('english').stem(w) for w in token]\n    \n    # Join the words back into one string separated by space, and return the result.\n    return \" \".join(review)\n    ","f441229b":"# test whether the function successfully preprocessed.\nX_train['review'].iloc[:2].apply(prep).iloc[0]","47edd4ab":"# If there is no problem at the previous cell, let's apply to all the rows.\nX_train['clean'] = X_train['review'].apply(prep)\nX_test['clean'] = X_test['review'].apply(prep)","8371f680":"X_train['clean'].iloc[3]","0e54a03e":"print('Training dim:',X_train.shape, 'Test dim:', X_test.shape)","a430003b":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import words","5f511ad5":"# analyzer is the parameter that the vectorizer reads the input data in word unit or character unit to create a matrix\n# vocabulary is the parameter that the vectorizer creates the matrix by using only input data or some other source \n# Other parameters are self-explanatory and already mentioned in other notebooks.\n\ntv = TfidfVectorizer(\n                    ngram_range = (1,3),\n                    sublinear_tf = True,\n                    max_features = 40000)","80fb72c8":"# Handle with care especially when you transform the test dataset. (Wrong: fit_transform(X_test))\n\ntrain_tv = tv.fit_transform(X_train['clean'])\ntest_tv = tv.transform(X_test['clean'])","51e9c5c7":"# Create the list of vocabulary used for the vectorizer.\n\nvocab = tv.get_feature_names()\nprint(vocab[:5])","899269fd":"print(\"Vocabulary length:\", len(vocab))","d272e792":"dist = np.sum(train_tv, axis=0)\nchecking = pd.DataFrame(dist,columns = vocab)","5c0d2fef":"checking","b8649582":"print('Training dim:',train_tv.shape, 'Test dim:', test_tv.shape)","875866e6":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n%matplotlib inline","8f1d865d":"def cloud(data,backgroundcolor = 'white', width = 800, height = 600):\n    wordcloud = WordCloud(stopwords = STOPWORDS, background_color = backgroundcolor,\n                         width = width, height = height).generate(data)\n    plt.figure(figsize = (15, 10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()\n    ","2cd7e8d8":"cloud(' '.join(X_train['clean']))","b03be044":"cloud(' '.join(X_test['clean']))","a6beb54c":"# We need to split each words in cleaned review and then count the number of each rows of data frame.\n\nX_train['freq_word'] = X_train['clean'].apply(lambda x: len(str(x).split()))\nX_train['unique_freq_word'] = X_train['clean'].apply(lambda x: len(set(str(x).split())))\n                                                 \nX_test['freq_word'] = X_test['clean'].apply(lambda x: len(str(x).split()))\nX_test['unique_freq_word'] = X_test['clean'].apply(lambda x: len(set(str(x).split())))                                                 ","c35bf587":"fig, axes = plt.subplots(ncols=2)\nfig.set_size_inches(10,5)\n\nsns.distplot(X_train['freq_word'], bins = 90, ax=axes[0], fit = stats.norm)\n(mu0, sigma0) = stats.norm.fit(X_train['freq_word'])\naxes[0].legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu0, sigma0)],loc='best')\naxes[0].set_title(\"Distribution Word Frequency\")\naxes[0].axvline(X_train['freq_word'].median(), linestyle='dashed')\nprint(\"median of word frequency: \", X_train['freq_word'].median())\n\n\nsns.distplot(X_train['unique_freq_word'], bins = 90, ax=axes[1], color = 'r', fit = stats.norm)\n(mu1, sigma1) = stats.norm.fit(X_train['unique_freq_word'])\naxes[1].set_title(\"Distribution Unique Word Frequency\")\naxes[1].legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu1, sigma1)],loc='best')\naxes[1].axvline(X_train['unique_freq_word'].median(), linestyle='dashed')\nprint(\"median of uniuqe word frequency: \", X_train['unique_freq_word'].median())","54bf8b34":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neural_network import MLPClassifier","199db4d9":"kfold = StratifiedKFold( n_splits = 5, random_state = 2018 )","78c2c61b":"# LinearSVC\n\nsv = LinearSVC(random_state=2018)\n\nparam_grid2 = {\n    'loss':['squared_hinge'],\n    'class_weight':[{1:4}],\n    'C': [0.2]\n}\n\n\ngs_sv = GridSearchCV(sv, param_grid = [param_grid2], verbose = 1, cv = kfold, n_jobs = 1, scoring = 'roc_auc')\ngs_sv.fit(train_tv, X_train['sentiment'])\ngs_sv_best = gs_sv.best_estimator_\nprint(gs_sv.best_params_)\n\n# {'C': 0.1, 'class_weight': {1: 3}, 'loss': 'squared_hinge'} - 0.87220\n# {'C': 0.1, 'class_weight': {1: 4}, 'loss': 'squared_hinge'} - 0.86060\n# {'C': 0.2, 'class_weight': {1: 4}, 'loss': 'squared_hinge'} - 0.87952","4a10bc95":"submission1 = gs_sv.predict(test_tv)","326d925c":"print(gs_sv.best_score_)","eaef3e9b":"bnb = BernoulliNB()\ngs_bnb = GridSearchCV(bnb, param_grid = {'alpha': [0.001],\n                                         'binarize': [0.001]}, verbose = 1, cv = kfold, n_jobs = 1, scoring = \"roc_auc\")\ngs_bnb.fit(train_tv, X_train['sentiment'])\ngs_bnb_best = gs_bnb.best_estimator_\nprint(gs_bnb.best_params_)\n\n# {'alpha': 0.001, 'binarize': 0.001} - 0.86960\n","f6dcdaba":"submission2 = gs_bnb.predict(test_tv)","b24b66d7":"print(gs_bnb.best_score_)","af46ff55":"MLP = MLPClassifier(random_state = 2018)\n\nmlp_param_grid = {\n    'hidden_layer_sizes':[(5)],\n    'activation':['relu'],\n    'solver':['adam'],\n    'alpha':[0.3],\n    'learning_rate':['constant'],\n    'max_iter':[1000]\n}\n\n\ngsMLP = GridSearchCV(MLP, param_grid = mlp_param_grid, cv = kfold, scoring = 'roc_auc', n_jobs= 1, verbose = 1)\ngsMLP.fit(train_tv,X_train['sentiment'])\nprint(gsMLP.best_params_)\nmlp_best0 = gsMLP.best_estimator_\n\n# {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'} - 0.89996\n# {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'} - 0.89896\n# {'activation': 'relu', 'alpha': 0.2, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'} - 0.90284\n# {'activation': 'relu', 'alpha': 0.3, 'hidden_layer_sizes': (5,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'} - 0.90356","be3fa046":"submission3 = gsMLP.predict(test_tv)","1f124dd6":"print(gsMLP.best_score_)","1dbb16fc":"lr = LogisticRegression(random_state = 2018)\n\nlr2_param = {\n    'penalty':['l2'],\n    'dual':[True],\n    'C':[6],\n    'class_weight':[{1:1}]\n    }\n\nlr_CV = GridSearchCV(lr, param_grid = [lr2_param], cv = kfold, scoring = 'roc_auc', n_jobs = 1, verbose = 1)\nlr_CV.fit(train_tv, X_train['sentiment'])\nprint(lr_CV.best_params_)\nlogi_best = lr_CV.best_estimator_\n\n# {'C': 6, 'class_weight': {1: 1}, 'dual': True, 'penalty': 'l2'} - 90.360","f2ed93a0":"submission6 = lr_CV.predict(test_tv)","cf55c441":"print(lr_CV.best_score_)","3826016a":"# Extract the coefficients from the best model Logistic Regression and sort them by index.\ncoefficients = logi_best.coef_\nindex = coefficients.argsort()","c3812f1e":"# Extract the feature names.\nfeature_names = np.array(tv.get_feature_names())","eaa3b69f":"# From the smallest to largest.\nfeature_names[index][0][:30]","660089bb":"# From the smallest to largest.\nfeature_names[index][0][-31::1]","b6b6701b":"# feature names: Smallest 30 + largest 30.\nfeature_names_comb = list(feature_names[index][0][:30]) + list(feature_names[index][0][-31::1])","7edaa6c9":"# coefficients magnitude: Smallest 30 + largest 30.\nindex_comb = list(coefficients[0][index[0][:30]]) + list(coefficients[0][index[0][-31::1]])","1808240d":"# Make sure the x-axis be the number from 0 to the length of the features selected not the feature names.\n# Once the bar is plotted, the features are placed as ticks.\nplt.figure(figsize=(25,10))\nbarlist = plt.bar(list(i for i in range(61)), index_comb)\nplt.xticks(list(i for i in range(61)),feature_names_comb,rotation=75,size=15)\nplt.ylabel('Coefficient magnitude',size=20)\nplt.xlabel('Features',size=20)\n\n# color the first smallest 30 bars red\nfor i in range(30):\n    barlist[i].set_color('red')\n\nplt.show()","160b5e6c":"output = pd.DataFrame( data = {'id': X_test['id'], 'sentiment': submission6 })\noutput.to_csv('submission26.csv', index = False, quoting = 3)","0cf6747c":"### 5.1 WordCloud <a id='wc'><\/a>\n\nAs alluded in the first notebook, the drawback for WordCloud is that the graphics only reflect the frequency of words, which can cause some uninformative words frequently appeared in the text can be highlighted on the cloud instead of informative words which is less frequently appeared in the text. These kind of uninformative words could be stopwords or just some words frequently appeared in documents that particularly longer than other documents. Although the WordCloud is not the best visualization method to show all the aspect of the data, it is worth plotting them so that we can quickly and intuitively see what the text is about.","0dd8148d":"As expected, most of emphasized words are just normal words like \"film\", \"one\", \"movie\", \"show\", and \"stori\" which appear to be not informative to distinguish one document from the others or distinguish between negative and positive movie reviews.","487cc7cc":"## 7. Investigating Model Coefficients <a id='imc'><\/a>\n\nSince there are 40000 features, it is impossible to look at all of the coefficients at the same time. Therefore, we can sort them and look at the largest coefficients. The following bar chart shows the 30 largest and 30 smallest coefficients of the Logistic Regression model, with the bars showing the size of each coefficients.","850a65a3":"The number of the feature of the matrix are almost ten times larger than the number of reviews. This can cause the curse of dimensionality but this project is for studying and trying many features of text mining tools so I decide to leave the option. Instead, the regularization term must be tuned with care when optimizing the parameters. ","0d5a147c":"## 6. Modeling <a id='modeling'><\/a>\n\nAs text data usually is very sparse and has a high dimensionality, using linear, and simple models such as Linear Support Vector Machine, Bernoulli Naive Bayes, Logistic Regression or MultiLayer Perceptron would be better choice rather than using Random Forest. ","9a1a15a8":"## 2. Reading the Data <a id='reading'><\/a>","8b0f482c":"As you can see the above, due to the vocabulary option 'set(words.words())', a lot of vocabularies are added to the matrix even more than review's vocabularies.","c93b0ff7":"## 5. Visualization <a id='viz'><\/a>","ad821904":"As mentioned in previous notebook about CountVectorizer, the blue bar indicates positive movie reviews. On the other hand the red bar indicates negative move reviews. Interestingly, there are many words in common on both barplot based on CountVectorizer and TF-IDF such as: worst, bad, aw, wast, disappoint, excel, perfect, great, high recommend, etc.","3a506d19":"### 6.1 Support Vector Machine <a id='svm'><\/a>","fd139368":"### 6.2 Bernoulli Naive Bayes Classifier <a id='bnb'><\/a>","42b5331e":"## Table of Contents","e8bbf8e7":"# 1. Bag of Words Meets Bags of Popcorn : TF-IDF","41e3b3c1":"This is the second notebook for IMDb sentiment analysis (First notebook: [Bag of Words Meets Bags of Popcorn: CountVectorizer](https:\/\/www.kaggle.com\/kyen89\/0-sentiment-analysis-countvectorizer\/)). In this notebook, instead of CountVectorizer, we will be analyzing the movie reviews by using TF-IDF (Term Frequency - Inverse Document Frequency). Also, we could compare how differently these methods work and the performance of the predictions. Above all, the most important takeaway from this notebook is to learn how to use TF-IDF and the usage of important TF-IDF parameters.","7740590d":"Among many models, the best Kaggle score is 90.36% performed by Logistic Regression. Compared to the best model (linear SVM) based on CountVectorizer, it was improved by approximately 2 percents. Not only Logistic Regression but also all the models show the improvement except Linear SVM. This improvement is due to the fact that TF-IDF is more complicated method than simple word counting method (CountVectorizer) and also the max features we set for TF-IDF are way more than that of CountVectorizer (40000 vs 18000).","9ebe5829":"### 6.3 Perceptron <a id='perceptron'><\/a>","3ec1cabb":"1. [Introduction](#intro)<br>\n2. [Reading the Data](#reading)<br>\n3. [Text Preprocessing](#preprocessing)<br>\n4. [TF-IDF](#tf-idf)<br>\n5. [Visualization](#viz)<br>- [5.1. WordCloud](#wc)<br> - [5.2. Distribution](#dis)<br>\n6. [Modeling](#modeling)<br>- [6.1. Support Vector Machine](#svm)<br>- [6.2. Bernoulli Naive Bayes Classifier](#bnb)<br>- [6.3. Perceptron](#perceptron)<br>- [6.4. Logistic Regression](#logi)<br>\n7. [Investigating Model Coefficients](#imc)<br>\n8. [Submission](#submission)<br><br>\n\nFirst notebook: [Bag of Words Meets Bags of Popcorn: CountVectorizer](https:\/\/www.kaggle.com\/kyen89\/0-sentiment-analysis-countvectorizer\/)<br>\nThird notebook: [Bag of Words Meets Bags of Popcorn: Word2Vec](https:\/\/www.kaggle.com\/kyen89\/2-sentiment-analysis-word2vec\/)","86fb4350":"### 5.2 Distribution <a id='dis'><\/a>","0540d850":"## 8. Submission <a id='submission'><\/a>","bbf5fc3e":"The black contour of the distribution graphs represent the normal distribution if the data would have been distributed as normal. Compared to the black contour, the actual distribution is pretty skwed; therefore, median would be better to use as a measure of representative of data since mean is very sensitive to outliers and noise especially the distribution is highly skewed. As shown in the legend, the mean of the word frequency is 236.89 and the mean of the unique word is 135.61. It means 236.89 words and 135.61 unique words are used for each review. Also the dashed lines represent the median of the distribution. Another thing to notice is that the median values are very closely located to the normal distribution's mean points. Compared to CountVectorizer methods, there are 117.39 words used more for train set and 41.57 words used more for test set. This is due to the different parameter setting and we used more words for max features for TF-IDF. The distribution of the graphs are somehow similar to that of CountVectorizer.","a0371e1a":"## 4. TF-IDF <a id='tf-idf'><\/a>\n\nTF-IDF (Term Frequency - Inverse Document Frequency) can be represented tf(d,t) X idf(t). TF-IDF uses the method diminishing the weight (importance) of words appeared in many documents in common, considered them incapable of discerning the documents, rather than simply counting the frequency of words as CountVectorizer does. The outcome matrix consists of each document (row) and each word (column) and the importance (weight) computed by tf * idf (values of the matrix).\n","9e59982f":"## 3. Text Preprocessing <a id='preprocessing'><\/a>","a0b875dc":"## 1. Introduction <a id='intro'><\/a>\n\n","eba278b4":"### 6.4 Logistic Regression <a id='logi'><\/a>"}}