{"cell_type":{"05c62c3f":"code","652cdc39":"code","cc0d7592":"code","40f1ef80":"code","ebd1b400":"code","8f67328f":"code","4a9000d7":"code","53217e20":"code","592ebdf3":"code","5d8eaae6":"code","c6662f5e":"code","b53e78f3":"code","b9d09949":"code","c00c8a83":"code","651df07f":"code","fe23e393":"code","b7ff1128":"code","26adbc37":"code","098ba11b":"code","39abbfe0":"code","129bc024":"code","018556d8":"markdown","b8ff617e":"markdown","522f9e69":"markdown","f0d5e09d":"markdown","dcbd3d2d":"markdown","6f4fcbdb":"markdown","c26cc7e6":"markdown","95ec2b07":"markdown","dc7e6afa":"markdown","921b24a8":"markdown","498b7815":"markdown","a2f70bb7":"markdown","436b4972":"markdown","82c619db":"markdown","f733a884":"markdown","7f42d2a3":"markdown","8c4bbfd6":"markdown","67450237":"markdown","69afa903":"markdown","2d00df8f":"markdown","f22ddc11":"markdown","38a675d8":"markdown"},"source":{"05c62c3f":"# Load libraries or packages for Machine Learning purposes\n#load packages\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n#misc libraries\nimport random\nimport time\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.impute import SimpleImputer\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","652cdc39":"# Step 3. Data preprocessing\n# load data\ntrain_data_full=pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data_full=pd.read_csv('..\/input\/titanic\/test.csv')\n\n# omit missing values on target data (Survived)\ntrain_data_full.dropna(subset=['Survived'], axis=0, inplace=True)\n\n# preview data\n# print(train_data_full.head())\n# print(test_data_full.head())\n\n# view missing values and summary of data\ntrain_miss_cols=train_data_full.columns[train_data_full.isnull().sum()>0]\ntest_miss_cols=test_data_full.columns[test_data_full.isnull().sum()>0]\nprint('-'*10)\nprint('Missing values on train data:')\nprint(train_data_full[train_miss_cols].isnull().sum())\nprint('-'*10)\nprint('Train data summary:')\nprint(train_data_full.describe())\nprint('-'*10)\nprint('Missing values on test data:')\nprint(test_data_full[test_miss_cols].isnull().sum())\nprint('-'*10)\nprint('Test data summary:')\nprint(test_data_full.describe())\n\n# ignore\/drop Cabin column because there are so many missing values\n# drop PassengerId in train data because it's just an ID for testing\n# drop Ticket because it's determined by Pclass\ntrain_data_full.drop(['Cabin','PassengerId','Ticket'], axis=1, inplace=True)\ntest_data_full.drop(['Cabin','Ticket'], axis=1, inplace=True)\ntrain_miss_cols=train_miss_cols.drop(['Cabin'])\ntest_miss_cols=test_miss_cols.drop(['Cabin'])\n\n# data cleaner for data preprocessing\ndata_cleaner=[train_data_full, test_data_full]\n\n# show distribution on data that have missing values to decide the type of categorical encoder that fits to data that have missing values\n# filter categorical columns that have missing values \ntrain_miss_cat=train_data_full[[i for i in train_miss_cols if train_data_full[i].dtypes=='object']]\ntest_miss_cat=test_data_full[[i for i in test_miss_cols if test_data_full[i].dtypes=='object']]\nmiss_cat_data=[train_miss_cat, test_miss_cat]\n\n# encode with label encoder for categorical columns in missing columns \ndef label_encoder(df):\n    label_encoder = LabelEncoder()\n    for col_name in df.columns:\n        series = df[col_name]\n        df[col_name] = pd.Series(\n            label_encoder.fit_transform(series[series.notnull()]),\n            index=series[series.notnull()].index\n        )\n    return df\nencoded_miss_cat_data=[label_encoder(i) for i in miss_cat_data]\n# join numeric type with encoded categorical columns in train\/test data that have missing values\ntrain_miss=train_data_full[list(set(train_miss_cols)-set(train_miss_cat))].join(encoded_miss_cat_data[0])\ntest_miss=test_data_full[list(set(test_miss_cols)-set(test_miss_cat))].join(encoded_miss_cat_data[1])\n\n# view data distribution with pairplot seaborn on variables that contain missing values\ndata_miss=train_data_full[train_miss_cols].join(test_data_full[test_miss_cols].add_suffix('_Test'))\ndata_miss_plot=train_miss.join(test_miss.add_suffix('_Test'))\nskew_val=data_miss_plot.skew(axis=0, skipna=True)\nprint('Missing values data skewness: \\n',skew_val)\ng0=sns.pairplot(data_miss_plot, diag_kind='kde')\ng0=g0.fig.suptitle('Train and Test data distribution', y=1)\n\n# show boxplot to see skewness and outliers\nplt.figure(figsize=(10,5))\nplt.subplots_adjust(wspace=0.9)\nmax_col=len(data_miss_plot.columns)\nn=max_col-1\nfor idx,col in enumerate(data_miss_plot.columns):\n    plt.subplot(1,max_col,max_col-n)\n    sns.boxplot(data=data_miss_plot[col], showmeans = True, meanline = True)\n    plt.title(col)\n    n-=1","cc0d7592":"# impute missing values\n# from data distribution and box plot above can be known that:\n# Age and Age_Test=positive skewness -> strategy=median\n# Embarked=categorical variable -> strategy=mode\n# Fare=positive skewness -> strategy=median\nimp_med=SimpleImputer(strategy='median')\nimp_mod=SimpleImputer(strategy='most_frequent')\nfor col in skew_val.index.drop('Embarked'):\n    data_miss[col]=pd.DataFrame(imp_med.fit_transform(data_miss[[col]]))\ndata_miss['Embarked']=pd.DataFrame(imp_mod.fit_transform(data_miss[['Embarked']]))\n# check for missing values\nprint(data_miss.isnull().sum())\n\n# join imputed missing values to train\/test data\ntrain_miss_imputed=data_miss[train_miss.columns]\ntest_miss_imputed_cols=list(set(data_miss.columns)-set(train_miss.columns))\ntest_miss_imputed_cols.sort(reverse=True)\ntest_miss_imputed=data_miss[test_miss_imputed_cols].rename(columns=dict(zip(test_miss_imputed_cols, test_miss.columns)))\ntrain_data_full.drop(train_miss_imputed.columns, axis=1, inplace=True)\ntest_data_full.drop(test_miss_imputed.columns, axis=1, inplace=True)\ntrain_data_full=train_data_full.join(train_miss_imputed)\ntest_data_full=test_data_full.join(test_miss_imputed)\n# preview encoded data (train, test) without missing values\nprint('Check missing values on baseline data')\nprint('-'*10)\nprint(train_data_full.isnull().sum())\nprint('-'*10)\nprint(test_data_full.isnull().sum())\ntrain_data_full.head()","40f1ef80":"data_df=[train_data_full, test_data_full]\n# cleaning name and extracting Title\nfor df in data_df:\n    df['Title']=df['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n\n# replacing rare Title with more common ones and also drop column Name\nmapping={'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n         'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\nfor df in data_df:\n    df.replace({'Title':mapping}, inplace=True)\n# preview train dataset\n# data_df[0].sample(5)","ebd1b400":"# add Family_Size in each dataset\nfor df in data_df:\n    df['Family_Size']=df['Parch']+df['SibSp']\n# preview train dataset\n# data_df[0].sample(5)","8f67328f":"# making bins\nlabel=LabelEncoder()\nfor df in data_df:\n    # create bin\n    df['FareBin']=pd.qcut(df['Fare'], 5)\n    # use label encoder for categorical variables\n    df['FareBin_Code']=label.fit_transform(df['FareBin'])\n    # drop Fare column\n# preview train dataset\n# data_df[0].sample(5)","4a9000d7":"label=LabelEncoder()\nfor df in data_df:\n    df['AgeBin']=pd.qcut(df['Age'], 4)\n    df['AgeBin_Code']=label.fit_transform(df['AgeBin'])\n# preview train dataset\n# data_df[0].sample(5)","53217e20":"# check categorical variables\ncat_cols=[]\nfor df in data_df:\n    cat_cols.append([col for col in df.columns if df[col].dtypes=='object'])\n# print(cat_cols)\n# print('-'*10)\n    \n# use label encoder to encode categorical variables\nlabel=LabelEncoder()\ntrain_encoded=data_df[0][cat_cols[0]].apply(label.fit_transform)\ntest_encoded=data_df[1][cat_cols[1]].apply(label.fit_transform)\n\n# use one-hot encoder\noh_cols=['Title', 'Sex', 'Embarked']\ntrain_encoded_oh=pd.get_dummies(data_df[0][['Title','Sex','Embarked']]).add_suffix('_OH')\ntest_encoded_oh=pd.get_dummies(data_df[1][['Title','Sex','Embarked']]).add_suffix('_OH')\nOH_encoded_cols=train_encoded_oh.columns\n\n# join encoded columns to dataset\ntrain_data_full=data_df[0].join(train_encoded.add_suffix('_Code'))\ntest_data_full=data_df[1].join(test_encoded.add_suffix('_Code'))\n# join one-hot encoded columns to dataset\ntrain_data_full=train_data_full.join(train_encoded_oh)\ntest_data_full=test_data_full.join(test_encoded_oh)\n\n# select features for baseline and engineered dataset\nbaseline_feature_cols=['Pclass', 'Sex_Code', 'Embarked_Code', 'Title_Code', 'Family_Size']\nengineered_feature_cols=['Pclass', 'Family_Size', 'FareBin_Code', 'AgeBin_Code']+list(OH_encoded_cols)\ntrain_data_full[engineered_feature_cols].head()","592ebdf3":"# create function for data splits and train model\ndef get_data_splits(df, valid_fraction=0.1):\n    valid_size=int(len(df)*valid_fraction)\n    train=df[:-valid_size*2]\n    valid=df[-valid_size*2:-valid_size]\n    test=df[-valid_size:]\n    return train,valid,test\n\n# split training data for train, valid and test\ntrain_,valid_,test_=get_data_splits(train_data_full)","5d8eaae6":"# discrete variable correlation on Train data by survival using groupby\nfor col in train_data_full[['Pclass','Sex','Embarked', 'FareBin', 'AgeBin', 'Title']]:\n    print('Survival correlation by: ', col)\n    print(train_data_full[[col, 'Survived']].groupby(col, as_index=False).mean())\n    print('-'*10)","c6662f5e":"# graph distribution for quantitative data (Age, Fare, Family size) on training data\nplt.figure(figsize=(14,12))\n# plt.subplots_adjust(wspace=0.9, hspace=0.3)\n\n# Fare boxplot\nplt.subplot(231)\nsns.boxplot(data=train_data_full['Fare'], showmeans=True, meanline=True)\nplt.title('Fare Distribution')\nplt.ylabel('Fare ($)')\n# Age subplot\nplt.subplot(232)\nsns.boxplot(data=train_data_full['Age'], showmeans=True, meanline=True)\nplt.title('Age Distribution')\nplt.ylabel('Age (Years)')\n# Family size boxplot\nplt.subplot(233)\nsns.boxplot(data=train_data_full['Family_Size'], showmeans=True, meanline=True)\nplt.title('Family size Distribution')\nplt.ylabel('Family size (#))')\n# Fare vs survived histogram plot\nplt.subplot(234)\nplt.hist(x=[train_data_full[train_data_full['Survived']==1]['Fare'], \n           train_data_full[train_data_full['Survived']==0]['Fare']], color=['g','r'], label=['Survived', 'Dead'], bins=20)\nplt.title('Fare by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of passengers')\nplt.legend()\n# Age vs survival\nplt.subplot(235)\nplt.hist(x=[train_data_full[train_data_full['Survived']==1]['Age'], \n           train_data_full[train_data_full['Survived']==0]['Age']], color=['g','r'], label=['Survived', 'Dead'], bins=30)\nplt.title('Age by Survival')\nplt.xlabel('Age (years)')\nplt.ylabel('# of passengers')\nplt.legend()\n# Family size vs survival\nplt.subplot(236)\nplt.hist(x=[train_data_full[train_data_full['Survived']==1]['Family_Size'], \n           train_data_full[train_data_full['Survived']==0]['Family_Size']], color=['g','r'], label=['Survived', 'Dead'], bins=20)\nplt.title('Family size by Survival')\nplt.xlabel('Family size (#)')\nplt.ylabel('# of passengers')\n_=plt.legend()","b53e78f3":"# graph for categorical data (Title, Sex, Pclass, Embarked, FareBin, AgeBin)\n# graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(14,12))\nsns.barplot(x='Title', y='Survived', data=train_data_full, ax=saxis[0,0])\nsaxis[0,0].set_title('Title vs Survived')\nsns.barplot(x='Sex', y='Survived', data=train_data_full, ax=saxis[0,1])\nsaxis[0,1].set_title('Sex vs Survived')\nsns.barplot(x='Pclass', y='Survived', data=train_data_full, ax=saxis[0,2])\nsaxis[0,2].set_title('Pclass vs Survived')\nsns.barplot(x='Embarked', y='Survived', data=train_data_full, ax=saxis[1,0])\nsaxis[1,0].set_title('Title vs Embarked')\nfarebinplot=sns.barplot(x='FareBin', y='Survived', data=train_data_full, ax=saxis[1,1])\nfarebinplot.set_xticklabels(farebinplot.get_xticklabels(), rotation=45, horizontalalignment='right')\nsaxis[1,1].set_title('FareBin vs Survived')\nsns.barplot(x='AgeBin', y='Survived', data=train_data_full, ax=saxis[1,2])\n_=saxis[1,2].set_title('AgeBin vs Survived')","b9d09949":"# graph distribution of qualitative data Pclass compared to other features\n# Pclass is mattered for survival\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,7))\n\nsns.boxplot(x='Pclass', y='Fare', hue='Survived', data=train_data_full, ax=axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\nsns.boxplot(x='Pclass', y='Age', hue='Survived', data=train_data_full, ax=axis2)\naxis2.set_title('Pclass vs Age Survival')\nsns.boxplot(x='Pclass', y='Family_Size', hue='Survived', data=train_data_full, ax=axis3)\n_=axis3.set_title('Pclass vs Family Size Survival Comparison')","c00c8a83":"# graph distribution of qualitative data Sex compared to other features\n# Sex is mattered for survival\nfig, saxis = plt.subplots(1,3,figsize=(14,7))\n\nsns.barplot(x='Sex', y='Survived', hue='Embarked', data=train_data_full, ax=saxis[0])\nsaxis[0].set_title('Sex vs Embarked Survival Comparison')\nsns.barplot(x='Sex', y='Survived', hue='Pclass', data=train_data_full, ax=saxis[1])\nsaxis[1].set_title('Sex vs Pclass Survival')\nsns.barplot(x='Sex', y='Survived', hue='Pclass', data=train_data_full, ax=saxis[2])\n_=saxis[2].set_title('Sex vs Pclass Survival')","651df07f":"# pairplot of entire dataset\ndisplay_cols=['Survived','Pclass','SibSp','Parch','Fare', 'Age','Family_Size','FareBin_Code', 'AgeBin_Code', 'Title_Code']\npp=sns.pairplot(train_data_full[display_cols], hue='Survived', palette='deep', size=1.2, diag_kind='kde',diag_kws=dict(shade=True), plot_kws=dict(s=10))\nfor axis in pp.fig.axes:   # get all the axis\n    axis.set_xlabel(axis.get_xlabel(), rotation=45)\n_=pp.set(xticklabels=[])","fe23e393":"# heatmap correlation of train dataset\ndef heatmap_correlation(df):\n    _, ax=plt.subplots(figsize=(14,12))\n    colormap=sns.diverging_palette(220, 10, as_cmap=True)\n    _=sns.heatmap(\n        df.corr(),\n        cmap=colormap,\n        square=True,\n        cbar_kws={'shrink':.9},\n        ax=ax,\n        annot=True,\n        linewidths=0.1, vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12}\n    )\n    plt.title('Pearson Correlation of features', y=1.05, size=15)\nheatmap_correlation(train_data_full[display_cols])","b7ff1128":"# Machine Learning Algorithm (MLA) selection and initialization\nMLA=[\n    # Ensemble methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    \n    # XGBoost\n    XGBClassifier(),\n    \n    # LightGBM\n    lgb.LGBMClassifier(),\n    \n    # Gaussian process\n    gaussian_process.GaussianProcessClassifier(),\n    \n    # GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    # Naive bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    # Nearest neighbors\n    neighbors.KNeighborsClassifier(),\n    \n    # SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    # Trees\n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    # Discrimant analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n]\n\n# split dataset in cross-validation with the splitter class\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .2, train_size = .8, random_state = 0 )\n\n# table to compare MLA metrics\nMLA_columns=['MLA Name', 'MLA Parameters', 'MLA Train Accuracy Mean', \n            'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD', 'MLA Time']\nMLA_compare=pd.DataFrame(columns=MLA_columns)\n\nmodels={}\n\n# feature cols\n# cols=[col for col in baseline_feature_cols if col!='Survived']\ncols=[col for col in engineered_feature_cols if col!='Survived']\n\n# iterate through MLA and save performance to table\nfor idx, alg in enumerate(MLA):\n    # set name and params\n    MLA_name=alg.__class__.__name__\n    MLA_compare.loc[idx, 'MLA Name']=MLA_name\n    MLA_compare.loc[idx, 'MLA Parameters']=str(alg.get_params())\n    \n    # score model with cross validation\n    cv_results=model_selection.cross_validate(alg, train_data_full[cols], train_data_full['Survived'], cv=cv_split, return_train_score=True)\n    MLA_compare.loc[idx, 'MLA Time']=cv_results['fit_time'].mean()\n    MLA_compare.loc[idx, 'MLA Train Accuracy Mean']=cv_results['train_score'].mean()\n    MLA_compare.loc[idx, 'MLA Test Accuracy Mean']=cv_results['test_score'].mean()\n    MLA_compare.loc[idx, 'MLA Test Accuracy 3*STD']=cv_results['test_score'].std()*3\n    \n    # save MLA predictions score and model\n    models[MLA_name]=[cv_results['test_score'].mean(), alg]\n\n# show and sort table\nMLA_compare.sort_values(by=['MLA Test Accuracy Mean'], ascending=False, inplace=True)\nMLA_compare\n    ","26adbc37":"# barplot for MLA comparison\nax=sns.barplot(x='MLA Test Accuracy Mean', y='MLA Name', data=MLA_compare, color='#52518f')\ntest_acc=MLA_compare['MLA Test Accuracy Mean'].apply(lambda x: f'{x*100:.2f}')\ntarget_name=MLA_compare['MLA Name']+' ('+ test_acc +'%)'\nax.set_yticklabels(target_name)\n\n# pretify using pyplot\nplt.title('Machine Learning Algorithm Test Accuracy Score\\n')\nplt.xlabel('Accuracy Score (%)')\n_=plt.ylabel('Algorithm')\n","098ba11b":"# hyper-parameters tune with GridSearchCV\ngrid_ratio=[.1, .25, .5, .75, 1.0]\nbest_models={}\n\nestimators=[\n    ('svc', svm.SVC()),\n    ('nusvc', svm.NuSVC()),\n    ('rc', linear_model.RidgeClassifierCV()),\n    ('lsvc', svm.LinearSVC()),\n    ('lda', discriminant_analysis.LinearDiscriminantAnalysis())\n]\n\ngrid_param=[\n    [{\n        # SVC\n        'C': [1,2,3,4,5],\n        'gamma': grid_ratio,\n        'decision_function_shape': ['ovo', 'ovr'],\n        'probability': [True],\n        'random_state': [0]\n    }],\n    [{\n        # NuSVC\n        'nu': [0.5, 0.7],\n        'gamma': grid_ratio,\n        'decision_function_shape': ['ovo', 'ovr'],\n        'probability': [True, False],\n        'random_state': [0]\n        \n    }],\n    [{\n        # RidgeClassifierCV\n        'alphas':[(0.1, 0.5, 7.0), (0.1, 0.7, 10.0), (0.1, 1.0, 10.0)],\n        'normalize':[True, False],\n        'scoring':[None],\n        'class_weight': ['balanced', None]\n    }],\n    [{\n        # LinearSVC\n        'penalty':['l2'],\n        'loss':['hinge', 'squared_hinge'],\n        'C': [1,2,3,4,5]\n    }],\n    [{\n        # LinearDiscriminantAnalysis\n        'solver':['svd', 'lsqr'],\n        'shrinkage':[None]\n    }]\n]\n\nstart_total=time.perf_counter()\nfor clf, param in zip(estimators, grid_param):\n    start=time.perf_counter()\n    best_search=model_selection.GridSearchCV(estimator=clf[1], param_grid=param, cv=cv_split, scoring='roc_auc', n_jobs=-1)\n    best_search.fit(train_data_full[cols], train_data_full['Survived'])\n    run=time.perf_counter()-start\n    \n    best_param=best_search.best_params_\n    best_score=best_search.best_score_\n    best_models[clf[1].__class__.__name__]=[best_score, best_search, run]\n    print('Name: ', clf[1].__class__.__name__)\n    print('Best score: ', best_score)\n    print('best param: ', best_param)\n    print('runtime: ', run)\n    print('-'*10)\n    clf[1].set_params(**best_param)\n\nrun_total=time.perf_counter()-start_total\nprint('Total optimization time: {:.2f} minutes'.format(run_total\/60))\nprint('Finish')","39abbfe0":"# features selection\nfor idx, features in enumerate([baseline_feature_cols, engineered_feature_cols]):\n    print('='*35)\n    print('Features type: ', ['Baseline Features', 'Engineered Features'][idx])\n    print('='*35)\n    for model in best_models.items():\n        model[1][1].best_estimator_.fit(train_[features], train_['Survived'])\n        y_pred=model[1][1].best_estimator_.predict(valid_[features])\n        score=metrics.roc_auc_score(valid_['Survived'], y_pred)\n        print('Model: ', model[0])\n        print(f'Validation score: {score:.4f}')\n        print('-'*25)","129bc024":"# generate CSV file for submitting survival predictions\nmodel=best_models['SVC'][1]\nmodel.best_estimator_.fit(train_data_full[engineered_feature_cols], train_data_full['Survived'])\ny_pred=model.best_estimator_.predict(test_data_full[engineered_feature_cols])\n\nsubmit=pd.DataFrame({'PassengerId':test_data_full.PassengerId,\n                    'Survived':y_pred})\nsubmit.to_csv('submission.csv', index=False)","018556d8":"## Step 3. Data preprocessing\nIn data preprocessing, the data is removed from missing values, duplicates, aberrant data. Data type conversion in categorical variables and also normalization if needed.","b8ff617e":"## 12. Optimize and strategy\nWhen the model predict full test data and submitted to Kaggle, it yields [0.7846] public score. It's far from the validation score and denote there are some reasons, such as inaccurate model or features that are not quite right. And for this problem seems most likely on features selection. For the next strategy, to boost the prediction accuracy we will try to re-engineered features more thoroughly such as generating new features from Name, Title, Fare, and Ticket and then trying fature normalization. And then selecting the best model for generated new features.\n\n<img src=\"https:\/\/www.dropbox.com\/s\/gm1687wp1x413ua\/kaggle%20submit.png?raw=1\" width=\"650\">","522f9e69":"* Adding Family_Size\nFamily_Size=Parch+SibSp","f0d5e09d":"#### Handle Categorical variables and create baseline dataset\nCategorical variables contain a finite number of categories or distinct groups. Categorical data might not have logical order. Example: male or female, young or old, etc. Some Machine Learning algorithm cannot handle categorical data, so we must handle it before feeding to algorithm. There are some appoaches to handle categorical data such as **Drop** categorical variables, **Label encoding**, and **One-hot encoding**.","dcbd3d2d":"## Step 5. Exploratory Data Analysis with Statistics\nNow our data is cleaned and ready to feed to model for training and testing. But before that, we will explore our data with descriptive and graphical statistics to describe and summarize the variables. We will classifying features and determining correlation between features and target variable.","6f4fcbdb":"## 10. Tune model with feature selection\nTop 5 model will try to predict outcome with different features selection. There are two collection of features, baseline_feature_cols and engineered_feature_cols.","c26cc7e6":"#### Split dataset for training, validation and testing data\nTraining data will be split into training, validation and testing data with a 80\/10\/10 ratio. In this case we will use **train_test_split** from sklearn.","95ec2b07":"## Step 2. Data collection\nin this case, the data is taken from Kaggle Titanic: Machine Learning from disaster. So we no need to find out or collect the raw data from another source using some methods like web scraping, survey or online quiz, and interviews. Data source: https:\/\/www.kaggle.com\/c\/titanic\/data","dc7e6afa":"## Step 7. Training the initial model with cross-validation (CV)\nTraining is process of learning of a model. The algorithm will find the best model during learning by minimizing the error (difference between prediction and target).","921b24a8":"## Conclusion\nFor this practice, some conclusions can be drawn such as: \n\n* Feature engineering (like generating new features, categorical encoding, and feature selection) is very important to gain the prediction accuracy.\n* Every model has specific hyper-parameters that need to be tuned to make better prediction.\n* Every case in dataset such as number of features, type of fatures (integer, float or binary(0\/1)) has an effect of selecting the best model (must choose ensemble method, k-NN or SVC to fit to dataset better)\n\nThanks.","498b7815":"* Making Fare Bins\nBinning (quantization\/dicretization) is used to transform continuous numeric features to discrete ones (categories). Each bin represents a specific degree of intensity and hence specific range of continuous numeric values that fall into it. Qcut (quantile cut) is quantile-based discretization. It's ordinal, FareBin=3 is indeed greater than FareBin=1. [ https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.qcut.html ].","a2f70bb7":"## Step 1. Define problem\nRMS (Royal Mail Ship) Titanic, British luxury ship that sank on 14-15, 1912 during its mayden voyage en route to New York city from Southampton, England after colliding with iceberg. Killing about 1500 people including passengers and crew. One of the reason that shipwreck led to such loss of life was because there were not enough of lifeboats for passengers and crew altough there are some elements of luck for some groups of people that likely have more chance to survive such as women, children, and upper-class. The challenge is to make model that can predict the people are likely to survive from the infamous tragedy. Source: https:\/\/www.britannica.com\/topic\/Titanic","436b4972":"#### From data exploratory analysis we can see some fatcs such as:\n* Correlation between Pclass and Survival\n\nPclass (Ticket class): A proxy of socio-economy status (SES), types of Pclass:\n1st (Upper), 2nd (Middle), 3rd (Lower). From the correlation we know that Upper ticket class has more chance to survive than other ticket class with a percentage of about 60%.\n\n* Correlation between Sex and Survival\n\nFemale has more chance to survive than male with a percentage of about 74%. It can also be seen in Titanic Movie that women and children take precendence to aboard the lifeboats.\n\n* Correlation between Embarked and Survival\n\nEmbarked: Port of embarkation, C = Cherbourg, Q = Queenstown, S = Southampton. From correlation analysis, people who embarked from Cherbourg have more chance to survive than others with percentage of about 55%.\n\n* Correlation between Fare and Survival\n\nFare: Passenger fare. From the correlation analysis, it seems the higher the fare of passenger the higher the chance to survive than lower fare. It can be seen on correlation between FareBin and Survival. Fare with values between 39 to 512 have more chance to survive with a percentage of about 64%.\n\n* Correlation between Age and Survival\n\nPeople with age between 28 and 35 years old have more chance to survive than other with a percentage of about 43%.\n\n* Correlation between Title and Survival\n\nFrom the analysis we can see that people who have title Miss and Mrs have more chance than others with a percentage of 70% and 79% respectively. And there is no survivors for Reverend (Rev) title.\n\n* The most correlate with Survived feature\n\nThe most correlate with Survived feature from heatmap plot is Sex_Code with correlation strength is -0.54. This relate with the following probability:\n\nSex -> Probability to survive\n* 0 (female) -> 0.742038\n* 1 (male) -> 0.188908","82c619db":"## Step 8. Evaluate the model performace\nThe top 5 models for this features are **SVC** (Support Vector Classifier) (83.69%), **NuSVC** (83.63%), **RidgeClassifierCV** (83.46%), **LinearSVC** (83.41%), and **LinearDiscrimantAnalysis** (83.35%). The best test accuracy score for those models is SVC (Support Vector Classifier), so we will use it for predict survival on test data. And the difference between train and test accuracy mean is almost the same, so the models are well generalizing for unseen data (not **underfitting** nor **overfitting**).\n\n<img src=\"https:\/\/www.dropbox.com\/s\/jsfv6xemog1qeys\/Data%20Science%20Role.png?raw=1\" width=\"400\">","f733a884":"## Load all library or package for Machine Learning\nThere are many library\/package that used in machine Learning purpose such as scikit-learn, scipy, pandas, numpy, matplotlib, seaborn, etc.","7f42d2a3":"## Step 4. Feature Engineering\n* Create Title feature\nCreate Title feature from Name such as Mr, Miss, Mrs, etc. It will help uas to categorize passengers to children, young, old, etc.","8c4bbfd6":"## 11. Validate and implement\nAfter selecting the best model and features for dataset and validating it with validation data. It's time to implement the best model to predict outcome (to predict survival in this case) for test data. From the model and features selection, the bset model for this case is SVC (Support Vector Classifier) which can predict the survivor with the ROC_AUC score of about 0.88 in engineered feature.","67450237":"## Titanic: Machine Learning from Disaster Challenge [Beginner-Intermediate] version 1\nUse Machine Learning to create the model that predicts which passenger survived from Titanic shipwreck. Dataset available at https:\/\/www.kaggle.com\/c\/titanic\/data\n\n### Steps to create the model for prediction\n1. Define problem\n2. Data collection\n3. Data preprocessing\n4. Feature engineering\n5. Exploratory Data Analysis with Statistics (EDA)\n6. Create model data\n7. Training the model\n8. Evaluate the model performance\n9. Tune model with hyperparameters\n10. Tune model with feature selections\n11. Validate and implement\n12. Optimize and strategy","69afa903":"* Making Age Bins\nCreate bins for Age using qcut() function","2d00df8f":"## Step 6. Create model data\n\n**Model**: A machine learning model can be a mathematical representation of a real-world process. The learning algorithm finds patterns in the training data such that the input parameters correspond to the target. The output of the training process is a machine learning model which you can then use to make predictions. Machine Learning can be categorized as **Supervised learning, Unsupervised learning and Reinforced learning**. Supervised learning is where you train the model by presenting it a training dataset that includes the correct answer. Unsupervised learning is where the model is trained by training dataset that not includes the corresct answer. Reinforced learning is a hybrid of the previous two, where the model is not given the correct answer immediately, but later after a sequence of events to reinforce learning. There are many Machine Learning (ML) algorihtms, however they can be reduced to four categories: regression, classification, clustering and dimensionality reduction.\n#### Machine Learning Algorithms\n* Regression (supervised)\n    1. Linear Regression\n    2. Decision Tree Regressor\n    3. k-Nearest Neighbors (k-NN) Regressor\n    4. Random Forest Regressor\n    5. Gradient Boosting Regressor\n    6. XGBoost Regressor\n    7. Light Gradient Boosted Machine (LGBM) Regressor\n    8. CatBoost Regressor\n    9. Naive Bayes (GaussianNB)\n    10. Neural Network\n\n\n* Classification (supervised)\n    1. Generalized Linear Model (GLM) (Logistic Regression, Passive Aggresive, Ridge Classifier, SGD Classifier, Perceptron)\n    2. Decision Tree Classifier\n    3. Extra Tree Classifier\n    4. Support Vector Machine (SVM)\n    5. Naive Bayes (BernoulliNB, MultinomialNB)\n    6. k-NN Classifier\n    7. Gaussian Process\n    8. Discriminant Analysis\n    10. Extra Trees\n    11. Random Forest Classifier\n    12. GBM Classifier\n    13. XGBoost Classifier\n    14. AdaBoost Classifier\n    15. Bagging Classifier\n    16. LGBM Classifier\n    17. CatBoost Classifier\n    18. Neural Network Classifier\n    \n    \n* Clustering (unsupervised)\n    1. k-Means\n    2. Apriori\n    \n    \n* Dimensionality Reduction (unsupervised)\n    1. Principal Component Analysis (PCA)\n    2. Principal Component Regression (PCR)\n    3. Partial Least Squares Regression (PLSR)\n\n\nSurvival prediction on Titanic shipwreck is classification problem, so some classification algorithms will be used such as Ensemble methods (AdaBoost, Bagging, Extra Trees, Gradient Boosting, LGBM, XGBoost, Random Forest), Gaussian proccess, GLM, Naive Bayes, k-NN, SVM, Trees, and Discriminant Analysis.","f22ddc11":"#### Define Imputation method based on data distribution on variables that have missing values\nA distribution is simply a collection of data, or scores, on a variable. Usually, these scores are arranged in order from smallest to largest and then they can be presented graphically [Page 6, Statistics in Plain English, Third Edition, 2010].\nA sample of data will form a distribution and by far the most well-known distribution is Gaussian distribution or normal distribution. Skewness is the degree of distortion from the symmetrical bell curve or normal distribution. It measures the lack of symmetry in data distribution. It differentiates extreme values in one versus the other tail. Normal distribution will have a skewness of 0.\n<img src=\"https:\/\/www.dropbox.com\/s\/gmyeqwieueedmz8\/distribution.png?raw=1\" width=\"500px\">\n* **Positive Skewness** means when the tail on right side of the distribution is longer or fatter. The mean and median will be greater than the mode.\n* **Negative Skewness** means when the tail on the left side of the distribution is longer or fatter. The mean and median will be less than the mode. \nWe will use data imputation method to handle missing values. There 3 strategies that can be uses in data imputation to fill the missing values such as mean, median, and mode. Because the median is mostly greater than mean in a skewness distribution, so we will use **strategy=median** for skewness and **strategy=mean** for normal distribution in continuous variable. And use **strategy=mode** for categorical variable. But I don't think this matters a lot, you can use mean too.","38a675d8":"## Step 9. Tune model with hyper-parameters\nTune hyper-parameters is imprtant to boost model performance. We will tune top 5 models models for predictions (SVC, NuSVC, RidgeClassifierCV, LinearSVC, LinearDiscriminantAnalysis). "}}