{"cell_type":{"e6e2a913":"code","27aabdf6":"code","2cb66c68":"code","98e2724d":"code","80cb8563":"code","2734bf1d":"code","2a371581":"code","da75c4d1":"code","71959f88":"code","370d4b55":"code","52ed2012":"code","8bf92f6c":"code","0316e81e":"code","e61008bd":"code","6428f1e9":"code","fe530788":"code","445ae725":"markdown","7f115910":"markdown","4b41753a":"markdown"},"source":{"e6e2a913":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","27aabdf6":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_excel('..\/input\/imdb-movie-review-nlp-project\/Data_Train.xlsx')","2cb66c68":"train.head()","98e2724d":"train.shape","80cb8563":"#Printing the dataset info\nprint(train.info)","2734bf1d":"#Printing the group by description of each category\ntrain.groupby(\"SECTION\").describe()","2a371581":"#Removing duplicates to avoid overfitting\n\ntrain.drop_duplicates(inplace=True)","da75c4d1":"import nltk\nfrom nltk.corpus import stopwords\nimport string","71959f88":"\n\n#A punctuations string for reference (added other valid characters from the dataset)\nall_punctuations = string.punctuation + '\u2018\u2019,:\u201d][],' \n\n#Method to remove punctuation marks from the data\ndef punc_remover(raw_text):\n    no_punct = \"\".join([i for i in raw_text if i not in all_punctuations])\n    return no_punct\n\n#Method to remove stopwords from the data\ndef stopword_remover(no_punc_text):\n    words = no_punc_text.split()\n    no_stp_words = \" \".join([i for i in words if i not in stopwords.words('english')])\n    return no_stp_words\n\n#Method to lemmatize the words in the data\nlemmer = nltk.stem.WordNetLemmatizer()\ndef lem(words):\n    return \" \".join([lemmer.lemmatize(word,'v') for word in words.split()])\n\n#Method to perform a complete cleaning\ndef text_cleaner(raw):\n    cleaned_text = stopword_remover(punc_remover(raw))\n    return lem(cleaned_text)","370d4b55":"#Applying the cleaner method to the entire data\ntrain['CLEAN_STORY'] = train['STORY'].apply(text_cleaner)\n\n#Checking the new dataset\nprint(train.values) ","52ed2012":"#Creating TF-IDF Vectors\n#Importing TfidfTransformer from sklearn\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n#Fitting the bag of words data to the TF-IDF transformer\ntfidf_transformer = TfidfTransformer().fit(bow)\n\n#Transforming the bag of words model to TF-IDF vectors\nstorytfidf = tfidf_transformer.transform(bow)","8bf92f6c":"#Importing sklearn\u2019s Countvectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#Creating a bag-of-words dictionary of words from the data\nbow_dictionary = CountVectorizer().fit(train['CLEAN_STORY'])\n\n#Total number of words in the bow_dictionary\nlen(bow_dictionary.vocabulary_)\n\n#Using the bow_dictionary to create count vectors for the cleaned data.\nbow = bow_dictionary.transform(train['CLEAN_STORY'])\n\n#Printing the shape of the bag of words model\nprint(bow.shape)","0316e81e":"#Creating a Multinomial Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\n\n#Fitting the training data to the classifier\nclassifier = MultinomialNB().fit(storytfidf, train['SECTION'])","e61008bd":"#Importing and cleaning the test data\ntest = pd.read_excel('..\/input\/imdb-movie-review-nlp-project\/Data_Test.xlsx')\ntest['CLEAN_STORY'] = test['STORY'].apply(text_cleaner)\n\n#Printing the cleaned data\nprint(test.values)","6428f1e9":"#Importing the Pipeline module from sklearn\nfrom sklearn.pipeline import Pipeline\n\n#Initializing the pipeline with necessary transformations and the required classifier\npipe = Pipeline([\n    ('bow', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('classifier', MultinomialNB())])","fe530788":"#Fitting the training data to the pipeline\npipe.fit(train['CLEAN_STORY'], train['SECTION'])\n\n#Predicting the SECTION \ny_pred = pipe.predict(test['CLEAN_STORY'])\n\n#Writing the predictions to an excel sheet\npd.DataFrame(y_pred, columns = ['SECTION']).to_excel(\"predictions.xlsx\")","445ae725":"### END","7f115910":"### Data Preprocessing: Count Vectors and TF-IDF Vectors\n##### Creating Count vectors","4b41753a":"### Data Cleaning\n"}}