{"cell_type":{"20df16b4":"code","95c5b277":"code","f963c8ab":"code","955ff033":"code","e2ce211a":"code","c1840feb":"code","4f16a89a":"code","25f3acec":"code","c7ea0ee2":"code","013ea87d":"code","c50e9395":"code","0abf6808":"code","847e2713":"code","3ce5589b":"code","a920eb0c":"code","4cdd1252":"code","dbc5d941":"code","fe4db4c0":"code","3b04f01f":"code","2e20ebd3":"code","cafdca92":"code","73d8c572":"code","ceaf36d6":"code","f071a17e":"code","65132e4f":"code","26384122":"code","abe39657":"code","80026eb0":"code","91f79fd4":"markdown","2854902d":"markdown","24fda1ea":"markdown","1188072c":"markdown","2076e203":"markdown","6ef2d105":"markdown","37ecdf04":"markdown","382fd20a":"markdown","c333e726":"markdown","de696b2c":"markdown","c3c9f466":"markdown"},"source":{"20df16b4":"# Import the necessary packages\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report\nfrom xgboost.sklearn import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","95c5b277":"# Import and read dataset\ninput_ = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\ndf = pd.read_csv(input_)\n\ndf.head(10)","f963c8ab":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","955ff033":"df.describe()","e2ce211a":"x = df.drop(columns='DEATH_EVENT')\ny = df['DEATH_EVENT']\n\nmodel = XGBClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","c1840feb":"for i in range(0,len(df.columns)):\n    print(\"{} = {}\".format(i,df.columns[i]))","4f16a89a":"# Delete outlier\ndf = df[df['ejection_fraction']<70]","25f3acec":"inp_data = df.drop(df[['DEATH_EVENT']], axis=1)\n#inp_data = df.iloc[:,[11,7,4,0,1,8]]\nout_data = df[['DEATH_EVENT']]\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=0, shuffle=True)\n\n## Applying Transformer\nsc= StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","c7ea0ee2":"## X_train, X_test, y_train, y_test Shape\n\nprint(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","013ea87d":"# I coded this method for convenience and to avoid writing the same code over and over again\n\ndef result(clf):\n    clf.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=[(X_test, y_test)], verbose=False)\n    y_pred = clf.predict(X_test)\n    \n    print('Accuracy Score    : {:.4f}'.format(accuracy_score(y_test, y_pred)))\n    print('XGBoost f1-score      : {:.4f}'.format(f1_score( y_test , y_pred)))\n    print('XGBoost precision     : {:.4f}'.format(precision_score(y_test, y_pred)))\n    print('XGBoost recall        : {:.4f}'.format(recall_score(y_test, y_pred)))\n    print(\"XGBoost roc auc score : {:.4f}\".format(roc_auc_score(y_test,y_pred)))\n    print(\"\\n\",classification_report(y_pred, y_test))\n    \n    plt.figure(figsize=(6,6))\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    sns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")\n    plt.title(\"XGBoost Confusion Matrix (Rate)\")\n    plt.show()\n    \n    cm = confusion_matrix(y_test,y_pred)\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\",\n                xticklabels=[\"FALSE\",\"TRUE\"],\n                yticklabels=[\"FALSE\",\"TRUE\"],\n                cbar=False)\n    plt.title(\"XGBoost Confusion Matrix (Number)\")\n    plt.show()\n    \n    \ndef report(**params):\n    scores = [] \n    for i in range(0,250): # 250 samples\n        X_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, shuffle=True)\n        sc = StandardScaler()\n        clf = XGBClassifier(**params)\n        X_train = sc.fit_transform(X_train)\n        X_test = sc.fit_transform(X_test)\n        clf.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=[(X_test, y_test)], verbose=False)\n        scores.append(accuracy_score(clf.predict(X_test), y_test)) \n        \n    Importance = pd.DataFrame({'Importance':clf.feature_importances_*100},index=df.columns[:12])\n    Importance.sort_values(by='Importance',axis=0,ascending=True).plot(kind='barh',color='lightblue')\n    plt.xlabel('Importance for variable');\n    plt.hist(scores)\n    plt.show()\n    print(\"Best Score: {}\\nMean Score: {}\".format(np.max(scores), np.mean(scores)))","c50e9395":"report()","0abf6808":"param_grid = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\n\nclf = XGBClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=2, cv=10)\ngrid.fit(X_train, y_train)\ngrid.best_params_","847e2713":"clf = XGBClassifier(\n    max_depth= 5,\n    min_child_weight= 5,\n)\n\nresult(clf)","3ce5589b":"report(\n    max_depth= 5,\n    min_child_weight= 5,\n)","a920eb0c":"param_grid = {\n    'gamma': [i\/10.0 for i in range(0,8)]\n}\n\nclf = XGBClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=2, cv=10)\ngrid.fit(X_train, y_train)\ngrid.best_params_","4cdd1252":"clf = XGBClassifier(\n    max_depth= 5,\n    min_child_weight= 5,\n    gamma = 0.5,\n)\n\nresult(clf)","dbc5d941":"report(\n    max_depth= 5,\n    min_child_weight= 5,\n    gamma = 0.5,\n)","fe4db4c0":"param_grid = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree': [i\/10.0 for i in range(7,15)]\n}\n\nclf = XGBClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=2, cv=10)\ngrid.fit(X_train, y_train)\ngrid.best_params_","3b04f01f":"clf = XGBClassifier(\n    max_depth= 5,\n    min_child_weight= 5,\n    gamma = 0.5,\n    colsample_bytree= 0.9,\n    subsample= 0.8,\n    seed=0\n)\n\nresult(clf)","2e20ebd3":"report(\n    max_depth= 5,\n    min_child_weight= 5,\n    gamma = 0.5,\n    colsample_bytree= 0.9,\n    subsample= 0.8,\n)","cafdca92":"param_grid = {\n 'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05]\n}\n\nclf = XGBClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=2, cv=10)\ngrid.fit(X_train, y_train)\ngrid.best_params_","73d8c572":"clf = XGBClassifier(\n    max_depth= 5,\n    min_child_weight= 5,\n    gamma = 0.5,\n    colsample_bytree= 0.9,\n    subsample= 0.8,\n    reg_alpha= 0.01,\n    seed=0\n)\n\nresult(clf)","ceaf36d6":"report(\n    max_depth= 5,\n    min_child_weight= 5,\n    gamma = 0.5,\n    colsample_bytree= 0.9,\n    subsample= 0.8,\n    reg_alpha= 0.01\n)","f071a17e":"clf = XGBClassifier(\n    max_depth= 5,\n    min_child_weight= 5,\n    gamma = 0.5,\n    colsample_bytree= 0.9,\n    subsample= 0.8,\n    reg_alpha= 0.01,\n    learning_rate=0.1,\n    objective= 'binary:logistic',\n    seed= 0\n)\n\nresult(clf)","65132e4f":"report(\n    max_depth= 5,\n    min_child_weight= 5,\n    gamma = 0.5,\n    colsample_bytree= 0.9,\n    subsample= 0.8,\n    reg_alpha= 0.01,\n    learning_rate=0.1,\n    objective= 'binary:logistic',\n)\n","26384122":"param_grid = {\n    'max_depth':range(3,10,2),\n    'min_child_weight':range(1,6,2),\n    'gamma': [i\/10.0 for i in range(0,8)],\n    'subsample':[i\/10.0 for i in range(6,10)],\n    'colsample_bytree': [i\/10.0 for i in range(7,15)],\n    'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05],\n    'objective':['binary:logistic']\n}\n\nclf = XGBClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=2, cv=10)\ngrid.fit(X_train, y_train)\ngrid.best_params_","abe39657":"clf = XGBClassifier(\n    max_depth= 5,\n    min_child_weight= 3,\n    gamma = 0.2,\n    colsample_bytree= 0.7,\n    subsample= 0.6,\n    reg_alpha= 0.01,\n    learning_rate=0.1,\n    objective= 'binary:logistic',\n    seed= 0\n)\n\nresult(clf)","80026eb0":"report(\n    max_depth= 5,\n    min_child_weight= 3,\n    gamma = 0.2,\n    colsample_bytree= 0.7,\n    subsample= 0.6,\n    reg_alpha= 0.01,\n    learning_rate=0.01,\n    objective= 'binary:logistic',\n)","91f79fd4":"---\n## Simple Metod\nI applied XGBoost directly without changing anything and the result is as follows:","2854902d":"### clf = XGBClassifier(random_state=0)\nresult(clf)","24fda1ea":"---\n\n#### Step 2: Tune gamma","1188072c":"## Advanced Metod\n### Parameters\n\n1. **eta [default=0.3]**\n  - Analogous to learning rate in GBM\n  - Makes the model more robust by shrinking the weights on each step\n  - Typical final values to be used: 0.01-0.2\n\n\n2. **min_child_weight [default=1]**\n  - Defines the minimum sum of weights of all observations required in a child.\n  - This is similar to min_child_leaf in GBM but not exactly. This refers to min \u201csum of weights\u201d of observations while GBM has min \u201cnumber of observations\u201d.\n  - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n  - Too high values can lead to under-fitting hence, it should be tuned using CV.\n\n\n3. **max_depth [default=6]**\n  - The maximum depth of a tree, same as GBM.\n  - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n  - Should be tuned using CV.\n  - Typical values: 3-10\n\n\n4. **max_leaf_nodes**\n  - The maximum number of terminal nodes or leaves in a tree.\n  - Can be defined in place of max_depth. Since binary trees are created, a depth of \u2018n\u2019 would produce a maximum of 2^n leaves.\n  - If this is defined, GBM will ignore max_depth.\n\n\n5. **gamma [default=0]**\n  - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n  - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n\n\n6. **max_delta_step [default=0]**\n  - In maximum delta step we allow each tree\u2019s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n  - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n  - This is generally not used but you can explore further if you wish.\n\n\n7. **subsample [default=1]**\n  - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n  - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n  - Typical values: 0.5-1\n\n\n8. **colsample_bytree [default=1]**\n  - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n  - Typical values: 0.5-1\n\n\n9. **colsample_bylevel [default=1]**\n  - Denotes the subsample ratio of columns for each split, in each level.\n  - I don\u2019t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n\n\n10. **lambda [default=1]**\n  - L2 regularization term on weights (analogous to Ridge regression)\n  - This used to handle the regularization part of XGBoost. Though many data scientists don\u2019t use it often, it should be explored to reduce overfitting.\n\n\n11. **alpha [default=0]**\n  - L1 regularization term on weight (analogous to Lasso regression)\n  - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n\n\n12. **scale_pos_weight [default=1]**\n  - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.\n\n\n---\n\n### Learning Task Parameters\nThese parameters are used to define the optimization objective the metric to be calculated at each step.\n\n1. objective [default=reg:linear]\n    - This defines the loss function to be minimized. Mostly used values are:\n        - binary:logistic \u2013logistic regression for binary classification, returns predicted probability (not class)\n        - multi:softmax \u2013multiclass classification using the softmax objective, returns predicted class (not probabilities)\n            - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n        - multi:softprob \u2013same as softmax, but returns predicted probability of each data point belonging to each class.\n2. eval_metric [ default according to objective ]\n    - The metric to be used for validation data.\n    - The default values are rmse for regression and error for classification.\n    - Typical values are:\n        - rmse \u2013 root mean square error\n        - mae \u2013 mean absolute error\n        - logloss \u2013 negative log-likelihood\n        - error \u2013 Binary classification error rate (0.5 threshold)\n        - merror \u2013 Multiclass classification error rate\n        - mlogloss \u2013 Multiclass logloss\n        - auc: Area under the curve\n3. seed [default=0]\n    - The random number seed.\n    - Can be used for generating reproducible results and also for parameter tuning.\n\nIf you\u2019ve been using Scikit-Learn till now, these parameter names might not look familiar. A good news is that xgboost module in python has an sklearn wrapper called XGBClassifier. It uses sklearn style naming convention. The parameters names which will change are:\n   1. eta \u2013> learning_rate\n   2. lambda \u2013> reg_lambda\n   3. alpha \u2013> reg_alpha\n\nYou must be wondering that we have defined everything except something similar to the \u201cn_estimators\u201d parameter in GBM. Well this exists as a parameter in XGBClassifier. However, it has to be passed as \u201cnum_boosting_rounds\u201d while calling the fit function in the standard xgboost implementation.","2076e203":"---\n\n#### Step 1: Tune max_depth and min_child_weight","6ef2d105":"XGBoost is a decison-tree based and gradient-boosting ML system. If your data is non-structured data such as picture \/ text \/ sound, deep learning with artificial neural networks will be the right choice.\nHowever, if you don't have a lot of data , I recommend starting with decision-tree based algorithms. Decision-tree based algorithms have evolved a lot over time. You can see this evolution more easily in the flow below.\n\n![](https:\/\/miro.medium.com\/proxy\/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg)\n\nXGBoost was first presented as an article at the SIGKDD 2016 conference by two researchers at Washington University, Tianqi Chen and Carlos Guestrin, and made a tremendous impression in the ML world. After its presentation, it became the star of not only academic competitions, but also of Kaggle competitions and started to find application in industry. Thanks to this, it has an open-source repo and a strong community contributed by many data scientists today.\n\n\u201cRandom forests are made up of multiple decision trees working together (ensemble). Combining and working models with no correlation between each other performs better than any model that works alone, ensemble learning is based on this. The lack of correlation between each other helps each tree avoid its own mistakes from others. \"\nAs a one-to-one analogy, we cannot make important decisions in real life alone. In random forests, multiple weak decision trees (we call them weak learner) come together to create a stronger tree. You start with a very weak tree, you make a mistake in that tree, and you make your way to a stronger tree by not making the mistake you dug from that tree on the next tree. The reason why these trees are weak is that the data they are trained on consists of random subsets of our data set, which we provide by bagging.\n\n## Why Does XGBoost Work Well?\nIn fact, both XGBoost and Gradient-Boosting Machines (GBMs) use community-based poor learners, poor learners are supported by the gradient-descent method.\n\n![](https:\/\/miro.medium.com\/proxy\/1*FLshv-wVDfu-i54OqvZdHg.png)\n\n## System Optimizations\n- **Parallel Working:** XGBoost enables the creation of decision trees much faster by parallelization while creating. The underlying reason for doing this is that while creating base-learners, it is able to switch between internal and external cycles. Normally, the external cycloids compute the internal cyclic attributes while creating the leaves of the decision-tree. However, parallelization is limited because the outer cycles cannot be completed before the inner cycles are finished, that is, the leaves of the tree will not be formed without calculating the features. XGBoost speeds up runtime by varying the computing power allocated to the internal and external cycles and greatly reduces the parallelization overhead.\n- **Tree-Pruning:** It stops separating according to negative-loss criterion while separating tree branches in GBMs. XGBoost, on the contrary, determines the depth of the tree with the max_depth parameter from the very beginning, and if the tree is too advanced downward it pruns backwards. Because XGBoost prioritizes depth, it significantly increases complexity and thus computational performance.\n- **Hardware Optimization:** XGBoost was designed from the outset to make better use of hardware resources. For example, each thread keeps an internal buffer and gradient statistics in this buffer, keeping in mind the fullness of the buffer. Apart from that, it can fit larger data into memory by optimizing the disk space thanks to improvements such as \"out-of-core\" computing.\n\n\n## Algorithmic Improvements\n- **Regularization:** Overfitting can be prevented using both LASSO and Ridge regularization.\n- **Sparse Compatibility:** In real life, unfortunately, data sets contain many missing values. XGBoost is able to learn the most accurate way with poor-learners by looking at the loss of education. Or sometimes, the data set has missing values in a certain order (sensor \/ communication errors, etc.). In these cases, XGBoost can collect the situation.\n- **Weighted Quarter Drawing:** One of the biggest advantages of XGBoost is that it uses the observation points in the data set by weighing them in order to distinguish them from the most accurate point while separating into trees.\n- **Cross-validation:** XGBoost comes with a cross-validation (cv) application in itself, so scikit-learn etc. from outside. You don't need to make a cv using it. Also, you do not need to specify how many iterations will be made in each run.","37ecdf04":"---\n\n#### Step 4: Tuning Regularization Parameters","382fd20a":"![XGBoost](https:\/\/i.ibb.co\/DDM7r46\/xgboost.png)\n\n- **ML Part 1** - Logistic Regression\n- **ML Part 2** - K-Nearest Neighbors (KNN)\n- **ML Part 3** - Support Vector Machine (SVM)\n- **ML Part 4** - Artificial Neural Network (NN)\n- **ML Part 5** - Classification and Regression Tree (CART)\n- **ML Part 6** - Random Forests\n- **ML Part 7** - Gradient Boosting Machines (GBM)\n- **ML Part 8 - XGBoost**\n- **ML Part 9** - LightGBM\n- **ML Part 10** - CatBoost","c333e726":"---\n\n#### Step 3: Tune subsample and colsample_bytree","de696b2c":"---","c3c9f466":"---\n\n#### Step 5: Reducing Learning Rate"}}