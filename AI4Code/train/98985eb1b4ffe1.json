{"cell_type":{"a8101614":"code","98057069":"code","8945e563":"code","5ad42706":"code","cba4fa78":"code","735f02d0":"code","29f5fdf6":"code","c6819aab":"code","d655b173":"code","999618e5":"code","ef6d3691":"code","a9db61eb":"code","4a1d0522":"code","d7965f39":"code","df781849":"code","4c83f8b3":"code","82b9204b":"code","df092dfc":"code","1034c52c":"code","e8ff093e":"code","d2307a61":"code","86fb0d94":"code","c947ccf5":"code","a3dba56d":"code","5306c143":"code","ef934c07":"code","9372b4d7":"code","2ab1a322":"code","f7a15370":"code","76014ef2":"code","f1f743f5":"code","fe18783b":"code","74d0be43":"code","5960f129":"code","1a320060":"code","5e0577f0":"code","0a6ce39e":"code","0f6eed17":"code","cb0f3a69":"code","21b6338b":"code","d789c629":"code","13702cfd":"markdown","869c35d7":"markdown","f5bec1ef":"markdown","2f0327f9":"markdown","d408e843":"markdown","f6557c24":"markdown","7ed9e402":"markdown","b621263f":"markdown","c6075fb6":"markdown","83a08069":"markdown","2bdbffd9":"markdown","241ea083":"markdown","bc2adfe2":"markdown","fa032974":"markdown","28828ed6":"markdown","0a7ff071":"markdown","037feb1e":"markdown","53fc5bad":"markdown","6f4fba9a":"markdown","469a3446":"markdown","57243cda":"markdown","c4f06ad3":"markdown","9da0f1f9":"markdown","305513ea":"markdown","c3f36452":"markdown","e54aad5e":"markdown","a572455d":"markdown","6103f439":"markdown","27de4474":"markdown","e041eea0":"markdown","cefb064c":"markdown","e265126a":"markdown","7007c799":"markdown","167d2295":"markdown","41107d1e":"markdown"},"source":{"a8101614":"import numpy as np, pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))","98057069":"path = '..\/input\/'\ncomp = 'jigsaw-toxic-comment-classification-challenge\/'\n\nEMBEDDING_FILE = f'{path}glove6b50d\/glove.6B.50d.txt'\n#EMBEDDING_FILE = f'{path}glove6b100dtxt\/glove.6B.100d.txt'\n\nTRAIN_DATA_FILE = f'{path}{comp}train.csv'\nTEST_DATA_FILE = f'{path}{comp}test.csv'","8945e563":"train = pd.read_csv(TRAIN_DATA_FILE)\ntest = pd.read_csv(TEST_DATA_FILE)\n\nprint('train shape:', train.shape,\n      '\\ntest shape:', test.shape)","5ad42706":"# Check the data\ntrain.isnull().any(), test.isnull().any()","cba4fa78":"#visualize word distribution:\n#first, we add a new column where we put the number of words of the corresponding comment_text\ntrain[\"document_length\"] = train[\"comment_text\"].apply(lambda words: len(words.split(\" \")))\nmax_seq_len = np.round(train[\"document_length\"].mean() + train[\"document_length\"].std()).astype(int)\n\nplt.figure(figsize=(15,8))\nsns.set(font_scale = 1.5)\nsns.distplot(train[\"document_length\"], hist=True, kde=True, color='b', label='Document length')\nlabel = 'Max length = {}'.format(max_seq_len)\nplt.axvline(x=max_seq_len, color='k', linestyle='--', label=label)\nplt.legend()\nplt.show()\n\n#free space\ndel train[\"document_length\"]","735f02d0":"list_classes = list(train.columns[2:].values)\nnum_classes = len(list_classes)\ny_train = train[list_classes].to_numpy()\ndistrib_classes = train[list_classes].sum(axis=0)\n\n#visualize classes distribution\nplt.figure(figsize=(15,8))\nsns.set(font_scale = 1.5)\nax= sns.barplot(list_classes, distrib_classes)\n\nplt.title(\"Comments in each category\", fontsize=24)\nplt.ylabel('Number of comments', fontsize=18)\nplt.xlabel('Comment Type ', fontsize=18)\n\n#add the count above:\nrects = ax.patches\nfor rect, distrib_classe in zip(rects, distrib_classes):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, distrib_classe, ha='center', va='bottom', fontsize=18)\n\nplt.show()","29f5fdf6":"#Sum on each row: results go from 0 (good comment) to 6 (the \"winners\" that have all tags)\nrowSums = train[list_classes].sum(axis=1)\nmultiLabel_counts = rowSums.value_counts()\nmultiLabel_counts = multiLabel_counts.iloc[1:]\n\nsns.set(font_scale = 1.5)\nplt.figure(figsize=(15,8))\nax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\n\nplt.title(\"Comments having multiple labels \")\nplt.ylabel('Number of comments', fontsize=18)\nplt.xlabel('Number of labels', fontsize=18)\n\n#adding the text labels\nrects = ax.patches\nlabels = multiLabel_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n    \nplt.show()","c6819aab":"from wordcloud import WordCloud\n\nplt.figure(figsize=(18,7))\n\nfor i in range (1,7):\n    plt.subplot(2, 3, i)\n    subset = train[train[train.keys()[i+1]] == 1]\n    text = subset.comment_text.values\n    cloud_i = WordCloud(background_color='black',\n                        collocations=False,\n                        max_words = 100\n                       ).generate(\" \".join(text))\n    \n    plt.axis('off')\n    title = list_classes[i-1]\n    plt.title(title,fontsize=15)\n    plt.imshow(cloud_i)\n\nplt.show()","d655b173":"#load embeddings\nprint('loading word embeddings...')\nembeddings_index = {}\nf = open(EMBEDDING_FILE)\n\n#more readable:\nfor line in f:\n    values = line.strip().split(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('found %s word vectors.' % len(embeddings_index))\n\n#alternative, shorter but less readable:\n#def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n#embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))","999618e5":"#Import the librairies\nimport re   # module re for regular expression\nimport nltk #Natural Language Toolkit\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer #Tokenizer for preprocessing\n\npreprop_tokenizer = RegexpTokenizer(r'\\w+')","ef6d3691":"stop_words = set(stopwords.words('english'))\n#stop_words","a9db61eb":"stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}', '_'])\nstop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])","4a1d0522":"raw_comments_train = train['comment_text'].tolist()\nraw_comments_test = test['comment_text'].tolist() \n\nprint(\"pre-processing training data...\")\nprocessed_comments_train = []\nfor comment in raw_comments_train:\n    tokens = preprop_tokenizer.tokenize(comment)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_comments_train.append(\" \".join(filtered))\n\nprint(\"pre-processing test data...\")\nprocessed_comments_test = []\nfor comment in raw_comments_test:\n    tokens = preprop_tokenizer.tokenize(comment)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_comments_test.append(\" \".join(filtered))\n    \nprint(\"Done.\")","d7965f39":"#Config parameters:\nembed_size = 50 # how big is each word vector\nmax_features = 20000#20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = max_seq_len # max number of words in a comment to use. Here, mean+std as defined earlier","df781849":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nprint(\"tokenizing input data...\")\ntokenizer = Tokenizer(num_words=max_features)\n\n####creates the vocabulary index (i.e. word -> index dictionary) based on word frequency. (0 is reserved for padding, and lower integer means more frequent word.)\ntokenizer.fit_on_texts(processed_comments_train)\n\n#Transforms each text in texts to a sequence of integers taken from the word_index dictionary:\nlist_tokenized_train = tokenizer.texts_to_sequences(processed_comments_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(processed_comments_test)\n\n#pad or trunc the sequences\nX_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' %len(word_index))","4c83f8b3":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std","82b9204b":"#embedding matrix\nprint('preparing embedding matrix...')\nwords_not_found = []\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: \n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('Done.')","df092dfc":"print(\"sample words not found: \", np.random.choice(words_not_found, 10))","1034c52c":"from keras.callbacks import Callback\nfrom sklearn.metrics import roc_auc_score\n\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","e8ff093e":"from sklearn.model_selection import train_test_split\n\n#Create validation split\nX_train_splitted_tmp, X_val, y_train_splitted_tmp, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, shuffle=True)\n#Create training and evaluation split\nX_train_splitted, X_eval, y_train_splitted, y_eval = train_test_split(X_train_splitted_tmp, y_train_splitted_tmp, test_size=0.1, random_state=42)","d2307a61":"print(np.shape(X_train_splitted), np.shape(y_train_splitted))\nprint(np.shape(X_val) , np.shape(y_val))\nprint(np.shape(X_eval) , np.shape(y_eval))","86fb0d94":"#New train set:\ndistrib_classes_y_train_splitted = y_train_splitted.sum(axis=0)\ndistrib_classes_y_train_splitted\/len(y_train_splitted)*100","c947ccf5":"#Validation set:\ndistrib_classes_y_val = y_val.sum(axis=0)\ndistrib_classes_y_val\/len(y_val)*100","a3dba56d":"#Evaluation set:\ndistrib_classes_y_eval = y_eval.sum(axis=0)\ndistrib_classes_y_eval\/len(y_eval)*100","5306c143":"from keras import backend as K\n\ndef calculating_class_weights(y_true):\n    from sklearn.utils.class_weight import compute_class_weight\n    number_dim = np.shape(y_true)[1]\n    weights = np.empty([number_dim, 2])\n    for i in range(number_dim):\n        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n    return weights\nclass_weights = calculating_class_weights(y_train_splitted)\nclass_weights","ef934c07":"def get_weighted_loss(weights):\n    def weighted_loss(y_true, y_pred):\n        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n    return weighted_loss","9372b4d7":"#How to load this model and the custom weighted loss (for model ensembling)\n#model = load_model(\"path\/to\/model.hd5f\", custom_objects={\"weighted_loss\": get_weighted_loss(weights)}","2ab1a322":"from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Conv1D, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model\nfrom keras.optimizers import adam","f7a15370":"inputs = Input(shape=(maxlen,))\n#x = BatchNormalization()(inputs)\nx = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable = False)(inputs)\n#x = SpatialDropout1D(0.2)(x)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1, kernel_initializer='he_normal'))(x)\n#x = Conv1D(32, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_normal\")(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\noutputs = Dense(6, activation=\"sigmoid\")(x) #Sigmoid gives independent probabilities for each class.\n\nmodel_LSTM = Model(inputs=inputs, outputs=outputs)\nmodel_LSTM.name = 'model_LSTM'\n\nmodel_LSTM.compile(loss=get_weighted_loss(class_weights), optimizer=adam(lr=1e-3), metrics=['acc']) #binary_crossentropy independently optimises each class.\n\nmodel_LSTM.summary()","76014ef2":"batch_size = 128\nepochs = 6","f1f743f5":"from keras.callbacks import EarlyStopping,ModelCheckpoint\n\nfilepath=\"best_weights.hdf5\"\nmcp = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True,  save_weights_only=True, mode='min')\nearlystop = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=4)\nRocAuc_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n\ncallbacks_list = [RocAuc_val, mcp, earlystop]","fe18783b":"history_model_LSTM = model_LSTM.fit(X_train_splitted, \n                                    y_train_splitted, \n                                    batch_size=batch_size, \n                                    epochs=epochs, \n                                    validation_data=(X_val, y_val),\n                                    callbacks = callbacks_list, \n                                    verbose=1)","74d0be43":"#Define a smooth function to display the training and validation curves\ndef plot_learning_curves(history):\n    val_loss = history.history['val_loss']\n    loss = history.history['loss']\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    \n    epochs = range(1, len(loss)+1 )\n    \n    # Plot the loss and accuracy curves for training and validation \n    fig, ax = plt.subplots(2,1, figsize=(12, 12))\n    ax[0].plot(epochs, loss, 'bo', label=\"Training loss\")\n    ax[0].plot(epochs, val_loss, 'b', label=\"Validation loss\",axes =ax[0])\n    legend = ax[0].legend(loc='best', shadow=True)\n    ax[0].set_xlabel('Epochs')\n    ax[0].set_ylabel('Loss')#\n\n    ax[1].plot(epochs, acc, 'bo', label=\"Training accuracy\")\n    ax[1].plot(epochs, val_acc, 'b',label=\"Validation accuracy\")\n    legend = ax[1].legend(loc='best', shadow=True)\n    ax[1].set_xlabel('Epochs')\n    ax[1].set_ylabel('Accuracy')\n    return","5960f129":"# Visualisation:\nplot_learning_curves(history_model_LSTM)","1a320060":"#Loading model weights\nmodel_LSTM.load_weights(filepath)\n#Get the prediction:\nprint('Predicting....')\ny_pred = model_LSTM.predict(X_eval,batch_size=1024,verbose=1)\nprint('Done.')","5e0577f0":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_eval, np.where(y_pred > 0.9, 1, 0)) #np.where(a > 0.5, 1, 0)","0a6ce39e":"from sklearn.metrics import multilabel_confusion_matrix\n\n#you can fine tune the threshold for increasing recall or precision\ny_pred_col0 = np.where(y_pred[:,0] > 0.4, 1, 0)\ny_pred_col1 = np.where(y_pred[:,1] > 0.5, 1, 0)\ny_pred_col2 = np.where(y_pred[:,3] > 0.3, 1, 0)\ny_pred_col3 = np.where(y_pred[:,3] > 0.6, 1, 0)\ny_pred_col4 = np.where(y_pred[:,4] > 0.5, 1, 0)\ny_pred_col5 = np.where(y_pred[:,5] > 0.5, 1, 0)\n\ny_pred_col0 = np.expand_dims(y_pred_col0, axis=1)\ny_pred_col1 = np.expand_dims(y_pred_col1, axis=1)\ny_pred_col2 = np.expand_dims(y_pred_col2, axis=1)\ny_pred_col3 = np.expand_dims(y_pred_col3, axis=1)\ny_pred_col4 = np.expand_dims(y_pred_col4, axis=1)\ny_pred_col5 = np.expand_dims(y_pred_col5, axis=1)\n\ny_pred_colTot = np.concatenate((y_pred_col0, y_pred_col1, y_pred_col2, y_pred_col3, y_pred_col4, y_pred_col5), axis=1)\n\nmcm = multilabel_confusion_matrix(y_eval, y_pred_colTot, sample_weight=None, samplewise=False)\n\nfig = plt.figure(figsize = (12,10))\nfor i in range(1,7):\n    plt.subplot(2,3,i)\n    if i%2==0:\n        cmap = \"Reds\"\n    else:\n        cmap = \"Blues\"\n    sns.set(font_scale=0.8)\n    title = '{}'.format(list_classes[i-1])\n    plt.title(title, fontsize = 15)\n    sns.heatmap(mcm[i-1], cmap=cmap, square=True, fmt='.0f', cbar=False, annot=True)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","0f6eed17":"from sklearn.metrics import classification_report\n\ncr = classification_report(y_eval, np.round(y_pred), target_names = list_classes)\nprint(cr)","cb0f3a69":"#Production: \nhistory_model_LSTM = model_LSTM.fit(X_train, \n                                    y_train, \n                                    batch_size=batch_size, \n                                    epochs=epochs,\n                                    callbacks = callbacks_list, \n                                    verbose=1)","21b6338b":"print('Predicting....')\ny_test = model_LSTM.predict(X_test,batch_size=1024,verbose=1)\nsample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\nsample_submission[list_classes] = y_test\nsample_submission.to_csv('submission.csv', index=False)","d789c629":"###---  Test with the LabelPowerset + RandomOverSampler   ---####\n###-------------------Not used-------------------------------####\n\n#from skmultilearn.problem_transform import LabelPowerset\n#from imblearn.over_sampling import RandomOverSampler\n#\n#lp = LabelPowerset()\n#ros = RandomOverSampler(random_state=42)\n## Applies the above stated multi-label (ML) to multi-class (MC) transformation.\n#yt = lp.transform(y_train_splitted)\n#X_resampled, y_resampled = ros.fit_sample(X_train_splitted, yt)\n## Inverts the ML-MC transformation to recreate the ML set\n#y_train_splitted = lp.inverse_transform(y_resampled)","13702cfd":"We'll also look at how are distributed the different classes:\n","869c35d7":"## Visualization","f5bec1ef":"## Preprocessing the text","2f0327f9":"## Wordcloud representations","d408e843":"## To do list:\n- Test labelPowerset","f6557c24":"First, we'll define stopwords for future removing because we don't want these words taking up space in our database, or taking up processing time.\n\n(Note to self: not sure if it's a good idea, we might loose information by removing them... Have to check it)","7ed9e402":"## Callbacks","b621263f":"Let's set the list of stopwords:","c6075fb6":"We have an unbalanced dataset: a lot of \"good\" comments compared to \"bad\" ones. In particular, we have very few \"threat\" comments (only 0.3%)... This might pose a problem if we want to split the training set to create an evaluation set, as the \"stratify\" argument in train_test_split might fail.","83a08069":"## Classification report","2bdbffd9":"Check the distibutions in the new sets:","241ea083":"Accuracy is not helpful with imbalanced dataset. Thus, we'll use Area under ROC (ROC-AUC) as our performance metric. We'll also look at the precision and recall","bc2adfe2":"Simple bidirectional LSTM with two fully connected layers and dropout.","fa032974":"Using train_test_split without the stratifying option still gives us distributions that are somehow similar. That will be good enough for a first try with a first model.","28828ed6":"## Load and parse the GloVe word-embeddings file","0a7ff071":"## Model : GloVe-Pretrained Bidirectional LSTM + dropout","037feb1e":"## Split into a train, val, and eval sets","53fc5bad":"Let's see how many comments have multiple labels:","6f4fba9a":"## Loading libraries","469a3446":"## Submission","57243cda":"In multilabel classification, the [accuracy classification score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html) computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_eval.","c4f06ad3":"In this competition, we have a multi-label scenario, because a sample can have any number of labels (or none at all). To solve it, we'll use an ensemble of models that includes:\n- Bi-directionnal LSTM + visu (present notebook)\n- ConvLSTM ?\n- Random Forest (works well with imbalanced datasets)\n- XGBoost","9da0f1f9":"## Tokenizing the text","305513ea":"## Define paths","c3f36452":"## Preparing the GloVe word-embeddings matrix","e54aad5e":"Let's look at some words not found in the embeddings:","a572455d":"We are ready to pre-process our data:","6103f439":"## Tackle imbalance with class_weight","27de4474":"## Confusion matrices","e041eea0":"## Accuracy classification score","cefb064c":"We'll add some punctuation marks to the list. We will keep '?' and '!' as they may help classify violent comments","e265126a":"No missing values, we're good to go.","7007c799":"## Evaluation of the model","167d2295":"We will now create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and standard deviation of embeddings the GloVe has when generating the random init.","41107d1e":"## Load train and test sets"}}