{"cell_type":{"0ff79b2f":"code","31c27993":"code","b005b3ad":"code","05c41805":"code","196458d8":"code","0400bc71":"code","a835c42d":"code","9103f955":"code","a8f0dd24":"code","b0689640":"code","a8755165":"code","53b91eae":"code","fb74cea0":"code","c0f945f2":"code","a1798f2d":"code","a61e24e0":"code","5dd0c471":"code","94cac408":"code","aa28319f":"code","ebbc3356":"code","8d0d2df9":"code","1aab8e78":"code","31369b3c":"code","f96f5d63":"code","e2b62e6e":"code","b2ab5367":"code","2e4617b4":"code","c22d685b":"markdown","6ea95983":"markdown","980dc914":"markdown","f83bef40":"markdown","b1d2953a":"markdown","0af5d8a6":"markdown","ac66f7cc":"markdown","a3aa44e0":"markdown","f2d48779":"markdown","3ea6c3f6":"markdown","a9f65eeb":"markdown","525e44eb":"markdown","727be8eb":"markdown","67955b4f":"markdown","6af06f49":"markdown","fdf2d755":"markdown","e23d0a04":"markdown","d18c4a4e":"markdown","2621ec2a":"markdown","945ab271":"markdown","a95801ad":"markdown","4f4207fb":"markdown"},"source":{"0ff79b2f":"import seaborn as sns \nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import zscore\nfrom datetime import datetime\n\n#Machine Learning Packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","31c27993":"df_gender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')","b005b3ad":"## Print the 5 first rows of the DataFrame\n#df_train.head()\n#df_test.head()\n\n## Print the shape\n#df_train.shape\n#df_test.shape\n\n## Print the datatypes\n#df_train.dtypes\n#df_test.dtypes\n\n## Info of DataFrame\n#df_train.info()\n#df_test.info()\n\n## Describe the DataFrame. Basic descriptive stats\n#df_train.describe()\n#df_test.describe()\n\n## Null data amout\n#print(pd.isnull(df_train).sum())\n#print(pd.isnull(df_test).sum())\n","05c41805":"df_train['Sex'].replace(['female','male'],[0,1],inplace=True)\ndf_test['Sex'].replace(['female','male'],[0,1],inplace=True)","196458d8":"fig, ax = plt.subplots()\nsns.countplot(x='Sex', hue ='Survived', data = df_train)\nnew_xtick = ['Females', 'Males']\nax.set_xticklabels(new_xtick)\nnew_legend = ['Deceased', 'Survivors']\nplt.legend(new_legend)\nplt.show()","0400bc71":"df_train['Embarked'].replace(['Q','S', 'C'],[0,1,2],inplace=True)\ndf_test['Embarked'].replace(['Q','S', 'C'],[0,1,2],inplace=True)","a835c42d":"fig, ax = plt.subplots()\nsns.countplot(x='Embarked',hue='Survived',data=df_train)\nnew_xtick = ['Q','S', 'C']\nax.set_xticklabels(new_xtick)\nnew_legend = ['Deceased', 'Survivors']\nplt.legend(new_legend)\nplt.show()","9103f955":"avg_age = ((df_train[\"Age\"].mean() * df_train[\"Age\"].shape[0]) + (df_test[\"Age\"].mean() * df_test[\"Age\"].shape[0]))\/ (df_train[\"Age\"].shape[0] + df_test[\"Age\"].shape[0])\navg_age = np.round(avg_age)\n\ndf_train['Age'] = df_train['Age'].replace(np.nan, avg_age)\ndf_test['Age'] = df_test['Age'].replace(np.nan, avg_age)","a8f0dd24":"#Bands: (1) 0-7, (2) 8-15, (3) 16-25, (4) 26-32, (5) 33-40, (6) 41-60, (7) 61-100\nbins = [0, 7, 15, 25, 32, 40, 60, 100]\nnames = ['1', '2', '3', '4', '5', '6', '7']\ndf_train['Age'] = pd.cut(df_train['Age'], bins, labels = names)\ndf_test['Age'] = pd.cut(df_test['Age'], bins, labels = names)\n\ndf_train['Age'].groupby(df_train.Age).count()","b0689640":"fig, ax = plt.subplots()\nsns.countplot(x='Age',hue='Survived',data=df_train)\nnew_xtick = ['0-7', '8-15', '16-25','26-32','33-40','41-60','61-100']\nax.set_xticklabels(new_xtick)\nnew_legend = ['Deceased', 'Survivors']\nplt.legend(new_legend)\nplt.show()","a8755165":"df_train.drop(['Cabin'], axis = 1, inplace=True)\ndf_test.drop(['Cabin'], axis = 1, inplace=True)\ndf_train = df_train.drop(['PassengerId','Name','Ticket'], axis=1)\ndf_test = df_test.drop(['Name','Ticket'], axis=1)","53b91eae":"df_train.dropna(axis=0, how='any', inplace=True)\ndf_test.dropna(axis=0, how='any', inplace=True)","fb74cea0":"def column_dissociation(DataFrame, ColumnName):\n    df = DataFrame\n    uniques = df[ColumnName].drop_duplicates()#unique results of the column\n    uniques = uniques.sort_values( ascending=True) #Sort the values\n    new_column_name = \"\"\n    for i, value in enumerate(uniques):\n        new_column_name = ColumnName + '_' + str(value)\n        df[new_column_name] = np.zeros(df.shape[0])\n        df[new_column_name] = df[new_column_name].astype('int64')\n        df.loc[df[ColumnName] == value, new_column_name] = 1\n    df.drop([ColumnName], axis = 1, inplace=True)\n    return df","c0f945f2":"def column_comparator(DataFrame1, DataFrame2):\n    bool_test = False\n    result = []\n    for i, column1 in enumerate(DataFrame1):\n        bool_test = False\n        for j, column2 in enumerate(DataFrame2):\n            if column1 == column2:\n                bool_test = True \n                \n        if not bool_test:\n            result.append(column1)\n    return result","a1798f2d":"def create_zeros_column(DataFrame, columns):\n    df = DataFrame\n    if len(columns) == 0:\n        print('No columns added')\n    elif isinstance(columns, str):\n        df[columns] = np.zeros(df.shape[0])\n        df[columns] = df[columns].astype('int64')\n    else:\n        for i, col in enumerate(columns):\n            df[col] = np.zeros(df.shape[0])\n            df[col] = df[col].astype('int64')\n    return df","a61e24e0":"#- Age (Age Group)\ndf_train = column_dissociation(df_train, 'Age')\ndf_test = column_dissociation(df_test, 'Age')\n#- Embarket\ndf_train = column_dissociation(df_train, 'Embarked')\ndf_test = column_dissociation(df_test, 'Embarked')\n#- Pclass\ndf_train = column_dissociation(df_train, 'Pclass')\ndf_test = column_dissociation(df_test, 'Pclass')\n#- SibSp\ndf_train = column_dissociation(df_train, 'SibSp')\ndf_test = column_dissociation(df_test, 'SibSp')\n#- Parch\ndf_train = column_dissociation(df_train, 'Parch')\ndf_test = column_dissociation(df_test, 'Parch')\n\n#Are there any column in a dataFrame that are not in the other\ncc1 = column_comparator(df_test,df_train)\nprint(cc1)\ncc2 = column_comparator(df_train, df_test)\nprint(cc2)\ndf_train = create_zeros_column(df_train, 'Parch_9')","5dd0c471":"# Correlation Heatmap\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':6 }\n    )\n    \n    plt.title('Heatmap Correlation', y=1.05, size=15)\n\ncorrelation_heatmap(df_train)","94cac408":"X = np.array(df_train.drop(['Survived'], 1))\ny = np.array(df_train['Survived'])","aa28319f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","ebbc3356":"##Logistic Regression\nlogreg = LogisticRegression(max_iter = 100000)\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\n\nprint('Logistic Regression')\nprint('Score: ' + str(logreg.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n##K neighbors\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred = knn.predict(X_test)\n\nprint('K-Nearest Neighbors Classifier')\nprint('Score: ' + str(knn.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n##Support Vector Machines\nsvc = SVC()\nsvc.fit(X_train, y_train)\nY_pred = svc.predict(X_test)\n\nprint('Support Vector Machine Classifier')\nprint('Score: ' + str(svc.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n##Perceptron\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\nY_pred = perceptron.predict(X_test)\n\nprint('Perceptron Classifier')\nprint('Score: ' + str(perceptron.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n##XGBoost Classifier\nxgboost = XGBClassifier(learning_rate=1.3, n_estimators=2000, max_depth=40, min_child_weight=40, \n                      gamma=0.4,nthread=10, subsample=0.8, colsample_bytree=.8, \n                      objective= 'binary:logistic',scale_pos_weight=10,seed=29)\nxgboost.fit(X_train, y_train)\nY_pred = xgboost.predict(X_test)\n\nprint('XGBoost Classifier')\nprint('Score: ' + str(xgboost.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n##Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=1000, random_state = 0)\nrandom_forest.fit(X_train, y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\n\nprint('Random Forest Classifier')\nprint('Score: ' + str(random_forest.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n##Multi-Layer Perceptron Classifier\nmlp_classifier = MLPClassifier(hidden_layer_sizes = 1000, alpha = 0.00001, learning_rate = 'adaptive', learning_rate_init = 0.001, random_state = 0, max_iter = 100000 )\nmlp_classifier.fit(X_train, y_train)\nY_pred = mlp_classifier.predict(X_test)\nmlp_classifier.score(X_train, y_train)\n\nprint('Multi-Layer Perceptron Classifier')\nprint('Score: ' + str(mlp_classifier.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n\n##AdaBoostClassifier\nadaboost = AdaBoostClassifier()\nadaboost.fit(X_train, y_train)\nY_pred = adaboost.predict(X_test)\nadaboost.score(X_train, y_train)\n\nprint('AdaBoost Classifier')\nprint('Score: ' + str(adaboost.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n##Linear Discriminant Analysis\nlineardiscriminant = LinearDiscriminantAnalysis()\nlineardiscriminant.fit(X_train, y_train)\nY_pred = lineardiscriminant.predict(X_test)\nlineardiscriminant.score(X_train, y_train)\n\nprint('Linear Discriminant Analysis')\nprint('Score: ' + str(lineardiscriminant.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n##Gradient Boosting Classifier\ngradient_boosting = GradientBoostingClassifier()\ngradient_boosting.fit(X_train, y_train)\nY_pred = gradient_boosting.predict(X_test)\ngradient_boosting.score(X_train, y_train)\n\nprint('Gradient Boosting Classifier')\nprint('Score: ' + str(gradient_boosting.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n\n##Decision Tree Classifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nY_pred = decision_tree.predict(X_test)\ndecision_tree.score(X_train, y_train)\n\nprint('Decision Tree Classifier')\nprint('Score: ' + str(decision_tree.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n\n##Decision Tree Classifier\nextra_tree = DecisionTreeClassifier()\nextra_tree.fit(X_train, y_train)\nY_pred = extra_tree.predict(X_test)\nextra_tree.score(X_train, y_train)\n\nprint('Decision Tree Classifier')\nprint('Score: ' + str(extra_tree.score(X_train, y_train)))\nprint('Accuracy: '+ str(accuracy_score(y_test, Y_pred)))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, Y_pred))\nprint(classification_report(y_test, Y_pred))\nprint('------------------------------------')\n\n","8d0d2df9":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)\n\n\n# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\n\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(Perceptron(random_state=random_state))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(XGBClassifier())\nclassifiers.append(MLPClassifier(random_state=random_state))\n\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"Perceptron\",\n\"RandomForest\",\"KNeighborsClassifier\",\"LogisticRegression\",\"XGBClassifier\",\"MultipleLayerPerceptron\", \"AdaBoostClassifier\", \"LinearDiscriminantAnalysis\", \"GradientBoosting\", \"DecisionTree\", \"ExtraTreesClassifier\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng.set_xlim(xmin=0.6)\ng = g.set_title(\"Cross validation scores\")\n","1aab8e78":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300, 400, 500, 750, 1000],\n              'learning_rate': [0.1, 0.05, 0.01, 0.005, 0.001],\n              'max_depth': [4, 8, 16, 32, 64, 128],\n              'min_samples_leaf': [100,150, 200, 250],\n              'max_features': [0.3, 0.1, 0.05, 0.01] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n","31369b3c":"gsGBC.fit(X_train,y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","f96f5d63":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","e2b62e6e":"g = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,y_train,cv=kfold)","b2ab5367":"## Choose the best algorithms (ensembled mode)\nmodel = gsGBC\n\nids = df_test['PassengerId']\n\n##Result\nprediction = model.predict(df_test.drop('PassengerId', axis=1))\nout_pred = pd.DataFrame({ 'PassengerId' : ids, 'Survived': prediction })\n","2e4617b4":"df_sup = pd.DataFrame({\"PassengerId\":[1044], \"Survived\":[0]}) \nout_pred = out_pred.append(df_sup)\nout_pred = out_pred.sort_values(by='PassengerId', ascending=True)\n\nout_pred.to_csv('Submission.csv', index = False)","c22d685b":"## Machine Learning Data Analysis\n\nFor this problem we have to select the classification algorithms. The algorithms I have to compare will be:\n1. [Logistic Regression](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression)\n2. [K-nearest neighbors classifier](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm) \n3. [Support vector machine](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine)\n4. [Perceptron](https:\/\/en.wikipedia.org\/wiki\/Multilayer_perceptron)\n5. [Random Forest](https:\/\/en.wikipedia.org\/wiki\/Random_forest)\n6. [XGBoost Classifier](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)\n7. [Multi-Layer Perceptron](https:\/\/en.wikipedia.org\/wiki\/Multilayer_perceptron)\n\n### Split the column of surveillance of the rest of DataFrame","6ea95983":"### Drop no necessary columns for the data analysis","980dc914":"## Data Wrangling\n\nAdjust the data to be able to be processed.\n\n### Parse sex to numbers","f83bef40":"# Titanic Competition - Data Visualization & Machine Learning\n\n\n## Introduction\n**The most famous shipwreck in history**. Is there a pattern between the people who died and the people who survived?\n\n![Titanic Infographic](https:\/\/images.squarespace-cdn.com\/content\/v1\/5006453fe4b09ef2252ba068\/1351660113175-514SN9PXFWB9N2MNB8DV\/ke17ZwdGBToddI8pDm48kDJ9qXzk7iAyoryvGQVNA_F7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UcJGkCP2eEqMWqTLP4RLEZaxwnphfgit-_H6Nuosp50rm0sD-Bab7E9MY8W31A7zMQ\/TItanic-Survival-Infographic.jpg?format=1000w)\n\n### Imports\n\nImport the **data science** and **machine learning** libraries.","b1d2953a":"### Parse shipping data to numbers","0af5d8a6":"### Split a part of the train dataset for test the algorithms\nAdjust the test size to a 20%.","ac66f7cc":"#### Print the death by sex","a3aa44e0":"### Test the algoritms","f2d48779":"### Replace null age values for the average age","3ea6c3f6":"### Learning curves","a9f65eeb":"### Tunning the hyperparameters\nI choose the most responsive algorithm which is the **Gradient Descent** and we tune the hyperparameters.","525e44eb":"\n### From .csv to DataFrame\nRead the data and import to a DataFrame.","727be8eb":"### Explore the DataFrames\n\nBasic exploration of DataFrames: data, datatypes and data info.","67955b4f":"### Choose the best algorithm (the most accurate)\n\nExport the result to `.csv`. The result of these notebook is `0.77033`.","6af06f49":"### Correlation Heatmap","fdf2d755":"### Columns dissociation\nFor improve the precission of the algorithm the best is dissasociate specific columns. Columns to dissociate:\n- Age (Age Group)\n- Embarked\n- Pclass\n- SibSp\n- Parch","e23d0a04":"### Prediction","d18c4a4e":"### Drop the rows with a null value","2621ec2a":"### Segment the groups by age","945ab271":"In this case, the most accurate algorithm is the [random forest](https:\/\/en.wikipedia.org\/wiki\/Random_forest) and the multi-layer preceptron ([neural network](https:\/\/en.wikipedia.org\/wiki\/Artificial_neural_network)).\n\n### Cross Validation\nSelect the best model using [cross validation](https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics)).\nFrom this point on, notebook code is reused [Titanic Top 4% with ensemble modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling). \nI highly recommend to view this notebook.","a95801ad":"### Display death by embarked city","4f4207fb":"### Display death by age group"}}