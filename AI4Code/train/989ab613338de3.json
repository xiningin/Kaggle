{"cell_type":{"53cd5ae2":"code","7ffb51f1":"code","72ee012f":"code","0d282d9e":"code","2390bea8":"code","2501b23f":"code","233bf9ba":"code","b366cbae":"code","29f0f729":"code","fea2b650":"code","8d27e924":"code","bfe6346d":"code","ab1b434e":"code","20d7cd8e":"code","9e235537":"code","a4315c57":"code","1e568aac":"code","17b53ddb":"code","4c1709b8":"code","2b5f8475":"code","81012a3c":"code","17c1d577":"code","09e266bf":"code","d806f5e0":"code","efd5ac56":"code","b851b54e":"code","30a30e3f":"code","e7d649ee":"code","979959b4":"code","47b8ee0a":"code","02a11024":"code","7264e655":"code","ed5cac7c":"code","1677ce50":"code","6d214dd4":"code","55cf7337":"code","d5d3c1aa":"code","4404942a":"code","aa22f1bc":"code","ad39725e":"markdown","c965e620":"markdown","73b020a8":"markdown"},"source":{"53cd5ae2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ffb51f1":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm","72ee012f":"#Reading datasets\ndf = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntts = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\nttns = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntd = pd.read_csv('\/kaggle\/input\/lish-moa\/train_drug.csv')\nte_df = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\nsub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","0d282d9e":"df.shape","2390bea8":"df.head()","2501b23f":"#embedding cp_type, cp_time and cp_dose categorical columns of train dataset\ndf['cp_type'] = df['cp_type'].map({'trt_cp':0, 'ctl_vehicle':1})\ndf['cp_time'] = df['cp_time'].map({24:0, 48:1, 72:2})\ndf['cp_dose'] = df['cp_dose'].map({'D1':0, 'D2':1})","233bf9ba":"#embedding cp_type, cp_time and cp_dose categorical columns of test dataset\nte_df['cp_type'] = te_df['cp_type'].map({'trt_cp':0, 'ctl_vehicle':1})\nte_df['cp_time'] = te_df['cp_time'].map({24:0, 48:1, 72:2})\nte_df['cp_dose'] = te_df['cp_dose'].map({'D1':0, 'D2':1})","b366cbae":"#Seperating gene and cell columns\ngene_cols = [c for c in df.columns if c.startswith('g-')]\ncell_cols = [c for c in df.columns if c.startswith('c-')]","29f0f729":"#making copy of train_features dataset\ndf_cp = df.copy()\nte_df_cp = te_df.copy()","fea2b650":"#using QunatileTransformer to transform oue gene and cell columns\n#QunatileTransformer method transforms the features to follow a uniform or a normal distribution.\nfrom sklearn.preprocessing import QuantileTransformer","8d27e924":"qt = QuantileTransformer(n_quantiles=100, random_state=0)\nqt.fit(df_cp[gene_cols + cell_cols])","bfe6346d":"df_cp[gene_cols+cell_cols] = qt.transform(df_cp[gene_cols + cell_cols])\nte_df_cp[gene_cols+cell_cols] = qt.transform(te_df[gene_cols + cell_cols])","ab1b434e":"df_cp.drop('sig_id', axis=1, inplace=True)","20d7cd8e":"te_df_cp.drop('sig_id', axis=1, inplace=True)","9e235537":"from tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.models import load_model","a4315c57":"# AutoEncoder Model Preparation\nn_inputs = df_cp.shape[1]\n# define encoder\ninput_data_shape= Input(shape=(n_inputs,))\n# encoder level\nencoder= Dense(512, activation='relu')(input_data_shape)\nencoder= Dense(128, activation='relu')(encoder)\nencoder= Dense(64, activation='relu')(encoder)\nencoder= Dense(32, activation='relu')(encoder)\n# bottleneck\nn_bottleneck = 50\nbottleneck = Dense(n_bottleneck)(encoder)\n# define decoder\ndecoder = Dense(32, activation='relu')(bottleneck)\ndecoder = Dense(64, activation='relu')(decoder)\ndecoder = Dense(128, activation='relu')(decoder)\ndecoder = Dense(512, activation='relu')(decoder)","1e568aac":"# output layer\noutput = Dense(n_inputs, activation='linear')(decoder)\n# define autoencoder model\nmodel = Model(inputs=input_data_shape, outputs=output)\n# compile autoencoder model\nmodel.compile(optimizer='adam', loss='mse')","17b53ddb":"model.summary()","4c1709b8":"# fit the autoencoder model to reconstruct input\nhistory = model.fit(df_cp, df_cp, epochs=50, batch_size=16, verbose=2, validation_data=(te_df_cp,te_df_cp))","2b5f8475":"# define an encoder model (without the decoder)\nencoder = Model(inputs=input_data_shape, outputs=bottleneck)\n# save the encoder to file\nencoder.save('encoder.h5')","81012a3c":"# loading the encoder model\nencoder = load_model('encoder.h5')","17c1d577":"# encode the train data\nX_train_encode = encoder.predict(df_cp)\n# encode the test data\nX_test_encode = encoder.predict(te_df_cp)","09e266bf":"from xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d806f5e0":"num_moa_each_sample = np.sum(tts.drop('sig_id', axis=1), axis=1)","efd5ac56":"X_1 = pd.concat([df_cp, pd.DataFrame(X_train_encode)], axis=1)    #concatenating original train data with data left after autoencoding\ny_1 = num_moa_each_sample.map({1:1,2:1,3:1,4:1,5:1,7:1,0:0})    #if MoA is present then map it to 1 elso 0","b851b54e":"#final model for Step 1\nmodel_1 = SGDClassifier(loss='log')\nmodel_1.fit(X_1, y_1)","30a30e3f":"#removing first column i.e sig_id column from all datasets and storing in different variables\nX = pd.concat([df_cp, pd.DataFrame(X_train_encode)], axis=1)\ny = tts.iloc[:,1:]\ntest = pd.concat([te_df_cp, pd.DataFrame(X_test_encode)], axis=1)","e7d649ee":"# X = df_cp\n# y = tts.iloc[:,1:]\n# test = te_df_cp","979959b4":"print(X.shape, y.shape, test.shape)","47b8ee0a":"kf = KFold(n_splits=2, shuffle=True, random_state=22)","02a11024":"'''\nbest_model = None      #initializing best_model variable for storing best_model\nbest_loss = 99999999   #initializing best_loss variable to store least log-loss\ncv = 1                 #initializing cv variable to store number of cross validation iterating\n\nfor train_idx, test_idx in tqdm(kf.split(X, y)):     #iterating for each cv\n  X_train , X_val = X.iloc[train_idx], X.iloc[test_idx]\n  y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n  #training the model\n  print('FIT')\n  #model = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', max_iter=10000,tol=0.00001, eta0=0.002), n_jobs=-1)\n  #model = OneVsRestClassifier(GaussianNB(), n_jobs=-1)\n  model = OneVsRestClassifier(RandomForestClassifier(max_depth=2, n_estimators = 100))\n  model.fit(X_train, y_train)\n\n  #predicting target values for validation set and computing log-loss for each target features\n  print('PREDICT')\n  pred = model.predict_proba(X_val)\n  pred = np.array(pred)\n  \n  loss = log_loss(np.ravel(np.array(y_val)), np.ravel(pred))\n  print('Log loss for ',cv,' cv = ',loss)\n  \n  #saving best model and least log-loss\n  if loss < best_loss:\n      best_model = model\n      best_loss = loss\n  \n  cv += 1    #updating cv variable\n''' ","7264e655":"# #best_model = OneVsRestClassifier(RandomForestClassifier(max_depth=2, n_estimators = 100))\nbest_model = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', max_iter=10000,tol=0.00001, eta0=0.002), n_jobs=-1)\nbest_model.fit(X, y)","ed5cac7c":"#predicting target value for test dataset using first model (step 1 model) to find whether MoA is present or not\nmodel_1_pred = model_1.predict(test)#[:,1]","1677ce50":"#np.sum(model_1_pred<0.4)","6d214dd4":"#predicting target values for test dataset\ntest_pred = best_model.predict_proba(test)","55cf7337":"test_pred.shape","d5d3c1aa":"test_pred[2].max()","4404942a":"sub.iloc[:,1:] = test_pred\nsub.to_csv('submission.csv', index=False)","aa22f1bc":"sub","ad39725e":"**Combining step 1 and step 2**","c965e620":"**Step 2**","73b020a8":"**Step 1**"}}