{"cell_type":{"2f16ac73":"code","60ebfafb":"code","cfcd613f":"code","ec535d9e":"code","902d6619":"code","ec7a68b4":"code","882b72b5":"code","a19b8d31":"code","80c2070b":"code","6bd8b351":"code","d1a3508d":"code","672ddeae":"code","cf4f4de8":"code","b8d7f123":"code","1216e157":"code","a623c9d4":"code","f3b1b627":"code","cc7caeff":"code","a9a74d50":"code","be119480":"code","1da17ad1":"code","ba2aff65":"code","c2001c98":"code","6bcef502":"code","433f75ad":"code","7bcae4f5":"code","f02df15f":"code","84586e47":"code","43cb9d81":"code","a5a07fd0":"code","1fb09c02":"code","05c6a9fb":"code","c566a60f":"code","c5d79a90":"code","961ffd03":"code","582426fd":"code","6e20a5ca":"code","1c1d0b6a":"code","6bd5116c":"code","cab219a7":"code","8216ad09":"code","41e5990d":"code","6a476766":"code","7e997bd9":"code","0b1d1daf":"code","08a9db54":"code","2b32331d":"code","c858b22f":"code","b4737a8e":"markdown","ba1bbca2":"markdown","78894488":"markdown","d2df405b":"markdown","8fab49bc":"markdown","6c3cb274":"markdown","42464aaa":"markdown","8860c957":"markdown","00f7e8ff":"markdown","7d3ce160":"markdown","47b1f892":"markdown","fd8cfd2e":"markdown","65d1aea9":"markdown","d9413e62":"markdown","13d3ba09":"markdown","2f9fa33a":"markdown","1f810fd6":"markdown","1c3e703b":"markdown","eb8eb6ff":"markdown","c731464c":"markdown","b209753d":"markdown","0243b03d":"markdown","c8c5fc97":"markdown","fe4cf89b":"markdown","79566472":"markdown","d7be104f":"markdown","60aac954":"markdown","41ea7597":"markdown","4550ab3d":"markdown","98bcd1a4":"markdown","f249bd74":"markdown","ea540032":"markdown","97cc67e9":"markdown","1c2f66a2":"markdown","6141e9dc":"markdown","32a2a759":"markdown","561311ce":"markdown","400515fa":"markdown","2406aae2":"markdown"},"source":{"2f16ac73":"```{r read in the data, warning=FALSE, message=FALSE, results = 'hide'}\n# Load the data\nlibrary(readr)\ntitanic <- read_csv(\"train.csv\", col_types = \n                    cols(Embarked = col_factor(levels = c(\"S\", \"C\", \"Q\")), \n                         PassengerId = col_character(), \n                         Pclass = col_factor(levels = c(\"1\", \"2\", \"3\")), \n                         Sex = col_factor(levels = c(\"male\",\"female\")),\n                         Survived = col_factor(levels = c(\"0\", \"1\"))))\nsummary(titanic)\n```","60ebfafb":"```{r load tidyverse, warning=FALSE, results='hide', message=FALSE}\n# Let's start by exploring the data\nlibrary(tidyverse)\n```","cfcd613f":"```{r plot dependent}\np1 <- ggplot(titanic, aes(Survived, fill = Survived)) +\n  geom_bar(stat = 'count') +\n  labs(x = \"How many people survived?\") +\n  geom_label(stat = 'count', aes(label = ..count..), size = 7) +\n  theme_grey(base_size = 18)\np1\n```","ec535d9e":"```{r plot sex, warnings=FALSE}\n\n# Now let's explore sex\np2 <- ggplot(titanic, aes(x = Sex, fill = Survived)) +\n  geom_bar(stat = 'count', position = 'dodge') + theme_grey() +\n  labs(x = \"Is Sex Important?\") +\n  geom_label(stat = 'count', aes(label = ..count..))\np2\n```","902d6619":"```{r plot class,  warning=FALSE, results='hide'}\n# How about class?\np3 <- ggplot(titanic, aes(x = Pclass, fill = \"green\")) +\n  geom_bar(stat='count', position='dodge', show.legend = FALSE, fill = \"darkgreen\") +\n  labs(x = 'P Class') +\n  theme(legend.position=\"none\") + coord_flip()\np4 <- ggplot(titanic, aes(x = Pclass, fill = Survived)) +\n  geom_bar(stat='count', position='dodge', show.legend = FALSE) + labs(x = 'All data') +\n  theme(legend.position=\"none\") + theme_grey()+ coord_flip()\np5 <- ggplot(titanic, aes(x = Pclass, fill = Survived)) +\n  geom_bar(stat='count', position='stack', show.legend = FALSE) +\n  labs(x = 'Is Cabin class important?', y= \"Count\") + facet_grid(.~Sex) +\n  theme(legend.position=\"pclass\") + theme_grey()+ coord_flip()\np6 <- ggplot(titanic, aes(x = Pclass, fill = Survived)) +\n  geom_bar(stat='count', position='fill', show.legend = FALSE) +\n  labs(x = 'Is Cabin class important?', y= \"Percent\") + facet_grid(.~Sex) +\n  theme(legend.position=\"none\") + theme_grey()+ coord_flip()\n```","ec7a68b4":"```{r plot the grid, message=FALSE, warning=FALSE}\nlibrary(gridExtra)\ngrid.arrange(p3, p4, p5, p6, ncol=2)\n```","882b72b5":"```{r being weird, include=FALSE, results='hide'}\ntitanic$PclassSex[titanic$Pclass== 1  & titanic$Sex=='male'] <- 'P1Male'\n```","a19b8d31":"```{r combine sex class, warnings=FALSE}\n# We can combine class and sex\ntitanic$PclassSex[titanic$Pclass== 1  & titanic$Sex=='male'] <- 'P1Male'\ntitanic$PclassSex[titanic$Pclass== 2  & titanic$Sex=='male'] <- 'P2Male'\ntitanic$PclassSex[titanic$Pclass== 3  & titanic$Sex=='male'] <- 'P3Male'\ntitanic$PclassSex[titanic$Pclass== 1  & titanic$Sex=='female'] <- 'P1Female'\ntitanic$PclassSex[titanic$Pclass== 2  & titanic$Sex=='female'] <- 'P2Female'\ntitanic$PclassSex[titanic$Pclass== 3  & titanic$Sex=='female'] <- 'P3Female'\ntitanic$PclassSex <- as.factor(titanic$PclassSex)\n```\n","80c2070b":"\n```{r name split}\n# We can also do feature engineering, first taking the title and surname\ntitanic$Surname <- sapply(titanic$Name, function(x) {strsplit(x, split='[,.]')[[1]][1]})\n\n#correcting some surnames that also include a maiden name\ntitanic$Surname <- sapply(titanic$Surname, function(x) {strsplit(x, split='[-]')[[1]][1]})\n\n# Now to get the title of each person\ntitanic$Title <- sapply(titanic$Name, function(x) {strsplit(x, split='[,.]')[[1]][2]})\ntitanic$Title <- sub(' ', '', titanic$Title) #removing spaces before title\n\n# Now to reduce the number of titles\ntitanic$Title[titanic$Title %in% c(\"Mlle\", \"Ms\")] <- \"Miss\"\ntitanic$Title[titanic$Title== \"Mme\"] <- \"Mrs\"\ntitanic$Title[!(titanic$Title %in% c('Master', 'Miss', 'Mr', 'Mrs'))] <- \"Rare Title\"\ntitanic$Title <- as.factor(titanic$Title)\n```","6bd8b351":"\n```{r plot title}\n# And plot the result\np7 <- ggplot(titanic, aes(x = Title, fill = Survived)) +\n  geom_bar(stat='count', position='dodge') +\n  labs(x = 'Title') + theme_grey()\np7\n```","d1a3508d":"```{r family size}\n# A next step is to create a family size for each person\ntitanic$Fsize <- titanic$SibSp + titanic$Parch + 1\n\n# And plot\np8 <- ggplot(titanic, aes(x = Fsize, fill = Survived)) +\n  geom_bar(stat='count', position='dodge') +\n  scale_x_continuous(breaks=c(1:11)) +\n  labs(x = 'Family Size') + theme_grey()\np8\n```","672ddeae":"```{r skimmed package, warning=FALSE, results='hide', message=FALSE}\n# Perhaps if we summarise the data\nlibrary(skimr)\n```","cf4f4de8":"```{r skimmed}\nskimmed <- skim(titanic)\nskimmed\n```","b8d7f123":"```{r cabin}\n# Examine Canin variable\ntitanic$Cabin[is.na(titanic$Cabin)] <- \"U\"\ntitanic$Cabin <- substring(titanic$Cabin, 1, 1)\ntitanic$Cabin <- as.factor(titanic$Cabin)\n\n# We caan generate a plot\np9 <- ggplot(titanic[(!is.na(titanic$Survived)& titanic$Cabin!='U'),], \n             aes(x=Cabin, fill=Survived)) +\n  geom_bar(stat='count') + theme_grey() + facet_grid(.~Pclass) + \n  labs(title=\"Survivor split by class and Cabin\")\np9\n```","1216e157":"```{r find missing data}\n# Find NAs\nsapply(titanic, function(x) {sum(is.na(x))})\n```","a623c9d4":"```{r code for missingness}\n# Code age missingess\ntitanic$age_present <- ifelse(is.na(titanic$Age),\"No\", \"Yes\")\ntitanic$age_present <- as.factor(titanic$age_present)\n```","f3b1b627":"```{r remove unhelpful}\n# Remove the remaining unhelpful variables\ntitanic$Name <- NULL\ntitanic$Ticket <- NULL\ntitanic$Parch <- NULL\ntitanic$Surname <- NULL\ntitanic$Cabin <- NULL\ntitanic$PassengerId <- NULL\n```","cc7caeff":"```{r embarked factor}\n# And change embarked to a factor\ntitanic$Embarked <- as.factor(titanic$Embarked)\nskim_to_wide(titanic)\n```","a9a74d50":"```{r impute, results='hide', results='hide', message=FALSE}\n# Now lets attempt to impute the age, fare and embarked\nlibrary(mice)\ntempData <- mice(titanic,m=5,maxit=50,meth='pmm',seed=500)\n```","be119480":"```{r skimmed2}\ntitanic <- complete(tempData,1)\nskim_to_wide(titanic)\nrm(tempData)\n```","1da17ad1":"```{r formula define}\nform <- as.formula(survived ~ pclass + age + fare + Fsize + sibsp + Title +\n                     sex + PclassSex + embarked + \n                     age_present + cabin_present)\n```","ba2aff65":"```{r add controls, warning=FALSE, results='hide', message=FALSE}\n# And train control with \"normal\", \"down\" and \"smote\"\nlibrary(caret)\nlibrary(DMwR)\nctrl <- trainControl(method= \"repeatedcv\",\n                     number = 5,\n                     repeats= 5,\n                     summaryFunction = twoClassSummary,\n                     classProbs = TRUE)\nctrl_down <- trainControl(method= \"repeatedcv\",\n                          number = 5,\n                          repeats= 5,\n                          summaryFunction = twoClassSummary,\n                          classProbs = TRUE,\n                          sampling = \"down\")\nctrl_smote <- trainControl(method= \"repeatedcv\",\n                           number = 5,\n                           repeats= 5,\n                           summaryFunction = twoClassSummary,\n                           classProbs = TRUE,\n                           sampling = \"smote\")\n```","c2001c98":"```{r multicore, warning=FALSE, results='hide', eval=FALSE}\n# Configure multicore\nlibrary(doParallel)\ncl <- makeCluster(detectCores()-1)\nregisterDoParallel(cl)\n```","6bcef502":"```{r function for model training, warning=FALSE}\n# Define a function for training the models\ntrain_models <- function(method, ctrl, tuning) {\n  model.train <- train(form,\n                       data = titanic,\n                       method = method,\n                       trControl = ctrl,\n                       tuneGrid = tuning,\n                       maxit = 1000000,\n                       preProcess = c('range'))\n  return(model.train)\n}\n```","433f75ad":"```{r xgb, warning=FALSE, eval=FALSE}\n# 1. Extreme gradient boosting model\nset.seed(2018)\ntune.grid.xgb <- expand.grid(eta = c(0.05, 0.075, 0.1),\n                             nrounds = c(50, 75, 100),\n                             max_depth = 6:8,\n                             min_child_weight = c(2.0, 2.25, 2.5),\n                             colsample_bytree = c(0.3, 0.4, 0.5),\n                             gamma = 0,\n                             subsample = 1)\n# Normal non-sampled\nmodel_xgb <- train_models(\"xgbTree\", ctrl, tune.grid.xgb)\n# Down Sampled\nmodel_xgb_down <- train_models(\"xgbTree\", ctrl_down, tune.grid.xgb)\n# Smote Sampled\nmodel_xgb_smote <- train_models(\"xgbTree\", ctrl_smote, tune.grid.xgb)\n```","7bcae4f5":"```{r random forest, warning=FALSE, eval=FALSE}\n# 2. Random Forest\nset.seed(2018)\ntune.grid.rf <- expand.grid(.mtry = sqrt(ncol(titanic)))\n# Normal non-sampled\nmodel_rf <- train_models(\"rf\", ctrl, tune.grid.rf)\n# Down Sampled\nmodel_rf_down <- train_models(\"rf\", ctrl_down, tune.grid.rf)\n# Smote Sampled\nmodel_rf_smote <- train_models(\"rf\", ctrl_smote, tune.grid.rf)\n```","f02df15f":"```{r Nnet, warning=FALSE, results='hide', eval=FALSE}\n# 3. Model Averaged Neural Network\nset.seed(2018)\nlibrary(nnet)\ntune.grid.nn <- expand.grid(size=c(10), decay=c(0.1))\n# Normal non-sampled\nmodel_net <- train_models(\"nnet\", ctrl, tune.grid.nn)\n# Down Sampled\nmodel_net_down <- train_models(\"nnet\", ctrl_down, tune.grid.nn)\n# Smote Sampled\nmodel_net_smote <- train_models(\"nnet\", ctrl_smote, tune.grid.nn)\n```","84586e47":"```{r glm, warning=FALSE, eval=FALSE}\n# 4. Logistic Regression\nset.seed(2018)\nlamda.grid <- 10^seq(2, -2, length = 100)\nalpha.grid <- seq(0,1, length = 10)\nsrchGrd <- expand.grid(.alpha = alpha.grid,\n                       .lambda = lamda.grid)\n# Normal non-sampled\nmodel_glm <- train_models(\"glmnet\", ctrl, srchGrd)\n# Down Sampled\nmodel_glm_down <- train_models(\"glmnet\", ctrl_down, srchGrd)\n# Smote Sampled\nmodel_glm_smote <- train_models(\"glmnet\", ctrl_smote, s","43cb9d81":"```{r load models, echo = FALSE}\nmodel_xgb <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_xgb.rds\")\nmodel_xgb_down <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_xgb.rds\")\nmodel_xgb_smote <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_xgb_smote.rds\")\n\nmodel_rf <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_rf.rds\")\nmodel_rf_down <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_rf.rds\")\nmodel_rf_smote <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_rf_smote.rds\")\n\nmodel_net <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_net.rds\")\nmodel_net_down <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_net.rds\")\nmodel_net_smote <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_net_smote.rds\")\n\nmodel_glm <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_glm.rds\")\nmodel_glm_down <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_glm.rds\")\nmodel_glm_smote <- read_rds(\"C:\/Users\/david\/Documents\/Work\/Project 2\/R\/Machine Learning\/Titanic\/Titanic\/model_glm_smote.rds\")\n\n```","a5a07fd0":"```{r group the models}\n# Group the models\nmodels_compare <- resamples(list(RF = model_rf,\n                                 XGB = model_xgb,\n                                 ENet = model_glm,\n                                 NNet = model_net))\nmodels_compare_down <- resamples(list(RF = model_rf_down,\n                                      XGB = model_xgb_down,\n                                      ENet = model_glm_down,\n                                      NNet = model_net_down))\nmodels_compare_smote <- resamples(list(RF = model_rf_smote,\n                                       XGB = model_xgb_smote,\n                                       ENet = model_glm_smote,\n                                       NNet = model_net_smote))\n# Create the scales for the plotting\nscales <- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\n```","1fb09c02":"\n```{r dotplots}\n# Dot plot each one so we can compare the models smote, down and normal sampling\ndotplot <- dotplot(models_compare, scales = scales)\ndotplot_down <- dotplot(models_compare_down, scales=scales)\ndotplot_smote <- dotplot(models_compare_smote, scales=scales)\ndotplot\ndotplot_down\ndotplot_smote\n```","05c6a9fb":"```{r boxplot}\n# Box plot\nboxplot <- bwplot(models_compare, scales=scales)\nboxplot\n```","c566a60f":"\n```{r denplot}\n# Density Plot\ndenplot <- densityplot(models_compare, scales=scales, pch = \"|\")\ndenplot\n```","c5d79a90":"```{r dotplot}\n# Dotplot\ndotplot <- dotplot(models_compare, scales=scales)\ndotplot\n```","961ffd03":"```{r paraplot}\n# Parallel plot\nparaplot <- parallelplot(models_compare)\nparaplot\n```","582426fd":"```{r scatter}\n# Scatterplot\nscatter <- splom(models_compare)\nscatter\n```","6e20a5ca":"```{r plot function}\n# Define Function\nplot_imp <- function(model, title) {\n  FI_mod <- varImp(model) # Takes variable Importance\n  FI_dat <- FI_mod$importance # Puts into data frame\n  FI_gini <- data.frame(Variables = row.names(FI_dat),\n                        MeanDecreaseGini = FI_dat$Overall) # Renames\n  # Plots: \n  FI_plot <- ggplot(FI_gini, aes(x=reorder(Variables, MeanDecreaseGini),\n                               y=MeanDecreaseGini, fill= \"green\")) +\n    geom_bar(stat='identity') + coord_flip() +\n    theme(legend.position=\"none\") + labs(x=\"\") +\n    ggtitle(title) + theme(plot.title = element_text(hjust = 0.5))\nreturn(FI_plot)\n}","1c1d0b6a":"```{r run function}\n# Now to plot the feature importance of the best group of models\nxgb_imp_plot <- plot_imp(model_xgb, \"xGB Feature Importance\")\nglm_imp_plot <- plot_imp(model_glm, \"GLM Feature Importance\")\nrf_imp_plot <- plot_imp(model_rf, \"RF Feature Importance\")\nnet_imp_plot <- plot_imp(model_net, \"NNet Feature Importance\")\n```","6bd5116c":"```{r compare models plot, warning=FALSE, message=FALSE}\n# It seems 4 of the models are easily comparable:\nlibrary(gridExtra)\nplot_imp <- grid.arrange(xgb_imp_plot, rf_imp_plot,\n                                 net_imp_plot, glm_imp_plot, ncol=2)\nplot_imp\n```","cab219a7":"Now I will run the function for each model, as well as changing the rank variable name for clarity:","8216ad09":"```{r run rank features}\nrank_xgb <- rank_features(model_xgb)\nrank_xgb <- rename(rank_xgb, xgb = rank)\n\nrank_glm <- rank_features(model_glm)\nrank_glm <- rename(rank_glm, glm = rank)\n\nrank_rf <- rank_features(model_rf)\nrank_rf <- rename(rank_rf, rf = rank)\n\nrank_net <- rank_features(model_net)\nrank_net <- rename(rank_net, net = rank)\n```\n","41e5990d":"```{r join the data}\n# Now, we can join these data sets\nlist_dfs <- list(rank_xgb, rank_glm, rank_net, rank_rf)\nranks <- Reduce(function(...) merge(..., all=T), list_dfs)\n\n# And create an average for each variable\nranks <- mutate(ranks, average = (xgb + glm + rf + net)\/4) %>%\n  arrange(average) %>%\n  mutate(rank.average = 1:nrow(ranks))\n```","6a476766":"```{r plot average rank}\n# We can plot the average rank\nplot_rank <- ggplot(ranks, aes(x = reorder(Variables, -average), y = average,\n                          fill = Variables)) + geom_col() +\n  coord_flip()\nplot_rank\n```","7e997bd9":"```{r test set code, warning=FALSE, results='hide', message = FALSE}\n# Predict\nlibrary(readr)\ntest <- read_csv(\"test.csv\", col_types = \n                cols(Embarked = col_factor(levels = c(\"S\", \"C\", \"Q\")), \n                PassengerId = col_character(), \n                Pclass = col_factor(levels = c(\"1\", \"2\", \"3\")), \n                Sex = col_factor(levels = c(\"male\",\"female\"))))\n\ntest$Sex <- as.factor(test$Sex)\ntest$PclassSex[test$Pclass== 1  & test$Sex=='male'] <- 'P1Male'\ntest$PclassSex[test$Pclass== 2  & test$Sex=='male'] <- 'P2Male'\ntest$PclassSex[test$Pclass== 3  & test$Sex=='male'] <- 'P3Male'\ntest$PclassSex[test$Pclass== 1  & test$Sex=='female'] <- 'P1Female'\ntest$PclassSex[test$Pclass== 2  & test$Sex=='female'] <- 'P2Female'\ntest$PclassSex[test$Pclass== 3  & test$Sex=='female'] <- 'P3Female'\ntest$PclassSex <- as.factor(test$PclassSex)\ntest$Surname <- sapply(test$Name, function(x) {strsplit(x, split='[,.]')[[1]][1]})\ntest$Surname <- sapply(test$Surname, function(x) {strsplit(x, split='[-]')[[1]][1]})\ntest$Title <- sapply(test$Name, function(x) {strsplit(x, split='[,.]')[[1]][2]})\ntest$Title <- sub(' ', '', test$Title) #removing spaces before title\ntest$Title[test$Title %in% c(\"Mlle\", \"Ms\")] <- \"Miss\"\ntest$Title[test$Title== \"Mme\"] <- \"Mrs\"\ntest$Title[!(test$Title %in% c('Master', 'Miss', 'Mr', 'Mrs'))] <- \"Rare Title\"\ntest$Title <- as.factor(test$Title)\ntest$Fsize <- test$SibSp + test$Parch + 1\ntest$Cabin[is.na(titanic$Cabin)] <- \"U\"\ntest$Cabin <- substring(test$Cabin, 1, 1)\ntest$Cabin <- as.factor(test$Cabin)\ntest$age_present <- ifelse(is.na(test$Age),\"No\", \"Yes\")\ntest$age_present <- as.factor(test$age_present)\ntest$Name <- NULL\ntest$Ticket <- NULL\ntest$Parch <- NULL\ntest$Surname <- NULL\ntest$Cabin <- NULL\ntempData <- mice(test,m=5,maxit=50,meth='pmm',seed=500)\ntest <- complete(tempData,1)\n```","0b1d1daf":"\n```{r hme dest, include=FALSE, results='hide'}\ntest$home.dest_present <- ifelse(is.na(test$Embarked), \"No\", \"Yes\")\ntest$Pclass <- as.numeric(test$Pclass)\n```\n","08a9db54":"```{r test final frame, echo=FALSE}\ntest <- test %>% rename(pclass = Pclass, sex = Sex, age = Age,\n                        sibsp = SibSp, fare = Fare, embarked = Embarked)\n```","2b32331d":"```{r make predicitons}\n# Make Predictions\nprediction_frame <- data.frame(matrix(nrow=418, ncol=0))\nprediction_frame <- as.data.frame(predict(model_xgb, test))\nprediction_frame$xgb_down <- predict(model_xgb_down, test)\nprediction_frame$xgb_smote <- predict(model_xgb_smote, test)\n\nprediction_frame$rf <- predict(model_rf, test)\nprediction_frame$rf_down <- predict(model_rf_down, test)\nprediction_frame$rf_smote <- predict(model_rf_smote, test)\n\nprediction_frame$net <- predict(model_net, test)\nprediction_frame$net_down <- predict(model_net_down, test)\nprediction_frame$net_smote <- predict(model_net_smote, test)\n\nprediction_frame$glm <- predict(model_glm, test)\nprediction_frame$glm_down <- predict(model_glm_down, test)\nprediction_frame$glm_smote <- predict(model_glm_smote, test)\n\nprediction_frame$final <- \n  apply(prediction_frame,1,function(x) names(which.max(table(x))))\n\n\nfinal_prediction <- prediction_frame %>% select(final)\npassenger_ids <- test %>% select(PassengerId)\nfinal_prediction <- cbind(passenger_ids, final_prediction)\n\nfinal_prediction$final <- ifelse(final_prediction$final == \"Survived\",1,0)\n\n```","c858b22f":"```{r save csv, eval=FALSE}\nwrite.csv(final_prediction, \"final_prediction.csv\")\n```","b4737a8e":"It appears that sex is important. Women and children were prioritised and it is these people who were more liekly to survive. We can also examine the class of passenger (ticket) and its impact on survival. I will create four plots and put them together with *gridExtra*.","ba1bbca2":"For the remaining data I will impute it using the *mice* package. ","78894488":"##Feature Engineering\n\nIt is also helpful to do some feature engineering before plotting the models in order to get more out of the data. The title of the person from name might be important, since women and children were prioritised. I will first take out the surname, then the title, then reduce the number of titles by combining ones we might consider similar.","d2df405b":"Logistic Regression with elastic net regularisation\n","8fab49bc":"We can also create and plot the family size. We do this by adding the number of siblings, parents, children and the person themself. \n","6c3cb274":"We can summarise the data using the package *skimr*:","42464aaa":"It seems that cabin is unimportant for predicting survival. There is a 50:50 split for each deck. I will therefore ignore this variable.\n\n## Dealing with Missing Data\n\nWe can find the missing data like this:","8860c957":"It seems that the resampling doesn't make much difference. This is unsuprising given that the resampling is for very large imbalances in the data. I will therefore do the following analysis only on the unsampled models. Firstly, I look at 5 plots of the models: ","00f7e8ff":"```{r rank features function}\nrank_features <- function(model) {\n  FI_mod <- varImp(model)\n  FI_df <- FI_mod$importance\n  FI_df <- data.frame(Variables = row.names(FI_df),\n                      overall = FI_df$Overall)\n  FI_df <- arrange(FI_df, desc(overall)) %>%\n    mutate(rank = 1:nrow(FI_df))\n  FI_df$overall <- NULL \n  \n  return(FI_df)\n}\n```","7d3ce160":"Now plot the importance:","47b1f892":"I will configure multicore to allow extra cores to be used. This is possible because each of the cross validated models is analysed separately.\n","fd8cfd2e":"Now we can remove the variables we don't need because they are are useless of unusable:","65d1aea9":"##Data Exploration\n\nNow I will begin to explore the data starting with the dependent survived variable. I will use ggplot2 from the tidyverse to explore the data.","d9413e62":"I will first change the survived variable to make it clear which is which, and convert to factor.\n","13d3ba09":"## Model Setup \n\nI am now ready to run the models. I will start by defining a formula for the models:","2f9fa33a":"The random forest:","1f810fd6":"##Training the Models\n\nEach model has its own tuning defined by me. The algorithm selects the best model based on optimal parameters. I will train 3 models for each algorithm, and evaluate them together at the end. Firstly, the extreme gradient boosting algorithm:\n","1c3e703b":"The next step is to define the controls for each model. From p1 it\u2019s clear there is an imbalance in the data. There are several ways to deal with this. The models will be built using the Caret package so I will focus on Caret's built in methods. Briefly, they are downsampling the major class, upsampling the minor class, or using a hybrid of the 2 (SMOTE and ROSE). I will use the normal data, as well as 'down' and 'smote' methods for dealing with the imbalance. \n\nI will define train control with normal, down and smote, and assess which method is best. These algorithms are implemented within the repeated cross validation. Repeated cross validation avoids the need for manually splitting a training and test set. It divides the data into k folds, holding 1 set for testing and the others for training. The resampling using \u201cup\u201d or \u201csmote\u201d is done to the training data only. This is repeated and the result is an average. This avoids over fitting. \n\n\nI will repeat the 5 fold cross validation 5 times. This is standard. I will focus on the ROC curve with the two class summary.\n","eb8eb6ff":"##Evaluating Model Performance\n\nThe next step is to evaluate how the models perform. I will group the models together and then use a simple dot plot with 95% confidence intervals to examine the result. \n","c731464c":"For age, I'll code for the missingness before imputation.","b209753d":"Next, I will display a dotplot for each set of models:","0243b03d":"The models are achieving between 85 and 87% accuracy, which is reasonably impressive given we would expect the data to feature some randomness. We would expect many of the people who survived to have ended up on the lifeboat by chance. The scatterplot shows that ensembling the models may not be suitable because there is a high correlation across each of them. The parallel plot is colourful but not particularly helpful, only for showing that the xgb is consistently the best performiong model.\n\n##Feature Importance\n\nFinally, I will do some feature importance, which is to look which factors were most important in the models using the gini impurity. More details can be found [here](https:\/\/people.cs.pitt.edu\/~milos\/courses\/cs2750-Spring03\/lectures\/class19.pdf). Firstly, I will define a function which takes each model and its importance, then plots them.\n","c8c5fc97":"I will generate a plot and add a label, changing the theme:","fe4cf89b":"Age is the most important. Younger people were more likely to survive. Title3 \"Mister\" is important, seemingly they were more liekly to die. Family size is important, fare paid (related to pclass), sex is important, as is the dummy for whether the class was present. A logical next step would be to re run the models with a succint set of predictors.\n\n##Test Set\nFinally, I want to run the same code on the test set. It is perhaps easier to combine the 2 data sets at the start, but I think this is annoying for plotting. I'm just going to run the code for the test set:","79566472":"Now I can define a function which will train the models. The maxit isn't relevant for every model but will work for all of them. Each model has it's own tuning parameters which I will define individually. The data is preprocessed using range, which puts all the variables between 0 and 1. This makes the variables comparable with the dummy variables, which will be generated automatically by the models for every variable which is a factor. ","d7be104f":"A neural network:","60aac954":"Finally, I will plot the average rank. There are clearly some predictors which do very well.","41ea7597":"I can use the function *grid.arrange to display 4 functions at once:","4550ab3d":"Next, I will join the data sets using *reduce*, and add variables for average and then rank the average, using *dplyr* from the tidyverse.","98bcd1a4":"It appears that class is important. People in first class were less likely to die. For that reason I will combine these 2 variables into another separate variable (it seems common to do this on other kernels)","f249bd74":"Finally, save the data as a csv:","ea540032":"For the cabin variable, we there is rather a lot to imupte but we could change all the missing observations to 'u' and then extract the first letter from the remaining observations.The first letter represents the deck so might be important for survival if lifeboats were all on the same deck. We can then plot the result","97cc67e9":"Finally, use all the 12 models and generate an ensemble predicion based on whichever is highest. ","1c2f66a2":"Now it could be helpful to assign the variables a rank from 1 to 20 and then see which is important across the model. To do this, I start by defining a function which takes each model, gets its feature importance and ranks them leaving just the variable and rank in each data frame.","6141e9dc":"After the data is split we can again use *ggplot* to look at the number in each title, and whether they survived.","32a2a759":"There is a slight imbalance in the classes in that more people died than survived. I will address this later. Perhaps if we look at sex a a predictor. ","561311ce":"Now we run the function for each of the models:","400515fa":"##Introduction\n\nThis is an attempt to use machine learning on the Titanic dataset, which classifies passenegers on the titanic by whether they died or survived. The script uses ensemble modelling and currently has a score in the top 3%.","2406aae2":"And change embarked also to a factor. We can then view a summary of the data again using skimmed."}}