{"cell_type":{"2e6cc4d0":"code","66717471":"code","f2f208cf":"code","3bcf66c6":"code","bcdb6642":"code","e69b51c0":"code","f8123c68":"code","0cf547c2":"code","4a6aca68":"code","43f85805":"code","14b535cd":"code","e9ad15f2":"code","2fa7ef88":"code","4a54d3a1":"code","499eea81":"code","26f186a7":"code","355f8574":"code","a963c47f":"code","b9ce2c9f":"code","ccb4d71a":"code","85166874":"code","d7844090":"code","b9ed1967":"code","cbc12580":"code","d567e64e":"code","776c66a6":"code","642c281b":"code","3d9c16fd":"code","dc336861":"code","34242cec":"code","04eb1aaa":"markdown","99d81202":"markdown","71ad0e7c":"markdown","de7957ef":"markdown","caff4b10":"markdown","0edefe01":"markdown","acb99703":"markdown","7e127a3a":"markdown","00a8b47a":"markdown","4855e55a":"markdown","911497f9":"markdown","ceb05f76":"markdown","641670f0":"markdown","636ac405":"markdown","ede53d4f":"markdown","a2fe434d":"markdown","031baa00":"markdown","27b395ff":"markdown","6652c218":"markdown","309a7583":"markdown","49fb97cb":"markdown"},"source":{"2e6cc4d0":"reset -fs","66717471":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nfrom sklearn.metrics import *\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.base import BaseEstimator\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport matplotlib.pyplot as plt ","f2f208cf":"df = pd.read_csv('fixed_cocaine_listings.csv')","3bcf66c6":"df.head()","bcdb6642":"# Split predictors & response variables, \n# do log transformation on grams, \n# and convert all True-False to 1s and 0s.\n\ny = df.btc_price\ndf_b = df.drop(\"btc_price\", axis=1)\ndf_b = df_b.drop([\"cost_per_gram_pure\", \"Unnamed: 0\", \"product_title\", \"ships_from_to\", \"cost_per_gram\",\n                    \"product_link\", \"vendor_link\", \"vendor_name\", \"ships_from\", \"ships_to\", \n                    \"ships_to_GR\", \"ships_from_GR\", \"ships_to_PL\", \"ships_from_PL\", \"ships_to_CO\", \n                    \"ships_from_CO\", \"ships_to_SE\", \"ships_from_SE\", \"ships_to_S. America\", \n                    \"ships_from_S. America\", \"ships_from_DK\", \"ships_to_DK\", \"ships_to_CN\", \"ships_to_CZ\", \n                    \"ships_to_BR\", \"ships_to_IT\", \"ships_from_SI\", \"ships_to_BE\", \"ships_from_N. America\", \n                    \"ships_to_ES\", \"ships_to_CH\", \"ships_from_CH\", \"ships_from_CZ\", \"ships_from_CN\", \n                    \"ships_to_WW\", \"ships_to_DE\"], axis=1)\n\ndf_b = df_b * 1\ndf_b[\"grams\"] = np.log(df_b[\"grams\"])\ny = np.log(y)\n\ndf_b.head()","e69b51c0":"# Extract key words from product title for feature engineering\n\nmy_regex = {\"intro\": \"intro|sample\",\n           \"columbia\": \"columbia\",\n           \"peru\": \"peru\",\n           \"bolivia\": \"bolivia\",\n           \"free_ship\": \"freeship\",\n           \"uncut\": \"uncut\",\n           \"fishcut\": \"fish\",\n           \"brick\": \"brick\",\n           \"crack\": \"crack\",\n           \"crystal\": \"crystal\",\n           \"flake\": \"flake\",\n           \"pure\": \"pure\"}\n\nfor my_key in my_regex.keys():\n    df_b.loc[df.product_title.str.replace(\"\\s\", \"\")\\\n                                .str.lower()\n                                .str.contains(my_regex[my_key]), my_key] = 1\n    df_b.loc[~df.product_title.str.replace(\"\\s\", \"\")\\\n                                .str.lower()\n                                .str.contains(my_regex[my_key]), my_key] = 0\n\n\ndf_b[\"caps\"] = df.product_title.str.findall(r'[A-Z]').str.len()\/df.product_title.str.len()\n\ndf_b.head()","f8123c68":"data = pd.read_csv('data_keywords.csv')","0cf547c2":"df['ship_from'] = df.ships_from_to.str[:2]\ndf.head()\nship_from = df.groupby('ship_from').agg(\n    {'btc_price': ['mean', 'count']}).reset_index()\nship_from = pd.concat([ship_from['ship_from'], ship_from['btc_price']], axis=1)\nship_from.loc[ship_from['count'] < 5, 'count_less_than_5'] = 'Yes'\nship_from.loc[ship_from['count'] >= 5, 'count_less_than_5'] = 'No'\nship_from = ship_from.sort_values('mean', ascending=False)\nship_from","4a6aca68":"colors = {'Yes': 'coral', 'No': 'dodgerblue'}\nship_from_plt = ship_from.rename(columns={'mean': 'count_is_less_than_5'})\nax = ship_from_plt.plot(x='ship_from', y='count_is_less_than_5', kind='bar',\n                        color=ship_from_plt['count_less_than_5'].apply(lambda x: colors[x]), figsize=(10, 5), width=0.8)\nax.set_xlabel('Ship From Which Country')\nax.set_ylabel('mean of price')\nax.set_title('Mean Cocaine Price for each Country Vender')\nplt.show()","43f85805":"col_lst = list(data.columns[-13:])\ncol_lst.append('escrow')\ncol_lst.remove('caps')\ncol_lst","14b535cd":"def plot_avg_price(col_name):\n    new_dataset = data.groupby(col_name).mean()['btc_price'].reset_index()\n    new_dataset.plot(x=col_name, y='btc_price', kind='bar', legend=False, figsize=(2,5))\n    plt.show()","e9ad15f2":"def plot_graphs(col_lst):\n    fig, axs = plt.subplots(3, 5, figsize=(20, 20))\n    i = 0\n    for col in col_lst:\n        row_index = int(i \/ 5)\n        col_index = i % 5\n        new_dataset = data.groupby(col).mean()['btc_price'].reset_index()\n        axs[row_index, col_index].bar(\n            new_dataset[col], new_dataset['btc_price'])\n        axs[row_index, col_index].set_title(col)\n        axs[row_index, col_index].set_title(col)\n        axs[row_index, col_index].set_ylabel('btc_price')\n        i += 1\nplot_graphs(col_lst)","2fa7ef88":"# Train test split\nX_train, X_test, y_train, y_test = train_test_split(df_b, y, test_size=0.2,shuffle = True)","4a54d3a1":"print(\"Cross Validation Score\")\npipelines = [LinearRegression(),\n             Lasso(),\n             Ridge(),\n             RandomForestRegressor(criterion='mae')]\n\nfor pipe in pipelines:\n    pipe.fit(X_train,y_train)\n    name = pipe.__class__.__name__.split('.')[-1]\n    cv_medae = cross_val_score(pipe, X_train,y_train, scoring = 'neg_median_absolute_error', cv =5)\n    cv_score = cross_val_score(pipe, X_train,y_train, scoring = 'r2', cv =5)\n    print(f\"{name}\")\n    print(f\"Average cross validation R^2: {cv_score.mean():.4}\")\n    print(f\"{cv_score}\")\n    print(f\"Average cross validation Medae: {cv_medae.mean():.4}\")\n    print(f\"{cv_medae}\",end = \"\\n\\n\")","499eea81":"cv = 5\nn_iter = 20","26f186a7":"hyperparameters = dict(n_estimators=range(10, 200),\n                       max_depth=range(3, 12))\nrf_random = RandomizedSearchCV(RandomForestRegressor(\n    criterion='mae', random_state=42), hyperparameters, cv=cv, n_iter=n_iter)","355f8574":"hyperparameters = dict(n_estimators=range(10, 200),\n                       max_depth=range(3, 12))\nrf_random = RandomizedSearchCV(RandomForestRegressor(\n    criterion='mae', random_state=42), hyperparameters,n_iter = n_iter)","a963c47f":"rf_random.fit(X_train, y_train)","b9ce2c9f":"cross_val_score(rf_random, X_train, y_train)","ccb4d71a":"y_test = np.exp(y_test)\ny_train = np.exp(y_train)\nX_test[\"grams\"] = np.exp(X_test[\"grams\"])\nX_train[\"grams\"] = np.exp(X_train[\"grams\"])","85166874":"for pipe in pipelines:\n    name = pipe.__class__.__name__\n    pred_test = pipe.predict(X_test)\n    medae_value = median_absolute_error(y_test, pred_test)\n    print(f\"{medae_value:.4f} medae on {name} test set\")\n    mse_value = mean_squared_error(y_test, pred_test)\n    rmse_value = np.sqrt(mse_value)\n    print(f\"{rmse_value:.4f} mse on {name} test set\")","d7844090":"lm = LinearRegression() \nlm.fit(X_train, y_train) \npred_train = np.exp(lm.predict(X_train))\npred_test = np.exp(lm.predict(X_test))\nmedae_value = median_absolute_error(y_train, pred_train)\nprint(f\"{medae_value:.4f} medae on training set\")\nmedae_value = median_absolute_error(y_test, pred_test)\nprint(f\"{medae_value:.4f} medae on test set\")","b9ed1967":"l1 = Lasso() \nl1.fit(X_train, y_train) \npred_train = np.exp(l1.predict(X_train))\npred_test = np.exp(l1.predict(X_test))\nmedae_value = median_absolute_error(y_train, pred_train)\nprint(f\"{medae_value:.4f} medae on training set\")\nmedae_value = median_absolute_error(y_test, pred_test)\nprint(f\"{medae_value:.4f} medae on test set\")","cbc12580":"l2 = Ridge()\nl2.fit(X_train, y_train) \npred_train = np.exp(l2.predict(X_train))\npred_test = np.exp(l2.predict(X_test))\nmedae_value = median_absolute_error(y_train, pred_train)\nprint(f\"{medae_value:.4f} medae on training set\")\nmedae_value = median_absolute_error(y_test, pred_test)\nprint(f\"{medae_value:.4f} medae on test set\")","d567e64e":"rf = RandomForestRegressor(criterion='mae',n_estimators = 150,max_depth = 5 )\nrf.fit(X_train, y_train) \npred_train = np.exp(rf.predict(X_train))\npred_test = np.exp(rf.predict(X_test))\nmedae_value = median_absolute_error(y_train, pred_train)\nprint(f\"{medae_value:.4f} medae on training set\")\nmedae_value = median_absolute_error(y_test, pred_test)\nprint(f\"{medae_value:.4f} medae on test set\")","776c66a6":"lm = LinearRegression()\nlm.fit(X_train, y_train)","642c281b":"# Interpreting coefficients\nls = []\nls1 = []\nfor x,y in zip(list(df_b.columns), lm.coef_):\n    if abs(y) < 0.05:\n        print(x,y)\n        ls.append(x)\n        ls1.append(y)\nfig, ax = plt.subplots(figsize=(20,5))\nplt.bar(ls,ls1)\nplt.show()","3d9c16fd":"# Interpret coefficients\nls = []\nls1 = []\nfor x,y in zip(list(df_b.columns), lm.coef_):\n    if abs(y) >= 0.5:\n        print(x,y)\n        ls.append(x)\n        ls1.append(y)\nfig, ax = plt.subplots(figsize=(20,5))\nplt.bar(ls,ls1)\nplt.show()","dc336861":"plt.scatter(y_train, X_train[\"grams\"])\nplt.show()","34242cec":"plt.scatter(y_test, X_test[\"grams\"])\nplt.show()","04eb1aaa":"As can be seen above, our Random Forest Regressor Model performed the best with the lowest MEDAE on our test set.  Initially our Linear Regression Model with the additional feature engineering done on it performed the best.  That model's interpretation is given below. ","99d81202":"Student Names\n----","71ad0e7c":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Group-Name\" data-toc-modified-id=\"Group-Name-1\">Group Name<\/a><\/span><\/li><li><span><a href=\"#Student-Names\" data-toc-modified-id=\"Student-Names-2\">Student Names<\/a><\/span><\/li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-3\">Load Data<\/a><\/span><\/li><li><span><a href=\"#Data-Cleaning-and-Feature-Engineering\" data-toc-modified-id=\"Data-Cleaning-and-Feature-Engineering-4\">Data Cleaning and Feature Engineering<\/a><\/span><\/li><li><span><a href=\"#Initial-fit-for-linear-regression,-l1,-l2-and-random-forest-and-cross-validation\" data-toc-modified-id=\"Initial-fit-for-linear-regression,-l1,-l2-and-random-forest-and-cross-validation-5\">Initial fit for linear regression, l1, l2 and random forest and cross validation<\/a><\/span><\/li><li><span><a href=\"#Random-Forest-Random-Search-for-Hyperparameter-Tuning\" data-toc-modified-id=\"Random-Forest-Random-Search-for-Hyperparameter-Tuning-6\">Random Forest Random Search for Hyperparameter Tuning<\/a><\/span><\/li><li><span><a href=\"#Evaluation-Metrics\" data-toc-modified-id=\"Evaluation-Metrics-7\">Evaluation Metrics<\/a><\/span><\/li><li><span><a href=\"#Interpretation\" data-toc-modified-id=\"Interpretation-8\">Interpretation<\/a><\/span><\/li><li><span><a href=\"#Visualization\" data-toc-modified-id=\"Visualization-9\">Visualization<\/a><\/span><\/li><\/ul><\/div>","de7957ef":" Random_Forest_Elves\n","caff4b10":"# Predicting the Price of Dark Market Cocaine","0edefe01":"Data Visualization\n----","acb99703":"Data Cleaning and Feature Engineering\n-----","7e127a3a":"# Conclusion & Summary of Results","00a8b47a":"Model Comparison & Evaluation Metrics\n----","4855e55a":"In this project we used various machine learning models to predict the btc price of dark market cocaine.  Our dataset was composed of approximately 1,400 cleaned and standardized product listings from Dream Market's \"Cocaine\" category.\n\nInitial exploratory data analysis revealed that we could split up our *shipped from-to* column into their individual components from which we then one-hot encoded into their respective categorical variables.  Further feature engineering via string extraction and manipulation was done on our *product title* column as we noticed that not all amounts in the *price*  column were scraped correctly and that there were some mistakes in encoding price schemes resulting from commas and decimals not being uniform in their use across countries.\n\nWe then trained a variety of machine learning models ranging from simple linear regression, linear regression with added feature engineering, L1, L2, and a Random Forest Regressor model.  Our Random Forest model ended up undergoing further hyperparameter tuning using Random Search, and all models underwent Cross Validation to assess how well our models would generalize.  Our models were evaluated using RMSE and MEDAE, with MEDAE being our metric of choice due to the interpretability it lent via errors in price prediction.\n\nAlthough we initially found our linear regression model with added feature engineering to be our strongest performing model (and from which we interpreted our coefficients and predictive variables), we later found our Random Forest Regressor model to have the strongest predictive power when comparing MEDAE across the board. \n\nIn interpreting our model, we split our analysis between numeric and categorical variables.  For our numeric variables, we found that the grams variable had the strongest predictive power of btc price, which is consistent with our intuition.  However, in analyzing our categorical variables, we gleaned useful insights such as the place where this cocaine was shipped from was more important than where it was shipped to, probably because of strict drug laws in the home country posing an extra risk on the vendor's side which drove up price, as can be seen in cases like China.  Another interesting feature ended up being the type and cut of the product and whether or not it was in the form of \"crystal\" or not.  \n\nOverall, our model performed with a median absolute error of about $44 USD, which as of this time is the best performing model on Kaggle.  ","911497f9":"Models that were used and placed into pipeline include Linear Regression, L1, L2, and Random Forest.","ceb05f76":"Initial Fit of Models & Cross Validation Scores\n----","641670f0":"---","636ac405":"Random Forest Random Search for Hyperparameter Tuning\n----","ede53d4f":"Checking the relationship between our strongest predictor variable:","a2fe434d":"1. Shirley Li\n2. Jingxian Li \n3. Michael Schulze\n4. Mundy Reimer","031baa00":"Group Name\n-----","27b395ff":"Both RMSE and MEDAE were used as evaluation metrics with MEDAE being our North Star Metric.","6652c218":"Load Data\n-----","309a7583":"Model Interpretation\n-----","49fb97cb":"In this project we used various machine learning models to predict the btc price of dark market cocaine.  \n\nThe dataset is composed of approximately 1,400 cleaned and standardized product listings from Dream Market's \"Cocaine\" category. It was collected with web-scraping and text extraction techniques in July 2017.  See the README.md in our Github repository for more information."}}