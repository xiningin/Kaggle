{"cell_type":{"f6bee774":"code","aedd2874":"code","8064635a":"code","acc438df":"code","3d8bd1b2":"code","c3820fdc":"code","25becfa8":"code","8c7865a3":"code","74e2d346":"code","c7900d13":"code","78d66eac":"code","187e9611":"code","98e87560":"code","0a786de2":"code","c36c8e11":"code","2c343717":"code","8070f19a":"code","c02ac34e":"code","d3b6d565":"markdown","130e4bed":"markdown","0d15a721":"markdown","2c870b91":"markdown","05faadeb":"markdown","e6a72ecb":"markdown","7001c99a":"markdown","9f758709":"markdown","7afa2267":"markdown","c0122534":"markdown","3cda6f03":"markdown"},"source":{"f6bee774":"import pandas as pd\nimport scipy.io\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport tensorflow as tf\nimport time\nrand_state = 42\ntf.set_random_seed(rand_state)\nnp.random.seed(rand_state)\n\nimport warnings\nwarnings.simplefilter('ignore')","aedd2874":"qm7 = scipy.io.loadmat('..\/input\/qm7.mat')\n# compute the Eigenvectors of the pairwise distance matrix?\nR = qm7['R']\n\ny = np.transpose(qm7['T']).reshape((7165,))\ny_scaling_factor = 2000.\ny_scaled = y \/ y_scaling_factor\n\n# k=0 # 0 = include diagnol, 1 = do not include diagnol\n\nnum_atoms = 23\niu = np.triu_indices(num_atoms,k=0) \niu_dist = np.triu_indices(num_atoms,k=1) # for the pairwise distance matrix, all diagonol entries will be 0 \n\n\nCM = np.zeros((qm7['X'].shape[0], num_atoms*(num_atoms+1)\/\/2), dtype=float)\neigs = np.zeros((qm7['X'].shape[0], num_atoms), dtype=float)\ncentralities = np.zeros((qm7['X'].shape[0], num_atoms), dtype=float)\ninteratomic_dist = np.zeros((qm7['X'].shape[0], ((num_atoms*num_atoms)-num_atoms)\/\/2), dtype=float) \n\nverbose=True\n\nfor i, cm in enumerate(qm7['X']):\n    coulomb_vector = cm[iu]\n    # Sort elements by decreasing order\n    shuffle = np.argsort(-coulomb_vector)\n    CM[i] = coulomb_vector[shuffle]\n    dist = squareform(pdist(R[i]))\n    # we can extract the upper triangle of the distance matri: return vector of dimension (1,num_atoms)\n    dist_vector = dist[iu_dist]\n    shuffle = np.argsort(-dist_vector)\n    interatomic_dist[i] = dist_vector[shuffle]\n    \n    w,v = np.linalg.eig((dist))\n    eigs[i] = w[np.argsort(-w)]\n    centralities[i] = np.array(list(nx.eigenvector_centrality(nx.Graph(dist)).values()))\n    \n    if verbose and i % 500 == 0:\n        print(\"Processed {} molecules\".format(i))\n    \nX = np.concatenate((CM, eigs, centralities, interatomic_dist), axis=1)\nX.shape","8064635a":"def mean_dist(x):\n    x[x == 0] = np.nan\n    return np.nanmean(x, axis=0)\n\n\n\nmean_dists = np.apply_along_axis(mean_dist, axis=1, arr=interatomic_dist)","acc438df":"plt.figure(figsize=(8,6))\nsns.distplot(mean_dists)\nplt.xlabel('Interatomic Distance')\nplt.ylabel('Probability')\nplt.title('Distribution of interatomic distances')\nplt.show()\n\nplt.figure(figsize=(8,6))\nsns.distplot(y)\nplt.xlabel('Energy (kcal\/mol)')\nplt.ylabel('Probability')\nplt.title('Distribution of Atomization Energy')\nplt.show()","3d8bd1b2":"from sklearn.decomposition import KernelPCA\nfrom matplotlib import cm\n\n# scale Coulomb Matrices, divide by 370\nCM = qm7['X'].reshape((7165, 529))\n\n\nstart_time = time.time()\nkpca = KernelPCA(n_components=2, kernel=\"rbf\")\nCM_reduced = kpca.fit_transform(CM)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nexplained_variance = np.var(CM_reduced, axis=0)\nexplained_variance_ratio = explained_variance \/ np.sum(explained_variance)\nprint(\"Variance Explained: \", np.sum(explained_variance_ratio))\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(CM_reduced[:,0], CM_reduced[:,1], c=y, s=60, edgecolors='black', cmap=cm.jet_r)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"(kcal\/mol)\")\nplt.xlabel(r'$k-PCA_1$')\nplt.ylabel(r'$k-PCA_1$')\nplt.title(\"Coulomb Matrix: Kernel PCA (RBF)\")\nsns.despine()\nplt.show()","c3820fdc":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=rand_state, perplexity=50)\nX_tsne = tsne.fit_transform(CM)\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_tsne[:,0], X_tsne[:,1], c=y, s=45, edgecolors='green', cmap=cm.jet_r, alpha=0.5)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E (kcal\/mol) \")\nplt.xlabel(r'$Z_1$')\nplt.ylabel(r'$Z_2$')\nplt.title('Coulomb Matrix: t-SNE (perplexity = 50)')\nsns.despine()\nplt.show()","25becfa8":"start_time = time.time()\nkpca = KernelPCA(n_components=2, kernel=\"rbf\")\nX_reduced = kpca.fit_transform(X)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nexplained_variance = np.var(X_reduced, axis=0)\nexplained_variance_ratio = explained_variance \/ np.sum(explained_variance)\nprint(\"Variance Explained: \", np.sum(explained_variance_ratio))\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_reduced[:,0], X_reduced[:,1], c=y, s=60, edgecolors='black', cmap=cm.jet_r)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E (kcal\/mol)\")\nplt.xlabel(r'$k-PCA_1$')\nplt.ylabel(r'$k-PCA_1$')\nplt.title(\"Visualizing All Features: Kernel PCA (RBF)\")\nplt.tight_layout()\nsns.despine()\nplt.show()","8c7865a3":"tsne = TSNE(n_components=2, random_state=rand_state, perplexity=50)\nX_tsne = tsne.fit_transform(X)\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_tsne[:,0], X_tsne[:,1], c=y, s=45, edgecolors='green', cmap=cm.jet_r, alpha=0.5)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E (kcal\/mol) \")\nplt.xlabel(r'$Z_1$')\nplt.ylabel(r'$Z_2$')\nplt.title('All Features: t-SNE (perplexity = 50)')\nsns.despine()\nplt.show()","74e2d346":"def get_category(x, total_range, num_bins):\n    bin_size = total_range\/num_bins\n    total_range = 1787.119995\n    bin_size = total_range\/num_bins\n\n    return int(np.floor(np.abs(x\/bin_size)))\n\ntotal_range = 1787.119995\nnum_bins = 10\n\ny_class = pd.Series(y).apply(lambda x: get_category(x, total_range, num_bins))","c7900d13":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\nX_2, X_val, y_class_2, y_class_val = train_test_split(X, y_class.values, \n                                                    test_size=0.15, \n                                                    random_state=rand_state)\n\n# with cross validation no need to further split the data. if not using cross validation you should do one...\nX_train, X_dev, y_class_train, y_class_dev = train_test_split(X_2, y_class_2, \n                                                    test_size=0.18, \n                                                    random_state=rand_state)\n\nclf = SVC(kernel='linear', random_state=rand_state)\nclf.fit(X_train, y_class_train)\nprint(\"Train score: \", clf.score(X_train, y_class_train))\nprint(\"Test score: \", clf.score(X_dev, y_class_dev))\n\nprint('--------\\nEVALUATE on validation set\\n--------')\nclass_preds = clf.predict(X_val)\nprint(\"Validation score: \", accuracy_score(y_class_val, class_preds))\nprint(classification_report(y_class_val, class_preds))","78d66eac":"import xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nX = np.concatenate((CM, eigs, centralities), axis=1)\n\n\n# with cross validation no need to further split the data. if not using cross validation you should do one...\nX_2, X_val, y_2, y_val = train_test_split(X, y, \n                                          test_size=0.15, \n                                          random_state=rand_state)\n\n# with cross validation no need to further split the data. if not using cross validation you should do one...\nX_train, X_dev, y_train, y_dev = train_test_split(X_2, y_2, \n                                                  test_size=0.18, \n                                                  random_state=rand_state)\n\n\nn_folds = 5\nearly_stopping = 50\n\n\nstart_time = time.time()\nxg_train = xgb.DMatrix(X_train, label=y_train)\n\nnum_iters = 300\n\nparams = {\"objective\":\"reg:linear\", \n          'booster': 'gbtree', \n          'eval_metric': 'mae',\n          'subsample': 0.9,\n          'colsample_bytree':0.2,\n          'learning_rate': 0.05,\n          'max_depth': 6, \n          'reg_lambda': .9, \n          'reg_alpha': .01,\n          'seed': rand_state}\n\n\n\ncv = xgb.cv(params,\n            xg_train, \n            num_boost_round=num_iters, \n            nfold=n_folds, \n            early_stopping_rounds=early_stopping, \n            verbose_eval = 0, \n            seed=rand_state,\n            as_pandas=False)\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nplt.figure(figsize=(8,8))\nplt.plot(cv['train-mae-mean'][100:], label='Train loss: ' + str(np.min(cv['train-mae-mean'])))\nplt.plot(cv['test-mae-mean'][100:], label='Test loss: ' + str(np.min(cv['test-mae-mean'])))\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Mean absolute error')\nplt.show()\n\nmodel_xgb = xgb.XGBRegressor(**params, random_state=rand_state, n_estimators=num_iters)\nmodel_xgb.fit(X_train, y_train, \n              early_stopping_rounds=early_stopping, \n              eval_metric='mae', \n              eval_set=[(X_dev, y_dev)], \n              verbose=False)\n\ny_dev_pred = model_xgb.predict(X_dev)\nprint('Dev mean absoulte error: ', mean_absolute_error(y_dev, y_dev_pred))\n\ny_val_pred = model_xgb.predict(X_val)\nprint('Validation mean absoulte error: ', mean_absolute_error(y_val, y_val_pred))","187e9611":"from yellowbrick.regressor import PredictionError\n\n# Instantiate the linear model and visualizer\nvisualizer = PredictionError(xgb.XGBRegressor(**params, n_estimators=num_iters, random_state=rand_state))\n\nvisualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_val, y_val)  # Evaluate the model on the test data\ng = visualizer.poof()       ","98e87560":"from yellowbrick.regressor import ResidualsPlot\n\n# Instantiate the linear model and visualizer\nvisualizer = ResidualsPlot(xgb.XGBRegressor(**params, n_estimators=num_iters, random_state=rand_state))\n\nvisualizer.fit(X_train, y_train)  # Fit the training data to the model\nvisualizer.score(X_val, y_val)  # Evaluate the model on the test data\nvisualizer.poof()                 # Draw\/show\/poof the data","0a786de2":"CM_scaled = (qm7['X'] \/ 370.0).reshape((7165, 529, 1))\n\n# now pull out 10% of the data for validation\nx_2, x_val, y_2, y_val = train_test_split(CM_scaled, y_scaled,  \n                                                    test_size=.15, \n                                                    random_state=rand_state)\n\n# the remaining 90% of the data will be used to build\/test our model\nx_train, x_dev, y_train, y_dev = train_test_split(x_2, y_2,  \n                                                    test_size=.18, \n                                                    random_state=rand_state)","c36c8e11":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, Flatten\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import optimizers\nimport math\n\nmodel_1 = Sequential()\n# our input is now a (276, 1) 1D Tensor\nkernel_initializer='he_normal'\n\nmodel_1.add(Conv1D(32, 10, activation='relu', kernel_initializer=kernel_initializer))\nmodel_1.add(Conv1D(128, 7, activation='relu', kernel_initializer=kernel_initializer))\nmodel_1.add(Conv1D(128, 5, activation='relu', kernel_initializer=kernel_initializer))\nmodel_1.add(Conv1D(256, 3, activation='relu', kernel_initializer=kernel_initializer))\nmodel_1.add(Flatten())\nmodel_1.add(Dense(1, activation='linear'))\n\nmodel_1.compile(loss='mae',\n              optimizer=optimizers.Adam(.001),\n              metrics=['mae'])\n\n\n\nstart = time.time()\n\nestop = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\nmcp_save = ModelCheckpoint('cnn_1d.hdf5', save_best_only=True, monitor='val_loss', mode='min')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1, epsilon=1e-4, mode='min')\n\nhistory_1 = model_1.fit(x_train, y_train,\n                      batch_size=4,\n                      epochs=10, \n                      callbacks=[estop, mcp_save, reduce_lr_loss],\n                      verbose=1,\n                      validation_data=(x_dev, y_dev))\n\nmodel_1.summary()\n\nend = time.time()\nprint('Execution time: ', end-start)\nprint(\"Epochs: \", len(history_1.history['val_loss']))\nprint('Train loss: ', y_scaling_factor*np.min(history_1.history['loss']))\nprint('Test loss: ', y_scaling_factor*np.min(history_1.history['val_loss']))\ny_preds = model_1.predict(x_val)\nprint(\"Validation error: \", y_scaling_factor*mean_absolute_error(y_val, y_preds))","2c343717":"plt.figure(figsize=(8,8))\ntrain_label = \"Train loss: {}\".format(np.round(np.min(history_1.history['loss'])*y_scaling_factor, 3))\ntest_label = \"Test loss: {}\".format(np.round(np.min(history_1.history['val_loss'])*y_scaling_factor, 3))\n(pd.Series(history_1.history['loss']).ewm(alpha=.1).mean()*y_scaling_factor).plot(label=train_label)\n(pd.Series(history_1.history['val_loss']).ewm(alpha=.1).mean()*y_scaling_factor).plot(label=test_label)\nplt.legend()\nplt.title('MAE Loss for 1D CNN')\nplt.xlabel('Epoch')\nplt.ylabel('E (kcal\/mol)')\nplt.show()","8070f19a":"from keras.layers import Conv2D, MaxPooling2D, Dropout\n\n\n'''\nconvnets require specific input dimensions, in this case every example should be a (23, 23, 1) Coulom Matrix\n\n'''\nCM_scaled = qm7['X'].reshape((7165, 23, 23, 1)) \/ 370.0\n\nX_cm_train, X_cm_test, y_train, y_test  = train_test_split(CM_scaled, y_scaled, \n                                                           test_size=.2, random_state=rand_state)\n\nkernel_initializer='he_normal'\nmodel_2 = Sequential()\nmodel_2.add(Conv2D(32, kernel_size=(7,7), padding='same',\n                 activation='relu',\n                 kernel_initializer=kernel_initializer,\n                 input_shape=(23, 23, 1)))\nmodel_2.add(Conv2D(64, kernel_size=(5,5), kernel_initializer=kernel_initializer, activation='relu'))\nmodel_2.add(Conv2D(64, kernel_size=(3,3), kernel_initializer=kernel_initializer, activation='relu'))\nmodel_2.add(Conv2D(64, kernel_size=(3,3), kernel_initializer=kernel_initializer, activation='relu'))\nmodel_2.add(Conv2D(128, kernel_size=(3,3), kernel_initializer=kernel_initializer, activation='relu'))\nmodel_2.add(Conv2D(128, kernel_size=(3,3), kernel_initializer=kernel_initializer, activation='relu'))\nmodel_2.add(Flatten())\nmodel_2.add(Dense(1, activation='linear'))\n\nmodel_2.compile(loss='mae',\n              optimizer=optimizers.Adam(lr=.001),\n              metrics=['mae'])\n\nstart = time.time()\n\n\nestop = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\nmcp_save = ModelCheckpoint('cnn_2d.hdf5', save_best_only=True, monitor='val_loss', mode='min')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1, epsilon=1e-4, mode='min')\n\nhistory_2 = model_2.fit(X_cm_train, y_train,\n                      batch_size=4,\n                      epochs=10, \n                      callbacks=[estop, mcp_save, reduce_lr_loss],\n                      verbose=1,\n                      validation_data=(X_cm_test, y_test))\n\nend = time.time()\n\nprint(model_2.summary())\n\nprint('Execution time: ', end-start)\nprint(\"Epochs: \", len(history_2.history['val_loss']))\nprint('Train loss: ', y_scaling_factor*np.min(history_2.history['loss']))\nprint('Test loss: ', y_scaling_factor*np.min(history_2.history['val_loss']))","c02ac34e":"plt.figure(figsize=(8,8))\ntrain_label = \"Train loss: {}\".format(np.round(np.min(history_2.history['loss'])*y_scaling_factor, 3))\ntest_label = \"Test loss: {}\".format(np.round(np.min(history_2.history['val_loss'])*y_scaling_factor, 3))\n(pd.Series(history_2.history['loss']).ewm(alpha=.1).mean()*y_scaling_factor).plot(label=train_label)\n(pd.Series(history_2.history['val_loss']).ewm(alpha=.1).mean()*y_scaling_factor).plot(label=test_label)\nplt.legend()\nplt.title('MAE Loss for 2D CNN')\nplt.xlabel('Epoch')\nplt.ylabel('E (kcal\/mol)')\nplt.show()","d3b6d565":"# Deep Learning \n\nTranslation invariance is a crucial aspect of why convnet's are so effective when dealing with images, and here I show that a similar approach can be taken when dealing with molecules; rotational-invariance is ubiquitous in chemistry. I will use both 2D and 1D convnets.  \n\n## 1D Convolutional Neural Network  \n\nThese networks are typically used when dealing with time series (sequences), and we can view our molecule as a sequence of atoms; specifically sequences of the raw Coulomb Matrices, unraveled into row vectors.","130e4bed":"# Outline  \n\n1. Introduction\n2. Data\n  * Feature Engineering\n3. Visualization\n  * Kernel PCA\n  * t-SNE\n4. Classification\n  * SVM\n  * Classification metrics\n5. Regression\n  * XGBoost\n  * Deep Learning\n    * MLP\n    * CNN\n6. Conclusions\n\n\n\n# Introduction  \n\nResearch and development (R&D) costs for US based pharmaceutical companies have risen 43.9% just over the last 5 years ([source](https:\/\/www.statista.com\/statistics\/265085\/research-and-development-expenditure-us-pharmaceutical-industry\/)). R&D spending has increased by 27% since 2010 and is projected to increase by another 23.6% over the next 5 years; R&D accounts for roughly 21% of all pharmaceutical sales, this pipeline can be summarized as so:  \n\n  1. Pre-clinical  \n  2. Phase 1 clinical trials  \n  3. Phase II  \n  4. Phase III  \n  5. Pre-registration  \n  6. Registered  \n  7. Launched  \n  8. Suspended  \n  \nThe most resources (by far) are spent in the pre-clinical stage ([source](https:\/\/www.statista.com\/statistics\/791288\/r-and-d-pipeline-drugs-worldwide-by-phase-development\/)), with drug discovery being one of the initial steps in this process. Drug discovery is the process by which new candidate medications are discovered. In classical pharmacology, chemical libraries of synthetic small molecules, natural products or extracts are utilized in combination with in-vitro studies to identify substances that have a desirable therapeutic effect. \n\nSince the sequencing of the human genome, it has become common practice to use high throughput screening of large compounds libraries against isolated biological targets which are hypothesized to be disease modifying in a process known as reverse pharmacology. The candidate drugs are then tested for efficacy in-vivo. Various chemical properties, such as ADME (absorption, distribution, metabolism and excretion) as well as atomization energy have been shown to play a crucial role in the screening of such compounds, and that is where machine learning can help! The accurate prediction of molecular energetics in chemical compound space is a crucial ingredient for rational compound design.  \n\nCurrently only high-level quantum-chemistry calculations, which can take days per molecule depending on property and system, yield the desired \u201cchemical\naccuracy\u201d of 1 kcal\/mol required for computational molecular design. Here, I am proposing a less expensive machine learning approach to predicting this the quantum mechanical property of atomization energy. To accomplish this objective, I will engineer features to work well with a gradient boosting regression model, support vector machine classification model, as well as a multilayer perceptron network and a convolutional neural network. \n\n# Data  \n\nI am using the QM7 dataset, which is a subset of GDB-13 (a database of nearly 1 billion stable and synthetically accessible organic molecules) containing up to 7 heavy atoms C, N, O, and S. The 3D Cartesian coordinates of the most stable conformations and their atomization energies were determined using ab-initio density functional theory (PBE0\/tier2 basis set). This dataset also provided Coulomb matrices as calculated in [Rupp et al. PRL, 2012]:  \n  * $C_{i,i} = 0.5 \\cdot Z^2.4$  \n  * $C_{i,j} = Z_i \\cdot \\frac{Z_j}{|(R_i\u2212R_j)|}$ \n  * $Z_i$ - nuclear charge of atom i  \n  * $R_i$ - cartesian coordinates of atom i  \n\nThe data file (.mat format, we recommend using `scipy.io.loadmat` for python users) contains five arrays:  \n  * \"X\" - (7165 x 23 x 23), Coulomb matrices  \n  * \"T\" - (7165), atomization energies (unit: kcal\/mol)  \n  * \"P\" - (5 x 1433), cross-validation splits as used in [Montavon et al. NIPS, 2012]  \n  * \"Z\" - (7165 x 23), atomic charges  \n  * \"R\" - (7165 x 23 x 3), cartesian coordinate (unit: Bohr) of each atom in the molecules   \n\n\n## Feature Engineering  \n\nThe data is stored in a MATLAB file, so we read that using SciPy. For the regression models, I take the upper triangle of each Coulomb Matrix (with diagonal) and unroll it so each has the shape: (1, num_atoms*(num_atoms+1)\/2). I then compute the pairwise distance matrix and from this get the corresponding Eigenvalues and [Eigenvector Centralities](https:\/\/en.wikipedia.org\/wiki\/Eigenvector_centrality). I am attempting to estimate the influence one particular atom plays on the entire molecule; typically, this is performed on an adjacency matrix (graph), however, I am assuming the connectivity (bonds) are not known upfront (can be accurately predicted via Monte Carlo simulations).  \n\n## Previous work  \nThis is an extension to some earlier research that I previously [conducted](https:\/\/www.kaggle.com\/mjmurphy28\/predicting-ground-state-energy). That study (and therefore this one also) is primarily based off two previous studies: a 2016 publication by [B. Himmetoglu](https:\/\/arxiv.org\/abs\/1609.07124) and a team of researchers at [Stanford](http:\/\/moleculenet.ai). The earlier work by Himmetoglu reported a RMSE of 38.75 kcal\/mol on the train set and 36.83 on the test set. I was not able to find the full results from the team at Stanford, however, on their [site](http:\/\/moleculenet.ai\/latest-results) they list a MAE of 8.56 (I am assuming that this metric comes from evaluating their TensorFlow regression model on the test set).","0d15a721":"Let us try to visualize just the **Coulomb Matrices**.","2c870b91":"# Classification  \n\nFirst I will create a model to classify a given atom into one of four classes, based on atomization energy. I have manually constructed my classes so that they are roughly balanced. A tool like this can provide us insight into 'tricky' molecules, those that are difficult to distinguish among.  \n\nI will split my data into **70\/20\/10** train\/dev\/val sets.","05faadeb":"## 2D Convolutional Neural Network  \n\nThis type of network operates on matrix input, so raw Coulomb Matrices seem natural. Convnets essentially eliminate feature engineering, however, the choice of network topology, filters and optimizers (as well as other things) play a crucial role in the performance (and training) of the network.","e6a72ecb":"Notice the shape of the distributions, notably those belonging to class 2.","7001c99a":"# Regression\n\nFirst, I am going to split my data into 70\/15\/15 train\/dev\/validation sets, and use 3-fold cross validation when training my model. Through experimentation (as well as previous [work](https:\/\/www.kaggle.com\/mjmurphy28\/predicting-ground-state-energy)) XGBoost provides excellent accuracy, while very low training time. I settled on the following hyper-parameters through intuition, literature as well as trial and error. As mentioned above, a number of the features that I engineered  solely for classification purposes do not add any explanatory power to my regression model, therefore I will only be using the unrolled Coulomb Matrix, Eigenvector centralities and Eigenvalues (of the interatomic distance matrix). ","9f758709":"# Visualization","7afa2267":"Note that this model (underlying data\/problem) overfits badly. By adding regularization I was able to decrease the amount our model overfits to the train data. Note that if you run this model for more boosting rounds (iterations) the model begins to significantly overfit. Setting the max depth of each tree also helps reduce this.","c0122534":"Now let us add our other features and see how this changes (visually): specifically, we unroll and sort the Coulmb Matrices, add the Eigenvectors and centralities.","3cda6f03":"# Conclusion  \n\n\n# References  \n\n  * https:\/\/papers.nips.cc\/paper\/4830-learning-invariant-representations-of-molecules-for-atomization-energy-prediction.pdf\n  * Rupp, Matthias, et al. \"Fast and accurate modeling of molecular atomization energies with machine learning.\" Physical review letters 108.5 (2012): 058301.\n  * Montavon, Gr\u00e9goire, et al. [\"Learning invariant representations of molecules for atomization energy prediction.\"](https:\/\/pdfs.semanticscholar.org\/5761\/d22bb67798167a832879a473e0ece867df04.pdf) Advances in Neural Information Processing Systems. 2012.\n  "}}