{"cell_type":{"9c1f7531":"code","1e62dfdd":"code","3fafae39":"code","e9b87254":"code","4a2fb7d0":"code","5758cea0":"code","00202801":"code","7af927cf":"code","5e7fbf0a":"code","4231c365":"code","c15142d1":"code","4b33390c":"code","72dd888c":"code","b8d10ed8":"code","8708ecea":"code","bce37e78":"code","d7bdaa9a":"code","96385900":"code","a13c71fb":"code","e0161ecb":"code","88ed30ef":"code","43c6062a":"code","46021272":"code","28abc484":"code","55397f4c":"code","412d981b":"code","252fd4e1":"code","701b208e":"code","9369f057":"code","37118237":"code","ce1a0456":"code","784a2650":"code","098fd772":"code","fb64c64c":"code","d1e8997a":"code","fab8a3da":"code","3328f49f":"code","a7cd1417":"code","65041439":"code","1c4146f2":"code","c673a12d":"code","a60015d7":"code","c155f364":"code","e1135db4":"code","459d2dd4":"code","aa83d8ae":"code","4d94b850":"code","a4c31da0":"code","ec17871d":"code","da74e06c":"code","4f038891":"code","ee550a22":"code","e6c7a237":"code","e1475c8a":"code","6ef5bf92":"code","3f5ea013":"code","a97a388e":"code","4b701567":"code","6cc7a362":"code","6c4b0472":"code","e61bccdb":"code","f88b13e3":"code","63fa4672":"code","8eab05fc":"code","b6d0959b":"code","d075ba43":"code","0845cb6c":"code","278b0e5c":"code","f5d1e32e":"code","b9499491":"code","042b6cca":"code","5655420d":"code","39e68c94":"code","140f93ed":"code","b7b54360":"code","fd325f0d":"code","001ed924":"code","c85a46fe":"code","8bea999c":"code","927e7ea9":"code","bdf367ef":"code","fa341cb0":"code","313d1209":"code","5b28ad0e":"code","6894c327":"code","b9a13dce":"code","ee780ae8":"code","874d1796":"code","bf955f0a":"code","09608a13":"code","c387b436":"code","01568740":"code","49aced94":"code","45eed05d":"code","4dd5a00e":"code","a4c75505":"code","4eba0b61":"code","c226aae6":"code","dff2ada3":"code","69c8f409":"code","79a6ab0e":"code","811098f6":"code","750acc27":"markdown","d05bbe7e":"markdown","87c33291":"markdown","d7a94063":"markdown","013fe0b2":"markdown","fca04109":"markdown","db334908":"markdown","2ca207ce":"markdown","abd71a53":"markdown","41261ce1":"markdown","5aee7cf7":"markdown","d0cc5a37":"markdown","c93f2cd4":"markdown","bcebad3a":"markdown","9c2213b7":"markdown","bdf7319b":"markdown","27614be1":"markdown","3ea2d5a6":"markdown","4968b4eb":"markdown","46a51eee":"markdown","7355b735":"markdown","9ef8b19f":"markdown","6ce9f007":"markdown","0854edb9":"markdown","c9ef2c8d":"markdown","76fea029":"markdown","fa3acdd5":"markdown","cf2f76e5":"markdown","a586897a":"markdown","05967e2b":"markdown","45f09ffc":"markdown","500ea408":"markdown","bc66dd30":"markdown","4285afe4":"markdown","2cbf1375":"markdown","e5922bd8":"markdown","13ba4351":"markdown","9ce1234f":"markdown","984b0539":"markdown","28c9cc6a":"markdown","8f90eed4":"markdown","a9f7f99b":"markdown","5c4a0aa0":"markdown","e917159e":"markdown","2641e1e3":"markdown","37d00d56":"markdown","a83c9642":"markdown","26cc4fc6":"markdown","6bcbfba6":"markdown","78c49a0c":"markdown","723672c0":"markdown","e7782f8f":"markdown","7383737c":"markdown","8d6de8b6":"markdown","c67ee463":"markdown","437274c2":"markdown","b70f9f4c":"markdown","e116a93a":"markdown","0575505c":"markdown","f66798d9":"markdown","4a0e7a2c":"markdown","d1fe2c59":"markdown","10a99212":"markdown","bea9e2aa":"markdown","f2bd4418":"markdown","23f2213a":"markdown","b9ba32ab":"markdown","e1c41986":"markdown","73d40ca5":"markdown","737c6c7b":"markdown","6907d656":"markdown","a280cca7":"markdown","ba6dbe60":"markdown","21c1c4d7":"markdown","afda00d6":"markdown","63eafcbc":"markdown","114e61ac":"markdown","354284d6":"markdown","975e265a":"markdown","2908fd96":"markdown","80562cc0":"markdown","b015515f":"markdown","b43c6ff4":"markdown","733d7e5d":"markdown","aa9ab2a6":"markdown","2870cf7d":"markdown","485c5fc4":"markdown","fa9de5b1":"markdown","83dd4025":"markdown","b91d9385":"markdown","0dd7c437":"markdown","14a1eb3c":"markdown","973177ab":"markdown","bb853b30":"markdown","db740e8c":"markdown"},"source":{"9c1f7531":"# Import the English language class\nfrom spacy.lang.en import English\n\n# Create the nlp object\nnlp = English()\n\n# Process a text\ndoc = nlp(\"This is a sentence.\")\n\n# Print the document text\nprint(doc.text)","1e62dfdd":"from spacy.lang.te import Telugu\nsentence = \"\u0c28\u0c47\u0c28\u0c41 \u0c2c\u0c3e\u0c17\u0c41\u0c28\u0c4d\u0c28\u0c3e\u0c28\u0c41. \u0c2e\u0c40\u0c30\u0c41 \u0c0f\u0c32\u0c3e \u0c09\u0c28\u0c4d\u0c28\u0c3e\u0c30\u0c41 ?\"\nnlp = Telugu()\ndoc = nlp(sentence)\nprint(doc.text)\nprint(doc)","3fafae39":"# Import the German language class\nfrom spacy.lang.de import German\n\n# Create the nlp object\nnlp = German()\n\n# Process a text (this is German for: \"Kind regards!\")\ndoc = nlp(\"Liebe Gr\u00fc\u00dfe!\")\n\n# Print the document text\nprint(doc.text)","e9b87254":"# Import the Spanish language class\nfrom spacy.lang.es import Spanish\n\n# Create the nlp object\nnlp = Spanish()\n\n# Process a text (this is Spanish for: \"How are we?\")\ndoc = nlp(\"\u00bfC\u00f3mo est\u00e1s?\")\n\n# Print the document text\nprint(doc.text)","4a2fb7d0":"nlp = English()\n\n# Process the text\ndoc = nlp(\"I like tree kangaroos and narwhals.\")\n\n# Select the first token\nfirst_token = doc[0]\n\n# Print the first token's text\nprint(first_token.text)","5758cea0":"# A slice of the Doc for \"tree kangaroos\"\ntree_kangaroos = doc[2:4]\nprint(tree_kangaroos.text)\n\n# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\ntree_kangaroos_and_narwhals = doc[2:6]\nprint(tree_kangaroos_and_narwhals.text)","00202801":"# Process the text\ndoc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")\n\n# Iterate over the tokens in the doc\nfor token in doc:\n    # Check if the token resembles a number\n    if token.like_num:\n        # Get the next token in the document\n        next_token = doc[token.i+1]\n        # Check if the next token's text equals '%'\n        if next_token.text == '%':\n            print('Percentage found:', ''.join([token.text, doc[token.i+1].text]))","7af927cf":"import spacy\n\n# Load the 'en_core_web_sm' model \u2013 spaCy is already imported\nnlp = spacy.load('en_core_web_sm')\n\ntext = \"It\u2019s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n\n# Process the text\ndoc = nlp(text)\n\n# Print the document text\nprint(doc.text)","5e7fbf0a":"# # Load the 'de_core_news_sm' model \u2013 spaCy is already imported\n# nlp = spacy.load(\"de_core_news_sm\")\n\n# text = \"Als erstes Unternehmen der B\u00f6rsengeschichte hat Apple einen Marktwert von einer Billion US-Dollar erreicht\"\n\n# # Process the text\n# doc = nlp(text)\n\n# # Print the document text\n# print(doc.text)","4231c365":"nlp = spacy.load('en_core_web_sm')\ntext = \"It\u2019s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n\n# Process the text\ndoc = nlp(text)\n\nfor token in doc:\n    # Get the token text, part-of-speech tag and dependency label\n    token_text = token.text\n    token_pos = token.pos_\n    token_dep = token.dep_\n    # This is for formatting only\n    print('{:<12}{:<10}{:<10}'.format(token_text, token_pos, token_dep))","c15142d1":"print(spacy.explain('amod'))\nprint(spacy.explain('PART'))","4b33390c":"text = \"It\u2019s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n\n# Process the text\ndoc = nlp(text)\n\n# Iterate over the predicted entities\nfor ent in doc.ents:\n    # print the entity text and its label\n    print(ent.text, ent.label_)","72dd888c":"text = \"It\u2019s official. Apple is the first U.S. public company. It is the first to reach a $1 trillion market value\"\ndoc=nlp(text)\nfor sent in doc.sents:\n    print(sent)","b8d10ed8":"from spacy import displacy\ndisplacy.render(doc, style='ent')","8708ecea":"displacy.render(doc, style='dep')","bce37e78":"displacy.render(doc.sents, style='dep')","d7bdaa9a":"text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n\n# Process the text\ndoc = nlp(text)\n\n# Iterate over the entities\nfor ent in doc.ents:\n    # print the entity text and label\n    print(ent.text, ent.label_)","96385900":"# Get the span for \"iPhone X\"\niphone_x = doc[1:3]\n\n# Print the span text\nprint('Missing entity:', iphone_x.text)","a13c71fb":"# Import the Matcher\nfrom spacy.matcher import Matcher\n\n# Initialize the Matcher with the shared vocabulary\nmatcher = Matcher(nlp.vocab)","e0161ecb":"# Create a pattern matching two tokens: \"iPhone\" and \"X\"\npattern = [[{\"TEXT\": \"iPhone\"}], [{\"TEXT\": \"X\"}]]\n\n# Add the pattern to the matcher\nmatcher.add('IPHONE_X_PATTERN', pattern)","88ed30ef":"# Use the matcher on the doc\nmatches = matcher(doc)\nprint(matches)\nprint('Matches:', [doc[start:end].text for match_id, start, end in matches])","43c6062a":"matcher = Matcher(nlp.vocab)\n\ndoc = nlp(\"After making the iOS update we won't notice a radical system-wide redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture remains the same as in iOS 10. But we will discover some tweaks once we delve a little deeper.\")\n\n# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\npattern = [[{'TEXT': 'iOS'}, {'IS_DIGIT': True}]]\n\n# Add the pattern to the matcher and apply the matcher to the doc\nmatcher.add('IOS_VERSION_PATTERN', pattern)\nmatches = matcher(doc)\nprint('Total matches found:', len(matches))\n\n# Iterate over the matches and print the span text\nfor match_id, start, end in matches:\n    print('Match found:', doc[start:end].text)","46021272":"doc = nlp(\"i downloaded Fortnite on my laptop and can't open the game at all. Help? so when I was downloading Minecraft, I got the Windows version where it is the '.zip' folder and I used the default program to unpack it... do I also need to download Winzip?\")\n\n# Write a pattern that matches a form of \"download\" plus proper noun\npattern = [[{'LEMMA': 'download'}, {'POS': 'PROPN'}]]\n\n# Add the pattern to the matcher and apply the matcher to the doc\nmatcher.add('DOWNLOAD_THINGS_PATTERN', pattern)\nmatches = matcher(doc)\nprint('Total matches found:', len(matches))\n\n# Iterate over the matches and print the span text\nfor match_id, start, end in matches:\n    print('Match found:', doc[start:end].text)","28abc484":"doc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n\n# Write a pattern for adjective plus one or two nouns\npattern = [[{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]]\n\n# Add the pattern to the matcher and apply the matcher to the doc\nmatcher.add('ADJ_NOUN_PATTERN', pattern)\nmatches = matcher(doc)\nprint('Total matches found:', len(matches))\n\n# Iterate over the matches and print the span text\nfor match_id, start, end in matches:\n    print('Match found:', doc[start:end].text)","55397f4c":"# Look up the hash for the string label \"PERSON\"\nperson_hash = nlp.vocab.strings['PERSON']\nprint(person_hash)\n\n# Look up the person_hash to get the string\nperson_string = nlp.vocab.strings[person_hash]\nprint(person_string)","412d981b":"# # Look up the hash for the word \"cat\"\n# nlp = English()\n\n# cat_hash = nlp.vocab.strings['cat']\n# print(cat_hash)\n\n# # Look up the cat_hash to get the string\n# cat_string = nlp.vocab.strings[cat_hash]\n# print(cat_string)","252fd4e1":"# nlp = English()\n\n# Srinivas_id = nlp.vocab.strings['Srinivas']\n# print(Srinivas_id)\n    \n# print(nlp.vocab.strings[Srinivas_id])","701b208e":"for i in range(1,100):\n    print(nlp.vocab.strings[i])","9369f057":"# Import the Doc class\nfrom spacy.tokens import Doc\n\n# Desired text: \"spaCy is cool!\"\nwords = ['spaCy', 'is', 'cool', '!']\nspaces = [True, True, False, False]\n\n# Create a Doc from the words and spaces\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\nprint(doc.text)","37118237":"# Desired text: \"Go, get started!\"\nwords = ['Go', ',', 'get', 'started', '!']\nspaces = [False, True, True, False, False]\n\n# Create a Doc from the words and spaces\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\nprint(doc.text)","ce1a0456":"# Desired text: \"Oh, really?!\"\nwords = ['Oh', \",\", 'really', '?', '!']\nspaces = [False, True, False, False, False]\n\n# Create a Doc from the words and spaces\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\nprint(doc.text)","784a2650":"# Import the Doc and Span classes\nfrom spacy.tokens import Doc, Span\n\nwords = ['I', 'like', 'Valmeti', 'Srinivas']\nspaces = [True, True, True, False]\n\n# Create a doc from the words and spaces\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\nprint(doc.text)","098fd772":"# Import the Doc and Span classes\nfrom spacy.tokens import Doc, Span\n\n# Create a doc from the words and spaces\ndoc = Doc(nlp.vocab, words=['I', 'like', 'Valmeti', 'Srinivas'], spaces=[True, True, True, False])\n\n# Create a span for \"Valmeti Srinivas\" from the doc and assign it the label \"PERSON\"\nspan = Span(doc, start=2, end=4, label='PERSON')\nprint(span.text, span.label_)","fb64c64c":"# Add the span to the doc's entities\ndoc.ents = [span]\n\n# Print entities' text and labels\nprint([(ent.text, ent.label_) for ent in doc.ents])","d1e8997a":"nlp = spacy.load('en_core_web_sm')\ndoc = nlp('Berlin is a nice city')\n# Get all tokens and part-of-speech tags\npos_tags = [token.pos_ for token in doc]\nprint(pos_tags)","fab8a3da":"doc = nlp('Berlin is a nice city')\n# Get all tokens and part-of-speech tags\npos_tags = [token.pos_ for token in doc]\nprint(pos_tags)\nfor index, pos in enumerate(pos_tags):\n    # Check if the current token is a proper noun\n    if pos == 'PROPN':\n        # Check if the next token is a verb\n        if (pos_tags[index + 1] == 'AUX') | (pos_tags[index + 1] == 'VERB'):\n            print('Found a verb after a proper noun!')","3328f49f":"pos_es = [\"AUX\", \"VERB\"]\n        \nfor pos in pos_es:\n    for token in doc:\n        # Check if the current token is a proper noun\n        if token.pos_ == 'PROPN':\n            # Check if the next token is a verb\n            if doc[token.i + 1].pos_ == pos:\n                print('Found a verb after a proper noun!')","a7cd1417":"# Load the en_core_web_md model. Loading Small model because Kaggle by default does not provide APIS for Medium model\nnlp = spacy.load('en_core_web_sm')\n\n# Process a text\ndoc = nlp(\"Two bananas in pyjamas\")\n\n# Get the vector for the token \"bananas\"\nbananas_vector = doc[1].vector\ndisplay(len(bananas_vector),bananas_vector)","65041439":"doc1 = nlp(\"It's a warm summer day\")\ndoc2 = nlp(\"It's sunny outside\")\n\n# Get the similarity of doc1 and doc2\nsimilarity = doc1.similarity(doc2)\nprint(similarity)","1c4146f2":"doc = nlp(\"TV and books\")\ntoken1, token2 = doc[0], doc[2]\n\n# Get the similarity of the tokens \"TV\" and \"books\" \nsimilarity = token1.similarity(token2)\nprint(similarity)","c673a12d":"doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n\n# Create spans for \"great restaurant\" and \"really nice bar\"\nspan1 = doc[3:5]\nspan2 = doc[12:15]\ndisplay(span1.text, span2.text)\n\n# Get the similarity of the spans\nsimilarity = span1.similarity(span2)\nprint(similarity)","a60015d7":"matcher = Matcher(nlp.vocab)\ndoc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")\npattern = [[{'LOWER': 'silicon'}, {'TEXT': ' '}, {'LOWER': 'valley'}]]\nmatcher.add('SC', pattern)\nmatcher(doc)","c155f364":"pattern = [[{'LOWER': 'silicon'},  {'LOWER': 'valley'}]]\nmatcher.add('SC', pattern)\nmatcher(doc)","e1135db4":"matcher = Matcher(nlp.vocab)\ndoc = nlp(\"Twitch Prime, the perks program for Amazon Prime members offering free loot, games and other benefits, is ditching one of its best features: ad-free viewing. According to an email sent out to Amazon Prime members today, ad-free viewing will no longer be included as a part of Twitch Prime for new members, beginning on September 14. However, members with existing annual subscriptions will be able to continue to enjoy ad-free viewing until their subscription comes up for renewal. Those with monthly subscriptions will have access to ad-free viewing until October 15.\")","459d2dd4":"# Create the match patterns\npattern1 = [[{'LOWER': 'Amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]]\npattern2 = [[{'LOWER': 'ad-free'}, {'POS': 'NOUN'}]]\n\n# Initialize the Matcher and add the patterns\nmatcher = Matcher(nlp.vocab)\nmatcher.add('PATTERN1', pattern1)\nmatcher.add('PATTERN2', pattern2)\n\n# Iterate over the matches\nfor match_id, start, end in matcher(doc):\n    # Print pattern string name and text of matched span\n    print(match_id, doc.vocab.strings[match_id], doc[start:end].text)","aa83d8ae":"# Create the correct match patterns\npattern1 = [[{'LOWER': 'amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]]\npattern2 = [[{'LOWER': 'ad'}, {'TEXT': '-'}, {'LOWER': 'free'}, {'POS': 'NOUN'}]]\n\n# Initialize the Matcher and add the patterns\nmatcher = Matcher(nlp.vocab)\nmatcher.add('PATTERN1',  pattern1)\nmatcher.add('PATTERN2',  pattern2)\n\n# Iterate over the matches\nfor match_id, start, end in matcher(doc):\n    # Print pattern string name and text of matched span\n    print(match_id, doc.vocab.strings[match_id], doc[start:end].text)","4d94b850":"COUNTRIES = ['Afghanistan', '\u00c5land Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia (Plurinational State of)', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'United States Minor Outlying Islands', 'Virgin Islands (British)', 'Virgin Islands (U.S.)', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cabo Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo (Democratic Republic of the)', 'Cook Islands', 'Costa Rica', 'Croatia', 'Cuba', 'Cura\u00e7ao', 'Cyprus',\n 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', \"C\u00f4te d'Ivoire\", 'Iran (Islamic Republic of)', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg',\n 'Macao', 'Macedonia (the former Yugoslav Republic of)', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia (Federated States of)', 'Moldova (Republic of)', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', \"Korea (Democratic People's Republic of)\", 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestine, State of', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Republic of Kosovo', 'R\u00e9union', 'Romania', 'Russian Federation', 'Rwanda',\n 'Saint Barth\u00e9lemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Korea (Republic of)', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom of Great Britain and Northern Ireland', 'United States of America', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela (Bolivarian Republic of)', 'Viet Nam', 'Wallis and Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe']","a4c31da0":"doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")","ec17871d":"# Import the PhraseMatcher and initialize it\nfrom spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab)\n\n# Create pattern Doc objects and add them to the matcher\n# This is the faster version of: [nlp(country) for country in COUNTRIES]\npatterns = list(nlp.pipe(COUNTRIES))\nprint(len(patterns),'\\n', patterns[:10],'\\n')\n\nmatcher.add('COUNTRY', None, *patterns)\n\n# Call the matcher on the test document and print the result\nmatches = matcher(doc)\nprint([doc[start:end] for match_id, start, end in matches])","da74e06c":"text = 'After the Cold War, the UN saw a radical expansion in its peacekeeping duties, taking on more missions in ten years than it had in the previous four decades.Between 1988 and 2000, the number of adopted Security Council resolutions more than doubled, and the peacekeeping budget increased more than tenfold. The UN negotiated an end to the Salvadoran Civil War, launched a successful peacekeeping mission in Namibia, and oversaw democratic elections in post-apartheid South Africa and post-Khmer Rouge Cambodia. In 1991, the UN authorized a US-led coalition that repulsed the Iraqi invasion of Kuwait. Brian Urquhart, Under-Secretary-General from 1971 to 1985, later described the hopes raised by these successes as a \"false renaissance\" for the organization, given the more troubled missions that followed. Though the UN Charter had been written primarily to prevent aggression by one nation against another, in the early 1990s the UN faced a number of simultaneous, serious crises within nations such as Somalia, Haiti, Mozambique, and the former Yugoslavia. The UN mission in Somalia was widely viewed as a failure after the US withdrawal following casualties in the Battle of Mogadishu, and the UN mission to Bosnia faced \"worldwide ridicule\" for its indecisive and confused mission in the face of ethnic cleansing. In 1994, the UN Assistance Mission for Rwanda failed to intervene in the Rwandan genocide amid indecision in the Security Council. Beginning in the last decades of the Cold War, American and European critics of the UN condemned the organization for perceived mismanagement and corruption. In 1984, the US President, Ronald Reagan, withdrew his nation\\'s funding from UNESCO (the United Nations Educational, Scientific and Cultural Organization, founded 1946) over allegations of mismanagement, followed by Britain and Singapore. Boutros Boutros-Ghali, Secretary-General from 1992 to 1996, initiated a reform of the Secretariat, reducing the size of the organization somewhat. His successor, Kofi Annan (1997\u20132006), initiated further management reforms in the face of threats from the United States to withhold its UN dues. In the late 1990s and 2000s, international interventions authorized by the UN took a wider variety of forms. The UN mission in the Sierra Leone Civil War of 1991\u20132002 was supplemented by British Royal Marines, and the invasion of Afghanistan in 2001 was overseen by NATO. In 2003, the United States invaded Iraq despite failing to pass a UN Security Council resolution for authorization, prompting a new round of questioning of the organization\\'s effectiveness. Under the eighth Secretary-General, Ban Ki-moon, the UN has intervened with peacekeepers in crises including the War in Darfur in Sudan and the Kivu conflict in the Democratic Republic of Congo and sent observers and chemical weapons inspectors to the Syrian Civil War. In 2013, an internal review of UN actions in the final battles of the Sri Lankan Civil War in 2009 concluded that the organization had suffered \"systemic failure\". One hundred and one UN personnel died in the 2010 Haiti earthquake, the worst loss of life in the organization\\'s history. The Millennium Summit was held in 2000 to discuss the UN\\'s role in the 21st century. The three day meeting was the largest gathering of world leaders in history, and culminated in the adoption by all member states of the Millennium Development Goals (MDGs), a commitment to achieve international development in areas such as poverty reduction, gender equality, and public health. Progress towards these goals, which were to be met by 2015, was ultimately uneven. The 2005 World Summit reaffirmed the UN\\'s focus on promoting development, peacekeeping, human rights, and global security. The Sustainable Development Goals were launched in 2015 to succeed the Millennium Development Goals. In addition to addressing global challenges, the UN has sought to improve its accountability and democratic legitimacy by engaging more with civil society and fostering a global constituency. In an effort to enhance transparency, in 2016 the organization held its first public debate between candidates for Secretary-General. On 1 January 2017, Portuguese diplomat Ant\u00f3nio Guterres, who previously served as UN High Commissioner for Refugees, became the ninth Secretary-General. Guterres has highlighted several key goals for his administration, including an emphasis on diplomacy for preventing conflicts, more effective peacekeeping efforts, and streamlining the organization to be more responsive and versatile to global needs. Added United States of America'","4f038891":"nlp = English()\nmatcher = PhraseMatcher(nlp.vocab)\npatterns = list(nlp.pipe(COUNTRIES))\nmatcher.add('COUNTRY', None, *patterns)\n\n# Create a doc and find matches in it\ndoc = nlp(text)\nprint(doc.ents, '\\n')\n\n# Iterate over the matches\nfor match_id, start, end in matcher(doc):\n    # Create a Span with the label for \"GPE\"\n    span = Span(doc, start, end, label='GPE')\n    print(span, span.label_)\n    # Overwrite the doc.ents and add the span\n    doc.ents = list(doc.ents) + [span]\n\n# Print the entities in the document\nprint('\\n',[(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE'])","ee550a22":"# Create a doc and find matches in it\ndoc = nlp(text)\n\n# Iterate over the matches\nfor match_id, start, end in matcher(doc):\n    # Create a Span with the label for \"GPE\" and overwrite the doc.ents\n    span = Span(doc, start, end, label='GPE')\n    doc.ents = list(doc.ents) + [span]\n    \n    # Get the span's root head token\n    span_root_head = span.root.head\n    # Print the text of the span root's head token and the span text\n    print(span_root_head.text, '-->', span.text)","e6c7a237":"# Load the en_core_web_sm model\nnlp = spacy.load('en_core_web_sm')\n\n# Print the names of the pipeline components\nprint(nlp.pipe_names)\n\n# Print the full pipeline of (name, component) tuples\nprint(nlp.pipeline)","e1475c8a":"# # Kaggle yet to support spacy3.x. So this block does not work. Instead try below blocks\n# from spacy.language import Language\n\n# # add decorator\n# @Language.component('len_comp')\n# # Define the custom component\n# def length_component(doc):\n#     # Get the doc's length\n#     doc_length = len(doc)\n#     print(\"This document is {} tokens long.\".format(doc_length))\n#     # Return the doc\n#     return doc","6ef5bf92":"# Define the custom component\ndef length_component(doc):\n    # Get the doc's length\n    doc_length = len(doc)\n    print(\"This document is {} tokens long.\".format(doc_length))\n    # Return the doc\n    return doc","3f5ea013":"# # Kaggle yet to support spacy3.x. So this block does not work. Instead try below blocks\n# # Load the small English model\n# nlp = spacy.load('en_core_web_sm')\n  \n# # Add the component first in the pipeline and print the pipe names\n# nlp.add_pipe('len_comp', first = True)\n# print(nlp.pipe_names)","a97a388e":"# Load the small English model\nnlp = spacy.load('en_core_web_sm')\n  \n# Add the component first in the pipeline and print the pipe names\nnlp.add_pipe(length_component, first = True)\nprint(nlp.pipe_names)","4b701567":"# Process a text\ndoc = nlp(\"People in India live happy, healthy, wealthy and peaceful lives.\")","6cc7a362":"nlp = spacy.load('en_core_web_sm')\nanimal_patterns = ['Golden Retriever', 'cat', 'turtle', 'Rattus norvegicus']\np_matcher = PhraseMatcher(nlp.vocab)\npatterns = list(nlp.pipe(animal_patterns))\np_matcher.add('ANIMALS', None, *patterns)","6c4b0472":"# # Kaggle yet to support spacy3.x. So this block does not work. Instead try below blocks\n# # add decorator\n# @Language.component('my_animals')\n# # Define the custom component\n# def animal_component(doc):\n#     # Apply the matcher to the doc\n#     matches = p_matcher(doc)\n#     # Create a Span for each match and assign the label 'ANIMAL'\n#     spans = [Span(doc, start, end, label='ANIMAL')\n#              for match_id, start, end in matches]\n#     # Overwrite the doc.ents with the matched spans\n#     doc.ents = tuple(spans)\n#     return doc","e61bccdb":"# Define the custom component\ndef animal_component(doc):\n    # Apply the matcher to the doc\n    matches = matcher(doc)\n    # Create a Span for each match and assign the label 'ANIMAL'\n    spans = [Span(doc, start, end, label='ANIMAL')\n             for match_id, start, end in matches]\n    # Overwrite the doc.ents with the matched spans\n    doc.ents = spans\n    return doc","f88b13e3":"# # Kaggle yet to support spacy3.x. So this block does not work. Instead try below blocks\n# # Add the component to the pipeline after the 'ner' component \n# nlp.add_pipe('my_animals', after='ner')\n# print(nlp.pipe_names)","63fa4672":"# Add the component to the pipeline after the 'ner' component \nnlp.add_pipe(animal_component, after='ner')\nprint(nlp.pipe_names)","8eab05fc":"# Process the text and print the text and label for the doc.ents\ndoc = nlp(\"I have purchased a German cat and a Golden Retriever from USA Wallmart\")\nprint([(ent.text, ent.label_) for ent in doc.ents])","b6d0959b":"from spacy.tokens import Token\nnlp = English()\n# Register the Token extension attribute 'is_country' with the default value False\nToken.set_extension('is_country', default=False, force=True)\n\n# Process the text and set the is_country attribute to True for the token \"Spain\"\ndoc = nlp(\"I live in Spain.\")\n\ndoc[3]._.is_country = True\n\n# Print the token text and the is_country attribute for all tokens\nprint([(token.text, token._.is_country) for token in doc])","d075ba43":"# Define the getter function that takes a token and returns its reversed text\ndef get_reversed(token):\n    return token.text[::-1]\n  \n# Register the Token property extension 'reversed' with the getter get_reversed\nToken.set_extension('reversed', getter = get_reversed, force=True)\n\n# Process the text and print the reversed attribute for each token\ndoc = nlp(\"All generalizations are false, including this one.\")\nfor token in doc:\n    print('reversed:', token._.reversed)","0845cb6c":"# Define the getter function\ndef get_has_number(doc):\n    # Return if any of the tokens in the doc return True for token.like_num\n    return any(token.like_num for token in doc)\n\n# Register the Doc property extension 'has_number' with the getter get_has_number\nDoc.set_extension('has_number', getter=get_has_number, force=True)\n\n# Process the text and check the custom has_number attribute \ndoc = nlp(\"The museum closed for five years in 2012.\")\nprint('has_number:', doc._.has_number)","278b0e5c":"# Define the method\ndef to_html(span, tag):\n    # Wrap the span text in a HTML tag and return it\n    return '<{tag}>{text}<\/{tag}>'.format(tag=tag, text=span.text)\n\n# Register the Span property extension 'to_html' with the method to_html\nSpan.set_extension('to_html', method=to_html, force=True)\n\n# Process the text and call the to_html method on the span with the tag name 'Srinivas'\ndoc = nlp(\"Welcome to Hello world, this is a sentence.\")\nspan = doc[0:4]\nprint(span._.to_html('Srinivas'))","f5d1e32e":"# Kaggle yet to support medium model. So loaded small here\nnlp= spacy.load('en_core_web_sm')\ndef get_wikipedia_url(span):\n    # Get a Wikipedia URL if the span has one of the labels\n    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n        entity_text = span.text.replace(' ', '_')\n        return \"https:\/\/en.wikipedia.org\/w\/index.php?search=\" + entity_text\n\n# Set the Span extension wikipedia_url using get getter get_wikipedia_url\nSpan.set_extension('wikipedia_url', getter=get_wikipedia_url, force=True)\n\ndoc = nlp(\"In over fifty years from his very first recordings right through to his last album, Valmeti Srinivas was at the vanguard of contemporary culture.\")\n\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n    \nfor ent in doc.ents:\n    if ent.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n    # Print the text and Wikipedia URL of the entity\n        print(ent.text, ent._.wikipedia_url)","b9499491":"capitals = {'Afghanistan': 'Kabul',\n 'Albania': 'Tirana',\n 'Algeria': 'Algiers',\n 'American Samoa': 'Pago Pago',\n 'Andorra': 'Andorra la Vella',\n 'Angola': 'Luanda',\n 'Anguilla': 'The Valley',\n 'Antarctica': '',\n 'Antigua and Barbuda': \"Saint John's\",\n 'Argentina': 'Buenos Aires',\n 'Armenia': 'Yerevan',\n 'Aruba': 'Oranjestad',\n 'Australia': 'Canberra',\n 'Austria': 'Vienna',\n 'Azerbaijan': 'Baku',\n 'Bahamas': 'Nassau',\n 'Bahrain': 'Manama',\n 'Bangladesh': 'Dhaka',\n 'Barbados': 'Bridgetown',\n 'Belarus': 'Minsk',\n 'Belgium': 'Brussels',\n 'Belize': 'Belmopan',\n 'Benin': 'Porto-Novo',\n 'Bermuda': 'Hamilton',\n 'Bhutan': 'Thimphu',\n 'Bolivia (Plurinational State of)': 'Sucre',\n 'Bonaire, Sint Eustatius and Saba': 'Kralendijk',\n 'Bosnia and Herzegovina': 'Sarajevo',\n 'Botswana': 'Gaborone',\n 'Bouvet Island': '',\n 'Brazil': 'Bras\u00edlia',\n 'British Indian Ocean Territory': 'Diego Garcia',\n 'Brunei Darussalam': 'Bandar Seri Begawan',\n 'Bulgaria': 'Sofia',\n 'Burkina Faso': 'Ouagadougou',\n 'Burundi': 'Bujumbura',\n 'Cabo Verde': 'Praia',\n 'Cambodia': 'Phnom Penh',\n 'Cameroon': 'Yaound\u00e9',\n 'Canada': 'Ottawa',\n 'Cayman Islands': 'George Town',\n 'Central African Republic': 'Bangui',\n 'Chad': \"N'Djamena\",\n 'Chile': 'Santiago',\n 'China': 'Beijing',\n 'Christmas Island': 'Flying Fish Cove',\n 'Cocos (Keeling) Islands': 'West Island',\n 'Colombia': 'Bogot\u00e1',\n 'Comoros': 'Moroni',\n 'Congo': 'Brazzaville',\n 'Congo (Democratic Republic of the)': 'Kinshasa',\n 'Cook Islands': 'Avarua',\n 'Costa Rica': 'San Jos\u00e9',\n 'Croatia': 'Zagreb',\n 'Cuba': 'Havana',\n 'Cura\u00e7ao': 'Willemstad',\n 'Cyprus': 'Nicosia',\n 'Czech Republic': 'Prague',\n \"C\u00f4te d'Ivoire\": 'Yamoussoukro',\n 'Denmark': 'Copenhagen',\n 'Djibouti': 'Djibouti',\n 'Dominica': 'Roseau',\n 'Dominican Republic': 'Santo Domingo',\n 'Ecuador': 'Quito',\n 'Egypt': 'Cairo',\n 'El Salvador': 'San Salvador',\n 'Equatorial Guinea': 'Malabo',\n 'Eritrea': 'Asmara',\n 'Estonia': 'Tallinn',\n 'Ethiopia': 'Addis Ababa',\n 'Falkland Islands (Malvinas)': 'Stanley',\n 'Faroe Islands': 'T\u00f3rshavn',\n 'Fiji': 'Suva',\n 'Finland': 'Helsinki',\n 'France': 'Paris',\n 'French Guiana': 'Cayenne',\n 'French Polynesia': 'Papeet\u0113',\n 'French Southern Territories': 'Port-aux-Fran\u00e7ais',\n 'Gabon': 'Libreville',\n 'Gambia': 'Banjul',\n 'Georgia': 'Tbilisi',\n 'Germany': 'Berlin',\n 'Ghana': 'Accra',\n 'Gibraltar': 'Gibraltar',\n 'Greece': 'Athens',\n 'Greenland': 'Nuuk',\n 'Grenada': \"St. George's\",\n 'Guadeloupe': 'Basse-Terre',\n 'Guam': 'Hag\u00e5t\u00f1a',\n 'Guatemala': 'Guatemala City',\n 'Guernsey': 'St. Peter Port',\n 'Guinea': 'Conakry',\n 'Guinea-Bissau': 'Bissau',\n 'Guyana': 'Georgetown',\n 'Haiti': 'Port-au-Prince',\n 'Heard Island and McDonald Islands': '',\n 'Holy See': 'Rome',\n 'Honduras': 'Tegucigalpa',\n 'Hong Kong': 'City of Victoria',\n 'Hungary': 'Budapest',\n 'Iceland': 'Reykjav\u00edk',\n 'India': 'New Delhi',\n 'Indonesia': 'Jakarta',\n 'Iran (Islamic Republic of)': 'Tehran',\n 'Iraq': 'Baghdad',\n 'Ireland': 'Dublin',\n 'Isle of Man': 'Douglas',\n 'Israel': 'Jerusalem',\n 'Italy': 'Rome',\n 'Jamaica': 'Kingston',\n 'Japan': 'Tokyo',\n 'Jersey': 'Saint Helier',\n 'Jordan': 'Amman',\n 'Kazakhstan': 'Astana',\n 'Kenya': 'Nairobi',\n 'Kiribati': 'South Tarawa',\n \"Korea (Democratic People's Republic of)\": 'Pyongyang',\n 'Korea (Republic of)': 'Seoul',\n 'Kuwait': 'Kuwait City',\n 'Kyrgyzstan': 'Bishkek',\n \"Lao People's Democratic Republic\": 'Vientiane',\n 'Latvia': 'Riga',\n 'Lebanon': 'Beirut',\n 'Lesotho': 'Maseru',\n 'Liberia': 'Monrovia',\n 'Libya': 'Tripoli',\n 'Liechtenstein': 'Vaduz',\n 'Lithuania': 'Vilnius',\n 'Luxembourg': 'Luxembourg',\n 'Macao': '',\n 'Macedonia (the former Yugoslav Republic of)': 'Skopje',\n 'Madagascar': 'Antananarivo',\n 'Malawi': 'Lilongwe',\n 'Malaysia': 'Kuala Lumpur',\n 'Maldives': 'Mal\u00e9',\n 'Mali': 'Bamako',\n 'Malta': 'Valletta',\n 'Marshall Islands': 'Majuro',\n 'Martinique': 'Fort-de-France',\n 'Mauritania': 'Nouakchott',\n 'Mauritius': 'Port Louis',\n 'Mayotte': 'Mamoudzou',\n 'Mexico': 'Mexico City',\n 'Micronesia (Federated States of)': 'Palikir',\n 'Moldova (Republic of)': 'Chi\u0219in\u0103u',\n 'Monaco': 'Monaco',\n 'Mongolia': 'Ulan Bator',\n 'Montenegro': 'Podgorica',\n 'Montserrat': 'Plymouth',\n 'Morocco': 'Rabat',\n 'Mozambique': 'Maputo',\n 'Myanmar': 'Naypyidaw',\n 'Namibia': 'Windhoek',\n 'Nauru': 'Yaren',\n 'Nepal': 'Kathmandu',\n 'Netherlands': 'Amsterdam',\n 'New Caledonia': 'Noum\u00e9a',\n 'New Zealand': 'Wellington',\n 'Nicaragua': 'Managua',\n 'Niger': 'Niamey',\n 'Nigeria': 'Abuja',\n 'Niue': 'Alofi',\n 'Norfolk Island': 'Kingston',\n 'Northern Mariana Islands': 'Saipan',\n 'Norway': 'Oslo',\n 'Oman': 'Muscat',\n 'Pakistan': 'Islamabad',\n 'Palau': 'Ngerulmud',\n 'Palestine, State of': 'Ramallah',\n 'Panama': 'Panama City',\n 'Papua New Guinea': 'Port Moresby',\n 'Paraguay': 'Asunci\u00f3n',\n 'Peru': 'Lima',\n 'Philippines': 'Manila',\n 'Pitcairn': 'Adamstown',\n 'Poland': 'Warsaw',\n 'Portugal': 'Lisbon',\n 'Puerto Rico': 'San Juan',\n 'Qatar': 'Doha',\n 'Republic of Kosovo': 'Pristina',\n 'Romania': 'Bucharest',\n 'Russian Federation': 'Moscow',\n 'Rwanda': 'Kigali',\n 'R\u00e9union': 'Saint-Denis',\n 'Saint Barth\u00e9lemy': 'Gustavia',\n 'Saint Helena, Ascension and Tristan da Cunha': 'Jamestown',\n 'Saint Kitts and Nevis': 'Basseterre',\n 'Saint Lucia': 'Castries',\n 'Saint Martin (French part)': 'Marigot',\n 'Saint Pierre and Miquelon': 'Saint-Pierre',\n 'Saint Vincent and the Grenadines': 'Kingstown',\n 'Samoa': 'Apia',\n 'San Marino': 'City of San Marino',\n 'Sao Tome and Principe': 'S\u00e3o Tom\u00e9',\n 'Saudi Arabia': 'Riyadh',\n 'Senegal': 'Dakar',\n 'Serbia': 'Belgrade',\n 'Seychelles': 'Victoria',\n 'Sierra Leone': 'Freetown',\n 'Singapore': 'Singapore',\n 'Sint Maarten (Dutch part)': 'Philipsburg',\n 'Slovakia': 'Bratislava',\n 'Slovenia': 'Ljubljana',\n 'Solomon Islands': 'Honiara',\n 'Somalia': 'Mogadishu',\n 'South Africa': 'Pretoria',\n 'South Georgia and the South Sandwich Islands': 'King Edward Point',\n 'South Sudan': 'Juba',\n 'Spain': 'Madrid',\n 'Sri Lanka': 'Colombo',\n 'Sudan': 'Khartoum',\n 'Suriname': 'Paramaribo',\n 'Svalbard and Jan Mayen': 'Longyearbyen',\n 'Swaziland': 'Lobamba',\n 'Sweden': 'Stockholm',\n 'Switzerland': 'Bern',\n 'Syrian Arab Republic': 'Damascus',\n 'Taiwan': 'Taipei',\n 'Tajikistan': 'Dushanbe',\n 'Tanzania, United Republic of': 'Dodoma',\n 'Thailand': 'Bangkok',\n 'Timor-Leste': 'Dili',\n 'Togo': 'Lom\u00e9',\n 'Tokelau': 'Fakaofo',\n 'Tonga': \"Nuku'alofa\",\n 'Trinidad and Tobago': 'Port of Spain',\n 'Tunisia': 'Tunis',\n 'Turkey': 'Ankara',\n 'Turkmenistan': 'Ashgabat',\n 'Turks and Caicos Islands': 'Cockburn Town',\n 'Tuvalu': 'Funafuti',\n 'Uganda': 'Kampala',\n 'Ukraine': 'Kiev',\n 'United Arab Emirates': 'Abu Dhabi',\n 'United Kingdom of Great Britain and Northern Ireland': 'London',\n 'United States Minor Outlying Islands': '',\n 'United States of America': 'Washington, D.C.',\n 'Uruguay': 'Montevideo',\n 'Uzbekistan': 'Tashkent',\n 'Vanuatu': 'Port Vila',\n 'Venezuela (Bolivarian Republic of)': 'Caracas',\n 'Viet Nam': 'Hanoi',\n 'Virgin Islands (British)': 'Road Town',\n 'Virgin Islands (U.S.)': 'Charlotte Amalie',\n 'Wallis and Futuna': 'Mata-Utu',\n 'Western Sahara': 'El Aai\u00fan',\n 'Yemen': \"Sana'a\",\n 'Zambia': 'Lusaka',\n 'Zimbabwe': 'Harare',\n '\u00c5land Islands': 'Mariehamn'}","042b6cca":"COUNTRIES = ['Afghanistan', '\u00c5land Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia (Plurinational State of)', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'United States Minor Outlying Islands', 'Virgin Islands (British)', 'Virgin Islands (U.S.)', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cabo Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo (Democratic Republic of the)', 'Cook Islands', 'Costa Rica', 'Croatia', 'Cuba', 'Cura\u00e7ao', 'Cyprus',\n 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', \"C\u00f4te d'Ivoire\", 'Iran (Islamic Republic of)', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg',\n 'Macao', 'Macedonia (the former Yugoslav Republic of)', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia (Federated States of)', 'Moldova (Republic of)', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', \"Korea (Democratic People's Republic of)\", 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestine, State of', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Republic of Kosovo', 'R\u00e9union', 'Romania', 'Russian Federation', 'Rwanda',\n 'Saint Barth\u00e9lemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Korea (Republic of)', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom of Great Britain and Northern Ireland', 'United States of America', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela (Bolivarian Republic of)', 'Viet Nam', 'Wallis and Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe']","5655420d":"nlp = English()\np_matcher = PhraseMatcher(nlp.vocab)\npatterns = list(nlp.pipe(COUNTRIES))\np_matcher.add('COUNTRY', None, *patterns)","39e68c94":"# # Kaggle yet to support spacy3.x. So this block does not work. Instead try below blocks\n# # add decorator\n# @Language.component('countries & capitals')\n# def countries_component(doc):\n#     # Create an entity Span with the label 'GPE' for all matches\n#     doc.ents = [Span(doc, start, end, label='GPE')\n#                 for match_id, start, end in matcher(doc)]\n#     return doc\n\n# # Add the component to the pipeline\n# nlp.add_pipe('countries & capitals')\n# print(nlp.pipe_names)","140f93ed":"def countries_component(doc):\n    # Create an entity Span with the label 'GPE' for all matches\n    doc.ents = [Span(doc, start, end, label='GPE')\n                for match_id, start, end in matcher(doc)]\n    return doc\n\n# Add the component to the pipeline\nnlp.add_pipe(countries_component)\nprint(nlp.pipe_names)","b7b54360":"# Getter that looks up the span text in the dictionary of country capitals\nget_capital = lambda span: capitals.get(span.text)\n\n# Register the Span extension attribute 'capital' with the getter get_capital \nSpan.set_extension('capital', getter = get_capital, force=True)","fd325f0d":"# Process the text and print the entity text, label and capital attributes\ndoc = nlp(\"United States of America, Australia, Japan and India have joined hands to form Quad\")\nprint([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])","001ed924":"TEXTS = ['McDonalds is my favorite restaurant.',\n 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n 'People really still eat McDonalds :(',\n 'The McDonalds in Spain has chicken wings. My heart is so happy ',\n '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']","c85a46fe":"# Kaggle yet to support medium model. So loaded small here\nnlp = spacy.load('en_core_web_sm')\n# Process the texts and print the adjectives\nfor text in TEXTS:\n    doc = nlp(text)\n    print([token.text for token in doc if token.pos_ == 'ADJ'])","8bea999c":"# Kaggle yet to support medium model. So loaded small here\nnlp = spacy.load('en_core_web_sm')\n# Process the texts and print the adjectives\nfor doc in nlp.pipe(TEXTS):\n    print([token.text for token in doc if token.pos_ == 'ADJ'])","927e7ea9":"# Process the texts and print the entities\ndocs = [nlp(text) for text in TEXTS]\nentities = [doc.ents for doc in docs]\nprint(*entities)","bdf367ef":"# Process the texts and print the entities\ndocs = list(nlp.pipe(TEXTS))\nentities = [doc.ents for doc in docs]\nprint(*entities)","fa341cb0":"people = ['Valmeti Srinivas', 'Angela Merkel', 'Lady Gaga']\n\n# Create a list of patterns for the PhraseMatcher\npatterns = [nlp(person) for person in people]\npatterns","313d1209":"# Create a list of patterns for the PhraseMatcher\npatterns = list(nlp.pipe(people))\npatterns","5b28ad0e":"# Import the Doc class\nfrom spacy.tokens import Doc\n\n# Register the Doc extension 'author' (default None)\nDoc.set_extension('author', default = None, force=True)\n\n# Register the Doc extension 'book' (default None)\nDoc.set_extension('book', default = None, force=True)","6894c327":"DATA = [('One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.',\n  {'author': 'Franz Kafka', 'book': 'Metamorphosis'}),\n (\"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n  {'author': 'Herman Melville', 'book': 'Moby-Dick or, The Whale'}),\n ('It was the best of times, it was the worst of times.',\n  {'author': 'Charles Dickens', 'book': 'A Tale of Two Cities'}),\n ('The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.',\n  {'author': 'Jack Kerouac', 'book': 'On the Road'}),\n ('It was a bright cold day in April, and the clocks were striking thirteen.',\n  {'author': 'George Orwell', 'book': '1984'}),\n ('Nowadays people know the price of everything and the value of nothing.',\n  {'author': 'Oscar Wilde', 'book': 'The Picture Of Dorian Gray'})]","b9a13dce":"for doc, context in nlp.pipe(DATA, as_tuples=True):\n    # Set the doc._.book and doc._.author attributes from the context\n    doc._.book = context['book']\n    doc._.author = context['author']\n    \n    # Print the text and custom attribute data\n    print(doc.text, '\\n', \"\u2014 '{}' by {}\".format(doc._.book, doc._.author), '\\n')","ee780ae8":"nlp = spacy.load('en_core_web_sm')\ntext = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n\n# Only tokenize the text\ndoc = nlp(text)\n\nprint([(token.text,token.pos_) for token in doc])","874d1796":"# # Only tokenize the text\n# doc = nlp.make_doc(text)\n\n# print([(token.text,token.pos_) for token in doc], '\\n','\\n',doc.pos_)","bf955f0a":"text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n\n# Disable the tagger and parser\nwith nlp.disable_pipes('tagger', 'parser'):\n    # Process the text\n    doc = nlp(text)\n    # Print the entities in the doc\n    print(doc.ents,'\\n','\\n', [(token.text,token.pos_) for token in doc])","09608a13":"nlp = English()\nmatcher = Matcher(nlp.vocab)\n\nTEXTS = ['How to preorder the iPhone X',\n 'iPhone X is coming',\n 'Should I pay $1,000 for the iPhone X?',\n 'The iPhone 8 reviews are here',\n 'our iPhone goes up to 11 today',\n 'I need a new phone! Any tips?']","c387b436":"# Two tokens whose lowercase forms match 'iphone' and 'x'\npattern1 = [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n\n# Token whose lowercase form matches 'iphone' and an optional digit\npattern2 = [{'LOWER': 'iphone'}, {'IS_DIGIT': True, 'OP': '?'}]\n\n# Add patterns to the matcher\nmatcher.add('GADGET',[pattern1])","01568740":"# Create a Doc object for each text in TEXTS\nfor doc in nlp.pipe(TEXTS):\n    # Find the matches in the doc\n    matches = matcher(doc)\n    \n    # Get a list of (start, end, label) tuples of matches in the text\n    entities = [(start, end, 'GADGET') for id, start, end in matches]\n    print(doc.text, entities)  ","49aced94":"TRAINING_DATA = []\n\n# Create a Doc object for each text in TEXTS\nfor doc in nlp.pipe(TEXTS):\n    # Match on the doc and create a list of matched spans\n    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n    # Get (start character, end character, label) tuples of matches\n    entities = [(span.start_char, span.end_char, 'GADGET') for span in spans]\n    \n    # Format the matches as a (doc.text, entities) tuple\n    training_example = (doc.text, {'entities': entities})\n    # Append the example to the training data\n    TRAINING_DATA.append(training_example)\n    \nprint(*TRAINING_DATA, sep='\\n')  ","45eed05d":"# # Kaggle yet to support spacy3.x. So this block does not work. Instead try below blocks\n# # Create a blank 'en' model\n# nlp = spacy.blank('en')\n\n# # Create a new entity recognizer and add it to the pipeline\n# ner = nlp.add_pipe('ner', source=spacy.load('en_core_web_sm'))\n\n# # Add the label 'GADGET' to the entity recognizer\n# ner.add_label('GADGET')","4dd5a00e":"# Create a blank 'en' model\nnlp = spacy.blank('en')\n\n# Create a new entity recognizer and add it to the pipeline\nner = nlp.create_pipe('ner')\nnlp.add_pipe(ner)\n\n# Add the label 'GADGET' to the entity recognizer\nner.add_label('GADGET')","a4c75505":"nlp.pipeline","4eba0b61":"# # Kaggle yet to support spacy3.x. So this block does not work. Instead try below blocks\n\n# import random\n# from spacy.training import Example\n\n# # Start the training\n# nlp.begin_training()\n\n# # Loop for 10 iterations\n# for itn in range(10):\n#     # Shuffle the training data\n#     random.shuffle(TRAINING_DATA)\n#     losses = {}\n    \n#     # Batch the examples and iterate over them\n#     for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n#         for text, annotations in batch:\n#             # create Example\n#             doc = nlp.make_doc(text)\n#             example = Example.from_dict(doc, annotations)\n#             # Update the model\n#             nlp.update([example], losses=losses)\n#             print(losses)","c226aae6":"TEST_DATA = ['Apple is slowing down the iPhone 8 and iPhone X - how to stop it',\n \"I finally understand what the iPhone X 'notch' is for\",\n 'Everything we need to know about the Samsung Galaxy S9',\n 'Looking to compare iPad models? Here\u2019s how the 2018 lineup stacks up',\n 'The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple',\n 'what is the cheapest ipad, especially ipad pro???',\n 'Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics']\n","dff2ada3":"# # Kaggle yet to support spacy3.x. So this block does not work. Instead try below blocks\n# # Process each text in TEST_DATA\n# for doc in nlp.pipe(TEST_DATA):\n#     # Print the document text and entitites\n#     print(doc.text)\n#     print(doc.ents, '\\n\\n')","69c8f409":"TRAINING_DATA = [\n    (\"i went to amsterdem last year and the canals were beautiful\", {'entities': [(10, 19, 'GPE')]}),\n    (\"we should visit Paris once in our life, but the Eiffel Tower is kinda boring\", {'entities': [(17, 22, 'GPE')]}),\n    (\"There's also a Paris in Arkansas, lol\", {'entities': [(15,20, 'GPE'), (24,32,'GPE')]}),\n    (\"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\", {'entities': [(0, 6, 'GPE')]})\n]\n     \nprint(*TRAINING_DATA, sep='\\n')","79a6ab0e":"TRAINING_DATA = [\n    (\"Reddit partners with Patreon to help creators build communities\", \n     {'entities': [(0, 6, 'WEBSITE'), (len('Reddit partners with '), len('Reddit partners with ')+len('Patreon'), 'WEBSITE')]}),\n  \n    (\"PewDiePie smashes weTube record\", \n     {'entities': [(0,len('PewDiePie'),'WEBSITE'),(len('PewDiePie smashes '), len('PewDiePie smashes ')+len('weTube'), 'WEBSITE')]}),\n  \n    (\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\", \n     {'entities': [(0, len('Reddit'), 'WEBSITE')]}),\n    # And so on...\n]\n\nTRAINING_DATA[1][1][\"entities\"]","811098f6":"TRAINING_DATA = [\n    (\"Reddit partners with Patreon to help creators build communities\", \n     {'entities': [(0, 6, 'WEBSITE'), (21, 28, 'WEBSITE')]}),\n  \n    (\"PewDiePie smashes weTube record\", \n     {'entities': [(0,9,'PERSON'), (18, 25, 'WEBSITE')]}),\n  \n    (\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\", \n     {'entities': [(0, 6, 'WEBSITE'), (len('Reddit founder '), len('Reddit founder ')+len('Alexis Ohanian'), 'PERSON')]}),\n    # And so on...\n]\n\nTRAINING_DATA[2][1][\"entities\"]","750acc27":"When we create a doc object, we can access the individual sentences in the doc object using its 'sents' attribute as shown below.","d05bbe7e":"### Setting extension attributes (1)\n\nLet's practice setting some extension attributes. \n\nRemember that if we run our code more than once, we might see an error message that the extension already exists. That's because DataCamp will re-run our code in the same session. To solve this, we can set force=True on set_extension, or reload to start a new Python session.\n\nSteps:\n    \n    - Use Token.set_extension to register is_country (default False).\n    - Update it for \"Spain\" and print it for all tokens.","87c33291":"For the token '_', we can match on the attribute TEXT, LOWER or even SHAPE. All of those are correct. As we can see, paying close attention to the tokenization is very important when working with the token-based Matcher. Sometimes it's much easier to just match exact strings instead and use the PhraseMatcher","d7a94063":"Similarly, we can visualize the dependency and interrelationship of the tokens, we can change the style of the visualization to 'dep'","013fe0b2":"### Extracting countries and relationships\n\nIn the previous exercise, we wrote a script using spaCy's PhraseMatcher to find country names in text. Let's use that country matcher on a longer text, analyze the syntax and update the document's entities with the matched countries. The nlp object has already been created.\n\nSteps:\n    \n    - Iterate over the matches and create a Span with the label \"GPE\" (geopolitical entity).\n    - Overwrite the entities in doc.ents and add the matched span.","fca04109":"In this notebook we use Spacy and explore some of its vast functionality to perform various NLP tasks.","db334908":"    - One more example","2ca207ce":"### Predicting named entities in context\n\nModels are statistical and not always right. Whether their predictions are correct depends on the training data and the text we're processing. Let's take a look at an example. The previously created small English model is available as the variable nlp.\n\nSteps:\n    \n    - Process the text with the nlp object.\n    - Iterate over the entities with the iterator ent and print the entity text and label.","abd71a53":"    - Overwrite the doc.ents with a list of one entity, the \"Valmeti Srinivas\" span.","41261ce1":"    - Try out the new pipeline and process any text with the nlp object \u2013 for example \"This is a sentence.\".","5aee7cf7":"### Complex components\n\nWe will be writing a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents.\n\nSteps:\n    \n    - Define the custom component and apply the matcher to the doc.\n    - Create a Span for each match, assign the label ID for 'ANIMAL' and overwrite the doc.ents with the new spans.","d0cc5a37":"    - Register the Span extension attribute 'capital' with the getter get_capital.","c93f2cd4":"### Debugging patterns (1)\n\nWhy does this pattern not match the tokens \"Silicon Valley\" in the doc?\n\npattern = [{'LOWER': 'silicon'}, {'TEXT': ' '}, {'LOWER': 'valley'}]\n\ndoc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")\n\n\nPossible Answers:\n\n    - The tokens \"Silicon\" and \"Valley\" are not lowercase, so the 'LOWER' attribute won't match.\n    - The tokenizer doesn't create tokens for single spaces, so there's no token with the value ' ' in between.\n    -The tokens are missing an operator 'OP' to indicate that they should be matched exactly once.","bcebad3a":"### Simple components\n\nThe example shows a custom component that prints the character length of a document. Can we complete it?\n\nSteps:\n\n    - Define the component function with the doc's length.","9c2213b7":"### Training multiple labels\n\nHere's a small sample of a dataset created to train a new entity type WEBSITE. We will be doing the labeling by hand. In real life, we probably want to automate this and use an annotation tool \u2013 for example, Brat, a popular open-source solution, or Prodigy, our own annotation tool that integrates with spaCy.\n\nSteps:\n    \n    - Define the entity offsets for the WEBSITE entities in the data. We use len() as we don't want to count the characters.","bdf7319b":"Steps:\n    \n    - Process the (text, context) tuples in DATA using nlp.pipe with as_tuples=True.\n    - Overwrite the doc._.book and doc._.author with the respective info passed in as the context.","27614be1":"Now, let us creating a doc object using some other language. Let us try Teleugu here and to do so, we follow the same steps as we did for English.","3ea2d5a6":"    - Process the text and print the entity text and entity label for the entities in doc.ents.","4968b4eb":"### Lexical attributes\n\nWe will use spaCy's Doc and Token objects, and lexical attributes to find percentages in a text. We will be looking for two subsequent tokens: a number and a percent sign. \n\nSteps:\n\n    - Use the like_num token attribute to check whether a token in the doc resembles a number.\n    - Get the token following the current token in the document. The index of the next token in the doc is token.i + 1.\n    - Check whether the next token's text attribute is a percent sign \"%\".","46a51eee":"    - Next use spacy.load to load the small German model 'de_core_news_sm'.\n    - Process the text and print the document text.","7355b735":"### Building a training loop\n\nLet's write a simple training loop from scratch!\n\nSteps:\n    \n    - Call nlp.begin_training, create a training loop for 10 iterations and shuffle the training data.\n    - Create batches of training data using spacy.util.minibatch and iterate over the batches.\n    - Convert the (text, annotations) tuples in the batch to lists of texts and annotations.\n    - For each batch, use nlp.update to update the model with the texts and annotations.","9ef8b19f":"### Setting up the pipeline\n\nWe will prepare a spaCy pipeline to train the entity recognizer to recognize 'GADGET' entities in a text \u2013 for exampe, \"iPhone X\".\n\nSteps:\n\n    - Create a blank 'en' model, for example using the spacy.blank method.\n    - Add a new entity recognizer using nlp.add_pipe.\n    - Add the new label 'GADGET' to the entity recognizer using the add_label method on the pipeline component.","6ce9f007":"Next:\n    \n    - Process the text and create a doc object.\n    - Iterate over the doc.ents, for all named entities in the model, and print the entity text and label_ attribute.","0854edb9":"This is a great example of how we can add structured data to our spaCy pipeline.","c9ef2c8d":"### Setting extension attributes (2)\n\nLet's try setting some more complex attributes using getters and method extensions. \n\nRemember that if we run our code more than once, we might see an error message that the extension already exists. That's because DataCamp will re-run our code in the same session. To solve this, we can set force=True on set_extension, or reload to start a new Python session. None of this will affect the answer we submit.\n\nSteps:\n    \n    - Define the has_number function .\n    - Use Doc.set_extension to register 'has_number' (getter get_has_number) and print its value.","76fea029":"The tokenizer doesn't create tokens for single spaces, so there's no token with the value ' ' in between.","fa3acdd5":"It only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships.","cf2f76e5":"### Good data vs. bad data\n\nHere's an excerpt from a training set that labels the entity type TOURIST_DESTINATION in traveler reviews.\n\n('i went to amsterdem last year and the canals were beautiful', {'entities': [(10, 19, 'TOURIST_DESTINATION')]})\n('we should visit Paris once in our life, but the Eiffel Tower is kinda boring', {'entities': [(17, 22, 'TOURIST_DESTINATION')]})\n(\"There's also a Paris in Arkansas, lol\", {'entities': []})\n('Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!', {'entities': [(0, 6, 'TOURIST_DESTINATION')]})\n)\n\nWhy is this data and label scheme problematic?\n\nPossible Answers\n\n    - Whether a place is a tourist destination is a subjective judgement and not a definitive category. It will be very difficult for the entity recognizer to learn.\n    - \"Paris\" and \"Arkansas\" should also be labelled as tourist destinations for consistency. Otherwise, the model will be confused.\n    - Rare out-of-vocabulary words like the misspelled \"amsterdem\" shouldn't be labelled as entities.","a586897a":"### Components with extensions\n\nExtension attributes are especially powerful if they're combined with custom pipeline components. We will write a pipeline component that finds country names and a custom extension attribute that returns a country's capital, if available.\n\n\nSteps:\n    \n    - Define the countries_component and create a Span with the label 'GPE' (geopolitical entity) for all matches.\n    - Add the component to the pipeline.","05967e2b":"### Inspecting the pipeline\n\nLet's inspect the small English model's pipeline!\n\nSteps:\n    \n    - Load the en_core_web_sm model and create the nlp object.\n    - Print the names of the pipeline components using nlp.pipe_names.\n    - Print the full pipeline of (name, component) tuples using nlp.pipeline.","45f09ffc":"We now have a pipeline component that uses named entities predicted by the model to generate Wikipedia URLs and adds them as a custom attribute.","500ea408":"### Exploring the model\n\nLet's see how the model performs on unseen data! A list of test texts is available as TEST_DATA.\n\nSteps:\n\n    - Process each text in TEST_DATA using nlp.pipe.\n    - Print the document text and the entities in the text.","bc66dd30":"Steps:\n    \n    - Rewrite the code to use the native token attributes instead of a list of pos_tags.\n    - Loop over each token in the doc and check the token.pos_ attribute.\n    - Use doc[token.i + 1] to check for the next token and its .pos_ attribute.","4285afe4":"### Processing data with context\n\nWe will be using custom attributes to add author and book meta information to quotes.\n\nA list of (text, context) examples is available as the variable DATA. The texts are quotes from famous books, and the contexts dictionaries with the keys 'author' and 'book'. \n\nSteps:\n    \n    - Import the Doc class and use the set_extension method to register the custom attributes 'author' and 'book', which default to None.","2cbf1375":"### Data structures best practices\n\nThe code in this example is trying to analyze a text and collect all proper nouns. If the token following the proper noun is a verb\/ auxillary verb, it should also be extracted. \n\n##### Get all tokens and part-of-speech tags\npos_tags = [token.pos_ for token in doc]\n\nfor index, pos in enumerate(pos_tags):\n    # Check if the current token is a proper noun\n    if pos == 'PROPN':\n        # Check if the next token is a verb\n        if pos_tags[index + 1] == 'VERB':\n            print('Found a verb after a proper noun!')\nQuestion\n\nWhy is the code bad?\n\nPossible Answers\n\n    - The tokens in the result should be converted back to Token objects. This will let we reuse them in spaCy.\n    - It only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships.\n    - pos_ is the wrong attribute to use for extracting proper nouns. we should use tag_ and the 'NNP' and 'NNS' labels instead.","e5922bd8":"    - Process the text and print the entity text, entity label and entity capital for each entity span in doc.ents.","13ba4351":"We can do a lot of very powerful analyses using the tokens and their attributes.","9ce1234f":"Steps:\n    \n    - Match on the doc and create a list of matched spans.\n    - Format each example as a tuple of the text and a dict, mapping 'entities' to the entity tuples.\n    - Append the example to TRAINING_DATA and inspect the printed data.","984b0539":"### Using the Matcher\n\nLet's try spaCy's rule-based Matcher. We will be using the previous example and write a pattern that can match the phrase \"iPhone X\" in the text. \n\nSteps:\n\n    - Import the Matcher from spacy.matcher.\n    - Initialize it with the nlp object's shared vocab.","28c9cc6a":"The numbers printed represent the loss on each iteration, the amount of work left for the optimizer. The lower the number, the better. In real life, we normally want to use a lot more data than this, ideally at least a few hundred or a few thousand examples.","8f90eed4":"    - Next, create a slice of the Doc for the tokens \"tree kangaroos\" and \"tree kangaroos and narwhals\".","a9f7f99b":"Let us try one last language, Spansih.\n\nSteps:\n\n    -Import the Spanish class from spacy.lang.es and create the nlp object.\n    -Create a doc and print its text.","5c4a0aa0":" Whenever we're unsure about the current pipeline, we can inspect it by printing nlp.pipe_names or nlp.pipeline","e917159e":"### Efficient phrase matching\n\nSometimes it's more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things \u2013 like all countries of the world.\n\nWe already have a list of countries (COUNTRIES), so let's use this as the basis of our information extraction script.\n\nSteps:\n    \n    - Import the PhraseMatcher and initialize it with the shared vocab as the variable matcher.\n    - Add the phrase patterns and call the matcher on the doc.","2641e1e3":"### Inspecting word vectors\n\nIn spacy, tokens are represented as vectors. We will use a larger English model, which includes around 20000 word vectors. \n\nSteps:\n\n    - Load the medium 'en_core_web_md' model with word vectors.\n    - Print the vector for \"bananas\" using the token.vector attribute.","37d00d56":"### Vocab, hashes and lexemes\n\nWhy does this code throw an error?\n\nfrom spacy.lang.en import English\nfrom spacy.lang.de import German\n\n#Create an English and German nlp object\n    \n    nlp = English()\n\n#Get the ID for the string 'Srinivas'\n    \n    Srinivas_id = nlp.vocab.strings['Srinivas']\n    print(Srinivas_id)\n\n#Look up the ID for 'Srinivas' in the vocab\n    \n    print(nlp.vocab.strings[Srinivas_id])\n\nProbable Answers:\n\n    - The string 'Srinivas' isn't present in the German vocab, so the hash can't be resolved in the string store.\n    - 'Srinivas' is not a regular word in the English or German dictionary, so it can't be hashed.\n    - nlp is not a valid name. The vocab can only be shared if the nlp objects have the same name.","a83c9642":"### Documents, spans and tokens\n\nWhen we call nlp on a string, spaCy first tokenizes the text and creates a document object. We will learn more about the Doc, as well as its views Token and Span.\n\nSteps:\n    \n    - Import the English language class and create the nlp object.\n    - Process the text and instantiate a Doc object in the variable doc.\n    - Select the first token of the Doc and print its text.","26cc4fc6":"Running the below code will through because teh doc object created usingnlp.make_doc class only tokenizes the text and does not have pos attribute (part of speech component)","6bcbfba6":"    - One more example","78c49a0c":"Spacy provides functionality to visualize the named entities in a document using teh displacy function.","723672c0":"    - Add the new component to the pipeline after the 'ner' component.","e7782f8f":"Steps:\n\n    - Create a pattern that matches the 'TEXT' values of two tokens: \"iPhone\" and \"X\".\n    - Use the matcher.add method to add the pattern to the matcher.","7383737c":"Now let's use those patterns to quickly bootstrap some training data for our model.","8d6de8b6":"### Question\n\nA model was trained with the data we just labelled, plus a few thousand similar examples. After training, it's doing great on WEBSITE, but doesn't recognize PERSON anymore. Why could this be happening?\n\nPossible Answers\n\n    - It's very difficult for the model to learn about different categories like PERSON and WEBSITE.\n    - The training data included no examples of PERSON, so the model learned that this label is incorrect.\n    - The hyperparameters need to be retuned so that both entity types can be recognized.","c67ee463":"We can have a look at the first 100 vocab elemnst in spacy's small model as below.","437274c2":"The same technique is useful for a variety of tasks. For example, we could pass in page or paragraph numbers to relate the processed Doc back to the position in a larger document. Or we could pass in other structured data like IDs referring to a knowledge base.","b70f9f4c":"### Creating training data (1)\n\nspaCy's rule-based Matcher is a great way to quickly create training data for named entity models. A list of sentences is available as the variable TEXTS. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as 'GADGET'.\n\nSteps:\n    \n    - Write a pattern for two tokens whose lowercase forms match 'iphone' and 'x'.\n    - Write a pattern for two tokens: one token whose lowercase form matches 'iphone' and an optional digit using the '?' operator.","e116a93a":"### Entities and extensions\n\nWe will combine custom extension attributes with the model's predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location.\n\nSteps:\n\n    - Define the get_wikipedia_url getter so it only returns the URL if the span's label is in the list of labels.\n    - Set the Span extension 'wikipedia_url' using the getter get_wikipedia_url.\n    - Iterate over the entities in the doc and output their Wikipedia URL.","0575505c":"Steps:\n    \n    - Use Span.set_extension to register 'to_html' (method to_html).\n    - Call it on doc[0:2] with the tag 'Srinivas'.","f66798d9":"Before we train a model with the data, we always want to double-check that our matcher didn't identify any false positives. But that process is still much faster than doing everything manually.","4a0e7a2c":"### Debugging patterns (2)\n\nBoth patterns in this exercise contain mistakes and won't match as expected. Can we fix them?\n\nIf we get stuck, try printing the tokens in the doc to see how the text will be split and adjust the pattern so that each dictionary represents one token.\n\nSteps:\n\n    - Edit pattern1 so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n    - Edit pattern2 so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun.","d1fe2c59":"### Getting Started\n\nWith spaCy, we'll be able to try out some of the 45+ available languages.\n\nWe will start with some basic concepts.\n\nSteps:\n    \n    - Import the English class from spacy.lang.en and create the nlp object.\n    - Passing a string to an NLP object creates a doc. Create a doc and print its text.","10a99212":"After including both examples of the next WEBSITE entities, as well as existing entity types like PERSON, the model now performs much better.","bea9e2aa":"    - Add the length_component to the existing pipeline as the first component.","f2bd4418":"The tokenizer already takes care of splitting off whitespace and each dictionary in the pattern describes one token.","23f2213a":"Custom components are great for adding custom values to documents, tokens and spans, and customizing the doc.ents.","b9ba32ab":"### Writing match patterns\n\nWe will practice writing more complex match patterns using different token attributes and operators. A matcher is already initialized and available as the variable matcher.\n\nSteps:\n    \n    - Write one pattern that only matches mentions of the full iOS versions: \"iOS 7\", \"iOS 11\" and \"iOS 10\".\n    - Write one pattern that only matches forms of \"download\" (tokens with the lemma \"download\"), followed by a token with the part-of-speech tag 'PROPN' (proper noun).\n    - Write one pattern that matches adjectives ('ADJ') followed by one or two 'NOUN's (one noun and one optional noun).","e1c41986":"### What happens when we call nlp?\n\nWhat does spaCy do when we call nlp on a string of text?\n\ndoc = nlp(\"This is a sentence.\")\n\nPossible Answers:\n\n    - Run the tagger, parser and entity recognizer and then the tokenizer.\n    - Tokenize the text and apply each pipeline component in order.\n    - Connect to the spaCy server to compute the result and return it.\n    - Initialize the language, add the pipeline and load in the binary model weights.","73d40ca5":"Note that doc object produces same text as the string but it is a doc object and is fundamental for further steps in NLP. Now, let us try German langauge.","737c6c7b":"Steps:\n\n    - Call the matcher on the doc and store the result in the variable matches.\n    - Iterate over the matches and get the matched span from the start to the end index.","6907d656":"Looks like the model didn't predict \"iPhone X\". Let us create a span for those tokens manually.","a280cca7":"### Docs, spans and entities from scratch\n\nWe will create the Doc and Span objects manually, and update the named entities \u2013 just like spaCy does behind the scenes.\n\nSteps:\n    \n    - Import the Doc and Span classes from spacy.tokens.\n    - Use the Doc class directly to create a doc from the words and spaces.","ba6dbe60":"Steps:\n    \n    - Use Token.set_extension to register 'reversed' (getter function get_reversed).\n    - Print its value for each token.","21c1c4d7":"Running the above 2 blocks of code produces errorbecause Hashes can't be reversed. To prevent this problem, add the word to the new vocab by processing a text or looking up the string, or use the same vocab to resolve the hash back to a string.","afda00d6":"We'll be using spaCy to predict similarities between documents, spans and tokens via the word vectors under the hood.","63eafcbc":"Update the training data to include annotations for the PERSON entities \"PewDiePie\" and \"Alexis Ohanian\".","114e61ac":"We'll learn about spaCy's rule-based matcher, which can help us to find certain words and phrases in text.","354284d6":"Steps:\n    \n    - Disable the tagger and parser using the nlp.disable_pipes method.\n    - Process the text and print all entities in the doc.","975e265a":"### Strings to hashes\n\nSteps:\n    \n    - Look up the string label \"PERSON\" in nlp.vocab.strings to get the hash.\n    - Look up the hash to get back the string.\n    - Look up the string \"cat\" in nlp.vocab.strings to get the hash.\n    - Look up the hash to get back the string.","2908fd96":"### Creating a Doc\n\nLet's create some Doc objects from scratch! \n\nSteps:\n    \n    - Import the Doc from spacy.tokens.\n    - Create a Doc from the words and spaces. Don't forget to pass in the vocab!","80562cc0":"Creating spaCy's objects manually and modifying the entities will come in handy later when we're writing our own information extraction pipelines.","b015515f":"Once the model achieves good results on detecting GPE entities in the traveler reviews, we could add a rule-based component to determine whether the entity is a tourist destination in this context. For example, we could resolve the entities types back to a knowledge base or look them up in a travel wiki.","b43c6ff4":"If we want to visualize the relationships and dependencies for each sentence seperately, we can call the doc.sents object","733d7e5d":"### Processing streams\n\nWe will be using nlp.pipe for more efficient text processing. A list of tweets about a popular American fast food chain are available as the variable TEXTS.\n\nSteps:\n    \n    - Rewrite the example to use nlp.pipe. Instead of iterating over the texts and processing them, iterate over the doc objects yielded by nlp.pipe.","aa9ab2a6":"Steps:\n    \n    - Update the script and get the matched span's root head token.\n    - Print the text of the head token and the span.","2870cf7d":"### Selective processing\n\nWe will use the nlp.make_doc and nlp.disable_pipes methods to only run selected components when processing a text.\n\nSteps:\n\n    - Rewrite the code to only tokenize the text using nlp.make_doc.","485c5fc4":"### Predicting linguistic annotations\n\nWe will now get to try one of spaCy's pre-trained model packages and see its predictions in action. \n\nTo find out what a tag or label means, we can call spacy.explain in the IPython shell. For example: spacy.explain('PROPN') or spacy.explain('GPE').\n\nSteps:\n    \n    - Load small english model\n    - Process the text with the nlp object and create a doc.\n    - For each token, print the token text, the token's .pos_ (part-of-speech tag) and the token's .dep_ (dependency label).","fa9de5b1":"    - Create a Span for \"Valmeti Srinivas\" from the doc and assign it the label \"PERSON\".","83dd4025":"Steps:\n\n    - Import the German class from spacy.lang.de and create the nlp object.\n    - Create a doc and print its text.","b91d9385":"Let's move on to a practical example that uses nlp.pipe to process documents with additional meta data.","0dd7c437":"### Loading models\n\nLet's start by loading a model.\n\nSteps:\n\n    - Import spacy\n    - Use spacy.load to load the small English model 'en_core_web_sm'.\n    - Process the text and print the document text.","14a1eb3c":"The tokenizer turns a string of text into a Doc object. spaCy then applies every component in the pipeline on document, in order.","973177ab":"### Creating training data (2)\n\nLet's use the match patterns we've created in the previous exercise to bootstrap a set of training examples. \n\nSteps:\n\n    - Create a doc object for each text using nlp.pipe and find the matches in it.\n    - Create a list of (start, end, label) tuples for the matches.","bb853b30":"Given a vocabulary element, we can find its hash and vice versa in spacy using its vocab.strings class","db740e8c":"### Comparing similarities\n\nWe will be using spaCy's similarity methods to compare Doc, Token and Span objects and get similarity scores. A similarity score of 1 indicates perfect similarity, whereas 0 indicates no similarity. The medium English model is already available as the nlp object.\n\nSteps:\n\n    - Use the doc.similarity method to compare doc1 to doc2 and print the result.\n    - Use the token.similarity method to compare token1 to token2 and print the result.\n    - Create spans for \"great restaurant\"\/\"really nice bar\".\n    - Use span.similarity to compare them and print the result."}}