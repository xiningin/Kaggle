{"cell_type":{"fa77f354":"code","b76c2b49":"code","8ce79edf":"code","8dd35023":"code","12b35910":"code","b384898f":"code","fb4c3606":"code","c95108c4":"code","51fff71c":"code","45464c07":"code","03ebe0be":"code","705d2364":"code","2345b82c":"code","59f5457d":"code","597bb3be":"code","93a60d09":"code","8145175e":"code","5bc70b4e":"code","595d8614":"code","dc69aeca":"code","1fd5d061":"code","90405616":"code","968a55d3":"code","633fcb63":"code","ed7f0a6c":"code","c67d2fc9":"code","e32fabec":"code","f4776cc0":"code","5bf3b8d2":"code","204f91c8":"code","709c6a33":"code","617848fc":"code","91be266b":"code","3c9f4009":"code","bb29db06":"code","d7210cdb":"code","75ab233d":"code","63da74db":"code","982debf8":"code","24249c0f":"code","e1cb99bb":"code","132cd7d9":"code","dc08f15b":"code","f2ebad95":"code","2ea8ac94":"code","61170f3e":"code","dcc98f66":"code","933339d6":"code","f13d19f6":"code","9f804577":"code","8dc028f2":"code","fc64c55a":"code","9a0e6ce1":"code","a7540163":"code","e1557c68":"code","589011e5":"code","edfa6bab":"code","4af2d3b7":"code","8285ad91":"code","1d1f8bef":"code","a61cd00a":"code","144fe052":"code","90cf19c5":"code","2dfcdf42":"markdown","d66766d7":"markdown","df88268d":"markdown","68e9c8e7":"markdown","b8aeee5a":"markdown","3b7c9a3e":"markdown","56dc808d":"markdown","4f4731c5":"markdown","8951f44c":"markdown","51d778a6":"markdown","4fdc3321":"markdown","7c685323":"markdown","eaaa805f":"markdown","4d8b9dcf":"markdown","ad6933f6":"markdown","3278ce46":"markdown"},"source":{"fa77f354":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns","b76c2b49":"sp_df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","8ce79edf":"sp_df.head()","8dd35023":"sp_df.shape","12b35910":"sp_df.info()","b384898f":"sp_df['stroke'].describe()","fb4c3606":"sns.distplot(sp_df['stroke'])","c95108c4":"sns.distplot(sp_df['bmi'])","51fff71c":"sns.distplot(sp_df['avg_glucose_level'])","45464c07":"sns.distplot(sp_df['heart_disease'])","03ebe0be":"sns.pairplot(sp_df)","705d2364":"#skewness and kurtosis\nprint(f\"Skewness: {sp_df['stroke'].skew()}\" )\nprint(f\"Kurtosis: {sp_df['stroke'].kurt()}\" )","2345b82c":"#correlation matrix\ncorrmat = sp_df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8,annot=True,square=True);","59f5457d":"#missing data\ntotal = sp_df.isnull().sum().sort_values(ascending=False)\npercent = (sp_df.isnull().sum()\/sp_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","597bb3be":"def missing_zero_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = round(df.isnull().mean().mul(100), 2)\n        mz_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mz_table = mz_table.rename(\n        columns = {df.index.name:'col_name', 0 : 'Missing Values', 1 : '% of Total Values'})\n        mz_table['Data_type'] = df.dtypes\n        mz_table = mz_table[\n            mz_table.iloc[:,1] != 0 ].sort_values(\n        '% of Total Values', ascending=False)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n            \"There are \" + str(mz_table.shape[0]) +\n              \" columns that have missing values.\")\n        return mz_table.reset_index()\n","93a60d09":"missing = missing_zero_values_table(sp_df)\nmissing[:20].style.background_gradient(cmap='Reds')","8145175e":"#filling null values with the mean\nsp_df['bmi'].fillna(sp_df['bmi'].mean(), inplace= True)","5bc70b4e":"sp_df.isnull().sum()","595d8614":"#import seaborn as sns\n#sns.countplot(sp_df['stroke'])\n\n\n# Plot the value counts with a bar graph\nsp_df.stroke.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);","dc69aeca":"sp_df.head()","1fd5d061":"#Heart Disease Frequency according to Gender\n\nsp_df.gender.value_counts()","90405616":"# Compare target column with sex column\npd.crosstab(sp_df.stroke,sp_df.gender)","968a55d3":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.gender).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"lightblue\",\"crimson\"]);\n\nplt.title(\"Stroke Frequencey based on Gender\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\", \"Male\",\"Other\"])\nplt.xticks(rotation=0); #  labels on the x-axis is kept vertical","633fcb63":"sp_df['smoking_status'].value_counts()","ed7f0a6c":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.smoking_status).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"lightblue\",\"crimson\",\"orange\"]);\nplt.title(\"Stroke Frequencey based on Smoking Status\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","c67d2fc9":"sp_df['work_type'].value_counts()","e32fabec":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.work_type).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"lightblue\",\"crimson\",\"orange\",\"blue\"]);\nplt.title(\"Stroke Frequencey based on Work Type\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","f4776cc0":"sp_df[\"hypertension\"].value_counts()","5bf3b8d2":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.hypertension).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"crimson\",]);\nplt.title(\"Stroke Frequencey based on Hyper Tension\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.legend([\"HyperTension\",\"No HyperTension\"])\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","204f91c8":"sp_df[\"heart_disease\"].value_counts()","709c6a33":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.hypertension).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"crimson\",]);\nplt.title(\"Stroke Frequencey based on Heart Disease\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.legend([\"HeartDisease\",\"No HeartDisease\"])\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","617848fc":"sp_df[\"Residence_type\"].value_counts()","91be266b":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.Residence_type).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"crimson\",]);\nplt.title(\"Stroke Frequencey based on Residence type\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","3c9f4009":"sp_df[\"ever_married\"].value_counts()","bb29db06":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.ever_married).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"crimson\",]);\nplt.title(\"Stroke Frequencey based on Martial Status\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.legend([\"Married\",\"Not Married\"])\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","d7210cdb":"sp_df.head()","75ab233d":"#Handling Categorical Variables\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\nsp_df['gender'] = label.fit_transform(sp_df['gender'])\nsp_df['ever_married'] = label.fit_transform(sp_df['ever_married'])\nsp_df['work_type']= label.fit_transform(sp_df['work_type'])\nsp_df['Residence_type']= label.fit_transform(sp_df['Residence_type'])\n","63da74db":"sp_df['smoking_status'] = label.fit_transform(sp_df['smoking_status'])","982debf8":"#standardizing the dataset with Standard Scaler\nfrom sklearn.preprocessing import StandardScaler \n  \nscalar = StandardScaler() \n  \nscalar.fit(sp_df) \nscaled_data = scalar.transform(sp_df)","24249c0f":"sp_df.head()","e1cb99bb":"# modelling\n# Reference : https:\/\/www.kaggle.com\/neisha\/heart-disease-prediction-using-logistic-regression\n\nfrom statsmodels.tools import add_constant as add_constant\n\nstroke_df_constant = add_constant(sp_df)\nstroke_df_constant.head()","132cd7d9":"import statsmodels.api as sm\nimport scipy.stats as st\n\nst.chisqprob = lambda chisq, df: st.chi2.sf(chisq, df)\ncols = stroke_df_constant.columns[:-1]\nmodel = sm.Logit(sp_df.stroke,stroke_df_constant[cols])\nresult = model.fit()\nresult.summary()","dc08f15b":"def back_feature_elem (data_frame,dep_var,col_list):\n    \"\"\" Takes in the dataframe, the dependent variable and a list of column names, runs the regression repeatedly eleminating feature with the highest\n    P-value above alpha one at a time and returns the regression summary with all p-values below alpha\"\"\"\n\n    while len(col_list)>0 :\n        model=sm.Logit(dep_var,data_frame[col_list])\n        result=model.fit(disp=0)\n        largest_pvalue=round(result.pvalues,3).nlargest(1)\n        if largest_pvalue[0]<(0.05):\n            return result\n            break\n        else:\n            col_list=col_list.drop(largest_pvalue.index)\n\nresult=back_feature_elem(stroke_df_constant,sp_df.stroke,cols)","f2ebad95":"result.summary()","2ea8ac94":"params = np.exp(result.params)\nconf = np.exp(result.conf_int())\nconf['OR'] = params\npvalue=round(result.pvalues,3)\nconf['pvalue']=pvalue\nconf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\nprint ((conf))","61170f3e":"y = sp_df['stroke']\nX = sp_df.drop(['stroke'],axis=1)\n","dcc98f66":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5)","933339d6":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","f13d19f6":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","9f804577":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")","8dc028f2":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP\/float(TP+FN)\nspecificity=TN\/float(TN+FP)","fc64c55a":"print('The acuuracy of the model = TP+TN\/(TP+TN+FP+FN) = ',(TP+TN)\/float(TP+TN+FP+FN),'\\n',\n\n'The Missclassification = 1-Accuracy = ',1-((TP+TN)\/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate = TP\/(TP+FN) = ',TP\/float(TP+FN),'\\n',\n\n'Specificity or True Negative Rate = TN\/(TN+FP) = ',TN\/float(TN+FP),'\\n',\n\n'Positive Predictive value = TP\/(TP+FP) = ',TP\/float(TP+FP),'\\n',\n\n'Negative predictive Value = TN\/(TN+FN) = ',TN\/float(TN+FN),'\\n',\n\n'Positive Likelihood Ratio = Sensitivity\/(1-Specificity) = ',sensitivity\/(1-specificity),'\\n',\n\n'Negative likelihood Ratio = (1-Sensitivity)\/Specificity = ',(1-sensitivity)\/specificity)","9a0e6ce1":"y_pred_prob=logreg.predict_proba(x_test)[:,:]\ny_pred_prob_df=pd.DataFrame(data=y_pred_prob, columns=['Prob of no Stroke(0)','Prob of Stroke (1)'])\ny_pred_prob_df.head()","a7540163":"from sklearn.preprocessing import binarize\nfor i in range(1,5):\n    cm2=0\n    y_pred_prob_yes=logreg.predict_proba(x_test)\n    y_pred2=binarize(y_pred_prob_yes,i\/10)[:,1]\n    cm2=confusion_matrix(y_test,y_pred2)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n    ","e1557c68":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Stroke disease classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)","589011e5":"from sklearn import metrics\nmetrics.roc_auc_score(y_test,y_pred_prob_yes[:,1])","edfa6bab":"from sklearn.model_selection import cross_val_score\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nknn_scores = []\nfor k in range(1,21):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    score=cross_val_score(knn_classifier,X,y,cv=10)\n    knn_scores.append(score.mean())\n","4af2d3b7":"\nplt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\nfor i in range(1,21):\n    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","8285ad91":"\nknn_classifier = KNeighborsClassifier(n_neighbors = 12)\nscore=cross_val_score(knn_classifier,X,y,cv=10)","1d1f8bef":"score.mean()","a61cd00a":"from sklearn.ensemble import RandomForestClassifier","144fe052":"randomforest_classifier= RandomForestClassifier(n_estimators=10)\n\nscore=cross_val_score(randomforest_classifier,X,y,cv=10)","90cf19c5":"score.mean()","2dfcdf42":"## Logistic Regression","d66766d7":"## Reference \n\n1. https:\/\/www.kaggle.com\/salmaeng\/statistical-analysis-eda\n2. https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n3. https:\/\/www.kaggle.com\/swatis1\/stroke-prediction\n4. https:\/\/www.kaggle.com\/neisha\/heart-disease-prediction-using-logistic-regression    \n","df88268d":"### Random Forest Classifier","68e9c8e7":"### k-NN","b8aeee5a":"### Confusion Matrix","3b7c9a3e":"### Feature Selection: Backward elemination (P-value approach)\n\n","56dc808d":"From the above statistics it is clear that the model is highly specific than sensitive. The negative values are predicted more accurately than the positives.","4f4731c5":"### Area under the curve","8951f44c":"### ROC Curve","51d778a6":"The confusion Matrix shows 967 correct predictions and 52+3=55 incorrect ones\n\n### True Positives : 0\n### True Negatives : 967\n### False Positives: 3 (Type 1 Error)\n### False Negatives: 52 (Type 2 Error)\n\n","4fdc3321":"### Lower the threshold","7c685323":"#### Accuracy of the model is 94 %","eaaa805f":"Since the model is predicting Stroke disease too many type II errors is not advisable. A False Negative ( ignoring the probability of disease when there actualy is one) is more dangerous than a False Positive in this case. Hence inorder to increase the sensitivity, threshold can be lowered.","4d8b9dcf":"### Model Evaluation \n#### Model Accuracy","ad6933f6":"### Interpreting the results: Odds Ratio, Confidence Intervals and Pvalues","3278ce46":"### Model Evaluation - Statistics"}}