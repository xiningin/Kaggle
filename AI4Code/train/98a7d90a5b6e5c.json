{"cell_type":{"65dc3fc8":"code","39774c5d":"code","a14d748a":"code","cf983ff2":"code","5994a42d":"code","4aef923e":"code","aa01e3e6":"code","5b158a6b":"code","dfb8a4c3":"code","e6095c32":"code","64391beb":"code","0f0a2f22":"code","57c136f1":"code","5abaadad":"code","d94dde6e":"code","d8eb2be5":"markdown","ea378da7":"markdown","2b3097db":"markdown","29f33eef":"markdown","468eaa7b":"markdown"},"source":{"65dc3fc8":"import os\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","39774c5d":"base_dir = '..\/input\/state-farm-distracted-driver-detection'\ntrain_dir = os.path.join(base_dir, \"imgs\/train\")\ntest_dir = os.path.join(base_dir, \"imgs\")\ndata = pd.read_csv(os.path.join(base_dir, 'driver_imgs_list.csv'))\ndata.head()","a14d748a":"data_samples = data.sample(12)\n\nfig, axs = plt.subplots(3, 4, figsize=(17, 12))\nfor i, ax in enumerate(axs.flatten()):\n    ax.imshow(plt.imread(os.path.join(train_dir, data_samples['classname'].iloc[i], data_samples['img'].iloc[i])))\n    ax.axis(False)\n    ax.set_title(data_samples['classname'].iloc[i])","cf983ff2":"args ={\n    \"kind\":\"bar\",\n    \"title\":\"Classes Count\",\n    \"figsize\": (7,5)\n}\n\nclasses_count = data['classname'].value_counts() \nfig = classes_count.plot(**args)","5994a42d":"print(\"Average images per class = {:.3f} with std = {:.3f}\".format(classes_count.mean(),\n                                                          classes_count.std()))","4aef923e":"# constats\nval_ratio = 0.2\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32","aa01e3e6":"train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n    validation_split = val_ratio\n)\n\ntest_gen = tf.keras.preprocessing.image.ImageDataGenerator()","5b158a6b":"train_data = train_gen.flow_from_directory(\n    train_dir,\n    target_size = IMG_SIZE,\n    batch_size = BATCH_SIZE,\n    seed = 42,\n    subset = \"training\"\n)\n\nval_data = train_gen.flow_from_directory(\n    train_dir,\n    target_size = IMG_SIZE,\n    batch_size = BATCH_SIZE,\n    seed = 42,\n    subset = \"validation\"\n)","dfb8a4c3":"def build_model(num_class):\n    inputs = tf.keras.layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n    model = tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights='imagenet', input_tensor=inputs)\n    \n    x = tf.keras.layers.GlobalAveragePooling2D()(model.output)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Dropout(0.2)(x)\n    outputs = tf.keras.layers.Dense(units=num_class, activation=tf.keras.activations.softmax)(x)\n    \n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n    optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n    loss= tf.keras.losses.CategoricalCrossentropy(),\n    metrics=['accuracy','Recall'],\n    )\n    return model","e6095c32":"model = build_model(10)\nmodel.summary()","64391beb":"history = model.fit(train_data,epochs=3,validation_data=val_data)","0f0a2f22":"model.save_weights('my_checkpoint')","57c136f1":"test_data = test_gen.flow_from_directory(\n    test_dir,\n    shuffle = False,\n    target_size = IMG_SIZE,\n    classes = ['test'],\n    batch_size = 32\n)","5abaadad":"preds = model.predict(test_data)","d94dde6e":"test_imgs = os.path.join(base_dir, \"imgs\/test\")\n\ntest_ids = sorted(os.listdir(test_imgs))\npred_df = pd.DataFrame(columns = ['img','c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\nfor i in range(len(preds)):\n    pred_df.loc[i, 'img'] = test_ids[i]\n    pred_df.loc[i, 'c0':'c9'] = preds[i]\n    \npred_df.to_csv('predictions2.csv', index=False)\n","d8eb2be5":"# Modeling\n[keras Applications](https:\/\/keras.io\/api\/applications\/) API provides a set of deep learning models that can be used for prediction, feature extraction, and fine-tuning.\nIn this notebook we will use the EfficientNet model which is devolped by Mingxing Tan and Quoc V. Le of Google Research, Brain team. they proposed a new scaling method that uniformly scales all dimensions of depth, width and resolution of a CNN network using a simple compound coefficient.\n\nThey developed a new baseline network using neural architecture search (NAS). the main building block for the network is the __MBConv__ which was introduced in MobileNetV2 architecture  \n\n<img src=\"https:\/\/amaarora.github.io\/images\/mbconv.png\" style = \"display: block;\n  margin-left: auto; margin-right: auto; width: 300;\"\/>\n  \n <div align=\"center\"><i>MBConv Layer<\/i><\/div>\n <br\/>\n  \n#### The baseline network is called EfficientNet-B0 and the network is scalled to B7\n\n<img src=\"https:\/\/1.bp.blogspot.com\/-DjZT_TLYZok\/XO3BYqpxCJI\/AAAAAAAAEKM\/BvV53klXaTUuQHCkOXZZGywRMdU9v9T_wCLcBGAs\/s1600\/image2.png\" style = \"display: block;\n  margin-left: auto; margin-right: auto; width: 300;\"\/>\n  \n <div align=\"center\"><i>The Baseline Architecture<\/i><\/div>\n <br\/>\n \n <img src=\"https:\/\/1.bp.blogspot.com\/-oNSfIOzO8ko\/XO3BtHnUx0I\/AAAAAAAAEKk\/rJ2tHovGkzsyZnCbwVad-Q3ZBnwQmCFsgCEwYBhgL\/s1600\/image3.png\" width=\"500\" style = \"display: block;\n  margin-left: auto; margin-right: auto; width: 500;\"\/>\n  \n <div align=\"center\"><i>EfficientNet Performance<\/i><\/div>\n <br\/>\n  \n**Additional Resources**\n\nMBConv Block: https:\/\/paperswithcode.com\/method\/inverted-residual-block\n\nGreat Article About EfficientNet: https:\/\/amaarora.github.io\/2020\/08\/13\/efficientnet.html\n\nEfficientNet Orginal Paper: [Rethinking Model Scaling for Convolutional Neural Networks](https:\/\/arxiv.org\/pdf\/1905.11946.pdf)\n\nGoogle AI Blog Post: https:\/\/ai.googleblog.com\/2019\/05\/efficientnet-improving-accuracy-and.html","ea378da7":"# Predictions","2b3097db":"# Introduction\nIn this notebook we will disccus\n* How to load images and do data augmentation with Keras\n* How to use pretrained models and fine-tune them","29f33eef":"# Training Data","468eaa7b":"## ImageDataGenerator\nTensorflow through keras API provides a very easy way to load images and do real-time data augmentation check the [documentation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator) for more details\n"}}