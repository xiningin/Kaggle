{"cell_type":{"824f015d":"code","ccdcefeb":"code","019ab259":"code","cef12324":"code","a17b0564":"code","bd5b172a":"code","9439b797":"code","4986addb":"code","bf3a0f83":"code","5d7b1645":"code","66f68b99":"code","4bc29c25":"code","74834166":"code","3748ae17":"code","904f42fa":"code","c6d4fba0":"code","1cf25dae":"code","69efac52":"code","e1a2a05b":"code","3c4a2c5d":"code","58bb6db2":"code","7157714b":"code","ce0eaefe":"code","f9aaa4f8":"code","5a786cbc":"code","b82925c5":"code","8ae19b7d":"code","63d24737":"code","c69f531b":"code","478527c5":"code","287dfbde":"code","eded5fde":"code","35c200e2":"code","07c0c603":"code","9c886449":"code","0f510c89":"code","efb6dbc0":"code","c95456a7":"code","7493eaa9":"markdown","660db407":"markdown","aab46749":"markdown","6ed515c8":"markdown","54397433":"markdown","163a4dca":"markdown","32debabf":"markdown","476efb46":"markdown","d898fa7d":"markdown","0200452a":"markdown","48901c7b":"markdown","b2e9fd98":"markdown","ed0a7eea":"markdown","ed143374":"markdown","8c436371":"markdown","4c3fb981":"markdown","efdaa468":"markdown","b8deb80c":"markdown"},"source":{"824f015d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport datetime\nimport matplotlib.pyplot as plt\n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\n \nimport seaborn as sns\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ccdcefeb":"sample_df = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/sampleSubmission.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/test.csv')\ndf = pd.read_csv(\"\/kaggle\/input\/bike-sharing-demand\/train.csv\")","019ab259":"df.shape","cef12324":"df.head()","a17b0564":"df.dtypes","bd5b172a":"print(df['season'].value_counts())\nsns.distplot(a= df['season'], kde = False)\n","9439b797":"sns.barplot(data =df, x='season', y = 'count')","4986addb":"sns.lineplot(data =df, x='holiday', y = 'count')\nprint()","bf3a0f83":"sns.barplot(data =df, x='holiday', y = 'count')\nprint(df['holiday'].value_counts())","5d7b1645":"df['weather'].value_counts()","66f68b99":"sns.lineplot(data =df, x='weather', y = 'count')\n","4bc29c25":"sns.barplot(data =df, x='weather', y = 'count')","74834166":"sns.lineplot(data =df, x='workingday', y = 'count')\nprint(df['workingday'].value_counts())","3748ae17":"df['workingday'].value_counts()","904f42fa":"cont_names=['temp','atemp','humidity','windspeed']\n\n        \n#sns.boxplot(train_df['season'])   \ni=0\nfor name in cont_names:\n    i=i+1\n    plt.subplot(2,2,i)\n    sns.boxplot(name,data=df) \n    \nplt.show()","c6d4fba0":"cat_names=['season', 'holiday', 'workingday', 'weather']\n\ni=0\nfor name in cat_names:\n    i=i+1\n    plt.subplot(2,2,i)\n    sns.countplot(name, data=df) \nplt.show()","1cf25dae":"\n\n\n\n#fig, axes = plt.subplots(nrows=1,ncols=1)\n#fig.set_size_inches(12, 10)\nsns.boxplot(data=df,y=\"count\",x=\"season\",orient=\"v\")\n#print(df['count'].describe())\n\n#print(df['count'].skew())","69efac52":"sns.boxplot(data=df,y=\"count\",orient=\"v\")","e1a2a05b":"print(df[df['season']== 1]['count'].mean())\nprint(df[df['season']== 2]['count'].mean())\nprint(df[df['season']== 3]['count'].mean())\nprint(df[df['season']== 4]['count'].mean())","3c4a2c5d":"df.isnull().sum()","58bb6db2":"df[\"date\"] = df.datetime.apply(lambda x : x.split()[0])\ndf['date']","7157714b":"df[\"hour\"] = df.datetime.apply(lambda x : x.split()[1].split(\":\")[0])\n","ce0eaefe":"df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\ndf[\"year\"]=df[\"datetime\"].dt.year\ndf[\"month\"]=df[\"datetime\"].dt.month\ndf[\"day\"]=df[\"datetime\"].dt.day\ndf[\"dayofweek\"]=df[\"datetime\"].dt.dayofweek","f9aaa4f8":"test_df[\"datetime\"] = pd.to_datetime(test_df[\"datetime\"])\ntest_df[\"year\"]=test_df[\"datetime\"].dt.year\ntest_df[\"month\"]=test_df[\"datetime\"].dt.month\ntest_df[\"day\"]=test_df[\"datetime\"].dt.day\ntest_df[\"dayofweek\"]=test_df[\"datetime\"].dt.dayofweek\ntest_df[\"hour\"]=test_df[\"datetime\"].dt.hour","5a786cbc":"df.corr()","b82925c5":"sns.heatmap(df.corr(), annot=True)","8ae19b7d":"df.drop(['casual','registered'], axis =1, inplace = True)","63d24737":"sns.barplot(x=\"hour\",y=\"count\",data=df)\nprint(df['hour'].value_counts())","c69f531b":"sns.barplot(x=\"dayofweek\",y=\"count\",data=df)","478527c5":"sns.barplot(x=\"month\",y=\"count\",data=df)","287dfbde":"from sklearn.model_selection import train_test_split\nX = df.drop(['count', 'datetime', 'atemp','date'], axis=1)\ny = df['count']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","eded5fde":"X_train","35c200e2":"\n\nmodels=[RandomForestRegressor(),AdaBoostRegressor(),BaggingRegressor(),SVR(),KNeighborsRegressor()]\nmodel_names=['RandomForestRegressor','AdaBoostRegressor','BaggingRegressor','SVR','KNeighborsRegressor']\nrmsle=[]\nd={}\nfor model in range (len(models)):\n    clf=models[model]\n    clf.fit(X_train,y_train)\n    test_pred=clf.predict(X_test)\n    rmsle.append(np.sqrt(mean_squared_log_error(test_pred,y_test)))\nd={'Modelling Algo':model_names,'RMSLE':rmsle}   \nd\n    \n\n","07c0c603":"no_of_test=[500]\nparams_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2']}\nclf_rf=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='neg_mean_squared_log_error')\nclf_rf.fit(X_train,y_train)\npred=clf_rf.predict(X_test)\nprint((np.sqrt(mean_squared_log_error(pred,y_test))))","9c886449":"\n\nclf_rf.best_params_\n\n","0f510c89":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_log_error\n\npredictions = clf_rf.predict(X_test)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_test)))\nprint((np.sqrt(mean_squared_log_error(predictions,y_test))))\nprint(predictions)","efb6dbc0":"pred=clf_rf.predict(test_df.drop(['datetime', 'atemp'],axis=1))\npred","c95456a7":"d={'datetime':test_df['datetime'],'count':pred}\nans=pd.DataFrame(d)\nans.to_csv('answer.csv',index=False)","7493eaa9":"Ideally, I should have concatenated both the dataframes and done all manipluations at the same time. ","660db407":"* Mean Absolute Error: 24.071840220385674\n* mean_squared_log_error = 0.3255321535061589 (The metric used by this competiton)","aab46749":"* Weather data is highly imbalaced. Values ranging from 7k to only 1 data point\n","6ed515c8":"Holiday doesnt seem to have that much contribution towards the overall. (both 0 and 1 contrbute almost same)","54397433":" ## Modelling","163a4dca":"Submitting results to the competiton","32debabf":"> *Co-relation is a very useful function. As, it gives a very clear quantative answer about features contribute to the target label and how by much*\n","476efb46":"Quite clear that the data is equally distributed across all seasons.\nAlso, that season does have a effect on bike sales, bike sales are especially low during \"1\".\n","d898fa7d":"**'casual' and 'resgistered'** both show very high correlation with our target variable 'count', including them would result in a very accurate model when applying it on training and validation data. \nBut will perform very poorly, when tested on new data as they wont have data about 'casual' and 'resgistered' as that can only be known after someone has rented the bike","0200452a":"Temp, Atemp and humidity are normally distributed. Although, windspped has a lot of outliers.","48901c7b":"Time for some parameter tuning. Going with Random Forest as it performed best.","b2e9fd98":"These graphs show  how our datetime features that we manipluated might contribute towards the target variable. ","ed0a7eea":"Visual representation of correlation using a heatmap.","ed143374":"Lets convert our date feature to date time and do it more sunnictly.\n","8c436371":"**Checking for missing data\n(No data is missing)**","4c3fb981":"**Exploratory Data Analysis of all our features **","efdaa468":"#  **DATA LEAKAGE**","b8deb80c":"This is one way of converting your datetime feature into valueable feature, but i found much faster way"}}