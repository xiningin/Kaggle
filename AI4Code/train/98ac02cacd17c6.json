{"cell_type":{"10fde1f0":"code","f3c7a0ba":"code","c4664f9d":"code","0670c803":"code","fdb5e09c":"code","71ac2e8a":"code","6d56786b":"code","09b792c4":"code","faf8530b":"code","2dff58d2":"code","e3b56180":"code","d447fcce":"code","d0c703cd":"code","4da1ef62":"code","cd321b21":"code","6b8d9f56":"code","70afe5eb":"code","d4ddd110":"code","350ec028":"code","49c4a2f1":"code","e034db60":"code","b5ee3f74":"code","a948712d":"code","88f76d39":"code","c3e8f253":"code","e48939e9":"markdown","9a787be5":"markdown","22d466b1":"markdown","d6ed104f":"markdown","54f7087e":"markdown","f8bb1bd5":"markdown","36bfc962":"markdown"},"source":{"10fde1f0":"### Upvote if you find it helpful","f3c7a0ba":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n#import torch\n#from torch import nn, optim\nimport seaborn as sns\nfrom pathlib import Path\nimport PIL\nimport json\nimport gc","c4664f9d":"# Read data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\nsample_sub  = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv') ","0670c803":"cat_vars = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7',\n       'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15',\n       'cat16', 'cat17', 'cat18']\ncont_vars = [ 'cont0', 'cont1', 'cont2', 'cont3', 'cont4',\n       'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10']","fdb5e09c":"# LABEL ENCODE\ndef encode_LE(col,train,test):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    \n    train[col] = df_comb[:len(train)].astype('int16')\n    test[col] = df_comb[len(train):].astype('int16')\n    del df_comb; \n    gc.collect()\n    print(col,', ',end='')\n\n    \n# FREQ ENCODE\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        col_dict = df.value_counts(dropna=True, normalize=True).to_dict()\n        col_dict[-1] = -1\n        colname = col+'_FE'\n        df1[colname] = df1[col].map(col_dict)\n        df1[colname] = df1[colname].astype('float32')\n        \n        df2[colname] = df2[col].map(col_dict)\n        df2[colname] = df2[colname].astype('float32')\n        print(colname,', ',end='')","71ac2e8a":"# Label Encode\nencode_LE('cat0',train,test)\nencode_LE('cat11',train,test)\nencode_LE('cat12',train,test)\nencode_LE('cat13',train,test)\nencode_LE('cat14',train,test)\nencode_LE('cat15',train,test)\nencode_LE('cat16',train,test)\nencode_LE('cat17',train,test)\nencode_LE('cat18',train,test)","6d56786b":"# Frequency Encode\nencode_FE(train,test,['cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10'])","09b792c4":"train[cat_vars] = train[cat_vars].astype('category')\ntest[cat_vars] = test[cat_vars].astype('category')","faf8530b":"usecols = cat_vars + cont_vars\ndep_var = 'target'","2dff58d2":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import train_test_split\nX = train[usecols]\ny = train[dep_var]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","e3b56180":"from scipy.stats import uniform as sp_uniform\nparam_test ={'num_leaves': [31,40,50], \n             'max_depth': [10,15,20,25], \n             'n_estimators': [1000,1500,2000,3000],\n             'learning_rate': [0.1,0.15,0.2,0.012,0.01],\n             'reg_alpha': [0, 1e-1, 1, 2],\n             'bagging_fraction': [0.7,0.8, 0.65],\n             'feature_fraction': [0.7,0.8, 0.65],\n             'params' : ['gbdt'],\n             'reg_lambda': [0, 1e-1, 1, 5]}","d447fcce":"import lightgbm as lgb\nclf = lgb.LGBMClassifier(n_estimators=1000,\n                    early_stopping_rounds=500, verbose_eval=50)","d0c703cd":"n_HP_points_to_test = 30\ngs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=n_HP_points_to_test,    \n    cv=5,\n    refit=True,\n    random_state=2021,\n    verbose=True)","4da1ef62":"# %%time\n# gs.fit(train_X[usecols], train_y,\n#        eval_set = [(val_X[usecols], val_y)], verbose = 50, \n#         early_stopping_rounds=500, eval_metric='auc')\n# print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","cd321b21":"params = {}\nparams[\"objective\"] = \"binary\"\nparams[\"boosting\"] = \"gbdt\"\nparams['metric']= \"AUC\",\nparams[\"max_depth\"] = 20\nparams[\"min_data_in_leaf\"] = 1\nparams[\"min_child_samples\"] = 45\nparams[\"reg_alpha\"] =  4.20\nparams[\"reg_lambda\"] = 6.34\nparams[\"learning_rate\"] = 0.01\nparams[\"bagging_fraction\"] = 0.65\nparams[\"feature_fraction\"] = 0.65\nparams[\"reg_lambda\"] = 0.1\nparams[\"reg_alpha\"] = 0\nparams[\"num_leaves\"] = 223 #50\nparams[\"n_estimators\"] = 6000\nparams[\"cat_smooth\"] = 74\nparams[\"nthread\"] =  4\nparams[\"verbosity\"] = -1\nparams['early_stopping_rounds'] = 500\nnum_rounds = 500\n","6b8d9f56":"%%time\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\ncv_scores = []\npred_test_full = 0\nooflgb = np.zeros(train.shape[0])\npredictionslgb= np.zeros(test.shape[0])\n\nfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=2021)\ni=1\n\nfor dev_index, val_index in fold.split(train[usecols],train[dep_var]):    \n\n    dev_X, val_X = train[usecols].loc[dev_index,:], train[usecols].loc[val_index,:]\n    dev_y, val_y = train[dep_var][dev_index], train[dep_var][val_index]\n    lgtrain = lgb.Dataset(dev_X, label=dev_y)\n    lgtest = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, num_rounds, \n                          valid_sets=[lgtest], early_stopping_rounds=100, verbose_eval=50)\n    \n    pred_val  = model.predict(val_X, num_iteration=model.best_iteration)\n    pred_test = model.predict(test[usecols], num_iteration=model.best_iteration)\n      \n    ooflgb[val_index] = pred_val\n    predictionslgb += pred_test\n    \npredictionslgb \/= 5.\n","70afe5eb":"predictionslgb","d4ddd110":"%%time\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\nfrom catboost import CatBoostClassifier\n\n\ncategorical_features_indices = np.where(X.dtypes =='category')[0]\ncategorical_features_indices\noofcat = np.zeros(X.shape[0])\n\nerrcb=[]\ny_pred_totcb=[]\ny_pred_totcb = 0 \n\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=2021)\ni=1\nfor train_index, test_index in fold.split(X,y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    m=CatBoostClassifier(n_estimators=6000,random_state=2021,\n                         eval_metric='AUC',max_depth=6,\n                         learning_rate=0.01,od_wait=50\n                     ,l2_leaf_reg=10,cat_features=categorical_features_indices,\n                         bagging_temperature=0.80,random_strength=100,\n                     use_best_model=True)\n    \n    m.fit(X_train,y_train,eval_set=[(X_test, y_test)], early_stopping_rounds=100,verbose=100)\n    \n    oofcat[test_index] = m.predict_proba(X_test)[:,-1]\n    #preds=m.predict(X_test)[:,-1]\n\n    p = m.predict_proba(test[usecols])[:,-1]\n    #y_pred_totcb.append(p)\n    y_pred_totcb += p\n\ny_pred_totcb = y_pred_totcb\/5 ","350ec028":"# sample_sub['target'] = pd.DataFrame(predictionslgb).rank(pct=True)\n# sample_sub.to_csv('submission.csv',index=False)","49c4a2f1":"#sample_sub['target'] = y_pred_totcb\nsample_sub['target'] = pd.DataFrame(y_pred_totcb).rank(pct=True) * 0.60 + pd.DataFrame(predictionslgb).rank(pct=True) * 0.40\nsample_sub.to_csv('submission.csv',index=False)","e034db60":"y_pred = sample_sub['target']\nplt.figure(figsize=(8,4))\nplt.hist(oofcat[np.where(y == 0)], bins=100, alpha=0.75, label='neg class')\nplt.hist(oofcat[np.where(y == 1)], bins=100, alpha=0.75, label='pos class')\nplt.legend()\nplt.show()","b5ee3f74":"y_pred = sample_sub['target']\nplt.figure(figsize=(8,4))\nplt.hist(ooflgb[np.where(y == 0)], bins=100, alpha=0.75, label='neg class')\nplt.hist(ooflgb[np.where(y == 1)], bins=100, alpha=0.75, label='pos class')\nplt.legend()\nplt.show()","a948712d":"from sklearn.model_selection import train_test_split\n\nrow_to_show = 5\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\ndata_for_prediction = val_X.iloc[row_to_show]  \ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\nm.predict_proba(data_for_prediction_array)","88f76d39":"import shap\nexplainer = shap.TreeExplainer(m)\nshap_values = explainer.shap_values(data_for_prediction_array)","c3e8f253":"shap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction_array)","e48939e9":"#### Read the data","9a787be5":"### Understanding the predictions","22d466b1":"### Tune the parameters","d6ed104f":"## Tabular Playground Series - Mar 2021","54f7087e":"### How did the model do...","f8bb1bd5":"### Train the Models - Cross Validation","36bfc962":"#### The below code creates a blend of a tuned LGBM Model and a CatBoost Model to obtain a good LB Score."}}