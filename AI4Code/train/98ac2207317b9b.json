{"cell_type":{"6ba1625f":"code","bf484e5c":"code","93d63977":"code","ea98d99f":"code","068d01e9":"code","34e46a48":"code","a26021a0":"code","20c4d6c5":"code","4c5f9e49":"code","fda172a0":"code","8948febc":"code","e905d2e9":"code","2856f722":"code","a6d8188e":"code","db48eece":"code","dd160dde":"code","78d832fc":"code","acea08b4":"code","18b25729":"code","db35d24e":"code","117cc3b1":"code","e60c5e1e":"code","e9c63e95":"code","67a6ffb7":"code","9690030f":"code","473385dc":"code","a2d74301":"code","99117c27":"code","5e2f21b8":"code","8c5fff91":"code","178a776f":"code","083ad74a":"code","f73ac89c":"code","cc457798":"code","7cbf9728":"code","a7ba1984":"code","900dd871":"code","88a91dc7":"code","a9c71373":"code","58d17603":"code","2ae87d54":"code","0ad6ddf6":"markdown","83d1ae8c":"markdown","57ab7633":"markdown","25d1aac2":"markdown","bcc17c95":"markdown","7f29825c":"markdown","f47fad7d":"markdown","07743edd":"markdown","16d9d5b0":"markdown","45b37bfb":"markdown","11632537":"markdown","c66c17ea":"markdown","d48dad19":"markdown"},"source":{"6ba1625f":"import nltk\nfrom nltk import word_tokenize, sent_tokenize\nimport gensim\nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.manifold import TSNE\nimport pandas as pd\nfrom bokeh.io import output_notebook, output_file\nfrom bokeh.plotting import show, figure\n%matplotlib inline","bf484e5c":"nltk.download('punkt')","93d63977":"import string\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nfrom gensim.models.phrases import Phraser, Phrases\nfrom keras.preprocessing.text import one_hot","ea98d99f":"nltk.download('stopwords')","068d01e9":"nltk.download('gutenberg')","34e46a48":"from nltk.corpus import gutenberg\ngberg_sents = gutenberg.sents()","a26021a0":"gberg_sents[4]","20c4d6c5":"[w.lower() for w in gberg_sents[4]]","4c5f9e49":"stpwrds = stopwords.words('english') + list(string.punctuation)","fda172a0":"stpwrds","8948febc":"[w.lower() for w in gberg_sents[4] if w not in stpwrds]","e905d2e9":"stemmer = PorterStemmer()","2856f722":"[stemmer.stem(w.lower()) for w in gberg_sents[4] if w not in stpwrds]","a6d8188e":"phrases = Phrases(gberg_sents)","db48eece":"bigram = Phraser(phrases)","dd160dde":"bigram.phrasegrams ","78d832fc":"\"Jon lives in New York City\".split()","acea08b4":"bigram[\"Jon lives in New York City\".split()]","18b25729":"lower_sents = []\nfor s in gberg_sents:\n    lower_sents.append([w.lower() for w in s if w not in list(string.punctuation)])","db35d24e":"lower_sents[0:5]","117cc3b1":"lower_bigram = Phraser(Phrases(lower_sents))","e60c5e1e":"lower_bigram.phrasegrams","e9c63e95":"lower_bigram[\"jon lives in new york city\".split()]","67a6ffb7":"lower_bigram = Phraser(Phrases(lower_sents, min_count=32, threshold=64))\nlower_bigram.phrasegrams","9690030f":"clean_sents = []\nfor s in lower_sents:\n    clean_sents.append(lower_bigram[s])","473385dc":"clean_sents[0:9]","a2d74301":"clean_sents[6]","99117c27":"model = Word2Vec(sentences=clean_sents, size=64, sg=1, window=10, min_count=10, seed=42, workers=8)\nmodel.save('clean_gutenberg_model.w2v')","5e2f21b8":"model = gensim.models.Word2Vec.load('..\/input\/dataset\/clean_gutenberg_model.w2v')","8c5fff91":"len(model.wv.vocab)","178a776f":"model['ma_am']","083ad74a":"model.most_similar('ma_am') ","f73ac89c":"model.most_similar(positive=['ma_am', 'man'], negative=['woman'])","cc457798":"model.most_similar(positive=['father', 'woman'], negative=['man']) ","7cbf9728":"tsne = TSNE(n_components=2, n_iter=1000)","a7ba1984":"X_2d = tsne.fit_transform(model[model.wv.vocab])","900dd871":"coords_df = pd.DataFrame(X_2d, columns=['x','y'])\ncoords_df['token'] = model.wv.vocab.keys()","88a91dc7":"coords_df.head()","a9c71373":"coords_df.to_csv('clean_gutenberg_tsne.csv', index=False)","58d17603":"coords_df = pd.read_csv('..\/input\/dataset\/clean_gutenberg_tsne.csv')","2ae87d54":"_ = coords_df.plot.scatter('x', 'y', figsize=(12,12), marker='.', s=10, alpha=0.2)","0ad6ddf6":"# remove stopwords and punctuation:","83d1ae8c":"# Iteratively preprocess a sentence:","57ab7633":"# to lowercase:","25d1aac2":"# Reduce word vector dimensionality with t-SNE","bcc17c95":"# Visualise","7f29825c":"# handle bigram collocations:","f47fad7d":"# Load data","07743edd":"# Load dependencies","16d9d5b0":" # Best Practices for Preprocessing Natural Language Data\nIn this notebook, we improve the quality of our Project Gutenberg word vectors by adopting best-practices for preprocessing natural language data.","45b37bfb":"# Explore model","11632537":"# Run word2vec","c66c17ea":"# Preprocess the corpus","d48dad19":"# stem words:"}}