{"cell_type":{"c7aa0a07":"code","f0856df3":"code","118a4851":"code","899d2736":"code","0dd7dadd":"code","9a475ca5":"code","c6a9d830":"code","5d98dd31":"code","38196c4c":"code","fa24e49c":"code","cc96ad6b":"code","b5fc43b0":"code","14927109":"code","659765e2":"code","62275e03":"code","6669e46e":"code","a1be1e17":"code","61ac62cb":"code","b0a66046":"code","d941693c":"code","395d1cf9":"code","522d4345":"code","76788a54":"code","b587d678":"code","70bbbd52":"code","a8e7b155":"code","86cb77d9":"code","6e6eb260":"code","0568e27f":"code","9ee8e8a4":"code","4210adc7":"code","977a63fc":"code","e4917c30":"code","215fe4a0":"code","493823ac":"code","771b2946":"code","56dea576":"code","3c42beae":"code","af596c94":"code","e2aeb8c4":"code","7f06ddae":"code","62150336":"code","53cc8698":"code","14dc632f":"code","0e10bb22":"code","fcb03fc8":"code","e14117b1":"code","6d2a6dde":"code","4b89b745":"code","83fcad97":"code","1fcc1d3d":"code","900a9af2":"code","38d3f170":"code","2c13be27":"code","a9e8360d":"code","8ac19804":"code","e0c03b94":"code","0efdc25e":"code","9bf5a205":"markdown","53672631":"markdown","6677faf0":"markdown","3e188751":"markdown","1897aa70":"markdown","df900478":"markdown","d2423a4a":"markdown","1facd561":"markdown","9c8c454b":"markdown","00b3e20f":"markdown","5b3f93e4":"markdown","3fc422b1":"markdown","e2f084fc":"markdown","253ecb2f":"markdown","9d0a5b7a":"markdown","e1297f8e":"markdown","05888bc3":"markdown","d0f50613":"markdown","5977d860":"markdown"},"source":{"c7aa0a07":"#Vta92\n#from https:\/\/github.com\/vta92\/DS_projects\/blob\/honey_bees_cnn\/honey_bees\/honey_bees.ipynb\n#updated with more architectures. Best accuracy ~ 96%\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#to import images\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","f0856df3":"directory = \"..\/input\/bee_imgs\/bee_imgs\/\"\n#64x64 for faster training.\npicture3 = image.load_img(directory+\"041_073.png\", target_size=(64,64))\npicture3","118a4851":"df = pd.read_csv(\"..\/input\/bee_data.csv\")\n#create a list to hold the 4d image tensors data\nX_pics = [image.load_img(directory+img_name,target_size=(64,64)) for img_name in df[\"file\"]]\n\n#a list of np tensors\nX = [np.array(image.img_to_array(i)) for i in X_pics]\n#rescale for training, using minmax scaling\nX = [i\/255.0 for i in X]","899d2736":"#verified to be in order. Should be identical to the picture above\nX_pics[2] #third picture","0dd7dadd":"#summary of the target\/labels\nprint(df.health.value_counts())\ntarget_ids = []\nfor i in df.health:\n    if i not in target_ids:\n        target_ids.append(i)","9a475ca5":"#doing a label assignment by using a sparse matrix\n#we can also utilize the keras util library for this task\n\ny_keys = {\"healthy\":np.array([1,0,0,0,0,0]),\n         \"few varrao, hive beetles\":np.array([0,1,0,0,0,0]),\n         \"Varroa, Small Hive Beetles\":np.array([0,0,1,0,0,0]),\n         \"ant problems\":np.array([0,0,0,1,0,0]),\n         \"hive being robbed\":np.array([0,0,0,0,1,0]),\n         \"missing queen\":np.array([0,0,0,0,0,1])}\ny = [y_keys[i] for i in df.health]","c6a9d830":"#helper function\n#input as 1 type of target only, return some random indices for image showing\ndef random_imgs(df,num_images,X_pics):\n    index_lst = df[\"file\"].sample(n=num_images,random_state=1).index\n    image_lst = []\n    for i in index_lst:\n        image_lst.append(X_pics[i])\n    return image_lst","5d98dd31":"healthy = random_imgs(df[df[\"health\"]==\"healthy\"],4,X_pics)\nhive_beetles = random_imgs(df[df[\"health\"] == \"few varrao, hive beetles\"],4,X_pics)\nant_probs = random_imgs(df[df[\"health\"] == \"ant problems\"],4,X_pics)\nhive_robbed = random_imgs(df[df[\"health\"] == \"hive being robbed\"],4,X_pics)\nvarroa = random_imgs(df[df[\"health\"] == \"Varroa, Small Hive Beetles\"],4,X_pics)","38196c4c":"\n#only plot 2x2 images. Helper function. One can always generalize the function if neccessary\ndef plot_bees(img_lst,title):\n    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(8,8))\n    ax[0].imshow(img_lst[0])\n    ax[0].set_title(title)\n    ax[1].imshow(img_lst[1])\n    #ax[1].set_title(title)\n    ax[2].imshow(img_lst[2])\n    #ax[2].set_title(title)\n    ax[3].imshow(img_lst[3])\n    #ax[3].set_title(title)\n    \n    plt.show()\n    \n#plot_bees(healthy,\"healthy\")","fa24e49c":"plot_bees(healthy,\"healthy\")\nplot_bees(hive_beetles,\"few varrao, hive beetles\")\nplot_bees(ant_probs,\"ant problems\")\nplot_bees(hive_robbed,\"hive being robbed\")\nplot_bees(varroa,\"Varroa, Small Hive Beetles\")","cc96ad6b":"#Keras CNN\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Activation\nfrom keras.layers import BatchNormalization\nfrom keras import optimizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import callbacks\n\n\nfrom sklearn.model_selection import train_test_split\nhistory = callbacks.History() #need to be defined first","b5fc43b0":"#LeNet's conv->pool->conv patterns\ndef train_cnn():\n    #to combat overfitting, better optimization for CNN, we'll be using Batch normalization PRIOR to activation.\n    #There has been a debate on where to use it, but the consensus has been to use it prior\/after non-linearity (activation)\n    model = Sequential()\n\n    #3x3 matrix with 11 feature maps in total, conventional. 3d array for colored img, RGB. 255 in term of intensity max\/min\n    model.add(Convolution2D(11,3,3, input_shape=(64,64,3)))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2,2),padding=\"SAME\"))\n    \n\n    model.add(Convolution2D(21,3,3, activation=\"relu\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2,2),padding=\"SAME\"))\n\n    #third convo layer with more feature filter size, 41 for better detection.\n    model.add(Convolution2D(41,3,3, activation=\"relu\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2,2),padding=\"SAME\"))\n\n    #Flattening to input the fully connected layers\n    model.add(Flatten())\n\n    #dense layer section with after flattening\n    #hidden layer, 200\n    model.add(Dense(200, activation=\"relu\"))\n    model.add(Dropout(0.2))\n    model.add(Dense(6, activation=\"softmax\"))\n    \n    #smaller learning rate to optimize better. Default has periodic dips\n    model.compile(optimizer=optimizers.rmsprop(lr=0.0001), loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n\n    return model\n","14927109":"#splitting into train,test, val datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y,random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2,stratify=y_train, random_state=1)","659765e2":"#uncomment for training\nmodel1 = train_cnn()\nhistory1 = model1.fit(np.array(X_train),np.array(y_train),validation_data=(np.array(X_val),np.array(y_val)),\n                      verbose=True,shuffle=True,epochs=50)","62275e03":"def model_plot(history,epochs,title,y_range=[0.5,1.0],save=0 ):\n    train_losses = history.history[\"loss\"]\n    val_losses = history.history[\"val_loss\"]\n    plt.plot([i for i in range(0,epochs)],train_losses,val_losses)\n    plt.legend([\"Train Loss\",\"Val Loss\"])\n    plt.title(title)\n    \n    if save == 1:\n        plt.savefig(title+\"_Losses.jpg\")\n    plt.show()\n    \n    \n    train_losses = history.history[\"acc\"]\n    val_losses = history.history[\"val_acc\"]\n    plt.plot([i for i in range(0,epochs)],train_losses,val_losses)\n    plt.legend([\"Train_acc\",\"Val_acc\"])\n    plt.title(title)\n    plt.ylim(y_range)\n    \n    if save == 1:\n        plt.savefig(title+\"_Accuracy.jpg\")\n    plt.show()","6669e46e":"#uncomment for plotting\nmodel_plot(history1,epochs=len(history1.epoch),title=\"baseline_cnn\")","a1be1e17":"def datasets_split(X,y):\n    print(\"original \\n\",pd.Series(y).value_counts(normalize=True))\n    #split out test set and train set. To make things easier, let's make the y into a pandas Series for stratifying\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y,random_state=1)\n    #now from the train set, we split out the train set and the validation set. Remember, we can't validate with\n    #newly generated data. it won't do our model any good!\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2,stratify=y_train, random_state=1)\n    \n    return X_train, X_test, X_val, y_train, y_test, y_val\n\n#example of stratified result\nX_train, X_test, X_val, y_train, y_test, y_val = datasets_split(X,y)\nprint(pd.Series(y_val).value_counts(normalize=True))","61ac62cb":"#Data Augmentation. Generating additional data for testing\n#refer to Keras for extra documentation as well as\n#https:\/\/machinelearningmastery.com\/image-augmentation-deep-learning-keras\/ for a brief introduction\ndef data_aug(X_train):\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True, \n        vertical_flip=True,\n        width_shift_range=0.3,\n        height_shift_range=0.3)\n    \n    datagen.fit(X_train)\n    # fits the model on batches with real-time data augmentation:\n    return datagen\n\ndatagen = data_aug(X_train)","b0a66046":"\n#uncomment for training\nmodel2 = train_cnn()\nhistory2 = model2.fit_generator(datagen.flow(np.array(X_train),np.array(y_train),batch_size=50),\n                              validation_data= (np.array(X_val),np.array(y_val)),\n                              steps_per_epoch=len(X_train) \/ 50,epochs=50)","d941693c":"#uncomment for plotting after training\nmodel_plot(history2,epochs=50,title=\"basic_CNN_with_image_augmentation\")","395d1cf9":"df.head()","522d4345":"#The point of this excercise is to see whether oversampling helps at all, or we are just better off using the image augmentation technique\n#we'll upsample until our each of our target has as many datapoints as the highest target value (healthy)\n#X_pics were defined from the very beginning to contain all our images, and y contains all the targets.\ndef max_oversampling(df,X_pics,y):\n    \n    #we need to resplit the original dataset in a form of dataframe.\n    #this is a total df with additional pixels and target columns\n    \n    df[\"pixels\"] = [np.array(image.img_to_array(i)) for i in X_pics]\n    df[\"pixels\"] = df[\"pixels\"]\/255.0 #we re-imported, so have to rescale (otherwise the cnn won't learn)\n    df[\"target\"] = y\n    \n    #so we'll input the whole df, with pixels and target to help create a true\"oversample\" split solely with\n    #the training data, not contaminating the val or test. We'll ignore the test\/val sets\n    #the df itself (X_train), should contain both the target and pixels.\n    #we only use df_train (including both target and features as added in the above lines)\n    df_train, temp1, temp2, temp3, temp4, temp5 = datasets_split(df,y)\n    #print(df_train.pixels[0:5])\n    #print(df_train.head())\n    max_size = df_train[\"health\"].value_counts().max()\n    lst = [df_train]\n    for classification, group in df_train.groupby('health'):\n        lst.append(group.sample(max_size-len(group), replace=True))\n    df_new = pd.concat(lst)\n    return df_new","76788a54":"#re-split again for no reason, then just to keep the reference the same prior to any partition.\n#X_train, X_test, X_val, y_train, y_test, y_val = datasets_split(X,y)","b587d678":"#df_new is the new training set only, NOT val or test set\ndf_new = max_oversampling(df,X_pics,y)","70bbbd52":"#quite important to shuffle our training set, since our function to upsample only concat the pd's together at the end\n#for each categories. Thus, there is no randomization in our current df based on labels\/target\ndf_new = shuffle(df_new)\nprint(df_new.health.value_counts())\ndf_new.pixels.head()","a8e7b155":"#now we need to extract the right y's from the new training dataframe using the previously defined keys (y_keys dict)\nX_upsampled = df_new.pixels.tolist()\ny_upsampled = [y_keys[i] for i in df_new[\"health\"]]","86cb77d9":"#uncomment for training\nmodel3 = train_cnn()\nhistory3 = model3.fit(np.array(X_upsampled),np.array(y_upsampled),validation_data=(np.array(X_val),np.array(y_val)), verbose=True,shuffle=True,epochs=50)","6e6eb260":"#uncomment for plotting\nmodel_plot(history3,epochs=50,title=\"max_oversampling\")","0568e27f":"print(df_new.health.value_counts())\nprint(df.health.value_counts()) \n#the whole dataset only has 40 missing queens data. So less than 80% of that will be in training set.\n#oversampling these data to 2000+ points won't help it learn from the 30 data points. The same can be applied to other\n#labels\/targets","9ee8e8a4":"#let's create a proportional upsampling, where we don't upsampe the \"healthy\" label, and for every other label, we would\n#upsample them by at most 4x, with the majority label having no multiplier.\n\n#proportional_dict will contain the label (key), and mulplier of upsampling\n#after playing around with the ratio, this is one of the more optimal result we've got\nmultiplier_dict = {\"healthy\":1,\n         \"few varrao, hive beetles\":2, \n         \"Varroa, Small Hive Beetles\":2,\n         \"ant problems\":2,\n         \"hive being robbed\":3,\n         \"missing queen\":4}\ndef proportional_oversampling(df,X_pics,y,multiplier_dict):\n    \n    #we need to resplit the original dataset in a form of dataframe.\n    #this is a total df with additional pixels and target columns\n    \n    df[\"pixels\"] = [np.array(image.img_to_array(i)) for i in X_pics]\n    df[\"pixels\"] = df[\"pixels\"]\/255.0 #normalize due to fresh import of dataframe\n    df[\"target\"] = y\n    #again, we only care about the training oversampling, the val and the test should still be the same!\n    df_train, temp1, temp2, temp3, temp4, temp5 = datasets_split(df,y)\n    \n    label_ids = df_train.health.value_counts().index.tolist() #list\n    label_size = df_train.health.value_counts().values #list\n    print(\"initial train set:\\n\",df_train.health.value_counts())\n    result = [] #a list to hold all the sampled df's\n    for i in range(len(label_ids)):\n        #this function will 1: filter our the label\/target of each \"health\" column,\n        #multiply the number of sample size by the multiplier factor in the dictionary by randomly sampling with replacement\n        #and finally append to the list for concat back to a single training dataframe.\n        df_sampled = df_train[df_train[\"health\"] == label_ids[i]].sample(n=(multiplier_dict[label_ids[i]]*label_size[i]),\n        replace=True,random_state=101)\n        result.append(df_sampled)\n    df_new = pd.concat(result)\n    #Shuffle so our data will come out in batches representing a generalized siutation. Otherwise, there will only\n    #be 1 label for each batch.\n    df_new = shuffle(df_new,random_state=202)\n    return df_new","4210adc7":"#since the last df_new was useless, we'll be re-using this variable name to establish our new baseline\ndf_new = proportional_oversampling(df,X_pics,y,multiplier_dict)","977a63fc":"df_new.health.value_counts()","e4917c30":"#example of stratified result\nX_train, X_test, X_val, y_train, y_test, y_val = datasets_split(X,y)","215fe4a0":"X_upsampled2 = df_new.pixels.tolist()\ny_upsampled2 = [y_keys[i] for i in df_new[\"health\"]]\ndatagen = data_aug(X_upsampled2)\n\n#uncomment for training\n\nmodel4 = train_cnn()\nhistory4 = model4.fit_generator(datagen.flow(np.array(X_upsampled2),np.array(y_upsampled2),batch_size=50),\n                              validation_data=(np.array(X_val),np.array(y_val)),\n                              steps_per_epoch=len(X_train)\/50,epochs=50)","493823ac":"#uncomment for plotting\nmodel_plot(history4,epochs=50,title=\"proportionally oversampled & img augmentation\")","771b2946":"from keras import layers, Input, Model\n#Since both Alex's net and VGG make use of the multiple conv blocks prior to pooling, let's just uniquely\n#modify our structure to take advantage of these new concepts (in a way to fit our 64x64x3 features)\n#and see if we can get any sort of improvement on our previous baseline cnn with augmentation. One could import\n#the architecture from keras and tweak them; but let's build them from scratch!\n\n#AlexNet's and VGG's conv->pool->conv->pool->[[conv->conv->conv->pool]] blocks ->fc\n#pool sizes of 2x2 because we only have 64x64 features for each channel. We'll only be using 1 unit \"block\". Each block and subsequent blocks have the same hyperparameters.\n#needed to reduce down learning rate by 2\/3, we're seeing optimization issues\n#Adam in favor of sgd\n\n#to do: refactor the code with a helper function\ndef conv_block(input_tensor,avg_pool=False):\n    convb = input_tensor\n    for _ in range(3):\n        convb = layers.Conv2D(filters=121,kernel_size=(3,3),padding=\"SAME\")(convb)\n        convb = layers.BatchNormalization()(convb)\n        convb = layers.Activation(\"relu\")(convb)\n    if avg_pool == True:\n        return layers.AveragePooling2D(pool_size=(2,2),padding=\"SAME\")(convb)\n    return layers.MaxPool2D(pool_size=(2,2),padding=\"SAME\")(convb)\n    \n####\n\ndef train_alex_vgg():\n    input_tensor = Input(shape=(64,64,3,))\n    conv1 = layers.Conv2D(filters=31,kernel_size=(7,7),strides=(1,1),padding=\"SAME\")(input_tensor) \n    conv1 = layers.BatchNormalization()(conv1)\n    conv1 = layers.Activation(\"relu\")(conv1)\n    pool1 = layers.MaxPool2D(pool_size=(2,2),padding=\"SAME\")(conv1)\n    \n    conv2 = layers.Conv2D(filters=71,kernel_size=(5,5),padding=\"SAME\")(pool1)\n    conv2 = layers.BatchNormalization()(conv2)\n    conv2 = layers.Activation(\"relu\")(conv2)\n    pool2 = layers.MaxPool2D(pool_size=(2,2),padding=\"SAME\")(conv2)\n    \n    #Block1\n    convb1 = conv_block(pool2,avg_pool=False)    \n    #Block2\n    convb2 = conv_block(convb1,avg_pool=False)    \n    #Block3\n    convb3 = conv_block(convb2,avg_pool=False)\n    #Block4\n    convb4 = conv_block(convb3,avg_pool=True) #1x1 at this point, deeper architecture won't help.\n    \n    flat = layers.Flatten()(convb4)\n    dense1 = layers.Dense(units=1000,activation=\"relu\")(flat)\n    dense1 = layers.Dropout(rate=0.2)(dense1)\n    dense2 = layers.Dense(units=500)(dense1)\n    dense2 = layers.Dropout(rate=0.2)(dense2)\n    dense3 = layers.Dense(units=500)(dense2)\n    dense3 = layers.Dropout(rate=0.2)(dense3)\n    \n    output_tensor = layers.Dense(units=6,activation=\"softmax\")(dense3)\n\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    model.compile(optimizer=optimizers.Adam(lr=0.00003), loss=\"categorical_crossentropy\",metrics=[\"accuracy\"]) #lr needs to be this low, otherwise cannot converge with 10x mag.\n    print(model.summary())\n    \n    return model","56dea576":"model5 = train_alex_vgg()\nhistory5 = model5.fit_generator(datagen.flow(np.array(X_train),np.array(y_train),batch_size=50),\n                              validation_data= (np.array(X_val),np.array(y_val)),\n                              steps_per_epoch=len(X_train)\/50,epochs=100) ","3c42beae":"#uncomment for plotting\nmodel_plot(history5,epochs=len(history5.epoch),title=\"Alexnet_vgg_cnn\",save=1)","af596c94":"#inception-like architecture\n#another popular architecture with multiple branches to learn the spatials features differently.\n#also, we'll be using conv1x1 to reduce the number of channels down to 1 prior to looking.\n#Since deep networks tend to overfit, we'll see how each conv layer of different kernel sizes will eval the data\n#going wide instead of deeper.\n#known to be better than alex net and vgg\n\n#hard-coded like the original block, with dimensional reduction\ndef inception_block(input_tensor):\n    branch1 = layers.Conv2D(filters=5,kernel_size=(1,1),padding=\"SAME\")(input_tensor) \n    branch1 = layers.Conv2D(filters=5,kernel_size=(3,3),padding=\"SAME\")(branch1)\n    \n    branch2 = layers.Conv2D(filters=5,kernel_size=(1,1),padding=\"SAME\")(input_tensor)\n    branch2 = layers.Conv2D(filters=5,kernel_size=(5,5),padding=\"SAME\")(branch2)\n    \n    branch3 = layers.MaxPooling2D(pool_size=(3,3),strides=(1,1),padding=\"SAME\")(input_tensor)\n    branch3 = layers.Conv2D(filters=5,kernel_size=(1,1),padding=\"SAME\")(branch3)\n    #print(branch1.shape,branch2.shape,branch3.shape)\n    output_tensor = layers.concatenate([branch1,branch2,branch3],axis=3)\n    return output_tensor\n\ndef train_inception():\n    input_tensor = Input(shape=(64,64,3,))\n    #we won't go deep into the first stack of convolution due to the input sizes\n    conv1 = layers.Conv2D(filters=64,kernel_size=(3,3),strides=(2,2),padding=\"SAME\")(input_tensor) \n    conv1 = layers.BatchNormalization()(conv1)\n    conv1 = layers.Activation(\"relu\")(conv1)   \n    conv2 = layers.Conv2D(filters=128,kernel_size=(3,3),padding=\"SAME\")(conv1)\n    conv2 = layers.BatchNormalization()(conv2)\n    conv2 = layers.Activation(\"relu\")(conv2)\n    conv3 = layers.Conv2D(filters=128,kernel_size=(3,3),padding=\"SAME\")(conv2)\n    conv3 = layers.BatchNormalization()(conv3)\n    conv3 = layers.Activation(\"relu\")(conv3)\n    pool1 = layers.MaxPooling2D(pool_size=(2,2),padding=\"SAME\")(conv3)\n    \n    inception1 = inception_block(pool1)\n    inception2 = inception_block(inception1)\n    inception3 = inception_block(inception2)\n    inception4 = inception_block(inception3)\n    inception5 = inception_block(inception4)\n    inception6 = inception_block(inception5)\n    inception7 = inception_block(inception6)\n    pool_final = layers.AveragePooling2D(pool_size=(2,2),padding=\"SAME\")(inception6)\n    \n    flat = layers.Flatten()(pool_final)\n    dense1 = layers.Dense(units=500,activation=\"relu\")(flat)\n    dense1 = layers.Dropout(rate=0.2)(dense1)\n    \n    output_tensor = layers.Dense(units=6,activation=\"softmax\")(dense1)\n\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    model.compile(optimizer=optimizers.Adam(lr=0.0001), loss=\"categorical_crossentropy\",metrics=[\"accuracy\"]) #lr needs to be this low, otherwise cannot converge with 10x mag.\n    print(model.summary())\n    return model","e2aeb8c4":"model6 = train_inception()\nhistory6 = model6.fit_generator(datagen.flow(np.array(X_train),np.array(y_train),batch_size=100),\n                              validation_data= (np.array(X_val),np.array(y_val)),\n                              steps_per_epoch=len(X_train)\/100,epochs=100) ","7f06ddae":"#uncomment for plotting\nmodel_plot(history6,epochs=len(history6.epoch),title=\"inception_cnn\",save=1)","62150336":"#Let's put them all together\nmodel_plot(history1,epochs=len(history1.epoch),title=\"baseline_cnn\",y_range=[0.5,1],save=1)\nmodel_plot(history2,epochs=len(history2.epoch),title=\"img_augmentation\",y_range=[0.5,1],save=1)\nmodel_plot(history3,epochs=len(history3.epoch),title=\"max_oversampling\",y_range=[0.5,1],save=1)\nmodel_plot(history4,epochs=len(history4.epoch),title=\"proportional_oversampling_and_img_aug\",y_range=[0.5,1],save=1)\nmodel_plot(history5,epochs=len(history5.epoch),title=\"alex_vgg_block\",y_range=[0.5,1],save=1)\nmodel_plot(history6,epochs=len(history6.epoch),title=\"inception_block\",y_range=[0.5,1],save=1)","53cc8698":"\n#to save all the weights for the future\n'''\nmodel1.save_weights(\"model1.h5\",overwrite=False)\nmodel2.save_weights(\"model2.h5\",overwrite=False)\nmodel3.save_weights(\"model3.h5\",overwrite=False)\nmodel4.save_weights(\"model4.h5\",overwrite=False)\nmodel5.save_weights(\"model5.h5\",overwrite=False)\nmodel6.save_weights(\"model6.h5\",overwrite=False)\n'''","14dc632f":"#to load the saved weights, remember to run all the cnn_defined stuff\n'''\nmodel1 = train_cnn()\nmodel2 = train_cnn()\nmodel3 = train_cnn()\nmodel4 = train_cnn()\nmodel5 = train_alex_vgg()\nmodel6 = train_inception()\n\nmodel1.load_weights(\"model1.h5\")\nmodel2.load_weights(\"model2.h5\")\nmodel3.load_weights(\"model3.h5\")\nmodel4.load_weights(\"model4.h5\")\nmodel5.load_weights(\"model5.h5\")\nmodel6.load_weights(\"model6.h5\")\n'''","0e10bb22":"#input is the list of models\ndef multi_pred(models,X_test,y_test):\n    preds = [] #store all the predictions\n    for model in models:\n        pred_ = model.predict(np.array(X_test))\n        preds.append([np.argmax(i) for i in pred_]) #return the index with maximum probability\n    \n    #transpose due to the fact that dataframe takes in the sample number as columns, and the 3 models as rows\n    preds = pd.DataFrame(data=np.array(preds).T,columns=[\"model2\",\"model3\",\"model4\",\"model5\",\"model6\"])\n    preds[\"target\"] = y_test\n    preds[\"target\"] = preds[\"target\"].apply(np.argmax) #vectorization, no parameter input\n    return preds","fcb03fc8":"preds = multi_pred([model2,model3,model4,model5,model6],X_test,y_test)\npreds.head(10)","e14117b1":"accuracy_2 = accuracy_score(preds[\"model2\"],preds[\"target\"])\naccuracy_3 = accuracy_score(preds[\"model3\"],preds[\"target\"])\naccuracy_4 = accuracy_score(preds[\"model4\"],preds[\"target\"])\naccuracy_5 = accuracy_score(preds[\"model5\"],preds[\"target\"])\naccuracy_6 = accuracy_score(preds[\"model6\"],preds[\"target\"])","6d2a6dde":"print(accuracy_2,accuracy_3,accuracy_4,accuracy_5,accuracy_6)","4b89b745":"#input input is a test_prediction dataframe as defined by \"preds\" above\ndef accuracy_table(df):\n    models_lst = df.columns.tolist() #putting model names to a list\n    models_lst.remove(\"target\")\n    health_cols = [\"healthy\",\"few varrao, hive beetles\",\"Varroa, Small Hive Beetles\",\"ant problem\",\"hive being robbed\", \"missing queen\"]\n    df_acc = []\n    \n    for model in models_lst:\n        acc_lst = []\n        for i in range(6): #i is health from 0 -> 5 as defined previously\n            size = (df[\"target\"] == i).sum()\n            true = ((df[model] == i) &((df[\"target\"] == i))).sum()\n            acc_lst.append(true\/size)\n        #print(acc_lst)\n        df_acc.append(acc_lst) #append each model accuracy into our df\n    df_acc = pd.DataFrame(df_acc,columns=health_cols)\n    df_acc.index = models_lst\n    return df_acc","83fcad97":"df_acc = accuracy_table(preds)\ndf_acc","1fcc1d3d":"from keras.models import load_model\nmodel2.summary()","900a9af2":"\n#now let's grab a sample from previous import. Let's use a \"ant_problem\" label that we had pulled out randomly prev\n#this is crucial because ant problems are generally well-classified as seen above. We'll see what makes our filters\n#to recognize that behavior\nvis_sample = ant_probs[0]\nvis_sample = np.expand_dims(vis_sample,axis=0) #need the extra dimension for processing\nprint(np.shape(vis_sample))\nvis_sample = vis_sample\/255.","38d3f170":"plt.imshow(vis_sample[0])","2c13be27":"from keras import models\nlayer_outputs = [layer.output for layer in model2.layers[:12]] #top 10 layer, look at the summary above (top 10 lines) \nactivation_model = models.Model(inputs=model2.input, outputs=layer_outputs)","a9e8360d":"layer_outputs","8ac19804":"activations = activation_model.predict(vis_sample)","e0c03b94":"#first layer activation\nfirst = activations[0]\n#4th channel of the first layer on the ant-problem picture. There are 11 filters total in the first conv layer\nfor i in range(0,11):\n    plt.matshow(first[0,:,:,i],cmap='viridis')","0efdc25e":"#visualizing every channel in all those conv layers\n#helper function courtesy of Francois Challet in Deep Learning With Python book\nnp.seterr(invalid='warn')\nlayer_names = []\nfor layer in model2.layers[:12]: #up to 11 layers stack\n    layer_names.append(layer.name)\n    #print(layer_names)\n    \nimg_per_row = 10\nfor layer_name, layer_activation in zip(layer_names, activations):\n    n_features = layer_activation.shape[-1]\n    size = layer_activation.shape[1]\n    \n    n_cols = n_features\/\/img_per_row\n    display_grid = np.zeros((size*n_cols,img_per_row*size))\n    \n    for col in range(n_cols):\n        for row in range(img_per_row):\n            channel_img = layer_activation[0,:,:,col*img_per_row+row]\n            channel_img -= channel_img.mean()\n            channel_img \/= channel_img.std()\n            channel_img *64\n            channel_img += 128\n            #channel_img = np.clip(channel_img,0,255).astype(\"uint8\")\n            display_grid[col*size : (col + 1) * size, row*size : (row + 1)*size] = channel_img\n\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","9bf5a205":"As expected, the first conv layer are mainly edge detectors, We have not lost any \"generalization\". As we move further and further down, we should see changes such that the later filters will be focused on \"local\" features: ie, stripes, head, wings, ect...","53672631":"Other improvements and different architectures are done in the recent years, we can take a look at how they can affect the accuracy and improve upon the current models. Generally, these are available as part of the keras pre-trained models. One should not create these from scratch. However, let's create a couple of these architecture \"blocks\" for educational purposes.\n\nWe'll use augmentation alone for these models","6677faf0":"It seems that on average, \"Varrao,hive beetles\" and \"few varrao,hive beetles\" are similar and these 2 are confused by the network. However, it's surprising that we're being able to classified everything else at such a high accuracy! If one had merged the 2 labels mentioned above, it is in no doubt that we'll be able to improve our accuracy a lot more. It also looks like \"missing queen\" hasn't been seen much by model 2.","3e188751":"So what's going on here? It seems that the first layers of filters are generalized edge dectectors. As we move further down the chain, we are facing more and more abstractions, being encoded in localized areas of the bees. However, at a certain point, we see there are blank filters...that means these filters to detect the classification of bee labels didn't get activated. Hence, we have narrowed down our search by classification even further.","1897aa70":"### Oversampling on the dataset","df900478":"### Image Augmentation","d2423a4a":"### Convolution Network","1facd561":"Overall, the model isn't terrible, but our accuracy is very high compared to vanilla ~85% prior to multiple changes such as filters, batch norm, dropout, dense 200 layers, and using Adam optimizer\n\nWhat we have learned on our model ? After playing around with a lot of filter sizes and dropouts\/batch normalization (with the general rules of starting out with small # of filters, and increase them as we make the output of each conv layers smaller, as each filter is now more responsible for more detailed inspection the further down we go), we have a somewhat accurate prediction with an overfitting problem. The data begins to overfit around epoch #10. The model stopped learning when our validation loss stopped decreasing.\n\nAlso, it's worth to note that there's an optimization problem(spikes on val accuracy, perhaps with limited dataset - these images aren't a lot to work it, especially on the minority targets. The fix for this was to reduce the learning rate from 0.001 to 0.0001 and increased the epochs).\n\nLet's improve on the current condition with 2 options: generating extra images on our data set, and\/or upsampling our dataset. Due to the limited computing resources, we will be sticking with the pre-existing architecture for our CNN.","9c8c454b":"It is well-known that we are exposed to overfitting with max upsampling. Copying sub 30 images of a \"missing queen\" category up to 2000+ data point won't help our model. It will overfit as it has only seen so many \"queen\" types images. The problem of high computation and overfitting has been associated with oversampling.\n\nAll in all, the validation loss has stopped very early on. Meaning our network isn't really learning anything new unfortunately. At that point, we also see a big divergence on our overfitting problem.\n\nCase in point, don't use oversampling like this in a cnn","00b3e20f":"### Preditions","5b3f93e4":"### Plotting accuracy per Label of Bee's Health","3fc422b1":"## Other CNN Architectures","e2f084fc":"### Visualizing the Filters","253ecb2f":"similar result to the previous alex\/vgg convolution block technique ~95-96%.","9d0a5b7a":"As expected, image augmentation is a technique widely used for small datasets in cnn, as well as forcing our cnn to learn \"better\" with the small variation changes. The overfitting problem is also mostly gone","e1297f8e":"Summary:\n\ndon't over-use too many filters, as it will overfit our problem.\nThe number of filters should increase as we go deeper, while the img sizes should decrease.\nMake sure we don't bottleneck\/over-noding the fully connected layers. A right # of nodes will help.\nproportional sampling and image augmentation are great way to help our model to predict the problem.\nControl the learning rate to smooth out our benchmark, with the expense of higher epoch count.\nOversampling to the max is not a good way to approach cnn problem for the most part.\nTake advantage of recent architectures and blocks (they are available in keras libraries).\nFilter visualization to help visualize what features of our images are important in specifying a class\/label type.","05888bc3":"Instead of using max oversampling, let's engineer the dataset to use a proportional oversampling. We'll be putting a higher weights on the lesser classes, but not overemphasize them to the point of being on \"balanced\" with the majority. Instead, we'll give them proportionally to the amount of data they have.","d0f50613":"pas mal! we are able to peak around ~95% without much sacrifices on computational cost. This is a huge improvement over the bottleneck at 92% from previous architecture","5977d860":"Unfortunately, due to noise in our training optimization, the model#3 max oversampling seems to have a high value, and the baseline with augmentation has a lower value. However, recall that the more complicated architectures like vgg\/alex\/inception were not overfitting, thus making them the ideal models to be deployed. Oversampling model was extremely overfit, thus should be discarded."}}