{"cell_type":{"bb69049c":"code","f08aa393":"code","05a7f004":"code","499d593c":"code","fe2a3af9":"code","e411a346":"code","87905ad4":"code","d373c1be":"code","6b28cd69":"code","0e220ef2":"code","d4f2be64":"code","17cbe754":"code","bd611368":"code","5534b4cb":"code","00ae219d":"code","4d5b6ba7":"code","6cbff883":"code","3cc08bf8":"code","9b3b8efa":"code","af723b91":"code","67632120":"code","440ab1e5":"code","6c5b7bef":"code","274f3ac8":"code","e70eb6c8":"code","4a52f83d":"code","61213648":"code","0c1341da":"code","5f31ad40":"code","748cf4ae":"code","959c5ca6":"code","ed6d242b":"code","98358d67":"code","7d2b13cf":"code","d01e43ce":"code","cea876c8":"code","506bee81":"code","7150de32":"code","b386c5c4":"code","31169004":"code","7d142edb":"code","a0a99f0d":"code","82dfda1e":"code","cb6c9092":"markdown","272cc6c3":"markdown","f92af1ff":"markdown"},"source":{"bb69049c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f08aa393":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.linear_model import BayesianRidge, LassoLarsIC\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","05a7f004":"# get data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n# sample = pd.read_csv(\"..\/input\/sample_submission.csv\")\nprint(\"train.csv shape: \" + str(train.shape))\nprint(\"test.csv shape: \" + str(test.shape))\n# print(\"sample.csv shape: \" + str(sample.shape))","499d593c":"train.head()","fe2a3af9":"train.SalePrice.describe()","e411a346":"# get a overview of the SalePrice distribution\nsns.distplot(train.SalePrice);","87905ad4":"# look at some general outliers\nplt.scatter(train.GrLivArea, train.SalePrice, c=\"blue\", s=2)\nplt.title(\"Looking for outliers\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","d373c1be":"# remove outliers, i.e. GrLivArea > 4000\ntrain = train.drop(train[(train.GrLivArea > 4000) & (train.SalePrice < 300000)].index)","6b28cd69":"plt.scatter(train.GrLivArea, train.SalePrice, c=\"blue\", s=2);\nplt.title(\"Looking for outliers\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","0e220ef2":"# log transform the SalePrice, so that the bigger values does not have a to big impact\n# on the smaller ones\n#train = train.drop(train.loc[train.Electrical.isnull()].index)\n# save the \"Id\" column\ntrain_ID = train[\"Id\"]\ntest_ID = test[\"Id\"]\n\n# drop the \"Id\" column\ntrain.drop(\"Id\", axis=1, inplace=True)\ntest.drop(\"Id\", axis=1, inplace=True)","d4f2be64":"# build a correlation matrix to get an idea of the important or relevant categories\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, square=True);","17cbe754":"cols = corrmat.nlargest(15, \"SalePrice\")[\"SalePrice\"].index\nvalues = corrmat.nlargest(15, \"SalePrice\")[\"SalePrice\"].values\nhigh_corrmat = np.corrcoef(train[cols].values.T)\nf, ax = plt.subplots(figsize=(10,8))\nsns.set(font_scale=1.25)\nhigh_heatmap = sns.heatmap(high_corrmat, cbar=True, annot=True, fmt=\".2f\",\n                          annot_kws={\"size\": 10}, square=True,\n                          yticklabels=cols.values, xticklabels=cols.values)\nhigh_cor = pd.concat([pd.Series(cols), pd.Series(values)], keys=[\"index\", \"value\"], axis=1)\nprint(high_cor)","bd611368":"train.SalePrice = np.log1p(train.SalePrice)","5534b4cb":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","00ae219d":"# find the missing data\ntotal_na = all_data.isnull().sum()\ntotal_na = total_na.drop(total_na[total_na == 0].index).sort_values(ascending=False)\nmissing = pd.DataFrame({\"Total Missing data\": total_na})\nmissing.head(20)","4d5b6ba7":"# handel missing values for features where median\/mean or most common value does not\n# make sense\n\n# Alley: NA for Alley means \"no alley access\"\nall_data.loc[:, \"Alley\"] = all_data.loc[:, \"Alley\"].fillna(\"NoAl\")\n\n# BedroomAbvGr: NA for Bedrooms above ground means 0 Bedrooms\nall_data.loc[:, \"BedroomAbvGr\"] = all_data.loc[:, \"BedroomAbvGr\"].fillna(0)\n\n# BsmtXXX: NA for basement features means there is \"no basement\"\nall_data.loc[:, \"BsmtQual\"] = all_data.loc[:, \"BsmtQual\"].fillna(\"NoBa\")\nall_data.loc[:, \"BsmtCond\"] = all_data.loc[:, \"BsmtCond\"].fillna(\"NoBa\")\nall_data.loc[:, \"BsmtExposure\"] = all_data.loc[:, \"BsmtExposure\"].fillna(\"NoBa\")\nall_data.loc[:, \"BsmtFinType1\"] = all_data.loc[:, \"BsmtFinType1\"].fillna(\"NoBa\")\nall_data.loc[:, \"BsmtFinType2\"] = all_data.loc[:, \"BsmtFinType2\"].fillna(\"NoBa\")\n\n# Electrical: NA means no electricity\n# Electrical: Should be dropped\n#train = train.drop(train.loc[train.Electrical.isnull()].index)\nall_data.loc[:, \"Electrical\"] = all_data.loc[:, \"Electrical\"].fillna(\"NoEL\")\n\n# Fence: NA means \"no fence\"\nall_data.loc[:, \"Fence\"] = all_data.loc[:, \"Fence\"].fillna(\"NoFe\")\n\n# FireplaceQu: data description says NA means \"no fireplace\"\nall_data.loc[:, \"FireplaceQu\"] = all_data.loc[:, \"FireplaceQu\"].fillna(\"NoFi\")\n\n# GarageType etc: data description says NA for garage features is \"no garage\"\nall_data.loc[:, \"GarageType\"] = all_data.loc[:, \"GarageType\"].fillna(\"NoGa\")\nall_data.loc[:, \"GarageFinish\"] = all_data.loc[:, \"GarageFinish\"].fillna(\"NoGa\")\nall_data.loc[:, \"GarageQual\"] = all_data.loc[:, \"GarageQual\"].fillna(\"NoGa\")\nall_data.loc[:, \"GarageCond\"] = all_data.loc[:, \"GarageCond\"].fillna(\"NoGa\")\n# use for GarageYrBlt the average\n#train.loc[:, \"GarageArea\"] = train.loc[:, \"GarageArea\"].fillna(0)\n#train.loc[:, \"GarageCars\"] = train.loc[:, \"GarageCars\"].fillna(0)\n\n#\n# LotFrontage : NA most likely means no lot frontage\n# to much data missing, try mean()\n#train.loc[:, \"LotFrontage\"] = train.loc[:, \"LotFrontage\"].fillna(0)\n#\n\n# MasVnrType: NA means no veneer\nall_data.loc[:, \"MasVnrType\"] = all_data.loc[:, \"MasVnrType\"].fillna(\"None\")\nall_data.loc[:, \"MasVnrArea\"] = all_data.loc[:, \"MasVnrArea\"].fillna(0)\n\n# MiscFeature: NA means \"no misc feature\"\nall_data.loc[:, \"MiscFeature\"] = all_data.loc[:, \"MiscFeature\"].fillna(\"NoFe\")\n#train.loc[:, \"MiscVal\"] = train.loc[:, \"MiscVal\"].fillna(0)\n\n# PoolQC: NA means \"no pool\"\nall_data.loc[:, \"PoolQC\"] = all_data.loc[:, \"PoolQC\"].fillna(\"NoPo\")\n#train.loc[:, \"PoolArea\"] = train.loc[:, \"PoolArea\"].fillna(0)","6cbff883":"#Some numerical features are actually really categories\nall_data = all_data.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\",\n                                   5 : \"May\", 6 : \"Jun\", 7 : \"Jul\", 8 : \"Aug\",\n                                   9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"},\n                       \n                      })\n# all_data.MSSubClass = all_data.MSSubClass.apply(str)\nall_data.OverallCond = all_data.OverallCond.astype(str)\n# all_data.MoSold = all_data.MoSold.astype(str)\nall_data.YrSold = all_data.YrSold.astype(str)","3cc08bf8":"# encode some categorical features as ordered numbers when there is information in\n# the order\nall_data = all_data.replace({#\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\": {\"NoBa\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4,\n                                   \"Ex\": 5},\n                       \"BsmtExposure\": {\"NoBa\": 0, \"Mn\": 1, \"Av\": 2, \"Gd\": 3},\n                       \"BsmtFinType1\" : {\"NoBa\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\n                                        \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"NoBa\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\n                                        \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"NoBa\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4,\n                                    \"Ex\" : 5},\n                       \n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       # try both\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \n                       \"GarageCond\" : {\"NoGa\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"NoGa\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       # try both\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       # try both\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \n                       #\"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \n                       \"PoolQC\" : {\"NoPo\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \n                       #\"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       # try both\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}\n                            })","9b3b8efa":"# differentiate numerical and categorical features\ncat_features = all_data.select_dtypes(include=[\"object\"]).columns\nnum_features = all_data.select_dtypes(exclude=[\"object\"]).columns\n#num_features = num_features.drop(\"SalePrice\")\nprint(\"Numerical features: \" + str(len(num_features)))\nprint(\"Categorical features: \" + str(len(cat_features)))\nall_data_num = all_data[num_features]\nall_data_cat = all_data[cat_features]","af723b91":"print(\"NAs for numerical features in train : \" + str(all_data_num.isnull().values.sum()))\nall_data_num = all_data_num.fillna(all_data_num.mean())\nprint(\"Remaining NAs for numerical features in train : \" + str(all_data_num.isnull().values.sum()))","67632120":"# log transform of the skewed numerical features to lessen impact the outliers\nskewness = all_data_num.apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\nprint(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\nskewed_features = skewness.index\nall_data_num[skewed_features] = np.log1p(all_data_num[skewed_features])","440ab1e5":"# create dummy features for categorical values via one-hot encoding\nprint(\"NAs for categorical features in train : \" + str(all_data_cat.isnull().values.sum()))\nall_data_cat = pd.get_dummies(all_data_cat)\nprint(\"Remaining NAs for categorical features in train : \" + str(all_data_cat.isnull().values.sum()))","6c5b7bef":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","274f3ac8":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","e70eb6c8":"# join numerical and categorical features\nall_data = pd.concat([all_data_num, all_data_cat], axis=1)\nprint(\"New number of features : \" + str(all_data.shape[1]))\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]\n# # Partition the dataset in train + validation sets\n# X_train, X_test, y_train, y_test = train_test_split(train.values, y_train, test_size = 0.3, random_state = 0)\n# print(\"X_train : \" + str(X_train.shape))\n# print(\"X_test : \" + str(X_test.shape))\n# print(\"y_train : \" + str(y_train.shape))\n# print(\"y_test : \" + str(y_test.shape))\nprint(len(train))\nprint(len(y_train))","4a52f83d":"# stdSc = StandardScaler()\n# train.values.loc[:, num_features] = stdSc.fit_transform(train.values.loc[:, num_features])\n# X_test.loc[:, num_features] = stdSc.transform(X_test.loc[:, num_features])","61213648":"scorer = make_scorer(mean_squared_error, greater_is_better=False)\n\ndef rmse_cv_train(model):\n    rmse = np.sqrt(-cross_val_score(model, train.values, y_train, scoring=scorer, cv=10))\n    return(rmse)","0c1341da":"l1 = [0.00001, 0.00003, 0.00006, 0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60, 100]\nl2 = np.arange(100) + 1\nl3 = l2 * 0.1\nl4 = l2 * 0.01\nl5 = l2 * 0.001\nl6 = l2 * 0.0001\nl7 = l2 * 0.00001\nl8 = l2 * 0.000001\nprint(l3)","5f31ad40":"l2_1 = np.arange(1000) + 1\nl3_1 = l2_1 * 0.1\nl4_1 = l2_1 * 0.01\n","748cf4ae":"ridge = RidgeCV(alphas = [5.63])\nridge.fit(train.values, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha: \", alpha)\nprint(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())","959c5ca6":"lasso = LassoCV(alphas = [0.00031], max_iter=50000, cv=10)\nlasso.fit(train.values, y_train)\n# alpha = lasso.alpha_\n# print(\"Best alpha: \", alpha)\nprint(\"Lasso RMSE on Training set: \", rmse_cv_train(lasso).mean())","ed6d242b":"# 4* ElasticNet\nelasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(train.values, y_train)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(train.values, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(train.values, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\ny_train_ela = elasticNet.predict(train.values)","98358d67":"KRR = KernelRidge(alpha = [600], kernel=\"polynomial\", degree=1.94, coef0=40)\nKRR.fit(train.values, y_train)\nprint(\"Kernel Ridge Regression RMSE on Training set :\", rmse_cv_train(KRR).mean())","7d2b13cf":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nGBoost.fit(train.values, y_train)\nprint(\"XGB RMSE on Training set: \", rmse_cv_train(GBoost).mean())","d01e43ce":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # define clones of ther original models to fit the data\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # train all cloned models\n        for model in self.models_:\n            model.fit(X, y)\n            \n        return self\n    \n    # predictions for cloned models\n    def predict(self, X):\n        predictions = np.column_stack([model.predict(X) for model in self.models_])\n        return np.mean(predictions, axis=1)","cea876c8":"averaged_models = AveragingModels(models = (lasso, elasticNet, KRR, GBoost))\n\nprint(\"Averaged Models RMSE on Training set :\", rmse_cv_train(averaged_models).mean())\n","506bee81":"# averaged2 = AveragingModels(models = (lasso, KRR, GBoost))\n# print(\"Averaged Models RMSE on Training set: \", rmse_cv_train(averaged2).mean())","7150de32":"# averaged3 = AveragingModels(models = (elasticNet, KRR, GBoost))\n# print(\"Averaged Models RMSE on Training set: \", rmse_cv_train(averaged3).mean())\n# averaged4 = AveragingModels(models = (KRR, GBoost))\n# print(\"Averaged Models RMSE on Training set: \", rmse_cv_train(averaged4).mean())\n# averaged5 = AveragingModels(models = (lasso, GBoost))\n# print(\"Averaged Models RMSE on Training set: \", rmse_cv_train(averaged5).mean())","b386c5c4":"def rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","31169004":"averaged_models.fit(train.values, y_train)\naveraged_train_pred = averaged_models.predict(train.values)\naveraged_pred = np.expm1(averaged_models.predict(test.values))\nprint(rmse(y_train, averaged_train_pred))","7d142edb":"sub = pd.DataFrame()\nsub[\"Id\"] = test_ID\nsub[\"SalePrice\"] = averaged_pred\nsub.to_csv(\"submission2.csv\", index=False)","a0a99f0d":"# averaged5.fit(train.values, y_train)\n# averaged5_train_pred = averaged5.predict(train.values)\n# averaged5_pred = np.expm1(averaged5.predict(test.values))\n# print(rmse(y_train, averaged5_train_pred))","82dfda1e":"# sub = pd.DataFrame()\n# sub[\"Id\"] = test_ID\n# sub[\"SalePrice\"] = averaged5_pred\n# sub.to_csv(\"submission.csv\", index=False)","cb6c9092":"Modelling","272cc6c3":"* We can see now which data could be relevant\n* Furthermore you can see that there is some missing data...","f92af1ff":"So fill in the missing gaps of the categories..."}}