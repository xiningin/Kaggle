{"cell_type":{"428d3ce6":"code","0ee2b538":"code","45dd3d85":"code","ce6eae83":"code","320478be":"code","4feaa498":"code","25642d34":"code","c044affd":"code","e9030c1a":"code","33c31016":"code","9207497d":"code","e2bec125":"code","5db0c012":"code","e081d4c5":"code","9c216f33":"code","ccc2b264":"code","e0080a8f":"code","1c8ad192":"code","bf35739c":"code","6692ed8f":"code","c796eb64":"code","5d3d30b2":"code","84444b64":"code","9c53ec40":"code","31c0b6ac":"code","983be52c":"code","47645305":"markdown","a2cbfc0b":"markdown","ea5b1fc4":"markdown","fc0884db":"markdown","0e18a828":"markdown","c94af735":"markdown","77676d1a":"markdown","35648a03":"markdown","8e718650":"markdown","8f1733e7":"markdown","f31d33f7":"markdown","62d8bc10":"markdown","bb14598e":"markdown","c4fdd052":"markdown"},"source":{"428d3ce6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0ee2b538":"import pandas as pd\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","45dd3d85":"train.info()","ce6eae83":"train.head(5)","320478be":"# RETAINING THE 'TEXT' VARIABLE AS WE WILL BE WORKING OFF THAT \n\ntrain = train[['text']]","4feaa498":"# SHALL USE CONTRACTIONS FOR DATA PRE-PROCESSING (EXPLAINED BELOW)\n! pip install contractions","25642d34":"# FUNCTION TO IMPLEMENT BASIC TEXT PREPROCESSING : LOWER-CASING & REMOVING NUMBERS\/PUNCTUATION\n\nimport re\nimport string\nimport contractions\n\n\ndef basic_preprocessor(text):\n    \n    \"\"\"\n    Input, text to be preprocessed in the string format\n    returns the preprocessed text\n    \"\"\"\n  \n    import re\n    import string\n    import contractions\n  \n\n    # EXPANDING OUT CONTRACTIONS\n    # e.g : don't -> do not\n    text = contractions.fix(text)\n\n    # TEXT TO LOWERCASE\n    text = text.lower()\n\n    # The syntax of re.sub() is:: re.sub(pattern, replace, source_string)\n    \n    # CODE TO HANDLE POSSESSIVES ('s) \n    # e.g: movie's -> movie s\n    # THE REASON IS THAT LSTMs ARE ABLE TO PROCESS \"s\" \n    text = re.sub(r\"(\\w+)'s\", r'\\1 s', text)\n  \n    # HANDLING OTHER PUNCTUATION\n    text = re.sub('\\[.*#?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    return text","c044affd":"# PREPROCESSING THE TRAINING SET\ntrain['text'] = train.apply(lambda x: basic_preprocessor(x['text']),axis=1)","e9030c1a":"# DEFINING A FUNCTION THAT CONVERTS A LIST OF SENTENCES TO A VOCABULARY DICTIONARY\n\nfrom tqdm import tqdm # TQDM LIBRARY IS USED TO CREATE A PROGRESS BAR\n\n\ndef build_vocab(sentences):\n\n    \"\"\"\n    Purpose, Converts a list of sentences into a vocabulary dictionary\n    Input, a list of sentences\n    Prints, the size (length) of the vocabulary\n    Returns, a vocabulary dictionary -> {word: frequency}\n    \"\"\"\n\n    length = len(sentences) # NO OF SENTENCES\n    vocab ={} \n\n    for s in tqdm(sentences): # ACCESSING EACH SENTENCE IN A LIST OF SENTENCES       \n        temp_list = [] \n        temp_list = s.split() # LIST OF WORDS IN A SENTENCE\n\n        for word in temp_list:\n            if word not in vocab.keys():\n                vocab[word] = 1\n            else:\n                vocab[word] += 1\n\n    print(\"The size of the vocabulary is : \", len(vocab))\n\n    return vocab","33c31016":"# CONVERTING THE TEXT FIELD INTO A LIST OF SENTENCES\n\nsentences = []\nm = len(train['text'])\n\nfor i in range(m):\n    sentences.append(train['text'].iloc[i])\n\nprint(\"Number of sentences : \", len(sentences))","9207497d":"# CHECKING IF THE LISTS OF LISTS HAS BEEN CREATED\nfor i in range(5):\n    print(\"\\n\", sentences[i])","e2bec125":"# IMPLEMENTING WORD FREQUENCY FUNCTION, STORING RESULTS IN master_vocab VARIABLE\nmaster_vocab = build_vocab(sentences)","5db0c012":"# IMPORTING THE GLOVE 100 DIMENSIONAL EMBEDDINGS\n# THANKS TO: LAURENCE MORONEY, AI ADVOCATE AT GOOGLE  \n\n\n!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/glove.6B.100d.txt \\\n    -O \/tmp\/glove.6B.100d.txt\n\nembeddings_index = {};\n\nwith open('\/tmp\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;","e081d4c5":"# EXPLORING THE EMBEDDINGS_INDEX DICTIONARY\n\nprint(\"Glove vocabulary size: \",len(embeddings_index))\nprint(\"Glove embedding dimensions: \",len(embeddings_index['the']))","9c216f33":"# WRITING A FUNCTION TO DETERMINE THE OVERLAP BETWEEN THE 2 DICTIONARIES\n\nfrom tqdm import tqdm\nimport operator\n\ndef check_overlap(vocab,embeddings):\n    \n    \"\"\"\n    Purpose, to determine the overlap between the corpus and the embeddings disctionary\n    Inputs, 2 dictionaries, one for the vocabulary and the other for the corpus\n    Prints, a set of useful information on the overlap\n    Returns, a sorted version dictionary with out_of_vocabulary words and their frequency\n    \"\"\"\n  \n    not_in_glove = {}\n    total_corpus_words = 0\n    total_oov_words = 0\n\n    for i in tqdm(vocab.keys()):\n        total_corpus_words += vocab[i] \n\n        if i not in embeddings.keys():\n            not_in_glove[i] = vocab[i]\n            total_oov_words += vocab[i]\n\n    x = len(not_in_glove)\n    y = len(vocab)\n    z = len(embeddings)\n\n    print(\"\\n\\nVOCABULARY INSIGHTS:\")\n    print(\"The vocabulary size is (unique word-count) : \",y)\n    print(f\"Embeddings found for {y-x} ({round((y-x)*100\/y,2)})% of the words\")\n  \n    print(\"\\nCORPUS INSIGHTS:\")\n    print(\"The corpus size is (total, non-unique word-count) : \",total_corpus_words)\n    print(f\"{total_oov_words} word(s), representing, {round(total_oov_words*100\/total_corpus_words,2)}% of the corpus vocabulary is unmapped\")\n\n    print(\"\\n Top 20 words (by frequency) not present in embeddings dictionary :\")\n    p = min(len(not_in_glove), 20)\n    print(sorted(not_in_glove, key = lambda x: (-not_in_glove[x], x))[0:p])\n\n    not_in_glove_sorted = sorted(not_in_glove.items(), key=operator.itemgetter(1))[::-1]\n  \n\n    return not_in_glove_sorted","ccc2b264":"# IMPLEMENTING THE FUCNTION (oov -> out of vocabulary)\n\noov = check_overlap(master_vocab,embeddings_index)","e0080a8f":"# ACCESSING THE TOP 20 WORDS\n\noov[0:20]","1c8ad192":"# DEFINE A MIS-SPELLING DICTIONARY\n\nmispell_dict = {\n  'prebreak': 'pre break',\n  'nowplaying': 'now playing',\n  'typhoondevastated': 'typhoon devastated',\n  'lmao': 'funny'\n}","bf35739c":"# AUTHOR: Dr. CRISTOF HENKEL\n\nimport re\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\n\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)","6692ed8f":"# IMPLEMENTING THE FUNCTION \n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: replace_typical_misspell(x))","c796eb64":"# SPECIFYING HYPERPARAMETERS \n# REPRESENTED IN CAPS AS PER CONVENTION\nVOCAB_SIZE = 7000\nEMBEDDING_DIM = 20\nMAX_LENGTH = 15   \nTRUNC_TYPE = 'post'\nOOV_TOKEN = \"<OOV>\"\n\n# IMPORTING LIBRARIES AND SETTING-UP TOKENIZER\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# IMPLEMENTING TOKENIZER\ntokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token = OOV_TOKEN)\ntokenizer.fit_on_texts(train['text'])\n\n# I haven't included the rest of the Tokenizer code","5d3d30b2":"word_index = tokenizer.word_index\n\nprint(\"\\n Data-Type and Length of word_index :\", type(word_index), len(word_index))\nprint(\"\\n Value for a common word 'the' :\", word_index['the'])\nprint(\"\\n Value for a less common word 'india' :\", word_index['india'])\nprint(\"\\n Value for an oov word 'mtvhottest' :\", word_index['mtvhottest'])\n","84444b64":"# A FUNCTION TO GENERATE EMBEDDINGS_MATRIX\n\n\ndef gen_embeddings_matrix(max_features, word_index,embeddings_index):\n\n    \"\"\"\n    Purpose, to generate embedding_matrix\n    Inputs, no of features, word_index and embeddings_index\n    Returns, embedding_matrix of size (max_features * embedding_dims)\n    \"\"\"\n\n    # CONVERTING THE EMBEDDINGS DIMENSIONS TO NUMPY (100 d)\n    all_emb_dims = np.stack(embeddings_index.values())\n\n    # CALCULATING MEAN AND SD FOR THE DIMENSIONS\n    emb_mean = all_emb_dims.mean()\n    emb_std = all_emb_dims.std()\n\n    # NO OF EMBEDDING DIMENSIONS\n    embed_size = all_emb_dims.shape[1]\n\n    # EFFECTIVE VOCAB SIZE\n    no_words = min(max_features, len(word_index)) # no of words\n\n    # CREATING AN EMBEDDINGS MATRIX\n    # INITIALIZING IT WITH MEAN AND SD VALUES (EMBEDDINGS INDEX)\n    np.random.seed(42)\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (no_words, embed_size))\n\n    # LOGIC TO CHECK AND POPULATE EMBEDDING MATRIX\n\n    for word in word_index.keys():\n    \n        # IGNORE IF WORD IS \"BEYOND\" THE FREQ RANGE\n        if word_index[word] >= max_features: \n            continue\n\n        # CHECK IF WORD IS IN THE EMBEDDINGS_INDEX OR IS AN OOV WORD\n        if word in embeddings_index.keys(): \n            embedding_vector = embeddings_index[word]\n            embedding_matrix[word_index[word]] = embedding_vector\n\n    return embedding_matrix","9c53ec40":"# IMPLEMENTING THE FUNCTION TO GENERATE EMBEDDINGS MATRIX\n\nmax_features = VOCAB_SIZE\nword_index = tokenizer.word_index\n\nembedding_matrix = gen_embeddings_matrix(max_features, word_index,embeddings_index)","31c0b6ac":"# TO CONCLUSIVELY ESTABLISH THE LINK BETWEEN word_index, embeddings_index AND embeddings_matrix\n\nword_index['the'] # equals 2\n\n# CHECK IF embeddings_index[2] = embedding_matrix[2] = embeddings FOR 'the' WORD\nprint(np.all(embeddings_index['the'] == embedding_matrix[2] ))","983be52c":"# keras.layers.Embedding(input_dim = vocab_size,\n#                                          output_dim = embed_size,\n#                                          input_length = sentence_length,\n#                                          weights=[embedding_matrix], trainable= False)","47645305":"### Function 2 `build_vocab` - List of Sentences to Vocabulary\n","a2cbfc0b":"### Using Pre-Trained Embeddings to Determine Extent of Data Standardization","ea5b1fc4":"I hope that you found this notebook to be useful. [This](https:\/\/github.com\/raamav\/Text-Classification\/blob\/master\/Twitter_for_PublicEmergencies_Part1.ipynb) notebook, located on my github page has the complete implementation details. \n\n**Thanks for reading**","fc0884db":"### Function 3 `check_overlap` - Determine the quality of vocabulary of the corpus (Tweet Vocabulary)","0e18a828":"### Function 5 `gen_embeddings_matrix` - Generate Embeddings Matrix","c94af735":"The purpose of the notebook is to walk through a few useful functions that can be used for text preprocessing.\n1. For Preprocessing Text (with a couple of effective inclusions)\n2. A slightly more intuitive way of building word vocabulary\n3. Checking instances of mis-spelt or non-standardized words\n4. Correcting mispelt words\n5. Preparing the Embedding Matrix\n\nSince other notebooks have covered visualizations and model building in great detail, I wont be emphasizing on those parts.","77676d1a":"Next up, I will be tokenizing the sequence using Keras' inbuilt library. Since, tokenizing isn't a focus point for this notebook, I'll *rush through* this section and move to the part where the embeddings matrix is generated.","35648a03":"Just to recap:\n1. We have two sets of dictionary: `master_vocab`, that representes the vocabuary of the processed tweets. `embeddings_index`, which represents the vocabulary of the GloVe embeddings\n2. Given that the GloVe index has 400,000 words (which is huge), we can use this to benchmark our vocabulary (`master_vocab`). Ths is what the next function does","8e718650":"Lambda functions greatly speed up text preprocessing. [This](https:\/\/medium.com\/@chaimgluck1\/have-messy-text-data-clean-it-with-simple-lambda-functions-645918fcc2fc) is a good introductory note of using these functions","8f1733e7":"### Function 1 `basic_preprocessor` - Preprocesses Text\n\n**On Contractions**\nThe Contraction library expands out words, for example *doesn't* is expanded *does not* and *won't* as *will not*. In my experience, LSTM models work well with contractions.\n\n**Expanding Out `'s`**\nIn the proprocessor below, I have also implemented the expansion of *apostrophe s*, for example the word *sister's* is expanded as *sister s*. In my experience, I have observed that LSTMs are able to process this information.\n\nBesides these points, the function implements a set of standard regular expressions to systematically cleanse text","f31d33f7":"The next step is to correct some of these mis-spellings. Dr. Cristof Henkel, a Kaggle Grandmaster has written an excellent function for the same. Reproducing it here.\n\n### Function 4 `_get_mispell` and `replace_typical_misspell` - Correct mispelt words","62d8bc10":"Next up, I will be accessing the `word_index` dictionary which is in the {Word: Code} format. The words that occur more frequently in the corpus have lower \"code\" values","bb14598e":"The next step is to coclusively establish the link between the `word_index`, the `embeddings_index` and `embeddings_matrix`","c4fdd052":"This `embedding_matrix` can be used in the embedding layer of the sequence based model."}}