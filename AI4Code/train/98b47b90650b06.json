{"cell_type":{"1f554c26":"code","e53897d9":"code","3b566527":"code","24ad5220":"code","e554140f":"code","89f0d18e":"code","8bd9c91f":"code","c06529f8":"code","2023047b":"code","130caac6":"code","90b5cb68":"code","d46b9abd":"code","2382c471":"code","5942c9ce":"code","df801de6":"code","09ab6336":"code","de9e8e39":"code","63114625":"code","dd75a432":"code","694835a0":"code","bc8acb12":"code","0c62f823":"code","d5edb64d":"code","56ae073c":"code","b46a8376":"code","9deef80d":"markdown","04bc17ca":"markdown","2c09d8c1":"markdown","8b63366e":"markdown","b6a4e02a":"markdown","0311db84":"markdown","5fcfc26a":"markdown","864b05ff":"markdown","a80aace2":"markdown","12fefb4e":"markdown"},"source":{"1f554c26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e53897d9":"df1 = pd.read_csv('\/kaggle\/input\/book-recommendation-dataset\/Ratings.csv', low_memory=False)\ndf2 = pd.read_csv('\/kaggle\/input\/book-recommendation-dataset\/Users.csv', low_memory=False)\ndf3 = pd.read_csv('\/kaggle\/input\/book-recommendation-dataset\/Books.csv', low_memory=False)","3b566527":"df1.info()","24ad5220":"df2.info()","e554140f":"df3.info()","89f0d18e":"# remove 'DK Publishing Inc', 'Gallimard' values from 'Year-Of-Publication' column and convert to int\ndf3 = df3[~df3['Year-Of-Publication'].isin(['DK Publishing Inc', 'Gallimard'])]\ndf3['Year-Of-Publication'] = pd.to_numeric(df3['Year-Of-Publication'])","8bd9c91f":"def plot_distribution(feature, data):\n    sns.displot(x=feature, data=data, kde=True, color='#244747');\n    plt.figtext(0.2, 1, '%s Distribution'%feature, fontfamily='serif', fontsize=17, fontweight='bold');","c06529f8":"df2 = df2[df2['Age']<=120]\n# Distribution of user as per age\nplot_distribution('Age', df2)","2023047b":"df3 = df3[(df3['Year-Of-Publication']>1950) & (df3['Year-Of-Publication']<=2016)]\n# Distribution across year of publication\nplot_distribution('Year-Of-Publication', df3)","130caac6":"data = df1.groupby('ISBN').agg(['mean', 'count'])['Book-Rating'].reset_index()\n\n# generate score based on mean rating and total number of times the book is rated\nm = data['count'].quantile(0.99) # minimum votes required to be listed in the Top 250\ndata = data[data['count']>m]\nprint('m =', m)\nprint(data.shape)\nR = data['mean'] # average for the book (mean) = (Rating)\nv = data['count'] # number of votes for the book = (votes)\nC = data['mean'].mean() # mean vote across all books\ndata['weighted rating'] = (v\/(v+m))*R + (m\/(v+m))*C\ndata = data.sort_values('weighted rating', ascending=False).reset_index(drop=True)\n\n# get title of books\ndata = pd.merge(data, df3, on='ISBN')[['Book-Title', 'Book-Author', 'mean', 'count', 'weighted rating', \n                              'Year-Of-Publication']].drop_duplicates('Book-Title').iloc[:20]\ndata","90b5cb68":"# drop any duplicates in df3\ndf3 = df3.drop_duplicates(['Book-Author', 'Book-Title'])\n\n# get book-author and title from df3\ndata = pd.merge(df3, df1, on='ISBN')[['Book-Author', 'Book-Rating', 'Book-Title', 'ISBN']]\n\ndata = data.groupby('Book-Author').agg(['mean', 'count'])['Book-Rating'].reset_index()\n\n# generate score based on mean rating and total number of times the author is rated\nm = data['count'].quantile(0.99) # minimum votes required to be listed in the Top 250\ndata = data[data['count']>m]\nprint('m =', m)\nprint(data.shape)\nR = data['mean'] # average for the author (mean) = (Rating)\nv = data['count'] # number of votes for the author = (votes)\nC = data['mean'].mean() # mean vote across all authors\ndata['weighted rating'] = (v\/(v+m))*R + (m\/(v+m))*C\ndata = data.sort_values('weighted rating', ascending=False).reset_index(drop=True)\n\ndata.iloc[:20]","d46b9abd":"# merge df1 and df5 to get movie titles and drop rows for which title is not available\ndata = pd.merge(df1, df3, on='ISBN')\n\n# get total counts of no. of occurence of book\ndata['count'] = data.groupby('ISBN').transform('count')['User-ID']\n\n# fetch top 100 books based on count\nisbn = data.drop_duplicates('ISBN').sort_values(\n    'count', ascending=False).iloc[:100]['ISBN']\n\n# filter out data as per the ISBN\ndata = data[data['ISBN'].isin(isbn)].reset_index(drop=True)","2382c471":"# create a user book rating matrix\ndf = data.pivot(index='User-ID', columns='ISBN', values='Book-Rating')\ndf.head()","5942c9ce":"# get user-ID for users who have read more than 50 books\ntemp = df[~df.isna()].count(axis=1).reset_index()\ntemp[temp[0]>50]","df801de6":"from surprise import Reader, Dataset, SVD\nfrom surprise.model_selection import train_test_split, cross_validate","09ab6336":"reader = Reader(rating_scale=(0, 10))\nsurprise_data = Dataset.load_from_df(data[['User-ID', 'ISBN', 'Book-Rating']], reader)\ntrainset, testset = train_test_split(surprise_data, test_size=0.25)","de9e8e39":"benchmark = []\n# Iterate over all algorithms\nfor algorithm in [SVD()]:\n    # Perform cross validation\n    results = cross_validate(algorithm, surprise_data, measures=['RMSE'], cv=3, verbose=False)\n    \n    # Get results & append algorithm name\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n    benchmark.append(tmp)\n    \npd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse') ","63114625":"svd = SVD() \nsvd.fit(trainset)","dd75a432":"index_val = 2131\n# get user id\nuserId = df.index[index_val]\nbooks = []\nratings = []\ntitles = []\n\nfor isbn in df.iloc[index_val][df.iloc[index_val].isna()].index:\n    books.append(isbn)\n    title = data[data['ISBN']==isbn]['Book-Title'].values[0]\n    titles.append(title)\n    ratings.append(svd.predict(userId, isbn).est)\n\nprediction = pd.DataFrame({'ISBN':books, 'title':titles, 'rating':ratings, 'userId':userId})  \nprediction = prediction.sort_values('rating', ascending=False).iloc[:10].reset_index(drop=True)\n\n# get other high rated books by user\ntemp = data[data['User-ID']==df.index[index_val]].sort_values(\n    'Book-Rating', ascending=False)[['Book-Rating', 'Book-Title', 'User-ID']].iloc[:10].reset_index(drop=True)\nprediction['Book Read'] = temp['Book-Title']\nprediction['Rated']= temp['Book-Rating']\nprediction","694835a0":"index_val = 912\n# get user id\nuserId = df.index[index_val]\nbooks = []\nratings = []\ntitles = []\n\nfor isbn in df.iloc[index_val][df.iloc[index_val].isna()].index:\n    books.append(isbn)\n    title = data[data['ISBN']==isbn]['Book-Title'].values[0]\n    titles.append(title)\n    ratings.append(svd.predict(userId, isbn).est)\n\nprediction = pd.DataFrame({'ISBN':books, 'title':titles, 'rating':ratings, 'userId':userId})  \nprediction = prediction.sort_values('rating', ascending=False).iloc[:10].reset_index(drop=True)\n\n# get other high rated books by user\ntemp = data[data['User-ID']==df.index[index_val]].sort_values(\n    'Book-Rating', ascending=False)[['Book-Rating', 'Book-Title', 'User-ID']].iloc[:10].reset_index(drop=True)\nprediction['Book Read'] = temp['Book-Title']\nprediction['Rated']= temp['Book-Rating']\nprediction","bc8acb12":"from sklearn.metrics.pairwise import cosine_similarity","0c62f823":"# replace NaN with user based average rating in pivot dataframe\ndf_imputed = df.fillna(df.mean(axis=0))\n\n# get similarity between all users\nsimilarity_matrix = cosine_similarity(df_imputed.values)","d5edb64d":"def get_recommendation(user_index):\n    idx = user_index\n    sim_scores = list(enumerate(similarity_matrix[idx]))\n\n    # get books that are unrated by the given user\n    unrated_books = df.iloc[idx][df.iloc[idx].isna()].index\n\n    # get weighted ratings of unrated books by all other users\n    book_ratings = (df[unrated_books].T * similarity_matrix[idx]).T\n\n    # get top 100 similar users by skipping the current user\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:101]\n\n    # get mean of book ratings by top 100 most similar users for the unrated books\n    book_ratings = book_ratings.iloc[[x[0] for x in sim_scores]].mean()\n    \n    # get rid of null values and sort it based on ratings\n    book_ratings = book_ratings.reset_index().dropna().sort_values(0, ascending=False).iloc[:10]\n    \n    # get recommended book titles in sorted order\n    recommended_books = data[data['ISBN'].isin(book_ratings['ISBN'])][['ISBN', 'Book-Title']]\n    recommended_books = recommended_books.drop_duplicates('ISBN').reset_index(drop=True)\n    assumed_ratings = book_ratings[0].reset_index(drop=True)\n\n    return pd.DataFrame({'ISBN':recommended_books['ISBN'], \n                         'Recommended Book':recommended_books['Book-Title'], \n                         'Assumed Rating':assumed_ratings})","56ae073c":"user_index = 2131\nrecommended_books = get_recommendation(user_index)\n# get other high rated books by user\ntemp = data[data['User-ID']==df.index[user_index]].sort_values(\n    'Book-Rating', ascending=False)[['Book-Rating', 'Book-Title', 'User-ID']].iloc[:10].reset_index(drop=True)\nrecommended_books['userId'] = temp['User-ID']\nrecommended_books['Book Read'] = temp['Book-Title']\nrecommended_books['Rated']= temp['Book-Rating']\nrecommended_books","b46a8376":"user_index = 6349\nrecommended_books = get_recommendation(user_index)\n# get other high rated books by user\ntemp = data[data['User-ID']==df.index[user_index]].sort_values(\n    'Book-Rating', ascending=False)[['Book-Rating', 'Book-Title', 'User-ID']].iloc[:10].reset_index(drop=True)\nrecommended_books['userId'] = temp['User-ID']\nrecommended_books['Book Read'] = temp['Book-Title']\nrecommended_books['Rated']= temp['Book-Rating']\nrecommended_books","9deef80d":"## 1. Using Surpise Library","04bc17ca":"# Recommender 2: Collaborative Filtering\nBased on records from various users provide recommendations based on user similarities","2c09d8c1":"## 2. Top 20 Highest Rated Authors","8b63366e":"remove books before 1950 as there are not many and before 2016","b6a4e02a":"remove users with age above 120","0311db84":"## 1. Top 20 Highest Rated Books","5fcfc26a":"# EDA","864b05ff":"* Dataset 1 - Rating.csv (11149780)\n    * User-ID - user ID\n    * ISBN - book ID\n    * Book-Rating - Rating given by user\n    \n* Dataset 2 - Users.csv (278858)\n    * User-ID - user ID\n    * Location - location of user\n    * Age - age of user\n\n* Dataset 3 - Books.csv (271360)\n    * ISBN - book ID\n    * Book-Title - Book Name\n    * Book-Author\n    * Year-Of-Publication\n    * Publisher\n    * Image-URL-S\n    * Image-URL-M\n    * Image-URL-L","a80aace2":"# Recommendor 1 : Demographic Filtering","12fefb4e":"## 2. Using mean of other user' weighted ratings based on similarity matrix\nHere assumed rating may not make much sense as most of the books are not read by other users"}}