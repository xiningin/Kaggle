{"cell_type":{"13110ae7":"code","4a671d4d":"code","5a59a271":"code","da5f3d04":"code","c614fbff":"code","53fb608c":"code","a665603a":"code","35ed635c":"code","9957e3d0":"code","62bac6e9":"code","215cb9de":"code","1470b11f":"code","6de7c22b":"code","aa335dfd":"code","bd130271":"code","520523dc":"code","8e6a5319":"code","1e08ee06":"markdown","7878ebb9":"markdown","d68b903f":"markdown","96e81c34":"markdown","198fb2f1":"markdown","7a3e9abf":"markdown","6db37ec8":"markdown","62215250":"markdown","bd945758":"markdown","c1eeba72":"markdown"},"source":{"13110ae7":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4a671d4d":"# solve_missing {'drop' , 'mean', 'median'}\ndef clean_data(dataframe, \n               solve_missing : str ='drop', \n               solve_missing_cat : str = 'drop',\n               drop_list : list = [], \n               category_list : list = []): \n    df = dataframe.drop_duplicates()\n    df = df.drop(drop_list, axis=1)\n    if solve_missing == 'drop' or solve_missing_cat == 'drop':\n        df = df.dropna()\n        object_dtype = list(df.select_dtypes(include='object').columns)\n        for col in df.columns:\n            if col in category_list or col in object_dtype:\n                df[col] = df[col].astype('category')\n    else:\n        object_dtype = list(df.select_dtypes(include='object').columns)\n        for col in df.columns:\n            if col in category_list or col in object_dtype:\n                df[col] = df[col].astype('category')\n                if solve_missing_cat == 'mode':\n                    df[col] = df[col].fillna(df[col].mode()[0], inplace=False)\n            else:\n                if solve_missing == 'mean':\n                    df[col] = df[col].fillna(df[col].mean(), inplace=False)\n                else:\n                    df[col] = df[col].fillna(df[col].median(), inplace=False)\n    return df\n\ndef split_xy(dataframe, label : str):\n    return dataframe.drop(labels=label, axis=1), dataframe[label]\n\ndef encode(df):\n    category_dtype = list(df.select_dtypes(include='category').columns)\n    cat = pd.get_dummies(df, columns = category_dtype, drop_first = True)\n    for col in cat.columns:\n        if cat[col].dtype == np.uint8:\n            cat[col] = cat[col].astype('category')\n    return cat\n\ndef scale(features : tuple):\n    trainFeatures, valFeatures, testFeatures = features # type pandas DataFrame\n    scaler = StandardScaler()\n    category_dtype = list(trainFeatures.select_dtypes(include='category').columns)\n    continuous_dtype = list(filter(lambda c: c not in category_dtype, trainFeatures.columns))\n    # SCALING\n    scaler.fit(trainFeatures[continuous_dtype])\n    cont_xtrain = scaler.transform(trainFeatures[continuous_dtype])\n    cont_xval = scaler.transform(valFeatures[continuous_dtype])\n    cont_xtest = scaler.transform(testFeatures[continuous_dtype])\n    # ENCODING\n    cat_xtrain = trainFeatures[category_dtype]\n    cat_xval = valFeatures[category_dtype]\n    cat_xtest = testFeatures[category_dtype]\n    print(cat_xtrain.shape, cat_xval.shape, cat_xtrain.shape)\n    xtrain = np.concatenate((cont_xtrain, cat_xtrain), axis=1)\n    xval = np.concatenate((cont_xval, cat_xval), axis=1)\n    xtest = np.concatenate((cont_xtest, cat_xtest), axis=1)\n    return scaler, xtrain, xval, xtest","5a59a271":"df = pd.read_csv('\/kaggle\/input\/patient\/dataset.csv')\nTARGET = 'hospital_death'\n#df.info()\nunique_labels = np.unique(df[TARGET])\nplt.pie(np.array([len(df[df[TARGET]==label]) for label in unique_labels ]), labels = list(unique_labels), autopct='%1.1f%%')","da5f3d04":"category_list = ['elective_surgery', 'ethnicity', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem', \n                 'aids', 'cirrhosis', 'diabetes_mellitus', 'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis', 'hospital_death']\ndrop_list = ['encounter_id', 'patient_id', 'hospital_id', 'Unnamed: 83']\n\nclean_df = clean_data(df, 'mean', 'mode', drop_list, category_list)\nplt.pie(np.array([len(clean_df[clean_df[TARGET]==label]) for label in unique_labels ]), labels = list(unique_labels), autopct='%1.1f%%')\nplt.show()\nplt.bar(list(unique_labels), [len(clean_df[clean_df[TARGET]==label]) for label in unique_labels ], color = ['cyan', 'green'])\nplt.show()","c614fbff":"features, labels = split_xy(clean_df, TARGET)\nVAL_SIZE = 0.2\nTEST_SIZE = 0.3\nencoded_features = encode(features)\nprint('Encoded Features Shape : {}'.format(encoded_features.shape))\n# split all samples to train and test with respect to the ratio of each class\nX_train, X_test, ytrain, ytest = train_test_split(encoded_features, labels, test_size = (VAL_SIZE+TEST_SIZE), stratify = labels)\n# split test sample to test and validation with respect to the ratio of each class\nX_test, X_val, ytest, yval = train_test_split(X_test, ytest, test_size = VAL_SIZE\/(VAL_SIZE+TEST_SIZE), stratify = ytest)\n# scale continous variables with standard scaler\nscaler, xtrain, xval, xtest = scale((X_train, X_val, X_test))\n#print(scaler.mean_)\n#print(scaler.var_)\n#print(xtrain.shape, ytrain.shape)\n#print(xval.shape, yval.shape)\n#print(xtest.shape, ytest.shape)\n# convert features to numpy array format\nxtrain = np.array(xtrain, dtype='float32')\nxval = np.array(xval, dtype='float32')\nxtest = np.array(xtest, dtype='float32')\nytrain = np.array(ytrain, dtype='float32')\nyval = np.array(yval, dtype='float32')\nytest = np.array(ytest, dtype='float32')","53fb608c":"import torch\n# locate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","a665603a":"import torch.nn.functional as F\nimport torch.nn as nn\n#from torch.autograd import Variable\n\nclass Model(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 1024)\n        self.gnorm1 = nn.GroupNorm(32, 1024)\n        \n        self.layer2 = nn.Linear(1024, 512)\n        self.gnorm2 = nn.GroupNorm(16, 512)\n        \n        self.layer3 = nn.Linear(512, 128)\n        self.gnorm3 = nn.GroupNorm(4, 128)\n        self.layer4 = nn.Linear(128, output_dim)\n        \n    def forward(self, x):\n        x = F.relu(self.gnorm1(self.layer1(x)))\n        x = F.relu(self.gnorm2(self.layer2(x)))\n        x = F.relu(self.gnorm3(self.layer3(x)))\n        x = torch.sigmoid(self.layer4(x))\n        return x","35ed635c":"from torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, x, y, device):\n        self._x = x\n        self._y = y\n        self._device = device\n\n    def __len__(self):\n        return len(self._x)\n\n    def __getitem__(self, idx):\n        X, Y = self._x[idx], self._y[idx].ravel()\n        return torch.as_tensor(X, dtype=torch.float32, device=self._device), torch.as_tensor(Y, dtype=torch.float32, device=self._device)","9957e3d0":"# batch size of train, validation, test don't have to be the same\nTRAIN_BATCH_SIZE, VAL_BATCH_SIZE, TEST_BATCH_SIZE = 256, 256, 1\n\ntrain_dataset = CustomDataset(xtrain, ytrain, device)\nval_dataset = CustomDataset(xval, yval, device)\ntest_dataset = CustomDataset(xtest, ytest, device)\n\ntrain_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)","62bac6e9":"# unable to import F1Score\nfrom torchmetrics import Accuracy, MeanSquaredError, Precision, Recall\nif torch.cuda.is_available():\n    loss_fn   = nn.BCELoss().cuda()\n    metrics = [\n        {'name': 'Accuracy', 'fn' : Accuracy().cuda(), 'type' : 'int'},\n        {'name': 'MeanSquaredError', 'fn': MeanSquaredError().cuda(), 'type' : 'float'},\n        {'name': 'Precision', 'fn': Precision().cuda(), 'type': 'int'},\n        {'name': 'Recall', 'fn': Recall().cuda(), 'type':'int'}\n    ]\nelse:\n    loss_fn = nn.BCELoss().cpu()\n    metrics = [\n        {'name': 'Accuracy', 'fn' : Accuracy().cpu(), 'type' : 'int'},\n        {'name': 'MeanSquaredError', 'fn': MeanSquaredError().cpu(), 'type' : 'float'},\n        {'name': 'Precision', 'fn': Precision().cpu(), 'type' : 'int'},\n        {'name': 'Recall', 'fn': Recall().cpu(), 'type' : 'int'}\n    ]","215cb9de":"model     = Model(xtrain.shape[-1], 1)\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.000001)","1470b11f":"def L1(params, device, l1 = 0.0001):\n    reg = torch.tensor(0., device=device)\n    for param in params:\n        reg += torch.sum(torch.abs(param))\n    return l1 * reg\n\ndef L2(params, device, l2 = 0.0001):\n    reg = torch.tensor(0., device=device)\n    for param in params:\n        reg += torch.sum(torch.pow(param, 2))\n    return l2 * reg\n\ndef train_loop(dataloader, valloader, model, loss_fn, optimizer, max_iter, metrics : dict, device):\n    model.train()\n    history = dict()\n    history['Loss'] = []\n    history['val_Loss'] = []\n    for m in metrics:\n        history[m['name']] = []\n        history['val_{}'.format(m['name'])] = []\n    size = len(dataloader.dataset)\n    for itr in range(max_iter):\n        real_time = dict()\n        real_time['Loss'] = []\n        for m in metrics:\n            real_time[m['name']] = []\n        for batch, (X, y) in enumerate(dataloader):\n            # Compute prediction and loss\n            X.to(device)\n            y.to(device)\n            pred = model(X)\n            loss = loss_fn(pred, y)\n            loss += L2(model.parameters(), device) + L1(model.parameters(), device)\n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # compute metrics\n            real_time['Loss'].append(loss.item())\n            for m in metrics:\n                if m['type'] == 'float':\n                    real_time[m['name']].append(m['fn'](pred, y).item())\n                else:\n                    real_time[m['name']].append(m['fn'](pred, y.type(torch.uint8)).item())\n        history['Loss'].append(np.mean(real_time['Loss']))\n        for m in metrics:\n            history[m['name']].append(np.mean(real_time[m['name']]))\n            print(f\"{m['name']}: {np.mean(real_time[m['name']]):>8f}\", end='\\t')\n        print('')\n        epoch_loss = np.mean(real_time['Loss'])\n        print(f\"Loss: {epoch_loss:>7f} [{itr:>5d}\/{max_iter:>5d}]\")\n        print('Val', end = '\\t')\n        tmp = dict()\n        tmp['Loss'] = []\n        for m in metrics:\n            tmp[m['name']] = []\n        model.eval()\n        with torch.no_grad():\n            for X, y in valloader:\n                X.to(device)\n                y.to(device)\n                pred = model(X)\n                test_loss = loss_fn(pred, y)\n                tmp['Loss'].append(test_loss.item())\n                for m in metrics:\n                    if m['type'] == 'float':\n                        tmp[m['name']].append(m['fn'](pred, y).item())\n                    else:\n                        tmp[m['name']].append(m['fn'](pred, y.type(torch.uint8)).item())\n        for k, v in tmp.items():\n            print(f\"{k}: {np.mean(v):>8f}\", end='\\t')\n            history['val_{}'.format(k)].append(np.mean(v))\n        print('###END')\n        #torch.save(model, f'.\/model_E{itr:>5d}.pt')\n    return history\n\ndef test_loop(dataloader, model, loss_fn, metrics : dict, device):\n    score = dict()\n    tmp = dict()\n    tmp['Loss'] = []\n    for m in metrics:\n        tmp[m['name']] = []\n    model.eval()\n    with torch.no_grad():\n        for X, y in dataloader:\n            X.to(device)\n            y.to(device)\n            pred = model(X)\n            test_loss = loss_fn(pred, y)\n            tmp['Loss'].append(test_loss.item())\n            for m in metrics:\n                if m['type'] == 'float':\n                    tmp[m['name']].append(m['fn'](pred, y).item())\n                else:\n                    tmp[m['name']].append(m['fn'](pred, y.type(torch.uint8)).item())\n    for k, v in tmp.items():\n        print(f\"{k}: {np.mean(v):>8f}\", end='\\t')\n        score[k] = np.mean(v)\n    return score\n\ndef predict(dataloader, model, device):\n    ypred = []\n    model.eval()\n    with torch.no_grad():\n        for X, y in dataloader:\n            X.to(device)\n            pred = model(X)\n            ypred.append([torch.ravel(pred), torch.ravel(y)])\n    return ypred","6de7c22b":"EPOCHS = 50\ntrain_history = train_loop(train_loader, val_loader, model, loss_fn, optimizer, EPOCHS, metrics, device)","aa335dfd":"score = test_loop(test_loader, model, loss_fn, metrics, device)","bd130271":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n\nypred = predict(test_loader, model, device)\nypred_np = np.array(ypred, dtype='int64')\npredictions, gt = ypred_np[:,0], ypred_np[:,1]\n\ncfm = confusion_matrix(gt, predictions)\nplt.imshow(cfm, cmap='cool', interpolation='nearest')\nplt.show()\n\nprint('Confusion Matrix')\nprint(cfm)","520523dc":"import matplotlib.pyplot as plt","8e6a5319":"fig = plt.gcf()\nfig.set_size_inches(16, 30)\nplt.figure(1)\n\nnplot = len(train_history.keys()) \/\/ 2\nprint(nplot)\nfor idx, m in enumerate(metrics, 0):\n    plt.subplot(nplot*100+10+idx+1)\n    plt.plot([i+1 for i in range(len(train_history[m['name']]))], train_history[m['name']], color='black', linewidth=1)\n    plt.plot([i+1 for i in range(len(train_history[m['name']]))], train_history['val_{}'.format(m['name'])], color='blue', linewidth=1)\n    ax = plt.gca()\n    ax.set_ylabel(m['name'])\nplt.subplot(nplot*100+10+nplot)\nplt.plot([i+1 for i in range(len(train_history['Loss']))], train_history['Loss'], color='black', linewidth=1)\nplt.plot([i+1 for i in range(len(train_history['val_Loss']))], train_history['val_Loss'], color='blue', linewidth=1)\nax = plt.gca()\nax.set_ylabel('Loss')\n","1e08ee06":"# **Analysis**","7878ebb9":"# **Code Block - Define Train and Test Loop**\nSimilar to TensorFlow model.fit and model.evaluate","d68b903f":"# **Code Block - Preprocessing \u524d\u8655\u7406**","96e81c34":"# **Train**","198fb2f1":"# **Declare Metrics to Monitor and Load to Device**","7a3e9abf":"# **Evaluate**","6db37ec8":"# **Code Block - Convert Dataset to PyTorch Compatible Dataset**","62215250":"# **Dependency**","bd945758":"# **Data Loading**","c1eeba72":"# **Model**"}}