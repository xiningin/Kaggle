{"cell_type":{"60ea69a9":"code","b5ba73e9":"code","ee27ceda":"code","1f1fa8ab":"code","4bc069ea":"code","cd7b8082":"code","8fd45987":"code","b72b25ce":"code","3997226e":"code","2c7496be":"code","efe20304":"code","16b8e6df":"code","ae688ec4":"code","81cec9b9":"code","aa5ae846":"markdown","bae74703":"markdown","fc0fb34a":"markdown","d4201fb0":"markdown","5324bbc8":"markdown","cbe55a8a":"markdown"},"source":{"60ea69a9":"import tensorflow as tf\ntf.__version__","b5ba73e9":"from tensorflow.keras.datasets import imdb\nfrom tensorflow.keras import models, layers, optimizers\nimport numpy as np\nimport matplotlib.pyplot as plt","ee27ceda":"(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)","1f1fa8ab":"print(\"Words: \", train_data[2])\nprint(\"Labels: \", train_labels[2])","4bc069ea":"# We can access words followed by the above indexes by calling get_word_index\n# which returns a dictionary of words and their indexes\nword_index = imdb.get_word_index()\nreversed_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\" \".join([ reversed_word_index.get(i-3, \"?\") for i in train_data[2]])","cd7b8082":"def vectorize_sequences(sequences, dimension=10000):\n  results = np.zeros((len(sequences), dimension))\n  for i, sequence in enumerate(sequences):\n    results[i, sequence] = 1\n  return results","8fd45987":"x_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\nx_train.shape","b72b25ce":"y_train = np.asarray(train_labels).astype(\"float32\")\ny_test = np.asarray(test_labels).astype(\"float32\")\n\ny_train.shape","3997226e":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000, )))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","2c7496be":"# Spliting data further for validation\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n\nx_val.shape","efe20304":"model.compile(optimizer='rmsprop', loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])\nhistory = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))","16b8e6df":"print(model.summary)","ae688ec4":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training Loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\nplt.clf()\n\nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc']\n\nplt.plot(epochs, acc_values, 'bo', label='Trainig Accuracy')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show","81cec9b9":"model.evaluate(x_test, y_test)","aa5ae846":"# IMDB Data Classification (Sentiment Analysis) with Tensorflow","bae74703":"## Building Model","fc0fb34a":"## Model Evaluation","d4201fb0":"## Loading libraries and datasets","5324bbc8":"## A Look at Data we got","cbe55a8a":"## Preparing Dataset"}}