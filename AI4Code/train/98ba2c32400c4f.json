{"cell_type":{"817b7160":"code","2f793251":"code","e3a87ec7":"code","6f1f5f83":"code","3d02a446":"code","f746eda9":"code","a629d441":"code","c8b7a512":"code","1d3d4a7d":"code","1a8db533":"code","0dc5a494":"code","fd07afed":"code","fa828bcf":"code","6b623213":"code","c829d7b4":"code","bbc38fe4":"code","8aa5ae1b":"code","e3b375e7":"code","1b9fbfc4":"code","9d471598":"code","5b40d78e":"code","7564a505":"code","345becf5":"code","4b37dca2":"code","10199a84":"code","5ce136e0":"code","a480e318":"code","599b6526":"code","c4d27008":"code","d89a8d33":"code","8a5d0233":"code","4f5ccde0":"code","5ab01095":"code","7862383b":"code","64957b73":"code","94233b15":"code","db4a7ca8":"code","f57e4235":"code","d484a437":"code","b9086a54":"code","0e8ea467":"code","52d8e96b":"code","0f967b3c":"code","2df6a427":"code","3b972b23":"code","9590557b":"code","237f662b":"code","52837116":"code","b1d4ffc7":"code","375065ad":"code","143fc2da":"code","9c6c197b":"markdown","2e3ae4ed":"markdown","3e708453":"markdown","275d3b15":"markdown","5f95cc9d":"markdown","d8495b5e":"markdown","123ca970":"markdown","0172aa8c":"markdown","44976595":"markdown","26a9b004":"markdown","bc73ed24":"markdown","d80c6e3f":"markdown","ed867479":"markdown","36a3e161":"markdown","f177a55b":"markdown","234acaa5":"markdown","06928e65":"markdown","622b0a39":"markdown","ed600b27":"markdown","4b005e6b":"markdown","62f61cb6":"markdown","f7efc7c6":"markdown","98b091a6":"markdown","2da168d8":"markdown","327e6a56":"markdown","94b61920":"markdown","747739ad":"markdown","4b685ad9":"markdown","f8e25cbf":"markdown","7f86fe08":"markdown","3e458bd0":"markdown","9452d7ae":"markdown","aab18250":"markdown","07a49396":"markdown","d11494f0":"markdown","c75b23d5":"markdown","6237ae1c":"markdown","2bbd02fd":"markdown","b1ab9274":"markdown","1f5e968c":"markdown","499fda7c":"markdown","b6934a67":"markdown","d6c5c2c4":"markdown","6f87afda":"markdown","e326c129":"markdown","c1fd3e97":"markdown","aa2b6d92":"markdown","392749ae":"markdown","699b26f2":"markdown","a60df7f8":"markdown","3eeb113f":"markdown","3af7db6a":"markdown","fc4550fb":"markdown"},"source":{"817b7160":"!pip install -U seaborn # seaborn>=0.11","2f793251":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize']=(23,8)\ndef twosubplots(figsize=(23,8)):\n    return plt.subplots(1,2,figsize=figsize)[1]\n\n!ls \/kaggle\/input\/*\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e3a87ec7":"ROOT_DIR = '\/kaggle\/input\/competitive-data-science-predict-future-sales'\nROOT_DIR_EN = '\/kaggle\/input\/predict-future-sales-supplementary'\nD = pd.read_csv(ROOT_DIR+ '\/sales_train.csv')\nDtest = pd.read_csv(ROOT_DIR+ '\/test.csv')\nD_itemcategory = pd.read_csv(ROOT_DIR_EN+'\/item_category.csv')\n\nprint(\"train dataset has %d rows\" % len(D))\nD=D.drop('date',axis=1) # dropping date column.\n#D = pd.merge(D, D_item[['item_id','item_category_id', 'item_name']], on='item_id')\nD = pd.merge(D,D_itemcategory, on='item_id')\nD = D.rename(columns={'item_name_translated':'item_name'})\n\nDtest = pd.merge(Dtest,D_itemcategory, on='item_id')\nDtest = Dtest.rename(columns={'item_name_translated':'item_name'})\nDtest['date_block_num']=34\n\ndisplay(D.head(3))\ndisplay(Dtest.head(3))","6f1f5f83":"import pandas_profiling # library for automatic EDA\nreport = pandas_profiling.ProfileReport(D)","3d02a446":"display(report)","f746eda9":"D_category_dup = D_itemcategory[D_itemcategory['item_name_translated'].duplicated(keep=False)]\nD_category_dup.sort_values('item_name_translated')","a629d441":"dftmp = D_category_dup.groupby('item_name_translated')['item_id'].min()\nitem_id_mapping = {itemid:dftmp[iname] for _,itemid,iname in D_category_dup[['item_id','item_name_translated']].itertuples()}\nD=D.replace({'item_id':item_id_mapping})\nDtest=Dtest.replace({'item_id':item_id_mapping})","c8b7a512":"Duniq = D['item_id'].unique()\nDtest_uniq = Dtest['item_id'].unique()\n\nitem_id_inter = np.intersect1d(Duniq,Dtest_uniq)\n\nlen(item_id_inter),len(Duniq),len(Dtest_uniq)","1d3d4a7d":"new_items_test = D_itemcategory[D_itemcategory['item_id'].isin(np.setdiff1d(Dtest_uniq,Duniq))]\nnew_items_test","1a8db533":"sns.histplot(new_items_test, x='item_cat1');","0dc5a494":"Dgtmp = D.groupby(['item_id'])['item_price'].mean()\nDgtmp.name='avg_item_price'\nD = pd.merge(D,Dgtmp, on='item_id')\ndisplay(D.head(3))","fd07afed":"sns.boxplot(y=D['item_price']);\nD['item_price'].describe()","fa828bcf":"D=D[D['item_price']>0]\nD[D['item_price']>300000]","6b623213":"D[D['item_id']==6066]['item_price'].describe()","c829d7b4":"D=D[D['item_price']<300000];","bbc38fe4":"D['item_price_relative'] = D['item_price']\/D['avg_item_price']\nsns.boxplot(y=D['item_price_relative']);","8aa5ae1b":"dftmp = D[D['item_price_relative']>=10].sort_values(by='item_price_relative', ascending=False)\ndftmp","e3b375e7":"Dgroup = D.groupby(['date_block_num','shop_id','item_id','item_cat1','item_cat2'])\nD = Dgroup.agg({'item_cnt_day':'sum',\n                 'item_price':'mean'})\nD = D.rename({'item_cnt_day':'item_cnt_month'}, axis=1).reset_index()\nDgroup = D.groupby('item_id')['item_price'].mean()\nDgroup.name = 'avg_item_price'\nD = D.merge(Dgroup, on='item_id')\nD.head(3)","1b9fbfc4":"D['item_price_relative'] = D['item_price']\/D['avg_item_price']\nD.head(3)","9d471598":"sns.boxplot(y=D['item_price_relative']);","5b40d78e":"dftmp = D[D['item_price_relative']>=8].sort_values(by='item_price_relative', ascending=False)\ndftmp","7564a505":"ax1,ax2 = twosubplots()\ndftmp=D.groupby(['date_block_num']).sum()\nsns.barplot(data=dftmp,x=dftmp.index, y='item_cnt_month', ax=ax1);\ndftmp=D.groupby(['date_block_num']).mean()\nsns.barplot(data=dftmp,x=dftmp.index, y='item_cnt_month', ax=ax2);\nax2.set_ylabel(\"Average item_cnt_month\");","345becf5":"ax1,ax2 = twosubplots()\ndftmp=D.groupby(['date_block_num']).sum()\nsns.barplot(data=dftmp,x=dftmp.index, y='item_price', ax=ax1);\nax1.set_ylabel('Total Money Spent')\ndftmp=D.groupby(['date_block_num']).mean()\nsns.barplot(data=dftmp,x=dftmp.index, y='item_price', ax=ax2);\nax2.set_ylabel('Average item_price');","4b37dca2":"dftmp = D.groupby(['date_block_num','shop_id'])[['item_cnt_month']].sum().sort_values(['date_block_num','item_cnt_month'], ascending=False) #.groupby('date_block_num').max()\n#dftmp = dftmp[dftmp.groupby('date_block_num').idxmax()].reset_index()\ndftmp = dftmp.reset_index().groupby(['date_block_num']).nth([0,1]).reset_index()\ndftmp = D[D['shop_id'].isin(dftmp['shop_id'].unique())]\ndftmp = dftmp.groupby(['date_block_num','shop_id'])['item_cnt_month'].sum().reset_index()\nsns.barplot(data=dftmp,x='date_block_num',y='item_cnt_month',hue='shop_id');","10199a84":"dftmp = D.groupby(['date_block_num','item_id'])[['item_cnt_month']].sum().sort_values(['date_block_num','item_cnt_month'], ascending=False) #.groupby('date_block_num').max()\n#dftmp = dftmp[dftmp.groupby('date_block_num').idxmax()].reset_index()\ndftmp = dftmp.reset_index().groupby(['date_block_num']).nth(0).reset_index()\nmost_sold_items = dftmp['item_id'].unique()\ndftmp = D[D['item_id'].isin(most_sold_items)]\ndftmp = dftmp.groupby(['date_block_num','item_id'])['item_cnt_month'].sum().reset_index()\nsns.barplot(data=dftmp,x='date_block_num',y='item_cnt_month',hue='item_id');\n#sns.barplot(data=dftmp,x='date_block_num',y='item_cnt_month',hue='item_id');\n","5ce136e0":"D_itemcategory[D_itemcategory['item_id'].isin(most_sold_items)]","a480e318":"ax1,ax2 = twosubplots()\nsns.scatterplot(data=D, x='item_price_relative', y='item_cnt_month', ax=ax1);\nsns.scatterplot(data=D, x='item_price', y='item_cnt_month', ax=ax2);","599b6526":"#sns.histplot(data=D,x='item_price_relative', hue=pd.cut(D['item_cnt_month'], bins=[-24,0,1,2,3,5,128,2000]));","c4d27008":"#D.to_csv('data_train1.csv') # backup\n#Dtest.to_csv('data_test1.csv')","d89a8d33":"#pd.read_csv('data_train1.csv', index_col=0)","8a5d0233":"import gc\ndftmp=None; Dgroup=None\ngc.collect()","4f5ccde0":"D = D.append(Dtest, ignore_index=True)\ntrain_idxs = D['date_block_num']<=33\nlen(D)","5ab01095":"D['item_cat2']=D['item_cat2'].fillna('na')","7862383b":"D['item_cnt_month'].clip(0,20, inplace=True)","64957b73":"from calendar import monthrange\nD['month'] = D['date_block_num'] % 12\nD['year'] = D['date_block_num']\/\/ 12 + 2013\nD['num_days_month'] = D.apply(lambda x:monthrange(x['year'], x['month']+1)[1],axis=1)","94233b15":"group_keys =[['shop_id'],\n            ['item_id'],\n            ['item_cat1'],\n            ['item_cat2'],\n            ['shop_id','item_cat1'],\n            ['shop_id','item_cat2'],\n            ['shop_id','item_id']]\n\nnew_alltimefeatures_names = ['shop', 'itemid','itemcat1','itemcat2','shop-itemcat1','shop-itemcat2', 'shop-itemid']\nnew_alltimefeatures_cnt_names = [\"alltime_%s_cnt\" % name for name in new_alltimefeatures_names] # inserts a prefix and a suffix\nnew_alltimefeatures_price_names = [\"alltime_%s_avgprice\" % name for name in new_alltimefeatures_names] # inserts a prefix and a suffix\n\n\nfor name,k in zip(new_alltimefeatures_cnt_names,group_keys):\n    dftmp = D.groupby(k)['item_cnt_month'].mean()#sum()\n    dftmp.name = name\n    D = D.merge(dftmp, on=k, how='left')\n    \nfor name,k in zip(new_alltimefeatures_price_names,group_keys):\n    dftmp = D.groupby(k)['item_price'].mean()\n    dftmp.name = name\n    D = D.merge(dftmp, on=k, how='left')\nD[['shop_id','item_id']+new_alltimefeatures_cnt_names+new_alltimefeatures_price_names]","db4a7ca8":"Itemfirst = D.groupby(['item_id'])['date_block_num'].min()\nItemfirst.name = 'item_id_first_time'\nD = pd.merge(D,Itemfirst,on='item_id')\nD['item_id_first_time'] = D['date_block_num']-D['item_id_first_time']\n#sns.boxplot(D['item_id_first_time_lag']);","f57e4235":"cnt_keys = [[],\n            ['shop_id'],\n            ['item_id'],\n            ['item_cat1'],\n            ['item_cat2'],\n            ['shop_id','item_cat1'],\n            ['shop_id','item_cat2']]\n\nnew_monthfeatures_names = ['','shop', 'itemid','itemcat1','itemcat2','shop-itemcat1','shop-itemcat2']\nnew_monthfeatures_cnt_names = [\"month_%s_cnt\" % name for name in new_monthfeatures_names] # inserts a prefix and a suffix\nnew_monthfeatures_price_names = [\"month_%s_avgprice\" % name for name in new_monthfeatures_names] # inserts a prefix and a suffix","d484a437":"LAGS = [1,2,3,12]\nD.sort_values('date_block_num', inplace=True)\n\nlag_features = []\nfor feature_name,k in zip(new_monthfeatures_cnt_names,cnt_keys):\n    for lag in LAGS:\n        dftmp = D.groupby(k+['date_block_num'])['item_cnt_month'].mean()#.sum()\n        if(len(k)==0):\n            dftmp = dftmp.shift(lag)\n        else:\n            dftmp = dftmp.groupby(k).shift(lag)\n        dftmp.name = '%s-lag%d' % (feature_name,lag)\n        lag_features.append(dftmp.name)\n        D = D.merge(dftmp,how='left',on=k+['date_block_num'])\n        \nfor feature_name,k in zip(new_monthfeatures_price_names,cnt_keys):\n    for lag in LAGS:\n        dftmp = D.groupby(k+['date_block_num'])['item_price'].mean()\n        if(len(k)==0):\n            dftmp = dftmp.shift(lag)\n        else:\n            dftmp = dftmp.groupby(k).shift(lag)\n        dftmp.name = '%s-lag%d' % (feature_name,lag)\n        lag_features.append(dftmp.name)\n        D = D.merge(dftmp,how='left',on=k+['date_block_num'])\n    \nD[(D['shop_id']==24) & (D['item_id']==32)]","b9086a54":"prices_features_lag = [fname for fname in lag_features if 'price' in fname and 'item' in fname]\nprices_features_lag_relative_names = ['%s_relative' % fname for fname in prices_features_lag]\nfor fname,new_fname in zip(prices_features_lag,prices_features_lag_relative_names):\n    D[new_fname] = D[fname]\/D['alltime_itemid_avgprice']\nD.sample(3)","0e8ea467":"from sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\nD['item_cat1'] = LE.fit_transform(D['item_cat1'])\nD['item_cat2'] = LE.fit_transform(D['item_cat2'])","52d8e96b":"final_features = ['date_block_num','month','year','num_days_month','item_cnt_month','item_id_first_time','item_cat1','item_cat2','ID']\nfinal_features += lag_features\nfinal_features += new_alltimefeatures_cnt_names + new_alltimefeatures_price_names\nfinal_features += prices_features_lag_relative_names\nfinal_features","0f967b3c":"D=D[final_features]\nD=D.fillna(0)\nDtrain=D[(D['date_block_num']<=33) & (D['date_block_num']>=3)]\nassert(len(D[D['date_block_num']==34]) == len(Dtest))\nDtest = D[D['date_block_num']==34]\n#Dtrain.to_csv('date_train_final.csv', index=False)\n#Dtest.to_csv('date_test_final.csv', index=False)","2df6a427":"#Dtrain=Dtrain[Dtrain['date_block_num']<=7]\nX = Dtrain.drop(['item_cnt_month','ID'], axis=1)\nY = Dtrain['item_cnt_month']\nX.shape","3b972b23":"class TimeSplitter:\n    def __init__(self, n_splits=1):\n        self.n_splits=n_splits\n        \n    def split(self,X,y=None,groups=None):\n        date_block_num = X['date_block_num']\n        max_date = date_block_num.max()\n        for i in range(self.n_splits):\n            test_date=max_date-i\n            train_idxs, = np.where(date_block_num<test_date)\n            test_idxs, = np.where(date_block_num==test_date)\n            yield train_idxs,test_idxs\n        \n    def get_n_splits(self,X=None,y=None,groups=None):\n        return self.n_splits\n        ","9590557b":"from sklearn.base import BaseEstimator, clone\nfrom sklearn.linear_model import LinearRegression\n\nclass TimeStackingRegressor(BaseEstimator):\n    def __init__(self, estimators, final_estimator=LinearRegression(), n_splits=1, passthrough=False):\n        self.estimators=estimators\n        self.final_estimator = final_estimator\n        self.n_splits = n_splits\n        self.passthrough=passthrough\n        \n    def fit(self, X,y):\n        timesplitter = TimeSplitter(self.n_splits)\n        Xlvl2=[]\n        Xlvl1=[]\n        ylvl2=[]\n        for train_idxs, test_idxs in timesplitter.split(X):\n            preds = np.empty((len(test_idxs),len(self.estimators)))\n            Xtrain,ytrain = X.values[train_idxs], y.values[train_idxs]\n            Xtest,ytest = X.values[test_idxs], y.values[test_idxs]\n            for i,(name,estimator) in enumerate(self.estimators):\n                estimator = clone(estimator)\n                estimator.fit(Xtrain,ytrain)\n                preds[:,i]=estimator.predict(Xtest)\n            Xlvl2.append(preds)\n            Xlvl1.append(Xtest)\n            ylvl2.append(ytest)\n        Xlvl1 = np.vstack(Xlvl1)\n        Xlvl2 = np.vstack(Xlvl2)\n        ylvl2 = np.hstack(ylvl2)\n        if(self.passthrough):\n            Xlvl2 = np.hstack((Xlvl1,Xlvl2))\n        self.final_estimator.fit(Xlvl2, ylvl2)\n        \n        for name, estimator in self.estimators:\n            estimator.fit(X,y)\n        \n        return self\n    \n    def predict(self, X):\n        preds = np.empty((len(X),len(self.estimators)))\n        for i,(name,estimator) in enumerate(self.estimators):\n            preds[:,i] = estimator.predict(X)\n        if(self.passthrough):\n            Xlvl2 = np.hstack((X,preds))\n        else:\n            Xlvl2 = preds\n        return self.final_estimator.predict(Xlvl2)\n            ","237f662b":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_validate, PredefinedSplit, GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import svm\nfrom tqdm import tqdm\nimport time\nfrom sklearn.feature_selection import SelectPercentile, f_regression\nimport xgboost\n\nRANDOM_STATE=42\n\ndef createClassifiers():\n    clfs = []\n    lr = Pipeline([('feature_selector',SelectPercentile(f_regression, percentile=70)),\n                  ('lr',LinearRegression(n_jobs=-1))])\n    knn = KNeighborsRegressor(3, n_jobs=-1)\n    knn = Pipeline([('feature_selector',SelectPercentile(f_regression, percentile=50)),\n                    ('scaler',StandardScaler()),\n                    ('clf',knn)])\n    dt = DecisionTreeRegressor(min_impurity_decrease=0.001)\n    rf = RandomForestRegressor(n_estimators=100, max_features=0.5, min_impurity_decrease=0.001,\n                               criterion='mse', n_jobs=-1, random_state=RANDOM_STATE) #best_params for random forest{'max_features': 0.5, 'min_impurity_decrease': 0.001}\n    rf2 = GridSearchCV(RandomForestRegressor(n_estimators=100, criterion='mse',min_impurity_decrease=0.001, n_jobs=-1, random_state=RANDOM_STATE),\n                       {'max_features':[0.4,0.5,0.6]}, cv=TimeSplitter())\n    gbreg = GradientBoostingRegressor(n_estimators=50, learning_rate=0.15, min_impurity_decrease=0.001)\n    \n    \n    stack1_estimators= [('lr',lr),\n                        ('rf',rf)]\n    #stack1_reg = TimeStackingRegressor(estimators=stack1_estimators, n_splits=5)\n    xgb = xgboost.XGBRegressor(max_depth = 10, n_estimators=100, min_child_weight=200, subsample = 1, eta = 0.5, seed = RANDOM_STATE)\n\n    #clfs.append(('knn',knn))\n    #clfs.append(('dt',dt))\n    #clfs.append(('stacking1',stack1_reg))\n    #clfs.append(('rf',rf))\n    #clfs.append(('gbreg',gbreg))\n    #clfs.append(('rf2',rf2))\n    #clfs.append(('knn',knn))\n    clfs.append(('xgb',xgb))\n    clfs.append(('lr',lr))\n    \n    return clfs\n\n\nclfs = createClassifiers()","52837116":"import os \nimport pickle\n\nbest_model = None\nif(os.path.isdir('\/kaggle\/input\/my-modelpkl')):\n    with open('\/kaggle\/input\/my-modelpkl\/best_model.pkl','rb') as f:\n        best_model = pickle.load(f)","b1d4ffc7":"%%time\nif(best_model is None):\n    sampler = TimeSplitter()\n    Results={}\n    pbar = tqdm(clfs)\n    for clf_name,clf in pbar:\n        pbar.set_description(\"Training %s\" % clf_name)\n        Results[clf_name] = cross_validate(clf, X, Y, cv=sampler, scoring='neg_root_mean_squared_error', return_estimator=True, return_train_score=True)\n    best_model = Results['xgb']['estimator'][0]\n    \n    R = []\n    for clf_name,res in Results.items():\n        R.append([clf_name,'mse', -res['test_score'][0], -res['train_score'][0]])\n    R = pd.DataFrame(R, columns=['classifier name','metric','test_rms', 'train_rms'])\n    display(R)","375065ad":"from xgboost import plot_importance\n\n_, ax = plt.subplots(1,1,figsize=(10,14))\nplot_importance(booster=best_model, ax=ax);","143fc2da":"Dtest['item_cnt_month'] = best_model.predict(Dtest.drop(['item_cnt_month','ID'],axis=1))\nDtest = Dtest.sort_values('ID')\nDtest['ID'] = Dtest['ID'].astype(int)\nDtest['item_cnt_month'] = Dtest['item_cnt_month'].clip(0,20)\nDtest.to_csv('submission.csv', columns=['ID','item_cnt_month'], index=False)","9c6c197b":"We can see shops 25,28,31 and 54 sells a lot. Shop 31 is the shop with most item_counts, except in the last month where shop 25 sold a little more. Note that shop 25 is almost always the second shop with most sold items.\nInterestingly, shop 54 dissapeared after month 27.","2e3ae4ed":"<a id=\"regression\"><\/a>\n# 7. Regression","3e708453":"## 5.1 All-time based features\nThese features are made based on all months.\n\n### 5.1.1 Absolute item count and item price for each group","275d3b15":"### 5.2.3 Relative features\nFeatures that are the proportion\/ration of two features.","5f95cc9d":"Filling NA values and saving data. The first three dates (date_block_num<3) are not saved, since not enough lag time feature is available.","d8495b5e":"Maybe there are item_ids present in the test set but not in the train dataset. Lets see what are they:","123ca970":"There are about 20 duplicated names. I will use the one with the lowest item_id.","0172aa8c":"Adding month, year and number of days in month as features.","44976595":"## 5.2 Time-series based features or lag features\n### 5.2.1 First time appeared \nItems at launch date are more sold.","26a9b004":"Clip item counts to (0,20) due to the fact the test dataset is already clipped to (0,20)","bc73ed24":"### 5.2.2 Month Lag Features","d80c6e3f":"Separating features from target label:","ed867479":"### Other features","36a3e161":"Sampler to be used in time datasets.","f177a55b":"<a id=\"finaldataset\"><\/a>\n# 6. Final dataset","234acaa5":"As it can been seen, items with high item_cnt_month have low price low relative price. However, it is really strange that item_price_relative can have values like 10 or above.","06928e65":"Concatenating training dataset with test dataset so feature engineering is done on both at the same time.","622b0a39":"## 3.3 Item_id\n","ed600b27":"It seens reasoable to check if there are items sold much more above average price:","4b005e6b":"## 4.1 Item_price","62f61cb6":"* item_price: the price of a specific item and specific shop for that month.\n* alltime_avg_item_price: The average price of a specific item, averaged over all months and shops.\n* item_cnt_month: The target variable.","f7efc7c6":"## Predicting test dataset","98b091a6":"Most item sold have item_cnt_month high and at a item_price_relative=1. ","2da168d8":"<a id=\"data_preprocessing\"><\/a>\n# 2. Data Preprocessing, cleaning and Outliers removal\nLets see the data first","327e6a56":"What is the average price for item id 6066?","94b61920":"There is only one item of this type. I am getting rid of it.","747739ad":"Wow, there items being sold by more than 10 times the average price! Lets investigate what those items are.","4b685ad9":"Lets load dataset. I have got a dataset where item categories are translated to english.","f8e25cbf":"Its seens reasoable to put the average item price.","7f86fe08":"## 2.3 Data preparation\nTest dataset is concerned with month item solds, not days. We will process train data in order to become similar to test dataset. We will lose day information, but will insert it later somehow, if necessary.","3e458bd0":"Constructs all classifiers. Various classifiers were tested, but Random Forest and Xgboost are the best.","9452d7ae":"### Model Training\nTrains a model **if necessary**","aab18250":"**Table of contents**  \n[1. Introduction](#intro)  \n[2. Data Preprocessing and cleaning](#data_preprocessing)  \n[3. Temporal Feature Analysis](#temporal_analysis)  \n[4. Individual Feature Analysis](#individual_analysis)  \n[5. Feature Engineering](#feature_engineering)  \n[6. Final Dataset](#finaldataset)  \n[7. Regression](#regression)","07a49396":"### Feature Importance","d11494f0":"## item_price x time","c75b23d5":"## date_block_num\nLooking the histogram of date_block_num, we can clearly see a seasonality (12 months period) on the number of items sold on a month. Its seens to be in December. Lets investigate:","6237ae1c":"# 5.1 Date features","2bbd02fd":"# 2.1 Data cleaning\nLets see which items ids, categories ids are common in both train and test dataset.","b1ab9274":"Stacking algorithm. Did not worked for me quite as well as Xgboost.","1f5e968c":"The Average item price is inscreasing with time while the total money spent remains the same (except November,December and January). Maybe people are buying more expensive stuff. We will see this later.","499fda7c":"There are no missing values, but the histogram for item_price is really strange. This will be investigated.\n## 2.2 item_price","b6934a67":"Features based on the past months.","d6c5c2c4":"## 3.2 shop\nLets see the two shops with most items sold in each month","6f87afda":"* There are no missing values.\n* item_price with negatives values. This represents returned items or something like that.\n* There are only 60 shops ids.\n* Item_price and item_cnt_day probably have outliers.","e326c129":"In the graph we see a seasonality and a decreasing trend on the total number of items sold per month. However, there is no trend in the average number of itens sold per row.","c1fd3e97":"Most items on test dataset are in train dataset. There are 361 new items in test dataset that are not present in train dataset.","aa2b6d92":"There is a single row with negative value for item_price. Also, there is a huge value for item_price. Lets remove first this negative value and then proceed to investigate about the huge value.","392749ae":"<a id=\"temporal_analysis\"><\/a>\n# 3. Temporal Feature Analysis\n## 3.1 item_cnt_month x time","699b26f2":"<a id=\"intro\"><\/a>\n# 1. Introduction\nThis is a time-series problems, so data presents an order.  Time-based features are probably crucial information for achieveing a good performance. We may want also investigate features based on seasonality, as this is a sales dataset.","a60df7f8":"<a id=\"individual_analysis\"><\/a>\n# 4. Individual feature analysis\nLets start by analysing individual information of each feature, such as the data type, unique values, means, max, etc..","3eeb113f":"Most of it really seems to be new products launched only the month of the test dataset. I need to remember this later on the feature engineering phase. ","3af7db6a":"Items below the average price and\/or low price, are probably more sold at once. Lets investigate:","fc4550fb":"<a id=\"feature_engineering\"><\/a>\n# 5 Feature Engineering"}}