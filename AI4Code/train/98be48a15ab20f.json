{"cell_type":{"1824d4ee":"code","cdaeb110":"code","429dce09":"code","c2056ac6":"code","033fe153":"code","afab6d04":"code","e496e8f5":"code","d4a3dcb2":"code","a85d8ac6":"code","f36e3635":"code","929da0c8":"code","96e204f0":"code","bd67b5f9":"code","c2fae1b0":"code","024ee756":"code","be7c72a2":"code","dece2167":"code","6f665008":"code","123ec157":"code","0cc5537f":"code","24569c0f":"code","dd5cc1fa":"code","a29be7fa":"markdown","74cc993b":"markdown","4bb19259":"markdown","966a837c":"markdown","a94ffce2":"markdown","7cff1dcd":"markdown"},"source":{"1824d4ee":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns # used to plot interactive graph.\nfrom sklearn.metrics import f1_score, confusion_matrix  # evaluate models\n\n# tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re, string\n\n# NLP\nimport nltk\n","cdaeb110":"df_test = pd.read_csv('\/kaggle\/input\/question-classification-android-or-ios\/test.csv')\ndf_train = pd.read_csv('\/kaggle\/input\/question-classification-android-or-ios\/train.csv')\ndf_valid = pd.read_csv('\/kaggle\/input\/question-classification-android-or-ios\/valid.csv')","429dce09":"print(df_train.shape)\ndf_train.head()","c2056ac6":"print(df_valid.shape)\ndf_valid.head()","033fe153":"print(df_test.shape)\ndf_test.head(10)","afab6d04":"df_train.groupby(['LabelNum']).mean()","e496e8f5":"df_train.describe()","d4a3dcb2":"nltk.download('punkt')                # this is a tokenizer\nnltk.download('wordnet')                    # lexical database (determine base word)\nnltk.download('averaged_perceptron_tagger'); # context of a word\nnltk.download('stopwords'); # stopwords","a85d8ac6":"\n\ndef remove_noise(tweet_tokens, stop_words = ()):\n\n    cleaned_tokens = []\n\n    for token, tag in pos_tag(tweet_tokens):\n        token = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token) # remove tagging of users\n        token = re.sub(\"(<\\\/?\\w*>)\", \"\", token) # remove html\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens","f36e3635":"# stopwords to be parsed into function `remove_noise` defined above \nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\nall_tokens = df_train.apply(lambda row: nltk.word_tokenize(row['Title']), axis=1)\n","929da0c8":"cleaned_tokens = list()\nfor tokens in all_tokens:\n    cleaned_tokens.append(remove_noise(tokens, stop_words))\n    \ndf_train['cleaned_tokenized_titles'] = cleaned_tokens","96e204f0":"df_train.cleaned_tokenized_titles[df_train.LabelNum == 0]\n\ndef get_all_words(cleaned_tokens_list):\n    for tokens in cleaned_tokens_list:\n        for token in tokens:\n            yield token\n            \nfrom nltk import FreqDist\n\nfreq_dist_apple = FreqDist(get_all_words(df_train.cleaned_tokenized_titles[df_train.LabelNum == 1].values))\nfreq_dist_android = FreqDist(get_all_words(df_train.cleaned_tokenized_titles[df_train.LabelNum == 0].values))","bd67b5f9":"freq_dist_apple.most_common(10)","c2fae1b0":"freq_dist_android.most_common(10)","024ee756":"from sklearn.model_selection import train_test_split\nimport random\n\ndef prep_tokens_for_model(cleaned_tokens_list):\n    for tokens in cleaned_tokens_list:\n        yield dict([token, True] for token in tokens)\n\n\n# NLTK requires the data in this format:\nandroid_data = [(title, 'Android') for title in prep_tokens_for_model(df_train.cleaned_tokenized_titles.values[df_train.LabelNum == 0])]\napple_data = [(title, 'Apple') for title in prep_tokens_for_model(df_train.cleaned_tokenized_titles[df_train.LabelNum == 1])]\n\nX_train = android_data + apple_data\nrandom.shuffle(X_train)\n","be7c72a2":"from nltk import classify\nfrom nltk import NaiveBayesClassifier\n\nclf = NaiveBayesClassifier.train(X_train)\n\nprint(clf.show_most_informative_features(10))","dece2167":"testing = [i[0] for i in X_train] # removing y_test, the correct label\ny_test = [i[1] for i in X_train]  # saving y_test to evaluate the classifications\n\n# making predictions\ny_preds = list()\nfor test in testing:\n    y_preds.append(clf.classify(test))","6f665008":"print(\"y_preds length\", len(y_preds))\nprint(y_preds[:2])\nprint(\"y_test length\", len(y_test))\nprint(y_test[:2])","123ec157":"\n# evaluating by f1_score\n\nf1_score(y_test, y_preds, labels=['Android', 'Apple'], pos_label='Apple')\nconfusion_matrix(y_test, y_preds, labels=['Android', 'Apple'], normalize='true')","0cc5537f":"\nprint(\"Accuracy is: \", classify.accuracy(clf, X_train))\n","24569c0f":"def prep_for_model(df):\n    \n    apple = df_test.Title[df_test.LabelNum == 1].copy()\n    android = df_test.Title[df_test.LabelNum == 0].copy()\n    \n    apple_tokens = [nltk.word_tokenize(app) for app in apple.values]\n    android_tokens = [nltk.word_tokenize(andr) for andr in android.values]\n    \n    apple_cleaned_tokens = list()\n    android_cleaned_tokens = list()\n    \n    for tokens in apple_tokens:\n        apple_cleaned_tokens.append(remove_noise(tokens, stop_words))\n        \n    for tokens in android_tokens:\n        android_cleaned_tokens.append(remove_noise(tokens, stop_words))\n    \n    apple_tokens_for_model = prep_tokens_for_model(apple_cleaned_tokens)\n    android_tokens_for_model = prep_tokens_for_model(android_cleaned_tokens)\n    \n    data_android = [(title, \"Android\")\n                         for title in android_tokens_for_model]\n\n    data_apple = [(title, \"Apple\")\n                         for title in apple_tokens_for_model]\n    \n    X = data_android + data_apple\n    \n    random.shuffle(X)\n    \n    return X","dd5cc1fa":"X = prep_for_model(df_test)\n\nX_test = [i[0] for i in X] # removing y_test, the correct label\ny_test = [i[1] for i in X]  # saving y_test to evaluate the classifications\n\n# making predictions\ny_preds = list()\nfor test in X_test:\n    y_preds.append(clf.classify(test))\n    \n\nprint(\"F1 score is: \", f1_score(y_test, y_preds, labels=['Android', 'Apple'], pos_label='Apple'))\nprint(confusion_matrix(y_test, y_preds, labels=['Android', 'Apple'], normalize='true'))\n\nprint(\"Accuracy is: \", classify.accuracy(clf, X))","a29be7fa":"# Lets look at our tokens","74cc993b":"# **Evaluating Model on `test.csv`:**","4bb19259":"### **Tokenize**","966a837c":"# Naive Bayes Classification Model","a94ffce2":"# **Using NLTK to perform NLP** (on just the titles)","7cff1dcd":"## ***Very* Basic Data Viz**"}}