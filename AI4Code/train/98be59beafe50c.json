{"cell_type":{"30a717fe":"code","ef9e3e11":"code","2f56963e":"code","8cfd849d":"code","7a3e617e":"code","fbc7204b":"code","c4e704c7":"code","83bad5ba":"code","da42802e":"code","49ab4618":"code","2e7b0e26":"code","4e1c4381":"code","b2874b09":"code","5481d1dc":"code","de63333e":"code","385aafe7":"code","b75d2e85":"code","2a5784d7":"code","dcaf61f9":"code","b00ce922":"code","2b919259":"code","9f68416e":"code","bf5a1793":"code","04993b58":"code","00b2cf89":"code","3d7ea181":"code","fb0fab27":"markdown","14a49382":"markdown","334265d0":"markdown","a80946c8":"markdown","8e419b25":"markdown","163f3f7d":"markdown","cd3620bc":"markdown","302e0059":"markdown","e2a557e7":"markdown","311df3e7":"markdown","fa0b5bdb":"markdown","a56fda75":"markdown","605a0eb3":"markdown","105e7738":"markdown","5231c13b":"markdown"},"source":{"30a717fe":"!pip install ax-platform","ef9e3e11":"import numpy as np\nimport pandas as pd\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score\nimport seaborn as sns","2f56963e":"train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\",index_col=0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\",index_col=0)\ntrain.replace([np.inf, -np.inf], np.nan, inplace=True)\ntest.replace([np.inf, -np.inf], np.nan, inplace=True)\n","8cfd849d":"df=train\nprint(df.dropna().shape[0])\nprint(df.shape[0])\n(df.shape[0] - df.dropna().shape[0])\/df.shape[0]","7a3e617e":"np.isnan(test).sum(axis=0)","fbc7204b":"features = [x for x in train.columns.values if x[0]==\"f\"]\n\ntrain['max_row'] = train[features].max(axis=1)\ntrain['min_row'] = train[features].min(axis=1)\ntrain['std'] = train[features].std(axis=1)\ntrain['n_missing'] = train[features].isna().sum(axis=1)\n\n\ntest['max_row'] = test[features].max(axis=1)\ntest['min_row'] = test[features].min(axis=1)\ntest['std'] = test[features].std(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)","c4e704c7":"from sklearn.impute import KNNImputer\nimport numpy as np\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer,SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n#imputer = KNNImputer(n_neighbors= 1)\ntrain[:]= imputer.fit_transform(train)\ntest[:]= imputer.fit_transform(test)","83bad5ba":"sample_solution = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")","da42802e":"train.head()","49ab4618":"train[\"claim\"].value_counts()","2e7b0e26":"test.head()","4e1c4381":"sample_solution.head()","b2874b09":"train_targets = train.pop(\"claim\")","5481d1dc":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (15, 71))\ncols = train.columns.tolist()[1:119]\nfor i in cols:\n    plt.subplot(24,5,cols.index(i)+1)\n    sns.set_style(\"white\")\n    plt.title(i, size = 12, fontname = 'monospace')\n    a = sns.kdeplot(train[i], color = '#f9ba32', linewidth = 1.3)\n    sns.kdeplot(test[i], color = '#426e86', linewidth = 1.3)\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.figtext(0.335, 1.02, 'Distribution of features', color = '#2f3131', fontname = 'monospace', size = 25)\nplt.figtext(0.3, 1.01, 'train', color = '#f9ba32', fontname = 'monospace', size = 18)\nplt.figtext(0.66, 1.01, 'test', color = '#426e86', fontname = 'monospace', size = 18)\n\nplt.show()","de63333e":"train_targets.head()","385aafe7":"for item in train.columns:\n    if abs(train[item].max()) \/ (abs(train[item].min()) + 10e-10) > 20:\n        train[item] = np.sign(train[item]) * np.log2(np.abs(train[item]) + 1)\n        test[item] = np.sign(test[item]) * np.log2(np.abs(test[item]) + 1)\n    train_mean = train[item].mean()\n    train_std = train[item].std()\n    train[item] = (train[item] - train_mean) \/ train_std\n    test[item] = (test[item] - train_mean) \/ train_std\n    # Missing Value Imputation seems to have a bad effect to final results\n    #train[item].replace(np.NAN, train[item].mean(), inplace=True)\n    #test[item].replace(np.NAN, test[item].mean(), inplace=True)\n","b75d2e85":"train_features, valid_features, train_targets, valid_targets = train_test_split(train, train_targets, test_size=0.01, random_state=np.random.randint(1000))\ntrain_features.shape, train_targets.shape, valid_features.shape, valid_targets.shape","2a5784d7":"def evaluate(valid_targets, probs, name):\n    y_pred = np.array(probs > 0.5, dtype=int)\n    acc = accuracy_score(valid_targets, y_pred)\n    loss = log_loss(valid_targets, y_pred)\n    auc = roc_auc_score(valid_targets, probs)\n    print(\"Accuracy score: %.2f\"%(acc))\n    print(\"Log loss: %.2f\"%(loss))\n    print(\"AUC score:\", auc)\n    print(\"Classification report:\")\n    print(classification_report(valid_targets, y_pred))\n    return {\n        \"name\": name, \n        \"accuracy_score\": acc, \n        \"log_loss\": loss, \n        \"auc\": auc\n    }","dcaf61f9":"train_features.shape","b00ce922":"def hyperparameter(params):\n    cat_params={'iterations': 5000, \n      'loss_function': 'Logloss', \n      'depth':params.get('depth', 7), \n      'task_type' : 'GPU',\n      'use_best_model': True,\n      'eval_metric': 'AUC',\n      'early_stopping_rounds': 500,\n      'learning_rate': params.get('lr', 0.03),\n      'border_count': params.get('bc', 32),\n      'l2_leaf_reg': params.get('leaf', 3),\n      \"verbose\": 500}\n    cat = CatBoostClassifier(\n        **cat_params\n    )\n    cat.fit(train_features, train_targets,eval_set=[(valid_features, valid_targets)])\n    probs = cat.predict_proba(valid_features)[:, 1]\n    return roc_auc_score(valid_targets, probs)\n\n","2b919259":"'''from ax.plot.contour import plot_contour\nfrom ax.plot.trace import optimization_trace_single_method\nfrom ax.service.managed_loop import optimize\nfrom ax.utils.notebook.plotting import render\nbest_parameters, values, experiment, model=optimize(\n    parameters=[\n        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-6, 0.1], \"log_scale\": True},\n        {\"name\": 'depth', \"type\": \"range\", \"bounds\": [5, 10]},\n        {\"name\": \"bc\", \"type\": \"range\", \"bounds\": [30, 40]},\n        {\"name\": \"leaf\",\"type\":\"range\",\"bounds\":[0,5]},\n\n    ],\n     \n    evaluation_function=hyperparameter,\n    minimize=False\n)'''","9f68416e":"#best_parameters","bf5a1793":"cat_params = {\n    'iterations': 20000, \n    'loss_function': 'Logloss', \n    'depth': 8, \n    'task_type' : 'GPU',\n    'use_best_model': True,\n    'eval_metric': 'AUC',\n    'early_stopping_rounds': 1000,\n    'learning_rate': 0.01,\n    'border_count': 33,\n    'l2_leaf_reg': 1,\n    \"verbose\": 500\n}\ncat = CatBoostClassifier(\n    **cat_params\n)\ncat.fit(train_features, train_targets, eval_set=[(valid_features, valid_targets)])","04993b58":"probs = cat.predict_proba(valid_features)[:, 1]\nprobs[:10]","00b2cf89":"result_cat = evaluate(valid_targets, probs, \"catboost\")\nresult_cat","3d7ea181":"claim = cat.predict_proba(test)[:, 1]\n#claim=cat.predict(test, num_iteration=cat.best_iteration)\nsample_solution[\"claim\"] = claim\nsample_solution.to_csv(\"submission.csv\", index=False)","fb0fab27":"## Import Packages","14a49382":"## Model Development & Evaluation\n","334265d0":"Percent of NaNs","a80946c8":"### Train Validation Split","8e419b25":"## Import datasets","163f3f7d":" <font size=\"5\">Hyper Parameter tunning <\/font>\n","cd3620bc":"## Feature Scaling","302e0059":"## Bayesian grid search","e2a557e7":"## Using CatBoost","311df3e7":"## Feature Engineering\n\nExtra feature column generation for better prediction output.","fa0b5bdb":"Installation of ax-platform for bayesian search","a56fda75":"## Submisssion","605a0eb3":"### Get Train data Targets","105e7738":"## EDA & Data Preprocessing","5231c13b":"### Evaluation Method"}}