{"cell_type":{"b8305d22":"code","73ed521f":"code","39012091":"code","9faa303d":"code","cdc1536a":"code","32b6aeb4":"code","e9feac19":"code","59d82471":"code","b8f3e752":"code","dc93c560":"code","0c4890d0":"code","f26ccc71":"code","52ad87e0":"code","797653c1":"code","587704a8":"code","8ce9a6a7":"code","288b8c39":"code","28631003":"code","39ae00c7":"code","7699e72e":"code","3f3d496b":"code","934156dd":"code","4f28eff8":"code","59a8548b":"code","939a4c69":"markdown","5ec93b1e":"markdown","68111d6e":"markdown","ea5a76ca":"markdown","6bec2621":"markdown","d26f467a":"markdown","340e60c0":"markdown","f98c634d":"markdown","52df43ce":"markdown","afac7b07":"markdown","155bc81c":"markdown","30e99bd7":"markdown","0ae38de9":"markdown","794d0a71":"markdown","ee920a40":"markdown","02787d15":"markdown","3b0aceca":"markdown","049a9bff":"markdown","2c52d8e3":"markdown","91620220":"markdown","fcd01228":"markdown","09def438":"markdown","e582906f":"markdown","72287f2e":"markdown","0a0d7d88":"markdown","90cd2e7d":"markdown","2beddfc8":"markdown","05552919":"markdown","c0a46965":"markdown","dc42869c":"markdown","89f9d18d":"markdown"},"source":{"b8305d22":"#!pip install efficientnet >> \/dev\/null\n!pip install ..\/input\/tf-efficientnet-b4\/efficientnet-1.0.0-py3-none-any.whl >> \/dev\/null","73ed521f":"# LOAD LIBRARIES\nimport time\nstartNB = time.time()\nimport tensorflow as tf, os\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import recall_score, accuracy_score\nimport albumentations as albu, cv2, gc\nprint('TensorFlow version =',tf.__version__)","39012091":"# CONFIGURE GPUs\n#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\ngpus = tf.config.list_physical_devices('GPU'); print(gpus)\nif len(gpus)==1: strategy = tf.distribute.OneDeviceStrategy(device=\"\/gpu:0\")\nelse: strategy = tf.distribute.MirroredStrategy()","9faa303d":"# ENABLE MIXED PRECISION for speed\n#tf.config.optimizer.set_jit(True)\ntf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nprint('Mixed precision enabled')","cdc1536a":"# VERSION MAJOR and MINOR for logging\nmm = 1; rr = 0\n\n# BEGIN LOG FILE\nf = open(f'log-{mm}-{rr}.txt','a')\nprint('Logging to \"log-%i-%i.txt\"'%(mm,rr))\nf.write('#############################\\n')\nf.write(f'Trial mm={mm}, rr={rr}\\n')\nf.write('efNetB4, batch_size=512, seed=42, 64x64, fold=0, LR 1e-3 with 0.75 decay\\n')\nf.write('#############################\\n')\nf.close()\n\nBATCH_SIZE = 512\nDIM = 64","32b6aeb4":"%%time\ntrain = []\n#for x in [0,1,2,3]:\nfor x in [0]:\n    f = 'train_image_data_%i.parquet'%x\n    print (f,end='')\n    img = pd.read_parquet('..\/input\/bengaliai-cv19\/'+f) # Pandas dataframe\n    img = img.iloc[:,1:].values.reshape((-1,137,236,1)) # Numpy Array\n    img2 = np.zeros((img.shape[0],DIM,DIM,1),dtype='float32')\n    for j in range(img.shape[0]):\n        img2[j,:,:,0] = cv2.resize(img[j,],(DIM,DIM),interpolation = cv2.INTER_AREA)\n        if j%1000==0: print(j,', ',end='')\n    print()\n    img2 = (255 - img2)\/255.\n    train.append(img2)","e9feac19":"X_train = np.concatenate(train)\nprint('Train Shape',X_train.shape)","59d82471":"del img, img2, train\n_ = gc.collect()","b8f3e752":"row=3; col=4;\nplt.figure(figsize=(20,(row\/col)*12))\nfor x in range(row*col):\n    plt.subplot(row,col,x+1)\n    plt.imshow(X_train[x,:,:,0])\nplt.show()","dc93c560":"train = pd.read_csv('..\/input\/bengaliai-cv19\/train.csv')\ny_train = train.iloc[:,1:4].values[:len(X_train)]\nprint('Labels\\nGrapheme Root, Vowel Diacritic, Consonant Diacritic')\ny_train","0c4890d0":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, X, y, list_IDs, batch_size=BATCH_SIZE, shuffle=False, augment=False, \n            labels=True, cutmix=False, yellow=False): \n\n        self.X = X\n        self.y = y\n        self.augment = augment\n        self.list_IDs = list_IDs\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.labels = labels\n        self.cutmix = cutmix\n        self.yellow = yellow\n        self.on_epoch_end()\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.list_IDs) \/\/ self.batch_size\n        ct += int((len(self.list_IDs) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X, y = self.__data_generation(indexes)\n        if self.augment: X = self.__augment_batch(X)\n        if self.labels: return X, [y[:,0:168],y[:,168:179],y[:,179:186]]\n        else: return X\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange( len(self.list_IDs) )\n        if self.shuffle: np.random.shuffle(self.indexes)\n\n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples'        \n        X = self.X[ self.list_IDs[indexes], ]\n        if self.yellow: X = np.ones((len(indexes),DIM,DIM,1))\n        y = np.zeros((len(indexes),186))\n        for j in range(len(indexes)):\n            y[j, int(self.y[ self.list_IDs[indexes[j]],0]) ] = 1\n            y[j, 168 + int(self.y[ self.list_IDs[indexes[j]],1]) ] = 1\n            y[j, 179 + int(self.y[ self.list_IDs[indexes[j]],2]) ] = 1\n            \n        if self.cutmix:\n            for j in range(len(indexes)):\n                \n                # CHOOSE RANDOM CENTER\n                yy = np.random.randint(0,DIM)\n                xx = np.random.randint(0,DIM)\n                z = np.random.choice(self.list_IDs)\n                               \n                # CHOOSE RANDOM WIDTH AND HEIGHT\n                h = np.random.randint(DIM\/\/2-DIM\/\/16,DIM\/\/2+DIM\/\/16)\n                w = np.random.randint(DIM\/\/2-DIM\/\/16,DIM\/\/2+DIM\/\/16)\n                        \n                # CUT AND MIX IMAGES\n                ya = max(0,yy-h\/\/2); yb = min(DIM,yy+h\/\/2)\n                xa = max(0,xx-w\/\/2); xb = min(DIM,xx+w\/\/2)\n                X[j,ya:yb,xa:xb,0] = self.X[z,ya:yb,xa:xb,0]\n                \n                # CUT AND MIX LABELS\n                r = (yb-ya)*(xb-xa)\/DIM\/DIM\n                y2 = np.zeros((1,186))\n                y2[0, int(self.y[ z,0]) ] = 1\n                y2[0, 168 + int(self.y[ z,1]) ] = 1\n                y2[0, 179 + int(self.y[ z,2]) ] = 1                \n                y[j,] = (1-r)*y[j,] + r*y2[0,]\n                    \n        return X, y\n \n    def __random_transform(self, img):\n        composition = albu.Compose([\n            albu.OneOf([\n                albu.ShiftScaleRotate(rotate_limit=8,scale_limit=0.16,shift_limit=0,border_mode=0,value=0,p=0.5),\n                albu.CoarseDropout(max_holes=16,max_height=DIM\/\/10,max_width=DIM\/\/10,fill_value=0,p=0.5)\n            ], p=0.5),\n            albu.ShiftScaleRotate(rotate_limit=0, scale_limit=0., shift_limit=0.12, border_mode=0, value=0, p=0.5)\n        ])\n        return composition(image=img)['image']\n    \n    def __augment_batch(self, img_batch):\n        for i in range(img_batch.shape[0]):\n            img_batch[i, ] = self.__random_transform(img_batch[i, ])\n        return img_batch","f26ccc71":"print('Cutmix Augmentation with first image displayed as all yellow')\ngen = DataGenerator(X_train,y_train,np.arange(len(X_train)),shuffle=True,augment=True,\n            batch_size=BATCH_SIZE,cutmix=True,yellow=True)\n\nrow=3; col=4;\nplt.figure(figsize=(20,(row\/col)*12))\nfor batch in gen:\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.imshow(batch[0][j,:,:,0])\n    plt.show()\n    break","52ad87e0":"def build_model():\n    \n    inp = tf.keras.Input(shape=(DIM,DIM,1))\n    inp2 = tf.keras.layers.Concatenate()([inp, inp, inp])\n    #base_model = efn.EfficientNetB4(weights='imagenet',include_top=False, input_shape=(DIM,DIM,3))\n    base_model = efn.EfficientNetB4(weights=None,include_top=False, input_shape=(DIM,DIM,3)) \n    base_model.load_weights('..\/input\/tf-efficientnet-b4\/efnB4.h5')\n\n    x = base_model(inp2)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)    \n    x1 = tf.keras.layers.Dense(168, activation='softmax',name='x1',dtype='float32')(x)\n    x2 = tf.keras.layers.Dense(11, activation='softmax',name='x2',dtype='float32')(x)\n    x3 = tf.keras.layers.Dense(7, activation='softmax',name='x3',dtype='float32')(x)\n    \n    model = tf.keras.Model(inputs=inp, outputs=[x1,x2,x3])\n    opt = tf.keras.optimizers.Adam(lr=0.00001)\n    wgt = {'x1': 1.5, 'x2': 1.0, 'x3':1.0}\n    model.compile(loss='categorical_crossentropy', optimizer = opt,\\\n              metrics=['categorical_accuracy'], loss_weights=wgt)\n        \n    return model","797653c1":"from sklearn.metrics import f1_score\nclass CustomCallback(tf.keras.callbacks.Callback):\n    def __init__(self, valid_data, target, fold, mm=0, rr=0, patience=10):\n        self.valid_inputs = valid_data\n        self.valid_outputs = target\n        self.fold = fold\n        self.patience = patience\n        self.mm = mm\n        self.rr = rr\n        \n    def on_train_begin(self, logs={}):\n        self.valid_f1 = [0]\n        \n    def on_epoch_end(self, epoch, logs={}):\n        \n        preds = self.model.predict(self.valid_inputs)\n        preds0 = np.argmax(preds[0],axis=1)\n        preds1 = np.argmax(preds[1],axis=1)\n        preds2 = np.argmax(preds[2],axis=1)\n        \n        r1 = recall_score(self.valid_outputs[0],preds0,average='macro')\n        r2 = recall_score(self.valid_outputs[1],preds1,average='macro')\n        r3 = recall_score(self.valid_outputs[2],preds2,average='macro')\n                \n        a1 = accuracy_score(self.valid_outputs[0],preds0)\n        a2 = accuracy_score(self.valid_outputs[1],preds1)\n        a3 = accuracy_score(self.valid_outputs[2],preds2)\n        \n        f1 = 0.5*r1+0.25*r2+0.25*r3\n                \n        # LOG TO FILE\n        f = open('log-%i-%i.txt'%(self.mm,self.rr),'a')\n        f.write('#'*25); f.write('\\n')\n        f.write('#### FOLD %i EPOCH %i\\n'%(self.fold+1,epoch+1))\n        f.write('#### ACCURACY: a1=%.5f, a2=%.5f, a3=%.5f\\n' % (a1,a2,a3) )\n        f.write('#### MACRO RECALL: r1=%.5f, r2=%.5f, r3=%.5f\\n' % (r1,r2,r3) )\n        f.write('#### CV\/LB: %.5f\\n' % f1 ) \n       \n        print('\\n'); print('#'*25)\n        print('#### FOLD %i EPOCH %i'%(self.fold+1,epoch+1))\n        print('#### ACCURACY: a1=%.5f, a2=%.5f, a3=%.5f' % (a1,a2,a3) )\n        print('#### MACRO RECALL: r1=%.5f, r2=%.5f, r3=%.5f' % (r1,r2,r3) )\n        print('#### CV\/LB: %.5f' % f1 )\n        print('#'*25)\n\n        self.valid_f1.append(f1)\n        x = np.asarray(self.valid_f1)\n        if np.argsort(-x)[0]==(len(x)-self.patience-1):\n            print('#### CV\/LB no increase for %i epochs: EARLY STOPPING' % self.patience)\n            f.write('#### CV\/LB no increase for %i epochs: EARLY STOPPING\\n' % self.patience)\n            self.model.stop_training = True\n            \n        if (f1>0.000)&(f1>np.max(self.valid_f1[:-1])):\n            print('#### Saving new best...')\n            f.write('#### Saving new best...\\n')\n            self.model.save_weights('fold%i-m%i-%i.h5' % (self.fold,self.mm,self.rr))\n            \n        f.close()","587704a8":"# CUSTOM LEARNING SCHEUDLE\nLR_START = 1e-5\nLR_MAX = 1e-3\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_STEP_DECAY = 0.75\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)\/\/10)\n    return lr\n    \nlr2 = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\nrng = [i for i in range(100)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y); \nplt.xlabel('epoch',size=14); plt.ylabel('learning rate',size=14)\nplt.title('Training Schedule',size=16); plt.show()","8ce9a6a7":"# TRAIN MODEL\n\noof1 = np.zeros((X_train.shape[0],168))\noof2 = np.zeros((X_train.shape[0],11))\noof3 = np.zeros((X_train.shape[0],7))\n\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor fold,(idxT,idxV) in enumerate(skf.split(X_train,y_train[:,0])):\n         \n    print('#'*25)\n    print('### FOLD %i' % (fold+1))\n    print('### train on %i images. validate on %i images'%(len(idxT),len(idxV)))\n    print('#'*25)\n    \n    K.clear_session()\n    with strategy.scope():\n        model = build_model()\n    \n    train_gen = DataGenerator(X_train,y_train,idxT,shuffle=True,augment=True,\n            batch_size=BATCH_SIZE,cutmix=True)\n    val_x = DataGenerator(X_train,y_train,idxV,shuffle=False,augment=False,cutmix=False,\n            labels=False,batch_size=BATCH_SIZE*4)\n    val_y = [y_train[idxV,0],y_train[idxV,1],y_train[idxV,2]]\n        \n\n    cc = CustomCallback(valid_data=val_x, target=val_y, fold=fold, mm=mm, rr=rr, patience=15)\n    h = model.fit(train_gen, epochs = 20, verbose=1, callbacks=[cc,lr2])\n\n    print('#### Loading best weights...')\n    model.load_weights('fold%i-m%i-%i.h5' % (fold,mm,rr))\n    \n    val_x = DataGenerator(X_train,y_train,idxV,shuffle=False,augment=False,cutmix=False,\n            labels=False,batch_size=BATCH_SIZE*4)\n    oo = model.predict(val_x)\n    oof1[idxV,] = oo[0]\n    oof2[idxV,] = oo[1]\n    oof3[idxV,] = oo[2]\n\n    # SAVE OOF and IDXV\n    np.save('oof1-%i-%i'%(mm,rr),oof1)\n    np.save('oof2-%i-%i'%(mm,rr),oof2)\n    np.save('oof3-%i-%i'%(mm,rr),oof3)\n    np.save('idxV-%i-%i'%(mm,rr),idxV)\n    np.save('y_train-%i-%i'%(mm,rr),y_train)\n    break","288b8c39":"oo = model.predict(val_x,verbose=1)\noof1[idxV,] = oo[0]\noof2[idxV,] = oo[1]\noof3[idxV,] = oo[2]","28631003":"oof1b = np.argmax(oof1[idxV,],axis=1)\noof2b = np.argmax(oof2[idxV,],axis=1)\noof3b = np.argmax(oof3[idxV,],axis=1)\n\nr1 = recall_score(y_train[idxV,0],oof1b,average='macro')\nr2 = recall_score(y_train[idxV,1],oof2b,average='macro')\nr3 = recall_score(y_train[idxV,2],oof3b,average='macro')\nr = 0.5*r1+0.25*r2+0.25*r3\nprint('CV\/LB without Post Process %.5f'%r)","39ae00c7":"EXP = -0.9\n\ns = pd.Series(oof1b)\nvc = s.value_counts().sort_index()\nmat1 = np.diag(vc.astype('float32')**(EXP))\n\ns = pd.Series(oof2b)\nvc = s.value_counts().sort_index()\nmat2 = np.diag(vc.astype('float32')**(EXP))\n\ns = pd.Series(oof3b)\nvc = s.value_counts().sort_index()\nmat3 = np.diag(vc.astype('float32')**(EXP))","7699e72e":"oof1b = np.argmax(oof1[idxV,].dot(mat1),axis=1)\noof2b = np.argmax(oof2[idxV,].dot(mat2),axis=1)\noof3b = np.argmax(oof3[idxV,].dot(mat3),axis=1)\n\nr1 = recall_score(y_train[idxV,0],oof1b,average='macro')\nr2 = recall_score(y_train[idxV,1],oof2b,average='macro')\nr3 = recall_score(y_train[idxV,2],oof3b,average='macro')\nr = 0.5*r1+0.25*r2+0.25*r3\nprint('CV\/LB with Post Process %.5f'%r)","3f3d496b":"# PREPROCESS TEST\ndef getTest(x):\n    f = 'test_image_data_%i.parquet'%x\n    img = pd.read_parquet('..\/input\/bengaliai-cv19\/'+f) # Pandas dataframe\n    img = img.iloc[:,1:].values.reshape((-1,137,236,1)) # Numpy Array\n    img2 = np.zeros((img.shape[0],DIM,DIM,1),dtype='float32')\n    for j in range(img.shape[0]):\n        img2[j,:,:,0] = cv2.resize(img[j,],(DIM,DIM),interpolation = cv2.INTER_AREA)\n    return (255 - img2)\/255.","934156dd":"EXP = -0.9\ndel X_train; gc.collect()\n\n# LOAD BEST MODEL\nmodel.load_weights('fold%i-m%i-%i.h5' % (fold,mm,rr))\n\n# PREDICT TEST\npreds1 = []; preds2 = []; preds3 = []\nfor x in [0,1,2,3]:\n    preds = model.predict(getTest(x),verbose=1)\n    preds1.append(preds[0])\n    preds2.append(preds[1])\n    preds3.append(preds[2])\n\npred1 = np.argmax(np.vstack(preds1),axis=1)\npred2 = np.argmax(np.vstack(preds2),axis=1)\npred3 = np.argmax(np.vstack(preds3),axis=1)\n\n# APPLY POST PROCESS\ns = pd.Series(pred1)\nvc = s.value_counts().sort_index()\ndf = pd.DataFrame({'a':np.arange(168),'b':np.ones(168)})\ndf.b = df.a.map(vc); df.fillna(df.b.min(),inplace=True)\nmat1 = np.diag(df.b.astype('float32')**(EXP))\n\ns = pd.Series(pred2)\nvc = s.value_counts().sort_index()\ndf = pd.DataFrame({'a':np.arange(11),'b':np.ones(11)})\ndf.b = df.a.map(vc); df.fillna(df.b.min(),inplace=True)\nmat2 = np.diag(df.b.astype('float32')**(EXP))\n\ns = pd.Series(pred3)\nvc = s.value_counts().sort_index()\ndf = pd.DataFrame({'a':np.arange(7),'b':np.ones(7)})\ndf.b = df.a.map(vc); df.fillna(df.b.min(),inplace=True)\nmat3 = np.diag(df.b.astype('float32')**(EXP))\n\n# APPLY POST PROCESS\npred1 = np.argmax(np.vstack(preds1).dot(mat1),axis=1)\npred2 = np.argmax(np.vstack(preds2).dot(mat2),axis=1)\npred3 = np.argmax(np.vstack(preds3).dot(mat3),axis=1)","4f28eff8":"row_id = []; target = []\nfor i in range(len(pred1)):\n    row_id += [f'Test_{i}_grapheme_root', f'Test_{i}_vowel_diacritic', f'Test_{i}_consonant_diacritic']\n    target += [pred1[i], pred2[i], pred3[i]]\nsub = pd.DataFrame({'row_id': row_id, 'target': target})\nsub.to_csv('submission.csv', index=False)\nsub.head()","59a8548b":"print('Notebook elapsed time = %.1f minutes'%((time.time()-startNB)\/60))","939a4c69":"![white4.jpg](attachment:white4.jpg)","5ec93b1e":"## Preprocess - EDA","68111d6e":"# Gold - 14th place with teammates: Bojan, Shai, Yasin, Chris, Jahmed\n![LB4.jpg](attachment:LB4.jpg)","ea5a76ca":"![header2.jpg](attachment:header2.jpg)\n![desc2.jpg](attachment:desc2.jpg)","6bec2621":"![white4.jpg](attachment:white4.jpg)","d26f467a":"# Train Model\nWhen validation score doesn't increase for 15 epochs, we stop training to save time. In this tutorial, we only train for 20 epochs. If you increase training to 150 epochs, then this model can score CV\/LB 0.98. (Also change code to load 100% train images instead of 25%). Using larger image size and larger EfficientNet can increase CV\/LB to 0.99","340e60c0":"![white6.jpg](attachment:white6.jpg)","f98c634d":"![white4.jpg](attachment:white4.jpg)","52df43ce":"![white4.jpg](attachment:white4.jpg)","afac7b07":"# STEP 1 : Preprocess\nLoad image data and resize. For optimal GPU speed, we do this one time before the data loader instead of every time each epoch in the data loader. In this tutorial, we only load 25% of data. ","155bc81c":"# STEP 2: Data Augmentation\n# IMPORTANT: Optimize data loader for GPU Speed!\nBuild data loader. We want the time to process one batch on CPU less than the time to train one batch on GPU. Also we want to maximize batch size to fully utilize GPU compute.","30e99bd7":"## Preprocess Test","0ae38de9":"![slide3.jpg](attachment:slide3.jpg)","794d0a71":"# Let's Review","ee920a40":"![white6.jpg](attachment:white6.jpg)","02787d15":"# Validation Metric and Logging\nCompetition metric is macro recall","3b0aceca":"![white6.jpg](attachment:white6.jpg)","049a9bff":"# STEP 5: Post Process\nCompetition metric is macro recall. Training loss did not optimize this so we will below.","2c52d8e3":"![white4.jpg](attachment:white4.jpg)","91620220":"# STEP 3: Build Model\nWe will use transfer learning from a pretrained ImageNet CNN\n![pretrain2.jpg](attachment:pretrain2.jpg)","fcd01228":"## Data Augmentation - EDA\nCutmix augmentation mixes two images together. In the examples below, the first of the two images has been displayed as all yellow to help us visualize the augmentation better. (During training, the first image will not be converted to yellow). We also observe rotation, scaling, shift and coarse dropout augmentation below.","09def438":"## Predict and Postprocess Test","e582906f":"![white6.jpg](attachment:white6.jpg)","72287f2e":"![slide1.jpg](attachment:slide1.jpg)","0a0d7d88":"![slide2.jpg](attachment:slide2.jpg)","90cd2e7d":"![white6.jpg](attachment:white6.jpg)","2beddfc8":"# Predict Test and Submit to Kaggle\nAfter experimenting many times, we use our best model to predict Kaggle test set.","05552919":"# Initialize Enviroment","c0a46965":"## Write Submission CSV","dc42869c":"# STEP 4: Training Schedule","89f9d18d":"![slide3.jpg](attachment:slide3.jpg)\n![slide2.jpg](attachment:slide2.jpg)"}}