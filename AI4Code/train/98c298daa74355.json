{"cell_type":{"0679a237":"code","1406c3d8":"code","3988f313":"code","a0a3a452":"code","e52d694c":"code","ebb4a017":"code","fee3c5a1":"code","78ec208d":"code","e75b6802":"code","e6bafa61":"code","9d84c761":"code","b5ccb43a":"code","ea56c89a":"code","da784b1d":"code","7eb8ea1b":"code","6605dd3d":"markdown","2a6f8e6c":"markdown","12f82674":"markdown","f69f2503":"markdown","93f2bf5c":"markdown","959f1d89":"markdown"},"source":{"0679a237":"import numpy as np \nimport pandas as pd \nimport sklearn.datasets\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom tqdm import tqdm\n\nimport os\nprint(os.listdir(\"..\/input\"))\n%matplotlib inline","1406c3d8":"X, y = sklearn.datasets.make_classification(n_samples=5000, n_features=20, n_informative=20, n_redundant=0, n_repeated=0, n_classes=2)","3988f313":"X_train, X_test, y_train, y_test = train_test_split(X, y)","a0a3a452":"X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\nX_test = torch.from_numpy(X_test).type(torch.FloatTensor)\ny_train = torch.from_numpy(y_train).type(torch.FloatTensor)\ny_test = torch.from_numpy(y_test).type(torch.FloatTensor)","e52d694c":"class CustomDataset(Dataset):\n    def __init__(self, x_tensor, y_tensor):\n        self.x = x_tensor\n        self.y = y_tensor\n    \n    def __getitem__(self, index):\n        return (self.x[index], self.y[index])\n    \n    def __len__(self):\n        return len(self.x)","ebb4a017":"train_data = CustomDataset(X_train, y_train)\ntest_data = CustomDataset(X_test, y_test)","fee3c5a1":"train_loader = DataLoader(train_data, batch_size=64, shuffle=True)","78ec208d":"class NN_Classifier(nn.Module):\n    \n    def __init__(self):\n        super(NN_Classifier, self).__init__()\n        self.in_layer = nn.Linear(in_features=20, out_features=40)\n        self.hidden_layer = nn.Linear(in_features=40, out_features=1)\n        self.out_layer = nn.Sigmoid()\n    \n    def forward(self, x):\n        x = self.in_layer(x)\n        x = self.hidden_layer(x)\n        x = self.out_layer(x)\n        \n        return x\n    \n    def predict(self, x):\n        pred = self.forward(x)\n        return pred","e75b6802":"model = NN_Classifier()\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)","e6bafa61":"model.eval()","9d84c761":"epochs = 1000","b5ccb43a":"losses = []\nfor i in range(epochs):\n    batch_loss = []\n    for x_batch, y_batch in train_loader:\n        y_pred = model.forward(x_batch)\n        loss = criterion(y_pred, y_batch)\n        losses.append(loss.item())\n        batch_loss.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    if i%100==0:\n        print('Epoch %s: ' % i + str(np.mean(batch_loss)))","ea56c89a":"y_hat = model.predict(X_test)","da784b1d":"yhat = []\nfor i in y_hat:\n    if i >= .5:\n        yhat.append(1)\n    else:\n        yhat.append(0)","7eb8ea1b":"print(classification_report(y_test, yhat))","6605dd3d":"** Create the model itself. You need to have function to make all of the different layers (init), a forward function that actually creates the networks architecture (forward), and a predict function to get your answers out (predict). The way I've architected this network I could just call forward instead of predict but I like to keep them seperate. **","2a6f8e6c":"** Standard Train\/Test split **","12f82674":"** Convert the samples into Torch Tensors. I used Floats in this instance because of my approach using Binary Cross Entropy loss and a Sigmoid output layer. **","f69f2503":"**Create a dummy dataset**","93f2bf5c":"** I like to look at the networks architecture, even when it's really simple like this example. **","959f1d89":"** Instantiate model, determine loss, and optimizer. **"}}