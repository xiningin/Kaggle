{"cell_type":{"a4983aac":"code","7d2436c9":"code","1edd18a4":"code","5fcc7880":"code","7470996a":"code","b6c7449b":"code","6c77657d":"code","1787b8b4":"code","42c46930":"code","2e102872":"code","4243d11c":"code","fc3bf120":"code","6028e4ed":"code","b25618cb":"code","8889b90f":"code","e4089e38":"code","f004f80e":"code","5e8fb027":"code","7dbfaaa2":"code","4a07815e":"code","f5e28711":"code","459ce1fa":"code","56435dd3":"markdown","e337ae4d":"markdown","e29f2d9a":"markdown","d823e456":"markdown","f8fa2bdf":"markdown","30d3e825":"markdown","0532b68b":"markdown","e6d230f1":"markdown","a9669013":"markdown","092d7409":"markdown","fda02edc":"markdown","91976b93":"markdown","fa36d9cf":"markdown","1da5ca70":"markdown","fe5e2337":"markdown"},"source":{"a4983aac":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7d2436c9":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1edd18a4":"# load datasets\ndf = pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv', encoding=\"ISO-8859-1\")\ndf","5fcc7880":"df.drop(['Unnamed: 2', 'Unnamed: 3','Unnamed: 4'], axis=1, inplace=True)\ndf","7470996a":"df.isnull().sum()","b6c7449b":"target_count = df.groupby('v1').v1.count()\ntarget_count","6c77657d":"percent_target = (target_count \/ len(df)) * 100\npercent_target","1787b8b4":"df.groupby('v1').v1.count().plot.bar(ylim=0)\nplt.show()","42c46930":"dic = {'ham':1 ,'spam':0}\ndf.v1 = df.v1.map(dic)\ndf","2e102872":"#create new column\ndf['processedtext'] = df['v2']\ndf","4243d11c":"import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nstemmer = PorterStemmer()\nwords = stopwords.words(\"english\")\n\ndf['processedtext'] = df['processedtext'].apply(lambda x: \" \".join([stemmer.stem(i) \nfor i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())","fc3bf120":"#make all words lower case\ndf['processedtext'] = df['processedtext'].str.lower()\n\n# remove special characters, numbers, punctuations\ndf['processedtext'] = df['processedtext'].str.replace(\"[^a-zA-Z#]\", \" \")\n\n#remove words less than 3 characters\ndf['processedtext'] = df['processedtext'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","6028e4ed":"spam_words = ' '.join([text for text in df['processedtext']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(spam_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","b25618cb":"#define X and y\ny = df['v1']\nX = df['processedtext']","8889b90f":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\ndf_tfIdf = vectorizer_tfidf.fit_transform(X.values.astype('U'))\nprint(vectorizer_tfidf.get_feature_names()[:10])","e4089e38":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(df_tfIdf, y, test_size=0.10, random_state=1, shuffle=True)\nX_train.shape, X_val.shape, y_train.shape,y_val.shape","f004f80e":"from sklearn.linear_model import PassiveAggressiveClassifier\n\nmodel = PassiveAggressiveClassifier(max_iter=1000, random_state=1,tol=1e-3).fit(X_train, y_train)\nprint(model.score(X_train, y_train))","5e8fb027":"y_pred = model.predict(X_val)\nprint(model.score(X_val, y_val))","7dbfaaa2":"from sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_val,y_pred))","4a07815e":"df_val = pd.DataFrame({'Actual': y_val, 'Predicted':y_pred})\ndf_val","f5e28711":"from sklearn.decomposition import TruncatedSVD\n\nsvd_val = TruncatedSVD(n_components=2, random_state=1)\nprincipalComponents_val = svd_val.fit_transform(X_val)","459ce1fa":"plt.figure(figsize = (12, 8))\nplt.scatter(principalComponents_val[:, 0], principalComponents_val[:,1], c = y_pred == y_val - 1, alpha = .8, s = 50)","56435dd3":"Load file","e337ae4d":"Drop columns","e29f2d9a":"Confusion Matrix","d823e456":"Problem statement\n\nDetect whether a text message is spam or ham.\n\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.","f8fa2bdf":"Predict on validation set","30d3e825":"Plot errors","0532b68b":"Define X and y variables","e6d230f1":"Map v1","a9669013":"Split X for training and validation","092d7409":"Preprocess raw text and get ready for machine learning","fda02edc":"Convert text to word frequency vectors","91976b93":"Select model","fa36d9cf":"Import libraries","1da5ca70":"Check for null values","fe5e2337":"Analyse target"}}