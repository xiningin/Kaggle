{"cell_type":{"aad9abe8":"code","07d51dca":"code","0cd5f440":"code","83e4197e":"code","53b14d7d":"code","8bc3d591":"code","03616de9":"code","d8255979":"code","e53972f5":"code","fe2b9be3":"code","a749d478":"code","60b06941":"code","2f42737c":"code","31ff5339":"markdown","2650c561":"markdown","f407dfa1":"markdown","7913e3e1":"markdown","e9ff4ba7":"markdown","a8aa1b7e":"markdown","589f9e17":"markdown","deb68dc9":"markdown"},"source":{"aad9abe8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","07d51dca":"\n%load_ext autoreload\n%autoreload 2\nimport numpy as np\nfrom tqdm import tqdm_notebook\nimport re","0cd5f440":"def tokenize(text):\n    # obtains tokens with a least 1 alphabet\n    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n    return pattern.findall(text.lower())\n\ndef mapping(tokens):\n    word_to_id = dict()\n    id_to_word = dict()\n\n    for i, token in enumerate(set(tokens)):\n        #print (i, token)\n        word_to_id[token] = i\n        id_to_word[i] = token\n\n    return word_to_id, id_to_word\n\ndef generate_training_data(tokens, word_to_id, window_size):\n    N = len(tokens)\n    X, Y = [], []\n\n    for i in range(N):\n        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n                   list(range(i + 1, min(N, i + window_size + 1)))\n        for j in nbr_inds:\n            X.append(word_to_id[tokens[i]])\n            #print(X)\n            Y.append(word_to_id[tokens[j]])\n            #print(Y)\n            \n    X = np.array(X)\n    X = np.expand_dims(X, axis=0)\n    Y = np.array(Y)\n    Y = np.expand_dims(Y, axis=0)\n            \n    return X, Y","83e4197e":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = pd.DataFrame([1,3,4])\n\n# create a link to download the dataframe\ncreate_download_link(df)\n","53b14d7d":"doc = \"After the deduction of the costs of investing, \" \\\n      \"beating the stock market is a loser's game.\"\ntokens = tokenize(doc)\nword_to_id, id_to_word = mapping(tokens)\nX, Y = generate_training_data(tokens, word_to_id, 3)\nvocab_size = len(id_to_word)\nm = Y.shape[1]\n# turn Y into one hot encoding\nY_one_hot = np.zeros((vocab_size, m))\nY_one_hot[Y.flatten(), np.arange(m)] = 1","8bc3d591":"def initialize_wrd_emb(vocab_size, emb_size):\n    \"\"\"\n    vocab_size: int. vocabulary size of your corpus or training data\n    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n    \"\"\"\n    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n    \n    assert(WRD_EMB.shape == (vocab_size, emb_size))\n    return WRD_EMB\n\ndef initialize_dense(input_size, output_size):\n    \"\"\"\n    input_size: int. size of the input to the dense layer\n    output_szie: int. size of the output out of the dense layer\n    \"\"\"\n    W = np.random.randn(output_size, input_size) * 0.01\n    \n    assert(W.shape == (output_size, input_size))\n    return W\n\ndef initialize_parameters(vocab_size, emb_size):\n    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n    W = initialize_dense(emb_size, vocab_size)\n    \n    parameters = {}\n    parameters['WRD_EMB'] = WRD_EMB\n    parameters['W'] = W\n    \n    return parameters","03616de9":"def ind_to_word_vecs(inds, parameters):\n    \"\"\"\n    inds: numpy array. shape: (1, m)\n    parameters: dict. weights to be trained\n    \"\"\"\n    m = inds.shape[1]\n    WRD_EMB = parameters['WRD_EMB']\n    word_vec = WRD_EMB[inds.flatten(), :].T\n    \n    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n    \n    return word_vec\n\ndef linear_dense(word_vec, parameters):\n    \"\"\"\n    word_vec: numpy array. shape: (emb_size, m)\n    parameters: dict. weights to be trained\n    \"\"\"\n    m = word_vec.shape[1]\n    W = parameters['W']\n    Z = np.dot(W, word_vec)\n    \n    assert(Z.shape == (W.shape[0], m))\n    \n    return W, Z\n\ndef softmax(Z):\n    \"\"\"\n    Z: output out of the dense layer. shape: (vocab_size, m)\n    \"\"\"\n    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n    \n    assert(softmax_out.shape == Z.shape)\n\n    return softmax_out\n\ndef forward_propagation(inds, parameters):\n    word_vec = ind_to_word_vecs(inds, parameters)\n    W, Z = linear_dense(word_vec, parameters)\n    softmax_out = softmax(Z)\n    \n    caches = {}\n    caches['inds'] = inds\n    caches['word_vec'] = word_vec\n    caches['W'] = W\n    caches['Z'] = Z\n    \n    return softmax_out, caches\n","d8255979":"def cross_entropy(softmax_out, Y):\n    \"\"\"\n    softmax_out: output out of softmax. shape: (vocab_size, m)\n    \"\"\"\n    m = softmax_out.shape[1]\n    cost = -(1 \/ m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n    return cost","e53972f5":"def softmax_backward(Y, softmax_out):\n    \"\"\"\n    Y: labels of training data. shape: (vocab_size, m)\n    softmax_out: output out of softmax. shape: (vocab_size, m)\n    \"\"\"\n    dL_dZ = softmax_out - Y\n    \n    assert(dL_dZ.shape == softmax_out.shape)\n    return dL_dZ\n\ndef dense_backward(dL_dZ, caches):\n    \"\"\"\n    dL_dZ: shape: (vocab_size, m)\n    caches: dict. results from each steps of forward propagation\n    \"\"\"\n    W = caches['W']\n    word_vec = caches['word_vec']\n    m = word_vec.shape[1]\n    \n    dL_dW = (1 \/ m) * np.dot(dL_dZ, word_vec.T)\n    dL_dword_vec = np.dot(W.T, dL_dZ)\n\n    assert(W.shape == dL_dW.shape)\n    assert(word_vec.shape == dL_dword_vec.shape)\n    \n    return dL_dW, dL_dword_vec\n\ndef backward_propagation(Y, softmax_out, caches):\n    dL_dZ = softmax_backward(Y, softmax_out)\n    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n    \n    gradients = dict()\n    gradients['dL_dZ'] = dL_dZ\n    gradients['dL_dW'] = dL_dW\n    gradients['dL_dword_vec'] = dL_dword_vec\n    \n    return gradients\n\ndef update_parameters(parameters, caches, gradients, learning_rate):\n    vocab_size, emb_size = parameters['WRD_EMB'].shape\n    inds = caches['inds']\n    dL_dword_vec = gradients['dL_dword_vec']\n    m = inds.shape[-1]\n    \n    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n\n    parameters['W'] -= learning_rate * gradients['dL_dW']","fe2b9be3":"from datetime import datetime\n\nimport matplotlib.pyplot as plt\n\n\ndef skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, \n                            parameters=None, print_cost=False, plot_cost=True):\n    costs = []\n    m = X.shape[1]\n    \n    if parameters is None:\n        parameters = initialize_parameters(vocab_size, emb_size)\n    \n    begin_time = datetime.now()\n    for epoch in range(epochs):\n        epoch_cost = 0\n        batch_inds = list(range(0, m, batch_size))\n#         print(batch_inds)\n        np.random.shuffle(batch_inds)\n        for i in batch_inds:\n            X_batch = X[:, i:i+batch_size]\n            Y_batch = Y[:, i:i+batch_size]\n\n            softmax_out, caches = forward_propagation(X_batch, parameters)\n            gradients = backward_propagation(Y_batch, softmax_out, caches)\n            update_parameters(parameters, caches, gradients, learning_rate)\n            cost = cross_entropy(softmax_out, Y_batch)\n            epoch_cost += np.squeeze(cost)\n            \n        costs.append(epoch_cost)\n        if print_cost and epoch % (epochs \/\/ 500) == 0:\n            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n        if epoch % (epochs \/\/ 100) == 0:\n            learning_rate *= 0.98\n    end_time = datetime.now()\n    print('training time: {}'.format(end_time - begin_time))\n            \n    if plot_cost:\n        plt.plot(np.arange(epochs), costs)\n        plt.xlabel('# of epochs')\n        plt.ylabel('cost')\n    return parameters","a749d478":"paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, \\\n                                batch_size=128, parameters=None, print_cost=False)","60b06941":"X_test = np.arange(vocab_size)\nX_test = np.expand_dims(X_test, axis=0)\nsoftmax_test, _ = forward_propagation(X_test, paras)\ntop_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]","2f42737c":"for input_ind in range(vocab_size):\n    input_word = id_to_word[input_ind]\n    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n    print(\"{}'s neighbor words: {}\".format(input_word, output_words))","31ff5339":"## Forward Propagation","2650c561":"## Cost Function","f407dfa1":"https:\/\/towardsdatascience.com\/word2vec-from-scratch-with-numpy-8786ddd49e72","7913e3e1":"## Data Prep","e9ff4ba7":"## Backward Propogation","a8aa1b7e":"## Initialization","589f9e17":"## Code to download data from Kernal","deb68dc9":"## Evaluate"}}