{"cell_type":{"c6b17e39":"code","046e2d41":"code","28506ef1":"code","235ca814":"code","a94e2c4c":"code","67c02d1d":"code","1ae2b2b9":"code","8b02658b":"markdown","f70393fc":"markdown","00f5de0b":"markdown","97b16ce0":"markdown","689b72c1":"markdown","3dff1fb0":"markdown"},"source":{"c6b17e39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport sklearn.ensemble as se\nimport xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\nimport sklearn.model_selection as sm\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","046e2d41":"def assess(y_true,y_pred):\n    score = np.sqrt(np.mean(np.power(np.log(y_pred)-np.log(y_true),2)))\n    return score\ndef assess_no_avg(y_true,y_pred):\n    score = np.sqrt(np.power(np.log(y_pred)-np.log(y_true),2))\n    return score","28506ef1":"train_set = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_set = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntotal_set = pd.concat([train_set,test_set])\nprint(train_set.shape,test_set.shape,total_set.shape)\nprint(train_set.columns)\nprint(train_set.dtypes)\nprint(train_set['SaleCondition'])","235ca814":"plt.hist(train_set['SalePrice'],bins=50)\nplt.show()","a94e2c4c":"X_train,X_valid,y_train,y_valid = sm.train_test_split(train_set.iloc[:,:-1],train_set.iloc[:,-1],test_size=0.3,random_state=12356)\nprint(X_train.shape,X_valid.shape,y_train.shape,y_valid.shape)\nX_train.head()","67c02d1d":"numerical_features = X_train.select_dtypes(include='number').columns.tolist()\ncategorical_features = X_train.select_dtypes(exclude='number').columns.tolist()\n\nnumeric_pipeline = Pipeline(steps=[('impute',SimpleImputer(strategy='mean')),('scale',MinMaxScaler())])\ncategorical_pipeline = Pipeline(steps=[('impute',SimpleImputer(strategy='most_frequent')),('one-hot',OneHotEncoder(handle_unknown='ignore',sparse=False))])\nnumeric_pipeline.fit_transform(X_train.select_dtypes(include='number'))\ncategorical_pipeline.fit_transform(X_train.select_dtypes(exclude='number'))\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, numerical_features),\n    ('category', categorical_pipeline, categorical_features)\n])\nregr_GB = se.GradientBoostingRegressor(n_estimators=1000)\nGBT_pipeline = Pipeline(steps=[('preprocess',full_processor),('model',regr_GB)])","1ae2b2b9":"GBT_pipeline.fit(X_train,y_train)\nprint(assess(y_valid,GBT_pipeline.predict(X_valid)))\nprint(mean_absolute_error(y_valid,GBT_pipeline.predict(X_valid)))\n\npredictions = GBT_pipeline.predict(test_set)\ny_test_dict = {'Id' : test_set['Id'],'SalePrice' : predictions}\ny_test_df = pd.DataFrame(y_test_dict)\nprint(y_test_df)\ny_test_df.to_csv('\/kaggle\/working\/house-prices-advanced-regression-techniques.csv',columns=['Id','SalePrice'],index=False)","8b02658b":"# Define assessment RMSE function","f70393fc":"# Run pipeline for fit and score","00f5de0b":"# split training set into training and sample eval","97b16ce0":"# Set up pipelines\nIncluding imputation strategies.  I impute numeric values using the mean of the category and scale them such that they are zero mean.  I impute categorical values with the most frequent in the category and use one-hot encoding for categorical data.","689b72c1":"# Read in","3dff1fb0":"# histogram of final sale prices"}}