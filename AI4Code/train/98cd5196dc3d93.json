{"cell_type":{"0ed1f6b5":"code","3960d72a":"code","988f4a70":"code","00cf3f83":"code","a7b1e44e":"code","98f0848e":"code","38584a0d":"code","db0c9e1f":"code","c9c6685a":"code","1d280e12":"code","318c437b":"code","1c7868b1":"code","193bc825":"code","f148eb22":"code","d8e6ad56":"code","e876726a":"code","0b5efe9d":"code","d4dea0d0":"code","c3846717":"code","e14e15c8":"code","fc33f9d0":"code","eac0148e":"code","08ccb907":"code","8e3a1a05":"code","231d17b6":"code","bba890f3":"code","86f1898d":"code","517e3d76":"code","a38d23d1":"code","5e4e2bec":"code","2c3e9c6f":"code","f7dd86ac":"code","581f1fc7":"code","07d4e8fd":"code","390a0b4e":"code","7c3d2eca":"code","0cce951f":"code","96df6b21":"code","233416f0":"code","2d26ed7d":"code","c42639e1":"code","1bcfe4a8":"code","cf6e0c2b":"code","09277779":"code","b5c0acfc":"code","a8c947a0":"code","ea8cabbf":"code","669de745":"code","b74410b7":"code","fd1dfac6":"code","03264ed9":"code","b8a196fe":"code","19b2e638":"code","ea05305d":"code","b922f547":"code","005b2674":"code","aa40ba92":"code","db1362f3":"code","e62861f8":"code","4c3d5921":"code","12b3cd7c":"code","cd1b0dd8":"code","b5b79a57":"code","da279ef2":"code","0757cc85":"code","7a6a4d39":"code","258a0d17":"code","075014ae":"code","4ab07194":"code","3d1182a9":"code","fc693ec0":"markdown","617c8511":"markdown","fc39fc00":"markdown","05c78e8f":"markdown","f63e3f35":"markdown","ff23ffab":"markdown","20b1e3e6":"markdown","e0becf4b":"markdown","48580be2":"markdown","a124f7d8":"markdown","59f24ad5":"markdown","72adfc93":"markdown","11eaba5c":"markdown","70997dda":"markdown","ecf9a5a5":"markdown","bf4e1838":"markdown","1af8b68b":"markdown","6d2d8dd6":"markdown","21964447":"markdown","8035f604":"markdown","b92bb6a0":"markdown","ca25cd2b":"markdown","359b6a13":"markdown","f5be5d50":"markdown","a1aeae24":"markdown","ef54f910":"markdown","95019ab2":"markdown"},"source":{"0ed1f6b5":"# import data from kaggle\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3960d72a":"import numpy as np # Linear Algebra, Matrix operations .....etc\nimport pandas as pd # Data manipulation.\nimport seaborn as sns # Data Visualization.\nimport matplotlib.pyplot as plt # plots\n%matplotlib inline \n#plot in jupyter notebook don't open new window for plotting.\n\n# Future warnings ignore\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Machine Learning API's\nfrom sklearn.model_selection import cross_val_score # cross validation.\nfrom sklearn.linear_model import LinearRegression,Lasso \nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor","988f4a70":"train_df=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\") # train data\ntest_df=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\") # test data.\ntrain_df.head() # view top rows default 5 rows get displayed.","00cf3f83":"test_df.head() # view test data.","a7b1e44e":"target=train_df[\"SalePrice\"].copy()\ntrain_df.drop(\"SalePrice\",axis=1,inplace=True) # Drop target from train dataset.","98f0848e":"df=pd.concat([train_df,test_df])","38584a0d":"# check shape of combined data.\ndf.shape","db0c9e1f":"# Droping Id column\ndel df[\"Id\"]","c9c6685a":"from IPython.display import Image\nImage(\"\/kaggle\/input\/missing-values-mechanism\/Missingtheory.png\")","1d280e12":"null_cols=df.columns[df.isna().any()] # Get cols with missing values","318c437b":"null_per=round(df[null_cols].isna().agg(\"mean\").sort_values(ascending=False),5) # Get percentage of missing values of each column.","1c7868b1":"null=pd.DataFrame({\"Features\":null_per.index,\"percentage\":null_per.values}) # create data frame  for barplot.","193bc825":"plt.figure(figsize=(15,5)) # set width and height for plot\nsns.barplot(x=null.columns[0],y=null.columns[1],data=null)\nplt.xticks(rotation = 90) # Prevent labels from overlapping.\nplt.show()","f148eb22":"# Drop features with 50% values missing in it.\ndf.dropna(thresh=int(0.5*len(df)),inplace=True,axis=1)","d8e6ad56":"df.shape # shape after dropping cols.","e876726a":"rem_null=null[4:]# Get remaining missing values which are less than 50%.\nplt.figure(figsize=(15,5)) # set width and height for plot\nsns.barplot(x=rem_null.columns[0],y=rem_null.columns[1],data=rem_null)\nplt.xticks(rotation = 90) # Prevent labels from overlapping.\nplt.show()","0b5efe9d":"# Get numeric features and store into new variable\nnumeric_features=df.select_dtypes(include=np.number)","d4dea0d0":"numeric_features.dtypes.value_counts() # check dtypes.","c3846717":"# Imputation using KNN it finds similar values to impute missing ones.\nfrom fancyimpute import KNN\nimpute=KNN(k=3).fit_transform(numeric_features) # imputing","e14e15c8":"# Creating dataframe because knn imputation returns series of numpy arrays.\nimputed=pd.DataFrame(impute,columns=numeric_features.columns)\nimputed.head()","fc33f9d0":"cast_int=imputed.dtypes[numeric_features.dtypes != imputed.dtypes] # Get dtypes to change into int format.\nimputed[cast_int.index]=imputed[cast_int.index].applymap(int) # casting into original dtypes.","eac0148e":"imputed.head()","08ccb907":"imputed.dtypes.value_counts() # count of dtypes","8e3a1a05":"# check you imputated to null values\nimputed.isna().any().sum()","231d17b6":"# Statistical information of numeric data.\nimputed.describe()","bba890f3":"# Check for missingness count\ncate_features=df.select_dtypes(exclude=np.number)\ncate_cols=cate_features.columns[cate_features.isna().any()]\ncate_per=round(df[cate_cols].isna().agg(\"mean\").sort_values(ascending=False),5) # Get percentage of missing values of each column.\ncate_per","86f1898d":"cate_features[cate_cols].isna().sum()","517e3d76":"cate_features[\"FireplaceQu\"]=cate_features[\"FireplaceQu\"].fillna(\"Missing\")\ncols=cate_features[cate_cols].isna().sum() <100\nimpute_cols=cols.index\nfor i in impute_cols:\n    cate_features[i].fillna(cate_features[i].mode()[0],inplace=True)","a38d23d1":"cate_features[cate_cols].isna().sum()","5e4e2bec":"cate_features.columns.value_counts().sum()","2c3e9c6f":"# check distribution of target variable.\nplt.figure(figsize=(8,5))\nsns.distplot(target)","f7dd86ac":"target.skew() # skewness","581f1fc7":"# converting right skewed to normal distribution (or) close to normal.\nplt.figure(figsize=(8,5))\nsns.distplot(np.log(target))\nnp.log(target).skew()","07d4e8fd":"recombined=pd.concat([numeric_features,cate_features],axis=1)","390a0b4e":"recombined.head()","7c3d2eca":"train=recombined[:train_df.shape[0]]\ntest=recombined[train_df.shape[0]:]\ntrain.shape,test.shape","0cce951f":"trainn=pd.concat([train,target],axis=1)","96df6b21":"plt.figure(figsize=(25,15))\ncorr=trainn.corr(method=\"spearman\")\nsns.heatmap(corr,annot=True)","233416f0":"print (corr['SalePrice'].sort_values(ascending=False)[:20]) #top 15 values\nprint ('----------------------')\nprint (corr['SalePrice'].sort_values(ascending=False)[-5:]) #last 5 values`","2d26ed7d":"# 1st correlated variable.\ntrainn['OverallQual'].unique()","c42639e1":"#let's check the mean price per quality and plot it.\npivot = trainn.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.median)\nsns.barplot(x=\"OverallQual\",y=\"SalePrice\",data=trainn)","1bcfe4a8":"pivot = trainn.pivot_table(index='GrLivArea', values='SalePrice', aggfunc=np.median)\nsns.jointplot(x=\"GrLivArea\",y=\"SalePrice\",data=trainn)","cf6e0c2b":"cate_data=recombined.select_dtypes(include=\"object\")\ncate_data.describe()","09277779":"# we need to check the relation between categorical values using ANOVA .\nfrom scipy import stats\ncat = [f for f in trainn.columns if trainn.dtypes[f] == 'object']\ndef anova(frame):\n    anv = pd.DataFrame()\n    anv['features'] = cat\n    pvals = []\n    for c in cat:\n           samples = []\n           for cls in frame[c].unique():\n                  s = frame[frame[c] == cls]['SalePrice'].values\n                  samples.append(s)\n           pval = stats.f_oneway(*samples)[1]\n           pvals.append(pval)\n    anv['pval'] = pvals\n    return anv.sort_values('pval')\n\ncate_data['SalePrice'] = trainn.SalePrice\nk = anova(cate_data) \nk['disparity'] = np.log(1.\/k['pval'].values) \nplt.figure(figsize=(15,8))\nsns.barplot(data=k, x = 'features', y='disparity') \nplt.xticks(rotation=90) \nplt.show()","b5c0acfc":"# View all numeric features distribution.\ntrainn.hist(figsize=(20,18))\nplt.show()","a8c947a0":"# Also visualize the categorical variables using boxplot\ndef boxplot(x,y,**kwargs):\n            sns.boxplot(x=x,y=y)\n            x = plt.xticks(rotation=90)\n\ncat = [f for f in train.columns if trainn.dtypes[f] == 'object']\n\np = pd.melt(trainn, id_vars='SalePrice', value_vars=cat)\ng = sns.FacetGrid (p, col='variable', col_wrap=4, sharex=False, sharey=False, height=4)\ng = g.map(boxplot, 'value','SalePrice')\ng","ea8cabbf":"# we need to convert categorical variables in to numeric. unless it gives you any error all models works with numerical mathematical formuales.\n# combine train and test data ,split target into new variable\ntarget=trainn[\"SalePrice\"].copy()\ndel trainn[\"SalePrice\"]\nfina_df=pd.concat([trainn,test])","669de745":"# Look out features behavior and perform feature engineering.\n# Select features which contribute to target variable and also create new .\n\nfina_df.select_dtypes(include=\"object\").nunique() # unique values in each categorical variable.","b74410b7":"# perfrom label encoding on ordinal data or mapping function.\n# features which are have quality are ordinal.\nfeatures_ord=[\"GarageQual\",\"KitchenQual\",\"FireplaceQu\",\"BsmtQual\",\"ExterQual\"]\nfina_df[features_ord].nunique()","fd1dfac6":"# multiple column label encoder class\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)","03264ed9":"fina_df=MultiColumnLabelEncoder(columns = features_ord).fit_transform(fina_df)","b8a196fe":"fina_df.head()","19b2e638":"# converting other categorical values to numeric using OHE , pandas get_dummies.\nfina_df=pd.get_dummies(fina_df)","ea05305d":"# splitting train and test data.\ntrain_split=fina_df[:train_df.shape[0]]\ntest_split=fina_df[train_df.shape[0]:]","b922f547":"# log transformation target before training\ny=np.log(target)","005b2674":"train_split.head()","aa40ba92":"# some cols still have null values that are hidden by imputation methods.\nremaining=train_split.columns[train_split.isna().any()]\nremaining","db1362f3":"for i in remaining:\n    train_split[i].fillna(0,inplace=True) # Filling with zeros","e62861f8":"remaining=test_split.columns[test_split.isna().any()]\nfor i in remaining:\n    test_split[i].fillna(0,inplace=True) # filling with zeros","4c3d5921":"# final check for null values in both train and test data.\nprint(train_split.columns[train_split.isna().any()])\nprint(test_split.columns[test_split.isna().any()])","12b3cd7c":"from sklearn.preprocessing import StandardScaler\nstand=StandardScaler()\nvalues=stand.fit_transform(train_split)\nX_train=pd.DataFrame(values,columns=train_split.columns)","cd1b0dd8":"from sklearn.preprocessing import StandardScaler\nstand=StandardScaler()\nvalues=stand.fit_transform(test_split)\nX_test=pd.DataFrame(values,columns=test_split.columns)","b5b79a57":"import xgboost as xgb\n# below parameters are valued by cross-validation.\nregr = xgb.XGBRegressor(colsample_bytree=0.2,\n                       gamma=0.0,\n                       learning_rate=0.05,\n                       max_depth=6,\n                       min_child_weight=1.5,\n                       n_estimators=7200,\n                       reg_alpha=0.9,\n                       reg_lambda=0.6,\n                       subsample=0.2,\n                       seed=42,\n                       silent=1)\n\nregr.fit(X_train, y) # train data, log transformed target.","da279ef2":"from sklearn.metrics import mean_squared_error\n# root mean square error function\ndef rmse(y_test,y_pred):\n      return np.sqrt(mean_squared_error(y_test,y_pred)) \n\n# Run prediction on training set to get an idea of how well it does\ny_pred = regr.predict(X_train)\ny_test = y\nprint(\"XGBoost score on training set: \", rmse(y_test, y_pred))","0757cc85":"# make prediction on the test set\ny_pred_xg = regr.predict(X_test)\nxg_ex = np.exp(y_pred_xg)\npred1 = pd.DataFrame({'Id': test_df['Id'], 'SalePrice': xg_ex})\npred1.to_csv('xgboost.csv', header=True, index=False)","7a6a4d39":"# Lasso\nfrom sklearn.linear_model import Lasso\n\n# found this best alpha through cross-validation\nbest_alpha = 0.00099\n\nlasso = Lasso(alpha=best_alpha, max_iter=50000)\nlasso.fit(X_train, y)\n\n# Metrics score on train data\ny_pred = lasso.predict(X_train)\ny_test = y\nprint(\"Lasso score on training set: \", rmse(y_test, y_pred))","258a0d17":"#make prediction on the test set\nlasso_pred = lasso.predict(X_test)\nlasso_ex = np.exp(lasso_pred)\npred1 = pd.DataFrame({'Id': test_df['Id'], 'SalePrice': lasso_ex})\npred1.to_csv('lasso.csv', header=True, index=False)","075014ae":"# Gradient boosting regressor model training\nfrom sklearn.ensemble import GradientBoostingRegressor\nest = GradientBoostingRegressor(n_estimators= 1000, max_depth= 2, learning_rate= .01)\nest.fit(X_train, y)","4ab07194":"# metrics score on train data.\ny_pred = est.predict(X_train)\ny_test = y\nprint(\"Gradient score on training set: \", rmse(y_test, y_pred))","3d1182a9":"# submission to kaggle\nGBC_pred = est.predict(X_test) # prediction\nGBC_ex = np.exp(GBC_pred) # converting back to original values\npred1 = pd.DataFrame({'Id': test_df['Id'], 'SalePrice': GBC_ex})\npred1.to_csv('GBC.csv', header=True, index=False)","fc693ec0":"# 6. Data exploration","617c8511":"# 9. Model building.\n- Using 3 models Xgboost, Lasso, Gradient Boosting.","fc39fc00":"- Here we see that among all categorical variables Neighborhood turned out to be the most important feature followed by ExterQual, KitchenQual, etc.  ","05c78e8f":"# Problem Statement\n\n- This project aims at predicting house prices (residential) in Ames, Iowa, USA using advanced regression techniques.\n- Train data consist of 1460 rows and 80 cols including target variable.\n- Test data consist of 1459 rows and 79 cols.\n- This dataset have lot of features to deal with. So, Feature Engineering comes to play.","f63e3f35":"# 4. Combine train and test data.","ff23ffab":"# Visualize the relation between correlated variables with target.","20b1e3e6":"- PoolQC Followed by MiscFeature and Alley, Fence features having highest percentage of missing values above 50%. ","e0becf4b":"- It is right skewed or positive skewed. \n- Check skewness of target feature.","48580be2":"# Imputing categorical columns.","a124f7d8":"### Gradient Boosting Regressor","59f24ad5":"> - **NOTE**: It returns series of numpy arrays and convert whole data dtypes into float you need to convert back original dtypes.","72adfc93":"- combined data has 2919 rows with 80 features","11eaba5c":"# 8. Feature engineering","70997dda":"# 5. Check for missing values.\n- Missing values treatment.","ecf9a5a5":"# 2. Load data into pandas dataframe.","bf4e1838":"# Analysing categorical variables.","1af8b68b":"# 7. Corelation between numeric variables","6d2d8dd6":"> Converting target from right skewed to normal will helps model to map relationship between independent variables to target variable. ","21964447":"# 1. Importing Libraries.","8035f604":"- It consist of outliers, outliers spoil the model .we need to analyse it and then decide remove or leave.\n- we are using advanced regression models randomforest,xgboost they handle outliers.\n- Likewise, analyse other variables which are correlated with target.","b92bb6a0":"- Overall quality increases saleprice also increases","ca25cd2b":"# Imputing numeric features.","359b6a13":"- Check if target variable distribution is matching with independent variables.","f5be5d50":"# 3. Copy target variable to new variable.","a1aeae24":"- we need to convert this values into numeric by label encoding and one-hot encoding.\n- Also you should know the difference between label encoding and OHE\n- label assumes assumes they are ordinal values. it should be appiled to only ordinal data.\n- OHE extends the dimensionality of data should be used wisely.","ef54f910":"### Lasso Regression.","95019ab2":"# we need to impute this missing values \n- You need to have good understanding of each variable.\n- Three types of missing data mechanisms - MCAR, MAR, MNAR.\n   1. Missing completely at random (MCAR) - Missingness is not related to any variables. Missingness on variable is completely unsystematic.\n   2. Missing at random (MAR) - Missingness is related to other variables not itself. it can be imputated with other variables which relates to missing                                 data.\n   3. Missing not at random (MNAR) - Also know as \"non-ignorable\" because should not be ignored. this shows up when neither MCAR or MAR and it's related                                      to both observed(Missing) and unobserved(other features) data.\n- MCAR and MAR is found by statistical testing.\n- MNAR is found by understanding data."}}