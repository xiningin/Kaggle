{"cell_type":{"d6b13ac4":"code","e3059004":"code","28e05a7b":"code","b091b901":"code","ecc690d2":"code","0c3b670c":"code","aa5b8ce4":"code","7e174717":"code","93dac3a7":"code","0c9e8221":"code","4b9eef2f":"markdown","5fbcf61a":"markdown","8d950e43":"markdown","35437630":"markdown","b869b9cb":"markdown","75e0a9bb":"markdown","a2aa995a":"markdown","de7060aa":"markdown","f545a8d4":"markdown"},"source":{"d6b13ac4":"%pip install -U -q scikit-learn","e3059004":"import random\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn import (\n    linear_model,\n    neural_network,\n    model_selection,\n    pipeline,\n    preprocessing,\n    svm,\n    ensemble,\n    metrics\n)\n\nrandom.seed(42)\nnp.random.seed(42)\nfolds = model_selection.StratifiedKFold(5, shuffle=True, random_state=42)\n\ndf = pd.read_csv(\n    '..\/input\/tabular-playground-series-nov-2021\/train.csv', dtype=np.float32\n).astype({'id': np.int32})","28e05a7b":"clf = pipeline.Pipeline([\n    ('scaler', preprocessing.StandardScaler()), # name of the step to the left\n    ('clf', linear_model.LogisticRegression()), # name of the step to the left\n])\n\nX = df.drop(columns=['id', 'target'])\ny = df.target\n\nmodel_selection.cross_val_score(clf, X, y, cv=folds)","b091b901":"grid = [ # A grid can be a single dict, or a list of dicts\n    dict(\n        # All combinations of options in this dict will be tried together\n        # 3 here -- 1 scaler times 3 different models\n        scaler=[preprocessing.StandardScaler()], # name of the pipeline step!\n        clf=[ # name of the pipeline step!\n            # These classifiers prefer centered input, so they work well with StandardScaler\n            svm.LinearSVC(), \n            linear_model.LogisticRegression(),\n            neural_network.MLPClassifier(early_stopping=True, hidden_layer_sizes=(32, 8)),\n        ]\n    ),\n    dict( \n        # You can pass as many dicts as you'd like, \n        # let's try using different scalers with SGDClassifier\n        scaler=[preprocessing.StandardScaler(), preprocessing.MinMaxScaler()],\n        clf=[linear_model.SGDClassifier()],\n        clf__loss=['hinge', 'log'], # run SGD twice, once with hinge-loss and once with logloss\n    ),\n    dict(\n        scaler=[None], # These two don't care about scaling, so we can run it without\n        clf=[ensemble.RandomForestClassifier(), ensemble.HistGradientBoostingClassifier()]\n    )\n]\ngrid","ecc690d2":"gridsearch = model_selection.GridSearchCV(\n    clf, # the pipeline\n    grid, # the grid,\n    cv=folds, # the folds\n    scoring='roc_auc', # optionally the scoring\n    verbose=True, # If you want more output\n)","0c3b670c":"%time gridsearch.fit(X, y)","aa5b8ce4":"pd.DataFrame(gridsearch.cv_results_)","7e174717":"chosen = gridsearch.best_estimator_\nchosen, chosen.get_params()","93dac3a7":"%time chosen.fit(X, y)","0c9e8221":"df_test = pd.read_csv(\n    '..\/input\/tabular-playground-series-nov-2021\/test.csv', dtype=np.float32\n).astype({'id': np.int32})\nX_test = df_test.drop(columns=['id'])\ndf_test[['id']].assign(\n    target=chosen.predict_proba(X_test)[:, 1]\n).to_csv('submission.csv', index=False)","4b9eef2f":"And you can easily retrieve the best estimator it found:","5fbcf61a":"Then, do lots of imports, set the most important seeds (for reproducible results), and read in the data. I also make sure to always use the same cross validation folds:","8d950e43":"Once fitted, `GridSearchCV` has a convenient `cv_results_` object that can be turned into a `pd.DataFrame` and insepected:","35437630":"Grid search\n==\n\nIt's the first half hour that you're looking a new problem. What's the first thing you should do?\n\nWell, that's exploratory data analysis, or EDA for short. To me, that involves doing a quick and dirty search for models that will work **while** I'm doing analysis. I'll do a quick check to look for missing data, if needed, I might impute or fill some of that. Then I'll start -- with the goal of getting a promising lead for a model I should pursue later.\n\nAt this point, I barely know the data at all. But it's better to start this job now, when I'm not waiting for it to finish, than later, when I have time to work, but don't know where to take it. So I think this is often a good idea for one of the first steps. Grid searches are not only for hyper parameter tuning, they're actually not bad for finding out where you should start.\n\nFirst, I'll install a more recent version of scikit-learn -- namely, I want one that has `HistGradientBoostingClassifier`. Just because that makes it super fast to set up an experiment that can check the classifiers that I'll most commonly want to try. So, here's that:","b869b9cb":"`LogisticRegression` with `StandardScaler` is very often a good first choice for tabular problems that are already numeric. It's fast to evaluate and can often get you decent models. With categorical features involved, I might try some kind of tree first, since it's fast to get started.\n\nSetting up a GridSearchCV\n==\n\nOnce you've a pipeline that works, it's super easy to have `sklearn.model_selection.GridSearchCV` try replacing parts of the pipeline for other parts, to find the combination of parts that work best.\n\nThis is fully automatic, and fully exhaustive. Eg. if your grid contains 3 options, with 4 choices each, that's 4 x 4 x 4 combinations that it'll search. So this could take a while. Which is a good reason to start the job early on, while you have other things to do than wait for it!\n\nUsually you should try a few different preprocessing steps. I tried to keep the clutter out of this demo, though.","75e0a9bb":"And that's just a normal estimator, so we use it by calling `.fit()`. This will take a while, depending on how many models you asked it to check:","a2aa995a":"And that's how I ended up spending more energy on neural networks (MLPClassifier is a simple NN), than on gradient boosters this time around.","de7060aa":"You can use the same mechanism to pass different kinds of options to the different classifiers -- for SGD, we're trying two different loss functions here. Now we can set up the GridSearchCV estimator:","f545a8d4":"The next thing I do, if I know the data has no glaring, huge, problems, is that I set up a `sklearn.pipeline.Pipeline` and verify that it works by running it in a `cross_val_score`"}}