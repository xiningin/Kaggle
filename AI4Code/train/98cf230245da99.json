{"cell_type":{"af505698":"code","cc057687":"code","db6f606e":"code","57e645f4":"code","3ee8941b":"code","9f3f8460":"code","4ced32e5":"code","6d90d1d6":"code","262e7796":"code","fc661268":"code","3904cc57":"code","67378a68":"code","667b2dde":"code","ac0d0b4f":"code","22a11840":"code","2144f77e":"code","32521bb5":"code","7662efc1":"code","5e6085f4":"code","cf8a397d":"code","2369e7d7":"code","2be994e7":"code","e343e33e":"code","a60d90bc":"code","4b77a0b8":"code","be143ab6":"code","627b5216":"code","c7677da6":"code","fb5dcefb":"code","f794a5c1":"code","0c15fd2a":"code","846b0a08":"code","de5a9ddd":"code","3ca0aff3":"code","bd358b93":"code","fb094006":"code","9bb4d06a":"code","601cf422":"code","ef7f6d35":"code","8152e97d":"code","8c103c6f":"code","a510cf7e":"code","739f6dfa":"code","33d6bdcb":"code","1167fec4":"code","5c6ebe9f":"code","5f98bc73":"code","e2e36b95":"code","4fb71265":"code","c40ccd69":"code","78059da8":"code","7656887f":"code","57f3aeae":"code","da49e3c0":"markdown","d908ef16":"markdown","f49b406d":"markdown","3a3aabf0":"markdown","8d97f56d":"markdown","48ba6d5c":"markdown"},"source":{"af505698":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, fbeta_score, confusion_matrix, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n#NAIVE_BAYES MODEL\nfrom sklearn.naive_bayes import GaussianNB\n\n#SVC \nfrom sklearn.svm import SVC\n\n#XGBOOST\nfrom xgboost import XGBClassifier\nimport pandas as pd\n\nfrom sklearn.metrics import classification_report","cc057687":"df1 = pd.read_csv(\"..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv\")\ndf1.info()","db6f606e":"df1","57e645f4":"df2 = pd.read_csv(\"..\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv\")\ndf2.info()","3ee8941b":"df2","9f3f8460":"df = df1.append(df2)\ndf.info()","4ced32e5":"df","6d90d1d6":"df.dtypes","262e7796":"df[\"Credit_History\"].value_counts()","fc661268":"df[\"Loan_Status\"].value_counts()","3904cc57":"df.columns = map(str.lower, df.columns)","67378a68":"df[\"credit_history\"]=df[\"credit_history\"].replace(np.nan,1.0)\ndf[\"credit_history\"]= df[\"credit_history\"].astype(str)","667b2dde":"df.isna().sum()","ac0d0b4f":"#MODE IMPUTATION FOR LOAN_STATUS \ndf[\"loan_status\"] = df[\"loan_status\"].replace(np.nan,\"Y\")\ndf[\"loan_status\"]= df[\"loan_status\"].replace(\"Y\",1).astype(str)\ndf[\"loan_status\"]= df[\"loan_status\"].replace(\"N\",0).astype(str)","22a11840":"#REMOVOING THE LOAN_ID AS IT IS NOT AN IMPORTANT VARIABLE\ndf.drop(\"loan_id\",axis=1,inplace=True)","2144f77e":"df.duplicated().any()\ndf.shape","32521bb5":"df.drop_duplicates(keep=False, inplace=True)\ndf.duplicated().any()","7662efc1":"df.shape","5e6085f4":"#user-defined function for knowing the number of cat and num varriables in a data-set\ndef cat_num(df):\n    total = 0\n    cat = 0\n    num = 0\n    for col in df.columns.values:\n        if df[col].dtype == \"object\":\n            cat = cat+1\n        else:\n            num=num+1\n    print(\"numerical:\",num)\n    print(\"categorical:\",cat)\ncat_num(df) \n\n#TOTALLY 4 NUMERICAL AND 8 CATEGORICAL VARIABLES ARE THERE ","cf8a397d":"#lets split the data-frame into Numerical and categorical variable\ncategory = [col for col in df.columns.values if df[col].dtype == 'object']\n\n# CATEGORICAL\ndata_cat = df[category]\n\n#Numerical variable\ndata_num = df.drop(category,axis =1)","2369e7d7":"data_num.isna().sum()","2be994e7":"#LOAN_AMOUNT\namt = data_num[\"loanamount\"]\namtfil = amt.fillna(amt.median())\ndata_num[\"loanamount\"] = amtfil\n\n#LOAN_AMOUNT_TERM\namt1 = data_num[\"loan_amount_term\"]\namt1fil = amt1.fillna(amt1.median())\ndata_num[\"loan_amount_term\"] = amt1fil","e343e33e":"data_num.isna().sum()","a60d90bc":"data_cat.info()","4b77a0b8":"data_cat.isna().sum()","be143ab6":"import warnings\nwarnings.filterwarnings(\"ignore\")\ndata_cat[\"gender\"].value_counts()\ndata_cat[\"gender\"]= data_cat[\"gender\"].replace(np.nan,\"Male\")\n\ndata_cat[\"self_employed\"]=data_cat[\"self_employed\"].replace(np.nan,\"No\")\n\ndata_cat[\"dependents\"]=data_cat[\"dependents\"].replace(np.nan,0)\n\ndata_cat[\"married\"]= data_cat[\"married\"].replace(np.nan,\"Yes\")","627b5216":"data_cat.isna().sum()","c7677da6":"#OUTLIERS CHECK FOR Numerical data\nfor i in data_num.columns:\n    print(i)\n    sns.set(style=\"whitegrid\")\n    sns.boxplot(data_num[i])\n    plt.show()\n    \n#Categorical data count plot percentage\nfor i in data_cat.columns:\n    print(i)\n    total = float(len(data_cat))\n    plt.figure(figsize=(8,10))\n    sns.set(style=\"whitegrid\")\n    ax = sns.countplot(data_cat[i])\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,height + 3,'{:1.2f}'.format(height\/total),ha=\"center\") \n    plt.show()","fb5dcefb":"df[\"gender\"].value_counts()","f794a5c1":"# let's look at the target percentage\n\nplt.figure(figsize=(8,6))\nsns.countplot(df['loan_status']);\n\nprint('The percentage of Y class : %.2f' % (df['loan_status'].value_counts()[1] \/ len(df)))\nprint('The percentage of N class : %.2f' % (df['loan_status'].value_counts()[0] \/ len(df)))\n\n# We can consider it as imbalanced data, but for now i will not","0c15fd2a":"#Credit_History\n\ngrid = sns.FacetGrid(df,col='loan_status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'credit_history');\n\n# we didn't give a loan for most people who got Credit History = 0\n# but we did give a loan for most of people who got Credit History = 1\n# so we can say if you got Credit History = 1 , you will have better chance to get a loan\n\n# important feature","846b0a08":"# Gender\n\ngrid = sns.FacetGrid(df,col='loan_status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'gender');\n\n# most males got loan and most females got one too so (No pattern)\n\n# i think it's not so important feature...","de5a9ddd":"# Married\nplt.figure(figsize=(15,5))\nsns.countplot(x='married', hue='loan_status', data=df);\n\n# most people who get married did get a loan\n# if you'r married then you have better chance to get a loan :)\n","3ca0aff3":"# Dependents\n\nplt.figure(figsize=(15,5))\nsns.countplot(x='dependents', hue='loan_status', data=df);\n\n# first if Dependents = 0 , we got higher chance to get a loan ((very hight chance))\n","bd358b93":"# Education\n\ngrid = sns.FacetGrid(df,col='loan_status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'education');\n\n# If you are graduated or not, you will get almost the same chance to get a loan (No pattern)\n# Here you can see that most people did graduated, and most of them got a loan\n# on the other hand, most of people who did't graduate also got a loan, but with less percentage from people who graduated\n","fb094006":"# Self_Employed\n\ngrid = sns.FacetGrid(df,col='loan_status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'self_employed')","9bb4d06a":"df.groupby('loan_status').median() # median because Not affected with outliers\n\n# we can see that when we got low median in CoapplicantInocme we got Loan_Status = N","601cf422":"corr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","ef7f6d35":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\ndata_cat[\"loan_status\"]=data_cat[\"loan_status\"].replace(\"1\",\"Y\")\ndata_cat[\"loan_status\"]=data_cat[\"loan_status\"].replace(\"0\",\"N\")\ndata_cat","8152e97d":"# transform the target column\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ntarget_values = {'Y': 1 , 'N' : 0}\n\ntarget = data_cat['loan_status']\ndata_cat.drop('loan_status', axis=1, inplace=True)\n\ntarget = target.map(target_values)","8c103c6f":"#Label Encoding\nfrom sklearn.preprocessing import LabelEncoder  \nle = LabelEncoder()","a510cf7e":"import warnings\nwarnings.filterwarnings(\"ignore\")\ndata_cat[\"gender\"]=le.fit_transform(data_cat[\"gender\"])\n\ndata_cat[\"married\"]=le.fit_transform(data_cat[\"married\"])\n\ndata_cat[\"education\"]=le.fit_transform(data_cat[\"education\"])\n\ndata_cat[\"self_employed\"]=le.fit_transform(data_cat[\"self_employed\"])\n\ndata_cat[\"property_area\"]=le.fit_transform(data_cat[\"property_area\"])","739f6dfa":"data_cat.drop(\"dependents\",axis=1,inplace=True)","33d6bdcb":"data_num.skew()","1167fec4":"for col in data_num.columns:\n    data_num[col] = (data_num[col]-data_num[col].min())\/(data_num[col].max() - data_num[col].min())\n    \ndata_num.head()","5c6ebe9f":"data= pd.concat([data_num, data_cat],axis=1)\ndata","5f98bc73":"X = data#independent variable\n\ny = target #dependant variable\n#train and test data split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 42)\n\nprint(X_train.shape, X_test.shape)\n#NOTE:\n#.values will store the values in the form of array\n#if you not give x will store the values in series","e2e36b95":"#Random-Forest Model\nmodel = RandomForestClassifier(n_estimators = 100, random_state = 42).fit(X_train,y_train)\ny_pred = model.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_pred,y_test))","4fb71265":"#StratifiedShuffleSplit to split the data \n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train, test in sss.split(X, y):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    y_train, y_test = y.iloc[train], y.iloc[test]\n    \nprint('X_train shape', X_train.shape)\nprint('y_train shape', y_train.shape)\nprint('X_test shape', X_test.shape)\nprint('y_test shape', y_test.shape)\n","c40ccd69":"#FOR-NOW JUST TAKING THE THREE VARIABLES\nmodels = {\n    'LogisticRegression': LogisticRegression(random_state=42),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'SVC': SVC(random_state=42),\n    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=1, random_state=42)\n}","78059da8":"from sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\n\ndef loss(y_true, y_pred, retu=False):\n    pre = precision_score(y_true, y_pred)\n    rec = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    loss = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    \n    if retu:\n        return pre, rec, f1, loss, acc\n    else:\n        print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f' % (pre, rec, f1, loss, acc))","7656887f":"# train_eval_train\n\ndef train_eval_train(models, X, y):\n    for name, model in models.items():\n        print(name,':')\n        model.fit(X, y)\n        loss(y, model.predict(X))\n        print('-'*30)\n        \ntrain_eval_train(models, X_train, y_train)\n\n# we can see that best model is LogisticRegression at least for now, SVC is just memorizing the data so it is overfitting .\n","57f3aeae":"#Stratified K-fold corss validation\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n\ndef train_eval_cross(models, X, y, folds):\n    \n    X = pd.DataFrame(X) \n    y = pd.DataFrame(y)\n    idx = [' pre', ' rec', ' f1', ' loss', ' acc']\n    for name, model in models.items():\n        ls = []\n        print(name,':')\n\n        for train, test in folds.split(X, y):\n            model.fit(X.iloc[train], y.iloc[train]) \n            y_pred = model.predict(X.iloc[test]) \n            ls.append(loss(y.iloc[test], y_pred, retu=True))\n        print(pd.DataFrame(np.array(ls).mean(axis=0), index=idx)[0])  \n        print('-'*30)\n        \ntrain_eval_cross(models, X_train, y_train, skf)","da49e3c0":"# MISSING VALUES IDENTIFICATION AND IMPUTATION","d908ef16":"# Numerical variables\n","f49b406d":"# ENCODING CONCEPTS","3a3aabf0":"# VISUALISATION PART","8d97f56d":"# NUMERICAL-VARIABLE","48ba6d5c":"# CATEGORICAL VARAIBLE"}}