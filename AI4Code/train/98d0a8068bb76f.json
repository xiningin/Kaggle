{"cell_type":{"e8e56848":"code","a1212292":"code","cb737171":"code","f28a7513":"code","86e3a646":"code","d04a5e1f":"code","39219ac7":"code","e8769c10":"code","e06715ae":"code","40094496":"code","84cf141e":"code","db59283d":"code","d8899e50":"code","2e7c5bfe":"code","1be3b6e7":"code","d2a8ac9e":"code","ff0e6153":"code","ea428f51":"code","4d940300":"code","6b23c5dd":"code","bf186df7":"code","af748797":"code","753013d2":"code","aa5db02d":"code","c3142ea2":"code","5cb05664":"code","dfc06fe8":"code","2ce907dd":"code","bbd52067":"code","9d0a229b":"code","3d068083":"code","17a28d89":"code","39d2fa93":"code","46de6b52":"code","74575ba9":"code","3f1e1b5c":"code","48c57d38":"code","63b01b7c":"code","6492a24b":"code","eb9d198a":"code","681e9103":"code","1b35bdb5":"code","abf9c9d3":"code","acecb307":"code","4524dc4e":"code","c9cbc80d":"code","b24e6e00":"markdown","a5740577":"markdown","d2e1fe4a":"markdown","fada9483":"markdown","39395901":"markdown","7ffd9c62":"markdown","e3823b05":"markdown","4c3c22ec":"markdown","bdb3455e":"markdown","8a946777":"markdown","e48cc385":"markdown","058b2279":"markdown","dfb095cc":"markdown","930986e1":"markdown","2fdb809e":"markdown","1e70dcce":"markdown","f19f5dd5":"markdown","d0aa2d03":"markdown","1eada674":"markdown","8e8a20b0":"markdown","32dde52b":"markdown","7a5462ea":"markdown","48bf82c9":"markdown","5de5e4af":"markdown","7463294e":"markdown","25c3174f":"markdown","e9bfd102":"markdown","ed036b36":"markdown","c8903f13":"markdown","1194b177":"markdown"},"source":{"e8e56848":"import pandas as pd\nimport numpy as np","a1212292":"import seaborn as sns\nfrom matplotlib import pyplot as plt","cb737171":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","f28a7513":"# Parameters to be used\ndrop_list = [] # list for parameters that will be dropped","86e3a646":"train_data.head()","d04a5e1f":"train_data.info()","39219ac7":"print(\"Train data\", train_data.shape)\nprint(\"Test data\", test_data.shape) ## Test data does not include target feature","e8769c10":"train_data[train_data['Age'].isnull()][:20]","e06715ae":"drop_list.append('Name')\ndrop_list.append('PassengerId') # PassengerId holds no valuable information","40094496":"train_data['Ticket'].unique()[:40]","84cf141e":"drop_list.append('Ticket')","db59283d":"# Verifying columns dropped\ntrain_data.columns","d8899e50":"import seaborn as sns\nfrom matplotlib import pyplot as plt","2e7c5bfe":"categorical = train_data.select_dtypes(include='object').copy()\ncategorical.columns","1be3b6e7":"# In order to facilitate and makes sure the same procedures are done on both\ndef transform_category_data_to_numerics(dataset):\n    dataset['CAT_has_cabin'] = dataset.Cabin.notnull().replace(False,0).replace(True,1)\n    dataset['CAT_is_male'] = dataset.Sex.replace('male',1).replace('female', 0)\n    dataset['CAT_embarkment_city'] = dataset.Embarked.replace('S', 0).replace('C', 1).replace('Q', 2)\n    return dataset\n    ","d2a8ac9e":"drop_list.append('Embarked')\ndrop_list.append('Cabin')\ndrop_list.append('Sex')","ff0e6153":"train_data.info()","ea428f51":"train_data.describe() # Remember: shows only numeric ","4d940300":"train_data[train_data.Fare == 0].describe()","6b23c5dd":"def handle_null_values(dataset, base_dataset):\n    dataset.Age = dataset.Age.fillna(base_dataset.Age.mean())\n    #dataset.Fare = dataset.FE_.fillna(0) # This is handled \n    dataset.CAT_embarkment_city = dataset.CAT_embarkment_city.fillna(0)\n    return dataset\n    #X = X.fillna(lambda x: x.mode())","bf186df7":"X_train_visualize = train_data\nX_train_visualize = transform_category_data_to_numerics(X_train_visualize)\nX_train_visualize = X_train_visualize.drop(drop_list, axis=1)\nfig = plt.figure(figsize=(16,16))\nuse_columns = X_train_visualize.columns #['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked'] #Excluding columsn that will be dropped later\ncols = 3\nrows = int((len(use_columns) \/ cols)+1)\nfor i, col in enumerate(use_columns):\n    plt.subplot(rows,cols,i+1)\n    sns.histplot(x=col, data=train_data)","af748797":"#Since we are only looking at correlation, we don't need to know if it is a neg\/pos correlation\nplt.figure(figsize=(12,8))\nsns.heatmap(X_train_visualize.corr().abs(), annot=True) ","753013d2":"#df1['A'] = df1['A'].apply(lambda x: [y if y <= 9 else 11 for y in x])\nX_train_visualize['FE_has_sibsp'] = X_train_visualize['SibSp'].apply(lambda x: 0 if x == 0 else 1)\nX_train_visualize['FE_has_parch'] = X_train_visualize['Parch'].apply(lambda x: 0 if x == 0 else 1)\nX_train_visualize['FE_relatives_onboard'] = X_train_visualize['Parch'] + X_train_visualize['SibSp']\nX_train_visualize['FE_is_alone'] = X_train_visualize['FE_relatives_onboard'].apply(lambda x: 0 if x == 0 else 1)\n","aa5db02d":"X_train_visualize.describe()","c3142ea2":"X_train_visualize.head()","5cb05664":"plt.figure(figsize=(12,8))\nsns.heatmap(X_train_visualize.corr().abs(), annot=True) ","dfc06fe8":"# Add Featured Engineering value\ndef fe_values(dataset):\n    dataset['TMP_relatives_onboard'] = dataset['Parch'] + dataset['SibSp']\n    dataset['FE_is_alone'] = dataset['TMP_relatives_onboard'].apply(lambda x: 0 if x == 0 else 1)\n    dataset['EF_fare_class'] = dataset['Fare'].fillna(0).apply(lambda x: x \/\/ 10 if x < 100 else 10 )\n    dataset = dataset.drop('TMP_relatives_onboard', axis=1)\n    return dataset\n\ndrop_list.append('SibSp')\ndrop_list.append('Parch')","2ce907dd":"sns.histplot(x='Fare', data=X_train_visualize.fillna(0), bins=50)","bbd52067":"X_train_visualize['EF_fare_class'] = X_train_visualize['Fare'].apply(lambda x: x \/\/ 10 if x < 100 else 10 )","9d0a229b":"sns.histplot(x='EF_fare_class', data=X_train_visualize, bins=50)","3d068083":"sns.heatmap(X_train_visualize[['Survived', 'Fare', 'EF_fare_class']].corr().abs(), annot=True) ","17a28d89":"# Add EF_class function to Feature Engineering function\ndrop_list.append('Fare')","39d2fa93":"print(\"Drop list\", drop_list)\nprint(\"Train data columns\", train_data.columns)","46de6b52":"# Train data\nX_pretrain = train_data\nX_pretrain = transform_category_data_to_numerics(X_pretrain)\nX_pretrain = handle_null_values(X_pretrain, X_pretrain)\nX_pretrain = fe_values(X_pretrain)\ny_train = X_pretrain['Survived']\nX_pretrain = X_pretrain.drop(drop_list, axis=1)\nX_visualize_pretrain = X_pretrain\nX_pretrain = X_pretrain.drop('Survived', axis=1)\n\n# Test data\nX_test = test_data\nX_test = transform_category_data_to_numerics(X_test)\nX_test = handle_null_values(X_test, X_pretrain) # Means are based on training data\nX_test = fe_values(X_test)\nX_test = X_test.drop(drop_list, axis=1)\n","74575ba9":"sns.heatmap(X_visualize_pretrain.corr().abs(), annot=True) ","3f1e1b5c":"X_pretrain.head()","48c57d38":"X_test.head()","63b01b7c":"y_train.head()","6492a24b":"# Check\nprint(X_pretrain.isnull().any())\nprint(y_train.isnull().any())\nprint(\"X_pretrain shape\", X_pretrain.shape)\nprint(\"y_train shape\", y_train.shape)\nprint(\"X_test shape\", X_test.shape)","eb9d198a":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\nX_pretrain_scaled = scaler.fit_transform(X_pretrain)\nX_test_scaled = scaler.fit_transform(X_test)","681e9103":"#X_scaled[:5]","1b35bdb5":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_pretrain_scaled, y_train, test_size=0.25, random_state=42)","abf9c9d3":"from sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\n\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\n\nxgb_cl = xgb.XGBClassifier()\n\nlog_reg_params = {'C':[0.5,1,5,10] }\nlgbm_params = {\n    'num_leaves': [31, 62, 99, 127],\n    'min_data_in_leaf': [30, 50, 100, 300, 400,1000],\n    'verbose': [-1]\n    }\nsvc_params = {'C':[0.5,1,5,10], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'degree': [1,3,5,7]}\nrf_params = { 'n_estimators': [250,300,350,400,500,550,600], 'max_depth': [4,5,6,7]}\nknn_params = {'n_neighbors': [1, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 30, 35, 40]}\nnb_params = {}\nxgb_params = {\n    \"max_depth\": [3, 4, 5, 7],\n    \"learning_rate\": [0.1, 0.01, 0.05],\n    \"gamma\": [0, 0.25, 1],\n    \"reg_lambda\": [0, 1, 10],\n    \"scale_pos_weight\": [1, 3, 5],\n    \"subsample\": [0.8],\n    \"colsample_bytree\": [0.5],\n}\n\n#model_list = [[LogisticRegression(), log_reg_params], \n#              [lgb.LGBMClassifier(), lgbm_params],\n#            [svm.SVC(), svc_params],\n#            [RandomForestClassifier(), rf_params], \n#            [KNeighborsClassifier(), knn_params], \n#            [GaussianNB(), nb_params],\n#            [xgb.XGBClassifier(), xgb_params]\n#]\n\n#for clf, params in model_list:\n#    print(\"--- \", clf, \" ---\")\n#    gs_clf = GridSearchCV(clf, params, cv=5)\n#    gs_clf.fit(X_train,y_train)\n#    print(gs_clf.best_estimator_)    \n#    y_pred = gs_clf.predict(X_val)\n#    print(gs_clf, accuracy_score(y_val, y_pred))\n#    print(\"\\n\\n\")\n","acecb307":"winner_clf = RandomForestClassifier(max_depth=5, n_estimators=570)\nwinner_clf.fit(X_train, y_train)\ny_pred = winner_clf.predict(X_val)\nprint(winner_clf, accuracy_score(y_val, y_pred))","4524dc4e":"y_submit = winner_clf.predict(X_test_scaled)","c9cbc80d":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = y_submit\nsubmission.to_csv('submission.csv', index=False)","b24e6e00":"...it does not. Let's add 'Name' to drop_list from now on.","a5740577":"### Given knowledge about data\nsurvival\tSurvival\t0 = No, 1 = Yes\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\nsex\tSex\t\nAge\tAge in years\t\nsibsp\t# of siblings \/ spouses aboard the Titanic\t\nparch\t# of parents \/ children aboard the Titanic\t\nticket\tTicket number\t\nfare\tPassenger fare\t\ncabin\tCabin number\t\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton","d2e1fe4a":"### Preprocessing","fada9483":"## 1.2 Numerics","39395901":"**Histplot Notes:**\n- SigSp, Parch, and Fare all have uneven distributions - and should be re-engineered. Most travel alone it seems (although it cannot to a 100% be understood in these graphs as people without Parch may travel with SibSp).\n- All else looks ok.","7ffd9c62":"Tried preprocessing.normalize first but it dropped the accuracy from 0.80 to 0.70","e3823b05":"**Sex**: should be transferred directly to boolean\n**Cabin**: Perhaps the letter in the cabin name signify importance - perhaps as to how 'high up' the cabin is - but let's ignore it for now. Make it boolean.\n**Embarked:** should be transferred to a OneHotEncoder","4c3c22ec":"## Investigative preprocessing","bdb3455e":"## 1.1 Categoricals","8a946777":"Before deleting 'Name' feature, it would be interesting to see if the Name feature could bring clarity to when age is null. ","e48cc385":"Perhaps some information lies within the numeric vs non-numeric ticket numbers but it seems unlikely. We can surely exclude it.","058b2279":"**Notes:**\n\n- Cabin number is *likely* not important per se but the existance of one should be.\n- Ticket number cannot be an important feature. Will remove.\n- Name should have no significance unless \"Miss\/Mr\/Mrs\" would be extracted to perhaps have some importance - if so only perhaps for Mrs\/Miss as of \"being married\". Will remove.\n- Why are Siblings and Spouses as well as Parents \/ Children mapped together? The common feature would be \"same generation \/ age-group - loved ones\" vs \"Other generation loved ones\". Makes some sense in that case. \n- Place of embarkment *should* not be important but perhaps people of Southampton are 'tougher' than other from Queenstown. Keep it.","dfb095cc":"**Notes:**\n\n- Not every passenger has age submitted. Could this be because they are infants? This needs to be looked into.","930986e1":"### Check result","2fdb809e":"All traveling for free are young single-traveling male from Southampton. Not sure what to do with this information. It is about 1% of the data.","1e70dcce":"**Conclusion:** EF_fare_class has a somewhat stronger correlation with Survived than Fare - so let's go with EF_fare_class. Let's drop Fare.","f19f5dd5":"### Side investigation: Fare = 0","d0aa2d03":"## Correlations in the data","1eada674":"**Note:** \n- Fare = 0 are free tickets - not cheap tickets. Who got them? C class people or A class?\n- Age is likely an important feature. Should the null values be removed or modified?\n- Would the *amount* of siblings and spouses (as well as parents and children) be an important factor or just the existence of it. Perhaps (saving 5 children is more difficult than 1, and you are more likely to die in the process). Let's keep it as such.","8e8a20b0":"# 1. Univariate analysis","32dde52b":"### Handle null values","7a5462ea":"KNN seems to be the most promising. ","48bf82c9":"It seems like being alone or not is more important (more correlated to survival) than the amount, or sort, of family onboard. Let's just keep that item.","5de5e4af":"### Create submission data","7463294e":"### Base model test","25c3174f":"Let's look into Ticket feature to see if there is anything in there that seems interesting.","e9bfd102":"## Data preprocessing","ed036b36":"What we want to look at here are values that are highly-uncorrelated with the target feature (i.e. around 0) as well as features that are very correlated with each other.\n\n\n**Uncorrelation - Notes:**\n- Surprisingly, age seems to have a low correlation with target feature. This needs to be investigated.\n\n**Correlation between variable - Notes:**\n- Fare, Pclass and has_cabin are highly correlated. Obviously, the more you pay the better class and lodging you get (embarkment_city, however, is not). Fare is the least correlated, which could signify that *paying* more does not necessary mean higher chances or survival - as long as you get important features for what you pay: cabin or better ticket class.","c8903f13":"### Fare","1194b177":"## Dataset histograms"}}