{"cell_type":{"706d697f":"code","513d9aeb":"code","ae2f1ab9":"code","97359943":"code","6a370b34":"code","b4bd1df8":"code","f1db913a":"code","47ea6226":"code","6887a508":"code","ed074341":"code","0a2407e4":"code","99f644e6":"code","4fc79e5b":"code","28ad7d92":"code","bb359e22":"code","b362a26d":"code","9f84fc34":"code","a4e48c4d":"code","e5abe0f2":"code","228bd2d4":"code","26051a85":"code","2d2bdaa5":"code","16781f89":"code","09e4e88f":"code","3eaf90e3":"code","409993ce":"code","833d6687":"code","06fe9cac":"code","f6aa53ab":"code","c75a62c8":"code","9e3640fd":"code","6a7995ec":"code","a9835e1a":"code","32a4f03e":"code","4c7a439f":"code","be173322":"code","ee5d00c4":"code","29278735":"markdown","dd7d2766":"markdown","ff562896":"markdown","0be4dad1":"markdown","231b3daf":"markdown","3ea51397":"markdown","0235c2ed":"markdown","e38cfd7b":"markdown","c053f3f0":"markdown","f4bc0023":"markdown","2c0cfc23":"markdown","c4955bc5":"markdown","b723a0a0":"markdown","2aef0f5b":"markdown","4e1e2f7d":"markdown","aba6cc18":"markdown","f35426cd":"markdown","d1a8269d":"markdown","f7314015":"markdown","490a7c08":"markdown","bdab1feb":"markdown","6653d816":"markdown","7d00752e":"markdown","10e150ba":"markdown","c599c18a":"markdown","67913bee":"markdown","0bcccac3":"markdown","19e5b72a":"markdown","3c7b1a69":"markdown"},"source":{"706d697f":"import os\nimport warnings\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 10)","513d9aeb":"PATH = \"..\/input\/open-shopee-code-league-marketing-analytics\/\"\n    \nos.listdir(PATH)","ae2f1ab9":"train_df = pd.read_csv(f\"{PATH}train.csv\")\nuser_df = pd.read_csv(f\"{PATH}users.csv\")\n\ntest_df = pd.read_csv(f\"{PATH}test.csv\")","97359943":"available_dfs = {\n    \"train.csv\": train_df, \n    \"test.csv\": test_df, \n    \"users.csv\": user_df\n}\n\nfor available_df in available_dfs.keys():\n    columns = available_dfs[available_df].columns.tolist()\n    \n    print(f\"{available_df} contains {len(columns)} columns\")\n    print(f\"They are : {columns}\")\n    print(\"\")","6a370b34":"train_df = pd.merge(train_df, user_df, how=\"left\", left_on=\"user_id\", right_on=\"user_id\")\ntest_df = pd.merge(test_df, user_df, how=\"left\", left_on=\"user_id\", right_on=\"user_id\")","b4bd1df8":"train_df.describe()","f1db913a":"test_df.describe()","47ea6226":"def get_summary_df(df):\n    \n    columns = df.columns.tolist()\n    \n    dtypes = []\n    unique_counts = []\n    missing_counts = []\n    missing_percentages = []\n    total_counts = [df.shape[0]] * len(columns)\n\n    for column in columns:\n        dtype = str(df[column].dtype)\n        dtypes.append(dtype)\n        \n        unique_count = df[column].nunique()\n        unique_counts.append(unique_count)\n\n        missing_count = df[column].isnull().sum()\n        missing_counts.append(missing_count)\n        \n        missing_percentage = round((missing_count\/df.shape[0]) * 100, 2)\n        missing_percentages.append(missing_percentage)\n        \n\n\n    summary_df = pd.DataFrame({\n        \"column\": columns,\n        \"dtype\": dtypes,\n        \"unique_count\": unique_counts,\n        \"missing_count\": missing_counts,\n        \"missing_percentage\": missing_percentages,\n        \"total_count\": total_counts,\n    })\n    \n    summary_df = summary_df.sort_values(by=\"missing_percentage\", ascending=False).reset_index(drop=True)\n    \n    return summary_df","6887a508":"get_summary_df(train_df)","ed074341":"train_df[\"last_open_day\"].unique()","0a2407e4":"test_df[\"last_open_day\"].unique()","99f644e6":"activities = [\"login\", \"checkout\", \"open\"]\nfor activity in activities:\n    train_df[f\"last_{activity}_day\"] = train_df[f\"last_{activity}_day\"].apply(lambda x: np.nan if x == f\"Never {activity}\" else int(x))\n    test_df[f\"last_{activity}_day\"] = test_df[f\"last_{activity}_day\"].apply(lambda x: np.nan if x == f\"Never {activity}\" else int(x))","4fc79e5b":"last_activity_column_names = [f\"last_{activity}_day\" for activity in activities]","28ad7d92":"get_summary_df(train_df[last_activity_column_names])","bb359e22":"get_summary_df(test_df[last_activity_column_names])","b362a26d":"train_df[last_activity_column_names].describe()","9f84fc34":"test_df[last_activity_column_names].describe()","a4e48c4d":"def plot_feature_scatter(df1, df2, features):\n    i = 0\n    sns.set_style(\"whitegrid\")\n    plt.figure\n    \n    fig, ax = plt.subplots(5, 4, figsize=(20, 20))\n    \n    for feature in features:\n        i += 1\n        plt.subplot(5, 4, i)\n        plt.scatter(df1[feature], df2[feature], marker=\"+\", color='#2B3A67', alpha=0.2)\n        plt.xlabel(feature, fontsize=9)\n        \n    plt.show()","e5abe0f2":"features = [\"subject_line_length\", \"login_count_last_10_days\", \"login_count_last_30_days\", \"login_count_last_60_days\", \"open_count_last_10_days\", \"open_count_last_30_days\", \"open_count_last_60_days\", \"checkout_count_last_10_days\", \"checkout_count_last_30_days\", \"checkout_count_last_60_days\", \"age\", \"attr_1\", \"attr_2\", \"attr_3\", \"last_open_day\", \"last_checkout_day\", \"last_login_day\"]\n\nplot_feature_scatter(train_df.sample(50000), test_df.sample(50000), features)","228bd2d4":"def plot_feature_distribution(df1, df2, label1, label2, features, size=[4,5]):\n    i = 0\n    sns.set_style(\"whitegrid\")\n    \n    plt.figure()\n    fig, ax = plt.subplots(size[0], size[1], figsize=(22, 20))\n    \n    for feature in features:\n        i += 1\n        plt.subplot(size[0], size[1], i)\n        \n        sns.distplot(df1[feature], hist=False, label=label1, kde_kws={'bw':1.5}, color=\"#5982C5\")\n        sns.distplot(df2[feature], hist=False, label=label2, kde_kws={'bw':1.5}, color=\"#FB3523\")\n        \n        plt.xlabel(feature, fontsize=9)\n        \n        locs, labels = plt.xticks()\n        plt.tick_params(axis=\"x\", which=\"major\", labelsize=6, pad=6)\n        plt.tick_params(axis=\"y\", which=\"major\", labelsize=6)\n        \n    plt.show()","26051a85":"target_0_df = train_df.loc[train_df[\"open_flag\"] == 0]\ntarget_1_df = train_df.loc[train_df[\"open_flag\"] == 1]\n\nplot_feature_distribution(target_0_df, target_1_df, '0', '1', features, size=[4,5])","2d2bdaa5":"plot_feature_distribution(train_df, test_df, 'train', 'test', features, size=[4,5])","16781f89":"train_df[train_df[\"last_login_day\"] < 18000][\"last_login_day\"].max()","09e4e88f":"test_df[test_df[\"last_login_day\"] < 18000][\"last_login_day\"].max()","3eaf90e3":"train_df[train_df[\"last_login_day\"] > 18000].sort_values(by=\"last_login_day\").head(10)[[\"grass_date\", \"last_login_day\"]]","409993ce":"test_df[test_df[\"last_login_day\"] > 18000].sort_values(by=\"last_login_day\").head(10)","833d6687":"train_df[train_df[\"age\"] < 0]","06fe9cac":"test_df[test_df[\"age\"] < 0]","f6aa53ab":"train_df[train_df[\"age\"] < 18].shape","c75a62c8":"test_df[test_df[\"age\"] < 18].shape","9e3640fd":"train_df[train_df[\"age\"] > 100][\"age\"].value_counts()","6a7995ec":"test_df[test_df[\"age\"] > 100][\"age\"].value_counts()","a9835e1a":"plot_feature_distribution(train_df[(train_df[\"age\"] == 50) & (train_df[\"open_flag\"] == 0)], train_df[(train_df[\"age\"] == 50) & (train_df[\"open_flag\"] == 1)], \"0\", \"1\", features, size=[4,5])","32a4f03e":"plot_feature_distribution(train_df[(train_df[\"age\"] != 50) & (train_df[\"open_flag\"] == 0)], train_df[(train_df[\"age\"] != 50) & (train_df[\"open_flag\"] == 1)], \"0\", \"1\", features, size=[4,5])","4c7a439f":"def plot_feature_distribution_by_elems(df1, df2, label1, label2, elems, size=[2,4]):\n    i = 0\n    sns.set_style(\"whitegrid\")\n    \n    ig, ax = plt.subplots(size[0], size[1], figsize=(20, 10))\n    \n    for i, elem in enumerate(elems):\n        plt.subplot(size[0], size[1], i+1)\n\n        sns.kdeplot(df1[df1[\"country_code\"] == elem][\"subject_line_length\"], bw=0.5, label=label1, color=\"#5982C5\")\n        sns.kdeplot(df2[df2[\"country_code\"] == elem][\"subject_line_length\"], bw=0.5, label=label2, color=\"#FB3523\")\n\n        plt.xlabel(elem, fontsize=11)\n        locs, labels = plt.xticks()\n\n        plt.tick_params(axis=\"x\", which=\"major\", labelsize=8)\n        plt.tick_params(axis=\"y\", which=\"major\", labelsize=8)\n\n    plt.show()","be173322":"country_codes = sorted(train_df[\"country_code\"].unique().tolist())\n\nplot_feature_distribution_by_elems(target_0_df, target_1_df, \"0\", \"1\", country_codes, size=[2,4])","ee5d00c4":"plot_feature_distribution_by_elems(train_df, test_df, \"train\", \"test\", country_codes, size=[2,4])","29278735":"**Interesting Findings**:\n1. The value `50` that is date-related is already interesting to begin with, especially knowing the fact that 50 years ago is the UNIX epoch time and in this competition, this didn't only happen to the `age` but also to the `last_login_day` so it leads to another question, could this be the default value of ***something***?\n2. The distribution to the target is more or less similar between customers whose age is 50 years old and those whose ages are not, except for `last_open_day` in which the clients with `last_open_day` closer to zero are likely to open the campaign email, the peak is quite interesting for this one.","dd7d2766":"It turns out that the max last login days excluding the 18k values is 1445 days in train and 1399 days in test which is around 4 years ago or if we trace back from 2019 (the grass date in this competition) it goes back to the day Shopee was created which is in 2015, which actually makes sense. But what about these data points with 18k value for `last_login_day`. Let's check them out.","ff562896":"Hi All, in this notebook I'll take you a little bit deeper to see some interesting patterns in the data and hopefully you'll be able able to uncover the ***magic*** features and find new insights from the notebook.","0be4dad1":"### More to come soon, stay tune and thank you for reading.","231b3daf":"Let's explore the `last_{activity}_day` a little bit deeper.","3ea51397":"**Interesting Findings**:\n1. `last_{activity}_day` is in the form of `object` not `int64`\n2. Despite having different number of unique values `age` and `attr_1` has similar count of missing values\n3. Age can be as low as -17 and as high as 118\n4. Maximum values of `{activity}_count_last_{duration}_days` can be 100% higher than the 75% percentile value\n\n<span style=\"font-size: 5\">activity = `open`, `login`, `checkout`<span><br\/>\n<span style=\"font-size: 5\">duration = `10`, `30`, `60`<span>","0235c2ed":"Let's see the density plots for all of the numerical features that we had","e38cfd7b":"Now that we have `last_{activity}_day` fixed. Let's explore our data a little bit more. First let's start by comparing the scatter of 50000 samples between our train data and test data.","c053f3f0":"Let's see the clients with ages lower than legal age (I'm using 18 here but you can use whatever)","f4bc0023":"**Interesting Findings**:\n1. The distribution of `open_flag` = 1 and `open_flag` = 0 is somewhat dissimilar. Like for example, when `subject_line_length`is in-between length of 35 and 45 the probability of having `open_flag` = 1 is high. But maybe we shouldn't do it this way because of the fact that every country has different languages and some country's `subject_line_length` 30 is not similar to the other countries `subject_line_length` 30. Well, let's dive into it deeper later.\n2. From the distribution graph, we can also understand that feature about the often the clients open their mail (`open_count_{duration}_day`), the higher the tendency of the clients to open the sent campaign mail from Shopee which is actually no brainer.\n3. We can also observe that there is a considerable number of features with significant different distribution for the two target values, like;\n    - `age`: millenials tend to not open the email and people with the age of 100+ tends too, also what's with the sudden increase in the number of the population having age around 50? interesting\n    - `last_open_day`: the longer the clients didn't open the email the less likely the clients to open the campaign email (no brainer) but see the sudden peak between 600 and 700, let's explore it later.\n    - `last_login_day`: there's this tendency to not open the campaign email between 0 and around 2500 login days","2c0cfc23":"Let's see the clients with ages lower than 0 years old","c4955bc5":"and let's compare it with all the other ages","b723a0a0":"Let's see the density plots to compare the distribution between our train and test data","2aef0f5b":"Let's see the clients with ages higher than 100","4e1e2f7d":"**Interesting Findings**:\n1. Similar to our previous finding in test and train data `last_login_day` can be as old as 18k and there's an empty range between the 18k data point from where the scatter gathers. Hmmm, interesting!\n2. Scatter forming vertical and horizontal line near 0, 50 and 120 years old age both in test and train data\n3. The low number of train samples with 0 value for `attr_2` and `attr_1`\n4. The accumulation of `subject_line_length` with a value of 10 in train data and the gap between the `subject_line_length` with a length of 10 and 20","aba6cc18":"The definition of each column can be seen from the Shopee competition page.","f35426cd":"It turns out we need to merge the `train.csv` and `test.csv` with the `users.csv` in order to fully harness the power of the data.","d1a8269d":"Let's also see the summary of the df","f7314015":"Let's see the distribution for the train and test too!","490a7c08":"**Interesting Findings**:\n1. In train you can see someone with the `age` of negative value while fortunately in test you can't\n2. From the result you can also see that we still have clients with `age`s lower than 18 and can't be as low as zero which is somehow impossible\n3. Also, you can see from train data that the clients whose `age` is higher than 100 are mostly having similar age which is 118. So how is this possible?\n\nWell it turns out that you can change your age to as old as 1900 in Shopee's platform. So that's explain the 118 years old clients.\n\n![birthdate.PNG](attachment:birthdate.PNG)\n\nSo yeah you can also consider this when you try to remove\/change the data points. \n\nPreviously we also saw that we have a high increase in number of  clients whose age is 50 years old, which is somehow odd. Let's take a look at it.","bdab1feb":"Let's start with exploring the `last_login_day` since the value of 18k itself is already interesting to begin with. Let's start by finding the maximum days exluding the 18k days.","6653d816":"In total we have four different comma separated values data, namely; `train.csv`, `test.csv` and `sample_submission_0_1.csv` which is self-explanatory and `users.csv` containing the information of the user. Let's load the data to see what it contains.","7d00752e":"**Interesting Findings**:\n1. `last_checkout_day` has 1\/5 of its value as `Never` while `last_login_day` has the lowest `Never` valuess.\n2. `last_login_day` can be as old as 18k days ago which is around 50 years ago.","10e150ba":"I also mentioned the fact that country has different languages and some country's `subject_line_length` 30 may not be similar to the other countries `subject_line_length` 30, so let's take a closer look on that too.","c599c18a":"If you see the `grass_date` and `last_login_day`, `last_login_day` is increasing as the `grass_date` increases, interesting! and if we actually substract the `last_login_day` from the `grass_date` they all trace back to the similar date which is January 1st, 1970. Yes you got that right it's UNIX epoch time. So this can be a consideration for the next step on what to do with these data points during the modelling.","67913bee":"It turns out that we have the `Never {activity}` data point to indicate that the user never logs in\/opens\/checks out. Let's replace the values with `NaN` for now recheck the summary of the df","0bcccac3":"**Interesting Findings**:\n1. No really clear pattern but we can see in some country at some points in the graph the distribution between the `open_flag` 0 and 1 is different. The distribution for the country with `id` 3 and 6 almost look similar between `open_flag` 0 and 1.\n2. For the country with `id` 5 the `subject_line_length` can be as low as 10. If you follow the EDA from the start we also observed the gap between the `subject_line_length` 10 and 20 when we tried to scatter the train and test data, so this was the cause of it. But the interesting part is country with `id` 5 is the only country with such `subject_line_length`, the tendency of the clients to not open the email campaign when sent such email is somehow low and the fact that we don't have such data points in our test data makes it more interesting.\n3. As written in point 2, in test data we can't see some data points that we saw previously in data points like the `subject_line_length` 10 in 5 and `subject_line_length` 30 in 7.","19e5b72a":"**Interesting Findings**:\n1. The distribution of our train and test distribution is almost similar for all of the features except for `subject_line_length`, `login_count_last_10_days` and `last_checkout_day`\n2. What's more interesting is the fact that we have slightly higher percentage of value between 0 and 50 in train for `login_count_last_10_days` but similar pattern doesn't occur in `login_count_last_30_days` and `login_count_last_60_days`\n3. The tail of train data is slightly longer in `open_count_last_30_days`, `open_count_last_60_days`, `checkout_count_last_30_days`, `checkout_count_last_60_days` and `age`","3c7b1a69":"How about `age`? We also see some inconsitencies in age, like how can someone whose age is -17 years old or 0 years old or 117 years old can be Shopee's client? Well let's take a look at it!"}}