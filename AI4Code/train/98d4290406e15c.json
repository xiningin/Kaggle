{"cell_type":{"e060cb90":"code","a82e0a5d":"code","f7738e02":"code","6026df1a":"code","eb3fac8e":"code","c38fa40a":"code","c3278a9e":"code","0da7eee4":"code","07280c9f":"code","240f9c27":"code","5f44b9a4":"code","7481082b":"code","d721ecda":"code","43479ed3":"code","00a3e047":"code","ba6b3b0d":"code","f729183b":"code","0f3f8a94":"code","755e9fe0":"code","79ae62b5":"code","820adb13":"code","23c952aa":"code","e9a523d7":"code","252c2053":"code","4f58cdc9":"code","0f70c41e":"code","354edfa6":"code","5a35a33e":"code","743f1841":"code","fb59ed12":"code","6211b07c":"code","b6d633a9":"code","50006d99":"code","71c532f8":"code","f9e0b868":"code","2aab252c":"code","b9ff85e3":"code","6211ed32":"code","917de1c6":"code","d5c5919c":"code","27a9bdbd":"code","e691efc6":"code","abd2b095":"code","b1ca8668":"code","b81d96f4":"code","4731b2c2":"code","7182f7fb":"code","81337173":"code","a10704c1":"code","9e1df993":"code","eb4e3e9f":"code","ae75084c":"markdown","c2b046a4":"markdown","bf679280":"markdown","1d22f8af":"markdown","cd21c325":"markdown","111b2a77":"markdown","67e0f315":"markdown","ae3caabc":"markdown","5155d059":"markdown","5ab18dcd":"markdown","ff223d15":"markdown","1eb7f0db":"markdown","aa2827f2":"markdown","883b649a":"markdown","f628a1af":"markdown"},"source":{"e060cb90":"# Ignore Warnings.\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Basic Libraries\nimport os\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport operator\nimport re\n\n# import textstat\nimport gensim.downloader as api\n\n# Import Data Visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Import sklearn packages\nfrom sklearn.linear_model import *\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.manifold import TSNE\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n# Import NLP pacakeges.\nimport nltk\nfrom textblob import TextBlob\nfrom scipy.stats import probplot\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.tokenize import word_tokenize\nfrom scipy.stats import probplot\n\n# Specify print options.\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","a82e0a5d":"# Evaluate Model Performace.\ndef model_performance(model, X, y):\n    y_pred = model.predict(X)\n    rmse = mean_squared_error(y, y_pred)\n    print(rmse)\n\n# Cross Validation.    \ndef create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    data = data.sample(frac=1).reset_index(drop=True)\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    data.loc[:, \"bins\"] = pd.cut(data[\"target\"], bins=num_bins, labels=False)\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    data = data.drop(\"bins\", axis=1)\n    return data","f7738e02":"PATH = '\/kaggle\/input\/commonlitreadabilityprize\/'","6026df1a":"df_train = pd.read_csv(f'{PATH}train.csv')\ndf_test = pd.read_csv(f'{PATH}test.csv')","eb3fac8e":"df_train.head()","c38fa40a":"df_test.head()","c3278a9e":"df_train.info()","0da7eee4":"df_test.info()","07280c9f":"df_train.describe()","240f9c27":"df_raw['polarity'] = df_raw['excerpt'].apply(lambda x: TextBlob(x).sentiment.polarity)","5f44b9a4":"# 1. What does the distribution of label look like? What does it convey?\n# 2. How is standard error distributed? What does it mean?\n# 3. What do low and high values of target mean?\n# 4. Get the feel of a few high & low values excerpt and read them. \n# 5. How does word count, character length, avg. char. length per word, count of punctuation marks vary with target? \n# 6. Try to get the topic for each excerpt? \n# 7. Get top unigram, bigram & trigram. How do they vary by different target buckets?\n# 8. Study other notebooks to get ideas for feature engineering. ","7481082b":"# Distribution of target\nsns.kdeplot(data=df_train, x='target', fill=True)","d721ecda":"# Distribution of se\nsns.kdeplot(data=df_train, x='standard_error', fill=True)","43479ed3":"fig, axes = plt.subplots(ncols=2, figsize=(12,6), dpi=100)\nsns.kdeplot(data=df_train, x='target', fill=True, ax=axes[0])\naxes[0].axvline(df_train['target'].mean(), label=f'target Mean', color='r', linewidth=2, linestyle='--')\naxes[0].axvline(df_train['target'].median(), label=f'target Median', color='b', linewidth=2, linestyle='--')\nprobplot(df_train['target'], plot=axes[1])\naxes[0].legend(prop={'size': 10})\n\nfor i in range(2):\n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=12)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\naxes[0].set_title('target Distribution in Training Set', fontsize=10, pad=12)\naxes[1].set_title('target Probability Plot', fontsize=10, pad=12)","00a3e047":"# Print top 2 excerpts.\ntop2 = df_train.nlargest(2, 'target').reset_index(drop=True)\nfor i in range(2):\n    print(f'Excerpt with target value = {top2.loc[i, \"target\"]}')\n    print(top2.loc[i, 'excerpt'])\n    print('\\n')","ba6b3b0d":"# Print bottom 2 excerpts.\nbottom2 = df_train.nsmallest(2, 'target').reset_index(drop=True)\nfor i in range(2):\n    print(f'Excerpt with target value = {bottom2.loc[i, \"target\"]}')\n    print(bottom2.loc[i, 'excerpt'])\n    print('\\n')","f729183b":"# How does word count, character length, avg. char. length per word, count of punctuation marks vary with target? ","0f3f8a94":"# Character length.\ndf_train['char_count'] = df_train['excerpt'].apply(lambda x: len(str(x)))","755e9fe0":"# Word Count - split on space.\ndf_train['word_count_sp'] = df_train.excerpt.str.split().apply(lambda x: len(x))","79ae62b5":"# Word Count tokenized.\n#paragraphs = df_train[\"excerpt\"]\n\n# Tokenize each paragraph\ndf_train['tokenized_excerpt'] = [word_tokenize(p.lower()) for p in df_train[\"excerpt\"]]","820adb13":"df_train['word_count_tk'] = df_train['tokenized_excerpt'].apply(lambda x: len(x))","23c952aa":"# Avg. word length (ignore the punctuations)\ndf_train['avg_word_length'] = df_train['char_count'] \/ df_train['word_count_sp']","e9a523d7":"# Punctuation count\nexcerpt = []\npunct_marks = list()\ncount_punct = 0\nfor punct in punct_marks:\n    for token in excerpt:\n        if punct in token:\n            count_punct = count_punct + 1\n            \n        \n    ","252c2053":"# Digit count. \ndf_train['num_digits'] = df_train['excerpt'].apply(lambda excerpt: sum(char.isdigit() for char in excerpt))","4f58cdc9":"df_train.head()","0f70c41e":"X = df_train.loc[:, 'excerpt']\ny = df_train.loc[:, 'target']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X.values, y, random_state=42, test_size=0.25, shuffle=True)","354edfa6":"df_out = pd.DataFrame()\ndf_out['id'] = df_test.loc[:, 'id']\nX_test = df_test.loc[:, 'excerpt']","5a35a33e":"# TFIDF for feature extraction.\ntfv = TfidfVectorizer(\n    min_df=3,\n    max_features=None, \n    strip_accents='unicode', \n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    ngram_range=(1, 3), \n    use_idf=1,smooth_idf=1,sublinear_tf=1,\n    stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(X_train) + list(X_valid))\n\nX_train_tfv =  tfv.transform(X_train) \nX_valid_tfv = tfv.transform(X_valid)","743f1841":"X_test_tfv = tfv.transform(X_test)","fb59ed12":"# Build a linear model\n# Fitting a simple Logistic Regression on TFIDF\n\nlr = Ridge()\nlr.fit(X_train_tfv, y_train)","6211b07c":"# Train performacne\nmodel_performance(lr, X_train_tfv, y_train)\n# Validation performacne\nmodel_performance(lr, X_valid_tfv, y_valid)\n# Test set\ndf_out['target'] = lr.predict(X_test_tfv)\n# Submission\ndf_out.to_csv('submission.csv', index = False)","b6d633a9":"excerpt = df_train['excerpt']","50006d99":"text = \"This is Andrew's text, isn't it?\"","71c532f8":"tkz = nltk.tokenize.WhitespaceTokenizer()\ntkz.tokenize(text)","f9e0b868":"# Punctuations might be useful to gauge reading difficulty.\ntkz = nltk.tokenize.WordPunctTokenizer()\ntkz.tokenize(text)","2aab252c":"tkz = nltk.tokenize.TreebankWordTokenizer()\ntkz.tokenize(text)","b9ff85e3":"# Tokenization\ntk_excerpt = excerpt.apply(word_tokenize)\ntk_excerpt.head()","6211ed32":"# Stemming\nporter = nltk.PorterStemmer()\ntk_st_excerpt = tk_excerpt.apply(lambda x: [porter.stem(y) for y in x])\ntk_st_excerpt.head()","917de1c6":"# Stemming example\ntext1 = excerpt[0]\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tkz.tokenize(text1)\nprint(tokens)\n\nstemmer = nltk.stem.PorterStemmer()\nstemmed = \" \".join(stemmer.stem(token) for token in tokens )\nprint(stemmed)\n","d5c5919c":"# Lemmatization.\nwnl = nltk.WordNetLemmatizer()\ntk_lm_excerpt = tk_excerpt.apply(lambda x: [wnl.lemmatize(w) for w in x])\ntk_lm_excerpt.head()","27a9bdbd":"# Sentence Splitting\nexcerpt_sent_tokenized = excerpt.apply(lambda x: nltk.sent_tokenize(x))\nexcerpt_sent_tokenized.head()","e691efc6":"print(excerpt_sent_tokenized)","abd2b095":"# Bag of words.\n# Among medium frequency n-grams, the n-grams with smaller frrequency can be more discriminating because it can capture, \n# a specific issue in the review.\n\ntexts = [\"good movie\", \"not a good movie\", \"did not like\", \"i like it\", \"good one\"]\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\nfeatures = tfidf.fit_transform(texts)\npd.DataFrame(\n  features.todense(),\n  columns = tfidf.get_feature_names()\n)","b1ca8668":"import nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import wordnet_ic\nfrom nltk.collocations import *","b81d96f4":"## Path Similarity\n# 1. Shortest path between two concepts in the heirarchy \n# 2. similarity measure inversely related to this distance.\n\ngenerator = wn.synset('generator.n.01')\ncoil = wn.synset('coil.n.01')\ncar = wn.synset('car.n.01')\n\nprint(generator.path_similarity(car))","4731b2c2":"## Lowest Common Subsumer\n# 1. Find the lowest commom ancestor to both concepts.\n# 2. Calculate Lin Smilarity: Similarity measure based on information contained in LCS of both concepts.\n\nbrown_ic = wordnet_ic.ic('ic-brown.dat')\nprint(generator.lin_similarity(coil, brown_ic))\nprint(generator.lin_similarity(car, brown_ic))","7182f7fb":"## Collocations and Distributional similarity.\n# 1. Two words that frequently appear in similar contexts are more likely to be semantically related.\n# 2. Words before, after, in a small window.\n# 3. POS of words before, after, in a small window.\n# 4. Compute strength of association between words\n\ntext = ' '.join(df_train['excerpt'].to_list())\nbigram_meausres = nltk.collocations.BigramAssocMeasures()\nfinder = BigramCollocationFinder.from_words(text)\nfinder.nbest(bigram_meausres.pmi, 10)","81337173":"finder.apply_freq_filter(10)","a10704c1":"#doc_set = 'This is a kaggle notebook. I like kaggling'","9e1df993":"#doc_set.split()","eb4e3e9f":"#import gensim\n#from gensim import corpora, models\n\n#dictionary = corpora.Dictionary(doc_set.split())\n#corpus = [dictionary.doc2bow(doc) for doc in doc_set]\n#lda_model = gensim.ldamodel.LdaModel(corpus, num_topics=4 , id2word=dictionary , passes = 50)\n#print(lda_model.print_topics(num_topics=4 , num_words=5))","ae75084c":"### Feature Extraction from text","c2b046a4":"### The compexity of an excerpt increase as the target value increases.","bf679280":"### Topic Modelling: Generative Models and LDA\n* A coarse level analysis of what's in a text collection.\n* Topic: the subject\/theme of a discourse.\n* Topics are represented as word distribution.\n* A document is assumed to be a mixture of topics.\n* You're given a corpus and a set of topics.\n* Essentially it's a text clustering problem, documents & words clustered simultaneously.","1d22f8af":"### Semantic Similarity","cd21c325":"# Take a look at the Data & Summary Stats.","111b2a77":"# Exploratory Data Analysis","67e0f315":"# Utility Functions","ae3caabc":"# CommonLit Readability Prize","5155d059":"# Import Libraries","5ab18dcd":"# Import Data ","ff223d15":"# Learning & Practice Concepts","1eb7f0db":"### Tokenization: A process that splits an input sequence into so-called tokens\n* Token can be thought of as a useful unit for semantic processing.\n* Can be a word sentence or a paragraph\n* Examples of popular tokenizers are nltk.tokenize.WhitespaceTokenizer, PunctTokenizer, TreebankWordTokenizer","aa2827f2":"# Building first models","883b649a":"What is CommonLit?\n* CommonLit, Inc., is a nonprofit education technology organization. \n* They are serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. \n* They want to improve the readability rating methods of these lessons.\n\nProblem Summary\n* Identify the appropriate reading level of a passage of text by rating the complexity of reading passages for grade 3-12 classroom use.\n* A dataset is provided that includes readers from a wide variety of age groups and a large collection of texts taken from various domains.\n\nCurrent Gaps.\n* As of now,most educational texts are matched to readers using traditional readability methods or commercially available formulas.\n  The traditional readability formulas are not roboust enough are often inaccurate whereas commercially available solutions are expensive, non-          transparent, and lack evidence that supports their effectiveness.\n\nFuture Desired State\n* Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. \n* Rating algorithms will no lenger be a black box and will be available to all. \n* Students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.\n","f628a1af":"### Token Normalization\n* We may want the same token for different forms of the word. Ex wolfs, wolves -> wolf.\n* Stemming: A process of removing and replacing suffixes to get the root form of the word known as a stem. wolves -> wolv. Produces non words\n* Lemmatization: Return the base of dictionary form of the word known as lemma."}}