{"cell_type":{"f373c4e9":"code","05d18e24":"code","660d20ae":"code","c7804262":"code","04000e3f":"code","796df839":"code","537c3bea":"code","0820da4f":"code","7a1e28e7":"code","2d94acaa":"code","ae7636ed":"code","2dbf3f19":"code","7d6f168a":"code","bc35cbd5":"code","ad02ffb2":"code","caa162cb":"code","8f41bf07":"code","d3993d76":"code","54f4fba7":"code","8459e940":"code","d9276478":"code","7d648ec4":"code","d086488e":"code","6e6af3db":"code","e0e41a81":"code","6f0e23c2":"code","c6ae1a18":"code","3a357889":"code","a42458f8":"code","937572fd":"code","726ce8af":"code","718d40cc":"code","8a7d1b91":"code","cee591a4":"code","36cf1484":"code","1208ce3a":"code","e1ff7c6f":"code","4275079d":"code","37101a43":"code","60d29a30":"code","d90a0930":"code","4116dedd":"code","91a1c6cb":"code","6bff0e79":"code","0b0e1e80":"code","223551f6":"code","0b959605":"code","2c34be76":"code","04f0064f":"code","2a436674":"code","58f158a4":"code","b92c0334":"code","dabdfddb":"code","f3180d1f":"code","bc255066":"code","f9592d34":"code","780be482":"code","03f4e59a":"code","de760a9c":"code","6e81b171":"code","eafd5564":"markdown","9da5179d":"markdown","915fa713":"markdown","45cacd04":"markdown","963cadca":"markdown","48807b32":"markdown","f79bae05":"markdown","9d23fdbc":"markdown","fa9f598c":"markdown","1c79fe31":"markdown","7a7865aa":"markdown","28a44d0b":"markdown","95aae6b8":"markdown","2350971b":"markdown","38dc5d72":"markdown","ab3c6cbb":"markdown","a6a5d626":"markdown","786bcee8":"markdown","2f171297":"markdown","6214d895":"markdown","f8fae7ab":"markdown","2316af60":"markdown","fcd88428":"markdown","7738585c":"markdown","7e6e6b65":"markdown","d22ff82b":"markdown","8ba15a37":"markdown","2af9e7aa":"markdown","1071fc82":"markdown","f4b76f67":"markdown","615c9838":"markdown","0fa6ef18":"markdown","3dc437e2":"markdown","f9490cc8":"markdown","9733e551":"markdown","7b9932f8":"markdown","9cb2ab88":"markdown","737d7b05":"markdown","f90b85bf":"markdown","e7271337":"markdown","11326d46":"markdown","3eacebb5":"markdown","c8888a62":"markdown","09dbbaf6":"markdown","fefb8896":"markdown","ac3bd9e0":"markdown","3fd7d792":"markdown","cabbe34f":"markdown","c667ce25":"markdown","3fa508ad":"markdown","7b912f14":"markdown","541f74ad":"markdown","e4edc37b":"markdown","173ce03c":"markdown","5bcdce0e":"markdown","27d81319":"markdown","48e27a94":"markdown","bcabf8e1":"markdown","8e579c77":"markdown","d8af8989":"markdown","d0ff42e9":"markdown","98070129":"markdown","225f4ec0":"markdown","c47247f4":"markdown","a1b1d7ba":"markdown","71018590":"markdown"},"source":{"f373c4e9":"# loading necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\nfrom sklearn import metrics\nimport missingno as msno\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport scipy.sparse\nimport gc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import roc_auc_score\nimport warnings \nfrom lightgbm import LGBMClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, matthews_corrcoef\nfrom sklearn.metrics import f1_score\nimport scipy as sp\nimport os\nimport scikitplot as skplt\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","05d18e24":"# to increase the display capacity\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","660d20ae":"df_train = pd.read_csv('..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv')","c7804262":"df_train.head()","04000e3f":"df_train.info()","796df839":"df_train.isnull().sum()","537c3bea":"print(df_train.Exited.value_counts())\n#Visualising non-churners and churners cases\nplt.bar(\"Churn\", df_train[\"Exited\"].value_counts()[1], color=\"red\")\nplt.bar(\"No Churn\", df_train[\"Exited\"].value_counts()[0], color=\"green\")\nplt.ylabel(\"Count\", fontsize=14)\nplt.title(\"Churn VS No Churn\")","0820da4f":"df_train.groupby('Exited')['CreditScore'].mean() ","7a1e28e7":"df_train.groupby('Exited')['Age'].mean() ","2d94acaa":"df_train.groupby('Exited')['Balance'].mean() ","ae7636ed":"df_train.groupby('Exited')['EstimatedSalary'].mean() ","2dbf3f19":"df_train.describe()","7d6f168a":"df_train.Tenure.value_counts()","bc35cbd5":"df_train.NumOfProducts.value_counts()","ad02ffb2":"df_train.HasCrCard.value_counts()","caa162cb":"df_train.IsActiveMember.value_counts()","8f41bf07":"df_train.describe(exclude = 'number')","d3993d76":"df_train.Geography.value_counts()","54f4fba7":"df_train.Gender.value_counts()","8459e940":"df_train.groupby('Geography')['Exited'].value_counts()","d9276478":"df_train.groupby('Gender')['Exited'].value_counts()","7d648ec4":"df_train.groupby('NumOfProducts')['Exited'].value_counts()","d086488e":"df_train.groupby('Gender')['NumOfProducts'].value_counts()","6e6af3db":"df_train.groupby('Tenure')['Exited'].value_counts()","e0e41a81":"def describe(datatrain, feature):\n    d = pd.DataFrame(columns=[feature,'Train','Train - Churn','Train - No Churn'])\n    d[feature] = ['count','mean','std','min','25%','50%','75%','max','unique','NaN','NaNshare']\n    for i in range(0,8):\n        d['Train'].iloc[i] = datatrain[feature].describe().iloc[i]\n        d['Train - Churn'].iloc[i]=datatrain[datatrain['Exited']==1][feature].describe().iloc[i]\n        d['Train - No Churn'].iloc[i]=datatrain[datatrain['Exited']==0][feature].describe().iloc[i]\n    d['Train'].iloc[8] = len(datatrain[feature].unique())\n    d['Train - Churn'].iloc[8]=len(datatrain[datatrain['Exited']==1][feature].unique())\n    d['Train - No Churn'].iloc[8]=len(datatrain[datatrain['Exited']==0][feature].unique())\n    d['Train'].iloc[9] = datatrain[feature].isnull().sum()\n    d['Train - Churn'].iloc[9] = datatrain[datatrain['Exited']==1][feature].isnull().sum()\n    d['Train - No Churn'].iloc[9] = datatrain[datatrain['Exited']==0][feature].isnull().sum()\n    d['Train'].iloc[10] = datatrain[feature].isnull().sum()\/len(datatrain)\n    d['Train - Churn'].iloc[10] = datatrain[datatrain['Exited']==1][feature].isnull().sum()\/len(datatrain[datatrain['Exited']==1])\n    d['Train - No Churn'].iloc[10] = datatrain[datatrain['Exited']==0][feature].isnull().sum()\/len(datatrain[datatrain['Exited']==0])\n    return d","6f0e23c2":"BalanceAmtDescribe = describe(df_train,'Balance')","c6ae1a18":"BalanceAmtDescribe","3a357889":"df_train[df_train['Balance']>222000]","a42458f8":"AgeDescribe = describe(df_train,'Age')","937572fd":"AgeDescribe","726ce8af":"df_train[df_train['Age']>=80]","718d40cc":"train_age = (df_train.groupby(['Exited'])['Age']\n                     .value_counts(normalize=True)\n                     .rename('percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values('Age'))\n                     \nplt.figure(figsize=(20,20))\nsns.barplot(x=\"Age\", y=\"percentage\", hue=\"Exited\", data=train_age)","8a7d1b91":"# Visualize the distribution of 'Day_Mins'\nsns.distplot(df_train.CreditScore)\n\n# Display the plot\nplt.show()","cee591a4":"# Visualize the distribution of 'Day_Mins'\nsns.distplot(df_train.Age)\n\n# Display the plot\nplt.show()","36cf1484":"# Visualize the distribution of 'Day_Mins'\nsns.distplot(df_train.Balance)\n\n# Display the plot\nplt.show()","1208ce3a":"# Visualize the distribution of 'Day_Mins'\nsns.distplot(df_train.EstimatedSalary)\n\n# Display the plot\nplt.show()","e1ff7c6f":"sns.boxplot(x = 'Exited',\n           y = 'Balance',\n           data = df_train)\nplt.show()","4275079d":"sns.boxplot(x = 'Exited',\n           y = 'Balance',\n           data = df_train,\n           hue = 'Geography')\nplt.show()","37101a43":"sns.boxplot(x = 'Exited',\n           y = 'CreditScore',\n           data = df_train)\nplt.show()","60d29a30":"# Printing the filtered dataframe to verify to verify\ndf_train[df_train['CreditScore'] < 400]","d90a0930":"sns.boxplot(x = 'Exited',\n           y = 'Age',\n           data = df_train)\nplt.show()","4116dedd":"df_train.head()","91a1c6cb":"correlations = df_train.corr()\nfig = plt.figure(figsize = (9, 6))\n\nsns.heatmap(correlations, vmax = .8, square = True)\nplt.show()","6bff0e79":"k = 10 #number of variables for heatmap\ncols = correlations.nlargest(k, 'Exited')['Exited'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(15, 15))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","0b0e1e80":"# Drop the unnecessary features\ndf_train = df_train.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1)","223551f6":"# Replacing 'Male' with 0 and 'Female' with 1 \ndf_train['Gender'] = df_train['Gender'].replace({'Male': 0, 'Female': 1})\n\n# Print the results to verify\nprint(df_train['Gender'].head())","0b959605":"# Perform one hot encoding on 'Geography'\ndf_train = pd.get_dummies(data=df_train, columns=['Geography', 'Tenure', 'NumOfProducts'])","2c34be76":"# Create feature variables\nX = df_train.drop('Exited', axis=1)\n\n# Create target variable\ny = df_train['Exited']\n\n# Create training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y)","04f0064f":"# Import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instantiate the classifier\nclf = RandomForestClassifier(class_weight='balanced_subsample', random_state=123)\n\n# Fit to the training data\nclf.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = clf.predict(X_test)\n\n# Import roc_auc_score\nfrom sklearn.metrics import roc_auc_score\n\n# Compute accuracy\n#print(clf.score(X_test, y_test))\n\n# Generate the probabilities\ny_pred_prob = clf.predict_proba(X_test)[:, 1]\n\n# Print the AUC\nprint(roc_auc_score(y_test, y_pred_prob))\n\nimport scikitplot as skplt\nskplt.metrics.plot_confusion_matrix(y_test, y_pred)","2a436674":"from xgboost import XGBClassifier\n# Instantiate the classifier\nclf = XGBClassifier(random_state=123)\n\n# Fit to the training data\nclf.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = clf.predict(X_test)\n\n# Generate the probabilities\ny_pred_prob = clf.predict_proba(X_test)[:, 1]\n\n# Print the AUC\nprint(roc_auc_score(y_test, y_pred_prob))\n\nskplt.metrics.plot_confusion_matrix(y_test, y_pred)","58f158a4":"from lightgbm import LGBMClassifier\n# Instantiate the classifier\nclf = LGBMClassifier(random_state=123)\n\n# Fit to the training data\nclf.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = clf.predict(X_test)\n\n# Generate the probabilities\ny_pred_prob = clf.predict_proba(X_test)[:, 1]\n\n# Print the AUC\nprint(roc_auc_score(y_test, y_pred_prob))\n\nskplt.metrics.plot_confusion_matrix(y_test, y_pred)","b92c0334":"from catboost import CatBoostClassifier\n# Instantiate the classifier\nclf = CatBoostClassifier(random_state=123, logging_level='Silent')\n\n# Fit to the training data\nclf.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = clf.predict(X_test)\n\n# Generate the probabilities\ny_pred_prob = clf.predict_proba(X_test)[:, 1]\n\n# Print the AUC\nprint(roc_auc_score(y_test, y_pred_prob))\n\nskplt.metrics.plot_confusion_matrix(y_test, y_pred)","dabdfddb":"clf = CatBoostClassifier()\nparams = {'iterations': [500],\n          'depth': [4, 5, 6, 7],\n          'loss_function': ['Logloss', 'CrossEntropy'],\n          'l2_leaf_reg': np.logspace(-20, -19, 3),\n          'eval_metric': ['AUC'],\n#           'use_best_model': ['True'],\n          'logging_level':['Silent'],\n          'random_seed': [42]\n         }\n#scorer = make_scorer(accuracy_score)\nclf_grid = GridSearchCV(estimator=clf, param_grid=params, cv=5)","f3180d1f":"clf_grid.fit(X_train, y_train)\nbest_param = clf_grid.best_params_\nbest_param","bc255066":"from sklearn.metrics import classification_report","f9592d34":"y_pred = clf_grid.predict(X_test)\n\n# Generate the probabilities\ny_pred_prob = clf_grid.predict_proba(X_test)[:, 1]\n\n# Print the AUC\nprint(roc_auc_score(y_test, y_pred_prob))\n\n# Generate the probabilities\ny_pred_prob = clf_grid.predict_proba(X_test)[:, 1]\n\n# Print the AUC\nprint('The AUC score using GridSearchCV is :\\n', roc_auc_score(y_test, y_pred_prob))\n\n# Obtain the results from the classification report and confusion matrix \nprint(skplt.metrics.plot_confusion_matrix(y_test, y_pred))\ntarget_names = ['No Churn', 'Churn']\nprint(classification_report(y_test, y_pred, target_names=target_names))","780be482":"# This is the pipeline module we need for this from imblearn\nfrom imblearn.pipeline import Pipeline \nfrom imblearn.over_sampling import SMOTE\n\n# Define which resampling method and which ML model to use in the pipeline\nresampling = SMOTE()\nmodel = CatBoostClassifier(logging_level = 'Silent')\n\n# Define the pipeline, tell it to combine SMOTE with the CatBoost model\npipeline = Pipeline([('SMOTE', resampling), ('CatBoost ', model)])","03f4e59a":"# Split your data X and y, into a training and a test set and fit the pipeline onto the training data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n# Fit your pipeline onto your training set and obtain predictions by fitting the model onto the test data \npipeline.fit(X_train, y_train) \npredicted = pipeline.predict(X_test)\n\n# Generate the probabilities\ny_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n\n# Print the AUC\nprint('The AUC score using SMOTE is :\\n', roc_auc_score(y_test, y_pred_prob))\n\n# Obtain the results from the classification report and confusion matrix \nconf_mat = skplt.metrics.plot_confusion_matrix(y_test, predicted)\nprint('Confusion matrix:\\n', conf_mat)\nprint(classification_report(y_test, predicted, target_names=target_names))","de760a9c":"# Import the package\nfrom sklearn.ensemble import VotingClassifier\n\n# Define the three classifiers to use in the ensemble\nclf1 = LogisticRegression(class_weight={0:1, 1:15}, random_state=5)\nclf2 = RandomForestClassifier(class_weight={0:1, 1:12}, criterion='gini', max_depth=8, max_features='log2',\n            min_samples_leaf=10, n_estimators=30, n_jobs=-1, random_state=5)\nclf3 = DecisionTreeClassifier(random_state=5, class_weight=\"balanced\")\n\n# Combine the classifiers in the ensemble model\nensemble_model = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('dt', clf3)], voting='hard')","6e81b171":"#fitting the ensemble model onto the training set\nensemble_model.fit(X_train, y_train)\n\n#making predictions\ny_pred = ensemble_model.predict(X_test)\n\n# Obtain the results from the classification report and confusion matrix \nconf_mat = skplt.metrics.plot_confusion_matrix(y_test, y_pred)\nprint('Confusion matrix:\\n', conf_mat)\nprint(classification_report(y_test, y_pred, target_names=target_names))","eafd5564":"The data is imbalanced i.e. we have way more non-churners (7963) than churners (2037). This can bias the model's performance.","9da5179d":"The best model is the **classifier** with an auc score of 0.87. To improve the model, I used a grid search cross-validation to identify the hyperparameters that lead to optimal model performance.","915fa713":"There seems to be more young clients staying than old clients. Which might be a hint that the bank doesn't offer good retirement plans","45cacd04":"#### Summary statistics for both classes","963cadca":"#### Creating training and test sets","48807b32":"##### XGBoost","f79bae05":"Half of the clients are from France while the remaining reside in Germany and Spain.","9d23fdbc":"## Comments <a id=\"7\"><\/a>","fa9f598c":"#### Churn by Country","1c79fe31":"# Predicting Churn for Bank Customers challenge\n\nIn this challenge we are going to predict whether a customer will leave the bank or not using machine learning algorithms.\n\n## Table of Contents:\n&nbsp;&nbsp;1. [Exploratory Data Analysis](#1)\n   \n&nbsp;&nbsp;2. [Preprocessing for Churn Modeling](#2)   \n\n&nbsp;&nbsp;3. [Churn Prediction](#3)   \n\n&nbsp;&nbsp;4. [Model Tuning](#4)\n\n&nbsp;&nbsp;5. [Data Resampling](#5)\n\n&nbsp;&nbsp;6. [Stacking Ensemble Method](#6)\n\n&nbsp;&nbsp;7. [Comments](#7)\n\n&nbsp;&nbsp;8. [Conclusion](#8)","7a7865aa":"##### CatBoost","28a44d0b":"## 1. Exploratory Data Analysis (EDA) <a id=\"1\"><\/a>","95aae6b8":"Moreover, we will look at the precision and recall of the best performing models to decide which model to use depending on the bank's needs.","2350971b":"There is a high risk to churn among middle aged clients (42-65)","38dc5d72":"## 2. Preprocessing for Churn Modeling <a id=\"2\"><\/a>","ab3c6cbb":"After using GridSearchCV, there is a small improvement.","a6a5d626":"We are going to try out different algorithms and pick the one with the best performance. To measure a model's performance, we will use the Area Under the Curve of the test set. We will also use the confusion matrix to gain a more nuanced understanding of its performance","786bcee8":"There is no gender gap. It is pretty well balanced.","2f171297":"The last method I used is the stacking ensemble method. The SEM is implemented by combining multiple models via a 'voting' rule on the model outcome. The base level models are each trained based on the training set. More often than not, this method produces more accurate predictions than a single model would. ","6214d895":"We split the dataset into two: a training set which will be used to build the churn model, and a test set which will be used to validate the model. Since the distribution of the target variable is uneven, we will use the parameter 'stratify' which will take into account the distribution of the classes ('Churn' or 'No churn').","f8fae7ab":"#### Churn by Age","2316af60":"#### Churn by Tenure","fcd88428":"None of the clients in Germany have 0 in their account and they also tend to have more cash in their accounts compared to other clients from the other 2 countries.","7738585c":"#### Exploring feature distributions","7e6e6b65":"Three different techniques were used when fitting the models. The first method consisted of fitting different algorithms, picking the best performing one, and then using GridSearchCV to tune the hyperparameters of that model. The second method used the Synthetic Minority Oversampling Technique to balance the dataset. Lastly, we used the stacking ensemble method which combines a set of models.\n\nTo decide which model is appropriate mainly depends on what the bank wants. If they mostly care about finding churners, and not so much about the false positives, then the ensemble method would work best. But if the goal is to find non-churners, then the other two models would be appropriate.\n\nLastly, the first two models did well in capturing non-churners. They had very few false positives but they didn't do well in capturing churners. This can be caused by the lack of clear indicators of churners. A way to improve the models would be to include more features like the number of customer service calls the customer made, marital status, employment status, etc.","d22ff82b":"![Annotation%202020-03-10%20185251.png](attachment:Annotation%202020-03-10%20185251.png)","8ba15a37":"## 3. Churn Prediction <a id=\"3\"><\/a>","2af9e7aa":"To implement the ensemble, I used the **voting** classifier. ","1071fc82":"##### lightgbm","f4b76f67":"## 4. Model Tuning <a id=\"4\"><\/a>","615c9838":"![Annotation%202020-03-10%20181639.png](attachment:Annotation%202020-03-10%20181639.png)","0fa6ef18":"## Conclusion <a id=\"8\"><\/a>","3dc437e2":"Features such as 'RowNumber', 'CustomerId', 'Surname' are not useful when it comes to predicting customer churn, and they need to be dropped prior to modeling. ","f9490cc8":"Both female and male prefer the same products (1 & 2).","9733e551":"#### Product Number by Gender","7b9932f8":"There are more women leaving the bank than men.","9cb2ab88":"All users of product 4 have churned, an indication that it is not a good product!!\nAlso most users of product 3 end up leaving too.","737d7b05":"The first model we'll look at is:","f90b85bf":"There is a problem in germany. Almost half of the clients in Germany end up leaving.","e7271337":"![Annotation%202020-03-07%20225710.png](attachment:Annotation%202020-03-07%20225710.png)","11326d46":"![Annotation%202020-03-10%20180015.png](attachment:Annotation%202020-03-10%20180015.png)","3eacebb5":"## Data Resampling <a id=\"5\"><\/a>","c8888a62":"#### One hot encoding the 'Geography', 'Tenure', and 'NumOfProducts' columns since they have more two unique outcome.","09dbbaf6":"#### Correlation matrix","fefb8896":"## Stacking Ensemble Method (SEM) <a id=\"6\"><\/a>","ac3bd9e0":"There is no difference in balance between the two classes","3fd7d792":"#### Churn by Gender","cabbe34f":"A few observations can be made from the table above:\n    - They youngest client(s) is 18 years old while the oldest is 92.\n    - There are no negative balance in the dataset. The least cash anyone has is 0.\n    - The columns 'HasCrCard' and 'IsActiveMember' can only take two possible values, 0 or 1.","c667ce25":"#### Encoding binary features","3fa508ad":"#### Differences in Balance","7b912f14":"![precision_and_recall.png](attachment:precision_and_recall.png)","541f74ad":"##### Author: Juste Nyirimana","e4edc37b":"Senior citizens are less likely to leave but also half of them have 0 in their account.","173ce03c":"It is also important to note that each resampling technique has its own drawbacks.","5bcdce0e":"#### Statistics between Churners and non churners","27d81319":"The most used product is 'product 1' while the least used one is 'product 4'","48e27a94":"#### Churn by Product Number","bcabf8e1":"There is no real difference in credit score between churners and non churners except that all the Clients with a credit score lower than 400 churned.","8e579c77":"#### Random Forest","d8af8989":"#### Dropping unnecessary features","d0ff42e9":"The aim of doing an Exploratory Data Analysis (EDA) is to understand the data we have which in turn will help us to build a good solution.","98070129":"The following are the few observations and comments I made during this analysis:\n1. Most users of product 1 and 2 end up churning. This is a sign that something is wrong with them. In cases like this, a market research would be helpful in finding out what is wrong with the two products which can lead to an implementation of better products that customers would enjoy and end up not churning\/leaving the bank.\n\n2. The churn rate among women is high. A possible explanation to this is that there is a mismatch between the financial products and services offered by the bank and the needs of women clients.","225f4ec0":"Great news!!! There are no missing values.","c47247f4":"The clients with the highest account balances churned. Not good.","a1b1d7ba":"I used another approach called resampling. The aim of resampling is to have a perfect balance between churners and non-churners. There are different [techniques](https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets) used to resample an imbalanced dataset. In this case I used and oversampling technique called the **Synthetic Minority Oversampling Technique** (SMOTE). SMOTE uses characteristics of the nearest neighbors of the churn cases (the minority class) to create new synthetic churn cases.","71018590":"More than 70% of the clients have\/own a credit card."}}