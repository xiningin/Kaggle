{"cell_type":{"c43cdbfd":"code","110a9a12":"code","e610576d":"code","379baa38":"code","4cb3f79c":"code","8225c842":"code","dc8af35e":"code","efae8eb4":"code","5ce676ce":"code","647f660e":"code","e9ea5d6e":"code","9143ee75":"code","3c34f5fb":"code","a2e891b8":"code","7a15cfe6":"code","c96fe784":"code","f8d88bbd":"code","bc0922cd":"code","bd51d3cb":"code","8f0d9106":"code","e815d9d6":"code","f71cdff1":"code","0107ac43":"code","03c8d64e":"code","b1dd0578":"code","b7407961":"code","ca6ed619":"code","eab17c72":"code","ba7c6717":"code","a2e36d47":"code","a50b46a7":"code","eb84acd1":"code","8533faa5":"code","ec3a3c86":"code","e185811e":"code","65c9e56a":"code","7634e26f":"code","2465c6a8":"code","b88d724a":"code","9c2d769e":"code","2fbe10e8":"code","ef4d4eec":"code","4541ae5f":"code","35107aa1":"code","76c75312":"markdown","6d92fcfa":"markdown","ae0648d3":"markdown","76300791":"markdown","fa92208e":"markdown","8492b2b8":"markdown","4cb9ea74":"markdown","e4ceda19":"markdown","1a222552":"markdown","e45264db":"markdown","d937a77f":"markdown","443c2fa0":"markdown","c8a1d4ce":"markdown"},"source":{"c43cdbfd":"import sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\n\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows',None)","110a9a12":"train = pd.read_csv(\"..\/input\/human-activity-recognition-with-smartphones\/train.csv\")\ntest = pd.read_csv(\"..\/input\/human-activity-recognition-with-smartphones\/test.csv\")","e610576d":"train.shape, test.shape","379baa38":"train.head()","4cb3f79c":"train.describe().transpose()","8225c842":"print('Total number of missing values in train : ', train.isna().values.sum())\nprint('Total number of missing values in test : ', test.isna().values.sum())","dc8af35e":"train['subject'].unique()","efae8eb4":"train['Activity'].unique()","5ce676ce":"pd.crosstab(train.subject, train.Activity, margins=True)","647f660e":"chart = sns.countplot(x=train['Activity'])\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')","e9ea5d6e":"train.Activity.value_counts()","9143ee75":"facetgrid = sns.FacetGrid(train, hue='Activity', height=5, aspect=3)\nfacetgrid.map(sns.distplot,'tBodyAccMag-mean()', hist=False).add_legend()","3c34f5fb":"facetgrid = sns.FacetGrid(train, hue='Activity', height=5,aspect=3)\nfacetgrid.map(sns.distplot,'tBodyGyroMag-mean()', hist=False).add_legend()","a2e891b8":"fig = plt.figure(figsize=(32,24))\nax1 = fig.add_subplot(221)\nax1 = sns.stripplot(x='Activity', y='tBodyAcc-max()-X', data=train.loc[train['subject']==1], jitter=True)\nax2 = fig.add_subplot(222)\nax2 = sns.stripplot(x='Activity', y='tBodyAcc-max()-Y', data=train.loc[train['subject']==1], jitter=True)\nplt.show()","7a15cfe6":"plt.figure(figsize=(10,7))\nsns.boxplot(x='Activity', y='angle(X,gravityMean)', data=train)\nplt.ylabel(\"Angle between X-axis and gravityMean\")\nplt.title('Box plot of angle(X,gravityMean) column across various activities')\nplt.xticks(rotation = 90)","c96fe784":"plt.figure(figsize=(10,7))\nsns.boxplot(x='Activity', y='angle(Y,gravityMean)', data = train)\nplt.ylabel(\"Angle between Y-axis and gravityMean\")\nplt.title('Box plot of angle(Y,gravityMean) column across various activities')\nplt.xticks(rotation = 90)","f8d88bbd":"train.drop(['subject'], axis=1, inplace=True)\ntest.drop(['subject'], axis=1, inplace=True)","bc0922cd":"X_train = train.iloc[:,:-1]\ny_train = train.iloc[:,-1]\n\nX_test = test.iloc[:,:-1]\ny_test = test.iloc[:,-1]","bd51d3cb":"encoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train)\ny_train = pd.get_dummies(y_train).values\n\ny_test = encoder.fit_transform(y_test)\ny_test = pd.get_dummies(y_test).values","8f0d9106":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","e815d9d6":"scaler=MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f71cdff1":"model = Sequential()\nmodel.add(Dense(units=64, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=6, activation='softmax'))\n\nmodel.compile(optimizer=Adam(lr=0.001), metrics=['accuracy'], loss='categorical_crossentropy')\nprint(model.summary())","0107ac43":"history=model.fit(X_train, y_train, batch_size=256, epochs=20, validation_data=(X_test, y_test), shuffle=True)","03c8d64e":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show","b1dd0578":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show","b7407961":"y_pred = model.predict(X_test)","ca6ed619":"pred = np.argmax(y_pred,axis = 1) \ny_actual = np.argmax(y_test,axis = 1)","eab17c72":"confusion_matrix(y_actual, pred)","ba7c6717":"print(classification_report(y_actual, pred))","a2e36d47":"results = pd.DataFrame({'Actual':y_actual, 'Predicted':pred})\nresults.iloc[:75,:]","a50b46a7":"tf.reset_default_graph()","eb84acd1":"n_input = 561\nn_hidden1 = 64\nn_hidden2 = 128\nn_hidden3 = 32\nn_output = 6","8533faa5":"x = tf.placeholder(tf.float32, shape=[None, 561])\ny = tf.placeholder(tf.float32, shape=[None, 6])\nhold_prob = tf.placeholder(tf.float32)","ec3a3c86":"def init_weights(shape):\n    w = tf.truncated_normal(shape=shape, stddev=0.1)\n    return tf.Variable(w)","e185811e":"def init_bias(shape):\n    b = tf.constant(0.1, shape=shape)\n    return tf.Variable(b)","65c9e56a":"def next_batch(j, batch_size):\n    x = X_train[j:j+batch_size]\n    y = y_train[j:j+batch_size]\n    j = (j+batch_size)%len(X_train)\n    return x,y,j","7634e26f":"hidden1 = {'weights':init_weights([n_input,n_hidden1]), 'bias':init_bias([n_hidden1])}\nhidden2 = {'weights':init_weights([n_hidden1, n_hidden2]), 'bias':init_bias([n_hidden2])}\nhidden3 = {'weights':init_weights([n_hidden2, n_hidden3]), 'bias':init_bias([n_hidden3])}\noutput = {'weights':init_weights([n_hidden3, n_output]), 'bias':init_bias([n_output])}","2465c6a8":"def feed_forward(x):\n    h1 = tf.add(tf.matmul(x,hidden1['weights']), hidden1['bias'])\n    h1 = tf.nn.relu(h1)\n    \n    h2 = tf.add(tf.matmul(h1,hidden2['weights']), hidden2['bias'])\n    h2 = tf.nn.relu(h2)\n    \n    h3 = tf.add(tf.matmul(h2,hidden3['weights']), hidden3['bias'])\n    h3 = tf.nn.relu(h3)\n    \n    dropout = tf.nn.dropout(h3, hold_prob)\n    \n    out = tf.matmul(dropout,output['weights']) +output['bias']\n    out = tf.nn.softmax(out)\n    \n    return out","b88d724a":"y_pred = feed_forward(x)","9c2d769e":"cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_pred))","2fbe10e8":"optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\ntrain = optimizer.minimize(cross_entropy)","ef4d4eec":"init = tf.global_variables_initializer()","4541ae5f":"epochs = 5000\nj = 0\nbatch_size = 256\ntrain_acc = []\ntest_acc = []\nwith tf.Session() as sess:\n    sess.run(init)\n    for i in range(epochs):\n        x_batch, y_batch, j = next_batch(j, batch_size)\n        sess.run(train, feed_dict={x:x_batch, y:y_batch, hold_prob:0.5})\n        \n        if (i % 100 == 0 and i != 0):\n            print('Epoch', i, 'completed out of', epochs)\n            correct = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n            trainacc = sess.run(accuracy, feed_dict = {x: x_batch, y: y_batch, hold_prob:0.5})\n            print('Train set Accuracy:', trainacc)\n            train_acc.append(trainacc)\n            testacc = sess.run(accuracy, feed_dict = {x: X_test, y: y_test, hold_prob:1.0})\n            print('Test set Accuracy:', testacc)\n            test_acc.append(testacc)\n            print()","35107aa1":"plt.plot(train_acc)\nplt.plot(test_acc)\nplt.legend(['train','test'])","76c75312":"# Load data","6d92fcfa":"Passive activities fall mostly below the active ones. It makes sense that maximum acceleration is higher during the walking activities. ","ae0648d3":"if(tBodyAccMag-mean()<=-0.5):\n    Activity = \"static\"\nelse:\n    Activity = \"dynamic\"\n    \nif(tBodyGyroMag-mean()<=-0.4):\n    Activity = \"static\"\nelse:\n    Activity = \"dynamic\"","76300791":"There are almost the same number of observations per activity and hence the data does not suffer from class imbalance problem.","fa92208e":"The above 2 plots seperate the laying activity from the others based on the angle made by gravityMean with X and Y axis respectively.","8492b2b8":"We can classify the activities into 2 categories:\n1. Passive Activities - Standing, Sitting, Laying\n2. Active Activities - Walking, Walking_Downstairs, Walking_Upstairs","4cb9ea74":"The above table shows that the activities are nearly equally distributed among the various subjects.","e4ceda19":"# Data Preprocessing","1a222552":"# Imports","e45264db":"Using the above density plot we can easily come up with a condition to seperate static activities from dynamic activities.","d937a77f":"# Data Exploration and Visualization","443c2fa0":"# Build Model using Tensorflow","c8a1d4ce":"# Build Model using Keras"}}