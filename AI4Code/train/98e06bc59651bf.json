{"cell_type":{"655de773":"code","3da86dcd":"code","7a5f799b":"code","8615e999":"code","41c0ca67":"code","68aea14e":"code","f8cc1702":"code","b1af4948":"code","050c01da":"code","a1b7ffd0":"code","95b9b167":"code","7e8e0eb4":"code","66c86dfb":"code","a1b72eae":"code","d5bdab34":"code","49c4a0c4":"code","1d620f45":"code","ae6a68a4":"code","84c1db69":"markdown","e06835a8":"markdown","973b7dbb":"markdown","06ec54ef":"markdown","45260ca7":"markdown","2af23a17":"markdown","4c386bd1":"markdown","a4a75980":"markdown","555437e9":"markdown","bf533365":"markdown","af941869":"markdown","8a385670":"markdown","02070669":"markdown","42d1383d":"markdown","076e2f83":"markdown","4a61e981":"markdown","e44d06d5":"markdown"},"source":{"655de773":"import numpy as np\nimport pandas as pd\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom boruta import BorutaPy\nimport shap\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nimport gc\nwarnings.filterwarnings(\"ignore\")","3da86dcd":"data = pd.read_csv('..\/input\/train.csv', index_col='id')\ntest = pd.read_csv('..\/input\/test.csv', index_col='id')\ndf = pd.concat([data, test])","7a5f799b":"today = pd.to_datetime('2016-01-01')\n    \n# datetime features\ndf['was_renovated'] = (df['yr_renovated'] != 0).astype('uint8')\nnot_renovated = df[df['was_renovated'] == 0].index\ndf.loc[not_renovated, 'yr_renovated'] = df.loc[not_renovated, 'yr_built']\ndf['date'] = pd.to_datetime(df['date'].str[:8])\ndf['yr_built'] = pd.to_datetime({'year': df['yr_built'], 'month': [1]*len(df), 'day': [1]*len(df)})\ndf['yr_renovated'] = pd.to_datetime({'year': df['yr_renovated'], 'month': [1]*len(df), 'day': [1]*len(df)}, errors='coerce')\ndf['today-D-date'] = (today - df['date']).dt.days\ndf['today-D-yr_renovated'] = (today - df['yr_renovated']).dt.days\ndf['today-D-yr_built'] = (today - df['yr_built']).dt.days\ndf['date-D-yr_built'] = (df['date'] - df['yr_built']).dt.days\ndf['yr_renovated-D-yr_built'] = (df['yr_renovated'] - df['yr_built']).dt.days\ndf = df.drop(['date', 'yr_built', 'yr_renovated'], axis=1)\n\n# feature interactions\ndf['room_count'] = df['bedrooms'] + df['bathrooms']\ndf['sqft_living_per_rooms'] = df['sqft_living'] \/ (df['room_count']+1)\ndf['sqft_lot_per_rooms'] = df['sqft_lot'] \/ (df['room_count']+1)\ndf['room_per_floors'] = df['room_count'] \/ df['floors']\ndf['sqft_living_per_floors'] = df['sqft_living'] \/ df['floors']\ndf['sqft_lot_per_floors'] = df['sqft_lot'] \/ df['floors']\ndf['sqft_living_per_bedrooms'] = df['sqft_living'] \/ (df['bedrooms']+1)\ndf['sqft_lot_per_bedrooms'] = df['sqft_lot'] \/ (df['bedrooms']+1)\ndf['bedroom_per_floors'] = df['bedrooms'] \/ df['floors']\ndf['sqft_lot-D-sqft_living'] = df['sqft_lot'] - df['sqft_living']\ndf['sqft_lot-R-sqft_living'] = df['sqft_lot'] \/ df['sqft_living']\ndf['sqft_living15-D-sqft_living'] = df['sqft_living15'] - df['sqft_living']\ndf['sqft_living15-R-sqft_living'] = df['sqft_living15'] \/ df['sqft_living']\ndf['sqft_lot15-D-sqft_lot'] = df['sqft_lot15'] - df['sqft_lot']\ndf['sqft_lot15-R-sqft_lot'] = df['sqft_lot15'] \/ df['sqft_lot']\ndf['rooms_mul']=df['bedrooms']*df['bathrooms']\ndf['total_score']=df['condition']+df['grade']+df['view']\n\n# binary features\ndf['has_basement'] = (df['sqft_basement']>0).astype('uint8')\ndf['has_attic'] = ((df['floors'] % 1) != 0).astype('uint8')\n\n# one hot encode\ndf['zipcode'] = df['zipcode'].astype('str')\ndf = pd.get_dummies(df)\n\n# log transform target\ndf['price'] = np.log1p(df['price'])\n\ndata = df.loc[data.index]\ntest = df.loc[test.index]\n\ndel df; gc.collect();","8615e999":"data.head()","41c0ca67":"def rmse_expm1(pred, true):\n    return -np.sqrt(np.mean((np.expm1(pred)-np.expm1(true))**2))\n\ndef evaluate(x_data, y_data):\n    model = LGBMRegressor(objective='regression', num_iterations=10**5)\n    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, random_state=0)\n    model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=False)\n    val_pred = model.predict(x_val)\n    score = rmse_expm1(val_pred, y_val)\n    return score\n\ndef rfe(x_data, y_data, method, ratio=0.9, min_feats=40):\n    feats = x_data.columns.tolist()\n    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])\n    while True:\n        model = LGBMRegressor(objective='regression', num_iterations=10**5)\n        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=0)\n        model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=False)\n        val_pred = model.predict(x_val)\n        score = rmse_expm1(val_pred, y_val)\n        n_feats = len(feats)\n        print(n_feats, score)\n        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)\n        if method == 'basic':\n            feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)\n        elif method == 'perm':\n            perm = PermutationImportance(model, random_state=0).fit(x_val, y_val)\n            feat_imp = pd.Series(perm.feature_importances_, index=feats).sort_values(ascending=False)\n        elif method == 'shap':\n            explainer = shap.TreeExplainer(model)\n            shap_values = explainer.shap_values(x_data[feats])\n            feat_imp = pd.Series(np.abs(shap_values).mean(axis=0), index=feats).sort_values(ascending=False)\n        next_n_feats = int(n_feats * ratio)\n        if next_n_feats < min_feats:\n            break\n        else:\n            feats = feat_imp.iloc[:next_n_feats].index.tolist()\n    return archive\n\nfeats = [col for col in data.columns if col != 'price']\nlen(feats)","68aea14e":"model = LGBMRegressor(objective='regression', num_iterations=10**5)\nx_data = data[feats]\ny_data = data['price']\nx_train, x_val, y_train, y_val = train_test_split(x_data, y_data, random_state=0)\nmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=False)\nval_pred = model.predict(x_val)\nscore = rmse_expm1(val_pred, y_val)\nscore","f8cc1702":"%%time\nbasic_archive = rfe(x_data, y_data, 'basic')","b1af4948":"feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)\nfor i in range(40, 90, 5):\n    print(i, evaluate(data[feat_imp.iloc[:i].index], data['price']))","050c01da":"%%time\nperm_archive = rfe(x_data, y_data, 'perm')","a1b7ffd0":"perm = PermutationImportance(model, random_state=0).fit(x_val, y_val)\nperm_feat_imp = pd.Series(perm.feature_importances_, index=feats).sort_values(ascending=False)\neli5.show_weights(perm)","95b9b167":"for i in range(40, 90, 5):\n    print(i, evaluate(data[perm_feat_imp.iloc[:i].index], data['price']))","7e8e0eb4":"%%time\nshap_archive = rfe(x_data, y_data, 'shap')","66c86dfb":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(x_data)\nshap_feat_imp = pd.Series(np.abs(shap_values).mean(axis=0), index=feats).sort_values(ascending=False)\nshap.summary_plot(shap_values, x_data)","a1b72eae":"for i in range(40, 90, 5):\n    print(i, evaluate(data[shap_feat_imp.iloc[:i].index], data['price']))","d5bdab34":"feat_imp_archive = pd.DataFrame(index=feats, columns=['basic', 'perm', 'shap', 'mean'])\nfeat_imp_archive['basic'] = feat_imp.rank(ascending=False)\nfeat_imp_archive['perm'] = perm_feat_imp.rank(ascending=False)\nfeat_imp_archive['shap'] = shap_feat_imp.rank(ascending=False)\nfeat_imp_archive['mean'] = feat_imp_archive[['basic', 'perm', 'shap']].mean(axis=1)\nfeat_imp_archive = feat_imp_archive.sort_values('mean')\nfeat_imp_archive[feat_imp_archive['mean']<20].plot(kind='bar', figsize=(20, 10), title='feature importance rankings');","49c4a0c4":"for i in range(40, 90, 5):\n    print(i, evaluate(data[feat_imp_archive.iloc[:i].index], data['price']))","1d620f45":"%%time\nrf = RandomForestRegressor(n_jobs=-1, n_estimators=200, max_depth=5)\nfeat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=0)\nfeat_selector.fit(data[feats].values, data['price'].values)","ae6a68a4":"evaluate(data[np.array(feats)[feat_selector.support_]], data['price'])","84c1db69":"# Basic Feature Importance","e06835a8":"feature engineering\uc740 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub9e4\uc6b0 \uc911\uc694\ud55c \ub2e8\uacc4\uc785\ub2c8\ub2e4. \uadf8\ub7ec\ub098 feature\uac00 \ub108\ubb34 \ub9ce\uc73c\uba74 overfitting\uc758 \uc704\ud5d8\uc774 \uc788\uace0, \ubaa8\ub378\uc774 \uc774\uc0c1\uc801\uc778 \ubc29\ud5a5\uc73c\ub85c \ud559\uc2b5\uc744 \ud558\uc9c0 \uc54a\uc744 \uc704\ud5d8\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc2e4\uc81c\ub85c\ub3c4 feature\uac00 \ub108\ubb34 \ub9ce\uc740 \uacbd\uc6b0 \uc6d0\ub798 \ubaa8\ub378\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc548\uc88b\uac8c \ub098\uc624\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c feature selection\uc744 \ud574\uc57c \ud569\ub2c8\ub2e4.\n\n\uc800\ub294 feature selection\uc73c\ub85c \ud06c\uac8c \ub450 \uac00\uc9c0 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud574\ubd24\uc2b5\ub2c8\ub2e4. \uccab\uc9f8, feature\uc744 \ud558\ub098\uc529 \ucd94\uac00\ud574\ubcf4\uace0 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uba74 \ucc44\ud0dd\ud558\uace0 \uc131\ub2a5\uc774 \ub098\ube60\uc9c0\uba74 \ubc84\ub9ac\ub294 \ubc29\ubc95. \ub458\uc9f8, \ub3c4\uc6c0\uc774 \ub420 \ub9cc\ud55c feature\ub4e4\uc744 \ubaa8\ub450 \ucd94\uac00\ud574\ubcf8 \ub4a4, feature selection \ud14c\ud06c\ub2c9\ub4e4\uc744 \ud1b5\ud574 \uc911\uc694\ud55c feature\ub9cc \ubf51\uc544\ub0b4\ub294 \ubc29\ubc95. \uc774 \ucee4\ub110\uc5d0\uc11c\ub294 \ub450 \ubc88\uc9f8 \ubc29\ubc95\uc744 \ub2e4\ub904\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \n\n\uc5ec\ub7ec feature\ub4e4 \uc911 \uc77c\ubd80\ub9cc \uc120\ud0dd\ud558\ub294 \ubc29\ubc95\uc740 \ub2f9\uc5f0\ud788 \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ub300\ud55c \uadf8 feature\uc758 \uc911\uc694\ub3c4\uc5d0 \ub530\ub77c\uc11c\uc774\uaca0\uc8e0. feature importance\ub97c \uc5bb\uae30 \uc704\ud574\uc11c\ub294 \uae30\ubcf8\uc801\uc73c\ub85c gbm \ubaa8\ub378\ub4e4\uc5d0\uc11c \uc81c\uacf5\ub418\ub294 feature_importances_ attribute\uc744 \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uaca0\uc9c0\ub9cc, \uc774 \uc678\uc5d0\ub3c4 permutation importance, shap \ub4f1\uc744 \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c \uae30\ubcf8 \ubaa8\ub378\uc5d0 feature importance\ub97c \ud55c\ubc88\ub9cc \uc54c\uc544\ubd10\uc11c top n\uac1c\uc758 feature\ub9cc \uc120\ud0dd\ud558\ub294 \ubc29\ubc95(one-shot)\ub3c4 \uc788\uc744 \uc218 \uc788\uace0, rfe(recurrent feature elimination)\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\uace0 feature\uc744 \uc904\uc5ec\ub098\uac00\ub294 \uac83\uc744 \ubc18\ubcf5\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c boruta\ub77c\ub294 \ubc29\ubc95\uc744 \uc774\uc6a9\ud574 \uc790\ub3d9\uc73c\ub85c feature\uc744 \uc120\ud0dd\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc815\ub9ac\ud558\uc790\uba74 \uc774 \ucee4\ub110\uc5d0\uc11c\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \uc2e4\ud5d8\uc744 \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n1. [\uae30\ubcf8 feature_importances_](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html#lightgbm.LGBMRegressor.feature_importances_)\n    1. one-shot\n    2. rfe\n2. [permutation importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)\n    1. one-shot\n    2. rfe\n3. [shap](https:\/\/github.com\/slundberg\/shap)\n    1. one-shot\n    2. rfe\n4. [boruta](https:\/\/www.kaggle.com\/tilii7\/boruta-feature-elimination)","973b7dbb":"# Combine","06ec54ef":"boruta\ub97c \uc0ac\uc6a9\ud588\ub354\ub2c8 confirm\ub41c feature\ub4e4\uc774 \ub108\ubb34 \uc801\uc5b4\uc11c \uc131\ub2a5\uc774 \uc624\ud788\ub824 \ub5a8\uc5b4\uc9c0\ub124\uc694. \uc81c\uac00 \uc798\ubabb \uc0ac\uc6a9\ud55c \uac83\uc77c\uc218\ub3c4?","45260ca7":"### 1. RFE","2af23a17":"# Boruta","4c386bd1":"# Shap","a4a75980":"# Baseline","555437e9":"# \ub3c4\uc6c0 \ud568\uc218 \uc815\uc758","bf533365":"### 1. RFE","af941869":"### 2. One-shot","8a385670":"# Feature Engineering","02070669":"### 2. One-shot","42d1383d":"# \uacb0\ub860\n\n\uc5ec\ub7ec feature selection \ud14c\ud06c\ub2c9\ub4e4\uc744 \uc54c\uc544\ubd24\uc2b5\ub2c8\ub2e4. \uc774 \uc0c1\ud669\uc5d0\uc11c\ub294 permutation importance with rfe\uc5d0\uc11c n_feats=80\uc77c \ub54c\uac00 -111049\ub85c \uac00\uc7a5 \uc810\uc218\uac00 \ub192\uc558\uc2b5\ub2c8\ub2e4. baseline \uc810\uc218\uc778 -114196\ubcf4\ub2e4 3000\uac00\ub7c9 \ud5a5\uc0c1\ud588\ub124\uc694.\n\n\ubb3c\ub860 computational cost\uc758 \ubb38\uc81c\ub85c KFold\uac00 \uc544\ub2cc \ub2e8\uc21c train-val set\uc73c\ub85c \uac80\uc99d\ud588\uace0, \uac01 \ubaa8\ub378 \ubcc4\ub85c hyperparameter tuning \ub610\ud55c \ud558\uc9c0 \uc54a\uc558\uae30 \ub54c\ubb38\uc5d0 \uc815\ud655\ud55c \ube44\uad50\ub294 \uc544\ub2d9\ub2c8\ub2e4. \uadf8\ub798\ub3c4 \uc548\ud558\ub294 \uac83\ubcf4\ub2e4\ub294 \ub098\uc744 \uac83\uc774\ub77c\uace0 \ubbff\ub294 \uc815\ub3c4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\ub2e4\ub978 \ub370\uc774\ud130\uc14b\uc774\uac70\ub098 \ub2e4\ub978 feature engineering\uc744 \ud588\uc744 \uacbd\uc6b0 \uc774 \uacb0\uacfc\uac12\uc740 \ub610 \ub2ec\ub77c\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\ub7ec \uac00\uc9c0 feature selection \ud14c\ud06c\ub2c9\ub4e4\uc744 \uc0ac\uc6a9\ud574\ubcf4\uace0 \uac00\uc7a5 \uc131\ub2a5\uc774 \uc88b\uc740 feature set\uc744 \uc120\ud0dd\ud558\ub294 \uac83\uc774 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4.","076e2f83":"### 1. RFE","4a61e981":"### 2. One-shot","e44d06d5":"# Permutation Importance"}}