{"cell_type":{"8429748d":"code","adb7fbf7":"code","e517585a":"code","c4dca455":"code","5d113d10":"code","d27147df":"markdown"},"source":{"8429748d":"import pandas as pd\nimport numpy as np\nimport os\nimport glob","adb7fbf7":"ob_dir = '..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/'\ntrade_dir = '..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/'\n\nob_files = ob_dir+'*'\ntrade_files = trade_dir+'*'\nob_files = glob.glob(ob_files)\ntrade_files = glob.glob(trade_files)","e517585a":"parquet_dir = '.\/optiver_parquet'\ntry:\n    os.mkdir(parquet_dir)\nexcept:\n    pass\n\nfor f in ob_files:\n    stock_id = f.split('\/')[-1].split('=')[-1]\n    book_file = '..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id='+str(stock_id)\n    trade_file = '..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id='+str(stock_id)\n    book_df = pd.read_parquet(book_file)\n    trade_df =  pd.read_parquet(trade_file)\n    \n    # Reindex both dataframes:\n    book_df['reindex'] = book_df['time_id'].astype(str) + ':' +book_df['seconds_in_bucket'].astype(str)\n    book_df = book_df.drop(columns=['time_id', 'seconds_in_bucket'])\n    book_df.set_index('reindex', inplace=True)\n    \n    trade_df['reindex'] = trade_df['time_id'].astype(str) + ':' +trade_df['seconds_in_bucket'].astype(str)\n    trade_df = trade_df.drop(columns=['time_id', 'seconds_in_bucket'])\n    trade_df.set_index('reindex', inplace=True)\n    \n    # Now they can be joined:\n    full_data = book_df.join(trade_df)\n    \n    \n    # Reindex:\n    full_data['index_values'] = full_data.index.to_series()\n    full_data['time_id'] = full_data['index_values'].apply(lambda x : x.split(':')[0])\n    full_data['sib'] = full_data['index_values'].apply(lambda x : x.split(':')[1])\n    full_data.reset_index(inplace=True)\n    full_data.drop(columns=['reindex','index_values'], inplace = True)\n    \n    # Fill values that should be zero:\n    full_data['size'].fillna(0, inplace=True)\n    full_data['order_count'].fillna(0, inplace=True)\n    \n    # Fill forward the price and drop missing values:\n    full_data['price'] = full_data.groupby(['time_id'], sort=False)['price'].apply(lambda x: x.ffill())\n    full_data.dropna(inplace = True)\n    \n    '''\n    # Alternatively, fill in two passes, introduces some bias while preserving data:\n    full_data['price'] = full_data.groupby(['time_id'], sort=False)['price'].apply(lambda x: x.ffill().bfill())\n    '''\n    \n    # Save parquet:\n    parquet_name = parquet_dir + '\/' + stock_id + '.parquet'\n    full_data.to_parquet(parquet_name)","c4dca455":"import shutil\nshutil.make_archive('optiver_parquet', 'zip', parquet_dir)","5d113d10":"full_data","d27147df":"Join Order Book and Trade data into a single element, dropping excess Order Book data."}}