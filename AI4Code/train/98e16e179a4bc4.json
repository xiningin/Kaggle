{"cell_type":{"79f5bdba":"code","5383fbbc":"code","e5fbc16c":"code","fc0b4ccd":"code","e4d2e421":"code","482be70a":"code","f3f6d166":"code","905fe75a":"code","b143a6cc":"code","e3f3ccfa":"code","7371b945":"markdown","6c4714d3":"markdown","2bbf2b92":"markdown","460dbb03":"markdown","eb3780d0":"markdown","4a399c04":"markdown"},"source":{"79f5bdba":"# import the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5383fbbc":"# Load the dataset from the csv file using pandas\ntry:\n    data = pd.read_csv('..\/input\/creditcard.csv')\nexcept:\n    print('Sorry, the dataset could not be loaded!')","e5fbc16c":"data.head()","fc0b4ccd":"# Print the shape of the data\ndata = data.sample(frac=0.1, random_state = 1)\nprint('The dataset has {} samples and {} features each'.format(*data.shape))\nprint(data.describe())\n\n# V1 - V28 are the results of a PCA Dimensionality reduction to protect user identities and sensitive features","e4d2e421":"# Plot histograms of each parameter \ndata.hist(figsize=(20,20));","482be70a":"# Determine number of fraud cases in dataset\n\nFraud = data[data['Class'] == 1]\nValid = data[data['Class'] == 0]\n\noutlier_fraction = len(Fraud)\/float(len(Valid))\nprint('The fraction of Fraud classes in the dataset is {}'.format(outlier_fraction))\n\nprint('Fraud Cases: {}'.format(len(data[data['Class'] == 1])))\nprint('Valid Transactions: {}'.format(len(data[data['Class'] == 0])))","f3f6d166":"# Correlation matrix\ncorrmat = data.corr()\nfig = plt.figure(figsize = (12, 9))\n\nsns.heatmap(corrmat, vmax = .8, square = True)\nplt.show()","905fe75a":"# Get all the columns from the dataFrame\ncolumns = data.columns.tolist()\n\n# Filter the columns to remove data we do not want\ncolumns = [c for c in columns if c not in [\"Class\"]]\n\n# Store the variable we'll be predicting on\ntarget = \"Class\"\n\nX = data[columns]\nY = data[target]\n\n# Print shapes\nprint(X.shape)\nprint(Y.shape)","b143a6cc":"from sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# define random states\nstate = 1\n\n# define outlier detection tools to be compared\nclassifiers = {\n    \"Isolation Forest\": IsolationForest(max_samples=len(X),\n                                        contamination=outlier_fraction,\n                                        random_state=state),\n    \"Local Outlier Factor\": LocalOutlierFactor(\n        n_neighbors=20,\n        contamination=outlier_fraction)}","e3f3ccfa":"# Fit the model\nplt.figure(figsize=(9, 7))\nn_outliers = len(Fraud)\n\n\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    \n    # fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X)\n        scores_pred = clf.negative_outlier_factor_\n    else:\n        clf.fit(X)\n        scores_pred = clf.decision_function(X)\n        y_pred = clf.predict(X)\n    \n    # Reshape the prediction values to 0 for valid, 1 for fraud. \n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    \n    n_errors = (y_pred != Y).sum()\n    \n    # Run classification metrics\n    print('{}: {}'.format(clf_name, n_errors))\n    print(accuracy_score(Y, y_pred))\n    print(classification_report(Y, y_pred))\n","7371b945":"#### V11-class has strong positive correlation and v17-class has strong negative correlation... An interesting observation: There isn't a strong correlation between Amount and whether or not it was fraud transaction or time and whether or not it was fraud transaction.","6c4714d3":"#### We will remove the Class column as it is an unsupervised(anomoly detetction) algorithm so we do not fed the labels to the network ahead of time. Split the data with features and target variables as X and Y.****","2bbf2b92":"## Credit Card Fraud Detection ##\nThroughout the financial sector, machine learning algorithms are being developed to detect fraudulent transactions. In this project, that is exactly what we are going to be doing as well. Using a dataset of of nearly 28,500 credit card transactions and multiple unsupervised anomaly detection algorithms, we are going to identify transactions with a high probability of being credit card fraud. In this project, we will build and deploy the following two machine learning algorithms:\n\n* Local Outlier Factor (LOF)\n* Isolation Forest Algorithm\nFurthermore, using metrics suchs as precision, recall, and F1-scores, we will investigate why the classification accuracy for these algorithms can be misleading.\n\nIn addition, we will explore the use of data visualization techniques common in data science, such as parameter histograms and correlation matrices, to gain a better understanding of the underlying distribution of data in our data set. Let's get started!","460dbb03":"### Conclusion:\u00b6\n#### For local outlier method, we got 99.6% accuracy but only 0.2% precision and 0.2% recall. This is because there were many more valid cases than the fraudulent cases and our algorithm did not perform a good job to identify majority of the fraudulent classes.\n#### For Isolation Forest method, we got 99.7% accuracy and about 30% in detecting the outliers or fraud cases(precision and recall) which is better than the local outlier factor.\n#### In order to achieve better results and improve the model performance, we would need to train the model on more number of data samples. Thanks.","eb3780d0":"#### There are very few fraud classes represented by 1 in comparison to the valid transaction classes represented by 0. Now let us calculate the number of fraudulant cases and number of valid cases that we have so we can get an outlier fraction for anomoly detection.","4a399c04":"## Outlier Detection using Unsupervised algorithms\nNow that we have processed our data, we can begin deploying our machine learning algorithms. We will use the following techniques:\n\n#### Local Outlier Factor (LOF)\n\nThe anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood.\n\nThe number of neighbors considered, (parameter n_neighbors) is typically chosen \n* greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and \n* smaller than the maximum number of close by objects that can potentially be local outliers. \n* In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.\n\n#### Isolation Forest Algorithm\n\nThe IsolationForest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. This path length, averaged over a forest of such random trees, is a measure of normality and our decision function. Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\nTypical machine learning methods tend to work better when the patterns they try to learn are balanced, meaning the same amount of good and bad behaviors are present in the dataset.\n\nThe logic argument goes: isolating anomaly observations is easier because only a few conditions are needed to separate those cases from the normal observations. On the other hand, isolating normal observations require more conditions. Therefore, an anomaly score can be calculated as the number of conditions required to separate a given observation.\n\nThe way that the algorithm constructs the separation is by first creating isolation trees, or random decision trees. Then, the score is calculated as the path length to isolate the observation."}}