{"cell_type":{"e64ba3a7":"code","a78a399f":"code","51770d4f":"code","40ffa92e":"code","ad437e8d":"code","9ccc8814":"code","31ac0c76":"code","933fa649":"code","04f77eeb":"code","230c1586":"code","f943ea71":"markdown","b117e4d7":"markdown","3060b52f":"markdown","eb4dca35":"markdown","14f027a5":"markdown","e1332d2b":"markdown","c72304ef":"markdown","ebefcd27":"markdown","5c2c13f3":"markdown","f2391be4":"markdown"},"source":{"e64ba3a7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Activation, Flatten, Dropout, Dense\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\n# Loading the data\n# I'm adding the \".values\" to the test set to convert it to a numpy array\ntrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\").values\n\nprint(train.shape, test.shape)","a78a399f":"# I'm adding the \".values\" to convert them to numpy arrays for easier algebraic manipulation\ntrain_y = train[\"label\"].values\ntrain_x = train.drop(labels = [\"label\"], axis = 1).values","51770d4f":"train_x = train_x.reshape((train_x.shape[0], 28, 28, 1))\ntest = test.reshape((test.shape[0], 28, 28, 1))","40ffa92e":"train_x = train_x.astype(\"float32\") \/ 255.0\ntest = test.astype(\"float32\") \/ 255.0","ad437e8d":"# We one-hot encode the trainning labels\ntrain_y = keras.utils.to_categorical(train_y, 10)","9ccc8814":"# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, test_size = 0.1)","31ac0c76":"# We initialize the Keras model as a Sequential model\nmodel = Sequential()\n\n# The input layer is a convolutional layer with 32 filters\n# The shape of the kernel in this layer is 3x3\n# We add padding in this layer (so we can start the kernel right at the beginning of the image)\n# and in this case we use padding \"same\" for it to add values to the padding that are copied from the original matrix (it could also be 0)\nmodel.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(28, 28, 1)))\n\n# For this layer we add a ReLU activation\n# We need to add ReLU because a convolution is still a linear transformation\n# so we add ReLU for it to be a non linear transformation\nmodel.add(Activation(\"relu\"))\n\n# We add batch normalization here\n# This normalizes the output from the previous layer in order\n# for the input of the next layer to be normalized\n# In this case we put the channels at the end so we don't need to specify the axis of normalization\n# otherwise we would need to specify\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(32, (3, 3), padding=\"same\", activation = \"relu\", input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\n\n# In this layer we Pool the layer before in order to reduce the number of features\n# Since we are using a 2x2 pooling size we are keeping only half of the features in each dimension\n# So instead of a 28*28 vector we now have a 14*14 tensor\n# Since we are omitting the stride Keras assumes the same stride as pool size which is what we want\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# We add a dropout layer of 25% dropout for regularization\nmodel.add(Dropout(0.25))\n\n# We add another convolution layer, in this case we don't need to specify the input shape\n# because keras finds out the right input shape\nmodel.add(Conv2D(64, (3, 3), padding=\"same\", activation = \"relu\"))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3, 3), padding=\"same\", activation = \"relu\"))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3, 3), padding=\"same\", activation = \"relu\"))\nmodel.add(BatchNormalization())\n\n# After this pooling we have a 7*7 tensor\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))","933fa649":"# We add a Flatten layer in order to transform the input tensor into a vector\n# In this case we had a 7*7*64 (7*7*the number of filters we have)\nmodel.add(Flatten())\n\n# We have 512 neurons in this layer\nmodel.add(Dense(512, activation = \"relu\"))\n\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","04f77eeb":"# defining the learning rate, the number of epochs and the batch size\nINIT_LR = 0.001\nNUM_EPOCHS = 30\nBS = 86\nopt = RMSprop(lr = INIT_LR, rho=0.9, epsilon=1e-08, decay=0.0)\n\n# We track the metrics \"accuracy\"\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n\n# Reduce the learning rate by half if validation accuracy has not increased in the last 3 epochs\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n\nfitted_network = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=BS, epochs=NUM_EPOCHS, callbacks=[learning_rate_reduction])","230c1586":"# predict results\nresults = model.predict(test)\n\n# now we want to retrieve the index that had the higher probability, that will be our prediction\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results, name = \"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist.csv\",index = False)","f943ea71":"We'll also split our training data into 2, one for training another for validating","b117e4d7":"Now we define a simple feed forward neural network","3060b52f":"Great! We have our neural network structured out! All we need now is to compile it, define a gradient descent optimizer and run it!\n\nFor the optimizer I choose just a the RMSProp gradient descent optimizer, but there's many other to choose from","eb4dca35":"Conv2D layers (keras CNNs) assume our data comes in three dimensions: height, width and channel. Each picture in is 28x28 pixels (that's why we have 784 feature values) and we have just one channel value which is black and white\n\nWe need to reshape our datasets to match these required settings","14f027a5":"And now we're ready to start building our model!\n\nWe'll define first the CNN part of our network, then well flatten it to conect to our fully conected feed-forward neural network","e1332d2b":"Now that our network is trained, we just predict the training set and store everything the right way for submission!","c72304ef":"we need to split our training set into features and labels to facilitate our process, this is not needed in the test set as we can see that it has one less feature (the label)","ebefcd27":"Now that that's done we need to standerdize our data (which as of right now ranges from rgb values 0-254) so that every value ranges between 0 and 1","5c2c13f3":"Next we need to one-hot encode our labels. Our labels are simply the number the 784 pixels are representing (0 through 9) that means that in the last layer of our feed-forward neural network we will need 9 neurons to specify the probability of the number being 0, 1, 2 etc. Because in the training process of our network we need to compare this result the neural network gives us with the label, we need them to be in the exact same format in order to have a proper comparison, so we have to transform each number in our labels to an array of length 10 where we put a 1 in the index of the correct number, and zeros everywhere else. It's best to show by example:\n\n0 -> [**1**, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n<br\/>\n1 -> [0, **1**, 0, 0, 0, 0, 0, 0, 0, 0]\n<br\/>\n2 -> [0, 0, **1**, 0, 0, 0, 0, 0, 0, 0]\n<br\/>\n3 -> [0, 0, 0, **1**, 0, 0, 0, 0, 0, 0]\n<br\/>\n...","f2391be4":"# Image Recognition with CNNs (keras)\n\nThis notebook serves as a quick demonstration of a simple and accurate keras CNN connected to a feed-forward neural network for classifying the MNIST dataset\n\nFirst things first we need to import the needed libraries and load our data up!"}}