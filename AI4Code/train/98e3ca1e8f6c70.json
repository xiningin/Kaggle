{"cell_type":{"a8efa7cc":"code","925e7fb8":"code","973dcbc1":"code","c888cf72":"code","1899a087":"code","8313aee9":"code","f448be5e":"code","683861ff":"markdown","16868f5d":"markdown","4649b83e":"markdown","7f6b4190":"markdown","959c7f0d":"markdown","92b631d4":"markdown","6798a3c9":"markdown","3a501c07":"markdown","499c982c":"markdown","d69e9c4c":"markdown","6615c9da":"markdown","84d248c3":"markdown"},"source":{"a8efa7cc":"import numpy as np\nimport pandas as pd\nfrom matplotlib import style\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nplt.style.use(\"seaborn\")\npd.set_option(\"display.max_columns\", None)\n\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsub_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")","925e7fb8":"train_df.drop(\"Id\", axis=1, inplace=True)\ntest_df.drop(\"Id\", axis=1, inplace=True)\n\nidx = train_df[train_df[\"Cover_Type\"] == 5].index\ntrain_df.drop(idx, axis=0, inplace=True)","973dcbc1":"cols = [\n    \"Elevation\",\n    \"Aspect\",\n    \"Slope\",\n    \"Horizontal_Distance_To_Hydrology\",\n    \"Vertical_Distance_To_Hydrology\",\n    \"Horizontal_Distance_To_Roadways\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"Horizontal_Distance_To_Fire_Points\"\n]","c888cf72":"from sklearn.feature_selection import f_classif, SelectKBest\n\n\nselector = SelectKBest(f_classif, k=\"all\")\nselector.fit(\n    train_df[cols],\n    train_df[\"Cover_Type\"]\n)\n\npd.Series(selector.scores_, index=cols).nlargest(len(cols)).plot(kind=\"bar\", figsize=(20, 5))","1899a087":"# This code is borrowed from -> https:\/\/www.kaggle.com\/thalesgaluchi\/histograms-for-a-good-prediction\nfig, ax = plt.subplots(figsize=(20, 5))\nfor item in train_df[\"Cover_Type\"].unique():\n    ax.hist(train_df[train_df.Cover_Type.isin([item])][\"Elevation\"], alpha=0.5, label='Cover'+str(item), bins= 100)\n    ax.set_title(\"Elevation\")\n\nax.legend()","8313aee9":"# Modelling using only elevation\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split\n\n\nclf = DecisionTreeClassifier(max_depth=2)\nclf.fit(\n    train_df[[\"Elevation\"]],\n    train_df[\"Cover_Type\"]\n)\n\n_, ax = plt.subplots(figsize=(20, 10))\n\nplot_tree(\n    clf,\n    feature_names=[\"Elevation\"],\n    class_names=train_df[\"Cover_Type\"].unique().astype(str),\n    filled=True,\n    ax=ax,\n    fontsize=13\n)\n\nplt.show()","f448be5e":"def predict(x):\n    if x <= 2503:\n        return 3\n    elif x <= 3053:\n        return 2\n    else:\n        return 1\n\nsub_df[\"Cover_Type\"] = test_df[\"Elevation\"].apply(predict)\nsub_df.to_csv(\"submission.csv\", index=False)\n\nsub_df[\"Cover_Type\"].value_counts()","683861ff":"This submission scored 89.719% = ~90% on public leaderboard.","16868f5d":"In this month's TPS the dataset is quite imbalanced. Also there are very few features that shows good correlation with the target variable. Thus, some simple rules can be easily made on these features to classify the majority classes fairly accurately. In this notebook, we will see how we can come with up some of these rules.","4649b83e":"Due to the low variance of categrical columns, we will consider only continuos columns.","7f6b4190":"Clearly, the **Elevation** feature shows far better correlation with **Cover_Type** compared to other features so we will perform modelling only with this feature.","959c7f0d":"Clearly, the new, slightly more accurate rules to ensure maximum possible seperation of the three classes are:\n1. Elevation <= 2503 -> **Class 3: Ponderosa Pine**\n2. 2503 < Elevation <= 3053 -> **Class 2: Lodgepole Pine**\n3. Elevation > 3053 -> **Class 1: Spruce\/Fir**","92b631d4":"Since the features are continuous and target is categorical, we can use one-way ANOVA to perform further feature elimination.","6798a3c9":"# Part 2: Feature Elimination\n\nSince our goal is to select features that are highly correlated to the target variable, we will try to drop less important feature at each step.","3a501c07":"Clearly, the following **approximate** simple rules can seperate most of the samples:\n1. Elevation < 2500 -> **Class 3: Ponderosa Pine**\n2. 2500 <= Elevation < 3200 -> **Class 2: Lodgepole Pine**\n3. Elevation >= 3200 -> **Class 1: Spruce\/Fir**\n\nHowever, the boundary values - 2500 and 3200 - are approximate and we may misclassify some samples. The boundary values for which the seperation is the maximum possible can be found by maximizing classification accuracy or by minimizing the misclassification cost. We can use a [DecisionTreeClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html) for this purpose for it makes decision rules of the sort we need in this case. Also, we would not want a decision tree to create complex decision boundaries as they may be hard to interpret. Setting **max_depth=2** in the classifier will ensure that we don't create decision boundaries more complex than the already discussed approximate rules.","499c982c":"A lot of inspiration from these great notebooks:\n1. [TPS Dec. 2021 \"Covertype\": Simple elevation model](https:\/\/www.kaggle.com\/carlmcbrideellis\/tps-dec-2021-covertype-simple-elevation-model\/comments#1613693) by @carlmcbrideellis.\n2. [Histograms for a good prediction](https:\/\/www.kaggle.com\/thalesgaluchi\/histograms-for-a-good-prediction) by @thalesgaluchi\n\nThanks a lot for sharing. I learned a great deal from your notebooks :)","d69e9c4c":"Dropping **Id** column from both train and test datasets. Also dropping **Cover_Type=5** as there is only a single row corresponding to it.","6615c9da":"# Part 1: Preprocessing","84d248c3":"# Part 3: White-box modelling"}}