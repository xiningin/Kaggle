{"cell_type":{"aae7f890":"code","17e5a8bb":"code","7e121181":"code","30eb29ea":"code","10770374":"code","48870c0d":"code","819c4036":"code","879a0473":"code","60964246":"code","0b83c5f4":"code","6a3ae238":"code","6cc8da03":"code","d9b51108":"code","6e7ed5c6":"code","8df7e7b4":"code","ee003d83":"code","7687de6e":"code","83995c67":"code","96aa8bc1":"code","89734fac":"code","9e96574f":"code","16cea30a":"code","84b0758d":"code","4c09ced3":"code","4d0c0aec":"code","8412109f":"code","1e158d05":"code","330ce253":"code","1ad6d7c2":"code","127ccc61":"code","39808223":"markdown","1c96e594":"markdown","bca1b2dc":"markdown","91524790":"markdown","0e5bebdd":"markdown","6637b53b":"markdown","e78ffe77":"markdown","90dbc431":"markdown","9fad69d9":"markdown","a22b65a0":"markdown","cf7b0025":"markdown","a9173a0c":"markdown","42ffacef":"markdown","63a879be":"markdown","844d3094":"markdown","40b9aa36":"markdown","cdff4de5":"markdown","104b1084":"markdown"},"source":{"aae7f890":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","17e5a8bb":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nfrom wordcloud import WordCloud\nimport re\nimport gensim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score,f1_score","7e121181":"df=pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv')\ndf.head(4)","30eb29ea":"df.info()","10770374":"print('shape of train dataset',df.shape)\ndf.label.value_counts()","48870c0d":"# \nsns.countplot(df.label,)\nplt.xlabel('class label')\nplt.ylabel('number of tweets')\nplt.show()","819c4036":"plt.hist(df[df['label']==1].tweet.str.len(),bins=20,label='class 1')\nplt.legend()\nplt.xlabel('length of tweets')\nplt.ylabel('number of tweets')\nplt.show()\nplt.hist(df[df['label']==0].tweet.str.len(),color='yellow',bins=20,label='class 0')\nplt.legend()\nplt.xlabel('length of tweets')\nplt.ylabel('number of tweets')\nplt.show()","879a0473":"text=df['tweet'].values.tolist()\ntext_test=test['tweet'].values.tolist()\ntext+=text_test\nprint(len(text))","60964246":"import nltk\nstopword=nltk.corpus.stopwords.words('english')\nstopword.remove('not')\nfor index,text_ in enumerate(text):\n    text_=re.sub(r'@[\\w]*','',text_) #Removing Twitter Handles (@user)\n    text_=re.sub(r'http\/S+','',text_) #Removing urls from text \n    text_=re.sub(r'[^A-Za-z#]',' ',text_) #Removing Punctuations, Numbers, and Special Characters\n    text_=\" \".join(i.lower() for i in text_.split() if i.lower() not in stopword) #Removing stopword\n    text[index]=text_","0b83c5f4":"#Stemming the word\npt=PorterStemmer()\nwordnet=WordNetLemmatizer()\nfor index,text_ in enumerate(text):\n    text_=\" \".join(pt.stem(i) for i in text_.split())\n    text_=\" \".join(wordnet.lemmatize(i) for i in text_.split())  \n    text[index]=text_\n","6a3ae238":"df['preprocess_tweet']=text[:len(df)]\ndf['length_tweet']=df['preprocess_tweet'].str.len()\ntest['preprocess_tweet']=text[len(df):]\ndf.head()","6cc8da03":"train=df.copy()\ntrain.drop(columns=['id','tweet','preprocess_tweet'],inplace=True)\n","d9b51108":"bow=CountVectorizer( min_df=2, max_features=1000)\nbow.fit(df['preprocess_tweet'])\nbow_df=bow.transform(df['preprocess_tweet']).toarray()\nprint('feature name==',bow.get_feature_names()[:10])\nprint('number of uniqe words',bow_df.shape[1])\nprint('shape',bow_df.shape)\nbow_train=pd.DataFrame(bow_df)\nbow_train['length_tweet']=df['length_tweet']\nbow_train['label']=df['label']\nbow_train.head()","6e7ed5c6":"tfidf=TfidfVectorizer(ngram_range=(1, 2),min_df=2,max_features=1000)\ntfidf.fit(df['preprocess_tweet'])\ntfidf_df=tfidf.transform(df['preprocess_tweet']).toarray()\nprint('number of uniqe words',bow_df.shape[1])\nprint('shape',tfidf_df.shape)\ntfidf_train=pd.DataFrame(tfidf_df)\ntfidf_train['length_tweet']=df['length_tweet']\ntfidf_train['label']=df['label']\ntfidf_train.head()","8df7e7b4":"tokenize=df['preprocess_tweet'].apply(lambda x: x.split())\nw2vec_model=gensim.models.Word2Vec(tokenize,min_count = 1, size = 100, window = 5, sg = 1)\nw2vec_model.train(tokenize,total_examples= len(df['preprocess_tweet']),epochs=20)","ee003d83":"w2vec_model.most_similar('father')","7687de6e":"w2v_words = list(w2vec_model.wv.vocab)\nprint(\"number of words that occured minimum 5 times \",len(w2v_words))\nprint(\"sample words \", w2v_words[0:50])","83995c67":"vector=[]\nfrom tqdm import tqdm\nfor sent in tqdm(tokenize):\n  sent_vec=np.zeros(100)\n  count =0\n  for word in sent: \n    if word in w2v_words:\n      vec = w2vec_model.wv[word]\n      sent_vec += vec \n      count += 1\n  if count != 0:\n    sent_vec \/= count #normalize\n  vector.append(sent_vec)\nprint(len(vector))\nprint(len(vector[0]))        ","96aa8bc1":"#example\nl='father dysfunct selfish drag kid dysfunct'\ncount=0\nvcc=np.zeros(100)\nfor word in l:\n  if word in w2v_words:\n    v=w2vec_model.wv[word]\n    vcc+=v\n    count+=1\nvcc","89734fac":"\nprint('number of uniqe words',len(vector[1]))\nw2v_train=pd.DataFrame(vector)\nw2v_train['length_tweet']=df['length_tweet']\nw2v_train['label']=df['label']\nw2v_train.head()","9e96574f":"major_class_0,major_class_1=bow_train.label.value_counts()\ndf_major=bow_train[bow_train['label']==0]\ndf_minor=bow_train[bow_train['label']==1]\ndf_minor_upsampled = resample(df_minor, \n                                 replace=True,     # sample with replacement\n                                 n_samples=major_class_0)\ndf_bow_upsampled = pd.concat([df_major, df_minor_upsampled])\nprint('shape',df_bow_upsampled.shape)\nsns.countplot(df_bow_upsampled.label)","16cea30a":"major_class_0,major_class_1=tfidf_train.label.value_counts()\ndf_major=tfidf_train[tfidf_train['label']==0]\ndf_minor=tfidf_train[tfidf_train['label']==1]\ndf_minor_upsampled = resample(df_minor, \n                                 replace=True,     # sample with replacement\n                                 n_samples=major_class_0)\ndf_tfidf_upsampled = pd.concat([df_major, df_minor_upsampled])\nprint('shape',df_tfidf_upsampled.shape)\nsns.countplot(df_tfidf_upsampled.label)","84b0758d":"major_class_0,major_class_1=w2v_train.label.value_counts()\ndf_major=w2v_train[w2v_train['label']==0]\ndf_minor=w2v_train[w2v_train['label']==1]\ndf_minor_upsampled = resample(df_minor, \n                                 replace=True,     # sample with replacement\n                                 n_samples=major_class_0)\ndf_w2v_upsampled = pd.concat([df_major, df_minor_upsampled])\nprint('shape',df_w2v_upsampled.shape)\nsns.countplot(df_w2v_upsampled.label)","4c09ced3":"x=df_bow_upsampled.iloc[:,0:-1]\ny=df_bow_upsampled['label']\nx_train_bow,x_test_bow,y_train_bow,y_test_bow=train_test_split(x,y,test_size=0.2)","4d0c0aec":"x=df_tfidf_upsampled.iloc[:,0:-1]\ny=df_tfidf_upsampled['label']\nx_train_tfidf,x_test_tfidf,y_train_tfidf,y_test_tfidf=train_test_split(x,y,test_size=0.2)","8412109f":"x=df_w2v_upsampled.iloc[:,0:-1]\ny=df_w2v_upsampled['label']\nx_train_w2v,x_test_w2v,y_train_w2v,y_test_w2v=train_test_split(x,y,test_size=0.2)","1e158d05":"def f1_score_(y_proba,y_test):\n  proba = y_proba[:,1] >= 0.3\n  proba = proba.astype(np.int) \n  return f1_score( proba,y_test)   \n","330ce253":"#use Bow\nfrom sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(x_train_bow,y_train_bow)\ny_pred=model.predict(x_test_bow)\nacc=accuracy_score(y_pred,y_test_bow)\nprint('Accuracy Score',acc)\naccuracy.append(acc)\ny_proba=model.predict_proba(x_test_bow)\nf1_scor=f1_score_(y_proba,y_test_bow)\nprint('f1 score ',f1_scor)","1ad6d7c2":"#use tfidf\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(x_train_tfidf,y_train_tfidf)\ny_pred=model.predict(x_test_tfidf)\nacc=accuracy_score(y_pred,y_test_tfidf)\nprint('Accuracy Score',acc)\ny_proba=model.predict_proba(x_test_tfidf)\nf1_scor=f1_score_(y_proba,y_test_tfidf)\nprint('f1 score ',f1_scor)","127ccc61":"#use word2vec\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(x_train_w2v,y_train_w2v)\ny_pred=model.predict(x_test_w2v)\nacc=accuracy_score(y_pred,y_test_w2v)\nprint('Accuracy Score',acc)\ny_proba=model.predict_proba(x_test_w2v)\nf1_scor=f1_score_(y_proba,y_test_w2v)\nprint('f1 score ',f1_scor)","39808223":"# Load the libraries","1c96e594":"# Summary\n<table>\n<tr>\n<td colspan=2>BOW<\/td>\n<td colspan=2>TF-IDF<\/td>\n<td colspan=2>WORD2VEC<\/td>\n<\/tr>\n <tr>\n<td>Accuray<\/td>\n<td>f1_score<\/td>\n<td>Accuray<\/td>\n<td>f1_score<\/td>\n<td>Accuray<\/td>\n<td>f1_score<\/td>\n<\/tr>\n<tr>\n<td>0.9733344549125168<\/td>\n<td>0.9462331732661575<\/td>\n<td>0.9733344549125168<\/td>\n<td>0.9494853523357087<\/td>\n<td>0.993606998654105<\/td>\n<td>0.9792162983652345<\/td>\n<\/tr>\n\n<\/table>\n\n","bca1b2dc":"# Featurization","91524790":"#### Table of contents\n\n1. Load the libraries\n1. Load Dataset\n1. EDA\n1. Preprocessing Tweet Text\n1. Featurization\n    1. Bag-of-Words\n    1. TF-IDF\n    1. Word2vec\n1. Resample\n    1. Upsampling BOW\n    2. Upsampling TF-IDF\n    1. Upsampling word2vec\n1. Split Dataset\n1. Model Selection\n    1. KNN\n1. Summary\n","0e5bebdd":"## RandomForest","6637b53b":"# Model Selection","e78ffe77":"### Word2vec\n__size:__ The number of dimensions of the embeddings and the default is 100.\n\n__window:__ The maximum distance between a target word and words around the target word. The default window is 5.\n\n__min_count:__ The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n\n__workers:__ The number of partitions during training and the default workers is 3.\n\n__sg:__ The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.","90dbc431":"### BOW","9fad69d9":"# Resample","a22b65a0":"# Load Dataset","cf7b0025":"## Upsampling BOW","a9173a0c":"#  EDA","42ffacef":"__What is Sentiment Analysis?__\n\nSentiment analysis is a process of identifying an attitude of the author on a topic that is being written about. ","63a879be":"# Split Dataset","844d3094":"## Upsampling  word2vec","40b9aa36":"## Upsampling TF-IDF","cdff4de5":"### TF-IDF Features (Bi-Grams)","104b1084":"#   Preprocessing Tweet Text\n\n1. Removing Twitter Handles (@user)\n2. Removing urls from text \n3. Removing Punctuations, Numbers, and Special Characters\n\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Stemming the word\n8. Lemmatization<br>\n\nAfter which we collect the words used to describe positive and negative reviews"}}