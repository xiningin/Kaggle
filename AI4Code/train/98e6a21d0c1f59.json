{"cell_type":{"1200957d":"code","59f4b39d":"code","72e1acd2":"code","4cc1d9c2":"code","d3e008d5":"code","c86c9dc2":"code","a2be959b":"code","769d3e40":"code","483edf6a":"code","80f44d4d":"code","081b87b1":"code","ea222651":"code","159dff98":"code","95533bde":"markdown","aad1273d":"markdown","d5dc8663":"markdown","f8a3baf1":"markdown","e17e9fa5":"markdown","b86a03d3":"markdown","32202399":"markdown","42c29d48":"markdown","d544a489":"markdown","8b908990":"markdown"},"source":{"1200957d":"!pip install lmfit #installing lmfit needed for fit analysis","59f4b39d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom lmfit.models import ExponentialModel\nfrom lmfit.models import StepModel\n\ndef fit_and_obtain_parameters(x,y,model):\n    \"\"\"\n    Just a wrapper of lmfit fit to then obtain the best fit values. \n    \"\"\"\n    \n    params = model.guess(y, x=x)\n    result = model.fit(y, params, x=x)\n    \n    values = np.array([])\n    std_err = np.array([])\n    \n    for p in result.params:\n        values = np.append(values,result.params[p].value)\n        std_err = np.append(std_err,result.params[p].stderr)\n        \n        \n    return values, std_err, result.redchi\n\n\ndef return_parameters_over_time(x,y,model,position_param=0,min_days=16):\n    \"\"\"\n    A function which performs a fit per day (after min_days days)\n    and returns the value and the standard error of the\n    parameter in position position_param. \n    \"\"\"\n    \n    values_param_per_day =  np.array([])\n    stderr_param_per_day =  np.array([])\n    rchi_per_day = np.array([])\n    \n    for j in range(min_days,len(y)):\n        \n        values, stds, red_chi = fit_and_obtain_parameters(x.head(j),\n                                                 y.head(j),\n                                                 model)\n        \n        values_param_per_day = np.append(values_param_per_day,values[position_param])\n        stderr_param_per_day = np.append(stderr_param_per_day,stds[position_param])\n        rchi_per_day = np.append(rchi_per_day, red_chi)\n        \n    \n    return values_param_per_day, stderr_param_per_day, rchi_per_day\n\nimport scipy\n\n\n\ndef rate_estimate(x,n_bay_0=0,n_bay_1=0):\n    \"\"\"\n    Estimate of the mortality rate using beta function\n    \"\"\"\n    rate = scipy.stats.beta.ppf(0.5 , \n                                x[0]+n_bay_0+1, \n                               (x[1]+n_bay_1)-(x[0]+n_bay_0)+1)\n    return rate\n\ndef confidence_beta_distribution(x, alpha=0.90, n_bay_0=0,n_bay_1=0):\n    \"\"\"\n    Estimate of the confidence intervals of the mortality rate using beta function\n    \"\"\"\n    lower = scipy.stats.beta.ppf((1.0 - alpha)\/2.0 ,\n                                x[0]+n_bay_0+1, \n                               (x[1]+n_bay_1)-(x[0]+n_bay_0)+1)\n    upper = scipy.stats.beta.ppf((1.0 + alpha)\/2.0 ,\n                                x[0]+n_bay_0+1, \n                               (x[1]+n_bay_1)-(x[0]+n_bay_0)+1)\n    return  upper - lower \n\n\ndef create_df_country(df_world, country_name,n_bay_0=0,n_bay_1=0):\n    \"\"\"\n    Create dataframe for country and order by days after first case. \n    \"\"\"\n    df_country = df_world[df_world[\"countriesandterritories\"] == country_name]\n    df_country[\"date_time_date\"] = pd.to_datetime(df_country[[\"year\", \"month\",\"day\"]])\n    \n    df_country = df_country.sort_values(\"date_time_date\",ascending=True)    \n    df_country[\"Total_cumulative_Cases\"] = df_country[\"cases\"].cumsum()\n    df_country[\"Total_cumulative_Deaths\"] = df_country[\"deaths\"].cumsum()\n    df_country[\"mortality_rate\"] = df_country[[\"Total_cumulative_Deaths\", \"Total_cumulative_Cases\"]].apply(rate_estimate, axis=1)\n    df_country[\"mortality_error\"] = df_country[[\"Total_cumulative_Deaths\", \"Total_cumulative_Cases\"]].apply(confidence_beta_distribution, axis=1)\n    df_country[\"mortality_relative_error\"] = df_country[\"mortality_error\"] \/ df_country[\"mortality_rate\"]\n    df_country[\"mortality_rate_bayes\"] = df_country[[\"Total_cumulative_Deaths\", \"Total_cumulative_Cases\"]].apply(rate_estimate,\n                                                                                                       axis=1,\n                                                                                                       n_bay_0=n_bay_0,\n                                                                                                       n_bay_1=n_bay_1)\n    df_country[\"mortality_error_bayes\"] = df_country[[\"Total_cumulative_Deaths\", \"Total_cumulative_Cases\"]].apply(confidence_beta_distribution,\n                                                                                                        axis=1,\n                                                                                                        n_bay_0=n_bay_0,\n                                                                                                        n_bay_1=n_bay_1)\n    \n    df_country = df_country[df_country[\"Total_cumulative_Cases\"]>0]\n    first_day = df_country[\"date_time_date\"].iloc[0]\n    df_country[\"number_days\"] =  [abs((day - first_day).days) for day in df_country[\"date_time_date\"]]\n    \n    return df_country","72e1acd2":"plt.rcParams['figure.figsize'] = [10, 8] #larger plots","4cc1d9c2":"df_world = df = pd.read_csv(\"\/kaggle\/input\/uncover\/ECDC\/current-data-on-the-geographic-distribution-of-covid-19-cases-worldwide.csv\")\ndf_world.head(5)","d3e008d5":"top20_countries = df_world.groupby(\"countriesandterritories\").sum().sort_values(\"cases\", ascending=False).reset_index().head(20)[\"countriesandterritories\"].tolist()\n\nn_world_top20_cases = df_world[df_world[\"countriesandterritories\"].isin(top20_countries)][\"cases\"].sum()\nn_world_top20_death = df_world[df_world[\"countriesandterritories\"].isin(top20_countries)][\"deaths\"].sum()\n\ndict_of_df = {}\n\nfor country in top20_countries:\n    \n    df = create_df_country(df_world, country, n_bay_0=n_world_top20_death,n_bay_1=n_world_top20_cases)  \n\n    dict_of_df[country] = df   ","c86c9dc2":"df_world_italy = dict_of_df[\"Italy\"] #defining dataset for Italy and China for convi\ndf_world_china = dict_of_df[\"China\"] ","a2be959b":"model_exp = ExponentialModel()\nparams_exp = model_exp.guess(df_world_italy[\"Total_cumulative_Cases\"], x=df_world_italy[\"number_days\"])\nresult_exp = model_exp.fit(df_world_italy[\"Total_cumulative_Cases\"], params_exp, x=df_world_italy[\"number_days\"])\n\nmodel_log = StepModel(form='logistic')\nparams_log = model_log.guess(df_world_italy[\"Total_cumulative_Cases\"], x=df_world_italy[\"number_days\"])\nresult_log = model_log.fit(df_world_italy[\"Total_cumulative_Cases\"], params_log, x=df_world_italy[\"number_days\"])\n\n\ntanti_giorni = np.array([i for i in range(1,len(df_world_italy)+20)])\n\nplt.plot(df_world_italy[\"number_days\"],\n         result_log.best_fit,\n         label=\"Logistic $\\chi^2 = {:.2E}$\".format(result_log.redchi))\n\nplt.plot(df_world_italy[\"number_days\"],\n         result_exp.best_fit,\n         label=\"ExponentialModel $\\chi^2 = {:.2E}$\".format(result_exp.redchi))\n\nplt.scatter(df_world_italy[\"number_days\"],\n            df_world_italy[\"Total_cumulative_Cases\"], \n            marker = 'o',color='black',\n           label =\"Data\")\n\ndely_log = result_log.eval_uncertainty(x=df_world_italy[\"number_days\"],sigma=3)\n\nplt.fill_between(df_world_italy[\"number_days\"], \n                 result_log.best_fit-dely_log,\n                 result_log.best_fit+dely_log, \n                 color='b',\n                 alpha=0.5)\n\ndely_exp = result_exp.eval_uncertainty(x=df_world_italy[\"number_days\"],sigma=3)\n\nplt.fill_between(df_world_italy[\"number_days\"], \n                 result_exp.best_fit-dely_exp,\n                 result_exp.best_fit+dely_exp, \n                 color='orange',\n                 alpha=0.5)\n\nparams_exp = model_exp.make_params(decay = result_exp.params[\"decay\"].value, \n                            amplitude = result_exp.params[\"amplitude\"].value)\n\nplt.plot(tanti_giorni, result_exp.eval(params_exp, x=tanti_giorni),color='orange')\n\n\nparams_log = model_log.make_params(sigma = result_log.params[\"sigma\"].value, \n                                   amplitude = result_log.params[\"amplitude\"].value,\n                                   center = result_log.params[\"center\"].value) \n\n\n#params_log = model_log.make_params(result_log.params) \n\nplt.plot(tanti_giorni, result_log.eval(params_log, x=tanti_giorni),color='b')\n\nplt.ylim(100,result_log.params[\"amplitude\"].value*1.1)\nplt.xlim(1,tanti_giorni[-1])\n\nplt.title(\"Total number of observed cases for Italy\")\nplt.xlabel(\"Days after the fist observed case\")\nplt.legend()","769d3e40":"model_log = StepModel(form='logistic')\nparams_log = model_log.guess(df_world_italy[\"Total_cumulative_Cases\"], x=df_world_italy[\"number_days\"])\n\nresult_log = model_log.fit(df_world_italy[\"Total_cumulative_Cases\"], params_log, x=df_world_italy[\"number_days\"])\n\nn_less = 7 \n\nn_days = len(df_world_italy) - n_less \n\nparams_log_5days_ago = model_log.guess(df_world_italy[\"Total_cumulative_Cases\"].head(n_days), \n                                       x=df_world_italy[\"number_days\"].head(n_days))\n\n\nresult_log_5days_ago = model_log.fit(df_world_italy[\"Total_cumulative_Cases\"].head(n_days), \n                                     params_log, \n                                     x=df_world_italy[\"number_days\"].head(n_days))\n\nplt.plot(df_world_italy[\"number_days\"],\n         result_log.best_fit,\n         label=\"Logistic total $\\chi^2 = {:.2E}$\".format(result_log.redchi))\n\nplt.plot(df_world_italy[\"number_days\"].head(n_days),\n         result_log_5days_ago.best_fit,\n         label=\"Logistic {}\u00a0days less $\\chi^2 = {:.2E}$\".format(n_less, result_log_5days_ago.redchi))\n\nplt.scatter(df_world_italy[\"number_days\"],\n            df_world_italy[\"Total_cumulative_Cases\"], \n            marker = 'o',color='black',\n           label = 'Data')\n\ndely_log = result_log.eval_uncertainty(x=df_world_italy[\"number_days\"],sigma=3)\n\nplt.fill_between(df_world_italy[\"number_days\"], \n                 result_log.best_fit-dely_log,\n                 result_log.best_fit+dely_log, \n                 color='b',\n                 alpha=0.5)\n\n\ndely_log_5days_ago = result_log_5days_ago.eval_uncertainty(x=df_world_italy[\"number_days\"].head(n_days),sigma=3)\n\nplt.fill_between(df_world_italy[\"number_days\"].head(n_days), \n                 result_log_5days_ago.best_fit-dely_log_5days_ago,\n                 result_log_5days_ago.best_fit+dely_log_5days_ago, \n                 color='orange',\n                 alpha=0.5)\n\n\nparams_log = model_log.make_params(sigma = result_log.params[\"sigma\"].value, \n                                   amplitude = result_log.params[\"amplitude\"].value,\n                                   center = result_log.params[\"center\"].value) \n\nplt.plot(tanti_giorni, result_log.eval(params_log, x=tanti_giorni),color='b')\n\n\nparams_log_5days_ago = model_log.make_params(sigma = result_log_5days_ago.params[\"sigma\"].value, \n                                   amplitude = result_log_5days_ago.params[\"amplitude\"].value,\n                                   center = result_log_5days_ago.params[\"center\"].value) \n\nplt.plot(tanti_giorni, result_log_5days_ago.eval(params_log_5days_ago, x=tanti_giorni),color='orange')\n\nplt.ylim(0,120000)\nplt.xlim(10,70)\nplt.legend()\n\nplt.title(\"Total number of observed cases for Italy\")\nplt.xlabel(\"Days after the fist observed case\")\n","483edf6a":"model_log = StepModel(form='logistic')\nparams_log = model_log.guess(df_world_china[\"Total_cumulative_Cases\"], x=df_world_china[\"number_days\"])\n\nresult_log = model_log.fit(df_world_china[\"Total_cumulative_Cases\"], params_log, x=df_world_china[\"number_days\"])\n\nn_days = len(df_world_china) - 20\n\nparams_log_5days_ago = model_log.guess(df_world_china[\"Total_cumulative_Cases\"].head(n_days), \n                                       x=df_world_china[\"number_days\"].head(n_days))\n\n\nresult_log_5days_ago = model_log.fit(df_world_china[\"Total_cumulative_Cases\"].head(n_days), \n                                     params_log, \n                                     x=df_world_china[\"number_days\"].head(n_days))\n\nplt.plot(df_world_china[\"number_days\"],\n         result_log.best_fit,\n         label=\"Logistic total $\\chi^2 = {:.2E}$\".format(result_log.redchi))\n\nplt.plot(df_world_china[\"number_days\"].head(n_days),\n         result_log_5days_ago.best_fit,\n         label=\"Logistic twenty days less $\\chi^2 = {:.2E}$\".format(result_log_5days_ago.redchi))\n\n\nplt.scatter(df_world_china[\"number_days\"],\n            df_world_china[\"Total_cumulative_Cases\"], \n            marker = 'o',color='black')\n\n\ndely_log = result_log.eval_uncertainty(x=df_world_china[\"number_days\"],sigma=3)\n\nplt.fill_between(df_world_china[\"number_days\"], \n                 result_log.best_fit-dely_log,\n                 result_log.best_fit+dely_log, \n                 color='b',\n                 alpha=0.5)\n\n\ndely_log_5days_ago = result_log_5days_ago.eval_uncertainty(x=df_world_china[\"number_days\"].head(n_days),sigma=3)\n\nplt.fill_between(df_world_china[\"number_days\"].head(n_days), \n                 result_log_5days_ago.best_fit-dely_log_5days_ago,\n                 result_log_5days_ago.best_fit+dely_log_5days_ago, \n                 color='orange',\n                 alpha=0.5)\n\n\nparams_log = model_log.make_params(sigma = result_log.params[\"sigma\"].value, \n                                   amplitude = result_log.params[\"amplitude\"].value,\n                                   center = result_log.params[\"center\"].value) \n\nplt.plot(tanti_giorni, result_log.eval(params_log, x=tanti_giorni),color='b')\n\n\nparams_log_5days_ago = model_log.make_params(sigma = result_log_5days_ago.params[\"sigma\"].value, \n                                   amplitude = result_log_5days_ago.params[\"amplitude\"].value,\n                                   center = result_log_5days_ago.params[\"center\"].value) \n\nplt.plot(tanti_giorni, result_log_5days_ago.eval(params_log_5days_ago, x=tanti_giorni),color='orange')\n\nplt.ylim(0,90000)\nplt.xlim(0,95)\nplt.title(\"Total number of observed cases for China\")\nplt.xlabel(\"Days after the fist observed case\")\nplt.legend()","80f44d4d":"countries = ['United_States_of_America', 'Italy', \n             'Spain', 'China', \n             'Germany', 'South_Korea',\n             'Iran','France']\n\ncmap = plt.cm.get_cmap(\"jet\", len(countries)+1)\ni=0 \n\nmodel_log = StepModel(form='logistic')\n\nparams_log = model_log.guess(df_world_china[\"Total_cumulative_Cases\"], \n                             x=df_world_china[\"number_days\"]) #using China as a starting point for minization of parameters\n\n\n\nfor country in countries:\n    \n    i+= 1 \n    \n    df = dict_of_df[country]\n    values_name = country + '_values_amplitude_per_day'\n    stderr_name = country + '_stderr_amplitude_per_day'\n    red_chi_name = country + '_red_chi_per_day'\n    \n    values_amplitude_per_day, stderr_amplitude_per_day, red_chi_log = return_parameters_over_time(\n                                                df[\"number_days\"],\n                                                df[\"Total_cumulative_Cases\"],\n                                                model_log,\n                                                position_param=0,\n                                                min_days=30)\n    \n    dict_of_df[values_name] = values_amplitude_per_day\n    dict_of_df[stderr_name] = stderr_amplitude_per_day\n    dict_of_df[red_chi_name] = red_chi_log\n       \n    relative_error = stderr_amplitude_per_day \/ values_amplitude_per_day\n    \n    filter_error = relative_error < 1.0 #excludying values that have error that are \"too high\"\n    \n    plt.errorbar(df[\"number_days\"][30:len(df)+1][filter_error], \n             values_amplitude_per_day[filter_error] \/ values_amplitude_per_day[-1],\n             linestyle = '--',\n             marker = 'o',\n             color = cmap(i),\n             yerr = stderr_amplitude_per_day[filter_error] \/ values_amplitude_per_day[-1] ,\n             ecolor = cmap(i),\n             label=country)\n    \n    \nplt.ylim(0.4,1.3)\nplt.xlim(35,80)\nplt.vlines(74,-10,30, linestyles='--') #\"end of quarantine\" in China\nplt.legend(loc=\"lower right\")\nplt.xlabel(\"Days after first case\")\nplt.ylabel(\"$A(t) \/ A(t_O)$\")","081b87b1":"countries = ['United_States_of_America', 'Italy', \n             'Spain', 'China', \n             'Germany', 'South_Korea',\n             'Iran','France']\n\ncmap = plt.cm.get_cmap(\"jet\", len(countries)+1)\ni=0 \n\nfor country in countries:\n    \n    i+=1\n    \n    df = dict_of_df[country]\n    df = df[df[\"mortality_relative_error\"]<1.5]\n    \n    plt.errorbar(df[\"number_days\"],\n                 df[\"mortality_rate\"]*100,\n                 linestyle = '--',\n                 marker = 'o',\n                 color = cmap(i),\n                 yerr = df[\"mortality_error\"]*100,\n                 ecolor = cmap(i),\n                 label = country)\n    \nplt.legend()\nplt.ylim(-0.01,14.2)\nplt.xlim(10,80)\nplt.xlabel(\"Number of days after first case in the country\")\nplt.title(\"Daily mortality rate estimate\")\n\n#plt.title(\"$\\hat{t}^i_{A}$ per day\")","ea222651":"cmap = plt.cm.get_cmap(\"hsv\", len(top20_countries)+1)\n\n\nfrom random import shuffle\n\nshuffle(top20_countries) #shuffling for better reading the colors on plots\n\nx = np.arange(0,100) # just a straight line \n\ni=0\nfor country in top20_countries:\n    \n    i += 1\n     \n    plt.errorbar(100*dict_of_df[country].iloc[-1][\"mortality_rate\"], \n                 100*dict_of_df[country].iloc[-1][\"mortality_rate_bayes\"],\n             linestyle = ' ',\n             marker = ' ',\n             xerr = 100*dict_of_df[country].iloc[-1][\"mortality_error\"],\n             yerr = 100*dict_of_df[country].iloc[-1][\"mortality_error_bayes\"],\n             label = country,\n             color = cmap(i) )\n    \n    plt.text(100*dict_of_df[country].iloc[-1][\"mortality_rate\"], \n            100*dict_of_df[country].iloc[-1][\"mortality_rate_bayes\"],\n            s=country,\n            fontsize=8,\n            bbox=dict(facecolor=cmap(i), alpha=0.5),\n            rotation=0)\n    \n \nplt.xlabel('$\\hat{t}^i_{A}$')\nplt.ylabel('$\\hat{t}^i_{T}$')\nplt.fill_between(x,x,np.zeros(len(x)),alpha = 0.1)\nplt.fill_between(x,x,10*np.ones(len(x)),alpha = 0.1)\n\nplt.xlim(0.0,12.5)\nplt.ylim(4.0,6.6)\n#plt.legend(bbox_to_anchor=(1.0, 1.0)) ","159dff98":"i=0\nfor country in top20_countries:\n    \n    i += 1\n     \n    plt.errorbar(100*dict_of_df[country].iloc[-15][\"mortality_rate\"], \n                 100*dict_of_df[country].iloc[-15][\"mortality_rate_bayes\"],\n             linestyle = ' ',\n             marker = ' ',\n             xerr = 100*dict_of_df[country].iloc[-15][\"mortality_error\"],\n             yerr = 100*dict_of_df[country].iloc[-15][\"mortality_error_bayes\"],\n             label = country,\n             color = cmap(i) )\n    \n    plt.text(100*dict_of_df[country].iloc[-15][\"mortality_rate\"], \n            100*dict_of_df[country].iloc[-15][\"mortality_rate_bayes\"],\n            s=country,\n            fontsize=8,\n            bbox=dict(facecolor=cmap(i), alpha=0.5),\n            rotation=0)\n    \n \n\nplt.xlabel('$\\hat{t}^i_{A}$')\nplt.ylabel('$\\hat{t}^i_{T}$')\nplt.fill_between(x,x,np.zeros(len(x)),alpha = 0.1)\nplt.fill_between(x,x,10*np.ones(len(x)),alpha = 0.1)\n\nplt.xlim(0.0,8.5)\nplt.ylim(4.75,5.3)\n#plt.legend(bbox_to_anchor=(1.0, 1.0)) ","95533bde":"As we can see from the plot above, the fit is very unstable and the parameters change dramatically.\nThis , combined with the fact the fit singularly show a similar $ \\chi^2$, shows that the **forecasting power of a logistic function during an ongoing pandemic might be very limited** and particular care should be taken when using it. \n\nSo, should we abandon the Logistic function as a candidate to describe this growth?\n\nNo, not entirely at least.In fact, the logistic function does a discrete job in describing and forecasting values, **once that the spread of the virus is more under control and fit starts to stabilize**. \n\nThis can be shown by fitting the Chinese data using all available data and comparing them to a fit made by using all data from the beginning up to the 20 days ago: ","aad1273d":"In the previous plot, each point represents a country and on the x axis the we find the estimate of the mortality rate with a uniform prior (the \"single-country\" estimate) and on the y axis the estimate of the mortality rate with the beta function priors from the 20 most impacted countries. \n\nAs we can see, the beta prior reduces dramatically the range of the estimate of the mortality rates and to a lesser extend to their confidence intervals. \n\nThis is due to the fact that the estimate at a country level is **in general very poorly representative to the \"global\" one, due to the different phases of the epidemics.** In particular, countries for which $\\hat{t}^i_{A}>\\hat{t}^i_{T}$ (blue region) are probably **overestimating the conversion rate** due to the fact that high number of causalities while the opposite is true for countries in which $\\hat{t}^i_{A}<\\hat{t}^i_{T}$ (pink region) since they are in phase of the epidemics for which the number of causalities is low, either because they are towards the \"beginning\" (eg the USA) or toward the \"end\" (eg China) of the epidemics. \n\nThe latter can be seen also by drawing the same plot with data taken two weeks ago, where the phase of mass testing in the USA and Germany had not yet started and the epidemics was much more under control in France and Spain): ","d5dc8663":"where I have excluded the point for which the relative error on the mortality rate is higher than 1.5 to avoid noisy estimates. \nThere are several interesting things about this plot: firstly, we can see that there a huge variety in the estimates among countries. This is due the fact that different countries are not only measuring differently the total number of infected cases and causalities but also are at different stages of the epidemics. \n\nLooking at the shape of the curves, we can see that there are at least 3 \"stage of measurements\" for the  mortality rates that corresponds roughly a to different phases of the epidemics (see also [here](https:\/\/ourworldindata.org\/covid-testing)):\n\n1. a fist phase in which the mortality rate decreases in time, which corresponds to a first phase of testing with a relative low number of causalities;\n2. as the epidemics advances, the number of causality starts to grow \"just exponentially\" and the mortality rate increases very steadily because the number of tests grows less than the number of causalities (it is almost impossible to make the latter grow exponentially due to the very high \"costs\");  \n3. a third phase (in which only few countries are as of today), in which the epidemics is more under control and the mortality rate decreases since there are less causalities and the same (or even more) number of tests, that corresponds to a decrease or to a stabilization of the mortality rate (see Iran or China).  \n\nIn particular, France, Italy and Spain have very high $\\hat{t}^i_{A}$ since the number of causalities is still unfortunately growing very fast. On the other hand Germany and the USA started testing very aggressively at the beginning and now their mortality rates are increasing due to the growing number of causalities.\n\n## $\\hat{t}^i_{A}$ v.s. $\\hat{t}^i_{T}$ plots\n\n\nIn this section I will compare $\\hat{t}^i_{A}$ to $\\hat{t}^i_{T}$ as of today 03\/04\/2020, to give some insight on which countries are under\/over estimating their mortality rates. ","f8a3baf1":"As mentioned below, I have plotted $A(t) \/ A(t=t_N)$, i.e. the **estimated total number of cases as a function of time renormalized over its value today  A(t=t_N)**. \n\nIn the plot the black dashed line represents the day where the Chinese government started to lift some of the quarantine's restrictions. This corresponds to roughly **14 days before the ratio $A(t) \/ A(t=t_0)$ became stable**, due lack of new cases. \n\nThis suggest that studying the curve $A(t) \/ A(t=t_0)$  (and its stability) might give some insight on the duration of the quarantine. \n\nUnfortunately, the ratios $A(t) \/ A(t=t_0)$ for most of the countries still show a growing trend and in the very optimistic scenario where the ratio stabilizes during the next days, I would **not expect the quarantine to end in the next 15\/20 days.**\n\n*any comment on this assumption is more than welcomed. \n\n\n## CONCLUSIONS ON STABILITY OF THE FIT\n\nIn this notebook I have studied some of the data available on the ongoing global pandemic of COVID-19. \n\nIn particular I have focused on the time trends of the total number of observed infected people in China, Italy, Japan, South Korea and France. \n\nI have showed that, as of today 22\/03\/2020, the Italian data seems to prefer a Logistic model but with huge instability and little forecasting power, due to the ongoing spread of the virus. On the other hand in China, where the epidemic is more under control, the fit with a Logistic function is stable and has a good forecasting power.\n\nThis inspired me in studying the behavior of $A(t) \/ A(t=t_0)$ for several countries in order to give some insight on the duration of the quarantine. Findings are still quite weak but they suggest for European countries as of today 03\/04\/2020 the quarantine won't be lifted before the 15\/20 days, even in the best case scenario.\n\nIn any case, I argue here that it would be beneficial to study more in depth $A(t) \/ A(t=t_0)$, in particular its forecasting power but more analysis are probably needed. \n\nThese findings are still \"weak\" at the moment for a variety of reasons that I have tried to highlight here but, as I have mentioned before, **any comment\/suggestion is more than welcomed.**\n\n\n# MORTALITY RATE\n\nIn this section, I will try to study more in details the mortality rate related to the COVID-19 pandemics. \nIn particular, I will try to gain some understanding on the variation of the estimated mortality rate across countries using data from the [European Center for Disease Prevention and Control](https:\/\/www.ecdc.europa.eu\/en\/publications-data\/download-todays-data-geographic-distribution-covid-19-cases-worldwide) about the _ongoing global COVID-19 pandemic_ .\n\nThis is of particular importance since the measured values of the mortality rates $\\hat{t}$ vary dramatically across countries due to a variety of reasons such as (but are not limited to) how much is the virus spread in the country and which kind of the measurements the country is taking in terms of testing. \n\nIn order to address some of these problems, I have develop here a model for mortality rate based on **Bayesian ranking**, which allows me to compute mortality rates for each country and gives me credible confidence intervals that take into account the **information coming from the estimate of the mortality rates in other countries.**\n\nTo do so, let me define the total number of cases and causalities for each country respectively $N_c^{i}$ and $N_d^{i}$ and model the estimate of the mortality for COVID-19 in each country as:\n\n\\begin{equation}\n\\mathrm{P}\\left ( t^{i} = \\hat{t}^{i} |  N_c^{i}, N_d^{i} \\right ) = \\frac{\\mathrm{P}\\left (    N_c^{i}, N_d^{i} | t^{i} = \\hat{t}^{i} \\right ) \\mathrm{P}\\left ( t^{i} \\right ) }{\\int \\mathrm{P} \\left ( y \\right ) \\mathrm{P} \\left (  N_c^{i}, N_d^{i} | y  \\right ) dy  } \n\\end{equation}\nwhere $t^{i}$ is the mortality rate and $\\hat{t}^{i}$ is its estimate. \n\nThe conditional probability can be modeled as a Bernoulli trial over $N_c^{i}$, $N_d^{i}$ as: \n\\begin{equation}\n\\mathrm{P}\\left (   N_c^{i}, N_d^{i} | t^{i} = \\hat{t}^{i} \\right )  = \\binom{N_d^{i}}{N_c^{i}} t^{N_d^{i}} (1-t)^{N_c^{i} - N_d^{i}}. \n\\end{equation}\n\nLet me choose for the moment $\\mathrm{P}\\left ( t^{i} \\right ) = 1$, a uniform prior which will lead to an estimate of the mortality rate for each country that **is independent from the estimate of the mortality rates in other countries (which I will call here $\\hat{t}^i_{C}$)**.\n\nUsing this prior we have:\n\\begin{equation}\n\\mathrm{P} \\left ( t^{i}_{C} = \\hat{t}^{i}_{C} |  N_d^{i}, N_c^{i} \\right )   = \\frac{t^{N_d^{i}}_{C} (1-t_{C})^{N_c^{i} - N_d^{i}}}{B\\left ( N_d^{i}+1, N_c^{i}- N_d^{i} +1  \\right )} = f_B \\left ( t_{C}, N_d^{i}+1, N_c^{i} - N_d^{i} + 1 \\right ),\n\\end{equation}\n\nwhere $B(\\alpha, \\beta)$ is beta function with parameters $\\alpha$ and $\\beta$ and $f_B \\left ( x, \\gamma, \\delta  \\right )$ is the beta distribution function with parameters $\\gamma$ and $\\delta$.\n\nThis will allow me to easily compute the mortality rates and its **confidence interval by using the beta distribution.** In particular $\\hat{t}^i_{C}$ will be the median of the beta distribution with parameters $N_c^{i}, N_d^{i}$ and its confidence interval will correspond to the 90 % interval of the distribution around the median value.  \n\nMoreover, thanks to the Bayesian approach, we can easily introduce the **information about the estimate of the conversion rates in another countries by choosing another prior**. \n\nI have decided here* to introduce this information by choosing as a prior **a beta function with parameters $N^{T}_c$ , $N^{T}_d$, i.e. the total number of cases in the 20 most impacted countries by the COVID-19.** \nOne could [demonstrate](https:\/\/www.chrisstucchio.com\/blog\/2013\/bayesian_analysis_conversion_rates.html) that \nchoosing a beta function as a prior gives\n\\begin{equation}\n\\mathrm{P} \\left ( t^{i}_{T} = \\hat{t}^{i}_{T} | N^{T}_d + N_d^{i}, N^{T}_c + N_c^{i} \\right )   = f_B \\left ( t_{T}, N_d^{i}+N^{T}_d+1, \\left (N^{T}_c + N_c^{i}  \\right ) - \\left (N^{T}_d + N_d^{i}  \\right ) + 1 \\right ) \n\\end{equation}\n\ni.e. **another beta function with different parameters and where $\\hat{t}^i_{T}$ is the estimate of the conversion rate for each country using the prior information about the 20 most impacted countries.**\n\nIn the next sections, I will compare $\\hat{t}^i_{A}$ and $\\hat{t}^i_{T}$ to try to shed some light on the variation of the estimated mortality rate across countries. \n\n*the beta function is chosen since it is mathematically easy to handle in the case and it corresponds to a \"good distribution\" for a binary outcome\n\n## Time dependence of the estimate of the mortality rate\n\nLet me start by plotting  $\\hat{t}^i_{A}$ for some of the most impacted countries as a function of the days after the first case measured in each country:","e17e9fa5":"and the fit seem to **slightly** prefer a Logistic trend instead of an exponential one (note the values of the $\\chi^2$ in the legend).\n\nHowever almost all data points fall inside the $3-\\sigma$ error bars of both the fitted functions and so very little can be said about the which **function would be better at forecasting new data**, due to the quite fast growth of the number observed of cases in Italy. \n\nArmed with fits, we could ask ourselves questions about the time stability of the Logistic fit. In particular, I think it is beneficial to ask the question: \n- how much a the fit of 7 days ago would be able to **forecast the observed number of cases in last 7 days**? ","b86a03d3":"### Exponential vs Logistic fit for Italy and China\n\nAs a starting point, we can ask ourselves a simple question for Italy:\n\n- as of today 22\/03\/2020, is the number of total observed cases $N_c$ still growing **in exponential fashion or is its growth more similar to a logistic one**? \n\nIn order to answer this question, it might be useful to give a quick reminder of the definition and some proprieties of the two functions. \n\nIf the $N_c(t)$ follows an exponential we have: \n\\begin{equation}\nN^e_c(t) = C e^{-t \/ \\tau}\n\\end{equation}\n\nwith $t$ the number of days after the first observed case and $\\tau$ the \"decay time\", which will be negative for us in order to model an exponential growth. \n\nOn the other hand, if $N_c(t)$ follows has a logistic growth:\n\\begin{equation}\nN^L_c(t) =  \\frac{A}{1+e^{(t-\\mu)\/\\sigma}}\n\\end{equation}\nwhere A is the total **observed number of cases over the whole pandemic**, a parameter which might be of particular interest for our analysis.\n\nAs of **22\/01\/2020** data for Italy look like: ","32202399":"#\u00a0A SIMPLE STATISTICAL ANALYSIS OF ROBUSTNESS OF COVID-19 DATA FIT  \n\nIn this notebook I analyse data taken from the [European Center for Disease Prevention and Control](https:\/\/www.ecdc.europa.eu\/en\/publications-data\/download-todays-data-geographic-distribution-covid-19-cases-worldwide) about the _ongoing global COVID-19 pandemic_ , which can be found here : \"\/kaggle\/input\/uncover\/ECDC\/current-data-on-the-geographic-distribution-of-covid-19-cases-worldwide.csv\"\n\nAll the analsyis can be found also on my [github](https:\/\/github.com\/lalbrizzo\/COVID-19-models-and-forecast-)\n\nI will focus my analysis on the **total number of observed cases in China, Italy, South Korea, Japan and France** in order to try to comprehend better the stability and the forecasting power of a simple logistic (and to lesser extend exponential) function while the **pandemic is still ongoing**. \n\nMoreover, I will try to understand if we can gain some insight on the **duration of the quarantines, which were put in place in all the countries analyzed here.**\n\nDue to the sensitivity and importance of this topic, this analysis should be taken as not more than as an intellectual exercise about the current state of the global pandemic. \n\nI will, in fact, try to compare data among different countries which have been collected in very hazardous and complex situations and they probably contain biases and errors. \n\nTo mitigate this, I will **renormalize data at the country level, before any comparison** : this is probably not enough but it is the most that I was able with available data (and any suggestion on how to improved this is more than welcomed). \n\nFurther analysis will be needed to asses the validity of this and other studies and I am sure that the scientific community and society at large will tackle them, once the pandemic will be more under control.\n\n\nFrom a more technical point of view, I used in this analysis tools and techniques that should be familiar to most people with a background in data science\/analysis and any question\/comment is (again) more than welcomed.\n\nDue to my personal background, some of this analysis will focus on Italy but it should generic enough to be easily reproducible in other countries. \n\n## Analysis of fit stability for Italy and China \n\nIn this section I will analysis the **time stability of fits on the observed cases of COVID-19 in Italy and China**. \nIn order to that, I define here some function that will be handy for future analysis: ","42c29d48":"As I have said in the previous section, the data are taken from European Center for Disease Prevention and Control and have to be _manually downloaded_ (at least as of today, 22\/03\/2020).\n\nMore refined data about the Italian situation can be found on the [great Github of the Protezione Civile](https:\/\/github.com\/pcm-dpc\/COVID-19) which is update daily and there exists also a [Kaggle competition with global data](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge). \nHowever, I personally find data from ECPC more handy for cross-country comparison and that is why I use them here.\n\nAfter loading the dataset, and I have create a dataframe for each country taken data starting from the **first day in which a case was observed in each one of these countries,** which roughly corresponds to beginning of the epidemy in the country of interest. \n\nMoreover, in order to able to compare results among different countries, I have created, for each dataset, a column called \"number_days\" which represents the days passed from the first case.  ","d544a489":"# CONCLUSIONS ON MORTALITY RATES\n\nAs we can see, all countries have a $\\hat{t}^i_{T}$  that is compatible with rest of the other countries since most of the countries were able to \"correctly\" estimate the mortality rate. \n\nFrom these plots we can draw some important conclusions about the mortality rates at the country level:\n1. the mortality rate is **a time dependent quantity** and we should avoid using daily estimates to drawn any conclusion; \n2. during an epidemic, it is **very hard to test enough people to have a fair estimate of the mortality rate due to the fast growth in the number of causalities**; \n3. a Bayesian approach might be **very beneficial in comparing mortality rate across countries**, which will be of high importance in understanding better the confounders associated to the number of causalities (average age of the population, pollution, ...); ","8b908990":"As we can see from above, **the fit is much more stable and the fit done 20 days ago does a good job in forecasting values in last 20 days.** \n\nTo summarize, as long as the spread of **the virus is too fast, the fits using Logistic functions are not stable enough to draw any conclusion on future data. However, once the spread starts to slow down, the fit becomes stable and does a good job in forecasting future values.** \n\n\n### A (TENTATIVE) COMPARISON AMONG COUNTRIES TO GIVE AN ESTIMATE OF THE DURATION OF THE QUARANTINE \n\nAs advised from the WHO, the quarantine has been taken place in several countries in order to try to slow down the growth total number of cases and therefore minimize the damage of the ongoing pandemics.  \n\nSince the quarantine has a strong effect on the population and the economics of the countries, it would be beneficial to have a rough estimate of the its duration. This is no easy task, since different countries acted in different ways at different paces due to a variety of reasons. Moreover there exist only one known case were the quarantine was (at least partially) lifted up, namely China, and so any method to estimate of the duration of the quarantine cannot be properly verified. \n\nIn any case, I will try to understand if I can use **some of the findings about the stability of the Logistic fits to to gain some insight of the duration of the quarantine.** \n\nTo achieve this, I have runt, for each country of interest, several fits of the Logistic function using every time data **from the first day in which COVID-19 was first observed ($t_0$) up to day $t$**, with $t$ that runs from $t_0$ up to today. \n\nIn particular, this procedure will give us for each day the estimated total number of infected people  for each country (see plot below), which can be seen as a function of the (cumulative) time, called here $A(t)$.\n\nI will argue here that we can gain some insight about the duration of the quarantine by studying $A(t)$, once that the latter is properly renormalized. (see below)\n\n\nBelow I run the model described here, where I have excluded the first _min_days_ days where the fit of Logistic functions was too noisy."}}