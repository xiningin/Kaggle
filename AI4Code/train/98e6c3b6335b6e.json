{"cell_type":{"3434e25d":"code","d18da551":"code","7b6d9b34":"code","192e82ff":"code","83b5aa43":"code","3811dcb6":"code","de66eadf":"code","75a02fd7":"code","3f3e836b":"code","45e3f401":"code","c2798321":"code","81744d8f":"code","837903bb":"code","2acaef21":"markdown","216946a6":"markdown","e35458e4":"markdown","2c0dec2b":"markdown","a60dc599":"markdown","2bd9be81":"markdown","f682e5ae":"markdown","f1043a9c":"markdown","041d4d59":"markdown","1371dc85":"markdown","6072fc83":"markdown","bb29d857":"markdown","02908f3c":"markdown","b966a8cd":"markdown"},"source":{"3434e25d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Data import from the csv file to a dictionary with numpy arrays\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n#Shuffle dataset\ntrain_data = train_data.sample(frac=1)\n\ncolumns = dict()\nfor column in train_data.columns:\n    columns[column] = np.array(train_data[column]).reshape(-1,1)","d18da551":"from scipy import stats #Needed for convienient mode function\n\ndef binarize(vector):\n    #Get rid of nan values and replace them with the most common value in the whole feature\n    for i in range(len(vector)):\n        try: \n            if np.isnan(np.array(vector[i],dtype=np.float64)): vector[i] = stats.mode(vector).mode\n        except: \n            pass\n    \n    #Create new matrix storing the binary endcoded vector\n    binarized_matrix = np.zeros(shape=(vector.shape[0],len(np.unique(vector))))\n    \n    for no,value in enumerate(np.unique(vector)):\n        for entry in range(vector.shape[0]):\n            if vector[entry] == value: binarized_matrix[entry,no] = 1\n        \n    #Return binarized matrix without the last column, to avoid the curse of dimensionality\n    return binarized_matrix[:,:len(np.unique(vector))-1]\n    \ndef standarize(vector,mean,std):\n    #Get rid of nan values and replace them with mean\n    vector[np.isnan(vector)==True] = np.nanmean(vector)\n    \n    #Subtract mean and divide by variance\n    vector -= mean\n    vector \/= std\n    \n    return vector","7b6d9b34":"#Continuous features are Age and Fare, all of them should be standarized\nAge_mean = np.nanmean(columns['Age'])\nAge_std = np.nanstd(columns['Age'])\nFare_mean = np.nanmean(columns['Fare'])\nFare_std = np.nanstd(columns['Fare'])\ncolumns['Age'] = standarize(columns['Age'],Age_mean,Age_std)\ncolumns['Fare'] = standarize(columns['Fare'],Fare_mean,Fare_std)\n\n#Discrete features are: Pclass, Sex, SibSp, Parch,Embarked, out of which Pclass, Sex and Embarked should be binarized\ncolumns['Pclass'] = binarize(columns['Pclass'])\ncolumns['Sex'] = binarize(columns['Sex'])\ncolumns['Embarked'] = binarize(columns['Embarked'])\n\n#All of the features mentioned above shall be concatenated into one array:\nX = columns['Age']\nfor column in {'Embarked','Fare','Sex','SibSp','Parch','Pclass'}:\n    X = np.concatenate((X,columns[column]),axis=1)\n\nY = columns['Survived']\n\n#Divide into subsets:\nm = X.shape[0]\nX_train = X[:(int)(m*0.6),:]\nX_cv = X[(int)(m*0.6):(int)(m*0.8),:]\nX_test = X[(int)(m*0.8):,:]\n\nY_train = Y[:(int)(m*0.6),:]\nY_cv = Y[(int)(m*0.6):(int)(m*0.8),:]\nY_test = Y[(int)(m*0.8):,:]","192e82ff":"def relu(x):\n    x = np.copy(x)\n    x[x<0] = 0\n    return x\n\ndef relu_backward(x):\n    x = np.copy(x)\n    x[x<0] = 0\n    x[x>=0] = 1\n    return x\n\ndef sigmoid(x):\n    return 1\/(1+np.exp(-x))\n\ndef sigmoid_backward(x):\n    return sigmoid(x)*(1-sigmoid(x))","83b5aa43":"EPSILON = 1e-10 #To avoid dividing by zero and taking logs from negative numbers\n\n#Function for random initialization of weights:\ndef initialize_weights(m,n):\n    w = np.random.randn(m,n)\n    return w\n\nclass Network:\n    def __init__(self,in_size,hidden_size,out_size):\n        \n        self.in_1 = None #Input to first layer\n        self.w1 = initialize_weights(in_size + 1,hidden_size) #Weights of first layer (+1 because of bias unit)\n        self.w1_grad = np.zeros_like(self.w1) #Gradients of w1\n        self.z_1 = None #Output of first layer\n        self.out_1 = None #Output of first layer after activation\n        \n        self.in_2 = None #Input to second layer\n        self.w2 = initialize_weights(hidden_size + 1,out_size) #Weights of second layer (+1 because of bias unit)\n        self.w2_grad = np.zeros_like(self.w2) #Gradients of w2\n        self.z_2 = None #Output of second layer\n        self.out_2 = None #Output of second layer after activation\n    \n  \n    def forward(self,X):\n        \n        #First layer--------------------------\n        self.in_1 = np.concatenate((np.ones(shape=(X.shape[0],1)),X),axis=1) #Add bias unit\n        \n        self.z_1 = np.dot(self.in_1,self.w1) #matmul\n        \n        self.out_1 = relu(self.z_1) #relu\n        #--------------------------------------\n        \n        #Second layer--------------------------\n        self.in_2 = np.concatenate((np.ones(shape=(self.out_1.shape[0],1)),self.out_1),axis=1) #Add bias unit\n        \n        self.z_2 = np.dot(self.in_2,self.w2) #matmul\n        \n        self.out_2 = sigmoid(self.z_2) #sigmoid\n        \n        #--------------------------------------\n        \n        return self.out_2\n\n    def compute_cost(self,h,Y):\n        #Compute cost based on cross entropy\n        \n        loss = - (Y * np.log(h+EPSILON) +  (1-Y)*np.log(1-h+EPSILON))\/Y.shape[0]\n        \n        return sum(loss)\n    \n    def backward(self,h,Y):\n        #Backprop through cost:\n        delta = - (Y * 1\/(h+EPSILON) - (1-Y) * 1\/(1-h+EPSILON))\/Y.shape[0]\n        \n        #Second layer----------------------------------------------------\n        delta = sigmoid_backward(self.z_2)*delta #Backprop through sigmoid\n        \n        self.w2_grad = np.dot(self.in_2.T,delta) #Backprop through matmul\n        \n        delta = np.dot(delta,self.w2.T)[:,1:] #Backprop the gradients to next layer\n        #----------------------------------------------------------------\n        \n        \n        #First layer-----------------------------------------------------\n        delta = relu_backward(self.z_1)*delta #Backprop through relu\n        \n        self.w1_grad = np.dot(self.in_1.T,delta) #Backprop through matmul\n        \n        \n        #----------------------------------------------------------------\n    \n    def update_weights(self,learning_rate):\n        self.w1 -= learning_rate * self.w1_grad\n        self.w2 -= learning_rate * self.w2_grad","3811dcb6":"model = Network(9,18,1)\ncost = []\ncost_cv = []","de66eadf":"lr = 1e-3 #Learning rate\nmaxit = 30000 #Number of iterations\n\nfor it in range(maxit):\n    h = model.forward(X_train)\n    model.backward(h,Y_train)\n    model.update_weights(lr)\n    \n    cost.append(model.compute_cost(h,Y_train))\n    cost_cv.append(model.compute_cost(model.forward(X_cv),Y_cv))\n    \n    #Decrease learning rate after 15K iterations\n    if it==15000: lr \/=3","75a02fd7":"import matplotlib.pyplot as plt #Needed for plotting\n\nplt.plot(cost)\nplt.plot(cost_cv)\nplt.show()","3f3e836b":"TRESHOLD = 0.5\ndef accuracy(h,Y):\n    r = np.zeros_like(h)\n    r[h>TRESHOLD] = 1\n    return sum(r==Y)\/Y.shape[0]\n\nprint('Accuracy on the test set is equal to: ',accuracy(model.forward(X_test),Y_test))","45e3f401":"#Importing and preprocessing data in the same way as before\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_columns = dict()\nfor column in test_data.columns:\n    test_columns[column] = np.array(test_data[column]).reshape(-1,1)\n\n#Continuos data need to be standarized using the same mean and std as before\ntest_columns['Age'] = standarize(test_columns['Age'],Age_mean,Age_std) \ntest_columns['Fare'] = standarize(test_columns['Fare'],Fare_mean,Fare_std)\n\ntest_columns['Pclass'] = binarize(test_columns['Pclass'])\ntest_columns['Sex'] = binarize(test_columns['Sex'])\ntest_columns['Embarked'] = binarize(test_columns['Embarked'])\n\ntest = test_columns['Age']\nfor column in {'Embarked','Fare','Sex','SibSp','Parch','Pclass'}:\n    test = np.concatenate((test,test_columns[column]),axis=1)","c2798321":"predicted_scores = model.forward(test)\n\npredicted_labels = np.zeros_like(predicted_scores)\npredicted_labels[predicted_scores>TRESHOLD] = 1","81744d8f":"index = np.arange(test.shape[0]) + 892\nsubmission = np.concatenate((index.reshape(-1,1),predicted_labels),axis=1)\nsubmission = np.array(submission,dtype=np.int32)","837903bb":"pd.DataFrame(submission,columns=['PassengerId','Survived']).to_csv('Submission.csv',index=False)","2acaef21":"**4.Making a submission**\n\nAs we already have a model that can make quite a good predictions on an unseen example, lets now submit predictions of the unlabelled data. ","216946a6":"Lets now look at what happend to the cost during the learning process:","e35458e4":"**2. Coding the Neuron Network**\n\nFor our neuron network we are gonna need two activation function, sigmoid and relu. Lets first define them as well as their gradients that will later be usefull for backpropagation:","2c0dec2b":"I will now define the neuron network class. While creating an object of this class, user has to specify the number of input features, size of the hidden layer, as well as the size of output. The class will have the forward() function performing forward propagation, backward() function for backward prop. and storing gradients in internal variables w1_grad and w2_grad, compute_cost() to compute the current cost of the model, as well as update_weights() which updates weights based on the functions stored in the network's memory.\n\nMy proposed architecture of the network is linear -> relu -> linear -> sigmoid, if you want you can modify my code and change the network's architecture.\n","a60dc599":"Now we just need to convert the data to a suitable format:","2bd9be81":"Now lets apply those function to our data, and then concatenate it to one matrix X containing everything that is going to be treated as an input to our network. Then we will divide X into train set, cross validation set and test set.","f682e5ae":"As we see, the cost on both training and cv set has been significantly decreased. As I have chosen the network's architecture and training parameters based on the cv set, lets now see how network behaves on a completely unseen data. To estimate that I will measure its prediction's accuracy:","f1043a9c":"To help the network with handling the data, we need to first preprocess it. Lets define functions that will be later used for this purpose:","041d4d59":"I will now proceed to the training process. We will need a loop that excecutes forward and then backward propagation. This loop shall also store the cost history of training and cross validation set. I found out that the model fits the data quite good after 30K iterations with learning rate (lr) equal to 1e-3, which is being decreased to 3e-4 after 15K iterations have been executed. ","1371dc85":"And save it to .csv file using pandas:","6072fc83":"Now to make a prediction:","bb29d857":"**1. Data Import and Preprocessing**\n\nFirst we need to load the data from .csv file. I will create a dictionary filled with numpy vector containing the data just for the sake of convienince.","02908f3c":"**Titanic Neuron Network on Numpy ONLY ** \n\nIn this kernel, I would like to show a basic solution to the famous Kaggle Titanic Challenge, by using hand-coded Neuron Network, which will use only numpy to run. Of course, libraries like PyTorch or Keras can be extremely useful, as they provide fast and convienient way to construct a ML model, however, I think it is extremely vital to be able to code those networks by yourself and understand the mathematical principals behind them to be able to fully use their potential. If you are only interested in the network itself, you can skip straight to the Neuron Network chapter. Additional chapters of this kernel are for people new to Kaggle or Python and are showing how to import and preprocess data and also how to create a submission.","b966a8cd":"**3. Fitting the model**\n\nNow lets try to create and train a model based on the Network class created earlier. There are 9 input features and we want to have 1 output; we also need to choose the size of the hidden layer. There is a very important trade-off here: if we add more neurons the network will be larger, slower and more prone to overfitting, but it will also be able to find more correlations. Here, we have very little features and also not many examples, so the network performance is highly dependent on the initial values of weights, which are chosen randomly. We need to have enough layers so that the weights have a chance to initialize in beneficial ways for at least some of the neurons. I found out that the size of 18 (number of features twice) does the job quite well."}}