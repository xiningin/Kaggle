{"cell_type":{"780b3fa5":"code","dda6edde":"code","13594de0":"code","c04e7dc9":"code","3e557fe4":"code","2c7e65fc":"code","bb2296d2":"code","cee9b523":"code","942a3809":"code","e123966f":"code","2f563159":"code","86388b32":"code","24c885f8":"code","c243e93f":"code","fd2fc230":"code","ee3b1f7f":"code","4d1ad044":"code","c8b6fef0":"code","216f3978":"code","30b01c7a":"code","431e5469":"code","b129400f":"code","8be2e479":"code","b0427856":"code","d51810c9":"code","eb742a9b":"code","58ed9c9d":"code","117645fb":"code","06b3962c":"code","4a280e7b":"code","d9120ef0":"code","96bb0f38":"code","88823c01":"code","d2e26721":"code","c0d06026":"code","48905ba7":"code","350d7ac3":"code","07893b3c":"code","1e7e269d":"code","d5344110":"code","99a28c9d":"code","6b05bd7e":"code","f170f7ec":"code","17c590a6":"code","0b799de8":"code","2d77a8a8":"code","af72bd98":"code","cb2eb129":"code","be4b590a":"code","cfdfb3de":"code","e6065204":"code","1021182e":"code","d0b783b2":"code","156affd0":"code","eb2f2173":"code","58151fa6":"code","c58ff4e8":"code","07b900a9":"code","78b87632":"code","00515215":"code","112fdb90":"code","ba409e9b":"code","11a4dea1":"code","6eb14247":"code","705fc15f":"code","38094f8a":"code","21f6f119":"code","a4428d91":"markdown","391cd3c8":"markdown","7703e863":"markdown","1d34834b":"markdown","9b185eb0":"markdown","97a88345":"markdown","a6bfee5b":"markdown","d78393a8":"markdown","a11ea1e1":"markdown","1f2c6844":"markdown","09818026":"markdown","e2d24bdc":"markdown","cce85397":"markdown","74200211":"markdown","d6000eec":"markdown","25c2a10e":"markdown","dbe17aec":"markdown","2afff83e":"markdown"},"source":{"780b3fa5":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import cycle\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","dda6edde":"train_data = pd.read_csv('\/kaggle\/input\/nmlo-contest-3\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/nmlo-contest-3\/test.csv')\ntrain = train_data.copy()\ntest = test_data.copy()","13594de0":"train.head()","c04e7dc9":"train.info()","3e557fe4":"train.shape","2c7e65fc":"train.isnull().sum()","bb2296d2":"plt.figure(figsize=(15,5))\nplt.plot(train.cases,linewidth=2,color=next(color_cycle))\nplt.title('Distribution Plot for Covid Cases')\nplt.ylabel('# of people');","cee9b523":"plt.figure(figsize=(15,5))\nplt.plot(train.cases.sort_values().reset_index(drop=True),color=next(color_cycle))\nplt.title('Distribution Plot for Covid Cases')\nplt.ylabel('# of people');","942a3809":"sns.distplot(train['cases']);","e123966f":"train['cases'].max()","2f563159":"train.info()","86388b32":"var = 'ed'\ndata = pd.concat([train['cases'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='cases', ylim=(0,125000));","24c885f8":"var = 'inc'\ndata = pd.concat([train['cases'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='cases', ylim=(0,125000));","c243e93f":"#scatter plot grlivarea\/saleprice\nvar = 'pop'\ndata = pd.concat([train['cases'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='cases', ylim=(0,125000));","fd2fc230":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(10, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","ee3b1f7f":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'cases')['cases'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","4d1ad044":"from sklearn.preprocessing import StandardScaler","c8b6fef0":"cases_scaled = StandardScaler().fit_transform(train['cases'][:,np.newaxis]);\nlow_range = cases_scaled[cases_scaled[:,0].argsort()][:10]\nhigh_range= cases_scaled[cases_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","216f3978":"#bivariate analysis saleprice\/grlivarea\nvar = 'ed'\ndata = pd.concat([train['cases'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='cases', ylim=(0,250000));","30b01c7a":"var = 'pop'\ndata = pd.concat([train['cases'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='cases', ylim=(0,125000));","431e5469":"from scipy.stats import norm, skew\nfrom scipy import stats","b129400f":"test.head()","8be2e479":"train['cases'].replace([0], 1, inplace = True)\n\ntrain['ed'].replace([0], 1, inplace = True)\ntrain['inc'].replace([0], 1, inplace = True)\ntrain['pop'].replace([0], 1, inplace = True)\nprint((train['cases']==0).value_counts())\ntest['ed'].replace([0], 1, inplace = True)\n\ntest['inc'].replace([0], 1, inplace = True)\ntest['pop'].replace([0], 1, inplace = True)","b0427856":"\n\ntrain['cases'] = np.log(train['cases'])\n","d51810c9":"train['cases'].max()","eb742a9b":"res = stats.probplot(train['cases'], plot=plt)","58ed9c9d":"sns.distplot(train['ed'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['ed'], plot=plt)","117645fb":"#transformed histogram and normal probability plot\nsns.distplot(train['ed'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['ed'], plot=plt)\ntrain['ed'] = np.log(train['ed'])\ntest['ed'] = np.log(test['ed'])\nres = stats.probplot(train['ed'], plot=plt)","06b3962c":"res = stats.probplot(train['inc'], plot=plt)\ntrain['inc'] = np.log(train['inc'])\ntest['inc'] = np.log(test['inc'])\n","4a280e7b":"sns.distplot(train['inc'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['inc'], plot=plt)","d9120ef0":"res = stats.probplot(train['pop'], plot=plt)\n\ntrain['pop'] = np.log(train['pop'])\ntest['pop'] = np.log(test['pop'])\n","96bb0f38":"sns.distplot(train['pop'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['pop'], plot=plt)","88823c01":"plt.scatter(train['ed'], train['cases']);","d2e26721":"plt.scatter(train['inc'], train['cases']);","c0d06026":"plt.scatter(train['pop'], train['cases']);","48905ba7":"numeric_feats = train.dtypes[train.dtypes != \"object\"].index\n\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","350d7ac3":"#skewness = skewness[abs(skewness) > 0.75]\n#print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\n#from scipy.special import boxcox1p\n#skewed_features = skewness.index\n#lam = 0.15\n#for feat in skewed_features:\n#    #all_data[feat] += 1\n#    train[feat] = boxcox1p(train[feat], lam)","07893b3c":"train = pd.get_dummies(train)\nprint(train.shape)","1e7e269d":"train.head()","d5344110":"from sklearn.model_selection import train_test_split      ","99a28c9d":"train_y = train['cases']\ntrain.drop('id', axis = 1, inplace = True)\ntrain.drop('cases', axis = 1, inplace = True)","6b05bd7e":"X_train, X_test, y_train, y_test = train_test_split(train, train_y, test_size=0.2, shuffle=True)","f170f7ec":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","17c590a6":"n_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","0b799de8":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n","2d77a8a8":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n","af72bd98":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n","cb2eb129":"GBoost = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","be4b590a":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","cfdfb3de":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","e6065204":"model_xgb.fit(X_train, y_train)","1021182e":"xg = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.4603, gamma=0.0468,\n             gpu_id=-1, importance_type='gain', interaction_constraints='',\n             learning_rate=0.05, max_delta_step=0, max_depth=3,\n             min_child_weight=1.7817, missing=np.nan, monotone_constraints='()',\n             n_estimators=2200, n_jobs=-1, nthread=-1, num_parallel_tree=1,\n             random_state=7, reg_alpha=0.464, reg_lambda=0.8571,\n             scale_pos_weight=1, silent=1, subsample=0.5213,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n","d0b783b2":"score_lasso = rmsle_cv(lasso)\nscore_lasso.mean()\n#print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n","156affd0":"score = rmsle_cv(model_xgb)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","eb2f2173":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","58151fa6":"score = rmsle_cv(model_lgb)\nprint(\" score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","c58ff4e8":"score = rmsle_cv(lasso)\nprint(\" score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","07b900a9":"score = rmsle_cv(GBoost)\nGBoost.fit(X_train, y_train)\nsample = GBoost.predict(test)\nsample = np.exp(sample)\nprint(\" score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","78b87632":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","00515215":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\naveraged_models.fit(X_train, y_train)\nsubmission = averaged_models.predict(test)\nsubmission = np.exp(submission)\n#xgb_train_pred = averaged_models.predict(X_train)\n#xgb_pred = np.expm1(averaged_models.predict(test))\n#print(rmsle(y_train, xgb_train_pred))","112fdb90":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self","ba409e9b":"\n\nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","11a4dea1":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","6eb14247":"#stacked_averaged_models.fit(X_train, y_train)\n#stacked_train_pred = stacked_averaged_models.predict(X_train)\n#stacked_pred = np.expm1(stacked_averaged_models.predict(X_test))\n#print(rmsle(y_train, stacked_train_pred))","705fc15f":"test.shape\ntest.head()","38094f8a":"I decided to use the stock lasso regression. Xgboost and Lgb were both banned for this competition.","21f6f119":"sub = pd.DataFrame()\nsub['id'] = range(0,993)\nsub['cases'] = sample\nsub.to_csv('submission.csv',index=False)\nprint(sub)","a4428d91":"# **Lasso**","391cd3c8":"If you could drop an upvote that would be awesome !! <3","7703e863":"**Thank you for viewing my notebook!**\n**I hope you found this informative**\n\n","1d34834b":"# **EDA**","9b185eb0":"sns.distplot(train['cases'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['cases'], plot=plt)\n","97a88345":"Let's some some data exploration!","a6bfee5b":"**Looks like there is a huge spike of cases at the end.**","d78393a8":"**Lets first import some modules**.....","a11ea1e1":"![](https:\/\/ewscripps.brightspotcdn.com\/dims4\/default\/7ee8cb6\/2147483647\/strip\/true\/crop\/1280x720+0+0\/resize\/1280x720!\/quality\/90\/?url=http%3A%2F%2Fewscripps-brightspot.s3.amazonaws.com%2F72%2F52%2F83d95c7b4318a34a8373c722bc6b%2Fcoronavirus-background-1280x720-1.jpg)","1f2c6844":"# **Modeling**","09818026":"**Lets import**","e2d24bdc":"**Lets Get Coding!!**","cce85397":"# **Predictions**","74200211":"# **Imports**","d6000eec":"# **Introduction**","25c2a10e":"**Problem Statement**\n\nPredict the number of covid cases per region based off of demographic!","dbe17aec":"# **Skew**","2afff83e":"We are first going to split our data into different train and test subests using train-test-split"}}