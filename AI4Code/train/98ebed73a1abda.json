{"cell_type":{"b07543b0":"code","33804466":"code","0d414138":"code","2d88c7a2":"code","b384fe3f":"code","2020f5b9":"code","d8afd243":"code","949d7394":"code","49296287":"code","60b747e5":"code","8cd6e675":"code","1f2fcd9c":"code","dcdf23f3":"code","f9407218":"code","0c2d0166":"code","f3af8231":"code","3a6f1767":"code","fd0e8713":"code","d3649a7e":"code","3d3e0f0b":"code","53bf8454":"code","dbd1a9b0":"code","6382e499":"markdown","4f59a23f":"markdown","8c7fe427":"markdown","54389dec":"markdown","b9423874":"markdown","48a0484b":"markdown","c068a9d1":"markdown","6af31240":"markdown","3ec6e0fd":"markdown","5b8f4400":"markdown"},"source":{"b07543b0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","33804466":"df=pd.read_csv('\/kaggle\/input\/stock-sentiment-analysis\/Stock_Dataa.csv', encoding=\"ISO-8859-1\")\ndf.head()","0d414138":"train=df[df['Date']<'20150101']\ntest=df[df['Date']>'20141231']\ntrain.shape","2d88c7a2":"#Removing punctuations \ndata=train.iloc[:,2:27]\ndata.replace(\"[^a-zA-Z]\", \" \",regex=True, inplace=True)","b384fe3f":"data.columns","2020f5b9":"for col in data.columns:\n    data[col]=data[col].str.lower()\ndata.head(1)","d8afd243":"headlines = []\nfor row in range(0,len(data.index)):\n    headlines.append(' '.join(str(x) for x in data.iloc[row,0:25]))","949d7394":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier","49296287":"#implement TF-IDF\ntfvector=TfidfVectorizer(ngram_range=(2,3))\ntrain_df=tfvector.fit_transform(headlines)","60b747e5":"# implement RandomForest Classifier\nrandomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\nrandomclassifier.fit(train_df,train['Label'])","8cd6e675":"from sklearn import metrics\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    See full source and example: \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","1f2fcd9c":"# Predict for the Test Dataset\ntest_transform= []\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset = tfvector.transform(test_transform)\npredictions = randomclassifier.predict(test_dataset)","dcdf23f3":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nreport=classification_report(test['Label'],predictions)\nprint(report)\nplot_confusion_matrix(matrix, classes=['Down', 'Up'])","f9407218":"from sklearn.naive_bayes import MultinomialNB\nnb=MultinomialNB()\nnb.fit(train_df,train['Label'])","0c2d0166":"predictions = nb.predict(test_dataset)\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nreport=classification_report(test['Label'],predictions)\nprint(report)\nplot_confusion_matrix(matrix, classes=['Down', 'Up'])","f3af8231":"from sklearn.linear_model import PassiveAggressiveClassifier\npa = PassiveAggressiveClassifier()\npa.fit(train_df,train['Label'])","3a6f1767":"predictions = pa.predict(test_dataset)\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nreport=classification_report(test['Label'],predictions)\nprint(report)\nplot_confusion_matrix(matrix, classes=['Down', 'Up'])","fd0e8713":"from sklearn.feature_extraction.text import CountVectorizer\n#implement bag of words\nbow=CountVectorizer(ngram_range=(2,3))\ntrain_df=bow.fit_transform(headlines)","d3649a7e":"# implement RandomForest Classifier\nrandomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\nrandomclassifier.fit(train_df,train['Label'])","3d3e0f0b":"predictions = randomclassifier.predict(test_dataset)\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nreport=classification_report(test['Label'],predictions)\nprint(report)","53bf8454":"from sklearn.naive_bayes import MultinomialNB\nnb=MultinomialNB()\nnb.fit(train_df,train['Label'])\n\npredictions = nb.predict(test_dataset)\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nreport=classification_report(test['Label'],predictions)\nprint(report)\nplot_confusion_matrix(matrix, classes=['Down', 'Up'])","dbd1a9b0":"from sklearn.linear_model import PassiveAggressiveClassifier\npa = PassiveAggressiveClassifier()\npa.fit(train_df,train['Label'])\n\npredictions = pa.predict(test_dataset)\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nreport=classification_report(test['Label'],predictions)\nprint(report)\nplot_confusion_matrix(matrix, classes=['Down', 'Up'])","6382e499":"# RandomForestClassifier using Bag of words","4f59a23f":"# PassiveAggressiveClassifier using Bag of Words","8c7fe427":"# Using TF-IDF","54389dec":"# MultinomialNB using Bag of words","b9423874":"# RandomForestClassifier","48a0484b":"# Stock Sentiment Analysis using News Headlines.\n\n**About the problem and the dataset used.**\n* The data set in consideration is a combination of the world news and stock price shifts.\n* Data ranges from 2008 to 2016 and the data from 2000 to 2008 was scrapped from Yahoo finance.\n* There are 25 columns of top news headlines for each day in the data frame.\n* Class 1- the stock price increased.\n* Class 0- the stock price stayed the same or decreased.\n\n**About the approach.**\n* Used TF-IDF and Bag of Words for extracting featues from the headlines.\n* Used Random Forest Classifier, Multinational Naive Bayes and Passive Aggressive Classifier for analysis.","c068a9d1":"# plot_confusion_matrix","6af31240":"# MultinomialNB","3ec6e0fd":"# PassiveAggressiveClassifier","5b8f4400":"# Using bag of words"}}