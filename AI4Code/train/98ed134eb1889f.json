{"cell_type":{"595a1a14":"code","1a113cbc":"code","6d9fb86a":"code","9e2911cf":"code","53ecd478":"code","1dc153db":"code","0027d7e7":"code","aa6a6b44":"code","1bb45562":"code","51f1d984":"code","6d629b4e":"code","2fd54547":"code","2ce6af77":"code","7dd9e7a2":"code","57bb832c":"code","5dcf2a03":"code","9b16bd71":"code","274a53fc":"code","c1cd7790":"code","35e7cf89":"code","79c50aa2":"code","d8094c32":"code","c8d01202":"code","bf34b2f3":"code","111b6e45":"code","709986bb":"code","3c4187d7":"code","cc107654":"code","45b37d54":"code","6e38d8ab":"code","63ceada3":"code","05e57ada":"code","e7e45abc":"code","aec78b02":"code","2150f519":"code","34c774e7":"code","396c46c7":"code","d31f2f10":"code","42e70dbc":"code","447de926":"code","3fd5a85c":"code","755e07d3":"code","af58cfe4":"code","79a11341":"code","07490cfe":"code","ba2df764":"code","6710cd1c":"code","94c409a7":"code","4f11a5ba":"code","a2ef3600":"code","f9cab358":"code","b6bea6fb":"code","a508cd0c":"code","f2ef0162":"code","e8f1b423":"code","2b5cf5d1":"code","b0bc537d":"code","99c296b7":"code","3a939ffc":"code","e9f0f4f8":"code","1d1243b1":"code","15fb7e81":"code","8c2ab919":"code","74c2c6d6":"code","2b5f1d91":"code","9fc3922d":"code","ff9b6283":"code","e7bc604e":"code","23e3eb10":"code","5b99c8de":"code","b3ae55db":"code","26112c41":"code","8c850d6a":"code","ce1f27bf":"code","e435ba32":"code","977c6266":"code","596c6625":"markdown","31b66d73":"markdown","81889611":"markdown","e9ea46e0":"markdown","bc8aa54e":"markdown","5cc91473":"markdown","beda68cb":"markdown","4e98c455":"markdown","65bb49b2":"markdown","af420771":"markdown","786e9bae":"markdown","ed1973d4":"markdown","e28e26de":"markdown","bd337f17":"markdown","b4901abf":"markdown","57382c11":"markdown","4545e468":"markdown","031d573c":"markdown","72284292":"markdown","b3222f83":"markdown","672e1032":"markdown","2de0f4a2":"markdown","e7b15813":"markdown","ed47ade2":"markdown","e6f29840":"markdown","7f473792":"markdown","c73c4c8f":"markdown","17402bc0":"markdown","f0a28ed1":"markdown"},"source":{"595a1a14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom scipy import stats\nfrom scipy.stats import norm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a113cbc":"# Reading the train and test data\ntrain_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","6d9fb86a":"#checking out the first lines of my DataFrame with the help of the head() function\ntrain_data.head(n=5)","9e2911cf":"test_data.head(n=5)","53ecd478":"#looking number of rows and columns in the dataset\nprint('Rows and Columns of train data: ',train_data.shape)\nprint('Rows and Columns of test data: ',test_data.shape)\n# 1460 rows and 81 columns in the data","1dc153db":"# Get the overall concise summary of the DataFrame\n# Finding non null values and datatype of each column in train data set\ntrain_data.info()","0027d7e7":"# Finding number of missing values in each column of train data set\ntraindata_columns=[]\ntraindata_columns = train_data.columns.values\nfor column in traindata_columns:\n    if(train_data[column].isnull().sum()>0):\n        print(column,train_data[column].isnull().sum())","aa6a6b44":"## Here i am checking the percentage of missing values present in each feature\n\nfeatures_with_na=[features for features in train_data.columns if train_data[features].isnull().sum()>0]\n#step print the feature name and the percentage of missing values\nfor feature in features_with_na:\n    print(feature, np.round(train_data[feature].isnull().mean(), 4),  ' % missing values.\\n')","1bb45562":"# Checking numerical features\nnumerical_features = [feature for feature in train_data.columns if train_data[feature].dtype != 'O']\nprint(\"Number of numerical features: \", len(numerical_features))","51f1d984":"# finding the number of temporal features\ntemporal_features = [feature for feature in numerical_features if 'Year' in feature or 'Yr' in feature]\nprint(\"Number of temporal features: \", len(temporal_features))","6d629b4e":"#Discrete features\ndiscrete_features = [feature for feature in numerical_features if len(train_data[feature].unique()) <=25 \n                     and feature not in temporal_features + ['Id']]\nprint(\"Length of discrete features: \", len(discrete_features))","2fd54547":"#Continous features\ncontinuous_features = [feature for feature in numerical_features if feature not in discrete_features + temporal_features + ['Id']]\nprint(\"Length of continuous features: \", len(continuous_features))","2ce6af77":"# Finding the length of categorical features\ncategorial_features = [feature for feature in train_data.columns if train_data[feature].dtypes == 'O']\nprint(categorial_features)\nprint(\"Length of categorical features: \", len(categorial_features))","7dd9e7a2":"#checking percentage of missing values in categorial data\ncategorial_with_nan = [feature for feature in categorial_features if train_data[feature].isnull().sum() > 0]\nprint(categorial_with_nan)\nfor feature in categorial_with_nan:\n    print(\"Feature {}, has {}% missing values in train dataset\", (feature, np.round(train_data[feature].isnull().mean(), 4)))","57bb832c":"for feature in categorial_with_nan:\n    train_data[feature].fillna('Missing', inplace=True)","5dcf2a03":"print(numerical_features)","9b16bd71":"# Replace the missing values in train set with median\nnumerical_with_nan = [feature for feature in numerical_features if train_data[feature].isnull().sum() > 0]\nfor feature in numerical_with_nan:\n      train_data[feature].fillna(train_data[feature].median(), inplace=True)","274a53fc":"train_data.shape","c1cd7790":"# Lets see the relation between Fullbath and SalesPrice.\nfig, ax = plt.subplots()\nax.scatter(x = train_data['FullBath'], y = train_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('FullBath', fontsize=15)\nplt.show()\n","35e7cf89":"train_data[temporal_features].head()","79c50aa2":"for feature in temporal_features:\n    data = train_data.copy()\n    \n    train_data.groupby(feature)['SalePrice'].median().plot()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","d8094c32":"for feature in temporal_features:\n    data = train_data.copy()\n    \n    if feature != 'YrSold':\n        data[feature] = data['YrSold'] - data[feature]\n        plt.scatter(data[feature], data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.title(feature)\n        plt.show()","c8d01202":"# finding variables which are highly correlated (both positive and negative) with \u201cSalePrice\u201d.\ncorrmat = train_data.corr()\n\ndef getCorrelatedFeature(corrdata, threshold):\n    feature = []\n    value = []\n    for i , index in enumerate(corrdata.index):\n        if abs(corrdata[index]) > threshold:\n            feature.append(index)\n            value.append(corrdata[index])\n    df2 = pd.DataFrame(data = value, index=feature, columns=['corr value'] )\n    return df2\n\ncorr_df = getCorrelatedFeature(corrmat['SalePrice'], 0.5)\ncorr_df\n# Below features are highly correlated with SalePrice","bf34b2f3":"colormap = plt.cm.Blues\ncorrelated_data = train_data[corr_df.index]\nfig, ax = plt.subplots(figsize=(11, 11))\nsns.heatmap(correlated_data.corr(), annot = True, annot_kws={'size': 12}, square=True,cmap=colormap, linecolor='w', linewidths=0.1)","111b6e45":"# The Living area and Sale Price have roughly a linear relationship\nfig, ax = plt.subplots()\nax.scatter(x = train_data['GrLivArea'], y = train_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GrLivArea', fontsize=15)\nplt.show()","709986bb":"#Positive correlation between overallqual and salesprice\nsns.regplot(x='OverallQual', y='SalePrice', data=train_data, ci=None, scatter = False)","3c4187d7":"sns.boxplot(y='SalePrice', x = 'GarageArea', data=train_data)\n# The below plot clearly shows a linear relationship between \u2018SalePrice\u2019 and \u2018GarageArea\u2019. The \u2018SalePrice\u2019 increases with an increase in \u2018GarageArea\u2019.","cc107654":"# plot of \u2018YearBuilt\u2019 shows that the distribution is skewed towards the year 2000 and has a long tail which extends till 1900. The linear relationship between the variables is clearer in cases of recently built houses.\nsns.jointplot(x='SalePrice', y='YearBuilt', data=train_data, kind='reg', dropna = True)","45b37d54":"# TotalBsmtSF is very highly correlated with our target variable SalePrice and\nsns.jointplot(x='SalePrice', y='TotalBsmtSF', data=train_data, kind='scatter')","6e38d8ab":"sns.relplot(x='SalePrice', y='YearRemodAdd', data=train_data)\n# the YearRemodAdd also has a linear relationship with SalePrice.","63ceada3":"# It is highly negatively correlated with our target variable #Relational plot\nsns.relplot(x='SalePrice', y='FullBath', data=train_data)","05e57ada":"# Finding number of missing values for test data in each column\ntest_data_columns=[]\ntest_data_columns = test_data.columns.values\nfor column in test_data_columns:\n    if(test_data[column].isnull().sum()>0):\n        print(column,test_data[column].isnull().sum())","e7e45abc":"categorial_with_nan = [feature for feature in categorial_features if test_data[feature].isnull().sum() > 0]\nfor feature in categorial_with_nan:\n    test_data[feature].fillna('Missing', inplace=True)","aec78b02":"numerical_with_nan = [feature for feature in numerical_features if feature not in ['SalePrice'] and test_data[feature].isnull().sum() > 0]\nfor feature in numerical_with_nan:\n#     test[feature+'NaN'] = np.where(test[feature].isnull(), 1, 0)\n      test_data[feature].fillna(test_data[feature].median(), inplace=True)","2150f519":"test_data.head()","34c774e7":"print(\"Train Dataset\", train_data.shape)\nprint(\"Test Dataset\", test_data.shape)","396c46c7":"test_data[temporal_features].head()","d31f2f10":"num_non_zero_skewed_features_train_set = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ntrain_data[num_non_zero_skewed_features_train_set].head()","42e70dbc":"for feature in num_non_zero_skewed_features_train_set:\n    train_data[feature] = np.log(train_data[feature])","447de926":"num_non_zero_skewed_features_test_set = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\ntest_data[num_non_zero_skewed_features_test_set].head()","3fd5a85c":"for feature in num_non_zero_skewed_features_test_set:\n    test_data[feature] = np.log(test_data[feature])","755e07d3":"train_data[num_non_zero_skewed_features_train_set].head()","af58cfe4":"test_data[num_non_zero_skewed_features_test_set].head()","79a11341":"train1 = train_data.copy()\ntest1 = test_data.copy()","07490cfe":"data = pd.concat([train1,test1], axis=0)\ntrain_rows = train1.shape[0]\nprint(data.get('MSZoning'))\nfor feature in categorial_features:\n    dummy = pd.get_dummies(data[feature])\n    for col_name in dummy.columns:\n        dummy.rename(columns={col_name: feature+\"_\"+col_name}, inplace=True)\n    data = pd.concat([data, dummy], axis = 1)\n    data.drop([feature], axis = 1, inplace=True)\ntrain1 = data.iloc[:train_rows, :]\ntest1 = data.iloc[train_rows:, :] ","ba2df764":"train1.head()","6710cd1c":"test1.head()","94c409a7":"print(\"Train\",train1.shape)\nprint(\"Test\",test1.shape)","4f11a5ba":"from sklearn.preprocessing import MinMaxScaler\n\nscaling_features = [feature for feature in train1.columns if feature not in ['Id', 'SalePrice']]\nscaling_features","a2ef3600":"train1[scaling_features].head()","f9cab358":"print(len(scaling_features))","b6bea6fb":"train1[scaling_features].head()","a508cd0c":"scaler = MinMaxScaler()\nscaler.fit(train1[scaling_features])","f2ef0162":"X_train = scaler.transform(train1[scaling_features])\nX_test = scaler.transform(test1[scaling_features])","e8f1b423":"print(\"Train\", X_train.shape)\nprint(\"Test\", X_test.shape)","2b5cf5d1":"y_train = train1['SalePrice']","b0bc537d":"X = pd.concat([train1[['Id','SalePrice']].reset_index(drop=True), pd.DataFrame(X_train, columns = scaling_features)], axis =1)\nprint(X.shape)\nX.head()","99c296b7":"from sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n# from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.svm import SVR","3a939ffc":"# Training models using k-fold cross-validation and seeing the performance(RMSE) of all the models in the given dataset.\n# The below function rmse_cv is used to train models of the data created and it returns the RMSE score for the model based on the predictions compared with the actual predictions.\n\nn_folds = 10\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmsle(y_train, y_pred):\n    return np.sqrt(mean_squared_error(y_train, y_pred))","e9f0f4f8":"lasso = Lasso(alpha =0.0005, random_state=0)\nelasticNet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=0)\nkernelRidge = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nsvr = SVR(C= 20, epsilon= 0.008, gamma=0.0003)\ngradientBoosting = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =0)\nxgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =0, nthread = -1)\nlgbm = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11, random_state=0)\nrandomForest = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=0)","1d1243b1":"scores ={}","15fb7e81":"# Using Lasso to add the penalty equivalent to the absolute value of the sum of coefficients. This penalty is added to the \n# least square loss function and replaces the squared sum of coefficients from Ridge\nscore = rmsle_cv(lasso)\nprint(\"Lasso:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['lasso'] = (score.mean(), score.std())\nlasso_model = lasso.fit(X_train, y_train)\ny_pred_lasso = lasso_model.predict(X_train)\nrmsle(y_train,y_pred_lasso)","8c2ab919":"# Elastic Net is the combination of both Ridge and Lasso. It adds both the sum of squared coefficients \n# and the absolute sum of the coefficients with the ordinary least square function\nscore = rmsle_cv(elasticNet)\nprint(\"ElasticNet:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['elasticNet'] = (score.mean(), score.std())\nelasticNet_model = elasticNet.fit(X_train, y_train)\ny_pred_elasticNet = elasticNet_model.predict(X_train)\nrmsle(y_train,y_pred_elasticNet)","74c2c6d6":"# Kernel ridge regression is a non-parametric form of ridge regression. The aim is to learn a function in the space induced by the respective kernel \ud835\udc58 \n# by minimizing a squared loss with a squared norm regularization term.\nscore = rmsle_cv(kernelRidge)\nprint(\"KernelRidge:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['kernelRidge'] = (score.mean(), score.std())\nkernelRidge_model = kernelRidge.fit(X_train, y_train)\ny_pred_kernelRidge = kernelRidge_model.predict(X_train)\nrmsle(y_train,y_pred_kernelRidge)","2b5f1d91":"score = rmsle_cv(svr)\nprint(\"SVR:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['svr'] = (score.mean(), score.std())\nsvr_model = svr.fit(X_train, y_train)\ny_pred_svr = svr_model.predict(X_train)\nrmsle(y_train,y_pred_svr)","9fc3922d":"score = rmsle_cv(gradientBoosting)\nprint(\"GradientBoostingRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['gradientBoosting'] = (score.mean(), score.std())\ngradientBoosting_model = gradientBoosting.fit(X_train, y_train)\ny_pred_gradientBoosting = gradientBoosting_model.predict(X_train)\nrmsle(y_train,y_pred_gradientBoosting)","ff9b6283":"score = rmsle_cv(xgb)\nprint(\"XGBRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['xgb'] = (score.mean(), score.std())\nxgb_model = xgb.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_train)\nrmsle(y_train,y_pred_xgb)","e7bc604e":"score = rmsle_cv(lgbm)\nprint(\"LGBMRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['lgbm'] = (score.mean(), score.std())\nlgbm_model = lgbm.fit(X_train, y_train)\ny_pred_lgbm = lgbm_model.predict(X_train)\nrmsle(y_train,y_pred_lgbm)","23e3eb10":"score = rmsle_cv(randomForest)\nprint(\"RandomForestRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['randomForest'] = (score.mean(), score.std())\nrandomForest_model = randomForest.fit(X_train, y_train)\ny_pred_randomForest = randomForest_model.predict(X_train)\nrmsle(y_train,y_pred_randomForest)","5b99c8de":"def ensemble_models(X):\n    return ((0.1 * lasso_model.predict(X)) +\n            (0.1 * elasticNet_model.predict(X)) +\n           (0.1 * kernelRidge_model.predict(X)) +\n           (0.4 * gradientBoosting_model.predict(X)) + \n           (0.1 * xgb_model.predict(X)) +\n           (0.2 * lgbm_model.predict(X)))","b3ae55db":"averaged_score = rmsle(y_train, ensemble_models(X_train))\nscores['averaged'] = (averaged_score, 0)\nprint('RMSLE score on train data:', averaged_score)","26112c41":"sns.set_style(\"white\")\nfig = plt.figure(figsize=(20, 10))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\nplt.title('Scores of Models', size=20)\nplt.show()","8c850d6a":"\nsubmission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission.shape","ce1f27bf":"test_predict = np.exp(ensemble_models(X_test))\nprint(test_predict[:5])","e435ba32":"sub = pd.DataFrame()\nsub['Id'] = test_data['Id']\nsub['SalePrice'] = test_predict\nsub.to_csv('submission.csv',index=False)","977c6266":"sub1 = pd.read_csv('submission.csv')\nsub1.head()","596c6625":"# Advanced house price prediction\n## The goal is to predict the Sales Price for the dataset with the given features.","31b66d73":"#### Temporal Variables (Date-time variables)\nIn the train dataset we have 4 temporal variables","81889611":"# Explore Data","e9ea46e0":"# Performing data analysis on train data set","bc8aa54e":"### Read data","5cc91473":"* XGBRegressor is used to fine tune and retrieve optimal parameters to reduce rmse","beda68cb":"Now, let\u2019s have a look at the features that are highly correlated with the sale price","4e98c455":"We may assume the same numeric features will be skewed in test set as well.","65bb49b2":"Reference: https:\/\/www.kaggle.com\/charumakhijani\/house-price-prediction-top-6-on-leaderboard","af420771":"# Handling the missing values","786e9bae":" # Importing libraries","ed1973d4":"### Lets see the relation between temporal variables and SalesPrice.","e28e26de":"The first 3 plots here look fine as the recent the year house is built\/remodling done\/garage build, the higher the SalesPrice. But in the 4th plot Sales Price is decreasing as the Year is increasing. Ideally SalesPrice should increase with every passing year.\n\nSo lets see the relation between the first 3 year variables and the Year Sold","bd337f17":"# Scaling\n* Min-Max scaler is used to normalize the input features\/variables\n* now i am scaling data by creating an instance of the scaler and scaling it:","b4901abf":"* Using one of the machine learning technique called Gradient boosting algorithm to minimize the loss of the model","57382c11":"* Here we are \"regularizing\" models ability to structurally prevent overfitting by imposing a penalty on the coefficients.\n* Below models with the optimal hyperparameters were evaluated by comparing the predictions of each model with validation data. Each model was evaluated using the root mean square error (RMSE) of model predictions, which is a metric for describing the differences between the predicted values and the observed values for SalePrice. lower RMSE scores are better.","4545e468":"# Training data","031d573c":"'OverallQual', 'GrLivArea' and 'TotalBsmtSF' have strong correlation with 'SalePrice'","72284292":"# Visualize model scores\n* Plotting the predictions for each model","b3222f83":"# Stack Models\nEnsemble model: Here i am combining multiple models using ensemble model,to boost overall accuracy. \nThe combination can be implemented by aggregating the output from each model with two objectives: reducing the model error and maintaining its generalization.","672e1032":"* Here Iam using Random Forest regression since the target variable is a continuous real number and also it combines the prediction of multiple decision trees to get more accurate final prediction.","2de0f4a2":"* Filling the missed values in test data set","e7b15813":"#### It is observed that Sales price and fullbath is negatively correlated","ed47ade2":"# Prediction submission","e6f29840":"### Creating dummy data","7f473792":"* Since the numeric variables are skewed, we will perform log normal distribution.","c73c4c8f":"* We have fitted the model and seen its performance. Lets predict the prices for the houses in the actual test data.","17402bc0":"So above scatter plot indicates:\n\nThe lesser the difference between house YrSold and house year built\/remodling done\/garagebuilt, the higher the Sales Price. When Sales Price is less then it means the house is old with no\/not recent alterations done. So now we also know that \"Houses where faeture values are missing have comparatively low price\", because no remodelling or feature enhancements are done recently.","f0a28ed1":"# Analysing Test data set"}}