{"cell_type":{"fc9ae120":"code","f272ffba":"code","594c2bdb":"code","c01e608c":"code","ad6e7b5c":"code","73f5d3cb":"code","94021737":"code","e437b108":"code","26bf4bb2":"code","7c6a0fd8":"code","7420f56b":"code","03732229":"code","db8499e1":"code","a3baf541":"code","4124f588":"code","4105b762":"code","00563cd0":"code","18bdd6b8":"code","95e16328":"code","fc7f073f":"code","2946d0c2":"code","74e97b79":"code","1ea4d12a":"code","f49fb3ec":"markdown","612909ce":"markdown","f8b2ddb0":"markdown","5f5caa7b":"markdown","83a5b040":"markdown","62608f37":"markdown","f1442401":"markdown","cd5bda0d":"markdown","1b43917c":"markdown","1ea952da":"markdown","04d0b6ac":"markdown","dbb9ca03":"markdown","a9e18ea0":"markdown","32451484":"markdown","38587eaa":"markdown","603323cb":"markdown","41fca8b7":"markdown","091d6b82":"markdown","3d65a566":"markdown","f6049e64":"markdown","5a8c2b4d":"markdown","5e39dc02":"markdown"},"source":{"fc9ae120":"# You can edit the font size here to make rendered text more comfortable to read\n# It was built on a 13\" retina screen with 18px\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.rendered_html { font-size: 18px; }<\/style>\"))","f272ffba":"%matplotlib inline\n\nimport os\nfrom collections import Counter, OrderedDict\nimport numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nfrom astropy.table import Table\nimport multiprocessing\nfrom cesium.time_series import TimeSeries\nimport cesium.featurize as featurize\nfrom tqdm import tnrange, tqdm_notebook\nimport sklearn \nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns","594c2bdb":"pbmap = OrderedDict([(0,'u'), (1,'g'), (2,'r'), (3,'i'), (4, 'z'), (5, 'Y')])\n\n# it also helps to have passbands associated with a color\npbcols = OrderedDict([(0,'blueviolet'), (1,'green'), (2,'red'),\\\n                      (3,'orange'), (4, 'black'), (5, 'brown')])\n\npbnames = list(pbmap.values())","c01e608c":"datadir = '..\/input\/plasticc-astronomy-starter-kit-media'\nmetafilename = '..\/input\/PLAsTiCC-2018\/training_set_metadata.csv'\nmetadata = Table.read(metafilename, format='csv')\nnobjects = len(metadata)\nmetadata","ad6e7b5c":"extragal = metadata['hostgal_specz'] != 0.\ng = sns.jointplot(metadata['hostgal_specz'][extragal],\\\n              metadata['hostgal_photoz'][extragal], kind='hex',\\\n                  xlim=(-0.01, 3.01), ylim=(-0.01,3.01), height=8)\n\noutliers = np.abs(metadata['hostgal_specz'] - metadata['hostgal_photoz']) > 0.1\nfig = g.fig\nfig.axes[0].scatter(metadata['hostgal_specz'][outliers],\\\n                    metadata['hostgal_photoz'][outliers], color='C3', alpha=0.05)\nfig.tight_layout()","73f5d3cb":"counts = Counter(metadata['target'])\nlabels, values = zip(*sorted(counts.items(), key=itemgetter(1)))\nfig, ax = plt.subplots()\n\ncmap = plt.cm.tab20\nnlines = len(labels)\nclasscolor =  list(cmap(np.linspace(0,1,nlines)))[::-1]\n\n# we'll create a mapping between class and color\nclasscolmap = dict(zip(labels, classcolor))\n\nindexes = np.arange(nlines)\nwidth = 1\nax.bar(indexes, values, width, edgecolor='k',\\\n       linewidth=1.5, tick_label=labels, log=True, color=classcolor)\nax.set_xlabel('Target')\nax.set_ylabel('Number of Objects')\nfig.tight_layout()","94021737":"lcfilename = '..\/input\/PLAsTiCC-2018\/training_set.csv'\nlcdata = Table.read(lcfilename, format='csv')\nlcdata","e437b108":"tsdict = OrderedDict()\nfor i in tnrange(nobjects, desc='Building Timeseries'):\n    row = metadata[i]\n    thisid = row['object_id']\n    target = row['target']\n    \n    meta = {'z':row['hostgal_photoz'],\\\n            'zerr':row['hostgal_photoz_err'],\\\n            'mwebv':row['mwebv']}\n    \n    ind = (lcdata['object_id'] == thisid)\n    thislc = lcdata[ind]\n\n    pbind = [(thislc['passband'] == pb) for pb in pbmap]\n    t = [thislc['mjd'][mask].data for mask in pbind ]\n    m = [thislc['flux'][mask].data for mask in pbind ]\n    e = [thislc['flux_err'][mask].data for mask in pbind ]\n\n    tsdict[thisid] = TimeSeries(t=t, m=m, e=e,\\\n                        label=target, name=thisid, meta_features=meta,\\\n                        channel_names=pbnames )\n    \ndel lcdata","26bf4bb2":"features_to_use = [\"amplitude\",\n                   \"percent_beyond_1_std\",\n                   \"maximum\",\n                   \"max_slope\",\n                   \"median\",\n                   \"median_absolute_deviation\",\n                   \"percent_close_to_median\",\n                   \"minimum\",\n                   \"skew\",\n                   \"std\",\n                   \"weighted_average\"]","7c6a0fd8":"# we'll turn off warnings for a bit, because numpy can be whiny. \nimport warnings\nwarnings.simplefilter('ignore')","7420f56b":"def worker(tsobj):\n    global features_to_use\n    thisfeats = featurize.featurize_single_ts(tsobj,\\\n    features_to_use=features_to_use,\n    raise_exceptions=False)\n    return thisfeats","03732229":"featurefile = f'{datadir}\/plasticc_featuretable.npz'\nif os.path.exists(featurefile):\n    featuretable, _ = featurize.load_featureset(featurefile)\nelse:\n    features_list = []\n    with tqdm_notebook(total=nobjects, desc=\"Computing Features\") as pbar:\n        with multiprocessing.Pool() as pool:  \n            results = pool.imap(worker, list(tsdict.values()))\n            for res in results:\n                features_list.append(res)\n                pbar.update()\n            \n    featuretable = featurize.assemble_featureset(features_list=features_list,\\\n                              time_series=tsdict.values())\n    featurize.save_featureset(fset=featuretable, path=featurefile)","db8499e1":"old_names = featuretable.columns.values\nnew_names = ['{}_{}'.format(x, pbmap.get(y,'meta')) for x,y in old_names]\ncols = [featuretable[col] for col in old_names]\nallfeats = Table(cols, names=new_names)\ndel featuretable","a3baf541":"splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\nsplits = list(splitter.split(allfeats, metadata['target']))[0]\ntrain_ind, test_ind = splits","4124f588":"corr = allfeats.to_pandas().corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Draw the heatmap with the mask and correct aspect ratio\ncorr_plot = sns.heatmap(corr, mask=mask, cmap='RdBu', center=0,\n                square=True, linewidths=.2, cbar_kws={\"shrink\": .5})","4105b762":"Xtrain = np.array(allfeats[train_ind].as_array().tolist())\nYtrain = np.array(metadata['target'][train_ind].tolist())\n\nXtest  = np.array(allfeats[test_ind].as_array().tolist())\nYtest  = np.array(metadata['target'][test_ind].tolist())","00563cd0":"ncols = len(new_names)\nnpca  = (ncols  - 3)\/\/len(pbnames)  + 3","18bdd6b8":"pca = PCA(n_components=npca, whiten=True, svd_solver=\"full\", random_state=42)\nXtrain_pca = pca.fit_transform(Xtrain)\nXtest_pca = pca.transform(Xtest)","95e16328":"fig, ax = plt.subplots()\nax.plot(np.arange(npca), pca.explained_variance_ratio_, color='C0')\nax2 = ax.twinx()\nax2.plot(np.arange(npca), np.cumsum(pca.explained_variance_ratio_), color='C1')\nax.set_yscale('log')\nax.set_xlabel('PCA Component')\nax.set_ylabel('Explained Variance Ratio')\nax2.set_ylabel('Cumulative Explained Ratio')\nfig.tight_layout()","fc7f073f":"clf = RandomForestClassifier(n_estimators=200, criterion='gini',\\\n                       oob_score=True, n_jobs=-1, random_state=42,\\\n                      verbose=1, class_weight='balanced', max_features='sqrt')","2946d0c2":"clf.fit(Xtrain_pca, Ytrain)\nYpred = clf.predict(Xtest_pca)","74e97b79":"cm = confusion_matrix(Ytest, Ypred, labels=labels)\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nannot = np.around(cm, 2)","1ea4d12a":"fig, ax = plt.subplots(figsize=(9,7))\nsns.heatmap(cm, xticklabels=labels, yticklabels=labels, cmap='Blues', annot=annot, lw=0.5)\nax.set_xlabel('Predicted Label')\nax.set_ylabel('True Label')\nax.set_aspect('equal')","f49fb3ec":"Which we can plot up...","612909ce":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-PLAsTiCC-Astronomy-&quot;Starter-Kit&quot;-Classification-Demo\" data-toc-modified-id=\"The-PLAsTiCC-Astronomy-&quot;Starter-Kit&quot;-Classification-Demo-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span><strong>The PLAsTiCC Astronomy \"Starter Kit\" Classification Demo<\/strong><\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-Gautham-Narayan,-Ren\u00e9e-Hlo\u017eek,-Emilie-Ishida-20180831\" data-toc-modified-id=\"-Gautham-Narayan,-Ren\u00e9e-Hlo\u017eek,-Emilie-Ishida-20180831-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;<\/span>-Gautham Narayan, Ren\u00e9e Hlo\u017eek, Emilie Ishida 20180831<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Resources<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Useful-Packages-for-Astrophysics---particularly-feature-extraction\" data-toc-modified-id=\"Useful-Packages-for-Astrophysics---particularly-feature-extraction-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;<\/span>Useful Packages for Astrophysics - particularly feature extraction<\/a><\/span><\/li><li><span><a href=\"#External-Data-Sources\" data-toc-modified-id=\"External-Data-Sources-2.0.0.2\"><span class=\"toc-item-num\">2.0.0.2&nbsp;&nbsp;<\/span>External Data Sources<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/div>","f8b2ddb0":"Since there's multiple passbands in the data, and the passbands are all sampling the same underlying astrophysical event, it's reasonable to expect that the features we've computed to be correlated. \n\nTo prevent overfitting, it's a probably a good idea to reduce the dimensionality of the dataset. We'll look at this correlation structure.","5f5caa7b":"Looks like a few features seem to hold all the weight! You might think to yourself that this is trivial! Lets build a simple classifier - we'll use a random forest here, but you can switch this out for whatever you like.","83a5b040":"Now let's apply the classifier to the data:","62608f37":"In this Kernel, we'll look at how astronomers have approached light curve classification, and provide you some references and useful packages if you want to get a sense for what has been done in the past.\n\nIt's almost certain that some astronomers will approach PLAsTiCC with these kinds of methods, so if you want to win, you'll likely have to do something better!\n\nWe'll begin by importing some packages that might be useful. ","f1442401":"You can see the class distribution in the training set is imbalanced. This reflects reality. The Universe doesn't produce all kinds of events at equal rates, and even if it did, some events are fainter than others, so we'd naturally find fewer of them than bright events.\n\nNext, we'll read the light curve data. All the objects in the training set are in a single file:","cd5bda0d":"We'll start computing the features. This takes a while, so it's good to do this only the once, and save state. \n\nIn principle you can avoid loading the data even if you run this again, but it's always a good idea to have access to your raw data while doing exploratory data analysis.\n\nIf you have access to a multiprocessing capabilities, it is usually a good idea to divide the feature computation task up. Particularly when you are dealing with the test set with about 3.5 million objects... \n\nWe'll use whatever cores you have on your machine.","1b43917c":"Again, find us on the Kaggle forums if you have questions we can answer.\n\nGood luck!","1ea952da":"You can see some of the bands are strongly correlated with each other, so lets try to reduce the dimensionality of the dataset looking for a two effective passbands.","04d0b6ac":"The list of features available with packages like `cesium` is <a href=\"http:\/\/cesium-ml.org\/docs\/feature_table.html\">huge<\/a>. We'll only compute a subset now.","dbb9ca03":"There's many potential choices. \n\nFor now we'll leave everything together.","a9e18ea0":"The computed feature table has descriptive statistics for each object - a low-dimensional encoding of the information in the light curves. `cesium` assembles it as a `MultiIndex` which does not make `sklearn` happy, so we'll make a simpler table out of it.","32451484":"And see just how well it worked by constructing a confusion matrix.","38587eaa":"# **The PLAsTiCC Astronomy \"Starter Kit\" Classification Demo**\n\n### -Gautham Narayan, Ren\u00e9e Hlo\u017eek, Emilie Ishida 20180831\n\nThis kernel was developed for PLAsTiCC on Kaggle. The original version of this kernel is available as a Jupyter Notebook on <a href=\"https:\/\/github.com\/LSSTDESC\/plasticc-kit\">LSST DESC GitHub<\/a>. ","603323cb":"Since we can split between extragalactic and galactic sources on the basis of redshift, you may choose to build separate classifiers for the two sets. \n\nOr you might split the deep-drilling fields (DDF i.e. `ddf_bool` = 1) up from the wide-fast-deep fields (WFD i.e. `ddf_bool` = 0). \n\nYou could even try making classifiers for different redshift bins, if you believe some classes will not be present at some redshifts. \n\nOr you might use that some of the training _and test_ data includes spectroscopic and photometric redshift, and see if you can use that information to derive bias corrections and apply non-linear transformations to the data before processing it.\n\nBecause there are definitely biases... \n\n(Also, if you made this kind of redshift-redshift plot from the test data, you'd see how non-representative the training set was.)","41fca8b7":"# Resources\n\n|Reference   | Light Curve fit  | Dimensionality <br> Reduction  | Classification <br> algorithm  | Use redshift  | Source Code | \n|---|---|---|---|---|---|\n|[Poznanski *et al*, 2006](https:\/\/arxiv.org\/pdf\/astro-ph\/0610129.pdf)| --- | --- | Template fit | Yes | [pSNiD II](https:\/\/www.sas.upenn.edu\/~gladney\/html-physics\/psnid\/psnidII\/)|\n|[Newling *et al*, 2010](https:\/\/arxiv.org\/pdf\/1010.1005.pdf)   | parametric | parameters from fit  | Kernel Density Estimation <br> Boosting  | Yes  | No |\n|[Richards *et al*, 2011](https:\/\/arxiv.org\/pdf\/1103.6034.pdf)   | spline  | diffusion maps  | Random Forest  | Yes  | No |\n|[Karpenka *et al*, 2012](https:\/\/arxiv.org\/pdf\/1208.1264.pdf)   |parametric | parameters from fit  | Neural Network  | No  | No |\n|[Ishida & de Souza, 2013](https:\/\/arxiv.org\/pdf\/1201.6676.pdf)| spline | kernel PCA | Nearest Neighbor | No |[github](https:\/\/github.com\/emilleishida\/snclass) |\n|[Mislis *et al*, 2015](https:\/\/arxiv.org\/abs\/1511.03456) | --- | descriptive statistics | Random Forest | No | No | \n|[Varughese *et al*, 2015](https:\/\/arxiv.org\/pdf\/1504.00015.pdf)| spline | Wavelets | Nearest Neighbor <br> Support Vector Machine | No| No |\n|[Hernitschek *et al*, 2016](http:\/\/iopscience.iop.org\/article\/10.3847\/0004-637X\/817\/1\/73\/meta)| $\\chi^{2}$ | --- | Random Forest | No | No | \n|[Lochner *et al*, 2016](https:\/\/arxiv.org\/pdf\/1603.00882.pdf)|parametric <br> Gaussian Process | Wavelets<br> PCA <br> Model Fit | Naive Bayes<br> Nearest Neighbor <br> Support Vector Machine <br> Boosted Decision Trees |No| No |\n|[Moller *et al*, 2016](https:\/\/arxiv.org\/pdf\/1608.05423.pdf) | parametric | parameters from fit| Boosted Decision Trees  <br> Random Forest | Yes | No |\n|[Charnok and Moss, 2017](https:\/\/arxiv.org\/pdf\/1606.07442.pdf)| --- | --- | Recurrent Neural Network | No|[github](https:\/\/github.com\/adammoss\/supernovae) |\n|[Mahabal *et al*, 2017](https:\/\/arxiv.org\/abs\/1709.06257)| rate of change | ---| Neural Network | No | No |\n|[Narayan *et al*, 2018](https:\/\/arxiv.org\/abs\/1801.07323) | parametric <br> Gaussian Process <br> | Wavelets<br> PCA <br> | Random Forest | No | No |\n|[Revsbech *et al*, 2018](https:\/\/arxiv.org\/pdf\/1706.03811.pdf)|Gaussian Process|Diffusion Maps| Random Forest |Yes| [github](https:\/\/github.com\/rtrotta\/STACCATO)|\n| [Dai *et al*, 2018](https:\/\/arxiv.org\/pdf\/1701.05689.pdf)|parametric|parameters from fit| Random Forest| No| No|\n\nWe're not suggesting reading all of these papers in detail and the references therein, but they will give you an overview of the sort of techniques astronomers have tried in the past, and will likely employ in this challenge.\n\n\n#### Useful Packages for Astrophysics - particularly feature extraction\n\n- [astropy](http:\/\/www.astropy.org\/)\n- [astroML](http:\/\/www.astroml.org\/)\n- [cesium](https:\/\/github.com\/cesium-ml\/cesium)\n- [celerite](https:\/\/celerite.readthedocs.io\/en\/stable\/)\n- [FATS](https:\/\/github.com\/isadoranun\/FATS)\n- [feets](https:\/\/github.com\/carpyncho\/feets)\n- [gatspy](https:\/\/www.astroml.org\/gatspy\/)\n- [sncosmo](http:\/\/sncosmo.github.io\/)\n- [tsfresh](https:\/\/tsfresh.readthedocs.io\/en\/latest\/)\n- [vartools](https:\/\/www.astro.princeton.edu\/~jhartman\/vartools.html)\n\nAll of these packages were developed for astrophysics and in particular light curve processing. Celerite is a general 1-D Gaussian Process package but the particular class of kernels it offers is suitable for astrophysics and allows it to be much faster than many other packages.\n\n#### External Data Sources\n\nIf you are determined to try and augment the training set, or to learn more about the astrophysical sources in the data before building your data, then these resources might be useful to you. Note that you build in what biases are present in the training set into your classifier.\n\n- [AAVSO](https:\/\/www.aavso.org\/research-portal)\n- [Open Astronomy Catalogs](https:\/\/astrocats.space\/)\n- [OGLE Collection of Variable Stars](http:\/\/ogledb.astrouw.edu.pl\/~ogle\/OCVS\/)\n- [SDSS DR12 Variables and Transients](https:\/\/www.sdss.org\/dr12\/algorithms\/ancillary\/boss\/transient82\/)\n\nRemember that LSST may discover objects that are predicted but have never been seen yet. We can't give you examples of things that haven't been seen of course.\n\n***","091d6b82":"Next, we'll make a `Timeseries` object using the `cesium` python package for each lightcurve.","3d65a566":"Next, we'll read the metadata table.","f6049e64":"Ooer... Some classes seem to be easy to pick out. Others are being confused very badly. This reflects what happens in reality to astronomers. Some classes are easier to distinguish than others. \n\nYou can also see the performance (or lack thereof) on the test data. \n\nNow this classifier is not the best we can do (by far), and it should be trivial for you to beat, but the fact is we can definitely use your expertise on this problem. There's many methods you can use to tackle this problem, and how you approach different aspects is going to be interesting to us regardless of how you do on the Kaggle leader board.\n\nFrom data augmentation to feature engineering to re-balancing classes to outlier identification and hierarchical multi-class probabilistic classification, there's much we can learn from this.\n\nWe'll leave you with a few resources and references that might be useful to tackle this challenge:","5a8c2b4d":"We described how the the data is split into a light curve table and a metadata table in the astronomy starter kit. We've described the format of these files there, so if you need a refresher, take a look there again.\n\nWe described a mapping from string passband name to integer passband IDs. Here, we'll create an `OrderedDict` to invert that transformation. ","5e39dc02":"We'll split the training set into two - one for training in this demo and the other for testing.\n\nWe'll do this preserving class balance in the full training set, but remember the training set might not be representative. The class balance in the training set reflects whatever we've followed up spectroscopically prior to LSST science operations starting."}}