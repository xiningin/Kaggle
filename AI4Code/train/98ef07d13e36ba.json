{"cell_type":{"2ffd0eb9":"code","df298648":"code","d3d45616":"code","15655401":"code","c61d06c8":"code","de709571":"code","7a97266d":"code","983c2f38":"code","fa59f2da":"code","dea62d71":"code","b6dea429":"code","0097d417":"code","a201b1bf":"code","f6365e25":"code","607f95eb":"code","d523d5dc":"code","ad3c494f":"code","b0af5214":"code","6aa5cdcf":"code","c960331f":"code","98522fda":"code","21151d93":"code","b970e839":"code","d242d41a":"code","1cb44167":"code","c8f54e77":"code","6b7878cf":"code","d5e660b0":"code","1fdb6a7c":"code","6e605379":"code","42a45f93":"code","2f80f715":"code","bba66f76":"code","eb8ac77e":"code","e23a705a":"code","b162f592":"code","ee387e79":"code","a31c8bfc":"code","d80cbf7f":"code","faa4e710":"code","fe019825":"code","c09a01ba":"code","5638f3ab":"code","3d2bc774":"code","1ce37ae7":"code","4c960319":"code","2bd41894":"code","64252c19":"code","4a7bc736":"code","3fa6c6fd":"code","a519cccb":"code","76759ffd":"markdown","3878d745":"markdown","041597b2":"markdown","242e37c7":"markdown","eabbfcb8":"markdown","07b253f1":"markdown","63d873e5":"markdown","47d737d7":"markdown","07af9390":"markdown","6c64de03":"markdown","50eee6f5":"markdown","3ffdada8":"markdown","7608a83c":"markdown","a2bb8f0a":"markdown","b214636c":"markdown","4132c9fc":"markdown","7a840ff7":"markdown","fe9979ea":"markdown","ef70b38d":"markdown","ae1033f8":"markdown","41b9d1c3":"markdown","a07635ec":"markdown","01fc7f3d":"markdown","3ac2cc46":"markdown","f2c1cccb":"markdown","b338db9e":"markdown","7261517b":"markdown","f80a4be6":"markdown"},"source":{"2ffd0eb9":"#Importing necessary packages in Python \n%matplotlib inline \nimport matplotlib.pyplot as plt \n\nimport numpy as np ; np.random.seed(sum(map(ord, \"aesthetics\")))\nimport pandas as pd\n\nfrom sklearn.datasets import make_classification \nfrom sklearn.learning_curve import learning_curve \n#from sklearn.cross_validation import train_test_split \n#from sklearn.grid_search import GridSearchCV\n#from sklearn.cross_validation import ShuffleSplit\nfrom sklearn.metrics import classification_report,confusion_matrix, roc_curve, roc_auc_score, auc, accuracy_score\nfrom sklearn.model_selection import ShuffleSplit,train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize, StandardScaler, MinMaxScaler\n\nimport seaborn \nseaborn.set_context('notebook') \nseaborn.set_style(style='darkgrid')\n\nfrom pprint import pprint \n \n","df298648":"# Function for evaluation reports\ndef get_eval1(clf, X,y):\n    # Cross Validation to test and anticipate overfitting problem\n    scores1 = cross_val_score(clf, X, y, cv=2, scoring='accuracy')\n    scores2 = cross_val_score(clf, X, y, cv=2, scoring='precision')\n    scores3 = cross_val_score(clf, X, y, cv=2, scoring='recall')\n    scores4 = cross_val_score(clf, X, y, cv=2, scoring='roc_auc')\n    \n    # The mean score and standard deviation of the score estimate\n    print(\"Cross Validation Accuracy: %0.2f (+\/- %0.2f)\" % (scores1.mean(), scores1.std()))\n    print(\"Cross Validation Precision: %0.2f (+\/- %0.2f)\" % (scores2.mean(), scores2.std()))\n    print(\"Cross Validation Recall: %0.2f (+\/- %0.2f)\" % (scores3.mean(), scores3.std()))\n    print(\"Cross Validation roc_auc: %0.2f (+\/- %0.2f)\" % (scores4.mean(), scores4.std()))\n    \n    return \n\ndef get_eval2(clf, X_train, y_train,X_test, y_test):\n    # Cross Validation to test and anticipate overfitting problem\n    scores1 = cross_val_score(clf, X_test, y_test, cv=2, scoring='accuracy')\n    scores2 = cross_val_score(clf, X_test, y_test, cv=2, scoring='precision')\n    scores3 = cross_val_score(clf, X_test, y_test, cv=2, scoring='recall')\n    scores4 = cross_val_score(clf, X_test, y_test, cv=2, scoring='roc_auc')\n    \n    # The mean score and standard deviation of the score estimate\n    print(\"Cross Validation Accuracy: %0.2f (+\/- %0.2f)\" % (scores1.mean(), scores1.std()))\n    print(\"Cross Validation Precision: %0.2f (+\/- %0.2f)\" % (scores2.mean(), scores2.std()))\n    print(\"Cross Validation Recall: %0.2f (+\/- %0.2f)\" % (scores3.mean(), scores3.std()))\n    print(\"Cross Validation roc_auc: %0.2f (+\/- %0.2f)\" % (scores4.mean(), scores4.std()))\n    \n    return  \n  \n# Function to get roc curve\ndef get_roc (y_test,y_pred):\n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    fpr, tpr, _ = roc_curve(y_test, y_pred)\n    roc_auc = auc(fpr, tpr)\n    #Plot of a ROC curve\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"upper left\")\n    plt.show()\n    return\n","d3d45616":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n#print('XGBoost v',xgb.__version__)\n\n# fit, train and cross validate Decision Tree with training and test data \ndef xgbclf(params, X_train, y_train,X_test, y_test):\n  \n    eval_set=[(X_train, y_train), (X_test, y_test)]\n    \n    model = XGBClassifier(**params).\\\n      fit(X_train, y_train, eval_set=eval_set, \\\n                  eval_metric='auc', early_stopping_rounds = 100, verbose=100)\n        \n    #print(model.best_ntree_limit)\n\n    model.set_params(**{'n_estimators': model.best_ntree_limit})\n    model.fit(X_train, y_train)\n    #print(model,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = model.predict(X_test, ntree_limit=model.best_ntree_limit) #model.best_iteration\n    #print(y_pred)\n   \n    # Get Cross Validation and Confusion matrix\n    #get_eval(model, X_train, y_train)\n    #get_eval2(model, X_train, y_train,X_test, y_test)\n    \n    # Create and print confusion matrix    \n    abclf_cm = confusion_matrix(y_test,y_pred)\n    print(abclf_cm)\n    \n    #y_pred = model.predict(X_test)\n    print (classification_report(y_test,y_pred) )\n    print ('\\n')\n    print (\"Model Final Generalization Accuracy: %.6f\" %accuracy_score(y_test,y_pred) )\n    \n    # Predict probabilities target variables y for test data\n    y_pred_proba = model.predict_proba(X_test, ntree_limit=model.best_ntree_limit)[:,1] #model.best_iteration\n    get_roc (y_test,y_pred_proba)\n    return model\n\ndef plot_featureImportance(model, keys):\n  importances = model.feature_importances_\n\n  importance_frame = pd.DataFrame({'Importance': list(importances), 'Feature': list(keys)})\n  importance_frame.sort_values(by = 'Importance', inplace = True)\n  importance_frame.tail(10).plot(kind = 'barh', x = 'Feature', figsize = (8,8), color = 'orange')","15655401":"file = '..\/input\/germancreditdata\/german.data'\nurl = \"http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/statlog\/german\/german.data\"\n\nnames = ['existingchecking', 'duration', 'credithistory', 'purpose', 'creditamount', \n         'savings', 'employmentsince', 'installmentrate', 'statussex', 'otherdebtors', \n         'residencesince', 'property', 'age', 'otherinstallmentplans', 'housing', \n         'existingcredits', 'job', 'peopleliable', 'telephone', 'foreignworker', 'classification']\n\ndata = pd.read_csv(file,names = names, delimiter=' ')\nprint(data.shape)\nprint (data.columns)\ndata.head(10)","c61d06c8":"# Binarize the y output for easier use of e.g. ROC curves -> 0 = 'bad' credit; 1 = 'good' credit\ndata.classification.replace([1,2], [1,0], inplace=True)\n# Print number of 'good' credits (should be 700) and 'bad credits (should be 300)\ndata.classification.value_counts()","de709571":"#numerical variables labels\nnumvars = ['creditamount', 'duration', 'installmentrate', 'residencesince', 'age', \n           'existingcredits', 'peopleliable', 'classification']\n\n# Standardization\nnumdata_std = pd.DataFrame(StandardScaler().fit_transform(data[numvars].drop(['classification'], axis=1)))","7a97266d":"from collections import defaultdict\n\n#categorical variables labels\ncatvars = ['existingchecking', 'credithistory', 'purpose', 'savings', 'employmentsince',\n           'statussex', 'otherdebtors', 'property', 'otherinstallmentplans', 'housing', 'job', \n           'telephone', 'foreignworker']\n\nd = defaultdict(LabelEncoder)\n\n# Encoding the variable\nlecatdata = data[catvars].apply(lambda x: d[x.name].fit_transform(x))\n\n# print transformations\nfor x in range(len(catvars)):\n    print(catvars[x],\": \", data[catvars[x]].unique())\n    print(catvars[x],\": \", lecatdata[catvars[x]].unique())\n\n#One hot encoding, create dummy variables for every category of every categorical variable\ndummyvars = pd.get_dummies(data[catvars])","983c2f38":"data_clean = pd.concat([data[numvars], dummyvars], axis = 1)\n\nprint(data_clean.shape)","fa59f2da":"# Unscaled, unnormalized data\nX_clean = data_clean.drop('classification', axis=1)\ny_clean = data_clean['classification']\nX_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean,y_clean,test_size=0.2, random_state=1)","dea62d71":"X_train_clean.keys()","b6dea429":"params={}\nxgbclf(params, X_train_clean, y_train_clean, X_test_clean, y_test_clean)","0097d417":"params={}\n\nparams1={\n    'n_estimators':3000,\n    'objective': 'binary:logistic',\n    'learning_rate': 0.05,\n    'gamma':0.1,\n    'subsample':0.8,\n    'colsample_bytree':0.3,\n    'min_child_weight':3,\n    'max_depth':3,\n    #'seed':1024,\n    'n_jobs' : -1\n}\n\nparams2={\n    'n_estimators':3000,\n    'objective': 'binary:logistic',\n    'learning_rate': 0.005,\n    #'gamma':0.01,\n    'subsample':0.555,\n    'colsample_bytree':0.7,\n    'min_child_weight':3,\n    'max_depth':8,\n    #'seed':1024,\n    'n_jobs' : -1\n}\n\nxgbclf(params2, X_train_clean, y_train_clean, X_test_clean, y_test_clean)","a201b1bf":"\nfrom imblearn.over_sampling import SMOTE\n\n# Oversampling\n# http:\/\/contrib.scikit-learn.org\/imbalanced-learn\/auto_examples\/combine\/plot_smote_enn.html#sphx-glr-auto-examples-combine-plot-smote-enn-py\n\n# Apply SMOTE\nsm = SMOTE(ratio='auto')\nX_train_clean_res, y_train_clean_res = sm.fit_sample(X_train_clean, y_train_clean)\n\n# Print number of 'good' credits and 'bad credits, should be fairly balanced now\nprint(\"Before\/After clean\")\nunique, counts = np.unique(y_train_clean, return_counts=True)\nprint(dict(zip(unique, counts)))\nunique, counts = np.unique(y_train_clean_res, return_counts=True)\nprint(dict(zip(unique, counts)))","f6365e25":"#Great, before we do anything else, let's split the data into train\/test.\nX_train_clean_res = pd.DataFrame(X_train_clean_res, columns=X_train_clean.keys())\n#y_train_clean_res = pd.DataFrame(y_train_clean_res)","607f95eb":"print(np.shape(X_train_clean_res))\nprint(np.shape(y_train_clean_res))\nprint(np.shape(X_test_clean)) \nprint(np.shape(y_test_clean))","d523d5dc":"#BASE MODEL\nparams={}\nxgbclf(params,X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","ad3c494f":"params = {}\n\nparams1={\n    'n_estimators':3000,\n    'objective': 'binary:logistic',\n    'learning_rate': 0.05,\n    'gamma':0.1,\n    'subsample':0.8,\n    'colsample_bytree':0.3,\n    'min_child_weight':3,\n    'max_depth':3,\n    #'seed':1024,\n    'n_jobs' : -1\n}\n\nparams2={\n    'n_estimators':3000,\n    'objective': 'binary:logistic',\n    'learning_rate': 0.005,\n    #'gamma':0.01,\n    'subsample':0.555,\n    'colsample_bytree':0.7,\n    'min_child_weight':3,\n    'max_depth':8,\n    #'seed':1024,\n    'n_jobs' : -1\n}\n\n#xgbclf(params, X_train, y_train,X_test,y_test)\nmodel = xgbclf(params2,X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)\nmodel\n#plot_featureImportance(model, X_train_clean_res.keys())","b0af5214":"#model = xgbclf(params1,X_train_clean_res[importance_col], y_train_clean_res,X_test_clean[importance_col], y_test_clean)\n\nimportances = model.feature_importances_\nimportance_frame = pd.DataFrame({'Importance': list(importances), 'Feature': list(X_train_clean_res.keys())})\nimportance_frame.sort_values(by = 'Importance', inplace = True, ascending=False)\nimportance_col = importance_frame.Feature.head(10).values","6aa5cdcf":"params = {}\n\nparams1={\n    'n_estimators':3000,\n    'objective': 'binary:logistic',\n    'learning_rate': 0.01,\n    #'gamma':0.1,\n    #'subsample':0.8,\n    #'colsample_bytree':0.3,\n    #'min_child_weight':3,\n    'max_depth':3,\n    #'seed':1024,\n    'n_jobs' : -1\n}\n\nxgbclf(params,X_train_clean_res[importance_col], y_train_clean_res,X_test_clean[importance_col], y_test_clean)","c960331f":"from sklearn.grid_search import GridSearchCV\n\nprint('XGBoost with grid search')\n# play with these params\nparams={\n    'learning_rate': [0.01, 0.02],\n    'max_depth': [3], # 5 is good but takes too long in kaggle env\n    #'subsample': [0.6], #[0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n    #'colsample_bytree': [0.5], #[0.5,0.6,0.7,0.8],\n    'n_estimators': [50, 100, 200, 300, 400, 500]\n    #'reg_alpha': [0.03] #[0.01, 0.02, 0.03, 0.04]\n}\n\n\nxgb_clf = xgb.XGBClassifier()\n\nrs = GridSearchCV(xgb_clf,\n                  params,\n                  cv=2,\n                  scoring=\"roc_auc\",\n                  n_jobs=1,\n                  verbose=False)\nrs.fit(X_train_clean_res[importance_col], y_train_clean_res)\nbest_est = rs.best_estimator_\nprint(best_est)\nprint(rs.best_score_)\n\n# Roc AUC with test data\nprint(rs.score(X_test_clean[importance_col],y_test_clean))\n\n# Roc AUC with all train data\n#y_pred_proba = best_est.predict_proba(X_test_clean[importance_col])[:,1]\n#print(\"Roc AUC: \", roc_auc_score(y_test_clean, y_pred_proba))\n\n#xgbclf(params1,X_train_clean_res[importance_col], y_train_clean_res,X_test_clean[importance_col], y_test_clean)","98522fda":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\nimport lightgbm as lgb\n\n# fit, train and cross validate Decision Tree with training and test data \ndef lgbclf(X_train, y_train,X_test, y_test):\n\n    model = lgb.LGBMClassifier().fit(X_train, y_train)\n    print(model,'\\n')\n\n    # Predict target variables y for test data\n    y_pred = model.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(model, X_train, y_train,y_test,y_pred)\n    #get_eval2(model, X_train, y_train,X_test, y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n\n# Logistic Regression\n#lgbclf(X_train, y_train,X_test,y_test)\nlgbclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","21151d93":"from sklearn.linear_model import LogisticRegression\n\n# fit, train and cross validate Decision Tree with training and test data \ndef logregclf(X_train, y_train,X_test, y_test):\n    print(\"LogisticRegression\")\n    model = LogisticRegression().fit(X_train, y_train)\n    print(model,'\\n')\n\n    # Predict target variables y for test data\n    y_pred = model.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(model, X_train, y_train,y_test,y_pred)\n    #get_eval2(model, X_train, y_train,X_test, y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n\n# Logistic Regression\n#logregclf(X_train, y_train,X_test,y_test)\nlogregclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","b970e839":"from sklearn.ensemble import RandomForestClassifier \n\n# fit, train and cross validate Decision Tree with training and test data \ndef randomforestclf(X_train, y_train,X_test, y_test):\n    print(\"RandomForestClassifier\")\n    randomforest = RandomForestClassifier().fit(X_train, y_train)\n    print(randomforest,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = randomforest.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(randomforest, X_train, y_train,y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return randomforest\n\n# Random Forest\n# Choose clean data, as tree is robust\nrf = randomforestclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","d242d41a":"from sklearn.ensemble import ExtraTreesClassifier\n\n# fit, train and cross validate Decision Tree with training and test data \ndef extratreesclf(X_train, y_train,X_test, y_test):\n    print(\"ExtraTreesClassifier\")\n    extratrees = ExtraTreesClassifier().fit(X_train, y_train)\n    print(extratrees,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = extratrees.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(extratrees, X_train, y_train,y_test,y_pred)\n    \n    get_roc (y_test,y_pred)\n    return\n \n# Extra Trees\n# Choose clean data, as tree is robust\nextratreesclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","1cb44167":"from sklearn.tree import DecisionTreeClassifier \n# fit, train and cross validate Decision Tree with training and test data \ndef dectreeclf(X_train, y_train,X_test, y_test):\n    print(\"DecisionTreeClassifier\")\n    dec_tree = DecisionTreeClassifier(min_samples_split=10,min_samples_leaf=5).fit(X_train, y_train)\n    print(dec_tree,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = dec_tree.predict_proba(X_test)[:,1]\n\n    \n    # Get Cross Validation and Confusion matrix\n    #get_eval(dec_tree, X_train, y_train,y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n\n# Decisiontree\ndectreeclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","c8f54e77":"from sklearn.ensemble import GradientBoostingClassifier\n\n# fit, train and cross validate GradientBoostingClassifier with training and test data \ndef gradientboostingclf(X_train, y_train, X_test, y_test):  \n    print(\"GradientBoostingClassifier\")\n    gbclf = GradientBoostingClassifier().fit(X_train, y_train)\n    print(gbclf,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = gbclf.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(gbclf, X_train, y_train,y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n  \n# GradientBoostingClassifier\n# Choose clean data, as tree is robust\ngradientboostingclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","6b7878cf":"from sklearn.ensemble import AdaBoostClassifier\n\n# fit, train and cross validate GradientBoostingClassifier with training and test data \ndef adaboostclf(X_train, y_train, X_test, y_test):  \n    print(\"AdaBoostClassifier\")\n    abclf = AdaBoostClassifier().fit(X_train, y_train)\n    print(abclf,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = abclf.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(abclf, X_train, y_train,y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n\n# AdaBoostClassifier\n# Choose clean data, as tree is robust\nadaboostclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","d5e660b0":"import eli5","1fdb6a7c":"import xgboost as xgb\nxgc = xgb.XGBClassifier(n_estimators=500, max_depth=5, base_score=0.5,\n                        objective='binary:logistic', random_state=42)\nxgc.fit(X_train_clean_res, y_train_clean_res)\n","6e605379":"y_preds = xgc.predict(X_test_clean)\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import mean_squared_error\nmetrics.accuracy_score(y_test_clean,y_preds)\nprint(\"confusion matrix=\",metrics.confusion_matrix(y_test_clean,y_preds))\nprint(\"classification report=\\n\",classification_report(y_test_clean,y_preds))\nprint(\"accuracy=\",metrics.accuracy_score(y_test_clean,y_preds))\nprint(\"mean squared error=\",mean_squared_error(y_test_clean, y_preds))\nprint(\"roc_auc score is=\",roc_auc_score(y_test_clean, y_preds))","42a45f93":"fig = plt.figure(figsize = (16, 12))\ntitle = fig.suptitle(\"Default Feature Importances from XGBoost\", fontsize=14)\n\nax1 = fig.add_subplot(2,2, 1)\nxgb.plot_importance(xgc, importance_type='weight', ax=ax1, max_num_features=20)\nt=ax1.set_title(\"Feature Importance - Feature Weight\")\n\nax2 = fig.add_subplot(2,2, 2)\nxgb.plot_importance(xgc, importance_type='gain', ax=ax2, max_num_features=20)\nt=ax2.set_title(\"Feature Importance - Split Mean Gain\")\n\nax3 = fig.add_subplot(2,2, 3)\nxgb.plot_importance(xgc, importance_type='cover', ax=ax3, max_num_features=20)\nt=ax3.set_title(\"Feature Importance - Sample Coverage\")","2f80f715":"import eli5\n\neli5.show_weights(xgc.get_booster())","bba66f76":"y_test_clean.iloc[3]","eb8ac77e":"\n\ndoc_num = 2\nprint('Actual Label:', y_test_clean.iloc[doc_num])\nprint('Predicted Label:', y_preds[doc_num])\neli5.show_prediction(xgc.get_booster(), X_test_clean.iloc[doc_num], \n                     feature_names=list(X_test_clean.columns),\n                     show_feature_values=True)\n\n","e23a705a":"y_test_clean.head()","b162f592":"\ndoc_num = 3\nprint('Actual Label:', y_test_clean.iloc[doc_num])\nprint('Predicted Label:', y_preds[doc_num])\neli5.show_prediction(xgc.get_booster(), X_test_clean.iloc[doc_num], \n                     feature_names=list(X_test_clean.columns),\n                     show_feature_values=True)","ee387e79":"\n\nfrom pdpbox import pdp, get_dataset, info_plots\n\ndef plot_pdp(model, df, feature, cluster_flag=False, nb_clusters=None, lines_flag=False):\n    \n    # Create the data that we will plot\n    pdp_goals = pdp.pdp_isolate(model=model, dataset=df, model_features=df.columns.tolist(), feature=feature)\n\n    # plot it\n    pdp.pdp_plot(pdp_goals, feature, cluster=cluster_flag, n_cluster_centers=nb_clusters, plot_lines=lines_flag)\n    plt.show()","a31c8bfc":"\n\n# plot the PD univariate plot\nplot_pdp(xgc, X_train_clean_res, 'creditamount')\n\n","d80cbf7f":"plot_pdp(xgc, X_train_clean_res, 'age')","faa4e710":"plot_pdp(xgc, X_train_clean_res, 'duration')","fe019825":"import shap\n\n# load JS visualization code to notebook\nshap.initjs()","c09a01ba":"explainer = shap.TreeExplainer(xgc)\nshap_values = explainer.shap_values(X_test_clean)","5638f3ab":"X_shap = pd.DataFrame(shap_values)\nX_shap.tail()","3d2bc774":"print('Expected Value: ', explainer.expected_value)","1ce37ae7":"shap.summary_plot(shap_values, X_test_clean, plot_type=\"bar\", color='blue')","4c960319":"shap.summary_plot(shap_values, X_test_clean)","2bd41894":"import lime\nimport lime.lime_tabular\n","64252c19":"categorical_features = np.argwhere(np.array([len(set(X_train_clean_res.values[:,x]))\nfor x in range(X_train_clean_res.values.shape[1])]) <= 10).flatten()","4a7bc736":"explainer = lime.lime_tabular.LimeTabularExplainer(X_train_clean_res.values, \n                                                   feature_names=X_train_clean_res.columns.values.tolist(), \n                                                   categorical_features=categorical_features, \n                                                   verbose=True, mode='regression')","3fa6c6fd":"\ni = 2\n \nexp = explainer.explain_instance(X_test_clean.iloc[i], rf.predict, num_features=5)\nexp.show_in_notebook(show_table=True)\n","a519cccb":"i = 3\n \nexp = explainer.explain_instance(X_test_clean.iloc[i], rf.predict, num_features=5)\nexp.show_in_notebook(show_table=True)\n","76759ffd":"### DecisionTreeClassifier (ROC_AUC:0.64)","3878d745":"### Encoding Categorical Feature\n\nLabelencoding to transform categorical to numerical, Enables better Visualization than one hot encoding","041597b2":"### AdaBoostClassifier (ROC_AUC:0.75)","242e37c7":"### GradientBoostingClassifier (ROC_AUC:0.76)","eabbfcb8":"### Import Dataset\n\nOK let's get started. We'll download the data from the UCI website.","07b253f1":"### XGBoost 2a: Balanced (Base Model: ROC_AUC:0.77)","63d873e5":"# Table of Content\n\n**1. [Introduction](#Introduction)** <br>\n    - Import Library\n    - Evaluation Function\n    - XGBoost Model\n**2. [Preprocess](#Preprocess)** <br>\n    - Importing Dataset\n    - StandardScaler\n    - Encoding Categorical Feature\n    - Concate Transformed Dataset\n    - Split Training Dataset\n    - XGBoost  1a: Unbalance Dataset (Base Model: ROC_AUC:0.74)\n    - XGBoost  1b: Unbalance Dataset (ROC_AUC:0.79)\n**3. [Balanced Dataset](#Balanced Dataset)** <br>    \n    - XGBoost 2a: Balanced (Base Model: ROC_AUC:0.77)\n    - **XGBoost 2b: Balanced (ROC_AUC:0.80)**\n**4. [Others](#Others)** <br>  \n    - Lighgbm (ROC_AUC:0.73)\n    - LogisticRegression (ROC_AUC:0.77)\n    - RandomForestClassifier (ROC_AUC:0.69)\n    - ExtraTreesClassifier (ROC_AUC:0.74)\n    - DecisionTreeClassifier (ROC_AUC:0.64)\n    - GradientBoostingClassifier (ROC_AUC:0.76)\n    - AdaBoostClassifier (ROC_AUC:0.72)","47d737d7":"### Evaluation Function\n","07af9390":"### Concate Transformed Dataset\nappend the dummy variable of the initial numerical variables numvars# append ","6c64de03":"<a id=\"Others\"><\/a> <br>\n# 5. Others\n- Lighgbm (ROC_AUC:0.73)\n- LogisticRegression (ROC_AUC:0.77)\n- RandomForestClassifier (ROC_AUC:0.69)\n- ExtraTreesClassifier (ROC_AUC:0.74)\n- DecisionTreeClassifier (ROC_AUC:0.64)\n- GradientBoostingClassifier (ROC_AUC:0.76)\n- AdaBoostClassifier (ROC_AUC:0.72)","50eee6f5":"### ExtraTreesClassifier (ROC_AUC:0.74)","3ffdada8":"### StandardScaler","7608a83c":"#### XGBoost Model","a2bb8f0a":"### Split Training Dataset","b214636c":"<a id=\"Balanced Dataset\"><\/a> <br>\n# **3. Balanced Dataset** \n- XGBoost 2a: Balanced (Base Model: ROC_AUC:0.77)\n- XGBoost 2b: Balanced (ROC_AUC:0.80)","4132c9fc":"### XGBoost  1a: Unbalance Dataset (Base Model: ROC_AUC:0.74)","7a840ff7":"### GridSearchCV (ROC_AUC:0.70)","fe9979ea":"### LogisticRegression (ROC_AUC:0.77)","ef70b38d":"### XGBoost3 (Base Model:ROC_AUC:0.73)","ae1033f8":"### Lighgbm (ROC_AUC:0.73)","41b9d1c3":"### Import Library","a07635ec":"# 4.  Feature Selection\n- XGBoost3 (Base Model:ROC_AUC:0.73)\n- GridSearchCV (ROC_AUC:0.70)","01fc7f3d":"### XGBoost  1b: Unbalance Dataset (ROC_AUC:0.79)","3ac2cc46":"### RandomForestClassifier (ROC_AUC:0.69)","f2c1cccb":"<a id=\"Introduction\"><\/a> <br>\n# **1. Introduction:** \n- Import Library\n- Evaluation Function\n- XGBoost Model","b338db9e":"### XGBoost 2b: Balanced (ROC_AUC:0.80)","7261517b":"<a id=\"Preprocess\"><\/a> <br>\n# **2. Preprocess** \n- Importing Dataset\n- StandardScaler\n- Encoding Categorical Feature\n- Concate Transformed Dataset\n- Split Training Dataset\n- XGBoost  1a: Unbalance Dataset (Base Model: ROC_AUC:0.74)\n- XGBoost  1b: Unbalance Dataset (ROC_AUC:0.79)","f80a4be6":"The German Credit data set is a publically available data set downloaded from the UCI Machine Learning Repository. The data contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risk for 1000 loan applicants.\n\n### [Data Source](https:\/\/archive.ics.uci.edu\/ml\/datasets\/statlog+(german+credit+data))\n- Professor Dr. Hans Hofmann  \n- Institut f\"ur Statistik und \"Okonometrie  \n- Universit\"at Hamburg  \n- FB Wirtschaftswissenschaften  \n- Von-Melle-Park 5    \n- 2000 Hamburg 13\n\n### Benchmark\n![Credit Risk Classification: Faster Machine Learning with Intel Optimized Packages](https:\/\/i.imgur.com\/nL1l7WI.png)\n\naccording to [1] the best model is Random Forest with balanced feature selection data. it's has Accuracy 82%, Precision 84%, Recall 82% and F1-Score 81%. \n\n<br>\n\n\nThe goal of this kernel is to beat The benchmark with  :\n- Convert dataset to Machine Learning friendly (Feature Engginering)\n- Develop XGBoost model to predict whether a loan is a good or bad risk.\n- Find the Best parameter for XGBoost Model (Hyperparameter Tunning)\n- Beat the Benchmark"}}