{"cell_type":{"274240f5":"code","21c33b51":"code","33121213":"code","61de6d7e":"code","2a5c1a50":"code","f692e5e2":"code","ada73250":"code","91c7a147":"code","1dc8bc68":"code","fd53bf53":"code","5ac17454":"code","c596c77a":"code","f9a18561":"code","3a9e1c08":"code","bfecc8f0":"code","d1f921c9":"code","1e6c7386":"code","e219b5d9":"code","776441d4":"code","d914bf13":"code","b8055ec1":"code","1b5fc70e":"code","18f3a778":"code","c9d1f26a":"code","9927ae1c":"code","f0f3799d":"code","45f0fbe7":"code","05799d72":"code","7716db02":"code","97513317":"code","2aba7b95":"code","f8d4bf50":"code","05bc0a26":"code","210da25b":"code","d64b6caa":"code","0b05b10f":"code","f0871771":"code","26c4fe4e":"code","aeaa8fb7":"code","b780512f":"code","be145684":"code","6307d1db":"code","d3d94c62":"code","87079ba1":"code","10015922":"code","519271a5":"code","1e7cd209":"code","a6b638f2":"code","6cf091e6":"code","5c5a81ee":"code","06486ef5":"code","6bedd26b":"code","faca5fee":"code","0af67f71":"code","2a22947d":"code","0e5ae4f8":"code","31ecc8dc":"code","5ac54449":"code","6fe405bb":"code","26f1cfa4":"code","af1a91b7":"code","3ee941d9":"code","01e0e6c6":"code","accf8f9f":"code","d989f5ee":"markdown","53ad53d1":"markdown","358c1850":"markdown","f6be7a90":"markdown","ac7dc94a":"markdown","83d790aa":"markdown","dc91b376":"markdown","c98d9a72":"markdown","eb6e4e42":"markdown","66880c87":"markdown","689b6a96":"markdown","ac2561d4":"markdown","b3e3aff6":"markdown","8329036e":"markdown","817b4376":"markdown","c78889dc":"markdown","a281f2cb":"markdown","9476f21a":"markdown","fdcd0b01":"markdown","7a1652df":"markdown","85c94450":"markdown","003ca6a3":"markdown","10076e81":"markdown","b4bb44c1":"markdown","26a7a39b":"markdown","c9d66f2a":"markdown","8981bb08":"markdown","6ab68aa0":"markdown","73799f2b":"markdown","59325b5d":"markdown","a2fbddcb":"markdown","6913e494":"markdown","9b3872dd":"markdown","ca955ce2":"markdown","a2160ee6":"markdown","16a72b60":"markdown","31f39d2a":"markdown","5f31f1e0":"markdown","44fa5cdf":"markdown","d2f22d90":"markdown","8ad368b8":"markdown","5fd0c581":"markdown","49614c43":"markdown","f7b3e98a":"markdown","d5e05d99":"markdown","0977c6ff":"markdown","16e51d26":"markdown"},"source":{"274240f5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","21c33b51":"dataset = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","33121213":"dataset.head()","61de6d7e":"dataset.info()","2a5c1a50":"dataset['sentiment'].value_counts()","f692e5e2":"review = dataset['review'].loc[1]\nreview","ada73250":"from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(review, \"html.parser\")\nreview = soup.get_text()\nreview","91c7a147":"import re\n\nreview = re.sub('\\[[^]]*\\]', ' ', review)\nreview = re.sub('[^a-zA-Z]', ' ', review)\nreview","1dc8bc68":"review = review.lower()\nreview","fd53bf53":"review = review.split()\nreview","5ac17454":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nreview = [word for word in review if not word in set(stopwords.words('english'))]\nreview","c596c77a":"from nltk.stem.porter import PorterStemmer\n\nps = PorterStemmer()\nreview_s = [ps.stem(word) for word in review]\nreview_s","f9a18561":"from nltk.stem import WordNetLemmatizer\n\nlem = WordNetLemmatizer()\nreview = [lem.lemmatize(word) for word in review]\nreview","3a9e1c08":"review = ' '.join(review)\nreview","bfecc8f0":"corpus = []\ncorpus.append(review)","d1f921c9":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vec = CountVectorizer()\nreview_count_vec = count_vec.fit_transform(corpus)\n\nreview_count_vec.toarray()","1e6c7386":"count_vec_bin = CountVectorizer(binary=True)\nreview_count_vec_bin = count_vec_bin.fit_transform(corpus)\n\nreview_count_vec_bin.toarray()","e219b5d9":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vec = TfidfVectorizer()\nreview_tfidf_vec = tfidf_vec.fit_transform(corpus)\n\nreview_tfidf_vec.toarray()","776441d4":"from sklearn.model_selection import train_test_split\n\ndataset_train, dataset_test, train_data_label, test_data_label = train_test_split(dataset['review'], dataset['sentiment'], test_size=0.25, random_state=42)","d914bf13":"train_data_label = (train_data_label.replace({'positive': 1, 'negative': 0})).values\ntest_data_label  = (test_data_label.replace({'positive': 1, 'negative': 0})).values","b8055ec1":"corpus_train = []\ncorpus_test  = []\n\nfor i in range(dataset_train.shape[0]):\n    soup = BeautifulSoup(dataset_train.iloc[i], \"html.parser\")\n    review = soup.get_text()\n    review = re.sub('\\[[^]]*\\]', ' ', review)\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    review = review.lower()\n    review = review.split()\n    review = [word for word in review if not word in set(stopwords.words('english'))]\n    lem = WordNetLemmatizer()\n    review = [lem.lemmatize(word) for word in review]\n    review = ' '.join(review)\n    corpus_train.append(review)\n    \nfor j in range(dataset_test.shape[0]):\n    soup = BeautifulSoup(dataset_test.iloc[j], \"html.parser\")\n    review = soup.get_text()\n    review = re.sub('\\[[^]]*\\]', ' ', review)\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    review = review.lower()\n    review = review.split()\n    review = [word for word in review if not word in set(stopwords.words('english'))]\n    lem = WordNetLemmatizer()\n    review = [lem.lemmatize(word) for word in review]\n    review = ' '.join(review)\n    corpus_test.append(review)","1b5fc70e":"corpus_train[-1]","18f3a778":"corpus_test[-1]","c9d1f26a":"tfidf_vec = TfidfVectorizer(ngram_range=(1, 3))\n\ntfidf_vec_train = tfidf_vec.fit_transform(corpus_train)\ntfidf_vec_test = tfidf_vec.transform(corpus_test)","9927ae1c":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC(C=0.5, random_state=42)\nlinear_svc.fit(tfidf_vec_train, train_data_label)\n\npredict = linear_svc.predict(tfidf_vec_test)","f0f3799d":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(\"Classification Report: \\n\", classification_report(test_data_label, predict,target_names=['Negative','Positive']))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict))\nprint(\"Accuracy: \\n\", accuracy_score(test_data_label, predict))","45f0fbe7":"count_vec = CountVectorizer(ngram_range=(1, 3), binary=False)\ncount_vec_train = count_vec.fit_transform(corpus_train)\ncount_vec_test = count_vec.transform(corpus_test)","05799d72":"linear_svc_count = LinearSVC(C=0.5, random_state=42, max_iter=5000)\nlinear_svc_count.fit(count_vec_train, train_data_label)\n\npredict_count = linear_svc_count.predict(count_vec_test)","7716db02":"print(\"Classification Report: \\n\", classification_report(test_data_label, predict_count,target_names=['Negative','Positive']))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_count))\nprint(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_count))","97513317":"ind_vec = CountVectorizer(ngram_range=(1, 3), binary=True)\nind_vec_train = ind_vec.fit_transform(corpus_train)\nind_vec_test = ind_vec.transform(corpus_test)","2aba7b95":"linear_svc_ind = LinearSVC(C=0.5, random_state=42)\nlinear_svc_ind.fit(ind_vec_train, train_data_label)\n\npredict_ind = linear_svc_ind.predict(ind_vec_test)","f8d4bf50":"print(\"Classification Report: \\n\", classification_report(test_data_label, predict_ind,target_names=['Negative','Positive']))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_ind))\nprint(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_ind))","05bc0a26":"tfidf_vec_NB = TfidfVectorizer(ngram_range=(1, 1))\ntfidf_vec_train_NB = tfidf_vec_NB.fit_transform(corpus_train)\n\ntfidf_vec_test_NB = tfidf_vec_NB.transform(corpus_test)\n\nprint(tfidf_vec_train_NB.toarray().shape, tfidf_vec_test_NB.toarray().shape)","210da25b":"from sklearn.feature_selection import SelectKBest, chi2\n\nch2 = SelectKBest(chi2, k=50000)\ntfidf_vec_train_NB = ch2.fit_transform(tfidf_vec_train_NB, train_data_label)\ntfidf_vec_test_NB  = ch2.transform(tfidf_vec_test_NB)","d64b6caa":"feature_names = tfidf_vec_NB.get_feature_names()\nfeature_names = [feature_names[i] for i\n                         in ch2.get_support(indices=True)]\nfeature_names = np.asarray(feature_names)\nfeature_names[32245]","0b05b10f":"from sklearn.naive_bayes import MultinomialNB\n\nmulti_clf = MultinomialNB()\nmulti_clf.fit(tfidf_vec_train_NB, train_data_label)\n\npredict_NB = multi_clf.predict(tfidf_vec_test_NB)","f0871771":"print(\"Classification Report: \\n\", classification_report(test_data_label, predict_NB,target_names=['Negative','Positive']))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_NB))\nprint(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_NB))","26c4fe4e":"count_vec_NB = CountVectorizer(ngram_range=(1, 3), binary=False)\ncount_vec_train_NB = count_vec_NB.fit_transform(corpus_train)\ncount_vec_test_NB = count_vec_NB.transform(corpus_test)","aeaa8fb7":"multi_clf_count = MultinomialNB()\nmulti_clf_count.fit(count_vec_train_NB, train_data_label)\n\npredict_NB_count = multi_clf_count.predict(count_vec_test_NB)","b780512f":"print(\"Classification Report: \\n\", classification_report(test_data_label, predict_NB_count,target_names=['Negative','Positive']))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_NB_count))\nprint(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_NB_count))","be145684":"dataset_predict = dataset_test.copy()\ndataset_predict = pd.DataFrame(dataset_predict)\ndataset_predict.columns = ['review']\ndataset_predict = dataset_predict.reset_index()\ndataset_predict = dataset_predict.drop(['index'], axis=1)\ndataset_predict.head()","6307d1db":"test_actual_label = test_data_label.copy()\ntest_actual_label = pd.DataFrame(test_actual_label)\ntest_actual_label.columns = ['sentiment']\ntest_actual_label['sentiment'] = test_actual_label['sentiment'].replace({1: 'positive', 0: 'negative'})","d3d94c62":"test_predicted_label = predict.copy()\ntest_predicted_label = pd.DataFrame(test_predicted_label)\ntest_predicted_label.columns = ['predicted_sentiment']\ntest_predicted_label['predicted_sentiment'] = test_predicted_label['predicted_sentiment'].replace({1: 'positive', 0: 'negative'})","87079ba1":"test_result = pd.concat([dataset_predict, test_actual_label, test_predicted_label], axis=1)\ntest_result.head()","10015922":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Masking, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","519271a5":"text = ['subha is a good boy', 'swati is a very good girl', 'yes...we will go home for sure', 'India\"s gdp is less than USA.']","1e7cd209":"text","a6b638f2":"tokenizer = Tokenizer(num_words=20)\n\ntokenizer.fit_on_texts(text)\nsequences = tokenizer.texts_to_sequences(text)\nsequences","6cf091e6":"tokenizer.word_index","5c5a81ee":"data = pad_sequences(sequences, maxlen=8, padding='post')\ndata","06486ef5":"word_index = tokenizer.word_index\nlen(word_index)","6bedd26b":"embedding = Embedding(input_dim=20, output_dim=4, mask_zero=True)\nmasked_output = embedding(data)\n\nprint(masked_output.numpy())","faca5fee":"print(\"Shape before Embedding: \\n\", data.shape)\nprint(\"Shape after Embedding: \\n\", masked_output.shape)","0af67f71":"max_features = 20000\nmaxlen = 200\ntokenizer = Tokenizer(num_words=max_features)","2a22947d":"train = pd.DataFrame(dataset_train)\ntrain.columns = ['review']\n\ntest = pd.DataFrame(dataset_test)\ntest.columns = ['review']","0e5ae4f8":"tokenizer.fit_on_texts(train['review'])\nX_train_token = tokenizer.texts_to_sequences(train['review'])\n\ntokenizer.fit_on_texts(test['review'])\nX_test_token = tokenizer.texts_to_sequences(test['review'])","31ecc8dc":"X_train = pad_sequences(X_train_token, maxlen=maxlen, padding='post')\nX_test  = pad_sequences(X_test_token, maxlen=maxlen, padding='post')\nprint(X_train.shape, X_test.shape)","5ac54449":"y_train = train_data_label.copy()\ny_test  = test_data_label.copy()","6fe405bb":"model = Sequential([Embedding(max_features, 64, mask_zero=True),\n                    Bidirectional(LSTM(64, dropout=0.2)),\n                    Dense(64, activation='sigmoid'),\n                    Dense(1)])","26f1cfa4":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","af1a91b7":"history = model.fit(X_train, y_train,\n                    batch_size=50,\n                    epochs=3,\n                    validation_data=(X_test, y_test))","3ee941d9":"history.history","01e0e6c6":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'], '')\nplt.xlabel(\"Epochs\")\nplt.ylabel('Accuracy')\nplt.title('Change of Accuracy over Epochs')\nplt.legend(['accuracy', 'val_accuracy'])\nplt.show()","accf8f9f":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'], '')\nplt.xlabel(\"Epochs\")\nplt.ylabel('Loss')\nplt.title('Change of Loss over Epochs')\nplt.legend(['loss', 'val_loss'])\nplt.show()","d989f5ee":"#### Our next step will be to bring this text in mathematical forms and to do so we will create a Corpus first.","53ad53d1":"#### The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work. We will now use CountVectorizer as our Bag-of-Words model before applying MultinomialNB model.","358c1850":"#### Let's proceed with the LSTM model.","f6be7a90":"#### So we can see there are 2 columns - review and sentiment. sentiment is the target column that we need to predict. The dataset is completely balanced and it has equal number of positive and negative sentiments.","ac7dc94a":"#### So we can see that the 4x8 array is converted to a 4x8x4 tensor.","83d790aa":"#### So LinearSVC using TF-IDF vectorization gives the maximum accuracy and we can see the outcome on our makeshift test dataset.","dc91b376":"#### Next we will bring everything into lowercase.","c98d9a72":"#### Let's understand how it works.","eb6e4e42":"#### We can see that 'little' has become 'littl' after Stemming but remained 'little' after Lemmatization. We will use Lemmatization.","66880c87":"#### Let's measure its performance.","689b6a96":"#### Think this is our text which we need to vectorize.","ac2561d4":"#### We will now fit the data to Multinomial Naive Bayes classifier. Bayesian model uses prior probabilities to predict posterior probabilites which is helpful for classification with discrete features like text classification.","b3e3aff6":"#### We can conclude that Bi-directional LSTM takes more time to train and is performing poorly compared to TF-IDF vectorization and Linear Classifier or Multinomial Naive Bayes Classifier.","8329036e":"#### Let's measure its performance.","817b4376":"#### Let's fit the data to Multinomial Naive Bayes model.","c78889dc":"#### Apply the Padding.","a281f2cb":"#### Plotting model performance.","9476f21a":"#### We can apply Recurrent Neural Networks like LSTM to perform sentiment analysis and we have a different vectorization technique called *Word Embeddings*.\n\n#### Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, we do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n\nReference: https:\/\/www.tensorflow.org\/tutorials\/text\/word_embeddings","fdcd0b01":"#### Import the basic libraries.","7a1652df":"#### Clean the text and build the train and test corpus.","85c94450":"#### Let's measure its performance.","003ca6a3":"#### Merge the words to form cleaned up version of the text.","10076e81":"#### Now we can see that the 4 lines in the text have different length; so we need to standardize their length thru a technique called *Padding*.\n\n#### padding='post' means zeroes will be appended after actual text and vice versa if padding='pre'.","b4bb44c1":"#### Now we will vectorize using CountVectorizer(binary=False) and fit it on LinearSVC model.","26a7a39b":"#### Convert sentiments to numeric forms.","c9d66f2a":"#### So we can see the data has become numeric with 1,2 and 3s based on the number of times they appear in the text.\n\n#### There is another variation of CountVectorizer with binary=True and in that case all zero entries will have 1.","8981bb08":"#### Apply the Tokenizer.","6ab68aa0":"#### We can see HTML tags are removed; so in the next step we will remove everything except lower\/upper case letters using Regular Expressions.","73799f2b":"#### We will now apply all the techniques that we discussed on the whole dataset but there is no test dataset so we will keep 25% of the data aside to test the performance of the model.","59325b5d":"#### We will now vectorize using TF-IDF technique.","a2fbddcb":"#### Stemming\/Lemmatization - we will apply both and see the difference.","6913e494":"#### So we are getting maximum accuracy using TF-IDF vectorizer.","9b3872dd":"#### Explore the dataset.","ca955ce2":"#### Now we will vectorize using CountVectorizer(binary=True) and fit it on LinearSVC model.","a2160ee6":"#### So there are 81301 terms in the corpus and we will use a *Chi-Square* test to select top 50000 features.","16a72b60":"#### NLP stands for Natural Language Processing which is the task of mining the text and find out meaningful insights like Sentiments, Named Entity, Topics of Discussion and even Summary of the text.\n\n#### With this dataset we will explore Sentiment Analaysis.\n\n#### Since texts are free form, we need to apply a set of text cleaning techniques.\n\n#### We can't apply texts to our ML model; we have to convert them in mathematical form and we will explore different techniques of Text Encoding.\n\n#### I hope you will like it and please upvote!","31f39d2a":"#### We can see the top features as well.","5f31f1e0":"#### Let's see the performance.","44fa5cdf":"#### Let's validate one sample entry.","d2f22d90":"#### So we can see we are able to tokenize the textual data.","8ad368b8":"#### I am going to use LinearSVC as my first model.","5fd0c581":"#### Load the dataset.","49614c43":"#### To vectorize the text we will apply -\n\n1. CountVectorizer (Bag of Words Model)\n2. TfidfVectorizer (Bag of Words Model)\n3. Keras Tokenizer (Embedding)","f7b3e98a":"#### Let's take one review as sample and understand why we need to clean the text.","d5e05d99":"#### So there is no 2s and 3s in the vector.\n\n#### We will now explore TF-IDF - TF stands for Text Frequency which means how many times a word (term) appears in a text (document). IDF means Inverse Document Frequency and is calculated as log(# of documents in corpus\/# of documents containing the term).\n\n#### Finally TF-IDF score is calculated as TF * IDF.\n\n#### IDF acts as a balancing factor and diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.","0977c6ff":"#### Normally any NLP task involves following text cleaning techniques -\n\n1. Removal of HTML contents like \"< br>\".\n2. Removal of punctutions, special characters like '\\'.\n3. Removal of stopwords like is, the which do not offer much insight.\n4. Stemming\/Lemmatization to bring back multiple forms of same word to their common root like 'coming', 'comes' into 'come'.\n5. Vectorization - Encode the numeric values once you have cleaned it.\n6. Fit the data to the ML model.\n\n#### We will apply all these techniques on this sample review and understand how it works.\n\n#### First of all we will remove HTML contents.","16e51d26":"#### Stopwords removal - since stopwords removal works on every word in your text we need to split the text."}}