{"cell_type":{"00bf4e79":"code","0e8efda3":"code","0885f8ff":"code","7f569ba1":"code","c8523280":"code","63b0384e":"code","ae56b177":"code","5df4cffa":"code","4b06ef2b":"code","5b3bab30":"code","15e392eb":"code","bfe02634":"code","e466781a":"code","97c17a69":"code","b619e887":"code","13011e5d":"code","85e57e72":"code","d5d2cc50":"code","ef7f2406":"markdown"},"source":{"00bf4e79":"## Reproducibility\n## Random seed given\nimport random ## import the random library\nrandom.seed(10) ## Setting the seed to get the same answer no matter how many times and who runs the model","0e8efda3":"## Downloading specific libraries\nimport numpy as np ## Library that enables linear functions \nimport pandas as pd ##  # Enables data processing\nimport glob ## returns an array of filenames that match a pattern\nimport cv2 ## helps add labels to image classifications\nimport matplotlib.pyplot as plt ## library for producing figures\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img ## importing image processing packages from \n## keras\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout ## Import libraries for model building ","0885f8ff":"## Read the train csv file\ntrain_dir='\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/' ## assigning a name to the location of the train images\ntrain=pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/train.csv') ## assigning a name to the location of the CSV file\n\n## Read the test csv file\ntest_dir='\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/test\/' ## assigning a name to the location of the test images\ntest=pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/test.csv') ## assigning a name to the location of the CSV file ","7f569ba1":"## Finding the unique patient ids from train csv file\nprint(f\"The total patient ids are {train['patient_id'].count()}, from those the unique ids are {train['patient_id'].value_counts().shape[0]} \")\n\n## Finding the unique patient ids from test csv file\nprint(f\"The total patient ids are {test['patient_id'].count()}, from those the unique ids are {test['patient_id'].value_counts().shape[0]} \")\n","c8523280":"train['path'] = train_dir + train.image_name + \".jpg\" ## adding the location of the image to the row for the train data set\ntrain.head() ## showing the first 5 lines of the train data set, note the \"path\" coloumn \n\ntest['path'] = test_dir + test.image_name + \".jpg\"  ## adding the location of the image to the row for the test data set\ntest.head()  ## showing the first 5 lines of the test data set, note the \"path\" coloumn ","63b0384e":"## Class Distribution\ntrain.target.value_counts() ## Count the number of images that were classified as malinnent or non malignent","ae56b177":"df_0=train[train['target']==0].sample(1000) ## produce a data frame using 1000 images from the train data set where the target equals zero\ndf_1=train[train['target']==1] ## produce a data frame using all the images from the test data set where the target equals 584\ntrain=pd.concat([df_0,df_1]) ## create a new dataset using the smaller training data set\ntrain=train.reset_index() ## making sure the new \"train\" data set is being used for the model","5df4cffa":"train.shape ## how many observations and variables are in the training set being used for the model ","4b06ef2b":"train.head() ## First 5 rows of the new train set  ","5b3bab30":"# we will resize the given images to 200 x 200 size images for faster processing\nIMG_DIM = (200, 200) ## changing the image dimensions ","15e392eb":"from sklearn.model_selection import train_test_split ## importing the train test split function \nX_train, X_val, y_train, y_val = train_test_split(train, train.target, test_size=0.2, random_state=42) ## taking 20% of the training data set ","bfe02634":"train_files = X_train.path ## Image path for the training data set\nval_files = X_val.path ## Image path for the validation data set\n\ntrain_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files] ## load images using load_img function from keras \n## preprocessing using the target_size function \nvalidation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in val_files] ## using the img_to_array will tranform the loaded image to an array\n\ntrain_imgs = np.array(train_imgs) ## converting the list of arrays to array for the training dataset \ntrain_labels = y_train\n\nvalidation_imgs = np.array(validation_imgs) ## converting the list of arrays to array for the validation dataset \nval_labels = y_val\n\n\nprint('Train dataset shape:', train_imgs.shape, \n      '\\tValidation dataset shape:', validation_imgs.shape)","e466781a":"## Scale Images\n## scale each image with pixel values between (0, 255) to values between (0, 1) because deep learning models work really\n## well with small input values.\ntrain_imgs_scaled = train_imgs.astype('float32')\n\nvalidation_imgs_scaled  = validation_imgs.astype('float32')\n\n# divide the pixels by 255 to scale the pixels between 0 and 1\ntrain_imgs_scaled \/= 255\nvalidation_imgs_scaled \/= 255\n\nprint(train_imgs[0].shape)\n\narray_to_img(train_imgs[0]) ## using the array_to_img function will convert the given array to image","97c17a69":"# setup basic configuration\nbatch_size = 30 ## indicating the total number of images passed to the model per iteration\nnum_classes = 2 \nepochs = 75 ## establishing the training time \ninput_shape = (200, 200, 3)\n\nfrom keras.models import Sequential ## importing the sequential library\nfrom keras import optimizers ## importing optimizers","b619e887":"train_datagen = ImageDataGenerator(rescale=1.\/255, zoom_range=0.3, rotation_range=70,\n                                   width_shift_range=0.3, height_shift_range=0.3, shear_range=0.3, \n                                   horizontal_flip=True, fill_mode='nearest')\n\nval_datagen = ImageDataGenerator(rescale=1.\/255)\n","13011e5d":"# lets take a random image and see how transformated images actually looks\nimg_id = 1\n\nimg_generator = train_datagen.flow(train_imgs[img_id:img_id+1], train_labels[img_id:img_id+1],\n                                   batch_size=1)\n\nimg = [next(img_generator) for i in range(0,5)]\n\nfig, ax = plt.subplots(1,5, figsize=(16, 6))\nprint('Labels:', [item[1][0] for item in img])\nl = [ax[i].imshow(img[i][0][0]) for i in range(0,5)]","85e57e72":"import random ## import the random library\nrandom.seed(10) ## Setting the seed to get the same answer no matter how many times and who runs the model\n\ntrain_generator = train_datagen.flow(train_imgs, train_labels, batch_size=30)\nval_generator = val_datagen.flow(validation_imgs, val_labels, batch_size=20)\n\ninput_shape = input_shape\n\nmodel = Sequential()  ## creating and instance of Sequential\n\nmodel.add(Conv2D(16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['accuracy'])\n              \nhistory = model.fit_generator(train_generator, steps_per_epoch=32, epochs=75,\n                              validation_data=val_generator, validation_steps=12, \n                              verbose=1)","d5d2cc50":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Model 2- with image argumentation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(1,76))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 76, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 76, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","ef7f2406":"CNN Model 2"}}