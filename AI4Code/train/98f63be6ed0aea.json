{"cell_type":{"2c653bb8":"code","ee7c860b":"code","9e173966":"code","45f276ab":"code","b4166f7e":"code","74b3732e":"code","66319602":"code","8be45627":"code","c4a5eec1":"code","ee40aca1":"code","5c6f55f3":"code","dfdffb1d":"code","a1257b22":"code","c196823c":"code","e1431aac":"code","07295b63":"code","6940ed02":"code","05b2f019":"code","0e752fe1":"code","220c6a71":"code","4c7e0d3e":"code","56d751f5":"code","98f21de2":"code","6c1108a2":"code","241ad0e2":"code","c375e6cd":"code","b4bd5e80":"code","4e7b7969":"code","9cc99bb9":"code","cd1852d1":"code","9f925eea":"code","74d896fa":"code","73413fbd":"code","9d30f57e":"code","3ae3c528":"code","5097c85a":"code","e54c36fe":"code","aa8428dd":"code","f28c24c0":"code","cd24044c":"markdown","2c8b3e82":"markdown","67c73c33":"markdown","4afaeb49":"markdown","68a4f8b7":"markdown","5abe8a29":"markdown","68cd1e16":"markdown","a4d97501":"markdown","53d79804":"markdown","d98772fa":"markdown","3d2aff1c":"markdown","9fe88283":"markdown","6fbcd7e3":"markdown","201710fd":"markdown","5d42e004":"markdown","48edecaf":"markdown","afdce4b9":"markdown","9bbc5368":"markdown","6e2b6cbd":"markdown"},"source":{"2c653bb8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ee7c860b":"train_data=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\",encoding=\"latin1\")\ntest_data=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\",encoding=\"latin1\")","9e173966":"train_data.info()","45f276ab":"test_data.info()","b4166f7e":"train_data.head()","74b3732e":"test_data.tail()","66319602":"train_data.isnull().sum()","8be45627":"test_data.isnull().sum()","c4a5eec1":"print(\"Sample count and Feature count for Train Data :\",train_data.shape)\nprint(\"Sample count and Feature count for Test Data :\",test_data.shape)","ee40aca1":"train_data.dropna(how='any',axis=0,inplace=True)\ntest_data.dropna(how='any',axis=0,inplace=True)","5c6f55f3":"test_data.isnull().sum()","dfdffb1d":"print(\"Sample count and Feature count for Train Data :\",train_data.shape)\nprint(\"Sample count and Feature count for Test Data :\",test_data.shape)","a1257b22":"import re \ntrain_data['text'] = [re.sub(\"[^a-zA-Z]\",\" \",text).lower() for text in train_data['text']]\ntest_data['text'] = [re.sub(\"[^a-zA-Z]\",\" \",text).lower() for text in test_data['text']]\n\ntrain_data.head()","c196823c":"test_data.head()","e1431aac":"from textblob import TextBlob\n\ndef polarity_check_label(dataframe):\n    polarity_list = []\n    \n    for text in dataframe['text']:\n        polarity_point = TextBlob(text).sentiment.polarity\n        if polarity_point < 0 : polarity_state= 'Negative'\n        elif polarity_point == 0 : polarity_state = 'Neutral'\n        else : polarity_state = 'Positive'\n        polarity_list.append(polarity_state)\n        \n    dataframe['polarity_state'] = polarity_list\n    \npolarity_check_label(train_data)\npolarity_check_label(test_data)","07295b63":"train_data.head()","6940ed02":"test_data.head()","05b2f019":"import nltk # natural language tool kit for word_tokenize ...\nnltk.download(\"stopwords\")      # stopwords is download in corpus directory\nfrom nltk.corpus import stopwords  # import stopwords","0e752fe1":"train_data['text'] = [nltk.word_tokenize(text) for text in train_data['text']]\ntest_data['text'] = [nltk.word_tokenize(text) for text in test_data['text']]\n\ntrain_data.text.head()","220c6a71":"test_data.text.head()","4c7e0d3e":"def lemma_and_join(dataframe):\n    lemma = nltk.WordNetLemmatizer()\n    text_list = []\n    for text in dataframe['text']:\n        text = [ word for word in text if not word in set(stopwords.words(\"english\"))]\n        text = [lemma.lemmatize(word) for word in text]\n        text = \" \".join(text)\n        text_list.append(text)\n    return text_list","56d751f5":"train_data['text'] = lemma_and_join(train_data)\ntest_data['text'] = lemma_and_join(test_data)\ntrain_data.head()","98f21de2":"test_data.head()","6c1108a2":"from sklearn.feature_extraction.text import CountVectorizer # used method for create bag of word\nmax_features = 5000\n\ncount_vectorizer = CountVectorizer(max_features=max_features) #(max_features=max_features)\n\nsparce_matrix_train= count_vectorizer.fit_transform(train_data['text'])  # x\nspace_matrix_test = count_vectorizer.transform(test_data['text'])  # x_test\n\nprint(\"{} most used word: {}\".format(max_features,count_vectorizer.get_feature_names()))\n","241ad0e2":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt \n\n\n\ntx=' '.join(train_data['text'].values.tolist())\nfig,ax1=plt.subplots(1,1,figsize=(12,16))\nwordcl=WordCloud().generate(tx)\nplt.imshow(wordcl,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Most used words for real disaster tweets\",fontsize=16)\nplt.show()\n\n","c375e6cd":"sparce_matrix_train.shape","b4bd5e80":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, recall_score, precision_score","4e7b7969":"f1_score_list=[]\ntrain_accuracy_list=[]\ntest_accuracy_list = []\nclassifier_list = []\ndef fit_and_predict(model,x_train,x_test,y_train,y_test):\n    \n    classifier = model\n    classifier.fit(x_train,y_train)\n    y_pred = classifier.predict(x_test)\n    #print('{}'.format(classifier))\n    cmatrix = confusion_matrix(y_test,y_pred)\n    \n    f,ax = plt.subplots(figsize=(3,3))\n    sns.heatmap(cmatrix,annot=True,linewidths=0.5,cbar=False,linecolor=\"red\",fmt='.0f',ax=ax)\n    plt.xlabel(\"y_predict\")\n    plt.ylabel(\"y_true\")\n    ax.set(title=str(classifier))\n    plt.show()\n    \n    \n    f1score = f1_score(y_test,y_pred,average='weighted')\n    train_accuracy = round(classifier.score(x_train,y_train)*100)\n    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)\n    \n    classifier_list.append(str(classifier))\n    train_accuracy_list.append(str(train_accuracy))\n    test_accuracy_list.append(str(test_accuracy))\n    f1_score_list.append(str(round(f1score*100)))\n    \n    \n    print(classification_report(y_test,y_pred))\n    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))\n    print('-'*50)\n    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))\n    \n","9cc99bb9":"y = train_data.target\nx = sparce_matrix_train\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)","cd1852d1":"random_state = 29\nmodels=[\n        LogisticRegression(random_state=29),\n        SVC(random_state=random_state),\n        MultinomialNB(),\n        DecisionTreeClassifier(random_state = 29),\n        KNeighborsClassifier(),\n        RandomForestClassifier(random_state=29),\n       ]","9f925eea":"for model in models:\n    fit_and_predict(model,x_train,x_test,y_train,y_test)","74d896fa":"from sklearn.model_selection import GridSearchCV","73413fbd":"lr_param_grid = {'solver':['lbfgs','liblinear'],\n                 'C': np.logspace(-3,3,10),\n                 'penalty': ['l1', 'l2']}\n\nsvc_param_grid={#'kernel' : ['linear' , 'poly' , 'rbf'],\n                'C' : [0.01, 0.1, 1]\n                #'gamma': np.logspace(-3, 2, 6)\n                }\n\nrandom_param_grid = {\"max_features\": [1,3,10],\n                    \"min_samples_split\":[2,3,10],\n                    \"min_samples_leaf\":[1,3,10],\n                    \"bootstrap\":[False],\n                    \"n_estimators\":[100,300],\n                    \"criterion\":[\"gini\"]}\n\nknn_param_grid = {'n_neighbors': np.arange(1,10),\n                  \"weights\": [\"uniform\",\"distance\"],\n                  \"metric\":[\"euclidean\",\"manhattan\"]}\n\ndecision_param_grid = {'criterion':['gini', 'entropy'],\n                      'min_samples_leaf': list(range(1,5)),\n                      'min_samples_split': list(range(7,10))}\n\nmultinm_param_grid = {'alpha': np.linspace(0.5,1.5, 5),\n                    'fit_prior' : [True,False]}\n    \n\nclassifier_param = [lr_param_grid,\n                    svc_param_grid,\n                    multinm_param_grid, \n                    decision_param_grid,\n                    knn_param_grid,\n                    random_param_grid]\n\n","9d30f57e":"cv_score = []\ncv_f1_score = []\ncv_precision = []\ncv_recall = []\nfor i in range(len(models)):\n    clf = GridSearchCV(models[i], classifier_param[i], cv=10, scoring=\"accuracy\" ).fit(x_train,y_train)\n    y_pred = clf.predict(x_test)\n    \n    cmatrix = confusion_matrix(y_test,y_pred)\n    f,ax = plt.subplots(figsize=(3,3))\n    sns.heatmap(cmatrix,annot=True,linewidths=0.5,cbar=False,linecolor=\"red\",fmt='.0f',ax=ax)\n    plt.xlabel(\"y_predict\")\n    plt.ylabel(\"y_true\")\n    ax.set(title=str(models[i]))\n    plt.show()\n    \n    acc_score = round(clf.score(x_test,y_test)*100)\n    f1score = f1_score(y_test,y_pred)\n    precision = precision_score(y_test,y_pred)\n    recall = recall_score(y_test,y_pred)\n    \n    \n    cv_score.append(acc_score)\n    cv_f1_score.append(str(round(f1score*100)))\n    cv_precision.append(str(round(precision*100)))\n    cv_recall.append(str(round(recall*100)))\n    \n    \n    print(classification_report(y_test,y_pred))\n    print(\"Tuned hyperparameters : {}\".format(clf.best_params_))\n    print(\"Best Accuracy: {}\".format(acc_score))\n    \n","3ae3c528":"results_dict={'Classifier':classifier_list,\n              'Train_Accuracy':train_accuracy_list,\n              'Test_Accuracy':test_accuracy_list,\n              'F1-Score':f1_score_list\n            }\n\nresults=pd.DataFrame(results_dict)\nresults","5097c85a":"trace1 = go.Bar(\n                x = results.Classifier,\n                y = results.Train_Accuracy,\n                name = \"Train Accuracy\",\n                marker = dict(color = 'rgba(255, 174, 255, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\n\ntrace2 = go.Bar(\n                x = results.Classifier,\n                y = results.Test_Accuracy,\n                name = \"Test Accuracy\",\n                marker = dict(color = 'rgba(255, 255, 128, 0.5)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\ntrace3 = go.Bar(\n                x = results.Classifier,\n                y = results['F1-Score'],\n                name = \"F1-Score\",\n                marker = dict(color = 'rgba(0,191,255,0.5)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\ndata = [trace1, trace2, trace3]\nlayout = go.Layout(barmode = \"group\")\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","e54c36fe":"cv_results_dict={'Classifier':classifier_list,\n                 'Test_Accuracy':cv_score,\n                 'F1-Score':cv_f1_score,\n                 'Recall' : cv_recall,\n                 'Precision' : cv_precision\n            }\n\ncv_results=pd.DataFrame(cv_results_dict)\ncv_results","aa8428dd":"trace1 = go.Bar(\n                x = cv_results.Classifier,\n                y = cv_results.Test_Accuracy,\n                name = \"CV Accuracy\",\n                marker = dict(color = 'red',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\ntrace2 = go.Bar(\n                x = cv_results.Classifier,\n                y = cv_results['F1-Score'],\n                name = \"CV F1 Score\",\n                marker = dict(color = 'rgba(255, 255, 128, 0.5)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\ntrace3 = go.Bar(\n                x = cv_results.Classifier,\n                y = cv_results.Recall,\n                name = \"CV Recall\",\n                marker = dict(color = 'rgba(0,191,255,0.5)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\ntrace4 = go.Bar(\n                x = cv_results.Classifier,\n                y = cv_results.Precision,\n                name = \"CV Precision\",\n                marker = dict(color = 'green',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\ndata = [trace1, trace2, trace3, trace4]\nlayout = go.Layout(barmode = \"group\")\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","f28c24c0":"x = cv_results.Classifier\ntrace1 = {\n  'x': x,\n  'y': results.Test_Accuracy,\n  'name': 'Simple Classification',\n  'type': 'bar'  \n};\ntrace2 = {\n  'x': x,\n  'y': cv_results.Test_Accuracy,\n  'name': 'GridSearchCV Classification',\n  'type': 'bar'  \n};\ndata = [trace1, trace2];\nlayout = {\n  'xaxis': {'title': 'Consistent Classification'},\n  'barmode': 'relative',\n  'title': 'Simple Classification Scores vs GridLayoutCV Classification Scores'\n};\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","cd24044c":"**Records with null values were removed.**","2c8b3e82":"<a id='2'><\/a>\n# 2. Data Cleaning\n\n1. Regular Expression (RE)\n2. Sentitiment (Create of Polarity Feature)\n3. Irrelavant Words (Stopwords), Lemmatization and Join","67c73c33":"<a id='6.2'><\/a>\n## 6.2. Classification Result with GridSearchCV","4afaeb49":"<a id='6.1'><\/a>\n\n## 6.1. Simple Classification Result","68a4f8b7":"# INTRODUCTION\n\n\n- **The purpose of this kernel is to take a step towards NLP using the disaster_tweets data set.**\n- **The necessary steps for a correct nlp are expressed in a clear language in the kernel.**\n\n<b> Content: <\/b>\n\n1. [Data Reading and Data Pre-Processing](#1)<br>\n2. [Data Cleaning](#2)<br>\n    2.1.[Regular Expression](#2.1)<br>\n    2.2.[Sentitiment (Create of Polarity Feature)](#2.2)<br>\n    2.3.[Irrelavant Words (Stopwords)](#2.3)<br>\n3. [Bag of Words](#3)<br>\n4. [Create Model](#4)<br>\n    4.1.[Start Predict](#4.1)<br>\n5. [5. Classification with Correct Parameters (Hyperparameter Optimization)](#5)<br>\n6. [Conclusion !!!](#6)<br>\n    6.1. [Simple Classification Result](#6.1)<br>\n    6.2. [GridSearchCV Classification Result](#6.2)<br>\n    6.3. [Simpe Classification Result vs GridSearchCV Classifcation Result](#6.3.)","5abe8a29":"<a id='5'><\/a>\n# 5. Classification with Correct Parameters (Hyperparameter Optimization)","68cd1e16":"<a id='1'><\/a>\n# 1- Data Reading and Data Pre-Processing","a4d97501":"<a id='3'><\/a>\n# 3. Bag of Words\n\n- The word bag model is a way of showing text vectorially while modeling text with machine learning algorithms. In this model, we represent the text as if inside a bag.\n\n![](http:\/\/i.hizliresim.com\/9Wd7k6.png)","53d79804":"<a id='6.3.'><\/a>\n## 6.3. Simple Classification Result vs GridSearchCV Classification Result","d98772fa":"<a id='4.1'><\/a>\n## 4.1. Start Predict","3d2aff1c":"- Lemma is a method that allows the word to be separated by word roots.\n- With the join operation, the words are re-combined into sentences.","9fe88283":"<a id='6'><\/a>\n# 6. Conclusion\n\n**Classification have been completed. Results Listed and Compared**","6fbcd7e3":"<a id='2.2'><\/a>\n## 2.2. Sentitiment (Create of Polarity Feature)\n- textblob is a library used to process text data.\n- The purpose here is to decompose the comments made into negative, neutral or positive situations.\n- A polarity feature of the parsed states should be created.","201710fd":"<a id='2.3'><\/a>\n## 2.3. Irrelavant Words (Stopwords)\n\n\n- For a good analysis, unnecessary words and attachments in sentences should be removed.\n- Stopwords in the corpus file downloaded from ntlk contains these words and their attachments.\n\n- **Word_tokenize library is used to separate words in sentences.**\n\n","5d42e004":"**We will compare 5 machine learning classifier and evaluate mean accuracy of each of them by stratified cross validation.**\n\n- **Parameter Tuning for LogisticRegression**\n\n- C: Regularization value, the more, the stronger the regularization(double). \n- RegularizationType: Can be either \"L2\" or \u201cL1\u201d. Default is \u201cL2\u201d. \n\n\n- **Parameter Tuning for Support Vector Machine**\n\n- Kernel and gamma are interpreted to reduce processing time.\n\n\n- **Parameter Tuning for MultinomialNB**\n\n- Alpha: Additive (Laplace\/Lidstone) smoothing parameter (0 for no smoothing). Default 1.0\n- fit_prior: Whether to learn class prior probabilities or not. If false, a uniform prior will be used. Default True\n\n\n- **Parameter Tuning for Decision Tree Classifier**\n\n- max_depth: Maximum depth of the tree (double).\n- max_features: Proportion of columns (features) to consider in each level (double).\n\n\n- **Parameter Tuning for KNN**\n\n- n_neighbors: Number of neighbors to use by default for k_neighbors queries\n\n\n","48edecaf":"\n* There are many missing data in key and location columns.\n* Null values are removed from the data set","afdce4b9":"- **Models Created**","9bbc5368":"<a id='4'><\/a>\n# 4. Create Models\n","6e2b6cbd":"<a id='2.1'><\/a>\n## 2.1. Regular Expression\n- For an accurate analysis with texts: characters other than letters that make up words such as blank values, emojies, punctuation marks, numbers should be removed.\n- In textual studies, the type of letters is important. All letters should be made in lowercase."}}