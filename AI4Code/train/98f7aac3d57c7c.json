{"cell_type":{"892b219d":"code","2023fa6a":"code","621e88d7":"code","9a1a7a2a":"code","ddce284b":"code","2968a010":"code","fb7f9f2a":"code","cc090835":"code","ffa4366f":"code","bcb86752":"code","1b5d7333":"code","07529640":"code","79f2998a":"code","ad308be9":"code","2770b272":"code","b7096126":"code","22245639":"code","80f4db0a":"code","07c72664":"code","72c82f99":"code","d09668b7":"code","6497cdb8":"code","d6fd018c":"code","65fecfdf":"code","5db9a811":"code","112802de":"code","f3acd9e6":"code","088eaffb":"code","43888774":"code","9b886a88":"code","3d9dd4ad":"code","dbc4eeb1":"code","43246b94":"code","d833d95e":"code","5835d916":"code","9a5eb035":"code","060248af":"code","a3ac9a8f":"code","d73e343c":"code","e547a06a":"code","befe7c02":"code","001d80fa":"code","42c91dc4":"code","33795320":"code","a00710be":"code","8c265b38":"code","ab521587":"code","6421e07e":"code","13b42e44":"code","990af899":"code","74b5e5fa":"code","8f5fd591":"code","3ee3631d":"code","5bfce2b2":"code","3b0a6a5d":"code","3f4861cd":"code","3069aa51":"code","674cf35a":"markdown","ff59679e":"markdown","9eacedbf":"markdown","5b166a60":"markdown","b7071d41":"markdown","4de4fadc":"markdown","616349e5":"markdown","2be3ea4b":"markdown","41a93a5c":"markdown","2c72a10f":"markdown","efdd6a57":"markdown","8e7937a8":"markdown","654271d2":"markdown","0edc96ef":"markdown","3f87fddc":"markdown","a868483e":"markdown","bad897fc":"markdown","6903cf50":"markdown","bddfcfb1":"markdown","4990a791":"markdown","e9853d0b":"markdown","51eb3379":"markdown","0728f799":"markdown","414e84fb":"markdown","0a065623":"markdown","5dffb9ff":"markdown","5333581f":"markdown","8f43f398":"markdown","39c60335":"markdown","9a808165":"markdown","c2dea251":"markdown","39d23df5":"markdown","98107f4f":"markdown","fa09410f":"markdown","83ae44a5":"markdown","251ac39e":"markdown","977cc3f7":"markdown","135645db":"markdown"},"source":{"892b219d":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import preprocessing\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","2023fa6a":"INPUT_TRAIN = \"..\/input\/tabular-playground-series-jan-2022\/train.csv\"\nINPUT_TEST = \"..\/input\/tabular-playground-series-jan-2022\/test.csv\"\nSUBMISSION = \"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\"","621e88d7":"def check_and_plot_nan_percentage(df=None, x_offset=0, y_offset=0, print_values=True):\n    \"\"\"\n    Plots the percentage of missing values on each columns of an input dataframe\n\n            Parameters:\n                    df (DataFrame): A pandas Dataframe\n                    x_offset (float): x_offset on each bar value\n                    y_offset (float): y_offset on each bar value\n                    print_values (boolean): Set it to True to display percentage on bars\n\n    \"\"\"\n    if df is None:\n        print(\"Input dataframe is None : exit\")\n        return\n    else:\n        values = []\n        for c in df.columns:\n            values.append(100*df[c].isna().sum() \/ df.shape[0])\n        plt.figure(figsize=(18, 12))\n        plt.title(\"NaN percentage per column\",\n                  fontsize=16,\n                  fontweight='bold',\n                  pad=20\n                  )\n        plt.bar(range(0, len(df.columns)), values, edgecolor='black')\n        plt.xticks(range(0, len(df.columns)), df.columns, rotation=90)\n        xlocs, xlabs = plt.xticks()\n        if print_values:\n            for i, v in enumerate(values):\n                if v > 0:\n                    if i % 2 == 0:\n                        plt.text(xlocs[i] + x_offset, v +\n                                 y_offset, str(round(v, 1)))\n                    else:\n                        plt.text(xlocs[i] + x_offset, v +\n                                 y_offset, str(round(v, 1)))\n        plt.show()","9a1a7a2a":"df_train = pd.read_csv(INPUT_TRAIN)\ndf_test = pd.read_csv(INPUT_TEST)","ddce284b":"df_train.info()","2968a010":"df_test.info()","fb7f9f2a":"print(\"Are test columns in train columns ?\",\n      df_test.columns.isin(df_train.columns).all())","cc090835":"check_and_plot_nan_percentage(\n    df=df_train, x_offset=0, y_offset=0, print_values=True\n)","ffa4366f":"check_and_plot_nan_percentage(\n    df=df_test, x_offset=0, y_offset=0, print_values=True\n)","bcb86752":"df_train.head()","1b5d7333":"df_test.head()","07529640":"CATEGORICAL_COLUMNS = [\"country\", \"store\", \"product\"]","79f2998a":"for c in CATEGORICAL_COLUMNS:\n    print(\"Train:\", df_train[c].unique())\n    print(\"Test :\", df_test[c].unique())\n    print(\"Are train and test values the same ?\",\n          (df_test[c].unique() == df_train[c].unique()).all())","ad308be9":"format = '%Y\/%m\/%d'\ndf_train['date'] = pd.to_datetime(df_train['date'], format=format)\ndf_test['date'] = pd.to_datetime(df_test['date'], format=format)","2770b272":"for c in df_train.columns:\n    if c not in CATEGORICAL_COLUMNS:\n        print(df_train[c].describe())\n        print((\"\\n\"))","b7096126":"df_train['num_sold'].hist()\nplt.show()","22245639":"df_train = df_train.drop(columns=[\"row_id\"])","80f4db0a":"df_train[\"weekday\"] = df_train[\"date\"].dt.dayofweek\ndf_train[\"month\"] = df_train[\"date\"].dt.month\ndf_train[\"year\"] = df_train[\"date\"].dt.year\ndf_train['is_weekend'] = (df_train['date'].dt.weekday >= 5).astype(int)\n\ndf_test[\"weekday\"] = df_test[\"date\"].dt.dayofweek\ndf_test[\"month\"] = df_test[\"date\"].dt.month\ndf_test[\"year\"] = df_test[\"date\"].dt.year\ndf_test['is_weekend'] = (df_test['date'].dt.weekday >= 5).astype(int)","07c72664":"df_train = df_train.drop(columns=[\"date\"])\ndf_test = df_test.drop(columns=[\"date\"])","72c82f99":"years = list(df_train[\"year\"].unique())\nmonths = list(df_train[\"month\"].unique())\ndays = list(df_train[\"weekday\"].unique())\ncountries = list(df_train[\"country\"].unique())\nstores = list(df_train[\"store\"].unique())\nproducts = list(df_train[\"product\"].unique())","d09668b7":"ax = df_train[\"is_weekend\"].value_counts(normalize=True).plot(kind=\"bar\",\n                                                              title=\"Sales distribution over weekends\",\n                                                              ylabel=\"Sales percentage\"\n                                                              )\nax.set_xticklabels([\"During week\", \"During weekend\"])\nplt.show()","6497cdb8":"total_sales_per_year = []\ntotal_sales_per_month_per_year = []\ntotal_sales_per_day_over_year = []\n\nfor i in range(0, len(years)):\n    total_sales_per_month_per_year.append([])\n    total_sales_per_day_over_year.append([])\n    total_sales_per_year.append(\n        df_train[df_train[\"year\"] == years[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"year\"] == years[i]].copy(deep=True)\n    for month in months:\n        total_sales_per_month_per_year[i].append(\n            sub_df[sub_df[\"month\"] == month][\"num_sold\"].sum())\n    for day in days:\n        total_sales_per_day_over_year[i].append(\n            sub_df[sub_df[\"weekday\"] == day][\"num_sold\"].sum())\n\ntotal_sales_per_year \/= sum(total_sales_per_year)\n\nplt.bar(years, total_sales_per_year)\nplt.xticks(years)\nplt.title(\"Total sales over years\")\nplt.ylabel(\"Percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(years), figsize=[16, 6], sharey=True)\nfor i in range(0, len(years)):\n    axs[i].bar(months, total_sales_per_month_per_year[i])\n    axs[i].set_title(str(\"Year: \" + str(years[i])))\n    axs[i].set_xticks(months)\n    axs[i].set_xlabel(\"Month\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(years), figsize=[16, 6], sharey=True)\nfor i in range(0, len(years)):\n    axs[i].bar(days, total_sales_per_day_over_year[i])\n    axs[i].set_title(str(\"Year: \" + str(years[i])))\n    axs[i].set_xticks(days)\n    axs[i].set_xlabel(\"Weekday\")\nplt.show()","d6fd018c":"total_sales_per_country = []\ntotal_sales_per_country_over_years = []\nfor i in range(0, len(countries)):\n    total_sales_per_country_over_years.append([])\n    total_sales_per_country.append(\n        df_train[df_train[\"country\"] == countries[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"country\"] == countries[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_country_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n\ntotal_sales_per_country \/= sum(total_sales_per_country)\nplt.bar(countries, total_sales_per_country)\nplt.xticks(countries)\nplt.title(\"Total sales over countries\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(countries), figsize=[16, 6], sharey=True)\nfor i in range(0, len(countries)):\n    axs[i].bar(years, total_sales_per_country_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Country: \" + str(countries[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()","65fecfdf":"total_sales_per_store = []\ntotal_sales_per_store_over_years = []\ntotal_sales_per_store_per_country = []\ntotal_sales_per_store_per_product = []\n\nfor i in range(0, len(stores)):\n    total_sales_per_store_over_years.append([])\n    total_sales_per_store_per_country.append([])\n    total_sales_per_store_per_product.append([])\n    total_sales_per_store.append(\n        df_train[df_train[\"store\"] == stores[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"store\"] == stores[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_store_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n    for country in countries:\n        total_sales_per_store_per_country[i].append(\n            sub_df[sub_df[\"country\"] == country][\"num_sold\"].sum())\n    for product in products:\n        total_sales_per_store_per_product[i].append(\n            sub_df[sub_df[\"product\"] == product][\"num_sold\"].sum())\n\ntotal_sales_per_store \/= sum(total_sales_per_store)\nplt.bar(stores, total_sales_per_store)\nplt.xticks(stores)\nplt.title(\"Total sales per store over the years\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(years, total_sales_per_store_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(countries, total_sales_per_store_per_country[i])\n    axs[i].set_xticks(countries)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Country\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(products, total_sales_per_store_per_product[i])\n    axs[i].set_xticks(products)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Product\")\nplt.show()","5db9a811":"total_sales_per_product = []\ntotal_sales_per_product_over_years = []\ntotal_sales_per_product_per_country = []\n\nfor i in range(0, len(products)):\n    total_sales_per_product_over_years.append([])\n    total_sales_per_product_per_country.append([])\n    total_sales_per_product.append(\n        df_train[df_train[\"product\"] == products[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"product\"] == products[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_product_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n    for country in countries:\n        total_sales_per_product_per_country[i].append(\n            sub_df[sub_df[\"country\"] == country][\"num_sold\"].sum())\n\ntotal_sales_per_product \/= sum(total_sales_per_product)\nplt.bar(products, total_sales_per_product)\nplt.xticks(products)\nplt.title(\"Total sales per product over the years\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(products), figsize=[16, 6], sharey=True)\nfor i in range(0, len(products)):\n    axs[i].bar(years, total_sales_per_product_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Products: \" + str(products[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(products), figsize=[16, 6], sharey=True)\nfor i in range(0, len(products)):\n    axs[i].bar(countries, total_sales_per_product_per_country[i])\n    axs[i].set_xticks(countries)\n    axs[i].set_title(str(\"Product: \" + str(products[i])))\n    axs[i].set_xlabel(\"Country\")\nplt.show()","112802de":"le = preprocessing.LabelEncoder()\n\nfor c in CATEGORICAL_COLUMNS:\n    df_train[c] = le.fit_transform(df_train[c])\n    df_test[c] = le.transform(df_test[c])","f3acd9e6":"y = np.array(df_train[\"num_sold\"])\ny = y.reshape(-1, 1)","088eaffb":"X = np.array(df_train.drop(columns=\"num_sold\"))\nX_to_pred = np.array(df_test.drop(columns=[\"row_id\"]))","43888774":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","9b886a88":"def smape(y_true, y_pred):\n    return (100\/y_true.shape[0]) * np.sum(2 * np.abs(y_pred - y_true) \/ (np.abs(y_true) + np.abs(y_pred)))\n\n\n# SMAPE must be lowered to increase performances\nsmape_score = make_scorer(score_func=smape, greater_is_better=False)","3d9dd4ad":"rfr = RandomForestRegressor()\nparameters = {'n_estimators': (100, 200, 300, 500, 1000, 1500, 2000),\n              'max_depth': [None, 3, 5],\n              'bootstrap': [True, False]\n              }\nres = GridSearchCV(estimator=rfr, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch: \", res.best_params_)\n\nopti_rfr = res.best_estimator_","dbc4eeb1":"y_pred = np.round(opti_rfr.predict(X_test)).reshape(-1, 1).astype(int)","43246b94":"smp = smape(y_true=y_test, y_pred=y_pred)\nprint(\"SMAPE on test set =\", smp)","d833d95e":"biais = [i for i in range(-30, 30)]","5835d916":"smps = []\nfor i in biais:\n    smp = smape(y_true=y_test, y_pred=y_pred+i)\n    smps.append(smp)\n\nprint(\"Best SMAPE on test =\", min(smps),\n      \" biais =\", biais[smps.index(min(smps))])","9a5eb035":"plt.plot(biais, smps)\nplt.title(\"Impact of biais on predictions\")\nplt.ylabel(\"SMAPE Score\")\nplt.xlabel(\"Biais value\")\nplt.show()","060248af":"gboost = GradientBoostingRegressor(random_state=0)\nparameters = {'n_estimators': (50, 75, 100, 150, 200, 250, 300, 500),\n              'learning_rate': (0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.15, 0.2),\n              'max_depth': (3, 5, 8), \"max_features\": [\"auto\", \"log2\"]\n              }\nres = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost = res.best_estimator_","a3ac9a8f":"y_pred = np.round(opti_gboost.predict(X_test)).reshape(-1, 1).astype(int)","d73e343c":"smp = smape(y_true=y_test, y_pred=y_pred)\nprint(\"SMAPE on test set =\", smp)","e547a06a":"smps = []\nfor i in biais:\n    smp = smape(y_true=y_test, y_pred=y_pred+i)\n    smps.append(smp)\n\nprint(\"Best SMAPE on test =\", min(smps),\n      \" biais =\", biais[smps.index(min(smps))])","befe7c02":"plt.plot(biais, smps)\nplt.title(\"Impact of biais on predictions\")\nplt.ylabel(\"SMAPE Score\")\nplt.xlabel(\"Biais value\")\nplt.show()","001d80fa":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y > limit\ny[outliers] = np.mean(y)\ny = y.reshape(-1, 1)","42c91dc4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","33795320":"gboost = GradientBoostingRegressor(random_state=0)\nparameters = {'n_estimators': (50, 75, 100, 150, 200, 250, 300, 500),\n              'learning_rate': (0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.15, 0.2),\n              'max_depth': (3, 5, 8), \"max_features\": [\"auto\", \"log2\"]\n              }\nres = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost2 = res.best_estimator_","a00710be":"y_pred = np.round(opti_gboost2.predict(X_test)).reshape(-1, 1).astype(int)","8c265b38":"smp = smape(y_true=y_test, y_pred=y_pred)\nprint(\"SMAPE on test set =\", smp)","ab521587":"smps = []\nfor i in biais:\n    smp = smape(y_true=y_test, y_pred=y_pred+i)\n    smps.append(smp)\n\nprint(\"Best SMAPE on test =\", min(smps),\n      \" biais =\", biais[smps.index(min(smps))])","6421e07e":"plt.plot(biais, smps)\nplt.title(\"Impact of biais on predictions\")\nplt.ylabel(\"SMAPE Score\")\nplt.xlabel(\"Biais value\")\nplt.show()","13b42e44":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y > limit\ny[outliers] = limit\ny = y.reshape(-1, 1)","990af899":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","74b5e5fa":"gboost = GradientBoostingRegressor(random_state=0)\nparameters = {'n_estimators': (50, 75, 100, 150, 200, 250, 300, 500),\n              'learning_rate': (0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.15, 0.2),\n              'max_depth': (3, 5, 8), \"max_features\": [\"auto\", \"log2\"]\n              }\nres = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost3 = res.best_estimator_","8f5fd591":"y_pred = np.round(opti_gboost3.predict(X_test)).reshape(-1, 1).astype(int)","3ee3631d":"smp = smape(y_true=y_test, y_pred=y_pred)\nprint(\"SMAPE on test set =\", smp)","5bfce2b2":"smps = []\nfor i in biais:\n    smp = smape(y_true=y_test, y_pred=y_pred+i)\n    smps.append(smp)\n\nprint(\"Best SMAPE on test =\", min(smps),\n      \" biais =\", biais[smps.index(min(smps))])\n\nplt.plot(biais, smps)\nplt.title(\"Impact of biais on predictions\")\nplt.ylabel(\"SMAPE Score\")\nplt.xlabel(\"Biais value\")\nplt.show()","3b0a6a5d":"best_biais = -1","3f4861cd":"num_sold = np.round(opti_gboost.predict(X_to_pred)).reshape(-1, 1).astype(int) + best_biais","3069aa51":"df = pd.read_csv(SUBMISSION)\ndf[\"num_sold\"] = num_sold\ndf.to_csv(\"submission.csv\", index=False)","674cf35a":"# 4. FEATURE ENGINEERING","ff59679e":"<b> Evaluate the performances with the SMAPE metric between predictions and true values.","9eacedbf":"# FUNCTIONS","5b166a60":"<b> Evaluate the impact of introducing a biais on the predictions","b7071d41":"### 2.4 Convert date to datetime object with the right format","4de4fadc":"# 1. DATA LOADING","616349e5":"# 9. Impact of replacing outliers by limit","2be3ea4b":"# 7. Training","41a93a5c":"<b> Evaluate the impact of introducing a biais on the predictions","2c72a10f":"# CONSTANTS","efdd6a57":"### 7.1 Random forest regressor","8e7937a8":"### 6.3 Split train dataframe into train and test set","654271d2":"### 6.1 Encode categorical columns","0edc96ef":"* Tensor girl: https:\/\/www.kaggle.com\/usharengaraju\/tensorflow-tf-data-keraspreprocessinglayers-w-b for the cheat code to compute the feature is_weekend\n\n* SMAPE formula : https:\/\/en.wikipedia.org\/wiki\/Symmetric_mean_absolute_percentage_error","3f87fddc":"# 6.PREPROCESSING","a868483e":"### 6.4 Create the appropriate score","bad897fc":"# 8. Impact of replacing outliers by mean","6903cf50":"### 7.2 GradientBoosting","bddfcfb1":"For training, i will train Random forest regressor + Gradient boosting model on the train set. The training will be performed on a grid to optimize hyperparameters and validate them with a cross validation on 5-folds. \n\nSo, GridSearchCV will be used.","4990a791":"# 3. CLEANING","e9853d0b":"### 2.5 Describe numerical columns","51eb3379":"# 2. FIRST DATA LOOK AROUND","0728f799":"* Sales growth from year to year\n* A seasonality is observable over the years: Sales increase in December and January, decrease in February, increase again in March, April and May, then decrease until August and increase again from September\n* Weekend concentrate sales","414e84fb":"<b> Evaluate the performances with the SMAPE metric between predictions and true values.","0a065623":"# 10. Forecast the number of sales for the coming years (Test dataframe)","5dffb9ff":"<b> Evaluate the impact of introducing a biais on the predictions","5333581f":"### 2.1 Check column names","8f43f398":"# REFERENCES","39c60335":"<b> Evaluate the performances with the SMAPE metric between predictions and true values.","9a808165":"# IMPORTS","c2dea251":"# 5. DATA EXPLORATION","39d23df5":"### 2.2 Check missing values on each column","98107f4f":"I drop date column on each dataset after doing the feature engineering because the intertia will be coupled otherwise.","fa09410f":"Note : using a y_train filtered or clamped decrease performance on the test dataframe (8.91 with replacement by limit vs. 8.71 without any processing)","83ae44a5":"<b> Evaluate the performances with the SMAPE metric between predictions and true values.","251ac39e":"### 2.3 Store categorcial columns and check unique values between train and test set","977cc3f7":"<b> Evaluate the impact of introducing a biais on the predictions","135645db":"### 6.2 Construct X and y arrays"}}