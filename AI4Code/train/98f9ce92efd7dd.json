{"cell_type":{"571af6a7":"code","d2a0354c":"code","7cf597a9":"code","3e0e71f8":"code","452a9a06":"code","535d8102":"code","bd739f99":"code","6b44a340":"code","777880c4":"code","809a6ffc":"code","6d9fdf97":"code","30edee94":"code","63c56fe9":"code","c1aeb443":"code","b1e62759":"code","446b876e":"code","7ba77664":"code","698a263d":"code","eb45a29a":"code","9a3c45ad":"code","2a8009ae":"code","5e04ed93":"code","db5370af":"code","8755ab7b":"code","4be84230":"code","bf4bd62c":"markdown","3583765e":"markdown","bbdbcf4a":"markdown","92575df8":"markdown","3e93d259":"markdown"},"source":{"571af6a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2a0354c":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\", low_memory=False)\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\", low_memory=False)\ntrain_df.info(memory_usage=\"deep\")","7cf597a9":"train_df.head(10)","3e0e71f8":"train_df[\"target\"].unique() # There are total of 9 classes","452a9a06":"Summary = pd.DataFrame(train_df.dtypes, columns=['Dtype'])\nSummary[\"max\"] = train_df.max()\nSummary[\"min\"] = train_df.min()\nSummary[\"Null\"] = train_df.isnull().sum() # to get null values\nSummary[\"First\"] = train_df.iloc[0] # to get first value\nSummary[\"Second\"] = train_df.iloc[1] # to get second value\nSummary\n","535d8102":"train_df = train_df.drop([\"id\"], axis = 1)","bd739f99":"target = train_df[\"target\"]","6b44a340":"train_df = train_df.drop([\"target\"], axis = 1)","777880c4":"test = test.drop([\"id\"], axis = 1)","809a6ffc":"values, counts = np.unique(target, return_counts=True)","6d9fdf97":"values","30edee94":"res = (counts\/len(target)) * 100\nprint(res)","63c56fe9":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(train_df, target, test_size = 0.1, stratify=target, random_state = 123)","c1aeb443":"values, counts = np.unique(y_train, return_counts=True)\nprint(values)\nprint(counts)\nres = (counts\/len(y_train)) * 100\nprint(res)","b1e62759":"values, counts = np.unique(y_test, return_counts=True)\nprint(values)\nprint(counts)\nres = (counts\/len(y_test)) * 100\nprint(res)","446b876e":"enc = LabelEncoder()\ny_train = enc.fit_transform(y_train)\ny_test = enc.transform(y_test)    \n# test = enc.transform(test)","7ba77664":"train_df.columns","698a263d":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\ntest = scaler.transform(test)","eb45a29a":"# model = LogisticRegression(random_state=123 )\nimport lightgbm as lgb\n# model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=3, max_depth=10, random_state=123)\n# model = RandomForestClassifier(n_estimators=300, max_depth=15, n_jobs=-1, random_state=123)\n# model.fit(X_train, y_train)\nmodel=lgb.LGBMClassifier()\n\n# clf.fit(X_train_valid, y_train_valid)","9a3c45ad":"model.fit(X_train, y_train)\ny_pred = model.predict_proba(X_test)","2a8009ae":"print(y_pred)\nprint(y_test)\nvalues, counts = np.unique(y_pred, return_counts=True)\nprint(values)\nprint(counts)","5e04ed93":"from sklearn.metrics import log_loss\nval = log_loss(y_test,y_pred)\nprint(val)\n# 1.8307023065814374 for LR\n# 4.002083813135203 for RC\n# 1.8306874700003668 for parameterized RC\n# 1.7616033784491318 for more parametrized RC\n# 1.755212422361672 for lightbgm","db5370af":"sub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\ny_res = model.predict_proba(test)","8755ab7b":"y_res.shape","4be84230":"sub.head(10)\npredictions = pd.DataFrame()\npredictions[\"id\"] = sub[\"id\"]\npredictions = pd.concat([predictions, pd.DataFrame(y_res, columns=[\"Class_\" + str(x) for x in np.arange(1, 10, 1)])], axis=1)\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","bf4bd62c":"We can see that the values are not evenly distributed so we need to split the data carefully","3583765e":"Checking whether straification worked?","bbdbcf4a":"Since the values are at diff scales, we need to deal with scaling.","92575df8":"We can see that there are 200,000 values in here which are alot","3e93d259":"We can see that all the columns have non null values so no worries"}}