{"cell_type":{"667d131e":"code","c96ffe0d":"code","9079d6d7":"code","ca4cae56":"code","da5abcf7":"code","42447fc4":"code","abb91a04":"code","ff83ed6d":"code","b94c3c4d":"code","7bf76687":"code","c677553c":"code","857e3a3c":"code","553367f4":"code","5af9a36e":"code","cc8a8a2f":"code","302121f3":"code","268d5db8":"code","4e29dead":"code","09f84f9e":"code","e947016a":"code","9c62fec0":"code","4c697764":"code","3b62659e":"markdown","18efca25":"markdown","0df3af34":"markdown","91f27b59":"markdown","22c1ab52":"markdown"},"source":{"667d131e":"%matplotlib inline\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c96ffe0d":"import gc\nfrom urllib.request import urlopen\nimport json\nimport requests\n\nimport sys\nfrom itertools import chain\nfrom tqdm import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nplt.style.use('fivethirtyeight')\n\nfrom datetime import datetime, timedelta\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.express as px","9079d6d7":"def display_missing(df, head=True):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","ca4cae56":"%%time\n\ndf_cases = pd.read_csv('https:\/\/usafactsstatic.blob.core.windows.net\/public\/data\/covid-19\/covid_confirmed_usafacts.csv')\ndf_cases = df_cases.melt(id_vars=[\"countyFIPS\", \"County Name\", \"State\", 'stateFIPS'], var_name=\"Date\", value_name=\"Value\")\ndf_cases = df_cases.rename(columns={'countyFIPS': 'fips', 'Value': 'cases', \"County Name\":'county', 'State':'state', 'Date':'date'})\ndf_cases.date = pd.to_datetime(df_cases.date)\n\ndf_deaths = pd.read_csv('https:\/\/usafactsstatic.blob.core.windows.net\/public\/data\/covid-19\/covid_deaths_usafacts.csv')\ndf_deaths = df_deaths.melt(id_vars=[\"countyFIPS\", \"County Name\", \"State\", 'stateFIPS'], var_name=\"Date\", value_name=\"Value\")\ndf_deaths = df_deaths.rename(columns={'countyFIPS': 'fips', 'Value': 'deaths', 'Date':'date'})\ndf_deaths = df_deaths[['fips', 'date', 'deaths']]\ndf_deaths.date = pd.to_datetime(df_deaths.date)\n\ndf_all = df_cases.merge(df_deaths, how='left', left_on=['fips', 'date'], right_on=['fips', 'date'])\ndf_all.drop(df_all[df_all['fips'] == 0].index, inplace=True)\ndf_all = df_all.reset_index(drop=True)\n\ndf_pop = pd.read_csv('https:\/\/usafactsstatic.blob.core.windows.net\/public\/data\/covid-19\/covid_county_population_usafacts.csv')[['countyFIPS', 'population']]\ndf_pop = df_pop.rename(columns={'countyFIPS': 'fips'})\ndf_pop.fips = df_pop.fips.astype(int)\ndf_pop.population = df_pop.population.astype(int)\n\ndf_all = df_all.merge(df_pop, how='left', left_on=['fips'], right_on=['fips']).drop('stateFIPS', axis=1)\n\nus_confirmed = df_all.sort_values(by=['date', 'state', 'county' ])\nus_confirmed.reset_index(drop=True)\nus_confirmed['county'] = us_confirmed['county'].apply(lambda x: x.replace(' County', '') )\n\nlatest_date = us_confirmed.date.max()\n\n## drop dates earlier then 3\/1\/2020\nus_confirmed.drop(us_confirmed[us_confirmed['date'] < pd.to_datetime('03-15-2020')].index, inplace=True)\nus_confirmed","da5abcf7":"%%time\n\n## https:\/\/www.kaggle.com\/jmarfati\/actual-spread-of-covid19-us-county-level-analysis\n\n## adding mortality features\nus_confirmed['mortality'] = us_confirmed['deaths']\/ us_confirmed['cases']\nus_confirmed['mortality'] = us_confirmed['mortality'].fillna(0)\n\nus_confirmed['deaths_per_million'] = us_confirmed['deaths'] * 1000000\/ us_confirmed['population']\nus_confirmed['cases_per_million'] = us_confirmed['cases'] * 1000000\/ us_confirmed['population']\n\n# Results of recent antibody test in California suggest that the number of people likely \n# infected is 28 to 80 time the number of confirmed cases. If indeed the spread is 28-fold; \n# the distribution of percentage of population carrying Covid-19 antibody is displayed below    \n# https:\/\/news.usc.edu\/168987\/antibody-testing-results-covid-19-infections-los-angeles-county\/\n\nus_confirmed['likely_infected_80'] = np.round(us_confirmed['cases'] * 80\/ us_confirmed['population'], 2)\nus_confirmed['likely_infected_80'] = np.clip(us_confirmed['likely_infected_80'], 0, 1)\nus_confirmed['likely_infected_28'] = np.round(us_confirmed['cases'] * 28\/ us_confirmed['population'], 2)\nus_confirmed['likely_infected_28'] = np.clip(us_confirmed['likely_infected_28'], 0, 1)\n\nus_confirmed = us_confirmed.replace([np.nan, np.inf, -np.inf], 0)   \nus_confirmed = us_confirmed[us_confirmed['fips'] > 0]\n\n## save dataset\nus_confirmed.to_csv('us-daily-counties-population-permil-likely-infected.csv')\nus_confirmed.info()","42447fc4":"cases_by_state = us_confirmed[us_confirmed['date'] == latest_date].groupby(by=['state'])['cases'].sum().sort_values()\ndeaths_by_state = us_confirmed[us_confirmed['date'] == latest_date].groupby(by=['state'])['deaths'].sum().sort_values()\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 20))\nax1.title.set_text(F'Cases by state as of {latest_date}')\ncases_by_state.plot(kind='barh', ax=ax1)\n\nax2.title.set_text(F'Deaths by state as of {latest_date}')\ndeaths_by_state.plot(kind='barh', ax=ax2)","abb91a04":"def fill_for_state( us_df, state, valid_counties):\n    \"\"\"this function extract state data from US dataset, filling missisng dates for counties\"\"\"\n    \n    state_df = us_df[us_df['state']==state]\n    ## data cleanup\n    state_df = state_df[state_df['county'].isin(valid_counties)]\n    state_df = state_df.sort_values(by='date').reset_index(drop=True)\n    \n    ## we need to insert missing dates for counties\n    unique_dates = state_df['date'].unique()\n\n    i = 0\n    added_rows = dict()\n    for idx, d in enumerate(unique_dates):\n        one_day = state_df[state_df['date'] == d]\n        for county in valid_counties:\n            exists = one_day[ one_day['county'] == county].any()\n            if not exists['date']:\n                fips = state_df[ state_df['county'] == county]['fips']\n                population = state_df[ state_df['county'] == county]['population']\n                added_rows[i] = { 'date':d, 'county':county, 'state':state, 'fips':fips.iloc[0], 'cases':0, 'deaths':0, 'population':population.iloc[0]}\n                i = i + 1\n                \n    added = pd.DataFrame.from_dict(added_rows, \"index\")\n    #print(f'missing rows: {len(added)}')\n    state_df = pd.concat([state_df, added]) \n    state_df = state_df.replace([np.nan, np.inf, -np.inf], 0)   \n    state_df = state_df.sort_values(by='date').reset_index(drop=True)\n    return state_df","ff83ed6d":"%%time\n\n## get MD dataset\nmd_counties = np.unique(us_confirmed[us_confirmed['state']=='MD']['county'].values)\nmd_df = fill_for_state(us_confirmed, 'MD', md_counties)\nmd_df.to_csv('us-md-daily-counties-population-permil-likely-infected.csv')\n\n## get VA dataset\nva_counties = np.unique(us_confirmed[us_confirmed['state']=='VA']['county'].values)\nva_df = fill_for_state(us_confirmed, 'VA', va_counties)\nva_df.to_csv('us-va-daily-counties-population-permil-likely-infected.csv')\n\n## get DC dataset, no counties\ndc_df = us_confirmed[us_confirmed['state']=='DC']\n\n\ndef to_daily_num(grp):\n    grp['cases'] = grp['cases'] - grp['cases'].shift()\n    grp['deaths'] = grp['deaths'] - grp['deaths'].shift()\n    return grp\n\n## to daily numbers\ndc_df['cases'] = dc_df['cases'] - dc_df['cases'].shift()\ndc_df['deaths'] = dc_df['deaths'] - dc_df['deaths'].shift()\ndc_df = dc_df.replace([np.nan, np.inf, -np.inf], 0)   \n\nva_df.groupby(by=['county']).apply(to_daily_num)\nva_df = va_df.replace([np.nan, np.inf, -np.inf], 0)   \n\nmd_df.groupby(by=['county']).apply(to_daily_num)\nmd_df = md_df.replace([np.nan, np.inf, -np.inf], 0)","b94c3c4d":"def counties_with_top5_cases_by_state(state_df, column, title, ax):    \n    unique_dates = np.unique(md_df['date'].apply(lambda x: x.strftime('%Y-%m-%d')).values)\n    top5 = state_df[state_df.date == unique_dates.max()].nlargest(5, column)\n    date_ticks = range(0, len(unique_dates), 5)\n    ax.set_xticks(date_ticks);\n    ax.set_xticklabels([unique_dates[i] for i in date_ticks], rotation='vertical');\n    ax.set_xlabel('Date');\n    sns.lineplot( x=unique_dates, y = state_df[state_df['county'] == top5['county'].values[0]][column], ax=ax)\n    if len(top5) > 1:\n        sns.lineplot( x=unique_dates, y = state_df[state_df['county'] == top5['county'].values[1]][column], ax=ax)\n        sns.lineplot( x=unique_dates, y = state_df[state_df['county'] == top5['county'].values[2]][column], ax=ax)\n        sns.lineplot( x=unique_dates, y = state_df[state_df['county'] == top5['county'].values[3]][column], ax=ax)\n        sns.lineplot( x=unique_dates, y = state_df[state_df['county'] == top5['county'].values[4]][column], ax=ax)\n    ax.legend(top5['county'].values, loc='upper left', shadow=True)\n    ax.set_title(title)\n    ","7bf76687":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21, 6))\ncounties_with_top5_cases_by_state(md_df, 'cases', 'Top 5 counties cases in Maryland', ax1)\ncounties_with_top5_cases_by_state(va_df, 'cases', 'Top 5 counties cases in Virginia', ax2)\ncounties_with_top5_cases_by_state(dc_df, 'cases', 'Cases in District of Columbia', ax3)\nplt.show()","c677553c":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21, 6))\ncounties_with_top5_cases_by_state(md_df, 'deaths', 'Top 5 counties deaths in Maryland', ax1)\ncounties_with_top5_cases_by_state(va_df, 'deaths', 'Top 5 counties deaths in Virginia', ax2)\ncounties_with_top5_cases_by_state(dc_df, 'deaths', 'Deaths in District of Columbia', ax3)\nplt.show()","857e3a3c":"## according to google MD population as of 2020 is 5,773,552, so not exactly sure why there is a difference\nmd_population = md_df[md_df['date'] == latest_date]['population'].sum()\n\nmd_mortality = md_df['deaths'].sum() \/ md_df['cases'].sum()\nva_mortality = va_df['deaths'].sum() \/ va_df['cases'].sum()\ndc_mortality = dc_df['deaths'].sum() \/ dc_df['cases'].sum()\n\ny = [md_mortality, va_mortality, dc_mortality]\nx = ['MD', 'VA', 'DC']\n\nf, ax = plt.subplots(1, 1, figsize=(10, 6))\nsns.barplot(x=x, y=y, ax=ax)\nax.set_title('Mortality rate')","553367f4":"## this dataset has geo info in it, so let use it\nus = pd.read_csv('\/kaggle\/input\/uncover\/USAFacts\/confirmed-covid-19-cases-in-us-by-state-and-county.csv', header=0, parse_dates=True)\ndc_metro_geo = us[us['state_name'].isin(['VA', 'MD', 'DC'])].drop(['state_fips','date', 'confirmed'], axis=1)\ndc_metro_geo.drop_duplicates(keep='first', inplace=True)\ndc_metro_geo.drop(['county_name','state_name'], inplace=True, axis=1)\n\n## merge\ndc_metro = pd.concat([md_df, va_df, dc_df])\ndc_metro = dc_metro.merge( dc_metro_geo[['county_fips', 'lat','long','geometry']], how='left', left_on='fips', right_on='county_fips').drop('county_fips', axis=1)\n#dc_metro","5af9a36e":"## folium use is from notebook\n## https:\/\/www.kaggle.com\/soham1024\/covid-19-india-visualization-forecasting\n\nimport folium\nfrom folium.plugins import HeatMap\n\n#heat_data = [[[row['Latitude'],row['Longitude']] for index, row in heat_df[heat_df['Weight'] == i].iterrows()] for i in range(0,13)]","cc8a8a2f":"md_top10_cases = md_df[md_df.date == latest_date].nlargest(2, 'cases')[['fips','cases']]\nmd_top10_cases = md_top10_cases.merge(dc_metro[['fips','county', 'lat', 'long']], how='left', left_on='fips', right_on='fips')\nmd_top10_cases.drop_duplicates(keep='first', inplace=True)\n\nva_top10_cases = va_df[va_df.date == latest_date].nlargest(2, 'cases')[['fips','cases']]\nva_top10_cases = va_top10_cases.merge(dc_metro[['fips','county', 'lat', 'long']], how='left', left_on='fips', right_on='fips')\nva_top10_cases.drop_duplicates(keep='first', inplace=True)\n\ndc_top_cases = dc_df[dc_df.date == latest_date].nlargest(1, 'cases')[['fips','cases']]\ndc_top_cases = dc_top_cases.merge(dc_metro[['fips','county', 'lat', 'long']], how='left', left_on='fips', right_on='fips')\ndc_top_cases.drop_duplicates(keep='first', inplace=True)\n\nmd_heat_data = [[row['lat'],row['long']] for index, row in md_top10_cases.iterrows()]\nva_heat_data = [[row['lat'],row['long']] for index, row in va_top10_cases.iterrows()]\ndc_heat_data = [[row['lat'],row['long']] for index, row in dc_top_cases.iterrows()]\n\nheat_data = pd.concat([pd.DataFrame(md_heat_data), pd.DataFrame(va_heat_data), pd.DataFrame(dc_heat_data)])\nheat_data = head_data.reset_index(drop=True)","302121f3":"f_map = folium.Map(location=[38.889248, -77.050636], zoom_start=7, tiles='cartodbpositron')\nHeatMap(heat_data,radius=20.5, blur = 6.5).add_to(f_map)\nf_map","268d5db8":"## Convert topology json into geojson\n## The code is from https:\/\/gist.github.com\/perrygeo\/1e767e42e8bc54ad7262\n\n\ndef rel2abs(arc, scale=None, translate=None):\n    \"\"\"Yields absolute coordinate tuples from a delta-encoded arc.\n    If either the scale or translate parameter evaluate to False, yield the\n    arc coordinates with no transformation.\"\"\"\n    if scale and translate:\n        a, b = 0, 0\n        for ax, bx in arc:\n            a += ax\n            b += bx\n            yield scale[0]*a + translate[0], scale[1]*b + translate[1]\n    else:\n        for x, y in arc:\n            yield x, y\n\ndef coordinates(arcs, topology_arcs, scale=None, translate=None):\n    \"\"\"Return GeoJSON coordinates for the sequence(s) of arcs.\n    \n    The arcs parameter may be a sequence of ints, each the index of a\n    coordinate sequence within topology_arcs\n    within the entire topology -- describing a line string, a sequence of \n    such sequences -- describing a polygon, or a sequence of polygon arcs.\n    \n    The topology_arcs parameter is a list of the shared, absolute or\n    delta-encoded arcs in the dataset.\n    The scale and translate parameters are used to convert from delta-encoded\n    to absolute coordinates. They are 2-tuples and are usually provided by\n    a TopoJSON dataset. \n    \"\"\"\n    if isinstance(arcs[0], int):\n        coords = [\n            list(\n                rel2abs(\n                    topology_arcs[arc if arc >= 0 else ~arc],\n                    scale, \n                    translate )\n                 )[::arc >= 0 or -1][i > 0:] \\\n            for i, arc in enumerate(arcs) ]\n        return list(chain.from_iterable(coords))\n    elif isinstance(arcs[0], (list, tuple)):\n        return list(\n            coordinates(arc, topology_arcs, scale, translate) for arc in arcs)\n    else:\n        raise ValueError(\"Invalid input %s\", arcs)\n\ndef geometry(obj, topology_arcs, scale=None, translate=None):\n    \"\"\"Converts a topology object to a geometry object.\n    \n    The topology object is a dict with 'type' and 'arcs' items, such as\n    {'type': \"LineString\", 'arcs': [0, 1, 2]}.\n    See the coordinates() function for a description of the other three\n    parameters.\n    \"\"\"\n    return {\n        \"type\": obj['type'], \n        \"coordinates\": coordinates(\n            obj['arcs'], topology_arcs, scale, translate )}","4e29dead":"from shapely.geometry import asShape\n\n# get geo data for plotting\nr = requests.get(url='https:\/\/raw.githubusercontent.com\/deldersveld\/topojson\/master\/countries\/us-states\/MD-24-maryland-counties.json')\nmd_json = r.json()\n\ntopojson_path = sys.argv[1]\ngeojson_path = sys.argv[2]\n\ntopology = md_json\n\n# file can be renamed, the first 'object' is more reliable\nlayername = list(topology['objects'].keys())[0]  \n\nfeatures = topology['objects'][layername]['geometries']\nscale = topology['transform']['scale']\ntrans = topology['transform']['translate']\n\nfc = {'type': \"FeatureCollection\", 'features': []}\n\nfor id, tf in enumerate(features):\n    f = {'id': id, 'type': \"Feature\"}\n    f['properties'] = tf['properties'].copy()\n\n    geommap = geometry(tf, topology['arcs'], scale, trans)\n    geom = asShape(geommap).buffer(0)\n    assert geom.is_valid\n    f['geometry'] = geom.__geo_interface__\n\n    fc['features'].append(f) ","09f84f9e":"district_mapping = {\n    'Allegany' : 6, \n    'Anne Arundel' : 20, \n    'Baltimore' : 15, \n    'Baltimore City' : 14,\n    'Calvert' : 21, \n    'Caroline' : 13, \n    'Carroll' : 3, \n    'Cecil' : 12, \n    'Charles' : 10, \n    'Dorchester' : 11,\n    'Frederick' : 22, \n    'Garrett' : 7, \n    'Harford' : 16, \n    'Howard' : 23, \n    'Kent' : 8, \n    'Montgomery' : 1,\n    \"Prince George's\" : 9, \n    \"Queen Anne's\" : 5, \n    'Somerset' : 4,\n    \"St. Mary's\" : 17, \n    'Talbot' : 2, \n    'Washington' : 18, \n    'Wicomico' : 19,\n    'Worcester': 0,    \n}\n\ndf = md_df[['date','cases', 'county']]\ndf['total'] = df.sort_values('date').groupby('county').cumsum()\ndf['id'] = df['county'].apply(lambda x: district_mapping[x])\ndf['date'] = df['date'].astype(str)\n\n\nfig = px.choropleth(df,\n                    geojson=fc,\n                    locations='id',\n                    animation_frame='date',\n                    color_continuous_scale=\"OrRd\",\n                    hover_name='county',\n                    range_color=(0, df['total'].max()),\n                    color='total',\n                   title='Maryland: COVID-19 cases per county')\n\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_geos(projection_type=\"orthographic\")\nfig.update_layout(height=600, margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","e947016a":"plt.figure(figsize=(10,6))\nsns.distplot(md_df.likely_infected_80, hist=True, kde=False, color = 'blue', hist_kws={'edgecolor':'black', 'linewidth':1}, kde_kws={'linewidth': 2})\nsns.distplot(md_df.likely_infected_28, hist=True, kde=False, color = 'green', hist_kws={'edgecolor':'black', 'linewidth':1}, kde_kws={'linewidth': 2})\nplt.xlim(0, 0.3)\nplt.title('Distribution of Maryland population likely infected 28\/80')\nplt.xlabel('Percentage of population likely infected')\nplt.show()","9c62fec0":"plt.figure(figsize=(10,6))\ng=sns.barplot(x='likely_infected_28', y='county',data=md_df.sort_values(['likely_infected_28'], ascending=False).head(20), color=\"lightblue\")\nplt.xlim(0, 0.5)\nplt.xlabel(\"Percentage of population infected\")\nplt.ylabel(\"County\")\nplt.title(\"Likely spread of virus if spread is 28 fold\")","4c697764":"plt.figure(figsize=(10,6))\ng=sns.barplot(x='likely_infected_80', y='county',data=md_df.sort_values(['likely_infected_80'], ascending=False).head(20), color=\"lightblue\")\nplt.xlim(0, 1)\nplt.xlabel(\"Percentage of population infected\")\nplt.ylabel(\"County\")\nplt.title(\"Likely spread of virus if spread is 80 fold\")","3b62659e":"### Geo data","18efca25":"### Heatmap","0df3af34":"### Maryland, DC and Virginia","91f27b59":"### Load latest US data by county","22c1ab52":"###  This notebook was created to \n\n- study the spread and activity of COVID19 virus in DC Metro area\n- comparison of results with other states\/counties\n- use of new visualization techniques\n\n\n### Thanks\n\nInspired by this great kernel: https:\/\/www.kaggle.com\/jmarfati\/actual-spread-of-covid19-us-county-level-analysis\n\nPlotly visualizations: \n\n- https:\/\/www.kaggle.com\/samusram\/covid-19-person-level-drill-down-czechia-canada\n- https:\/\/www.kaggle.com\/artgor\/finland-and-coronavirus"}}