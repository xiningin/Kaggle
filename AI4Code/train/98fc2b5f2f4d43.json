{"cell_type":{"7bcacc00":"code","42340366":"code","bb909baf":"code","73aea890":"code","8ed427b1":"code","09ef6493":"code","095439aa":"code","02606444":"code","8ef83a6a":"code","dd6890e1":"code","85a33d97":"code","2141bd54":"code","c6e2b763":"code","66a04944":"code","f91db74c":"code","2f6689e2":"code","3804c4e0":"code","81cf7219":"code","b2b93914":"code","67e2675e":"code","25a6cb45":"code","d199c3df":"code","269b26aa":"code","5a881950":"code","0d402dc8":"markdown","69c76092":"markdown","8090901a":"markdown","942d8b17":"markdown","7278f73b":"markdown","f28c1c17":"markdown","6601aca7":"markdown","2166a7fa":"markdown","ac135a94":"markdown"},"source":{"7bcacc00":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\ntrain = pd.read_csv(\"\/kaggle\/input\/bike-sharing-demand\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/bike-sharing-demand\/test.csv\")\n\n                   \nbike_df=train.copy()\n#test_df=test.copy()\n\n#bike_df = pd.read_csv('.\/bike_train.csv')\n#print(bike_df.shape)\nbike_df.head(3)\n","42340366":"bike_df.info()","bb909baf":"# Boxplot between count & each categorical features\nfig, axes = plt.subplots(nrows=2,ncols=2)\nfig.set_size_inches(20, 10)\nsns.boxplot(data=train, y=\"count\", x=\"season\", ax=axes[0][0])\nsns.boxplot(data=train, y=\"count\", x=\"holiday\", ax=axes[0][1])\nsns.boxplot(data=train, y=\"count\", x=\"workingday\", ax=axes[1][0])\nsns.boxplot(data=train, y=\"count\", x=\"weather\", ax=axes[1][1])\n\naxes[0][0].set(xlabel='Season', ylabel=\"Count\")\naxes[0][1].set(xlabel='Holiday', ylabel='Count')\naxes[1][0].set(xlabel='Workingday', ylabel='Count')\naxes[1][1].set(xlabel='Weather', ylabel='Count')","73aea890":"# Correlation between each features\nplt.figure(figsize=(10,10))\nsns.heatmap(train.corr(\"pearson\"),\n            vmin=-1, vmax=1,\n            cmap='coolwarm',\n            annot=True, \n            square=True)","8ed427b1":"# \ubb38\uc790\uc5f4\uc744 datetime \ud0c0\uc785\uc73c\ub85c \ubcc0\uacbd. \nbike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)\n\n# datetime \ud0c0\uc785\uc5d0\uc11c \ub144, \uc6d4, \uc77c, \uc2dc\uac04 \ucd94\ucd9c\nbike_df['year'] = bike_df.datetime.apply(lambda x : x.year)\nbike_df['month'] = bike_df.datetime.apply(lambda x : x.month)\nbike_df['day'] = bike_df.datetime.apply(lambda x : x.day)\nbike_df['dayofweek'] = bike_df.datetime.apply(lambda x: x.dayofweek)\nbike_df['hour'] = bike_df.datetime.apply(lambda x: x.hour)\nbike_df.head(3)","09ef6493":"drop_columns = ['datetime','casual','registered']\nbike_df.drop(drop_columns, axis=1,inplace=True)","095439aa":"# Boxplot between count & each categorical features\nfig, axes = plt.subplots(nrows=1,ncols=3)\nfig.set_size_inches(25, 5)\nsns.barplot(data=bike_df, x='year', y=train['count'], ax=axes[0])\nsns.barplot(data=bike_df, x='month', y=train['count'], ax=axes[1])\nsns.pointplot(data=bike_df, x='hour', y=train['count'], ax=axes[2], hue='dayofweek')","02606444":"# Count column looks skew.\nsns.distplot(bike_df['count'])","8ef83a6a":"# Take a log for count column\nbike_df['count'] = np.log1p(bike_df['count'])\nsns.distplot(bike_df['count'])","dd6890e1":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# log \uac12 \ubcc0\ud658 \uc2dc NaN\ub4f1\uc758 \uc774\uc288\ub85c log() \uac00 \uc544\ub2cc log1p() \ub97c \uc774\uc6a9\ud558\uc5ec RMSLE \uacc4\uc0b0\ndef rmsle(y, pred):\n    log_y = np.log1p(y)\n    log_pred = np.log1p(pred)\n    squared_error = (log_y - log_pred) ** 2\n    rmsle = np.sqrt(np.mean(squared_error))\n    return rmsle\n\n# \uc0ac\uc774\ud0b7\ub7f0\uc758 mean_square_error() \ub97c \uc774\uc6a9\ud558\uc5ec RMSE \uacc4\uc0b0\ndef rmse(y,pred):\n    return np.sqrt(mean_squared_error(y,pred))\n\n# MSE, RMSE, RMSLE \ub97c \ubaa8\ub450 \uacc4\uc0b0 \ndef evaluate_regr(y,pred):\n    rmsle_val = rmsle(y,pred)\n    rmse_val = rmse(y,pred)\n    # MAE \ub294 scikit learn\uc758 mean_absolute_error() \ub85c \uacc4\uc0b0\n    mae_val = mean_absolute_error(y,pred)\n    print('RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}'.format(rmsle_val, rmse_val, mae_val))","85a33d97":"from sklearn.model_selection import train_test_split , GridSearchCV\nfrom sklearn.linear_model import LinearRegression , Ridge , Lasso\n\ny_target = bike_df['count']\nX_features = bike_df.drop(['count'],axis=1,inplace=False)\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0)\n\nlr_reg = LinearRegression()\nlr_reg.fit(X_train, y_train)\npred = lr_reg.predict(X_test)\n\nevaluate_regr(y_test ,pred)","2141bd54":"def get_top_error_data(y_test, pred, n_tops = 5):\n    # DataFrame\uc5d0 \uceec\ub7fc\ub4e4\ub85c \uc2e4\uc81c \ub300\uc5ec\ud69f\uc218(count)\uc640 \uc608\uce21 \uac12\uc744 \uc11c\ub85c \ube44\uad50 \ud560 \uc218 \uc788\ub3c4\ub85d \uc0dd\uc131. \n    result_df = pd.DataFrame(y_test.values, columns=['real_count'])\n    result_df['predicted_count']= np.round(pred)\n    result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count'])\n    # \uc608\uce21\uac12\uacfc \uc2e4\uc81c\uac12\uc774 \uac00\uc7a5 \ud070 \ub370\uc774\ud130 \uc21c\uc73c\ub85c \ucd9c\ub825. \n    print(result_df.sort_values('diff', ascending=False)[:n_tops])\n    \nget_top_error_data(y_test,pred,n_tops=5)\n","c6e2b763":"y_target.hist()","66a04944":"y_log_transform = np.log1p(y_target)\ny_log_transform.hist()","f91db74c":"# \ud0c0\uac9f \uceec\ub7fc\uc778 count \uac12\uc744 log1p \ub85c Log \ubcc0\ud658\ny_target_log = np.log1p(y_target)\n\n# \ub85c\uadf8 \ubcc0\ud658\ub41c y_target_log\ub97c \ubc18\uc601\ud558\uc5ec \ud559\uc2b5\/\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc14b \ubd84\ud560\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, test_size=0.3, random_state=0)\nlr_reg = LinearRegression()\nlr_reg.fit(X_train, y_train)\npred = lr_reg.predict(X_test)\n\n# \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc14b\uc758 Target \uac12\uc740 Log \ubcc0\ud658\ub418\uc5c8\uc73c\ubbc0\ub85c \ub2e4\uc2dc expm1\ub97c \uc774\uc6a9\ud558\uc5ec \uc6d0\ub798 scale\ub85c \ubcc0\ud658\ny_test_exp = np.expm1(y_test)\n\n# \uc608\uce21 \uac12 \uc5ed\uc2dc Log \ubcc0\ud658\ub41c \ud0c0\uac9f \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\ub418\uc5b4 \uc608\uce21\ub418\uc5c8\uc73c\ubbc0\ub85c \ub2e4\uc2dc exmpl\uc73c\ub85c scale\ubcc0\ud658\npred_exp = np.expm1(pred)\n\nevaluate_regr(y_test_exp ,pred_exp)\n","2f6689e2":"coef = pd.Series(lr_reg.coef_, index=X_features.columns)\ncoef_sort = coef.sort_values(ascending=False)\nsns.barplot(x=coef_sort.values, y=coef_sort.index)","3804c4e0":"# 'year', month', 'day', hour'\ub4f1\uc758 \ud53c\ucc98\ub4e4\uc744 One Hot Encoding\nX_features_ohe = pd.get_dummies(X_features, columns=['year', 'month','day', 'hour', 'holiday',\n                                              'workingday','season','weather'])\n\n","81cf7219":"# \uc6d0-\ud56b \uc778\ucf54\ub529\uc774 \uc801\uc6a9\ub41c feature \ub370\uc774\ud130 \uc138\ud2b8 \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\/\uc608\uce21 \ub370\uc774\ud130 \ubd84\ud560. \nX_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log,\n                                                    test_size=0.3, random_state=0)\n\n# \ubaa8\ub378\uacfc \ud559\uc2b5\/\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc14b\uc744 \uc785\ub825\ud558\uba74 \uc131\ub2a5 \ud3c9\uac00 \uc218\uce58\ub97c \ubc18\ud658\ndef get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=False):\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    if is_expm1 :\n        y_test = np.expm1(y_test)\n        pred = np.expm1(pred)\n    print('###',model.__class__.__name__,'###')\n    evaluate_regr(y_test, pred)\n# end of function get_model_predict    \n\n# model \ubcc4\ub85c \ud3c9\uac00 \uc218\ud589\nlr_reg = LinearRegression()\nridge_reg = Ridge(alpha=10)\nlasso_reg = Lasso(alpha=0.01)\n\nfor model in [lr_reg, ridge_reg, lasso_reg]:\n    get_model_predict(model,X_train, X_test, y_train, y_test,is_expm1=True)\n","b2b93914":"coef = pd.Series(lr_reg.coef_ , index=X_features_ohe.columns)\ncoef_sort = coef.sort_values(ascending=False)[:20]\nsns.barplot(x=coef_sort.values , y=coef_sort.index)\n","67e2675e":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8, GBM, XGBoost, LightGBM model \ubcc4\ub85c \ud3c9\uac00 \uc218\ud589\nrf_reg = RandomForestRegressor(n_estimators=500)\ngbm_reg = GradientBoostingRegressor(n_estimators=500)\nxgb_reg = XGBRegressor(n_estimators=500)\nlgbm_reg = LGBMRegressor(n_estimators=500)\n\nfor model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]:\n    # XGBoost\uc758 \uacbd\uc6b0 DataFrame\uc774 \uc785\ub825 \ub420 \uacbd\uc6b0 \ubc84\uc804\uc5d0 \ub530\ub77c \uc624\ub958 \ubc1c\uc0dd \uac00\ub2a5. ndarray\ub85c \ubcc0\ud658.\n    get_model_predict(model,X_train.values, X_test.values, y_train.values, y_test.values,is_expm1=True)","25a6cb45":"def evaluate(reg_cls, params=None):\n    reg = reg_cls()\n    if params:\n        reg = GridSearchCV(reg, param_grid=params, refit=True)\n    reg.fit(X_train, y_train)\n    pred = reg.predict(X_test)\n    \n    y_test_exp = np.expm1(y_test)\n    pred_exp = np.expm1(pred)\n    print('\\n', reg_cls)\n    if params:\n        print(reg.best_params_)\n        reg = reg.best_estimator_\n    print(rmsle(y_test_exp, pred_exp))\n    return reg, pred_exp","d199c3df":"# lr_reg, pred_lr = evaluate(LinearRegression)\n# rg_reg, pred_rg = evaluate(Ridge)\n# ls_reg, pred_ls = evaluate(Lasso)\n# rf_reg, pred_rf = evaluate(RandomForestRegressor)\n# gb_reg, pred_gb = evaluate(GradientBoostingRegressor)\n\nparams = {'n_estimators': [100*i for i in range(1, 6)]}\nxg_reg, pred_xg = evaluate(XGBRegressor, params)\nlg_reg, pred_lg = evaluate(LGBMRegressor, params)","269b26aa":"def feature_importances(reg):\n    plt.figure(figsize=(20, 10))\n    print(type(reg))\n    df = pd.DataFrame(sorted(zip(X_train.columns, reg.feature_importances_)), columns=['Feature', 'Value'])\n    sns.barplot(x=\"Value\", y=\"Feature\", data=df.sort_values(by=\"Value\", ascending=False))\n    plt.show()\nfeature_importances(xg_reg)","5a881950":"feature_importances(xg_reg)","0d402dc8":"****\uc2dc\uac01\ud654****","69c76092":"### LinearRegression\uc774 \uac00\uc7a5 \uc6b0\uc218 ###","8090901a":"****Data preprocessing****","942d8b17":"****RMSLE\uac12 \uc88b\uc544\uc84c\uc74c****","7278f73b":"****\ud3c9\uac00\uc9c0\ud45c(rmsle)****","f28c1c17":"##  Regression  - Bike Sharing Demand Practice\n###","6601aca7":"****LinearRegression()\uc774 \uac00\uc7a5 \uc88b\uc74c****","2166a7fa":"***LGBMRegressor\uc774 \uac00\uc7a5 \uc6b0\uc218  ###","ac135a94":"### \ub85c\uadf8 \ubcc0\ud658, \ud53c\ucc98 \uc778\ucf54\ub529, \ubaa8\ub378 \ud559\uc2b5\/\uc608\uce21\/\ud3c9\uac00 "}}