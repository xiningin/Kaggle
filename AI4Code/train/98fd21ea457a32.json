{"cell_type":{"d8a2a130":"code","331204ae":"code","c4deb4df":"code","4cbd087c":"code","09fa0a81":"code","5901b22d":"code","232fa74b":"markdown","5ac9958b":"markdown","d22a6894":"markdown","627e9033":"markdown"},"source":{"d8a2a130":"# oos_scale = 1\/w_12_train.oos_level_12_scale\ndef oos_rmsse(preds, train_data): \n        actuals = train_data.get_label()\n        diff = actuals - preds\n        grad = -diff * oos_scale\n        hess = np.ones_like(diff)\n        return grad, hess\n    \n# The scaling factor, w_12_train.oos_level_12_scale, \n# is the same as described in the competition, except \n# I removed zeros that I believed were due to being \n# out of stock before calculation.","331204ae":"from m5_helpers import * \nDATA_RAW_PATH = '..\/input\/m5-forecasting-accuracy\/'","c4deb4df":"######################### oos comp scale ################################\nDATA_INTERIM_PATH = '..\/input\/m5-oos-grid-df\/'\noos_train_df = pd.read_pickle(f'{DATA_INTERIM_PATH}oos_train_df.pkl')\n\ndef get_oos_scale(oos_train_df):\n    rec = oos_train_df.iloc[:, 6:-28]\n    rdiff = np.diff(rec, axis=1)\n    return np.sqrt(np.nanmean(rdiff**2, axis=1))","4cbd087c":"########################### Load competition data #########################\ncal_df = pd.read_csv(F'{DATA_RAW_PATH}calendar.csv')\nprices_df = pd.read_csv(F'{DATA_RAW_PATH}sell_prices.csv')\ntrain_df = pd.read_csv(F'{DATA_RAW_PATH}sales_train_evaluation.csv')\n\nSTART_TEST = 1942\n\n####################### Make evaluator object #############################\n# This object has a lot that of extras that you can find out about by \n# running WRMSSE?? in a code block. Most importantly, it has a dataframe \n# with all the series with weights and scales. \ne = WRMSSE(train_df, cal_df, prices_df, START_TEST)\n\n############################ Weights and scales alignment #################\n# Get a dataframe with all the weights and scales for all series. \nw_df = e.w_df.copy()\n\n# Initialize a weight dataframe for just the level 12 series. \nw_12 = w_df.loc[12]\n\n# For each item, for each level we want to add the scaled weight W\/sqrt(S) \n# of the series to which that item belongs. \nw_12['level_1_sw'] = w_df.loc[1].scaled_weight[0]\nw_12['level_2_sw'] = w_12.index.map(lambda x: w_df.loc[(2,x.split('_')[3])].scaled_weight)\nw_12['level_3_sw'] = w_12.index.map(lambda x: w_df.loc[(3,x[-4:])].scaled_weight)\nw_12['level_4_sw'] = w_12.index.map(lambda x: w_df.loc[(4,x.split('_')[0])].scaled_weight)\nw_12['level_5_sw'] = w_12.index.map(lambda x: w_df.loc[(5, x.split('_')[0] + '_' +  x.split('_')[1])].scaled_weight)\nw_12['level_6_sw'] = w_12.index.map(lambda x: w_df.loc[(6, x.split('_')[3] + '_' +  x.split('_')[0])].scaled_weight)\nw_12['level_7_sw'] = w_12.index.map(lambda x: w_df.loc[(7, x.split('_')[3] + '_' +  x.split('_')[0] + '_' +  x.split('_')[1])].scaled_weight)\nw_12['level_8_sw'] = w_12.index.map(lambda x: w_df.loc[(8, x.split('_')[3] + '_' +  x.split('_')[4] + '_' +  x.split('_')[0])].scaled_weight)\nw_12['level_9_sw'] = w_12.index.map(lambda x: w_df.loc[(9, x.split('_')[3] + '_' +  x.split('_')[4] + '_' +  x.split('_')[0] + '_' +  x.split('_')[1])].scaled_weight)\nw_12['level_10_sw'] = w_12.index.map(lambda x: w_df.loc[(10, x.split('_')[0] + '_' +  x.split('_')[1] + '_' +  x.split('_')[2])].scaled_weight)\nw_12['level_11_sw'] = w_12.index.map(lambda x: w_df.loc[(11, x.split('_')[0] + '_' +  x.split('_')[1] + '_' +  x.split('_')[2] + '_' +  x.split('_')[3])].scaled_weight)\nw_12['total_sw'] = w_12[['scaled_weight', 'level_1_sw', 'level_2_sw', 'level_3_sw', 'level_4_sw',\n       'level_5_sw', 'level_6_sw', 'level_7_sw', 'level_8_sw', 'level_9_sw',\n       'level_10_sw', 'level_11_sw']].sum(axis=1)\n\n################ Add oos_scale #################################\nw_12.index = w_12.index + '_evaluation'\nw_12 = w_12.reindex(e.train_df.id)\nif START_TEST == 1942:\n    w_12['oos_level_12_scale'] = get_oos_scale(oos_train_df)\ndel w_df\n\n####################### Output files ###########################\nw_12.to_pickle(F'w_12_{START_TEST}.pkl')","09fa0a81":"w_12.head()","5901b22d":"#################### Custom objective ####################\ndef get_wrmsse(w_12_train):\n    \"\"\"w_12_train must be aligned with grid_df like\n    w_12_train = w_12.reindex(grid_df[train_mask].id)\n    \"\"\"\n      I normalise each weight with the average weight of \n      of all items. \n    weight = w_12_train['total_sw'] \/ w_12_train['total_sw'].mean()\n    \n    def wrmsse(preds, train_data): \n        actuals = train_data.get_label()\n        diff = actuals - preds\n        grad = -diff * weight\n        hess = np.ones_like(diff)\n        return grad, hess\n    return wrmsse\n\n################### Custom metric ########################\ndef get_oos_rmsse_metric(w_12_valid): \n    oos_scale = 1\/w_12_valid.oos_level_12_scale\n    \n    def oos_rmsse_metric(preds, train_data): \n        actuals = train_data.get_label()\n        diff = actuals - preds\n        res = np.sum(diff**2 * oos_scale**2)\n        return 'oos_rmsse', res, False\n    return oos_rmsse_metric\n\n################## Fit custom functions ##################\nw_12_train = w_12.reindex(grid_df[train_mask].id)\nw_12_valid = w_12.reindex(grid_df[valid_mask].id)\n\n################## Objective #############################\nwrmsse = get_wrmsse(w_12_train)\n\n######################### Metrics ########################\noos_rmsse_metric = get_oos_rmsse_metric(w_12_valid)","232fa74b":"So essentially, I am using an RMSE objective with some weight for each item. With W being an items level 12 weight, W\/sqrt(scaling factor) never worked well. Using 1\/(scaling factor) as a weight gave reasonable results, but scored worse than tweedie about half the time, and sometimes it lost really bad, so it was looking like tweedie was going to be my objective function. But I had spent tens if not hundreds of hours working on a custom loss function that paid respect to the WRMSSE across all levels, so I decided to give it one more think. I thought,\n### \"if the answer was really simple and elegant, what would it be?\"\nSo no fancy aggregations during training, and no gradients that would tell my predictions to go the wronge direction to compensate for other wrong predictions in service to higher levels. I just wanted a proper weight for each item to add to my RMSE function that would would reflect the items contribution to the full WRMSSE score. \n\n### For a given item how does an error affect the total score?\nWe know that part of the penalty comes from each of the twelve hierarchical levels. From level 12, we know for sure that the penalty will be $\\frac{W_{12}}{\\sqrt{S_{12}}}$ multiplied by our error (I guess its actually more complicated than this because the error is actually the square root of the average of squared errors across time, but for simplicities sake, I continue my thinking like this). For the other levels, it appears to be more complicated, given that an error for this item could balance out another items error, but if we assume all other items are correct, then the error is simply $\\frac{W_{L}}{\\sqrt{S_{L}}}$, where W and S are the weight and scaling factor of the series on level L to which the particular item belongs. Therefore, I decided to try the simple formula \n$$\nW_{item} = \\sum_{L=1}^{12}\\frac{W_{L}}{\\sqrt{S_{L}}}\n$$\n\nNow I take a simple RMSE function and weight each item with these weights. It turns out this loss function performed better than any other loss functions I had, including tweedie, but it did need the right early stopping function. \n\n### Early stopping metric\nI tried various early stopping metrics, including the proper WRMSSE, level 12 RMSSE, level 12 'out of stock ' adjusted RMSSE, and a level 12 WRMSSE with the weights computed as they are in my loss function. Interestingly, the metric I created using the same idea as the loss function consistently performed worse as an early stopping metric, compared to both the oos_RMSSE and the WRMSSE. In the end, using the oos_RMSSE gave the best results as an early stopper. \n\nBelow is code that starts with a dataframe with weights and scales for all series, and yields a dataframe with the proper sum of weights and scales for each level 12 item. ","5ac9958b":"With about 17.5 hours to go, it was 11:30 PM in my timezone I was getting ready to run tests to decide on features I would use in my final model. I was running all my tests with at least two main objective functions: tweedie, and an RMSSE (root mean squared scaled error) function. This was the RMSSE function:","d22a6894":"# Define and fit custom objective and metric: Needs data to run\nHere is a code snippet that shows how to define the custom objective and custom metric using our new weights. \nIt is only for example. The [raw code of my final model](https:\/\/www.kaggle.com\/chrisrichardmiles\/77th-place-custom-loss-custom-metric) demonstrates how I use it in my final training. I will also update this notebook with code demonstrating tests for different custom objectives and metrics. ","627e9033":"# Competition scoring metric\n$$\nWRMSSE = \\sum_{i=1}^{42,800} \\left(W_i \\times \\sqrt{\\frac{\\sum_{j=1}^{28}{({Y_t - \\hat{Y_t}})^2}}{S_i}}\\right)    \n$$\n\n\n$$\nS_i = \\frac{1}{n-1}\\sum_{t=2}^{n}({Y_t - Y_{t-1}})^2\n$$\n\n* W_i is the weight of the ith series \n\nTherefore, the penalty for each series simplifies to \n$$\n\\frac{W}{\\sqrt{S}}\\sum{\\sqrt{({Y_t - \\hat{Y_t}})^2}}\n$$\n"}}