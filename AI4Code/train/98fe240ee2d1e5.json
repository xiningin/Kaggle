{"cell_type":{"5ad02dbb":"code","ab773096":"code","771718e5":"code","36e80a93":"code","429ff6e1":"code","74e6a538":"code","1526fb88":"code","2d4cdc9c":"code","8ccc3be7":"code","0a3c34e1":"code","66d710eb":"code","b65f90cc":"code","54aa0592":"code","9e5d54bb":"code","329e9b2a":"code","c9a57d0b":"code","89b40404":"code","7ab0c253":"code","5a17d65b":"code","047f4493":"code","0b3c8cf1":"code","3a1ca2a4":"code","b6fb3e42":"code","7952fc55":"code","7631ee3f":"code","8531fceb":"code","736e0093":"code","ec833538":"code","30fa4026":"code","4fbd589b":"code","7ff72069":"code","c65ad50d":"code","5c117d14":"code","87f74185":"code","f5c7816a":"code","cdda8240":"code","3d0936ae":"code","5902fffb":"code","12f14611":"code","ae2e745b":"code","29870ddf":"code","a45114a3":"code","a11779dd":"code","e17271a9":"code","df17bf8c":"code","b030c1b7":"code","607178ef":"code","f405d681":"code","7b140e61":"code","26847ede":"code","c43267bf":"markdown","1df4b254":"markdown","5e5eb1ff":"markdown","e20d7b0f":"markdown","aaab549a":"markdown","89bb9832":"markdown"},"source":{"5ad02dbb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab773096":"sad = pd.read_csv('\/kaggle\/input\/emotion\/Emotion(sad).csv')\nhappy= pd.read_csv('\/kaggle\/input\/emotion\/Emotion(happy).csv')\nangry=pd.read_csv('\/kaggle\/input\/emotion\/Emotion(angry).csv')","771718e5":"sad.shape","36e80a93":"happy.shape","429ff6e1":"angry.shape","74e6a538":"sad.head()","1526fb88":"sad = sad.drop_duplicates(subset='content', keep=\"first\")","2d4cdc9c":"happy = happy.drop_duplicates(subset='content', keep=\"first\")","8ccc3be7":"angry = angry.drop_duplicates(subset='content', keep=\"first\")","0a3c34e1":"frames = [sad, happy, angry]\n\ndf = pd.concat(frames)","66d710eb":"df.shape","b65f90cc":"df.info()","54aa0592":"df.head()","9e5d54bb":"df = df.drop_duplicates(subset='content', keep=\"first\")","329e9b2a":"df.shape","c9a57d0b":"df['sentiment'].value_counts()","89b40404":"df['sentiment'].replace({'happy':1,'angry':0,'sad':2},inplace=True)","7ab0c253":"df['sentiment'].value_counts()","5a17d65b":"## cleaning html tag\n\nimport re\ndef clean_html(text):\n    \n    clean = re.compile('<.*?>')\n    return re.sub(clean, '',text)\n    \ndf['content']=df['content'].apply(clean_html)","047f4493":"## converting to lower \n\ndef convert_lower(text):\n    return text.lower()\n\ndf['content']=df['content'].apply(convert_lower)\n\n","0b3c8cf1":"def remove_special(text):\n        x=''\n        for i in text:\n            if i.isalnum():\n                x=x+i\n            else:\n                x=x+' '\n        return x\n\ndf['content']=df['content'].apply(remove_special)\n\n","3a1ca2a4":"import nltk\nfrom nltk.corpus import stopwords\n\ndef remove_stopwords(text):\n    x=[]\n    for i in text.split():\n        \n        if i not in stopwords.words('english'):\n            x.append(i)\n    y=x[:]\n    x.clear()\n    return y\ndf['content']=df['content'].apply(remove_stopwords)","b6fb3e42":"def join_back(list_input):\n    return \" \".join(list_input)\n    \n\ndf['content']=df['content'].apply(join_back)\n\n","7952fc55":"import nltk\n\nfrom nltk.stem.porter import PorterStemmer\nps= PorterStemmer()\ny=[]\n\ndef stem_words(text):\n    for i in text:\n        y.append(ps.stem(i))\n    z=y[:]\n    y.clear()\n    return z\ndf['content']=df['content'].apply(stem_words)\n","7631ee3f":"def joinback2(list_input):\n    return \"\".join(list_input)\n    \n\n\ndf['content']=df['content'].apply(joinback2)","8531fceb":"df","736e0093":"X=df['content']","ec833538":"X.shape","30fa4026":"y=df.iloc[:,-1].values","4fbd589b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.3) ","7ff72069":"X_test.shape","c65ad50d":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(max_features=1500)","5c117d14":"X_train = cv.fit_transform(X_train).toarray()\nX_test=cv.transform(X_test).toarray()","87f74185":"from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nclf1=GaussianNB()\nclf2=MultinomialNB()\nclf3=BernoulliNB()","f5c7816a":"clf1.fit(X_train,y_train)\nclf2.fit(X_train,y_train)\nclf3.fit(X_train,y_train)","cdda8240":"y_pred1=clf1.predict(X_test)\ny_pred2=clf2.predict(X_test)\ny_pred3=clf3.predict(X_test)","3d0936ae":"from sklearn.metrics import accuracy_score\n\nprint(\"Gaussian\",accuracy_score(y_test,y_pred1))\nprint(\"Multinomial\",accuracy_score(y_test,y_pred2))\nprint(\"Bernaulli\",accuracy_score(y_test,y_pred3))","5902fffb":"test=\"i am sad & depressed too\"","12f14611":"test = np.array([test])\ntest = cv.transform(test)","ae2e745b":"clf2.predict(test)","29870ddf":"X1=df['content']\ny=df.iloc[:,-1].values","a45114a3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test = train_test_split(X1,y,test_size=0.3) ","a11779dd":"from sklearn.feature_extraction.text import TfidfVectorizer\ntv = TfidfVectorizer(min_df= 1, max_features=1500, strip_accents='unicode',analyzer='word',ngram_range=(1,3),stop_words='english')","e17271a9":"X_train = tv.fit_transform(X_train).toarray()\nX_test = tv.transform(X_test).toarray()","df17bf8c":"from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nclf1=GaussianNB()\nclf2=MultinomialNB()\nclf3=BernoulliNB()","b030c1b7":"clf1.fit(X_train,y_train)\nclf2.fit(X_train,y_train)\nclf3.fit(X_train,y_train)\ny_pred1=clf1.predict(X_test)\ny_pred2=clf2.predict(X_test)\ny_pred3=clf3.predict(X_test)","607178ef":"print(\"Gaussian\",accuracy_score(y_test,y_pred1))\nprint(\"Multinomial\",accuracy_score(y_test,y_pred2))\nprint(\"Bernaulli\",accuracy_score(y_test,y_pred3))","f405d681":"test=\"just go to jail\"","7b140e61":"test = np.array([angry['content'][0]])\ntest = tv.transform(test)","26847ede":"clf2.predict(test)","c43267bf":"## Replacing categorical values as :\n     `happy ----> 1`\n        \n     `angry ----> 0`\n        \n     `sad   ----> 2`","1df4b254":"# Testing","5e5eb1ff":"# Following basic steps are required for nlp :\n\n## 1.Cleaning html tag\n## 2.Converting to lower \n## 3.Remove special characters\n## 4.Remove stop words\n## 5.Perform stemming\n","e20d7b0f":"# 1.Using CountVectorizer","aaab549a":"#### TfidfVectorizer and CountVectorizer both are the methods of converting text data into vectors.\n\nIn CountVectorizer the number of times a word appears in the document are being counted. disadvantage is it's ignores some words which are present in a less number in the document. And combination of words dosen't have any existance. \n\n##### Let's take an example, \nif a 'not' is present in front of any positive word it's make the word negative. (I am not happy)\n\n\nTfidfVectorizer considers overall document weightage(weights the word counts by a measure of how often they appear in the documents) of a word and morw over, It helps us in dealing with most frequent words. Using it we can penalize them.\n\n\n![image.png](attachment:image.png)\n\n\nsource : https:\/\/www.oreilly.com\/library\/view\/applied-text-analysis\/9781491963036\/ch04.html","89bb9832":"# 2.Usiing TfidfVectorizer"}}