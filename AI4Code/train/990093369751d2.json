{"cell_type":{"7ea5b3fb":"code","ad829b30":"code","7d076dcb":"code","bee9f39e":"code","839c88bb":"code","ca5c168d":"code","4c32072a":"code","0fe3c639":"code","03dc6402":"code","4f4cbabf":"code","912ffa40":"code","8516de9e":"code","c2272c87":"code","c1b945a7":"code","023dadfb":"code","2d3bd5a0":"code","6fc5ca99":"code","bc9b8b89":"code","030148c3":"code","bbb96b96":"code","565bd334":"code","e0f33e7f":"code","d04f6ebd":"code","9196a7f8":"code","1556f0d9":"code","cc264a44":"code","5c0310f0":"code","038ea651":"code","774eaa8f":"code","262e2ed6":"code","c540d8f3":"code","f5101343":"code","8b202cd5":"code","94d5dee0":"code","9381bf2e":"code","296ce6cc":"code","1540c702":"code","cb2e4d0a":"code","d28470ad":"code","f9f5c6be":"code","423aa31d":"code","2d99417d":"code","ca745c05":"code","5c580eb2":"code","a0427dbb":"code","a072369b":"code","6c368833":"code","239c9eba":"code","b4d004e8":"code","dc9cf172":"code","682e54b7":"code","62817ee5":"code","54335761":"code","ac3d509a":"code","29d41a69":"code","f1b8b2f9":"code","0b37955b":"code","72554d12":"code","238800cf":"code","761810df":"code","a18b6301":"code","b01c4358":"code","c28ab1e1":"code","18b40d71":"code","908a3447":"markdown","8f1e1294":"markdown","40da9c40":"markdown","dc1d6b2a":"markdown","e89460d2":"markdown","b2ebfbe8":"markdown","3f8850fb":"markdown","db738ec8":"markdown","6a1862b6":"markdown","0445c0ef":"markdown"},"source":{"7ea5b3fb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\ntqdm.pandas()\n\ntrain_df = pd.read_csv('\/kaggle\/input\/amazon-pet-product-reviews-classification\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/amazon-pet-product-reviews-classification\/test.csv')\nval_df = pd.read_csv('\/kaggle\/input\/amazon-pet-product-reviews-classification\/valid.csv')\n\nunlabeled = pd.read_csv('\/kaggle\/input\/amazon-pet-product-reviews-classification\/unlabeled.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/amazon-pet-product-reviews-classification\/sample_submission.csv')","ad829b30":"train_df.shape, test_df.shape, val_df.shape, unlabeled.shape","7d076dcb":"train_df['label'].value_counts()","bee9f39e":"val_df['label'].value_counts()","839c88bb":"sample_submission.head()","ca5c168d":"labels_to_ids = {}\nids_to_labels = {}\nfor i, label in enumerate(sorted(train_df['label'].unique())):\n    labels_to_ids[label] = i\n    ids_to_labels[i] = label\n    \nlabels_to_ids, ids_to_labels","4c32072a":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.linear_model import LogisticRegression\n\nimport re","0fe3c639":"def generate_features(df_, return_vocab=False):\n    \n    df = df_.copy()\n    stops = stopwords.words('english')\n    stemmer = PorterStemmer()\n    \n    # removing special characters\n    df['prepared_text'] = df['text'].progress_apply(lambda text: re.sub('[^A-Za-z]', ' ', text))\n    # transform text to lowercase\n    df['prepared_text'] = df['prepared_text'].str.lower()\n    # tokenize the texts\n    df['prepared_text'] = df['prepared_text'].progress_apply(lambda text: word_tokenize(text))\n    # removing stopwords\n    df['prepared_text'] = df['prepared_text'].progress_apply(lambda words: [word for word in words if word not in stops])\n    # stemming\n    df['prepared_text'] = df['prepared_text'].progress_apply(lambda words: [stemmer.stem(word) for word in words])\n    \n    # join prepared_+text to use as corpus\n    df['joined_prepared_text'] = df['prepared_text'].progress_apply(lambda words: \" \".join(words))\n    \n    if (return_vocab):\n        vocabulary = set(np.concatenate(train_df['prepared_text'].values))\n        print(f\"There are {len(vocabulary)} words in vocabulary\")\n        \n        return df, vocabulary\n    \n    return df","03dc6402":"train = generate_features(train_df)","4f4cbabf":"val = generate_features(val_df)","912ffa40":"test = generate_features(test_df)","8516de9e":"corpus = train['joined_prepared_text'].values\ncorpus[:2]","c2272c87":"val_corpus = val['joined_prepared_text'].values\nval_corpus[:2]","c1b945a7":"test_corpus = test['joined_prepared_text'].values\ntest_corpus[:2]","023dadfb":"vectorizer = TfidfVectorizer(max_features=10000)\nX = vectorizer.fit_transform(corpus)\nX.shape","2d3bd5a0":"y = train_df['label'].map(labels_to_ids).values","6fc5ca99":"clf = LogisticRegression(random_state=42, max_iter=200).fit(X, y)","bc9b8b89":"y_pred = clf.predict(X)","030148c3":"labels = list(ids_to_labels.values())\n\nlabels","bbb96b96":"print(classification_report(y, y_pred, target_names=labels))","565bd334":"X_val = vectorizer.transform(val_corpus)\nX_val.shape","e0f33e7f":"y_true = val['label'].map(labels_to_ids).values\ny_pred = clf.predict(X_val)","d04f6ebd":"print(classification_report(y_true, y_pred, target_names=labels))","9196a7f8":"test.head()","1556f0d9":"X_test = vectorizer.transform(test_corpus)\nX_test.shape","cc264a44":"y_pred = clf.predict(X_test)","5c0310f0":"sample_submission['label'] = y_pred","038ea651":"sample_submission['label'] = sample_submission['label'].map(ids_to_labels)","774eaa8f":"sample_submission['label'].value_counts()","262e2ed6":"sample_submission.to_csv('baseline_submission.csv', index=None, header=True)","c540d8f3":"from transformers import (AutoModelForSequenceClassification, AdamW, \n                          Trainer, TrainingArguments, PreTrainedTokenizerFast,\n                          EarlyStoppingCallback, AutoTokenizer)\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom sklearn.metrics import classification_report","f5101343":"train_df = train_df.sample(1000, random_state=42)","8b202cd5":"val_df = val_df.sample(500, random_state=42)","94d5dee0":"train_df['label'].value_counts()","9381bf2e":"val_df['label'].value_counts()","296ce6cc":"X_train = train_df['text'].tolist()\nX_val = val_df['text'].tolist()","1540c702":"tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')","cb2e4d0a":"train_tokenized_batch = tokenizer(X_train, truncation=True, max_length=256, padding=True, return_tensors='pt')\nval_tokenized_batch = tokenizer(X_val, truncation=True, max_length=256, padding=True, return_tensors='pt')","d28470ad":"train_tokenized_batch['labels'] = train_df['label'].map(labels_to_ids).tolist()\nval_tokenized_batch['labels'] = val_df['label'].map(labels_to_ids).tolist()","f9f5c6be":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.tokenized_batch = data\n\n    def __getitem__(self, idx):   \n        return {\n            'input_ids': self.tokenized_batch['input_ids'][idx],\n            'attention_mask': self.tokenized_batch['attention_mask'][idx],\n            'labels': self.tokenized_batch['labels'][idx]\n        }\n\n    def __len__(self):\n        return len(self.tokenized_batch['input_ids'])","423aa31d":"train_dataset = Dataset(train_tokenized_batch)\nval_dataset = Dataset(val_tokenized_batch)","2d99417d":"len(train_dataset), len(val_dataset)","ca745c05":"dl_train = DataLoader(train_dataset, batch_size=8, shuffle=False)\ndl_val = DataLoader(val_dataset, batch_size=16, shuffle=False)","5c580eb2":"model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=len(ids_to_labels.values()))\n_ = model.cuda()","a0427dbb":"optimizer = AdamW(model.parameters(), lr=5e-6)","a072369b":"epochs = 5\nfor _ in tqdm(range(epochs), desc=\"Epoch\"):\n    tr_loss, nb_tr_steps = 0, 0\n    \n    for batch in dl_train:\n        output = model(input_ids=batch['input_ids'].cuda(), attention_mask=batch['attention_mask'].cuda(), labels=batch['labels'].cuda())\n\n        loss = output.loss\n        loss.backward()\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        \n        nb_tr_steps += 1\n        tr_loss += loss.item()\n        \n    print(f\"Train Loss: {tr_loss \/ nb_tr_steps}\")\n    \n    eval_loss, nb_eval_steps = 0, 0\n    for batch in dl_val:\n        with torch.no_grad():\n            output = model(input_ids=batch['input_ids'].cuda(), attention_mask=batch['attention_mask'].cuda(), labels=batch['labels'].cuda())\n            \n        loss = output.loss\n        \n        eval_loss += loss.item()\n        nb_eval_steps += 1\n        \n    print(f\"Eval loss: {eval_loss \/ nb_eval_steps}\")","6c368833":"training_args = TrainingArguments(\n    output_dir='.\/results',          # output directory\n    num_train_epochs=20,              # total # of training epochs\n    per_device_train_batch_size=8,  # batch size per device during training\n    per_device_eval_batch_size=16,   # batch size for evaluation\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    prediction_loss_only=True,\n    logging_dir='.\/logs',            # directory for storing logs\n    seed=42,\n    fp16=True,\n    save_total_limit=1,\n    gradient_accumulation_steps=2,\n    load_best_model_at_end=True,\n    learning_rate=5e-6, \n)","239c9eba":"trainer = Trainer(\n    model=model,                         # the instantiated \ud83e\udd17 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,            # evaluation dataset\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n)","b4d004e8":"trainer.train()","dc9cf172":"best_model = trainer.model","682e54b7":"dl_val = DataLoader(val_dataset, batch_size=64, shuffle=False)","62817ee5":"predictions = []\ntrue_labels = []\nfor batch in tqdm(dl_val):\n    \n    with torch.no_grad():\n        output = best_model(input_ids=batch['input_ids'].cuda(), attention_mask=batch['attention_mask'].cuda())\n    \n    logits = output.logits  \n    val_batch_preds = torch.argmax(output.logits, axis=1).cpu().numpy()\n    predictions.extend(val_batch_preds)\n    true_labels.extend(batch['labels'])","54335761":"print(classification_report(true_labels, predictions, target_names=labels))","ac3d509a":"trainer.evaluate()","29d41a69":"X_test = test_df['text'].tolist()","f1b8b2f9":"test_tokenized_batch = tokenizer(X_test, return_token_type_ids=False, truncation=True, max_length=512, padding=True, return_tensors='pt')","0b37955b":"class PredictDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.tokenized_batch = data\n\n    def __getitem__(self, idx):   \n        return {\n            'input_ids': self.tokenized_batch['input_ids'][idx],\n            'attention_mask': self.tokenized_batch['attention_mask'][idx],\n        }\n\n    def __len__(self):\n        return len(self.tokenized_batch['input_ids'])","72554d12":"predict_dataset = PredictDataset(test_tokenized_batch)","238800cf":"dl = DataLoader(predict_dataset, batch_size=64, shuffle=False)","761810df":"predictions = []\nfor batch in tqdm(dl):\n    \n    with torch.no_grad():\n        output = best_model(input_ids=batch['input_ids'].cuda(), attention_mask=batch['attention_mask'].cuda())\n    \n    logits = output.logits  \n    val_batch_preds = torch.argmax(output.logits, axis=1).cpu().numpy()\n    predictions.extend(val_batch_preds)","a18b6301":"sample_submission['label'] = predictions","b01c4358":"sample_submission['label'] = sample_submission['label'].map(ids_to_labels)","c28ab1e1":"sample_submission['label'].value_counts()","18b40d71":"sample_submission.to_csv('roberta_submission.csv', index=None, header=True)","908a3447":"Check validation metrics","8f1e1294":"With this model we achieve 0.84324 micro f1 score on test_set! :)\n\nLets see if RoBERTa model help us to give much better results","40da9c40":"Check validation metrics","dc1d6b2a":"Prepare submission ","e89460d2":"Prepare submission","b2ebfbe8":"# RoBERTa Model","3f8850fb":"Create dicts to help us map labels to ids and vice versa.","db738ec8":"# Bag of Words Model","6a1862b6":"Sample Submission","0445c0ef":"Check training metrics"}}