{"cell_type":{"d6898c97":"code","f7204c43":"code","4c1526b9":"code","7a0dabea":"code","60b993fc":"code","ad3cdea8":"code","50dce2f2":"code","f1c997da":"code","65727904":"code","c80cc518":"code","f0dfb0ad":"code","74db1f15":"code","ab7d53ea":"code","4cb8c5cd":"code","39a1b39a":"code","9d3c7248":"code","9efd60c5":"code","6b18a4c2":"code","8678b69d":"code","1945a3b0":"code","33ed94e3":"code","6f709f5f":"code","8f809397":"code","4e172f26":"code","366c205f":"code","3c881f5a":"code","bed4c965":"code","bf834527":"code","d4a61719":"code","e31f5880":"code","518a0887":"code","ce53c3af":"code","81c4f52e":"code","0147320c":"code","1dcb1a79":"code","d1431fb7":"code","bf5277e2":"code","9ace7ee5":"markdown","50b1b8fd":"markdown","942944ae":"markdown","03a64485":"markdown","0848ca0f":"markdown","d3162c5d":"markdown","708228d2":"markdown","e01e34ad":"markdown","8788c7f9":"markdown","2e5c4c72":"markdown","d1c0b4a8":"markdown","ddc09cac":"markdown","64ab75cf":"markdown"},"source":{"d6898c97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import decomposition\nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv(\"..\/input\/data.csv\", index_col = 'id')\ndf.drop('Unnamed: 32',axis = 1 ,inplace = True)\ndf['diagnosis'] = df['diagnosis'].map({'M': 1, 'B':0})\nX = df.drop('diagnosis',axis = 1)","f7204c43":"from sklearn.preprocessing import StandardScaler\nX_scaled = StandardScaler().fit_transform(X)\n\npca = decomposition.PCA(n_components=2)\nX_pca_scaled = pca.fit_transform(X_scaled)\n\nprint('Projecting %d-dimensional data to 2D' % X_scaled.shape[1])\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_pca_scaled[:, 0], X_pca_scaled[:, 1], c=df['diagnosis'], alpha=0.7, s=40);\nplt.colorbar()\nplt.title('MNIST. PCA projection');","4c1526b9":"# Invoke the TSNE method\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=2000,random_state = 17)\n\ndf_tsne_scaled = tsne.fit_transform(X_scaled)\n\nplt.figure(figsize=(12,10))\nplt.scatter(df_tsne_scaled[:, 0], df_tsne_scaled[:, 1], c=df['diagnosis'], \n            alpha=0.7, s=40)\nplt.colorbar()\nplt.title('MNIST. t-SNE projection');","7a0dabea":"pca = decomposition.PCA().fit(X_scaled)\n\nplt.figure(figsize=(10,7))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\nplt.xlim(0, 29)\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.axvline(6, c='b')\nplt.axhline(0.91, c='r')\nplt.show();","60b993fc":"perimeters = [x for x in df.columns if 'perimeter' in x]\nareas = [x for x in df.columns if 'area' in x]\ndf.drop(perimeters, axis = 1 ,inplace = True)\ndf.drop(areas, axis = 1 ,inplace = True)\nworst = [col for col in df.columns if col.endswith('_worst')]\ndf.drop(worst, axis = 1 ,inplace = True)","ad3cdea8":"X = df.drop(['diagnosis'], axis=1)\n(X+0.001).hist(figsize=(20, 15), color = 'c');","50dce2f2":"#Log transformation\nX = df.drop(['diagnosis'], axis=1)\nX_log = np.log(X+0.001)\nX_log.hist(figsize=(20, 15), color = 'c');","f1c997da":"from sklearn.model_selection import train_test_split\n\n#Scaler should be trained on train set only to prevent information about future from leaking.\n\ny = df['diagnosis']\n\nX_log_train, X_log_holdout, y_train, y_holdout = train_test_split(X_log, y, test_size=0.3, random_state=17)","65727904":"from sklearn.model_selection import GridSearchCV\n\nX_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3, random_state=17)\n\ntree = DecisionTreeClassifier(random_state=17)\n\ntree_params = {'max_depth': range(1,5), 'max_features': range(3,6), 'criterion': ['gini','entropy']}\n\ntree_grid = GridSearchCV(tree, tree_params, cv=10, scoring='recall')\ntree_grid.fit(X_train, y_train)","c80cc518":"tree_grid.best_params_, tree_grid.best_score_","f0dfb0ad":"from sklearn.metrics import accuracy_score, recall_score, precision_score\n\ntree_pred = tree_grid.predict(X_holdout)\n\nprint (\"Accuracy Score : \",accuracy_score(y_holdout, tree_pred) )\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, tree_pred))\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, tree_pred))","74db1f15":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_holdout, tree_pred)","ab7d53ea":"from sklearn.tree import export_graphviz\ntree_graph = export_graphviz(tree_grid.best_estimator_, class_names = ['benign', 'malignant'], feature_names = df.drop(['diagnosis'], axis=1).columns, filled=True, out_file='tree.dot')\n!dot -Tpng tree.dot -o tree.png ","4cb8c5cd":"from IPython.display import Image\nImage(filename = 'tree.png')","39a1b39a":"numerical = df.drop('diagnosis',axis=1).columns\n\ndf.groupby(['diagnosis'])[numerical].agg([np.mean, np.std, np.min, np.max])","9d3c7248":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nCs = np.logspace(-1, 8, 5)\n\nlr_pipe = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(random_state=17,solver='liblinear'))])\n\nlr_params = {'lr__C': Cs}\n\nlr_pipe_grid = GridSearchCV(lr_pipe, lr_params, cv=10, scoring='recall')\nlr_pipe_grid.fit(X_log_train, y_train)","9efd60c5":"lr_pipe_grid.best_params_, lr_pipe_grid.best_score_","6b18a4c2":"scores=[]\nfor C in Cs:\n    pipe = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(C=C, random_state=17,solver='liblinear'))])\n    scores.append(cross_val_score(pipe,X_log_train, y_train,cv=10, scoring='recall').mean())","8678b69d":"score_C_1 = lr_pipe_grid.best_score_\nsns.set()\nplt.figure(figsize=(10,8))\nplt.plot(Cs, scores, 'ro-')\nplt.xscale('log')\nplt.xlabel('C')\nplt.ylabel('Recall')\nplt.title('Regularization Parameter Tuning')\n# horizontal line -- model quality with default C value\nplt.axhline(y=score_C_1, linewidth=.5, color='b', linestyle='dashed') \nplt.show()","1945a3b0":"print (\"Accuracy Score : \",accuracy_score(y_holdout, lr_pipe_grid.predict(X_log_holdout)) )\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, lr_pipe_grid.predict(X_log_holdout)))\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, lr_pipe_grid.predict(X_log_holdout)))","33ed94e3":"lr_best_pipe = lr_pipe_grid.best_estimator_.named_steps['lr']\n\n#Create Data frame of Regression coefficients\ncoef= pd.DataFrame(lr_best_pipe.coef_.ravel())\n#Merge Regression coefficients with feature names\ndf_columns = pd.DataFrame(df.drop(['diagnosis'], axis=1).columns)\ncoef_and_feat = pd.merge(coef,df_columns,left_index= True,right_index= True, how = \"left\")\ncoef_and_feat.columns = [\"coefficients\",\"features\"]\ncoef_and_feat = coef_and_feat.sort_values(by = \"coefficients\",ascending = False)\n\n#Set up the matplotlib figure\nplt.rcParams['figure.figsize'] = (10,8)\n# Let's draw top 10 important features \nsns.barplot(x = 'features', y = 'coefficients', data = coef_and_feat).set_title('Feature importance')\nplt.xticks(rotation=45);","6f709f5f":"C_scores = np.logspace(-1, 8, 5)\n\nlr = LogisticRegression(random_state=17,solver='liblinear')\n\nlr_params = {'C': C_scores}\n\nlr_grid = GridSearchCV(lr, lr_params, cv=10, scoring='recall')\nlr_grid.fit(X_train, y_train)","8f809397":"lr_grid.best_params_, lr_grid.best_score_","4e172f26":"print (\"Accuracy Score : \",accuracy_score(y_holdout, lr_grid.predict(X_log_holdout)) )\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, lr_grid.predict(X_log_holdout)))\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, lr_grid.predict(X_log_holdout)))","366c205f":"lr_best= lr_grid.best_estimator_\n\n#Create Data frame of Regression coefficients\ncoef= pd.DataFrame(lr_best.coef_.ravel())\n#Merge Regression coefficients with feature names\ndf_columns = pd.DataFrame(df.drop(['diagnosis'], axis=1).columns)\ncoef_and_feat = pd.merge(coef,df_columns,left_index= True,right_index= True, how = \"left\")\ncoef_and_feat.columns = [\"coefficients\",\"features\"]\ncoef_and_feat = coef_and_feat.sort_values(by = \"coefficients\",ascending = False)\n\n#Set up the matplotlib figure\nplt.rcParams['figure.figsize'] = (10,8)\n# Let's draw top 10 important features \nsns.barplot(x = 'features', y = 'coefficients', data = coef_and_feat).set_title('Feature importance')\nplt.xticks(rotation=90);","3c881f5a":"from sklearn.ensemble import RandomForestClassifier\n\n#Stratified split for the validation process\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=17)\n\n#initialize the set of parameters for exhaustive search and fit to find out the optimal parameters\nrfc_params = {'max_features': range(1,11), 'min_samples_leaf': range(1,3), 'max_depth': range(3,13), 'criterion':['gini','entropy']}\n\nrfc = RandomForestClassifier(n_estimators=100, random_state=17, n_jobs= -1)\n\ngcv = GridSearchCV(rfc, rfc_params, n_jobs=-1, cv=skf, scoring='recall')\n\ngcv.fit(X_train, y_train)","bed4c965":"gcv.best_params_, gcv.best_score_","bf834527":"#RandomForest classifier with the default parameters \nrfc = RandomForestClassifier(n_estimators=100, criterion ='gini', max_depth = 8, max_features = 6, min_samples_leaf = 1, random_state = 17, n_jobs=-1)\nforest_pred = gcv.predict(X_holdout)\n\nprint (\"Accuracy Score : \",accuracy_score(y_holdout, forest_pred) )\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, forest_pred))\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, forest_pred))","d4a61719":"rfc = gcv.best_estimator_\nestimators_tree_98 = rfc.estimators_[98]\n\nestimators_tree_3 = rfc.estimators_[3]\n\nestimators_tree_47 = rfc.estimators_[47]","e31f5880":"estimators_tree_3.n_features_","518a0887":"tree_graph_98 = export_graphviz(estimators_tree_98, class_names = ['benign', 'malignant'], feature_names = df.drop(['diagnosis'], axis=1).columns, filled=True, out_file='tree_98.dot')\n!dot -Tpng tree_98.dot -o tree_98.png \n\ntree_graph_3 = export_graphviz(estimators_tree_3, class_names = ['benign', 'malignant'], feature_names = df.drop(['diagnosis'], axis=1).columns, filled=True, out_file='tree_3.dot')\n!dot -Tpng tree_3.dot -o tree_3.png \n\ntree_graph_47 = export_graphviz(estimators_tree_47, class_names = ['benign', 'malignant'], feature_names = df.drop(['diagnosis'], axis=1).columns, filled=True, out_file='tree_47.dot')\n!dot -Tpng tree_47.dot -o tree_47.png ","ce53c3af":"Image(filename = 'tree_98.png')\n","81c4f52e":"\nImage(filename = 'tree_3.png')\n\n","0147320c":"Image(filename = 'tree_47.png')","1dcb1a79":"rf_pipe = Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier(n_estimators=100, random_state=17, n_jobs= -1))])\n\nrf_params = {'rf__max_features': range(3,10), 'rf__min_samples_leaf': range(1,3), 'rf__max_depth': range(5,12), 'rf__criterion':['gini','entropy']}\n\n\nrf_pipe_grid = GridSearchCV(rf_pipe, rf_params, cv=10, scoring='recall')\nrf_pipe_grid.fit(X_log_train, y_train)","d1431fb7":"rf_pipe_grid.best_params_, rf_pipe_grid.best_score_","bf5277e2":"print (\"Accuracy Score on scaled data: \",accuracy_score(y_holdout, rf_pipe_grid.predict(X_log_holdout)) )\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, rf_pipe_grid.predict(X_log_holdout)))\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, rf_pipe_grid.predict(X_log_holdout)))","9ace7ee5":"As it can be seen, that the first 6 components correspond to approximately 91% of the cumulative sum over all the variance. I am going to stick with PCA, since it provides similar results to t-SNE and takes less time to compute the components. And later, I will compare classification performance for the initial dataset and for pca components.\n\n## Data cleaning\n\nAs the dataset is not large, I am not going to remove any outliers in order to keep as much data as possible. But to avoid multicollinearity, I will remove some of the features to prevent overfitting. From the EDA we now know that `radius`, `perimeter`, and `area` are highly correlated, which makes sense. That is why it would be better to remove, say, `perimeter`, and `area`, as well as all features from \"worst\" samples, since worst (or largest) instances are also considered in the initial sample, which means and standart errors were computed for, therefore it leads to high correlation (>0.80), which is not surprising, too. For example, the correlation between `radius_worst` and `radius_mean` is 0.97, for `texture_mean` and `texture_worst` pair it equals to 0.91, and so on and so forth. ","50b1b8fd":"## t-SNE\n\nt-Distributed stochastic neighbor embedding (t-SNE) minimizes the divergence between two distributions: *a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding*.\n\nWith t-SNE, the picture looks better since PCA has a linear constraint, while t-SNE uses a non-linear approach in the background. However, even with such a small dataset, the t-SNE algorithm takes significantly more time to complete than PCA.","942944ae":"Accuracy Score :  0.9532, Recall is 0.9016, and precision is 0.9649","03a64485":"Recall equals to 0.9672, Precision Score is 0.8310, and Accuracy Score = 0.9181","0848ca0f":"## Normalization","d3162c5d":"Now let's fit RF on scaled data:","708228d2":"#### with non-normalized data","e01e34ad":"### Logistic Regression\n\n#### with normalized data","8788c7f9":"Hello Kagglers! This work is part of my ongoing project in Predictive Analytics to classify Breast Cancer tumors: whether it's Malignant or Benign.The first part, which is the Explanatory Data Analysis and data visualization, was done [here](https:\/\/www.kaggle.com\/sulianova\/eda-breast-cancer).\n\nContent:\n\n1. [PCA](#PCA)\n2. [t-SNE](#t-SNE)\n3. [Data cleaning](#Data-cleaning)\n4. [Normalization](#Normalization)\n5. [Decision tree](#Decision-tree)\n6. [Logistic Regression](#Logistic-Regression)\n    \n    a) [with normalized data](#with-normalized-data)\n    \n    b) [with non-normalized data](#with-non-normalized-data)\n    \n7. [Random Forest](#Random-Forest)\n\n\n## Principal Component Analysis \n\nThe data has 30 dimensions, but we are going to reduce it to 2 and to see whether variables are separated into clusters. It is better to scale the data to avoid a differencing in the values and to normalize the ranges.","2e5c4c72":"### Random Forest","d1c0b4a8":"The Standard Scaler is one of the most widely used scaling algorithms. It assumes that the data follows a Gaussian distribution. Before scaling numerical features, I have to check whether they follow normal distribution:","ddc09cac":"### Decision tree\n\nLet's find out what are the best parameters for the Decision tree with `GridSearch`:","64ab75cf":"In practice, we would choose the number of principal components such that we can explain 90% of the initial data dispersion (via the `explained_variance_ratio`). Here, that means retaining 6 principal components; therefore, we reduce the dimensionality from 30 features to 6."}}