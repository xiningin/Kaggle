{"cell_type":{"463355b9":"code","4132d8b3":"code","bca8122a":"code","24756495":"code","63f7a205":"code","20f98b0e":"code","42b5d842":"code","8e78cb85":"code","c77de472":"code","3bfa1839":"code","919aa0b7":"code","3cba442b":"code","773f8220":"code","36bf9850":"code","a828b9a3":"code","d64f4763":"code","d8f72d3f":"code","af13cff6":"code","a76a0224":"code","a580cc63":"code","246dc244":"code","c9e70e3f":"code","6653cd3e":"code","723c489a":"code","d28dbd2e":"code","81ba479b":"markdown","7468522e":"markdown","2e257703":"markdown","4e440986":"markdown","02425943":"markdown","9cec9355":"markdown","116fca01":"markdown","ad70e128":"markdown","58f7604b":"markdown","81aaa085":"markdown","83c2742f":"markdown","137b65a1":"markdown","5cbee844":"markdown","6ca1006f":"markdown","c8b55d5b":"markdown","03463d27":"markdown","033bb97e":"markdown","fb34e7fa":"markdown"},"source":{"463355b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4132d8b3":"# Taken from this kernel: https:\/\/www.kaggle.com\/vipoooool\/plant-diseases-classification-using-alexnet\n# Docs: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\n\nfrom tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Define a data generator with data augmentation transformations for the \n# training dataset.\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   fill_mode='nearest')\n\nvalid_datagen = ImageDataGenerator(rescale=1.\/255)\n\nbatch_size = 32\nbase_dir = \"..\/input\/new-plant-diseases-dataset\/new plant diseases dataset(augmented)\/New Plant Diseases Dataset(Augmented)\"\n\ntrain_set = train_datagen.flow_from_directory(base_dir + '\/train',\n                                              target_size=(224, 224),\n                                              batch_size=batch_size,\n                                              class_mode='categorical')\n\nvalid_set = valid_datagen.flow_from_directory(base_dir + '\/valid',\n                                              target_size=(224, 224),\n                                              batch_size=batch_size,\n                                              class_mode='categorical')","bca8122a":"base_model = keras.applications.VGG16(\n    weights=\"imagenet\",  # load weights pretrained on the ImageNet\n    input_shape=(224, 224, 3),\n    include_top=False  # do not include the ImageNet classifier at the top\n)  ","24756495":"base_model.summary()","63f7a205":"# Freeze the base_model\nbase_model.trainable = False\n\n# Create new model on top\ninputs = keras.Input(shape=(224, 224, 3))\n\n# The base model contains batchnorm layers. We want to keep them in inference mode\n# when we unfreeze the base model for fine-tuning, so we make sure that the\n# base_model is running in inference mode here.\nx = base_model(inputs, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\noutputs = keras.layers.Dense(38)(x)\n\nvgg16_model = keras.Model(inputs, outputs, name='pretrained_vgg16')\nvgg16_model.summary()","20f98b0e":"vgg16_model.compile(optimizer=keras.optimizers.Adam(),\n              loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=[keras.metrics.CategoricalAccuracy()]\n)\n\nepochs = 10\n\n# train_num = train_set.samples  # num of training samples\n# valid_num = valid_set.samples  # num of validation samples\n\nvgg16_history = vgg16_model.fit(train_set,\n                                steps_per_epoch=150,  # use 150 random batches (= 4800 samples) for training\n                                validation_data=valid_set,\n                                epochs=epochs,\n                                validation_steps=100,  # use 100 random batches (= 3200 samples) for validation \n)","42b5d842":"results = vgg16_model.evaluate(valid_set)\nprint('val loss:', results[0])\nprint('val acc:', results[1])","8e78cb85":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set parameters for plotting\nplt.rc('figure', figsize=(8, 4))\nsns.set(font_scale=1)","c77de472":"train_acc = vgg16_history.history['categorical_accuracy']\nval_acc = vgg16_history.history['val_categorical_accuracy']\n\nepochs_list = list(range(1, epochs + 1))\n\nplt.plot(epochs_list, train_acc, label='train acc')\nplt.plot(epochs_list, val_acc, label='val acc')\nplt.title(\"VGG-16's Accuracy\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='best')","3bfa1839":"train_loss = vgg16_history.history['loss']\nval_loss = vgg16_history.history['val_loss']\n\nplt.plot(epochs_list, train_loss, label='train loss')\nplt.plot(epochs_list, val_loss, label='val loss')\nplt.title(\"VGG-16's Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc='best')","919aa0b7":"vgg16_model.save('vgg16')","3cba442b":"base_model = keras.applications.MobileNet(\n    weights=\"imagenet\",  # load weights pretrained on the ImageNet\n    input_shape=(224, 224, 3),\n    include_top=False  # do not include the ImageNet classifier at the top\n)  ","773f8220":"base_model.summary()","36bf9850":"# Freeze the base_model\nbase_model.trainable = False\n\n# Create new model on top\ninputs = keras.Input(shape=(224, 224, 3))\n\n# The base model contains batchnorm layers. We want to keep them in inference mode\n# when we unfreeze the base model for fine-tuning, so we make sure that the\n# base_model is running in inference mode here.\nx = base_model(inputs, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\noutputs = keras.layers.Dense(38)(x)\n\nmobilenet_model = keras.Model(inputs, outputs, name='pretrained_mobilenet')\nmobilenet_model.summary()","a828b9a3":"mobilenet_model.compile(optimizer=keras.optimizers.Adam(),\n                        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n                        metrics=[keras.metrics.CategoricalAccuracy()]\n)\n\nepochs = 10\n\nmobilenet_history = mobilenet_model.fit(train_set,\n                                        steps_per_epoch=150,  # use 150 random batches (= 4800 samples) for training\n                                        validation_data=valid_set,\n                                        epochs=epochs,\n                                        validation_steps=100,  # use 100 random batches (= 3200 samples) for validation \n)","d64f4763":"results = mobilenet_model.evaluate(valid_set)\nprint('val loss:', results[0])\nprint('val acc:', results[1])","d8f72d3f":"train_acc = mobilenet_history.history['categorical_accuracy']\nval_acc = mobilenet_history.history['val_categorical_accuracy']\n\nepochs_list = list(range(1, epochs + 1))\n\nplt.plot(epochs_list, train_acc, label='train acc')\nplt.plot(epochs_list, val_acc, label='val acc')\nplt.title(\"MobileNet's Accuracy\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='best')","af13cff6":"train_loss = mobilenet_history.history['loss']\nval_loss = mobilenet_history.history['val_loss']\n\nplt.plot(epochs_list, train_loss, label='train loss')\nplt.plot(epochs_list, val_loss, label='val loss')\nplt.title(\"MobileNet's Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc='best')","a76a0224":"mobilenet_model.save('mobilenet')","a580cc63":"# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training=False` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model.trainable = True\nmobilenet_model.summary()","246dc244":"mobilenet_model.compile(optimizer=keras.optimizers.Adam(1e-5),  # set a small learning rate\n                        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n                        metrics=[keras.metrics.CategoricalAccuracy()]\n)\n\nepochs = 5\n\nmobilenet_ft_history = mobilenet_model.fit(train_set,\n                                           steps_per_epoch=150,   # use 150 random batches (= 4800 samples) for training\n                                           validation_data=valid_set,\n                                           epochs=epochs,\n                                           validation_steps=100  # use 100 random batches (= 3200 samples) for validation\n)","c9e70e3f":"results = mobilenet_model.evaluate(valid_set)\nprint('val loss:', results[0])\nprint('val acc:', results[1])","6653cd3e":"train_acc = mobilenet_ft_history.history['categorical_accuracy']\nval_acc = mobilenet_ft_history.history['val_categorical_accuracy']\n\nepochs_list = list(range(1, epochs + 1))\n\nplt.plot(epochs_list, train_acc, label='train acc')\nplt.plot(epochs_list, val_acc, label='val acc')\nplt.title(\"Fine-tuned MobileNet's Accuracy\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='best')","723c489a":"train_loss = mobilenet_ft_history.history['loss']\nval_loss = mobilenet_ft_history.history['val_loss']\n\nplt.plot(epochs_list, train_loss, label='train loss')\nplt.plot(epochs_list, val_loss, label='val loss')\nplt.title(\"MobileNet's Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc='best')","d28dbd2e":"mobilenet_model.save('mobilenet-fine-tuned')","81ba479b":"### Inspired by this tutorial: https:\/\/keras.io\/guides\/transfer_learning\/","7468522e":"## Pack the input data into data generators with applied data augmentation","2e257703":"## Define the model","4e440986":"## Save the model","02425943":"## Plot the training progress","9cec9355":"## Evaluate the model on the validation data","116fca01":"## Unfreeze the base model and apply fine-tuning of the weights","ad70e128":"## Plot the training progress","58f7604b":"## Evaluate the fine-tuned model","81aaa085":"## Plot the training progress of the fine-tuned model","83c2742f":"## Compile and train the model","137b65a1":"## Save the fine-tuned model","5cbee844":"# Transfer learning with VGG-16 pretrained on the ImageNet","6ca1006f":"## Define the model","c8b55d5b":"## Save the model","03463d27":"## Evaluate the model on the validation data","033bb97e":"## Compile and train the model","fb34e7fa":"# Transfer learning with MobileNet pretrained on the ImageNet"}}