{"cell_type":{"f3c9cf5a":"code","085ecd7d":"code","23ba89c7":"code","166fd7bc":"code","824e0ff5":"code","d02e4bd8":"code","13b6246a":"code","2579f176":"code","b39f5c3c":"code","8bb23111":"code","cc4a7c12":"code","760eebdc":"code","8e251c48":"code","bff5291b":"code","eaa70c0a":"code","ba3d17f9":"code","d080172b":"code","a3529de4":"code","774ad370":"code","a1d15d99":"code","c33a8a7a":"code","6048287e":"code","b2c4de32":"code","fa5c5d0c":"code","d7244e62":"code","b09caa75":"code","f5b6636c":"code","6c7dd5fd":"code","1ff1af7c":"code","da9e611f":"markdown","d00750ed":"markdown","202838f4":"markdown","1343241a":"markdown","892e2e7c":"markdown","4f2c3131":"markdown","3f6b2b58":"markdown","72b7579a":"markdown","e9fc515e":"markdown","2fe5b9ee":"markdown","e07d8a28":"markdown","e639f596":"markdown","80ffe4b4":"markdown","75667e7d":"markdown","142e883b":"markdown","af1c8549":"markdown","4480fcec":"markdown","f9354f65":"markdown","041a99f7":"markdown","e4b8ade5":"markdown","7368e902":"markdown","215c00ad":"markdown","007d98ca":"markdown","0e625168":"markdown"},"source":{"f3c9cf5a":"# Import everything\nfrom mlxtend.plotting import plot_decision_regions\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport shap\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","085ecd7d":"#from google.colab import files\n#uploads = files.upload()\n","23ba89c7":"#Loading the dataset\ndf = pd.read_csv('..\/input\/4class\/ChoiceAddress-4Class.csv')\n\n#Print the all of the dataframe.\ndf","166fd7bc":"print(df.columns)       # \u5217\u51fa\u6bcf\u4e00\u500b column \u7684\u6a19\u982d\n","824e0ff5":"df.describe()","d02e4bd8":"df.describe().T","13b6246a":"p=sns.pairplot(df, hue = 'Class')","2579f176":"# Show the joint distribution using kernel density estimation\n\n# x=\"University\", y=\"Performance\", hue=\"Class\"\ng = sns.jointplot(\n    data=df,\n    x=\"University\", y=\"Performance\", hue=\"Class\",\n    kind=\"kde\",\n)\n# x=\"Convenience\", y=\"Performance\", hue=\"Class\"\ng = sns.jointplot(\n    data=df,\n    x=\"Convenience\", y=\"Performance\", hue=\"Class\",\n    kind=\"kde\",\n)\n# x=\"Fastfood\", y=\"Performance\", hue=\"Class\"\ng = sns.jointplot(\n    data=df,\n    x=\"Fastfood\", y=\"Performance\", hue=\"Class\",\n    kind=\"kde\",\n)","b39f5c3c":"g = sns.lmplot(\n    data=df,\n    x=\"University\", y=\"Performance\", hue=\"Class\",\n    height=5\n)\n# Use more informative axis labels than are provided by default\ng.set_axis_labels(\"University\", \"Performance\")\n\n# x=\"Convenience\", y=\"Performance\", hue=\"Class\"\ng = sns.lmplot(\n    data=df,\n    x=\"Convenience\", y=\"Performance\", hue=\"Class\",\n    height=5\n)\n# Use more informative axis labels than are provided by default\ng.set_axis_labels(\"Convenience\", \"Performance\")\n# x=\"ConvenienceStore\", y=\"Performance\", hue=\"Class\"\ng = sns.lmplot(\n    data=df,\n    x=\"Fastfood\", y=\"Performance\", hue=\"Class\",\n    height=5\n)\n# Use more informative axis labels than are provided by default\ng.set_axis_labels(\"Fastfood\", \"Performance\")","8bb23111":"df.corr()","cc4a7c12":"# on this line I just set the size of figure to 12 by 10.\n# seaborn has very simple solution for heatmap\nplt.figure(figsize=(12,10))  \np=sns.heatmap(df.corr(), annot=True,cmap ='RdYlGn',vmax=0.6, center=0,)  ","760eebdc":"p=sns.pairplot(df.corr())","8e251c48":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(df.drop('Class', axis=1))\nsc_transform = scaler.transform(df.drop('Class', axis=1))\n#scaler.fit(df)\n#sc_transform = scaler.transform(df)\nsc_df = pd.DataFrame(sc_transform)\n\n# Now you can safely use sc_df as your input features.\nsc_df","bff5291b":"sc_transform","eaa70c0a":"from sklearn.model_selection import train_test_split\n\nX = sc_df\n\ny0 = sc_df[0]\ny = df['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n\nprint(X)\nprint(\"-------------------------------------------\")\nprint(y0)\nprint(\"-------------------------------------------\")\nprint(y)","ba3d17f9":"# Initialize an array that stores the error rates.\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#error_rates = []\n\ntest_scores = []\ntrain_scores = []\n\nfor a in range(1, 40):\n    k = a\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    preds = knn.predict(X_test)\n    #error_rates.append(np.mean(y_test - preds))\n\n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))\n\n#plt.figure(figsize=(12, 7))\n#plt.plot(range(1,40),error_rates,color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n#plt.title('Error Rate vs. K Value')\n#plt.xlabel('K')\n#plt.ylabel('Error Rate')","d080172b":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","a3529de4":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))\nts=list(map(lambda x: x+1, test_scores_ind))","774ad370":"plt.figure(figsize=(12,7))\np = sns.lineplot(range(1,40),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,40),test_scores,marker='o',label='Test Score')\nplt.title('Score vs. K Value')\nplt.xlabel('K Value')\nplt.ylabel('Score')","a1d15d99":"k = ts[0]\nprint(\"Use K =\",k)\nknn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(X_train, y_train)\npreds = knn.predict(X_test)","c33a8a7a":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, preds)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","6048287e":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, preds))","b2c4de32":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,110)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=20)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","fa5c5d0c":"from sklearn import svm\nclf = svm.SVC()\n# X,y = sc_df , df[\"Class\"]\nclf.fit(X,y)\n\nfrom joblib import dump, load\ndump(clf, 'ChoiceAddress-4Class.joblib') ","d7244e62":"# explain AI with Shap\nimport sklearn\n\n# knn Classifier model\n# model = KNeighborsClassifier(n_neighbors=k)\n# knn.fit(X_train, y_train)\n# preds = knn.predict(X_test)\n\n\nknn = KNeighborsClassifier()\nknn.fit(X, y)\n\nf = lambda x: knn.predict_proba(x)[:,1]\nmed = X_train.median().values.reshape((1,X_train.shape[1]))\n\n# compute SHAP values\nexplainer = shap.Explainer(f, med)\nshap_values = explainer(X_train.iloc[0:110,:])\n","b09caa75":"shap.plots.beeswarm(shap_values, max_display=16)\n","f5b6636c":"shap.plots.heatmap(shap_values)\n","6c7dd5fd":"shap.plots.heatmap(shap_values, max_display=16)\n","1ff1af7c":"shap.plots.heatmap(shap_values, max_display=16, feature_values=shap_values.abs.max(0))\n","da9e611f":"\u756b\u51fa\u6df7\u6dc6\u77e9\u9663\u5716","d00750ed":"\u5f9eGoogle Colab\u532f\u5165files\u6a21\u7d44\n\u4e26\u4e0a\u50b3\u8cc7\u6599\u6a94\u5230Google Colab\n\u5728Kaggle\u57f7\u884c\u6703\u6709\u932f\u8aa4--->ModuleNotFoundError: No module named 'google.colab'\n","202838f4":"\u5217\u51fa\u8cc7\u6599\u96c6\u7684\u5404\u9805\u7d71\u8a08\u8cc7\u6599","1343241a":"\u5c0d\u8cc7\u6599\u9032\u884c\u76f8\u95dc\u4fc2\u6578\uff08corr\uff09\u5206\u6790","892e2e7c":"\u6700\u4f73\u53c3\u6578 K\u503c\u7684\u63a2\u8a0e","4f2c3131":"\u5c07\u76f8\u95dc\u4fc2\u6578corr\uff0c\u4f7f\u7528pair \u756b\u51fa\u4f86\u3002","3f6b2b58":"# **\u9078\u5740KNN-4\u985e\u5225**\n\n\u4ee5\u6708\u5e73\u5747\u71df\u696d\u984d\uff08Performance\uff09\u4f86\u5224\u65b7\u5e97\u7684\u7b49\u7d1a\n1.   Performance >= 600000 , Calss = A\n1.   600000 > Performance >= 500000 , Calss = B\n2.   500000 > Performance >= 430000 , Calss = C\n2.   Performance < 430000 , Calss = D\n\n\n\u53c3\u8003\u8cc7\u6599\uff1a\n1. https:\/\/www.kdnuggets.com\/2020\/04\/introduction-k-nearest-neighbour-algorithm-using-examples.html\n1. https:\/\/www.kaggle.com\/kwoshunhung\/step-by-step-diabetes-classification-knn-detailed\n1. https:\/\/seaborn.pydata.org\/examples\/index.html","72b7579a":"\u8a66\u756b\u51fa\u4ee5x=\"University\", y=\"Performance\", hue=\"Class\"\u7684lm\u5716\u5f62","e9fc515e":"\u5217\u51fa\u6bcf\u4e00\u500b column \u7684\u6a19\u982d","2fe5b9ee":"\u5206\u985e\u9810\u6e2c\u7d50\u679c\u5831\u544a","e07d8a28":"\u627e\u51fa\u8a13\u7df4\u96c6\u6700\u9ad8\u5206\u6578\u7684K\u503c","e639f596":"\u756b\u51fa\u8cc7\u6599\u4ee5\"Class\"\u70ba\u4e3b\u7684pair\u5716\u5f62","80ffe4b4":"\u8a66\u756b\u51fa\u4ee5x=\"University\", y=\"Performance\", hue=\"Class\"\u7684joint\u5716\u5f62","75667e7d":"\u4ee5\u6708\u5e73\u5747\u71df\u696d\u984d\uff08Performance\uff09\u4f86\u5224\u65b7\u5e97\u7684\u7b49\u7d1a\n\n1. Performance >= 600000 , Calss = A\n1. 600000 > Performance >= 500000 , Calss = B\n1. 500000 > Performance >= 430000 , Calss = C\n1. Performance < 430000 , Calss = D\n\n\n\n","142e883b":"Explain AI with Shapley values","af1c8549":"\u5c07\u8cc7\u6599\u6a94ChoiceAddress-4Class.csv\u76f4\u63a5\u653e\u5230kaggle\uff0c\u4e26\u4e14\u5c07\u8cc7\u6599\u6a94\u8b80\u9032\u4f86\uff0c\u4e26\u5217\u51fa\u4f86\u3002","4480fcec":"\u5c07\u76f8\u95dc\u4fc2\u6578corr\uff0c\u4f7f\u7528HeatMap \u756b\u51fa\u4f86\u3002","f9354f65":"\u5c07\u6700\u9ad8\u5206\u6578\u7684K\u503c\u4ee3\u5165KNN\u6f14\u7b97\u6cd5\u904b\u7b97\u201c\u9810\u6e2c\u503c\u201d","041a99f7":"\u5c07\u8cc7\u6599\u6a19\u6e96\u5316","e4b8ade5":"\u628a\u6a21\u578b\u5b58\u8d77\u4f86","7368e902":"\u5c07\u8cc7\u6599\u5207\u5206\u70ba\u8a13\u7df4\u96c6\u8207\u6e2c\u8a66\u96c6\uff0c\u6e2c\u8a66\u96c6\u7684\u6bd4\u4f8b\u70ba30%","215c00ad":"\u5c07K\u503c\u4ee51~40\u4ee3\u5165\u904b\u7b97\uff0c\u5c0b\u627e\u8a13\u7df4\u96c6\u8207\u6e2c\u8a66\u96c6\u6700\u9ad8\u5206\u7684K\u503c\uff0cError Rates\u5728\u6b64\u7121\u6cd5\u4f7f\u7528\u3002","007d98ca":"\u627e\u51fa\u6e2c\u8a66\u96c6\u6700\u9ad8\u5206\u6578\u7684K\u503c","0e625168":"\u756b\u51fa K = 1 ~ 40 \u7684 Score vs. K Value \u7684\u66f2\u7dda"}}