{"cell_type":{"d353c967":"code","bd7e66c6":"code","987f312b":"code","a69047aa":"code","42308d07":"code","af726f6d":"code","086b12fd":"code","2ce59f30":"code","5ec32fb5":"code","5068845a":"code","609146a1":"code","aa8e7c6c":"code","9eb15e5c":"code","5a0f91b1":"code","9f01f55d":"code","0f07777c":"code","687143aa":"code","880bf66f":"code","4877dc41":"code","333552bb":"code","9004d9da":"code","ce5faa99":"code","4174f199":"code","1c7f2b88":"code","e586ca09":"markdown","bc57c0e4":"markdown","3c129403":"markdown","d074252d":"markdown","ebe19389":"markdown","6027862d":"markdown","d3891f68":"markdown","c0f08cba":"markdown","11012e1e":"markdown","fcc8443a":"markdown","d071e279":"markdown","b6c1d390":"markdown","2a02b66d":"markdown","8832061c":"markdown","d4604287":"markdown","d9218dc1":"markdown","ce39e869":"markdown","d3203d22":"markdown","b30ed77c":"markdown","c50bcb1f":"markdown","8c62f509":"markdown","272a0c67":"markdown","59eef7d2":"markdown","b676ec42":"markdown","428e7d7a":"markdown","03e0e769":"markdown","2bfff61f":"markdown","cc2c111d":"markdown","7cc08277":"markdown","72754f1e":"markdown"},"source":{"d353c967":"# import Python libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom mpl_toolkits.mplot3d import Axes3D","bd7e66c6":"# create a list of columns for a dataset\ncolumn_names = ['CRIM', 'ZN','INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\npath = '..\/input\/boston-house-prices\/housing.csv'\n# read the dataset\ndf = pd.read_csv(path, delim_whitespace=True, header = None, names = column_names)","987f312b":"# display the first five rows\ndf.head()","a69047aa":"null_values = sum(df.isnull().sum())\nif null_values == 0:\n    print('There is no null_values')\nelse:\n    print('Please deal with null_values')","42308d07":"duplicates = df.duplicated().sum()\nif duplicates == 0:\n    print('There is no duplicated rows')\nelse:\n    print('Please deal with duplicates')","af726f6d":"# Find which variables correlate the most with the target variable\ncorrelation = abs(df.corr())\n\n# Plot the heat map with respect to target variable only\nplt.figure(figsize=(12,4), tight_layout=True)\nsns.heatmap(correlation, cmap=\"YlGnBu\")\nplt.ylim(13,14)\nplt.xlim(0,13);","086b12fd":"# Assign features and the target to the the variables\nX = df.drop('MEDV', axis = 1)\ny = df.MEDV\n\n# fit the model\nmodel = RandomForestRegressor().fit(X, y)\n\n# find fearute importances\nfeature_importances = model.feature_importances_\n\n# prepare data for plotting\nfeatures = X.columns\nfeatures_df = (\n    pd.DataFrame({'Features': features, 'Importance Score': feature_importances})\n    .sort_values(by='Importance Score', ascending=False))\n# Create a barchart\nplt.figure(figsize=(6,4), tight_layout=True)\nsns.barplot(data = features_df, x = 'Importance Score', y = 'Features', color = '#33B3FF')\nplt.title('Feature Importance');","2ce59f30":"# Assign features and target variables to X and y\nX = df.RM.values\ny = df.MEDV.values\n\n# Setup learning rate\nl_rate = 0.00001\n\n# Setup a number of iterations \nn_iters = 5000\n\n# Assign a random integer to a slope and intercept\nslope, intercept = np.random.rand(), np.random.rand()\n\n# Store a number of feature values\nlength =len(X)\n\n# Create starting point for calculation of partial derivatives\nslope_deriv, intercept_deriv = 0, 0\n    \n# Create empty lists for storing the output of each iteration\ncost_list, slope_list, intercept_list = [], [], []\n\n# State the learning rate\nlearning_rate = l_rate\n\n# State the number of iterations \nfor i in range (n_iters):\n    #formulate a linear equation\n    y_pred = slope * X + intercept\n\n    # cost function\n    cost = 0\n    for i in range(length):\n        cost += ((slope * X[i] + intercept) - y[i]) ** 2\n    cost = cost \/ length\n    # Calculate partial derivatives of a cost function with respect to slope and to intercept\n    for i in range(length):\n        slope_deriv += ((slope * X[i] + intercept) - y[i]) * 2 * X[i]\n        intercept_deriv += (slope * X[i] + intercept) - y[i]\n    # Update slope and intercept\n    slope -= 1\/length * learning_rate * slope_deriv\n    intercept -= 1\/length * learning_rate * intercept_deriv\n    # Appending lists for future processing\n    slope_list.append(slope)\n    intercept_list.append(intercept)\n    cost_list.append(cost)","5ec32fb5":"# Calculate min of the cost function\ncost_min = cost_list.index(np.min(cost_list))\nprint('The minimum of the cost function is {:.2f}, which can be found at {} iteration'.format(np.min(cost_list), cost_min))","5068845a":"# visualize the cost function and its minimum\ndef cost_plot():\n    plt.plot(cost_list, label = 'Cost function')\n    plt.scatter(cost_list.index(np.min(cost_list)), np.min(cost_list),\n            color = 'red', marker = 'x', s = 70, label = 'Minimum')\n    plt.xlabel('Iterations')\n    plt.legend()\n    plt.ylim(0, np.max(cost_list) + 100)","609146a1":"# Make some additions to visualization of the cost function and the minimum\nplt.subplots(figsize=(16, 4))\nplt.subplot(1,2,1)\ncost_plot()\nplt.ylabel('Value of cost function')\nplt.title('Cost function with its minimum')\nplt.xlim(0, n_iters)\nplt.subplot(1,2,2)\ncost_plot()\nplt.title('Cost function with its minimum zoomed in')\nplt.xlim(cost_min -100, cost_min +100);","aa8e7c6c":"# Implementation of sklearn linear regression\nmodel = LinearRegression().fit(X.reshape(-1,1), y.reshape(-1, 1))\npred = model.predict(X.reshape(-1, 1))","9eb15e5c":"def single_regr_plot(prediction):\n    plt.scatter(X, y, color = '#20B648',\n            marker = 'o', edgecolor = '#56798B', alpha = 0.7, label = 'Original Data')\n\n    plt.plot(X, prediction, color = '#15029A', label = 'Prediction')\n    plt.xlabel(\"Average Number of Rooms per Dwelling\")\n    plt.legend();","5a0f91b1":"# Plot linear regression\nplt.subplots(figsize=(16, 4))\nplt.subplot(1,2,1)\nsingle_regr_plot(slope_list[cost_min] * X + intercept_list[cost_min])\nplt.ylabel(\"Median value of owner-occupied homes in $1000's\")\nplt.title('Manual model')\nplt.subplot(1,2,2)\nsingle_regr_plot(pred)\nplt.title('Sklearn LinearRegression');","9f01f55d":"# Calculate score for manual model\nmanual_score = (1 - (((y - (slope_list[cost_min] * X + intercept_list[cost_min]))** 2).sum())\n \/(((y - y.mean()) ** 2).sum()))\n# Calculate score for sklearn model\nscore = model.score(X.reshape(-1, 1), y.reshape(-1, 1))\n# Display the results\nprint('Mean squared error of the manual model is {:.2f} \\nMean squared error of the sklearn model is {:.2f}'.format(manual_score, score))","0f07777c":"# Assign features and target variables to X and y\nX = df[['LSTAT', 'RM']].values\ny = df.MEDV.values\n\n# Setup learning rate\nl_rate = 0.00001\n\n# Setup a number of iterations \nn_iters = 2000\n\n# Assign a random integer to a slope and intercept\nslope1, slope2, intercept = np.random.rand(), np.random.rand(), np.random.rand()\n\n# Store a number of feature values\nlength =len(X)\n\n# Create starting point for calculation of partial derivatives\nslope1_deriv, slope2_deriv, intercept_deriv = 0, 0, 0\n    \n# Create empty lists for storing the output of each iteration\ncost_list, slope1_list, slope2_list, intercept_list = [], [], [], []\n\nfor _ in range (n_iters):\n    #formulate a linear equation\n    y_pred = slope1 * X[:, 0] + slope2 * X[:, 1] + intercept\n\n    # Cost function\n    cost = 0\n    for i in range(length):\n        cost += ((slope1 * X[:, 0][i] + slope2 * X[:, 1][i] + intercept) - y[i]) ** 2\n    cost = cost \/ length\n\n    # Calculate partial derivatives of a cost function with respect to slopes and intercept\n    for i in range(length):\n        slope1_deriv += ((slope1 * X[:, 0][i] + slope2 * X[:, 1][i] + intercept) - y[i]) * X[:, 0][i]\n        slope2_deriv += ((slope1 * X[:, 0][i] + slope2 * X[:, 1][i] + intercept) - y[i]) * X[:, 1][i]\n        intercept_deriv += (slope1 * X[:, 0][i] + slope2 * X[:, 1][i] + intercept) - y[i]\n    # Update the slopes and the intercept\n    slope1 -= 1\/length * learning_rate * slope1_deriv\n    slope2 -= 1\/length * learning_rate * slope2_deriv\n    intercept -= 1\/length * learning_rate * intercept_deriv\n    \n    # Appending lists for future processing\n    cost_list.append(cost)\n    slope1_list.append(slope1)\n    slope2_list.append(slope2)\n    intercept_list.append(intercept)","687143aa":"# Calculate min of the cost function\ncost_min = cost_list.index(np.min(cost_list))\nprint('The minimum of the cost function is {:.2f}, which can be found at {} iteration'.format(np.min(cost_list), cost_min))","880bf66f":"# Make some additions to visualization of the cost function and the minimum\nplt.subplots(figsize=(16, 4))\nplt.subplot(1,2,1)\ncost_plot()\nplt.ylabel('Value of cost function')\nplt.title('Cost function with its minimum')\nplt.xlim(0, n_iters)\nplt.subplot(1,2,2)\ncost_plot()\nplt.title('Cost function with its minimum zoomed in')\nplt.xlim(cost_min -200, cost_min +50);","4877dc41":"# Implementation of sklearn LinearRegression\nmodel = LinearRegression().fit(X, y)\npred = model.predict(X)","333552bb":"# Preparation data for 3d plot\nxx = X[:, 0]\nyy = X[:, 1]\nz = y\n\none_pred = np.linspace(xx.min(), xx.max(), 50)  \ntwo_pred = np.linspace(yy.min(), yy.max(), 50)  \nxx_pred, yy_pred = np.meshgrid(one_pred, two_pred)\nmodel_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T","9004d9da":"# Manual prediction\nmanual = slope1_list[cost_min] * model_viz[:, 0] + slope2_list[cost_min] * model_viz[:, 1] + intercept_list[cost_min]\n# Sklearn prediction\npredicted = model.predict(model_viz)","ce5faa99":"# Define a function for plotting 3d scatter\ndef mult_regr_plot(axis, prediction):\n    axis.plot(xx, yy, z, color='k', zorder=15, linestyle='none', marker='.', alpha=0.5)\n    axis.scatter(xx_pred.flatten(), yy_pred.flatten(), prediction, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n    axis.set_xlabel('LSTAT')\n    axis.set_ylabel('RM')\n    axis.set_zlabel('MEDV')\n    axis.view_init(30, 70);","4174f199":"# Create visualization for manual and sklearn LinearRegression model\nfig = plt.figure(figsize=(25, 20))\nax1 = fig.add_subplot(131, projection='3d')\nmult_regr_plot(ax1, manual)\nplt.title('Manual model')\n\nax2 = fig.add_subplot(132, projection='3d')\nplt.title('Sklearn LinearRegression');\nmult_regr_plot(ax2, predicted)","1c7f2b88":"# Calculate score for manual model\nmanual_score = 1 - (((y - (slope1_list[cost_min] * X[:, 0] + slope2_list[cost_min] * X[:, 1] \n            + intercept_list[cost_min]))** 2).sum())\/(((y - y.mean()) ** 2).sum())\n# Calculate score for sklearn model\nscore = model.score(X, y)\n# Display the results\nprint('Mean squared error of the manual model is {:.2f} \\nMean squared error of the sklearn model is {:.2f}'.format(manual_score, score))","e586ca09":"<a id='Variables'><\/a>\n## Description of variables\n[to the top](#Table)\n\n| VariableName | Description |\n|:--|:--|\n|CRIM |per capita crime rate by town|\n|ZN |proportion of residential land zoned for lots over 25,000 sq.ft.|\n|INDUS |proportion of non-retail business acres per town.|\n|CHAS |Charles River dummy variable (1 if tract bounds river; 0 otherwise)|\n|NOX |nitric oxides concentration (parts per 10 million)|\n|RM |average number of rooms per dwelling|\n|AGE |proportion of owner-occupied units built prior to 1940|\n|DIS |weighted distances to five Boston employment centres|\n|RAD |index of accessibility to radial highways|\n|TAX |full-value property-tax rate per \\$10,000|\n|PTRATIO |pupil-teacher ratio by town|\n|B |1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town|\n|LSTAT |\\% lower status of the population|\n|MEDV |Median value of owner-occupied homes in $1000's|","bc57c0e4":"In conclusion, I can say that indeed we can understand the behavior of Linear Regression by using simple mathematic concepts. ","3c129403":"## Table of contents\n- [Introduction](#Introduction)  \n- [Import packages](#Packages)\n- [Description of variables](#Variables)\n- [Reading the dataset](#Dataset)\n- [Data assessment](#Assessment)\n- [Feature Selection](#Feature)\n- [Single variable linear regression](#Single)\n- [Multiple variable linear regression](#Insights)  \n- [Conclusions](#Conclusions)\n- [Credits](#Credits)","d074252d":"<a id='Dataset'><\/a>\n## Reading the dataset\n[to the top](#Table)","ebe19389":"<a id='Single'><\/a>\n## Single variable linear regression\n[to the top](#Table)","6027862d":"Now I will compare the results of my manual model and sklearn LinearRegression","d3891f68":"We need to perform an iterative process to find the line of best fit. This process:  \n- creates a linear function with a random slope and intercept\n- calculates a mean squared error (cost) function, which is the average of the squared difference between the predicted value and target value\n- calculates partial derivatives of a cost function with respect to both the slope and the intercept.  \n- Updates the slope and the intercept by way of multiplying partial derivatives by a predetermined rate (learning rate) and deducting the product from the slope and the intercept\n- The model is ready at the point where the cost function is at its minimum value.\n\nNow I will try to find the prediction line manually. First, I will focus on constructing the model with only one feature variable and then compare the results with sklearn LinearRegression","c0f08cba":"<a id='Table'><\/a>\n# <center> Manual Linear Regression","11012e1e":"##### Calulating feature importance with Random Forest Regressor","fcc8443a":"I watched a lot of videos on Youtube and what I learned about Linear regression is that it is a simple model. It requires knowledge of linear function, cost function, some derivatives rules, and partial derivatives. I will leave links at the end of this notebook.","d071e279":"Here we can see that RM and LSTAT have the highest correlation with the target variable","b6c1d390":"The graph above shows that the most important features in predicting the target variable are __LSTAT__ and __RM__. I will use them for modeling","2a02b66d":"I chose the Boston House Price dataset. This dataset is easy to find on the Internet. The dataset is suitable for supervised learning for predicting continuous variables. Below is the description of the variables.","8832061c":"Here I wanted to select a maximum of two features to predict a target variable. I will start with looking at the correlation between each of the features with target. I will use a heatmap to visualize the outcome. To make the heatmap more interpretable, the absolute value of the correlation coefficient will be used. it is assumed that the features with the higher correlation coefficient are the best choice for including in the model ","d4604287":"<a id='Packages'><\/a>\n## Import packages\n[to the top](#Table)","d9218dc1":"The results of both models look similar.","ce39e869":"Since the most important feature is RM (average number of rooms per dwelling), I will use it to predict MEDV (median value of owner-occupied homes). __I will not be splitting the dataset by training and testing portions, because the main objective of this exercise is just to recreate the fit method of LinearRegression__ ","d3203d22":"<a id='Assessment'><\/a>\n## Data assessment\n[to the top](#Table)","b30ed77c":"In this notebook, I wanted to recreate LinearRegression _fit_ method manually. I am new to Machine Learning and was curious how this model works. So this notebook may be interesting for beginners.","c50bcb1f":"I hope you liked this notebook. If you have any advice on how to improve the code, please leave a comment. I will be glad to discuss. ","8c62f509":"<a id='Conclusions'><\/a>\n## Conclusions\n[to the top](#Table)","272a0c67":"<a id='Multiple'><\/a>\n## Multiple variable linear regression\n[to the top](#Table)","59eef7d2":"Now I will check if the dataset has any missing values or duplicated rows","b676ec42":"<a id='Feature'><\/a>\n## Feature selection\n[to the top](#Table)","428e7d7a":"<a id='Introduction'><\/a>\n## Introduction\n[to the top](#Table)","03e0e769":"Now I want to check whether the selection from the step above is reasonable. This time I will use feature_importances from RandomForestRegressor","2bfff61f":"In this section, I will be trying to recreate the fit method of LinearRegression for a dataset with 2 features. I will choose the two most important features __RM__ and __LSTAT__ . For 2 features or the process is essentially the same. However, instead of dealing with one slope, I will be dealing with two slopes. The linear function for two features is __slope1 x feature1 + slope2 x feature2 + intercept__. Therefore partial derivatives should be calculated with respect to both slopes and the intercept","cc2c111d":"- I watched [this free course](https:\/\/www.khanacademy.org\/math\/algebra\/x2f8bb11595b61c86:linear-equations-graphs) from Khan Academy to understand the principles of linear equation\n- [This course on differential caculus](https:\/\/www.khanacademy.org\/math\/differential-calculus) helped me to understand power rule and chain rule\n- Partial Derivatives can be found [here](https:\/\/www.khanacademy.org\/math\/multivariable-calculus\/multivariable-derivatives)\n- In-depth explanation from XuanKhanh Nguyen about how Linear Regression works can be found [here](https:\/\/towardsdatascience.com\/minimizing-the-cost-function-gradient-descent-a5dd6b5350e1)\n- I used [this source](https:\/\/aegis4048.github.io\/mutiple_linear_regression_and_visualization_in_python) to understand how to create 3d plot for visualisation of Multiple Linear Regression","7cc08277":"# <center><img src=\"https:\/\/www.analyzemath.com\/line\/graph-of-line-example-1.gif\" style=\"width: 200px;\"\/>","72754f1e":"<a id='Credits'><\/a>\n## Credits\n[to the top](#Table)"}}