{"cell_type":{"47e04d4c":"code","b4df64c4":"code","1e0a1c8b":"code","da49cc1a":"code","e12f40eb":"code","b6bef7fa":"code","1e2b452d":"code","2ffeb06c":"code","4d1eb92b":"code","6127122a":"code","ee0f7048":"code","9c42056d":"code","85292900":"code","1a81d23a":"code","d2d195d5":"code","ad480577":"code","03177100":"code","4cd92f6d":"code","0577e7cc":"code","6887522b":"code","6d3cd6fe":"code","7d88c766":"code","8efe8fc3":"code","3789c544":"code","396678b6":"code","5a7d5de4":"code","25a1b3df":"code","79e356f6":"code","55e684e7":"code","6d2847dc":"code","9653e9f5":"code","03d874a2":"code","36b77c4e":"code","c3c5a423":"code","e0f33c71":"code","fe9107a7":"code","d58e3f69":"code","4c7078f7":"code","9ac055f1":"code","9f2d882e":"code","fa8c17c3":"code","ed394013":"code","20291422":"code","25c3e18a":"code","122b2bb4":"code","3e38d392":"code","7bf31b75":"code","d864f526":"code","acf857d0":"code","4dacae0c":"code","b08e655d":"code","abe2500b":"code","b97eb53f":"code","c723cb7c":"code","9a47789a":"code","c1f8776d":"code","b03a1e73":"code","1ce12c29":"code","cd0741c2":"code","05fd32d4":"code","79345210":"code","675b527d":"code","9408e997":"code","59e413b7":"code","a9d3a090":"code","f288c42a":"code","f64852f3":"code","61703cd9":"code","cfeeb659":"code","f34660c2":"code","1e2ca84f":"code","87696ba3":"code","71798172":"code","49674e41":"code","076f407e":"code","f7f0b576":"code","cde21407":"code","f5735588":"code","ad1f10cc":"code","01cc47af":"code","be8bdad2":"code","7b94d207":"code","ba137252":"code","fee0fe8d":"code","14f83678":"code","75446094":"code","0600a8b1":"code","25b7a609":"code","dad170b2":"code","71c8d8ee":"code","6b65cfae":"code","d37d0542":"code","19a3b8d5":"code","deeaeb46":"markdown","4f207900":"markdown","ae44b80a":"markdown","63883b3b":"markdown","4597f1f6":"markdown","a9911f99":"markdown","f3fa89b0":"markdown","039ed229":"markdown","21d7b0c4":"markdown","62d95443":"markdown","80ac3ebe":"markdown","22bcaba3":"markdown","5f496729":"markdown","7736221b":"markdown","6059ba54":"markdown","c9f78c28":"markdown","6b729084":"markdown","ed189415":"markdown","54fc415f":"markdown","fa66abd0":"markdown","186e18fe":"markdown","5bd9d939":"markdown","5477a9ea":"markdown","9d8a6387":"markdown","35e7acd9":"markdown","c74bd1c8":"markdown","64e8e527":"markdown","cfb00326":"markdown","f3761be8":"markdown","55706fcb":"markdown","2d40b0a3":"markdown","f6de5160":"markdown","d16aefce":"markdown","c49c6257":"markdown","7327cc0f":"markdown","9e82433b":"markdown","c605bbff":"markdown"},"source":{"47e04d4c":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nimport gc\nfrom itertools import product\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nfrom statsmodels.tsa.stattools import acf\ndata_path = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'","b4df64c4":"def downcast_dtypes(df):\n    start_size = df.memory_usage(deep = True).sum() \/ 1024**2\n    print('Memory usage: {:.2f} MB'.format(start_size))\n\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int32)\n    end_size = df.memory_usage(deep = True).sum() \/ 1024**2\n    print('New Memory usage: {:.2f} MB'.format(end_size))\n    return df\n\ndef create_record_for_features(df, attrs, target, time_col, aggfunc = np.sum, fill = 0):\n    target_for_attrs = df.pivot_table(index = attrs,\n                                   values = target, \n                                   columns = time_col, \n                                   aggfunc = aggfunc, \n                                   fill_value = fill,\n                                  ).reset_index()\n    target_for_attrs.columns = target_for_attrs.columns.map(str)\n    target_for_attrs = target_for_attrs.reset_index(drop = True).rename_axis(None, axis = 1)\n    return target_for_attrs\n\ndef display_df_info(df, name):\n    print('-----------Shape of '+ name + '-------------')\n    print(df.shape)\n    print('-----------Missing values---------')\n    print(df.isnull().sum())\n    print('-----------Null values------------')\n    print(df.isna().sum())\n    print('-----------Data types-------------')\n    print(df.dtypes)\n    print('-----------Memory usage (MB)------')\n    print(np.round(df.memory_usage(deep = True).sum() \/ 1024**2, 2))","1e0a1c8b":"sales = pd.read_csv(data_path + 'sales_train.csv')\nsales = downcast_dtypes(sales)","da49cc1a":"items = pd.read_csv(data_path + 'items.csv')\nitems = downcast_dtypes(items)","e12f40eb":"item_categories = pd.read_csv(data_path + 'item_categories.csv')\nitem_categories = downcast_dtypes(item_categories)","b6bef7fa":"shops = pd.read_csv(data_path + 'shops.csv')\nshops = downcast_dtypes(shops)","1e2b452d":"test = pd.read_csv(data_path + 'test.csv')\ntest = downcast_dtypes(test)","2ffeb06c":"display_df_info(sales, 'Sales')","4d1eb92b":"display_df_info(items, 'items')","6127122a":"display_df_info(item_categories, 'item Categories')","ee0f7048":"display_df_info(shops, 'shops')","9c42056d":"display_df_info(test, 'Test set')","85292900":"sales_sampled = sales.sample(n = 10000)\nsns.pairplot(sales_sampled[['date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day']], diag_kind = 'kde')\nplt.show()","1a81d23a":"del sales_sampled\ngc.collect()","d2d195d5":"sns.boxplot(x = sales['item_price'])\nplt.show()","ad480577":"sales.loc[:, 'item_price'] = sales.loc[:, 'item_price'].clip(-1, 10**5)\nsale_with_negative_price = sales[sales['item_price'] < 0]\nsale_with_negative_price","03177100":"sale = sales[(sales.shop_id == 32) & (sales.item_id == 2973) & (sales.date_block_num == 4) & (sales.item_price > 0)]\nmedian = sale.item_price.median()\nsales.loc[sales.item_price < 0, 'item_price'] = median","4cd92f6d":"del sale \ndel median\ndel sale_with_negative_price\ngc.collect()","0577e7cc":"sns.boxplot(sales['item_cnt_day'])\nplt.show()","6887522b":"sales_temp = sales[sales['item_cnt_day'] > 500]\nprint('Sold item outliers')\nitems[items['item_id'].isin(sales_temp['item_id'].values)].merge(sales_temp[['item_id', 'item_cnt_day', 'date_block_num']], on = 'item_id')","6d3cd6fe":"del sales_temp\ngc.collect()","7d88c766":"print('Number of duplicates:', len(sales[sales.duplicated()]))","8efe8fc3":"sales = sales.drop_duplicates(keep = 'first')\nprint('Number of duplicates:', len(sales[sales.duplicated()]))","3789c544":"start = time.time()\nsales.date = sales.date.apply(lambda x: datetime.datetime.strptime(x, '%d.%m.%Y'))\nprint('First sale took place: ', sales.date.min())\nprint('Last sale took place: ', sales.date.max())\nprint('It tooks: ', round(time.time() - start), 'seconds')","396678b6":"start = time.time()\npairs_trans = create_record_for_features(sales, ['shop_id', 'item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.count_nonzero, fill = 0)\nprint('It tooks: ', round(time.time() - start), 'seconds')","5a7d5de4":"for month in range(1, 12):\n    pairs_temp = pairs_trans[['shop_id', 'item_id']][pairs_trans.loc[:,'0': str(month)].sum(axis = 1) == 0]\n    print('From month: 0 until ', month,', ', np.round(100 * len(pairs_temp) \/ len(pairs_trans), 2), '% of the item\/shop pairs have made no sales')","25a1b3df":"for month in range(21, 33):\n    pairs_temp = pairs_trans[['shop_id', 'item_id']][pairs_trans.loc[:,str(month): '33'].sum(axis = 1) == 0]\n    print('From month: ', month, ' until month: 33, ', np.round(100 * len(pairs_temp) \/ len(pairs_trans), 2), '% of the item\/shop pairs have made no sales')","79e356f6":"del pairs_trans\ndel pairs_temp\ngc.collect()","55e684e7":"start = time.time()\nitems_trans = create_record_for_features(sales, ['item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.count_nonzero, fill = 0)\nprint('It tooks: ', round(time.time() - start), 'seconds')","6d2847dc":"for month in range(1, 12):\n    items_temp = items_trans['item_id'][items_trans.loc[:,'0': str(month)].sum(axis = 1) == 0]\n    print('From month: 0 until ', month,', ', np.round(100 * len(items_temp) \/ len(items_trans), 2), '% of the items have made no sales')","9653e9f5":"for month in range(21, 33):\n    items_temp = items_trans['item_id'][items_trans.loc[:,str(month): '33'].sum(axis = 1) == 0]\n    print('From month: ', month, ' until month: 33, ', np.round(100 * len(items_temp) \/ len(items_trans), 2), '% of the items have made no sales')","03d874a2":"del items_trans\ndel items_temp\ngc.collect()","36b77c4e":"start = time.time()\nshops_trans = create_record_for_features(sales, ['shop_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.count_nonzero, fill = 0)\nprint('It tooks: ', round(time.time() - start), 'seconds')","c3c5a423":"for month in range(1, 12):\n    shops_temp = shops_trans['shop_id'][shops_trans.loc[:,'0': str(month)].sum(axis = 1) == 0]\n    print('From month: 0 until ', month,', ', np.round(100 * len(shops_temp) \/ len(shops_trans), 2), '% of the shops have made no sales')","e0f33c71":"for month in range(21, 33):\n    shops_temp = shops_trans['shop_id'][shops_trans.loc[:, str(month): '33'].sum(axis = 1) == 0]\n    print('From month: ', month, ' until month: 33, ', np.round(100 * len(shops_temp) \/ len(shops_trans), 2), '% of the shops have made no sales')","fe9107a7":"del shops_trans\ndel shops_temp\ngc.collect()\n","d58e3f69":"item_id = 20949\nshop_ids = [25, 24]\nitem_sales = sales[(sales['item_id'] == item_id) & (sales['shop_id'].isin(shop_ids))]\nfig, axs = plt.subplots(figsize = (10, 6),  constrained_layout=True)\nsns.pointplot(x = 'date_block_num', y = 'item_cnt_day', hue = 'shop_id', data = item_sales)\naxs.set_title('Sales for item: ' + str(item_id))\nplt.show()","4c7078f7":"print('test shape: ', test.shape)\nprint('number of items in test set: ', test['item_id'].nunique())\nprint('number of shops in test set: ', test['shop_id'].nunique())\n","9ac055f1":"test['item_id'].nunique() * test['shop_id'].nunique() == len(test)","9f2d882e":"items_test = set(test.item_id)\nitems_sales = set(sales.item_id)\nitem_in_test_and_sales = items_test.intersection(items_sales)\nitem_in_test_not_sales = set(test.item_id) - items_sales.intersection(items_test)\nprint('There is sales history for:', np.round(100 * len(item_in_test_and_sales) \/ len(items_test), 2) , '% items in test set')\nprint('There is No sales history for:', np.round(100 * len(item_in_test_not_sales) \/ len(items_test), 2), '% items in test set')","fa8c17c3":"shops_test = set(test.shop_id)\nshops_sales = set(sales.shop_id)\nshops_in_test_and_sales = shops_test.intersection(shops_sales)\nshops_in_test_not_sales = set(test.shop_id) - shops_test.intersection(shops_sales)\nprint('There is sales history for:', np.round(100 * len(shops_in_test_and_sales) \/ len(shops_test), 2), '% shops in test set')\nprint('There is No sales history for:', np.round(100 * len(shops_in_test_not_sales) \/ len(shops_test), 2), '% shops in test set')","ed394013":"item_shop_test = set(test.item_id.astype(str) + '_' + test.shop_id.astype(str))\nitem_shop_sales = set(sales.item_id.astype(str) + '_' + sales.shop_id.astype(str))\npairs_with_history = len(item_shop_test.intersection(item_shop_sales) )\npairs_with_no_history = len(shops_in_test_and_sales) * len(item_in_test_not_sales)\njust_item_with_history = test.shape[0] - (pairs_with_history + pairs_with_no_history)\nprint('There is sales history for:', np.round(100 * pairs_with_history \/ len(test), 2) , '% items in the same shops')\nprint('There is No sales history for:',  np.round(100 * pairs_with_no_history \/ len(test), 2), '% shop\/item pairs')\nprint('There is sales history for:',  np.round(100 * just_item_with_history \/ len(test), 2), '% items but in different shops')","20291422":"del items_test\ndel items_sales\ndel item_in_test_and_sales\ndel item_in_test_not_sales\ndel shops_test\ndel shops_sales\ndel shops_in_test_and_sales\ndel shops_in_test_not_sales\ndel item_shop_test\ndel item_shop_sales\ndel pairs_with_history\ndel pairs_with_no_history\ndel just_item_with_history\ngc.collect()","25c3e18a":"shops['shop_name'] = shops['shop_name'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\nshops['shop_city'], shops['shop_name'] = shops['shop_name'].str.split(' ', 1).str\nshops['shop_type'] = shops['shop_name'].apply(lambda x: '\u043c\u0442\u0440\u0446' if '\u043c\u0442\u0440\u0446' in x else '\u0442\u0440\u0446' if '\u0442\u0440\u0446' in x else '\u0442\u0440\u043a' if '\u0442\u0440\u043a' in x else '\u0442\u0446' if '\u0442\u0446' in x else '\u0442\u043a' if '\u0442\u043a' in x else 'unkown')\nshops.head()","122b2bb4":"print('Shops number:', shops['shop_id'].nunique())\nprint('Shop names number:', shops['shop_name'].nunique())\nprint('Shop cities number:', shops['shop_city'].nunique())\nprint('Shop types number:', shops['shop_type'].nunique())","3e38d392":"sales.loc[sales['shop_id'] == 11, 'shop_id'] = 10\nshops.loc[shops['shop_id'] == 11, 'shop_id'] = 10\n\nsales.loc[sales['shop_id'] == 23, 'shop_id'] = 24\nshops.loc[shops['shop_id'] == 23, 'shop_id'] = 24\n\nsales.loc[sales['shop_id'] == 0, 'shop_id'] = 57\nshops.loc[shops['shop_id'] == 0, 'shop_id'] = 57\n\nsales.loc[sales['shop_id'] == 1, 'shop_id'] = 58\nshops.loc[shops['shop_id'] == 1, 'shop_id'] = 58\n\nsales.loc[sales['shop_id'] == 40, 'shop_id'] = 39\nshops.loc[shops['shop_id'] == 40, 'shop_id'] = 39\n\nshops = shops.drop_duplicates(subset = 'shop_id')","7bf31b75":"print('Shops number:', shops['shop_id'].nunique())\nprint('Shop names number:', shops['shop_name'].nunique())\nprint('Shop cities number:', shops['shop_city'].nunique())\nprint('Shop types number:', shops['shop_type'].nunique())","d864f526":"def clean_names(df, cols):\n    for col in cols:\n        df[col] = df[col].str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ').str.lower()\n        df[col] = df[col].str.strip()\n        df.loc[df[col] == '', col] = 'unknown'","acf857d0":"items['item_name'], items['item_type'] = items['item_name'].str.split('[', 1).str\nitems['item_name'], items['item_subtype'] = items['item_name'].str.split('(', 1).str\nclean_names(items, ['item_name', 'item_type', 'item_subtype'])\nitems = items.fillna('unkown')\nitems.head()\n\n\n","4dacae0c":"print('Number of items:', items['item_id'].nunique())\nprint('Number of item_name:', items['item_name'].nunique())\nprint('Number of item_type:', items['item_type'].nunique())\nprint('Number of item_subtype:', items['item_subtype'].nunique())","b08e655d":"sales = sales.merge(items[['item_id', 'item_category_id']], on = 'item_id', how = 'left')","abe2500b":"time_shift = 14","b97eb53f":"pairs_sales = create_record_for_features(sales, ['shop_id', 'item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)","c723cb7c":"shops_items_sales_temp = pairs_sales.sample(1000)#.nlargest(10**4, columns = [str(itr) for itr in range(27, 33)])\npairs_acf = np.zeros((shops_items_sales_temp.shape[0], time_shift + 1))\nfor i, (ind, shop_item_sales) in enumerate(shops_items_sales_temp.iterrows()):\n    pair = shop_item_sales.loc['0': ]\n    acf_12 = acf(pair, nlags = time_shift, fft = True)\n    pairs_acf[i, :] = acf_12","9a47789a":"avgs = np.mean(pairs_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of shop\/item pairs')\nplt.show()","c1f8776d":"pair_lags = [1, 2, 3, 4, 8, 10, 11, 12]","b03a1e73":"del shops_items_sales_temp\ndel pairs_sales\ndel shop_item_sales\ndel pair\ndel acf_12\ndel pairs_acf\ngc.collect()","1ce12c29":"items_sales = create_record_for_features(sales, ['item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)","cd0741c2":"items_acf = np.zeros((items_sales.shape[0], time_shift + 1))\nfor i, item_sales in items_sales.iterrows():\n    item_temp = item_sales.loc['0': ]\n    if np.sum(item_temp) != 0:\n        acf_12 = acf(item_temp, nlags = time_shift, fft = True)\n        items_acf[i, :] = acf_12","05fd32d4":"avgs = np.mean(items_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of items')\nplt.show()","79345210":"item_lags = [1, 2, 3, 4, 5, 10, 11, 12]","675b527d":"del items_sales\ndel item_sales\ndel items_acf\ndel item_temp\ndel acf_12\ndel avgs\ngc.collect()","9408e997":"shops_sales = create_record_for_features(sales, ['shop_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)\nshops_acf = np.zeros((shops_sales.shape[0], time_shift + 1))\nfor i, shop_sales in shops_sales.iterrows():\n    shop_temp = shop_sales.loc['0': ]\n    acf_12 = acf(shop_temp, nlags = time_shift, fft = True)\n    shops_acf[i, :] = acf_12","59e413b7":"avgs = np.mean(shops_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of shops')\nplt.show()","a9d3a090":"shop_lags = [1, 2, 3, 4, 7, 8, 10, 12]","f288c42a":"del shops_sales\ndel shop_sales\ndel shops_acf\ndel shop_temp\ndel acf_12\ndel avgs\ngc.collect()","f64852f3":"categories_sales = create_record_for_features(sales, ['item_category_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)\ncategories_acf = np.zeros((categories_sales.shape[0], time_shift + 1))\nfor i, category_sales in categories_sales.iterrows():\n    category_temp = category_sales.loc['0': ]\n    acf_12 = acf(category_temp, nlags = time_shift, fft = True)\n    categories_acf[i, :] = acf_12","61703cd9":"avgs = np.mean(categories_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of categories')\nplt.show()\n","cfeeb659":"category_lags = [1, 2, 3, 4, 5, 6, 12]","f34660c2":"del categories_sales\ndel category_sales\ndel categories_acf\ndel category_temp\ndel acf_12\ndel avgs\ngc.collect()","1e2ca84f":"index_cols = ['shop_id', 'item_id', 'date_block_num'] \n\ndef create_train_set(df, index_cols = index_cols):\n    grid = []\n    for month in df['date_block_num'].unique():\n        curr_shops = df[df['date_block_num'] == month]['shop_id'].unique()\n        curr_items = df[df['date_block_num'] == month]['item_id'].unique()\n        grid.append(np.array(list(product(*[curr_shops, curr_items, [month]])), dtype = 'int32'))\n\n    grid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype = np.int32)\n\n    gb = df.groupby(index_cols, as_index = False).agg({'item_cnt_day':'sum'})\n    gb.columns = index_cols + ['target']\n    all_data = pd.merge(grid, gb, how = 'left', on = index_cols).fillna(0)\n    all_data.sort_values(['date_block_num','shop_id','item_id'], inplace = True)\n    all_data.loc[:, 'target'] = all_data['target'].clip(0, 20).astype(np.float32)\n    print('Sales data shape:', df.shape)\n    print('Generated Train data shape:', all_data.shape)\n    del curr_shops\n    del curr_items\n    del gb\n    del grid\n    gc.collect()\n    return all_data","87696ba3":"start = time.time()\ngrid = create_train_set(sales, index_cols)\ntest['date_block_num'] = 34\ngrid = pd.concat([grid, test[['item_id', 'shop_id', 'date_block_num']]], ignore_index = True, sort = False, keys = index_cols)\ngrid = grid.merge(items[['item_id', 'item_category_id']], on = 'item_id', how = 'left')\nprint(round(time.time() - start), 'seconds')","71798172":"def lag_features(df, features, go_back_in_time):\n    for month_shift in go_back_in_time:\n        df_shift = df[index_cols + features].copy()\n        df_shift['date_block_num'] = df_shift['date_block_num'] + month_shift\n        lag_cols = lambda x: '{}_lag_{}'.format(x, month_shift) if x in features else x\n        df_shift = df_shift.rename(columns = lag_cols)\n        df = pd.merge(df, df_shift, on = index_cols, how='left')\n    return df\n\ndef fast_lag_features(df, features, go_back_in_time): \n    features_sales = create_record_for_features(sales, features, 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)\n    for month in go_back_in_time:\n        max_month = df.date_block_num.max()\n        cols = [str(itr) for itr in np.arange(0, max_month)]\n        gb = features_sales.melt( id_vars = features, \n                                 var_name = 'date_block_num' , \n                                 value_vars= cols, \n                                 value_name = 'target_' + '_'.join(features) + '_lag_' + str(month)\n                                )\n        gb.date_block_num = gb.date_block_num.astype(np.int16)\n        gb.date_block_num = gb.date_block_num + month\n        df = pd.merge(df, gb, on = features + ['date_block_num'], how='left')\n    return df","49674e41":"start = time.time()\ngrid = lag_features(grid, ['target'], pair_lags)\nprint(round(time.time() - start), 'seconds')","076f407e":"start = time.time()\ngrid = fast_lag_features(grid, ['item_id'], item_lags)\nprint(round(time.time() - start), 'seconds')","f7f0b576":"start = time.time()\ngrid = fast_lag_features(grid, ['shop_id'], shop_lags)\nprint(round(time.time() - start), 'seconds')","cde21407":"start = time.time()\ngrid = fast_lag_features(grid, ['item_category_id'], category_lags)\nprint(round(time.time() - start), 'seconds')","f5735588":"grid.isnull().sum()","ad1f10cc":"grid = grid[grid['date_block_num'] > 11]\ngrid = grid.fillna(0)","01cc47af":"grid = downcast_dtypes(grid)","be8bdad2":"dates_train = sales.loc[:, ['date', 'date_block_num']].drop_duplicates()\ndates_train = dates_train.reset_index(drop = True)\ndates_test = dates_train.loc[dates_train.loc[:, 'date_block_num'] == 34 - 12]\ndates_test = dates_test.reset_index(drop=True)\ndates_test.loc[:,'date_block_num'] = 34\ndates_test.loc[:, 'date'] = dates_test.loc[:, 'date'] + pd.DateOffset(years = 1)\ndates_all = pd.concat([dates_train, dates_test])\ndates_all.loc[:, 'dow'] = dates_all.loc[:, 'date'].dt.dayofweek\ndates_all.loc[:, 'year'] = dates_all.loc[:, 'date'].dt.year\ndates_all.loc[:, 'month'] = dates_all.loc[:, 'date'].dt.month\n\ndates_all = pd.get_dummies(dates_all, columns = ['dow'])\ndow_col = ['dow_' + str(x) for x in range(7)]\ndate_features = dates_all.groupby(['year', 'month', 'date_block_num'])[dow_col].agg('sum').reset_index()\ndate_features.loc[:, 'days_of_month'] = date_features.loc[:, dow_col].sum(axis=1)\ndate_features.loc[:, 'year'] = date_features.loc[:, 'year'] - 2013\ndate_features = date_features.loc[:, ['month', 'date_block_num']]","7b94d207":"grid = grid.merge(date_features, on = 'date_block_num', how = 'left')","ba137252":"del dates_train\ndel dates_test\ndel dates_all\ndel dow_col\ndel date_features\ngc.collect()","fee0fe8d":"grid['category_shop_inter'] = grid['item_category_id'].astype(str) + '_' + grid['shop_id'].astype(str)\ngrid.loc[:, 'category_shop_inter'] = LabelEncoder().fit_transform(grid.loc[:, 'category_shop_inter'].values)","14f83678":"grid = downcast_dtypes(grid)","75446094":"cols_to_drop = ['target', 'date_block_num']\ndef train_val_test_split(df):\n    dates = df['date_block_num']\n    last_block = dates.max()\n    print('Test `date_block_num` is %d' % last_block)\n    print('Validation `date_block_num` is %d' % (last_block - 1))\n    print('Train `date_block_num` is < %d' % (last_block - 1))\n    print('------------------------------')\n\n    X_train = df.loc[dates < last_block - 1].drop(cols_to_drop, axis = 1)\n    X_val = df.loc[dates == last_block - 1].drop(cols_to_drop, axis = 1)\n    X_test =  df.loc[dates == last_block].drop(cols_to_drop, axis = 1)\n\n    y_train = df.loc[dates < last_block - 1, 'target'].values\n    y_val =  df.loc[dates == last_block - 1, 'target'].values\n    \n    print('X_train shape: ', X_train.shape)\n    print('y_train shape: ', y_train.shape)\n    print('------------------------------')\n    print('X_val shape: ', X_val.shape)\n    print('y_val shape: ', y_val.shape)\n    print('------------------------------')\n    print('X_test shape: ', X_test.shape)\n    print('------------------------------')\n    return (X_train, y_train, X_val, y_val, X_test)\n\ndef rmse(y, y_hat):\n    return np.sqrt(mean_squared_error(y, y_hat))\n\ndef create_lgbm_model(X_train, y_train, X_val, y_val, params, cat_feats):\n    n_estimators = 8000\n    d_train = lgb.Dataset(X_train, y_train)\n    d_valid = lgb.Dataset(X_val, y_val)\n    watchlist = [d_train, d_valid]\n    evals_result = {}\n    model = lgb.train(params, \n                      d_train, \n                      n_estimators,\n                      valid_sets = watchlist, \n                      evals_result = evals_result, \n                      early_stopping_rounds = 50,\n                      verbose_eval = 0,\n                      categorical_feature = cat_feats,\n                    )\n    lgb.plot_metric(evals_result)\n    return model\n\ndef evaluate_model(model, X_train, y_train, X_val, y_val): \n    y_hat = model.predict(X_train)\n    print('Training error;', rmse(y_train, y_hat))\n    y_val_hat = model.predict(X_val)\n    print('Validation error:', rmse(y_val, y_val_hat))","0600a8b1":"categorical_features = ['shop_id', 'item_category_id', 'month', 'category_shop_inter']\nfor col in categorical_features:\n    grid.loc[:, col] = grid[col].astype('category')","25b7a609":"X_train, y_train, X_val, y_val, X_test = train_val_test_split(grid)","dad170b2":"start = time.time()\nparams = {\n  'metric': 'rmse',\n  'objective': 'mse',\n  'verbose': 0, \n  'learning_rate': 0.1,\n  'num_leaves': 31,\n  'min_data_in_leaf': 20 ,\n  'max_depth': -1,\n  'save_binary': True,\n  'bagging_fraction': 0.8,\n  'bagging_freq': 1,\n  'bagging_seed': 2**7, \n  'feature_fraction': 0.8,\n}\nlgbm_model = create_lgbm_model(X_train, y_train, X_val, y_val, params, categorical_features)\nprint('it tooks: ', round(time.time() - start), 'seconds')","71c8d8ee":"start = time.time()\nevaluate_model(lgbm_model, X_train, y_train, X_val, y_val)\nprint('it tooks: ', round(time.time() - start), 'seconds')","6b65cfae":"ax = lgb.plot_importance(lgbm_model, max_num_features = 40, figsize = (8, 10))\nplt.show()","d37d0542":"y_test_pred = lgbm_model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.ID, \n    \"item_cnt_month\": y_test_pred\n})\nsubmission.to_csv('lgbm_submission.csv', index = False)","19a3b8d5":"submission.item_cnt_month.hist()","deeaeb46":"## Sales history","4f207900":"# Modeling\n\n","ae44b80a":"### shop\/item pair sales history","63883b3b":"## Date Features","4597f1f6":"## Create training grid","a9911f99":"### How much to go back in time for shops","f3fa89b0":"## train\/val split","039ed229":"## Test set distribution","21d7b0c4":"## item_price outliers","62d95443":"## Convert sales.date form string to datetime","80ac3ebe":"**We will clip all sales amount for shops\/items to [0, 20] when we constract the train set.**","22bcaba3":"## Add lag feature to grid","5f496729":"In order to create lag features, we need to determine how much the previous values of the features affect the prediction of the current one.\n\nI will use Autocorrelation function **(acf)** to perfom this analysis.","7736221b":"## Remove duplicated shops\n\nThe shops below have similar names","6059ba54":"# EDA","c9f78c28":"### item sales history","6b729084":"# Data cleaning","ed189415":"### How much to go back in time for categories","54fc415f":"## Concatenate test set to the grid","fa66abd0":"### Category\/Shop","186e18fe":"## Shop name preprocessing\n\nShop name contains the following info(https:\/\/www.kaggle.com\/kyakovlev):\n\ncity | shop_type | shop_name","5bd9d939":"### shop sales history","5477a9ea":"# Load data","9d8a6387":"## Delete duplicated records","35e7acd9":"## Evaluate model","c74bd1c8":"### Summary\nAs we see, We don't have full sale history for items, shops and item\/shop pairs during 2 years and 10 months. For example\n\n- for the first year 38.54 % of the item\/shop pairs have made no sales\n- for the last year 57.91 % of the item\/shop pairs have made no sales\n\nThe absence of sales historical data will make the prediction is a challenging task.\n\nWe have 214200 pairs of shop\/item that we want to predict their future sales for the month 34. The problem is modeled as:\n\ntarget_pair1(t = 0), target_pair1(t = 1), target_pair1(t = 2), ..., target_pair1(t = k) >>>*predict*>>> target_pair1(t = 34).\n\ntarget_pair2(t = 0), target_pair2(t = 1), target_pair2(t = 2), ..., target_pair2(t = k) >>>*predict*>>> target_pair2(t = 34)\n\ntarget_pair_m(t = 0), target_pair_m(t = 1), target_pair_m(t = 2), ..., target_pair_m(t = k) >>>*predict*>>> target_pair_m(t = 34)\n\n**Note**\n\nThe amount of selling items is dependant on the time and the shop as well, see the following plot:","64e8e527":"## Lag features","cfb00326":"### How much to go back in time for items","f3761be8":"# Feature Engineering","55706fcb":"## Train a model","2d40b0a3":"## item_cnt_day outliers","f6de5160":"### How much to go back in time for pairs(shop\/item)\nI will choose 1000 pairs and compute how much the previous values of the series (lags) may be helpful in predicting the current value.","d16aefce":"**Introduction**\n\nOur mission in this project is to find a mapping between **a(t)** and **a(t - k)**, where ***a*** is the amount of selling each item in each shop and ***t*** is time (months). In other words we want to find a series A where n = A(n-1). \n\n**Table of contents**\n- Loading and downcasting data\n- Exploratory data analysis\n    * Pair plot\n    * Outliers removal\n    * Duplicated sales removal\n    * Sales history\n    * Test set distribution\n- Cleaning data\n    * Shops names preprocessing\n    * Duplicated shops removal\n    * Items names preprocessing\n- Feature extraction\n    * [Lag features](https:\/\/machinelearningmastery.com\/basic-feature-engineering-time-series-data-python\/)\n    * Date features\n    * Feature interactions\n- Modeling\n    * X_train, X_val splitting\n    * Training LightGBM model\n    * Evaluate model","c49c6257":"# Submission","7327cc0f":"## Pair plot for sales\nThis is an interesting plot because:\n- it shows the histogram (the diagonal) for each columns\n- it shows if there is outliers in each columns\n- it draws each columns with respect to other columns","9e82433b":"## Feature Interaction","c605bbff":"## Item_name preprocessing\nitem_name contains the name, type, and subtype in following format (https:\/\/www.kaggle.com\/kyakovlev):\n    \n- item_name [item_type] (item_subtype)."}}