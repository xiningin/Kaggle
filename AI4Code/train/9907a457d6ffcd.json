{"cell_type":{"ba23fa96":"code","b4e1a08e":"code","4123f2b1":"code","0f97f54c":"code","33de6bd6":"code","01943e2f":"code","051ee229":"code","36bd00fc":"code","ab882b9f":"code","03cfbb09":"code","eb0c9a1e":"code","fb9cc487":"code","6ab22c17":"code","590b428c":"code","d829a06e":"code","e9e8049b":"code","f13d628e":"code","bc4a9911":"code","6e4f49a3":"code","4ff3eca9":"code","014151fd":"code","88668722":"code","68fb3011":"code","216e8088":"code","5c267880":"code","65968572":"code","36014cf2":"code","86ab1331":"code","c4765b4a":"code","9e6aa4fb":"code","9ee7d54e":"code","d4c85a43":"code","9346c6de":"code","81b47156":"code","b995cf55":"code","d4240874":"code","553f570e":"code","21872476":"code","6ebcbd31":"code","5852baae":"code","767d3875":"code","40701d73":"code","ac69a4dc":"code","637c2a98":"code","3769f758":"code","bb0b89e0":"code","650f3529":"code","015e4515":"code","41b689e5":"code","b72db55c":"code","0284a0af":"code","35f46283":"code","7202da85":"code","aff0f905":"code","aa1ee8cc":"code","144ff6b1":"code","af196eef":"code","8fd37b33":"code","7c089863":"code","9c0f0b42":"code","a8236a61":"code","132f31a7":"code","145e61e3":"code","eb48206b":"code","b529797b":"code","bdb2f1ee":"code","0127abfb":"code","f8f42bf6":"code","d625d16b":"code","0883aa99":"code","194cb713":"code","11ae6d7d":"code","bc4d2d52":"code","627c9684":"code","d04bf8cd":"code","9d9cfb86":"code","8657f68a":"code","c8c6ebb5":"markdown","1e910768":"markdown","94c12d02":"markdown","9029c969":"markdown","c9313698":"markdown","70cdd28a":"markdown","5a69d667":"markdown","95f3f5e4":"markdown","b7a2af5f":"markdown","b8cbf433":"markdown","7034d769":"markdown","31c6e1bc":"markdown","41673f90":"markdown","4705c43e":"markdown","ac206efe":"markdown","d86f8b49":"markdown","cb3de2f7":"markdown","29870c90":"markdown","a5eec282":"markdown","a687328c":"markdown","7ce38804":"markdown","73bdceac":"markdown","d1a89899":"markdown","5d43d481":"markdown","693bb6c4":"markdown","2b4a178b":"markdown","06cc605d":"markdown","0121386d":"markdown","b02c9545":"markdown","3994ec8e":"markdown","3815d109":"markdown","374bfbd6":"markdown","3b947204":"markdown","5e792fdb":"markdown","c8b3fac4":"markdown","31a3f6a5":"markdown","d5608056":"markdown","8d7cc61f":"markdown","52aede59":"markdown","130f177e":"markdown","ad29b766":"markdown","1b1d8e29":"markdown","230d8d87":"markdown","365cc0c5":"markdown","1682dc5f":"markdown","dc3f258b":"markdown","5fa08b5c":"markdown","c27136d9":"markdown","42bf8c25":"markdown","1ef71ab0":"markdown","a4d6ba08":"markdown","177b5885":"markdown","8a220fec":"markdown","39209873":"markdown","41ebd618":"markdown","221bb46c":"markdown","2063cb9e":"markdown","3dafe317":"markdown","27c379ea":"markdown","a14aee35":"markdown","5ccff2e6":"markdown","97a38461":"markdown","326574ba":"markdown","7b8480f5":"markdown","27f42cc5":"markdown","a9290fa0":"markdown","e3244eac":"markdown","3e12dece":"markdown","b58fb2a5":"markdown","6018f6e0":"markdown","6e348991":"markdown","9d5efbd3":"markdown","42e18976":"markdown","9ddbf39e":"markdown","0c63bd44":"markdown","e5dfdba8":"markdown","5e8b0926":"markdown","8adc8e92":"markdown","76b1dc6f":"markdown","59789153":"markdown","350fe840":"markdown","c3e25d8b":"markdown"},"source":{"ba23fa96":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom IPython.display import Image, display\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV\nimport re\nimport math","b4e1a08e":"# read the data in \ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ncombined = train.append(test, sort=False)\n\nprint('Training set:')\nprint('Number of samples: {}'.format(train.shape[0]))\nprint('Number of variables: {}'.format(train.shape[1]))\nprint('Variables: {}\\n'.format(train.columns.tolist()))\nprint('Testing set:')\nprint('Number of samples: {}'.format(test.shape[0]))\nprint('Number of variables: {}'.format(test.shape[1]))\nprint('Variables: {}\\n'.format(test.columns.tolist()))","4123f2b1":"train.info()","0f97f54c":"test.info()","33de6bd6":"train.isnull().sum()","01943e2f":"test.isnull().sum()","051ee229":"train.head(5)","36bd00fc":"display(Image('https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/5d\/Titanic_side_plan_annotated_English.png'))","ab882b9f":"combined.nunique()","03cfbb09":"combined.Cabin.value_counts().head()","eb0c9a1e":"combined.loc[(combined.Parch == 0) & (combined.SibSp == 0), 'Ticket'].value_counts().head()","fb9cc487":"combined.Name.value_counts()[combined.Name.value_counts() > 1]","6ab22c17":"combined.loc[(combined.Name == 'Connolly, Miss. Kate') | (combined.Name == 'Kelly, Mr. James'),:].sort_values('Name')","590b428c":"# extract prefix from Name\ncombined['prefix'] = combined.Name.str.extract(r'(\\, .*?\\.)')\ncombined['prefix'] = combined.prefix.str.replace('\\, ', '')\n\nfem_prefix = ['Miss.', 'Mrs.', 'Mme.', 'Mlle.']\nmale_prefix = ['Mr.', 'Mister.', 'Master.']\n\ninconsistent_index = []\nfor i, val in combined.iterrows():\n    if val['prefix'] in fem_prefix:\n        if val['Sex'] == 'Male':\n            inconsistent_index.append(i)\n    elif val['prefix'] in male_prefix:\n        if val['Sex'] == 'Female':\n            inconsistent_index.append(i)\n\nprint('Number of Prefix and Sex inconsistency: {}'.format(len(inconsistent_index)))","d829a06e":"combined[(combined.Parch > 2) & (combined.Age <= 17)]","e9e8049b":"combined.loc[(combined.Parch > 2) & (combined.Age <= 17), ['SibSp', 'Parch']] = [3, 1]","f13d628e":"Fords = ['Ford, Mr. William Neal', 'Ford, Miss. Robina Maggie \"Ruby\"', 'Ford, Miss. Doolina Margaret \"Daisy\"', 'Ford, Mrs. Edward (Margaret Ann Watson)', 'Ford, Mr. Edward Watson']\n\ncombined.loc[combined.Name.isin(Fords)]","bc4a9911":"sibs = ['Ford, Miss. Robina Maggie \"Ruby\"', 'Ford, Miss. Doolina Margaret \"Daisy\"', 'Ford, Mr. Edward Watson']\n\ncombined.loc[combined.Name.isin(sibs), ['SibSp', 'Parch']] = [3, 1]","6e4f49a3":"combined.Age.describe()","4ff3eca9":"combined.Fare.describe()","014151fd":"combined[combined.Fare == 0]","88668722":"combined[['Survived', 'Pclass', 'Name', 'Fare', 'Cabin', 'Ticket']].sort_values('Fare', ascending = False).head(20)","68fb3011":"# extract prefix from Name\ncombined['prefix'] = combined.Name.str.extract(r'(\\, .*?\\.)')\ncombined['prefix'] = combined.prefix.str.replace('\\, ', '')\n\ncombined.prefix.value_counts()","216e8088":"prefix_dict = {\n    'Capt.' : 'Officer',\n    'Col.' : 'Officer',\n    'Don.' : 'Royalty',\n    'Dona.' : 'Royalty',\n    'Dr.' : 'Officer',\n    'Jonkheer.' : 'Royalty',\n    'Lady.' : 'Royalty',\n    'Major.' : 'Officer',\n    'Master.' : 'Master',\n    'Miss.' : 'Miss',\n    'Mlle.' : 'Miss',\n    'Mme.' : 'Miss',\n    'Mr.' : 'Mr',\n    'Mrs.' : 'Mrs',\n    'Ms.' : 'Mrs',\n    'Rev.' : 'Officer',\n    'Sir.' : 'Royalty',\n    'the Countess.' : 'Royalty'\n}\n\ncombined.prefix = combined.prefix.apply(lambda row: prefix_dict[row])","5c267880":"combined.prefix.value_counts()","65968572":"# Deck of cabin\n# 'M' represents missing\ncombined.Cabin = combined.Cabin.apply(lambda row: row[0] if pd.notnull(row) else 'M')\n\ncombined.Cabin.value_counts()","36014cf2":"combined['logFare'] = np.log10(combined.Fare + 1)","86ab1331":"combined['FamilySize'] = combined.Parch + combined.SibSp","c4765b4a":"combined['noFamily'] = combined.apply(lambda row: 1 if row['FamilySize'] == 0 else 0, axis=1)","9e6aa4fb":"# split into train\/test with new variable\ntrain = combined.iloc[:891,:]\ntest = combined.iloc[891:,:]","9ee7d54e":"tickets = set()\nfor ticket in train.Ticket:\n    tickets.add(ticket)\n\nind = 0\nfor ticket in test.Ticket:\n    if ticket in tickets:\n        ind += 1\n    else:\n        tickets.add(ticket)\n\nnonfam_tickets = set()\nind_2 = 0\nfor ticket in train.loc[train.noFamily == 1, 'Ticket']:\n    nonfam_tickets.add(ticket)\n\nfor ticket in test.loc[test.noFamily == 1, 'Ticket']:\n    if ticket in nonfam_tickets:\n        ind_2 += 1\n    else:\n        nonfam_tickets.add(ticket)\n        \nprint('{} group tickets out of {} tickets show up in both training and testing set, \\n among them, {} were from families and {} were not.'.format(ind, len(tickets), ind - ind_2, ind_2))","d4c85a43":"print('Survival rate in training set: {}'.format(round(train.Survived.mean(), 2)))\nsns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nsns.countplot(train.Survived)","9346c6de":"# 'M' represents missing\ncombined.Cabin = combined.Cabin.apply(lambda row: row[0] if pd.notnull(row) else 'M')\n\n# T only consists of 1 passenger and is from class1\n# replace cabin T with C cause C has the most people from class 1\ncombined.loc[combined.Cabin == 'T', 'Cabin'] = 'C'","81b47156":"cate_variables = ['Sex', 'Embarked', 'Pclass', 'Parch', 'SibSp', 'FamilySize', 'Cabin']\n\ndef cate_countplot(data, var_list, cols=4, width=16, height=8, hspace=0.3, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    sns.set(font_scale=1.5)\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(len(var_list) \/ cols)\n    \n    for i, var in enumerate(var_list):\n        ax = fig.add_subplot(rows, cols, i+1)\n        sns.countplot(var, data=data)\n        plt.xlabel(var, weight='bold')\n        \ncate_countplot(data=combined, var_list=cate_variables)","b995cf55":"def create_bars(data, variables, cols=4, width=16, height=8, hspace=0.3, wspace=0.5):\n    sns.set(font_scale=1.5)\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(len(variables) \/ cols)\n\n    for i, column in enumerate(data[variables].columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        sns.barplot(column,'Survived', data=data.sort_values(column))\n        plt.xticks(rotation=0)\n        plt.xlabel(column, weight='bold')\n\ncreate_bars(combined, cate_variables)","d4240874":"# survival rate on different features\n# numerical variables\nnum_variables = ['Age', 'Fare', 'logFare']\n\n# you are encouraged to play around with the bins and see the effect on plots\nbins = [range(0, 81, 1), range(0, 300, 4), np.arange(0, 1.7, 0.02)]\ndef create_hists(data, variables, bins = bins, cols=3, width=20, height=6, hspace=0.5, wspace=0.5):\n    sns.set(font_scale=1.5)\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(wspace=wspace, hspace=hspace)\n    rows = math.ceil(len(variables) \/ cols)\n    \n    survived = data[data['Survived'] == 1]\n    passed = data[data['Survived'] == 0]\n\n    for i, column in enumerate(data[variables].columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        sns.distplot(survived[column].dropna(), bins = bins[i], kde = False, color = 'blue')\n        sns.distplot(passed[column].dropna(), bins = bins[i], kde = False, color = 'red')\n        plt.xticks(rotation=0)\n        plt.xlabel(column, weight='bold')\n        plt.legend(['Survived', 'Deceased'])\n        \ncreate_hists(combined, num_variables)","553f570e":"variables_list = ['Survived', 'Age', 'Fare', 'logFare', 'Parch', 'SibSp', 'FamilySize']\n\nplt.figure(figsize=(10, 10))\nsns.heatmap(combined.iloc[:891,][variables_list].corr(), annot=True, fmt='.2f', square=True)","21872476":"sns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(14, 4))\nfig.subplots_adjust(wspace=0.3, hspace=0.5)\nax = fig.add_subplot(1, 2, 1)\nsns.countplot(combined.Pclass, hue=combined.Sex)\nplt.title('Sex Count by Pclass ')\n\nax = fig.add_subplot(1, 2, 2)\nsns.barplot(x=train.Pclass, y=train.Survived, hue=train.Sex)\nplt.legend(title = 'Sex', loc='upper right')\nt = plt.title('Survival Rate by Pclass & Sex')","6ebcbd31":"class_list = [1, 2, 3]\ndef age_dists_by_class(data, class_list, cols=3, width=16, height=6, hspace=0.5, wspace=0.25):\n    sns.set(font_scale=1.5)\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(wspace=wspace, hspace=hspace)\n    rows = math.ceil(len(class_list) \/ cols)\n    \n    colors = ['blue', 'green', 'pink']\n    bins = range(0, 80, 5)\n    for i, cls in enumerate(class_list):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        sns.distplot(data[(data.Age.notnull()) & (data.Pclass == cls)].Age, bins=bins, color=colors[i])\n        plt.axvline(data[data.Pclass == cls].Age.mean(), color = 'red', label='Mean')\n        plt.axvline(data[data.Pclass == cls].Age.median(), color = 'blue', label='Median')\n        plt.legend(loc='upper right')\n        plt.title('Class {}'.format(str(cls)))\n\nage_dists_by_class(combined, class_list)","5852baae":"sns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(6, 4))\nsns.kdeplot(combined.loc[combined.Sex == 'male', 'Age'].dropna(), color='blue', label = 'male',shade=True)\nsns.kdeplot(combined.loc[combined.Sex == 'female', 'Age'].dropna(), color='red', label = 'female', shade=True)\nplt.xlabel('Age', weight='bold')\nt = plt.title('Age by Sex')","767d3875":"# sex_age_vs_class\nsex_list = ['male', 'female']\ndef sex_age_by_class(data, sex_list, cols=3, width=18, height=10, hspace=0.5, wspace=0.25):\n    sns.set(font_scale=1.5)\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(wspace=wspace, hspace=hspace)\n    rows = math.ceil(len(sex_list) * 2 \/ cols)\n    \n    survived = data[data.Survived == 1]\n    passed = data[data.Survived == 0]\n    \n    ind = 1\n    for i, sex in enumerate(sex_list):\n        for j, cls in enumerate([1, 2, 3]):\n            ax = fig.add_subplot(rows, cols, ind)\n            sns.kdeplot(survived.loc[(survived.Pclass == cls) & (survived.Sex == sex), 'Age'].dropna(), color='blue', shade=True)\n            sns.kdeplot(passed.loc[(passed.Pclass == cls) & (passed.Sex == sex), 'Age'].dropna(), color='red', shade=True)\n            plt.legend(['Survived', 'Passed'], loc='upper right')\n            plt.xlabel('Age', weight='bold')\n            plt.title('Class {} - {}'.format(str(cls), sex))\n            ind += 1\n        \nsex_age_by_class(combined, sex_list)","40701d73":"sex_list = ['female', 'male']\ndef fare_dists_by_sex(data, sex_list, cols=3, width=16, height=4, hspace=0.5, wspace=0.25):\n    sns.set(font_scale=1.5)\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(wspace=wspace, hspace=hspace)\n    rows = math.ceil(len(sex_list) \/ cols)\n    \n    colors = ['blue', 'pink']\n    bins = range(0, 400, 40)\n    for i, sex in enumerate(sex_list):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        sns.distplot(data[(data.Fare.notnull()) & (data.Sex == sex)].Fare, bins=bins, color = colors[i])\n#         plt.axvline(data[data.Sex == sex].Fare.mean(), color = 'red', label='Mean')\n#         plt.axvline(data[data.Sex == sex].Fare.median(), color = 'blue', label='Median')\n#         plt.legend(loc='upper right')\n        plt.title(str(sex))\n\nfare_dists_by_sex(combined, sex_list)","ac69a4dc":"class_list = [1, 2, 3]\ndef fare_dists_by_class(data, class_list, cols=3, width=16, height=4, hspace=0.5, wspace=0.25):\n    sns.set(font_scale=1.5)\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(wspace=wspace, hspace=hspace)\n    rows = math.ceil(len(class_list) \/ cols)\n    \n    colors = ['blue', 'green', 'pink']\n    \n    for i, cls in enumerate(class_list):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        sns.distplot(data[(data.Fare.notnull()) & (data.Pclass == cls)].Fare, color = colors[i])\n        plt.axvline(data[data.Pclass == cls].Fare.mean(), color = 'red', label='Mean')\n        plt.axvline(data[data.Pclass == cls].Fare.median(), color = 'blue', label='Median')\n        plt.legend(loc='upper right')\n        plt.title('Class {}'.format(str(cls)))\n\nfare_dists_by_class(combined, class_list)","637c2a98":"class_list = [1, 2, 3]\ndef class_violins(data, class_list, cols=3, width=16, height=6, hspace=0.5, wspace=0.25):\n    sns.set(font_scale=1.5)\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(wspace=wspace, hspace=hspace)\n    rows = math.ceil(len(class_list) \/ cols)\n\n    for i, cls in enumerate(class_list):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        sns.violinplot(x='Survived', y='Fare', data=combined[combined.Pclass == cls], palette='Set1')\n        plt.title('Class {}'.format(str(cls)))\n        ax.set_xticklabels(['Deceased','Survived'])\n        plt.xlabel('')\n\nclass_violins(combined, class_list)","3769f758":"class_list = [1, 2, 3]\ndef class_violins_by_sex(data, class_list, cols=3, width=16, height=8, hspace=0.5, wspace=0.25):\n    sns.set(font_scale=1.5)\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(wspace=wspace, hspace=hspace)\n    rows = math.ceil(len(class_list) \/ cols)\n\n    for i, cls in enumerate(class_list):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax = sns.violinplot(x='Survived', y='Fare', hue='Sex', data=combined[combined.Pclass == cls].sort_values('Sex'))\n        plt.legend(loc='upper left')\n        ax.set_xticklabels(['Deceased','Survived'])\n        plt.title('Class {}'.format(str(cls)))\n        plt.xlabel('')\n\nclass_violins_by_sex(combined, class_list)","bb0b89e0":"sns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(6, 4))\ntable = pd.crosstab(combined.Embarked, combined.Pclass)\ntable = pd.crosstab(combined.Embarked, combined.Pclass)\ntable = table.div(table.sum(axis=1), axis=0)\ntable.plot(kind=\"bar\", stacked=True)\nplt.xlabel('Embarked')\nplt.ylabel('Fraction of Total')\nplt.legend(title='Pclass', loc='lower right', bbox_to_anchor=(1.25, 0))","650f3529":"sns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(6, 4))\ntable = pd.crosstab(combined.Embarked, combined.Pclass)\nsns.countplot(combined.Embarked, hue=combined.Sex)","015e4515":"sns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(6, 4))\nsns.barplot(x='Cabin', y='Survived', data=combined.sort_values(by=['Cabin']))","41b689e5":"#cabin by class\n\ntable = pd.crosstab(combined.Cabin, combined.Pclass)\nprint(table)\ntable = table.div(table.sum(axis=1), axis=0)\nsns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(6, 4))\ntable.plot(kind='bar', stacked=True)\nplt.ylabel('Fraction of Total')\nplt.legend(title='Pclass', loc='lower right', bbox_to_anchor=(1.25, 0))","b72db55c":"# cabin by sex\ntable = pd.crosstab(combined.Cabin, combined.Sex)\nprint(table)\ntable = table.div(table.sum(axis=1), axis=0)\nsns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(6, 4))\ntable.plot(kind='bar', stacked=True)\nplt.xticks(rotation=0)\nplt.legend(loc='lower right', bbox_to_anchor=(1.4, 0))","0284a0af":"table = pd.crosstab(combined.Pclass, combined.FamilySize)\nprint(table)\nsns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nsns.pointplot(x='FamilySize', y='Survived', hue='Pclass', data=combined)","35f46283":"sns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nsns.pointplot(x='FamilySize', y='Survived', hue='Sex', data=combined)","7202da85":"# sex_age_vs_class\ndef survival_by_class_sex(data, cols=3, width=18, height=4, hspace=0.5, wspace=0.25):\n    sns.set(font_scale=1.5)\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(wspace=wspace, hspace=hspace)\n    rows = 2\n\n    for i, cls in enumerate([1, 2, 3]):\n        ax = fig.add_subplot(1, 3, i+1)\n        sns.pointplot(x='FamilySize', y='Survived', hue='Sex', data=data[data.Pclass == cls].sort_values('Sex'))\n        if i == 2:\n            plt.legend(loc='upper right')\n        else:\n            plt.legend(loc='lower right')\n        plt.title('Class {}'.format(str(cls)))\n        \nsurvival_by_class_sex(combined, sex_list)","aff0f905":"combined[combined.Fare.isnull()]","aa1ee8cc":"F_fill = train[(train.Pclass == 3) & (train.noFamily == 1)].Fare.mean()\ncombined.loc[combined.Fare.isnull(), 'Fare'] = F_fill\ncombined.loc[combined.logFare.isnull(), 'logFare'] = np.log(F_fill)","144ff6b1":"combined[combined.Embarked.isnull()]","af196eef":"combined.loc[combined.Embarked.isnull(), 'Embarked'] = train[train.Pclass == 3].Embarked.mode()","8fd37b33":"prefix_list = ['Master', 'Miss', 'Mr', 'Mrs', 'Officer', 'Royalty']\ndef hists_by_prefix(data, prefix_list, cols=3, width=15, height=8, hspace=0.5, wspace=0.25):\n    sns.set(font_scale=1.5)\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width, height))\n    fig.subplots_adjust(wspace=wspace, hspace=hspace)\n    rows = math.ceil(len(prefix_list) \/ cols)\n    \n    colors = ['blue', 'green', 'yellow', 'pink', 'orange', 'grey']\n    \n    for i, prefix in enumerate(prefix_list):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        sns.distplot(data[(data.Age.notnull()) & (data.prefix == prefix)].Age, color = colors[i])\n        plt.axvline(data[data.prefix == prefix].Age.mean(), color = 'blue')\n        plt.axvline(data[data.prefix == prefix].Age.median(), color = 'red')\n        plt.title(prefix)\n\nhists_by_prefix(combined, prefix_list)","7c089863":"# dict of mean for each prefix group\nprefix_mean = train.groupby('prefix').Age.mean().to_dict()\n\n# impute missing age\ncombined.Age = combined.groupby('prefix').Age.transform(lambda x: x.fillna(x.mean()))","9c0f0b42":"# get_dummies\ncombined_cate_var = pd.get_dummies(combined[['Cabin', 'Embarked', 'prefix']])\n\n# include Pclass\ncombined_cate_var['Pclass'] = combined.Pclass\ncombined_cate_var['noFamily'] = combined.noFamily\ncombined_cate_var['Sex'] = combined.Sex.map({'male':1, 'female':0})\n\ncombined_cate_var.columns","a8236a61":"num_list = ['Age', 'SibSp', 'Parch', 'logFare', 'FamilySize']\n\ncombined_num = combined[num_list].copy()\n\ncombined_num[num_list] = StandardScaler().fit_transform(combined_num[num_list].values)","132f31a7":"# combined features\ncombined_processed = pd.concat([combined_cate_var, combined_num], axis=1)\n\ncombined_processed.head(5)","145e61e3":"# train test split \nx_train = combined_processed.iloc[:891,:]\ny_train = combined.iloc[:891,:].Survived\n\nx_test = combined_processed.iloc[891:,:]","eb48206b":"# define model\nbaseline_model = RandomForestClassifier(random_state=4, n_estimators=100, min_samples_split=15, oob_score=True)\n\n# fit\nbaseline_model.fit(x_train, y_train)\n\n# feature importance\nfeature_importances = pd.DataFrame({'Importance Score': baseline_model.feature_importances_}, index=x_train.columns).sort_values(by='Importance Score', ascending=False)\n\n# top 10 features by importance\nfeature_importances[:10]","b529797b":"rfe = RFE(estimator=baseline_model, n_features_to_select=1, step=1)\nrfe.fit(x_train, y_train)","bdb2f1ee":"importance = pd.DataFrame()\nimportance['features'] = x_train.columns.values\nimportance['ranking'] = rfe.ranking_\nimportance = importance.sort_values('ranking')\n\ncv_result = pd.DataFrame()\ncv_result['num_features'] = range(1, 11, 1)\ncv_result['training_score'] = 0\ncv_result['testing_score'] = 0\ncv_result['testing_std'] = 0\n\nfor i in range(1, 11, 1):\n    # top features\n    features = importance['features'].iloc[:i]\n    \n    # compute training and CV accuracy\n    cv_score = cross_validate(baseline_model, x_train[features], y_train, cv=10, return_train_score=True)\n    cv_result.loc[i-1, 'training_score'] = cv_score['train_score'].mean()\n    cv_result.loc[i-1, 'testing_score'] = cv_score['test_score'].mean()\n    cv_result.loc[i-1, 'testing_std'] = cv_score['test_score'].std()","0127abfb":"sns.set(font_scale=1.5)\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(8, 6))\nsns.lineplot(cv_result['num_features'], cv_result['training_score'], marker=\"o\", label='Training')\nsns.lineplot(cv_result['num_features'], cv_result['testing_score'], marker=\"^\", label='Testing')\nt = plt.xticks(range(0, 11, 1))\nplt.xlabel('Number of Features')\nplt.ylabel('CV Accuracy')\nplt.legend()","f8f42bf6":"importance['features'].iloc[:5]","d625d16b":"tree_param_grid = {'max_depth': [10, 20, 30, 40],\n 'min_samples_split': [2, 5, 10, 15, 20, 25],\n 'n_estimators': [50, 100, 150, 200]}\n\ngrid = GridSearchCV(RandomForestClassifier(), param_grid=tree_param_grid, cv=10, n_jobs=-1)\ngrid.fit(x_train[importance['features'].iloc[:5]], y_train)","0883aa99":"print('Best CV score {} was achieved with:'.format(round(grid.best_score_, 4)))\nparams = grid.best_estimator_.get_params()\nprint('max_depth: {}'.format(str(params['max_depth'])))\nprint('min_samples_split: {}'.format(str(params['min_samples_split'])))\nprint('n_estimators: {}'.format(str(params['n_estimators'])))","194cb713":"model = RandomForestClassifier(**params)\nmodel.fit(x_train[importance['features'].iloc[:6]], y_train)\nprediction = model.predict(x_test[importance['features'].iloc[:6]]).astype(int)\nsubmission = pd.DataFrame(test.PassengerId.copy())\nsubmission['Survived'] = pd.Series(prediction)\nsubmission.to_csv('submission.csv', index = False)","11ae6d7d":"train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\n\n# get mean of passenger's age from training set\nage_mean = train.Age.mean()\n\n# replace in training set\ntrain.Age = train.apply(lambda row: age_mean if pd.isnull(row['Age']) else row['Age'], axis=1)\n\n# same for testing set\ntest.Age = test.apply(lambda row: age_mean if pd.isnull(row['Age']) else row['Age'], axis=1)","bc4d2d52":"# define the model\n# same model that we use before\nbaseline_model = RandomForestClassifier(random_state=4, n_estimators=100, min_samples_split=15, oob_score=True)\n\n# separate training and target variables\nX = train[['Sex', 'Age', 'Pclass']].copy()\nY = train['Survived']\n\n# encode Sex \n# 0 for female, 1 for male\nX.Sex = X.apply(lambda row: 0 if row['Sex'] == 'female' else 1, axis=1)\n\n# fit\nbaseline_model.fit(X, Y)\n\nprint('OOB score of baseline model: {}'.format(round(baseline_model.oob_score_, 3)))","627c9684":"# data processing on test set\ntest_X = test[['Sex', 'Age', 'Pclass']].copy()\ntest_X.Sex = test_X.apply(lambda row: 0 if row['Sex'] == 'female' else 1, axis=1)\n\n# make prediction on test data\nbaseline_prediction = baseline_model.predict(test_X)\n\n# produce csv file for submission\nbaseline_submission = pd.DataFrame(test.PassengerId.copy())\nbaseline_submission['Survived'] = pd.Series(baseline_prediction)\nbaseline_submission.to_csv('baseline_submission.csv', index = False)","d04bf8cd":"train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\ncombined = train.append(test, sort=False)","9d9cfb86":"class_mean = train.groupby('Pclass').Age.mean().to_dict()\n\ntrain.Age = train.groupby('Pclass').Age.transform(lambda x: x.fillna(x.mean()))\ntest.Age = test.apply(lambda row: class_mean[row['Pclass']], axis=1)\n\nX = train[['Sex', 'Age', 'Pclass']].copy()\nY = train['Survived']\n\n# encode Sex \n# 0 for female, 1 for male\nX.Sex = X.apply(lambda row: 0 if row['Sex'] == 'female' else 1, axis=1)\n\n# fit\nbaseline_model.fit(X, Y)\n\nprint('OOB score of baseline model: {}'.format(round(baseline_model.oob_score_, 3)))\n\n# data processing on test set\ntest_X = test[['Sex', 'Age', 'Pclass']].copy()\ntest_X.Sex = test_X.apply(lambda row: 0 if row['Sex'] == 'female' else 1, axis=1)\n\n# make prediction on test data\nbaseline_prediction = baseline_model.predict(test_X)\n\n# produce csv file for submission\nbaseline_submission = pd.DataFrame(test.PassengerId.copy())\nbaseline_submission['Survived'] = pd.Series(baseline_prediction)\nbaseline_submission.to_csv('age_by_class_submission.csv', index = False)","8657f68a":"baseline_model.fit(x_train[['Age', 'Sex', 'Pclass']], y_train)\nprediction = baseline_model.predict(x_test[['Age', 'Sex', 'Pclass']]).astype(int)\nsubmission = pd.DataFrame(test.PassengerId.copy())\nsubmission['Survived'] = pd.Series(prediction)\nsubmission.to_csv('age_by_prefix_submission.csv', index = False)","c8c6ebb5":"# 0. Introduction\n\n[Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic\/overview) is a competition held by well-known data science community [Kaggle](https:\/\/www.kaggle.com\/).\n\n*The sinking of the Titanic is one of the most infamous shipwrecks in history.*\n\n*On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.*\n\n*While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.*\n\n*In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).*\n\nIn this legendary competition, contestants are asked to predict the survival of passengers on Titanic, this is a educational competition with no dead line and serves as a great starting point for learning data science\/machine learning. Being the first and my favorite data science project, once a while when I achieved new skills, learned new models, I come back to this competition and apply them. \n\nRecently, I visited the discussion section of this competition and found many people novice to the field suffering on how to improve their prediction accuracy of their models. Now a lot of questions like this were answered by people throwing out combination of features, or how to combine\/transform features with a focus on prediction accuracy. Which was exactly what I followed when I started machine learning.\n\nIn this context, the importance of EDA is rarely mentioned. My belief is, with good EDA, you will gain comprehensive understanding of the data, and with those knowledge about the data, you build good models. Therefore, I'm wirting this article and hopefully this serves as a good tutorial of EDA and introduction to how EDA can help you improve the performance of your machine learning models.","1e910768":"# 5. Example Model Building\n\nIn this section, I will only focus on random forest model and explore different features combination bacause the purpose of this article is to show how a good, in-depth EDA could help you build success model.  \n  \nNine out fo ten times, I choose random forest as my model to go if I want to get a sense of how my features are doing, or when I am in need of a baseline model for examining whether the steps I took help in prediction. \n\nRandom forest is good in this context for three reasons:\n- **It non-parametric**, meaning that we make no assumption about the structure of the data. For example, when using linear models, you assume there's a linear relationship between your predictors and target variable, but when the true relationship is different than your assumption, you'll probably get low scores. Random forest is free of that problem.  \n- **It gives feautre importance scores**, this gives you a clue of what features are more useful, this is useful when you have many features to select from. Be careful, this importance does not apply to different models, it's only meaninigful to the random forest model with the set of parameters that generated the importance scores. \n- **It handles missing data**. The methods it took may not be appropriate for every case, but it's useful to get a quick sense of your data.  \n\nNow that build a model that explore all the features.","94c12d02":"With these features, let's search for the best paramters using grid-search CV","9029c969":"## 1.3 A Peek at the Data\n","c9313698":"Summary:\n- **Sex**: Female yielded higher chance to survive than male  \n- **Pclass**: Not surprisingly, passengers in higher class were more likely to survive  \n- **FamilySize**: Survival rate rises with FamilySize at 0 to 3, it encounters a sharp drop at FamilySize = 4 and continues to drop at FamilySize = 5, which too us by surprise  \n- **Cabin**: Passengers missing Cabin information have lower chance to survive in general\n- **Cabin**: Less than 50% of passengers in Cabin A, G survivied, while passengers in Cabin B, D, E have roughly the same survival rate","70cdd28a":"Turns out the numbers are all wrong for siblings of William, let's correct them.  \nBe noted that Mrs Margaret Ann Watson Ford is actually travelling with a sibling [Elizabeth Johnston](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/eliza-johnston.html).","5a69d667":"It was shown earlier that survival rates for Embarked are C > Q > S, this could be explained by the plots above: \n- Higher proportion of 1st class passengers accounts for higher chance of surviving if embarked at C\n- Even though passengers embarked at port Q mostly belonged to 3rd class, it had a fairly balance sex ratio, accounts for higher survival chance than port S\n- Large proportion of male embarked at port S lowered the chance of survival\n\nThese findings lead to conclusion that Embarked may not provides extra information other than Sex and Pclass.","95f3f5e4":"# Conclusion\n\nWhat does this result tell us?\n\nA person that didn't do EDA would probably impute missing values with mean and get a score of 0.74.  \nA person that did a litte EDA would probably impute missing values with Pclass mean and get a score of 0.76.  \nA person that did more EDA would probably impute missing values with prefix mean and get a score of 0.775.  \n  \nMoreover, with only three features we found from EDA, and without all the works including feature selection, cross-validation, we get the same score with the example model!  \n\nOf course, there are lots of randomness involved in this result and it's not guarantee you'll always get good results without all the works involved in model building stage. However, spending time doing EDA is always rewarding, you'll get more understanding of your data, know what feauture is worth being considered, and know more about how features interact with each other, and thus shorten the amount of time you spend on trying combinations of features exhaustively. Moreover, building a baseline model with important features you found from doing EDA, and compare that with the overall modelling result, one can get a clearer sense of how well the model is preforming.\n","b7a2af5f":"We know the two passengers missing Embarked information were travelling together because they share the same ticket number. It makes sense to impute the missing Embarked by the most common port in their Pclass\/Cabin.","b8cbf433":"### logFare  \n\nFare was taken log with base 10 to produce logFare, taking log of skewed continuous variables reduces the dynamic range, makes large difference in the variable comparable meanwhile preserving the differences between the values. Note that Fare was added by 1 beofore taken log, this is because there are 0s in Fare which goes to negative infinity, which is not desired. Adding 1 has little effect to the log values while preventing the value to explode.\n\nWe'll see wheter this is a good transformation later.","7034d769":"### Parch Size \n\nAccroding to the definition given by Kaggle, Parch is the total number of parents + children on board, it's fair to assume that for passenger under age of 17, this number should always be less than 2.","31c6e1bc":"# 2.7 A Closer Look at Each Variable and How They Affect Survival","41673f90":"### Data Leakage - Why I chose not to calculate SplitFare\n\nSplitFare for families can be easily calculated even if family members span across training and testing data set, because family size information was given for every passenger.\n\nWhat about those 23 non-family groups? I've seen a lot of kernel on Kaggle which combine training and testing set, group passengers by ticket, then divide Fare by group size. **This should never happen in real world machine learning problem**, more precisely, you cannot make it happens.  \n\nThe reason why we use a testing set to evaluate model accuracy is to see how well the model perform on data it hasn't seen before, if any information from the testing set enter the training set(in this case the ticket information), it's considered data leakage. By doing so, you're forcing you're model to adapt for your testing set(even the slightest tweak), or in other words, you model is over-fitting on your testing set. \n\nAnother reason why avoiding data leakage is important, you would not be able to obtain the same information if given another set of testing data. Imagine your model is used to predict survival for a unseen passenger that's not in the testing set, how do you now whether he\/she was travelling in group that's not formed by family members?\n\nYes, I know accuracy matters the most in Kaggle, and people use information from testing set to boost their scores, but since Titanic is usually a starting project for biginers and Kaggle new-comvers, it's important to bring out this concept to them and spread awareness that this should not happen in production.  \n\nFor more information on data leakage, refer to [this page](https:\/\/www.kaggle.com\/alexisbcook\/data-leakage).","4705c43e":"Kaggle submission score: 0.77511","ac206efe":"Let's select up to 10 features using [Recursive Feature Elimination (RFE)](https:\/\/machinelearningmastery.com\/rfe-feature-selection-in-python\/).","d86f8b49":"# 4. Data Processing \n\nTransforming data into appropriate format is crucial for model building.  \nFirst, for categorical features like Pclass, Sex, Title, mapping each level of the features into an integer is required for most of the machine learning algorithm to work with. Two common ways of encoding categorical features are:  \n1. **Integer Encoding**: This is mapping each level of the feautre to an integer, for example, male = 1, female = 0. This encoding method may be sufficient when there are natural ordering between each level in your feature, like Pclass in our case.  \n\n2. **One-hot encoding**: Probably the most common encoding method, it's widely used when the levels in a categorical features have no ordinal relationship, like Sex. This method would map each level of the feature to a newly-created binary features, and discard the original feature. Be careful when applying one hot-encoding when there are many level in your feature, adding many binary features to your data set is generally not a good idea, this is called the [curse of dimensionality](https:\/\/towardsdatascience.com\/the-curse-of-dimensionality-50dc6e49aa1e). In that case, binning some levels of your feature may be a good idea. \n","cb3de2f7":"Summary: \n- **Age**: Children under the age of 15 yielded a significant higher chance to survive. \n- **Age**: People in their middle age are more likely to perish\n- **Age**: Seems like people in their 50s have higher chance to survive, but those older are more likely to perished\n- **Fare**: Lower fare gives lower survival rate, the effect of fare on survival rate is vague when it comes to a higher level of fare\n- **Fare**: Nearly all of those who did not pay perished, this might be telling us that they should be excludes from the model","29870c90":"### Fare","a5eec282":"### Between Pclass, Sex and Age","a687328c":"There is only one such passenger, a little research on the [Encyclopedia](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/neal-thomas-ford.html) shows there were 4 Fords travelling together:\n- [Miss Dollina Margaret Ford](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/dollina-margaret-ford.html) (Sister)\n- [Mr Edward Watson Ford](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/edward-watson-ford.html) (Brother)\n- [Miss Robina Maggie Ford](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/robina-maggie-ford.html) (Sister)\n- [Mrs Margaret Ann Watson Ford](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/margaret-ann-watson-ford.html) (Mother)\n\nSo it appears that the values of SibSp and Parch is swapped, let's correct that.","7ce38804":"### Prefix and Sex","73bdceac":"# Key Findings from EDA:\n\n- **Age**, **Sex**, **Pclass** are the most important features\n- **Age**: **Female** owned a better chance to survive than **male**\n- **Fare**: For 1st class passengers, those paid a higher fare(or not travelling alone) had a better chance to survive. For 2nd and 3rd class **male** passengers, those who paid the highest fees(or travelling with large group of people) tended to perished\n- **Embarked** seems to not provide extra predictive power to whether a passenger survive on top of Sex and Pclass, same for **Cabin**\n- **FamilySize**: Having FamilySize >= 4 decreases the chance for surviving, this applies to all Pclass and both Sex\n- **FamilySize**: The trend of 'larger family tends to survive' disappears when data is separated by Sex and Pclass. Implying the trend we observed earlier is only a cross effect of Sex Pclass.","d1a89899":"## 2.2 Distinct Values in each Variables","5d43d481":"Second, standardization of numerical features is recommended in most of the case, this is transforming your features to have mean of 0 and stadard deviation of 1, in other words, allowing your feautures to be on the same scales. This is useful because if you're having, say people's hieght and weight in your list of features, with height in feet and weight in grams. You'll most likely to see height ranging from 5 to 7 feet and weights from 40000 to 100000 grams. In such case, the weight feature is likely to introduce larger errors comparing to height, so your choice of algorithm would focus more on reducing the errors from weight. Bringing numerical features onto the same scale assure equal weight of your features on errors. ","693bb6c4":"# 1. Describe the Data","2b4a178b":"Let's further investigate these values for the orther Fords:","06cc605d":"### Is Cabin a Good Variable?","0121386d":"### SplitFare\n\nI want to spend some paragraphs to talk about wheter we can calculate splitFare for passengers that travel together. Earlier we saw that there were non-family passengers who travel together, let's see how many of them were separated by training and testing set.","b02c9545":"The distribution of **Age** in both **Sex** are nearly identical, the predictive power of these two variables are distinguishable from each other.","3994ec8e":"It was shown earlier that having a family of size greater than 4 is disadvantageous:\n- This applies to 1st and 3rd class passengers, the effect on 2nd class passengers is unknown because of small sample size\n- This also applies to both male and female\n- The trend that larger groups under 4 people yield higher trend to survive disappears when looking at each Sex in each Pclass\n\nThis implies that adding a binary variable FamilySize > 4 to the model may helps.","3815d109":"Summary:  \n- **Fare** and **logFare** both have positive correlaiton with Survivied, but **logFare** is higher, which may imply that **logFare** is a better predictor than **Fare**\n- Negative correlations betweem **Age** and **Parch**, **SibSp**, and **FamilySize** are observed. Since larger families are more likely to carry children with them so this make sense\n- Sharing ticket and cumulative fare lead to positive correlation between **Fare**\/**logFare** with **Familysize** ","374bfbd6":"[The Titanic Encyclopedia](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/kate-connolly-2.html) tells us there were indeed two Kate Connolly(one should be Catherine), who boarded Titanic as a third class passenger, however the age of the one holding Ticket 330972 is shown to be 41 on the Encyclopedia, not sure which record is correct so I'll leave it at 30.  \n\nThere were also two Kelly James who boarded as a third class passenger, though the ages on the website do not align with the result too. ","3b947204":"Summary: \n- Earlier we found that paying a higher fare may increase the chance of survival, this rule looks like it only applies to 1st class.\n- In 2nd class, male passengers with the highest Fare tended to perish, while top-paying female tended to survive. This could be viewed as large group of male passengers perished together while female travelling in groups survived together.\n- Without calculating split fare for each passengers, the effect of Fare on survival rate is vague","5e792fdb":"Summary:\n- Age is serving as a pretty good descriminative variable for Survived within each Sex of each Pclass, especially for first class female and second class male\n- Being in younger age is advantageous for every group excepy first class female, this is mainly because of the lack of female passenger under 15 in 1st class","c8b3fac4":"### Embarked","31a3f6a5":"### Prefix \nIn previous section we see Name variable contains prefix information, that could be one useful features to add.  ","d5608056":"Summary: \n- Number of male passengers were nearly the double of female passengers  \n- Most people embarked at Southampton, much more than the other two ports. Note that Titanic set sail from Southamton, then head to Cherbourg and Queenstown consecutively to pick up passengers  \n- There were more third class passengers than first and second class. Second class and first class passengers were roughly the same size  \n- Most passengers travelled without companion of family  \n- Most values in Cabin were missing, we'll look at this further in next section","8d7cc61f":"**male** paid lower Fare on average, this could be justified by the fact that most male were 3rd class passengers while 1st class consisted of more female.","52aede59":"### noFamily\n\nCreate a binary variable indicates wheter the passenger travel with family members","130f177e":"# 2. Exploratory Data Analysis","ad29b766":"Matching our expectation, higher class passengers paid higher Fare on average. The distribution of Fare within each class is skewed, so log transformation might be a good idea","1b1d8e29":"Summary:\n- 1st class passengers are older on average, most people were at 20 - 65\n- 2nd and 3rd class have less elder people but more children\n- Young adults domimnated 2nd and 3rd class","230d8d87":"# 3. Missing Values Imputation","365cc0c5":"Kaggle submission score: 0.77511","1682dc5f":"### How Each Variable Affects Survival","dc3f258b":"Noticed that these passengers with Fare = 0 all embarked at Southampton, which is where Titanic set sail. A little a Googling tells us:  \n-  Some of them are Titanic workers\n-  Some of them are the people who runs the company of Titinic and their relatives, for example Joseph Bruce Ismay is the chairman of the company  \n\nSo it's safe to assume these 0s are legit, they are people who actually got a free ride. However, the survival rate of crew members is likely to be significant different than passengers, so we can consider leaving these people out of our model.","5fa08b5c":"# 6. Simple Model with Findings from EDA\n\nLet's build a simple model with only the three most important features we found by EDA: **Pclass, Sex, Age** and see how it performs.  \n\nWith these features, let's also experiment, see how missing values imputation of Age affect model result.\n\n### 1. Impute with mean","c27136d9":"Titanic problem is a **binary classification** problem, you're classifying passenger into two group, survived and perished. It's crucial that you identify the ratio of two groups in the training set for this kind of problem.  \n\nBut Why?  \n\nImaging in your training set, 99% of passengers survived, then by simply predict all passenger to survive, you'll get 99% of training accuracy. We call this **imbalanced classes**. A lot of machine learning problem suffers from imbalanced data in natural, for example detecting frauds from bank transactions, naturally there would be only a few frauds among a million of transactions. There're many ways to tackle imbalance in the data. See more [here](https:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/).  \n\nSo we have about 38% of passengers survived in the training set, which is good because we know the true survival rate for passengers is 38%. We don't need to worry about imbalanced class in this problem.","42bf8c25":"## 2.5 Feature Engineering\n*Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques.*.  - Wikipedia  \n\nBy the definition above, feature engineering should be a dynamic process and ideas of new features come up as we get more familar with the data. Normally this process is involved in EDA, but I decided to put feature engineering as a stand-alone section in order to keep the stucture of this article straightforward. ","1ef71ab0":"### FamilySize\n\nIt's intuitive to combine parch, sibling, and spouse to create a FamiliSize variable.","a4d6ba08":"So including 5 features give us the highest testing CV accuracy, these features are:","177b5885":"# 2.6 Survival Rate in Training Set","8a220fec":"## 2.1 Backgroud Knowledge  \n\nDomain knowledge is crucial for any type of data science problem, though we might no have been a Titanic expert, we have Google. A little bit of background research tells us: \n\n- Around 2200 were on board, approximately 215 of 900 crew members survived, and 500 out out of 1300 passengers survived. About 38% passengers survived, this number may help us evaluate our model.\n- The iceberg strucked the front of Titanic, water entered the ship from the front of the ship and eventually the ship broke in half. \n- According to Wikipedia: *'The thoroughness of the muster was heavily dependent on the class of the passengers; the first-class stewards were in charge of only a few cabins, while those responsible for the second- and third-class passengers had to manage large numbers of people.'*  \n- There was a 'women and children first' order to the crew members, which was misunderstood as 'women and children only' by the Second Officer. This may thinner the chance of survival for male passengers. \n- Immigrants(mostly 3rd-class passengers) were blocked from 1st and 2nd-class area bt the U.S. immigration law at that time. They need to travel more distance to the lifeboat deck.  \n\nFrom these facts, **Pclass, Age and Sex** seem to be the most important features determining survival or not.","39209873":"Beside families travelling together, there are many group of passengers boarded with one ticket. This may lead to difficulty calculating split fare for each members if people in one group is separated in training and testing set.  \n  \nMore details about this would be addressed in feature engineering section. \n","41ebd618":"### Is Cabin a Good Variable?\n\n#### Survival Rate Revisit","221bb46c":"Fit full model and make prediction","2063cb9e":"## 2.4 Identifying Outliers\n\n### Age\n\nLooks Fine.","3dafe317":"### Between Fare, Sex and Pclass","27c379ea":"Summary: \n- Pclass 3 was dominant by male, while there were roughly same size of male and female in Pclass 1\n- In every class, male passengers yielded significant lower chance to survive than female\n- Earlier we discovered that higher class passengers have a higher chance to survive, this pattern applies to both male and female passengers","a14aee35":"Some rarely seen prefixes were combined together, the variable was reduced to have 6 distinct values, they were distributed as follows:","5ccff2e6":"### Between FamilySize, Pclass and Sex","97a38461":"Summary: \n- There are duplicates in Name, which may implies there're duplicate records for one person, we'll check that later.\n- Duplicates in Ticket implies there are people in groups who boarded with one ticket. Further investigation tells us the Fare is cumulative for group tickets, we may need to split the fare for those.  \n- There are 186 distinct values for Cabin, which is much larger than the actual number of Cabin on Titanic, we'll handle that right after.  \n\n\n#### Cabin","326574ba":"### Important Note about Cabin\n\n- Missing vlaues in Cabin were replace by 'M' \n- One first class passenger was in Cabin 'T', not sure what it means. Since the information I gathered on the Internet shows no presence of Cabin T, that value was replaced by C cause C has the most people from class 1","7b8480f5":"### 3. Impute with prefix mean","27f42cc5":"### 2. Impute with Pclass mean","a9290fa0":"Train test split so we preserve the variables we added.","e3244eac":"# 2.3 Spotting Data Inconsistency\n\n### Name\nEarlier we saw there're passengers who share same name, let's examine are those duplicate entries or not.","3e12dece":"Findings from the info:\n\n- Object type:  \n**Name, Sex, Cabin, Embarked** are object type, they're most likey to be strings.  \n**PassengerId, Pclass, SibSp, Parch** are integers.  \n**Age and Fare** are floating point numbers.  \n<br>\n- Missing Values are found in both training and testing set:  \nIn **training set**, **Age, Cabin, Embarked** contain missing values.  \nIn **testing set**, **Age, Fare, Cabin** contain missing values.  \nWe'll impute these missing values later.\n<br>\n- Variable Cabin has more than 80% of values missing, which may lead to difficulty imputing them.\n\n","b58fb2a5":"There was one passenger missing Fare, turns out he's a 3rd class passenger travelling without family members. Let's impute the value by the mean Fare price of 3rd class passengers travelling alone.","6018f6e0":"The value of Cabin is formed as Cabin + berth or suite number. The effect of berth\/suite number on the chance of survuval is nearly impossible to find out. For simplicity we'll focus on Cabin and discard berth\/suite number.  \n  \n#### Ticket","6e348991":"The plot shows location of each cabin, area in green are damaged parts caused by collision with the iceberg.  \n*[Source: Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Sinking_of_the_Titanic)*","9d5efbd3":"Summary:\n- Low survival rate in Cabin A can be explained by large proportion of male in it\n- Cabin B to E were almost dominated by 1st class passengers, they also had a balance sex ratio, thus lead to higher chance to survive\n- Passengers missing Cabin information were mainly 3rd class passengers, a higher proportion of male passengers can also be observed, these lead to the low survival rate\n\nWe may conclude here that, same as Embarked, Cabin provides little to no extra information other than Sex and Pclass.","42e18976":"### Fare \n\nIt's not clear why for some passengers fare price is 0. We'll investigate this.","9ddbf39e":"## 1.2 Closer Look at the Variables\n\nTraining set contains 891 samples and 12 variables:\n\n- **PassengerId**: unique index to identify each passenger, no actual effect on predicting target variable\n- **Survived**: target variable, boolean, with 1 indicates the passenger survived and 0 on the contrast\n- **Pclass**: a proxy for socio-economic status (SES) of the passenger, contains 3 unique values:  \n1 = Upper  \n2 = Middle  \n3 = Lower  \n- **Name**: contains prefix, for example Braund, Mr. Owen Harris\n- **Sex**: male and female\n- **Age**: passenger's age, fractional if less than 1. If the age is estimated, is it in the form of xx.5\n- **SibSp**: total number of the passengers' siblings and spouse  \n    Sibling = brother, sister, stepbrother, stepsister  \n    Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)  \n- **Parch**: total number of the passengers' parents and children  \n    Parent = mother, father  \n    Child = daughter, son, stepdaughter, stepson  \n    Some children travelled only with a nanny, therefore parch=0 for them.  \n- **Ticket**: the passenger's ticket number\n- **Fare**: passenger fare\n- **Cabin**: cabin number of the passenger\n- **Embarked**: port the passenger embarked, there are 3 ports (C, Q or S):  \nC = Cherbourg\nQ = Queenstown\nS = Southampton","0c63bd44":"# 2.8 Exploring Relationship between Variables","e5dfdba8":"### Age\n\nAge is the trickiest features to handle in this part. Missing values account for about 20% of this features, we also concluded that Age is one of the most important features, so the chosen method used to fill in those value would impact the model result directly.  \n\nWe know that children under 16 yielded a greater chance of survival, so it would be nice if we can spot whether the passenger missing Age information is under that age. Luckily, the title Master, belongs only to children under 16, so let's fill missing Age of Masters with the mean of known Masters age. We'll also use the age mean within each prefix to replace missing values for other passengers.","5e8b0926":"Kaggle submission score: 0.76555","8adc8e92":"### Cabin  \n\nIn order to get a brighter sense of how cabin imapcts survival and interacts with other variables, missing values were coded 'M'.  ","76b1dc6f":"Kaggle submission score: 0.74401","59789153":"Sorted by fare price, we see the top 4 values of fare belongs to 4 groups of 1st-class passengers, therefore the high price makes sense.","350fe840":"## 1.1 Data Shape\n\nA training set and testing set are provided.\n\n- Training set contains 891 samples and 12 variables  \n- Test set contains 418 samples, 11 variables (no target variable)","c3e25d8b":"### Frequency of Values in Each Variable"}}