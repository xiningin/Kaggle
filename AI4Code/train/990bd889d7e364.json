{"cell_type":{"01986984":"code","aef0799a":"code","3f3d781c":"code","309e3520":"code","686892de":"code","d1ac03c7":"code","30286d28":"code","ec75cf18":"code","d0fa8118":"code","48c729e2":"code","e5058404":"code","28eb7661":"code","18f805e2":"code","288045d0":"code","ba899ba6":"code","7e740f6b":"code","eff91e13":"code","0f5953ec":"code","82751d3a":"code","c09d2074":"code","78961b88":"markdown","643a7c18":"markdown","fc233cb1":"markdown","54289382":"markdown","16198bcb":"markdown","ef5d31a5":"markdown","bad60808":"markdown"},"source":{"01986984":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nimport re\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport tensorflow as tf","aef0799a":"train_df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin-1')\ntest_df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='latin-1')","3f3d781c":"train_df","309e3520":"test_df","686892de":"train_df.info()","d1ac03c7":"test_df.info()","30286d28":"train_inputs = train_df['OriginalTweet'].copy()\ntest_inputs = test_df['OriginalTweet'].copy()\n\ntrain_labels = train_df['Sentiment'].copy()\ntest_labels = test_df['Sentiment'].copy()","ec75cf18":"sentiment_encoding = {\n    'Extremely Negative': 0,\n    'Negative': 0,\n    'Neutral': 1,\n    'Positive': 2,\n    'Extremely Positive': 2\n}\n\ntrain_labels = train_labels.replace(sentiment_encoding)\ntest_labels = test_labels.replace(sentiment_encoding)","d0fa8118":"train_inputs","48c729e2":"stop_words = stopwords.words('english')\n\ndef process_tweet(tweet):\n    \n    # remove urls\n    tweet = re.sub(r'http\\S+', ' ', tweet)\n    \n    # remove html tags\n    tweet = re.sub(r'<.*?>', ' ', tweet)\n    \n    # remove digits\n    tweet = re.sub(r'\\d+', ' ', tweet)\n    \n    # remove hashtags\n    tweet = re.sub(r'#\\w+', ' ', tweet)\n    \n    # remove mentions\n    tweet = re.sub(r'@\\w+', ' ', tweet)\n    \n    #removing stop words\n    tweet = tweet.split()\n    tweet = \" \".join([word for word in tweet if not word in stop_words])\n    \n    return tweet\n\n# Function taken from @Shahraiz's wonderful notebook","e5058404":"train_inputs = train_inputs.apply(process_tweet)\ntest_inputs = test_inputs.apply(process_tweet)","28eb7661":"train_inputs","18f805e2":"max_seq_length = np.max(train_inputs.apply(lambda tweet: len(tweet)))","288045d0":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_inputs)\n\nvocab_length = len(tokenizer.word_index) + 1\n\n\ntrain_inputs = tokenizer.texts_to_sequences(train_inputs)\ntest_inputs = tokenizer.texts_to_sequences(test_inputs)\n\ntrain_inputs = pad_sequences(train_inputs, maxlen=max_seq_length, padding='post')\ntest_inputs = pad_sequences(test_inputs, maxlen=max_seq_length, padding='post')","ba899ba6":"print(\"Vocab length:\", vocab_length)\nprint(\"Max sequence length:\", max_seq_length)","7e740f6b":"train_inputs.shape","eff91e13":"embedding_dim = 16\n\n\ninputs = tf.keras.Input(shape=(max_seq_length,), name='input_layer')\n\nembedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=embedding_dim,\n    input_length=max_seq_length,\n    name='word_embedding'\n)(inputs)\n\ngru_layer = tf.keras.layers.Bidirectional(\n    tf.keras.layers.GRU(units=256, return_sequences=True, name='gru_layer'),\n    name='bidirectional_layer'\n)(embedding)\n\nmax_pooling = tf.keras.layers.GlobalMaxPool1D(name='max_pooling')(gru_layer)\n\ndropout_1 = tf.keras.layers.Dropout(0.4, name='dropout_1')(max_pooling)\n\ndense = tf.keras.layers.Dense(64, activation='relu', name='dense')(dropout_1)\n\ndropout_2 = tf.keras.layers.Dropout(0.4, name='dropout_2')(dense)\n\noutputs = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(dropout_2)\n\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nprint(model.summary())\n\ntf.keras.utils.plot_model(model)","0f5953ec":"model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\nbatch_size = 32\nepochs = 2\n\nhistory = model.fit(\n    train_inputs,\n    train_labels,\n    validation_split=0.12,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=2\n)","82751d3a":"fig = px.line(\n    history.history,\n    y=['loss', 'val_loss'],\n    labels={'index': \"epoch\", 'value': \"loss\"}\n)\n\nfig.show()","c09d2074":"model.evaluate(test_inputs, test_labels)","78961b88":"# Preprocessing","643a7c18":"# Results","fc233cb1":"# Training","54289382":"# Task for Today  \n\n***\n\n## COVID-19 Tweet Sentiment Prediction  \n\nGiven *tweets about the COVID-19 pandemic*, let's try to predict the **sentiment** of a given tweet.  \n  \nWe will use a TensorFlow RNN to make our predictions.","16198bcb":"# Getting Started","ef5d31a5":"# Modeling","bad60808":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/-jPZvbG4M2c"}}