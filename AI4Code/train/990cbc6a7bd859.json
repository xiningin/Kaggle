{"cell_type":{"4e6e7164":"code","bee050ca":"code","db34dd36":"code","d8291872":"code","a8dee286":"code","19d0faed":"code","4bc7353c":"code","3cd22b24":"code","0db6d017":"code","1f2a28a8":"code","d37fe0db":"code","d66fd095":"code","6f67b765":"code","9e59afe7":"code","263bf982":"code","15e8ebc1":"code","063f88c4":"code","9e793560":"code","523accfa":"code","0cdad7fe":"code","53adecee":"code","6cda456b":"code","be59e5a8":"code","262df996":"code","2b6bbcb9":"code","f995a42d":"code","b62a02f1":"code","33b32627":"code","34e0a8c9":"code","aa9a9165":"code","79ea1f64":"code","48f8e382":"markdown","93dabd85":"markdown","d81e37ab":"markdown","1619a7bf":"markdown","68074ad5":"markdown","b8dff9da":"markdown","56c360dd":"markdown","f61a4b6f":"markdown","9db2d063":"markdown","1ca11f90":"markdown","bf1b99cc":"markdown","3be06e68":"markdown","6ff6fe7f":"markdown","05f72b51":"markdown","96c09cdb":"markdown","08f186ab":"markdown","42a1f28a":"markdown","3f54e4fd":"markdown","81e443b8":"markdown","c692f152":"markdown","3f505370":"markdown","b7268c1f":"markdown","dc9af922":"markdown","68ec31e3":"markdown","61ff246c":"markdown","30ecadfd":"markdown","6d33f82b":"markdown"},"source":{"4e6e7164":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bee050ca":"from matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier","db34dd36":"df = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","d8291872":"df.head(10)","a8dee286":"df.isna().sum()","19d0faed":"df = df.drop(['salary'], axis = 1)\ndf.head()","4bc7353c":"le = LabelEncoder()\ndf['gender'] = le.fit_transform(df['gender'])\ndf['ssc_b'] = le.fit_transform(df['ssc_b'])\ndf['hsc_b'] = le.fit_transform(df['hsc_b'])\ndf['hsc_s'] = le.fit_transform(df['hsc_s'])\ndf['degree_t'] = le.fit_transform(df['degree_t'])\ndf['workex'] = le.fit_transform(df['workex'])\ndf['specialisation'] = le.fit_transform(df['specialisation'])\ndf['status'] = le.fit_transform(df['status'])","3cd22b24":"df.head(10)","0db6d017":"df.describe()","1f2a28a8":"plt.figure(figsize=(20, 20))\ncorr_mat = df.corr().round(2)\nsns.heatmap(data=corr_mat, annot=True)","d37fe0db":"features = ['ssc_p', 'hsc_p', 'degree_p', 'workex', 'specialisation']","d66fd095":"y = df['status']\nfor i in features:\n  x = df[i]\n  plt.xlabel(i)\n  plt.ylabel(\"Placed or not\")\n  plt.scatter(x, y)\n  plt.show()","6f67b765":"X = df[features]\nY = df['status']","9e59afe7":"X_train, x_test, Y_train, y_test = train_test_split(X, Y, random_state=4, test_size=0.3)","263bf982":"def perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)","15e8ebc1":"svmModel = SVC()\nsvmModel.fit(X_train, Y_train)\nsvmModel.score(x_test, y_test)","063f88c4":"y_svm_hat = svmModel.predict(x_test)","9e793560":"plot_roc_curve(svmModel, x_test, y_test)","523accfa":"plot_confusion_matrix(svmModel, x_test, y_test)","0cdad7fe":"truePositive, falsePositive, trueNegative, falseNegative = perf_measure(np.asarray(y_test), np.asarray(y_svm_hat))\nprint(\"Precision is\", (truePositive \/ (truePositive + falsePositive)))\nprint(\"Recall is\", (truePositive \/ (truePositive + falseNegative)))\nprint(\"Specificity is\", (trueNegative \/ (trueNegative + falsePositive)))\nprint(\"Accuracy is\", ((truePositive + trueNegative) \/ (truePositive + falsePositive + falseNegative + trueNegative)))","53adecee":"lrModel = LogisticRegression()\nlrModel.fit(X_train, Y_train)\nlrModel.score(x_test, y_test)","6cda456b":"y_lr_hat = lrModel.predict(x_test)","be59e5a8":"plot_roc_curve(lrModel, x_test, y_test)","262df996":"plot_confusion_matrix(lrModel, x_test, y_test)","2b6bbcb9":"truePositive, falsePositive, trueNegative, falseNegative = perf_measure(np.asarray(y_test), np.asarray(y_lr_hat))\nprint(\"Precision is\", (truePositive \/ (truePositive + falsePositive)))\nprint(\"Recall is\", (truePositive \/ (truePositive + falseNegative)))\nprint(\"Specificity is\", (trueNegative \/ (trueNegative + falsePositive)))\nprint(\"Accuracy is\", ((truePositive + trueNegative) \/ (truePositive + falsePositive + falseNegative + trueNegative)))","f995a42d":"scores = []\nfor i in range(1, 21):\n  knnModel = KNeighborsClassifier(n_neighbors=i)\n  knnModel.fit(X_train, Y_train)\n  score = knnModel.score(x_test, y_test)\n  scores.append(score)\n\nmax(scores)","b62a02f1":"errors = [(1 - x) for x in scores]\n\nplt.figure(figsize=(12, 12))\nplt.plot(range(1, 21), errors, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value') \nplt.xlabel('K') \nplt.ylabel('Error Rate')\nplt.show()","33b32627":"best_n = scores.index(max(scores)) + 1\nknnModel = KNeighborsClassifier(n_neighbors=best_n)\nknnModel.fit(X_train, Y_train)\ny_knn_hat = knnModel.predict(x_test)","34e0a8c9":"plot_roc_curve(knnModel, x_test, y_test)","aa9a9165":"plot_confusion_matrix(knnModel, x_test, y_test)","79ea1f64":"truePositive, falsePositive, trueNegative, falseNegative = perf_measure(np.asarray(y_test), np.asarray(y_knn_hat))\nprint(\"Precision is\", (truePositive \/ (truePositive + falsePositive)))\nprint(\"Recall is\", (truePositive \/ (truePositive + falseNegative)))\nprint(\"Specificity is\", (trueNegative \/ (trueNegative + falsePositive)))\nprint(\"Accuracy is\", ((truePositive + trueNegative) \/ (truePositive + falsePositive + falseNegative + trueNegative)))","48f8e382":"# Conclusion\nThis data set consists of Placement data of students in Jain University Bangalore campus. It includes secondary and higher secondary school percentage and specialization. It also includes degree specialization, type and Work experience and salary offers to the placed students.\n\nDisplayed the correlation between different features in the dataset using heatmaps and graphs. Also calculated the accuracy, specificity indicating the accuracy of our model. Also, visualised our predictions in the form of a confusion matrix.\n\nThe KNN model makes pretty good predictions and has a good accuracy rate of about 89%.","93dabd85":"Plotting a ROC Curve for our KNN model and seeing how it fares.","d81e37ab":"## Finding relation between the target and features\nWe plot a graph to see how the target feature vary with different features we selected above.","1619a7bf":"We shape our dataset into X and Y variables according to features selected from the heatmap and graphs","68074ad5":"## Splitting the dataset\nWe use train_test_split to test our dataset into training and testing variables.","b8dff9da":"Plotting a ROC Curve for the same SVM Model.","56c360dd":"Plotting a ROC curve for our logistic regression model.","f61a4b6f":"### Creating a SVM Model","9db2d063":"## Using heatmaps\nGraphs can give a pretty fair picture about the relationship between the targetted data and the feature. But using a heatmap shows a more accurate picture about the correlation between different features and the target variable.","1ca11f90":"Plotting a confusion matrix for our KNN Model.","bf1b99cc":"Calculating specificity, recall, precision and accuracy for the KNN Model.","3be06e68":"Based on the correlation factor we choose a few features.","6ff6fe7f":"## Calculating TF, TN, FP, FN\nWriting a function to calculate True Positives, False Positives, True Negatives and False Negatives.","05f72b51":"## Describing our dataset\nWe display the first 10 entries of the DataFrame object of the dataset.","96c09cdb":"Calculating specificity, recall, precision and accuracy.","08f186ab":"### Using KNN","42a1f28a":"Counting null values in the dataset","3f54e4fd":"We load our dataset into a Pandas DataFrame object.","81e443b8":"Plotting a confusion matrix to see how the SVM Model fares.","c692f152":"Dropping the salary column as it has a lot of null values.","3f505370":"Calculating specificity, recall, precision and accuracy for our Logistic Regression model.","b7268c1f":"Plotting a confusion matrix for our Logistic Regression Model.","dc9af922":"Making predictions using the given x test values","68ec31e3":"Label Encoding various columns in the dataset.","61ff246c":"Summarising our dataset.","30ecadfd":"Making predictions using the given x test values","6d33f82b":"### Creating a Logistic Regression Model"}}