{"cell_type":{"33569769":"code","fef67ae3":"code","ad456714":"code","928b374f":"code","d7cdf17f":"code","7646bd38":"code","138a604d":"code","63b1841e":"code","0f62ec3a":"code","584b831a":"code","1b8cb0ec":"code","18a43104":"code","a3017649":"code","be540ad8":"code","7dd8829f":"code","198cbcc3":"code","22b81f6c":"code","f67da897":"code","407ea9b4":"code","534f84c1":"code","b0053eb4":"code","9759d282":"code","86a10c3c":"code","7217d8da":"code","0bb77c4b":"code","0864d5dd":"code","b4dda1b4":"code","937d9168":"code","41f171c1":"code","b3378d6a":"code","1b364833":"code","8d737c2f":"code","96a38c53":"code","269f7a6d":"code","7ad82fb0":"code","32534978":"code","926010df":"code","c52b7d70":"code","17811201":"code","4e468e5a":"markdown","7b78805a":"markdown","866e1996":"markdown","bf910342":"markdown","839aa237":"markdown","14fbc42a":"markdown","ccfe8c61":"markdown","eab6e3e7":"markdown","67d07973":"markdown","70691dec":"markdown","75f3ab19":"markdown","e3800fff":"markdown","271cb3b0":"markdown","61e54bc6":"markdown","7fa74d75":"markdown","dd576e36":"markdown","cedbacd6":"markdown","91562954":"markdown","d98987cd":"markdown","4b393924":"markdown"},"source":{"33569769":"import numpy as np\nimport pandas as pd\nimport os\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils as ku\nimport tensorflow as tf\nimport matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport pylab as pl\n\nfrom tqdm import tqdm\ntqdm.pandas()","fef67ae3":"frames = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print('Loading', os.path.join(dirname, filename))\n        frames.append(pd.read_csv(os.path.join(dirname, filename)))\ndf = pd.concat(frames)\ndf = df.sample(frac=1).reset_index(drop=True)  # shuffles the rows, just in case the files contain a certain order etc...","ad456714":"def make_train_test_val(key, topn, train_ratio=0.7, val_ratio=0.15):\n    topn_df = df[key].value_counts().head(topn)\n    min_nb = min(topn_df)\n    train_size = int(train_ratio * min_nb)\n    val_size = int(val_ratio * min_nb)\n    test_size = min_nb - train_size - val_size\n    # print('\\nFor each %s: TRAIN=%s, VAL=%s, TEST=%s' % (key, train_size, val_size, test_size))\n    unique_keys = topn_df.index\n    \n    training_frames = []\n    val_frames = []\n    test_frames = []\n\n    for elem in topn_df.index:\n        # print('Building', key, '|', elem, 'dataset (train\/val\/test)')\n        sub_df = df[df[key] == elem]\n        training_frames.append(sub_df[0: train_size - 1])\n        val_frames.append(sub_df[train_size: train_size + val_size - 1])\n        test_frames.append(sub_df[-test_size:])\n    train_data = pd.concat(training_frames).sample(frac=1).reset_index(drop=True)\n    val_data = pd.concat(val_frames).sample(frac=1).reset_index(drop=True)\n    test_data = pd.concat(test_frames).sample(frac=1).reset_index(drop=True)\n    return train_data, val_data, test_data, unique_keys","928b374f":"def unique2categ(unique_keys):\n    elem2categ = dict(zip(unique_keys, range(len(unique_keys))))\n    categ2elem = dict(zip(range(len(unique_keys)), unique_keys))\n    print(elem2categ)\n    print(categ2elem)\n    return elem2categ, categ2elem","d7cdf17f":"def tokenize_dataset(train_data, val_data, test_data, tokenizer):\n    tokenizer.fit_on_texts(train_data.content)\n    tokenizer.fit_on_texts(val_data.content)\n    tokenizer.fit_on_texts(test_data.content)\n    total_words = len(tokenizer.word_index) + 1\n    \n    train_sequences = tokenizer.texts_to_sequences(train_data.content)\n    val_sequences = tokenizer.texts_to_sequences(val_data.content)\n    test_sequences = tokenizer.texts_to_sequences(test_data.content)\n    \n    max_sequence_len_train = max([len(x) for x in train_sequences])\n    max_sequence_len_val = max([len(x) for x in val_sequences])\n    max_sequence_len_test = max([len(x) for x in test_sequences])\n    max_sequence_len = max([max_sequence_len_train, max_sequence_len_val, max_sequence_len_test])\n\n    train_input = np.array(pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre'))\n    val_input = np.array(pad_sequences(val_sequences, maxlen=max_sequence_len, padding='pre'))\n    test_input = np.array(pad_sequences(test_sequences, maxlen=max_sequence_len, padding='pre'))\n    return train_input, val_input, test_input, max_sequence_len\n","7646bd38":"def build_model(vocab_size, input_length, nb_classes, embedding_dim, units_Bi_LSTM=50, units_LSTM_2=30, dropout=0.3):\n    model = tf.keras.Sequential()\n    model.add(Embedding(vocab_size, embedding_dim, input_length=input_length))\n    model.add(Bidirectional(LSTM(units_Bi_LSTM, return_sequences=True)))\n    model.add(Dropout(dropout))\n    model.add(LSTM(units_LSTM_2))\n    model.add(Dense(vocab_size \/ 2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n    model.add(Dense(nb_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    return model","138a604d":"def plot_confusion_matrix(model, val_input, val_expected, xlabel, ylabel, title, suptitle, labels):\n    val_predicted = model.predict_classes(val_input, verbose=0)\n    cm = tf.math.confusion_matrix(\n        val_expected,\n        val_predicted,\n    ) #\/ val_size\n    fig = pl.figure(num=None, figsize=(16, 14), dpi=80, facecolor='w', edgecolor='k')\n    hm = sns.heatmap(cm, annot=True, fmt=\"d\", linewidths=.5)\n    hm.set_yticklabels(labels, rotation=0)\n    hm.set_xticklabels(labels, rotation=90)\n    pl.xlabel(xlabel)\n    pl.ylabel(ylabel)\n    pl.title(title, size=20)\n    pl.suptitle(suptitle, size=40)\n","63b1841e":"def plot_accuracy_loss(history):\n    acc     = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss    = history.history['loss']\n    val_loss= history.history['val_loss']\n    epochs  = range(len(acc))\n\n    plt.plot(epochs, acc, 'r')\n    plt.plot(epochs, val_acc, 'b')\n    plt.title('Training and validation accuracy')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n    plt.figure()\n\n    plt.plot(epochs, loss, 'r')\n    plt.plot(epochs, val_loss, 'b')\n    plt.title('Training and validation loss')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend([\"Loss\", \"Validation Loss\"])\n    plt.figure()","0f62ec3a":"accountCateg_detect_train, accountCateg_detect_val, accountCateg_detect_test, unique_accountCateg = make_train_test_val('account_category', 10)\naccountCateg2categ, categ2accountCateg = unique2categ(unique_accountCateg)\n\naccountCateg_detect_train['content'] = accountCateg_detect_train['content'].str.lower()\naccountCateg_detect_val['content'] = accountCateg_detect_val['content'].str.lower()\naccountCateg_detect_test['content'] = accountCateg_detect_test['content'].str.lower()\n\naccountCateg_detect_train['label'] = accountCateg_detect_train['account_category'].apply(lambda r: accountCateg2categ[r])\naccountCateg_detect_val['label'] = accountCateg_detect_val['account_category'].apply(lambda r: accountCateg2categ[r])\naccountCateg_detect_test['label'] = accountCateg_detect_test['account_category'].apply(lambda r: accountCateg2categ[r])","584b831a":"nb_words = 30000\ntokenizer = Tokenizer(num_words=nb_words, oov_token='<OOV_TOK>')\ntrain_input, val_input, test_input, max_sequence_len = tokenize_dataset(accountCateg_detect_train, accountCateg_detect_val, accountCateg_detect_test, tokenizer)\n\nnb_classes = len(unique_accountCateg)\ntrain_labels = ku.to_categorical(accountCateg_detect_train.label, num_classes=nb_classes)\nval_labels = ku.to_categorical(accountCateg_detect_val.label, num_classes=nb_classes)\ntest_labels = ku.to_categorical(accountCateg_detect_test.label, num_classes=nb_classes)","1b8cb0ec":"model = build_model(\n    vocab_size=nb_words,\n    input_length=max_sequence_len,\n    embedding_dim=50,\n    nb_classes=nb_classes,\n)","18a43104":"history = model.fit(\n    train_input,\n    train_labels,\n    epochs=5,\n    validation_data=(val_input, val_labels),\n    verbose=1\n)","a3017649":"plot_confusion_matrix(\n    model,\n    val_input,\n    accountCateg_detect_val.label,\n    xlabel='Expected Account Category',\n    ylabel='Predicted Account Category',\n    title='Using a Bi-LSTM with word embeddings',\n    suptitle='Account Category detection confusion matrix (Validation)',\n    labels=unique_accountCateg,\n)","be540ad8":"plot_accuracy_loss(history)","7dd8829f":"lang_detect_train, lang_detect_val, lang_detect_test, unique_languages = make_train_test_val('language', 12)\nlang2categ, categ2lang = unique2categ(unique_languages)\n\nlang_detect_train['content'] = lang_detect_train['content'].str.lower()\nlang_detect_val['content'] = lang_detect_val['content'].str.lower()\nlang_detect_test['content'] = lang_detect_test['content'].str.lower()\n\nlang_detect_train['label'] = lang_detect_train['language'].apply(lambda r: lang2categ[r])\nlang_detect_val['label'] = lang_detect_val['language'].apply(lambda r: lang2categ[r])\nlang_detect_test['label'] = lang_detect_test['language'].apply(lambda r: lang2categ[r])\n\nnb_words = 30000\ntokenizer = Tokenizer(num_words=nb_words, oov_token='<OOV_TOK>')\ntrain_input, val_input, test_input, max_sequence_len = tokenize_dataset(lang_detect_train, lang_detect_val, lang_detect_test, tokenizer)\n\nnb_classes = len(unique_languages)\ntrain_labels = ku.to_categorical(lang_detect_train.label, num_classes=nb_classes)\nval_labels = ku.to_categorical(lang_detect_val.label, num_classes=nb_classes)\ntest_labels = ku.to_categorical(lang_detect_test.label, num_classes=nb_classes)\n\nmodel = build_model(\n    vocab_size=nb_words,\n    input_length=max_sequence_len,\n    embedding_dim=50,\n    nb_classes=nb_classes,\n)\n\nhistory = model.fit(\n    train_input,\n    train_labels,\n    epochs=10,\n    validation_data=(val_input, val_labels),\n    verbose=1\n)","198cbcc3":"plot_confusion_matrix(\n    model,\n    val_input,\n    lang_detect_val.label,\n    xlabel='Expected language',\n    ylabel='Predicted language',\n    title='Using a Bi-LSTM with token embeddings',\n    suptitle='Language detection confusion matrix (Validation)',\n    labels=unique_languages,\n)","22b81f6c":"plot_accuracy_loss(history)","f67da897":"train_chars = set.union(*lang_detect_train.content.apply(lambda r: set(r)))\nval_chars = set.union(*lang_detect_val.content.apply(lambda r: set(r)))\ntest_chars = set.union(*lang_detect_test.content.apply(lambda r: set(r)))\nchars_vocab = sorted(set.union(train_chars, val_chars, test_chars))\n\nprint(chars_vocab)\nprint ('{} unique characters'.format(len(chars_vocab)))","407ea9b4":"char2idx = {u:i for i, u in enumerate(chars_vocab)}\nidx2char = np.array(chars_vocab)\nfor _df in [lang_detect_train, lang_detect_test, lang_detect_val]:\n    _df['encoded_chars'] = _df.content.apply(lambda r: np.array([char2idx[c] for c in r][:100]))\n\ntrain_input = np.array(pad_sequences(lang_detect_train.encoded_chars, maxlen=100, padding='pre'))\nval_input = np.array(pad_sequences(lang_detect_val.encoded_chars, maxlen=100, padding='pre'))\ntest_input = np.array(pad_sequences(lang_detect_test.encoded_chars, maxlen=100, padding='pre'))","534f84c1":"model = build_model(\n    vocab_size=len(chars_vocab),\n    embedding_dim=256,\n    input_length=100,\n    dropout=0.2,\n    nb_classes=nb_classes,\n)","b0053eb4":"history = model.fit(\n    train_input,\n    train_labels,\n    epochs=16,\n    validation_data=(val_input, val_labels),\n    verbose=1\n)","9759d282":"plot_confusion_matrix(\n    model,\n    val_input,\n    lang_detect_val.label,\n    xlabel='Expected language',\n    ylabel='Predicted language',\n    title='Using a Bi-LSTM with character embeddings',\n    suptitle='Language detection confusion matrix (Validation)',\n    labels=unique_languages,\n)","86a10c3c":"plot_accuracy_loss(history)","7217d8da":"author_detect_train, author_detect_val, author_detect_test, unique_authors = make_train_test_val('author', 20)\nauthor2categ, categ2author = unique2categ(unique_authors)\n\nauthor_detect_train['content'] = author_detect_train['content'].str.lower()\nauthor_detect_val['content'] = author_detect_val['content'].str.lower()\nauthor_detect_test['content'] = author_detect_test['content'].str.lower()\n\nauthor_detect_train['label'] = author_detect_train['author'].apply(lambda r: author2categ[r])\nauthor_detect_val['label'] = author_detect_val['author'].apply(lambda r: author2categ[r])\nauthor_detect_test['label'] = author_detect_test['author'].apply(lambda r: author2categ[r])","0bb77c4b":"nb_words = 30000\ntokenizer = Tokenizer(num_words=nb_words, oov_token='<OOV_TOK>')\ntrain_input, val_input, test_input, max_sequence_len = tokenize_dataset(author_detect_train, author_detect_val, author_detect_test, tokenizer)\n\nnb_classes = len(unique_authors)\ntrain_labels = ku.to_categorical(author_detect_train.label, num_classes=nb_classes)\nval_labels = ku.to_categorical(author_detect_val.label, num_classes=nb_classes)\ntest_labels = ku.to_categorical(author_detect_test.label, num_classes=nb_classes)","0864d5dd":"model = build_model(\n    vocab_size=nb_words,\n    input_length=max_sequence_len,\n    embedding_dim=50,\n    nb_classes=nb_classes,\n)","b4dda1b4":"history = model.fit(\n    train_input,\n    train_labels,\n    epochs=5,\n    validation_data=(val_input, val_labels),\n    verbose=1\n)","937d9168":"plot_confusion_matrix(\n    model,\n    val_input,\n    author_detect_val.label,\n    xlabel='Expected Author',\n    ylabel='Predicted Author',\n    title='Using a Bi-LSTM with word embeddings',\n    suptitle='Author detection confusion matrix (Validation)',\n    labels=unique_authors,\n)","41f171c1":"plot_accuracy_loss(history)","b3378d6a":"from nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\ntqdm.pandas()\nsub_df = df[df.language == 'English'].head(100000)\nsub_df['tokens'] = sub_df.content.progress_apply(lambda r: [i.lower() for i in word_tokenize(r)])","1b364833":"from gensim.models import Word2Vec\nmodel = Word2Vec(sub_df.tokens, size=100, sg=1, window=10, min_count=20, workers=10)","8d737c2f":"print(len(model.wv.vocab))","96a38c53":"print('man', 'boy =', model.wv.similarity('man', 'boy'))\nprint('man', 'woman =', model.wv.similarity('man', 'woman'))\nprint('man', 'cat =', model.wv.similarity('man', 'cat'))\nprint('man', 'idea =', model.wv.similarity('man', 'idea'))\nprint('man', '5 =', model.wv.similarity('man', '5'))","269f7a6d":"model.wv.most_similar('republicans', topn=10)","7ad82fb0":"model.wv.most_similar('russia', topn=10)","32534978":"model.wv.most_similar(positive=['boy', 'woman'], negative=['man'], topn=1)","926010df":"model.wv.most_similar(positive=['men', 'woman'], negative=['man'], topn=1)","c52b7d70":"from sklearn.decomposition import PCA\nimport numpy as np\n\ndef plot_labels_PCA(model, labels, light_up_labels):\n    fig = pl.figure(num=None, figsize=(30, 30), dpi=80, facecolor='w', edgecolor='k')\n    X = model[labels]\n    pca = PCA(n_components=2)\n    result = pca.fit_transform(X)\n    fig, ax = pl.subplots(num=None, figsize=(16, 16), dpi=80, facecolor='w', edgecolor='k')\n    ax.scatter(result[:, 0], result[:, 1])\n    for x, y, label in zip(result[:, 0], result[:, 1], labels):\n        if label in light_up_labels:\n            pl.annotate(\n                label,  # this is the text\n                (x, y),  # this is the point to label\n                textcoords=\"offset points\",  # how to position the text\n                xytext=(0, 10),  # distance from text to points (x,y)\n                ha='center',\n                size=16,\n                #color='red' if label in light_up_labels else 'black'\n            )  # horizontal alignment can be left, right or center\n    pl.show()","17811201":"labels = [i for i in model.wv.vocab if model.wv.vocab[i].count > 500]  # we only keep the most common labels, and this is already a LOT of information to plot... too much\nplot_labels_PCA(model, labels, light_up_labels={'man', 'woman', 'trump', 'obama', 'week', 'year', 'love', 'like'})","4e468e5a":"# Objectives\n\nThe main objective is to show how to do basic supervised learning with tensorflow for NLP data categorisation\n\n* Basic NLP tasks with tensorflow\n* word base VS chars based, embeddings\n* Categorical prediction with tf\n* Tracking how well your model works: train VS validation performance, confusion matrix\n","7b78805a":"An other example: from singular to plural","866e1996":"# Building a custom embedding\n\nEmbeddings are a way to represent tokens as vectors (It can be different\/more than words, such as expressions, sentences...), the vector encoding the word\/token meaning.   \nAgain I won't dwelve too deep on the idea behind word embeddings, but two points you can keep in mind:\n\n### How does the model learns the \"meaning\" of a word? \n\nHere we're going to use the word2vec approach (you can also check out GloVe, FastText ...)  \nLet's say we're trying to understand the meaning of the word XXXXXX, as humans.  \nConsider the following sentences:\n\n> XXXXXX is very sweet and tasty  \n> You can find XXXXXX growing on trees  \n> People use XXXXXX to make alcohol  \n\nEven if you never heard about XXXXXX before, you can understand we are talking about some kind of fruit.  \nTo simplify a lot, Word2Vec generalizes this idea to a whole corpus of text: for each word it will use the word context (the words around it) to infer its meaning.","bf910342":"## Predicting the language of a tweet using characters","839aa237":"## Predicting tweet language using words","14fbc42a":"#### One last graph: PCA\n\nPCA (Principal component analysis) will allow us to project a set of vectors of the N dimensions we have (here 100) to an other number, here 2 for visualization.  \nWe're obviously losing a lot of information, but we can still gain some indsight and understanding of what are word embeddings.\n\nI highlighted some couples, so you can see how close they end up:\n```\n{'man', 'woman'}\n{'trump', 'obama'}\n{'week', 'year'}\n{'love', 'like'}\n```","ccfe8c61":"## Predicting tweet category using words","eab6e3e7":"### Word2Vec model building\n\nA quick overview of the parameters\n\n* `size`: embedding (vector) size for each token\n* `sg`: algorithm to use, here skip-gram (else: CBOW). CBOW is more similar to the approach explained earlier, skipgram is a bit more complex and tends to be slower but to perform better especially with little data like here.\n* `window`: when looking at a word (to learn its meaning), defines the **context** size (words to the right and the left)\n* `min_count`: we ignore any token present less than min_count times in the corpus. If we have too little examples of a word it will be hard to get a good representation\n* `workers`: unerlated to word2vec, use to parallelise on 10 cores\n\nOne again this is a simplified overview, but if you are interested you can find plenty more littlerature about word2vec online. Don't hesitate to play with the parameters and train your own model!","67d07973":"#### Compute similarity between words\n\nThis, for a pair of words, compares their vector using cosine similarity. The higher the number, the most similar.","70691dec":"## Predicting a tweet author using words","75f3ab19":"## Model performance visualization\n\nSince we will me working with categorical data I am making graphs of two informations I like to have to evaluate my performance (in addition to my basic model accuracy etc...)\n\n* **confusion matrix**: for inputs associated with a given category, which categories does the model tends to predict. Useful to see if the model struggles to discernate between a subset of categories etc... Here we do the confusion matrix on the validation data.\n* **accuracy and loss evolution per epoch**: useful to track how well the model learns, as in \"am I underfitting or overfitting?\", by comparing scores on train and validation. Also useful to tune early-stopping.","e3800fff":"#### Find the words with the closest meaning\n\nSame idea as the previous point: for a given word, it will rank the words in the vocabulary from most similar to least similar.  \nNote that:\n\n* this model is trained on little data so it isn't perfect\n* it is very hard to compare words even for us humans. For example would you say that the word \"wine\" has more in common with \"alcohol\" or with \"grapes\"?","271cb3b0":"# Let's predict stuff!","61e54bc6":"# Setup code","7fa74d75":"#### Some more complex operations\n\nIf for \"man\" you have the word \"boy\" when younger, what is the equivalent for \"woman\"?  \nThis can be modelized as:\n\n```\nv(WOMAN) + v(BOY) - V(MAN)\n```\n\n![](http:\/\/)Meaning, we want a notion that as to do with woman and boy but NOT man, which roughly translates to `\"female + adult + male + young - made - adult\"` (once again it is a bit more complex but you get the general idea)","dd576e36":"### So what makes word embeddings so interesting\n\nWell, since you try to encode the words \/ tokens semantics as a vector, you can actually use mathematical operations on these vectors that hold meaning when you look at the associated words. It will be clearer with examples.","cedbacd6":"## Tensorflow model\n\nWe're going to make use of a model with different types of Layers and activations. I am not looking for the best model, since this one model is a bit too complex and tends to overfit the data quite fast given how \"little\" (compared to the model complexity) we have.\n\nNotably, we'll use the folowing layers:\n\n* Embedding (report to the last section where I dive deeper into what are embeddings if interested)\n* Dense\n* LSTM\n* Dropout\n\nI am not going to explain all the layers and activations since there are plenty of ressources online doing this better than I could.","91562954":"What do we get from these plots?\n\n* We struggle a bit more to differentiate between *Right Troll* and *Left Troll*.\n* The model also struggles a bit with the *Unknown* category, probably because it isn't as homogeneous as the rest.\n* We're pretty good with *comercial* data.\n* The model learns pretty fast but also overfits quite fast (you can see the accuracy\/loss keeps improving on training but not validation). When I have more time I'll try to address this issue.","d98987cd":"# Data loading\nSmall enough so we can load everything in RAM","4b393924":"## Datasets building functions\n\nGeneric code to transform the dataframe into inputs and labels ingestible by tensorflow.  \nMostly:\n\n* Spliting the data into train, validation and test\n* String labels to categorical data mapping\n* Overall, converting data to tf usable format\n\nOne important note: what the following code also does is, when picking categorical data (author, language, type tc...), balancing the classes so we have as many examples in each. This is to prevent categories inbalance. Ex: if we had 10000 examples for English, 100 for Russian and 100 for Italian, we could make a very simple model reaching 98% accuracy by always predicting English for everything."}}