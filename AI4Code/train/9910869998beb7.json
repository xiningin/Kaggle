{"cell_type":{"b7870b0a":"code","4e8e3bbe":"code","46b01f30":"code","25b2f890":"code","a0382af7":"code","ede4a637":"code","fe48181e":"code","3d1eb225":"code","7634bf65":"code","0c2c64d8":"code","dcb1a0df":"code","1513f267":"code","af745b82":"code","80e4ce71":"code","760024b1":"code","e6029862":"code","b960bda0":"code","4d4e8f69":"code","1c337455":"markdown","3088fa9a":"markdown","f53350b6":"markdown","37f139cd":"markdown"},"source":{"b7870b0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e8e3bbe":"test = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\ntrain = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\nlabels = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip')\nsub = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip')","46b01f30":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ntargets = train[labels.drop('id',axis = 1).columns]\ntargets\nfig, axes = plt.subplots(figsize = (32,8))\nsns.countplot(targets.sum(axis =1 )) # axis = 1 adds everything along columns , axis = 0 adds everything along rows\naxes.set_xticklabels(targets.columns, rotation=40, ha=\"right\",fontsize = 30)    \n# b.set_xlabel(\"X Label\",fontsize=30)\n# b.set_ylabel(\"Y Label\",fontsize=20)\nplt.show()","25b2f890":"train.head()","a0382af7":"from sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom sklearn import preprocessing\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import *\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, LSTM\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Input\nfrom keras.layers.merge import Concatenate\nfrom keras.utils import plot_model\n","ede4a637":"# def preprocess_text(sen):\n#     # removing punctuations and numbers\n#     sentence = re.sub('[^a-zA-Z]',' ',sen)\n#     # remove single character\n#     sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\",' ',sentence)\n#     # remove multiple spaces\n#     sentence = re.sub(r'\\s+',' ',sentence)\n    \n#     return sentence","fe48181e":"# x = []\n# sentences = list(train['comment_text'])\n# for i in sentences:\n#     x.append(preprocess_text(i))\n\n# test1 = []\n# sentences = list(test['comment_text'])\n# for i in sentences:\n#     test1.append(preprocess_text(i))\n    \n# y = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]","3d1eb225":"# X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)\n\n# y_train = to_categorical(y_train)\n# y_test = to_categorical(y_test)\n\n\n# tokenizer = Tokenizer(num_words = 5000)\n# tokenizer.fit_on_texts(X_train)\n\n# X_train = tokenizer.texts_to_sequences(X_train)\n# X_test = tokenizer.texts_to_sequences(X_test)\n\n\n# test1 = tokenizer.texts_to_sequences(test1)\n\n\n\n# vocab_size = len(tokenizer.word_index) + 1\n# maxlen = 200\n\n# X_train = pad_sequences(X_train,padding = 'post',maxlen = maxlen)\n# X_test = pad_sequences(X_test,padding = 'post',maxlen = maxlen)\n\n# test1 = pad_sequences(test1,padding = 'post',maxlen = maxlen)\n\n\n\n\n# embeddings_dic = dict()\n# glove_file = open('..\/input\/glove-word-embedding-twitter\/glove.twitter.27B.50d.txt',encoding = \"utf8\")\n# for line in glove_file:\n#     records = line.split()\n#     word = records[0]\n#     vector_dimensions = asarray(records[1:],dtype = 'float32')\n#     embeddings_dic[word] = vector_dimensions\n    \n# glove_file.close()\n\n# embedding_matrix = zeros((vocab_size,50))\n# for word,index in tokenizer.word_index.items():\n#     embedding_vector = embeddings_dic.get(word)\n#     if embedding_vector is not None:\n#         embedding_matrix[index] = embedding_vector","7634bf65":"# deep_inputs = Input(shape = (maxlen,))\n# embedding_layer = Embedding(vocab_size,50,weights = [embedding_matrix],trainable = False)(deep_inputs)\n# LSTM_Layer_1 = LSTM(128)(embedding_layer)\n# dense_layer_1 = Dense(2,activation = 'softmax')(LSTM_Layer_1)\n# dense_layer_2 = Dense(2,activation = 'softmax')(LSTM_Layer_1)\n# model1 = Model(inputs= deep_inputs,outputs = dense_layer_1)\n# model1.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['acc'])\n# print(model1.summary())\n\n# plot_model(model1,to_file = 'model_plot1.png',show_shapes = True,show_layer_names = True)\n# tox = model1.fit(X_train,y_train,batch_size = 128,epochs = 10,verbose = 1,validation_split = 0.2)","0c2c64d8":"# test_pred = model1.predict(test1)\n# submission = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip', index_col='id')\n# submission['prediction'] = [i[0] for i in test_pred]\n# submission.reset_index(drop=False, inplace=True)\n# submission.to_csv('submission.csv', index=False)","dcb1a0df":"x = []\nsentences = list(train['comment_text'])\nfor i in sentences:\n    x.append(preprocess_text(i))\n\ntest1 = []\nsentences = list(test['comment_text'])\nfor i in sentences:\n    test1.append(preprocess_text(i))\n    \ny = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]","1513f267":"X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)\n","af745b82":"import string\nfrom sklearn.feature_extraction import stop_words\n\nstop_words = stop_words.ENGLISH_STOP_WORDS\n\ndef clean(doc):\n    # Removal of punctuation marks (.,\/\\][{} etc) and numbers\n    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n    # Removal of stopwords\n    doc = \" \".join([token for token in doc.split() if token not in stop_words])\n    return doc.lower()","80e4ce71":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer(max_features = 5000,preprocessor = clean)\nX_train_dtm = vect.fit_transform(X_train)\nX_val_dtm = vect.transform(X_test)","760024b1":"y_cols = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]","e6029862":"print(X_train_dtm.shape, X_val_dtm.shape)\n","b960bda0":"from sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nnav = MultiOutputClassifier(MultinomialNB()).fit(X_train_dtm,y_train)","4d4e8f69":"test = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\nsample = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip')\ndf_test = pd.merge(test,sample,on = 'id')\nx_test_dtm = vect.transform(df_test['comment_text'])\ny_preds = np.transpose(np.array(nav.predict_proba(x_test_dtm))[:,:,1])\ndf_test[y_cols] = y_preds\n\ndf_test.drop(['comment_text'],axis = 1,inplace = True)\n\ndf_test.to_csv('sample_submissiong.csv',index = False)","1c337455":"You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n\n- toxic\n- severe_toxic\n- obscene\n- threat\n- insult\n- identity_hate\n\nYou must create a model which predicts a probability of each type of toxicity for each comment.\n\nFile descriptions\n\n- train.csv - the training set, contains comments with their binary labels\n- test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.\n- sample_submission.csv - a sample submission file in the correct format\n- test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)","3088fa9a":"## Basic proper model","f53350b6":"# Categories ","37f139cd":"# Basic model Predicting only Toxicity"}}