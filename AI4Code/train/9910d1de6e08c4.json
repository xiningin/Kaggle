{"cell_type":{"d4496d96":"code","1b34f0ba":"code","ccef56ee":"code","050cc8ce":"code","8160c933":"code","bfba709d":"code","04a4f54f":"code","9b6d212e":"code","4c61a7c1":"code","434c1155":"code","2a57fe46":"code","f947d0eb":"code","d7c05154":"code","2c2326e8":"code","f10a71da":"code","f63c5dd6":"code","4284df02":"code","812c55a9":"code","edc5b4c6":"code","1f6ee59a":"code","64d2224a":"code","55ed2b93":"code","854f076b":"markdown","4302ff43":"markdown","ea98b00c":"markdown","c2756b9f":"markdown","66754a21":"markdown","a7d1ac8d":"markdown","e137c9df":"markdown","ec21e43e":"markdown","89329c01":"markdown","a9fa140f":"markdown","da920c6c":"markdown","479f4da8":"markdown","e5ccbd75":"markdown","ab119f90":"markdown","4a37168d":"markdown","d5c6cd55":"markdown","a51ea743":"markdown"},"source":{"d4496d96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1b34f0ba":"raw_data = pd.read_csv('..\/input\/train.csv', low_memory=False)\nraw_test_data = pd.read_csv('..\/input\/test.csv', low_memory=False)","ccef56ee":"train_data = raw_data.drop('Survived', axis = 1)\ntest_data = raw_test_data.copy()","050cc8ce":"raw_data.tail().T","8160c933":"raw_data.describe(include='all')","bfba709d":"raw_data.isna().sum()","04a4f54f":"train_data_age_mean = train_data.Age.mean()\ntrain_data.Age.fillna(train_data_age_mean,inplace=True)\ntest_data.Age.fillna(train_data_age_mean, inplace=True)","9b6d212e":"train_data.Embarked.value_counts()","4c61a7c1":"train_data.Embarked.fillna('S', inplace=True)","434c1155":"train_data['CabinBool'] = train_data.Cabin.isnull().astype('int64')\ntest_data['CabinBool'] = test_data.Cabin.isnull().astype('int64')\n\ntrain_data.drop('Cabin', inplace=True, axis=1)\ntest_data.drop('Cabin', inplace=True, axis=1)","2a57fe46":"sex_mapping = {'male':1, 'female':0}\ntrain_data.Sex.replace(sex_mapping, inplace=True)\ntest_data.Sex.replace(sex_mapping, inplace=True)","f947d0eb":"train_data.Embarked.unique()","d7c05154":"embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain_data.Embarked.replace(embarked_mapping, inplace=True)\ntest_data.Embarked.replace(embarked_mapping, inplace=True)","2c2326e8":"train_data.dtypes","f10a71da":"train_data.drop(labels = ['Name', 'PassengerId', 'Ticket'], axis=1,inplace=True)\n\ntest_data.drop(labels = ['Name', 'PassengerId', 'Ticket'], axis=1,inplace=True)","f63c5dd6":"model = RandomForestRegressor(n_jobs=-1)","4284df02":"model.fit(train_data, raw_data.Survived)\nmodel.score(train_data, raw_data.Survived)","812c55a9":"results = model.predict(test_data)","edc5b4c6":"test_data.Fare.fillna(test_data.Fare.mean(), inplace=True)","1f6ee59a":"results = model.predict(test_data)","64d2224a":"results = [1 if x > 0.5 else 0 for x in results]","55ed2b93":"ids = raw_test_data.PassengerId\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': results })\noutput.to_csv('submission.csv', index=False)","854f076b":"You can see that without applying much brain power we have gotten ourselves a decent model that has an r<sup>2<\/sup> score of **0.86**. Not bad ! Let's execute our final step and predict the results.","4302ff43":"# References\n[Awesome kernel for beginners on Titanic dataset][1]\n\n[1]: http:\/\/https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner\/comments\n","ea98b00c":"# Conclusion\nIn this kernel we saw how easy it is to start building your model even without having any in depth knowledege of how an algorithim works. Now, getting your hands dirty is one thing but getting competent at data science is another. It requires a lot of practice and intution that will only come through practice. I will post another indepth kernel on the same dataset as I move onto become (Contributor\/expert\/master\/grand master). Good luck !","c2756b9f":"Note that, we will be applying the same transformations to the test set as we do on the train set. Now let's move onto 'Embarked'.","66754a21":"There are certain other columns that we'll be droping. Although, we can extract futher features from the data let us stick to making the simplest model with the most minimal changes","a7d1ac8d":"Let's put a threshold on the predicted probabilities of our result. Let's say if the probability of survival in greater than **0.5**, then we'll  predict the value as **1** ","e137c9df":"Now onto **step (1)**,  we'll see which all features have null values in them and then devise a plan to elimanate them","ec21e43e":"Now that we have handled missing values, let us move onto **step 2**. We'll replace categorical variables with their coded representation.","89329c01":"There are 3 different columns that we'll have to handle for missing values\n1. Age\n2. Embarked\n3. Cabin\n\nLet us start with age. \nThere are many ways to impute missing data. But let's just stick to replacing age with the **mean age** of the whole data.","a9fa140f":"Let's load our data into dataframes","da920c6c":"We'll seperate our independent(features used to predict outcome) and dependent(outcome) variables from the training data.","479f4da8":"For embarked let's replace the missing value with the most common value of the lot, i.e., 'S'","e5ccbd75":"Now we will see what our training data looks like","ab119f90":"Bummer !! We forgot to check for NA's in our test set. Let's handle this with our known friend **mean**","4a37168d":"Onto **strep 3**, let's train a Random Forest classifier over our dataset.","d5c6cd55":"# Introduction\n\n*If you learning by failing then fastest way to learn is to fail often*\n\nThis kernel will help you start applying machine learning to datasets without any prior knowledge of how any algorithm works. My whole aim is to show that you can begin modelling data straight away without having to put much thought into it and treating the algorithms as black boxes.\n\nFollowing will be the my approach\n1.  Fill in the missing values (More often than not our data will contain missing values, we must chose between filling in the wholes or reject the data point entirely )\n2. Convert the whole data into numerical form (Machine Learning models are based on mathematical computation, so we must convert all are fields to numerical form)\n3. Chose an algorithm (here we use Random forest) and fit our data\n4. Predict and Submit","a51ea743":"We'll be handling 'Cabin' differently. Let's say we'll segregate people on the basis of whether they had a cabin or not. After that we'll be dropping the 'Cabin' variable altogether.  "}}