{"cell_type":{"6b189a28":"code","8da8900b":"code","a2eb27cc":"code","2f103e95":"code","d8e91c95":"code","d26358c5":"code","a35e7b52":"code","166a8753":"code","ed4b7b2a":"code","05b54e63":"code","a7a8df67":"code","5e2dc8bc":"code","8afcbf3a":"code","30d83a6a":"code","5df69087":"code","e534c607":"code","ff5f5ad2":"code","fc4e07ec":"code","ef6f3094":"code","c921c67c":"code","8f0430ff":"code","76f1b4e6":"code","6e772723":"code","18906b25":"markdown","3cc5c6f2":"markdown","203d180b":"markdown","e0bb5062":"markdown","3e14659d":"markdown","67046a60":"markdown","e5a6bd9d":"markdown","b498ed9c":"markdown","3c65449d":"markdown","326fa69e":"markdown","64cc7a8b":"markdown","c33a6d40":"markdown"},"source":{"6b189a28":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8da8900b":"# load the  dataset\ndata = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')\ndata.head()","a2eb27cc":"# Get the label column\nlabel = data[data.columns[2]]\n\n# Create a figure for 2 subplots (2 rows, 1 column)\nfig, ax = plt.subplots(2, 1, figsize = (9,12))\n\n# Plot the histogram   \nax[0].hist(label, bins=100)\nax[0].set_ylabel('Frequency')\n\n# Add lines for the mean, median, and mode\nax[0].axvline(label.mean(), color='magenta', linestyle='dashed', linewidth=2)\nax[0].axvline(label.median(), color='cyan', linestyle='dashed', linewidth=2)\n\n# Plot the boxplot   \nax[1].boxplot(label, vert=False)\nax[1].set_xlabel('Price')\n\n# Add a title to the Figure\nfig.suptitle('Price Distribution')\n\n# Show the figure\nfig.show()\n","2f103e95":"data = data[data['price']<4000000]\n# Get the label column\nlabel = data[data.columns[2]]\n\n# Create a figure for 2 subplots (2 rows, 1 column)\nfig, ax = plt.subplots(2, 1, figsize = (9,12))\n\n# Plot the histogram   \nax[0].hist(label, bins=100)\nax[0].set_ylabel('Frequency')\n\n# Add lines for the mean, median, and mode\nax[0].axvline(label.mean(), color='magenta', linestyle='dashed', linewidth=2)\nax[0].axvline(label.median(), color='cyan', linestyle='dashed', linewidth=2)\n\n# Plot the boxplot   \nax[1].boxplot(label, vert=False)\nax[1].set_xlabel('Label')\n\n# Add a title to the Figure\nfig.suptitle('Label Distribution')\n\n# Show the figure\nfig.show()","d8e91c95":"data.dtypes\n","d26358c5":"data = data.drop(['date','id'],axis=1)","a35e7b52":"data.head()","166a8753":"for col in data[data.columns[0:-1]]:\n    fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    feature = data[col]\n    correlation = feature.corr(label)\n    plt.scatter(x=feature, y=label)\n    plt.xlabel(col)\n    plt.ylabel('Correlations')\n    ax.set_title('Label vs ' + col + '- correlation: ' + str(correlation))\nplt.show()\n","ed4b7b2a":"X = data.drop(['price'],axis=1)\ny=data['price']","05b54e63":"X.head()","a7a8df67":"from sklearn.model_selection import train_test_split\n\n\n# Split data 70%-30% into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n\nprint ('Training Set: %d, rows\\nTest Set: %d rows' % (X_train.shape[0], X_test.shape[0]))","5e2dc8bc":"# Train the model\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n\n# Define preprocessing for numeric columns (scale them)\nnumeric_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n    ])\n\n# Create preprocessing and training pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('regressor', RandomForestRegressor())])\n\n\n# fit the pipeline to train a linear regression model on the training set\nmodel_rf = pipeline.fit(X_train, (y_train))\nprint (model_rf)","8afcbf3a":"from sklearn.metrics import mean_squared_error, r2_score\n\n# Get predictions\npredictions = model_rf.predict(X_test)\n\n# Display metrics\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nplt.title('Predictions vs Actuals')\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","30d83a6a":"from sklearn.linear_model import Lasso\n\n# Define preprocessing for numeric columns (scale them)\nnumeric_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n    ])\n\n# Create preprocessing and training pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('regressor', Lasso())])\n\n\n# fit the pipeline to train a linear regression model on the training set\nmodel_ls = pipeline.fit(X_train, (y_train))\nprint (model_ls)","5df69087":"# Get predictions\npredictions = model_ls.predict(X_test)\n\n# Display metrics\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nplt.title('Predictions vs Actuals')\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","e534c607":"# Train the model\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Define preprocessing for numeric columns (scale them)\nnumeric_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n    ])\n\n# Create preprocessing and training pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('regressor', GradientBoostingRegressor())])\n\n\n# fit the pipeline to train a linear regression model on the training set\nmodel_gb = pipeline.fit(X_train, (y_train))\nprint (model_gb)\n\n# Evaluate the model using the test data\npredictions = model_gb.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nplt.title('Predictions vs Actuals')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","ff5f5ad2":"import joblib\n\n# Save the model as a pickle file\nfilename = 'house-price-rf.pkl'\njoblib.dump(model_rf, filename)","fc4e07ec":"X_train.head()","ef6f3094":"# Load the model from the file\nloaded_model = joblib.load(filename)\n\n# Create a numpy array containing a new observation (for example tomorrow's seasonal and weather forecast information)\nX_new = np.array([[3,1,1500,5240,1.0,0,0,3,7,1450,1000,2004,2000,98144,47.5942,-122.296,1780,840]]).astype('float64')\nprint ('New sample: {}'.format(list(X_new[0])))\n\n# Use the model to predict tomorrow's rentals\nresult = loaded_model.predict(X_new)\nprint('Prediction: {:.0f} '.format(np.round(result[0])))","c921c67c":"# An array of features based on five-day weather forecast\nX_new = np.array([[2,3,1250,4840,2.0,0,0,3,6,720,400,2500,0,98117,47.5165,-122.268,1540,5110],\n                  [4,2,1800,3240,1.0,0,0,2,5,1950,500,1999,1992,98144,47.5619,-122.183,2140,1820],\n                  [2,2,1000,2240,2.0,0,0,3,4,1280,300,2005,1990,98116,47.3378,-122.162,1500,7700],\n                  [4,4,1900,5240,3.0,1,1,4,2,1980,700,2000,1990,98117,47.3378,-122.162,1800,5700],\n                  [3,2,500,340,1.0,0,0,1,2,680,200,2000,0,98115,47.5942,-122.296,400,1000]])\n\n# Use the model to predict rentals\nresults = loaded_model.predict(X_new)\nprint('predictions of 5 housing\/ flats:')\nfor prediction in results:\n    print(np.round(prediction))","8f0430ff":"def plot_feature_importance(importance,names,model_type):\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n#Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n#Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n#Define size of bar plot\n    plt.figure(figsize=(10,8))\n#Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n#Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","76f1b4e6":"rf = RandomForestRegressor()\nrf_fit = rf.fit(X_train, y_train)\nfeature_importances = rf_fit.feature_importances_","6e772723":"plot_feature_importance(rf_fit.feature_importances_,X_train.columns,'RANDOM FOREST')","18906b25":"### Separate features and label and split data for training and validation\n\n","3cc5c6f2":"## Removing Outlier","203d180b":"## Plotting Feature Importance","e0bb5062":"The model's **predict** method accepts an array of observations, so you can use it to generate multiple predictions as a batch. For example, suppose you have details for five housings\/flats; you could use the model to predict price of those housings for each day based on the housing flat details.","3e14659d":"(**date** and **id** doesn't seem to be very predictive, so omit it)","67046a60":"**Hope** you liked this kernel. If like please do upvote ","e5a6bd9d":"For good measure, let's also try a *boosting* ensemble algorithm. We'll use a Gradient Boosting estimator, which like a Random Forest algorithm builds multiple trees, but instead of building them all independently and taking the average result, each tree is built on the outputs of the previous one in an attempt to incrementally reduce the *loss* (error) in the model.","b498ed9c":"We've now seen a number of common techniques used to train predictive models for regression. In a real project, you'd likely try a few more algorithms, hyperparameters, and preprocessing transformations; but by now you should have got the general idea. Let's explore how you can use the trained model with new data.\n\n### Use the Trained Model\n\nFirst, let's save the model. We will be using our best model i.e., RandomForestRegressor","3c65449d":"### Try Another Algorithm\n\nLet's try training our regression model by using a **Lasso** algorithm. We can do this by just changing the estimator in the training code.","326fa69e":"### Preprocess the data and train a model in a pipeline\n\nNormalize the numeric features, then use a RandomForestRegressor to train a model.","64cc7a8b":"## View numeric correlation","c33a6d40":"Now, we can load it whenever we need it, and use it to predict labels for new data. This is often called scoring or inferencing."}}