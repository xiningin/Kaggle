{"cell_type":{"98fff444":"code","4b531a07":"code","2711b5ed":"code","b7344b0d":"code","dfcaf84a":"code","4da75e8e":"code","b3c57031":"code","089d5913":"code","0719f9b3":"code","d60c98cf":"code","141c3d1f":"code","788096cd":"code","343aae02":"code","65f086bd":"code","0e7525a4":"code","a82146f7":"code","4a73938f":"code","13c78e7e":"code","a97f6c4b":"code","6f2c0ec3":"code","04eb1bf5":"code","56f71073":"code","7eae2ba7":"code","832dcd63":"code","f8c367fb":"code","fd3e46f9":"code","c789ddb1":"code","fdc46b38":"code","8d558600":"code","93da49b1":"code","5cebc11c":"code","d979f82b":"code","f2be7374":"code","b5028dec":"code","5e1c23d3":"code","7fcf145a":"code","6f448f2a":"code","616b81b0":"code","dc6286ab":"code","e5b872de":"code","4f2a00ba":"code","e48c5d85":"code","b92bd2b2":"code","d4d9e958":"code","dd3bde82":"code","92f23579":"code","121fbc0f":"code","0176cfa5":"code","f794d20c":"code","c745b3ba":"code","da8a831c":"code","34254835":"code","8ec13b5f":"code","7ddc9832":"code","8026b6ae":"code","b04f5e7d":"code","525f2e3c":"code","75a7c4b3":"code","854c5185":"markdown","87c88f0a":"markdown","17584d7f":"markdown","093e4e99":"markdown","09b96811":"markdown","0213908a":"markdown","4b4365bf":"markdown","1c715d59":"markdown","ada8c5d0":"markdown","267989ec":"markdown","e300d3af":"markdown","63502134":"markdown","91740768":"markdown","5fa70268":"markdown","eff0d359":"markdown","57dabbb2":"markdown","43a98289":"markdown","2c7c7e3e":"markdown","4c4eb79e":"markdown","a515dd56":"markdown","435e83ef":"markdown","dd3b6cd0":"markdown","328d2bb6":"markdown","1acd3e84":"markdown","30713444":"markdown","6c88e4b4":"markdown","5b26b5d0":"markdown","c04c546f":"markdown","644936dd":"markdown","38ea416c":"markdown","2d2f3321":"markdown","9b135b4c":"markdown","5439bd9f":"markdown","e79c0103":"markdown","89746033":"markdown"},"source":{"98fff444":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py     # \u7ed8\u56fe\u7684\u51fd\u6570\n\n\nimport plotly.graph_objs as go              # \u53ef\u7528\u4e8e\u7ed8\u5236\u4e0d\u540c\u56fe\u578b\uff0c\u5982 go.bar()\nimport plotly.express as px                 # \u53ef\u7528\u4e8e\u7ed8\u5236\u4e0d\u540c\u56fe\u578b\uff0c\u5982 px.bar()\nfrom plotly.subplots import make_subplots   # \u521b\u5efa\u5b50\u56fe\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)    #THIS LINE IS MOST IMPORTANT AS THIS WILL DISPLAY PLOT ON \n#NOTEBOOK WHILE KERNEL IS RUNNING","4b531a07":"train = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/test.csv')","2711b5ed":"train.head()","b7344b0d":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n\n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n\n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n\n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n\n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n\n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n            \" columns that have missing values.\")\n\n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","dfcaf84a":"train_copy = train\ntrain_copy = train_copy.replace(-1, np.NaN)","4da75e8e":"# Missing values statistics\nmissing_values = missing_values_table(train_copy)  # train_copy \u662f\u4e00\u4e2a dataframe\nmissing_values.head(20)","b3c57031":"# train_counts = train.target.value_counts()\n# train_counts = pd.DataFrame(train_counts)\n\n# fig = px.bar(train_counts,x=train_counts.index,y='target',barmode='group',color='target')\n# fig.update_traces(textposition='outside')\n# fig.update_layout(template='seaborn',title='target (counts)')\n# fig.show()","089d5913":"train_counts = train.target.value_counts()\ntrain_counts = pd.DataFrame(train_counts)\n\nfig = px.bar(train_counts,x=train_counts.index,y='target',barmode='group',color='target',text='target') # text \u53ef\u4ee5\u6807\u4e0a\u6570\u503c\nfig.update_traces(textposition='outside')\nfig.update_layout(yaxis_title='counts',xaxis_title='target',template='seaborn',title='target (counts)')\nfig.show()","0719f9b3":"bin_col = [col for col in train.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col]==0).sum())\n    one_list.append((train[col]==1).sum())","d60c98cf":"trace1 = go.Bar(\n    x=bin_col,\n    y=zero_list ,\n    name='Zero count'\n)\ntrace2 = go.Bar(\n    x=bin_col,\n    y=one_list,\n    name='One count'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack',\n    title='Count of 1 and 0 in binary variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()\n","141c3d1f":"from sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score","788096cd":"train_target = train['target']\n# train_feature = train.drop(columns='target')\ntrain_feature = train.drop(columns = ['target','id'])\n\n\nx_train,x_test,y_train,y_test = train_test_split(train_feature,train_target,test_size= 0.2,random_state=10)","343aae02":"# model = XGBClassifier(n_estimators=1000)\nmodel = XGBClassifier()\n\n\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\nacc = accuracy_score(y_test,y_pred)\nrecall = recall_score(y_test,y_pred)\n\nprint('Accuracy: {:.3f}'.format(acc* 100.0))\nprint('recall: {:.3f}'.format(recall* 100.0))","65f086bd":"def plot_confusion_matrix(cm, classes,\n                                normalize=False,\n                                title='Confusion matrix',\n                                cmap=plt.cm.Blues):\n        \"\"\"\n            \u6b64\u51fd\u6570\u6253\u5370\u5e76\u7ed8\u5236\u6df7\u6dc6\u77e9\u9635\u3002\n            \u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u201c normalize = True\u201d\u6765\u5e94\u7528\u5f52\u4e00\u5316\u3002\n        \"\"\"\n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(title)\n        plt.colorbar()\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=45)\n        plt.yticks(tick_marks, classes)\n\n        if normalize:\n            cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n            print(\"Normalized confusion matrix\")\n        else:\n            print('Confusion matrix, without normalization')\n\n        print(cm)\n\n        thresh = cm.max() \/ 2.\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, cm[i, j],\n                        horizontalalignment=\"center\",\n                        color=\"white\" if cm[i, j] > thresh else \"black\")\n\n        plt.tight_layout()\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')","0e7525a4":"classes = ['target_0','target_1'] # \u987a\u5e8f\u522b\u641e\u9519\nnp.set_printoptions(precision=2)\n\ncm = confusion_matrix(y_test,y_pred)\nplt.figure()\nplot_confusion_matrix(cm,classes)\nplt.show()\n\n# plt.figure()\n# plot_confusion_matrix(cm,classes,normalize=True)\n# plt.show()","a82146f7":"from imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler","4a73938f":"# \u5b9a\u4e49\u7ed8\u56fe\u51fd\u6570\ndef plot_2d_space(X, y, label='Classes'):   \n            colors = ['#1F77B4', '#FF7F0E']\n            markers = ['o', 's']\n            for l, c, m in zip(np.unique(y), colors, markers):\n                plt.scatter(\n                    X[y==l, 0],\n                    X[y==l, 1],\n                    c=c, label=l, marker=m\n                )\n            plt.title(label)\n            plt.legend(loc='upper right')\n            plt.show()\n            \nprint(\"label0: \",len(x_train[y_train==0]))\nprint(\"label1: \",len(x_train[y_train==1]))\n\nss = StandardScaler()\nX = ss.fit_transform(x_train)\n\n# `2\u3001`\u5982\u679c\u6570\u636e\u5b58\u5728\u591a\u7ef4\u7279\u5f81\u53ef\u4f7f\u7528PCA\u6765\u964d\u7ef4\uff0c\u4f7f\u5176\u80fd\u57282D\u56fe\u4e2d\u5c55\u793a\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, y_train, 'Imbalanced dataset (2 PCA components)')","13c78e7e":"oversampler=SMOTE(random_state=0)\n# \u5f00\u59cb\u4eba\u5de5\u5408\u6210\u6570\u636e\nos_features,os_labels=oversampler.fit_sample(x_train,y_train)\n\n# \u67e5\u770b\u751f\u6210\u7ed3\u679c\nprint(\"label1: \",len(os_labels[os_labels==1]))\nprint(\"label0: \",len(os_labels[os_labels==0]))","a97f6c4b":"oversampler=SMOTE(random_state=0)\n# \u5f00\u59cb\u4eba\u5de5\u5408\u6210\u6570\u636e\nos_features_test,os_labels_test=oversampler.fit_sample(x_test,y_test)\n\n# \u67e5\u770b\u751f\u6210\u7ed3\u679c\nprint(\"label1: \",len(os_labels_test[os_labels_test==1]))\nprint(\"label0: \",len(os_labels_test[os_labels_test==0]))","6f2c0ec3":"ss = StandardScaler()\nX = ss.fit_transform(os_features)\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, os_labels, 'Imbalanced dataset (2 PCA components)')","04eb1bf5":"new_os_features = os_features.copy()\nnew_os_features['target'] = os_labels\n\n# Find correlations with the target and sort\ncorrs = new_os_features.corr()['target'].sort_values(ascending=False)\ncorrelations = pd.DataFrame(corrs)\n\n# Display correlations\nprint('Most Positive Correlations:\\n')\ncorrelations.head()","56f71073":"print('Most Negative Correlations:\\n')\ncorrelations.tail()","7eae2ba7":"correlations.loc[correlations.index.isin(['ps_ind_10_bin','ps_ind_11_bin','ps_ind_12_bin','ps_ind_13_bin'])]","832dcd63":"np.abs(corrs).sort_values(ascending=False).tail(15)","f8c367fb":"corrs = os_features.corr()","fd3e46f9":"# \u8bbe\u7f6e\u9608\u503c\nthreshold = 0.8\n\n# \u521b\u5efa\u4e00\u4e2a\u7a7a\u5b57\u5178\u4ee5\u5bb9\u7eb3\u76f8\u5173\u53d8\u91cf\nabove_threshold_vars = {}\n\n# \u5bf9\u4e8e\u6bcf\u4e00\u5217\uff0c\u8bb0\u5f55index\u884c\u4e2d\u7684\u90a3\u4e2a\u503c\u9ad8\u4e8e\u9608\u503c\u7684\u53d8\u91cf\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])","c789ddb1":"# above_threshold_vars","fdc46b38":"# \u8ddf\u8e2a\u8981\u5220\u9664\u7684\u5217\u548c\u5df2\u68c0\u67e5\u7684\u5217\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# \u904d\u5386\u5217\u548c\u76f8\u5173\u5217\nfor key, value in above_threshold_vars.items():\n    # \u8ddf\u8e2a\u5df2\u68c0\u67e5\u7684\u5217\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # \u5982\u679c\u5b58\u5728\u9ad8\u76f8\u5173\u7684\u7279\u5f81\uff0c\u53ea\u4fdd\u7559\u4e00\u4e2a\n            if x not in cols_seen:                  # \u5982\u679c\u8be5\u7279\u5f81\u5728\u4e4b\u524d\u7684 key \u6570\u636e\u4e2d\u6ca1\u6709\u51fa\u73b0\u8fc7\u3002\n                cols_to_remove.append(x)            # \u5b58\u5728\u9ad8\u5ea6\u76f8\u5173\uff0c\u5c06\u9ad8\u5ea6\u76f8\u5173\u7684\u7279\u5f81\u653e\u5165 cols_to_remove \u4e2d\u3002\n                cols_to_remove_pair.append(key)     # cols_to_remove \u548c cols_to_remove_pair \u5f97\u5230\u7684\u7ed3\u679c\u4e00\u81f4\u3002\n\ncols_to_remove = list(set(cols_to_remove))\nprint('Name of columns to remove: ', cols_to_remove)\nprint('Number of columns to remove: ', len(cols_to_remove))","8d558600":"train_corrs_removed = os_features.drop(columns = cols_to_remove)\ntest_corrs_removed = os_features_test.drop(columns = cols_to_remove)\n\nprint('Training Corrs Removed Shape: ', train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ', test_corrs_removed.shape)","93da49b1":"train_corrs_removed.to_csv('train_corrs_removed.csv', index = False)\ntest_corrs_removed.to_csv('test_corrs_removed.csv', index = False)","5cebc11c":"# `1\u3001\u5b9a\u4e49\u57fa\u5c3c\u7cfb\u6570\uff1a`\n\ndef gini(y, pred):\n    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() \/ g[:,0].sum()\n    gs -= (len(y) + 1) \/ 2.\n    return gs \/ len(y)\n\n# `2\u3001\u5b9a\u4e49 xgb gini \u7cfb\u6570\uff1a`\n\n# \u8fd4\u56de\u4e00\u4e2a normalized \u540e\u7684 gini \u5206\u6570\ndef gini_xgb(pred, y):\n    y = y.get_label()\n    return 'gini', gini(y, pred) \/ gini(y, y)","d979f82b":"from sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nimport lightgbm as lgb","f2be7374":"params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n          'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n\nnrounds=200  \nkfold = 2\nskf = StratifiedKFold(n_splits=kfold, random_state=0)\n\nfor i, (train_index, test_index) in enumerate(skf.split(train_feature, train_target)):\n    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n    X_train, X_valid = train_feature.loc[train_index], train_feature.loc[test_index]\n    y_train, y_valid = train_target.loc[train_index], train_target.loc[test_index]\n    d_train = xgb.DMatrix(X_train, y_train) \n    d_valid = xgb.DMatrix(X_valid, y_valid) \n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    xgb_model = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n                          feval=gini_xgb, maximize=True, verbose_eval=100)\n    \n# xgb_y_pred = xgb_model.predict(xgb.DMatrix(test[features].values), \n#                         ntree_limit=xgb_model.best_ntree_limit+50)","b5028dec":"features = train_feature.columns\ntest_ids = test['id']","5e1c23d3":"xgb_test_predictions1 = xgb_model.predict(xgb.DMatrix(test[features]),ntree_limit=xgb_model.best_ntree_limit+50)\n\nsubmission = pd.DataFrame({'id': test_ids, 'target': xgb_test_predictions1})\nsubmission.to_csv('before_oversampling_xgb.csv', index = False, float_format='%.5f')","7fcf145a":"def model_feature_importances(model):\n    trace = go.Scatter(\n        y = np.array(list(model.get_fscore().values())),\n        x = np.array(list(model.get_fscore().keys())),\n        mode='markers',\n        marker=dict(\n            sizemode = 'diameter',\n            sizeref = 1,\n            size = 13,\n            #size= model.feature_importances_,\n            #color = np.random.randn(500), #set color equal to a variable\n            color =  np.array(list(model.get_fscore().values())),\n            colorscale='Portland',\n            showscale=True\n        ),\n        text = np.array(list(model.get_fscore().keys()))\n    )\n    data = [trace]\n\n    layout= go.Layout(\n        autosize= True,\n        title= 'xgb Feature Importance',\n        hovermode= 'closest',\n         xaxis= dict(\n             ticklen= 5,\n             showgrid=False,\n            zeroline=False,\n            showline=False\n         ),\n        yaxis=dict(\n            title= 'Feature Importance',\n            showgrid=False,\n            zeroline=False,\n            ticklen= 5,\n            gridwidth= 2\n        ),\n        showlegend= False\n    )\n    fig = go.Figure(data=data, layout=layout)\n    fig.show()\n","6f448f2a":"model_feature_importances(xgb_model)","616b81b0":"xgb_importance = np.array(list(xgb_model.get_fscore().values()))\nxgb_features = np.array(list(xgb_model.get_fscore().keys()))\n\nx, y = (list(x) for x in zip(*sorted(zip(xgb_importance,xgb_features), reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Random Forest Feature importance',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of Feature importances',\n    width = 900, height = 2000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n#         domain=[0, 0.85],\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","dc6286ab":"new_features = pd.DataFrame(xgb_model.get_fscore(),index=['features_importance']).T\\\n                        .sort_values(by='features_importance',ascending=False)\\\n                        .iloc[0:38]\nnew_features ","e5b872de":"train_feature.shape","4f2a00ba":"new_train_feature = train_feature[new_features.index]\nnew_train_feature.head()","e48c5d85":"new_train_feature.shape","b92bd2b2":"params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n          'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n\nnrounds=200  \nkfold = 2\nskf = StratifiedKFold(n_splits=kfold, random_state=0)\n\nfor i, (train_index, test_index) in enumerate(skf.split(new_train_feature, train_target)):\n    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n    X_train, X_valid = new_train_feature.loc[train_index], new_train_feature.loc[test_index]\n    y_train, y_valid = train_target.loc[train_index], train_target.loc[test_index]\n    d_train = xgb.DMatrix(X_train, y_train) \n    d_valid = xgb.DMatrix(X_valid, y_valid) \n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    xgb_model2 = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n                          feval=gini_xgb, maximize=True, verbose_eval=100)","d4d9e958":"xgb_test_predictions2 = xgb_model2.predict(xgb.DMatrix(test[new_features.index]),ntree_limit=xgb_model2.best_ntree_limit+50)\n\nsubmission = pd.DataFrame({'id': test_ids, 'target': xgb_test_predictions2})\nsubmission.to_csv('before_oversampling_after_feature_choose_xgb.csv', index = False, float_format='%.5f')","dd3bde82":"def gini_lgb(preds, dtrain):\n    y = list(dtrain.get_label())\n    score = gini(y, preds) \/ gini(y, y)\n    return 'gini', score, True","92f23579":"# https:\/\/www.kaggle.com\/rshally\/porto-xgb-lgb-kfold-lb-0-282\n\n# xgb\n# params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n#         'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n\n# submission=test['id'].to_frame()\n# submission['target']=0\n\n# nrounds=200  # need to change to 2000\n# kfold = 2  # need to change to 5\n# skf = StratifiedKFold(n_splits=kfold, random_state=0)\n# for i, (train_index, test_index) in enumerate(skf.split(new_train_feature, train_target)):\n#     print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n#     X_train, X_valid = new_train_feature.loc[train_index], new_train_feature.loc[test_index]\n#     y_train, y_valid = train_target.loc[train_index], train_target.loc[test_index]\n#     d_train = xgb.DMatrix(X_train, y_train) \n#     d_valid = xgb.DMatrix(X_valid, y_valid) \n#     watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#     xgb_model3 = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n#                         feval=gini_xgb, maximize=True, verbose_eval=100)\n\n#     # \u7ed3\u5c3e \u9664\u4ee5 (2*kfold)\uff0c\u662f\u56e0\u4e3a\u8981\u5c06 xgb \u548c lgb \u53bb\u5e73\u5747\u7136\u540e\u5c06\u7ed3\u679c\u76f8\u52a0\u5408\u5e76\n#     submission['target'] += xgb_model3.predict(xgb.DMatrix(test[new_features.index]), \n#                         ntree_limit=xgb_model3.best_ntree_limit+50) \/ (2*kfold)\n    \n# submission.head(2)\n\n# # lgb\n# params = {'metric': 'auc', 'learning_rate' : 0.01, 'max_depth':10, 'max_bin':10,  'objective': 'binary', \n#         'feature_fraction': 0.8,'bagging_fraction':0.9,'bagging_freq':10,  'min_data': 500}\n\n# skf = StratifiedKFold(n_splits=kfold, random_state=1)\n# for i, (train_index, test_index) in enumerate(skf.split(new_train_feature, os_labels)):\n#     print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\n#     X_train, X_eval = new_train_feature.loc[train_index], new_train_feature.loc[test_index]\n#     y_train, y_eval = train_target.loc[train_index], train_target.loc[test_index]\n#     lgb_model = lgb.train(params, lgb.Dataset(X_train, label=y_train), nrounds, \n#                 lgb.Dataset(X_eval, label=y_eval), verbose_eval=100, \n#                 feval=gini_lgb, early_stopping_rounds=100)\n\n#     # \u7ed3\u5c3e \u9664\u4ee5 (2*kfold)\uff0c\u662f\u56e0\u4e3a\u8981\u5c06 xgb \u548c lgb \u53bb\u5e73\u5747\u7136\u540e\u5c06\u7ed3\u679c\u76f8\u52a0\u5408\u5e76\n#     submission['target'] += lgb_model.predict(test[new_features.index], \n#                         num_iteration=lgb_model.best_iteration) \/ (2*kfold)\n\n# submission.to_csv('before_oversampling_lgb+xgb.csv', index=False, float_format='%.5f') \n\n# submission.head(2)","121fbc0f":"# submission.to_csv('before_oversampling_after_feature_choose_xgb.csv', index = False, float_format='%.5f')","0176cfa5":"# params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n#           'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n\n# nrounds=200  \n# kfold = 2  \n# skf = StratifiedKFold(n_splits=kfold, random_state=0)\n\n# for i, (train_index, test_index) in enumerate(skf.split(os_features, os_labels)):\n#     print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n#     X_train, X_valid = os_features.loc[train_index], os_features.loc[test_index]\n#     y_train, y_valid = os_labels.loc[train_index], os_labels.loc[test_index]\n#     d_train = xgb.DMatrix(X_train, y_train) \n#     d_valid = xgb.DMatrix(X_valid, y_valid) \n#     watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#     xgb_model2 = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n#                           feval=gini_xgb, maximize=True, verbose_eval=100)\n","f794d20c":"# xgb_test_predictions = xgb_model2.predict(xgb.DMatrix(test[features]), \n#                         ntree_limit=xgb_model2.best_ntree_limit+50)","c745b3ba":"# submission = pd.DataFrame({'id': test_ids, 'target': xgb_test_predictions})\n# submission.to_csv('after_oversampling_xgb.csv', index = False, float_format='%.5f')","da8a831c":"# from sklearn.ensemble import RandomForestClassifier","34254835":"# os_features.drop(['id'],axis=1,inplace=True)\n# os_features_test.drop(['id'],axis=1,inplace=True)\n\n\n# rdf_clf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\n# rdf_clf.fit(os_features, os_labels)\n# features = os_features.columns.values\n# print(\"----- Training Done -----\")","8ec13b5f":"# RandomForestClassifier feature importances Scatter plot \n\n# def rdf_feature_importances(model):\n#     trace = go.Scatter(\n#         y = model.feature_importances_,\n#         x = features,\n#         mode='markers',\n#         marker=dict(\n#             sizemode = 'diameter',\n#             sizeref = 1,\n#             size = 13,\n#             #size= model.feature_importances_,\n#             #color = np.random.randn(500), #set color equal to a variable\n#             color = model.feature_importances_,\n#             colorscale='Portland',\n#             showscale=True\n#         ),\n#         text = features\n#     )\n#     data = [trace]\n\n#     layout= go.Layout(\n#         autosize= True,\n#         title= 'Random Forest Feature Importance',\n#         hovermode= 'closest',\n#          xaxis= dict(\n#              ticklen= 5,\n#              showgrid=False,\n#             zeroline=False,\n#             showline=False\n#          ),\n#         yaxis=dict(\n#             title= 'Feature Importance',\n#             showgrid=False,\n#             zeroline=False,\n#             ticklen= 5,\n#             gridwidth= 2\n#         ),\n#         showlegend= False\n#     )\n#     fig = go.Figure(data=data, layout=layout)\n#     fig.show()\n\n# rdf_feature_importances(model=rdf_clf)","7ddc9832":"# x, y = (list(x) for x in zip(*sorted(zip(rdf_clf.feature_importances_, features), \n#                                                             reverse = False)))\n# trace2 = go.Bar(\n#     x=x ,\n#     y=y,\n#     marker=dict(\n#         color=x,\n#         colorscale = 'Viridis',\n#         reversescale = True\n#     ),\n#     name='Random Forest Feature importance',\n#     orientation='h',\n# )\n\n# layout = dict(\n#     title='Barplot of Feature importances',\n#      width = 900, height = 2000,\n#     yaxis=dict(\n#         showgrid=False,\n#         showline=False,\n#         showticklabels=True,\n# #         domain=[0, 0.85],\n#     ))\n\n# fig1 = go.Figure(data=[trace2])\n# fig1['layout'].update(layout)\n# py.iplot(fig1, filename='plots')","8026b6ae":"# y_pred = rdf_clf.predict(os_features_test)\n\n# acc = accuracy_score(os_labels_test,y_pred)\n# recall = recall_score(os_labels_test,y_pred)\n\n# print('Accuracy: {:.3f}'.format(acc* 100.0))\n# print('recall: {:.3f}'.format(recall* 100.0))","b04f5e7d":"# model = XGBClassifier()\n\n\n# model.fit(os_features,os_labels)\n# y_pred = model.predict(os_features_test)\n\n# acc = accuracy_score(os_labels_test,y_pred)\n# recall = recall_score(os_labels_test,y_pred)\n\n# print('Accuracy: {:.3f}'.format(acc* 100.0))\n# print('recall: {:.3f}'.format(recall* 100.0))","525f2e3c":"# test_predictions = rdf_clf.predict(new_test)\n\n# submission = pd.DataFrame({'id': test_ids, 'target': test_predictions})\n# submission.to_csv('RandomForestClassifier_predict_1.csv', index = False)","75a7c4b3":"# submission.head()","854c5185":"\u968f\u673a\u68ee\u6797\u9884\u6d4b\u7ed3\u679c\uff1a","87c88f0a":"### \u4e0d\u8fdb\u884c\u91c7\u6837\uff0c\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9884\u6d4b\u67e5\u770b\u6a21\u578b\u8d28\u91cf\n\n* \u901a\u8fc7\u5bf9\u6bd4\u51c6\u786e\u5ea6\u548c\u53ec\u56de\u7387\n    \n* \u901a\u8fc7\u7ed8\u5236\u6df7\u6dc6\u77e9\u9635","17584d7f":"\u6211\u4eec\u53ef\u4ee5\u770b\u51fa \u5728 `bin` \u7c7b\u578b\u7684\u7279\u5f81\u4e2d `ps_ind_10_bin`,`ps_ind_11_bin`,`ps_ind_12_bin`,`ps_ind_13_bin` \u786e\u5b9e\u662f\u5f71\u54cd\u529b\u6700\u4f4e\u7684\u7279\u5f81","093e4e99":"### 1\u3001\u5bf9\u6bd4\u51c6\u786e\u5ea6\u548c\u53ec\u56de\u7387","09b96811":"### \u76ee\u6807\u53d8\u91cf\u68c0\u6d4b\n\n\u5bf9\u4e8e\u5206\u7c7b\u53d8\u91cf\u6211\u4eec\u9700\u8981\u8fdb\u884c\u76ee\u6807\u53d8\u91cf\u68c0\u6d4b\uff0c\u5982\u679c\u6570\u636e\u5b58\u5728\u4e25\u91cd\u7684\u4e0d\u5e73\u8861\uff0c\u9884\u6d4b\u5f97\u51fa\u7684\u7ed3\u8bba\u5f80\u5f80\u4e5f\u662f\u6709\u504f\u7684\uff0c\u5373\u5206\u7c7b\u7ed3\u679c\u4f1a\u504f\u5411\u4e8e\u8f83\u591a\u89c2\u6d4b\u7684\u7c7b\u3002\n","0213908a":"### \u4e8c\u8fdb\u5236\u6570\u636e\u68c0\u6d4b `bin`","4b4365bf":"### XGBoost\u8bad\u7ec3\u6a21\u578b\n\n\u4f7f\u7528 gini \u7cfb\u6570\u8bc4\u4f30 + \u4ea4\u53c9\u9a8c\u8bc1","1c715d59":"\u6211\u4eec\u80fd\u770b\u5230\uff0c\u5bf9\u4e8e`10_bin` `11_bin` `12_bin` `13_bin` \u57fa\u672c\u4e0a\u5168\u662f\u76ee\u6807\u503c\u90fd\u4e3a 0\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u521d\u6b65\u5224\u65ad\u8fd9\u51e0\u4e2a\u7279\u5f81\u53ef\u80fd\u5bf9\u6211\u4eec\u7684\u76ee\u6807\u503c\u9884\u6d4b\u8d77\u4e0d\u4e86\u4f5c\u7528\uff0c\u7b49\u4e00\u4e0b\u6211\u4eec\u4e5f\u53ef\u4ee5\u518d\u8fdb\u4e00\u6b65\u7684\u53bb\u9a8c\u8bc1\u3002","ada8c5d0":"### \u8fc7\u91c7\u6837\u524d\u6570\u636e\u8fdb\u884c\u8bad\u7ec3","267989ec":"### \u7279\u5f81\u9009\u62e9\n\n\u7ed8\u5236 xgb \u6a21\u578b\u7279\u5f81\u7684\u91cd\u8981\u6027\u56fe","e300d3af":"# <center>III. ML Approach","63502134":"# <center>II. Data Cleaning & Visualisation","91740768":"`Kfold = 2` and `nrounds = 200` \u63d0\u4ea4\u540e\uff1a`score=0.254` \n\n`Kfold = 5` and `nrounds = 2000` \u63d0\u4ea4\u540e\uff1a`score=0.2767` ","5fa70268":"\u66f4\u8fd1\u4e00\u6b65\uff0c\u6211\u4eec\u8fd8\u80fd\u901a\u8fc7\u7ed8\u5236\u67f1\u72b6\u56fe(\u6a2a)\u6765\u5bf9\u7279\u5f81\u91cd\u8981\u6027\u8fdb\u884c\u6548\u679c\u5c55\u793a","eff0d359":"### \u7279\u5f81\u76f8\u5173\u6027\u68c0\u6d4b","57dabbb2":"### RandomForestClassifier \u8bad\u7ec3\u6570\u636e\n\n\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\uff1a\n\n* 1\u3001\u4f7f\u7528 RandomForestClassifier \u6765\u8fdb\u884c\u8bad\u7ec3\u6570\u636e\n\n* 2\u3001\u7ed8\u5236\u7279\u5f81\u7684\u91cd\u8981\u6027\u56fe\n\n* 3\u3001\u5e76\u4e14\u8fdb\u4e00\u6b65\u8fdb\u884c\u7279\u5f81\u9009\u62e9","43a98289":"1\u3001\u7279\u5f81\u548c\u76ee\u6807\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u68c0\u6d4b","2c7c7e3e":"\u66f4\u8fd1\u4e00\u6b65\uff0c\u6211\u4eec\u8fd8\u80fd\u901a\u8fc7\u7ed8\u5236\u67f1\u72b6\u56fe(\u6a2a)\u6765\u5bf9\u7279\u5f81\u91cd\u8981\u6027\u8fdb\u884c\u6548\u679c\u5c55\u793a","4c4eb79e":"# <h1><center> Predictive Analysis - Porto Seguro\u2019s Safe Driver Prediction | Kaggle","a515dd56":"### \u67e5\u770b2D\u6570\u636e\u7684\u5206\u5e03","435e83ef":"### 2\u3001\u7ed8\u5236\u6df7\u6dc6\u77e9\u9635\n\n\u6df7\u6dc6\u77e9\u9635\u7528\u6cd5\u793a\u4f8b\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u5206\u7c7b\u5668\u8f93\u51fa\u7684\u8d28\u91cf\u3002 \u5bf9\u89d2\u7ebf\u5143\u7d20\u8868\u793a\u9884\u6d4b\u6807\u7b7e\u7b49\u4e8e\u771f\u5b9e\u6807\u7b7e\u7684\u70b9\u6570\uff0c\u800c\u975e\u5bf9\u89d2\u7ebf\u5143\u7d20\u5219\u662f\u5206\u7c7b\u5668\u672a\u6b63\u786e\u6807\u8bb0\u7684\u5143\u7d20\u3002 \u6df7\u6dc6\u77e9\u9635\u7684\u5bf9\u89d2\u7ebf\u503c\u8d8a\u9ad8\uff0c\u8868\u793a\u5bf9\u6570\u8d8a\u591a\u8d8a\u597d\u3002\u6df7\u6dc6\u77e9\u9635\u7528\u6cd5\u793a\u4f8b\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u5206\u7c7b\u5668\u8f93\u51fa\u7684\u8d28\u91cf\u3002 \u5bf9\u89d2\u7ebf\u5143\u7d20\u8868\u793a\u9884\u6d4b\u6807\u7b7e\u7b49\u4e8e\u771f\u5b9e\u6807\u7b7e\u7684\u70b9\u6570\uff0c\u800c\u975e\u5bf9\u89d2\u7ebf\u5143\u7d20\u5219\u662f\u5206\u7c7b\u5668\u672a\u6b63\u786e\u6807\u8bb0\u7684\u5143\u7d20\u3002 \u6df7\u6dc6\u77e9\u9635\u7684\u5bf9\u89d2\u7ebf\u503c\u8d8a\u9ad8\uff0c\u8868\u793a\u5bf9\u6570\u8d8a\u591a\u8d8a\u597d\u3002","dd3b6cd0":"2\u3001\u7279\u5f81\u4e0e\u7279\u5f81\u4e4b\u95f4\u8fdb\u884c\u76f8\u5173\u6027\u68c0\u6d4b","328d2bb6":"\u53ef\u4ee5\u770b\u51fa `ps_car_03_cat` \u548c `ps_car_05_cat` \u6240\u5360\u7f3a\u5931\u503c\u6bd4\u4f8b\u5f88\u9ad8","1acd3e84":"### xgb + lgb \u5806\u53e0\u8bad\u7ec3\u9884\u6d4b\n\n\u5bf9\u9009\u51fa\u6765\u7684\u7279\u5f81\u8fdb\u884c\u8bad\u7ec3","30713444":"3\u3001\u5220\u9664\u5171\u7ebf\u7279\u5f81","6c88e4b4":"\u4ece\u56fe\u4e2d\u6211\u4eec\u80fd\u53d1\u73b0\u4e2a\u522b\u7279\u5f81\u91cd\u8981\u6027\u975e\u5e38\u4f4e\uff0c\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c\u5254\u9664","5b26b5d0":"### \u4f7f\u7528 SMOTE \u8fdb\u884c\u8fc7\u91c7\u6837","c04c546f":"# <center> IV. Extend...","644936dd":"\u6211\u4eec\u53ef\u4ee5\u770b\u51fa\u76f8\u6bd4\u4e8e\u4e4b\u524d\u7684\u4e0d\u5e73\u8861\u6570\u636e\uff0c\u8fc7\u91c7\u6837\u540e\u7684\u6570\u636e\uff0c\u53ec\u56de\u7387\u5f97\u5230\u4e86\u660e\u663e\u7684\u4e0a\u5347","38ea416c":"## \u6570\u636e\u4e0d\u5e73\u8861\n\n\u53ef\u4ee5\u53d1\u73b0\u6807\u7b7e\u4e4b\u524d\u5b58\u5728\u4e0d\u5e73\u8861\u7684\u72b6\u6001\uff0c\u5982\u679c\u6570\u636e\u5b58\u5728\u4e25\u91cd\u7684\u4e0d\u5e73\u8861\uff0c\u9884\u6d4b\u5f97\u51fa\u7684\u7ed3\u8bba\u5f80\u5f80\u4e5f\u662f\u6709\u504f\u7684\uff0c\u5373\u5206\u7c7b\u7ed3\u679c\u4f1a\u504f\u5411\u4e8e\u8f83\u591a\u89c2\u6d4b\u7684\u7c7b\u3002\n\n\u6bd4\u5982\u6211\u4eec\u4f7f\u7528\u51c6\u786e\u7387\u6765\u8fdb\u884c\u6a21\u578b\u7684\u8bc4\u4f30\uff0c\u5373\u4f7f\u6211\u4eec\u5168\u90e8\u9884\u6d4b\u6210 `target == 0`\uff0c\u90a3\u4e48\u4e5f\u6709\u5f88\u9ad8\u7684\u51c6\u786e\u7387: `573518\/(573518+21694)=0.96`\u3002\n\n\u6240\u4ee5\u5bf9\u4e8e\u5206\u7c7b\u4e0d\u5e73\u8861\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c\u5982\u4e0b\u64cd\u4f5c\uff1a\n\n### \u6b20\u91c7\u6837\uff1a\n\n\u968f\u673a\u6b20\u91c7\u6837\uff08\u4e0b\u91c7\u6837\uff09\u7684\u76ee\u6807\u662f\u901a\u8fc7\u968f\u673a\u5730\u6d88\u9664\u5360\u591a\u6570\u7684\u7c7b\u7684\u6837\u672c\u6765\u5e73\u8861\u7c7b\u5206\u5e03\uff1b\u76f4\u5230\u591a\u6570\u7c7b\u548c\u5c11\u6570\u7c7b\u7684\u5b9e\u4f8b\u5b9e\u73b0\u5e73\u8861\uff0c\u76ee\u6807\u624d\u7b97\u8fbe\u6210\u3002\n\n* `\u968f\u673a\u6b20\u91c7\u6837\uff08\u4e0b\u91c7\u6837\uff09`\u7684\u76ee\u6807\u662f\u901a\u8fc7\u968f\u673a\u5730\u6d88\u9664\u5360\u591a\u6570\u7684\u7c7b\u7684\u6837\u672c\u6765\u5e73\u8861\u7c7b\u5206\u5e03\uff1b\u76f4\u5230\u591a\u6570\u7c7b\u548c\u5c11\u6570\u7c7b\u7684\u5b9e\u4f8b\u5b9e\u73b0\u5e73\u8861\uff0c\u76ee\u6807\u624d\u7b97\u8fbe\u6210\u3002\n\n* `\u968f\u673a\u4e0b\u91c7\u6837\u7684\u4f18\u70b9\uff1a`\n    \n    \u5b83\u53ef\u4ee5\u63d0\u5347\u8fd0\u884c\u65f6\u95f4\uff1b\u5e76\u4e14\u5f53\u8bad\u7ec3\u6570\u636e\u96c6\u5f88\u5927\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7\u51cf\u5c11\u6837\u672c\u6570\u91cf\u6765\u89e3\u51b3\u5b58\u50a8\u95ee\u9898\u3002\n\n* `\u968f\u673a\u4e0b\u91c7\u6837\u7684\u7f3a\u70b9\uff1a`\n    \n    \u5b83\u4f1a\u4e22\u5f03\u5bf9\u6784\u5efa\u89c4\u5219\u5206\u7c7b\u5668\u5f88\u91cd\u8981\u7684\u6709\u4ef7\u503c\u7684\u6f5c\u5728\u4fe1\u606f\u3002\n\n    \u88ab\u968f\u673a\u6b20\u91c7\u6837\u9009\u53d6\u7684\u6837\u672c\u53ef\u80fd\u5177\u6709\u504f\u5dee\u3002\u5b83\u4e0d\u80fd\u51c6\u786e\u4ee3\u8868\u5927\u591a\u6570\u3002\u4ece\u800c\u5728\u5b9e\u9645\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e0d\u7cbe\u786e\u7684\u7ed3\u679c\u3002\u679c\u3002\n\n\n### \u8fc7\u91c7\u6837\n\n\n* `\u968f\u673a\u8fc7\u91c7\u6837` \u901a\u8fc7\u968f\u673a\u590d\u5236\u5c11\u6570\u7c7b\u6765\u589e\u52a0\u5176\u4e2d\u7684\u5b9e\u4f8b\u6570\u91cf\uff0c\u4ece\u800c\u53ef\u589e\u52a0\u6837\u672c\u4e2d\u5c11\u6570\u7c7b\u7684\u4ee3\u8868\u6027\u3002\n\n* `\u968f\u673a\u8fc7\u91c7\u6837\u7684\u4f18\u70b9\uff1a`\n\n    \u4e0e\u6b20\u91c7\u6837\u4e0d\u540c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4f1a\u5e26\u6765\u4fe1\u606f\u635f\u5931\u3002\n\n    \u8868\u73b0\u4f18\u4e8e\u6b20\u91c7\u6837\u3002\n\n* `\u968f\u673a\u8fc7\u91c7\u6837\u7684\u7f3a\u70b9\uff1a`\n    \n    \u7531\u4e8e\u590d\u5236\u5c11\u6570\u7c7b\u4e8b\u4ef6\uff0c\u5b83\u52a0\u5927\u4e86\u8fc7\u62df\u5408\u7684\u53ef\u80fd\u6027\u3002\n    \n\n\u672c notebook \u5c06\u91c7\u7528 SMOTE \u6765\u8fdb\u884c\u8fc7\u91c7\u6837","2d2f3321":"### \u8fc7\u91c7\u6837\u540e\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\n\noversampling good or bad?\n\n\u5bf9\u91c7\u6837\u540e\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u53d1\u73b0\u9884\u6d4b\u51fa\u6765\u7684\u7ed3\u679c\u5206\u6570\u5f88\u4f4e","9b135b4c":"### \u7279\u5f81\u9009\u62e9","5439bd9f":"### \u5bf9\u6570\u636e\u8fdb\u884c\u9884\u6d4b\n\n\u4f7f\u7528 \u4ea4\u53c9\u9a8c\u8bc1 \u653e\u6cd5\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3","e79c0103":"# <h1><center> I. Importation & Missing Value Check","89746033":"\u53ef\u4ee5\u770b\u5230\uff0c\u5373\u4f7f\u4f60\u7684\u51c6\u786e\u7387\u975e\u5e38\u9ad8\uff0c\u4f46\u662f\u53ec\u56de\u7387\u5374\u975e\u5e38\u4f4e    "}}