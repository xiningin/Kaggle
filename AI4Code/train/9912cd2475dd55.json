{"cell_type":{"645398f5":"code","ee5f6454":"code","2c1d4762":"code","d383416f":"code","e795d0e4":"code","3587c415":"code","8af2ff41":"code","ed0a8dee":"code","44637ddd":"markdown","36dde5c0":"markdown","ef68f304":"markdown","318bd6bd":"markdown","6cb581af":"markdown","c03110d3":"markdown"},"source":{"645398f5":"!ls ..\/input\/sarcasm\/","ee5f6454":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","2c1d4762":"train_df = pd.read_csv('..\/input\/sarcasm\/train-balanced-sarcasm.csv')","d383416f":"train_df.head()","e795d0e4":"train_df.info()","3587c415":"train_df.dropna(subset=['comment'], inplace=True)","8af2ff41":"train_df['label'].value_counts()","ed0a8dee":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)","44637ddd":"Some comments are missing, so we drop the corresponding rows.","36dde5c0":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","ef68f304":"We notice that the dataset is indeed balanced","318bd6bd":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words\/bigrams which a most predictive of sarcasm (you can use [eli5](https:\/\/github.com\/TeamHG-Memex\/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n\n## Links:\n  - Machine learning library [Scikit-learn](https:\/\/scikit-learn.org\/stable\/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-2-classification) and its applications to [text classification](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https:\/\/github.com\/TeamHG-Memex\/eli5) to explain model predictions","6cb581af":"## <center> Assignment 4. Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https:\/\/arxiv.org\/abs\/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https:\/\/www.kaggle.com\/danofer\/sarcasm).\n\nSarcasm detection is easy. \n<img src=\"https:\/\/habrastorage.org\/webt\/1f\/0d\/ta\/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" \/>","c03110d3":"We split data into training and validation parts."}}