{"cell_type":{"e1db4e4c":"code","cfde36e2":"code","0f24b4ff":"code","4b9f3ea8":"code","e9684183":"code","9f88b5e9":"code","bd534c81":"code","9a123aa4":"code","373fd6d4":"code","67c27c30":"code","1b9dcf4f":"code","f173bf11":"code","7dd3871e":"code","6995771f":"code","a7de4bda":"code","b4f2ae54":"code","f0e45253":"code","f2807448":"code","f5c22361":"code","d5dcd92a":"code","f1a59ced":"code","caaeecaf":"code","9b81f999":"code","893f3f91":"code","4cf583a2":"code","8bc221e3":"code","c2fa1aeb":"code","725c4154":"code","574d6ed7":"code","91acc74b":"code","05be0e47":"code","86dd626e":"code","a55699a2":"code","2ccc441b":"code","10c1169c":"code","bc0c5c2a":"code","f5d0063d":"code","3a3fac5b":"code","0644ee8b":"code","652bfdcf":"code","751306c6":"code","ed251126":"code","62545b41":"code","3c6d5a72":"code","4f466992":"code","7fe0b96f":"code","e61b75aa":"code","96682701":"code","e25c8b40":"code","a06da619":"code","575f5ba0":"code","e18f6975":"code","b30c797b":"code","9734533f":"code","1d93e19f":"markdown","97d5f40c":"markdown","c5e370ec":"markdown","46d56cc0":"markdown","a049e875":"markdown","d265cd65":"markdown","5afc0224":"markdown","994ff3c2":"markdown","362198a4":"markdown","727160d5":"markdown","95084cc2":"markdown","0389fc8b":"markdown","ce9353e3":"markdown","d8cba141":"markdown","12942968":"markdown","40f58419":"markdown","2a7a49a5":"markdown","9a6169ea":"markdown","f3344b08":"markdown","6ab73ee2":"markdown","d6769062":"markdown","36fcb844":"markdown","681c8c8e":"markdown","1797ec21":"markdown","89dd8f60":"markdown","b4b82fba":"markdown","4efe84a9":"markdown","9aa761de":"markdown","671ea91f":"markdown","b1d680da":"markdown","416803be":"markdown","c4214eb0":"markdown","2034ebfa":"markdown","059d1015":"markdown","59bebd0a":"markdown","04dbc7b9":"markdown","c541a2fe":"markdown","46add1ef":"markdown","8237805f":"markdown","ddb8188c":"markdown","cc7e33ce":"markdown","297a2e22":"markdown","792ad992":"markdown","953b5d0d":"markdown","89b593eb":"markdown","511ff45d":"markdown","d57d1032":"markdown","2c9cd003":"markdown","09b8bfb2":"markdown","88b1fc6b":"markdown","f536618e":"markdown","fdd3e0cb":"markdown","5fa4ccf7":"markdown","bb0eaa75":"markdown","1974d06d":"markdown","1dfb66e1":"markdown","9eb5d84c":"markdown","d146a3bf":"markdown","3f28b34e":"markdown","8765b995":"markdown"},"source":{"e1db4e4c":"from sklearn.metrics import log_loss\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pprint import pprint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,r2_score\nimport warnings\nfrom mlxtend.classifier import StackingClassifier\nimport missingno as msno\nfrom sklearn.ensemble import VotingClassifier\nimport shap\nshap.initjs()\nimport lime\nfrom lime import lime_tabular\nwarnings.simplefilter('ignore')\nimport os\nplt.style.use('fivethirtyeight')\nplt.style.use('dark_background')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","cfde36e2":"insurance_df = pd.read_csv('..\/input\/prudential-life-insurance-assessment\/train.csv.zip', index_col='Id')\ninsurance_df.head()","0f24b4ff":"insurance_df.shape","4b9f3ea8":"insurance_df['Response'].value_counts()","e9684183":"sns.countplot(x=insurance_df['Response']);","9f88b5e9":"#Combining the Categores to 3 categories\ninsurance_df['Modified_Response']  = insurance_df['Response'].apply(lambda x : 0 if x<=7 and x>=0 else (1 if x==8 else -1))","bd534c81":"sns.countplot(x= insurance_df['Modified_Response']);","9a123aa4":"# Dropping old response columns\ninsurance_df.drop('Response',axis = 1, inplace=True)","373fd6d4":"# Making lists with categorical and numerical features.\ncategorical =  [col for col in insurance_df.columns if insurance_df[col].dtype =='object']\n\nnumerical = categorical =  [col for col in insurance_df.columns if insurance_df[col].dtype !='object']","67c27c30":"# Doing count plots for categorical\nfor col in categorical:\n    counts = insurance_df[col].value_counts().sort_index()\n    if len(counts) > 10 and len(counts) < 50 :\n      fig = plt.figure(figsize=(30, 10))\n    elif len(counts) >50 :\n      continue\n    else:\n      fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    counts.plot.bar(ax = ax, color='steelblue')\n    ax.set_title(col + ' counts')\n    ax.set_xlabel(col) \n    ax.set_ylabel(\"Frequency\")\nplt.show()","1b9dcf4f":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_1'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_1'], ax=axes[1])","f173bf11":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_4'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_4'], ax=axes[1])","7dd3871e":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_6'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_6'], ax=axes[1])","6995771f":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Family_Hist_4'], ax=axes[0])\nsns.boxplot(insurance_df['Family_Hist_4'], ax=axes[1])","a7de4bda":"# I just checked correlated feature with greater than .8 here \ncorr = insurance_df.corr()\ncorr_greater_than_80 = corr[corr>=.8]\ncorr_greater_than_80\n","b4f2ae54":"plt.figure(figsize=(12,8))\nsns.heatmap(corr_greater_than_80, cmap=\"Reds\");","f0e45253":"#setting max columns to 200\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)","f2807448":"#checking percentage of missing values in a column\nmissing_val_count_by_column = insurance_df.isnull().sum()\/len(insurance_df)\n\nprint(missing_val_count_by_column[missing_val_count_by_column > 0.4].sort_values(ascending=False))","f5c22361":"# Dropping all columns in which greater than 40 percent null values\ninsurance_df = insurance_df.dropna(thresh=insurance_df.shape[0]*0.4,how='all',axis=1)","d5dcd92a":"# Does not contain important information\ninsurance_df.drop('Product_Info_2',axis=1,inplace=True)","f1a59ced":"# Data for all the independent variables\nX = insurance_df.drop(labels='Modified_Response',axis=1)\n\n# Data for the dependent variable\nY = insurance_df['Modified_Response']","caaeecaf":"# Filling remaining missing values with mean\nX = X.fillna(X.mean())","9b81f999":"# Train-test split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.25, random_state=1)","893f3f91":"# Check the shape of train dataset\nprint(X_train.shape,Y_train.shape)\n\n# Check the shape of test dataset\nprint(X_test.shape, Y_test.shape)","4cf583a2":"# Utility Functions\ndef check_scores(model, X_train, X_test ):\n  # Making predictions on train and test data\n\n  train_class_preds = model.predict(X_train)\n  test_class_preds = model.predict(X_test)\n\n\n  # Get the probabilities on train and test\n  train_preds = model.predict_proba(X_train)[:,1]\n  test_preds = model.predict_proba(X_test)[:,1]\n\n\n  # Calculating accuracy on train and test\n  train_accuracy = accuracy_score(Y_train,train_class_preds)\n  test_accuracy = accuracy_score(Y_test,test_class_preds)\n\n  print(\"The accuracy on train dataset is\", train_accuracy)\n  print(\"The accuracy on test dataset is\", test_accuracy)\n  print()\n  # Get the confusion matrices for train and test\n  train_cm = confusion_matrix(Y_train,train_class_preds)\n  test_cm = confusion_matrix(Y_test,test_class_preds )\n\n  print('Train confusion matrix:')\n  print( train_cm)\n  print()\n  print('Test confusion matrix:')\n  print(test_cm)\n  print()\n\n  # Get the roc_auc score for train and test dataset\n  train_auc = roc_auc_score(Y_train,train_preds)\n  test_auc = roc_auc_score(Y_test,test_preds)\n\n  print('ROC on train data:', train_auc)\n  print('ROC on test data:', test_auc)\n  \n  # Fscore, precision and recall on test data\n  f1 = f1_score(Y_test, test_class_preds)\n  precision = precision_score(Y_test, test_class_preds)\n  recall = recall_score(Y_test, test_class_preds) \n  \n  \n  #R2 score on train and test data\n  train_log = log_loss(Y_train,train_preds)\n  test_log = log_loss(Y_test, test_preds)\n\n  print()\n  print('Train log loss:', train_log)\n  print('Test log loss:', test_log)\n  print()\n  print(\"F score is:\",f1 )\n  print(\"Precision is:\",precision)\n  print(\"Recall is:\", recall)\n  return model, train_auc, test_auc, train_accuracy, test_accuracy,f1, precision,recall, train_log, test_log\n\n\ndef check_importance(model, X_train):\n  #Checking importance of features\n  importances = model.feature_importances_\n  \n  #List of columns and their importances\n  importance_dict = {'Feature' : list(X_train.columns),\n                    'Feature Importance' : importances}\n  #Creating a dataframe\n  importance_df = pd.DataFrame(importance_dict)\n  \n  #Rounding it off to 2 digits as we might get exponential numbers\n  importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n  return importance_df.sort_values(by=['Feature Importance'],ascending=False)\n\ndef grid_search(model, parameters, X_train, Y_train):\n  #Doing a grid\n  grid = GridSearchCV(estimator=model,\n                       param_grid = parameters,\n                       cv = 2, verbose=2, scoring='roc_auc')\n  #Fitting the grid \n  grid.fit(X_train,Y_train)\n  print()\n  print()\n  # Best model found using grid search\n  optimal_model = grid.best_estimator_\n  print('Best parameters are: ')\n  pprint( grid.best_params_)\n\n  return optimal_model\n\n\n\n# This function will show how a feature is pushing towards 0 or 1\ndef interpret_with_lime(model, X_test):\n  # New data\n  interpretor = lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X_train),\n    feature_names=X_train.columns,\n    mode='classification')\n  \n\n  exp = interpretor.explain_instance(\n      data_row=X_test.iloc[10], \n      predict_fn=model.predict_proba\n  )\n\n  exp.show_in_notebook(show_table=True)\n\n# This gives feature importance\ndef plot_feature_importance(model, X_train):\n  # PLotting features vs their importance factors\n  fig = plt.figure(figsize = (15, 8))\n  \n  # Extracting importance values\n  values =check_importance(model, X_train)[check_importance(model, X_train)['Feature Importance']>0]['Feature Importance'].values\n  \n  \n  # Extracting importance features\n  features = check_importance(model, X_train)[check_importance(model, X_train)['Feature Importance']>0]['Feature'].values\n\n  plt.bar(features, values, color ='blue',\n          width = 0.4)\n  plt.xticks( rotation='vertical')\n  plt.show()","8bc221e3":"\n# Number of trees\nn_estimators = [50,80,100]\n\n# Maximum depth of trees\nmax_depth = [4,6,8]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [50,100,150]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [40,50]\n\n# Hyperparameter Grid\nrf_parameters = {'n_estimators' : n_estimators,\n              'max_depth' : max_depth,\n              'min_samples_split' : min_samples_split,\n              'min_samples_leaf' : min_samples_leaf}\n\npprint(rf_parameters)\n\n#finding the best model\nrf_optimal_model = grid_search(RandomForestClassifier(), rf_parameters, X_train, Y_train)","c2fa1aeb":"# Getting scores from all the metrices\nrf_model, rf_train_auc, rf_test_auc, rf_train_accuracy, rf_test_accuracy,rf_f1, rf_precision,rf_recall,rf_train_log, rf_test_log = check_scores(rf_optimal_model, X_train, X_test )","725c4154":"# Getting the feature importance for all the features\ncheck_importance(rf_model, X_train)","574d6ed7":"# PLotting only those features which are contributing something\nplot_feature_importance(rf_model, X_train)","91acc74b":"\n# Interpretting the model using lime\ninterpret_with_lime(rf_model,X_test)","05be0e47":"\n# Interpretting the model using shaply\nX_shap=X_train\n\nrf_explainer = shap.TreeExplainer(rf_model)\nrf_shap_values = rf_explainer.shap_values(X_shap)\nshap.summary_plot(rf_shap_values, X_shap, plot_type=\"bar\")","86dd626e":"# Plotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Wt','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, rf_shap_values[0], X_train)","a55699a2":"#finding the best model\ngb_parameters ={\n    \"n_estimators\":[5,50,250],\n    \"max_depth\":[1,3,5,7],\n    \"learning_rate\":[0.01,0.1,1]\n}\n\npprint(gb_parameters)\n\ngb_optimal_model = grid_search(GradientBoostingClassifier(), gb_parameters, X_train, Y_train)","2ccc441b":"# Getting the scpres for all the score metrics used here\ngb_model, gb_train_auc, gb_test_auc, gb_train_accuracy, gb_test_accuracy,gb_f1, gb_precision,gb_recall,gb_train_log, gb_test_log = check_scores(gb_optimal_model, X_train, X_test )","10c1169c":"# Getting feature importance\ncheck_importance(gb_model, X_train)","bc0c5c2a":"# PLotting only those features which are contributing something\nplot_feature_importance(gb_model, X_train)","f5d0063d":"# Interpretting the model using lime\ninterpret_with_lime(gb_model,X_test)","3a3fac5b":"\n# Interpretting the model using shaply\nX_shap=X_train\n\ngb_explainer = shap.TreeExplainer(gb_model)\ngb_shap_values = gb_explainer.shap_values(X_shap)\nshap.summary_plot(gb_shap_values, X_shap, plot_type=\"dot\")","0644ee8b":"#PLotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Product_Info_4','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, gb_shap_values, X_train)","652bfdcf":"# Parameter grid for xgboost\nxgb_parameters = {'max_depth': [1,3,5], 'n_estimators': [2,5,10], 'learning_rate': [.01 , .1, .5]}\nprint('XGB parameters areL:')\npprint(xgb_parameters)\n#finding the best model\nxgb_optimal_model = grid_search(XGBClassifier(), xgb_parameters, X_train, Y_train)\n\n\n","751306c6":"# Getting the scores for all the score metrics used here\nxgb_model, xgb_train_auc, xgb_test_auc, xgb_train_accuracy, xgb_test_accuracy,xgb_f1, xgb_precision,xgb_recall,xgb_train_log, xgb_test_log= check_scores(xgb_optimal_model, X_train, X_test )","ed251126":"# Getting feature importance\n\ncheck_importance(xgb_model, X_train)","62545b41":"\n# Interpretting the model using shaply\n\nxgb_explainer = shap.TreeExplainer(xgb_model)\nxgb_shap_values = xgb_explainer.shap_values(X_shap)\nshap.summary_plot(xgb_shap_values, X_shap, plot_type=\"dot\")","3c6d5a72":"#PLotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Product_Info_4','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, xgb_shap_values, X_train)","4f466992":"\n# Parameter grid for Logistic Regression\nsolvers = ['lbfgs']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\nlr_parameters = dict(solver=solvers,penalty=penalty,C=c_values)# define grid search\n\n#finding the best model\nlr_optimal_model = grid_search(LogisticRegression( max_iter=5000), lr_parameters, X_train, Y_train)\n\n","7fe0b96f":"# Getting the scores for all the score metrics used here\n\nlr_model, lr_train_auc, lr_test_auc, lr_train_accuracy, lr_test_accuracy,lr_f1, lr_precision, lr_recall,lr_train_log, lr_test_log = check_scores(lr_optimal_model, X_train, X_test )","e61b75aa":"# Making a dataframe with coefficients and the feature names respectively\nimportance_df_lr = pd.concat([ pd.DataFrame(data =((X_train.columns).values).reshape(-1,1), columns = ['Feature']), pd.DataFrame(data =np.round(lr_optimal_model.coef_,2).reshape(-1,1), columns = ['Feature Importance'])], axis=1 )\nimportance_df_lr.sort_values(by=['Feature Importance'],ascending=False, inplace = True)\nimportance_df_lr","96682701":"# Plotting feature vs importance\nfig = plt.figure(figsize = (15, 8))\n\nvalues =importance_df_lr[importance_df_lr['Feature Importance']>0]['Feature Importance'].values\n\nfeatures = importance_df_lr[importance_df_lr['Feature Importance']>0]['Feature'].values\n\nplt.bar(features, values, color ='blue',\n          width = 0.4)\nplt.xticks( rotation='vertical')\nplt.show()","e25c8b40":"\n# Interpretting the model using lime\ninterpret_with_lime(lr_model,X_test)","a06da619":"# Appending all the models to estimators list\nestimators = []\n\nestimators.append(('logistic', lr_optimal_model))\nestimators.append(('XGB', xgb_optimal_model))\nestimators.append(('GB', gb_optimal_model))\nestimators.append(('rf', rf_optimal_model))\n\n# create the voting model\nvoting_model = VotingClassifier(estimators, voting='soft')\n\nvoting_model.fit(X_train, Y_train)\n","575f5ba0":"# Getting all the scores and errors\nvoting_model, voting_train_auc, voting_test_auc, voting_train_accuracy, voting_test_accuracy, voting_f1, voting_precision, voting_recall, voting_train_log, voting_test_log = check_scores(voting_model, X_train, X_test )","e18f6975":"#Building a stacked classifier\nstacked_classifier = StackingClassifier(classifiers =[lr_optimal_model, xgb_optimal_model, gb_model], meta_classifier = RandomForestClassifier(), use_probas = True, use_features_in_secondary = True)\n\n# training of stacked model\nstacked_model = stacked_classifier.fit(X_train, Y_train)   \n","b30c797b":"stacked_model, stacked_train_auc, stacked_test_auc, stacked_train_accuracy, stacked_test_accuracy, stacked_f1, stacked_precision, stacked_recall, stacked_train_log, stacked_test_log = check_scores(stacked_model, X_train, X_test )","9734533f":"# Making a dataframe of all the scores for every model\n\nscores_ = [(\"Random Forest\", rf_train_auc, rf_test_auc, rf_train_accuracy, rf_test_accuracy,rf_train_log, rf_test_log,rf_f1, rf_precision, rf_recall),\n(\"Gradient Boosting\",  gb_train_auc, gb_test_auc, gb_train_accuracy, gb_test_accuracy,gb_train_log, gb_test_log,gb_f1, gb_precision,gb_recall,),\n(\"XG Boost\", xgb_train_auc, xgb_test_auc, xgb_train_accuracy, xgb_test_accuracy,xgb_train_log, xgb_test_log,xgb_f1, xgb_precision, xgb_recall),\n(\"Logistic Regression\", lr_train_auc, lr_test_auc, lr_train_accuracy, lr_test_accuracy,lr_train_log, lr_test_log,lr_f1, lr_precision, lr_recall,),\n(\"Voting Classifier\", voting_train_auc, voting_test_auc, voting_train_accuracy, voting_test_accuracy, voting_train_log, voting_test_log, voting_f1, voting_precision, voting_recall),\n(\"Stacked Model\", stacked_train_auc, stacked_test_auc, stacked_train_accuracy, stacked_test_accuracy, stacked_train_log, stacked_test_log, stacked_f1, stacked_precision, stacked_recall)]\n\nScores_ =pd.DataFrame(data = scores_, columns=['Model Name', 'Train ROC', 'Test ROC', 'Train Accuracy', 'Test Accuracy', 'Train Log Loss','Test Log Loss','F-Score', 'Precision','Recall',])\nScores_.set_index('Model Name', inplace = True)\n\nScores_\n","1d93e19f":"# **Making categorical and numerical columns list**","97d5f40c":"### **Using Lime**","c5e370ec":"> Right skewed.\n\n> Outliers can be seen.","46d56cc0":"# **Shape**","a049e875":"#### **Findings**\n\n> Medical keyword 15,medical history 9,  Wt, medical history 3 all pushing towards 1.\n\n> Orange ones are pusing towards 1.","d265cd65":"## **Dependence Plots**","5afc0224":"> For product info 4 and wt we see some interesting trend","994ff3c2":"# **Feature Importance For XGBoost**","362198a4":"# **Model Interpretability for XGBoost**","727160d5":"### **CONCLUSION:**\n\n> BMI, weight, Medical_History_23, Medical_History_4 and Medical_Keyword_15 seems to be important features according to random forest.\n\n> Also, only these features are contributing to the model prediction. Some features can be elmininated which are not contributing on further investigation.","95084cc2":"# **Models And Their Accuracies**","0389fc8b":"### **Using Shap**","ce9353e3":"# **Final Results**\n\n> **Gradient Boosting, Voting Classifier and Stacked models are performing really well. Their train and test errors and also the roc scores and f scores are really close and good.**","d8cba141":"# **Model Interpretability for logistic regression**","12942968":"# **Shapes of Train and Test Data**","40f58419":"## **Using Lime**","2a7a49a5":"> **Still some imbalance can be seen**","9a6169ea":"# **Feature Importance For Logistic Regression**","f3344b08":"# **Removing old target variable**","6ab73ee2":"### **Findings**\n\n> With high medical history 23 and low bmi we get class 1","d6769062":"> Again BMI is pushing towards class 0.\n\n> MEdical history 4 pushing towards class 1.","36fcb844":"### **Using Lime**","681c8c8e":"# **Removing unimportant column**","1797ec21":"# **Filling Remaining Missing Values**","89dd8f60":"# **XGBOOST**","b4b82fba":"# **Plotting only those features which are contributing something**\n","4efe84a9":"# **Feature Importance For Random Forest**","9aa761de":"#### **Conclusion**\n\n> **And again the same pattern when doing feature importance**","671ea91f":"# **Train Test Split**","b1d680da":"# **Gradient Boosting**\n\n\n","416803be":"# **Visualizations On Categorical Features**","c4214eb0":"# **Importing Modules**\n","2034ebfa":"# **X and Y split**","059d1015":"### **Conclusion:**\n\n> Same trend is seen here.\n\n> They all are giving similar scores also so it could be that same features are contributing the most thus similar scores.\n","59bebd0a":"# **Processing Target Variable**","04dbc7b9":"# **Logistic Regression**","c541a2fe":"**Response 8 has highest values and 3 has the least**","46add1ef":"## **Model Interpretability For Random Forest**\n\n\n\n\n","8237805f":"## **Dependence Plots**","ddb8188c":"### **Using Shap**","cc7e33ce":"# **Stacked Model**","297a2e22":"### **Findings**\n\n> Only BMI and medical history 4 pushing towards class 0","792ad992":"# **Random Forest**\n","953b5d0d":"# **Checking Correlation For Features greater than .8**","89b593eb":"# **Model Interpretability For Gradient Boosting**","511ff45d":"#### **Findings**\n> BMI is pushing models prediction towards 0.\n\n>Medical keyword 15 is pushing towards 1. However, medical keyword 4 is pushing towards 0.\n\n> Also, according to feature plot Wt. was in top 5 most important features, same isn't followed here.","d57d1032":"## **Using Shap**","2c9cd003":"# **Null Value Check**","09b8bfb2":"# **Classification With Model Interpretation**  \ud83d\udcaf \ud83d\udcaf","88b1fc6b":"# **Some Important functions that I will be using throughout**","f536618e":"> **Class imbalance can be seen here. Also there 8 categories, lets combine them to 3 categories**\n","fdd3e0cb":"#### **CONCLUSION:**\n\n> BMI, weight, Medical_History_23, Medical_History_4 and Medical_Keyword_15 seems to be the most important 5 features according to Gradient boosting.\n","5fa4ccf7":"# **Feature Importance For Gradient Boosting**","bb0eaa75":"### **CONCLUSION**\n\n> BMI and Weight are highly correlated, which makes sense also as these 2 features are directly proprtional.\n\n> Ins_Age and Family_Hist_4, Family_Hist_2 highly correlated\n\n> Although, I am not going to perform any transformation on any feature or drop any as these are tree based models and they don't get affected by correlation much because of their non parametric nature.","1974d06d":"### **Findings**\n\n> For low BMI and high medical history 23 we get class as 1.\n\n","1dfb66e1":"# **Max Voting Model**","9eb5d84c":"# **Distribution of Target Variable**","d146a3bf":"# **Reading Data**","3f28b34e":"> **D3 has the highest frequencies**\n\n> Most of the features here are unbalanced.","8765b995":"## **Dependence Plots**"}}