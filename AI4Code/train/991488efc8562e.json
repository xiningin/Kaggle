{"cell_type":{"e2c87a21":"code","a77be732":"code","3f6317c7":"code","367ac003":"code","0bc20f35":"code","eeba400b":"code","dcaa9dab":"code","0eb33b77":"code","e2ac2724":"code","cfa94d5a":"code","18131ed1":"code","a10f4e0d":"code","cb09a2cd":"code","b423dbc5":"code","2174e688":"code","e386cd60":"code","55d1ff4b":"code","73cbbdef":"code","bc940a74":"code","2c84f6dc":"code","a72834fd":"code","614a1c1d":"code","0587aba0":"code","0928bd83":"code","edf908db":"code","4441421f":"code","f5ff5a85":"code","b54a1091":"code","a52281e7":"code","da2ba148":"code","57c6140c":"code","fb71d45a":"code","30e5ff69":"code","54d8134b":"code","ef7b58a7":"code","d964ce08":"code","97bab9a4":"code","7cfac244":"code","84baba15":"code","33ab9be1":"code","43b1de15":"code","2f49dbb1":"code","8a4447c2":"code","e8271acc":"code","697c2fca":"code","a04908a1":"code","d26810c1":"code","aa8e4d37":"code","b7958b3e":"code","630106ba":"code","75b324b0":"code","af6d4ffa":"code","086d8cf7":"code","ae6766bb":"markdown","7b2ece0b":"markdown","2006a2cc":"markdown","4c684323":"markdown","77fc6da4":"markdown","e2ab239d":"markdown","f0fe1da5":"markdown","730bea90":"markdown","bf39ec80":"markdown","16b29b28":"markdown","ec463e0c":"markdown","01ae20d6":"markdown","7e1f36a2":"markdown","06528863":"markdown","bdc2bf69":"markdown","6d10bb28":"markdown","b20f9f26":"markdown","b9d1cf21":"markdown","44ca4f95":"markdown","157b3815":"markdown","662ed460":"markdown","dbbfc643":"markdown","bee4f06f":"markdown","9620afa9":"markdown","4fdac6e1":"markdown","d7f1af26":"markdown","e05a1586":"markdown","8141af68":"markdown","521a01b5":"markdown","b22a43fe":"markdown","9ac16c45":"markdown","1a58d520":"markdown","74aa10cd":"markdown","8238217d":"markdown","cd291939":"markdown","35ced430":"markdown","2422402b":"markdown","472fe413":"markdown","6fec1ade":"markdown"},"source":{"e2c87a21":"# Import Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport math\nimport matplotlib\nimport tensorflow as tf\n\n# Print versions of libraries\nprint(f\"Numpy version : Numpy {np.__version__}\")\nprint(f\"Pandas version : Pandas {pd.__version__}\")\nprint(f\"Matplotlib version : Matplotlib {matplotlib.__version__}\")\nprint(f\"Seaborn version : Seaborn {sns.__version__}\")\nprint(f\"Tensorflow version : Tensorflow {tf.__version__}\")\n\n#Magic function to display In-Notebook display\n%matplotlib inline\n\n# Setting seabon style\nsns.set(style='darkgrid', palette='Set2')","a77be732":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv', encoding = 'latin-1')","3f6317c7":"df.head(10).T","367ac003":"df.columns","0bc20f35":"df.info()","eeba400b":"df.drop(['id','Unnamed: 32'],axis=1, inplace=True)","dcaa9dab":"df.head().T","0eb33b77":"df.describe().T","e2ac2724":"df[\"diagnosis\"].value_counts().plot(kind = 'pie',explode=[0, 0.1],figsize=(6, 6),autopct='%1.1f%%',shadow=True)\nplt.title(\"Malignant and Benign Distribution\",fontsize=20)\nplt.legend([\"Benign\", \"Malignant\"])\nplt.show()","cfa94d5a":"print(df['diagnosis'].value_counts())\nprint('\\n')\nprint(df['diagnosis'].value_counts(normalize=True))","18131ed1":"plt.figure(figsize=(12,10))\n\nsns.distplot(df[df['diagnosis'] == 'M'][\"radius_mean\"], color='g', label = \"Bening\") \nsns.distplot(df[df['diagnosis'] == 'B'][\"radius_mean\"], color='r', label = \"Malignant\") \n\nplt.xlabel(\"Radius Mean Values\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Radius Mean for Bening and Malignant Tumors\", fontsize=14)\nplt.legend()\n\nplt.show()","a10f4e0d":"# most_frequent_bening_radius_mean\ndf[df[\"diagnosis\"] == 'B']['radius_mean'].value_counts().idxmax()","cb09a2cd":"# most_frequent_malignant_radius_mean\ndf[df[\"diagnosis\"] == 'M']['radius_mean'].value_counts().idxmax()","b423dbc5":"features_mean=list(df.columns[1:11])\n# split dataframe into two based on diagnosis\ndfM=df[df['diagnosis'] == 'M']\ndfB=df[df['diagnosis'] == 'B']\n\n#Stack the data\nplt.rcParams.update({'font.size': 8})\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(14,16))\naxes = axes.ravel()\n\nfor idx,ax in enumerate(axes):\n    ax.figure\n    binwidth= (max(df[features_mean[idx]]) - min(df[features_mean[idx]]))\/50\n    \n    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], \n            bins=np.arange(min(df[features_mean[idx]]), max(df[features_mean[idx]]) + binwidth, binwidth) , \n            alpha=0.5,\n            stacked=True, \n            density = True, \n            label=['M','B'],\n            color=['r','g'])\n    \n    ax.legend(loc='upper right')\n    ax.set_title(features_mean[idx])\nplt.tight_layout()\nplt.show()","2174e688":"melted_data = pd.melt(df,id_vars = \"diagnosis\",value_vars = ['radius_mean', 'texture_mean'])\n\nplt.figure(figsize = (14,8))\nsns.boxplot(x = \"variable\", y = \"value\", hue=\"diagnosis\",data= melted_data, fliersize=0)\n\n# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n# plt.legend([\"Benign\", \"Malignant\"])\nplt.show()","e386cd60":"# Also we can look relationship between more than 2 distribution\n# sns.set(style = \"white\")\n\nsns.pairplot(df, vars=[\"radius_mean\",\"area_mean\",\"texture_mean\",'smoothness_mean',\"fractal_dimension_se\"], hue='diagnosis')\nplt.suptitle('Relations ship between features');\nplt.show()","55d1ff4b":"plt.figure(figsize = (15,10))\nsns.jointplot(df['radius_mean'],df['area_mean'],kind=\"reg\")\nplt.show()","73cbbdef":"plt.figure(figsize=(18,18))\nplt.title('Pearson Correlation Matrix')\n# Generating correlation\ncorr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nsns.heatmap(corr,mask = mask,linewidths=0.25,vmax=0.7,square=True,cmap=\"viridis\",linecolor='w',annot=True,cbar_kws={\"shrink\": .7});\nplt.show()","bc940a74":"df.reset_index(inplace = True , drop = True)","2c84f6dc":"df['diagnosis'].value_counts()","a72834fd":"df['diagnosis'] = df['diagnosis'].map({'M': 1,'B': 0})","614a1c1d":"df['diagnosis'].value_counts()","0587aba0":"X = df.drop('diagnosis',axis=1).values\ny = df['diagnosis'].values","0928bd83":"from sklearn.model_selection import train_test_split","edf908db":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)","4441421f":"# Quick sanity check with the shapes of Training and testing datasets\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","f5ff5a85":"from sklearn.preprocessing import MinMaxScaler","b54a1091":"scaler = MinMaxScaler()","a52281e7":"scaler.fit(X_train)","da2ba148":"X_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","57c6140c":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout","fb71d45a":"model = Sequential()\n\n# https:\/\/stats.stackexchange.com\/questions\/181\/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n\nmodel.add(Dense(units=30,activation='relu'))\nmodel.add(Dense(units=15,activation='relu'))\nmodel.add(Dense(units=1,activation='sigmoid'))\n\n# For a binary classification problem\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","30e5ff69":"# https:\/\/stats.stackexchange.com\/questions\/164876\/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\n# https:\/\/datascience.stackexchange.com\/questions\/18414\/are-there-any-rules-for-choosing-the-size-of-a-mini-batch\n\nmodel.fit(x=X_train, \n          y=y_train, \n          epochs=600,\n          validation_data=(X_test, y_test), verbose=1\n          )","54d8134b":"model_loss = pd.DataFrame(model.history.history)","ef7b58a7":"plt.figure(figsize=(12,8))\nmodel_loss.plot()\nplt.show()","d964ce08":"model = Sequential()\n\nmodel.add(Dense(units=30,activation='relu'))\nmodel.add(Dense(units=15,activation='relu'))\nmodel.add(Dense(units=1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","97bab9a4":"from tensorflow.keras.callbacks import EarlyStopping","7cfac244":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)","84baba15":"model.fit(x=X_train, \n          y=y_train, \n          epochs=600,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop]\n          )","33ab9be1":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","43b1de15":"from tensorflow.keras.layers import Dropout","2f49dbb1":"model = Sequential()\n\nmodel.add(Dense(units=30,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=15,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","8a4447c2":"model.fit(x=X_train, \n          y=y_train, \n          epochs=600,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop]\n          )","e8271acc":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","697c2fca":"y_train_pred = model.predict_classes(X_train)\ny_test_pred = model.predict_classes(X_test)","a04908a1":"from sklearn import metrics","d26810c1":"# https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall\nprint(metrics.classification_report(y_test, y_test_pred))","aa8e4d37":"y_test_pred = y_test_pred.flatten()","b7958b3e":"# Heatmap for Confusion Matrix\ncnf_matrix = metrics.confusion_matrix(y_test,y_test_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1, fontsize = 22)\nplt.ylabel('Actual',fontsize = 18)\nplt.xlabel('Predicted',fontsize = 18)\nplt.show()","630106ba":"# Printing the Overall Accuracy of the model\nprint(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(y_test, y_test_pred)))","75b324b0":"print(\"Count of Actual values of Test data :\")\nprint(pd.Series(y_test).value_counts())\n\nprint(\"\\n\")\n\nprint(\"Count of Predected values of Test data :\")\nprint(pd.Series(y_test_pred).value_counts())","af6d4ffa":"54\/55","086d8cf7":"cnf_matrix[1][1]\/pd.Series(y_test).value_counts()[1]","ae6766bb":"# **Splitting data into Training and Testing samples**\n\nWe dont use the full data for creating the model. Some data is randomly selected and kept aside for checking how good the model is. This is known as Testing Data and the remaining data is called Training data on which the model is built. Typically 70% of data is used as Training data and the rest 30% is used as Tesing data.","7b2ece0b":"# **Introduction**\n\nDetection of breast cancer is the preliminary phase in cancer diagnosis. So, classifiers with higher accuracy are always desired. A classifier with high accuracy offers very less chance to wrongly classify a patient of cancer. \n\nBreast cancer refers to cancer from a malignant tumor in the cells of the breast tissue. A malignant tumor is a group of cancer cells that can grow into surrounding tissues or spread to distant areas of the body. Breast cancer is uncontrolled multiplication of cells in breast tissue. A group of rapidly dividing cells may form a lump or architectural distortions. \n\nThere are two main classifications of tumors. One is known as **benign** and the other as **malignant**. A benign tumor is a tumor that does not invade its surrounding tissue or spread around the body. A malignant tumor is a tumor that may invade its surrounding tissue or spread around the body.\n\n**Benign** tumors are **non-malignant\/non-cancerous tumor**. A benign tumor is usually localized, and does not spread to other parts of the body.  **Malignant** tumors are **cancerous** growths. They are often resistant to treatment, may spread to other parts of the body and they sometimes recur after they were removed.\n\nThere are two aspects of diagnosis of cancerous cells while doing testing.  A **false-positive** test occurs when test results appear to be abnormal, even though there is actually no cancer. A **false-negative** is when test results show no cancer when there really is cancer.\n\nNo test is perfect: a perfect test would give only **true positive** and **true negative** results, but a good screening test should have a low rate of **false-positive** and **false-negative** results. **False-positive** results can create undue stress, anxiety, and can lead to other unnecessary testing. **False-negative** results can delay treatment. I feel that **false-negative is more dangerous** in case of cancer detection, because patient think that he do not have cancer and therefore he will not take any precation and medical treatment. Although cancerous cells keep growing inside and can lead to next phase of cancer. When again patient starting feel uncomfortable, then he may go for another round of testing, but by that time it may be too late to cure the cancer.\n\nSo, in this case we have emphasis more on find accurate false-negative rather than the accuracy rate.\n\nLet's explore a classification task with **Keras API for TF 2.0 with Early Stopping and Dropout Layer**.\n","2006a2cc":"**Observations**\n\n* radius_mean and texture_mean are higher for Malignant cells. It means cancerous cells have higher (about radius_mean = 17.5 & texture_mean = 22) values of radius_mean and texture_mean as compared to non-cancerous cells.","4c684323":"There are two aspects of diagnosis of cancerous cells while doing testing.  A **false-positive** test occurs when test results appear to be abnormal, even though there is actually no cancer. A **false-negative** is when test results show no cancer when there really is cancer.\n\nNo test is perfect: a perfect test would give only **true positive** and **true negative** results, but a good screening test should have a low rate of **false-positive** and **false-negative** results. **False-positive** results can create undue stress, anxiety, and can lead to other unnecessary testing. **False-negative** results can delay treatment. I feel that **false-negative is more dangerous** in case of cancer detection, because patient think that he do not have cancer and therefore he will not take any precation and medical treatment. Although cancerous cells keep growing inside and can lead to next phase of cancer. When again patient starting feel uncomfortable, then he may go for another round of testing, but by that time it may be too late to cure the cancer.\n\nSo, in this case we have emphasis more on finding the \n\nhttps:\/\/en.wikipedia.org\/wiki\/Sensitivity_and_specificity\n\nhttps:\/\/kennis-research.shinyapps.io\/Bayes-App\/\n","77fc6da4":"# **Exploratory Data Analysis**\n\nOnce the data is read into python, we need to explore\/clean\/filter it before processing it for machine learning It involves adding\/deleting few colums or rows, joining some other data, and handling qualitative variables like dates.","e2ab239d":"<p style=\"font-weight:bold;color:#1E90FF;font-size:18px\">I welcome comments, suggestions, corrections and of course votes also.<\/p>","f0fe1da5":"# Creating the Model","730bea90":"**Observations**\n* Mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.\n\n* Mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other. In any of the histograms there are no noticeable large outliers that warrants further cleanup.","bf39ec80":"# **Breast Cancer Wisconsin Diagnostic**","16b29b28":" \n\n# Example One: Choosing too many epochs and overfitting!","ec463e0c":"### Real Accuracy ","01ae20d6":"## Import libraries","7e1f36a2":"## Malignant and Benign Distribution","06528863":"## Import dataset","bdc2bf69":"# **Model Evaluation**","6d10bb28":"## Generate descriptive statistics\n\nLets summarize the central tendency, dispersion and shape of a dataset's distribution.","b20f9f26":"## Relationship between more than 2 distribution","b9d1cf21":"# **Scale Amount Feature**\n\n* It is good idea to scale the data, so that the column(feature) with lesser significance might not end up dominating the objective function due to its larger range. like a column like age has a range between 0 to 80, but a column like salary has range from thousands to lakhs, hence, salary column will dominate to predict the outcome even if it may not be important.\n* In addition, features having different unit should also be scaled thus providing each feature equal initial weightage. Like Age in years and Sales in Dollars must be brought down to a common scale before feeding it to the ML algorithm\n* This will result in a better prediction model.\n","44ca4f95":"## Reset the index","157b3815":"**Observations**\n\n* From this graph you can see that radius mean of malignant tumors are bigger than radius mean of bening tumors mostly.\n* The bening distribution (green in graph) is approcimately bell-shaped that is shape of normal distribution (gaussian distribution)\n* Also you can find result like that most frequent malignant radius mean is 15.46 and most frequent bening radius mean is 11.06.","662ed460":"## Correlation Among Explanatory Variables\n\nHaving **too many features** in a model is not always a good thing because it might cause overfitting and worser results when we want to predict values for a new dataset. Thus, **if a feature does not improve your model a lot, not adding it may be a better choice.**\n\nAnother important thing is **correlation. If there is very high correlation between two features, keeping both of them is not a good idea most of the time not to cause overfitting.** However, this does not mean that you must remove one of the highly correlated features. ","dbbfc643":"## __Accuracy , Precision and Recall__\n\n\n### __Accuracy__ : The most used and classic classification metric : Suited for binary classification problems.\n\n$$  \\text{Accuracy} = \\frac{( TP + TN ) }{ (TP + TN + FP + FN )}$$\n\nBasically Rightly predicted results amongst all the results , used when the classes are balanced\n\n### __Precision__ : What proportion of predicted positives are truly positive ? Used when we need to predict the positive thoroughly, sure about it !\n\n$$ \\text{Precision} = \\frac{( TP )}{( TP + FP )} $$\n\n### __Sensitivity or Recall__ : What proportion of actual positives is correctly classified ? choice when we want to capture as many positives as possible\n\n$$ \\text{Recall} = \\frac{(TP)}{( TP + FN )} $$\n\n### F1 Score : Harmonic mean of Precision and Recall. It basically maintains a balance between the precision and recall for your classifier\n\n$$ F1 = \\frac{2 * (\\text{ precision } * \\text{ recall })}{(\\text{ precision } + \\text{ recall } )} $$\n\n","bee4f06f":"**Stop training when a monitored quantity has stopped improving.**","9620afa9":"## Checking concise summary of dataset\n\nIt is also a good practice to know the features and their corresponding data types,along with finding whether they contain null values or not.","4fdac6e1":"# Example Three: Adding in DropOut Layers\n\n### Dropout Layers\n**Dropout is a regularization technique for neural network models. So in this technique randomly selected neurons are ignored during training. They are \u201cdropped-out\u201d randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n\nMore at : https:\/\/machinelearningmastery.com\/dropout-regularization-deep-learning-models-keras","d7f1af26":"## Confusion Matrix","e05a1586":"## Distribution of other features","8141af68":"# Example Two: Early Stopping\n\nWe obviously trained too much! Let's use early stopping to track the val_loss and stop training once it begins increasing too much!\n\n### Early stopping:\n**Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.**\n\n> More at : https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/","521a01b5":"# **Load the data**","b22a43fe":"**Observations**\n\nThis dataset contain about 37% cancerous Malignant cells and about 62.7% Bening non cancerous cells.","9ac16c45":"### So 98.18% is our real accuracy.","1a58d520":"**Attribute Information:**\n\n* 1) ID number\n* 2) Diagnosis (M = malignant, B = benign) \n\nAttribute 3-32:\n\nTen real-valued features are computed for each cell nucleus:\n\n* a) radius (mean of distances from center to points on the perimeter)\n* b) texture (standard deviation of gray-scale values)\n* c) perimeter\n* d) area\n* e) smoothness (local variation in radius lengths)\n* f) compactness (perimeter^2 \/ area - 1.0)\n* g) concavity (severity of concave portions of the contour)\n* h) concave points (number of concave portions of the contour)\n* i) symmetry\n* j) fractal dimension (\"coastline approximation\" - 1)\n\nThe **mean**, **standard error (se)** and **worst** or **largest** (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\n**diagnosis**: (Wisconsin Diagnostic Breast Cancer - WDBC)\n* WDBC-Malignant\n* WDBC-Benign","74aa10cd":"## so 1 represents M and 0 represts B.","8238217d":"## Histogram of Radius Mean for Bening and Malignant Tumors","cd291939":"**Observations**\n\n* All expect area_mean are more or less have bell shape curves. It means they have normal distribution.","35ced430":"# **Training the Model**","2422402b":"**Observations**\n* Dataset contains details of 569 transactions with 33 features.\n* Data has float, integer, and object\/String type values.\n* Diagnosis is string haing values M and B (M = malignant, B = benign).\n* Every feature has 569 values, so there is no missing values in the form of NaN or NA.\n* There is feature with name \"Unnamed\" with NAN values.\n* All data types are float64 ,except 1 : diagnosis \n* Memory Usage : 147KB only, not so Harsh !!","472fe413":"## Delete unwanted columns\n\nId and the column 'Unnamed: 32' is not useful for data analysis, so lets remove them first.","6fec1ade":"**Observations**\n* area_mean, perimeter_se, area_se, area_worst are highly positive skewed."}}