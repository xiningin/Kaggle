{"cell_type":{"d40afdea":"code","7fd57beb":"code","7dc2c113":"code","fd2a7b37":"code","bf3f2517":"code","e008aa3a":"code","8902c674":"code","e58072f7":"code","4009b8e2":"markdown","ca783a86":"markdown","35e2b1ff":"markdown","abd55dfa":"markdown","8de25b4c":"markdown","ef11c3bc":"markdown","e39a6618":"markdown","a6087c3e":"markdown","61e3bdc8":"markdown","fc82b374":"markdown"},"source":{"d40afdea":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold as SK\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\n\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\n\ntest = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\n\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\n\ndef reduce_mean_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        \n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] =='int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min> np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    \n    if verbose:\n        print('Memory usage is decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem)\/ start_mem))\n    \n    return df\n\n\ntrain = reduce_mean_usage(train)\ntest = reduce_mean_usage(test)\n\n# Use sample method with frac =1 .\ntrain = train.sample(frac=1).reset_index(drop=True)\n\ntrain_x = train.drop(['Id','Cover_Type'],axis=1)\n\ntest_x = test.drop(['Id'],axis=1)\n\ntrain_y = train['Cover_Type']\n\ndrop_5_index = train_y[train_y == 5].index[0]\n\ntrain_x = train_x.drop([drop_5_index],axis=0).reset_index().drop(['index'],axis=1)\ntrain_y = train_y.drop([drop_5_index],axis=0).reset_index().drop(['index'],axis=1)\n\nscaler = MinMaxScaler()\ntrain_df = scaler.fit_transform(train_x)\ntrain_df = pd.DataFrame(train_df)\ntrain_df.columns = train_x.columns\n\ntest_df = pd.DataFrame(scaler.transform(test_x))\ntest_df.columns = test_x.columns\n\ntrain_non_dummy, train_dummy = train_df.columns[:10], train_df.columns[10:]\n\ntrain_df_dummy = train_df[train_dummy]\ntrain_df_non_dummy = train_df[train_non_dummy]\n\ntest_df_dummy = test_df[train_dummy]\ntest_df_non_dummy = test_df[train_non_dummy]\n\ntrain=[]\ntest=[]\ntrain_x=[]\ntest_x=[]\ntrain_df=[]\ntest_df=[]\n\nEncoder = LabelEncoder()\n\ny_encoded = Encoder.fit_transform(train_y)\ntrain_y = []\n\n","7fd57beb":"# Set input two input layers for dummy and non-dummy.\nINPUT = tf.keras.layers.Input(shape = train_df_non_dummy.shape[1:], name = 'Input')\nINPUT_dummy = tf.keras.layers.Input(shape = train_df_dummy.shape[1:], name = 'Input_Dummy')\n\n# Set dense layer for non-dummy variable.\ndense1 = tf.keras.layers.Dense(300, activation='elu',  kernel_initializer = 'he_normal', name = 'Dense1')(INPUT)\ndense2 = tf.keras.layers.Dense(300, activation='elu',  kernel_initializer = 'he_normal',name = 'Dense2')(dense1)\ndense3 = tf.keras.layers.Dense(300, activation='elu',  kernel_initializer = 'he_normal',name = 'Dense3')(dense2)\ndense4 = tf.keras.layers.Dense(300, activation='elu',  kernel_initializer = 'he_normal',name = 'Dense4')(dense3)\ndense5 = tf.keras.layers.Dense(300, activation='elu',  kernel_initializer = 'he_normal',name = 'Dense5')(dense4)\ndense6 = tf.keras.layers.Dense(300, activation='elu',  kernel_initializer = 'he_normal',name = 'Dense6')(dense5)\ndense_dropout = tf.keras.layers.Dropout(0.5, name = 'Dropout')(dense6)\n\n# Set dense layer for dummy variable.\ndummy_dense1 = tf.keras.layers.Dense(200, activation = 'elu', kernel_initializer = 'he_normal',name = 'Dummy_Dense1')(INPUT_dummy)\ndummy_dense2 = tf.keras.layers.Dense(200, activation = 'elu', kernel_initializer = 'he_normal',name = 'Dummy_Dense2')(dummy_dense1)\ndummy_dense3 = tf.keras.layers.Dense(200, activation = 'elu', kernel_initializer = 'he_normal',name = 'Dummy_Dense3')(dummy_dense2)\ndummy_dense4 = tf.keras.layers.Dense(200, activation = 'elu', kernel_initializer = 'he_normal',name = 'Dummy_Dense4')(dummy_dense3)\ndummy_dense5 = tf.keras.layers.Dense(200, activation = 'elu', kernel_initializer = 'he_normal',name = 'Dummy_Dense5')(dummy_dense4)\ndummy_dense6 = tf.keras.layers.Dense(200, activation = 'elu', kernel_initializer = 'he_normal',name = 'Dummy_Dense6')(dummy_dense5)\ndummy_dense_dropout = tf.keras.layers.Dropout(0.5, name = 'Dummy_Dropout')(dummy_dense6)\n\n# Concatenating the two structures.\nconnect = tf.keras.layers.Concatenate(axis=1, name='Connection')([dense_dropout, dummy_dense_dropout])\n\n# Set dense layer for combined structure.\nconnect_dense1 = tf.keras.layers.Dense(100, activation = 'elu', kernel_initializer = 'he_normal',name = 'Connect_Dense1')(connect)\nconnect_dense2 = tf.keras.layers.Dense(100, activation = 'elu', kernel_initializer = 'he_normal',name = 'Connect_Dense2')(connect_dense1)\nconnect_dense3 = tf.keras.layers.Dense(100, activation = 'elu', kernel_initializer = 'he_normal',name = 'Connect_Dense3')(connect_dense2)\nconnect_dropout = tf.keras.layers.Dropout(0.5, name = 'Connect_Dropout')(connect_dense3)\n\n# Set the output layer\nOUTPUT = tf.keras.layers.Dense(6, activation = 'softmax', name = 'Output')(connect_dropout)\n\n# Set your model\nmodel = tf.keras.Model(inputs = [INPUT, INPUT_dummy], outputs = [OUTPUT], name = 'pythonash_model')","7dc2c113":"model.summary()","fd2a7b37":"tf.keras.utils.plot_model(model, show_shapes = True, show_layer_names=True, rankdir='TB')","bf3f2517":"lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.002,\n    decay_steps=10000,\n    decay_rate=0.98\n)\nopt = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\nepoch_number = 20\ncheck_pt = tf.keras.callbacks.ModelCheckpoint('pythonash_model.h5', save_best_only = True, verbose = 2)","e008aa3a":"model.compile(loss ='sparse_categorical_crossentropy', optimizer = opt, metrics = ['sparse_categorical_accuracy'])\nmodel.fit(\nx= [train_df_non_dummy, train_df_dummy], y = y_encoded, validation_split = 0.1 , batch_size = 1000, validation_batch_size = 1000,\ncallbacks = [check_pt], verbose =2, workers = 3, epochs = epoch_number)","8902c674":"best_model = tf.keras.models.load_model('pythonash_model.h5')\nresult = best_model.predict([test_df_non_dummy, test_df_dummy])\nresult = Encoder.inverse_transform(np.argmax(result,axis=1))\nfinal_result = pd.DataFrame(result)\nfinal_result.columns =['Cover_Type']\nfinal_result","e58072f7":"submission['Cover_Type'] = final_result['Cover_Type']\n\nsubmission.to_csv('.\/submission.csv', index=False)\n\nsubmission","4009b8e2":"# Data loading and handling will be skipped.\n\nThe handling procedure is shown as in my previous notebook (see \"A way you can handle dataset, simply).\n\nJust implement the code for preparing dataset.","ca783a86":"## Compile your model and fit","35e2b1ff":"## Set hyper parameters","abd55dfa":"# Deep learning with parallel strategy!!\n\nWe use DNN model for predicting the result with parallel strategy.\n\nParallel startegy means that we will design two structures for dummy variables and non-dummy variables, respectively.\n\nAnd we will concatenate the two structures with one output.\n\nFrom my idea, you will get at least 95% accuracy.\n\n*Note*:\n\n> We will use elu as activation function and 'he_normal' as kerner initializer to this model.\n\n> It will be helpful for rapid convergence and commonly used as a useful way for training.","8de25b4c":"# Read me!!\n\nHello, welcome to my notebook!!\n\nThis notebook is inline with my previous notebook ([A way you can handle dataset, simply]).\n\nFurthermore, you will learn about predicting through deep learning model.\n\nWe divided the dataset into dummy cluster and non-dummy cluster because of this notebook.\n\nThe key point is parallel strategy which uses parallel layers in training the dummy and non-dummy cluster, respectively.\n\nHow can we do this strategy?? Please take a look model discussion, carefully.\n\nIf you have any questions, please leave the comments.\n\nI hope you to gain more imformation about data handling, DNN, CNN, and etc..\n\n## **Knowledge can be improved by being shared.**\n\nPlease upvote!!\n\n\n## [You can learn more skills for handling dataset or neural network.]\n\n### [A way you can handle dataset, simply] - Tabular Playground Series - Dec 2021 (best 0.95583 score!!)\n - https:\/\/www.kaggle.com\/pythonash\/a-way-you-can-handle-dataset-simply#It-has-done!!\n\n### [How to use csv and img at the same time] - Pawpularity Contest\n - https:\/\/www.kaggle.com\/pythonash\/how-to-use-csv-and-img-at-the-same-time\n\n### [Parallel combination DNN with CNN] - Pawpularity Contest\n - https:\/\/www.kaggle.com\/pythonash\/parallel-dnn-and-cnn-network-for-beginners\n \n### [Image data handling without memory exploded] - Pawpularity Contest\n - https:\/\/www.kaggle.com\/pythonash\/how-to-handle-dataset-for-beginners\n\n### [Data handling & Deep learning] - Titanic competition (Top 7%)\n - https:\/\/www.kaggle.com\/pythonash\/how-to-handle-raw-dataset-and-analyze-with-dl\n \n### [Deep learning model with SeLU activation function] - Titanic competition\n- https:\/\/www.kaggle.com\/pythonash\/selu-activation-function-in-dl\n\n### [Preparing a completed dataset with proper imputation method] - Titanic competition\n - https:\/\/www.kaggle.com\/pythonash\/making-completed-dataset\n\n**Let's start!**","ef11c3bc":"## Submit your result!!","e39a6618":"## Model summary\n\n> You can identify what the structure is.","a6087c3e":"## Use your recorded model with best performance\n\n> With using ModelCheckpoint in tf.keras.callbacks, we can load and use the best model which was the best performance.","61e3bdc8":"# It's your turn!!\n\nYou have many opportunities that you can change this model parameters and get your submission score.\n\nI recommend that you change the hyper parameters such as learning_rate, batch_size, activation function, the number of neurons, layers, and so on...\n\nIf you get any helps from my notebook, please upvote!!\n\nFingers crossed!!","fc82b374":"## Plotting your model\n\n> You can see the structure in visualiation.\n\n> As you can see, your model will be concatenated."}}