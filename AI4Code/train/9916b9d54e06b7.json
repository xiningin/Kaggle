{"cell_type":{"cfaec8fe":"code","fae2df2f":"code","bbe45711":"code","aaee3584":"code","93582a8c":"code","e5091343":"code","c923cc53":"code","39502025":"code","175fe7f1":"code","e51d15ee":"code","10fa9466":"code","2db76ea6":"code","1786c8f2":"code","7dc6782a":"code","c180a184":"code","74172f10":"code","d07fb961":"code","e38c15c2":"code","ac110055":"code","14dbd953":"code","e156f698":"code","579d4f0b":"code","9afc08b0":"code","b7048b4e":"code","354e7b63":"code","ca3e61ed":"code","536e8ef9":"code","12c6438e":"code","43fac013":"code","a83ff079":"code","1ab90168":"code","1c54930e":"code","151d0439":"code","790aa468":"markdown","d5dcb07e":"markdown","dbdd26d4":"markdown","aed3f326":"markdown","9ef00cd4":"markdown","6563381d":"markdown","afed35ba":"markdown","6132df1e":"markdown","fc00dfe6":"markdown","bc986c12":"markdown"},"source":{"cfaec8fe":"from __future__ import print_function, division\n!pip install torchsummary pywick captum shap kornia pycm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nfrom tqdm import tqdm\nfrom torchsummary import summary\nfrom pycm import *\n\nplt.ion()   # interactive mode","fae2df2f":"import torch\nfrom torch import Tensor\nimport torch.nn as nn\ntry:\n    from torch.hub import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url\nfrom typing import Type, Any, Callable, Union, List, Optional\n\n\n__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n           'wide_resnet50_2', 'wide_resnet101_2']\n\n\nmodel_urls = {\n    'resnet18': 'https:\/\/download.pytorch.org\/models\/resnet18-f37072fd.pth',\n    'resnet34': 'https:\/\/download.pytorch.org\/models\/resnet34-b627a593.pth',\n    'resnet50': 'https:\/\/download.pytorch.org\/models\/resnet50-0676ba61.pth',\n    'resnet101': 'https:\/\/download.pytorch.org\/models\/resnet101-63fe2227.pth',\n    'resnet152': 'https:\/\/download.pytorch.org\/models\/resnet152-394f9c45.pth',\n    'resnext50_32x4d': 'https:\/\/download.pytorch.org\/models\/resnext50_32x4d-7cdf4587.pth',\n    'resnext101_32x8d': 'https:\/\/download.pytorch.org\/models\/resnext101_32x8d-8ba56ff5.pth',\n    'wide_resnet50_2': 'https:\/\/download.pytorch.org\/models\/wide_resnet50_2-95faca4d.pth',\n    'wide_resnet101_2': 'https:\/\/download.pytorch.org\/models\/wide_resnet101_2-32ee1156.pth',\n}\n\n\ndef conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n        # Added another relu here\n        self.relu2 = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n\n        # Modified to use relu2\n        out = self.relu2(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https:\/\/arxiv.org\/abs\/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https:\/\/ngc.nvidia.com\/catalog\/model-scripts\/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width \/ 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        \n        # Added another relu here\n        self.relu2 = nn.ReLU(inplace=True)\n        self.relu3 = nn.ReLU(inplace=True)\n\n    def forward(self, x: Tensor) -> Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu2(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        # Modified to use relu2\n        out = self.relu3(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        layers: List[int],\n        num_classes: int = 1000,\n        zero_init_residual: bool = False,\n        groups: int = 1,\n        width_per_group: int = 64,\n        replace_stride_with_dilation: Optional[List[bool]] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https:\/\/arxiv.org\/abs\/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n\n    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)\n\n\ndef _resnet(\n    arch: str,\n    block: Type[Union[BasicBlock, Bottleneck]],\n    layers: List[int],\n    pretrained: bool,\n    progress: bool,\n    **kwargs: Any\n) -> ResNet:\n    model = ResNet(block, layers, **kwargs)\n    if pretrained:\n        state_dict = load_state_dict_from_url(model_urls[arch],\n                                              progress=progress)\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-152 model from\n    `\"Deep Residual Learning for Image Recognition\" <https:\/\/arxiv.org\/pdf\/1512.03385.pdf>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n                   **kwargs)","bbe45711":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225])\n    ]),\n}","aaee3584":"batch_size = 10\ndata_dir = '..\/input\/dataset140000\/dataverse-split'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n                                             shuffle=True, num_workers=26)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\nprint(class_names)","93582a8c":"idx_to_labels = class_names\n\nmodel = resnet152()\nnum_ftrs = model.fc.in_features\nmodel.avgpool = nn.AvgPool2d(7, stride=1)\nmodel.fc = nn.Linear(num_ftrs, len(class_names))\nmodel.load_state_dict(torch.load('..\/input\/resnet-final\/resnet_lesions_pytorch_final.pt'))\nmodel.eval().to(device)","e5091343":"import torch.nn.functional as F\ndef get_prediction_and_actual_y(model):\n    model.eval()\n    \n    y_true = torch.tensor([], dtype=torch.long, device=device)\n    all_outputs = torch.tensor([], device=device)\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            y_true = torch.cat((y_true, labels), 0)\n            all_outputs = torch.cat((all_outputs, outputs), 0)\n            \n    y_true = y_true.cpu().numpy()  \n    _, y_pred = torch.max(all_outputs, 1)\n    y_pred = y_pred.cpu().numpy()\n    y_pred_prob = F.softmax(all_outputs, dim=1).cpu().numpy()\n    \n    return y_true, y_pred, y_pred_prob\n            ","c923cc53":"y_true, y_pred, y_pred_prob = get_prediction_and_actual_y(model)","39502025":"cm = ConfusionMatrix(actual_vector=y_true.tolist(), predict_vector=y_pred.tolist())","175fe7f1":"print(cm)","e51d15ee":"cm.print_normalized_matrix()","10fa9466":"import sklearn.metrics as skm\n\nprint(skm.classification_report(y_true.tolist(), y_pred.tolist()))","2db76ea6":"from PIL import Image\n\nfrom captum.attr import IntegratedGradients\nfrom captum.attr import GradientShap\nfrom captum.attr import Saliency\nfrom captum.attr import DeepLift\nfrom captum.attr import NoiseTunnel\nfrom captum.attr import Occlusion\nfrom captum.attr import Lime\nfrom captum.attr import DeepLiftShap\nfrom captum.attr import visualization as viz\nfrom torchvision import transforms\nfrom matplotlib.colors import LinearSegmentedColormap\n\nimport torch.nn.functional as F","1786c8f2":"def open_transform_image(path):\n    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n\n    # transform val\n    img_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std, inplace=True)\n        ])\n    img = Image.open(path)\n    image = img_transforms(img)\n    \n    return image\n\ndef interpretation_transform(path):\n    img_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor()\n        ])\n    img = Image.open(path)\n    image = img_transforms(img)\n    \n    return image\n\ndef predict_label(img, model=model):\n    # Convert to a batch of 1\n    xb = img.unsqueeze(0)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    output = F.softmax(yb, dim=1)\n    prediction_score, pred = torch.topk(output, 1)\n    \n    with torch.no_grad():\n        yb = model(xb)\n        output = F.softmax(yb, dim=1)\n        print(\"{:.3f}%\".format(max(output.cpu().numpy().tolist()[0])))\n\n    return pred.squeeze()","7dc6782a":"def interpretation_image(image_input):\n    test_image = open_transform_image(image_input)\n    transformed_img = interpretation_transform(image_input)\n    pred_ix = predict_label(test_image.to(device))\n    print(class_names[pred_ix])\n    return (test_image.unsqueeze(0), pred_ix, transformed_img)","c180a184":"import time\n\nimages_list = [\n    '..\/input\/dataset140000\/dataverse-split\/val\/akiec\/ISIC_0025423.jpg',\n    '..\/input\/dataset140000\/dataverse-split\/val\/bcc\/ISIC_0026064.jpg',\n    '..\/input\/dataset140000\/dataverse-split\/val\/bkl\/ISIC_0024788.jpg',\n    '..\/input\/dataset140000\/dataverse-split\/val\/df\/df_original_ISIC_0024447.jpg_1950099b-d572-45a5-94e1-192629853e6d.jpg',\n    '..\/input\/dataset140000\/dataverse-split\/val\/mel\/ISIC_0027461.jpg',\n    '..\/input\/dataset140000\/dataverse-split\/val\/nv\/ISIC_0024465.jpg',\n    '..\/input\/dataset140000\/dataverse-split\/val\/vasc\/ISIC_0030005.jpg'\n]\n\nimages_interpreted = []\n\nfor image in images_list:\n    images_interpreted.append(interpretation_image(image))","74172f10":"def perturb_fn(inputs):\n    noise = torch.tensor(np.random.normal(0, 0.003, inputs.shape)).float().to(device)\n    return noise, inputs - noise","d07fb961":"import pandas as pd","e38c15c2":"sens_list = []\ninf_list = []","ac110055":"%%time\n\nfrom captum.metrics import sensitivity_max, infidelity\n\ndl = DeepLift(model)\ndef deeplift_model_visualize(interpretation_image, pred_ix, transformed_img):\n    start_time = time.time()\n    print(class_names[pred_ix])\n    attributions_dl = dl.attribute(interpretation_image.to(device),\n                                    baselines=interpretation_image.to(device)*0,\n                                    target=pred_ix)\n    _ = viz.visualize_image_attr_multiple(np.transpose(attributions_dl.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                          np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                          [\"original_image\", \"blended_heat_map\", \"heat_map\"],\n                                          [\"all\", \"all\", \"all\"],\n                                          show_colorbar=True,\n                                          outlier_perc=2,\n                                         )\n    \n    print(\"time\", time.time() - start_time)\n    \n    infid = infidelity(model.to(device), perturb_fn, interpretation_image.to(device), attributions_dl, n_perturb_samples=1)\n    print(\"Infidelity\", infid.item())\n    inf_list.append(infid.item())","14dbd953":"%%time\n\nfor image, pred, transformed_img in images_interpreted:\n    deeplift_model_visualize(image, pred, transformed_img)\n    sens = sensitivity_max(dl.attribute, image.to(device),target=pred, baselines=image.to(device)*0)\n\n    print(\"Sensitivity\", sens.item())\n    sens_list.append(sens.item())\n    print(\"\\n\")","e156f698":"%%time\n\nocclusion = Occlusion(model)\n\ndef occulusion_model_visualize(interpretation_image, pred_ix, transformed_img):\n    start_time = time.time()\n    print(class_names[pred_ix])\n    attributions_occ = occlusion.attribute(interpretation_image.to(device),\n                                           strides = (3, 8, 8),\n                                           target=pred_ix,\n                                           sliding_window_shapes=(3,15, 15),\n                                           baselines=0)\n\n    _ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                          np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                          [\"original_image\", \"blended_heat_map\", \"heat_map\"],\n                                          [\"all\", \"positive\", \"positive\"],\n                                          show_colorbar=True,\n                                          outlier_perc=2,\n                                         )\n    _ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                          np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                          [\"original_image\", \"blended_heat_map\", \"heat_map\"],\n                                          [\"all\", \"all\", \"all\"],\n                                          show_colorbar=True,\n                                          outlier_perc=2,\n                                         )\n    print(\"time\", time.time() - start_time)\n    \n    infid = infidelity(model.to(device), perturb_fn, interpretation_image.to(device), attributions_occ, n_perturb_samples=1)\n    print(\"Infidelity\", infid.item())\n    inf_list.append(infid.item())","579d4f0b":"%%time\nfor image, pred, transformed_img in images_interpreted:\n    occulusion_model_visualize(image, pred, transformed_img)\n    sens = sensitivity_max(occlusion.attribute, image.to(device), target=pred, sliding_window_shapes=(3,15, 15), strides = (3, 8, 8),baselines=0)\n    print(\"Sensitivity\", sens.item())\n    sens_list.append(sens.item())\n    print(\"\\n\")","9afc08b0":"%%time\n\nintegrated_gradients = IntegratedGradients(model)\ndef integrated_gradients_visualize(interpretation_image, pred_ix, transformed_img):\n    start_time = time.time()\n    print(class_names[pred_ix])\n    attributions_ig = integrated_gradients.attribute(interpretation_image.to(device), target=pred_ix, n_steps=5)\n    default_cmap = LinearSegmentedColormap.from_list('custom blue', \n                                                     [(0, '#ffffff'),\n                                                      (0.25, '#000000'),\n                                                      (1, '#000000')], N=256)\n\n    _ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                 np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                 methods=[\"original_image\", \"blended_heat_map\", \"heat_map\"],\n                                 cmap=default_cmap,\n                                 show_colorbar=True,\n                                 signs=[\"all\", \"positive\",  \"absolute_value\"],\n                                 outlier_perc=1)\n    print(\"time\", time.time() - start_time)\n    \n    infid = infidelity(model.to(device), perturb_fn, interpretation_image.to(device), attributions_ig, n_perturb_samples=1)\n    print(\"Infidelity\", infid.item())\n    inf_list.append(infid.item())","b7048b4e":"%%time\nfor image, pred, transformed_img in images_interpreted:\n    integrated_gradients_visualize(image, pred, transformed_img)\n    \n    sens = sensitivity_max(integrated_gradients.attribute, image.to(device), target=pred, n_steps=5)\n\n    print(\"Sensitivity\", sens.item())\n    sens_list.append(sens.item())\n    print(\"\\n\")","354e7b63":"%%time\n\ngradient_shap = GradientShap(model)\ndef gradient_shap_visualize(interpretation_image, pred_ix, transformed_img):\n    start_time = time.time()\n    print(class_names[pred_ix])\n    \n    default_cmap = LinearSegmentedColormap.from_list('custom blue', \n                                                     [(0, '#ffffff'),\n                                                      (0.25, '#000000'),\n                                                      (1, '#000000')], N=256)\n\n    # Defining baseline distribution of images\n    rand_img_dist = torch.cat([interpretation_image * 0, interpretation_image * 1])\n\n    attributions_gs = gradient_shap.attribute(interpretation_image.to(device),\n                                              baselines=rand_img_dist.to(device),\n                                              target=pred_ix)\n\n    _ = viz.visualize_image_attr_multiple(np.transpose(attributions_gs.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                          np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                          [\"original_image\", \"blended_heat_map\", \"heat_map\"],\n                                          [\"all\", \"absolute_value\", \"absolute_value\"],\n                                          cmap=default_cmap,\n                                          show_colorbar=True)\n    print(\"time\", time.time() - start_time)\n    \n    infid = infidelity(model.to(device), perturb_fn, interpretation_image.to(device), attributions_gs, n_perturb_samples=1)\n    print(\"Infidelity\", infid.item())\n    inf_list.append(infid.item())","ca3e61ed":"%%time\nfor image, pred, transformed_img in images_interpreted:\n    rand_img_distr = torch.cat([image * 0, image * 1])\n    gradient_shap_visualize(image, pred, transformed_img)\n    sens = sensitivity_max(gradient_shap.attribute, image.to(device), target=pred, baselines=rand_img_distr.to(device))\n\n    print(\"Sensitivity\", sens.item())\n    sens_list.append(sens.item())\n    print(\"\\n\")","536e8ef9":"torch.cuda.empty_cache()","12c6438e":"%%time\n\ndls = DeepLiftShap(model)\nbatch = next(iter(dataloaders['val']))\ndef deeplift_shap_visualize(interpretation_image, pred_ix, transformed_img):\n    start_time = time.time()\n    print(class_names[pred_ix])\n\n    images, pred = batch\n    \n    baseline = images[:-1].to(\"cuda\")*0\n    \n    image = interpretation_image.to(device)\n\n    attributions_dls = dls.attribute(image,\n                                    baselines=baseline,\n                                    target=pred_ix)\n    _ = viz.visualize_image_attr_multiple(np.transpose(attributions_dls.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                          np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                          [\"original_image\", \"blended_heat_map\", \"heat_map\"],\n                                          [\"all\", \"all\", \"all\"],\n                                          show_colorbar=True,\n                                          outlier_perc=2\n                                         )\n    print(\"time\", time.time() - start_time)\n    \n    infid = infidelity(model.to(device), perturb_fn, interpretation_image.to(device), attributions_dls, n_perturb_samples=1)\n    print(\"Infidelity\", infid.item())\n    inf_list.append(infid.item())\n    sens_list.append(\"-\")\n    torch.cuda.empty_cache()\n    \n#     sens = sensitivity_max(dls.attribute, image, target=pred_ix, baselines=baseline)\n#     print(\"Sensitivity\", sens.item())","43fac013":"%%time\nfor image, pred, transformed_img in images_interpreted:    \n    deeplift_shap_visualize(image, pred, transformed_img)\n    print(\"\\n\")","a83ff079":"torch.cuda.empty_cache()","1ab90168":"gradcam_sens_list = []\ngradcam_inf_list = []","1c54930e":"%%time \n\nfrom captum.attr import GuidedGradCam\n\ndef guided_gc_shap_visualize(interpretation_image, pred_ix, transformed_img):\n    print(class_names[pred_ix])\n    for layer in [model.layer1,model.layer2,model.layer3,model.layer4]:\n        start_time = time.time()\n        guided_gc = GuidedGradCam(model, layer)\n        attribution = guided_gc.attribute(interpretation_image.to(device), pred_ix)\n\n        _ = viz.visualize_image_attr_multiple(np.transpose(attribution.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                              np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                              [\"original_image\", \"blended_heat_map\", \"heat_map\"],\n                                              [\"all\", \"all\", \"all\"],\n                                              show_colorbar=True,\n                                              outlier_perc=2\n                                             )\n        print(\"time\", time.time() - start_time)\n        \n        infid = infidelity(model.to(device), perturb_fn, interpretation_image.to(device), attribution, n_perturb_samples=1)\n        print(\"Infidelity\", infid.item())\n        gradcam_inf_list.append(infid.item())\n        \n        sens = sensitivity_max(guided_gc.attribute, interpretation_image.to(device), target=pred_ix)\n        print(\"Sensitivity\", sens.item())\n        gradcam_sens_list.append(sens.item())\n        \n","151d0439":"%%time\nfor image, pred, transformed_img in images_interpreted:\n    guided_gc_shap_visualize(image, pred, transformed_img)\n    print(\"\\n\")","790aa468":"## Occlusion","d5dcb07e":"## Get Dataset","dbdd26d4":"## Gradient Shap","aed3f326":"## DeepLift Shap","9ef00cd4":"## Load ResNet","6563381d":"## GradCam","afed35ba":"## DeepLift","6132df1e":"## Pertubation function","fc00dfe6":"## Integrated Gradients","bc986c12":"## Init Pandas"}}