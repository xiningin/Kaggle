{"cell_type":{"591b5c81":"code","67f3f821":"code","46374575":"code","4e4d9d61":"code","abe9dce3":"code","54b0734d":"code","0a12319b":"code","cd90bcae":"code","2932d65b":"code","470094c8":"code","a54fa05a":"code","5f7f814e":"code","84c93f65":"code","860d97ef":"code","a7812e1f":"code","4d5bee1b":"code","74ec1efb":"code","96ce412a":"code","605151d9":"code","c712d83b":"code","dd886d92":"code","bcdccced":"code","05615033":"code","8121f600":"code","d90ce774":"code","a249956d":"code","6e6564ec":"code","55fac015":"code","a0e8714d":"code","5a088515":"code","88a39209":"code","6e63145f":"code","5ebdb94f":"code","72166dc1":"markdown","3bbd1c97":"markdown","37739076":"markdown","689cf088":"markdown","4c319022":"markdown","c06e41bf":"markdown","1fc9993a":"markdown","dfb08fdb":"markdown","ddcf7bcf":"markdown","dc61ad36":"markdown","07ea5467":"markdown","18e4831e":"markdown","74603971":"markdown"},"source":{"591b5c81":"import pandas as pd\nimport numpy as np\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, roc_curve, plot_roc_curve, auc, precision_recall_curve, confusion_matrix, classification_report, f1_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style=\"white\", color_codes=True)\nsns.set_style('ticks')\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","67f3f821":"train = pd.read_csv('..\/input\/user-data-for-fraud-detection\/train.csv', header=None)\ntrain_labels = pd.read_csv('..\/input\/user-data-for-fraud-detection\/train_labels.csv', header=None)\ntest = pd.read_csv('..\/input\/user-data-for-fraud-detection\/test.csv', header=None)","46374575":"print('Rows and columns in train dataset:', train.shape)\nprint('Rows and columns in train labels dataset:', train_labels.shape)\nprint('Rows and columns in test dataset:', test.shape)","4e4d9d61":"train.head()","abe9dce3":"train_labels.columns = ['labels']","54b0734d":"train_labels.head()","0a12319b":"test.head()","cd90bcae":"print('Missing value in train dataset:', sum(train.isnull().sum()))\nprint('Missing value in test dataset:', sum(test.isnull().sum()))","2932d65b":"#change the lables 1 and 2 to 1.\n\ntrain_labels = train_labels.applymap(lambda x: 1 if x == 2 or x == 1 else 0) ","470094c8":"plt.figure(figsize=(6, 4))\nsns.barplot(train_labels.labels.value_counts().index.astype(int),\n            train_labels.labels.value_counts().values, palette='bwr')\nplt.ylabel('Number of rows', fontsize=12)\nplt.xlabel('Target', fontsize=12)\nplt.show();","a54fa05a":"#Oversampling data\n\noversample = SMOTE()\nsmote = SMOTE(random_state = 42)\nX_smote, y_smote = smote.fit_resample(train, train_labels)","5f7f814e":"plt.figure(figsize=(6, 4))\nsns.barplot(y_smote.labels.value_counts().index.astype(int),\n            y_smote.labels.value_counts().values, palette='bwr')\nplt.ylabel('Number of rows', fontsize=12)\nplt.xlabel('Target', fontsize=12)\nplt.title('After Upsampling')\n\nplt.show();","84c93f65":"#Splitting data\n\nX_train, X_test, y_train, y_test = train_test_split(X_smote,\n                                                    y_smote,\n                                                    test_size=0.2,\n                                                    random_state=42)","860d97ef":"#Make pipeline add there Scaler and Model\n\nrf_pipe = Pipeline(steps =[('scale', StandardScaler()), (\"RF\", RandomForestClassifier(random_state=42))])\nada_pipe = Pipeline(steps =[('scale', StandardScaler()), (\"RF\", AdaBoostClassifier(random_state=42))])\nsvm_pipe = Pipeline(steps =[('scale', StandardScaler()), (\"RF\", SVC(random_state=42, kernel='rbf'))])\nxgb_pipe = Pipeline(steps =[('scale', StandardScaler()), (\"RF\", XGBClassifier(random_state=42, n_jobs = 2))])\n\nrf_f1_cross_val_scores = cross_val_score(rf_pipe, X_train, y_train, cv=5, scoring='f1')\nada_f1_cross_val_scores=cross_val_score(ada_pipe, X_train, y_train, cv=5, scoring='f1')\nsvm_f1_cross_val_scores=cross_val_score(svm_pipe, X_train, y_train, cv=5, scoring='f1')\nxgb_f1_cross_val_scores=cross_val_score(xgb_pipe, X_train, y_train, cv=5, scoring='f1')","a7812e1f":"print('Random Forest Model f1 score: ', rf_f1_cross_val_scores.max())\nprint('AdaBoost Model f1 score: ', rf_f1_cross_val_scores.max())\nprint('SVM Model f1 score: ', rf_f1_cross_val_scores.max())\nprint('XGB Model f1 score: ', rf_f1_cross_val_scores.max())","4d5bee1b":"#Model Evaluation\n\nrf_pipe.fit(X_train, y_train)\nrf_prediction = rf_pipe.predict(X_test)\n\nada_pipe.fit(X_train, y_train)\nada_prediction = ada_pipe.predict(X_test)\n\nsvm_pipe.fit(X_train, y_train)\nsvm_prediction = svm_pipe.predict(X_test)\n\nxgb_pipe.fit(X_train, y_train)\nxgb_prediction = xgb_pipe.predict(X_test)","74ec1efb":"print('F1 Score of Random Forest Model On Test Set - {}'.format(f1_score(rf_prediction, y_test)))\nprint('F1 Score of AdaBoost Model On Test Set - {}'.format(f1_score(ada_prediction, y_test)))\nprint('F1 Score of SVM Model On Test Set - {}'.format(f1_score(svm_prediction, y_test)))\nprint('F1 Score of XGB Model On Test Set - {}'.format(f1_score(xgb_prediction, y_test)))","96ce412a":"#Create instance of Decision Tree Classifier\nxgb_pipe.fit(X_train, y_train)\n\n#Predictions and Evaluations of Decision Tree\npredictions = xgb_pipe.predict(X_test)","605151d9":"print(classification_report(y_test, predictions))","c712d83b":"cf_matrix = confusion_matrix(y_test, predictions)\ngroup_names = ['True Neg', 'False Pos', 'False Neg' , 'True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in \n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in \n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\nplt.title('Before tuning')\n\nplt.show()","dd886d92":"xgb_model =  XGBClassifier(random_state=42, n_jobs = -1)\n\nparams = {   \n    \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n    \"max_depth\"        : [4, 6, 8, 10, 12, 14],\n    \"min_child_weight\" : [1, 3, 5, 7 ],\n    \"gamma\"            : [0.0, 0.1, 0.2 , 0.3, 0.4],\n    \"colsample_bytree\" : [0.3, 0.4, 0.5, 0.7]\n        }\n\ngridF = RandomizedSearchCV(xgb_model, params, cv = 5, verbose = 1)","bcdccced":"clf_grid = gridF.fit(X_train, y_train)","05615033":"clf_grid.best_params_","8121f600":"best_est = clf_grid.best_estimator_","d90ce774":"predictions_tuned = best_est.predict(X_test)","a249956d":"print(classification_report(y_test, predictions_tuned))","6e6564ec":"cf_matrix = confusion_matrix(y_test, predictions_tuned)\ngroup_names = ['True Neg', 'False Pos', 'False Neg' , 'True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in \n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in \n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\nplt.title('After tuning')\n\nplt.show()","55fac015":"#Fitting the model with best parameters\n\nbest_est.fit(X_smote, y_smote)","a0e8714d":"#Predicting\n\nresult = best_est.predict(test)","5a088515":"print('Rows in predicted:', result.shape)\nprint('Rows and columns in test dataset:', test.shape)","88a39209":"#Creating column in test dataset with result\n\ntest['prediction'] = result","6e63145f":"test.head()","5ebdb94f":"test.to_csv(\"test_result.csv\", index = 0)","72166dc1":"#### <p style=\"text-align: center; font-size: 20px\">Loading data<\/p>","3bbd1c97":"The XGB Model showed itself best of all","37739076":"#### <p style=\"text-align: center; font-size: 20px\">Model Selection<\/p>","689cf088":"#### <p style=\"text-align: center; font-size: 20px\">General Analysis<\/p>","4c319022":"Precision bigger then 95%","c06e41bf":"Target \n- 0 - Decent user\n- 1 \u2013 Fraudulent account\n  \nAs you can see, here we have imbalanced data. Our data set is imbalanced as our class is not a 50\/50 or 60\/40 distribution.","1fc9993a":"#### <p style=\"text-align: center; font-size: 20px\">Result<\/p>","dfb08fdb":"After tuning hyperparameters","ddcf7bcf":"#### <p style=\"text-align: center; font-size: 20px\">Target Distribution<\/p>","dc61ad36":"#### <p style=\"text-align: center; font-size: 20px\">Tuning Hyperparameters <\/p>","07ea5467":"#### <p style=\"text-align: center; font-size: 20px\">Data Upsampling Using SMOTE<\/p>","18e4831e":"#### <p style=\"text-align: center; font-size: 20px\">Setup<\/p>","74603971":"Before tuning hyperparameters"}}