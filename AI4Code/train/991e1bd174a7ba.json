{"cell_type":{"654a5631":"code","fbc555c4":"code","951521cb":"code","521f1293":"code","561d6adc":"code","48de777f":"code","b0265056":"code","52e265ac":"code","1c584846":"code","754349d7":"code","9812233a":"code","c727a029":"code","90890c08":"code","1258454f":"code","b508ee0e":"code","86a673d5":"code","09edfd3d":"code","9cacd109":"code","c20e32c7":"code","d0709f58":"code","a55bf40b":"code","f5a08f6d":"code","7dc99efd":"code","70a40ca9":"code","df7668fa":"code","ff752144":"code","b8cb3f4d":"code","0e088334":"code","c7e5efbd":"code","526b505c":"code","1c8c91ac":"code","156e386c":"code","56124412":"code","cf0424b5":"code","9e39daf2":"code","eed3bd8d":"code","175b1649":"code","757a7bc3":"code","99dbf8be":"code","23cb053f":"code","5b71037f":"markdown","d2abfddd":"markdown","2c6ae7b0":"markdown","13cb456f":"markdown","811ad572":"markdown","ccfaa1aa":"markdown","48931364":"markdown","83db47b9":"markdown","95086d22":"markdown","2eb3c905":"markdown","862f8d97":"markdown","1ab0c13b":"markdown","ab52d9f9":"markdown","b697b62f":"markdown","5ca8a782":"markdown","1fe7175c":"markdown","bfbcd518":"markdown","a53177ee":"markdown","294dbd94":"markdown","ea3b647d":"markdown","c8355815":"markdown","1f2f498c":"markdown","8adeb580":"markdown","4d27c017":"markdown","37070cf7":"markdown","bea6548c":"markdown","c23fb701":"markdown","f167dbee":"markdown","f66bd3b1":"markdown","843307c6":"markdown"},"source":{"654a5631":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport ast\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport scipy\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom sklearn import metrics   #Additional scklearn functions\n\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\n#rcParams['figure.figsize'] = 12, 4","fbc555c4":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nfull_data = [train, test]\n\n# Convert string into dict\n# 'belongs_to_collection'\ndict_columns = ['genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\nfor dataset in full_data:\n    for column in dict_columns:\n        dataset[column] = dataset[column].apply(lambda x: [] if x!=x else ast.literal_eval(x)) # x!=x means x is nan","951521cb":"train.head(5)","521f1293":"train.isnull().sum()","561d6adc":"for index, row in train['belongs_to_collection'][0:5].iteritems():\n    print(index, row)","48de777f":"train['belongs_to_collection'].isnull().value_counts()","b0265056":"for dataset in full_data:\n    dataset['HasCollection'] = dataset['belongs_to_collection'].apply(lambda x: 1 if x!=x else 0)\ntrain.drop('belongs_to_collection', axis=1, inplace = True)\ntest.drop('belongs_to_collection', axis=1, inplace = True)\ntrain.head()","52e265ac":"for index, row in train['genres'][0:5].iteritems():\n    print(index, row)\n#train[train['genres'].isna()]\ntrain['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts()","1c584846":"genre_list = train['genres'].apply(lambda x: [i['name'] for i in x])\ngenre_rank = pd.Series(genre_list.sum()).value_counts()\nprint(genre_rank)\n\nfor dataset in full_data:\n    dataset['genres'] = dataset['genres'].apply(lambda x: [i['name'] for i in x])\n    dataset['num_genres'] = dataset['genres'].apply(lambda x: len(x))\n    for i in genre_rank.keys():\n        dataset[i] = dataset['genres'].apply(lambda x: 1 if i in x else 0)\n    dataset[\"all_genres\"] = dataset['genres'].apply(lambda x: \"_\".join(x))\n","754349d7":"\nall_genres_rank = train[\"all_genres\"].value_counts()[0:30]\n\nfor dataset in full_data:\n    for i in all_genres_rank.keys():\n        dataset['all_genres_'+i] = dataset['all_genres'].apply(lambda x: 1 if i in x else 0)\n","9812233a":"train.head()","c727a029":"for index, row in train['production_companies'][0:5].iteritems():\n    print(index, row)\ncompany_list = train['production_companies'].apply(lambda x: [i['name'] for i in x])\ncompany_rank = pd.Series(company_list.sum()).value_counts()\nprint(company_rank[company_rank>30])\nfor dataset in full_data:\n    dataset['production_companies'] = dataset['production_companies'].apply(lambda x: [i['name'] for i in x])\n    dataset['num_companies'] = dataset['production_companies'].apply(lambda x:len(x))\ntrain.head(3)","90890c08":"for dataset in full_data:\n    for i in company_rank.keys()[0:30]:\n        dataset[i] = dataset['production_companies'].apply(lambda x: 1 if i in x else 0)\ntrain.head(3)","1258454f":"for index, row in train.production_countries[0:5].iteritems():\n    print(index, row)\ncountry_list = train['production_countries'].apply(lambda x: [i['name'] for i in x])\ncountry_rank = pd.Series(country_list.sum()).value_counts()\nprint(country_rank[0:15])\n\nfor dataset in full_data:\n    dataset['production_countries'] = dataset['production_countries'].apply(lambda x: [i['name'] for i in x])\n    dataset['num_countries'] = dataset['production_countries'].apply(lambda x:len(x))\n    for i in country_rank.keys()[0:30]:\n        dataset[i] = dataset['production_countries'].apply(lambda x: 1 if i in x else 0)","b508ee0e":"for index, row in train.spoken_languages[0:5].iteritems():\n    print(index, row)\n\nlangauge_list = train['spoken_languages'].apply(lambda x: [i['iso_639_1'] for i in x])\nlangauge_rank = pd.Series(langauge_list.sum()).value_counts()\nprint(langauge_rank[0:15])\nfor dataset in full_data:\n    dataset['spoken_languages'] = dataset['spoken_languages'].apply(lambda x: [i['iso_639_1'] for i in x])\n    dataset['num_languages'] = dataset['spoken_languages'].apply(lambda x:len(x))\n    for i in langauge_rank.keys()[0:30]:\n        dataset[i] = dataset['spoken_languages'].apply(lambda x: 1 if i in x else 0)\n","86a673d5":"langauge_rank","09edfd3d":"for index, row in train['Keywords'][0:5].iteritems():\n    print(index, row)\nkeyword_list = train['Keywords'].apply(lambda x: [i['name'] for i in x])\nkeyword_rank = pd.Series(keyword_list.sum()).value_counts()\nprint(keyword_rank[0:15])\n\nfor dataset in full_data:\n    dataset['Keywords'] = dataset['Keywords'].apply(lambda x: [i['name'] for i in x])\n    dataset['num_keywords'] = dataset['Keywords'].apply(lambda x:len(x))\n    for i in keyword_rank.keys()[0:30]:\n        dataset[i] = dataset['Keywords'].apply(lambda x: 1 if i in x else 0)","9cacd109":"#for index, row in train['cast'][0:1].iteritems():\n#    print(index, row)\n\ncast_name_list = train['cast'].apply(lambda x: [i['name'] for i in x])\ncast_name_rank = pd.Series(cast_name_list.sum()).value_counts()\nprint(cast_name_rank[0:30])\n\ncast_character_list = train['cast'].apply(lambda x: [i['character'] for i in x])\ncast_character_rank = pd.Series(cast_character_list.sum()).value_counts()\nprint(cast_character_rank[0:30])\n\ncast_gender_list = train['cast'].apply(lambda x: [i['gender'] for i in x])\ncast_gender_rank = pd.Series(cast_gender_list.sum()).value_counts()\nprint(cast_gender_rank)\n\nfor dataset in full_data:\n    dataset['cast_name'] = dataset['cast'].apply(lambda x: [i['name'] for i in x])\n    dataset['cast_character'] = dataset['cast'].apply(lambda x: [i['character'] for i in x])\n    dataset['cast_gender'] = dataset['cast'].apply(lambda x: [i['gender'] for i in x])\n    \n    dataset['num_cast'] = dataset['cast'].apply(lambda x:len(x))\n    for i in cast_name_rank.keys()[0:30]:\n        dataset[i] = dataset['cast_name'].apply(lambda x: 1 if i in x else 0)\n    for i in cast_character_rank.keys()[0:30]:\n        dataset[i] = dataset['cast_character'].apply(lambda x: 1 if i in x else 0)\n    for i in cast_gender_rank.keys():\n        dataset[i] = dataset['cast_gender'].apply(lambda x: 1 if i in x else 0)","c20e32c7":"train.head(2)","d0709f58":"### Temporarily ignore this comlumn\n#for index, row in train.crew[0:1].iteritems():\n#    print(index, row)\ncrew_name_list = train['crew'].apply(lambda x: [i['name'] for i in x])\ncrew_name_rank = pd.Series(crew_name_list.sum()).value_counts()\nprint(crew_name_rank[0:15])\n\ncrew_job_list = train['crew'].apply(lambda x: [i['job'] for i in x])\ncrew_job_rank = pd.Series(crew_job_list.sum()).value_counts()\nprint(crew_job_rank[0:15])\n\ncrew_department_list = train['crew'].apply(lambda x: [i['department'] for i in x])\ncrew_department_rank = pd.Series(crew_department_list.sum()).value_counts()\nprint(crew_department_rank[0:15])\n\ncrew_gender_list = train['crew'].apply(lambda x: [i['gender'] for i in x])\ncrew_gender_rank = pd.Series(crew_gender_list.sum()).value_counts()\nprint(crew_gender_rank)\n\nfor dataset in full_data:\n    dataset['crew_name'] = dataset['crew'].apply(lambda x: [i['name'] for i in x])\n    dataset['crew_job'] = dataset['crew'].apply(lambda x: [i['job'] for i in x])\n    dataset['crew_department'] = dataset['crew'].apply(lambda x: [i['department'] for i in x])\n    dataset['crew_gender'] = dataset['crew'].apply(lambda x: [i['gender'] for i in x])\n   \n    dataset['num_crew'] = dataset['crew'].apply(lambda x:len(x))\n    for i in crew_name_rank.keys()[0:30]:\n        dataset[i] = dataset['crew_name'].apply(lambda x: 1 if i in x else 0)\n    for i in crew_job_rank.keys()[0:30]:\n        dataset[i] = dataset['crew_job'].apply(lambda x: 1 if i in x else 0)\n    for i in crew_department_rank.keys()[0:30]:\n        dataset[i] = dataset['crew_department'].apply(lambda x: 1 if i in x else 0)\n    for i in crew_gender_rank.keys():\n        dataset[i] = dataset['crew_gender'].apply(lambda x: 1 if i in x else 0)\n","a55bf40b":"for dataset in full_data:\n    dataset['HasHomepage'] = dataset['homepage'].apply(lambda x: 1 if x==x else 0)","f5a08f6d":"for dataset in full_data:\n    le = LabelEncoder()\n    dataset['original_langauge_code'] = le.fit_transform(dataset['original_language'])","7dc99efd":"train.head(5)","70a40ca9":"# Fill the missing data\ntest.loc[test['release_date'].isnull() == True, 'release_date'] = '01\/01\/98'","df7668fa":"for dataset in full_data:\n    dataset['real_release_date'] = dataset['release_date'].apply(lambda x: (x[:-2] + '20' + x.split('\/')[2]) \n                                                                  if int(x.split('\/')[2]) <= 19 else (x[:-2] + '19' + x.split('\/')[2]))\n    dataset['real_release_date'] = pd.to_datetime(dataset['real_release_date'])\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        dataset['release_date_' + part] = getattr(dataset['real_release_date'].dt, part).astype(int)\n  ","ff752144":"train.head()","b8cb3f4d":"# data fixes from https:\/\/www.kaggle.com\/somang1418\/happy-valentines-day-and-keep-kaggling-3\ntrain.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\ntrain.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \ntrain.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\ntrain.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\ntrain.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \ntrain.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\ntrain.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\ntrain.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\ntrain.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\ntrain.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\ntrain.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\ntrain.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\ntrain.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\ntrain.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \ntrain.loc[train['id'] == 1542,'budget'] = 1              # All at Once\ntrain.loc[train['id'] == 1570,'budget'] = 15800000       # Crocodile Dundee II\ntrain.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\ntrain.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\ntrain.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\ntrain.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\ntrain.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\ntrain.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\ntrain.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\ntrain.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\ntrain.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\ntest.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\ntest.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\ntest.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\ntest.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\ntest.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\ntest.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\ntest.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\ntest.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\ntest.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\ntest.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee\n\npower_six = train.id[train.budget > 1000][train.revenue < 100]\n\nfor k in power_six :\n    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000","0e088334":"fig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train['revenue']);\nplt.title('Distribution of revenue');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(train['revenue']));\nplt.title('Distribution of log of revenue');","c7e5efbd":"train.head(2)","526b505c":"drop_columns = ['id', 'genres', 'homepage', 'imdb_id', 'original_language', 'original_title', 'overview', 'poster_path', 'production_companies',\n               'production_countries', 'release_date', 'spoken_languages', 'status', 'tagline', 'title', 'Keywords', 'cast', 'crew', 'real_release_date',\n               'cast_name', 'cast_character', 'cast_gender', 'crew_name', 'crew_job', 'crew_department', 'crew_gender', 'all_genres']\ntrain['runtime'].fillna(train['runtime'].median(), inplace=True)\ntest['runtime'].fillna(train['runtime'].median(), inplace=True)\n\ntrain.drop(drop_columns, axis=1, inplace=True)\ntest.drop(drop_columns, axis=1, inplace=True)","1c8c91ac":"print(train.dtypes)","156e386c":"X = train.drop('revenue', axis=1)\ny = train['revenue']\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nprint(X.shape, y.shape, test.shape)\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\nprint(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)","56124412":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport math\n\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle(real, predicted):\n    sum=0.0\n    for x in range(len(predicted)):\n        if predicted[x]<0 or real[x]<0: #check for negative values\n            continue\n        p = np.log(predicted[x]+1)\n        r = np.log(real[x]+1)\n        sum = sum + (p - r)**2\n    return (sum\/len(predicted))**0.5\n\n\ndef modelfit(alg, X_train, y_train, X_valid, y_valid, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, \n                          metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(X_train, y_train,eval_metric='rmse')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(X_valid)\n      \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"RMSE : %.4f\" % rmsle(y_valid.values, dtrain_predictions))\n                    \n    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)[0:20]\n    \n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')","cf0424b5":"#Choose all predictors except target & IDcols\nimport xgboost as xgb\nxgb1 = xgb.XGBRegressor(\n    learning_rate =0.1,\n    n_estimators=140,\n    max_depth=4,\n    min_child_weight=4,\n    gamma=0,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    nthread=4,\n    scale_pos_weight=1,\n    seed=27)\n\nmodelfit(xgb1, X_train, y_train, X_valid, y_valid)","9e39daf2":"final = xgb.XGBRegressor(learning_rate =0.003, n_estimators=170, max_depth=4,\n                                                  min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,nthread=4, \n                                                  scale_pos_weight=1, seed=27)\nfinal.fit(X,y,eval_metric='rmse')\nxgb_y_pred = final.predict(test)\nsub['revenue'] = xgb_y_pred\nsub.to_csv(\"XGBoost_advance.csv\", index=False)","eed3bd8d":"'''\n# XGBoosting\nimport xgboost as xgb\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\nxg_reg.fit(X,y)\nXG_y_pred = xg_reg.predict(test)\nsub['revenue'] = XG_y_pred\nsub.to_csv(\"XG.csv\", index=False)\nprint(X.shape, y.shape, test.shape)\n'''","175b1649":"'''\nimport xgboost as xgb\nxgb_params = {'objective': 'reg:linear'}\nparams = {'objective': 'reg:linear', \n    'eta': 0.01, \n    'max_depth': 6, \n    'subsample': 0.6, \n    'colsample_bytree': 0.7,  \n    'eval_metric': 'rmse', \n    'seed': 2019, \n    'silent': True,\n}\n\ntrain_data = xgb.DMatrix(X, label=y)\n#watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\nxgb_model = xgb.train(dtrain=train_data, num_boost_round=200, params=params)\nxgb_y_pred = xgb_model.predict(xgb.DMatrix(test))\n\nsub['revenue'] = xgb_y_pred\nsub.to_csv(\"XGBoost.csv\", index=False)\n'''","757a7bc3":"# LGB\nimport lightgbm as lgb\n#params = {'objective': 'regression'}\nparams = {'objective':'regression',\n    'num_leaves' : 30,\n    'min_data_in_leaf' : 20,\n    'max_depth' : 9,\n    'learning_rate': 0.004,\n    #'min_child_samples':100,\n    'feature_fraction':0.9,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.9,\n    'lambda_l1': 0.2,\n    \"bagging_seed\": 2019,\n    \"metric\": 'rmse',\n    #'subsample':.8, \n    #'colsample_bytree':.9,\n    \"random_state\" : 2019,\n    \"verbosity\": -1\n}\nmodel1 = lgb.LGBMRegressor(**params, n_estimators = 3000)\nmodel1.fit(X_train.values, y_train.values, \n        eval_set=[(X_train.values, y_train.values), (X_valid.values, y_valid.values)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)\n\nLGB_y_pred = model1.predict(test, num_iteration=model1.best_iteration_)\nsub['revenue'] = LGB_y_pred\nsub.to_csv(\"LGB.csv\", index=False)\nprint(LGB_y_pred[0:10].astype(int))","99dbf8be":"# GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nGDB = GradientBoostingRegressor().fit(X,y)\nGDB_y_pred = GDB.predict(test)\n\nsub['revenue'] = GDB_y_pred\nsub.to_csv(\"GDB.csv\", index=False)","23cb053f":"###### BaggingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nBR = BaggingRegressor().fit(X,y)\nBR_y_pred = BR.predict(test)\nsub['revenue'] = BR_y_pred\nsub.to_csv(\"BR.csv\", index=False)","5b71037f":"param_test4 = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\n\ngsearch4 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.01, n_estimators=140, max_depth=4,\n                                                  min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,nthread=4, \n                                                  scale_pos_weight=1, seed=27), param_grid = param_test4, scoring='neg_mean_squared_error',\n                                                  n_jobs=-1,iid=False, cv=5)\ngsearch4.fit(X_train,y_train)\ngsearch4.best_params_, gsearch4.best_score_","d2abfddd":"#from sklearn.metrics import make_scorer\n#score = make_scorer(rmsle, greater_is_better=False)\n\nparam_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\ngsearch1 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=5,\n                                                  min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,nthread=4, \n                                                  scale_pos_weight=1, seed=27), param_grid = param_test1, scoring='neg_mean_squared_error',\n                                                  n_jobs=4,iid=False, cv=5)\ngsearch1.fit(X_train,y_train)\ngsearch1.best_params_, gsearch1.best_score_","2c6ae7b0":"### 3. LGBoosting","13cb456f":"## original_language","811ad572":"Note: Thus, we will max_depth=3, min_child_weight=5 as our best parameters!","ccfaa1aa":"## release_date","48931364":"param_test6 = {\n 'reg_alpha':[0, 0.01, 0.05, 0.1, 0.5]\n}\ngsearch6 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.01, n_estimators=140, max_depth=4,\n                                                  min_child_weight=4, gamma=0, subsample=0.9, colsample_bytree=0.7,nthread=4, \n                                                  scale_pos_weight=1, seed=27), param_grid = param_test6, scoring='neg_mean_squared_error',\n                                                  n_jobs=-1,iid=False, cv=5)\ngsearch6.fit(X_train,y_train)\ngsearch6.best_params_, gsearch6.best_score_","83db47b9":"param_test3 = {\n 'gamma':[i\/100.0 for i in range(0,20, 3)]\n}\ngsearch3 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=4,\n                                                  min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,nthread=4, \n                                                  scale_pos_weight=1, seed=27), param_grid = param_test3, scoring='neg_mean_squared_error',\n                                                  n_jobs=-1,iid=False, cv=5)\ngsearch3.fit(X_train,y_train)\ngsearch3.best_params_, gsearch3.best_score_","95086d22":"Note: This time we select max_depth=4, min_child_weight=4 as our best parameters!","2eb3c905":"## Keywords","862f8d97":"## cast","1ab0c13b":"### 2. XGB in another way","ab52d9f9":"Note: Hidden Linear Regression Code\n\n<!---\nfrom sklearn.linear_model import LinearRegression\n# LinearRegression\nLR = LinearRegression()\nLR.fit(X,y)\nLR_y_pred = LR.predict(test)\nsub['revenue'] = LR_y_pred\nsub.to_csv(\"LR.csv\", index=False)\n--->","b697b62f":"param_test5 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch5 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.01, n_estimators=140, max_depth=4,\n                                                  min_child_weight=4, gamma=0, subsample=0.9, colsample_bytree=0.7,nthread=4, \n                                                  scale_pos_weight=1, seed=27), param_grid = param_test5, scoring='neg_mean_squared_error',\n                                                  n_jobs=-1,iid=False, cv=5)\ngsearch5.fit(X_train,y_train)\ngsearch5.best_params_, gsearch5.best_score_","5ca8a782":"Note: Hidden AdaBoostRegressor Code\n\n<!---\n# AdaBoostRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.model_selection import GridSearchCV\nABR = AdaBoostRegressor()\ntuned_parameters = [{'n_estimators': [10, 25, 50, 75, 100], 'learning_rate': [0.1, 0.3, 1, 3],\n                     'loss': ['linear','square']}]\n#k = cross_validate(ABR, X, y, scoring='neg_mean_squared_error',cv=10)\n#, 'square', 'exponential'\nclf = GridSearchCV(AdaBoostRegressor(), tuned_parameters, cv=5,\n                       scoring='mean_squared_error')\nclf.fit(X, y)\nprint(clf.best_params_)\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, clf.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\nprint()\nABR_y_pred = clf.predict(test)\nprint()\nsub['revenue'] = ABR_y_pred\nsub.to_csv(\"ABR.csv\", index=False)\n--->","1fe7175c":"## Modeling","bfbcd518":"modelfit(gsearch6.best_estimator_,  X_train, y_train, X_valid, y_valid)","a53177ee":"## spoken_languages","294dbd94":"## belongs_to_collection","ea3b647d":"## production_companies","c8355815":"modelfit(gsearch4.best_estimator_,  X_train, y_train, X_valid, y_valid)","1f2f498c":"## homepage","8adeb580":"## Get new data from real world and fillthe missing data","4d27c017":"## crew","37070cf7":"## production_countries","bea6548c":"# Data Exploration","c23fb701":"modelfit(gsearch3.best_estimator_,  X_train, y_train, X_valid, y_valid)","f167dbee":"## genres\nNote: This process may be able to use dummy node to expand the genres in the future!!!","f66bd3b1":"param_test2 = {\n 'max_depth': [2,3,4],\n 'min_child_weight': [4,5,6]\n}\ngsearch2 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=3,\n                                                  min_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,nthread=4, \n                                                  scale_pos_weight=1, seed=27), param_grid = param_test2, scoring='neg_mean_squared_error',\n                                                  n_jobs=4,iid=False, cv=5)\ngsearch2.fit(X_train,y_train)\ngsearch2.best_params_, gsearch2.best_score_","843307c6":"### 1. XGBoosting"}}