{"cell_type":{"f1317f24":"code","0e498ceb":"code","16735470":"code","bea90ccd":"code","fb4cbc92":"code","1ee767f0":"code","e3a26961":"code","19b9f9fa":"code","bd2dddc5":"code","949acc00":"code","8468b9b1":"code","d5ab70bc":"code","07256656":"code","9324c2d3":"code","f1fd92b4":"code","4351f9ac":"code","81de216d":"code","21b97d58":"code","250a28b3":"code","ff666548":"code","6089adf5":"code","ee6aae9f":"code","1cc32001":"code","300a3c75":"code","ce87b3b0":"code","034db430":"code","ce422a91":"code","c941e814":"code","d39c5555":"code","c8172ee3":"code","84617217":"code","71eced4e":"code","2f17552e":"code","5a693012":"code","5fbd5a4e":"code","6fa07b14":"code","53def90d":"code","961c6063":"code","1dea72e9":"code","2abd931b":"code","9aee854a":"code","690f73f9":"code","9afc6bcf":"code","875c0622":"code","d485326c":"code","71abca01":"code","02c8765b":"code","c56c2c49":"code","5b2fa733":"code","0874e513":"code","c924538c":"code","40ea4fb4":"code","5d66ec27":"code","add6d288":"code","2d10bcdb":"code","051e8c6d":"code","99e19213":"code","64df5ab7":"code","c43d2d4b":"code","cab397f8":"code","def65a46":"code","4dff4cf9":"code","27498e1e":"code","48cc4562":"code","d8a94cb8":"code","10b158cb":"code","e5a0875d":"code","91d91ae6":"code","a549e289":"code","e904102b":"code","863459d8":"code","85b4bb83":"code","00217d87":"code","7d7467d4":"code","7ec4f400":"code","143a2fb5":"code","7d2b0ee8":"code","5979af5d":"code","7ee9bad0":"code","cb806e23":"code","bc26f99b":"code","bd4112e9":"code","8264cc40":"code","b1163304":"code","6940e399":"code","83c4d48f":"code","8c92114d":"code","f506dc9d":"code","28d8b3d8":"code","69f673b2":"code","549064f9":"code","e09dcf76":"code","e1c660fe":"code","32197149":"code","e9290261":"code","d5a0dd74":"code","6f4604aa":"code","7a0d3224":"code","03a8e274":"code","0b96b235":"code","4c559112":"code","a9ca975a":"code","cdfd3f37":"code","23df985a":"code","97a59581":"code","9a44567c":"code","cb11200e":"code","261a0a52":"code","e762adb2":"code","9f017a52":"code","669b00e2":"code","789862f0":"code","33e46e1b":"code","0c8b00f9":"code","4d3e0d0e":"code","ba46d6cc":"code","f2e70dad":"code","c404340f":"code","968bebd1":"code","942b8958":"code","7b6f2f90":"code","f98b54d8":"code","24a3a3da":"code","f1604e3c":"code","181e0c01":"code","76fa7be0":"code","5c5bf423":"code","ffa5ba76":"code","1c0dd2f0":"code","c42d6f13":"code","c5690d72":"code","b323daa7":"code","c19b88bc":"code","25cc3523":"code","f70e7cef":"code","da1a195d":"code","34705bbc":"code","c8afe593":"code","786e0410":"code","371aa59a":"code","54d8d4d9":"code","008cf8ee":"code","5f69e604":"code","64d6e395":"code","b513d5c4":"code","b049e109":"code","3f05efe3":"code","f2b448e2":"code","e01a56f9":"code","7ba4e2d8":"code","cef0e309":"code","ba6e0e63":"markdown","5c476691":"markdown","d9b2f0f9":"markdown","5a20cf72":"markdown","14530edd":"markdown","098898f6":"markdown","5c8e068c":"markdown","2a49f6a4":"markdown","b2f638f7":"markdown","51980603":"markdown","d4f909da":"markdown","204b061f":"markdown","02940649":"markdown","d1f4a3a2":"markdown","30bef9a3":"markdown","81b55ce6":"markdown","bfee3ea8":"markdown","eab00c72":"markdown","3ffaaeac":"markdown","62cdf6c5":"markdown","294cf986":"markdown","3177fe7c":"markdown","d72bcf09":"markdown","9235601e":"markdown","8d9c7e22":"markdown","3b2f8b34":"markdown","6c110a05":"markdown","db6865ad":"markdown","ec7db217":"markdown","f18dfca3":"markdown","78099ad6":"markdown","cd7c47ee":"markdown","7d5d104f":"markdown","d442c81b":"markdown","83fba33a":"markdown","3928a351":"markdown","941224d5":"markdown","0fd28e8f":"markdown","efc707c0":"markdown","289b6785":"markdown"},"source":{"f1317f24":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.metrics import roc_auc_score # official evaluation score of challenge\nfrom sklearn.metrics import roc_curve\nimport xgboost as xgb\nimport lightgbm as lgbm\n\nimport warnings\nimport gc\nwarnings.filterwarnings(\"ignore\")","0e498ceb":"def load_data(which=\"sample\", skiprows=160000000):\n    dtypes = {\n        \"ip\" : \"uint64\",\n        \"app\": \"uint64\",\n        \"device\": \"uint64\",\n        \"os\": \"uint64\",\n        \"channel\": \"uint64\",\n        \"is_attributed\": \"uint64\"\n    }\n    if which == \"sample\":\n        data = pd.read_csv(\"..\/input\/train_sample.csv\", dtype=dtypes)\n    elif which == \"whole\":\n        data = pd.read_csv(\"..\/input\/train.csv\", names = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'attributed_time', 'is_attributed'], skiprows=skiprows, dtype=dtypes)\n    return data","16735470":"train_df = load_data(which=\"sample\") # which = \"sample\" or \"whole\"\ntrain_df.head()","bea90ccd":"train_df.info()","fb4cbc92":"train_df.isnull().sum()\/len(train_df.index)*100","1ee767f0":"train_df = train_df.drop(\"attributed_time\", axis=1)","e3a26961":"train_df[\"click_month\"] = pd.to_datetime(train_df[\"click_time\"]).dt.month\ntrain_df[\"click_day_of_week\"] = pd.to_datetime(train_df[\"click_time\"]).dt.dayofweek\ntrain_df[\"click_hour\"] = pd.to_datetime(train_df[\"click_time\"]).dt.hour\ntrain_df[\"click_year\"] = pd.to_datetime(train_df[\"click_time\"]).dt.year\ntrain_df = train_df.drop(\"click_time\", axis=1)","19b9f9fa":"train_df.head()","bd2dddc5":"train_df.click_year.value_counts()","949acc00":"train_df = train_df.drop([\"click_year\", \"click_month\"], axis=1)","8468b9b1":"train_df.head()","d5ab70bc":"cols = [\"click_day_of_week\", \"click_hour\"]\ntrain_df[cols] = train_df[cols].astype(\"uint64\")\ntrain_df.info()","07256656":"train_df_int = train_df.select_dtypes(include=[\"uint64\"])\ntrain_df_int = train_df_int.apply(pd.to_numeric, downcast=\"unsigned\")\ntrain_df_int.info()","9324c2d3":"train_df = train_df.drop(train_df.dtypes[train_df.dtypes==\"uint64\"].index, axis=1)\ntrain_df = pd.concat([train_df, train_df_int], axis=1)\ntrain_df.head()","f1fd92b4":"train_df.isnull().sum()","4351f9ac":"variable_value_counts = train_df[\"app\"].value_counts()\nvariable_value_quantile = variable_value_counts[variable_value_counts>variable_value_counts.quantile(0.8)]\nvariable_value_quantile = pd.Series(variable_value_quantile).reset_index(name=\"count\").rename(index=str, columns={\"index\": \"app\", \"count\": \"count\"})\nvariable_value_quantile","81de216d":"shortened_data = pd.merge(train_df, variable_value_quantile, on=\"app\", how=\"inner\").drop(\"count\", axis=1)\n# Plot app count distribution - for 80 % larger counts\nplt.figure(figsize=(18, 8))\nsns.countplot(x=\"app\", data=shortened_data)","21b97d58":"variable_value_counts = train_df[\"device\"].value_counts()\nvariable_value_quantile = variable_value_counts[variable_value_counts>variable_value_counts.quantile(0.8)]\nvariable_value_quantile = pd.Series(variable_value_quantile).reset_index(name=\"count\").rename(index=str, columns={\"index\": \"device\", \"count\": \"count\"})\nvariable_value_quantile","250a28b3":"shortened_data = pd.merge(train_df, variable_value_quantile, on=\"device\", how=\"inner\").drop(\"count\", axis=1)\n# Plot device count distribution - for 80 % larger counts\nplt.figure(figsize=(18, 8))\nsns.countplot(x=\"device\", data=shortened_data)","ff666548":"variable_value_counts = train_df[\"os\"].value_counts()\nvariable_value_quantile = variable_value_counts[variable_value_counts>variable_value_counts.quantile(0.8)]\nvariable_value_quantile = pd.Series(variable_value_quantile).reset_index(name=\"count\").rename(index=str, columns={\"index\": \"os\", \"count\": \"count\"})\nvariable_value_quantile","6089adf5":"shortened_data = pd.merge(train_df, variable_value_quantile, on=\"os\", how=\"inner\").drop(\"count\", axis=1)\n# Plot os count distribution - for 80 % larger counts\nplt.figure(figsize=(18, 8))\nsns.countplot(x=\"os\", data=shortened_data)","ee6aae9f":"variable_value_counts = train_df[\"channel\"].value_counts()\nvariable_value_quantile = variable_value_counts[variable_value_counts>variable_value_counts.quantile(0.5)]\nvariable_value_quantile = pd.Series(variable_value_quantile).reset_index(name=\"count\").rename(index=str, columns={\"index\": \"channel\", \"count\": \"count\"})\nvariable_value_quantile","1cc32001":"shortened_data = pd.merge(train_df, variable_value_quantile, on=\"channel\", how=\"inner\").drop(\"count\", axis=1)\n# Plot os count distribution - for 80 % larger counts\nplt.figure(figsize=(18, 8))\nsns.countplot(x=\"channel\", data=shortened_data)","300a3c75":"train_df.channel.value_counts()[train_df.channel.value_counts() == train_df.channel.value_counts().max()]","ce87b3b0":"perc_attributed = pd.DataFrame((train_df.is_attributed.value_counts()\/train_df.is_attributed.value_counts().sum()*100).values, columns=[\"perc_of_occur[%]\"]).reset_index().rename(columns={\"index\": \"is_attributed\"})\nperc_attributed","034db430":"print(\"{:.1f}% of the IPs are unique and {:.1f}% are repetitions.\".format(len(train_df.ip.unique())\/len(train_df.index)*100, 100-len(train_df.ip.unique())\/len(train_df.index)*100))","ce422a91":"ip_counts = train_df[\"ip\"].value_counts()\nsuspicious_ips = pd.DataFrame(ip_counts[ip_counts>50].reset_index(name=\"count\")).rename(columns={\"index\": \"ip\"}) # 50 clicks or more\nsuspicious_ips","c941e814":"suspicious_ips_shortened = pd.merge(train_df, suspicious_ips, on=\"ip\", how=\"inner\").drop(\"count\", axis=1)\n# Plot IP count distribution\nplt.figure(figsize=(16, 8))\nsns.countplot(x=\"ip\", data=suspicious_ips_shortened)","d39c5555":"train_df.head()","c8172ee3":"suspicious_df = train_df.set_index(\"ip\").loc[suspicious_ips.ip.values]\nsuspicious_df.head()","84617217":"def mode(x):\n    return x.mode()\n\nsuspicious_df.groupby([\"ip\"]).apply(mode)","71eced4e":"ip_fraud_count = suspicious_df[suspicious_df[\"is_attributed\"]==0].groupby(\"ip\").size()\nip_fraud_perc = pd.DataFrame(ip_fraud_count\/suspicious_df.groupby(\"ip\").size()*100, columns=[\"Fraud_Percentage[%]\"], dtype=\"float16\")\ndel(ip_fraud_count)\nip_fraud_perc.head()","2f17552e":"ip_is_attributed = train_df.groupby([\"ip\"]).is_attributed.sum()\nip_is_attributed = ip_is_attributed[ip_is_attributed > 0].sort_values(ascending=False).reset_index(name=\"is_attributed_count\")\nip_is_attributed = ip_is_attributed.iloc[:int(0.1*ip_is_attributed.shape[0])]\nip_is_attributed.head()","5a693012":"sns.set(font_scale=1.0)\nplt.figure(figsize=(18, 6))\nsns.barplot(x=\"ip\", y=\"is_attributed_count\", data=ip_is_attributed)","5fbd5a4e":"app_is_attributed = train_df.groupby([\"app\"]).is_attributed.sum()\napp_is_attributed = app_is_attributed[app_is_attributed > 0].sort_values(ascending=False).reset_index(name=\"is_attributed_count\")\napp_is_attributed = app_is_attributed.iloc[:int(0.5*app_is_attributed.shape[0])]\napp_is_attributed.head()","6fa07b14":"sns.set(font_scale=1.0)\nplt.figure(figsize=(18, 6))\nsns.barplot(x=\"app\", y=\"is_attributed_count\", data=app_is_attributed)","53def90d":"os_is_attributed = train_df.groupby([\"os\"]).is_attributed.sum()\nos_is_attributed = os_is_attributed[os_is_attributed > 0].sort_values(ascending=False).reset_index(name=\"is_attributed_count\")\nos_is_attributed = os_is_attributed.iloc[:int(0.5*os_is_attributed.shape[0])]\nos_is_attributed.head()","961c6063":"sns.set(font_scale=1.0)\nplt.figure(figsize=(18, 6))\nsns.barplot(x=\"os\", y=\"is_attributed_count\", data=os_is_attributed)","1dea72e9":"device_is_attributed = train_df.groupby([\"device\"]).is_attributed.sum()\ndevice_is_attributed = device_is_attributed[device_is_attributed > 0].sort_values(ascending=False).reset_index(name=\"is_attributed_count\")\ndevice_is_attributed = device_is_attributed.iloc[:int(0.5*device_is_attributed.shape[0])]\ndevice_is_attributed.head()","2abd931b":"sns.set(font_scale=1.0)\nplt.figure(figsize=(18, 6))\nsns.barplot(x=\"device\", y=\"is_attributed_count\", data=device_is_attributed)","9aee854a":"channel_is_attributed = train_df.groupby([\"channel\"]).is_attributed.sum()\nchannel_is_attributed = channel_is_attributed[channel_is_attributed > 0].sort_values(ascending=False).reset_index(name=\"is_attributed_count\")\nchannel_is_attributed = channel_is_attributed.iloc[:int(0.5*channel_is_attributed.shape[0])]\nchannel_is_attributed.head()","690f73f9":"sns.set(font_scale=1.0)\nplt.figure(figsize=(18, 6))\nsns.barplot(x=\"channel\", y=\"is_attributed_count\", data=channel_is_attributed)","9afc6bcf":"try:\n    \n    del channel_is_attributed\n    del device_is_attributed\n    del os_is_attributed\n    del app_is_attributed\n    del ip_is_attributed\n    \nfinally:\n    \n    gc.collect()","875c0622":"train_df.groupby(\"click_day_of_week\").is_attributed.size().plot()\nplt.ylabel(\"Click Counts\")\nplt.xlabel(\"Day of week\")\nplt.xticks(ticks=[0, 1, 2, 3], labels=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\"])\n_ = plt.title(\"Clicks per Weekday\", {\"fontsize\": 15})","d485326c":"train_df.groupby(\"click_day_of_week\").is_attributed.sum().plot()\nplt.ylabel(\"Download Counts\")\nplt.xlabel(\"Day of week\")\nplt.xticks(ticks=[0, 1, 2, 3], labels=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\"])\n_ = plt.title(\"Downloads per Weekday\", {\"fontsize\": 15})","71abca01":"train_df.groupby(\"click_day_of_week\").is_attributed.mean().plot()\nplt.ylabel(\"Download Ratio\")\nplt.xlabel(\"Day of week\")\nplt.xticks(ticks=[0, 1, 2, 3], labels=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\"])\n_ = plt.title(\"Download Ratio per Weekday\", {\"fontsize\": 15})","02c8765b":"plt.figure(figsize=(10, 6))\nclick_hour_attributed = train_df.groupby(\"click_hour\").is_attributed.size()\nclick_hour_attributed[24] = click_hour_attributed[0]\nclick_hour_attributed =  click_hour_attributed.drop(0)\nclick_hour_attributed.plot()\nplt.ylabel(\"Click Counts\")\nplt.xlabel(\"Hour\")\nplt.xticks(ticks=range(1, 25), labels=range(1, 25))\n_ = plt.title(\"Clicks per Hour\", {\"fontsize\": 15})","c56c2c49":"plt.figure(figsize=(10, 6))\nclick_hour_attributed = train_df.groupby(\"click_hour\").is_attributed.sum()\nclick_hour_attributed[24] = click_hour_attributed[0]\nclick_hour_attributed =  click_hour_attributed.drop(0)\nclick_hour_attributed.plot()\nplt.ylabel(\"Download Counts\")\nplt.xlabel(\"Hour\")\nplt.xticks(ticks=range(1, 25), labels=range(1, 25))\n_ = plt.title(\"Downloads per Hour\", {\"fontsize\": 15})","5b2fa733":"plt.figure(figsize=(10, 6))\nclick_hour_attributed = train_df.groupby(\"click_hour\").is_attributed.mean()\nclick_hour_attributed[24] = click_hour_attributed[0]\nclick_hour_attributed =  click_hour_attributed.drop(0)\nclick_hour_attributed.plot()\nplt.ylabel(\"Download Ratio\")\nplt.xlabel(\"Hour\")\nplt.xticks(ticks=range(1, 25), labels=range(1, 25))\n_ = plt.title(\"Downloads Ratio per Hour\", {\"fontsize\": 15})","0874e513":"day_week_hour_count = train_df.groupby([\"click_day_of_week\", \"click_hour\"]).is_attributed.count().reset_index(name=\"click_count\")\nday_week_hour_count[\"index\"] = day_week_hour_count[\"click_day_of_week\"].astype(str) + \"_\" + day_week_hour_count[\"click_hour\"].astype(str)\nday_week_hour_count.head()","c924538c":"plt.figure(figsize=(50, 20))\nsns.lineplot(x=\"index\", y=\"click_count\", data=day_week_hour_count.loc[:, \"click_count\":\"index\"])\nplt.xticks(ticks=range(len(day_week_hour_count[\"index\"])), labels=day_week_hour_count[\"index\"])\nplt.tick_params(labelsize=25)\nplt.title(\"Clicks per Day of Week per Hour\")\nsns.set(font_scale=3.0)","40ea4fb4":"plt.figure(figsize=(50, 20))\nsns.lineplot(x=\"index\", y=\"click_count\", data=day_week_hour_count.loc[:30, \"click_count\":\"index\"])\nplt.xticks(ticks=range(len(day_week_hour_count.loc[:30, \"index\"])), labels=day_week_hour_count.loc[:30, \"index\"])\nplt.tick_params(labelsize=25)\nplt.title(\"Downloaded per Day of Week per Hour [:30]\")\nsns.set(font_scale=3.0)","5d66ec27":"plt.figure(figsize=(50, 20))\nsns.lineplot(x=\"index\", y=\"click_count\", data=day_week_hour_count.loc[28:, \"click_count\":\"index\"])\nplt.xticks(ticks=range(len(day_week_hour_count.loc[28:, \"index\"])), labels=day_week_hour_count.loc[28:, \"index\"])\nplt.tick_params(labelsize=22)\nplt.title(\"Downloaded per Day of Week per Hour [28:]\")\nsns.set(font_scale=3.0)","add6d288":"day_week_hour_ratio = train_df.groupby([\"click_day_of_week\", \"click_hour\"]).is_attributed.mean().reset_index(name=\"is_attributed_ratio\")\nday_week_hour_ratio[\"index\"] = day_week_hour_ratio[\"click_day_of_week\"].astype(str) + \"_\" + day_week_hour_ratio[\"click_hour\"].astype(str)\nplt.figure(figsize=(50, 20))\nsns.lineplot(x=\"index\", y=\"is_attributed_ratio\", data=day_week_hour_ratio.loc[:, \"is_attributed_ratio\":\"index\"])\nplt.xticks(ticks=range(len(day_week_hour_ratio[\"index\"])), labels=day_week_hour_ratio[\"index\"])\nplt.tick_params(labelsize=25)\nplt.title(\"Downloaded Ratio per Day of Week per Hour\")\nsns.set(font_scale=1.5)","2d10bcdb":"train_df.head()","051e8c6d":"new_features = [\n    {\"op\": \"mode\", \"groupby\": [\"ip\"], \"select\": \"os\", \"agg\": lambda x: x.mode() if x.mode() is int else x.mode().max()},\n    {\"op\": \"mode\", \"groupby\": [\"ip\"], \"select\": \"channel\", \"agg\": lambda x: x.mode() if x.mode() is int else x.mode().max()},\n    {\"op\": \"mode\", \"groupby\": [\"ip\"], \"select\": \"device\", \"agg\": lambda x: x.mode() if x.mode() is int else x.mode().max()},\n    \n    {\"op\": \"count\", \"groupby\": [\"ip\"], \"select\": \"app\", \"agg\": lambda x: x.count()},\n    \n    {\"op\": \"mode\", \"groupby\": [\"ip\", \"app\", \"device\"], \"select\": \"os\", \"agg\": lambda x: x.mode() if x.mode() is int else x.mode().max()},\n    {\"op\": \"mode\", \"groupby\": [\"ip\", \"app\", \"device\"], \"select\": \"channel\", \"agg\": lambda x: x.mode() if x.mode() is int else x.mode().max()},\n    {\"op\": \"mode\", \"groupby\": [\"ip\", \"app\", \"device\", \"os\"], \"select\": \"channel\", \"agg\": lambda x: x.mode() if x.mode() is int else x.mode().max()}\n]\nfor new_feature in new_features:\n    new_feature_name = str(new_feature[\"op\"]) + \"_\" + str(new_feature[\"select\"]) + \"_per_\" + '_'.join(new_feature[\"groupby\"])\n    new_feature_df = train_df.groupby(new_feature[\"groupby\"])[new_feature[\"select\"]].agg(new_feature[\"agg\"]).reset_index(name=new_feature_name)\n    train_df = pd.merge(train_df, new_feature_df, how=\"inner\", on=new_feature[\"groupby\"])\n                                                          ","99e19213":"train_df.head()","64df5ab7":"test_df = pd.read_csv(\"..\/input\/test.csv\")","c43d2d4b":"ip_occur_train = len(test_df.ip.value_counts()[train_df.ip.unique()].index)*100\/len(test_df.ip.value_counts().index)\nprint(round(ip_occur_train, 2), \"% of the test set IPs have appeared in the train set\", round(100 - ip_occur_train, 2), \"% are new occurences.\")","cab397f8":"train_df = train_df.drop(\"click_day_of_week\", axis=1)","def65a46":"def perc_of_train_fea_cat_in_test_data(test_df, features):\n    for feature in features:\n        test_df_unique = test_df[feature].unique()\n        train_df_unique = train_df[feature].unique()\n        train_df_unique_len = len(train_df_unique)\n        count = 0\n        for unique_feature in test_df_unique:\n            if unique_feature in train_df_unique:\n                count += 1\n        perc = round(count \/ train_df_unique_len * 100, 2)\n        print(perc, \"% of feature named: \" + feature.upper() + \"'s categories of the training set have appeared in the test set.\")\n        \ndef perc_of_new_fea_in_test(test_df, features):\n    for feature in features:\n        test_df_unique = test_df[feature].unique()\n        test_df_unique_len = len(test_df_unique)\n        train_df_unique = train_df[feature].unique()\n        count = 0\n        for unique_feature in test_df_unique:\n            if unique_feature not in train_df_unique:\n                count += 1\n        perc = round((count \/ test_df_unique_len) * 100, 2)\n        print(perc, \"% of categories in feature: \" + feature.upper() + \" are new occurences in the test set.\")","4dff4cf9":"perc_of_train_fea_cat_in_test_data(test_df, test_df.columns[(test_df.columns!=\"click_id\") & (test_df.columns!=\"click_time\")])","27498e1e":"perc_of_new_fea_in_test(test_df, test_df.columns[(test_df.columns!=\"click_id\") & (test_df.columns!=\"click_time\")])","48cc4562":"# train_df[\"attributed_time\"] = pd.to_datetime(train_df[\"attributed_time\"])\n# train_df.attributed_time[~train_df.attributed_time.isnull()]","d8a94cb8":"# train_df[\"attributed_time_day\"] = train_df[\"attributed_time\"].dt.day\n# train_df[\"attributed_time_hour\"] = train_df[\"attributed_time\"].dt.hour\n# train_df[\"attributed_time_weekday\"] = train_df[\"attributed_time\"].dt.dayofweek\n# train_df = train_df.drop(\"attributed_time\", axis=1)\n# train_df.head()","10b158cb":"train_df.is_attributed.mean()","e5a0875d":"train_df = train_df.drop(\"ip\", axis=1)","91d91ae6":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer","a549e289":"KFold = StratifiedKFold(n_splits=int(train_df.shape[0]\/10000), shuffle=True)","e904102b":"scale_pos_weight = round(train_df.is_attributed.value_counts()[0]\/train_df.is_attributed.value_counts()[1], 2)\nscale_pos_weight","863459d8":"param_grid = {\"max_depth\": [2, 4, 5, 10],\n             \"learning_rate\": [0.0001, 0.001, 0.01],\n             \"n_estimators\": [10, 100, 200],\n             }","85b4bb83":"bst = xgb.XGBModel(objective=\"binary:logistic\", booster=\"dart\",\n                  scale_pos_weight=scale_pos_weight, n_jobs=-1)","00217d87":"grid_search = GridSearchCV(estimator=bst,\n                        param_grid=param_grid,\n                        scoring=make_scorer(roc_auc_score),\n                        cv=KFold,\n                        verbose=1,\n                        return_train_score=True)","7d7467d4":"grid_search.fit(X=train_df.drop(\"is_attributed\", axis=1), y=train_df[\"is_attributed\"])","7ec4f400":"xgb_df = pd.DataFrame(grid_search.cv_results_)\nxgb_df","143a2fb5":"print(\"The best auc score is:\", grid_search.best_score_)\nprint(\"The best params are:\", grid_search.best_params_)","7d2b0ee8":"plt.plot(list(range(1, 37)), xgb_df[\"mean_train_score\"], label=\"Train Score\")\nplt.plot(list(range(1, 37)), xgb_df[\"mean_test_score\"], label=\"Test Score\")\nplt.grid()\nplt.xlabel(\"Param Index\")\nplt.ylabel(\"AUC Score\")\nplt.show()","5979af5d":"param_grid = {\"max_depth\": [2, 4, 5, 10],\n             \"learning_rate\": [0.0001, 0.001, 0.01],\n             \"n_estimators\": [10, 100, 200],\n             }","7ee9bad0":"lg = lgbm.LGBMClassifier(objective=\"binary\", scale_pos_weight=scale_pos_weight, n_jobs=-1)","cb806e23":"grid_search_lg = GridSearchCV(estimator=lg,\n                        param_grid=param_grid,\n                        scoring=make_scorer(roc_auc_score),\n                        cv=KFold,\n                        verbose=1,\n                        return_train_score=True)","bc26f99b":"grid_search_lg.fit(X=train_df.drop(\"is_attributed\", axis=1), y=train_df[\"is_attributed\"])","bd4112e9":"lg_df = pd.DataFrame(grid_search_lg.cv_results_)\nlg_df","8264cc40":"print(\"The best auc score is:\", grid_search_lg.best_score_)\nprint(\"The best params are:\", grid_search_lg.best_params_)","b1163304":"plt.plot(list(range(1, 37)), lg_df[\"mean_train_score\"], label=\"Train Score\")\nplt.plot(list(range(1, 37)), lg_df[\"mean_test_score\"], label=\"Test Score\")\nplt.grid()\nplt.xlabel(\"Param Index\")\nplt.ylabel(\"AUC Score\")\nplt.show()","6940e399":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df.drop(\"is_attributed\", axis=1), train_df[\"is_attributed\"], test_size=.3, shuffle=True)","83c4d48f":"print(\"Ratio of 'is_attributed (1)' in train data:\", y_train.mean())\nprint(\"Ratio of 'is_attributed (1)' in test data:\", y_test.mean())","8c92114d":"xgb_final = xgb.XGBClassifier(objective=\"binary:logistic\", booster=\"dart\",\n                  scale_pos_weight=scale_pos_weight, n_jobs=-1, **grid_search.best_params_)","f506dc9d":"xgb_final.fit(X_train, y_train)","28d8b3d8":"xgb_predict = xgb_final.predict(X_test)\nxgb_proba = xgb_final.predict_proba(X_test)","69f673b2":"xgb_predict","549064f9":"xgb_proba","e09dcf76":"roc_auc_score(y_test, xgb_proba[:, 1])","e1c660fe":"count = 0\nfor i, j in zip(y_test.values, xgb_predict):\n    if i == j:\n        count += 1\nprint(\"Accuracy:\", count\/xgb_predict.shape[0])","32197149":"fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, xgb_proba[:, 1])","e9290261":"plt.plot(fpr_xgb, tpr_xgb)\nplt.grid()\nplt.title(\"XGBoost ROC Curve\")\nplt.xlabel(\"False Positive Ratio (FPR)\")\nplt.ylabel(\"True Positive Ratio (TPR)\")\nplt.show()","d5a0dd74":"ROC_df_xgb = pd.DataFrame(data=np.concatenate([thresholds_xgb.reshape(-1, 1), tpr_xgb.reshape(-1, 1), fpr_xgb.reshape(-1, 1)], axis=1), columns=[\"Threshold\", \"True Positive Ratio (TPR)\", \"False Positive Ratio (FPR)\"])\nROC_df_xgb.head()","6f4604aa":"lg_final = lgbm.LGBMClassifier(objective=\"binary\", scale_pos_weight=scale_pos_weight, n_jobs=-1, **grid_search_lg.best_params_)","7a0d3224":"lg_final.fit(X_train, y_train)","03a8e274":"lg_predict = lg_final.predict(X_test)\nlg_proba = lg_final.predict_proba(X_test)","0b96b235":"lg_predict","4c559112":"lg_proba","a9ca975a":"roc_auc_score(y_test, lg_proba[:, 1])","cdfd3f37":"count = 0\nfor i, j in zip(y_test.values, lg_predict):\n    if i == j:\n        count += 1\nprint(\"Accuracy:\", count\/lg_predict.shape[0])","23df985a":"fpr_lg, tpr_lg, thresholds_lg = roc_curve(y_test, xgb_proba[:, 1])","97a59581":"plt.plot(fpr_lg, tpr_lg)\nplt.grid()\nplt.title(\"LightGBM ROC Curve\")\nplt.xlabel(\"False Positive Ratio (FPR)\")\nplt.ylabel(\"True Positive Ratio (TPR)\")\nplt.show()","9a44567c":"thresholds_lg","cb11200e":"ROC_df_lg = pd.DataFrame(data=np.concatenate([thresholds_lg.reshape(-1, 1), tpr_lg.reshape(-1, 1), fpr_lg.reshape(-1, 1)], axis=1), columns=[\"Threshold\", \"True Positive Ratio (TPR)\", \"False Positive Ratio (FPR)\"])\nROC_df_lg","261a0a52":"try:\n    \n    del train_df\n    del X_train\n    del X_test\n    del y_train\n    del y_test\n    del test_df # Delete it because of RAM shortage when working on the whole data; It's gonna be loaded later again after the final model is trained and tested on the whole data.\n    del ROC_df_xgb\n    del ROC_df_lg\n    \nexcept:\n    pass\n    \nfinally:\n    _ = gc.collect()","e762adb2":"train_whole =  load_data(which=\"whole\")\ntrain_whole = train_whole.drop(\"attributed_time\", axis=1)","9f017a52":"train_whole.head()","669b00e2":"train_whole.info()","789862f0":"int_columns = [\"ip\", \"app\", \"device\", \"os\", \"channel\", \"is_attributed\"]\ntrain_whole[int_columns] = train_whole[int_columns].apply(pd.to_numeric, downcast=\"unsigned\")\ntrain_whole.info()","33e46e1b":"gc.collect()","0c8b00f9":"import sys\nvar, obj = None, None\ntotal_size = 0\nfor var, obj in locals().items():\n    print(str(var) + \" : \" + str(sys.getsizeof(obj)))\n    total_size += sys.getsizeof(obj)\nprint(\"Total memory usage:\", total_size)\ndel total_size","4d3e0d0e":"try:\n    del train_df_int\n    del variable_value_counts\n    del variable_value_quantile\n    del shortened_data\n    del ip_counts\n    del suspicious_ips\n    del suspicious_ips_shortened\n    del suspicious_df\n    del click_hour_attributed\n    del day_week_hour_count\n    del day_week_hour_ratio\n    del new_feature_df\n    del xgb_df\n    del lg_df\n    del xgb_predict\n    del lg_predict\n    del StratifiedKFold    \nfinally:\n    _ = gc.collect()","ba46d6cc":"import sys\nvar, obj = None, None\ntotal_size = 0\nfor var, obj in locals().items():\n    print(str(var) + \" : \" + str(sys.getsizeof(obj)))\n    total_size += sys.getsizeof(obj)\nprint(\"Total memory usage:\", total_size)\ndel total_size","f2e70dad":"train_whole[\"click_month\"] = pd.to_datetime(train_whole[\"click_time\"]).dt.month\ntrain_whole[\"click_day_of_week\"] = pd.to_datetime(train_whole[\"click_time\"]).dt.dayofweek\ntrain_whole[\"click_hour\"] = pd.to_datetime(train_whole[\"click_time\"]).dt.hour\ntrain_whole[\"click_year\"] = pd.to_datetime(train_whole[\"click_time\"]).dt.year\ntrain_whole = train_whole.drop(\"click_time\", axis=1)","c404340f":"train_whole.info()","968bebd1":"train_whole = train_whole.drop([\"click_year\", \"click_month\"], axis=1)","942b8958":"int_columns = [\"click_day_of_week\", \"click_hour\"]\ntrain_whole[int_columns] = train_whole[int_columns].apply(pd.to_numeric, downcast=\"unsigned\")\ntrain_whole.info()","7b6f2f90":"for new_feature in new_features:\n    new_feature_name = str(new_feature[\"op\"]) + \"_\" + str(new_feature[\"select\"]) + \"_per_\" + '_'.join(new_feature[\"groupby\"])\n    new_feature_df = train_whole.groupby(new_feature[\"groupby\"])[new_feature[\"select\"]].agg(new_feature[\"agg\"]).reset_index(name=new_feature_name)\n    train_whole = pd.merge(train_whole, new_feature_df, how=\"inner\", on=new_feature[\"groupby\"])","f98b54d8":"train_whole.info()","24a3a3da":"# float_columns = [\"attributed_time_day\", \"attributed_time_hour\", \"attributed_time_weekday\"]\n# train_whole[float_columns] = train_whole[float_columns].apply(pd.to_numeric, downcast=\"float\")\nint_columns = [\"count_app_per_ip\"]\ntrain_whole[int_columns] = train_whole[int_columns].apply(pd.to_numeric, downcast=\"unsigned\")\ntrain_whole.info()","f1604e3c":"train_whole = train_whole.drop(\"ip\", axis=1)","181e0c01":"vars_ = dir()\nvar_list = []\nfor var in vars_:\n    if not var.startswith(\"_\"):\n        var_list.append(var)\n        \nvar_list","76fa7be0":"var, obj = None, None\ntotal_size = 0\nfor var, obj in locals().items():\n    print(str(var) + \" : \" + str(sys.getsizeof(obj)) + \" Bytes\")\n    total_size += sys.getsizeof(obj)\nprint(\"Total memory usage:\", total_size\/1000000000, \"GB\")\ndel total_size","5c5bf423":"try:\n    del new_feature_df\nfinally:\n    _ = gc.collect()","ffa5ba76":"var, obj = None, None\ntotal_size = 0\nfor var, obj in locals().items():\n    print(str(var) + \" : \" + str(sys.getsizeof(obj)) + \" Bytes\")\n    total_size += sys.getsizeof(obj)\nprint(\"Total memory usage:\", total_size\/1000000000, \"GB\")\ndel total_size","1c0dd2f0":"# Not enough memory, maybe some more can be freed in order to perform this operation and test the accuracy of the model trained on almost all the data\n# X_train, X_test, y_train, y_test = train_test_split(train_whole.drop(\"is_attributed\", axis=1), train_whole[\"is_attributed\"], test_size=.3, shuffle=True)\n# try:\n#     del train_whole\n# except:\n#     pass","c42d6f13":"# print(\"Ratio of 'is_attributed' in y_train:\", y_train.mean())\n# print(\"Ratio of 'is_attributed' in y_test:\", y_test.mean())","c5690d72":"# lg_final = lgbm.LGBMClassifier(objective=\"binary\", is_unbalance=True, n_jobs=-1, **grid_search_lg.best_params_).fit(X_train, y_train)\nlg_final = lgbm.LGBMClassifier(objective=\"binary\", scale_pos_weight=scale_pos_weight, n_jobs=-1, **grid_search_lg.best_params_).fit(train_whole.drop(\"is_attributed\", axis=1), train_whole[\"is_attributed\"])","b323daa7":"try:\n    del train_whole\n#     del X_train\n#     del X_test\n#     del y_train\n#     del y_test\nfinally:\n    _ = gc.collect()","c19b88bc":"test_df = pd.read_csv(\"..\/input\/test.csv\")\ntest_df.head()","25cc3523":"test_df.head()","f70e7cef":"test_df.info()","da1a195d":"test_df[\"click_month\"] = pd.to_datetime(test_df[\"click_time\"]).dt.month\ntest_df[\"click_day_of_week\"] = pd.to_datetime(test_df[\"click_time\"]).dt.dayofweek\ntest_df[\"click_hour\"] = pd.to_datetime(test_df[\"click_time\"]).dt.hour\ntest_df[\"click_year\"] = pd.to_datetime(test_df[\"click_time\"]).dt.year\ntest_df = test_df.drop(\"click_time\", axis=1)\ntest_df.info()","34705bbc":"test_df = test_df.drop([\"click_year\", \"click_month\"], axis=1)","c8afe593":"test_df = test_df.astype(\"uint64\")\ntest_df.info()","786e0410":"test_df_int = test_df.select_dtypes(include=[\"uint64\"])\ntest_df_int = test_df_int.apply(pd.to_numeric, downcast=\"unsigned\")\ntest_df = test_df.drop(test_df.dtypes[test_df.dtypes==\"uint64\"].index, axis=1)\ntest_df = pd.concat([test_df, test_df_int], axis=1)\ntest_df.info()","371aa59a":"for new_feature in new_features:\n    new_feature_name = str(new_feature[\"op\"]) + \"_\" + str(new_feature[\"select\"]) + \"_per_\" + '_'.join(new_feature[\"groupby\"])\n    new_feature_df = test_df.groupby(new_feature[\"groupby\"])[new_feature[\"select\"]].agg(new_feature[\"agg\"]).reset_index(name=new_feature_name)\n    test_df = pd.merge(test_df, new_feature_df, how=\"inner\", on=new_feature[\"groupby\"])","54d8d4d9":"try:\n    del new_feature_df\n    del test_df_int\nfinally:\n    _ = gc.collect()","008cf8ee":"test_df.info()","5f69e604":"int_columns = [\"count_app_per_ip\"]\ntest_df[int_columns] = test_df[int_columns].apply(pd.to_numeric, downcast=\"unsigned\")\ntest_df.info()","64d6e395":"test_df.head()","b513d5c4":"click_id = test_df.click_id\nX_test = test_df.drop([\"click_id\", \"ip\"], axis=1)","b049e109":"lg_predict = lg_final.predict_proba(X_test)","3f05efe3":"lg_predict","f2b448e2":"try:\n    del test_df\n    del X_test\nexcept:\n    pass\nfinally:\n    _ = gc.collect()","e01a56f9":"results = pd.concat([click_id, pd.Series(lg_predict[:, 1], name=\"is_attributed\")], axis=1)\nresults.head()","7ba4e2d8":"results = results.sort_values(by=\"click_id\", axis=0).reset_index().drop(\"index\", axis=1)\nresults.head()","cef0e309":"results.to_csv(\"submission_file.csv\", sep=',', index=False)","ba6e0e63":"There are two most popular os in China, probably, iOS and Android.","5c476691":"### Create Datetime Features","d9b2f0f9":"## Prepare the Test Data","5a20cf72":"### Checking for missing values (NaN)","14530edd":"Even that the clicks are well distributed between the channel, there is one that's exceptionally large, the channel #280.","098898f6":"The sparkle during the downtrend click cycle always happens at noon.","5c8e068c":"According to the graph, the download counts increases rapidly from Monday to Tuesday, decreasing its increase ratio, but maintaining a up trend until Wednesday, when it starts to decrease until Thurday.","2a49f6a4":"#### lightGBM","b2f638f7":"**ZOOM (30 first indexes)** ","51980603":"The **Download Ratio** follows the same patterns of **Download Counts**.","d4f909da":"The XGBoost model achieved a `roc_auc_score` of approximately **90.7%** and Accuracy of **94.5%** (at threshold = **50%**).","204b061f":"The **Click Counts** decreases rapidly between 14:00 and 20:00 (where it reaches its minimum), after that it starts to increase even faster than it declined.","02940649":"## Univariate Analysis","d1f4a3a2":"Interestingly, the **Downloads per Hour** count has a much more no-uniform graph, with random spikes, compared to the **Clicks per Hour** graph that's much more uniform and **organized**. This random spikes reflects the interest of the user in downloading apps in certain part of the day (marketing and quality of certain apps, for instance, sparkled the interest). Where the **Clicks per Hour** graph reflects an organized approach to distribute clicks in order to make profits.","30bef9a3":"# TalkingData AdTracking Fraud Detection Challenge","81b55ce6":"## Model Training (Sample data)","bfee3ea8":"## Feature Engineering","eab00c72":"**ZOOM (From index 28 to final)** ","3ffaaeac":"* ## Final Model (Whole - lightGBM)","62cdf6c5":"### Final Model (Sample Data)","294cf986":"The overall amount of clicks increases rapidly between Monday and Tuesday, losing strength between Tuesday and Wednesday, when it starts to decline in a ratio smaller than it grew in the first period. Probably, the downtrend will continue, decreasing slowly until the weekend.","3177fe7c":"As expected, almost **ALL** those suspicious IPs' clicks are **fraudulent**. The percentage that aren't **click fraud** can be considered as noise, because of its significance.","d72bcf09":"In this graph, some nice patterns can be spotted, depending on the week and on the hour of the day.","9235601e":"## Loading whole dataset","8d9c7e22":"Despite that the value counts were restrained to the 80 % larger numbers. The number of devices that were not #1 or #2 was insignificant. Doing a quick search, the most used mobile device in 2017 was Oppo, that's problably the one indicated by #1.","3b2f8b34":"As can be observed, the percentage of clicks that resulted in downloads were almost null. It really indicates a serious problem not only to the advertisers but for the model construction. It's hugely likely that the model becomes enbiased to class #0. \n\nDespite that the objective of this problem is to deliver the best metric score, maybe a class balancing technique will be applied further in order to make the predictions more realistic (or not).","6c110a05":"## Import libraries and load data","db6865ad":"### Data Processing","ec7db217":"No relationship can be detected from this graph, it just looks like random noise\/peaks. Reflecting the little correlation between the time series and user interest to download apps.","f18dfca3":"The #3 app was the most clicked and 80% of the clicks are distributed between the apps in the X-axis.","78099ad6":"## Predictions for the Test Data","cd7c47ee":"The reflection between the coordinated **Clicks per Hour** graph and the noisy **Downloads per Hour**.","7d5d104f":"## Bivariate Analysis","d442c81b":"#### xgBoost","83fba33a":"Unlike the previous categorical variables, \"channel\" counts is well distributed. This means that the clicks are well distributed between the channel ids of mobile ad publishers.","3928a351":"Most of **attributed_time** values are missing, moreover, **attributed_time** is not present in the test data.","941224d5":"Notice that the values presented above are not valid for the sample took from the training data. After the model is tested on the training sample data, the statistics above should be repeated for the whole training set.","0fd28e8f":"The LightGBM model achieved a `roc_auc_score` of approximately **92.9%** and Accuracy of **95.3%** (at threshold = **50%**).","efc707c0":"For the reason above, features built on top of the IP feature and the target variable would not be an effective predictive feature. Dropping `click_day_of_week` column in train dataframe.","289b6785":"The \"clicks per ip\" above was limited to values above 50 clicks. This IPs are at least suspicious, for example, the IP 5348 clicked 669 times in ads, it can be considered an evidence that they are using some kind of automated process to perform the clicks (like bots), or its a group.\n\nIt's highly inlikely that an individual did that alone. \nA more profound analysis will be performed in the **bivariate analysis** section, highlighting what **channels**, **apps**, **OSes** and **devices** they are using. After that, a percentage of **clicked_and_downloaded** will be calculated.\n\n**OBS**: Considering the highly **unbalanced data** it's certain that they are entirely **click fraud** cases. "}}