{"cell_type":{"e1682e5f":"code","0d9f0e92":"code","2ac2e148":"code","c6144db7":"code","8cc8e7f2":"code","071e8039":"code","54634c5b":"code","98dfa1cf":"code","77272e47":"code","11e231c2":"code","2b48b008":"code","9fcd11c9":"code","e133622d":"code","d748189d":"code","6d4f70bb":"code","1541b31f":"code","c4ebdd40":"code","754c0d16":"code","7502e0bc":"code","f85b5bcb":"code","4ca9da33":"code","3b38bb6d":"code","2acc1a24":"code","73afb8fa":"code","c64997ff":"code","c8e24420":"code","7e00ffbf":"code","8a175062":"code","65d7562b":"code","fb338d0d":"code","06381eaf":"code","e402f679":"code","d547bb50":"code","44cd9f67":"code","1c30dca8":"code","894d33c6":"code","2f17df2d":"code","73c02ac7":"code","224b6300":"code","84411a5a":"code","b12b2a21":"code","d22c29a8":"code","fe87c72f":"code","0b3e63eb":"code","d09e17fb":"code","9f175031":"code","7ef1302a":"code","043432b8":"code","d435194e":"code","c8bbef09":"code","1aa354db":"code","2c754f4f":"code","8c29d4f2":"code","6b7bb2b9":"code","3199c4ea":"code","e3f37c02":"code","79ca9c16":"code","f44d6bf1":"code","4cb87638":"code","8309a6c2":"code","d9d376fa":"code","29ec966f":"code","1aaed46d":"code","b060582e":"code","14803b45":"code","2ff15d7f":"code","e5bf1d49":"code","5042e990":"markdown","0853b823":"markdown","5336d8a6":"markdown","14c2292b":"markdown","9a4901ef":"markdown","17ae340a":"markdown","850a6886":"markdown","6b4a4a81":"markdown","0b83d798":"markdown","6c258f99":"markdown","f9b6103f":"markdown","500c7e04":"markdown","564e8ef8":"markdown","08752d19":"markdown","f4f05094":"markdown","b74ab473":"markdown","a0cf4f3f":"markdown","98dfa898":"markdown","7499dcba":"markdown","752e0f15":"markdown","6213e16c":"markdown","1cbc8a04":"markdown","e3672831":"markdown","c8c0f22e":"markdown","ed6ab435":"markdown","4881f954":"markdown","bb85efc7":"markdown","66d7d4c7":"markdown","dc9efb82":"markdown"},"source":{"e1682e5f":"# Import the required Libraries \n\n# Data Handling\nimport pandas as pd\nimport numpy as np\nimport warnings\n\n# Visualization\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data pre-processing - Scaling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Feature Dimension Engineering\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport statsmodels.api as sm\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\n# Model Tuning \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Calculate Metrics\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer,precision_recall_curve \nfrom sklearn import metrics\nfrom scipy.stats import norm\n\n# Set the session options\n%matplotlib inline\npd.options.display.max_columns = None\npd.options.display.max_rows = None\nwarnings.filterwarnings(\"ignore\")\npd.options.display.float_format = '{:,.2f}'.format","0d9f0e92":"# Read train data into Dataframe\ntrain_data=pd.read_csv('..\/input\/train.csv')\n# Read test data into Dataframe\ntest_data=pd.read_csv('..\/input\/test.csv')\n# Check the dimensions\nprint('Train Size: ',train_data.shape)\nprint('Test Size: ',test_data.shape)","2ac2e148":"# Get the feel of train data\ntrain_data.head()","c6144db7":"# Get the feel of test data\ntest_data.head()","8cc8e7f2":"# Add a feature called 'flag' to train and test data. This feature is used to distinguish train from test data.\n# Also, add 'Survived' feature to test and initialise that to a dummy variable ..say '9' in this case.\ntest_data['flag']='test'\ntrain_data['flag']='train'\ntest_data['Survived']='9'\n\n# Vertically stack the train and test data and check shape\ndata=train_data.append(test_data)\nprint('Stacked data size: ',data.shape)\n\n# Get the feel of the merged data\ndata.head()","071e8039":"# Age - Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.Age.dtype)\nprint('Number of Unique values: ',data.Age.nunique())\nprint('Number of NaNs : ', data.Age.isnull().sum(axis=0))","54634c5b":"# Age - Calculate the mean and median for various Segments. \n#How the mean \/ median varies with in train, test data and with in Survived and Not survived data points. \n\nprint(data[data['flag']=='train'].groupby(['Survived'])['Age'].mean())\nprint(data[data['flag']=='train'].groupby(['Survived'])['Age'].median())\n\nprint('Mean Age of full data  ', data.Age.mean())\nprint('Median Age of full data  ', data.Age.median())\n\nprint('Mean Age of full test data  ', data[data['flag']=='test'].Age.mean())\nprint('Median Age of full test data  ', data[data['flag']=='test'].Age.median())\n\nprint('Mean Age of full train data  ', data[data['flag']=='train'].Age.mean())\nprint('Median Age of full train data  ', data[data['flag']=='train'].Age.median())","98dfa1cf":"# Impute the NaNs with median \ndata.Age=data.Age.fillna(data.Age.median())\n\nprint('Mean Age of full data  ', data.Age.mean())\nprint('Median Age of full data  ', data.Age.median())\n\n# Print the dist plots to see how the feature is distributed\nplt.subplot(1,2,1)\nplt.title('Train')\nsns.distplot(data[data['flag']=='train']['Age'])\nplt.subplot(1,2,2)\nplt.title('Test')\nsns.distplot(data[data['flag']=='test']['Age'])\nplt.show()","77272e47":"# Cabin - Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.Cabin.dtype)\nprint('Number of Unique values: ',data.Cabin.nunique())\nprint('Number of NaNs : ', data.Cabin.isnull().sum(axis=0))","11e231c2":"print('Unique values: ',list(data.Cabin.unique()))","2b48b008":"#data['Cabin']=data['Cabin'].fillna('XOC')\n#data['Cabin_code']=data['Cabin'].apply(lambda x: str(x[0]))\n#print('Number of Unique values: ',data.Cabin_code.nunique())\n#print('Unique values: ',data.Cabin_code.unique())\n#print('Number of NaNs : ', data.Cabin_code.isnull().sum(axis=0))\n\n# Convert cabin_code to dummies\n#CC_dummies = pd.get_dummies(data['Cabin_code'], drop_first=True)\n#data=pd.concat([CC_dummies,data],axis=1)","9fcd11c9":"# Embarked - Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.Embarked.dtype)\nprint('Number of Unique values: ',data.Embarked.nunique())\nprint('Number of NaNs : ', data.Embarked.isnull().sum(axis=0))\nprint('Number of Train NaNs : ', data[data['flag']=='train']['Embarked'].isnull().sum(axis=0))\nprint('Number of Test NaNs : ', data[data['flag']=='test']['Embarked'].isnull().sum(axis=0))","e133622d":"# See what are those Unique Values\nprint('Unique values: ',data.Embarked.unique())","d748189d":"#Check how the 'Embarked' code is distributed across the data \ndata.groupby(['Embarked','flag'])['Survived'].count()","6d4f70bb":"# Check how Embarked is distributed across the other features \ndata[data['flag']=='train'].groupby(['Embarked','Survived','Sex'])['PassengerId'].count()","1541b31f":"# Check the data that has 'Embarked' code as NaN\ndata[data['Embarked'].isnull()]","c4ebdd40":"# Impute 'S' for NaNs in Embarked code\ndata['Embarked']=data['Embarked'].fillna('S')\nprint('Number of NaNs : ', data.Embarked.isnull().sum(axis=0))\n\n# Convert Embarked to dummies\nEM_dummies = pd.get_dummies(data['Embarked'], drop_first=True)\ndata=pd.concat([EM_dummies,data],axis=1)","754c0d16":"# Fare - Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.Fare.dtype)\nprint('Number of Unique values: ',data.Fare.nunique())\nprint('Number of NaNs : ', data.Fare.isnull().sum(axis=0))","7502e0bc":"data[data['Fare'].isnull()]","f85b5bcb":"print(data[data['flag']=='train'].groupby(['Survived'])['Fare'].mean())\nprint(data[data['flag']=='train'].groupby(['Survived'])['Fare'].median())\nprint(data[data['flag']=='train'].groupby(['Survived'])['PassengerId'].count())\nprint(data[data['flag']=='train'].groupby(['Survived'])['PassengerId'].count())\nprint('Mean Fare of full data  ', data.Fare.mean())\nprint('Median Fare of full data  ', data.Fare.median())\nprint('Mean Fare of full test data  ', data[data['flag']=='test'].Fare.mean())\nprint('Mean Fare of full train data  ', data[data['flag']=='train'].Fare.mean())\nprint('Median Fare of full test data  ', data[data['flag']=='test'].Fare.median())\nprint('Median Fare of full train data  ', data[data['flag']=='train'].Fare.median())","4ca9da33":"# Impute the median value \ndata['Fare']=data['Fare'].fillna(data['Fare'].median())","3b38bb6d":"# Print the dist plots to see how the feature is distributed\nplt.subplot(1,2,1)\nplt.title('Train')\nsns.distplot(data[data['flag']=='train']['Fare'])\nplt.subplot(1,2,2)\nplt.title('Test')\nsns.distplot(data[data['flag']=='test']['Fare'])\nplt.show()","2acc1a24":"# Name - Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.Name.dtype)\nprint('Number of Unique values: ',data.Name.nunique())\nprint('Number of NaNs : ', data.Name.isnull().sum(axis=0))","73afb8fa":"#Parch Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.Parch.dtype)\nprint('Number of Unique values: ',data.Parch.nunique())\nprint('Number of NaNs : ', data.Parch.isnull().sum(axis=0))","c64997ff":"# See what are those Unique Values\nprint('Unique values: ',data.Parch.unique())","c8e24420":"# PassengerId - Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.PassengerId.dtype)\nprint('Number of Unique values: ',data.PassengerId.nunique())\nprint('Number of NaNs : ', data.PassengerId.isnull().sum(axis=0))","7e00ffbf":"# Pclass - Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.Pclass.dtype)\nprint('Number of Unique values: ',data.Pclass.nunique())\nprint('Number of NaNs : ', data.Pclass.isnull().sum(axis=0))","8a175062":"# See what are those Unique Values\nprint('Unique values: ',data.Pclass.unique())","65d7562b":"# Sex Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.Sex.dtype)\nprint('Number of Unique values: ',data.Sex.nunique())\nprint('Number of NaNs : ', data.Sex.isnull().sum(axis=0))","fb338d0d":"# See what are those Unique Values\nprint('Unique values: ',data.Sex.unique())","06381eaf":"# Convert Embarked to dummies\nSex_dummies = pd.get_dummies(data['Sex'], drop_first=True)\ndata=pd.concat([Sex_dummies,data],axis=1)","e402f679":"#SibSp Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.SibSp.dtype)\nprint('Number of Unique values: ',data.SibSp.nunique())\nprint('Number of NaNs : ', data.SibSp.isnull().sum(axis=0))","d547bb50":"# See what are those Unique Values\nprint('Unique values: ',data.SibSp.unique())","44cd9f67":"#Ticket  Find the data type, Unique values and Number of NaNs\nprint('Dtype: ',data.Ticket.dtype)\nprint('Number of Unique values: ',data.Ticket.nunique())\nprint('Number of NaNs : ', data.Ticket.isnull().sum(axis=0))","1c30dca8":"# Derive a new feature which is the sum of siling count and parent count \n#data['Mem_size']=data['SibSp']+data['Parch']","894d33c6":"#Delete unnecessary cols\ndata_cleaned=data.drop(columns=['Cabin','Embarked','Name','PassengerId','Sex','Ticket'])","2f17df2d":"print(data_cleaned.shape)\ndata_cleaned.head()","73c02ac7":"# Write a function to display subplots of box plot for given variables.\ndef box_plot(k,fx=16,fy=8):\n    fig=plt.figure(figsize=(fx, fy), dpi=70, facecolor='w', edgecolor='k')\n    sns.set(style=\"darkgrid\")\n    i=1\n    for col in k:\n        plt.subplot(2,3,i)\n        sns.boxplot(y=col,\n                    x='Survived',\n                palette='pastel',\n                data=data_cleaned[data_cleaned['flag']=='train'])\n        i=i+1\n\nplt.show()\n\n# Write a function to display subplots of bar plot for given variables.\ndef count_plot(k,fx=14,fy=10,df=data_cleaned):\n    fig=plt.figure(figsize=(fx, fy), dpi=90, facecolor='w', edgecolor='k')\n    sns.set(style=\"darkgrid\")\n    i=1\n    for col in k:\n        plt.subplot(4,2,i)\n        plt.xticks(rotation='horizontal')\n        ax=sns.countplot(x=col,\n            data=df[df['flag']=='train'],\n            palette='pastel',\n            hue='Survived',\n            order=data_cleaned[col].value_counts().index)  \n        i=i+1\n    \n    \nplt.show()","224b6300":"box_plot(['Age','Fare'])","84411a5a":"# See the percentiles of the Age\ndata_cleaned.Age.describe(percentiles=[0.25,0.5,0.75,0.95,0.99])","b12b2a21":"# Cap the Age at 99 percentile - this meant any Age more than 99 percentile will be capped \na=np.percentile(data_cleaned.Age,99.00)\ndata_cleaned.Age=data_cleaned['Age'].apply(lambda x: a if x > a else x)","d22c29a8":"box_plot(['Age'])","fe87c72f":"# Apply logrithmic transformation on Fare\ndata_cleaned['Fare_log']=data_cleaned['Fare'].apply(lambda x: np.log(x))","0b3e63eb":"# Check the Fare_log distribution\nbox_plot(['Fare_log'])","d09e17fb":"count_plot(['Pclass','male','Q','S'])","9f175031":"count_plot(['SibSp','Parch'])","7ef1302a":"fig=plt.figure(figsize=(18, 16), dpi=50, facecolor='w', edgecolor='k')\nsns.set(style=\"ticks\", palette=\"pastel\")\nax = sns.heatmap(data_cleaned.corr(),annot=True)","043432b8":"# Remove logrithmic transformations as we will do standardisation\ndata_cleaned.drop(columns=['Fare_log'],axis=1,inplace=True)\ndata_cleaned.head()","d435194e":"# prepare the x and y variables\nx_train = data_cleaned[data_cleaned['flag']=='train'].drop(columns=['Survived','flag'],axis=1)\ny_train = data_cleaned[data_cleaned['flag']=='train']['Survived']\nx_test = data_cleaned[data_cleaned['flag']=='test'].drop(columns=['Survived','flag'],axis=1)\n\nprint('shape of x - Full train data: ', x_train.shape)\nprint('shape of y - Full train data: ', y_train.shape)\nprint('shape of x - Full test data: ', x_test.shape)\n\n# Split the training set into training and validation set (80:20)\nx_train_set, x_val_set, y_train_set, y_val_set = train_test_split(x_train,y_train, train_size=0.8,test_size=0.2,random_state=100)\nprint(y_train.mean())\nprint(y_train.value_counts())\n\nprint('shape of x - Model train data: ', x_train_set.shape)\nprint('shape of x - Model Validate data: ', x_val_set.shape)\nprint('shape of y - Model train data: ', y_train_set.shape)\nprint('shape of y - Model Validate data: ', y_val_set.shape)\n","c8bbef09":"# Define a function for Standardizing the values\ndef scaler_f(a):\n    cols=list(a)\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(a)\n    scaled_frame=pd.DataFrame(scaled, columns=cols)\n    return scaler,scaled_frame\n\n\n# Define a function for PCA\ndef pca_show(a):\n    pca = PCA(svd_solver='randomized', random_state=42)\n    pca_frame=pca.fit(a)\n    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n    plt.xlabel('number of components')\n    plt.ylabel('cumulative explained variance')\n    plt.show()\n    \n# Incremental PCA\ndef pca_incr(a,n):\n    pca_mo = IncrementalPCA(n_components=n)\n    pca_frame = pca_mo.fit_transform(a)\n    return pca_mo,pca_frame ","1aa354db":"# Call the scaler function for the training set. Then, transform the validation set.\nscale,x_train_scaled=scaler_f(x_train_set)\nx_val_scaled=pd.DataFrame(data=scale.transform(x_val_set),columns=list(x_train_set))","2c754f4f":"# Perform PCA using the scaled set\npca_show(x_train_scaled)","8c29d4f2":"# Apply PCA to extract 7 components and transform the validation set\npca_final,x_train_pca=pca_incr(x_train_scaled,7)\nx_val_pca=pca_final.transform(x_val_scaled)","6b7bb2b9":"# Check the shapes\ny_train=y_train.astype('int')\ny_train_set=y_train_set.astype('int')\ny_val_set=y_val_set.astype('int')\nprint(x_train_set.shape)\nprint(x_val_set.shape)\nprint(x_train_pca.shape)\nprint(x_val_pca.shape)","3199c4ea":"# Create a frame and empty lists to store the predictor and actual values that help easy model visualisation\nscore_frame=pd.DataFrame()\nalgorithm_name=[]\nrecall_scores=[]\nf1_scores=[]\naccuracy_scores_test=[]\naccuracy_scores_train=[]\nplot_vars=[]\n\n# Define function to predict the test data set usinf given algorithm\ndef run_algorithm(algo,tr_fe,tr_lb,ts_fe,ts_lb,algo_name,roc_req):\n    # algo - model object\n    # tr_fe - Independent variables from test set\n    # tr_lb - Predictor variable from training set \n    # ts_fe - Independent variables from test set \n    # ts_lb - Predictor variable from test set\n    # algo_name - Algorithm Name \n    # roc_req - Calculations of roc, acu and coefficeints\n    \n    algo_model = algo.fit(tr_fe,tr_lb)\n\n    # predict values and probabilities\n    y_pred_test=algo_model.predict(ts_fe)\n    y_prob_test=algo_model.predict_proba(ts_fe)[:,1]\n    y_pred_train=algo_model.predict(tr_fe)\n    model_roc_auc = roc_auc_score(ts_lb,y_pred_test) \n        \n    # Set Values for Display \n    algorithm_name.append(algo_name)\n    recall_scores.append(metrics.recall_score(ts_lb,y_pred_test))\n    f1_scores.append(metrics.f1_score(ts_lb,y_pred_test))\n    accuracy_scores_test.append(accuracy_score(ts_lb,y_pred_test))\n    accuracy_scores_train.append(accuracy_score(tr_lb,y_pred_train))\n    \n    # Print Values\n    print (algo_name +' : ')\n    print('-------------------------')\n    print(\"Accuracy   Score on Test: \",accuracy_score(ts_lb,y_pred_test))\n    print(\"Accuracy  Score on Train: \",accuracy_score(tr_lb,y_pred_train))\n    print (\"Area under curve : \",model_roc_auc,\"\\n\")\n    #print(\"Classification report : \",\"\\n\", classification_report(y_test_set,y_pred_test))\n    #print(\"Confusion Matrix: \",\"\\n\",metrics.confusion_matrix(y_test_set,y_prob_test.round()))\n    #print(\"Recall Score on train Set: \",metrics.recall_score(y_train_set,y_pred_train))\n    #print(\"Recall Score on test Set: \",metrics.recall_score(y_test,y_pred))\n    \n    # Set up frames for model scores for later use\n    score_frame[algo_name+'_Test']=ts_lb\n    score_frame[algo_name+'_Prob']=y_prob_test\n    plot_vars.append((algo_name+'_Test',algo_name+'_Prob',algo_name))\n       \n    # Plot the ROC curve \n    if roc_req == 'y':\n        fpr,tpr,threshold = roc_curve(ts_lb,y_prob_test)\n        plt.figure(figsize=(5, 5))\n        plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % model_roc_auc)\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver operating characteristic example')\n        plt.legend(loc=\"lower right\")\n        plt.show()\n    \n    \ndef model_engine(x_tr,y_tr,x_ts,y_ts,roc_curve='n'):\n    # Logistic Regression tunes using Grid Serch\n    logreg = LogisticRegression(random_state=42)\n    param = {'C':[0.001,0.003,0.005,0.01,0.03,0.05,0.1,0.3,0.5,1,2,3,3,4,5,10,20,22,25,27,30]}\n    clf = GridSearchCV(logreg,param,scoring='accuracy',refit=True,cv=10)\n    clf.fit(x_tr,y_tr)\n    print('Best Accuracy: {:.4}, with best C: {}'.format(clf.best_score_, clf.best_params_)+'\\n')\n    logreg_cv = LogisticRegression(random_state=42, C=clf.best_params_['C'])\n    run_algorithm(logreg_cv,x_tr,y_tr,x_ts,y_ts,'Logistic_Reg_P','n')\n    \n    # AdaBoost Classifier\n    adbc = AdaBoostClassifier(n_estimators=500,learning_rate=1,random_state=42)\n    run_algorithm(adbc,x_tr,y_tr,x_ts,y_ts,'Adaboost_Classifier',roc_curve) \n    \n    # XGB Classifier\n    xgb=XGBClassifier(random_state=42)\n    run_algorithm(xgb,x_tr,y_tr,x_ts,y_ts,'XGB_Classifier',roc_curve)\n    \n    # Basic random forest model\n    rf = RandomForestClassifier(random_state=42)\n    run_algorithm(rf,x_tr,y_tr,x_ts,y_ts,'Random_Forest_Classifier',roc_curve)\n    depth=[estimator.tree_.max_depth for estimator in rf.estimators_]\n    \n    # Tuned Random Forest Classifier using GridSearchCV to find optimal maximum depth and min_samples_leaf\n    n_folds = 4\n    parameters = {'max_depth': range(5,max(depth)+6, 3),\n              'min_samples_leaf': [3,4,5,6] }\n    rf = RandomForestClassifier(random_state=42)\n    rf_gs = GridSearchCV(rf, param_grid=parameters,\n                      cv=n_folds, \n                     scoring=\"accuracy\")\n    rf_gs.fit(x_tr, y_tr)\n    print('Best parameters: ',rf_gs.best_params_,'\\n')\n    # model with the best hyperparameters\n    rf_tuned = RandomForestClassifier(bootstrap=True,\n                             max_depth=rf_gs.best_params_['max_depth'],\n                             min_samples_leaf=rf_gs.best_params_['min_samples_leaf'],\n                             n_estimators=1500,\n                             random_state=42)\n    run_algorithm(rf_tuned,x_tr,y_tr,x_ts,y_ts,'Random_Forest_Classifier_GS',roc_curve)\n    print('Importance of Features : '+'\\n')\n    for i,j in zip(list(x_train_scaled),rf_tuned.feature_importances_):\n        print(i,\" : \",j)\n\n    print('\\n')\n    # gaussian Naive Bayes Classifier\n    gnb=GaussianNB()\n    run_algorithm(gnb,x_tr,y_tr,x_ts,y_ts,'Gaussian_NB',roc_curve)\n    \n    # SVM Classifier\n    folds = KFold(n_splits = 8, shuffle = True, random_state = 4)\n    params = [ {'gamma': [0.001,0.01,0.1,1,2,3],\n                     'C': [1,2,2.1,2.2,2,3,5]}]\n    model = SVC(kernel=\"rbf\")\n    model_cv = GridSearchCV(estimator = model, param_grid = params, \n                        scoring= 'accuracy', \n                        cv = folds, \n                        verbose = 1,\n                       return_train_score=True)\n    model_cv.fit(x_tr, y_tr) \n    cv_results = pd.DataFrame(model_cv.cv_results_)\n    cv_results.param_C=cv_results.param_C.astype(float)\n    cv_results.param_gamma=cv_results.param_gamma.astype(float)\n    print(' Best Parameters are : ',model_cv.best_params_,'\\n')\n\n    svm_gs = SVC(kernel='rbf',gamma=model_cv.best_params_['gamma'],C=model_cv.best_params_['C'],probability=True)\n    run_algorithm(svm_gs,x_tr,y_tr,x_ts,y_ts,'SVM_Classifier_RBF_Tuned',roc_curve)\n    \n    # Emsemble Classifier\n    voting_clf=VotingClassifier(estimators=[('lr',logreg_cv),('rf',rf_tuned),('svc',svm_gs),('adb',adbc)],\n                            voting='soft')\n    run_algorithm(voting_clf,x_tr,y_tr,x_ts,y_ts,'Voting_Classifier',roc_curve)\n    \n    return [logreg_cv,adbc,xgb,rf_tuned,gnb,svm_gs,voting_clf]\n\ndef model_select():\n# Use the score_frame to draw consolidated ROC curve and Score matris to select the appropriate model\n    score_mat=pd.DataFrame(data={'Algorithm':algorithm_name,\n      'Recall':recall_scores,\n      'F1':f1_scores,\n      'Test Accuracy':accuracy_scores_test,\n      'Train Accuracy':accuracy_scores_train})\n    print(score_mat)\n\n    plt.figure(figsize=(8, 8))\n\n    for i in plot_vars:\n        fpr,tpr,threshold = roc_curve(score_frame[i[0]],score_frame[i[1]])\n        plt.plot( fpr, tpr, label=i[2])\n\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristics')\n    plt.legend(loc=\"lower right\")\n\n    plt.show()\n\n    plt.figure(figsize=(8, 8))\n    for i in plot_vars:\n        precision, recall, threshold = precision_recall_curve(score_frame[i[0]],score_frame[i[1]])\n        plt.plot( precision, recall, label=i[2])\n\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Recall vs Precision Charecteric')\n    plt.legend(loc=\"top left\")\n    plt.show()","e3f37c02":"clfs=model_engine(x_train_pca,y_train_set,x_val_pca,y_val_set)","79ca9c16":"# Define function for a logistic classifier and apply RFE to derive the prominent features. Check multi colienarity.\n\ndef logrec_rfe(x, y):\n    # x = Feature variable \n    # y =  response variable  \n   \n    # Run RFE on logistic Regression\n    logreg_rfe = LogisticRegression(random_state=42)\n    y=list(y.astype('int'))\n    rfe = RFE(logreg_rfe,len(list(x)))             # running RFE with ALL variables as output\n    rfe = rfe.fit(x, y)\n\n    # Select the columns that are supported by RFE\n    col = x.columns[rfe.support_]\n\n    x_train_sm = sm.add_constant(x[col])\n    logm2 = sm.GLM(y,x_train_sm, family = sm.families.Binomial())\n    res = logm2.fit()\n    \n    # Check Multi colinearity using VIF\n    vif = pd.DataFrame()\n    vif['features'] = x[col].columns\n    vif['VIF'] = [variance_inflation_factor(x[col].values, i) for i in range(x[col].shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    \n    # Print stat summary and VIF table\n    print(res.summary())\n    print(vif)\n    \n    return list(vif['features'])\n","f44d6bf1":"prom_f = logrec_rfe(x_train_set,y_train_set)","4cb87638":"# Remove the variable with High VIF value  -'Pclass'\nprom_f=logrec_rfe((x_train_set.drop(columns=['Pclass'],axis=1)),y_train_set)","8309a6c2":"# Remove the variable with High P value  -'Parch'\nprom_f=logrec_rfe((x_train_set.drop(columns=['Pclass','Parch'],axis=1)),y_train_set)","d9d376fa":"model_select()","29ec966f":"# x_train is the full train data and x_test is the full test data.\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)","1aaed46d":"# Call the scaler function for the training set. Then, transform the validation set.\nscale,x_train_sc=scaler_f(x_train)\nx_test_sc=pd.DataFrame(data=scale.transform(x_test),columns=list(x_train))","b060582e":"# Apply PCA to extract 7 components and transform the validation set\npca_final,x_train_pc=pca_incr(x_train_sc,7)\nx_test_pc=pca_final.transform(x_test_sc)","14803b45":"print(clfs[3])","2ff15d7f":"# Run the selected model\nrf_tuned=clfs[3]  # Tuned Random Forest Model\nrf_tuned_model = rf_tuned.fit(x_train_sc,y_train)\n\n# predict values \npred_test=rf_tuned_model.predict(x_test_sc)\npred_train=rf_tuned_model.predict(x_train_sc)\nprint('Training Accuracy: ', accuracy_score(y_train,pred_train))","e5bf1d49":"# Write Submission File\nd={'PassengerId': test_data['PassengerId'],'Survived':pred_test}\nsubmission_df=pd.DataFrame(d)\nsubmission_df.to_csv('Titanic_Submission.csv',index=False)","5042e990":"# Stage 4 : Build Different Classification Models","0853b823":"Too many unique values. I will choose to ignore this cloumn. We might be loosing some potential information by ignoring this column. Instead of ignoring the column, we can derive some features for example - from the titles - Mr., Miss, Mrs etc... but for now, I will go with ignoring the column. ","5336d8a6":"So, it appears, that higher is the fare, higher is the chance of survival.","14c2292b":"In this Analysis, I think, tried to cover as much Machine Learning Concepts as applicable so there is enough Hands On. \nPlease feel free to comment (and upvote if you like) , as this keeps me motivated :) ","9a4901ef":"# Stage 6 - Model Selection","17ae340a":"Define a function to execute RFE (Recursive Feature Elimination) and identify the supporting columns. Run a Logistic Regression on the RFE supported features and look at the summary statistics.<br>\nNote that we will run the RFE on the scaled tranining set , rather than the PCA data.","850a6886":"# Stage 7 - Predict labels for Test data","6b4a4a81":"The investigation is much similar to what i have done for Age. I have decided to impute the ,missing Fare with the median value of Full data.","0b83d798":"# Stage 3: Data preparation for modelling","6c258f99":"From the box plots, what I can infer is:\n - Most of the passengers with lower and higer end of Age have not survived.\n - The is however remarkably higher aged passenger survived. - This can be counted as an outlier.\n - Fare is skewed. We can apply a logrithmic transformation to even the feature.\n - Again, a higher Fare - an outlier can be seen. ","f9b6103f":"# Stage - 1\nThe idea is to merge the training and test data after  - \n- Ensuring that the two data frames can be stacked (are same size). \n- Can be easily seperable based on a flag, when we needed.\n\nThen investigate each of the feature and apply the clean techniques as applicable.","500c7e04":"So, I will impute 'S' for the missing 'Embarked' codes.","564e8ef8":"Clearly the data is not clean - there are NaNs, categorical variables etc.. which needs to be treated. \nThe following lines details the idea of data cleaning methodology at high level - \n- For numeric features, I will take the mean or median as the value to impute.\n- For the features that have too many NaNs or very less unique ness will be excluded from the analysis.\n- Apply one hot encoding for the categorical variables","08752d19":"The approach I follow is, run the full train data aganist the selected model and then, use the model for predictions.","f4f05094":"So, 'Cabin' has high degree of variation, and also, the number of NaNs are 1014 - which is ~77% of total data. I choose to ignore this feature in my analysis.Please note that, before arriving to this conclusion, I tried extracting the first charecter of the Cabin and then encoded that to a numeric value( one -hot encoding). I have used a seperate Cabin 'XOC' to fill the vlaues. But, this methodology has introuduced lot of bais in my models. So, I have removed the feature. ","b74ab473":"# Titanic Disaster - Predicting Survivability Using Classification Techniques","a0cf4f3f":"Based on the above accuracies, Tuned Random Forest Classifier can be selected. It has given 84% accuracy on the train data.","98dfa898":"So, the most prominent features are - 'Age', 'Embarked','Fare','SibSp' and 'Sex'. Based on this, the varience in the data can be explained and to some degree the output label can be predicted. However, we have simply removed some features and this results in loss of accurancy.\nIf we compare these prominent features with the important features that Random Forest Algorithm has identified, we can see some similarities - \n- male  :  0.17055916349260553\n- Q  :  0.10184557188402982\n- S  :  0.17221016502344105\n- Age  :  0.26675961592763947\n- Fare  :  0.1184624028916517\n- Parch  :  0.09209078733810572\n- Pclass  :  0.0780722934425268","7499dcba":"Quite a few findings here , that gave me a clue on how to impute the missing values :  \n- The most common 'Embarked' code in test and train data is 'S'\n- There are no NaNs in test data, but there are 2 NaNs in Train data.\n- The 'Sex' of the two passengers who have missing Embarked Code is 'female' anf they both survived.\n- From the 'groupby' statistic above,we can see, there is higher chance, that a passenger has Embarked code 'S', if the sex is 'female' and they have 'survived'.","752e0f15":"# Stage 5 - Derive Prominent Features","6213e16c":"Ok, we can run the modelling with just 7 components to explain maximum varience. ","1cbc8a04":"Average value of y_train is 38.38% percent which meant, 38% of the data has Survival cases. So the data is fairly balanced and we don't have to apply imbalance techniques","e3672831":"What I can infer from above count plots :\n\n- Chance of Survival is higher in class 1 and 2 compared to class 3. However, this can be offset by the reason, that there are large number of travellers in class 3.\n- The chance of Survival is higher for femal passenger than a male passenger.","c8c0f22e":"Please Note that - I have detailed some inferences\/notes with in Markdown cells. Code level comments are with in the code cells.","ed6ab435":"There are some cases of string correlation , for example :\n - Pclass and Logrithmic transformation of Fare\n - Q and S ('Embarked') are correlated","4881f954":"So, from the above statistics, it is evident that the test data is slightly skewed than the train data. But, if we consider the 'Full data'( train + test), the mean and median are almost close. So, I have decided to impute the NaNs of Age feature with the median of Full data. Mean values have some bearing of outliers and therefore, it is always a good practive to choose median to impute NaNs.","bb85efc7":"# Stage 2: Visualisation to understand Data statistically","66d7d4c7":"I left all that code in comments, just to show how I handled this variable - if some one can give any better idea , that really helps :) ","dc9efb82":"Following is the flow of Analysis :  \n- Stage 1 - Apply standard Data Cleaning techniques, Treat Outliers , Derive Features as applicable\n- Stage 2 - EDA - Data Visualisation and Inferences\n- Stage 3 - Data Preparation for Modelling - Apply Dimensionality Reduction (PCA) and Balance the classes as appropriate\n- Stage 4 - Train ML models and derive the associated metrics\n- Stage 5 - Derive the prominent features \n- Stage 6 - Metric and Model Selection \n- Stage 7 - Predict Labels for Test Data"}}