{"cell_type":{"93f0c07a":"code","43f7dee6":"code","25becc81":"code","880590cf":"code","f74dee49":"code","e9c17698":"code","35e9d90d":"code","e4c769fa":"code","5e68b26d":"code","9a09df8e":"code","0f65e171":"code","b2df83fd":"code","2181dcf3":"code","bd939f77":"code","23ce0552":"code","19328a54":"code","61482fdf":"code","f24d1fb2":"code","f8da65d8":"code","0100cffc":"code","ce659c35":"code","ea0a6498":"code","21799cc2":"code","f1a1c55a":"code","272d78c6":"code","e402c4b0":"code","c7b11f13":"code","724410f1":"code","4dce9cc8":"code","87ed9f87":"code","bfc33052":"code","d440f2c4":"code","56a770c2":"code","140bec40":"code","a80d97c2":"code","b0d4cda9":"code","7b4d5cf2":"code","c318d330":"code","90d4f5e7":"code","7a780c63":"code","39199c95":"code","b97fdab0":"code","47ba3713":"code","aa4453db":"code","d93ec8c4":"code","c30885d7":"code","9ebed385":"code","28ae68f3":"code","f70f6523":"code","ed99707a":"code","dc0c3ff6":"code","56b383d0":"markdown","56cc07a1":"markdown","1ef72155":"markdown","dcadfb79":"markdown","014b65e4":"markdown","358d88d9":"markdown","98551cde":"markdown","1e854e43":"markdown","80c0d22a":"markdown","a193404d":"markdown","510b64d0":"markdown","56788a00":"markdown","9113859f":"markdown","8bb1ddf9":"markdown","67a70eb8":"markdown","16ef9d27":"markdown","4d0ae67a":"markdown","b17c668f":"markdown","b671c15b":"markdown","7c9b85a2":"markdown","6f2236ac":"markdown","1f1f17e0":"markdown","47521154":"markdown","f907dba3":"markdown","28513a0a":"markdown","4f9d9468":"markdown","978048a6":"markdown","19b3ab79":"markdown","d2d1dfdf":"markdown","79110d29":"markdown","fb33d85d":"markdown","77d841d6":"markdown","bdfa4f8e":"markdown"},"source":{"93f0c07a":"# import libraries\nimport pandas as pd\nimport numpy as np\n\nfrom zipfile import ZipFile \n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\nimport os,shutil\n\nimport keras\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, MaxPooling2D,Flatten,Dropout\nfrom keras import optimizers\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\nimport matplotlib.pyplot as plt","43f7dee6":"# unzip given input files\nimport zipfile\n\nwith zipfile.ZipFile('..\/input\/dogs-vs-cats-redux-kernels-edition\/train.zip') as z:\n    z.extractall()\n    \nwith zipfile.ZipFile('..\/input\/dogs-vs-cats-redux-kernels-edition\/test.zip') as z:\n    z.extractall()","25becc81":"# load sample submission file\nsub = pd.read_csv(\"..\/input\/dogs-vs-cats-redux-kernels-edition\/sample_submission.csv\")\nsub.shape","880590cf":"sub.head()","f74dee49":"# creating base directory directories\nbase_dir = '.\/cats_and_dogs_small'\nos.mkdir(base_dir)","e9c17698":"# creat sub directories(train,validation and test) under base directory\ntrain_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\n\nvalidation_dir = os.path.join(base_dir, 'validation')\nos.mkdir(validation_dir)\n\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)","35e9d90d":"# create sub-directories(cats and dogs) under each sub directories(train,validation and test) created above\ntrain_cats_dir = os.path.join(train_dir, 'cats')\nos.mkdir(train_cats_dir)\n\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\nos.mkdir(train_dogs_dir)\n\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\nos.mkdir(validation_cats_dir)\n\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\nos.mkdir(validation_dogs_dir)\n\ntest_cats_dir = os.path.join(test_dir, 'cats')\nos.mkdir(test_cats_dir)\n\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\nos.mkdir(test_dogs_dir)","e4c769fa":"# save the path of train and test directories where we saved cats and dogs images after unzipping the input files\noriginal_dataset_train_dir = '.\/train'\noriginal_dataset_test_dir = '.\/test'","5e68b26d":"# copy 1000 cats images to \".\/cats_and_dogs_small\/train\/cats\" sub directory we created above, these images will be used for training the model\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_train_dir, fname) # source folder \n    dst = os.path.join(train_cats_dir, fname) # destination folder\n    shutil.copyfile(src, dst) # copying cats images from source to destination folders","9a09df8e":"# copy 500 cats images to \".\/cats_and_dogs_small\/validation\/cats\" sub directory we created above, these images will be used for validing the model\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_train_dir, fname)\n    dst = os.path.join(validation_cats_dir, fname)\n    shutil.copyfile(src, dst)","0f65e171":"# copy 500 cats images to \".\/cats_and_dogs_small\/test\/cats\" sub directory we created above, these images will be used for testing the model\nfnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_train_dir, fname)\n    dst = os.path.join(test_cats_dir, fname)\n    shutil.copyfile(src, dst)","b2df83fd":"# copy 1000 dogs images to \".\/cats_and_dogs_small\/train\/dogs\" sub directory we created above, these images will be used for training the model\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_train_dir, fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)","2181dcf3":"# copy 500 dogs images to \".\/cats_and_dogs_small\/validation\/dogs\" sub directory we created above, these images will be used for validating the model\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000,1500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_train_dir, fname)\n    dst = os.path.join(validation_dogs_dir, fname)\n    shutil.copyfile(src, dst)","bd939f77":"# copy 500 dogs images to \".\/cats_and_dogs_small\/test\/dogs\" sub directory we created above, these images will be used for testing the model\nfnames = ['dog.{}.jpg'.format(i) for i in range(1500,2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_train_dir, fname)\n    dst = os.path.join(test_dogs_dir, fname)\n    shutil.copyfile(src, dst)","23ce0552":"# sanity check - lets check the folder where we copied the data has the data as expected\nprint(\"train_cats_dir:\",len(os.listdir(train_cats_dir)))\nprint(\"validation_cats_dir:\",len(os.listdir(validation_cats_dir)))\nprint(\"test_cats_dir:\",len(os.listdir(test_cats_dir)))\n\nprint(\"train_dogs_dir:\",len(os.listdir(train_dogs_dir)))\nprint(\"validation_dogs_dir:\",len(os.listdir(validation_dogs_dir)))\nprint(\"test_dogs_dir:\",len(os.listdir(test_dogs_dir)))","19328a54":"# import ImageDataGenerator for pre pro-processing JPEG images as explained above\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# scale pixes between 0 and 1 by dividing the pixes by 255\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,target_size=(150, 150),batch_size=20,class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(validation_dir,target_size=(150, 150),batch_size=20,class_mode='binary')","61482fdf":"# since image generators yields bathches indefinitely, we need to break the loop\nfor data_batch, labels_batch in train_generator:\n    print('data batch shape:', data_batch.shape)\n    print('labels batch shape:', labels_batch.shape)\n    break","f24d1fb2":"EPOCH_1 = 1\nEPOCH_100 = 100\nEPOCH_30 = 30","f8da65d8":"# Basic CNN archietechture without Regularization\ninput_shape = [150,150,3]\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","0100cffc":"# compile the model\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['accuracy'])","ce659c35":"# fitting the model as discussed above\nhistory = model.fit_generator(\n            train_generator,\n            steps_per_epoch=100,\n            epochs=EPOCH_30,\n            validation_data=validation_generator,\n            validation_steps=50)","ea0a6498":"# Let\u2019s plot the loss and accuracy of the model over the training and validation data during training\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\n\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","21799cc2":"datagen = ImageDataGenerator(\n            rotation_range=40,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            shear_range=0.2,\n            zoom_range=0.2,\n            horizontal_flip=True,\n            fill_mode='nearest')","f1a1c55a":"from keras.preprocessing import image\n\nfnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n\nimg_path = fnames[3]\nimg = image.load_img(img_path, target_size=(150, 150))\n\nx = image.img_to_array(img)\nx = x.reshape((1,) + x.shape)\nplt.figure(figsize = (12,8))\n\ni=0\n\nfor batch in datagen.flow(x, batch_size=1):\n    plt.subplot(2,2,i+1)\n    imgplot = plt.imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\n\nplt.show()","272d78c6":"# Basic CNN archietechture with Regularization\ninput_shape = [150,150,3]\n\nmodel_1 = Sequential()\n\nmodel_1.add(Conv2D(32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_1.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_1.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel_1.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_1.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel_1.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_1.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel_1.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_1.add(Flatten())\nmodel_1.add(Dense(512, activation='relu'))\nmodel_1.add(Dropout(0.3)) # to overcome overfitting\nmodel_1.add(Dense(512, activation='relu'))\nmodel_1.add(Dropout(0.3)) # to overcome overfitting\nmodel_1.add(Dense(1, activation='sigmoid'))","e402c4b0":"train_datagen = ImageDataGenerator(\n                rescale=1.\/255,\n                rotation_range=40,\n                width_shift_range=0.2,\n                height_shift_range=0.2,\n                shear_range=0.2,\n                zoom_range=0.2,\n                horizontal_flip=True,)\n\n# note, we are not applying these augmentations to the test data\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","c7b11f13":"train_generator = train_datagen.flow_from_directory(train_dir,target_size=(150, 150),batch_size=32,class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(validation_dir,target_size=(150, 150),batch_size=32,class_mode='binary')","724410f1":"# compile the model\nmodel_1.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['accuracy'])","4dce9cc8":"history = model_1.fit_generator(\n            train_generator,\n            steps_per_epoch=63,            \n            epochs = EPOCH_100,\n            validation_data=validation_generator,\n            validation_steps=32)","87ed9f87":"# Let\u2019s plot the loss and accuracy of the model over the training and validation data during training\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\n\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","bfc33052":"# lets import VGG16 pre-trained model\nfrom keras.applications import VGG16\n\nconv_base = VGG16(weights='imagenet',include_top=False,input_shape=(150, 150, 3))","d440f2c4":"conv_base.summary()","56a770c2":"#base_dir = '\/Users\/fchollet\/Downloads\/cats_and_dogs_small'\n#train_dir = os.path.join(base_dir, 'train')\n#validation_dir = os.path.join(base_dir, 'validation')\n#test_dir = os.path.join(base_dir, 'test')\n\ndatagen = ImageDataGenerator(rescale=1.\/255)\nbatch_size = 20\n\ndef extract_features(directory, sample_count):\n    features = np.zeros(shape=(sample_count, 4, 4, 512))\n    labels = np.zeros(shape=(sample_count))\n    generator = datagen.flow_from_directory(directory,target_size=(150, 150),batch_size=batch_size,class_mode='binary')\n    \n    i=0\n    for inputs_batch, labels_batch in generator:\n        features_batch = conv_base.predict(inputs_batch)\n        features[i * batch_size : (i + 1) * batch_size] = features_batch\n        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n        i += 1\n        if i * batch_size >= sample_count:\n            break\n    return features, labels","140bec40":"train_features, train_labels = extract_features(train_dir, 2000)\nvalidation_features, validation_labels = extract_features(validation_dir, 1000)","a80d97c2":"train_features = np.reshape(train_features, (2000, 4*4* 512))\nvalidation_features = np.reshape(validation_features, (1000, 4*4* 512))","b0d4cda9":"# Densly Connected Classifier\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\nmodel.add(keras.layers.Dropout(0.5))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))","7b4d5cf2":"# compile the model\nmodel.compile(optimizer=optimizers.RMSprop(lr=2e-5),\nloss='binary_crossentropy',\nmetrics=['acc'])","c318d330":"# fit the model on the features extracted\nhistory = model.fit(train_features, train_labels,epochs=EPOCH_30,batch_size=20,validation_data=(validation_features, validation_labels))","90d4f5e7":"# plotting the results\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","7a780c63":"#  Adding a densely connected classifier on top of the convolutional base\nmodel = keras.models.Sequential()\nmodel.add(conv_base) # adding pre-trained conv base model\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(256, activation='relu'))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))","39199c95":"model.summary()","b97fdab0":" print('This is the number of trainable weights before freezing the conv base:', len(model.trainable_weights))","47ba3713":"# freezing the layers of pre-trained model\nconv_base.trainable = False","aa4453db":"print('This is the number of trainable weights after freezing the conv base:', len(model.trainable_weights))","d93ec8c4":"#  Training the model end to end with a frozen convolutional base\ntrain_datagen = ImageDataGenerator(\n                rescale=1.\/255,\n                rotation_range=40,\n                width_shift_range=0.2,\n                height_shift_range=0.2,\n                shear_range=0.2,\n                zoom_range=0.2,\n                horizontal_flip=True,\n                fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n                    train_dir,\n                    target_size=(150, 150),\n                    batch_size=20,\n                    class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(validation_dir,target_size=(150, 150),batch_size=20,class_mode='binary')","c30885d7":"model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(lr=2e-5),metrics=['acc'])","9ebed385":"history = model.fit_generator(\n            train_generator,\n            steps_per_epoch=100,\n            epochs=EPOCH_30,\n            validation_data=validation_generator,\n            validation_steps=50)","28ae68f3":"# plotting the results\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","f70f6523":"conv_base.trainable = True\nset_trainable = False\n\nfor layer in conv_base.layers:\n    if layer.name == 'block5_conv1':\n        set_trainable = True\n    \n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","ed99707a":"model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(lr=1e-5),metrics=['acc'])\n\nhistory = model.fit_generator(train_generator,steps_per_epoch=100,epochs=EPOCH_100,validation_data=validation_generator,validation_steps=50)","dc0c3ff6":"# plotting the results\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","56b383d0":"The final feature map has shape (4, 4, 512). That\u2019s the feature on top of which you\u2019ll stick a densely connected classifier.\n\nAt this point, there are two ways we could proceed:\n\n1) Running the convolutional base over our dataset, recording its output to a Numpy array on disk, and then using this data as input to a standalone, densely connected classifier, This solution is fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, this technique won\u2019t allow you to use data augmentation.\n\n2) Extending the model we have (conv_base) by adding Dense layers on top, and running the whole thing end to end on the input data. This will allow you to use data augmentation, because every input image goes through the convolutional base every time it\u2019s seen by the model. But for the same reason, this technique is far more expensive than the first\n\nWe\u2019ll discuss both the techniques here!","56cc07a1":"As we can see, the convolutional base of VGG16 has 14,714,688 parameters, which is very large. The classifier we\u2019re adding on top has 2 million parameters.\n\nBefore we compile and train the model, it\u2019s very important to freeze the convolutional base. Freezing a layer or set of layers means preventing their weights from being updated during training. \n\nIf we don\u2019t do this, then the representations that were previously learned by the convolutional base will be modified during training,because the Dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.\n\nIn Keras, we can freeze a network by setting its trainable attribute to False. Let's check how to do it!","1ef72155":"We passed three arguments to the constructor:\n\n1) `weights` specifies the weight checkpoint from which to initialize the model.\n\n2) `include_top` refers to including (or not) the densely connected classifier on top of the network. By default, this densely connected classifier corresponds to the 1,000 classes from ImageNet. Because we intend to use our own densely connected classifier (with only two classes: cat and dog), we don\u2019t need to include it.\n\n3) `input_shape` is the shape of the image tensors that you\u2019ll feed to the network. This argument is purely optional: if we don\u2019t pass it, the network will be able to process inputs of any size.\n\nHere\u2019s the detail of the architecture of the VGG16 convolutional base. It\u2019s similar to the simple convnets you\u2019re already familiar with:","dcadfb79":"In this kernel we will see how we can get the best results by using only a part of the given data for training.\n\nThis dataset contains 25,000 images of dogs and cats (12,500 from each class)\n\nWe will pick only 4000 images from this dataset and build our model over it.","014b65e4":"We have 93% accuracy now, which is way better than the model that we build from scratch.\n\nIn the next version of this notebook, we will make predictions on the test dataset!","358d88d9":"# How to build models with limited amount of data","98551cde":"### 3.1) Basic CNN Model","1e854e43":"**Let\u2019s look at the output of one of these generators:**\n\nIt yields batches of 150 \u00d7 150 RGB images (shape (20, 150, 150, 3)) and binary labels (shape (20,)). \n\nThere are 20 samples in each batch (the batch size). \n\nNote that the generator yields these batches indefinitely: it loops endlessly over the images in the target folder.\nFor this reason, you need to break the iteration loop at some point:","80c0d22a":"**Before we fit the model to the data using the generator, lets discuss few important points.**\n\nWe will use `fit_generator` method to fit the data using generators. \n\nIt expects as its `first argument` a `Python generator` that will yield batches of inputs and targets indefinitely(as discussed above). \n\n**Because the data is being generated endlessly, the Keras model needs to know how many samples to draw from the generator before declaring an epoch over. This is the role of the `steps_per_epoch` argument** \n\nIn this case, batches are 20 samples, so it will take 100 batches until you see your target of 2,000 samples.\n \nWhen using fit_generator, you can pass a validation_data argument, much as with the fit method. It\u2019s important to note that this argument is allowed to be a data generator, but it could also be a tuple of Numpy arrays. \n \nIf you pass a generator as validation_data, then this generator is expected to yield batches of validation data endlessly; thus you should also specify the validation_steps argument, which tells the process how many batches to draw from the validation generator for evaluation, let build the model now.","a193404d":"### 3.3) Model Using a pretrained convnet(VGG16)\nA common and highly effective approach to deep learning on small image datasets is to use a `pretrained network`. \n\nA pretrained network is a saved network that was previously trained on a large dataset, typically on a large-scale image\nclassification task. \n\nIf this original dataset is large enough and general enough, then the spatial hierarchy of features learned by the pretrained network can effectively act as a generic model of the visual world, and hence its features can prove useful for many different computervision problems, even though these new problems may involve completely different classes than those of the original task. \n\nFor instance, you might train a network on ImageNet (where classes are mostly animals and everyday objects) and then repurpose this trained network for something as remote as identifying furniture items in images. \n\nSuch portability of learned features across different problems is a key advantage of deep learning compared to many older, shallow-learning approaches, and it makes deep learning very effective for small-data problems.\n \nIn this case, let\u2019s consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1,000 different classes). ImageNet contains many animal classes, including different species of cats and dogs, and you can thus expect to\nperform well on the dogs-versus-cats classification problem.\n\nWe\u2019ll use the VGG16 architecture, developed by Karen Simonyan and Andrew Zisserman in 2014; it\u2019s a simple and widely used convnet architecture for ImageNet.\n\nThere are two ways to use a pretrained network: feature extraction and fine-tuning. We\u2019ll cover both of them in this notebook. \n\nLet\u2019s start with feature extraction.","510b64d0":"So we indeed have 2,000 training images, 1,000 validation images. \n\nEach split contains the same number of samples from each class: this is a balanced binary-classification problem, which means classification accuracy will be an appropriate measure of success. ","56788a00":"These plots are characteristic of overfitting. The training accuracy increases linearly over time, until it reaches nearly 100%, whereas the validation accuracy stalls at 70\u201372%.\n\nThe validation loss reaches its minimum after only five epochs and then stalls, whereas the training loss keeps decreasing linearly until it reaches nearly 0.\n\nBecause we have relatively few training samples (2,000), overfitting will be our number-one concern. \nWe already know about a number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). \n\nWe\u2019re now going to work with a new one, specific to computer vision and used almost universally when processing images with deep-learning models: `data augmentation`","9113859f":"Training is very fast, because we only have to deal with two Dense layers","8bb1ddf9":"#### 3.3.2) FAST FEATURE EXTRACTION WITHOUT DATA AUGMENTATION\n\nWe\u2019ll start by running instances of the previously introduced ImageDataGenerator to extract images as Numpy arrays as well as their labels. We\u2019ll extract features from these images by calling the predict method of the conv_base model.","67a70eb8":"At this point, we can define our densely connected classifier (note the use of dropout for regularization) and train it on the data and labels that we just recorded.","16ef9d27":"If you train a new network using this data-augmentation configuration, the network will never see the same input twice. \n\nBut the inputs it sees are still heavily intercorrelated, because they come from a small number of original images\u2014you can\u2019t produce new information, you can only remix existing information. \n\nAs such, this may not be enough to completely get rid of overfitting. To further fight overfitting, you\u2019ll also add a Dropout layer to your model, right before the densely connected classifier.","4d0ae67a":"The extracted features are currently of shape (samples, 4, 4, 512). You\u2019ll feed them to a densely connected classifier, so first you must flatten them to (samples, 8192):","b17c668f":"## 1) Make Directories","b671c15b":"Now we can begin fine-tuning the network. \n\nWe\u2019ll do this with the RMSProp optimizer, using a very low learning rate. The reason for using a low learning rate is that\nwe want to limit the magnitude of the modifications we make to the representations of the three layers we\u2019re fine-tuning. \n\nUpdates that are too large may harm these representations.","7c9b85a2":"### 3.2) Model with data augmentation(to overcome overfitting)\n\nOverfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. \n\nGiven infinite data, your model Training a convnet from scratch on a small dataset would be exposed to every possible aspect of the data distribution at hand: you would never overfit. \n\nData augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images.\n\nThe goal is that at training time, your model will never see the exact same picture twice. This helps expose the model\nto more aspects of the data and generalize better.\n\nIn Keras, this can be done by configuring a number of random transformations to be performed on the images read by the ImageDataGenerator instance. Let\u2019s get started with an example.","6f2236ac":"We reached a validation accuracy of about 90%\u2014much better than we achieved earlier with the small model trained from scratch. \n\nBut the plots also indicate that we\u2019re overfitting almost from the start\u2014despite using dropout with a fairly large\nrate\n\nThat\u2019s because this technique doesn\u2019t use data augmentation, which is essential for preventing overfitting with small image datasets. ","1f1f17e0":"## 3) Model Building","47521154":"#### 3.3.1) Feature extraction \n\nFeature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch.\n\nAs you saw previously, convnets used for image classification comprise two parts:\n\n**Part-1:** They start with a series of pooling and convolution layers.This part is called the convolutional base of the model.\n\n**Part-2:** They end with a densely connected classifier.  \n\nIn the case of convnets, feature extraction consists of taking the convolutional base of apreviously trained network, running the new data through it, and training a new classifier on top of the output(see figure below)\n\n![image.png](attachment:image.png)\n\nWhy only reuse the convolutional base? Could you reuse the densely connected classifier as well? \n\nIn general, doing so should be avoided. The reason is that the representations learned by the convolutional base are likely to be more generic and therefore more reusable whereas the representations learned by the classifier will necessarily be specific to the\nset of classes on which the model was trained.  \n\nNote that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. \n\nLayers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), whereas layers that are higher up extract more-abstract concepts (such as \u201ccat ear\u201d or \u201cdog eye\u201d). \n\nSo if your new dataset differs a lot from the dataset on which the original model was trained, you may be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base.\n\nIn this case, because the ImageNet class set contains multiple dog and cat classes, it\u2019s likely to be beneficial to reuse the information contained in the densely connected layers of the original model. \n\nBut we\u2019ll choose not to, in order to cover the more general case where the class set of the new problem doesn\u2019t overlap the class set of the original model. \n\nLet\u2019s put this in practice by using the convolutional base of the VGG16 network, trained on ImageNet, to extract interesting features from cat and dog images, and then train a dogs-versus-cats classifier on top of these features.\n\nThe VGG16 model, among others, comes prepackaged with Keras. You can import it from the keras.applications module. Here\u2019s the list of image-classification models (all pretrained on the ImageNet dataset) that are available as part of keras.applications:\n\n* Xception\n* Inception V3\n* ResNet50\n* VGG16\n* VGG19\n* MobileNet\n* \nLet\u2019s instantiate the VGG16 model.","f907dba3":"Let\u2019s plot the results again. \n\nGreat, we reached a validation accuracy of about 90%. This is much better than we achieved with the small convnet trained from scratch.","28513a0a":"We will create a new dataset containing three subsets: \n\n* A training set with 1,000 samples of each class(CAT & DOG)\n \n* A validation set with 500 samples of each class","4f9d9468":"Let's check augmented images!","978048a6":"### 3.3.2) Fine Tuning\n\nAnother widely used technique for model reuse, complementary to feature extraction, is fine-tuning (see figure below). \n\n![image.png](attachment:image.png)\n\nFine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training\nboth the newly added part of the model (in this case, the fully connected classifier) and these top layers. \n\nThis is called fine-tuning because it slightly adjusts the more abstract representations of the model being reused, in order to make them more relevant for the problem at hand.\n\nAs stated earlier that it\u2019s necessary to freeze the convolution base of VGG16 in order to be able to train a randomly initialized classifier on top. \n\nFor the same reason, it\u2019s only possible to fine-tune the top layers of the convolutional base once the classifier on top\nhas already been trained. If the classifier isn\u2019t already trained, then the error signal propagating through the network during training will be too large, and the representations previously learned by the layers being fine-tuned will be destroyed. \n\nThus the steps for fine-tuning a network are as follow:\n\n* Add custom network on top of an already-trained base network.\n* Freeze the base network.\n* Train the part you added.\n* Unfreeze some layers in the base network.\n* Jointly train both these layers and the part you added.\n\nWe already completed the first three steps when doing feature extraction. \n\nLet\u2019s proceed with step 4: unfreeze conv_base and then freeze individual layers inside it.\n\n\nWe\u2019ll fine-tune the last three convolutional layers, which means all layers up to block4_pool should be frozen, and the layers \nblock5_conv1, block5_conv2, and block5_conv3 should be trainable.\n \n**Why not fine-tune more layers? Why not fine-tune the entire convolutional base?**\n\nWe could. But we need to consider the following:\n\nEarlier layers in the convolutional base encode more-generic, reusable features,whereas layers higher up encode more-specialized features. \n\nIt\u2019s more useful to fine-tune the more specialized features, because these are the ones that need to be repurposed on your new problem. There would be fast-decreasing returns in fine-tuning lower layers.\n\nThe more parameters we\u2019re training, the more we\u2019re at risk of overfitting.\n\nThe convolutional base has 15 million parameters, so it would be risky to attempt to train it on our small dataset.\n\nThus, in this situation, it\u2019s a good strategy to fine-tune only the top two or three layers in the convolutional base. \n\nLet\u2019s set this up, starting from where we left off in the previous example.","19b3ab79":"These are just a few of the options available (for more, see the Keras documentation).\n\n**Lets quickly go over this code:**\uf0a1 \n\n* `rotation_range` is a value in degrees (0\u2013180), a range within which to randomly rotate pictures.\n* `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n* `shear_range` is for randomly applying shearing transformations.\n* `zoom_range` is for randomly zooming inside pictures.\n* `horizontal_flip` is for randomly flipping half the images horizontally\u2014relevant when there are no assumptions of horizontal asymmetry (for example,real-world pictures).\n* `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width\/height shift.","d2d1dfdf":"## 2) Data preprocessing\n\nTo feed the data into a Neural Networks, it should be formatted into appropriately preprocessed floating point tensors.\nCurrently, our data is in JPEG format, so the steps for getting it into the network are roughly as follows:\n\n* Read the picture files.\n\n* Decode the JPEG content to RGB grids of pixels.\n\n* Convert these into floating-point tensors.\n\n* Rescale the pixel values between 0 and 1 (as you know,neural networks prefer to deal with small input values).\n\nIt may seem a bit daunting, but fortunately Keras has utilities to take care of these steps automatically.\n\nKeras has a module with image-processing helper tools, located at `keras.preprocessing.image`. \n\nIn particular, it contains the class `ImageDataGenerator`, which lets you quickly set up Python generators that can automatically turn image files on disk into batches of preprocessed tensors. This is what we\u2019ll use here.","79110d29":"**You must be already familiar with the following CNN archietechture, if not, you can go through [This Kernel](https:\/\/www.kaggle.com\/jagdmir\/all-you-need-to-know-about-cnns)**.","fb33d85d":"By using regularization techniques even further, and by tuning the network\u2019s parameters (such as the number of filters per convolution layer, or the number of layers in the network), we may be able to get an even better accuracy, likely up to 86% or 87%.\n\nBut it would prove difficult to go any higher just by training your own convnet from scratch, because we have so little data to work with. As a next step to improve your accuracy on this problem, we will use a pretrained model.","77d841d6":"With this setup, only the weights from the two Dense layers that we added will be trained. That\u2019s a total of four weight tensors: two per layer (the main weight matrix and the bias vector). \n\nNote that in order for these changes to take effect, we must first compile the model. \n\nIf we ever modify weight trainability after compilation, we should then recompile the model, or these changes will be ignored.\n \nNow you can start training your model, with the same data-augmentation configuration that we used in the previous example.","bdfa4f8e":"#### 3.3.2) FEATURE EXTRACTION WITH DATA AUGMENTATION\n\nNow, let\u2019s review the second technique we mentioned for doing feature extraction,which is much slower and more expensive, but which allows us to use data augmentation during training: extending the conv_base model and running it end to end on the inputs.\n\nBecause models behave just like layers, you can add a model (like conv_base) to a Sequential model just like you would add a layer."}}