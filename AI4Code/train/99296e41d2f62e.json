{"cell_type":{"9f0596ce":"code","ae9cc326":"code","245f66d9":"code","f68eaf82":"code","67059ffc":"code","c0b65468":"code","03f36c9b":"code","42aafda2":"code","5614d796":"code","db9187df":"code","9a2f8ed8":"code","29d21718":"code","ed5d4620":"code","af928fab":"code","5b239a68":"code","201016c3":"code","e1d28036":"code","e3e31910":"code","e73ab7ef":"code","7384fa0b":"code","0d76671b":"code","575ec5de":"code","c1ce2265":"code","49f9e25f":"code","929f28d3":"code","9fadffed":"code","dc591db3":"code","42cca7a0":"code","9615d87a":"code","97ddcda9":"markdown","d0b6df64":"markdown","870657eb":"markdown","14d7460d":"markdown","7cc05557":"markdown"},"source":{"9f0596ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sys, string\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nimport keras as K\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae9cc326":"arabic_rt = pd.read_json('..\/input\/arabic-rt-news-headlines-20200419\/arabic.rt.com_20190419_titles.json')\narabic_rt","245f66d9":"arabic_rt_test = arabic_rt.sample(frac=0.2, random_state=1007)\narabic_rt_train = arabic_rt.drop(arabic_rt_test.index)","f68eaf82":"def clean_text(txt):\n    txt = \" \".join(v for v in txt if v not in string.punctuation)\n    txt = txt.encode(\"utf8\").decode(\"utf-8\",'ignore')\n    txt = txt.replace('\\r', ' ').replace('\\n', ' ')\n    return txt \n\n#arabic_rt_train\ntext = clean_text(arabic_rt_train['headline'])\n#convert the characters in our input to numbers\nchars = sorted(list(set(text)))\n# save the char and it indice in dic\nchar_to_idx = dict((c, i) for i, c in enumerate(chars))\n# show insights\nprint(\"Total unique vocab:\",  len(chars))","67059ffc":"text[:300]","c0b65468":"# To feed our network, we defined the long of an individual sequence\nseq_length = 90","03f36c9b":"idx_to_char = np.array(chars)\n# conert the text to a secunece of numbers\ntext_as_int = np.array([char_to_idx[c] for c in text])\n# show sample of results\nfor char,_ in zip(char_to_idx, range(20)):\n    print('{:4s}: {:3d},'.format(repr(char), char_to_idx[char]))","42aafda2":"# Show how the first 13 characters from the text are mapped to integers\nprint('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))","5614d796":"# The maximum length sentence as a single input in characters\nseq_length = 90\nexamples_per_epoch = len(text)\/\/(seq_length+1)\nprint('{} exaple per epoch'.format(examples_per_epoch))\n\n# Create training examples x_data \/ targets y_data\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)","db9187df":"# show first 5 items in our dataset, but until now its chars\nfor i in char_dataset.take(5):\n    print(idx_to_char[i.numpy()])","9a2f8ed8":"# convert char to sentanses becuse we handel with sequences\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n# show first 5 items\nfor item in sequences.take(5):\n    print(repr(''.join(idx_to_char[item.numpy()])))","29d21718":"# Each sequence starts with the next character in the input data \n# (The input sequence would be \"Hell\", and the target sequence \"ello\".)\n\ndef split_input_target(chunk):\n    input_text = chunk[:-1] # string except last char\n    target_text = chunk[1:] # string except first char\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","ed5d4620":"# show a sample of input and target sequences\nfor input_example, target_example in  dataset.take(1):\n    print('Input data: ', repr(''.join(idx_to_char[input_example.numpy()])))\n    print('Target data:', repr(''.join(idx_to_char[target_example.numpy()])))","af928fab":"# other example which char to predict\nfor i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"input: {} ({:s})\".format(input_idx, repr(idx_to_char[input_idx])))\n    print(\"expected output: {} ({:s})\".format(target_idx, repr(idx_to_char[target_idx])))","5b239a68":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","201016c3":"# Length of the vocabulary in chars\nvocab_size = len(chars)\n# The embedding dimension\nembedding_dim = 256\n# Number of RNN units\nrnn_units = 1024","e1d28036":"def perplexity(y_true, y_pred):\n    cross_entropy = \\\n        tf.keras.backend.sparse_categorical_crossentropy(y_true, y_pred)\n\n    # perplexity = tf.keras.backend.exp(cross_entropy)\n    # return perplexity\n\n    #cross_entropy = \\\n        #tf.keras.backend.cast(tf.keras.backend.equal(tf.keras.backend.max(y_true,\n                              #axis=-1),\n                              #tf.keras.backend.cast(tf.keras.backend.argmax(y_pred,\n                              #axis=-1), tf.keras.backend.floatx())),\n                              #tf.keras.backend.floatx())\n\n    #perplexity = tf.keras.backend.exp(cross_entropy)\n    # as NLTK implementation\n    perplexity = tf.keras.backend.pow(2.0, cross_entropy)\n    return perplexity","e3e31910":"K = tf.keras.backend # Alias to Keras' backend namespace.\n\nclass PerplexityMetric(tf.keras.metrics.Metric):\n    def __init__(self, name='perplexity', **kwargs):\n      super(PerplexityMetric, self).__init__(name=name, **kwargs)\n      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n      self.perplexity = self.add_weight(name='tp', initializer='zeros')\n\n    # @tf.function\n    def _calculate_perplexity(self, real, pred):\n        # The next 4 lines zero-out the padding from loss calculations, \n        # this follows the logic from: https:\/\/www.tensorflow.org\/beta\/tutorials\/text\/transformer#loss_and_metrics\n      mask = tf.math.logical_not(tf.math.equal(real, 0))\n      loss_ = self.cross_entropy(real, pred)\n      mask = tf.cast(mask, dtype=loss_.dtype)\n      loss_ *= mask\n      # Calculating the perplexity steps:\n      step1 = K.mean(loss_, axis=-1)\n      step2 = K.exp(step1)\n      perplexity = K.mean(step2)\n\n      return perplexity \n\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n      # TODO:FIXME: handle sample_weight ! \n      if sample_weight is not None:\n          print(\"WARNING! Provided 'sample_weight' argument to the perplexity metric. Currently this is not handled and won't do anything differently..\")\n      perplexity = self._calculate_perplexity(y_true, y_pred)\n      # Remember self.perplexity is a tensor (tf.Variable), so using simply \"self.perplexity = perplexity\" will result in error because of mixing EagerTensor and Graph operations \n      self.perplexity.assign_add(perplexity)\n        \n    def result(self):\n      return self.perplexity\n\n    def reset_states(self):\n      # The state of the metric will be reset at the start of each epoch.\n      self.perplexity.assign(0.)","e73ab7ef":"# define loss function\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","7384fa0b":"# test data\n# arabic_rt_test","0d76671b":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                                  batch_input_shape=[batch_size, None]),\n        tf.keras.layers.RNN(\n            tf.keras.layers.GRUCell(rnn_units), return_sequences=True,stateful=True),\n        \n        tf.keras.layers.Dense(vocab_size)\n    ])\n    return model","575ec5de":"model = build_model(\n    vocab_size=vocab_size,\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n    batch_size=BATCH_SIZE)\n\n# display model summary\nmodel.summary()","c1ce2265":"# compile the model by adding adam optimizer and our loss function metrics=[perplexity]\nmodel.compile(optimizer='adam', loss=loss, metrics=[PerplexityMetric()])","49f9e25f":"# Directory where the checkpoints will be saved \n# ( you must create it befor run this code)\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","929f28d3":"# number of traing epochs\nEPOCHS = 7\n# train model\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","9fadffed":"# load traind weights and files form our dir\ntf.train.latest_checkpoint(checkpoint_dir)","dc591db3":"# init the model\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n#load weights\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n# build the model\nmodel.build(tf.TensorShape([1, None]))","42cca7a0":"def generate_text(model, start_string):\n    # Evaluation step (generating text using the learned model)\n\n    # Number of characters to generate\n    num_generate = 90\n\n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char_to_idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our results\n    text_generated = []\n\n    # Low temperature results in more predictable text.\n    # Higher temperature results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 1.0\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # Pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx_to_char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","9615d87a":"print(generate_text(model, start_string=u\"\u0627\u0644\u0639\u062b\u0648\u0631\"))","97ddcda9":"# Recurrent Neural Language Model RNLM\n\n","d0b6df64":"## Train the model","870657eb":"# Train-test split ","14d7460d":"## Create training batches","7cc05557":"## Execute the training"}}