{"cell_type":{"76397666":"code","2e9e3734":"code","614b4686":"code","a6e3c434":"code","e1686e2b":"code","ebfca415":"code","68de2626":"code","f0e5cf2e":"code","4f20d55d":"code","1fdcbcfd":"code","fd35f8aa":"code","4b5390f6":"code","f4ea0707":"code","b62401e7":"code","733fdd85":"code","33f96199":"code","5eb63c20":"code","7d20f041":"code","7e1668af":"code","94040c43":"code","fc584745":"code","acd09548":"code","cbac9214":"code","1212e9df":"code","0361e4df":"code","d0b16dd4":"code","6bb9341f":"code","23e2eddb":"code","585f163e":"code","05749785":"code","e33c44a6":"code","88163f6a":"code","86c10a13":"code","2d7b9a43":"code","49dd7092":"code","b6dd0afa":"code","e893929b":"code","0cb1868b":"code","c1996da4":"code","1d0ba731":"code","e191718a":"code","18f29af5":"code","077a1e9c":"code","25ff6fbc":"code","113544f0":"code","f135400d":"code","d225a563":"code","57cd4a52":"code","d3ea66c0":"code","26cb6087":"code","ae5bef6e":"code","19927dd4":"code","239376c9":"code","83da394b":"code","d4a2ae5a":"code","14dbb730":"code","24ecbc3a":"code","570b7e10":"markdown","a0201dca":"markdown","f7c8f56c":"markdown","fe0b8e99":"markdown","c15e974b":"markdown","3a91c003":"markdown","c94ff4d4":"markdown"},"source":{"76397666":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2e9e3734":"import matplotlib.pyplot as plt\nimport seaborn as sns\n","614b4686":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","a6e3c434":"train.head()","e1686e2b":"test.head()","ebfca415":"print(train.shape)\nprint(test.shape)","68de2626":"train.isna().sum()","f0e5cf2e":"test.isna().sum()","4f20d55d":"train.columns","1fdcbcfd":"train['Survived'].value_counts(normalize=True)","fd35f8aa":"sns.countplot(train['Survived'])","4b5390f6":"train['Fare'].describe()","f4ea0707":"pd.crosstab(train['Sex'],train['Survived']).plot.bar(stacked=True)\nplt.ylabel('Frequency')\nplt.show()\n","b62401e7":"sns.boxplot(y=train['Age'])","733fdd85":"plt.figure(figsize=(10,5))\nsns.boxplot(train['Pclass'],train['Age'])\nplt.show()","33f96199":"train[train['Age']<1]","5eb63c20":"fix,ax = plt.subplots(1,3,figsize=(12,5))\nsns.boxplot(y=train['Fare'],ax=ax[0])\nsns.distplot(train['Fare'],ax=ax[1])\nsns.distplot(train['Age'].dropna(),ax=ax[2])\nplt.tight_layout()\nplt.show()","7d20f041":"train['Age'].skew()","7e1668af":"pd.crosstab(train['Pclass'],train['Survived']).plot.bar(stacked=True)\nplt.xlabel('Frequency')\nplt.show()","94040c43":"pd.crosstab(train['Parch'],train['Survived']).plot.bar(stacked=True)\nplt.show()","fc584745":"pd.crosstab(train['Embarked'],train['Survived'],).plot.bar(stacked=True)\nplt.show()","acd09548":"sns.heatmap(pd.crosstab(train['SibSp'],train['Survived']),annot=True,cmap='Blues')\nplt.show()","cbac9214":"from wordcloud import WordCloud\nfor col in ['Name','Cabin']:\n    \n    text = \" \".join(review for review in train[col].dropna())\n    word = WordCloud(width=1000,height=800,margin=0,max_font_size=150,background_color='white').generate(text)\n\n    plt.figure(figsize=[8,8])\n    plt.imshow(word,interpolation='bilinear')\n    plt.axis('off')\n    plt.show()","1212e9df":"#IQR method\nfor col in ['Age','Fare']:\n    \n    q1= train[col].quantile(0.25)\n    q3 = train[col].quantile(0.75)\n    iqr=q3-q1\n    print(col,'IQR',iqr)\n\n    upper_limit = q3+1.5*iqr\n    lower_limit = q1-1.5*iqr\n    print(col,'Upper limit for age',upper_limit)\n    print(col,'Lower limit for age',lower_limit)","0361e4df":"sns.heatmap(train.corr(),cmap='Blues',annot=True)\nplt.show()","d0b16dd4":"train_1 = train.copy()","6bb9341f":"age_median=train_1['Age'].median()\ntrain_1['Age'] = train_1['Age'].fillna(age_median)","23e2eddb":"for col in train.columns:\n    \n    print(col,'Percentage of missing values',train[col].isna().sum()\/train.shape[0]*100)","585f163e":"train_1.drop(columns=['Cabin'],inplace=True)","05749785":"from sklearn_pandas import CategoricalImputer\nimputer = CategoricalImputer()\ntrain_1['Embarked']=imputer.fit_transform(train['Embarked'])","e33c44a6":"train_1.isna().sum()","88163f6a":"#we will drop passengerid,Ticket,Name\ntrain_1.drop(columns=['Name','Ticket','PassengerId'],inplace=True)","86c10a13":"train_1 = pd.get_dummies(data=train_1,columns=['Sex','Embarked'],drop_first=True)\ntrain_1.head()","2d7b9a43":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier,VotingClassifier\nfrom sklearn.naive_bayes import BernoulliNB,GaussianNB\nfrom sklearn.svm import SVC","49dd7092":"X = train_1.drop(columns=['Survived'],axis=1)\ny= train_1['Survived']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","b6dd0afa":"model = DecisionTreeClassifier(max_depth=6,class_weight='balanced',random_state=0)\nmodel.fit(X_train,y_train)\nacc_decision_tree=model.score(X_train,y_train)*100\nprint(model.score(X_train,y_train))\nprint(model.score(X_test,y_test))","e893929b":"model.feature_importances_","0cb1868b":"sns.barplot(x=model.feature_importances_,y=['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q',\n       'Embarked_S'])\nplt.title('Feature Importance Plot')\nplt.show()","c1996da4":"model1  = LogisticRegression(class_weight='balanced',C=4.5,random_state=0)\nmodel1.fit(X_train,y_train)\nacc_logistic=model1.score(X_train,y_train)*100\nprint(model1.score(X_train,y_train))\nprint(model1.score(X_test,y_test))","1d0ba731":"model2  = RandomForestClassifier(n_estimators=21,max_depth=6,criterion='gini',random_state=0,class_weight='balanced',\n                                min_samples_split=2)\nmodel2.fit(X_train,y_train)\nacc_random_forest=model2.score(X_train,y_train)*100\nprint(model2.score(X_train,y_train))\nprint(model2.score(X_test,y_test))","e191718a":"rf  = RandomForestClassifier(n_estimators=1000,min_samples_split=30,min_samples_leaf=5,random_state=42,warm_start=True)\nrf.fit(X_train,y_train)\nacc_random_forest=rf.score(X_train,y_train)*100\nprint(rf.score(X_train,y_train))\nprint(rf.score(X_test,y_test))","18f29af5":"model2.feature_importances_","077a1e9c":"sns.barplot(x=model2.feature_importances_,y=['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q',\n       'Embarked_S'])\nplt.title('Feature Importance Plot')\nplt.show()","25ff6fbc":"#Learning rate =0.01 got from the grid serach cv\nmodel3 = AdaBoostClassifier(base_estimator=model2,random_state=0,learning_rate=0.001)\nmodel3.fit(X_train,y_train)\nacc_ada_boost=model3.score(X_train,y_train)*100\nprint(model3.score(X_train,y_train))\nprint(model3.score(X_test,y_test))","113544f0":"#from the Grid Searchcv I have got the parameters as 36,1,1\nmodel4 =GradientBoostingClassifier(random_state=1,n_estimators=36,max_depth=1,learning_rate=1)\nmodel4.fit(X_train,y_train)\nacc_gradient_boost=model4.score(X_train,y_train)*100\nprint(model4.score(X_train,y_train))\nprint(model4.score(X_test,y_test))","f135400d":"# By gridsearch we have got the values for the hyperparameters\nmodel5 = SVC(kernel='rbf',C=10,gamma=0.1,random_state=1)\nmodel5.fit(X_train,y_train)\nacc_svc=model5.score(X_train,y_train)*100\nprint(model5.score(X_train,y_train))\nprint(model5.score(X_test,y_test))","d225a563":"model6 = VotingClassifier(estimators=[('DT',model),('LR',model1),('RF',model2),('AD',model3),('GB',model4),('SVC',model5)],\n                          voting='hard')\nmodel6.fit(X_train,y_train)\nacc_voting_classifier=model6.score(X_train,y_train)*100\nprint(model6.score(X_train,y_train))\nprint(model6.score(X_test,y_test))","57cd4a52":"from sklearn.ensemble import BaggingClassifier\nbc = BaggingClassifier(base_estimator=model2,n_estimators=20,random_state=1)\nbc.fit(X_train,y_train)\nacc_Bagging_classifier=bc.score(X_train,y_train)*100\nprint(bc.score(X_train,y_train))\nprint(bc.score(X_test,y_test))","d3ea66c0":"from sklearn.model_selection import GridSearchCV\n\nparameters = [{'learning_rate':[0.01,0.1,0.001,1,5,10,20]}]\ngrid_search = GridSearchCV(estimator = model3,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10)\n                           #n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n","26cb6087":"print('Best Accuracy',best_accuracy)\nprint('Best Parameters',best_parameters)               ","ae5bef6e":"from sklearn.model_selection import cross_val_score\nfor models  in [model,model1,model2,model3,model4,model5,model6,bc]:\n    \n    accuracies = cross_val_score(estimator = models, X = X_train, y = y_train, cv = 10)\n    print(models,accuracies.mean())\n    print(accuracies.std())","19927dd4":"models = pd.DataFrame({\n    'Model': ['Decision Tree','Logistic Regression','Random Forest','Adaboost Classifier','Gradient boost',\n             'Support Vector Classifier','Voting Classifier','Bagging Classifier'],\n    'Score': [acc_decision_tree,acc_logistic,acc_random_forest,acc_ada_boost,acc_gradient_boost,\n             acc_svc,acc_voting_classifier,acc_Bagging_classifier]})\nmodels.sort_values(by='Score', ascending=False)","239376c9":"test_1 = test.copy()\ntest_1['Age'] = test_1['Age'].fillna(test_1['Age'].median())\ntest_1['Fare'] = test_1['Fare'].fillna(test_1['Fare'].mean())\ntest_1.drop(columns=['Name','Ticket','PassengerId','Cabin'],inplace=True)","83da394b":"test_1.head()","d4a2ae5a":"test_1 = pd.get_dummies(data=test_1,columns=['Sex','Embarked'],drop_first=True)\ntest_1.head()","14dbb730":"y_pred = rf.predict(test_1)\ny_pred","24ecbc3a":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","570b7e10":"The people who were travelling alone has less survival rate when compared to the people travelling with the family","a0201dca":"As the percentage of missing values is High for the cabin we will remove that column","f7c8f56c":"As we can see from that graph that ratio of people who died were from Pclass 3 (Lower class).\nThe Pclass 1(Upper class) has more number of Survivals when compared to the other class.","fe0b8e99":"The childeren who travelled with their nanny's have high deaths when compared to the\nother parch.","c15e974b":"The people in the Plcass 3(Lower Class) age median is around 25\nThe people in the Plcass 2(Middle Class) age median is around 29\nThe people in the Plcass 1(Upper Class) age median is around 38\n\n","3a91c003":"Females rate of Survival is High when compared to the Males","c94ff4d4":"The passengers who were in the embarked S i.e Southampton their death rate is high when compared to the other Port of Embarkation."}}