{"cell_type":{"4ab4948b":"code","e1f58de7":"code","f61297de":"code","a20febde":"code","dc0f76ad":"code","9c160edc":"code","3e0a7c2d":"code","a12ae201":"code","d281ea7c":"code","af66bf01":"markdown","0396125d":"markdown","91d93363":"markdown","a8fd1947":"markdown","fd76d1f8":"markdown","01f85806":"markdown"},"source":{"4ab4948b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt","e1f58de7":"netflix_data = pd.read_csv(\"..\/input\/netflix-shows\/netflix_titles.csv\")","f61297de":"netflix_data.head(5)","a20febde":"#Finding the index of the column with any null value\nnetflix_data[netflix_data.isna().any(axis=1)]","dc0f76ad":"any_with_null = len(netflix_data[netflix_data.isna().any(axis=1)])\nnot_null = len(netflix_data['show_id']) - any_with_null\nnull_type = ['Rows_with_null', 'Rows_without_null']\nvalue = [any_with_null, not_null]\nplt.bar(null_type, value)\nplt.show()","9c160edc":"print(\"The length of the data is :\")\nprint(len(netflix_data['show_id']))\nprint(\"\\nNumber of Null Values under every category\")\nprint(netflix_data.isna().sum())","3e0a7c2d":"##installing and importing required libraries\n\n%pip install bs4\n%pip install lxml\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport lxml\nimport re\nimport urllib.parse\nheaders = {\"Accept-Language\": \"en-US, en;q=0.7\"}\n","a12ae201":"#extracting rows with NULL values\nwith_null = netflix_data[netflix_data.isna().any(axis=1)]\n\n#generating required variables for storing data for post analysis\nyrNotFound = []\ndrNotFound = []\ncastNotFound = []\nnotFound = []\ndrSeq = []\ncastSeq = []\nyrSeq = []\n\n#length of the dataframe \ntotal_rows = len(with_null.index)\n\nfor count in range(0, total_rows):\n    print(\" \")\n    print(count)\n\n\n    #extracting first row and its null field\n    try:\n        val = with_null.iloc[count].isnull()\n        missing = []\n        \n    except IndexError:\n        print(\"out of bounds\")\n        continue\n\n    # generating a ist of missing values for the column\n    for i in range(0, len(val.values)):\n        if val.values[i] == True:\n            missing.append(val.index[i])\n\n    if not missing:\n        continue\n        \n    #extracting required data\n    required_data = list(with_null.iloc[count][['type', 'title', 'release_year', 'duration']])\n\n    #url for extraction\n    if required_data[0] == \"Movie\":\n        movie = required_data[1].replace(\" \",\"\")\n        url = \"https:\/\/www.imdb.com\/find?q=\" + movie + \"&s=tt&ttype=ft&ref_=fn_ft\"\n\n\n    elif required_data[0] == \"TV Show\":\n        show = required_data[1].replace(\" \",\"+\")\n\n        #list of special characters\n        regex = re.compile('[@_!#$%^&*()<>?\/\\|}{~:]')\n        sp_match = regex.search(required_data[1])\n\n        #encoding special characters and replacing the code in URL\n        if sp_match != None:\n            encode = urllib.parse.quote_plus(sp_match.group(0))\n            show = show.replace(sp_match.group(0), encode)\n\n        url = \"https:\/\/www.imdb.com\/find?s=tt&q=\" + show + \"&ref_==nv_sr_sm\"\n\n\n\n    #scraping data from imdb\n    response = requests.get(url, headers = headers)\n    dsoup =  BeautifulSoup(response.text, 'lxml')\n\n    #extracting only the first two options from the search page\n    odd_mshow = [dsoup.find('tr', class_ = \"findResult odd\")]\n    eve_mshow = [dsoup.find('tr', class_ = \"findResult even\")]\n\n\n\n    def find_show(i):\n\n        try:\n            info = i.find('td', class_ = \"result_text\").text\n            year = info.split(\"(\")\n\n            if len(year) > 2:\n                year = year[-2].replace(')', \"\")\n                #print(year)\n\n            else:\n                year = year[-1].replace(')', \"\")\n                #print(year)\n                \n                \n        except AttributeError:\n            print(\"not found\")\n            yrSeq.append(count)\n            return -1\n        \n  \n        \n        try:\n            if required_data[0] == \"TV Show\":\n                \n                print(\"Entered TV\")\n                tv_year = int(required_data[3][0]) \n                print(\"Number of Seasons {0}\".format(tv_year))\n                print(\"release year {0}\".format(required_data[2]))\n                print(\"Year {0}\".format(year))\n                if int(required_data[2]) == int(year):\n                    return 1\n                    \n                    \n                else:\n            \n                    for i in range(0, tv_year + 1):\n                        year_nxt = int(year) + i\n                        print(\"year_nxt {0}\".format(year_nxt))\n                        year_prev = int(year) - i\n                        print(\"year_prev {0}\".format(year_prev))\n\n                        if int(required_data[2]) == year_nxt:\n                            return 1\n                            break\n\n                        elif int(required_data[2]) == year_prev:\n                            return 1\n                            break\n                            \n            elif int(year) == int(required_data[2]):\n                return 1\n                \n            else:\n                return 0\n                \n        except ValueError:\n            print(\"Year not found\")\n            return -1\n      \n\n    def cast_crew(i):\n        data_info = i.find('a').get('href')\n        data_url = \"https:\/\/www.imdb.com\" + data_info + \"?ref_=fn_ft_tt_1\"\n        cast_crew_url = \"https:\/\/www.imdb.com\" + data_info + \"fullcredits\/?ref_=tt_ql_cl\"\n        print(data_url)\n        return cast_crew_url, data_url\n\n\n    def find_director(url):\n        try:\n            response = requests.get(url, headers = headers)\n            dsoup =  BeautifulSoup(response.text, 'lxml')\n            ver_text = dsoup.find(\"h4\",attrs={\"class\":\"dataHeaderWithBorder\",\"id\":\"director\"}).text\n            print(ver_text)\n            name = dsoup.find('table', class_ = \"simpleTable simpleCreditsTable\").text\n            name = re.sub(r'[(].+?[)]', '', name)\n            name = name.split('...')\n            directors = ''\n            \n            for i in name:\n                i = i.strip()\n                directors = directors + \" \" + i + ','\n            print(directors)\n            \n        except AttributeError:\n            print(\"data not found\")\n            drNotFound.append(url)\n            drSeq.append(count)\n        return directors\n\n    def find_cast(url):\n        try:\n            response = requests.get(url, headers = headers)\n            dsoup =  BeautifulSoup(response.text, 'lxml')\n            ver_text = dsoup.find(\"h4\",attrs={\"class\":\"dataHeaderWithBorder\",\"id\":\"cast\"}).text\n            print(ver_text)\n            cast = dsoup.find('table', class_ = 'cast_list').text\n            cast = cast.split('...')\n            cast_list = \"\"\n        \n            for i in cast:\n                i = i.strip()\n                i = i.split(\"\\n\")\n                i = [j for j in i if j]\n\n                if re.search(r'[a-z]', i[-1]) == None:\n                    j = re.search(r'[a-z]', i[-1])\n                    continue\n                if re.search(r'[0-9]+', i[-1]) != None:\n                    continue\n                cast_list = cast_list + i[-1] + \", \"\n                \n            print(cast_list)\n                \n        except AttributeError:\n            print(\"data not found\")\n            castNotFound.append(url)\n            castSeq.append(count)\n            return \"NaN\"\n        \n      \n\n        return cast_list\n\n    def find_country(url):\n        response = requests.get(url, headers = headers)\n        dsoup =  BeautifulSoup(response.text, 'lxml')\n        country = dsoup.find('ul' ,class_ = \"ipc-metadata-list ipc-metadata-list--dividers-all ipc-metadata-list--base\").text\n        m_st = country.find(\"Country of origin\")\n        m_end = country.find(\"Official\")\n        st_index = m_st + 17 \n        country_name = country[st_index:m_end]\n        country_name = re.sub('.*[0-9]', \"\", country_name)\n        print(country_name)\n        return country_name\n\n    for i in odd_mshow:\n        odd = find_show(i)\n\n        if odd == 1:\n            cast_url = cast_crew(i)\n\n\n            missing_pos = ['director', 'cast', 'country']    \n            j = -1\n            print(missing)\n            for i in missing:\n                if i == missing_pos[0]:\n                    try:\n                        director = find_director(cast_url[0])\n                    except:\n                        print(\"data not found\")\n                        continue\n                        \n                    netflix_data['director'][count] = director\n\n\n                elif i == missing_pos[1]:\n                    cast = find_cast(cast_url[0])\n                    netflix_data['cast'][count] = cast\n\n\n                elif i == missing_pos[2]:\n                    country = find_country(cast_url[1])\n                    netflix_data['country'][count] = country\n\n\n        else:\n            for j in eve_mshow:\n                eve = find_show(j)\n\n                if eve == 1:\n                    cast_url = cast_crew(j)\n                else:\n                    print(\"Not among the first two choices\")\n                    notFound.append(count)\n    \n    \n       \n        \n    #match the year and adjust the year on the basis of number of seasons in the case of TV show        \n    #get the link\n    #check for what's missing \n    #Get the required data from cast crew page\n    #add it to the dataset\n    ","d281ea7c":"df = netflix_data\ndf.to_csv('data_set.csv',index=False)","af66bf01":"##### Data descrition and initail understanding derived from the data\n\n1. From listed in and description we can try to find out the genere or the type of story lines which are prefered directors    for TV shows and movies. \n2. If incase the same genere exists as movie and tv series then what sort of ratimg does it gather.\n3. Has the prefernce of directors changed over the years in terms of generes and content.\n4. Prefrence of content in various countries and factors affecting the same.","0396125d":"#### Saving the extracted data to the excel file","91d93363":"### Data Description","a8fd1947":"# Description\n\nThe **aim** is to find the missing data fields (director, country and cast) using web scraping.\nFor this I used webscraping library called **Beautiful Soup**.\n\nLogic used for the same is as follows:\n1. Extract details (title, release date, Type, duration) from the dataset. \n2. Use title and type to generate the correct URL for searching the TV show\/ Movie from the IMDB website. **eg:** https:\/\/www.imdb.com\/find?q=Zak+Storm&ref_=nv_sr_sm\n3. Extract the first two (or as many as you want) search results from IMDB search page\n4. Using release year (incase of movies) and release year and duration (incase of TV show) verify the movie and then scrape the the URL to that movie\/show. **eg:** https:\/\/www.imdb.com\/title\/tt4209752\/?ref_=fn_al_tt_1\n5. To extract country, we scrape the webpage's details section to find the country.\n6. Now Inorder to extract data regrading Director and Cast we generate another URL that directs us to the cast&crew page of that particlar movie\/show. **eg:** https:\/\/www.imdb.com\/title\/tt4209752\/fullcredits\/?ref_=tt_ql_cl\n7. The data is then copied to the orginal dataframe.\n\n\nOther libraries used are:\n\n* **pandas** for creating and manipulating data frames.\n* **matplotlib.pyplot** for creating graphs\n* **re** for string manipulation\n* **requests** to send HTTP requests using python\n* **lxml** for handeling XML and HTML files\n* **urllib.parse** for handeling URL\n\n","fd76d1f8":"### Identifying Null Values","01f85806":"### Web Scraping"}}