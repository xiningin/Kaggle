{"cell_type":{"d86ff52a":"code","b8a8d0e0":"code","d64c31f3":"code","7f29b1bf":"code","9c86ee04":"code","507a8410":"code","fc856fd5":"code","e6623761":"code","72583909":"markdown","7d02c393":"markdown","5b3cddb7":"markdown","5bbdc6a7":"markdown","b6704fb7":"markdown","8a14fe39":"markdown","2c74548c":"markdown","af5d65a6":"markdown","e1180cf8":"markdown","3936613c":"markdown"},"source":{"d86ff52a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport tensorflow as tf\nimport random\nimport os\n\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nSEED=42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n    \ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\n","b8a8d0e0":"from sklearn.preprocessing import MinMaxScaler\n\nlabel_map = {\n    'Class_1' : 0,\n    'Class_2' : 1,\n    'Class_3' : 2,\n    'Class_4' : 3,\n}\ntrain['target'] = train['target'].map(label_map)","d64c31f3":"features = ['feature_{}'.format(x) for x in range(50)]\nqt = train[features].quantile(np.arange(0,1,0.002))\n\ndef clip(df):\n    df = df.copy()\n    for feature in features:\n        df[feature] = df[feature].clip(lower=0, upper=qt.loc[0.998][feature])\n    return df\n","7f29b1bf":"values=[]\nlabels=[0,1,2,3,]\nfor feature in features:\n    grouped = clip(train).groupby(feature)\n    for value, group in grouped:\n        value=[feature, value]\n        for label in labels:\n            p =  (group['target'] == label).mean()\n            p = np.clip(p, 1e-06, 1 - 1e-06)\n            value.append(np.log(p+0.5))\n            value.append(np.log(p\/(1-p)))\n        values.append(value)\ndf_proba = pd.DataFrame(values,\n                        columns=['feature', 'value',\n                                 'Class_1_proba1',\n                                 'Class_1_proba2',\n                                 'Class_2_proba1',\n                                 'Class_2_proba2',\n                                 'Class_3_proba1',\n                                 'Class_3_proba2',\n                                 'Class_4_proba1',\n                                 'Class_4_proba2',\n                                ])\nproba_dict_1={}\nproba_dict_2={}\n\nfor i in range(len(df_proba)):\n    feature = df_proba.iloc[i]['feature']\n    value = df_proba.iloc[i]['value']\n    proba_dict_1[feature, value] = df_proba.iloc[i][['Class_1_proba1','Class_2_proba1','Class_3_proba1','Class_4_proba1',]].values.astype(float)\n    proba_dict_2[feature, value] = df_proba.iloc[i][['Class_1_proba2','Class_2_proba2','Class_3_proba2','Class_4_proba2',]].values.astype(float)\n    ","9c86ee04":"from sklearn.base import TransformerMixin\n\ndef reshape(df):\n    values=[]\n    for value in df.values:\n        values.append([_ for _ in value])\n    return np.array(values)\n\nclass MyTransformer1(TransformerMixin):\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n    def transform(self, X):\n        newX = pd.DataFrame()\n        for feature in features:\n            newX[feature] = X[feature].clip(lower=qt.loc[0.002][feature], upper=qt.loc[0.998][feature])\n            newX[feature] = 1 \/ (newX[feature] - newX[feature].min() + 1)\n        return newX\n\nclass MyTransformer2(TransformerMixin):\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n    def transform(self, X):\n        newX = pd.DataFrame()\n        X = clip(X)\n        for feature in features:\n            newX[feature] = X[feature].apply(lambda x:proba_dict_1[feature, x])\n        return reshape(newX).reshape((-1,200))\n\nclass MyTransformer3(MyTransformer2):\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n    def transform(self, X):\n        newX = pd.DataFrame()\n        X = clip(X)\n        for feature in features:\n            newX[feature] = X[feature].apply(lambda x:proba_dict_1[feature, x])\n        return reshape(newX)\n\ndef normalize(df, columns):\n    \"\"\"\n    sklearn.preprocessing.MinMaxScaler\n    \"\"\"\n\n    for column in columns:\n        min_val, max_val = df[column].agg([min,max])\n        df[column] = (df[column] - min_val) \/ (max_val - min_val)\n    return df\n\nclass MyTransformer4(TransformerMixin):\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n    def transform(self, X):\n        newX = pd.DataFrame()\n        for feature in features:\n            newX[feature] = X[feature].clip(lower=0).apply(lambda x:1\/(x+1))\n        return normalize(newX, features)\n\nclass MyTransformer5(TransformerMixin):\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n    def transform(self, X):\n        newX = pd.DataFrame()\n        for feature in features:\n            newX[feature] = X[feature].clip(lower=qt.loc[0.002][feature], upper=qt.loc[0.998][feature])\n        return newX\n","507a8410":"from sklearn.base import ClassifierMixin\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\nmy_model1 = CatBoostClassifier(\n    iterations=887,\n    min_child_samples=200,\n    random_state=SEED,\n    max_depth=6,\n    verbose=0)\n\nmy_model2 = CatBoostClassifier(\n    iterations=160,\n    min_child_samples=200,\n    max_depth=2,\n    eval_metric='MultiClass',\n    random_state=SEED,\n    verbose=0)\n\ninitial_learning_rate = 0.001\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=10000,\n    decay_rate=0.96,\n    staircase=True)\noptimizer = tf.keras.optimizers.Adam(\n    learning_rate=lr_schedule, \n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name='Adam'\n)\nclass TensorflowClassifier(ClassifierMixin):\n    def __init__(self):\n        self.histories=[]\n        self.classes_ = [0,1,2,3]\n        self.model = tf.keras.Sequential([\n            tf.keras.layers.Flatten(input_shape=(50,4)),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.Dense(32, activation='relu'),\n            tf.keras.layers.Dense(4, activation='softmax')\n        ])\n        self.model.compile(\n            optimizer=optimizer, \n            loss='sparse_categorical_crossentropy',\n            metrics=['sparse_categorical_crossentropy',])\n    def get_params(self,deep):\n        return {}\n    def fit(self, X, y):\n        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n        history = self.model.fit(X, y, epochs=300, batch_size = 200, validation_split=0.1, callbacks=[callback],verbose=0)\n        self.histories.append(history)\n        return self\n    def predict_proba(self, X):\n        return self.model.predict(X).reshape((-1, 4))\n\nmy_model3 = TensorflowClassifier()\n\nmy_model4 = LGBMClassifier(\n    random_state=SEED,\n    min_child_samples=150,\n    n_estimators=74,\n)\n\nmy_model5 = RandomForestClassifier(\n    random_state=SEED,\n    min_samples_leaf=200,\n    n_estimators=200,\n)\n\npipeline1 = make_pipeline(MyTransformer1(),my_model1)\npipeline2 = make_pipeline(MyTransformer2(),my_model2)\npipeline3 = make_pipeline(MyTransformer3(),my_model3)\npipeline4 = make_pipeline(MyTransformer4(),my_model4)\npipeline5 = make_pipeline(MyTransformer5(),my_model5)\n\nmy_final_estimator = LogisticRegression(\n    random_state=SEED,\n    max_iter=2000,\n)\n","fc856fd5":"from sklearn.ensemble import VotingClassifier,StackingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss,accuracy_score\n\nvoting_estimators = [\n    ('mod1', pipeline1),\n    ('mod2', pipeline2),\n    ('mod3', pipeline3),\n    ('mod4', pipeline4),\n]\nstacking_estimators = [\n    ('mod1', pipeline1),\n    ('mod2', pipeline2),\n    ('mod3', pipeline3),\n    ('mod4', pipeline4),\n    ('mod5', pipeline5),\n]\n\nX = train[features]\ny = train['target']\n\nmod_vot = VotingClassifier(\n    estimators=voting_estimators,\n    voting = 'soft',\n).fit(X, y)\n\nmod_stk = StackingClassifier(\n    estimators=stacking_estimators,\n    final_estimator=my_final_estimator,\n    stack_method='predict_proba',\n    cv=4,\n).fit(X, y)\n\n","e6623761":"y_pred_vot  = mod_vot.predict_proba(test[features])\ny_pred_stk  = mod_stk.predict_proba(test[features])\n\ny_pred_test = (y_pred_vot + y_pred_stk) \/ 2\n\nsubmission = test[['id']].copy()\nsubmission['Class_1'] = y_pred_test[:,0]\nsubmission['Class_2'] = y_pred_test[:,1]\nsubmission['Class_3'] = y_pred_test[:,2]\nsubmission['Class_4'] = y_pred_test[:,3]\nsubmission.to_csv('submission.csv', index=False)","72583909":"## 5. train VotingClassifier, StackingClassifier","7d02c393":"### 2.2. clip datasets.","5b3cddb7":"# Voting + Stacking ensemble","5bbdc6a7":"## 3. define my transformer","b6704fb7":"## 2. preprocessing datasets.","8a14fe39":"## 1. read datasets.","2c74548c":"### 2.1. convert 'target' value to 0,1,2,3. ","af5d65a6":"## 4. define my classifier","e1180cf8":"## 6. create my submission file","3936613c":"### 2.3. prepare feature's value based on the probability of target's rate. "}}