{"cell_type":{"3246d5a8":"code","a9d4af0e":"code","89ed728d":"code","7a7937f8":"code","c40fa524":"code","8a0bdfa9":"code","df309b79":"code","0ccd6e74":"code","0956ad6a":"code","93590fdc":"code","732962dd":"code","09adce61":"code","a777983c":"code","d0019818":"code","d361b7d6":"code","04a580d4":"code","02261270":"code","7d1d367e":"code","528b4c70":"code","8a44f79d":"code","439db23c":"code","34f2b2ce":"code","216df38d":"code","5c173c98":"code","2a834883":"code","34a39807":"code","b3bfbc33":"code","a9bc4d24":"code","e631ef21":"code","3200c616":"code","94807028":"code","c95bdb8e":"markdown","61bb7927":"markdown","e173e74a":"markdown","8245a882":"markdown","52931cd5":"markdown","0dcc275f":"markdown","60bfc1dc":"markdown","601c68c4":"markdown","687ade9f":"markdown","2d32c81a":"markdown","4f7032a4":"markdown","83b906ed":"markdown","3a6a562c":"markdown","a86a46a4":"markdown","c7b596e2":"markdown","10e2fb07":"markdown","99ace369":"markdown","1f97f15c":"markdown","d90f2808":"markdown","10454faf":"markdown","572af586":"markdown","8e2618a2":"markdown"},"source":{"3246d5a8":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom keras.layers import Dense, BatchNormalization, Dropout, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\nfrom keras import callbacks\nfrom keras.optimizers import Adam\n\nnp.random.seed(0)","a9d4af0e":"data = ('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\n\ndf = pd.read_csv(data)","89ed728d":"df.shape","7a7937f8":"df.head()","c40fa524":"col_names = df.columns\n\ncol_names","8a0bdfa9":"df.info()","df309b79":"df.describe()","0ccd6e74":"df['RainTomorrow'].isnull().sum()","0956ad6a":"df['RainTomorrow'].nunique()","93590fdc":"df['RainTomorrow'].unique()","732962dd":"q = sns.countplot(x = df['RainTomorrow'], palette = 'crest')\nq.set(xlabel = 'Value')\nq.set(ylabel = 'Count')\nq.set(title = 'Count of each unique value')\nplt.tight_layout()","09adce61":"# Parsing datetime\n# exploring the length of date objects\nlengths = df[\"Date\"].str.len()\nlengths.value_counts()","a777983c":"# As far as we can see from the value above, there are no errors. Now we can parse into datetime\ndf['Date']= pd.to_datetime(df['Date'])\n# Creating a 'Year' column\ndf['year'] = df.Date.dt.year\n\n# Now we will create a function to encode datetime into cyclic parameters.\n# This data will be used in a neural network, therefore having months and days in a cyclic continuous feature will make things easier for us.\n\ndef encode(df, col, max_val):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]\/max_val)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]\/max_val)\n    return df\n\ndf['month'] = df.Date.dt.month\ndf = encode(df, 'month', 12)\n\ndf['day'] = df.Date.dt.day\ndf = encode(df, 'day', 31)\n\ndf.head()\n","d0019818":"# View list of categorical variables\ns = (df.dtypes == \"object\")\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","d361b7d6":"# Missing values in categorical variables\n\nfor i in object_cols:\n    print(i, df[i].isnull().sum())","04a580d4":"# Filling missing values with the mode\n\nfor i in object_cols:\n    df[i].fillna(df[i].mode()[0], inplace=True)","02261270":"# View list of numerical variables\nt = (df.dtypes == 'float64')\nnum_cols = list(t[t].index)\n\nprint('Numerical Variables:')\nprint(num_cols)","7d1d367e":"# Amount of missing values\n\nfor i in num_cols:\n    print(i, df[i].isnull().sum())","528b4c70":"# Filling missing values with the median of the column value\n\nfor i in num_cols:\n    df[i].fillna(df[i].median(), inplace=True)\n    \ndf.info()","8a44f79d":"# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor i in object_cols:\n    df[i] = label_encoder.fit_transform(df[i])\n    \ndf.info()","439db23c":"# Preparing attributes of scale data\n# Dropping extra columns\nfeatures = df.drop(['RainTomorrow', 'Date','day', 'month'], axis=1)\n\n# Defining our target columns\n\ntarget = df['RainTomorrow']\n\n# Set up a standard scaler for the features\ncol_names = list(features.columns)\ns_scaler = preprocessing.StandardScaler()\nfeatures = s_scaler.fit_transform(features)\nfeatures = pd.DataFrame(features, columns=col_names)\n\nfeatures.describe().T","34f2b2ce":"# Detecting outliers in the data by looking at the scaled features\n\nplt.figure(figsize=(20,10))\nsns.boxenplot(data = features,palette = 'pastel')\nplt.xticks(rotation=90)\nplt.show()","216df38d":"features['RainTomorrow'] = target\n\n# Dropping outliers\n\nfeatures = features[(features[\"MinTemp\"]<2.3)&(features[\"MinTemp\"]>-2.3)]\nfeatures = features[(features[\"MaxTemp\"]<2.3)&(features[\"MaxTemp\"]>-2)]\nfeatures = features[(features[\"Rainfall\"]<4.5)]\nfeatures = features[(features[\"Evaporation\"]<2.8)]\nfeatures = features[(features[\"Sunshine\"]<2.1)]\nfeatures = features[(features[\"WindGustSpeed\"]<4)&(features[\"WindGustSpeed\"]>-4)]\nfeatures = features[(features[\"WindSpeed9am\"]<4)]\nfeatures = features[(features[\"WindSpeed3pm\"]<2.5)]\nfeatures = features[(features[\"Humidity9am\"]>-3)]\nfeatures = features[(features[\"Humidity3pm\"]>-2.2)]\nfeatures = features[(features[\"Pressure9am\"]< 2)&(features[\"Pressure9am\"]>-2.7)]\nfeatures = features[(features[\"Pressure3pm\"]< 2)&(features[\"Pressure3pm\"]>-2.7)]\nfeatures = features[(features[\"Cloud9am\"]<1.8)]\nfeatures = features[(features[\"Cloud3pm\"]<2)]\nfeatures = features[(features[\"Temp9am\"]<2.3)&(features[\"Temp9am\"]>-2)]\nfeatures = features[(features[\"Temp3pm\"]<2.3)&(features[\"Temp3pm\"]>-2)]\n\nfeatures.shape","5c173c98":"# Looking at the scaled features without outliers\n\nplt.figure(figsize = (20,10))\nsns.boxenplot(data = features, palette = 'pastel')\nplt.xticks(rotation = 90)\nplt.show()","2a834883":"X = features.drop(['RainTomorrow'], axis=1)\ny = features[\"RainTomorrow\"]\n\n# Splitting the test and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nX.shape","34a39807":"# Early stopping\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, #minimum amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\n# Initialising the NN\nmodel = Sequential()\n\n# Layers\n\nmodel.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = 26))\nmodel.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nopt = Adam(learning_rate=0.00009)\nmodel.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Train the ANN\n\nhistory = model.fit(X_train, y_train, batch_size = 32, epochs = 150, callbacks=[early_stopping], validation_split=0.2)","b3bfbc33":"history_df = pd.DataFrame(history.history)\n\nplt.plot(history_df.loc[:, ['loss']], \"#BDE2E2\", label='Training loss')\nplt.plot(history_df.loc[:, ['val_loss']],\"#C2C4E2\", label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc=\"best\")\n\nplt.show()","a9bc4d24":"history_df = pd.DataFrame(history.history)\n\nplt.plot(history_df.loc[:, ['accuracy']], \"#BDE2E2\", label='Training accuracy')\nplt.plot(history_df.loc[:, ['val_accuracy']], \"#C2C4E2\", label='Validation accuracy')\n\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","e631ef21":"# Predicting the test set results\ny_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)","3200c616":"# confusion matrix\ncmap1 = sns.diverging_palette(260,-10,s=50,l=75,n=5, as_cmap=True)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test,y_pred)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), cmap = cmap1, annot = True, annot_kws = {'size':15})","94807028":"print(classification_report(y_test, y_pred))","c95bdb8e":"**Label encoding the categorical variable**","61bb7927":"# Australia Rain Prediction using a Neural Network Model","e173e74a":"## Conclusions","8245a882":"**Numerical variables**\n\nFilling missing numerical values with the median of the column value","52931cd5":"Check for, and then view unique values","0dcc275f":"**Concluding the model with:**\n- Testing on the test set\n- Evaluating the confusion matrix\n- Evaluating the classification report","60bfc1dc":"#### 6. Data Visualisation and Cleanup\n\n\n**Now I will parse Dates into datetime**","601c68c4":"#### 5. Univariate Analysis\n\n##### Explore 'RainTomorrow' variable\n\nCheck for missing values","687ade9f":"The two unique values are 'No' and 'Yes'.\n\nLet's visualise this.","2d32c81a":"Next, I will deal with missing values in categorical and numeric attributes separately.","4f7032a4":"### 2. Import Libraries","83b906ed":"##### Things to note:\n- We can see that the dataset contains a mixture of categorical and numerical variables.\n- Categorical variables are type 'object'\n- Numerical variables are type 'float64'\n- There are quite a lot of missing values in the dataset.","3a6a562c":"We're now looking at a consistent dataframe which is perfect for building a neural network.","a86a46a4":"**Method of approach** \n- Assigning X and y the status of 'Attributes' and 'Tags'\n- Splitting test and training sets\n- Initialising the neural network\n- Defining by adding layers to the network\n- Compiling the neural network\n- Training the neural network","c7b596e2":"## Model Building","10e2fb07":"### 3. Import Dataset","99ace369":"**Categorical Variables**\n\nWe're going to fill missing values with the mode of the column value","1f97f15c":"In this notebook, I will create a classification model using an Artificial Neural Network (ANN) to determine whether or not it will rain tomorrow in Australia.\n\nI've used the \"Rain in Australia\" dataset for this project.\n\n","d90f2808":"### 1. What do we want to achieve?","10454faf":"#### Data Preprocessing\n**Steps involved:**\n- Label encoding columns with categorical data\n- Perform the scaling of the features\n- Detecting outliers\n- Dropping the outliers based on data analysis","572af586":"#### Finding out the general format of the data","8e2618a2":"### 4. EDA (Exploratory Data Analysis)"}}