{"cell_type":{"6ecfee65":"code","9f02ddfd":"code","b33e7249":"code","7bc52f65":"code","586504c0":"code","405de9df":"code","6ed8e6a5":"code","5dbb15b0":"code","e27524f9":"code","98d95b4c":"code","0a4a6f1f":"code","3b10fecb":"code","497e2ba7":"code","4b4b289a":"code","174c89c6":"code","9502c9ca":"code","dd573b82":"code","1375dac9":"code","c611dd15":"code","beb2b088":"code","661b5dd5":"code","979dbc2f":"code","22709cd0":"code","d1b239e8":"code","712ab67e":"code","b539386f":"code","7387077c":"code","7df95298":"code","cf1fa699":"code","fa04f68f":"code","0875ad8a":"code","55506e3d":"code","6d2af93e":"code","6e3f1055":"code","7e4cb5e9":"code","9e666f66":"code","dad074d5":"code","65308952":"code","3dc825a6":"code","0e38cdf3":"code","010391aa":"code","9473b67c":"code","b8b7e7a4":"code","da457aeb":"code","a1f334c8":"code","41806951":"code","e653f8a4":"code","f131ca9e":"code","c1bb13fb":"code","3c0ca60c":"code","242916b7":"code","dc5ec6c9":"code","8c4b6fdb":"code","b7f7c7a8":"code","79bdd724":"code","259d61d9":"code","62b7f256":"code","0ec32037":"code","2be83444":"code","652a0dc6":"code","5e9aa6f3":"code","8b6c0ea0":"code","4b247f4d":"code","55de6a82":"code","055aafd3":"code","9ca2ba84":"code","6544f1aa":"code","014ba3b7":"code","4952d2de":"code","7a05854e":"code","5550c1cb":"code","51b03f4d":"code","f7011f12":"code","39c910bd":"code","fbd8bc1a":"code","e359cc8f":"code","6edc4eb5":"code","d8512216":"code","7f79e528":"code","5658d32c":"code","206b1dd8":"code","95dafbf1":"code","9c70063c":"code","d48dd732":"code","920462db":"code","6a188901":"code","1e9c62dc":"code","f51ff385":"code","0415bfe9":"code","0f2061c7":"code","e0ab7877":"code","8535cf4f":"code","9e3c4aed":"code","e3042b03":"code","5a22e471":"code","9b391555":"code","2fb43232":"code","9e1c7748":"code","45def8f7":"code","b9f25cd9":"code","820a3ac2":"code","37b3d4bc":"code","52597b3a":"code","3bfce984":"code","3833024c":"code","4fa99169":"code","ec267a86":"code","6347e983":"code","131cfe32":"code","a7cc97e7":"code","eb9e6a0d":"code","528ccbe7":"code","b717d91b":"code","7b1afe29":"code","8e2a3bc7":"code","392130eb":"code","0b5c99d7":"code","ccffdaf0":"code","2ee435d3":"code","f3b72920":"code","beb74b94":"markdown","6f955c2a":"markdown","52fab749":"markdown","db3c6876":"markdown","7e05657c":"markdown","64263c48":"markdown","acc6c7e8":"markdown","d2726211":"markdown","47e0e22a":"markdown","403882c5":"markdown","0d8bfa9a":"markdown","c54efd93":"markdown","159c7cc0":"markdown","54ee41fa":"markdown","a35f4b54":"markdown","daeb8198":"markdown","ef3f75ed":"markdown","1313e4b0":"markdown","f1fd3426":"markdown","1a5e69d5":"markdown","0e3c6d70":"markdown","6b1b0e25":"markdown","60ebeeac":"markdown","7facb4bd":"markdown","190c59e5":"markdown","39787cae":"markdown","0f951e3a":"markdown","8efd8886":"markdown","8abe5b12":"markdown","8e8851c6":"markdown","15930a91":"markdown","6e634150":"markdown","fb7b1f0f":"markdown","5b6b481e":"markdown","6483c880":"markdown","2eb1c023":"markdown","b0ca4500":"markdown","2762d99a":"markdown","e513361e":"markdown","7b1e205c":"markdown","ecab5dd8":"markdown","04d7eaba":"markdown","89028eab":"markdown","739e6caf":"markdown","e08c89f1":"markdown","883bc4a1":"markdown","87d80f77":"markdown","5b3f0fd9":"markdown","3c04f876":"markdown","d86ef9c2":"markdown","5c4d70e1":"markdown","c4e061e8":"markdown","d8ebe6b2":"markdown","d4932d70":"markdown","6f08ec3f":"markdown","2579f61c":"markdown"},"source":{"6ecfee65":"import os;\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename));","9f02ddfd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nimport pylab \nimport scipy.stats as stats\nimport statsmodels.api as sm\n\nfrom numpy import mean\nfrom numpy import median\nfrom numpy import percentile\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\n\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom pandas import DataFrame\n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score","b33e7249":"df = pd.read_csv('\/kaggle\/input\/daily_values_ee.csv')","7bc52f65":"df.info()","586504c0":"df.head(3)","405de9df":"print('Size of df data', df.shape)","6ed8e6a5":"plt.figure(figsize=(16,4))\nsns.heatmap(df.isnull()) # plot the missing data\nplt.title('Missing Data')\n\ntotal = df.isnull().sum().sort_values(ascending = False) # count list of missing data per column\npercent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False) # percentage list of missing data per column\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) # create a df of the missing data\nmissing_data.head(6)","5dbb15b0":"df=df.tail(593) # this way we will exclude the first 7 rows\ndf.head()","e27524f9":"## Function to reduce the DF size\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndf = reduce_mem_usage(df);","98d95b4c":"plt.figure(figsize=(16,6))\nsns.distplot(df['53_kW_mean'].dropna(), bins=40, label='53')\nsns.distplot(df[df['71_kW_mean']!=0]['71_kW_mean'].dropna(), bins=20, label='71')\nsns.distplot(df['71A_kW_mean'].dropna(), bins=40, label='71A')\nsns.distplot(df['83_kW_mean'].dropna(), bins=40, label='83')\nplt.legend()\nplt.xlim(0,300)\nplt.xlabel(\"kW\")\nplt.title('Distribution + kde - Substations (kW)', fontsize=20);\nplt.ylabel('p.u');\n\nplt.figure(figsize=(16,6))\nsns.distplot(df['totalkW_mean'].dropna(), bins=40, color='orange');\nplt.title('Distribution + kde - Total SEs (kW)', fontsize=20);\nplt.xlabel(\"kW\")\nplt.ylabel('p.u');","0a4a6f1f":"plt.figure(figsize=[16,12])\n\nplt.subplot(321)\nplt.plot(df['53_kW_mean'])\nplt.ylabel('kW ')\nplt.title('53', fontsize=20)\n\nplt.subplot(322)\nplt.plot(df['71_kW_mean'], color='orange')\nplt.ylabel('kW')\nplt.title('71', fontsize=20)\n\nplt.subplot(323)\nplt.plot(df['71A_kW_mean'], color='orange')\nplt.ylabel('kW')\nplt.title('71A', fontsize=20)\n\nplt.subplot(324)\nplt.plot(df['83_kW_mean'])\nplt.ylabel('kW')\nplt.title('83', fontsize=20)\n\nplt.subplot(325)\nplt.plot(df['71_71AkW_mean'])\nplt.ylabel('kW')\nplt.title('71 + 71A', fontsize=20)\n\nplt.subplot(326)\nplt.plot(df['totalkW_mean'], color='orange')\nplt.ylabel('kW')\nplt.title('Total', fontsize=20);\n\n","3b10fecb":"plt.figure(figsize=(12,5))\n#plt.title('Correlations - Total kW');\nplt.plot(df['totalkW_mean'], color='black')\nplt.ylabel('kW', fontsize=14)\nplt.xlabel('dia', fontsize=14)\n\n#plt.title('Total', fontsize=20)\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14);","497e2ba7":"# Since we will work initially with the total values, let's filter it\ntotal_cols = ['Date','totalkW_mean','totalkW_max','totalkW_time_max','totalkW_d-1','totalkW_w-1','totalkW_d\/1','totalkW_w\/1','Month','Year','Day','WD','CDD_15','HDD_25','NWD','temp_max','insolation','temp_mean','RH']\ntotal_df = df[total_cols]\ntotal_df.info() # review the variables","4b4b289a":"corr = total_df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 12))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap='viridis', vmax=1, center=0,\n            square=False, linewidths=1, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.title('Correlations', fontsize = 20)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12);","174c89c6":"# focusing on the dependent variable\nplt.figure(figsize=(20,1))\nsns.heatmap(total_df.corr()[-18:1], mask=False, annot=True, cmap='viridis')\n#plt.title('Correlations - Total kW');\nplt.xticks(fontsize = 18)\nplt.yticks(fontsize = 18);","9502c9ca":"total_df2=total_df","dd573b82":"total_df2=total_df2.drop(['Year'], axis=1)","1375dac9":"plt.figure(figsize=(22,1))\nsns.heatmap(total_df2.corr()[-18:1], mask=False, annot=True, cmap='viridis')","c611dd15":"plt.figure(figsize=(14,6))\nplt.plot(total_df['HDD_25'], color='#CD661D', label='HDD')\nplt.plot(total_df['CDD_15'], color='#3D59AB', label='CDD')\nplt.plot(total_df['temp_mean'], color='black', label='Mean Temperature')\nplt.legend()\nplt.xlabel('day')\nplt.title('Weather Data - HDD, CDD and Mean Temperature (\u00baC)', fontsize = 20);","beb2b088":"# here we will plot the rolling mean with a 7 window day period to analyze the weekly frequency\nplt.figure(figsize=(16,6))\nplt.plot(total_df['totalkW_mean'].rolling(window=7,center=False).mean(),label='Rolling Mean') # analyze the rolling mean considering a 7 days window\nplt.plot(total_df['totalkW_mean'].rolling(window=7,center=False).std(),label='Rolling Std Dev')# analyze the rolling std dev considering a 7 days window\nplt.legend()\nplt.xlabel('day')\nplt.title('7 day window - Mean & Std. Deviation (kW)', fontsize=20);","661b5dd5":"N, M = 12, 5\nfig, ax = plt.subplots(figsize=(N, M))\ncorr_plot = plot_acf(total_df['totalkW_mean'].dropna(), lags=26, ax=ax)\n#plt.show()\n#plt.title('Autocorrelation', fontsize=20)\nplt.xlabel('Atraso (dias)', fontsize=14)\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14);","979dbc2f":"Consumo_WD_M = total_df[['WD','Month','totalkW_mean']].groupby(by=['WD','Month']).mean()['totalkW_mean'].unstack()\nplt.figure(figsize=(12,5))\n#plt.figure(fontsize=14)\nplt.rcParams.update({'font.size': 15})\n\nsns.heatmap(Consumo_WD_M.astype(int),cmap='viridis', annot=True, fmt='g')\n#plt.title(\"Mean (kW)\", fontsize=20);\nplt.xlabel('m\u00eas', fontsize=14)\nplt.ylabel('dia da semana', fontsize=14)\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14);","22709cd0":"plt.figure(figsize=(12,5))\nsns.boxplot(x='WD', y='totalkW_mean', data=total_df, color='#3D59AB')\nplt.title(\"Day of the Week Mean (kW)\", fontsize=20);\nplt.ylabel('m\u00eas', fontsize=14)\nplt.xlabel('dia da semaa', fontsize=14)\n\nplt.figure(figsize=(12,5))\nsns.boxplot(x='Month', y='totalkW_mean', data=total_df, color='#CD661D')\nplt.title(\"Monthly Mean (kW)\", fontsize=20);","d1b239e8":"qweqweqwe=total_df[['WD','Month','totalkW_mean']].groupby(by=['WD','Month']).mean()['totalkW_mean']\n","712ab67e":"qweqweqwe","b539386f":"qweqweqwe[qweqweqwe['WD']==1]","7387077c":"total_df[['WD','totalkW_mean']].groupby(by=['WD']).mean()['totalkW_mean']","7df95298":" # this plot shows the time during the day where it has the greatest power\ncount=total_df[['WD','totalkW_time_max','totalkW_max']].groupby(by=['WD','totalkW_time_max']).count()['totalkW_max'].unstack()\ncount[np.isnan(count)] = 0\nplt.figure(figsize=(13,5))\nsns.heatmap(count.dropna(), cmap='viridis')\nplt.title(\"Count of maximum power per hour\", fontsize=20);","cf1fa699":"total = total_df['totalkW_time_max'].value_counts()\ncount  = pd.concat([total], axis=1, keys=['Count'])\ncount.reset_index(drop=False, inplace=True)\ncount.head(8)\ncount.sort_values(by=['index'], inplace=True, ascending=True)\nplt.figure(figsize=(14,4))\nsns.barplot(x='index', y='Count', data=count, color='#3D59AB')\nplt.title(\"Count of maximum power per hour\", fontsize=20);\nplt.xlabel('totalkW_time_max');\n\n#count.sort_values(by=['Count'], inplace=True, ascending=False)\n#plt.figure(figsize=(14,4))\n#sns.barplot(x='index', y='Count', data=count, color='#3D59AB')\n#plt.xlabel('totalkW_time_max')\n#plt.title(\"Count of maximum power per hour - descendant\", fontsize=20);","fa04f68f":"plt.figure(figsize=(12,4))\nsns.heatmap(total_df.groupby(by='WD')['totalkW_mean'].describe().astype(int), cmap='viridis', annot=True , fmt='g');\nplt.title('TotalkW_mean description', fontsize=20);","0875ad8a":"total_df.groupby(by='WD')['totalkW_mean'].describe()","55506e3d":"total_df['is_weekend'] = np.where(total_df['WD'].isin(['5','6']),1,0);","6d2af93e":"plt.figure(figsize=(10,4))\nsns.distplot(total_df['totalkW_mean'].loc[(total_df['is_weekend'] == 0)])\nsns.distplot(total_df['totalkW_mean'].loc[(total_df['is_weekend'] == 1)])\nplt.ylabel('p.u.')\nplt.title('Distribution - Weekends and weekdays', fontsize=20);","6e3f1055":"# X is the independent variable - aka INPUT\n# y is the dependent variable - aka OUTPUT\n\nX = total_df.drop(['Date','Year','Day','totalkW_mean','totalkW_max','totalkW_time_max','insolation','RH'], axis=1).values\ny = total_df['totalkW_mean'].values\n\n# the model only works with arrays, not DF, that's why the \".values\" on the code","7e4cb5e9":"# splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n# since there are not too many data, I thought it was better to have a bigger training set,    not the usual 2\/3.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# normalizing it\nscaler = MinMaxScaler()\nscaler.fit(X_train) # scale the X_train\nX_train = scaler.transform(X_train)  # apply for the train data\nX_test = scaler.transform(X_test) # apply for the test data\n","9e666f66":"X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1])) # need to add a column for using LSTM Models\nX_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1])) # need to add a column for using LSTM Models\n\nprint('X_train shape = ',X_train.shape)\nprint('X_test shape = ' ,X_test.shape)","dad074d5":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.optimizers import Adam\n\n\n# Here I create a Long Short-Term Memory artificial recurrent neural network as a model\nmodel_ann = Sequential()\nmodel_ann.add(LSTM( units=4, input_shape=(1, 12), activation='sigmoid', return_sequences=True))\n# first layer receiving the input. Sigmoid was choosing as an activation funcion in order to work properly for LSTM\n\nmodel_ann.add(LSTM( units=2, input_shape=(1, 12), activation='linear',recurrent_activation='linear', return_sequences=False))\n# the second layer work with a linear activation function to behave more in tune with our input data\n\nmodel_ann.add(Dense(units=1,activation='linear'))\n# finally a last layer with only one output, which is the respective mean power of the day\n\nmodel_ann.compile(loss='mse', optimizer='adam')\n# for the optimization model, the Minimum Squared Error is minimized as the objective function\n","65308952":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=250)\n# added an early stop that is trigger when the difference of the validation loss at each epoch is minimum.","3dc825a6":"model_ann.fit(x = X_train, \n          y = y_train, \n          epochs = 8000, \n          validation_data = (X_test, y_test), \n          batch_size = 128,\n          callbacks=[early_stop],\n          verbose=0\n         );\n\n# 8000 epochs to train the model\n# 128 cases for updating the model parameters, which is the batch size","0e38cdf3":"model_loss = pd.DataFrame(model_ann.history.history)\nprint(model_loss.tail(1))\nplt.figure(figsize=(12,5))\nplt.plot(model_loss['loss'], label = 'Perdas de Treinamento')\nplt.plot(model_loss['val_loss'], label = 'Perdas de Valida\u00e7\u00e3o')\nplt.legend()\nplt.ylabel(\"MSE\")\nplt.xlabel(\"\u00e9pocas\")\n#plt.title(\"Losses - Training & Validation\", fontsize=20);","010391aa":"pd.options.display.float_format = \"{:.2f}\".format\n\npredictions = model_ann.predict(X_test)\n\ny_test_std_dev=y_test.astype(float).std()\nprint(\"Real Mean = %.2f\" % total_df['totalkW_mean'].astype(float).mean())\nprint(\"y_test Mean = %.2f\" % y_test.astype(float).mean())\nprint(\"y_test Std Dev = %.2f\" % y_test.astype(float).std())\n\nalpha = 5.0\nlower_p = alpha \/ 2.0 # calculate lower percentile (e.g. 2.5)\nlower = max(0.0, percentile(y_test, lower_p)) # retrieve observation at lower percentile\n#print('y_test %.1fth percentile = %.2f' % (lower_p, lower)) # calculate upper percentile (e.g. 97.5)\nupper_p = (100 - alpha) + (alpha \/ 2.0) # retrieve observation at upper percentile\nupper = max(1.0, percentile(y_test, upper_p))\n#print('y_test %.1fth percentile = %.2f' % (upper_p, upper))\nprint(\"Predictions Mean = %.2f\" % predictions.astype(float).mean())\nprint(\"Predictions Std Dev = %.2f\" % predictions.astype(float).std())\n\nmodel_1_MAE = mean_absolute_error(y_test,predictions)\nmodel_1_RMSE = np.sqrt(mean_squared_error(y_test,predictions))\nmodel_1_EVS = explained_variance_score(y_test,predictions)\n\nprint(\"MAE = %.2f\" % model_1_MAE)\nprint(\"RMSE = %.2f\" % model_1_RMSE)\nprint(\"EVS = %.2f\" % model_1_EVS)","9473b67c":"explained_variance_score(y_test,predictions)","b8b7e7a4":"np.corrcoef(y_test,predictions)","da457aeb":"q1=pd.Series(y_test)","a1f334c8":"q1=np.array(q1)","41806951":"q2=np.array(predictions).transpose()","e653f8a4":"q1","f131ca9e":"np.array(predictions)","c1bb13fb":"np.corrcoef(q1,q2)","3c0ca60c":"plt.figure(figsize=(12,5))\nplt.scatter(y_test,predictions) # Our predictions\n\nplt.plot(y_test,y_test, 'orange') # y=x line\nplt.plot(y_test,y_test+y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.plot(y_test,y_test-y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.xlabel(\"Demanda di\u00e1ria de valida\u00e7\u00e3o (kW)\")\nplt.ylabel(\"Demanda di\u00e1ria de predi\u00e7\u00e3o (kW)\")\nplt.title(\"Scatter -  test vs. Predictions (kW)\", fontsize=20);\n\nplt.figure(figsize=(12,5)) # Plot predictions and the real values together\nsns.distplot(y_test, label='Valida\u00e7\u00e3o', bins=30)\nsns.distplot(predictions, label='Predi\u00e7\u00e3o', bins=30)\nplt.title(\"Distribution Model -  test vs. Predicted values\", fontsize=20);\nplt.legend()\nplt.ylabel(\"p.u.\")\nplt.xlabel(\"kW\");\n#CD661D\nplt.figure(figsize=(12,5))\nplt.plot(y_test, '#000000', linewidth=.5, label='Valida\u00e7\u00e3o',  marker='o', markersize=5)\nplt.plot(predictions, '#CD661D',linewidth=1, label='Predi\u00e7\u00e3o',  marker='x', markersize=5)\n#plt.plot(df_pred_alt[250:350], '#CD661D', label= 'Predictions XGBoost', linewidth=.5, marker='o', markersize=5)\n\nplt.title(\"  test vs. Predictions (kW)\", fontsize=20);\nplt.legend()\nplt.ylabel(\"kW\")\nplt.xlabel(\"dias\");","242916b7":"y_test_pd = DataFrame(y_test)\npredictions_pd = DataFrame(predictions)\ncomparison  = pd.concat([y_test_pd, predictions_pd], axis=1, keys=['y_test', 'predictions'])\ncomparison['resid'] = comparison['y_test'] -  comparison['predictions']\ncomparison['resid_ratio'] = (comparison['predictions'] \/ comparison['y_test'])\ncomparison['residsqr'] = comparison['resid']*comparison['resid']\n\nplt.figure(figsize=[12,4])\nsns.distplot((comparison['resid']), bins=30)\nplt.title('Distribution - Residuals', fontsize=20);\nplt.xlabel(\"kW\")\nplt.ylabel(\"p.u.\");\n\nprint(\"Skewness = %.2f\" % comparison['resid'].skew())\nprint(\"Kurtosis = %.2f\" % comparison['resid'].kurt())\n\nplt.figure(figsize=[12,4])\nplt.scatter(comparison['resid'],comparison['resid_ratio'])\nplt.title('Scatter - Residuals vs. Residuals Ratio', fontsize=20);\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Residuals Ratio\");","dc5ec6c9":"N, M = 12, 4\nfig, ax = plt.subplots(figsize=(N, M))\ncorr_plot = plot_acf(comparison['resid'].dropna(), lags=26, ax=ax)\n#plt.show()\nplt.title('Autocorrelation - Residuals', fontsize=20)\nplt.xlabel('atraso');\n\nN, M = 12, 4\nfig, ax = plt.subplots(figsize=(N, M))\ncorr_plot = plot_acf(comparison['residsqr'].dropna(), lags=26, ax=ax)\n#plt.show()\nplt.title('Autocorrelation - Residuals Squared', fontsize=20)\nplt.xlabel('atraso');","8c4b6fdb":"X_df = total_df.drop(['Date','Day','Year','totalkW_max','totalkW_mean','insolation','totalkW_time_max','RH'], axis=1)\nX_df = X_df.reset_index(drop=True)\n\ny_df = total_df['totalkW_mean']\ny_df = pd.concat([y_df], axis=1, keys=\"total\")\ny_df = y_df.reset_index(drop=True)\n\ninitial_values = 30\n\nnl3 = y_df.head(initial_values)\nnl4 = X_df.head(initial_values)","b7f7c7a8":"i=len(nl3)-1\nj=1\npred=0\nsteps=564\nfor j in range(1,steps):\n    nl4 = nl4.append(pd.Series([nl3.loc[i][0]-nl3.loc[i-1][0]] + [nl3.loc[i][0]-nl3.loc[i-7][0]] + [nl3.loc[i][0]\/nl3.loc[i-1][0]] + [nl3.loc[i][0]\/nl3.loc[i-7][0]] + [X_df.loc[i][4]] + [nl4.loc[i-6][5]] + [X_df.loc[i][6]] + [X_df.loc[i][7]] + [X_df.loc[i][8]] + [X_df.loc[i][9]] + [X_df.loc[i][10]] + [nl4.loc[i-6][11]],  index=nl4.columns), ignore_index=True)\n    #print(nl4)\n    nly = nl4.tail(1).values\n    nly = scaler.transform(nly)\n    nly = np.reshape(nly, (nly.shape[0], 1, nly.shape[1]))\n    pred = float(model_ann.predict(nly))\n    #print(pred)\n    nl3 = nl3.append(pd.Series(pred, index=nl3.columns), ignore_index=True)\n    j=j+1\n    i=i+1","79bdd724":"df_real = y_df['t']\ndf_real = df_real.tail(steps)\ndf_pred = nl3.tail(steps)\n\nmodel_2_df = nl4\nmodel_2_df['predictions'] = df_pred\nmodel_2_df['real'] = df_real\n\n# join the predictions and the real values to the used DF, so we can correlate all the dependent and independent variables","259d61d9":"plt.figure(figsize=(12,5))\nplt.plot(nl3.head(initial_values),label='S\u00e9rie original')\nplt.plot(nl3.tail(steps), label='Predi\u00e7\u00e3o')\nplt.legend()\n#plt.title(\"Prediction Model - Application (kW)\", fontsize=20)\nplt.ylabel(\"kW\")\nplt.xlabel(\"dia\");","62b7f256":"df_real = y_df['t']\ndf_real = df_real.tail(steps)\ndf_pred = nl3.tail(steps)\n\ny_test_std_dev=y_test.astype(float).std()\nprint(\"Real Mean = %.2f\" % df_real.astype(float).mean())\nprint(\"Real Std Dev = %.2f\" % df_real.astype(float).std())\n\nalpha = 5.0\nlower_p = alpha \/ 2.0 # calculate lower percentile (e.g. 2.5)\nlower = max(0.0, percentile(df_real, lower_p)) # retrieve observation at lower percentile\n#print('Real %.1fth percentile = %.2f' % (lower_p, lower)) # calculate upper percentile (e.g. 97.5)\nupper_p = (100 - alpha) + (alpha \/ 2.0) # retrieve observation at upper percentile\nupper = max(1.0, percentile(df_real, upper_p))\n#print('Real %.1fth percentile = %.2f' % (upper_p, upper))\nprint(\"Predictions Mean = %.2f\" % df_pred.astype(float).mean())\nprint(\"Predictions Std Dev = %.2f\" % df_pred.astype(float).std())\n\nmodel_1_app_MAE = mean_absolute_error(df_real,df_pred)\nmodel_1_app_RMSE = np.sqrt(mean_squared_error(df_real,df_pred))\nmodel_1_app_EVS = explained_variance_score(df_real,df_pred)\n                           \nprint(\"MAE = %.2f\" % model_1_app_MAE)\nprint(\"RMSE = %.2f\" % model_1_app_RMSE)\nprint(\"EVS = %.2f\" % model_1_app_EVS)","0ec32037":"df_real","2be83444":"qq1=df_pred","652a0dc6":"qq1.corr()","5e9aa6f3":"plt.figure(figsize=(12,5)) \nsns.scatterplot(x='real', y='predictions', data=model_2_df, hue='is_weekend')\nplt.plot(df_real,df_real,'black')  # y=x line\nplt.plot(y_test,y_test+y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.plot(y_test,y_test-y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.title(\"Scatter Prediction Model - Real Series vs. Predicted values (kW)\", fontsize=20)\nplt.xlabel(\"Real Series\")\nplt.ylabel(\"Predictions\");\n\nplt.figure(figsize=(12,5)) # Plot predictions and the real values together\nsns.distplot(df_real, label='Real Series', bins=30)\nsns.distplot(df_pred, label='Predictions', bins=30)\nplt.title(\"Distribution Prediction Model - Real Series vs. Predicted values\", fontsize=20)\nplt.legend()\nplt.ylabel(\"p.u.\")\nplt.xlabel(\"kW\");\n\nplt.figure(figsize=(18,6)) # Plot predictions and the real values together\nplt.plot(df_real[200:400], '#CD661D', label= 'Real Series', linewidth=.7) # get only a part of it\nplt.plot(df_pred[200:400], '#3D59AB', label= 'Predictions', linewidth=1)\nplt.title(\"Prediction Model - Real Series vs. Predicted Values - day 200 to 400 (kW)\", fontsize=20)\nplt.xlabel(\"days\")\nplt.ylabel(\"kW\");\nplt.legend();\n","8b6c0ea0":"model_2_df['resid'] = model_2_df['real'] -  model_2_df['predictions']\nmodel_2_df['resid_ratio'] = (model_2_df['predictions'] \/ model_2_df['real'])\nmodel_2_df['residsqr'] = model_2_df['resid']*model_2_df['resid']\n\nplt.figure(figsize=[12,4])\nsns.distplot(model_2_df['resid'].loc[(model_2_df['is_weekend'] == 0)], bins=30, label='weekday')\nsns.distplot(model_2_df['resid'].loc[(model_2_df['is_weekend'] == 1)], bins=30, label='weekend')\nplt.title('Distribution - Residuals', fontsize=20)\nplt.xlabel(\"kW\")\nplt.xlim(-220,120)\nplt.legend()\nplt.ylabel(\"p.u.\");\n\nprint(\"Skewness: %.2f\" % model_2_df['resid'].skew())\nprint(\"Kurtosis: %.2f\" % model_2_df['resid'].kurt())\n\nplt.figure(figsize=[12,4])\nsns.scatterplot(x='resid', y='resid_ratio', data=model_2_df, hue='is_weekend')\nplt.title('Scatter - Residuals vs. Residuals Ratio', fontsize=20)\nplt.xlabel(\"Residuals\")\nplt.xlim(-230,110)\nplt.ylabel(\"Residuals Ratio\");","4b247f4d":"plt.figure(figsize=[12,4])\nsns.distplot((comparison['resid']), bins=30, label='RNR-LSTM')\nsns.distplot((model_2_df['resid']), bins=30, label='XGBoost')\n\n#plt.figure(figsize=[12,4])\n#sns.distplot(model_2_df['resid'], bins=40, label='RNR LSTM')\n#sns.distplot(model_alt_df['resid'], bins=40, label='XGBoost')\n\n#plt.title('Distribution - Residuals', fontsize=20);\nplt.xlabel(\"kW\")\nplt.ylabel(\"p.u.\");\nplt.legend()","55de6a82":"# comparing the distribution of the following values\n\nplt.figure(figsize=[18,8])\nplt.subplot(221)\nsns.distplot(nl4['totalkW_d\/1'].head(initial_values), label='Real Series')\nsns.distplot(nl4['totalkW_d\/1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Total kW [d\/1]\", fontsize=20)\nplt.xlabel(\"\");\n\nplt.subplot(223)\nsns.distplot(nl4['totalkW_w\/1'].head(initial_values), label='Real Series')\nsns.distplot(nl4['totalkW_w\/1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Total kW [w\/1]\", fontsize=20)\nplt.xlabel(\"\");\n\nplt.subplot(222)\nsns.distplot(nl4['totalkW_d-1'].head(initial_values), label='Real Series')\nsns.distplot(nl4['totalkW_d-1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Total kW [d-1]\", fontsize=20)\nplt.xlabel(\"\");\n\nplt.subplot(224)\nsns.distplot(nl4['totalkW_w-1'].head(initial_values), label='Real Series')\nsns.distplot(nl4['totalkW_w-1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Total kW [w-1]\", fontsize=20)\nplt.xlabel(\"\");","055aafd3":"plt.figure(figsize=(12,4))\nsns.lineplot(x='WD', y='real', data=model_2_df, label='Real Series')\nsns.lineplot(x='WD', y='predictions', data=model_2_df, label='Predictions');\nplt.title(\"Prediction Model - Real Series vs. Predicted Values (kW)\", fontsize=20)\nplt.ylabel(\"kW\")\nplt.legend();\n","9ca2ba84":"g = sns.FacetGrid(model_2_df, col='WD', xlim=(50,450), ylim=(0,30))\ng = g.map(plt.hist, 'real')\nh = sns.FacetGrid(model_2_df, col='WD', xlim=(50,450), ylim=(0,30))\nh = h.map(plt.hist, 'predictions')","6544f1aa":"plt.figure(figsize=(14,1))\nrwd = model_2_df.pivot_table(values='real',columns='WD')\nsns.heatmap(rwd.astype(int), annot=True,linecolor='white',linewidths=1, vmin=200, vmax=400, cmap='viridis', fmt='g')\nplt.title(\"Prediction Model - Real Series (kW)\", fontsize=20)\nplt.ylabel(\" \");\n\nplt.figure(figsize=(14,1));\npwd = model_2_df.pivot_table(values='predictions',columns='WD')\nsns.heatmap(pwd.astype(int), annot=True,linecolor='white',linewidths=1, vmin=200, vmax=400, cmap='viridis', fmt='g')\nplt.title(\"Prediction Model - Predictions (kW)\", fontsize=20)\nplt.ylabel(\" \");","014ba3b7":"Pred_Power_WD_M = model_2_df[['WD','Month','predictions']].groupby(by=['WD','Month']).mean()['predictions'].unstack()\nConsumo_WD_M = total_df[['WD','Month','totalkW_mean']].groupby(by=['WD','Month']).mean()['totalkW_mean'].unstack()\nplt.figure(figsize=(14,6))\ncomp=(Pred_Power_WD_M\/Consumo_WD_M)\nsns.heatmap(comp,cmap='viridis', annot=True, vmin=0.8, vmax=1.2, fontsize=12)\nplt.title(\"Prediction Model - Predictions \/ Real Series\", fontsize=20);\n","4952d2de":"plt.figure(figsize=(14,1))\nsns.heatmap(model_2_df.corr()[-4:-3:18], annot=True,linecolor='white',linewidths=1, cmap='viridis')\nplt.title(\"Real Series - Correlations\", fontsize=20);\n\nplt.figure(figsize=(14,1))\nsns.heatmap(model_2_df.corr()[-5:-4:18], annot=True,linecolor='white',linewidths=1, cmap='viridis')\nplt.title(\"Prediction Model - Correlations\", fontsize=20);","7a05854e":"# Models\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, LassoLars\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\n# Metrics and Grid Search\nfrom sklearn import model_selection, metrics\nfrom sklearn.model_selection import GridSearchCV","5550c1cb":"X = total_df.drop(['Date','Year','Day','totalkW_mean','totalkW_max','totalkW_time_max','insolation','RH'], axis=1).values\ny = total_df['totalkW_mean'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n\nscaler = MinMaxScaler()\nscaler.fit(X_train) \nX_train = scaler.transform(X_train) \nX_test = scaler.transform(X_test) ","51b03f4d":"# Creating a predefined function to test the models\ndef modelfit(model):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    print(\"MAE = %.2f\" % mean_absolute_error(y_test,preds))\n    print(\"RMSE = %.2f\" % np.sqrt(mean_squared_error(y_test,preds)))\n    print(\"EVS = %.2f\" % explained_variance_score(y_test,preds));","f7011f12":"# Linear Regression\n\nlm = LinearRegression(n_jobs = 10000)\nmodelfit(lm)","39c910bd":"# Random Forest Regressor\n\nrf = RandomForestRegressor(n_jobs = 1000, verbose=0)\nmodelfit(rf)","fbd8bc1a":"# XGBoost\nxg = XGBRegressor(learning_rate=0.1, n_estimators=5000)\nmodelfit(xg)","e359cc8f":"# Decision Tree\ndt = DecisionTreeRegressor()\nmodelfit(dt)","6edc4eb5":"# Bayesian Linear Model\nbr = BayesianRidge(alpha_1=1e-06, alpha_2=0.5, compute_score=False, copy_X=True,\n              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=10,\n              normalize=False, tol=0.1, verbose=False)\nmodelfit(br)","d8512216":"# Lasso Lars\nls = LassoLars()\nmodelfit(ls)","7f79e528":"#apply an array of learning rate values in order to get the one with the lowest error\nimport numpy as np\nsize=100\nlr = np.linspace(0.01,1,size)\nj = 0\nMAE = np.zeros(size)\nRMSE = np.zeros(size)\nEVS = np.zeros(size)\n\nfor j in range(0,len(lr)-1):\n    xg = XGBRegressor(learning_rate=lr[j], n_estimators=5000)\n    xg.fit(X_train, y_train)\n    preds = xg.predict(X_test)\n    MAE[j] = mean_absolute_error(y_test,preds)\n    RMSE[j] = np.sqrt(mean_squared_error(y_test,preds))\n    EVS[j] = explained_variance_score(y_test,preds)\n\nxg_df = pd.DataFrame(MAE[:-1], columns = ['MAE']) \nxg_df['RMSE'] = RMSE[:-1]\nxg_df['EVS'] = EVS[:-1]\nxg_df['lr'] = lr[:-1]","5658d32c":"sns.lineplot(x='lr', y='RMSE', data=xg_df, label='Real Series')","206b1dd8":"sns.lineplot(x='lr', y='EVS', data=xg_df, label='Real Series')","95dafbf1":"sns.lineplot(x='lr', y='MAE', data=xg_df, label='Real Series')","9c70063c":"xg_df.sort_values(by=['MAE'], inplace=True, ascending=True)\n\nmodel_alt_MAE = xg_df['MAE'].min()\nmodel_alt_RMSE = xg_df['RMSE'].min()\nmodel_alt_EVS = xg_df['EVS'].max()\n\nprint(\"MAE = %.2f\" % model_alt_MAE)\nprint(\"RMSE = %.2f\" % model_alt_RMSE)\nprint(\"EVS = %.2f\" % model_alt_EVS)\nprint(\"learning rate = %.2f\" % xg_df['lr'].head(1))\nlr_ideal = xg_df['lr'].head(1).astype(float).values\nxg_df.head(3)","d48dd732":"xg_df","920462db":"alt_model = XGBRegressor(learning_rate=0.14, n_estimators=5000)\nalt_model.fit(X_train, y_train)","6a188901":"eval_set = [(X_train, y_train), (X_test, y_test)]\n\nqq11 = alt_model.fit(X_train, y_train, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\n","1e9c62dc":"from matplotlib import pyplot\n","f51ff385":"results = qq11.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)","0415bfe9":"model_loss = pd.DataFrame(model_ann.history.history)\nprint(model_loss.tail(1))\nplt.figure(figsize=(12,5))\nplt.plot(model_loss['loss'], label = 'Perdas de Treinamento')\nplt.plot(model_loss['val_loss'], label = 'Perdas de Valida\u00e7\u00e3o')\nplt.legend()\nplt.ylabel(\"MSE\")\nplt.xlabel(\"\u00e9pocas\")\n#plt.title(\"Losses - Training & Validation\", fontsize=20);","0f2061c7":"model_loss = pd.DataFrame(model_ann.history.history)\nprint(model_loss.tail(1))\nplt.figure(figsize=(12,5))\nplt.plot(model_loss['loss'], label = 'Perdas de Treinamento')\nplt.plot(model_loss['val_loss'], label = 'Perdas de Valida\u00e7\u00e3o')\nplt.legend()\nplt.ylabel(\"MSE\")\nplt.xlabel(\"\u00e9pocas\")\n#plt.title(\"Losses - Training & Validation\", fontsize=20);","e0ab7877":"\n\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\npyplot.ylabel('Classification Error')\npyplot.title('XGBoost Classification Error')\npyplot.show()","8535cf4f":"print(\"Predictions Mean = %.2f\" % preds.astype(float).mean())\nprint(\"Predictions Std Dev = %.2f\" % preds.astype(float).std())","9e3c4aed":"np.corrcoef(y_test,preds)","e3042b03":"plt.figure(figsize=(12,5))\n\n\nplt.plot(y_test,y_test, 'orange') # y=x line\nplt.plot(y_test,y_test+y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.plot(y_test,y_test-y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.xlabel(\"y_test\")\n\n#plt.figure(figsize=(12,5))\nplt.scatter(y_test,predictions, marker='x', label='RNR LSTM' ) # Our predictions\nplt.scatter(y_test,preds, marker='x', label='XGBoost') # Our predictions\n\n#plt.figure(figsize=(12,5))\n#plt.scatter(df_real,df_pred, marker='x', label='RNR LSTM') # Our predictions\n#plt.figure(figsize=(12,5))\n#plt.scatter(df_real,df_pred_alt, marker='x', label='XGBoost') # Our predictions\nplt.legend()\nplt.ylabel(\"Predictions\")\n#plt.title(\"Scatter - y_test vs. Predictions (kW)\", fontsize=20);\nplt.xlabel(\"Demanda di\u00e1ria de valida\u00e7\u00e3o (kW)\")\nplt.ylabel(\"Demanda di\u00e1ria de predi\u00e7\u00e3o (kW)\")\n#plt.title(\"Scatter -  test vs. Predictions (kW)\", fontsize=20);","5a22e471":"plt.figure(figsize=(12,5))\nplt.scatter(y_test,predictions) # Our predictions","9b391555":"plt.figure(figsize=(12,5))\nplt.scatter(df_real,df_pred, marker='x', label='RNR LSTM') # Our predictions","2fb43232":"plt.figure(figsize=(12,5)) # Plot predictions and the real values together\nsns.distplot(y_test, label='Valida\u00e7\u00e3o', bins=30)\nsns.distplot(preds, label='Predi\u00e7\u00e3o', bins=30)\nplt.title(\"Distribution Model -  test vs. Predicted values\", fontsize=20);\nplt.legend()\nplt.ylabel(\"p.u.\")\nplt.xlabel(\"kW\");\n#CD661D\n\nplt.figure(figsize=(18,8))\nplt.plot(y_test, '#000000', linewidth=.5, label='Valida\u00e7\u00e3o',  marker='o', markersize=5)\nplt.plot(preds, '#CD661D',linewidth=1, label='Predi\u00e7\u00e3o',  marker='x', markersize=5)\n#plt.plot(df_pred_alt[250:350], '#CD661D', label= 'Predictions XGBoost', linewidth=.5, marker='o', markersize=5)\n\nplt.title(\"  test vs. Predictions (kW)\", fontsize=20);\nplt.legend()\nplt.ylabel(\"kW\")\nplt.xlabel(\"dias\");","9e1c7748":"X_df = total_df.drop(['Date','Day','Year','totalkW_max','totalkW_mean','insolation','totalkW_time_max','RH'], axis=1)\nX_df = X_df.reset_index(drop=True)\n\ny_df = total_df['totalkW_mean']\ny_df = pd.concat([y_df], axis=1, keys=\"total\")\ny_df = y_df.reset_index(drop=True)\n\ninitial_values = 30\n\nnl1 = y_df.head(initial_values)\nnl2 = X_df.head(initial_values)","45def8f7":"i=len(nl1)-1\nj=1\npred=0\nsteps=564\nfor j in range(1,steps):\n    nl2 = nl2.append(pd.Series([nl1.loc[i][0]-nl1.loc[i-1][0]] + [nl1.loc[i][0]-nl1.loc[i-7][0]] + [nl1.loc[i][0]\/nl1.loc[i-1][0]] + [nl1.loc[i][0]\/nl1.loc[i-7][0]] + [X_df.loc[i][4]] + [nl2.loc[i-6][5]] + [X_df.loc[i][6]] + [X_df.loc[i][7]] + [X_df.loc[i][8]] + [X_df.loc[i][9]] + [X_df.loc[i][10]] + [nl2.loc[i-6][11]],  index=nl2.columns), ignore_index=True)\n    #print(nl4)\n    nly = nl2.tail(1).values\n    nly = scaler.transform(nly)\n    #nly = np.reshape(nly, (nly.shape[0], 1, nly.shape[1]))\n    pred = float(alt_model.predict(nly))\n    #print(pred)\n    nl1 = nl1.append(pd.Series(pred, index=nl3.columns), ignore_index=True)\n    j=j+1\n    i=i+1\n","b9f25cd9":"df_pred_alt.astype(float)","820a3ac2":"k=df_pred_alt","37b3d4bc":"k['w']=df_real_alt","52597b3a":"k","3bfce984":"k.corr()","3833024c":"from scipy.stats import pearsonr\n","4fa99169":"model_alt_app_r2 = np.corrcoef(df_real_alt,df_pred_alt)","ec267a86":"model_alt_app_r2 = pearsonr(df_real_alt,df_pred_alt)","6347e983":"df_real_alt = y_df['t']\ndf_real_alt = df_real.tail(steps)\ndf_pred_alt = nl1.tail(steps)\n\nmodel_alt_df = nl2\nmodel_alt_df['predictions'] = df_pred_alt\nmodel_alt_df['real'] = df_real_alt\n\ny_test_std_dev=y_test.astype(float).std()\nprint(\"Real Mean = %.2f\" % df_real_alt.astype(float).mean())\nprint(\"Real Std Dev = %.2f\" % df_real_alt.astype(float).std())\n\nalpha = 5.0\nlower_p = alpha \/ 2.0 # calculate lower percentile (e.g. 2.5)\nlower = max(0.0, percentile(df_real_alt, lower_p)) # retrieve observation at lower percentile\n#print('Real %.1fth percentile = %.2f' % (lower_p, lower)) # calculate upper percentile (e.g. 97.5)\nupper_p = (100 - alpha) + (alpha \/ 2.0) # retrieve observation at upper percentile\nupper = max(1.0, percentile(df_real_alt, upper_p))\n#print('Real %.1fth percentile = %.2f' % (upper_p, upper))\nprint(\"Predictions Mean = %.2f\" % df_pred_alt.astype(float).mean())\nprint(\"Predictions Std Dev = %.2f\" % df_pred_alt.astype(float).std())\n\nmodel_alt_app_MAE = mean_absolute_error(df_real_alt,df_pred_alt)\nmodel_alt_app_RMSE = np.sqrt(mean_squared_error(df_real_alt,df_pred_alt))\nmodel_alt_app_EVS = explained_variance_score(df_real_alt,df_pred_alt)\nmodel_alt_app_r2 = pearsonr(df_real_alt,df_pred_alt)\n\n                             \nprint(\"MAE = %.2f\" % model_alt_app_MAE)\nprint(\"RMSE = %.2f\" % model_alt_app_RMSE)\nprint(\"EVS = %.2f\" % model_alt_app_EVS)\nprint(\"R2 = %.2f\" % model_alt_app_r2)","131cfe32":"plt.figure(figsize=(18,6)) # Plot predictions and the real values together\nplt.plot(df_real_alt[200:400], '#CD661D', label= 'Real Series', linewidth=.7)\nplt.plot(df_pred_alt[200:400], '#3D59AB', label= 'Predictions', linewidth=1)\nplt.title(\"Prediction Model - Real Series vs. Predicted Values (kW)\", fontsize=20)\nplt.xlabel(\"days\")\nplt.ylabel(\"kW\");\nplt.legend();","a7cc97e7":"model_alt_df['resid'] = model_alt_df['real'] -  model_alt_df['predictions']\nmodel_alt_df['resid_ratio'] = (model_alt_df['predictions'] \/ model_alt_df['real'])\nmodel_alt_df['residsqr'] = model_alt_df['resid']*model_alt_df['resid']\n\nplt.figure(figsize=[12,4])\nsns.distplot(model_alt_df['resid'].loc[(model_alt_df['is_weekend'] == 0)], bins=30, label='weekday')\nsns.distplot(model_alt_df['resid'].loc[(model_alt_df['is_weekend'] == 1)], bins=30, label='weekend')\nplt.title('Distribution - Residuals', fontsize=20)\nplt.xlabel(\"kW\")\nplt.xlim(-220,120)\nplt.legend()\nplt.ylabel(\"p.u.\");\n\nprint(\"Skewness: %.2f\" % model_alt_df['resid'].skew())\nprint(\"Kurtosis: %.2f\" % model_alt_df['resid'].kurt())\n\nplt.figure(figsize=[12,4])\nsns.scatterplot(x='resid', y='resid_ratio', data=model_alt_df, hue='is_weekend')\nplt.title('Scatter - Residuals vs. Residuals Ratio', fontsize=20)\nplt.xlabel(\"Residuals\")\nplt.xlim(-230,110)\nplt.ylabel(\"Residuals Ratio\");","eb9e6a0d":"df_real[30]","528ccbe7":"df_pred","b717d91b":"plt.figure(figsize=(18,6)) # Plot predictions and the real values together\nplt.plot(df_real[250:592], '#A9A9A9', label= 'S\u00e9rie original', linewidth=.3, marker='o', markersize=5)\nplt.plot(df_pred[29:592], '#3D59AB', label= 'RNR LSTM', linewidth=.5, marker='o', markersize=5)\nplt.plot(df_pred_alt[29:592], '#CD661D', label= 'XGBoost', linewidth=.5, marker='o', markersize=5)\n#plt.title(\"Prediction Model Comparison - Real Series vs. Predicted Values - day 250 to 350 (kW)\", fontsize=20)\nplt.xlabel(\"dias\")\nplt.ylabel(\"kW\");\nplt.legend();","7b1afe29":"592\nplt.figure(figsize=(12,4)) # Plot predictions and the real values together\nplt.plot(df_real, '#A9A9A9', label= 'S\u00e9rie original', linewidth=.3, marker='x', markersize=2)\nplt.plot(df_pred, '#3D59AB', label= 'RNR LSTM', linewidth=.5, marker='x', markersize=2)\nplt.plot(df_pred_alt, '#CD661D', label= 'XGBoost', linewidth=.5, marker='x', markersize=2)\n#plt.title(\"Prediction Model Comparison - Real Series vs. Predicted Values - day 250 to 350 (kW)\", fontsize=20)\nplt.xlabel(\"dias\")\nplt.ylabel(\"kW\");\nplt.legend();\n","8e2a3bc7":"\n592\nplt.figure(figsize=(12,4)) # Plot predictions and the real values together\nplt.plot(df_real[250:350], '#A9A9A9', label= 'S\u00e9rie original', linewidth=.3, marker='o', markersize=5)\nplt.plot(df_pred[250:350], '#3D59AB', label= 'RNR LSTM', linewidth=.5, marker='o', markersize=5)\nplt.plot(df_pred_alt[250:350], '#CD661D', label= 'XGBoost', linewidth=.5, marker='o', markersize=5)\n#plt.title(\"Prediction Model Comparison - Real Series vs. Predicted Values - day 250 to 350 (kW)\", fontsize=20)\nplt.xlabel(\"dias\")\nplt.ylabel(\"kW\");\nplt.legend();\n","392130eb":"plt.figure(figsize=(12,5))\nplt.scatter(df_real,df_pred, marker='x', label='RNR LSTM') # Our predictions\n#plt.figure(figsize=(12,5))\nplt.scatter(df_real,df_pred_alt, marker='x', label='XGBoost') # Our predictions\n\nplt.plot(df_real,df_real,'black')  # y=x line\nplt.plot(y_test,y_test+y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.plot(y_test,y_test-y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.title(\"Scatter Prediction Model - Real Series vs. Predicted values (kW)\", fontsize=20)\nplt.xlabel(\"Real Series\")\nplt.ylabel(\"Predictions\")\nplt.legend();\nplt.title(\"Scatter - y_test vs. Predictions (kW)\", fontsize=20);\nplt.xlabel(\"Demanda di\u00e1ria de valida\u00e7\u00e3o (kW)\")\nplt.ylabel(\"Demanda di\u00e1ria de predi\u00e7\u00e3o (kW)\")\nplt.title(\"Scatter -  test vs. Predictions (kW)\", fontsize=20);","0b5c99d7":"plt.figure(figsize=(12,5)) \nsns.scatterplot(x='real', y='predictions', data=model_2_df, hue='is_weekend')\nplt.plot(df_real,df_real,'black')  # y=x line\nplt.plot(y_test,y_test+y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.plot(y_test,y_test-y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.title(\"Scatter Prediction Model - Real Series vs. Predicted values (kW)\", fontsize=20)\nplt.xlabel(\"Real Series\")\nplt.ylabel(\"Predictions\");\n","ccffdaf0":"plt.figure(figsize=(12,5)) # Plot predictions and the real values together\n#A9A9A9\nsns.distplot(df_real, label='S\u00e9rie Original', bins=40, color='#000000')\nsns.distplot(df_pred, label='RNR LSTM', bins=40, color='#3D59AB')\nsns.distplot(df_pred_alt, label='XGBoost', bins=40, color='#CD661D')\n\nplt.title(\"Distribution Prediction Model - Real Series vs. Predicted values\", fontsize=20)\nplt.legend()\nplt.ylabel(\"p.u.\")\nplt.xlabel(\"kW\");","2ee435d3":"plt.figure(figsize=[12,4])\nsns.distplot(model_2_df['resid'], bins=40, label='RNR LSTM')\nsns.distplot(model_alt_df['resid'], bins=40, label='XGBoost')\n\n\nplt.title('Distribution Model Comparison - Residuals', fontsize=20)\nplt.xlabel(\"kW\")\nplt.xlim(-220,120)\nplt.legend()\nplt.ylabel(\"p.u.\");\n\nprint(\"Skewness RNN+LSTM = %.2f\" % model_2_df['resid'].skew())\nprint(\"Kurtosis RNN+LSTM = %.2f\" % model_2_df['resid'].kurt())\nprint(\"Skewness XGBoost = %.2f\" % model_alt_df['resid'].skew())\nprint(\"Kurtosis XGBoost = %.2f\" % model_alt_df['resid'].kurt())","f3b72920":"Model_alt_app = pd.DataFrame([model_alt_app_MAE, model_alt_app_RMSE, model_alt_app_EVS], columns = ['Model_alt_app'])\nModel_alt = [model_alt_MAE, model_alt_RMSE, model_alt_EVS]\nmodel_1_app = [model_1_app_MAE, model_1_app_RMSE, model_1_app_EVS ]\nmodel_1 = [model_1_MAE, model_1_RMSE, model_1_EVS ]\n\nModel_perf_df = pd.DataFrame(['RNN+LSTM',  'XGBoost','RNN+LSTM Applied', 'XGBoost Applied' ], columns = ['Model'])\nModel_perf_df['MAE'] = [model_1_MAE, model_alt_MAE,model_1_app_MAE,model_alt_app_MAE ]\nModel_perf_df['RMSE'] = [model_1_RMSE, model_alt_RMSE, model_1_app_RMSE, model_alt_app_RMSE ]\nModel_perf_df['EVS'] = [model_1_EVS, model_alt_EVS,model_1_app_EVS, model_alt_app_EVS ]\n\nModel_perf_df","beb74b94":"### It is interesting as well to visualize how the data changes during the days of the week and the months of the year.","6f955c2a":"### Since the goal of the model is to create a prediction of the whole system, we will work only with the total data, so the data can be filtered to a new DataFrame called \"total_df\", which excludes the information of the 4 SEs individually.","52fab749":"### Both tools (RNN LSTM & XGBoost) have similar results, but XGBoost is easier - faster and require less computational effort - to tune and apply it.","db3c6876":"# 4.1 Model optimization","7e05657c":"# 3.1 For loop - 564 days","64263c48":"### The model presented a great EVS (Explained Variance Score). MAE (Mean Absolute Error) and RMSE (Root Mean Squared Error) are not too significant when compared with the output variable ('totalkW_mean') mean.","acc6c7e8":"# 1.4 Analyze the data","d2726211":"# 1.5 Feature Eng. - Dummy variables","47e0e22a":"### Above, we can clearly see that there is a major difference between the weekends (5 and 6) and the working days.\n### Let's analyze another relations between the dependent variable (output - 'totalkW_mean') and other dependent variables (inputs - Day of the week, month, ...).","403882c5":"### Below, there are 2 plots: the first illustrating the HDD, CDD and Mean Temperature during the internal. On the second, it is shown the Relative Humidity. The weather variables have a considerable impact on the 'totalkW_mean', as it was presented right before.","0d8bfa9a":"## Creating the model!\n### Here is where the Artificial Neural Network is created. The choosen architecture is the LSTM, because it also consider temporal relations between the variables.","c54efd93":"### Since there are information about the time where occured the power peak on each day, it is possible to count when it ocurred, based on the day of the week.","159c7cc0":"### The ideia of the code below is to run a loop for each day, to consider the info from the last day. Simply, it collects all the INPUTs needed, which are the mean power of the day before and the weather data (temperature, HDD, ...).","54ee41fa":"# 4.2 RNN LSTM vs. XGBoost","a35f4b54":"### Finally, there are some differences between the correlations of the INPUT variables with the Real Series and the Predictions, mostly regarded to the totalkW relations (d-1, w-1, d\/1 & w\/1), and with 'is_weekend'.","daeb8198":"### Let's check the missing values of the gathered data!","ef3f75ed":"### The best alternative code is the XGBoost. Let's see how can it can be optimized when changing its parameters.","1313e4b0":"### Above we analyzed the residuals (errors between the predicted and the real value). It is good to have a normal distribution, with a good skewness value and a kurtosis value bigger than 3 (normal distribution), indicating a longer tail.","f1fd3426":"### Now, it is known that it peaks around 14-15h during the week and there is a distribution during the day on the weekends.","1a5e69d5":"# 2.2 Fitting\n","0e3c6d70":"### Since we dropped the columns cited above, we remain with the following INPUT variables:\n* totalkW_d-1\n* totalkW_w-1\n* totalkW_d\/1\n* totalkW_w\/1\n* Month\n* WD\n* CDD_15\n* HDD_25\n* NWD\n* temp_max\n* temp_mean\n* is_weekend","6b1b0e25":"### The ideal model will have the same results for training and validation losses, as well as lower values, indicating a great fitting of the model. In this case, the validation is a few % higher than the training losses, indicating an overfitting.","60ebeeac":"### There are a lot of missing values on insolation, so we will discard it later.\n### Additionally, since we are missing the **first** 7 rows of 'totalkW_w\/1' and 'totalkW_w-1', we will discard them as well.","7facb4bd":"# 1 Data!","190c59e5":"### The autocorrelation plot illustrates that our dataset has a high correlation with its 7th day in a row. Basically, it proves that the same week day has similar properties through time.","39787cae":"# 1.3 Reduce the file size\n\n### This is a code that I didn't create. It is just used to reduce the file size, when working with a big amount of data.","0f951e3a":"# 1.2 Data description\n\n* Date = Date (dd\/mm\/yyyy)\n* 53_kW_mean = Mean electrical energy power from substation # 53 during the day (kW)\n* 53_kW_max  = Maximum electrical energy power from substation # 53 during the day (kW)\n* N_kW_mean = Mean electrical energy power from substation # N during the day (kW)\n* N_kW_max  = Maximum electrical energy power from substation # N during the day (kW)\n\n\n\n\n* totalkW_mean = Mean electrical energy power from all substations during the day (kW)\n* totalkW_max  = Maximum electrical energy power from all substations during the day (kW)\n\n\n\n* totalkW_d-1 = totalkW_mean from day [i] minus totalkW_mean from day [i-1] (kW)\n* totalkW_w-1  = totalkW_mean from day [i] minus totalkW_mean from day [i-7] (kW)\n* totalkW_d\/1 = totalkW_mean from day [i] over totalkW_mean from day [i-1] (kW)\n* totalkW_w\/1  = totalkW_mean from day [i] over totalkW_mean from day [i-7] (kW)\n\n\n\n* 71_71A_kW_mean = Mean electrical energy power from substations # 71 & 71A during the day (kW)\n* 71_71A_kW_max  = Maximum electrical energy power from substations # 71 & 71A during the day (kW)\n\n\n\n* Month = Month of the data\n* Year = Year of the data\n* Day = Day of the month of the data\n* WD = Day of the week - 0 is Monday, 6 is Sunday\n\n\n\n* N_kW_time_max = Hour of the day for the power peak from substation N (hh:mm)\n\n\n\n* CDD_15 = Cooling degree day of a 15\u00baC base (\u00baC)\n* HDD_25 = Heating degree day of a 25\u00baC base (\u00baC)\n\n\n\n* NWD = Non Working Day - 0 is False, 1 is True\n\n\n\n* temp_max = Maximum temperature during the day (\u00baC)\n* insolation = Average insolation during the day (Wh\/m\u00b2)\n* temp_mean = Mean temperature during the day (\u00baC)\n* RH = Relative Humidity (%)\n","8efd8886":"# 1.1 Data info","8abe5b12":"### When comparing the mean values of the Predictions and the Real Series, it becomes clear that the model has a great accuracy, for both weekdays and weekends. \n","8e8851c6":"### Then, we can see how it behaves through time (from day 0 to day 593):","15930a91":"### If there are any questions or feedback, please don't hesitate on contacting me.\n\n### Mail: dcluzzi@gmail.com\n### LinkedIn: https:\/\/www.linkedin.com\/in\/daniel-luzzi-850085139\/\n### Github: https:\/\/github.com\/danielluzzi\/python\n### Kaggle: https:\/\/www.kaggle.com\/danielluzzi\n","6e634150":"# 2.1 Create the RNN LSTM Model","fb7b1f0f":"qq1['rnr']=df_real","5b6b481e":"### Both series are behaving well when comparing to the real series, but let's check the values distribution and the erros.","6483c880":"## Splitting and normalizing the data!\n","2eb1c023":"# 3 Prediction Model","b0ca4500":"## 'totalkW_mean' correlations:\n### Weekday is a meaningful variable, as well as the relations between the measurements (d-1, w-1, d\/1 & w\/1). Luckly, insolation has an almost zero correlation, and the same applies to RH and Day.\n### The variables 'Day' and 'RH' will be excluded from the model.","2762d99a":"# 4 Model Comparing","e513361e":"### Here we will plot the 4 substations (53, 71, 7A and 83), plus the joint values of SEs 71 & 71A and finally the Total, which is the sum of the 4 SEs.\n### First, let's see the distribution of the data:","7b1e205c":"# 3.2 Analyze the results","ecab5dd8":"### The scatter plot shows that there are some outliers - week day with a low consumption - that our model couldn't predict perfectly. The segragation from weekends is really clear now.","04d7eaba":"### Adding an Early Stop! \n","89028eab":"**# 2 RNN Model","739e6caf":"### There is already a category for each day of the week, but, as we seen before, there is a STRONG correlation between the read power and if the day is or not on the weekend. Consequently, it will be created another column called 'is_weekend' that will try to capture this behavior. This column will be a dummy variable, which means that, if the day is on a weekend, it will have a value of 1 and, if it is not, it will be zero.","e08c89f1":"### The scattered plot illustrates that the model is working properly. The values are close to the y=x line, indicating a good match between the real and the predicted values.","883bc4a1":"### The data seems to have a 7 day period, which means that each day of the week has a similar behavior. Let's explore this idea with a rolling mean plot and an autocorrelation plot.","87d80f77":"### When analyzing the power distribution on each day of the week there are some differences as well.","5b3f0fd9":"### It is needed to split the gathered data to train and test our model. Therefore, it will use 75% of the data for training and 25% for testing. It is also important to scale the data, which means that, for each variable, we will work with values from 0 to 1. It makes the process of optimizing the model easier.","3c04f876":"### Since these 4 variables have a significant impact on the output of the model, it is great to visualize if there is a differece between the predicted and the real values. The weekly data has a more simple behavior, that our model predicted well. The daily has more outliers, represented by a 3 peak distribution plot, but the model could also identify those items and replicate them correctly.","d86ef9c2":"# The End!","5c4d70e1":"# 2.3 Results","c4e061e8":"## Energy Predictor 1.3 - RNN LSTM + Other models \n\n## Main Purpose:\n\n### This code was created with an intention of predicting the energy consumption of a system. The system is composed by 4 SEs with a respective meter for each one of them. The measurements are real and were collected over a 600 days period.\n\n### When working with energy efficieny projects, it is really important to follow the international performance measurement and verification protocol, that might be hard sometimes, mostly when a lot of implementations are made, in different sectors, and there aren't enough meters to measure them. Therefore, a robust shadow virtual plant can be used as a benchmark for the previous system - before energy effiency actions were implemented - and, consequently, it can be known what are the energy savings.\n\n### As it will be shown below, many variables are needed for using this tool properly, like HDD and CDD, for example, which implies that, without future weather data, the prediction won't work.\n\n### Finally, the code is more focused on data analysis from the SEs, some feature engineering, RNN w\/ LSTM modeling, statistital model evaluation, model application and statistical analysis of its performance.\n\n### *In v1.3, it was added a comparison between the RNN and other models. The XGBoost got a similar accuracy for this data set.\n\n## Hope you enjoy it! ","d8ebe6b2":"### The residuals should be white noize for an effective model. They were plotted before, but now its autocorrelation is analyzed. There are no significant correlation values for either the residuals and the residuals squared.","d4932d70":"### The model ran until it converged to the minimum possible MSE to the validation set.","6f08ec3f":"# df_pred","2579f61c":"## DF correlations"}}