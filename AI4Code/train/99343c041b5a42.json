{"cell_type":{"7f36193f":"code","f916fdba":"code","c51f5022":"code","02307317":"code","08a4f787":"code","69cc6e2b":"code","d0ae9c70":"code","b10e0606":"code","2322e765":"code","042d2b92":"code","1731fcec":"code","52e294cb":"code","0b23da02":"code","95636e2f":"code","2849e404":"code","377266d3":"code","7aea52b7":"code","6940ad53":"code","6e73edbb":"code","935efd13":"code","9f5a483a":"code","ad689333":"code","9cc9a72f":"code","60a10167":"code","c668db95":"code","8350206a":"code","c632670c":"code","569ca177":"code","7e261bd4":"code","50bff63d":"code","5c31ebff":"code","bc0af205":"code","17fb3bf6":"code","a3c5f019":"code","56d216d9":"code","8211dbc2":"code","34f184a3":"code","f8febd7b":"code","157925d2":"markdown","9ab2a214":"markdown","50802b1e":"markdown","8844cfcd":"markdown","e53e8e51":"markdown","32f138c4":"markdown","7f8f838a":"markdown","655700f2":"markdown","c5c5bab1":"markdown","052685fc":"markdown","c68600dd":"markdown","bda8cb39":"markdown","3c7a12e4":"markdown","35d07e2d":"markdown","0ef963cc":"markdown","60ed4220":"markdown","3956ce34":"markdown","fcb6458e":"markdown","2d89b9d3":"markdown","ec57bc1b":"markdown","ddfc2e49":"markdown"},"source":{"7f36193f":"!pip install neattext\nimport pandas as pd\nimport numpy as np\nimport neattext as nt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom nltk.stem import PorterStemmer\nfrom functools import partial\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","f916fdba":"true_path = '\/kaggle\/input\/fake-and-real-news-dataset\/True.csv'\nfake_path = '\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv'","c51f5022":"trueDf = pd.read_csv(true_path)\nfakeDf = pd.read_csv(fake_path)","02307317":"trueDf.columns","08a4f787":"fakeDf.columns","69cc6e2b":"trueDf['class'] = pd.Series(1, index=trueDf.index)\nfakeDf['class'] = pd.Series(0, index=fakeDf.index)","d0ae9c70":"trueDf.head()","b10e0606":"fakeDf.head()","2322e765":"df = pd.concat([trueDf, fakeDf])","042d2b92":"del trueDf\ndel fakeDf","1731fcec":"df['class'].value_counts()","52e294cb":"df.duplicated().sum()","0b23da02":"df.drop_duplicates(inplace=True)","95636e2f":"df['class'].value_counts()","2849e404":"def preProcess(txt):\n    txt = txt.lower()\n    txt = nt.remove_punctuations(txt)\n    txt = nt.remove_special_characters(txt)\n    txt = nt.remove_urls(txt)\n    txt = nt.remove_dates(txt)\n    txt = nt.remove_numbers(txt)\n    txt = nt.remove_stopwords(txt)\n    return txt","377266d3":"df['preProcessTitle'] = df['title'].apply(preProcess)\ndf['preProcessText'] = df['text'].apply(preProcess)","7aea52b7":"textTxt = ' '.join(df['preProcessText'].values)\n\nwordcloudTxt = WordCloud(width = 800, height = 800,\n                background_color ='black',\n                min_font_size = 10).generate(textTxt)\n  \n                       \nplt.figure(figsize = (10, 8), facecolor = None)\nplt.imshow(wordcloudTxt)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","6940ad53":"words, freq = list(wordcloudTxt.words_.keys()), list(wordcloudTxt.words_.values())\ntxtWords = pd.DataFrame(data = list(zip(words[:10], freq[:10])), columns=['words', 'freq'])\nplt.figure(figsize=(15,8))\nsns.barplot(x=txtWords['words'], y=txtWords['freq'])","6e73edbb":"titleTxt = ' '.join(df['preProcessTitle'].values)\n\nwordcloudTitle = WordCloud(width = 800, height = 800,\n                background_color ='black',\n                min_font_size = 10).generate(titleTxt)\n  \n                       \nplt.figure(figsize = (10, 8), facecolor = None)\nplt.imshow(wordcloudTitle)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","935efd13":"words, freq = list(wordcloudTitle.words_.keys()), list(wordcloudTitle.words_.values())\ntitleWords = pd.DataFrame(data = list(zip(words[:10], freq[:10])), columns=['words', 'freq'])\nplt.figure(figsize=(15,8))\nsns.barplot(x=titleWords['words'], y=titleWords['freq'])","9f5a483a":"plt.figure(figsize=(10,8))\nsns.countplot(x=df['class']);","ad689333":"plt.figure(figsize=(10,8))\nsns.countplot(x=df['subject'], hue=df['class']);","9cc9a72f":"ps = PorterStemmer()\nstem = lambda x: ' '.join(list(map(ps.stem, x.split())))","60a10167":"df['preProcessTextStem'] = df['preProcessText'].apply(stem)\ndf['preProcessTitleStem'] = df['preProcessTitle'].apply(stem)","c668db95":"df.head()","8350206a":"X, Y = df['preProcessTextStem'], df['class']","c632670c":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","569ca177":"del df\ndel X\ndel Y","7e261bd4":"pipLogR = Pipeline([('tfidf', TfidfVectorizer()), ('logR', LogisticRegression())])\nscore_logR = cross_val_score(estimator=pipLogR, X=X_train, y=y_train, cv=5)\nprint('Mean Score: ', score_logR.mean())","50bff63d":"pipNb = Pipeline([('tfidf', TfidfVectorizer()), ('Nb', BernoulliNB())])\nscore_Nb = cross_val_score(estimator=pipNb, X=X_train, y=y_train, cv=5)\nprint('Mean Score: ', score_Nb.mean())","5c31ebff":"pipRf = Pipeline([('tfidf', TfidfVectorizer()), ('Rf', RandomForestClassifier())])\nscore_Rf = cross_val_score(estimator=pipRf, X=X_train, y=y_train, cv=5)\nprint('Mean Score: ', score_Rf.mean())","bc0af205":"pipLogR_cv = Pipeline([('cv', CountVectorizer()), ('logR', LogisticRegression(max_iter=500))]) # increased the max_iter since default (100) didn't converge \nscore_logR_cv = cross_val_score(estimator=pipLogR_cv, X=X_train, y=y_train, cv=5)\nprint('Mean Score: ', score_logR_cv.mean())","17fb3bf6":"pipNb_cv = Pipeline([('cv', CountVectorizer()), ('Nb', BernoulliNB())])\nscore_Nb_cv = cross_val_score(estimator=pipNb_cv, X=X_train, y=y_train, cv=5)\nprint('Mean Score: ', score_Nb_cv.mean())","a3c5f019":"pipRf_cv = Pipeline([('cv', CountVectorizer()), ('Rf', RandomForestClassifier())])\nscore_Rf_cv = cross_val_score(estimator=pipRf_cv, X=X_train, y=y_train, cv=5)\nprint('Mean Score: ', score_Rf_cv.mean())","56d216d9":"pipe = Pipeline([('cv', CountVectorizer()), ('logR', LogisticRegression(max_iter=500))])\npipe.fit(X_train, y_train)","8211dbc2":"pipe.score(X_test, y_test)","34f184a3":"pred_y = pipe.predict(X_test)\nplt.figure(figsize=(10,8))\nsns.heatmap(confusion_matrix(y_test, pred_y), annot=True, fmt='d');","f8febd7b":"print(classification_report(y_test, pred_y))","157925d2":"# EDA","9ab2a214":"### Top 10 words in 'text'","50802b1e":"### Top 10 words in 'title'","8844cfcd":"### True vs Fake news","e53e8e51":"#### deleting unrequired dataframes","32f138c4":"> Combination of CountVectorizer and LogisticRegression outperformed the others!","7f8f838a":"# Predicting the unseen test data","655700f2":"# Load The Data","c5c5bab1":"### Subject w.r.t the class (True vs Fake news)","052685fc":"### TF-IDF with LogisticRegression, BernaoulliNb and RandomForest","c68600dd":"### Creating wordcloud for 'title'","bda8cb39":"### Creating wordcloud for 'text'","3c7a12e4":"### Preprocessing the text data (both title and text)","35d07e2d":"# Creating The Final Pipeline","0ef963cc":"# Splitting the Data into Train - Test (80-20)","60ed4220":"### CountVectorizer with LogisticRegression, BernaoulliNb and RandomForest","3956ce34":"# Add Labels (1 - True, 0 - Fake)","fcb6458e":"# Comparing various Classification Models using Cross Validation","2d89b9d3":"# Import Libraries","ec57bc1b":"# Stemming the text data","ddfc2e49":"# Merge trueDf and fakeDf"}}