{"cell_type":{"602fc6e6":"code","13887943":"code","2ddafe1e":"code","d733c800":"code","00048f62":"code","d6d86aca":"code","5bb80445":"code","711e8f2e":"code","ed8306ae":"code","8f2a02ce":"code","c8407690":"code","5e0d0765":"code","41f4ea86":"markdown","031a52a9":"markdown","da145317":"markdown","179747c2":"markdown","8f6a79ed":"markdown","b493d602":"markdown","4a72475e":"markdown","27bc32e6":"markdown"},"source":{"602fc6e6":"import re\nimport os\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom glob import glob\nfrom collections import OrderedDict\n# import tensorflow_hub as hub\nimport tensorflow as tf\n# import bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","13887943":"!pip install ..\/input\/sacremoses > \/dev\/null\n\nsys.path.insert(0, \"..\/input\/transformers\/\")\nfrom transformers import *","2ddafe1e":"PATH = '..\/input\/google-quest-challenge\/'\n\n# BERT_PATH = '..\/input\/bert-base-from-tfhub\/bert_en_uncased_L-12_H-768_A-12'\n# tokenizer = tokenization.FullTokenizer(BERT_PATH+'\/assets\/vocab.txt', True)\n\ncore = 'bert'\nBERT_PATH = '..\/input\/bert-base-uncased-huggingface-transformer\/'\nWEIGHTS_FILES_10 = sorted(glob('..\/input\/bertbasetriplet5cv\/*.h5'))\nWEIGHTS_FILES_6_NOES = sorted(glob('..\/input\/bert-base-5cv\/*.h5'))\nWEIGHTS_FILES_6 = sorted(glob('..\/input\/bert-base-double-5cv-early-stop\/*.h5'))\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)\n\nprint('\\ntrained weights files (10 inputs):\\n\\t', WEIGHTS_FILES_10)\nprint('\\ntrained weights files (6 inputs, without early stop):\\n\\t', WEIGHTS_FILES_6_NOES)\nprint('\\ntrained weights files (6 inputs):\\n\\t', WEIGHTS_FILES_6)","d733c800":"def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, 'longest_first', max_sequence_length)\n    \n    input_ids_qa, input_masks_qa, input_segments_qa = return_id(\n        title + ' ' + question, answer, 'longest_first', max_sequence_length)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a,\n            input_ids_qa, input_masks_qa, input_segments_qa]\n\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    input_ids_qa, input_masks_qa, input_segments_qa = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        inputs = _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        ids_q, masks_q, segments_q = inputs[:3]\n        ids_a, masks_a, segments_a = inputs[3:6]\n        ids_qa, masks_qa, segments_qa = inputs[6:]\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n        input_ids_qa.append(ids_qa)\n        input_masks_qa.append(masks_qa)\n        input_segments_qa.append(segments_qa)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32),\n            np.asarray(input_ids_qa, dtype=np.int32), \n            np.asarray(input_masks_qa, dtype=np.int32), \n            np.asarray(input_segments_qa, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","00048f62":"# Count feature functions\ndef split_sentence(sentence):\n    return re.split('[\\.\\?]+\\s*', sentence)\n\ndef count_wps(sentence):\n    sentence_list = split_sentence(sentence)\n    w_per_s = np.mean([len(sentence.split()) for sentence in sentence_list\n                       if len(sentence) > 0])\n    return w_per_s\n\ndef build_count_features(df):\n    count_features = OrderedDict()\n\n    count_features['n_title_words'] = df.question_title.apply(lambda x: len(x.split()))\n    count_features['n_body_words'] = df.question_body.apply(lambda x: len(x.split()))\n    count_features['n_answer_words'] = df.answer.apply(lambda x: len(x.split()))\n\n    count_features['n_body_sentences'] = df.question_body.apply(lambda x: len(split_sentence(x)))\n    count_features['n_answer_sentences'] = df.answer.apply(lambda x: len(split_sentence(x)))\n\n    count_features['n_title_wps'] = df.question_title.apply(count_wps)\n    count_features['n_body_wps'] = df.question_body.apply(count_wps)\n    count_features['n_answer_wps'] = df.answer.apply(count_wps)\n\n    df_count_features = pd.DataFrame(count_features)\n    return df_count_features\n\n\n# Host feature function\ndef build_host_features(df, df_host_count, popular_host_thr):\n    def _convert(x):\n        if x not in df_host_count.index:\n            return 'other'\n        if df_host_count[x] > popular_host_thr:\n            return x\n        else:\n            return 'other'\n    return df.host.apply(_convert)\n\ndef build_features(df, df_host_count, popular_host_thr):\n    return pd.concat(\n        [build_count_features(df),\n         build_host_features(df, df_host_count, 100)],\n        axis=1)","d6d86aca":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, batch_size, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        self.best_rho = -1\n        self.best_weights = None\n        self.best_epoch = 0\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        rho_val = compute_spearmanr(\n            self.valid_outputs,\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n\n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n\n        if rho_val > self.best_rho:\n            self.best_rho = rho_val\n            self.best_weights = self.model.get_weights()\n            self.best_epoch = epoch+1\n        else:\n            self.model.stop_training = True\n            \n    def on_train_end(self, logs={}):\n        self.model.set_weights(self.best_weights)\n        self.model.save_weights(f'bert-base-{self.fold+1}fold-{self.best_epoch}epoch.h5')\n        print(f'Training ends with rho: {self.best_rho}')\n\n\ndef create_model_10inputs():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    qa_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    qa_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    qa_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n   \n    feat_count = tf.keras.layers.Input((8, ), dtype=tf.float32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    # bert_model = TFBertForSequenceClassification.from_pretrained(\n    #     BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5',\n                                             config=config)\n    \n    # will only use the transformer (\"bert\") from TFBertForSequencabseClassification\n    # if config.output_hidden_states = True, obtain hidden states via .bert(...)[-1]\n    # q_embedding = bert_model.bert(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    # a_embedding = bert_model.bert(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    qa_embedding = bert_model(qa_id, attention_mask=qa_mask, token_type_ids=qa_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    qa = tf.keras.layers.GlobalAveragePooling1D()(qa_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a, qa])\n    x = tf.keras.layers.Concatenate()([x, feat_count]) # pre-dropout\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn,\n                                          a_id, a_mask, a_atn,\n                                          qa_id, qa_mask, qa_atn,\n                                          feat_count], \n                                  outputs=x)    \n    return model","5bb80445":"def create_model_6inputs():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    # bert_model = TFBertForSequenceClassification.from_pretrained(\n    #     BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5',\n                                             config=config)\n    \n    # will only use the transformer (\"bert\") from TFBertForSequencabseClassification\n    # if config.output_hidden_states = True, obtain hidden states via .bert(...)[-1]\n    # q_embedding = bert_model.bert(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    # a_embedding = bert_model.bert(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn,\n                                          a_id, a_mask, a_atn,], \n                                  outputs=x)\n    \n    return model","711e8f2e":"outputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","ed8306ae":"inputs_count = build_count_features(df_train).apply(np.log1p)\ntest_inputs_count = build_count_features(df_test).apply(np.log1p)\n\nss = StandardScaler()\ninputs_count = ss.fit_transform(inputs_count)\ntest_inputs_count = ss.transform(test_inputs_count)\n\ninputs_count = np.asarray(inputs_count, dtype=np.float32)\ntest_inputs_count = np.asarray(test_inputs_count, dtype=np.float32)","8f2a02ce":"n_splits = 5\nepochs = 10\nbatch_size = 4\nlearning_rate = 3e-5\n\ngkf = GroupKFold(n_splits=n_splits).split(X=df_train.question_body, groups=df_train.question_body)\n\ntest_inputs += [test_inputs_count]\n\nrhos_val = []\ntest_preds = []\nmodel_10inp = create_model_10inputs()\nmodel_6inp = create_model_6inputs()\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    # will actually only do 2 folds (out of 5) to manage < 2h\n    # if fold in [0, 1]:\n    train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n    train_inputs += [inputs_count[train_idx]]\n    train_outputs = outputs[train_idx]\n\n    valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n    valid_inputs += [inputs_count[valid_idx]]\n    valid_outputs = outputs[valid_idx]\n    \n    K.clear_session()\n\n    if False:\n        model = create_model()\n        optimizer = tf.keras.optimizers.Adam(learning_rate)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n        custom_callback = CustomCallback(\n            valid_data=(valid_inputs, valid_outputs),\n            batch_size=batch_size,\n        )\n        model.fit(train_inputs, train_outputs, \n                  epochs=epochs,\n                  batch_size=batch_size,\n                  callbacks=[custom_callback])\n        model.save_weights(f'bert-base-{fold}of{n_splits}-{epochs}epoch.h5')\n        \n        test_preds.append(model.predict(test_inputs))\n    \n    if True:\n        weights_10 = WEIGHTS_FILES_10[fold]\n        print(f'\\nLoad weights from {weights_10}')\n        model_10inp.load_weights(weights_10)\n        \n        # valid_predicts = model.predict(valid_inputs, batch_size=batch_size)\n        # rho_val = compute_spearmanr(valid_outputs, valid_predicts)\n        # print(f'{fold+1}in5fold model rho: {rho_val}')\n        # rhos_val.append(rho_val)\n        test_preds.append(model_10inp.predict(test_inputs, batch_size=batch_size))\n        \n        # weights_6 = WEIGHTS_FILES_6_NOES[fold]\n        weights_6 = WEIGHTS_FILES_6[fold]\n        print(f'\\nLoad weights from {weights_6}')\n        model_6inp.load_weights(weights_6)\n        test_preds.append(model_6inp.predict(test_inputs[:6], batch_size=batch_size))\n        \n        weights_6_noes = WEIGHTS_FILES_6_NOES[fold]\n        print(f'\\nLoad weights from {weights_6}')\n        model_6inp.load_weights(weights_6_noes)\n        test_preds.append(model_6inp.predict(test_inputs[:6], batch_size=batch_size))","c8407690":"# x = np.linspace(0, 1, 100)\n\ndef cut_values(x, nbins=15):\n    # cut into [0, 0, ... | ... | ,,, | ,,, | 1, 1, 1] nbins-regions\n    bins = [-1000] + list(np.linspace(0.05, 0.95, nbins-1)) + [1000]\n    return pd.cut(x, bins, labels=False)\/(nbins-1)\n\n# df_train[output_categories].apply(lambda x: cut_values(x, 10))'\n# df_sub.iloc[:, 1:] = df_train[output_categories].iloc[:476].apply(lambda x: cut_values(x, 15))\n# df_sub","5e0d0765":"df_sub.iloc[:, 1:] = np.average(test_preds, axis=0)\n                                # weights=[0, 1, 0, 0, 1]) # for weighted average set weights=[...]\n# df_sub.iloc[:, 1:] = df_sub[output_categories].apply(lambda x: cut_values(x, 15))\n\ndf_sub.to_csv('submission.csv', index=False)","41f4ea86":"#### 3. Create model\n\n`compute_spearmanr()` is used to compute the competition metric for the validation set\n<br><br>\n`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset.\n","031a52a9":"#### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)","da145317":"#### 4. Obtain inputs and targets, as well as the indices of the train\/validation splits","179747c2":"### Bert-base TensorFlow 2.0\n\nThis kernel does not explore the data. For that you could check out some of the great EDA kernels: [introduction](https:\/\/www.kaggle.com\/corochann\/google-quest-first-data-introduction), [getting started](https:\/\/www.kaggle.com\/phoenix9032\/get-started-with-your-questions-eda-model-nn) & [another getting started](https:\/\/www.kaggle.com\/hamditarek\/get-started-with-nlp-lda-lsa). This kernel is an example of a TensorFlow 2.0 Bert-base implementation, using ~~TensorFow Hub~~ Huggingface transformer. <br><br>\n\n---\n**Update 1 (Commit 7):**\n* removing penultimate dense layer; now there's only one dense layer (output layer) for fine-tuning\n* using BERT's sequence_output instead of pooled_output as input for the dense layer\n---\n\n**Update 2 (Commit 8):**\n* adjusting `_trim_input()` --- now have a q_max_len and a_max_len, instead of 'keeping the ratio the same' while trimming.\n* **importantly:** now also includes question_title for the input sequence\n---\n\n**Update 3 (Commit 9)**\n<br><br>*A lot of experiments can be made with the title + body + answer sequence. Feel free to look into e.g. (1) inventing new tokens (add it to '..\/input\/path-to-bert-folder\/assets\/vocab.txt'), (2) keeping \\[SEP\\] between title and body but modify `_get_segments()`, (3) using the \\[PAD\\] token, or (4) merging title and body without any kind of separation. In this commit I'm doing (2). I also tried (3) offline, and they both perform better than in commit 8, in terms of validation rho.*<br>\n\n* ignoring first \\[SEP\\] token in `_get_segments()`.\n\n---\n\n**Update 4 (Commit 11)**\n* **Now using Huggingface transformer instead of TFHub** (note major changes in the code). This creates the possibility to easily try out different architectures like XLNet, Roberta etc. As well as easily outputting the hidden states of the transformer.\n* two separate inputs (title+body and answer) for BERT\n* removed snapshot average (now only using last (third) epoch). This will likely decrease performance, but it's not feasible to use ~ 5 x 4 models for a single bert prediction in practice. \n* only training for 2 epochs instead of 3 (to manage 2h limit)\n---","8f6a79ed":"Count-based features","b493d602":"#### 5. Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-\/loss-function. ","4a72475e":"#### 2. Preprocessing functions\n\nThese are some functions that will be used to preprocess the raw text data into useable Bert inputs.<br>\n\n*update 4:* credits to [Minh](https:\/\/www.kaggle.com\/dathudeptrai) for this implementation. If I'm not mistaken, it could be used directly with other Huggingface transformers too! Note that due to the 2 x 512 input, it will require significantly more memory when finetuning BERT.","27bc32e6":"#### 6. Process and submit test predictions\n\nAverage fold predictions, then save as `submission.csv`"}}