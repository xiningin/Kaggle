{"cell_type":{"80dff63a":"code","749c4378":"code","a5eb17cc":"code","142b364a":"code","f0ca8ee9":"code","3ccd7383":"code","44194a3f":"code","c8b73c65":"code","f80a51b0":"code","985916aa":"code","a86f4af2":"code","bba33f6c":"code","77e90429":"code","57a2dc21":"code","f7ca0fa0":"code","a92f6c4f":"code","d96eecb6":"code","173249e5":"code","1a9b1959":"code","d270d887":"code","7e274b57":"code","1769bd41":"code","d7ac75dd":"code","96b2e8cc":"markdown","7ec52b0f":"markdown"},"source":{"80dff63a":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","749c4378":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n","a5eb17cc":"train=pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_features.csv\")\ntrain","142b364a":"# train.isnull().sum(axis=0).sum(), train.isnull().sum(axis=1).sum()","f0ca8ee9":"# train.nunique(dropna=False).sort_values()","3ccd7383":"# # This code is used to check duplicate columns (if any). It runs for a long time: the result is None, so avoid running this cell\n\n# train_factorized = pd.DataFrame(index=train.index)\n# for col in tqdm.notebook.tqdm(train.columns):\n#     train_factorized[col] = train[col].map(train[col].value_counts())\n\n\n# dup_cols = {}\n\n# for i, c1 in enumerate(tqdm_notebook(train_factorized.columns)):\n#     for c2 in train_factorized.columns[i + 1:]:\n#         if c2 not in dup_cols and np.all(train_factorized[c1] == train_factorized[c2]):\n#             dup_cols[c2] = c1\n            \n# dup_cols","44194a3f":"# # Check for classes distribution\ntrain_target = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")\n# limit = 0\n# for col in tqdm_notebook(train_target.columns):\n#     if col != \"sig_id\":\n#         print(train_target[col].value_counts())\n#     limit+=1\n#     if limit >= 15:\n#         break","c8b73c65":"ctlVehicle_idx = train[\"cp_type\"] != \"ctl_vehicle\"\ntrain = train.loc[ctlVehicle_idx].reset_index(drop=True)\ntrain = train.drop(\"cp_type\", axis=1)\ntrain_target = train_target.loc[ctlVehicle_idx].reset_index(drop=True)","f80a51b0":"gcols = [g for g in train.columns if \"g-\" in g]\nccols = [c for c in train.columns if \"c-\" in c]\ncpcols = [cp for cp in train.columns if \"cp_\" in cp]","985916aa":"train","a86f4af2":"train_target","bba33f6c":"from sklearn.preprocessing import LabelEncoder\n\ntest = pd.read_csv(\"\/kaggle\/input\/lish-moa\/test_features.csv\")\nctlVehicle_test = test[\"cp_type\"] == \"ctl_vehicle\"\ntest = test.drop(\"cp_type\", axis=1)\n\n# enc = LabelEncoder()\n# for col in train[cpcols]:\n#     train[col] = enc.fit_transform(train[col])\n    \n# enc = LabelEncoder()\n# for col in test[cpcols]:\n#     test[col] = enc.fit_transform(test[col])\ntrain[\"cp_time\"]=train[\"cp_time\"].map({24: 1, 48: 2, 72: 3})\ntrain[\"cp_dose\"] = train[\"cp_dose\"].map({\"D1\": 1, \"D2\": 2})\ntest[\"cp_time\"]= test[\"cp_time\"].map({24: 1, 48: 2, 72: 3})\ntest[\"cp_dose\"]=test[\"cp_dose\"].map({\"D1\": 1, \"D2\": 2})\ntrain","77e90429":"test","57a2dc21":"for col in train.iloc[:, 3:].columns:\n    percent = train[col].quantile([0.01, 0.99]).values\n    train[col] = np.clip(train[col], percent[0], percent[1])\n\nfor col in test.iloc[:, 3:].columns:\n    percent = test[col].quantile([0.01, 0.99]).values\n    test[col] = np.clip(test[col], percent[0], percent[1])","f7ca0fa0":"from sklearn.decomposition import PCA\n\ng_pca = PCA(n_components=0.99)\nc_pca = PCA(n_components=0.99)\n\ntrain_test_g_concat = pd.concat([train[gcols], test[gcols]], axis=0)\ntrain_test_c_concat = pd.concat([train[ccols], test[ccols]], axis=0)\ng_pca.fit(train_test_g_concat)\nc_pca.fit(train_test_c_concat)\n\ntrain_gtrans = pd.DataFrame(g_pca.transform(train[gcols]), columns=[\"g_PCA\" + str(i) for i in range(g_pca.n_components_)], index=train.index)\ntest_gtrans = pd.DataFrame(g_pca.transform(test[gcols]), columns=[\"g_PCA\" + str(i) for i in range(g_pca.n_components_)], index=test.index)\n\ntrain_ctrans = pd.DataFrame(c_pca.transform(train[ccols]), columns=[\"c_PCA\" + str(i) for i in range(c_pca.n_components_)], index=train.index)\ntest_ctrans = pd.DataFrame(c_pca.transform(test[ccols]), columns=[\"c_PCA\" + str(i) for i in range(c_pca.n_components_)], index=test.index)\n\ng_pca.n_components_, c_pca.n_components_","a92f6c4f":"train = pd.concat([train_gtrans, train_ctrans, train[cpcols]], axis=1)\ntest = pd.concat([test_gtrans, test_ctrans, test[cpcols]], axis=1)\ntrain","d96eecb6":"test","173249e5":"train_target = train_target.drop(\"sig_id\", axis=1)\ntrain_target","1a9b1959":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow_addons as tfa\n\ndef create_shallow_model():\n    model = tf.keras.Sequential([\n        tfa.layers.WeightNormalization(L.Dense(train.shape[1], input_shape=(train.shape[1],))),\n        L.BatchNormalization(),\n        L.Dropout(0.2),\n        tfa.layers.WeightNormalization(L.Dense(128, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.2),\n        tfa.layers.WeightNormalization(L.Dense(train_target.shape[1], activation=\"sigmoid\"))\n    ])\n    \n    sgd = tf.keras.optimizers.SGD()\n    adamw = tfa.optimizers.AdamW(weight_decay = 0.0001)\n    adam = tf.keras.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead_radam = tfa.optimizers.Lookahead(radam)\n    lookahead_adamw = tfa.optimizers.Lookahead(adamw)\n    \n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-6), optimizer=adam)\n    return model\n\ndef create_mid_model():\n    model = tf.keras.Sequential([\n        tfa.layers.WeightNormalization(L.Dense(train.shape[1], input_shape=(train.shape[1],))),\n        L.BatchNormalization(),\n        tfa.layers.WeightNormalization(L.Dense(512, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.3),\n        tfa.layers.WeightNormalization(L.Dense(512, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.3),\n        tfa.layers.WeightNormalization(L.Dense(train_target.shape[1], activation=\"sigmoid\"))\n    ])\n    \n    sgd = tf.keras.optimizers.SGD()\n    adamw = tfa.optimizers.AdamW(weight_decay = 0.0001)\n    adam = tf.keras.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead_radam = tfa.optimizers.Lookahead(radam)\n    lookahead_adamw = tfa.optimizers.Lookahead(adamw)\n    \n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-6), optimizer=lookahead_adamw)\n    return model\n\ndef create_deep_model():\n    model = tf.keras.Sequential([\n        tfa.layers.WeightNormalization(L.Dense(train.shape[1], input_shape=(train.shape[1],))),\n        L.BatchNormalization(),\n        tfa.layers.WeightNormalization(L.Dense(512, activation=\"selu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.42),\n        tfa.layers.WeightNormalization(L.Dense(512, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.42),\n        tfa.layers.WeightNormalization(L.Dense(256, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.42),\n        tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.1),\n        tfa.layers.WeightNormalization(L.Dense(train_target.shape[1], activation=\"sigmoid\"))\n    ])\n    \n    sgd = tf.keras.optimizers.SGD()\n    adamw = tfa.optimizers.AdamW(weight_decay = 0.0001)\n    adam = tf.keras.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead_radam = tfa.optimizers.Lookahead(radam)\n    lookahead_adamw = tfa.optimizers.Lookahead(adamw)\n    \n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-6), optimizer=lookahead_radam)\n    return model","d270d887":"# from sklearn.model_selection import KFold\n\npredictions = []\nkf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ntf.random.set_seed(42)\nfor fold_id, (train_idx, valid_idx) in enumerate(kf.split(train, train_target)):\n    model1 = create_shallow_model()\n    model2 = create_mid_model()\n    model3 = create_deep_model()\n    \n    history1 = model1.fit(train.iloc[train_idx], train_target.iloc[train_idx], batch_size=32,\n              validation_data=(train.iloc[valid_idx], train_target.iloc[valid_idx]),\n             epochs=100,\n             verbose=2,\n             callbacks=[\n    tf.keras.callbacks.ReduceLROnPlateau(),\n    tf.keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True),\n    tf.keras.callbacks.ModelCheckpoint(\"model1_fold\" + str(fold_id) + \".h5\", save_best_only=True, save_weights_only=True)\n])\n    print(\"Model 1, Fold ID: {}, train loss: {}, valid loss: {}\".format(fold_id, min(history1.history[\"loss\"]), min(history1.history[\"val_loss\"])))\n    history2 = model2.fit(train.iloc[train_idx], train_target.iloc[train_idx], batch_size=32,\n              validation_data=(train.iloc[valid_idx], train_target.iloc[valid_idx]),\n             epochs=100,\n             verbose=2,\n             callbacks=[\n    tf.keras.callbacks.ReduceLROnPlateau(),\n    tf.keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True),\n    tf.keras.callbacks.ModelCheckpoint(\"model2_fold\" + str(fold_id) + \".h5\", save_best_only=True, save_weights_only=True)\n])\n    print(\"Model 2, Fold ID: {}, train loss: {}, valid loss: {}\".format(fold_id, min(history2.history[\"loss\"]), min(history2.history[\"val_loss\"])))\n    history3 = model3.fit(train.iloc[train_idx], train_target.iloc[train_idx], batch_size=32,\n              validation_data=(train.iloc[valid_idx], train_target.iloc[valid_idx]),\n             epochs=100,\n             verbose=2,\n             callbacks=[\n    tf.keras.callbacks.ReduceLROnPlateau(),\n    tf.keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True),\n    tf.keras.callbacks.ModelCheckpoint(\"model3_fold\" + str(fold_id) + \".h5\", save_best_only=True, save_weights_only=True)\n])\n    \n    print(\"Model 3, Fold ID: {}, train loss: {}, valid loss: {}\".format(fold_id, min(history3.history[\"loss\"]), min(history3.history[\"val_loss\"])))\n    \n    model1.load_weights(\"model1_fold\" + str(fold_id) + \".h5\")\n    model2.load_weights(\"model2_fold\" + str(fold_id) + \".h5\")\n    model3.load_weights(\"model3_fold\" + str(fold_id) + \".h5\")\n    pred1 = model1.predict(test)\n    pred2 = model2.predict(test)\n    pred3 = model3.predict(test)\n    predictions.append(np.average([pred1, pred2, pred3], weights=[0.15, 0.7, 0.15], axis=0))","7e274b57":"pred = np.mean(predictions, axis=0)\npred = np.clip(pred, 0.001, 0.999)\npred.shape","1769bd41":"sub = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")\nsub.loc[:, 1:] = pred\nsub.loc[ctlVehicle_test, sub.columns != \"sig_id\"] = 0\n\n# sub.loc[:, 1:] = tf.keras.utils.normalize(pred)\nsub","d7ac75dd":"sub.to_csv(\"submission.csv\", index=False)","96b2e8cc":"# Current best version: 12","7ec52b0f":"- train_features.csv - Features for the training set. Features `g-` signify gene expression data, and `c-` signify cell viability data. `cp_type` indicates samples treated with a compound (`cp_vehicle`) or with a control perturbation (`ctrl_vehicle`); control perturbations have no MoAs; `cp_time` and `cp_dose` indicate treatment duration (24, 48, 72 hours) and `dose` (high or low).\n- train_targets_scored.csv - The binary MoA targets that are scored.\n- train_targets_nonscored.csv - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\n- test_features.csv - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.\n- sample_submission.csv - A submission file in the correct format."}}