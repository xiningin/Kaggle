{"cell_type":{"335f637d":"code","7aa42804":"code","a9057c65":"code","7f0639c1":"code","54bfed3a":"code","103a6e60":"code","746f8212":"code","9407df1c":"code","6160453c":"code","a2a7457f":"code","dd9faab7":"code","d09a8f04":"code","6c39d2a5":"code","802fc183":"code","d9e49215":"code","6e7987be":"code","1a866d5d":"code","77fd6045":"code","fc9f79f4":"code","820e0c00":"code","ddb9cf55":"code","8243f6a4":"code","0362a805":"code","d3436c48":"code","52fc945f":"code","f5bb4f0a":"code","9ff0cc99":"code","40371134":"code","78bd71a7":"code","f8216af1":"code","b8e95092":"code","9e228df4":"code","af5f42d1":"code","44e0bf86":"code","24f537e5":"code","3a77aa70":"code","89fa8aea":"code","da842cf9":"code","8a4e7700":"code","bff47de3":"code","463f4392":"code","7f987da8":"code","cad53ac0":"code","38b6902b":"code","80f3dec6":"code","af2fdc49":"code","d5027a1a":"code","75ec539b":"code","bf831fd8":"code","8aa81351":"markdown","1b4a2550":"markdown","21ef823e":"markdown"},"source":{"335f637d":"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","7aa42804":"#\u0110\u1ecdc file \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a9057c65":"#Gi\u1ea3i n\u00e9n d\u1eef li\u1ec7u \n!apt-get install p7zip\n!p7zip -d -f -k \/kaggle\/input\/mercari-price-suggestion-challenge\/train.tsv.7z\n!p7zip -d -f -k \/kaggle\/input\/mercari-price-suggestion-challenge\/test.tsv.7z\n!p7zip -d -f -k \/kaggle\/input\/mercari-price-suggestion-challenge\/sample_submission.csv.7z","7f0639c1":"#Gi\u1ea3i n\u00e9n d\u1eef li\u1ec7u \n!unzip \/kaggle\/input\/mercari-price-suggestion-challenge\/sample_submission_stg2.csv.zip\n!unzip \/kaggle\/input\/mercari-price-suggestion-challenge\/test_stg2.tsv.zip","54bfed3a":"#D\u00f9ng pd \u0111\u1ec3 \u0111\u1ecdc \ntrain_data = pd.read_csv('train.tsv', sep='\\t')\ntrain_data.head(5)","103a6e60":"#\u0111\u1ebfm t\u1ed5ng c\u00e1c nh\u00e3n c\u00f3 gi\u00e1 tr\u1ecb null c\u1ee7a t\u1eadp train. Nh\u1eadn th\u1ea5y c\u00f3 r\u1ea5t nhi\u1ec1u gi\u00e1 tr\u1ecb r\u1ed7ng c\u1ee7a nh\u00e3n brand_name v\u00e0 m\u1ed9t s\u1ed1 \u00edt \u1edf category_name\ntrain_data.isnull().sum()","746f8212":"#ki\u1ec3m tra s\u1ed1 chi\u1ec1u c\u1ee7a ma tr\u1eadn v\u00e0 c\u00e1c nh\u00e3n\nprint(train_data.shape)\nprint(train_data.columns)","9407df1c":"#l\u1ecdc c\u00e1c s\u1ea3n ph\u1ea9m c\u00f3 gi\u00e1 > 0 \ntrain_data = train_data[train_data['price'] > 0].reset_index(drop=True)\ntrain_data,validation_data=train_test_split(train_data,test_size=0.1,random_state=42)\nprint(train_data.shape)\nprint(validation_data.shape)","6160453c":"# Ki\u1ec3m tra c\u00e1c ph\u1ea7n t\u1eed null c\u1ee7a t\u1eadp train\ntrain_data.isnull().sum()","a2a7457f":"# Ki\u1ec3m tra c\u00e1c ph\u1ea7n t\u1eed null c\u1ee7a t\u1eadp validation\nvalidation_data.isnull().sum()","dd9faab7":"train = train_data.copy()\nvalid = validation_data.copy()","d09a8f04":"#chia category_name th\u00e0nh c\u00e1c sub_category \u0111\u1ec3 ph\u00e2n chia th\u00f4ng tin r\u00f5 r\u00e0ng h\u01a1n, \ndef split_categories(category):\n    try:\n      sub_category1,sub_category2,sub_category3 = category.split(\"\/\")\n      return sub_category1,sub_category2,sub_category3\n    except:\n      return (\"No label\",\"No label\",\"No label\")\n\ndef create_split_categories(data):\n    data['sub_category1'],data['sub_category2'],data['sub_category3']=zip(*data['category_name'].\\\n                                                                  apply(lambda x: split_categories(x)))","6c39d2a5":"create_split_categories(train_data)\ncreate_split_categories(validation_data)","802fc183":"#thay th\u1ebf c\u00e1c gi\u00e1 tr\u1ecb null b\u1eb1ng c\u00e1c gi\u00e1 tr\u1ecb unknow_cat, unknow_brand v\u00e0 unknow_description\ndef fill_missing_values(data):\n    data['category_name'].fillna('unknown_cat', inplace=True)\n    data['brand_name'].fillna('unknown_brand', inplace=True)\n    data['item_description'].fillna('unknown_description', inplace=True)\n    return data","d9e49215":"fill_missing_values(train_data)\nfill_missing_values(validation_data)","6e7987be":"test_data = pd.read_csv('test_stg2.tsv',sep='\\t')\ntest = test_data.copy()","1a866d5d":"#5 h\u00e0ng d\u1eef li\u1ec7u \u0111\u1ea7u ti\u00ean c\u1ee7a t\u1ec7p test\ntest_data.head(5)","77fd6045":"#t\u1ec7p test g\u1ed3m 3460725 h\u00e0ng v\u00e0 7 c\u1ed9t\ntest_data.shape","fc9f79f4":"#T\u1ec7p test c\u00f3 gi\u00e1 tr\u1ecb null \u1edf h\u00e0ng brand_name l\u00e0 l\u1edbn nh\u1ea5t, ti\u1ebfp theo \u0111\u00f3 l\u00e0 category_name.\ntest_data.isnull().sum()","820e0c00":"#t\u00e1ch categories cho t\u00eap test v\u00e0 thay th\u1ebf c\u00e1c gi\u00e1 tr\u1ecb NaN th\u00e0nh unknow...\ncreate_split_categories(test_data)\nfill_missing_values(test_data)","ddb9cf55":"#t\u00ednh logarit gi\u00e1 s\u1ea3n ph\u1ea9m c\u1ee7a t\u1eadp train \u0111\u1ec3 c\u1ed1 g\u1eafng \u0111\u01b0a v\u1ec1 ph\u00e2n ph\u1ed1i chu\u1ea9n\ntrain_data['log_prices']= np.log(train_data['price']+1)","8243f6a4":"#t\u00ednh logarit gi\u00e1 s\u1ea3n ph\u1ea9m c\u1ee7a t\u1eadp valid \u0111\u1ec3 c\u1ed1 g\u1eafng \u0111\u01b0a v\u1ec1 ph\u00e2n ph\u1ed1i chu\u1ea9n\nvalidation_data['log_prices']= np.log(validation_data['price']+1)","0362a805":"#v\u1ebd \u0111\u1ed3 th\u1ecb bi\u1ec3u di\u1ec5n gi\u00e1 tr\u1ecb logarit c\u1ee7a gi\u00e1\nsns.kdeplot(data=train_data['log_prices'])\nplt.title(\"Distribution of Price variable\")\nplt.grid(color='b')","d3436c48":"train_data.head(5)","52fc945f":"#import th\u01b0 vi\u1ec7n NLTK \u0111\u1ec3 x\u00f3a stopword\nimport nltk\nnltk.download('stopwords')","f5bb4f0a":"#x\u00f3a stop words (a, an, the,...) \nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\n\ndef remove_stop_words(x):\n    x = ' '.join([i for i in x.lower().split(' ') if i not in stop])\n    return x","9ff0cc99":"#x\u00f3a stopwords trong c\u00e1c t\u1ec7p train, valid v\u00e0 test\ntrain_data['item_description'] = train_data['item_description'].apply(remove_stop_words)\nvalidation_data['item_description'] = validation_data['item_description'].apply(remove_stop_words)\ntest_data['item_description'] = test_data['item_description'].apply(remove_stop_words)","40371134":"#Ti\u1ec1n x\u1eed l\u00fd, chuy\u1ec3n nh\u1eefng k\u00fd hi\u1ec7u vi\u1ebft t\u1eaft th\u00e0nh c\u00e1c t\u1eeb ho\u00e0n ch\u1ec9nh, v\u00e0 ch\u1ec9nh to\u00e0n b\u1ed9 c\u00e1c ch\u1eef in hoa th\u00e0nh in th\u01b0\u1eddng. thay th\u1ebf c\u00e1c k\u00fd t\u1ef1 \\r, \\n, \\\\ \n#th\u00e0nh c\u00e1c k\u00fd t\u1ef1 r\u1ed7ng\nfrom tqdm import tqdm\nimport re\n\ndef decontracted(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndef text_preprocessing(text_col):\n  preprocessed_total = []\n  for sentence in tqdm(text_col.values):\n    sent = decontracted(sentence)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    preprocessed_total.append(sent.lower().strip())\n  return preprocessed_total","78bd71a7":"#\u00e1p d\u1ee5ng c\u00e1c h\u00e0m ti\u1ec1n x\u1eed l\u00fd \u0111\u00e3 vi\u1ebft \u1edf b\u00ean tr\u00ean cho c\u00e1c t\u1ec7p train, validation, test cho c\u00e1c tr\u01b0\u1eddng description v\u00e0 name.\ntrain_data['item_description']=text_preprocessing(train_data['item_description'])\nvalidation_data['item_description']=text_preprocessing(validation_data['item_description'])\ntest_data['item_description']=text_preprocessing(test_data['item_description'])\n\ntrain_data['name']=text_preprocessing(train_data['name'])\nvalidation_data['name']=text_preprocessing(validation_data['name'])\ntest_data['name']=text_preprocessing(test_data['name'])","f8216af1":"#so s\u00e1nh gi\u1eefa t\u1eadp train_data sau khi ti\u1ec1n x\u1eed l\u00fd v\u00e0 t\u1eadp train ch\u01b0a \u0111\u01b0\u1ee3c x\u1eed l\u00fd. Ta c\u00f3 th\u1ec3 th\u1ea5y, nh\u1eefng k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t \u0111\u00e3 \u0111\u01b0\u1ee3c x\u00f3a, k\u00fd th\u1ef1c vi\u1ebft hoa s\u1ebd th\u00e0nh \n# vi\u1ebft th\u01b0\u1eddng.\nprint(train_data['item_description'].iloc[33],len(train_data['item_description'].iloc[33].split(' ')))\nprint(train['item_description'].iloc[33],len(train['item_description'].iloc[33].split(' ')))","b8e95092":"#h\u00e0m x\u1eed l\u00fd d\u1eef li\u1ec7u\ndef clean_cat(cat_col):\n    cat_list = []\n    for i in tqdm(cat_col.values):\n        i = re.sub('[^A-Za-z0-9]+', ' ', i)\n        i = i.replace(' ','')\n        i = i.replace('&','_')\n        cat_list.append(i.strip())\n    \n    return cat_list","9e228df4":"#s\u1eed d\u1ee5ng h\u00e0m x\u1eed l\u00fd d\u1eef li\u1ec7u b\u00ean tr\u00ean \u0111\u1ec3 l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u cho categories cho 3 t\u1ec7p train_data, validation_data v\u00e0 test_data\ntrain_data['sub_category1'] = clean_cat(train_data['sub_category1'])\nvalidation_data['sub_category1'] = clean_cat(validation_data['sub_category1'])\ntest_data['sub_category1'] = clean_cat(test_data['sub_category1'])\n\ntrain_data['sub_category2'] = clean_cat(train_data['sub_category2'])\nvalidation_data['sub_category2'] = clean_cat(validation_data['sub_category2'])\ntest_data['sub_category2'] = clean_cat(test_data['sub_category2'])\n\ntrain_data['sub_category3'] = clean_cat(train_data['sub_category3'])\nvalidation_data['sub_category3'] = clean_cat(validation_data['sub_category3'])\ntest_data['sub_category3'] = clean_cat(test_data['sub_category3'])","af5f42d1":"#s\u1eed d\u1ee5ng h\u00e0m x\u1eed l\u00fd d\u1eef li\u1ec7u b\u00ean tr\u00ean \u0111\u1ec3 l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u cho brand_name cho 3 t\u1ec7p train_data, validation_data v\u00e0 test_data\ntrain_data['brand_name'] = clean_cat(train_data['brand_name'])\nvalidation_data['brand_name'] = clean_cat(validation_data['brand_name'])\ntest_data['brand_name'] = clean_cat(test_data['brand_name'])","44e0bf86":"#\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncountvectorizer=CountVectorizer().fit(train_data['sub_category1'])                 \nbow_cat1_train=countvectorizer.transform(train_data['sub_category1'])\nbow_cat1_val=countvectorizer.transform(validation_data['sub_category1'])\nbow_cat1_test=countvectorizer.transform(test_data['sub_category1'])\nprint(\"Shape of sub_category1 features:\")\nprint(bow_cat1_train.shape)\nprint(bow_cat1_val.shape)\nprint(bow_cat1_test.shape)\nprint(\"Some Features are:\")\nprint(countvectorizer.get_feature_names())","24f537e5":"countvectorizer=CountVectorizer().fit(train_data['sub_category2'])   \nbow_cat2_train=countvectorizer.transform(train_data['sub_category2'])\nbow_cat2_val=countvectorizer.transform(validation_data['sub_category2'])\nbow_cat2_test=countvectorizer.transform(test_data['sub_category2'])\nprint(\"Shape of sub_category2 features:\")\nprint(bow_cat2_train.shape)\nprint(bow_cat2_val.shape)\nprint(bow_cat2_test.shape)\nprint(\"Some Features are: \")\nprint(countvectorizer.get_feature_names()[50:60])","3a77aa70":"countvectorizer=CountVectorizer().fit(train_data['sub_category3'])   \nbow_cat3_train=countvectorizer.transform(train_data['sub_category3'])\nbow_cat3_val=countvectorizer.transform(validation_data['sub_category3'])\nbow_cat3_test=countvectorizer.transform(test_data['sub_category3'])\n(\"Shape of sub_category3 features:\")\nprint(bow_cat3_train.shape)\nprint(bow_cat3_val.shape)\nprint(bow_cat3_test.shape)\nprint(\"Some Features are: \")\nprint(countvectorizer.get_feature_names()[200:210])","89fa8aea":"countvectorizer=CountVectorizer().fit(train_data['brand_name'])  \nbow_brand_train=countvectorizer.transform(train_data['brand_name'])\nbow_brand_val=countvectorizer.transform(validation_data['brand_name'])\nbow_brand_test=countvectorizer.transform(test_data['brand_name'])\n(\"Shape of brand_name features:\")\nprint(bow_brand_train.shape)\nprint(bow_brand_val.shape)\nprint(bow_brand_test.shape)\nprint(\"Some Features are: \")\nprint(countvectorizer.get_feature_names()[35:45])","da842cf9":"countvectorizer=CountVectorizer(min_df=10).fit(train_data['name'])  \nbow_name_train=countvectorizer.transform(train_data['name'])\nbow_name_val=countvectorizer.transform(validation_data['name'])\nbow_name_test=countvectorizer.transform(test_data['name'])\nprint(\"After Vectorization of name feature: \")\nprint(bow_name_train.shape)\nprint(bow_name_val.shape)\nprint(bow_name_test.shape)\nprint(\"Some Features are: \")\nprint(countvectorizer.get_feature_names()[210:220])","8a4e7700":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidfvectorizer=TfidfVectorizer(ngram_range=(1,2),min_df=10,max_features=5000).fit(train_data['item_description']) \ntfidf_description_train=tfidfvectorizer.transform(train_data['item_description'])\ntfidf_description_val=tfidfvectorizer.transform(validation_data['item_description'])\ntfidf_description_test=tfidfvectorizer.transform(test_data['item_description'])\nprint(\"After Vectorization of item description feature: \")\nprint(tfidf_description_train.shape)\nprint(tfidf_description_val.shape)\nprint(tfidf_description_test.shape)\nprint(\"Some Features are: \")\nprint(tfidfvectorizer.get_feature_names()[222:234])","bff47de3":"from scipy.sparse import csr_matrix\n\nfeatures_train = csr_matrix(pd.get_dummies(train_data[['item_condition_id', 'shipping']],sparse=True).values)\nfeatures_val = csr_matrix(pd.get_dummies(validation_data[['item_condition_id', 'shipping']],sparse=True).values)\nfeatures_test = csr_matrix(pd.get_dummies(test_data[['item_condition_id', 'shipping']],sparse=True).values)\nprint(features_train.shape)\nprint(features_val.shape)\nprint(features_test.shape)","463f4392":"from scipy.sparse import hstack\nX_train=hstack((bow_cat1_train,bow_cat2_train,bow_cat3_train,bow_brand_train,bow_name_train,tfidf_description_train,features_train)).tocsr()\nX_val=hstack((bow_cat1_val,bow_cat2_val,bow_cat3_val,bow_brand_val,bow_name_val,tfidf_description_val,features_val)).tocsr()\nX_test=hstack((bow_cat1_test,bow_cat2_test,bow_cat3_test,bow_brand_test,bow_name_test,tfidf_description_test,features_test)).tocsr()\nprint(\"Shape of train data: \",X_train.shape) \nprint(\"Shape of cv data: \",X_val.shape)   \nprint(\"Shape of test data: \",X_test.shape)   ","7f987da8":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n\nlinearregression=LinearRegression(normalize=True)\nlinearregression.fit(X_train,train_data['log_prices'])  \nytrain_predict=linearregression.predict(X_train)\nyval_predict=linearregression.predict(X_val)\ntrain_error=np.sqrt(mean_squared_log_error(train_data['log_prices'],ytrain_predict))\nval_error=np.sqrt(mean_squared_log_error(validation_data['log_prices'],yval_predict))\nprint(\" Linear Regression RMSLE on train is {} RMSLE on cv is {}\".format(train_error,val_error))\n\n","cad53ac0":"yval_linear=linearregression.predict(X_val)\nytest_linear=linearregression.predict(X_test)","38b6902b":"submission_data = pd.read_csv('sample_submission_stg2.csv')\nsubmission_data.head(5)","80f3dec6":"submission_data.shape","af2fdc49":"submission_data.loc[:, 'price'] = np.expm1(ytest_linear)","d5027a1a":"submission_data.head(5)","75ec539b":"submission_data.to_csv('submission.csv', index=False)","bf831fd8":"# train_data = train[['name', 'price', 'item_condition_id', 'brand_name', 'shipping', 'item_description', 'cat_1', 'cat_2', 'cat_3']]\n# test = test[['name', 'item_condition_id', 'brand_name', 'shipping', 'item_description', 'cat_1', 'cat_2', 'cat_3']]","8aa81351":"#**Vectorization**","1b4a2550":"#**Model 1:Linear regression**","21ef823e":"#**Text processing**"}}