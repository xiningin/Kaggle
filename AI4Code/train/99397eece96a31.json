{"cell_type":{"eb641c32":"code","818406d7":"code","ae658642":"code","b85d2ec8":"code","d5d987a9":"code","81caf93b":"code","4b435fb8":"code","560fd93d":"code","a6e3fb4e":"code","3cf8fc11":"code","b1963f20":"code","bd8ae0af":"code","0673e5b8":"code","45dd5f1f":"code","a411b818":"code","c869cc2f":"code","015cb175":"code","b6d6c162":"code","174ee997":"code","1118d462":"code","c264ec38":"code","364c1492":"code","d789d850":"code","bddfa907":"code","711ea301":"code","a680aef0":"code","23a51c13":"code","b53f1482":"code","69930bef":"code","d82c0857":"code","16f6af02":"code","4867d679":"code","c48df367":"code","8693dddc":"code","615175c4":"code","75a2ba3f":"code","919d0b94":"code","154f627e":"code","0728d0a6":"code","d1506697":"code","6a5b3361":"code","bc680cde":"code","2653ba33":"code","e56dff4c":"code","304c5847":"code","31d31cfe":"code","11a93ee4":"code","646bd8c1":"code","24ea7cd6":"code","dabf96df":"code","1f06e421":"code","cca75dc5":"code","317e03f1":"code","80f5a133":"code","47a00ac6":"code","75b3d153":"code","d9a6c648":"code","00e9eb19":"code","0e48d57e":"code","80b327dc":"code","b372ca09":"code","91e6c7cb":"code","63dc4a3c":"code","a6fb59fc":"code","6380a474":"code","abefa95a":"code","e4e0b688":"code","9c8fd7ae":"code","56add65e":"code","2aba16b2":"code","68cb6289":"code","a53de017":"code","fde80abf":"code","47e63afa":"code","fcf07cf3":"code","b1643cc3":"code","5c359b0a":"code","7bee5b73":"code","baae469f":"code","1ec9e9a3":"code","1dbbc55d":"code","87357cae":"code","8eb109bf":"code","43b8e9bb":"code","7810e3fb":"code","f0b3759e":"code","7147de0a":"code","19832641":"code","0217c2b6":"code","f32cfcb7":"code","cedbdad9":"code","2954536e":"code","e9e95102":"code","221ce1c1":"code","6b3aa2c5":"code","82a57b96":"code","40253835":"code","3a32a1fe":"code","800d1f80":"code","c6952b46":"code","fae95b9d":"code","94d605d4":"code","bef4aad5":"code","cabbf84d":"code","9a89ef87":"code","7a48a3e6":"code","e7c61ce9":"code","d49c5def":"code","095b4d17":"code","2db6483f":"code","5e47ce15":"code","cf154cb1":"code","6043019b":"code","da6667a1":"code","e2a12fce":"code","43f3db29":"code","999b51db":"code","9e84a5df":"code","96a64c0e":"code","fab33be5":"code","f3559abd":"code","076b3e5b":"code","b62f1897":"code","97431c9b":"code","45eb7982":"markdown","392ec28f":"markdown","2096d91e":"markdown","b37ba131":"markdown","28460478":"markdown","238f274f":"markdown","327212bc":"markdown","b21d8eae":"markdown","5fe5ff46":"markdown","946b5483":"markdown","774e665e":"markdown","1f6b0b34":"markdown","a50b77f7":"markdown","8e7bf0fb":"markdown","5a083e49":"markdown","93a76d41":"markdown","af4296cb":"markdown","e92907ff":"markdown","91e58008":"markdown","c96c0dbf":"markdown","490567af":"markdown","a9ac22bf":"markdown","ea6b7451":"markdown","c2200031":"markdown","1c5133a8":"markdown","e95518dd":"markdown","96fe8131":"markdown","a3b49222":"markdown","71f9b5be":"markdown","ea5bbfe6":"markdown","2ea2f180":"markdown","f6a87046":"markdown","c49eb8bb":"markdown","c4cf5b8a":"markdown","4ff57c04":"markdown","beadab2f":"markdown","25156259":"markdown","a1a91d99":"markdown","4d39ec9a":"markdown","4e89fd52":"markdown","c67ba584":"markdown","2f04fe7f":"markdown","210b8bcc":"markdown","1517f9f7":"markdown","8638cfd7":"markdown","4c7fac62":"markdown","c8da4839":"markdown","f6491dd7":"markdown","6685fcc0":"markdown","b01f1651":"markdown","d6db4356":"markdown","1b3ac81c":"markdown","2b5f83b7":"markdown","87bb22e0":"markdown","f7fe982e":"markdown","88a297b9":"markdown","8928684d":"markdown","4f1a6e00":"markdown","e7e167db":"markdown"},"source":{"eb641c32":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import wordpunct_tokenize\n# from nltk.tokenize import sent_tokenize (Tokenization)\nfrom nltk.probability import FreqDist\nfrom nltk.metrics import ConfusionMatrix\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV","818406d7":"# predictor\nX_col = 'tweet_text'\n# classifier\ny_col = 'sentiment' ","ae658642":"print(os.listdir('..\/input\/portuguese-tweets-for-sentiment-analysis\/trainingdatasets'))\n# Train3Classes.csv","b85d2ec8":"train_ds = pd.read_csv('..\/input\/portuguese-tweets-for-sentiment-analysis\/trainingdatasets\/Train3Classes.csv', delimiter=';')","d5d987a9":"# update classifiers to nominal value\ntrain_ds[y_col] = train_ds[y_col].map({0: 'Negative', 1: 'Positive', 2: 'Neutral'})","81caf93b":"X_train = train_ds.loc[:, X_col].values\ny_train = train_ds.loc[:, y_col].values","4b435fb8":"# train_ds.head(5)\ntrain_ds.sample(5)","560fd93d":"series = train_ds['sentiment'].value_counts()\nax = series.plot(kind='bar', title='Number for each sentiment')\nax.set_xlabel('Sentiment')\nax.set_ylabel('Count')\nplt.show()","a6e3fb4e":"series = train_ds['query_used'].value_counts()\nax = series.plot(kind='bar', title='Number for each sentiment')\nax.set_xlabel('Sentiment')\nax.set_ylabel('Count')\nplt.show()","3cf8fc11":"# check data\nfor i in range(0, 5):\n    print(y_train[i], ' => ', X_train[i])","b1963f20":"print(os.listdir('..\/input\/portuguese-tweets-for-sentiment-analysis\/testdatasets'))\n# Test3classes.csv","bd8ae0af":"test_ds = pd.read_csv('..\/input\/portuguese-tweets-for-sentiment-analysis\/testdatasets\/Test3classes.csv', delimiter=';')","0673e5b8":"# update classifiers to nominal value\ntest_ds[y_col] = test_ds[y_col].map({0: 'Negative', 1: 'Positive', 2: 'Neutral'})","45dd5f1f":"X_test = test_ds.loc[:, X_col].values\ny_test = test_ds.loc[:, y_col].values","a411b818":"# test_ds.head(5)\ntest_ds.sample(5)","c869cc2f":"series = test_ds['sentiment'].value_counts()\nax = series.plot(kind='bar', title='Number for each sentiment')\nax.set_xlabel('Sentiment')\nax.set_ylabel('Count')\nplt.show()","015cb175":"series = test_ds['query_used'].value_counts()\nax = series.plot(kind='bar', title='Number for each sentiment')\nax.set_xlabel('Sentiment')\nax.set_ylabel('Count')\nplt.show()","b6d6c162":"# check data\nfor i in range(0, 5):\n    print(y_test[i], ' => ', X_test[i])","174ee997":"def _remove_url(data):\n    ls = []\n    words = ''\n    regexp1 = re.compile('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    regexp2 = re.compile('www?.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    \n    for line in data:\n        urls = regexp1.findall(line)\n\n        for u in urls:\n            line = line.replace(u, ' ')\n\n        urls = regexp2.findall(line)\n\n        for u in urls:\n            line = line.replace(u, ' ')\n            \n        ls.append(line)\n    return ls","1118d462":"X_train = _remove_url(X_train)\nX_test = _remove_url(X_test)","c264ec38":"def _remove_regex(data, regex_pattern):\n    ls = []\n    words = ''\n    \n    for line in data:\n        matches = re.finditer(regex_pattern, line)\n        \n        for m in matches: \n            line = re.sub(m.group().strip(), '', line)\n\n        ls.append(line)\n\n    return ls","364c1492":"# hashtags\nregex_pattern = '#[\\w]*'\nX_train = _remove_regex(X_train, regex_pattern)\nX_test = _remove_regex(X_test, regex_pattern)","d789d850":"# notations\nregex_pattern = '@[\\w]*'\nX_train = _remove_regex(X_train, regex_pattern)\nX_test = _remove_regex(X_test, regex_pattern)","bddfa907":"# check data\nfor i in range(0, 5):\n    print(X_train[i])","711ea301":"def _replace_emoticons(data, emoticon_list):\n    ls = []\n\n    for line in data:\n        for exp in emoticon_list:\n            line = line.replace(exp, emoticon_list[exp])\n\n        ls.append(line)\n\n    return ls","a680aef0":"emoticon_list = {':))': 'positive_emoticon', ':)': 'positive_emoticon', ':D': 'positive_emoticon', ':(': 'negative_emoticon', ':((': 'negative_emoticon', '8)': 'neutral_emoticon'}","23a51c13":"X_train = _replace_emoticons(X_train, emoticon_list)\nX_test = _replace_emoticons(X_test, emoticon_list)","b53f1482":"# check data\nfor i in range(0, 5):\n    print(X_train[i])","69930bef":"def _tokenize_text(data):\n    ls = []\n\n    for line in data:\n        tokens = wordpunct_tokenize(line)\n        ls.append(tokens)\n\n    return ls","d82c0857":"X_train_tokens = _tokenize_text(X_train)\nX_test_tokens = _tokenize_text(X_test)","16f6af02":"# check data\nfor i in range(0, 5):\n    print(X_train_tokens[i])","4867d679":"def _apply_standardization(tokens, std_list):\n    ls = []\n\n    for tk_line in tokens:\n        new_tokens = []\n        \n        for word in tk_line:\n            if word.lower() in std_list:\n                word = std_list[word.lower()]\n                \n            new_tokens.append(word) \n            \n        ls.append(new_tokens)\n\n    return ls","c48df367":"# create your own list\nstd_list = {'eh': '\u00e9', 'vc': 'voc\u00ea', 'vcs': 'voc\u00eas','tb': 'tamb\u00e9m', 'tbm': 'tamb\u00e9m', 'obg': 'obrigado', 'gnt': 'gente', 'q': 'que', 'n': 'n\u00e3o', 'cmg': 'comigo', 'p': 'para', 'ta': 'est\u00e1', 'to': 'estou', 'vdd': 'verdade'}","8693dddc":"# check data\nprint(X_train[4], X_train[35008])","615175c4":"X_train_tokens = _apply_standardization(X_train_tokens, std_list)\nX_test_tokens = _apply_standardization(X_test_tokens, std_list)","75a2ba3f":"# check data\nprint(X_train_tokens[4], X_train_tokens[35008])","919d0b94":"print(X_test_tokens[0:5])","154f627e":"def _remove_stopwords(tokens, stopword_list):\n    ls = []\n\n    for tk_line in tokens:\n        new_tokens = []\n        \n        for word in tk_line:\n            if word.lower() not in stopword_list:\n                new_tokens.append(word) \n            \n        ls.append(new_tokens)\n        \n    return ls","0728d0a6":"stopword_list = []","d1506697":"# get nltk portuguese stopwords\nnltk_stopwords = nltk.corpus.stopwords.words('portuguese')","6a5b3361":"# get custom stopwords from a file (pt-br). You can create your own database of stopwords on a text file, mongodb, so on...\ndf = pd.read_fwf('..\/input\/stopwords-ptbr\/stopwords-pt-br.txt', header = None)\ndf.head(10)","bc680cde":"# list of array\ncustom_stopwords = df.values.tolist()\n# transform list of array to list\ncustom_stopwords = [s[0] for s in custom_stopwords]","2653ba33":"# You can also add stopwords manually instead of loading from the database. Generally, we add stopwords that belong to this context.\nstopword_list.append('\u00e9')\nstopword_list.append('vou')\nstopword_list.append('que')\nstopword_list.append('t\u00e3o')\nstopword_list.append('...')\nstopword_list.append('\u00ab')\nstopword_list.append('\u2794')\nstopword_list.append('|')\nstopword_list.append('\u00bb')\nstopword_list.append('uai') # 'expression from the mineiros (MG\/Brazil)'","e56dff4c":"# join all stopwords\nstopword_list.extend(nltk_stopwords)\nstopword_list.extend(custom_stopwords)","304c5847":"# remove duplicate stopwords (unique list)\nstopword_list = list(set(stopword_list))","31d31cfe":"X_train_tokens = _remove_stopwords(X_train_tokens, stopword_list)\nX_test_tokens = _remove_stopwords(X_test_tokens, stopword_list)","11a93ee4":"# check data\nfor i in range(0, 5):\n    print(X_train_tokens[i])","646bd8c1":"def _apply_stemmer(tokens):\n    ls = []\n    stemmer = nltk.stem.RSLPStemmer()\n\n    for tk_line in tokens:\n        new_tokens = []\n        \n        for word in tk_line:\n            word = str(stemmer.stem(word))\n            new_tokens.append(word) \n            \n        ls.append(new_tokens)\n        \n    return ls","24ea7cd6":"X_train_tokens = _apply_stemmer(X_train_tokens)\nX_test_tokens = _apply_stemmer(X_test_tokens)","dabf96df":"# check data\nfor i in range(0, 5):\n    print(X_train_tokens[i])","1f06e421":"def _get_text_cloud(tokens):\n    text = ''\n\n    for tk_line in tokens:\n        new_tokens = []\n        \n        for word in tk_line:\n            text += word + ' '\n        \n    return text","cca75dc5":"# print train WordCloud\nsample_train = random.sample(X_train_tokens, 10000)\ntext_cloud = _get_text_cloud(sample_train)\n\nword_cloud = WordCloud(max_font_size = 100, width = 1520, height = 535)\nword_cloud.generate(text_cloud)\nplt.figure(figsize = (16, 9))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","317e03f1":"# print test WordCloud\nsample_test = random.sample(X_test_tokens, len(X_test_tokens))\ntext_cloud = _get_text_cloud(sample_test)\n\nword_cloud = WordCloud(max_font_size = 100, width = 1520, height = 535)\nword_cloud.generate(text_cloud)\nplt.figure(figsize = (16, 9))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","80f5a133":"def _get_freq_dist_list(tokens):\n    ls = []\n\n    for tk_line in tokens:\n        for word in tk_line:\n            ls.append(word)\n\n    return ls","47a00ac6":"# Frequency Distribution on training dataset\nfd_list = _get_freq_dist_list(X_train_tokens)\nfdist = FreqDist(fd_list)\nprint(fdist)","75b3d153":"# most common words\nmost_common = fdist.most_common(25)\nprint(most_common)","d9a6c648":"# most uncommon words (words that appear once)\nmost_uncommon = fdist.hapaxes()\nprint(most_uncommon[0:30])","00e9eb19":"# find the word occuring max number of times\nfdist.max()","0e48d57e":"# print most common words\nseries = pd.Series(data=[v for k, v in most_common], index=[k for k, v in most_common], name='')\nax = series.plot(kind='bar', title='Frequency Distribution')\nax.set_xlabel('Word')\nax.set_ylabel('Count')\nplt.show()","80b327dc":"# number of times\nfdist.get('brasil')","b372ca09":"# Freq = Number of occurences \/ total number of words\nfdist.freq('brasil') # 2453 \/ 984823","91e6c7cb":"def _untokenize_text(tokens):\n    ls = []\n\n    for tk_line in tokens:\n        new_line = ''\n        \n        for word in tk_line:\n            new_line += word + ' '\n            \n        ls.append(new_line)\n        \n    return ls","63dc4a3c":"X_train = _untokenize_text(X_train_tokens)\nX_test = _untokenize_text(X_test_tokens)","a6fb59fc":"# check data\nfor i in range(0, 5):\n    print(X_train[i])","6380a474":"# create a count vectorizer object\nvectorizer = CountVectorizer()\nX_train_vect = vectorizer.fit_transform(X_train)","abefa95a":"print(X_train_vect.shape)","e4e0b688":"print(vectorizer.vocabulary_.get(u'brasil'))","9c8fd7ae":"tfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_vect)","56add65e":"print(X_train_tfidf.shape)","2aba16b2":"model = MultinomialNB()\nmodel.fit(X_train_tfidf, y_train)","68cb6289":"new_corpus = [\n        '@acme A alegria est\u00e1 na luta, na tentativa, no sofrimento envolvido e n\u00e3o na vit\u00f3ria propriamente dita!', \n        'A alegria evita mil males e prolonga a vida.',\n        'n\u00e3o se deve maltratar os idosos, eles possuem muita sabedoria!',\n        '#filmedevampiro tome muito cuidado com o dracula... :( www.filmedevampiro.com.br'\n        ]","a53de017":"X_new = new_corpus","fde80abf":"# Remove urls from text (http(s), www)\nX_new = _remove_url(X_new)","47e63afa":"# Remove hashtags\nregex_pattern = '#[\\w]*'\nX_new = _remove_regex(X_new, regex_pattern)","fcf07cf3":"# Remove notations\nregex_pattern = '@[\\w]*'\nX_new = _remove_regex(X_new, regex_pattern)","b1643cc3":"# Replace emoticons \":)) :) :D :(\" to positive_emoticon or negative_emoticon or neutral_emoticon\nX_new = _replace_emoticons(X_new, emoticon_list)","5c359b0a":"# Tokenize text\nX_new_tokens = _tokenize_text(X_new)","7bee5b73":"# Object Standardization\nX_new_tokens = _apply_standardization(X_new_tokens, std_list)","baae469f":"# remove stopwords\nX_new_tokens = _remove_stopwords(X_new_tokens, stopword_list)","1ec9e9a3":"# Lemmatization (not implemented...)","1dbbc55d":"# Stemming (dimensionality reduction)\nX_new_tokens = _apply_stemmer(X_new_tokens)","87357cae":"# Dataset preparation\n# Untokenize text (transform tokenized text into string list)\nX_new = _untokenize_text(X_new_tokens)","8eb109bf":"# Text to Features\n# Feature extraction from text \n# Method: bag of words\nX_new_vect = vectorizer.transform(X_new)","43b8e9bb":"print(X_new_vect.shape)","7810e3fb":"print(vectorizer.vocabulary_.get(u'idos'))\nprint(vectorizer.vocabulary_.get(u'alegr'))","f0b3759e":"# TF-IDF: Term Frequency - Inverse Document Frequency\n# use the transform(...) method to transform count-matrix to a tf-idf representation.\nX_new_tfidf = tfidf_transformer.transform(X_new_vect)","7147de0a":"print(X_new_tfidf.shape)","19832641":"standalone_predictions = model.predict(X_new_tfidf)","0217c2b6":"for doc, prediction in zip(new_corpus, standalone_predictions):\n    print('%r => %s' % (doc, prediction))","f32cfcb7":"def _get_accuracy(matrix):\n    acc = 0\n    n = 0\n    total = 0\n    \n    for i in range(0, len(matrix)):\n        for j in range(0, len(matrix)):\n            if(i == j): \n                n += matrix[i,j]\n            \n            total += matrix[i,j]\n            \n    acc = n \/ total\n    return acc","cedbdad9":"# Text to Features\n# Feature extraction from text \n# Method: bag of words\nX_test_vect = vectorizer.transform(X_test)\nprint(X_test_vect.shape)","2954536e":"# TF-IDF: Term Frequency - Inverse Document Frequency\n# use the transform(...) method to transform count-matrix to a tf-idf representation.\nX_test_tfidf = tfidf_transformer.transform(X_test_vect)\nprint(X_test_tfidf.shape)","e9e95102":"predictions = model.predict(X_test_tfidf)","221ce1c1":"matrix = metrics.confusion_matrix(y_test, predictions)\nprint(matrix)","6b3aa2c5":"print(model.classes_)","82a57b96":"acc1 = np.mean(predictions == y_test)\nacc2 = _get_accuracy(matrix)\nprint(acc1, acc2)","40253835":"for doc, prediction, y in zip(X_test[0:10], predictions[0:10], y_test[0:10]):\n    print('%r => %s [%s]' % (doc, prediction, y))","3a32a1fe":"# performance analysis of the results\nprint(metrics.classification_report(y_test, predictions, target_names=model.classes_))","800d1f80":"model_MNB = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', MultinomialNB()),\n])","c6952b46":"model_MNB.fit(X_train, y_train)\npredictions_MNB = model_MNB.predict(X_test)","fae95b9d":"matrix = metrics.confusion_matrix(y_test, predictions_MNB)\nacc = _get_accuracy(matrix)\nprint(acc)","94d605d4":"model_SGD = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier(loss = 'hinge', penalty = 'l2', alpha = 1e-3, random_state = 42, max_iter = 5, tol = None)),\n])","bef4aad5":"model_SGD.fit(X_train, y_train)\npredictions_SGD = model_SGD.predict(X_test)","cabbf84d":"matrix = metrics.confusion_matrix(y_test, predictions_SGD)\nacc = _get_accuracy(matrix)\nprint(acc)","9a89ef87":"parameters = {\n    'vect__ngram_range': [(1, 1), (1, 2)],\n    'tfidf__use_idf': (True, False),\n    'clf__alpha': (1e-2, 1e-3),\n}","7a48a3e6":"gs_model_SGD = GridSearchCV(model_SGD, parameters, cv = 5, iid = False, n_jobs = -1)\ngs_model_SGD = gs_model_SGD.fit(X_train, y_train)","e7c61ce9":"gs_predictions_SGD = gs_model_SGD.predict(['alegr lut tent sofr envolv n\u00e3o vit propri dit.'])\nprint(gs_predictions_SGD)","d49c5def":"X_new","095b4d17":"gs_predictions_SGD = gs_model_SGD.predict(X_new)\nprint(gs_predictions_SGD)","2db6483f":"print(gs_model_SGD.best_score_)","5e47ce15":"print(gs_model_SGD.best_params_)","cf154cb1":"for param_name in sorted(parameters.keys()):\n    print(\"%s: %r\" % (param_name, gs_model_SGD.best_params_[param_name]))","6043019b":"model_LR = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')),\n])","da6667a1":"model_LR.fit(X_train, y_train)\npredictions_LR = model_LR.predict(X_test)","e2a12fce":"matrix = metrics.confusion_matrix(y_test, predictions_LR)\nacc = _get_accuracy(matrix)\nprint(acc)","43f3db29":"'''\nmodel_SVC = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SVC()),\n])\n'''","999b51db":"'''\nmodel_SVC.fit(X_train, y_train)\npredictions_SVC = model_SVC.predict(X_test)\n'''","9e84a5df":"'''\nmatrix = metrics.confusion_matrix(y_test, predictions_SVC)\nacc = _get_accuracy(matrix)\nprint(acc)\n'''","96a64c0e":"'''\nmodel_RF = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', RandomForestClassifier()),\n])\n'''","fab33be5":"'''\nmodel_RF.fit(X_train, y_train)\npredictions_RF = model_RF.predict(X_test)\n'''","f3559abd":"'''\nmatrix = metrics.confusion_matrix(y_test, predictions_RF)\nacc = _get_accuracy(matrix)\nprint(acc)\n'''","076b3e5b":"'''\nmodel_GB = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', GradientBoostingClassifier()),\n])\n'''","b62f1897":"'''\nmodel_GB.fit(X_train, y_train)\npredictions_GB = model_GB.predict(X_test)\n'''","97431c9b":"'''\nmatrix = metrics.confusion_matrix(y_test, predictions_GB)\nacc = _get_accuracy(matrix)\nprint(acc)\n'''","45eb7982":"<h2>Bag-of-Words<\/h2>\nThe bag-of-words model is one of the feature extraction algorithms for text.","392ec28f":"<h4>Exploratory Analysis<\/h4>","2096d91e":"Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values","b37ba131":"<h4>Make predictions on validation dataset<\/h4>","28460478":"<h2>Implementing Neural Networks (not implemented...)<\/h2>\ncontinue in other kernels...","238f274f":"<h5 style=\"color:red;\">Note: Observe that I have not manually added stopwords [, . : - ( ' ; \" &] previously in \"stopword_list\" on purpose,  so to see its efects in this session.<\/h5>","327212bc":"<h4>Bag-of-Words<\/h4>\nThe bag-of-words model is one of the feature extraction algorithms for text.","b21d8eae":"<h4>Noise Removal<\/h4>","5fe5ff46":"<h1>Portuguese Tweets for Sentiment Analysis using nltk and sklearn<\/h1>","946b5483":"<h4>Frequency Distribution<\/h4>","774e665e":"Remove urls from text (http(s), www)","1f6b0b34":"<h2>Implementing a Random Forest Model<\/h2>","a50b77f7":"<h5 style=\"color:red;\">Note: We can remove or replace with tags some noisy words or special characters like \u2728\ud83d\udc96\ud83c\udf88\ud83c\udf89\ud83c\udf8a\u2764\ufe0f, but we need to evaluate if they are important to express some sentiment<\/h5>","8e7bf0fb":"Checking how many times word 'brasil' appeared","5a083e49":"<h2>Evaluating other models<\/h2>\nLogistic Regression\n<br\/>\nSupport Vector Machine (SVM)\n<br\/>\nRandom Forest\n<br\/>\nGradient Boosting Model","93a76d41":"<h2>Implementing Gradient Boosting Model<\/h2>","af4296cb":"<h4>Lexicon Normalization<\/h4>","e92907ff":"<h2>Prepare algorithm and run model<\/h2>","91e58008":"<h4>Word Cloud<\/h4>","c96c0dbf":"<h4>Planning<\/h4>\n<ul>\n  <li>Noise Removal<\/li>\n    <ul>\n      <li>Remove urls from text (http(s), www)<\/li>\n      <li>Remove a regex pattern (hashtags, notations)<\/li>\n      <li>Prepare text, replace emoticons \":)) :) :D :(\" to positive_emoticon or negative_emoticon or neutral_emoticon tags<\/li>\n    <\/ul>\n  <li>Tokenize text with nltk<\/li>\n  <li>Object Standardization<\/li>\n  <li>Remove stopwords (noise removal and dimensionality reduction)<\/li>\n  <li>Lexicon Normalization<\/li>\n    <ul>\n      <li>Lemmatization (not implemented...)<\/li>\n      <li>Stemming (dimensionality reduction)<\/li>\n    <\/ul>\n<\/ul> ","490567af":"<h4>Load and prepare X_train, y_train<\/h4>","a9ac22bf":"<h4>Load and prepare X_test, y_test<\/h4>","ea6b7451":"Stemming (dimensionality reduction)","c2200031":"<h2>Evaluate the model<\/h2>","1c5133a8":"<h2>Content<\/h2>\n<ul>\n  <li>Load libraries<\/li>\n  <li>Setup<\/li>\n  <li>Load training dataset<\/li>\n  <li>Load validation dataset<\/li>\n  <li>Text preprocessing<\/li>\n  <li>Exploratory analysis in words<\/li>\n  <li>Bag-of-Words<\/li>\n  <li>Prepare algorithm and run model<\/li>\n  <li>Standalone predictions<\/li>\n  <li>Evaluate the model<\/li>\n  <li>Building a pipeline MultinomialNB<\/li>\n  <li>Building a pipeline SGDClassifier, a linear support vector machine (SVM)<\/li>\n  <li>Parameter tuning using grid search SGDClassifier<\/li>\n  <li>Evaluating other models<\/li>\n  <li>Implementing a Linear Classifier (Logistic Regression)<\/li>\n  <li>Implementing a Support Vector Machine (SVM) Model<\/li>\n  <li>Implementing Gradient Boosting Model<\/li>\n  <li>Implementing Neural Networks (not implemented...)<\/li>\n<\/ul>","e95518dd":"<h2>Context<\/h2>\n<br\/>\nThis dataset has portuguese tweets divided in positive, negative and neutral classes for sentiment polarity classification.\n<br\/>\nThis dataset is very interesting, we are dealing with the popular language in social networks.\n<br\/>\nIt is necessary to perform some more careful treatment, such as noise removal, dimensionality reduction, object standardization and evaluation of some symbols, if they are important to express some sentiment.\n\n<h4>Dataset<\/h4>\n<ul>\n  <li>Training dataset: Train3Classes.csv<\/li>\n  <li>Test dataset: Test3Classes.csv<\/li>\n   <li>Custom Stopwords: stopwords-pt-br.txt<\/li>\n<\/ul> \n\n<h4>Columns description<\/h4>\n<ul>\n  <li>id: String identifier directly from Twitter<\/li>\n  <li>tweet_text: Full text from the tweet<\/li>\n  <li>tweet_date: Tweet creation date<\/li>\n  <li>sentiment: Sentiment label (classifier)<\/li>\n  <li>query_used: Query used to collect the tweet<\/li>\n<\/ul> ","96fe8131":"<h4>Object Standardization<\/h4>","a3b49222":"<h4>New documents to predict<\/h4>","71f9b5be":"<h4>Remove stopwords (noise removal and dimensionality reduction)<\/h4>","ea5bbfe6":"<h2>Load validation dataset<\/h2>","2ea2f180":"<h4>Dataset preparation<\/h4>\nUntokenize text (transform tokenized text into string list)","f6a87046":"<h4>Training a classifier - MultinomialNB<\/h4>","c49eb8bb":"<h2>Building a pipeline SGDClassifier, a linear support vector machine (SVM)<\/h2>","c4cf5b8a":"<h4>Analyze results<\/h4>","4ff57c04":"<h4>Text to Features<\/h4>\nFeature extraction from text\n<br\/>\nMethod: bag of words ","beadab2f":"Lemmatization (not implemented...)","25156259":"<h4>TF-IDF: Term Frequency - Inverse Document Frequency<\/h4>\nuse the transform(...) method to transform count-matrix to a tf-idf representation.","a1a91d99":"<h2>Implementing a Linear Classifier (Logistic Regression)<\/h2>","4d39ec9a":"<h4>Lexicon Normalization<\/h4>","4e89fd52":"<h4>Bag-of-Words<\/h4>\nThe bag-of-words model is one of the feature extraction algorithms for text.","c67ba584":"<h2>Setup<\/h2>","2f04fe7f":"<h2>Parameter tuning using grid search SGDClassifier<\/h2>","210b8bcc":"<h4>Tokenize text with nltk<\/h4>","1517f9f7":"<h4>Preprocessing new corpus<\/h4>","8638cfd7":"<h4>Make predictions on new_corpus<\/h4>","4c7fac62":"<h2>Building a pipeline MultinomialNB<\/h2>\nIn order to make the vectorizer => transformer => classifier easier to work with, scikit-learn provides a Pipeline class that behaves like a compound classifier:","c8da4839":"Prepare text, replace emoticons \":)) :) :D :(\" to positive_emoticon or negative_emoticon or neutral_emoticon","f6491dd7":"<h2>Standalone predictions<\/h2>","6685fcc0":"<h2>Text Preprocessing<\/h2>","b01f1651":"<h4>Examples of emoticons<\/h4>","d6db4356":"Remove a regex pattern (hashtags, notations)","1b3ac81c":"Remove stopwords","2b5f83b7":"<h2>Exploratory analysis in words<\/h2>","87bb22e0":"<h5 style=\"color:red;\">Note: Numbers, some punctuations (.,;:) and special characters such as (_- = &%) are not relevant to express some sentiment, they can be considered noisy words. You can treat them here.<\/h5>\n<h5 style=\"color:red;\">noisy_words = ['.', '?', '!', ':', ',', ';', '(', ')', '-']<\/h5>\n<h5 style=\"color:red;\">Note: ? and ! can be important to express sentiment.<\/h5>","f7fe982e":"<h2>Load libraries<\/h2>","88a297b9":"The names vect, tfidf and clf (classifier) are arbitrary. We will use them to perform grid search for suitable hyperparameters below. We can now train the model with a single command:","8928684d":"<h2>Load training dataset<\/h2>","4f1a6e00":"<h4>Exploratory Analysis<\/h4>","e7e167db":"<h2>Implementing a Support Vector Machine (SVM) Model<\/h2>"}}