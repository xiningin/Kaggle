{"cell_type":{"323772f5":"code","704eec5d":"code","3c92faaf":"code","c3313ecd":"code","2e49167c":"code","7f1ead09":"code","d3d4751f":"code","6512b1df":"code","92046a25":"code","9b6249ec":"code","09b69869":"code","bef0ecf9":"code","5bab1f70":"code","b5829a76":"code","076ea967":"code","84c8abeb":"code","27a5fa0a":"code","c9ad210a":"code","534c29a0":"markdown","ebb1187b":"markdown","1dddc2d1":"markdown","ec915d58":"markdown","2ba81a33":"markdown","560c7554":"markdown","cde1b478":"markdown","abf51722":"markdown"},"source":{"323772f5":"import tensorflow as tf\nimport IPython\nimport tensorflow_datasets as tfds","704eec5d":"!pip install -q -U keras-tuner\nimport kerastuner as kt","3c92faaf":"# loding the datasets from tensorflow_datasets\ntrain, train_info = tfds.load('fashion_mnist', split='train', \n                              as_supervised=True, \n                              with_info=True)\nval, val_info = tfds.load('fashion_mnist', \n                          split='test', \n                          as_supervised=True, \n                          with_info=True)","c3313ecd":"def resize(img, lbl):\n  img_size = 96\n  return (tf.image.resize(img, [img_size, img_size])\/255.) , lbl\n\ntrain = train.map(resize)\nval = val.map(resize)\n\ntrain = train.batch(32, drop_remainder=True)\nval = val.batch(32, drop_remainder=True)     ","2e49167c":"def baseline_alexnet():\n     return tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4,\n                               activation='relu'),\n        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n        tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding='same',\n                               activation='relu'),\n        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n        tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n                               activation='relu'),\n        tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n                               activation='relu'),\n        tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same',\n                               activation='relu'),\n        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(4096, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(4096, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])","7f1ead09":"#training on gpu if available\ndef try_gpu(i=0): \n    if len(tf.config.experimental.list_physical_devices('GPU')) >= i + 1:\n        return tf.device(f'\/GPU:{i}')\n    return tf.device('\/CPU:0')\ndevice_name = try_gpu()._device_name\nstrategy = tf.distribute.OneDeviceStrategy(device_name)","d3d4751f":"with strategy.scope():\n  model = baseline_alexnet()\n\nmodel.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\ncallback = tf.keras.callbacks.EarlyStopping(patience=3)\nhistory = model.fit(train, \n                    epochs=100, \n                    validation_data=val,\n                    \n                    callbacks = [callback])","6512b1df":"# storing the result on baseline_val_acc so that we can use it to compare later.\nbaseline_val_acc = max(history.history['val_accuracy'])","92046a25":"baseline_val_acc","9b6249ec":"def modified_alexnet(hp):\n  alexnet = tf.keras.models.Sequential()\n  # filter size from 96-256\n  alexnet.add(tf.keras.layers.Conv2D(filters = hp.Int(name='conv_block_1',\n                                                      min_value=96,\n                                                       max_value=256,\n                                                       default=96,\n                                                       step=32),\n                                      kernel_size=11, strides=4, activation='relu'))\n  alexnet.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2))\n  # filter size from 256-512\n  alexnet.add(tf.keras.layers.Conv2D(filters = hp.Int(name='conv_block_2',\n                                                       min_value=256,\n                                                       max_value=512,\n                                                       default=256,\n                                                       step=32),\n                                      kernel_size=5, padding='same', activation='relu'))\n  alexnet.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2))\n  # filter size from 384-512\n  alexnet.add(tf.keras.layers.Conv2D(filters = hp.Int(name='conv_block_3',\n                                                       min_value=384,\n                                                       max_value=512,\n                                                       default=384,\n                                                       step=32),\n                                      kernel_size=3, padding='same', activation='relu'))\n  # filter size from 384-512\n  alexnet.add(tf.keras.layers.Conv2D(filters = hp.Int(name='conv_block_4',\n                                                       min_value=384,\n                                                       max_value=512,\n                                                       default=384,\n                                                       step=32),\n                                      kernel_size=3, padding='same', activation='relu'))\n   # filter size from 256-512\n  alexnet.add(tf.keras.layers.Conv2D(filters = hp.Int(name='conv_block_5',\n                                                       min_value=256,\n                                                       max_value=512,\n                                                       default=256,\n                                                       step=32),\n                                      kernel_size=3, padding='same', activation='relu'))\n  \n  alexnet.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2))\n  alexnet.add(tf.keras.layers.Flatten())\n  # dense unit from 4096 to 8192\n  alexnet.add(tf.keras.layers.Dense(units = hp.Int(name='units_1',\n                                                  min_value=4096,\n                                                  max_value=8192,\n                                                  default=4096,\n                                                  step=256),\n                                                  activation='relu'))\n  # dropout value from 0-0.5\n  alexnet.add(tf.keras.layers.Dropout(hp.Float('dropout_1', 0, 0.5, step=0.1, default=0.5)))\n  alexnet.add(tf.keras.layers.Dense(units = hp.Int(name='units_2',\n                                                   min_value=4096,\n                                                    max_value=8192,\n                                                    default=4096,\n                                                    step=256),\n                                                    activation='relu'))\n  # dropout value from 0-0.5\n  alexnet.add(tf.keras.layers.Dropout(hp.Float('dropout_2', 0, 0.5, step=0.1, default=0.5)))\n  alexnet.add(tf.keras.layers.Dense(10, activation='softmax'))\n  # choice for the learning rate, i.e 0.01, 0.001, 0.0001 \n  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n  \n  alexnet.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = hp_learning_rate),\n                loss = tf.keras.losses.SparseCategoricalCrossentropy(), \n                metrics = ['accuracy'])\n\n  return alexnet","09b69869":"tuner = kt.Hyperband(modified_alexnet,\n                     objective = 'val_accuracy', \n                     max_epochs = 10,\n                     factor = 3,\n                     directory = 'my_dir',\n                      distribution_strategy=tf.distribute.OneDeviceStrategy(device_name),\n                     project_name ='intro_to_kt')    ","bef0ecf9":"#callback to clear the training output\nclass ClearTrainingOutput(tf.keras.callbacks.Callback):\n  def on_train_end(*args, **kwargs):\n    IPython.display.clear_output(wait = True)","5bab1f70":"tuner.search(train, epochs = 10, validation_data = (val), callbacks = [ClearTrainingOutput()])\n\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]","b5829a76":"# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]","076ea967":"# Build the model with the optimal hyperparameters and train it on the data\nmodel = tuner.hypermodel.build(best_hps)\ncallback = tf.keras.callbacks.EarlyStopping(patience=3)\nhistory = model.fit(train, \n                    epochs=100, \n                    validation_data=val,\n                    callbacks = [callback])","84c8abeb":"# storing the result on modified_alexnet_val_acc baseline_val_acc so that we can use it to compare baseline_val_acc.\nmodified_alexnet_val_acc = max(history.history['val_accuracy'])","27a5fa0a":"modified_alexnet_val_acc","c9ad210a":"print(\"The validation accuracy of the baseline alexnet model is {} VS The validation accuracy of the baseline alexnet model is {}\".format(baseline_val_acc, modified_alexnet_val_acc))","534c29a0":"Keras Tuner: The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program.\n\nTuning Hyperparameters are very much essisential for any ML development cycle. These are the constant variables that governs the training process. Right choise of hyperparameters leads to early convergence and also increases accuracy of the model.\n\nIt's basically two types:\n1. Model hyperparameters which influence model selection such as the number and width of hidden layers\n2. Alogrithm hyperparameters : which influence the speed and quality of the learning algorithm such as the learning rate of optimizer.\n","ebb1187b":"PS: If You belive that you've learn anything from this kernel, then please upvote this kerenel and follow me on kaggle.","1dddc2d1":"### By doing Hyper Parameter tuning from hands is a very time consuming + tedious task.\n### So, here comes keras Tuner to save us.It automatically finds the best hyperparameter for your model.\n\n","ec915d58":"Things you can do beyond this kernel is try to do ```hp.choice('sgd','adam')``` for the optimizer and try out on diffrent DL tasks, say image segmentation.","2ba81a33":"Now, let's make our  ```modified_alexnet``` function, this function will be used for  model-building. It takes an arguement ```hp``` from which we can sample\nhyperparameter. \\\nFor example if you want to sample units in a dense layer we can do simply by,\n```hp.Int('units', min_value=32, max_value=512, step=32)``` here you'll notice that \nwe're using ```hp.Int``` because the units in dense layer are alwayes ```Integers```.  \\\nUse ```hp.Int``` where the hyperparameters are integer type and use ```hp.Float``` where the hyperparamers are of Float type. \\\nSay, for dropout units ```hp.Float('dropout`, min_values=0, max_value=0.5, step=0.1)```\n\nThis ```hp.Int``` or ```hp.Float``` takes the following arguements:\n1. ```name``` It's the required arguement, make sure to name them unquely.\n2. ``` min_value``` It's the minimum value from which the keras tuner starts finding the best hyperparametrs\n3. ```max_value``` It's the maximum value upto which the keras tuner finds the best \nhyperparameter.\n4. ```step``` it's the number of steps skips in order to find the best hyperparameter values.\\\n5.```default``` It's an optional arguement which you can set.\n\n","560c7554":"Let's first build the ```baseline_alexnet```  and train it on ``` fashion_mnist``` dataset.","cde1b478":"So, in this kernel first we'll be going make a ```baseline_alexnet``` model(Alexnet) . \nThen we'll train this model on fashion mnist datasets.\n\nAnd after that I'll tell you, how to use keras tuner.\nThen we'll be making ```modified_alexnet``` model in which we'll be tuning\n```Model hyperparameters``` and after that we'll use this model to tune the \n```Algorithm hyperparameters```(learning rate).\n\nAnd finally we'll compare the results of the ```baseline_alexnet``` and the modified hyper parameter tuned ```modified_alexnet``` .","abf51722":"Now, we've to instantiate the tuner and perform hypertuning.\\\nHere, we're having for choices(algorithms) for instantiating the tuner.\\\nThese are, ```RandomSearch```, ```Hyperband```, ```BayesianOptimization```, and  ```sklearn```.\\\nFor this kernel we're going to use ```Hyperband``` algorithm.\n\nTo instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (max_epochs).\n"}}