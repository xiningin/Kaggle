{"cell_type":{"a55f85a5":"code","2ee8261e":"code","0f699b19":"code","fb3b83b4":"code","c1843b21":"code","6fbfd4f0":"code","a85aef40":"code","d56ac6d6":"code","4c2263e3":"code","e88710b7":"code","92980263":"code","40533fd0":"code","57500715":"code","9c6d46c1":"code","d214d036":"code","9cadce57":"code","b3f51a96":"code","dcabcba7":"code","fabce22d":"markdown","fab5cd8d":"markdown","3be1cde0":"markdown","7580a382":"markdown","adb7d5a7":"markdown","4d2d046a":"markdown","48f0c9d0":"markdown"},"source":{"a55f85a5":"#1.0 Call libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt","2ee8261e":"# 2.0 Import train and test dataset\ntrain = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\ntest = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')","0f699b19":"# 2.1 find the shape of train and test dataset\ntrain.shape\ntest.shape","fb3b83b4":"# 2.2 Seperate Target column\ny_train = train.pop('label')\ny_test =  test.pop('label')\nX_train = train\nX_test = test","c1843b21":"# 2.3 Get shape of train and test dataset\nX_train.shape\nX_test.shape\ny_train.shape\ny_test.shape","6fbfd4f0":"# 2.4 change datasets into numpy arrays\nX_train_1 = X_train.to_numpy()\nX_test_1 = X_test.to_numpy()\ny_train_1 = y_train.to_numpy()\ny_test_1 = y_test.to_numpy()","a85aef40":"# 3.0 Use PCA on train dataset\npca = PCA()\npca.fit(X_train_1)","d56ac6d6":"# 3.1 Find the statistics from PCA\n#     How much variance is explained by each principal component\npca.explained_variance_ratio_[0:20]\n#     Cumulative sum of variance of each principal component\ncumsum = np.cumsum(pca.explained_variance_ratio_)\ncumsum[0:20]","4c2263e3":"# 3.2 Get the column (principal component) number \n#      when cum explained variance threshold just exceeds 0.95\nd = np.argmax(cumsum >= 0.95) + 1\nd","e88710b7":"# 3.3 Let us also plot cumsum\n#     Saturation occurs are Elbow\n\nabc = plt.figure(figsize=(6,4))\nabc = plt.plot(cumsum, linewidth=3)\n# 4.3.1 Define axes limits\nabc = plt.axis([0, 400, 0, 1])\n# 4.3.2 Axes labels\nabc = plt.xlabel(\"Dimensions\")\nabc = plt.ylabel(\"Explained Variance\")\n# 4.3.3 Draw a (vertical) line from (d,0) to (d,0.95)\n#       Should be black and dotted\nabc = plt.plot([d, d], [0, 0.95], \"k:\")\n# 4.3.4 Draw another dotted (horizontal) line \n#       from (0,0.95) to (d,0.95)\nabc = plt.plot([0, d], [0.95, 0.95], \"k:\")\n# 4.3.5 Draw a point at (d,0.95)\nabc = plt.plot(d, 0.95, \"ko\")\n# 4.3.6 Annotate graph\nabc = plt.annotate(\n                   \"Elbow\",             # Text to publish\n                   xy=(65, 0.85),       # This parameter is the point (x, y) to annotate.\n                   xytext=(70, 0.7),    # The position (x, y) to place the text at.\n                   arrowprops=dict(arrowstyle=\"->\"), # A dictionary with properties used\n                                                     #  to draw an arrow between the\n                                                     #    positions xy and xytext.\n                   fontsize=16\n                  )\n# 4.3.7 Draw a grid\nplt.grid(True)\nplt.show()","92980263":"# 4.0 out of 784 ,187 columns have cumulative variance of 95%  \n# Get transformed dataset upto 95% explained variance \npca = PCA(n_components=187)\nX_reduced =pca.fit_transform(X_train_1)\nX_reduced.shape","40533fd0":"# 4.1 Recheck sum of explained variance\nnp.sum(pca.explained_variance_ratio_)","57500715":"# 4.2 Use PCA's function inverse_transform() to get origianl\n#      dimensions back from reduced dimesionality\nX_recovered = pca.inverse_transform(X_reduced)","9c6d46c1":"# 4.3 Check shape of recovered dataset\nX_recovered.shape  ","d214d036":"# 5.0 Plot few digits from original dataset\n#     Digit shapes\nfig,axe = plt.subplots(2,5)\naxe = axe.flatten()\nfor i in range(10):\n    abc = axe[i].imshow(X_train_1[i,:].reshape(28,28))\n    \n# 5.1 And few digits from compressed dataset\n#     And compare both\nfig,axe = plt.subplots(2,5)\naxe = axe.flatten()\nfor i in range(10):\n    abc = axe[i].imshow(X_recovered[i,:].reshape(28,28))","9cadce57":"# 6.0 Use RandomForestClassifier to train the reduced train  dataset\nrf1 = rf()\nrf1.fit(X_reduced,y_train_1)","b3f51a96":"# 6.1 Apply PCA on test dataset and predict the target.\nX_test_reduced =pca.fit_transform(X_test_1)\ny_predict = rf1.predict(X_test_reduced)","dcabcba7":"#6.2 Check accuracy score \naccuracy_score(y_predict,y_test)","fabce22d":" ## Using RandomForestClassifier for classification","fab5cd8d":"# Objective\nTo find whether the input dataset contains redundant pixels in an image.Using PCA to remove those redundant pixels and draw the image .We shall also use reverse PCA to get the original image from the reduced dataset.\n\nThe dataset contains class 'Label' based on which appearlsa has been grouped.\nWe will use RandomForestClassifier to find the Label of test dataset and check the accuracy.","3be1cde0":" ## Applying PCA","7580a382":"# Coding\n1. [Applying PCA](#Applying-PCA)\n2. [Using RandomForestClassifier for classification](#Using-RandomForestClassifier-for-classification)","adb7d5a7":"### PCA Analysis\n\nHere we find that applying PCA gives us a good result as there is no major change in shape and color of Images","4d2d046a":"### RandomForestClassifier Analysis\nRandomForestClassifier is not a good classifier for this dataset as accuracy score is not very great","48f0c9d0":"# Fashion MNIST\nFashion MNIST is a dataset of Zelando's Article Images consisting of 70000 images. Out of 700000, training dataset consists of 60000 images whille 10000 represents test dataset.\n<br><br>Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\nEach pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. \n<br><br> Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the [original MNIST dataset](https:\/\/en.wikipedia.org\/wiki\/MNIST_database) for benchmarking machine learning algorithms."}}