{"cell_type":{"2894bd71":"code","4d30535e":"code","604d54f8":"code","e6710b90":"code","79ce934d":"code","c0afe10c":"code","d9a2ed64":"code","eb976791":"code","77c810e7":"code","53260572":"code","a940d960":"code","e389a53c":"code","9b3ff201":"code","e2a1471b":"code","19e96fdb":"code","26d7b474":"code","bc8d3255":"code","84b9b3fb":"code","230c1fe8":"code","5695ecbd":"code","bf0c9cd4":"code","4a9ecd64":"code","595f4355":"code","50a62f47":"code","52816251":"code","d45b714a":"code","926ba80c":"code","6937ebb1":"code","e6913839":"code","177c8259":"code","3b9323c2":"code","2835d9ba":"code","07037cc2":"code","a469351b":"code","58b4c41a":"code","9945c9f5":"code","1e73724c":"code","6f92edb8":"code","d9f98429":"markdown","ac28287b":"markdown","f2f6c00c":"markdown","b35a86dd":"markdown","02542318":"markdown","dad635e6":"markdown","f883479f":"markdown","7941209e":"markdown","f4a1093c":"markdown","2a684c39":"markdown","5cb82802":"markdown","f25341c6":"markdown","cfc0d69b":"markdown","4c2572ce":"markdown","329e7940":"markdown"},"source":{"2894bd71":"# Importing libraries\n# pandas\nimport pandas as pd\n\n# numpy, matplotlib, seaborn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4d30535e":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ndf.head()","604d54f8":"# we dont need those columns\ndf.drop(columns=['PassengerId','Name','Ticket'],inplace=True)\ntest.drop(columns=['Name','Ticket'],inplace=True)","e6710b90":"# Null values in data\nprint(df.isnull().sum())\nprint('--------------------------')\nprint(test.isnull().sum())","79ce934d":"df['Embarked'].value_counts()","c0afe10c":"# Embarked\ndf['Embarked'].fillna('S',inplace=True)\n\nsns.countplot(x='Survived', hue=\"Embarked\", data=df, order=[1,0])\n\n\nembark_dummies_df = pd.get_dummies(df['Embarked'])\nembark_dummies_test = pd.get_dummies(test['Embarked'])\n\n\nembark_dummies_df.drop('S',axis=1, inplace=True)\nembark_dummies_test.drop('S',axis=1, inplace=True)\n\n\ndf = df.join(embark_dummies_df)\ntest = test.join(embark_dummies_test)\n\ndf.drop('Embarked',inplace=True,axis=1)\ntest.drop('Embarked',inplace=True,axis=1)","d9a2ed64":"# Fare\ndf['Fare'].fillna(df['Fare'].median(), inplace=True)\ntest['Fare'].fillna(test['Fare'].median(), inplace=True)\n\ndf['Fare'] = df['Fare'].astype(np.int)\ntest['Fare'] = test['Fare'].astype(np.int)\n\nsns.histplot(df['Fare'],bins=15)","eb976791":"# Cabin has lot of null values so we simply drop that column\ndf.drop(columns='Cabin',inplace=True)\ntest.drop(columns='Cabin',inplace=True)","77c810e7":"# Age\navg_age_df = df.Age.mean()\nstd_age_df = df.Age.std()\ncount_nan_age_df = df.Age.isnull().sum()\n\navg_age_test = test.Age.mean()\nstd_age_test = test.Age.std()\ncount_nan_age_test = test.Age.isnull().sum()\n\n# Getting random value from age column and then replace with nan   \nrand_ag_df = np.random.randint(avg_age_df-std_age_df, avg_age_df+std_age_df, size=count_nan_age_df)\nrand_ag_test = np.random.randint(avg_age_test-std_age_test, avg_age_test+std_age_test, size=count_nan_age_test)\n\ndf['Age'][np.isnan(df['Age'])] = rand_ag_df\ntest['Age'][np.isnan(test['Age'])] = rand_ag_test\n\nsns.kdeplot(df.loc[df['Survived'] == 0, 'Age'], label = 'Not Survived')\nsns.kdeplot(df.loc[df['Survived'] == 1, 'Age'], label = 'Survived')\nplt.legend()","53260572":"# Family\n# Create a new column using parch and sibsp\ndf['Family'] = df['Parch'] + df['SibSp']\ntest['Family'] = test['Parch'] + test['SibSp']\n\ndf.drop(columns=['Parch','SibSp'],inplace=True)\ntest.drop(columns=['Parch','SibSp'],inplace=True)\n\nsns.kdeplot(df.loc[df['Survived'] == 0, 'Family'], label = 'Not Survived')\nsns.kdeplot(df.loc[df['Survived'] == 1, 'Family'], label = 'Survived')\nplt.legend()","a940d960":"# Sex\ndef get_cat(passenger):\n    sex, age = passenger\n    return 'child' if age < 16 else sex\n\ndf['Person'] = df[['Sex','Age']].apply(get_cat,axis=1)\ntest['Person'] = test[['Sex','Age']].apply(get_cat,axis=1)\n\ndf.drop(['Sex'],axis=1,inplace=True)\ntest.drop(['Sex'],axis=1,inplace=True)\n\n# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers\nperson_dummies_titanic  = pd.get_dummies(df['Person'])\nperson_dummies_titanic.columns = ['Child','Female','Male']\nperson_dummies_titanic.drop(['Male'], axis=1, inplace=True)\n\nperson_dummies_test  = pd.get_dummies(test['Person'])\nperson_dummies_test.columns = ['Child','Female','Male']\nperson_dummies_test.drop(['Male'], axis=1, inplace=True)\n\ndf = df.join(person_dummies_titanic)\ntest = test.join(person_dummies_test)\n\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\n\n# sns.factorplot('Person',data=df,kind='count',ax=axis1)\nsns.countplot(x='Person', data=df, ax=axis1)\n\n# average of survived for each Person(male, female, or child)\nperson_perc = df[[\"Person\", \"Survived\"]].groupby(['Person'],as_index=False).mean()\nsns.barplot(x='Person', y='Survived', data=person_perc, ax=axis2, order=['male','female','child'])\n\ndf.drop(['Person'],axis=1,inplace=True)\ntest.drop(['Person'],axis=1,inplace=True)","e389a53c":"# Pclass\n\n# sns.factorplot('Pclass',data=df,kind='count',order=[1,2,3])\nsns.factorplot('Pclass','Survived',order=[1,2,3], data=df,size=5)\n\n# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\npclass_dummies_titanic  = pd.get_dummies(df['Pclass'])\npclass_dummies_titanic.columns = ['Class_1','Class_2','Class_3']\npclass_dummies_titanic.drop(['Class_3'], axis=1, inplace=True)\n\npclass_dummies_test  = pd.get_dummies(test['Pclass'])\npclass_dummies_test.columns = ['Class_1','Class_2','Class_3']\npclass_dummies_test.drop(['Class_3'], axis=1, inplace=True)\n\ndf.drop(['Pclass'],axis=1,inplace=True)\ntest.drop(['Pclass'],axis=1,inplace=True)\n\ndf = df.join(pclass_dummies_titanic)\ntest    = test.join(pclass_dummies_test)","9b3ff201":"X = df.drop(columns='Survived')\ny = df.Survived","e2a1471b":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","19e96fdb":"# Logistic Regression\nlr = LogisticRegression(solver='liblinear')\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nprint(accuracy_score(y_test,y_pred))","26d7b474":"# Random Forests\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\nprint(accuracy_score(y_test,y_pred))","bc8d3255":"# KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(accuracy_score(y_test,y_pred))","84b9b3fb":"# GaussianNB\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_test)\nprint(accuracy_score(y_test,y_pred))","230c1fe8":"# Support vector machine\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint(accuracy_score(y_test,y_pred))","5695ecbd":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.ensemble import StackingClassifier","bf0c9cd4":"def get_stacking():\n    \"\"\"\n    Return \n    ----------------\n    Stack model  \n    \"\"\"\n    level0 = list()  # First layer\n    level0.append(('lr',LogisticRegression(solver='liblinear')))\n    level0.append(('RF',RandomForestClassifier()))\n    level0.append(('GB',GaussianNB()))\n\n    level1 = LogisticRegression(solver='liblinear') #Second layer\n    \n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return model","4a9ecd64":"def evaluate_model(model,X,y):\n    \"\"\"\n    Return\n    -----------\n    Return accracy score that perform base on Cross Validation\n    \"\"\"\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    print(np.mean(scores))\n    return scores","595f4355":"scores = evaluate_model(get_stacking(), X, y)","50a62f47":"# define Folds\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)","52816251":"lr = LogisticRegression()\n# define search space\nspace = dict()\nspace['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\nspace['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\nspace['C'] = [0.001,0.01,0.1,1,10,100]\n# define search\nsearch = RandomizedSearchCV(lr, space, n_iter=500, scoring='accuracy', n_jobs=-1, cv=cv, random_state=1)\n# execute search\nresult = search.fit(X, y)\n\n# Score print\nprint('Best Score of LogisticRegression: %s' % result.best_score_)\nprint('Best Hyperparameters of LogisticRegression: %s' % result.best_params_)","d45b714a":"rf = RandomForestClassifier()\n# define search space\nspace = dict()\nspace['n_estimators'] = [10, 50, 100, 300, 1000]\nspace['max_features'] = ['sqrt', 'log2']\nspace['max_depth'] = [10, 20, 60, 100]\n\n# define search\nsearch = RandomizedSearchCV(rf, space, n_iter=10, scoring='accuracy', n_jobs=-1, cv=cv, random_state=1)\n# execute search\nresult = search.fit(X, y)\n\n# Score print\nprint('Best Score of RandomForestClassifier: %s' % result.best_score_)\nprint('Best Hyperparameters of RandomForestClassifier: %s' % result.best_params_)","926ba80c":"GB = GaussianNB()\n# define search space\nspace = dict()\nspace['var_smoothing'] = np.logspace(0,-9, num=100)\n# define search\nsearch = RandomizedSearchCV(GB, space, n_iter=20, scoring='accuracy', n_jobs=-1, cv=cv, random_state=1)\n# execute search\nresult = search.fit(X, y)\n\n# Score print\nprint('Best Score of GaussianNB: %s' % result.best_score_)\nprint('Best Hyperparameters of GaussianNB: %s' % result.best_params_)","6937ebb1":"def get_stacking():\n    level0 = list()\n    level0.append(('lr',LogisticRegression(solver= 'liblinear', penalty= 'l2', C=1)))\n    level0.append(('RF',RandomForestClassifier(n_estimators=300, max_features='sqrt', max_depth= 10)))\n    level0.append(('GB',GaussianNB(var_smoothing= 1.23e-06)))\n    \n    level1 = RandomForestClassifier(n_estimators=300, max_features='sqrt', max_depth= 10)\n    \n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5,stack_method='predict')\n    return model","e6913839":"def evaluate_model(model,X,y):\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    print(np.mean(scores))\n    return scores\nfinal_model = get_stacking()\nscores = evaluate_model(final_model, X, y)","177c8259":"level0 = list()\nlevel0.append(('lr',LogisticRegression(solver= 'newton-cg', penalty= 'l2', C=10)))\nlevel0.append(('RF',RandomForestClassifier(n_estimators=300, max_features='sqrt', max_depth= 10)))\nlevel0.append(('GB',GaussianNB(var_smoothing= 8.1e-08)))\n\nlevel1 = GaussianNB(var_smoothing= 8.1e-08)\n\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5,stack_method='predict')\nmodel.fit(X,y).score(X,y)","3b9323c2":"from sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier","2835d9ba":"# GradientBoosting\ngbc = GradientBoostingClassifier().fit(X_train, y_train)\ny_pred = gbc.predict(X_test)\nprint(accuracy_score(y_test,y_pred))","07037cc2":"# XGBoost\nxgbc = XGBClassifier(verbosity = 0).fit(X_train, y_train)\ny_pred = gbc.predict(X_test)\nprint(accuracy_score(y_test,y_pred))","a469351b":"def get_stacking():\n    level0 = list()\n    level0.append(('lr',LogisticRegression(solver= 'liblinear', penalty= 'l2', C=1)))\n    level0.append(('RF',RandomForestClassifier(n_estimators=300, max_features='sqrt', max_depth= 10)))\n    level0.append(('GB',GaussianNB(var_smoothing= 1.23e-06)))\n    level0.append(('gbc',GradientBoostingClassifier()))\n    level0.append(('xgbc',XGBClassifier()))\n    \n    level1 = LogisticRegression()\n    \n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5,stack_method='predict')\n    return model","58b4c41a":"def evaluate_model(model,X,y):\n    cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=1, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    print(np.mean(scores))\n    return scores\nfinal_model = get_stacking()\nscores = evaluate_model(final_model, X, y)","9945c9f5":"level0 = list()\nlevel0.append(('lr',LogisticRegression(solver= 'liblinear', penalty= 'l2', C=1)))\nlevel0.append(('RF',RandomForestClassifier(n_estimators=300, max_features='sqrt', max_depth= 10)))\nlevel0.append(('GB',GaussianNB(var_smoothing= 1.23e-06)))\nlevel0.append(('gbc',GradientBoostingClassifier()))\nlevel0.append(('xgbc',XGBClassifier(verbosity = 0)))\n\nlevel1 = LogisticRegression(solver= 'liblinear')\n\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5,stack_method='predict')\nmodel.fit(X,y).score(X,y)","1e73724c":"test_out = model.predict(test.drop(columns='PassengerId'))\nsub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsub['PassengerId'] = test['PassengerId']\nsub['Survived'] = test_out","6f92edb8":"sub.to_csv('titanic_submission_ensemble.csv',index=False)\nprint('Submission file ready!!')","d9f98429":"# Stacking Ensemble ML Model","ac28287b":"## Hyper Tune Each Model","f2f6c00c":"## Pretty much good score\nWe can improve that by doing more feature engineering, but that not the scope of this NoteBook ","b35a86dd":"## Importing Libraries","02542318":"![Cross validation](https:\/\/i.imgur.com\/yJJZoPb.png)","dad635e6":"# What is Model Stacking?\nThe idea of stacking is to train several models, usually with different algorithm types (called as base-learners), on the train data, and then rather than picking the best model, all the models are aggregated using another model (called as meta learner), to make the final prediction. The inputs for the meta-learner is the prediction outputs of the base-learners. (It look like Neural Network)","f883479f":"Add some more advance models","7941209e":"### Model stacking should always be accompanied by cross validation, to reduce overfitting models to training data.","f4a1093c":"![Stacking Model](https:\/\/i.imgur.com\/lACpyqp.png)","2a684c39":"## Train individual models","5cb82802":"Further Reading:\n\nStackingCVClassifier by mlextend:\nhttp:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingCVClassifier\/\n\nStacked generalization: when does it work? :\nhttps:\/\/researchcommons.waikato.ac.nz\/handle\/10289\/1066","f25341c6":"## Importing Data","cfc0d69b":"### Prepare data for training","4c2572ce":"## Our main focus of this NoteBook is Stacking \n##### For feature engineering I use https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic\n\nThankYou... OMAR ELGABRY : https:\/\/www.kaggle.com\/omarelgabry ","329e7940":"# Hope this is helpful !!! \ud83d\udc4d\nFollow for more"}}