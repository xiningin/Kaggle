{"cell_type":{"c9fdda74":"code","4ee6a8b1":"code","8abb403f":"code","3447f1d4":"code","bdd01789":"code","6b7c5e96":"code","3b64d86b":"code","4e914e17":"code","890f8425":"code","b6a91a5b":"code","4c564f3d":"code","1d2486a2":"code","e576af2b":"code","85e69f09":"code","bf4f9279":"code","275f5e11":"code","a95823e1":"code","df68d159":"code","dac549e7":"code","1161df03":"code","c7c435a0":"code","30cf4176":"code","9270a1cb":"code","b7429c29":"code","264853a8":"code","9985e0e1":"code","a597842a":"code","ffc46ee6":"code","5b7279b2":"code","aa058660":"code","45f7edd9":"code","8a1dbb0a":"code","74935dce":"code","cdcf085f":"code","bfc0923d":"code","aac585ca":"code","3c758bda":"code","dd30aa6e":"code","85215c3a":"code","1aec4dbd":"code","44b28118":"markdown","1a39c86a":"markdown","b52d5fd6":"markdown","d1e6903e":"markdown","84bbd0c0":"markdown","83680810":"markdown","6535d99d":"markdown","49fc1f14":"markdown","74fe3236":"markdown","8c63d50f":"markdown","43f71dac":"markdown","ecf81930":"markdown","87518d2c":"markdown","60bfc25f":"markdown","20382b93":"markdown","58f5a99f":"markdown","a2e92520":"markdown","709753d4":"markdown","3d78109c":"markdown","42e88ef2":"markdown","5973c677":"markdown","984a81b0":"markdown","c2c13d38":"markdown","d05bdc14":"markdown","4497a766":"markdown","fcdf0323":"markdown"},"source":{"c9fdda74":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n# model selection and preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler\n#models\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\n\n#Boosting Algorithms\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n\n# metrics\nfrom sklearn.metrics import accuracy_score,classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# To deal with those annoying deprecated warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4ee6a8b1":"raw_data = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","8abb403f":"raw_data_copy = raw_data","3447f1d4":"raw_data_copy.head()","bdd01789":"raw_data_copy.shape","6b7c5e96":"raw_data_copy.isnull().sum()","3b64d86b":"raw_data_copy.info()","4e914e17":"raw_data_copy.drop_duplicates(inplace=True)\nraw_data_copy.reset_index(drop=True, inplace=True)\nraw_data_copy.isnull().sum()","890f8425":"sns.countplot(raw_data_copy['output'])","b6a91a5b":"ax= px.pie(raw_data_copy['output'], names= \"output\",title= \"Output\")\nax.show()","4c564f3d":"columns=[\"age\",\"cp\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\nfig, ax1 = plt.subplots(3,2, figsize=(20,20))\nk = 0\nfor i in range(3):\n    for j in range(2):\n            sns.distplot(raw_data_copy[columns[k]], ax = ax1[i][j], color = 'green')\n            k += 1\nplt.show()","1d2486a2":"plt.figure(figsize=(20,6))\nsns.distplot(raw_data_copy[\"age\"],color=\"red\",bins=\"auto\")\nplt.title(\"Total age distribution\")\nplt.show()","e576af2b":"ax= px.pie(raw_data_copy['sex'], names= \"sex\",title= \"Gender Distribution\")\nax.show()","85e69f09":"data1 = raw_data_copy[raw_data_copy['sex'] == 1].reset_index()\ndata0 = raw_data_copy[raw_data_copy['sex'] == 0].reset_index()","bf4f9279":"\nax = px.pie(data1, names = 'output',title= \"Fatality in Gender 1\")\nax.show()","275f5e11":"ax = px.pie(data0,names = \"output\",title=\"Fatality in Gender 0\")\nax.show()","a95823e1":"fig,ax = plt.subplots(7,2,figsize = (20,36))\nk = 0\nfor i in range(7):\n    for j in range(2):\n        sns.countplot(raw_data_copy[raw_data_copy.columns[k]],ax = ax[i][j])\n        k+=1\nplt.show()","df68d159":"columns=[\"age\",\"cp\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\nfig, ax1 = plt.subplots(3,2, figsize=(20,20))\nk = 0\nfor i in range(3):\n    for j in range(2):\n            sns.boxplot(raw_data_copy[columns[k]], ax = ax1[i][j], color = 'red',width = 0.5)\n            k += 1\nplt.show()","dac549e7":"corr = raw_data_copy.corr()","1161df03":"plt.figure(figsize=(16,8))\nsns.heatmap(corr,annot=True)\nplt.show()","c7c435a0":"corr['output']","30cf4176":"# Q1 \nq1 = raw_data_copy.quantile(0.25)\n# Q3\nq3 = raw_data_copy.quantile(0.75)\n# IQR\nIQR = q3 - q1\n# Outlier range\nupper = q3 + IQR * 1.5\nlower = q1 - IQR * 1.5\nupper_dict = dict(upper)\nlower_dict = dict(lower)","9270a1cb":"for i,v in raw_data_copy.items():\n    v_col = v[( v<= lower_dict[i]) | (v >= upper_dict[i])]\n    perc = np.shape(v_col)[0] * 100.0 \/ np.shape(raw_data_copy)[0]\n    print(\"Column {} outliers = {} => {}%\".format(i,len(v_col),round((perc),3)))","b7429c29":"raw_data_copy['trtbps'].replace(list(raw_data_copy[raw_data_copy['trtbps'] > upper_dict['trtbps']].trtbps) ,upper_dict['trtbps'],inplace=True)\nraw_data_copy['chol'].replace(list(raw_data_copy[raw_data_copy['chol'] > upper_dict['chol']].chol) ,upper_dict['chol'],inplace=True)\nraw_data_copy['oldpeak'].replace(list(raw_data_copy[raw_data_copy['oldpeak'] > upper_dict['oldpeak']].oldpeak) ,upper_dict['oldpeak'],inplace=True)","264853a8":"raw_data_copy['trtbps'].replace(list(raw_data_copy[raw_data_copy['trtbps'] < lower_dict['trtbps']].trtbps) ,lower_dict['trtbps'],inplace=True)\nraw_data_copy['chol'].replace(list(raw_data_copy[raw_data_copy['chol'] < lower_dict['chol']].chol) ,lower_dict['chol'],inplace=True)\nraw_data_copy['oldpeak'].replace(list(raw_data_copy[raw_data_copy['oldpeak'] < lower_dict['oldpeak']].oldpeak) ,lower_dict['oldpeak'],inplace=True)","9985e0e1":"# This standardisation technique uses median and interquartile range for standardisation rather than mean and variance.\nscaler = RobustScaler()\nrobust_df = scaler.fit_transform(raw_data_copy.iloc[:,:13])\nrobust_df = pd.DataFrame(robust_df, columns =['age','sex','cp','trtbps','chol','fbs','restecg','thalachh','exng','oldpeak','slp','caa','thall'])\nrobust_df","a597842a":"X = robust_df.values\n# X = raw_data_copy.iloc[:,1:-1].values\n\nY = raw_data_copy['output'].values\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n# Normalization of data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","ffc46ee6":"model_LG = LogisticRegression(random_state=0)\nmodel_LG.fit(X_train,Y_train)\nY_pred = model_LG.predict(X_test)\nmodel_LG_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\nprint(classification_report(Y_test,Y_pred))","5b7279b2":"model_KNN = KNeighborsClassifier(n_neighbors=7)\nmodel_KNN.fit(X_train,Y_train)\nY_pred = model_KNN.predict(X_test)\nmodel_KNN_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100\nprint(classification_report(Y_test,Y_pred))","aa058660":"model_NB = GaussianNB()\nmodel_NB.fit(X_train,Y_train)\n  \npredicted = model_NB.predict(X_test)\nprint(classification_report(Y_test,predicted))","45f7edd9":"model_svm=SVC(kernel=\"rbf\",random_state=0)\nmodel_svm.fit(X_train,Y_train)\nY_pred=model_svm.predict(X_test)\n\nmodel_svm_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100\nprint(classification_report(Y_test,Y_pred))","8a1dbb0a":"\nmodel_RF = RandomForestClassifier(n_estimators = 100, random_state = 0)  \nmodel_RF.fit(X_train, Y_train)  \npredicted = model_RF.predict(X_test)\nprint(classification_report(Y_test,predicted))","74935dce":"model_XG = xgb.XGBClassifier(use_label_encoder=False)\nmodel_XG.fit(X_train, Y_train)\n   \npredicted = model_XG.predict(X_test)\n   \nprint(classification_report(Y_test,predicted))","cdcf085f":"model_ADA=AdaBoostClassifier(learning_rate= 0.15,n_estimators= 25,random_state=0)\nmodel_ADA.fit(X_train,Y_train)\nY_pred= model_ADA.predict(X_test)\n\n\nmodel_ADA_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\nprint(classification_report(Y_test,Y_pred))\n","bfc0923d":"\nmodel_GB= GradientBoostingClassifier(random_state=0,n_estimators=20,learning_rate=0.29,loss=\"deviance\")\nmodel_GB.fit(X_train,Y_train)\nY_pred= model_GB.predict(X_test)\n\nmodel_GB_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\nprint(classification_report(Y_test,Y_pred))","aac585ca":"\nlgbm = LGBMClassifier(random_state=0)\n\nlgbm.fit(X_train, Y_train)\n\ny_pred = lgbm.predict(X_test)\nprint(classification_report(Y_test,Y_pred))","3c758bda":"param_grid = {'C': [0.1,1,10,100,1000], \n              'degree' : [4,5,6,7,8,9],\n              'kernel': ['rbf']} \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 5)\n  \n# fitting the model for grid search\ngrid.fit(X_train, Y_train)","dd30aa6e":"\n# print best parameter after tuning\nprint(grid.best_params_)\n  \n# print how our model looks after hyper-parameter tuning\nprint(grid.best_estimator_)","85215c3a":"\ngrid_predictions = grid.predict(X_test)\n  \n# print classification report\nprint(classification_report(Y_test, grid_predictions))","1aec4dbd":"accuracy_score(Y_test,grid_predictions) * 100","44b28118":"\nOutliers:\nIt is the data that is way too large or way too low in respect of the other data.It highly affects the measures as a lot measure directly depend on all the data points.\n\nIQR(inter quartile range):\nIt is the difference between Q1 and Q3 and is the range of middle 50% of the data. Ways to identify outliers:\n\n- Find Q1,Q2,Q3\n- Find IQR = Q3 - Q1\n- Multiply IQR by 1.5\n- Subtract this number from Q1 and add this number to Q3\n- If the point lie in the above acquired range then it is not an outlier, if doesn't then it is an outlier.","1a39c86a":"### Since dataset is already very small , we won't be dealing with them by removing the outliers rather we will replace them with acceptable upper or lower limit.","b52d5fd6":"# Applying Grid Search on SVM","d1e6903e":"Here are some outliers present in our features","84bbd0c0":"Only the \"OldPeak\" is the only feature which is highly skewewd.","83680810":"# Ada Boost Classifier","6535d99d":"We can clearly see that gender \"0\" is high risk at heart attack","49fc1f14":"# EDA","74fe3236":"# Outliers","8c63d50f":"# KNN","43f71dac":"# Logistic Regression","ecf81930":"## Checking if the data is balanced for output or not","87518d2c":"## Importing libraries","60bfc25f":"# SVM","20382b93":"We can see that the least related to our 'output' are 'fbs' and 'chol'.","58f5a99f":"# Random Forest Classifier","a2e92520":"# Gradient Boost Classifier","709753d4":"# Data Description","3d78109c":"### Duplicates","42e88ef2":"# Data Pre-Processing","5973c677":"NO null values","984a81b0":"# Conclusion:\n* Most of the models are working brilliantly on this dataset after normalising the dataset.\n* SVM and ADAboost are particularly best models.\n* After using Grid search , Adaboost beats SVM.\n* Only looking at accuracy as evaluation metrics in this case might be deadly as we need to look for **False Negative**.\n* Hence , we are looking at complete classification report , especisally **Recall**","c2c13d38":"# XGboost Classifier","d05bdc14":"We have \"caa\" , \"trtbps\" ,\"chol\" and \"oldpeak\" with some amount of outliers.","4497a766":"# Train Test Split","fcdf0323":"# Gaussian NB"}}