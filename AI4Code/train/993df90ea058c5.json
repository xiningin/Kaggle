{"cell_type":{"1b95eb68":"code","dd38812c":"code","52cad63b":"code","ed2b08cd":"code","93376c94":"code","d4cec35b":"code","67b9ec16":"code","9264e832":"code","1880bd19":"code","a56b29f8":"code","8e8e5ea7":"code","adf4d519":"code","a79d7311":"code","d2427549":"code","f14b7a92":"code","33f17052":"code","554fe5d3":"code","5f62dc2c":"code","b64a6d6a":"code","2deaf47a":"code","da5ddcd0":"code","1d2efd32":"code","f3398195":"code","bc7e5ce7":"code","481fc5ac":"code","8b77b794":"code","e2ca6007":"code","bc7a3d2e":"code","6bb2b7d1":"code","a700a292":"code","31788c9c":"code","f826ef53":"code","21d0a9b3":"code","30b8e78a":"code","69f2d55e":"code","50f0b44d":"code","b520ac9a":"code","2da46d83":"code","d9e35665":"code","164a7734":"code","254f2789":"code","f2b9698d":"code","2f9338d7":"markdown","293b8b99":"markdown","ef6f60fe":"markdown","f3e529cb":"markdown","c861ab92":"markdown","b31886a2":"markdown","dbfeefa4":"markdown","def51ef8":"markdown","bd6f46b4":"markdown","39d77c86":"markdown","cb099a4e":"markdown","c30096ed":"markdown","a80651de":"markdown","59bd7a48":"markdown","6bffb5a2":"markdown","88a281ce":"markdown","bbddb307":"markdown","cd7b3206":"markdown","6181412c":"markdown","dcffbe14":"markdown","de046a21":"markdown","2548b775":"markdown","4971431e":"markdown","061e4a53":"markdown","6b5ed5ca":"markdown","eed28486":"markdown","e9e30865":"markdown","bb809e96":"markdown","7cdcaae9":"markdown"},"source":{"1b95eb68":"#data analysis libraries \nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","dd38812c":"#Reading and keep the tables in easy variables\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n#Looking at the closer details\ntrain.describe()","52cad63b":"#To see the collumn names only\nprint(train.columns)","ed2b08cd":"#To see it on a graph\ntrain.sample(20)","93376c94":"print(train.dtypes)","d4cec35b":"print(train.isnull().sum())","67b9ec16":"sns.barplot(data= train, x=\"Sex\", y=\"Survived\")\nprint(\"Percentage of females surviving:\", train[\"Survived\"][train.Sex == \"female\"].value_counts(normalize=True)*100)\nprint(\"Number males surviving:\", train[\"Survived\"][train.Sex == \"male\"].value_counts(normalize = True)*100)","9264e832":"train[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"]= test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60,np.inf]\nlabels = [\"Unknown\",\"Small children\",\"Children\",\"Teens\",\"Adolescent\",\"Young Adults\",\"Adults\",\"Elders\"]\ntrain[\"AgeGroups\"] = pd.cut(train.Age,labels=labels, bins=bins)\ntest[\"AgeGroups\"] = pd.cut(test.Age,labels=labels, bins=bins)\nsns.barplot(data = train,x= \"AgeGroups\",y= \"Survived\")\n","1880bd19":"#pclass Factor\nsns.barplot(data= train, x=\"Pclass\", y=\"Survived\")\nprint(\"Percentage of High earners surviving:\", train[\"Survived\"][train.Pclass == 1].value_counts(normalize=True)*100)\nprint(\"Percentage of middle class earners surviving:\", train[\"Survived\"][train.Pclass == 2].value_counts(normalize = True)*100)\nprint(\"Percentage of low class earners surviving:\", train[\"Survived\"][train.Pclass == 3].value_counts(normalize = True)*100)","a56b29f8":"#Family factors\nsns.barplot(data =train, x=\"SibSp\",y=\"Survived\")\nprint(\"Survivors with one sibling\/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Survivors with two sibling\/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"survivors with three sibling\/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 3].value_counts(normalize = True)[1]*100)\nprint(\"Survivors with four sibling\/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 4].value_counts(normalize = True)[1]*100)\n","8e8e5ea7":"#Parch Factor\nsns.barplot(data = train,x=\"Parch\",y=\"Survived\")","adf4d519":"train[\"CabinBool\"] = (train.Cabin.notnull().astype(\"int\"))\ntest[\"CabinBool\"] =  (train.Cabin.notnull().astype(\"int\"))\nprint(\"Percentage of recorded Cabins that survived:\",train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of recorded Cabins that didn't survive:\",train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\nsns.barplot(data = train,x=\"CabinBool\",y=\"Survived\")","a79d7311":"test.describe(include=\"all\")","d2427549":"train = train.drop([\"Ticket\"],axis = 1)\ntest = test.drop(['Ticket'], axis = 1)\n\ntrain = train.drop([\"Cabin\"],axis = 1)\ntest = test.drop([\"Cabin\"],axis = 1)\n\ntrain = train.drop([\"CabinBool\"],axis = 1)\ntest = test.drop([\"CabinBool\"],axis = 1)","f14b7a92":"print(\"Number of people embarking in Southampton (S):\",train[train[\"Embarked\"] == \"S\"].shape[0])\n\nprint(\"Number of people embarking in Cherbourg (C):\",train[train[\"Embarked\"] == \"C\"].shape[0])\n\nprint(\"Number of people embarking in Queenstown (Q):\",train[train[\"Embarked\"] == \"Q\"].shape[0])","33f17052":"#replacing the missing values in the Embarked feature with S\ntrain = train.fillna({\"Embarked\": \"S\"})","554fe5d3":"#create a combined group of both datasets\ncombine = [train, test]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","5f62dc2c":"#Replace the titles with numbers that would let out computer understand\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","b64a6d6a":"title_map = {\"Master\":1,\"Miss\":2,\"Mr\":3,\"Mrs\":4,\"Rare\":5,\"Royal\":5}\nfor dataset in combine:\n    dataset[\"Title\"] = dataset[\"Title\"].map(title_map)\n    dataset[\"Title\"] = dataset[\"Title\"].fillna(0)\n\ntrain.head()","2deaf47a":"Master_age = train[train[\"Title\"]==1][\"AgeGroups\"].mode()#Small children (Somehow, don't ask me)\nMs_age = train[train[\"Title\"]==2][\"AgeGroups\"].mode() #Teens\nMr_age = train[train[\"Title\"]==3][\"AgeGroups\"].mode() #Young Adults\nMrs_age = train[train[\"Title\"]==4][\"AgeGroups\"].mode() #Adults\nRare_age = train[train[\"Title\"]==5][\"AgeGroups\"].mode() #Adult\nRoyal_age = train[train[\"Title\"]==6][\"AgeGroups\"].mode() #Adults\n#Demonstrating what I'm doing when using .mode\nprint(Master_age)\nprint(Ms_age)\nprint(Mrs_age)\ntrain.head()","da5ddcd0":"title_age_map = {1: \"Small children\", 2: \"Teens\", 3: \"Young Adults\", 4: \"Adults\", 5: \"Adults\", 6: \"Adults\"}\n\nfor x in range(len(train[\"AgeGroups\"])):\n    if train[\"AgeGroups\"][x] == \"Unknown\":\n        train[\"AgeGroups\"][x] = title_age_map[train[\"Title\"][x]]\n\nfor x in range(len(test[\"AgeGroups\"])):\n    if test[\"AgeGroups\"][x] == \"Unknown\":\n        test[\"AgeGroups\"][x] = title_age_map[train[\"Title\"][x]]","1d2efd32":"Age_map = {\"Small children\": 1, \"Children\": 2, \"Teens\": 3, \"Adolescent\": 4, \"Young Adults\": 5, \"Adults\": 6, \"Elders\": 7}\n\ntrain[\"AgeGroups\"] = train[\"AgeGroups\"].map(Age_map)\ntest[\"AgeGroups\"] = test[\"AgeGroups\"].map(Age_map)\ntrain.head()","f3398195":"#drop the name feature since it contains no more useful information.\ntrain = train.drop([\"Name\"], axis = 1)\ntest = test.drop([\"Name\"], axis = 1)\ntrain = train.drop([\"Age\"], axis = 1)\ntest = test.drop([\"Age\"], axis = 1)","bc7e5ce7":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","481fc5ac":"#map each Embarked value to a numerical value that can be read by the computer. Almost there!\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","8b77b794":"print(train.Fare.max())\nprint(train.Fare.min())","e2ca6007":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","bc7a3d2e":"#Check train data to see what we ended up with\ntrain.head()","6bb2b7d1":"#Check test data\ntest.head()","a700a292":"from sklearn.model_selection import train_test_split\n\npredict = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predict, target, test_size = 0.22, random_state = 0)","31788c9c":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","f826ef53":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","21d0a9b3":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","30b8e78a":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","69f2d55e":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","50f0b44d":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","b520ac9a":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","2da46d83":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","d9e35665":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","164a7734":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","254f2789":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","f2b9698d":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","2f9338d7":"Testing Different Models  \nI will be testing the following models with my training data (got the list from [here](http:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner\/notebook)): \n  \n*   Gaussian Naive Bayes  \n*   Logistic Regression  \n*   Support Vector Machines  \n*   Perceptron  \n*   Decision Tree Classifier  \n*   Random Forest Classifier  \n*   KNN or k-Nearest Neighbors  \n*   Stochastic Gradient Descent  \n*   Gradient Boosting Classifier  \n*   For each model, we set the model, fit it with 80% of our training data, predict for 20% of the training data and check the accuracy.","293b8b99":"#Data Analysis\nWe have two choices now.\nWe can either take a look at the graph as a whole to see the collumns and some variables with either the 'head()' or the '.sample()' functions,\nor we can use the '.collumns' function to see the collumns on the screen\n","ef6f60fe":"Now that we are finally extracted everything we could from the name and the age columns, we can now get rid of them.","f3e529cb":"As more siblings or spouses were present, the probability of survival was also lower. However it is interesting to note that people with 1 sibling\/spouse was mos likely to survive is a very interesting  that people with no siblings or sposes were less likely to survive than the ones who had one or two.","c861ab92":"Yikes!  \nIt seems like we have many missing values from our DataSet.  \n19.86% of the age column is missing.  \nA whooping 70% of the cabin values are missing.   \nOnly 0.22% of the embarked column is missing which shouldn't hurt our graph too much.\nAge factor is important for the survival rate, so we should try and fill in those values as much as we can.\nCabin values are mostly missing so dropping the table may be the wisest option. We can however figure that higher \"fares\" would equal to a higher cabin, therefore making a dependence graph can still give us an idea. It is not necessary but could be certainly interesting.\n    ","b31886a2":"I decided to use the Gradient Boosting Classifier since it has a higher score overall.","dbfeefa4":"Embarked Feature","def51ef8":"Let's see if we have any missing values in any of the columns","bd6f46b4":"Sex Feature","39d77c86":"7) Creating Submission File  \nIt's time to create a submission.csv file to upload to the Kaggle competition!","cb099a4e":"As predicted, females had a way higher chance of survival compared to males.  \n\n   ##Age Factor:","c30096ed":"Embarked","a80651de":"By using our own intuition and the results of our tests and graphs, let's drop some of the columns that don't affect the results or fill in the columns with the appropriate values.\n(Don't forget to drop the same columns from the two datasets)","59bd7a48":"That was one hell of a ride!  \nIf you made it all the way here, congrats you're doing amazing :)  \nIf anyone has ay questions or suggestions regarding the process, I'd be more than happy to help!  \nHope you have a wonderfull day and keep on coding!","6bffb5a2":"Finally, the fare feature.  \nWe'll try and categorize this into more logical \"bins\"","88a281ce":"Cabin Feature\nCabin features are a little bit tricky. They don't really mean anything, unless we assume that the ones recorded were people that were more important, or of a higher economic class.","bbddb307":"Splitting the Training Data  \nWe will use part of our training data (22% in this case) to test the accuracy of our different models.","cd7b3206":"Now that we have all of the libraries in place, let's take a look at our data. \nWe will first import it and read it with the commands: 'pd.csv_read' and 'describe()' ","6181412c":"Numeric values: \"PassengerId\", \"Survived\", \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"  \nString values: \"Name\", \"Sex\", \"Embarked\"  \nAlphanumeric values(Numbers and strings together): \"Cabin\", \"Ticket\"","dcffbe14":"First things first. Let's import the required libraries and tools.","de046a21":"#Graphing  \nIf you were bored from all the numbers and tables, this part may be more fun for you.\n##Sex Factor:","2548b775":"Let's take a look at how our values were stored shall we?\nPandas has a simple built in function, 'dtypes', allowing us to very easily look at the data types.","4971431e":"##Time for predictions!  \nSex: Since people have the conception of \"Women and children first\", it's likely that more women survived than men.  \nAge: Just thinking rationally would let us see that people who were younger (Not  so young that they have to be carried by someone of course) than the most were likely to survive more aswell.  \npclass: This is an interesting one. Higher fares may have let people get cabins from a higher part of the Titanic, which would've allowed easier acces to lifeboats.  \nSibSp\/Parch: Who knows, maybe the people travelling alone had better chances. Maybe people's sheer willpower allowed them to survive the harsh event in the end.","061e4a53":"Now that we filled in the missing values, let's turn those into numbers that our computer can process easily.","6b5ed5ca":"We see that most embarked from Southhampton, so we can fill in the empty values with \"S\"","eed28486":"Time to clean up the dataset\nWe should take a look at out test dataset aswell for this one","e9e30865":"#Contents\n    1. Import Necessary Libraries\n    2. Read In and Explore the Data\n    3. Data Analysis\n    4. Data Visualization\n    5. Cleaning Data\n    6. Choosing the Best Model\n    7. Creating Submission File\nAny and all feedback is welcome!","bb809e96":"Well hello there!\nI'm a 17 year old student who was dumb enough to go into data science with not prior coding experience and here we are.\nI'll be trying my best to go along the code and explain it as we get into graphs and all that cool stuff.   \nLet's begin then!","7cdcaae9":"6) Choosing the Best Model"}}