{"cell_type":{"571d1c41":"code","26866b57":"code","f1fc094f":"code","d35985d3":"code","793cbc5b":"code","df6c254f":"code","88b89c75":"code","d6fe6ce8":"code","b22a598e":"code","1f0fa82b":"code","5b8ec370":"code","e1a99379":"code","f7410419":"code","9dd9bcc0":"code","efdf657e":"code","ed78bdfd":"code","9f4abe13":"code","12bf4383":"code","fd5e5469":"code","c81ddc61":"code","0f011168":"code","26c5e1b4":"code","fc471776":"code","f8d8844f":"code","5e29804f":"code","9074eebf":"code","2fa051be":"code","cbe9d132":"code","dfffea1a":"code","6c3f989e":"code","789ecdb0":"markdown","dc1ff702":"markdown","4107d3a4":"markdown","13d90fde":"markdown","d98ac860":"markdown","233e5e42":"markdown","92e48450":"markdown","20b77128":"markdown","2ac48dea":"markdown","2cb7f2a6":"markdown","6deb61c3":"markdown","17d4c843":"markdown","8110add3":"markdown","c17c0fa2":"markdown","66774d13":"markdown"},"source":{"571d1c41":"import numpy as np\nimport random\nimport pandas as pd\nimport re\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.express as px\nimport tensorflow as tf\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","26866b57":"data=pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',header=None,engine='python',encoding='latin1')\nstopwords_man=pd.read_csv('\/kaggle\/input\/stopwords-manual\/stopwords.txt',delimiter='\\t',header=None)[0].values\ndata.columns=['Sentiment','Id','Date','Query','User','Text']\ntrain_data=data.iloc[:,-1].values\ntrain_label=data.iloc[:,0].values","f1fc094f":"stemmer=SnowballStemmer('english')","d35985d3":"data['Sentiment'].value_counts()","793cbc5b":"print(\"A few negative comments\")\ndata.head(20)","df6c254f":"print(\"A few positive comments\")\ndata.tail(20)","88b89c75":"\ndef tweet_clean(tweet):\n    tweet=re.sub(r'@[A-Za-z0-9]+',\" \",tweet) ##Removing the usernames\n    tweet=re.sub(r'^[A-Za-z0-9.!?]+',\" \",tweet) ##Removing digits and punctuations\n    tweet=re.sub(r'https?:\/\/[A-Za-z0-9.\/]+',\" \",tweet) ## removing links\n    tweet=re.sub(r' +',\" \",tweet)\n    tweet = tweet.lower()\n    tweet = re.sub(r\"\\'s\", \" \", tweet)\n    tweet = re.sub(r\"\\'ve\", \" have \", tweet)\n    tweet = re.sub(r\"can't\", \"cannot \", tweet)\n    tweet = re.sub(r\"n't\", \" not \", tweet)\n    tweet = re.sub(r\"\\'d\", \" would \", tweet)\n    tweet = re.sub(r\"\\'ll\", \" will \", tweet)\n    tweet = re.sub(r\"\\'scuse\", \" excuse \", tweet)\n    tweet = tweet.strip(' ')\n    tweet = tweet.strip('. .')\n    tweet = tweet.replace('.',' ')\n    tweet = tweet.replace('-',' ')\n    tweet = tweet.replace(\"\u2019\", \"'\").replace(\"\u2032\", \"'\").replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\n    tweet = tweet.replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\n    tweet = tweet.replace(\"\u20ac\", \" euro \").replace(\"'ll\", \" will\")\n    tweet = tweet.replace(\"don't\", \"do not\").replace(\"didn't\", \"did not\").replace(\"im\",\"i am\").replace(\"it's\", \"it is\")\n    tweet = tweet.replace(\",000,000\", \"m\").replace(\"n't\", \" not\").replace(\"what's\", \"what is\")\n    tweet = tweet.replace(\",000\", \"k\").replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\n    tweet = tweet.replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\n    tweet = re.sub('\\s+', ' ', tweet)\n    tweet=tweet.split()\n    tweet=[stemmer.stem(word) for word in tweet if word not in stopwords_man]\n    tweet=' '.join(word for word in tweet)\n\n    #all_stopwords = stopwords.words('english')\n    return tweet.lower()\n\ntweets_clean=[tweet_clean(tweet) for tweet in train_data]","d6fe6ce8":"data['Text_clean']=tweets_clean\ndata['No_of_Words']=[len(text.split()) for text in data['Text_clean']]","b22a598e":"train_label[train_label==4]=1  ##Resetting the labels for positive tweets to 1","1f0fa82b":"negatives=data['Sentiment']==0\npositives=data['Sentiment']==1","5b8ec370":"fig,ax =plt.subplots(nrows=1,ncols=2,figsize=(15,7.5))\n\nsns.countplot(x=data[positives]['No_of_Words'],label='Positive',ax=ax[0])\nsns.countplot(x=data[negatives]['No_of_Words'],label='Negative',ax=ax[1])\nax[0].set_title('Number of words for positive comments')\nax[1].set_title('Number of words for negative comments')\nplt.tight_layout()\nplt.show()","e1a99379":"data['User'].value_counts().head(20) #Top 20 tweeters","f7410419":"data['Words'] = data['Text_clean'].apply(lambda x:str(x).split())\n\ntop_pos = Counter([word for text in data[positives]['Words'] for word in text])\ntop_pos_df=pd.DataFrame(top_pos.most_common(100),columns=['Words','Counts'])\n\ntop_neg = Counter([word for text in data[negatives]['Words'] for word in text])\ntop_neg_df=pd.DataFrame(top_neg.most_common(100),columns=['Words','Counts'])","9dd9bcc0":"fig,ax=plt.subplots(nrows=1,ncols=2,figsize=(15,7.5))\nsns.barplot(y='Words',x='Counts',data=top_pos_df[:20],color='deepskyblue',ax=ax[0])\nsns.barplot(y='Words',x='Counts',data=top_neg_df[:20],color='coral',ax=ax[1])\nax[0].set_title(\"Most Frequent words in Positive tweets\")\nax[1].set_title(\"Most Frequent words in Negative tweets\")\nplt.show()","efdf657e":"fig = px.treemap(top_pos_df, path=['Words'], values='Counts',title='Most Common Words in Positive Tweets')\nfig.show()","ed78bdfd":"fig = px.treemap(top_neg_df, path=['Words'], values='Counts',title='Most Common Words in Negative Tweets')\nfig.show()","9f4abe13":"def word_cloud(array,add_stopwords):\n    texts=' '.join(text for text in array)\n    stopwords = set(STOPWORDS)\n    stopwords = stopwords.union(add_stopwords)\n    wordcloud = WordCloud(width = 800, height = 800, \n                    background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(texts)\n\n\n    plt.figure(figsize = (15, 7.5), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show()\n    #return texts\nstp = {'ti', \"ame\",\"amp\",\"quot\",\"lol\"}","12bf4383":"word_cloud(data[positives]['Text_clean'],stp)","fd5e5469":"word_cloud(data[negatives]['Text_clean'],stp)","c81ddc61":"tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n    tweets_clean, target_vocab_size=2**16\n)\ntokenizer.save_to_file('tokenizer_tweets')  ##Save our tokenizer to use later","0f011168":"data_inputs = [tokenizer.encode(sentence) for sentence in tweets_clean]","26c5e1b4":"### Defining a constant size for each array and Padding the shorter ones\n\nMax_len=np.max([len(sentence) for sentence in data_inputs])\n\n### Padding with 0s at the end of the sentences since 0 has no value and it wouldn't change the meaning of our sentence\ndata_inputs=tf.keras.preprocessing.sequence.pad_sequences(data_inputs,value=0,padding='post',maxlen=Max_len)","fc471776":"idx=np.random.randint(0,800000,8000)\ntest_idx=np.concatenate((idx,idx+800000))\n\nX_test=data_inputs[test_idx]\ny_test=train_label[test_idx]\n\nX_test=data_inputs[test_idx]\ny_test=train_label[test_idx]\nX_train=np.delete(data_inputs,test_idx,axis=0)\ny_train=np.delete(train_label,test_idx,axis=0)","f8d8844f":"\nclass DCNN(tf.keras.Model):\n\n    def __init__(self,\n                 vocab_size,\n                 emb_dim=128,\n                 nb_filters=50,\n                 FFN_units=256,\n                 nb_classes=2,\n                 dropout_rate=0.1,\n                 training=False,\n                 name='dcnn'):\n        super(DCNN,self).__init__(name=name)\n\n        self.embeddings=layers.Embedding(vocab_size,emb_dim)\n\n        self.bigram=layers.Conv1D(filters=nb_filters,kernel_size=2,\n                                  padding='valid',activation='relu')\n\n        self.trigram=layers.Conv1D(filters=nb_filters,kernel_size=3,\n                                  padding='valid',activation='relu')\n\n        self.fourgram=layers.Conv1D(filters=nb_filters,kernel_size=4,\n                                  padding='valid',activation='relu')   \n\n        self.pooling=layers.GlobalMaxPool1D()\n\n        self.dense_1=layers.Dense(units=FFN_units,activation='relu')\n        self.dropout=layers.Dropout(rate=dropout_rate)\n\n        if nb_classes==2:\n            self.dense_2=layers.Dense(units=1,activation='sigmoid')\n        else:\n            self.dense_2=layers.Dense(units=nb_classes,activation='softmax')  \n\n    def call(self,inputs,training):\n        x=self.embeddings(inputs)\n        x_1=self.bigram(x)\n        x_1=self.pooling(x_1)\n        x_2=self.trigram(x)\n        x_2=self.pooling(x_2)\n        x_3=self.bigram(x)\n        x_3=self.pooling(x_3)\n\n        merged=tf.concat([x_1,x_2,x_3],axis=-1)\n        merged=self.dense_1(merged)\n        merged=self.dropout(merged,training)\n        output=self.dense_2(merged)\n\n        return output","5e29804f":"VOCAB_SIZE = tokenizer.vocab_size\n\nEMB_DIM = 200\nNB_FILTERS = 100\nFFN_UNITS = 256\nNB_CLASSES = 2 #len(set(train_labels))\n\nDROPOUT_RATE = 0.2\n\nBATCH_SIZE = 32\nNB_EPOCHS = 1","9074eebf":"Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n            emb_dim=EMB_DIM,\n            nb_filters=NB_FILTERS,\n            FFN_units=FFN_UNITS,\n            nb_classes=NB_CLASSES,\n            dropout_rate=DROPOUT_RATE)\n\nif NB_CLASSES == 2:\n    Dcnn.compile(loss=\"binary_crossentropy\",\n                 optimizer=\"adam\",\n                 metrics=[\"accuracy\"])\nelse:\n    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n                 optimizer=\"adam\",\n                 metrics=[\"sparse_categorical_accuracy\"])","2fa051be":"###Saving Checkpoints    \n    \ncheckpoint_path = \"\/kaggle\/working\/ckpt\/\"\n\nckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print(\"Latest checkpoint restored!!\")        \n","cbe9d132":"Dcnn.fit(X_train,\n         y_train,\n         batch_size=BATCH_SIZE,\n         epochs=NB_EPOCHS)","dfffea1a":"ckpt_manager.save()  ##Saving our Checkpoints\nDcnn.save('Tweet_sentiment')","6c3f989e":"results=Dcnn.evaluate(X_test,y_test,batch_size=BATCH_SIZE)\nprint(results)","789ecdb0":"### Introduction: \n\nThe objective of this kernel is to analyse the over 1.6 M tweets and process the sentiments. However, the Idea here is to more to see how we can use CNN for Natural Language processing. \n\nCNN can be a very useful for any classification task like spam detection, fraud detection and sentimental analysis.\n\nLet's understand how CNN can be applied to texts for NLP.\n\n- The idea is to like look for local features throught the whole sentence as we'd do for an image.\n- We will try convert the sentences into a matrix form where each row will correspond to a word in the sentence each column will be some feature of the word that CNN will process.\n- Instead of using bag of words method and one hot encoding each word we will have a smaller representation of each word.\n\n- For example lets assume we have `n` different words in our word corpus. To represent a word `Elephants are huge` we can represent it as a `n` dimensional feature vector where each element will be a `0` and `1s` only at the position where `Elephant`, `are` or `huge` are present.\n    With CNN we can reduce the size greatly by representing the sentence in a much smaller non-binary vector with each number between 0 and 1. This can be done by text embedding.\n    \n    `A word embedding is a learned representation for text where words that have the same meaning have a similar representation.`\n    \n ![Embedding.png](attachment:Embedding.png)\n ![linear-relationships.svg](attachment:linear-relationships.svg)\n","dc1ff702":"### Importing Libraries","4107d3a4":"Now that we've cleaned our data, lets do some exploratory data analysis and see how the words are distributed in the positive and negative tweets.","13d90fde":"Due to long training time I just trained it for 1 epoch. But the accuracy of the model can be further increase and reaches around 81% with 5 training epochs.","d98ac860":"Let's take a look at our data","233e5e42":"### Word Cloud","92e48450":"The following visualizations of real embeddings show geometrical relationships that capture semantic relations like the relation between a country and its capital.\n\nWe can introduce mathematical relation between the mapped vectors, which was not earlier possible with the texts. Now we can sum the vectors which will be equivalent to some the meaning.\n   \n                                   [king]-[man]+[woman]=[queen]\n    \n                                   [ottawa]-[canada]+[new delhi]=[india]\n                                   \n #### How cool is that!","20b77128":"### Evaluating our Model","2ac48dea":"### Data Cleaning\n\nLooks like there is lot of cleaning required. We'll use re library and some string functions to remove the user names, digits and punctutaions, hyperlinks etc. We will also remove all the stop words and use stemming to convert all the words to their stem words. For example we can map all sets of words: delivered, deliveries, delivery, deliver to their stem word \"deliver\".","2cb7f2a6":"### Padding\n\nSince we are going to feed our data to a neural network in batches, we have to keep all our examples in same length. In order to do that we are going to use padding to add 0s at the end of each sentences.\nThe good thing about using a zero is that it doesn't corresponds to any word in our tokenizer so we can freely use it without altering the meaning of our sentences.","6deb61c3":"#### Train-Test split\nOur file has sentiments in ordered way, first 800000 are negative sentiments, last 800000 positive.\nSo we have to randomly select 1% from each to keep the test set balanced","17d4c843":"### Data Preprocessing\n\nWe are going to do tokenization, which is essentially converting the list of words in the sentences to lis of numbers. After creating the tokenizer we will use that to encode our data to get set of numbers. If required we can decode the numbers back to get the same exact sentence\/words.","8110add3":"### Finally Let's create our Model","c17c0fa2":"### Building Model\nNow that we have seen how are data looks and how the words are distributed, let's build the model.","66774d13":"Please upvote the Kernel if you found it helpful.\n## Thank You."}}