{"cell_type":{"3b48266b":"code","ac5b169b":"code","c15b068f":"code","879c8d6a":"code","5b5508b3":"code","9d65e19d":"code","64c20bce":"code","fe80c4ba":"markdown","ba0cc75b":"markdown","cf988686":"markdown"},"source":{"3b48266b":"#\u5bfc\u5165\u5fc5\u8981\u7684\u5e93\nimport keras.backend as K\n\nfrom keras.models import Model\n\n# \u5bfc\u5165Keras\u4e0d\u540c\u7684\u5c42\nfrom keras.layers import Conv2D, BatchNormalization, Input, Dropout, Add\nfrom keras.layers import Conv2DTranspose, Reshape, Activation, Dense\nfrom keras.layers import Concatenate, UpSampling2D, Flatten, GlobalAveragePooling2D\n\n# \u5bfc\u5165Adam\u4f18\u5316\u5668\nfrom keras.optimizers import Adam\n\n# \u5bfc\u5165\u8981\u7528\u5230\u7684\u6fc0\u6d3b\u51fd\u6570\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.activations import relu,tanh\n\n# \u5bfc\u5165\u56fe\u50cf\u5904\u7406\u5e93\nfrom keras.preprocessing.image import load_img, img_to_array, array_to_img\nimport numpy as np\n\n# glob\u7528\u4e8e\u5904\u7406\u6587\u4ef6\nimport glob\n\n# \u968f\u673a\nimport random\n\nfrom keras.initializers import RandomNormal\nconv_init = RandomNormal(0, 0.02)","ac5b169b":"# \u5904\u7406\u6570\u636e\ndef load_image(fn, image_size):\n    \"\"\"\n    \u52a0\u8f7d\u4e00\u5f20\u56fe\u7247\n    fn:\u56fe\u50cf\u6587\u4ef6\u8def\u5f84\n    image_size:\u56fe\u50cf\u5927\u5c0f\n    \"\"\"\n    \n    # \u548cImage\u6709\u5173\u7684\u6587\u6863\n    # https:\/\/pillow.readthedocs.io\/en\/3.1.x\/reference\/Image.html\n    # \u6253\u5f00\u56fe\u7247\u5e76\u8f6c\u5316\u6210RGB\u683c\u5f0f\n    im = load_img(fn)\n    img_a = im.crop((0, 0, im.size[0] \/ 2, im.size[1]))\n    img_b = im.crop((im.size[0] \/ 2, 0, im.size[0], im.size[1]))\n    def clip(im,image_size):\n        # \u5207\u5272\u56fe\u50cf(\u622a\u53d6\u56fe\u50cf\u4e2d\u95f4\u7684\u6700\u5927\u6b63\u65b9\u5f62)\n        # crop \u5207\u5272\u56fe\u50cf\uff0c\u63a5\u53d7\u4e00\u4e2a\u56db\u5143\u7ec4\uff0c\u5206\u522b\u8868\u793a\u5de6\u4e0a\u53f3\u4e0b\n        if (im.size[0] >= im.size[1]):\n            im = im.crop(((im.size[0] - im.size[1])\/\/2, 0, (im.size[0] + im.size[1])\/\/2, im.size[1]))\n        else:\n            im = im.crop((0, (im.size[1] - im.size[0])\/\/2, im.size[0], (im.size[0] + im.size[1])\/\/2))\n        im = im.resize((image_size, image_size))\n        return im\n    img_a = clip(img_a, image_size)\n    img_b = clip(img_b, image_size)\n    #\u5c060-255\u7684RGB\u503c\u8f6c\u6362\u5230[-1,1]\u4e0a\u7684\u503c\n    arr_a = img_to_array(img_a) \/ 255 * 2 - 1\n    arr_b = img_to_array(img_b) \/ 255 * 2 - 1\n    return arr_a, arr_b\n\nclass DataSet(object):\n    \"\"\"\n    \u7528\u4e8e\u7ba1\u7406\u6570\u636e\u7684\u7c7b\n    \"\"\"\n    def __init__(self, data_path, image_size = 256):\n        \"\"\"\n        \u6784\u9020\u51fd\u6570\n        \"\"\"\n        \n        # \u6570\u636e\u96c6\u8def\u5f84\n        self.data_path = data_path\n        # \u8f6e\u6570\n        self.epoch = 0\n        # \u521d\u59cb\u5316\u6570\u636e\u5217\u8868\uff08\u8c03\u7528\u81ea\u8eab\u65b9\u6cd5\uff09\n        self.__init_list()\n        # \u56fe\u7247\u5c3a\u5bf8\n        self.image_size = image_size\n        \n    def __init_list(self):\n        # glob.glob \u8f93\u5165pathname, \u8fd4\u56de\u7b26\u5408pathname\u7684\u6587\u4ef6\u540d\u7684\u5217\u8868\n        # \u53ef\u4ee5\u4f7f\u7528\u901a\u914d\u7b26\n        # https:\/\/docs.python.org\/3\/library\/glob.html\n        self.data_list = glob.glob(self.data_path)[6:]\n        \n        # random.shuffle \u6253\u4e71\u5217\u8868\n        # https:\/\/docs.python.org\/3\/library\/random.html#random.shuffle\n        random.shuffle(self.data_list)\n        \n        # \u521d\u59cb\u5316\u6307\u9488\n        self.ptr = 0\n        \n    def get_batch(self, batchsize):\n        \"\"\"\n        \u53d6\u51fabatchsize\u5f20\u56fe\u7247\n        \"\"\"\n        if (self.ptr + batchsize >= len(self.data_list)):\n            # \u5217\u8868\u4e2d\u56fe\u7247\u5df2\u7ecf\u5168\u90e8\u88ab\u53d6\u5b8c\n            # \u5148\u628a\u5217\u8868\u91cc\u7684\u52a0\u8fdb\u6765\n            batch = [load_image(x, self.image_size) for x in self.data_list[self.ptr:]]\n            rest = self.ptr + batchsize - len(self.data_list)\n            \n            # \u91cd\u65b0\u521d\u59cb\u5316\u5217\u8868\n            self.__init_list()\n            \n            # \u518d\u52a0\u5269\u4e0b\u7684\n            batch.extend([load_image(x, self.image_size) for x in self.data_list[:rest]])\n            self.ptr = rest\n            self.epoch += 1\n        else:\n            # \u5df2\u7ecf\u591f\u4e86\n            batch = [load_image(x, self.image_size) for x in self.data_list[self.ptr:self.ptr + batchsize]]\n            self.ptr += batchsize\n        \n        return self.epoch, batch\n    \n    def get_val(self):\n        return np.array([load_image(x, self.image_size) for x in glob.glob(self.data_path)[:6]])\n    \n#     def get_pics(self, num):\n#         \"\"\"\n#         \u53d6\u51fanum\u5f20\u56fe\u7247\uff0c\u7528\u4e8e\u5feb\u7167\n#         \u4e0d\u4f1a\u5f71\u54cd\u961f\u5217\n#         \"\"\"\n#         return np.array([load_image(x, self.image_size) for x in random.sample(self.data_list, num)])\n\ndef arr2image(X):\n    \"\"\"\n    \u5c06RGB\u503c\u4ece[-1,1]\u91cd\u65b0\u8f6c\u56de[0,255]\u7684\u6574\u6570\n    \"\"\"\n    int_X = (( X + 1) \/ 2 * 255).clip(0,255).astype('uint8')\n    return array_to_img(int_X)\n\ndef generate(img, fn):\n    \"\"\"\n    \u5c06\u4e00\u5f20\u56fe\u7247img\u9001\u5165\u751f\u6210\u7f51\u7edcfn\u4e2d\n    \"\"\"\n    r = fn([np.array([img])])[0]\n    return arr2image(np.array(r[0]))\n","c15b068f":"def build_generator(inputs, image_size):\n    x1 = Conv2D(64, kernel_size = 3, padding = \"same\", kernel_initializer = conv_init, strides = 2)(inputs)\n#     x1 = BatchNormalization()(x1, training = 1)\n    x2 = LeakyReLU(alpha = 0.2)(x1)\n    x2 = Conv2D(128, kernel_size = 3, padding = \"same\", strides = 2, kernel_initializer = conv_init,use_bias = False)(x1)\n    x2 = BatchNormalization()(x2, training = 1)\n    x3 = LeakyReLU(alpha = 0.2)(x2)\n    x3 = Conv2D(256, kernel_size = 3, padding = \"same\", strides = 2, kernel_initializer = conv_init, use_bias = False)(x2)\n    x3 = BatchNormalization()(x3, training = 1)\n    x4 = LeakyReLU(alpha = 0.2)(x3)\n    x4 = Conv2D(512, kernel_size = 3, padding = \"same\", strides = 2, kernel_initializer = conv_init, use_bias = False)(x3)\n    x4 = BatchNormalization()(x4, training = 1)\n    x4_1 = LeakyReLU(alpha = 0.2)(x4)\n    x4_1 = Conv2D(1024, kernel_size = 3, padding = \"same\", strides = 2, kernel_initializer = conv_init, use_bias = False)(x4_1)\n    x4_1 = BatchNormalization()(x4_1)\n    x4_1 = LeakyReLU(alpha = 0.2)(x4_1)\n    x4_1 = Conv2DTranspose(512, kernel_size = 3, padding=\"same\", strides = 2, kernel_initializer = conv_init, use_bias = False)(x4_1)\n    x4_1 = BatchNormalization()(x4_1, training = 1)\n    x4_1 = Dropout(0.5)(x4_1)\n    x4_2 = Concatenate()([x4, x4_1]) # \u8df3\u5c42\u8fde\u63a5\n    x4_2 = Activation(\"relu\")(x4_2)\n    x4_2 = Conv2DTranspose(256, kernel_size = 3, padding=\"same\", strides = 2, kernel_initializer = conv_init, use_bias = False)(x4_2)\n    x4_2 = BatchNormalization()(x4_2, training = 1)\n    x4_2 = Dropout(0.5)(x4_2)\n    x5 = Concatenate()([x3, x4_2]) # \u8df3\u5c42\u8fde\u63a5\n    x5 = Activation(\"relu\")(x5)\n    x5 = Conv2DTranspose(128, kernel_size = 3, padding=\"same\", strides = 2, kernel_initializer = conv_init, use_bias = False)(x5)\n    x5 = BatchNormalization()(x5, training = 1)\n    x5 = Dropout(0.5)(x5)\n    x6 = Concatenate()([x2, x5]) # \u8df3\u5c42\u8fde\u63a5\n    x6 = Activation(\"relu\")(x6)\n    x6 = Conv2DTranspose(64, kernel_size = 3, padding=\"same\", strides = 2, kernel_initializer = conv_init, use_bias = False)(x6)\n    x6 = BatchNormalization()(x6, training = 1)\n    x6 = Dropout(0.5)(x6)\n    x7 = Concatenate()([x1, x6]) # \u8df3\u5c42\u8fde\u63a5\n    x7 = Activation(\"relu\")(x7)\n    x7 = Conv2DTranspose(64, kernel_size = 3, padding=\"same\", strides = 2, kernel_initializer = conv_init, use_bias = False)(x7)\n    x7 = BatchNormalization()(x7, training = 1)\n    x7 = Activation(\"relu\")(x7)\n    x7 = Conv2D(3, kernel_size = 3, padding=\"same\", kernel_initializer = conv_init)(x7)\n    x7 = Activation(\"tanh\")(x7)\n    return Model(inputs=inputs, outputs=x7)","879c8d6a":"def build_discriminator(input_a, input_b):\n    # \u6784\u5efa\u5224\u522b\u7f51\u7edc\n    kernel_size = 4\n    layer_filters = [64, 128, 128, 256, 256,]\n\n    x = Concatenate()([input_a, input_b])\n    x = Conv2D(filters=64, kernel_initializer = conv_init,\n                   kernel_size=kernel_size,\n                   strides=2,\n                   padding='same')(x)\n    x = LeakyReLU(alpha = 0.2)(x)\n    for filters in layer_filters:\n        x = Conv2D(filters=filters, kernel_initializer = conv_init,\n                   kernel_size=kernel_size,\n                   strides=2,\n                   padding='same',\n                   use_bias = False\n                  )(x)\n        x = LeakyReLU(alpha = 0.2)(x)\n        x = BatchNormalization()(x, training = 1)\n    x = Conv2D(filters=1, kernel_initializer = conv_init,\n               kernel_size=kernel_size,\n               strides=2,\n               padding='same')(x)    \n    x = Flatten()(x)\n    x = Dense(1)(x)\n    x = Activation('sigmoid')(x)\n    discriminator = Model([input_a, input_b], x)\n    return discriminator","5b5508b3":"LATENT_SIZE= 100\nIMAGE_SIZE = 128\nTRAIN_STEPS = 100000\nEPOCH = 50\n\n# \u6784\u5efa\u6574\u4f53\u7f51\u7edc\n# \u521b\u5efa\u5224\u522b\u7f51\u7edc\ninput_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\ninput_a = Input(shape=input_shape)\ninput_b = Input(shape=input_shape)\ndiscriminator = build_discriminator(input_a, input_b)\noptimizer = Adam(lr=2e-4, beta_1=0.5)\ndiscriminator.compile(loss='binary_crossentropy',\n                      optimizer=optimizer,\n                      metrics=['accuracy'])\n\n# \u521b\u5efa\u751f\u6210\u7f51\u7edc\uff0c\u5e76\u4e14\u548c\u5224\u522b\u7f51\u7edc\u76f8\u8fde\ninput_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\ninput_a = Input(shape=input_shape)\ngenerator = build_generator(input_a, IMAGE_SIZE)\noptimizer = Adam(lr=2e-4, beta_1=0.5)\ndiscriminator.trainable = False\nfake = generator(input_a)\nadversarial = Model(inputs=input_a, outputs=[discriminator([input_a, fake]), fake])\n\nadversarial.compile(loss=['binary_crossentropy', 'mae'], loss_weights=[1, 20],\n                    optimizer=optimizer)","9d65e19d":"BATCH_SIZE = 1\nfrom IPython.display import display\n\ndef train():\n    dataset = DataSet(\"..\/input\/contours2cats\/contours2cats\/contours2cats\/*.png\", IMAGE_SIZE)\n    epoch = 0\n    iteration = 0  \n    while epoch < EPOCH:\n        iteration += 1\n        epoch, data = dataset.get_batch(BATCH_SIZE)\n        data_a = np.array(data)[:, 0, :, :, :]\n        real_images = np.array(data)[:, 1 ,:,:,:]\n        fake_images = generator.predict(data_a)\n        loss1, acc1 = discriminator.train_on_batch([data_a, real_images], np.ones([BATCH_SIZE, 1]))\n        loss2, acc2 = discriminator.train_on_batch([data_a, fake_images], np.zeros([BATCH_SIZE, 1]))\n        #\u8bad\u7ec3\u5224\u522b\u5668\n        \n        log = \"%d %d: [discriminator loss: %f, acc: %f]\" % (epoch, iteration, (loss1  + loss2)\/2, (acc1+acc2)\/2)\n        \n        y = np.ones([BATCH_SIZE, 1])\n        loss = adversarial.train_on_batch(data_a, [y, real_images])\n        # \u8bad\u7ec3\u751f\u6210\u5668\n        log = \"%s [adversarial loss: %s\" % (log, loss)\n        \n        if (iteration % 20 == 0):\n            print(log)\n        \n        if (iteration % 200 == 0):\n            data_val = dataset.get_val()\n            data_a_val = np.array(data_val)[:, 0, :, :, :]\n            real_images_val = np.array(data_val)[:, 1 ,:,:,:]\n            out_val = np.array(np.concatenate([generator.predict(np.array([x])) for x in data_a_val]))\n            d1 = np.concatenate(out_val, axis = 1)\n            d2 = np.concatenate(data_a_val, axis = 1)\n            d3 = np.concatenate(real_images_val, axis = 1)\n            d = np.concatenate([d1, d2, d3])\n            display(arr2image(d))\n        \n        if (epoch % 500 == 0):\n            generator.save(\"g_model.h5\")\n","64c20bce":"# train()","fe80c4ba":"## \u6570\u636e\u5904\u7406","ba0cc75b":"## \u751f\u6210\u7f51\u7edc\uff08UNET\uff09","cf988686":"## \u5224\u522b\u7f51\u7edc"}}