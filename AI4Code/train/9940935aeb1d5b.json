{"cell_type":{"5c7bd39d":"code","38b2e631":"code","458907fa":"code","48e953ac":"code","bf5cb8c5":"code","a0d52771":"code","c5d1e32d":"code","7cb61b68":"code","ca0daf12":"code","8db283f7":"code","31e6ba38":"code","6fd0620b":"code","28a2d280":"code","6b86a136":"code","21a57daf":"code","239b5b29":"code","651a1b6d":"code","920da2b5":"code","32bafa04":"code","6744528a":"code","db3e066b":"code","f566437d":"code","f18ebc2b":"code","471fb58a":"code","f61bf31a":"code","8977f95a":"code","bee0a2a1":"code","8aea07bd":"code","c6d0702e":"code","cde3dc97":"code","b3acaf57":"code","784417c2":"code","69cd2978":"code","cf3030c4":"code","b4db09e2":"code","219445ed":"code","a2820ce2":"code","da4387b2":"code","f45b50c2":"code","c1b1a95b":"code","a5b20fb7":"code","8b08d332":"code","657b3e1e":"code","991eb9e1":"code","48ea7dbd":"code","65182066":"code","4d9a6b1f":"code","2843e52c":"code","be9945f4":"code","486187ee":"code","9efccdb6":"code","11cf4897":"code","f4bea9c1":"code","423689cf":"code","f47b9d50":"code","35d65533":"markdown","12682e2a":"markdown","c347d59b":"markdown","f8e6f998":"markdown","d65dbad5":"markdown","d1b9377a":"markdown","d30e21f3":"markdown","05cd4846":"markdown","af1b07af":"markdown","e84ab4ca":"markdown","60290a45":"markdown","79c719e8":"markdown","93a1bea3":"markdown","a635f9f8":"markdown","4dc8f6cf":"markdown","618122b0":"markdown","537fbb77":"markdown","5eb53e50":"markdown","48f343b8":"markdown","4492ec78":"markdown","4196168e":"markdown","4415b38e":"markdown","34eebde2":"markdown","661e91e7":"markdown"},"source":{"5c7bd39d":"!pip install pysastrawi","38b2e631":"!pip install tweepy\n!pip install tweet-preprocessor","458907fa":"pip install googletrans==3.1.0a0","48e953ac":"consumer_key = \"u65iNRI1d358KZFVbVUBf3AlM\"\nconsumer_secret = \"4FQWz4JiKpGx6hJbY9SLuLctTVeTupqov1QqTYlZzi4eaEsxjv\"\naccess_key = \"1376057725944094720-p6zzP242Aww20YcAkdyNaF6VJC3OJY\"\naccess_secret = \"eRb3PdKRLe59KlBuEH2SFNpYgLteuKk0qf1Y1SgJhcNOg\"","bf5cb8c5":"import os\nimport pandas as pd\nimport tweepy\nimport re\nimport string\nfrom textblob import TextBlob\nimport preprocessor as p\nfrom preprocessor.api import clean, tokenize, parse\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport ast\n\nnltk.download('stopwords')\nfrom nltk.stem.porter import PorterStemmer","a0d52771":"\n#the code here is based on the reference below, after some modification and combine improvement from various other sources\n#references : https:\/\/towardsdatascience.com\/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf\n#used after some modification \n\n#Twitter credentials for the app\n\n\n#pass twitter credentials to tweepy [1]\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_key, access_secret)\napi = tweepy.API(auth)\n\n#file location changed to \"data\/telemedicine_data_extraction\/\" for clearer path [2]\nif not os.path.exists('data'):\n    os.mkdir('data')\nif not os.path.exists('data\/data_extraction'):\n    os.mkdir('data\/data_extraction')\n\ncovid19_indonesia = \"data\/data_extraction\/covid19_indonesia.csv\"\n\n#columns of the csv file [3]\nCOLS = ['id', 'created_at', 'source', 'original_text','clean_text',  'lang',\n        'favorite_count', 'retweet_count', 'original_author', 'possibly_sensitive', 'hashtags',\n        'user_mentions', 'place', 'place_coord_boundaries']\n\n#set two date variables for date range [4]\nstart_date = '2021-05-20'\nend_date = '2021-06-20'\n\n# Happy Emoticons [5]\nemoticons_happy = set([\n    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n    '<3'\n    ])\n\n# Sad Emoticons\nemoticons_sad = set([\n    ':L', ':-\/', '>:\/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n    ':-[', ':-<', '=\\\\', '=\/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n    ':c', ':{', '>:\\\\', ';('\n    ])\n\n#Emoji patterns\nemoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n\n#combine sad and happy emoticons\nemoticons = emoticons_happy.union(emoticons_sad)\n\n\n#mrhod clean_tweets() [6]\ndef clean_tweets(tweet):\n    #use slang words and stopwords to clean the data\n    # the stop words and slang words that is used, comes from https:\/\/github.com\/louisowen6\/NLP_bahasa_resources\n    # after some modification\n\n    # [7]\n    my_file = open(\"..\/input\/cleaning-source\/combined_stop_words.txt\", \"r\")\n    content = my_file.read()\n    stop_words = content.split(\"\\n\")\n    file_2  = open(\"..\/input\/cleaning-source\/update_combined_slang_words.txt\", \"r\")\n    content2 = file_2.read()\n    slang_words = ast.literal_eval(content2)\n    my_file.close()\n    file_2.close()\n    \n    # [8]\n    tweet = tweet.lower()\n    #after tweepy preprocessing the colon left remain after removing mentions\n    #or RT sign in the beginning of the tweet\n    tweet = re.sub(r':', '', tweet)\n    tweet = re.sub(r'\u201a\u00c4\u00b6', '', tweet)\n    #replace consecutive non-ASCII characters with a space\n    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n\n    #remove emojis from tweet\n    tweet = emoji_pattern.sub(r'', tweet)\n\n    #remove punctuation manually\n    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n    \n    #remove tags\n    tweet=re.sub(\"&lt;\/?.*?&gt;\",\"&lt;&gt;\",tweet)\n    \n    #remove digits and special chars\n    tweet=re.sub(\"(\\\\d|\\\\W)+\",\" \",tweet)\n\n    #remove other symbol from tweet\n    tweet = re.sub(r'\u00e2', '', tweet)\n    tweet = re.sub(r'\u20ac', '', tweet)\n    tweet = re.sub(r'\u00a6', '', tweet)\n\n    # [9]\n    #modify the slang words into a more proper one\n    word_tokens = word_tokenize(tweet)\n    for w in word_tokens:\n        if w in slang_words.keys():\n            word_tokens[word_tokens.index(w)] = slang_words[w]\n\n    # [10]\n    #filter using NLTK library append it to a string\n    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n    filtered_tweet = []\n\n    # [11]\n    #looping through conditions\n    for w in word_tokens:\n        #check tokens against stop words , emoticons and punctuations\n        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n            filtered_tweet.append(w.lower())\n    return ' '.join(filtered_tweet)\n    #print(word_tokens)\n    #print(filtered_sentence)\n\n#method write_tweets()\ndef write_tweets(keyword, file):\n    # If the file exists, then read the existing data from the CSV file.\n    if os.path.exists(file):\n        df = pd.read_csv(file, header=0)\n    else:\n        df = pd.DataFrame(columns=COLS)\n    #page attribute in tweepy.cursor and iteration\n    for page in tweepy.Cursor(api.search, q=keyword,\n                              count=200, include_rts=False, since=start_date, tweet_mode=\"extended\").pages(100):\n        for status in page:\n            new_entry = []\n            status = status._json\n\n            #when run the code, below code replaces the retweet amount and\n            #no of favorires that are changed since last download.\n            if status['created_at'] in df['created_at'].values:\n                i = df.loc[df['created_at'] == status['created_at']].index[0]\n                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n                   status['retweet_count'] != df.at[i, 'retweet_count']:\n                    df.at[i, 'favorite_count'] = status['favorite_count']\n                    df.at[i, 'retweet_count'] = status['retweet_count']\n                continue\n\n           #tweepy preprocessing called for basic preprocessing\n            clean_text = clean(status['full_text'])\n\n            #call clean_tweet method for extra preprocessing\n                \n            filtered_tweet=clean_tweets(clean_text)\n           \n            #new entry append\n            new_entry += [status['id'], status['created_at'],\n                          status['source'], status['full_text'],filtered_tweet,  status['lang'],\n                          status['favorite_count'], status['retweet_count']]\n\n            #to append original author of the tweet\n            new_entry.append(status['user']['screen_name'])\n\n            try:\n                is_sensitive = status['possibly_sensitive']\n            except KeyError:\n                is_sensitive = None\n            new_entry.append(is_sensitive)\n\n            # hashtagas and mentiones are saved using comma separted\n            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n            new_entry.append(hashtags)\n            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n            new_entry.append(mentions)\n\n            #get location of the tweet if possible\n            try:\n                location = status['user']['location']\n            except TypeError:\n                location = ''\n            new_entry.append(location)\n\n            try:\n                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n            except TypeError:\n                coordinates = None\n            new_entry.append(coordinates)\n\n            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n            df = df.append(single_tweet_df, ignore_index=True)\n            csvFile = open(file, 'a' ,encoding='utf-8')\n    df.to_csv(csvFile, mode='a', columns=COLS, index=False, encoding=\"utf-8\")\n\n#declare keywords as a query for three categories\ncovid19_indonesia_keywords = '#PSBB OR ((#covid19 OR covid19) AND (indonesia OR pemerintah)) OR ((#wfh OR wfh) AND (indonesia OR pemerintah)) OR PSBB '\n\n#call main method passing keywords and file path\n\nwrite_tweets(covid19_indonesia_keywords,covid19_indonesia)","c5d1e32d":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime,timedelta\nimport pytz \nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport ast\nimport string\nfrom wordcloud import WordCloud\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom googletrans import Translator","7cb61b68":"df = pd.read_csv('..\/input\/cleaning-source\/covid19_indonesia (1).csv')","ca0daf12":"df = df.drop_duplicates()\ndf = df.reset_index(drop=True)","8db283f7":"len(df[df['clean_text'].isnull()==True])","31e6ba38":"pd.set_option('display.max_colwidth', None)\ndf[df['clean_text'].isnull()==True]['original_text']","6fd0620b":"df = df.dropna(subset=['clean_text'])\ndf = df.reset_index(drop=True)","28a2d280":"df.isnull().sum()","6b86a136":"df[df['lang']!='in']['lang'].value_counts()","21a57daf":"def trans(x,src):\n    translator = Translator()\n    try:\n        sentence = translator.translate(x, src=src,dest='id').text\n    except:\n        sentence = x\n    return sentence","239b5b29":"df['clean_text'] = df.apply(lambda x: trans(x['clean_text'],x['lang']) if(x['lang']!='in') else x['clean_text'],axis=1)","651a1b6d":"df.head(2)","920da2b5":"clean_text = df['clean_text'].copy()","32bafa04":"pd.set_option('display.max_colwidth', 100)\nclean_text.tail(15)","6744528a":"def repair_exaggeration(x):\n    word_tokens = word_tokenize(x)\n    new_x =''\n    for i in word_tokens:\n        if (i =='psbb'):\n            new = re.sub(r'(\\w)\\1\\1+',r'\\1\\1',i)\n            new_x = new_x +new+' '\n        elif(i =='psb'):\n            new = 'psbb'\n            new_x = new_x +new+' '\n        else:\n            new = re.sub(r'(\\w)\\1\\1\\1+',r'\\1',i)\n            new_x = new_x +new+' '\n    return new_x\n\ndef del_word(x,key_list):\n    n = len(key_list)\n    word_tokens = word_tokenize(x)\n    new_x =''\n    for word in word_tokens:\n        if word not in key_list:\n            new_x = new_x+word+' '\n    return new_x\n\ndef clean_tweets(tweet):\n   # nltk.download('stopwords')\n    my_file = open(\"..\/input\/cleaning-source\/combined_stop_words.txt\", \"r\")\n    content = my_file.read()\n    stop_words = content.split(\"\\n\")\n    file_2  = open(\"..\/input\/cleaning-source\/update_combined_slang_words.txt\", \"r\")\n    content2 = file_2.read()\n    slang_words = ast.literal_eval(content2)\n    my_file.close()\n    file_2.close()\n\n    tweet = tweet.lower()\n    #after tweepy preprocessing the colon left remain after removing mentions\n    #or RT sign in the beginning of the tweet\n    tweet = re.sub(r':', '', tweet)\n    tweet = re.sub(r'\u201a\u00c4\u00b6', '', tweet)\n    #replace consecutive non-ASCII characters with a space\n    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n\n    #remove emojis from tweet\n    #tweet = emoji_pattern.sub(r'', tweet)\n    \n    #remove punctuation manually\n    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n    \n    #remove tags\n    tweet=re.sub(\"&lt;\/?.*?&gt;\",\"&lt;&gt;\",tweet)\n    \n    #remove digits and special chars\n    tweet=re.sub(\"(\\\\d|\\\\W)+\",\" \",tweet)\n\n    #remove other symbol from tweet\n    tweet = re.sub(r'\u00e2', '', tweet)\n    tweet = re.sub(r'\u20ac', '', tweet)\n    tweet = re.sub(r'\u00a6', '', tweet)\n\n    word_tokens = word_tokenize(tweet)\n    for w in word_tokens:\n        if w in slang_words.keys():\n            word_tokens[word_tokens.index(w)] = slang_words[w]\n\n    #filter using NLTK library append it to a string\n    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n    filtered_tweet = []\n\n    #looping through conditions\n    for w in word_tokens:\n        #check tokens against stop words , emoticons and punctuations\n        if w not in stop_words and w not in string.punctuation:\n            filtered_tweet.append(w.lower())\n    return ' '.join(filtered_tweet)\n\ndef count_words(x):\n    words = word_tokenize(x)\n    n=len(words)\n    return n","db3e066b":"clean_text_exag = clean_text.apply(lambda x: repair_exaggeration(x))","f566437d":"clean_text_exag.tail(15)","f18ebc2b":"re_clean = clean_text_exag.apply(lambda x: clean_tweets(x))","471fb58a":"keyword = ['psbb','psb','corona','covid19','indonesia','pemerintah','wfh','covid']\nclean_text_extra = re_clean.apply(lambda x: del_word(x,keyword))","f61bf31a":"clean_text_extra.tail(15)","8977f95a":"df['clean_text'] = clean_text_extra","bee0a2a1":"df.to_csv('clean_df.csv')","8aea07bd":"df = pd.read_csv('..\/input\/cleaning-source\/clean_df.csv')","c6d0702e":"df.head()","cde3dc97":"df['clean_text']","b3acaf57":"df = df.drop_duplicates(subset='clean_text', keep='first').reset_index()\ndf.shape","784417c2":"from textblob import TextBlob as tb\n\ndef trans(x):\n    translator = Translator()\n    try:\n        sentence = translator.translate(x,dest='en').text\n    except:\n        sentence = x\n    return sentence\n\ndef en_to_id(sentence):\n    if tb(sentence).detect_language() == 'en':\n        return tb(sentence)\n    \n    translator = Translator()\n    \n    output = translator.translate(sentence, dest='en')\n    return tb(output.text)\n    \ndef get_sentiment(sentence):\n    sentence = en_to_id(sentence)\n    return sentence.sentiment\n\ndef round_polarity(value):\n    if value >= 0.3:\n        return 1\n    elif value == 0:\n        return 0\n    return -1\n\ndef round_subjectivity(value):\n    if value >= 0:\n        return 1\n    elif value == 0:\n        return 0\n    return -1","69cd2978":"translator = Translator()\nsentence = translator.translate(\"makanan\",dest='en').text\nsentence","cf3030c4":"tweets = df['clean_text']\npolarity = []\nsubjectivity = []\n\ntw_trans = []\nfor tweet in tweets:    \n#     translate = trans(tweet)\n#     tw_trans.append(translate)\n    an_sentiment = get_sentiment(tweet)\n    polarity.append(an_sentiment.sentiment[0])    \n    subjectivity(an_sentiment.sentiment[1])","b4db09e2":"tw_trans[4].text","219445ed":"polarity = []\nsubjectivity = []\n\nfor twtrans in tw_trans:\n    an_sentiment = get_sentiment(twtrans)\n    polarity.append(an_sentiment.sentiment[0])    \n    subjectivity(an_sentiment.sentiment[1])","a2820ce2":"tw_df = pd.read_csv('..\/input\/cleaning-source\/dataframe-2.csv')","da4387b2":"tw_df.head()","f45b50c2":"import matplotlib.pyplot as plt\ndata_pos = tw_df[tw_df['sentiment']=='positif']\nword_data_pos = ' '.join(word for word in data_pos['original_tweets'])\nwordcloud = WordCloud(\n    colormap='Reds',\n    width=750, \n    height=750, \n    mode='RGBA',\n    background_color='white'\n).generate(word_data_pos)\n\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.show()","c1b1a95b":"import matplotlib.pyplot as plt\ndata_neg = tw_df[tw_df['sentiment']=='negatif']\nword_data_pos = ' '.join(word for word in data_neg['original_tweets'])\nwordcloud = WordCloud(\n    colormap='Blues_r',\n    width=750, \n    height=750, \n    mode='RGBA',\n    background_color='white'\n).generate(word_data_pos)\n\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.show()","a5b20fb7":"import matplotlib.pyplot as plt\ndata_neu = tw_df[tw_df['sentiment']=='neutral']\nword_data_pos = ' '.join(word for word in data_neu['original_tweets'])\nwordcloud = WordCloud(\n    colormap='Blues_r',\n    width=750, \n    height=750, \n    mode='RGBA',\n    background_color='white'\n).generate(word_data_pos)\n\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.show()","8b08d332":"def cleansing(tweet_txt):\n    # lower text\n    tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet_txt)\n    tweet = tweet.lower()\n    tweet = re.sub(r\"\\d+\", \"\", tweet)\n    tweet = re.sub('rt', '', tweet)\n    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n    \n    return tweet","657b3e1e":"tweets_ = []\n\nfor i, row in tw_df.iterrows():\n    tweets_.append(cleansing(row['original_tweets']))","991eb9e1":"tw_df['clean_tweet'] = tweets_\ntw_df.head()","48ea7dbd":"from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\nfactory = StopWordRemoverFactory()\nstopword_sastrawi = factory.create_stop_word_remover()\n\nkalimat = 'Dengan Menggunakan Python dan Library Sastrawi Saya Dapat Melakukan Proses Stopword Removal'\nstop = stopword_sastrawi.remove(kalimat)\nstop","65182066":"tweets_ = []\nfor i, tw in tw_df.iterrows():\n    tweets_.append(stopword_sastrawi.remove(tw['clean_tweet']))","4d9a6b1f":"tw_df['clean_tweet'] = tweets_\ntw_df.head()","2843e52c":"from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n\nstemmer_factory = StemmerFactory()\nstemmer = stemmer_factory.create_stemmer()\n\ntweets_ = []\nfor i, tw in tw_df.iterrows():\n    tweets_.append(stemmer.stem(tw['clean_tweet']))\n    \ntw_df['clean_tweet'] = tweets_\ntw_df.head()","be9945f4":"import sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    tw_df['clean_tweet'], \n    tw_df['sentiment'],\n    test_size=0.3,\n    random_state=9\n)","486187ee":"print('Jumlah pembagian data:')\nprint(f'Train: {len(X_train)}')\nprint(f'Train: {len(X_test)}')","9efccdb6":"from sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\ntext_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', MultinomialNB())])","11cf4897":"text_clf.fit(X_train, y_train)","f4bea9c1":"from sklearn.metrics import classification_report\n \nprint(classification_report(y_test, text_clf.predict(X_test), digits=3))","423689cf":"pred = text_clf.predict(X_test)","f47b9d50":"from sklearn.metrics import recall_score, precision_score, confusion_matrix, accuracy_score\n\nprint(f'Confusion Matrix:\\n {confusion_matrix(y_test, pred)}')\nprint(f'Accuracy:\\n {accuracy_score(y_test, pred)}')\nprint(f'Recall:\\n {recall_score(y_test, pred, average=None)}')\nprint(f'Precision:\\n {precision_score(y_test, pred, average=None)}')","35d65533":"## Import dataset","12682e2a":"## Menggunakan library sastrawi untuk Menghapus STOPWORD PADA DATA TRAIN","c347d59b":"## Stemming Dengan Sastrawi","f8e6f998":"## Menerapkan Sastrawi Stopword Remover","d65dbad5":"## Prediksi model dengan data test ","d1b9377a":"## Method untuk menghapus kata yang berlebihan, Contoh: psbbbbbbbbb","d30e21f3":"# RUN HERE","05cd4846":"## Menghapus Duplikat","af1b07af":"## menghapus data duplikat","e84ab4ca":"# TRAINING & KLASIFIKASI\n## Cleaning data kembali untuk memastikan dokumen bersih","60290a45":"## Contoh Translator","79c719e8":"## Mendefinisikan translator untuk translate tweet","93a1bea3":"## menerapkan pembersihan dokumen","a635f9f8":"## Mendefinisikan Pipeline Training Data\n### CountVectorizer untuk menghitung vector setiap kata\n### TfidfTransformer untuk mengekstrak featur\n### MultinomialNB untuk Klasifikasi","4dc8f6cf":"# Penjelasan kode\nKode dibawah untuk meng-generate dataset\n\n[1] Mendefinisikan credential pada library tweepy (consumer key, consumer secret, access key, access secret)\n[2] Definisikan lokasi output Dataset\n[3] Mendefinisikan kolom tabel dalam CSV\n[4] Mendefinisikan rentang waktu post tweet\n[5] Membuat Array yang berisikan emoticon dan emoji untuk stopwords\n[6] Membuat Method untuk membersighkan data\n[7] Mendefinisikan lokasi stopword eksternal untuk kata-kata yang tidak terdapat pada Stopword NLTK \n[8] Preprocesing data, membuat huruf jadi kecil, tanda baca, ascii karakter, angka,emoji\n[9] filter dengan stopword NLTK\n","618122b0":"# Visualisasi Word Cloud Setiap Kelas","537fbb77":"[Negatif,Neutral,Positif]","5eb53e50":"## Translate data dan Labeling data\nLangkah-langkah:\n1. mendefinisikan array polarity, subjectivity\n2. Iterasi Kolom Tweet yang sudah Bersih\n3. Translat setiap Tweet dalam bahasa inggris\n4. Analisa Polaritas Menggunakan Text Blob yang telah didefinisikan dalam method get_sentiment\n5. Translate data berfungsi agar library TextBlob dapat bekerja dengan baik, karena library ini belum memiliki adopsi bahasa indonesia","48f343b8":"## Mencetak classification Metric -> Confusion Matrix, Accuracy, Recall, dan Precision","4492ec78":"## Training Model NAIVE BAYES melalui Pipeline","4196168e":"## Melihat laporan Training Data","4415b38e":"## Membagi data Train-Test\nPerbandingan nya\n70:30","34eebde2":"## Mendefinisika method untuk translate TWEET, dan Mendefinisikan Library TextBlob untuk mendeteksi POLARITY\n","661e91e7":"## Export dataset dalam bentuk CSV"}}