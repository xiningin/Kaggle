{"cell_type":{"858b15c6":"code","67f12c1d":"code","e58bac1d":"code","b6b1b74c":"code","bb296354":"code","eb56e6e6":"code","114feece":"code","dd7c48d1":"code","083ae27a":"code","24ced8ff":"code","076baf40":"code","c9ec34c3":"code","c7655ef0":"code","2f8e6301":"code","4104f4da":"code","9ac977a4":"code","0fef6dd5":"code","f434e4a6":"code","1fbde9ae":"code","853067a2":"code","173f3218":"code","8bcbaba5":"code","ecbd565b":"code","72ddd5f4":"code","6e1d4187":"code","0f7ccdf8":"code","9b38b68f":"code","46d847b2":"code","548734e4":"code","c551b280":"markdown","1b9ce394":"markdown","8fea8679":"markdown","520dd5c3":"markdown","0ee7ef9d":"markdown","7a68e8ba":"markdown","d6bc2c16":"markdown","b8b42807":"markdown","9699bed0":"markdown"},"source":{"858b15c6":"import re\nimport pandas as pd\nfrom io import StringIO\nimport io\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport glob\nfrom torch.utils.data import DataLoader,Dataset\nfrom torch import nn\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision import models as models\nfrom torchvision.models.utils import load_state_dict_from_url\nfrom sklearn.metrics import recall_score,precision_score,f1_score\nfrom PIL import Image    \nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nmatplotlib.style.use('ggplot')","67f12c1d":"FILENAME = ['train.csv', 'test.csv']\n\n# Import train and test files\nwith open(FILENAME[0]) as file:\n    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1\/\"\\2', line) for line in file]\ntrain = pd.read_csv(StringIO(''.join(lines)), escapechar=\"\/\")\n\nwith open(FILENAME[1]) as file:\n    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1\/\"\\2', line) for line in file]\ntest = pd.read_csv(StringIO(''.join(lines)), escapechar=\"\/\")\ntrain.head(3)","e58bac1d":"# Use only selected columns\ntrain_csv = train[['ImageID', 'Labels']]\ntest_csv = test[['ImageID']]\npath = os.getcwd()\n#\uff08In windows environment\uff09In linux should change the location\nimg_path = path+'\\\\data\\\\'  # Image data path\ntrain_csv.head(3)","b6b1b74c":"# Check the number of class\nraw_labels = train[\"Labels\"]\nint_labels = []\nfinal_labels = []\nfor i in raw_labels:    \n    int_labels.append(list(map(int, i.split())))\n    for j in int_labels:\n        for k in j:\n            final_labels.append(k)\n    \nset(final_labels)","bb296354":"# Define a class that get all images used for calculate mean&std\nclass ImageNormal(Dataset):\n\n    def __init__(self, csv, path): \n        self.csv = csv\n        self.path = path\n        self.transform = transforms.Compose([   # Resize and to tensor\n                        transforms.RandomResizedCrop(224),\n                        transforms.RandomHorizontalFlip(), \n                        transforms.ToTensor() \n                    ])\n        \n    def __len__(self):\n        return len(self.csv.index)        \n\n    def __getitem__(self, idx):            \n        img_name = os.path.join(self.path, self.csv.iloc[idx, 0])\n        img = Image.open(img_name)\n        img = self.transform(img)\n        return (img) # Return only images\n\ntrain_ds = ImageNormal(csv=train_csv, path=img_path)\ntrain_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n\ntest_ds = ImageNormal(csv=test_csv, path=img_path)\ntest_dl = DataLoader(train_ds, batch_size=64, shuffle=True)","eb56e6e6":"# Define the calculator which will return the mean&std in RGB 3 channels\n# Got this calculator from pytorch forum\ndef cal_mean_std(loader):\n    \"\"\"\n        Var[x] = E[X^2] - E^2[X]\n    \"\"\"\n    cnt = 0\n    fst_moment = torch.empty(3)\n    snd_moment = torch.empty(3)\n \n    for images in loader:\n \n        b, c, h, w = images.shape\n        nb_pixels = b * h * w\n        sum_ = torch.sum(images, dim=[0, 2, 3])\n        sum_of_square = torch.sum(images ** 2, dim=[0, 2, 3])\n        fst_moment = (cnt * fst_moment + sum_) \/ (cnt + nb_pixels)\n        snd_moment = (cnt * snd_moment + sum_of_square) \/ (cnt + nb_pixels)\n \n        cnt += nb_pixels\n \n    return fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)","114feece":"# Get the normalization outpouts\ntrain_mean, train_std = cal_mean_std(train_dl)\ntest_mean, test_std = cal_mean_std(test_dl)\nprint(train_mean, train_std)\nprint(test_mean, test_std)\n\n# Note that due to randomness every time the output is very slightly different, which does not affect the performance.\n# tensor([0.4666, 0.4472, 0.4182]) tensor([0.2686, 0.2640, 0.2779])\n# tensor([0.4673, 0.4481, 0.4194]) tensor([0.2689, 0.2644, 0.2783])","dd7c48d1":"# Split train data to train and validation datasets\ntrain_df,val_df = train_test_split(train_csv, test_size=0.1) # Use 0.1 to keep more data to train\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\nprint(f\"Validation_Data Length: {len(val_df)}\\n Train_Data Length: {len(train_df)}\")","083ae27a":"# Define another class to pre-process and get the dataset\nclass ImageDataset(Dataset):\n\n    def __init__(self, csv, path, test): \n        self.csv = csv\n        self.path = path\n        self.test = test\n        if self.test == False:  # If train or validation\n                self.transform = transforms.Compose([\n                                transforms.RandomResizedCrop(224), # Resize\n                                transforms.RandomHorizontalFlip(p=0.5), # Random flip\n                                transforms.RandomVerticalFlip(p=0.5), # Random flip\n                                transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5), # Random color\n                                transforms.ToTensor(),\n                                transforms.Normalize([0.4664, 0.4470, 0.4183], [0.2685, 0.2641, 0.2779]) # We got earlier\n                            ])\n        elif self.test == True:  # If test, no augmentation\n                self.transform = transforms.Compose([\n                transforms.Resize(224),\n                transforms.ToTensor(),\n                transforms.Normalize([0.4660, 0.4467, 0.4179], [0.2686, 0.2640, 0.2778])\n    ])\n\n    def __len__(self):\n        return len(self.csv.index)\n\n    def __getitem__(self, idx):     # Get images and labels \n        img_name = os.path.join(self.path, self.csv.iloc[idx, 0]) # Get image names from the first column of data\n        img = Image.open(img_name) \n        \n        if self.test == False: # If train or validation, get labels as well\n            img_labels = self.csv.iloc[idx, 1]\n            label_tensor = torch.zeros((1, 20)) # Define 20 one-hot vectors, the index will be its class\n            \n            for label in img_labels.split(\" \"):\n                label_tensor[0, int(label)] = 1\n\n            img_label = torch.tensor(label_tensor,dtype= torch.float32)\n            img = self.transform(img)\n            return (img, img_label.squeeze()) # Return both image and label\n        elif self.test == True: # If test\n            img = self.transform(img)\n            img = torch.tensor(img,dtype = torch.float32)\n        return (img) # Return only image","24ced8ff":"# Define batch size\nbatch_size = 32\n# train dataset\ntrain_data = ImageDataset(\n    train_df, path=img_path, test = False\n)\n\n# train data loader\ntrain_loader = DataLoader(\n    train_data, \n    batch_size=batch_size,\n    shuffle=True\n)\n\n# validation dataset\nval_data = ImageDataset(\n    val_df, path=img_path, test = False\n)\n\n# validation data loader\nval_loader = DataLoader(\n    val_data, \n    batch_size=batch_size,\n    shuffle=True\n)","076baf40":"# Get pretrained resnet50 model\ndef model(requires_grad):\n    model = models.resnet50(progress=True, pretrained=True)\n    \n    if requires_grad == False:  # Freeze weights\n        ct = 0\n        for child in model.children():\n            ct += 1\n            if ct < 7:\n                for param in child.parameters():\n                    param.requires_grad = False\n\n    elif requires_grad == True:  # Not freeze weights\n        for param in model.parameters():\n            param.requires_grad = True\n\n    model.fc = nn.Linear(2048, 20)  # To 20 classes\n    return model","c9ec34c3":"# training function\ndef train(model, dataloader, optimizer, criterion, train_data, device):\n    print('Training')\n    model.train()\n    counter = 0\n    train_running_loss = 0.0\n    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)\/dataloader.batch_size)):\n        counter += 1\n        data, target = data\n        data, target = data.to(device), target.to(device)  # To GPU\n        optimizer.zero_grad()\n        outputs = model(data)  # Get model outputs\n        outputs = torch.sigmoid(outputs)  # Apply sigmoid to [0, 1]\n        loss = criterion(outputs, target)  # Compute loss\n        train_running_loss += loss.item()\n        loss.backward()  # Backpropagation\n        optimizer.step()  # Update\n        \n    train_loss = train_running_loss \/ counter\n    return train_loss","c7655ef0":"# validation function: Similar to training function but no BP and Update\ndef validate(model, dataloader, criterion, val_data, device):\n    print('Validating')\n    model.eval()\n    counter = 0\n    val_running_loss = 0.0\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)\/dataloader.batch_size)):\n            counter += 1\n            data, target = data\n            data, target = data.to(device), target.to(device) # To GPU\n            outputs = model(data)\n            outputs = torch.sigmoid(outputs)\n            loss = criterion(outputs, target)\n            val_running_loss += loss.item()\n        \n        val_loss = val_running_loss \/ counter\n        return val_loss","2f8e6301":"# initialize device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","4104f4da":"#intialize the model\nmodel = model(requires_grad=False).to(device)\n# hyperparameters\nlr = 0.0001\nepochs = 15\nbatch_size = 32\noptimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.00005)\ncriterion = nn.BCELoss()","9ac977a4":"%%time\n# start the training and validation\ntrain_loss = []\nvalid_loss = []\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss = train(\n        model, train_loader, optimizer, criterion, train_data, device\n    )\n    valid_epoch_loss = validate(\n        model, val_loader, criterion, val_data, device\n    )\n    train_loss.append(train_epoch_loss)\n    valid_loss.append(valid_epoch_loss)\n    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n    print(f'Val Loss: {valid_epoch_loss:.4f}')","0fef6dd5":"# # Loss visualization\n# plt.figure(figsize=(8, 6))\n# plt.plot(train_loss, color='orange', label='train loss')\n# plt.plot(valid_loss, color='red', label='validataion loss')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n# # plt.savefig(path+'\\\\basemodelloss0.0003206433.png')\n# plt.show()","f434e4a6":"# Path is here\n#\uff08In windows environment\uff09In linux should change the location\nmodel = torch.load(path+'\/\/model.pkl', map_location ='cpu')  # Model path","1fbde9ae":"# Load validation data as test with no label\nval_data_nolabel = ImageDataset(\n    val_df, path=img_path, test = True  # Set True to get image only\n)\n\n# Test validation data loader\nval_nolabel_loader = DataLoader(\n    val_data_nolabel, \n    batch_size=1,  # Tes size to 1 \n    shuffle=False  # No shuffle\n)","853067a2":"#Try to get the best percentile by using the validation data\n#Use a list to validate for several times\nPERCENTILE = [0.3,0.4,0.5]\nsig = torch.nn.Sigmoid()\npreds = []\npreds_list = []\npreds_all = []\n#Get the index in the sigmoid function output which larger than percentile\n#Return the largest one If there is no output larger than percentile\nfor k in PERCENTILE:\n    for i, test_batch in enumerate(val_nolabel_loader):\n        model.eval().to(device)\n        test_batch = test_batch.to(device)\n        batch_preds = model(test_batch).detach().cpu()\n        batch_preds = sig(batch_preds)\n        sig_array = batch_preds.detach().numpy().reshape(20)\n        before_sort = []\n        output = []\n        max_index = 0\n        max_value = -100\n        for index,value in enumerate(sig_array):\n            if value > max_value:\n                max_index = index\n                max_value = value\n            if value>k:\n                before_sort.append((-value,index))\n        after_sort = sorted(before_sort)\n        if after_sort:\n            for x,y in after_sort:\n                output.append(y)\n        else:\n            output.append(max_index)\n        preds.append(output)\n        \n        # Print progress\n        if i % 500 == 499:\n            print('Epoch: %d' %(i + 1))\n    preds_all.append(preds)\n    preds = []\n","173f3218":"# Get true test labels\ntrue_labels = list(val_df[\"Labels\"])\n\ndef convert_list(labels): # input predicted labels\n    one_hot = []\n    for label in labels:\n            temp=[0] * 20\n            for i in label:\n                temp[int(i)]=1\n            one_hot.append(temp)\n    return one_hot\n\ndef convert_string(labels): # input true labels\n    one_hot_string=[]\n    for label in labels:\n        split_label=label.split()\n        temp=[0] * 20\n        for i in split_label:\n            temp[int(i)]=1\n        one_hot_string.append(temp)\n    return one_hot_string\n\nscore = []\nfor i in range(len(preds_all)):\n    preds = preds_all[i]\n    y_true=convert_string(true_labels) #converting into one hot labels for comparison with predicted set\n    y_pred=convert_list(preds) #converting into one hot labels for comparison with true set\n    print(\"Recall Score for validation set is:\",recall_score(y_true=y_true, y_pred=y_pred, average='samples'))\n    print(\"Precision Score for validation set is:\",precision_score(y_true=y_true, y_pred=y_pred, average='samples'))\n    print(\"F1 Score for validation set is:\",f1_score(y_true=y_true, y_pred=y_pred, average='samples'))\n    score.append([PERCENTILE[i],recall_score(y_true=y_true, y_pred=y_pred, average='samples'),precision_score(y_true=y_true, y_pred=y_pred, average='samples'),f1_score(y_true=y_true, y_pred=y_pred, average='samples')])","8bcbaba5":"per = []\nrecall =[]\nprecision = []\nF =[]\nfor row in score:\n    per.append(row[0])\n    recall.append(row[1])\n    precision.append(row[2])\n    F.append(row[3])","ecbd565b":"# Plot\nplt.figure(figsize=(10, 7))\n\nplt.title('PERCENTILE and Scores')\nplt.plot(per, recall, color='green', label='Recall')\nplt.plot(per, precision, color='red', label='Precision')\nplt.plot(per, F,  color='skyblue', label='F1_Score')\n\nplt.legend() \n\nplt.xlabel('PERCENTILE')\nplt.ylabel('Score')\nplt.show()","72ddd5f4":"# test data set\ntest_data = ImageDataset(\n    test_csv, path=img_path, test = True\n)\n\n# test data loader\ntest_loader = DataLoader(\n    test_data, \n    batch_size=1,\n    shuffle=False\n)","6e1d4187":"#In this part use the percentile we choose in the validation\n#The output methon is as same as validation\nPERCENTILE = 0.4\nsig = torch.nn.Sigmoid()\npreds = []\npreds_list = []\n\nfor i, test_batch in enumerate(test_loader):\n    model.eval().to(device)\n    test_batch = test_batch.to(device)\n    batch_preds = model(test_batch).detach().cpu()\n    batch_preds = sig(batch_preds)\n    sig_array = batch_preds.detach().numpy().reshape(20)\n\n    before_sort = []\n    output = []\n    max_index = 0\n    max_value = -100\n\n    for index,value in enumerate(sig_array):        \n        if value > max_value:\n            max_index = index\n            max_value = value\n        if value>PERCENTILE:\n            before_sort.append((-value,index))\n    after_sort = sorted(before_sort)\n    if after_sort:\n        for x,y in after_sort:\n            output.append(y)\n    else:\n        output.append(max_index)\n    preds.append(output)\n        \n    # Print progress\n    if i % 500 == 499:\n        print('Epoch: %d' %(i + 1))","0f7ccdf8":"test_pred_list = []\n\nfor ele in preds:\n    new_str = ''\n    for number in ele:\n        new_str += str(number)\n        new_str += ' '\n    no_space = new_str[:-1]\n    plus_space = \" \"+ no_space\n    test_pred_list+=[plus_space]\ntest_df = pd.DataFrame(test_pred_list)\ntest_df.rename(columns={0:'Labels'}, inplace = True)","9b38b68f":"final = test_csv\ntempdf = test_df['Labels']\nfinal['Labels'] = tempdf.values","46d847b2":"output_path = os.path.abspath(os.path.join(os.getcwd(), \"..\")) + \"\/\/Output\"","548734e4":"final.to_csv(output_path+'\/\/Predicted_labels.txt',index = False)","c551b280":"## Section 4: Train model","1b9ce394":"## Section 2: Compute Normalization values","8fea8679":"##### Be careful to run the next section. It takes much time and will not affect the final output.","520dd5c3":"##### Be careful to run the next section. It takes some time and will not affect the final output.","0ee7ef9d":"##### Be careful to run the next section. It takes some time and will not affect the final output.","7a68e8ba":"## Section 1: Load data","d6bc2c16":"## Section 3: Prepare dataloaders","b8b42807":"## Section 5: Validate the model","9699bed0":"##### Be careful to run the next section. It takes some time and will not affect the final output."}}