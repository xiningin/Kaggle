{"cell_type":{"a3169dcd":"code","6a5c9695":"code","c5c9d237":"code","3701ad41":"code","891ef79f":"code","8efc26dd":"code","c1252f79":"code","e0b250b5":"code","8b67ef8b":"code","e4310c34":"code","af209901":"code","be5ef1e8":"code","8f3ac43b":"code","ca322e05":"code","4cd9c413":"code","88619f56":"code","0ef8f5a1":"code","1ab1dd7a":"code","1ed067bc":"code","1ec07257":"code","ab96385a":"code","4c0833bb":"code","06264403":"code","e21a7e62":"code","eb963931":"code","9c1cb037":"code","d17c3b89":"code","34b91457":"code","f2203459":"code","1e5e8253":"code","0bd43b83":"code","d02527d3":"code","db8ba94e":"code","3cd1568a":"code","745aeb63":"code","e2e4aa9c":"code","2d4e93f4":"code","8cb9b28b":"code","22c54fd4":"code","b1157082":"code","1451d1e3":"code","df915de2":"code","52b94ea1":"code","9b4721ff":"code","48a1ac24":"code","3b910e25":"code","0339db3c":"markdown","b94d4ce9":"markdown","453a6ab6":"markdown","1942a34d":"markdown","21961ea1":"markdown","c42fbd8c":"markdown","4d8922af":"markdown","fdaaeca3":"markdown","c84049c0":"markdown","c51549bc":"markdown","d1741569":"markdown","434eab5a":"markdown","cc957385":"markdown","a7228260":"markdown","d7484fac":"markdown","72f35fbe":"markdown","4932fe3a":"markdown","7930c11e":"markdown","dcd4f9a3":"markdown","a6dea588":"markdown","8359a8cd":"markdown","fd98eb51":"markdown","43bbc057":"markdown","1ac35435":"markdown","193d8aee":"markdown","4d3bf6e1":"markdown","f65e6faf":"markdown","d3b61734":"markdown","49d73520":"markdown","faef818d":"markdown","528f4e05":"markdown","cc8e2dc3":"markdown","ba34df84":"markdown","0739283c":"markdown","03b6e7a9":"markdown","d8f3147c":"markdown","d6bfc57c":"markdown","1611a917":"markdown"},"source":{"a3169dcd":"# Import all the dependencies\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, PowerTransformer\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import VarianceThreshold\n","6a5c9695":"# Load data from csv files\ntrain_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","c5c9d237":"combined_data = pd.concat([train_data, test_data], axis=0)\ncombined_data['Label'] =  combined_data['SalePrice'].isnull()\nmapping = {False:'Tain Data',True:'Test Data' }\ncombined_data['Label'] = combined_data['Label'].map(mapping)","3701ad41":"train_data.head()","891ef79f":"print(train_data.shape)","8efc26dd":"train_data.info()","c1252f79":"s1 = train_data.dtypes\ns1.groupby(s1).count()","e0b250b5":"train_data.isnull().sum().sort_values(ascending = False).head(20)","8b67ef8b":"test_data.head()","e4310c34":"print(test_data.shape)","af209901":"test_data.info()","be5ef1e8":"s2 = test_data.dtypes\ns2.groupby(s2).count()","8f3ac43b":"test_data.isnull().sum().sort_values(ascending = False).head(33)","ca322e05":"s1_train = s1.drop('SalePrice')\ns1_train.compare(s2)","4cd9c413":"test_data['GarageArea'].head(5)","88619f56":"null_train = train_data.isnull().sum()\nnull_test = test_data.isnull().sum()\nnull_train = null_train.drop('SalePrice')\nnull_train.compare(null_test).sort_values(['self','other'], ascending= [False,False])","0ef8f5a1":"con_var = s1[s1.values != 'object'].index\nf, axes = plt.subplots(7,6 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(con_var):\n    sns.histplot(data=combined_data, x = feature, hue=\"Label\",ax=axes[i%7, i\/\/7])","1ab1dd7a":"f, axes = plt.subplots(7,6 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(con_var):\n    sns.scatterplot(data=train_data, x = feature, y= \"SalePrice\",ax=axes[i%7, i\/\/7])","1ed067bc":"cat_var = s1[s1.values == 'object'].index\nf, axes = plt.subplots(7,7 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(cat_var):\n    sns.countplot(data = combined_data, x = feature, hue=\"Label\",ax=axes[i%7, i\/\/7])","1ec07257":"f, axes = plt.subplots(7,7 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(cat_var):\n    sort_list = sorted(train_data.groupby(feature)['SalePrice'].median().items(), key= lambda x:x[1], reverse = True)\n    order_list = [x[0] for x in sort_list ]\n    sns.boxplot(data = train_data, x = feature, y = 'SalePrice', order=order_list, ax=axes[i%7, i\/\/7])\nplt.show()","ab96385a":"con_data = train_data.copy()\nfor col in cat_var:\n    con_data = con_data.drop(col, axis = 1)\n\ntraining_corr = con_data.corr(method='spearman')\nmask = np.zeros_like(training_corr)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,10))\nsns.heatmap(training_corr, mask=mask, cmap=\"YlGnBu\", linewidths=.5)","4c0833bb":"# Top 10 correlated features vs. SalePrice\ncorrelations = con_data.corr(method='spearman')['SalePrice'].sort_values(ascending=False)\ncorrelations_abs = correlations.abs()\nprint(correlations_abs.head(11))","06264403":"def pre_process(train_data, test_data, fillna_dict = {}, drop_list = [], convert_list=[], log_list=[], regroup_dict={}):\n    combined_data = pd.concat([train_data, test_data], ignore_index=True, axis=0)\n    \n    # Step 1: Fill missing values\n    for col, fill_value in fillna_dict.items():\n        combined_data[col] = combined_data[col].fillna(value=fill_value)\n        \n    # Step 2: Drop columns\n    combined_data.drop(columns=drop_list, inplace=True, errors='ignore')\n    \n    # Step 3: Convert \"numerical\" feature to categorical\n    for col in convert_list:\n        combined_data[col] = combined_data[col].astype('str')\n        \n    # Step 4: Apply PowerTransformer to columns\n    for col in log_list:\n        log = PowerTransformer()\n        log.fit(train_data[[col]]) # TODO: fit with combined_data to avoid overfitting with training data?\n        combined_data[col] = log.transform(combined_data[[col]])\n        \n    # Step 5: Regroup features\n    for col, regroup_value in regroup_dict.items():\n        mask = combined_data[col].isin(regroup_value)\n        combined_data[col][mask] = 'Other'\n        \n    # Step 6: Drop categorical features with an absolute dominatining value\n#     for i in combined_data.columns:\n#         if combined_data[i].dtype != 'object':\n#             continue\n#         counts = combined_data[i].value_counts()\n#         zeros = counts.iloc[0]\n#         if zeros \/ len(combined_data) > 0.995:\n#             print(f'Feature {combined_data[i].name} value {counts.index[0]} is dominating with percentage {zeros \/ len(combined_data)}, dropping the feature')\n#             combined_data.drop(columns=combined_data[i].name, inplace=True, errors='ignore')\n    \n    # Step 7: Add features\n#     combined_data['Total_Home_Quality'] = combined_data['OverallQual'] + combined_data['OverallCond']\n\n#     combined_data['Total_Bathrooms'] = (combined_data['FullBath'] + (0.5 * combined_data['HalfBath']) +\n#                                combined_data['BsmtFullBath'] + (0.5 * combined_data['BsmtHalfBath']))\n\n#     combined_data[\"HighQualSF\"] = combined_data[\"1stFlrSF\"] + combined_data[\"2ndFlrSF\"]\n\n#     combined_data['Total_sqr_footage'] = (combined_data['BsmtFinSF1'] + combined_data['BsmtFinSF2'] +\n#                                  combined_data['1stFlrSF'] + combined_data['2ndFlrSF'])\n    \n    # Step -1: Generate one-hot dummy columns\n    combined_data = pd.get_dummies(combined_data).reset_index(drop=True)\n    \n    new_train_data = combined_data.iloc[:len(train_data), :]\n    new_test_data = combined_data.iloc[len(train_data):, :]\n    X_train = new_train_data.drop('SalePrice', axis=1)\n    y_train = np.log1p(new_train_data['SalePrice'].values.ravel())\n    X_test = new_test_data.drop('SalePrice', axis=1)\n    return X_train, y_train, X_test","e21a7e62":"fillna_dict = {\n    'Alley': 'NA',\n    'PoolQC': 'NA',\n    'LotFrontage': train_data['LotFrontage'].mean(),\n    'MasVnrArea': 0.0,\n    'GarageYrBlt': 0.0,\n    'BsmtFinSF1': 0.0,\n    'BsmtFinSF2': 0.0,\n    'BsmtUnfSF': 0.0,\n    'TotalBsmtSF': 0.0,\n    'BsmtFullBath': 0.0,\n    'BsmtHalfBath': 0.0,\n    'GarageCars': 0.0,\n    'GarageArea': 0.0,\n    'MiscFeature': 'NA',\n    'Fence':'NA',\n    'FireplaceQu': 'NA',\n    'GarageFinish': 'NA',\n    'GarageQual': 'NA',\n    'GarageCond': 'NA',\n    'GarageType': 'NA',\n    'BsmtCond': 'NA',\n    'BsmtQual': 'NA',\n    'BsmtExposure':'NA',\n    'BsmtFinType1':'NA',\n    'BsmtFinType2':'NA',\n    'MasVnrType': 'None',\n    'MSZoning': train_data['MSZoning'].mode()[0]\n}\n\ndrop_list = ['Id']\n\nconvert_to_str_list = ['MSSubClass']\n\nlog_list = [\n    'BsmtUnfSF', 'LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea',\n    'TotalBsmtSF', 'GarageArea',\n]\n\nregroup_dict = {\n#     'LotConfig': ['FR2','FR3'],\n#     'LandSlope':['Mod','Sev'],\n#     'BldgType':['2FmCon','Duplex'],\n#     'RoofStyle':['Mansard','Flat','Gambrel'],\n#     'Electrical':['FuseF','FuseP','FuseA','Mix'],\n#     'SaleCondition':['Abnorml','AdjLand','Alloca','Family'],\n#     'BsmtExposure':['Min','Av'],\n#     'Functional':['Min1','Maj1','Min2','Mod','Maj2','Sev'],\n#     'LotShape':['IR2','IR3'],\n    'HeatingQC':['Fa','Po'],\n    'FireplaceQu':['Fa','Po'],\n    'GarageQual':['Fa','Po'],\n    'GarageCond':['Fa','Po'],\n}\n\nX, y, X_test = pre_process(train_data, test_data,\n                           fillna_dict=fillna_dict,\n                           drop_list=drop_list,\n                           convert_list=convert_to_str_list,\n                           log_list=log_list,\n                           regroup_dict=regroup_dict,\n                          )\n\nprint(X.shape)\n\npre_precessing_pipeline = make_pipeline(RobustScaler(), \n                                        # VarianceThreshold(0.001),\n                                       )\n\nX = pre_precessing_pipeline.fit_transform(X)\nX_test = pre_precessing_pipeline.transform(X_test)\n\nprint(X.shape)\nprint(X_test.shape)","eb963931":"RANDOM_SEED = 42\n\n# 10-fold CV\nkfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)","9c1cb037":"def tune(objective):\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    params = study.best_params\n    best_score = study.best_value\n    print(f\"Best score: {best_score} \\nOptimized parameters: {params}\")\n    return params","d17c3b89":"def ridge_objective(trial):\n\n    _alpha = trial.suggest_float(\"alpha\", 0.1, 20)\n\n    ridge = Ridge(alpha=_alpha, random_state=RANDOM_SEED)\n\n    score = cross_val_score(\n        ridge, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.12599963936207256 \n# ridge_params = tune(ridge_objective)\nridge_params = {'alpha': 7.491061624529043}","34b91457":"ridge = Ridge(**ridge_params, random_state=RANDOM_SEED)\nridge.fit(X, y)","f2203459":"def lasso_objective(trial):\n\n    _alpha = trial.suggest_float(\"alpha\", 0.0001, 1)\n\n    lasso = Lasso(alpha=_alpha, random_state=RANDOM_SEED)\n\n    score = cross_val_score(\n        lasso, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score:-0.1237148073006272\n# lasso_params = tune(lasso_objective)\nlasso_params = {'alpha': 0.00041398687418613947}","1e5e8253":"lasso = Lasso(**lasso_params, random_state=RANDOM_SEED)\nlasso.fit(X, y)","0bd43b83":"def elasticnet_objective(trial):\n\n    _alpha = trial.suggest_float(\"alpha\", 0.0001, 1)\n    _l1_ratio = trial.suggest_float(\"l1_ratio\", 0.01, 1)\n\n    elastic = ElasticNet(alpha=_alpha, l1_ratio=_l1_ratio, random_state=RANDOM_SEED)\n\n    score = cross_val_score(\n        elastic, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score:-0.12399332824933833\n# elasticnet_params = tune(elasticnet_objective)\nelasticnet_params = {'alpha': 0.00048679709811971084, 'l1_ratio': 0.5791344072946181}","d02527d3":"elasticnet = ElasticNet(**elasticnet_params, random_state=RANDOM_SEED)\nelasticnet.fit(X, y)","db8ba94e":"def svr_objective(trial):\n\n    _C = trial.suggest_float(\"C\", 0.1, 10)\n    _epsilon = trial.suggest_float(\"epsilon\", 0.01, 10)\n#     _kernel = trial.suggest_categorical(\"kernel\", ['linear', 'poly', 'rbf', 'sigmoid'])\n    _coef0 = trial.suggest_float(\"coef0\", 0.01, 1)\n\n    svr = SVR(C=_C, epsilon=_epsilon, kernel='poly')\n\n    score = cross_val_score(\n        svr, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n\n# Best score: -0.396815691360129 \n# svr_params = tune(svr_objective)\nsvr_params = {'C': 1.5959075672900394, 'epsilon': 1.1567830632624725, 'coef0': 0.4841022473611773}","3cd1568a":"svr = SVR(kernel='poly', **svr_params)\nsvr.fit(X, y)","745aeb63":"def randomforest_objective(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    _max_depth = trial.suggest_int(\"max_depth\", 5, 20)\n    _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n    _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 10)\n    _max_features = trial.suggest_int(\"max_features\", 10, 50)\n\n    rf = RandomForestRegressor(\n        max_depth=_max_depth,\n        min_samples_split=_min_samp_split,\n        min_samples_leaf=_min_samples_leaf,\n        max_features=_max_features,\n        n_estimators=_n_estimators,\n        n_jobs=-1,\n        random_state=RANDOM_SEED,\n    )\n\n    score = cross_val_score(\n        rf, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.13808317983522972\n# randomforest_params = tune(randomforest_objective)\nrandomforest_params = {'n_estimators': 180, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 49}","e2e4aa9c":"rf = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED, **randomforest_params)\nrf.fit(X, y)","2d4e93f4":"def gbr_objective(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 2000)\n    _learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n    _max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n    _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n    _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 20)\n    _max_features = trial.suggest_int(\"max_features\", 10, 50)\n\n    gbr = GradientBoostingRegressor(\n        n_estimators=_n_estimators,\n        learning_rate=_learning_rate,\n        max_depth=_max_depth, \n        max_features=_max_features,\n        min_samples_leaf=_min_samples_leaf,\n        min_samples_split=_min_samp_split,\n        \n        random_state=RANDOM_SEED,\n    )\n\n    score = cross_val_score(\n        gbr, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.11848459248013568\n# gbr_params = tune(gbr_objective)\ngbr_params = {'n_estimators': 1808, 'learning_rate': 0.03603208066350368, 'max_depth': 3, 'min_samples_split': 12, 'min_samples_leaf': 2, 'max_features': 42}","8cb9b28b":"gbr = GradientBoostingRegressor(random_state=RANDOM_SEED, **gbr_params)\ngbr.fit(X, y)","22c54fd4":"def xgb_objective(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 2000)\n    _max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n    _learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n    _gamma = trial.suggest_float(\"gamma\", 0.01, 1)\n    _min_child_weight = trial.suggest_float(\"min_child_weight\", 0.1, 10)\n    _subsample = trial.suggest_float('subsample', 0.01, 1)\n    _reg_alpha = trial.suggest_float('reg_alpha', 0.01, 10)\n    _reg_lambda = trial.suggest_float('reg_lambda', 0.01, 10)\n\n    \n    xgbr = xgb.XGBRegressor(\n        n_estimators=_n_estimators,\n        max_depth=_max_depth, \n        learning_rate=_learning_rate,\n        gamma=_gamma,\n        min_child_weight=_min_child_weight,\n        subsample=_subsample,\n        reg_alpha=_reg_alpha,\n        reg_lambda=_reg_lambda,\n        random_state=RANDOM_SEED,\n    )\n    \n\n    score = cross_val_score(\n        xgbr, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.1225190827846444\n# xgb_params = tune(xgb_objective)\nxgb_params = {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.03314181092616917, 'gamma': 0.03861572735293306, 'min_child_weight': 2.5264657011723335, 'subsample': 0.69824536298609, 'reg_alpha': 0.021753223362733998, 'reg_lambda': 3.216048970671949}","b1157082":"xgbr = xgb.XGBRegressor(random_state=RANDOM_SEED, **xgb_params)\nxgbr.fit(X, y)","1451d1e3":"import lightgbm as lgb\n\ndef lgb_objective(trial):\n    _num_leaves = trial.suggest_int(\"num_leaves\", 50, 100)\n    _max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n    _learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 2000)\n    _min_child_weight = trial.suggest_float(\"min_child_weight\", 0.1, 10)\n    _reg_alpha = trial.suggest_float('reg_alpha', 0.01, 10)\n    _reg_lambda = trial.suggest_float('reg_lambda', 0.01, 10)\n    _subsample = trial.suggest_float('subsample', 0.01, 1)\n\n\n    \n    lgbr = lgb.LGBMRegressor(objective='regression',\n                             num_leaves=_num_leaves,\n                             max_depth=_max_depth,\n                             learning_rate=_learning_rate,\n                             n_estimators=_n_estimators,\n                             min_child_weight=_min_child_weight,\n                             subsample=_subsample,\n                             reg_alpha=_reg_alpha,\n                             reg_lambda=_reg_lambda,\n                             random_state=RANDOM_SEED,\n    )\n    \n\n    score = cross_val_score(\n        lgbr, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.12497294451988177 \n# lgb_params = tune(lgb_objective)\nlgb_params = {'num_leaves': 81, 'max_depth': 2, 'learning_rate': 0.05943111506493225, 'n_estimators': 1668, 'min_child_weight': 4.6721695700874015, 'reg_alpha': 0.33400189583009254, 'reg_lambda': 1.4457484337302167, 'subsample': 0.42380175866399206}","df915de2":"lgbr = lgb.LGBMRegressor(objective='regression', random_state=RANDOM_SEED, **lgb_params)\nlgbr.fit(X, y)","52b94ea1":"# stack models\nstack = StackingRegressor(\n    estimators=[\n        ('ridge', ridge),\n        ('lasso', lasso),\n        ('elasticnet', elasticnet),\n        ('randomforest', rf),\n        ('gradientboostingregressor', gbr),\n        ('xgb', xgbr),\n        ('lgb', lgbr),\n        # ('svr', svr), # Not using this for now as its score is significantly worse than the others\n    ],\n    cv=kfolds)\nstack.fit(X, y)","9b4721ff":"def cv_rmse(model):\n    rmse = -cross_val_score(model, X, y,\n                            scoring=\"neg_root_mean_squared_error\",\n                            cv=kfolds)\n    return (rmse)\n","48a1ac24":"def compare_models():\n    models = {\n        'Ridge': ridge,\n        'Lasso': lasso,\n        'Elastic Net': elasticnet,\n        'Random Forest': rf,\n        'Gradient Boosting': gbr,\n        'XGBoost': xgbr,\n        'LightGBM': lgbr,\n        'Stacking': stack, \n        # 'SVR': svr, # TODO: Investigate why SVR got such a bad result\n    }\n\n    scores = pd.DataFrame(columns=['score', 'model'])\n\n    for name, model in models.items():\n        score = cv_rmse(model)\n        print(\"{:s} score: {:.4f} ({:.4f})\\n\".format(name, score.mean(), score.std()))\n        df = pd.Series(score, name='score').to_frame()\n        df['model'] = name\n        scores = scores.append(df)\n\n    plt.figure(figsize=(20,10))\n    sns.boxplot(data = scores, x = 'model', y = 'score')\n    plt.show()\n    \ncompare_models()","3b910e25":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\n\nsubmission.iloc[:,1] = np.expm1(stack.predict(X_test))\n\nsubmission.to_csv('my_submission.csv', index=False)","0339db3c":"## 4. Modeling and Prediction","b94d4ce9":"## 5. Conclusion\n\n#### Next Step\n- Dive deeper into feature engineering\n- Investigate why SVR got such a bad result. (It is expected that its result is worse than the others, but not to this extent)\n- Try other hyperparameter tuning tools like [Ray](https:\/\/docs.ray.io\/en\/latest\/tune\/)\n- Try NN (maybe training data is too few for NN?)\n","453a6ab6":"#### 2.4.3 Distribution Comparison - Continous Variables","1942a34d":"#### 2.3.4 Data Type - Test Data","21961ea1":"#### 2.2.5 Null Value - Train Data","c42fbd8c":"The inconsistent data types are int64&float64 and not considered a problem.","4d8922af":"### 2.4 Train Data & Test Data Comnparison","fdaaeca3":"The comparison of the categorical variables showed that:\n- Train data and test data distributions are similar for most features\n- Some features have dominant items, we can consider to combine some minor items into a group, such as\n    - 'Fa' & 'Po' in 'HeatingQC', 'FireplaceQu', 'GarageQual' and 'GarageCond'","c84049c0":"## 1. Introduction\n\nHouse Pricing Prediction has been a classic challenge, as the price of houses are usually dependent on many features. The goal of our study is to set up prediction models for Boston house prices, based on 80 given features.\n\nTwo data sets are given: the train data set consist of 1460 rows each representing a distinct house transaction, and 81 columns (1 SalePrice & 80 features),\nand the test data set consist of 1459 rows, and 80 columns (features).\n\nOur study follows the process: exploratory data analysis - feature engineering - prediction model setting. Finally 7 models are used and ensembles. The result turns out that:","c51549bc":"Here, we could see that sale prices for 'Fa' & 'Po' in 'HeatingQC', 'FireplaceQu', 'GarageQual' and 'GarageCond' are similar, so we may consider go ahead and combine the items.","d1741569":"## 3. Feature engineeing\n","434eab5a":"### 2.2 Train Data Exploration","cc957385":"#### 2.4.1 Data Type Comparison","a7228260":"### Model Comparison","d7484fac":"#### 2.2.3 Data Information - Trian Data","72f35fbe":"Here, we can see that some relations seem positive but not quite linear:\n- 'SalePrice' VS.'BsmtUnfSF', \n- 'SalePrice' VS.'LotFrontage', \n- 'SalePrice' VS.'LotArea', \n- 'SalePrice' VS.'1stFlrSF', \n- 'SalePrice' VS.'GrLivArea',\n- 'SalePrice' VS.'TotalBsmtSF', \n- 'SalePrice' VS.'GarageArea',\nSo we will consider transform such features into log forms.","4932fe3a":"We noticed PoolQC, MiscFeature, Alley, Fence are the features with most missing values, we will consider to drop a few such features later.","7930c11e":"### 2.5 Data Correlation","dcd4f9a3":"#### 2.2.2 Data Shape - Train Data","a6dea588":"For both train and test data, we explored the items as below:\n- first 5 rows\n- data shape\n- general data information\n- data type\n- null value","8359a8cd":"#### 2.2.1 First Five Records - Train Data","fd98eb51":"We also want to confirm that the items we want to combine has similar prices.","43bbc057":"Then we also want to see the linearity between all features and the response variable (saleprice)","1ac35435":"#### 2.4.4 Distribution Comparison - Catagorical Variables","193d8aee":"#### 2.3.3 Data Information - Test Data","4d3bf6e1":"## Table of Contents\n\n1. Introduction\n1. Exploratory data analysis\n1. Feature engineeing\n1. Modeling and Prediction\n1. Conclusion\n","f65e6faf":"#### 2.2.4 Data Type - Train Data","d3b61734":"After exploration, we noticed that data types seem not match between train and test data, so we want to compare the differences.\nIn this part, we mainly compared features below between train and test data:\n- Data Types\n- Null Values\n- Data Distribution","49d73520":"#### 2.3.1 First Five Records - Test Data","faef818d":"#### 2.3.2 Data Type - Test data","528f4e05":"The distribution above shows that:\n- The distribution of train data and test data are similar for most continous features;\n- Some features can be reclassified as 'Categorical', such as 'MSSubClass';\n- Some features are dominated by 0\/null (eg:PoolArea), thus we can consider to drop.","cc8e2dc3":"### Submission","ba34df84":"#### 2.4.2 Null Value Comparison","0739283c":"#### 2.3.5 Null Data - Test Data","03b6e7a9":"### 2.3 Test Data Exploration","d8f3147c":"Here we see that 'OverallQual', 'GrlivArea', 'GarageCars' are highly correlated to SalePrice.","d6bfc57c":"In this section, we will fit the model with the data, tune the hyperparameters of them, and make prediction for the testing data.\n\nWe selected the following regressor models:\n- [Ridge](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html)\n- [Lasso](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html)\n- [Elastic Net](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.ElasticNet.html)\n- [SVR](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVR.html)\n- [Random Forest Regressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html)\n- [Gradient Boosting Regressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html)\n- [Stacking Regressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingRegressor.html) with the other models\n\nRegarding hyperparameter tuning, we preferred the searching algorithm of [Optuna](https:\/\/github.com\/optuna\/optuna).","1611a917":"## 2. Exploratory data analysis\n\n### 2.1 Load the data"}}