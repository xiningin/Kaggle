{"cell_type":{"2c893417":"code","23905f9f":"code","7f450a39":"code","b10e8de3":"code","cf012e74":"code","093da10e":"code","56c0a9dd":"code","4a33c7cf":"code","c6bf423b":"code","33043b6d":"code","2ef44bdc":"code","4b5715ad":"code","2f94749e":"code","0f9589bc":"markdown","d8cd3bf2":"markdown","0e17e288":"markdown","5d7c6765":"markdown","fb7e9ed6":"markdown","a1f7e531":"markdown","53535b71":"markdown","9bccfedf":"markdown"},"source":{"2c893417":"# Install Dependencies","23905f9f":"!pip install git+https:\/\/github.com\/glmcdona\/LuxPythonEnvGym.git","7f450a39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b10e8de3":"import sys\nimport time\nfrom functools import partial  # pip install functools\nimport copy\nimport random\nfrom typing import Union, Any\n\nimport numpy as np\nfrom gym import spaces\n\nfrom luxai2021.env.agent import Agent, AgentWithModel\nfrom luxai2021.game.actions import *\nfrom luxai2021.game.game_constants import GAME_CONSTANTS\nfrom luxai2021.game.position import Position","cf012e74":"# https:\/\/codereview.stackexchange.com\/questions\/28207\/finding-the-closest-point-to-a-list-of-points\ndef closest_node(node, nodes):\n    dist_2 = np.sum((nodes - node) ** 2, axis=1)\n    return np.argmin(dist_2)\ndef furthest_node(node, nodes):\n    dist_2 = np.sum((nodes - node) ** 2, axis=1)\n    return np.argmax(dist_2)\n\ndef smart_transfer_to_nearby(game, team, unit_id, unit, target_type_restriction=None, **kwarg):\n    \"\"\"\n    Smart-transfers from the specified unit to a nearby neighbor. Prioritizes any\n    nearby carts first, then any worker. Transfers the resource type which the unit\n    has most of. Picks which cart\/worker based on choosing a target that is most-full\n    but able to take the most amount of resources.\n\n    Args:\n        team ([type]): [description]\n        unit_id ([type]): [description]\n\n    Returns:\n        Action: Returns a TransferAction object, even if the request is an invalid\n                transfer. Use TransferAction.is_valid() to check validity.\n    \"\"\"\n\n    # Calculate how much resources could at-most be transferred\n    resource_type = None\n    resource_amount = 0\n    target_unit = None\n\n    if unit != None:\n        for type, amount in unit.cargo.items():\n            if amount > resource_amount:\n                resource_type = type\n                resource_amount = amount\n\n        # Find the best nearby unit to transfer to\n        unit_cell = game.map.get_cell_by_pos(unit.pos)\n        adjacent_cells = game.map.get_adjacent_cells(unit_cell)\n\n        \n        for c in adjacent_cells:\n            for id, u in c.units.items():\n                # Apply the unit type target restriction\n                if target_type_restriction == None or u.type == target_type_restriction:\n                    if u.team == team:\n                        # This unit belongs to our team, set it as the winning transfer target\n                        # if it's the best match.\n                        if target_unit is None:\n                            target_unit = u\n                        else:\n                            # Compare this unit to the existing target\n                            if target_unit.type == u.type:\n                                # Transfer to the target with the least capacity, but can accept\n                                # all of our resources\n                                if( u.get_cargo_space_left() >= resource_amount and \n                                    target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Both units can accept all our resources. Prioritize one that is most-full.\n                                    if u.get_cargo_space_left() < target_unit.get_cargo_space_left():\n                                        # This new target it better, it has less space left and can take all our\n                                        # resources\n                                        target_unit = u\n                                    \n                                elif( target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Don't change targets. Current one is best since it can take all\n                                    # the resources, but new target can't.\n                                    pass\n                                    \n                                elif( u.get_cargo_space_left() > target_unit.get_cargo_space_left() ):\n                                    # Change targets, because neither target can accept all our resources and \n                                    # this target can take more resources.\n                                    target_unit = u\n                            elif u.type == Constants.UNIT_TYPES.CART:\n                                # Transfer to this cart instead of the current worker target\n                                target_unit = u\n    \n    # Build the transfer action request\n    target_unit_id = None\n    if target_unit is not None:\n        target_unit_id = target_unit.id\n\n        # Update the transfer amount based on the room of the target\n        if target_unit.get_cargo_space_left() < resource_amount:\n            resource_amount = target_unit.get_cargo_space_left()\n    \n    return TransferAction(team, unit_id, target_unit_id, resource_type, resource_amount)\n","093da10e":"########################################################################################################################\n# This is the Agent that you need to design for the competition\n########################################################################################################################\nclass AgentPolicy(AgentWithModel):\n    def __init__(self, mode=\"train\", model=None) -> None:\n        \"\"\"\n        Arguments:\n            mode: \"train\" or \"inference\", which controls if this agent is for training or not.\n            model: The pretrained model, or if None it will operate in training mode.\n        \"\"\"\n        super().__init__(mode, model)\n\n        # Define action and observation space\n        # They must be gym.spaces objects\n        # Example when using discrete actions:\n        self.actions_units = [\n            partial(MoveAction, direction=Constants.DIRECTIONS.CENTER),  # This is the do-nothing action\n            partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n            partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n            partial(smart_transfer_to_nearby, target_type_restriction=Constants.UNIT_TYPES.CART), # Transfer to nearby cart\n            partial(smart_transfer_to_nearby, target_type_restriction=Constants.UNIT_TYPES.WORKER), # Transfer to nearby worker\n            SpawnCityAction,\n            PillageAction,\n        ]\n        self.actions_cities = [\n            SpawnWorkerAction,\n            SpawnCartAction,\n            ResearchAction,\n        ]\n        self.action_space = spaces.Discrete(max(len(self.actions_units), len(self.actions_cities)))\n\n        # Observation space: (Basic minimum for a miner agent)\n        # Object:\n        #   1x is worker\n        #   1x is cart\n        #   1x is citytile\n        #\n        #   5x direction_nearest_wood\n        #   1x distance_nearest_wood\n        #   1x amount\n        #\n        #   5x direction_nearest_coal\n        #   1x distance_nearest_coal\n        #   1x amount\n        #\n        #   5x direction_nearest_uranium\n        #   1x distance_nearest_uranium\n        #   1x amount\n        #\n        #   5x direction_nearest_city\n        #   1x distance_nearest_city\n        #   1x amount of fuel\n        #\n        #   28x (the same as above, but direction, distance, and amount to the furthest of each)\n        #\n        #   5x direction_nearest_worker\n        #   1x distance_nearest_worker\n        #   1x amount of cargo\n        # Unit:\n        #   1x cargo size\n        # State:\n        #   1x is night\n        #   1x percent of game done\n        #   2x citytile counts [cur player, opponent]\n        #   2x worker counts [cur player, opponent]\n        #   2x cart counts [cur player, opponent]\n        #   1x research points [cur player]\n        #   1x researched coal [cur player]\n        #   1x researched uranium [cur player]\n\n        # self.observation_shape = (3 + 7 * 5 * 2 + 1 + 1 + 1 + 2 + 2 + 2 + 3,)\n\n        self.observation_shape = (22,32,32,)\n        self.observation_space = spaces.Box(low=0, high=1, shape=\n        self.observation_shape, dtype=np.float16)\n\n        self.object_nodes = {}\n        self.obs_map = None\n\n    def get_agent_type(self):\n        \"\"\"\n        Returns the type of agent. Use AGENT for inference, and LEARNING for training a model.\n        \"\"\"\n        if self.mode == \"train\":\n            return Constants.AGENT_TYPE.LEARNING\n        else:\n            return Constants.AGENT_TYPE.AGENT\n\n    def get_observation(self, game, unit, city_tile, team, is_new_turn):\n        \"\"\"\n        Implements getting a observation from the current game for this unit or city\n        \"\"\"\n        # Observation space: (Basic minimum for a miner agent)\n        #   new 22 features\n        #   0 unit self\n        #   1 friend unit\n        #   2 opponent unit\n\n        #   3 citytile self\n        #   4 friend citytile\n        #   5 opponent citytile\n\n        #   6 unit cargo amount\n        #   7 unit cooldown\n        #   8 opponent unit cargo amount\n        #   9 opponent unit unit cooldown\n\n        #   10 city light up fuel\n        #   11 city cooldown\n        #   12 opponent city light up fuel\n        #   13 opponent city cooldown\n\n        #   14 resource wood amount\n        #   15 resource coal amount\n        #   16 resource uranium amount\n\n        #   17 self research point\n        #   18 opponent research point (team + 1) % 2\n        #   19 day night\n        #   20 turn\n        #   21 map size\n\n        width, height = game.map.width, game.map.height\n        x_shift = (32 - width) \/\/ 2\n        y_shift = (32 - height) \/\/ 2\n\n        if is_new_turn:\n            # It's a new turn this event. This flag is set True for only the first observation from each turn.\n            # Update any per-turn fixed observation space that doesn't change per unit\/city controlled.\n\n            # Build a list of object nodes by type for quick distance-searches\n            self.obs_map = np.zeros(self.observation_shape, dtype=np.float32)\n            \n            # Add resources\n            for cell in game.map.resources:\n                if cell.has_resource():\n                    r_amt = cell.resource.amount\n                    r_type = cell.resource.type\n                    x = cell.pos.x + x_shift\n                    y = cell.pos.y + y_shift\n                    self.obs_map[{'wood': 14, 'coal': 15, 'uranium': 16}[r_type], x, y] = r_amt \/ 800\n\n            # Add your own and opponent units\n            for t in [team, (team + 1) % 2]:\n                for u in game.state[\"teamStates\"][team][\"units\"].values():\n                    x = u.pos.x + x_shift\n                    y = u.pos.y + y_shift\n                    cd = u.cooldown \/ 6\n                    wood = u.cargo['wood']\n                    coal = u.cargo['coal']\n                    uranium = u.cargo['uranium']\n                    cargo_amt = (wood + coal + uranium) \/ 100.0\n                    if t != team:   # opponent unit\n                        self.obs_map[8, x, y] = cargo_amt\n                        self.obs_map[9, x, y] = cd\n                    else:\n                        self.obs_map[6, x, y] = cargo_amt\n                        self.obs_map[7, x, y] = cd\n\n            # Add your own and opponent cities\n            for city in game.cities.values():\n                light_upkeep = city.get_light_upkeep()\n                fuel = city.fuel\n                city_life = min(fuel \/ light_upkeep, 10) \/ 10.0\n                for city_cell in city.city_cells:\n                    x = city_cell.pos.x + x_shift\n                    y = city_cell.pos.y + y_shift\n                    cd = city_cell.city_tile.cooldown \/ 6\n                    if city.team != team:   # opponent city tile\n                        self.obs_map[5, x, y] = 1.0\n                        self.obs_map[12, x, y] = city_life\n                        self.obs_map[13, x, y] = cd\n                    else:\n                        self.obs_map[4, x, y] = 1.0\n                        self.obs_map[10, x, y] = city_life\n                        self.obs_map[11, x, y] = cd\n        \n        pos = Position(0,0)\n        if unit is not None:\n            pos = copy.copy(unit.pos)\n        elif city_tile is not None:\n            pos = copy.copy(city_tile.pos)\n        pos.x += x_shift\n        pos.y += y_shift\n\n        obs = self.obs_map.copy()\n        \n        if unit is not None:\n            obs[0, pos.x, pos.y] = 1.0  # Worker\n            wood = unit.cargo['wood']\n            coal = unit.cargo['coal']\n            uranium = unit.cargo['uranium']\n            cargo_amt = (wood + coal + uranium) \/ 100.0\n            obs[6, pos.x, pos.y] = cargo_amt    # unit cargo amount\n\n        if city_tile is not None:\n            obs[3, pos.x, pos.y] = 1.0  # CityTile\n\n        # self research points\n        obs[17, pos.x, pos.y] = min( game.state[\"teamStates\"][team][\"researchPoints\"], 200 ) \/ 200.0\n        # opponent research points\n        obs[18, pos.x, pos.y] = min( game.state[\"teamStates\"][ (team + 1) % 2 ][\"researchPoints\"], 200 ) \/ 200.0\n\n        # day night\n        obs[19, pos.x, pos.y] = game.state[\"turn\"] % 40 \/ 40.0\n        # turn\n        obs[20, pos.x, pos.y] = game.state[\"turn\"] \/ GAME_CONSTANTS[\"PARAMETERS\"][\"MAX_DAYS\"]\n        # map size\n        obs[21, x_shift:32 - x_shift, y_shift:32 - y_shift] = 1.0\n        \n        return obs\n\n    def action_code_to_action(self, action_code, game, unit=None, city_tile=None, team=None):\n        \"\"\"\n        Takes an action in the environment according to actionCode:\n            action_code: Index of action to take into the action array.\n        Returns: An action.\n        \"\"\"\n        # Map action_code index into to a constructed Action object\n        try:\n            x = None\n            y = None\n            if city_tile is not None:\n                x = city_tile.pos.x\n                y = city_tile.pos.y\n            elif unit is not None:\n                x = unit.pos.x\n                y = unit.pos.y\n            \n            if city_tile != None:\n                action =  self.actions_cities[action_code%len(self.actions_cities)](\n                    game=game,\n                    unit_id=unit.id if unit else None,\n                    unit=unit,\n                    city_id=city_tile.city_id if city_tile else None,\n                    citytile=city_tile,\n                    team=team,\n                    x=x,\n                    y=y\n                )\n            else:\n                action =  self.actions_units[action_code%len(self.actions_units)](\n                    game=game,\n                    unit_id=unit.id if unit else None,\n                    unit=unit,\n                    city_id=city_tile.city_id if city_tile else None,\n                    citytile=city_tile,\n                    team=team,\n                    x=x,\n                    y=y\n                )\n            \n            return action\n        except Exception as e:\n            # Not a valid action\n            print(e)\n            return None\n\n    def take_action(self, action_code, game, unit=None, city_tile=None, team=None):\n        \"\"\"\n        Takes an action in the environment according to actionCode:\n            actionCode: Index of action to take into the action array.\n        \"\"\"\n        if unit == None and city_tile == None:\n            return\n        action = self.action_code_to_action(action_code, game, unit, city_tile, team)\n        self.match_controller.take_action(action)\n\n    def game_start(self, game):\n        \"\"\"\n        This function is called at the start of each game. Use this to\n        reset and initialize per game. Note that self.team may have\n        been changed since last game. The game map has been created\n        and starting units placed.\n\n        Args:\n            game ([type]): Game.\n        \"\"\"\n        self.units_last = 0\n        self.city_tiles_last = 0\n        self.fuel_collected_last = 0\n\n    def get_reward(self, game, is_game_finished, is_new_turn, is_game_error):\n        \"\"\"\n        Returns the reward function for this step of the game. Reward should be a\n        delta increment to the reward, not the total current reward.\n        \"\"\"\n        if is_game_error:\n            # Game environment step failed, assign a game lost reward to not incentivise this\n            print(\"Game failed due to error\")\n            return -1.0\n\n        if not is_new_turn and not is_game_finished:\n            # Only apply rewards at the start of each turn or at game end\n            return 0\n\n        # Get some basic stats\n        unit_count = len(game.state[\"teamStates\"][self.team][\"units\"])\n\n        city_count = 0\n        city_count_opponent = 0\n        city_tile_count = 0\n        city_tile_count_opponent = 0\n        for city in game.cities.values():\n            if city.team == self.team:\n                city_count += 1\n            else:\n                city_count_opponent += 1\n\n            for cell in city.city_cells:\n                if city.team == self.team:\n                    city_tile_count += 1\n                else:\n                    city_tile_count_opponent += 1\n        \n        rewards = {}\n        \n        # Give a reward for unit creation\/death. 0.05 reward per unit.\n        rewards[\"rew\/r_units\"] = (unit_count - self.units_last) * 0.5\n        self.units_last = unit_count\n\n        # Give a reward for city creation\/death. 0.1 reward per city.\n        rewards[\"rew\/r_city_tiles\"] = (city_tile_count - self.city_tiles_last) * 1\n        self.city_tiles_last = city_tile_count\n\n        # Give a reward for research points\n        if game.state[\"teamStates\"][self.team][\"researchPoints\"] <=200:\n            rewards[\"rew\/r_research_points\"] = (game.state[\"teamStates\"][self.team][\"researchPoints\"]- self.research_points_last) * 0.5\n            self.research_points_last = game.state[\"teamStates\"][self.team][\"researchPoints\"]\n            \n        # Reward collecting fuel\n        fuel_collected = game.stats[\"teamStats\"][self.team][\"fuelGenerated\"]\n        rewards[\"rew\/r_fuel_collected\"] = ( (fuel_collected - self.fuel_collected_last) \/ 20000 )\n        self.fuel_collected_last = fuel_collected\n        \n        # Give a reward of 1.0 per city tile alive at the end of the game\n        rewards[\"rew\/r_city_tiles_end\"] = 0\n        if is_game_finished:\n            self.is_last_turn = True\n            rewards[\"rew\/r_city_tiles_end\"] = city_tile_count\n\n            '''\n            # Example of a game win\/loss reward instead\n            if game.get_winning_team() == self.team:\n                rewards[\"rew\/r_game_win\"] = 100.0 # Win\n            else:\n                rewards[\"rew\/r_game_win\"] = -100.0 # Loss\n            '''\n        \n        reward = 0\n        for name, value in rewards.items():\n            reward += value\n\n        return reward\n\n    def turn_heurstics(self, game, is_first_turn):\n        \"\"\"\n        This is called pre-observation actions to allow for hardcoded heuristics\n        to control a subset of units. Any unit or city that gets an action from this\n        callback, will not create an observation+action.\n\n        Args:\n            game ([type]): Game in progress\n            is_first_turn (bool): True if it's the first turn of a game.\n        \"\"\"\n        return\n\n","56c0a9dd":"import argparse\nimport glob\nimport os\nimport sys\nimport random\nimport gym\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom stable_baselines3 import PPO  # pip install stable-baselines3\nfrom stable_baselines3.common.callbacks import EvalCallback\nfrom stable_baselines3.common.utils import set_random_seed, get_schedule_fn\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\nfrom stable_baselines3.common.torch_layers import NatureCNN\n\nfrom luxai2021.env.agent import Agent\nfrom luxai2021.env.lux_env import LuxEnvironment, SaveReplayAndModelCallback\nfrom luxai2021.game.constants import LuxMatchConfigs_Default\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n","4a33c7cf":"# Neural Network for Lux AI\nclass BasicConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            input_dim, output_dim, \n            kernel_size=kernel_size, \n            padding=(kernel_size[0] \/\/ 2, kernel_size[1] \/\/ 2)\n        )\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = self.conv(x)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\nclass LuxCNN(BaseFeaturesExtractor):\n    \"\"\"\n    :param observation_space: (gym.Space)\n    :param features_dim: (int) Number of features extracted.\n        This corresponds to the number of unit for the last layer.\n    \"\"\"\n\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n        super(LuxCNN, self).__init__(observation_space, features_dim)\n        # We assume CxHxW images (channels first)\n        # Re-ordering will be done by pre-preprocessing or wrapper\n        layers, filters = 6, 32\n        n_input_channels = observation_space.shape[0]\n\n        self.conv0 = BasicConv2d(n_input_channels, filters, (3, 3), True)\n        self.blocks = nn.ModuleList([BasicConv2d(filters, filters, (3, 3), True) for _ in range(layers)])\n        self.head_p = nn.Linear(filters, features_dim, bias=False)\n\n    def forward(self, x: th.Tensor) -> th.Tensor:\n        h = F.relu_(self.conv0(x))\n        for block in self.blocks:\n            h = F.relu_(h + block(h))\n        h_head = (h * x[:,:1]).view(h.size(0), h.size(1), -1).sum(-1)\n        p = self.head_p(h_head)\n        return p","c6bf423b":"def get_command_line_arguments():\n    \"\"\"\n    Gbatch_sizeet the command line arguments\n    :return:(ArgumentParser) The command line arguments as an ArgumentParser\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Training script for Lux RL agent.')\n    parser.add_argument('--id', help='Identifier of this run', type=str, default=str(random.randint(0, 10000)))\n    parser.add_argument('--learning_rate', help='Learning rate', type=float, default=0.0001)\n    parser.add_argument('--gamma', help='Gamma', type=float, default=0.999)\n    parser.add_argument('--gae_lambda', help='GAE Lambda', type=float, default=0.95)\n    parser.add_argument('--batch_size', help='batch_size', type=int, default=2048)  # 64\n    parser.add_argument('--step_count', help='Total number of steps to train', type=int, default=5000000)\n    parser.add_argument('--n_steps', help='Number of experiences to gather before each learning period', type=int, default=4096)\n    parser.add_argument('--path', help='Path to a checkpoint to load to resume training', type=str, default=None)\n    parser.add_argument('--n_envs', help='Number of parallel environments to use in training', type=int, default=1)\n    #args = parser.parse_args()\n    args = parser.parse_known_args()[0]\n\n    return args","33043b6d":"# https:\/\/stable-baselines3.readthedocs.io\/en\/master\/guide\/examples.html?highlight=SubprocVecEnv#multiprocessing-unleashing-the-power-of-vectorized-environments\ndef make_env(local_env, rank, seed=0):\n    \"\"\"\n    Utility function for multi-processed env.\n\n    :param local_env: (LuxEnvironment) the environment \n    :param seed: (int) the initial seed for RNG\n    :param rank: (int) index of the subprocess\n    \"\"\"\n\n    def _init():\n        local_env.seed(seed + rank)\n        return local_env\n\n    set_random_seed(seed)\n    return _init","2ef44bdc":"def train(args):    \n    \"\"\"\n    The main training loop\n    :param args: (ArgumentParser) The command line arguments\n    \"\"\"\n    print(args)\n\n    # Run a training job\n    configs = LuxMatchConfigs_Default\n\n    # Create a default opponent agent\n    opponent = Agent()\n\n    # Create a RL agent in training mode\n    player = AgentPolicy(mode=\"train\")\n\n    # Train the model\n    env_eval = None\n    if args.n_envs == 1:\n        env = LuxEnvironment(configs=configs,\n                             learning_agent=player,\n                             opponent_agent=opponent)\n    else:\n        env = SubprocVecEnv([make_env(LuxEnvironment(configs=configs,\n                                                     learning_agent=AgentPolicy(mode=\"train\"),\n                                                     opponent_agent=opponent), i) for i in range(args.n_envs)])\n    \n    run_id = args.id\n    print(\"Run id %s\" % run_id)\n\n    policy_kwargs = dict(\n        features_extractor_class=LuxCNN,\n        features_extractor_kwargs=dict(features_dim=256),\n    )\n\n    if args.path:\n        # by default previous model params are used (lr, batch size, gamma...)\n        model = PPO.load(args.path)\n        model.set_env(env=env)\n\n        # Update the learning rate\n        model.lr_schedule = get_schedule_fn(args.learning_rate)\n\n        # TODO: Update other training parameters\n    else:\n        model = PPO(\"CnnPolicy\",\n                    env,\n                    verbose=1,\n                    tensorboard_log=\".\/lux_tensorboard\/\",\n                    learning_rate=args.learning_rate,\n                    gamma=args.gamma,\n                    gae_lambda=args.gae_lambda,\n                    batch_size=args.batch_size,\n                    n_steps=args.n_steps,\n                    policy_kwargs=policy_kwargs\n                    )\n    print(\"----model----\\n\",model.policy)\n    \n    \n    callbacks = []\n\n    # Save a checkpoint and 5 match replay files every 100K steps\n    player_replay = AgentPolicy(mode=\"inference\", model=model)\n    callbacks.append(\n        SaveReplayAndModelCallback(\n                                save_freq=100000,\n                                save_path='.\/models\/',\n                                name_prefix=f'model{run_id}',\n                                replay_env=LuxEnvironment(\n                                                configs=configs,\n                                                learning_agent=player_replay,\n                                                opponent_agent=Agent()\n                                ),\n                                replay_num_episodes=5\n                            )\n    )\n    \n    # Since reward metrics don't work for multi-environment setups, we add an evaluation logger\n    # for metrics.\n    if args.n_envs > 1:\n        # An evaluation environment is needed to measure multi-env setups. Use a fixed 4 envs.\n        env_eval = SubprocVecEnv([make_env(LuxEnvironment(configs=configs,\n                                                     learning_agent=AgentPolicy(mode=\"train\"),\n                                                     opponent_agent=opponent), i) for i in range(4)])\n\n        callbacks.append(\n            EvalCallback(env_eval, best_model_save_path=f'.\/logs_{run_id}\/',\n                             log_path=f'.\/logs_{run_id}\/',\n                             eval_freq=args.n_steps*2, # Run it every 2 training iterations\n                             n_eval_episodes=30, # Run 30 games\n                             deterministic=False, render=False)\n        )\n\n    print(\"Training model...\")\n    model.learn(total_timesteps=args.step_count,\n                callback=callbacks)\n    if not os.path.exists(f'models\/rl_model_{run_id}_{args.step_count}_steps.zip'):\n        model.save(path=f'models\/rl_model_{run_id}_{args.step_count}_steps.zip')\n    print(\"Done training model.\")\n\n    # Inference the model\n    print(\"Inference model policy with rendering...\")\n    saves = glob.glob(f'models\/rl_model_{run_id}_*_steps.zip')\n    latest_save = sorted(saves, key=lambda x: int(x.split('_')[-2]), reverse=True)[0]\n    model.load(path=latest_save)\n    obs = env.reset()\n    total_reward = 0\n    for i in range(600):\n        action_code, _states = model.predict(obs, deterministic=True)\n        obs, rewards, done, info = env.step(action_code)\n        total_reward += rewards\n        if i % 5 == 0:\n            print(\"Turn %i\" % i)\n            env.render()\n\n        if done:\n            print(\"Episode done, resetting.\")\n            obs = env.reset()\n    print(\"Done, total_reward:\",total_reward)\n\n    #mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n    #print(f'Mean reward: {mean_reward:,.0f} +\/- {std_reward:,.0f}')","4b5715ad":"# Get the command line arguments\nlocal_args = get_command_line_arguments()\n\n# Train the model\ntrain(local_args)","2f94749e":"#eval_swde = SimpleWorkerDictEnv(LuxMatchConfigs_Default)\n#eval_swfe = FlattenObservation(eval_swde)\n#eval_menv = Monitor(eval_swfe, '.\/logdir')\n\n#model2 = PPO.load(\"best_ppo_worker_py37_20211103_001\")\n#mean_reward, std_reward = evaluate_policy(model2, eval_menv, n_eval_episodes=20)\n#print(f'Mean reward: {mean_reward:,.0f} +\/- {std_reward:,.0f}')","0f9589bc":"## args","d8cd3bf2":"## main training","0e17e288":"## train code","5d7c6765":"# Utility Functions","fb7e9ed6":"# Evaluate the model","a1f7e531":"# train","53535b71":"# Agent policy","9bccfedf":"# Features Extractor Model"}}