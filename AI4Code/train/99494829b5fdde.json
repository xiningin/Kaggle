{"cell_type":{"7515cd9e":"code","142f758c":"code","1100afd8":"code","7e34430e":"code","fbfde6b6":"code","ed06fcdc":"code","bd8292f7":"code","ce269210":"code","ddfc3b40":"code","954ca293":"code","af2fff68":"code","0b50c35d":"code","799710d9":"code","5c32a42a":"code","94d265c8":"code","0b0fb5fb":"code","3229a9ba":"code","3b8876b8":"code","a7aa04df":"code","6a9a9edc":"code","214cb463":"code","cef8239f":"code","2bdc271c":"code","c9fc976b":"code","6efaf740":"code","bbf8f9d4":"code","e710c5a7":"code","65b04e1d":"code","d5b163b5":"code","5b5d16d2":"code","d1a6788d":"code","f097ddff":"code","4984eab0":"code","85ca7aae":"markdown","ceb570aa":"markdown","d7fa9258":"markdown","47fa1cb1":"markdown","e537bcd7":"markdown","d509f9b2":"markdown","d0a0e15c":"markdown","ecbb17ae":"markdown","3a845260":"markdown","ef7e530d":"markdown","b0f57d06":"markdown","f69409b6":"markdown","f3fa57d5":"markdown","015c6bcc":"markdown"},"source":{"7515cd9e":"%matplotlib inline\n\n# importando bibliotecas necess\u00e1rias\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nsns.set_theme()\nplt.style.use('seaborn')\nimport warnings\nfrom scipy.stats import uniform as sp_randFloat\nfrom scipy.stats import randint as sp_randInt\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n\n\n\n\n\n\n\n","142f758c":"#Importing data and substituting non number characters \ndata = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\",sep=r'\\s*,\\s*',engine='python',na_values=\"?\")\ndata_test = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\",sep=r'\\s*,\\s*',engine='python',na_values=\"?\")\ndata \n \n","1100afd8":"data.head(5)","7e34430e":"#Here the code gets how many values are null on the collumns \nnulo = data.isna().sum().sum()\nprint(\" Null characters equals to: \",nulo)\n","fbfde6b6":"#Code calculates the percentage of null values and check on their columns  \nx = (data.isna().sum().sum())*100 \/ (data.shape[0]*data.shape[1])\n\nprint(\"Null data equals to {0:.3}%\".format(x))\ndata.isna().sum()","ed06fcdc":"Imp = SimpleImputer(strategy='most_frequent')\n\ndata[['workclass', 'occupation']]= pd.DataFrame(Imp.fit_transform(data[['workclass', 'occupation']]))","bd8292f7":"#Lines below verifies if there are either some null values or any special characters \n\nprint(\"There are {} null values on the data\".format((data.isna().sum().sum())))\nprint(\"There are {} special characters\".format(data.isin(['?']).sum().sum()))\n ","ce269210":"data.describe()","ddfc3b40":"data.head()","954ca293":"data2 = data.copy(deep=True)\n\ndata2=data.drop(columns=[\"income\"])\n\ndata2[\"Target\"] = [0 if s ==\"<=50K\" else 1 for s in data.income ]\n\ndata2.head()","af2fff68":"#Showing income for groups in wich 50k is the limit for categoricals variables\n\nls = ['workclass', 'education','marital.status', 'occupation', \n         'relationship', 'race', 'sex','native.country'\n        ]\nplt.figure(figsize = (40,75))\ni = 0\nfor v in ls:\n    i+= 1\n    plt.subplot(4,2,i)\n    sns.barplot(x = data2[v],y = data2[\"Target\"])\n    plt.xticks(rotation= 90 )\n\nplt.show()","0b50c35d":"pd.crosstab(data2.race, data2.Target,margins=True)","799710d9":"#Verifying null values in data \n\ndata_test[['workclass','occupation']] = pd.DataFrame(Imp.fit_transform(data_test[['workclass', 'occupation']]))\nx = data_test.isna().sum().sum()\nprint(\"Null values in data_test: {} values\".format(x))\n","5c32a42a":"data_test.shape","94d265c8":"data_test[['workclass','occupation']] = pd.DataFrame(Imp.fit_transform(data_test[['workclass', 'occupation']]))","0b0fb5fb":"print(\"The data test has {} empty values, after handling it\".format(data_test.isna().sum().sum()))","3229a9ba":"data_test.head()","3b8876b8":"#Here the data shownm in the graph are distributed according its occurency on the data frame\n\nvariables = ['education.num','capital.gain', 'capital.loss', 'hours.per.week', 'age']\n\nplt.figure(figsize=(25,15))\n\ni = 0\nfor column in variables:\n    i=i+1\n    plt.subplot(3,2,i)\n    sns.distplot(x = data[column])\n    plt.xlabel(column)\nplt.show()","a7aa04df":"# On the histogram is possible to visualize the discret and continuous variables\n\n\nplt.figure(figsize=(15,10))\ndata.hist(bins=100, figsize=(20, 20))","6a9a9edc":"tX1 = pd.get_dummies(data.drop(columns=[\"income\", 'native.country', 'education.num', 'Id', 'fnlwgt']))\ntX1.shape\ntY = data.income","214cb463":"data_test.shape","cef8239f":"data_test.shape","2bdc271c":"tX1.columns","c9fc976b":"randomParam_ = { \"penalty\":  ['l1','l2', 'elasticnet'],\n                  \"C\"    : sp_randInt(1,24),\n                  \"solver\" :  ['lbfgs','sag','saga', 'newton-cg']               \n                 }\n\nrandomm_ = RandomizedSearchCV(estimator=LogisticRegression(random_state=53, n_jobs=1), param_distributions = randomParam_, cv = 2,\\\n                              n_iter = 10,random_state=42)\nrandomm_.fit(tX1,tY)\n\nprint(\"O melhor estimador \u00e9: \", randomm_.best_estimator_ )\nprint(\"Os melhores par\u00e2metros s\u00e3o: \", randomm_.best_params_) \nprint(\"O melhor score foi: \", randomm_.best_score_) ","6efaf740":"gaussianParameters = { \"var_smoothing\": np.logspace(0,-9, num= 150)}\n\ngaussianRandm = RandomizedSearchCV(estimator=GaussianNB(), param_distributions = gaussianParameters, cv = 2,\\\n                              n_iter = 10,random_state=42)\ngaussianRandm.fit(tX1,tY)\n\nprint(\"O melhor estimador dentre todos os par\u00e2metros pesquisados \u00e9 {}, j\u00e1 os melhores par\u00e2metros s\u00e3o {}.\\n O melhor escore obtido foi: {}\".\\\n    format(gaussianRandm.best_estimator_, gaussianRandm.best_params_, gaussianRandm.best_score_))\n\nprint(\"O melhor estimador \u00e9: \", gaussianRandm.best_estimator_ )\nprint(\"Os melhores par\u00e2metros s\u00e3o: \", gaussianRandm.best_params_) \nprint(\"O melhor score foi: \", gaussianRandm.best_score_) ","bbf8f9d4":"\n\ndecisionTCParameters = { \"criterion\":  ['gini','entropy'],\n                   \"max_depth\"    : sp_randInt(1,27),\n                   \"max_features\" :  ['auto', 'sqrt', 'log2']               \n                 }\ndecisionTCrandm = RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=62), param_distributions = decisionTCParameters, cv = 2,\\\n                              n_iter = 10,random_state=62)\ndecisionTCrandm.fit(tX1,tY)\n\nprint(\"O melhor estimador \u00e9: \", decisionTCrandm.best_estimator_ )\nprint(\"Os melhores par\u00e2metros s\u00e3o: \", decisionTCrandm.best_params_) \nprint(\"O melhor score foi: \", decisionTCrandm.best_score_) ","e710c5a7":"randomFParameters = {\"n_estimators\": sp_randInt(600, 2000),\n                 \"criterion\": ['entropy','gini'],\n                 \"max_depth\": sp_randInt(1, 10)\n                 }\nrandomFRandm = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=57, n_jobs=1), param_distributions = randomFParameters, cv = 2,\\\n                              n_iter = 10,random_state=57)\nrandomFRandm.fit(tX1,tY)\n\n\nprint(\"O melhor estimador \u00e9: \", randomFRandm.best_estimator_ )\nprint(\"Os melhores par\u00e2metros s\u00e3o: \", randomFRandm.best_params_) \nprint(\"O melhor score foi: \", randomFRandm.best_score_)     ","65b04e1d":"gradBRandmParameters = {\"learning_rate\":  sp_randFloat(0.0001,1),\n                  \"subsample\"    : sp_randFloat(),\n                  \"n_estimators\" : sp_randInt(200, 400),\n                  \"max_depth\"    : sp_randInt(2, 7)\n                 }\ngradBRandm = RandomizedSearchCV(estimator=GradientBoostingClassifier(random_state=110), param_distributions = gradBRandmParameters,\\\n                              cv = 5, n_iter = 10, n_jobs=-1, random_state=110)\ngradBRandm.fit(tX1,tY)\n\nprint(\"O melhor estimador \u00e9: \",gradBRandm.best_estimator_ )\nprint(\"Os melhores par\u00e2metros s\u00e3o: \",gradBRandm.best_params_) \nprint(\"O melhor score foi: \",gradBRandm.best_score_)      ","d5b163b5":"score = {}\nscore = {'Logistic Regression': randomm_.best_score_, \n         'Gaussian NB' : gaussianRandm.best_score_,\n         'Decision Tree Classifier' : decisionTCrandm.best_score_,\n         'Random Forest' : randomFRandm.best_score_,\n         'Gradient Boosting Classifier': gradBRandm.best_score_}\n\npd.DataFrame.from_dict(score, orient='index').sort_values(by=0, ascending=False).rename(columns={0:'score'})","5b5d16d2":"X_Test = pd.get_dummies(data_test.drop(columns=['Id','fnlwgt', 'native.country', 'education.num' ])) ","d1a6788d":"predicted = pd.DataFrame(gradBRandm.predict(X_Test), data_test['Id'], columns=['income'])","f097ddff":"predicted.head()","4984eab0":"predicted.to_csv('Submission.csv', index=True, index_label = 'Id')","85ca7aae":"# Comparison of results","ceb570aa":"## models","d7fa9258":"# Gradient Boosting Classifier\n","47fa1cb1":"# Descriptive Analysis","e537bcd7":"# Decision Tree Classifier","d509f9b2":"# Libraries","d0a0e15c":"## Training and handling data ","ecbb17ae":"# Gradient Boosting presented the best result of score","3a845260":"# Random Forest","ef7e530d":"# Explorer Analysis","b0f57d06":"## Functional variables","f69409b6":"# Logistic Regression","f3fa57d5":"# Gaussian NB","015c6bcc":"## Testing"}}