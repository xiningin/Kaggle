{"cell_type":{"aa21db5e":"code","53510868":"code","e0a71255":"code","f3e6fcb4":"code","23b0b99b":"code","f4040c5b":"code","9d517613":"code","0b625359":"code","79fe18d2":"code","2e3b3fd3":"code","6b2c3424":"markdown"},"source":{"aa21db5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53510868":"train = pd.read_csv('\/kaggle\/input\/analytics-vidhya-jobathon\/AV_train_JobAThon.csv')\ntest = pd.read_csv('\/kaggle\/input\/analytics-vidhya-jobathon\/AV_test_JobAThon.csv')","e0a71255":"from functools import wraps\nimport datetime as dt\n\ndef log_step(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        tic = dt.datetime.now()\n        result = func(*args, **kwargs)\n        time_taken = str(dt.datetime.now() - tic)\n        print(f\"just ran step {func.__name__} shape={result.shape} took {time_taken}s\")\n        return result\n    return wrapper\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n@log_step\ndef encoding(data):\n\n    \"\"\"\n    One Hot Encoding and Label Encoding \n    \"\"\"\n    le = LabelEncoder()\n    data['Holding_Policy_Duration'] = le.fit_transform(data['Holding_Policy_Duration'])\n\n    var_mod = ['Accomodation_Type', 'Reco_Insurance_Type', 'Is_Spouse','Health Indicator', 'Holding_Policy_Duration']\n\n    for i in var_mod:\n        data[i] = le.fit_transform(data[i])\n\n    # One Hot Encoding : \n    data = pd.get_dummies(data, columns = ['Accomodation_Type', 'Reco_Insurance_Type', 'Is_Spouse','Health Indicator', 'Holding_Policy_Duration'])\n    \n    return data\n\n@log_step\ndef preprocess(data):\n\n    data['Holding_Policy_Type'] = data['Holding_Policy_Type'].astype(str)\n\n    data['Reco_Policy_Cat'] = data['Reco_Policy_Cat'].astype(str)\n\n    data['Region_Code'] = data['Region_Code'].astype(str)\n    \n    return data\n    \n@log_step\ndef impute(data):\n    \n    data['Holding_Policy_Duration'] = data['Holding_Policy_Duration'].fillna(str(0.0))\n    data['Holding_Policy_Type'] = data['Holding_Policy_Type'].fillna('no_policies')\n    data['Health Indicator'] = data['Health Indicator'].fillna(data['Health Indicator'].mode()[0])\n    \n    return data\n\n@log_step\ndef start_pipeline(dataf):\n    return dataf.copy() \n","f3e6fcb4":"train_df = (train\n      .pipe(start_pipeline)\n      .pipe(impute)\n      .pipe(encoding))","23b0b99b":"test_df = (test\n      .pipe(start_pipeline)\n      .pipe(impute)\n      .pipe(encoding))","f4040c5b":"def min_recall_precision(y_true, y_pred):\n    \n    recall = recall_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    return min(recall, precision)","9d517613":"X = train_df.drop('Response', axis = 1).select_dtypes(exclude = 'object')\ny = train['Response']","0b625359":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import make_scorer, recall_score, precision_score\n\ngrid = GridSearchCV(\n                    estimator = LogisticRegression(max_iter = 1000),\n                    scoring = {'precision': make_scorer(precision_score),\n                              'recall': make_scorer(recall_score),\n                              'min_both': make_scorer(min_recall_precision)},\n                    param_grid = {'class_weight': [{0: 1, 1:v} for v in range(1, 4)]},\n                    refit = 'precision',\n                    return_train_score = True,\n                    cv = 10,\n                    n_jobs = -1\n                    )\n\ngrid.fit(X, y)","79fe18d2":"import matplotlib.pyplot as plt\n\nplt.figure(figsize = (8, 10))\ndf_results = pd.DataFrame(grid.cv_results_)\n\nfor score in ['mean_train_recall', 'mean_train_precision', 'mean_test_recall', 'mean_test_precision']:\n    \n    plt.scatter(x = [_[1] for _ in df_results['param_class_weight']],\n               y = df_results[score.replace('test', 'train')],\n               label = score)\nplt.legend()","2e3b3fd3":"df_results","6b2c3424":"# We are able to track precision and recall metrics at each split and also the min of both"}}