{"cell_type":{"68e79ada":"code","bb4e1ed7":"code","1b053caf":"code","266fb331":"code","75db989f":"code","3a0e34f0":"code","5f00dc72":"code","c8d2b2d6":"code","e761a1ca":"code","abaf61ba":"code","6214dfba":"code","8f72c6c6":"code","cdbd2852":"code","ab103530":"code","11bcf71e":"code","2cba243c":"code","b064a974":"code","5870905d":"code","4d4e8403":"code","766d4db4":"code","0743ee22":"code","33b6c0be":"code","f407294f":"code","e1440151":"code","5eef23ad":"markdown","fa48866b":"markdown","3f269746":"markdown","67f94b87":"markdown","9134e686":"markdown","de328f47":"markdown","79e0d67b":"markdown","5c10f841":"markdown","20fd95f9":"markdown","3b8c3069":"markdown","63e243a6":"markdown","db5a7b05":"markdown","0b8a01d6":"markdown","a856ae17":"markdown","b019fe42":"markdown","d5441654":"markdown","500daca0":"markdown","7af23784":"markdown","aecccdc3":"markdown","eca23db7":"markdown","d52822db":"markdown","3c1b03fe":"markdown","70104023":"markdown","f450f7bb":"markdown","68a39cfa":"markdown","15105b57":"markdown"},"source":{"68e79ada":"# Importing all the dependencies (Hidden Input)\nimport gc  # For Garbage collection. Helps free up memory (RAM)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score  # For evaluation\nfrom sklearn.linear_model import LogisticRegression","bb4e1ed7":"# Helper function to reduce memory usage\n# credits -- https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file)\n    df = reduce_mem_usage(df)\n    return df","1b053caf":"# Reading the train and test data using Pandas\n\ntrain = import_data('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest = import_data('..\/input\/tabular-playground-series-oct-2021\/test.csv')\n\nprint(\"No. of rows and columns in Train Data: \", train.shape)\nprint(\"No. of rows and columns in Test Data: \", test.shape)","266fb331":"gc.collect() # Garbage Collection","75db989f":"# Here, we create y panda-series with only the target column. \n\ny = train['target']\n\n# Next we will drop the 'id' and 'target' from the train and test\n\ntrain.drop(columns = ['id', 'target'], inplace = True)\ntest.drop(columns = 'id', inplace = True)","3a0e34f0":"# Now we will combine the train and test into a master dataframe to perform an EDA\n\ndf_main = pd.concat([train,test], axis=0)\nprint(\"Number of rows and columns in the Master dataframe: \", df_main.shape)","5f00dc72":"del train, test\n\ngc.collect()","c8d2b2d6":"# Checking the descriptive Statistics of the master dataframe (Hidden Input)\n\ndf_main.loc[:,:'f241'].describe(percentiles=[0.01,0.1,0.98,0.99]).T.style.bar(\n    subset=['mean'],\n    color='lightsalmon').background_gradient(\n    subset=['std'], cmap='plasma').background_gradient(subset=['99%','98%'], cmap='plasma').background_gradient(\n    subset=['max'], cmap='plasma')","e761a1ca":"gc.collect()","abaf61ba":"gc.collect()","6214dfba":"# With the above features, we can convert them into binary features\n\n# For example: if we take the mean of the values in the columns, we can split the data into binary with values above mean as 1 and others 0.\n\nlis_cols = ['f44', 'f138', 'f139', 'f144', 'f146', 'f157', 'f158', 'f160']\n\nfor col in lis_cols:\n    mean = df_main[col].mean()\n    df_main[col] = df_main[col].apply(lambda x: 1 if x > mean else 0)\n    gc.collect()","8f72c6c6":"# Creating a heatmap of the numerical column (Hidden Input)\n\nheat = df_main.corr()\nplt.figure(figsize=[16,8])\nplt.title(\"Correlation between all the Features\", size=25, pad=20, color='#1f2833')\nsns.heatmap(heat, cmap=['#0b0c10', '#1f2833','#c5c6c7','#45a29e','#66fcf1'], annot=False)\nplt.show()","cdbd2852":"gc.collect()","ab103530":"# Splitting the data into train and test again and merging the target for train\n\ntrain = df_main.iloc[:1000000,:]\ntest = df_main.iloc[1000000:,:]\n\nprint(train.shape)\nprint(test.shape)","11bcf71e":"del df_main\n\ngc.collect()","2cba243c":"# Creation of the Dataframe\n\ndef Stacking_Data_Loader(model, model_name, train, y, test, fold):\n    '''\n    Put your train, test datasets and fold value!\n    This function returns train, test datasets for stacking ensemble :)\n    '''\n\n    stk = StratifiedKFold(n_splits = fold, random_state = 42, shuffle = True)\n    \n    # Declaration Pred Datasets\n    train_fold_pred = np.zeros((train.shape[0], 1))\n    test_pred = np.zeros((test.shape[0], fold))\n    \n    for counter, (train_index, valid_index) in enumerate(stk.split(train, y)):\n        x_train, y_train = train.iloc[train_index], y[train_index]\n        x_valid, y_valid = train.iloc[valid_index], y[valid_index]\n\n        print('------------ Fold', counter+1, 'Start! ------------')\n        if model_name == 'cat':\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)])\n        elif model_name == 'xgb':\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric = 'auc', verbose = 500, early_stopping_rounds = 200)\n        else:\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric = 'auc', verbose = 500, early_stopping_rounds = 200)\n            \n        print('------------ Fold', counter+1, 'Done! ------------')\n        \n        train_fold_pred[valid_index, :] = model.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n        test_pred[:, counter] = model.predict_proba(test)[:, 1]\n        \n        del x_train, y_train, x_valid, y_valid\n        gc.collect()\n        \n    test_pred_mean = np.mean(test_pred, axis = 1).reshape(-1, 1)\n    \n    del test_pred\n    gc.collect()\n    \n    print('Done!')\n    \n    return train_fold_pred, test_pred_mean","b064a974":"# All the necessary parameters\n\nlgb_params = {\n    'objective': 'binary',\n    'n_estimators': 23000,\n    'random_state': 42,\n    'learning_rate': 0.004,\n    'subsample': 0.8,\n    'subsample_freq': 7,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 0.5,\n    'min_child_weight': 256,\n    'min_child_samples': 15,\n    'metric': 'auc',\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id' : 0,\n    'n_jobs': -1\n}\n\n\nxgb_params = {'n_estimators': 7000,\n               'learning_rate': 0.05,\n               'max_depth': 6,\n               'colsample_bytree': 0.2,\n               'subsample': 0.7,\n               'eval_metric': 'auc',\n               'use_label_encoder': False,\n               'gamma': 0,\n               'reg_lambda': 60.0,\n               'tree_method': 'gpu_hist',\n               'gpu_id': 0,\n               'predictor': 'gpu_predictor',\n               'n_jobs': -1,\n               'random_state': 42}\n\ncat_params = {'iterations': 15000,\n               'learning_rate': 0.01,\n               'reg_lambda': 0.4,\n               'subsample': 0.9,\n               'random_strength': 25,\n               'depth': 8,\n               'min_data_in_leaf': 6,\n               'leaf_estimation_iterations': 6,\n               'task_type':\"GPU\",\n               'bootstrap_type':'Poisson',\n               'verbose' : 500,\n               'early_stopping_rounds' : 200,\n               'eval_metric' : 'AUC',\n               'thread_count': -1}","5870905d":"gc.collect()","4d4e8403":"# Creating intance of the object\n\nlgbm = LGBMClassifier(**lgb_params)\n\nxgb = xgb.XGBClassifier(**xgb_params)\n\ncat = CatBoostClassifier(**cat_params)","766d4db4":"# Stacking all the models\n\ncat_train, cat_test = Stacking_Data_Loader(cat, 'cat', train, y, test, 5)\ndel cat\ngc.collect()\n\nlgbm_train, lgbm_test = Stacking_Data_Loader(lgbm, 'lgbm', train, y, test, 5)\ndel lgbm\ngc.collect()\n\nxgb_train, xgb_test = Stacking_Data_Loader(xgb, 'xgb', train, y, test, 5)\ndel xgb\ngc.collect()","0743ee22":"# Getting the X_train and X_test\n\nstack_x_train = np.concatenate((cat_train, lgbm_train, xgb_train), axis = 1)\nstack_x_test = np.concatenate((cat_test, lgbm_test, xgb_test), axis = 1)\n\ndel cat_train, lgbm_train, xgb_train, cat_test, lgbm_test, xgb_test\ngc.collect()\n\nstack_x_train","33b6c0be":"# Stacking\n\nstk = StratifiedKFold(n_splits = 5)\n\ntest_pred_lo = 0\nfold = 1\ntotal_auc = 0\n\nfor train_index, valid_index in stk.split(stack_x_train, y):\n    x_train, y_train = stack_x_train[train_index], y[train_index]\n    x_valid, y_valid = stack_x_train[valid_index], y[valid_index]\n    \n    lr = LogisticRegression(n_jobs = -1, random_state = 1, C = 10, max_iter = 17000)\n    lr.fit(x_train, y_train)\n    \n    valid_pred_lo = lr.predict_proba(x_valid)[:, 1]\n    test_pred_lo += lr.predict_proba(stack_x_test)[:, 1]\n    auc = roc_auc_score(y_valid, valid_pred_lo)\n    total_auc += auc \/ 5\n    print('Fold', fold, 'AUC :', auc)\n    fold += 1\n    \nprint('Total AUC score :', total_auc)","f407294f":"gc.collect()","e1440151":"# Submitting into the csv file\n\nsub = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\nsub['target'] = valid_pred_lo\nsub.to_csv('my_stack_sub.csv', index = 0)\nsub","5eef23ad":"# Strange features with High Std. Deviations:","fa48866b":"### Different Approaches:\n\n<p style=\"font-size:120%\"> Now, deciding which model to build is the most common question that you might ask. The answer to this can be found using various AutoML libraries. One such great Library known as FLAML is a great choice, since it also performs Hyperparameter tuning for you. Read more about them <a href=\"https:\/\/www.kaggle.com\/general\/275778#1530938\" style=\"text-decoration:none\">here<\/a>.<\/p>\n\n<p style=\"font-size:120%\"> Once you have your model decided, you can use a single model to build! And further, you can perform more hyperparameter tuning on it using Optuna. In this competition, single models have shown great performances! Check out one such model <a href=\"https:\/\/www.kaggle.com\/mohammadkashifunique\/tsp-single-xgboost-model\" style=\"text-decoration:none\">here<\/a>.<\/p>\n\n<p style=\"font-size:120%\"> Another very important thing to keep in mind while competing is, Esemble models sometimes give you an edge over single models. Techniques like Stacking and Blending have proven to improve score. Therefore, Stacking is the approach that I learnt personally from Jun Hyeok Park's <a href=\"https:\/\/www.kaggle.com\/junhyeok99\/stacking-ensemble-tutorial\" style=\"text-decoration:none\">notebook<\/a>.<\/p>\n\n<p style=\"font-size:120%\"> For the approach below, I am not running the code, since it will take forever. You are free to experiment with it. <\/p>","3f269746":"<p style=\"font-size:120%\"> Finally, to sum up, there were a lot of things to learn in this competition. Met many people, had great discussions on many ideas with them.<\/p>\n\n<p style=\"font-size:120%\"> One of the Kaggler had very well summed up her experience in Kaggle competitions. You can read about it <a href=\"https:\/\/www.kaggle.com\/getting-started\/275912\" style=\"\">here<\/a>.<\/p>","67f94b87":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       My Leaderboard Score <\/h1>\n<\/div>","9134e686":"### Here are some Resources that I found in the Discussion:\n<ul>\n    <li style=\"font-size:120%\" >Understanding the Evaluation metric: AUC ( Area Under Curve) by <a href=\"https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275733\" style=\"text-decoration:none\">Kriti Doneria<\/a><\/li>\n    <li style=\"font-size:120%\" >Understand Evaluation metric For this Competition - ROC Curve? by <a href=\"https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275641\" style=\"text-decoration:none\">Shravan Kumar Koninti.<\/a><\/li>\n<\/ul>","de328f47":"<p style=\"font-size:120%\"> Before we move further, let's have a quick look at our data.<\/p>","79e0d67b":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Let's have a look at the data! <\/h1>\n<\/div>","5c10f841":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Purpose of this Notebook <\/h1>\n<\/div>","20fd95f9":"![image.png](attachment:d415eec1-bb3c-4305-92c4-4ba356e77c5d.png)","3b8c3069":"<div class=\"alert alert-success\">\n    <h3 style=\"text-align:center\"> If you liked\/forked this notebook, consider giving it an upvote. Also, go check out other Kagglers' mentioned throughout this notebook. They are all gems! and there are many things to learn from each one of them. <\/h3>\n    <h1 style=\"text-align:center\">All the Best!<\/h1>\n<\/div>","63e243a6":"<div class=\"alert alert-success\">\n\n<p style=\"font-size:120%;color:black\"> \u2b50 For more on EDA, I would suggest you look into <a href=\"https:\/\/www.kaggle.com\/legendsoul\/tps-october-21-comprehensive-insight-of-eda\" style=\"text-decoration:none\">this notebook<\/a> by EMIN KAZDALO\u011eLU <\/p>\n    \n<\/div>","db5a7b05":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Brief Exploratory Data Analysis <\/h1>\n<\/div>","0b8a01d6":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Everything I Learnt in a Kaggle Competition \ud83e\udde0\ud83d\udca1 TPS_October_21\n<\/div>","a856ae17":" <p style=\"text-align:center; font-size:150%\"><i><strong> \"Life is a race \u2026 If you don\u2019t run fast\u2026 you will be like a broken andaa(egg in Hindi)\u2026\" <\/i>\u2013 Aamir Khan (3 Idiots) <\/strong><\/p>","b019fe42":"<p style=\"font-size:120%\"> A little context before we begin. I am a beginner who recently started picking up Machine Learning solely due to personal interest.(C'mon, who's gonna disagree to the fact that ML is fun!\ud83d\ude1c) This time, I decided to enroll into a competition to see how well my ML skills are compared to others on this platform. Little did I know, I was way behind.<\/p>\n\n<p style=\"font-size:120%\"> Kaggle Competitions are a great way to improve your skill and increase your knowledge through a lot of collaborations, learnings and discussions. We are blessed, Kaggle allows us to do just that. Thank you Kaggle \ud83e\udd73<\/p>\n    \n<p style=\"font-size:120%\"> Therefore, the sole purpose of this notebook is to help beginners or participants in any competition understand, which factors are important and what one should keep in mind while competing in a Kaggle Competition based off my experience. In-case you disagree to some points, or even have more points to add to this, please feel free to add them in the comments. Let's all grow together by teaching each other.\ud83c\udf93<\/p>\n\n<p style=\"font-size:120%\"> All the Credits goes to all the individuals that came up with several brilliant ideas and discussions during <strong>TPS_Oct_21<\/strong>. You guys are amazing!<\/p>","d5441654":"<p style=\"font-size:120%\"> By looking at the above Descriptive Statistics of the Master Dataframe, we can see that, the Standard Deviation of the features: <strong>f44, f138, f139, f144 , f146, f157, f158 and f160<\/strong> are abnormally high! <\/p>\n\n<p style=\"font-size:120%\"> To better understand this, let's look at some kdeplot.<\/p>","500daca0":"<p style=\"font-size:120%\"> The above helper function, assists us in reducing the memory that is consumed by modifying the data types in each of the columns. As, you can see below, it reduces memory usage by 76.9%<\/p>","7af23784":"![image.png](attachment:6354f4f1-3c52-4a20-88ca-598658b6360b.png)","aecccdc3":"# The Competition:\n\n<p style=\"font-size:120%\"> The dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features.<\/p>\n\n<p style=\"font-size:120%\"> To put in simple words, it is a binary classification problem statement.<\/p>\n\n# Evaluation Metric:\n\n<p style=\"font-size:120%\"> The model performance will be evaluated based on the values predicted by our model on the Test dataset on the basis of <strong>Area Under the Curve<\/strong>, or <strong>roc_auc_score.<\/strong><\/p>","eca23db7":"<p style=\"font-size:120%\"> For those of you wondering why we have used <strong>gc.collect() (Garbage Collection)<\/strong>, it has been used to clean up a huge amount of objects that would have been created. This is done to free up memory. Read more about it <a href=\"https:\/\/stackify.com\/python-garbage-collection\/\" style=\"text-decoration:none\">here.<\/a><\/p>\n\n<p style=\"font-size:120%\"> If we do not do that however, It might crash the notebook.<\/p>\n\n### Here are some Resources that I found in the Discussion:\n<ul>\n    <li style=\"font-size:120%\" >Getting memory exceeded | reduce memory by <a href=\"https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275663\" style=\"text-decoration:none\">shuvo.<\/a><\/li>\n    <li style=\"font-size:120%\" >Save memory for running your best models by <a href=\"https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275854\" style=\"text-decoration:none\">Luca Massaron.<\/a><\/li>\n<\/ul>","d52822db":"<p style=\"font-size:120%\"> Now that took a <strong>PRETTY LONG TIME!<\/strong><\/p>\n\n<p style=\"font-size:120%\"> It is evident by the fact that there are <strong>1MILLION rows!<\/strong> in the Train data and 500k in the Test.<\/p>\n\n### The Solution?\n\n<p style=\"font-size:120%\"> There are better and FASTER ways to work with a dataframe that has many records.<\/strong><\/p>\n\n### Here are some Resources that I found in the Discussion:\n<ul>\n    <li style=\"font-size:120%\" >1M rows: ...how to read in only some of the data by <a href=\"https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275669\" style=\"text-decoration:none\">Carl McBride Ellis.<\/a><\/li>\n    <li style=\"font-size:120%\" >\ud83d\udd25\ud83d\udd25Tutorial compilation for handling larger datasets by <a href=\"https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275712\" style=\"text-decoration:none\">Tensor Girl.<\/a><\/li>\n    <li style=\"font-size:120%\" >Datatable loads data 6.6x faster than pandas! by <a href=\"https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/276162\" style=\"text-decoration:none\">L0Z1K.<\/a><\/li>\n    <li style=\"font-size:120%\" >How to speed up Pandas (Article) by <a href=\"https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275670\" style=\"text-decoration:none\">Maximiliano Diaz Battan.<\/a><\/li>\n<\/ul>\n\n<p style=\"font-size:120%\"> For now, I'm going to go ahead with good 'ol Pandas.<\/strong><\/p>","3c1b03fe":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Understanding the Competition and the Data <\/h1>\n<\/div>","70104023":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       My Submission <\/h1>\n<\/div>","f450f7bb":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Model Building <\/h1>\n<\/div>","68a39cfa":"<img src=\"https:\/\/i1.wp.com\/awajis.com\/wp-content\/uploads\/2017\/12\/comp.png?resize=768%2C337&ssl=1\" style=\"display: block; margin-left: auto; margin-right: auto; width: 50%;\"><\/img>","15105b57":"<p style=\"font-size:120%\"> You can find more on the above analysis here in my <a href=\"https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/276416\" style=\"text-decoration:none\">discussion.<\/a><\/p>"}}