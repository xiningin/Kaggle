{"cell_type":{"3039bd68":"code","7dc5e4b6":"code","f6570ecd":"code","206a878e":"code","7c532219":"code","b85dbadb":"code","1d56e331":"code","1532dc8a":"code","77eb658d":"code","6b22c0d6":"code","f2a0d69b":"code","e4b19556":"code","ac23d046":"code","9c84dd60":"code","56cf71da":"code","3fee3a0a":"code","d7f1ae31":"code","41aab7ec":"code","417ec8b1":"code","900b1b91":"code","ea53755e":"code","a954dd9f":"code","bd527590":"code","051dde24":"code","50e36357":"code","06c05fd9":"code","f7d3d95f":"code","6f7b6dfe":"code","8a68af77":"code","5d526bf8":"code","9b572484":"code","528b3e2b":"code","3ef8cd73":"code","77ab2e70":"code","c940bf52":"code","baf77b62":"code","76efa779":"code","63de6d27":"code","ae0f371f":"code","b1bf5e2f":"code","94d130df":"code","44fc942e":"code","b1cef520":"markdown","c0540e3f":"markdown","d2e71add":"markdown","5f1ec3e3":"markdown","70a1a096":"markdown"},"source":{"3039bd68":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7dc5e4b6":"!pip install --upgrade 'lightgbm>=3.0.0'\n!pip install \"pycaret\"","f6570ecd":"import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport pdb\nimport warnings\nimport seaborn as sns\nimport shap\n\nfrom sklearn import metrics\nfrom pycaret.classification import *\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss, f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom lightgbm import LGBMClassifier\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12, 4)\nwarnings.filterwarnings('ignore')","206a878e":"STOP_WORDS = ['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', '\u00e9', 'com', 'n\u00e3o', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'foi', 'ao', 'ele', 'das', 'tem', '\u00e0', 'seu', 'sua', 'ou', 'ser', 'quando', 'muito', 'h\u00e1', 'nos', 'j\u00e1', 'est\u00e1', 'eu', 'tamb\u00e9m', 's\u00f3', 'pelo', 'pela', 'at\u00e9', 'isso', 'ela', 'entre', 'era', 'depois', 'sem', 'mesmo', 'aos', 'ter', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'est\u00e3o', 'voc\u00ea', 'tinha', 'foram', 'essa', 'num', 'nem', 'suas', 'meu', '\u00e0s', 'minha', 't\u00eam', 'numa', 'pelos', 'elas', 'havia', 'seja', 'qual', 'ser\u00e1', 'n\u00f3s', 'tenho', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'fosse', 'dele', 'tu', 'te', 'voc\u00eas', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'est\u00e1', 'estamos', 'est\u00e3o', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'est\u00e1vamos', 'estavam', 'estivera', 'estiv\u00e9ramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estiv\u00e9ssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'h\u00e1', 'havemos', 'h\u00e3o', 'houve', 'houvemos', 'houveram', 'houvera', 'houv\u00e9ramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houv\u00e9ssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houver\u00e1', 'houveremos', 'houver\u00e3o', 'houveria', 'houver\u00edamos', 'houveriam', 'sou', 'somos', 's\u00e3o', 'era', '\u00e9ramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'f\u00f4ramos', 'seja', 'sejamos', 'sejam', 'fosse', 'f\u00f4ssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'ser\u00e1', 'seremos', 'ser\u00e3o', 'seria', 'ser\u00edamos', 'seriam', 'tenho', 'tem', 'temos', 't\u00e9m', 'tinha', 't\u00ednhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tiv\u00e9ramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tiv\u00e9ssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'ter\u00e1', 'teremos', 'ter\u00e3o', 'teria', 'ter\u00edamos', 'teriam']\nCLUB_WORDS = [\"verd\u00e3o\", \"tricolor\", \"fla\", \"tim\u00e3o\", \"rubro\", \n              \"negro\", \"flamengo\", \"paulo\", \"palmeirense\", \"paulista\", \n              \"inter\", \"colorado\", \"internacional\", \"colorados\", \"colorada\"]\nSTOP_WORDS += CLUB_WORDS\nMAX_VOCABULARY = 1000\npositive_case = \"flamengo\"\ntrain_end_date = \"2018-01\"\nholdout_end_date = \"2020-10\"\ntarget = \"target\"\ntime_column = \"year-month\"","7c532219":"def fix_small_data_period(data):\n    data = data[(data[\"year-month\"] < \"2015-12\") | (data[\"year-month\"] > \"2016-08\")]   \n    return data \n\ndef exclude_periods_without_positive_case(data, positive_case, period_column, threshold=20):\n    df = data.groupby(period_column)[\"club\"].apply(lambda x: np.sum(x == positive_case))\n    df = df[df > threshold]\n    return data[data[period_column].isin(df.index)]\n    \ndef clean_club_name_from_article(data):\n    data[\"text\"] = data.apply(lambda x: x[\"text\"].lower().replace(x[\"club\"].replace(\"-\", \" \"), \"\"), axis=1)\n    return data\n\ndef exclude_numbers(data):\n    data[\"text\"] = data[\"text\"].apply(lambda x: ''.join([i for i in x.lower() if not i.isdigit()]))\n    return data\n\ndef drop_multiple_teams_news(data):\n    multiple_team_news = data.groupby(\"link\", as_index=False)[\"club\"].count()\n    multiple_team_news = multiple_team_news[multiple_team_news[\"club\"] > 1][\"link\"]\n    return data[~data[\"link\"].isin(multiple_team_news)]","b85dbadb":"data = pd.read_csv(\"\/kaggle\/input\/ge-soccer-clubs-news\/ge_news.csv\", index_col=False)","1d56e331":"data.shape","1532dc8a":"data = clean_club_name_from_article(data)\ndata = exclude_numbers(data)\ndata = drop_multiple_teams_news(data)","77eb658d":"data.loc[:, \"year\"] = data[\"date\"].apply(lambda x: x.split(\"\/\")[-1])\ndata.loc[:, \"month\"] = data[\"date\"].apply(lambda x: x.split(\"\/\")[1])\ndata.loc[:, \"date\"] = pd.to_datetime(data[\"date\"])","6b22c0d6":"### Monthly context\ndata[\"year-month\"] = data[\"year\"] + \"-\" + data[\"month\"]","f2a0d69b":"data[\"target\"] = data[\"club\"].apply(lambda x: 1 if x == positive_case else 0)\n\nprint(\"The fraction of positive cases is {:.2f}\".format(data[\"target\"].mean()))","e4b19556":"data = exclude_periods_without_positive_case(data, positive_case, \"year-month\")","ac23d046":"data.shape","9c84dd60":"in_time = data[data[\"year-month\"] < train_end_date]\n\ntrain, test = train_test_split(in_time, \n                               test_size=0.2, \n                               random_state=42)\n\nout_of_time = data[(data[\"year-month\"] >= train_end_date) & (data[\"year-month\"] <= holdout_end_date)]","56cf71da":"print(\"Dataset shapes:\")\nprint(\"Train: {}\".format(train.shape))\nprint(\"Test: {}\".format(test.shape))\nprint(\"Out of time: {}\".format(out_of_time.shape))","3fee3a0a":"vectorizer = TfidfVectorizer(max_features=MAX_VOCABULARY,\n                                 stop_words=STOP_WORDS,\n                                 binary=True,\n                                 use_idf=True)\n\nvectorizer = vectorizer.fit(train[\"text\"])\n\ntrain_vectors = pd.DataFrame(vectorizer.transform(train[\"text\"]).toarray(), columns=vectorizer.vocabulary_)\ntest_vectors = pd.DataFrame(vectorizer.transform(test[\"text\"]).toarray(), columns=vectorizer.vocabulary_)\noot_vectors = pd.DataFrame(vectorizer.transform(out_of_time[\"text\"]).toarray(), columns=vectorizer.vocabulary_)","d7f1ae31":"model = LGBMClassifier()\nmodel.fit(train_vectors, train[\"target\"].values)","41aab7ec":"experiment_dict = {}\nfor period in in_time[time_column].unique():\n    print(period)\n    in_time_period = in_time[in_time[time_column] == period]\n    test_period = test[test[time_column] == period]\n\n    test_vectors_ = pd.DataFrame(vectorizer.transform(test_period[\"text\"]).toarray(), columns=vectorizer.vocabulary_)\n    predictions = model.predict(test_vectors_, pred_contrib=True)\n    experiment_dict[period] = predictions","417ec8b1":"importance = []\nfor key, value in experiment_dict.items():\n    shap_contrib = np.abs(value[:, :-1]).mean(0)\n    df = pd.DataFrame(shap_contrib.reshape(1, MAX_VOCABULARY), columns=vectorizer.vocabulary_)\n    df[\"period\"] = key\n    importance.append(df)\n\nimportance = pd.concat(importance)","900b1b91":"average = importance.mean()","ea53755e":"average.sort_values()","a954dd9f":"deviation = importance.std() \/ average","bd527590":"deviation.fillna(0, inplace=True)\ndeviation","051dde24":"median_average_contrib = average.median()\nmedian_std_contrib = deviation.median()\n\nmedian_average_contrib = np.percentile(average, 50)\nmedian_std_contrib = np.percentile(deviation, 80)","50e36357":"plt.scatter(average, deviation)\n\nxmin, xmax, ymin, ymax = plt.axis()\nplt.hlines(median_std_contrib, xmin, xmax, linestyle=\"dotted\", color=\"red\")\nplt.vlines(median_average_contrib, ymin, ymax, linestyle=\"dotted\", color=\"red\")\nplt.legend(bbox_to_anchor=(1.05, 1.0))\nplt.title(\"Contribution x Instability\")\nplt.ylabel(\"Instability\")\nplt.xlabel(\"Contribution\")\nplt.show()","06c05fd9":"aggregate_importance = pd.DataFrame()\naggregate_importance[\"average\"] = average.values\naggregate_importance[\"instability\"] = deviation.values\naggregate_importance.index = average.index\n\naggregate_importance","f7d3d95f":"mask = (aggregate_importance[\"average\"] >= median_average_contrib) & (aggregate_importance[\"instability\"] <= median_std_contrib)\nstable_features = aggregate_importance[mask].index","6f7b6dfe":"train_vectors = pd.DataFrame(vectorizer.transform(train[\"text\"]).toarray(), columns=vectorizer.vocabulary_)\ntest_vectors = pd.DataFrame(vectorizer.transform(test[\"text\"]).toarray(), columns=vectorizer.vocabulary_)","8a68af77":"in_time_data = pd.DataFrame(vectorizer.transform(in_time[\"text\"]).toarray(), columns=vectorizer.vocabulary_)\nin_time_data[target] = in_time[target].values","5d526bf8":"clf1 = setup(in_time_data, target=target,\n                      session_id=123, \n                      log_experiment=True, \n                      experiment_name=\"exp1\",\n            silent=True)\n\nlgbm = create_model(\"lightgbm\")\n\ntuned_lgbm = tune_model(lgbm,\n                        fold=5,\n                        n_iter=50,\n                        optimize=\"AUC\")\n\nbest = automl(optimize = 'AUC')","9b572484":"model = best\nmodel.fit(train_vectors, train[\"target\"].values)\npredictions = model.predict_proba(test_vectors)[:, 1]\ntest[\"benchmark\"] = predictions\nroc_auc_score(test[\"target\"], predictions)","528b3e2b":"predictions = model.predict_proba(oot_vectors)[:, 1]\nout_of_time[\"benchmark\"] = predictions\nroc_auc_score(out_of_time[\"target\"], predictions)","3ef8cd73":"out_of_time.groupby(time_column).apply(lambda x: roc_auc_score(x[target], x[\"benchmark\"])).plot()\nplt.title(\"AUC for the out of time set (unseen future data)\")\nplt.xlabel(\"Period (monthly)\")\nplt.ylabel(\"AUC\")\nplt.legend()","77ab2e70":"out_of_time.groupby(time_column).apply(lambda x: roc_auc_score(x[target], x[\"benchmark\"])).rolling(6).mean().plot()\nplt.title(\"Moving average AUC with a 6 months window in the out of time set\")\nplt.ylabel(\"AUC\")\nplt.xlabel(\"Period (monthly)\")\nplt.show()","c940bf52":"clf2 = setup(in_time_data[list(stable_features) + [target]], target=target,\n                      session_id=123, \n                      log_experiment=True, \n                      experiment_name=\"exp1\",\n                     silent=True)\n\nlgbm = create_model(\"lightgbm\")\n\ntuned_lgbm = tune_model(lgbm,\n                        fold=5,\n                        n_iter=50,\n                        optimize=\"AUC\")\n\nbest = automl(optimize = 'AUC')\nmodel = best","baf77b62":"model.fit(train_vectors[stable_features], train[\"target\"].values)","76efa779":"predictions = model.predict_proba(test_vectors[stable_features], pred_contrib=True)\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(test_vectors[stable_features])\nshap.summary_plot(shap_values, test_vectors[stable_features], plot_type=\"bar\")","63de6d27":"predictions = model.predict_proba(test_vectors[stable_features])[:, 1]\ntest[\"temporal_prediction\"] = predictions\nprint(roc_auc_score(test[\"target\"], predictions))\npredictions = model.predict_proba(oot_vectors[stable_features])[:, 1]\nout_of_time[\"temporal_prediction\"] = predictions\nprint(roc_auc_score(out_of_time[\"target\"], predictions))","ae0f371f":"pd.concat([test, out_of_time]).groupby(\"year\").apply(lambda x: roc_auc_score(x[target], x[\"benchmark\"])).plot(label=\"All features (benchmark)\")\npd.concat([test, out_of_time]).groupby(\"year\").apply(lambda x: roc_auc_score(x[target], x[\"temporal_prediction\"])).plot(label=\"Stable features(challenger)\", \n                                                                                                                        color=\"green\")\nplt.legend()\nplt.title(\"AUC\")","b1bf5e2f":"pd.concat([test, out_of_time]).groupby(\"year\").apply(lambda x: log_loss(x[target], x[\"benchmark\"])).plot(label=\"All features (benchmark)\")\npd.concat([test, out_of_time]).groupby(\"year\").apply(lambda x: log_loss(x[target], x[\"temporal_prediction\"])).plot(label=\"Stable features(challenger)\", color=\"green\")\nplt.legend()\nplt.title(\"Log loss\")","94d130df":"pd.concat([test, out_of_time]).groupby(time_column).apply(lambda x: roc_auc_score(x[target], x[\"benchmark\"])).plot(label=\"All features (benchmark)\")\npd.concat([test, out_of_time]).groupby(time_column).apply(lambda x: roc_auc_score(x[target], x[\"temporal_prediction\"])).plot(label=\"Stable features(challenger)\")\nplt.legend()\nplt.title(\"AUC\")","44fc942e":"fig, ax = plt.subplots(figsize=(12, 4))\npd.concat([test, out_of_time]).groupby(time_column).apply(lambda x: roc_auc_score(x[target], x[\"benchmark\"])).rolling(6).mean().plot(label=\"All features (benchmark)\")\npd.concat([test, out_of_time]).groupby(time_column).apply(lambda x: roc_auc_score(x[target], x[\"temporal_prediction\"])).rolling(6).mean().plot(label=\"Stable features (challenger)\", color=\"green\")\n\nxmin, xmax, ymin, ymax = plt.axis()\ntrain_end_date_position = np.argmax(np.sort(pd.concat([test, out_of_time])[time_column].unique()) == train_end_date)\nplt.vlines(train_end_date_position, ymin, ymax, linestyle=\"dotted\", color=\"red\", label=\"Out of time split\")\nplt.legend()\nplt.title(\"Moving average AUC with a 6 months window for both test and out of time periods\", pad=16)\nplt.ylabel(\"AUC\")\nplt.xlabel(\"Period (monthly)\")\nplt.show()\n","b1cef520":"### Stable features model (challenger)","c0540e3f":"### Benchmark (All features)","d2e71add":"## Comparing a model using stable features Vs model using all features","5f1ec3e3":"# Temporal feature selection with shap values\n","70a1a096":"## Calculating Features instability"}}