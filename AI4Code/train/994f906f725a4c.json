{"cell_type":{"0e80cc80":"code","09976f7f":"code","5eff969e":"code","865c8ccf":"code","65e7c6af":"code","f6ab03d9":"code","66918a3f":"code","2932ea88":"code","854ed898":"code","c8aad86b":"code","389d47dd":"markdown"},"source":{"0e80cc80":"#the basics\nimport pandas as pd, numpy as np\nimport math, json, gc, random, os, sys\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\n#tensorflow deep learning basics\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import Layer,Input,Activation, Lambda,Conv1D, SpatialDropout1D,Convolution1D,Dense,add,GlobalMaxPooling1D,GlobalAveragePooling1D,concatenate,Embedding\n\nfrom tensorflow.keras.models import Model\nfrom typing import List, Tuple\nfrom tensorflow.keras.utils import plot_model\n\n#for model evaluation\nfrom sklearn.model_selection import train_test_split, KFold,  StratifiedKFold","09976f7f":"#get comp data\ntrain = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsample_sub = pd.read_csv(\"\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv\")","5eff969e":"#target columns\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","865c8ccf":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n\ntrain_inputs_all = preprocess_inputs(train)\ntrain_labels_all = np.array(train[target_cols].values.tolist()).transpose((0, 2, 1))","65e7c6af":"def channel_normalization(x):\n    # type: (Layer) -> Layer\n    \"\"\" Normalize a layer to the maximum activation\n    This keeps a layers values between zero and one.\n    It helps with relu's unbounded activation\n    Args:\n        x: The layer to normalize\n    Returns:\n        A maximal normalized layer\n    \"\"\"\n    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n    out = x \/ max_values\n    return out\n\n\ndef wave_net_activation(x):\n    # type: (Layer) -> Layer\n    \"\"\"This method defines the activation used for WaveNet\n    described in https:\/\/deepmind.com\/blog\/wavenet-generative-model-raw-audio\/\n    Args:\n        x: The layer we want to apply the activation to\n    Returns:\n        A new layer with the wavenet activation applied\n    \"\"\"\n    tanh_out = Activation('tanh')(x)\n    sigm_out = Activation('sigmoid')(x)\n    return L.multiply([tanh_out, sigm_out])\n\n\ndef residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0):\n    # type: (Layer, int, int, str, int, int, float, str) -> Tuple[Layer, Layer]\n    \"\"\"Defines the residual block for the WaveNet TCN\n    Args:\n        x: The previous layer in the model\n        s: The stack index i.e. which stack in the overall TCN\n        i: The dilation power of 2 we are using for this residual block\n        activation: The name of the type of activation to use\n        nb_filters: The number of convolutional filters to use in this block\n        kernel_size: The size of the convolutional kernel\n        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n        name: Name of the model. Useful when having multiple TCN.\n    Returns:\n        A tuple where the first element is the residual model layer, and the second\n        is the skip connection.\n    \"\"\"\n\n    original_x = x\n    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n                  dilation_rate=i, padding=padding,\n                  )(x)\n    if activation == 'norm_relu':\n        x = Activation('relu')(conv)\n        x = Lambda(channel_normalization)(x)\n    elif activation == 'wavenet':\n        x = wave_net_activation(conv)\n    else:\n        x = Activation(activation)(conv)\n\n    x = SpatialDropout1D(dropout_rate)(x)\n\n    # 1x1 conv.\n    x = Convolution1D(nb_filters, 1, padding='same')(x)\n    res_x = L.add([original_x, x])\n    return res_x, x\n\n\ndef process_dilations(dilations):\n    def is_power_of_two(num):\n        return num != 0 and ((num & (num - 1)) == 0)\n\n    if all([is_power_of_two(i) for i in dilations]):\n        return dilations\n\n    else:\n        new_dilations = [2 ** i for i in dilations]\n        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n        return new_dilations\n\n\nclass TCN(Layer):\n    \"\"\"Creates a TCN layer.\n        Args:\n            input_layer: A tensor of shape (batch_size, timesteps, input_dim).\n            nb_filters: The number of filters to use in the convolutional layers.\n            kernel_size: The size of the kernel to use in each convolutional layer.\n            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n            nb_stacks : The number of stacks of residual blocks to use.\n            activation: The activations to use (norm_relu, wavenet, relu...).\n            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n            name: Name of the model. Useful when having multiple TCN.\n        Returns:\n            A TCN layer.\n        \"\"\"\n\n    def __init__(self,\n                 nb_filters=64,\n                 kernel_size=2,\n                 nb_stacks=1,\n                 dilations=None,\n                 activation='norm_relu',\n                 padding='causal',\n                 use_skip_connections=True,\n                 dropout_rate=0.0,\n                 return_sequences=True,\n                 ):\n        super().__init__()\n        self.return_sequences = return_sequences\n        self.dropout_rate = dropout_rate\n        self.use_skip_connections = use_skip_connections\n        self.activation = activation\n        self.dilations = dilations\n        self.nb_stacks = nb_stacks\n        self.kernel_size = kernel_size\n        self.nb_filters = nb_filters\n        self.padding = padding\n\n        # backwards incompatibility warning.\n        # o = tcn.TCN(i, return_sequences=False) =>\n        # o = tcn.TCN(return_sequences=False)(i)\n\n        if padding != 'causal' and padding != 'same':\n            raise ValueError(\"Only 'causal' or 'same' paddings are compatible for this layer.\")\n\n        if not isinstance(nb_filters, int):\n            print('An interface change occurred after the version 2.1.2.')\n            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n            raise Exception()\n\n    def __call__(self, inputs):\n        if self.dilations is None:\n            self.dilations = [1, 2, 4, 8, 16, 32]\n        x = inputs\n        x = Convolution1D(self.nb_filters, 1, padding=self.padding)(x)\n        skip_connections = []\n        for s in range(self.nb_stacks):\n            for i in self.dilations:\n                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n                                             self.kernel_size, self.padding, self.dropout_rate)\n                skip_connections.append(skip_out)\n        if self.use_skip_connections:\n            x = L.add(skip_connections)\n        x = Activation('relu')(x)\n\n        if not self.return_sequences:\n            output_slice_index = -1\n            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n        return x\n","f6ab03d9":"# custom loss_fnc\ndef MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\ndef gru_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.GRU(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer = 'orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.LSTM(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer = 'orthogonal'))\n\n\ndef build_model(seq_len=107, pred_len=68,embed_dim=100,units=128,dropout=0.4):\n    \n    inputs =Input(shape=(seq_len, 3))\n    \n    embed = Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n    reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    x = SpatialDropout1D(.2)(reshaped)\n    \n    x1 = TCN(2*units, return_sequences=True, dilations = [1, 2, 4, 8, 16,32])(x) #, activation = 'wavenet'\n    x2 = K.reverse(x,axes=1)\n    x2 = TCN(2*units, return_sequences=True, dilations = [1, 2, 4, 8, 16,32])(x2) #,dilations = [1, 2, 4]\n    x =add([x1,K.reverse(x2,axes=1)])\n    x1 = TCN(2*units, return_sequences=True, dilations = [1, 2, 4, 8, 16,32])(x) #, activation = 'wavenet'\n    x2 = K.reverse(x,axes=1)\n    x2 = TCN(2*units, return_sequences=True, dilations = [1, 2, 4, 8, 16,32])(x2) #,dilations = [1, 2, 4]\n    hidden =add([x1,K.reverse(x2,axes=1)])\n    \n    hidden = lstm_layer(units, dropout)(hidden)\n    #hidden = gru_layer(units, dropout)(hidden)\n    truncated = hidden[:, :pred_len]\n    \n    out = Dense(5, activation='linear')(truncated)\n    model = Model(inputs=inputs, outputs=out)\n    adam = tf.optimizers.Adam()\n    model.compile(optimizer=adam, loss=MCRMSE)\n\n\n    return model","66918a3f":"plot_model(build_model(),show_shapes=True)","2932ea88":"def train_and_predict(n_folds=5, model_name=\"model\", epochs=90, debug=True):\n\n    print(\"Model:\", model_name)\n\n    ensemble_preds = pd.DataFrame(index=sample_sub.index, columns=target_cols).fillna(0) # test dataframe with 0 values\n    kf = KFold(n_folds, shuffle=True, random_state=42)\n    skf = StratifiedKFold(n_folds, shuffle=True, random_state=42)\n    val_losses = []\n    historys = []\n\n    for i, (train_index, val_index) in enumerate(skf.split(train_inputs_all, train['SN_filter'])):\n        print(\"Fold:\", str(i+1))\n\n        model_train = build_model()\n        model_short = build_model(seq_len=107, pred_len=107)\n        model_long = build_model(seq_len=130, pred_len=130)\n\n        train_inputs, train_labels = train_inputs_all[train_index], train_labels_all[train_index]\n        val_inputs, val_labels = train_inputs_all[val_index], train_labels_all[val_index]\n\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{model_name}.h5')\n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=2)\n\n        history = model_train.fit(\n            train_inputs , train_labels, \n            validation_data=(val_inputs,val_labels),\n            batch_size=64,\n            epochs=epochs, # changed 70\n            callbacks=[tf.keras.callbacks.ReduceLROnPlateau(),checkpoint,early_stopping],\n            verbose=2 if debug else 0\n        )\n\n        print(f\"{model_name} Min training loss={min(history.history['loss'])}, min validation loss={min(history.history['val_loss'])}\")\n\n        val_losses.append(min(history.history['val_loss']))\n        historys.append(history)\n\n        model_short.load_weights(f'{model_name}.h5')\n        model_long.load_weights(f'{model_name}.h5')\n\n        public_preds = model_short.predict(public_inputs)\n        private_preds = model_long.predict(private_inputs)\n\n        preds_model = []\n        for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n            for i, uid in enumerate(df.id):\n                single_pred = preds[i]\n\n                single_df = pd.DataFrame(single_pred, columns=target_cols)\n                single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n                preds_model.append(single_df)\n\n        preds_model_df = pd.concat(preds_model)\n        ensemble_preds[target_cols] += preds_model_df[target_cols].values \/ n_folds\n\n        if debug:\n            print(\"Intermediate ensemble result\")\n            print(ensemble_preds[target_cols].head())\n\n    ensemble_preds[\"id_seqpos\"] = preds_model_df[\"id_seqpos\"].values\n    ensemble_preds = pd.merge(sample_sub[\"id_seqpos\"], ensemble_preds, on=\"id_seqpos\", how=\"left\")\n\n    print(\"Mean Validation loss:\", str(np.mean(val_losses)))\n\n    if debug:\n        fig, ax = plt.subplots(1, 1, figsize = (10, 5))\n        for i, history in enumerate(historys):\n            ax.plot(history.history['loss'])\n            ax.plot(history.history['val_loss'])\n            ax.set_title('model_'+str(i+1))\n            ax.set_ylabel('Loss')\n            ax.set_xlabel('Epoch')\n        plt.show()\n\n    return ensemble_preds\n","854ed898":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)\nensembles = []\nmodel_name = \"model\"\n\nensemble_final = train_and_predict(n_folds=5, model_name=model_name, epochs=100)\nprint(ensemble_final)","c8aad86b":"ensemble_final.to_csv('ensemble_final.csv', index=False)","389d47dd":"Codes are forked from https:\/\/www.kaggle.com\/gandagorn\/gru-lstm-mix-with-custom-loss\n\nModel is from https:\/\/github.com\/philipperemy\/keras-tcn\n\nSingle TCN doesn't work well with only 0.4 on public LB. So I use Two-layer Bidirectional TCN followed by LSTM or GRU.\n\nAdding TCN makes model easier to converge but no improvement on the local validation set.\n"}}