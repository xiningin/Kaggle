{"cell_type":{"7cd9d6cb":"code","8f6dba58":"code","13846d3e":"code","c1b280cd":"code","0d9c7f16":"code","27efc2df":"code","52ea8ad5":"code","8427ba22":"code","6af20a46":"code","529af7b2":"code","3e50e05b":"code","8f5369d1":"code","e2e1d2b4":"code","83c21980":"code","3309f107":"code","317a35fb":"code","5205e405":"code","1d78c2d9":"code","1e4bab60":"code","49ec7d28":"code","cd1277cf":"code","2e7affaa":"code","45912109":"code","48ce93dd":"code","93fd14e6":"code","f60fb113":"code","b07f8936":"code","6aa41b67":"code","c6769837":"code","8494e7c2":"code","794bee10":"code","12c15fe5":"code","f5d488ed":"code","075182e5":"code","97102e5e":"code","4a767b9f":"code","cea252e0":"code","8671ab02":"code","cd1100a5":"code","4840c7e2":"code","004010e0":"code","f3678902":"code","2af12ec0":"code","002d22a8":"code","57b761b0":"code","72b1446d":"code","10461536":"code","c57092d1":"code","15bb2b46":"code","f47b1574":"code","54219d91":"code","54a4dec0":"code","f3b0e9d5":"code","7b3146d7":"code","21f3d4a4":"code","8d8ee442":"code","fed74331":"code","c70fc0b5":"markdown","ad2b07e8":"markdown","951f0b84":"markdown","cb54e40f":"markdown","e42a63d6":"markdown","19be9454":"markdown","ce1de00b":"markdown","5ee57514":"markdown","e3adca88":"markdown","80fb9dec":"markdown","5f9cf835":"markdown","2870cd80":"markdown","1ca9a797":"markdown","38321430":"markdown","cc29924e":"markdown","ae47c5bb":"markdown","8f76ff8e":"markdown","a99cb4b8":"markdown","ca6155b1":"markdown","8ad739b1":"markdown","0cbd18d8":"markdown","b1241b13":"markdown","cdc7708b":"markdown","f95f21a5":"markdown","8e92e1f9":"markdown","27349884":"markdown","945c75c2":"markdown","9903d4b1":"markdown","9cc8cb2d":"markdown","425008ef":"markdown","6ba31c61":"markdown","1b709cfa":"markdown","22535a8f":"markdown","eecfff79":"markdown","9c1a98e4":"markdown","dbfe1dd9":"markdown","cfaa9508":"markdown","d7409417":"markdown","c910a5a7":"markdown","0095f1d9":"markdown","4f367274":"markdown"},"source":{"7cd9d6cb":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# to divide train and test set\nfrom sklearn.model_selection import train_test_split\n\n# feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\n# for tree binarisation\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\n\n\n# to build the models\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb\n\n# to evaluate the models\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\npd.pandas.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.filterwarnings('ignore')","8f6dba58":"# load dataset\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint(data.shape)\ndata.head()","13846d3e":"# Load the dataset for submission (the one on which our model will be evaluated by Kaggle)\n# it contains exactly the same variables, but not the target\n\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsubmission.head()","c1b280cd":"# let's inspect the type of variables in pandas\ndata.dtypes","0d9c7f16":"# we also have an Id variable, that we shoulld not use for predictions:\n\nprint('Number of House Id labels: ', len(data.Id.unique()))\nprint('Number of Houses in the Dataset: ', len(data))","27efc2df":"# find categorical variables\ncategorical = [var for var in data.columns if data[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))","52ea8ad5":"# make a list of the numerical variables first\nnumerical = [var for var in data.columns if data[var].dtype!='O']\n\n# list of variables that contain year information\nyear_vars = [var for var in numerical if 'Yr' in var or 'Year' in var]\n\nyear_vars","8427ba22":"data[year_vars].head()","6af20a46":"data.groupby('MoSold')['SalePrice'].median().plot()\nplt.title('House price variation in the year')\nplt.ylabel('mean House price')","529af7b2":"# let's visualise the values of the discrete variables\ndiscrete = []\n\nfor var in numerical:\n    if len(data[var].unique())<20 and var not in year_vars:\n        print(var, ' values: ', data[var].unique())\n        discrete.append(var)\nprint()\nprint('There are {} discrete variables'.format(len(discrete)))","3e50e05b":"# find continuous variables\n# let's remember to skip the Id variable and the target variable SalePrice, which are both also numerical\n\nnumerical = [var for var in numerical if var not in discrete and var not in ['Id', 'SalePrice'] and var not in year_vars]\nprint('There are {} numerical and continuous variables'.format(len(numerical)))","8f5369d1":"# let's visualise the percentage of missing values for each variable\nfor var in data.columns:\n    if data[var].isnull().sum()>0:\n        print(var, data[var].isnull().mean())","e2e1d2b4":"# let's now determine how many variables we have with missing information\n\nvars_with_na = [var for var in data.columns if data[var].isnull().sum()>0]\nprint('Total variables that contain missing information: ', len(vars_with_na))","83c21980":"# let's inspect the type of those variables with a lot of missing information\nfor var in data.columns:\n    if data[var].isnull().mean()>0.80:\n        print(var, data[var].unique())","3309f107":"# let's look at the numerical variables\nnumerical","317a35fb":"# let's make boxplots to visualise outliers in the continuous variables \n# and histograms to get an idea of the distribution\n\nfor var in numerical:\n    plt.figure(figsize=(15,6))\n    plt.subplot(1, 2, 1)\n    fig = data.boxplot(column=var)\n    fig.set_title('')\n    fig.set_ylabel(var)\n    \n    plt.subplot(1, 2, 2)\n    fig = data[var].hist(bins=20)\n    fig.set_ylabel('Number of houses')\n    fig.set_xlabel(var)\n\n    plt.show()","5205e405":"# outlies in discrete variables\nfor var in discrete:\n    (data.groupby(var)[var].count() \/ np.float(len(data))).plot.bar()\n    plt.ylabel('Percentage of observations per label')\n    plt.title(var)\n    plt.show()\n    #print(data[var].value_counts() \/ np.float(len(data)))\n    print()","1d78c2d9":"no_labels_ls = []\nfor var in categorical:\n    no_labels_ls.append(len(data[var].unique()))\n    \n \ntmp = pd.Series(no_labels_ls)\ntmp.index = pd.Series(categorical)\ntmp.plot.bar(figsize=(12,8))\nplt.title('Number of categories in categorical variables')\nplt.xlabel('Categorical variables')\nplt.ylabel('Number of different categories')","1e4bab60":"# Let's separate into train and test set\n\nX_train, X_test, y_train, y_test = train_test_split(data, data.SalePrice, test_size=0.1,\n                                                    random_state=0)\nX_train.shape, X_test.shape","49ec7d28":"# function to calculate elapsed time\n\ndef elapsed_years(df, var):\n    # capture difference between year variable and year the house was sold\n    df[var] = df['YrSold'] - df[var]\n    return df","cd1277cf":"for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    X_train = elapsed_years(X_train, var)\n    X_test = elapsed_years(X_test, var)\n    submission = elapsed_years(submission, var)","2e7affaa":"X_train[['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']].head()","45912109":"# drop YrSold\nX_train.drop('YrSold', axis=1, inplace=True)\nX_test.drop('YrSold', axis=1, inplace=True)\nsubmission.drop('YrSold', axis=1, inplace=True)","48ce93dd":"# print variables with missing data\n# keep in mind that now that we created those new temporal variables, we\n# are going to treat them as numerical and continuous as well:\n\n# remove YrSold because it is no longer in our dataset\nyear_vars.remove('YrSold')\n\n# examine percentage of missing values\nfor col in numerical+year_vars:\n    if X_train[col].isnull().mean()>0:\n        print(col, X_train[col].isnull().mean())","93fd14e6":"# add variable indicating missingness + median imputation\nfor df in [X_train, X_test, submission]:\n    for var in ['LotFrontage', 'GarageYrBlt']:\n        df[var+'_NA'] = np.where(df[var].isnull(), 1, 0)\n        df[var].fillna(X_train[var].median(), inplace=True) \n\nfor df in [X_train, X_test, submission]:\n    df['MasVnrArea'].fillna(X_train.MasVnrArea.median(), inplace=True)","f60fb113":"# print variables with missing data\nfor col in discrete:\n    if X_train[col].isnull().mean()>0:\n        print(col, X_train[col].isnull().mean())","b07f8936":"# print variables with missing data\nfor col in categorical:\n    if X_train[col].isnull().mean()>0:\n        print(col, X_train[col].isnull().mean())","6aa41b67":"# add label indicating 'Missing' to categorical variables\n\nfor df in [X_train, X_test, submission]:\n    for var in categorical:\n        df[var].fillna('Missing', inplace=True)","c6769837":"# check absence of null values\nfor var in X_train.columns:\n    if X_train[var].isnull().sum()>0:\n        print(var, X_train[var].isnull().sum())","8494e7c2":"# check absence of null values\nfor var in X_train.columns:\n    if X_test[var].isnull().sum()>0:\n        print(var, X_test[var].isnull().sum())","794bee10":"# check absence of null values\nsubmission_vars = []\nfor var in X_train.columns:\n    if var!='SalePrice' and submission[var].isnull().sum()>0:\n        print(var, submission[var].isnull().sum())\n        submission_vars.append(var)","12c15fe5":"# Fill NA with median value for those variables that show NA only in the submission set\n\nfor var in submission_vars:\n    submission[var].fillna(X_train[var].median(), inplace=True)","f5d488ed":"def tree_binariser(var):\n    score_ls = [] # here I will store the mse\n\n    for tree_depth in [1,2,3,4]:\n        # call the model\n        tree_model = DecisionTreeRegressor(max_depth=tree_depth)\n\n        # train the model using 3 fold cross validation\n        scores = cross_val_score(tree_model, X_train[var].to_frame(), y_train, cv=3, scoring='neg_mean_squared_error')\n        score_ls.append(np.mean(scores))\n\n    # find depth with smallest mse\n    depth = [1,2,3,4][np.argmin(score_ls)]\n    #print(score_ls, np.argmin(score_ls), depth)\n\n    # transform the variable using the tree\n    tree_model = DecisionTreeRegressor(max_depth=depth)\n    tree_model.fit(X_train[var].to_frame(), X_train.SalePrice)\n    X_train[var] = tree_model.predict(X_train[var].to_frame())\n    X_test[var] = tree_model.predict(X_test[var].to_frame())\n    submission[var] =  tree_model.predict(submission[var].to_frame())","075182e5":"for var in numerical:\n    tree_binariser(var)","97102e5e":"X_train[numerical].head()","4a767b9f":"# let's explore how many different buckets we have now among our engineered continuous variables\nfor var in numerical:\n    print(var, len(X_train[var].unique()))","cea252e0":"for var in numerical:\n    X_train.groupby(var)['SalePrice'].mean().plot.bar()\n    plt.title(var)\n    plt.ylabel('Mean House Price')\n    plt.xlabel('Discretised continuous variable')\n    plt.show()","8671ab02":"def rare_imputation(variable):\n    # find frequent labels \/ discrete numbers\n    temp = X_train.groupby([variable])[variable].count()\/np.float(len(X_train))\n    frequent_cat = [x for x in temp.loc[temp>0.03].index.values]\n    \n    X_train[variable] = np.where(X_train[variable].isin(frequent_cat), X_train[variable], 'Rare')\n    X_test[variable] = np.where(X_test[variable].isin(frequent_cat), X_test[variable], 'Rare')\n    submission[variable] = np.where(submission[variable].isin(frequent_cat), submission[variable], 'Rare')","cd1100a5":"# the following vars in the submission dataset are encoded in different types\n# so first I cast them as int, like in the train set\n\nfor var in ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars']:\n    submission[var] = submission[var].astype('int')","4840c7e2":"# find infrequent labels in categorical variables and replace by Rare\nfor var in categorical:\n    rare_imputation(var)\n    \n# find infrequent labels in categorical variables and replace by Rare\n# remember that we are treating discrete variables as if they were categorical\nfor var in discrete:\n    rare_imputation(var)","004010e0":"# check that we haven't created missing values in the submission dataset\nfor var in X_train.columns:\n    if var!='SalePrice' and submission[var].isnull().sum()>0:\n        print(var, submission[var].isnull().sum())\n        submission_vars.append(var)","f3678902":"# let's check that it worked\nfor var in categorical:\n    (X_train.groupby(var)[var].count() \/ np.float(len(X_train))).plot.bar()\n    plt.ylabel('Percentage of observations per label')\n    plt.title(var)\n    plt.show()","2af12ec0":"# let's check that it worked\nfor var in discrete:\n    (X_train.groupby(var)[var].count() \/ np.float(len(X_train))).plot.bar()\n    plt.ylabel('Percentage of observations per label')\n    plt.title(var)\n    plt.show()","002d22a8":"def encode_categorical_variables(var, target):\n        # make label to price dictionary\n        ordered_labels = X_train.groupby([var])[target].mean().to_dict()\n        \n        # encode variables\n        X_train[var] = X_train[var].map(ordered_labels)\n        X_test[var] = X_test[var].map(ordered_labels)\n        submission[var] = submission[var].map(ordered_labels)\n\n# encode labels in categorical vars\nfor var in categorical:\n    encode_categorical_variables(var, 'SalePrice')\n    \n# encode labels in discrete vars\nfor var in discrete:\n    encode_categorical_variables(var, 'SalePrice')","57b761b0":"# sanity check: let's see that we did not introduce NA by accident\nfor var in X_train.columns:\n    if var!='SalePrice' and submission[var].isnull().sum()>0:\n        print(var, submission[var].isnull().sum())","72b1446d":"#let's inspect the dataset\nX_train.head()","10461536":"X_train.describe()","c57092d1":"# let's create a list of the training variables\ntraining_vars = [var for var in X_train.columns if var not in ['Id', 'SalePrice']]\n\nprint('total number of variables to use for training: ', len(training_vars))","15bb2b46":"training_vars","f47b1574":"# fit scaler\nscaler = MinMaxScaler() # create an instance\nscaler.fit(X_train[training_vars]) #  fit  the scaler to the train set for later use","54219d91":"xgb_model = xgb.XGBRegressor()\n\neval_set = [(X_test[training_vars], np.log(y_test))]\nxgb_model.fit(X_train[training_vars], np.log(y_train), eval_set=eval_set, verbose=False)\n\npred = xgb_model.predict(X_train[training_vars])\nprint('xgb train mse: {}'.format(mean_squared_error(y_train, np.exp(pred))))\nprint('xgb train rmse: {}'.format(sqrt(mean_squared_error(y_train, np.exp(pred)))))\nprint()\npred = xgb_model.predict(X_test[training_vars])\nprint('xgb test mse: {}'.format(mean_squared_error(y_test, np.exp(pred))))\nprint('xgb test rmse: {}'.format(sqrt(mean_squared_error(y_test, np.exp(pred)))))","54a4dec0":"rf_model = RandomForestRegressor(n_estimators=800, max_depth=6)\nrf_model.fit(X_train[training_vars], np.log(y_train))\n\npred = rf_model.predict(X_train[training_vars])\nprint('rf train mse: {}'.format(mean_squared_error(y_train, np.exp(pred))))\nprint('rf train rmse: {}'.format(sqrt(mean_squared_error(y_train, np.exp(pred)))))\n\nprint()\npred = rf_model.predict(X_test[training_vars])\nprint('rf test mse: {}'.format(mean_squared_error(y_test, np.exp(pred))))\nprint('rf test rmse: {}'.format(sqrt(mean_squared_error(y_test, np.exp(pred)))))","f3b0e9d5":"SVR_model = SVR()\nSVR_model.fit(scaler.transform(X_train[training_vars]), np.log(y_train))\n\npred = SVR_model.predict(X_train[training_vars])\nprint('SVR train mse: {}'.format(mean_squared_error(y_train, np.exp(pred))))\nprint('SVR train rmse: {}'.format(sqrt(mean_squared_error(y_train, np.exp(pred)))))\n\nprint()\npred = SVR_model.predict(X_test[training_vars])\nprint('SVR test mse: {}'.format(mean_squared_error(y_test, np.exp(pred))))\nprint('SVR test rmse: {}'.format(sqrt(mean_squared_error(y_test, np.exp(pred)))))","7b3146d7":"lin_model = Lasso(random_state=2909, alpha=0.005)\nlin_model.fit(scaler.transform(X_train[training_vars]), np.log(y_train))\n\npred = lin_model.predict(scaler.transform(X_train[training_vars]))\nprint('Lasso Linear Model train mse: {}'.format(mean_squared_error(y_train, np.exp(pred))))\nprint('Lasso Linear Model train rmse: {}'.format(sqrt(mean_squared_error(y_train, np.exp(pred)))))\n\nprint()\npred = lin_model.predict(scaler.transform(X_test[training_vars]))\nprint('Lasso Linear Model test mse: {}'.format(mean_squared_error(y_test, np.exp(pred))))\nprint('Lasso Linear Model test rmse: {}'.format(sqrt(mean_squared_error(y_test, np.exp(pred)))))","21f3d4a4":"# make predictions for the submission dataset\nfinal_pred = pred = lin_model.predict(scaler.transform(submission[training_vars]))","8d8ee442":"temp = pd.concat([submission.Id, pd.Series(np.exp(final_pred))], axis=1)\ntemp.columns = ['Id', 'SalePrice']\ntemp.head()","fed74331":"temp.to_csv('submit_housesale.csv', index=False)","c70fc0b5":"There are a few variables in the submission dataset, that did not show NA in the training dataset. For these variables, and to be able to score them using machine learning algorithms, I will fill the NA with the median value.","ad2b07e8":"This model shows some over-fitting. Compare the rmse for train and test.","951f0b84":"### Load Datasets","cb54e40f":"#### Support vector machine","e42a63d6":"#### Regularised linear regression","19be9454":"#### Find temporal variables\n\nThere are a few variables in the dataset that are temporal. They indicate the year in which something happened. We shouldn't use these variables straightaway for model building. We should instead transform them to capture some sort of time information. Let's inspect these temporal variables:\n","ce1de00b":"We can see that because, we used the SalePrice target  to encode both our numerical continuous and discrete and categorical variables, all our variables show the mean house price as mean value. The standard deviation however, varies, following the nature of the original variable.","5ee57514":"#### Outliers in discrete variables\n\nNow, let's identify outliers in numerical discrete variables. I will call outliers, those values that are present in less than 1% of the houses. This is exactly the same as finding rare labels in categorical variables. Discrete variables, in essence can be pre-processed \/ engineered as if they were categorical. Keep this in mind.","e3adca88":"Most of the discrete variables show values that are shared by a tiny proportion of houses in the dataset. For linear regression, this may not be a problem, but it most likely will be for tree methods.\n\n\n#### Number of labels: cardinality\n\nLet's go ahead now and examine our categorical variables. First I will determine whether they show high cardinality. This is, a high number of labels.","80fb9dec":"We can see that the mean House Price value increases with the value of the bucket. This means we managed to create a monotonic distribution between the numerical variable and the target.","5f9cf835":"The scaler is now ready, we can use it in a machine learning algorithm when required. See below.\n\n### Machine Learning algorithm building\n\n**Note**\n\nThe distribution of SalePrice is also skewed, so I will fit the model to the log transformation of the house price.\n\nThen, to evaluate the models, we need to convert it back to prices.\n\n#### xgboost","2870cd80":"- LotFrontage and GarageYrBlt: These variables show more than 5% NA, so I create **additional variable with NA + median imputation**\n- CMasVnrArea: Less than 5% NA so: **median imputation**","1ca9a797":"The variables that contain a lot of missing data are categorical variables. We will need to fill those out later in the feature engineering section.\n\n#### Outliers","38321430":"Most of the variables, contain only a few labels. Then, we do not have to deal with high cardinality. That is good news!","cc29924e":"There are quite a few variables with missing information. And they differ in the percentage of observations for which information are missing. \nLet's go ahead and inspect those variables that show missing information for most of their observations.","ae47c5bb":"\n### Predicting Sale Price of Houses\n\nThe problem at hand aims to predict the final sale price of homes based on different explanatory variables describing aspects of residential homes. Predicting house prices is useful to identify fruitful investments, or to determine whether the price advertised for a house is over or underestimated, before making a buying judgment.\n\nTo download the House Price dataset go this website:\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\nScroll down to the bottom of the page, and click on the link 'train.csv', and then click the 'download' blue button towards the right of the screen, to download the dataset.\nSave it to a directory of your choice.\n\nFor the Kaggle submission, download also the 'test.csv' file, which is the one we need to score and submit to Kaggle. Rename the file to 'house_price_submission.csv'\n\n**Note that you need to be logged in to Kaggle in order to download the datasets**.\n\nIf you save it in the same directory from which you are running this notebook and name the file 'houseprice.csv' then you can load it the same way I will load it below.\n\n====================================================================================================","8f76ff8e":"#### Discrete variables","a99cb4b8":"### Engineering rare labels in categorical and discrete variables ","ca6155b1":"We can see that the labels have now been replaced by the mean house price.\n\n### Feature scaling ","8ad739b1":"The House Price dataset is bigger than the Titanic dataset. It contains more variables for each one of the houses. Thus, manual inspection of each one of them is a bit time demanding. Therefore, here instead of deciding variable by variable what is the best way to proceed, I will try to automate the feature engineering pipeline, making some a priori decisions on when I will apply one technique or the other, and then expanding them to the entire dataset.","0cbd18d8":"The majority of the continuous variables seem to contain outliers. In addition, the majority of the variables are not normally distributed. If we are planning to build linear regression, we might need to tackle these to improve the model performance. To tackle the 2 aspects together, I will do discretisation. And in particular, I will use trees to find the right buckets onto which I will divide the variables.","b1241b13":"### Outliers in Numerical variables (15)\n\nIn order to tackle outliers and skewed distributions at the same time, I suggested I would do discretisation. And in order to find the optimal buckets automatically, I would use decision trees to find the buckets for me.","cdc7708b":"## House Prices dataset","f95f21a5":"I will add a 'Missing' Label to all of them. If the missing data are rare, I will handle those together with rare labels in a subsequent engineering step.","8e92e1f9":"### Separate train and test set","27349884":"### Engineering missing values in numerical variables \n#### Continuous variables","945c75c2":"The best model is the Lasso, so I will submit only that one for Kaggle.\n\n### Submission to Kaggle","9903d4b1":"Instead of years, now we have the amount of years passed since the house was built or remodeled and the house was sold. Next, we drop the YrSold variable from the datasets, because we already extracted its value.","9cc8cb2d":"Fantastic, we have replaced infrequent labels in both categorical and numerical variables. We see the presence of the label rare in both!\n\n### Encode categorical and discrete variables \n\nI will use target encoding for categorical variables. This way, the labels will be replaced by the mean of the SalePrice, and will remain in a similar scale to the one that now show our numerical variables.","425008ef":"### Types of variables \n\nLet's go ahead and find out what types of variables there are in this dataset","6ba31c61":"Id is a unique identifier for each of the houses. Thus this is not a variable that we can use.\n\n#### Find categorical variables","1b709cfa":"#### Continuous variables","22535a8f":"This model shows some over-fitting. Compare the rmse for train and test.","eecfff79":"### Bespoke feature engineering\n\nFirst, let's extract information from temporal variables.\n#### Temporal variables\n\nFirst, we will create those temporal variables we discussed a few cells ago","9c1a98e4":"The price seems to vary depending on the time of the year in which the house is sold. This information will be captured when we engineer this variable later on.\n\n\n#### Find discrete variables\n\nTo identify discrete variables, I will select from all the numerical ones, those that contain a finite and small number of distinct values. See below.","dbfe1dd9":"There are a mixture of categorical and numerical variables. Numerical are those of type int and float and categorical those of type object.","cfaa9508":"#### Random Forests","d7409417":"Perfect!! Now we have inspected and have a view of the different types of variables that we have in the house price dataset. Let's move on to understand the types of problems that these variables have.\n\n### Types of problems within the variables (section 3)\n\n#### Missing values","c910a5a7":"**Now we will move on and engineer the features of this dataset. The most important part for this course.**","0095f1d9":"There are no missing data in the discrete variables. Good, then we don't have to engineer them.\n\n### Engineering Missing Data in categorical variables ","4f367274":"We can see that these variables correspond to the years in which the houses were built or remodeled or a garage was built, or the house was indeed sold. It would be better if we captured the time elapsed between the time the house was built and the time the house was sold for example. We are going to do that in the feature engineering section. \n\nWe have another temporal variable: MoSold, which indicates the month in which the house was sold. Let's inspect if the house price varies with the time of the year in which it is sold:"}}