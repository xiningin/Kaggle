{"cell_type":{"67453a1b":"code","6320d29c":"code","8a3b9b97":"code","e9e9c089":"code","846a6464":"code","853c1301":"code","6da55c33":"code","8f38b5fe":"code","146923a0":"code","1802917e":"code","42862b60":"code","960f6459":"code","987f54f5":"code","fd6043ee":"code","3ab93902":"markdown"},"source":{"67453a1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\n","6320d29c":"'''Trains a Siamese MLP on pairs of digits from the MNIST dataset.\nIt follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\noutput of the shared network and by optimizing the contrastive loss (see paper\nfor mode details).\n# References\n- Dimensionality Reduction by Learning an Invariant Mapping\n    http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/hadsell-chopra-lecun-06.pdf\nGets to 97.2% test accuracy after 20 epochs.\n2 seconds per epoch on a Titan X Maxwell GPU\n'''\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport random\nfrom keras.models import Model\nfrom keras.layers import Input, Flatten, Dense, Dropout, Lambda\nfrom keras.optimizers import RMSprop\nfrom keras import backend as K\n\nnum_classes = 10\nepochs = 10\n\n\ndef euclidean_distance(vects):\n    x, y = vects\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n\n\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)\n\n\ndef contrastive_loss(y_true, y_pred):\n    '''Contrastive loss from Hadsell-et-al.'06\n    http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/hadsell-chopra-lecun-06.pdf\n    '''\n    margin = 1\n    sqaure_pred = K.square(y_pred)\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\n    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n\n\ndef create_pairs(x, digit_indices):\n    '''Positive and negative pair creation.\n    Alternates between positive and negative pairs.\n    '''\n    pairs = []\n    labels = []\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n    for d in range(num_classes):\n        for i in range(n):\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n            pairs += [[x[z1], x[z2]]]\n            inc = random.randrange(1, num_classes)\n            dn = (d + inc) % num_classes\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n            pairs += [[x[z1], x[z2]]]\n            labels += [1, 0]\n    return np.array(pairs), np.array(labels)\n\n\ndef create_base_network(input_shape):\n    '''Base network to be shared (eq. to feature extraction).\n    '''\n    input = Input(shape=input_shape)\n    x = Flatten()(input)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n    x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n    return Model(input, x)\n\n\ndef compute_accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    pred = y_pred.ravel() < 0.5\n    return np.mean(pred == y_true)\n\n\ndef accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))","8a3b9b97":"# load and reshape data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain_labels =train['label']\ntrain_images =train.drop(['label'],axis=1)\n\ntrain_data = np.array(train_images).reshape(-1,28,28)\ntrain_label = np.array(train_labels)\ntest_x = np.array(test).reshape(-1,28,28)","e9e9c089":"# the data, split between train and test sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, shuffle=True, random_state=1)\nprint(\"Train data shape: {}.\".format(x_train.shape))\nprint(\"Test data shape {}.\".format(test_x.shape))\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train \/= 255\nx_test \/= 255\ntest_x = test_x\/255\ninput_shape = x_train.shape[1:]","846a6464":"# create training+test positive and negative pairs\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\n\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\nte_pairs, te_y = create_pairs(x_test, digit_indices)","853c1301":"# network definition\nbase_network = create_base_network(input_shape)\n\ninput_a = Input(shape=input_shape)\ninput_b = Input(shape=input_shape)\n\n# because we re-use the same instance `base_network`,\n# the weights of the network\n# will be shared across the two branches\nprocessed_a = base_network(input_a)\nprocessed_b = base_network(input_b)\n\ndistance = Lambda(euclidean_distance,\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n\nmodel = Model([input_a, input_b], distance)\n\n# train\nrms = RMSprop()\n#rms = Adam()\n#rms = SGD()\n\nmodel.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n# train\nhistory = model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n          batch_size=128,\n          epochs=epochs,\n          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n\n# compute final accuracy on training and test sets\ny_pred_tr = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\ntr_acc = compute_accuracy(tr_y, y_pred_tr)\ny_pred_te = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\nte_acc = compute_accuracy(te_y, y_pred_te)\n\nprint('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\nprint('* Accuracy on test set: %0.2f%%' % (100 * te_acc))","6da55c33":"# LOSS Learning curves\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, (len(history.history['val_accuracy']) + 1))\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","8f38b5fe":"# ACCURACY Learning Curves\n\nhistory_dict = history.history\nloss_values = history_dict['accuracy']\nval_loss_values = history_dict['val_accuracy']\nepochs = range(1, (len(history.history['accuracy']) + 1))\nplt.plot(epochs, loss_values, 'bo', label='Training Acc')\nplt.plot(epochs, val_loss_values, 'b', label='Validation Acc')\nplt.title('Training and validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","146923a0":"# create test pairs\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\nexample_numbers = 10\nexample_indexes = [np.random.choice((digit_indices[i]), example_numbers, replace=False) for i in range(len(digit_indices))]","1802917e":"predictions =[]\nfor sub in range(0,len(test_x)):\n    image_sub = test_x[sub].reshape(1,28,28)\n    pred_sum = []\n    for i in range(0,num_classes):\n        raw_prediction = []\n        for exam in range(0,example_numbers):\n            example_image = x_train[example_indexes[i][exam]].reshape(1,28,28)\n            prd = np.squeeze(model.predict([image_sub,example_image]))\n            raw_prediction.append(prd)\n        pred_sum.append(sum(raw_prediction)) \n    prediction = [np.argmin(pred_sum),min(pred_sum)]\n    predictions.append(prediction[0])","42862b60":"# SUBMISSION\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\n#print(sample_submission.shape)\nresult=pd.DataFrame({'ImageId':sample_submission.ImageId, 'Label':predictions})\nresult.to_csv(\"submission.csv\",index=False)\nprint(result)","960f6459":"# Plot the representation learned from the siamese network\n#embedding_model = model.layers[2]\n#embeddings = embedding_model.predict(x_train)\nembeddings = base_network.predict(x_train)","987f54f5":"from sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2,random_state=10).fit_transform(embeddings)","fd6043ee":"mnist_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n              '#bcbd22', '#17becf']\n\nplt.figure(figsize=(10,10))\nfor i in range(10):\n    inds = np.where(y_train==i)[0]\n    plt.scatter(X_embedded[inds,0], X_embedded[inds,1], alpha=0.5, color=colors[i])\nplt.legend(mnist_classes)","3ab93902":"*  Applied siamese NN from the Keras examples to the Kaggle MNIST dataset.\n\n*  Added simple prediction based on the distance from the 10 random samples of every class.\n\n*  Added NN embeddings visualisation."}}