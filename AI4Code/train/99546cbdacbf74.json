{"cell_type":{"0b0ad0d1":"code","e1e658f2":"code","d470150c":"code","39d4ab20":"code","0088fa05":"code","bc54c755":"code","8348ae64":"code","08e24ddf":"code","84cf3be7":"code","9ee85466":"code","98ef1221":"code","f80f7035":"code","97899074":"code","49c3afd3":"code","a3bbf6ee":"code","05dfb94e":"code","c3c06bce":"code","32788d82":"code","d162ec62":"code","0af0f94c":"code","62ad9863":"code","648cd42d":"code","bfbe611c":"markdown","3cb7d644":"markdown","fa4b058c":"markdown","378fb507":"markdown","b0e4998e":"markdown","506624c3":"markdown","85844473":"markdown","7a928485":"markdown","df68730a":"markdown","7798d1da":"markdown","bb6c5d6c":"markdown","a2f2a50b":"markdown","26c15cfb":"markdown","49669039":"markdown","6c65888d":"markdown","aafd34a4":"markdown","8b00fd9b":"markdown","55055d61":"markdown","de6a7680":"markdown","6a9f2255":"markdown","b5efe6af":"markdown","2269ab43":"markdown","8958931d":"markdown","a5b6f782":"markdown","4246c475":"markdown","a00d8ef2":"markdown","66a8b051":"markdown","63d1429c":"markdown"},"source":{"0b0ad0d1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn \n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","e1e658f2":"# load dataset\nplacement = pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")","d470150c":"data = placement.copy()\ndata.head()","39d4ab20":"print (str(data.shape))\nprint (data.info())","0088fa05":"# count how many null value are there\nprint(data.isnull().sum(), sep = '\\n')","bc54c755":"data['salary'].fillna(value = 0, inplace = True)","8348ae64":"sns.countplot(x = \"hsc_b\", hue = \"status\", data = data)\nplt.show()","08e24ddf":"sns.countplot(x = \"ssc_b\", hue = \"status\", data = data)\nplt.show()","84cf3be7":"# drop these three columns\ndata.drop(['ssc_b','hsc_b', 'sl_no'], axis = 1, inplace = True) \ndata.head()","9ee85466":"plt.figure(figsize = (5, 5))\n\nplt.boxplot(data['hsc_p'])\nax.set_title('hsc_p')","98ef1221":"q1 = data['hsc_p'].quantile(0.25)\nq3 = data['hsc_p'].quantile(0.75)\ndata = data.loc[(data['hsc_p'] >= q1 - 1.5 * (q3 - q1)) & (data['hsc_p'] <= q3 + 1.5 * (q3 - q1))]\n\n# data.shape # (207, 12)\ndata.head()","f80f7035":"data[\"gender\"] = data.gender.map({\"M\":0,\"F\":1})\ndata[\"workex\"] = data.workex.map({\"No\":0, \"Yes\":1})\ndata[\"status\"] = data.status.map({\"Not Placed\":0, \"Placed\":1})\ndata[\"specialisation\"] = data.specialisation.map({\"Mkt&HR\":0, \"Mkt&Fin\":1})\n\nencoded_hsc_s = pd.get_dummies(data['hsc_s'], prefix='encoded')\nencoded_degree_t = pd.get_dummies(data['degree_t'], prefix='encoded')\ndata = pd.concat([data, encoded_hsc_s, encoded_degree_t], axis = 1)\ndata.drop(['hsc_s','degree_t','salary'], axis = 1, inplace = True)\n\ndata.head()","97899074":"from sklearn.model_selection import train_test_split\n\nX = data.drop(['status'], axis = 1)\ny = data.status\nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size = 0.7, random_state = 1)\nprint(\"X_train:\",X_train.shape)\nprint(\"X_test:\",X_test.shape)\nprint(\"y_train:\",y_train.shape)\nprint(\"y_test:\",y_test.shape)","49c3afd3":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\ndt = DecisionTreeClassifier(criterion = \"gini\", max_depth=3) # 0.7619\n# dt = DecisionTreeClassifier(criterion = \"entropy\", max_depth=3) # 0.7778\n\n# dt = DecisionTreeClassifier(criterion = \"gini\", max_depth=4) # 0.7619\n# dt = DecisionTreeClassifier(criterion = \"entropy\", max_depth=4) # 0.7619\n\n# dt = DecisionTreeClassifier(criterion = \"gini\", max_depth=5) # 0.7460\n# dt = DecisionTreeClassifier(criterion = \"entropy\", max_depth=5) # 0.7619\n\ndt = dt.fit(X_train,y_train)\ny_pred = dt.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","a3bbf6ee":"from sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = \\\n    learning_curve(dt, X, y, train_sizes=np.linspace(0.1, 1, 10), scoring=\"neg_mean_squared_error\", cv = 10)\n\nplt.plot(train_sizes, -test_scores.mean(1), 'o-', color='r')\nplt.xlabel(\"Train Size\")\nplt.ylabel(\"Mean Sqaured Error\")\nplt.title(\"Learning Curves\")\nplt.legend(loc=\"best\")","05dfb94e":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nmlp = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=400)\nmlp.fit(X_train,y_train)\n\npredict_train = mlp.predict(X_train)\npredict_test = mlp.predict(X_test)\n\nprint(confusion_matrix(y_train,predict_train))\nprint(classification_report(y_train,predict_train))\n\nprint(confusion_matrix(y_test,predict_test))\nprint(classification_report(y_test,predict_test))","c3c06bce":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nxg_reg = xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\nxg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","32788d82":"data_dmatrix = xgb.DMatrix(data=X,label=y)\nparams = {\"objective\":\"reg:logistic\",'colsample_bytree': 0.2,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 5}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\nprint((cv_results[\"test-rmse-mean\"]).tail(1))","d162ec62":"from sklearn.neighbors import KNeighborsClassifier\nerror_rate = []\n\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","0af0f94c":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","62ad9863":"from sklearn.metrics import confusion_matrix\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(classification_report(y_test, y_pred))","648cd42d":"from sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear') # 0.87 precision\n# svclassifier = SVC(kernel='rbf') # 0.79 precision\n# svclassifier = SVC(kernel='sigmoid') # 0.42 precision\n\nsvclassifier.fit(X_train, y_train)\ny_pred = svclassifier.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test,y_pred)\nprint(classification_report(y_test,y_pred))","bfbe611c":"+ 215 rows in total\n+ salary has null value, that's reasonable beacause not all students get placed","3cb7d644":"Seems k = 5 would be a good choose","fa4b058c":"the plot above shows `hsc_p` has a few outliers, it's essential to clear them up because those outliers will make our model overfit in later phases.","378fb507":"## Step 2: Data Cleaning & Preprocessing","b0e4998e":"# Assignment 1 for Gatech CS7641","506624c3":"**with Cross Validation**","85844473":"**Draw the boxplot so that we can observe outliers:**","7a928485":"the percentage that student's get placed is pretty much the same between `Others` & `Central`\n\nwhich means we can safely drop the `hsc_b` column, as it doesn't contribute too much to our model\n\nDo the same plot with other attributes, and I found `ssc_b` is also irrelavant and can be dropped:","df68730a":"### Choosing a K value ","7798d1da":"## Step 1: Get some basic info about the data","bb6c5d6c":"which give 0.7619 accuracy.\n\nDraw the learn curve:","a2f2a50b":"## Step 3: Running various ML algorithms on the problem\n\nTwo classification problems:\n\n+ Predict whether a student got placed or not\n+ Predict the gender of a student based on other attributes (university degree, salary ...)\n\nBelow I will examine the first problem. The second problem is exactly the same, except we need to change X, y to corresponding columns:\n\n```python\nX = data.drop(['gender'], axis = 1)\ny = data.gender\n```","26c15cfb":"### Error rate vs K-value","49669039":"### Plot the relationship graph between hsc_b & status","6c65888d":"which gives RMSE value of 0.577350","aafd34a4":"### Decision Tree","8b00fd9b":"### Split training set and test set\n\nI decide to use 70:30 split","55055d61":"**KNN gives 0.82 precision**","de6a7680":"**Not all columns are useful when training our model**\n\nIt's better to draw some graph to show which attribute is less important in terms of affecting placement status","6a9f2255":"### Nerual Network","b5efe6af":"which gives 0.79 percision","2269ab43":"Since not all column values are numeric, it's better to encode those objects value to a number so that our model can reason about.\n\n`gender`, `workex`, `status`, `specialisation` columns are binary and can only have two values, it's ok to represent them as `0` and `1`\n\n`hsc_s`, `degree_t`, `salary` have more than two values, so we use one hot encoding","8958931d":"Reduced the model error to **0.39** ","a5b6f782":"+ linear function gives 0.87 percision\n+ rbf function gives 0.79 precision\n+ sigmoid gives 0.42 precision","4246c475":"### Support Vector Machine","a00d8ef2":"### XGBoost","66a8b051":"As shown above, `salary` column has 67 null values, which means 67 students didn't get placed\n\nAnd it would be a good choose to replace those null values with 0:","63d1429c":"### KNN"}}