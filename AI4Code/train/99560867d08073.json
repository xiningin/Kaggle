{"cell_type":{"db2827eb":"code","d1aa4a14":"code","ef0fca81":"code","8c1c7dcc":"code","c9a3590c":"code","e703f2da":"code","34baeb59":"code","d378b195":"code","288174a1":"code","ef0b53f0":"code","4135321f":"code","7cbc84d2":"code","885d3979":"code","b279fa60":"code","339e9fd3":"code","8d411d88":"code","c452f222":"code","02ab8c4b":"code","325c5429":"code","943b9a16":"code","d3936c4e":"code","bb84e4a1":"code","1d3d2f30":"code","385d9d37":"code","38fa419c":"code","c3ac88a9":"code","82121a76":"code","bb147831":"code","02e05bea":"code","c6415c35":"code","adc223b2":"code","7d04fce7":"code","b9ae2a19":"code","b87234b1":"code","c0968846":"code","cbc7d527":"code","a86ab29f":"code","0270b24c":"code","2cd37362":"code","73e928b7":"code","329c0468":"code","daf9305d":"code","34c310f6":"code","19c5f65f":"code","250dc06e":"code","f606d47f":"code","0a3c71dc":"code","924a2614":"code","d7fbe902":"code","b67ca414":"code","de442c3d":"code","21ebce44":"code","f4955413":"code","b703c4f2":"code","d706b1fa":"code","d192a113":"code","8515699f":"code","30efa5f2":"code","7dd3a169":"code","121fa780":"code","a0185a30":"code","91c73d09":"code","915c9646":"code","2d59b05f":"code","1a970d36":"code","19d2c4fd":"markdown","0d9f34a9":"markdown","925c3be2":"markdown","c0b5c8c7":"markdown","a1874a2c":"markdown","19f6cdb1":"markdown","781e35c8":"markdown","751104d1":"markdown","0cdf3ab2":"markdown","7d345c88":"markdown","e6f4c5b2":"markdown","8d2f9737":"markdown","adff3124":"markdown","3bf96db5":"markdown","331e9217":"markdown","a873f4f4":"markdown","a10ec3c9":"markdown","85cc347f":"markdown","cea847ad":"markdown","6d680ad7":"markdown","71730751":"markdown","3023d70d":"markdown","cfbb352b":"markdown","e5417280":"markdown","d0d57749":"markdown","056f1403":"markdown","99848949":"markdown","4964a208":"markdown","43fce725":"markdown","3832bc0f":"markdown","271f92ed":"markdown"},"source":{"db2827eb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy as sp\nimport sklearn\n\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import norm, skew\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import neighbors\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import neural_network\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nimport datetime\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import confusion_matrix\npd.set_option('display.max_columns', 500)\nimport warnings\nwarnings.filterwarnings('ignore')","d1aa4a14":"train = pd.read_csv('..\/input\/train.csv')\ntrain.head()","ef0fca81":"train_x = train.drop(train.columns[1],axis = 1)\ntrain_x.head()","8c1c7dcc":"train_y = train[['Survived']]\ntrain_y.head()","c9a3590c":"train_x.shape","e703f2da":"train_x.dtypes","34baeb59":"train_x.isna().sum()","d378b195":"train_x = train.drop(train.columns[[0,1,3,8,10]],axis = 1)\ntrain_x.head()","288174a1":"def colsExcept(df,colsList):\n  return df.loc[:, ~df.columns.isin(colsList)]  \n\n\n#transofrming all object dtypes to categorical\ndef changeDtypes(df,from_dtype,to_dtype):\n    #changes inplace, affects the passed dataFrame\n#     df[df.select_dtypes(from_dtype).columns] = df.select_dtypes(from_dtype).astype(to_dtype)\n    df[df.select_dtypes(from_dtype).columns] = df.select_dtypes(from_dtype).apply(lambda x: x.astype(to_dtype))\n    \n    \nchangeDtypes(train_x,'object','category')\ntrain_x.dtypes","ef0b53f0":"train_y.Survived.value_counts()","4135321f":"new_train = train_x.join(train_y) \nnew_train.head()","7cbc84d2":"new_train.Pclass.value_counts()","885d3979":"def pclass_groups(series):\n    if series == 1:\n        return \"class 1\"\n    elif series == 2:\n        return \"class 2\"\n    elif series == 3:\n        return \"class 3\"\n\nnew_train['TicketClass'] = new_train['Pclass'].apply(pclass_groups)","b279fa60":"new_train = new_train.drop(new_train.columns[0],axis = 1)","339e9fd3":"new_train = new_train.dropna(subset=['Embarked'])\nnew_train[\"Age\"].fillna(new_train[\"Age\"].median(), inplace=True)\nnew_train.isna().sum()","8d411d88":"boxplot = new_train.boxplot(column = ['Fare'])","c452f222":"#new_train.describe()\n#new_train.Sex.value_counts()\n#new_train.Age.plot.density(ind=list(range(100)))\n#Age is skewed to the right, needs log transform","02ab8c4b":"new_train['LogAge'] = np.log(new_train['Age'])\nnew_train.head()","325c5429":"sns.distplot(new_train['LogAge'],hist = False)","943b9a16":"new_train = new_train.drop(['Age'],axis = 1)","d3936c4e":"sns.distplot(new_train['Fare'],hist = False)","bb84e4a1":"new_train['CbrtFare'] = np.cbrt(new_train['Fare'])\nnew_train.head()","1d3d2f30":"sns.distplot(new_train['CbrtFare'],hist = False)","385d9d37":"new_train = new_train.drop(['Fare'],axis = 1)\nnew_train.head()","38fa419c":"changeDtypes(new_train,'object','category')\nnew_train.dtypes","c3ac88a9":"Sex_dummies = pd.get_dummies(new_train.Sex, prefix='Sex').iloc[:, 1:]\nEmbarked_dummies = pd.get_dummies(new_train.Embarked, prefix='Embarked').iloc[:, 1:]\nTicketClass_dummies = pd.get_dummies(new_train.TicketClass, prefix='TicketClass').iloc[:, 1:]","82121a76":"new_train = pd.concat([new_train, Sex_dummies, Embarked_dummies, TicketClass_dummies], axis=1)\nnew_train = new_train.drop(['Sex','Embarked','TicketClass'],axis = 1)\nnew_train.head()","bb147831":"new_train.isna().sum()","02e05bea":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = new_train.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","c6415c35":"new_train_x = new_train.drop(['Survived'],axis = 1)\nnew_train_y = pd.DataFrame(new_train['Survived'])","adc223b2":"X_train, X_test, Y_train, Y_test = train_test_split(new_train_x,new_train_y, test_size=0.2, random_state=42)","7d04fce7":"#Gaussian Naive Bayes - Accuracy\nmodel = GaussianNB()\nmodel.fit(X_train,Y_train)\nY_model = model.predict(X_test)\naccuracy_score(Y_test, Y_model)","b9ae2a19":"#Gaussian Naive Bayes - Confusion Matrix\nmat = confusion_matrix(Y_test, Y_model)\nsns.heatmap(mat, square=True, annot=True, cbar=False)\nplt.xlabel('predicted value')\nplt.ylabel('true value')","b87234b1":"#Gaussian Naive Bayes - F1 Score\nf1_score(Y_test, Y_model)","c0968846":"#Decision Tree - Accuracy\nmodel = DecisionTreeClassifier(min_samples_split = 100)\nmodel.fit(X_train,Y_train)\nY_model = model.predict(X_test)\naccuracy_score(Y_test, Y_model)","cbc7d527":"#Decision Tree - Confusion Matrix\nmat = confusion_matrix(Y_test, Y_model)\nsns.heatmap(mat, square=True, annot=True, cbar=False)\nplt.xlabel('predicted value')\nplt.ylabel('true value')","a86ab29f":"#Decision Tree - F1 Score\nf1_score(Y_test, Y_model)","0270b24c":"#Hyper Parameter Tuning - KNN\n#k_range = range(1,31)\n#for k in k_range:\n#    model = KNeighborsClassifier(n_neighbors=k)\n#param_grid = dict(n_neighbors = k_range)\n#grid = GridSearchCV(model, param_grid, cv = 5, scoring = 'f1')\n#grid.fit(new_train_x,new_train_y)\n#print(max(grid.cv_results_['mean_test_score']))","2cd37362":"#Hyper Parameter Tuning - Random Forest\n#model = RandomForestClassifier()\n#param_grid = {'n_estimators':[100,200],\n#              'criterion':['gini','entropy'],\n#              'max_depth':[4,5,6],\n#              'max_features':['sqrt','log2',None]}\n#grid = GridSearchCV(model, param_grid, cv = 5, scoring = 'f1')\n#grid.fit(new_train_x,new_train_y)\n#print(max(grid.cv_results_['mean_test_score']))","73e928b7":"#Logistic Regression - Accuracy\n#model = LogisticRegression()\n#model.fit(X_train,Y_train)\n#Y_model = model.predict(X_test)\n#accuracy_score(Y_test, Y_model)","329c0468":"#Logistic Regression - Confusion Matrix\n#mat = confusion_matrix(Y_test, Y_model)\n#sns.heatmap(mat, square=True, annot=True, cbar=False)\n#plt.xlabel('predicted value')\n#plt.ylabel('true value')","daf9305d":"#Logistic Regression - F1 Score\n#f1_score(Y_test, Y_model)","34c310f6":"#Hyperparameter Tuning - Logistic Regression\n#model = LogisticRegression()\n#param_grid = {'penalty':['l1','l2'],\n#              'C':[0.01,0.1,1,10,100]}\n#grid = GridSearchCV(model, param_grid, cv = 5, scoring = 'f1')\n#grid.fit(new_train_x,new_train_y)\n#print(max(grid.cv_results_['mean_test_score']))","19c5f65f":"#Hyperparameter Tuning - Support Vector Machines\n#model = SVC()\n#param_grid = {'C':[0.1,1,10],\n#              'kernel':['linear','poly','rbf','sigmoid'],\n#              'degree':[2,3,4],\n#              'gamma':['auto_deprecated','scale']}\n#grid = GridSearchCV(model, param_grid, cv = 5, scoring = 'f1')\n#grid.fit(new_train_x,new_train_y)\n#print(max(grid.cv_results_['mean_test_score']))","250dc06e":"#Hyperparameter Tuning - Neural Networks (https:\/\/www.kaggle.com\/hhllcks\/neural-net-with-gridsearch)\n#model = neural_network.MLPClassifier()\n#param_grid = {'solver': ['lbfgs']}\n#grid = GridSearchCV(model, param_grid, cv = 5, scoring = 'f1')\n#grid.fit(new_train_x,new_train_y)\n#print(max(grid.cv_results_['mean_test_score']))","f606d47f":"#Hyperparameter Tuning - Adaptive Boost \nmodel = AdaBoostClassifier()\nparam_grid = {'n_estimators': [50, 100, 150, 200],\n          'learning_rate': [0.5, 1.0, 1.5, 2.0]}\ngrid = GridSearchCV(model, param_grid, cv = 5, scoring = 'f1')\ngrid.fit(new_train_x,new_train_y)\nprint(max(grid.cv_results_['mean_test_score']))","0a3c71dc":"grid.best_params_","924a2614":"final_model = AdaBoostClassifier(learning_rate = 1, n_estimators = 50)\nfinal_model.fit(new_train_x,new_train_y)","d7fbe902":"list(new_train_x)","b67ca414":"test = pd.read_csv('..\/input\/test.csv')\ntest.head()","de442c3d":"test_x = test.drop(test.columns[[2,7,9]],axis = 1)\ntest_x.head()","21ebce44":"changeDtypes(test_x,'object','category')\ntest_x.dtypes","f4955413":"new_test = test_x\nnew_test['TicketClass'] = new_test['Pclass'].apply(pclass_groups)\nnew_test.head()","b703c4f2":"new_test[\"Age\"].fillna(train_x[\"Age\"].median(), inplace=True)\nnew_test[\"Fare\"].fillna(train_x[\"Fare\"].median(), inplace=True)\nnew_test.isna().sum()","d706b1fa":"new_test['LogAge'] = np.log(new_test['Age'])\nnew_test.head()","d192a113":"new_test = new_test.drop(['Age'],axis = 1)\nnew_test['CbrtFare'] = np.cbrt(new_test['Fare'])\nnew_test = new_test.drop(['Fare'],axis = 1)\nnew_test.head()","8515699f":"new_test = new_test.drop(['Pclass'],axis = 1)\nnew_test = new_test.drop(['PassengerId'],axis = 1)","30efa5f2":"changeDtypes(new_test,'object','category')\nnew_test.dtypes","7dd3a169":"Sex_dummies = pd.get_dummies(new_test.Sex, prefix='Sex').iloc[:, 1:]\nEmbarked_dummies = pd.get_dummies(new_test.Embarked, prefix='Embarked').iloc[:, 1:]\nTicketClass_dummies = pd.get_dummies(new_test.TicketClass, prefix='TicketClass').iloc[:, 1:]\nnew_test = pd.concat([new_test, Sex_dummies, Embarked_dummies, TicketClass_dummies], axis=1)\nnew_test = new_test.drop(['Sex','Embarked','TicketClass'],axis = 1)\nnew_test.head()","121fa780":"yhat = pd.DataFrame(final_model.predict(new_test))","a0185a30":"yhat.shape","91c73d09":"test['Survived'] = yhat","915c9646":"test = test[['PassengerId','Survived']]","2d59b05f":"test.Survived.value_counts()","1a970d36":"#test.to_csv(\"Titanic_Survival_Predictions.csv\",index=False)","19d2c4fd":"OUTLIER REMOVAL:","0d9f34a9":"DATA TYPE CONVERSION:","925c3be2":"TRANSFORMATIONS:","c0b5c8c7":"Data Types?","a1874a2c":"Shape of the Data?","19f6cdb1":"Let's go with this one then and drop the Fare column","781e35c8":"Missing Values?","751104d1":"Standard Import Packages","0cdf3ab2":"Okay, the tail seems to be heavy on one end, let's try cube root","7d345c88":"Convert object types into categorical types","e6f4c5b2":"As there are many outliers, transformations make a lot of sense","8d2f9737":"FEATURE SELECTION","adff3124":"We are almost set, now we just need to decide if it's worth upsampling or downsampling the data","3bf96db5":"Convert some numerical columns to categorical, because they are categorical","331e9217":"For now, we are assuming that we do not need a special rule for Poisson data","a873f4f4":"Okay, mental note.. SibSp and Parch seems to have high correlation.. this will be obvious in the coming steps","a10ec3c9":"POISSON DATA TREATMENT:","85cc347f":"MACHINE LEARNING CLASSIFICATION:","cea847ad":"CREATE DUMMIES FOR CATEGORICAL VARIABLES","6d680ad7":"Let's use histogram on all numeric data and value counts on all categorical data","71730751":"Okay, so no feature reduction here, also there is no scope for feature engineering either","3023d70d":"Ok, seems fine, doesn't make sense to upsample\/downsample the data","cfbb352b":"Seems fine for now, let's proceed to remove Age and keep log of Age","e5417280":"Examining the train dataset","d0d57749":"Please read this before proceeding with Logistic Regression\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html","056f1403":"MISSING DATA TREATMENT","99848949":"Splitting train into x and y","4964a208":"Ignoring the Poisson data when it comes to transformation","43fce725":"GETTING THE TEST DATASET IN THE SAME FORMAT AS THE TRAIN DATASET","3832bc0f":"WRITE THE DATA FRAME TO CSV","271f92ed":"Ok, now age has 177 missing values, which can be treated, but cabin cannot.. because 687 out of 891 is a lot, and while we're at it, let's remove id,name and ticket also"}}