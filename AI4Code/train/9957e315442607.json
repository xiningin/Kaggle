{"cell_type":{"f3af2e40":"code","029e13e2":"code","93070416":"code","73f53f87":"code","6df06bab":"code","52a688b4":"code","660973b2":"code","4afbab54":"code","792f9740":"code","7407c73e":"code","60aa46fe":"code","9f8434f6":"code","46a92f0e":"code","62a40942":"code","5d10e7e2":"code","ff9d8fcc":"code","3a783544":"code","954bc236":"code","13355c2d":"code","26ab831d":"code","3fe9e702":"code","21cd6c8f":"code","fafe06f7":"code","90d81033":"code","e35172cf":"code","9fb04711":"code","f2aec3bc":"code","b480df6e":"code","47021316":"code","aaa06f27":"code","d1c34db4":"code","622cc8df":"code","fbb8a5e2":"code","2a3f9aee":"code","f0ff67ac":"code","ad04dbd1":"code","ecca5076":"markdown","563ba561":"markdown","c22b33c4":"markdown","b12b6267":"markdown","dd42ff2c":"markdown","ae08223b":"markdown","d1c81b40":"markdown","eeccc197":"markdown","5b7710a8":"markdown","b4e31a7a":"markdown","838b9160":"markdown"},"source":{"f3af2e40":"import numpy as np\nimport pandas as pd","029e13e2":"import matplotlib.pyplot as plt\nfrom matplotlib import style, gridspec\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom xgboost import XGBRegressor\n","93070416":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint('Conjunto de entrenamiento: ', train.shape, '\\nConjunto de test: ', test.shape)\n\ntrain.head()","73f53f87":"train.info()","6df06bab":"def plotting_3_chart(df, feature): \n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout = True, figsize = (12,8))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols = 3, nrows = 3, figure = fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient = 'v', ax = ax3 );\n\nplotting_3_chart(train, 'SalePrice')","52a688b4":"#Vemos como varia el precio segun el tama\u00f1o del inmueble\nimport plotly.express as px\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=train['SalePrice'],\n                                y=train['LotArea'],\n                                mode='markers',\n                                marker_color=train['SalePrice'],\n                                text=train['SalePrice'])) # hover text goes here\n\nfig.update_layout(title='Precio de venta segun el tama\u00f1o')\nfig.show()","660973b2":"#Vemos como varia el precio segun el a\u00f1o de construccion\nfig2 = go.Figure(data=go.Scatter(x=train['SalePrice'],\n                                y=train['YearBuilt'],\n                                mode='markers',\n                                marker_color=train['SalePrice'],\n                                text=train['SalePrice'])) # hover text goes here\n\nfig2.update_layout(title='Precio de venta segun el a\u00f1o de construccion')\nfig2.show()","4afbab54":"#Mostramos un mapa de calor con todas las variables\n#.corr() encuentra la correlacion entre variables de un dataframe\n\nfig, ax = plt.subplots(figsize = (30, 30))\nsns.heatmap(train.corr(), \n            annot = True, \n            ax=ax,\n            fmt=\".2f\", \n            cmap='coolwarm',\n            cbar_kws={\"shrink\": .8})","792f9740":"#Demasiadas variables\n#sns.barplot(train['LotArea'], train[\"SalePrice\"])\nsns.boxplot(train['MSSubClass'], train[\"SalePrice\"])","7407c73e":"sns.boxplot(train['MoSold'], train[\"SalePrice\"])","60aa46fe":"sns.boxplot(train['OverallQual'], train[\"SalePrice\"])","9f8434f6":"y = train['SalePrice'].reset_index(drop=True)","46a92f0e":"y2 = np.log1p(y)\n\nplotting_3_chart(pd.DataFrame(y2), 'SalePrice')","62a40942":"def CheckMissingData(Dataframe):\n    missing_df=pd.DataFrame({\"type\": Dataframe.dtypes, \n                             \"total\": Dataframe.isnull().sum(), \n                             \"Percentage\" : Dataframe.isnull().sum()\/len(Dataframe)})\n    missing_df = missing_df.loc[missing_df.Percentage > 0]\n    return missing_df\n\nCheckMissingData(train)","5d10e7e2":"train.drop(['Alley','PoolQC','MiscFeature','Id','Fence'], axis=1, inplace = True)","ff9d8fcc":"CheckMissingData(train)","3a783544":"#imputacion de valores perdidos\n\ntrain['MasVnrType'] = train['MasVnrType'].fillna( value = train['MasVnrType'].mode()[0])\ntrain['BsmtCond'] = train['BsmtCond'].fillna( value = train['BsmtCond'].mode()[0])\ntrain['BsmtExposure'] = train['BsmtExposure'].fillna( value = train['BsmtExposure'].mode()[0])\ntrain['BsmtFinType1'] = train['BsmtFinType1'].fillna( value = train['BsmtFinType1'].mode()[0])\ntrain['BsmtFinType2'] = train['BsmtFinType2'].fillna( value = train['BsmtFinType2'].mode()[0])\ntrain['Electrical'] = train['Electrical'].fillna( value = train['Electrical'].mode()[0])\ntrain['FireplaceQu'] = train['FireplaceQu'].fillna( value = train['FireplaceQu'].mode()[0])\ntrain['GarageType'] = train['GarageType'] .fillna( value = train['GarageType'].mode()[0])\ntrain['GarageFinish'] = train['GarageFinish'].fillna( value = train['GarageFinish'].mode()[0])\ntrain['GarageQual'] = train['GarageQual'].fillna( value = train['GarageQual'].mode()[0])\ntrain['GarageCond'] = train['GarageCond'].fillna( value = train['GarageCond'].mode()[0])\ntrain['BsmtQual'] = train['BsmtQual'].fillna( value = train['BsmtQual'].mode()[0])\n\ntrain['LotFrontage'] = train['LotFrontage'].fillna( value = train['LotFrontage'].mean())\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna( value = train['MasVnrArea'].mean())\ntrain['GarageYrBlt'] = train['GarageYrBlt'].fillna( value = train['GarageYrBlt'].mean())\n\nCheckMissingData(train)\n\n#Tambien se puede hacer importando SimpleImputer de Sci-Kit Learn","954bc236":"CheckMissingData(test)","13355c2d":"test.drop(['Alley','PoolQC','MiscFeature','Id','Fence'], axis=1, inplace = True)","26ab831d":"test['MSZoning'] = test['MSZoning'].fillna( value = test['MSZoning'].mode()[0])\ntest['Utilities'] = test['Utilities'].fillna( value = test['Utilities'].mode()[0])\ntest['Exterior1st'] = test['Exterior1st'].fillna( value = test['Exterior1st'].mode()[0])\ntest['Exterior2nd'] = test['Exterior2nd'].fillna( value = test['Exterior2nd'].mode()[0])\ntest['MasVnrType'] = test['MasVnrType'].fillna( value = test['MasVnrType'].mode()[0])\ntest['BsmtCond'] = test['BsmtCond'] .fillna( value = test['BsmtCond'].mode()[0])\ntest['BsmtExposure'] = test['BsmtExposure'].fillna( value = test['BsmtExposure'].mode()[0])\ntest['BsmtFinType1'] = test['BsmtFinType1'].fillna( value = test['BsmtFinType1'].mode()[0])\ntest['BsmtFinType2'] = test['BsmtFinType2'].fillna( value = test['BsmtFinType2'].mode()[0])\ntest['BsmtQual']= test['BsmtQual'].fillna( value = test['BsmtQual'].mode()[0])\ntest['KitchenQual']= test['KitchenQual'].fillna( value = test['KitchenQual'].mode()[0])\ntest['Functional']= test['Functional'].fillna( value = test['Functional'].mode()[0])\ntest['FireplaceQu']= test['FireplaceQu'].fillna( value = test['FireplaceQu'].mode()[0])\ntest['GarageType']= test['GarageType'].fillna( value = test['GarageType'].mode()[0])\ntest['GarageFinish']= test['GarageFinish'].fillna( value = test['GarageFinish'].mode()[0])\ntest['GarageQual']= test['GarageQual'].fillna( value = test['GarageQual'].mode()[0])\ntest['GarageCond']= test['GarageCond'].fillna( value = test['GarageCond'].mode()[0])\ntest['SaleType']= test['SaleType'].fillna( value = test['SaleType'].mode()[0])\n\ntest['LotFrontage'] = test['LotFrontage'].fillna( value = test['LotFrontage'].mean())\ntest['BsmtFinSF1'] = test['BsmtFinSF1'].fillna( value = test['BsmtFinSF1'].mean())\ntest['BsmtFinSF2'] = test['BsmtFinSF2'].fillna( value = test['BsmtFinSF2'].mean())\ntest['BsmtUnfSF'] = test['BsmtUnfSF'].fillna( value = test['BsmtUnfSF'].mean())\ntest['TotalBsmtSF'] = test['TotalBsmtSF'].fillna( value = test['TotalBsmtSF'].mean())\ntest['BsmtFullBath'] = test['BsmtFullBath'].fillna( value = test['BsmtFullBath'].mean())\ntest['BsmtHalfBath'] = test['BsmtHalfBath'].fillna( value = test['BsmtHalfBath'].mean())\ntest['MasVnrArea'] = test['MasVnrArea'].fillna( value = test['MasVnrArea'].mean())\ntest['GarageYrBlt'] = test['GarageYrBlt'].fillna( value = test['GarageYrBlt'].mean())\ntest['GarageCars'] = test['GarageCars'].fillna( value = test['GarageCars'].mean())\ntest['GarageArea'] = test['GarageArea'].fillna( value = test['GarageArea'].mean())\n\nCheckMissingData(test)","3fe9e702":"y = train['SalePrice'].reset_index(drop=True)\nX_tr = train\ntest_feat = test.copy()\n\nfeat = pd.concat([X_tr, test_feat]).reset_index(drop=True)\nfeat.shape","21cd6c8f":"# from sklearn.preprocessing import LabelEncoder\n\n# cols = X.columns[:10]\n# for col in cols:\n#     encoder = LabelEncoder()\n#     encoder.fit(X[col])\n#     X[col] = encoder.fit_transform(X[col])\n#     test[col] = encoder.transform(test[col])\n    \n# X.head()","fafe06f7":"cat_col =[]\nnum_col =[]\n\ni = 0\nfor a in feat.dtypes:\n    if a ==float or a==int:\n        num_col.append(feat.columns[i])\n    elif a ==object:\n        cat_col.append(feat.columns[i])\n    i= i+1\n    \nfeat = pd.get_dummies(feat, columns = cat_col, drop_first = True)\nfeat.reset_index(drop = True, inplace = True)\nfeat = feat.loc[:,~feat.columns.duplicated()]\nfeat_train = feat.loc[feat['SalePrice'] >= 0]\nfeat_test = feat.loc[feat['SalePrice'].isnull()]\nfeat_train.shape","90d81033":"from sklearn.model_selection import train_test_split,RandomizedSearchCV\n\ny = np.log(feat_train['SalePrice'])\nX = feat_train.drop(['SalePrice'], axis = 1, inplace=False)","e35172cf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","9fb04711":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f2aec3bc":"import xgboost as xgb\n\nxgb_r = xgb.XGBRegressor()\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster = ['gbtree','gblinear']\nlearning_rate = [0.05,0.1,0.15,0.20]\nmin_child_weight = [1,2,3,4]\nbase_score = [0.25,0.5,0.75,1]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }\n\nrandom_cv = RandomizedSearchCV(\n            estimator = xgb_r,\n            param_distributions = hyperparameter_grid,\n            cv = 5, n_iter = 50,\n            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True,\n            random_state = 42)","b480df6e":"my_model = xgb.XGBRegressor(base_score = 0.5,\n                            booster = 'gbtree',\n                            colsample_bylevel = 1,\n                            colsample_bynode = 1,\n                            colsample_bytree = 1, gamma = 0,\n                            gpu_id = -1, importance_type = 'gain',\n                            interaction_constraints = '',\n                            learning_rate = 0.001,\n                            max_delta_step = 0, max_depth = 6,\n                            min_child_weight = 1, missing = None,\n                            monotone_constraints = '()',\n                            n_estimators = 15000\n                            )","47021316":"my_model.fit(\n    X_train,\n    y_train,\n    early_stopping_rounds = 10,\n    eval_set = [(X_test, y_test)],\n    verbose = 10\n)","aaa06f27":"pred = my_model.predict(X_test)","d1c34db4":"from sklearn.metrics import mean_squared_error,accuracy_score,r2_score\nimport math\n\nprint(mean_squared_error(y_test, pred))","622cc8df":"px.scatter(x=pred, y=y_test)","fbb8a5e2":"r2_score(y_test, pred)","2a3f9aee":"# import lightgbm as lgb\n\n# model_2 = lgb.LGBMRegressor(random_state = 100, \n#                         n_estimators = 10000, \n#                         min_data_per_group = 5, \n#                         boosting_type = 'gbdt',\n#                         num_leaves = 128, \n#                         learning_rate = 0.002, \n#                         subsample_for_bin = 200000, \n#                         importance_type ='split', \n#                         metric ='rmse', \n#                         min_data_in_leaf = 50,\n#                         verbose = 10)\n\n# model_2.fit(X_train, y_train,\n#             early_stopping_rounds = 5,\n#             eval_set = [(X_test, y_test)],\n#             verbose = 100)","f0ff67ac":"sub = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsub['target'] = pred\nsub.to_csv('submission.csv', index = False)","ad04dbd1":"kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission.csv -m \"Message\"","ecca5076":"Con el siguiente c\u00f3digo vemos como se distribuyen los datos, primero con una distribuici\u00f3n de precios, despues con un diagrama de vela japonesa y por \u00faltimo,un diagrama de puntos con una recta de predcci\u00f3n dividido por quantiles","563ba561":"Vamos a dividir el conjunto de entrenamiento en dos variables, una parte para el entrenamiento propiamente dicho (que ser\u00e1 el 70% de total del conjunto) y otra para la validaci\u00f3n cruzada, la cual ocupar\u00e1 un 30% del total del conjunto de datos. Para el primer grupo se ha seleccionado anteriormente la variable que nos interesa (SalePrice) y para el segundo se elimina esa variable para no contar con ella (esto ultimo esta hecho mas arriva)","c22b33c4":"* ***ANAL\u00cdSIS DE DATOS***\n\nSe cargan los datos de train y test que nos proporciona la p\u00e1gina, tras eso, mostramos por pantalla el tama\u00f1o de los mismos y la cabecera del conjunto de entrenamiento","b12b6267":"Mostramos la informaci\u00f3n al completo de las variables ","dd42ff2c":"La siguiente funci\u00f3n busca los valores perdidos que tenemos que eliminar","ae08223b":"En el mapa de calor resultante vemos como afectan las distintas variables al precio final de la vivienda, por ello, podemos hacer una grafica con la informaci\u00f3n de las variables m\u00e1s relevantes, tanto en positivo (colores m\u00e1s claros) como en negativo (colores m\u00e1s oscuros)","d1c81b40":"Repetimos la imputacion de datos con el conjunto test","eeccc197":"Al ser este un proyecto de machine learning b\u00e1sico, se seguiran los pasos basicos de un proyecto de machine learning, primero, se explorar\u00e1n los datos de manera que podamos sacar conclusiones sobre los mismos,tras esto, se limpiaran los datos para evitar errores. Con los datos limpios y ya explorados, se usar\u00e1 un modelo XGBoost, otro modelo que se podr\u00eda usar el Bosque Aleatorio o una red neuronal, pero me he decantado por la mejora de gradiente por que me apetec\u00eda usarlo","5b7710a8":"Empezamos el proyecto cargando las librer\u00edas que vamos a usar, este primer apartado se va actualizando con las variaciones de necesidades del mismo","b4e31a7a":"Al mostrar la cabecera, podemos ver las cinco primeras lineas del conjunto y las columnas del documento, esto \u00faltimo es \u00fatil para determinar que variables ser\u00e1n la X (la variable de entrada) y c\u00faal ser\u00e1n y (la variable que nos interesa calcular para el conjunto de prueba), se mejora el resultado mostrando todos los detalles de las columnas","838b9160":"Normalizamos los datos"}}