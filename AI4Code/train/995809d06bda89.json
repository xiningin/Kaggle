{"cell_type":{"c5c1424a":"code","38c5150f":"code","f4f0e947":"code","b8180f97":"code","4d09f5ae":"code","13047cde":"code","0ecc0d00":"code","61e18bd5":"code","df34aea0":"code","9d67ba53":"code","ec736dc2":"code","1cc9a16a":"code","4386ad48":"code","2b15d2b6":"code","d2c54666":"code","2525f6c8":"code","aa8f84ea":"code","773a95ce":"code","aadd6f30":"code","e3f02e00":"code","e2e12bb9":"code","50130920":"code","5930d20c":"code","1e8b1c1d":"markdown","967710d7":"markdown","7ba9b908":"markdown","e1612160":"markdown","aa26e666":"markdown","05ab396c":"markdown","8a1a00b7":"markdown","94e78310":"markdown","535308f9":"markdown","ca56147b":"markdown","3316b6ec":"markdown","9e676b34":"markdown","2c5efae4":"markdown","004fd47a":"markdown","12493691":"markdown","11408085":"markdown","7213b0b1":"markdown","f839fe9f":"markdown","6fa8d5d2":"markdown","759aaeaa":"markdown","4fe1669b":"markdown","8219a909":"markdown","add9e2ca":"markdown"},"source":{"c5c1424a":"from pandas import read_csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport numpy as np","38c5150f":"df = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\ndf.head()","f4f0e947":"# indexing on Serial No.\ndf.set_index('Serial No.', inplace = True)","b8180f97":"df.head()","4d09f5ae":"# All of our given features\ncolumns = df.columns.values\ncolumns","13047cde":"col_fix = []\nfor col in columns:\n    col_fix += [col.strip()]\ndf.columns = col_fix\ndf.columns.values","0ecc0d00":"scatter_matrix(df)\nplt.show()","61e18bd5":"df.plot(x = 'Research', y = 'Chance of Admit', kind = 'scatter')","df34aea0":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ndef vif_calc(features):\n    vif = pd.DataFrame()\n    vif['features'] = features.columns\n    vif['VIF'] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]\n    return vif","9d67ba53":"X_temp = df.copy()\nX_temp.insert(0,'Intercept', 1)\nX_temp.drop(columns = ['Chance of Admit'], inplace = True)\nvif = vif_calc(X_temp)\nvif","ec736dc2":"temp = df[['GRE Score', 'TOEFL Score', 'CGPA', 'Chance of Admit']]\ntemp.corr(method = 'pearson')","1cc9a16a":"df.drop(columns = ['TOEFL Score'], inplace = True)\ndf.head()","4386ad48":"X_temp = df.copy()\nX_temp.insert(0,'Intercept', 1)\nX_temp.drop(columns = ['Chance of Admit'], inplace = True)\nvif = vif_calc(X_temp)\nvif","2b15d2b6":"df['GRE Score'] = df['GRE Score'].divide(340)\ndf['CGPA'] = df['CGPA'].divide(10)\ndf.head()","d2c54666":"df['Academics'] = df['GRE Score'] * df['CGPA']\ndf.head()","2525f6c8":"df.drop(columns = ['GRE Score', 'CGPA'], inplace = True)\ndf.head()","aa8f84ea":"X_temp = df.copy()\nX_temp.insert(0,'Intercept', 1)\nX_temp.drop(columns = ['Chance of Admit'], inplace = True)\nvif = vif_calc(X_temp)\nvif","773a95ce":"from sklearn.model_selection import train_test_split\nX = df.copy()\nX.drop(columns = ['Chance of Admit'], inplace = True)\ny = df['Chance of Admit']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","aadd6f30":"# scaling the features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","e3f02e00":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import regularizers\ndef model_create():\n    model = Sequential()\n    model.add(Dense(20, kernel_initializer='normal', activation = 'relu', input_dim = 5))\n    model.add(Dense(20, activation = 'relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation = 'sigmoid'))\n    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n    return model","e2e12bb9":"from keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nestimator = KerasRegressor(build_fn = model_create, epochs = 500, batch_size = 500, verbose=0)\nkfold = KFold(n_splits=15)\nresults = cross_val_score(estimator, X_train, y_train, cv=kfold)\nprint(\"Baseline: %f (%f) MSE\" % (results.mean(), results.std()))","50130920":"estimator.fit(X_train, y_train)\npredictions = estimator.predict(X_test)","5930d20c":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nprint(\"Final mean squared error: %f      R^2 value: %f\" %(mean_squared_error(y_test, predictions), r2_score(y_test, predictions)))","1e8b1c1d":"taking the same steps with the data as the other file...","967710d7":"There is not much else to do in terms of data cleaning, this dataset was made nicely on purpose. This means we can go straight into analysis and exploration of the data","7ba9b908":"<h2> Model Preparation","e1612160":"<h2> Final Evaluation","aa26e666":"Now that we have our model (*estimator*) we can evaluate it on our test data set","05ab396c":"<h2> Get the Data","8a1a00b7":"<h2> Data exploration\/cleaning","94e78310":"Using the general guideline for VIF[1] GRE, CGPA and TOEFL will be further analyzed to see if any corrections need to be made. <br>\n[1] https:\/\/online.stat.psu.edu\/stat462\/node\/180\/#:~:text=As%20the%20name%20suggests%2C%20a,much%20the%20variance%20is%20inflated.&text=The%20general%20rule%20of%20thumb,of%20serious%20multicollinearity%20requiring%20correction.","535308f9":"We can now drop GRE and CGPA","ca56147b":"<h3> Model Creation","3316b6ec":"From the correlation matrix, it seems that there is a high correlation (>0.8) between all three features. However, since the TOEFL score has the lowest correlation coefficient to the Chance of admission, we will drop the TOEFL score. \n\n**NOTE**: a low correlation coefficient does not necessarily mean that the feature is unrelated to the target. It simply means that there is a *weak linear relationship*. It's entirely possible for that feature to be related to the target, albeit it would be a higher dimension relationship. ","9e676b34":"It makes logical sense that these 3 features have a high variance inflation factor. People who have a higher GPA will most likely score higher on the TOEFL and GRE, because these 2 examinations are based in acadeamics. <br>\n\nThe main difference between the GRE and TOEFL is that the GRE is used to see how well a student can take graduate-level coursework and TOEFL is used to measure the participant's skill in English [2].\n\n\n[2]https:\/\/www.prepscholar.com\/toefl\/blog\/gre-and-toefl\/#:~:text=The%20TOEFL%20and%20GRE%20are,TOEFL%20measures%20English%20language%20skills.&text=It's%20possible%20you%20may%20have%20to%20take%20both%20exams.","2c5efae4":"Now that we've cleaned our data and condensed several features, we can now begin to separate our data to put into our model","004fd47a":"It appears that people that have research generally have a higher chance of admissions so we will keep this as a feature.\n\nNow let us determine if multicollinearity will be an issue. The variance inflation factor (VIF) will be used to determine the significance of the effect of multicollinearity. Note that this was ignored in the other version.","12493691":"Using Kfolds","11408085":"Note that the chance of admission and letter of reccomendation column have an additional whitespace at the end of their respective column names. This might lead to issues\/confusion, so first removing these whitespaces","7213b0b1":"Note that there are 3 main areas that we can choose to tune our model:\n1. in our function *model_create*, we can add\/remove more layers and tweak the parameters of those layers to obtain a better model. Note that we have a limited sample size so a large # of nodes is not required\n2. we can change the # of epochs to use\n3. we can choose the # of folds to use (i.e. change value of k for kfolds)\n\nTo determine how well our tweaks to our model are, we want a low final mean squared error and a high (max 100) R^2 value.","f839fe9f":"<h3> GRE, CGPA, TOEFL analysis","6fa8d5d2":"This is still the original task: predict university admission. This time, only neural networks will be used for personal practice.","759aaeaa":"We can now see that our VIF scores are all satisfactory","4fe1669b":"Because of the small # of available samples, we will be using 2 Dense layers and use iterated k fold with shuffling to create a more accurate model. To avoid overfitting, we will also be using l1 regularization.","8219a909":"The research category is extremely binary and may not be as useful to include as a feature in our model. Let's look more in depth into it to determine if we should remove it.\n\nMajority of the features appear to be linearly correlated with each other, which could potentially be an issue in the context of multicollinearity. This will be addressed after dealing with the research column.","add9e2ca":"While the VIF has gone down, it is still high for GRE and CGPA. It would be best if we can merge these 2 columns into one. Let's divide each column by their respective max score, then multiply those 2 values to obtain our metric for academic potential\/ability"}}