{"cell_type":{"0ba3a9b8":"code","b52839f6":"code","4277b22a":"code","5a043604":"code","9b023fdd":"code","43dda08a":"code","b579fc53":"code","d5faa457":"code","df2de4c2":"code","4ebe7417":"code","910a07bc":"code","b9833c12":"code","6e882512":"code","9fec0752":"code","73a7fe57":"code","41bbb6ef":"code","5b35fd93":"code","d2df427a":"code","039f56a0":"code","f1218a29":"code","d50fbc2c":"code","b6465c75":"code","17680270":"code","3a7493db":"code","ed8c9281":"code","c5f5695e":"code","a3de3e2d":"code","2fff76b0":"code","4404a789":"code","31146213":"code","b7131d94":"code","8dd39e11":"code","d9fba2e8":"code","1ef16c77":"code","89c3d242":"code","37a90cd9":"code","f3504a3f":"code","9a1cf600":"code","13d3c02a":"code","94a72dd5":"code","057dec0c":"code","94634f74":"code","b9f6c3f9":"code","47ad510c":"code","6a7ba860":"code","7324c08d":"code","15100622":"code","b6d7bcbf":"code","0df63d96":"code","a595b3b1":"code","a47d26bc":"code","c16d94a6":"code","b535e2e0":"code","2d4f7e4e":"code","a527018f":"code","4eb64adb":"markdown","34aa97ed":"markdown","be042a32":"markdown","7c1d7c2f":"markdown","d6416a29":"markdown","2877b8ea":"markdown","814ae787":"markdown","28d46753":"markdown","b4a8dfd0":"markdown","64038211":"markdown","d37b201c":"markdown","7fbfa0b4":"markdown","20495526":"markdown","9d39f52b":"markdown","15ae84d0":"markdown","78b3f4b3":"markdown","282d1cff":"markdown","31af490a":"markdown","8064df2e":"markdown","10a1747d":"markdown","2c1c0adb":"markdown","66f9c7bd":"markdown"},"source":{"0ba3a9b8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport plotly.offline as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nplt.rc(\"font\", size=15)\nimport warnings\nwarnings.simplefilter(action='ignore')","b52839f6":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","4277b22a":"test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","5a043604":"train.head()","9b023fdd":"test.head()","43dda08a":"df = pd.concat([train,test],axis=0)","b579fc53":"df.describe()","d5faa457":"df.info()","df2de4c2":"#divide dataset into two parts(categorical, contineous)\ncategorical, numerical = [],[]\nfor z in df.columns:\n    t = df.dtypes[z]\n    if t=='object':\n        categorical.append(z)\n    else:\n        numerical.append(z)\nprint(\"CategoricaL:\\n{}\".format(categorical))\nprint(\"\\nNumericaL:\\n{}\".format(numerical))","4ebe7417":"print('Numerical Features:{}'.format(len(numerical)))\nprint('Categorical Features:{}'.format(len(categorical)))","910a07bc":"year = [feature for feature in numerical if 'Mo' in feature or 'Yr' in feature or 'Year' in feature]\nyear","b9833c12":"#finding the unique values in each column (type object)\nfor col in df.select_dtypes('O').columns:\n    print('We have {} unique values in {} column : {}'.format(len(df[col].unique()),col,df[col].unique()))\n    print('-'*100)","6e882512":"#print the count of unique values in each categorical columns\nprint('Categorical columns Unique values count\\n')\nfor col in categorical:\n    print(col,'-'*(30-len(col)),'>',len(df[col].unique()))","9fec0752":"plt.figure(figsize=(20,6));\nsns.heatmap(df.isnull(),yticklabels=False, cbar=False, cmap='mako')","73a7fe57":"variable = [feature for feature in categorical if df[feature].isnull().sum()]\nfor feature in variable:\n    print(\"{}: {}%\".format(feature,np.round(df[feature].isnull().mean(),3)))","41bbb6ef":"variable = [feature for feature in numerical if df[feature].isnull().sum()]\nfor feature in variable:\n    print(\"{}: {}%\".format(feature,np.round(df[feature].isnull().mean(),3)))","5b35fd93":"# Meaning that there is no Miscellaneous feature.\ndf.fillna({'Alley': 'None', 'Fence':'None', 'MiscFeature':'None', \n           'PoolQC':'None', 'FireplaceQu':'None', 'MasVnrType':'None'}, inplace = True)\n\n#Meaning that there is no basement(Categorical).\ndf.fillna({'BsmtQual':'None', 'BsmtCond':'None',\n           'BsmtExposure':'None', 'BsmtFinType1':'None',\n           'BsmtFinType2':'None'},inplace=True)\n\n#Missing Basement Columns(Numerical)\nBsmt_con = ['MasVnrArea','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n            'TotalBsmtSF', 'BsmtFullBath','BsmtHalfBath','BsmtFinSF1', \n            'BsmtFinSF2', 'BsmtUnfSF']\nfor Bsmt in Bsmt_con:\n    df[Bsmt].fillna(0, inplace=True) \n    \n#missing Garage columns(Categorical)\ndf.fillna({'GarageType':'None','GarageCond': 'None', 'GarageQual':'None', \n           'GarageQual':'None', 'GarageFinish': 'None'}, inplace=True)\n\n#Missing Garage Columns(Numerical)\ndf.fillna({'GarageCars':0, 'GarageArea': 0}, inplace = True)\n\n#Replacing Other categorical variable with its mode\ndf['MSZoning']=df['MSZoning'].fillna(df['MSZoning'].mode()[0])\ndf['Electrical']=df['Electrical'].fillna(df['Electrical'].mode()[0])\ndf['Functional']=df['Functional'].fillna(df['Functional'].mode()[0])\ndf['KitchenQual']=df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])\ndf['SaleType']=df['SaleType'].fillna(df['SaleType'].mode()[0])\ndf['Utilities']=df['Utilities'].fillna(df['Utilities'].mode()[0])\ndf['LotFrontage']=df['LotFrontage'].fillna(df['LotFrontage'].mean())\ndf['GarageYrBlt']=df['GarageYrBlt'].fillna(df['GarageYrBlt'].median())\n\n\ndf['Exterior1st'].fillna('Other' ,inplace=True)\ndf['Exterior2nd'].fillna('Other' ,inplace=True)","d2df427a":"corr =df.corr()\ncorr.sort_values(['SalePrice'], ascending= False, inplace=True)\nprint(corr.SalePrice)","039f56a0":"# most correlated features\ncorrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(14,8))\ng = sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"cubehelix\")","f1218a29":"categorical_num = [feature for feature in numerical if len(df[feature].unique())<20 and feature not in year+['Id']]","d50fbc2c":"contineous = [feature for feature in numerical if len(df[feature]) and feature not in year+['Id']+categorical_num]","b6465c75":"plt.figure(figsize=(25, 15))\nheatmap =sns.heatmap(df[contineous].corr(), annot = True,  cmap=\"crest\")\nheatmap.set_title('Correlation Heatmap');","17680270":"year_features = ['GarageYrBlt', 'YearBuilt', 'YearRemodAdd', 'YrSold']\n\nplt.figure(figsize=(15, 8))\nsns.set(font_scale= 1.2)\nsns.set_style('whitegrid')\n\nfor i, features in enumerate(year_features):\n    plt.subplot(2, 2, i+1)\n    plt.scatter(data=train, x=features, y='SalePrice', color =\"maroon\")  \n    plt.xlabel(features)\n    plt.ylabel('SalePrice')\n    \nsns.despine()","3a7493db":"plt.figure(figsize=(14, 8))\nsns.kdeplot(data=train,x=\"SalePrice\", hue =\"MoSold\", fill=True,common_norm=False, palette=\"husl\", alpha=.5, linewidth=1)","ed8c9281":"Quality_features = [ 'RoofMatl', 'ExterQual', 'BsmtQual', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'GarageQual']\n\nplt.figure(figsize=(30, 20))\nsns.set(font_scale= 1.2)\nsns.set_style('darkgrid')\n\nfor i, feature in enumerate(Quality_features):\n    plt.subplot(3, 4, i+1)\n    sns.barplot(data=train, x=feature, y='SalePrice', palette=\"ch:.10\")  \n    \n    \nsns.despine()","c5f5695e":"# We shall plot these figures\nplt.figure(figsize=(30, 70))\nsns.set(font_scale= 1.2)\nsns.set_style('whitegrid')\n\nfor i, features in enumerate(numerical):\n    plt.subplot(10, 4, i+1)\n    plt.scatter(data=df.iloc[:len(train)], x=features, y='SalePrice', color =\"blue\")\n    plt.xlabel(features)\n    plt.ylabel('SalePrice')\n    \n    \nsns.despine()","a3de3e2d":"df.LotFrontage[(df.LotFrontage >= 160)] = 160\ndf.LotArea[(df.LotArea >= 75000)] = 75000\ndf.MasVnrArea[(df.MasVnrArea >= 1000)] = 1000\ndf.BsmtFinSF1[(df.BsmtFinSF1 >= 2500)] = 2500\ndf.TotalBsmtSF[(df.TotalBsmtSF >= 3000)] = 3000\ndf['1stFlrSF'][(df['1stFlrSF'] >= 3000)] = 3000\ndf.GrLivArea[(df.GrLivArea >= 3500)] = 3500\ndf.GarageArea[(df.GarageArea >= 1500)] = 1500","2fff76b0":"plt.figure(figsize=(10, 5))\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nsns.distplot(df.iloc[:len(train)]['SalePrice'] , fit=norm, color='maroon');\n(mu, sigma) = norm.fit(df.iloc[:len(train)]['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')","4404a789":"plt.figure(figsize=(10, 5))\ndf['SalePrice'] = np.log1p(df.iloc[:len(train)]['SalePrice'])\nsns.distplot(df.iloc[:len(train)]['SalePrice'] , fit=norm, color='maroon')\n(mu, sigma) = norm.fit(df.iloc[:len(train)]['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')","31146213":"#if it is more than 1 or -1 it is highly skewed and between 0.5 and 1 it is moderatly skewed, between 0.5 and 0 it is almost symmetric\nskewed_clm = df[contineous].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_clm= skewed_clm[skewed_clm > 0.75]\nskewed_clm= skewed_clm.index\n\ndf[skewed_clm] = np.log1p(df[skewed_clm])","b7131d94":"df =pd.get_dummies(df, columns=categorical, drop_first=True)","8dd39e11":"df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']","d9fba2e8":"del df['TotalBsmtSF']","1ef16c77":"del df['1stFlrSF']","89c3d242":"del df['2ndFlrSF']","37a90cd9":"from sklearn.model_selection import train_test_split","f3504a3f":"new_train = df.iloc[:1460,:]\nnew_test = df.iloc[1460:,:]","9a1cf600":"x = new_train.drop(['SalePrice'], axis=1)\ny = new_train['SalePrice']","13d3c02a":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42) # 75% training and 25% test","94a72dd5":"from sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb","057dec0c":"def print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)","94634f74":"ridge = RidgeCV(alphas = [1, 0.1, 0.001, 0.0005])\nridge.fit(X_train, y_train)\npred = ridge.predict(X_test)\n\ntest_pred = ridge.predict(X_test)\ntrain_pred = ridge.predict(X_train)\n\nprint('Test set evaluation:\\n')\nprint_evaluate(y_test, test_pred)\nprint('*'*30)\nprint('Train set evaluation:\\n')\nprint_evaluate(y_train, train_pred)","b9f6c3f9":"from yellowbrick.regressor import PredictionError\nvis = PredictionError(ridge)\nvis.fit(X_train, y_train)\nvis.score(X_train, y_train)\nvis.show()","47ad510c":"from yellowbrick.regressor import ResidualsPlot\nvis = ResidualsPlot(ridge)\nvis.fit(X_train, y_train)\nvis.score(X_train, y_train)\nvis.show()","6a7ba860":"lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005])\nlasso.fit(X_train, y_train)\n\ntest_pred = lasso.predict(X_test)\ntrain_pred = lasso.predict(X_train)\n\nprint('Test set evaluation:\\n')\nprint_evaluate(y_test, test_pred)\nprint('*'*30)\nprint('Train set evaluation:\\n')\nprint_evaluate(y_train, train_pred)","7324c08d":"from yellowbrick.regressor import PredictionError\nvis = PredictionError(lasso)\nvis.fit(X_train, y_train)\nvis.score(X_train, y_train)\nvis.show()","15100622":"from yellowbrick.regressor import ResidualsPlot\nvis = ResidualsPlot(lasso)\nvis.fit(X_train, y_train)\nvis.score(X_train, y_train)\nvis.show()","b6d7bcbf":"E_model = ElasticNetCV(alphas = [1, 0.1, 0.001, 0.0005])\nE_model.fit(X_train, y_train)\n\ntest_pred = E_model.predict(X_test)\ntrain_pred = E_model.predict(X_train)\n\nprint('Test set evaluation:\\n')\nprint_evaluate(y_test, test_pred)\nprint('*'*30)\nprint('Train set evaluation:\\n')\nprint_evaluate(y_train, train_pred)","0df63d96":"from yellowbrick.regressor import PredictionError\nvis = PredictionError(E_model)\nvis.fit(X_train, y_train)\nvis.score(X_train, y_train)\nvis.show()","a595b3b1":"from yellowbrick.regressor import ResidualsPlot\nvis = ResidualsPlot(E_model)\nvis.fit(X_train, y_train)\nvis.score(X_train, y_train)\nvis.show()","a47d26bc":"lasso.fit(X_train, y_train)\ntrain_pred = np.expm1(lasso.predict(X_train))\npred = np.expm1(lasso.predict(X_test))\nprint('Test set evaluation:\\n')\nprint_evaluate(np.expm1(y_test), pred)\nprint('*'*30)\nprint('Train set evaluation:\\n')\nprint_evaluate(np.expm1(y_train),train_pred)","c16d94a6":"final_test=new_test.copy()","b535e2e0":"X = new_test.drop(['SalePrice'], axis=1)\nY = new_test[['SalePrice']]","2d4f7e4e":"final_test['SalePrice'] = np.expm1(lasso.predict(X))\nfinal_test['Id'] = new_test['Id']\n\nlogistic_submission = final_test[['Id','SalePrice']]\n\nlogistic_submission.to_csv(\"submission.csv\", index=False)\n\nlogistic_submission.tail()","a527018f":"logistic_submission.head(50)","4eb64adb":"**\u2714overfitting:**\n\nWhen we have a lot of measurements, we can be confident that the least squares line accurately reflects the relationship. But if we have a few traing dataset we fit new lines which overlaps dataset and sum of squared residuals is 0 or it can be too small. and after that if we test same fitted model to testing dataset then sum fo squared residuals will large. \n Hence we say that the line or model overfit the traing data and have a high varince. Hence the main idea behind ridge regression is to find the new line that does't fit traing data as well\n \n\n**\u2714Regularization:**\n\nit is a technique used to reduced the error at the cost of introducing some bias to the given  training set and avoid overfitting. commonly used regularisation techniques are:\n1. L1 regularisation\n2. L2 regularisation\n\n\n1. L1 Regularisation: is also called LASSO(Least Absolute Shrinkage and Selection Operatore)Regression\n2. L2 Regularisation: is also called Ridge Regression.\n\n\n**\u2714Ridge Regression:**\n\nRidge Regression is a model tuning method that is used to analyse any data that suffers from multicollinearity (multicollinearity is simply correlation between predictore variables or independant variables).\nwhen the issue of multicollinearity occurs, least-squares are unbaised, and variance are large, this results in predicted values to be far away from the actual values. In Ridge Regression we have the small amount of bias due to the penalty has less variance.\nThe bias added to the model is known as the Ridge Regression Penalty. we compute it by multiplying lamda by the square of each frature. The equation for the Ridge Regression Penalty is:\n               \n                  The sum of the squared residuals + lamda1*(slope)^2           \n1. lamda - determine how severe penalty is.\n2. slope - penalty for traditional least square method.\n       \nIn Ridge Regression smaller the slope means ridge regression line are less sensitive to dependant variable that is least square line.\n when lamda is 0 then:               \n                   \n                   The sum of the squared residuals + 0*(slope)^2\n                   \nwhich is the least square line.\n                    \n                    \n\n**\u2714Lasso Regression:**\n\nLasso Regression is almost identical to Ridge Regression, the only difference being that we take the absolute value as opposed to the squaring the features when computing the ridge regression penalty.\nThe equation for the Ridge Regression Penalty is:                  \n                 \n                    The sum of squared residuals + lamda2*|Slope|\n\nAs a result of taking the absolute value, Lasso Regression can shrink the slope all the way down to 0. whereas Ridge Regression can only shrink the slope asymptotically close to 0.\n\n\n\n**\u2714Elastic-Net Regression:**\nElastic-Net Regression combines penalty for lasso and ridge regression.It also groups and shrinks the parameters associated with correlated variables and leaves them in the equation or removes them all at once.\nEquation for Elastic-Net:\n                    \n                    lamda1*|var1|+------|varx| + lamda2*(var1)^2+-------(varx)^2\n\n1. lamda1 > 0 and lamda2 = 0 then we get lasso regression or vise versa.\n2. lamda1 >0 and lamda2 > 0 then we will get Elastic_net Regression.\n     \n\n\n**\u2714Ridge Regression vs. Linear Regression:**\n\nIn simple linear regression, we determine the best fitting line by minimizing the sum of the squared residuals. but in Ridge Regression we minimiz sum of squred of residuals as well as Ridge Regression Penalty.\n\nSimple Regression doesn\u2019t differentiate \u201cimportant\u201d from \u201cless-important\u201d predictors in a model, so it includes all of them. This leads to overfitting a model and failure to find unique solutions. \n\nlinear regression produces unbiased estimates, variances can be so large that they may be wholly inaccurate. Ridge regression adds just enough bias to make the estimates reasonably reliable approximations to true population values.","34aa97ed":"### Correlation","be042a32":"###### Attribute","7c1d7c2f":"### Outliers","d6416a29":"# \u2728House Price: Advanced Regression Techniques\u2728","2877b8ea":"### Exploratory Data Analysis","814ae787":"###### Contineous Variable","28d46753":"#### Data Transaformation","b4a8dfd0":"### Train-Test Split","64038211":"### Visualization","d37b201c":"   * Linear Assumption:\n   Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n   \n   \n   * Remove Noise: \n   Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\n   \n   \n   * Remove Collinearity:\n   Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n   \n   \n   * Gaussian Distributions:\n   Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\n   \n   \n   * Rescale Inputs:\n   Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization.","7fbfa0b4":"###### Categories in Number","20495526":"from above graphs we can see the varibles LotFrontage, LotArea, MasVnrArea, BsmtFinsf1, TotalBsmtsf, \n1stFlrsf, 2ndFlrsf, GrLivArea, GrageArea this variables have high outliers. means extremely large areas for very low prices. so we replace these outliers by its lower values.","9d39f52b":"#### Lasso Regression","15ae84d0":"#### Ridge Regression","78b3f4b3":"### Importing Libreries","282d1cff":"**\u2714Simple linear regression:**\nis a statistical method that allowa us to summarize and study relationship between two contineous \nvariables i.e 'predictore' and 'response' variables.\n\n\n**\u2714Multiple Linear Regression:**\nmultiple linear regression is an extension of simple linear regression. it is used when we want to \npredict more than one 'predictore' variable.","31af490a":"#### Elastic-Net Regression","8064df2e":"### Creating Dummy Variables","10a1747d":"### Skewness","2c1c0adb":"### Model Fitting","66f9c7bd":"### Missing Values"}}