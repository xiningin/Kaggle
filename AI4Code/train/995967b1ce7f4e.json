{"cell_type":{"a80daa7e":"code","97b71b95":"code","0edbc4e3":"code","e4fc9c44":"code","e6602d9f":"code","ff78964c":"code","92b7825b":"code","d5e997f0":"code","1509151c":"code","fb14b98b":"code","d3acfa3f":"code","9bcf60ca":"code","39539c68":"code","1a3e6c3f":"code","9c159aef":"code","d78f2e78":"code","c0b4dc46":"code","1e973eb1":"code","f6ff8ffa":"code","7060b43b":"code","0e7a1175":"code","f4277cd9":"code","830e31cc":"code","cccd02b1":"code","e38276bb":"code","0e7898c5":"code","8dc26410":"code","d287c5d3":"code","8c4305ce":"code","5127480e":"code","cd59b395":"code","4e80c24c":"code","bbd334f7":"code","8dca75ab":"code","810ee35a":"code","f52ebb41":"code","6942bcce":"code","a86a7fc0":"code","4f1139f4":"code","0da67b86":"code","55bd0bb1":"code","97865c14":"code","21422003":"code","a10457ef":"code","ef98331a":"code","bc148a72":"code","4b6af3ec":"markdown","ec8d7995":"markdown","5ce38532":"markdown","c64f84a1":"markdown","072f810a":"markdown","0dad19f8":"markdown","ce7ee257":"markdown","6605a58d":"markdown","d36d4f92":"markdown","f0850b82":"markdown","1abe76fd":"markdown","49f0a6b6":"markdown","3c9886c9":"markdown","6e2ea37e":"markdown","c2c0617c":"markdown","fa4bca41":"markdown","13aae069":"markdown","2138b31b":"markdown","decf2770":"markdown","b85aeaf3":"markdown","e8bd91ef":"markdown","a40d4c67":"markdown"},"source":{"a80daa7e":"import numpy as np\nimport pandas as pd\nimport time\nimport tensorflow as tf\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n# Visualization Libraries\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px \nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport collections\n\n# Other Important Libraries\nfrom sklearn.model_selection import train_test_split  # used for splitting the dataset\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","97b71b95":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0edbc4e3":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","e4fc9c44":"df.shape","e6602d9f":"df.info()","ff78964c":"df.describe()","92b7825b":"# Let's see the percentage of data having fraud and non-fraud transaction\nprint(\"Frauds\", round(df['Class'].value_counts()[1]\/len(df)*100, 3), '% of the dataset')\nprint(\"No Frauds\", round(df['Class'].value_counts()[0]\/len(df)*100, 3), '% of the dataset')","d5e997f0":"labels = df[\"Class\"].value_counts()[:10].index\nvalues = df[\"Class\"].value_counts()[:10].values\n\ncolors=['#2678bf', '#98adbf']\n\nfig = go.Figure(data=[go.Pie(labels = labels, values=values, textinfo=\"label+percent\",\n                            insidetextorientation=\"radial\", marker=dict(colors=colors))])\n\nfig.show()","1509151c":"fig, ax = plt.subplots(1,2, figsize=(8,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title(\"Distribution of Tranaction Amount\")\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='g')\nax[1].set_title('Distribution of Transaction Time')\nax[1].set_xlim([min(time_val), max(time_val)])\n\nplt.show()","fb14b98b":"# We know that most of the data has already been scaled so we need to scale the columns \n# that are not scaled i.e. Amount and Time\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\n# Since we have added a new columns in place of Amount and and time so we need to drop the\n# existing columns\n\ndf = df.drop(['Time', 'Amount'], axis = 1)","d3acfa3f":"df.head()","9bcf60ca":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis = 1, inplace = True)\ndf.insert(0, 'scaled_amount', scaled_amount) # now scaled_amount is at the 1st position of columns list\ndf.insert(1, 'scaled_time', scaled_time) # now scaled_time is at the 2nd position of the columns lis\n\ndf.head()","39539c68":"# We already know from the above that our datasets contains\n# Frauds: 0.173 % of the dataset\n# No Frauds: 99.827 % of the dataset\n\nX = df.drop('Class', axis = 1)\ny = df['Class']\n\nSS = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in SS.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n    \n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\n\ntrain_uniques_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-'*100)\n\nprint('Label Distribution : \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","1a3e6c3f":"# Since we need equal number of both fraud and non-fraud data so we need to first shuffle \n# to take equal amount of non-fraud datas\n\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows, so there must be an equal amount of non-fraud classes too\n\nfraud_df = df.loc[df['Class']==1]\nnon_fraud_df = df.loc[df['Class']==0][:492]  # its not necessary to pick the 1st 492 to non-fraud classes we can take it from anywhere\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# now shuffle dataframe rows\n\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","9c159aef":"new_df.shape","d78f2e78":"print('Classe Distribution in the Dataset Subsample')\nprint(new_df['Class'].value_counts()\/len(new_df))","c0b4dc46":"\n\nlabels = new_df[\"Class\"].value_counts()[:10].index\nvalues = new_df[\"Class\"].value_counts()[:10].values\n\ncolors=['#2678bf', '#98adbf']\n\nfig = go.Figure(data=[go.Pie(labels = labels, values=values, textinfo=\"label+percent\",\n                            insidetextorientation=\"radial\", marker=dict(colors=colors))])\n\nfig.show()","1e973eb1":"f, (ax1, ax2) = plt.subplots(2,1, figsize=(28,24))\n\n# for the original dataset\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Original Dataset Correlation Matrix \\n (This won't be used for reference)\")\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title(\"Subsample Correlation Matrix \\n (This will be use for reference)\", fontsize=14)\nplt.show()","f6ff8ffa":"from scipy.stats import norm\nf, (ax1,ax2,ax3, ax4) = plt.subplots(1,4, figsize=(20,6))\n\nv14_fraud_dist = new_df['V14'].loc[new_df['Class']==1].values # trying to get to know how much V14 is affecting the chances of being Fraud\nsns.distplot(v14_fraud_dist, ax=ax1, fit=norm, color='r')\nax1.set_title('V14 Distribution \\n (Fraud Transaction)', fontsize=14)\n\nv12_fraud_dist = new_df['V12'].loc[new_df['Class']==1].values # trying to get to know how much V12 is affecting the chances of being Fraud\nsns.distplot(v12_fraud_dist, ax=ax2, fit=norm, color='g')\nax2.set_title('V12 Distribution \\n (Fraud Transaction)')\n\nv10_fraud_dist = new_df['V10'].loc[new_df['Class']==1].values # trying to get to know how much V10 is affecting the chances of being Fraud\nsns.distplot(v10_fraud_dist, ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V10 Distribution \\n (Fraud Transaction)')\n\nv17_fraud_dist = new_df['V17'].loc[new_df['Class']==1].values # trying to get to know how much V17 is affecting the chances of being Fraud\nsns.distplot(v10_fraud_dist, ax=ax4, fit=norm)\nax4.set_title('V17 Distribution \\n (Fraud Transaction)')\n\nplt.show()","7060b43b":"from scipy.stats import norm\nf, (ax1,ax2,ax3, ax4) = plt.subplots(1,4, figsize=(20,6))\n\nv2_fraud_dist = new_df['V2'].loc[new_df['Class']==1].values # trying to get to know how much V2 is affecting the chances of being Fraud\nsns.distplot(v2_fraud_dist, ax=ax1, fit=norm, color='r')\nax1.set_title('V2 Distribution \\n (Fraud Transaction)', fontsize=14)\n\nv4_fraud_dist = new_df['V4'].loc[new_df['Class']==1].values # trying to get to know how much V4 is affecting the chances of being Fraud\nsns.distplot(v4_fraud_dist, ax=ax2, fit=norm, color='g')\nax2.set_title('V4 Distribution \\n (Fraud Transaction)')\n\nv11_fraud_dist = new_df['V11'].loc[new_df['Class']==1].values # trying to get to know how much V11 is affecting the chances of being Fraud\nsns.distplot(v11_fraud_dist, ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V11 Distribution \\n (Fraud Transaction)')\n\nv19_fraud_dist = new_df['V19'].loc[new_df['Class']==1].values # trying to get to know how much V19 is affecting the chances of being Fraud\nsns.distplot(v19_fraud_dist, ax=ax4, fit=norm)\nax4.set_title('V19 Distribution \\n (Fraud Transaction)')\n\nplt.show()","0e7a1175":"# Rmoving the V14 Outliers\n\nv14_fraud = new_df['V14'].loc[new_df['Class']==1].values\nq25 = np.percentile(v14_fraud, 25)  # Calculating the 25 percentile\nq75 = np.percentile(v14_fraud, 75)  # Calculating the 75 percentile\n\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75-q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr*1.5\nv14_lower = q25-v14_cut_off # setting the lower limit\nv14_upper = q75+v14_cut_off # setting the upper limit\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Features V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V14 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\n\n\n","f4277cd9":"# Removing the V12 Outliers\n\nv12_fraud = new_df['V12'].loc[new_df['Class']==1].values\nq25 = np.percentile(v12_fraud, 25)  # Calculating the 25 percentile\nq75 = np.percentile(v12_fraud, 75)  # Calculating the 75 percentile\n\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv12_iqr = q75-q25\nprint('iqr: {}'.format(v12_iqr))\n\nv12_cut_off = v12_iqr*1.5\nv12_lower = q25-v12_cut_off # setting the lower limit\nv12_upper = q75+v12_cut_off # setting the upper limit\nprint('Cut Off: {}'.format(v12_cut_off))\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\n\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('Features V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V12 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\n\n\n","830e31cc":"# Removing the V10 Outliers\n\nv10_fraud = new_df['V10'].loc[new_df['Class']==1].values\nq25 = np.percentile(v10_fraud, 25)  # Calculating the 25 percentile\nq75 = np.percentile(v10_fraud, 75)  # Calculating the 75 percentile\n\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv10_iqr = q75-q25\nprint('iqr: {}'.format(v10_iqr))\n\nv10_cut_off = v10_iqr*1.5\nv10_lower = q25-v10_cut_off # setting the lower limit\nv10_upper = q75+v10_cut_off # setting the upper limit\nprint('Cut Off: {}'.format(v10_cut_off))\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\n\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('Features V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\n\n\n","cccd02b1":"\nf, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(20,6))\n\ncolors=['#2678bf', '#98adbf']\n\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, ax=ax1,palette = colors)\nax1.set_title(\"Feature V14 \\n Reduction of Outliers\")\nax1.annotate(\"Fewer extreme \\n Outliers\", xy=(0.98, -17.5), xytext=(0,-12), arrowprops=dict(facecolor='black'))\n\n# Feature V12\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\nax2.set_title(\"Feature V12 \\n Reduction of Outliers\")\nax2.annotate(\"Fewer extreme \\n Outliers\", xy=(0.98, -17.3), xytext=(0,-12), arrowprops=dict(facecolor='black'))\n\n# Feature V10\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\nax2.set_title(\"Feature V10 \\n Reduction of Outliers\")\nax2.annotate(\"Fewer extreme \\n Outliers\", xy=(0.98, -17.3), xytext=(0,-12), arrowprops=dict(facecolor='black'))\n\n\nplt.show()","e38276bb":"X = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n# Let's check how much time the given dimensionlity reduction algorithms take\n\n# Implementing T-SNE \nt0 = time.time()  # Initial Time\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()  # Final Time\ntime_diff1 = t1-t0\nprint(\"T-SNE took {:.2} s\".format(time_diff1))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\ntime_diff2 = t1-t0\nprint(\"PCA took {:.2} s\".format(time_diff2))\n\n# TruncatedSVD Implementation\nt0 = time.time()\nX_reduced_pca = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\nt1 = time.time()\ntime_diff2 = t1-t0\nprint(\"Truncated SVD took {:.2} s\".format(time_diff2))\n\n\n","0e7898c5":"# Undersampling before cross validation (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']","8dc26410":"# Since we have already scaled our data we shpuld split our training and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)","d287c5d3":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","8c4305ce":"# Implementing the Classifiers\n\nclassifiers = {\n    \"LogisticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","5127480e":"# let's try to get the score using the croos-validation\n\nfrom sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=7)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 3) * 100, \"% accuracy score\")","cd59b395":"# Let's use GridSearchCV to find the best parameters for each of the classifiers described above\nfrom sklearn.model_selection import GridSearchCV\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n\n# The below line will help us to find the best parameters automatically\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n\n# The below line will help us to find the best parameters automatically\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# The below line will help us to find the best parameters automatically\nsvc = grid_svc.best_estimator_\n\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# The below line will help us to find the best parameters automatically\ntree_clf = grid_tree.best_estimator_","4e80c24c":"# Now calculating the accuracy in the Overfitting cases\n\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=7)\nprint(\"Logistic Regression Cross Validation Score: \", round(log_reg_score.mean()*100,3).astype(str)+'%')\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=7)\nprint(\"Knears Neighbors Cross Validation Score:\", round(knears_score.mean()*100,3).astype(str)+'%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=7)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean()*100, 3).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=7)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean()*100, 3).astype(str) + '%')","bbd334f7":"# We will undersample during cross validating\n\n\nundersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nfor train_index, test_index in SS.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss \n\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n\n\n# Cross Validating the right way\n\nfor train, test in SS.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","8dca75ab":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    return plt","810ee35a":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","f52ebb41":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\n# Creating a dataframe with all the scores and the classifier names\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=7, method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=7)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=7, method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=7)","6942bcce":"from sklearn.metrics import roc_auc_score\n\nprint(\"Logistic Regression: \", roc_auc_score(y_train, log_reg_pred))\nprint(\"KNears Neighbors: \", roc_auc_score(y_train, knears_pred))\nprint(\"Support Vector Classifier: \", roc_auc_score(y_train, svc_pred))\nprint(\"Decision Tree Classifier: \", roc_auc_score(y_train, tree_pred))","a86a7fc0":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n","4f1139f4":"def logistic_roc_curve(log_fpr, log_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Logistic Regression ROC Curve', fontsize=16)\n    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nlogistic_roc_curve(log_fpr, log_tpr)\nplt.show()","0da67b86":"def knear_roc_curve(knear_fpr, knear_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('KNears Neighbors ROC Curve', fontsize=16)\n    plt.plot(knear_fpr, knear_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nknear_roc_curve(knear_fpr, knear_tpr)\nplt.show()","55bd0bb1":"def svc_roc_curve(svc_fpr, svc_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Support Vector Classifier ROC Curve', fontsize=16)\n    plt.plot(svc_fpr, svc_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nsvc_roc_curve(svc_fpr, svc_tpr)\nplt.show()","97865c14":"def tree_roc_curve(tree_fpr, tree_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Decision Tree ROC Curve', fontsize=16)\n    plt.plot(tree_fpr, tree_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \ntree_roc_curve(tree_fpr, tree_tpr)\nplt.show()","21422003":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)","a10457ef":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)\n","ef98331a":"undersample_y_score = log_reg.decision_function(original_Xtest)","bc148a72":"from sklearn.metrics import average_precision_score\n\nundersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      undersample_average_precision))","4b6af3ec":"Since the data is highly imbalanced so inorder to avoid the overfitting we will take an equal number of both fraud and non-fraud cases for our training purpose.\nSince we know that we have 492 fraud cases we take simply take an eqaul number of non-fraud cases from the entire dataset inorder to create a new sub-sample","ec8d7995":"From the above heatmap we can see that V2,V4,V11,V19 are positively correlated so higher these values, there is higher chance of transaction to be Fraud\nOn the other hand V17, V14, V12 and V10 are all negatively correlated so lower the value of these, higher is the chance of transaction turning out to be Fraud.","5ce38532":"### If you found this insightful please Upvote it","c64f84a1":"From above we can see that excluding V2 all are highly positively related","072f810a":"### Spliting the Data (Original Dataframe)","0dad19f8":"One can see that TruncatedSVD took the least amount of time","ce7ee257":"### Removing the Outliers","6605a58d":"## Correlation Matrices\n\nLets try to get the correlation matrix, we will get the correlational matrix for both the original (df) and also the subsampl dataset (new_df)","d36d4f92":"### Plotting Boxplots with Outliers Removed","f0850b82":"From the above plot we can notice that the data distribution in above one is quite skewed we can also see the further distribution of other features too.","1abe76fd":"Plotting the Curve for all the 4 Classifiers","49f0a6b6":"Doing Analysis for the Logistic Regression","3c9886c9":"Since we don't know what value are shown by the V's column so lets bring the column 'scaled_amount' and 'scaled_time' in the begining of the columns","6e2ea37e":"## Anomaly Detection\n\nThe main aim in this section is to remove the 'extreme outliers' from features that have a high correlation with our classes. So from the above heatmap we know that we have 4 positively and 4 negatively highly correlated values","c2c0617c":"We can see above that V14 is highly negatively correlated \n\nLets try to plot the same for positively correlated values","fa4bca41":"## Classifiers (Undersampling)\n\nIn this I will train 4 different classifiers and decide which classifier will be more effective in detecting fraud transactions.","13aae069":"From the above all we can see that Logistic Regression has the best output","2138b31b":"One can observe that our dataset is highly unbalanced because most of the transactions are genuine, a very few tansactions are fraud one, so if we are using this dataframes as the base for our predictive models and analysis we are sure to get a lot of errors and there is chance of our algorithms overfitting the data since they will assume that most of the transactions are not fraud. Surely we don't want our model to assume that, else we want our model to detect patterns that give signs of fraud on fraud transactions.","decf2770":"## Importing the Necessary Libraries","b85aeaf3":"## Analysing the Data","e8bd91ef":"## Dimensionality Reduction and Clustering:","a40d4c67":"There is no null values in the above datasets so need to worry about imputation also all the datasets are of float or int type"}}