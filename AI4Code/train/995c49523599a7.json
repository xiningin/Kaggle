{"cell_type":{"cad30b4c":"code","d813c782":"code","6c8e7d1a":"code","5ad3296e":"code","f1273b6c":"code","36d25070":"code","57962468":"code","86114d94":"code","9f9ac7c0":"code","95ed582c":"code","ce8a6156":"code","5d888ae1":"code","e26eb726":"code","be29dfdb":"code","0fb0449e":"code","104effb2":"code","9c6bf8a6":"code","59c1a578":"code","b0234423":"code","0e542dfe":"code","e1ec7328":"markdown","3c5f83de":"markdown","03711d4a":"markdown","2ddd3c66":"markdown","42fd657a":"markdown","9782feeb":"markdown","316c2ae8":"markdown","171e1e1d":"markdown","34d4eb40":"markdown","fd8fcd36":"markdown","ac9ea7c5":"markdown","6eaeff8b":"markdown","a087437b":"markdown","12fbb462":"markdown","8803b92e":"markdown","0e7337cb":"markdown","c7f34fcb":"markdown","465eec11":"markdown","e4bd3763":"markdown","ca8de4d2":"markdown","7f240a21":"markdown","6882b76c":"markdown","41f9a519":"markdown"},"source":{"cad30b4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d813c782":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndf.head()","6c8e7d1a":"df.dtypes","5ad3296e":"df.info()","f1273b6c":"df.shape","36d25070":"df.isnull().sum()","57962468":"df.columns.isna()\n","86114d94":"df.isin([' ?']).sum()","9f9ac7c0":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set()\nfig = plt.figure(figsize = [15,20])\ncols = ['quality']\ncnt = 1\nfor col in cols :\n    plt.subplot(4,3,cnt)\n    sns.boxplot(data = df, y = col)\n    cnt+=1\nplt.show()","95ed582c":"df['quality'] = ['good' if i>=7 else 'bad' for i in df['quality']]\n","ce8a6156":"df.info()","5d888ae1":"categorical_df = df.select_dtypes(include=['object'])\ncategorical_df.columns\n\n\nfrom sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\n\n\ncategorical_df = categorical_df.apply(enc.fit_transform)\ncategorical_df.head()\n\ndf = df.drop(categorical_df.columns, axis=1)\n\n\ndf = pd.concat([df, categorical_df], axis=1)\ndf.head()\n\n","e26eb726":"X = df.drop('quality', axis=1)\ny = df['quality']\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","be29dfdb":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=5)\n","0fb0449e":"from sklearn.linear_model import LogisticRegression\n\ndef_lr= LogisticRegression()\ndef_lr.fit(X_train, y_train)\n\nlr_pred = def_lr.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Logistic Regression accuracy: \", accuracy_score(y_test, lr_pred))\n","104effb2":"from sklearn import svm\ndef_svm = svm.SVC()\ndef_svm.fit(X_train, y_train)\n\nsvm_pred = def_svm.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint(\"SVM accuracy: \", accuracy_score(y_test, svm_pred))\n","9c6bf8a6":"from sklearn.tree import DecisionTreeClassifier\n\ndef_dt= DecisionTreeClassifier()\ndef_dt.fit(X_train, y_train)\n\ndt_pred = def_dt.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Decision Tree accuracy\", accuracy_score(y_test, dt_pred))\n  \n    \n","59c1a578":"from sklearn.ensemble import RandomForestClassifier\n\ndef_rf = RandomForestClassifier()\ndef_rf.fit(X_train, y_train)\n\n\nrf_pred = def_rf.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Random Forests accuracy\", accuracy_score(y_test, rf_pred))\n","b0234423":"\"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV , KFold\n\n\nrf = RandomForestClassifier()\n\ngs_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 6, 8, 10],\n              \"min_samples_split\": [2, 3, 6, 8, 10],\n              \"min_samples_leaf\": [1, 3, 6, 8, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100, 300, 500],\n              \"criterion\": [\"gini\"]}\n\n\nrf_CV = GridSearchCV(estimator = rf, param_grid=gs_grid, cv=kfold ,scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nresult = rf_CV.fit(X_train, y_train)\n\nprint(result.best_params_)\nprint(result.best_score_)\n\"\"\"","0e542dfe":"from sklearn.ensemble import RandomForestClassifier\n\nfinal_rf = RandomForestClassifier(bootstrap=False , criterion='gini', max_features=3, min_samples_leaf=1, min_samples_split=6, n_estimators=100)\nfinal_rf.fit(X_train, y_train)\n\n\nfinal_rf_pred = final_rf.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Random Forests accuracy\", accuracy_score(y_test, final_rf_pred))\n","e1ec7328":"As the best accuracy is Random Forest, so we will use Random Forest to tune the ML model","3c5f83de":"Comparing 4 Model's accuary scoure:\n\nI will choose **Random Forest** as Final Model","03711d4a":"# 2.3 Standardisation","2ddd3c66":"Random Forest","42fd657a":"# 2.2 Encoding categorical variables","9782feeb":"Decision Tree","316c2ae8":"*Input Variables:*\n\n**fixed acidity**: most acids involved with wine or fixed or nonvolatile\n\n**volatile acidity**: the amount of acetic acid in wine\n\n**citric acid**: found in small quantities, citric acid can add 'freshness' and flavor to wines\n\n**residual sugar**: the amount of sugar remaining after fermentation stops\n\n**chlorides**: the amount of salt in the wine\n\n**free sulfur dioxide**: the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion\n\n**total sulfur dioxide**: amount of free and bound forms of S02\n\n**density**: the density of water is close to that of water depending on the percent alcohol and sugar content\n\n**pH**: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic)\n\n**sulphates**: a wine additive which can contribute to sulfur dioxide gas (S02) levels\n\n**alcohol**: the percent alcohol content of the wine\n\n\n*Output Variable:*\n\nquality: target variable (score between 0 and 10, expect output 'good' \/ 'bad')","171e1e1d":"Using default model:\n\n1. Logistic Regression\n\n2. SVM\n\n3. Decision Tree\n\n4. Random Forest","34d4eb40":"SVM","fd8fcd36":"# 2. Data Preprocessing","ac9ea7c5":"# 3.2 Tuning model","6eaeff8b":"~1. Overview\n\n\n~2. Data Preprocessing\n\n    ~2.1  Defining \"Quality\" Logic\n    \n    ~2.2  Encoding categorical variables\n    \n    ~2.3  Standardisation \n    \n    \n~3. Classification Model\n\n    ~3.1 Sample Model\n    \n    ~3.2 Tuning model\n    \n","a087437b":"# 3.  Classification Model","12fbb462":"# 3.1 Sample Model","8803b92e":"Input the param into model","0e7337cb":"From this graph, taking 75th or above for 'Good' definition, so\n\nAssuming:\n\n-quality >= 7.0 is GOOD\n\n-quality < 7.0 is BAD","c7f34fcb":"We need to convert features which contain strings to numerical values. \n\nThis is required by most model algorithms.","465eec11":"Logistic Regression","e4bd3763":"Checking all data stype again","ca8de4d2":"Understanding the data","7f240a21":"# 1. Overview","6882b76c":"Split Data","41f9a519":"# 2.1 Defining \"Quality\" Logic"}}