{"cell_type":{"310c4858":"code","b9c487d6":"code","c0f9812b":"code","af1b85c0":"code","4e9aea3c":"code","344435d2":"code","3fa5514c":"code","be8a0c36":"code","6487d05c":"code","ea957d99":"markdown","6e5d700b":"markdown","50cd195b":"markdown"},"source":{"310c4858":"# step 1 >> Removing Noise \n#                --> number, punctuation, url\n# step 2 >> Transform any abnormalities\n#                --> spell correction\n#                --> demojization\n#                --> remove html tag\n#                --> process hashtags\n# step 3 >> Lowercase and split\n#                --> convert text to lowercase\n#                --> convert the text into a list of words\n# step 4 >> Remove stop words\n# step 5 >> stem the words\n# step 6 >> Join and returns\n#                --> join the list of words in text again\n#                --> return the joined text    ","b9c487d6":"# Domain specific pre-processing >> Libraries\n#                                   -->emoji() >> to convert emoji to text\n#                                   -->regex() >> to remove hashtags\n#                                   -->PyEnchant() >> for spell correction\n#                                   -->lxml() >> to remove html tags\n# --> removing the stop words i.e. the, me, and, my --> NLTK\n# --> stemming >> converts words to it's root form which is not in the vocabulari i.e. amazing becomes amaz after stemming --> NLTK\n# --> lemmatization >> does the same things as stemming; the only difference is that the lemmatized word is in the vocabulary --NLTK\n# --> removing the contraction >> I'm --> I am, He've --> He have","c0f9812b":"## Frequency Based Embedding >>\n#                          Type 1 >> Count Vectorizer \n#                                          --> convert a sentence into a bag of word representation\n#                          Type 2 >> TF-IDF Vectorizer\n#                                          --> convert a sentence into a vector","af1b85c0":"import re\nimport pandas as pd\nimport numpy as np\n\nfrom lxml import html      # --> to remove html tag\nfrom emoji import demojize  # --> to convert emoji into text\n\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer","4e9aea3c":"df = pd.read_csv('\/kaggle\/input\/sentimental-analysis-for-tweets\/sentiment_tweets3.csv')\ndf.drop(['Index'], axis=1, inplace=True)\ndf.head()","344435d2":"df.rename(columns={'message to examine':'text', 'label (depression result)': 'sentiment'}, inplace=True)\ndf.head()","3fa5514c":"df['sentiment'].value_counts()","be8a0c36":"stemmer = PorterStemmer()\nstop = stopwords.words('english')\n\ndef clean_text(text):\n    # Convert emoji to text\n    text = demojize(text)\n    \n    # Remove HTML Tags\n    try:\n        text = html.document_fromstring(text).text_content()\n    except:\n        pass\n    \n    # Remove hyperlinks\n    text = re.sub('http\\S+', ' ', text)\n    \n    # Remove non alphabets\n    text = re.sub('[^a-zA-Z ]+', ' ', text)\n    \n    # lowercase and stem\n    text = text.lower().split()\n    \n    # Remove stopwords and start words\n    text = [stemmer.stem(word) for word in text if word not in stop and len(word) > 2]\n    \n    # Join and Return\n    return ' '.join(text)","6487d05c":"sample_text = \"I'm learning Natural Language Processing\"\ncleaned_text = clean_text(sample_text)\ncleaned_text","ea957d99":"# Data Pre-Processing","6e5d700b":"# Sentiment Analysis: Model","50cd195b":"# Text Preprocessing Pipeline"}}