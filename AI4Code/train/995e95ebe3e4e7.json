{"cell_type":{"55f9f9a8":"code","721e2c47":"code","ac29b372":"code","71413e70":"code","250637d4":"code","7eb68531":"markdown","3b6d0ac3":"markdown","53ba8626":"markdown"},"source":{"55f9f9a8":"from __future__ import print_function\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras.datasets import mnist, fashion_mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import backend as K\nimport numpy as np\nimport matplotlib.pyplot as plt","721e2c47":"# input image dimensions\nimg_rows, img_cols = 28, 28\nnum_classes = 10\n\ndef preprocess(data=\"mnist\", permutation=False):\n    # the data, split between train and test sets\n    if data == \"mnist\":\n        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    elif data == \"fashion_mnist\":\n        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    x_train \/= 255\n    x_test \/= 255\n\n    x_train = x_train.reshape(x_train.shape + (1,))\n    x_test = x_test.reshape(x_test.shape + (1,))\n    \n    \"\"\"\n    If wanted, permutate pixels randomly with the following steps:\n    - Concatenate train and test set along axis 0\n    - Flatten dimension 1 and 2 (28, 28) to 784\n    - Swap axes 0 and 1 to get shape (784, samples, 1)\n    - Permutate axis 0. This way, all samples are permutated in the same way\n    - Swap axes 0 and 1 back, reshape flattened dimensions to (28, 28), split train and test again\n    \"\"\"\n    if permutation:\n        X = np.concatenate((x_train, x_test), axis=0)\n        X = np.random.permutation(X.reshape(((X.shape[0],) + (img_rows * img_cols, 1)))\n                                  .swapaxes(1, 0)).swapaxes(1, 0).reshape((X.shape[0], img_rows, img_cols, 1))\n        x_train, x_test = X[:60000], X[60000:]\n\n    # convert class vectors to binary class matrices\n    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n    \n    return x_train, x_test, y_train, y_test","ac29b372":"# CNN described in chapter 14\ndef cnn(opt=\"adam\"):\n    model = Sequential()\n    model.add(tf.keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]))\n    model.add(tf.keras.layers.MaxPooling2D(2))\n    model.add(tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n    model.add(tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"))\n    model.add(tf.keras.layers.MaxPooling2D(2))\n    model.add(tf.keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n    model.add(tf.keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"))\n    model.add(tf.keras.layers.MaxPooling2D(2))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n    \n    model.compile(loss=tf.keras.losses.categorical_crossentropy, \n                  optimizer=opt,\n                  metrics=['accuracy'])\n    \n    return model\n\n# MLP described in chapter 10\ndef mlp(opt=\"adam\", layers=(300, 100)):\n    model = Sequential()\n    model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))\n    model.add(tf.keras.layers.Dense(layers[0], activation=\"relu\"))\n    model.add(tf.keras.layers.Dense(layers[1], activation=\"relu\"))\n    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n    \n    return model\n\ndef best_mlp_fashion():\n    model = Sequential()\n    model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))\n    model.add(tf.keras.layers.Dense(500, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.25))\n    model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    \n    return model\n\n","71413e70":"def evaluate_best_models(permutation=False):\n    # Load and preprocess MNIST data\n    x_train, x_test, y_train, y_test = preprocess(data=\"mnist\", permutation=permutation)\n\n    # Create, train and test best performing MLP model\n    best_mlp_model_mnist = mlp(opt=\"adam\")\n    best_mlp_model_mnist.fit(x_train, y_train, batch_size=256, epochs=10, verbose=0, validation_data=(x_test, y_test))\n    acc1 = best_mlp_model_mnist.evaluate(x_test, y_test, verbose=0)[1]\n    \n    # Create, train and test best performing CNN model\n    best_cnn_model_mnist = cnn(opt=\"adam\")\n    best_cnn_model_mnist.fit(x_train, y_train, batch_size=256, epochs=10, verbose=0, validation_data=(x_test, y_test))\n    acc2 = best_cnn_model_mnist.evaluate(x_test, y_test, verbose=0)[1]\n\n    # Load and preprocess FASHION-MNIST data\n    x_train, x_test, y_train, y_test = preprocess(data=\"fashion_mnist\", permutation=permutation)\n\n    # Create, train and test best performing MLP model\n    best_mlp_model_fashion = best_mlp_fashion()\n    best_mlp_model_fashion.fit(x_train, y_train, batch_size=256, epochs=10, verbose=0, validation_data=(x_test, y_test))\n    acc3 = best_mlp_model_fashion.evaluate(x_test, y_test, verbose=0)[1]\n\n    # Create, train and test best performing CNN model\n    best_cnn_model_fashion = cnn(opt=\"adam\")\n    best_cnn_model_fashion.fit(x_train, y_train, batch_size=256, epochs=10, verbose=0, validation_data=(x_test, y_test))\n    acc4 = best_cnn_model_fashion.evaluate(x_test, y_test, verbose=0)[1]\n    \n    return acc1, acc2, acc3, acc4\n","250637d4":"# Build, train and test best performing models on original datasets\nacc_mlp_mnist, acc_cnn_mnist, acc_mlp_fashion, acc_cnn_fashion = evaluate_best_models(permutation=False)\nprint(f\"Accuracy of best MLP model on mnist on original data: {acc_mlp_mnist}\")\nprint(f\"Accuracy of best CNN model on mnist on original data: {acc_cnn_mnist}\")\nprint(f\"Accuracy of best MLP model on fashion mnist on original data: {acc_mlp_fashion}\")\nprint(f\"Accuracy of best CNN model on fashion mnist on original data: {acc_mlp_fashion}\\n\")\n\nacc_mlp_mnist, acc_cnn_mnist, acc_mlp_fashion, acc_cnn_fashion = evaluate_best_models(permutation=True)\nprint(f\"Accuracy of best MLP model on mnist on permutated data: {acc_mlp_mnist}\")\nprint(f\"Accuracy of best CNN model on mnist on permutated data: {acc_cnn_mnist}\")\nprint(f\"Accuracy of best MLP model on fashion mnist on permutated data: {acc_mlp_fashion}\")\nprint(f\"Accuracy of best CNN model on fashion mnist on permutated data: {acc_cnn_fashion}\\n\")","7eb68531":"Some observations for first part:\n- SGD Optimizer performs very well on small batch sizes\n- SGD Optimizer performs liniary worse on larger batch sizes\n- ADAM performs relatively insensitive for batch size\n- ADAM performs best on batch size of 256, although relatively small differences\n- Increasing complexity of MLP (more layers, more neurons) does not improve score for MNIST, does for FASHION-MNIST\n- Decreasing complexity of MLP gets very bad fast\n- Adding dropout does not improve performance for MNIST, does for FASHION-MNIST\n- Sigmoid performs little bit worse than softmax for last layer\n- Relu performs way better than sigmoid for hidden layers\n- FASHION-MNIST more complex data, needs more complex network\n","3b6d0ac3":"Experiment with both networks, trying various options: initializations, activations,\ntraining algorithms (and their hyperparameters), regularizations (L1, L2, Dropout, no Dropout).\nYou may also experiment with changing the architecture of both networks: adding\/removing layers,\nnumber of convolutional filters, their sizes, etc.","53ba8626":"First, we preprocess the classic mnist data. "}}