{"cell_type":{"c3aeb5fd":"code","53cf00bc":"code","f2967fd1":"code","52551a60":"code","8de633ce":"code","4c7da4e0":"code","810e77ab":"code","2d0e489b":"code","37de1d93":"code","b4d72704":"code","4ae00685":"code","f73531dd":"code","d510301e":"code","fae162a3":"code","88b9434d":"code","979696e9":"code","f0de4836":"code","a78f061d":"code","8bf88aae":"code","855742d2":"code","ca96d22d":"code","d5c54825":"code","4a965006":"code","c646cf4a":"code","6d541bb7":"code","3b48d1eb":"code","4fd54c33":"code","ff376f22":"code","60485a30":"code","b06bbd00":"code","c08655fe":"code","ab7faa3c":"code","cf58e93b":"code","800cf7bc":"code","5647adfa":"code","237bf5a3":"code","19644ad4":"markdown","719a82f4":"markdown","57206e24":"markdown","6f91b380":"markdown","4bd0ea6c":"markdown","d0a6a480":"markdown","de4695cd":"markdown","59eeacd2":"markdown","530c0f10":"markdown","797d567a":"markdown","2ebfdd42":"markdown","c9df6208":"markdown","5aff4ff3":"markdown"},"source":{"c3aeb5fd":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom wordcloud import WordCloud\n\nfrom sklearn.metrics import roc_auc_score\n\n\n\n!pip install contractions\nimport contractions\n\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import wordnet as wn\nfrom nltk import pos_tag\nfrom collections import defaultdict\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","53cf00bc":"df_train = pd.read_csv(\"\/kaggle\/input\/emotions-dataset-for-nlp\/train.txt\",sep=';',names=['text','emotion'])\ndf_val = pd.read_csv(\"\/kaggle\/input\/emotions-dataset-for-nlp\/val.txt\",sep=';',names=['text','emotion'])\ndf_test = pd.read_csv(\"\/kaggle\/input\/emotions-dataset-for-nlp\/test.txt\",sep=';',names=['text','emotion'])","f2967fd1":"df_train.emotion.value_counts()","52551a60":"data_total = [df_train,df_val,df_test]\n\n    ","8de633ce":"\nfig,ax = plt.subplots(1,3,figsize=(20,5))\nfor i,data in enumerate(data_total):\n    sns.countplot(data.emotion,ax=ax[i])","4c7da4e0":"def get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)","810e77ab":"\ntag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\n","2d0e489b":"stop_words = set(stopwords.words('english')) \nstem = WordNetLemmatizer ()\ndef clean_text(text):\n    text = text.lower()\n    text = word_tokenize(text)\n    text = [contractions.fix(word) for word in text]\n    text = [stem.lemmatize(w,tag_map[tag[0]]) for w, tag in pos_tag(text) if w not in stop_words]\n    return \" \".join(text)\n    \n    ","37de1d93":"for i,data in enumerate(data_total):\n    data.text = data.text.apply(clean_text)","b4d72704":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 \n).generate(str(data))\n    return wordcloud\n\n","4ae00685":"categories = df_train[\"emotion\"].unique()\n\n","f73531dd":"\n\nfig, axes = plt.subplots(ncols=2, nrows=3,figsize=(30,25))\nplt.axis('off')\nfor category, ax in zip(categories, axes.flat):\n    wordcloud = show_wordcloud(df_train[df_train[\"emotion\"]==category]['text'])\n    ax.imshow(wordcloud)\n    ax.title.set_text(category)\n    ax.axis('off')\nplt.subplots_adjust(wspace=0.05, hspace=0.01)\n\n","d510301e":"laberEncoder = LabelEncoder()\nlaberEncoder.fit(df_train.emotion)\nfor i,data in enumerate(data_total):\n    data.emotion = laberEncoder.transform(data.emotion)","fae162a3":"vectorizer = TfidfVectorizer(max_features=1000,ngram_range=(1, 2))\nvectors = vectorizer.fit_transform(df_train.text)\nfeature_names = vectorizer.get_feature_names()\ndense = vectors.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(denselist, columns=feature_names)","88b9434d":"vectors = vectorizer.transform(df_val.text)\ndense = vectors.todense()\ndenselist = dense.tolist()\ndf_valfeature = pd.DataFrame(denselist, columns=feature_names)\n\n","979696e9":"class EnsembleHelper(object):\n    def __init__(self,models,seed=42,params=None,cv=5):\n        self.seed = seed\n        self.params = params\n        \n        \n    def gridSearchCV(self,estimator,params,X_train,Y_train):\n        if not params :\n            params = {}\n        grid = GridSearchCV(estimator=estimator, param_grid=params, cv=5)\n        grid.fit(X_train,Y_train)\n        rf_best = grid.best_estimator_\n        print(estimator,grid.best_params_,grid.best_score_,sep=\"|\")\n        return rf_best\n        \n    \n    def fit(self, X_train,Y_train):\n        ensemblemodel = []\n        for key , estimator in models:\n            Dict = {search_key.split(\"__\")[1]:val for search_key, val in self.params.items() if search_key.startswith(key)}\n            bestmodel = self.gridSearchCV(estimator,Dict,X_train,Y_train)\n            final = (key , bestmodel)\n            ensemblemodel.append(final)\n        \n        self.clf = VotingClassifier(estimators=ensemblemodel,voting='hard')\n        self.clf.fit(X_train,Y_train)\n        \n        \n    def predict(self,X_test):\n        predictions = self.clf.predict(X_test)\n        return predictions\n        \n    def score(self,X_test,Y_test):\n        return self.clf.score(X_test,Y_test)    \n        \n        ","f0de4836":"models = [\n    ('lr' , LogisticRegression(random_state=0,max_iter=10000)),\n    ('sgd', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=10, tol=None)),\n    ('mnb', MultinomialNB()),\n    ('nb',BernoulliNB()),\n    ('dt',DecisionTreeClassifier())\n]\n\nparams = {'lr__C': [0.1,1.0,10.0],\n          'mnb__alpha': np.linspace(0.0, 1.0, 5),\n          'nb__alpha': np.linspace(0.0, 1.0, 5),\n          'dt__criterion' : ['gini', 'entropy']}\n","a78f061d":"ensemble = EnsembleHelper(models,cv=5,params=params)\nensemble.fit(df, df_train.emotion)\npredict = ensemble.predict(df_valfeature)\nprint(\"Ensemble score : \",ensemble.score(df_valfeature,df_val.emotion))","8bf88aae":"from gensim.models import word2vec\nimport tqdm","855742d2":"num_features = 5000  \nmin_word_count = 2 \nnum_workers = 4    \ncontext = 10        \ndownsampling = 1e-3 \n\n\nword2VecModel = word2vec.Word2Vec(df_train.text,\\\n                          workers=num_workers,\\\n                          size=num_features,\\\n                          min_count=min_word_count,\\\n                          window=context,\n                          sample=downsampling)\n\n\nword2VecModel.init_sims(replace=True)\n\n\n#word2VecModel_name = \"emotionsword2Vec\"\n#word2VecModel.save(word2VecModel_name)\n","ca96d22d":"\n\ndef document_vector(doc):\n    \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n    doc = [word for word in doc if word in word2VecModel.wv.vocab]\n    return np.mean(word2VecModel[doc], axis=0)\n\n","d5c54825":"train_vec = df_train.text.apply(document_vector)\nval_vec = df_val.text.apply(document_vector)","4a965006":"len(train_vec[0])","c646cf4a":"models = [\n    ('lr' , LogisticRegression(random_state=0,max_iter=10000)),\n    ('sgd', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=10, tol=None))\n]\n\nparams = {'lr__C': [0.1,1.0,10.0],\n          'dt__criterion' : ['gini', 'entropy']}","6d541bb7":"ensemble = EnsembleHelper(models,cv=5,params=params)\nensemble.fit(list(train_vec), df_train.emotion)\npredict = ensemble.predict(list(val_vec))\nprint(\"Ensemble score : \",ensemble.score(list(val_vec),df_val.emotion))","3b48d1eb":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom sklearn import utils","4fd54c33":"def tokenize_text(text):\n    tokens = []\n    for word in word_tokenize(text):\n        if len(word) < 2:\n            continue\n        tokens.append(word.lower())\n    return tokens\n\ndef vector_for_learning(model, input_docs):\n    sents = input_docs\n    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n    return targets, feature_vectors","ff376f22":"train_documents = [TaggedDocument(words=tokenize_text(x), tags=[y] ) for x, y in zip(df_train['text'], df_train['emotion'])]\nval_documents = [TaggedDocument(words=tokenize_text(x), tags=[y] ) for x, y in zip(df_val['text'], df_val['emotion'])]","60485a30":"model_dbow = Doc2Vec(dm=1, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=4, alpha=0.025, min_alpha=0.001)\nmodel_dbow.build_vocab([x for x in train_documents])\ntrain_documents  = utils.shuffle(train_documents)\nmodel_dbow.train(train_documents,total_examples=len(train_documents), epochs=30)\n\n","b06bbd00":"y_train, X_train = vector_for_learning(model_dbow, train_documents)\ny_test, X_test = vector_for_learning(model_dbow, val_documents)","c08655fe":"models = [\n    ('lr' , LogisticRegression(random_state=0,max_iter=10000)),\n    ('sgd', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=10, tol=None)),\n    ('dt',DecisionTreeClassifier())\n]\n\nparams = {'lr__C': [0.1,1.0,10.0],\n          'dt__criterion' : ['gini', 'entropy']}","ab7faa3c":"ensemble = EnsembleHelper(models,cv=5,params=params)\nensemble.fit(X_train, y_train)\npredict = ensemble.predict(X_test)\nprint(\"Ensemble score : \",ensemble.score(X_test,y_test))","cf58e93b":"from numpy import array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.merge import concatenate\nfrom keras import regularizers","800cf7bc":"class DLModel(object):\n    def __init__(self,X_train,Y_train):\n        self.X_train = X_train\n        self.Y_train = Y_train\n        self.output = len(Y_train.unique())\n        self.length = max([len(s.split()) for s in self.X_train])\n        self.tokenizer = self.create_tokenizer(X_train)\n        self.vocab_size = len(self.tokenizer.word_index) + 1\n        self.model = self.define_model(self.length, self.vocab_size)\n        \n     \n    def preprocessing(self,X_train):\n        tokenizer = self.create_tokenizer(X_train)\n        encoded = tokenizer.texts_to_sequences(X_train)\n        padded = pad_sequences(encoded, maxlen=self.length, padding='post')\n        return padded\n        \n    def create_tokenizer(self,sentences):\n        tokenizer = Tokenizer()\n        tokenizer.fit_on_texts(sentences)\n        return tokenizer\n\n    \n    # define the model\n    def define_model(self,length, vocab_size):\n        # channel 1\n        inputs1 = Input(shape=(length,))\n        embedding1 = Embedding(vocab_size, 100)(inputs1)\n        conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n        drop1 = Dropout(0.5)(conv1)\n        pool1 = MaxPooling1D(pool_size=2)(drop1)\n        flat1 = Flatten()(pool1)\n        # channel 2\n        inputs2 = Input(shape=(length,))\n        embedding2 = Embedding(vocab_size, 100)(inputs2)\n        conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n        drop2 = Dropout(0.5)(conv2)\n        pool2 = MaxPooling1D(pool_size=2)(drop2)\n        flat2 = Flatten()(pool2)\n        # channel 3\n        inputs3 = Input(shape=(length,))\n        embedding3 = Embedding(vocab_size, 100)(inputs3)\n        conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n        drop3 = Dropout(0.5)(conv3)\n        pool3 = MaxPooling1D(pool_size=2)(drop3)\n        flat3 = Flatten()(pool3)\n        # merge\n        merged = concatenate([flat1, flat2, flat3])\n        # interpretation\n        dense1 = Dense(20,kernel_regularizer=regularizers.l2(0.001), activation='relu')(merged)\n        drop4 = Dropout(0.5)(dense1)\n        dense2 = Dense(10,kernel_regularizer=regularizers.l2(0.001), activation='relu')(drop4)\n        drop5 = Dropout(0.5)(dense2)\n        outputs = Dense(self.output, activation='sigmoid')(drop5)\n        model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n        # compile\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n        # summarize\n        print(model.summary())\n        plot_model(model, show_shapes=True, to_file='multichannel.png')\n        return model\n    \n    def fit(self,X_train,Y_train,X_val,Y_val):\n        trainX = self.preprocessing(X_train)\n        valX = self.preprocessing(X_val)\n        self.model.fit([trainX,trainX,trainX], pd.get_dummies(Y_train),validation_data=([valX,valX,valX], pd.get_dummies(Y_val)), epochs=20, batch_size=16)\n        \n    def evaluate(self,X_test,Y_test):\n        testX = self.preprocessing(X_test)\n        loss, acc = self.model.evaluate([testX,testX,testX], pd.get_dummies(Y_test), verbose=0)\n        return loss, acc\n    \n    def predict(self,X_test):\n        X_test = self.preprocessing(X_test)\n        ynew = self.model.predict(X_test)\n        return ynew\n        ","5647adfa":"model = DLModel(df_train.text,df_train.emotion)\nmodel.fit(df_train.text,df_train.emotion,df_val.text,df_val.emotion)\n","237bf5a3":"loss, acc = model.evaluate(df_test.text,df_test.emotion)\nprint(loss, acc)","19644ad4":"# Vectorizing Dependent text","719a82f4":"# Analyzing dependent Text Data","57206e24":"# Encoding Target Variable ","6f91b380":" * # Using Word2Vec","4bd0ea6c":"* # Using Doc2Vec","d0a6a480":"# Predicting Data Using Machine Learning\n\n* # Using TF-IDF","de4695cd":"         \n   ![](https:\/\/s01.sgp1.cdn.digitaloceanspaces.com\/article\/107360-byxhzxvmlb-1544009839.jpg)","59eeacd2":"# Predicting Data Using Deep Learning","530c0f10":"# Cleaning the Data Set","797d567a":"#  ***Thank you For Reading . Please ,Upvote Comment and Follow to inspire  me***\nWork In Progress More to Come ","2ebfdd42":"# Analyzing the Target variable","c9df6208":"![](https:\/\/image.shutterstock.com\/image-vector\/thanks-vector-letter-260nw-1105434290.jpg)","5aff4ff3":"# Reading the Files"}}