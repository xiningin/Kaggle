{"cell_type":{"d0e0e5cb":"code","6ed7c964":"code","2fe965d7":"code","f3e3c06b":"code","03c880d6":"code","0d4f3c4a":"code","7a4e3e2b":"code","60e04fe1":"code","0d63fda4":"code","8d2f868a":"code","77ebf588":"markdown","ed646202":"markdown","bbe7cc51":"markdown","f8d295da":"markdown","c4f00f4b":"markdown","86154a4f":"markdown","9d094ba5":"markdown","32db43bf":"markdown","78458a35":"markdown"},"source":{"d0e0e5cb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport keras\nfrom keras.layers import Dense, Dropout, Input\nfrom keras.models import Model,Sequential\nfrom keras.datasets import mnist\nfrom tqdm import tqdm\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.optimizers import adam","6ed7c964":"def load_data():\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    x_train = (x_train.astype(np.float32) - 127.5)\/127.5\n    \n    # convert shape of x_train from (60000, 28, 28) to (60000, 784) \n    # 784 columns per row\n    x_train = x_train.reshape(60000, 784)\n    return (x_train, y_train, x_test, y_test)\n(X_train, y_train,X_test, y_test)=load_data()\nprint(X_train.shape)","2fe965d7":"def adam_optimizer():\n    return adam(lr=0.0002, beta_1=0.5)","f3e3c06b":"in_dim = 100\ndef create_generator():\n    generator=Sequential()\n    generator.add(Dense(units=256,input_dim=in_dim))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=512))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=1024))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=784, activation='tanh'))\n    \n    generator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n    return generator\ng=create_generator()\ng.summary()\n","03c880d6":"def create_discriminator():\n    discriminator=Sequential()\n    discriminator.add(Dense(units=1024,input_dim=784))\n    discriminator.add(LeakyReLU(0.2))\n    discriminator.add(Dropout(0.3))\n       \n    \n    discriminator.add(Dense(units=512))\n    discriminator.add(LeakyReLU(0.2))\n    discriminator.add(Dropout(0.3))\n       \n    discriminator.add(Dense(units=256))\n    discriminator.add(LeakyReLU(0.2))\n    \n    discriminator.add(Dense(units=1, activation='sigmoid'))\n    \n    discriminator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n    return discriminator\nd =create_discriminator()\nd.summary()","0d4f3c4a":"def create_gan(discriminator, generator):\n    discriminator.trainable=False\n    gan_input = Input(shape=(in_dim,))\n    x = generator(gan_input)\n    gan_output= discriminator(x)\n    gan= Model(inputs=gan_input, outputs=gan_output)\n    gan.compile(loss='binary_crossentropy', optimizer='adam')\n    return gan\ngan = create_gan(d,g)\ngan.summary()","7a4e3e2b":"def plot_generated_images(epoch, generator, examples=100, dim=(10,10), figsize=(10,10)):\n    noise= np.random.normal(loc=0, scale=1, size=[examples, in_dim])\n    generated_images = generator.predict(noise)\n    generated_images = generated_images.reshape(100,28,28)\n    plt.figure(figsize=figsize)\n    for i in range(generated_images.shape[0]):\n        plt.subplot(dim[0], dim[1], i+1)\n        plt.imshow(generated_images[i], interpolation='nearest')\n        plt.axis('off')\n    plt.tight_layout()\n    plt.savefig('gan_generated_image %d.png' %epoch)","60e04fe1":"\ndef training(epochs=1, batch_size=128):\n    \n        #Loading the data\n    batch_size=128\n    (X_train, y_train, X_test, y_test) = load_data()\n    batch_count = X_train.shape[0] \/ batch_size\n\n    # Creating GAN\n    generator= create_generator()\n    discriminator= create_discriminator()\n    gan = create_gan(discriminator, generator)\n    \n    for e in range(1,epochs+1 ):\n        print(\"Epoch %d\" %e)\n        for _ in tqdm(range(batch_size)):\n        #generate  random noise as an input  to  initialize the  generator\n            noise= np.random.normal(0,1, [batch_size, in_dim])\n#             print(noise.shape)\n            # Generate fake MNIST images from noised input\n            generated_images = generator.predict(noise)\n            \n            # Get a random set of  real images\n            image_batch =X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]\n            \n            #Construct different batches of  real and fake data \n            X= np.concatenate([image_batch, generated_images])\n            \n            # Labels for generated and real data\n            y_dis=np.zeros(2*batch_size)\n            y_dis[:batch_size]=0.9\n            \n            #Pre train discriminator on  fake and real data  before starting the gan. \n            discriminator.trainable=True\n            discriminator.train_on_batch(X, y_dis)\n            \n            #Tricking the noised input of the Generator as real data\n            noise= np.random.normal(0,1, [batch_size, in_dim])\n            y_gen = np.ones(batch_size)\n#             print(noise.shape,y_gen.shape)\n#             print(noise[0])\n#             print(y_gen[0])\n            # During the training of gan, \n            # the weights of discriminator should be fixed. \n            #We can enforce that by setting the trainable flag\n            discriminator.trainable=False\n#             print(noise.shape)\n            #training  the GAN by alternating the training of the Discriminator \n            #and training the chained GAN model with Discriminator\u2019s weights freezed.\n            gan.train_on_batch(noise, y_gen)\n            \n        if e == 1 or e % 20 == 0:\n           \n            plot_generated_images(e, generator)\n    generator.save(\"generator.h5\")\n    discriminator.save(\"discriminator.h5\")\n    gan.save(\"gan.h5\")\n    \n","0d63fda4":"training(400,1024)","8d2f868a":"!ls","77ebf588":"We will use Adam optimizer as it is computationally efficient and has very little memory requirement. Adam is a combination of Adagrad and RMSprop.","ed646202":"We create Generator which uses MLP using simple dense layers activated by tanh","bbe7cc51":"Let\u2019s import the required libraries\n\n","f8d295da":"We finally start to train GAN. We will first have the full code for training GAN and then break it step by step for understanding how the training happens","c4f00f4b":"We now create the GAN where we combine the Generator and Discriminator. When we train the generator we will freeze the Discriminator.\n\nWe will input the noised image of shape 100 units to the Generator. The output generated from the Generator will be fed to the Discriminator.","86154a4f":"Loading the data from mnist dataset. we create a function load_data() function\n\n","9d094ba5":"Before we start training the model, we will write a function plot_generated_images to plot the generated images. This way we can see how the images are generated. We save the generated images to file that we can view later","32db43bf":"Let\u2019s visualize the GAN architecture that we plan to build\n\n![gd](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*Sqhji7Zz4IK2HDgCOabhXQ.png)","78458a35":"We now create the Discriminator which is also MLP. Discriminator will take the input from real data which is of the size 784 and also the images generated from Generator."}}