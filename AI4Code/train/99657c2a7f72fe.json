{"cell_type":{"cbdf587e":"code","31945c1a":"code","a84f375a":"code","14466e50":"code","afc3a563":"code","774743d4":"code","515a7da5":"code","054c5f0f":"code","ff007073":"code","1f80a7de":"code","1b75e60c":"code","459417e5":"code","29a389f3":"code","7cb818d8":"code","379f2922":"code","e9dd1917":"code","0aadf013":"code","a0ba01ff":"code","0ed17cc6":"code","81c56069":"code","dff6cf65":"code","af11f279":"code","6e38c546":"code","49396be3":"code","140422fa":"code","c474c522":"code","8200a40d":"code","b67a0a36":"code","b8896450":"code","2c984b66":"code","45f8ca4c":"code","ce2994a7":"code","4f000ebb":"code","0cc2b19b":"code","926fff52":"code","a9ea7fa0":"code","ffd092a1":"code","8bd53b07":"code","db3c765e":"code","c3074cae":"code","67d0a48e":"code","b26072e0":"code","5917e270":"code","4efabaa7":"code","d7dba7a1":"code","de6cfc45":"code","e680cffb":"code","17ace21a":"code","01e64fd4":"code","ab219a1b":"code","b94441ca":"code","9f72f930":"code","4a55a660":"code","3ad34b60":"code","1886398c":"markdown","84e633f4":"markdown","5a91e376":"markdown","c76f863b":"markdown","b8991c40":"markdown","1e8d87ae":"markdown","358e3537":"markdown","cc3eca76":"markdown","b452f144":"markdown","dc7dfe07":"markdown","c90c1693":"markdown","1709c9bd":"markdown","40425198":"markdown","cfc4bb82":"markdown","676aa900":"markdown","89632d04":"markdown","5dccd00c":"markdown","3fdb3335":"markdown","2c8be3f1":"markdown","824f9aa5":"markdown","96ae7738":"markdown","68d205f4":"markdown","2ad0d2c6":"markdown","84e74307":"markdown","e32dd95e":"markdown","5faa8bc3":"markdown","049b8e2e":"markdown","fc1aebdc":"markdown","b87a4cf2":"markdown","a8fb0634":"markdown","5bcd60f5":"markdown","71dddfee":"markdown","cfcbe4e7":"markdown"},"source":{"cbdf587e":"import pandas as pd\nimport numpy as np","31945c1a":"df = pd.read_csv(\"..\/input\/dataset.csv\")\ndf.head()","a84f375a":"df[\"timestamp\"] = pd.to_datetime(df.timestamp)","14466e50":"n_users = df.user_id.nunique()\nn_items = df.item_id.nunique()\n\nprint(\"Number of unique users : \" + str(n_users))\nprint(\"Number of unique items : \" + str(n_items))","afc3a563":"df.item_category.value_counts()","774743d4":"df.user_age.describe()","515a7da5":"df.groupby(\"item_category\").user_age.mean().sort_values()","054c5f0f":"import matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.figure(figsize=(20,10))\nplt.title(\"Popularity of items (long tail)\")\nplt.plot(range(n_items), df.item_id.value_counts())\nplt.xlabel(\"Items\")\nplt.ylabel(\"Number of views\")","ff007073":"df.item_id.value_counts().head(5)","1f80a7de":"%matplotlib inline\nplt.figure(figsize=(20,10))\nplt.title(\"Activity of the users (long tail)\")\nplt.plot(range(n_users), df.user_id.value_counts())\nplt.xlabel(\"Users\")\nplt.ylabel(\"Activity of users\")","1b75e60c":"df.user_id.value_counts().head(10)","459417e5":"target = \"nb_views\"\ninter = df.groupby([\"user_id\",\"user_age\" ,\"item_id\", \"item_category\"]).count().rename(columns={\"timestamp\":target}).reset_index()","29a389f3":"inter.sample(10)","7cb818d8":"inter.shape","379f2922":"from sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import *\n\nX_train, X_test = train_test_split(inter, test_size=0.2, random_state = 2018)","e9dd1917":"print(\"Number of USERS in the TRAIN set: \" + str(X_train.user_id.nunique()))\nprint(\"Number of ITEMS in the TRAIN set: \" + str(X_train.item_id.nunique()))","0aadf013":"print(\"Number of USERS in the VALIDATION set: \" + str(X_test.user_id.nunique()))\nprint(\"Number of ITEMS in the VALIDATION set: \" + str(X_test.item_id.nunique()))","a0ba01ff":"X_train.shape, X_test.shape","0ed17cc6":"def apk(actual, predicted, k=10): \n\n    if len(predicted)>k: \n        predicted = predicted[:k] \n        \n    score = 0.0 \n    num_hits = 0.0 \n \n    for i,p in enumerate(predicted): \n        if p in actual and p not in predicted[:i]: \n            num_hits += 1.0 \n            score += num_hits \/ (i+1.0) \n \n    if not actual: \n        return 0.0 \n \n    return score \/ min(len(actual), k) ","81c56069":"model = dict(X_train.groupby(\"item_category\").nb_views.mean())\nmodel","dff6cf65":"print(\"MAE (train) : \" + str(mean_absolute_error(X_train[target], \n                                                 X_train[\"item_category\"].apply(lambda x: model[x]))))","af11f279":"print(\"MAE (test) : \" + str(mean_absolute_error(X_test[target], \n                                                X_test[\"item_category\"].apply(lambda x: model[x]))))","6e38c546":"#R = pd.DataFrame([], index=df.user_id.unique(), columns=df.item_id.unique())","49396be3":"#for (user, item, inter) in X_train[[\"user_id\",\"item_id\", target]].values:\n    #R.loc[user,item] = inter","140422fa":"def runALS(A, R, n_factors, n_iterations, lambda_):\n    '''\n    Runs Alternating Least Squares algorithm in order to calculate matrix.\n    :param A: User-Item Matrix with ratings\n    :param R: User-Item Matrix with 1 if there is a rating or 0 if not\n    :param n_factors: How many factors each of user and item matrix will consider\n    :param n_iterations: How many times to run algorithm\n    :param lambda_: Regularization parameter\n    :return:\n    '''\n    (n, m) = A.shape\n    Users = 5 * np.random.rand(n, n_factors)\n    Items = 5 * np.random.rand(n_factors, m)\n\n    def get_error(A, Users, Items, R):\n        # This calculates the MSE of nonzero elements\n        return np.sum((R * (A - np.dot(Users, Items))) ** 2) \/ np.sum(R)\n\n    MSE_List = []\n\n    print(\"Starting Iterations\")\n    for iter in range(n_iterations):\n        print(iter)\n        \n        print(\"solving user matrix\")\n        for i, Ri in enumerate(R):\n            print(i)\n            Users[i] = np.linalg.solve(np.dot(Items, np.dot(np.diag(Ri), Items.T)) + lambda_ * np.eye(n_factors),\n                                       np.dot(Items, np.dot(np.diag(Ri), A[i].T))).T\n\n        print(\"solving item matrix\")\n        for j, Rj in enumerate(R.T):\n            print(j)\n            Items[:,j] = np.linalg.solve(np.dot(Users.T, np.dot(np.diag(Rj), Users)) + lambda_ * np.eye(n_factors),\n                                     np.dot(Users.T, np.dot(np.diag(Rj), A[:, j])))\n        \n    return Users, Items","c474c522":"#(Users, Items) = runALS(R.fillna(0).values, (R>0).applymap(int).values, n_factors=3, n_iterations=1, lambda_=0.1)","8200a40d":"from keras.layers.core import Dense, Reshape, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import concatenate, Input\nfrom keras.models import Model","b67a0a36":"item_map = {}\nfor i, item_id in enumerate(inter.item_id.unique()):\n    item_map[item_id] = i\n    \nuser_map = {}\nfor i, user_id in enumerate(inter.user_id.unique()):\n    user_map[user_id] = i","b8896450":"from sklearn.preprocessing import LabelEncoder\nenc_cat = LabelEncoder()\nenc_cat.fit(inter.item_category)","2c984b66":"# parameters\nn_emb_user = 10\nn_emb_item = 30\nunits = 10\ndrop = 0.2\n\nn_epochs = 5\nbatch = 256\n\n# input layers\nuser_input = Input(shape=(1,))\nuser_age_input = Input(shape=(1,))\nitem_input = Input(shape=(1,))\nitem_cat_input = Input(shape=(1,))\n\ninputs  = [user_input, user_age_input, item_input, item_cat_input]\n\n# embedding layers\nuser_emb = Embedding(input_dim=n_users, output_dim=n_emb_user, input_length=1)(user_input)\nuser_emb = Reshape(target_shape=(n_emb_user,))(user_emb)\n\nitem_emb = Embedding(input_dim=n_items, output_dim=n_emb_item, input_length=1)(item_input)\nitem_emb = Reshape(target_shape=(n_emb_item,))(item_emb)\n\nemb_layer = concatenate([user_emb, item_emb])\n\n# dense layers\nlay = Dense(units, activation='relu')(emb_layer)\nlay = Dropout(drop)(lay)\n\n# output layer\noutputs = Dense(1, kernel_initializer='normal')(lay)\n\n# create the NN\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])","45f8ca4c":"model.summary()","ce2994a7":"model.fit([X_train.user_id.apply(lambda x: user_map[x]).values, \n           X_train.user_age.values,\n           X_train.item_id.apply(lambda x: item_map[x]).values, \n           enc_cat.transform(X_train.item_category), \n          ], \n          X_train[target].values, \n          epochs=n_epochs, \n          batch_size=batch, \n          verbose=1,\n          validation_data=([X_test.user_id.apply(lambda x: user_map[x]).values, \n                            X_test.user_age.values,\n                            X_test.item_id.apply(lambda x: item_map[x]).values, \n                            enc_cat.transform(X_test.item_category), \n                           ], \n                           X_test[target].values)\n          )","4f000ebb":"def NN_predict(X):\n\n    pred = model.predict([X.user_id.apply(lambda x: user_map[x]).values, \n                          X.user_age.values,\n                          X.item_id.apply(lambda x: item_map[x]).values,\n                          enc_cat.transform(X.item_category)\n                         ])\n\n    results = pd.DataFrame([X.user_id.values, \n                            X.user_age.values,\n                            X.item_id.values, \n                            X.item_category.values, \n                            X[target].values, \n                            pred[:,0]]).T\n    \n    results.columns = [\"user_id\", \"user_age\", \"item_id\", \"item_category\", target, \"predictions\"]\n    \n    return results","0cc2b19b":"results_test = NN_predict(X_test)\nresults_train = NN_predict(X_train)","926fff52":"results_test.sample(10)","a9ea7fa0":"print(\"MAE (train) : \" + str(mean_absolute_error(results_train[target], results_train.predictions)))","ffd092a1":"print(\"MAE (test) : \" + str(mean_absolute_error(results_test[target], results_test.predictions)))","8bd53b07":"model.fit([inter.user_id.apply(lambda x: user_map[x]).values, \n           inter.user_age.values,\n           inter.item_id.apply(lambda x: item_map[x]).values, \n           enc_cat.transform(inter.item_category), \n          ], \n          inter[target].values, \n          epochs=n_epochs, \n          batch_size=batch, \n          verbose=0,\n          )","db3c765e":"user_emb = pd.DataFrame(model.get_weights()[0], index = inter.user_id.unique())\nuser_emb.head()","c3074cae":"user_emb.shape","67d0a48e":"from sklearn.neighbors import NearestNeighbors\n\nk = 5\nknn = NearestNeighbors(k+1)\nknn.fit(user_emb)","b26072e0":"import random\n\nnp.random.seed(8)\nuser_id = np.random.choice(df.user_id.unique(),1)[0]","5917e270":"inter[inter.user_id==user_id]","4efabaa7":"similar_users = user_emb.index[knn.kneighbors([user_emb.loc[user_id]])[1][0]]\nsimilar_users","d7dba7a1":"i = 4\ninter[inter.user_id==similar_users[i]]","de6cfc45":"from sklearn.decomposition import PCA\nproj = PCA(n_components=2)\nuser_emb_proj = pd.DataFrame(proj.fit_transform(user_emb), columns = [\"proj1\", \"proj2\"], index=user_emb.index)\n\nplt.figure(figsize=(20,10))\nplt.plot(user_emb_proj.values[:,0], user_emb_proj.values[:,1],'o',color='b')","e680cffb":"item_emb = pd.DataFrame(model.get_weights()[1], index = inter.item_id.unique())\nitem_emb.head()","17ace21a":"item_emb.shape","01e64fd4":"k = 5\nknn = NearestNeighbors(k+1)\nknn.fit(item_emb)","ab219a1b":"np.random.seed(23)\nitem_id = np.random.choice(df.item_id.unique(),1)[0]","b94441ca":"inter[inter.item_id==item_id]","9f72f930":"similar_items = item_emb.index[knn.kneighbors([item_emb.loc[item_id]])[1][0]]\nsimilar_items","4a55a660":"i = 2\ninter[inter.item_id==similar_items[i]]","3ad34b60":"from sklearn.decomposition import PCA\nproj = PCA(n_components=2)\nitem_emb_proj = pd.DataFrame(proj.fit_transform(item_emb), columns = [\"proj1\", \"proj2\"], index=item_emb.index)\n\nplt.figure(figsize=(20,10))\nplt.plot(item_emb_proj.values[:,0], item_emb_proj.values[:,1],'o', color='r')","1886398c":"For each user, you need to score online all the possible items, rank them and propose k personnalized items (and eventually apply a business rule on the top) ","84e633f4":"## Outline ","5a91e376":"## a) Content based ","c76f863b":"### Displaying some stats","b8991c40":"# 3) Implement and validate different methods","1e8d87ae":"# 1) The settings","358e3537":"please read : https:\/\/arxiv.org\/pdf\/1111.1797.pdf","cc3eca76":"**for the users**","b452f144":"compile the NN ","dc7dfe07":"### matrix reshaping : get implicit interactions","c90c1693":"## 5) multi armed bandit","1709c9bd":"## c) Hybrid approach using neural networks","40425198":"- Let's set \"the average watch time of the recommended videos\" as our online metric.\n\n- X = \"watch time of a recommended video\" with mean $\\mu$ and variance $\\sigma$ unknowns. For two different recsys 1 and 2 applied on two different populations of size $n_{1}$ et $n_{2}$, we measure X. We observe $m_{1}$ and $m_{2}$ the means on the 2 populations, and $s_{1}$ and $s_{2}$ the variances.\n\n- CL theorem on X : the variable \"the average watch time of the recommended videos\" : $\\overline{X} = \\frac{1}{n}\\sum\\limits_{i=1}^n X_{i}$ converge to a Gaussian distribution with mean $\\mu$ and variance $\\frac{\\sigma}{\\sqrt{n}}$    \n\n- Also the random variable $z=\\frac{m_{1}-m_{2}}{\\sqrt{\\frac{{s_{1}}\u00b2}{n_{1}-1} + \\frac{{s_{2}}\u00b2}{n_{2}-1}}}$  is a Student distribution under $H_{0}$\n\n- $H_{0} : \\mu_{1} = \\mu_{2}$ and $\\alpha = 0.05$ (risk of rejecting $H_{0}$ when $H_{0}$ is true) : we compute the p-value = probability of rejecting $H_{0}$ when $H_{0}$ is true.\n\n- If p < $\\alpha$, we can reject $H_{0}$ and affirm there exists a difference between recsys1 and recsys 2 ","cfc4bb82":"In our example, we can set : \n\n- K = number of items (or K = number of RecSys)\n- T : we do it online \n- r (reward) : 0 or 1 if the user has watched the recommended item\n- we can run the Thompson sampling for each users... ","676aa900":"# 2) Set up the evaluation metrics","89632d04":"Just fit a regression model on the item category to predict the number of views...","5dccd00c":"#### A&B test methodology","3fdb3335":"**test**","2c8be3f1":"- implicit : https:\/\/github.com\/benfred\/implicit\n- LightFM : https:\/\/github.com\/lyst\/lightfm\n- Spotlight : https:\/\/github.com\/maciejkula\/spotlight","824f9aa5":"**for the items**","96ae7738":"# References (python package for recsys)","68d205f4":"### reading","2ad0d2c6":"**test**","84e74307":"# Udacity workshop on Recommendation Systems","e32dd95e":"#### Serving","5faa8bc3":"mapping indexes for users and items","049b8e2e":"### Offline metric : Mean Absolute Error (eventually MAP@10)","fc1aebdc":"- Set up the settings : user, item and interaction matrix \n- Set up the evaluation metrics\n- Implement and validate different methods\n- Get an embedding of the items and the users \n- Implement a multi-armed bandit using Thompson sampling ","b87a4cf2":"encoding item category","a8fb0634":"### Online metric ","5bcd60f5":"# 4) Get an embedding of the users and the items","71dddfee":"## b) Collaborative filtering ","cfcbe4e7":"**reminder : AP@k**"}}