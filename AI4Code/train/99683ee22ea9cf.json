{"cell_type":{"df956d22":"code","c782ab13":"code","13142a43":"code","97609cdb":"code","078bb17f":"code","b15587cd":"code","1d9bed66":"code","f903fe4a":"code","892940f0":"code","e9f0fa13":"code","725b8cd1":"code","c2893be1":"code","ee22b2c9":"markdown","1f7a7076":"markdown","5d21bf9e":"markdown","c1495f6b":"markdown","c4eeb674":"markdown"},"source":{"df956d22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c782ab13":"train_X = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest_X = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\")","13142a43":"# Any duplicates? - Yes, drop them all - we won't make any assumption which one is right\ndupes = train_X.drop(['id','target'], axis=1).duplicated(keep=False)\nprint(dupes.value_counts())\ndropthese = list(dupes[dupes == True].index)\ntrain_X = train_X.drop(dropthese)","97609cdb":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold # For creating folds\nfrom sklearn.metrics import log_loss # Evaluation metrics\n\nimport optuna","078bb17f":"train_X[\"kfold\"] = -1\ndf = train_X.sample(frac=1,random_state=14000605).reset_index(drop=True)\ny = df.target\nkf = StratifiedKFold(n_splits=5)\nfor f, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n  df.loc[v_,\"kfold\"] = f","b15587cd":"# My previous best guess - redone with stratified k-fold.\nmodel_lgb = LGBMClassifier(\n    objective = 'multiclass',\n    reg_lambda = 10,\n    learning_rate = 0.1,\n    max_depth = 4,\n    seed = 14000605,\n    colsample_bytree = 0.5,\n    subsample = 0.9,\n    is_unbalance = True\n    )\nlogloss = []\nlgbm_pred = 0\nfor f in range(5): # Looping around 5 folds\n    \n    #Splitting the data into train and validation set\n    train = df[df.kfold!= f].reset_index(drop=True) \n    valid = df[df.kfold== f].reset_index(drop=True)\n    \n    #Creating X_train and y_train\n    X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_train = train.target\n    X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_valid = valid.target\n    X_test = test_X.drop([\"id\"], axis=1)\n    \n    \n    #Fitting the model\n    model_lgb.fit(X_train,y_train)\n    \n    #Predicting for valid and test datasets\n    valid_preds = model_lgb.predict_proba(X_valid)\n    lgbm_pred += model_lgb.predict_proba(X_test)\/5\n    \n    #Calculating log loss\n    logloss.append(log_loss(y_valid,valid_preds))\n    \nprint(logloss)\nprint(sum(logloss)\/len(logloss))","1d9bed66":"class_labels = sorted(train_X.target.value_counts().index)\nlgbpreds = model_lgb.predict(test_X.drop('id',axis=1), num_iteration=model_lgb.best_iteration_)\nlgbprods = model_lgb.predict_proba(test_X.drop('id',axis=1)) # used for submission\nsubmission = pd.DataFrame(lgbprods, columns=class_labels, index=test_X.index + 200000)\nsubmission.index.name = 'id'\nsubmission.head()\nsubmission.to_csv('submission_lgbtune.csv')","f903fe4a":"def optimize(trial):\n    param = {\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'objective': 'multiclass',\n        #'metric' : ''\n        \"random_state\" : 42}\n\n\n\n    model = LGBMClassifier(**param)\n    logloss = []\n    for f in range(5):\n        train = df[df.kfold!= f].reset_index(drop=True)\n        valid = df[df.kfold== f].reset_index(drop=True)\n\n        X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n        y_train = train[\"target\"]\n        X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n        y_valid = valid[\"target\"]\n\n        model.fit(X_train,y_train)\n        pred = model.predict_proba(X_valid)\n        fold_logloss = log_loss(y_valid, pred)\n        logloss.append(fold_logloss)\n    \n    return np.mean(logloss)","892940f0":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(optimize, n_trials=15)","e9f0fa13":"print(study.best_params)","725b8cd1":"# Best guess after learning new tricks\nmodel_lgb2 = LGBMClassifier(\n    lambda_l1= 4.2986828029788605e-08, \n    lambda_l2= 2.290124579576187, \n    num_leaves= 6, \n    feature_fraction= 0.9832712273137326, \n    bagging_fraction= 0.41331491154011984, \n    bagging_freq= 1, \n    min_child_samples= 73,\n    seed = 14000605\n    )\nlogloss = []\nlgbm_pred = 0\nfor f in range(5): # Looping around 5 folds\n    \n    #Splitting the data into train and validation set\n    train = df[df.kfold!= f].reset_index(drop=True) \n    valid = df[df.kfold== f].reset_index(drop=True)\n    \n    #Creating X_train and y_train\n    X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_train = train.target\n    X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_valid = valid.target\n    X_test = test_X.drop([\"id\"], axis=1)\n    \n    \n    #Fitting the model\n    model_lgb2.fit(X_train,y_train)\n    \n    #Predicting for valid and test datasets\n    valid_preds = model_lgb2.predict_proba(X_valid)\n    lgbm_pred += model_lgb2.predict_proba(X_test)\/5\n    \n    #Calculating log loss\n    logloss.append(log_loss(y_valid,valid_preds))\n    \nprint(logloss)\nprint(sum(logloss)\/len(logloss))","c2893be1":"class_labels = sorted(train_X.target.value_counts().index)\nlgbpreds = model_lgb2.predict(test_X.drop('id',axis=1), num_iteration=model_lgb.best_iteration_)\nlgbprods = model_lgb2.predict_proba(test_X.drop('id',axis=1)) # used for submission\nsubmission = pd.DataFrame(lgbprods, columns=class_labels, index=test_X.index + 200000)\nsubmission.index.name = 'id'\nsubmission.head()\nsubmission.to_csv('submission_lgboptuna.csv')","ee22b2c9":"Not a lot of commentary there because it turned out much worse. Not sure what, if anything, I might have done to make that turn out better. Let's try the non-random-forest version.","1f7a7076":"## LightGBM\nStill have quite a bit of learning to do on this one.","5d21bf9e":"# June 2021 TPS - Model 3: LightGBM with tuning\nI previously tried XGBoost with tuning, with poor results. You'll see this in older versions of this notebook. I didn't want to spam notebooks out here so I just relegated my previous attempts to prior versions.\n\nSee my [previous notebook](https:\/\/www.kaggle.com\/jdunavin\/june-2021-tps-eda-and-modeling) for EDA and a basic model.","c1495f6b":"## Load some libraries","c4eeb674":"## Load and clean the data"}}