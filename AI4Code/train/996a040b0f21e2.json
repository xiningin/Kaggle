{"cell_type":{"c9aedcdb":"code","f9c3ec32":"code","ca867967":"code","3e3e1154":"code","30e90b5a":"code","08a68dde":"code","1d4dde3a":"code","44d99cc4":"code","caefae1c":"code","3c4cc36f":"code","52a8d33c":"code","a0f3a063":"code","2bc9fc0b":"code","182aded0":"code","8e0b089a":"code","7057ef97":"code","171f2924":"code","5f392cb9":"code","c33c2818":"code","8b807966":"code","a535c3ac":"code","b9ac673b":"code","68aef1df":"code","2a450804":"code","4a7136d2":"code","67192d4e":"code","ea30b761":"code","18db2819":"code","21fee842":"code","2e0eb703":"code","6ec947cc":"code","0a57d917":"code","9e48ebb4":"code","b7aab536":"code","9d352446":"code","987e2ab4":"code","bb15f973":"code","9685628b":"code","d0fd2996":"code","dfa34f86":"code","bfbc1bfc":"code","7bc3106e":"code","f1b43432":"code","f1468dce":"code","9297df4f":"code","481e613d":"code","3829d134":"code","56b5ebc0":"code","95b787ff":"code","cbf4a8ba":"code","be1362c7":"code","fc9658ec":"code","1b0ad596":"code","711d6c90":"code","2be47585":"code","0e146982":"code","0fd29774":"code","2b43ce47":"code","0ae0df05":"code","ec937cc4":"code","08a99a44":"code","61cca2d8":"markdown","9b6c1bcd":"markdown","72e8dc31":"markdown","6cba1e06":"markdown","1aab940e":"markdown","8a81faa5":"markdown","4100eb84":"markdown","d10b6dae":"markdown","d9453666":"markdown","c3653bc8":"markdown","09f8bd83":"markdown","fa5cc76e":"markdown","0c9d98dc":"markdown","4a6b9752":"markdown","2609f0de":"markdown","c23e6a6b":"markdown","008416b0":"markdown","0fe1a3e8":"markdown","9d25f42c":"markdown","0cb7b157":"markdown","c3d85062":"markdown","ebc9e4f1":"markdown","dd027f78":"markdown","6a5fb23e":"markdown","e2d753c2":"markdown"},"source":{"c9aedcdb":"# Most basic stuff for EDA.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Core packages for text processing.\n\nimport string\nimport re\n\n# Libraries for text preprocessing.\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\n\n# Loading some sklearn packaces for modelling.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Some packages for word clouds and NER.\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter, defaultdict\nfrom PIL import Image\nimport spacy\n!pip install https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.2.5\/en_core_web_sm-2.2.5.tar.gz\nimport en_core_web_sm\n\n# Core packages for general use throughout the notebook.\n\nimport random\nimport warnings\nimport time\nimport datetime\n\n# For customizing our plots.\n\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\n# Loading pytorch packages.\n\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n\n# Setting some options for general use.\n\nstop = set(stopwords.words('english'))\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\n\n\n#Setting seeds for consistent results.\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","f9c3ec32":"# Loading the train and test data for visualization & exploration.\n\ntrainv = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntestv = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","ca867967":"# Taking general look at the both datasets.\n\ndisplay(trainv.sample(5))\ndisplay(testv.sample(5))","3e3e1154":"# Checking observation and feature numbers for train and test data.\n\nprint(trainv.shape)\nprint(testv.shape)","30e90b5a":"# Some basic helper functions to clean text by removing urls, emojis, html tags and punctuations.\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions\n\ntrainv['text_clean'] = trainv['text'].apply(lambda x: remove_URL(x))\ntrainv['text_clean'] = trainv['text_clean'].apply(lambda x: remove_emoji(x))\ntrainv['text_clean'] = trainv['text_clean'].apply(lambda x: remove_html(x))\ntrainv['text_clean'] = trainv['text_clean'].apply(lambda x: remove_punct(x))","08a68dde":"# Tokenizing the tweet base texts.\n\ntrainv['tokenized'] = trainv['text_clean'].apply(word_tokenize)\n\ntrainv.head()","1d4dde3a":"# Lower casing clean text.\n\ntrainv['lower'] = trainv['tokenized'].apply(\n    lambda x: [word.lower() for word in x])\n\ntrainv.head()","44d99cc4":"# Removing stopwords.\n\ntrainv['stopwords_removed'] = trainv['lower'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrainv.head()","caefae1c":"# Applying part of speech tags.\n\ntrainv['pos_tags'] = trainv['stopwords_removed'].apply(nltk.tag.pos_tag)\n\ntrainv.head()","3c4cc36f":"# Converting part of speeches to wordnet format.\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n\ntrainv['wordnet_pos'] = trainv['pos_tags'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\ntrainv.head()","52a8d33c":"# Applying word lemmatizer.\n\nwnl = WordNetLemmatizer()\n\ntrainv['lemmatized'] = trainv['wordnet_pos'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ntrainv['lemmatized'] = trainv['lemmatized'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrainv['lemma_str'] = [' '.join(map(str, l)) for l in trainv['lemmatized']]\n\ntrainv.head()","a0f3a063":"# Displaying target distribution.\n\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\nsns.countplot(trainv['target'], ax=axes[0])\naxes[1].pie(trainv['target'].value_counts(),\n            labels=['Not Disaster', 'Disaster'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Distribution of the Tweets', fontsize=24)\nplt.show()","2bc9fc0b":"# Creating a new feature for the visualization.\n\ntrainv['Character Count'] = trainv['text_clean'].apply(lambda x: len(str(x)))\n\n\ndef plot_dist3(df, feature, title):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.distplot(df.loc[:, feature],\n                 ax=ax2,\n                 kde_kws={'cumulative': True},\n                 hist_kws={'cumulative': True},\n                 color='#e74c3c')\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative Probability')\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(x=feature, data=df, orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=25))\n\n    plt.suptitle(f'{title}', fontsize=24)","182aded0":"plot_dist3(trainv[trainv['target'] == 0], 'Character Count',\n           'Characters Per \"Non Disaster\" Tweet')","8e0b089a":"plot_dist3(trainv[trainv['target'] == 1], 'Character Count',\n           'Characters Per \"Disaster\" Tweet')","7057ef97":"def plot_word_number_histogram(textno, textye):\n    \n    \"\"\"A function for comparing word counts\"\"\"\n\n    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)\n    sns.distplot(textno.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')\n    sns.distplot(textye.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')\n    \n    axes[0].set_xlabel('Word Count')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Non Disaster Tweets')\n    axes[1].set_xlabel('Word Count')\n    axes[1].set_title('Disaster Tweets')\n    \n    fig.suptitle('Words Per Tweet', fontsize=24, va='baseline')\n    \n    fig.tight_layout()","171f2924":"plot_word_number_histogram(trainv[trainv['target'] == 0]['text'],\n                           trainv[trainv['target'] == 1]['text'])","5f392cb9":"def plot_word_len_histogram(textno, textye):\n    \n    \"\"\"A function for comparing average word length\"\"\"\n    \n    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)\n    sns.distplot(textno.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[0], color='#e74c3c')\n    sns.distplot(textye.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[1], color='#e74c3c')\n    \n    axes[0].set_xlabel('Word Length')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Non Disaster Tweets')\n    axes[1].set_xlabel('Word Length')\n    axes[1].set_title('Disaster Tweets')\n    \n    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n    fig.tight_layout()\n    ","c33c2818":"plot_word_len_histogram(trainv[trainv['target'] == 0]['text'],\n                        trainv[trainv['target'] == 1]['text'])","8b807966":"lis = [\n    trainv[trainv['target'] == 0]['lemma_str'],\n    trainv[trainv['target'] == 1]['lemma_str']\n]","a535c3ac":"fig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis, axes):\n    try:\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word.lower() for i in new for word in i]\n        dic = defaultdict(int)\n        for word in corpus:\n            if word in stop:\n                dic[word] += 1\n\n        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:15]\n        x, y = zip(*top)\n        df = pd.DataFrame([x, y]).T\n        df = df.rename(columns={0: 'Stopword', 1: 'Count'})\n        sns.barplot(x='Count', y='Stopword', data=df, palette='plasma', ax=j)\n        plt.tight_layout()\n    except:\n        plt.close()\n        print('No stopwords left in texts.')\n        break","b9ac673b":"# Displaying most common words.\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis, axes):\n\n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:30]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n    sns.barplot(x=y, y=x, palette='plasma', ax=j)\naxes[0].set_title('Non Disaster Tweets')\n\naxes[1].set_title('Disaster Tweets')\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Word')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Word')\n\nfig.suptitle('Most Common Unigrams', fontsize=24, va='baseline')\nplt.tight_layout()","68aef1df":"def ngrams(n, title):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n    axes = axes.flatten()\n    for i, j in zip(lis, axes):\n\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n),\n                                  max_df=0.9,\n                                  stop_words='english').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(i, n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        sns.barplot(x=y, y=x, palette='plasma', ax=j)\n        \n        axes[0].set_title('Non Disaster Tweets')\n        axes[1].set_title('Disaster Tweets')\n        axes[0].set_xlabel('Count')\n        axes[0].set_ylabel('Words')\n        axes[1].set_xlabel('Count')\n        axes[1].set_ylabel('Words')\n        fig.suptitle(title, fontsize=24, va='baseline')\n        plt.tight_layout()","2a450804":"ngrams(2, 'Most Common Bigrams')","4a7136d2":"ngrams(3, 'Most Common Trigrams')","67192d4e":"def display_topics(text, no_top_words, topic):\n    \n    \"\"\" A function for determining the topics present in our corpus with nmf \"\"\"\n    \n    no_top_words = no_top_words\n    tfidf_vectorizer = TfidfVectorizer(\n        max_df=0.90, min_df=25, max_features=5000, use_idf=True)\n    tfidf = tfidf_vectorizer.fit_transform(text)\n    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n    doc_term_matrix_tfidf = pd.DataFrame(\n        tfidf.toarray(), columns=list(tfidf_feature_names))\n    nmf = NMF(n_components=10, random_state=0,\n              alpha=.1, init='nndsvd').fit(tfidf)\n    print(topic)\n    for topic_idx, topic in enumerate(nmf.components_):\n        print('Topic %d:' % (topic_idx+1))\n        print(' '.join([tfidf_feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\n\ndisplay_topics(lis[0], 10, 'Non Disaster Topics\\n')","ea30b761":"display_topics(lis[1], 10,'Disaster Topics\\n')","18db2819":"# Setting mask for wordcloud.\n\nmask = np.array(Image.open('\/kaggle\/input\/twittermaskn\/twittermask.png'))\nmask[mask.sum(axis=2) == 0] = 255","21fee842":"def plot_wordcloud(text, title, title_size):\n    \"\"\" A function for creating wordcloud images \"\"\"\n    words = text\n    allwords = []\n    for wordlist in words:\n        allwords += wordlist\n    mostcommon = FreqDist(allwords).most_common(140)\n    wordcloud = WordCloud(\n        width=1200,\n        height=800,\n        background_color='black',\n        stopwords=set(STOPWORDS),\n        max_words=150,\n        scale=3,\n        mask=mask,\n        contour_width=0.1,\n        contour_color='grey',\n    ).generate(str(mostcommon))    \n\n    def grey_color_func(word,\n                        font_size,\n                        position,\n                        orientation,\n                        random_state=None,\n                        **kwargs):\n        # A definition for creating grey color shades.\n        return 'hsl(0, 0%%, %d%%)' % random.randint(60, 100)\n\n    fig = plt.figure(figsize=(18, 18), facecolor='white')\n    plt.imshow(wordcloud.recolor(color_func=grey_color_func, random_state=42),\n               interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title,\n              fontdict={\n                  'size': title_size,\n                  'verticalalignment': 'bottom'\n              })\n    plt.tight_layout(pad=0)\n    plt.show()","2e0eb703":"plot_wordcloud(trainv[trainv['target'] == 0]['lemmatized'],\n               'Most Common Words in Non-Disaster Tweets',\n               title_size=30)","6ec947cc":"plot_wordcloud(trainv[trainv['target'] == 1]['lemmatized'],\n               'Most Common Words in Disaster Tweets',\n               title_size=30)","0a57d917":"# Loading NER.\nnlp = en_core_web_sm.load() ","9e48ebb4":"def plot_named_entity_barchart(text):\n    \n    \"\"\"A function for extracting named entities and comparing them\"\"\"\n    \n    def _get_ner(text):\n        doc = nlp(text)\n        return [X.label_ for X in doc.ents]\n\n    ent = text.apply(lambda x: _get_ner(x))\n    ent = [x for sub in ent for x in sub]\n    counter = Counter(ent)\n    count = counter.most_common()\n\n    x, y = map(list, zip(*count))\n    sns.barplot(x=y, y=x)","b7aab536":"fig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\nfor i, j in zip(lis, axes):\n\n    def _get_ner(i):\n        doc = nlp(i)\n        return [X.label_ for X in doc.ents]\n\n    ent = i.apply(lambda x: _get_ner(x))\n    ent = [x for sub in ent for x in sub]\n    counter = Counter(ent)\n    count = counter.most_common()[:15]\n\n    x, y = map(list, zip(*count))\n    sns.barplot(x=y, y=x, ax=j, palette='plasma')\n\naxes[0].set_title('Non Disaster Tweets')\naxes[1].set_title('Disaster Tweets')\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Named-Entity')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Named-Entity')\nfig.suptitle('Common Named-Entity Counts', fontsize=24, va='baseline')\n\n\n\npatch1 = mpatches.Patch(label='PERSON : People, including fictional')\npatch2 = mpatches.Patch(label='ORG : Companies, agencies, institutions, etc.')\npatch3 = mpatches.Patch(label='CARDINAL : Numerals that dont fall under another type.')\npatch4 = mpatches.Patch(label='GPE : Countries, cities, states.')\npatch5 = mpatches.Patch(label='NORP : Nationalities or religious or political groups.')\npatch6 = mpatches.Patch(label='TIME : Times smaller than a day.')\npatch7 = mpatches.Patch(label='QUANTITY : Measurements, as of weight or distance.')\npatch8 = mpatches.Patch(label='ORDINAL : \u201cfirst\u201d, \u201csecond\u201d, etc.')\npatch9 = mpatches.Patch(label='LOC : Non-GPE locations, mountain ranges, bodies of water.')\npatch10 = mpatches.Patch(label='FAC : Buildings, airports, highways, bridges, etc.')\npatch11 = mpatches.Patch(label='PRODUCT : Objects, vehicles, foods, etc. (Not services.)')\npatch12 = mpatches.Patch(label='EVENT : Named hurricanes, battles, wars, sports events, etc.')\npatch13 = mpatches.Patch(label='LANGUAGE : Any named language.')\npatch14 = mpatches.Patch(label='PERCENT : Percentage, including \u201d%\u201c.')\npatch15 = mpatches.Patch(label='DATE : Absolute or relative dates or periods.')\n\n\n\nplt.legend(handles=[patch1, patch2, patch3, patch4, patch5, patch6, patch7, patch8, patch9, patch10, patch11, patch12, patch13, patch14, patch15, ],bbox_to_anchor=(1.05, 0.85), loc='upper left', borderaxespad=0.)\n\nplt.show()","9d352446":"# If there's a GPU available...\n\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.  \n    \n    device = torch.device('cuda')    \n\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device('cpu')","987e2ab4":"# Loading the data for modelling.\n\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint(f'Number of training tweets: {train.shape[0]}\\n')\nprint(f'Number of training tweets: {test.shape[0]}\\n')\n\ndisplay(train.sample(10))","bb15f973":"# Setting target variables, creating combined data and saving index for dividing combined data later.\n\nlabels = train['target'].values\nidx = len(labels)\ncombined = pd.concat([train, test])\ncombined = combined.text.values","9685628b":"# Tokenizing the combined text data using bert tokenizer.\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)","d0fd2996":"# Print the original tweet.\n\nprint(' Original: ', combined[0])\n\n# Print the tweet split into tokens.\n\nprint('Tokenized: ', tokenizer.tokenize(combined[0]))\n\n# Print the sentence mapped to token ID's.\n\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(combined[0])))","dfa34f86":"max_len = 0\n\n# For every sentence...\n\nfor text in combined:\n\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n    \n    input_ids = tokenizer.encode(text, add_special_tokens=True)\n\n    # Update the maximum sentence length.\n    \n    max_len = max(max_len, len(input_ids))\n\nprint('Max sentence length: ', max_len)","bfbc1bfc":"# Making list of sentence lenghts:\n\ntoken_lens = []\n\nfor text in combined:\n    tokens = tokenizer.encode(text, max_length = 512)\n    token_lens.append(len(tokens))","7bc3106e":"# Displaying sentence length dist.\n\nfig, axes = plt.subplots(figsize=(14, 6))\nsns.distplot(token_lens, color='#e74c3c')\nplt.show()","f1b43432":"# Splitting the train test data after tokenizing.\n\ntrain= combined[:idx]\ntest = combined[idx:]\ntrain.shape","f1468dce":"def tokenize_map(sentence,labs='None'):\n    \n    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n    \n    global labels\n    \n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    \n    for text in sentence:\n        #   \"encode_plus\" will:\n        \n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        \n        encoded_dict = tokenizer.encode_plus(\n                            text,                      # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            truncation='longest_first', # Activate and control truncation\n                            max_length = 84,           # Max length according to our text data.\n                            pad_to_max_length = True, # Pad & truncate all sentences.\n                            return_attention_mask = True,   # Construct attn. masks.\n                            return_tensors = 'pt',     # Return pytorch tensors.\n                       )\n\n        # Add the encoded sentence to the id list. \n        \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        \n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    \n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n        labels = torch.tensor(labels)\n        return input_ids, attention_masks, labels\n    else:\n        return input_ids, attention_masks","9297df4f":"# Tokenizing all of the train test sentences and mapping the tokens to their word IDs.\n\ninput_ids, attention_masks, labels = tokenize_map(train, labels)\ntest_input_ids, test_attention_masks= tokenize_map(test)","481e613d":"# Combine the training inputs into a TensorDataset.\n\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# Create a 80-20 train-validation split.\n\n# Calculate the number of samples to include in each set.\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","3829d134":"# The DataLoader needs to know our batch size for training, so we specify it here. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n\nbatch_size = 32\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \n\ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\n\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","56b5ebc0":"prediction_data = TensorDataset(test_input_ids, test_attention_masks)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)","95b787ff":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-large-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification. You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the device which we set GPU in our case.\n\nmodel.to(device)","cbf4a8ba":"# Get all of the model's parameters as a list of tuples:\n\nparams = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))","be1362c7":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch).\n\n# The 'W' stands for 'Weight Decay fix' probably...\n\noptimizer = AdamW(model.parameters(),\n                  lr = 6e-6, # args.learning_rate\n                  eps = 1e-8 # args.adam_epsilon\n                )","fc9658ec":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n\n# We chose to run for 3, but we'll see later that this may be over-fitting the training data.\n\nepochs = 3\n\n# Total number of training steps is [number of batches] x [number of epochs] (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","1b0ad596":"def flat_accuracy(preds, labels):\n    \n    \"\"\"A function for calculating accuracy scores\"\"\"\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return accuracy_score(labels_flat, pred_flat)\n\ndef flat_f1(preds, labels):\n    \n    \"\"\"A function for calculating f1 scores\"\"\"\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return f1_score(labels_flat, pred_flat)","711d6c90":"def format_time(elapsed):    \n    \n    \"\"\"A function that takes a time in seconds and returns a string hh:mm:ss\"\"\"\n    \n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","2be47585":"# This training code is based on the `run_glue.py` script here:\n\n# https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\n\n# We'll store a number of quantities such as training and validation loss, validation accuracy, f1 score and timings.\n\ntraining_stats = []\n\n# Measure the total training time for the whole run.\n\ntotal_t0 = time.time()\n\n# For each epoch...\n\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print('')\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes:\n    \n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    \n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to `train` just changes the *mode*, it doesn't *perform* the training.\n    \n    # `dropout` and `batchnorm` layers behave differently during training vs. test ,\n    # source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch\n    \n    model.train()\n\n    # For each batch of training data...\n    \n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the device(gpu in our case) using the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        \n        b_input_ids = batch[0].to(device).to(torch.int64)\n        b_input_mask = batch[1].to(device).to(torch.int64)\n        b_labels = batch[2].to(device).to(torch.int64)\n\n        # Always clear any previously calculated gradients before performing a backward pass. PyTorch doesn't do this automatically because accumulating the gradients is 'convenient while training RNNs'. \n        # Source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch\n        \n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # The documentation for this `model` function is down here: \n        # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers BertForSequenceClassification.\n        \n        # It returns different numbers of parameters depending on what arguments given and what flags are set. For our useage here, it returns the loss (because we provided labels),\n        # And the 'logits' (the model outputs prior to activation.)\n        \n        loss, logits = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n\n        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end, \n        # `loss` is a tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\n        \n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        \n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0 This is to help prevent the 'exploding gradients' problem.\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        \n        # The optimizer dictates the 'update rule'(How the parameters are modified based on their gradients, the learning rate, etc.)\n        \n        optimizer.step()\n\n        # Update the learning rate.\n        \n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    \n    avg_train_loss = total_train_loss \/ len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    \n    training_time = format_time(time.time() - t0)\n\n    print('')\n    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n    print('  Training epcoh took: {:}'.format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on our validation set.\n\n    print('')\n    print('Running Validation...')\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n    \n    model.eval()\n\n    # Tracking variables:\n    \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    total_eval_f1 = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch.\n    \n    for batch in validation_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        \n        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n        \n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during the forward pass, since this is only needed for backprop (training part).\n        \n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the 'segment ids', which differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is down here: \n            # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers BertForSequenceClassification.\n            # Get the 'logits' output by the model. The 'logits' are the output values prior to applying an activation function like the softmax.\n            \n            (loss, logits) = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n            \n        # Accumulate the validation loss.\n        \n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU:\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n        \n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        total_eval_f1 += flat_f1(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    \n    avg_val_accuracy = total_eval_accuracy \/ len(validation_dataloader)\n    print('  Accuracy: {0:.2f}'.format(avg_val_accuracy))\n    \n    # Report the final f1 score for this validation run.\n    \n    avg_val_f1 = total_eval_f1 \/ len(validation_dataloader)\n    print('  F1: {0:.2f}'.format(avg_val_f1))\n\n    # Calculate the average loss over all of the batches.\n    \n    avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n    \n    \n    \n    # Measure how long the validation run took:\n    \n    validation_time = format_time(time.time() - t0)\n    \n    print('  Validation Loss: {0:.2f}'.format(avg_val_loss))\n    print('  Validation took: {:}'.format(validation_time))\n\n    # Record all statistics from this epoch.\n    \n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Val_F1' : avg_val_f1,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint('')\nprint('Training complete!')\n\nprint('Total training took {:} (h:mm:ss)'.format(format_time(time.time()-total_t0)))","0e146982":"# Display floats with two decimal places.\n\npd.set_option('precision', 2)\n\n# Create a DataFrame from our training statistics.\n\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index.\n\ndf_stats = df_stats.set_index('epoch')\n\n# Display the table.\n\ndisplay(df_stats)","0fd29774":"# Increase the plot size and font size:\n\nfig, axes = plt.subplots(figsize=(12,8))\n\n# Plot the learning curve:\n\nplt.plot(df_stats['Training Loss'], 'b-o', label='Training')\nplt.plot(df_stats['Valid. Loss'], 'g-o', label='Validation')\n\n# Label the plot:\n\nplt.title('Training & Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\nplt.legend()\nplt.xticks([1, 2, 3])\n\nplt.show()","2b43ce47":"# Prediction on test set:\n\nprint('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n\n# Put model in evaluation mode:\n\nmodel.eval()\n\n# Tracking variables :\n\npredictions = []\n\n# Predict:\n\nfor batch in prediction_dataloader:\n    \n  # Add batch to GPU\n\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader:\n    \n  b_input_ids, b_input_mask, = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and speeding up prediction:\n\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions:\n    \n      outputs = model(b_input_ids, token_type_ids=None, \n                      attention_mask=b_input_mask)\n\n  logits = outputs[0]\n\n  # Move logits and labels to CPU:\n    \n  logits = logits.detach().cpu().numpy()\n \n  \n  # Store predictions and true labels:\n    \n  predictions.append(logits)\n\n\nprint('    DONE.')","0ae0df05":"# Getting list of predictions and then choosing the target value with using argmax on probabilities.\n\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","ec937cc4":"# Creating submission data.\n\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission['target'] = flat_predictions\nsubmission.head(10)","08a99a44":"# Saving submission to '.csv' file:\n\nsubmission.to_csv('submission.csv', index=False, header=True)","61cca2d8":"<a id=\"Most_Common_Words\"><\/a>\n## Most Common Words\n\n#### It's time to move to words themselves instead of their quantitative features. We start with most common words in both classes. I'd say it's pretty obvious if it's from disaster tweets or not. Disaster tweets has words like fire, kill, bomb indicating disasters. Meanwhile non disaster ones looks like pretty generic.\n\n\n### [Back To Table of Contents](#top_section)","9b6c1bcd":"<a id=\"getting_the_text_data_ready\"><\/a>\n\n# Getting the Text Data Ready\n\n### **Ok lets get started. First we need to get our working enviroment ready for future analysis and modelling. We start with usual approach like installing and importing packages etc...**","72e8dc31":"<a id=\"Tokenization_and_Formatting_the_Inputs\"><\/a>\n## Tokenization and Formatting the Inputs\n\n#### For feeding our text to BERT we have to tokenize our text first and then these tokens must be mapped. For this job we gonna download and use BERT's own tokenizer. Thanks to Transformers library it's like one line of code, we also convert our tokens to lowercase for uncased model. You can see how the tokenizer works below there on first row of tweets for example.\n\n#### We set our max len according to our tokenized sentences for padding and truncation, then we use tokenizer.encode_plus it'll split the sentences into tokens, then adds special tokens for classificication [CLS]: \n> The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. (from the BERT paper)\n#### Then it adds [SEP] tokens for making BERT decide if sentences are related. In our case it shouldn't be that important I think.\n\n#### Then our tokenizer map's our tokens to their IDs first and pads or truncates all sentences to same length according to our max length. If sentence is longer than our limit it gets truncated, if it's shorter than our defined length then it adds [PAD] tokens to get them in same length.\n\n#### Finally tokenizer create attention masks which is consisting of 1's and 0's for differentiating [PAD] tokens from the actual tokens.\n\n#### We do these steps for each train and test set and then get our converted data for our BERT model. We also split train test on our train data for checking our models accuracy. \n\n#### Lastly we define how to load the data into our model for training, since we can't use it all at once because of memory restrictions. On the official BERT paper batch size of 16 or 32 is recommended so we went with 32 since Kaggle offers us decent GPU's thanks to them!\n\n### [Back To Table of Contents](#top_section)","6cba1e06":"<a id=\"Word_Lengths\"><\/a>\n## Word Lengths\n\n#### This time we're gonna check if word complexity differs from tweet class. It looks like disaster tweets has longer words than non disaster ones in general. It's pretty visible which is good sign, yet again we can only assume at this stage...\n\n### [Back To Table of Contents](#top_section)","1aab940e":"<a id=\"Word_Counts\"><\/a>\n## Word Counts\n\n#### Ok let's check number of words per tweet now, they both look somewhat normally distributed, again disaster tweets seems to have slightly more words than non disaster ones. We might dig this deeper to get some more info in next part...\n\n### [Back To Table of Contents](#top_section)","8a81faa5":"<a id=\"Importing_Neccesary_Packages\"><\/a>\n\n## Importing Neccesary Packages\n\n### **First we gonna load lots of libraries we are going to use them through our notebook. I tried to sort and group them by the use (divided by #'s) so I hope it's easier to read for you.**\n\n### [Back To Table of Contents](#top_section)","4100eb84":"<a id=\"Determining_Topics\"><\/a>\n## Determining Topics\n\n#### We'll be using a method called Non-Negative Matrix Factorization (NMF) to see if we can get some defined topics out of our TF-IDF matrix, with this way TF-IDF will decrease impact of the high frequency words, so we might get more specific topics.\n\n#### When we inspect our top ten topics we might need to use little imagination to help us understand them. Well actually they are pretty seperable again, I'd say disaster topics are much more clearer to read, we can see the topics directly by looking at them, meanwhile non disaster ones are more personal topics...\n\n### [Back To Table of Contents](#top_section)","d10b6dae":"## Well that's it then... Thank you all for reading! I hope it helps you on the way of learning NLP. I'm completely open to feedbacks for improving my work and learning more, I'm still a beginner and learning a lot every single day so please feel free to add your opinion in the comments.\n<div align='center'><font size=\"6\" color=\"#000000\"><b>And again please UPVOTE if you liked this notebook so it can reach more people, Thanks!<\/b><\/font><\/div>\n","d9453666":"<a id=\"Cleaning_Text\"><\/a>\n## Cleaning Text\n\n#### Alright! Before we visualize our text data I wanted to make it look better with some general helper functions to clear out things like: urls, emojis, html tags, punctuations... We'll add all of them in one column called 'text_clean' then move from there for next steps. When we have cleaner text we can apply our tokenizer to split each word into a token. I'll apply this and next steps to individual columns to show each step of our progress. Next we transforming all words to lowercase then we remove stopwords (they don't mean much in sentence alone) so we use NLTK stopwords for it.\n\n#### After removing these words we gonna lemmatize them but for that we need to add some extra steps to do it properly: We gonna apply part of speech tags to our text (like verb, noun etc.) then we convert them to wordnet format and finally we can apply lemmatizer and save it to 'lemmatized' column. And one last thing we convert these tokenized lists back to str version for future uses.\n\n#### So basically what we did are:\n- Removed urls, emojis, html tags and punctuations,\n- Tokenized the tweet base texts,\n- Lower cased clean text,\n- Removed stopwords,\n- Applied part of speech tags,\n- Converted part of speeches to wordnet format,\n- Applying word lemmatizer,\n- Converted tokenized text to string again.\n\n\n### [Back To Table of Contents](#top_section)","c3653bc8":"<a id=\"Visualizing_the_Data\"><\/a>\n# Visualizing the Data\n\n#### Well... Our text is ready for inspection, clean and in order. We can start visualizing the data to see if we can find some visible relations between tweet classes.\n\n### [Back To Table of Contents](#top_section)","09f8bd83":"<a id=\"Most_Common_Trigrams\"><\/a>\n## Most Common Trigrams\n\n#### Alright! Things are much clearer with sequences of 3 words. The confusing body bags were cross body bags (Who uses them in these days anyways!) which I found it pretty funny when I found the reason of the confusion. Anyways we can see disasters are highly seperable now from non disaster ones, which is great!\n\n### [Back To Table of Contents](#top_section)","fa5cc76e":"### I choose max len of 84 since it's the longest sentence we have here, playing with this number might get different results but the bigger you choose the slower the model will be!","0c9d98dc":"<a id=\"Word_Cloud\"><\/a>\n## Word Cloud\n\n#### Wordclouds are popular approach in NLP tasks. We're going to use the library exactly designed for it called \"WordCloud\", I also wanted to mask it with twitter logo shape and grey colors just for adding more interesting presentation and show what you can do with this library. When we look our word clouds we can clearly say which one is disaster on which one is not. Pretty good!\n\n### [Back To Table of Contents](#top_section)","4a6b9752":"<a id=\"Most_Common_Bigrams\"><\/a>\n## Most Common Bigrams\n\n#### Let's have a look for bigrams this time, which they are sequences of adjacent two words. Again it's pretty obvious to seperate two classes if it's disaster related or not. There are some confusing bigrams in non disaster ones like  body bag, emergency service etc. which needs deeper research but we'll leave it here since we got what we looking for in general.\n\n### [Back To Table of Contents](#top_section)","2609f0de":"<a id=\"top_section\"><\/a>\n<div align='center'><font size=\"6\" color=\"#000000\"><b>Disaster Tweets Neuro-Linguistic Programming: Exploratory Data Analysis, Application of BERT Using Transformers Library With Pytorch<\/b><\/font><\/div>\n<hr>\n<div align='center'><font size=\"5\" color=\"#000000\">A General Introduction<\/font><\/div>\n<hr>\n\n**I think we all know the power of Twitter by now. It's one of the main communications channel for most of the people on earth, we get most of our daily news through our screens via twitter in these days... With the smart phones entering in our lives our newsfeed is immense but there is one problem: How'd we know if we getting real info? What if we want to seperate real situations from the non relevant data? We're lucky! We have NLP models to do heavy work for us to get distilled info. In this notebook we're going to use some basic and common NLP approaches to give us most accurate results...**\n\n\n\n## Table of Contents\n\n* [Getting the Text Data Ready](#getting_the_text_data_ready)\n    - [Importing Neccesary Packages](#Importing_Neccesary_Packages)\n    - [Loading the Data](#Loading_the_Data)\n    - [Cleaning Text](#Cleaning_Text)    \n* [Visualizing the Data](#Visualizing_the_Data)\n    - [Target Distribution](#Target_Distribution)\n    - [Tweet Lengths](#Tweet_Lengths)\n    - [Word Counts](#Word_Counts)\n    - [Word Lengths](#Word_Lengths)\n    - [Most Common Words](#Most_Common_Words)\n    - [Most Common Bigrams](#Most_Common_Bigrams)\n    - [Most Common Trigrams](#Most_Common_Trigrams)    \n* [Some Extra Analysis](#Some_Extra_Analysis)\n    - [Determining Topics](#Determining_Topics)\n    - [Word Cloud](#Word_Cloud)\n    - [Named Entity Recognition](#Named_Entity_Recognition)    \n* [Building the Bert Model](#Building_the_Model)\n    - [Getting Things Ready](#Getting_Things_Ready)\n    - [Tokenization and Formatting the Inputs](#Tokenization_and_Formatting_the_Inputs)\n    - [Setting the Bert Classification Model](#Setting_the_Bert_Classification_Model)\n    - [Training and Evaluating](#Training_and_Evaluating)\n    - [Predicting and Submission](#Predicting_and_Submission)\n    \n\n    \n\n## Summary\n\n**In this notebook I tried to apply what I learned on NLP. (Well... That's why they called notebooks right?) This notebook includes: Preprocessing the text, visualizing the processed data by several methods like tweet lenghts, word counts, average word lengths, ngrams etc. I especially wanted to clean data before I visualize it, perhaps you should investigate the raw data you got first then move to cleaning in normal cases but I didn't want to pile it on so I went this way. Then I used some more analysis tecniques like Word Clouds, NER's etc. to give us different angles to look from. At the last part we're going to implement BERT model to do tokenization, classification and prediction with using transformers. I hope this notebook helps some of you as others helped me a lot. I'll talk about them in next chapter.**\n\n## References:\n\n- [BERT Research from  Chris McCormick](https:\/\/www.youtube.com\/watch?v=FKlPCK1uFrc): It was a great tutorial and helped me a lot through modelling part.\n- [The Official BERT Paper](https:\/\/arxiv.org\/pdf\/1810.04805.pdf): It's helpful for understanding theory behind it also I used it while finetuning the model\n- [Transformers Official Page](https:\/\/huggingface.co\/transformers\/quickstart.html): There are some good examples to write your own models.\n- [Word Cloud GitHub Page](https:\/\/github.com\/amueller\/word_cloud): Again good examples for your word clouds.\n- [Exploratory Data Analysis for NLP from Shahul Es](https:\/\/neptune.ai\/blog\/exploratory-data-analysis-natural-language-processing-tools): Learned a lot about NLP EDA's here.\n- [NLP | Exploratory Data Analysis of Text Data from Kamil Mysiak](https:\/\/towardsdatascience.com\/nlp-part-3-exploratory-data-analysis-of-text-data-1caa8ab3f79d): Again this EDA work was great example for me.\n\nWanted to thank all of them for their great effort and works!\n\n### Note: I hide some inputs on this kernel for cleaner looking you can see the codes by clicking \"Input\" button on top right of the cells. Also this is very early version of the notebook if encounter any problems or questions please let me know.\n\n### I highly appreciate your feedback, there might be some areas can be fixed or improved.\n\n## If you liked my work please dont forget to Upvote!","c23e6a6b":"<a id=\"Building_the_Model\"><\/a>\n# Building the Bert Model\n\n#### Finally it's time to start building our model. We gonna use BERT for this task with the help of library called Transformers which makes our work much more smoother...\n\n### [Back To Table of Contents](#top_section)","008416b0":"<a id=\"Setting_the_Bert_Classification_Model\"><\/a>\n## Setting the Bert Classification Model\n\n#### It's time to load our model, exciting right?! Thanks to Transformers library we have exact tools we need for classification task. We set bert-large-uncased for the more accurate results and assign 2 labels for classification.\n\n#### You can see the model parameters down there, it's pretty straightforward with the transformers.\n\n#### Then we choose our optimizer and fine-tune our model. Again these hyperparameters (learning_rate, epsilon, epochs etc.) are recommended on the official BERT paper.\n\n### [Back To Table of Contents](#top_section)","0fe1a3e8":"<a id=\"Named_Entity_Recognition\"><\/a>\n## Named Entity Recognition\n\n#### One last thing before we move on the modelling is Named Entity Recognition. It's a method for extracting information from text and returns which entities that are present in the text are classified into predefined entity types like \"Person\", \"Place\", \"Organization\", etc. By using NER we can get great insights about the types of entities present in the given text dataset.\n\n#### When we look our NER results we can get lots of great insights. We can see that in disaster tweets countries, cities, states are much more common than non disaster ones. Again nationality or religious or political group names are more likely to be mentioned in disaster tweets. These are great indicators for us...\n\n### [Back To Table of Contents](#top_section)","9d25f42c":"<a id=\"Some_Extra_Analysis\"><\/a>\n# Some Extra Analysis\n\n#### In this part we gonna apply some other analysis tecniques to gain some more insights about our dataset let's see what we can get from them...\n\n### [Back To Table of Contents](#top_section)","0cb7b157":"<a id=\"Training_and_Evaluating\"><\/a>\n## Training and Evaluating\n\n#### Time to train our model! First we set some helper functions to calculate our metrics and time spent on the process. Then it moves like this, directly from the original notebook, it's pretty good at explaining I shouldn't confuse you with my own way of telling I guess:\n\n\n> Training:\n\n>-    Unpack our data inputs and labels\n-    Load data onto the GPU for acceleration,\n-    Clear out the gradients calculated in the previous pass,\n     -   In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out,\n-    Forward pass (feed input data through the network),\n-    Backward pass (backpropagation),\n-    Tell the network to update parameters with optimizer.step(),\n-    Track variables for monitoring progress.\n\n>Evalution:\n\n>-    Unpack our data inputs and labels,\n-    Load data onto the GPU for acceleration,\n-    Forward pass (feed input data through the network),\n-    Compute loss on our validation data and track variables for monitoring progress.\n\nPytorch hides all of the detailed calculations from us, but we've commented the code to point out which of the above steps are happening on each line. \n\n\nThe code below trains according to our data and saves the learning progress on the way so we can summarize at the end and see our results. We can also turn these to dataframe and plot it to see our eavluation better. So we can decide if the model performs well and not overfitting...\n\n\n\n\n### [Back To Table of Contents](#top_section)","c3d85062":"<a id=\"Getting_Things_Ready\"><\/a>\n## Getting Things Ready\n\n#### Let's start with setting our main computing device. The code below automatically choose GPU if it's available otherwise goes with CPU but I highly recommend GPU which is much faster. Then we load our traning and test data for modelling, as I said I'm not going to use our visualization data, BERT uses punctuations in modelling and as you'll see BERT does great job on raw text by itself.\n\n### [Back To Table of Contents](#top_section)","ebc9e4f1":"<a id=\"Predicting_and_Submission\"><\/a>\n## Predicting and Submission\n\n#### Ok we trained our model and it's ready to make predictions on our test data. Then we save them to csv file for submission, this part is pretty straightforward for the most of classification tasks.\n\n### [Back To Table of Contents](#top_section)","dd027f78":"<a id=\"Target_Distribution\"><\/a>\n## Target Distribution\n\n#### When we check our target variables and look at how they disturbuted we can say it not bad. There is no huge difference between classes we can say it's good sign for modelling.\n\n### [Back To Table of Contents](#top_section)","6a5fb23e":"<a id=\"Tweet_Lengths\"><\/a>\n## Tweet Lengths\n\n#### Let's start with the number of characters per tweet and compare if it's disaster related or not. It seems disaster tweets are longer than non disaster tweets in general. We can assume longer tweets are more likely for disasters but this is only an assumption and might be not true...\n\n### [Back To Table of Contents](#top_section)","e2d753c2":"<a id=\"Loading_the_Data\"><\/a>\n## Loading the Data\n\n#### Let's load our train and test data, I added \"v\"'s at the end of our variables for visualization because some of the pre-processing are not needed for the modelling but we can use them for our EDA part. Anyways let's look at our given data:\n\n#### Well... We have keywords, locations, text and our target labels. We gonna use the text feature for our modelling here.\n\n### [Back To Table of Contents](#top_section)"}}