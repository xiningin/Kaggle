{"cell_type":{"944b386c":"code","e3aac88f":"code","c0412bd1":"code","5680fd8d":"code","8a39bd2e":"code","7b4cc123":"code","33739476":"code","653319f3":"code","0d94b67c":"code","82a758bd":"code","03b0832a":"code","6397e980":"code","79afaedd":"code","d1a27336":"code","a7291725":"code","2540342a":"code","00169398":"code","e58f4bae":"code","daae0d0c":"code","bf51fd71":"code","e064a47b":"code","c00a5478":"code","9f55450b":"code","78a315d5":"code","742fe920":"code","b516f423":"code","1d9e7fdc":"markdown","9d21c081":"markdown","ef7401f9":"markdown","26a63bcb":"markdown","4c6af9d6":"markdown","3bdcbce2":"markdown","68dea585":"markdown","671fe73d":"markdown","f44ae33e":"markdown","91fa97cb":"markdown","6bbb904f":"markdown","6fb74caa":"markdown","dd6c3069":"markdown","b4969ba4":"markdown","cdd16865":"markdown","594dd4ce":"markdown","d8f165f6":"markdown","9e6dea73":"markdown","ab596991":"markdown","21e86525":"markdown","2b7d5fbe":"markdown","27c14a51":"markdown","8f2e1451":"markdown","1f1baf8f":"markdown","c74ffcf5":"markdown","9a44f997":"markdown"},"source":{"944b386c":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\ndata = pd.read_csv('..\/input\/reddit-vaccine-myths\/reddit_vm.csv')\ndata.head()","e3aac88f":"import missingno as msno\nmsno.bar(data, figsize=(12, 5))","c0412bd1":"data['title'].value_counts()","5680fd8d":"top_5_scores = list(data.nlargest(5,'score').index)\nfor index in top_5_scores:\n    print(f'Score - {data[\"score\"].iloc[index]}, Title - {data[\"title\"].iloc[index]}')","8a39bd2e":"# urls = list(data['url'].unique())\nocr_indexes = []\nblog_text_indexes = []\nocr_urls = []\nblog_text_urls = []\n\nfor i,ele in enumerate(data['url']):\n    try:\n        fil_type = ele.rsplit('.',1)[-1]\n        if fil_type in ['jpg','png','jpeg']:\n            ocr_indexes.append(i)\n            ocr_urls.append(ele)\n        elif fil_type == 'html':\n            blog_text_indexes.append(i)\n            blog_text_urls.append(ele)\n    except:\n        continue","7b4cc123":"!pip install beautifulsoup4","33739476":"data['blog_text'] = ['']*data.shape[0]\ndata['ocr'] = ['']*data.shape[0]","653319f3":"from urllib.request import Request, urlopen\nfrom bs4 import BeautifulSoup\nimport re\n\nfor ind,url in zip(blog_text_indexes,blog_text_urls):\n    try:\n        req = Request(url,headers={'User-Agent': 'Mozilla\/5.0'})\n        page = urlopen(req)\n        soup = BeautifulSoup(page)\n        strings = soup.get_text().split(' ')\n\n        clean_string = ''\n        for ele in strings:\n            if re.findall('^[a-zA-Z0-9_]+$',ele):\n                clean_string = clean_string + ' ' + re.findall('^[a-zA-Z0-9_]+$',ele)[0]\n            else:\n                continue\n        data['blog_text'].iloc[ind] = clean_string\n    except:\n        continue","0d94b67c":"# import the necessary packages\nimport numpy as np\nimport urllib\nimport cv2\n# METHOD #1: OpenCV, NumPy, and urllib\ndef url_to_image(i,url):\n    # download the image, convert it to a NumPy array, and then read\n    # it into OpenCV format\n    resp = urllib.request.urlopen(url)\n    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n    cv2.imwrite(f'.\/reddit_images\/{i}.jpg',image)","82a758bd":"for i,url in enumerate(ocr_urls):\n    try:\n        url_to_image(i,url)\n    except Exception as e:\n        print(e)","03b0832a":"!pip install \"paddleocr>=2.0.1\"\n!pip install paddlepaddle","6397e980":"from paddleocr import PaddleOCR,draw_ocr\nocr = PaddleOCR(lang='en') # need to run only once to download and load model into memory\n# images = sorted(glob.glob('\/home\/vineeth\/reddit_images\/*.jpg'),key=os.path.getmtime)\n# print(images)\n\nfor i,ind in enumerate(ocr_indexes):\n    result = ocr.ocr(f'..\/input\/reddit-vm-images\/reddit_vm_images\/{i}.jpg')\n    ocr_text = ''\n    for line in result:\n        ocr_text  = ocr_text + ' '+''.join(line[-1][0])\n    data['ocr'].iloc[ind] = ocr_text","79afaedd":"data = pd.read_csv('..\/input\/reddit-ocr-blog-text\/reddit_ocr_blog_text.csv')","d1a27336":"import re\n\ndata['title'] = data['title'].replace(np.nan,'')\ndata['body'] = data['body'].replace(np.nan,'')\ndata['ocr'] = data['ocr'].replace(np.nan,'')\ndata['blog_text'] = data['blog_text'].replace(np.nan,'')\ndata['text']=data[['title', 'body','ocr','blog_text']].agg(' '.join, axis=1)\ndata['text']=data['text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\ndata['text']=data['text'].apply(lambda x:' '.join(re.findall(r'\\w+', x)))","a7291725":"!conda install -c anaconda gensim -y\n!pip install smart_open==2.0.0","2540342a":"import sys\n# !{sys.executable} -m spacy download en\nimport re, numpy as np, pandas as pd\nfrom pprint import pprint\n\n# Gensim\nimport gensim, spacy, logging, warnings\nimport gensim.corpora as corpora\nfrom gensim.utils import lemmatize, simple_preprocess\nfrom gensim.models import CoherenceModel\nimport matplotlib.pyplot as plt\n\n# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['comment','vaccine','virus','study','case','forum','from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)","00169398":"data_sent_list = list(data['text'])\ndata_words = []\nfor sent in data_sent_list:\n    sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n    data_words.append(sent)","e58f4bae":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# !python3 -m spacy download en  # run in terminal once\ndef process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n    texts = [bigram_mod[doc] for doc in texts]\n    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n    texts_out = []\n    nlp = spacy.load('en', disable=['parser', 'ner'])\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    # remove stopwords once more after lemmatization\n    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n    return texts_out\n\ndata_ready = process_words(data_words)  # processed Text Data!","daae0d0c":"id2word = corpora.Dictionary(data_ready)\n\n\ncorpus = [id2word.doc2bow(text) for text in data_ready]\n\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=4, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=10,\n                                           passes=10,\n                                           alpha='symmetric',\n                                           iterations=100,\n                                           per_word_topics=True)\n\npprint(lda_model.print_topics())","bf51fd71":"def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n   \n    sent_topics_df = pd.DataFrame()\n\n   \n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    \n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n\n\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.head(10)","e064a47b":"pd.options.display.max_colwidth = 100\n\nsent_topics_sorteddf_mallet = pd.DataFrame()\nsent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n\nfor i, grp in sent_topics_outdf_grpd:\n    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n                                            axis=0)\n\n  \nsent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n\n\nsent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n\n\nsent_topics_sorteddf_mallet.head(10)","c00a5478":"doc_lens = [len(d) for d in df_dominant_topic.Text]\n\nplt.figure(figsize=(4,3), dpi=130)\nplt.hist(doc_lens, bins = 1000, color='navy')\nplt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\nplt.text(750,  85, \"Median : \" + str(round(np.median(doc_lens))))\nplt.text(750,  70, \"Stdev   : \" + str(round(np.std(doc_lens))))\nplt.text(750,  55, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\nplt.text(750,  40, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n\nplt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\nplt.tick_params(size=12)\nplt.xticks(np.linspace(0,1400,9))\nplt.title('Distribution of Document Word Counts', fontdict=dict(size=10))\nplt.show()","9f55450b":"from matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(stopwords=stop_words,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=10,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","78a315d5":"from matplotlib.ticker import FuncFormatter\n\n\ndef topics_per_document(model, corpus, start=0, end=1):\n    corpus_sel = corpus[start:end]\n    dominant_topics = []\n    topic_percentages = []\n    for i, corp in enumerate(corpus_sel):\n        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n        dominant_topics.append((i, dominant_topic))\n        topic_percentages.append(topic_percs)\n    return(dominant_topics, topic_percentages)\n\ndominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n\n# Distribution of Dominant Topics in Each Document\ndf = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\ndominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\ndf_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n\n# Total Topic Distribution by actual weight\ntopic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\ndf_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n\n# Top 3 Keywords for each Topic\ntopic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n\ndf_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\ndf_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\ndf_top3words.reset_index(level=0,inplace=True)\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4), dpi=100, sharey=True)\n\n# Topic Distribution by Dominant Topics\nax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\nax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\ntick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\nax1.xaxis.set_major_formatter(tick_formatter)\nax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=8))\nax1.set_ylabel('Number of Documents')\nax1.set_ylim(0, 1000)\n\n# Topic Distribution by Topic Weights\nax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\nax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\nax2.xaxis.set_major_formatter(tick_formatter)\nax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=8))\n\nplt.show()","742fe920":"# Get topic weights and dominant topics ------------\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n\n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n# Keep the well separated points (optional)\narr = arr[np.amax(arr, axis=1) > 0.35]\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 4\nmycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=800, plot_height=600)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\nshow(plot)","b516f423":"import pyLDAvis.gensim_models\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary=lda_model.id2word)\nvis","1d9e7fdc":"**Now let us find dominant topic in each document, percentage contribution and keywords for that document.**","9d21c081":"# Plot - Word cloud for keywords in each topic","ef7401f9":"**We now split the sentence into a list of words using gensim\u2019s simple_preprocess().<br> Setting the deacc=True option removes punctuations.**","26a63bcb":"**Run ocr recognition on these images and add a new column 'ocr' into dataframe** ","4c6af9d6":"****To build the LDA topic model using LdaModel(), we need the corpus and the dictionary. <br>Let\u2019s create them first and then build the model.****","3bdcbce2":"**Similarly collect images and let us do ocr on these images for text.<br> I used <a href=\"https:\/\/github.com\/PaddlePaddle\/PaddleOCR\">paddleocr<\/a> package for this.<br>The recognition result is not good for some images but majority of the extractions are good enough to be considered as text data.**","68dea585":"**The top 5 scored titles are about vaccine side effects and how apprehensive some people are about taking the vaccine.**","671fe73d":"**For each topic lets look at their keywords and its representative text.**","f44ae33e":"**Top Four vaccine myths from my interpretation of results:**\n<ol>\n<li><b>Contents of vaccine (like mercury) and associated allergic reactions in body.<\/b><\/li>\n<li><b>Disorders due to vaccine and improper findings due to less data.<\/b><\/li>\n<li><b>Risk of autism in childern and Measles Antivaccine progaganda.<\/b><\/li>\n<li><b>Vaccine shedding with vaccinated virus strain.<\/b><\/li>\n<\/ol>","91fa97cb":"# Plot - Bar Chart - Topics vs Number of Documents and Sum of Weightage of each topic vs Number of Documents","6bbb904f":"# Plot - Cluster Visualization using sklearn t-SNE algorithm","6fb74caa":"# Plot - pyLDAvis","dd6c3069":"# Plot - Histogram of Document word counts","b4969ba4":"**Let us see the top five titles based on scores in score column**","cdd16865":"**The Top 4 topics from lda model with their weightages are obtained. <br>Below are my interpretation of these key words:**\n<ol>\n<li><b>Contents of vaccine (like mercury) and associated allergic reactions in body.<\/b><\/li>\n<li><b>Disorders due to vaccine and improper findings due to less data.<\/b><\/li>\n<li><b>Risk of autism in childern and Measles Antivaccine progaganda.<\/b><\/li>\n<li><b>Vaccine shedding with vaccinated virus strain.<\/b><\/li>\n<\/ol>  \n","594dd4ce":"<!-- ![Coronavirus-vaccine-710x400.png](attachment:e303de74-d550-4c49-94e3-8a2c67026d02.png) -->\n<img align=left src = \"https:\/\/fortisbangalore.com\/frontend\/blogimage\/Covid-vaccination--1614614615.png\" width=\"500\">","d8f165f6":"**Let\u2019s form the bigram and trigrams using the Phrases model.<br> Lemmatize each word to its root form, keeping only nouns, adjectives, verbs and adverbs.**","9e6dea73":"**Scarpe html for text and add a new column blog_text with this data.**","ab596991":"**Here we can see that there is only 450 unique titles out of which word comment occured 1030 times.<br> \nWe can remove this word using stop words later.**","21e86525":"**A final csv is obtained after adding data from images and blogs. <br>I have added the final csv in the input so results can be viewed by running cells from here directly also.**","2b7d5fbe":"> credits: https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-visualization-how-to-present-results-lda-models\/","27c14a51":"**We have missing values in url and body columns. <br>Lets see how unique the titles are from title column.**","8f2e1451":"**We have a mean word count of 23 words with a standard deviation of 77.**","1f1baf8f":"**From url column we can see that there are images and news articles in them. <br>Let us try to extract text from them to get more data.<br>We will collect the urls with filetype image and html and their indices into a list.**","c74ffcf5":"# Vaccine Myths from Reddit Data\n","9a44f997":"**Let us combine title, body, ocr result and blog_text to get final data for LDA.**"}}