{"cell_type":{"c4cb088a":"code","a7e0754e":"code","3a86c39d":"code","a39c96e8":"code","70668a13":"code","2318efcb":"code","c27b891c":"code","c5cf220e":"code","4d86451d":"code","f856d353":"code","c4658575":"code","caf60c5c":"code","b90c0130":"code","a18c1782":"code","19ebb7e9":"code","02d165f7":"code","dbbcaf8b":"code","e3e76c3a":"code","9d39072d":"markdown"},"source":{"c4cb088a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\ndf_submission=pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\n\ndf_test=pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')\ndf_submission.head(1), df_test.head(1)\n","a7e0754e":"#insert data with 78 features and target\ndf_tmdb = pd.read_csv(\"..\/input\/tmdb-2\/tmdb.csv\")\ndf_tmdb.head(5)","3a86c39d":"#insert some libraries\nimport json\nimport ast\nfrom pprint import pprint\nimport seaborn as sns \nfrom scipy.stats import norm,skew\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport numpy as np\npd.set_option('display.max_columns', None)","a39c96e8":"#split tmdb into train and test \ndf_train= df_tmdb.iloc[0:3000] # first 3000 rows of the tmdb dataframe\ndf_test=df_tmdb.iloc[3000:7398]","70668a13":"df_train.shape, df_test.shape","2318efcb":"#split data into features and target. All features are either integer, float or dummy.\nfeatures = df_train.select_dtypes(include=['int64', 'float64', 'uint8', 'int8']).columns.tolist()\nfeatures.remove('log_revenue')\nfeatures_unseen = df_test.select_dtypes(include=['int64', 'float64', 'uint8', 'int8']).columns.tolist()\nfeatures_unseen.remove('log_revenue')\n\nX, y = df_train[features], df_train['log_revenue']","c27b891c":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)\n","c5cf220e":"#import libraries for light gbm and feature importance \nimport lightgbm as lgb\nimport eli5\n\nparams = {'num_leaves': 15,\n         'min_data_in_leaf': 25,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.8,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.7,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\n","4d86451d":"#build mogel \nlgb_model= lgb.LGBMRegressor(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)\nlgb_model.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=150)\n\neli5.show_weights(lgb_model, feature_filter=lambda x: x != '<BIAS>')\n","f856d353":"#make predictions \n\n#Prediction\ny_pred=lgb_model.predict(X_test)","c4658575":"#look at rmse and R-Squared \nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrmse = sqrt(mean_squared_error(y_test, y_pred))\n\n\nprint (\"R-Squared is:\", metrics.r2_score(y_test, y_pred))\nprint (\"The rmse is:\", rmse)","caf60c5c":"#look at actual and predicted values \ncompare = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ncompare.head(5)","b90c0130":"# look at actual and predicted values of first 50 entries in the dataset\ncompare1 = compare.head(50)\ncompare1.plot(kind='bar',figsize=(30,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","a18c1782":"#make prediction on unseen test dataset\nX_unseen=df_test[features_unseen]\nprediction_unseen= lgb_model.predict(X_unseen)","19ebb7e9":"prediction_unseen","02d165f7":"df_submission['revenue'] = np.expm1(prediction_unseen)\n","dbbcaf8b":"df_submission.head(5)","e3e76c3a":"df_submission[['id','revenue']].to_csv('submission_lgb.csv', index=False)","9d39072d":"This notebook uses the output database from https:\/\/www.kaggle.com\/nickrood\/can-star-power-predict-box-office-revenue\/output\nIn that niotebook I made predictions with a linear and random forest regressor model but also want to see how a Light GBM would perform on the data (even though Light GBM is not really recommended on small datasets...)"}}