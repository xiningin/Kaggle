{"cell_type":{"7092dea9":"code","07a28814":"code","b6189af5":"code","3a9fd67d":"code","20bf0cd6":"code","f30930db":"code","d39cbc43":"code","d31f1870":"code","2e64a54c":"code","09359cf4":"code","3a285193":"code","b7286bfb":"code","ef480b86":"code","b7ed414f":"code","c699e42c":"code","feea5929":"code","9d80ca2f":"code","7ccd2404":"code","b19fcc28":"code","74733236":"code","2a7586d7":"code","6d5b912a":"code","dec1fa21":"code","578bfce3":"code","56a4259e":"code","5fbb76e6":"code","c799918f":"code","0a47bfd5":"code","59a2df3d":"code","ec303492":"code","eeba8c56":"markdown","da9505b4":"markdown","9c893a1c":"markdown","0d63db31":"markdown","d78328ae":"markdown","c629bf6a":"markdown","aa907e0b":"markdown","ea05cdcc":"markdown"},"source":{"7092dea9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras \nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom datetime import datetime\nfrom sklearn.preprocessing import StandardScaler","07a28814":"train = pd.read_csv('..\/input\/electricity-consumption\/train.csv')\ntest = pd.read_csv('..\/input\/electricity-consumption\/test.csv')","b6189af5":"print(train.info())\ntrain.head()","3a9fd67d":"test.head(10)","20bf0cd6":"w = train.windspeed\nplt.plot(w)\nplt.xlabel('samples')\nplt.ylabel('windspeed_frequency')\nplt.title('Distribution of Windspeed')\nplt.show()","f30930db":"print(f'The average wind speed is {round(train.windspeed.mean(),2)}')\nprint(f'The maximum wind speed is {train.windspeed.max()} and minimum wind speed is {train.windspeed.min()}')","d39cbc43":"print(f'The average Pressure parameter is {round(train.pressure.mean(),2)}')\nprint(f'The maximum Pressure value is {train.pressure.max()} and minimum pressure value is {train.pressure.min()}')\nplt.scatter(train.pressure,train.electricity_consumption,c='lightblue')\nplt.show()","d31f1870":"import seaborn as sns\nprint(train.var2.value_counts())\nsns.countplot(x='var2',data=train)","2e64a54c":"plt.scatter(train.temperature,train.electricity_consumption,c='green')\nplt.xlabel('Temperature')\nplt.ylabel('Electricity_Consumption')\nplt.title('Distribution of Windspeed')\nplt.show()","09359cf4":"plt.plot(train.electricity_consumption)\nplt.xlabel('Samples ---->')\nplt.ylabel('Electricity_Consumption')\nplt.show()","3a285193":"from sklearn.preprocessing import LabelEncoder\nLE = LabelEncoder()\ntrain['var2']=LE.fit_transform(train.var2)","b7286bfb":"print(train.var2.value_counts())\ntrain.head()","ef480b86":"test['var2']=LE.fit_transform(test.var2)\ntest.head()","b7ed414f":"def datetounix(df):\n    # Calling an list of unixtime\n    unixtime = []\n    \n    # Convert Date to seconds\n    for date in df['datetime']:\n        unixtime.append(time.mktime(date.timetuple()))\n    \n    # Replacing Date with unixtime list\n    df['datetime'] = unixtime\n    return(df)","c699e42c":"train['datetime'] = pd.to_datetime(train['datetime'])\ntest['datetime'] = pd.to_datetime(test['datetime'])\ntest.info()","feea5929":"train.info()","9d80ca2f":"train['Weekday'] = [datetime.weekday(date) for date in train.datetime]\ntrain['Year'] = [date.year for date in train.datetime]\ntrain['Month'] = [date.month for date in train.datetime]\ntrain['Day'] = [date.day for date in train.datetime]\ntrain['Time'] = [((date.hour*60+(date.minute))*60)+date.second for date in train.datetime]\ntrain['Week'] = [date.week for date in train.datetime]\ntrain['Quarter'] = [date.quarter for date in train.datetime]","7ccd2404":"test['Weekday'] = [datetime.weekday(date) for date in test.datetime]\ntest['Year'] = [date.year for date in test.datetime]\ntest['Month'] = [date.month for date in test.datetime]\ntest['Day'] = [date.day for date in test.datetime]\ntest['Time'] = [((date.hour*60+(date.minute))*60)+date.second for date in test.datetime]\ntest['Week'] = [date.week for date in test.datetime]\ntest['Quarter'] = [date.quarter for date in test.datetime]\ntest.head()","b19fcc28":"train.head()","74733236":"print(train.shape,test.shape)","2a7586d7":"X_train = train.drop(['ID','electricity_consumption'],axis=1)\nX_train.head()","6d5b912a":"X_train = datetounix(X_train)","dec1fa21":"X = X_train.values\ny = train['electricity_consumption'].values","578bfce3":"X","56a4259e":"X_test = datetounix(test).drop(['ID'], axis=1).values\nX_test","5fbb76e6":"# Standard Scaling\nsc = StandardScaler()\nX = sc.fit_transform(X)\n# Normalizing the target variables\ny_train = (y - min(y))\/(max(y) - min(y))\ny_train","c799918f":"X_test = sc.fit_transform(X_test)","0a47bfd5":"classifier = Sequential()\nclassifier.add(Dense(units = 10, kernel_initializer = 'uniform',input_dim =13, activation = 'relu'))\nclassifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 3, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\nclassifier.compile(optimizer = 'Adam', loss = 'mean_squared_error', metrics = ['mae']) # I used both Adam and SGD Optimzer, out of which the loss function was least in Adam.\nclassifier.fit(X, y_train, batch_size = 16, epochs = 25)","59a2df3d":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred * (max(y) - min(y))) + min(y)\n\npredictions = [int(i) for i in y_pred]\n\nSolution = pd.DataFrame()\nSolution['ID'] = test['ID']\n\n# Prepare Solution dataframe\nSolution['electricity_consumption'] = predictions\nSolution['electricity_consumption'].unique()\nSolution","ec303492":"Solution.to_csv('My_submission.csv')","eeba8c56":"Extracting data from datetime column and creating new features","da9505b4":"# 1. Exploratory Data Analysis","9c893a1c":"# 2.Feature Engineering Process","0d63db31":"\u2018Time\u2019 is the most important factor which ensures success in a business. It\u2019s difficult to keep up with the pace of time.  But, technology has developed some powerful methods using which we can \u2018see things\u2019 ahead of time!\nNope, not the time machine, we are talking about the methods of prediction & forecasting. As the name \u2018time series forecasting\u2019 suggests, it involves working on time (years, days, hours, minutes) based data, to derive hidden insights to make informed decision making. \nHence, here is the solution for the Janata Hack: Time series forecasting for Electricity usage that gave me **Top 2% (88th rank from 4600+ participants). **","d78328ae":"# Do UPVOTE if you like this kernel Notebook! Happy Learning!!","c629bf6a":"# 3. Model Building using Dense Neural Networks","aa907e0b":"REMEMBER: 'A'---> 1, 'B'--->1,'C'---> 2","ea05cdcc":"Convert the dataframe to csv file format and check the output area to download and submit your predictions.  "}}