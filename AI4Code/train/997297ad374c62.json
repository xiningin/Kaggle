{"cell_type":{"a99786ad":"code","5a3cbf10":"code","597c6aa6":"code","af213c34":"code","78f58cf8":"code","a49ade57":"code","8be3b481":"code","b47a345f":"code","4f5a8ecc":"code","5e0cbb4a":"code","acea60fa":"code","e248de28":"code","3004b223":"code","a71296da":"code","22c6ef87":"code","3f9023b7":"code","87cd9a14":"code","b220d39a":"code","24bd015d":"code","d65e580b":"code","843383a6":"markdown","a2ff45be":"markdown","ef82b286":"markdown","ad67563f":"markdown","5bd005dd":"markdown","890d21ec":"markdown","11543cb1":"markdown","677061b1":"markdown","5167d2ed":"markdown","401d0007":"markdown"},"source":{"a99786ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a3cbf10":"\n# Additional packages\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport random\n\nfrom sklearn import model_selection, metrics\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.metrics import roc_auc_score\nimport os, psutil\n\n#Lgbm\nimport lightgbm as lgb\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform","597c6aa6":"# Parameter Setting\np_verbose = 250\np_estimators_CV = 1000\np_estimators_FNL = 1500\np_iter = 30","af213c34":"# Read the data\ndf_test=pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\ndf_train=pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ndf_sub=pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","78f58cf8":"df_train.info()","a49ade57":"df_test.info()","8be3b481":"# Null value analysis\n# No. of variables and % of Null values\ncounter = 0\nfor col in df_train.columns:\n    if df_train[col].isnull().sum() > 0:\n        counter = counter + 1\n        print('{} with {:.4%} null value'.format(col, df_train[col].isnull().sum()\/len(df_train)))\n\nif counter == 0:\n    print('There is no variable with null value')","b47a345f":"standardEncoder = StandardScaler()\ndf_train_std = pd.DataFrame(standardEncoder.fit_transform(df_train.iloc[:,1:-1]), columns =df_train.iloc[:,1:-1].columns)\ndf_test_std = pd.DataFrame(standardEncoder.transform(df_test.iloc[:,1:]), columns =df_test.iloc[:,1:].columns)\nprint(df_train_std.shape)\ndf_train_std.hist(figsize=(16,20),color = 'g',xlabelsize=0,ylabelsize=0)","4f5a8ecc":"X = df_train_std\nY = df_train['target']\nprint(X.shape)\nprint('='  *  30)\nprint(Y.shape)","5e0cbb4a":"X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.05, random_state=4355)","acea60fa":"del df_train \ndel df_test \ndel df_train_std","e248de28":"def learning_rate_010_decay_power_099(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_010_decay_power_0995(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_005_decay_power_099(current_iter):\n    base_learning_rate = 0.05\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\nfit_params={\"early_stopping_rounds\":10, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,Y_test)],\n            'eval_names': ['valid'],\n            'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n            'verbose': p_verbose}\n\nparam_test ={'num_leaves':  random.uniform(900, 1000), \n             'min_child_samples': random.uniform(900, 1000), \n             'min_child_weight': random.uniform(9, 11), \n             'subsample': random.uniform(0.15, 0.25), \n             'colsample_bytree': random.uniform(0.7, 0.8),\n#             'max_depth' : sp_randint(5, 15),            \n             'reg_alpha': random.uniform(0.05, 0.15), \n             'reg_lambda': random.uniform(5, 15)}\n\nparam_test ={'num_leaves': sp_randint(900, 1000), \n             'min_child_samples': sp_randint(900, 1000), \n             'min_child_weight': sp_uniform(loc=9.0, scale=2.0),\n             'subsample': sp_uniform(loc=0.150, scale=0.1), \n             'colsample_bytree': [0.70, 0.72, 0.74, 0.76, 0.78, 0.80],\n             'reg_alpha': [0.01, 0.04, 0.08, 0.1, 0.14, 0.18, 0.2],\n             'reg_lambda': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n            }","3004b223":"clf = lgb.LGBMClassifier(max_depth=-1, random_state=1234, silent=True, metric='auc', n_estimators=p_estimators_CV,  class_weight='balanced', n_jobs = -1)","a71296da":"gs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=p_iter,\n    scoring='roc_auc',\n    n_jobs = -1,\n    cv=3,\n    refit=True,\n    verbose=p_verbose,\n    random_state=4563)","22c6ef87":"gs.fit(X_train, Y_train, **fit_params)","3f9023b7":"print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","87cd9a14":"opt_parameters = {'colsample_bytree': 0.7563492589437595, 'min_child_samples': 977, 'min_child_weight': 10.0\n                  , 'num_leaves': 934, 'reg_alpha': 0.1, 'reg_lambda': 10, 'subsample': 0.21965621584761524} \n\n# gs = lgb.LGBMClassifier(max_depth=-1, random_state=1234, silent=True, metric='auc', n_estimators=p_estimators_FNL,  class_weight='balanced', n_jobs = -1)\n#set optimal parameters\n# gs.set_params(**opt_parameters)\n\n#gs.fit(X_train, Y_train, **fit_params ) \n\n# preds = gs.predict_proba(df_test_std)[:,1]\n# df_rst = pd.concat([df_sub.iloc[:,0:1], pd.DataFrame(preds, columns = ['target'])], axis = 1)\n# df_rst.to_csv(\".\/submission.csv\",index=False)\n# print('Done!')","b220d39a":"\n#Configure from the HP optimisation\nclf_final = lgb.LGBMClassifier(**gs.best_estimator_.get_params())\n\nclf_final = lgb.LGBMClassifier(max_depth=-1, random_state=3453456, silent=True, metric='auc', n_estimators=p_estimators_FNL,  class_weight='balanced', n_jobs = -1)\n\npreds = np.zeros(df_test_std.shape[0])\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nkf = StratifiedKFold(n_splits = 5, random_state=434512,shuffle=True)\n\nauc = []\nmodel_lst = []\nn = 0\n\nfor train_idx, test_idx in kf.split(X,Y):\n    x_train, x_val = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n    #model = lgb.LGBMClassifier(**opt_parameters)\n    clf_final.fit(x_train, y_train, eval_set = [(x_val,y_val)], early_stopping_rounds = 30, eval_metric = \"auc\", verbose = 100)\n#    preds += model.predict_proba(df_test_sub)[:,1]\/kf.n_splits\n    model_lst.append(clf_final)\n    auc.append(roc_auc_score(y_val, clf_final.predict_proba(x_val)[:, 1]))\n#    gc.collect()\n    print(f\"fold: {n+1}, auc: {auc[n]}\")\n    n+=1   \n","24bd015d":"\n# Prediction\nfor i in range(len(model_lst)):\n    preds += model_lst[i].predict_proba(df_test_std)[:,1]\/kf.n_splits\ndf_rst = pd.concat([df_sub.iloc[:,0:1], pd.DataFrame(preds, columns = ['target'])], axis = 1)\ndf_rst.to_csv(\".\/submission.csv\",index=False)\nprint('Done!')    \n    ","d65e580b":"# Data Submission\n# preds = gs.predict_proba(df_test_std)[:,1]\n# df_rst = pd.concat([df_sub.iloc[:,0:1], pd.DataFrame(preds, columns = ['target'])], axis = 1)\n# df_rst.to_csv(\".\/submission.csv\",index=False)\n# print('Done!')\n\n","843383a6":"# Model  - LightGBM","a2ff45be":"<div style=\"color:#D81F26;\n           display:fill;\n           border-style: solid;\n           border-color:#C1C1C1;\n           font-size:14px;\n           font-family:Calibri;\n           background-color:#373737;\">\n<h2 style=\"text-align: center;\n           padding: 10px;\n           color:#FFFFFF;\">\n======= Playground Nov 2021 =======\n<\/h2>\n<\/div>","ef82b286":"# About this notebook","ad67563f":"### Use the Optimal parameters","5bd005dd":"## Check if there is any null value","890d21ec":"# Variables scaling","11543cb1":"# Modelling data preparation","677061b1":"This notebook is for submission to the Playground of Nov 2021. The LGBM Classifier has been used in the notebook.  \n\n## Summary\n\n* Check if there are any null values in the features\n* Standardize feature's values\n* Optimize the hyperparameters for LGBM\n* Get the optimal hyperparameters for cross-validation and data submission","5167d2ed":"# Exploratory Data Analysis","401d0007":"### Use the Best Parameter from HP"}}