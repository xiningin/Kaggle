{"cell_type":{"0e7baeb5":"code","1f3cead2":"code","e0b0dbb9":"code","88fe8e11":"code","18789212":"code","26fd078e":"code","e7a39bac":"code","c68483d3":"code","d190b355":"code","50f50f05":"code","d70cf261":"code","1fe5cc96":"code","3cd391d2":"code","5ba9d44f":"code","90f8733b":"code","83126568":"code","ce44e606":"code","91ad4c5b":"code","7b1080ac":"code","4aa8466f":"code","11b5d855":"code","53c2c71a":"code","3f343eb1":"code","fb5bb87b":"code","312c2bf6":"code","68b6fb9b":"code","59871cbe":"code","82b82aa4":"code","2ac0514e":"code","ac264df7":"code","94d3aade":"code","1f9455a5":"code","1218b1fd":"code","20d6ed82":"code","e396d914":"code","919a10a9":"code","ce4cb758":"code","3100c880":"code","62d9375e":"code","a049f169":"code","2134daf5":"code","69d3dac0":"code","ca84d16d":"code","eea89c4b":"code","c7821029":"code","d99a7659":"code","9dc0608d":"code","a7b4aa93":"code","732aeff0":"code","7b0eae1f":"code","a3641acc":"code","ed0d8007":"code","08828744":"code","2d4cbfd6":"code","9363926d":"code","77854130":"code","6cdb0871":"code","b1e3eff3":"code","760d1946":"code","0aa30fea":"code","5a9d8b2b":"code","94d7b8fe":"code","12e87a96":"code","73e27ff7":"code","a976ec6a":"code","5d3c3ba6":"code","f2b21f95":"code","09fc631c":"code","eae11fe5":"code","f2bedba1":"code","e756551f":"code","869a7fc8":"code","26ef059b":"code","79968b21":"code","491de5c3":"code","4631bb80":"code","dad2cc6d":"markdown","397ec794":"markdown"},"source":{"0e7baeb5":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\n","1f3cead2":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_train.tail()","e0b0dbb9":"df_train[df_train['CoapplicantIncome']==0]","88fe8e11":"df_train['Education'].unique()","18789212":"df_train.shape","26fd078e":"df_train['Loan_Status'].value_counts()","e7a39bac":"df_train.groupby('Gender')['Loan_Status'].count()","c68483d3":"df_train['Loan_Amount_Term'].unique()","d190b355":"df_train[df_train['Loan_Amount_Term'].isnull()]","50f50f05":"df_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_test.head()","d70cf261":"df_test.shape","1fe5cc96":"# lets check for null values but lest concate both dataset first\n#df = pd.concat([df_train,df_test])\n#df.head()","3cd391d2":"#df.shape","5ba9d44f":"df_train.isnull().sum()","90f8733b":"df_train.describe()","83126568":"df_train.info()","ce44e606":"#df.drop(['CoapplicantIncome'],axis=1,inplace=True)","91ad4c5b":"#lets work on categorical variables and identify all of them\ncat_vars1 = [feature for feature in df_train.columns if df_train[feature].dtypes == 'O' and feature !='Loan_Status']\ncat_vars1\ncat_vars2 = [feature for feature in df_test.columns if df_test[feature].dtypes == 'O' and feature !='Loan_Status']\ncat_vars2","7b1080ac":"df_train['Gender'].fillna('Female',inplace=True)\ndf_test['Gender'].fillna('Female',inplace=True)","4aa8466f":"# lets impute all those except loan status with mode values for null values\n#df[cat_vars] = df[cat_vars].fillna(df[cat_vars].mode()[0])\nfor feat in cat_vars1:\n    df_train[feat]= df_train[feat].fillna(df_train[feat].mode()[0])\nfor feat in cat_vars2:\n    df_test[feat]= df_test[feat].fillna(df_test[feat].mode()[0])","11b5d855":"df_train.isnull().sum()","53c2c71a":"df_test.isnull().sum()","3f343eb1":"df_train.info()","fb5bb87b":"df_train.head()","312c2bf6":"# lets find numerical variables\nnum_vars1 = [feature for feature in df_train.columns if df_train[feature].dtypes != 'O' and feature !='Loan_Status']\nnum_vars1\nnum_vars2 = [feature for feature in df_test.columns if df_test[feature].dtypes != 'O' and feature !='Loan_Status']\nnum_vars2","68b6fb9b":"df_train['Credit_History'] = df_train['Credit_History'].fillna(2)\ndf_test['Credit_History'] = df_test['Credit_History'].fillna(2)","59871cbe":"df_train[num_vars1].isnull().sum()","82b82aa4":"df_train.info()","2ac0514e":"for feat in num_vars1:\n    sns.boxplot(df_train[feat])\n    plt.show()","ac264df7":"# data is not much and and it is not giving any clear indication\n# lets impute with median as there seems to be outliers\ndf_train[num_vars1] = df_train[num_vars1].fillna(df_train[num_vars1].median())\ndf_test[num_vars2] = df_test[num_vars2].fillna(df_test[num_vars2].median())","94d3aade":"df_test.head()","1f9455a5":"df_train.isnull().sum()","1218b1fd":"df_train['Dependents'].unique()","20d6ed82":"df_train.groupby(['Dependents'])['Loan_Status'].count()","e396d914":"df_train['Dependents']= df_train['Dependents'].map({'0':0,'1':1,'2':2,'3+':3})\n#df['Dependents']= df['Dependents'].map({'0':'No_Dep','1':'OneR1+_Dep','2':'OneR1+_Dep','3+':'OneR1+_Dep'})\ndf_train['Loan_Status']= df_train['Loan_Status'].map({'Y':1,'N':0})\ndf_train['Gender']= df_train['Gender'].map({'Male':1,'Female':0})\ndf_train['Married']= df_train['Married'].map({'Yes':1,'No':0})\ndf_train['Self_Employed']= df_train['Self_Employed'].map({'Yes':1,'No':0})\ndf_train['Education']= df_train['Education'].map({'Graduate':1,'Not Graduate':0})\ndf_train['Property_Area']= df_train['Property_Area'].map({'Urban':0,'Semiurban':1,'Rural':2})","919a10a9":"df_test['Dependents']= df_test['Dependents'].map({'0':0,'1':1,'2':2,'3+':3})\n#df['Dependents']= df['Dependents'].map({'0':'No_Dep','1':'OneR1+_Dep','2':'OneR1+_Dep','3+':'OneR1+_Dep'})\n#df_test['Loan_Status']= df_test['Loan_Status'].map({'Y':1,'N':0})\ndf_test['Gender']= df_test['Gender'].map({'Male':1,'Female':0})\ndf_test['Married']= df_test['Married'].map({'Yes':1,'No':0})\ndf_test['Self_Employed']= df_test['Self_Employed'].map({'Yes':1,'No':0})\ndf_test['Education']= df_test['Education'].map({'Graduate':1,'Not Graduate':0})\ndf_test['Property_Area']= df_test['Property_Area'].map({'Urban':0,'Semiurban':1,'Rural':2})","ce4cb758":"#df.groupby('Credit_History')['Loan_Status'].sum()","3100c880":"#df.groupby('Credit_History')['Loan_Status'].count()","62d9375e":"#df.groupby('Self_Employed')['Loan_Status'].sum()","a049f169":"#lets get the dummy vars for categorical variables\n#df = pd.concat([df, pd.get_dummies(df['Married'],drop_first=True)],axis=1).drop('Married',axis=1)\n#df = pd.concat([df, pd.get_dummies(df['Gender'],drop_first=True)],axis=1).drop('Gender',axis=1)\n#df = pd.concat([df, pd.get_dummies(df['Dependents'],drop_first=True)],axis=1).drop('Dependents',axis=1)\n#df = pd.concat([df, pd.get_dummies(df['Education'],drop_first=True)],axis=1).drop('Education',axis=1)\n#df = pd.concat([df, pd.get_dummies(df['Self_Employed'],drop_first=True)],axis=1).drop('Self_Employed',axis=1)\n#df = pd.concat([df, pd.get_dummies(df['Property_Area'],drop_first=True)],axis=1).drop('Property_Area',axis=1)","2134daf5":"df_train.info()","69d3dac0":"df_test.head()","ca84d16d":"df_train['Total_Inc'] = df_train['ApplicantIncome'] + df_train['CoapplicantIncome']\ndf_train['Total_Inc'] = np.log(df_train['Total_Inc'])\n#df_train['Debt_Income_Ratio'] = df_train['Total_Inc'] \/ df_train['LoanAmount']\n#df_train['EMI'] = (df_train['LoanAmount']*0.09*(1.09**df_train['Loan_Amount_Term']))\/(1.09**df_train['Loan_Amount_Term']-1)\n","eea89c4b":"df_train.drop(['ApplicantIncome','CoapplicantIncome'],axis=1,inplace=True)\n#df_train.drop(['Total_Inc','LoanAmount','Loan_Amount_Term'],axis=1,inplace=True)","c7821029":"df_test['Total_Inc'] = df_test['ApplicantIncome'] + df_test['CoapplicantIncome']\ndf_test['Total_Inc'] = np.log(df_test['Total_Inc'])\n#df_test['Debt_Income_Ratio'] = df_test['Total_Inc'] \/ df_test['LoanAmount']\n#df_test['EMI'] = (df_test['LoanAmount']*0.09*(1.09**df_test['Loan_Amount_Term']))\/(1.09**df_test['Loan_Amount_Term']-1)\n#df_test['Total_Inc'] = np.log(df_test['Total_Inc'])","d99a7659":"df_test.drop(['ApplicantIncome','CoapplicantIncome'],axis=1,inplace=True)\n#df_test.drop(['Total_Inc','LoanAmount','Loan_Amount_Term'],axis=1,inplace=True)","9dc0608d":"#df_train = df[:614]\n#df_test = df[614:]","a7b4aa93":"df_1 = df_train.copy()","732aeff0":"df_train.drop(['Loan_ID'],inplace=True,axis=1)","7b0eae1f":"df_train.head()","a3641acc":"X = df_train.drop('Loan_Status',axis=1)\ny = df_train['Loan_Status']","ed0d8007":"# Rerunning above with resampled data\n\n#sm = SMOTE(random_state=1, ratio = 1)\n#X_train_res, y_train_res = sm.fit_sample(X, y)","08828744":"X.shape","2d4cbfd6":"# Normalize using MinMaxScaler to constrain values to between 0 and 1.\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n#scaler.fit(X_train_res)\nscaler.fit(X)\nX_train = scaler.transform(X)","9363926d":"#X_train = pd.DataFrame(X_train,columns=X.columns)\n#X_train.head()","77854130":"# Create first pipeline for base without reducing features.\n\npipe = Pipeline([('classifier' , RandomForestClassifier())])\n\n# Create param grid.\n\nparam_grid = [\n    {'classifier' : [LogisticRegression()],\n     'classifier__penalty' : ['l1', 'l2'],\n    'classifier__C' : np.logspace(-4, 4, 20),\n    'classifier__solver' : ['liblinear']},\n    {'classifier' : [RandomForestClassifier()],\n    'classifier__n_estimators' : list(range(10,101,10)),\n    'classifier__max_features' : list(range(5,15,5))}\n]\n\n# Create grid search object\n\nclf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n\n# Fit on data\n\nbest_clf = clf.fit(X_train, y)","6cdb0871":"best_clf.best_estimator_.get_params()['classifier']","b1e3eff3":"print('Model accuracy is',best_clf.score(X_train, y))","760d1946":"test = df_test.copy()","0aa30fea":"df_test.drop(['Loan_ID'],axis=1,inplace=True)","5a9d8b2b":"df_test.head()","94d7b8fe":"X_test = scaler.transform(df_test)","12e87a96":"clf = LogisticRegression()\nclf.fit(X_train, y)\ny_pred_log_reg = clf.predict(X_test)\nacc_log_reg = round( clf.score(X_train,y) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_log_reg) + '%')","73e27ff7":"clf = SVC()\nclf.fit(X_train, y)\ny_pred_SVC = clf.predict(X_test)\nacc_svc = round( clf.score(X_train, y) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_svc) + '%')","a976ec6a":"clf = LinearSVC()\nclf.fit(X_train, y)\ny_pred_linearsvc = clf.predict(X_test)\nacc_linear_svc = round( clf.score(X_train, y) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_linear_svc) + '%')","5d3c3ba6":"clf = KNeighborsClassifier(n_neighbors = 4)\nclf.fit(X_train, y)\ny_pred_knn = clf.predict(X_test)\nacc_knn = round( clf.score(X_train, y) * 100, 2) \nprint (\"Train Accuracy: \" + str(acc_knn) + '%')","f2b21f95":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y)\ny_pred_DT = clf.predict(X_test)\nacc_decision_tree = round( clf.score(X_train, y) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_decision_tree) + '%')","09fc631c":"clf = RandomForestClassifier(n_estimators=50)\nclf.fit(X_train, y)\ny_pred_random_forest = clf.predict(X_test)\nacc_random_forest = round( clf.score(X_train, y) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_random_forest) + '%')","eae11fe5":"clf = GaussianNB()\nclf.fit(X_train, y)\ny_pred_GB = clf.predict(X_test)\nacc_gnb = round( clf.score(X_train, y) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_gnb) + '%')","f2bedba1":"clf = Perceptron(max_iter=6, tol=None)\nclf.fit(X_train, y)\ny_pred_perceptron = clf.predict(X_test)\nacc_perceptron = round( clf.score(X_train, y) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_perceptron) + '%')","e756551f":"clf = SGDClassifier(max_iter=5, tol=None)\nclf.fit(X_train, y)\ny_pred_SGD = clf.predict(X_test)\nacc_sgd = round( clf.score(X_train, y) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_sgd) + '%')","869a7fc8":"#X_train.get_values","26ef059b":"!pip install xgboost\nimport xgboost\nclassifier = xgboost.XGBClassifier()\nclassifier.fit(X_train, y)\n# Predicting the Test set results\ny_pred_XGB = classifier.predict(X_test)\nacc_XGB = round( clf.score(X_train, y) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_XGB) + '%')","79968b21":"models = pd.DataFrame({\n    'Model': ['LR', 'SVM', 'L-SVC', \n              'KNN', 'DTree', 'RF', 'NB', \n              'Perceptron', 'SGD','XGB'],\n    \n    'Score': [acc_log_reg, acc_svc, acc_linear_svc, \n              acc_knn,  acc_decision_tree, acc_random_forest, acc_gnb, \n              acc_perceptron, acc_sgd, acc_XGB]\n    })\n\nmodels = models.sort_values(by='Score', ascending=False)\nmodels","491de5c3":"submission = pd.DataFrame({\n        \"Loan_ID\": test[\"Loan_ID\"],\n        \"Loan_Status\":y_pred_log_reg\n    })\nsubmission['Loan_Status'] = submission['Loan_Status'].map({1:'Y',0:'N'})\nsubmission.to_csv('Loan_submission.csv', index=False)","4631bb80":"submission.Loan_Status.value_counts()","dad2cc6d":"### train test split","397ec794":"### Exploratory data analysis"}}