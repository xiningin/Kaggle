{"cell_type":{"61993413":"code","3a753459":"code","90760460":"code","f0b48e64":"code","267a14f8":"code","6a7336df":"code","8edd4abb":"code","27c26b7a":"code","b2d74273":"code","e2247dc3":"code","321090f5":"code","28cca373":"code","896ecdfb":"code","781a22de":"code","b551d413":"code","335048f1":"code","b80cd0c1":"code","dd781b1e":"code","8b1144f6":"code","54104162":"code","ec8cb804":"code","b43096cb":"code","13d047cc":"code","16131eb2":"code","4da17b66":"code","d4770482":"code","1a46edf3":"code","cfa22561":"code","22ec08c6":"code","da9272fc":"code","1fb2350d":"markdown","b7a31dc9":"markdown","87e67e9e":"markdown","9e566945":"markdown","56efccaa":"markdown","ddf7c592":"markdown","6ed5ba11":"markdown","b8accdfe":"markdown","4c790fc1":"markdown","d5783779":"markdown","aeb43df9":"markdown","0b70e042":"markdown","77631c24":"markdown","dceffe2e":"markdown","6ad1bd5e":"markdown"},"source":{"61993413":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\nimport folium\nfrom folium import plugins\nsns.set()\n\nimport os\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ndata_folder = '..\/input\/crimes-in-boston'\n# Any results you write to the current directory are saved as output.","3a753459":"CRIME_DATA_FILE = os.path.join(data_folder, \"crime.csv\")\nprint(\"Crime data:\", CRIME_DATA_FILE)\ncrime_df = pd.read_csv(CRIME_DATA_FILE, encoding = 'ISO-8859-1')  # note the odd encoding...\n\nprint(\"Number of crimes:\", len(crime_df))\n\n# Peek a bit\ncrime_df.head()","90760460":"# Do a bit of data processing\ncrime_df.UCR_PART = crime_df.UCR_PART.astype('category')\ncrime_df.OFFENSE_CODE_GROUP = crime_df.OFFENSE_CODE_GROUP.astype('category')\ncrime_df.SHOOTING.fillna(\"N\", inplace=True)  # replace nan for shootings with 'N' for \"no\"\ncrime_df.OCCURRED_ON_DATE = pd.to_datetime(crime_df.OCCURRED_ON_DATE)  # use Pandas datetime\ncrime_df.Location = crime_df.Location.apply(eval)  # set location as tuple\ncrime_df.Lat.replace(-1.0, None, inplace=True)\ncrime_df.Long.replace(-1.0, None, inplace=True)\ncrime_df.dropna(subset=[\"Lat\", \"Long\"], inplace=True)\n\nrename_map = {\n    \"OFFENSE_CODE\": \"Code\",\n    \"OFFENSE_CODE_GROUP\": \"Code_group\",\n    \"OFFENSE_DESCRIPTION\": \"Description\",\n    \"OCCURRED_ON_DATE\": \"Date\",\n    \"DISTRICT\": \"District\",\n    \"STREET\": \"Street\",\n    \"YEAR\": \"Year\",\n    \"MONTH\": \"Month\",\n    \"HOUR\": \"Hour\",\n    \"DAY_OF_WEEK\": \"Day_of_week\",\n    \"SHOOTING\": \"Shooting\"\n}\n\ncrime_df.rename(columns=rename_map, inplace=True)\ncrime_df.sort_values(by=\"Date\", inplace=True)","f0b48e64":"# Let's keep the major crimes\ncrime_df = crime_df[crime_df.UCR_PART == \"Part One\"]\n# Remove the unused categories\ncrime_df.Code_group.cat.remove_unused_categories(inplace=True)","267a14f8":"crime_df.head()","6a7336df":"print(crime_df.Code_group.value_counts())\nprint()\nprint(crime_df.groupby(\"Year\").Code_group.value_counts())","8edd4abb":"# NB: removing unused categories was helpful here because\n# setting the code groups to 'categorical' before filtering Part One crimes\n# meant Seaborn would pickup the empty Part (One, Two) categories\ng = sns.catplot(y=\"Code_group\", kind=\"count\",\n                data=crime_df,\n                order=crime_df.Code_group.value_counts().index,\n                aspect=1.6,\n                palette=\"muted\")\ng.set_axis_labels(\"Number of occurrences\", \"Offense group\")","27c26b7a":"# NB: removing unused categories was helpful here because\n# setting the code groups to 'categorical' before filtering Part One crimes\n# meant Seaborn would pickup the empty Part (One, Two) categories\ng = sns.catplot(y=\"Code_group\", col=\"Year\", col_wrap=2, kind=\"count\",\n                data=crime_df,\n                order=crime_df.Code_group.value_counts().index,\n                aspect=1.5,\n                height=3,\n                palette=\"muted\")\ng.set_axis_labels(x_var=\"Number of occurrences\", y_var=\"Offense group\")","b2d74273":"base_location = crime_df.Location.iloc[0]  # grab location of one offense\nbase_location","e2247dc3":"boston_map = folium.Map(location=base_location,\n                        prefer_canvas=True,\n                        zoom_start=12,\n                        min_zoom=12)\nplugins.ScrollZoomToggler().add_to(boston_map)\n\nboston_map","321090f5":"# Now we add the homicides up to 2016\nyear = 2016\nfor row in crime_df[(crime_df.Code_group == \"Homicide\") & (crime_df.Year <= year)].itertuples(index=False):\n    icon = folium.Icon(color='red', icon='times', prefix='fa')\n    popup_txt = str(row.Date)\n    if row.Shooting == 'Y':\n        popup_txt = \"Shooting \" + popup_txt\n    folium.Marker(row.Location, icon=icon, popup=popup_txt).add_to(boston_map)\n\nboston_map","28cca373":"times_ = crime_df[crime_df.Code_group == \"Homicide\"].groupby(by=\"Year\").Date\n\n# Create counters for number of events\ncounts_ = []\nfor i, t in times_:\n    counts_.append(np.arange(len(t)))","896ecdfb":"fig, axes = plt.subplots(2, 2, figsize=(14, 9))\naxes = axes.flatten()\nfor i, (year, t) in enumerate(times_):\n    axes[i].step(t, counts_[i], where='post')\n    axes[i].set_title(f\"Number of homicides. Year = {year}\")\n    axes[i].set_xlabel(\"Date\")\n    axes[i].set_ylabel(\"Homicide count\")\nfig.tight_layout()","781a22de":"yearly_homic_rates = {}\nfor i, (y, ti) in enumerate(times_):\n    dt_window = ti.iloc[-1] - ti.iloc[0]\n\n    rate_mle = counts_[i][-1] \/ dt_window.days  # Poisson event rate in days\n    yearly_homic_rates[y] = rate_mle\n    print(f\"Rate for year {y} (days^{{-1}}): {rate_mle:.3f}\", f\" i.e. every {1.\/rate_mle:.2f} days\")","b551d413":"from scipy.spatial import distance as scdist\ndef ripley_K(data, t):\n    \"\"\"\n    Args\n        data (ndarray): point pattern data array\n        t (float): interval radius\n    \"\"\"\n    data = data.astype('datetime64[s]')  # ensure time data is in ms\n    dist = scdist.pdist(data, 'euclidean')  # compute the array of pair-wise distances\n    lbda = data.size \/ (data[-1] - data[0]).astype(float)\n    ksum = 1.\/lbda * np.sum((dist <= t), axis=-1) \/ data.shape[0]\n    return ksum","335048f1":"evt_time_data = crime_df[crime_df.Code_group == \"Homicide\"].Date[:, None].astype('datetime64[s]')\n\ndist = scdist.pdist(evt_time_data.astype(\"datetime64[s]\"), 'euclidean')\ndist","b80cd0c1":"lbda = evt_time_data.size \/ (evt_time_data[-1] - evt_time_data[0]).astype(float)\nprint(f\"Event rate {lbda.item():.3e} events\/s\")\nprint(f\"Event rate {86400*lbda.item():.3f} events\/day\")","dd781b1e":"# Let's make our testing time values run over 1 1\/2 years\nripley_test_times = np.linspace(0, 86400 * 365 * 1.5, 150)  # time expressed in seconds\n\nkstat = ripley_K(evt_time_data, ripley_test_times[:, None])","8b1144f6":"fig, ax = plt.subplots(1, 1, figsize=(10, 7))\nax.plot(ripley_test_times, kstat, label=\"Empirical Ripley $\\widehat{K}(t)$\")\nax.plot(ripley_test_times, 2*ripley_test_times, label=\"Theoretical Ripley $K(t) = 2t$\")\nax.legend()\nax.set_title(\"Ripley $K$-function\")\nax.set_xlabel(\"Time $t$ (s)\")\nax.set_ylabel(\"Statistic $\\widehat{K}(t)$\")","54104162":"class LocalAverageKernel:\n    def __init__(self, bandwidth):\n        self.bandwidth = bandwidth\n    \n    def __call__(self, x):\n        normalize_cst = float(2. * self.bandwidth)\n        ks = (np.abs(x) <= self.bandwidth).astype(dtype='float')\n        return ks \/ normalize_cst\n\nclass GaussianKernel:\n    def __init__(self, bandwidth):\n        self.bandwidth = bandwidth\n    \n    def __call__(self, x):\n        x = x.astype(float)\n        bw = float(self.bandwidth)  # in nanoseconds\n        argument = - 0.5 * (x \/ bw) ** 2\n        return np.exp(argument) \/ np.sqrt(2 * np.pi * bw ** 2)","ec8cb804":"dfmt = '%Y%m%d'","b43096cb":"# Let's do a kernel estimation for 2017\nevt_times_ = times_.get_group(2017)","13d047cc":"plot_dates = pd.date_range('20170101', '20180101', freq='D')","16131eb2":"print(plot_dates.shape)\n\nprint(evt_times_[None, :].shape)","4da17b66":"np.shape(plot_dates[:, None] - evt_times_[None, :])","d4770482":"bandwidths = []\nfor bw in [7, 10, 15, 20]:\n    bandwidth = pd.Timedelta('1D') * bw\n    print(\"Bandwidth:\", bandwidth)\n    bandwidths.append(bandwidth)\n\n# Let's get a few different kernels for our bandwidths\nkernels_ = [GaussianKernel(bw.asm8) for bw in bandwidths]","1a46edf3":"dts = plot_dates[:, None] - evt_times_[None, :]","cfa22561":"num_ns_day = 86400 * 1e9  # number of nanoseconds in a day","22ec08c6":"intensity_estims_ = [num_ns_day * kern(dts).sum(axis=-1) for kern in kernels_]","da9272fc":"plt.figure(figsize=(14, 9))\nfor j, ins_estim in enumerate(intensity_estims_):\n    plt.plot(plot_dates, ins_estim, linestyle='-', label=f\"Bandwidth = {bandwidths[j]}\")\nplt.title(\"Intensity estimate for homicides, year 2017\")\nylims = plt.ylim()\nplt.vlines(evt_times_, *ylims, linestyles='--', lw=1.0, alpha=0.8)\nplt.ylim(*ylims)\nplt.legend()\nplt.xlabel(\"Date\")\nplt.ylabel(\"Intensity (1\/days)\")","1fb2350d":"## Putting data on the map\n\nWe'll use Folium for data visualisation.","b7a31dc9":"We introduce the folliwing kernels:\n* local-average kernel $K_h(x) = \\frac{1}{2h}\\mathbf{1}_{|x|\\leq h}$.\n* Gaussian kernel:\n$$\n    K_h(x) = \\frac{1}{\\sqrt{2\\pi h^2}} \\exp\\left( - \\frac{x^2}{2h^2} \\right)\n$$","87e67e9e":"We can see that, supposing the daily homicide rate over a year is homogeneous, the rates computed for each year are pretty much the same.","9e566945":"### Kernel estimation\n\nGiven a set of observed events $\\mathcal T = \\{ t_i\\}$, we can estimate the intensity function as\n$$\n    \\widehat{\\lambda}(t) = \\sum_{t_i\\in\\mathcal T} K\\left(\\frac{t-t_i}{h}\\right)\n$$\nwhere $h > 0$ is the bandwidth and $K$ is a smoothing kernel, with $\\int K(x)\\,dx = 1$.","56efccaa":"We will focus on the major **Part One** crimes: burglaries, larceny, homicides...","ddf7c592":"Let's try modelling the occurrence of crimes as a random process of time.","6ed5ba11":"# Modelling","b8accdfe":"The likelihood function of a sequence of events $\\{t_1,\\ldots,t_n\\}$ under the model $\\mathrm{PP}(\\lambda)$ is\n$$\n    p(\\{t_1, \\ldots, t_n\\}\\mid \\lambda) = \\exp(-\\lambda \\Delta t)\\lambda^n\n$$\nThe MLE of the rate $\\lambda$ is given by\n$$\n    \\hat\\lambda = \\frac{n}{\\Delta t},\n$$\ncorresponding to the intuitive notion of what a \"rate\" is: the _crime rate_ or the _homicide rate_.\n\nLet's compute the rates for years 2015-2018:","4c790fc1":"## Statistics\n\nLet's get a few statistics on the number and nature of crimes.","d5783779":"Here, we have that the empirical Ripley function is $\\hat{K}(t) < 2t$, which means the temporal point pattern is repulsive (with increased repulsiveness at long distances) and we can **reject the homogeneous Poisson hypothesis**.","aeb43df9":"We'll consider the number of homicides as a counting process $N = \\{ N_t \\}$.\n\nLet's plot it:","0b70e042":"There are several questions with such a model: over which timeframe can we consider $\\lambda$ to be constant?","77631c24":"### Testing homogeneity - Ripley's $K$-function\n\nAccording to the literature, we can test for homogeneity in a dataset by introducing Ripley's $K$-function\n\\\\[\n    K(t) = \\mathbb{E} \\left[ \\#\\{t_i \\mid |t_j-t_i| \\leq t\\} \\mid t_j \\sim \\mathcal{U}(\\mathcal{T}) \\right]\n\\\\]\nwhich measures how the process fills up an area around any event, and where $\\mathcal{T} = \\{t_i\\}$ is a realization of the process and $t_j$ is uniformly chosen in $\\mathcal{T}$ (for a stationary process the random choice of $t_j$ can be dropped and replaced by a fixed point in the observation window). It can be computed empirically as\n$$\n    \\widehat{K}(t) = \\lambda^{-1}n ^{-1}\\sum_{i\\neq j} \\mathbf 1(|t_i-t_j|\\leq t)\n$$\n\nFor an actual Poisson process, the theoretical Ripley $K$ function in one dimension is given by $K(t) = 2t$.","dceffe2e":"## Simple model: Poisson process\n\nWe start by modelling the homicides as a Poisson process: the number of homicides occurring between $t_1$ and $t_2$ is taken to follow a Poisson distribution:\n$$\n    N(t_1,t_2] \\sim \\mathcal{P}\\left(\\lambda(t_2-t_1)\\right).\n$$\nwhere $\\lambda > 0$ is the _intensity_ parameter.","6ad1bd5e":"## Nonhomogeneous Poisson process\n\nNow we suppose that\n$$\n    N(t_1, t_2] \\sim \\mathcal{P}\\left(\\int_{t_1}^{t_2} \\lambda(s)\\,ds \\right)\n$$"}}