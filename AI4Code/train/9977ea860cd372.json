{"cell_type":{"798bc6d1":"code","e208f585":"code","78606f20":"code","67ae730e":"code","f0766419":"code","c3db0022":"code","2db6a330":"code","cf0e9f58":"code","82527eec":"code","9292896c":"code","8de612e1":"code","5db40087":"code","5c3f008c":"code","a97fe69a":"code","72d066fe":"code","a919debe":"code","9267d799":"code","401b2a80":"code","eed589bc":"code","62d3d9ec":"code","da22d0c4":"code","5f2518fc":"code","be541bd1":"code","5876eb42":"code","e36e7ff9":"code","c1e9c25a":"code","b840432d":"code","969e9bde":"code","4d9b8898":"code","cb44eb3c":"code","fdb76de4":"code","14f798b2":"code","1a6ba4c8":"code","9f37b49d":"code","e92db9d1":"code","344c1133":"code","b56d0b80":"code","8661e03e":"code","fd2c8647":"code","fae6c94e":"code","84333bae":"code","06ee7774":"code","14aad908":"code","696b7355":"code","5b3ce223":"code","e8a31a99":"code","bb442885":"markdown","31dc7b11":"markdown","3afdf0a5":"markdown","464e108a":"markdown","581e3073":"markdown","0e6b01f0":"markdown","1675af96":"markdown","7297ea9a":"markdown","e89dcfad":"markdown","14b623bf":"markdown","8c666437":"markdown","4193f0af":"markdown","8ce56764":"markdown","a0b891dc":"markdown","3a01a366":"markdown","1b774ed0":"markdown","201e7331":"markdown","5bad6f9f":"markdown","d73fe7fb":"markdown","05bbface":"markdown"},"source":{"798bc6d1":"# Setting package umum \nimport pandas as pd\nimport pandas_profiling as pp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\n%matplotlib inline\n\nfrom matplotlib.pylab import rcParams\n# For every plotting cell use this\n# grid = gridspec.GridSpec(n_row,n_col)\n# ax = plt.subplot(grid[i])\n# fig, axes = plt.subplots()\nrcParams['figure.figsize'] = [10,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm import tqdm\n\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 150)\npd.options.display.float_format = '{:.4f}'.format","e208f585":"### Install packages\nfrom IPython.display import clear_output\n\n!pip install googletrans\n!pip install p_tqdm\n!pip install transformers==3.0.2\nclear_output()","78606f20":"# Load dataset\ndf_train = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ndf_test = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')","67ae730e":"### Overview dataset\ndf_train.head(11)","f0766419":"### Translate text into english\nimport multiprocessing\nfrom multiprocessing import Pool\nfrom p_tqdm import p_map\nfrom googletrans import Translator\ntranslator = Translator()\n\n### Function for paralallel translation\ndef translate_to_en(text) :\n    global translator\n    translated_text = translator.translate(text, dest = 'en' ).text\n    \n    return translated_text\n\ndef pool_translated_en(list_text) :\n    threads = 8*multiprocessing.cpu_count()\n    \n    with Pool(threads) as executor:\n        result = list(tqdm(executor.imap(translate_to_en, list_text), total=len(list_text)))\n        \n    return result\n\ndef dataset_translated_en(df) :\n    \n    ### Initialize translated dataset\n    df_trans = df.copy()\n    \n    ### Get non-english text\n    list_idx = list(df_trans[df_trans['lang_abv']!='en'].index)\n    list_prem = list(df_trans.loc[list_idx]['premise'])\n    list_hyp = list(df_trans.loc[list_idx]['hypothesis'])\n    \n    ### Translate premise\n    print('\\nPremise Translation - En')\n    df_trans.loc[list_idx,'premise'] = pool_translated_en(list_prem)\n    \n    ### Translate hypotheses\n    print('\\nPremise Hypotheses - En')\n    df_trans.loc[list_idx,'hypothesis'] = pool_translated_en(list_hyp)\n\n    ### Change languange value\n    df_trans['lang_abv'] = 'en'\n    df_trans['language'] = 'English'\n    \n    return df_trans\n    \n### Translate!\ndf_train_en = dataset_translated_en(df_train)\ndf_test_en = dataset_translated_en(df_test)","c3db0022":"### Compare translation result\nimport random\nlist_non_en_idx = list(df_train[df_train['lang_abv']!='en'].index)\n\nfor i in range(5) :\n    idx = random.randint(0,len(list_non_en_idx))\n    print('\\nORIGINAL TEXT :',df_train['premise'][list_non_en_idx[idx]])\n    print('TRANSLATED TEXT :',df_train_en['premise'][list_non_en_idx[idx]])","2db6a330":"### Proportion for each languange in train dataset\nprint('Proportion for each languange in train dataset')\ndf_train['language'].value_counts() \/ len(df_train) * 100","cf0e9f58":"### Remove unnecessary whitespaces, lower text and remove punctuation\nimport string\n\ndef remove_punctuation(text) :\n    no_punct = ''.join([c for c in text if c not in string.punctuation])\n    \n    return no_punct\n\ndef quick_clean_data(dataset, var) :\n    df = dataset.copy()\n    \n    # Lowercase\n    df[var] = df[var].str.lower()\n    \n    # Strip whitespaces\n    df[var] = df[var].str.strip()\n    \n    # Remove punctuation\n    df[var] = df.apply(lambda x : remove_punctuation(x[var]), axis=1)\n    \n    # Remove double whitespaces\n    df[var] = df.apply(lambda x : \" \".join(x[var].split()), axis=1)\n    \n    return df\n\nlist_var = ['premise','hypothesis']\nfor var in list_var :\n    df_train = quick_clean_data(df_train, var)\n    df_test = quick_clean_data(df_test, var)\n    df_train_en = quick_clean_data(df_train_en, var)\n    df_test_en = quick_clean_data(df_test_en, var)","82527eec":"### Make another dataset with stopwords removed\nfrom nltk.corpus import stopwords\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\ndef remove_stop_words(text) :\n    \n    # List of stop words\n    en_stop_words = stopwords.words('english')\n    \n    # Remove stop words \n    text = ' '.join([c for c in text.split() if c not in en_stop_words])    \n    \n    return text\n\n### Initialize dataset\ndf_train_no_stop = df_train_en.copy()\ndf_test_no_stop = df_test_en.copy()\nlist_var = ['premise','hypothesis']\n\nfor var in list_var :\n    df_train_no_stop[var] = df_train_no_stop.progress_apply(lambda x : remove_stop_words(x[var]), axis=1)\n    df_test_no_stop[var] = df_test_no_stop.progress_apply(lambda x : remove_stop_words(x[var]), axis=1)","9292896c":"### Compare stopwords result\nimport random\nlist_idx = list(df_train_no_stop.index)\n\nfor i in range(5) :\n    idx = random.randint(0,len(list_idx))\n    print('\\nORIGINAL TEXT :',df_train_en['premise'][list_idx[idx]])\n    print('NO STOPWORDS TEXT :',df_train_no_stop['premise'][list_idx[idx]])","8de612e1":"### Distribution of word count in original dataset\nrcParams['figure.figsize'] = [15,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\ngrid = gridspec.GridSpec(1,2)\n\n### Setting up data for plot\ndf_plot = df_train.copy()\ndf_plot['premise_word_count'] = df_plot.apply(lambda x : len(x['premise'].split()), axis=1)\ndf_plot['hypothesis_word_count'] = df_plot.apply(lambda x : len(x['hypothesis'].split()), axis=1)\n\n### Plotting\nlist_var = ['premise_word_count', 'hypothesis_word_count']\nlist_color = ['#db3236','#4885ed']\nfor i,var in enumerate(list_var) :\n    ax = plt.subplot(grid[i])\n    sns.distplot(df_plot[var], kde=False, ax=ax, color=list_color[i])\n\nplt.suptitle('Distribution of word count on ORIGINAL dataset') ;\nplt.tight_layout() ;\nplt.subplots_adjust(top=0.9) ;","5db40087":"### Distribution of word count in all english dataset\nrcParams['figure.figsize'] = [15,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\ngrid = gridspec.GridSpec(1,2)\n\n### Setting up data for plot\ndf_plot = df_train_en.copy()\ndf_plot['premise_word_count'] = df_plot.apply(lambda x : len(x['premise'].split()), axis=1)\ndf_plot['hypothesis_word_count'] = df_plot.apply(lambda x : len(x['hypothesis'].split()), axis=1)\n\n### Plotting\nlist_var = ['premise_word_count', 'hypothesis_word_count']\nlist_color = ['#db3236','#4885ed']\nfor i,var in enumerate(list_var) :\n    ax = plt.subplot(grid[i])\n    sns.distplot(df_plot[var], kde=False, ax=ax, color=list_color[i])\n\nplt.suptitle('Distribution of word count on ALL ENGLISH dataset') ;\nplt.tight_layout() ;\nplt.subplots_adjust(top=0.9) ;","5c3f008c":"### Distribution of word count in no stopwords dataset\nrcParams['figure.figsize'] = [15,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\ngrid = gridspec.GridSpec(1,2)\n\n### Setting up data for plot\ndf_plot = df_train_no_stop.copy()\ndf_plot['premise_word_count'] = df_plot.apply(lambda x : len(x['premise'].split()), axis=1)\ndf_plot['hypothesis_word_count'] = df_plot.apply(lambda x : len(x['hypothesis'].split()), axis=1)\n\n### Plotting\nlist_var = ['premise_word_count', 'hypothesis_word_count']\nlist_color = ['#db3236','#4885ed']\nfor i,var in enumerate(list_var) :\n    ax = plt.subplot(grid[i])\n    sns.distplot(df_plot[var], kde=False, ax=ax, color=list_color[i])\n\nplt.suptitle('Distribution of word count on NO STOPWORDS dataset') ;\nplt.tight_layout() ;\nplt.subplots_adjust(top=0.9) ;","a97fe69a":"### Proportion of target class\nrcParams['figure.figsize'] = [8,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\n\n### Function to plot donut chart\ndef make_donut_chart(sizes, labels, colors=None, explode=None) :\n  \n    # Make pie chart\n    plt.pie(sizes, colors = colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)\n\n    # Make inner circle\n    centre_circle = plt.Circle((0,0),0.70,fc='white')\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n\n    plt.axis('equal')  \n    plt.tight_layout()\n    \n# Plot preparation\nsizes = df_train['label'].value_counts() \/ len(df_train) * 100\nlabels = ['entailment','contradiction','neutral']\ncolors = ['#4285F4','#EA4335','#6a737b']\nexplode_donut = [0.05, 0.05, 0.05]\n\n# Plot\nmake_donut_chart(sizes, labels, colors, explode_donut)\nplt.title('Percentage of label class', fontsize=18, fontname='Monospace', fontweight=\"bold\") ;","72d066fe":"### Make dummy label for stratified sampling on original dataset\nLANGUAGE_MAP = {\n            \"English\"   : 0,\n            \"Chinese\"   : 1,\n            \"Arabic\"    : 2,\n            \"French\"    : 3,\n            \"Swahili\"   : 4,\n            \"Urdu\"      : 5,\n            \"Vietnamese\": 6,\n            \"Russian\"   : 7,\n            \"Hindi\"     : 8,\n            \"Greek\"     : 9,\n            \"Thai\"      : 10,\n            \"Spanish\"   : 11,\n            \"German\"    : 12,\n            \"Turkish\"   : 13,\n            \"Bulgarian\" : 14\n        }\n\ndf_train['language'] = df_train['language'].map(LANGUAGE_MAP)\ndf_train['language_label'] = df_train['language'].astype(str) + \"_\" + df_train['label'].astype(str)","a919debe":"### Initialize accelerator\nimport tensorflow as tf\n\ndef initialize_accelerator(ACCELERATOR) :\n\n    # checking TPU first\n    if ACCELERATOR == \"TPU\":\n        print(\"Connecting to TPU\")\n        try:\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n            print(f\"Running on TPU {tpu.master()}\")\n        except ValueError:\n            print(\"Could not connect to TPU\")\n            tpu = None\n\n        if tpu:\n            try:\n                print(\"Initializing TPU\")\n                tf.config.experimental_connect_to_cluster(tpu)\n                tf.tpu.experimental.initialize_tpu_system(tpu)\n                strategy = tf.distribute.experimental.TPUStrategy(tpu)\n                print(\"TPU initialized\")\n            except :\n                print(\"Failed to initialize TPU\")\n                strategy = tf.distribute.get_strategy()\n        else:\n            print(\"Unable to initialize TPU\")\n            ACCELERATOR = \"GPU\"\n\n    # default for CPU and GPU\n    if ACCELERATOR != \"TPU\":\n        print(\"Using default strategy for CPU and single GPU\")\n        strategy = tf.distribute.get_strategy()\n\n    # checking GPUs\n    if ACCELERATOR == \"GPU\":\n        print(f\"GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n\n    # defining replicas\n    AUTO = tf.data.experimental.AUTOTUNE\n    REPLICAS = strategy.num_replicas_in_sync\n    print(f\"REPLICAS: {REPLICAS}\")\n    \n    return strategy, AUTO, REPLICAS\n\nSTRATEGY, AUTO, REPLICAS =  initialize_accelerator('TPU')","9267d799":"### Function to do experiment\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow.keras.backend as K\nimport gc\n\ndef run_experiments(df, var_stratified, encode_text) :\n\n    # Stratified K-fold\n    skf = StratifiedKFold(n_splits = CV_SPLIT, shuffle = True, random_state = SEED)\n\n    # Initializing predictions\n    acc_oof = []\n\n    # Iterating over folds\n    for (fold, (train_index, valid_index)) in enumerate(skf.split(df, df[var_stratified])):\n        \n        # Initialize Accelerator\n        STRATEGY, AUTO, REPLICAS =  initialize_accelerator('TPU')\n        \n        # Building model\n        K.clear_session()\n        with STRATEGY.scope():\n            model = build_model(MODEL_NAME, MAX_LENGTH, METRICS)\n            if fold == 0:\n                print(model.summary())\n\n        print(\"\\n\")\n        print(\"#\" * 19)\n        print(f\"##### Fold: {fold + 1} #####\")\n        print(\"#\" * 19)\n\n        # Splitting data into training and validation\n        X_train = df.iloc[train_index].sample(frac=1)\n        X_valid = df.iloc[valid_index]\n        \n        from tensorflow.keras.utils import to_categorical\n        y_train = to_categorical(X_train['label'].values)\n        y_valid = to_categorical(X_valid['label'].values)\n\n        print(\"\\nTokenizing\")\n\n        # Encoding text data using tokenizer\n        X_train_encoded = encode_text(texts = X_train, tokenizer = TOKENIZER, maxlen = MAX_LENGTH, padding = PADDING)\n        X_valid_encoded = encode_text(texts = X_valid, tokenizer = TOKENIZER, maxlen = MAX_LENGTH, padding = PADDING)\n        \n        # Creating TF Dataset\n        ds_train = (\n                            tf.data.Dataset\n                            .from_tensor_slices((X_train_encoded, y_train))\n                            .repeat()\n                            .shuffle(SEED)\n                            .batch(BATCH_SIZE)\n                            .cache()\n                            .prefetch(AUTO)\n                            )\n\n        ds_valid = (\n                            tf.data.Dataset\n                            .from_tensor_slices((X_valid_encoded, y_valid))\n                            .batch(BATCH_SIZE)\n                            .cache()\n                            .prefetch(AUTO)\n                            )\n        \n        n_train = X_train.shape[0]\n\n        # Saving model at best accuracy epoch\n        sv = tf.keras.callbacks.ModelCheckpoint(\n            \"model.h5\",\n            monitor = 'val_'+METRICS[0],\n            verbose = 0,\n            save_best_only = True,\n            save_weights_only = True,\n            mode = \"max\",\n            save_freq = \"epoch\"\n        )\n\n        print(\"\\nTraining\")\n\n        # Training model\n        history = model.fit(\n            ds_train,\n            epochs = EPOCHS,\n            callbacks = [sv],\n            steps_per_epoch = n_train \/\/ BATCH_SIZE,\n            validation_data = ds_valid,\n            verbose = VERBOSE\n        )\n        \n        \n        # Validation\n        model.load_weights(\"model.h5\")\n        \n        from sklearn.metrics import accuracy_score\n        pred = model.predict(ds_valid)\n        acc = accuracy_score(X_valid['label'].values, np.argmax(pred, axis=1))\n        acc_oof.append(acc)\n\n        print(f\"\\nFold {fold + 1} Accuracy: {round(acc, 4)}\\n\")\n\n        g = gc.collect()\n\n    # overall CV score and standard deviation\n    print(f\"\\nCV Mean Accuracy: {round(np.mean(acc_oof), 4)}\")\n    print(f\"CV StdDev Accuracy: {round(np.std(acc_oof), 4)}\\n\")","401b2a80":"### Function to build model\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom transformers import TFAutoModel\n\ndef build_model(model_name, max_len, metrics):\n\n    # Defining encoded inputs\n    input_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_ids\")\n    \n    # Defining transformer model embeddings\n    transformer_model = TFAutoModel.from_pretrained(model_name)\n    transformer_embeddings = transformer_model(input_ids)[0]\n    transformer_token = transformer_embeddings[:, 0, :]\n    \n    # Defining output layer\n    output_values = Dense(3, activation = \"softmax\")(transformer_token)\n\n    # defining model\n    model = Model(inputs = input_ids, outputs = output_values)\n\n    model.compile(optimizer = Adam(learning_rate = 1e-5), \n                  loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), \n                  metrics = metrics)\n\n    return model","eed589bc":"### Make the long sentence to be predictor\ndf_train['predictor'] = \" [CLS] \" + df_train['premise'] + \" [SEP] \" + df_train['hypothesis']\ndf_test['predictor'] = \" [CLS] \" + df_test['premise'] + \" [SEP] \" + df_test['hypothesis']","62d3d9ec":"### Experiment configuration\n# Note that all this parameter are being used in the run_experiments function\n# Think of this as a global parameter for the function (cause I'm lazy to code it as function parameter)\nMAX_LENGTH = 75\nMODEL_NAME = \"distilbert-base-multilingual-cased\"\nPADDING = True\nBATCH_SIZE = 16 * REPLICAS\nEPOCHS = 10\nCV_SPLIT = 5\nSEED = 2020\nVERBOSE = 1\nMETRICS = [\"categorical_accuracy\"]","da22d0c4":"### Load tokenizer\nimport transformers\nfrom transformers import AutoTokenizer\n\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)","5f2518fc":"### Function to encode predictor\ndef original_encode(texts, tokenizer, maxlen, padding):\n    \n    enc_di = tokenizer.batch_encode_plus(\n             texts['predictor'], \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=padding,\n             max_length=maxlen)\n    \n    return enc_di[\"input_ids\"]","be541bd1":"### Start experiment\nimport time\nstart = time.time()\n\nrun_experiments(df_train, 'language_label', original_encode)\n\nend = time.time()\nprint('Time used :',(end-start)\/60)","5876eb42":"### Make the long sentence to be predictor\ndf_train_en['predictor'] = \" [CLS] \" + df_train_en['premise'] + \" [SEP] \" + df_train_en['hypothesis']\ndf_test_en['predictor'] = \" [CLS] \" + df_test_en['premise'] + \" [SEP] \" + df_test_en['hypothesis']","e36e7ff9":"### Experiment configuration\n# Note that all this parameter are being used in the run_experiments function\n# Think of this as a global parameter for the function (cause I'm lazy to code it as function parameter)\nMAX_LENGTH = 75\nMODEL_NAME = \"distilbert-base-multilingual-cased\"\nPADDING = True\nBATCH_SIZE = 16 * REPLICAS\nEPOCHS = 10\nCV_SPLIT = 5\nSEED = 2020\nVERBOSE = 1\nMETRICS = [\"categorical_accuracy\"]","c1e9c25a":"### Load tokenizer\nimport transformers\nfrom transformers import AutoTokenizer\n\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)","b840432d":"### Function to encode predictor\ndef original_encode(texts, tokenizer, maxlen, padding):\n    \n    enc_di = tokenizer.batch_encode_plus(\n             texts['predictor'], \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=padding,\n             max_length=maxlen)\n    \n    return enc_di[\"input_ids\"]","969e9bde":"### Start experiment\nimport time\nstart = time.time()\n\nrun_experiments(df_train_en, 'label', original_encode)\n\nend = time.time()\nprint('Time used :',(end-start)\/60)","4d9b8898":"### Make the long sentence to be predictor\ndf_train_no_stop['predictor'] = \" [CLS] \" + df_train_no_stop['premise'] + \" [SEP] \" + df_train_no_stop['hypothesis']\ndf_test_no_stop['predictor'] = \" [CLS] \" + df_test_no_stop['premise'] + \" [SEP] \" + df_test_no_stop['hypothesis']","cb44eb3c":"### Experiment configuration\n# Note that all this parameter are being used in the run_experiments function\n# Think of this as a global parameter for the function (cause I'm lazy to code it as function parameter)\nMAX_LENGTH = 50\nMODEL_NAME = \"distilbert-base-multilingual-cased\"\nPADDING = True\nBATCH_SIZE = 16 * REPLICAS\nEPOCHS = 10\nCV_SPLIT = 5\nSEED = 2020\nVERBOSE = 1\nMETRICS = [\"categorical_accuracy\"]","fdb76de4":"### Function to encode predictor\ndef original_encode(texts, tokenizer, maxlen, padding):\n    \n    enc_di = tokenizer.batch_encode_plus(\n             texts['predictor'], \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=padding,\n             max_length=maxlen)\n    \n    return enc_di[\"input_ids\"]","14f798b2":"### Start experiment\nimport time\nstart = time.time()\n\nrun_experiments(df_train_no_stop, 'label', original_encode)\n\nend = time.time()\nprint('Time used :',(end-start)\/60)","1a6ba4c8":"### Function to build model\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom transformers import TFAutoModel\n\ndef build_model_multimodal(model_name, max_len, metrics):\n\n    # Defining encoded inputs\n    input_ids_prem = Input(shape = (MAX_LENGTH_PREM,), dtype = tf.int32, name = \"input_ids_prem\")\n    input_ids_hyp = Input(shape = (MAX_LENGTH_HYP,), dtype = tf.int32, name = \"input_ids_hyp\")\n    \n    # Defining transformer model embeddings\n    transformer_model_prem = TFAutoModel.from_pretrained(model_name)\n    transformer_embeddings_prem = transformer_model_prem(input_ids_prem)[0]\n    transformer_token_prem = transformer_embeddings_prem[:, 0, :]\n    \n    transformer_model_hyp = TFAutoModel.from_pretrained(model_name)\n    transformer_embeddings_hyp = transformer_model_hyp(input_ids_hyp)[0]\n    transformer_token_hyp = transformer_embeddings_hyp[:, 0, :]\n    \n    # Concat 2 token\n    transformer_concat = Concatenate()([transformer_token_prem, transformer_token_hyp])\n    \n    # Defining output layer\n    output_values = Dense(3, activation = \"softmax\")(transformer_concat)\n\n    # defining model\n    model = Model(inputs = [input_ids_prem, input_ids_hyp], outputs = output_values)\n\n    model.compile(optimizer = Adam(learning_rate = 1e-5), \n                  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.2 ), \n                  metrics = metrics)\n\n    return model","9f37b49d":"### Function to encode predictor\ndef original_encode_multimodal(texts, tokenizer, maxlen, padding):\n    \n    enc_di = tokenizer.batch_encode_plus(\n             texts, \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=padding,\n             max_length=maxlen)\n    \n    return enc_di[\"input_ids\"]","e92db9d1":"### Function to do experiment multimodal\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow.keras.backend as K\nimport gc\n\ndef run_experiments_multimodal(df, var_stratified, encode_text) :\n\n    # Stratified K-fold\n    skf = StratifiedKFold(n_splits = CV_SPLIT, shuffle = True, random_state = SEED)\n\n    # Initializing predictions\n    acc_oof = []\n\n    # Iterating over folds\n    for (fold, (train_index, valid_index)) in enumerate(skf.split(df, df[var_stratified])):\n        \n        # Initialize Accelerator\n        STRATEGY, AUTO, REPLICAS =  initialize_accelerator('TPU')\n        \n        # Building model\n        K.clear_session()\n        with STRATEGY.scope():\n            model = build_model_multimodal(MODEL_NAME, MAX_LENGTH_PREM, METRICS)\n            if fold == 0:\n                print(model.summary())\n\n        print(\"\\n\")\n        print(\"#\" * 19)\n        print(f\"##### Fold: {fold + 1} #####\")\n        print(\"#\" * 19)\n\n        # Splitting data into training and validation\n        X_train = df.iloc[train_index]\n        X_valid = df.iloc[valid_index]\n\n        from tensorflow.keras.utils import to_categorical\n        y_train = to_categorical(X_train['label'].values)\n        y_valid = to_categorical(X_valid['label'].values)\n\n        print(\"\\nTokenizing\")\n\n        # Encoding text data using tokenizer\n        X_train_encoded_prem = encode_text(texts = X_train['premise'], tokenizer = TOKENIZER, maxlen = MAX_LENGTH_PREM, padding = PADDING)\n        X_train_encoded_hyp = encode_text(texts = X_train['hypothesis'], tokenizer = TOKENIZER, maxlen = MAX_LENGTH_HYP, padding = PADDING)\n        X_valid_encoded_prem = encode_text(texts = X_valid['premise'], tokenizer = TOKENIZER, maxlen = MAX_LENGTH_PREM, padding = PADDING)\n        X_valid_encoded_hyp = encode_text(texts = X_valid['hypothesis'], tokenizer = TOKENIZER, maxlen = MAX_LENGTH_HYP, padding = PADDING)\n        \n        # Creating TF Dataset\n        ds_train = (\n                            tf.data.Dataset\n                            .from_tensor_slices(({\"input_ids_prem\": X_train_encoded_prem, \"input_ids_hyp\": X_train_encoded_hyp}, y_train))\n                            .repeat()\n                            .shuffle(SEED)\n                            .batch(BATCH_SIZE)\n                            .cache()\n                            .prefetch(AUTO)\n                            )\n\n        ds_valid = (\n                            tf.data.Dataset\n                            .from_tensor_slices(({\"input_ids_prem\": X_valid_encoded_prem, \"input_ids_hyp\": X_valid_encoded_hyp}, y_valid))\n                            .batch(BATCH_SIZE)\n                            .cache()\n                            .prefetch(AUTO)\n                            )\n        \n        n_train = X_train.shape[0]\n\n        # Saving model at best accuracy epoch\n        sv = tf.keras.callbacks.ModelCheckpoint(\n            \"model.h5\",\n            monitor = 'val_'+METRICS[0],\n            verbose = 0,\n            save_best_only = True,\n            save_weights_only = True,\n            mode = \"max\",\n            save_freq = \"epoch\"\n        )\n\n        print(\"\\nTraining\")\n\n        # Training model\n        history = model.fit(\n            ds_train,\n            epochs = EPOCHS,\n            callbacks = [sv],\n            steps_per_epoch = n_train \/\/ BATCH_SIZE,\n            validation_data = ds_valid,\n            verbose = VERBOSE\n        )\n        \n        # Validation\n        model.load_weights(\"model.h5\")\n        \n        from sklearn.metrics import accuracy_score\n        pred = model.predict(ds_valid)\n        acc = accuracy_score(X_valid['label'].values, np.argmax(pred, axis=1))\n        acc_oof.append(acc)\n\n        print(f\"\\nFold {fold + 1} Accuracy: {round(acc, 4)}\\n\")\n\n\n        g = gc.collect()\n\n    # overall CV score and standard deviation\n    print(f\"\\nCV Mean Accuracy: {round(np.mean(acc_oof), 4)}\")\n    print(f\"CV StdDev Accuracy: {round(np.std(acc_oof), 4)}\\n\")","344c1133":"### Experiment configuration\n# Note that all this parameter are being used in the run_experiments function\n# Think of this as a global parameter for the function (cause I'm lazy to code it as function parameter)\nMAX_LENGTH_PREM = 50\nMAX_LENGTH_HYP = 25\n\n# You can use different pre trained model for premise and hypothesis\n# In this notebook I use XLM-RoBERTa for both\nMODEL_NAME = \"distilbert-base-multilingual-cased\"\nPADDING = True\nBATCH_SIZE = 16 * REPLICAS\nEPOCHS = 10\nCV_SPLIT = 5\nSEED = 2020\nVERBOSE = 1\nMETRICS = [\"categorical_accuracy\"]","b56d0b80":"### Load tokenizer\nimport transformers\nfrom transformers import AutoTokenizer\n\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)","8661e03e":"### Start experiment\nimport time\nstart = time.time()\n\nrun_experiments_multimodal(df_train_en, 'label', original_encode_multimodal)\n\nend = time.time()\nprint('Time used :',(end-start)\/60)","fd2c8647":"### Experiment configuration\n# Note that all this parameter are being used in the run_experiments function\n# Think of this as a global parameter for the function (cause I'm lazy to code it as function parameter)\nMAX_LENGTH = 75\nMODEL_NAME = \"jplu\/tf-xlm-roberta-large\"\nPADDING = True\nBATCH_SIZE = 16 * REPLICAS\nEPOCHS = 10\nCV_SPLIT = 5\nSEED = 2020\nVERBOSE = 1\nMETRICS = [\"categorical_accuracy\"]","fae6c94e":"### Load tokenizer\nimport transformers\nfrom transformers import AutoTokenizer\n\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)","84333bae":"### Encode the dataset\nfrom tensorflow.keras.utils import to_categorical\ny_train = to_categorical(df_train_en['label'].values)\n\n# Encoding text data using tokenizer\nX_train_encoded = original_encode(texts = df_train_en, tokenizer = TOKENIZER, maxlen = MAX_LENGTH, padding = PADDING)\nX_test_encoded = original_encode(texts = df_test_en, tokenizer = TOKENIZER, maxlen = MAX_LENGTH, padding = PADDING)","06ee7774":"### Make TF dataset\nds_train = (\n                    tf.data.Dataset\n                    .from_tensor_slices((X_train_encoded, y_train))\n                    .repeat()\n                    .shuffle(SEED)\n                    .batch(BATCH_SIZE)\n                    .cache()\n                    .prefetch(AUTO)\n                    )\n\nds_test = (\n                    tf.data.Dataset\n                    .from_tensor_slices((X_test_encoded))\n                    .batch(BATCH_SIZE)\n                    .prefetch(AUTO)\n                    )","14aad908":"# Building model\nK.clear_session()\nwith STRATEGY.scope():\n    model = build_model(MODEL_NAME, MAX_LENGTH, METRICS)","696b7355":"### Train model\nn_train = df_train_en.shape[0]\n\nhistory = model.fit(\n    ds_train,\n    epochs = EPOCHS,\n    steps_per_epoch = n_train \/\/ BATCH_SIZE,\n    verbose = VERBOSE\n)","5b3ce223":"### Make prediction\npred = model.predict(ds_test)","e8a31a99":"### Make submission\nsub = pd.read_csv('..\/input\/contradictory-my-dear-watson\/sample_submission.csv')\nsub['prediction'] = np.argmax(pred, axis=1)\n\nsub.to_csv('submission.csv', index=False)","bb442885":"After removing the stopwords the premise now max out at 35 and hypothesis max out at 15. So there are around 10-15 word difference with the original dataset. For this dataset we will use MAX LENGTH of 50","31dc7b11":"Note that :\n- This config have better performance than the first config. With this config we can get the CV Acc of 60% with standard error of 0.2%, with a train acc of 82-85% (the same as the first config). \n- So this result conclude that Googletrans translation have done a pretty good job, translating without losing the context of the sentence. But still this config still overfit the training dataset","3afdf0a5":"# 2. Using all-Englisth translated data\nThe translation are made by using Google Translate package. Basically translate all the premise and hypothesis into English. In this method we still combine the premise and hypothesis into one sentence","464e108a":"# 1. Combine the hypotheses and premise into one sentence\nSo in this one we are gonna combine both premise and hypothesis into one sentence separated with [SEP]\n\nFor example :\n- **Premise** : I love food\n- **Hypothesis** : I love fried rice\n- **Final sentence** : I love food [SEP] I love fried rice","581e3073":"## Feel free to ask below and give an upvote if this notebook helps you","0e6b01f0":"So to conclude all the information above :\n- We will use MAX LENGTH of 75 for original dataset, 75 also for all-english dataset, and 50 for no-punctuation dataset\n- The proportion of the target variable is pretty equal\n- The languange is mostly english with the other languange consist of only 2-3% each","1675af96":"The distribution is pretty similar to the ORIGINAL dataset. So will use the same MAX LENGTH","7297ea9a":"# 3. Removing stopwords and punctuations\nOn the english translated dataset. Hypothetically this method will lose some context in the sentence but hey no pain to try","e89dcfad":"# Conclusion\nAll configuration give a stable performance based on the small standard error accuracy but the model trained using all-englisht text give the best CV Acc, but it sill have the overfitting issue\n\nSo for the next step :\n- Use translation and stop words as text augmentation. This is supported by the performance inconsistency of the affiliated configuration.\n- Benchmark other pre-trained model. JohnM have give us a list of possible model than can be used (https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/discussion\/171587)\n\nP.S : \n- I have try using different pre-trained model for benchmark. This include the RoBERTa model that have been trained using MNLI dataset which have a leakage to the test dataset. If you want to seek a high lb score you can try using this model, but I don't recommend it because it is a false result.\n- XLM RoBERTa give the most stable performance that can achieve CV Acc of 77-78%. I am not using it here because the model have many parameter (around 500k)","14b623bf":"Note that :\n<br> For this configuration we manage to get CV Acc of 52% with standard error 0.9% so yeah this configuration isn't suitable in this case. You can see more discussion in this link (https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/discussion\/172105). **But maybe we can use this stopwords for data augmentation (random stopwords deletion)**","8c666437":"# Setting up accelerator and function for modelling","4193f0af":"# Overview of DistilBERT\nThis is a quotation from the DistilBERT paper. DistilBERT is a **small, fast, cheap and light** Transformer model based on the BERT architecture. There are many ways to make the model smaller and one of is **distillation**. \n\nIn a nutshell, distillation mean we build a small model that can reproduce the behavior the large model (in DistilBERT the large model is BERT). This can be done by using the soft label produce by BERT model as the target variable for the small model. Basically its kinda the same as label smoothing. This process called **teacher-student learning**\n\nIn order to learn more about DistilBERT first we have to learn about BERT. There is a 13 min video in Youtube that explained it well (here is the link https:\/\/www.youtube.com\/watch?v=OR0wfP2FD3c), go check it out. Basically DistilBERT is a smaller version of BERT with a modification\n- Remove token type embedding and pooler\n- Only use half of the layer on BERT\n\nAlthough it remove many part of BERT, DistilBERT can still retain slightly worse performance than BERT with 60% faster than BERT in training and inferencing. This is exactly why I choose to use DistilBERT since this notebook goal is to do benchmarking\n\n![](https:\/\/miro.medium.com\/max\/1000\/1*hHwcSZEazpY_PwArgBzHtw.png)","8ce56764":"# Submission\nBased on the benchmark result, it safe to say that all-english dataset give the best performance so I will use it to make submission. In here I will use XLM-RoBERTa since it is a bigger model that have much more parameter than DistilBERT","a0b891dc":"# 4. Use hypotheses and premise as separate input (so the model will have 2 input)\nUse 2 pre-trained model for each input. For both input I will still use DistilBERT but feel free to change differ the model. It will make the training time longer but maybe this configuration can improve the performance of the model since each model focus on each input now","3a01a366":"Looks like the translator done a pretty good job. We know that the train dataset heavily consist of english text (56%). In the future I will try to translate the english word into other languange so the proportion can be more uniform","1b774ed0":"Fairly uniform proportion of label class so it is not an imbalanced multiclassification case","201e7331":"# So how to configure our input, Watson ?\nHello there, in this notebook I want to benchmark the performance of different input configuration using **DistilBERT** pre-trained model available in HuggingFace. DistilBERT is the modification of the original BERT model published by Google to focus on reducing the trainig time while minimizing the performance reduction, so it is good to use for benchmarking different configuration using Fold split\n\nHere is the list of input configuration that I will experiment on\n1. Combine the hypotheses and premise into one sentence\n2. Using all-Englisth translated data\n3. Removing stopwords and punctuations\n4. Use hypotheses and premise as separate input (so the model will have 2 input)\n\nAll configuration performance will be based 10 epochs training on 5-StratifiedFold train dataset. To do this I will use the function in this notebook with a slight modification (go and upvote this amazing notebook https:\/\/www.kaggle.com\/rohanrao\/tpu-sherlocked-one-stop-for-with-tf). First lets prep the data","5bad6f9f":"By using 1 model for each input we manage to get CV Acc of 52% with standard error 0.3%. This result is produced by using all-english dataset. Do note that the all-english single model can get CV Acc of 60% but with standard error of 1%, so this configuration also not suitable for this case","d73fe7fb":"Note that :\n- Using this configuration we manage to get CV Acc of 56% with standard error of 0.1%, so there are no significance variance with this configuration\n- The model give a sign of overfitting, we can conclude this by looking at the train accuracy for each fold. The train dataset can achieve accuracy of 82-85%. This is normal in deep learning model so the next step is to do **text augmentation** for the data","05bbface":"For the original dataset we can see that premise word count kinda max out at 50 and hypothesis word count max out at 25. So it safe to use 75 as the MAX_LENGTH for modelling"}}