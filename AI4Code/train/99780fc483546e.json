{"cell_type":{"502b9bed":"code","15b7cb8e":"code","95da1a3d":"code","b901a835":"code","540d0ed3":"code","3dc24696":"code","793c7bf4":"code","546ec7d3":"code","8808aee8":"code","8653e84a":"code","ead9e5d0":"code","e9d0fa13":"code","3273aa16":"code","5cd1aabd":"code","25b4846b":"code","bf77d493":"code","f1848303":"code","0d02545f":"code","956d5dec":"code","41f37a4b":"code","391b9138":"code","a5449b5f":"code","c006f8b2":"code","c4fd315f":"code","6097a7be":"code","6cbcac1f":"code","9f4b5445":"code","f9cbfea9":"code","dfa4c3ef":"code","2b997705":"code","7ebd89da":"code","e7adfcb8":"code","71f293a4":"code","c916432a":"code","da22119c":"code","1149f262":"code","1a6bce40":"code","50aac895":"code","2edb240a":"code","2a09c7b5":"code","86c48a1a":"code","6090e6e1":"code","225c0cb2":"code","6bbce27b":"code","d525dd2a":"code","a5332875":"code","a557e28c":"code","630a4d08":"code","68e78750":"code","cda6e533":"code","ac547196":"code","3fb18e63":"code","363067f9":"markdown","bf54a165":"markdown","4dfcaa24":"markdown","13377cdf":"markdown","55100b04":"markdown","1264f7d6":"markdown","802ea542":"markdown","5ab86ace":"markdown","57a34ed4":"markdown","f8b2e555":"markdown","f8230965":"markdown","8af98737":"markdown","0f9fcc3c":"markdown","d1c29107":"markdown","890a3a50":"markdown","3bf164a5":"markdown","c5cbdfa5":"markdown","e6a763a5":"markdown","058325ef":"markdown","d9050ea5":"markdown","80aede63":"markdown","ac163fc1":"markdown","e43fa80c":"markdown","8ec1b9a3":"markdown","ba56b3b5":"markdown","d5ad99fa":"markdown","9b89b035":"markdown","a09eda06":"markdown","83edd0ca":"markdown","fc3dfa7e":"markdown","5e8756e1":"markdown","dd7abe49":"markdown","887d058e":"markdown","04394206":"markdown","32885c97":"markdown"},"source":{"502b9bed":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\nimport seaborn as sns\nimport time\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nstoplist= stopwords.words('english')\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer= WordNetLemmatizer()\nfrom nltk.corpus import wordnet\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable logging\nimport logging\nlogging.basicConfig(level= logging.INFO)\n\n# You will find more libraries as they come in use!","15b7cb8e":"df= pd.read_csv(\"\/kaggle\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\", index_col=0)\ndf.columns= df.columns.str.replace(\" \", \"_\")\ndf.head()","95da1a3d":"# Relation between Division name, Department name and Class name? \ndf[['Division_Name','Department_Name','Class_Name']].groupby(['Division_Name','Department_Name','Class_Name']).agg('count')","b901a835":"# Since the reviews is our main content, dropping rows where 'Review Text' is null\ndf.dropna(subset=['Review_Text'], inplace=True)\ndf.shape","540d0ed3":"# Review word count\ndf['rev_word_count']= df['Review_Text'].apply(lambda x: len(x.strip().split()))\n\n# Unique word count\ndf['unique_word_count']= df['Review_Text'].apply(lambda x: len(set(str(x).split())))","3dc24696":"# Bucketing Clothing ID's with 1 or 2 count\n\nclothing_id_to_combine=[]\nfor val, cnt in df.Clothing_ID.value_counts().iteritems():\n    # If that Clothing_ID is present less than 1%(~200) of the total data, club it into '000' (default) id\n    if(cnt<200):         \n        clothing_id_to_combine.append(val)        \n\nprint(\"# of clothing ID's clubbed: \",len(clothing_id_to_combine))\n\ndf['new_clothingID']= df.Clothing_ID.apply(lambda x: '000' if x in clothing_id_to_combine else x)\ndf.new_clothingID.value_counts(normalize=True)","793c7bf4":"pip install vaderSentiment","546ec7d3":"import vaderSentiment\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyzer= SentimentIntensityAnalyzer()\n\n# Try it out!\nvs= analyzer.polarity_scores(\"Vader sentiment looks interesting, I have high hopes!\")\nprint(vs)","8808aee8":"df['review_sentiment']= [analyzer.polarity_scores(line)['compound'] for line in df['Review_Text']]","8653e84a":"# Title provides extra insight of the sentiment of the customer while writing the review. Hence we will also obtain Title sentiment alongwith review sentiment score.\n# However 13% reviews don't have a Title. Hence filling in the null values with 'no title' and assigning it 0(neutral) sentiment\n\ndf.Title.fillna('no title', inplace=True)\ndf['title_sentiment']= df['Title'].apply(lambda x: analyzer.polarity_scores(x)['compound'] if str(x)!= 'no title' else 0.0)\n\nfor index, row in df[100:120].iterrows():\n    print(row['Title'],\" >>>>>\", row['title_sentiment'])","ead9e5d0":"df['total_sentiment_score']= df['title_sentiment']+ df['review_sentiment']","e9d0fa13":"# Golden rule: Save up the original dataframe before encoding!\ndf_orig= df.copy()\ndf_orig.shape \n\n# df= df_orig.copy()","3273aa16":"# Dropping text columns- we have already used them to calcuate the total sentiment score\n\ndf.drop(columns=['Review_Text','Title','Clothing_ID','review_sentiment','title_sentiment'], inplace=True)","5cd1aabd":"# Deciding on categorical columns- object datatype -very slow processing\n\ncat_cols= ['Division_Name','Department_Name','Class_Name','new_clothingID']\nfor col in cat_cols:\n    print(col,\" has categories:\", df[col].nunique())\n    df[col]= df[col].astype('category')","25b4846b":"# Binary encoding our categorical columns\n\nimport category_encoders as ce\n\nbe= ce.BinaryEncoder(cols= cat_cols,drop_invariant=True).fit(df) \n\ndf= be.transform(df)","bf77d493":"from sklearn.pipeline import Pipeline \nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom scipy import stats\nfrom sklearn.cluster import KMeans\nimport pylab as pl\n%matplotlib inline\nimport matplotlib.pyplot as plt","f1848303":"pca_tsne= Pipeline([(\"pca\", PCA(n_components= 0.90, random_state=33)),\n                    (\"tsne\", TSNE(n_components=2,\n                                  perplexity= 170,\n                                  random_state=33, \n                                  learning_rate= 350, \n                                  n_iter= 5000,\n                                  n_jobs=-1,\n                                  n_iter_without_progress=150,\n                                  verbose=1))])\nt0= time.time()\ndf_pca_tsne_reduced= pca_tsne.fit_transform(df)\nt1= time.time()\n\nprint(\"pca+tsne took:{:.1f}s \".format(t1-t0))","0d02545f":"sns.set(rc= {'figure.figsize': (13,13)})\nsns.scatterplot(df_pca_tsne_reduced[:,0], df_pca_tsne_reduced[:,1])\nplt.show()","956d5dec":"review_data_std = stats.zscore(df_pca_tsne_reduced)\nreview_data_std = np.array(review_data_std)\n\nsns.set(rc= {'figure.figsize': (7,7)})\nnumber_of_clusters = range(1,20)\n\nt0= time.time()\nkmeans = [KMeans(n_clusters=i,max_iter=1000,random_state=33,n_jobs=-1) for i in number_of_clusters]\nscore = [-1*kmeans[i].fit(df_pca_tsne_reduced).score(df_pca_tsne_reduced) for i in range(len(kmeans))]\nt1= time.time()\n\npl.plot((number_of_clusters),score)\npl.xlabel('Number of Clusters')\npl.ylabel('Score')\npl.title('Elbow Curve')\npl.show()\n\nprint(\"Plotting the Elbow curve took:{:.1f}s \".format(t1-t0))","41f37a4b":"k_means_test = KMeans(n_clusters=3, max_iter=1500, random_state=33,verbose=1, n_jobs=-1)\n\n#fitting on your model\n-1*k_means_test.fit(df_pca_tsne_reduced).score(df_pca_tsne_reduced)\ny_pred= k_means_test.labels_\n\n# Assigning cluster labels to each data point\ndf_orig['klabels'] = k_means_test.labels_","391b9138":"# Analyzing \nsize_of_each_cluster= df_orig.groupby('klabels').size().reset_index()\nsize_of_each_cluster.columns = ['klabels','number_of_points']\nsize_of_each_cluster['percentage'] = (size_of_each_cluster['number_of_points']\/np.sum(size_of_each_cluster['number_of_points']))*100\n\nprint(size_of_each_cluster)","a5449b5f":"palette = sns.hls_palette(3, l=.4, s=.9)\n\nsns.set(rc= {'figure.figsize': (13,13)})\nsns.scatterplot(df_pca_tsne_reduced[:,0], df_pca_tsne_reduced[:,1], hue= y_pred, legend='full', palette=palette)\nplt.title(\"t-sne with KMeans labels\")\nplt.show()","c006f8b2":"def get_pos_tag(tag):\n    \"\"\"This function is used to get the part-of-speech(POS) for lemmatization\"\"\"\n    \n    if tag.startswith('N') or tag.startswith('J'):\n        return wordnet.NOUN\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN #default case","c4fd315f":"import re\ndef preprocess(text):\n    \"\"\" 1. Removes Punctuations\n        2. Removes words smaller than 3 letters\n        3. Converts into lowercase\n        4. Lemmatizes words\n        5. Removes Stopwords\n    \"\"\"   \n    punctuation= list(string.punctuation)\n    doc_tokens= nltk.word_tokenize(text)\n    word_tokens= [word.lower() for word in doc_tokens if not (word in punctuation or len(word)<=3)]\n    \n    # Lemmatize    \n    pos_tags=nltk.pos_tag(word_tokens)\n#     print(pos_tags)\n    doc_words=[wordnet_lemmatizer.lemmatize(word, pos=get_pos_tag(tag)) for word, tag in pos_tags]\n    doc_words= [word for word in doc_words if word not in stoplist]\n    \n    return doc_words\n\ndf_clean = df_orig['Review_Text'].apply(preprocess)\ndf_clean.head()","6097a7be":"# Adding business stopwords to exclude\n\ncommon_terms= [\"wear\",\"look\",\"ordered\",\"color\",\"purchase\",\"order\"]\n\nstoplist= stoplist+ common_terms","6cbcac1f":"# Tried multiple parts of speech and obtained best topic results using Nouns and Adjectives!\ndef get_nouns_adjs(series):\n    \n    \" Topic Modeling using only nouns and adjectives\"\n    \n    pos_tags= nltk.pos_tag(series)\n    all_adj_nouns= [word for (word, tag) in pos_tags if (tag==\"NN\" or tag==\"NNS\" or tag==\"JJ\")] \n    return all_adj_nouns\n\ndf_nouns_adj = df_clean.apply(get_nouns_adjs)","9f4b5445":"# Importing gensim related libraries\nimport gensim\nfrom gensim.models.ldamulticore import LdaMulticore\nfrom gensim.corpora import Dictionary\nfrom gensim.models import Phrases\nfrom collections import Counter\nfrom gensim.models import Word2Vec","f9cbfea9":"docs= list(df_nouns_adj)\nphrases = gensim.models.Phrases(docs, min_count=10, threshold=20)\nbigram_model = gensim.models.phrases.Phraser(phrases)","dfa4c3ef":"def make_bigrams(texts):\n    return [bigram_model[doc] for doc in texts]\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(docs)","2b997705":"# Checkout most frequent bigrams :\nbigram_counter1= Counter()\nfor key in phrases.vocab.keys():\n    if key not in stopwords.words('english'):\n        if len(str(key).split('_'))>1:\n            bigram_counter1[key]+=phrases.vocab[key]\n\nfor key, counts in bigram_counter1.most_common(20):\n    print(key,\">>>>\", counts)","7ebd89da":"w2vmodel = Word2Vec(bigram_model[docs], size=100, sg=1, hs= 1, seed=33, iter=35)\nbigram_counter = Counter()\n\nfor key in w2vmodel.wv.vocab.keys():\n    if key not in stoplist:\n        if len(str(key).split(\"_\")) > 1:\n            bigram_counter[key] += w2vmodel.wv.vocab[key].count\n\nfor key, counts in bigram_counter.most_common(30):\n    print(key,\">>>>> \" ,counts)","e7adfcb8":"# MostOften mentioned along with the word 'pregnant'\nw2vmodel.most_similar(positive= ['pregnant'])","71f293a4":"# Which color is to 'work' as 'white' is to 'wedding'\nw2vmodel.wv.most_similar(['work','white'], ['wedding'], topn=5)","c916432a":"w2vmodel.wv.most_similar(['price','steal'], ['discount'], topn=5)","da22119c":"# What is a 'deal_breaker', if 'quality'is 'worth_penny' \nw2vmodel.wv.most_similar(positive=[\"deal_breaker\",\"quality\"], negative=[\"worth_penny\"], topn=3)","1149f262":"dictionary= Dictionary(data_words_bigrams)\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents.\ndictionary.filter_extremes(no_below=20, no_above=0.6)\ncorpus = [dictionary.doc2bow(doc) for doc in docs]\n\nprint('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","1a6bce40":"from gensim.models.ldamulticore import LdaMulticore\n\nt0= time.time()\npasses= 150\nnp.random.seed(1) # setting up random seed to get the same results\nldamodel= LdaMulticore(corpus, \n                    id2word=dictionary, \n                    num_topics=4, \n#                   alpha='asymmetric', \n                    chunksize= 4000, \n                    batch= True,\n                    minimum_probability=0.001,\n                    iterations=350,\n                    passes=passes)                    \n\nt1= time.time()\nprint(\"time for\",passes,\" passes: \",(t1-t0),\" seconds\")","50aac895":"ldamodel.show_topics(num_words=25, formatted=False)","2edb240a":"lda_corpus= ldamodel[corpus]","2a09c7b5":"# Obtaining the main topic for each review:\n\nall_topics = ldamodel.get_document_topics(corpus)\nnum_docs = len(all_topics)\n\nall_topics_csr= gensim.matutils.corpus2csc(all_topics)\nall_topics_numpy= all_topics_csr.T.toarray()\n\nmajor_topic= [np.argmax(arr) for arr in all_topics_numpy]\ndf_orig['major_lda_topic']= major_topic","86c48a1a":"sns.set(rc= {'figure.figsize': (5,3)})\nsns.set_style('darkgrid')\n\ndf_orig.major_lda_topic.value_counts().plot(kind='bar')","6090e6e1":"df_orig.groupby(['klabels'])['major_lda_topic'].value_counts(ascending=False, normalize=True)","225c0cb2":"num_cols= ['Age','Positive_Feedback_Count','rev_word_count', 'unique_word_count','total_sentiment_score']\n\ncat_cols= ['major_lda_topic','Division_Name','Department_Name','Class_Name']\n\ncluster1= df_orig.loc[(df_orig.klabels==0)]\ncluster2= df_orig.loc[(df_orig.klabels==1)]\ncluster3= df_orig.loc[(df_orig.klabels==2)]","6bbce27b":"pd.DataFrame((cluster1.Rating.value_counts()*100)\/df_orig.Rating.value_counts()).plot(kind='bar')","d525dd2a":"print('Visualizing numerical features:')\nfor i, col in enumerate(num_cols):\n    plt.figure(i)\n    sns.distplot(cluster1[col])\n","a5332875":"print('Visualizing categorical features:')\nfor i, col in enumerate(cat_cols):\n    plt.figure(i)\n    chart= sns.countplot(cluster1[col], order= cluster1[col].value_counts().index)\n    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)","a557e28c":"# **Cluster 2 Analysis**\nprint('Visualizing numerical features:')\nfor i, col in enumerate(num_cols):\n    plt.figure(i)\n    sns.distplot(cluster2[col])","630a4d08":"pd.DataFrame((cluster2.Rating.value_counts()*100)\/df_orig.Rating.value_counts()).plot(kind='bar')","68e78750":"print('Visualizing categorical features:')\nfor i, col in enumerate(cat_cols):\n    plt.figure(i)\n    chart= sns.countplot(cluster2[col], order= cluster2[col].value_counts().index)\n    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)","cda6e533":"# Cluster 3 Analysis\npd.DataFrame((cluster3.Rating.value_counts()*100)\/df_orig.Rating.value_counts()).plot(kind='bar')","ac547196":"print('Visualizing numerical features:')\nfor i, col in enumerate(num_cols):\n    plt.figure(i)\n    sns.distplot(cluster3[col])\n","3fb18e63":"print('Visualizing categorical features:')\nfor i, col in enumerate(cat_cols):\n    plt.figure(i)\n    chart= sns.countplot(cluster3[col], order= cluster3[col].value_counts().index)\n    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)","363067f9":"STEP 1: Preprocessing text - Tokenizing sentences, stopwords removal and lemmatization","bf54a165":"### 1. Importing data and libraries","4dfcaa24":"<!-- **Well, what are the Topics saying??**\n\n**TOPIC 0**- *Top wear* <br>\n(Items) Top, sweater, shirt, jacket, tank, tee ; (And related stuff): color(white, black, blue), look, fabric(soft, material), price (sale), fit, quality <br>\n\n**TOPIC 1**- *Attributes of clothes*\nsize(small, medium, large, petite, big, short, little,  length, regular,  lb, true), fit (tight, true), fabric, color, fittings\/sizing in different body parts(arm, waist, shoulder, bottom, side), return if not a good one <br>\n\n**TOPIC 2**- *Lower wear*\n(Items) dress, jean, pant, skirt, boot (And related stuff): color, work\/casual, length-short, size-true, season- summer, fall, material <br> -->","13377cdf":"## INSIGHTS- CLUSTER AND TOPIC ANALYSIS\n\n<img src=\"https:\/\/media.giphy.com\/media\/uBfr9DFs9vc40\/giphy.gif\">","55100b04":"## K-Means Clustering\n\nOne can easily spot the clusters above. \nNow let us have the opinion of K-Means as well! \nWe will color by K-Means clustering to figure out if both the algorithms agree on the clustering!","1264f7d6":"![image.png](attachment:image.png)","802ea542":"### 3. Sentiment Analysis\n\nCalibrating Title and Review sentiment scores using [VADER](https:\/\/github.com\/cjhutto\/vaderSentiment) Sentiment! <br>\nVADER (Valence Aware Dictionary and sentiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.","5ab86ace":"Step 5: Train your LDA model- Topic Modeling","57a34ed4":"**Repeating this analysis for each cluster and summarizing the graphs, We obtain the following observations:**\n\n**Cluster 1 contains reviews having:**\n- Young\/Middle-aged women(age group 25-40) who have written descriptive reviews(50-80 words).\n- Reviews are related to topic 3(31%),2,1. \n\n**Cluster 2 contains reviews having:**\n- Middle\/Elderly women(age group 35-60) who have written rather precise reviews(10-40 words).\n- Reviews are related to topic 2(~40%),3,1. \n\n**Cluster 3 contains reviews having:**\n- All age groups inclusive(25-60) who have written very detailed reviews(80-110 words).\n- Reviews are related to topic 3(~37%),0,1. \n","f8b2e555":"**Feeding the bigrams into a Word2Vec model produces more meaningful bigrams**","f8230965":"After analyzing certain Title sentiments, we notice that titles with 0 scores consist of positive, negative and neutral sentiments:\n\n*Negative sentiments with 0 score*:<br>\nFalls flat  >>>>> 0.0<br>\nRuns short  >>>>> 0.0<br>\n\n*Positive sentiments with 0 score*:<br>\nMust have  >>>>> 0.0<br>\nComfy  >>>>> 0.0<br>\nStylish and versatile!  >>>>> 0.0 <br>\n\n*Neutral sentiments with 0 score*:<br>\nSimple, stylish, lovely-runs a bit big  >>>>> 0.0<br>\nSome things you should know...  >>>>> 0.0<br>\nMehh  >>>>> 0.0<br>\nNeutral blue  >>>>> 0.0<br>\n\n**Hence we have assigned score '0.0' score to 'No title' above.**","8af98737":"**Cluster 1 Analysis**","0f9fcc3c":"**Customer segmentation**, on the other hand, helps in targeted marketing, new customer acquisitions and hence more successful campaigns. \nToday we are going to perform Customer Segmentation by clustering the valuable **Customer Reviews**.\n\nMost simplified methods have been used in my notebook to enable ease of understanding for beginners such as myself!\n\nIn this project you will also get a glimpse of:\n- Text preprocessing for NLP (Stopwords removal, Lemmatization, part-of-speech tagging), \n- Sentiment analysis (vader sentiment)\n- Dimensionality reduction (pca+tsne)\n- Clustering (K-means)\n- Topic Modeling (LDA)\n<br><br>\nworking together to provide **meaningful insights**!\n<br>\nLET'S DIVE IN!","d1c29107":"Storing the major topic against each review!","890a3a50":"STEP 5: *Ta-Daa!* Here are your Topics!","3bf164a5":"**Analyze K-means Clustering against Topic Labeling**","c5cbdfa5":"**Understanding Topics:**","e6a763a5":"### Feature Encoding and prepping up our data for clustering!","058325ef":"**Checkout some cool stuff from the bigram model!**","d9050ea5":"**topic 0 contains reviews having:**\n- Reviews are related to Dress, jackets, skirts. \n- Concerning stuff: casual\/work wear, look, color, fit.\n\n**topic 1 contains reviews having:**\n- Reviews are related to bottomwear such as Jeans, pant, denim, skirts\n- Concerning stuff: Stretch, skinny, short\n\n**topic 2 contains reviews having:**\n- Reviews are related to Topwear- shirt, sweater, jacket.\n- Concerning stuff: Fabric, white, material, color, sleeve, arm, blue, boxy\n\n**topic 3 contains reviews having:**\n- Reviews are related to size related-issues- return!\n- Concerning stuff: Small, petite, large, medium, regular, little, short, tight\n- Related to- waist, bust, hip, shoulder!","80aede63":"### 2. Feature Engineering","ac163fc1":"Step 3: Add bigrams to your corpus using Word2vec model from gensim","e43fa80c":"## Topic Modeling- Latent Dirichlet Allocation(LDA) \n\nFor topic modeling, we will use the infamous LDA (Latent Dirichlet Allocation) algorithm. In LDA, each document can be described by a distribution of topics and each topic can be described by a distribution of words.","8ec1b9a3":"References:\n\nPFB few amazing blogs and notebooks : \n\nhttps:\/\/www.kaggle.com\/adhok93\/understanding-age-wise-sentiments-using-k-means <br>\nhttps:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\/notebook <br>\nhttps:\/\/github.com\/adashofdata\/nlp-in-python-tutorial\/blob\/master\/4-Topic-Modeling.ipynb <br>\nhttps:\/\/towardsdatascience.com\/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b<br>\nhttps:\/\/distill.pub\/2016\/misread-tsne\/ <br>\nUnderstanding LDA:<br>\nhttp:\/\/blog.echen.me\/2011\/08\/22\/introduction-to-latent-dirichlet-allocation\/ <br>","ba56b3b5":"The **compound score** obtained from valence scores is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.","d5ad99fa":"STEP 2: DATA CLEANING- PROCURE ONLY NOUNS AND ADJECTIVES TO OBTAIN MEANINGFUL TOPICS!","9b89b035":"### Deriving Conclusions- Looking at the data","a09eda06":"Note: While optimizing TSNE- Since intuition behind perplexity(K) is how many neighbors each data point can \u201csense\u201d, it is widely accepted that K~ N^(1\/2). Hence in our case, K~150","83edd0ca":"## Clustering Begins!\n\nAfter certain experiments,it is evident that pca followed by tsne gives most identifiable and clear clusters in low dimensions. Refer [example](https:\/\/towardsdatascience.com\/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b) below:","fc3dfa7e":"Step 4: Create a dictionary and corpus for input to our LDA model. Filter out the most common and uncommon words. ","5e8756e1":"**[Note](https:\/\/radimrehurek.com\/gensim\/auto_examples\/core\/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py)**\n\nCalling **model[corpus]** only creates a wrapper around the old corpus document stream \u2013 actual conversions are done on-the-fly, during document iteration. We cannot convert the entire corpus at the time of calling corpus_transformed = model[corpus], because that would mean storing the result in main memory, and that contradicts gensim\u2019s objective of memory-indepedence. If you will be iterating over the transformed corpus_transformed multiple times, and the transformation is costly, serialize the resulting corpus to disk first and continue using that.","dd7abe49":"### Welcome to my first and one of my favourite NLP analysis!\n\nIn these times of quarantine, online shopping have shown to be one of the greatest stress-busters for women!\ud83d\ude0d <br>\nWhile all of us are on our journeys to becoming shopaholics or broke \ud83d\ude01, let us analyze the importance of CUSTOMER REVIEWS while shopping online!\n\nAs per facts, **61%** of customers read online reviews before making a purchase decision, and they are now essential for *e-commerce sites*. Also, according to Reevoo, reviews produce an average **18%** uplift in sales. Hence ***USER REVIEWS*** are proven sales drivers, and something the majority of customers will definitely want to see before deciding to make a purchase.\n\n<img src=\"https:\/\/media.giphy.com\/media\/cqw80XStn460U\/giphy.gif\">\n\n","887d058e":"<img src=\"https:\/\/media.giphy.com\/media\/lD76yTC5zxZPG\/giphy.gif\">\n\n\n**Here my little attempt on NLP comes to end**\n\n**Thanks for reading my kernel!**\n\n**If you have any doubts or suggestions for improving my analysis, do let me know in the comments!**\n\n**If you liked my kernel, do upvote!**\n\n**Take care!**","04394206":"## INTRODUCTION","32885c97":"As we can see, the K-Means clusters also closely represents the clusters created by PCA and TSNE. Together it has produced some classic clustering. <br>\nApart from clustering the reviews together, we would also like to understand ***'meaning of each cluster'***. This can be achieved via **TOPIC MODELING**. <br>\n\nHence, now we will attempt to find the most significant words in each cluster. K-means clustered the articles but did not label the topics. Through topic modeling we will find out what the most important terms for each cluster are. This will add more meaning to the cluster by giving **keywords** to quickly identify the themes of the cluster."}}