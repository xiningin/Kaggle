{"cell_type":{"c1d717ed":"code","b9a8f7ff":"code","db03a40c":"code","7e166bab":"code","a021616f":"code","5797d120":"code","30b8d07e":"code","047c1de7":"code","16efcb8d":"code","74793dfa":"code","0743f73f":"code","e4ba07e9":"code","55080418":"code","5ac0fcdc":"code","596f95cd":"code","f231052c":"code","64dc5d29":"code","547db078":"code","425a7990":"code","d16f9e1d":"code","1770d270":"code","2939e1a5":"code","29903d8c":"code","8be9ed28":"code","44b65b7f":"code","7f0fa1c0":"code","6cfc227d":"code","aaaa84eb":"code","1efbac76":"code","ba17af52":"code","ba539d5c":"code","3ada9add":"code","84f90c11":"markdown","d45a633d":"markdown","8d307f81":"markdown","b296616b":"markdown"},"source":{"c1d717ed":"%%time\n\nimport os, psutil\nimport gc\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import cross_validate,cross_val_score,train_test_split, KFold, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score,make_scorer, precision_score, recall_score,f1_score, roc_curve,auc\nfrom sklearn import metrics\nfrom sklearn import ensemble,metrics,model_selection,neighbors,preprocessing, svm, tree\nfrom sklearn.preprocessing import MinMaxScaler\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport optuna\nfrom optuna.integration import LightGBMPruningCallback\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nimport scikitplot.metrics as skplot\nimport datatable as dt\n\n# machine learning tools\nimport h2o\nfrom h2o.estimators import H2OGeneralizedLinearEstimator, H2ORandomForestEstimator, H2OGradientBoostingEstimator\nfrom h2o.automl import H2OAutoML\n\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\n# from IPython.core.interactiveshell import InteractiveShell\n# InteractiveShell.ast_node_interactivity = 'all'\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b9a8f7ff":"def cpu_usage():\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    return f'Memory Usage : {round(py.memory_info()[0]\/2**30,2)}'","db03a40c":"# function to reduce data memory size\ndef reduce_memory_usage(df):\n    start_mem = df.memory_usage().sum()\/1024**2\n    numerics = ['int8', 'int16', 'int32','int64', 'float16','float32','float64']\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(df[col].dtype)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum()\/1024**2\n    print(f'Memory reduced from {round(start_mem,2)} -> {round(end_mem,2)}.\\nReduction in memory size by {round(((start_mem - end_mem)\/start_mem)*100,2)}%')\n    cpu_usage()","7e166bab":"def dtype_graph(df):\n    cat_cols = df.select_dtypes(include = bool).columns\n    cont_cols = df.select_dtypes(include = 'float16').columns\n    print(f'Categorical columns: {len(cat_cols)} | {round(len(cat_cols)\/(len(cat_cols) + len(cont_cols))*100,0)}%')\n    print(f'Numerical columns: {len(cont_cols)} | {round(len(cont_cols)\/(len(cat_cols) + len(cont_cols))*100,0)}%')\n    plt.bar([1,2],[len(cat_cols), len(cont_cols)])\n    plt.xticks([1,2],('Categorical', 'Continuos'));\n    plt.show()","a021616f":"%%time\ntrain = dt.fread('..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ntest = dt.fread('..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas()","5797d120":"%%time\nreduce_memory_usage(train)\nreduce_memory_usage(test)","30b8d07e":"cat_cols = test.select_dtypes(include = bool).columns\ncont_cols = test.select_dtypes(include = 'float16').columns","047c1de7":"features = train.columns.tolist()\nfeatures.remove('id')\nfeatures.remove('target')\ntrain[train.select_dtypes(include = bool).columns] = train[train.select_dtypes(include = bool).columns].astype(int)","16efcb8d":"train['target'] = train['target'].astype(int)","74793dfa":"# train_sub = train.sample(frac = .20, random_state = 42) # getting a sample of training data to run the models faster\n# X_train, X_test, y_train, y_test = train_test_split(train_sub[features],train_sub['target'],\n#                                                     train_size = 0.8, test_size = 0.2, \n#                                                     random_state = 42,stratify = train_sub['target'])","0743f73f":"# # logreg = LogisticRegression()\n# rf = RandomForestClassifier()\n# xgb = XGBClassifier(enable_categorical = True)\n# lgbm = LGBMClassifier()\n# cb = CatBoostClassifier(allow_writing_files = False, logging_level = 'Silent')\n\n# # model_list = [rf, xgb, lgbm, cb]\n# model_list = [lgbm,xgb]\n\n# scoring = {'auc_score' : make_scorer(roc_auc_score),\n#            'accuracy' : make_scorer(accuracy_score), \n#            'precision' : make_scorer(precision_score),\n#            'recall' : make_scorer(recall_score), \n#            'f1_score' : make_scorer(f1_score)}","e4ba07e9":"# def model_fit(model_list, cv = 3):\n#     results = pd.DataFrame(index = ['auc_score_train','auc_score_test','fit_time','precision', 'recall', 'f1_score'])\n#     for model in model_list:\n#         cv_score_list = []\n#         model.fit(X_train,y_train)\n#         y_preds = model.predict(X_test)\n#         roc_auc_score_test = roc_auc_score(y_test,y_preds)\n        \n#         cv_score =  cross_validate(model,X_train, y_train, \n#                          cv = StratifiedKFold(n_splits = 3, random_state = 42),\n#                          scoring = scoring, verbose = 2)\n        \n#         cv_score_list.append(cv_score['test_auc_score'].mean())\n#         cv_score_list.append(roc_auc_score_test)\n#         cv_score_list.append(cv_score['fit_time'].mean())\n#         cv_score_list.append(cv_score['test_precision'].mean())\n#         cv_score_list.append(cv_score['test_recall'].mean())\n#         cv_score_list.append(cv_score['test_f1_score'].mean())\n        \n#         results[model.__class__.__name__] = cv_score_list\n#         print(f'-----------------{model.__class__.__name__} Fitted -----------------')\n#     return results","55080418":"# all_models_results = model_fit(model_list, cv = 3)","5ac0fcdc":"# all_models_results","596f95cd":"\n### LGBM parameter tuning using Optuna","f231052c":"# train_sub = train_sub.reset_index(drop = True)","64dc5d29":"# def objective(trial, X = train_sub[features], y = train_sub['target']):\n    \n#     param_grid = {\n# #     \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n#     'n_estimators' : trial.suggest_categorical('n_estimators' ,[10000]),\n#     'learning_rate' : trial.suggest_float('learning_rate', .01,.3),\n#     'num_leaves' : trial.suggest_int('num_leaves', 20, 3000, step = 200),\n#     'max_depth' : trial.suggest_int('max_depth', 3, 12),\n#     'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 200, 10000, step = 10),\n#     'reg_alpha' : trial.suggest_int('reg_alpha', 0, 100, step = 5),\n#     'reg_lambda' : trial.suggest_int('reg_lambda', 0, 100, step = 5),\n#     'min_gain_to_split' : trial.suggest_float('min_gain_to_split', 0, 15),\n#     'bagging_fraction': trial.suggest_float('bagging_fraction', 0.2, 0.95, step=0.1),\n#     'bagging_freq': trial.suggest_categorical('bagging_freq', [1]),\n#     'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.95, step=0.1)\n#     }\n    \n#     cv = StratifiedKFold(shuffle= True, random_state= 42)\n#     cv_scores = np.empty(5)\n    \n#     for idx, (train_idx, test_idx) in enumerate(cv.split(X,y)):\n#         X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n#         y_train, y_test = y[train_idx], y[test_idx]\n    \n    \n#         model = LGBMClassifier(objective = 'binary', **param_grid, random_state= 42,\n#                                device = 'gpu', gpu_platform_id = 0, gpu_device_id = 0, silent = True)\n\n#         model.fit(X_train, y_train, \n#                   eval_set = [(X_test, y_test), (X_train, y_train)], \n#                   eval_metric= 'auc', \n#                   early_stopping_rounds= 100,\n#                   callbacks=[LightGBMPruningCallback(trial, 'auc')],verbose = False)\n\n#         y_preds = model.predict(X_test)\n#         auc_score = roc_auc_score(y_test, y_preds)\n#         cv_scores[idx] = auc_score\n    \n#     return np.mean(cv_scores)\n","547db078":"# study = optuna.create_study(direction = 'maximize', study_name = 'LGBM Classifier')\n# func = lambda trial : objective(trial)\n# study.optimize(func, n_trials = 10, timeout= 3600*2)","425a7990":"# for param, value in study.best_params.items():\n#     print(f'{param} : {value}')","d16f9e1d":"# tuned_lgb_params = {'n_estimators' : 10000,\n# 'learning_rate' : 0.15635058762282747,\n# 'num_leaves' : 1620,\n# 'max_depth' : 6,\n# 'min_data_in_leaf' : 3430,\n# 'reg_alpha' : 100,\n# 'reg_lambda' : 40,\n# 'min_gain_to_split' : 2.20631337496675,\n# 'bagging_fraction' : 0.8,\n# 'bagging_freq' : 1,\n# 'feature_fraction' : 0.5}","1770d270":"# features = train.columns.tolist()\n# features.remove('id')\n# features.remove('target')\n# # train[train.select_dtypes(include = bool).columns] = train[train.select_dtypes(include = bool).columns].astype('int16').astype('object')\n# X_train, X_test, y_train, y_test = train_test_split(train[features],train['target'],\n#                                                     train_size = 0.8, test_size = 0.2, \n#                                                     random_state = 42)\n","2939e1a5":"# lgbm_tuned = LGBMClassifier(**tuned_lgb_params,device = 'gpu', gpu_platform_id = 0, gpu_device_id = 0)\n# lgbm_tuned.fit(X_train, y_train, \n#                   eval_set = [(X_test, y_test), (X_train, y_train)], \n#                   eval_metric= 'auc', \n#                   early_stopping_rounds= 100,verbose = False)","29903d8c":"# y_preds  = lgbm_tuned.predict(X_test)\n# y_probas = lgbm_tuned.predict_proba(X_test)","8be9ed28":"# print(classification_report(y_test,y_preds))\n# fig, axes = plt.subplots(2,2,figsize = (20,10));\n# skplot.plot_confusion_matrix(y_test,y_preds, ax = axes[0,0], normalize = True);\n# skplot.plot_lift_curve(y_test,y_probas, ax = axes[0,1]);\n# skplot.plot_precision_recall_curve(y_test,y_probas, ax = axes[1,0]);\n# skplot.plot_roc_curve(y_test,y_probas, ax = axes[1,1]);\n# fig.tight_layout()\n","44b65b7f":"from category_encoders import TargetEncoder","7f0fa1c0":"train_sub = train.sample(frac = .20, random_state = 42) # getting a sample of training data to run the models faster\nX = train_sub[features]\ny = train_sub['target']","6cfc227d":"enc = TargetEncoder(cols = cat_cols)","aaaa84eb":"X = enc.fit_transform(X,y)\nX_test = enc.transform(test[features])","1efbac76":"params = {\n    'max_depth': 6,\n    'n_estimators': 9500,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'use_label_encoder': False\n    }","ba17af52":"%%time\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=786)\n\npreds = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    params['learning_rate']=0.007\n    model1 = XGBClassifier(**params)\n    \n    model1.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False)\n    \n    params['learning_rate']=0.01\n    model2 = XGBClassifier(**params)\n    \n    model2.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model1)\n    \n    params['learning_rate']=0.05\n    model3 = XGBClassifier(**params)\n    \n    model3.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model2)\n    \n    pred_valid = model3.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    test_preds = model3.predict_proba(X_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","ba539d5c":"# lgbm_preds = lgbm_tuned.predict(test[features])\n# lgbm_submission = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")\n# lgbm_submission.target = lgbm_preds\n# lgbm_submission.to_csv(\"lgbm_pbaseline_submission.csv\",index=False)\n# print('lgbm submission complete')   ","3ada9add":"predictions = np.mean(np.column_stack(preds),axis=1)\nxgb_submission = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")\nxgb_submission['target'] = predictions\nxgb_submission.to_csv('.\/xgb_submission.csv', index=False)","84f90c11":"## Submission file","d45a633d":"## Helper Functions","8d307f81":"## Get data","b296616b":"## Trying xgb"}}