{"cell_type":{"15d878b1":"code","bc4a1c76":"code","a81c6713":"code","5fff045c":"code","b3cec029":"code","d1a30b0d":"code","9a7ef074":"code","b579542f":"code","0aaedf53":"code","25b49e92":"code","84c8be4b":"code","5f369583":"code","7c3c0f3f":"code","04e0a6d2":"code","d996c6c9":"code","4626a12c":"code","f9e741d0":"code","2713f7e1":"code","286cd4ad":"code","2c531fc9":"code","97066100":"code","0d1bbefc":"code","002ea60d":"code","36f56e83":"code","7c38c481":"code","2c255a26":"code","028e1403":"code","2d09c7ba":"code","9fa591bc":"code","37cb2986":"code","6b824a2a":"code","bfe6d6a7":"code","9fa89826":"code","030918f4":"code","6ab20529":"code","670e67c4":"code","98e5ea2e":"code","5831ed8b":"code","9a1f6df5":"code","abd5b3eb":"markdown","ac0e670a":"markdown","3deb32ec":"markdown","3b384b48":"markdown","896b212e":"markdown","3a4b2ae8":"markdown","dc3c9732":"markdown","2afdfd0b":"markdown"},"source":{"15d878b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc4a1c76":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv',index_col='id')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","a81c6713":"train","5fff045c":"train.info()","b3cec029":"train['target'].value_counts().plot(kind='pie')","d1a30b0d":"train[train['keyword'].isna()==False]","9a7ef074":"train['keyword'].value_counts()","b579542f":"dummy= train.copy()\n\ndummy.loc[dummy['keyword'].isna()==False,'keyword_01'] = 1\ndummy.loc[dummy['keyword'].isna()==True,'keyword_01'] = 0\ndummy.loc[dummy['location'].isna()==True,'loc_01'] = 0\ndummy.loc[dummy['location'].isna()==False,'loc_01'] = 1\n\ndummy.head()","0aaedf53":"dummy.groupby('target')['keyword_01'].value_counts().plot(kind='bar')","25b49e92":"dummy.groupby('target')['loc_01'].value_counts().plot(kind='bar')","84c8be4b":"import nltk\nfrom nltk import word_tokenize, FreqDist\nfrom nltk.corpus import stopwords\nnltk.download\nnltk.download('wordnet')\nnltk.download('stopwords')\nimport string\nimport re\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer","5f369583":"# Text Pre-Processing\n\ndef process_tweet(tweets):\n\n    #remove RT style for retweet\n    tweets = tweets.apply(lambda x: re.sub(r'^RT[\\s]+','',x))\n\n    # remove hyperlinks\n    tweets = tweets.apply(lambda x: re.sub(r'\\w+:\\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\\/[^\\s\/]*))*','',x))\n\n    #remove hashtags\n    tweets = tweets.apply(lambda x: re.sub(r'#','',x))\n\n    #tokenizing\n    tokenizer = TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len=True)\n    tweets = tweets.apply(lambda x: tokenizer.tokenize(x))\n\n    # removing stop words\n    stopwords_eng = stopwords.words('english')\n    punc = string.punctuation\n    tweets = tweets.apply(lambda x: [t for t in x if t not in stopwords_eng and t not in punc])\n\n    # stemming\n    stemmer = PorterStemmer()\n    tweets = tweets.apply(lambda x: [stemmer.stem(t) for t in x])\n\n    return tweets","7c3c0f3f":"train.text = process_tweet(train.text)\ntrain.text","04e0a6d2":"from wordcloud import STOPWORDS, WordCloud, ImageColorGenerator\nimport matplotlib.pyplot as plt","d996c6c9":"train","4626a12c":"t = \" \".join(t for t in train[train.target==1]['text'].apply(lambda x: \" \".join(x)))\n\nstopwords_ = set(STOPWORDS)\n\nword = WordCloud(stopwords=stopwords_,background_color='white').generate(t)\n\nplt.figure(figsize=(20,5))\nplt.imshow(word, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Word Cloud for Disaster Tweets')\nplt.show()","f9e741d0":"t = t = \" \".join(t for t in train[train.target==0]['text'].apply(lambda x: \" \".join(x)))\n\nstopwords_ = set(STOPWORDS)\nstopwords_.update([\"https\",\"co\",\"will\",'amp',\"-\",\"_\"])\n\nword = WordCloud(stopwords=stopwords_,background_color='Black').generate(t)\n\nplt.figure(figsize=(20,5))\nplt.imshow(word, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Word Cloud for Non-Disaster Tweets')\nplt.show()","2713f7e1":"train.text","286cd4ad":"# word frequencies dictionary\n\nyslist = np.squeeze(train.target).tolist()\n\nfreqs={}\n\nfor y, word in zip(yslist,train.text):\n    for w in word:\n        pair = (w,y)\n        if pair in freqs:\n            freqs[pair] += 1\n        else:\n            freqs[pair] = 1\n\n    ","2c531fc9":"# word counts\n\ndata = []\n\nfor word in train.text.iloc[1]:\n    \n    pos = 0\n    neg = 0\n    \n    if (word,1) in freqs:\n        pos = freqs[(word,1)]\n        \n    if (word,0) in freqs:\n        neg = freqs[(word,0)]\n        \n    data.append([word,pos,neg])\n\ndata","97066100":"fig, ax = plt.subplots(figsize=(8,8))\n\nx = np.log([x[1]+1 for x in data])\ny = np.log([x[2]+1 for x in data])\n\nax.scatter(x,y)\n\nplt.xlabel('Log Disaster Count')\nplt.ylabel('Log Non-Disaster Count')\n\nfor i in range(len(data)):\n    ax.annotate(data[i][0],(x[i],y[i]), fontsize=12)\n\nax.plot([0,9],[0,9], color='red')\nplt.show()","0d1bbefc":"# word frequencies dictionary\n\ndef extract_features(tweet,freqs):\n    # word counts\n    \n    x = np.zeros((1,3))\n    \n    x[0,0] = 1\n    \n    for word in tweet:\n\n        if (word,1) in freqs:\n            x[0,1] += freqs[(word,1)] #number of related disaster word\n\n        if (word,0) in freqs:\n            x[0,2] += freqs[(word,0)] # number of non-related disaster word\n\n    assert(x.shape==(1,3))\n    return x","002ea60d":"!pip install pycaret","36f56e83":"from sklearn.model_selection import train_test_split\nfrom pycaret.classification import *","7c38c481":"df = train.text.apply(lambda x: extract_features(x,freqs))\ndf.head()","2c255a26":"X = pd.DataFrame()\nX['Bias'] = df.apply(lambda x: x[0][0])\nX['Pos'] = df.apply(lambda x: x[0][1])\nX['Neg'] = df.apply(lambda x: x[0][2])","028e1403":"X","2d09c7ba":"df_train = pd.concat([X,train.target],axis=1)\ndf_train.head()","9fa591bc":"# clf = setup(df_train, target = 'target')","37cb2986":"# compare_models(cross_validation = True)","6b824a2a":"from sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.1, loss='deviance', max_depth=3,\n                           max_features=None, max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, n_estimators=100,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=5039, subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False)\nmodel.fit(df_train.drop('target',axis=1),df_train['target'])","bfe6d6a7":"# dt = create_model('gbc')","9fa89826":"test.head()","030918f4":"test.text = process_tweet(test.text)\ntest.text = test.text.apply(lambda x: extract_features(x,freqs))","6ab20529":"X_test = pd.DataFrame()\nX_test['Bias'] = test.text.apply(lambda x: x[0][0])\nX_test['Pos'] = test.text.apply(lambda x: x[0][1])\nX_test['Neg'] = test.text.apply(lambda x: x[0][2])","670e67c4":"X_test.head()","98e5ea2e":"ypred = model.predict(X_test)\nypred","5831ed8b":"result  = pd.concat([test.id,pd.DataFrame(ypred, columns=['target'])], axis=1)\nresult.head()","9a1f6df5":"result.to_csv('result.csv',index=False)","abd5b3eb":"From the word cloud above, we can see the differences between real disaster and non-disaster tweets. For the next step, I am thinking on using tf-idf and classification method.","ac0e670a":"# Test Model","3deb32ec":"# Data Preparation\n1. Word frequencies dictionary","3b384b48":"# Text Preprocessing\nThis part is cleaning the text data. The step is lowercase, removing puctuation, tokenization, stopword filtering, and stemming.\n1. tokenizing\n2. lowercasing\n3. removing stop words and punctuation\n4. stemming","896b212e":"# Data Visualization","3a4b2ae8":"# Sentiment Analysis Disaster Tweets\nThis is my first sentiment analysis project. This notebook hasn't done yet.","dc3c9732":"# Data Exploratory","2afdfd0b":"# Training Model"}}