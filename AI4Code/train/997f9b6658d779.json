{"cell_type":{"87bc354c":"code","910360bc":"code","bd727299":"code","6f045bb6":"code","fa8e0cec":"code","1106b21c":"code","5bc783ed":"code","38273f65":"code","695294db":"code","d207e166":"code","6954c18c":"code","826219d9":"code","3cfa3374":"code","13637219":"code","26ab8fe5":"code","8dcdcdb5":"code","79d96af3":"code","fead795b":"code","ae622c22":"code","f8a8ea3c":"code","68c8631a":"code","41440350":"code","7cb87425":"code","86afbe0a":"code","0b4516ea":"code","4a1cf96a":"code","7de05ad3":"code","81638841":"code","ca3cb908":"code","8af253df":"code","0a7571bf":"code","6b57521d":"code","b57fd62e":"code","047253f6":"code","d3163e5a":"code","687a4801":"code","1a569ff4":"code","2f9ad369":"code","d5bd70c1":"code","bbb8c69d":"code","06bd02ec":"code","60dc9e65":"code","71ab1122":"code","827b2fc4":"code","91216e14":"code","410ec2ca":"code","81bd8128":"code","fac5d7ae":"code","0eb92d1a":"code","77e77a56":"code","b7014dee":"code","2efc1447":"markdown","87c1ed34":"markdown","64e2711a":"markdown","448accf7":"markdown","a1582a42":"markdown","29d87f8e":"markdown","0b0355f0":"markdown","a4ffc144":"markdown","65c5032a":"markdown","f2b94d9d":"markdown","e2ce1aaa":"markdown","b020107a":"markdown","7f767591":"markdown","6739a043":"markdown"},"source":{"87bc354c":"!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install wordcloud\n!pip install seaborn\n!pip install sklearn\n!pip install plotly","910360bc":"import numpy as np \nimport pandas as pd\nimport matplotlib\nimport seaborn as sns\nimport plotly\nimport matplotlib.pyplot as plt\n%matplotlib inline \nfrom wordcloud import WordCloud, STOPWORDS\npd.options.mode.chained_assignment = None ","bd727299":"tweets_df = pd.read_csv(\"..\/input\/covid19-tweets\/covid19_tweets.csv\")\ncovid_confirmed_cases = pd.read_csv(\"..\/input\/covid-cases\/time_series_covid19_confirmed_global.csv\")\ncovid_deaths = pd.read_csv(\"..\/input\/covid-cases\/time_series_covid19_deaths_global.csv\")","6f045bb6":"print(f\"data shape: {tweets_df.shape}\")","fa8e0cec":"tweets_df.info()","1106b21c":"tweets_df.describe()","5bc783ed":"tweets_df.head()","38273f65":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","695294db":"missing_data(tweets_df)","d207e166":"def unique_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    uniques = []\n    for col in data.columns:\n        unique = data[col].nunique()\n        uniques.append(unique)\n    tt['Uniques'] = uniques\n    return(np.transpose(tt))","6954c18c":"unique_values(tweets_df)","826219d9":"def most_frequent_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    items = []\n    vals = []\n    for col in data.columns:\n        itm = data[col].value_counts().index[0]\n        val = data[col].value_counts().values[0]\n        items.append(itm)\n        vals.append(val)\n    tt['Most frequent item'] = items\n    tt['Frequence'] = vals\n    tt['Percent from total'] = np.round(vals \/ total * 100, 3)\n    return(np.transpose(tt))","3cfa3374":"most_frequent_values(tweets_df)","13637219":"def plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()    ","26ab8fe5":"plot_count(\"user_name\", \"User name\", tweets_df,4)","8dcdcdb5":"plot_count(\"user_location\", \"User location\", tweets_df,4)","79d96af3":"plot_count(\"source\", \"Source\", tweets_df,4)","fead795b":"stopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=50,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","ae622c22":"### Text wordcloauds","f8a8ea3c":"show_wordcloud(tweets_df['text'], title = 'Prevalent words in tweets')","68c8631a":"india_df = tweets_df.loc[tweets_df.user_location==\"India\"]\nshow_wordcloud(india_df['text'], title = 'Prevalent words in tweets from India')","41440350":"us_df = tweets_df.loc[tweets_df.user_location==\"United States\"]\nshow_wordcloud(us_df['text'], title = 'Prevalent words in tweets from US')","7cb87425":"us_df = tweets_df.loc[tweets_df.user_location==\"United Kingdom\"]\nshow_wordcloud(us_df['text'], title = 'Prevalent words in tweets from UK')","86afbe0a":"us_df = tweets_df.loc[tweets_df.user_location==\"Canada\"]\nshow_wordcloud(us_df['text'], title = 'Prevalent words in tweets from Canada')","0b4516ea":"india_df = tweets_df.loc[tweets_df.user_location==\"South Africa\"]\nshow_wordcloud(india_df['text'], title = 'Prevalent words in tweets from South Africa')","4a1cf96a":"india_df = tweets_df.loc[tweets_df.user_location==\"Switzerland\"]\nshow_wordcloud(india_df['text'], title = 'Prevalent words in tweets from Switzerland')","7de05ad3":"us_df = tweets_df.loc[tweets_df.user_location==\"London\"]\nshow_wordcloud(us_df['text'], title = 'Prevalent words in tweets from London')","81638841":"def plot_features_distribution(features, title, df, isLog=False):\n    plt.figure(figsize=(12,6))\n    plt.title(title)\n    for feature in features:\n        if(isLog):\n            sns.distplot(np.log1p(df[feature]),kde=True,hist=False, bins=120, label=feature)\n        else:\n            sns.distplot(df[feature],kde=True,hist=False, bins=120, label=feature)\n    plt.xlabel('')\n    plt.legend()\n    plt.show()\n","ca3cb908":"tweets_df['hashtags'] = tweets_df['hashtags'].replace(np.nan, \"['None']\", regex=True)\ntweets_df['hashtags'] = tweets_df['hashtags'].apply(lambda x: x.replace('\\\\N',''))\ntweets_df['hashtags_count'] = tweets_df['hashtags'].apply(lambda x: len(x.split(',')))\nplot_features_distribution(['hashtags_count'], 'Hashtags per tweet (all data)', tweets_df)","8af253df":"tweets_df['hashtags_individual'] = tweets_df['hashtags'].apply(lambda x: x.split(','))\nfrom itertools import chain\nall_hashtags = set(chain.from_iterable(list(tweets_df['hashtags_individual'])))\nprint(f\"There are totally: {len(all_hashtags)}\")","0a7571bf":"country_df = pd.read_csv(\"..\/input\/country-code\/datasets_403474_773844_wikipedia-iso-country-codes.csv\")","6b57521d":"country_df.columns = [\"country\", \"alpha2\", \"alpha3\", \"numeric\", \"iso\"]\ncountry_df.head()","b57fd62e":"tweets_df['country'] = tweets_df['user_location']","047253f6":"tweets_df = tweets_df.merge(country_df, on=\"country\")","d3163e5a":"tweets_df.head(10)","687a4801":"tw_add_df = tweets_df.groupby([\"country\", \"iso\", \"alpha3\"])['text'].count().reset_index()\ntw_add_df.columns = [\"country\", \"iso\", \"alpha3\", \"tweets\"]","1a569ff4":"import plotly.express as px\n\ndef plot_map(dd_df, title):\n    hover_text = []\n    for index, row in dd_df.iterrows():\n        hover_text.append((f\"country: {row['country']}<br>tweets: {row['tweets']}\\\n                          <br>country code: {row['iso']}<br>country alpha3: {row['alpha3']}\"))\n    dd_df['hover_text'] = hover_text\n\n    fig = px.choropleth(dd_df, \n                        locations=\"alpha3\",\n                        hover_name='hover_text',\n                        color=\"tweets\",\n                        projection=\"natural earth\",\n                        color_continuous_scale=px.colors.sequential.Plasma,\n                        width=900, height=700)\n    fig.update_geos(   \n        showcoastlines=True, coastlinecolor=\"DarkBlue\",\n        showland=True, landcolor=\"LightGrey\",\n        showocean=True, oceancolor=\"LightBlue\",\n        showlakes=True, lakecolor=\"Blue\",\n        showrivers=True, rivercolor=\"Blue\",\n        showcountries=True, countrycolor=\"DarkBlue\"\n    )\n    fig.update_layout(title = title, geo_scope=\"world\")\n    fig.show()    ","2f9ad369":"plot_map(tw_add_df, \"Tweets per country (where country is specified)\")","d5bd70c1":"tweets_df['datedt'] = pd.to_datetime(tweets_df['date'])\n","bbb8c69d":"tweets_df['year'] = tweets_df['datedt'].dt.year\ntweets_df['month'] = tweets_df['datedt'].dt.month\ntweets_df['day'] = tweets_df['datedt'].dt.day\ntweets_df['dayofweek'] = tweets_df['datedt'].dt.dayofweek\ntweets_df['hour'] = tweets_df['datedt'].dt.hour\ntweets_df['minute'] = tweets_df['datedt'].dt.minute\ntweets_df['dayofyear'] = tweets_df['datedt'].dt.dayofyear\ntweets_df['date_only'] = tweets_df['datedt'].dt.date","06bd02ec":"tweets_agg_df = tweets_df.groupby([\"date_only\"])[\"text\"].count().reset_index()\ntweets_agg_df.columns = [\"date_only\", \"count\"]","60dc9e65":"def plot_time_variation(df, x='date_only', y='count', hue=None, size=1, title=\"\", is_log=False):\n    f, ax = plt.subplots(1,1, figsize=(4*size,3*size))\n    g = sns.lineplot(x=x, y=y, hue=hue, data=df)\n    plt.xticks(rotation=90)\n    if hue:\n        plt.title(f'{y} grouped by {hue} | {title}')\n    else:\n        plt.title(f'{y} | {title}')\n    if(is_log):\n        ax.set(yscale=\"log\")\n    ax.grid(color='black', linestyle='dotted', linewidth=0.75)\n    plt.show()","71ab1122":"plot_time_variation(tweets_agg_df, title=\"Number of tweets \/ day of year\",size=3)","827b2fc4":"#Rename state columns\ncovid_deaths = covid_deaths.rename(columns={\"Province\/State\":\"state\",\"Country\/Region\": \"country\"})\ncovid_confirmed_cases = covid_confirmed_cases.rename(columns={\"Province\/State\":\"state\",\"Country\/Region\": \"country\"})\n\n#Changing the conuntry names as required by pycountry_convert Lib\ncovid_deaths.loc[covid_deaths['country'] == \"US\", \"country\"] = 'USA'\ncovid_confirmed_cases.loc[covid_confirmed_cases['country'] == \"US\", \"country\"] = \"USA\"\n                          \ncovid_deaths.loc[covid_deaths['country'] == 'Korea, South', \"country\"] = 'South Korea'\ncovid_confirmed_cases.loc[covid_confirmed_cases['country'] == \"Korea, South\", \"country\"] = \"South Korea\"\n\ncovid_deaths.loc[covid_deaths['country'] == 'Taiwan', \"country\"] = 'Taiwan'\ncovid_confirmed_cases.loc[covid_confirmed_cases['country'] == \"Taiwan*\", \"country\"] = \"Taiwan\"\n  \ncovid_deaths.loc[covid_deaths['country'] == 'Congo (Kinshasa)', \"country\"] = 'Democratic Republic of the Congo'\ncovid_confirmed_cases.loc[covid_confirmed_cases['country'] == \"Congo (Kinshasa)\", \"country\"] = \"Democratic Republic of the Congo\"\n    \ncovid_deaths.loc[covid_deaths['country'] == \"Cote d'Ivoire\", \"country\"] = 'C\u00f4te d Ivoire'\ncovid_confirmed_cases.loc[covid_confirmed_cases['country'] == \"Cote d'Ivoire\", \"country\"] = \"C\u00f4te d'Ivoire\"\n\ncovid_deaths.loc[covid_deaths['country'] == \"Reunion\", \"country\"] = 'R\u00e9union'\ncovid_confirmed_cases.loc[covid_confirmed_cases['country'] == \"Reunion\", \"country\"] = \"R\u00e9union\"\n  \ncovid_deaths.loc[covid_deaths['country'] == 'Congo (Brazzaville)', \"country\"] = 'Republic of the Congo'\ncovid_confirmed_cases.loc[covid_confirmed_cases['country'] == \"Congo (Brazzaville)\", \"country\"] = \"Republic of the Congo\"\n  \ncovid_deaths.loc[covid_deaths['country'] == 'Bahamas, The', \"country\"] = 'Bahamas'\ncovid_confirmed_cases.loc[covid_confirmed_cases['country'] == \"Bahamas, The\", \"country\"] = \"Bahamas\"\n\ncovid_deaths.loc[covid_deaths['country'] == 'Gambia, The', \"country\"] = 'Gambia'\ncovid_confirmed_cases.loc[covid_confirmed_cases['country'] == \"Gambia, The\", \"country\"] = \"Gambia\"\n\n#Copy the death statistics (USA ONLY) to a new variable andstrip out the continent and latlng\ncovid_death_cases = covid_deaths.copy().drop([\"Lat\",\"Long\",\"state\"], axis=1)\ncovid_confirmed_cases = covid_confirmed_cases.copy().drop([\"Lat\",\"Long\",\"state\"], axis=1)\n\n#Set the index of the pandas dataframe\ncovid_death_cases = covid_death_cases.set_index([\"country\"])\ncovid_confirmed_cases = covid_confirmed_cases.set_index([\"country\"])\n\n#Convert column headers to date time\ncovid_death_cases.columns = pd.to_datetime(covid_death_cases.columns)\ncovid_confirmed_cases.columns = pd.to_datetime(covid_confirmed_cases.columns)\n","91216e14":"trimmed_tweets = tweets_df.copy().drop([\n        \"user_name\",\n        \"user_description\",\n        \"user_created\",\n        \"user_followers\",\n        \"user_friends\",\n        \"user_favourites\",\n        \"user_verified\",\n        \"text\",\n        \"hashtags\",\n        \"source\",\n        \"is_retweet\",\n        \"hashtags_count\",\n        \"hashtags_individual\",\n    ], axis=1)\n\ndef user_in_usa(location):\n    import re\n    usa_list = [\"USA\",\"AL\", \"AK\", \"AZ\", \"AR\",\n                \"CA\", \"CO\", \"CT\", \"DE\", \"FL\",\n                \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \n                \"IA\", \"KS\", \"KY\", \"LA\", \"ME\",\n                \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \n                \"MO\", \"MT\", \"NE\", \"NV\", \"NH\",\n                \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \n                \"OH\", \"OK\", \"OR\", \"PA\", \"RI\",\n                \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \n                \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n    location = str(location)\n    locations = location.replace('-', ' ').split(' ')\n\n    for location in locations:\n        if location in usa_list:\n            return True\n    return False\n\n#Filter USA only tweets\nusa_tweets = trimmed_tweets[trimmed_tweets.apply(lambda x: x['alpha2'] == \"US\", axis=1)]\n\n#Trim time away from date and convert to date format\nusa_tweets['date'] = usa_tweets.apply(lambda x: x['date'].split(\" \")[0], axis=1)\nusa_tweets['date'] = pd.to_datetime(usa_tweets['date'])\n\n#Daily tweet count\nusa_daily_counts = usa_tweets.groupby(usa_tweets['date'].dt.date).size()\n\n#convert the series to a dataframe\nusa_daily_counts = usa_daily_counts.to_frame()\nusa_daily_counts.index = pd.to_datetime(usa_daily_counts.index)\nusa_daily_counts","410ec2ca":"covid_death_cases.head()\ncovid_confirmed_cases.head()","81bd8128":"#Transpose the dataframe so that rows are columns and the columns are rows, order the dataframe by column name\nt_covid_death = covid_death_cases.T\nt_covid_death.sort_index(axis=1, inplace=True)\n\nt_covid_confirm = covid_confirmed_cases.T\nt_covid_confirm.sort_index(axis=1, inplace=True)\n\n# Resample the dataset and concatenate it so that data is aggregated weekly\nt_covid_death_weekly = t_covid_death.resample('D').sum()\nt_covid_confirmed_weekly = t_covid_confirm.resample('D').sum()\nusa_daily_counts = usa_daily_counts.resample('D').sum()\n\n#Isolate USA data\nw_usa_deaths = t_covid_death_weekly['USA']\nw_usa_confirmed = t_covid_confirmed_weekly['USA']\n\n#Join the deaths and confirmed cases into one df\nusa_data = pd.concat([w_usa_confirmed, w_usa_deaths, usa_daily_counts], axis=1, ignore_index=True)\n\n#Rename the columns for readability\nusa_data.columns = ['confirmed', 'deaths', 'tweets']\n\n#Fill NAN with 0\nusa_data = usa_data.fillna(0)\nusa_data","fac5d7ae":"t_covid_death.head()","0eb92d1a":"usa_data.plot.line(logy=True)\n\n#Last point in dataset is anomoly because dataset incomplete for that week\"","77e77a56":"usa_data_after_july = usa_data.loc[(usa_data.index > '2020-7-20')]\nusa_data_after_july","b7014dee":"usa_data_after_july.plot.line(logy=True)\n","2efc1447":"<h1>Coronavirus COVID-19 Tweets<\/h1>\n\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F769452%2F35db2dd68238bfd958efdabebc9fef8f%2Fcovid-19-4961257_1280-e1586986896105.jpg?generation=1595760042647275&alt=media\" width=\"600\"><\/img>\n\n\n# Introduction\n\n\nThe Dataset we are using here is collected using Twitter API, **tweepy** and Python package.\n","87c1ed34":"### User name","64e2711a":"# Data exploration\n\n\n## Glimpse the data","448accf7":"### Tweet source","a1582a42":"### Missing data","29d87f8e":"## Load data","0b0355f0":"### Most frequent values","a4ffc144":"### Hashtags analysis","65c5032a":"### Correlation Analysis","f2b94d9d":"### Sanitization\/Re-formatting of Data","e2ce1aaa":"# Data preparation\n\n## Load packages","b020107a":"### User location","7f767591":"### Unique values","6739a043":"## Visualize the data distribution"}}