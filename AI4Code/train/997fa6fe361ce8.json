{"cell_type":{"53b58d0c":"code","f62514f6":"code","96c62650":"code","c7530332":"code","902d040d":"code","4bac80ac":"code","6cd9c33d":"code","b4689acf":"code","6eb4cb4b":"code","91afa05e":"code","52f32eb9":"code","4261e9b4":"code","c36ab575":"code","29029d79":"code","c7a8690f":"code","3c58828f":"code","38904def":"code","70d4cfc6":"code","a20dc67c":"code","f61fc0e6":"code","24d85654":"code","7902535b":"code","f95f570d":"code","a503d645":"code","fdc7e828":"code","a15e3e0f":"code","8d80f1f4":"code","7b3c7075":"code","e20b6911":"code","b457c97a":"code","ca8aaae1":"code","c4ea2ea2":"code","a91eb451":"code","eab73b5a":"code","4a7247fd":"code","f4e6e884":"code","30c75f96":"code","d1481d99":"code","f0d5a3bd":"code","e586651f":"code","450288e9":"code","4b505511":"code","1b25fa3f":"code","91f1c371":"code","47ba1c07":"code","ffdf4eed":"code","215d6379":"code","f6251a9e":"code","1b2f1c11":"code","f8fd62c5":"code","9845b751":"code","be98fb51":"markdown","64388dd6":"markdown","fb41b481":"markdown","f2d8ae71":"markdown","4262edee":"markdown","4e6dad41":"markdown","d7ab956d":"markdown","c110dd0e":"markdown","8ba2df75":"markdown","7b7f0ffd":"markdown","de867568":"markdown","dde7349c":"markdown","7ae9fac1":"markdown","4be73417":"markdown","468c75f7":"markdown","3147d836":"markdown","c83821ca":"markdown","b4e40176":"markdown","c9f4ba0a":"markdown","568c1b96":"markdown","904bf887":"markdown","49847905":"markdown","cdc5ca4f":"markdown","f72fc2ec":"markdown","24c2e651":"markdown","df073431":"markdown","7b2c7e60":"markdown","1db07525":"markdown","82cdedac":"markdown","57779841":"markdown","b6a86a8b":"markdown","cd8576b2":"markdown","25a1f326":"markdown","3c0af7c4":"markdown","1ba94bfb":"markdown","e8f1d25f":"markdown","f83a1be6":"markdown","fe5955b8":"markdown","dbe2a433":"markdown","2cc02717":"markdown","744fffdc":"markdown","79f4fe9c":"markdown","c24b1d74":"markdown","0e9fb418":"markdown","2f54b50d":"markdown","4d30647e":"markdown","2ced2f20":"markdown","1006e67c":"markdown","7b9d453d":"markdown","a27d32ce":"markdown","8447ac68":"markdown","2344e0f0":"markdown","2cef7ee0":"markdown","87dba210":"markdown","3fa3a549":"markdown","01734651":"markdown","7d80f50c":"markdown","6acc2d99":"markdown","6bbf1892":"markdown","402b2963":"markdown","dcfb1640":"markdown","ed8f5ede":"markdown","17a6db85":"markdown","648af1fa":"markdown","ffc61f9e":"markdown","593caacb":"markdown"},"source":{"53b58d0c":"# use to visualize missing value\n!pip install missingno","f62514f6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport missingno as msno\n## Display all the columns of the dataframe\npd.pandas.set_option('display.max_columns',None)\n\nimport warnings # to ignore warning","96c62650":"training_set = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_set = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsubmission_file = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","c7530332":"training_set.head()","902d040d":"test_set.head()","4bac80ac":"submission_file.head()","6cd9c33d":"training_set.shape","b4689acf":"test_set.shape","6eb4cb4b":"training_set.info()","91afa05e":"#checking the number of features in the Datasets\nprint(\"\\n\\nNumber of features in the datasets :\\n\",'#' * 40)\nprint(\"\\nTraining Set : \\n\",'-' * 20, len(training_set.columns))\nprint(\"\\nTest Set : \\n\",'-' * 20,len(test_set.columns))","52f32eb9":"# checking the features in the Dataset\nprint(\"\\n\\nFeatuers in the datasets\\n\", '#' * 40)\nprint(\"\\nTraining Set: \\n\", '-' * 20, '\\n', list(training_set.columns))\nprint(\"\\nTest Set: \\n\", '-' * 20, '\\n', list(test_set.columns))","4261e9b4":"# checking the data types of features\nprint(\"\\n\\nDatatypes of features in datasets: \\n\", '#' * 40)\nprint(\"\\nTraining Set: \\n\", '-' * 20,'\\n', training_set.dtypes)\nprint(\"\\nTest Set: \\n\", '-' * 20, '\\n', test_set.dtypes)","c36ab575":"# checking the number of rows\nprint(\"\\n\\nNumber of observations in the datasets :\\n\",'#' * 40)\nprint(\"\\nTraining Set : \\n\",'-' * 20,len(training_set))\nprint(\"\\nTest Set : \\n\",'-' * 20,len(test_set))","29029d79":"# checking for NaNs or empty cells\nprint(\"\\n\\nEmpty cells or Nans in the datasets :\\n\",'#' * 40)\nprint(\"\\nTraining Set : \\n\",'-' * 20,training_set.isna().values.any())\nprint(\"\\nTest Set : \\n\",'-' * 20,test_set.isna().values.any())","c7a8690f":"# checking for NaNs or empty cells by features\nprint(\"\\n\\nNumber of empty cells or Nans in the datasets :\\n\",'#' * 40)\nprint(\"\\nTraining Set : \\n\",'-' * 20,\"\\n\", training_set.isna().sum())\nprint(\"\\nTest Set : \\n\",'-' * 20,\"\\n\",test_set.isna().sum())","3c58828f":"# checking for features with missings\nprint(\"\\n\\nName of columns or features with missings in the datasets :\\n\",'#' * 40)\nprint(\"\\nTraining Set : \\n\",'-' * 20,\"\\n\", training_set.loc[:, training_set.isna().any()].columns)\nprint(\"\\nTest Set : \\n\",'-' * 20,\"\\n\",test_set.loc[:, training_set.isna().any()].columns)","38904def":"## to find missing values\nnull_values = training_set.isnull().sum()\ntrain_null = null_values[null_values!=0]\ntrain_null.sort_values(inplace=True)\nprint(\"\\n\\nNumber of all empty cells or Nans in the train datasets :\\n\",'#' * 20,\"\\n\\n\", len(train_null))\nprint(\"\\n\",'#' * 20,\"\\n\")\ntrain_null","70d4cfc6":"sns.heatmap(training_set.isnull(),yticklabels=False,cbar=False,cmap= 'Reds')","a20dc67c":"msno.matrix(training_set)","f61fc0e6":"msno.heatmap(training_set)","24d85654":"msno.dendrogram(training_set)","7902535b":"## to find missing values\nnull_values = test_set.isnull().sum()\ntest_null = null_values[null_values!=0]\ntest_null.sort_values(inplace=True)\nprint(\"\\n\\nNumber of all empty cells or Nans in the test datasets :\\n\",'#' * 20,\"\\n\\n\", len(test_null))\nprint(\"\\n\",'#' * 20,\"\\n\")\ntest_null","f95f570d":"sns.heatmap(test_set.isnull(),yticklabels=False,cbar=False,cmap= 'Reds')","a503d645":"msno.matrix(test_set)","fdc7e828":"msno.heatmap(test_set)","a15e3e0f":"msno.dendrogram(test_set)","8d80f1f4":"# as 'SalePrice' Column is not available in test dataset. So we'll delete it.\ntrain_dtype = training_set.drop('SalePrice', axis= 1)\n\ntrain_dtype = train_dtype.dtypes\ntest_dtype = test_set.dtypes\n\ntrain_dtype.compare(test_dtype)","7b3c7075":"test_set['BsmtUnfSF'].head()","e20b6911":"miss_train = training_set.isna().sum()\nmiss_test = test_set.isna().sum()\nmiss_train = miss_train.drop('SalePrice')\nmiss_compare_train_test = miss_train.compare(miss_test).sort_values(['self'], ascending= [False])\nmiss_compare_train_test","b457c97a":"numerical_features = [col for col in training_set.columns if training_set[col].dtypes != 'O']\n# numerical_features\ndiscrete_features = [col for col in numerical_features if len(training_set[col].unique()) < 25 and col not in ['Id']]\ncontinuous_features = [feature for feature in numerical_features if feature not in discrete_features+['Id']]\ncategorical_features = [col for col in training_set.columns if training_set[col].dtypes == 'O']\n\nprint(\"Total Number of Numerical Columns : \",len(numerical_features), '\\n')\nprint(\"Number of discrete features : \",len(discrete_features), '\\n')\nprint(\"No of continuous features are : \", len(continuous_features), '\\n')\nprint(\"Number of discrete features : \",len(categorical_features), '\\n')","ca8aaae1":"# combined train and test datasets\ncombined_df = pd.concat([training_set,test_set],axis=0)\n\ncombined_df[\"Label\"] = \"test\"\ncombined_df[\"Label\"][:len(training_set)] = \"train\"","c4ea2ea2":"fig, axes = plt.subplots(3, 6, figsize = (30, 15), sharex= False)\nfor i, feature in enumerate(discrete_features):\n    sns.histplot(data= combined_df, x= feature, hue= 'Label', ax= axes[i%3, i\/\/3])\nfig.savefig(\"Distribution Comparison - Discrete\", dpi= 300)","a91eb451":"fig, axes = plt.subplots(4, 6, figsize = (30, 15), sharex= False, dpi= 200)\nfor i, feature in enumerate(continuous_features):\n    sns.histplot(data= combined_df, x= feature, hue= 'Label', ax= axes[i%4, i\/\/4])\nfig.savefig(\"Distribution Comparison - Continuous\", dpi= 300)","eab73b5a":"fig, axes = plt.subplots(7, 6, figsize = (30, 30), sharex= False, dpi= 300)\nfor i, feature in enumerate(numerical_features):\n    sns.scatterplot(data= combined_df, x= feature, y= 'SalePrice', ax= axes[i%7, i\/\/7])\nfig.savefig(\"linearity check\", dpi= 300)","4a7247fd":"fig, axes = plt.subplots(7, 7, figsize = (30, 30), sharex= False, dpi= 300)\nfor i, feature in enumerate(categorical_features):\n    sns.countplot(data= combined_df, x= feature, hue=\"Label\", ax= axes[i%7, i\/\/7])\nfig.savefig(\"Distribution Comparison - Categorical\", dpi= 300)","f4e6e884":"# categorical_features.stack().value_counts()\nfig, axes = plt.subplots(7, 7, figsize = (30, 30), sharex= False, dpi= 300)\nfor i, feature in enumerate(categorical_features):\n    sort_list = sorted(combined_df.groupby(feature)['SalePrice'].median().items(), key= lambda x:x[1], reverse= True)\n    order_list = [x[0] for x in sort_list ]\n    sns.boxplot(data = combined_df, x = feature, y = 'SalePrice', order= order_list, ax= axes[i%7, i\/\/7])\nplt.show()\nfig.savefig(\"Distribution Comparison - Categorical - BoxPlot\", dpi= 300)","30c75f96":"# check the normal distribution of columns having null values by filling with the mean value\nnull_features_numerical = [col for col in combined_df.columns if combined_df[col].isnull().sum() > 0 and col not in categorical_features]\nplt.figure(figsize=(30,20))\nsns.set()\n\nwarnings.simplefilter(\"ignore\")\n\nfor i,var in enumerate(null_features_numerical):\n    plt.subplot(4,3,i+1)\n    sns.distplot(combined_df[var],bins=20,kde_kws={'linewidth':3,'color':'red'},label=\"original\")\n    sns.distplot(combined_df[var],bins=20,kde_kws={'linewidth':2,'color':'yellow'},label=\"mean\")","d1481d99":"plt.figure(figsize=(30,20))\nsns.set()\nwarnings.simplefilter(\"ignore\")\nfor i,var in enumerate(null_features_numerical):\n    plt.subplot(4,3,i+1)\n    sns.distplot(combined_df[var],bins=20,kde_kws={'linewidth':3,'color':'red'},label=\"original\")\n    sns.distplot(combined_df[var],bins=20,kde_kws={'linewidth':2,'color':'yellow'},label=\"median\")","f0d5a3bd":"# show the counts of observations in each category in columns with missings\nnull_features_categorical = [col for col in combined_df.columns if combined_df[col].isnull().sum() > 0 and col not in numerical_features]\n# for i, var in enumerate(null_features_categorical):\n# #     print(combined_df[var].value_counts())\n#     print(combined_df.groupby([var]).size())\n\n\nwarnings.simplefilter(\"ignore\")\n\nfig, axes = plt.subplots(5, 5, figsize = (30, 30), sharex= False, dpi= 300)\nfor var, subplot in zip(null_features_categorical, axes.flatten()):\n    new = combined_df.groupby([var]).size().reset_index(name=\"count\")\n    sns.countplot(data= combined_df[null_features_categorical], x= var, ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\nplt.show()\nfig.savefig(\"Distribution Comparison - Categorical - CountPlot\", dpi= 300)","e586651f":"# variables which contain year information\nyear_feature = [col for col in combined_df.columns if 'Yr' in col or 'Year' in col]\nyear_feature","450288e9":"combined_df.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('House Price')\nplt.title('House price vs YearSold')","4b505511":"for feature in year_feature:\n  if feature != 'YrSold':\n    hs = combined_df.copy()\n    hs[feature] = hs['YrSold'] - hs[feature]\n    plt.scatter(hs[feature],hs['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","1b25fa3f":"training_corr = training_set.corr(method='spearman')\nplt.figure(figsize=(20,10))\nsns.heatmap(training_corr, cmap=\"YlGnBu\", linewidths=.5)","91f1c371":"drop_columns = [\"Id\", \"Alley\", \"Fence\", \"LotFrontage\", \"FireplaceQu\", \"PoolArea\", \"LowQualFinSF\", \"3SsnPorch\", \"MiscVal\", 'RoofMatl','Street','Condition2','Utilities','Heating','Label']\n#  Drop columns\nprint(\"Number of columns before dropping : \",len(combined_df.columns))\nprint(\"Number of dropping columns : \",len(drop_columns))\ncombined_df.drop(columns=drop_columns, inplace=True, errors='ignore')\nprint(\"Number of columns after dropping : \",len(combined_df.columns))","47ba1c07":"## Temporal Variables (Date Time Variables)\n\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n\n    combined_df[feature]=combined_df['YrSold']-combined_df[feature]\n\ncombined_df[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","ffdf4eed":"for col in null_features_numerical:\n    if col not in drop_columns:    \n#         combined_df[col] = combined_df[col].fillna(combined_df[col].mean())\n        combined_df[col] = combined_df[col].fillna(0.0)","215d6379":"null_features_categorical = [col for col in combined_df.columns if combined_df[col].isna().sum() > 0 and col in categorical_features]\ncat_feature_mode = [\"SaleType\", \"Exterior1st\", \"Exterior2nd\", \"KitchenQual\", \"Electrical\", \"Functional\"]\n\nfor col in null_features_categorical:\n    if col != 'MSZoning' and col not in cat_feature_mode:\n        combined_df[col] = combined_df[col].fillna('NA')\n    else:\n        combined_df[col] = combined_df[col].fillna(combined_df[col].mode()[0])","f6251a9e":"# Convert \"numerical\" feature to categorical\nconvert_list = ['MSSubClass']\nfor col in convert_list: \n    combined_df[col] = combined_df[col].astype('str')","1b2f1c11":"from sklearn.preprocessing import PolynomialFeatures, RobustScaler, PowerTransformer, LabelEncoder\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LassoCV, RidgeCV","f8fd62c5":"x_train1,x_test1,y_train1,y_test1=train_test_split(X_train,y_train,test_size=0.25)\n\nreg= LazyRegressor(verbose=0,ignore_warnings=True,custom_metric=None)\ntrain,test=reg.fit(x_train1,x_test1,y_train1,y_test1)\ntest","9845b751":"RANDOM_SEED = 101\n\n# 10-fold CV\nkfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)","be98fb51":"## **Step 6 : Model Development**","64388dd6":"The **dendrogram** uses a hierarchical clustering algorithm (courtesy of scipy) to bin variables against one another by their nullity correlation (measured in terms of binary distance). At each step of the tree the variables are split up based on which combination minimizes the distance of the remaining clusters. The more monotone the set of variables, the closer their total distance is to zero, and the closer their average distance (the x-axis) is to zero.\n\nTo interpret this graph, read it from a right-left perspective. **Cluster leaves which linked together at a distance of zero fully predict one another's presence\u2014one variable might always be empty when another is filled, or they might always both be filled or both empty, and so on. In this specific example the dendrogram glues together the variables which are required and therefore present in every record.**\n\nCluster leaves which split close to zero, but not at it, predict one another very well, but still imperfectly. If your own interpretation of the dataset is that these columns actually are or ought to be match each other in nullity (for example, as **POLLQC and MISCFEATURE ought to), then the width of the cluster leaf tells you, in absolute terms, how often the records are \"mismatched\" or incorrectly filed\u2014that is, how many values you would have to fill in or drop, if you are so inclined.**\n\n**As with matrix, only up to 50 labeled columns will comfortably display in this configuration. However the dendrogram more elegantly handles extremely large datasets by simply flipping to a horizontal configuration.**","fb41b481":"**Identifying the features or columns**","f2d8ae71":"#### **5.2. Temporal Variable Change**","4262edee":"#### **4.3.3.3. Linearity Check**","4e6dad41":"# House Prices - Advanced Regression Techniques\n**Predict sales prices and practice feature engineering, RFs, and gradient boosting**\n\n## **Competition Description**\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n## **Evaluation**\n\n### **Goal**\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable.\n\n### **Metric**\nSubmissions are evaluated on **Root-Mean-Squared-Error (RMSE)** between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n\n### **Submission File Format**\n\nThe file should contain a header and have the following format:\n\n    Id,SalePrice\n    1461,169000.1\n    1462,187724.1233\n    1463,175221\n    etc.\n\n## **Data Description**\n\n### **File descriptions**\n\n* train.csv - the training set\n* test.csv - the test set\n* data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n* sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\n### **Data fields**\n\nHere's a brief version of what you'll find in the data description file.\n\n*     SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n*    MSSubClass: The building class\n*    MSZoning: The general zoning classification\n*    LotFrontage: Linear feet of street connected to property\n*    LotArea: Lot size in square feet\n*    Street: Type of road access\n*    Alley: Type of alley access\n*    LotShape: General shape of property\n*    LandContour: Flatness of the property\n*    Utilities: Type of utilities available\n*    LotConfig: Lot configuration\n*    LandSlope: Slope of property\n*    Neighborhood: Physical locations within Ames city limits\n*    Condition1: Proximity to main road or railroad\n*    Condition2: Proximity to main road or railroad (if a second is present)\n*    BldgType: Type of dwelling\n*    HouseStyle: Style of dwelling\n*    OverallQual: Overall material and finish quality\n*    OverallCond: Overall condition rating\n*    YearBuilt: Original construction date\n*    YearRemodAdd: Remodel date\n*    RoofStyle: Type of roof\n*    RoofMatl: Roof material\n*    Exterior1st: Exterior covering on house\n*    Exterior2nd: Exterior covering on house (if more than one material)\n*    MasVnrType: Masonry veneer type\n*    MasVnrArea: Masonry veneer area in square feet\n*    ExterQual: Exterior material quality\n*    ExterCond: Present condition of the material on the exterior\n*    Foundation: Type of foundation\n*    BsmtQual: Height of the basement\n*    BsmtCond: General condition of the basement\n*    BsmtExposure: Walkout or garden level basement walls\n*    BsmtFinType1: Quality of basement finished area\n*    BsmtFinSF1: Type 1 finished square feet\n*    BsmtFinType2: Quality of second finished area (if present)\n*    BsmtFinSF2: Type 2 finished square feet\n*    BsmtUnfSF: Unfinished square feet of basement area\n*    TotalBsmtSF: Total square feet of basement area\n*    Heating: Type of heating\n*    HeatingQC: Heating quality and condition\n*    CentralAir: Central air conditioning\n*    Electrical: Electrical system\n*    1stFlrSF: First Floor square feet\n*    2ndFlrSF: Second floor square feet\n*    LowQualFinSF: Low quality finished square feet (all floors)\n*    GrLivArea: Above grade (ground) living area square feet\n*    BsmtFullBath: Basement full bathrooms\n*    BsmtHalfBath: Basement half bathrooms\n*    FullBath: Full bathrooms above grade\n*    HalfBath: Half baths above grade\n*    Bedroom: Number of bedrooms above basement level\n*    Kitchen: Number of kitchens\n*    KitchenQual: Kitchen quality\n*    TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n*    Functional: Home functionality rating\n*    Fireplaces: Number of fireplaces\n*    FireplaceQu: Fireplace quality\n*    GarageType: Garage location\n*    GarageYrBlt: Year garage was built\n*    GarageFinish: Interior finish of the garage\n*    GarageCars: Size of garage in car capacity\n*    GarageArea: Size of garage in square feet\n*    GarageQual: Garage quality\n*    GarageCond: Garage condition\n*    PavedDrive: Paved driveway\n*    WoodDeckSF: Wood deck area in square feet\n*    OpenPorchSF: Open porch area in square feet\n*    EnclosedPorch: Enclosed porch area in square feet\n*    3SsnPorch: Three season porch area in square feet\n*    ScreenPorch: Screen porch area in square feet\n*    PoolArea: Pool area in square feet\n*    PoolQC: Pool quality\n*    Fence: Fence quality\n*    MiscFeature: Miscellaneous feature not covered in other categories\n*    MiscVal: Value of miscellaneous feature\n*    MoSold: Month Sold\n*    YrSold: Year Sold\n*    SaleType: Type of sale\n*    SaleCondition: Condition of sale\n\n\n## **Table of Content:**\n\n1. Install & Import Libraries\n2. Fetch Dataset\n3. Data Overview\n4. Exploratory Data Analysis (EDA)\n5. Feature Engineering\n6. Model Development\n7. Find Prediction","d7ab956d":"**Identifying the data types of features**","c110dd0e":"**Here, we could see that sale prices for 'Fa' & 'Po' in 'HeatingQC', 'FireplaceQu', 'GarageQual' and 'GarageCond' are similar, so we can combine these items.**\n","8ba2df75":"> * **The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another.**","7b7f0ffd":"#### **5.3.1. Fill Missing Values - Numerical Feature**","de867568":"## **Step 5 : Feature Engineering (Determine the Features and Target Variable (Label))**","dde7349c":"## **Step4 : EDA (Exploratory Data Analysis)**\n\nExploratory Data Analysis or EDA is the first and foremost of all tasks that a dataset goes through. EDA lets us understand the data and thus helping us to prepare it for the upcoming tasks.\n\nSome of the key steps in EDA are identifying the features, a number of observations, checking for null values or empty cells etc.","7ae9fac1":"## **Step3 : Data overview**","4be73417":"#### **4.4.3. Find Suitable value for missing values - Numerical**","468c75f7":"Check is there any relation between \"Year Sold\" and \"Sales price\"","3147d836":"#### **4.1.1. Visualize missing value using Misingno and Seaborn - Train Data**","c83821ca":"#### **4.3.3.2. Distribution Comparison - Continuous**","b4e40176":"#### **4.3.3.4. Distribution Comparison - Categorical**","c9f4ba0a":"#### **4.4.2.1. A Simple Option: Drop Columns with Missing Values**\n\nIf your data is in a DataFrame called original_data, you can drop columns with missing values. One way to do that is\n\n    data_without_missing_values = original_data.dropna(axis=1)\n\nIn many cases, you'll have both a training dataset and a test dataset. You will want to drop the same columns in both DataFrames. In that case, you would write\n\n\n    cols_with_missing = [col for col in original_data.columns \n                                 if original_data[col].isnull().any()]\n                                 \n    reduced_original_data = original_data.drop(cols_with_missing, axis=1)\n    reduced_test_data = test_data.drop(cols_with_missing, axis=1)\n\n\nIf those columns had useful information (in the places that were not missing), your model loses access to this information when the column is dropped. Also, if your test data has missing values in places where your training data did not, this will result in an error.\n\nSo, it's somewhat usually not the best solution. However, it can be useful when most values in a column are missing.\n\n**Pros:**\n\n*     Complete removal of data with missing values results in robust and highly accurate model\n*     Deleting a particular row or a column with no specific information is better, since it does not have a high weightage\n\n**Cons:**\n\n*     Loss of information and data\n*     Works poorly if the percentage of missing values is high (say 30%), compared to the whole dataset","568c1b96":"#### **5.3.2. Fill Missing Values - Categorical Feature**","904bf887":"**Identifying the number of observations**","49847905":"* From the above visualization we saw that mean and median value both maintain the same destribution. So we can choose one of them to fill the missing values.","cdc5ca4f":"#### **4.2.1. Visualize missing value using Misingno and Seaborn - Test Data**","f72fc2ec":"#### **6.1. Find best algorithms using LazyPredict**","24c2e651":"**Determine columns with missings**","df073431":"#### **5.1. Drop Columns**\n\nHere we'll drop columns like\n\n* **ID**\n* **Column having more missing value**\n* **Column dominated by 0\/null or single value**","7b2c7e60":"Here we'll see the linearity between all features and the target variable.","1db07525":"## **4.2. Missing Value - Test Data**","82cdedac":"#### **4.4.2. Solutions**","57779841":"#### **5.5. Apply PowerTransformer to columns**\n\n* I saw in distribution of continuous features that some features are not linear towards target feature. So i need to transform this.\n* Lets check the skewness of all distributions","b6a86a8b":"All the above code blocks can be used as templates for any datasets.","cd8576b2":"**Checking if the dataset has empty cells or samples**","25a1f326":"**Identifying the number of features or columns**","3c0af7c4":"#### **4.3.2. Missing Value Comparison**","1ba94bfb":"#### **4.3.3. Distribution Comparison**","e8f1d25f":"## **Step1 : Import and Install all necessary libraries**","f83a1be6":"## **4.4. Find Suitable value for missing values**\n\nIn this step, you will learn three approaches to dealing with missing values. You will then learn to compare the effectiveness of these approaches on any given dataset.","fe5955b8":"* **Here We can see some columns have inconsistent data types i.e int64 & float64. It's not a problem.**","dbe2a433":"#### **4.4.6. Data Correlation**","2cc02717":"**Identifying the number of empty cells by features or columns**","744fffdc":"> * **The dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:**","79f4fe9c":"**Above distribution shows that:**\n\n*     **The distribution of train and test data are similar for most continous features.**","c24b1d74":"#### **4.4.1. Introduction**\n\nThere are many ways data can end up with missing values. For example\n\n*     **A 2 bedroom house wouldn't include an answer for How large is the third bedroom**\n*     **Someone being surveyed may choose not to share their income**\n\nPython libraries represent missing numbers as nan which is short for \"not a number\". You can detect which cells have missing values, and then count how many there are in each column with the command:\n\n\n    missing_val_count_by_column = (data.isnull().sum())\n    print(missing_val_count_by_column[missing_val_count_by_column] > 0)\n\n\nMost libraries (including scikit-learn) will give you an error if you try to build a model using data with missing values. So you'll need to choose one of the strategies below.","0e9fb418":"**Above distribution shows that:**\n\n*     **The distribution of train and test data are similar for most categorical features.**\n*     **Some features have dominant items, we can combine some minor items into a group otherwise we can drop these columns.**\n*     **Ex: 'RoofMatl','Street','Condition2','Utilities','Heating' (These columns should be dropped)**\n*     **Ex: 'Fa' & 'Po' in 'HeatingQC', 'FireplaceQu', 'GarageQual' and 'GarageCond'**\n\n**Now let's conform that the items we want to combine has similar prices(SalePrices value).**","2f54b50d":"#### **4.4.3.1. Fill Mean Value**","4d30647e":"#### **4.4.5. Temporal Variable Analysis**\n\n","2ced2f20":"#### **4.4.4. Find Suitable value for missing values - Categorical**","1006e67c":"Here we'll compare below things between train and test dataset.\n\n*     Data Type\n*     Missing values\n*     Data Distribution","7b9d453d":"**The popular methods which are used by the machine learning community to handle the missing value for categorical variables in the dataset are as follows:**\n\n**1. Delete the observations:** If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model.\n\n**2. Replace missing values with the most frequent value:** You can always impute them based on Mode in the case of categorical variables, just make sure you don\u2019t have highly skewed class distributions.\n\n**NOTE:** But in some cases, this strategy can make the data **imbalanced** wrt classes if there are a huge number of missing values present in our dataset.\n\n**3. Develop a model to predict missing values:** One smart way of doing this could be training a classifier over your columns with missing values as a dependent variable against other features of your data set and trying to impute based on the newly trained classifier.\n\n**4. Deleting the variable:** If there are an exceptionally larger set of missing values, try excluding the variable itself for further modeling, but you need to make sure that it is not much significant for predicting the target variable i.e, Correlation between dropped variable and target variable is very low or redundant.\n\n**5. Apply unsupervised Machine learning techniques:** In this approach, we use unsupervised techniques like K-Means, Hierarchical clustering, etc. The idea is that you can skip those columns which are having missing values and consider all other columns except the target column and try to create as many clusters as no of independent features(after drop missing value columns), finally find the category in which the missing row falls.\n","a27d32ce":"#### **4.4.2.2. A Better Option: Imputation**\n\nImputation is the process of substituting the missing data by some statistical methods. Imputation is useful in the sense that it preserves all cases by replacing missing data with an estimated value based on other available information. But imputation methods should be used carefully as most of them introduce a large amount of bias and reduce variance in the dataset.\n\nImputation fills in the missing value with some number. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.\n\nThis is done with\n\n    from sklearn.impute import SimpleImputer\n    my_imputer = SimpleImputer()\n    data_with_imputed_values = my_imputer.fit_transform(original_data)\n\n**Imputation by Mean\/Mode\/Median**\n\nIf the missing values in a column or feature are numerical, the values can be imputed by the mean of the complete cases of the variable. Mean can be replaced by median if the feature is suspected to have outliers. For a categorical feature, the missing values could be replaced by the mode of the column. The major drawback of this method is that it reduces the variance of the imputed variables. This method also reduces the correlation between the imputed variables and other variables because the imputed values are just estimates and will not be related to other values inherently.\n\nThe default behavior fills in the mean value for imputation. Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models.\n\nOne (of many) nice things about Imputation is that it can be included in a scikit-learn Pipeline. Pipelines simplify model building, model validation and model deployment.\n\n**Pros:**\n\n*     This is a better approach when the data size is small\n*     It can prevent data loss which results in removal of the rows and columns\n\n**Cons:**\n\n*     Imputing the approximations add variance and bias\n*     Works poorly compared to other multiple-imputations method","8447ac68":"**Here we'll see how the temporal variables(Year features) affect to House Price**","2344e0f0":"**We notice that some features are not linear towards target feature.**\n\n*     **'SalePrice' VS.'LotFrontage',**\n*     **'SalePrice' VS.'GrLivArea',**\n*     **'SalePrice' VS.'BsmtUnfSF',**\n*     **'SalePrice' VS.'LotArea',**\n*     **'SalePrice' VS.'TotalBsmtSF',**\n*     **'SalePrice' VS.'GarageArea',**\n*     **'SalePrice' VS.'1stFlrSF',**","2cef7ee0":"Import preprocessing package from sklearn for polynomial regression model","87dba210":"#### **4.3.1. Data Type Comparison**","3fa3a549":"## **Step2 : Fetch the dataset**\n\nWe now have two data frames, one consisting of the data to be trained and the other for predicting the target value which in this case is the price of the house.","01734651":"**Above distribution shows that:**\n\n*     **Some features can be reclassified as 'Categorical', such as 'MSSubClass'.**\n*     **Some features are dominated by 0\/null (eg:PoolArea, LowQualFinSF, 3SsnPorch, MiscVal ), thus we can consider to drop.**","7d80f50c":"> * **The \"msno.matrix\" nullity matrix is a data-dense display which lets you quickly visually pick out patterns in data completion.**","6acc2d99":"#### **4.3.3.1. Distribution Comparison - Discrete**","6bbf1892":"#### **4.4.3.2. Fill Median Value**","402b2963":"#### **6.3. Ridge Regression**","dcfb1640":"## **4.3. Train & Test Data Comparison**","ed8f5ede":"* **Here we can see that columns like \"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"LotFrontage\", \"FireplaceQu\" have maximum number of null value. So we will consider to drop these columns.**","17a6db85":"#### **4.4.2.3. An Extension To Imputation**\n\nImputation is the standard approach, and it usually works well. However, imputed values may by systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. Here's how it might look:\n\n    # make copy to avoid changing original data (when Imputing)\n    new_data = original_data.copy()\n\n    # make new columns indicating what will be imputed\n    cols_with_missing = (col for col in new_data.columns \n                                 if new_data[col].isnull().any())\n    for col in cols_with_missing:\n        new_data[col + '_was_missing'] = new_data[col].isnull()\n\n    # Imputation\n    my_imputer = SimpleImputer()\n    new_data = pd.DataFrame(my_imputer.fit_transform(new_data))\n    new_data.columns = original_data.columns\n\nIn some cases this approach will meaningfully improve results. In other cases, it doesn't help at all.","648af1fa":"Here we can see which algorithms give best accuracy in less time.\n\n* **Lasso**\n* **Ridge**","ffc61f9e":"## **4.1. Missing Value - Train Data**","593caacb":"#### **5.4. Convert Numerical feature to Categorical**"}}