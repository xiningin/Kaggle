{"cell_type":{"07b6e0cd":"code","0293b993":"code","9b0f68a5":"code","6bde2394":"code","363e5e8a":"code","865d90fe":"code","eee123e4":"code","5dce840c":"code","0422ff40":"code","8b677324":"code","2cc12cb6":"code","451284e3":"code","a04eb972":"code","c5df5c0f":"code","bd5184e0":"code","68db297a":"code","203179b3":"code","21101f81":"code","39f43c52":"code","c3029646":"code","eedb69bf":"code","ba5e3c5c":"code","b4c7bad8":"code","6135f2db":"code","87106ddb":"code","49e155f2":"code","abdef9cc":"code","5663b69a":"code","0cf37c7f":"code","af8977c6":"code","dd176537":"code","852ad24a":"code","a9406e2e":"code","f53d6212":"code","1609f797":"code","0e50c2f2":"code","779162fa":"code","7eeb8f52":"code","5bfd337a":"code","9a619a96":"code","8da67967":"code","c3069be4":"code","4bba4773":"code","fe9d3ccc":"code","22e2baac":"code","b1184af4":"markdown","a056f2d8":"markdown","0f5f3fa8":"markdown","18e5e36c":"markdown","f177176f":"markdown","8c040fb3":"markdown","51f155ef":"markdown","6e630fae":"markdown","fee55f0c":"markdown","dc75ecc9":"markdown","cd238c34":"markdown","84f04ac3":"markdown","9fc8cfc7":"markdown","450ba110":"markdown","168d4c31":"markdown","5e340f18":"markdown","084cb396":"markdown","f263c89f":"markdown","08d6f160":"markdown","2ba24e03":"markdown","8d786dc4":"markdown","71ed8dda":"markdown","fe7021c5":"markdown"},"source":{"07b6e0cd":"import numpy as np \nimport pandas as pd\nimport cv2\nimport os\nimport time\nimport shutil\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras \nfrom keras.applications import VGG16\nfrom keras.applications import DenseNet121\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.applications.vgg16 import preprocess_input as preprocess_input_vgg\nfrom keras.applications.densenet import preprocess_input as preprocess_input_densenet\nfrom keras.layers.experimental.preprocessing import Rescaling\nfrom keras.layers import Input, Dense, BatchNormalization, GlobalAveragePooling2D, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nimport pickle","0293b993":"data_dir = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset'\ncovid_image_dir = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/COVID'\nnon_covid_image_dir = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/Normal'","9b0f68a5":"print(f'Number of COVID Images: {len(os.listdir(covid_image_dir))} \\nNumber of Non-COVID Images: {len(os.listdir(non_covid_image_dir))}')","6bde2394":"def resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https:\/\/www.kaggle.com\/xhlulu\/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return np.array(im).reshape(size,size,3)","363e5e8a":"covid_example = resize(cv2.imread(os.path.join(covid_image_dir,os.listdir(covid_image_dir)[0])),224)\nnon_covid_example = resize(cv2.imread(os.path.join(non_covid_image_dir,os.listdir(non_covid_image_dir)[0])),224)\n\nfig, axs = plt.subplots(nrows=1,ncols=2,figsize=[10,10])\nfig.tight_layout()\naxs[0].imshow(covid_example)\naxs[0].set_title('COVID')\naxs[0].axis('off')\naxs[1].imshow(non_covid_example)\naxs[1].set_title('Non-COVID')\naxs[1].axis('off')\nfig.suptitle('X-Rays')\nplt.show()","865d90fe":"def flip(img):\n    return cv2.flip(img,1)","eee123e4":"def rotate(img, rotation_range=20):\n    return tf.keras.preprocessing.image.random_rotation(img,rotation_range,channel_axis=2)","5dce840c":"def shear(img, shear_range= 20):\n    return tf.keras.preprocessing.image.random_shear(img, shear_range,channel_axis=2)","0422ff40":"def zoom(img, zoom_range=0.75):\n    return tf.keras.preprocessing.image.random_zoom(img, (zoom_range,zoom_range),channel_axis=2)","8b677324":"from scipy import signal\nfrom scipy import ndimage\n\nimport copy\nimport warnings\n\ndef histogram_equalization(image_data):\n    if type(image_data) is not np.ndarray:\n        warnings.warn('Should be a numpy array')\n    shape = image_data.shape\n    _histogram, _bins = np.histogram(image_data.flatten(), 256, density=True)\n    cdf = _histogram.cumsum()\n    # Now we normalize the cdf\n    cdf = 255 * cdf \/ cdf[-1]\n    image_data = np.interp(image_data.flatten(), _bins[:-1], cdf)\n    return image_data.reshape(shape)\n\n\ndef histogram_s_curve(image_data):\n    image_data = histogram_equalization(image_data)\n    shape = image_data.shape\n    _histogram, _bins = np.histogram(image_data.flatten(), 256, density=True)\n    cdf = 1 \/ (1 + np.exp((1 \/ 25) * (158 - _bins)))\n    # print('hiu', new_curve.max())\n    # cdf = new_curve.cumsum()\n    # plt.plot(cdf)\n    # plt.show()\n    cdf = 255 * cdf \/ cdf[-1]\n    image_data = np.interp(image_data.flatten(), _bins, cdf)\n    return image_data.reshape(shape)\n\n\ndef find_shoulders(image_data, center=None, spine_start=None, spine_end=None, should_plot=False):\n    min_height_of_area_to_check = 0\n    max_height_of_area_to_check = 125\n    threshold = 64\n    sigma = 1\n    buffer_to_ignore = 10\n\n    if (center is None) or (spine_start is None) or (spine_end is None):\n        warnings.warn('Information not provided so find_spine may be running twice')\n        center, spine_start, spine_end = find_spine(image_data, False)\n    _image_data = copy.deepcopy(image_data)\n    _image_data[_image_data <= threshold] = 0\n    _image_data[_image_data > threshold] = 255\n    # Gaussian filter this\n    _image_data = ndimage.gaussian_filter(_image_data, sigma=sigma)\n    _image_data = np.transpose(ndimage.prewitt(np.transpose(_image_data)))\n#     plt.imshow(_image_data, cmap='gray')\n#     plt.show()\n    _image_data = np.concatenate((_image_data[:, :int(spine_start - buffer_to_ignore)],\n                                  _image_data[:, int(spine_end + buffer_to_ignore):]), axis=1)\n    energies = np.sum(_image_data, axis=1)\n    if should_plot:\n        plt.plot(energies)\n        plt.show()\n\n    maximum_index, info = signal.find_peaks(energies[min_height_of_area_to_check:max_height_of_area_to_check],\n                                            distance=max_height_of_area_to_check)\n    return maximum_index[0]\n\n\ndef find_spine(image_data, should_plot=False):\n    smoothness = 50  # pixels\n    width_of_area_to_check = 300  # pixels\n    interval = 1  # pixels\n\n    start_indexes = np.arange(int((image_data.shape[1] - width_of_area_to_check) \/ 2),\n                              int((image_data.shape[1] + width_of_area_to_check) \/ 2), interval)\n    end_indexes = start_indexes + smoothness\n    energies = start_indexes * 0\n    for i in range(start_indexes.shape[0]):\n        start_index = start_indexes[i]\n        end_index = end_indexes[i]\n        energy = np.sum(np.square(image_data[:, start_index:end_index]))\n        energies[i] = energy\n\n    if should_plot:\n        plt.plot(start_indexes, energies)\n        plt.show()\n\n    maximum_index, info = signal.find_peaks(energies, width=10, rel_height=0.5, distance=len(energies))\n    spine_start = start_indexes[maximum_index]\n    spine_end = end_indexes[maximum_index]\n    center = int((spine_start + spine_end) \/ 2)\n    spine_start = int(center - (info['widths'] \/ 2))\n    spine_end = int(center + (info['widths'] \/ 2))\n    return center, spine_start, spine_end\n\n\ndef find_sides_of_body(image_data, shoulders_index=None, center=None, spine_start=None, spine_end=None,\n                       should_plot=False):\n    width_to_check_from_edge = 100\n    if (center is None) or (spine_start is None) or (spine_end is None):\n        warnings.warn('Information not provided so find_spine may be running twice')\n        center, spine_start, spine_end = find_spine(image_data, should_plot)\n    image_center = np.round(image_data.shape[0] \/ 2)\n    if shoulders_index is None:\n        warnings.warn('Information not provided so find_shoulders may be running twice')\n        shoulders_index = find_shoulders(image_data, center, spine_start, spine_end, should_plot)\n    _image_data = copy.deepcopy(image_data)\n    _image_data = _image_data[shoulders_index:300, :]\n    _image_data = histogram_s_curve(_image_data)\n\n    energies = np.sum(_image_data, axis=0)\n    energies = np.convolve(energies, [1 for _ in range(20)], mode='same')\n    maximum_index, info = signal.find_peaks(energies, distance=125)\n    if should_plot:\n        x1 = np.arange(-1 * center, image_data.shape[1] - center)\n        plt.plot(x1, energies)\n        plt.show()\n    try:\n        return maximum_index[0], maximum_index[2]\n    except IndexError:\n        warnings.warn('An error occurred')\n        return 0, image_data.shape[1] - 1\n    \n    \ndef my_preprocessing_function(file_in):\n    data = np.mean(file_in, axis=2)\n    data = histogram_equalization(data)\n\n    try:\n        center, spine_start, spine_end = find_spine(data, False)\n        shoulders_index = find_shoulders(data, center, spine_start, spine_end)\n        body_start, body_end = find_sides_of_body(data, shoulders_index, center, spine_start, spine_end)\n#         data = data[body_start:body_end, int(shoulders_index):int(400 + shoulders_index)]\n        data = data[int(shoulders_index):int(400 + shoulders_index), body_start:body_end]\n        \n        \n#         data = np.pad(data, ((0, 400 - data.shape[0]), (0, 400 - data.shape[1])), mode='constant', constant_values=0)\n    except:\n        # then just scale the image\n        #print('error')\n        #data = cv2.resize(data, (400, 400))\n        pass\n    data = cv2.resize(data, (224, 224))\n    out = np.repeat(data, 3).reshape((224, 224, 3))\n    return out","2cc12cb6":"import numpy as np\nimport copy\n\n\ndef subhist(image_pdf, minimum, maximum, normalize):\n    \"\"\"\n    Compute the subhistogram between [minimum, maximum] of a given histogram image_pdf\n    :param image_pdf: numpy.array\n    :param minimum: int\n    :param maximum: int\n    :param normalize: boolean\n    :return: numpy.array\n    \"\"\"\n    hi = np.zeros(shape=image_pdf.shape)\n    total = 0\n    for idx in range(minimum, maximum+1):\n        total += image_pdf[idx]\n        hi[idx] = image_pdf[idx]\n    if normalize:\n        for idx in range(minimum, maximum+1):\n            hi[idx] \/= total\n    return hi\n\n\ndef CDF(hist):\n    \"\"\"\n    Compute the CDF of the input histogram\n    :param hist: numpy.array()\n    :return: numpy.array()\n    \"\"\"\n    cdf = np.zeros(shape=hist.shape)\n    cdf[0] = hist[0]\n    for idx in range(1, len(hist)):\n        cdf[idx] = cdf[idx - 1] + hist[idx]\n    return cdf\n\n\ndef BEASF(image, gamma = 1.5):\n    image = np.array(image)\n    \"\"\"\n    Compute the Bi-Histogram Equalization with Adaptive Sigmoid Functions algorithm (BEASF)\n    A python implementation of the original MATLAB code:\n    https:\/\/mathworks.com\/matlabcentral\/fileexchange\/47517-beasf-image-enhancer-for-gray-scale-images\n    The algorithm is introduced by E. F. Arriaga-Garcia et al., in the research paper:\n    https:\/\/ieeexplore.ieee.org\/document\/6808563\n    :param image: numpy.ndarray\n    :param gamma: float [0, 1]\n    :return: numpy.ndarray\n    \"\"\"\n    m = int(np.mean(image, dtype=np.int32))\n    h = np.histogram(image, bins=256)[0] \/ (image.shape[0] * image.shape[1])\n    h_lower = subhist(image_pdf=h, minimum=0, maximum=m, normalize=True)\n    h_upper = subhist(image_pdf=h, minimum=m, maximum=255, normalize=True)\n\n    cdf_lower = CDF(hist=h_lower)\n    cdf_upper = CDF(hist=h_upper)\n\n    # Find x | CDF(x) = 0.5\n    half_low = 0\n    for idx in range(0, m+2):\n        if cdf_lower[idx] > 0.5:\n            half_low = idx\n            break\n    half_up = 0\n    for idx in range(m, 256):\n        if cdf_upper[idx + 1] > 0.5:\n            half_up = idx\n            break\n\n    # sigmoid CDF creation\n    tones_low = np.arange(0, m+1, 1)\n    x_low = 5.0 * (tones_low - half_low) \/ m  # shift & scale intensity x to place sigmoid [-2.5, 2.5]\n    s_low = 1 \/ (1 + np.exp(-gamma * x_low))  # lower sigmoid\n\n    tones_up = np.arange(m, 256, 1)\n    x_up = 5.0 * (tones_up - half_up) \/ (255 - m)  # shift & scale intensity x to place sigmoid [-2.5, 2.5]\n    s_up = 1 \/ (1 + np.exp(-gamma * x_up))  # upper sigmoid\n\n    mapping_vector = np.zeros(shape=(256,))\n    for idx in range(0, m+1):\n        mapping_vector[idx] = np.int32(m * s_low[idx])\n\n    minimum = mapping_vector[0]\n    maximum = mapping_vector[m]\n    for idx in range(0, m+1):\n        mapping_vector[idx] = np.int32((m \/ (maximum - minimum)) * (mapping_vector[idx] - minimum))\n    for idx in range(m+1, 256):\n        mapping_vector[idx] = np.int32(m + (255 - m) * s_up[idx - m - 1])\n\n    minimum = mapping_vector[m + 1]\n    maximum = mapping_vector[255]\n    for idx in range(m+1, 256):\n        mapping_vector[idx] = (255 - m) * (mapping_vector[idx] - minimum) \/ (maximum - minimum) + m\n\n    res = copy.deepcopy(image)\n    res[:, :] = mapping_vector[image[:, :]]\n    return res","451284e3":"def CLAHE(img): \n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n    gray = clahe.apply(gray)\n    gray = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n    '''\n    img = np.array(img)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    R, G, B = cv2.split(img)\n\n    output1_R = clahe.apply(R)\n    output1_G = clahe.apply(G)\n    output1_B = clahe.apply(B)\n\n    equ = cv2.merge((output1_R, output1_G, output1_B))\n    '''\n    return gray","a04eb972":"histogram_equalizer = lambda x: np.array(histogram_equalization(x),dtype='uint8')","c5df5c0f":"base_image = covid_example\nhistogram_equalized = histogram_equalizer(base_image)\ns_curve = np.array(histogram_s_curve(base_image),dtype='uint8')\nwhole_preprocessing = np.array(my_preprocessing_function(base_image), dtype='uint8')\nbeasf = BEASF(base_image)\nc = np.array(CLAHE(base_image))\n#aaron's preprocessing visualized\nfig, axs = plt.subplots(nrows=3,ncols=2,figsize=[10,10])\nfig.tight_layout()\naxs[0,0].imshow(base_image)\naxs[0,0].set_title('Unaugmented Image')\naxs[0,0].axis('off')\n\naxs[0,1].imshow(histogram_equalized)\naxs[0,1].set_title('Histogram Equalized')\naxs[0,1].axis('off')\n\naxs[1,0].imshow(s_curve)\naxs[1,0].set_title('Histogram S Curve')\naxs[1,0].axis('off')\n\naxs[1,1].imshow(whole_preprocessing)\naxs[1,1].set_title('My Preprocessing Function')\naxs[1,1].axis('off')\n\naxs[2,0].imshow(beasf)\naxs[2,0].set_title('BEASF')\naxs[2,0].axis('off')\n\naxs[2,1].imshow(c)\naxs[2,1].axis('off')\n\nplt.show()","bd5184e0":"base_image = covid_example\nflipped_image = flip(base_image)\nrotated_image = rotate(base_image)\nsheared_image = shear(base_image)\nzoomed_image = zoom(base_image)\n\nw , h = 10, 10\nnrows, ncols = 1 , 5\nfigsize = [h,w]\nfig, axs  = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\nfig.tight_layout()\n\naxs[0].imshow(flipped_image)\naxs[0].set_title('Flipped Image', fontsize=10)\naxs[0].axis('off')\n\naxs[1].imshow(rotated_image)\naxs[1].set_title('Rotated Image', fontsize=10)\naxs[1].axis('off')\n\naxs[2].imshow(base_image)\naxs[2].set_title('Unaugmented Image', fontsize=10)\naxs[2].axis('off')\n\naxs[3].imshow(sheared_image)\naxs[3].set_title('Sheared Image', fontsize=10)\naxs[3].axis('off')\n\naxs[4].imshow(zoomed_image)\naxs[4].set_title('Zoomed Image', fontsize=10)\naxs[4].axis('off')\n\nplt.subplots_adjust(wspace=0.2,hspace=0)\nfig.suptitle('Image Augmentations', fontsize=20)\nplt.show()","68db297a":"labels = np.concatenate((np.repeat('COVID',len(os.listdir(covid_image_dir))),np.repeat('Normal',len(os.listdir(non_covid_image_dir)))))\nimage_files = np.concatenate((os.listdir(covid_image_dir),os.listdir(non_covid_image_dir)))\ndf = pd.DataFrame({'image_id':image_files,'class_id':labels})\ndf.head()","203179b3":"from sklearn.model_selection import train_test_split\ntrain,validation = train_test_split(df.index,stratify=df.class_id, test_size = 0.10)\ntrain_df = df.loc[train]\nvalidation_df = df.loc[validation]","21101f81":"print(f'Size of training set is: {train_df.shape[0]} images')\ntrain_df.class_id.value_counts(normalize=True)","39f43c52":"print(f'Size of validationset is: {validation_df.shape[0]} images')\nvalidation_df.class_id.value_counts(normalize=True)","c3029646":"os.makedirs('\/kaggle\/working\/train\/COVID', exist_ok = True)\nos.makedirs('\/kaggle\/working\/train\/Normal', exist_ok = True)\nos.makedirs('\/kaggle\/working\/validation\/COVID', exist_ok = True)\nos.makedirs('\/kaggle\/working\/validation\/Normal', exist_ok = True)\n\nfor i,row in tqdm(train_df.iterrows()):\n    path = os.path.join(data_dir,row.class_id,row.image_id)\n    img = cv2.imread(path)\n    img = my_preprocessing_function(img)\n    new_path = os.path.join('\/kaggle\/working\/train', row.class_id, row.image_id)\n    cv2.imwrite(new_path,img)\n    #shutil.copy(path,new_path)\n    \nfor i,row in tqdm(validation_df.iterrows()):\n    path = os.path.join(data_dir,row.class_id,row.image_id)\n    img = cv2.imread(path)\n    img = my_preprocessing_function(img)\n    new_path = os.path.join('\/kaggle\/working\/validation', row.class_id, row.image_id)\n    cv2.imwrite(new_path,img)\n    #shutil.copy(path,new_path)","eedb69bf":"from keras.preprocessing.image import ImageDataGenerator\ndef process_data(batch_size = 32, image_dim = 224):\n    train_gen = ImageDataGenerator(rescale=1\/256, rotation_range = 0.05,shear_range = 0.05, zoom_range=0.05,vertical_flip=True, horizontal_flip=True)\n    validation_gen = ImageDataGenerator(rescale = 1\/256)\n    \n    train_gen = train_gen.flow_from_directory(\n    directory = '\/kaggle\/working\/train',\n    batch_size = batch_size,\n    class_mode = 'binary',\n    shuffle = True,\n    target_size = (image_dim,image_dim)\n    )\n    \n    validation_gen = validation_gen.flow_from_directory(\n    directory = '\/kaggle\/working\/validation',\n    batch_size = batch_size,\n    class_mode = 'binary',\n    shuffle = True,\n    target_size = (image_dim,image_dim)\n    )\n    \n    return train_gen, validation_gen\nvgg_train_gen , vgg_validation_gen = process_data()","ba5e3c5c":"image_generator1 = ImageDataGenerator(rescale=1\/256, preprocessing_function=None, horizontal_flip=False)\nimage_generator2 = ImageDataGenerator(rescale=1\/256, preprocessing_function = None)\n\n\ntrain_gen = image_generator1.flow_from_directory(batch_size=32,\n                                                    directory=r'\/kaggle\/working\/train',\n                                                    shuffle=True,\n                                                    target_size=(512, 512),\n                                                    class_mode='binary')\n\nvalidation_gen = image_generator2.flow_from_directory(batch_size=32,\n                                                   directory=r'\/kaggle\/working\/validation',\n                                                   shuffle=True,\n                                                   target_size=(512, 512),\n                                                   class_mode='binary')","b4c7bad8":"METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]","6135f2db":"URL = r'https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_152\/feature_vector\/4'\n\nfeature_extractor = hub.KerasLayer(URL, input_shape=(224,224,3))\nfeature_extractor.trainable = False\n\nlinear_model = model = tf.keras.models.Sequential([    \n    feature_extractor,\n    tf.keras.layers.Dense(1, activation='linear'), \n    tf.keras.layers.Dense(1,activation='sigmoid')])\nlinear_model.compile(Adam(lr=.001), loss = 'binary_crossentropy', metrics =[METRICS])\nlinear_model.summary()","87106ddb":"linear_checkpoint_callback = ModelCheckpoint('linear_model_augs.h5', monitor='val_accuracy', verbose = True, save_best_only = True, save_weights_only = False, mode= 'max')\nlinear_early_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=5)\n\nwith tf.device('\/GPU:0'):\n    linear_fit = linear_model.fit_generator(generator=train_gen,\n                              validation_data=validation_gen,\n                              steps_per_epoch= (train_gen.samples \/\/ 32),\n                              validation_steps = (validation_gen.samples \/\/ 32),\n                              use_multiprocessing=True,\n                              workers=-1,\n                              verbose = True,\n                              epochs = 20,\n                              callbacks = [linear_checkpoint_callback, linear_early_stopping_callback])","49e155f2":"plt.plot(np.array(linear_fit.history['loss']),color='blue')\nplt.plot(np.array(linear_fit.history['val_loss']),color='red')\nplt.title('linear model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","abdef9cc":"plt.plot(np.array(linear_fit.history['accuracy']),color='blue')\nplt.plot(np.array(linear_fit.history['val_accuracy']),color='red')\nplt.title('linear model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","5663b69a":"with open('linear_model_augs_history.pickle','wb') as file:\n    pickle.dump(linear_fit.history,file)","0cf37c7f":"URL = r'https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_152\/feature_vector\/4'\n\nfeature_extractor = hub.KerasLayer(URL, input_shape=(224,224,3))\nfeature_extractor.trainable = False\n\ndeep_model = tf.keras.models.Sequential([    \n    feature_extractor,\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')    \n])\ndeep_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=METRICS )\ndeep_model.summary()","af8977c6":"deep_checkpoint_callback = ModelCheckpoint('deep_model_augs.h5', monitor='val_accuracy', verbose = True, save_best_only = True, save_weights_only = False, mode= 'max')\ndeep_early_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=5)\n\nwith tf.device('\/GPU:0'):\n    deep_fit = linear_model.fit_generator(generator=train_gen,\n                              validation_data=validation_gen,\n                              steps_per_epoch= (train_gen.samples \/\/ 32),\n                              validation_steps = (validation_gen.samples \/\/ 32),\n                              use_multiprocessing=True,\n                              workers=-1,\n                              verbose = True,\n                              epochs = 20,\n                              callbacks = [deep_checkpoint_callback, deep_early_stopping_callback])","dd176537":"plt.plot(np.array(deep_fit.history['loss']),color='blue')\nplt.plot(np.array(deep_fit.history['val_loss']),color='red')\nplt.title('Deep model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","852ad24a":"plt.plot(np.array(deep_fit.history['accuracy']),color='blue')\nplt.plot(np.array(deep_fit.history['val_accuracy']),color='red')\nplt.title('Deep model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","a9406e2e":"with open('deep_model_augs_history.pickle','wb') as file:\n    pickle.dump(deep_fit.history,file)","f53d6212":"base_model = VGG16(weights='imagenet',include_top=False, input_shape=(224,224,3))\ninput_layer = Input(shape=(224,224,3))\nx = tf.cast(input_layer,tf.float32)\nx = preprocess_input_vgg(x)\nx = base_model(x)\nx = GlobalAveragePooling2D()(x)\nx = Dense(512,activation = 'relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,activation = 'relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128)(x)\nx = Dropout(0.5)(x)\noutput_layer = Dense(1,activation = 'sigmoid')(x) \n\nfor layer in base_model.layers[:-2]:\n    layer.trainable = False\n\n\nvgg_model = Model(inputs=[input_layer], outputs=[output_layer])\nvgg_model.compile(Adam(lr=.001), loss = 'binary_crossentropy', metrics =[METRICS])\nvgg_model.summary()","1609f797":"vgg_checkpoint_callback = ModelCheckpoint('VGG_model_augs.h5', monitor='val_accuracy', verbose = True, save_best_only = True, save_weights_only = False, mode= 'max')\nvgg_early_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=5)\nwith tf.device('\/GPU:0'):\n    vgg_fit = vgg_model.fit_generator(generator=vgg_train_gen,\n                              validation_data=vgg_validation_gen,\n                              steps_per_epoch= (train_gen.samples \/\/ 32),\n                              validation_steps = (validation_gen.samples \/\/ 32),\n                              use_multiprocessing=True,\n                              workers=-1,\n                              verbose = True,\n                              epochs = 20,\n                              callbacks = [vgg_checkpoint_callback, vgg_early_stopping_callback])","0e50c2f2":"plt.plot(np.array(vgg_fit.history['loss']),color='blue')\nplt.plot(np.array(vgg_fit.history['val_loss']),color='red')\nplt.title('VGG16 model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","779162fa":"plt.plot(np.array(vgg_fit.history['accuracy']),color='blue')\nplt.plot(np.array(vgg_fit.history['val_accuracy']),color='red')\nplt.title('VGG16 model accuracy')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","7eeb8f52":"with open('vgg_model_augs_history.pickle','wb') as file:\n    pickle.dump(vgg_fit.history,file)","5bfd337a":"base_model = DenseNet121(include_top = False, weights='imagenet', input_shape=(224,224,3))\n\ninput_layer = Input(shape=(224,224,3))\nx = preprocess_input_densenet(input_layer)\nx = base_model(x)\nx = GlobalAveragePooling2D()(x)\nx = Dense(10, activation='relu')(x)\nx = Dropout(0.2)(x)\noutput_layer = Dense(1, activation = 'sigmoid')(x)\n\nfor layer in base_model.layers[:-6]:\n    layer.trainable = False\n\n\ndensenet_model = Model(inputs=[input_layer], outputs=[output_layer])\ndensenet_model.compile(Adam(lr=.0001), loss = 'binary_crossentropy', metrics =[METRICS])\ndensenet_model.summary()","9a619a96":"densenet_checkpoint_callback = ModelCheckpoint('DenseNet_model_augs.h5', monitor='val_accuracy', verbose = True, save_best_only = True, save_weights_only = False, mode= 'max')\ndensenet_early_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=5)\n\nwith tf.device('\/GPU:0'):\n    densenet_fit = densenet_model.fit_generator(generator=train_gen,\n                              validation_data=validation_gen,\n                              steps_per_epoch= (train_gen.samples \/\/ 32),\n                              validation_steps = (validation_gen.samples \/\/ 32),\n                              use_multiprocessing=True,\n                              workers=-1,\n                              verbose = True,\n                              epochs = 20,\n                              callbacks = [densenet_checkpoint_callback, densenet_early_stopping_callback])","8da67967":"plt.plot(np.array(densenet_fit.history['loss']),color='blue')\nplt.plot(np.array(densenet_fit.history['val_loss']),color='red')\nplt.title('DenseNet121 model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","c3069be4":"plt.plot(np.array(densenet_fit.history['accuracy']),color='blue')\nplt.plot(np.array(densenet_fit.history['val_accuracy']),color='red')\nplt.title('DenseNet121 model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","4bba4773":"with open('densenet_model_augs_history.pickle','wb') as file:\n    pickle.dump(densenet_fit.history,file)","fe9d3ccc":"with open('densenet_model_history.pickle', 'rb') as file:\n    history = pickle.load(file)","22e2baac":"shutil.rmtree('\/kaggle\/working\/validation')\nshutil.rmtree('\/kaggle\/working\/train')","b1184af4":"## VGG16 model: ","a056f2d8":"### Model Definition:","0f5f3fa8":"### Training:","18e5e36c":"# Define Models:\n\nWe will test many different models to see which is better fit for this problem. They all incorporate transfer learning, which utilize a pre-trained model to use as a base and layers are added afterwards to adapt to the problem we are working with. This reduces the time needed to train the network. All of the models utilizes a different base model, however each is a Convolutional Neural Network (CNN). CNNs are the base structure for image classification tasks as they are able to extract features from images - edges, lines, and other shapes - without explicitly defining them. ","f177176f":"# Problem Definition:","8c040fb3":"For this project we were motivated to see how machine learning could be applied to more complex datasets than have been discussed in class.\nWe immediately thought to look into the Coronavirus. While looking at relevent datasets we decided that an attainable goal would be to build a XRay Classified for Coronavirus, which provides both a real world scenario and the opportunity to teach ourselves about image classification. \n\nWe have settled on the [COVID-19 Radiography Database](https:\/\/www.kaggle.com\/tawsifurrahman\/covid19-radiography-database) hosted on Kaggle to build our classifier. This dataset was recently updated on March 6, 2021 and is the largest the we could find. \n\nWhile ","51f155ef":"The specific architecture used here is inspired by the research paper [**Deep convolutional neural network based medical image classification for disease diagnosis**](https:\/\/journalofbigdata.springeropen.com\/articles\/10.1186\/s40537-019-0276-2#Sec6), which is concerned with detecting pneumonia in chest Xrays. This model seemed like a good starting point as it tackles a problem similar to ours.\n\nThe base for this model is VGG16, which is pretrained on the ImageNet dataset.. To make the pretrained model adapt to our dataset, the last convolutional layer's weights are unlocked to be updated during model fitting. \n\nThe VGG16 architecture is shown below:\n![](https:\/\/neurohive.io\/wp-content\/uploads\/2018\/11\/vgg16-1-e1542731207177.png)\n\n\n\nThe paper proposes the general structure of the network and discusses possible augmentations to make to the images to better generalize to data outside of the training set. \n\nThe augmentations that worked the best according to the above paper were:\n\n* Random Rotation within the range of .05\n* Random Shearing within the range of .05\n* Random Zooming within the range of .05\n* Random Vertical Flipping\n* Random Horizontal Flipping\n\nThese augmentations have been incorporated into the data generator defined above.","6e630fae":"Here the files are copied to train and validation directories. This is necessary for the data generators. \nAdditionally, all of the files are preprocessed while copying them. The datagenerators do have the ability to pass a preprocessing funtion, however it is faster to only do it once instead of on each epoch for each model so we do it here. Furthermore, one of the models applies vertical flips which would be incompatible with the preprocessing funtion's ability to find the shoulders.  ","fee55f0c":"# Deep Model\n\nWe saw that the linear model performs well, but we wanted to see if adding more hidden layers could improve the performance. The following model has the same base model, ResNet152, and 3 hidden layers instead of none,","dc75ecc9":"### Analysis:","cd238c34":"> ## Data Generator:\n\nWhen working with a relatively large dataset, it is often impossible to load all of the datapoints into memory for model training. This is especially a problem with images, which take up a lot of RAM when decompressed from the .jpg or .png format to numpy arrays. The process of loading images also takes time. To combat both of these problems, a Data Generator is used. With a Data Generator, only a small portion of data is loaded to memory at a time while training. This is done by generating batches of images and applying augmentations to them in real time before passing them to the model. Additionally, this can be done at the same time as the model is training on a different thread. This eliminates both the problems discussed earlier.","84f04ac3":"The architecture used here is inspired by the paper [**COVID-CXNET: Detecting COVID-19 in Frontal Chest X-Ray Images using Deep Learning**](https:\/\/arxiv.org\/pdf\/2006.13807v2.pdf), which is directly related to our task. \n\nAn overview of this model is visualized below:\n![](https:\/\/d3i71xaburhd42.cloudfront.net\/18a690f7622f41638b42d4f07dabbd0fd45b784a\/6-Figure6-1.png)\n\nThe base of this model is DenseNet121, which is also pretrained on ImageNet weights. To make this model adapt to our task, the last 6 layers of DenseNet are unlocked to be trained.\n\nThe COVID-CXNET, paper discusses advanced data augmentation techniques such as the Bi-histogram Equilization with Adaptive Sigmoid Function (BEASF) to increase the contrast of the X-rays. However, this is complex and for this application we will stick with the augmentations suggested in the VGG16 model. This also allows us to use the reuse the data generator defined before. \n","9fc8cfc7":"## Analysis","450ba110":"# Data Exploration","168d4c31":"# Linear Model:","5e340f18":"## COVID-CXNET Model:","084cb396":"###  Train - Validation Split:","f263c89f":"### Training:","08d6f160":"### Analysis:","2ba24e03":"## Analysis:","8d786dc4":"### Model Definition:","71ed8dda":"We decided that as a baseline we would make a very simple model that has only one hidden layer that is trainable. A pretrained ResNet is used as a base but all of the parameters are set to be untraineable. The architecture is shown below:","fe7021c5":"# **Math 3094: Midterm Project**\n\n\n**Group Members: Aaron Spaulding & Timothy O'Reilly**"}}