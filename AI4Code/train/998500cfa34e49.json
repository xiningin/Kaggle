{"cell_type":{"97f0447e":"code","b91536e5":"code","76b39c54":"code","92623924":"code","68b632ce":"code","717a564b":"code","e99cc074":"code","3203b0a3":"code","9370f1a9":"code","8e2ab5f3":"code","538e132d":"code","db0718bb":"code","de733f3d":"code","83133c1e":"code","986ebc38":"code","2dca3a2b":"code","c536406e":"code","be9ac644":"code","06fc5b76":"code","352f64d9":"code","ea85c473":"code","615a1fe5":"code","208df2a6":"code","9a222d22":"markdown","f52b1a1a":"markdown","b247f9a5":"markdown","44c61f99":"markdown","cc84a650":"markdown","f8a7e0d9":"markdown","9e24e26f":"markdown","c68a439e":"markdown","9032da89":"markdown"},"source":{"97f0447e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b91536e5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.models import Sequential","76b39c54":"true_data = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\nfake_data = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')","92623924":"print(true_data.shape)\ntrue_data.head()","68b632ce":"\nprint(fake_data.shape)\nfake_data.head()","717a564b":"fake_data['label'] = 1\ntrue_data['label'] = 0","e99cc074":"fake_data.head()","3203b0a3":"data = pd.concat([true_data,fake_data],axis=0,ignore_index=True)","9370f1a9":"print(data.shape)\ndata.head()","8e2ab5f3":"data.tail()","538e132d":"x=data.copy()","db0718bb":"y = x['label']\nx = x.drop('label',axis = 1)","de733f3d":"x.isnull().sum()","83133c1e":"ps = PorterStemmer()\ncorpus = []\nfor i in range(len(x['title'])):\n    review = re.sub('[^a-zA-Z]', ' ', x['title'][i])\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)","986ebc38":"corpus","2dca3a2b":"voc_size = 10000\nonehot_repr = [one_hot(word,voc_size) for word in corpus]\nonehot_repr","c536406e":"sentlen = 20\nembedding_doc = pad_sequences(onehot_repr,padding = 'pre',maxlen=sentlen)\nembedding_doc","be9ac644":"embedding_feature = 60\nmodel = Sequential()\nmodel.add(Embedding(voc_size,embedding_feature,input_length=sentlen))\nmodel.add(LSTM(64,return_sequences=True))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(LSTM(32))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(Dense(1,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","06fc5b76":"x_final = np.array(embedding_doc)\n","352f64d9":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_final,y,test_size=0.3,random_state=1)","ea85c473":"early_stopping = tf.keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)","615a1fe5":"history = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10,batch_size=128,callbacks=[early_stopping])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n","208df2a6":"result = model.evaluate(x_test, y_test)\n\nloss = result[0]\naccuracy = result[1]\n\n\nprint(f\"[+] Accuracy: {accuracy*100:.2f}%\")","9a222d22":"Here i am introducing new column 'label' which is useful while concating fake_data and true_data","f52b1a1a":"# **Model Building**","b247f9a5":"* we are building a sequential model where first we apply word embedding of features = 60 here we use more advanced word embedding like word2vec and GloVe\n* BatchNormalization() and Dropout(0.5) are used for avoiding overfitting","44c61f99":"in the next cell i am applying pad_sequences so that every sentence become len of 20 and the sentences which are less than 20 have zero's behind them as we have applied padding='pre'","cc84a650":"# **Importing Libraries**","f8a7e0d9":"* re.sub('[^a-zA-Z]', ' ', x['title'][i]) this line is useful for removing all links,special characters,numbers etc\n* review.lower() this line is useful for converting all capital letters to lower letters\n* review.split() useful for spliting\n* [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] this line is stemming every word and removing stopwords\n","9e24e26f":"# **Loading Data**","c68a439e":"using onehot encoding of vocabulary size = 10000","9032da89":"# **Text-Data Preprocessing**"}}