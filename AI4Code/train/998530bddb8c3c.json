{"cell_type":{"f9d3ea58":"code","8bc2ff0f":"code","4eee4d9e":"code","f48a3dab":"code","e9b99891":"code","d1623beb":"code","6ef93034":"code","d64912a0":"code","1c4881db":"code","4de9fa94":"code","95305a81":"code","4d19a4c7":"code","c8a2132e":"code","df670848":"code","9538cd63":"code","5eda6345":"code","ce41cbff":"code","6a85fb5d":"code","891afaaf":"code","b501e3ef":"code","2e039851":"code","bfc21847":"code","7cdabbd1":"code","38b86ff1":"code","4133dab6":"code","9d084c7f":"code","85b65d8b":"markdown","21d1b464":"markdown","eb1a00e0":"markdown","e9294e90":"markdown","6b4438b8":"markdown","a4268bdd":"markdown","8d5eac57":"markdown","8b039d88":"markdown","55ed2cbe":"markdown","1da963e4":"markdown","e253baf0":"markdown","0e771568":"markdown","7b3b2457":"markdown","42f05a7d":"markdown","fd38a9f4":"markdown","6cb63042":"markdown","ad2c9545":"markdown","33bda103":"markdown","7bb34803":"markdown","4d5c8b42":"markdown","fc6cfbfb":"markdown","2c825190":"markdown","ea3bd574":"markdown","bfb4b3c8":"markdown","1f4b823b":"markdown"},"source":{"f9d3ea58":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    tfidf_vocab_size = 20000\n    sequence_length = 100 # Length of sequence\n    batch_size = 1024\n    validation_split = 0.15\n    embed_dim = 256\n    latent_dim = 256\n    epochs = 10 # Number of Epochs to train\n    best_auc_model_path = \"model_best_auc.tf\"\n    best_acc_model_path = \"model_best_acc.tf\"\n    lastest_model_path = \"model_latest.tf\"\n    output_dataset_path = \"..\/input\/toxicity-classification-word2vectfidf-output\"\n    labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n    modes = [\"training\", \"inference\"]\n    mode = modes[0]\nconfig = Config()","8bc2ff0f":"import pandas as pd\nimport tensorflow as tf\nimport pathlib\nimport random\nimport string\nimport re\nimport sys\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import TweetTokenizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom scipy.stats import rankdata\nimport json","4eee4d9e":"!unzip ..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\n!unzip ..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\n!unzip ..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip\n!unzip ..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip","f48a3dab":"train = pd.read_csv(\"\/kaggle\/working\/train.csv\")\ntrain.head()","e9b99891":"def custom_standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    stripped_html = tf.strings.regex_replace(lowercase, \"<br \/>\", \" \")\n    text = tf.strings.regex_replace(\n        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n    )\n    text = tf.strings.regex_replace(text, f\"[0-9]+\", \" \")\n    text = tf.strings.regex_replace(text, f\"[ ]+\", \" \")\n    text = tf.strings.strip(text)\n    return text","d1623beb":"tfidf_vectorizer = layers.TextVectorization(\n    standardize=custom_standardization, \n    max_tokens=config.tfidf_vocab_size, \n    output_mode=\"tf-idf\", \n    ngrams=2\n)\nwith tf.device(\"CPU\"):\n    # A bug that prevents this from running on GPU for now.\n    tfidf_vectorizer.adapt(list(train[\"comment_text\"]))","6ef93034":"word2vec_vectorizer = layers.TextVectorization(\n    standardize=custom_standardization, \n    max_tokens=config.vocab_size, \n    output_sequence_length=config.sequence_length\n)\nwith tf.device(\"CPU\"):\n    word2vec_vectorizer.adapt(train[\"comment_text\"])","d64912a0":"X_train, X_val, y_train, y_val = train_test_split(train[\"comment_text\"], train[config.labels], test_size=config.validation_split)","1c4881db":"X_train.shape, y_train.shape, X_val.shape, y_val.shape","4de9fa94":"def make_dataset(X, y, batch_size, mode):\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    if mode == \"train\":\n       dataset = dataset.shuffle(256) \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.cache().prefetch(16).repeat(1)\n    return dataset","95305a81":"train_ds = make_dataset(X_train, y_train, batch_size=config.batch_size, mode=\"train\")\nvalid_ds = make_dataset(X_val, y_val, batch_size=config.batch_size, mode=\"valid\")","4d19a4c7":"for batch in train_ds.take(1):\n    print(batch)","c8a2132e":"class FNetEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, dropout_rate=0.1, **kwargs):\n        super(FNetEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n\n    def call(self, inputs):\n        # Casting the inputs to complex64\n        inp_complex = tf.cast(inputs, tf.complex64)\n        # Projecting the inputs to the frequency domain using FFT2D and\n        # extracting the real part of the output\n        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n        proj_input = self.layernorm_1(inputs + fft)\n        proj_output = self.dense_proj(proj_input)\n       \n        layer_norm = self.layernorm_2(proj_input + proj_output)\n        return layer_norm","df670848":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","9538cd63":"def get_word2vec_model(config, inputs):\n    x = word2vec_vectorizer(inputs)\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(x)\n    x = FNetEncoder(config.embed_dim, config.latent_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.5)(x)\n    for i in range(3):\n        x = layers.Dense(100, activation=\"relu\")(x)\n        x = layers.Dropout(0.3)(x)\n    return x","5eda6345":"def get_tfidf_model(config, inputs):\n    x = tfidf_vectorizer(inputs)\n    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(x)\n    return x","ce41cbff":"def get_model(config):\n    inputs = keras.Input(shape=(None, ), dtype=\"string\", name=\"inputs\")\n    word2vec_x = get_word2vec_model(config, inputs)\n    tfidf_x = get_tfidf_model(config, inputs)\n    x = layers.Concatenate()([word2vec_x, tfidf_x])\n    output = layers.Dense(6, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, output, name=\"model\")\n    return model","6a85fb5d":"model = get_model(config)\nmodel.summary()","891afaaf":"keras.utils.plot_model(model, show_shapes=True)","b501e3ef":"model.compile(\n    \"adam\", loss=\"binary_crossentropy\", metrics=[\"categorical_accuracy\", keras.metrics.AUC()]\n)","2e039851":"if config.mode == config.modes[0]:\n    acc_checkpoint = keras.callbacks.ModelCheckpoint(config.best_acc_model_path, monitor=\"val_categorical_accuracy\",save_weights_only=True, save_best_only=True)\n    auc_checkpoint = keras.callbacks.ModelCheckpoint(config.best_auc_model_path, monitor=\"val_auc\",save_weights_only=True, save_best_only=True)\n    early_stopping = keras.callbacks.EarlyStopping(patience=10)\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(patience=5, min_delta=1e-4, min_lr=1e-6)\n    model.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[acc_checkpoint, auc_checkpoint, reduce_lr])\n    model.save_weights(config.lastest_model_path)","bfc21847":"if config.mode == config.modes[0]:\n    from sklearn.metrics import classification_report\n    y_pred = np.array(model.predict(valid_ds) > 0.5, dtype=int)\n    cls_report = classification_report(y_val, y_pred)\n    print(cls_report)","7cdabbd1":"test = pd.read_csv(\"\/kaggle\/working\/test.csv\")\ntest.head()","38b86ff1":"sample_submission = pd.read_csv(\"\/kaggle\/working\/sample_submission.csv\")\nsample_submission.head()","4133dab6":"scores = []\ntest_ds = tf.data.Dataset.from_tensor_slices((test[\"comment_text\"])).batch(config.batch_size).cache().prefetch(1)\nfor path in [config.best_auc_model_path]:\n#for path in [config.best_acc_model_path, config.best_auc_model_path, config.lastest_model_path]:\n    if config.mode == config.modes[1]:\n        path = config.output_dataset_path + \"\/\" + path\n    model.load_weights(path)\n    score = model.predict(test_ds)\n    scores.append(score)\nscore = np.mean(scores, axis=0)\nprint(score.shape)","9d084c7f":"sample_submission[config.labels] = score\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","85b65d8b":"<a id=\"5.5\"><\/a>\n### 5.5 Create TensorFlow Dataset","21d1b464":"<font color=\"red\" size=\"3\">If you found it useful and would like to back me up, just upvote.<\/font>","eb1a00e0":"Let's see what this data look like.","e9294e90":"<a id=\"6.2\"><\/a>\n### 6.2 Positional Embedding","6b4438b8":"<a id=\"5.4\"><\/a>\n### 5.4 Train Validation Split","a4268bdd":"# Toxicity Classification: Word2Vec+TFIDF\n## Table of Contents\n* [1. Overview](#1.)\n* [2. Configuration](#2.)\n* [3. Setup](#3.)\n* [4. Import datasets](#4.)\n* [5. EDA & Preprocessing](#5.)\n    * [5.1 Text Preprocessing Function](#5.1)\n    * [5.2 TF-IDF Vectorization](#5.2)\n    * [5.3 Word2Vec Vectorization](#5.3)\n    * [5.4 Train Validation Split](#5.4)\n    * [5.5 Create TensorFlow Dataset](#5.5)\n* [6. Model Development](#6.)\n    * [6.1 FNet Encoder](#6.1)\n    * [6.2 Positional Embedding](#6.2)\n    * [6.3 Word2Vec FNet Model](#6.3)\n    * [6.4 TFIDF DNN Model](#6.4)\n    * [6.5 The Whole Model](#6.5)\n    * [6.6 Model Training](#6.6)\n    * [6.7 Evaluation](#6.7)\n* [7. Submission](#7.)\n* [8. References](#8.)","8d5eac57":"\n<a id=\"8.\"><\/a>\n## 8. References\n- [FNet: Mixing Tokens with Fourier Transforms](https:\/\/arxiv.org\/abs\/2105.03824v3)\n- [Attention Is All You Need](https:\/\/arxiv.org\/abs\/1706.03762v5)\n- [Text Generation using FNet](https:\/\/keras.io\/examples\/nlp\/text_generation_fnet\/)\n- [English-Spanish Translation: FNet](https:\/\/www.kaggle.com\/lonnieqin\/english-spanish-translation-fnet)","8b039d88":"<a id=\"7.\"><\/a>\n## 7. Submission","55ed2cbe":"<a id=\"3.\"><\/a>\n## 3. Setup","1da963e4":"<a id=\"6.6\"><\/a>\n### 6.6 Model Training","e253baf0":"<a id=\"6.5\"><\/a>\n### 6.5 The Whole Model","0e771568":"<a id=\"4.\"><\/a>\n## 4. Import datasets","7b3b2457":"<a id=\"5.\"><\/a>\n## 5. EDA & Preprocessing","42f05a7d":"<a id=\"6.3\"><\/a>\n### 6.3 Word2Vec FNet Model","fd38a9f4":"<a id=\"1.\"><\/a>\n## 1. Overview\nIn this notebook, I am going to build a Jigsaw Toxicity Classification Model with FNet using Word2Vec Vectorization and DNN using TFIDF Vectorization. \n\nI will keep track of three Model: Model with Best Accuracy, Model with Best AUC, and Latest Model. So I can use them for inference and try different ensemble method to get a better score.\n\nMany people like to create a training notebook and inference notebook to separate training and inference process. But to me I think that maintaining two notebooks is very inconvenient, instead I create an output dataset for this notebook. And I add two modes to this notebook, training and inference. During training mode, this notebook is responsible for training, it also infernece using output of the training. Everytime we finish training, we can choose the output to update the version of this output dataset. During inference mode, this notebook will skip training and inference using output dataset directly.","6cb63042":"Let's visualize the Model.","ad2c9545":"<a id=\"6.7\"><\/a>\n### 6.7 Evaluation","33bda103":"<a id=\"5.3\"><\/a>\n### 5.3 Word2Vec Vectorization","7bb34803":"#### Classification Report","4d5c8b42":"<a id=\"5.2\"><\/a>\n### 5.2 TF-IDF Vectorization","fc6cfbfb":"<a id=\"6.4\"><\/a>\n### 6.4 TFIDF DNN Model","2c825190":"<a id=\"6.1\"><\/a>\n### 6.1 FNet Encoder","ea3bd574":"<a id=\"5.1\"><\/a>\n### 5.1 Text Preprocessing Function ","bfb4b3c8":"<a id=\"2.\"><\/a>\n## 2. Configuration","1f4b823b":"<a id=\"6.\"><\/a>\n## 6. Model Development"}}