{"cell_type":{"205b1dc3":"code","b7fa2d2e":"code","f0fa1fe0":"code","d80caf61":"code","cfa396bf":"code","80aa4e6e":"code","8b723042":"code","715a8acb":"code","9e19a61f":"code","395ac6b6":"code","1008c021":"code","66285e52":"code","79f02dd0":"code","f79ec66d":"markdown","5e531ca9":"markdown","25a103ec":"markdown","5f60edca":"markdown","6f098fa4":"markdown"},"source":{"205b1dc3":"!apt install -y ffmpeg  # for GIF creation\n!pip install ffmpeg     # P.S needs internet on","b7fa2d2e":"import torch\nimport torchvision.datasets  as dsets\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.utils as vutils\n\nimport tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom matplotlib import animation\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML, display, FileLink","f0fa1fe0":"batch_size = 32\nimage_size = 64\nnum_epochs = 2\ntorch.cuda.set_device(\"cuda:0\")","d80caf61":"dset = dsets.ImageFolder(root=\"\/kaggle\/input\/\",\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                           ]))\n# cut the size of the dataset\ndataset, _ = torch.utils.data.random_split(dset, [len(dset)\/\/2, len(dset)-len(dset)\/\/2])\ndel _\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True)","cfa396bf":"real_batch = next(iter(dataloader))\nplt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0], padding=2).cpu(),(1,2,0)))","80aa4e6e":"class Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\n\nclass UnFlatten(nn.Module):\n    def forward(self, input, size=1024):\n        return input.view(input.size(0), size, 1, 1)","8b723042":"class VAE(nn.Module):\n    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n        super().__init__()\n        \n        self.encoder = nn.Sequential(\n            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n            nn.ReLU(),\n            Flatten()\n        )\n        \n        self.h2mu = nn.Linear(h_dim, z_dim)\n        self.h2sigma = nn.Linear(h_dim, z_dim)\n        self.z2h = nn.Linear(z_dim, h_dim)\n        \n        self.decoder = nn.Sequential(\n            UnFlatten(),\n            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n            nn.Sigmoid(),\n        )\n        \n    def reparameterize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        eps = torch.randn(*mu.size())\n        z = mu + std * eps\n        return z\n    \n    def bottleneck(self, h):\n        mu = self.h2mu(h)\n        logvar = self.h2sigma(h)\n        z = self.reparameterize(mu, logvar)\n        return z, mu, logvar\n        \n    def encode(self, x):\n        return self.bottleneck(self.encoder(x))[0]\n\n    def decode(self, z):\n        return self.decoder(self.z2h(z))\n    \n    def forward(self, x):\n        h = self.encoder(x)\n        z, mu, logvar = self.bottleneck(h)\n        z = self.z2h(z)\n        return self.decoder(z), mu, logvar","715a8acb":"def vae_loss(recon_x, x, mu, logvar) -> float:\n    BCE = F.binary_cross_entropy(recon_x.view(-1, image_size*image_size*3),\n                                 x.view(-1, image_size*image_size*3), reduction='sum')\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return BCE + KLD","9e19a61f":"model = VAE()\ntry:\n    model.load_state_dict(torch.load('vae.pth'))\nexcept:\n    print(\"Weights not found ):\")\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","395ac6b6":"for epoch in range(num_epochs):\n    train_loss = 0\n    for data, _ in tqdm.tqdm(dataloader):\n        optimizer.zero_grad()\n        recon_batch, mu, logvar = model(data)\n        loss = vae_loss(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n        \n    torch.save(model.state_dict(), \"vae.pth\")\n    print('epoch %d, loss %.4f' % (epoch, train_loss \/ len(dataset)))\nmodel.eval()\nFileLink(r'vae.pth')","1008c021":"def get(x):\n    return dataset[x][0].view(1, 3, image_size, image_size)\n\ndef imshow(img):\n    pic = np.transpose(img.numpy(), (1,2,0))\n    plt.axis('off')\n    return plt.imshow(pic, animated=True)\n\ndef morph(inputs, steps: int, delay: int):\n    latent = [model.encode(get(k)).data for k in inputs]\n    fig = plt.figure()\n    images = []\n    for a, b in zip(latent, latent[1:] + [latent[0]]):\n        for t in np.linspace(0, 1, steps):\n            c = a*(1-t)+b*t\n            morphed = model.decode(c).data[0]\n            images.append([imshow(morphed)])\n    \n    ani = animation.ArtistAnimation(fig, images, interval=delay)\n\n    display(HTML(ani.to_html5_video()))","66285e52":"num_images = 30 # amount of images in GIF\nnum_steps = 20 # smoothness of transition between images\ndelay = 30\nmorph(np.random.randint(0, len(dataset), num_images), num_steps, delay)","79f02dd0":"decoded_batch = model.decode(model.encode(real_batch[0]).data).data\nplt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nplt.imshow(np.transpose(vutils.make_grid(decoded_batch, padding=2).cpu(),(1,2,0)))","f79ec66d":"### Model","5e531ca9":"## Imports","25a103ec":"### Data","5f60edca":"## training","6f098fa4":"### Visualizations time!"}}