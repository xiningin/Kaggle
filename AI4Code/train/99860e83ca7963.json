{"cell_type":{"1aa89de0":"code","76b54378":"code","19b2c941":"code","74f580ee":"code","29c4f930":"code","8078df09":"code","a4e825b7":"code","b86a88a1":"code","3774996c":"code","71e4bb85":"code","bea6215f":"code","fe05c8e3":"code","c2506a13":"code","f77be1b5":"code","181a895e":"code","365aaa20":"code","51de6464":"code","45003a6e":"code","6f22455e":"code","e7d5d2da":"code","a7107ab7":"code","85d35c43":"code","9113cff9":"code","e99f9797":"code","a7d91a57":"code","934a8454":"code","63b279f7":"code","c70f3170":"code","3c3e6cf2":"code","5c25d7fe":"code","f95ee0ba":"code","f8577013":"code","45201c6c":"code","adb06b17":"code","c03c5667":"code","2856c8c3":"code","1d9e2d30":"code","d1052980":"code","66d00f69":"code","a151df04":"code","274b122c":"code","cadf0515":"markdown","d917ffe9":"markdown","e7c85d4b":"markdown","bccd22fc":"markdown","f04c5c66":"markdown","fac0a17a":"markdown","eab352d5":"markdown","dfabe56d":"markdown","3066244f":"markdown","158a4972":"markdown","9a351831":"markdown","80354f75":"markdown","7da8fff2":"markdown","9952db34":"markdown","7c099a65":"markdown","fbd8fea9":"markdown","1157244e":"markdown","a0097574":"markdown","1db9d9f0":"markdown"},"source":{"1aa89de0":"# files loading\nimport os\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","76b54378":"# image pretreatment\nimport matplotlib.pyplot as plt\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom keras.utils.vis_utils import plot_model","19b2c941":"import keras\nfrom numpy import expand_dims\nfrom keras import backend as K\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\n","74f580ee":"import imageio\nimport imgaug as ia\nfrom imgaug import augmenters as iaa","29c4f930":"# libraries for a CNN\nimport tensorflow as tf\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Dropout, Activation\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.utils import to_categorical\n\nfrom tensorflow.python.client import device_lib\n","8078df09":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","a4e825b7":"ANNOTATION_DIR = '..\/input\/stanford-dogs-dataset\/annotations\/Annotation' \nIMAGES_DIR = '..\/input\/stanford-dogs-dataset\/images\/Images'","b86a88a1":"breed_list = os.listdir(IMAGES_DIR)\nprint(\"num. breeds total:\", len(breed_list))","3774996c":"filtered_breeds = [breed.split('-',1)[1] for breed in breed_list] #visualize breeds\nfiltered_breeds[:12]","71e4bb85":"def show_dir_images(breed, n_to_show):\n    plt.figure(figsize=(16,16))\n    img_dir = \"..\/input\/stanford-dogs-dataset\/images\/Images\/{}\/\".format(breed)\n    images = os.listdir(img_dir)[:n_to_show]\n    for i in range(n_to_show):\n        img = cv2.imread(img_dir + images[i])\n        plt.subplot(n_to_show\/4+1, 4, i+1)\n        plt.imshow(img)\n        plt.axis('off')\n\n\n\nprint(breed_list[11])\nshow_dir_images(breed_list[11], 4)","bea6215f":"img_dir = \"..\/input\/stanford-dogs-dataset\/images\/Images\/{}\/\".format(breed_list[11])\nimages = os.listdir(img_dir)[:12]\nimages = os.listdir(img_dir)[:4]\nimg = cv2.imread(img_dir + images[0])\n\n# transform image for equalization\nimg_RGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \nimg_grayscale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\nimg_to_yuv = cv2.cvtColor(img,cv2.COLOR_BGR2YUV)\n\nplt.imshow(img_to_yuv)","fe05c8e3":"hist,bins = np.histogram(img.flatten(),256,[0,256])\ncdf = hist.cumsum()\ncdf_normalized = cdf * float(hist.max()) \/ cdf.max()\nplt.plot(cdf_normalized, color = 'b')\nplt.hist(img_to_yuv.flatten(),256,[0,256], color = 'r')\nplt.xlim([0,256])\nplt.legend(('cdf','histogram'), loc = 'upper left')\nplt.show()","c2506a13":"img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0])\nequ = cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2BGR)\nres = np.hstack((img_to_yuv,equ)) #stacking images side-by-side\ncv2.imwrite('res.png',res)\nplt.imshow(res)","f77be1b5":"hist,bins = np.histogram(equ.flatten(),256,[0,256])\ncdf = hist.cumsum()\ncdf_normalized = cdf * float(hist.max()) \/ cdf.max()\nplt.plot(cdf_normalized, color = 'b')\nplt.hist(equ.flatten(),256,[0,256], color = 'r')\nplt.show()","181a895e":"print(\"Augmented by rotation:\")\n#ia.imshow(image_aug_2)\n# convert to numpy array\ndata = img_to_array(img_RGB)\n\n# expand dimension to one sample\nsamples = expand_dims(data, 0)\n# create image data augmentation generator\ndatagen = ImageDataGenerator(rotation_range=30)\n# prepare iterator\nit = datagen.flow(samples, batch_size=1)\n# generate samples and plot\nfor i in range(9):\n    # define subplot\n    plt.subplot(330 + 1 + i)\n    # generate batch of images\n    batch = it.next()\n    # convert to unsigned integers for viewing\n    image = batch[0].astype('uint8')\n    # plot raw pixel data\n    plt.imshow(image)\n# show the figure\nplt.figure(figsize = (15,15))\nplt.show()","365aaa20":"num_breeds = 10 # integer between 2 and 120\nbreeds = breed_list[:num_breeds]\n\ndef load_images_and_labels(breeds):\n    img_lst=[]\n    labels=[]\n    \n    for index, breed in enumerate(breeds):\n        for image_name in os.listdir(IMAGES_DIR+\"\/\"+breed):\n            img = cv2.imread(IMAGES_DIR+\"\/\"+breed+\"\/\"+image_name)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            \n            img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0]) # convert to yuv color space for equalization\n            equ = cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2RGB) # equalize\n            \n            img_array = Image.fromarray(img, 'RGB')\n            \n            #resize image to 227 x 227 because the input image resolution for AlexNet is 227 x 227\n            resized_img = img_array.resize((227, 227))\n            \n            img_lst.append(np.array(resized_img))\n            \n            labels.append(filtered_breeds[index])\n            \n    return img_lst, labels \n\nimages, labels = load_images_and_labels(breeds)\nprint(\"No. of images loaded = \",len(images),\"\\nNo. of labels loaded = \",len(labels))","51de6464":"# replace numbers with names\nle = LabelEncoder()\nnlabels = le.fit_transform(labels) # encode labels as number values. This prepares for categorical encoding\nY=to_categorical(nlabels,num_classes = num_breeds) # category encoding","45003a6e":"#Normalization for the images\nimages = np.array(images)\nimages = images.astype(np.float32)\n#labels = labels.astype(np.int32)\nX_norm = images\/255","6f22455e":"x_train, x_test, y_train, y_test = train_test_split(X_norm, Y, test_size = 0.2, random_state = 42)\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1)\n\nprint(\"x_train shape = \",x_train.shape)\nprint(\"y_train shape = \",y_train.shape)\n\nprint(\"\\nx_val shape = \",x_val.shape)\nprint(\"y_val shape = \",y_val.shape)\n\nprint(\"\\nx_test shape = \",x_test.shape)\nprint(\"y_test shape = \",y_test.shape)","e7d5d2da":"# example of training images\ndf_y_train = pd.DataFrame(y_train, columns = filtered_breeds[:num_breeds]) \ndf_given_train = df_y_train.apply(lambda s, n: pd.Series(s.nlargest(n).index), axis=1, n=1)\n","a7107ab7":"plt.figure(figsize = (20,20))\nfor i in range(5):\n    img = x_train[i]\n    plt.subplot(1,5,i+1)\n    plt.imshow(img)\n    #plt.axis(\"off\")\n    plt.xlabel(y_train[i], color = \"r\")\n    plt.title(df_given_train.iloc[i,0])\nplt.show()","85d35c43":"aug = ImageDataGenerator(rotation_range=30, #rotations (as seen above)\n                        width_shift_range=0.2,  # randomly shift images horizontally \n                        height_shift_range=0.2,# randomly shift images vertically \n                        shear_range=0.2, # shear image\n                        zoom_range=0.2, # zoom into image \n                        horizontal_flip=True, # randomly flip images horizontally\n                        fill_mode='reflect') #  creates a \u2018reflection\u2019 and fills the empty values in reverse order of the known values\n# fit parameters from data\naug.fit(x_train, augment=True)","9113cff9":"input_shape = (None, 227, 227, 3)\n \n\nmodel1 = tf.keras.Sequential() \nmodel1.add(tf.keras.layers.Conv2D(16, (3, 3), use_bias=False))\nmodel1.add(tf.keras.layers.MaxPool2D(pool_size=(4, 4), strides=(4, 4), padding='same'))        \nmodel1.add(tf.keras.layers.Flatten())        \nmodel1.add( tf.keras.layers.Dense(512, activation='relu'))       \nmodel1.add(tf.keras.layers.Dense(num_breeds, activation='softmax'))        \n\nmodel1.build(input_shape)\nmodel1.summary()\n","e99f9797":"model1.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, amsgrad=False), metrics=['accuracy']) #compile model\n\nplot_model(model1, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","a7d91a57":"%%time\nhistory_1 = model1.fit(aug.flow(x_train, y_train), \n          validation_data=(x_val, y_val),\n          epochs=5)","934a8454":"plt.figure(figsize=(18, 6))\n\nplt.subplot(121)\nloss = history_1.history['loss']\nval_loss = history_1.history['val_loss']\nplt.plot(loss,\"--\", linewidth=3 , label=\"train\")\nplt.plot(val_loss, linewidth=3 , label=\"valid\")\n\nplt.legend(['train','validation'], loc='upper left')\nplt.grid()\nplt.ylabel('loss')\nplt.ylim((1.5, 3))\nplt.xlabel('Epoch')\nplt.title('2 layers CNN Model Loss')\nplt.legend(['train','validation'], loc='upper left')\n\nplt.subplot(122)\nacc = history_1.history['accuracy']\nval_acc = history_1.history['val_accuracy']\n\nplt.plot(acc,\"--\", linewidth=3 , label=\"train\")\nplt.plot(val_acc, linewidth=3 , label=\"valid\")\n\nplt.legend(['train','validation'], loc='upper left')\nplt.grid()\n\nplt.ylabel('accuracy')\nplt.xlabel('Epoch')\nplt.title('2 layers CNN Model accuracy')\nplt.legend(['train','validation'], loc='upper left')\nplt.show()","63b279f7":"test_loss, test_accuracy = model1.evaluate(x_test, y_test)\n\nprint(\"Test results \\n Loss:\",test_loss,'\\n Accuracy',test_accuracy)","c70f3170":"# Initialize the classifier.\nclassifier = Sequential()\n          \n# 1st Convolutional Layer\nclassifier.add(Conv2D(filters = 96, input_shape = (227,227,3), kernel_size = (11,11), strides = (4,4), padding = 'valid'))\nclassifier.add(Activation('relu'))\n# Batch Normalisation before passing it to the next layer\nclassifier.add(BatchNormalization())\n# Pooling Layer\nclassifier.add(MaxPooling2D(pool_size = (3,3), strides = (2,2), padding = 'valid'))\n\n# 2nd Convolutional Layer\nclassifier.add(Conv2D(filters = 256, kernel_size = (5,5), strides = (1,1), padding = 'same'))\nclassifier.add(Activation('relu'))\n# Batch Normalisation\nclassifier.add(BatchNormalization())\n# Pooling Layer\nclassifier.add(MaxPooling2D(pool_size = (3,3), strides = (2,2), padding = 'valid'))\n\n# 3rd Convolutional Layer\nclassifier.add(Conv2D(filters = 384, kernel_size = (3,3), strides = (1,1), padding = 'same'))\nclassifier.add(Activation('relu'))\n# Batch Normalisation\nclassifier.add(BatchNormalization())\n# Dropout\nclassifier.add(Dropout(0.5))\n\n# 4th Convolutional Layer\nclassifier.add(Conv2D(filters = 384, kernel_size = (3,3), strides = (1,1), padding = 'same'))\nclassifier.add(Activation('relu'))\n# Batch Normalisation\nclassifier.add(BatchNormalization())\n# Dropout\nclassifier.add(Dropout(0.5))\n\n# 5th Convolutional Layer\nclassifier.add(Conv2D(filters = 256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\nclassifier.add(Activation('relu'))\n# Batch Normalisation\nclassifier.add(BatchNormalization())\n# Pooling Layer\nclassifier.add(MaxPooling2D(pool_size = (3,3), strides = (2,2), padding = 'valid'))\n# Dropout\nclassifier.add(Dropout(0.5))\n\n# Passing it to a dense layer\nclassifier.add(Flatten())\n\n# 1st Dense Layer\nclassifier.add(Dense(4096, input_shape = (227,227,3)))\nclassifier.add(Activation('relu'))\n# Add Dropout to prevent overfitting\nclassifier.add(Dropout(0.5))\n# Batch Normalisation\nclassifier.add(BatchNormalization())\n\n# 2nd Dense Layer\nclassifier.add(Dense(4096))\nclassifier.add(Activation('relu'))\n# Add Dropout\nclassifier.add(Dropout(0.3))\n# Batch Normalisation\nclassifier.add(BatchNormalization())\n\n# 3rd Dense Layer\nclassifier.add(Dense(1000))\nclassifier.add(Activation('relu'))\n# Add Dropout\nclassifier.add(Dropout(0.2))\n# Batch Normalisation\nclassifier.add(BatchNormalization())\n\n# Output Layer\nclassifier.add(Dense(num_breeds))\nclassifier.add(Activation('softmax'))\n\n# Get the classifier summary.\nclassifier.summary()\n\n","3c3e6cf2":"classifier.build(input_shape)","5c25d7fe":"#classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Compile \nINIT_LR = 1e-3\nDECAY = 1e-7\n\nopt = tf.keras.optimizers.Adam(lr = INIT_LR, decay = DECAY)\nclassifier.compile(loss=\"categorical_crossentropy\", optimizer = opt,metrics = [\"accuracy\"])\nprint(\"[INFO] Training network...\")\n\n#\u00a4K.set_value(classifier.optimizer.learning_rate, 0.00001)","f95ee0ba":"#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","f8577013":"%%time\nhistory_2 = classifier.fit( aug.flow(x_train, y_train,  shuffle = False),\n    validation_data = (x_val, y_val),\n    epochs = 40,\n    verbose=1)\n\n#hist = model.fit(datagen.flow(x_train, y_train), epochs=25)","45201c6c":"## plot the history of loss and accuracy for train and valid data for the Alexnet model\nplt.figure(figsize=(18, 6))\n\nplt.subplot(121)\nloss = history_2.history['loss']\nval_loss = history_2.history['val_loss']\nplt.plot(loss,\"--\", linewidth=3 , label=\"train\")\nplt.plot(val_loss, linewidth=3 , label=\"valid\")\n\nplt.legend(['train','validation'], loc='upper left')\nplt.grid()\nplt.ylabel('loss')\nplt.ylim((1, 6))\nplt.xlabel('Epoch')\nplt.title('Alexnet CNN Model Loss')\nplt.legend(['train','validation'], loc='upper left')\n\nplt.subplot(122)\nacc = history_2.history['accuracy']\nval_acc = history_2.history['val_accuracy']\n\nplt.plot(acc,\"--\", linewidth=3 , label=\"train\")\nplt.plot(val_acc, linewidth=3 , label=\"valid\")\n\nplt.legend(['train','validation'], loc='upper left')\nplt.grid()\n\nplt.ylabel('accuracy')\nplt.xlabel('Epoch')\nplt.title('Alexnet CNN Model accuracy')\nplt.legend(['train','validation'], loc='upper left')\nplt.show()","adb06b17":"test_loss, test_accuracy = classifier.evaluate(x_test, y_test)\n\nprint(test_loss,test_accuracy)","c03c5667":"pred = classifier.predict(x_test)\nprint(y_test.shape)\nprint(pred.shape)","2856c8c3":"roundpred = np.around(pred, decimals=1)\ndf_pred = pd.DataFrame(roundpred, columns = filtered_breeds[:num_breeds])\ndf_pred.head()","1d9e2d30":"df_breed_pred = df_pred.apply(lambda s, n: pd.Series(s.nlargest(n).index), axis=1, n=3)\ndf_breed_pred.columns = ['1st_prob_breed','2nd_prob_breed','3rd_prob_breed']\ndf_breed_pred.head()","d1052980":"prob_df = df_pred.apply(np.sort, axis=1).apply(lambda df_pred: df_pred[-3:]).apply(pd.Series)\nprob_df.columns = ['3rd_prob','2nd_prob','1st_prob']\nprob_df = prob_df*100\nprob_df = prob_df.astype(int)\nprob_df = pd.concat([prob_df, df_breed_pred], axis=1)","66d00f69":"prob_df['final']= prob_df[\"1st_prob_breed\"].astype(str) +\" \"+ prob_df[\"1st_prob\"].astype(str)+\"%, \"+prob_df[\"2nd_prob_breed\"].astype(str) +\" \"+ prob_df[\"2nd_prob\"].astype(str)+\"%, \"+prob_df[\"3rd_prob_breed\"].astype(str) +\" \"+ prob_df[\"3rd_prob\"].astype(str)+\"%\"","a151df04":"df_test = pd.DataFrame(y_test, columns = filtered_breeds[:num_breeds])\ngiven_df = df_test.apply(lambda s, n: pd.Series(s.nlargest(n).index), axis=1, n=1)\ngiven_df.head()\n","274b122c":"plt.figure(1 , figsize = (19 , 10))\nn = 0 \nr = np.random.randint(low=1, high=100, size=9)\nfor i in r:\n    n += 1 \n    \n    \n    plt.subplot(3, 3, n)\n    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n    \n    plt.imshow(x_test[i])\n    plt.title(given_df.iloc[i,0])\n \n    plt.xlabel(prob_df.iloc[i,6], wrap=True, color = \"r\")\n    plt.xticks([]) , plt.yticks([])\n\nplt.show()","cadf0515":"## 1.3 Example augmentations: mirroring and rotation","d917ffe9":"![Alexnet](https:\/\/engmrk.com\/wp-content\/uploads\/2018\/10\/AlexNet_Original_Image.jpg)","e7c85d4b":"Stanford Dogs Dataset has over 20k images categorized into 120 breeds with uniform bounding boxes. The number of photos for each breed is relatively low, which is usually a good reason to employ transfer learning; but this is a model trained from scratch using a CNN based on AlexNet for only 12 breeds.\n\nA much simpler two layers CNN also produces similar results. \n\nSome more possible tweaking for Alexnet layers is to adjust dropout to prevent overfitting","bccd22fc":"## 1. Import libraries and define data folders","f04c5c66":"### 2.3 test and train sets ","fac0a17a":"# Stanford dogs: breed classifier","eab352d5":"### 3.2 Compile the CNN model","dfabe56d":"## 2. Prepare data for training the CNN model\n","3066244f":"### 2.2 label encoding and features normalization","158a4972":"## 4. AlexNet","9a351831":"## 3. First CNN model (2 convolutional layers only)","80354f75":"### 1.1 Visualize list of breeds (classes) and a sample data (image)","7da8fff2":"### 4.2 compile and run","9952db34":"### 2.1 data loading and image equalization","7c099a65":"### 3.3 Fit the model using training data","fbd8fea9":"## 1.2 Example equalization ","1157244e":"### 3.1 define layers","a0097574":"### 2.4 augment the training data.","1db9d9f0":"### 4.1 define layers"}}