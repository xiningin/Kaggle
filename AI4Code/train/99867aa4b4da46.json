{"cell_type":{"702fa7ac":"code","18ffbdb6":"code","16e4bd44":"code","97e02c06":"code","30b5f374":"code","b905d23c":"code","5da9b7c8":"code","72fcf2c4":"code","53ae6d7f":"code","4ad77200":"code","2e56d142":"code","f44379f7":"code","9b5a624b":"code","9e2d3dca":"code","9f3b959e":"code","95c747eb":"code","66acb0a2":"code","915d00ed":"code","ab8f908f":"code","cd1e3a14":"code","672ca70e":"code","b4065222":"code","62b920a3":"code","f19431c5":"code","379d62af":"code","1bed90b8":"code","e97f252c":"code","b27f8453":"code","de36384e":"code","2b620bed":"code","94c81f7a":"code","05094cc6":"code","5805b6b1":"code","841fa41c":"code","96c70a60":"code","4a2e75b6":"code","ce21f7fb":"code","aebc8ed4":"code","30d8510d":"code","4bf330df":"code","aa2859d2":"code","6cbc127d":"code","35d3c41c":"code","e3b2ed4e":"code","6352b1b0":"code","5da8a8ef":"code","fdf52891":"code","64866591":"code","1d01587a":"code","80b03eff":"code","04c34fef":"code","70e1410d":"code","696c8a64":"code","1c48a138":"code","9924886f":"code","ae125e89":"code","012666d4":"code","c2fd5b13":"code","82d059c7":"markdown","030bdc8f":"markdown","0f54710d":"markdown","956ffe81":"markdown","879a0a9a":"markdown","dcca47cb":"markdown","e5d161b8":"markdown","d604bc9a":"markdown","52e32b09":"markdown","fdb6b78e":"markdown","06534430":"markdown","e77234e3":"markdown","c07e1614":"markdown","874d91be":"markdown","7fd77b15":"markdown","318d22c4":"markdown","99dca8c1":"markdown","a774796d":"markdown","524fd258":"markdown","b087e465":"markdown","24f8cc76":"markdown","6e414de5":"markdown","7c635eec":"markdown","53145fd6":"markdown","cb86ab29":"markdown","131f95c5":"markdown","0fea4b5f":"markdown","bccb44aa":"markdown","c167d7e8":"markdown","27d34fbb":"markdown","ac19afaa":"markdown","470c1d7d":"markdown","2fabd047":"markdown","beada2cc":"markdown","f102d0e3":"markdown","e3efecb2":"markdown","f8a999e0":"markdown","40cf4b31":"markdown","5277489c":"markdown","781c763f":"markdown","6cc5d0ea":"markdown","26f782bf":"markdown","ce2e4849":"markdown","4d2992db":"markdown"},"source":{"702fa7ac":"import numpy as np\nimport pandas as pa\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.patches as matp\n%matplotlib inline\nimport statsmodels.api as sm\nimport random\n\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,RandomizedSearchCV,learning_curve\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder,RobustScaler,Imputer\n\nfrom sklearn.linear_model import LinearRegression,ElasticNet,Lasso,Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor,BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn import metrics\nfrom scipy import stats\nfrom scipy.special import boxcox1p\ncolor = sn.color_palette()\nsn.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","18ffbdb6":"\nsample_submission = pa.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pa.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pa.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n\nprint(\"Shape of house data {}\".format(train.shape))\nprint(\"Length of house data {}\\n\".format(train.shape[0]))\n\ntest_id = test.Id\n\nprint(\"Shape of train data {}\".format(train.shape))\nprint(\"Length of house data {}\\n\".format(train.shape[0]))\n\nprint(\"Shape of test data {}\".format(test.shape))\nprint(\"Length of test data {}\\n\".format(test.shape[0]))\n\n# Here we can see that our train dataset contains 81 features and length is 1000 \n\n#Y_actual_test = pa.read_csv('Y_test.csv')","16e4bd44":"features = list(train.columns)\nprint(features)\nprint(\"\\nTotal Number Of Features {}\".format(len(features)))\ntrain.head()","97e02c06":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(16,5))\n\nsn.scatterplot(x='GrLivArea',y='SalePrice',data=train,ax=axis1).set_title(\"With Outliers \")\n\n\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<250000)].index)\nsn.scatterplot(x='GrLivArea',y='SalePrice',data=train,ax=axis2).set_title(\"Without Outliers\")\n","30b5f374":"fig, (axis1,axis2,axis3,axis4) = plt.subplots(1,4,figsize=(20,4))\n\nsn.lineplot(x='OverallQual',y='SalePrice',data=train,ax=axis1)\nsn.barplot(x='OverallQual',y='SalePrice',data=train,ax=axis2)\nsn.lineplot(x='OverallCond',y='SalePrice',data=train,ax=axis3)\nsn.barplot(x='OverallCond',y='SalePrice',data=train,ax=axis4)\n\nplt.figure(figsize=(20,6))\nsn.lineplot(x='YearBuilt',y='SalePrice',data=train)\nplt.show()","b905d23c":"plt.figure(figsize=(20,6))\nsn.lineplot(x='TotalBsmtSF',y='SalePrice',data=train)\n\nplt.show()\nplt.figure(figsize=(20,6))\n\nsn.scatterplot(x='TotalBsmtSF',y='SalePrice',data=train)\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(20,8))\nsn.scatterplot(x='1stFlrSF',y='SalePrice',data=train,ax=axis1)\nsn.scatterplot(x='2ndFlrSF',y='SalePrice',data=train,ax=axis2)","5da9b7c8":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(16,6))\nsn.boxplot(x='FullBath',y='SalePrice',data=train,ax=axis1)\n#sn.barplot(x='HalfBath',y='SalePrice',data=train,ax=axis2)\nsn.boxplot(x='HalfBath',y='SalePrice',data=train,ax=axis2)","72fcf2c4":"fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(16,6))\nsn.lineplot(x='KitchenAbvGr',y='SalePrice',data=train,ax=axis1)\nsn.lineplot(x='BedroomAbvGr',y='SalePrice',data=train,ax=axis2)\nsn.lineplot(x='TotRmsAbvGrd',y='SalePrice',data=train,ax=axis3)","53ae6d7f":"area = ['LotFrontage','GarageArea','OpenPorchSF','EnclosedPorch','ScreenPorch','SalePrice']\nsn.pairplot(train[area],size=2)","4ad77200":"plt.figure(figsize=(20,6))\nsn.barplot(x='Neighborhood',y='SalePrice',data=train)\nplt.xticks(rotation=45)","2e56d142":"fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(20,5))\n\nsn.barplot(x='PoolQC',y='SalePrice',data=train,ax=axis1)\nsn.barplot(x='PavedDrive',y='SalePrice',data=train,ax=axis2)\nsn.barplot(x='RoofStyle',y='SalePrice',data=train,ax=axis3)\n\nplt.figure(figsize=(20,6))\n## Here we will create a waffle chart for roofstyle to check the proportion of each roof style \n\nroof_dataframe = pa.DataFrame(train['RoofStyle'].value_counts())\nroof_dataframe.rename(columns={'RoofStyle':'Total'},inplace=True)\ntotal_values = sum(roof_dataframe['Total'])\n\ncategory_proportions = [(float(value) \/ total_values) for value in roof_dataframe['Total']]\n\nwidth = 90\nheight = 20\ntotal_number_tiles = width * height\n\n# compute the number of tiles for each catagory\ntiles_per_category = [round(proportion * total_number_tiles) for proportion in category_proportions]\n\n# initialize the waffle chart as an empty matrix\nwaffle_chart = np.zeros((height, width))\n\n# define indices to loop through waffle chart\ncategory_index = 0\ntile_index = 0\n\n# populate the waffle chart\nfor col in range(width):\n    for row in range(height):\n        tile_index += 1\n\n        # if the number of tiles populated for the current category is equal to its corresponding allocated tiles...\n        if tile_index > sum(tiles_per_category[0:category_index]):\n            \n            category_index += 1       \n            \n        # set the class value to an integer, which increases with class\n        waffle_chart[row, col] = category_index\n        \nprint ('Waffle chart populated!')\n\nplt.figure(figsize=(20,6))\n\ncolormap = plt.cm.coolwarm\nplt.matshow(waffle_chart,cmap=colormap)\nplt.colorbar()\n\nax = plt.gca()\n\nax.set_xticks(np.arange(-.5,(width),1),minor=True)\nax.set_yticks(np.arange(-.5,(height),1),minor=True)\n\nax.grid(which='minor',color='w',linestyle='-',linewidth=2)\nplt.xticks([])\n\nplt.yticks([])\n\nvalues_cumsum = np.cumsum(roof_dataframe['Total'])\ntotal_values = values_cumsum[len(values_cumsum) - 1]\n\n# create legend\nlegend_handles = []\nfor i, category in enumerate(roof_dataframe.index.values):\n    label_str = category + ' (' + str(roof_dataframe['Total'][i]) + ')'\n    color_val = colormap(float(values_cumsum[i])\/total_values)\n    legend_handles.append(matp.Patch(color=color_val, label=label_str))\n\n# add legend to chart\nplt.legend(handles=legend_handles,\n           loc='lower center', \n           ncol=len(roof_dataframe.index.values),\n           bbox_to_anchor=(0., -0.2, 0.95, .1)\n          )","f44379f7":"fig, (axis1) = plt.subplots(1,1,figsize=(16,5))\n\n\nplt.figure(figsize=(16,5))\nsn.distplot(train['SalePrice'],color='k',label='Skewness : %.2f'%train['SalePrice'].skew(),ax=axis1).set_title(\"Density Plot of SalePrice\")\nplt.ylabel(\"Frequency\")\n#plt.legend(loc='best')\n\nstats.probplot(train.SalePrice,plot=plt)\nplt.title(\"Probability plot of SalePrice\")\nplt.show()","9b5a624b":"# Here we can see that it need to be normalize \n### 3 types of normal check whch perfomr better means which better normalize the value\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(16,5))\nsn.distplot(train['SalePrice'],fit=stats.johnsonsu,ax=axis1,label='Skewness :%.2f'%train['SalePrice'].skew()).set_title(\"JhonsonSU Transformation\")\n\nsn.distplot(train['SalePrice'],fit=stats.lognorm,ax=axis2,label='Skewness :%.2f'%train['SalePrice'].skew()).set_title(\"Log Transformation\")\n\nsn.distplot(train['SalePrice'],fit=stats.norm,ax=axis3,label='Skewness :%.2f'%train['SalePrice'].skew()).set_title(\"Normal Transformation\")\n\nprint(\"From here we can see that log transformation performs much better than others so we will use Log Transformation to normalize\\n SalePrice\")","9e2d3dca":"train['SalePrice'] = np.log1p(train['SalePrice'])\nplt.figure(figsize=(16,5))\nsn.distplot(train['SalePrice'],color='k',label='Skewness : %.2f'%train['SalePrice'].skew()).set_title(\"Density Plot of Sale Price(Normalized)\")\nplt.ylabel(\"Frequency\")\nplt.legend(loc='best')\nplt.show()\nplt.figure(figsize=(16,5))\n\nstats.probplot(train.SalePrice,plot=plt)\nplt.show()\n\n","9f3b959e":"house_data = pa.concat([train,test])\nhouse_data.drop('SalePrice',axis=1,inplace=True)\nhouse_data.shape","95c747eb":"missing_data = (house_data.isnull().sum() \/ len(house_data)) * 100\nmissing_data = missing_data.drop(missing_data[missing_data == 0].index).sort_values(ascending=False)\nmissing_data_ratio = pa.DataFrame({'Missing Ratio':missing_data})\n\nplt.figure(figsize=(20,10))\ng = sn.barplot(x=missing_data_ratio.index,y='Missing Ratio',data=missing_data_ratio).set_title(\"Misssing Data Ratio\")\nplt.xticks(rotation=45)","66acb0a2":"corrmat = train.corr()\nplt.figure(figsize=(20,10))\nsn.heatmap(corrmat, vmax=0.9, square=True)","915d00ed":"house_data[\"PoolQC\"] = house_data[\"PoolQC\"].fillna(\"None\")\nhouse_data[\"MiscFeature\"] = house_data[\"MiscFeature\"].fillna(\"None\")\nhouse_data[\"Alley\"] = house_data[\"Alley\"].fillna(\"None\")\nhouse_data[\"Fence\"] = house_data[\"Fence\"].fillna(\"None\")\nhouse_data[\"FireplaceQu\"] = house_data[\"FireplaceQu\"].fillna(\"None\")\n\nhouse_data['LotFrontage'] = house_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x:x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    house_data[col] = house_data[col].fillna('None')\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    house_data[col] = house_data[col].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \n            'BsmtFullBath', 'BsmtHalfBath'):\n    house_data[col] = house_data[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n            'BsmtFinType2'):\n    house_data[col] = house_data[col].fillna('None')\n    \nhouse_data[\"MasVnrType\"] = house_data[\"MasVnrType\"].fillna(\"None\")\nhouse_data[\"MasVnrArea\"] = house_data[\"MasVnrArea\"].fillna(0)\nhouse_data['MSZoning'] = house_data['MSZoning'].fillna(house_data['MSZoning'].mode()[0])\n\nhouse_data = house_data.drop('Utilities',axis=1)\n\nhouse_data.Functional.value_counts()\nhouse_data[\"Functional\"] = house_data[\"Functional\"].fillna(\"Typ\")\n\nhouse_data.Electrical.value_counts()\nhouse_data['Electrical'] = house_data['Electrical'].fillna(house_data['Electrical'].mode()[0])\n\nhouse_data['KitchenQual'] = house_data['KitchenQual'].fillna(house_data['KitchenQual'].mode()[0])\nhouse_data['KitchenQual'] = house_data['KitchenQual'].fillna(house_data['KitchenQual'].mode()[0])\nhouse_data['Exterior1st'] = house_data['Exterior1st'].fillna(house_data['Exterior1st'].mode()[0])\nhouse_data['Exterior2nd'] = house_data['Exterior2nd'].fillna(house_data['Exterior2nd'].mode()[0])\n\nhouse_data['SaleType'] = house_data['SaleType'].fillna(house_data['SaleType'].mode()[0])\nhouse_data['MSSubClass'] = house_data['MSSubClass'].fillna(\"None\")","ab8f908f":"#MSSubClass=The building class\nhouse_data['MSSubClass'] = house_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nhouse_data['OverallCond'] = house_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nhouse_data['YrSold'] = house_data['YrSold'].astype(str)\nhouse_data['MoSold'] = house_data['MoSold'].astype(str)","cd1e3a14":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(house_data[c].values)) \n    house_data[c] = lbl.transform(list(house_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(house_data.shape))","672ca70e":"house_data['TotalSF'] = house_data['TotalBsmtSF'] + house_data['1stFlrSF'] + house_data['2ndFlrSF']\n\nhouse_data['Total_Bathrooms'] = (house_data['FullBath'] + (0.5 * house_data['HalfBath']) +\n                               house_data['BsmtFullBath'] + (0.5 * house_data['BsmtHalfBath']))\n\nhouse_data['Total_porch_sf'] = (house_data['OpenPorchSF'] + house_data['3SsnPorch'] +\n                              house_data['EnclosedPorch'] + house_data['ScreenPorch'] +\n                              house_data['WoodDeckSF'])","b4065222":"house_data['haspool'] = house_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nhouse_data['has2ndfloor'] = house_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nhouse_data['hasgarage'] = house_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nhouse_data['hasbsmt'] = house_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nhouse_data['hasfireplace'] = house_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","62b920a3":"numercic_features = house_data.dtypes[house_data.dtypes != 'object'].index\nskewed_fetures = house_data[numercic_features].apply(lambda x:x.skew()).sort_values(ascending=False)\nskewness = pa.DataFrame({'Skew' :skewed_fetures})\nskewness.info()","f19431c5":"skewness = skewness.drop(['haspool','has2ndfloor','hasgarage','hasbsmt','hasfireplace'],axis=0)\nplt.figure(figsize=(20,6))\nskewed_feaures_more = skewness[skewness['Skew'] > 0.75].index\nskewd_data = skewness.Skew[skewness['Skew'] > 0.75]\nsn.barplot(skewed_feaures_more,skewd_data)\nplt.xticks(rotation=90)","379d62af":"skewness = skewness[abs(skewness.Skew) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    house_data[feat] = boxcox1p(house_data[feat], lam)","1bed90b8":"house_data = pa.get_dummies(house_data)\nprint(\"After converting the remaining catagorical features into dummy variables we get {}\".format(house_data.shape))","e97f252c":"y_train =train.SalePrice.values\ntrain = house_data[:len(train)]\ntest = house_data[len(train):]","b27f8453":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","de36384e":"#linear = make_pipeline(RobustScaler(),LinearRegression())\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","2b620bed":"from sklearn.svm import SVR\nsvr = make_pipeline(RobustScaler(),\n                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","94c81f7a":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","05094cc6":"score = rmsle_cv(svr)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","5805b6b1":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","841fa41c":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","96c70a60":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","4a2e75b6":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ce21f7fb":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","aebc8ed4":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)  ","30d8510d":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n","4bf330df":"score = rmsle_cv(averaged_models)","aa2859d2":"print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","6cbc127d":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","35d3c41c":"stacked_averaged_models = StackingAveragedModels(base_models = (KRR, lasso, ENet, GBoost, model_xgb, model_lgb),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","e3b2ed4e":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","6352b1b0":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","5da8a8ef":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","fdf52891":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","64866591":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","1d01587a":"lasso.fit(train,y_train)\nlasso_train_pred = lasso.predict(train)\nlasso_pred = np.expm1(lasso.predict(test))\nprint(rmsle(y_train,lasso_train_pred))","80b03eff":"svr.fit(train,y_train)\nsvr_train_pred = svr.predict(train)\nsvr_pred = np.expm1(svr.predict(test.values))\nprint(rmsle(y_train,svr_train_pred))","04c34fef":"KRR.fit(train,y_train)\nKRR_train_pred = KRR.predict(train)\nKRR_pred = np.expm1(KRR.predict(test.values))\nprint(rmsle(y_train,KRR_train_pred)) ","70e1410d":"ENet.fit(train,y_train)\nENet_train_pred = ENet.predict(train)\nENet_pred = np.expm1(ENet.predict(test.values))\nprint(rmsle(y_train,ENet_train_pred))","696c8a64":"GBoost.fit(train,y_train)\nGBoost_train_pred = GBoost.predict(train)\nGBoost_pred = np.expm1(GBoost.predict(test.values))\nprint(rmsle(y_train,GBoost_train_pred)) ","1c48a138":"averaged_models.fit(train,y_train)\naveraged_models_train_pred = averaged_models.predict(train)\naveraged_models_pred =np.expm1(averaged_models.predict(test))\nprint(rmsle(y_train,averaged_models_train_pred)) ","9924886f":"print('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.25 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.10+GBoost_train_pred*0.15 + svr_train_pred*0.15 +\n               lasso_train_pred*0.05+KRR_train_pred*0.15))","ae125e89":"ensemble = stacked_pred*0.25 + xgb_pred*0.15 + lgb_pred*0.10 + GBoost_pred*0.15 + svr_pred*0.15 + lasso_pred*0.05+KRR_pred*0.15","012666d4":"Y_test = pa.DataFrame(ensemble + 0.11658)","c2fd5b13":"sub = pa.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = ensemble\nsub.to_csv('final_sub.csv',index=False)","82d059c7":"#### ENet","030bdc8f":"##### Co-relation plot ","0f54710d":"From this visualization we can see that on right bottom there is a oulier because the price of the house is nearly 150000 but\nGrLivArea is nearly 5000 which is impossible. So, we will drop the oulier in the next step.We did't remove other outliers for making this model a roboust model if our test data contains outliers removeing outliers from our train and then training the model will not teach the model how to handle ouliers so from this we concluded that the GrLivArea is an important feature and removing 1  outliers which is truely a outlier is completely fine.","956ffe81":"Here we can see that roofstyle Gable is mostly present in most of the house and we can also see that the house with shed roofstyle has a very high price","879a0a9a":"#### Lasso Regression","dcca47cb":"The price of house is increasing as the area of the totalbasement increases ( LinearRelationship) same in case of 1stFloorArea and 2ndFloorArea","e5d161b8":"##### Visualizing PoolQC,PavedRive,RoofStyle","d604bc9a":"##### Visualizing OverallCond,OverQual,YearBuilt","52e32b09":"## Oulier Analysis","fdb6b78e":"##### Box-Cox Transformation on Skewed Data","06534430":"###### Visualizing FullBath,HalfBath","e77234e3":"#### Rmsle score on train Data","c07e1614":"#### KRR","874d91be":"# House Price Prediction - Advanced Regression","7fd77b15":"3 types of normalalization checking - \nFrom here we can see that log transformation performs much better than others so we will use Log Transformation to normalize SalePrice ","318d22c4":"##### Visualizing features and first 5 data","99dca8c1":"KitchenAdvGr is quite interesting, It can be seen that if a house has more kitchen than rooms then tyhe price is decresing significantly and has a strong negative relationship if the number of kitchen increases then saleprice drops significantly .We can also see that if the totalrooms inreases then saleprice also increases which means it as linear relationship with saleprice and it is also a statistically significant feature ","a774796d":"Cost of the houses with roofstyle shed and Hp cost much more than other types , PoolQuality - If the pool quality is Good and Excellent then price is also excellent it's a important feature.","524fd258":"##### Visualizing KitchenAbvGr, BedroomAbvGr, TotRmsAbvGrd","b087e465":"#### SVR","24f8cc76":"LotFrontage,GarageArea,OpenPorchSF are strongly co-rlated with saleprice \n(Note: The diagonal shows the distribution of the variable)","6e414de5":"#### Transformation of Skewed Features","7c635eec":"After visualizing TotalBsmtSF,1stFlrSF & 2ndFlrSF we can conclude that totalbsmtSF has a linear relationship and 1stFlRSF and 2ndFlrSF also has a linear relationship but 1stFLR area seems more co-related with SalePrice than 2ndFlrSF ","53145fd6":"#### Xtreme Gradient Boosting","cb86ab29":"##### Encoding remaining Catagorical Features ","131f95c5":"Here we noticed that price of houses with neighborhood ( NridHt,NoRidge,Veenker,StoneBr,Timber) is much more than others.","0fea4b5f":"##### Visualizing Neighborhood","bccb44aa":"#### LGB","c167d7e8":"#### Averaged Models","27d34fbb":"In this step we have analysed overall quality,overall condition & YearBuilt with Sale Price from here we can see that overallquality has a linear relation with saleprice because as the quality of house increses then the sale price also increses\nagain the feature overallcondition has a linear relationship with saleprice but if the overallcondition is 5 then there is too much increse in the price which is quite fishy as it is not possible in real life. After analysing YearBulit we can say that there is a positive increasing trend(Modern house cost much more than older house as simple as that) and the data is also cyclical.","ac19afaa":"#### SalePrice (Dependent Variable)","470c1d7d":"##### Apply Log Transformation ","2fabd047":"#### Gradient Boosting","beada2cc":"#### Submission","f102d0e3":"## Modelling ","e3efecb2":"## Exploratory Data Analysis","f8a999e0":"##### Pair Plot on area features ","40cf4b31":"### Load Data","5277489c":"### Import all the required libraries","781c763f":"If the house having 0 or 1 FullBath then the price is almost same but if the house having 2 or 3 FullBath then there is a significant increase in price. Again the houses which have 1 halfbath cost more than the houses with 0 or 2 halfbath.","6cc5d0ea":"Here we can see that our SalePrice is not normalized and left skewed so we need to transform it.","26f782bf":"#### Feature Engineering ","ce2e4849":"##### From the corelation plot we noticed : \n\nGaragecars and GarageArea is highly corelated                                                                              \nTotalBsmtArea and 1stFlrArea is highly corelated                                                                          \nGrLivArea amd TotalRmsAbrGr is highly corelated      \nGarageYearBuilt and YearBulit is highly corelated","4d2992db":"### Handling Missing Data"}}