{"cell_type":{"3453b5de":"code","48f4bb08":"code","866626ac":"code","2d3b40f2":"code","07d45d71":"code","530a8fd5":"code","08cfd977":"code","633656f0":"code","869961d8":"code","8007ad8e":"code","ce71943b":"code","e7b26630":"code","a4c7ead6":"code","453c36cb":"code","4f94df68":"code","11445908":"code","348bb9f9":"code","f7ee7b83":"code","bac657ad":"code","c7cdf123":"code","1337ecbf":"code","6e9b6c25":"code","0d6d9d31":"code","6c0a2b79":"code","b255842a":"code","1673c6bd":"code","aa59670e":"code","6602bcab":"code","228d799f":"code","fd49ec46":"code","42ba56a8":"code","be8daded":"code","8f5a4e05":"code","d15e26cf":"code","c7bb9dea":"code","b26206cd":"code","d4ce9f1d":"code","0ce1d089":"code","b61af7c0":"code","71e15840":"code","cc3f693a":"code","aa4d660e":"code","5e86c182":"code","7424c4c0":"code","4ebba09b":"code","fd9d5af9":"code","f87ccf52":"code","b32759f6":"code","677cdb27":"markdown","5ee22498":"markdown","2b057363":"markdown","75eeb325":"markdown","278e217b":"markdown","3c56b1e2":"markdown","3eb19c4e":"markdown","9e7e6d65":"markdown","c3523474":"markdown","f3feb2d9":"markdown","725b369e":"markdown","fa0f058d":"markdown","a7fea60a":"markdown","9330b529":"markdown","c8c6db5f":"markdown","bd9d4709":"markdown","a0830c73":"markdown","23a60a05":"markdown","bf672979":"markdown","6f30f8cd":"markdown","c2bc3394":"markdown","3ac9c3d7":"markdown","749de548":"markdown","dec9b482":"markdown","c147a635":"markdown","abff13ef":"markdown","2db70cb7":"markdown","008ac72c":"markdown","fa91f1f7":"markdown","1579f3eb":"markdown","e6c729ec":"markdown","aa19b14b":"markdown","4bb247ba":"markdown","e9e28902":"markdown","586c93da":"markdown","716a4969":"markdown","1c9a9786":"markdown"},"source":{"3453b5de":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > \/dev\/null 2>&1","48f4bb08":"import os\nimport gc\nimport sys\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datatable as dt\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom mlens.ensemble import SuperLearner\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nfrom bokeh.layouts import row, column\nfrom bokeh.models import ColumnDataSource, CustomJS, Label,Range1d, Slider, Span\nfrom bokeh.plotting import figure, output_notebook, show\n\noutput_notebook()\n\n# sys.path.append('..\/input\/pytorchtabnet\/')\n# !pip install '..\/input\/pytorchtabnet\/pytorch_tabnet-2.0.1-py3-none-any.whl'\n\n# from pytorch_tabnet.tab_model import TabNetClassifier","866626ac":"%%time\nfolder_path = '..\/input\/jane-street-market-prediction\/'\ntrain_data = dt.fread(folder_path + 'train.csv').to_pandas()\nfeatures = dt.fread(folder_path + 'features.csv').to_pandas()\nsample = dt.fread(folder_path + 'example_sample_submission.csv').to_pandas()\ntest_data = dt.fread(folder_path + 'example_test.csv').to_pandas()","2d3b40f2":"print(\"{0}Number of rows in train data: {1}{2}\\n{0}Number of columns in train data: {1}{3}\".format(y_,r_,train_data.shape[0],train_data.shape[1]))\nprint(\"{0}Number of rows in test data: {1}{2}\\n{0}Number of columns in test data: {1}{3}\".format(m_,r_,test_data.shape[0],test_data.shape[1]))\nprint(\"{0}Number of rows in features data: {1}{2}\\n{0}Number of columns in features data: {1}{3}\".format(b_,r_,features.shape[0],features.shape[1]))\nprint(\"{0}Number of rows in sample data: {1}{2}\\n{0}Number of columns in sample data: {1}{3}\".format(c_,r_,sample.shape[0],sample.shape[1]))","07d45d71":"train_data.head()","530a8fd5":"features.head()","08cfd977":"sample.head()","633656f0":"test_data.head()","869961d8":"print(\"Number of features with null values:\",np.sum(train_data.isna().sum()>0))","8007ad8e":"train_data.fillna(train_data.mean(),inplace=True)\nprint(\"Number of features with null values:\",np.sum(train_data.isna().sum()>0))","ce71943b":"def distribution1(feature,color,df=train_data):\n    plt.figure(figsize=(15,7))\n    plt.subplot(121)\n    sns.distplot(df[feature],color=color)\n    plt.subplot(122)\n    sns.violinplot(df[feature])\n    print(\"{}Max value of {} is: {} {:.2f} \\n{}Min value of {} is: {} {:.2f}\\n{}Mean of {} is: {}{:.2f}\\n{}Standard Deviation of {} is:{}{:.2f}\"\\\n      .format(y_,feature,r_,df[feature].max(),g_,feature,r_,df[feature].min(),b_,feature,r_,df[feature].mean(),m_,feature,r_,df[feature].std()))","e7b26630":"distribution1('weight',color='yellow')","a4c7ead6":"print(\"Number of rows with weight zero:\",np.sum(train_data.weight==0))\nprint(\"Percentage of rows with weight zero\",np.sum(train_data.weight==0)\/len(train_data))","453c36cb":"distribution1('resp',color='green')","4f94df68":"def utility_score(df,score=0.5,testing=False,true_percentage=0.8):\n    if testing:\n        df['action'] = 0\n        \n        false_percentage = 1 - true_percentage\n        mask1 = df['resp'] >= 0\n        mask2 = df['resp'] < 0\n        \n        df.loc[mask1,'action'] = score\n        \n        #make incorrect prediction on the positive resp \n        change_idx = df[mask1].sample(frac=false_percentage).index\n        df.loc[change_idx,'action'] = 0\n        \n        #make incorrect prediction on the negative resp \n        change_idx = df[mask2].sample(frac=false_percentage).index\n        df.loc[change_idx,'action'] = score\n        \n        df['pj'] = df['action'] * df['resp'] * df['weight']\n        pi = df[['date','pj']].groupby('date').agg({'pj':['sum']})\n        pi.columns = ['pi']\n        pi['pi_sq'] = pi['pi']**2\n        t = np.sum(pi['pi'])\/np.sqrt(np.sum(pi['pi_sq'])) * np.sqrt(250\/len(pi['pi'])) \n        u = min(max(t,0),6) * np.sum(pi['pi'])\n        \n        return u\n    \n    else:\n        df['pj'] = df['action'] * df['resp'] * df['weight']\n        pi = df[['date','pj']].groupby('date').agg({'pj':['sum']})\n        pi.columns = ['pi']\n        pi['pi_sq'] = pi['pi']**2\n        t = np.sum(pi['pi'])\/np.sqrt(np.sum(pi['pi_sq'])) * np.sqrt(250\/len(pi['pi'])) \n        u = min(max(t,0),6) * np.sum(pi['pi'])\n        \n        return u","11445908":"train_data['action'] =0\ntrain_data.loc[train_data['resp']>0.0,'action'] = 1\nfeatures = [f\"feature_{x}\" for x in range(130)]","348bb9f9":"train_data.head()","f7ee7b83":"scores = list()\nfor i in np.arange(0,1.05,0.05):\n    if i == 0:\n        score = utility_score(train_data,score=0.01,testing=True) \n    else:\n        score = utility_score(train_data,score=i,testing=True)\n    scores.append(score)\npx.scatter(x=np.arange(0.0,1.05,.05),y=scores)","bac657ad":"scores = list()\nfor i in np.arange(0,1.05,0.05):\n    score = utility_score(train_data,score=0.5,testing=True,true_percentage=i)\n    scores.append(score)\n    \npx.scatter(x=np.arange(0.0,1.05,.05),y=scores)","c7cdf123":"train_data['pj'] = train_data['action'] * train_data['resp'] * train_data['weight']\npi = train_data[['date','pj']].groupby('date').agg({'pj':['sum']})\npi.columns = ['pi']\ndistribution1('pi','blue',df=pi)\nprint(\"sum of pi is\",np.sum(pi['pi']))","1337ecbf":"corr = train_data[features].corr()\nfig = px.imshow(corr)\nfig.show()","6e9b6c25":"plt.figure(figsize=(10,10))\nsns.pairplot(train_data.loc[:10000,[f\"feature_{x}\" for x in range(18,39)]]);","0d6d9d31":"def pca_plot1(features,n_components,target,nrows=10**4):\n    pca = PCA(n_components=n_components)\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    train_g_pca = pca.fit_transform(train_d[features])\n\n    total_var = pca.explained_variance_ratio_.sum()*100\n    labels = {str(i): f\"PC {i+1}\" for i in range(n_components)}\n\n    fig = px.scatter_matrix(\n        train_g_pca,\n        dimensions=range(n_components),\n        labels=labels,\n        title=f\"Total explained variance ratio{total_var:.2f}%\",\n        color=train_d[target].values\n    )\n\n    fig.update_traces(diagonal_visible=True,opacity=0.5)\n    fig.show()","6c0a2b79":"pca_plot1([f\"feature_{x}\" for x in range(0,130)],4,'resp')","b255842a":"def pca_plot_3d(features,target,nrows=10**4):\n    pca = PCA(n_components=3)\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    train_g_pca = pca.fit_transform(train_d[features])\n\n    total_var = pca.explained_variance_ratio_.sum()*100\n\n    fig = px.scatter_3d(\n        train_g_pca,x=0,y=1,z=2,\n        title=f\"Total explained variance ratio{total_var:.2f}%\",\n        color=train_d[target].values,\n        labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n    )\n\n    fig.show()","1673c6bd":"pca_plot_3d([f\"feature_{x}\" for x in range(0,130)],'resp')","aa59670e":"def plot_exp_var(features,nrows=10**4):\n    pca = PCA()\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    pca.fit(train_d[features])\n    exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\n    fig = px.area(\n        x=range(1, exp_var_cumul.shape[0] + 1),\n        y=exp_var_cumul,\n        labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n    )\n    fig.show()","6602bcab":"plot_exp_var([f\"feature_{x}\" for x in range(0,130)])","228d799f":"distribution1('date','red');","fd49ec46":"plt.figure(figsize=(15,10))\nplt.subplot(511)\nsns.kdeplot(train_data['resp_1'],color='#4285F4',shade=True,alpha=1)\nplt.subplot(512)\nsns.kdeplot(train_data['resp_2'],color='#EA4335',shade=True,alpha=1)\nplt.subplot(513)\nsns.kdeplot(train_data['resp_3'],color='#FBBC05',shade=True,alpha=1)\nplt.subplot(514)\nsns.kdeplot(train_data['resp_4'],color='#34A853',shade=True,alpha=1)\nplt.subplot(515)\nsns.kdeplot(train_data['resp'],color='#4285F4',shade=True,alpha=1);","42ba56a8":"def distribution2(feature):\n    sns.distplot(train_data.loc[train_data['resp']>0,feature],label='negative resp')\n    sns.distplot(train_data.loc[train_data['resp']<0,feature],label='positive resp')\n    plt.legend()","be8daded":"temp = train_data.groupby('action')[[f\"feature_{x}\" for x in range(130)]].mean()\ndiff_mean = (temp.iloc[1] - temp.loc[0]).abs()\nhighest_diff_in_mean = diff_mean.sort_values(ascending=False)[:10].index.tolist()","8f5a4e05":"plt.figure(figsize=(20,15))\nfor i,feature in enumerate(highest_diff_in_mean):\n    plt.subplot(2,5,i+1)\n    distribution2(feature)","d15e26cf":"px.bar(train_data['action'].value_counts(),color=['yellow','blue'],labels=['negative resp','positive resp'])","c7bb9dea":"distribution2('weight')","b26206cd":"feature_df = train_data[features+['date']].groupby('date').mean()\ncolors = ['#8ECAE6','#219EBC','#023047','#023047','#023047','#0E402D','#023047','#023047','#F77F00','#D62828']\n\nplt.figure(figsize=(20,20))\nfor i in range(0,10):\n    plt.subplot(10,1,i+1)\n    ax = sns.lineplot(x=feature_df.index,y=feature_df[f'feature_{i}'],color=colors[i],lw=1)\nplt.show()","d4ce9f1d":"plt.figure(figsize=(15,5))\nresp_date = train_data[['resp','date']].groupby('date').mean()\nsns.lineplot(x=[0,500],y=[0,0],lw=1.5)\nsns.lineplot(x=resp_date.index,y=resp_date['resp'],color='#dc2f02',lw=1);","0ce1d089":"dates = np.random.randint(low=0,high=500,size=10)\ncolors = ['#8ECAE6','#219EBC','#023047','#023047','#023047','#0E402D','#023047','#023047','#F77F00','#D62828']\nplt.figure(figsize=(20,20))\nfor i,date in enumerate(dates):\n    plt.subplot(10,1,i+1)\n    sns.lineplot(data=train_data.loc[train_data.date==date,'resp'],lw=1,color=colors[i])\n    plt.ylabel(date)\nplt.show()","b61af7c0":"features = [f'feature_{x}' for x in range(130)]\n\ntarget = 'action'\n\ntrain_mean = train_data.mean()\n\ntrain_data.fillna(train_mean,inplace=True)\ntrain_data['action'] = 0\ntrain_data.loc[train_data['resp']>0.0,'action'] = 1\n\ntarget = train_data[target].to_numpy()\ntrain_data = train_data[features].to_numpy()","71e15840":"params = {\n    'objective': 'binary',\n    'metrics':['auc'],\n}\n\nnfolds=3","cc3f693a":"kfold = StratifiedKFold(n_splits=nfolds)\nlgb_models = list()\n\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(train_data,target)): \n    \n    lgb_train = lgb.Dataset(train_data[train_idx],target[train_idx].ravel())\n    lgb_valid = lgb.Dataset(train_data[valid_idx],target[valid_idx].ravel())\n    \n    model = lgb.train(\n        params,\n        lgb_train,\n        valid_sets = [lgb_train,lgb_valid],\n        num_boost_round = 10000,\n        verbose_eval = 50,\n        early_stopping_rounds = 10,\n    )\n    \n    lgb_models.append(model)","aa4d660e":"predictions = np.zeros(test_data.shape[0])\n\nfor model in  lgb_models:\n    predictions += model.predict(test_data[features])\n    \npredictions \/= len(lgb_models)\nsns.distplot(predictions);","5e86c182":"del train_data, target\ngc.collect()","7424c4c0":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","4ebba09b":"for (test_df, sample_prediction_df) in iter_test:\n    \n    test_df.fillna(train_mean,inplace=True)\n    \n    prediction = 0\n    for model in lgb_models:\n        prediction += model.predict(test_df[features])[0]\n    \n    prediction \/= len(lgb_models)\n    prediction = prediction > 0.5\n    sample_prediction_df.action = prediction.astype(int)\n    env.predict(sample_prediction_df)","fd9d5af9":"# env.predict(sample)","f87ccf52":"submission = pd.read_csv('.\/submission.csv')\nsubmission.head()","b32759f6":"sns.countplot(submission.action);","677cdb27":"### 4.3 Distribution of resp","5ee22498":"## 4.4 Understanding Metrics \ud83d\udccf\ud83d\udcd0\n\nOne thing is clear that negative resp values contribute towards lower utility score and vice-verca.<br\/>\nso we need to develope a model that gives resp values for the feature but<br\/>\nwhat should be the action value for the negative and positive value of let's try to understand that<br\/>\n\nLets make a function which make n% correct prediction i.e score 'v' for n% positive resp values and score 0 for <br\/>\nn percent negative resp values and see what are utility scores.","2b057363":"\ud83d\ude06\ud83d\ude06\ud83d\ude06\nOh man EDA is fun.\nI mean look at it, It looks like those optical illusion images<br\/>","75eeb325":"## 4. Exploratory Data Analysis \ud83d\udcca\ud83d\udcc8\ud83d\udcc9\ud83d\udc40","278e217b":"Threre are many features which are highly correlated<br\/>\nI think we should look into some highly correlated features to see their distribution.\n\nI used plotly so that you can zoom into the feature.<br\/>\nYou can see that most of high correlated feature are near the center diagonal line<br\/>\nMost important patter is in the bottom right corner if you zoom into it you will see that<br\/>\nThere is alternate high and low correlation<br\/>\n\nAnother interesing one is 18 to 36 feature<br\/>","3c56b1e2":"There are too many features with null values right now we will just fill those values with mean values.","3eb19c4e":"### 4.2 Distribution of weights ","9e7e6d65":"### 4.4.2 plot utility score vs Accuracy for action 0.5 ","c3523474":"## 4.14 Mean Value of resp features over time","f3feb2d9":"## 4.13 Mean Value of some features over time","725b369e":"### 4.4.1 Utility Score vs action with 80% accuracy for train_data","fa0f058d":"## Jane Streek Market Price Prediction \ud83d\udcc8\n\n![image](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/23304\/logos\/thumb76_76.png?t=2020-11-16-17-41-20)\n\nToday's Trading system is lot depended on use of technology, market is evolving everyday and inorder to survive this dynamic nature of market one needs to use all the tools available to him\/her. <br\/>\n\nThe use of machine learning in market price prediction has been increased as machine learning models are very good at understading patterns and making predictions.\n\nMachine learning model along with human knowledge can help you to make proper market predictions.\n\n## what is competition about \ud83d\udca1?\n\nHere the challange is to make a model which can predict wether to accept a trade(1) or reject a trade(0)<br\/>\ngiven the data. \nSo it is sort of a classification problem.\n","a7fea60a":"We can see that there is positive linear relation ship between action and score which is obvious given the linear metrics<br\/>","9330b529":"## Importing Libraries \ud83d\udcd7.","c8c6db5f":"### 4.6 Pairplot for highly related features (18 to 39)","bd9d4709":"## 2. Metrics \ud83d\udcd0\n\nFor each row in the test set we have to predict an action value 1 to make the trade and 0 to pass on it.\n\neach trade has associated weight and resp score which will be used to calculate utility score which is the final score.\n\nFor each i\n\npi= \u2211j(weightij\u2217respij\u2217actionij)\n\nt= \u2211pi\/\u2211p**2i * \u221a250|i|\n\ni is no of unique dates in test set.\n\nutitily = min(max(t,0),6)\u2211pi.","a0830c73":"### 4.7.2 Ploting explained variance ","23a60a05":"### 4.5 Heatmap of features.","bf672979":"## Make Prediction","6f30f8cd":"Even thought these features have highet diff in mean of their resp.<br\/>\nit is difficult to see any difference in their graphs so it is difficult to seprate<br\/>\ndata using few features so we need group of features to seprate a model<br\/>\n\nIn all the dataset that I have seen there is atleast one feature that you can see distplot <br\/>\nand tell that there is a difference but this dataset is different.\n\nAnd almost every feature have mean value around zero which I think makes it easier for us<br\/>\nas it would not require any extra preprocessing.","c2bc3394":"## 4.10 Distplot of some features\n\nI am going to plot distribution plot of those 10 features which has highest difference in mean<br\/>\nof data with negative resps and positive resps.","3ac9c3d7":"## 4.8 Distribution of date \n\nDistribution of transaction per day","749de548":"## 4.7 Let's play with PCA","dec9b482":"## 3. Loading Data and Stats\ud83d\udcbd","c147a635":"It is good to see that there is not much disturbance in the trade happening per day<br\/>\nThat mean sufficient amount of trade happen everday.<br\/>\nwe do see that number of transaction at the start from 0 to 50 is high.<br\/>\nThere might be some reason for that.<br\/>\nBut there is no information about the month or year the data was produced so it is hard to tell the reason<br\/>","abff13ef":"## 4.11 Count of neg and pos resps in train_data","2db70cb7":"## 4.15 Value of resp over 10 random days","008ac72c":"## 4.9 Distribution of Resps","fa91f1f7":"## LGBM Model\n\nWe are going to make a baseline model which is simple classification model.<br\/>","1579f3eb":"### 4.1 Checking for Null Values","e6c729ec":"### 4.7.1 How good are 4 components of pca at seprating data","aa19b14b":"## 4.12 Distplot of weight with neg and pos resps","4bb247ba":"## 1. Given Data \ud83d\udcbd\n\nWe are provided with 4 files \n\n1. example_sample_submission.csv\n2. features.csv\n3. example_test.csv\n4. train.csv\n\ntrain.csv is the main file which contains feature_0 to feature_129 for each trade which are market data which we will use for making prediction and each trade has associated weight and resp which together represents a return on the trade.<br\/>\n\nIt also contain date column for day of trade. <br\/>\nts_id is for time ordering.<br\/>\n\nIt also has resp_1,2,3,4 for returns over different     time horizons, but these are not present in test data.\n\nfeatures.csv contains metadata of features.\n\nexample_test shows how private test data will look like.\n\nexample_sample_submission is example of how submission file should look like.\n\nWe have to use api provided by kaggle to make the prediction.\n\n\n","e9e28902":"### 4.4.3 Distribution of pi - (sum(weight * action * resp)) if action is 0.5","586c93da":"You will only get a utility score if your prediction is higher than 50%.","716a4969":"Number of datapoints in negative and positive resps are almost same which is a good news<br\/>\nAs we do not have to worry abot unbalanced dataset","1c9a9786":"One might find this one thing mentioned in the [data](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/data) section is confusing.<br\/>\n\n\"Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\"<br\/>\n\nAs you can see there are many rows that has weight 0<br\/>\n\nDo we have to remove those rows while training ? \ud83e\udd37\u200d\u2640\ufe0f<br\/>\n\nAnswer: \nMaybe we can use it while training because we are predicting resp (or action) value.<br\/>\nbut while calculating utility score for validation we should be carefull that<br\/>\nthere are not so many values with weight zerobecause as we see that in metrics <br\/>\npi is \u2211j(weightij\u2217respij\u2217actionij) so 0 weight will not<br\/>\ncontribute in utility score."}}