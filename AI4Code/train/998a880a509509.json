{"cell_type":{"84f5edf2":"code","c7c41428":"code","b5d49634":"code","f5777bf8":"code","5ab81136":"code","9fc17fb5":"code","a8339dc2":"code","f9910658":"code","9b3d33a2":"code","af3cfa28":"code","d9476697":"code","3fcd5515":"code","f2480000":"code","d1ef01a8":"code","bfbd19df":"code","45c3e941":"code","7a82b6d2":"code","68615942":"code","52f4f216":"code","c1757c5d":"code","b99de22c":"code","fb665c76":"code","7c379b75":"markdown","06fb90e0":"markdown","992af0e1":"markdown","1e82fc7e":"markdown","c1e44836":"markdown","3cd74842":"markdown","d2c9948b":"markdown","bf4031a1":"markdown","6e411a1f":"markdown"},"source":{"84f5edf2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7c41428":"df = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')\ndf.head()","b5d49634":"df.info()","f5777bf8":"df.describe()","5ab81136":"df.corr()","9fc17fb5":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n                      \nnumeric_cols = df.select_dtypes(include = ['float', 'int']).columns\n\ndef plot_numeric_classes(df, col, bins = 10, hist = True, kde = True):\n  sns.distplot(df[col],hist = hist, kde = kde, bins = bins)\n \ndef distribution_numeric(df, numeric_cols, figsize = (20, 15), row = 4, col = 3): \n  print('Number of numeric fields: ', len(numeric_cols))\n  plt.figure(figsize = figsize)\n  for i in range(1, len(numeric_cols), 1):\n    try:\n      plt.subplot(row, col, i)\n      plot_numeric_classes(df, numeric_cols[i - 1])\n      plt.title(numeric_cols[i - 1])\n    except:\n      print('Error: {}'.format(numeric_cols[i - 1]))\n      break \n\ndistribution_numeric(df, numeric_cols)\nplt.tight_layout()\n","a8339dc2":"categorical_cols = df.select_dtypes(include = ['O']).columns\n\ndef plot_categorical_classes(df, col):\n  df[col].value_counts().plot.bar()\n\ndef distribution_categorical(df, categorical_cols, row = 3, col = 3, figsize = (20, 15)):\n  print('Number of categorical: ', len(categorical_cols))\n  plt.figure(figsize = figsize)\n  for i in range(1, len(categorical_cols) + 1, 1):\n    try:\n      plt.subplot(row, col, i)\n      plot_categorical_classes(df, categorical_cols[i - 1])\n      plt.title(categorical_cols[i - 1])\n    except:\n      print('Error: {}'.format(categorical_cols[i - 1]))\n      break\n\ndistribution_categorical(df, categorical_cols)\nplt.tight_layout()","f9910658":"from sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size = 0.2, stratify = df['BAD'])\n'''\nThis stratify parameter makes a split so that \nthe proportion of values in the sample produced will be the same as the \nproportion of values provided to parameter stratify.\n\nFor example, if variable y is a binary categorical variable with values 0 and 1 \nand there are 25% of zeros and 75% of ones, stratify=y will make sure that your \nrandom split has 25% of 0's and 75% of 1's.\n'''\nX_train = df_train.copy()\ny_train = X_train.pop('BAD')\n\nX_test = df_test.copy()\ny_test = X_test.pop('BAD')\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","9b3d33a2":"#get numeric features and categorical features\nnumeric_feats = list(X_train.select_dtypes(include = ['float', 'int']).columns)\ncategorical_feats = list(X_train.select_dtypes('O').columns)","af3cfa28":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler","d9476697":"#Pipeline for categorical features\n\ncat_pl = Pipeline(\n    steps = [\n          ('imputer', SimpleImputer(strategy = 'most_frequent')),  #Fill missing data with the most frequent\n          ('onehot', OneHotEncoder())  #One hot encoding for categorical features\n    ]\n)","3fcd5515":"#Pipeline for numerical features\n\nnum_pl = Pipeline(\n    steps = [\n             ('imputer', KNNImputer()),\n             ('scaler', RobustScaler())\n    ]\n)","f2480000":"#Using ColumnTransformer to apply trasformer to dataframe\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers = [\n                    ('numerical', num_pl, numeric_feats),       #Apply transformers for numerical features\n                    ('categorical', cat_pl, categorical_feats)  #Apply transfomers for categorical features\n    ]\n)","d1ef01a8":"from sklearn.metrics import fbeta_score\nfrom sklearn.metrics import make_scorer\n\ndef fbeta(y_true, y_pred):\n  return fbeta_score(y_true, y_pred, beta = np.sqrt(1))\n\nmetrics = make_scorer(fbeta)","bfbd19df":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n#list model\nmodels = [RandomForestClassifier(), LogisticRegression(), KNeighborsClassifier(), MLPClassifier()]\nall_scores = []\n#Initial K-Fold\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 42)\n\n#Evaluate model\nfor model in models:\n  complete_pl = Pipeline(\n      steps = [\n               ('preprocessing', preprocessor),\n               ('classifier', model)\n      ]\n  )\n  scores = cross_val_score(complete_pl, X_train, y_train, scoring = metrics, cv = cv, n_jobs = -1)\n  all_scores.append(scores)","45c3e941":"print('Mean fbeta: {:.03f}, STD fbeta: {:.03f}'.format(np.mean(all_scores), np.std(all_scores)))","7a82b6d2":"model_names = ['RandomForest', 'Logistic', 'KNN', 'MLP']\n\n# Draw bboxplot \nplt.figure(figsize=(16, 8))\nplt.boxplot(all_scores)\nplt.xlabel('Model', fontsize=16)\nplt.ylabel('Fbeta', fontsize=16)\nplt.xticks(np.arange(len(model_names))+1, model_names, rotation=45, fontsize=16)\nplt.title(\"Scores Metrics\", fontsize=18)","68615942":"model_pl = Pipeline(\n    steps = [\n             ('pre', preprocessor),\n             ('rdfr', RandomForestClassifier())\n    ]\n)\n\nmodel_pl.fit(X_train, y_train)","52f4f216":"from sklearn.metrics import roc_curve, auc\ndef plot_roc(y_test, y_score):\n  fpr, tpr, threshold = roc_curve(y_test, y_score, pos_label = 1)\n  print(threshold)\n  plt.figure()\n  plt.plot(fpr, tpr, label = 'ROC curve (Area: %0.2f)' % auc(fpr, tpr))\n  plt.plot([0, 1], [0, 1])\n  plt.xlim([0, 1])\n  plt.ylim([0, 1.05])\n  plt.xlabel('False Possitive Rate')\n  plt.ylabel('True Possitive Rate')\n  plt.legend()\n  plt.show()","c1757c5d":"from sklearn.metrics import precision_score, recall_score, plot_confusion_matrix, confusion_matrix, classification_report\ndef evaluate_classifier(y_true, y_pred):\n  print('Precision: {:.03f}'.format(precision_score(y_true, y_pred, pos_label = 1)))\n  print('Recall: {:.03f}'.format(recall_score(y_true, y_pred, pos_label = 1)))\n  cfm = confusion_matrix(y_true, y_pred, normalize = 'true')\n  sns.heatmap(cfm, annot = True, cmap = 'Blues')\n  print(classification_report(y_test, y_pred))","b99de22c":"plot_roc(y_test, model_pl.predict_proba(X_test)[:, 1])","fb665c76":"evaluate_classifier(y_test, model_pl.predict(X_test))","7c379b75":"##***2.1 Cross Validation***","06fb90e0":"##***1.2 Split The Data***","992af0e1":"The Home Equity dataset (HMEQ) contains baseline and loan performance information for 5,960 recent home equity loans. The target (BAD) is a binary variable indicating whether an applicant eventually defaulted or was seriously delinquent. This adverse outcome occurred in 1,189 cases (20%). For each applicant, 12 input variables were recorded.\n\n\n*   **BAD**: 1 = client defaulted on loan 0 = loan repaid\n*   **LOAN**: Amount of the loan request\n*   **MORTDUE**: Amount due on existing mortgage\n*   **VALUE**: Value of current property\n*   **REASON**: DebtCon = debt consolidation HomeImp = home improvement\n*   **JOB**: Six occupational categories\n*   **YOJ**: Years at present job\n*   **DEROG**: Number of major derogatory reports\n*   **DELINQ**: Number of delinquent credit lines\n*   **CLAGE**: Age of oldest trade line in months\n\n\n\n\n\n\n\n\n","1e82fc7e":"#***2. Training Model***","c1e44836":"In this step, we will transform raw data to standard data when put into the model\n\n\n*   **OneHotEncoder**: for categorical data\n*   **SimpleImputer**: for missing data\n*   **MinMaxScaler**: standard data and remove the outliers\n\nTo control the preprocessing model step, we will create pipeline and put all step into it\n\n\n\n","3cd74842":"***Distribution***","d2c9948b":"##***1.1 Exploratory Data***","bf4031a1":"#**1. Design Pipeline**","6e411a1f":"##***1.3 Preprocessing Model***"}}