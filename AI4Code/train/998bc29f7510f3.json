{"cell_type":{"13332927":"code","7378ba68":"code","c2d4704c":"code","dd1e4498":"code","788c0b09":"code","ef519f38":"code","76d5b4ec":"code","642f8d94":"code","87f029e5":"code","41e8ad12":"code","691f58fe":"code","fede2484":"code","283f8327":"code","24974d06":"code","3e9e0732":"code","38f1d65b":"code","2d88e7e8":"code","7e6a1125":"code","f2f12ce1":"code","8232127c":"code","80e66bcc":"code","ade611e3":"code","000c16c3":"code","63cb5084":"code","523f5d3e":"code","38b707a1":"code","cca22570":"code","f60b0345":"markdown","0b9e8e6b":"markdown","d44b97eb":"markdown","54cba0ca":"markdown","61932796":"markdown","cee6b00e":"markdown","e5efd05e":"markdown","7c59771b":"markdown","cbfa415a":"markdown","2ceb3681":"markdown","0b399bc7":"markdown","a9e91bf8":"markdown","45fd7172":"markdown","31e7cbd6":"markdown","2757240e":"markdown","fcaba1f2":"markdown","cea0df4c":"markdown","7753cb8b":"markdown","2fa30b51":"markdown","58ea98d4":"markdown","edddf60c":"markdown","096cd880":"markdown","014951eb":"markdown","47744b10":"markdown","b2c43cd7":"markdown","c7cec08a":"markdown","488e7731":"markdown","6955fea0":"markdown","997c459f":"markdown","a9b986b5":"markdown"},"source":{"13332927":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb","7378ba68":"data = pd.read_excel('..\/input\/telecom-customer-churn-prediction\/Telco_Cust_Churn.xls')\n\ndata.head()","c2d4704c":"data.info()","dd1e4498":"data.TotalCharges = pd.to_numeric(data.TotalCharges)","788c0b09":"data = data.replace(' ', value=0)\ndata.TotalCharges = pd.to_numeric(data.TotalCharges)\ndata.TotalCharges.dtype","ef519f38":"g = sns.countplot(x=\"Churn\",data=data, palette=\"muted\")\ng.set_ylabel(\"Customers\", fontsize=14)\ng.set_xlabel(\"Churn\", fontsize=14)\n\ndata.Churn.value_counts(normalize=True)","76d5b4ec":"data = data.drop('customerID', axis = 1)","642f8d94":"catagorical = [i for i in data.columns if data[i].dtypes == 'object']\n\nfor i in catagorical:   \n    print(i, ':', data[i].unique())","87f029e5":"data = data.replace(regex=r'No\\s[a-z]+\\sservice', value='No')\n\nfor i in catagorical:   \n    if len(data[i].unique()) == 2:        \n        data[i] = data[i].map({'Male': 0, 'Female': 1, 'No': 0, 'Yes': 1})","41e8ad12":"catagorical = [i for i in data.columns if data[i].dtypes == 'object']\n\nfor i in catagorical:   \n    print(i, ':', data[i].unique())","691f58fe":"data = data.replace(regex=r'\\s\\(automatic\\)', value='')\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\nplt.subplots_adjust(wspace=0.05, hspace=0.05)\n\ng1 = sns.barplot(ax=axes[0], x=\"InternetService\",y=\"Churn\",data=data, palette=\"muted\")\ng1.set_xlabel(\"InternetService\", fontsize=14)\ng1.set_ylabel(\"Churn probability\", fontsize=14)\n\ng2 = sns.barplot(ax=axes[1], x=\"Contract\",y=\"Churn\",data=data, palette=\"muted\")\ng2.set_xlabel(\"Contract\", fontsize=14)\ng2.set_ylabel('')\n\ng3 = sns.barplot(ax=axes[2], x=\"PaymentMethod\",y=\"Churn\",data=data, palette=\"muted\")\ng3.set_xlabel(\"PaymentMethod\", fontsize=14)\ng3.set_ylabel('')\ng3.set_xticklabels(labels=data.PaymentMethod.unique(), fontsize=8)","fede2484":"internetservice_keys = data.InternetService.unique()\ninternetservice_mapping = dict(zip(internetservice_keys, [1, 2, 0]))\ndata.InternetService = data.InternetService.map(internetservice_mapping)\n\ncontract_keys = data.Contract.unique()\ncontract_mapping = dict(zip(contract_keys, [2, 1, 0]))\ndata.Contract = data.Contract.map(contract_mapping)","283f8327":"data.head()","24974d06":"corrmat = data.corr()\nf, ax = plt.subplots(figsize = (12, 9))\nsns.heatmap(corrmat, vmax = 0.8, square = True, cmap = \"coolwarm\")","3e9e0732":"g = sns.lineplot(x=\"tenure\", y=\"Churn\", data=data, palette=\"muted\")\ng.set_xlabel(\"tenure\", fontsize=14)\ng.set_ylabel(\"Churn probability\", fontsize=14)","38f1d65b":"mask = data.index[data.tenure == 0]\ndata.loc[mask, 'tenure'] = data.tenure.max() + 1\n\ng = sns.lineplot(x=\"tenure\", y=\"Churn\", data=data, palette=\"muted\")\ng.set_xlabel(\"tenure\", fontsize=14)\ng.set_ylabel(\"Churn probability\", fontsize=14)","2d88e7e8":"data['tenureBand'] = pd.cut(data.tenure, 3)\n\ng = sns.barplot(x=\"tenureBand\", y=\"Churn\", data=data, palette=\"muted\")\ng.set_xlabel(\"tenureBand\", fontsize=14)\ng.set_ylabel(\"Churn probability\", fontsize=14)\n\ndata = data.drop('tenure', axis = 1)","7e6a1125":"tenureBand_keys = data.tenureBand.unique()\ntenureBand_mapping = dict(zip(tenureBand_keys, [2, 1, 0]))\ndata.tenureBand = data.tenureBand.map(tenureBand_mapping)\n\ndata.tenureBand = pd.to_numeric(data.tenureBand)","f2f12ce1":"data = pd.get_dummies(data)\ndata.head()","8232127c":"y = data.Churn.values\n\nx = data.drop('Churn', axis = 1).values","80e66bcc":"x = MinMaxScaler().fit_transform(x)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=51)","ade611e3":"x_smote, y_smote = SMOTE().fit_resample(x, y)\n\nx_nearmiss, y_nearmiss = NearMiss(version=3).fit_resample(x, y)","000c16c3":"classifiers = []\nclassifiers.append(LogisticRegression(max_iter=500))\nclassifiers.append(RandomForestClassifier())\nclassifiers.append(AdaBoostClassifier())\nclassifiers.append(xgb.XGBClassifier())\nclassifiers.append(SVC(C = 10))","63cb5084":"def cross_validate(classifiers, x_train, y_train, x_test, y_test):\n    cv_results = []\n    for classifier in classifiers:\n        cv_results.append((cross_val_score(classifier, x_train, y_train,\\\n                                           scoring = \"f1_weighted\")).mean())\n    best_clf = classifiers[np.argmax(cv_results)]\n    return best_clf\n\nbest_clf_wo_balancing = cross_validate(classifiers, x_train, y_train, x_test, y_test)\nbest_clf_smote = cross_validate(classifiers, x_smote, y_smote, x_test, y_test)\nbest_clf_nearmiss = cross_validate(classifiers, x_nearmiss, y_nearmiss, x_test, y_test)","523f5d3e":"def print_best_clf_results(best_clf, x_train, y_train, x_test, y_test):\n    \n    print(f'\\nThe classifier with the best f1 result is: {best_clf}')\n    \n    best_clf.fit(x_train, y_train)\n    y_pred = best_clf.predict(x_test)\n    \n    print('\\nClassification Report:')\n    print(classification_report(y_test, y_pred))\n    print('\\n\\n')\n    f_i = best_clf.feature_importances_\n    return f_i","38b707a1":"print('-'*70)\nprint('Without balncing:')\nprint('-'*70)\nf_i = print_best_clf_results(best_clf_wo_balancing, x_train, y_train, x_test, y_test)\n\nprint('-'*70)\nprint('Over-sampling using SMOTE technique:')\nprint('-'*70)\nf_i_smote = print_best_clf_results(best_clf_smote, x_smote, y_smote, x_test, y_test)\n\nprint('-'*70)\nprint('Under-sampling using NearMiss3 technique:')\nprint('-'*70)\nf_i_nearmiss = print_best_clf_results(best_clf_nearmiss, x_nearmiss, y_nearmiss, x_test, y_test)","cca22570":"col_names = data.columns.drop('Churn')\n\nfeature_importances = pd.DataFrame({'Feature': col_names, 'Feature Importances': f_i_smote})\nfeature_importances = feature_importances.sort_values('Feature Importances', ascending=False, ignore_index=True).head()\n\n#aligning text to the left:\nfeature_importances = feature_importances.style.set_properties(**{'text-align': 'left'})\nfeature_importances = feature_importances.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\nfeature_importances","f60b0345":"**Heatmap:**","0b9e8e6b":"# Testing the models:","d44b97eb":"I will check if some of them can be considerd as ordinal:","54cba0ca":"#  Importing the data:","61932796":"**The Random Forest Classifier got an outstanding result on the over-sampling data using SMOTE technique!**\n\nWe can see that without balancing the data, we got an ok accuracy with AdaBoost algorithm but the precision and recall of the churn customer is very bad. It's means that the model is not reliable and it's not good in recognaizing the churn customer correctly - our goal. This is why it's always importent to balnce the data.\nThe NearMiss technique got an even worst results, it's probably from losing importent data from the under-sampling.","cee6b00e":"We can see that for each customer we have some demographic info, the services he uses and information about his contract. ","e5efd05e":"# Preprocessing:","7c59771b":"Nice:), it's look like we can divide the \"tenure\" feature to 3 part: ","cbfa415a":"Like we saw on the heatmap, **the features about the contract - charge, type and tenure, are the ones that affect the most about whether a customer is going to leave or not.**","2ceb3681":"Normalizing the data and splitting it to train and test:","0b399bc7":"The feature \"TotalCharges\" is a product of the features \"tenure\" * \"MonthlyCharges\". In the sampels where \"tenure\" = 0, there was suppose to be \"TotalCharges\" = 0, but instead there is just a space \" \". I will fix it.","a9e91bf8":"First of all, I will arange the data so the algorithm could work with it:","45fd7172":"The first column, \"customerID\", is unnecessary so I will drop it.","31e7cbd6":"The churn customers are only a quarter from the data. Later I will balanced it to get a better model.","2757240e":"**The Random Forest Classifier in combination with SMOTE technique got 100% correct predictions on the test data :)**\n\nLet's check which features were the most important for the predictions:","fcaba1f2":"I will check the results of this 5 classifiers:","cea0df4c":"Customer churn is the percentage of customers that stopped using the company\u2019s product or service during a given time period, and it's one of the most important metrics for businesses to evaluate. Customer churn is a critical metric because it is much less expensive to retain existing customers than it's to acquire new ones.\n\nA model that can get a good prediction whether a customer is going to leave or not, could be a valuable asset to companies. The companies could improve their customer retention by knowing on whitch customers they need to put their afffort to preserve.\nIn this notebook I built this kind of model using the \"Telcom Customer Churn\" dataset. Telcom companies experiencing a lot of fluctuation in their customers and this model can be very usfull to this type of companies.\n\nIn addition, I will demonstrate the importance of balancing the train data (in case that is imbalanced). When dealing with churn data, useally the churn precantage will be very low (if it's not, it's a much bigger problem that even this model won't help..)","7753cb8b":"Now I can also change this feature to be ordinal:","2fa30b51":"We can see, like we expected, there is a strong correlation between \"tenure\" (or \"MonthlyCharges\") and \"TotalCharges\" because \"TotalCharges\" = \"tenure\" * \"MonthlyCharges\". In addition, we can see that there is a strong correlation between \"OnlineSecurity\" and \"MonthlyCharges\". This is probably because the \"fiber optic\" is the most expensive service and this is the reason for high monthly charges. It's also can explain why the churn probability is higher for the customers who own this service - the price is too high for them.\nAnother strong negative correlation is between \"tenure\" and \"Contract\".It make sense that the higher the tenure - the contract will be yearly and not monthly.\nFinaly if we look at the stronget correlation with \"Churn\", it's \"tenure\". I will check it out:","58ea98d4":"Now I will check if the data is indeed imbalanced:","edddf60c":"We have a lot of \"yes\\no\" features (also in the \"Churn\" column), I will change them and the \"gender\" feature (male\\female) to 0 and 1.\nBefore that I will change the 'No phone\/internet service' to 'No', because it's not adding any new informaion (we have these information in the \"PhoneService\" and \"InternetService\" features).","096cd880":"We can see that we don't have any nulls, great!\nBut the type of feature \"TotalCharges\" suppose to be 'float64', not 'object', because it's numerical. I will check why:","014951eb":"We can clearly see that the features \"InternetService\" and \"Contract\" can be considerd ordinals and they have good correlation with the churn labels. So I will change them accordingly.","47744b10":"Let's check what kind of catagorical data do we have:","b2c43cd7":"I will evaluate this 5 classifiers by using cross validation. This next function finds the best classifier, the classifier who got the best score in the cross validation. I will run it for each of the balancing techniques and without balancing at all.","c7cec08a":"We can clearly see the trend that the higher the tenure - the lesser chance the customer will leave, make sense. I will fix the anomaly at the beggining where the tenure is 0 - it's a new customer that just joined the compeny services so there is a 0 chance for him to be a churn customer. I will change the 0 tenure to be the maximum tenure so it will fit the trend.","488e7731":"This function is for printing the classification report on the test data for each of the best classifiers:","6955fea0":"The catagorical features that remained after the changes:","997c459f":"To deal with the imbalanced data, I will check two techniques:\n**1.**  **Over-sampling - SMOTE:**\n    Synthetic Minority Over-sampling Technique\n    SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic \n    instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line \n    segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a \n    and b. (Page 47, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.)\n\n**2.**  **Under-sampling - NearMiss3:**\n    When instances of two different classes are very close to each other, it removes the instances of the majority class to \n    increase the spaces between the two classes. It works in 2 steps: Firstly, for each minority class instance, their M \n    nearest-neighbors will be stored. Then finally, the majority class instances are selected for which the average distance to \n    the N nearest-neighbors is the largest.","a9b986b5":"# Fitting the model:"}}