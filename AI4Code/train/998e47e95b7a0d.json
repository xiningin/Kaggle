{"cell_type":{"3702645e":"code","9b8a8e68":"code","50df8474":"code","ddcca398":"code","28fb2c1e":"code","1b6dd262":"code","35d75802":"code","006df135":"code","3fddde83":"code","aba232b8":"code","97a4be33":"code","b92ede46":"code","10f392b8":"code","b808d851":"code","0a9fc417":"code","2afff3ff":"code","3f465412":"code","d9840042":"code","4d4fde21":"code","652a3c3a":"code","4c87b1a3":"code","500dd73a":"code","7a7c456d":"markdown","65e9765b":"markdown","6666abf8":"markdown","0033cc6f":"markdown","638a4f8e":"markdown","05661131":"markdown","a34ed024":"markdown","239cd31f":"markdown","08e9a1e0":"markdown","08196745":"markdown","034d1520":"markdown","246ee547":"markdown","7a99f145":"markdown","c454ab9b":"markdown"},"source":{"3702645e":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9b8a8e68":"path = '..\/input\/diamonds\/diamonds.csv'\ndf = pd.read_csv(path)\ndf.drop('Unnamed: 0', inplace=True, axis=1)\ndf.head()","50df8474":"cut_to_idx = {v:k for k,v in enumerate(df['cut'].unique())}\ncolor_to_idx = {v:k for k,v in enumerate(df['color'].unique())}\nclarity_to_idx = {v:k for k,v in enumerate(df['clarity'].unique())}","ddcca398":"df['cut'].replace(cut_to_idx, inplace=True)\ndf['color'].replace(color_to_idx, inplace=True)\ndf['clarity'].replace(clarity_to_idx, inplace=True)\n\ndf.head()","28fb2c1e":"maxes = {}\nfor col in list(df.columns):\n    maxes[col] = df[col].max()\n    df[col] \/= df[col].max()","1b6dd262":"df.head()","35d75802":"plt.figure(figsize=(16, 6))\n\nheatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","006df135":"y = df['price'].values\nX = df[['x','y','z']].values","3fddde83":"def tanh(x):\n    return np.tanh(x)\n\ndef d_tanh(x):\n    return 1 - np.tanh(x) ** 2\n\ndef arctan(x):\n    return np.arctan(x)\n\ndef d_arctan(x):\n    return 1 \/ (1 + x ** 2)\n\ndef iden(x):\n    return x\n\ndef d_iden(x):\n    return 1\n\ndef d_abs(x):\n    mask = (x >= 0) *1.0\n    mask2 = (x<0) * -1.0\n    return mask + mask2","aba232b8":"min_range = np.min(X)\nmax_range = np.max(X)","97a4be33":"num_epoch = 200\nlearning_rate = 0.0004\nalpha = 3\nX_with_bias = np.insert(X,0,1,axis=1)\nX_with_bias[:,[0, 1]] = X_with_bias[:,[1, 0]]\ny_with_dim = np.expand_dims(y, axis=1)","b92ede46":"theta_with_bias = np.array([np.linspace(min_range , max_range, 1000) for a in range(4)])\ntheta_with_bias = np.swapaxes(theta_with_bias,0,1)","10f392b8":"np.random.seed(456789)\n\nw1 = np.random.randn(4, 100)\nw2 = np.random.randn(100, 104)\nw3 = np.random.randn(104, 200)\nw4 = np.random.randn(200, 1)","b808d851":"w1_l1,w2_l1,w3_l1,w4_l1 = w1,w2,w3,w4\nw1_l2,w2_l2,w3_l2,w4_l2 = w1,w2,w3,w4\n\nw1_l1_reg,w2_l1_reg,w3_l1_reg,w4_l1_reg = w1,w2,w3,w4\nw1_l2_reg,w2_l2_reg,w3_l2_reg,w4_l2_reg = w1,w2,w3,w4\n\nw1_l1_l2_reg,w2_l1_l2_reg,w3_l1_l2_reg,w4_l1_l2_reg = w1,w2,w3,w4\nw1_l2_l1_reg,w2_l2_l1_reg,w3_l2_l1_reg,w4_l2_l1_reg = w1,w2,w3,w4","0a9fc417":"for iter in range(num_epoch):\n    \n    layer_1 = X_with_bias.dot(w1_l1)\n    layer_1_act = tanh(layer_1)\n\n    layer_2 = layer_1_act.dot(w2_l1)\n    layer_2_act = iden(layer_2)\n\n    layer_3 = layer_2_act.dot(w3_l1)\n    layer_3_act = arctan(layer_3)\n\n    layer_4 = layer_3_act.dot(w4_l1)\n    layer_4_act = iden(layer_4)\n\n    cost = np.abs(layer_4_act - y_with_dim).sum()  \/ len(X)\n    print(\"Current Iter: \",iter, \" current cost: \",cost,end=\"\\r\")\n\n    grad_4_part_1 = d_abs(layer_4_act - y_with_dim)\/ len(X)\n    grad_4_part_2 = d_iden(layer_4)\n    grad_4_part_3 = layer_3_act\n    grad_4 =    grad_4_part_3.T.dot(grad_4_part_1*grad_4_part_2) \n\n    grad_3_part_1 = (grad_4_part_1 * grad_4_part_2).dot(w4_l1.T)\n    grad_3_part_2 = d_arctan(layer_3)\n    grad_3_part_3 = layer_2_act\n    grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)\n\n    grad_2_part_1 =  (grad_3_part_1 * grad_3_part_2).dot(w3_l1.T)\n    grad_2_part_2 = d_iden(layer_2)\n    grad_2_part_3 = layer_1_act\n    grad_2 =     grad_2_part_3.T.dot(grad_2_part_1*grad_2_part_2)\n\n    grad_1_part_1 =  (grad_2_part_1 * grad_2_part_2).dot(w2_l1.T)\n    grad_1_part_2 = d_tanh(layer_1)\n    grad_1_part_3 = X_with_bias\n    grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 * grad_1_part_2)  \n\n    w4_l1 = w4_l1 - learning_rate * grad_4\n    w3_l1 = w3_l1 - learning_rate * grad_3\n    w2_l1 = w2_l1 - learning_rate * grad_2\n    w1_l1 = w1_l1 - learning_rate * grad_1\n\nprint(\"Case 1 final error :\",cost)\nlayer_1 = theta_with_bias.dot(w1_l1)\nlayer_1_act = tanh(layer_1)\nlayer_2 = layer_1_act.dot(w2_l1)\nlayer_2_act = iden(layer_2)\nlayer_3 = layer_2_act.dot(w3_l1)\nlayer_3_act = arctan(layer_3)\nlayer_4 = layer_3_act.dot(w4_l1)\nlayer_4_l1 = iden(layer_4)","2afff3ff":"for iter in range(num_epoch):\n    \n    layer_1 = X_with_bias.dot(w1_l2)\n    layer_1_act = tanh(layer_1)\n\n    layer_2 = layer_1_act.dot(w2_l2)\n    layer_2_act = iden(layer_2)\n\n    layer_3 = layer_2_act.dot(w3_l2)\n    layer_3_act = arctan(layer_3)\n\n    layer_4 = layer_3_act.dot(w4_l2)\n    layer_4_act = iden(layer_4)\n\n    cost = np.square(layer_4_act - y_with_dim).sum() \/ len(X)\n    print(\"Current Iter: \",iter, \" current cost: \",cost,end=\"\\r\")\n\n    grad_4_part_1 = 2.0 * (layer_4_act - y_with_dim) \/ len(X)\n    grad_4_part_2 = d_iden(layer_4)\n    grad_4_part_3 = layer_3_act\n    grad_4 =    grad_4_part_3.T.dot(grad_4_part_1*grad_4_part_2) \n\n    grad_3_part_1 = (grad_4_part_1 * grad_4_part_2).dot(w4_l2.T)\n    grad_3_part_2 = d_arctan(layer_3)\n    grad_3_part_3 = layer_2_act\n    grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)\n\n    grad_2_part_1 =  (grad_3_part_1 * grad_3_part_2).dot(w3_l2.T)\n    grad_2_part_2 = d_iden(layer_2)\n    grad_2_part_3 = layer_1_act\n    grad_2 =     grad_2_part_3.T.dot(grad_2_part_1*grad_2_part_2)\n\n    grad_1_part_1 =  (grad_2_part_1 * grad_2_part_2).dot(w2_l2.T)\n    grad_1_part_2 = d_tanh(layer_1)\n    grad_1_part_3 = X_with_bias\n    grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 * grad_1_part_2)  \n\n    w4_l2 = w4_l2 - learning_rate * grad_4 \n    w3_l2 = w3_l2 - learning_rate * grad_3 \n    w2_l2 = w2_l2 - learning_rate * grad_2 \n    w1_l2 = w1_l2 - learning_rate * grad_1\n\nprint(\"Case 2 final error :\",cost)    \nlayer_1 = theta_with_bias.dot(w1_l2)\nlayer_1_act = tanh(layer_1)\nlayer_2 = layer_1_act.dot(w2_l2)\nlayer_2_act = iden(layer_2)\nlayer_3 = layer_2_act.dot(w3_l2)\nlayer_3_act = arctan(layer_3)\nlayer_4 = layer_3_act.dot(w4_l2)\nlayer_4_l2 = iden(layer_4)","3f465412":"for iter in range(num_epoch):\n    \n    layer_1 = X_with_bias.dot(w1_l1_reg)\n    layer_1_act = tanh(layer_1)\n\n    layer_2 = layer_1_act.dot(w2_l1_reg)\n    layer_2_act = iden(layer_2)\n\n    layer_3 = layer_2_act.dot(w3_l1_reg)\n    layer_3_act = arctan(layer_3)\n\n    layer_4 = layer_3_act.dot(w4_l1_reg)\n    layer_4_act = iden(layer_4)\n\n    cost = np.abs(layer_4_act - y_with_dim).sum()  \/ len(X) + alpha*(np.abs(w1_l1_reg).sum() +\n                                                                    np.abs(w2_l1_reg).sum() +\n                                                                    np.abs(w3_l1_reg).sum() +\n                                                                    np.abs(w4_l1_reg).sum()  )\n    print(\"Current Iter: \",iter, \" current cost: \",cost,end=\"\\r\")\n\n    grad_4_part_1 = d_abs(layer_4_act - y_with_dim)\/ len(X)\n    grad_4_part_2 = d_iden(layer_4)\n    grad_4_part_3 = layer_3_act\n    grad_4 =    grad_4_part_3.T.dot(grad_4_part_1*grad_4_part_2) \n\n    grad_3_part_1 = (grad_4_part_1 * grad_4_part_2).dot(w4_l1_reg.T)\n    grad_3_part_2 = d_arctan(layer_3)\n    grad_3_part_3 = layer_2_act\n    grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)\n\n    grad_2_part_1 =  (grad_3_part_1 * grad_3_part_2).dot(w3_l1_reg.T)\n    grad_2_part_2 = d_iden(layer_2)\n    grad_2_part_3 = layer_1_act\n    grad_2 =     grad_2_part_3.T.dot(grad_2_part_1*grad_2_part_2)\n\n    grad_1_part_1 =  (grad_2_part_1 * grad_2_part_2).dot(w2_l1_reg.T)\n    grad_1_part_2 = d_tanh(layer_1)\n    grad_1_part_3 = X_with_bias\n    grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 * grad_1_part_2)  \n\n\n    w4_l1_reg = w4_l1_reg - learning_rate * (grad_4 + alpha *  d_abs(w4_l1_reg))\n    w3_l1_reg = w3_l1_reg - learning_rate * (grad_3 + alpha *  d_abs(w3_l1_reg))\n    w2_l1_reg = w2_l1_reg - learning_rate * (grad_2 + alpha *  d_abs(w2_l1_reg))\n    w1_l1_reg = w1_l1_reg - learning_rate * (grad_1 + alpha *  d_abs(w1_l1_reg))\n\nprint(\"Case 3 final error :\",cost) \nlayer_1 = theta_with_bias.dot(w1_l1_reg)\nlayer_1_act = tanh(layer_1)\nlayer_2 = layer_1_act.dot(w2_l1_reg)\nlayer_2_act = iden(layer_2)\nlayer_3 = layer_2_act.dot(w3_l1_reg)\nlayer_3_act = arctan(layer_3)\nlayer_4 = layer_3_act.dot(w4_l1_reg)\nlayer_4_l1_reg = iden(layer_4)","d9840042":"for iter in range(num_epoch):\n    \n    layer_1 = X_with_bias.dot(w1_l2_reg)\n    layer_1_act = tanh(layer_1)\n\n    layer_2 = layer_1_act.dot(w2_l2_reg)\n    layer_2_act = iden(layer_2)\n\n    layer_3 = layer_2_act.dot(w3_l2_reg)\n    layer_3_act = arctan(layer_3)\n\n    layer_4 = layer_3_act.dot(w4_l2_reg)\n    layer_4_act = iden(layer_4)\n\n    cost = (np.square(layer_4_act - y_with_dim).sum() \/ len(X)) + alpha * ( np.sum(w4_l2_reg ** 2)  + \n                                                                        np.sum(w3_l2_reg ** 2) +\n                                                                        np.sum(w2_l2_reg ** 2) +\n                                                                        np.sum(w1_l2_reg ** 2))\n\n    print(\"Current Iter: \",iter, \" current cost: \",cost,end=\"\\r\")\n\n    grad_4_part_1 = 2*(layer_4_act - y_with_dim) \/ len(X)\n    grad_4_part_2 = d_iden(layer_4)\n    grad_4_part_3 = layer_3_act\n    grad_4 =    grad_4_part_3.T.dot(grad_4_part_1*grad_4_part_2) \n\n    grad_3_part_1 = (grad_4_part_1 * grad_4_part_2).dot(w4_l2_reg.T)\n    grad_3_part_2 = d_arctan(layer_3)\n    grad_3_part_3 = layer_2_act\n    grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)\n\n    grad_2_part_1 =  (grad_3_part_1 * grad_3_part_2).dot(w3_l2_reg.T)\n    grad_2_part_2 = d_iden(layer_2)\n    grad_2_part_3 = layer_1_act\n    grad_2 =     grad_2_part_3.T.dot(grad_2_part_1*grad_2_part_2)\n\n    grad_1_part_1 =  (grad_2_part_1 * grad_2_part_2).dot(w2_l2_reg.T)\n    grad_1_part_2 = d_tanh(layer_1)\n    grad_1_part_3 = X_with_bias\n    grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 * grad_1_part_2)  \n\n    w4_l2_reg = w4_l2_reg - learning_rate * (grad_4 + 2*alpha * w4_l2_reg)\n    w3_l2_reg = w3_l2_reg - learning_rate * (grad_3 + 2*alpha * w3_l2_reg)\n    w2_l2_reg = w2_l2_reg - learning_rate * (grad_2 + 2*alpha * w2_l2_reg)\n    w1_l2_reg = w1_l2_reg - learning_rate * (grad_1 + 2*alpha * w1_l2_reg)\n    \nprint(\"Case 4 final error :\",cost) \nlayer_1 = theta_with_bias.dot(w1_l2_reg)\nlayer_1_act = tanh(layer_1)\nlayer_2 = layer_1_act.dot(w2_l2_reg)\nlayer_2_act = iden(layer_2)\nlayer_3 = layer_2_act.dot(w3_l2_reg)\nlayer_3_act = arctan(layer_3)\nlayer_4 = layer_3_act.dot(w4_l2_reg)\nlayer_4_l2_reg = iden(layer_4)","4d4fde21":"for iter in range(num_epoch):\n    \n    layer_1 = X_with_bias.dot(w1_l1_l2_reg)\n    layer_1_act = tanh(layer_1)\n\n    layer_2 = layer_1_act.dot(w2_l1_l2_reg)\n    layer_2_act = iden(layer_2)\n\n    layer_3 = layer_2_act.dot(w3_l1_l2_reg)\n    layer_3_act = arctan(layer_3)\n\n    layer_4 = layer_3_act.dot(w4_l1_l2_reg)\n    layer_4_act = iden(layer_4)\n\n    cost = np.abs(layer_4_act - y_with_dim).sum()  \/ len(X) + alpha * ( np.sum(w4_l1_l2_reg ** 2)  + \n                                                                        np.sum(w3_l1_l2_reg ** 2) +\n                                                                        np.sum(w2_l1_l2_reg ** 2) +\n                                                                        np.sum(w1_l1_l2_reg ** 2))\n    print(\"Current Iter: \",iter, \" current cost: \",cost,end=\"\\r\")\n\n    grad_4_part_1 = d_abs(layer_4_act - y_with_dim)\/ len(X)\n    grad_4_part_2 = d_iden(layer_4)\n    grad_4_part_3 = layer_3_act\n    grad_4 =    grad_4_part_3.T.dot(grad_4_part_1*grad_4_part_2) \n\n    grad_3_part_1 = (grad_4_part_1 * grad_4_part_2).dot(w4_l1_l2_reg.T)\n    grad_3_part_2 = d_arctan(layer_3)\n    grad_3_part_3 = layer_2_act\n    grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)\n\n    grad_2_part_1 =  (grad_3_part_1 * grad_3_part_2).dot(w3_l1_l2_reg.T)\n    grad_2_part_2 = d_iden(layer_2)\n    grad_2_part_3 = layer_1_act\n    grad_2 =     grad_2_part_3.T.dot(grad_2_part_1*grad_2_part_2)\n\n    grad_1_part_1 =  (grad_2_part_1 * grad_2_part_2).dot(w2_l1_l2_reg.T)\n    grad_1_part_2 = d_tanh(layer_1)\n    grad_1_part_3 = X_with_bias\n    grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 * grad_1_part_2)  \n\n\n    w4_l1_l2_reg = w4_l1_l2_reg - learning_rate * (grad_4 + 2*alpha * w4_l1_l2_reg)\n    w3_l1_l2_reg = w3_l1_l2_reg - learning_rate * (grad_3 + 2*alpha * w3_l1_l2_reg)\n    w2_l1_l2_reg = w2_l1_l2_reg - learning_rate * (grad_2 + 2*alpha * w2_l1_l2_reg)\n    w1_l1_l2_reg = w1_l1_l2_reg - learning_rate * (grad_1 + 2*alpha * w1_l1_l2_reg)\n\nprint(\"Case 5 final error :\",cost) \nlayer_1 = theta_with_bias.dot(w1_l1_l2_reg)\nlayer_1_act = tanh(layer_1)\nlayer_2 = layer_1_act.dot(w2_l1_l2_reg)\nlayer_2_act = iden(layer_2)\nlayer_3 = layer_2_act.dot(w3_l1_l2_reg)\nlayer_3_act = arctan(layer_3)\nlayer_4 = layer_3_act.dot(w4_l1_l2_reg)\nlayer_4_l1_l2_reg = iden(layer_4)","652a3c3a":"for iter in range(num_epoch):\n    \n    layer_1 = X_with_bias.dot(w1_l2_l1_reg)\n    layer_1_act = tanh(layer_1)\n    layer_2 = layer_1_act.dot(w2_l2_l1_reg)\n    layer_2_act = iden(layer_2)\n    layer_3 = layer_2_act.dot(w3_l2_l1_reg)\n    layer_3_act = arctan(layer_3)\n    layer_4 = layer_3_act.dot(w4_l2_l1_reg)\n    layer_4_act = iden(layer_4)\n\n    cost = (np.square(layer_4_act - y_with_dim).sum() \/ len(X)) + alpha*(np.abs(w1_l2_l1_reg).sum() +\n                                                                    np.abs(w2_l2_l1_reg).sum() +\n                                                                    np.abs(w3_l2_l1_reg).sum() +\n                                                                    np.abs(w4_l2_l1_reg).sum()  )\n\n    print(\"Current Iter: \",iter, \" current cost: \",cost,end=\"\\r\")\n\n    grad_4_part_1 = 2*(layer_4_act - y_with_dim) \/ len(X)\n    grad_4_part_2 = d_iden(layer_4)\n    grad_4_part_3 = layer_3_act\n    grad_4 =    grad_4_part_3.T.dot(grad_4_part_1*grad_4_part_2) \n\n    grad_3_part_1 = (grad_4_part_1 * grad_4_part_2).dot(w4_l2_l1_reg.T)\n    grad_3_part_2 = d_arctan(layer_3)\n    grad_3_part_3 = layer_2_act\n    grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)\n\n    grad_2_part_1 =  (grad_3_part_1 * grad_3_part_2).dot(w3_l2_l1_reg.T)\n    grad_2_part_2 = d_iden(layer_2)\n    grad_2_part_3 = layer_1_act\n    grad_2 =     grad_2_part_3.T.dot(grad_2_part_1*grad_2_part_2)\n\n    grad_1_part_1 =  (grad_2_part_1 * grad_2_part_2).dot(w2_l2_l1_reg.T)\n    grad_1_part_2 = d_tanh(layer_1)\n    grad_1_part_3 = X_with_bias\n    grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 * grad_1_part_2)  \n\n    w4_l2_l1_reg = w4_l2_l1_reg - learning_rate * (grad_4  + alpha *  d_abs(w4_l2_l1_reg))\n    w3_l2_l1_reg = w3_l2_l1_reg - learning_rate * (grad_3  + alpha *  d_abs(w3_l2_l1_reg))\n    w2_l2_l1_reg = w2_l2_l1_reg - learning_rate * (grad_2  + alpha *  d_abs(w2_l2_l1_reg))\n    w1_l2_l1_reg = w1_l2_l1_reg - learning_rate * (grad_1  + alpha *  d_abs(w1_l2_l1_reg))\n\nprint(\"Case 6 final error :\",cost)     \nlayer_1 = theta_with_bias.dot(w1_l2_l1_reg)\nlayer_1_act = tanh(layer_1)\nlayer_2 = layer_1_act.dot(w2_l2_l1_reg)\nlayer_2_act = iden(layer_2)\nlayer_3 = layer_2_act.dot(w3_l2_l1_reg)\nlayer_3_act = arctan(layer_3)\nlayer_4 = layer_3_act.dot(w4_l2_l1_reg)\nlayer_4_l2_l1_reg = iden(layer_4)","4c87b1a3":"print(\"L1 weights absolute sum: \",np.abs(w1_l1).sum(),\n                                    np.abs(w2_l1).sum(), \n                                    np.abs(w3_l1).sum(),\n                                    np.abs(w4_l1).sum() )\n\nprint(\"L2 weights absolute sum: \",\n                        np.abs(w1_l2).sum(),\n                        np.abs(w2_l2).sum(), \n                        np.abs(w3_l2).sum(),\n                        np.abs(w4_l2).sum())\n\nprint(\"L1 with L1 Reg weights absolute sum:\",\n                np.abs(w1_l1_reg).sum(),\n                np.abs(w2_l1_reg).sum(),\n                np.abs(w3_l1_reg).sum(),\n                np.abs(w4_l1_reg).sum())\n\nprint(\"L2 with L2 Reg weights absolute sum:\",\n    np.abs(w1_l2_reg).sum(),\n    np.abs(w2_l2_reg).sum(),\n    np.abs(w3_l2_reg).sum(),\n    np.abs(w4_l2_reg).sum())\n\nprint(\"L1 with L2 Reg weights absolute sum:\",\n    np.abs(w1_l1_l2_reg).sum(),\n    np.abs(w2_l1_l2_reg).sum(),\n    np.abs(w3_l1_l2_reg).sum(),\n    np.abs(w4_l1_l2_reg).sum())\n\nprint(\"L2 with L1 Reg weights absolute sum:\",\n    np.abs(w1_l2_l1_reg).sum(),\n    np.abs(w2_l2_l1_reg).sum(),\n    np.abs(w3_l2_l1_reg).sum(),\n    np.abs(w4_l2_l1_reg).sum())","500dd73a":"fig = plt.figure(figsize=(14, 7))\nplt.plot(theta_with_bias[:,0],layer_4_l1,c='r',linewidth=1, label='L1 Norm')\nplt.plot(theta_with_bias[:,0],layer_4_l2,c='g',linewidth=1,label='L2 Norm')\nplt.plot(theta_with_bias[:,0],layer_4_l1_reg,c='b',linewidth=1,label='L1 Norm with L1 R eg')\nplt.plot(theta_with_bias[:,0],layer_4_l2_reg,c='y',linewidth=1,label='L2 Norm with L2 Reg')\nplt.plot(theta_with_bias[:,0],layer_4_l1_l2_reg,c='k',linewidth=1,label='L1 Norm with L2 Reg')\nplt.plot(theta_with_bias[:,0],layer_4_l2_l1_reg,c='c',linewidth=1,label='L2 Norm with L1 Reg')\nplt.legend()\nplt.show()","7a7c456d":"<h1 id=\"l2_norm\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>L2 Norm\n        <a class=\"anchor-link\" href=\"#l2_norm\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","65e9765b":"<h1 id=\"activation\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>Activation Functions\n        <a class=\"anchor-link\" href=\"#activation\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","6666abf8":"<h1 id=\"l1_l1reg\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>L1 Norm + L1 Reg\n        <a class=\"anchor-link\" href=\"#l1_l1reg\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","0033cc6f":"<h1 id=\"reference\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>Reference\n        <a class=\"anchor-link\" href=\"#reference\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","638a4f8e":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1312\/2368\/23808724f313005d570be372003594fa\/dataset-cover.jpg\" \/>\n<\/div>","05661131":"<h1 id=\"l2norm_l1reg\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>L2 Norm + L1 Reg\n        <a class=\"anchor-link\" href=\"#l2norm_l1reg\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","a34ed024":"<h1 id=\"analysis\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>Analysis\n        <a class=\"anchor-link\" href=\"#analysis\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","239cd31f":"<h1 id=\"l2norm_l2reg\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>L2 Norm + L2 Reg\n        <a class=\"anchor-link\" href=\"#l2norm_l2reg\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","08e9a1e0":"<h1 id=\"l1_norm\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>L1 Norm\n        <a class=\"anchor-link\" href=\"#l1_norm\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","08196745":"To read more on the mathematics of this implementation : [Towards Science by Jae Duk Seo](https:\/\/towardsdatascience.com\/only-numpy-implementing-different-combination-of-l1-norm-l2-norm-l1-regularization-and-14b01a9773b)","034d1520":"<h1 id=\"parameters\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>Parameters\n        <a class=\"anchor-link\" href=\"#parameters\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","246ee547":"<h1 id=\"weights\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>Weights\n        <a class=\"anchor-link\" href=\"#weights\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","7a99f145":"<h1 id=\"dataset\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","c454ab9b":"<h1 id=\"l1norm_l2reg\" style=\"color:white; background:black; border:0.5px dotted white;\"> \n    <center>L1 Norm + L2 Reg\n        <a class=\"anchor-link\" href=\"#l1norm_l2reg\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}