{"cell_type":{"53d8752d":"code","683c6a11":"code","31c41471":"code","764e6f6f":"code","31ec574c":"code","6abd0368":"code","709b6e89":"code","42decd05":"code","93f0aa96":"code","a1da5271":"code","c263296c":"code","cc014df8":"code","9d593688":"code","00bd2805":"code","2451d97c":"code","fcf771fa":"code","e135ef3b":"code","4f246fb2":"code","0e531705":"code","21f7fe28":"code","2181cc27":"code","e07f32d7":"code","41fe0c62":"code","f392d44c":"code","954678e0":"code","2396b2e5":"code","30afa985":"code","3a937167":"code","55b79801":"code","e8d95989":"code","67793c0b":"code","f2f17870":"code","345c3577":"code","5356ef0a":"code","87545a9f":"code","6a5b5f97":"code","6316598e":"code","ea5f3014":"code","0a610f64":"code","a6f204c5":"code","f97fbcdc":"markdown","32a1b542":"markdown","bae18fa0":"markdown","a8c4e3b2":"markdown","6a9a9fca":"markdown","9c030e32":"markdown","65dc0a85":"markdown","bd6578cb":"markdown","d3c75124":"markdown","e8a63dd4":"markdown","bd4f5a6e":"markdown","6a86d5d5":"markdown","57a54795":"markdown"},"source":{"53d8752d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","683c6a11":"# import libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA","31c41471":"creditcard_df = pd.read_csv('\/kaggle\/input\/credit-card-segmentation\/CC GENERAL.csv')","764e6f6f":"creditcard_df","31ec574c":"# By using following info function, we can see data types and get to know about null value existance \n#(i.e, credit limit and Min payments)\ncreditcard_df.info()","6abd0368":"# By using following descibe function, we can get to know about important features of a coulmn, i.e, min, max and mean values\ncreditcard_df.describe()\n#This helps to give insights about data, i.e, Balance is frequently updated on average ~0.9, scale-->(0,1)\n# On average 15 percent people make full payment using CC","709b6e89":"# Suppose we want to know about a person who made maximum \"ONEOFF_PURCHASES\" which is \"40761.250000\" given in above \"describe\" fun.\ncreditcard_df[creditcard_df['ONEOFF_PURCHASES']==40761.250000]","42decd05":"# Now lets get the features of customer who made the maximum cash advance transactions.\ncreditcard_df[creditcard_df['CASH_ADVANCE']>= 47137]","93f0aa96":"# Lets check misiing values, it seems that we have very less amount of missing values\nsns.heatmap(creditcard_df.isnull(), yticklabels= False, cbar =False, cmap = 'winter_r')","a1da5271":"#We can see we have 1 null value in \"CREDIT_LIMIT\" and 313 in \"Minimum_Payments\"\ncreditcard_df.isnull().sum()","c263296c":"# Lets fill these missing values with meab\ncreditcard_df.loc[(creditcard_df['MINIMUM_PAYMENTS'].isnull() == True), 'MINIMUM_PAYMENTS'] = creditcard_df['MINIMUM_PAYMENTS'].mean()\n#We will use an alternate method to fill NAN value with mean in \"CREDIT_LIMIT\" coulmn\ncreditcard_df['CREDIT_LIMIT'].fillna(value=creditcard_df['CREDIT_LIMIT'].mean(), inplace= True)","cc014df8":"creditcard_df['MINIMUM_PAYMENTS'].isnull().sum()","9d593688":"creditcard_df['CREDIT_LIMIT'].isnull().sum()","00bd2805":"# So now we can see that we dont have any missing values left\nsns.heatmap(creditcard_df.isnull(), yticklabels= False, cbar =False, cmap = 'winter_r')","2451d97c":"# Now lets see if we have any duplicated entries and the result shows that all entries are unique\ncreditcard_df.duplicated().sum()","fcf771fa":"# Lets drop the ID column which dosent provide any info but a sequentail order\ncreditcard_df.drop(columns= 'CUST_ID', axis = 1, inplace= True)","e135ef3b":"print( 'Number of columns = {}'.format(len(creditcard_df.columns)))","4f246fb2":"creditcard_df.columns","0e531705":"# Now er are going to use dist_plot which is a combination of \"hist\" function in matplotlib and \"KDE\" in seaborn\n# KDE is used to plot the probability distribution function of a variable\nplt.figure(figsize=(10,50))\nfor i in range (len(creditcard_df.columns)):\n    plt.subplot(17,1,i+1)\n    sns.distplot(creditcard_df[creditcard_df.columns[i]], kde_kws= {'color' : 'b', 'lw': 3, 'label': 'KDE', 'bw': 1.5}, hist_kws= {'color' : 'g'})\n    plt.title(creditcard_df.columns[i])\nplt.tight_layout()","21f7fe28":"creditcard_df.head()","2181cc27":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncorr = creditcard_df.corr()\nsns.heatmap(corr, annot=True, cmap=plt.cm.Reds)\nplt.show()","e07f32d7":"#Lets re-scale data\nscaler = StandardScaler()\ncreditcard_df_scaled = scaler.fit_transform(creditcard_df)","41fe0c62":"creditcard_df_scaled.shape","f392d44c":"creditcard_df_scaled","954678e0":"# Now we are going to implement Elbow method to final optimal number of clusters\nfirst_score = []\nfor i in range(1,20):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(creditcard_df_scaled)\n    first_score.append(kmeans.inertia_) #inertia gives the within cluster distance of each point from its centroid as we discussed above.\nplt.plot (first_score, 'bx')","2396b2e5":"# We can see from above plot that the optimal number of clusters in this case are 7 or 8.\n# So lets apply kmeans method.\nkmeans = KMeans(7)\nkmeans.fit(creditcard_df_scaled)\nlabels = kmeans.labels_ #labels --> clusters","30afa985":"labels","3a937167":"kmeans.cluster_centers_.shape","55b79801":"# Lets create a dataframe consists of cluster centers\ncluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns= [creditcard_df.columns])\ncluster_centers","e8d95989":"# As the data is scaled so lets perform inverse transform to know better what this data actually means\ncluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers, columns= [creditcard_df.columns])\ncluster_centers\n#We can seprate the four clusters given at the start of problem (i.e, Transactors, VIP) by monitoring the given attributes.","67793c0b":"labels.shape # values associated to each poin","f2f17870":"labels.max()","345c3577":"labels.min()","5356ef0a":"# Now we can have the label associated with each point\nykmeans = kmeans.fit_predict(creditcard_df_scaled)\nykmeans","87545a9f":"#Lets concatenate the cluster labels with original data, which will help to plot the histograms of each cluster\ncreditcard_df_cluster = pd.concat([creditcard_df, pd.DataFrame({'cluster': labels})], axis = 1)\ncreditcard_df_cluster.head()","6a5b5f97":"# Now lets plot histogram of each cluster\nfor i in creditcard_df.columns:\n    plt.figure(figsize=(35,5))\n    for j in range(7):\n        plt.subplot(1,7,j+1)\n        cluster = creditcard_df_cluster[creditcard_df_cluster['cluster']==j]\n        cluster[i].hist(bins=20)\n        plt.title('{} \\nCluster {} '.format(i,j))\n    plt.show()","6316598e":"#Lets convert our data to only 2D using PCA\npca = PCA(n_components=2)\npca_components = pca.fit_transform(creditcard_df_scaled)\npca_components","ea5f3014":"# create a dataframe of these two componenets\npca_df = pd.DataFrame(data = pca_components, columns = ['pca1', 'pca2'])\npca_df.head()","0a610f64":"#concatenate with labels\npca_df = pd.concat([pca_df, pd.DataFrame({'cluster': labels})], axis = 1)\npca_df.head()","a6f204c5":"plt.figure(figsize=(10,10))\nax = sns.scatterplot(x='pca1', y='pca2', hue = 'cluster', data=pca_df, palette=['red', 'green', 'blue', 'pink', 'yellow', 'gray', 'black'])\nplt.show()","f97fbcdc":"## Elbow Method","32a1b542":"Elbow method is a very popular method to calculate optimal number of clusters for a given problem. Within clusters the sum of \nsquare distance is calculated and plotted against the number of clusters. The elbow point in plot is selected as optimal number\nof clusters for given problem. For more detalis, please click [here](https:\/\/predictivehacks.com\/k-means-elbow-method-code-for-python\/#:~:text=K-Means%20Elbow%20Method%20code%20for%20Python.%20K-Means%20is,number%20is%20not%20optimal%20for%20the%20specific%20case.)","bae18fa0":"## Acknowledgment","a8c4e3b2":"K- means is an un-supervised machine learning algorithm. It groups data in clusters in an un-supervised fashion. It uses Euclidian distance to measure similarity between attribute values. For more details, please click [here](https:\/\/towardsdatascience.com\/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203).","6a9a9fca":"## Principal componenet Analysis (PCA)","9c030e32":"## Visualize and Explore Data","65dc0a85":"I am really thankful to [Coursera](https:\/\/www.coursera.org\/projects\/machine-learning-for-customer-segmentation) and [Ryan Ahmed](https:\/\/www.coursera.org\/instructor\/~48777395) for proving such a valuable opportunity to learn using real time project.","bd6578cb":"The steps performed in this task are:\n1. Visualize and explore datasets\n2. Scikit-Learn library to find the optimal number of clusters using elbow method\n3. k-means using Scikit-Learn to perform customer segmentation\n4. Principal Component Analysis (PCA) technique to perform dimensionality reduction and data visualization","d3c75124":"The goal of this project is to leverage AI\/ ML model to segment customers for launching a specific targeted Ad-campaign. To make it successful, we have to segment them in at-least 3 distinct groups known as \"marketing segmentation\". It will help to maximize the marketing campaign conversion rate. For example the general four segments are:\n1. Transactors: Customers who pay least amount of interest and very careful with the money. Generally they have lower balance(USD 104), cash advance (USD 303) and perecnt of full paymenet = 23% \n2. Revolvers : (Most lucrative sector) use credit card as a loan, generally they have highest balance (USD 5000), cash advance (USD 5000), low purchase frequency, high cash advance frequency (0.5), high cash advance transactions (16).\n3. VIP\/Prime : (This group is specific target to increase credit limit and spend habbit) High credit limit (USD 16K), high percentage of full payment.\n4. Low Tenure: Low tenure (7 Years), low balance.","e8a63dd4":"Now we will plot correlation between features.\nThe correlation coefficient has values between -1 to 1.\n1.  A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n2.  A value closer to 1 implies stronger positive correlation\n3.  A value closer to -1 implies stronger negative correlation","bd4f5a6e":"## Note: This notebook will be updated with the passage of time. Your feedback will be highly appreciated. Please upvote, if you like it and find it helpful. Your support in terms of upvote and positive feedback will keep me motivated :)","6a86d5d5":"## K-Means Algorithm","57a54795":"1. PCA is an unsupervised ML algorithm that tries to reduce the dimension of data while preserving the actual information. \n2. PCA algorithm could be used for dimensionality reduction by trying to find a new set of features called components while maintaining the original information. \nFor more details and real time example, please click [here](https:\/\/towardsdatascience.com\/pca-using-python-scikit-learn-e653f8989e60)"}}