{"cell_type":{"3475db34":"code","74dd1f2b":"code","a30dfb8c":"code","507f0cd2":"code","49aa461c":"code","ef972be7":"code","f087cb5c":"code","bd9bdbd0":"code","32e195b1":"code","b9ac03ae":"code","09c4e4f0":"code","6b9abf25":"code","25e2640c":"code","d5261cea":"code","657ddadf":"code","24afe27a":"code","fff8b054":"code","26ee7873":"code","48def32a":"code","3fd245bd":"code","2e7ece4a":"code","ecb34cd5":"code","5e6b9ce8":"code","bd6a7c11":"code","22181d28":"code","9a380282":"code","3d19a5f0":"code","203da209":"code","b633bb25":"code","8837aa04":"code","9f47c3a0":"code","3f6d5823":"code","33a0827b":"code","12b05561":"code","cd5f4bbc":"code","c8be397e":"code","51c4a06a":"code","16cf8888":"code","66a004a1":"code","1405b72a":"code","0db8523d":"code","a2643508":"code","47ff8daa":"code","d7ee139c":"code","67f7688e":"code","442f1ef7":"code","f371ffac":"code","0d4b9655":"code","fabfbcf6":"code","37dca28d":"code","63234b5c":"code","a230854b":"code","0680a88c":"code","5c5d8125":"code","cf2637c6":"code","8fac83fa":"code","f54a0c0b":"code","a2fdf2a4":"code","dc90d32e":"code","b10292e5":"code","9f3d216c":"code","5b73b111":"code","35a75ff0":"code","6d9b32f2":"code","b478db82":"code","d814b762":"code","41eac034":"code","dca4ca9d":"code","60aa7809":"code","54a4cefb":"code","169aa945":"code","c959241c":"code","874c76c0":"code","68bc2820":"code","ad6575ba":"markdown","5d6cdd88":"markdown","fe54c691":"markdown","fa83177b":"markdown","073c8e84":"markdown","a3c00f4c":"markdown","0e4cf6bd":"markdown","1c7c3e49":"markdown","a9e9a5dc":"markdown","87df8fff":"markdown","54df5cca":"markdown","1e001ddf":"markdown","be090bd5":"markdown","9c503b41":"markdown","c830f2cc":"markdown","844eadd3":"markdown","1085df6c":"markdown","db4e0d83":"markdown","3eb40e88":"markdown","c53ac748":"markdown","e69ea301":"markdown","6a3015c0":"markdown","2d9ca7b4":"markdown","0f3dfe01":"markdown","a0a2e8a8":"markdown","ac9a0ff8":"markdown","a87db7d6":"markdown","75d67344":"markdown","ea57285b":"markdown","e693b962":"markdown","d08bc133":"markdown","376b12ce":"markdown","c40ecfa7":"markdown","2fe5cf82":"markdown","bc0414e5":"markdown","a576a768":"markdown","e94798de":"markdown","d2bb95a0":"markdown","313a8555":"markdown","54ad624c":"markdown","1aac2d94":"markdown","75d94767":"markdown","a04d3465":"markdown","bb3df3d3":"markdown","560c5421":"markdown","7da55b63":"markdown","737200eb":"markdown","f9627c64":"markdown","c3197261":"markdown","187b0a71":"markdown","cfb2d8e0":"markdown","4d6314f2":"markdown","a1993027":"markdown","6a3d6749":"markdown","7107c1cd":"markdown","d7d504c2":"markdown","ddb003b2":"markdown","1c6b1f2d":"markdown","f2a5f9c7":"markdown","e863e87d":"markdown","5b12cc9b":"markdown","e9698e9c":"markdown","9f0af035":"markdown","8192e9a1":"markdown","101394c4":"markdown","11605343":"markdown","15290d58":"markdown","3e0c715d":"markdown","a8d5a58e":"markdown","1c0c74bb":"markdown","76fe8ee5":"markdown","b2325472":"markdown"},"source":{"3475db34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74dd1f2b":"df=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf","a30dfb8c":"df.describe()","507f0cd2":"l=df.isnull().sum()\nprint(l , df.shape)","49aa461c":"import seaborn as sns\nsns.heatmap(df.corr())","ef972be7":"sns.countplot(data=df, x='Survived')","f087cb5c":"from matplotlib import pyplot as plt\nfig, axes = plt.subplots(1,2, figsize=(20,5))\nsns.countplot(ax=axes[0], data=df,x='Sex')\nsns.countplot(ax=axes[1],data=df,x='Sex',hue='Survived')","bd9bdbd0":"sns.set_palette('Paired')\nfig, axes = plt.subplots(1,2, figsize=(20,5))\nsns.countplot(ax=axes[0], data=df,x='Embarked')\nsns.barplot(ax=axes[1],data=df, x= 'Embarked', y='Survived')\n","32e195b1":"sns.set_palette('Paired')\nfig, axes = plt.subplots(1,2, figsize=(20,5))\nsns.countplot(ax=axes[0], data=df,x='Embarked', hue='Survived')\nsns.barplot(ax=axes[1],data=df, x= 'Embarked', y='Survived', hue='Sex')","b9ac03ae":"fig, axes = plt.subplots(1,2, figsize=(20,5))\nsns.countplot(ax=axes[0], data=df,x='Pclass')\nsns.barplot(ax=axes[1], data=df,x='Sex',y='Survived', hue=\"Pclass\")","09c4e4f0":"sns.histplot(data=df,x='Age')","6b9abf25":"df.Age.nunique()","25e2640c":"Age_group=[]\nfor c in df.Age:\n    if c<11:\n        Age_group.append(\"0-10\")\n    elif 10<c<21:\n        Age_group.append(\"11-20\")\n    elif 20<c<31:\n        Age_group.append(\"21-30\")\n    elif 30<c<41:\n        Age_group.append(\"31-40\")\n    elif 40<c<51:\n        Age_group.append(\"41-50\")\n    elif 50<c<61:\n        Age_group.append(\"51-60\")\n    elif 60<c<71:\n        Age_group.append(\"61-70\")\n    else:\n        Age_group.append(\"71-80\")      \n \ndf['age_group']=Age_group\n\n\nfig, axes = plt.subplots(1,2, figsize=(20,5))\nsns.set_palette('Paired')\ndf1=df.sort_values('Age', ascending=True)\nsns.countplot(ax=axes[0], data=df1,x='age_group', hue=\"Survived\")\nsns.barplot(ax=axes[1], x='age_group', hue='Sex', data=df1, y='Survived')","d5261cea":"sns.barplot(data=df, x='Sex',hue='age_group',  y='Survived')","657ddadf":"fig, axes = plt.subplots(1,2, figsize=(20,5))\nsns.countplot(ax=axes[0], data=df,x='SibSp')\nsns.countplot(data=df, x='SibSp', hue='Survived')","24afe27a":"fig, axes = plt.subplots(1,2, figsize=(20,5))\nsns.countplot(ax=axes[0], data=df,x='Parch')\nsns.countplot(data=df, x='Parch', hue='Survived')","fff8b054":"df.Cabin.nunique()","26ee7873":"plt.figsize=(50,10)\nsns.countplot(data=df, x='Cabin', hue='Survived', dodge=False)\n","48def32a":"fare_group=[]\nfor c in df.Fare:\n    if c<11:\n        fare_group.append(\"0-10\")\n    elif 10<c<21:\n        fare_group.append(\"11-20\")\n    elif 20<c<31:\n        fare_group.append(\"21-30\")\n    elif 30<c<41:\n        fare_group.append(\"31-40\")\n    elif 40<c<51:\n        fare_group.append(\"41-50\")\n    elif 50<c<101:\n        fare_group.append(\"50-100\")\n    elif 100<c<201:\n        fare_group.append(\"101-200\")\n    elif 200<c<301:\n        fare_group.append(\"201-300\")\n    elif 300<c<401:\n        fare_group.append(\"301-400\")\n    elif 400<c<501:\n        fare_group.append(\"401-500\")\n    else:\n        fare_group.append(\"501-550\")      \n \ndf['Fare_group']=fare_group\ndf['Fare_group'].value_counts()","3fd245bd":"df2=df.sort_values('Fare', ascending=True)\nfig, axes = plt.subplots(1,2, figsize=(20,5))\nsns.barplot(ax=axes[1], x='Fare_group', hue='Sex', data=df2, y='Survived')\nsns.countplot(ax=axes[0],data=df2, x='Fare_group', hue='Survived')\n","2e7ece4a":"df.drop(['age_group','Fare_group'], axis=1,inplace =True)\nd=df.isnull().sum()\nd","ecb34cd5":"test_data=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_data.head","5e6b9ce8":"all_data=pd.concat([df,test_data], ignore_index=True)  # we are setting the ignore_index to true so our combined data set would be indexed continously (indexed from 0_1309)\nall_data.head","bd6a7c11":"all_data.info()","22181d28":"all_data.Embarked.unique()","9a380282":"all_data.value_counts()","3d19a5f0":"for i,j in enumerate(all_data.Embarked):  # gettin a value pair of each value (i) and its index location (j)\n    if type(j)!=str:\n        print(i,j)","203da209":"all_data.iloc[[61,829]]['Embarked']","b633bb25":"all_data.Embarked[61]='S'\nall_data.Embarked[829]='S'\nall_data.Embarked.value_counts()","8837aa04":"for i,j in enumerate(all_data.Fare):  # gettin a value pair of each value (i) and its index location (j)\n    if np.isnan(j):\n         all_data.Fare[i]=test_data.Fare.mean()\nprint(all_data.Fare[i])","9f47c3a0":"df1=all_data.groupby(['Sex','Pclass','Embarked']).mean()\ndf1\n","3f6d5823":"for j,i in enumerate(all_data.Age): # gettin a value pair of each value (i) and its index location (j)\n    if np.isnan(i):   # checking if the value is a null (nan) value\n        if all_data.Sex[j]=='female':   #check the sex using the index (j) locaion\n            if all_data.Pclass[j] == 1:  # check the Pclass using the index (j) locaion\n                if all_data.Embarked[j]=='C':  # check where the passenger embarked from using the index (j) locaion\n                    all_data.Age[j]=38  # impute mean age for that group using the index (j) locaion\n                if all_data.Embarked[j]=='Q':\n                    all_data.Age[j]=35\n                if all_data.Embarked[j]=='S':\n                    all_data.Age[j]=35\n            if all_data.Pclass[j] == 2:\n                if all_data.Embarked[j]=='C':\n                    all_data.Age[j]=23\n                if all_data.Embarked[j]=='Q':\n                    all_data.Age[j]=30\n                if all_data.Embarked[j]=='S':\n                    all_data.Age[j]=28\n            if all_data.Pclass[j] == 3:\n                if all_data.Embarked[j]=='C':\n                    all_data.Age[j]=15\n                if all_data.Embarked[j]=='Q':\n                    all_data.Age[j]=22\n                if all_data.Embarked[j]=='S':\n                    all_data.Age[j]=22\n        if all_data.Sex[j]=='male':\n            if all_data.Pclass[j] == 1:\n                if all_data.Embarked[j]=='C':\n                    all_data.Age[j]=39\n                if all_data.Embarked[j]=='Q':\n                    all_data.Age[j]=44\n                if all_data.Embarked[j]=='S':\n                    all_data.Age[j]=42\n            if all_data.Pclass[j] == 2:\n                if all_data.Embarked[j]=='C':\n                    all_data.Age[j]=29\n                if all_data.Embarked[j]=='Q':\n                    all_data.Age[j]=59\n                if all_data.Embarked[j]=='S':\n                    all_data.Age[j]=29\n            if all_data.Pclass[j] == 3:\n                if all_data.Embarked[j]=='C':\n                    all_data.Age[j]=24.25\n                if all_data.Embarked[j]=='Q':\n                    all_data.Age[j]=25\n                if all_data.Embarked[j]=='S':\n                    all_data.Age[j]=25","33a0827b":"all_data.info()","12b05561":"all_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0]) # we use the 'str' method to strip the cabin number and take only the first character (the same applies for 'nan' values).\nall_data.info()","cd5f4bbc":"all_data.Name.head","c8be397e":"all_data['tittle'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\nall_data.info()","51c4a06a":"all_data.Ticket.head(20)","16cf8888":"all_data['ticket_typ']= all_data.Ticket.apply(lambda x: x.split('\/')[0]) #split the ticket at '\/' and take the first value\nall_data['ticket_typ']= all_data.Ticket.apply(lambda x: x.split(' ')[0]) #split the ticket(the output above) at ' ' (space) and take the first value\n                \nfor i,t in enumerate(all_data.ticket_typ):  # this loop checks the length of the tickets and picks only the first character if the length  id more than 1. if the first character is a number it replaces it with 'x' otherwise it keeps it if is an alphabet\n    if len(t)>1:\n        all_data.ticket_typ[i]= t[0]\n        if t[0].isdigit():\n            all_data.ticket_typ[i]='x'\nall_data.ticket_typ.value_counts()","66a004a1":"train=all_data.iloc[0:891,:] # our training set had 891 rows, it occupies the forst 891 rows of our combined data set\ntrain","1405b72a":"test=all_data.iloc[891:,:] # our test set had 418 rows, it occupies the last 418 rows of our combined data set\ntest","0db8523d":"fig, axes = plt.subplots(1,2, figsize=(20,5))\nsns.barplot(ax=axes[1], x='cabin_adv', data=train, y='Survived')\nsns.countplot(ax=axes[0],data=train, x='cabin_adv', hue='Survived')\n\n","a2643508":"pd.pivot_table(train,index='Survived',columns='tittle', values = 'Ticket', aggfunc='count')","47ff8daa":"pd.pivot_table(train, index='Survived', columns='ticket_typ', values='Ticket', aggfunc='count')","d7ee139c":"features=[ 'Pclass','Sex','Age', 'SibSp',\n       'Parch', 'Embarked','Fare', 'cabin_adv', 'tittle','ticket_typ',] # notice this does not contain our Survived column because we are going to save them seperatly\n\nall_data_features=all_data[features]","67f7688e":"all_data_dummies=pd.get_dummies(all_data_features)\n","442f1ef7":"X=all_data_dummies.iloc[0:891, : ] # this is being extracted from our combined dataset with dummie variables\ny=all_data.Survived.iloc[0:891]# this is being extracted from our combined dataset beefore dropping some columns. it's basically our survived column","f371ffac":"y","0d4b9655":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\n","fabfbcf6":"X.shape\n","37dca28d":"\nmodel = KNN(n_neighbors=7)\ncv = cross_val_score(model,X,y,cv=5)\nprint(cv)\nprint(cv.mean())","63234b5c":"cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\nv=cross_val_score(LinearRegression(), X, y, cv=cv)\nv.mean()","a230854b":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X,y,cv=5)\nprint(cv)\nprint(cv.mean())","0680a88c":"#xgb = XGBClassifier(random_state =1)\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X,y,cv=5)\nprint(cv)\nprint(cv.mean())","5c5d8125":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ncv = cross_val_score(gnb,X,y,cv=5)\nprint(cv)\nprint(cv.mean())","cf2637c6":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X,y,cv=5)\nprint(cv)\nprint(cv.mean())","8fac83fa":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X,y,cv=5)\nprint(cv)\nprint(cv.mean())","f54a0c0b":"'''xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [450,500,550],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train,y_train)\nclf_performance(best_clf_xgb,'XGB')'''","a2fdf2a4":"xgb = XGBClassifier(colsample_bytree= 0.8, gamma= 0.5, learning_rate= 0.5, max_depth= None, min_child_weight= 0.01, n_estimators= 550, reg_alpha= 1, reg_lambda= 10, sampling_method='uniform', subsample= 0.65)\ncv = cross_val_score(xgb,X,y,cv=5)\nprint(cv)\nprint(cv.mean())\n","dc90d32e":"# BElow are the codes for tuning the logistic regression model, i commented them because i ran them once and it took a little while, so i just copied the output into the next cell so i dont have to repeat the process.\n'''def model_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))\n\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n                                  \nrf_model = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_rf_model = rf_model.fit(X_train,y_train)\nmodel_performance(best_rf_model,'Random Forest')'''","b10292e5":"test=all_data_dummies.iloc[891: , : ]\ntest.columns, test.shape  # this shows us the shape and columns","9f3d216c":"\"\"\"model = LogisticRegression(max_iter = 2000)\nmodel.fit(X,y)\npredictions=model.predict(test).astype(int)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\"\"\"","5b73b111":"X.head","35a75ff0":"d=list(range(0,5))\nd","6d9b32f2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","b478db82":"x_train_with_metapreds = np.zeros((X_train.shape[0], X_train.shape[1]+2))\nx_train_with_metapreds[:, :-2] = X_train\nx_train_with_metapreds[:, -2:] = -1\nprint(x_train_with_metapreds)","d814b762":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5, random_state=11, shuffle=True)\nfor train_indices, val_indices in kf.split(X_train):\n    kfold_x_train, kfold_x_val = X_train.iloc[train_indices], X_train.iloc[val_indices]\n    kfold_y_train, kfold_y_val = y_train.iloc[train_indices], y_train.iloc[val_indices]\n    #svm = LinearSVC(random_state=11, max_iter=1000)\n    rf.fit(kfold_x_train, kfold_y_train)\n    rf_pred = rf.predict(kfold_x_val)\n    #knn = KNeighborsClassifier(n_neighbors=4)\n    xgb.fit(kfold_x_train, kfold_y_train)\n    xgb_pred = xgb.predict(kfold_x_val)\n    x_train_with_metapreds[val_indices, -2] = rf_pred\n    x_train_with_metapreds[val_indices, -1] = xgb_pred","41eac034":"x_val_with_metapreds = np.zeros((X_test.shape[0], X_test.shape[1]+2))\nx_val_with_metapreds[:, :-2] = X_test\nx_val_with_metapreds[:, -2:] = -1\nprint(x_val_with_metapreds)","dca4ca9d":"#svm = LinearSVC(random_state=11, max_iter=1000)\nrf.fit(X_train, y_train)\n#knn = KNeighborsClassifier(n_neighbors=4)\nxgb.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\nxgb_pred = xgb.predict(X_test)\nx_val_with_metapreds[:, -2] = rf_pred\nx_val_with_metapreds[:, -1] = xgb_pred","60aa7809":"lr = LogisticRegression(random_state=11)\nlr.fit(x_train_with_metapreds, y_train)\nlr_preds_train = lr.predict(x_train_with_metapreds)\nlr_preds_val = lr.predict(x_val_with_metapreds)","54a4cefb":"from sklearn.metrics import accuracy_score\nprint('Stacked Classifier:\\n> Accuracy on training data = {:.4f}\\n> Accuracy on validation data = {:.4f}'.format(accuracy_score(y_true=y_train, y_pred=lr_preds_train),accuracy_score(y_true=y_test, y_pred=lr_preds_val)))","169aa945":"model = LogisticRegression(max_iter = 2000)\nmodel.fit(X,y)\npredictions=model.predict(test).astype(int)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","c959241c":"x_test_with_metapreds = np.zeros((test.shape[0], test.shape[1]+2))\nx_test_with_metapreds[:, :-2] = test\nx_test_with_metapreds[:, -2:] = -1\nprint(x_train_with_metapreds)","874c76c0":"xgb = XGBClassifier(random_state =1)\nrf = RandomForestClassifier(random_state = 1)\nrf.fit(X, y)\nxgb.fit(X, y)\nrf_pred = rf.predict(test)\nxgb_pred = xgb.predict(test)\nx_test_with_metapreds[:, -2] = rf_pred\nx_test_with_metapreds[:, -1] = xgb_pred","68bc2820":"#model = LogisticRegression(max_iter = 2000)\n#model.fit(X,y)\npredictions=lr.predict(x_test_with_metapreds).astype(int)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","ad6575ba":"# Using logistic regression","5d6cdd88":"We would be using cross validation score where our data set is shuffled 5 times (cv=5) to get 5 different instances of training and validation sets across our data.","fe54c691":"The plot above shows that more females that embarked from each category survived. Let's go ahead and check survival rate with respectc to a passengers class (Pclass). \n\n\n","fa83177b":"Making a countplot for the cabins is not the best ideas as seen below. The graph is not interpretable due to the large number of cabins, later on we would be doing some featyre engineering to get a better idea of the relationship between 'Cabin' and 'Survival'. ","073c8e84":"In the cell below we would be extracting our training set from our combined data set just as done earlier, but saving them as X (independent variables) and y(dependent variables)","a3c00f4c":"Below is a table showing survival rates by tittle, nan represents 0","0e4cf6bd":"# Using Extreme gradient bost","1c7c3e49":"From the plot above it's clear that more poeple died than survived, over 500 passengers died while less than 400 survived. lets have a look at how the \nindependent variables (sex, age, embarhed, fare, Pclass etc) are related to the survival rate. We would be starting with sex.","a9e9a5dc":"Lets read our training data and have a look at the first 10 rows. The aim here is to have a proper idea of the type data contained in each column","87df8fff":"Now lets split our data set (all_data) into training and test set so we can have a look it the distributions of our columns with imputations done in our new columns","54df5cca":"Knowing we have over 800 passengers, below is the age distribution of our passengers. Majority ofthe passengers fall within the ages of 15 to 40 years.\n\n","1e001ddf":"Now lets extract our test from our combined data.","be090bd5":"Lets have a look at the mean, maximum and minimum values of columns with numerical data.","9c503b41":"# Using Linear regression","c830f2cc":"In this section we would be trying to develop some new features (columns) from the existing features (columns).THE AIM OF THIS IS TO DEVELOP HELPFULL FEATURES WHICH CAN IMPROVE OUR PREDICTIONS.","844eadd3":"From the plots it is evident that more passengers were in the passenger class '3' while the least amount of passengers were in class '2'. Conversly male and female passengers in passenger class '3' had the lowest chance of survival while those in class '1' had the highest chance of survival. Up next we would be checking the survival rate by age.\n","1085df6c":"Let's have a look at where we have null values.","db4e0d83":"We can also extract the title of each passenger from their names and create a separate column for that. Let have a look at how the names are arranged first.","3eb40e88":"From the graphs above, there were more passengers who paid between 0$ to 10$ and females had a higher survival rate in each fare group apart from those that paid 501$ to 550$ where everyone survived.","c53ac748":"Fitting 5 folds for each of 288 candidates, totalling 1440 fits\nRandom Forest\nBest Score: 0.8370136905348173\nBest Parameters: {'bootstrap': True, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 400}","e69ea301":"To fill the values in the 'Age' column we would be taking a different approach, we would group the passengers by sex, Pclass and Embarked before getting their mean age for that group, this should give us a better result than just generalizing the mean age for all the passengers. We would be using the group_by method to actualize this.","6a3015c0":"The graph below shows the rate of survival with respect to SibSp (number of siblings\/parents), passengers with 0 and 1 'SibSP' have the highest chance of survival while those with 6 and 8 have the lowest chance of survival (0 chance of survival).","2d9ca7b4":"# **Data exploration**","0f3dfe01":"The cell above shows us all unique values from the Embarked column, we can see that the column contains 'nan' values. Let's write a code to replace the nan values with the values with maximum occurance. The cell below checks the count of the other values in the column.","a0a2e8a8":"From the cell below there are some missing\/null values in the 'Cabin','Age' and 'EMbarked' column (from their count), lets have a look at the number of null values in all columns. lets also have a look at the shape of our training set.","ac9a0ff8":"'S' occurs more than any other value, therefore we would be replacing the 'nan' values with 'S'. We first have to get the locationo of the 'nan' values, that is what's done in the cell below.","a87db7d6":"The plots above show the survival rate with respect to sex. The plot on the left shows the total number of males and females that boarded the ship while the one on the right shows the amount of males and females that survived or not. Less than 110 (20 percent) of the males on the ship survived while over 200 (70 percent) of the females survived. Hence a female has a higher chance of survival (4 times more) than a male. we can go ahead to check the survival rate with respect to the point of embarkment.\n","75d67344":"Checking the cabin_adv (cabin group\/class) its evident that the 'n' class\/group had more passengers. However the 'E', 'D' and 'F' classes\/groups had the best survival rates even though they had far less passengers.","ea57285b":"now that we have combined our data set, let's have a look at a summary of each column so we can identify columns with missing values.","e693b962":"# **MODEL PREPROCESSING**","d08bc133":"This notebook is under construction, do check back for updates. Feel free to comments and ask questions and if you find it helpfull don't forget to upvote","376b12ce":"From the outputs of our models we can infer that Logistic regression gave us our best score of 0.8229 while linear regression gave our worse of 0.31. Thus we would be making a submission with logistic regression as our model.\nLets do some hyper parameterr tuning for the Logistic regression and gradient boosting model and see if our models can perform better.","c40ecfa7":"# Using Support Vectors","2fe5cf82":"# **FEATURE ENGINEERING**","bc0414e5":"Lets check if our imputation for the age column was sucessfull","a576a768":"#                                      **TITANIC PROJECT**\n\nThis is a walktrough of the steps i took solving the Titanic data set. The objective of this project is to predict correctly who survived and did not survive the wreckage. In this notebook, 82.26% is the best score on the training set using Logistic regression while 0.77 is the public score.","e94798de":"The cell below imports the numpy and pandas library along with our datasets.","d2bb95a0":"Time to import libraries","313a8555":"If we are to plot the relationship between fares and survival, we qould have a similar graph to the one above due to a large number of different values for the fares. Hence we would be splitting the fares into fare groups in a new column.","54ad624c":"looking at the survival rate with respect to 'Parch' (number of children\/parents) we can see that most of the passengers had 0 'Parch' with 6 being the least. However Passengers with 'Parch' of 2 had a 50% chance of survival, which is the highest chance of survival with regards to 'Parch'.","1aac2d94":"Cheching for null values in our dataset we can see that there are no null values excpt in the 'Cabin' column, this is not a problem as we would be using our 'cabin_adv' column instead. we can also see the new columns we created (tittle, cabin_adv).\n","75d94767":"# Using Naive Bayes","a04d3465":"**Below is an overview of the steps taken** \n1. Data exploration\n2. Data cleaning\n3. Feature engineering\n4. Data preprocessing\n5. Model development and comparisms\n6. Results\n7. Submission","bb3df3d3":"Next is the 'Cabin', let's have a look at how many cabins we have below.","560c5421":"The names are in the format of Name\/Tittle\/Other Names. Hence we split the names at the 'comma' after the name at the first position and pick the tittle, then we split the tittle of the 'full stop' at its end and take the tittle alone as in the first line of the cell below.","7da55b63":"There is only a single missing value in the 'Fare' column, the cell below checks all the rows in the 'Fare' column for the missing value and replaces it with the mean of all the values in the column.","737200eb":"The plot on the left shows the total number of passengers that embarked from fifferent points, while 'nan' stands for those passengers whose point of embarkment was not recorded, while that on the right represents the percentage of passengers that survived from each point of embarkment.\nFrom the barplot it is evident that passengers that embarked at 'C' had a higher chance of survival, about 55% of them survived while less than 40% of the passengers that embarked at 'S' and 'C' survived with 'S' having the lowest survival rate (note that very few passengers had their point of embarkment not recorded and they all survived). Below is a look at the survival rate of men and women with respect to where they embarked.\n\n","f9627c64":"From the graph on the left we can see that most of the passengers that survived were within the age group of 21-30 years while those within 61-70 were the most few that survived. From the graph on the right, males with the ages of 0-10 years had almost the same chance of survival as the females in the same age group and that is the age group among males with the highest chance of survival. The graph below gives us a clearer view of the age group with the highest chance of survival within males and females.\n\n","c3197261":"let's have a look at y which is the survived column.","187b0a71":"Below is a table showing survival rated by ticket type. Passengers with \"P' tickets had the highest survival rates.","cfb2d8e0":"To make our job easy when cleaning, preprocessing and carrying out imputations, we would be joining our test and train sets together so as the carry out the earlier mentioned steps once without the need for repeating any operation for both the training and test set.","4d6314f2":"Starting with the 'Cabin' column where we have a lot of missing values, the cabin numbers are arranged in a string_integr pair, so we can take the first letter of each cabin number and use that as a cabin class\/group, while the 'nan' values are taken as a seperate group thereby craating a new feature\/column with no missing values.","a1993027":"Relationship between variables refers to the relationsship between the dependent variable ('Survived' column) and the independent variable (other columns).\nlet's first have a look at the survival rate using seaborne.\n\n\nsurvival rate using seabornes.","6a3d6749":"Now lets gett dummy variables for our categorical columns (columns with objects\/strings and not numerical values)","7107c1cd":"Let's try fitting our model ang getting predictions","d7d504c2":"# Using Random forests","ddb003b2":"The cell below is used to check the degree of correlation between columns., note that each column is expected to be perfectly corrolated with itself and the only other columns with the most correlation are the 'SipSp' (sibling to parent ratio), and 'Parch' (parent to children ratio) columns.","1c6b1f2d":"The 'Embarked' column is missing 2 calumns (has 1307 out of 1309 values), while the 'Cabin' columns is missing over 1000 values and the 'Age' column is missing just over 200 values. Lets start our imputations on the Embarked column.","f2a5f9c7":"From the above we have 1309 non null values, which is the total number of values in the column.","e863e87d":"We have a large number of null values, so dropping them would not be a smart move as than would mean loosing over half of our data. Hence we would be doing some imputation. However let's first have a good look at our test set.","5b12cc9b":"# **MODEL DEVELOPMENT**","e9698e9c":"We would be doing something similar to our ticket colum as done to out cabin column. We would be taking only the first letter of our string_integer pair and using that to create a new feature\/column as our ticket class.Lets first have a look at how the ticket are arranged","9f0af035":"We can now see the number of missing values in the 'Age', 'Cabin' and 'Embarked' column, later on we would do some imputations to handle them.","8192e9a1":"If we are to plot the relationship between age and survival, we would have a cluttered graph due to a large number of different values for the fares. Hence we would be splitting the fares into fare groups in a new column. Now we create a column where we group the ages in groups of 10 and sort them for ease of plotting a clear graph showing the survival rate with respect to age and sex.\n\n\n","101394c4":"We would revert back to using our combined data set (all_data) for preprocessing, 'features' is the list of columns we would be usiing to train our model. Notice we have dropped the name, passenger_id,tittle, ticket and cabin column but are using tittle, ticket_typ, cabin_adv instead. This is because the dropped columns are of no use to our model since the are specific to each passenger and moreover we have extracted usefull information from them. ","11605343":"The next thing is the replace the null (nan) values with the mean age for the respective groups, the cell below contains a loop that scans through all the rows in the 'Age' column to check the group they belong to (i.e is the passager a male, then what is his Pclass and finally where did he embark from before imputing the mean age for passengers from that group) and then impute the mean age for that group. ","15290d58":"Let's have a look at how many unique (different) ages are distributed amongst the passengers. \n\n","3e0c715d":"# using KNN","a8d5a58e":"# DATA CLEANING","1c0c74bb":"Now that we have the index of the 'nan' values, we can input the value with the highest frequency, which is 'S' and we check if our imputation was a success","76fe8ee5":"X contains 891 rows and 45 columns. In the cells below we would be training using different models with their default parameters.","b2325472":"The output cell above shows the count for each fare group, now we can go ahead and sort the fare_group column then plot graphs using the fare_group colums"}}