{"cell_type":{"8bba1daf":"code","9688ac73":"code","633cb4db":"code","4210555a":"code","deaea071":"code","faebe65e":"code","14d1358f":"code","a2a806f2":"code","38ad1792":"code","c7b21099":"code","c601e2e8":"code","1e029eba":"code","ce93589e":"code","4e93b9d7":"code","07f20366":"code","8ccd8fc9":"code","3b9ea21e":"code","c3b0a03d":"code","536000d1":"code","ed030d08":"markdown","a0af6943":"markdown","8f36af53":"markdown","67b8c594":"markdown","a0384f1e":"markdown","dd9fc572":"markdown","97316ea5":"markdown","27241095":"markdown","8db1b8a0":"markdown","868c9508":"markdown","afb46c34":"markdown","186e9247":"markdown","2d99eda7":"markdown","41892e5e":"markdown","6473a589":"markdown","033c749c":"markdown"},"source":{"8bba1daf":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport nltk\nimport re\nimport functools\n\nfrom matplotlib import pyplot as plt\nfrom plotly import graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nfrom nltk.tokenize import TweetTokenizer, RegexpTokenizer\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nfrom collections import Counter\nplt.style.use('seaborn')","9688ac73":"# Define some colors for the plots\nCOLOR_DISASTER = '#DB2B44'\nCOLOR_DISASTER_DARK = '#61131f'\n\nCOLOR_NOT_DISASTER = '#65A6B2'\nCOLOR_NOT_DISASTER_DARK = \"#23393d\"","633cb4db":"PATH_CSV_TRAIN = '\/kaggle\/input\/nlp-getting-started\/train.csv'\ndataf = pd.read_csv(PATH_CSV_TRAIN)","4210555a":"dataf.head(5)","deaea071":"# Used Hashtags in each tweet\ndataf['hashtags'] = dataf.apply(lambda r: [e.lower() for e in re.findall('#[a-zA-Z]+', r['text'])], axis=1)\n\n# Number of words in each tweet\ndataf['nb_words'] = dataf.apply(lambda r: len(r['text'].split()), axis=1)\n\n# Number of URLs in each tweet\n#reg = re.compile('https?:\\\/\\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\/\/=]*)')\nreg = re.compile('https?\\S+(?=\\s|$)')\ndataf['nb_urls'] = dataf.apply(lambda r: len(re.findall(reg, r['text'])), axis=1)\n\n# Number of hashtags1 in each tweet\ndataf['nb_hashtags'] = dataf.apply(lambda r: len(r['hashtags']), axis=1)\n\n# TEXT CLEANING\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\nSTOPWORDS = STOPWORDS.union(['\\x89\u00fb_', '\u00fb_', '-', '\\&'])\n\n#Remove emojis and special chars\nreg = re.compile('\\\\.+?(?=\\B|$)')\ndataf['clean_text'] = dataf.apply(lambda r: re.sub(reg, string=r['text'], repl=''), axis=1)\nreg = re.compile('\\x89\u00db_')\ndataf['clean_text'] = dataf.apply(lambda r: re.sub(reg, string=r['clean_text'], repl=' '), axis=1)\nreg = re.compile('\\&amp')\ndataf['clean_text'] = dataf.apply(lambda r: re.sub(reg, string=r['clean_text'], repl='&'), axis=1)\nreg = re.compile('\\\\n')\ndataf['clean_text'] = dataf.apply(lambda r: re.sub(reg, string=r['clean_text'], repl=' '), axis=1)\n\n#Remove hashtag symbol (#)\ndataf['clean_text'] = dataf.apply(lambda r: r['clean_text'].replace('#', ''), axis=1)\n\n#Remove user names\nreg = re.compile('@[a-zA-Z0-9\\_]+')\ndataf['clean_text'] = dataf.apply(lambda r: re.sub(reg, string=r['clean_text'], repl=''), axis=1)\n\n#Remove URLs\nreg = re.compile('https?\\S+(?=\\s|$)')\ndataf['clean_text'] = dataf.apply(lambda r: re.sub(reg, string=r['clean_text'], repl=''), axis=1)","faebe65e":"tokenizer = RegexpTokenizer(r'[a-zA-Z]+\\b')\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\n\ndef tknize(string, tokenizer=tokenizer, lemmatizer=lemmatizer):\n    words = tokenizer.tokenize(string)\n    return [lemmatizer.lemmatize(w) for w in words if w not in STOPWORDS]","14d1358f":"fig = go.Figure(data=[go.Pie(labels=['No disaster', 'Disaster'], \n                             values=dataf.groupby('target')['target'].count(), \n                             marker_colors=[COLOR_NOT_DISASTER, COLOR_DISASTER],\n                             marker_line_color=[COLOR_NOT_DISASTER_DARK, COLOR_DISASTER_DARK],\n                             marker_line_width=[2,2]\n                            )])\nfig.update_layout(title='Percentage of each tweet by type', template='plotly_dark')\nfig.show()","a2a806f2":"hashtags = nltk.flatten(list(dataf['hashtags'].loc[dataf['target'] == 1]))\ncnt = Counter()\nfor h in hashtags:\n    cnt[h] += 1\nhashtags_disaster = pd.DataFrame.from_dict(dict(cnt), orient='index', columns=['times']).sort_values('times', ascending=False)#.iloc[:15]\n\nhashtags = nltk.flatten(list(dataf['hashtags'].loc[dataf['target'] == 0]))\ncnt = Counter()\nfor h in hashtags:\n    cnt[h] += 1\nhashtags_not_disaster = pd.DataFrame.from_dict(dict(cnt), orient='index', columns=['times']).sort_values('times', ascending=False)#.iloc[:15]","38ad1792":"TOP_N = 15\nfig = go.Figure()\n\nhover_disaster = [f\"{i} Tweets\" for i in hashtags_disaster.values.flatten()[:TOP_N]]\nhover_not_disaster = [f\"{i} Tweets\" for i in hashtags_not_disaster.values.flatten()[:TOP_N]]\n\nfig.add_trace(go.Bar(y=hashtags_disaster.index[:TOP_N],\n                            x=hashtags_disaster.values.flatten()[:TOP_N] * 100 \/ hashtags_disaster.values.sum(),\n                            alignmentgroup='a',\n                            showlegend=True,\n                            orientation='h',\n                            legendgroup='Dis',\n                            name='Disaster',\n                            text=hashtags_disaster.index,\n                            textposition='inside',\n                            textfont=dict(size=15, color='white'),\n                            hovertext=hover_disaster,\n                            hoverinfo='text',\n                            marker_color=COLOR_DISASTER,\n                            marker_line_color=COLOR_DISASTER_DARK,\n                            marker_line_width=1,\n                            opacity=0.7\n                        )\n)\n             \nfig.add_trace(go.Bar(y=hashtags_not_disaster.index[:TOP_N],\n                            x=-hashtags_not_disaster.values.flatten()[:TOP_N] * 100 \/ hashtags_not_disaster.values.sum(),\n                            alignmentgroup='a',\n                            showlegend=True,\n                            orientation='h',\n                            legendgroup='NDis',\n                            name='Not Disaster',\n                            text=hashtags_not_disaster.index,\n                            textposition='inside',\n                            textfont=dict(size=15, color='white'),\n                            hovertext=hover_not_disaster,\n                            hoverinfo='text',\n                            marker_color=COLOR_NOT_DISASTER,\n                            marker_line_color=COLOR_NOT_DISASTER_DARK,\n                            marker_line_width=1,\n                            opacity=0.7\n                        )\n)\n\nfig.update_layout(\n    yaxis=dict(showticklabels=False),\n    xaxis=dict(showticklabels=False),\n    title='Use of hashtags per tweet type (normalized)',\n    barmode='relative',\n    bargap=0.1,\n    template='plotly_dark',\n    height=800\n)\n","c7b21099":"fig_violin = go.Figure()\nfig_violin.add_trace(go.Violin(x=[\"Number of words\" for i in range(len(dataf[dataf['target'] == 1]))],\n                        y=dataf[dataf['target'] == 1]['nb_words'],\n                        name='Disaster',\n                        legendgroup='Dis', scalegroup='Y', scalemode='width',\n                        side='negative',\n                        line_color=COLOR_DISASTER)\n             )\nfig_violin.add_trace(go.Violin(x=[\"Number of words\" for i in range(len(dataf[dataf['target'] == 0]))],\n                        y=dataf[dataf['target'] == 0]['nb_words'],\n                        name='Non disaster',\n                        legendgroup='NotDis', scalegroup='Y', scalemode='width',\n                        side='positive',\n                        line_color=COLOR_NOT_DISASTER)\n             )\n\n\n\n\n\nd = dataf.groupby(['target', 'nb_hashtags'])['nb_hashtags'].count()\nfig_hashtags = go.Figure()\nfig_hashtags.add_trace(go.Bar(y=d[1].index[:6],\n                                x=d[1]*100\/d[1].sum(),\n                                alignmentgroup='b',\n                                showlegend=False,\n                                legendgroup='Dis',\n                                orientation='h',\n                                name='Disaster',\n                                dy=5,\n                                marker_color=COLOR_DISASTER,\n                                marker_line_color=COLOR_DISASTER_DARK,\n                                marker_line_width=2,\n                                opacity=0.6\n                            )\n)\n             \nfig_hashtags.add_trace(go.Bar(y=d[0].index[:6],\n                                x=d[0]*100\/d[0].sum(),\n                                alignmentgroup='b',\n                                showlegend=False,\n                                legendgroup='NotDis',\n                                orientation='h',\n                                name='Non Disaster',\n                                dy=5,\n                                marker_color=COLOR_NOT_DISASTER,\n                                marker_line_color=COLOR_NOT_DISASTER_DARK,\n                                marker_line_width=2,\n                                opacity=0.6\n                             )\n)\n\n\n\n\n\n\nd = dataf.groupby(['target', 'nb_urls'])['nb_urls'].count()\nfig_urls = go.Figure()\nfig_urls.add_trace(go.Bar(y=d[1].index[:6],\n                            x=d[1]*100\/d[1].sum(),\n                            alignmentgroup='a',\n                            showlegend=False,\n                            orientation='h',\n                            legendgroup='Dis',\n                            name='Disaster',\n                            dy=5,\n                            marker_color=COLOR_DISASTER,\n                            marker_line_color=COLOR_DISASTER_DARK,\n                            marker_line_width=2,\n                            opacity=0.6\n                        )\n)\n             \nfig_urls.add_trace(go.Bar(y=d[0].index[:6],\n                            x=d[0]*100\/d[0].sum(),\n                            alignmentgroup='a',\n                            showlegend=False,\n                            legendgroup='NotDis',\n                            orientation='h',\n                            name='Non Disaster',\n                            dy=5,\n                            marker_color=COLOR_NOT_DISASTER,\n                            marker_line_color=COLOR_NOT_DISASTER_DARK,\n                            marker_line_width=2,\n                            opacity=0.6\n                         )\n)\n\nfig_urls.update_layout(dict(\n    yaxis=dict(autorange=\"reversed\", dtick = 1, ticklen=50),\n    title='Percentage of Number of URLS for each tweet type'\n))\n\n\ncontainer = make_subplots(rows=1, cols=2, subplot_titles=('Percentage of number of hashtags',\n                                                          'Percentage of number of URLs'))\n\n#for f in fig_violin.data:\n#    container.append_trace(f, 1, 1)\n\nfor f in fig_hashtags.data:\n    container.append_trace(f, 1, 1)\n    \nfor f in fig_urls.data:\n    container.append_trace(f, 1, 2)\n\ncontainer.update_yaxes(autorange=\"reversed\", title=\"Number of hashtags\",row=1, col=1)\ncontainer.update_yaxes(autorange=\"reversed\", title=\"Number of URLs\", row=1, col=2)\n\ncontainer.update_xaxes(title=\"Percentage\", ticksuffix='%', row=1, col=1)\ncontainer.update_xaxes(title=\"Percentage\", ticksuffix='%', row=1, col=2)\n\ncontainer.update_layout(title='Textual stats for each tweet type', template='plotly_dark')\n\nfig_violin.update_layout(title='Number of words distribution per type', template='plotly_dark', height=500)\n\nfig_violin.show()\ncontainer.show()","c601e2e8":"text_disasters = functools.reduce(lambda a,b: a + \" \" + b, dataf[dataf['target']==1]['clean_text'].tolist()).lower()\ntext_disasters = [w for w in tknize(text_disasters) if w not in STOPWORDS]\n\ntext_not_disasters = functools.reduce(lambda a,b: a + \" \" + b, dataf[dataf['target']==0]['clean_text'].tolist()).lower()\ntext_not_disasters = [w for w in tknize(text_not_disasters) if w not in STOPWORDS]\n\nN_TOP=10\n\nunigrams_disaster =  pd.DataFrame(nltk.FreqDist(text_disasters).most_common()[:N_TOP], columns=['term', 'times'])\nbigrams_disaster = pd.DataFrame([(functools.reduce(lambda a,b: a+\" \"+b,r[0]), r[1]) for r in nltk.FreqDist(nltk.ngrams(text_disasters, 2)).most_common()[:N_TOP]], columns=['term', 'times'])\ntrigrams_disaster = pd.DataFrame([(functools.reduce(lambda a,b: a+\" \"+b,r[0]), r[1]) for r in nltk.FreqDist(nltk.ngrams(text_disasters, 3)).most_common()[:N_TOP]], columns=['term', 'times'])\n\nunigrams_not_disaster =  pd.DataFrame(nltk.FreqDist(text_not_disasters).most_common()[:N_TOP], columns=['term', 'times'])\nbigrams_not_disaster = pd.DataFrame([(functools.reduce(lambda a,b: a+\" \"+b,r[0]), r[1]) for r in nltk.FreqDist(nltk.ngrams(text_not_disasters, 2)).most_common()[:N_TOP]], columns=['term', 'times'])\ntrigrams_not_disaster = pd.DataFrame([(functools.reduce(lambda a,b: a+\" \"+b,r[0]), r[1]) for r in nltk.FreqDist(nltk.ngrams(text_not_disasters, 3)).most_common()[:N_TOP]], columns=['term', 'times'])","1e029eba":"#UNIGRAMS Fig\ncontainer = make_subplots(rows=1, cols=2, subplot_titles=('Disaster', 'Not Disaster'), horizontal_spacing=0.1)\n\ncontainer.add_trace(go.Bar(y=unigrams_disaster['term'].iloc[:15],\n                            x=unigrams_disaster['times'].iloc[:15],\n                            alignmentgroup='a',\n                            showlegend=True,\n                            legendgroup='Dis',\n                            orientation='h',\n                            name='Disaster',\n                            marker_color=COLOR_DISASTER,\n                            marker_line_color=COLOR_DISASTER_DARK,\n                            marker_line_width=2,\n                            opacity=0.6,\n                         ), row=1, col=1\n)\n\ncontainer.add_trace(go.Bar(y=unigrams_not_disaster['term'].iloc[:15],\n                            x=unigrams_not_disaster['times'].iloc[:15],\n                            alignmentgroup='a',\n                            showlegend=True,\n                            legendgroup='NotDis',\n                            orientation='h',\n                            name='Not Disaster',\n                            marker_color=COLOR_NOT_DISASTER,\n                            marker_line_color=COLOR_NOT_DISASTER_DARK,\n                            marker_line_width=2,\n                            opacity=0.6,\n                         ), row=1, col=2\n)\n\ncontainer.update_layout(title='Most common words for each tweet type', template='plotly_dark', height=500)\n\ncontainer.update_yaxes(tickfont=dict(color='white', size=12), autorange=\"reversed\")\n\ncontainer.show()\n\n# NGRAMS Fig\ncontainer = make_subplots(rows=2, cols=2, subplot_titles=('Bigrams','Trigrams'), \n                          horizontal_spacing=0.4, \n                          vertical_spacing=0.09)\n    \ncontainer.add_trace(go.Bar(y=bigrams_disaster['term'].iloc[:15],\n                            x=bigrams_disaster['times'].iloc[:15],\n                            alignmentgroup='a',\n                            showlegend=True,\n                            legendgroup='Dis',\n                            orientation='h',\n                            name='Disaster',\n                            marker_color=COLOR_DISASTER,\n                            marker_line_color=COLOR_DISASTER_DARK,\n                            marker_line_width=2,\n                            opacity=0.6,\n                         ), row=1, col=1\n)\n\ncontainer.add_trace(go.Bar(y=trigrams_disaster['term'].iloc[:15],\n                            x=trigrams_disaster['times'].iloc[:15],\n                            alignmentgroup='a',\n                            showlegend=False,\n                            legendgroup='Dis',\n                            orientation='h',\n                            name='Disaster',\n                            marker_color=COLOR_DISASTER,\n                            marker_line_color=COLOR_DISASTER_DARK,\n                            marker_line_width=2,\n                            opacity=0.6,\n                         ), row=1, col=2\n)\n\n####\n# Not disaster\n####\n\n\n\ncontainer.add_trace(go.Bar(y=bigrams_not_disaster['term'].iloc[:15],\n                            x=bigrams_not_disaster['times'].iloc[:15],\n                            alignmentgroup='a',\n                            showlegend=True,\n                            legendgroup='NotDis',\n                            orientation='h',\n                            name='Not Disaster',\n                            marker_color=COLOR_NOT_DISASTER,\n                            marker_line_color=COLOR_NOT_DISASTER_DARK,\n                            marker_line_width=2,\n                            opacity=0.6,\n                         ), row=2, col=1\n)\n\ncontainer.add_trace(go.Bar(y=trigrams_not_disaster['term'].iloc[:15],\n                            x=trigrams_not_disaster['times'].iloc[:15],\n                            alignmentgroup='a',\n                            showlegend=False,\n                            legendgroup='NotDis',\n                            orientation='h',\n                            name='Not Disaster',\n                            marker_color=COLOR_NOT_DISASTER,\n                            marker_line_color=COLOR_NOT_DISASTER_DARK,\n                            marker_line_width=2,\n                            opacity=0.6\n                         ), row=2, col=2\n)\n\ncontainer.update_layout(title='Most common N-grams for each tweet type', template='plotly_dark', height=750)\n\ncontainer.update_yaxes(tickangle=0, tickfont=dict(color='white', size=12))\n\ncontainer.show()","ce93589e":"reg = re.compile('\\d*\\s')\n\nTWEETS_DISASTER = [re.sub(reg, ' ', tweet) for tweet in dataf[dataf['target']==1]['clean_text'].tolist()]\ncvec = CountVectorizer(stop_words=STOPWORDS, min_df=10, max_df=0.7, ngram_range=(2,4), tokenizer=tknize)\nsf = cvec.fit_transform(TWEETS_DISASTER)\n\ntransformer = TfidfTransformer()\ntransformed_weights = transformer.fit_transform(sf)\nweights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\nweights_df_disaster = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights}).sort_values('weight', ascending=False)\n\n\nTWEETS_NOT_DISASTER = [re.sub(reg, ' ', tweet) for tweet in dataf[dataf['target']==0]['clean_text'].tolist()]\ncvec = CountVectorizer(stop_words=STOPWORDS, min_df=10, max_df=0.7, ngram_range=(2,4), tokenizer=tknize)\nsf = cvec.fit_transform(TWEETS_NOT_DISASTER)\n\ntransformer = TfidfTransformer()\ntransformed_weights = transformer.fit_transform(sf)\nweights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\nweights_df_not_disaster = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights}).sort_values('weight', ascending=False)","4e93b9d7":"container = make_subplots(rows=1, cols=2, subplot_titles=('Disaster', 'Not disaster'), horizontal_spacing=.3)\n\ntop_n = 40\ncontainer.add_trace(go.Bar(y=weights_df_disaster['term'].iloc[:top_n],\n                            x=weights_df_disaster['weight'].iloc[:top_n],\n                            alignmentgroup='a',\n                            showlegend=True, \n                            legendgroup='Dis',\n                            orientation='h',\n                            name='Disaster',\n                            marker_color=COLOR_DISASTER,\n                            marker_line_color=COLOR_DISASTER_DARK,\n                            marker_line_width=2,\n                            opacity=0.6,\n                         ), row=1, col=1\n)\n\ncontainer.add_trace(go.Bar(y=weights_df_not_disaster['term'].iloc[:top_n],\n                            x=weights_df_not_disaster['weight'].iloc[:top_n],\n                            alignmentgroup='a',\n                            showlegend=True,\n                            legendgroup='NotDis',\n                            orientation='h',\n                            name='Not Disaster',\n                            marker_color=COLOR_NOT_DISASTER,\n                            marker_line_color=COLOR_NOT_DISASTER_DARK,\n                            marker_line_width=2,\n                            opacity=0.6,\n                         ), row=1, col=2\n)\n\ncontainer.update_layout(title='Most weighted (important) terms for each tweet type', template='plotly_dark', height=1000)\n\ncontainer.update_yaxes(tickangle=0, tickfont=dict(color='white', size=14), autorange=\"reversed\")\ncontainer.update_xaxes(showticklabels=False, title='weight', titlefont=dict(size=10), color='gray')\n\ncontainer.show()","07f20366":"# This part is optional, but for making the things faster I will pre-build a vocabulary with the 700 most common words for each type and \n#\u00a0then use it with the TfidfVectorizer so it doesn't build a huge vocabulary. \n# If you wish to use all the words, you simply have to remove \"vocabulary=unigrams\" from TfidfVectorizer instantation\nunigrams_disaster =  pd.DataFrame(nltk.FreqDist(text_disasters).most_common()[:700], columns=['term', 'times'])\nunigrams_not_disaster =  pd.DataFrame(nltk.FreqDist(text_not_disasters).most_common()[:700], columns=['term', 'times'])\nunigrams = pd.concat([unigrams_disaster, unigrams_not_disaster])['term'].unique().tolist()","8ccd8fc9":"# remove \"vocabulary=unigrams\" if you wish to use all the words\nvectorizer = TfidfVectorizer(tokenizer=tknize, vocabulary=unigrams, decode_error='replace')\n\nX = vectorizer.fit_transform(TWEETS_DISASTER + TWEETS_NOT_DISASTER)\nX_D = vectorizer.transform(TWEETS_DISASTER) # Matrix of only the disaster tweets\nX_ND = vectorizer.transform(TWEETS_NOT_DISASTER) # Matrix of only the not disaster tweets","3b9ea21e":"# Train PCA with all tweet embeddings\npca = PCA(n_components=2).fit(X.toarray())","c3b0a03d":"# Get the 2D coordinates + original tweets together.\ndis = pd.DataFrame(pca.transform(X_D.toarray()))\ndis['tweet'] = TWEETS_DISASTER\n\nnotdis = pd.DataFrame(pca.transform(X_ND.toarray()))\nnotdis['tweet'] = TWEETS_NOT_DISASTER","536000d1":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=dis.iloc[:,0], y=dis.iloc[:,1], text=dis['tweet'].values,marker=dict(color=COLOR_DISASTER, size=2.3), mode='markers', opacity=1, name='Disaster'))\nfig.add_trace(go.Scatter(x=notdis.iloc[:,0], y=notdis.iloc[:,1], text=notdis['tweet'].values,marker=dict(color=COLOR_NOT_DISASTER, size=2.3), mode='markers', opacity=.7, name='Not Disaster'))\n\nfig.update_layout(title='2D projection of tweets', template='plotly_dark', height=900)\ncontainer.update_yaxes(showticklabels=False)\ncontainer.update_xaxes(showticklabels=False)\n\nfig.show()","ed030d08":"** How may samples of each class there are? **","a0af6943":"** What are the most used hashtags within each category? **","8f36af53":"This is far better in order to quickly take a look at the top topics the tweets belong to.\n\nFor the last part of the visualization I will calculate the embeddings (vector representations) of the tweets and then use PCA to get a 2D projection of them.","67b8c594":"# VISUAL DATA ANALYSIS OF TWEETS!\n\n** What it's this notebook about? **\n\nThis notebook it's aimed at more novice people who are starting with Data Science or want to start doing some textual analysis \/ NLP. However, you should take a quick look if you are interesting in learning about Plotly or even getting some more ideas to incorporate in your work!\n\n** What I will find here? **\n* Common textual data cleaning (focused on tweets)\n* Some questions about the data awnsered visually\n* Concussions from the visualizations\n* Naive topic detection (most used terms)\n* Classical TF-IDF to find the most important terms\n* Tweets 2D projection\n\nIf you want me to keep updating the notebook, upvote to let me know! :)\n\nHope it gives you some ideas or teach someone something","a0384f1e":"** What can the words tell us? **","dd9fc572":"##\u00a0Data generation \/ cleaning","97316ea5":"Here we can see more differences between the types, especially with the bi-grams\/tri-grams.\n\nBut this are the most repeated n-grams among each group, and there are some terms repeated (old pkk, yr old pkk, etc). It is better to get a stat that takes into account the importance of the terms. I will use TF-IDF for that.","27241095":"Although this TF-IDF + PCA technique it's not perfect (nor State of the art for text sequence embeddings), and it doesn't show pretty well the two classes segmented, I think it illustrates pretty well some kind of topic trend. For example, the tweets projected towards the right part of the plot, talk about fire, and the left ones more about war\/bombs\/nuclear.","8db1b8a0":"** See the most important terms (TF-IDF) **","868c9508":"### Plotting the tweets!\n\nThe steps I'm going to do are:\n\n0. Build a reduced vocabulary with the most common words in the dataset.\n1. Get a matrix of embeddings (each row is a tweet embedding and each column is a dimension which represents a word of the vocabulary).\n2. Reduce the dimensionality of the matrix using PCA.\n3. Plot the 2D projected result.","afb46c34":"##\u00a0Visualizations","186e9247":"## Takeaways\n\n1. Disaster tweets use more the hashtag \"#news\" than the non-disaster ones.\n2. Is more probably that a disaster tweet has one URL (suppose that for linking to the news article).\n3. A great number of tweets talk about the California fires, so we could guess the date range in which this dataset was collected.\n4. The most important terms of each category are pretty well separated (few repetitions between them).\n\n \n>**EXTRAS**\n>1. Plotly is a great tool to plot your data if you want to make it interactive and more visually appealing than standard Matplotlib plots. If you are fine whith using static ones, I personally recomend you to use Seaborn\/Pyplot as, IMO, the code looks more concise and easier to understand.\n>2. There are endless techniques to analize textual data, here I showed most common ones. Go on and test your ideas to awnser more questions!\n>3. This techniques can help us to broadly understand the data we are working with, but for the challenge NLU (Natural Language Understanding) algorithms will be needed.\n\n#### Upvote if you learned something!\n","2d99eda7":"## Data loading","41892e5e":"The main difference we can see here is that disaster tweets tend to link more URLs than the real ones, otherwise, they are pretty much the same\n\nLet's take a look at the most used N-grams of each category.","6473a589":"Seems like disaster news tweets use more the hastag than real ones.","033c749c":"** Function to tokenize texts using only word roots (i.e, singing->sing, players->player) **"}}