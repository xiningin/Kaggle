{"cell_type":{"457d31e5":"code","7481d741":"code","81ae8050":"code","57189740":"code","e9763b02":"code","2a20b980":"code","2cb89312":"code","aa8d1aa6":"code","f4c88b62":"code","f4b4382d":"code","65e7d1bd":"code","7496bf82":"code","9f131427":"markdown","15d6c0ff":"markdown","a0e351e4":"markdown","17136525":"markdown"},"source":{"457d31e5":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedShuffleSplit","7481d741":"df = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")","81ae8050":"df.shape","57189740":"df.head()","e9763b02":"df.info()","2a20b980":"df.describe()","2cb89312":"def transform(df):\n    X = df.copy()\n    y = X[\"Species\"]\n    X.drop([\"Id\", \"Species\"], axis=1, inplace=True)\n\n    temp = list(df[\"Species\"].unique())\n    d = {}\n    for i, j in zip(range(3), temp):\n        d[j] = i \n    \n    y = y.map(d)\n    \n    return [X, y]","aa8d1aa6":"X, y = transform(df)","f4c88b62":"# splitting the data\nsplit = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\nfor i, j in split.split(X, y):\n    X_train, y_train = X.loc[i], y.loc[i]\n    X_test, y_test = X.loc[j], y.loc[j]","f4b4382d":"# Naive Bayes Classifier\nimport scipy.stats\nclass NaiveBayesClassifier:\n\n    def __init__(self):\n        return\n\n    def fit(self, X, y):\n        self.X = X.copy()\n        self.X[\"y\"] = y\n\n    def predict(self, test):\n        preds = self.predict_proba(test)\n        final_preds = []\n        for i in range(len(test)):\n            c = None\n            max = 0\n            for key in preds.keys():\n                if preds[key][i] > max:\n                    c = key\n                    max = preds[key][i]\n            final_preds.append(c)\n\n        return final_preds\n\n    def predict_proba(self, test):\n        preds = []\n        final = {}\n\n        # looping over all labels\n        for label in list(self.X[\"y\"].unique()):\n            probs = []\n\n            # looping over row in the test set\n            for row in range(len(test)):\n                # num, deno = len(self.X[self.X[\"y\"] == label]) \/ len(self.X), 1\n                prob = len(self.X[self.X[\"y\"] == label])\/ len(self.X)\n                \n                # looping over cols in the test set\n                for col in range(len(test.T)):\n                    key = test.iloc[row, col]\n\n                    if len(list(self.X.iloc[:, col].unique())) > 3:\n                        \n                        prob *= scipy.stats.norm(self.X[self.X[\"y\"] == label].iloc[:, col].mean(), self.X[self.X[\"y\"] == label].iloc[:, col].std()).pdf(key)\n\n                    else:\n                        temp = len(self.X[self.X.iloc[:, col] == key])\n                        deno_temp = temp \/ len(self.X)\n                        # deno *= deno_temp\n                        num_temp = len(self.X[(self.X.iloc[:, col] == key) & (self.X[\"y\"] == label)]) \/ len(self.X[self.X[\"y\"] == label])\n                        # num *= num_temp\n                        prob *= (num_temp \/ deno_temp)\n\n                \n                probs.append(prob)\n            final[label] = probs\n        return final\n","65e7d1bd":"model = NaiveBayesClassifier()\nmodel.fit(X_train, y_train)","7496bf82":"preds = model.predict(X_test)\nprint(accuracy_score(y_test, preds))","9f131427":"# Model training\nI will be using my own model that works on the concept of Naive Bayes algorithm.","15d6c0ff":"# Preprocessing","a0e351e4":"# Importing data and libraries","17136525":"# Understanding the data"}}