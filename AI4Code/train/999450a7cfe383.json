{"cell_type":{"cb17de49":"code","1151c8ef":"code","7ef2118d":"code","64bcd235":"code","e60f71da":"code","95adb96f":"code","27b2daf4":"code","50961727":"code","2ab88830":"code","0fd8da5f":"code","fb19ec55":"code","f4fc5c0c":"code","8a92d291":"code","1ebc843f":"code","d906f630":"code","c1a4d4ce":"code","1eaa29c3":"code","ad5ccfb9":"code","6d9134ba":"code","1706b8ef":"code","f8e1af4d":"markdown","2777e9ff":"markdown","c1bcdb74":"markdown","47f66752":"markdown","cc5f6530":"markdown","c7465724":"markdown"},"source":{"cb17de49":"#Loading Libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn.datasets as datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.tree import plot_tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import classification_report ","1151c8ef":"# Loading the iris dataset\nurl = \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.data\"\ndf = pd.read_csv(url, header=None, names=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n                                          'petal width (cm)', 'Species'])\n\ndf.head() # To view first 5 rows","7ef2118d":"# To know number of rows and collumns\ndf.shape","64bcd235":"# Check the dataframe information\ndf.info()","e60f71da":"# To find if any null value is present\ndf.isnull().sum()","95adb96f":"# To see summary statistics\ndf.describe().T","27b2daf4":"# To find outliers\ncols = df.columns[0:-1]\nfor i in cols:\n    sns.boxplot(y=df[i])\n    plt.show()","50961727":"# To remove outliers from 'sepal width (cm)'\nq1 = df['sepal width (cm)'].quantile(0.25)\nq3 = df['sepal width (cm)'].quantile(0.75)\niqr = q3 - q1\ndf = df[(df['sepal width (cm)'] >= q1-1.5*iqr) & (df['sepal width (cm)'] <= q3+1.5*iqr)]\ndf.shape # To find out the number of rows and column after outlier treatment","2ab88830":"# Blocplot for sepal width (cm) after outlier treatment\nsns.boxplot(y=df['sepal width (cm)'])\nplt.show()","0fd8da5f":"# Splitting the data into train and test sets\nX = df.drop(\"Species\",axis=1)\ny = df[\"Species\"]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state= 1)","fb19ec55":"# Defining an object for DTC and fitting for whole dataset\ndt = DecisionTreeClassifier(max_depth=3, min_samples_leaf=10, random_state=1 )\ndt.fit(X, y)","f4fc5c0c":"# Plotting of decission tree\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\n\n!pip install pydotplus\nimport pydotplus\n\n\nfeatures = X.columns\ndot_data = export_graphviz(dt, out_file=None, feature_names=features)\ngraph = pydotplus.graph_from_dot_data(dot_data)\nImage(graph.create_png())","8a92d291":"# Defining an object for DTC and fitting for train dataset\ndt = DecisionTreeClassifier(random_state=1)\ndt.fit(X_train, y_train)\n\ny_pred_train = dt.predict(X_train)\ny_pred = dt.predict(X_test)\ny_prob = dt.predict_proba(X_test)","1ebc843f":"print('Accuracy of Decision Tree-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of Decision Tree-Test: ', accuracy_score(y_pred, y_test))","d906f630":"#Classification for test before hyperparameter tuning\nprint(classification_report(y_test,y_pred))","c1a4d4ce":"# Hyperparameter Tuning of DTC\n\ndt = DecisionTreeClassifier(random_state=1)\n\nparams = {'max_depth' : [2,3,4,5],\n        'min_samples_split': [2,3,4,5],\n        'min_samples_leaf': [1,2,3,4,5]}\n\ngsearch = GridSearchCV(dt, param_grid=params, cv=3)\n\ngsearch.fit(X,y)\n\ngsearch.best_params_","1eaa29c3":"# Passing best parameter for the Hyperparameter Tuning\ndt = DecisionTreeClassifier(**gsearch.best_params_, random_state=1)\n\ndt.fit(X_train, y_train)\n\ny_pred_train = dt.predict(X_train)\ny_prob_train = dt.predict_proba(X_train)[:,1]\n\ny_pred = dt.predict(X_test)\ny_prob = dt.predict_proba(X_test)[:,1]","ad5ccfb9":"print('Confusion Matrix - Train:','\\n',confusion_matrix(y_train,y_pred_train))\nprint('\\n','Confusion Matrix - Test:','\\n',confusion_matrix(y_test,y_pred))","6d9134ba":"#Classification for test after hyperparameter tuning\nprint(classification_report(y_test,y_pred))","1706b8ef":"print('Accuracy of Decision Tree-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of Decision Tree-Test: ', accuracy_score(y_pred, y_test))","f8e1af4d":"- After hyperparameter tuning we can say the overfitting has been reduced slightly","2777e9ff":"- From the above boxplot we can say that there are outliers in the column 'sepal width (cm)'","c1bcdb74":"- There are 4 - Numerical Features and one categorical column\n- There are totally 150 rows or observations are in data","47f66752":"- The accuracy of the DT has slightly overfit\n- Shall try hyperparameter tuning to reduce overfitting if possible","cc5f6530":"- We can see that after outlier treatment the number of rows are reduced to 146 from 150","c7465724":"## To Explore Decision Tree Algorithm\nFor the given \u2018Iris\u2019 dataset, create the Decision Tree classifier and visualize it graphically. The purpose is if we feed any new data to this classifier, it would be able to predict the right class accordingly."}}