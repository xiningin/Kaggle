{"cell_type":{"e6b7ae8a":"code","b5d67938":"code","f757e3b7":"code","0797b08b":"code","6da6fec7":"code","c33204f6":"code","ad525ab7":"code","eaea1334":"code","da5e5bbd":"code","20d146ca":"code","4a5303e9":"code","5a00194d":"code","ffe0bf88":"code","259d4a36":"code","06da52a7":"code","f6f1bd0d":"code","62e95954":"code","91bafa7b":"code","8d291a58":"code","80bf2ca4":"code","d2696e87":"code","5f91eb4c":"code","206a5ea4":"code","432aa8c7":"code","40c44d40":"code","5964afbb":"code","1bae3d2f":"code","789c971e":"code","af51e05e":"code","b285b340":"code","15e25052":"code","7982b53b":"code","06c3f300":"markdown","cb070324":"markdown","c3ea4ea2":"markdown","5f0dcc50":"markdown","940d6d76":"markdown","294a786e":"markdown","c5609318":"markdown","9654893d":"markdown","89ccbea3":"markdown","ab833a6a":"markdown","b876016b":"markdown","812b83cb":"markdown","33b0eadf":"markdown","655a75be":"markdown","3e4ff13b":"markdown"},"source":{"e6b7ae8a":"# import packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b5d67938":"%matplotlib inline\nsns.set_style(\"darkgrid\")","f757e3b7":"# import dataset\niris = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\niris.head()","0797b08b":"# check the data\niris.info()","6da6fec7":"# check the statistics\niris.describe()","c33204f6":"# Visualize distribution of indivisual parameter for different species\nplt.figure(figsize=(15, 15))\n# Sepal length\nplt.subplot(2, 2, 1)\nsns.violinplot(x='Species', y ='SepalLengthCm', data=iris, inner='quartile', palette='Paired')\nplt.title('Sepal Length for Different Species', fontsize=12)\nplt.xlabel('Species', fontsize=10)\nplt.ylabel('Sepal Length(cm)', fontsize=10);\n\n# Sepal width\nplt.subplot(2, 2, 2)\nsns.violinplot(x='Species', y ='SepalWidthCm', data=iris, inner='quartile', palette='Paired')\nplt.title('Sepal width for Different Species', fontsize=12)\nplt.xlabel('Species', fontsize=10)\nplt.ylabel('Sepal Width(cm)', fontsize=10);\n\n# Petal length\nplt.subplot(2, 2, 3)\nax3 = sns.violinplot(x='Species', y ='PetalLengthCm', data=iris, inner='quartile', palette='Paired')\nplt.title('Petal Length for Different Species', fontsize=12)\nplt.xlabel('Species', fontsize=10)\nplt.ylabel('Petal Width(cm)', fontsize=10);\n\n# petal width\nplt.subplot(2, 2, 4)\nsns.violinplot(x='Species', y ='PetalWidthCm', data=iris, inner='quartile', palette='Paired')\nplt.title('Petal width for Different Species', fontsize=12)\nplt.xlabel('Species', fontsize=10)\nplt.ylabel('Petal Width(cm)', fontsize=10);","ad525ab7":"# Visualize relationship between parameters\ng = sns.PairGrid(data=iris, vars=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'], hue='Species', size=3, palette='Set2')\ng.map_diag(sns.kdeplot)\ng.map_offdiag(plt.scatter)\ng.add_legend();","eaea1334":"# plot correlation matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(iris.drop('Id', axis=1).corr(), cmap='YlGnBu', annot=True, fmt='.2f', vmin=0);","da5e5bbd":"features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\nX = iris[features]\ny = iris['Species']\n\n# PCA is affected by scale, scale the dataset first\nfrom sklearn import preprocessing \n# Standardizing the features\nX = preprocessing.StandardScaler().fit_transform(X)","20d146ca":"# Set up 2 pca components\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nprint(pca.explained_variance_ratio_)","4a5303e9":"# get the new df\ndf = pd.DataFrame(data = components, columns = ['pc1', 'pc2'])\ndf = pd.concat([df, iris[['Species']]], axis = 1)\n\n\n# get dummy value\nle = preprocessing.LabelEncoder()\nle.fit(df['Species'])\ndf['target']=le.transform(df['Species'])\n\ndf.head()","5a00194d":"# visualize the new df\ng = sns.FacetGrid(data=df, hue='Species', palette='Set2', size=8)\ng.map(plt.scatter, 'pc1', 'pc2')\ng.add_legend();","ffe0bf88":"# get training and testing data\nfrom sklearn.model_selection import train_test_split\nX = df[['pc1', 'pc2']]\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","259d4a36":"X = df[['pc1', 'pc2']]\ny = df['target']\nx_min, x_max = X.iloc[:, 0].min()-1, X.iloc[:, 0].max()+1\ny_min, y_max = X.iloc[:, 1].min()-1, X.iloc[:, 1].max()+1\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = clf_nb.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nZ","06da52a7":"# visualize function\ndef DecisionBoundary(clf):\n    X = df[['pc1', 'pc2']]\n    y = df['target']\n    \n    h = .02  # step size in the mesh\n\n    # Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n    \n    #Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.figure(figsize=(8, 8))\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n    \n    # Plot also the training points\n    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.show()","f6f1bd0d":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\n# fit the model\nclf_nb = GaussianNB()\nclf_nb.fit(X_train, y_train)\n\npred_nb = clf_nb.predict(X_test)\n\n# get the accuracy score\nacc_nb = accuracy_score(pred_nb, y_test)\nprint(acc_nb)","62e95954":"# Visualize the model\nDecisionBoundary(clf_nb)","91bafa7b":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10, 100]}\n# fit the model, use gridsearch to choose the best params\nsvr = SVC()\nclf_svc = GridSearchCV(svr, parameters)\nclf_svc.fit(X_train, y_train)\nclf_svc.best_params_","8d291a58":"# fit with best parameter\nsvr2 = SVC(kernel='rbf', C=1)\nsvr2.fit(X_train, y_train)\n\npred_svr = svr.predict(X_test)\n# get the accuracy score\nacc_svr = accuracy_score(pred_svr, y_test)\nprint(acc_svr)","80bf2ca4":"# Visualize the model\nDecisionBoundary(svr2)","d2696e87":"# fit the decision tree model\nfrom sklearn import tree\nclf_tree = tree.DecisionTreeClassifier(min_samples_split=8)\nclf_tree.fit(X_train, y_train)\n\npred_tree = clf_tree.predict(X_test)\n\n# get the accuracy score\nacc_tree = accuracy_score(pred_tree, y_test)\nprint(acc_tree)","5f91eb4c":"DecisionBoundary(clf_tree)","206a5ea4":"# fit the model\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\nclf_rf.fit(X_train, y_train)\n\npred_rf = clf_tree.predict(X_test)\n\n# get the accuracy score\nacc_rf = accuracy_score(pred_rf, y_test)\nprint(acc_rf)","432aa8c7":"DecisionBoundary(clf_rf)","40c44d40":"# fit logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlm = LogisticRegression()\nlm.fit(X_train, y_train)\n\npred_lm = lm.predict(X_test)\nacc_lm = accuracy_score(pred_lm, y_test)\nprint(acc_lm)","5964afbb":"DecisionBoundary(lm)","1bae3d2f":"# fit the model\nfrom sklearn.neighbors import KNeighborsClassifier\nclf_knn = KNeighborsClassifier(n_neighbors=5) \nclf_knn.fit(X_train,y_train) \n\npred_knn = clf_knn.predict(X_test) \nacc_knn = accuracy_score(pred_knn, y_test)\nprint(acc_knn)","789c971e":"DecisionBoundary(clf_knn)","af51e05e":"data_dict = {'Naive Bayes': [acc_nb], 'SVM': [acc_svr], 'DT': [acc_tree], 'Random Forest': [acc_rf], 'Logistic Regression': [acc_lm], 'K_nearest Neighbors': [acc_knn]}\ndf_c = pd.DataFrame.from_dict(data_dict, orient='index', columns=['Accuracy Score'])\nprint(df_c)","b285b340":"# recall and precision\nfrom sklearn.metrics import recall_score, precision_score\n\n# params for nb \nrecall_nb = recall_score(pred_nb, y_test, average = None)\nprecision_nb = precision_score(pred_nb, y_test, average = None)\nprint('precision score for naive bayes: {}\\n recall score for naive bayes:{}'.format(precision_nb, recall_nb))","15e25052":"# params for knn\nrecall_knn = recall_score(pred_knn, y_test, average = None)\nprecision_knn = precision_score(pred_knn, y_test, average = None)\nprint('precision score for naive bayes: {}\\n recall score for naive bayes:{}'.format(precision_knn, recall_knn))","7982b53b":"DecisionBoundary(clf_nb)","06c3f300":"## PCA\nSince the sepal length, and petal size are highly correlated, and they are not correlated with the sepal width, maybe it's worth to use PCA to create latent feaures. ","cb070324":"From the accuracy score, we can see that Naive Bayes and K_nearest Neighbors predict the best. Next let's take a closer look at the recall and precision of these two methods.","c3ea4ea2":"In general, naive bayes seems to yield a better result! But we can tweak the parameters more to see wheterh there's a better solution","5f0dcc50":"## SVM","940d6d76":"# Conclusion","294a786e":"## K-nearest neighbours","c5609318":"## Decision Tree","9654893d":"# Model Selection","89ccbea3":"## Naive Bayes","ab833a6a":"# EDA","b876016b":"## Set up","812b83cb":"# Introduction","33b0eadf":"## Random Forest\nThe decision tree model show some overfiting, maybe we can try random forest to minimize the overfiting.","655a75be":"The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository. It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.The Iris dataset is a dataset for classification, machine learning, and data visualization.\n\nThe dataset contains: 3 classes (different Iris species) with 50 samples each, and then four numeric properties about those classes: Sepal Length, Sepal Width, Petal Length, and Petal Width.\n\nThe purpose of this notebook is to practice ML concepts along with data visualization. If you find it useful, please UPVOTE :)","3e4ff13b":"## Logistic Regression"}}