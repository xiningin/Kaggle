{"cell_type":{"3248b8ff":"code","56397998":"code","395c6b4f":"code","135b95f9":"code","4569cee7":"code","6fc3102b":"code","3dd8e699":"code","27f7787b":"code","60c495f1":"code","9b6c4165":"code","6fb49ad3":"code","4a9d7093":"code","dd0dc245":"code","cb4c5458":"code","f1ee3341":"code","64e2fc4f":"code","e4f234ec":"code","2a9c6a61":"code","d30338d1":"code","e0e6cf68":"code","81b3ba47":"code","dd2f0458":"code","a628ff20":"code","b5093730":"code","aabef21d":"code","dbd4fd1e":"code","9615dd7b":"code","6d5c3d96":"markdown","7200e457":"markdown","a22e63a8":"markdown","121f4eb8":"markdown","783499ee":"markdown","4eccecd6":"markdown","88bac947":"markdown","24caf739":"markdown","917dc0df":"markdown"},"source":{"3248b8ff":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd    \nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\ncredit = pd.read_csv('..\/input\/credit-card-approval-prediction\/credit_record.csv')  \napplication = pd.read_csv('..\/input\/credit-card-approval-prediction\/application_record.csv') ","56397998":"grouped = credit.groupby('ID')\n### convert credit data to wide format which every ID is a row\npivot_tb = credit.pivot(index = 'ID', columns = 'MONTHS_BALANCE', values = 'STATUS')\npivot_tb['open_month'] = grouped['MONTHS_BALANCE'].min() # smallest value of MONTHS_BALANCE, is the month when loan was granted\npivot_tb['end_month'] = grouped['MONTHS_BALANCE'].max() # biggest value of MONTHS_BALANCE, might be observe over or canceling account\npivot_tb['ID'] = pivot_tb.index\npivot_tb = pivot_tb[['ID', 'open_month', 'end_month']]\npivot_tb['window'] = pivot_tb['end_month'] - pivot_tb['open_month'] # calculate observe window\npivot_tb.reset_index(drop = True, inplace = True)\ncredit = pd.merge(credit, pivot_tb, on = 'ID', how = 'left') # join calculated information\ncredit0 = credit.copy()\ncredit = credit[credit['window'] > 20] # delete users whose observe window less than 20\ncredit['status'] = np.where((credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 1, 0) # analyze > 60 days past due \ncredit['status'] = credit['status'].astype(np.int8) # 1: overdue 0: not\ncredit['month_on_book'] = credit['MONTHS_BALANCE'] - credit['open_month'] # calculate month on book: how many months after opening account\ncredit.sort_values(by = ['ID','month_on_book'], inplace = True)\n\n##### denominator\ndenominator = pivot_tb.groupby(['open_month']).agg({'ID': ['count']}) # count how many users in every month the account was opened\ndenominator.reset_index(inplace = True)\ndenominator.columns = ['open_month','sta_sum']\n\n##### ventage table\nvintage = credit.groupby(['open_month','month_on_book']).agg({'ID': ['count']}) \nvintage.reset_index(inplace = True)\nvintage.columns = ['open_month','month_on_book','sta_sum'] \nvintage['due_count'] = np.nan\nvintage = vintage[['open_month','month_on_book','due_count']] # delete aggerate column\nvintage = pd.merge(vintage, denominator, on = ['open_month'], how = 'left') # join sta_sum colun to vintage table","395c6b4f":"larger_window = abs(vintage['open_month'].min())\nfor j in range(-larger_window,1): # outer loop: month in which account was opened\n    ls = []\n    for i in range(0,larger_window+1): # inner loop time after the credit card was granted\n        due = list(credit[(credit['status'] == 1) & (credit['month_on_book'] == i) & (credit['open_month'] == j)]['ID']) # get ID which satisfy the condition\n        ls.extend(due) # As time goes, add bad customers\n        vintage.loc[(vintage['month_on_book'] == i) & (vintage['open_month'] == j), 'due_count'] = len(set(ls)) # calculate non-duplicate ID numbers using set()\n        \nvintage['sta_rate']  = vintage['due_count'] \/ vintage['sta_sum'] # calculate cumulative % of bad customers        ","135b95f9":"def calculate_observe(credit, command):\n    '''calculate observe window\n    '''\n    larger_window = abs(credit['MONTHS_BALANCE'].min())\n    id_sum = len(set(pivot_tb['ID']))\n    credit['status'] = 0\n    exec(command)\n    #credit.loc[(credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\n    credit['month_on_book'] = credit['MONTHS_BALANCE'] - credit['open_month']\n    minagg = credit[credit['status'] == 1].groupby('ID')['month_on_book'].min()\n    minagg = pd.DataFrame(minagg)\n    minagg['ID'] = minagg.index\n    obslst = pd.DataFrame({'month_on_book':range(0,larger_window + 1), 'rate': None})\n    lst = []\n    for i in range(0,larger_window + 1):\n        due = list(minagg[minagg['month_on_book']  == i]['ID'])\n        lst.extend(due)\n        obslst.loc[obslst['month_on_book'] == i, 'rate'] = len(set(lst)) \/ id_sum \n    return obslst['rate']\n\ncommand = \"credit.loc[(credit['STATUS'] == '0') | (credit['STATUS'] == '1') | (credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"   \nmorethan1 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '1') | (credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"   \nmorethan30 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan60 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan90 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan120 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan150 = calculate_observe(credit, command)","4569cee7":"def calculate_rate(pivot_tb, command): \n    '''calculate bad customer rate\n    '''\n    credit0['status'] = None\n    exec(command) # excuate input code\n    sumagg = credit0.groupby('ID')['status'].agg(sum)\n    pivot_tb = pd.merge(pivot_tb, sumagg, on = 'ID', how = 'left')\n    pivot_tb.loc[pivot_tb['status'] > 1, 'status'] = 1\n    rate = pivot_tb['status'].sum() \/ len(pivot_tb)\n    return round(rate, 5)\n\ncommand = \"credit0.loc[(credit0['STATUS'] == '0') | (credit0['STATUS'] == '1') | (credit0['STATUS'] == '2') | (credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"   \nmorethan1 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '1') | (credit0['STATUS'] == '2') | (credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"   \nmorethan30 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '2') | (credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan60 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan90 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan120 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan150 = calculate_rate(pivot_tb, command)","6fc3102b":"#\"Bad\" client are identified as client that past due more than 30 days\ny = credit0[['ID','STATUS','status']]\ny['status'] = 0 #0 is the label for a \"good\" client\nexec(\"y.loc[(y['STATUS'] == '1') | (y['STATUS'] == '2') | (y['STATUS'] == '3' )| (y['STATUS'] == '4' )| (y['STATUS'] == '5'), 'status'] = 1\") #1 is the label for a \"Bad\" client\ny = y[['ID','status']].rename(columns={\"ID\": \"ID\", \"status\": \"target\"})\ny","3dd8e699":"#Numerical features\napplication = application.replace(['N','Y'],[0,1]) #Converts Yes\/No in 1\/0\napplication = application.rename(columns={\"CODE_GENDER\":\"F\", \"NAME_EDUCATION_TYPE\":\"EDUCATION\"})\napplication = application.replace(['F','M'],[1,0]) #Converts Female\/Male in 1\/0\napplication = application.replace(['Academic degree', 'Higher education', 'Incomplete higher', \n                                   'Secondary \/ secondary special', 'Lower secondary'],\n                                  [4,3,2,1,0]) #Converts education level into numerical features","27f7787b":"#Convert categorical variables\napplication['OCCUPATION_TYPE'] = application['OCCUPATION_TYPE'].apply(lambda x : 'Unknown' if pd.isnull(x) else x)\napplication = pd.get_dummies(application)","60c495f1":"#Normalize the dataset\n\n# apply the maximum absolute scaling in Pandas using the .abs() and .max() methods\ndef normal_scaling(df):\n    # copy the dataframe\n    df_scaled = df.copy()\n    # list to save the normal coef\n    normal_coefs = []\n    #We don't want to normalize the ID\n    columns = list(df.columns)\n    columns.remove(\"ID\") \n    for column in columns:\n        normal_coefs.append((df_scaled[column].mean(),df_scaled[column].std()))\n        df_scaled[column] = (df_scaled[column]-normal_coefs[-1][0]) \/ normal_coefs[-1][1]\n    return df_scaled, normal_coefs\n    \n# call the maximum_absolute_scaling function\napplication, normal_coefs = normal_scaling(application)\n\napplication = application.fillna(0)","9b6c4165":"#Merge both datasets\ntrain_valid_dataset = application\ntrain_valid_dataset = train_valid_dataset.merge(y, on=\"ID\", how=\"inner\").drop(columns=[\"ID\"])\n#train_valid_dataset = train_valid_dataset[:10000] #Test only\ntrain_valid_dataset = train_valid_dataset.to_numpy()","6fb49ad3":"#Split between train and test set\n\nimport torch\nimport torchvision.transforms as transforms\n\nvalid_ratio = 0.2  # Going to use 80%\/20% split for train\/valid\nweight_tensor = torch.tensor([len(train_valid_dataset)\/(len(train_valid_dataset)-train_valid_dataset[:,-1].sum()), \n                              len(train_valid_dataset)\/train_valid_dataset[:,-1].sum()]).float() #weight_matrix\n\n# Split it into training and validation sets\nnb_train = int((1.0 - valid_ratio) * len(train_valid_dataset))\nnb_valid =  int(valid_ratio * len(train_valid_dataset))\ntrain_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n\n#Define device\nuse_gpu = torch.cuda.is_available()\nif use_gpu:\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')","4a9d7093":"#Convert the dataset to a tensor\n\nclass DatasetTransformer(torch.utils.data.Dataset):\n\n    def __init__(self, base_dataset, transform=transforms.Lambda(lambda x: x)):\n        self.base_dataset = base_dataset\n        self.transform = transform\n\n    def __getitem__(self, index):\n        inpt, target = torch.from_numpy(self.base_dataset[index][:-1]), self.base_dataset[index][-1]\n        return self.transform(inpt).float(), int(target)\n\n    def __len__(self):\n        return len(self.base_dataset)\n\n\ntrain_dataset = DatasetTransformer(train_dataset)\nvalid_dataset = DatasetTransformer(valid_dataset)","dd0dc245":"#Dataloader\n\nnum_threads = 4     # Loading the dataset is using Y CPU threads\nbatch_size  = 1024   # Using minibatches of X samples\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=True,              # <-- this reshuffles the data at every epoch\n                                          num_workers=num_threads)\n\nvalid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False,\n                                          num_workers=num_threads)\n\n\nprint(\"The train set contains {} samples, in {} batches\".format(len(train_loader.dataset), len(train_loader)))\nprint(\"The validation set contains {} samples, in {} batches\".format(len(valid_loader.dataset), len(valid_loader)))","cb4c5458":"#Neural Network with fully connected layers\n\nimport torch.nn as nn\n\ndef linear_relu(dim_in, dim_out):\n    return [nn.Linear(dim_in, dim_out),\n            nn.ReLU(inplace=True)]\n\nclass FullyConnected(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(FullyConnected, self).__init__()\n        self.classifier =  nn.Sequential(\n            #nn.Dropout(0.2),\n            *linear_relu(input_size, 32),\n            #nn.Dropout(0.5), #Generally 0.2 for the input layer and 0.5 for the hidden layer\n            *linear_relu(32, 32),\n            #nn.Dropout(0.5),\n            nn.Linear(32, num_classes)\n        )\n\n    def forward(self, x):\n        x = x.view(x.size()[0], -1)\n        y = self.classifier(x)\n        return y\n\n\nmodel = FullyConnected(48, 2)\nmodel.to(device)","f1ee3341":"def train(model, loader, f_loss, optimizer, device):\n    \"\"\"\n    Train a model for one epoch, iterating over the loader\n    using the f_loss to compute the loss and the optimizer\n    to update the parameters of the model.\n\n    Arguments :\n\n        model     -- A torch.nn.Module object\n        loader    -- A torch.utils.data.DataLoader\n        f_loss    -- The loss function, i.e. a loss Module\n        optimizer -- A torch.optim.Optimzer object\n        device    -- a torch.device class specifying the device\n                     used for computation\n\n    Returns :\n    \"\"\"\n\n    # We enter train mode. This is useless for the linear model\n    # but is important for layers such as dropout, batchnorm, ...\n    model.train()\n\n    for i, (inputs, targets) in enumerate(loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        # Compute the forward pass through the network up to the loss\n        outputs = model(inputs)\n        loss = f_loss(outputs, targets)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","64e2fc4f":"def test(model, loader, f_loss, device):\n    \"\"\"\n    Test a model by iterating over the loader\n\n    Arguments :\n\n        model     -- A torch.nn.Module object\n        loader    -- A torch.utils.data.DataLoader\n        f_loss    -- The loss function, i.e. a loss Module\n        device    -- The device to use for computation \n\n    Returns :\n\n        A tuple with the mean loss, mean accuracy and mean unbiaised accuracy\n\n    \"\"\"\n    # We disable gradient computation which speeds up the computation\n    # and reduces the memory usage\n    with torch.no_grad():\n        # We enter evaluation mode. This is useless for the linear model\n        # but is important with layers such as dropout, batchnorm, ..\n        model.eval()\n        N = 0\n        tot_loss, correct, unbiaised_acc = 0.0, 0.0, 0.0\n        for i, (inputs, targets) in enumerate(loader):\n\n            # We got a minibatch from the loader within inputs and targets\n\n            # We need to copy the data on the GPU if we use one\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Compute the forward pass, i.e. the scores for each input\n            outputs = model(inputs)\n\n            # We accumulate the exact number of processed samples\n            N += inputs.shape[0]\n\n            # We accumulate the loss considering\n            # The multipliation by inputs.shape[0] is due to the fact\n            # that our loss criterion is averaging over its samples\n            tot_loss += inputs.shape[0] * f_loss(outputs, targets).item()\n\n            # For the accuracy, we compute the labels for each input\n            # Be carefull, the model is outputing scores and not the probabilities\n            # But given the softmax is not altering the rank of its input scores\n            # we can compute the label by argmaxing directly the scores\n            predicted_targets = outputs.argmax(dim=1)\n            correct += (predicted_targets == targets).sum().item()\n            \n            #Compute the unbiaised accuracy\n            for value in predicted_targets.unique() :\n                mask = (predicted_targets == targets) & (predicted_targets == value.item())\n                unbiaised_acc += mask.sum().item()\/(targets == value.item()).sum().item() * (inputs.shape[0]\/len(predicted_targets.unique()))\n            \n        return tot_loss\/N, correct\/N, unbiaised_acc\/N","e4f234ec":"#To save the best model over epochs\n\nclass ModelCheckpoint:\n\n    def __init__(self, filepath, model):\n        self.min_loss = None\n        self.filepath = filepath\n        self.model = model\n\n    def update(self, loss):\n        if (self.min_loss is None) or (loss < self.min_loss):\n            print(\"Saving a better model\")\n            torch.save(self.model.state_dict(), self.filepath)\n            self.min_loss = loss\n            \n            \nmodel_path = \"best_model.pt\"","2a9c6a61":"epochs = 50\noptimizer = torch.optim.Adam(model.parameters())\nf_loss = torch.nn.CrossEntropyLoss(weight=weight_tensor.to(device))\nmodel_checkpoint = ModelCheckpoint(model_path, model)\n\nfor t in range(epochs):\n    print(\"\\nEpoch {}\".format(t))\n    train(model, train_loader, f_loss, optimizer, device)\n    train_loss, train_acc, train_unb_acc = test(model, train_loader, f_loss, device)\n    print(\" Train : Loss : {:.4f}, Acc : {:.4f}, Unb.Acc. : {:.4f}\".format(train_loss, train_acc, train_unb_acc))\n\n    val_loss, val_acc, val_unb_acc = test(model, valid_loader, f_loss, device)\n    print(\" Validation : Loss : {:.4f}, Acc : {:.4f}, Unb.Acc. : {:.4f}\".format(val_loss, val_acc, val_unb_acc))\n\n    model_checkpoint.update(val_loss)\n\n\nmodel.load_state_dict(torch.load(model_path))\n\n# Switch to eval mode \nmodel.eval()\n\ntest_loss, test_acc, test_unb_acc = test(model, valid_loader, f_loss, device)\nprint(\"\\n\\n Test : Loss : {:.4f}, Acc. : {:.4f}, Unb.Acc. : {:.4f}\".format(test_loss, test_acc, test_unb_acc))","d30338d1":"#Plot the confusion matrix\ndef confusion(model, loader, device):\n    \"\"\"\n    Display the confusion matrix after iterating over the loader\n\n    Arguments :\n\n        model     -- A torch.nn.Module object\n        loader    -- A torch.utils.data.DataLoader\n        device    -- The device to use for computation \n\n    \"\"\"\n    # We disable gradient computation which speeds up the computation\n    # and reduces the memory usage\n    with torch.no_grad():\n        # We enter evaluation mode. This is useless for the linear model\n        # but is important with layers such as dropout, batchnorm, ..\n        model.eval()\n        N = 0\n        TP, FP, TN, FN = 0.0, 0.0, 0.0, 0.0\n        for i, (inputs, targets) in enumerate(loader):\n            # We got a minibatch from the loader within inputs and targets\n            \n            # We need to copy the data on the GPU if we use one\n            inputs, targets = inputs.to(device), targets.to(device)\n            N += inputs.shape[0]\n                \n            # Compute the forward pass, i.e. the scores for each input\n            outputs = model(inputs)\n            predicted_targets = outputs.argmax(dim=1)\n            \n            #Compute coefficients\n            TP += ((predicted_targets == targets) & (targets == 1)).sum().item()\n            FP += ((predicted_targets != targets) & (targets == 0)).sum().item()\n            TN += ((predicted_targets == targets) & (targets == 0)).sum().item()\n            FN += ((predicted_targets != targets) & (targets == 1)).sum().item()\n\n        cm_list = [['{0:.2%}'.format(TP\/N),'{0:.2%}'.format(FN\/N)],['{0:.2%}'.format(FP\/N),'{0:.2%}'.format(TN\/N)]] \n        cm_list[0].insert(0,'Real Rejected')\n        cm_list[1].insert(0,'Real Accepted')\n        print(tabulate(cm_list,headers=['Real\/Pred','Pred Rejected', 'Pred Accepted']))\n\nconfusion(model, valid_loader, device)","e0e6cf68":"#Get features' names\nfeatures = list(application.columns)[1:]\nnb_features = 5","81b3ba47":"#Measure the features impact on decision according to layers' weights\n#In the following analysis we ignore the impact of bias and ReLu\nlayers_array = []\n\nfor layer_param in list(model.parameters()) :\n    try : \n        a,b = layer_param.size() #Parameters are the weights of inputs\n        layers_array.append(np.array(layer_param.data))\n        \n    except : \n        #These paremeters are related to bias\n        continue","dd2f0458":"#Look at filters used by the neural network\n#Usefull for images analysis or any NN where data is ordered. \n#Add a regularization L1 to see if results are better\n#Try to order inputs by topics (job, revenu, status) and add legend\n\nimport matplotlib.pyplot as plt\n\n#We compute the relationship with inputs for every neurons by recursive matrix product\nlayer_weight = np.identity(len(features))\nfor ind in range(0,len(layers_array)) : \n    layer_weight = np.dot(layers_array[ind], layer_weight)\n    plt.imshow(layer_weight)\n    plt.title(\"Layer %s\" %(ind+1))\n    plt.colorbar()\n    plt.show()","a628ff20":"#Print the more determinant features\n\ndef invert_normal(value, normal_coefs) : \n    return (value*normal_coefs[1]) #No interest to add the average as we do a difference before calling this functions\n\ndef top_features(list_weight, normal=False) :\n    if normal :\n        return list(map(lambda x : [features[x],\"{:.2f}\".format(invert_normal(list_weight[x],normal_coefs[x]))], list_weight.argsort()[:nb_features]))\n    return list(map(lambda x : [features[x],\"{:.2f}\".format(list_weight[x])], list_weight.argsort()[:nb_features]))\n\ndef last_features(list_weight, normal=False) :\n    if normal : \n        return list(map(lambda x : [features[x],\"{:.2f}\".format(invert_normal(list_weight[x],normal_coefs[x]))], list_weight.argsort()[-nb_features:]))   \n    return list(map(lambda x : [features[x],\"{:.2f}\".format(list_weight[x])], list_weight.argsort()[-nb_features:]))\n\n\nprint('%s more determinant features for loan attribution' % (nb_features*2))\ndisp_tab = top_features(layer_weight[0])\ndisp_tab.append(['...','...'])\ndisp_tab += last_features(layer_weight[0])\nprint(tabulate(disp_tab[::-1], headers=['Features','weight']))\n\nprint('\\n\\n%s more determinant features for loan rejection' % (nb_features*2))\ndisp_tab = top_features(layer_weight[1])\ndisp_tab.append(['...','...'])\ndisp_tab += last_features(layer_weight[1])\nprint(tabulate(disp_tab[::-1], headers=['Features','weight']))","b5093730":"#Select clients\nnb_samples = 5\ntarget = 1 #0 loan accepted, 1 refused\n\ndef select_samples(model, loader, nb_samples, target):\n    \"\"\"\n    Return \"nb_samples\" estimations of the neural network of the target \"target\"\n\n    Arguments :\n\n        model     -- A torch.nn.Module object\n        loader    -- A torch.utils.data.DataLoader\n        nb_samples -- Number of samples to get\n        target    -- Desired target\n\n    \"\"\"\n    \n    # We disable gradient computation which speeds up the computation\n    # and reduces the memory usage\n    with torch.no_grad():\n        # We enter evaluation mode. This is useless for the linear model\n        # but is important with layers such as dropout, batchnorm, ..\n        model.eval()\n        \n        #Defining the returned tensors\n        inputs_tensor, outputs_tensor = torch.Tensor(), torch.Tensor()\n        current_samples = 0\n        total_samples = len(loader.dataset)\n        \n        while current_samples < nb_samples :\n            idx = int(np.random.random()*total_samples) #We chose randomly an index\n            inpt = valid_loader.dataset[idx][0].reshape(1,-1)\n            outpt = model(inpt) #We keep only the inputs, not the target\n            if outpt.argmax(dim=1) == target : \n                inputs_tensor = torch.cat([inputs_tensor, inpt])\n                outputs_tensor = torch.cat([outputs_tensor, outpt])\n                current_samples += 1\n        \n        return inputs_tensor, outputs_tensor\n            \ninputs, outputs = select_samples(model, valid_loader, nb_samples, target)","aabef21d":"#Print the confidence across samples\nimport matplotlib.pyplot as plt\nfrom torch.nn.functional import softmax\n\nconfidence = softmax(outputs).T\nconfigs = confidence[0]\nN = len(configs)\nind = np.arange(N)\n\nwidth = 0.4\n\np1 = plt.bar(ind, confidence[0], width, color='g')\np2 = plt.bar(ind, confidence[1], width, bottom=confidence[0], color='r')\n\nplt.ylim([0,1.2])\nplt.ylabel('Probability', fontsize=12)\nplt.xlabel('Samples', fontsize=12)\nplt.legend((p1[0], p2[0]), ('Accepted', 'Refused'), fontsize=12, ncol=2, framealpha=0, fancybox=True)\nplt.show()","dbd4fd1e":"#Take a look at the microvariations for a client to change its class\n\n#Improvment : Stop the gradient descent on incompatible direction \/ focus only on specific axis (as salary, status, not job...)\n\nmodel.eval()\n\nfor i in range(nb_samples) : \n    shift_input = inputs[i].reshape(1,-1)\n    shift_input.requires_grad_(True)\n    outpt = model(shift_input)\n\n    while outpt.argmax(dim=1) == target : \n        loss = abs(outpt[0,0]-(target)) + abs(outpt[0,1]-(1-target)) #We look how to modify the inputs to go in the other class\n        loss.backward()\n        grad = shift_input.grad.detach().clone()\n        shift_input.requires_grad_(False)\n        shift_input = shift_input.detach().clone() - 0.01*grad\n        shift_input.requires_grad_(True)\n        outpt = model(shift_input)\n    \n    shift = shift_input - inputs[i].reshape(1,-1)\n    \n    \n    \n    print(f'\\nThe status of the Client {i} would have changed if the following {2*nb_features} features were modified')\n    disp_tab = top_features(shift.reshape(-1), normal=True)\n    disp_tab.append(['...','...'])\n    disp_tab += last_features(shift.reshape(-1), normal=True)\n    print(tabulate(disp_tab[::-1], headers=['Features','Shift']))","9615dd7b":"#Do a KNN on the client properties (more adapted to this specific problem)","6d5c3d96":"# Data loading","7200e457":"### Focusing on specific candidates","a22e63a8":"# Explicability","121f4eb8":"## Definition of targets","783499ee":"# Deep Learning","4eccecd6":"## Features engineering","88bac947":"Possibilities : \n1. Past due more than X days\n2. Past more than Y% of dues","24caf739":"# Definition of \"Bad\" client\n\nDetailed explanation could be seen [here](https:\/\/www.listendata.com\/2019\/09\/credit-risk-vintage-analysis.html). \n\n\n_This part was adapted from a notebook of @Xiao Song_","917dc0df":"### General analysis"}}