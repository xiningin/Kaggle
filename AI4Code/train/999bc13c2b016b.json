{"cell_type":{"092d0210":"code","79b4866e":"code","d6b48c1b":"code","953b8288":"code","490d2abf":"code","6c91f5bb":"code","116ba905":"code","37d04357":"code","b13aeded":"code","8a78fbff":"code","104543db":"code","01dd2a94":"code","56c1a170":"code","4985f5a7":"code","d84c46e2":"code","88dc4e23":"code","99a4ecca":"code","b8b69bae":"code","d5a62f27":"code","7d0b9fcb":"code","f42ebb47":"code","fd8c8e04":"code","18f27324":"code","4167772c":"code","c3b48505":"code","ce5767d0":"code","1d25347e":"code","ff96492b":"code","2ebde7f8":"code","a66a791d":"markdown","6db2b18a":"markdown","855f35b2":"markdown","2fcc54f4":"markdown","a21a8447":"markdown"},"source":{"092d0210":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nimport warnings\n","79b4866e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d6b48c1b":"df_train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n","953b8288":"\n\n\n#**1-Identification of variables and data types\n\n\nX_train = df_train.shape[0]\nX_test = df_test.shape[0]\ny_train = df_train.SalePrice.values\nAlldata = pd.concat((df_train, df_test)).reset_index(drop=True)\nAlldata.drop(['SalePrice'], axis=1, inplace=True)\nprint(Alldata.shape)","490d2abf":"#Save the 'Id' column\nX_train_ID = df_train['Id']\nX_test_ID = df_test['Id']\n","6c91f5bb":"#9-Correlation Analysis\ncorrmat=df_train.corr()\nplt.show(sns.heatmap(corrmat,vmax=.8,square=True))","116ba905":"k = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice').index\ncm=np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nplt.show(sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values))\n\n\n","37d04357":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nplt.show(sns.pairplot(df_train[cols],size=1.5))\n","b13aeded":"#Graphical Univariate Analysis*\nvar='OverallQual'\ndata=pd.concat([df_train['SalePrice'],df_train[var]],axis=1)\nf,ax=plt.subplots(figsize=(8,6))\nplt.show(sns.boxplot(x=var,y=\"SalePrice\",data=data))","8a78fbff":"var='GrLivArea'\ndata=pd.concat([df_train['SalePrice'],df_train[var]],axis=1)\nplt.scatter(x=df_train[var],y=df_train['SalePrice'])\nplt.xlabel(var,fontsize=20)\nplt.ylabel('SalePrice',fontsize=20)\n#plt.show()","104543db":"#Missing value treatment\n\nAlldata_na = (Alldata.isnull().sum() \/ len(Alldata)) * 100\nAlldata_na = Alldata_na.drop(Alldata_na[Alldata_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :Alldata_na})\nprint(missing_data.head(20))","01dd2a94":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=Alldata_na.index, y=Alldata_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","56c1a170":"##Data Description says NA which means No Pool, No Misc Feature etc etc etc.\n\nAlldata[\"PoolQC\"] = Alldata[\"PoolQC\"].fillna(\"None\")\nAlldata[\"MiscFeature\"] = Alldata[\"MiscFeature\"].fillna(\"None\")\nAlldata[\"MiscFeature\"] = Alldata[\"MiscFeature\"].fillna(\"None\")\nAlldata[\"Alley\"] = Alldata[\"Alley\"].fillna(\"None\")\nAlldata[\"Fence\"] = Alldata[\"Fence\"].fillna(\"None\")\nAlldata[\"FireplaceQu\"] = Alldata[\"FireplaceQu\"].fillna(\"None\")","4985f5a7":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nAlldata[\"LotFrontage\"] = Alldata.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n","d84c46e2":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    Alldata[col] = Alldata[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    Alldata[col] = Alldata[col].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    Alldata[col] = Alldata[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    Alldata[col] = Alldata[col].fillna('None')\n","88dc4e23":"Alldata[\"MasVnrType\"] = Alldata[\"MasVnrType\"].fillna(\"None\")\nAlldata[\"MasVnrArea\"] = Alldata[\"MasVnrArea\"].fillna(0)\nAlldata['MSZoning'] = Alldata['MSZoning'].fillna(Alldata['MSZoning'].mode()[0])\nAlldata = Alldata.drop(columns=['Utilities'], axis=1)\nAlldata[\"Functional\"] = Alldata[\"Functional\"].fillna(\"Typ\")\nAlldata['Electrical'] = Alldata['Electrical'].fillna(Alldata['Electrical'].mode()[0])\nAlldata['KitchenQual'] = Alldata['KitchenQual'].fillna(Alldata['KitchenQual'].mode()[0])\nAlldata['Exterior1st'] = Alldata['Exterior1st'].fillna(Alldata['Exterior1st'].mode()[0])\nAlldata['Exterior2nd'] = Alldata['Exterior2nd'].fillna(Alldata['Exterior2nd'].mode()[0])\nAlldata['SaleType'] = Alldata['SaleType'].fillna(Alldata['SaleType'].mode()[0])\nAlldata['MSSubClass'] = Alldata['MSSubClass'].fillna(\"None\")\nAlldata['MSSubClass'] = Alldata['MSSubClass'].fillna(\"None\")\n","99a4ecca":"#Check remaining missing values if any\nAlldata_na = (Alldata.isnull().sum() \/ len(Alldata)) * 100\nAlldata_na = Alldata_na.drop(Alldata_na[Alldata_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :Alldata_na})\nmissing_data.head()","b8b69bae":"#transformin some numerical value that are categoricaL\n#MSSubClass=The building class\nAlldata['MSSubClass'] = Alldata['MSSubClass'].apply(str)\nprint(Alldata['MSSubClass'])","d5a62f27":"#Changing OverallCond into a categorical variable\nAlldata['OverallCond'] = Alldata['OverallCond'].astype(str)","7d0b9fcb":"#Year and month sold are transformed into categorical features.\nAlldata['YrSold'] = Alldata['YrSold'].astype(str)\nAlldata['MoSold'] = Alldata['MoSold'].astype(str)","f42ebb47":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',\n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',\n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond',\n        'YrSold', 'MoSold')","fd8c8e04":"# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(Alldata[c].values))\n    Alldata[c] = lbl.transform(list(Alldata[c].values))\n\n# shape\nprint('Shape Alldata: {}'.format(Alldata.shape))","18f27324":"# Adding total sqfootage feature\nAlldata['TotalSF'] = Alldata['TotalBsmtSF'] + Alldata['1stFlrSF'] + Alldata['2ndFlrSF']","4167772c":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nnumeric_feats = Alldata.dtypes[Alldata.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = Alldata[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n\n\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))","c3b48505":"from scipy.special import boxcox1p\n\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    # all_data[feat] += 1\n    Alldata[feat] = boxcox1p(Alldata[feat], lam)\n","ce5767d0":"Alldata = pd.get_dummies(Alldata)\nprint(Alldata.shape)","1d25347e":"train_data=Alldata[:X_train]\ntest_data=Alldata[X_train:]","ff96492b":"from sklearn.linear_model import LinearRegression\nHousepriceNLR= LinearRegression()\nHousepriceNLR.fit(train_data, y_train)\n\ny_predNLR = HousepriceNLR.predict(test_data)\n\nprint(HousepriceNLR.score(train_data, y_train))\nprint(HousepriceNLR.score(test_data, y_predNLR))","2ebde7f8":"sub = pd.DataFrame()\nsub['Id'] = X_test_ID\nsub['SalePrice'] = y_predNLR\nprint(sub)\nsub.to_csv('submission.csv',index=False)\n","a66a791d":"predective models\n\nwe have more and more way to make prediction from sklearn library\n\ni choose first one linearregression\n\nand i will make another kernel compare betwwen oters ways for prediction and comparing the scores of each one","6db2b18a":"Variable identification:\nThe very first step in exploratory data analysis is to identify the type of variables in the dataset. Variables are of two types \u2014 Numerical and Categorical. They can be further classified as follows:\n\nClassification of Variables\nOnce the type of variables is identified, the next step is to identify the Predictor (Inputs) and Target (output) variables.\n\n\n\n","855f35b2":"It is always better to explore each data set using multiple exploratory techniques and compare the results. The goal of this step is to understand the dataset, identify the missing values & outliers if any using visual and quantitative methods to get a sense of the story it tells. It suggests the next logical steps, questions or areas of research for your project.\nSteps in Data Exploration and Preprocessing:\n\n\n**1-Identification of variables and data types\n\n**2-Analyzing the basic metrics\n\n**3-Non-Graphical Univariate Analysis\n\n**4-Graphical Univariate Analysis**\n\n**5-Bivariate Analysis**\n\n**6-Variable transformations**\n\n**7-Missing value treatment\n**\n**8-Outlier treatment**\n\n**9-Correlation Analysis\n**\n**10-Dimensionality Reduction**","2fcc54f4":"Graphical Univariate Analysis:\nHistogram:\n\n\nHistograms are one of the most common graphs used to display numeric data. \n\nHistograms two important things we can learn from a histogram:distribution of the data \u2014 Whether the data is normally distributed or if it\u2019s skewed (to the left or right)\n\nTo identify outliers \u2014 Extremely low or high values that do not fall near any other data points.\n\nLets plot histogram for the \u2018ltv\u2019 feature in our dataset","a21a8447":"AUTOR:M.Gadallah\nEmail:m7mdgadallah86@gmail.com\n\ni try to learn machinelearning in 2018 and have a diploma in machinelearning with python....\n"}}