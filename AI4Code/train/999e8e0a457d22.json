{"cell_type":{"b24c7160":"code","483446ff":"code","acc86976":"code","c4f8b4f7":"code","368490e9":"code","51f8932c":"code","5dfa9a71":"code","d8faaa4f":"code","b38fb797":"code","4241b678":"code","14a7b3f3":"code","d13969da":"code","df991f40":"code","3254d82b":"code","d816c943":"code","352e2220":"code","250a12e3":"code","df12ffec":"code","9740bcfa":"code","f0ed6dcd":"code","f1a9516c":"code","3d48c256":"code","ff920c7d":"code","51f5c6e3":"code","a5542005":"code","9d6b8fd3":"code","6b54516a":"code","511548f7":"code","7a19f596":"code","1d5fe693":"code","ddf2acb0":"markdown","589c539b":"markdown","b8da72e9":"markdown","071f8df2":"markdown","53dccaf9":"markdown","ab1bacbf":"markdown","34e83789":"markdown","e3ac7779":"markdown","a7f0fc71":"markdown","98ba2cd6":"markdown","9590daf1":"markdown","2ef7b4f7":"markdown","2a6bf2ad":"markdown","de676ed2":"markdown","bed52865":"markdown","d9f16dfd":"markdown","801e49db":"markdown","bf5369b3":"markdown"},"source":{"b24c7160":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom scipy.stats import pearsonr\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# visualization and plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","483446ff":"df = pd.read_csv('..\/input\/indian_liver_patient.csv')\ndf.shape","acc86976":"#show the first five rows of data\ndf.head()","c4f8b4f7":"df.info(verbose=True)","368490e9":"df.describe()","51f8932c":"# let's look on target variable - classes imbalanced?\ndf.rename(columns={'Dataset':'target'},inplace=True)\ndf.head()","5dfa9a71":"# let's look on target variable - classes imbalanced?\ndf['target'].value_counts()","d8faaa4f":"print(df.isnull().sum())","b38fb797":"p = df.hist(figsize = (20,20))","4241b678":"df.Albumin_and_Globulin_Ratio.fillna(df['Albumin_and_Globulin_Ratio'].median(), inplace=True)","14a7b3f3":"le = preprocessing.LabelEncoder()\nle.fit(df.Gender.unique())\ndf['Gender_Encoded'] = le.transform(df.Gender)\ndf.drop(['Gender'], axis=1, inplace=True)","d13969da":"## checking the balance of the data by plotting the count of outcomes by their value\ncolor_wheel = {1: \"#0392cf\", \n               2: \"#7bc043\"}\ncolors = df[\"target\"].map(lambda x: color_wheel.get(x + 1))\nprint(df.target.value_counts())\np=df.target.value_counts().plot(kind=\"bar\")","df991f40":"p=sns.pairplot(df, hue = 'target')","3254d82b":"plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(df.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","d816c943":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(df.drop([\"target\"],axis = 1),),\n        columns=['Age', 'Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase',\n        'Alamine_Aminotransferase', 'Aspartate_Aminotransferase', 'Total_Protiens', \n        'Albumin', 'Albumin_and_Globulin_Ratio','Gender_Encoded'])","352e2220":"X.head()","250a12e3":"y = df.target","df12ffec":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42, stratify=y)","9740bcfa":"test_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","f0ed6dcd":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","f1a9516c":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","3d48c256":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')","ff920c7d":"knn = KNeighborsClassifier(12)\n\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","51f5c6e3":"#import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n#let us get the predictions using the classifier we had fit above\ny_pred = knn.predict(X_test)\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","a5542005":"y_pred = knn.predict(X_test)\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","9d6b8fd3":"#import classification_report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","6b54516a":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","511548f7":"best_clf = KNeighborsClassifier(34)\nbest_clf.fit(X_train, y_train)\nbest_clf.score(X_test,y_test)","7a19f596":"from yellowbrick.classifier import ROCAUC","1d5fe693":"fig, ax=plt.subplots(1,1,figsize=(12,8))\nroc_auc=ROCAUC(best_clf, ax=ax)\nroc_auc.fit(X_train, y_train)\nroc_auc.score(X_test, y_test)\n\nroc_auc.poof()","ddf2acb0":"The best result is captured at k = 12 hence 12 is used for the final model","589c539b":"<font size = '4'> Model Performance Analysis<\/font size>","b8da72e9":"#Basic Data Science and ML Pipeline\n\nOSEMN Pipeline\n\nO - Obtaining our data\n\nS - Scrubbing \/ Cleaning our data\n\nE - Exploring \/ Visualizing our data will allow us to find patterns and trends\n\nM - Modeling our data will give us our predictive power as a wizard\n\nN - INterpreting our data\n\nFor reference : https:\/\/www.linkedin.com\/pulse\/life-data-science-osemn-randy-lao\/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_post_details%3BmDlg5VsdSBCLBps2R0vRZA%3D%3D","071f8df2":"Test Train Split and Cross Validation methods\nTrain Test Split : To have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the model performance much better.\n![](https:\/\/imgur.com\/v1kaiTB)\n\nCross Validation: When model is split into training and testing it can be possible that specific type of data point may go entirely into either training or testing portion. This would lead the model to perform poorly. Hence over-fitting and underfitting problems can be well avoided with cross validation techniques\n\n![](https:\/\/imgur.com\/fbiqt9J)\n\nAbout Stratify : Stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n\nFor example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.\n\nFor Reference : https:\/\/towardsdatascience.com\/train-test-split-and-cross-validation-in-python-80b61beca4b6\n","53dccaf9":"The above graph shows that the data is biased towards datapoints having outcome value as 0 where it means that diabetes was not present actually. The number of non-diabetics is almost twice the number of diabetic patients","ab1bacbf":"Hyper Parameter optimization\nGrid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.\n\nLet\u2019s consider the following example:\n\nSuppose, a machine learning model X takes hyperparameters a1, a2 and a3. In grid searching, you first define the range of values for each of the hyperparameters a1, a2 and a3. You can think of this as an array of values for each of the hyperparameters. Now the grid search technique will construct many versions of X with all the possible combinations of hyperparameter (a1, a2 and a3) values that you defined in the first place. This range of hyperparameter values is referred to as the grid.\n\nSuppose, you defined the grid as: a1 = [0,1,2,3,4,5] a2 = [10,20,30,40,5,60] a3 = [105,105,110,115,120,125]\n\nNote that, the array of values of that you are defining for the hyperparameters has to be legitimate in a sense that you cannot supply Floating type values to the array if the hyperparameter only takes Integer values.\n\nNow, grid search will begin its process of constructing several versions of X with the grid that you just defined.\n\nIt will start with the combination of [0,10,105], and it will end with [5,60,125]. It will go through all the intermediate combinations between these two which makes grid search computationally very expensive.","34e83789":"Scaling the data\ndata Z is rescaled such that \u03bc = 0 and \ud835\uded4 = 1, and is done through this formula:\n![](https:\/\/imgur.com\/mJSnQvv)\n\nto learn more about scaling techniques\nhttps:\/\/medium.com\/@rrfd\/standardize-or-normalize-examples-in-python-e3f174b65dfc https:\/\/machinelearningmastery.com\/rescaling-data-for-machine-learning-in-python-with-scikit-learn\/","e3ac7779":"#Basic EDA and statistical analysis","a7f0fc71":"Skewness\nA left-skewed distribution has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That\u2019s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA right-skewed distribution has a long right tail. Right-skewed distributions are also called positive-skew distributions. That\u2019s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\n![](https:\/\/imgur.com\/a\/DwhqIDR)\nto learn more about skewness\nhttps:\/\/www.statisticshowto.datasciencecentral.com\/probability-and-statistics\/skewed-distribution\/","98ba2cd6":"#Result Visualisation","9590daf1":"2. Classification Report\nReport which includes Precision, Recall and F1-Score.\n\nPrecision Score\n    TP \u2013 True Positives\n    FP \u2013 False Positives\n\n    Precision \u2013 Accuracy of positive predictions.\n    Precision = TP\/(TP + FP)\n\n\nRecall Score\n    FN \u2013 False Negatives\n\n    Recall(sensitivity or true positive rate): Fraction of positives that were correctly identified.\n    Recall = TP\/(TP+FN)\n\nF1 Score\n    F1 Score (aka F-Score or F-Measure) \u2013 A helpful metric for comparing two classifiers.\n    F1 Score takes into account precision and the recall. \n    It is created by finding the the harmonic mean of precision and recall.\n\n    F1 = 2 x (precision x recall)\/(precision + recall)\n\n\nFor Reference: http:\/\/joshlawman.com\/metrics-classification-report-breakdown-precision-recall-f1\/ : https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/","2ef7b4f7":"for  categorical data two encoding are basically used label encoding and one-hot encoding...\nfor reference: https:\/\/medium.com\/@contactsunny\/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621","2a6bf2ad":"3. ROC - AUC\nROC (Receiver Operating Characteristic) Curve tells us about how good the model can distinguish between two things (e.g If a patient has a disease or no). Better models can accurately distinguish between the two. Whereas, a poor model will have difficulties in distinguishing between the two\n\nWell Explained in this video: https:\/\/www.youtube.com\/watch?v=OAl6eAyP-yo","de676ed2":"Pearson's Correlation Coefficient: helps you find out the relationship between two quantities. It gives you the measure of the strength of association between two variables. The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.\n\nA heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information.","bed52865":"DataFrame.describe() method generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values. This method tells us a lot of things about a dataset. One important thing is that the describe() method deals only with numeric values. It doesn't work with any categorical values. So if there are any categorical values in a column the describe() method will ignore it and display summary for the other columns unless parameter include=\"all\" is passed.\n\nNow, let's understand the statistics that are generated by the describe() method:\n\ncount tells us the number of NoN-empty rows in a feature.\nmean tells us the mean value of that feature.\nstd tells us the Standard Deviation Value of that feature.\nmin tells us the minimum value of that feature.\n25%, 50%, and 75% are the percentile\/quartile of each features. This quartile information helps us to detect Outliers.\nmax tells us the maximum value of that feature.","d9f16dfd":"1. Confusion Matrix\nThe confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs.\n![](https:\/\/imgur.com\/5PXVj7e)\n\nn the famous cancer example:\nCases in which the doctor predicted YES (they have the disease), and they do have the disease will be termed as TRUE POSITIVES (TP). The doctor has correctly predicted that the patient has the disease.\nCases in which the doctor predicted NO (they do not have the disease), and they don\u2019t have the disease will be termed as TRUE NEGATIVES (TN). The doctor has correctly predicted that the patient does not have the disease.\nCases in which the doctor predicted YES, and they do not have the disease will be termed as FALSE POSITIVES (FP). Also known as \u201cType I error\u201d.\nCases in which the doctor predicted NO, and they have the disease will be termed as FALSE NEGATIVES (FN). Also known as \u201cType II error\u201d.\n\n![](https:\/\/imgur.com\/6TOwzP4)\n\nFor Reference: https:\/\/medium.com\/@djocz\/confusion-matrix-aint-that-confusing-d29e18403327\n","801e49db":"The pairs plot builds on two basic figures, the histogram and the scatter plot. The histogram on the diagonal allows us to see the distribution of a single variable while the scatter plots on the upper and lower triangles show the relationship (or lack thereof) between two variables.\nFor Reference: https:\/\/towardsdatascience.com\/visualizing-data-with-pair-plots-in-python-f228cf529166","bf5369b3":"For  a large number or Nan values,\nTo fill these Nan values the data distribution needs to be understood, in this case only four Nan values in Albumin_and_Globin_Ratio column, we can easily fill with mean,mode,median or ?"}}