{"cell_type":{"94556c1a":"code","38a68ff1":"code","13297f34":"code","debba948":"code","050ea0f1":"code","0f5522da":"code","6a3ff01a":"code","45153973":"code","740ff5b9":"code","fdb5957c":"code","8c61129b":"code","ab596a98":"code","34de85f0":"code","8680a200":"code","dd7ef142":"code","6e2ae49a":"code","bf0b7d14":"code","5420a98e":"code","24330b6f":"code","50a19861":"markdown","c229970a":"markdown","f5309517":"markdown","11639c66":"markdown","4fcf39c9":"markdown","9f049e09":"markdown","ddc6380e":"markdown","f62f8e18":"markdown","e462294b":"markdown","1f6093d3":"markdown","41f6a6a3":"markdown","dd6c4c48":"markdown","5f585927":"markdown","ae6b4683":"markdown","84eaf3ec":"markdown","81b6ad3f":"markdown","e5044f25":"markdown"},"source":{"94556c1a":"import os\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAvgPool2D, Dense, Dropout, Flatten, experimental\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping","38a68ff1":"WEIGHTS_PATH = 'https:\/\/github.com\/GKalliatakis\/Keras-VGG16-places365\/releases\/download\/v1.0\/vgg16-places365_weights_tf_dim_ordering_tf_kernels.h5'\nWEIGHTS_PATH_NO_TOP = 'https:\/\/github.com\/GKalliatakis\/Keras-VGG16-places365\/releases\/download\/v1.0\/vgg16-places365_weights_tf_dim_ordering_tf_kernels_notop.h5'","13297f34":"TRAIN_DATA_DIR = '\/kaggle\/input\/personalplaces3\/train'\nTEST_DATA_DIR = '\/kaggle\/input\/personalplaces3\/test'\nIMAGE_SIZE = (192, 192)\nINPUT_SHAPE = (192, 192, 3)\nBATCH_SIZE = 128\nNUMBER_CLASSES = 3\nEPOCHS = 30","debba948":"# Initialize test and training data generator\ntrain_datagen = ImageDataGenerator(rescale=1.0\/255,\n                                   validation_split=0.2,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest',\n                                   zoom_range=0.3,\n                                   width_shift_range=0.3,\n                                   height_shift_range=0.3,\n                                   rotation_range=30\n                                   )\n\ntest_datagen = ImageDataGenerator(rescale=1.0\/255,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest',\n                                  zoom_range=0.3,\n                                  width_shift_range=0.3,\n                                  height_shift_range=0.3,\n                                  rotation_range=30\n                                  )","050ea0f1":"# Create train generator\ntrain_gen = train_datagen.flow_from_directory(directory=TRAIN_DATA_DIR,\n                                              target_size=IMAGE_SIZE,\n                                              color_mode='rgb',\n                                              class_mode='categorical',\n                                              batch_size=BATCH_SIZE,\n                                              shuffle=True,\n                                              subset='training')\n\n# Create validation generator\nvalidation_gen = train_datagen.flow_from_directory(directory=TRAIN_DATA_DIR,\n                                                   target_size=IMAGE_SIZE,\n                                                   color_mode='rgb',\n                                                   class_mode='categorical',\n                                                   batch_size=BATCH_SIZE,\n                                                   shuffle=True,\n                                                   subset='validation')\n\n# Create test generator\ntest_gen = test_datagen.flow_from_directory(directory=TEST_DATA_DIR,\n                                                   target_size=IMAGE_SIZE,\n                                                   color_mode='rgb',\n                                                   class_mode='categorical',\n                                                   batch_size=BATCH_SIZE,\n                                                   shuffle=True)","0f5522da":"classes_dict = train_gen.class_indices\nprint('Class and corresponding index:', classes_dict)","6a3ff01a":"\nclass_names = list(classes_dict.keys())\n\nfig, axes = plt.subplots(3,3, figsize=(12, 10))\nindices = np.random.randint(0, 624, size=9)\nfor i, ax in enumerate(axes.flatten()):\n    indx = indices[i]\n    im = Image.open(train_gen.filepaths[indx])\n    ax.imshow(im)\n    ax.set_title(class_names[train_gen.classes[indx]])\n    ax.axis('off')","45153973":"def vgg16(weights_path, top=True):\n\n    model = tf.keras.models.Sequential()\n\n    # Block 1\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)))\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n    # Block 2\n    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n    model.add( Conv2D(128, (3, 3), activation='relu', padding='same'))\n    model.add( MaxPooling2D((2, 2), strides=(2, 2)))\n\n    # Block 3\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add( Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add( Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add( MaxPooling2D((2, 2), strides=(2, 2)))\n\n    # Block 4\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add( Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n    # Block 5\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n    if top:\n        # Classification layer\n        model.add(layers.Flatten())\n        model.add(layers.Dense(4096, activation='relu'))\n        model.add(layers.Dropout(0.5))\n        model.add(layers.Dense(4096, activation='relu'))\n        model.add(layers.Dropout(0.5))\n        model.add(layers.Dense(365, activation='softmax'))\n        \n    else:\n        model.add( GlobalAvgPool2D())\n    \n    model.load_weights(filepath=weights_path)\n\n    sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)\n    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n\n    return model","740ff5b9":"tf.keras.backend.set_image_data_format('channels_last')","fdb5957c":"# Get the weights\nweights_file = 'vgg16-places365_weights_tf_dim_ordering_tf_kernels_notop.h5'\nweights_path = tf.keras.utils.get_file(weights_file, WEIGHTS_PATH_NO_TOP)","8c61129b":"base_model = vgg16(weights_path, top=False)\n\nbase_model.summary()","ab596a98":"base_model.trainable = False\nbase_model.summary()","34de85f0":"model = tf.keras.models.Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(NUMBER_CLASSES, activation='softmax'))\n","8680a200":"model.summary()","dd7ef142":"model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])","6e2ae49a":"# Save our model using specified conditions\ncheckpoint = ModelCheckpoint(\"checkpoints\/vgg16_12.h5\", monitor='accuracy', verbose=1, save_best_only=True )\nearly = EarlyStopping(monitor='accuracy', min_delta=0, patience=10, verbose=1, mode='auto')","bf0b7d14":"# Start training\nhistory = model.fit(train_gen,\n                    batch_size = BATCH_SIZE,\n                    epochs=EPOCHS,\n                    callbacks=[checkpoint, early],\n                    validation_data=validation_gen,\n                    shuffle=True,\n                    workers=16,\n                    use_multiprocessing=True\n                   )","5420a98e":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","24330b6f":"results = model.evaluate(test_gen)\nprint(f\"test loss {results[0]}, test accuracy {results[1]}\")","50a19861":"We can see that the number of trainable parameters are zero now.","c229970a":"#### Visualize training data\nLet's visulaize few training images by ploting ","f5309517":"Parameters and variables that will be needed","11639c66":"### 6. Evaluate the model\n\nEvaluate the model through the testing data.","4fcf39c9":"#### Parameters and Variables\nWeight file for pre-trained `VGG16` modle trained on `Places365` dataset are picked [from this git repo](https:\/\/github.com\/GKalliatakis\/Keras-VGG16-places365\/blob\/master\/vgg16_places_365.py).","9f049e09":"#### Freeze the VGG16_Places365 Pretrained model and to use as a feature extractor. \n- Freezing prevents the weights in a given layer from being updated during training.\n\n> Freezeing by setting `trainable=False`","ddc6380e":"#### Import required packages","f62f8e18":"### 4. Compile the model","e462294b":"#### Visualize learning curve\nVisualize the learning curve by plotting accuracy and loss of train and validation\n","1f6093d3":"Dispaly class names and corresponding integer representations","41f6a6a3":"### 2. Build model and load pre-trained weights\nBuilding the VGG16 model","dd6c4c48":">Load places365 pre-trained weight for `VGG16` model without classification layer","5f585927":"<p>We are going to classify our custom places image. We will used pretrained weights of VGG16 model trained with places365 dataset. Classification of data will be done through transfer learning and fine-tunning.<p>\n\n# Transfer learning of VGG16 on palces365 dataset in TensorFlow 2\n\nWe can use a pretrained model to classify new data as it is or use **transfer learning** to customize the model for a given task.\n\n### How we will do?\nTwo approach to customize a pre-trained model\n- **Feature extraction:** use the feature extractor part of the model to extract new feature for new data and train the classification layer.\n- **Fine tunning:** use few layers of the feature extractor part of the model to learn new feature and train both feature extractor part and classification part.\n\n### What we will do?\n\nWe will classify our new data by customizing the pre-trained **VGG16** model on **Places365** dataset through  **transfer learning**.\n\n### Steps\n1. Prepare data\n2. Build model and load pre-trained weights \n3. Add new classification layer\n4. Comile the model\n5. Train the model\n6. Evaluate the model","ae6b4683":"### 1. Prepare Data\n#### Image Preprocessing: Load and preprocessing images with `ImageDataGenerator` of keras.","84eaf3ec":"### 3. Add new classification layer\nWe will add layers to base model for classifaction of our new data.","81b6ad3f":"#### Create VGG16 model without classification layer\n We will create VGG16 model without classification layer and provide the path of the weights to load the weight of the feature extration part on the model.\n \n >Set image data format as `channels_first` to keras backend since we will not use GPU for our training.","e5044f25":"### 5. Train the model"}}