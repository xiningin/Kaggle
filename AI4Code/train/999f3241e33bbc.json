{"cell_type":{"173b543a":"code","92daac64":"code","24a164d0":"code","205e0cc2":"code","a5c1b71d":"code","b2385aba":"code","3879e4bb":"code","e120efe8":"code","436848ab":"code","6115bf9c":"code","87066aef":"code","6197171e":"code","c4c9fdb4":"code","c344c60a":"code","d8bc8a50":"code","3121384c":"code","6d1c5734":"code","02eaf758":"code","485184b5":"code","d29ed5d8":"code","db7d9f3d":"code","5ed0f340":"code","30f55000":"code","3b852045":"code","7b0a28d3":"code","4c0a6484":"code","06869f39":"code","5d416242":"code","a856b555":"code","67f7a8bd":"code","cff607ef":"code","a5663f2c":"code","98bc4f42":"code","44d75f45":"code","e6908b00":"code","d73ed0f6":"code","928e2280":"code","937bdc28":"code","4c879dc3":"code","a7697f86":"markdown","092e3e76":"markdown","7a1c6c73":"markdown","ce75089d":"markdown","8f933b53":"markdown","3a7fd79b":"markdown","ee03fef7":"markdown","e491617a":"markdown","a142ba07":"markdown","6b029234":"markdown","3e4a219e":"markdown","806237d0":"markdown","ccc2ef12":"markdown","82622c2c":"markdown","5fbf1e09":"markdown","41f3a018":"markdown","078fe4b3":"markdown"},"source":{"173b543a":"# Efficient Net \uc124\uce58\n!pip3 install keras_efficientnets","92daac64":"# Import\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport PIL\nimport os\nimport h5py\nimport datetime\nfrom pytz import timezone, utc\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import Xception\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\nfrom keras import layers, models, optimizers, utils\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings('ignore')\n\n# Efficient Net\nfrom keras_efficientnets import EfficientNetB3\nfrom keras_efficientnets import EfficientNetB4\nfrom keras_efficientnets import EfficientNetB5","24a164d0":"# Utility functions\n\n# commit \uba54\uc2dc\uc9c0\uc5d0 \ub85c\uadf8\ub97c \ub0a8\uae38\uc218 \uc788\uac8c \ud569\ub2c8\ub2e4\ndef print2(string):  \n    os.system(f'echo \\\"{string}\\\"')\n    print(string)\n\n# dataframe\uc744 \ud55c\uc904\uc5d0 \uc5ec\ub7ec\uac1c \ucd9c\ub825\ud558\uac8c \ud569\ub2c8\ub2e4.\nfrom IPython.display import display_html\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)","205e0cc2":"# cropped \ub370\uc774\ud130\uc14b \ud655\uc778\n!ls -l ..\/input\/3rd-ml-month-car-image-cropping-dataset","a5c1b71d":"IMG_PATH = '..\/input\/3rd-ml-month-car-image-cropping-dataset'\nTRAIN_IMG_PATH = os.path.join(IMG_PATH, 'train_crop')\nTEST_IMG_PATH = os.path.join(IMG_PATH, 'test_crop')\n\nDATA_PATH = '..\/input\/2019-3rd-ml-month-with-kakr'\ndf_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\ndf_class = pd.read_csv(os.path.join(DATA_PATH, 'class.csv'))\n\ndf_train['class'] = df_train['class'].astype('str')","b2385aba":"display_side_by_side(df_train.head(), df_test.head() )","3879e4bb":"#\ud559\uc2b5\ud560 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc815\ubcf4\nmodel_class = EfficientNetB3\nimage_size = 299\nbatch_size = 16\n#model_class = EfficientNetB4\n#image_size = 380\n#batch_size = 16\n#model_class = EfficientNetB5\n#image_size = 456\n#batch_size = 8\nRND_SEED = 1005\nbasename = 'b3d2ovs'\n\n#\uc2e4\uc81c \uc2e4\ud589\uc2dc\uc5d0\ub294 False\ub85c\nJUST_TEST = True","e120efe8":"#train_test_split\nfrom sklearn.model_selection import train_test_split\n\nvalidation_ratio = 0.3\nnb_train_data = df_train.shape[0]\nnb_test_data = df_test.shape[0]\n\nnb_validation_sample = nb_train_data * validation_ratio\nnb_train_sample = nb_train_data - nb_validation_sample\n\nX_train, X_valid, y_train, y_valid = \\\n    train_test_split(df_train, df_train['class'], test_size=validation_ratio, \\\n                     random_state=RND_SEED, \\\n                     stratify=df_train['class'])","436848ab":"min_sample = min(X_train.groupby('class')['img_file'].count())\nmax_sample = max(X_train.groupby('class')['img_file'].count())\nprint(min_sample,',',max_sample)\n\n## over sampling.\n\nmax_samples = max(X_train.groupby('class')['img_file'].count())\nXX_train = None\ndisplay(X_train.describe())\n#display(X_train['class'].unique())\nfor cls in X_train['class'].unique():\n    df = X_train[X_train['class']==cls]\n    df_size = df['img_file'].count()\n\n    #\ubd80\uc871\ud55c \uc815\uc218\ubc30\ub9cc\ud07c \ubcf5\uc81c\ud558\uc5ec \ucc44\uc6c0\n    for idx in range(max_samples \/\/ df_size):\n        if (XX_train is None):\n            XX_train = df\n        else:\n            XX_train = XX_train.append(df)\n    \n    #\ub098\uba38\uc9c0\ub294 \ube44\ubcf5\uc6d0 \ub79c\ub364\ucd94\ucd9c\n    if (max_samples % df_size > 0):\n        dfx = df.sample(n=max_samples % df_size, replace=False, random_state=int(cls)+1000)\n        if (XX_train is None):\n            XX_train = dfx\n        else:\n            XX_train = XX_train.append(dfx)\n\ndisplay(XX_train.describe())","6115bf9c":"min_sample = min(XX_train.groupby('class')['img_file'].count())\nmax_sample = max(XX_train.groupby('class')['img_file'].count())\nprint(min_sample, ',', max_sample)\n\nX_train = XX_train\ny_train = XX_train['class']\nprint(nb_train_sample)\nnb_train_sample = X_train.shape[0]\nprint(nb_train_sample)","87066aef":"# split count check\ndisplay_side_by_side(df_train[['img_file','class']].groupby('class').count().head(3), \\\n                    X_train[['img_file','class']].groupby('class').count().head(3), \\\n                    X_valid[['img_file','class']].groupby('class').count().head(3) )\n\n# split data check\ndisplay_side_by_side(df_train[['img_file','class']].groupby('class').max().head(3), \\\n                      X_train[['img_file','class']].groupby('class').max().head(3), \\\n                      X_valid[['img_file','class']].groupby('class').max().head(3) )","6197171e":"train_datagen = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=False,\n    zoom_range=0.1,\n    shear_range=0.2,\n    fill_mode='nearest'\n    )\nvalid_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()","c4c9fdb4":"# \ubd84\ud560\ub41c train_set \ud559\uc2b5\uc5d0 \uc4f0\ub294 generator\ntrain_generator = train_datagen.flow_from_dataframe( \n    dataframe=X_train, \n    directory=TRAIN_IMG_PATH, \n    x_col=\"img_file\", y_col=\"class\", \n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical',\n    seed=2019,\n    color_mode='rgb'\n)\n# \ubd84\ud560\ub41c validation_set \ud559\uc2b5\uc5d0 \uc4f0\ub294 generator\nvalidation_generator = train_datagen.flow_from_dataframe( \n    dataframe=X_valid, \n    directory=TRAIN_IMG_PATH, \n    x_col=\"img_file\", y_col=\"class\", \n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical',\n    seed=2019,\n    color_mode='rgb'\n)\n# test_data predict\uc5d0 \uc4f0\ub294 generator\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    directory=TEST_IMG_PATH,\n    x_col='img_file',\n    y_col=None,\n    target_size= (image_size,image_size),\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=batch_size,\n    shuffle=False\n)\n# train_data predict\uc5d0 \uc4f0\ub294 generator\nfulltrain_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_train,\n    directory=TRAIN_IMG_PATH,\n    x_col='img_file',\n    y_col=\"class\",\n    target_size=(image_size, image_size),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=batch_size,\n    shuffle=False\n)","c344c60a":"def f1_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\ndef get_model():\n    base_model = model_class(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n    # base_model.trainable = False\n\n    model = models.Sequential()\n    model.add(base_model)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dense(1024, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1024, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(196, activation='softmax'))\n    model.summary()\n\n    #optimizer = optimizers.SGD(lr=1e-4, decay=1e-6, momentum=0.9, nesterov=True)\n    #optimizer = optimizers.RMSprop(lr=0.0001)\n    optimizer = optimizers.Adamax()\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc', f1_m])\n\n    return model\n\nmodel = get_model()\nmodel_path = '.\/{}{}.hdf5'.format(basename, RND_SEED)","d8bc8a50":"def get_steps(num_samples, batch_size):\n    if (num_samples % batch_size) > 0:\n        return (num_samples \/\/ batch_size) + 1\n    else:\n        return num_samples \/\/ batch_size\n    \n# Train\uc911\uc5d0 \ub85c\uadf8\ub97c \ucd9c\ub825\ud558\ub294 CallBack\nKST = timezone('Asia\/Seoul')\nclass EpochLogWrite(Callback):\n    def on_train_begin(self, logs={}):\n        None\n    def on_batch_begin(self, batch, logs={}):\n        None\n    def on_epoch_begin(self, epoch, logs={}):\n        tmx = utc.localize(datetime.datetime.utcnow()).astimezone(KST).time()\n        print2('Epoch #{} begins at {}'.format(epoch+1, tmx))\n    def on_train_end(self, logs={}):\n        None\n    def on_batch_end(self, batch, logs={}):\n        None\n    def on_epoch_end(self, epoch, logs={}):\n        tmx = utc.localize(datetime.datetime.utcnow()).astimezone(KST).time()\n        print2('Epoch #{} ends at {}  acc={} val_acc={} val_f1={}'.format(epoch+1, tmx, round(logs['acc'],4), round(logs['val_acc'],4), round(logs['val_f1_m'],4) ))\n\npatient = 2\ncallbacks1 = [\n    EpochLogWrite(),\n    EarlyStopping(monitor='val_acc', patience=patient, mode='max', verbose=1),\n    ReduceLROnPlateau(monitor = 'val_acc', factor = 0.5, patience = patient \/ 2, min_lr=0.00001, verbose=1, mode='max'),\n    ModelCheckpoint(filepath=model_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max'),\n    ]\n\ncallbacks2 = [\n    EpochLogWrite(),\n    EarlyStopping(monitor='val_f1_m', patience=patient, mode='max', verbose=1),\n    ReduceLROnPlateau(monitor = 'val_f1_m', factor = 0.5, patience = patient \/ 2, min_lr=0.00001, verbose=1, mode='max'),\n    ModelCheckpoint(filepath=model_path, monitor='val_f1_m', verbose=1, save_best_only=True, mode='max'),\n    ]","3121384c":"if JUST_TEST==True:\n    history = model.fit_generator(\n        train_generator,\n        steps_per_epoch=10, #get_steps(nb_train_sample, batch_size),\n        epochs=3,\n        validation_data=validation_generator,\n        validation_steps=10, #get_steps(nb_validation_sample, batch_size),\n        verbose=1,\n        callbacks = callbacks1\n    )    \nelse:\n    history = model.fit_generator(\n        train_generator,\n        steps_per_epoch=get_steps(nb_train_sample, batch_size),\n        epochs=100,\n        validation_data=validation_generator,\n        validation_steps=get_steps(nb_validation_sample, batch_size),\n        verbose=1,\n        callbacks = callbacks1\n    )\n","6d1c5734":"print('val_acc=', np.round(history.history['val_acc'][-1], 4))\nprint('val_f1_m=', np.round(history.history['val_f1_m'][-1], 4))\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training Acc')\nplt.plot(epochs, val_acc, 'b', label='Validation Acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, 'bo', label='Traing loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Trainging and validation loss')\nplt.legend()\nplt.show()","02eaf758":"#\ud559\uc2b5\ud55c \ubaa8\ub378\ub85c test_data\ub97c prediction\ud558\uace0 \uc2f1\uae00\ubaa8\ub378 \uc81c\ucd9c\ud30c\uc77c\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. \nmodel.load_weights(model_path)\ntest_generator.reset()\n\nprediction = model.predict_generator(\n    generator=test_generator,\n    steps = get_steps(nb_test_data, batch_size),\n    verbose=1\n)\n\npredicted_class_indices=np.argmax(prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nsub0=pd.DataFrame({'img_file': df_test['img_file'], 'class':predictions})\nif JUST_TEST==False:\n    sub0.to_csv('.\/single_submit_{}{}.csv'.format(basename, RND_SEED), index=False)\n\nsub0.head()","485184b5":"#\ub098\uc911\uc5d0 \uc559\uc0c1\ube14\uc5d0 \uc4f0\uae30 \uc704\ud574\uc11c \ud559\uc2b5\ud55c \ubaa8\ub378\ub85c train_data\uc804\uccb4\ub97c prediction\ud569\ub2c8\ub2e4. \nfulltrain_prediction = model.predict_generator(\n    generator=fulltrain_generator,\n    steps = get_steps(nb_train_data, batch_size),\n    verbose=1\n)\n\npredicted_class_indices=np.argmax(fulltrain_prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (fulltrain_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nmidout=pd.DataFrame({'img_file': df_train.img_file, 'class':df_train['class'], 'predicted':predictions})\nmidout.to_csv('.\/fulltrain_{}{}.csv'.format(basename, RND_SEED), index=False)\n\nmidout.head()","d29ed5d8":"#prediction \uacb0\uacfc\ub97c h5\ud30c\uc77c\uc5d0 \uc800\uc7a5\ud569\ub2c8\ub2e4.\nhf = h5py.File('.\/{}{}_data.h5'.format(basename,RND_SEED), 'w')\nhf.create_dataset('full_pred', data=fulltrain_prediction)\nhf.create_dataset('test_pred0', data=prediction)\nhf.close()","db7d9f3d":"WEIGHT_PATH = '..\/input\/3rd-ml-month-efficientnet-weights'\n\n# weight path \ud655\uc778\n!ls -l ..\/input\/3rd-ml-month-efficientnet-weights | head -6","5ed0f340":"aug_datagen = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=False,\n    zoom_range=0.1,\n    shear_range=0.2,\n    fill_mode='nearest'\n    )\norg_datagen = ImageDataGenerator()\n\narr_test_generator = []\ntest_generator = org_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    directory=TEST_IMG_PATH,\n    x_col='img_file',\n    y_col=None,\n    target_size= (image_size,image_size),\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=batch_size,\n    shuffle=False\n)\narr_test_generator.append(test_generator)\n\nfor idx in range(9):\n  test_generator2 = aug_datagen.flow_from_dataframe(\n      dataframe=df_test,\n      directory=TEST_IMG_PATH,\n      x_col='img_file',\n      y_col=None,\n      target_size= (image_size,image_size),\n      color_mode='rgb',\n      class_mode=None,\n      batch_size=batch_size,\n      shuffle=False,\n      seed=2011 + idx\n  )\n  arr_test_generator.append(test_generator2)","30f55000":"model = get_model()","3b852045":"if JUST_TEST == True:\n    model_seeds = [1000]\nelse:\n    model_seeds = range(1000,1010)\n\nfor idx in model_seeds:\n    fname = '{}\/{}{}.hdf5'.format(WEIGHT_PATH,basename,idx)\n    fname_out = '.\/{}{}_data2.h5'.format(basename,idx)\n    print2('#Model: {} => {}'.format(fname,fname_out))\n    tmx = utc.localize(datetime.datetime.utcnow()).astimezone(KST).time()\n    print2('  - Load begins at {}'.format(tmx))\n    model.load_weights(fname)\n    tmx = utc.localize(datetime.datetime.utcnow()).astimezone(KST).time()\n    print2('  - Load ends at {}'.format(tmx))\n  \n    arr_predictions = []\n    for idx2 in range(1,10):\n        print2('  - Make test prediction #{}'.format(idx2+1) )\n        arr_test_generator[idx2].reset()\n        \n        if JUST_TEST==True:        \n            prediction = model.predict_generator(\n                generator=arr_test_generator[idx2],\n                steps = 4, #get_steps(nb_test_data, batch_size),\n                verbose=1\n            )\n        else:\n            prediction = model.predict_generator(\n                generator=arr_test_generator[idx2],\n                steps = get_steps(nb_test_data, batch_size),\n                verbose=1\n            )\n            \n        arr_predictions.append(prediction)\n    \n    print2('  - Write dataset')\n    hf = h5py.File(fname_out, 'w')\n    for idx2 in range(9):\n        datasetname = 'test_pred{}'.format(idx2+1)\n        hf.create_dataset(datasetname, data=arr_predictions[idx2])\n    hf.close()\n  \n    print(' ')","7b0a28d3":"PREDICTION_PATH1 = '..\/input\/3rd-ml-month-efficientnet-predictions-1'\nPREDICTION_PATH2 = '..\/input\/3rd-ml-month-efficientnet-predictions-2'\n\n# weight path \ud655\uc778\n!ls -l ..\/input\/3rd-ml-month-efficientnet-predictions-1 | head -6\n!ls -l ..\/input\/3rd-ml-month-efficientnet-predictions-2 | head -6\n","4c0a6484":"#\ucc98\ub9ac\ud574\ub193\uc740 \ud30c\uc77c\ub4e4\uc744 \ubd88\ub7ec\uc628\ub2e4.\narr_predictions = []\nfor basenm, rng in [('b3d2ovs',range(1000,1010)), ('b4d2ovs',range(1000,1020)), ('b5d2ovs', range(1000,1010))]:\n    for idx in rng:\n        fname_out = '{}\/{}{}_data.h5'.format(PREDICTION_PATH1, basenm, idx)\n        print('load: ', fname_out)\n        hf = h5py.File(fname_out, 'r')\n        #print('  keys = ',list(hf.keys()))\n        n1 = hf.get('test_pred0')\n        n1 = np.array(n1)\n        arr_predictions.append(n1)\n        hf.close()\n\n        fname_out = '{}\/{}{}_data2.h5'.format(PREDICTION_PATH2, basenm, idx)\n        print('load: ', fname_out)\n        hf = h5py.File(fname_out, 'r')\n        #print('  keys = ',list(hf.keys()))\n        for idx2 in range(9):\n            datanm = 'test_pred{}'.format(idx2+1)\n            n1 = hf.get(datanm)\n            n1 = np.array(n1)\n            #print(datanm,n1.shape)\n            arr_predictions.append(n1)\n        hf.close()        \n        ","06869f39":"# prediction average\navg_prediction = None\nfor idx in range(len(arr_predictions)):\n    #print(idx)\n    if (avg_prediction is None):\n        avg_prediction = arr_predictions[idx]\n    else:\n        avg_prediction = avg_prediction + arr_predictions[idx]\navg_prediction = avg_prediction \/ len(arr_predictions)   ","5d416242":"avg_predicted_class_indices=np.argmax(avg_prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (fulltrain_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\navg_predictions = [labels[k] for k in avg_predicted_class_indices]\n\nsub0=pd.DataFrame({'img_file': df_test['img_file'], 'class':avg_predictions})\nsub0.to_csv('avg_submission_40set.csv', index=False)\n\nsub0.head()\n!ls -l *.csv","a856b555":"#\ucc98\ub9ac\ud574\ub193\uc740 \ud30c\uc77c\ub4e4\uc744 \ubd88\ub7ec\uc628\ub2e4.\n\narr_fullpredictions = []\nfor basenm, rng in [('b3d2ovs',range(1000,1010)), ('b4d2ovs',range(1000,1020)), ('b5d2ovs', range(1000,1010))]:\n    for idx in rng:\n        fname_out = '{}\/{}{}_data.h5'.format(PREDICTION_PATH1, basenm, idx)\n        print('load: ', fname_out)\n        hf = h5py.File(fname_out, 'r')\n        n1 = hf.get('full_pred')\n        n1 = np.array(n1)\n        arr_fullpredictions.append(n1)\n        #print(n1.shape)\n        hf.close()","67f7a8bd":"# \uc5f4\ubc29\ud5a5\uc73c\ub85c \ub370\uc774\ud130 \ud569\uce68\nstacked_set = np.hstack(arr_fullpredictions)\nprint(stacked_set.shape)\nstacked_set2 = stacked_set.reshape((-1, len(arr_fullpredictions), 196))\nprint(stacked_set2.shape)","cff607ef":"df_train2 = df_train.copy()\ndf_train2['class'] = df_train2['class'].astype(str)\n\ntest_datagen = ImageDataGenerator()\nfulltrain_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_train2,\n    directory=TRAIN_IMG_PATH,\n    x_col='img_file',\n    y_col=\"class\",\n    target_size=(image_size, image_size),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=batch_size,\n    shuffle=False\n)\n\ndef batch_generator(X, y, batch_size=16):\n    #Return a random row from X, y\n\n    labels = (fulltrain_generator.class_indices)\n    meta_train_Y = y\n    meta_train_Y = [labels[x] for x in meta_train_Y]\n    meta_train_Y = utils.to_categorical(meta_train_Y)\n    \n    while True:\n        # choose batch_size random rows \/ labels from the data\n        idx = np.random.randint(0, X.shape[0], batch_size)\n        rows = X[idx]\n        enc_labels = meta_train_Y[idx]\n        \n        yield rows, enc_labels\n        \n#print(fulltrain_generator.class_indices)        ","a5663f2c":"from keras import backend\n\ndef get_model(obj):\n  # Create a keras model\n  \n  '''\n  #cnn model\n  input = layers.Input(shape=(obj.shape[1], obj.shape[2],))\n  input2 = layers.Reshape((len(arr_fullpredictions),196,1)) (input)\n\n  node1 = layers.Conv2D(1, (len(arr_fullpredictions), 1), data_format='channels_last') (input2)\n  \n  drop1 = layers.Dropout(0.5)(node1)\n  drop1 = layers.Flatten()(drop1)\n\n  node2 = layers.Dense(196, activation='relu')(drop1)\n  node2 = layers.Dropout(0.5)(node2)\n  node3 = layers.Dense(196, activation='softmax')(node2)\n  \n  model = models.Model(inputs=input, outputs=node3)\n  \n  model.summary()\n  '''\n\n\n  #'''\n  #dnn model\n  input = layers.Input(shape=(obj.shape[1], obj.shape[2],))\n  input2 = layers.Flatten()(input)\n  node1 = layers.Dense(196, activation='selu')(input2)\n  drop1 = layers.Dropout(0.5)(node1)\n  node2 = layers.Dense(196, activation='selu')(drop1)\n  drop2 = layers.Dropout(0.5)(node2)\n  node3 = layers.Dense(196, activation='softmax')(drop2)\n\n  model = models.Model(inputs=input, outputs=node3)\n \n  model.summary()\n  #'''\n\n  \n  '''\n  #dnn model2\n  input = layers.Input(shape=(obj.shape[1], obj.shape[2],))\n  input2 = layers.Flatten()(input)\n  node1 = layers.Dense(196*4, activation='relu')(input2)\n  drop1 = layers.Dropout(0.5)(node1)\n  node2 = layers.Dense(196*4, activation='relu')(drop1)\n  drop2 = layers.Dropout(0.5)(node2)\n  node3 = layers.Dense(196, activation='softmax')(drop2)\n\n  model = models.Model(inputs=input, outputs=node3)\n \n  model.summary()\n  '''\n  \n  optimizer = optimizers.Adamax()\n  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc', f1_m])  #\n\n  return model\n\nmodel = get_model(stacked_set2)","98bc4f42":"# Train\uc911\uc5d0 \ub85c\uadf8\ub97c \ucd9c\ub825\ud558\ub294 CallBack\nKST = timezone('Asia\/Seoul')\nclass EpochLogWrite(Callback):\n    def on_train_begin(self, logs={}):\n        None\n    def on_batch_begin(self, batch, logs={}):\n        None\n    def on_epoch_begin(self, epoch, logs={}):\n        tmx = utc.localize(datetime.datetime.utcnow()).astimezone(KST).time()\n        print2('Epoch #{} begins at {}'.format(epoch+1, tmx))\n    def on_train_end(self, logs={}):\n        None\n    def on_batch_end(self, batch, logs={}):\n        None\n    def on_epoch_end(self, epoch, logs={}):\n        tmx = utc.localize(datetime.datetime.utcnow()).astimezone(KST).time()\n        print2('Epoch #{} ends at {}  acc={} val_acc={} val_f1={}'.format(epoch+1, tmx, round(logs['acc'],4), round(logs['val_acc'],4), round(logs['val_f1_m'],4) ))\n        \npatient = 2\ncallbacks1 = [\n    EpochLogWrite(),\n    EarlyStopping(monitor='val_acc', patience=patient, mode='max', verbose=1),\n    ReduceLROnPlateau(monitor = 'val_acc', factor = 0.5, patience = patient \/ 2, min_lr=0.00001, verbose=1, mode='max'),\n    ModelCheckpoint(filepath='.\/dnn_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max'),\n    ]\n\ncallbacks2 = [\n    EpochLogWrite(),\n    EarlyStopping(monitor='val_f1_m', patience=patient, mode='max', verbose=1),\n    ReduceLROnPlateau(monitor = 'val_f1_m', factor = 0.5, patience = patient \/ 2, min_lr=0.00001, verbose=1, mode='max'),\n    ModelCheckpoint(filepath='.\/dnn_model.hdf5', monitor='val_f1_m', verbose=1, save_best_only=True, mode='max'),\n    ]","44d75f45":"print(y_valid.values.shape)\nprint(np.zeros(df_test.shape[0]).shape )\nprint(df_test.shape)","e6908b00":"\nX_train, X_valid, y_train, y_valid = \\\n    train_test_split(stacked_set2, df_train2['class'], test_size=validation_ratio, \\\n                     random_state=1005, \\\n                     stratify=df_train['class'])\n\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_valid.shape)\nprint(y_valid.shape)\n\ntrain_generator = batch_generator(X_train, y_train.values, batch_size=batch_size)\nvalid_generator = batch_generator(X_valid, y_valid.values, batch_size=batch_size)\n#test_generator = testbatch_generator(stacked_set, np.zeros(df_test.shape[0]), batch_size=batch_size)","d73ed0f6":"history = model.fit_generator(\n    train_generator,\n    steps_per_epoch = get_steps(nb_train_sample, batch_size),\n    epochs=100,\n    validation_data=valid_generator,\n    validation_steps = get_steps(nb_validation_sample, batch_size),\n    verbose=1,\n    callbacks = callbacks1\n)","928e2280":"model.load_weights('.\/dnn_model.hdf5')\n\narr_gross_predictions = []\nfor midx in range(10):\n    print('### Round: #{}'.format(midx))\n  \n    arr_predictions = []\n    for basenm, rng in [('b3d2ovs',range(1000,1010)), ('b4d2ovs',range(1000,1020)), ('b5d2ovs', range(1000,1010))]:\n        for idx in rng:\n            if (midx==0):\n                fname_out = '{}\/{}{}_data.h5'.format(PREDICTION_PATH1, basenm, idx)\n                #print('load: ', fname_out)\n                hf = h5py.File(fname_out, 'r')\n                n1 = hf.get('test_pred0')\n                n1 = np.array(n1)\n                arr_predictions.append(n1)\n                hf.close()\n            else:\n                fname_out = '{}\/{}{}_data2.h5'.format(PREDICTION_PATH2, basenm, idx)\n                #print('load: ', fname_out)\n                hf = h5py.File(fname_out, 'r')\n                n1 = hf.get('test_pred{}'.format(midx))\n                n1 = np.array(n1)\n                arr_predictions.append(n1)\n                hf.close()\n\n    # \uc5f4\ubc29\ud5a5\uc73c\ub85c \ub370\uc774\ud130 \ud569\uce68\n    stacked_tset = np.hstack(arr_predictions)\n    print(stacked_tset.shape)\n    stacked_tset2 = stacked_tset.reshape((-1, len(arr_predictions), 196))\n    print(stacked_tset2.shape)\n\n    dnn_prediction = model.predict(stacked_tset2)\n    arr_gross_predictions.append(dnn_prediction)\n\n","937bdc28":"avg_prediction = None\nfor idx in range(len(arr_gross_predictions)):\n    print('### Round: #{}'.format(idx))\n    if (avg_prediction is None):\n        avg_prediction = arr_gross_predictions[idx]\n    else:\n        avg_prediction = avg_prediction + arr_gross_predictions[idx]\navg_prediction = avg_prediction \/ len(arr_gross_predictions)   ","4c879dc3":"avg_predicted_class_indices=np.argmax(avg_prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (fulltrain_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\navg_predictions = [labels[k] for k in avg_predicted_class_indices]\n\nsub0=pd.DataFrame({'img_file': df_test['img_file'], 'class':avg_predictions})\nsub0.to_csv('dnn_submission_40set.csv', index=False)\n\nsub0.head()\n","a7697f86":"<h1><b>2019 3rd ML month with KaKR Solution (LB 13th)<\/b><\/h1>\n<br>\n3\ucc28 \ub300\ud68c\ub97c \ucc38\uac00\ud558\uc5ec \uc9c4\ud589\ud588\ub358 \ub0b4\uc6a9\ub4e4\uc744 \uc815\ub9ac\ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4.<br>\n\ud0dc\uba85\ub2d8\uc774 \uc81c\uacf5\ud574\uc8fc\uc2e0 crop \ub370\uc774\ud130\uc14b\uacfc \uc544\ub798 \ucee4\ub110\ub4e4\uc744 \ucc38\uace0\ud588\uc2b5\ub2c8\ub2e4.<br>\n\uc774\uc0c1\ud558\uac70\ub098 \uc798\ubabb\ub41c \ub0b4\uc6a9\uc774 \uc788\uc744\uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc758\uacac \uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4 ^^<br>\n<br>\n*\ucc38\uace0\ucee4\ub110 <br>\nhttps:\/\/www.kaggle.com\/fulrose\/3rd-ml-month-car-model-classification-baseline <br>\nhttps:\/\/www.kaggle.com\/tmheo74\/3rd-ml-month-car-image-cropping-updated-7-10 <br>\nhttps:\/\/www.kaggle.com\/easter3163\/3rd-ml-month-keras-efficientnet <br>\n","092e3e76":"## 1.8 Commit Monitoring\n\ucee4\ub110 \ucf54\ub4dc\ub97c \uc791\uc131\ud574\uc11c \ucee4\ubc0b\ud558\ub294\ub370 \uae38\uac8c\ub294 9\uc2dc\uac04\uae4c\uc9c0 \ub3cc\uc544\uc57c\ud558\ub294\ub370, \uc5bc\ub9c8\ub098 \ub3cc\uace0 \uc788\ub294\uc9c0 \uc54c\uc218\uc5c6\ub294\uac74 \ucc38 \ub2f5\ub2f5\ud55c \uc77c\uc774\uc5c8\uc2b5\ub2c8\ub2e4<br>\n\uad6c\uae00\ub9c1\uc73c\ub85c \ucc3e\uc740 \ub85c\uae45\ubc29\ubc95\uc744 \ud65c\uc6a9\ud574\uc11c \ubaa8\ub2c8\ud130\ub9c1 \uc815\ubcf4\ub97c \ucee4\ubc0b\ub85c\uadf8\uc5d0 \ub0a8\uae30\ub294 \ud301\uc744 \ub514\uc2a4\ucee4\uc158\uc5d0 \ub0a8\uaca8\ub193\uc558\uc2b5\ub2c8\ub2e4.<br>\n\ub2e4\ub4e4 \ud65c\uc6a9\ud558\uc168\uc73c\uba74 \uc88b\uaca0\uc2b5\ub2c8\ub2e4.<br>\nhttps:\/\/www.kaggle.com\/c\/2019-3rd-ml-month-with-kakr\/discussion\/103725#latest-597942\n<img src=\"http:\/\/cruiserx.cafe24.com\/kaggle\/1.8.commit-monitoring.png\">","7a1c6c73":"# 5. Average Ensemble Model \n\ud2b8\ub808\uc774\ub2dd\uc2dc\uc5d0 \uc0dd\uc131\ud55c \uc6d0\ubcf8test prediction, \uc717\ub2e8\uacc4\uc5d0 \uc0dd\uc131\ud55c augmented prediction\uc744 \ubd88\ub7ec\uc640\uc11c \uc559\uc0c1\ube14\ud569\ub2c8\ub2e4.\n","ce75089d":"## 1.7 Partial Kernel\n\uc815\ub9ac\ud558\ub294 \ucc28\uc6d0\uc5d0\uc11c \ucee4\ub110\uc744 \ud558\ub098\ub85c \uc791\uc131\ud588\uc2b5\ub2c8\ub2e4\ub9cc, \uc804\ucc98\ub9ac \ub530\ub85c \ud559\uc2b5\uc740 \ubaa8\ub378\ubcc4\ub85c \uac01\uac01 \uc778\ud37c\ub7f0\uc2a4\ub3c4 \uac01\uac01 \uc559\uc0c1\ube14\ub3c4 \uac01\uac01 \uc791\uc740 \ucee4\ub110\uc744 \ub9cc\ub4e4\uc5b4\uc11c \uc9c4\ud589\ud558\uc600\uc2b5\ub2c8\ub2e4.<br>\n\uce90\uae00\ud658\uacbd\ub9cc \uc0ac\uc6a9\ud558\ub2e4 \ubcf4\ub2c8 9\uc2dc\uac04 \uc81c\uc57d\ub3c4 \uc788\uace0 \uae38\uac8c \ub3cc\ub2e4\uac00 \uc5d0\ub7ec\uac00 \ub098\uba74 \uc911\uac04\uacb0\uacfc\ubb3c\uc744 \uc804\ubd80 \uc783\uc5b4\ubc84\ub9ac\uac8c \ub418\ub2c8 \ud0c0\uaca9\uc774 \ub108\ubb34 \ud07d\ub2c8\ub2e4.<br>\n\uadf8\ub798\uc11c \uacfc\uc815\ubcc4\ub85c \ud559\uc2b5\ud55c weight, train prediction, test prediction, augmented test prediction\uc740 \ubaa8\ub450 \ud30c\uc77c\ub85c \uc800\uc7a5\ud558\uc5ec \uc7ac\ud65c\uc6a9\ud588\uc2b5\ub2c8\ub2e4.<br>\n","8f933b53":"\ucc98\uc74c\uc5d0 \uc5b4\ub5a4 \ubaa8\ub378\uc744 \uace8\ub77c\uc57c \ud560\uc9c0 \ubab0\ub77c\uc11c \uc774 \uc0ac\uc774\ud2b8\ub97c \ucc38\uc870\ud558\uc600\uc2b5\ub2c8\ub2e4.<br>\nhttps:\/\/paperswithcode.com\/sota\/image-classification-on-imagenet <br>\n<img src=\"http:\/\/cruiserx.cafe24.com\/kaggle\/1.1.imagenet-sota.png\"><br>\n\ubaa8\ub378 \uc885\ub958\ub3c4 \ucc38 \ub9ce\uace0 \uac01\uac01 \ud2b9\uc131\uc774 \uc788\uaca0\uc9c0\ub9cc EfficientNet\uc774 \uc0c1\ub300\uc801\uc73c\ub85c \uba54\ubaa8\ub9ac\ub97c \uc801\uac8c\uc4f0\uba74\uc11c\ub3c4 \uc131\ub2a5\uc774 \uad1c\ucc2e\uc740\uac83 \uac19\uc2b5\ub2c8\ub2e4. <br>\n\uadf8\ub798\uc11c \uc774\ubc88 \ub300\ud68c\uc5d0\uc11c\ub294 EfficientNet\uc911\uc5d0\uc11c B3,B4,B5\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. B7\uc744 \uc4f0\uace0 \uc2f6\uc5c8\uc9c0\ub9cc \ub2e8\uacc4\ub9c8\ub2e4 \uc2dc\uac04\uc774 \ub450\ubc30\uc529 \uc99d\uac00\ud558\ub354\uad70\uc694. <br>\n\uc5ec\ub825\uc774 \uc788\ub2e4\uba74 \ud55c\uac00\uc9c0 \ubaa8\ub378\ub9cc \uc2dc\ub3c4\ud558\ub294 \uac83\ubcf4\ub2e4 \ub450\uc138\uac1c \ubaa8\ub378\uc744 \uc2dc\ud5d8\ud574\ubcf4\ub294 \uac83\uc774 \uc88b\uaca0\uace0 \uc559\uc0c1\ube14 \ud558\ub294\uac83\uc774 \uc88b\uc740\uac83 \uac19\uc2b5\ub2c8\ub2e4.","3a7fd79b":"## 1.3 Stratified Split\/OverSampling\n\n- \uce35\ud654\ucd94\ucd9c(Stratified Sampling)<br>\n train-set\uacfc validation-set\uc744 70:30\uc73c\ub85c \ub098\ub204\uae30\ub85c \uc815\ud558\uace0<br>\n \uac01\uac01 \ud074\ub798\uc2a4\ubcc4\ub85c \uac19\uc740 \ube44\uc728\ub85c \ubf51\ud788\ub3c4\ub85d stratify \uc635\uc158\uc744 \uc8fc\uc5c8\uc2b5\ub2c8\ub2e4.<br>\n \uc774 \uc635\uc158\uc744 \uc8fc\uc9c0 \uc54a\uc73c\uba74 \uc5b4\ub5a4 \ud074\ub798\uc2a4\ub294 \ub9ce\uace0 \uc5b4\ub5a4 \ud074\ub798\uc2a4\ub294 \uc801\uac8c \ucd94\ucd9c\ub429\ub2c8\ub2e4.<br>\n\n>  \n    train_test_split(df_train, df_train['class'], test_size=validation_ratio, random_state=1005, \\\n                     stratify=df_train['class'])\n\n- \uc624\ubc84\uc0d8\ud50c\ub9c1(Over-Sampling)<br>\n<img src=\"http:\/\/cruiserx.cafe24.com\/kaggle\/1.3.imbalance-data.png\"><br>\n \ud074\ub798\uc2a4 \ubcc4\ub85c \ub370\uc774\ud130 \ubd84\ud3ec\ub97c \ubcf4\uba74 \ub370\uc774\ud130\uac00 \uc801\uc740 \ud074\ub798\uc2a4\uc640 \ub9ce\uc740 \ud074\ub798\uc2a4\uac04\uc5d0 \ucc28\uc774\uac00 \uc788\ub294 imbalance dataset\uc785\ub2c8\ub2e4.<br>\n \ucc98\ub9ac\ud558\ub294\ub370\ub294 \uc624\ubc84\uc0d8\ud50c\ub9c1\ub3c4 \uc788\uace0 \uc5b8\ub354\uc0d8\ud50c\ub9c1\ub3c4 \uc788\uace0 \uc5ec\ub7ec\uac00\uc9c0 \ubc29\ubc95\ub4e4\uc774 \uc788\uc2b5\ub2c8\ub2e4\ub9cc, \uc800\ub294 \uc624\ubc84\uc0d8\ud50c\ub9c1 \ubc29\ubc95\uc744 \uc774\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.<br>\n \ub370\uc774\ud130\ub97c \ubc84\ub9ac\uae30\uc5d4 \uc880 \uc544\uae5d\ub2e4\ub294 \uc0dd\uac01\ub3c4 \ub4e4\uc5c8\uace0, train\uc2dc \uc81c\ub108\ub808\uc774\ud130\uac00 Augmentation\uc744 \ud574\uc8fc\ubbc0\ub85c,<br>\n \ub3d9\uc77c\ud55c \ub370\uc774\ud130\ub97c \ucc44\uc6cc\ub123\ub294\uac83\uc5d0 \ube44\ud574\uc11c\ub294 \ub098\uc058\uc9c4 \uc54a\uaca0\ub2e4 \uc0dd\uac01\ud588\uc2b5\ub2c8\ub2e4.","ee03fef7":"## 1.2 Preprocessing\n\uc8fc\uc5b4\uc9c4 \uc774\ubbf8\uc9c0\uc14b\uc5d0\uc11c \ubc15\uc2a4\ub85c crop\ud558\uace0, \uadf8 \uc548\uc5d0\uc11c\ub3c4 \ucc28 \uc774\uc678\uc758 \uc694\uc18c\ub97c \uc81c\uac70\ud558\ub294\uac78 \uace0\ubbfc\ud574\uc11c \uc544\ub798\ucc98\ub7fc \uc804\ucc98\ub9ac\ub97c \ud588\uc5c8\uc2b5\ub2c8\ub2e4.<br>\nhttps:\/\/www.kaggle.com\/cruiserx\/3rd-ml-month-car-image-segmentation-crop\n","e491617a":"# 4. Augmented Test Prediction\n\uc717\ub2e8\uacc4\uc5d0\uc11c \ud559\uc2b5\ud560\ub54c \uc0dd\uc131\ud55c \uc6e8\uc774\ud2b8 \ud30c\uc77c\uc744 \ubd88\ub7ec\uc640\uc11c Augmentation\ub41c Test Prediction\uc744 9\uc138\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n\ud2b8\ub808\uc774\ub2dd\uacfc \ub9c8\ucc2c\uac00\uc9c0\ub85c \uc2dc\uac04\uc774 \uaf64 \uac78\ub9ac\ubbc0\ub85c \uc77c\ubd80\ub9cc \ucc98\ub9ac\ud558\ub3c4\ub85d \ud574\ub450\uc5c8\uc2b5\ub2c8\ub2e4. ","a142ba07":"<img src=\"http:\/\/cruiserx.cafe24.com\/kaggle\/1.2.image-preprocessing.png?123\">","6b029234":"# 3. Model Training\n40\uac1c\uc758 \ubaa8\ub378\uc744 \ud2b8\ub808\uc774\ub2dd\ud560\ub584 \uc544\ub798\ucf54\ub4dc\ub4e4\uc5d0\uc11c \ubaa8\ub378\ud074\ub798\uc2a4\uc640 \ubcc0\uc218,\ub79c\ub364 \uc2dc\ub4dc\ub9cc \ubc14\uafd4\uac00\uba70 \uc2e4\ud589\ud588\uc2b5\ub2c8\ub2e4.<br>\n\uc2dc\uac04\uc774 \ub9ce\uc774 \uac78\ub9ac\ubbc0\ub85c \uc774 \ucee4\ub110\uc5d0\uc11c\ub294 \uc911\uac04\uc5d0 \ub05d\ub098\ub3c4\ub85d \ucc98\ub9ac\ud574\ub450\uc5c8\uc2b5\ub2c8\ub2e4. ","3e4a219e":"## 1.5 Model Training\n\n> \nEfficientNet B3, B4, B5\ub97c \uc2dc\ub4dc\ub97c \ubc14\uafd4\uac00\uc11c train set\uc744 \ub9cc\ub4e4\uc5b4\uc11c \ud559\uc2b5\uc2dc\ucf30\uc2b5\ub2c8\ub2e4. <br>\n\ub300\uccb4\ub85c B4\uac00 \uc798 \ub9de\ub294\uac83 \uac19\uc2b5\ub2c8\ub2e4\ub9cc, \uba54\ubaa8\ub9ac \ud55c\uacc4\ub85c B5\ub294 \ubc30\uce58\uc0ac\uc774\uc988\ub97c \uc904\uc5ec\uc11c \ub3cc\ub838\uc73c\ub2c8 <br>\n\uc815\ud655\ud55c \ube44\uad50\ub294 \uc548\ub420\uac83 \uac19\uc2b5\ub2c8\ub2e4. <br>\n<br>\n\n\nOptimizer\ub294 \ucc98\uc74c\uc5d0\ub294 rmsprop\uc744 \uc0ac\uc6a9\ud558\ub2e4\uac00 \uba87\uac00\uc9c0 \uc2dc\ud5d8\ud574\ubcf4\uace0 \uadf8\uc911\uc5d0 \uc810\uc218\uac00 \uc798 \ub098\uc624\ub358 adamax\ub85c \ubc14\uafe8\uc2b5\ub2c8\ub2e4.<br>\n<br>\n\uae30\uc900 metric\uc774 f1\uc774\ub77c\uc11c \uc544\ub798\ucc98\ub7fc \ud568\uc218\ub97c \uc815\uc758\ud574\uc11c \uc37c\uc9c0\ub9cc,<br>\n\n>\ndef f1_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\n<br>\n\ub0b4\uc7a5\ud568\uc218\uac00 \uc544\ub2c8\uc5c8\uae30 \ub54c\ubb38\uc778\uc9c0..\ud559\uc2b5\uc774 \ud6e8\uc52c \ub354 \uc624\ub798 \uac78\ub9ac\ub294 \ubb38\uc81c\uac00 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. <br>\nf1\uc5d0 \ub9de\ucdb0\uc11c \ub3cc\ub9ac\uba74 \uc810\uc218\uac00 \uc880 \ub0ab\ub2e4\ub294 \uae00\uc744 \ubcf4\uae34 \ud588\uc9c0\ub9cc, \uc2dc\uac04\uc0c1 \ud560\uc218\uc5c6\uc774accuracy\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.<br>\n<br>\n\uc544\ub798\ub294 \ubaa8\ub378\ub4e4\uc744 \ud559\uc2b5\uc2dc\ud0a4\uba74\uc11c \uae30\ub85d\ud574\ub450\uc5c8\ub358 \uc810\uc218\uc640 \uc218\ud589\uc2dc\uac04\uc785\ub2c8\ub2e4.<br>\n<br>\n- <b>EfficientNet B3 (10 set)<\/b>\n<br>\n<br>\n    - SplitSeed 1000 : val_acc= 0.9032 val_f1_m= 0.9066 (7182s)\n    - SplitSeed 1001 : val_acc= 0.9169 val_f1_m= 0.9189 (8044s)\n    - SplitSeed 1002 : val_acc= 0.9102 val_f1_m= 0.912  (8856s)\n    - SplitSeed 1003 : val_acc= 0.9129 val_f1_m= 0.9165 (6844s)\n    - SplitSeed 1004 : val_acc= 0.9126 val_f1_m= 0.9148 (7685s)\n    - SplitSeed 1005 : val_acc= 0.9146 val_f1_m= 0.916  (7274s)\n    - SplitSeed 1006 : val_acc= 0.9209 val_f1_m= 0.924  (9083s)\n    - SplitSeed 1007 : val_acc= 0.9166 val_f1_m= 0.9199 (8838s)\n    - SplitSeed 1008 : val_acc= 0.9146 val_f1_m= 0.9173 (8148s)\n    - SplitSeed 1009 : val_acc= 0.9069 val_f1_m= 0.9094 (5886s)\n<br>\n<br>\n- <b>EfficientNet B4 (20 set)<\/b>\n<br>\n<br>\n    - SplitSeed 1000 : val_acc= 0.9109 val_f1_m= 0.9141 (8100s)\n    - SplitSeed 1001 : val_acc= 0.9243 val_f1_m= 0.9278 (10902s)\n    - SplitSeed 1002 : val_acc= 0.9266 val_f1_m= 0.9283 (14297s)\n    - SplitSeed 1003 : val_acc= 0.9239 val_f1_m= 0.9265 (10237s)\n    - SplitSeed 1004 : val_acc= 0.9196 val_f1_m= 0.9218 (12780s)\n    - SplitSeed 1005 : val_acc= 0.9266 val_f1_m= 0.9287 (13675s)\n    - SplitSeed 1006 : val_acc= 0.9343 val_f1_m= 0.9364 (14473s)\n    - SplitSeed 1007 : val_acc= 0.9299 val_f1_m= 0.9323 (15496s)\n    - SplitSeed 1008 : val_acc= 0.9246 val_f1_m= 0.9267 (13735s)\n    - SplitSeed 1009 : val_acc= 0.9249 val_f1_m= 0.9261 (15751s)\n    - SplitSeed 1010 : val_acc= 0.9306 val_f1_m= 0.933  (13088s)\n    - SplitSeed 1011 : val_acc= 0.9243 val_f1_m= 0.9274 (11652s)\n    - SplitSeed 1012 : val_acc= 0.9229 val_f1_m= 0.9238 (13940s)\n    - SplitSeed 1013 : val_acc= 0.9276 val_f1_m= 0.9306 (14321s)\n    - SplitSeed 1014 : val_acc= 0.9249 val_f1_m= 0.9263 (14511s)\n    - SplitSeed 1015 : val_acc= 0.9206 val_f1_m= 0.9222 (13666s)\n    - SplitSeed 1016 : val_acc= 0.9152 val_f1_m= 0.9198 (11838s)\n    - SplitSeed 1017 : val_acc= 0.9213 val_f1_m= 0.9231 (12300s)\n    - SplitSeed 1018 : val_acc= 0.9243 val_f1_m= 0.9276 (15369s)\n    - SplitSeed 1019 : val_acc= 0.9253 val_f1_m= 0.9272 (10911s)\n<br>\n<br>\n- <b>EfficientNet B5 (10 set)<\/b>\n<br>\n<br>\n    - SplitSeed 1000 : val_acc= 0.9183 val_f1_m= 0.9213 (26633s)\n    - SplitSeed 1001 : val_acc= 0.9229 val_f1_m= 0.925  (30866s)\n    - SplitSeed 1002 : val_acc= 0.9266 val_f1_m= 0.929  (31631s)\n    - SplitSeed 1003 : val_acc= 0.9152 val_f1_m= 0.9174 (25085s)\n    - SplitSeed 1004 : val_acc= 0.9223 val_f1_m= 0.9247 (25701s)\n    - SplitSeed 1005 : val_acc= 0.9273 val_f1_m= 0.9292 (28102s)\n    - SplitSeed 1006 : val_acc= 0.9156 val_f1_m= 0.9182 (23569s)\n    - SplitSeed 1007 : val_acc= 0.9246 val_f1_m= 0.9265 (26997s)\n    - SplitSeed 1008 : val_acc= 0.9193 val_f1_m= 0.9198 (25223s)\n    - SplitSeed 1009 : val_acc= 0.9279 val_f1_m= 0.9289 (22363s)\n    ","806237d0":"\uadf8\ub7f0\ub370 \uc810\uc218\ub97c \ube44\uad50\ud574\ubcf4\ub2c8 box\ub85c crop\ub9cc \ud55c \uacbd\uc6b0\uc5d0 \ube44\ud574\uc11c \uc8fc\ubcc0\uc744 \uc798\ub77c\ub0b8\uac8c \uc810\uc218\uac00 \ub354 \ub0ae\uac8c \ub098\uc635\ub2c8\ub2e4. <br>\n\ucc28 \uc8fc\ubcc0\uc758 \uc694\uc18c\ub4e4\ub3c4 \ubaa8\ub378\uc758 \ubd84\ub958 \uc608\uce21\ub825\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4.<br>\n<br>\n\ube44\uc728\uc744 \uc720\uc9c0\ud558\ub3c4\ub85d \ud328\ub529\uc744 \ud558\ub294 \uac83\ub3c4 \uc548\ud558\ub294 \uac83\uc5d0 \ube44\ud574\uc11c \uc810\uc218\uac00 \ub0ae\uac8c \ub098\uc635\ub2c8\ub2e4.<br>\n\ud328\ub529 \uc0c9\uc0c1\uc5d0 \ub530\ub77c\uc11c\ub3c4 \uc810\uc218\uac00 \ub2e4\ub974\uac8c \ub098\uc624\ub294\ub370\uc694, \ub79c\ub364\/\uac80\uc740\uc0c9\/\ud770\uc0c9 \ubcf4\ub2e4 \ud68c\uc0c9\uc73c\ub85c \ud328\ub529\ud558\ub294\uac8c \uc810\uc218\uac00 \ub354 \ub192\uc558\uc2b5\ub2c8\ub2e4.<br>\n\uc0dd\uac01\ub098\ub294 \uc774\ub7f0\uc800\ub7f0 \uc804\ucc98\ub9ac\ub97c \uc2dc\ub3c4\ud574\ubcf4\uc558\uc9c0\ub9cc \uc810\uc218\uac00 \ub098\uc544\uc9c0\uc9c0 \uc54a\uc544\uc11c \uadf8\ub0e5 \ud0dc\uba85\ub2d8\uc758 crop dataset\uc744 \uc774\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.<br>\n\n\uc544\ub798 \uadf8\ub9bc\ub4e4\uc740 \uc774\ub7f0\uc800\ub7f0 \uc804\ucc98\ub9ac\ub97c \ud574\ubd24\ub358 \ub0b4\uc6a9\uc785\ub2c8\ub2e4. \ub4f1\uc218\uc5d0 \ub3c4\uc6c0\uc740 \uc548\ub418\uc5c8\uc9c0\ub9cc\uc694..<br>\n<img src=\"http:\/\/cruiserx.cafe24.com\/kaggle\/1.2.image-preprocessing2.png\"><br>\n<img src=\"http:\/\/cruiserx.cafe24.com\/kaggle\/1.2.image-preprocessing3.png\"><br>","ccc2ef12":"# 6. DNN Ensemble Model \n\ud2b8\ub808\uc774\ub2dd\uc2dc\uc5d0 \ubaa8\ub378\ubcc4\ub85c \uc0dd\uc131\ud574\ub450\uc5c8\ub358 40set\uc758 full-train prediction\uc744 hstack\ud558\uc5ec \uc785\ub825\uc73c\ub85c \ud558\uc5ec \ud074\ub798\uc2a4\ub97c \uc608\uce21\ud558\ub294 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4 \ud6c8\ub828\uc2dc\ud0b5\ub2c8\ub2e4.<br>\n\uc774 \ubaa8\ub378\uc5d0 test prediction\uc744 hstack\ud55c \uc785\ub825, augmented set\ubcc4\ub85c prediction\uc744 hstack\ud55c \uc785\ub825\uc744 \uac01\uac01 \ub123\uc5b4 \ucd9c\ub825\ub41c prediction 10\uac1c\ub97c \ud3c9\uade0\ud558\uc5ec \ucd5c\uc885 prediction\uc744 \uad6c\ud569\ub2c8\ub2e4.","82622c2c":"# 2. Common Task","5fbf1e09":"## 1.4 Augmentation\n- Train Time<br>\n \ucc98\uc74c \ucc38\uace0\ud588\ub358 \ucee4\ub110\uc758 \uc81c\ub108\ub808\uc774\ud130 \ud30c\ub77c\ubbf8\ud130\ub97c \uae30\uc900\uc810\uc73c\ub85c \ub193\uace0 \ubc94\uc704\ub97c \uc870\uae08\uc529 \ubc14\uafd4\uac00\uba74\uc11c \uc810\uc218\uac00 \ub192\uac8c \ub098\uc624\ub294\uac78\ub85c<br>\n \uace8\ub790\uc2b5\ub2c8\ub2e4. \uc2dc\ud5d8\ud574\ubcf8\uac83\ub4e4 \uc911\uc5d0\uc11c \uac00\uc7a5 \ub098\uc558\ub358\uac74 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4.<br>\n\n>  \n    aug_datagen = ImageDataGenerator( \n                        rotation_range=10,  \n                        width_shift_range=0.1,  \n                        height_shift_range=0.1, \n                        horizontal_flip=True,  \n                        vertical_flip=False,  \n                        zoom_range=0.1,  \n                        shear_range=0.2,  \n                        fill_mode='nearest'  \n                  )  \n\n- Test Time<br>\n \ud559\uc2b5\uc774 \uc644\ub8cc\ub41c \ubaa8\ub378\ub85c test\ub370\uc774\ud130\ub97c \uac00\uacf5\ud558\ub294\ub370 augmentation\uc744 \uc218\ud589\ud55c \uacb0\uacfc\ubb3c\ub4e4\uc744 \uc870\ud569\ud558\uba74 \uc810\uc218\uac00 \uc624\ub985\ub2c8\ub2e4.<br>\n \uc218\ub3d9\uc73c\ub85c \uc5b4\ub5a4 \ubd80\ubd84\uc744 \ubcc0\ud615\ud560\uc9c0 \uc9c0\uc815\ud558\ub294 \ubc29\ubc95\ub3c4 \uc788\uc9c0\ub9cc, \uc774\ubbf8 \ud559\uc2b5\ud560\ub54c \uc37c\ub358 \uc81c\ub108\ub808\uc774\ud130\uac00 \uc788\uc73c\ub2c8 SEED\ub97c \ubc14\uafd4\uac00\uba70 \ud65c\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.<br>\n\n>  \n    test_generator2 = aug_datagen.flow_from_dataframe(\n                           dataframe=df_test,\n                           directory=TEST_IMG_PATH,\n                           x_col='img_file',\n                           y_col=None,\n                           target_size= (image_size,image_size),\n                           color_mode='rgb',\n                           class_mode=None,\n                           batch_size=batch_size,\n                           shuffle=False,\n                           seed=2011 + seq\n                     )   \n\n","41f3a018":"# 1. Overview\n## 1.1 Network Model","078fe4b3":"## 1.6 Model Ensemble\n\uc0dd\uc131\ud55c \ubaa8\ub378\uc740 \ucd1d 40\uac1c, \uac01 \ubaa8\ub378\ubcc4\ub85c test-time\uc5d0 augmentation\ud558\uc5ec \uc5bb\uc5b4\uc9c4 prediction 10\uac1c.<br>\n\uc804\ubd80 \ud569\uce58\uba74 400\uac1c\uc758 prediction\uc774 \ub098\uc635\ub2c8\ub2e4.<br>\n<br>\n\uad6c\ud574\uc9c4 prediction\ub4e4\uc744 \ub2e8\uc21c\ud3c9\uade0\ud558\uc5ec \ucd5c\uc885 prediction\uc744 \uacc4\uc0b0\ud558\uc600\uace0, <br>\n<b>avg_submission_40set.csv<\/b><br>\n<br>\n\uc774\ub97c DNN\uc5d0 \ub123\uc5b4\uc11c \ucd5c\uc885 prediction\uc744 \uc870\ud569\ud574\ubcf4\uae30\ub3c4 \ud558\uc600\uc2b5\ub2c8\ub2e4.<br>\n<b>dnn_submission_40set.csv<\/b><br>\n<br>\n\uc880 \ub354 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc744 \uacb0\ud569\ud558\uc600\ub2e4\uba74 \ub354 \ub098\uc740 \uc810\uc218\ub97c \uc5bb\uc9c0 \uc54a\uc558\uc744\uae4c\ud558\ub294 \uc544\uc26c\uc6c0\uc774 \uc788\uc2b5\ub2c8\ub2e4.  "}}