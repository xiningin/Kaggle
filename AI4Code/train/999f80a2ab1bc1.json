{"cell_type":{"390bf1e3":"code","23f25817":"code","98b2c853":"code","ed3a10c1":"code","7142eafe":"code","3fccb41c":"code","dd591189":"code","7c83774e":"code","0e490c08":"code","288b17fa":"code","ba54fc39":"code","41054e44":"code","7699ce6d":"code","76f7829e":"code","fc9bd3e4":"code","a1c11d40":"code","8ae3203a":"code","324c72dd":"code","acde81d7":"code","5de3a73c":"code","49917f64":"code","48404720":"code","76b198ca":"code","30fa9dc0":"markdown","0e9ec07c":"markdown","ea3c2eff":"markdown","5b777e7e":"markdown","0da8e741":"markdown","5cd927d9":"markdown","699ad310":"markdown","3c0567e8":"markdown","0d81bb2f":"markdown","96e39fe5":"markdown","b7bf6e72":"markdown","68c3f38a":"markdown","21f8902f":"markdown"},"source":{"390bf1e3":"import pandas as pd\n\nimport numpy as np\nimport os\nimport re\n\nfrom tqdm.auto import tqdm\n\nimport pandas as pd\nimport os\nimport re\nfrom tqdm.auto import tqdm\nimport numpy as np","23f25817":"ISLOCAL = False\nV2 = True #Version 2 where we try to work on discourse_type number field\n\nIS_CORRECTED_TRAIN = True\nTRAIN_CHAR_LEVEL = True\n\nif V2:\n    PREDICTOR_DISCOURSE = \"discourse_type_num\"\nelse:\n    PREDICTOR_DISCOURSE = \"discourse_type\"\nif ISLOCAL:\n    if IS_CORRECTED_TRAIN:\n        train_csv = \".\/corrected_train.csv\"\n    else:\n        train_csv = \".\/train.csv\"\n    train_dir = \".\/train\"\n    test_dir = \".\/test\"\n    ss_dir = \".\/sample_submission.csv\"\nelse:\n    if IS_CORRECTED_TRAIN:\n        train_csv = \"..\/input\/feedback-prize-corrected-train-csv\/corrected_train.csv\"\n    else:\n        train_csv = \"..\/input\/feedback-prize-2021\/train.csv\"\n        \n    train_directory = \"..\/input\/feedback-prize-2021\/train\"\n    test_directory = \"..\/input\/feedback-prize-2021\/test\"\n    ss_dir = \"..\/input\/feedback-prize-2021\/sample_submission.csv\"\n    ","98b2c853":"if IS_CORRECTED_TRAIN:\n    print(\"CAUTION: USING CORRECTED TRAN FILE FROM @nboard\")\n    \ndf = pd.read_csv(\"..\/input\/feedback-prize-corrected-train-csv\/corrected_train.csv\")\ndf_ss = pd.read_csv(\"..\/input\/feedback-prize-2021\/sample_submission.csv\")\n","ed3a10c1":"df[\"discourse_start\"] = df[\"new_start\"]\ndf[\"discourse_end\"] = df[\"new_end\"]\ndf[\"predictionstring\"] = df[\"new_predictionstring\"]","7142eafe":"def read_train_file(currid = \"423A1CA112E2\", curr_dir = train_directory):\n    with open(os.path.join(curr_dir, \"{}.txt\".format(currid)), \"r\") as f:\n        filetext = f.read()\n        \n    return filetext\n","3fccb41c":"d_labels = df[\"discourse_type\"].unique()\ndiscourse_type_num_labels = {i:x for x,i in enumerate(df[\"discourse_type_num\"].unique())}","dd591189":"def add_file_features(train, \n                      is_train = True):\n    '''\n        Dont call directly. Use get_feature_engineered instead\n    '''    \n    df = train.copy()\n    if is_train == False:\n        direc = test_directory\n    else:\n        direc = train_directory\n        \n    ret = []\n    for i in tqdm(df[\"id\"].unique()):\n        txt = read_train_file(i, direc)\n        filelen = len(txt)\n        row = {\"id\" : i}\n        row[\"non_alpha\"] = len(re.findall( \"[^a-zA-Z\\d\\s:]\"\n                                  , txt))\n        row[\"alpha\"] = len(txt) - row[\"non_alpha\"]\n        row[\"numbers\"] = len(re.findall(\"[\\d]\", txt))\n        row[\"filelength\"] = filelen\n        row[\"commas\"] = len(re.findall(\"[,]\", txt))\n        row[\"splits\"] = len(re.findall(\"[\\n?.,\\r!]\", txt))\n        row[\"tokens\"] = len(txt.split())\n\n        row[\"first_line_pos\"] = txt.find(\"\\n\") \/ filelen\n        row[\"first_ques_pos\"] = txt.find(\"?\") \/ filelen\n        row[\"first_exclaim_pos\"] = txt.find(\"!\") \/ filelen\n        row[\"first_period_pos\"] = txt.find(\".\") \/ filelen\n        row[\"first_comma_pos\"] = txt.find(\",\") \/ filelen\n\n        row[\"last_line_pos\"] = txt.rfind(\"\\n\") \/ filelen\n        row[\"last_ques_pos\"] = txt.rfind(\"?\") \/ filelen\n        row[\"last_exclaim_pos\"] = txt.rfind(\"!\") \/ filelen\n        row[\"last_period_pos\"] = txt.rfind(\".\") \/ filelen\n        row[\"last_comma_pos\"] = txt.rfind(\",\") \/ filelen\n\n        # for d in d_labels:\n        #     row[\"num_{}\".format(d)] = train_pivot.loc[train_pivot[\"id\"] == i, d].values[0]\n\n\n        ret.append( row )\n    return pd.DataFrame(ret)\n\ndef merge_num_discourse(train):\n    '''\n        Dont call directly. Use get_feature_engineered instead.\n        This can merge the Number of Discourse types against a feature engineered dataframe.\n    '''\n    df = train.copy()\n    \n    train_pivot = df[[\"id\", \"discourse_type_num\"]].pivot_table(index = [\"id\"],\n                           columns = [\"discourse_type_num\"],\n                           aggfunc = len).reset_index()\n    train_pivot = train_pivot.fillna(0)\n    \n    return train_pivot #df.merge( train_pivot, on = [\"id\"] )\n\ndef merge_file_and_discourse_properties(train, is_train = True):\n    df = train.copy()\n    ret = []\n    if is_train:\n        for i in tqdm(df.itertuples(), total = len(df)):\n            ret.append( {\"id\" : getattr(i, \"id\"),\n                         \"discourse_type_num\" : getattr(i, \"discourse_type_num\"),\n                         \"discourse_start\" : int(getattr(i, \"discourse_start\")),\n                         \"discourse_end\" : int(getattr(i, \"discourse_end\")),\n                         \"token_start\" : int(getattr(i, \"predictionstring\").split()[0]),\n                         \"token_end\" : int(getattr(i, \"predictionstring\").split()[-1]),\n                        })\n    else:\n        for i in tqdm(df[\"id\"].unique()):\n            for d in discourse_type_num_labels:\n                ret.append( {\"id\" : i,\n                             \"discourse_type_num\" : d})\n\n    return pd.DataFrame(ret)\n\ndef get_feature_engineered(train, is_train = True):\n    df = train.copy()\n    \n\n    \n    file_properties = add_file_features(train, is_train = is_train)\n    \n    discourse_properties = merge_file_and_discourse_properties(df, \n                                                               is_train = is_train)\n    \n    train = discourse_properties.merge( file_properties,\n                              on = [\"id\"])\n    if is_train:\n        train[\"discourse_start\"] = train[\"discourse_start\"] \/ train[\"filelength\"]\n        train[\"discourse_end\"] = train[\"discourse_end\"] \/ train[\"filelength\"]\n        train[\"token_start\"] = train[\"token_start\"] \/ train[\"tokens\"]\n        train[\"token_end\"] = train[\"token_end\"] \/ train[\"tokens\"]\n    \n    train[\"discourse_type_num\"] = train[\"discourse_type_num\"].map(discourse_type_num_labels)\n    return train\n\n\ndef get_r_squared(preds, y_true):\n    u = np.sum(( (y_true  - preds) ** 2))\n\n    v = np.sum(  (y_true - np.mean( y_true )) ** 2 )\n    score = 1 - u\/v\n    print(score)\n    return score","7c83774e":"# Step 1 : Get the pivot\ntrain_pivot = merge_num_discourse(df)\n\ntrain = get_feature_engineered(df)\n\ntest = get_feature_engineered(df_ss, is_train = False)\n","0e490c08":"train_pivot = train.merge(train_pivot, on = [\"id\"])\n\ntrain_pivot = train_pivot.drop([\"discourse_type_num\"], axis = 1)\n\ntrain_pivot = train_pivot.drop_duplicates(subset = [\"id\"])","288b17fa":"train_size = 0.7\n\ntrain_inds = np.random.choice( train[\"id\"].unique(),\n                              int(train_size * len( train[\"id\"].unique())),\n                             replace = False)\n\nX_train = train[train[\"id\"].isin(train_inds)].copy()\nX_test = train[~train[\"id\"].isin(train_inds)].copy()","ba54fc39":"TARGETS = [x for x in train.columns if (x.find(\"start\") > -1) or (x.find(\"end\") > -1)]\nPREDICTORS = [x for x in train.columns if train[x].dtype != \"object\" and x not in TARGETS]\n\nTARGETS2 = [x for x,i in discourse_type_num_labels.items()]\nPREDICTORS2 = [x for x in PREDICTORS if x != \"discourse_type_num\"]","41054e44":"from sklearn.ensemble import GradientBoostingRegressor\n\ndef train_model_and_predict(df_train, targets, predictors, target_df, df_test = None):\n    predictions = []\n    for ind, i in enumerate(tqdm( targets )):\n        print(\"Training a model for {}\".format(i))\n        gb = GradientBoostingRegressor(random_state = 91,\n                              n_estimators = 500, \n                              n_iter_no_change = 20)\n        gb.fit( df_train[predictors], df_train[targets[ind]] )\n        if df_test is not None:\n            score = gb.score(df_test[predictors],\n                            df_test[targets[ind]])\n            print(\"Model R-squared score for {} is {}\".format(\n                        i, score))\n        predictions.append( gb.predict( target_df[predictors] ) )\n    \n    return predictions","7699ce6d":"predictions_01 = train_model_and_predict(X_train, TARGETS, PREDICTORS, test, df_test = X_test)","76f7829e":"predictions_02 = train_model_and_predict(train_pivot, TARGETS2, PREDICTORS2, test, df_test = None)","fc9bd3e4":"for ind, i in enumerate(tqdm(TARGETS)):\n    test[TARGETS[ind]] = predictions_01[ind]\nfor ind, i in enumerate(tqdm(TARGETS2)):\n    test[TARGETS2[ind]] = predictions_02[ind] >= 0.5","a1c11d40":"test.head(3)","8ae3203a":"def calc_word_indices(full_text, discourse_start, discourse_end):\n    start_index = len(full_text[:discourse_start].split())\n    token_len = len(full_text[discourse_start:discourse_end].split())\n    output = list(range(start_index, start_index + token_len))\n    if len(output) == 0:\n        return -91\n    if output[-1] >= len(full_text.split()):\n        output = list(range(start_index, start_index + token_len-1))\n    \n    return output","324c72dd":"rev_discourse_type_num_labels = {i:x for x,i in discourse_type_num_labels.items()}\n\nret = []\nfor i in tqdm(test[\"id\"].unique()):\n    txt = read_train_file(i, test_directory)\n    num_chars = len(txt)\n    \n    for j in test[test[\"id\"] == i].iterrows():\n        if j[1][rev_discourse_type_num_labels[ j[1][\"discourse_type_num\"] ]] == True: \n            # Only process if this entry is true to be processed. Else skip adding this row.\n            disc_start = j[1][\"discourse_start\"]\n            disc_end = j[1][\"discourse_end\"]\n            # From relative position to char positions\n            disc_start = np.floor(disc_start * num_chars)\n            disc_end = np.ceil(disc_end * num_chars)\n            \n            disc_start = 0 if disc_start < 0 else disc_start\n            disc_end = num_chars if disc_end > num_chars else disc_end\n            if disc_start > disc_end:\n                t = disc_start\n                disc_start = disc_end\n                disc_end = t\n            \n            predictionstring = calc_word_indices(txt,\n                                                               int(disc_start),\n                                                               int(disc_end))\n            if predictionstring == -91:\n                continue #Invalid calc_position output\n                \n            predictionstring = \" \".join([str(x) for x in predictionstring])\n            ret.append( {\"id\" : i,\n                         \"predictionstring\" : predictionstring,\n                         \"discourse_type_num\" : j[1][\"discourse_type_num\"]\n                        })","acde81d7":"disc_mapping = df[[\"discourse_type_num\", \"discourse_type\"]].drop_duplicates(subset = [\"discourse_type_num\"]).set_index(\"discourse_type_num\").to_dict()[\"discourse_type\"]\n","5de3a73c":"ret = pd.DataFrame(ret)","49917f64":"ret[\"discourse_type_num\"] = ret[\"discourse_type_num\"].map(rev_discourse_type_num_labels)\n\nret[\"discourse_type_num\"] = ret[\"discourse_type_num\"].map(disc_mapping)\n","48404720":"ret = ret.rename( columns = {\"discourse_type_num\" : \"class\"})\n\nret[[\"id\", \"class\", \"predictionstring\"]].to_csv(\"submission.csv\", index = False)","76b198ca":"ret","30fa9dc0":"# Discourse_type_num to Discourse_type Mapping\n\n* ALl our predictions are discourse type num category. For final submission we should convert it into discourse type only\n","0e9ec07c":"# Append Predictions\n\n* We add a prediction from Model 1 to final submission **only** if model 2 thinks that this category should be present in the final output","ea3c2eff":"# Targets \/ Predictors\n\nWE train 2 types of models.\n\n    * First will predict the discourse_start and discourse_end for a category based on the tabular data\n    * Second will predict if the said category is actually present in the dataset\n\nIndeed there is no significant information given to the models so the performance cannot be the same as models which use textual & positional information for predictions.\n","5b777e7e":"# That's All Folks","0da8e741":"# Get Features from Text\n\n* Get and merge 2 kinds of features. \n    1. Textual features and splits and positions for training discourse_start and discourse_end predictor\n    2. File features against the probability of having a Lead1 or Lead2 present in the text","5cd927d9":"# Train\/Test Split\n\n* Clean splits based on file ids\n","699ad310":"# Now map discourse_type_num onto discourse_type\n","3c0567e8":"# Prepare Submission\n","0d81bb2f":"# Append Predictions to Submission\n\n* Append the predictions to our processed submission dataframe\n\n","96e39fe5":"---\n\n# Introduction\n\nThis is the naivest implementation considering that most items in the passages follow specific positional patterns.\n\nI am using this notebook to demonstrate that such an approach can be used as a final filter for submitting the predictions from the main DL models.\n\nThis notebook only uses RandomForest. No deep learning Or complicated processing. Enjoy !\n\n> In this version I changed the **RandomForest** to **GradientBoosting** model for prediction\n\n---\n","b7bf6e72":"# Explore Submission DataFrame Now\n","68c3f38a":"# Model 2 Train Adjustments\n\n    * For predicting if a type exists or not, use only file details as inputs\n    * Remove the discourse_type_num feature from predictors in this case\n\n","21f8902f":"# Discourse to Predictionstring\n\n* Almost same code as from organizers. (Just adds a fail-safe for zero len outputs)\n"}}