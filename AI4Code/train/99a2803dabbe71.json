{"cell_type":{"38f307a9":"code","0c25bf91":"code","b6773616":"code","dfa4c051":"code","29f9fa75":"code","d42663d7":"code","b2e8cca0":"code","d888e8a2":"code","c95fa97f":"code","59f8b3d1":"code","465f9c5d":"code","13a55f1d":"code","eabd20f8":"code","d8f50dd9":"code","e58c9751":"code","3b9efe2c":"code","b3e58c5a":"markdown","6323d61d":"markdown","04497b28":"markdown"},"source":{"38f307a9":"#!pip install tensorflow==2.3.0\n!pip install imutils\n!pip install python-telegram-bot #For training updates on Telegram\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.layers import Dropout, MaxPooling2D, AveragePooling2D, Dense, Flatten, Input, Conv2D, add, Activation\nfrom tensorflow.keras.layers import (Dense, Dropout, Activation, Flatten, Reshape, Layer,\n                          BatchNormalization, LocallyConnected2D,\n                          ZeroPadding2D, Conv2D, MaxPooling2D, Conv2DTranspose,AveragePooling2D,\n                          GaussianNoise, UpSampling2D, Input)\n\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.models import Sequential , Model , load_model\nfrom tensorflow.keras.preprocessing.image import load_img , img_to_array , ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport cv2\nfrom imutils import paths\nimport numpy as np\nimport os\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0c25bf91":"print(\"Tensorflow version: \",tf.__version__)","b6773616":"import requests\nimport tensorflow as tf\n\nimport tensorflow.keras.utils as np_utils\n\naccess_token = '' #Access token here\n\nclass botCallback(tf.keras.callbacks.Callback):\n    def __init__(self,access_token):\n        self.access_token = access_token\n        self.ping_url = 'https:\/\/api.telegram.org\/bot'+str(self.access_token)+'\/getUpdates'\n        self.response = requests.get(self.ping_url).json()\n        #print(self.response)\n        self.chat_id = self.response['result'][0]['message']['chat']['id']\n        #self.chat_id = self.response['result']\n\n    def send_message(self,message):\n        #print('sending message')\n        self.ping_url = 'https:\/\/api.telegram.org\/bot'+str(self.access_token)+'\/sendMessage?'+\\\n                        'chat_id='+str(self.chat_id)+\\\n                        '&parse_mode=Markdown'+\\\n                        '&text='+message\n        self.response = requests.get(self.ping_url)\n    \n    def send_photo(self,filepath):\n        imagefile= open(filepath,\"rb\")\n        file_dict = {'photo':imagefile}\n        self.ping_url = 'https:\/\/api.telegram.org\/bot'+str(self.access_token)+'\/sendPhoto?chat_id='+str(self.chat_id)\n        self.response = requests.post(self.ping_url, files = file_dict)\n        imagefile.close()\n\n    def on_train_batch_begin(self, batch, logs=None):\n        pass\n    \n    def on_train_batch_end(self, batch, logs=None):\n        message = ' Iteration\/Batch {}\\n Training Accuracy : {:7.2f}\\n Training Loss : {:7.2f}\\n'.format(batch,logs['accuracy'],logs['loss'])\n        #print(logs)\n        try:\n            message += ' Validation Accuracy : {:7.2f}\\n Validation Loss : {:7.2f}\\n'.format(logs['val_accuracy'],logs['val_loss'])\n            self.send_message(message)\n        except:\n            pass\n\n    def on_test_batch_begin(self, batch, logs=None):\n        pass\n    \n    def on_test_batch_end(self, batch, logs=None):\n        message = ' Iteration\/Batch {}\\n Training Accuracy : {:7.2f}\\n Training Loss : {:7.2f}\\n'.format(batch,logs['accuracy'],logs['loss'])\n        try:\n            message += ' Validation Accuracy : {:7.2f}\\n Validation Loss : {:7.2f}\\n'.format(logs['val_accuracy'],logs['val_loss'])\n            self.send_message(message)\n        except:\n            pass\n\n    def on_epoch_begin(self, epoch, logs=None):\n        pass\n\n    def on_epoch_end(self, epoch, logs=None):\n\n        message = ' Epoch {}\\n Training Accuracy : {:7.2f}\\n Training Loss : {:7.2f}\\n'.format(epoch,logs['accuracy'],logs['loss'])\n        try:\n            message += ' Validation Accuracy : {:7.2f}\\n Validation Loss : {:7.2f}\\n'.format(logs['val_accuracy'],logs['val_loss'])\n            self.send_message(message)        \n        except:\n            pass\n\nclass Plotter(botCallback):\n def __init__(self,access_token):\n    \n     super().__init__(access_token)\n def on_train_begin(self,logs=None):\n     self.batch = 0\n     self.epoch = []\n     self.train_loss = []\n     self.val_loss = []\n     self.train_acc = []\n     self.val_acc = []\n     self.fig = plt.figure(figsize=(200,100))\n     self.logs = []\n\n def on_epoch_end(self, epoch, logs=None):\n     self.logs.append(logs)\n     self.epoch.append(epoch)\n     self.train_loss.append(logs['loss'])\n     self.val_loss.append(logs['val_loss'])\n     self.train_acc.append(logs['accuracy'])\n     self.val_acc.append(logs['val_accuracy'])\n     f,(ax1,ax2) = plt.subplots(1,2,sharex=True)\n     #clear_output(wait=True)\n     ax1.plot(self.epoch, self.train_loss, label='Training Loss')\n     ax1.plot(self.epoch, self.val_loss, label='Validation Loss')\n     ax1.legend()\n     ax2.plot(self.epoch, self.train_acc, label='Training Accuracy')\n     ax2.plot(self.epoch, self.val_acc, label='Validation Accuracy')\n     ax2.legend()\n     plt.savefig('Accuracy and Loss plot.jpg')\n     self.send_photo('Accuracy and Loss plot.jpg')","dfa4c051":"\ntrain_dir = '\/kaggle\/input\/soil-classification-image-data\/Soil_Dataset\/Train'\ntest_dir = '\/kaggle\/input\/soil-classification-image-data\/Soil_Dataset\/Test'\n\nimage_size = 224","29f9fa75":"batch_size = 32\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                            rotation_range=45,\n                            zoom_range=0.40,\n                            width_shift_range=0.2,\n                            height_shift_range=0.2,\n                            shear_range=0.15,\n                            horizontal_flip=True,\n                            vertical_flip= True,\n                            fill_mode=\"nearest\")\n\ntrain_data = train_datagen.flow_from_directory(train_dir,\n                                              target_size=(150,150),\n                                              batch_size=32,\n                                              class_mode=\"categorical\")\n","d42663d7":"test_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntest_data = test_datagen.flow_from_directory(test_dir,\n                                            target_size=(150,150),\n                                            batch_size=32,\n                                            class_mode=\"categorical\")","b2e8cca0":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n                                                              validation_split = 0.2,\n                                                              subset = \"training\",\n                                                              seed = 42,\n                                                              image_size = (150,150),\n                                                              batch_size = 40)\n\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n                                                             validation_split = 0.2,\n                                                             subset = \"validation\",\n                                                             seed = 42,\n                                                             image_size = (150,150),\n                                                             batch_size = 40)","d888e8a2":"## Configuring dataset for performance\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ntraining_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\ntesting_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)","c95fa97f":"\nmodel = Sequential(name=\"SoilNet\")\nmodel.add(Conv2D(64,(3,3),activation = \"relu\",padding =\"same\",kernel_initializer=\"he_normal\", input_shape=(150,150,3)))\n#model.add(tf.keras.layers.LeakyReLU())\n#model.add(BatchNormalization())\nmodel.add(Conv2D(64,(3,3),activation = \"relu\",padding =\"same\",kernel_initializer=\"he_normal\"))\n#model.add(tf.keras.layers.LeakyReLU())\nmodel.add(BatchNormalization())\n\nmodel.add(AveragePooling2D(pool_size = (2,2), strides=2))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(128,(3,3),activation = \"relu\",padding =\"same\",kernel_initializer=\"he_normal\"))\n#model.add(tf.keras.layers.LeakyReLU())\n#model.add(BatchNormalization())\nmodel.add(Conv2D(128,(3,3),activation = \"relu\",padding =\"same\",kernel_initializer=\"he_normal\"))\n#model.add(tf.keras.layers.LeakyReLU())\nmodel.add(BatchNormalization())\n\nmodel.add(AveragePooling2D(pool_size = (2,2), strides=2))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\n#lk = tf.keras.layers.LeakyReLU()\nmodel.add(Conv2D(256,(3,3),activation = \"relu\", padding =\"same\",kernel_initializer=\"he_normal\"))\n#model.add(BatchNormalization())\nmodel.add(Conv2D(256,(3,3),activation = \"relu\",padding =\"same\",kernel_initializer=\"he_normal\"))\n#model.add(tf.keras.layers.LeakyReLU())\nmodel.add(BatchNormalization())\n\nmodel.add(AveragePooling2D(pool_size = (2,2), strides=2))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation=\"relu\"))\nmodel.add(Dropout(0.7))\nmodel.add(Dense(4,activation=\"softmax\"))\n\nopt = RMSprop(learning_rate = 0.0001, rho = 0.99, epsilon = 1e-08, decay = 0.0)\nmodel.compile(optimizer=opt,loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n\nreduction_lr = ReduceLROnPlateau(monitor = \"val_accuracy\",patience = 2 ,verbose = 1, factor = 0.3, min_lr = 0.0000001)\nreduction_lr1 = ReduceLROnPlateau(monitor = \"val_loss\",patience = 2 ,verbose = 1, factor = 0.3, min_lr = 0.0000001)","59f8b3d1":"#bot_callback = botCallback(access_token)\n#plotter = Plotter(access_token)\n#callback_list = [bot_callback,plotter] callbacks=callback_list\n\nstart = time.time()\n\nhistory = model.fit_generator(train_data,\n                    validation_data = test_data,\n                    epochs=20,\n                    callbacks = [reduction_lr,reduction_lr1])\nend = time.time()\nprint(\"Total train time: \",(end-start)\/60,\" mins\")\n","465f9c5d":"\"\"\"\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.experimental.preprocessing.Rescaling(1.\/255),\n  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Conv2D(64, 3, activation='relu'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Conv2D(128, 3, activation='relu'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(256, activation='relu'),\n  tf.keras.layers.Dense(2, activation= 'softmax')\n])\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n\"\"\"","13a55f1d":"\"\"\"\n#=================================================================\nchanDim = 1\nmodel = Sequential(name=\"SoilNet\")\nmodel.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=(150,150,3)))\nmodel.add(Activation(\"relu\"))\n\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (4, 4), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\n\nmodel.add(Conv2D(64, (4, 4), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, (4, 4), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\n\nmodel.add(Conv2D(128, (4, 4), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4))\nmodel.add(Activation(\"softmax\"))\n\n\nmodel.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\nreduction_lr = ReduceLROnPlateau(monitor = \"val_accuracy\",patience = 2 ,verbose = 1, factor = 0.2, min_lr = 0.00001)\ncallback_list = [reduction_lr]\nmodel.summary()\nplot_model(model,show_shapes=True)\n\"\"\"\n","eabd20f8":"\"\"\"\nhistory = model.fit(train_ds,\n                    validation_data = test_ds,\n                    epochs=5)\n\"\"\"","d8f50dd9":"def plot_graph(history,string):\n    plt.figure(figsize=(12,8))\n    plt.plot(history.history[string],label=str(string))\n    plt.plot(history.history[\"val_\"+str(string)],label=\"val_\"+str(string))\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(str(string))\n    plt.legend()\n    plt.show()\nplot_graph(history,\"accuracy\")\nplot_graph(history,\"loss\")","e58c9751":"model.save(\"SoilNet.h5\")","3b9efe2c":"from IPython.display import FileLink\nFileLink('SoilNet.h5')\n","b3e58c5a":"# **Creating class for Telegram Bot Message Updates along with Graph**\n\nFor complete guidanc and example of making Telegram bot visit here:<br>\nhttps:\/\/github.com\/OMIII1997\/Telegram-Bot-for-Model-Training-Updates <br>\n\nGet access_token from Telegram app: \n* Open Telegram mobile app \n* Search for \"BotFather\" \n* Send \"\/start\"\n* After reply from BotFather send \"\/newbot\"\n* Give name to your Bot Eg: Example_Bot\n* Give username to your Bot Eg: My_Example_bot *Note: User name must end with '_bot'* \n* Done...Congratulations You have crated your own Telegram bot. Now you will get Token to access the HTTP API. Copy that Token Key.","6323d61d":"![](https:\/\/wallpaperaccess.com\/full\/803470.jpg)","04497b28":"# Soil-Type-Classification for Crops Suggestion \ud83c\udf33\ud83c\udf32\ud83c\udf84\ud83c\udf8b\ud83c\udf34\n<br>Problem Statement: Classifying the type of the Soil from Input Image.\n# Model deployed as Web-Application API at:\ud83c\udfaf\ud83d\udd17\ud83d\udcf3  <br>\nhttps:\/\/soilnet.herokuapp.com\/\n\n# GitHub Repo <br>\nhttps:\/\/github.com\/OMIII1997\/Soil-Type-Classification \n\n# Dataset available at: \ud83d\udcda\ud83d\udcd3\ud83d\uddde\ud83d\udcbe <br>\nhttps:\/\/www.kaggle.com\/omkargurav\/soil-classification-image-data\n<br>\n\n\nFor this project a deep learning model is trained with 903 images of four different types soil. \"Alluvial\", \"Black\", \"Clay\" , \"Red\". All images are collected from Google Search Engine  and crafted and filtered. \n<br>\n\nBased on the type of the Soil Crops will be suggested. Model is deployed on Heroku Platform\n\n# Acknoledgement: \ud83c\udf93\ud83d\udca1\ud83c\udff7\ud83d\udd16\ud83d\udccc  <br>\nI am expressing my gratitude towards Sir Krish Naik for his super clear explanation about Neural Network in\n Deep Learning Playlist and Model Deployment Tutorial on YouTube.\n\nI am also thankful to Sir Akash Zade for his model deployment explanation at AI in Agri Playlist.\n\n\n# Click on Below Link for Project Demo: \ud83d\ude0d\ud83d\udc40\ud83d\udd0d\ud83d\udcfa\ud83d\udce1 <br>\nhttps:\/\/youtu.be\/gnKmbgbPRJA\n\n# Sample:\ud83d\udda5\ud83d\udda8\u2714\ud83d\uddbc\ud83d\udcf7 <br>\n![Image](https:\/\/soilnet.herokuapp.com\/static\/css\/images\/Clay_2.jpg)"}}