{"cell_type":{"05d57a76":"code","a1d7c3c9":"code","0adbae60":"code","d8776fb7":"code","b96f894f":"code","4ed0b7cd":"code","2750ce1a":"code","26e9346f":"code","7b9138d8":"code","92bc853c":"code","571a6ccf":"code","f07d8da3":"code","8df28601":"code","6437e92a":"code","5f33e3fe":"code","579e8c48":"code","e72554b8":"code","e639aa2f":"code","226b3a80":"code","d895123c":"code","09443c2f":"code","fa199d86":"code","41c5855b":"code","154d48d2":"code","369d12ea":"code","9f3f0feb":"code","21a18e92":"code","4fbab693":"code","0f4e6104":"code","c45b76f3":"code","85460e28":"code","8259e776":"code","5167dbe5":"code","8432f151":"code","ca2d078b":"code","aac156d4":"code","27e3592a":"code","44e7c6a5":"code","19430452":"code","c65ba619":"markdown","4c07b64a":"markdown","a0a81f9b":"markdown","041c920e":"markdown","96b918ea":"markdown","09c32247":"markdown","7d72250f":"markdown","4340525e":"markdown","6c6c3563":"markdown","56c8627a":"markdown","369d2001":"markdown","b7ff7893":"markdown","64743d0c":"markdown","876ea7d1":"markdown","b1672121":"markdown","2ecc07ed":"markdown","ba5551d3":"markdown","97f8e539":"markdown","8b8e0bc4":"markdown","19d8508e":"markdown","9715531d":"markdown","af702b01":"markdown","3ae997fd":"markdown"},"source":{"05d57a76":"import numpy as np\nimport pandas as pd\nimport keras\n\n# \u53ef\u89c6\u5316\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score, precision_score, f1_score, recall_score\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1d7c3c9":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndata = [train_data, test_data]","0adbae60":"train_data.columns\n# Survived\u662f\u76ee\u6807","d8776fb7":"train_data.describe()","b96f894f":"train_data.head()","4ed0b7cd":"train_data.groupby(['Survived'], as_index=False)['Survived'].count()\n\n# \u6b63\u8d1f\u6837\u672c\u6bd4\u4f8b\u662f 5:3","2750ce1a":"# PassengerId\ntrain_data.PassengerId.head\n# \u57fa\u672c\u6ca1\u4ec0\u4e48\u7528","26e9346f":"# Pclass\nprint(train_data[['Pclass', 'Survived']].groupby(['Pclass']).count())\ntrain_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# \u53ef\u4ee5\u770b\u5230pcalss(\u8239\u8231\u7b49\u7ea7)\u548c\u662f\u5426survived\u6709\u975e\u5e38\u660e\u663e\u7684\u5173\u7cfb","7b9138d8":"# Sex\nprint(train_data[['Sex', 'Survived']].groupby(['Sex']).count())\ntrain_data[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# \u53ef\u4ee5\u770b\u5230\u6027\u522b\u548c\u662f\u5426survived\u6709\u975e\u5e38\u660e\u663e\u7684\u5173\u7cfb","92bc853c":"# age\ng = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","571a6ccf":"# sibSp\nprint(train_data[['SibSp', 'Survived']].groupby(['SibSp']).count())\ntrain_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f07d8da3":"# parch\n\nprint(train_data[['Parch', 'Survived']].groupby(['Parch']).count())\ntrain_data[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8df28601":"# ticket \nprint(set(train_data.Ticket.values))\n\ndef get_ticket_prefix(x):\n    li = x.split(' ')\n    if len(li) > 1:\n        return li[0]\n    else:\n        return None\n\ntrain_data['TicketPrefix'] = train_data.Ticket.map(get_ticket_prefix)\nprint(set(train_data.TicketPrefix.values))\nprint(len(train_data))\nprint(train_data['TicketPrefix'].count())\ntrain_data[[\"TicketPrefix\", \"Survived\"]].groupby(['TicketPrefix'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\n\n# ticket\u4fe1\u606f\u6709\u4e9b\u6742\u4e71\uff0c\u8986\u76d6\u7387\u8f83\u4f4e\uff0c\u6ca1\u6709\u770b\u51fa\u548c\u662f\u5426survived\u6709\u660e\u663e\u5173\u7cfb\uff0c\u56e0\u6b64\u8fd9\u91cc\u6401\u7f6e\u4e0d\u7528","6437e92a":"# Fare\ng = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Fare', bins=20)\n# \u6709\u76f8\u5173\u6027","5f33e3fe":"# cabin\nprint(set(train_data.Cabin.values))\nprint(train_data.Cabin.count(),len(train_data))\ntrain_data[[\"Cabin\", \"Survived\"]].groupby(['Cabin'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# \u53ef\u4ee5\u770b\u5230Cabin\uff08\u8231\u5ba4\uff09\u8986\u76d6\u7387\u4e0d\u9ad8\uff0c\u5e76\u4e14\u548c\u662f\u5426survived \u5e76\u65e0\u660e\u663e\u7684\u5173\u7cfb\uff0c \u56e0\u6b64\u8fd9\u91cc\u4e0d\u4f7f\u7528\u8be5\u7279\u5f81","579e8c48":"# Embarked\n\nprint(train_data[['Embarked', 'Survived']].groupby(['Embarked']).count())\ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e72554b8":"train_data.Name.head()","e639aa2f":"for dataset in data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(train_data['Title'], train_data['Sex'])\n","226b3a80":"for dataset in data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n# \u8fd9\u91cc\u4f7f\u7528name 1.\u662f\u4e3a\u4e86\u5c55\u793a\u6784\u9020\u7279\u5f81 2.\u662ftitle\u5e76\u4e0d\u662f\u5b8c\u5168\u7b49\u4e8eSex","d895123c":"# \u987a\u4fbf\u505a\u4e00\u4e0b\u9884\u5904\u7406\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data.head()","09443c2f":"train_data = train_data.drop(['TicketPrefix', 'Ticket', 'Cabin','Name', 'PassengerId'], axis=1)\ntest_data = test_data.drop(['Ticket', 'Cabin', 'Name'], axis=1)\n\ndata = [train_data,test_data]","fa199d86":"train_data.head()","41c5855b":"# Pclass \nprint(set(train_data['Pclass'].values),set(test_data['Pclass'].values))\n\n\n# \u65e0\u9700\u5f02\u5e38\u503c\/\u7f3a\u5931\u503c\u5904\u7406\n# Q1:\u662f\u5206\u6876\u8fd8\u662f\u76f4\u63a5\u7528\uff1f\ndef process_Pclass_toint(data):\n    for dataset in data:\n        dataset['PclassFeature'] = dataset['Pclass'].astype('int')\n        \ndef process_Pclass_by_bucketing(data):\n    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n    one_hot_encoder.fit(train_data.Pclass.values.reshape(-1,1))\n    for dataset in data:\n        dataset['PclassFeature'] = one_hot_encoder.transform(dataset.Pclass.values.reshape(-1,1)).todense().astype('int').tolist()\n#process_Pclass_toint(data)\n#train_data.head()\nprocess_Pclass_by_bucketing(data)\ntrain_data.head()","154d48d2":"print(set(train_data['Sex'].values),set(test_data['Sex'].values))\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map( lambda x:{'female': 1, 'male': 0}.get(x, x) )\n# \u65e0\u9700\u5f02\u5e38\u503c\/\u7f3a\u5931\u503c\u5904\u7406","369d12ea":"\ndef process_Sex_toint(data):\n    for dataset in data:\n        dataset['SexFeature'] = dataset['Sex'].astype('int')\n\ndef process_Sex_by_bucketing(data):\n    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n    one_hot_encoder.fit(train_data.Sex.values.reshape(-1,1))\n    for dataset in data:\n        dataset['SexFeature'] = one_hot_encoder.transform(dataset.Sex.values.reshape(-1,1)).todense().astype('int').tolist()\n\n\n# process_Sex_toint(data)\n# train_data.head()\nprocess_Sex_by_bucketing(data)\n#train_data.head()","9f3f0feb":"train_data.head()","21a18e92":"# \u770b\u4e00\u4e0bage\u7684\u5206\u5e03\u60c5\u51b5\nmedian = train_data.Age.dropna().median()\nmean = train_data.Age.dropna().mean()\nprint(median, mean)\n\n\n# age\u7684\u5206\u5e03\u662f\u5426\u548c\u5176\u4ed6\u6761\u4ef6\u6709\u5173\ngrid = sns.FacetGrid(train_data, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()\n\n\n## \u7f3a\u5931\u503c\u8865\u5145\ndef complete_age_by_median(data):\n    for dataset in data:\n        dataset['AgeFeature'] = dataset.Age.fillna(median)\n\ndef complete_age_by_mean(data):\n    for dataset in data:\n        dataset['AgeFeature'] = dataset.Age.fillna(mean).astype(int)\n\ndef complete_age_by_distribution(data):\n    # grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\n    guess_ages = dict()\n    def fillna_age(x):\n        if x.Age >= 0:\n            return x.Age\n        else:\n            return guess_ages[(x.Sex, x.Pclass)]\n    for dataset in data:\n        for i in range(0, 2):\n            for j in range(0, 3):\n                guess_df = train_data[(train_data['Sex'] == i) & \\\n                                  (train_data['Pclass'] == j+1)]['Age'].dropna()\n\n                # age_mean = guess_df.mean()\n                # age_std = guess_df.std()\n                # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n                age_guess = guess_df.median()\n                guess_ages[(i,j+1)] = age_guess\n        dataset['AgeFeature'] = dataset.apply(lambda x:fillna_age(x), axis=1)\n            \n\n    \n    \n\n# \u7279\u5f81\u5904\u7406\ndef process_Age_toint(data):\n    for dataset in data:\n        dataset['AgeFeature'] = dataset['AgeFeature'].astype('int')\n        \n\ndef process_Age_by_bucketing(data):\n    for dataset in data:\n        dataset['AgeFeature'] = dataset['AgeFeature'].map(lambda x:int(x\/16))\n    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n    one_hot_encoder.fit(train_data.AgeFeature.values.reshape(-1,1))\n    for dataset in data:\n        dataset['AgeFeature'] = one_hot_encoder.transform(dataset.AgeFeature.values.reshape(-1,1)).todense().astype('int').tolist()\ncomplete_age_by_distribution(data)\nprocess_Age_by_bucketing(data)\ntrain_data.head()\ntrain_data[train_data.Age.isna()]","4fbab693":"print(set(train_data.SibSp.values), set(test_data.SibSp.values))\n# {0, 1, 2, 3, 4, 5, 8} {0, 1, 2, 3, 4, 5, 8}\n# \u65e0\u9700\u7f3a\u5931\u503c\u5904\u7406","0f4e6104":"\ndef process_SibSp_toint(data):\n    for dataset in data:\n        dataset['SibSpFeature'] = dataset['SibSp'].astype('int')\n\ndef process_SibSp_by_bucketing(data):\n    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n    one_hot_encoder.fit(train_data.SibSp.values.reshape(-1,1))\n    for dataset in data:\n        dataset['SibSpFeature'] = one_hot_encoder.transform(dataset.SibSp.values.reshape(-1,1)).todense().astype('int').tolist()\n\n#process_SibSp_toint(data)\n# train_data.head()\nprocess_SibSp_by_bucketing(data)\ntrain_data.head()","c45b76f3":"print(set(train_data.Parch.values), set(test_data.Parch.values))\n# {0, 1, 2, 3, 4, 5, 6} {0, 1, 2, 3, 4, 5, 6, 9}\n# \u65e0\u9700\u7f3a\u5931\u503c\u5904\u7406","85460e28":"\ndef process_Parch_toint(data):\n    for dataset in data:\n        dataset['ParchFeature'] = dataset['Parch'].astype('int')\n\ndef process_Parch_by_bucketing(data):\n    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n    one_hot_encoder.fit(train_data.Parch.values.reshape(-1,1))\n    for dataset in data:\n        dataset['ParchFeature'] = one_hot_encoder.transform(dataset.Parch.values.reshape(-1,1)).todense().astype('int').tolist()\n\n#process_Parch_toint(data)\n# train_data.head()\n\nprocess_Parch_by_bucketing(data)\ntrain_data.head()","8259e776":"\ntrain_data['FareBand'] = pd.qcut(train_data['Fare'], 4)\nprint(train_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True))\n\ndef completing_fare_by_median(data):\n    for dataset in data:\n        median = dataset.dropna().median()\n        dataset['FareFeature'] = dataset['Fare'].fillna(median)\n\ndef bucket_func(fare):\n    if fare < 7.91:\n        return 0\n    elif fare < 14.454:\n        return 1\n    elif fare < 31.0:\n        return 2\n    else:\n        return 3\n\ndef process_fare_by_bucketing(data):\n    for dataset in data:\n        dataset['FareFeature'] = dataset.FareFeature.map(bucket_func)\n    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n    one_hot_encoder.fit(train_data.FareFeature.values.reshape(-1,1))\n    for dataset in data:\n        dataset['FareFeature'] = one_hot_encoder.transform(dataset.FareFeature.values.reshape(-1,1)).todense().astype('int').tolist()\n\n    \ncompleting_fare_by_median(data)\nprocess_fare_by_bucketing(data)\ntrain_data.head()","5167dbe5":"# \u5904\u7406embarked  \u7279\u5f81\nprint(set(train_data['Embarked'].values)) # {'S', 'Q', 'C', nan}\n# \u95ee\u9898\uff1a\u5982\u4f55\u8865\u5168\u7279\u5f81\uff1a1.\u7f3a\u7701\u503c 2.\u5e38\u89c1\u503c 3.\u5747\u503c\uff08\u6216\u8005\u6309\u7167\u67d0\u79cd\u5206\u5e03\u91c7\u6837\u5f97\u5230\u7684\u503c\uff09\n# \u8fd9\u91cc\u6211\u4eec\u7528\u6700\u5e38\u89c1\u503c\u8865\u5145\ndef complete_embarked_with_most_freq(data):\n    freq_port = train_data.Embarked.dropna().mode()[0]\n    for dataset in data:\n        dataset['EmbarkedFeature'] = dataset['Embarked'].fillna(freq_port)\n    \n\n# \u7279\u5f81\u5904\u7406\ndef process_embarked_by_bucketing(data):\n    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n    one_hot_encoder.fit(train_data.EmbarkedFeature.values.reshape(-1,1))\n    for dataset in data:\n        dataset['EmbarkedFeature'] = one_hot_encoder.transform(dataset.EmbarkedFeature.values.reshape(-1,1)).todense().astype('int').tolist()\n\n\ncomplete_embarked_with_most_freq(data)\nprocess_embarked_by_bucketing(data)\ntrain_data.head()","8432f151":"print(set(train_data.Title.values))# {1, 2, 3, 4, 5}\n# \u65e0\u9700\u8865\u5168\u7f3a\u5931\u503c\n# \u7279\u5f81\u5904\u7406\ndef process_title_by_bucketing(data):\n    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n    one_hot_encoder.fit(train_data.Title.values.reshape(-1,1))\n    for dataset in data:\n        dataset['TitleFeature'] = one_hot_encoder.transform(dataset.Title.values.reshape(-1,1)).todense().astype('int').tolist()\n\nprocess_title_by_bucketing(data)\ntrain_data.head()","ca2d078b":"def train_model(model, train_x, train_y):\n    model.fit(train_x, train_y)\n    pred_y = model.predict(train_x)\n    print('precision score', precision_score(train_y, pred_y))\n    print('recall score', recall_score(train_y, pred_y))\n    print('f1 score', f1_score(train_y,pred_y))\n    return model","aac156d4":"print(train_data.head())\nprint(test_data.head())","27e3592a":"## Pclass\n### \u7279\u5f81\u5904\u7406\uff1a\n#process_Pclass_by_bucketing(data)\nprocess_Pclass_toint(data)\n\n## Sex\n### \u7279\u5f81\u5904\u7406\nprocess_Sex_toint(data)\n#process_Sex_by_bucketing(data)\n\n## Age \n### \u7f3a\u5931\u503c\u5904\u7406\n#complete_age_by_median(data)\ncomplete_age_by_mean(data)\n#complete_age_by_distribution(data)\n###\u7279\u5f81\u5904\u7406\nprocess_Age_toint(data)\n#process_Age_by_bucketing(data)\n\n## SibSp\n### \u7279\u5f81\u5904\u7406\nprocess_SibSp_toint(data)\n#process_SibSp_by_bucketing(data)\n\n# Parch\n### \u7279\u5f81\u5904\u7406\nprocess_Parch_toint(data)\n#process_Parch_by_bucketing(data)\n\n## Fare\n### \u7f3a\u5931\u503c\u5904\u7406\ncompleting_fare_by_median(data)\n### \u7279\u5f81\u5904\u7406\nprocess_fare_by_bucketing(data)\n\n\n## Embarked\n### \u7f3a\u5931\u503c\u5904\u7406\ncomplete_embarked_with_most_freq(data)\n### \u7279\u5f81\u5904\u7406\nprocess_embarked_by_bucketing(data)\n\n## Title\n### \u7279\u5f81\u5904\u7406\nprocess_title_by_bucketing(data)\n\n\n## \u8bad\u7ec3\u6a21\u578b\n\nfeatures = ['PclassFeature', 'SexFeature', 'SibSpFeature', 'ParchFeature',\n       'AgeFeature', 'FareFeature', 'EmbarkedFeature',\n       'TitleFeature']\nlabel = ['Survived']\nprint(train_data.head())\ntrain_x, train_y = train_data[features].values, train_data[label].values\n\ntest_x = test_data[features].values\n\nlr_model = LogisticRegression()\n\ntrain_x = [np.hstack(a) for a in train_x]\ntest_x = [np.hstack(a) for a in test_x]\n\nprint(train_x[0].shape, test_x[0].shape)\nmodel = train_model(lr_model, train_x, train_y)\nprint(model.coef_.shape)","44e7c6a5":"pred_y = model.predict(test_x)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": pred_y\n    })\nsubmission.to_csv('..\/output\/submission.csv')","19430452":"# \u7279\u5f81\u5904\u7406\n\n## Pclass\n### \u7279\u5f81\u5904\u7406\uff1a\n#process_Pclass_by_bucketing(data)\n#process_Pclass_toint(data)\n\n## Sex\n### \u7279\u5f81\u5904\u7406\n# process_Sex_toint(data)\n# process_Sex_by_bucketing(data)\n\n## Age \n### \u7f3a\u5931\u503c\u5904\u7406\ncomplete_age_by_mean(data)\ncomplete_age_by_mean(data)\ncomplete_age_by_distribution(data)\n###\u7279\u5f81\u5904\u7406\nprocess_Age_toint(data)\nprocess_Age_by_bucketing(data)\n\n## SibSp\n### \u7279\u5f81\u5904\u7406\nprocess_SibSp_by_bucketing(data)\nprocess_SibSp_toint(data)\n\n# Parch\n### \u7279\u5f81\u5904\u7406\nprocess_Parch_by_bucketing(data)\nprocess_Parch_toint(data)\n\n## Fare\n### \u7f3a\u5931\u503c\u5904\u7406\ncompleting_fare_by_median(data)\n### \u7279\u5f81\u5904\u7406\nprocess_fare_by_bucketing(data)\n\n\n## Embarked\n### \u7f3a\u5931\u503c\u5904\u7406\ncomplete_embarked_with_most_freq(data)\n### \u7279\u5f81\u5904\u7406\nprocess_embarked_by_bucketing(data)\n\n## Title\n### \u7279\u5f81\u5904\u7406\nprocess_title_by_bucketing(data)\n\n\n## \u8bad\u7ec3\u6a21\u578b\n\nfeatures = ['PclassFeature', 'SexFeature', 'SibSpFeature', 'ParchFeature',\n       'AgeFeature', 'FareFeature', 'EmbarkedFeature',\n       'TitleFeature']\nlabel = ['Survived']\nprint(train_data.columns)\nprint(test_data.columns)\ntrain_x, train_y = train_data[features].values.reshape(len(train_data), -1), train_data[label]\ntest_x = test_data[features]\n\nlr_model = LogisticRegression()\n\ntrain_x = [np.hstack(a) for a in train_x]\ntest_x = [np.hstack(a) for a in test_x]\nprint(train_x[0])\nprint(np.hstack(train_x[1]))\n\ntrain_model(lr_model, train_x, train_y)","c65ba619":"# part2\u603b\u7ed3\uff1a\n1. \u8865\u5168\u7f3a\u5931\u503c\n2. \u5904\u7406\u8fde\u7eed\u7279\u5f81\n3. \u5904\u7406\u79bb\u6563\u7279\u5f81","4c07b64a":"# part4:\u80fd\u4e0d\u80fd\u505a\u7684\u66f4\u597d","a0a81f9b":"# part1:\u603b\u7ed3\uff1a\n1. \u786e\u8ba4\u54ea\u4e9b\u7279\u5f81\u5185\u5bb9\uff08\u8003\u8651\u5230\u8986\u76d6\u7387\uff0c\u76f8\u5173\u7a0b\u5ea6\uff09\n2. \u628a\u4e0d\u597d\u76f4\u63a5\u7528\u7684\u7279\u5f81 \u8f6c\u5316\u6210\u65b0\u7279\u5f81","041c920e":"## \u63d0\u4ea4\u4e00\u4e0b","96b918ea":"## \u7279\u5f81\u5206\u5e03 \u548c \u7ed3\u679c\u76f8\u5173\u6027","09c32247":"## fare\u7279\u5f81","7d72250f":"### SibSp, ParCh\u7279\u5f81","4340525e":"### embarked\u7279\u5f81","6c6c3563":"## \u6837\u672c\u5206\u5e03","56c8627a":"### Sex\u7279\u5f81","369d2001":"# part3:\u6a21\u578b\u8bad\u7ec3\uff08\u8fd9\u91cc\u7528LR\u6a21\u578b\u505a\u5c55\u793a\uff09","b7ff7893":"# part1\uff1a\u6570\u636e\u6982\u89c8\uff0c\u51b3\u5b9a\u4f7f\u7528\u54ea\u4e9b\u7279\u5f81","64743d0c":"### age \u7279\u5f81 \u8fde\u7eed\u578b\u7279\u5f81","876ea7d1":"## \u5982\u4f55\u8ba9\u6a21\u578b\u53d8\u5f97\u66f4\u5389\u5bb3\n1. \u8c03\u53c2 \n1. \u6362\u66f4\u5f3a\u5927\u7684\u6a21\u578b\n2. emsemble \u4e0d\u540c\u6a21\u578b\u4e4b\u524d\u7684\u914d\u5408\n","b1672121":"# part-1: \u4ec0\u4e48\u53eb\u6a21\u578b\u8bad\u7ec3","2ecc07ed":"## \u8bfb\u53d6\u6570\u636e","ba5551d3":"## name\u7279\u5f81\u5355\u8bb2\uff08\u6784\u9020\u7279\u5f81\uff09","97f8e539":"## \u5bfc\u5165\u4e00\u4e9b\u5fc5\u8981\u7684\u5305","8b8e0bc4":"# part2:\u7279\u5f81\u5904\u7406","19d8508e":"### title \u7279\u5f81","9715531d":"# part0:\u51c6\u5907\u9636\u6bb5","af702b01":"![\u88c5\u903c\u7ed3\u675f](http:\/\/ww1.sinaimg.cn\/large\/9150e4e5gw1facqnzagffj205i05i745.jpg)","3ae997fd":"## \u5148\u770b\u770b\u6570\u636e\u957f\u4ec0\u4e48\u6837"}}