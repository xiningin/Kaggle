{"cell_type":{"57d09f91":"code","988cc2d3":"code","10c05cb2":"code","1784cbbd":"code","93f78cfa":"code","ee06e10b":"code","a4dfa04c":"code","bf4959a0":"code","876c57ce":"code","b629f5f6":"code","33a85349":"code","6630e4bb":"code","9419f245":"code","05124ec9":"code","2c00d4a0":"code","8542b470":"code","50f23f5a":"code","75c8e00d":"code","3d83865b":"code","133109d9":"code","0483465f":"code","3e74f5a0":"code","b599c7bb":"code","12586b7d":"code","89f1a6f3":"code","8c1ca882":"code","b16c7798":"code","00369a75":"code","26734546":"code","d4e57580":"code","9b9b5215":"code","f2b81af3":"code","c846df8a":"code","10c8d3bf":"code","bacec126":"code","142694b9":"code","70b8a356":"code","0cc4d39f":"code","e18b3b99":"code","7ef17db2":"code","11fc2f9b":"code","bd391d30":"code","20c48397":"markdown","96f0c57b":"markdown","b3d656a7":"markdown","8aeb960a":"markdown","bac62d64":"markdown","19ba89bd":"markdown","aa59f701":"markdown","58f8ecc8":"markdown","36b18113":"markdown","9b65b551":"markdown","feb86b12":"markdown","1daca755":"markdown","e7680dc7":"markdown","988106f6":"markdown","31a13fdf":"markdown","72cdee64":"markdown","e403ba3f":"markdown","26cb8193":"markdown","b30c3c23":"markdown","ab71261e":"markdown","ca67ded3":"markdown","d9956290":"markdown","f7b81183":"markdown"},"source":{"57d09f91":"# Import essential libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\n\n# Set my favourite matplotlib style\n\nmpl.style.use('ggplot')","988cc2d3":"# Define a parsing function and the date format to detect\n\ndateparse = lambda x: pd.datetime.strptime(x, '%m\/%d\/%Y')\n\n# Load and parse a Date column simultaneously\n\ndf = pd.read_csv('..\/input\/clmbwth\/climbing_statistics.csv', parse_dates=['Date'], date_parser=dateparse)\ndf.sort_values('Date').head(4)","10c05cb2":"# Now, load Rainier_Weather dataset and parse a Date column as previously\n\nwth = pd.read_csv('..\/input\/clmbwth\/Rainier_Weather.csv', parse_dates=['Date'], date_parser=dateparse)\nwth.sort_values('Date').head(4)","1784cbbd":"wth.describe()","93f78cfa":"df.describe(include='all')","ee06e10b":"# Merge climbing and weather datasets on the Date columns\n\ndfm = df.merge(wth, on='Date')","a4dfa04c":"# Then, sort Date values ascending and reset index\n\ndfm.sort_values('Date', inplace=True)\ndfm.reset_index(drop=True, inplace=True)\ndfm.head(3)","bf4959a0":"dfm.info()","876c57ce":"# Lets see histograms for all numeric features\n\ndf_countable = dfm._get_numeric_data()\ndf_countable.hist(bins = 50, figsize=(20,15))\nplt.show()","b629f5f6":"# Now plot a correlation matrix:\n\ncorr = df_countable.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(10, 9))\n\ncmap = sns.diverging_palette(210, 5, as_cmap=True)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.1, cbar_kws={\"shrink\": .8})\nplt.title('Correlation Matrix for the combined dataset')\n\nplt.show()","33a85349":"dfm.describe(include='all')","6630e4bb":"# Let's first rename the target feature column (for convenience purpose only):\n\ndfm.rename(columns={'Success Percentage' : 'SuccPerc'}, inplace=True)","9419f245":"# Bring 'Succeeded' to be equal 'Attempted' where 'Succeeded' > 'Attempted'\n\ndfm.loc[dfm['Succeeded'] > dfm['Attempted'], 'Succeeded'] = dfm['Attempted']\n\n# Now, locate the outliers and bring them to 1.\n\ndfm.loc[dfm['SuccPerc'] > 1, 'SuccPerc'] = 1\ndfm['SuccPerc'] = dfm['SuccPerc'].round(2)","05124ec9":"# Set Date as index\n\nts = dfm.set_index('Date')\n\n# Sort it\n\nts.sort_index(inplace=True)","2c00d4a0":"print('The records about climbing successfulness from %s to %s' % (dfm.Date.dt.date.min(), dfm.Date.dt.date.max()))","8542b470":"# Brief observation:\n\nts['SuccPerc'].plot(style='k.', figsize = (12,7))\n\nplt.title('Climbing success rate distribution')\nplt.ylabel('Success Percentage')\nplt.yticks(np.linspace(0,1,21))\nplt.show()","50f23f5a":"# Set the range\n\nts_mo = ts['2015-03-01':'2015-10-01']","75c8e00d":"ts_mo.shape","3d83865b":"# Tiseries Multiplotting\n\ncols_plot = ['Temperature AVG', 'Wind Speed Daily AVG', 'Relative Humidity AVG', 'Wind Direction AVG','Solare Radiation AVG', 'SuccPerc']\n\n\naxes = ts_mo[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(15, 10), subplots=True)\n\nfor i, ax in enumerate(axes):\n    ax.title.set_text('Time Series Data from {} to {}'.format(str(ts_mo.index.date.min()), str(ts_mo.index.date.max())))\n    break\n\n\nplt.show()\n","133109d9":"g = sns.catplot(x = 'Route', data=dfm, aspect=1.5, kind=\"count\", color=\"r\")\ng.set_xticklabels(rotation=90)\nplt.title('Number of records for the each route')\nplt.annotate('The most popular route',\n             xy=(3,1250),\n             rotation=0,\n             va='bottom',\n             ha='left',\n            )\n\nplt.annotate('',\n             xy=(0.5, 1250),\n             xytext=(3, 1270),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2',\n                             connectionstyle='arc3', \n                             color='xkcd:red', \n                             lw=2\n                            ))\nplt.show()","0483465f":"dfm.Route.replace(\"Fuhrer's Finger\", \"Fuhrers Finger\", inplace=True)","3e74f5a0":"# Create dummies for the routes\n\ndummy_r = pd.get_dummies(dfm['Route'])\n\n# Concatenate dummies to the dfm\n\ndfd = pd.concat([dfm,dummy_r], axis=1).drop('Route', axis=1)","b599c7bb":"# Create bins [0,35%] , [35%,65%], [65%, 100%]\n\nbins = [0,0.35,0.65,1]\n\n# Give names to the bins\n\ngroup_names = ['Low', 'Medium', 'High']\n\n# Add SuccBinned column to the dataset\n\ndfd['SuccBinned'] = pd.cut(dfd['SuccPerc'], bins, labels=group_names, include_lowest=True)","12586b7d":"# Calculate the amount of the each bin\ndfd.SuccBinned.value_counts()","89f1a6f3":"# Import libraries for RandomForest models development, accuracy evaluation and train\/test\/split procedure\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split","8c1ca882":"# Now we walk through each route and create a data frame with the corresponding name.\n\nroutes = dfd.columns.to_list()[10:-1]\n\n# Initialize an empty list to add all generated data frames' names\n\nnew_list = []\n\n# Main assigning loop:\n\nfor r in routes:\n    rn = r.lower()\n    rn = rn.replace(' ', '').replace('-', '_').replace(\"'\", '') # We also want to bring the df names to more safe view\n    \n    globals()['df_' + rn] = dfd.loc[dfd[r] == 1] # Generate dataframe\n    globals()['df_' + rn].drop(routes, axis=1, inplace=True) # Drop unnecessary columns\n    \n    new_list.append('df_' + rn) # Add the name to the list","b16c7798":"# Check the list\n\npd.DataFrame(data= {'list of df names' : new_list}).head()","00369a75":"# Check the df created\n\ndf_disappointmentcleaver.head(2)","26734546":"# Create a function that trains a RF model and returns its accuracy, based on testing dataset. \n# We do not consider dataframes with a number of observations <= 5\n\ndef rf_training(df):\n    if len(df) > 5:\n        temp = df.drop(['Succeeded', 'SuccPerc'], axis=1) \n        x_tr, x_tst, y_tr, y_tst = train_test_split(temp.iloc[:,1:-1], temp['SuccBinned'], test_size=0.2, random_state=1)\n\n        rfc = RandomForestClassifier(n_estimators = 100)\n        rfc.fit(x_tr,y_tr)\n        yhat = rfc.predict(x_tst)\n        return metrics.accuracy_score(y_tst, yhat)\n    \n    else:\n        return -1\n    ","d4e57580":"# Initialize a dictionary where we'll add information about the route and a base model's accuracy built for it \n# Its view: {'route' : accuracy}\n\ndic = {}\n\n# Fill the dictionary\n\nfor i,v in enumerate(new_list):\n    dic[v] = rf_training(globals()[v])","9b9b5215":"# Create df from the dictionary\n\nsuccess = pd.DataFrame(dic, index=['before tuning'])\n\n# Filter models that have less than 6 observations\n\nsuccess = success.T.loc[success.T['before tuning'] != -1]\n\n# Show the df\n\nsuccess","f2b81af3":"# The tuning function\n\ndef rf_tuning(df, mode = 'p'):\n    \n# Create train and test sets\n    temp = df.drop(['Succeeded', 'SuccPerc'], axis=1) \n    x_tr, x_tst, y_tr, y_tst = train_test_split(temp.iloc[:,1:-1], temp['SuccBinned'], test_size=0.2, random_state=1)\n    \n#     Tune min_samples_leaf   \n    a = {}\n    samp = []\n    for sam in range(2,25):            \n        rfc = RandomForestClassifier(n_estimators = 100,\n                                n_jobs = -1,\n                                max_features = \"auto\",\n                                random_state = 1,\n                                min_samples_leaf=sam)\n        rfc.fit(x_tr,y_tr)\n        yhat = rfc.predict(x_tst)\n        a[sam] = metrics.accuracy_score(y_tst, yhat)\n        samp.append(metrics.accuracy_score(y_tst, yhat))\n        \n    for s, val in a.items():   \n        if val == max(samp):\n            max_sam = s\n            \n#     Tune n_estimators        \n    b = {}\n    est = []\n    for tr in range(100,2000,100):\n        rfc = RandomForestClassifier(n_estimators = tr,\n                                n_jobs = -1,\n                                max_features = \"auto\",\n                                random_state = 888,\n                                min_samples_leaf = max_sam)\n        rfc.fit(x_tr,y_tr)\n        yhat = rfc.predict(x_tst)\n        b[tr] = metrics.accuracy_score(y_tst, yhat)\n        est.append(metrics.accuracy_score(y_tst, yhat))\n    \n    for e, val in b.items():   \n        if val == max(est):\n            max_est = e\n            \n#     Tune max_features      \n    results3 = []\n    mf_opt=[\"auto\", None, \"sqrt\", \"log2\", 0.9, 0.2]\n    for max_f in mf_opt:\n        rfc = RandomForestClassifier(n_estimators = max_est,\n                                n_jobs = -1,\n                                max_features = max_f,\n                                random_state = 888,\n                                min_samples_leaf=max_sam)\n        rfc.fit(x_tr,y_tr)\n        yhat = rfc.predict(x_tst)\n        results3.append(metrics.accuracy_score(y_tst, yhat))\n        \n#     Return best accuracy, df's leght and model's parameters\n    if mode == 'p':\n        return max(results3), len(temp), [max_est, max_sam, max_f]\n    elif mode == 't':\n        return rfc, x_tr.columns\n    else:\n        pass","c846df8a":"# Initialize a dictionary for tuned models\n\ndic1 = {}\n\n# Initialize number of samples in the df\n\ndic_samples = {}\n\n# Initialize a dictionary for model's parameters\n\ndic_bparams = {}\n\n# Fill the tuned dictionary\n\nfor r in success.index.to_list():\n    dic1[r], dic_samples[r], dic_bparams[r] = rf_tuning(globals()[r], 'p')   ","10c8d3bf":"# Convert dicts to dfs\n\ntuned = pd.DataFrame(dic1, index=['after tuning'])\nnsamples = pd.DataFrame(dic_samples, index=['df_size'])\n\ntuned = tuned.T\nnsamples = nsamples.T","bacec126":"# Merge all columns into a single dataframe\n\ncompare = success.merge(tuned, left_index = True, right_index = True)\ncompare['improvement'] = (compare['after tuning'] - compare['before tuning'])\ncompare = compare.merge(nsamples, left_index = True, right_index = True)\n\ncompare['num_est'] = 0\ncompare['min_samp_leaf'] = 0\ncompare['max_features'] = 0\n\n# Unpack parameters lists\n\nk=0\nfor key,value in dic_bparams.items():\n    for j in range(len(dic_bparams[key])):\n        compare.iloc[k,-3+j] = value[j]\n    k+=1\ncompare","142694b9":"mod, col = rf_tuning(globals()['df_disappointmentcleaver'], 't')","70b8a356":"# Let's observe feature importances\n\nt = {}\nfor f, v in zip(col.values,mod.feature_importances_):\n    t[f] = v\n    \n\ntd = pd.DataFrame(t, index=['Imp. values']).T.sort_values('Imp. values')\ntd.plot(kind='bar', figsize=(12,7))\n\nfor i, v in enumerate(td['Imp. values']):\n    plt.annotate('{0:0.2f}%'.format(v*100), xy=(i, v+0.005), ha='center', color='black')\n    \nplt.ylim(0,0.3)\nplt.title('Feature Importances')\nplt.show()","0cc4d39f":"# Now let's plot and observe the frequency of attempts per day\n\nplt.figure(figsize=(12,7))\n\nplt.title(\"Disappointment Cleaver. Number of attempts frequency\")\nplt.annotate('',\n             xy=(3.1, 345),\n             xytext=(6.9, 300),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2',\n                             connectionstyle='arc3', \n                             color='xkcd:red', \n                             lw=2\n                            ))\nplt.annotate('',\n             xy=(10.9, 360),\n             xytext=(7.1, 300),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2',\n                             connectionstyle='arc3', \n                             color='xkcd:red', \n                             lw=2\n                            ))\n\nplt.annotate('Most popular: 2 and 12 attempts per day',\n             xy=(7,280),\n             rotation=0,\n             va='bottom',\n             ha='center',)\n\nsns.distplot(df_disappointmentcleaver.Attempted,  bins=11, kde=False, rug=True);\nplt.show()","e18b3b99":"df_disappointmentcleaver.head(2)","7ef17db2":"tp = df_disappointmentcleaver.drop(['Succeeded', 'Battery Voltage AVG'], axis=1)\ng = sns.PairGrid(tp, hue='SuccBinned',\n                 palette='Set2',\n                 hue_order=['Low', 'Medium', 'High'], hue_kws = {\"marker\": [\"o\", \"s\", \"D\"]})\n\ng.map_diag(plt.hist)\ng.map_offdiag(plt.scatter)\ng.add_legend();\n\ndel(tp)","11fc2f9b":"plt.figure(figsize=(12,7))\nax = sns.violinplot(x=\"SuccBinned\", y='Wind Speed Daily AVG', data=df_disappointmentcleaver, inner=None)\nax = sns.swarmplot(x='SuccBinned', y='Wind Speed Daily AVG', data=df_disappointmentcleaver)","bd391d30":"plt.figure(figsize=(12,7))\nax = sns.violinplot(x=\"SuccBinned\", y=\"Temperature AVG\", data=df_disappointmentcleaver, inner=None)\nax = sns.swarmplot(x='SuccBinned', y='Temperature AVG', data=df_disappointmentcleaver)","20c48397":"<h3>Intro<\/h3>\n\nThe problem that underlies this project is stated here:<br>\nhttps:\/\/www.kaggle.com\/codersree\/mount-rainier-weather-and-climbing-data\n\n<b>Briefly:<\/b><br>\nWe'll go through the EDA and build a prediction model that predicts the Success Rate of climbing, using weather data, at the end.<br>\nEnjoy the reading.","96f0c57b":"For now, I'm going to keep all the columns and see how efficient the model could be. ","b3d656a7":"The main idea of my further exploration is that each route has its own features such as its difficulty, the way wind blows there and many others.<br>\nThat is why I'm going to consider each route separately.<br>\nThe case where the model was be based on all routes' observations was also examined and the average accuracy score was 58%.<br> I hope it will be higher after we split the dataset on routes.","8aeb960a":"<b>Before we continue, I want to make an important note:<\/b>\n\nAll <b>Regression models<\/b> I tried to build showed very poor accuracy ~10%. That is why all further predictions will be based on <b>Classification<\/b> models.","bac62d64":"The cells below are all for the decisions we made previously:","19ba89bd":"Combine \"Fuhrer's Finger\" and \"Fuhrers Finger\"","aa59f701":"Everything looks awesome so far. Now let's go straight forward and build a classification model based on RandomForest algorithm without tuning, as is.","58f8ecc8":"<b>What we can notice from the table above and the correlation matrix:<\/b>\n1. 'Success Percentage' feature has an unexpected value of 14.2%, this is clearly an outlier (because '<b>Success Percentage<\/b>'  $\\subseteq [0, 1]$). To preserve observations, I prefer to fix it the following way: where the values > 1, we'll make them = 1. Additionally, all succeeded attempts we'll make = number of attempts where they exceed.<br>\n\n2. There is a strong correlation between 'Succeeded' and 'Success Percentage' (logically), so we'll drop 'Succeeded' column.<br>\n3. 'Battery Voltage AVG' indicated the battary voltage and seems to be backward correlated with 'Temperature AVG'. Nevertheless, we'll keep this column.","36b18113":"The dataset is complete, without NaNs, all features have proper types","9b65b551":"Here are some EDA I conducted when tried to get some insights on the \"Disappointment Cleaver\" dataset","feb86b12":"Another way to look at the data: PairGrid.","1daca755":"Now it's time to combine it to a single data frame","e7680dc7":"Now, let's look into the data we've got:","988106f6":"As we can notice here, Wind Direction has strong correlation with Relative Humidity. The same is for Wind Speed.<br>\nAnother point to make is that <b>SuccPerc<\/b> $\\subseteq [0] \\cup [0.5, 1]$ for the most cases.","31a13fdf":"Some additional EDA using Time Series:","72cdee64":"Another important decision here is binning. We will try to predict three variants of success criterion: Low, Medium, High.","e403ba3f":"Here we see that a <b>climbing season<\/b> starts roughly in <b>March<\/b> and ends in <b>October<\/b>. So, let's consider only this interval","26cb8193":"The next step was to try to improve the accuracy of the most popular route - \"Disappointment Cleaver\". Unfortunately, any further attempts to improve the accuracy was failed. Neither changing the model nor using a GridSearch technique led to success.","b30c3c23":"Overall, we see that all features contribute to prediction performance.","ab71261e":"<h3>Conclusoin<\/h3>\n\nTo conclude, the prediction model I've managed to build has 60% accuracy. This is quite risky to use it in a real-world.<br>\nTo my mind, this is mainly because of lack of observations and the vast influence of the individual groups' climbing experience. This results to cases when the equal features' values point to different outcomes. Unfortunately, there is no data about how skilled the attempted groups were.","ca67ded3":"The next step is attempt to improve the accuracy, changing models' parameters","d9956290":"These are the values of models' accuracy for each route (before tuning)","f7b81183":"At this stage, we'll skip dataset structure analysis and will back to it several cells later."}}