{"cell_type":{"29873772":"code","75af5051":"code","9f32de9c":"code","ec0d82ab":"code","1a873e82":"code","4da70aea":"code","aad94e75":"code","6843f1f8":"code","50c4b575":"code","4331ba32":"code","1b111390":"code","f103dd09":"code","ffb4ad2b":"code","20bd0a9b":"code","1f6ce4e7":"markdown","6fed98a6":"markdown","64eb9a29":"markdown","8c68b184":"markdown","e5f89da3":"markdown","2a9eadf4":"markdown","595a174f":"markdown","9cd35041":"markdown","ab6879d0":"markdown","4eb76849":"markdown","f086a5a5":"markdown","edca4cc4":"markdown","b0a33f8d":"markdown","3959aa7d":"markdown"},"source":{"29873772":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75af5051":"import glob\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Embedding,Dense,Dropout\nimport matplotlib.pyplot as plt\nimport random","9f32de9c":"file_list=glob.glob('..\/input\/donald-trumps-rallies\/*.txt')\nlen(file_list)    # There are total 35 text files","ec0d82ab":"# Reading each text file and apppending them into a list\ntext=[]\nfor file in file_list:\n    with open(file,'r') as file:\n        text.append(file.read())\ntext[0][:201]","1a873e82":"# The tokenizer is used to remove special characters,lower case all the remaining characters and split the sentences to words\ntokenizer=Tokenizer()\n\n# Fitting the tokenizer on the text to perform the necessary pre-processing\ntokenizer.fit_on_texts(text)\n\n#Gives the index of each word\nword_text=tokenizer.word_index\n\n#Reverses the above function\nidx_text=tokenizer.index_word\n\n# Frequency of each unique word\nword_count=tokenizer.word_counts\n\n# Total number of unique words\ntot_text=len(word_count)\ntot_text","4da70aea":"# Converts each of the sentence to a list of the corresponding index of each word\nseq=tokenizer.texts_to_sequences(text)\nseq[0][:10]","aad94e75":"# The 20 words from a sentence forms the feature and the next word i.e the 21st word forms the label.\nfeatures=[]\nlabels=[]\n\nfor s in seq:\n    for i in range(0,500):\n     \n        # Obtaining lists of 20 words\n        extract=s[i:i+20]\n        features.append(extract[:-1])\n        labels.append(extract[-1])","6843f1f8":"# Shuffling the order for better training\nfeatures,labels=shuffle(features,labels,random_state=10)\n\n#75% data will be considered for training\ntrain_end=int(len(labels)*0.75)\n\n#Independent data\ntrain_feat=features[:train_end]\ntest_feat=features[train_end:]\n\n#Dependent data\ntrain_lab=labels[:train_end]\ntest_lab=labels[train_end:]\n\n#Converting the lists to numpy arrays\nX_train=np.array(train_feat)\nX_test=np.array(test_feat)\n\n#Encoding the dependent variable\ny_train = np.zeros((len(train_lab), tot_text), dtype=np.int8)\ny_test = np.zeros((len(test_lab), tot_text), dtype=np.int8)\n\nfor example_index, word_index in enumerate(train_lab):\n    y_train[example_index, word_index] = 1\n\nfor example_index, word_index in enumerate(test_lab):\n    y_test[example_index, word_index] = 1","50c4b575":"# Example\nprint(\"Input:\",' '.join([idx_text[i] for i in X_train[1]]))\nprint(\"Label:\",' '.join([idx_text[train_lab[1]]]))","4331ba32":"#Initialising the model\nmodel=Sequential()\n\n#Adding an Embedding layer with input as total number of texts\nmodel.add(Embedding(input_dim=tot_text,output_dim=100,trainable=True))\n\n#Adding the LSTM layer\nmodel.add(LSTM(128, return_sequences=False, dropout=0.1, recurrent_dropout=0.1, activation='tanh'))\n\n#Flattening \nmodel.add(Dense(64,activation='relu'))\n#Adding a dropout layer\nmodel.add(Dropout(0.5))\n#Output Layer\nmodel.add(Dense(tot_text, activation='softmax'))\nmodel.summary()","1b111390":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","f103dd09":"my_model=model.fit(x=X_train,y=y_train,epochs=200,batch_size=128,validation_data=(X_test,y_test))","ffb4ad2b":"plt.plot(my_model.history['accuracy'],label='train')\nplt.plot(my_model.history['val_accuracy'],label='test')\nplt.title(\"Acuracy Comparison\")\nplt.legend()\nplt.show()","20bd0a9b":"np.random.seed(10)\nseed=20      \nnew=20\n\n#Randomly selecting a sequence\nrand_seq=random.choice(seq)\n\n#Randomly choosing a start for the sequence\nstart_idx=random.randint(0, len(rand_seq) - seed - 5)\n\n#End point defined as 20 words after the start point\nend_idx=start_idx+seed\n\n#Slicing the start and end indexes from the randomly selected sequence\nsent=rand_seq[start_idx:end_idx]\n\n#Converting the original sequence to words\noriginal=[idx_text[i] for i in sent]\n\n#The actual list consists of the the original words in addition to 20 following words\nactual=sent[:]+rand_seq[end_idx:end_idx+new]\n\n#The list of generated words to consist of the original words in addition to 20 words predicted by the model\ngenerated=sent[:]\n\nfor i in range(new):\n    \n    #The class of the sentence is predicted\n    pred=model.predict_classes(np.array(sent).reshape(1,-1))[0]\n    new_idx=np.max(pred)\n    sent+=[new_idx]\n    generated.append(new_idx)\n\n\n\nprint(\"Original:\",\" \".join(original),\"\\n\")\nprint(\"Actual:\",\" \".join([idx_text[i]for i in actual]),\"\\n\")\nprint(\"Generated:\",\" \".join([idx_text[i]for i in generated]))","1f6ce4e7":" ## Conclusion:","6fed98a6":"- The model built gave an accuracy score of 42% on Train data and 16% on Test data.\n\n- The model clearly exhibits signs of over-fitting and high bias\n\n- One way to resolve this issue would be to increase the epochs for training the model.","64eb9a29":"#### Inference:","8c68b184":"## Generating Text (Prediction):","e5f89da3":"## Reading the data files:","2a9eadf4":"To build an LSTM model that predicts words which falls after the end of a sentence .i.e. generates the words which are likely to appear next for a given sentence.This project considers Donald Trump's Rally Speeches for the purpose of Text generation.","595a174f":"## Building the Features and labels :","9cd35041":"## Import Required Libraries:","ab6879d0":"## Problem Statement:","4eb76849":"## Text Pre-processing:","f086a5a5":"## Fitting the Model on Train data:","edca4cc4":"## Compiling the Model:","b0a33f8d":"- A model was built to generate the rest of the words of an in-complete sentence.\n\n- As per the example above, the model was able to generate words.However, the words generated were not accurate in comparion with the actual words of the speech.\n\n- The reason for this in-accuracy could be the low performance of the model.The model needs to be trained for much higher number of epochs.","3959aa7d":"## Building an LSTM model:"}}