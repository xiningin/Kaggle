{"cell_type":{"8ba366fa":"code","3c1cbe8b":"code","b8b91e1c":"code","4c8f8794":"code","0bd8f4a9":"code","6b224964":"code","51d0f60f":"code","ff6fe649":"code","87fa4b71":"code","2fc38c2f":"code","1b2bf7cf":"code","bc410d64":"code","fb0237d8":"code","16ea1d59":"code","04ec7b2f":"code","6f5d764b":"code","19abcc0c":"code","7594a158":"code","6e0aee49":"code","614a9907":"code","1c1475e3":"code","b1e1c8c2":"code","4b3589f7":"code","19045b13":"code","7be1dde6":"code","0a1415cb":"code","71d310fa":"code","22144834":"code","c02f5017":"code","15345fc2":"code","bfe67975":"code","b6149b68":"code","feaaaead":"code","5efabcce":"markdown","9a4fe9dc":"markdown","508d7659":"markdown","0a2c800c":"markdown","416e85de":"markdown","17926d38":"markdown","deb279ab":"markdown","0368638e":"markdown","0324df2b":"markdown","d8833c24":"markdown","05b08f70":"markdown","4e85d726":"markdown"},"source":{"8ba366fa":"#Installing libraries\n!pip install regressors","3c1cbe8b":"import numpy as np \nimport pandas as pd \nfrom regressors import stats\nfrom sklearn import linear_model as lm\nimport statsmodels.formula.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nimport os\nprint(os.listdir(\"..\/input\"))","b8b91e1c":"#Data Preprocessing \nd = pd.read_csv(\"..\/input\/survey.csv\")\nd = d.rename(index=str,columns={\"Wr.Hnd\":\"WrHnd\"})\nd = d[[\"WrHnd\",\"Height\"]]\nd = d.dropna()\n\n#Model Fit \ninputDF = d[[\"WrHnd\"]]\noutcomeDF = d[[\"Height\"]]\nmodel = lm.LinearRegression()\nresults = model.fit(inputDF,outcomeDF)\n\n#Regression coefficients\nprint(model.intercept_, model.coef_)","4c8f8794":"print(\"Adjusted R-Squared:\\n\",stats.adj_r2_score(model, inputDF, outcomeDF))\n\nprint(\"P-value:\\n\",stats.coef_pval(model, inputDF, outcomeDF))","0bd8f4a9":"#Data Preprocessing \nd = pd.read_csv(\"..\/input\/survey.csv\")\nd = d.rename(index=str,columns={\"Wr.Hnd\":\"WrHnd\"})\nd = d[[\"WrHnd\",\"Height\"]]\nd = d.dropna()\n\n#Adjusted R-Squared & P-vlaue genarated using Statsmodels\nres = sm.ols(formula=\"Height ~ WrHnd\",data=d).fit()\nprint(res.summary())\n","6b224964":"#Adjusted R-Squared & P-vlaue genarated for cubic polynomial transformation\nres = sm.ols(formula=\"Height ~ WrHnd + I(WrHnd*WrHnd)+ I(WrHnd*WrHnd*WrHnd)\",data=d).fit()\nprint(res.summary())","51d0f60f":"#Adjusted R-Squared & P-vlaue genarated for logarithmic transformation\nres = sm.ols(formula = \"Height ~ np.log(WrHnd)\",data=d).fit()\nprint(res.summary())","ff6fe649":"#Declaring a dataframe\nd = {'sno': [1,2,3,4,5,6],\n     'Temperature': [0, 20, 40, 60, 80, 100], \n     'Pressure': [0.0002, 0.0012, 0.0060, 0.0300, 0.0900, 0.2700]}\n\ndf = pd.DataFrame(d)\ndf.head()","87fa4b71":"#Model Fit - Linear Regression\ninputDF = df.iloc[:, 1:2].values \noutputDF = df.iloc[:, 2].values\n\nlin = LinearRegression()  \nlin.fit(inputDF, outputDF)","2fc38c2f":"#Model Fit - Polynomial Regression\npoly = PolynomialFeatures(degree = 4) \ninputDF_poly = poly.fit_transform(inputDF) \n\npoly.fit(inputDF_poly, outputDF) \nlin2 = LinearRegression() \nlin2.fit(inputDF_poly, outputDF) ","1b2bf7cf":"#Scatter Plot - Linear Regression\nplt.scatter(inputDF, outputDF, color = 'blue') \n  \nplt.plot(inputDF, lin.predict(inputDF), color = 'red') \nplt.title('Linear Regression') \nplt.xlabel('Temperature') \nplt.ylabel('Pressure') \n  \nplt.show() ","bc410d64":"#Scatter Plot - Polynomial Regression\nplt.scatter(inputDF, outputDF, color = 'blue') \n  \nplt.plot(inputDF, lin2.predict(poly.fit_transform(inputDF)), color = 'red') \nplt.title('Polynomial Regression') \nplt.xlabel('Temperature') \nplt.ylabel('Pressure') \n  \nplt.show() ","fb0237d8":"#Model 1 - Evaluation using train_test_split\ndf = pd.read_csv(\"..\/input\/mtcars.csv\")\n\ninputDF = df[[\"hp\",\"am\"]]\noutputDF = df[[\"mpg\"]]\n\nX_train, X_test, y_train, y_test = train_test_split(inputDF, outputDF, test_size=0.2, random_state=0) \n\nmodel = lm.LinearRegression()\nresults = model.fit(X_train,y_train)\n\nprint(\"R - Squared value:\\n\",stats.adj_r2_score(model, X_train, y_train)) \n\nprint(model.intercept_, model.coef_)","16ea1d59":"#Model 1 - Prediction\ny_pred = model.predict(X_test) \nprint(\"Predicted value:\\n\", y_pred) \nprint(\"Originial value:\\n\", y_test) \nprint(\"RMSE:\\n\", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))      ","04ec7b2f":"#Model 2 - Evaluation using train_test_split\ninputDF = df[[\"hp\"]]\noutputDF = df[[\"mpg\"]]\n\nX_train, X_test, y_train, y_test = train_test_split(inputDF, outputDF, test_size=0.2, random_state=0) \nregressor = LinearRegression()  \nregressor.fit(X_train, y_train) \nprint(\"R - Squared value:\\n\",stats.adj_r2_score(regressor, X_train, y_train)) \nprint(regressor.intercept_)\nprint(regressor.coef_)  ","6f5d764b":"#Model 2 - Prediction\ny_pred = regressor.predict(X_test)\nprint(\"Predicted value:\\n\", y_pred) \nprint(\"Originial value:\\n\", y_test) \nprint(\"RMSE:\\n\", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","19abcc0c":"d=pd.read_csv(\"..\/input\/default.csv\")\nd.head()\nd[\"balance\"].describe()","7594a158":"#Add a new column DefaultYes which is 1 for Yes and 0 for No\nd['DefaultYes'] = d['default'].map({'Yes': 1, 'No': 0})\nd.head()","6e0aee49":"#Scikit-learn - Linear Regression\nregressor = LinearRegression()  \ninputDf = d[['balance']]\noutputDf = d[['DefaultYes']]\nregressor.fit(inputDf, outputDf) \nprint(regressor.intercept_)\nprint(regressor.coef_[0]) ","614a9907":"#Input Dataframe\nx1new = pd.DataFrame(np.hstack((np.arange(0,3000))))\nx1new.columns=[\"balance\"]\nyp2new = regressor.predict(x1new)","1c1475e3":"#Scatter Plot\nplt.scatter(d[\"balance\"],d[\"DefaultYes\"])\nplt.plot(x1new,yp2new,color=\"red\")\nplt.show()","b1e1c8c2":"#Logistic Regression\ninputDf = d[['balance']]\noutputDf = d[['DefaultYes']].values.ravel()","4b3589f7":"#Model Fit - Logistic Regression\nlogisticRegr = LogisticRegression(solver='lbfgs')\nlogisticRegr.fit(inputDf, outputDf)\nprint(logisticRegr.intercept_)\nprint(logisticRegr.coef_)","19045b13":"#New Dataframe and prediction\nx1new = pd.DataFrame(np.hstack((np.arange(0,3000))))\nx1new.columns=[\"balance\"]\nyp2new = logisticRegr.predict(x1new)","7be1dde6":"#Plot\nplt.scatter(d[\"balance\"],d[\"DefaultYes\"])\nplt.plot(x1new,yp2new,color=\"red\")\nplt.show()","0a1415cb":"d = pd.read_csv(\"..\/input\/default.csv\")\nd = d[[\"default\",\"balance\",\"income\",\"student\"]]\nd.head()","71d310fa":"#Add a new column DefaultYes which is 1 for Yes and 0 for No\nd['DefaultYes'] = d['default'].map({'Yes': 1, 'No': 0})\nd.head()","22144834":"d = d.drop(['default'], axis=1)\nd.head()","c02f5017":"#Categorical Predictors\nd = pd.get_dummies(d, prefix=['student'], columns=['student'])\nd.head()","15345fc2":"#Model - 1 Fit\ninputDF = d[[\"balance\",\"income\",\"student_No\",\"student_Yes\"]]\noutputDF = d[[\"DefaultYes\"]].values.ravel()\n\nlogisticRegr = LogisticRegression(solver='lbfgs')\nx_train, x_test, y_train, y_test = train_test_split(inputDF, outputDF, test_size=0.25, random_state=0)\nlogisticRegr.fit(inputDF, outputDF)\n\nprint(logisticRegr.intercept_)\nprint(logisticRegr.coef_)","bfe67975":"#Model - 1 Validation\ny_pred = logisticRegr.predict(x_test)\n#print(r2_score(y_test, y_pred)) \nprint(\"R - Squared value:\\n\",stats.adj_r2_score(logisticRegr, x_train, y_train)) \nprint(\"RMSE:\\n\", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","b6149b68":"#Model - 2 Fit\ninputDF = d[[\"income\",\"student_No\",\"student_Yes\"]]\noutputDF = d[[\"DefaultYes\"]].values.ravel()\n\nlogisticRegr = LogisticRegression(solver='lbfgs')\nx_train, x_test, y_train, y_test = train_test_split(inputDF, outputDF, test_size=0.25, random_state=0)\nlogisticRegr.fit(x_train, y_train)\n\nprint(logisticRegr.intercept_)\nprint(logisticRegr.coef_)","feaaaead":"#Model - 2 Validation\ny_pred = logisticRegr.predict(x_test)\nprint(\"R - Squared value:\\n\",stats.adj_r2_score(logisticRegr, x_train, y_train)) \nprint(\"RMSE:\\n\", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n#print(r2_score(y_test, y_pred)) ","5efabcce":"### **1. Regression Parameters: Adjusted R-Squared & P-value:**","9a4fe9dc":"### **5. Logistic Regression with Scikit-Learn:**","508d7659":"**References:** <br>\n1. Galarnyk, Michael, and Michael Galarnyk. \u201cLogistic Regression Using Python (Scikit-Learn).\u201d Towards Data Science, Towards Data Science, 13 Sept. 2017, towardsdatascience.com\/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a.\n2. Damian, Mihai DamianMihai. \u201cPolynomial Regression Using Scikit-Learn.\u201d Cross Validated, stats.stackexchange.com\/questions\/58739\/polynomial-regression-using-scikit-learn.\n3. \u201cUnderstanding Logistic Regression in Python.\u201d DataCamp Community, datacamp.com\/community\/tutorials\/understanding-logistic-regression-python.\n4. \u201cPython | Implementation of Polynomial Regression.\u201d GeeksforGeeks, 3 Oct. 2018, geeksforgeeks.org\/python-implementation-of-polynomial-regression\/.\n\n","0a2c800c":"### **2. Analysing numerical transformations with Statsmodels:**","416e85de":"**Linear Regression:**\n![](https:\/\/i.ibb.co\/yPtV2Jp\/img2.png)![](https:\/\/i.ibb.co\/znrYMMv\/img1.png) \n**Polynomial Regression:**\n![](https:\/\/i.ibb.co\/k9pCyZ7\/img4.png)![](https:\/\/i.ibb.co\/V2zckr1\/img3.png) ","17926d38":"### **6. Multiple Logistic Regression with Scikit-Learn (Categorical Predictors):**","deb279ab":"#  **Logistic Regression & Numerical Transformations**\nLab Exercises - Week 3\n\n----------","0368638e":"## **Notebook Contents:**\n1. Regression Parameters: Adjusted R-Squared & P-value.\n2. Analysing numerical transformations with Statsmodels.  \n3. Numerical transformations (Polynomial) with Scikit-Learn.\n4. Model evaluation with Linear Regression (Using R-squared).\n5. Logistic Regression with Scikit-Learn.\n6. Multiple Logistic Regression with Scikit-Learn (Categorical Predictors).\n\n---------","0324df2b":"### **Python Libraries:**","d8833c24":"**Voil\u00e0! This is the end of the lab session for week 3.** <br>\nDo not forget to commit your notebook and set the access to private. Share the notebook with Prof. Karim (Kaggle id: karimshaikh) and Manish Varma (Kaggle id: manishvarma).","05b08f70":"### **3. Numerical transformations (Polynomial) with Scikit-Learn:**","4e85d726":"### **4. Model evaluation with Linear Regression (Using R-squared):**"}}