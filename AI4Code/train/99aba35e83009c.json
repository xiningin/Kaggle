{"cell_type":{"798107ef":"code","3a8b4bce":"code","97b28263":"code","83757d1b":"code","4be47160":"code","6a892197":"code","8f284b89":"code","64e32131":"code","a5b984f5":"code","fbfe5771":"code","9efd0224":"code","99d0ab0f":"code","472272bd":"code","57bad53b":"code","eb3170aa":"code","38e430a2":"code","9cfd3889":"code","657ac428":"code","aae52fcd":"code","ebf337bc":"code","baa7527d":"code","7237b59e":"code","280bc266":"code","3ab6e6b6":"code","bf5d4ffc":"code","d3d01927":"code","4ce2d971":"code","361d187e":"code","2b086061":"code","f185d2b2":"code","bd445d73":"code","bee15250":"code","18b17f3f":"code","4f7f475b":"code","97a1b1c3":"code","4d5ee2e7":"code","73c3cb32":"code","8eec3b66":"code","322b3e33":"code","a97ffdaa":"code","1bb61448":"code","46bcb8bc":"markdown","d901549e":"markdown","cb8478c3":"markdown","6ec80ff8":"markdown","e5450514":"markdown","a583d027":"markdown","b75a8343":"markdown","1343acbc":"markdown","367cf9f4":"markdown","7b87fa37":"markdown","0163ca9c":"markdown","e2d99ba7":"markdown","17ddb1fb":"markdown","d3f2a56c":"markdown","805f7162":"markdown","7b472d30":"markdown","bbbddc43":"markdown","364a45b9":"markdown","4f660272":"markdown","b55710aa":"markdown","8bf3fc18":"markdown"},"source":{"798107ef":"import spacy\nimport pandas as pd\n\ndata = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n\n# 1. Loading the language library\nnlp = spacy.load('en_core_web_sm')\n\n# 2. Building a Pipline Object\ndoc = nlp(u'''\nTesla will start selling cars in India next year, government says. \nElon Mask (CEO of Tesla) is now the richest men in the world.\n''')\n\n\n# 3. Using Tokens\nfor token in doc:\n    print(f\"{token.text:{12}}{token.pos_:{12}}{token.dep_:{12}}{token.lemma_}\")","3a8b4bce":"data[data.target == 1]['text'][1]","97b28263":"data[data.target == 1]['text'][300]","83757d1b":"data[data.target == 0]['text']","4be47160":"nlp.pipeline","6a892197":"nlp.pipe_names","8f284b89":"text = \"\"\"\nElon Musk, the billionaire CEO of Tesla and SpaceX, is now the richest person in the world, surpassing former titleholder and Amazon chief Jeff Bezos with a net worth of $189.7 billion, according to Forbes\u2019s real-time billionaire net-worth estimates on Jan. 8, 2021 at 1pm. Since March, Musk\u2019s wealth has grown almost seven-fold, up a staggering $163.1 billion.\n\"\"\"\ndoc = nlp(text)","64e32131":"quote = doc[30:50]\nprint(quote)\nprint(type(quote))","a5b984f5":"for i, sentence in enumerate(doc.sents, 1):\n    print(f\"{i} - {sentence}\")","fbfe5771":"for entity in doc.ents:\n    print(f\"{entity.text:-<{20}}{entity.label_:-<{20}}{str(spacy.explain(entity.label_))}\")","9efd0224":"for chunk in doc.noun_chunks:\n    print(chunk.text)","99d0ab0f":"from spacy import displacy\n\ndisplacy.render(doc, style='dep', jupyter=True, options={'distance':90})","472272bd":"displacy.render(doc, style='ent', jupyter=True)","57bad53b":"import nltk\nfrom nltk.stem.porter import PorterStemmer\n\nwords = ['run', 'runner', 'ran', 'runs', 'easily', 'fairly', 'fairness']\np_stemmer = PorterStemmer()\n\nfor word in words:\n    print(f\"{word} --------> {p_stemmer.stem(word)}\")","eb3170aa":"from nltk.stem.snowball import SnowballStemmer\n\nwords = ['run', 'runner', 'ran', 'runs', 'easily', 'fairly', 'fairness']\ns_stemmer = SnowballStemmer(language='english')\n\nfor word in words:\n    print(f\"{word} --------> {s_stemmer.stem(word)}\")","38e430a2":"words = ['generous', 'generation', 'generously', 'generate']\n\nprint('===============SNOWBALL STEMMER================')\nfor word in words:\n    print(f\"{word} --------> {s_stemmer.stem(word)}\")\n    \nprint('===============PORTER STEMMER================')\nfor word in words:\n    print(f\"{word} --------> {p_stemmer.stem(word)}\")","9cfd3889":"text = nlp(u\"I am a runner running in a race because I love to run since I ran everyday\")\n\nfor token in text:\n    print(f\"{token.text:{12}}{token.pos_:{10}}\\t{token.lemma:{20}}\\t{token.lemma_}\")","657ac428":"nlp = spacy.load('en_core_web_sm')\n\nprint(nlp.Defaults.stop_words)\nprint(len(nlp.Defaults.stop_words))","aae52fcd":"words = ['is', 'and', 'Tesla', 'you', 'IS', 'AND']\n\nfor word in words:\n    print(f\"{word}: is stop word: {nlp.vocab[word].is_stop}\")","ebf337bc":"# We can add our own stop word\nnlp.Defaults.stop_words.add('btw')\nnlp.Defaults.stop_words.add('u')\n\nsentence = 'Where was u ? I was looking for you btw session...'\nfor word in sentence.split():\n    print(f\"{word:{20}}: is stop word: {nlp.vocab[word].is_stop}\")","baa7527d":"# We can also remove stop word\nnlp.vocab['for'].is_stop = False\nsentence = 'Where was u ? I was looking for you btw session...'\nfor word in sentence.split():\n    print(f\"{word:{20}}: is stop word: {nlp.vocab[word].is_stop}\")","7237b59e":"from spacy.matcher import Matcher\n\nmatcher = Matcher(nlp.vocab)\npattern_1 = [{'LOWER': 'solarpower'}] # ----> SolarPower\npattern_2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}] # ---> Solar-Power\npattern_3 = [{'LOWER': 'solar'}, {'LOWER': 'power'}] # ---> Solar Power\n\nmatcher.add('SolarPower', None, pattern_1, pattern_2, pattern_3)\n\ntext = u'''\nSolar Power is the conversion of energy from sunlight into electricity, \neither directly using photovoltaics (PV), indirectly using concentrated SolarPower, \nor a combination. Concentrated Solar-Power systems use lenses or mirrors and solar \ntracking systems to focus a large area of sunlight into a small beam.\n'''\ndoc = nlp(text)\nfound_matches = matcher(doc)\nprint(found_matches)","280bc266":"!python3 -m spacy download en_core_web_md","3ab6e6b6":"import en_core_web_md\n# Load a larger model with vectors\nnlp = en_core_web_md.load()\n\n# Compare two documents\ndoc_1 = nlp(\"I like fast food\")\ndoc_2 = nlp(\"I like pizza\")\n\nprint(doc_1.similarity(doc_2))\nprint(doc_2.similarity(doc_1))","bf5d4ffc":"# Compare two tokens\ndoc = nlp(\"I like pizza and pasta\")\n\ntoken_1 = doc[2]\ntoken_2 = doc[4]\nprint(token_1.similarity(token_2))","d3d01927":"# Compare a span with a document\nspan = nlp(\"I like pizza and pasta\")[2:5]\ndoc = nlp(\"McDonalds sells burgers\")\n\nprint(span.similarity(doc))","4ce2d971":"text = u'''\nSince March, Musk\u2019s wealth has grown almost seven-fold, up a staggering $163.1 billion.\n'''\ndoc = nlp(text)\nfor token in doc:\n    print(f\"{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_)}\")","361d187e":"doc = nlp(\"I read books on NLP.\")\nr = doc[1]\nprint(f\"{r.text:{10}} {r.pos_:{8}} {r.tag_:{6}} {spacy.explain(r.tag_)}\\n\")\n\ndoc = nlp(\"I am reading a book on NLP.\")\nr = doc[2]\nprint(f\"{r.text:{10}} {r.pos_:{8}} {r.tag_:{6}} {spacy.explain(r.tag_)}\")","2b086061":"doc = nlp(\"The quick brown fox jumped over the lazy dog's back.\")\n\npos_count = doc.count_by(spacy.attrs.POS)\nprint(pos_count)\nnew = {}\nfor key, value in pos_count.items():\n    new[doc.vocab[key].text] = value\n    \nprint(new)","f185d2b2":"!wget https:\/\/www.gutenberg.org\/files\/1342\/1342-0.txt","bd445d73":"import os\nimport spacy\n\nnlp = spacy.load('en')\n\ndef read_file(file_name):\n    with open(file_name, 'r') as file:\n        return file.read()\n    \ntext = read_file('1342-0.txt')\nprocessed_text = nlp(text)","bee15250":"# An example of corpus, consists of 7045 sentences\nsentences = [s for s in processed_text.sents]\n\nprint(len(sentences))","18b17f3f":"print(sentences[30:34])","4f7f475b":"len(processed_text.text.split())","97a1b1c3":"import gensim\nfrom gensim.models import Word2Vec\n\nprint(f\"Gensim Version: {gensim.__version__}\")\n\n# We need data for training the model\nprocessed_sentences = [sent.lemma_.split() for sent in processed_text.sents]\nprocessed_sentences[0]","4d5ee2e7":"# Word2Vec accepts several parameters that affect both training speed and quality\ninterchangeable_words_model = Word2Vec(\n    sentences=processed_sentences,\n    min_count=10, # Purning the internal dictionary\n    size=200, # the number of dimensions (N) gensim maps the word onto\n    window=2, # \n    compute_loss=True,\n    sg=1\n)\n\nprint(len(interchangeable_words_model.wv.vocab))\n\n# getting the training loss\ntraining_loss = interchangeable_words_model.get_latest_training_loss()\nprint(f\"Training Loss: {training_loss}\")\n\nfor w, sim in interchangeable_words_model.wv.most_similar('Darcy'):\n    print((w, sim))","73c3cb32":"from gensim.models import FastText\n\nmodel = FastText(window=2)\nmodel.build_vocab(sentences=processed_sentences)\nmodel.train(sentences=processed_sentences, total_examples=len(processed_sentences), epochs=10)\n\nfor w, sim in model.wv.most_similar('Darcy'):\n    print((w, sim))","8eec3b66":"for w, sim in model.wv.most_similar('Darcy', topn=20):\n    print((w, sim))","322b3e33":"model = FastText(window=50)\nmodel.build_vocab(sentences=processed_sentences)\nmodel.train(sentences=processed_sentences, total_examples=len(processed_sentences), epochs=10)\n\nfor w, sim in model.wv.most_similar('Darcy'):\n    print((w, sim))","a97ffdaa":"print(\"night\" in model.wv.vocab)\nprint(\"nights\" in model.wv.vocab)","1bb61448":"print(model.wv.similarity(\"nights\", \"night\"))\nprint(model.wv.similarity(\"tonight\", \"night\"))","46bcb8bc":"The above example loads the entire corpus into memory. In practice, corpora may be very large, so loading them into memory may be impossible. Gensim intelligently handles such corpus by streaming one document at a time.","d901549e":"## Word2Vec\n\n\nUsing large amounts of unannotated plain text, word2vec learns relationships between words automatically. The output are vectors, one vector per word, with remarkable linear relationships.\n\nWord2Vec is very useful in automatic text tagging, recommender systems and machine translation.\n\n`Word2Vec`: is a more recent model that embeds words in a lower-dimentional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings. For example, `strong` and `powerful` would be close together and `strong` and `Paris` would be relatively far.","cb8478c3":"## Word Vectors and Semantic Similarity\n\n- Spacy can compare two objects and predict similarity, `Doc.similarity()`, `Span.similarity()` and `Token.similarity()`. They take another object and return a similarity score (`0` to `1`).\n- `Important`: needs a model that has word vectors included, for example: `en_core_web_md`, `en_core_web_lg`, not `en_core_web_sm`.","6ec80ff8":"## Named Entity","e5450514":"## Corpus\nA corpus is a collection of document objects. Corpora serve two roles in Gensim:\n1. Input for training a model.\n2. Documents to orgnize.","a583d027":"# Noun Chunks","b75a8343":"## Counting POS Tags\n\nThe `Doc.count_by()` method accepts a specific token attribute as its argument, and returns a frequency count of the given attribute as a dictionary object.","1343acbc":"## View token tags\n\nRecall that we can obtain a particular token by its index position.\n- To view the description of either type of tag use `spacy.explain(tag)`","367cf9f4":"# Gensim\n\nGensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. \n\n## Document \nIn Gensim, a document is an object of the text sequence type (commonly known as str in Python 3). A document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book.","7b87fa37":"# \ud83d\udccc Notebook Goals\n> - Understand Basic NLP Topics (Tokenization, Stemming, Lemmatization, Stop words).\n> - Spacy for Vocabulary Matching.\n> - Gensim for topic modeling and semantic similarity\n> - General discussion of what Natural Language Processing is.\n\n# \ud83d\udcda What is Spacy?\n> - Spacy is an open source Natural Language Processing Library designed to effectively handle NLP tasks with the most efficient implementation of common algorithms.\n> - For many NLP tasks, Spacy only has one implementation method, choosing the most efficient algorithm currently available. This means you often don't have the option to choose other algorithms.\n\n# \ud83d\udcdd What is NLTK?\n> - NLTK - Natural Language Toolkit is a very popular open source. Initially released in 2001, it is much older than Spacy (released 2015). It also provides many functionalities, but includes less efficient implementations.\n\n# \ud83d\udcaa\ud83c\udffb NLTK vs Spacy\n> - For many common NLP tasks, Spacy is much faster and more efficient, at the cost of the user not being able to choose algorithmic implementations. However, Spacy does not include pre-created models for some applications, such as sentiment analysis, which is typically easier to perform with NLTK.\n\n# \ud83d\udcda spaCy Basics\n\n**spaCy** (https:\/\/spacy.io\/) is an open-source Python library that parses and \"understands\" large volumes of text. Separate models are available that cater to specific languages (English, French, German, etc.).\n\n\n## \u2714\ufe0f Working with spaCy\n- There are few keys steps for working with Spacy:\n> 1. Loading the language library.\n> 2. Building a Pipeline Object\n> 3. Using Tokens\n> 4. Parts-of-Speech Tagging\n> 5. Understanding Token Attributes\n___\n## \ud83d\udd2a Tokenization\n\nThe first step in processing text is to split up all the component parts (words & punctuation) into \"tokens\". These tokens are annotated inside the Doc object to contain descriptive information. \n\n![tokenization.png](attachment:tokenization.png)\n\n-  **Prefix**:\tCharacter(s) at the beginning &#9656; `$ ( \u201c \u00bf`\n-  **Suffix**:\tCharacter(s) at the end &#9656; `km ) , . ! \u201d`\n-  **Infix**:\tCharacter(s) in between &#9656; `- -- \/ ...`\n-  **Exception**: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied &#9656; `St. U.S.`\n\n> Notice that tokens are pieces of the original text. That is, we don't see any conversion to word stems or lemmas (base forms of words) and we haven't seen anything about organizations\/places\/money etc. Tokens are the basic building blocks of a Doc object - everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another.\n\n## \ud83d\udcda spaCy Objects\n\n> After importing the spacy module in the cell above we loaded a **model** and named it `nlp`.<br>Next we created a **Doc** object by applying the model to our text, and named it `doc`.<br>spaCy also builds a companion **Vocab** object that we'll cover in later sections.<br>The **Doc** object that holds the processed text is our focus here.\n\n## \u27bf Pipeline\n> When we run `nlp`, our text enters a *processing pipeline* that first breaks down the text and then performs a series of operations to tag, parse and describe the data.   Image source: \n![pipeline1.png](attachment:pipeline1.png)\nWe can check to see what components currently live in the pipeline. In later sections we'll learn how to disable components and add new ones as needed.\n\n___\n## \ud83d\udd16 Part-of-Speech Tagging (POS)\n> The next step after splitting the text up into tokens is to assign parts of speech. In the above example, `Tesla` was recognized to be a ***proper noun***. Here some statistical modeling is required. For example, words that follow \"the\" are typically nouns.\n\n___\n## \ud83e\uddee Dependencies\n> We also looked at the syntactic dependencies assigned to each token. `Tesla` is identified as an `nsubj` or the ***nominal subject*** of the sentence.\n\n___\n## \u2795 Additional Token Attributes\n> We'll see these again in upcoming lectures. For now we just want to illustrate some of the other information that spaCy assigns to tokens:\n\n|Tag|Description|doc2[0].tag|\n|:------|:------:|:------|\n|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Tesla`|\n|`.lemma_`|The base form of the word|`tesla`|\n|`.pos_`|The simple part-of-speech tag|`PROPN`\/`proper noun`|\n|`.tag_`|The detailed part-of-speech tag|`NNP`\/`noun, proper singular`|\n|`.shape_`|The word shape \u2013 capitalization, punctuation, digits|`Xxxxx`|\n|`.is_alpha`|Is the token an alpha character?|`True`|\n|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|\n\n\n___\n## \ud83e\uddfe Spans\n> Large Doc objects can be hard to work with at times. A **span** is a slice of Doc object in the form `Doc[start:stop]`.\n\n___\n## \ud83d\udcd1 Sentences\n> Certain tokens inside a Doc object may also receive a \"start of sentence\" tag. While this doesn't immediately build a list of sentences, these tags enable the generation of sentence segments through `Doc.sents`. Later we'll write our own segmentation rules.","0163ca9c":"## Coarse-grained Part-of-speech Tags\nEvery token is assigned a POS Tag from the following list:\n\n\n<table><tr><th>POS<\/th><th>DESCRIPTION<\/th><th>EXAMPLES<\/th><\/tr>\n    \n<tr><td>ADJ<\/td><td>adjective<\/td><td>**big, old, green, incomprehensible, first**<\/td><\/tr>\n<tr><td>ADP<\/td><td>adposition<\/td><td>*in, to, during*<\/td><\/tr>\n<tr><td>ADV<\/td><td>adverb<\/td><td>*very, tomorrow, down, where, there*<\/td><\/tr>\n<tr><td>AUX<\/td><td>auxiliary<\/td><td>*is, has (done), will (do), should (do)*<\/td><\/tr>\n<tr><td>CONJ<\/td><td>conjunction<\/td><td>*and, or, but*<\/td><\/tr>\n<tr><td>CCONJ<\/td><td>coordinating conjunction<\/td><td>*and, or, but*<\/td><\/tr>\n<tr><td>DET<\/td><td>determiner<\/td><td>*a, an, the*<\/td><\/tr>\n<tr><td>INTJ<\/td><td>interjection<\/td><td>*psst, ouch, bravo, hello*<\/td><\/tr>\n<tr><td>NOUN<\/td><td>noun<\/td><td>*girl, cat, tree, air, beauty*<\/td><\/tr>\n<tr><td>NUM<\/td><td>numeral<\/td><td>*1, 2017, one, seventy-seven, IV, MMXIV*<\/td><\/tr>\n<tr><td>PART<\/td><td>particle<\/td><td>*'s, not,*<\/td><\/tr>\n<tr><td>PRON<\/td><td>pronoun<\/td><td>*I, you, he, she, myself, themselves, somebody*<\/td><\/tr>\n<tr><td>PROPN<\/td><td>proper noun<\/td><td>*Mary, John, London, NATO, HBO*<\/td><\/tr>\n<tr><td>PUNCT<\/td><td>punctuation<\/td><td>*., (, ), ?*<\/td><\/tr>\n<tr><td>SCONJ<\/td><td>subordinating conjunction<\/td><td>*if, while, that*<\/td><\/tr>\n<tr><td>SYM<\/td><td>symbol<\/td><td>*$, %, \u00a7, \u00a9, +, \u2212, \u00d7, \u00f7, =, :), \ud83d\ude1d*<\/td><\/tr>\n<tr><td>VERB<\/td><td>verb<\/td><td>*run, runs, running, eat, ate, eating*<\/td><\/tr>\n<tr><td>X<\/td><td>other<\/td><td>*sfpksdpsxmsa*<\/td><\/tr>\n<tr><td>SPACE<\/td><td>space<\/td><\/tr>\n\n***\n","e2d99ba7":"## Phrase Matching and Vocabulary\n\n- We can think of this as a powerful version of Regular Expression where we actually take parts of speech into account for our patterns.","17ddb1fb":"## Stemming\n\n- Often when searching text for a certain keyword, it helps if the search returns variations of the word. For instance, searching for 'boat' might return 'boats' and 'boating'. Here, 'boat' would be the stem for [boat, boater, boating, boats].\n","d3f2a56c":"___\n## Fine-grained Part-of-speech Tags\nTokens are subsequently given a fine-grained tag as determined by morphology:\n<table>\n<tr><th>POS<\/th><th>Description<\/th><th>Fine-grained Tag<\/th><th>Description<\/th><th>Morphology<\/th><\/tr>\n<tr><td>ADJ<\/td><td>adjective<\/td><td>AFX<\/td><td>affix<\/td><td>Hyph=yes<\/td><\/tr>\n<tr><td>ADJ<\/td><td><\/td><td>JJ<\/td><td>adjective<\/td><td>Degree=pos<\/td><\/tr>\n<tr><td>ADJ<\/td><td><\/td><td>JJR<\/td><td>adjective, comparative<\/td><td>Degree=comp<\/td><\/tr>\n<tr><td>ADJ<\/td><td><\/td><td>JJS<\/td><td>adjective, superlative<\/td><td>Degree=sup<\/td><\/tr>\n<tr><td>ADJ<\/td><td><\/td><td>PDT<\/td><td>predeterminer<\/td><td>AdjType=pdt PronType=prn<\/td><\/tr>\n<tr><td>ADJ<\/td><td><\/td><td>PRP\\$<\/td><td>pronoun, possessive<\/td><td>PronType=prs Poss=yes<\/td><\/tr>\n<tr><td>ADJ<\/td><td><\/td><td>WDT<\/td><td>wh-determiner<\/td><td>PronType=int rel<\/td><\/tr>\n<tr><td>ADJ<\/td><td><\/td><td>WP\\$<\/td><td>wh-pronoun, possessive<\/td><td>Poss=yes PronType=int rel<\/td><\/tr>\n<tr><td>ADP<\/td><td>adposition<\/td><td>IN<\/td><td>conjunction, subordinating or preposition<\/td><td><\/td><\/tr>\n<tr><td>ADV<\/td><td>adverb<\/td><td>EX<\/td><td>existential there<\/td><td>AdvType=ex<\/td><\/tr>\n<tr><td>ADV<\/td><td><\/td><td>RB<\/td><td>adverb<\/td><td>Degree=pos<\/td><\/tr>\n<tr><td>ADV<\/td><td><\/td><td>RBR<\/td><td>adverb, comparative<\/td><td>Degree=comp<\/td><\/tr>\n<tr><td>ADV<\/td><td><\/td><td>RBS<\/td><td>adverb, superlative<\/td><td>Degree=sup<\/td><\/tr>\n<tr><td>ADV<\/td><td><\/td><td>WRB<\/td><td>wh-adverb<\/td><td>PronType=int rel<\/td><\/tr>\n<tr><td>CONJ<\/td><td>conjunction<\/td><td>CC<\/td><td>conjunction, coordinating<\/td><td>ConjType=coor<\/td><\/tr>\n<tr><td>DET<\/td><td>determiner<\/td><td>DT<\/td><td>determiner<\/td><td><\/td><\/tr>\n<tr><td>INTJ<\/td><td>interjection<\/td><td>UH<\/td><td>interjection<\/td><td><\/td><\/tr>\n<tr><td>NOUN<\/td><td>noun<\/td><td>NN<\/td><td>noun, singular or mass<\/td><td>Number=sing<\/td><\/tr>\n<tr><td>NOUN<\/td><td><\/td><td>NNS<\/td><td>noun, plural<\/td><td>Number=plur<\/td><\/tr>\n<tr><td>NOUN<\/td><td><\/td><td>WP<\/td><td>wh-pronoun, personal<\/td><td>PronType=int rel<\/td><\/tr>\n<tr><td>NUM<\/td><td>numeral<\/td><td>CD<\/td><td>cardinal number<\/td><td>NumType=card<\/td><\/tr>\n<tr><td>PART<\/td><td>particle<\/td><td>POS<\/td><td>possessive ending<\/td><td>Poss=yes<\/td><\/tr>\n<tr><td>PART<\/td><td><\/td><td>RP<\/td><td>adverb, particle<\/td><td><\/td><\/tr>\n<tr><td>PART<\/td><td><\/td><td>TO<\/td><td>infinitival to<\/td><td>PartType=inf VerbForm=inf<\/td><\/tr>\n<tr><td>PRON<\/td><td>pronoun<\/td><td>PRP<\/td><td>pronoun, personal<\/td><td>PronType=prs<\/td><\/tr>\n<tr><td>PROPN<\/td><td>proper noun<\/td><td>NNP<\/td><td>noun, proper singular<\/td><td>NounType=prop Number=sign<\/td><\/tr>\n<tr><td>PROPN<\/td><td><\/td><td>NNPS<\/td><td>noun, proper plural<\/td><td>NounType=prop Number=plur<\/td><\/tr>\n<tr><td>PUNCT<\/td><td>punctuation<\/td><td>-LRB-<\/td><td>left round bracket<\/td><td>PunctType=brck PunctSide=ini<\/td><\/tr>\n<tr><td>PUNCT<\/td><td><\/td><td>-RRB-<\/td><td>right round bracket<\/td><td>PunctType=brck PunctSide=fin<\/td><\/tr>\n<tr><td>PUNCT<\/td><td><\/td><td>,<\/td><td>punctuation mark, comma<\/td><td>PunctType=comm<\/td><\/tr>\n<tr><td>PUNCT<\/td><td><\/td><td>:<\/td><td>punctuation mark, colon or ellipsis<\/td><td><\/td><\/tr>\n<tr><td>PUNCT<\/td><td><\/td><td>.<\/td><td>punctuation mark, sentence closer<\/td><td>PunctType=peri<\/td><\/tr>\n<tr><td>PUNCT<\/td><td><\/td><td>''<\/td><td>closing quotation mark<\/td><td>PunctType=quot PunctSide=fin<\/td><\/tr>\n<tr><td>PUNCT<\/td><td><\/td><td>\"\"<\/td><td>closing quotation mark<\/td><td>PunctType=quot PunctSide=fin<\/td><\/tr>\n<tr><td>PUNCT<\/td><td><\/td><td>``<\/td><td>opening quotation mark<\/td><td>PunctType=quot PunctSide=ini<\/td><\/tr>\n<tr><td>PUNCT<\/td><td><\/td><td>HYPH<\/td><td>punctuation mark, hyphen<\/td><td>PunctType=dash<\/td><\/tr>\n<tr><td>PUNCT<\/td><td><\/td><td>LS<\/td><td>list item marker<\/td><td>NumType=ord<\/td><\/tr>\n<tr><td>PUNCT<\/td><td><\/td><td>NFP<\/td><td>superfluous punctuation<\/td><td><\/td><\/tr>\n<tr><td>SYM<\/td><td>symbol<\/td><td>#<\/td><td>symbol, number sign<\/td><td>SymType=numbersign<\/td><\/tr>\n<tr><td>SYM<\/td><td><\/td><td>\\$<\/td><td>symbol, currency<\/td><td>SymType=currency<\/td><\/tr>\n<tr><td>SYM<\/td><td><\/td><td>SYM<\/td><td>symbol<\/td><td><\/td><\/tr>\n<tr><td>VERB<\/td><td>verb<\/td><td>BES<\/td><td>auxiliary \"be\"<\/td><td><\/td><\/tr>\n<tr><td>VERB<\/td><td><\/td><td>HVS<\/td><td>forms of \"have\"<\/td><td><\/td><\/tr>\n<tr><td>VERB<\/td><td><\/td><td>MD<\/td><td>verb, modal auxiliary<\/td><td>VerbType=mod<\/td><\/tr>\n<tr><td>VERB<\/td><td><\/td><td>VB<\/td><td>verb, base form<\/td><td>VerbForm=inf<\/td><\/tr>\n<tr><td>VERB<\/td><td><\/td><td>VBD<\/td><td>verb, past tense<\/td><td>VerbForm=fin Tense=past<\/td><\/tr>\n<tr><td>VERB<\/td><td><\/td><td>VBG<\/td><td>verb, gerund or present participle<\/td><td>VerbForm=part Tense=pres Aspect=prog<\/td><\/tr>\n<tr><td>VERB<\/td><td><\/td><td>VBN<\/td><td>verb, past participle<\/td><td>VerbForm=part Tense=past Aspect=perf<\/td><\/tr>\n<tr><td>VERB<\/td><td><\/td><td>VBP<\/td><td>verb, non-3rd person singular present<\/td><td>VerbForm=fin Tense=pres<\/td><\/tr>\n<tr><td>VERB<\/td><td><\/td><td>VBZ<\/td><td>verb, 3rd person singular present<\/td><td>VerbForm=fin Tense=pres Number=sing Person=3<\/td><\/tr>\n<tr><td>X<\/td><td>other<\/td><td>ADD<\/td><td>email<\/td><td><\/td><\/tr>\n<tr><td>X<\/td><td><\/td><td>FW<\/td><td>foreign word<\/td><td>Foreign=yes<\/td><\/tr>\n<tr><td>X<\/td><td><\/td><td>GW<\/td><td>additional word in multi-word expression<\/td><td><\/td><\/tr>\n<tr><td>X<\/td><td><\/td><td>XX<\/td><td>unknown<\/td><td><\/td><\/tr>\n<tr><td>SPACE<\/td><td>space<\/td><td>_SP<\/td><td>space<\/td><td><\/td><\/tr>\n<tr><td><\/td><td><\/td><td>NIL<\/td><td>missing tag<\/td><td><\/td><\/tr>\n<\/table>","805f7162":"## Evaluating\n\n`Word2Vec` training is an unsupervised task, there's no good way to objectively evaluate the result.","7b472d30":"## Built-in Visualizers","bbbddc43":"## Visualizing the entity recongnizer","364a45b9":"## Working with POS Tags\n\nIn english language, the same string of characters can have different meanings, even within the same sentence. For this reason, morphology is important.","4f660272":"## FastText","b55710aa":"## Stop Words\n\n- Words like 'a' and 'the' appear so frequently that they don't require tagging as thoroughly as nouns, verbs and modifiers. We call these stop words, and they can be filtered from text to be processed. Spacy holds a built-in list of some `326` English stop words.","8bf3fc18":"## Lemmatization\n\n- In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a morphological analysis to words. The lemma of 'was' is 'be' and the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence. Lemmatization is typically seen as much more informative than simple stemming, which is why Spacy has opted to only have Lemmatization available instead of Stemming. "}}