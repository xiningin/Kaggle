{"cell_type":{"b414adbf":"code","08e321bd":"code","86b7944f":"code","209aabfa":"code","39053cc8":"code","d206ee0b":"code","94b1f9fe":"code","b3a85d2e":"code","570c864b":"code","f95f872a":"code","af17fae2":"code","3c949337":"code","e121b05e":"code","9d70aa06":"code","286989d0":"code","34829255":"code","b1c0f58a":"code","f5f8ac21":"code","20307caa":"code","01e1a45d":"code","aa1bb8d2":"code","8e85f603":"code","2314d7fa":"code","8f8152ab":"code","7c78dee4":"code","ee11712c":"code","3534135a":"code","35495c5e":"code","3f95f288":"code","a8bf2c95":"code","7ec27ad6":"code","3a1bd532":"code","c0ed495f":"code","932c4c10":"code","ae3dca3c":"code","41452e81":"code","56d07a56":"code","e22a6d92":"markdown","c98ac061":"markdown","71e838ba":"markdown","6f3982b0":"markdown"},"source":{"b414adbf":"from tensorflow import keras\nimport tensorflow as tf\nimport pandas as pd\nimport os\nimport re","08e321bd":"male_data = pd.read_csv('..\/input\/classify-nationalities\/Indian-Male-Names.csv')\nfemale_data = pd.read_csv('..\/input\/classify-nationalities\/Indian-Female-Names.csv')","86b7944f":"female_data.head()","209aabfa":"repl_list = ['s\/o','d\/o','w\/o','\/','&',',','-']\n\ndef clean_data(name):\n\tname = str(name).lower()\n\tname = (''.join(i for i in name if ord(i)<128)).strip()\n\tfor repl in repl_list:\n\t\tname = name.replace(repl,\" \")\n\tif '@' in name:\n\t\tpos = name.find('@')\n\t\tname = name[:pos].strip()\n\tname = name.split(\" \")\n\tname = \" \".join([each.strip() for each in name])\n\treturn name\n\ndef remove_records(merged_data):\n\tmerged_data['delete'] = 0\n\tmerged_data.loc[merged_data['name'].str.find('with') != -1,'delete'] = 1\t\n\tmerged_data.loc[merged_data['count_words']>=5,'delete']=1\n\tmerged_data.loc[merged_data['count_words']==0,'delete']=1\n\tmerged_data.loc[merged_data['name'].str.contains(r'\\d') == True,'delete']=1\n\tcleaned_data = merged_data[merged_data.delete==0]\n\treturn cleaned_data\n\nmerged_data = pd.concat((male_data,female_data),axis=0)\n\nmerged_data['name'] = merged_data['name'].apply(clean_data)\nmerged_data['count_words'] = merged_data['name'].str.split().apply(len)\n\ncleaned_data = remove_records(merged_data)\n\nindian_cleaned_data = cleaned_data[['name','count_words']].drop_duplicates(subset='name',keep='first')\nindian_cleaned_data['label'] = 'indian'\n\nlen(indian_cleaned_data)","39053cc8":"cleaned_data.head()","d206ee0b":"merged_data.race.value_counts()","94b1f9fe":"indian_cleaned_data.head()","b3a85d2e":"!pip install Faker","570c864b":"from faker import Faker\nfake = Faker('en_US')\nfake.name()","f95f872a":"from faker import Faker\nimport random\nreq = 15000\nnon_indian_names = []\n\nlangs = ['ar_EG','bs_BA','de_DE','dk_DK','en_AU','en_CA','en_GB',\n'en_IN','en_NZ','en_US','it_IT','no_NO','ro_RO']\n\nfor i in range(0,req):\n\tlng_indx = random.randint(0,len(langs)-1)\n\tfake = Faker(langs[lng_indx])\n\tnon_indian_names.append(fake.name().lower())\n\nnon_indian_names_orig = list(set(non_indian_names))\n","af17fae2":"len(non_indian_names_orig)","3c949337":"non_indian_data = pd.DataFrame({'name':non_indian_names_orig})\nnon_indian_data['count_words'] = non_indian_data['name'].str.split().apply(len)\nnon_indian_data.head()","e121b05e":"indian_cleaned_data['count_words'].value_counts()","9d70aa06":"non_indian_data['count_words'].value_counts()","286989d0":"two_word_names = non_indian_data[non_indian_data['count_words']==2]['name']\none_word_req = 5000\nnames_one_two_words = [each.split()[0] for each in two_word_names[:one_word_req]] + list(two_word_names[one_word_req:])\ncount_words = [1] * one_word_req + [2] * len(two_word_names[one_word_req:])\nnot_two_words_pd  = non_indian_data[non_indian_data['count_words']!=2]\none_two_words_pd = pd.DataFrame({'name':names_one_two_words,'count_words':count_words})\nnon_indian_data = pd.concat((not_two_words_pd,one_two_words_pd),axis=0)\nnon_indian_data['count_words'].value_counts()\nnon_indian_data['label'] = 'non_indian'\nnon_indian_data = non_indian_data[non_indian_data['count_words']<5]\nnon_indian_data['count_words'].value_counts()","34829255":"full_data = pd.concat((non_indian_data[['name','label']],indian_cleaned_data[['name','label']]),axis=0)\nname_data = full_data.sample(frac=1)\nname_data.head()","b1c0f58a":"name_data['label'].value_counts()","f5f8ac21":"from sklearn.model_selection import train_test_split\nX = name_data['name'].astype(str)\nY = name_data['label']\ntrain_names,test_names,train_labels,test_labels = train_test_split(X,Y,test_size=0.20,random_state =42,stratify=Y)","20307caa":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix,classification_report\n\nvectorizer = CountVectorizer()\nX_ = vectorizer.fit_transform(train_names.values.astype('U'))\nlen(vectorizer.get_feature_names())\n","01e1a45d":"model = MultinomialNB()\nmodel.fit(X_,train_labels)\n\nX_test = vectorizer.transform(test_names.values.astype('U'))\n\ntest_predicted = model.predict(X_test)\n\nprint(classification_report(test_labels,test_predicted))","aa1bb8d2":"check_new_names = ['lalitha','tyson','shailaja','shyamala','vishwanathan','ramanujam','conan','kryslovsky',\n'ratnani','diego','kakoli','shreyas','brayden','shanon']\n\nX_new = vectorizer.transform(check_new_names)\npredictions_nb_cv = model.predict(X_new)\ntest = pd.DataFrame({'names':check_new_names,'predictions_nb_cv':predictions_nb_cv}) \ntest","8e85f603":"from tokenizers import ByteLevelBPETokenizer,CharBPETokenizer,SentencePieceBPETokenizer,BertWordPieceTokenizer\n\n\nf = open(\"train_names.txt\",\"w\")\nfor each in list(train_names):\n\tf.write(str(each))\n\tf.write(\"\\n\")\n\nf.close()\n\ntokenizer = SentencePieceBPETokenizer()\ntokenizer.train([\".\/train_names.txt\"],vocab_size=2000,min_frequency=2)\n\nencoded_tokens = [tokenizer.encode(str(each)).tokens for each in train_names]\nencoded_tokens_test = [tokenizer.encode(str(each)).tokens for each in test_names]\n\nencoded_tokens = [\" \".join(each)  for each in encoded_tokens]\nencoded_tokens_test = [\" \".join(each)  for each in encoded_tokens_test]\n\nencoded_tokens[:10]","2314d7fa":"tfidf_vect = TfidfVectorizer()\nX_ = tfidf_vect.fit_transform(encoded_tokens)\nlen(tfidf_vect.get_feature_names())\n\nmodel = MultinomialNB()\nmodel.fit(X_,train_labels)\n\nX_test = tfidf_vect.transform(encoded_tokens_test)\n\ntest_predicted = model.predict(X_test)\n\nprint(classification_report(test_labels,test_predicted))","8f8152ab":"encoded_tokens_check = [tokenizer.encode(str(each).lower()).tokens for each in check_new_names]\nencoded_tokens_check = [\" \".join(each)  for each in encoded_tokens_check]\n\nX_new = tfidf_vect.transform(encoded_tokens_check)\npredictions_nb_enc_tf = model.predict(X_new)\ntest = pd.DataFrame({'names':check_new_names,'predictions_nb_enc_tf':predictions_nb_enc_tf}) \ntest","7c78dee4":"from keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers.embeddings import Embedding\nfrom keras.utils import to_categorical\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.callbacks import Callback\nnp.random.seed(42)","ee11712c":"def char_encoded_representation(data,tokenizer,vocab_size,max_len):\n\tchar_index_sentences = tokenizer.texts_to_sequences(data)\n\tsequences = [to_categorical(x, num_classes=vocab_size) for x in char_index_sentences]\n\tX = sequence.pad_sequences(sequences, maxlen=max_len)\n\treturn X","3534135a":"max_len = max([len(str(each)) for each in train_names])\n# mapping = get_char_mapping(train_names)\n# vocab_size = len(mapping)\n\ntok = Tokenizer(char_level=True)\ntok.fit_on_texts(train_names)\nvocab_size = len(tok.word_index) + 1\nX_train = char_encoded_representation(train_names,tok,vocab_size,max_len)\nX_train.shape","35495c5e":"X_test = char_encoded_representation(test_names,tok,vocab_size,max_len)\nX_test.shape","3f95f288":"le = LabelEncoder()\nle.fit(train_labels)\ny_train = le.transform(train_labels)\ny_test = le.transform(test_labels)\n","a8bf2c95":"# Model Specification\n\n\ndef build_model(hidden_units,max_len,vocab_size):\n\tmodel = Sequential()\n\t# model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n\tmodel.add(LSTM(hidden_units,input_shape=(max_len,vocab_size)))\n\tmodel.add(Dense(1, activation='sigmoid'))\n\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\tprint(model.summary())\n\treturn model\n\nclass myCallback(Callback): \n\tdef __init__(self,X_test,y_test):\n\t\tself.X_test = X_test\n\t\tself.y_test = y_test\n\tdef on_epoch_end(self, epoch, logs={}): \n\t\tloss,acc = model.evaluate(self.X_test, self.y_test, verbose=0)\n\t\tprint('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))","7ec27ad6":"model = build_model(100,max_len,vocab_size)\nmodel.fit(X_train, y_train, epochs=50, batch_size=64,callbacks=myCallback(X_test,y_test))","3a1bd532":"X_predict = char_encoded_representation(check_new_names,tok,vocab_size,max_len)\n\npredictions_prob = model.predict(X_predict)\npredictions = np.array(predictions_prob)\npredictions[predictions > 0.5] = 1\npredictions[predictions <= 0.5] = 0\npredictions = np.squeeze(predictions)\npredictions_lstm_char = le.inverse_transform(list(predictions.astype(int)))\ntest = pd.DataFrame({'names':check_new_names,'predictions_lstm_char':predictions_lstm_char}) \ntest","c0ed495f":"from tokenizers import ByteLevelBPETokenizer,CharBPETokenizer,SentencePieceBPETokenizer,BertWordPieceTokenizer\nvocab_size = 200\n\ntokenizer = SentencePieceBPETokenizer()\ntokenizer.train([\".\/train_names.txt\"],vocab_size=vocab_size,min_frequency=2)\n\n\ndef sent_piece_encoded_representation(data,tokenizer):\n\tencoded_tokens = [tokenizer.encode(str(each)).ids for each in data]\n\tsequences = [to_categorical(x, num_classes=vocab_size) for x in encoded_tokens]\n\tX = sequence.pad_sequences(sequences, maxlen=max_len)\n\treturn X","932c4c10":"max_len = max([len(str(each)) for each in train_names])\nle = LabelEncoder()\nle.fit(train_labels)\ny_train = le.transform(train_labels)\ny_test = le.transform(test_labels)\n\n\nX_train = sent_piece_encoded_representation(train_names,tokenizer)\nX_train.shape","ae3dca3c":"X_test = sent_piece_encoded_representation(test_names,tokenizer)\nX_test.shape","41452e81":"model = build_model(100,max_len,vocab_size)\nmodel.fit(X_train, y_train, epochs=50, batch_size=64,callbacks=myCallback(X_test,y_test))\n","56d07a56":"X_predict = sent_piece_encoded_representation(check_new_names,tokenizer)\n\npredictions_prob = model.predict(X_predict)\npredictions = np.array(predictions_prob)\npredictions[np.where(predictions > 0.5)[0]] = 1\npredictions[np.where(predictions <= 0.5)[0]] = 0\npredictions = np.squeeze(predictions)\n\npredictions_lstm_sent_enc = le.inverse_transform(list(predictions.astype(int)))\ntest = pd.DataFrame({'names':check_new_names,'predictions_lstm_sent_enc':predictions_lstm_sent_enc}) \ntest","e22a6d92":"# Naive Bayes with Count Vectorizer","c98ac061":"# Character based encoding with an LSTM model","71e838ba":"# Naive Bayes with SentencePiece Embedding","6f3982b0":"# SentencePiece Encoding with LSTM"}}