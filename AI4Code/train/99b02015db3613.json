{"cell_type":{"5dede87b":"code","d59462be":"code","45e64621":"code","4bf11f86":"code","d15acc73":"code","9c120b85":"code","262b79ed":"code","bda8d9ec":"code","de081d38":"code","7b3a3cfe":"code","54a9abb6":"code","02b202b0":"code","6b4d95df":"code","73a90a1f":"code","582409d5":"code","d0e10690":"code","d8368b41":"code","25982045":"code","965604c5":"code","9f2f7f1a":"code","98bd1156":"code","b105eab5":"code","b50dae10":"code","1d7950b8":"code","edabdc3e":"markdown","a0c71f14":"markdown","2ecee036":"markdown","6cf12910":"markdown","fde22b77":"markdown","6575ef12":"markdown","43ce19e4":"markdown","839bc5c4":"markdown","1376273b":"markdown","0a16d1da":"markdown","096ca1e3":"markdown","dab70f9b":"markdown","cc3242dc":"markdown"},"source":{"5dede87b":"import pandas as pd\nimport numpy as np  \nimport matplotlib.pyplot as plt  \n#import seaborn as seabornInstance \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.ensemble import AdaBoostClassifier\nimport sklearn.metrics as metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder","d59462be":"dflabels = pd.read_csv(\"..\/input\/crmchurn\/CRM Churn Labels Shared.tsv\", sep=\"\\t\", header = None)\ndf = pd.read_csv(\"..\/input\/crmchurn\/CRM Dataset Shared.tsv\", sep=\"\\t\")","45e64621":"x = df.dtypes","4bf11f86":"df.head()","d15acc73":"df = df.fillna(0)","9c120b85":"dtypes = pd.DataFrame([list(df.columns),list(df.dtypes)]).T\ndtypes.columns = [\"column\",\"type\"]","262b79ed":"enccol = []\nfor index,row in dtypes.iterrows():\n    if row[\"type\"] == \"object\":\n        enccol.append(row[\"column\"])","bda8d9ec":"df[enccol] = df[enccol].astype(\"string\")","de081d38":"class MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)","7b3a3cfe":"df.dtypes","54a9abb6":"df = MultiColumnLabelEncoder(columns = enccol).fit_transform(df)","02b202b0":"dflabels.head()","6b4d95df":"df.shape,dflabels.shape","73a90a1f":"dflabels.columns = [\"label\"]","582409d5":"dffinal = pd.concat([df, dflabels.reindex(df.index)], axis=1)","d0e10690":"dffinal.head()","d8368b41":"dffinal.shape","25982045":"X_train, X_test, y_train, y_test = train_test_split(df, dflabels, test_size=0.3)","965604c5":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","9f2f7f1a":"model1 = LogisticRegression(solver=\"liblinear\")\nmodel1.fit(X_train, y_train)","98bd1156":"model2 = AdaBoostClassifier()\nmodel2.fit(X_train, y_train)","b105eab5":"def evaluate(model,X_test,y_test):\n    probs = model.predict_proba(X_test)\n    preds = probs[:,1]\n    predicted_classes = model1.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test,predicted_classes)\n    confusion = metrics.confusion_matrix(y_test, model.predict(X_test))\n    TP = confusion[1, 1]\n    TN = confusion[0, 0]\n    FP = confusion[0, 1]\n    FN = confusion[1, 0]\n    classification_error = (FP + FN) \/ float(TP + TN + FP + FN)\n    sensitivity = TP \/ float(FN + TP)\n    specificity = TN \/ (TN + FP)\n    false_positive_rate = FP \/ float(TN + FP)\n    precision = TP \/ float(TP + FP)\n    print(\"Accuracy:\",accuracy)\n    print(\"\\nConfusion Matrix:\\n\",confusion)    \n    print(\"\\nROC_AUC_Score:\",(metrics.roc_auc_score(y_test, preds)))\n    print(\"\\nClassification Error:\", classification_error)\n    print(\"\\nSensitivity:\", sensitivity)\n    print(\"\\nSpecificity:\", specificity)\n    print(\"\\nFalse Positive Rate:\", false_positive_rate)\n    print(\"\\nPrecision:\", precision)\n    # calculate the fpr and tpr for all thresholds of the classification\n    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    return None","b50dae10":"evaluate(model1,X_test,y_test)","1d7950b8":"evaluate(model2,X_test,y_test)","edabdc3e":"Splitting test and train datasets.","a0c71f14":"Filling NaN with 0 in df.","2ecee036":"Creating our Logistic Regression Classifier and fitting it.","6cf12910":"Writing an \"evaluate\" function that takes in model, X_test and y_test and print out the metrics that we want to compare.","fde22b77":"Creating a multi column label encoder with the help of the base LabelEncoder from sklearn.preprocessing.\nRead more about classes in Python: https:\/\/www.geeksforgeeks.org\/python-classes-and-objects\/","6575ef12":"Joining both the datasets to prepare our test and train datasets.","43ce19e4":"Creating our Logistic Regression Classifier and fitting it.","839bc5c4":"Checking the types of columns and converting any object types to strings for further label encoding.","1376273b":"Importing the required libraries:\n\n* pandas: For reading and manipulating our dataset\n* numpy: Used for working on arrays\n* matplotlib: For plotting\n* sklearn.model_selection: For importing train_test_split\n* sklearn.metrics: Importing this will enable us to use different metrics for evaluating our models\n* sklearn.linear_model: For importing our Logistic Regression Classifier\n* sklearn.ensemble: For importing our AdaBoost Classifier\n* sklearn.preprocessing: For label encoding using LabelEncoder","0a16d1da":"Based on the primary performance metric, Precision, it shows that the Two-Class Boosted Decision Tree algorithm outperforms the Two-Class Logistic Regression algorithm.","096ca1e3":"Checking head and shape.","dab70f9b":"Reading the CRM Churn Dataset and Labels Dataset.\nUsing sep = \"\\t\" for specifying a tab delimiter and header = None for Labels dataset so that the first row is not automatically assumed to be a header.","cc3242dc":"# Lab 8: Compare the performance of the various two-class classifiers\n\nIn this lab, we will be compare the performance of two binary classifiers: Two-Class Boosted Decision Tree and Two-Class Logistic Regression for predicting customer churn. The goal is to run an expensive marketing campaign for high risk customers; thus, the precision metric is going to be key in evaluating performance of these two algorithms. "}}