{"cell_type":{"39444b6f":"code","771c7394":"code","4ec1aa35":"code","8ad5ee63":"code","34208de0":"code","6e13902f":"code","9b503fa6":"code","9f9340b6":"code","9d72599e":"code","8505d929":"code","cb18074b":"code","be9908a8":"code","d271f2f2":"code","253b051b":"code","79b97e28":"code","b49f0e6d":"code","94c9d34d":"code","0638cf24":"code","762a07a4":"code","81d95eba":"code","5e4cb573":"code","70f59f11":"code","e4ce71c7":"code","94c446fa":"markdown","96ef79a3":"markdown","06ff3aca":"markdown","7e8c7c67":"markdown","9f1504ba":"markdown","a48e6cf2":"markdown","4ac1e552":"markdown","3597ba25":"markdown","79f8f671":"markdown","ce9ff58a":"markdown","9324cccb":"markdown"},"source":{"39444b6f":"#importing libraries\nimport numpy as np \nimport pandas as pd \nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nimport time\nimport warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","771c7394":"#reading data\ntrain = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")","4ec1aa35":"#checking what it looks like\ntrain.head(5)","8ad5ee63":"test.head(5)","34208de0":"print(train.isna().sum().sort_values(ascending = False))","6e13902f":"print(test.isna().sum().sort_values(ascending = False))","9b503fa6":"# !rm -r kuma_utils\n!git clone https:\/\/github.com\/analokmaus\/kuma_utils.git","9f9340b6":"import sys\nsys.path.append(\"kuma_utils\/\")\nfrom kuma_utils.preprocessing.imputer import LGBMImputer","9d72599e":"#Defining features of Songs\nfea = [\n    \"song_duration_ms\",\n    \"acousticness\",\n    \"danceability\",\n    \"energy\",\n    \"instrumentalness\",\n    \"key\",\n    \"liveness\",\n    \"loudness\",\n    \"audio_mode\",\n    \"speechiness\",\n    \"tempo\",\n    \"time_signature\",\n    \"audio_valence\",\n]","8505d929":"%%time\nlgbm_imtr = LGBMImputer(n_iter=500)\n\ntrain_iterimp = lgbm_imtr.fit_transform(train[fea])\ntest_iterimp = lgbm_imtr.transform(test[fea])\n\n# Create train test imputed dataframe\ntrain_imp_df = pd.DataFrame(train_iterimp, columns=fea)\ntest_imp_df = pd.DataFrame(test_iterimp, columns=fea)","cb18074b":"#adding the Target column from train data\ntrain_imp_df['song_popularity'] = train['song_popularity']","be9908a8":"train_imp_df.head(5)","d271f2f2":"train = train_imp_df\ntest = test_imp_df","253b051b":"#Get the Independent and Dependent Features\nX= train.iloc[:,0:13]\ny= train.iloc[:,13]","79b97e28":"#spliiting train data for Optuna\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42, shuffle=True)","b49f0e6d":"#I wont be running optuna for this notebook all over again, feel free to remove the comments and try\n\n'''import optuna\nfrom optuna.samplers import GridSampler'''","94c9d34d":"'''def run(trial): \n    learning_rate     = trial.suggest_float(\"learning_rate\", 0.02, 0.30)\n    num_leaves        = trial.suggest_int(\"num_leaves\", 5,50)\n    max_depth          = trial.suggest_int(\"max_depth\", 2,20)\n    min_child_weight   = trial.suggest_float(\"min_child_weight\", 1,10)\n    colsample_bytree   = trial.suggest_float(\"colsample_bytree\", 0.3, 0.9)\n    n_estimators       = trial.suggest_int(\"n_estimators\", 150,250)\n    random_state       = trial.suggest_int(\"random_state\", 42,100)\n    subsample          = trial.suggest_float(\"subsample\", 0.1, 2.0)\n    max_bin            = trial.suggest_int(\"max_bin\", 800,1000)\n    feature_fraction   = trial.suggest_float(\"feature_fraction\", 0.20, 0.25)\n    bagging_fraction   = trial.suggest_float(\"bagging_fraction\", 0.60, 0.90)\n    min_child_samples  =trial.suggest_int(\"min_child_samples\", 100,500)\n    reg_alpha          =trial.suggest_float(\"reg_alpha\", 10.00, 80.00)\n    reg_lambda         =trial.suggest_float(\"reg_lambda\", 0.00, 100.00)\n    min_data_per_group  =trial.suggest_int(\"min_data_per_group\",100,300)\n    cat_smooth          = trial.suggest_float(\"cat_smooth \", 10.0, 100.0)\n    cat_l2              =trial.suggest_float(\"cat_l2\" , 10.0, 60.0)\n    \n    \n    model = LGBMClassifier(objective= 'binary',\n                            boosting_type = 'gbdt' ,\n                           bagging_freq =1,\n                           learning_rate =learning_rate ,\n                           num_leaves  =num_leaves  ,\n                           max_depth  =max_depth  ,\n                           min_child_weight=min_child_weight,\n                           colsample_bytree=colsample_bytree,\n                           n_estimators =n_estimators ,\n                           random_state=random_state,\n                           subsample =subsample ,\n                           max_bin  =max_bin  ,\n                           feature_fraction=feature_fraction,\n                           bagging_fraction =bagging_fraction ,\n                           min_child_samples =min_child_samples ,\n                           reg_alpha=reg_alpha,\n                            reg_lambda= reg_lambda,\n                            min_data_per_group=  min_data_per_group,\n                           cat_smooth    =cat_smooth    ,\n                           cat_l2 =cat_l2 \n                          )\n                           \n    model.fit(X_train, y_train)\n    \n    preds_valid = model.predict_proba(X_test)[:, 1]\n    auc = roc_auc_score(y_test,  preds_valid)\n    \n    return auc'''","0638cf24":"'''study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(run, n_trials=250)'''","762a07a4":"'''par=study.best_params\npar'''","81d95eba":"par= {'learning_rate': 0.10816849925824959,\n 'num_leaves': 8,\n 'max_depth': 12,\n 'min_child_weight': 1.016784615713669,\n 'colsample_bytree': 0.5549375416177718,\n 'n_estimators': 199,\n 'random_state': 95,\n 'subsample': 1.8044406658061907,\n 'max_bin': 853,\n 'feature_fraction': 0.2152346380315153,\n 'bagging_fraction': 0.7010922250759225,\n 'min_child_samples': 260,\n 'reg_alpha': 11.265876707930657,\n 'reg_lambda': 58.01908464196338,\n 'min_data_per_group': 264,\n 'cat_smooth ': 66.78604218691284,\n 'cat_l2': 44.80866022515556}","5e4cb573":"TARGET = 'song_popularity'\nFEATURES = [col for col in train.columns if col not in ['id', TARGET]]\nRANDOM_STATE = 42","70f59f11":"FOLDS=10\nlgb_predictions = 0\nlgb_scores = []\n\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[FEATURES], train[TARGET])):\n    \n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    \n    X_train, X_valid = train.iloc[train_idx][FEATURES], train.iloc[valid_idx][FEATURES]\n    y_train , y_valid = train[TARGET].iloc[train_idx] , train[TARGET].iloc[valid_idx]\n    \n    model = LGBMClassifier(**par)\n    model.fit(X_train, y_train)\n    \n    preds_valid = model.predict_proba(X_valid)[:, 1]\n    auc = roc_auc_score(y_valid,  preds_valid)\n    lgb_scores.append(auc)\n    run_time = time.time() - start_time\n    \n    print(f\"Fold={fold+1}, AUC score: {auc:.2f}, Run Time: {run_time:.2f}s\")\n    test_preds = model.predict_proba(test[FEATURES])[:, 1]\n    lgb_predictions += test_preds\/FOLDS\n    \nprint(\"Mean AUC :\", np.mean(lgb_scores))","e4ce71c7":"#Submission\n'''lgb_submission = submission.copy()\nlgb_submission['song_popularity'] = lgb_predictions\nlgb_submission.to_csv(\"lgb-subs.csv\",index=False)\nlgb_submission.head()'''","94c446fa":"# **Some useful resources that helped shape my tuning journey:**\n* https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\n* https:\/\/towardsdatascience.com\/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\n* https:\/\/neptune.ai\/blog\/lightgbm-parameters-guide\n* https:\/\/github.com\/Microsoft\/LightGBM\/issues\/695\n","96ef79a3":"<center><h2> <span style=\"font-family:Times New Roman;font-size:30px;\"> My recipe \ud83e\uded5 was a mix of Null value imputation \ud83c\udfaf, Hyperparameter Optimization using Optuna \ud83d\udee0\ufe0f & StratifiedKFold Cross Validation!\ud83c\udfcb\ud83c\udffe\u200d\u2640\ufe0f <\/span><h2><\/center>\n","06ff3aca":"<center><h3><span style=\"color:brown;font-size:40px;font-family:Times New Roman;\" > \u26a1 Dealing with NULL values \u26a1  <\/span><\/h3><\/center>\n\n<center><span style=\"font-size:30px;font-family:Times New Roman;\" >\"Huge gratitude for Abhishek & Rob \ud83d\ude4c\ud83c\udffd for <a href=\"https:\/\/www.youtube.com\/watch?v=EYySNJU8qR0\"> this <\/a> tutorial\n\n<center>I tried all these Imputers and LightGBM Imputer gave best results for me (<a href=\"https:\/\/github.com\/analokmaus\/kuma_utils\/blob\/master\/preprocessing\/imputer.py\">Source<\/a>) <\/center>","7e8c7c67":"<center><h3><span style=\"color:brown;font-size:40px;font-family:Times New Roman;\" > \ud83d\udc40 Checking Null values \ud83d\udc40  <\/span><\/h3><\/center>","9f1504ba":"<center><h3><span style=\"color:brown;font-size:40px;font-family:Times New Roman;\" >\ud83d\udcad Need for scaling? \ud83d\udcad <\/span><\/h3><\/center>\n\n<center><span style=\"font-size:30px;font-family:Times New Roman;\" >After trying a couple of Scaling methods (because thats what we do, right?), I observed that they did not have much impact on the results. Decision trees and ensemble methods do not require feature scaling to be performed as they are not sensitive to the the variance in the data.( <a href=\"https:\/\/towardsdatascience.com\/do-decision-trees-need-feature-scaling-97809eaa60c6\"> additional resource <\/a> )","a48e6cf2":"<center><h3><span style=\"color:brown;font-size:40px;font-family:Times New Roman;\" >\ud83c\udfa1 Modeling \ud83c\udfa1 <\/span><\/h3><\/center>","4ac1e552":"<center><h3><span style=\"color:brown;font-size:40px;font-family:Times New Roman;\" > \u2699\ufe0f Hyperparameter Optimization \u2699\ufe0f <\/span><\/h3><\/center>\n\n\n\n<center><span style=\"font-size:25px;font-family:Times New Roman;\">I tried GridSearch CV, RandomSearch & Optuna. GridSearchCV gave better results than RandomSearchCV but since GridSearchCV checks for all permutations of hyperparameters, it was taking a long time and I could not provide as many parameters as one would want. Therefore, I went with Optuna just like a True Kaggler XD <\/span><\/center>","3597ba25":"![](https:\/\/live.staticflickr.com\/5604\/15597430358_35f8d2b7cd_b.jpg)      \nabove image source: https:\/\/www.flickr.com\/photos\/128166878@N04\/15597430358","79f8f671":"<center><\/center>\n","ce9ff58a":"**The best paarmeters that I got after running 250 optuna trials are as follows:**","9324cccb":"<center><h1><span style=\"color:blue;font-size:60px;font-family:cursive;\" > Tardis to the moon \ud83d\ude80  <\/span><\/h1><\/center>\n\n<center><h2> <span style=\"font-family:Times New Roman;font-size:35px;\"> Hi, I am Mansi and my team (just me) callingTardis ranked 11th in the <a href=\"https:\/\/www.kaggle.com\/c\/song-popularity-prediction\/overview\"> Song Popularity Prediction<\/a> competition <\/span><h2><\/center>\n"}}