{"cell_type":{"596ade84":"code","60561b75":"code","ff59d1ef":"code","e5439790":"code","3c463f9b":"code","a2b3c698":"code","59a02685":"code","e33ec947":"code","bcecd681":"code","0753fd74":"code","dcc783f8":"code","85861188":"code","79d8f04d":"code","406549f5":"code","981bbae8":"code","3362ef6b":"code","4cd23964":"code","5b4429b8":"code","deb2fe8a":"code","3b8bb178":"code","5e39dd1c":"code","cfadfa50":"code","04c650c1":"code","20d97941":"code","93321902":"code","165ce18f":"code","ff1c2b92":"code","c1099fca":"code","2293fbdd":"code","7c4068ef":"code","1a79e58a":"code","602eb1c7":"code","1efd38df":"code","b8089c18":"code","3833a8f5":"code","3e766c23":"code","d8ef07f7":"code","7862ffd0":"code","6ed0c7d9":"code","11ef5277":"code","52ee5583":"code","67be80f9":"code","5a638a8c":"markdown","774b6191":"markdown","bca786a1":"markdown","11d71901":"markdown","6a612d28":"markdown","c8beb16a":"markdown","c735ed74":"markdown","8fabe119":"markdown","27168fed":"markdown","ba26845e":"markdown","def1100a":"markdown","b36abe85":"markdown","e68a47e9":"markdown"},"source":{"596ade84":"%pip install textstat\n%pip install eng_to_ipa\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nfrom math import ceil\nfrom collections import Counter\nimport nltk\nfrom nltk import pos_tag\nimport eng_to_ipa as ipa\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_squared_error as MSE\nimport re\nimport textstat\n# from tqdm import tqdm\n# import torch\n# import transformers as ppb","60561b75":"tt = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntn = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntn.head()","ff59d1ef":"totalen = tt.shape[0] + tn.shape[0]\n(totalen + 1)\/2, (totalen * (totalen - 1)) \/ 2","e5439790":"fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nsns.scatterplot(x=tn.target, y=tn.standard_error, s=10, color=\".15\")\nsns.kdeplot(x=tn.target, y=tn.standard_error, levels=5, color=\"r\", linewidths=1)\nplt.show()","3c463f9b":"tn = tn.drop(tn[tn.standard_error == 0].index[0])\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\nsns.scatterplot(x=tn.target, y=tn.standard_error, s=10, color=\".15\")\nsns.kdeplot(x=tn.target, y=tn.standard_error, levels=5, color=\"r\", linewidths=1)\nplt.show()","a2b3c698":"sorted_tn = tn.sort_values('target', ascending=False)\nsorted_tn.index = range(len(sorted_tn))\nsorted_tn","59a02685":"# these features was getting during the first running in txt_values() func\n# I uploaded its there (ttt.csv), because ipa.convert() works so long...\nsorted_tn[['t_length', 'coeff']] = pd.read_csv('..\/input\/length\/ttt.csv').drop('Unnamed: 0', axis=1)","e33ec947":"plt.plot(sorted_tn.target, sorted_tn.index, 'black')","bcecd681":"tn.license.unique()","0753fd74":"plt.plot(sorted_tn.standard_error, sorted_tn.index, 'g')","dcc783f8":"unq_lisence = dict()\nnumber = 1\nfor k in sorted_tn.license.unique():\n    unq_lisence[k] = number\n    number += 1\n\ndef licensed(value):\n    return unq_lisence[value]","85861188":"links = sorted_tn[sorted_tn.url_legal.isnull() == False].url_legal.unique()","79d8f04d":"links_set = set()\nfor link in links:\n    pattern = r':\/\/[a-z, 0-9 | \\.]+\/'\n    site = re.findall(pattern, link)[0][3:-1]\n    links_set.add(site)","406549f5":"lists_dict = {}\nnumber = 1\nfor k in sorted(links_set):\n    lists_dict[k] = number\n    number += 1    \nlists_dict","981bbae8":"def link_distrib(value):\n    \n    if isinstance(value, str):\n        for k, v in lists_dict.items():\n            if value[8:].startswith(k):\n                return v\n    return 0","3362ef6b":"fig, axes = plt.subplots(nrows = 1, ncols=2, figsize=(15, 5))\n\naxes[0].set(title='License distribution')\naxes[0].plot(sorted_tn.license.apply(licensed), [-i for i in range(len(sorted_tn))], 'ro')\naxes[1].set(title='URL distribution')\naxes[1].plot(sorted_tn.url_legal.apply(link_distrib), [-i for i in range(len(sorted_tn))], 'go')","4cd23964":"tt.url_legal.apply(link_distrib), lists_dict","5b4429b8":"k = 0\n\nindices5, indices12 = [], []\nfor url in sorted_tn.url_legal:\n    if isinstance(url, str):\n        if url[8:].startswith('en.wikipedia.org'):\n            indices5.append(k)\n        elif url[8:].startswith('www.africanstorybook.org'):\n            indices12.append(k)\n    k += 1\n\nsns.histplot(indices5, kde=True, color=\"r\", label='en.wikipedia.org')\nsns.histplot(indices12, kde=True, label='www.africanstorybook.org')\nplt.legend()","deb2fe8a":"def txt_values(text, test=False, with_tags=False):\n    \n    text = text.lower()\n       \n    pattern = r'[a-z]+-[a-z]+|[a-z]+'\n    sentences, words, tags = [], [], []\n    ind_1 = 0\n    for ind_2 in range(len(text)):\n        if text[ind_2] == '.' or text[ind_2] == '!' or text[ind_2] == '?':\n            if text[ind_2 - 1] != '.' or text[ind_2] != '!' or text[ind_2] != '?':\n                sentences.append(text[ind_1:ind_2])\n                ind_1 = ind_2\n    sentences = [sentences[0]] + list(map(lambda x: x[2:], sentences[1:]))\n        \n    for sentence in sentences:\n         words += re.findall(pattern, sentence)\n            \n    if test:\n#         I used ipa with train when was the first running. It locates in ttt.csv\n        transcripted = ''\n        for word in words:\n            transcripted += ipa.convert(word).replace(':', '').replace(\"'\", '').replace('.', '')    \n            \n    tagged_words = pos_tag(words)\n    tags = list(map(lambda x: x[1], tagged_words))\n    \n    if test:\n        t_text_length = len(transcripted)\n        \n    text_length = len(''.join(words))\n    nof_syllables = textstat.syllable_count(text)\n    nof_words = len(words)\n    nof_sentences = len(sentences)\n    avg_len_syllable = text_length \/ nof_syllables\n    avg_len_word = text_length \/ nof_words\n    avg_len_sentence = text_length \/ nof_sentences\n    gunning = textstat.gunning_fog(text)\n    ari = textstat.automated_readability_index(text)\n    coliau = textstat.coleman_liau_index(text)\n    lwf = textstat.linsear_write_formula(text)\n    FR_ease = 206.835 - 1.015 * (nof_words \/ nof_sentences) - 84.6 * (nof_syllables \/ nof_words)\n    FK_level = 0.39 * (nof_words \/ nof_sentences) + 11.8 * (nof_syllables \/ nof_words) - 15.59\n    \n    if with_tags:\n        tags_percentage = {}\n        for key, value in Counter(tags).items():\n            tags_percentage[key] = value \/ nof_words\n    \n        for tag in possible_tags:\n            if tag not in tags_percentage.keys():\n                tags_percentage[tag] = 0\n    \n        if test:\n            return pd.Series([text_length, t_text_length, t_text_length \/ text_length, nof_syllables,\n                              nof_words, nof_sentences, avg_len_syllable, avg_len_word,\n                              avg_len_sentence, FR_ease, FK_level, gunning, ari, coliau, lwf] + sorted(tags_percentage.values()))   \n    \n        return pd.Series([text_length, nof_syllables,\n                          nof_words, nof_sentences, avg_len_syllable, avg_len_word,\n                          avg_len_sentence, FR_ease, FK_level, gunning, ari, coliau, lwf] + sorted(tags_percentage.values()))\n    \n    if test:\n        return pd.Series([text_length, t_text_length, t_text_length \/ text_length, nof_syllables,\n                        nof_words, nof_sentences, avg_len_syllable, avg_len_word,\n                        avg_len_sentence, FR_ease, FK_level, gunning, ari, coliau, lwf])\n    \n    return pd.Series([text_length, nof_syllables,\n                          nof_words, nof_sentences, avg_len_syllable, avg_len_word,\n                          avg_len_sentence, FR_ease, FK_level, gunning, ari, coliau, lwf])\n        \n        \n\nfeatures = ['text_length', 't_length', 'coeff', 'nof_syllables', 'nof_words',\n           'nof_sentences', 'avg_len_syllable', 'avg_len_word',\n           'avg_len_sentence', 'FR_ease', 'FK_level', 'gunning', 'ari', \n           'coliau', 'lwf']\n\n\nsorted_tn[[features[0]] + features[3:]] = sorted_tn.excerpt.apply(txt_values)\ntt[features] = tt.excerpt.apply(txt_values, test=True)\n\n# sorted_tn[[features[0]] + features[3:] + possible_tags] = sorted_tn.excerpt.apply(txt_values, with_tags=True)\n# tt[features + possible_tags] = tt.excerpt.apply(txt_values, test=True, with_tags=True)","3b8bb178":"assert len(sorted_tn.columns) - 2 == len(tt.columns), 'Strange'","5e39dd1c":"def show_diffs(dataframe, features, figsize=(20, 20), reversed_inds=True):\n    \n    n = -1 if reversed_inds else 1\n    x = ceil(np.sqrt(len(features)))\n    fig, axes = plt.subplots(nrows=x, ncols=x, figsize=figsize)\n    for i in range(len(features)):\n        axes[i \/\/ x][i % x].set(title='Feature: ' + features[i])\n        axes[i \/\/ x][i % x].plot(dataframe[features[i]], n * dataframe.index)\n    \n    return plt.plot()","cfadfa50":"# reverse index for better understanding of dependencies\nshow_diffs(sorted_tn, features)","04c650c1":"# first text, the easiest one\nsorted_tn.loc[0].excerpt","20d97941":"# the hardest\nsorted_tn.loc[sorted_tn.shape[0] - 1].excerpt","93321902":"tagdict = nltk.data.load('help\/tagsets\/upenn_tagset.pickle')\npossible_tags = sorted([i for i in tagdict.keys() if i.isupper()])","165ce18f":"def tagtag(text):\n    \n    text = text.lower()\n    pattern = r'[a-z]+-[a-z]+|[a-z]+'\n    words = re.findall(pattern, text)\n    tagged_words = pos_tag(words)\n    tags = list(map(lambda x: x[1], tagged_words))\n    return tags","ff1c2b92":"tag2tag = {}\nfor i in range(sorted_tn.shape[0]):\n    tag_list = tagtag(sorted_tn.excerpt[i])\n    for j in range(len(tag_list) - 1):\n        dbletag = tag_list[j] + tag_list[j + 1]\n        if dbletag in tag2tag.keys():\n            tag2tag[dbletag] += 1\n        else:\n            tag2tag[dbletag] = 1","c1099fca":"wsum = sum(tag2tag.values())\nfor i in tag2tag:\n    tag2tag[i] \/= wsum","2293fbdd":"new_tag2tag = {k: v for k, v in sorted(tag2tag.items(), key=lambda x: x[1], reverse=True)}","7c4068ef":"leng = sorted_tn.shape[0]\ndef tag_to_tag(text):\n    \n    number = 0\n    text = text.lower()\n    pattern = r'[a-z]+-[a-z]+|[a-z]+'\n    words = re.findall(pattern, text)\n    tagged_words = pos_tag(words)\n    tags = list(map(lambda x: x[1], tagged_words))\n    for j in range(len(tags) - 1):\n        dbletag = tags[j] + tags[j + 1]\n        number += new_tag2tag[dbletag]\n    \n    return number","1a79e58a":"sorted_tn['to_tag'] = sorted_tn.excerpt.apply(tag_to_tag)\ntt['to_tag'] = tt.excerpt.apply(tag_to_tag)","602eb1c7":"sns.jointplot(sorted_tn.target, sorted_tn.to_tag)","1efd38df":"sorted_tn","b8089c18":"cols_to_drop = ['id', 'url_legal', 'license', 'excerpt']\ntn_excerpts = sorted_tn.excerpt\ntt_excerpts = tt.excerpt","3833a8f5":"sorted_tn = sorted_tn.drop(cols_to_drop, axis=1)\ntt = tt.drop(cols_to_drop, axis=1)","3e766c23":"# if interesting, wait few minutes\n# sns.pairplot(sorted_tn[sorted_tn.columns])","d8ef07f7":"# we have some useless features, at least one of them shown by this example\nsns.pairplot(sorted_tn[['nof_sentences', 'avg_len_sentence', 'text_length']])\n# later we'll use lasso to know all useless feats","7862ffd0":"show_diffs(sorted_tn, features)","6ed0c7d9":"# for now, let's take a look at correlation ratio \nsns.heatmap(sorted_tn[features[2:13]].corr(), annot=True)","11ef5277":"sns.heatmap(sorted_tn[features[:13] + ['to_tag']].corr(), annot=True)","52ee5583":"tt_features = []\nfor j in features:\n    if j not in ['standard_error', 'target']:\n        tt_features.append(j)","67be80f9":"# just interesting :)\nshow_diffs(tt, tt_features)","5a638a8c":"It is my work for CLRP Competition! I hope you'll get something useful there.\nMy name is Yuri, I'm novice on Kaggle :) \nAlso I need some help, I tell it in the last cell of this nb.\nWaiting your point by comment;)","774b6191":"# Next idea of the feature to be extracted. It's something like \"good order\" feature.","bca786a1":"![scheme.png](attachment:547c367a-5053-458a-a8f4-ac97e7ef8159.png)","11d71901":"# Summary\nNow, I'm going to use lasso to get sparse feature vector. Also I want to implement useful features (I'll get by Lasso) into PyTorch transformer with RobertaTokenizer are both at once. But I don't know how to do it :( It, might, something like a training new model with a pretrained one. If U know, please provide me with link, or your work after the competition ends. Need help :)\nIf interesting, please upvote and sorry again for my eng... ","6a612d28":"Now we're looking a few things ","c8beb16a":"**Note 1** *Sorry for my english, and non-optimized code*\n\n**Note 2** *I used there just one thing from notebook of some guy, and can't find it again to make a reference :( (it's scatter + kde plot design)*\n\n**Note 3** *Kurt would have loved python :)*","c735ed74":"Now we're seeing this:\n# $index \\approx \\frac{length}{(e^{target+mt}+1)^n}$\nAlso we able to know $n$, but we shouldn't care about it at all\nSo graph below:","8fabe119":"And later we saw this...\n\n> Hi everyone, \nWanted to post this so it be nearer the top. I have posted similar items in various discussions, but the discussion board is very healthy and I am afraid they are getting lost.\nThe target value is the result of a Bradley-Terry analysis of more than 111,000 pairwise comparisons between excerpts. Teachers spanning grades 3-12  a majority teaching between grade 6-10 served as the raters for these comparisons.\nStandard error is included as an output of the Bradley-Terry analysis because individual raters saw only a fraction of the excerpts, while every excerpt was seen by numerous raters. The test and train sets were split after the target scores and s.e. were computed.\nGreat to see such a spirited competition!\n\nScott Crossley, competition host","27168fed":"# Description\nCan machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.\n\nCurrently, most educational texts are matched to readers using traditional readability methods or commercially available formulas. However, each has its issues. Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence). As a result, they lack construct and theoretical validity. At the same time, commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available.\n\nCommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods.\n\nIn this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\nIf successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills. \n\n# Evaluation\n\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n# $ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\widehat{y_i})^2} $\nwhere is the predicted value,   is the original value, and   is the number of rows in the test data.","ba26845e":"# First glance\nLet us say, there are $m$ different excerpts, so its $T = \\{ t_1, t_2, ..., t_m \\}$.\nAlso we could use $k$ omnifarious features: $X = \\{ x_1, x_2, ..., x_k \\}$. Obviously, the set $X$ is countable, cause we able to use all operations to create new features by olds. So we have $n$ estimators of our excerpts. (We assume that all assessors used their own model and participated in each assessment)\nFinally, $D = \\{ d_1, d_2, ..., d_k \\}$, where $d_i$ is amount of using $x_i$, $ i\\in N$, $i\\in [1, k]$, and $f: X\\leftrightarrow D$ \n\n*Example:* Let $n=2$, and $ M_1(x_1, x_2, x_4)$ & $ M_2(x_1, x_4, x_5)$ - models of estimators 1 & 2, $x_1(t_i), x_2(t_i), x_3(t_i), x_4(t_i), x_5(t_i)$ - features applied to the text $t_i$. We also guess, that $x$ features transformed to some kind that: $M_1 = x_1 + x_2 + x_4$, $M_2 = x_1 + x_4 + x_5$. If we think that final model is: $M = \\frac{M_1 + M_2}{2}$, we have $d_1 = 2$, $d_2 = 1$, $d_3 = 0$, $d_4 = 2$, $d_5 = 1$ \n\nWe get: $y_j = \\frac{1}{n}\\sum_{i=1}^{k}d_ix_i(t_j)$, where: $d_i\\in Z$, $d_i\\in [0, n]$\nOr: $y_j = \\sum_{i=1}^{k}p_ix_i(t_j)$, where: $p_i\\in [0, 1]$, $p_i$ is frequency or probability of using $x_i$, $p_i=1$ - everyone used, and, accordingly, $p_i=0$ - no one.\n\nThis would be a good model using probabilities as weights, but the idea is too simple to be true. Getting a large number of features, single estimators using different models, a variable number of estimators evaluating excerpts etc.- the model is more complex than we thought. ","def1100a":"# Hi, kagglers!","b36abe85":"\"Target is the result of a Bradley-Terry analysis of more than 111,000 pairwise comparisons between excerpts...\"\nObviously, 111000 less than $\\binom{m}{2}$, also \"readability\" of text, should be positive number, to be compared by Bradley-Terry, but we have negative as well. And we have interesting dependence between index and target:\n# $index \\approx \\frac{length}{(e^{target+mt}+1)^n}$\nwhere $mt$ is median of target, and $n$ is something like curvature factor","e68a47e9":"We just count $\\sum_{w=1}^{l}sw_i$, Where $ l $ is length of ecxerpt, $sw$ is a single value, not multiplied.\nSo next step is counting $sw$ as a frequency of the same two-pos sequences "}}