{"cell_type":{"8cb6c328":"code","75690951":"code","3a671717":"code","84849944":"code","55a0eea2":"code","267fe1ee":"code","41b05b0f":"code","461dc926":"code","41f3b496":"code","9beb2b74":"code","29e95c5c":"code","370e49f2":"code","19d77252":"code","d839d3e2":"code","482458a4":"code","e7aa2701":"code","6381c9ef":"code","61340df5":"code","19c017a8":"code","dfb740ff":"code","6c0419b7":"code","276741b4":"code","7aade3d7":"code","af5e9fa6":"code","cf45122d":"code","45c8d877":"code","d3165a1f":"code","90311ed2":"code","a8da0f15":"code","08995e46":"code","1bb045e6":"code","1b32e937":"code","f51398fd":"code","edbc8243":"code","91b56669":"code","2c10d650":"code","2af8e415":"code","014f5abb":"code","f192ac9a":"code","ac8e6d3f":"code","c5d0cbe3":"code","f39768c5":"code","3e8d0f3a":"code","95185087":"code","a501fc75":"markdown","4460e71e":"markdown","ba6a5e52":"markdown","99e56bad":"markdown","1251bc5b":"markdown","8848dda7":"markdown","018dc540":"markdown","142946e8":"markdown","7755b54c":"markdown","e6dace2f":"markdown","3be31d16":"markdown","a2b226b3":"markdown","f64ace49":"markdown","f8c1821a":"markdown","05bc5112":"markdown","cb719f15":"markdown","3c9cd318":"markdown","0fbde265":"markdown","31313b1f":"markdown","aa6eb18a":"markdown","77c7e6a5":"markdown","57b77079":"markdown","6a1298ca":"markdown","5aae7622":"markdown","b8e31333":"markdown","6dac557e":"markdown","544fad41":"markdown","22893a82":"markdown","5e9b2675":"markdown","f1df5744":"markdown","a4c21403":"markdown","4ed31a24":"markdown","c379f74f":"markdown","0a81cac5":"markdown","527bef5b":"markdown","a5ccd464":"markdown","92c3e412":"markdown","c7a8a2c4":"markdown","b0e9f495":"markdown","7cb3c749":"markdown","e1d09470":"markdown","8b983c9d":"markdown","4db7cdce":"markdown","29ffe16a":"markdown","ff5aca12":"markdown","351aa94c":"markdown","a913a125":"markdown","223c989d":"markdown","dab85c90":"markdown","17c671d9":"markdown","a22fb71b":"markdown","394f7b01":"markdown","f43705f3":"markdown","cb9dcece":"markdown"},"source":{"8cb6c328":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.patches as patches\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\npd.set_option('max_columns', 200)\npd.set_option('max_rows', 200)","75690951":"%%time\ndataset = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')\ndataset.columns = [x.lower().strip().replace(' ','_') for x in dataset.columns]","3a671717":"dataset.head()","84849944":"dataset.info()","55a0eea2":"dataset.describe()","267fe1ee":"dataset['sars-cov-2_exam_result'] = dataset['sars-cov-2_exam_result'].replace(['negative','positive'], [0,1])","41b05b0f":"sns.countplot(dataset['sars-cov-2_exam_result'])","461dc926":"dataset['sars-cov-2_exam_result'].value_counts()","41f3b496":"dataset.isnull().sum()","9beb2b74":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","29e95c5c":"# Executando a fun\u00e7\u00e3o...\nmissing_data(dataset)","370e49f2":"# Importando a biblioteca\nimport missingno as msno","19d77252":"msno.matrix(dataset.head(20000))","d839d3e2":"hm = msno.heatmap(dataset)\nhm","482458a4":"dataset.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","e7aa2701":"### Consultando essas colunas (features)\n## sem dados\n#dataset['mycoplasma_pneumoniae'].value_counts()\n#dataset['urine_-_sugar'].value_counts()\n#dataset['prothrombin_time_(pt),_activity'].value_counts()\n#dataset['d-dimer'].value_counts()\n#dataset['partial_thromboplastin_time\\xa0(ptt)'].value_counts()\n\n## Valor 0.0 (apenas valores zeros)\n#dataset['fio2_(venous_blood_gas_analysis)'].value_counts()\n#dataset['myeloblasts'].value_counts()","6381c9ef":"dataset.drop('mycoplasma_pneumoniae',axis=1,inplace=True)\ndataset.drop('urine_-_sugar',axis=1,inplace=True)\ndataset.drop('prothrombin_time_(pt),_activity',axis=1,inplace=True)\ndataset.drop('d-dimer',axis=1,inplace=True)\ndataset.drop('partial_thromboplastin_time\\xa0(ptt)',axis=1,inplace=True)\n\n# Contem apenas 1 informa\u00e7\u00e3o e \u00e9 0.0\ndataset.drop('fio2_(venous_blood_gas_analysis)',axis=1,inplace=True)\ndataset.drop('myeloblasts',axis=1,inplace=True)\n\n# A coluna patient_id \u00e9 muito especifica e atrapalha o modelo, iremos apagar!\ndataset.drop('patient_id',axis=1,inplace=True)","61340df5":"# Visualizando o dataframe do dataset \ndataset","19c017a8":"%%time\n# Usando a m\u00e9dia para valores float\nfor c in dataset.columns:\n    if dataset[c].dtype=='float16' or  dataset[c].dtype=='float32' or  dataset[c].dtype=='float64':\n        dataset[c].fillna(dataset[c].mean())\n\n# 99999 para Categ\u00f3ricas\ndataset = dataset.fillna(99999)\n\n# Label Encoding para as Object\nfor f in dataset.columns:\n    if dataset[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(dataset[f].values))\n        dataset[f] = lbl.transform(list(dataset[f].values))\n        \nprint('Discretiza\u00e7\u00e3o Realizada')","dfb740ff":"dataset","6c0419b7":"from sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nimport xgboost as xgb\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom matplotlib import pyplot","276741b4":"svm = SVC()\ndt  = tree.DecisionTreeClassifier()\nrf  = RandomForestClassifier()\nknn = KNeighborsClassifier()\nmlp = MLPClassifier()\nadb = AdaBoostClassifier()\ngnb = GaussianNB()\nqda = QuadraticDiscriminantAnalysis()\nxgb = xgb.XGBClassifier()\nsdg = SGDClassifier()","7aade3d7":"X = dataset.drop('sars-cov-2_exam_result',axis=1)\ny = dataset['sars-cov-2_exam_result']","af5e9fa6":"all_models = [\n    (\"Modelo: SVM - Support Vector Machine\", svm),\n    (\"Modelo: Decision Tree\", dt),\n    (\"Modelo: Random Forest\", rf),\n    (\"Modelo: KNN Classifier\", knn),\n    (\"Modelo: MLP - Multi Layer Perceptron\", mlp),\n    (\"Modelo: AdaBoost Classifier\", adb),\n    (\"Modelo: Gaussian NB\", gnb),\n    (\"Modelo: Quadratic Discriminant Analysis\", qda),\n    (\"Modelo: XGB Classifier\", xgb),\n    (\"Modelo: Stochastic Gradient Descent\", sdg),\n]","cf45122d":"def benchmark_auc(model, X, y):\n    scores = []\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\n    scores.append(cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1))\n    print('-> M\u00e9dia ROC AUC: %.3f' % mean(scores))\n    return np.mean(scores)","45c8d877":"%%time\nfor name, model in all_models:\n    print(name)\n    benchmark_auc(model, X, y)\n\nprint(\"Resultados - ROC AUC - Conclu\u00eddos\")","d3165a1f":"X_train = dataset.drop('sars-cov-2_exam_result',axis=1)\ny = dataset['sars-cov-2_exam_result']","90311ed2":"#Visualizando a quantidade de dados por classe antes da execu\u00e7\u00e3o do SMOTE\nnp.bincount(y)","a8da0f15":"from imblearn.over_sampling import SMOTE","08995e46":"smt = SMOTE(k_neighbors=20)\nX_smt, y_smt = smt.fit_sample(X_train, y)","1bb045e6":"#Visualizando a quantidade de dados por classe ap\u00f3s a execu\u00e7\u00e3o do SMOTE\nnp.bincount(y_smt)","1b32e937":"from sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nimport xgboost as xgb\nfrom matplotlib import pyplot\nfrom numpy import mean","f51398fd":"svm = SVC()\ndt  = tree.DecisionTreeClassifier()\nrf  = RandomForestClassifier()\nknn = KNeighborsClassifier()\nmlp = MLPClassifier()\nadb = AdaBoostClassifier()\ngnb = GaussianNB()\nqda = QuadraticDiscriminantAnalysis()\nxgb = xgb.XGBClassifier()\nsdg = SGDClassifier()","edbc8243":"X = X_smt","91b56669":"y = y_smt","2c10d650":"%%time\nfor name, model in all_models:\n    print(name)\n    benchmark_auc(model, X, y)\n\nprint(\"Resultados - ROC AUC - Conclu\u00eddos (Balanceado com SMOTE)\")","2af8e415":"X_treino, X_teste, y_treino, y_teste = train_test_split(X, y,test_size=0.15,random_state=65)","014f5abb":"xgb.fit(X_treino, y_treino)","f192ac9a":"print (pd.crosstab(y_teste, xgb.predict(X_teste), rownames=['Real'], colnames=['Predito'], margins=True), '')","ac8e6d3f":"print (metrics.classification_report(y_teste,xgb.predict(X_teste)))","c5d0cbe3":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(xgb, X_teste, y_teste)","f39768c5":"import shap\nshap_values = shap.TreeExplainer(xgb).shap_values(X_treino)\nshap.summary_plot(shap_values, X_treino, plot_type=\"bar\")","3e8d0f3a":"shap.summary_plot(shap_values, X_treino)","95185087":"shap.dependence_plot('patient_age_quantile', shap_values, X_treino)","a501fc75":"# Desbalan\u00e7eamento das classes\nOcorre quando temos um *dataset* que possui muitos exemplos de uma classe e poucos exemplos da outra classe.\n\nNo modelo de classifica\u00e7\u00e3o, que \u00e9 o caso, o resultado do modelo ser\u00e1 enviesado, ou seja ele tende a classificar os novos dados como sendo da classe que possui mais exemplos.\n\nEm diagn\u00f3stico m\u00e9dico \u00e9 um problema t\u00edpico, pois o n\u00famero de pessoas diagnosticadas com uma determinada doen\u00e7a \u00e9 bem menor que o n\u00famero de pessoas sem a doen\u00e7a.\n\nExistem algumas t\u00e9cnicas para lidar com esses dados, podemos citar:\n1. *Undersampling* \u2013 Consiste em reduzir de forma aleat\u00f3ria os exemplos da classe majorit\u00e1ria. O **NearMiss** \u00e9 um algoritmo de *undersampling* que consiste em reduzir de forma aleat\u00f3ria os exemplos da classe majorit\u00e1ria, por\u00e9m ele seleciona os exemplos com base na dist\u00e2ncia.\n\n2. *Oversampling* \u2013 Consiste em replicar dados aleat\u00f3rios da classe minorit\u00e1ria. Como estamos duplicando os dados j\u00e1 existentes este m\u00e9todo est\u00e1 prop\u00edcio a dar overfitting. O **Smote** consiste em gerar dados sint\u00e9ticos (n\u00e3o duplicados) da classe minorit\u00e1ria a partir de seus vizinhos.****\n\nNo Estudo testamos tanto o **NearMiss** quanto o **Smote** e obtivemos os melhores resultados com o **Smote** e ser\u00e1 ele que iremos utilizar para tratar o desbalan\u00e7eamento entre as classes.\n","4460e71e":"4. Criando a rela\u00e7\u00e3o de Classificadores que ser\u00e3o utilizados (servir\u00e1 como entrada na nossa fun\u00e7\u00e3o de an\u00e1lise dos algoritmos utilizados)\n\n*Observa\u00e7\u00e3o, estamos utilizando os par\u00e2metros padr\u00f5es da biblioteca para cada um dos algoritmos.*","ba6a5e52":"Dividindo X e y em ``treino`` e ``teste`` para demonstrarmos a performance do modelo mais bem avaliado. Iremos usar para teste, 20% dos dados.","99e56bad":"# Classificador escolhido e teste!\nOs testes ser\u00e3o realizados para termos uma id\u00e9ia de como ser\u00e1 a performance, ser\u00e1 um pouco mais degradada, devido ao pequeno n\u00famero de registros.\n\nMas tende que com uma base maior pra treinamento e testes no dia a dia em produ\u00e7\u00e3o, esses resultados sejam melhores do que o aqui apresentado.","1251bc5b":"3. Chamado novamente a fun\u00e7\u00e3o (``benchmark_auc``) para an\u00e1lise dos algoritmos","8848dda7":"**Removendo essas *features* que n\u00e3o apresentam valores ou que apresenta apenas um valor e esse sendo zero**\n\n``mycoplasma_pneumoniae``, ``urine_-_sugar``, ``prothrombin_time_(pt),_activity``, ``d-dimer``, ``partial_thromboplastin_time\\xa0(ptt)``, ``fio2_(venous_blood_gas_analysis)``, ``myeloblasts`` e ``patient_id``\n","018dc540":"2. Instanciando os 10 classificadores utilizados","142946e8":"# Iremos utilizar o *crosstab* para visualizar as classifica\u00e7\u00f5es (Matriz de Confus\u00e3o)\n\nOnde veremos a propor\u00e7\u00e3o de acertos, comparando o que foi Predito com o Real.","7755b54c":"2. Instanciando os 10 classificadores utilizados","e6dace2f":"# Descri\u00e7\u00e3o estat\u00edsticas dos dados\nExplorando um pouco a estat\u00edtica dos dados, aqui j\u00e1 podemos observar que temos um grande n\u00famero de registros nulos, como esperado (conforme descri\u00e7\u00e3o dos organizadores, al\u00e9m de outros pontos como:)\n\n> **Dados ausentes** - A tomada de decis\u00e3o pelos profissionais de sa\u00fade \u00e9 um processo complexo, quando os m\u00e9dicos veem um paciente pela primeira vez com uma queixa aguda (por exemplo, in\u00edcio recente de febre e sintomas respirat\u00f3rios), eles fazem um hist\u00f3rico m\u00e9dico, realizam um exame f\u00edsico e baseiam suas decis\u00f5es sobre essas informa\u00e7\u00f5es. Solicitar ou n\u00e3o testes de laborat\u00f3rio, e quais solicitar, est\u00e1 entre essas decis\u00f5es e n\u00e3o existe um conjunto padr\u00e3o de testes que sejam solicitados a todos os indiv\u00edduos ou a uma condi\u00e7\u00e3o espec\u00edfica. Isso depender\u00e1 das queixas, dos resultados do exame f\u00edsico, hist\u00f3rico m\u00e9dico pessoal (por exemplo, doen\u00e7as diagnosticadas atuais e pr\u00e9vias, medicamentos em uso, cirurgias pr\u00e9vias, vacina\u00e7\u00e3o), h\u00e1bitos de vida (por exemplo, tabagismo, uso de \u00e1lcool, exerc\u00edcio), fam\u00edlia hist\u00f3rico m\u00e9dico e exposi\u00e7\u00f5es anteriores (por exemplo, viagens, ocupa\u00e7\u00e3o).\nO conjunto de dados reflete a complexidade da tomada de decis\u00e3o durante os cuidados cl\u00ednicos de rotina, em oposi\u00e7\u00e3o ao que acontece em um ambiente de pesquisa mais controlado, e a escassez de dados \u00e9, portanto, esperada.\n\n> **Vari\u00e1veis al\u00e9m dos resultados laboratoriais** - Entendemos que os dados cl\u00ednicos e de exposi\u00e7\u00e3o, al\u00e9m dos resultados laboratoriais, s\u00e3o informa\u00e7\u00f5es valiosas a serem adicionadas aos modelos, mas, no momento, eles n\u00e3o est\u00e3o dispon\u00edveis.\n\n> **Vari\u00e1veis laboratoriais adicionais** - O principal objetivo desse desafio \u00e9 desenvolver um modelo generaliz\u00e1vel que possa ser \u00fatil durante os cuidados cl\u00ednicos de rotina e, embora os exames laboratoriais solicitados possam variar para indiv\u00edduos diferentes, mesmo com a mesma condi\u00e7\u00e3o, pretendemos incluir testes laboratoriais mais comuns durante uma visita \u00e0 sala de emerg\u00eancia. Portanto, se voc\u00ea encontrou algum teste de laborat\u00f3rio adicional que n\u00e3o foi inclu\u00eddo, \u00e9 porque ele n\u00e3o foi considerado como uma ordem comum nessa situa\u00e7\u00e3o.","3be31d16":"Dividindo o conjunto de dados (Valores - X_train (vari\u00e1veis independentes) e R\u00f3tulos - y (*target* ou vari\u00e1vies dependentes)), agora para aplicarmos o **SMOTE**.","a2b226b3":"5. Fun\u00e7\u00e3o (``benchmark_auc``) para an\u00e1lise dos algoritmos, a m\u00e9trica utilizada foi a ``roc_auc`` (ela nos mostrar\u00e1 uma mudan\u00e7a relativa nos resultados para modelos com melhor desempenho) com ``cross-validation`` de **10**, onde pegaremos a m\u00e9dia.","f64ace49":"Os tr\u00eas algoritmos mais bem classificados foram: ``XGB Classifier``, ``AdaBoost Classifier`` e ``Random Forest``. \n\nNo resultado acima podemos observar um resultado relativamente baixo para a m\u00e9trica AUC devido ao desbalanceamento das classes, os modelos tiveram um desempenho em torno de 100% de *recall* para a classe 0 - Negativo (Classe Majorit\u00e1ria), mas muito baixo (*9%*) para a classe 1 - positivo (Classe Minorit\u00e1ria)\n\nDiante disso, iremos realizar um balan\u00e7eamento das classes e realizar novos testes","f8c1821a":"# Examinando *Missing Values* - Valores nulos (An\u00e1lise Explorat\u00f3ria dos Dados)\n\n1. Verificando a quantidade de registos nulos por colunas (*feature*)\n\nVisualizamos abaixo o que comentamos anteriormente, do grande n\u00famero de **valores faltantes (nulos)**.","05bc5112":"# Importando o *Dataset*\nImportando o *dataset* fornecido e aplicando algumas padroniza\u00e7\u00f5es b\u00e1sicas em nome de coluna e no tamanho das letras...\n\n> Este conjunto de dados cont\u00e9m dados anonimizados de pacientes atendidos no Hospital Israelita Albert Einstein, em S\u00e3o Paulo, Brasil, e que tiveram amostras coletadas para realizar o SARS-CoV-2 RT-PCR e testes laboratoriais adicionais durante uma visita ao hospital.\nTodos os dados foram anonimizados seguindo as melhores pr\u00e1ticas e recomenda\u00e7\u00f5es internacionais. Todos os dados cl\u00ednicos foram padronizados para ter uma m\u00e9dia de zero e um desvio padr\u00e3o unit\u00e1rio.","cb719f15":"**Fun\u00e7\u00e3o para verificar o percentual de nulos por coluna**","3c9cd318":"# Cria\u00e7\u00e3o do Modelo de Machine Learning (agora com as classes balanceadas)\n\nO passo 1 e 2, n\u00e3o precisaria fazer novamente, pois j\u00e1 fizemos anteriormente, mas por carater did\u00e1tico, vamos fazer...\n\n1. Importando as bibliotecas para Machine Learning, iremos utilizar basicamente as bibliotecas do pacote ``sklearn`` (maiores informa\u00e7\u00f5es, nas refer\u00eancias)","0fbde265":"2. Visualizando a vari\u00e1vel destino","31313b1f":"Agora os tr\u00eas algoritmos mais bem classificados foram: ``XGB Classifier``, ``Random Forest`` e ``Decision Tree``. \n\nNo resultado acima podemos observar uma melhoria em rela\u00e7\u00e3o aos resultados obtidos com o *dataset* antes do balanceamento de carga para a m\u00e9trica AUC. Isso quer dizer que com o balanceamento, os modelos geraram resultados melhores do que o *dataset* original desbalanceado.\n\nAgora, vamos pegar o modelo mais bem avaliado (``XGB Classifier``) e fazer testes utilizando o dataset para ``treinar`` e ``testar`` e veremos o desempenho.","aa6eb18a":"# Refer\u00eancias\n[Scikit-Learning](https:\/\/scikit-learn.org\/)\n\n[Machine Learning](https:\/\/pessoalex.wordpress.com\/ia\/machine-learning\/)\n\n[Classes Desbalanceadas](https:\/\/minerandodados.com.br\/lidando-com-classes-desbalanceadas-machine-learning\/)\n\n[ROC-UAC](https:\/\/medium.com\/bio-data-blog\/entenda-o-que-%C3%A9-auc-e-roc-nos-modelos-de-machine-learning-8191fb4df772)\n\n[Curva Precision-Recall](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_precision_recall.html)\n\n[SHAP](https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d)\n\n[Acompanhamento di\u00e1rio do Cononav\u00edrus (Brasil | Mundo)](https:\/\/pessoalex.wordpress.com\/2020\/02\/04\/monitoramento-do-coronavirus-usando-powerbi\/)","77c7e6a5":"# Vamos come\u00e7ar...","57b77079":"# Plotando a Curva *Precision-Recall*\nUm gr\u00e1fico pra verificarmos a curva *precision-recall*","6a1298ca":"# Importando as bibliotecas necess\u00e1rias\nImportando as bibliotecas \"base\" (``numpy``, ``pandas``, ``sklearn``, ``matplotlib``) e no decorrer do c\u00f3digo iremos importando as espec\u00edficas...","5aae7622":"Instanciando o **SMOTE** (o n\u00famero padr\u00e3o de vizinhos que ele pesquisa \u00e9 5, nos utilizamos 20)","b8e31333":"Chamando a fun\u00e7\u00e3o para cada um dos classificadores","6dac557e":"Treinando com o modelo ``XGB Classifier``","544fad41":"3. Dividindo o conjunto de dados (Valores - X_train (vari\u00e1veis independentes) e R\u00f3tulos - y (*target* ou vari\u00e1vies dependentes))","22893a82":"## M\u00e9tricas de Classifica\u00e7\u00e3o (Acur\u00e1cia: 70% e Recall: 90% para Classe 1 (com Covid-19))\nVamos agora gerar um **Relat\u00f3rio de Classifica\u00e7\u00e3o**, ele nos mostra com mais detalhes algumas m\u00e9tricas importantes, tais como: *precision, recall, f1-score*...","5e9b2675":"Observamos que agora temos apenas 103 columas e suas 5644 linhas","f1df5744":"# Tarefa\n\n> Prever casos confirmados de COVID-19 entre casos suspeitos. Com base nos resultados de exames laboratoriais comumente coletados para um caso suspeito de COVID-19 durante uma visita \u00e0 sala de emerg\u00eancia, seria poss\u00edvel prever o resultado do teste para SARS-Cov-2 (positivo \/ negativo)?","a4c21403":"3. Gr\u00e1fico de dependencia - Escolhi a *featute* ``patient_age_quantile`` e o **SHAP** escolhe a que mais se interage com ela.\n\n> A fun\u00e7\u00e3o inclui automaticamente outra vari\u00e1vel com a qual sua vari\u00e1vel escolhida interage mais.","4ed31a24":"# An\u00e1lise visual de valores nulos (An\u00e1lise Explorat\u00f3ria dos Dados)\nDiante desse grande n\u00famero de valores nulos, \u00e9 interessante termos uma vis\u00e3o disso, iremos utilizar a biblioteca *missingno* para gerarmos uma an\u00e1lise mais visual desses valores nulos. \n\nVamos criar 2 visualiza\u00e7\u00f5es:\n1. Matriz de Nulidade\n2. Mapa de Calor","c379f74f":"# Sele\u00e7\u00e3o de Atr\u00edbutos (*features*)\nCom base no que vimos anteriormente atrav\u00eas da fun\u00e7\u00e3o: *missing_data* e de forma visual, vamos apenas retirar apenas as *features* que n\u00e3o contem dados e uma que tem apenas 1 dado e esse dado \u00e9 0.0. \n\nAl\u00e9m da coluna patient_id que \u00e9 muito espec\u00edfica, uma vez que o objetivo \u00e9 generalizar.","0a81cac5":"# Explorando o *dataset*\n1. Visualizando as primeiras linhas do *dataset*","527bef5b":"# An\u00e1lise Explorat\u00f3ria dos Dados\n\nA An\u00e1lise Explorat\u00f3ria de Dados \u00e9 um processo aberto onde calculamos estat\u00edsticas com o objetivo de encontrar tend\u00eancias, anomalias, padr\u00f5es ou relacionamentos nos dados.","a5ccd464":"**1. Matriz de nulidade**\nA matriz de nulidade \u00e9 uma tela com densidade de dados e permite analisar rapidamente de forma visual como est\u00e3o dispostos os valores *null* do conjunto.","92c3e412":"2. *O gr\u00e1fico de valores SHAP pode mostrar ainda mais as rela\u00e7\u00f5es positivas e negativas dos preditores com a vari\u00e1vel de destino.*","c7a8a2c4":"# Agradecimentos\n\nAo **Hospital Israelita Albert Einstein** por disponibilizar os dados para que possamos de alguma forma ajudar nesse momento t\u00e3o dif\u00edcil para o mundo.\n\nA todos os trabalhadores da \u00e1rea da sa\u00fade.\n\nE a todos n\u00f3s que trabalhamos com dados! Estamos sempre dispon\u00edveis para ajudarmos com o que podemos!\n\n**#fiquememcasa**","b0e9f495":"# Observa\u00e7\u00f5es\nFui desenvolvendo v\u00e1rias vers\u00f5es no decorrer dos dias e as que realmente agregam valor para a an\u00e1lise feita s\u00e3o da 6, 7 e 9. Com base nesses \"rascunhos\" foi que chegamos nessa vers\u00e3o final.","7cb3c749":"Observamos que se trata de um *dataset* **n\u00e3o balan\u00e7eado**!\n\nVamos abaixo, a quantidade de dados em cada um dos *labels* (1 - Positivo e 0 - Negativo)","e1d09470":"# Criando modelos utilizando *Machine Learning* para previs\u00e3o de ocorr\u00eancia ou n\u00e3o do Covid-19 em pacientes\n\n1. Importando as bibliotecas para Machine Learning, iremos utilizar basicamente as bibliotecas do pacote ``sklearn`` (maiores informa\u00e7\u00f5es, nas refer\u00eancias)","8b983c9d":"# Tipo de dados das colunas (An\u00e1lise Explorat\u00f3ria dos Dados)\n\nVejamos o n\u00famero de colunas de cada tipo de dados. ``int64`` e ``float64`` s\u00e3o vari\u00e1veis ``num\u00e9ricas`` (que podem ser discretas ou cont\u00ednuas) enquanto as colunas de ``objetos`` cont\u00eam cadeias e s\u00e3o recursos ``categ\u00f3ricos``.","4db7cdce":"2. Quantidade de Linhas e Colunas, Tipos das Colunas, Tamanho entre outras informa\u00e7\u00f5es\n\nPodemos observar que temos: 5644 linhas e 111 colunas...","29ffe16a":"Visualizando novamente o dataframe do *dataset* (agora contendo um padr\u00e3o de dados que \u00e9 aceito pelos modelos de *Machine Learning*)\n","ff5aca12":"1. Mostrando as *featutes* mais importantes para o modelo ``xgb``\n\n*Um gr\u00e1fico de import\u00e2ncia vari\u00e1vel lista as vari\u00e1veis \u200b\u200bmais significativas em ordem decrescente. As principais vari\u00e1veis contribuem mais para o modelo do que as inferiores e, portanto, possuem alto poder preditivo.*","351aa94c":"\n**Examinando a distribui\u00e7\u00e3o da coluna de destino (*target column*)**\n\n1. Transformando da vari\u00e1vel destino em n\u00famerico (\"discretizando\")\n","a913a125":"# Motiva\u00e7\u00e3o\n\n> Uma das motiva\u00e7\u00f5es para esse desafio \u00e9 o fato de que, no contexto de um sistema de sa\u00fade sobrecarregado com a poss\u00edvel limita\u00e7\u00e3o para realizar testes para a detec\u00e7\u00e3o de SARS-CoV-2, testar todos os casos seria impratic\u00e1vel e os resultados dos testes poderiam ser postergados, mesmo se apenas uma subpopula\u00e7\u00e3o de destino seria testada.","223c989d":"**2. Mapa de calor**\nO mapa de calor de correla\u00e7\u00e3o mede a correla\u00e7\u00e3o de nulidade: qu\u00e3o fortemente a presen\u00e7a ou aus\u00eancia de uma vari\u00e1vel afeta a presen\u00e7a de outra:","dab85c90":"# O problema\n\n![CoronaV\u00edrus](https:\/\/pessoalex.files.wordpress.com\/2020\/02\/coronavirus19.jpg?w=474)\n\n> A Organiza\u00e7\u00e3o Mundial da Sa\u00fade (OMS) caracterizou o COVID-19, causado pela SARS-CoV-2, como uma pandemia em 11 de mar\u00e7o, enquanto o aumento exponencial no n\u00famero de casos estava em risco de sobrecarregar os sistemas de sa\u00fade em todo o mundo com uma demanda. para leitos de UTI muito acima da capacidade existente, com regi\u00f5es da It\u00e1lia sendo exemplos proeminentes.\n\n> O Brasil registrou o primeiro caso de SARS-CoV-2 em 26 de fevereiro, e a transmiss\u00e3o do v\u00edrus evoluiu apenas de casos importados, para transmiss\u00e3o local e finalmente comunit\u00e1ria muito rapidamente, com o governo federal declarando transmiss\u00e3o comunit\u00e1ria em 20 de mar\u00e7o.\n\n> At\u00e9 27 de mar\u00e7o, o estado de S\u00e3o Paulo registrava 1.223 casos confirmados de COVID-19, com 68 mortes relacionadas, enquanto o munic\u00edpio de S\u00e3o Paulo, com uma popula\u00e7\u00e3o de aproximadamente 12 milh\u00f5es de pessoas e onde o Hospital Israelita Albert Einstein est\u00e1 localizado, possu\u00eda 477 casos confirmados e 30 \u00f3bitos associados, em 23 de mar\u00e7o. Tanto o estado como o munic\u00edpio de S\u00e3o Paulo decidiram estabelecer medidas de quarentena e de distanciamento social, que ser\u00e3o aplicadas pelo menos at\u00e9 o in\u00edcio de abril, em um esfor\u00e7o para retardar a propaga\u00e7\u00e3o do v\u00edrus.","17c671d9":"**N\u00famero de classes exclusivas em cada coluna do objeto**","a22fb71b":"Pegando os dados gerados pelo **SMOTE** (X e y)","394f7b01":"# Algumas an\u00e1lises do modelo escolhido\nUtilizando a biblioteca SHAP, maiores informa\u00e7\u00f5es nas refer\u00eancias","f43705f3":"# Codifica\u00e7\u00e3o de Vari\u00e1veis\n\nAntes de prosseguirmos, precisamos lidar com vari\u00e1veis ``categ\u00f3ricas``. Infelizmente, um modelo de aprendizado de m\u00e1quina n\u00e3o pode lidar com vari\u00e1veis ``categ\u00f3ricas`` (exceto em alguns modelos como o LightGBM). Existem algumas maneiras de fazer isso, e usaremos as seguintes:","cb9dcece":"Para o balan\u00e7eamento de carga iremos utilizar o pacote de bibliotecas: ``imblearn``, como vemos a seguir:"}}