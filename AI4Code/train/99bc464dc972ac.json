{"cell_type":{"d03cd0e0":"code","616df770":"code","aff0f235":"code","b1549143":"code","2a9cf261":"code","ef62e925":"code","ae9c47a3":"code","007cdb9f":"code","ac1e9526":"code","04f973ae":"code","5da3eb7d":"code","adc27863":"code","09e909f3":"code","635fd494":"code","effe9bf2":"markdown","7b282bf3":"markdown","bc8cfa1c":"markdown","6bd5c8c7":"markdown","bffc07f3":"markdown"},"source":{"d03cd0e0":"# Utilizamos numpy y pandas para el manejo de los datos\nimport numpy as np \nimport pandas as pd\nimport random\nimport math\n\n# Se cargaron los archivos suministrados por la profesora a Kaggle\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","616df770":"# Declaramos la clase Perceptron para modelar su comportamiento\n\nclass Adaline():\n    def __init__(self, dimension=1, interval=(0,1), learning_rate=1):\n        self.weights = []\n        for i in range(dimension):\n            random.seed()\n            self.weights.append(random.uniform(interval[0],interval[1]))\n        self.input = [0]*dimension\n        self.learning_rate = learning_rate\n        self.output = 0\n\n    def activate(self, weight_vector, x_vector, bias=0):\n        weighted_sum = 0\n        for i in range(len(weight_vector)):\n            weighted_sum = weighted_sum + weight_vector[i] * x_vector[i]\n        self.output = weighted_sum\n        return self.output\n\n    def update_weights(self,w_difference):\n        i = 0\n        for i in range(len(self.weights)):\n            self.weights[i] = self.weights[i]+w_difference[i]\n            \n\n# Clase Layer o Capa, que agrupa distintos adaline.\nclass Layer():\n    def __init__(self,num_neurons=1, input_length=1, interval=(0,1), learning_rate=0.1):\n        self.neurons = [\n            Adaline(\n                dimension=input_length,\n                interval=interval,\n                learning_rate=learning_rate\n            ) for i in range(num_neurons)\n        ]\n\n    def __repr__(self):\n        return(str(\"Layer of \"+str(len(self.neurons))+\" neurons\"))\n    \n    print(\"Ready\")","aff0f235":"# Se obtienen los datos suministrados a Kaggle\n\nprint('Loading training data...')\ntrain_df = pd.read_csv('..\/input\/mnist-usb\/mnist_train.csv',header=None)\n\nprint(train_df.shape,'OK')","b1549143":"# Necesitamos una funci\u00f3n para el descenso de gradiente\n\ndef gradient_descent(x_input,difference):\n    \n    gradient_vector = []\n    for k in range(len(x_input)):\n        gradient_vector += [difference*x_input[k]]\n        \n    return gradient_vector","2a9cf261":"# Ejecuci\u00f3n del algoritmo de perceptr\u00f3n para\n# distintas tasas de entrenamiento\n\nfrom sklearn.utils import shuffle\n\ndef train_network(learning_rate, num_samples, epochs):\n\n    nn_layer = Layer(num_neurons=10, input_length=784, interval=(-0.05,0.05), learning_rate=0.01)\n\n    num_attributes = train_df.shape[1]\n\n    X = train_df.iloc[:num_samples,1:num_attributes+1]\/255\n    X = X.values\n    Y = [y[0] for y in train_df.iloc[:num_samples,[0,]].values]\n\n    X = np.array(X)\n    Y = np.array(Y)\n\n    print('Preparing algorithm')\n    print('Using Learning Rate = ',learning_rate)\n    for epoch in range(0,epochs):\n        print('\\tStarting Epoch',epoch)\n        score = 0\n\n        shuffler = np.random.permutation(len(X))\n        X_shuffle = X[shuffler]\n        Y_shuffle = Y[shuffler]\n\n        for i in range(num_samples):\n            if i%1000 == 0:\n                print('\\t\\tTraining sample #',i,'...')\n            y_train = Y_shuffle[i]\n            target_vector = [0]*10\n            target_vector[y_train] = 1\n            x_train = X_shuffle[i]\n\n            for neuron in nn_layer.neurons:\n                output = neuron.activate(neuron.weights, list(x_train))\n\n            layer_output = [neuron.output for neuron in nn_layer.neurons]\n            prediction = layer_output.index(max(layer_output))\n            \n            difference_vector=[]\n            for k in range(len(layer_output)):\n                difference_vector += [target_vector[k] - layer_output[k]]\n        \n            neuron_identifier = 0\n            for neuron in nn_layer.neurons:\n                gradient = gradient_descent(list(x_train),difference_vector[neuron_identifier])\n                w_difference = [g*learning_rate for g in gradient]\n                neuron.update_weights(w_difference) \n                neuron_identifier+=1\n            \n            validation = target_vector.index(max(target_vector))\n\n            if target_vector.index(max(target_vector)) == layer_output.index(max(layer_output)):\n                score+=1\n\n        print('\\tScore:',str(score\/num_samples*100),'%')\n        \n    return nn_layer","ef62e925":"# Entrenamos los modelos con distintas tasas de aprendizaje\n\nlearning_rates = [0.001, 0.01, 0.1]\n\nmodel_dict = {}\n\n\nfor rate in learning_rates:\n    model_dict[rate] = train_network(learning_rate=rate, num_samples=25000, epochs= 1)\n\n# train_network(0.001,10000,3)","ae9c47a3":"# Uso del conjunto de prueba\n\nprint('Loading testing data...')\ntest_df = pd.read_csv('\/kaggle\/input\/mnist-usb\/mnist_test.csv',header=None)\n\nprint('OK')","007cdb9f":"def test_model(nn_layer,learning_rate):\n    num_attributes = test_df.shape[1]\n    num_samples = test_df.shape[0]\n\n    X = test_df.iloc[:num_samples,1:num_attributes+1]\/255\n    X = X.values\n    \n    Y = [y[0] for y in test_df.iloc[:num_samples,[0,]].values]\n\n    X = np.array(X)\n    Y = np.array(Y)\n\n    score = 0\n\n    shuffler = np.random.permutation(len(X))\n    X_shuffle = X[shuffler]\n    Y_shuffle = Y[shuffler]\n\n    for i in range(num_samples):\n        if i%1000 == 0:\n            print('\\t\\tTesting sample #',i,'...')\n        y_train = Y_shuffle[i]\n        target_vector = [0]*10\n        target_vector[y_train] = 1\n        x_train = X_shuffle[i]\n\n        for neuron in nn_layer.neurons:\n            output = neuron.activate(neuron.weights, list(x_train))\n\n        layer_output = [neuron.output for neuron in nn_layer.neurons]\n        prediction = layer_output.index(max(layer_output))\n\n        difference_vector=[]\n        for k in range(len(layer_output)):\n            difference_vector += [target_vector[k] - layer_output[k]]\n\n        neuron_identifier = 0\n        for neuron in nn_layer.neurons:\n            gradient = gradient_descent(list(x_train),difference_vector[neuron_identifier])\n            w_difference = [g*learning_rate for g in gradient]\n            neuron.update_weights(w_difference) \n            neuron_identifier+=1\n\n        validation = target_vector.index(max(target_vector))        \n        \n        if target_vector.index(max(target_vector)) == layer_output.index(max(layer_output)):\n            score+=1\n\n    print('\\t Test score:',str(score\/num_samples*100),'%')","ac1e9526":"test_model(model_dict[0.001],0.001)","04f973ae":"# Cargamos los datos suministrados para conseguir un interpolador\n\nprint('Loading interpolation data...')\ninterpolation_df = pd.read_csv('..\/input\/interpolador-adaline\/datosT3 - datosT3.csv',header=None)\n\nprint('OK')","5da3eb7d":"interpolation_df.head()\n","adc27863":"print(interpolation_df.shape)","09e909f3":"from sklearn.model_selection import train_test_split\n\nX=interpolation_df.iloc[:,0].values\nX=X\/max(X)\ny=interpolation_df.iloc[:,1].values\ny=y\/max(y)\n","635fd494":"learning_rates = [0.001,0.01,0.1]\n\nepochs = 25\n\nfor rate in learning_rates:\n    neuron = Adaline(dimension=1)\n    for e in range(epochs):\n        print(\"Training with rate\",rate)\n        index = 0\n        score = 0\n        for sample in X:\n            prediction = neuron.activate(neuron.weights, [sample])        \n            difference = y[index] - prediction\n            \n                \n            gradient = gradient_descent([sample],difference)\n            if sample == -2.0:\n                print(gradient,sample,difference)\n            w_difference = [g*rate for g in gradient]\n\n            neuron.update_weights(w_difference)\n\n            validation = y[index]\n\n            if str(validation)[1:4] == str(prediction)[1:4]:\n                #print(validation,prediction)\n                score+=1\n\n            index += 1\n\n    print('\\t Training score:',str(score\/len(X)*100),'%. Interpolator =',neuron.weights)","effe9bf2":"UNIVERSIDAD SIMON BOLIVAR\n\nCO-6612, Introducci\u00f3n a las redes neuronales\n\nTarea 3: Adaline\n\n\nDaniel Francis. 12-10863","7b282bf3":"## Comentarios:\n\nFue imperativo normalizar los datos de entrada, dividiendo por su m\u00e1ximo.\n\nLas \u00e9pocas de entrenamiento no parecen afectar mucho. Esto es porque hay pocos datos de entrada.\n\nEntrenando en 10 \u00e9pocas:\n\nCon una tasa de aprendizaje de 0.001 se logr\u00f3 una precisi\u00f3n de 71% de 3 cifras decimales.\n\nCon una tasa de aprendizaje de 0.01 se logr\u00f3 una precisi\u00f3n de 72% de 3 cifras decimales.\n\nCon una tasa de aprendizaje de 0.1 se logr\u00f3 una precisi\u00f3n de 73% de 3 cifras decimales pero la precisi\u00f3n cay\u00f3 en las \u00faltimas 3 \u00e9pocas terminando en 39%.\n\n\nEntrenando en 25 \u00e9pocas:\n\nCon una tasa de aprendizaje de 0.001 se logr\u00f3 una precisi\u00f3n de 71% de 3 cifras decimales. (Interpolador:0.49619881795911)\n\nCon una tasa de aprendizaje de 0.01 se logr\u00f3 una precisi\u00f3n de 72% de 3 cifras decimales.\n(Interpolador:0.46125362227612154)\n\nCon una tasa de aprendizaje de 0.1 se logr\u00f3 una precisi\u00f3n de 6%\n(Interpolador: -0.07858109363520649)","bc8cfa1c":"## 3. Para los datos en datosT3.csv busque un interpolador utilizando un Adaline. Comente sobre las decisiones del algoritmo como por ejemplo n\u00famero de \u00e9pocas, tasa de aprendizaje, etc.","6bd5c8c7":"## 1. Programe el Adaline usando el algoritmo del LMS. Usted deber\u00e1 entregar su c\u00f3digo documentado.\n\n## 2. Para el conjunto de entrenamiento usado en la tarea del perceptr\u00f3n, repita la experiencia pero ahora con el Adaline. Eval\u00fae y compare este algoritmo con los resultados obtenidos en la y tarea anterior. Comente sobre su escogencia en los par\u00e1metros de aprendizaje.\n\n\nA continuaci\u00f3n se presenta el c\u00f3digo en distintos m\u00f3dulos para implementar la red neuronal artificial de una sola capa. Cada segmento puede ejecutarse de forma individual, lo que permite un mejor manejo del tiempo de ejecuci\u00f3n.","bffc07f3":"## Conclusi\u00f3n sobre los par\u00e1metros utilizados:\n\n\nParece que, similar al perceptr\u00f3n, el adaline se beneficia de tasas de aprendizaje bajas. A pesar de ello, para este problema, el adaline es capaz de dar mejores resultados con tasas de aprendizaje mayores (como 0.1)\n\nSe realizaron inicialmente sesiones de entrenamiento de 1 \u00e9poca y 10.000 muestras, de forma que pudiera elaborarse una respuesta de forma c\u00f3moda.\n\nObtuvimos puntuaciones (como m\u00ednimo) de\n\n- **80.4% para la tasa 0.001**\n- 67.3% para la tasa 0.01\n- 9.8% para la tasa 0.1\n\n\nLo ideal ser\u00eda entrenar los modelos con m\u00e1s muestras y \u00e9pocas\n\n\n"}}