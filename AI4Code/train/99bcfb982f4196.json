{"cell_type":{"cc74b2d6":"code","f6617f16":"code","b3dc3e2d":"code","abb6c2c9":"code","a4d0ac63":"code","e40cbbed":"code","ee5e4b8c":"code","a031c789":"code","66f08992":"code","0505b148":"code","b614e641":"code","429cbf17":"code","f7f1cdd3":"code","68558da3":"code","823681cc":"markdown","8ce4664a":"markdown","f83b628c":"markdown","23460c41":"markdown","edbea30b":"markdown","7c3192c1":"markdown","c9bd604d":"markdown","43329719":"markdown","5ce86253":"markdown","46498387":"markdown","4d183f3d":"markdown"},"source":{"cc74b2d6":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\n\n#Load data\ndf_train = pd.read_csv('..\/input\/learn-together\/train.csv')\ndf_test = pd.read_csv('..\/input\/learn-together\/test.csv')\n#Concatenate train and test sets into one DataFrame\ndf_full = pd.concat([df_train,df_test], sort=True, ignore_index = True)","f6617f16":"# count the number of missing data and show the top ten\ntotal = df_full.isnull().sum().sort_values(ascending=False)\npercent = (df_full.isnull().sum()\/df_full.isnull().count()).sort_values(ascending=False)*100\nmissing_data = pd.concat([total, round(percent,2)], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","b3dc3e2d":"df_train.info()","abb6c2c9":"df_train.describe()","a4d0ac63":"sns.distplot(df_train['Elevation'])","e40cbbed":"sns.distplot(df_train['Hillshade_9am'], hist_kws={'color':'b'}, kde_kws={\"label\":\"9am\",'color':'b'}) \nsns.distplot(df_train['Hillshade_Noon'], hist_kws={'color':'r'}, kde_kws={\"label\":\"Noon\", 'color':'r'})\nsns.distplot(df_train['Hillshade_3pm'], hist_kws={'color':'g'}, kde_kws={\"label\":\"3pm\", 'color':'g'})\n","ee5e4b8c":"df_train['Cover_Type'].value_counts(ascending = True)","a031c789":"sns.scatterplot(df_train['Horizontal_Distance_To_Hydrology'],df_train['Vertical_Distance_To_Hydrology'])","66f08992":"dims = (20, 12)\nfig, ax = plt.subplots(figsize=dims)\nsns.scatterplot(df_full['Horizontal_Distance_To_Hydrology'],df_full['Vertical_Distance_To_Hydrology'], hue = df_full['Horizontal_Distance_To_Fire_Points'], ax=ax)","0505b148":"X = df_train.iloc[:, 1:55].values #Excludes the Id attribute\ny = df_train.iloc[:, -1].values #Separates the dependant variable\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) #Uses 80% for training and 20% for testing\n\n# Fitting XGBoost to the Training set\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\n#from sklearn.metrics import confusion_matrix\n#cm = confusion_matrix(y_test, y_pred)\n\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\naccuracies.mean()","b614e641":"# Predicting the Test set results\ntest_id = df_test['Id']\ntest_set = df_test.iloc[:,1:55]\nX_train = pd.DataFrame(X_train)\ntest_set = pd.DataFrame(data=test_set.values)\ntest_set = np.array(test_set)","429cbf17":"y_pred_test = classifier.predict(test_set)","f7f1cdd3":"sub = pd.DataFrame({'Id':  test_id, 'Cover_Type': y_pred_test})\nsub.to_csv('submission_xgb.csv', index = False)","68558da3":"y_pred_test","823681cc":"Very even! That's useful, means the data isn't weighted one way or the other.\nLet's look at a scatterplot of the vertical and horizontal distance to surface water.","8ce4664a":"Ok, about 75% accuracy, not too bad for a completely untrained model.\nNow for the test set.","f83b628c":"Looks like all the Soil_Type columns are binary dummy variables (one column for each soil type with a 1 indicating which type it is). This could be simplified if we need to but this format is useful for some types of ML.\n\nThe Elevation attribute is simply the elevation in meters above sea level. There is a total vertical range in this set of 1986m. Lets see how this is distributed.","23460c41":"Now we'll make a very simple XGBoost model. \nFirst we'll split up the training set to create the model, do cross validation and then apply the model to the test set.","edbea30b":"No missing data?! That's gotta be a first on Kaggle.\n\nLet's look at the training set info.","7c3192c1":"Ok, looks like a trimodal distribution. Now let's see how the Hillsideshade columns compare to each other.","c9bd604d":"# Getting libraries and Datasets\n\nI'm going to keep this pretty simple, so won't need that many libraries. Just the basics for now.","43329719":"Ok, it shows a (sort of) linear relationship, I guess that makes sense.\n\nNow let's make the same graph, but increase the axis size and add a 'hue' parameter for the Horizontal_Distance_To_Fire_Points attribute, using the whole set, not just the training set.","5ce86253":"\nThis is my first published kernel on Kaggle.\n\nI'm just going to go through some basic visualisations and simple EDA (exploratory data analysis) and give a basic example of modelling using XGBoost and k-means cross validation. Then I'll use the same algorithm to create a basic benchmark entry. I'm not going to do any feature engineering or parameter tuning so the model isn't going to be that accurate. This is basically just an example.\n\nHope you guys find it useful.","46498387":"Looks pretty uniform.\nLet's look at some stats for the same set.","4d183f3d":"Let's take a look at how many of each different type of tree is included in the training set:"}}