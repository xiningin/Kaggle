{"cell_type":{"1d5f5ac5":"code","f5ccaafc":"code","4acecddd":"code","0c52269c":"code","1e2f150a":"code","8e5ee10e":"code","bf1daefc":"code","8ae001bd":"code","f643708c":"code","7ebb2c5b":"code","1af77899":"code","b5ccf52d":"code","5ae8214e":"code","8fbba206":"code","0e7bc11b":"code","789e4292":"code","e58c4a0b":"code","be391f7e":"code","be2eee81":"code","bde3ad08":"code","933cdd5e":"code","2ee83bef":"code","f8af8b4b":"code","8a35dfdb":"code","5f3d098b":"code","c9fd755a":"code","16b583e4":"code","75520372":"code","8ef6de23":"code","b44d551d":"code","1373cfff":"code","776a5619":"code","e7cf809a":"code","68285bb5":"code","dcfafaa4":"code","1f86c9a0":"code","b21ffa4e":"code","8e1dfad6":"code","f7f9b97f":"code","9555503d":"code","5b9ed5bb":"code","824b920e":"code","c9d94c0d":"code","a3288171":"code","04575c36":"code","7be5fe18":"code","9ead4596":"code","abfba71e":"code","4931af69":"code","ba07e1c7":"code","d8fe22d5":"code","147533ed":"code","a668580e":"code","0a3fcd8d":"code","35b1adc8":"code","3f930fd7":"code","3707301b":"code","35d044bb":"code","29445f02":"code","e2842619":"code","a7894f49":"code","964bfd49":"code","57441058":"code","67a6c509":"code","f15e828e":"code","f8ea9e3f":"code","3ee32c8c":"code","6b13533b":"code","30ece6aa":"code","88ad81f2":"code","0d666ef9":"code","cd486407":"code","bba7328c":"code","41c6c401":"code","998cf03f":"code","4349fd42":"markdown","65b3c522":"markdown","17750288":"markdown","38b20121":"markdown","380f616f":"markdown","df58e06c":"markdown","bcb8e913":"markdown","ccc073c3":"markdown","0180573d":"markdown","0de1aba6":"markdown","eaba01e6":"markdown","5ab1f558":"markdown","e7ab397e":"markdown","1bc48f7a":"markdown","35f80b83":"markdown","445822a9":"markdown","d7f07dd4":"markdown","502f5add":"markdown","94822084":"markdown","3fdc8955":"markdown","f4beea4e":"markdown","c2f50bbe":"markdown","b0e6e3bb":"markdown","56fde16e":"markdown","63b55665":"markdown","d2f5ad3b":"markdown","8941c0c2":"markdown","c35a2026":"markdown","afbfac34":"markdown"},"source":{"1d5f5ac5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5ccaafc":"\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scipy.stats as st\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings('ignore')\n","4acecddd":"train=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head()","0c52269c":"test=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.head()","1e2f150a":"train.info()","8e5ee10e":"train.describe()","bf1daefc":"train.shape , test.shape","8ae001bd":"numeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.columns","f643708c":"numeric_features.head()","7ebb2c5b":"# list of variables that contain year information\nyear_feature = [feature for feature in numeric_features if 'Yr' in feature or 'Year' in feature]\n\nyear_feature","1af77899":"# Let us explore the contents of temporal  variables\nfor feature in year_feature:\n    print(feature, train[feature].unique())","b5ccf52d":"for feature in year_feature:\n    if feature!='YrSold':\n        data=train.copy()\n        ## We will capture the difference between year variable and year the house was sold for\n        data[feature]=data['YrSold']-data[feature]\n\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel( feature)\n        plt.ylabel('SalePrice')\n        plt.show()","5ae8214e":"discrete_feature=[feature for feature in numeric_features if len(train[feature].unique())<25 and feature not in year_feature + ['Id']]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","8fbba206":"train[discrete_feature].head()","0e7bc11b":"for feature in discrete_feature:\n    data=train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","789e4292":"continuous_feature=[feature for feature in numeric_features if feature not in discrete_feature+year_feature+['Id']]\nprint(\"Continuous Feature Count {}\".format(len(continuous_feature)))","e58c4a0b":"for feature in continuous_feature:\n    data=train.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","be391f7e":"categorical_features = train.select_dtypes(include=[np.object])\ncategorical_features.columns","be2eee81":"train.skew(), train.kurt()","bde3ad08":"y = train['SalePrice']\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=False, fit=st.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=st.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=st.lognorm)","933cdd5e":"sns.distplot(train.skew(),color='blue',axlabel ='Skewness')","2ee83bef":"plt.figure(figsize = (12,8))\nsns.distplot(train.kurt(),color='r',axlabel ='Kurtosis',norm_hist= False, kde = True,rug = False)\n#plt.hist(train.kurt(),orientation = 'vertical',histtype = 'bar',label ='Kurtosis', color ='blue')\nplt.show()","f8af8b4b":"plt.hist(train['SalePrice'],orientation = 'vertical',histtype = 'bar', color ='blue')\nplt.show()","8a35dfdb":"target = np.log(train['SalePrice'])\ntarget.skew()\nplt.hist(target,color='black')","5f3d098b":"correlation = numeric_features.corr()\nprint(correlation['SalePrice'].sort_values(ascending = False),'\\n')","c9fd755a":"f , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of Numeric Features with Sale Price',y=1,size=16)\nsns.heatmap(correlation,square = True,  vmax=0.8)","16b583e4":"k= 11\ncols = correlation.nlargest(k,'SalePrice')['SalePrice'].index\nprint(cols)\ncm = np.corrcoef(train[cols].values.T)\nf , ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)","75520372":"sns.set()\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.pairplot(train[columns],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","8ef6de23":"saleprice_overall_quality= train.pivot_table(index ='OverallQual',values = 'SalePrice', aggfunc = np.median)\nsaleprice_overall_quality.plot(kind = 'bar',color = 'blue')\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.show()","b44d551d":"var = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","1373cfff":"var = 'Neighborhood'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nxt = plt.xticks(rotation=45)","776a5619":"var = 'SaleType'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nxt = plt.xticks(rotation=45)","e7cf809a":"var = 'SaleCondition'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nxt = plt.xticks(rotation=45)","68285bb5":"# checking percentage of missing values\ndata = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nfeatures_with_na=[features for features in data.columns if data[features].isnull().sum()>1]\nfor feature in features_with_na:\n    print(feature, np.round(data[feature].isnull().mean(), 4),  ' % of Missing Values')","dcfafaa4":"#test data\ndata_out = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nfeatures_with_na=[features for features in data_out.columns if data[features].isnull().sum()>1]\nfor feature in features_with_na:\n    print(feature, np.round(data_out[feature].isnull().mean(), 4),  ' % of Missing Values')","1f86c9a0":"# features with some missing values with sales Price\nfor feature in features_with_na:\n    dataset = data.copy()\n    dataset[feature] = np.where(dataset[feature].isnull(), 1, 0)\n   \n    # Calculate the mean of SalePrice where the information is missing or present\n    dataset.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.show()","b21ffa4e":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","8e1dfad6":"train=train.drop(['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','PoolQC','MiscFeature','Alley','Fence','FireplaceQu','Neighborhood','LotFrontage','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','GarageYrBlt', 'GarageArea', 'GarageCars','MasVnrType','MasVnrArea','MSZoning','Electrical','Utilities','Functional','KitchenQual','Exterior1st','Exterior2nd','SaleType','MSSubClass'],axis=1)\n","f7f9b97f":"train=train.dropna(axis=1)\ntest=test.dropna(axis=1)","9555503d":"train.isnull().sum()","5b9ed5bb":"train.info()","824b920e":"test.info()","c9d94c0d":"#train data\nsum([True for idx,row in train.iterrows() if any(row.isnull())])\n","a3288171":"#test data\nsum([True for idx,row in test.iterrows() if any(row.isnull())])","04575c36":"#convert categorical variable into dummy\ntrain = pd.get_dummies(train)","7be5fe18":"test = pd.get_dummies(test)","9ead4596":"train.isnull().sum()","abfba71e":"train.head(100)","4931af69":"test.head()","ba07e1c7":"a = np.intersect1d(test.columns, train.columns)\nprint (a)","d8fe22d5":"train_common=train[a]","147533ed":"test=test[a]","a668580e":"train_common.head()","0a3fcd8d":"X=train_common\nY=y_train","35b1adc8":"from sklearn import preprocessing","3f930fd7":"min_max_scaler = preprocessing.MinMaxScaler()\nX_scale = min_max_scaler.fit_transform(X)\n","3707301b":"X_scale","35d044bb":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\nprint(\"X_train's shape : \",X_train.shape)\nprint(\"X_test's shape : \",X_test.shape)\nprint(\"Y_train's shape : \",Y_train.shape)\nprint(\"Y_test's shape : \",Y_test.shape)","29445f02":"from sklearn.linear_model import LinearRegression","e2842619":"model=LinearRegression(normalize=True)\nmodel.fit(X_train,Y_train)","a7894f49":"from sklearn.ensemble import RandomForestRegressor","964bfd49":"rfc=RandomForestRegressor(n_estimators=10000, random_state=1, n_jobs=-1)\nrfc.fit(X_train,Y_train)","57441058":"# Model evaluation for training set\nY_train_pred = model.predict(X_train)\nrmse = (np.sqrt(mean_squared_error(Y_train, Y_train_pred))) #root mean square error\nr2 = r2_score(Y_train, Y_train_pred) # it gives the score based on the relationship between actual output and predicted output by the model\n\nprint(\"Model training performance:\")\nprint(\"---------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))\nprint(\"\\n\")\nY_test_pred=model.predict(X_test)\n# Model evaluation for testing set\nB_test_pred = model.predict(X_test)\nrmse = (np.sqrt(mean_squared_error(Y_test, Y_test_pred)))\nr2 = r2_score(Y_test, Y_test_pred)\n\nprint(\"Model testing performance:\")\nprint(\"--------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","67a6c509":"# Model evaluation for training set\nY_train_pred = rfc.predict(X_train)\nrmse = (np.sqrt(mean_squared_error(Y_train, Y_train_pred))) #root mean square error\nr2 = r2_score(Y_train, Y_train_pred) # it gives the score based on the relationship between actual output and predicted output by the model\n\n\n\nprint(\"Model training performance:\")\nprint(\"---------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))\nprint(\"\\n\")\nY_test_pred=rfc.predict(X_test)\n# Model evaluation for testing set\nB_test_pred = model.predict(X_test)\nrmse = (np.sqrt(mean_squared_error(Y_test, Y_test_pred)))\nr2 = r2_score(Y_test, Y_test_pred)\n\nprint(\"Model testing performance:\")\nprint(\"--------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","f15e828e":"feat_importances = pd.Series(rfc.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","f8ea9e3f":"from sklearn.feature_selection import SelectFromModel\n# Create a selector object that will use the random forest classifier to identify\n# It will select the features based on the importance score\nrf_sfm = SelectFromModel(rfc)","3ee32c8c":"rf_sfm = rf_sfm.fit(X_train, Y_train)","6b13533b":"X_important_train = rf_sfm.transform(X_train)\nX_important_test = rf_sfm.transform(X_test)","30ece6aa":"X_important_train","88ad81f2":"# Create a new random forest classifier for the most important features\nclf_important = RandomForestRegressor(n_estimators=200, random_state=1, n_jobs=-1)\n\n# Train the new classifier on the new dataset containing the most important features\nclf_important = clf_important.fit(X_important_train, Y_train)","0d666ef9":"# Model evaluation for training set\nY_train_pred = clf_important.predict(X_important_train)\nrmse = (np.sqrt(mean_squared_error(Y_train, Y_train_pred))) #root mean square error\nr2 = r2_score(Y_train, Y_train_pred) # it gives the score based on the relationship between actual output and predicted output by the model\n\n\n\nprint(\"Model training performance:\")\nprint(\"---------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))\nprint(\"\\n\")\nY_test_pred=clf_important.predict(X_important_test)\n# Model evaluation for testing set\nB_test_pred = clf_important.predict(X_important_test)\nrmse = (np.sqrt(mean_squared_error(Y_test, Y_test_pred)))\nr2 = r2_score(Y_test, Y_test_pred)\n\nprint(\"Model testing performance:\")\nprint(\"--------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","cd486407":"output_model=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\noutput_model.head()","bba7328c":"test_imp= rf_sfm.transform(test)","41c6c401":"output=clf_important.predict(test_imp)\nprediction=pd.DataFrame({'Id':test.Id,'SalePrice':output})\n","998cf03f":"prediction.to_csv('prediction_c.csv',index=False)","4349fd42":"I am simply used random forest regressor and linear regressor..If you liked this notebook upvote it....Thanks for viewing!!!","65b3c522":" # Missing Value Analysis \n \nWe will first check the percentage of missing values present in each feature","17750288":" #### 3. Continuous Variables:","38b20121":"# Interpret The Model\n\nNow the model has generated a LinearRegression model for us. Recall that a LinearRegression model consist of coefficient(s) and intercept. We can now have a look at the intercept and coefficients for our model and interpret them.","380f616f":"### Hi...In this notebook i used exploratory data analysis and Linear,Random forest model for predicting the house prices.","df58e06c":"### Random Forest Regressor","bcb8e913":"### Pair Plot \n\n#### Pair Plot between 'SalePrice' and correlated variables\n\nVisualisation of 'OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd' features \nwith respect to SalePrice in the form of pair plot & scatter pair plot for better understanding.","ccc073c3":"It is apparent that SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed. While log transformation does pretty good job, best fit is unbounded Johnson distribution.","0180573d":"### Correlation Heat Map","0de1aba6":"#### Box plot - Neighborhood","eaba01e6":"#### 4.Categorical Features","5ab1f558":"# Data Splitting","e7ab397e":"#### SalePrice Correlation matrix","1bc48f7a":"#### Now let us find the relationship between these discrete features and Sale Price","35f80b83":"# Model Building","445822a9":"#### Box plot - OverallQual","d7f07dd4":"Heatmaps are great to detect this kind of multicollinearity situations and in problems related to feature selection like this project, it comes as an excellent exploratory tool.\n\none aspect I observed here is the 'SalePrice' correlations.As it is observed that 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hello !' to SalePrice, however we cannot exclude the fact that rest of the features have some level of correlation to the SalePrice. To observe this correlation closer let us see it in Zoomed Heat Map ","502f5add":"To explore further we will start with the following visualisation methods to analyze the data better:\n\n - Correlation Heat Map\n - Zoomed Heat Map\n - Pair Plot \n ","94822084":"From above zoomed heatmap it is observed that GarageCars & GarageArea are closely correlated .\nSimilarly TotalBsmtSF and 1stFlrSF are also closely correlated.\n","3fdc8955":"Let us analyse the continuous values with data visualisation to understand the data distribution","f4beea4e":"#### 2.Discrete Variables","c2f50bbe":"#### Deleting different columns from test in train","b0e6e3bb":"### Deleting Columns","56fde16e":"# Exploratory Data Analysis","63b55665":"**Estimate Skewness and Kurtosis**","d2f5ad3b":"#### Housing Price vs Sales\n\n- Sale Type & Condition\n- Sales Seasonality","8941c0c2":"# Output predictions","c35a2026":"#### 1.Temporal Variables(Eg: Datetime Variables)\n","afbfac34":"### Linear Regressor"}}