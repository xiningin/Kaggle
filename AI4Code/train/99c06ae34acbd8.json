{"cell_type":{"f289f23a":"code","d6a55a4b":"code","4957cc05":"code","2daf6825":"code","615d9825":"code","467249ac":"code","f54597be":"code","fffc5227":"code","3c7d8659":"code","baf58480":"code","d94f1a72":"markdown","84a0cc28":"markdown","00693c33":"markdown","c810bd67":"markdown","48ad41c3":"markdown","40bf32d1":"markdown"},"source":{"f289f23a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d6a55a4b":"import numpy as np\nimport tensorflow as tf\n\nimport tensorflow_datasets as tfds\n\n# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) \n# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument\n# there are other arguments we can specify, which we can find useful\n# mnist_dataset = tfds.load(name='mnist', as_supervised=True)\nmnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n# we will use this information a bit below and we will store it in mnist_info\n\n# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n# alternatively, as_supervised=False, would return a dictionary\n# obviously we prefer to have our inputs and targets separated \n\n# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references\nmnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n\n# by default, TF has training and testing datasets, but no validation sets\n# thus we must split it on our own\n\n# we start by defining the number of validation samples as a % of the train samples\n# this is also where we make use of mnist_info (we don't have to count the observations)\nnum_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n# let's cast this number to an integer, as a float may cause an error along the way\nnum_validation_samples = tf.cast(num_validation_samples, tf.int64)\n\n# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)\nnum_test_samples = mnist_info.splits['test'].num_examples\n\n# once more, we'd prefer an integer (rather than the default float)\nnum_test_samples = tf.cast(num_test_samples, tf.int64)\n\n\n# normally, we would like to scale our data in some way to make the result more numerically stable\n# in this case we will simply prefer to have inputs between 0 and 1\n# let's define a function called: scale, that will take an MNIST image and its label\ndef scale(image, label):\n    # we make sure the value is a float\n    image = tf.cast(image, tf.float32)\n    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 \n    image \/= 255.\n\n    return image, label\n\n\n# the method .map() allows us to apply a custom transformation to a given dataset\n# we have already decided that we will get the validation data from mnist_train, so \nscaled_train_and_validation_data = mnist_train.map(scale)\n\n# finally, we scale and batch the test data\n# we scale it so it has the same magnitude as the train and validation\n# there is no need to shuffle it, because we won't be training on the test data\n# there would be a single batch, equal to the size of the test data\ntest_data = mnist_test.map(scale)\n\n\n# let's also shuffle the data\n\nBUFFER_SIZE = 10000\n# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets\n# then we can't shuffle the whole dataset in one go because we can't fit it all in memory\n# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them\n# if BUFFER_SIZE=1 => no shuffling will actually happen\n# if BUFFER_SIZE >= num samples => shuffling is uniform\n# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling\n\n# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size\nshuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n\n# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation\n# our validation data would be equal to 10% of the training set, which we've already calculated\n# we use the .take() method to take that many samples\n# finally, we create a batch with a batch size equal to the total number of validation samples\nvalidation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n\n# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset\ntrain_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n\n# determine the batch size\nBATCH_SIZE = 100\n\n# we can also take advantage of the occasion to batch the train data\n# this would be very helpful when we train, as we would be able to iterate over the different batches\ntrain_data = train_data.batch(BATCH_SIZE)\n\nvalidation_data = validation_data.batch(num_validation_samples)\n\n# batch the test data\ntest_data = test_data.batch(num_test_samples)\n\n\n# takes next batch (it is the only batch)\n# because as_supervized=True, we've got a 2-tuple structure\nvalidation_inputs, validation_targets = next(iter(validation_data))\n\n","4957cc05":"input_size = 784\noutput_size = 10\n# Use same hidden layer size for both hidden layers. Not a necessity.\nhidden_layer_size = 200\n    \n# define how the model will look like\nmodel = tf.keras.Sequential([\n    \n    # the first layer (the input layer)\n    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n    # or (28x28x1,) = (784,) vector\n    # this allows us to actually create a feed forward neural network\n    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n    \n    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n\n    \n    # the final layer is no different, we just make sure to activate it with softmax\n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])","2daf6825":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","615d9825":"# determine the maximum number of epochs\nNUM_EPOCHS = 5\n\n# we fit the model, specifying the\n# training data\n# the total number of epochs\n# and the validation data we just created ourselves in the format: (inputs,targets)\nmodel.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)","467249ac":"test_loss, test_accuracy = model.evaluate(test_data)","f54597be":"print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))","fffc5227":"test_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","3c7d8659":"preds = model.predict(test_data)\npreds = np.argmax(preds, axis=1)\n","baf58480":"output = pd.DataFrame({'ImageId' : test_data.index + 1, 'label' : preds})\n#     print(output.head())\noutput.to_csv('my_submission.csv' , index=False)","d94f1a72":"# Preprocessing data","84a0cc28":"# Training data","00693c33":"# optimizing model","c810bd67":"# Sequential model","48ad41c3":"# Submission","40bf32d1":"# Testing model"}}