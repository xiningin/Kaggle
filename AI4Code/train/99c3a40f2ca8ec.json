{"cell_type":{"2e401276":"code","8a6ed03f":"code","0816f182":"code","2de6692d":"code","528b25d1":"code","9ad6f994":"code","3c3ef7de":"code","e2b83d92":"code","1105c7ad":"code","2ad632b4":"code","72579e0c":"code","91960390":"code","8f25453a":"code","e2bf84cb":"code","8a8fd903":"code","df78bf16":"code","a46d8a80":"code","bd829049":"code","9ec02dab":"code","35c1107a":"code","6265ecc4":"code","f8ad510a":"code","299978d9":"code","c4a6d957":"code","c217ce67":"code","8020dd6e":"code","a17fd443":"code","140569f3":"code","37c5a7c8":"code","4214db95":"code","f7f02ef1":"code","002f6078":"code","362da2ca":"code","83d0f025":"code","c6e12333":"code","0430daa1":"code","d3092715":"code","3e25b91d":"code","0d9c61b1":"code","f1109ecd":"code","53cadb2c":"code","5602c471":"code","176a0970":"code","9efea623":"code","1095431a":"code","dd3b09cf":"code","26fe849f":"code","e531f066":"code","d88ae1c6":"code","ea4dfeed":"code","43084614":"code","71bed069":"code","df3e9f27":"code","fba8fb8c":"code","48df66ce":"code","e0e8de91":"code","d058be4a":"code","9020a35e":"code","e90f9caf":"code","d65e3d75":"code","4861b8ac":"code","dfcbcaee":"code","9656a13f":"code","eb9651b4":"code","616c6ff2":"code","b474beac":"code","8372d629":"code","671cca2b":"code","462090c2":"code","94dd0834":"code","2d6676e7":"code","12250dff":"code","e07981dd":"code","49a569b0":"code","dc1729cb":"code","99189436":"code","d0ea1782":"code","07c617dc":"code","4eeb470a":"code","488fae4b":"code","1a77b77b":"code","d7fc57eb":"code","143a4343":"code","1144afa6":"code","1202eca4":"markdown","4c908802":"markdown","ca94fc68":"markdown","01072341":"markdown","0297f0a2":"markdown","38fed088":"markdown","0f64127e":"markdown","a896d099":"markdown","b85f0baa":"markdown","82374450":"markdown","a0e48175":"markdown","24806832":"markdown","23ee8488":"markdown","143dde7a":"markdown","6206b03e":"markdown","ef48b771":"markdown","da0a5a73":"markdown","92dc62c3":"markdown","41e9f986":"markdown","ebeb5cb4":"markdown","00430e7d":"markdown","d0d1d4d6":"markdown","11657c0f":"markdown","90f48810":"markdown","d41ad6e9":"markdown","fb5f7bfc":"markdown","f58c5f35":"markdown","d4e9609a":"markdown","1dca0c80":"markdown","fc385077":"markdown","d927c866":"markdown","e1363c68":"markdown","4f0e1750":"markdown","41533a70":"markdown","7abe5610":"markdown","8a2c8117":"markdown","6eacb7c6":"markdown","3a97704f":"markdown","1a5265e2":"markdown","770f6e00":"markdown","3e22eb12":"markdown","99dff9cd":"markdown","9fcb7c01":"markdown","56d5a9de":"markdown","b5c36eeb":"markdown","84afc630":"markdown","126baf9c":"markdown","aa1eeef4":"markdown","11ac5a4e":"markdown","4c70c5cd":"markdown","532cdc5f":"markdown","53c5e267":"markdown","36c9a691":"markdown","ecb4bde6":"markdown","0fdd4148":"markdown","314ca152":"markdown","acb5ff9a":"markdown","645d4dc6":"markdown","26378b61":"markdown","1f609494":"markdown","c59851e8":"markdown","efb4ea74":"markdown","04e91ccc":"markdown","a0f14a55":"markdown","35964643":"markdown","d8b1fd93":"markdown","795eed29":"markdown","43af5e9a":"markdown","600e3cef":"markdown","ef2a2b2f":"markdown","2ca889b9":"markdown","627dfdac":"markdown","fdfb4bc1":"markdown","e3391488":"markdown","222b8079":"markdown","95029d2a":"markdown","bd57f52b":"markdown","1bb1ab65":"markdown","8a405bcf":"markdown","4323bc52":"markdown","b860cb92":"markdown","58ffab53":"markdown","c742574c":"markdown","df54112b":"markdown","ec5e0b03":"markdown","ba381bb4":"markdown","e1fb7fff":"markdown","5510d58d":"markdown","86f331af":"markdown"},"source":{"2e401276":"# @title Tutorial slides\n\n# @markdown These are the slides for the videos in this tutorial\n\n# @markdown If you want to locally download the slides, click [here](https:\/\/osf.io\/n263c\/download)\nfrom IPython.display import IFrame\nIFrame(src=f\"https:\/\/mfr.ca-1.osf.io\/render?url=https:\/\/osf.io\/n263c\/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)","8a6ed03f":"# @title Install dependencies\n\n# @markdown There may be `Errors`\/`Warnings` reported during the installation. However, they are to be ignored.\n!pip install torchtext==0.4.0 --quiet\n!pip install --upgrade gensim --quiet\n!pip install unidecode --quiet\n!pip install hmmlearn --quiet\n!pip install fasttext --quiet\n!pip install nltk --quiet\n!pip install pandas --quiet\n!pip install python-Levenshtein --quiet\n\n!pip install git+https:\/\/github.com\/NeuromatchAcademy\/evaltools --quiet\nfrom evaltools.airtable import AirtableForm\n\n# generate airtable form\natform = AirtableForm('appn7VdPRseSoMXEG','W2D3_T1','https:\/\/portal.neuromatchacademy.org\/api\/redirect\/to\/9c55f6cb-cdf9-4429-ac1c-ec44fe64c303')","0816f182":"# Imports\nimport time\nimport fasttext\nimport numpy as np\nimport pandas as pd\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom hmmlearn import hmm\nfrom scipy.sparse import dok_matrix\n\nfrom torchtext import data, datasets\nfrom torchtext.vocab import FastText\n\nimport nltk\nfrom nltk import FreqDist\nfrom nltk.corpus import brown\nfrom nltk.tokenize import word_tokenize\n\nfrom gensim.models import Word2Vec\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom tqdm import tqdm_notebook as tqdm","2de6692d":"# @title Figure Settings\nimport ipywidgets as widgets\n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/content-creation\/main\/nma.mplstyle\")","528b25d1":"# @title  Load Dataset from `nltk`\n# no critical warnings, so we supress it\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('brown')\nnltk.download('webtext')","9ad6f994":"# @title Helper functions\n\nimport requests\n\ndef cosine_similarity(vec_a, vec_b):\n    \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n    return np.dot(vec_a, vec_b) \/ (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n\n\ndef tokenize(sentences):\n  #Tokenize the sentence\n  #from nltk.tokenize library use word_tokenize\n  token = word_tokenize(sentences)\n\n  return token\n\n\ndef plot_train_val(x, train, val, train_label, val_label, title, y_label,\n                   color):\n  plt.plot(x, train, label=train_label, color=color)\n  plt.plot(x, val, label=val_label, color=color, linestyle='--')\n  plt.legend(loc='lower right')\n  plt.xlabel('epoch')\n  plt.ylabel(y_label)\n  plt.title(title)\n\n\ndef load_dataset(emb_vectors, sentence_length=50, seed=522):\n  TEXT = data.Field(sequential=True,\n                    tokenize=tokenize,\n                    lower=True,\n                    include_lengths=True,\n                    batch_first=True,\n                    fix_length=sentence_length)\n  LABEL = data.LabelField(dtype=torch.float)\n\n  train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n\n  TEXT.build_vocab(train_data, vectors=emb_vectors)\n  LABEL.build_vocab(train_data)\n\n  train_data, valid_data = train_data.split(split_ratio=0.7,\n                                            random_state=random.seed(seed))\n  train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data,\n                                                                  valid_data,\n                                                                  test_data),\n                                                                  batch_size=32,\n                                                                  sort_key=lambda x: len(x.text),\n                                                                  repeat=False,\n                                                                  shuffle=True)\n  vocab_size = len(TEXT.vocab)\n\n  print(f'Data are loaded. sentence length: {sentence_length} '\n        f'seed: {seed}')\n\n  return TEXT, vocab_size, train_iter, valid_iter, test_iter\n\n\ndef download_file_from_google_drive(id, destination):\n  URL = \"https:\/\/docs.google.com\/uc?export=download\"\n\n  session = requests.Session()\n\n  response = session.get(URL, params={ 'id': id }, stream=True)\n  token = get_confirm_token(response)\n\n  if token:\n    params = { 'id': id, 'confirm': token }\n    response = session.get(URL, params=params, stream=True)\n\n  save_response_content(response, destination)\n\n\ndef get_confirm_token(response):\n  for key, value in response.cookies.items():\n    if key.startswith('download_warning'):\n      return value\n\n  return None\n\n\ndef save_response_content(response, destination):\n  CHUNK_SIZE = 32768\n\n  with open(destination, \"wb\") as f:\n    for chunk in response.iter_content(CHUNK_SIZE):\n      if chunk: # filter out keep-alive new chunks\n        f.write(chunk)","3c3ef7de":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# for DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","e2b83d92":"# @title Set device (GPU or CPU). Execute `set_device()`\n\n# inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","1105c7ad":"DEVICE = set_device()\nSEED = 2021\nset_seed(seed=SEED)","2ad632b4":"# @title Video 1: Sequences & Markov Processes\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1jg411774B\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"ApkE7UFaJAQ\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 1: Sequences & Markov Processes')\n\ndisplay(out)","72579e0c":"category = ['editorial', 'fiction', 'government', 'news', 'religion']\nsentences = brown.sents(categories=category)\ntype(sentences)","91960390":"lengths = [len(sentence) for sentence in sentences]\nlengths = pd.Series(lengths)","8f25453a":"lengths.quantile(.8) # 80th percentil","e2bf84cb":"lengths.quantile(.5)","8a8fd903":"lengths.describe()","df78bf16":"np.argmin(lengths.values)","a46d8a80":"type(sentences)","bd829049":"sentences[81]","9ec02dab":"sentences[0:2]","35c1107a":"corpus_words = []\nfor sentence in sentences:\n  for word in sentence:\n    if \"''\" not in word and \"``\" not in word:\n      corpus_words.append(word)","6265ecc4":"print(f\"Corpus length: {len(corpus_words)}\")","f8ad510a":"corpus_words[0:20]","299978d9":"# @title Creating Matrices and Distinct Words\ndistinct_words = list(set(corpus_words))\nword_idx_dict = {word: i for i, word in enumerate(distinct_words)}\ndistinct_words_count = len(list(set(corpus_words)))\nnext_word_matrix = np.zeros([distinct_words_count, distinct_words_count])","c4a6d957":"print(\"Number of distinct words: \" + str(distinct_words_count))","c217ce67":"# @title Populating Matric that tracks next word\nfor i, word in enumerate(corpus_words[:-1]):\n  first_word_idx = word_idx_dict[word]\n  next_word_idx = word_idx_dict[corpus_words[i+1]]\n  next_word_matrix[first_word_idx][next_word_idx] +=1","8020dd6e":"first_word_idx","a17fd443":"# @title Function for most likely word\ndef most_likely_word_after(word):\n  # we check for the word most likely to occur using the matrix\n  most_likely = next_word_matrix[word_idx_dict[word]].argmax()\n  return distinct_words[most_likely]","140569f3":"# @title Function for building Naive Chain\ndef naive_chain(word, length=15):\n  current_word = word\n  sentence = word\n  # we now build a naive chain by picking up the most likely word\n  for _ in range(length):\n    sentence += ' '\n    next_word = most_likely_word_after(current_word)\n    sentence += next_word\n    current_word = next_word\n  return sentence","37c5a7c8":"naive_chain('the')","4214db95":"print(naive_chain('the'))\nprint(naive_chain('I'))\nprint(naive_chain('What'))\nprint(naive_chain('park'))","f7f02ef1":"# @title Function for weighted choice\ndef weighted_choice(objects, weights):\n  \"\"\"\n  Returns randomly an element from the sequence of 'objects',\n      the likelihood of the objects is weighted according\n      to the sequence of 'weights', i.e. percentages.\n  \"\"\"\n\n  weights = np.array(weights, dtype=np.float64)\n  sum_of_weights = weights.sum()\n  # standardization:\n  np.multiply(weights, 1 \/ sum_of_weights) # zero to one\n  weights = weights.cumsum()\n  x = random.random()\n  for i in range(len(weights)):\n    if x < weights[i]:\n      return objects[i]","002f6078":"# @title Function for sampling next word with weights\ndef sample_next_word_after(word, alpha=0):\n  next_word_vector = next_word_matrix[word_idx_dict[word]] + alpha\n  likelihoods = next_word_vector\/next_word_vector.sum()\n  return weighted_choice(distinct_words, weights=likelihoods)","362da2ca":"sample_next_word_after('The')","83d0f025":"sample_next_word_after('The')","c6e12333":"# @title Function for a stochastic chain using weighted choice\ndef stochastic_chain(word, length=15):\n  current_word = word\n  sentence = word\n\n  for _ in range(length):\n    sentence += ' '\n    next_word = sample_next_word_after(current_word)\n    sentence += next_word\n    current_word = next_word\n\n  return sentence","0430daa1":"stochastic_chain('Hospital')","d3092715":"k = 3","3e25b91d":"def sequences_matrices(k=3):\n  # @title Code to build sets of words for more realistic sequences\n  sets_of_k_words = [' '.join(corpus_words[i:i+k]) for i, _ in enumerate(corpus_words[:-k])]\n  sets_count = len(list(set(sets_of_k_words)))\n  next_after_k_words_matrix = dok_matrix((sets_count, len(distinct_words)))\n  distinct_sets_of_k_words = list(set(sets_of_k_words))\n  k_words_idx_dict = {word: i for i, word in enumerate(distinct_sets_of_k_words)}\n  distinct_k_words_count = len(list(set(sets_of_k_words)))\n  for i, word in tqdm(enumerate(sets_of_k_words[:-k])):\n    word_sequence_idx = k_words_idx_dict[word]\n    next_word_idx = word_idx_dict[corpus_words[i+k]]\n    next_after_k_words_matrix[word_sequence_idx, next_word_idx] += 1\n  return k_words_idx_dict,distinct_sets_of_k_words,next_after_k_words_matrix\n\nk_words_idx_dict, distinct_sets_of_k_words, next_after_k_words_matrix = sequences_matrices(k=k)","0d9c61b1":"distinct_sets_of_k_words[:10]","f1109ecd":"# @title Code to populate matrix of sets of words\nfor i, word in tqdm(enumerate(distinct_sets_of_k_words[:-k])):\n  word_sequence_idx = k_words_idx_dict[word]\n  next_word_idx = word_idx_dict[corpus_words[i+k]]\n  next_after_k_words_matrix[word_sequence_idx, next_word_idx] += 1","53cadb2c":"# @title Function for stochastic Chain for sets of words\ndef stochastic_chain_sequence(words, chain_length=15, k=2):\n  current_words = words.split(' ')\n  if len(current_words) != k:\n    raise ValueError(f'wrong number of words, expected {k}')\n  sentence = words\n\n  # pre-calculate seq embedding + transition matrix for a given k\n  matrices = sequences_matrices(k=k)\n\n  for _ in range(chain_length):\n    sentence += ' '\n    next_word = sample_next_word_after_sequence(matrices,' '.join(current_words))\n    sentence += next_word\n    current_words = current_words[1:]+[next_word]\n  return sentence","5602c471":"# @title Function to sample next word after sequence\ndef sample_next_word_after_sequence(matrices, word_sequence, alpha=0):\n  # unpack a tuple of matrices\n  k_words_idx_dict,distinct_sets_of_k_words, next_after_k_words_matrix = matrices\n\n  next_word_vector = next_after_k_words_matrix[k_words_idx_dict[word_sequence]] + alpha\n  likelihoods = next_word_vector\/next_word_vector.sum()\n  return weighted_choice(distinct_words, likelihoods.toarray())","176a0970":"stochastic_chain_sequence('Judges under the', chain_length=3, k=3)","9efea623":"stochastic_chain_sequence('Judges under the jurisdiction of', chain_length=10, k=5)","1095431a":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q1', text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","dd3b09cf":"# load the data\nsentences = brown.sents(categories=category)\nwords = [word.lower() for sentence in sentences for word in sentence]\nlengths = [len(sentence) for sentence in sentences]\nalphabet = set(words)\n\n# Encode words\nle = LabelEncoder()\n_ = le.fit(list(alphabet))\n\n# Find word freqeuncies\nseq = le.transform(words)\nfeatures = np.fromiter(seq, np.int64)\nfeatures = np.atleast_2d(features).T\nfd = FreqDist(seq)","26fe849f":"print(features)","e531f066":"set(['raro','raro','foo'])","d88ae1c6":"fd","ea4dfeed":"# @title Function to create default Multinomial HMM model\ndef get_model(num_states):\n  print(\"Initial parameter estimation using built-in method\")\n  model = hmm.MultinomialHMM(n_components=num_states, init_params='ste')\n  return model","43084614":"# @title Function to create default Multinomial HMM model information of relative frequencies of words\ndef frequencies(num_states):\n  print(\"Initial parameter estimation using relative frequencies\")\n\n  frequencies = np.fromiter((fd.freq(i) for i in range(len(alphabet))),\n                            dtype=np.float64)\n  emission_prob = np.stack([frequencies]*num_states)\n\n  model = hmm.MultinomialHMM(n_components=num_states, init_params='st')\n  model.emissionprob_ = emission_prob\n  return model\n\n\nprint(frequencies(2))","71bed069":"## Fitting a default multinomial HMM. This is lengthy (~17 mins)\ndef run_model(features, length, num_states):\n  model = get_model(num_states)\n  model = model.fit(features, lengths)\n\n  return model\n\n\nnum_states = 8\n## Uncomment, if you have time!\n# model = run_model(features, lengths, num_states)\n\n## Another way to get a model is to use default frequencies when initialising the model\nmodel = frequencies(num_states)","df3e9f27":"# @markdown Execute this cell to download the saved model.\nimport pickle\n\nurl = \"https:\/\/osf.io\/5k6cs\/download\"\nr = requests.get(url)\nwith open('model_w2d3_t1.pkl', 'wb') as fd:\n  fd.write(r.content)\n\n# Load the pickle file\nwith open(\"model_w2d3_t1.pkl\", \"rb\") as file:\n  model = pickle.load(file)","fba8fb8c":"# @title Function to generate words given a hmm model\ndef generate_text(model, num_lines = 5, random_len=15):\n  for _i in range(num_lines):\n    set_seed(_i)\n    symbols, _states = model.sample(random_len)\n\n    output = le.inverse_transform(np.squeeze(symbols))\n    for word in output:\n      print(word, end=\" \")\n    print()","48df66ce":"generate_text(model, num_lines=2, random_len=20)","e0e8de91":"# load your own dataset and create a model using the frequencies based HMM model!","d058be4a":"# @title Video 2: Textual Dimension Reduction\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1oM4y1P7Mn\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"kweySXAZ1os\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 2: Textual Dimension Reduction')\n\ndisplay(out)","9020a35e":"category = ['editorial', 'fiction', 'government', 'mystery', 'news', 'religion',\n            'reviews', 'romance', 'science_fiction']","e90f9caf":"def create_word2vec_model(category='news', size=50, sg=1, min_count=5):\n  try:\n    sentences = brown.sents(categories=category)\n    model = Word2Vec(sentences, vector_size=size, sg=sg, min_count=min_count)\n\n  except (AttributeError, TypeError):\n      raise AssertionError('Input variable \"category\" should be a string or list,'\n      '\"size\", \"sg\", \"min_count\" should be integers')\n\n  return model\n\ndef model_dictionary(model):\n  words = list(model.wv.key_to_index)\n  return words\n\ndef get_embedding(word, model):\n  if word in model.wv.key_to_index:\n    return model.wv[word]\n  else:\n    return None","d65e3d75":"all_categories = brown.categories()","4861b8ac":"all_categories","dfcbcaee":"w2vmodel = create_word2vec_model(all_categories)","9656a13f":"#print(model_dictionary(w2vmodel))","eb9651b4":"print(get_embedding('weather', w2vmodel))","616c6ff2":"keys = ['voters', 'magic', 'love', 'God', 'evidence', 'administration', 'governments']","b474beac":"def get_cluster_embeddings(keys):\n  embedding_clusters = []\n  word_clusters = []\n\n  # find closest words and add them to cluster\n  for word in keys:\n    embeddings = []\n    words = []\n    if not word in w2vmodel.wv.key_to_index:\n      print('The word ', word, 'is not in the dictionary')\n      continue\n\n    for similar_word, _ in w2vmodel.wv.most_similar(word, topn=10):\n      words.append(similar_word)\n      embeddings.append(w2vmodel.wv[similar_word])\n    embedding_clusters.append(embeddings)\n    word_clusters.append(words)\n\n  # get embeddings for the words in clusers\n  embedding_clusters = np.array(embedding_clusters)\n  n, m, k = embedding_clusters.shape\n  tsne_model_en_2d = TSNE(perplexity=10, n_components=2, init='pca', n_iter=3500, random_state=32)\n  embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n\n  return embeddings_en_2d, word_clusters","8372d629":"def tsne_plot_similar_words(title, labels, embedding_clusters,\n                            word_clusters, a, filename=None):\n  plt.figure(figsize=(16, 9))\n  colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n  for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n    x = embeddings[:, 0]\n    y = embeddings[:, 1]\n    plt.scatter(x, y, color=color, alpha=a, label=label)\n    for i, word in enumerate(words):\n      plt.annotate(word,\n                   alpha=0.5,\n                   xy=(x[i], y[i]),\n                   xytext=(5, 2),\n                   textcoords='offset points',\n                   ha='right',\n                   va='bottom',\n                   size=10)\n  plt.legend(loc=\"lower left\")\n  plt.title(title)\n  plt.grid(True)\n  if filename:\n    plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n  plt.show()","671cca2b":"embeddings_en_2d, word_clusters = get_cluster_embeddings(keys)\ntsne_plot_similar_words('Similar words from Brown Corpus', keys, embeddings_en_2d, word_clusters, 0.7)","462090c2":"# @title Download FastText English Embeddings of dimension 100\nimport os, io, zipfile\nfrom urllib.request import urlopen\n\nzipurl = 'https:\/\/osf.io\/w9sr7\/download'\nprint(f\"Downloading and unzipping the file... Please wait.\")\nwith urlopen(zipurl) as zipresp:\n  with zipfile.ZipFile(io.BytesIO(zipresp.read())) as zfile:\n    zfile.extractall('.')\nprint(\"Download completed!\")","94dd0834":"# Load 100 dimension FastText Vectors using FastText library\nft_en_vectors = fasttext.load_model('cc.en.100.bin')","2d6676e7":"print(f\"Length of the embedding is: {len(ft_en_vectors.get_word_vector('king'))}\")\nprint(f\"Embedding for the word King is: {ft_en_vectors.get_word_vector('king')}\")","12250dff":"ft_en_vectors.get_nearest_neighbors(\"king\", 10)  # Most similar by key","e07981dd":"# @title Video 3: Semantic Measurements\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV15w411R7SW\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"Y45KIAOw4OY\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 3: Semantic Measurements')\n\ndisplay(out)","49a569b0":"def getSimilarity(word1, word2):\n  v1 = ft_en_vectors.get_word_vector(word1)\n  v2 = ft_en_vectors.get_word_vector(word2)\n  return cosine_similarity(v1, v2)\n\nprint(\"Similarity between the words King and Queen: \", getSimilarity(\"king\", \"queen\"))\nprint(\"Similarity between the words King and Knight: \", getSimilarity(\"king\", \"knight\"))\nprint(\"Similarity between the words King and Rock: \", getSimilarity(\"king\", \"rock\"))\nprint(\"Similarity between the words King and Twenty: \", getSimilarity(\"king\", \"twenty\"))\nprint(\"Similarity between the words man and woman: \", getSimilarity(\"man\", \"woman\"))\nprint(\"Similarity between the words Tee and Caffee: \", getSimilarity(\"tea\", \"coffee\"))\n\n## Try the same for two more pairs\n# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n\n# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))","dc1729cb":"#######################     Words with multiple meanings     ##########################\nprint(\"Similarity between the words Cricket and Insect: \", getSimilarity(\"cricket\", \"insect\"))\nprint(\"Similarity between the words Cricket and Sport: \", getSimilarity(\"cricket\", \"sport\"))\n\n\n## Try the same for two more pairs\n# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n\n# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))","99189436":"## Use get_analogies() funnction. The words have to be in the order Positive, negative,  Positve\n\n# Man : Woman  ::  King : _____\n# Positive=(woman, king), Negative=(man)\nprint(ft_en_vectors.get_analogies(\"woman\", \"man\", \"king\",1))\n\n# Germany: Berlin :: France : ______\n# Positive=(berlin, frannce), Negative=(germany)\nprint(ft_en_vectors.get_analogies(\"berlin\", \"germany\", \"france\",1))\n\n# Leaf : Tree  ::  Petal : _____\n# Positive=(tree, petal), Negative=(leaf)\nprint(ft_en_vectors.get_analogies(\"tree\", \"leaf\", \"petal\",1))\n\n# Hammer : Nail  ::  Comb : _____\n# Positive=(nail, comb), Negative=(hammer)\nprint(ft_en_vectors.get_analogies(\"nail\", \"hammer\", \"comb\",1))","d0ea1782":"# Poverty : Wealth  :: Sickness : _____\nprint(ft_en_vectors.get_analogies(\"wealth\", \"poverty\", \"sickness\",1))\n\n# train : board :: horse : _____\nprint(ft_en_vectors.get_analogies(\"board\", \"train\", \"horse\",1))\nprint(ft_en_vectors.get_analogies(\"rich\", \"poor\", \"clever\",1))","07c617dc":"# @title Download embeddings and clear old variables to clean memory.\n# @markdown #### Execute this cell!\nif 'ft_en_vectors' in locals():\n  del ft_en_vectors\nif 'w2vmodel' in locals():\n  del w2vmodel\n\nembedding_fasttext = FastText('simple')","4eeb470a":"# @markdown Load the Dataset\nTEXT, vocab_size, train_iter, valid_iter, test_iter = load_dataset(embedding_fasttext, seed=SEED)","488fae4b":"class NeuralNet(nn.Module):\n  def __init__(self, output_size, hidden_size, vocab_size, embedding_length,\n               word_embeddings):\n    super(NeuralNet, self).__init__()\n\n    self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n    self.word_embeddings.weight = nn.Parameter(word_embeddings,\n                                               requires_grad=False)\n    self.fc1 = nn.Linear(embedding_length, hidden_size)\n    self.fc2 = nn.Linear(hidden_size, output_size)\n\n\n  def forward(self, inputs):\n\n    input = self.word_embeddings(inputs)  # convert text to embeddings batch x vocab x hidden\n    ####################################################################\n    # Fill in missing code below (...)\n    # raise NotImplementedError(\"Fill in the Neural Net\")\n    ####################################################################\n    # Average the word embeddings in a sentence\n    # Use torch.nn.functional.avg_pool2d to compute the averages\n \n    pooled = F.avg_pool2d(input, (input.shape[1],1)).squeeze(1) # batch x 1 x hidden\n\n    # Pass the embeddings through the neural net\n    # A fully-connected layer\n    x = self.fc1(pooled)\n    # ReLU activation\n    x = F.relu(x)\n    # Another fully-connected layer\n    x = self.fc2(x)\n    output = F.log_softmax(x, dim=1)\n\n    return output\n\n\n# add event to airtable\natform.add_event('Coding Exercise 3.1: Simple Feed Forward Net')\n\n# Uncomment to check your code\nnn_model = NeuralNet(2, 128, 100, 300, TEXT.vocab.vectors)\nprint(nn_model)","1a77b77b":"# @title Training and Testing Functions\n\n# @markdown #### `train(model, device, train_iter, valid_iter, epochs, learning_rate)`\n# @markdown #### `test(model, device, test_iter)`\n\ndef train(model, device, train_iter, valid_iter, epochs, learning_rate):\n  criterion = nn.CrossEntropyLoss()\n  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n  train_loss, validation_loss = [], []\n  train_acc, validation_acc = [], []\n\n  for epoch in range(epochs):\n    # train\n    model.train()\n    running_loss = 0.\n    correct, total = 0, 0\n    steps = 0\n\n    for idx, batch in enumerate(train_iter):\n      text = batch.text[0]\n      target = batch.label\n      target = torch.autograd.Variable(target).long()\n      text, target = text.to(device), target.to(device)\n\n      # add micro for coding training loop\n      optimizer.zero_grad()\n      output = model(text)\n      loss = criterion(output, target)\n      loss.backward()\n      optimizer.step()\n      steps += 1\n      running_loss += loss.item()\n\n      # get accuracy\n      _, predicted = torch.max(output, 1)\n      total += target.size(0)\n      correct += (predicted == target).sum().item()\n    train_loss.append(running_loss\/len(train_iter))\n    train_acc.append(correct\/total)\n\n    print(f'Epoch: {epoch + 1}, '\n          f'Training Loss: {running_loss\/len(train_iter):.4f}, '\n          f'Training Accuracy: {100*correct\/total: .2f}%')\n\n    # evaluate on validation data\n    model.eval()\n    running_loss = 0.\n    correct, total = 0, 0\n\n    with torch.no_grad():\n      for idx, batch in enumerate(valid_iter):\n        text = batch.text[0]\n        target = batch.label\n        target = torch.autograd.Variable(target).long()\n        text, target = text.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(text)\n\n        loss = criterion(output, target)\n        running_loss += loss.item()\n\n        # get accuracy\n        _, predicted = torch.max(output, 1)\n        total += target.size(0)\n        correct += (predicted == target).sum().item()\n\n    validation_loss.append(running_loss\/len(valid_iter))\n    validation_acc.append(correct\/total)\n\n    print (f'Validation Loss: {running_loss\/len(valid_iter):.4f}, '\n           f'Validation Accuracy: {100*correct\/total: .2f}%')\n\n  return train_loss, train_acc, validation_loss, validation_acc\n\n\ndef test(model, device, test_iter):\n  model.eval()\n  correct = 0\n  total = 0\n  with torch.no_grad():\n    for idx, batch in enumerate(test_iter):\n      text = batch.text[0]\n      target = batch.label\n      target = torch.autograd.Variable(target).long()\n      text, target = text.to(device), target.to(device)\n\n      outputs = model(text)\n      _, predicted = torch.max(outputs, 1)\n      total += target.size(0)\n      correct += (predicted == target).sum().item()\n\n    acc = 100 * correct \/ total\n    return acc","d7fc57eb":"# Model hyperparameters\nlearning_rate = 0.0003\noutput_size = 2\nhidden_size = 128\nembedding_length = 300\nepochs = 15\nword_embeddings = TEXT.vocab.vectors\nvocab_size = len(TEXT.vocab)\n\n# Model set-up\nnn_model = NeuralNet(output_size,\n                     hidden_size,\n                     vocab_size,\n                     embedding_length,\n                     word_embeddings)\nnn_model.to(DEVICE)\nnn_start_time = time.time()\nset_seed(522)\nnn_train_loss, nn_train_acc, nn_validation_loss, nn_validation_acc = train(nn_model,\n                                                                           DEVICE,\n                                                                           train_iter,\n                                                                           valid_iter,\n                                                                           epochs,\n                                                                           learning_rate)\nprint(f\"--- Time taken to train = {(time.time() - nn_start_time)} seconds ---\")\ntest_accuracy = test(nn_model, DEVICE, test_iter)\nprint(f'\\n\\nTest Accuracy: {test_accuracy}%')","143a4343":"# Plot accuracy curves\nplt.figure()\nplt.subplot(211)\nplot_train_val(np.arange(0, epochs), nn_train_acc, nn_validation_acc,\n               'train accuracy', 'val accuracy',\n               'Neural Net on IMDB text classification', 'accuracy',\n               color='C0')\nplt.legend(loc='upper left')\nplt.subplot(212)\nplot_train_val(np.arange(0, epochs), nn_train_loss,\n               nn_validation_loss,\n               'train loss', 'val loss',\n               '',\n               'loss [a.u.]',\n               color='C0')\nplt.legend(loc='upper left')\nplt.show()","1144afa6":"# @title Airtable Submission Link\nfrom IPython import display as IPydisplay\nIPydisplay.HTML(\n   f\"\"\"\n <div>\n   <a href= \"{atform.url()}\" target=\"_blank\">\n   <img src=\"https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/static\/AirtableSubmissionButton.png?raw=1\"\n alt=\"button link to Airtable\" style=\"width:410px\"><\/a>\n   <\/div>\"\"\" )","1202eca4":"###  Figure Settings\n","4c908802":"There! We don't see the same word twice, because of the added randomisation (i.e., stochasticity). Our algorithm calculates how likely it is to find a certain word after a given word (`The` in this case) in the corpus, and then generates 1 sample of the next word with a matching probability. \n\nIn this example, we generated only one next word. Now, using this function, we'll build a chain.","ca94fc68":"###  Function to generate words given a hmm model\n","01072341":"We now have what we need to build a stochastic chain over a `K` set of words.","0297f0a2":"```\nNeuralNet(\n  (word_embeddings): Embedding(100, 300)\n  (fc1): Linear(in_features=300, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=2, bias=True)\n)\n```","38fed088":"Define a vanilla neural network with linear layers. Then average the word embeddings to get an embedding for the entire review.\nThe neural net will have one hidden layer of size 128.","0f64127e":" There may be `Errors`\/`Warnings` reported during the installation. However, they are to be ignored.\n","a896d099":"####  Function to sample next word after sequence\n","b85f0baa":"##  Airtable Submission Link\n","82374450":"## Section 2.3: Exploring meaning with word embeddings\n\nWhile word2vec was the method that started it all, research has since boomed, and we now have more sophisticated ways to represent words. One such method is FastText, developed at Facebook AI research, which breaks words into sub-words: such a technique also allows us to create embedding representations for unseen words. In this section, we will explore how semantics and meaning are captured using embedidngs, after downloading a pre-trained FastText model. Downloading pre-trained models is a way for us to plug in word embeddings and explore them without training them ourselves.","a0e48175":" #### Execute this cell!\n","24806832":"###  Download embeddings and clear old variables to clean memory.\n","23ee8488":"##  Video 1: Sequences & Markov Processes\n","143dde7a":" Execute this cell to download the saved model.\n","6206b03e":"####  Function for building Naive Chain\n","ef48b771":"We see that a hidden markov model also does well in generating text. We encourage you to try out different initialisations and hyperparameters to see how the model does.","da0a5a73":"###   Load Dataset from `nltk`\n","92dc62c3":"###  Set random seed\n","41e9f986":"Now that we have our sentences, let us look at some statistics to get an idea of what we are dealing with.","ebeb5cb4":"---\n## Setup","00430e7d":"## Coding Exercise 3.1: Simple Feed Forward Net","d0d1d4d6":"----\n# Tutorial objectives\n\nBefore we begin with exploring how RNNs excel at modelling sequences, we will explore some of the other ways we can model sequences, encode text, and make meaningful measurements using such encodings and embeddings. ","11657c0f":"###  Download FastText English Embeddings of dimension 100\n","90f48810":"###  Set device (GPU or CPU). Execute `set_device()`\n","d41ad6e9":"But, does it always work?\n\n\n1.   Poverty : Wealth  :: Sickness : _____\n2.   train : board :: horse : _____","fb5f7bfc":"####  Function for most likely word\n","f58c5f35":"---\n# Section 1: Sequences, Markov Chains & HMMs\n\n*Time estimate: ~45mins*","d4e9609a":"\n## Why is this relevant? How are these sequences related to modern recurrent neural networks?\n\nLike we mentioned before, the notion of modelling sequences of data - in this particular case, **language**, is an ideal place to start. RNNs themselves were constructed keeping in mind sequences, and the ability to temporally model sequences is what inspired RNNs (and the family of LSTM, GRUs - we will see this in the next notebook).\n\nMarkov models and hidden markov models serve as an introduction to these concepts because they were some of the earliest ways to think about sequences. They do not capture a lot of the complexity that RNNs excel at, but are an useful way of thinking of sequences, probabilities, and how we can use these concepts to perform  tasks such as text generation, or classification - tasks that RNNs excel at today. \n\nThink of this section as an introduction to thinking with sequences and text data, and as a historical introduction to the world of modelling sequential data. ","1dca0c80":"### Useful links for Markov Models and HMM:\n\nHere are some useful links if you wish to explore this topic further.\n\n- [Markov Chain Text](https:\/\/towardsdatascience.com\/simulating-text-with-markov-chains-in-python-1a27e6d13fc6)\n\n- [Python QuantEcon: Finite Markov Chains with Finance](https:\/\/python.quantecon.org\/finite_markov.html)\n\n- [Markov Models from the ground up, with python](https:\/\/ericmjl.github.io\/essays-on-data-science\/machine-learning\/markov-models\/)\n\n- [GenTex](https:\/\/github.com\/nareshkumar66675\/GenTex)\n\n- [HMM learn](https:\/\/hmmlearn.readthedocs.io\/en\/latest\/tutorial.html)","fc385077":"\n\nIn this notebook we will be exploring the world of sequences - thinking of what kind of data can be thought of as sequences, and how these sequences can be represented as Markov Chains and Hidden Markov Models. These ideas and methods were an important part of natural language processing and language modelling, and serve as a useful way to ground ourselves before we dive into neural network methods.","d927c866":"####  Video 3: Semantic Measurements\n","e1363c68":"We notice that after the word `the`, `United States` comes up each time. All the other sequencies starting from other words also end up at `the` quite often. Since we use a *deterministic* markov chain model, its next state only depends on the previous one. Therefore, once the sequence comes to `the`, it inevitably continues the sequence with the `United States`.","4f0e1750":"###  Install dependencies\n","41533a70":" Executing `set_seed(seed=seed)` you are setting the seed\n","7abe5610":"##  Tutorial slides\n","8a2c8117":"**Note**:\n\nThe following lines of code are commented out because they take a long time (~17 mins for default Brown corpus categories). \n\nIf you do not have that time, you can download the default model to try to generate text. You have to uncomment the appropriate lines.\n\n**Note:** Either you may want to uncomment Line 11 or Line 14, not both, as the output variable `model` will be overwritten.","6eacb7c6":"Now we have the information ready to construct a markov chain. The next word matrix is crucial in this, as it allows us to go from one word in the sequence to the next. We will soon see how this is used.","3a97704f":"## Section 2.2: Visualizing Word Embedding\n\nWe can now obtain the word embeddings for any word in the dictionary using word2vec. Let's visualize these embeddings to get an inuition of what these embeddings mean. The word embeddings obtained from word2vec model are in high dimensional space. We will use `tSNE` (t-distributed stochastic neighbor embedding), a statistical method for dimensionality deduction that allow us to visualize high-dimensional data in a 2D or 3D space. Here, we will use `tSNE` from [`scikit-learn`] module(https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html) (if you are not familiar with this method, think about `PCA`) to project our high dimensional embeddings in the 2D space.\n\n\nFor each word in `keys`, we pick the top 10 similar words (using cosine similarity) and plot them.  \n\n What should be the arrangement of similar words?\n What should be arrangement of the key clusters with respect to each other?\n ","1a5265e2":"### Word Similarity","770f6e00":"####  Function for stochastic Chain for sets of words\n","3e22eb12":"###  Function to create default Multinomial HMM model information of relative frequencies of words\n","99dff9cd":"Since we will be modelling words as sequences in sentences, let us first collect all the words in our corpus.","9fcb7c01":"Alternatively, you could use a saved model. Here is a [link](https:\/\/drive.google.com\/file\/d\/1IymcmcO48V6q3x-6dhf7-OU5NByo5W2F\/view?usp=sharing) to the default model, which you can download and then upload into Colab.","56d5a9de":"# Tutorial 1: Modeling sequencies and encoding text\n**Week 2, Day 3: Modern RNNs**\n\n**By Neuromatch Academy**\n\n__Content creators:__ Bhargav Srinivasa Desikan, Anis Zahedifard, James Evans\n\n__Content reviewers:__ Lily Cheng, Melvin Selim Atay, Ezekiel Williams, Kelson Shilling-Scrivo\n\n__Content editors:__ Nina Kudryashova, Spiros Chavlis\n\n__Production editors:__ Roberto Guidotti, Spiros Chavlis\n","b5c36eeb":"Let us now use this naive chain to see what comes up, using some simple words.","84afc630":" Load the Dataset\n","126baf9c":"###  Creating Matrices and Distinct Words\n","aa1eeef4":"##  Video 2: Textual Dimension Reduction\n","11ac5a4e":"Cosine similarity is used for similarities between words. Similarity is a scalar between 0 and 1.","4c70c5cd":"Let's use the pretrained FastText embeddings to train a neural network on the IMDB dataset. \n\nTo recap, the data consists of reviews and sentiments attached to it. It is a binary classification task. As a simple preview of the upcoming neural networks, we are going to introduce neural net with word embeddings. We'll see detailed networks in the next tutorial.\n\n\n","532cdc5f":"####  Function for a stochastic chain using weighted choice\n","53c5e267":"**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n\n<p align='center'><img src='https:\/\/github.com\/NeuromatchAcademy\/widgets\/blob\/master\/sponsors.png?raw=True'\/><\/p>","36c9a691":"## Section 2.1: Creating Word Embeddings\n\nWe will create embeddings for a subset of categories in [Brown corpus](https:\/\/www1.essex.ac.uk\/linguistics\/external\/clmt\/w3c\/corpus_ling\/content\/corpora\/list\/private\/brown\/brown.html).  In order to achieve this task we will use [gensim](https:\/\/radimrehurek.com\/gensim\/) library to create word2vec embeddings. Gensim\u2019s word2vec expects a sequence of sentences as its input. Each sentence is a list of words.\nCalling `Word2Vec(sentences, iter=1)` will run two passes over the sentences iterator (or, in general iter+1 passes). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model. \n`Word2vec` accepts several parameters that affect both training speed and quality.\n\nOne of them is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there\u2019s not enough data to make any meaningful training on those words, so it\u2019s best to ignore them:\n\n`model = Word2Vec(sentences, min_count=10)  # default value is 5`\n\n\nA reasonable value for min_count is between 0-100, depending on the size of your dataset.\n\nAnother parameter is the size of the NN layers, which correspond to the \u201cdegrees\u201d of freedom the training algorithm has:\n\n`model = Word2Vec(sentences, size=200)  # default value is 100`\n\n\nBigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds.\n\nThe last of the major parameters (full list [here](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html#gensim.models.word2vec.Word2Vec)) is for training parallelization, to speed up training:\n\n`model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization`","ecb4bde6":"###  Training and Testing Functions\n","0fdd4148":"### Homonym Words$^\\dagger$\n\nFind the similarity for homonym words with their different meanings. The first one has been implemented for you.\n\n\n$^\\dagger$: Two or more words having the same spelling or pronunciation but different meanings and origins are called *homonyms*. E.g., ","314ca152":"Using our most likely word function, we can begin to create chains of words and create sequences. In the code below we create a naive chain that simply choses the most likely word.","acb5ff9a":"Now that we have our data setup, we can create our model. We use a multinomial HMM with 8 states, and can either do a random initialisation or use word frequences. We recommend trying both options!","645d4dc6":"More on similarity between words. Let's check how similar different pairs of word are. Feel free to play around.\n\n","26378b61":"### Exercise 1.3: Transition probabilities \n\n\nWe have seen how we can use sequences of text to form probability chains, as well as how we can use out of the box models to generate text. In this exercise, you will be using your own data to generate sequences using ```hmmlearn``` or any other implementation of a markov model. Explore the transition probabilities in your corpus and generate sentences. For example, one such exploration can be - how does using a model with the word frequencies incorporated in compare to using a default model?\n\nPerform any one such comparison or exploration, and generate 3 sentences or 50 words using your model. You should be able to use all the existing functions defined for this exercise.","1f609494":"Let's have a look at what that bit of code did.","c59851e8":"Neat - we can create stochastic chains for a single word. For a more effective language model, we would want to model sets of words - in the following cells, we create sets of words to predict a chain after a sequence.","efb4ea74":"---\n# Summary\n\nIn this tutorial, we explored two different concepts linked to sequences, and text in particular, that will be the conceptual foundation for Recurrent Neural Networks.\n\nThe first concept was that of sequences and probabilities. We saw how we can model language as sequences of text, and use this analogy to generate text. Such a setup is also used to classify text or identify parts of speech. We can either build chains manually using simple python and numerical computation, or use a package such as ```hmmlearn``` that allows us to train models a lot easier. These notions of sequences and probabilities (i.e, creating language models!) are key to the internals of a recurrent neural network as well. \n\nThe second concept is that of word embeddings, now a mainstay of natural language processing. By using a neural network to predict context of words, these neural networks learn internal representions of words that are a decent approximation of semantic meaning (i.e embeddings!). We saw how these embeddings can be visualised, as well as how they capture meaning. We finally saw how they can be integrated into neural networks to better classify text documents.","04e91ccc":"###  Function to create default Multinomial HMM model\n","a0f14a55":"####  Function for sampling next word with weights\n","35964643":"We can now be a little more sophisticated, and return words in a sequence using a *weighted choice*, which randomly selects the next word from a set of words with some probability (weight).","d8b1fd93":"<a href=\"https:\/\/colab.research.google.com\/github\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/W2D3_ModernRecurrentNeuralNetworks\/W2D3_Tutorial1.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\"\/><\/a>, \"<a href=\"https:\/\/kaggle.com\/kernels\/welcome?src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D3_ModernRecurrentNeuralNetworks\/student\/W2D3_Tutorial1.ipynb\" target=\"_blank\"><img alt=\"Open in Kaggle\" src=\"https:\/\/kaggle.com\/static\/images\/open-in-kaggle.svg\"\/><\/a>\"","795eed29":"####  Code to populate matrix of sets of words\n","43af5e9a":"In the following lines of code we are populating the matrix that tracks the next word in a sentence.","600e3cef":"####  Student Response\n","ef2a2b2f":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D3_ModernRecurrentNeuralNetworks\/solutions\/W2D3_Tutorial1_Solution_6b55212b.py)\n\n","2ca889b9":"This gives us an idea of what our dataset looks like, along with some average lengths. This kind of quick data exploration can be very useful - we know how long different sequences are, and how we might want to collect these words.","627dfdac":"###  Helper functions\n","fdfb4bc1":"Great! This sentence was created using two of the techniques we recently saw - creating sets of words, and using a weighted average stochastic chain. Both of these methods contributed in making it a more meaningful sequence of words. Some of these notions are also captured by Recurrent Neural Networks!","e3391488":"## Section 1.2: What is a Markov Chain or Model?\n\nA Markov Chain (or Model) is a:\n- stochastic model describing a sequence of possible events\n- the probability of each event depends only on the state attained in the previous event.\n- a countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC) [vs. a continuous-time process or CTMC].\n- The classic formal language model is a Markov Model\n\n*Helpful explanations from [eric mjl's tutorial](https:\/\/ericmjl.github.io\/essays-on-data-science\/machine-learning\/markov-models\/#non-autoregressive-homoskedastic-emissions)*!\n\n\n\nThe simplest Markov models assume that we have a _system_ that contains a finite set of states,\nand that the _system_ transitions between these states with some probability at each time step $t$,\nthus generating a sequence of states over time.\nLet's call these states $S$, where\n\n\\begin{equation}\nS = \\{s_1, s_2, ..., s_n\\}\n\\end{equation}\n\nTo keep things simple, let's start with three states:\n\n\\begin{equation}\nS = \\{s_1, s_2, s_3\\}\n\\end{equation}\n\nA Markov model generates a sequence of states, with one possible realization being:\n\n\\begin{equation}\n\\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\}\n\\end{equation}\n\nAnd generically, we represent it as a sequence of states $x_t, x_{t+1}... x_{t+n}$. (We have chosen a different symbol to not confuse the \"generic\" state with the specific realization. Graphically, a plain and simple Markov model looks like the following:\n\n<center><img src=\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D3_ModernRecurrentNeuralNetworks\/static\/cell_chain.png\" width=\"500\"\/><\/center>","222b8079":"Now find the 10 most similar words to \"King\"","95029d2a":"## Section 1.3: What is a Hidden Markov Model?\n\nA 1960s advance (by Leonard Baum and colleagues): Hidden Markov Models are:\n- a Markov model in which the system modeled is assumed to be a Markov process\/chain with unobservable (\"hidden\") states. \n- HMM assumes there is another surrogate process whose behavior \"depends\" on the state--you learn about the state by observing the surrogate process. \n- HMMs have successfully been applied in fields where the goal is to recover a data sequence not immediately observable (but other data that depend on the sequence are).\n- The first dominant application: Speech and text processing (1970s)\n\nIn this sub-section we will use the python library [hmmlearn](https:\/\/hmmlearn.readthedocs.io\/en\/latest\/tutorial.html#training-hmm-parameters-and-inferring-the-hidden-states), which is part of the *scikit-learn* ecosystem. [nlg-with-hmmlearn](https:\/\/github.com\/mfilej\/nlg-with-hmmlearn) offers useful code snippets to adapt ```hmmlearn``` for text data. Because we are using a package that offers many out of the box implementations for HMMs, we don't have to worry about the states, transition matrices, ","bd57f52b":"####  Function for weighted choice\n","1bb1ab65":"Great! Now we are going to create a transition matrix for the sets of words.","8a405bcf":"Find the 80-th percentile: the minimal length of such a sentence, which is longer than at least 80% of sentences in the *Brown corpus*.","4323bc52":"This will load 300 dim FastText embeddings. It will take around 2-3 minutes.","b860cb92":"### Think! 1.2: How does changing parameters affect the generated sentences?\n\nTry and use a set of words but using a naive chain, and try a stochastic chain with a low value of k (i.e., 2), and a higher value (i.e., 5). How do these different configurations change the quality of the sequences produced? Below you have sample code to try these out.\n\n```python\nstochastic_chain_sequence(..., chain_length=..., k=...)\n```\n\nYou should be able to use these matrices and the previous functions to be able to create the necessary configurations.","58ffab53":"## Section 1.1: What data are sequences?\n\nNative Sequences:\n\n- Temporally occurring events (e.g., history, stock prices)\n- Temporally processed events (e.g., communication)\n- Topologically connected components (e.g., polymers, peptides)\n\nSynthetic Sequences: \n\n- Anything processed as a sequence (e.g., scanned pixels in an image)\n\nSequences can be represented as a Markov Process - since this notion of sequential data is intrinsically linked to RNNs, it is a good place for us to start, and natural language (text!) will be our sequence of choice. \n\nWe will be using the Brown corpus which comes loaded with NLTK, and using the entire corpus - this requires a lot of RAM for some of the methods, so we recommend using a smaller subset of categories if you do not have enough RAM.\n\nWe will be using some of the code from this [tutorial](https:\/\/www.kdnuggets.com\/2019\/11\/markov-chains-train-text-generation.html) and this [Jupyter notebook](https:\/\/github.com\/StrikingLoo\/ASOIAF-Markov\/blob\/master\/ASOIAF.ipynb)\n\nThe first few cells of code all involve set-up; some of this code will be hidden because they are not necessary to understand the ideas of markov models, but the way data is setup can be vital to the way the model performs (something in common with neural network models!).\n\nLet us start with loading our corpus.\n\n","c742574c":"### Word Analogies\n\nEmbeddings can be used to find word analogies.\nLet's try it:\n1.   Man : Woman  ::  King : _____\n2.  Germany: Berlin :: France : ______\n3.  Leaf : Tree  ::  Petal : _____","df54112b":"\nWords or subword units such as morphemes are the basic units that we use to express meaning  in language. The technique of mapping words to vectors of real numbers is known as word embedding. \n\nWord2vec is based on theories of distributional semantics - words that appear around each other are more likely to mean similar things than words that do not appear around each other. Keeping this in mind, our job is to create a high dimensional space where these semantic relations are preserved. The innovation in word2vec is the realisation that we can use unlabelled, running text in sentences as inputs for a supervised learning algorithm--as a self-supervision task. It is supervised because we use the words in a sentence to serve as positive and negative examples. Let\u2019s break this down:\n\n... \"use the kitchen knife to chop the vegetables\"\u2026\n\n**C1   C2   C3   T   C4   C5   C6   C7**\n\nHere, the target word is knife, and the context words are the ones in its immediate (6-word) window. \nThe first word2vec method we\u2019ll see is called skipgram, where the task is to assign a probability for how likely it is that the context window appears around the target word. In the training process, positive examples are samples of words and their context words, and negative examples are created by sampling from pairs of words that do not appear nearby one another. \n\nThis method of implementing word2vec is called skipgram with negative sampling. So while the algorithm tries to better learn which context words are likely to appear around a target word, it ends up pushing the embedded representations for every word so that they are located optimally (e.g., with minimal semantic distortion). In this process of adjusting embedding values, the algorithm brings semantically similar words close together in the resulting high dimensional space, and dissimilar words far away. \n\nAnother word2vec training method, Continuous Bag of Words (CBOW), works in a similar fashion, and tries to predict the target word, given context. This is converse of skipgram, which tries to predict the context, given the target word. Skip-gram represents rare words and phrases well, often requiring more data for stable representations, while CBOW is several times faster to train than the skip-gram, but with slightly better accuracy for the frequent words in its prediction task. The popular gensim implementation of word2vec has both the methods included.  ","ec5e0b03":"We'll now get distinct (unique) words and create a matrix to represent all these words. This is necessary because we will be using this matrix to look at the probability of the words in sequences.","ba381bb4":"###  Populating Matric that tracks next word\n","e1fb7fff":"### Modelling transitions between states\n\nTo know how a system transitions between states, we now need a **transition matrix**.\n\nThe transition matrix describes the probability of transitioning from one state to another (The probability of staying in the same state is semantically equivalent to transitioning to the same state).\n\nBy convention, transition matrix rows correspond to the state at time $t$,\nwhile columns correspond to state at time $t+1$.\nHence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and all possible states are known and enumerated.\n\nLet's call the transition matrix $P_{transition}$:\n\n\\begin{equation}\nP_{transition} = \n  \\begin{pmatrix}\n  p_{11} & p_{12} & p_{13} \\\\\n  p_{21} & p_{22} & p_{23} \\\\\n  p_{31} & p_{32} & p_{33} \\\\\n  \\end{pmatrix}\n\\end{equation}\n\nUsing the transition matrix, we can express different behaviors of the system. For example:\n1. by assigning larger probability mass to the diagonals, we can express that the system likes to stay in the current state;\n2. by assigning larger probability mass to the off-diagonal, we can express that the system likes to transition out of its current state.\n\nIn our case, this matrix is created by measuring how often one word appeared after another.","5510d58d":"---\n# Section 2: Word Embeddings\n\n*Time estimate: ~60mins*","86f331af":"---\n# Section 3: Neural Net with word embeddings\n\n*Time estimate: ~16mins*"}}