{"cell_type":{"9be7cc15":"code","ea4337fe":"code","116db128":"code","777831ca":"code","592e491c":"code","d57653a1":"code","942e0463":"code","0a20d9a2":"code","4682a7fb":"code","dd823f2f":"code","03af86c6":"code","14468c40":"code","5ba4c37c":"code","1654cfa4":"code","fa70b349":"code","99eccddb":"code","e5bc6f61":"code","f128908d":"code","37c6d59b":"code","975fb1fa":"code","20f07e51":"code","55c42332":"code","62856cc1":"code","c3d6e983":"code","67f57468":"code","b343432f":"code","dda37ae3":"code","5861cc37":"code","7a7cc047":"code","050e09fa":"code","1942a258":"code","b7b0c184":"code","b1ecc990":"code","16e7d55b":"code","f1342134":"code","e0e55a86":"code","f2e0d168":"code","71394482":"code","59abe0a1":"code","490e3170":"code","06b82250":"code","3a130334":"code","6dd8d46a":"code","7a5bbdbf":"markdown","156fa027":"markdown","4f74bee8":"markdown","d72384d6":"markdown","77ea92aa":"markdown","cc729f44":"markdown","d91812e7":"markdown","52db8e61":"markdown","967b9272":"markdown","4f4c3cb6":"markdown","8f693160":"markdown","57fbbcc8":"markdown","0694f6c5":"markdown","2eba029a":"markdown","d58ded5d":"markdown","dac6d75a":"markdown","fb0c4cff":"markdown","b0266105":"markdown","3d84d700":"markdown","7bf20d0f":"markdown","95f6e73d":"markdown","56c58f5e":"markdown","9652f50a":"markdown","d146abb5":"markdown","43b3ef5a":"markdown","dc08cc69":"markdown","0cfb821d":"markdown","c5dbcd83":"markdown","ad1dbea8":"markdown","aac89eba":"markdown"},"source":{"9be7cc15":"! pip install --upgrade git+https:\/\/github.com\/keras-team\/keras.git \\\n                        git+https:\/\/github.com\/valeoai\/dl_utils.git \\\n                        imageio","ea4337fe":"import keras\nfrom keras.models import Sequential, Model \nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.layers import Dropout, Flatten,Dense\nfrom tensorflow.keras.optimizers import Adam\n\nimport numpy as np\nimport os\nfrom matplotlib import image,patches,patheffects\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n","116db128":"import os\nif not os.path.isdir('.\/VOCdevkit\/'):\n    ! wget http:\/\/pjreddie.com\/media\/files\/VOCtrainval_06-Nov-2007.tar\n    ! tar -xf VOCtrainval_06-Nov-2007.tar\n    #! wget https:\/\/storage.googleapis.com\/coco-dataset\/external\/PASCAL_VOC.zip\n    #! unzip -q PASCAL_VOC.zip -d VOCdevkit\/VOC2007\n    #! rm -Rf VOCdevkit\/VOC2007\/PASCAL_VOC\n    !mv  ..\/input\/pascal-voc-2007\/PASCAL_VOC\/PASCAL_VOC\/pascal_train2012.json VOCdevkit\/VOC2007\/pascal_train2007.json\n      ","777831ca":" !mv  ..\/input\/pascal-voc-2007\/PASCAL_VOC\/PASCAL_VOC\/pascal_train2007.json VOCdevkit\/VOC2007\/pascal_train2007.json","592e491c":"from pathlib import Path\n\nPATH = Path('VOCdevkit\/VOC2007\/')\nfor i in PATH.iterdir(): print(i)\n    \nJPEGS = PATH\/'JPEGImages'","d57653a1":"import json\nBD = json.load((PATH\/'pascal_train2007.json').open()) # it loads a dictionary\nprint('the dictionary of keys: ',BD.keys())","942e0463":"# Then, we can acces to the image\nBD['images'][:5]","0a20d9a2":"# The annotations\nBD['annotations'][:2]","4682a7fb":"# and all category\nBD['categories']","dd823f2f":"import collections\n\ndef hw_bb(bb): return np.array([bb[1], bb[0], bb[3]+bb[1]-1, bb[2]+bb[0]-1])\n\n# we convert categories into dictionary\ndata_category = dict((o['id'],o['name']) for o in BD['categories']) # all the categories\ndata_filename = dict((o['id'],o['file_name']) for o in BD['images']) # image id to image filename\ndata_ids = [o['id'] for o in BD['images']] # list of all the image IDs\n\nannotations = collections.defaultdict(lambda:[])\nfor o in BD['annotations']:\n    if not o['ignore']:\n        bb = o['bbox']\n        bb = hw_bb(bb)\n        annotations[o['image_id']].append((bb,o['category_id']))\n        \nprint('we have',len(BD['annotations']),'annotations')","03af86c6":"def bb_hw(a): return np.array([a[1],a[0],a[3]-a[1]+1,a[2]-a[0]+1])\n\ndef show_img(im, figsize=None, ax=None):\n    \"\"\"show images\"\"\"\n    if not ax: fig,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return ax\n\ndef draw_outline(o, lw):\n  \n    o.set_path_effects([patheffects.Stroke(\n        linewidth=lw, foreground='black'), patheffects.Normal()])\n    \ndef draw_rect(ax, b):\n    \"\"\"Draw rectangle around the object of interest\"\"\"\n    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor='white', lw=2))\n    draw_outline(patch, 4)\n    \ndef draw_text(ax, xy, txt, sz=14):\n    \"\"\"Write the text on the upper right corner of rectangle surrounding the object\"\"\"\n    text = ax.text(*xy, txt,\n        verticalalignment='top', color='white', fontsize=sz, weight='bold')\n    draw_outline(text, 1)    \n    \ndef draw_im(im, ann):\n    ax = show_img(im, figsize=(16,8)) # That's why the image with draw_im is zoomed in\n    for b,c in ann:\n        b = bb_hw(b)\n        draw_rect(ax, b)\n        draw_text(ax, b[:2], data_category[c], sz=16)\n        \ndef draw_idx(i):\n    im_a = annotations[i]\n    im = image.imread(JPEGS\/data_filename[i])\n    draw_im(im, im_a)    ","14468c40":"im = image.imread(JPEGS\/data_filename[data_ids[0]])\n    \nax = show_img(im)\nbbox = annotations[data_ids[0]][0][0]\nclas_id = annotations[data_ids[0]][0][1]\nbbox = bb_hw(bbox)\n\ndraw_rect(ax, bbox)\ndraw_text(ax, bbox[:2], data_category[clas_id])    ","5ba4c37c":"import pandas as pd\n\ndef get_largest_annotation(b):\n    if not b: raise Exception()\n    b = sorted(b, key=lambda x: np.product(x[0][-2:]-x[0][:2]), reverse=True)\n    return b[0]\n\n\nfilename=[]\nC=[]\nfor image_id,annotation in annotations.items():\n    filename.append(data_filename[image_id])\n    \n    C.append(data_category[get_largest_annotation(annotation)[1]])\n    \ndf = pd.DataFrame({'filename': filename, 'class': C}, columns=['filename','class'])\n\ndf[:5]","1654cfa4":"# create a training and validation set\n\ndef Split_Train_Valid(df,Split_train_val=0.7):\n    # step 1: shuffle the data\n    df = df.reindex(np.random.permutation(df.index))\n    df=df.set_index(np.arange(len(df)))\n    \n    # step 2: split in training and testing\n    df_train = df[:int(len(df)*Split_train_val)]\n    df_valid = df[int(len(df)*Split_train_val):]\n    df_train=df_train.set_index(np.arange(len(df_train)))\n    df_valid=df_valid.set_index(np.arange(len(df_valid)))\n    \n    return df_train,df_valid\n\ndf_train, df_valid = Split_Train_Valid(df,0.7)","fa70b349":"# our batch size\nbs=32\n# define the size of our input data\nsz=224\n\n# preprocess_input is for VGG16 in our case\ntrain_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                   rotation_range=20,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   shear_range=0.1,\n                                   zoom_range=0.1,\n                                   horizontal_flip=True) \n\nvalid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) \n\n\ntrain_batches = train_datagen.flow_from_dataframe(df_train, # The df\n                                                  JPEGS, # Place on desk\n                                                  x_col='filename', # The column to get x\n                                                  y_col='class', # The column to get y\n                                                  has_ext=True, \n                                                  target_size=(sz, sz), \n                                                  color_mode='rgb', \n                                                  classes=None, \n                                                  class_mode='categorical', \n                                                  batch_size=bs, \n                                                  shuffle=True)\n\n\n\nvalid_batches = valid_datagen.flow_from_dataframe(df_valid, \n                                                  JPEGS, \n                                                  x_col='filename', \n                                                  y_col='class', \n                                                  has_ext=True, \n                                                  target_size=(sz, sz), \n                                                  color_mode='rgb', \n                                                  classes=list(train_batches.class_indices), \n                                                  class_mode='categorical', \n                                                  batch_size=bs, \n                                                  shuffle=False)\n\nNbClasses = len(train_batches.class_indices)","99eccddb":"net = VGG16(include_top=False, weights='imagenet', input_shape=(sz,sz,3))\nfor layer in net.layers:\n        layer.trainable=False\n        \n# we add our classification layer\nx = net.output\nx = Flatten()(x)\nx = Dropout(0.5)(x)\noutput_layer = Dense(NbClasses, activation='softmax', name='softmax')(x)\nmodel = Model(inputs=net.input, outputs=output_layer)\nmodel.summary()","e5bc6f61":"epochs = 20\n\nopt = Adam(lr=1e-4)\nmodel.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])","f128908d":"history = model.fit_generator(train_batches,\n                              steps_per_epoch = train_batches.n \/\/ train_batches.batch_size,\n                              epochs=epochs,\n                              validation_data=valid_batches,\n                              validation_steps = valid_batches.n \/\/ valid_batches.batch_size)\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","37c6d59b":"#unfreeze initial layers\nFREEZE_LAYERS = 14\n\n# free the first layers\nfor layer in model.layers[:FREEZE_LAYERS]:\n    layer.trainable = False\nfor layer in model.layers[FREEZE_LAYERS:]:\n    layer.trainable = True","975fb1fa":"model.summary()","20f07e51":"epochs = 10\n\nopt = Adam(lr=1e-5)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit_generator(train_batches,\n                              steps_per_epoch = train_batches.n \/\/ train_batches.batch_size,\n                              epochs=epochs,\n                              validation_data=valid_batches,\n                              validation_steps = valid_batches.n \/\/ valid_batches.batch_size)\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","55c42332":"def unpreprocess(x, data_format,mode):\n    \"\"\"unpreprocesses a Numpy array encoding a batch of images.\n    # Arguments\n        x: Input array, 3D or 4D.\n        data_format: Data format of the image array.\n        mode: One of \"caffe\", \"tf\" or \"torch\".\n            - caffe: will convert the images from RGB to BGR,\n                then will zero-center each color channel with\n                respect to the ImageNet dataset,\n                without scaling.\n            - tf: will scale pixels between -1 and 1,\n                sample-wise.\n            - torch: will scale pixels between 0 and 1 and then\n                will normalize each channel with respect to the\n                ImageNet dataset.\n    # Returns\n        unreprocessed Numpy array.\n    \"\"\"\n    if not issubclass(x.dtype.type, np.floating):\n        x = x.astype(backend.floatx(), copy=False)\n\n    im = np.copy(x) \n\n    if mode == 'tf':\n        im += 1.\n        im *= 127.5\n        im = np.clip(im, 0, 255)\n        return im.astype(np.uint8)\n\n    if mode == 'torch':\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n    else:\n        mean = [103.939, 116.779, 123.68]\n        std = None\n\n    # Zero-center by mean pixel\n    if data_format == 'channels_first':\n        if im.ndim == 3:\n            if std is not None:\n                im[0, :, :] *= std[0]\n                im[1, :, :] *= std[1]\n                im[2, :, :] *= std[2]\n                \n            im[0, :, :] += mean[0]\n            im[1, :, :] += mean[1]\n            im[2, :, :] += mean[2]\n\n        else:\n            if std is not None:\n                im[:, 0, :, :] *= std[0]\n                im[:, 1, :, :] *= std[1]\n                im[:, 2, :, :] *= std[2]\n                \n            im[:, 0, :, :] += mean[0]\n            im[:, 1, :, :] += mean[1]\n            im[:, 2, :, :] += mean[2]\n\n    else:\n        if std is not None:\n            im[..., 0] *= std[0]\n            im[..., 1] *= std[1]\n            im[..., 2] *= std[2]        \n        im[..., 0] += mean[0]\n        im[..., 1] += mean[1]\n        im[..., 2] += mean[2]\n\n    if mode == 'torch':\n        im *= 255.\n    else:\n        if data_format == 'channels_first':\n            # 'RGB'->'BGR'\n            if im.ndim == 3:\n                im = im[::-1, ...]\n            else:\n                im = im[:, ::-1, ...]\n        else:\n            # 'RGB'->'BGR'\n            im = im[..., ::-1]\n         \n    im = np.clip(im, 0, 255)\n\n    return im.astype(np.uint8) ","62856cc1":"\n\nclass_name = list(valid_batches.class_indices)\n\nfig, axes = plt.subplots(3, 4, figsize=(12, 8))\nfor i,ax in enumerate(axes.flat):\n    x,y = valid_batches.next()\n    image = x[0]\n    proba = model.predict(np.expand_dims(image, axis=0), batch_size=None, verbose=0, steps=None)\n    class_id = np.argmax(proba)\n    ax = show_img(unpreprocess(image,'none','none'), ax=ax)\n    draw_text(ax, (0,0), class_name[class_id])\n\nplt.tight_layout()","c3d6e983":"# we create this time our data frame with the box coordiantes\n\nfilename=[]\nbbox=[]\nfor image_id,annotation in annotations.items():\n    filename.append(data_filename[image_id])\n    bbox.append(get_largest_annotation(annotation)[0])\n    \ndf = pd.DataFrame({'filename': filename, 'bbox': bbox}, columns=['filename','bbox'])\n\ndf[:5]","67f57468":"df_train, df_valid = Split_Train_Valid(df,0.7)","b343432f":"\nclass DataFrame_Generator(keras.utils.all_utils.Sequence):\n    'Generates data from a Dataframe'\n    def __init__(self, df, folder,preprocess_fct,batch_size=32, dim=(32,32), shuffle=True):\n        'Initialization'\n        self.preprocess_fct = preprocess_fct\n        self.dim = dim\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.folder = folder\n    \n        # Load the dataframe\n        # the database is not so big, especially when resized in 224*224.\n        # so we have either the option to load images online for each batch or\n        # we can load all image at once \n        self.df = df\n        self.n = len(df)            \n        self.nb_iteration = int(np.floor(self.n  \/ self.batch_size))\n        \n        self.indexes = np.arange(len(self.df))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return self.nb_iteration\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Generate data\n        X, y = self.__data_generation(indexes)\n\n        return X, y\n\n    def __data_generation(self, index):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, 3))\n        Y = np.zeros((self.batch_size,4))\n\n        # Generate data\n        for i, ID in enumerate(index):\n            # Read the image\n            img = Image.open(self.folder\/self.df['filename'][ID])\n            bb = self.df['bbox'][ID]\n                 \n            # Resize according to the required size\n            width, height = img.size\n            RatioX = width\/self.dim[0]\n            RatioY = height\/self.dim[1]\n                                                        \n            img = np.asarray(img.resize(self.dim))\n            \n            # Resize the bbox accordingly\n            bb = [bb[0]\/RatioY,bb[1]\/RatioX,bb[2]\/RatioY,bb[3]\/RatioX]\n                                 \n            # Same as done for VGG16\n            X[i,] = self.preprocess_fct(np.asarray(img))\n            \n            Y[i] = bb\n\n        return X, Y \n","dda37ae3":"train_gen = DataFrame_Generator(df_train,JPEGS,preprocess_input,bs,(sz,sz),True)\nvalid_gen = DataFrame_Generator(df_valid,JPEGS,preprocess_input,bs,(sz,sz),False)","5861cc37":"sz=224\nbs=64\n\nnet = VGG16(include_top=False, weights='imagenet', input_shape=(sz,sz,3))\nfor layer in net.layers:\n        layer.trainable=False\n        \nx = net.output\nx = Flatten()(x)\nx = Dropout(0.5)(x)\noutput_layer = Dense(4, activation='linear', name='linear')(x)\nmodel = Model(inputs=net.input, outputs=output_layer) \nmodel.summary()","7a7cc047":"optimizer=Adam(lr=1e-4)\n\nmodel.compile(optimizer=optimizer,loss='mean_absolute_error', metrics=['accuracy'])","050e09fa":"epochs = 15\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_gen.nb_iteration,\n                              epochs = epochs,\n                              validation_data=valid_gen, validation_steps=valid_gen.nb_iteration)\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","1942a258":"FREEZE_LAYERS = 14\n\n# free the first layers\nfor layer in model.layers[:FREEZE_LAYERS]:\n    layer.trainable = False\nfor layer in model.layers[FREEZE_LAYERS:]:\n    layer.trainable = True\n    \nmodel.compile(optimizer=optimizer,loss='mean_absolute_error', metrics=['accuracy'])    \n\nprint(model.summary())\n\nepochs = 10\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_gen.nb_iteration,\n                              epochs = epochs,\n                              validation_data=valid_gen, validation_steps=valid_gen.nb_iteration)\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","b7b0c184":"fig, axes = plt.subplots(3, 4, figsize=(16, 10))\nfor i,ax in enumerate(axes.flat):\n    x,y = next(iter(valid_gen))\n    image = x[i]\n    bb = model.predict(np.expand_dims(image, axis=0), batch_size=None, verbose=0, steps=None)\n\n    ax = show_img(unpreprocess(image,'none','none'), ax=ax)\n    draw_rect(ax, bb_hw(bb[0]))\n\nplt.tight_layout()","b1ecc990":"df = pd.DataFrame({'filename': filename, 'cat':C, 'bbox': bbox}, columns=['filename','cat','bbox'])\ndf_train, df_valid = Split_Train_Valid(df,0.7)","16e7d55b":"class GeneratorSingleObject(keras.utils.all_utils.Sequence):\n    \"\"\"Generates data from a Dataframe\"\"\"\n\n    def __init__(self, df, folder, preprocess_fct, batch_size=32, dim=(32, 32),\n                 shuffle=True):\n        'Initialization'\n        self.preprocess_fct = preprocess_fct\n        self.dim = dim\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.folder = folder\n        self.class_name = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n                           'car', 'cat', 'chair',\n                           'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n                           'person', 'pottedplant', 'sheep',\n                           'sofa', 'train', 'tvmonitor']\n        self.NbClasses = len(self.class_name)\n        self.class_dict = dict(\n            (self.class_name[o], o) for o in range(self.NbClasses))\n\n        self.df = df\n        self.n = len(df)\n        self.nb_iteration = int(np.floor(self.n \/ self.batch_size))\n\n        self.on_epoch_end()\n\n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch\"\"\"\n        return self.nb_iteration\n\n    def __getitem__(self, index):\n        \"\"\"Generate one batch of data\"\"\"\n        # Generate indexes of the batch\n        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n\n        # Generate data\n        X, y = self.__data_generation(indexes)\n\n        return X, y\n\n    def on_epoch_end(self):\n        \"\"\"Updates indexes after each epoch\"\"\"\n        self.indexes = np.arange(len(self.df))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, index):\n        \"\"\"Generates data containing batch_size samples\"\"\"\n        # Initialization\n        # X: (n_samples, *dim, n_channels)\n        X = np.empty((self.batch_size, *self.dim, 3))\n        Y_bb = np.zeros((self.batch_size, 4))\n        Y_clas = np.zeros((self.batch_size, 1))\n\n        # Generate data\n        for i, ID in enumerate(index):\n            # Read the image\n            img = Image.open(self.folder \/ self.df['filename'][ID])\n            bb = self.df['bbox'][ID]\n            #bb = np.fromstring(bb, dtype=np.int, sep=' ')\n\n            width, height = img.size\n            RatioX = width \/ self.dim[0]\n            RatioY = height \/ self.dim[1]\n\n            img = np.asarray(img.resize(self.dim))\n            bb = [bb[0] \/ RatioY, bb[1] \/ RatioX, bb[2] \/ RatioY, bb[3] \/ RatioX]\n\n            X[i] = self.preprocess_fct(np.asarray(img))\n            Y_bb[i] = bb\n            Y_clas[i] = self.class_dict[self.df['cat'][ID]]\n\n        Y_clas = keras.utils.all_utils.to_categorical(Y_clas, self.NbClasses)\n\n        return X, [Y_bb,Y_clas]\n\n\n","f1342134":"train_gen = GeneratorSingleObject(df_train,JPEGS,preprocess_input,bs,(sz,sz),True)\nvalid_gen = GeneratorSingleObject(df_valid,JPEGS,preprocess_input,bs,(sz,sz),False)","e0e55a86":"fig, axes = plt.subplots(3, 4, figsize=(12, 12))\n\nfor i,ax in enumerate(axes.flat):\n    x_batch,y_batch = next(iter(valid_gen))\n    bb = y_batch[0][i]\n    cat = y_batch[1][i]    \n    image = x_batch[i]\n\n    c = np.argmax(cat)\n    ax = show_img(unpreprocess(image,'none','none'), ax=ax)\n    draw_rect(ax, bb_hw(bb))\n    draw_text(ax, [bb[1],bb[0]], train_gen.class_name[c], sz=16)\n\nplt.tight_layout()","f2e0d168":"sz=224\nbs=64\n\nnet = VGG16(include_top=False, weights='imagenet', input_shape=(sz,sz,3))\nfor layer in net.layers:\n        layer.trainable=False\n \ny = net.output\ny = Flatten()(y)\ny = Dropout(0.5)(y)\n\n# branch for the regression --> BBox\noutput_layer_bbox = Dense(4, activation='linear', name='layer_bbox')(y)\n\n# Branch for the classification --> Category\noutput_layer_class = Dense(train_gen.NbClasses, activation='softmax', name='layer_class')(y)\n\nmodel = Model(inputs=net.input, outputs=[output_layer_bbox,output_layer_class])","71394482":"optimizer=Adam(lr=1e-4)\nmodel.compile(optimizer=optimizer,loss=['mean_absolute_error','categorical_crossentropy'], metrics=['accuracy'],loss_weights=[1., 5.])\nmodel.summary()","59abe0a1":"epochs = 20\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_gen.nb_iteration,\n                              epochs = epochs,\n                              validation_data=valid_gen, validation_steps=valid_gen.nb_iteration)\n\nprint(history.history.keys())","490e3170":"# summarize history for accuracy for Class\nplt.plot(history.history['layer_class_accuracy'])\nplt.plot(history.history['val_layer_class_accuracy'])\nplt.title('model accuracy - Class prediction')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training','validation'], loc='upper left')\nplt.show()\n\n# summarize history for accuracy for bbox regression\nplt.plot(history.history['layer_bbox_accuracy'])\nplt.plot(history.history['val_layer_bbox_accuracy'])\nplt.title('model accuracy - BBOX regression')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training','validation'], loc='upper left')\nplt.show()\n\n# summarize history for loss for Class\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_layer_class_loss'])\nplt.plot(history.history['layer_class_loss'])\nplt.title('model loss - Class cross-entropy')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Global_Training', 'Global_Validation','validation','training'], loc='upper left')\nplt.show()\n\n# summarize history for loss for BBOX\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_layer_bbox_loss'])\nplt.plot(history.history['layer_bbox_loss'])\nplt.title('model loss - BBOX L1')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Global_Training', 'Global_Validation','validation','training'], loc='upper left')\nplt.show()","06b82250":"FREEZE_LAYERS = 14\n\n# free the first layers\nfor layer in model.layers[:FREEZE_LAYERS]:\n    layer.trainable = False\nfor layer in model.layers[FREEZE_LAYERS:]:\n    layer.trainable = True\n    \noptimizer=Adam(lr=1e-5)\nmodel.compile(optimizer=optimizer,loss=['mean_absolute_error','categorical_crossentropy'], \n              metrics=['accuracy'],loss_weights=[1., 20.])\n \n\nepochs = 20\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_gen.nb_iteration,\n                              epochs = epochs,\n                              validation_data=valid_gen, validation_steps=valid_gen.nb_iteration)","3a130334":"# summarize history for accuracy for Class\nplt.plot(history.history['layer_class_accuracy'])\nplt.plot(history.history['val_layer_class_accuracy'])\nplt.title('model accuracy - Class prediction')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training','validation'], loc='upper left')\nplt.show()\n\n# summarize history for accuracy for bbox regression\nplt.plot(history.history['layer_bbox_accuracy'])\nplt.plot(history.history['val_layer_bbox_accuracy'])\nplt.title('model accuracy - BBOX regression')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training','validation'], loc='upper left')\nplt.show()\n\n# summarize history for loss for Class\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_layer_class_loss'])\nplt.plot(history.history['layer_class_loss'])\nplt.title('model loss - Class cross-entropy')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Global_Training', 'Global_Validation','validation','training'], loc='upper left')\nplt.show()\n\n# summarize history for loss for BBOX\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['val_layer_bbox_loss'])\nplt.plot(history.history['layer_bbox_loss'])\nplt.title('model loss - BBOX L1')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Global_Training', 'Global_Validation','validation','training'], loc='upper left')\nplt.show()","6dd8d46a":"fig, axes = plt.subplots(3, 4, figsize=(12, 12))\n\nfor i,ax in enumerate(axes.flat):\n    x_batch,y_batch = next(iter(iter(valid_gen)))\n    image = x_batch[i]\n    model.predict(np.expand_dims(image, axis=0), batch_size=None, verbose=0, steps=None)\n\n    bb = model.predict(np.expand_dims(image, axis=0), batch_size=None, verbose=0, steps=None)[0][0]\n    cat =model.predict(np.expand_dims(image, axis=0), batch_size=None, verbose=0, steps=None)[1][0]   \n    \n\n    c = np.argmax(cat)\n    ax = show_img(unpreprocess(image,'none','none'), ax=ax)\n    draw_rect(ax, bb_hw(bb))\n    draw_text(ax, [bb[1],bb[0]], train_gen.class_name[c], sz=16)\n\nplt.tight_layout()","7a5bbdbf":"### create a training and validation set","156fa027":"###upgrade and import some packages","4f74bee8":"### Use the preivious functions","d72384d6":"### 3.2: The model\n\nWe need one output activation for each class (for its probability) plus one for each bounding box coordinate. We'll use an extra linear layer this time, plus some dropout, to help us train a more flexible model.\n\nNotice, the importance of Keras functional API. With sequential API, this multi-output model is not possible.","77ea92aa":"### 2.4: It's time to train","cc729f44":"### show some images","d91812e7":"### Check the result\nunprocess function is just to inverse what preprocess_fct or preprocess_input (of VGG for for example) dir. Like normalization (multiply by std and add back the mean), or clipping (*255 again)","52db8e61":"### It is time to train now","967b9272":"# We cannot use data augmentation anymore!\n\nUnfortunately, we can't really use as before the <kbd>flow_from_dataframe<\/kbd> function. There are several reasons, but the most important one is that we need to resize the image to fit to our input tensor and as a consequence, bounding boxes should be resized too!, To solve this problem we will  Make our own data generator!","4f4c3cb6":"### unfreeze couple of layers for better learning and enhance the accuracy\n","8f693160":"## Step 3: Single object detection\n\nWe have now to combine the two individual tasks of classification and detection we have developped bedore. We know that independently, it works. Therefore, if we modify accordingly our data, our model, our Loss function... it should work too!\n\nlet's go...","57fbbcc8":"so, you have:\n- <kbd>JPEGImages<\/kbd> folder containing all the images\n- <kbd>Annotations<\/kbd> folder containing all the annotation, one XML annotation file per image file\n\nThe original version were in XML, which is a little hard to work with nowadays, so we use the more recent JSON version","0694f6c5":"### 2.5: unfreeze couple of layers","2eba029a":"The main topic of these Notebook is single object detection, which means getting a model to draw a box around every key object in an image, and label each one correctly.  we can use transfer learning from an <b>ImageNet<\/b> classifier that was never even trained to do detection.We are going to follow these steps:\n- Classify the biggest object\n- Predict Bounding box of biggest object\n- Put all together to predict and classify the biggest object\n\nwe\u2019ll use a single model to do both these at the same time.","d58ded5d":"## Data augmentation\nThe easiest way to do Data Augmentation is to create an <kbd>ImageDataGenerator<\/kbd> and specify the transformation properties.  the data needs to be preprocessed to be compatible with the model used (VGG16). ","dac6d75a":"### 3.3: The optimizer\n\nHere, we need to combine the two Loss function for each branch.\nBecause each Loss will have different range, it is then better to set a weight on each of them.\nWe can check from the previous individual training to put the weight accordingly so that each branch has the same influence","fb0c4cff":"## some convenient display functions","b0266105":"### 2.3: The optimizer\n\nOur optimizer will optimize the task our model is designed for. Compared to before, we used a Cross-entropy as our Loss function to predict Category. Now, as we want to predict continuous values, we are going to use L1 (mean_absolute_error) or L2 Loss (mean_squared_error)","3d84d700":"### Later we will enhance this model and use advanced techniques ex. yolo v4  to infer real time images","7bf20d0f":"## Pascal VOC\nWe will be looking at the [Pascal VOC](http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/) dataset. It's quite slow, so you may prefer to download from [this mirror](https:\/\/pjreddie.com\/projects\/pascal-voc-dataset-mirror\/). There are two different competition\/research datasets, from 2007 and 2012. We'll be using the 2007 version. later we can use the larger 2012 for better results.\n\n","95f6e73d":"### Split our data into training and validation set","56c58f5e":"## Step 2: Bbox regression\n### 2.1: The Data\nNow we'll try to find the bounding box of the largest object. This is simply a regression with 4 outputs.","9652f50a":"### 3.1: The Data\n\nThe data is still the image plus a tuple containing the bounding box coordinates and the category","d146abb5":"### Define an optimizer and a Loss function","43b3ef5a":"### Train again","dc08cc69":"### 2.2: The model\n\nHere, we want to predict the position of the bounding box.\nActually, we expect our model to predict 4 values: this is a regression problem.\n\nAs the difference with Classification, we do not expect a probability but real position values! Our network will be the same as before (especially because we want to share latter the feature encoder part) but instead of adding a classifcation layer with a SoftMax or Sigmoid function (to provide a probability) we only need to add a linear layer","0cfb821d":"###  Create our model\n\nit will be a VGG16 model.","c5dbcd83":"### 3.4: Time to train","ad1dbea8":"### 3.5: unfreeze few layers","aac89eba":"## Step 1: Largest item classifier\n\nWe are going to create a classifier to predict the class of the biggest object in the image. \nFor that purpose, we need to structure our learning data in a scalable manner as we will, step by step, increase the complexity of our task.\n\n### Setup the data folder\n___copy vs. point___\n\n__Flow from directory means to copy__\n\nSo, there are plenty of way to do that, but in Fast.ai, they use a csv file to list all images\/labels instead of creating a new directory and copying inside all the needed data. For sure, it sounds better than recreating a folder and copying all the need images and labels... not really convenient, neither scalable. And finally, they have this nice function to train directly from a CSV file. \n\n__Flow from dataframe__\nIn August 2018, Keras came with a super usefull function called <kbd>flow_from_dataframe<\/kbd>!!! Great, no more code to do, it is a method from <kbd>ImageDataGenerator<\/kbd>, and uses same properties, that is to say we can even do Data AUgmentation!\n\nSo, before doing anything with the data, that is to say jump to the conclusion, we have first to figure out how we will provide the data to our network. As we said before, Keras can use Dataframe to represent the data.\n\nWhat do we put them in our dataframe? We want for each image to have a classification tag. Sounds pretty easy, right?\n\nSo,it's supposed to look like this\n\n<img src=\"https:\/\/raw.githubusercontent.com\/valeoai\/tutorial-images\/master\/images\/06_Single_Object_Detection\/dataframe.png\" width=\"30%\">\n\nLet's do it now!"}}