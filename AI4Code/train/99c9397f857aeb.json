{"cell_type":{"90724755":"code","3b03d1f1":"code","35b6ca82":"code","8efada79":"code","8fbbee1b":"code","82f187f9":"code","d3671dda":"code","15df7183":"code","b965a368":"code","d084ea38":"code","e637b08a":"code","e0b72f75":"code","217082ee":"code","51f3e966":"code","25c91887":"code","81627aa1":"code","d2fa821c":"code","b6325147":"code","1dabd42f":"code","858f918e":"code","4f057466":"code","dcdd90d3":"code","1fe9c271":"code","1317be8f":"code","9f110598":"code","1e70a61e":"code","ee2cccef":"code","60736ed1":"code","3af92b94":"code","4caf1f51":"code","0d16c99d":"code","61fd893d":"code","76d5a521":"code","d6fcc9e4":"code","271f97ce":"code","38469fd7":"code","54625ac2":"code","198b4acc":"code","6d45bd62":"code","669d9264":"code","0c431cf6":"code","6a37d039":"code","55474099":"code","6c598913":"code","1a3714c8":"markdown","231e05f7":"markdown","5f318d65":"markdown","d98e0ae4":"markdown","385b84df":"markdown","47518a26":"markdown","9d485465":"markdown","ba62c8f5":"markdown","f8b9f87b":"markdown","81edd900":"markdown","4b96dd9c":"markdown","ecd984a2":"markdown","39ee0ec3":"markdown","cfa4d6e9":"markdown","fb333622":"markdown","a88aad9d":"markdown"},"source":{"90724755":"import pandas as pd\ncal = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsteval = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\nprice = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv') ","3b03d1f1":"import numpy as np\nimport plotly\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nid_list = sorted(list(set(steval['id'])))\nd_cols = [col for col in steval.columns if 'd_' in col]\nx_1 = steval.loc[steval['id'] == id_list[0]].set_index('id')[d_cols].values[0][:200]\nx_2 = steval.loc[steval['id'] == id_list[12]].set_index('id')[d_cols].values[0][300:500]\nx_3 = steval.loc[steval['id'] == id_list[36]].set_index('id')[d_cols].values[0][600:800]\nx_4 = steval.loc[steval['id'] == id_list[64]].set_index('id')[d_cols].values[0][1000:1200]\nx_5 = steval.loc[steval['id'] == id_list[128]].set_index('id')[d_cols].values[0][1300:1500]\nx_6 = steval.loc[steval['id'] == id_list[256]].set_index('id')[d_cols].values[0][1600:1800]\nfig = make_subplots(rows=6, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=True, \n                         mode='lines', name=\"1st sample\",\n                         marker=dict(color=\"cadetblue\")), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=True,\n                         mode='lines', name=\"2nd sample\",\n                         marker=dict(color=\"firebrick\")), row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=True,\n                         mode='lines', name=\"3rd sample\",\n                         marker=dict(color=\"yellowgreen\")), row=3, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_4)), y=x_4, showlegend=True,\n                         mode='lines', name=\"4th sample\",\n                         marker=dict(color=\"orangered\")), row=4, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_5)), y=x_5, showlegend=True,\n                         mode='lines', name=\"5th sample\",\n                         marker=dict(color=\"mediumpurple\")), row=5, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_6)), y=x_6, showlegend=True,\n                         mode='lines', name=\"6th sample\", \n                         marker=dict(color=\"navy\")), row=6, col=1)\n\nfig.update_layout(height=1200, width=900, title_text=\"Observing some randomly chosen sales trend :\")\nfig.show()","35b6ca82":"steval.head()","8efada79":"#For the remaining 28 days d1942 to d1969, filling with zero\n\nimport numpy as np\nfor i in range(1942,1970):\n    col = 'd_' + str(i)\n    steval[col] = 0\n    steval[col] = steval[col].astype(np.int16)","8fbbee1b":"#Check size\nsteval.info()","82f187f9":"#Check size\ncal.info()","d3671dda":"#Check size\nprice.info()","15df7183":"# Taken from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\nimport numpy as np\ndef reduce_mem_usage(df):\n   \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","b965a368":"reduce_mem_usage(steval)","d084ea38":"reduce_mem_usage(price)","e637b08a":"reduce_mem_usage(cal)","e0b72f75":"sales = pd.melt(steval, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()","217082ee":"sales.head()","51f3e966":"sales = pd.merge(sales, cal, on='d', how='left')\nsales = pd.merge(sales, price, on=['store_id','item_id','wm_yr_wk'], how='left') ","25c91887":"sales.info()","81627aa1":"#Encode categorical variables. Store the categories along with their codes\nd_id = dict(zip(sales.id.cat.codes, sales.id))\nd_item_id = dict(zip(sales.item_id.cat.codes, sales.item_id))\nd_dept_id = dict(zip(sales.dept_id.cat.codes, sales.dept_id))\nd_cat_id = dict(zip(sales.cat_id.cat.codes, sales.cat_id))\nd_store_id = dict(zip(sales.store_id.cat.codes, sales.store_id))\nd_state_id = dict(zip(sales.state_id.cat.codes, sales.state_id))","d2fa821c":"#Removing \"d_\" prefix from the values of column \"d\"\nsales.d = sales['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\ncols = sales.dtypes.index.tolist()\ntypes = sales.dtypes.values.tolist()\nfor i,type in enumerate(types):\n    if type.name == 'category':\n        sales[cols[i]] = sales[cols[i]].cat.codes","b6325147":"#Dropping date column        \nsales.drop('date',axis=1,inplace=True)","1dabd42f":"lags = [1,2,4,8,16,32]\nfor lag in lags:\n    sales['sold_lag_'+str(lag)] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)","858f918e":"#Combination of two vars with \"sold\" and their mean\nsales['item_sold_avg'] = sales.groupby('item_id')['sold'].transform('mean').astype(np.float16)\nsales['state_sold_avg'] = sales.groupby('state_id')['sold'].transform('mean').astype(np.float16)\nsales['store_sold_avg'] = sales.groupby('store_id')['sold'].transform('mean').astype(np.float16)\nsales['cat_sold_avg'] = sales.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\nsales['dept_sold_avg'] = sales.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\n\n#Combination of three vars with \"sold\" and their mean\nsales['cat_dept_sold_avg'] = sales.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\nsales['store_item_sold_avg'] = sales.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)\nsales['cat_item_sold_avg'] = sales.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\nsales['dept_item_sold_avg'] = sales.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)\nsales['dept_store_sold_avg'] = sales.groupby(['dept_id','store_id'])['sold'].transform('mean').astype(np.float16)\n\n#Combination of four vars with \"sold\" and their mean\nsales['store_cat_dept_sold_avg'] = sales.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\nsales['store_cat_item_sold_avg'] = sales.groupby(['store_id','cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\n\n#Some more combinations can be incorporated here, but to avoid memory allocation warning, the above combinations would suffice ","4f057466":"id_list = sorted(list(set(sales['id'])))\nsold_avg_cols = [col for col in sales.columns if '_sold_avg' in col]\nx_1 = sales.loc[sales['id'] == id_list[0]].set_index('id')[sold_avg_cols].values[0][:]\nx_2 = sales.loc[sales['id'] == id_list[12]].set_index('id')[sold_avg_cols].values[0][:]\nx_3 = sales.loc[sales['id'] == id_list[36]].set_index('id')[sold_avg_cols].values[0][:]\nx_4 = sales.loc[sales['id'] == id_list[64]].set_index('id')[sold_avg_cols].values[0][:]\nx_5 = sales.loc[sales['id'] == id_list[128]].set_index('id')[sold_avg_cols].values[0][:]\nx_6 = sales.loc[sales['id'] == id_list[256]].set_index('id')[sold_avg_cols].values[0][:]\nfig = make_subplots(rows=6, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=True, \n                         mode='lines+markers', name=\"First sample\",\n                         marker=dict(color=\"cadetblue\")), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=True,\n                         mode='lines+markers', name=\"Second sample\",\n                         marker=dict(color=\"firebrick\")), row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=True,\n                         mode='lines+markers', name=\"Third sample\",\n                         marker=dict(color=\"yellowgreen\")), row=3, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_4)), y=x_4, showlegend=True,\n                         mode='lines+markers', name=\"Fourth sample\",\n                         marker=dict(color=\"orangered\")), row=4, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_5)), y=x_5, showlegend=True,\n                         mode='lines+markers', name=\"Fifth sample\",\n                         marker=dict(color=\"mediumpurple\")), row=5, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_6)), y=x_6, showlegend=True,\n                         mode='lines+markers', name=\"Sixth sample\", \n                         marker=dict(color=\"navy\")), row=6, col=1)\n\nfig.update_layout(height=1500, width=900, title_text=\"Observing some randomly chosen '_sold_avg' trends after applying lag:\")\nfig.show()","dcdd90d3":"sales['rolling_sold_mean'] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=6).mean()).astype(np.float16)","1fe9c271":"sales['expanding_sold_mean'] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)","1317be8f":"#Clear some space\nimport gc\ngc.collect()","9f110598":"# Moving Average Trends\nsales['daily_avg_sold'] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sold'].transform('mean').astype(np.float16)\nsales['avg_sold'] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)\nsales['selling_trend'] = (sales['daily_avg_sold'] - sales['avg_sold']).astype(np.float16)\nsales.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)","1e70a61e":"# Since we introduced lags till 32 days, data for first 31 days should be removed.\nsales = sales[sales['d']>=32]","ee2cccef":"import gc\ngc.collect()","60736ed1":"# Save data for training\nsales.to_pickle('salesdata.pkl') #to_pickle: serializes an object to file\ndel sales","3af92b94":"gc.collect()","4caf1f51":"data = pd.read_pickle('salesdata.pkl')\nvalidation = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\ntest = data[data['d']>=1942][['id','d','sold']]\neval_prediction = test['sold']\nvalidation_prediction = validation['sold']","0d16c99d":"gc.collect()","61fd893d":"#Get the store ids\nstores = steval.store_id.cat.codes.unique().tolist()\nfor store in stores:\n    df = data[data['store_id']==store]","76d5a521":"gc.collect()","d6fcc9e4":"#Split the data\nX_train, y_train = df[df['d']<1914].drop('sold',axis=1), df[df['d']<1914]['sold']\nX_valid, y_valid = df[(df['d']>=1914) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sold']\nX_test = df[df['d']>=1942].drop('sold',axis=1)","271f97ce":"gc.collect()","38469fd7":"%%time\n\nfrom lightgbm import LGBMRegressor as lgb\nfrom hyperopt import hp, tpe, fmin\nfrom sklearn.model_selection import cross_val_score\n\nvalgrid = {'n_estimators':hp.quniform('n_estimators', 900, 1500, 100),\n           'learning_rate':hp.quniform('learning_rate', 0.01, 0.4, 0.01),\n           'max_depth':hp.quniform('max_depth', 3,10,1),\n           'num_leaves':hp.quniform('num_leaves', 25,100,25),\n           'subsample':hp.quniform('subsample', 0.5, 0.9, 0.1),\n           'colsample_bytree':hp.quniform('colsample_bytree', 0.5, 0.9, 0.1),\n           'min_child_weight':hp.quniform('min_child_weight', 100, 500, 100) \n          }\n\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']),\n              'learning_rate': params['learning_rate'],\n              'max_depth': int(params['max_depth']),\n              'num_leaves': int(params['num_leaves']),\n              'subsample': params['subsample'],\n              'colsample_bytree': params['colsample_bytree'],\n              'min_child_weight': params['min_child_weight']}\n    \n    lgb_a = lgb(**params)\n    score = cross_val_score(lgb_a, X_train, y_train, cv=2, n_jobs=-1).mean()\n    return score\n\nbestP = fmin(fn= objective, space= valgrid, max_evals=20, rstate=np.random.RandomState(123), algo=tpe.suggest)","54625ac2":"gc.collect()","198b4acc":"print(bestP)\nprint('N estimators:', int(bestP['n_estimators']))\nprint('Learning rate:', bestP['learning_rate'])\nprint('Subsample:', bestP['subsample'])\nprint('Colsample bytree:', bestP['colsample_bytree'])\nprint('Max depth:', int(bestP['max_depth']))\nprint('Num leaves:', int(bestP['num_leaves']))\nprint('Min child weight:', int(bestP['min_child_weight']))","6d45bd62":"%%time\n\nimport lightgbm\n\nmodel = lightgbm.LGBMRegressor(\n        n_estimators = int(bestP['n_estimators']),\n        learning_rate = bestP['learning_rate'],\n        subsample = bestP['subsample'],\n        colsample_bytree = bestP['colsample_bytree'],\n        max_depth = int(bestP['max_depth']),\n        num_leaves = int(bestP['num_leaves']),\n        min_child_weight = int(bestP['min_child_weight']))\n\nprint('Prediction for Store: {}**'.format(d_store_id[store]))\nmodel.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)], eval_metric='rmse', verbose=20, early_stopping_rounds=20)\nvalidation_prediction[X_valid.index] = model.predict(X_valid)\neval_prediction[X_test.index] = model.predict(X_test)\nfilename = 'model'+str(d_store_id[store])+'.pkl'","669d9264":"import joblib\n\njoblib.dump(model, filename)\ndel model, X_train, y_train, X_valid, y_valid\ngc.collect()","0c431cf6":"%%time\n\n# Validation results \nvalidation = steval[['id']+['d_' + str(i) for i in range(1914,1942)]]\nvalidation['id']=pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv').id\nvalidation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n\n# Evaluation results\ntest['sold'] = eval_prediction\nevaluation = test[['id','d','sold']]\nevaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()\nevaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n\n# Mapping category ids to their categories\nevaluation.id = evaluation.id.map(d_id)","6a37d039":"gc.collect()","55474099":"# Check submission file\nsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)\nsubmit.head()","6c598913":"# Submission\nprint(\"Generating CSV file\")\nsubmit.to_csv('submission.csv',index=False)\nprint(\"Submission Successful\")","1a3714c8":"Since the set of full training labels are released now, we can ignore \"sales_train_validation\" data set and proceed working with \"sales_train_evaluation\" instead. We will check some randomly selected id's trend now, just to get a view of how it is varying with time.","231e05f7":"Now, we will introduce lags into this data set. I have introduced lags at a sequence {2^0,2^1,2^2,2^3,2^4,2^5}. You can introduce lags at some other sequence like {1,3,6,12,24,36} etc.","5f318d65":"Now, we will check the sizes of three datasets and reduce them to avoid getting memory allocation alert.","d98e0ae4":"Checking all the columns in our final working data set :","385b84df":"For day indicators (variables with prefix \"d_\") in \"steval\" data set, the values can be more conveniently handled if they are reshaped to put into rows instead of columns. Because in the next step, we will actually merge the \"steval\" dataset with two other data sets (calendar and prices); and notably, in \"calendar\" data set, the column \"d\" has \"d_\" prefixed values arranged in vertical format (unlike \"steval\" where they are arranged horizontally). So, we will apply \"melt\" function on \"steval\" which will unpivot the dataframe from wide format (horizontal) to long format (vertical). This will help in merging the data set with calendar data set. ","47518a26":"We will encode the categorical variables to numericals. And for convenience, we will store their categories (a list) along with their codes (another list) in separate dictionaries, so that we can use them again during submission.","9d485465":"Now, we will use **\"rolling window\"** and **\"expanding window\"** concepts where the sold-mean of a particular window size will be stored in two different variables. It is important to decide the size of rolling window (for getting Moving Average). Longer the rolling window size, smoother the rolling window mean estimates become. For deciding expanding window size, suppose, the number of increments between successive rolling windows is 1 period; then it partitions the entire data set into N = T \u2013 m + 1 subsamples (ref: the picture below)\n![image.png](attachment:image.png)\n\n(pic taken from mathworks.com). Here, we have taken window size = 6 and expanding window size = 2 respectively.","ba62c8f5":"We will use LightGBM regressor model to train data. For getting the best set of hyperparameters for the LightGBM regressor, we will use **hyperopt** for tuning.","f8b9f87b":"We will now save the model.","81edd900":"We will now apply LightGBM regressor with the best parameters to get the prediction results.","4b96dd9c":"Next, we will merge calendar and price datasets with sales data to make it a single working data set.","ecd984a2":"We see that evaluation data set has columns from \"d_1\" till \"d_1941\". Since we need to get the result for next 28 days i.e. from \"d_1942\" till \"d_1969\", we can fill them up with zero values, initially.","39ee0ec3":"Now, the sales dataframe looks like below, compact, easy to merge with Calendar data set matched on column \"d\".","cfa4d6e9":"By observation, we can spot intermittency in all the samples, especially in second and fourth samples, intermittency can be spotted very prominently (Y-axis value is dropping to zero quite frequently).","fb333622":"We will now reduce the sizes of three datasets.","a88aad9d":"Now, we can spot some similarity. First, second, fourth, fifth samples show somewhat similar trend now. Third and sixth ones show similar trends as well."}}