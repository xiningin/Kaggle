{"cell_type":{"7b358023":"code","6a5ed646":"code","e3705eb3":"code","529d236d":"code","d5f90754":"code","b71e9b56":"code","44182295":"code","1c70b321":"code","b4dc0697":"code","dbe57ab2":"code","cf1d9c62":"code","6215d5fc":"code","6f983dc2":"code","881990ab":"code","4b94c439":"code","f1847461":"code","7c607728":"code","185f89be":"code","e4366a2e":"code","fbbce71f":"code","04cb98c6":"code","15dab396":"code","c80d3cb8":"code","2af41c08":"code","29966e02":"code","e2cd6509":"code","d61a767b":"code","748569e7":"code","238a8564":"code","29cc6ef1":"code","aee73225":"code","47ab81fc":"code","7dfc02f3":"code","2c5cfc80":"markdown","05eabc4d":"markdown","1b9452c7":"markdown"},"source":{"7b358023":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a5ed646":"df = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')","e3705eb3":"df.head()","529d236d":"df = df.assign(label = 0)\ndf.loc[df['sentiment'] == 'positive', 'label'] = 1\ndf.head()","d5f90754":"import re\ndf['review_clean'] = df['review'].map(lambda x: re.sub('<[^<]+?>', '', x))","b71e9b56":"df.head()","44182295":"df['review_clean_words_count'] = df['review_clean'].map(lambda x: len(x.split(' ')))\ndf.head()","1c70b321":"df.describe()","b4dc0697":"plt.hist(df['review_clean_words_count'], bins=5, range=(0, 1200))","dbe57ab2":"len(df['sentiment'])","cf1d9c62":"texts = df['review_clean'].tolist()\nlabels = df['label'].tolist()","6215d5fc":"import keras\nfrom keras.preprocessing import text, sequence\n\nmaxlen = 500\nmax_words = 10000\n\ntokenizer = text.Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)","6f983dc2":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = sequence.pad_sequences(sequences, maxlen=maxlen)","881990ab":"labels = np.asarray(labels)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)","4b94c439":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\ntraining_samples = 24000\nvalidation_samples = 6000\ntest_samples = 20000\n\nX_train = data[:training_samples]\ny_train = labels[:training_samples]\n\nX_val = data[training_samples: training_samples + validation_samples]\ny_val = labels[training_samples: training_samples + validation_samples]\n\nX_test = data[training_samples + validation_samples: training_samples + validation_samples + test_samples]\ny_test = labels[training_samples + validation_samples: training_samples + validation_samples + test_samples]","f1847461":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_val.shape)\nprint(y_val.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n","7c607728":"def plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs = range(1, len(acc) + 1)\n    \n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    \n    plt.figure()\n    \n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()","185f89be":"# Set the embedding layer\nfrom keras import models, layers\n\nmodel = models.Sequential()\nmodel.add(layers.Embedding(max_words, 128, input_length=maxlen))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\nmodel.summary()","e4366a2e":"history = model.fit(X_train, y_train,\n                   epochs=10,\n                   batch_size=32,\n                   validation_data=(X_val, y_val))","fbbce71f":"plot_history(history)","04cb98c6":"model.evaluate(X_test, y_test)","15dab396":"simple_rnn = models.Sequential()\nsimple_rnn.add(layers.Embedding(max_words, 32, input_length=maxlen))\nsimple_rnn.add(layers.SimpleRNN(32))\nsimple_rnn.add(layers.Dense(1, activation='sigmoid'))\n\nsimple_rnn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\nsimple_rnn.summary()","c80d3cb8":"simple_rnn_history = simple_rnn.fit(X_train, y_train,\n                                   epochs=10,\n                                   batch_size=128,\n                                   validation_data=(X_val, y_val))","2af41c08":"plot_history(simple_rnn_history)","29966e02":"simple_rnn.evaluate(X_test, y_test)","e2cd6509":"lstm = models.Sequential()\nlstm.add(layers.Embedding(max_words, 32, input_length=maxlen))\nlstm.add(layers.LSTM(32))\nlstm.add(layers.Dense(1, activation='sigmoid'))\n\nlstm.compile(optimizer='rmsprop',\n            loss='binary_crossentropy',\n            metrics=['acc'])\n\nlstm.summary()","d61a767b":"lstm_history = lstm.fit(X_train, y_train,\n                                      epochs=10,\n                                      batch_size=128,\n                                      validation_data=(X_val, y_val))","748569e7":"plot_history(lstm_history)","238a8564":"lstm.evaluate(X_test, y_test)","29cc6ef1":"lstm = models.Sequential()\nlstm.add(layers.Embedding(max_words, 32, input_length=maxlen))\nlstm.add(layers.LSTM(32, dropout=0.2, recurrent_dropout=0.5, return_sequences=True))\nlstm.add(layers.LSTM(32, dropout=0.2, recurrent_dropout=0.5))\nlstm.add(layers.Dense(1, activation='sigmoid'))\n\nlstm.compile(optimizer='rmsprop',\n            loss='binary_crossentropy',\n            metrics=['acc'])\n\nlstm.summary()","aee73225":"lstm_history = lstm.fit(X_train, y_train,\n                        epochs=30,\n                        batch_size=128,\n                        validation_data=(X_val, y_val))","47ab81fc":"plot_history(lstm_history)","7dfc02f3":"lstm.evaluate(X_test, y_test)","2c5cfc80":"Let's try a Simple RNN now","05eabc4d":"What about with Dropout","1b9452c7":"LSTM"}}