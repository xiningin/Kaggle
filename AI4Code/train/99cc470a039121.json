{"cell_type":{"6a165c64":"code","245b8c02":"code","516d6bf6":"code","5c11d13c":"code","6c3aa73c":"code","75e10538":"code","6b1acf99":"code","c778dce1":"code","cb592da9":"code","730409dd":"code","bb53650d":"code","adf1dab7":"code","159d7f4c":"code","10b65d9a":"code","73404c1a":"code","c566055b":"code","6863ad88":"code","2237bc42":"code","6eeef34d":"markdown"},"source":{"6a165c64":"import numpy as np \nimport pandas as pd\n\n# \ub370\uc774\ud130 \uc900\ube44 \ud558\ub294 \ud30c\ud2b8\n\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntrain_data.head()\ntest_val = pd.read_csv('..\/input\/test.csv')\n\n#\ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc624\uba74 \ubc18\ub4dc\uc2dc \uc77c\uc815 \ubd80\ubd84\uc740 \ub208\uc73c\ub85c \ud655\uc778\ud558\ub294 \uc2b5\uad00\uc744 \ub4e4\uc774\uc790\ntest_val.head()\n\n","245b8c02":"#Feature Engineering : \ubaa8\ub378\uc774 \ud559\uc2b5\ud560 \uc218 \uc788\ub3c4\ub85d \ub370\uc774\ud130\ub97c \uc900\ube44\ud558\uace0 feature\ub97c \uc120\ubcc4\ud558\ub294 \ub2e8\uacc4\n#category value-> numeric part(\uc9c1\uc811\ud574\ubcf4\uae30)\n\nprint(train_data.iloc[1,11])\nfor i in range(891):\n    temp = train_data.iloc[i,11]\n    if temp==\"S\":\n        train_data.iloc[i,11]=0\n    if temp==\"C\":\n        train_data.iloc[i,11]=1\n    if temp==\"Q\":\n        train_data.iloc[i,11]=2\ntrain_data.head()\n","516d6bf6":"#\ub9ce\uc774 \uc2e4\uc218\ud558\ub294 \ubd80\ubd84 \uc911\uc5d0 \ud558\ub098 train_set \ubcc0\uacbd \uc2dc test_set\uc758 value\ub3c4 \uac19\uc774 \ubc14\uafd4\uc918\uc57c\ud568\nfor i in range(418):\n    temp = test_val.iloc[i,10]\n    if temp==\"S\":\n        test_val.iloc[i,10]=0\n    if temp==\"C\":\n        test_val.iloc[i,10]=1\n    if temp==\"Q\":\n        test_val.iloc[i,10]=2\ntest_val.head()","5c11d13c":"train_data.Name = train_data.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\ntrain_data.head()\ntest_val.Name =test_val.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\ntest_val.head()\n","6c3aa73c":"train_data.iloc[0,4]","75e10538":"dummy=[]\nMr = ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir']\nMrs = ['the Countess', 'Mme', 'Lady']\nMiss = ['Mlle', 'Ms']\nfor i in range(891):\n    temp = train_data.iloc[i,3]\n    if temp in Mr:\n        train_data.iloc[i,3]=\"Mr\"\n    elif temp in Mrs:\n        train_data.iloc[i,3]=\"Mrs\"\n    elif temp in Miss :\n        train_data.iloc[i,3]=\"Miss\"\n    elif temp =='Dr':\n        if train_data.iloc[i,4]==\"male\":\n            train_data.iloc[i,3]=\"Mr\"\n        else:\n            train_data.iloc[i,3]=\"Mrs\"\n    else:\n        dummy.append(temp)\n        pass\n\nprint(set(dummy))\n\ntrain_data.head()\n","6b1acf99":"test_val.iloc[0,2]","c778dce1":"dummy=[]\nMr = ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir']\nMrs = ['the Countess', 'Mme', 'Lady']\nMiss = ['Mlle', 'Ms']\n\nfor i in range(418):\n    temp = test_val.iloc[i,2]\n    if temp in Mr:\n        test_val.iloc[i,2]=\"Mr\"\n    elif temp in Mrs:\n        test_val.iloc[i,2]=\"Mrs\"\n    elif temp in Miss :\n        test_val.iloc[i,2]=\"Miss\"\n    elif temp =='Dr':\n        if test_val.iloc[i,3]==\"male\":\n            test_val.iloc[i,2]=\"Mr\"\n        else:\n            test_val.iloc[i,2]=\"Mrs\"\n    elif temp ==\"Dona\":\n        test_val.iloc[i,2]=\"Mrs\"\n    else:\n        dummy.append(temp)\n        pass\n\nprint(set(dummy))\n\ntrain_data.head()\n","cb592da9":"import numpy as np # linear algebra\nfrom sklearn.preprocessing import LabelEncoder\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n#Label encoder\ub97c \ud65c\uc6a9\ud558\uba74 \uc704\uc758 \uc791\uc5c5\uc744 \ub354 \uc27d\uac8c\ud560 \uc218 \uc788\uc74c\nLabel_encoder=LabelEncoder()\nLabel_encoder.fit(train_data.Sex)\ntrain_data.loc[:,'Sex'] = Label_encoder.transform(train_data.Sex)\ntest_val.loc[:,'Sex'] = Label_encoder.transform(test_val.Sex)\n\n#NA value\ub97c \ucc44\uc6b0\uae30 \uc704\ud574 \ub354\ubbf8 \ubcc0\uc218 \uc0dd\uc131\ndummy_age =round(train_data.Age.mean(),1)\nprint(dummy_age)\n","730409dd":"Label_encoder=LabelEncoder()\nLabel_encoder.fit(train_data.Name)\ntrain_data.loc[:,'Name'] = Label_encoder.transform(train_data.Name)\ntest_val.loc[:,'Name'] = Label_encoder.transform(test_val.Name)\ntrain_data.head()","bb53650d":"from sklearn import preprocessing\n\n\n#\ub098\uc774 \ubcc0\uc218\ub294 \uc911\uc694\ud558\ubbc0\ub85c scaling\ud574\uc900\ub2e4\ntrain_data.loc[:, 'Age'] = train_data.Age.fillna(dummy_age)\n# train_data.loc[:, 'Age'] = preprocessing.scale(train_data.Age)\ntest_val.loc[:, 'Age'] = test_val.Age.fillna(dummy_age)\n# test_val.loc[:, 'Age'] = preprocessing.scale(test_val.Age)\n#category value \uc911 \uc5c6\ub294 \ubd80\ubd84\uc740 \uc6b0\ub9ac\uac00 0,1,2\ub85c \ud574\uc92c\uae30 \ub54c\ubb38\uc5d0 0 \ubd80\uc5ec\n#NA\uac00 2\uac1c\ub77c\uc11c \ud06c\uac8c \uc0c1\uad00\uc740 \uc5c6\uc744\ub4ef \ud568\ntrain_data.loc[:, 'Embarked'] = train_data.Embarked.fillna(2)\ntest_val.loc[:, 'Embarked'] = test_val.Embarked.fillna(2)\n\n#\ube48\uacf3 \ucc44\uc6b0\uae30 \uc774\uacf3\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\nval=  train_data.Fare.mean() \ntrain_data.loc[:, 'Fare'] = train_data.Fare.fillna(val)\n# train_data.loc[:, 'Fare'] = preprocessing.scale(train_data.Fare)\ntest_val.loc[:, 'Fare'] = test_val.Fare.fillna(val)\n# test_val.loc[:, 'Fare'] = preprocessing.scale(test_val.Fare)\n\ntest_val.head()","adf1dab7":"train_data.head()","159d7f4c":"# import itertools\n\n# stuff = ['Age', 'Sex', 'Pclass','Embarked',\"SibSp\",\"Parch\",\"Name\"]\n# alov =[]\n# for L in range(0, len(stuff)+1):\n#     for subset in itertools.combinations(stuff, L):\n#         alov.append(list(subset))\n        \n# alov.pop(0)\n\nX = train_data.loc[:,['Age', 'Sex', 'Pclass', 'Embarked', 'SibSp']] #\nY = train_data.loc[:,'Survived']\nXTest = test_val.loc[:,['Age', 'Sex', 'Pclass', 'Embarked', 'SibSp']]\n#\ud574\ub2f9 \ubcc0\uc218\ub4e4\uc5d0 \ube48 \uacf3\uc774 \uc788\ub294\uc9c0 \ud655\uc778\ud55c\ub2e4\n#\uc788\uc73c\uba74 \ucf54\ub4dc\uac00 \uc548\ub3cc\uc544\uac10\ncount_nan = len(Y) - Y.count()\ncount_nan = len(X) - X.count()","10b65d9a":"import seaborn as sns\ncorr = X.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","73404c1a":"#Train test validation set\uc740 6\/2\/2\uac00 \uc88b\uc73c\ub098 \uc5ec\uae30\uc11c\ub294 testset\uc774 \uc774\ubbf8 \ube60\uc838\uc788\uc73c\ubbc0\ub85c test_size 20%\uc73c\ub85c \uc120\uc5b8\nfrom sklearn.model_selection import train_test_split\nXTrain, XValid, YTrain, YValid = train_test_split(\n      X, Y, test_size=0.33)\n\n\n\n","c566055b":"from sklearn.linear_model import LogisticRegression\nfrom subprocess import check_output\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier,GradientBoostingClassifier,ExtraTreesClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm, neighbors\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\n\nmodel1 = svm.LinearSVC()\nmodel2 = neighbors.KNeighborsClassifier()\nmodel3 =RandomForestClassifier()\nmodel4 =LogisticRegression()\nmodel5 =LinearDiscriminantAnalysis()\nmodel6 = DecisionTreeClassifier()\nmodel7 = MLPClassifier()\nmodel8 = ExtraTreesClassifier()\nmodel9 = AdaBoostClassifier()\nmodel10 = GradientBoostingClassifier()\nmodel11 =XGBClassifier(Eta=0.2)\n\n\n\nclf = VotingClassifier(estimators=[\n                        ('lsvc', model1),  #0.78 0.78\n                        ('knn', model2), #0.75 #0.85\n                        ('rfor', model3), #0.8 #0.81\n                        ('lr', model4), #0.8 #0.78\n                        ('lda', model5), #0.78 #0.77\n                        ('dc', model6), #0.81 #0.82\n                        ('mlpc', model7), #0.78 #0.8\n                        ('etc', model8), #0.78 #0.81\n                        ('abc', model9), #0.8 #0.81\n                        (\"gbc\",model10), #0.76 #0.8\/\n                        (\"XG\",model11) #0.81\n                                ])\nclf.fit(XTrain, YTrain)\n\n\nconfidence = clf.score(XValid, YValid)\nprint('accuracy:',confidence)\npredictions = clf.predict(XTest)","6863ad88":"#\uc774\uc81c \uc6b0\ub9ac\uac00 \uc81c\ucd9c\ud560 \ubd80\ubd84\n# yPredTest = LR.predict(XTest)\n# # \uc798 \ub098\uc624\ub294\uc9c0 \ud55c\ubc88 \ub208\uc73c\ub85c \ud655\uc778\n# print(yPredTest)","2237bc42":"#\uc81c\ucd9c\n# sub = pd.DataFrame({'PassengerId': test_val['PassengerId'],\n#                     'Survived': yPredTest})\n# sub.to_csv('scikitLRExample.csv', index=False)\nsub = pd.DataFrame({'PassengerId': test_val['PassengerId'],\n                    'Survived': predictions})\nsub.to_csv('scikitLRExample.csv', index=False)","6eeef34d":"**\ubcf5\uc7a1\ud55c \ubaa8\ub378\ub9cc\uc774 \ud56d\uc0c1 \uc62c\ubc14\ub978 \uac83\uc740 \uc544\ub2d9\ub2c8\ub2e4.**\n\n**\uc81c\uc548 \ub4dc\ub9ac\ub294 \ubaa8\ub378\ud5a5\uc0c1 \ubc29\ubc95**\n\n**1. voting classifier \uc774\uc678\uc5d0 \ub2e4\ub978 \uc559\uc0c1\ube14 \ubaa8\ub378\uc744 \uc801\uc6a9\uc2dc\ucf1c \ubcfc \uac83**\n\n**2. hyper parameter engineering**\n\n**3. feature engineering**\n\n**50%\ub9cc \ucc44\uc810\ud558\ub294 \uc774\ubc88 \ucef4\ud53c\ud2f0\uc158\uc758 \ud2b9\uc131\uc0c1 95%\uc774\uc0c1\uc758 accuracy\ub294 over-fitting\uc77c \uc218\ub3c4 \uc788\uc74c**\n\nI made some changes in this codeline. I'll update my annotation but basically this model is ensanbled model.\n"}}