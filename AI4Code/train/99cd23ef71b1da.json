{"cell_type":{"23fbf5b3":"code","ce76cbe9":"code","cc9f132a":"code","d611eadf":"code","72e1bdef":"code","f4beee6d":"code","604c723f":"code","f48dedf2":"code","ec34c5ba":"code","cf30bef6":"code","12a5b91e":"code","b3c8336b":"code","3c68f402":"code","9a5ff479":"code","bf098d04":"code","92d2f897":"code","5691cb8b":"code","6b9ff9bb":"code","1b052385":"code","9c0be23c":"code","6c34d698":"code","64a78b6a":"code","fc3b9e3f":"code","31e54a92":"code","7ca23343":"code","a5c8555c":"code","866aa445":"code","253da0b2":"markdown","01f5d764":"markdown","13a38349":"markdown","43b44dd6":"markdown","c7b31822":"markdown","edac29d8":"markdown","af5aaf94":"markdown","f50cc47d":"markdown","d2edfea4":"markdown","78ba1deb":"markdown","b194a180":"markdown","021ef20b":"markdown","519ace72":"markdown","dfe438fd":"markdown","48d0381a":"markdown","a6a2e825":"markdown","c31e3c2e":"markdown","00a94f47":"markdown","9fab1339":"markdown"},"source":{"23fbf5b3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom tqdm import tqdm\nimport os\nimport keras","ce76cbe9":"\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cc9f132a":"Train_Dir = '\/kaggle\/input\/facial-keypoints-detection\/training.zip'\nTest_Dir = '\/kaggle\/input\/facial-keypoints-detection\/test.zip'\nlookid_dir = '\/kaggle\/input\/facial-keypoints-detection\/IdLookupTable.csv'\ntrain_data = pd.read_csv(Train_Dir)  \ntest_data = pd.read_csv(Test_Dir)\nlookid_data = pd.read_csv(lookid_dir)\nos.listdir('..\/input')","d611eadf":"train_data.info()","72e1bdef":"train_data.isnull().sum()","f4beee6d":"feature_8 = ['left_eye_center_x','left_eye_center_y','right_eye_center_x','right_eye_center_y','nose_tip_x','nose_tip_y','mouth_center_bottom_lip_x','mouth_center_bottom_lip_y','Image']\n\ntrain_8 = train_data[feature_8].dropna().reset_index()\ntrain_30=train_data.dropna().reset_index()","604c723f":"train_8.info()","f48dedf2":"train_30.info()","ec34c5ba":"def str_to_int(train1):\n    images = train1.Image.values\n    del train1['Image']\n    del train1['index']\n    y=train1.values\n    x = []\n    for i in tqdm(images):\n        q=[int(j) for j in i.split()]\n        x.append(q)\n    x=np.array(x)\n    x=x.reshape(-1,96,96,1)\n    x=x\/255.0\n    return([x,y])","cf30bef6":"X_train_8,Y_train_8 = str_to_int(train_8)\nX_train_30,Y_train_30 = str_to_int(train_30)\nprint('X_train with 8 feature shape: ', X_train_8.shape)\nprint('y_train with 8 feature shape: ', Y_train_8.shape)\nprint('X_train with 30 feature shape: ', X_train_30.shape)\nprint('y_train with 30 feature shape: ', Y_train_30.shape)","12a5b91e":"from keras.models import Sequential\nfrom keras.layers import Activation,Convolution2D,MaxPooling2D,BatchNormalization,Flatten,Dense,Dropout,Conv2D,MaxPool2D,ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU","b3c8336b":"def create_model(out=8):\n    \n    model = Sequential()\n\n    # Input dimensions: (None, 96, 96, 1)\n    model.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    # Input dimensions: (None, 96, 96, 32)\n    model.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n    # Input dimensions: (None, 48, 48, 32)\n    model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    # Input dimensions: (None, 48, 48, 64)\n    model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n    # Input dimensions: (None, 24, 24, 64)\n    model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    # Input dimensions: (None, 24, 24, 96)\n    model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n    # Input dimensions: (None, 12, 12, 96)\n    model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    # Input dimensions: (None, 12, 12, 128)\n    model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n    # Input dimensions: (None, 6, 6, 128)\n    model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    # Input dimensions: (None, 6, 6, 256)\n    model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n    # Input dimensions: (None, 3, 3, 256)\n    model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n    # Input dimensions: (None, 3, 3, 512)\n    model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n    model.add(LeakyReLU(alpha = 0.1))\n    model.add(BatchNormalization())\n\n    # Input dimensions: (None, 3, 3, 512)\n    model.add(Flatten())\n    model.add(Dense(512,activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(out))\n    model.summary()\n    \n    model.compile(optimizer = 'adam' , loss = \"mean_squared_error\", metrics=[\"mae\"])\n    return model","3c68f402":"#Prepare 2 models to handle 2 different datasets.\nmodel_30 = create_model(out=30)\nmodel_8 = create_model(out=8)","9a5ff479":"#Prepare callbacks\nLR_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=10, factor=.4, min_lr=.00001)\nEarlyStop_callback = keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True)","bf098d04":"history = model_8.fit(X_train_8,Y_train_8,validation_split=.1,batch_size=64,epochs=50,callbacks=[LR_callback,EarlyStop_callback])","92d2f897":"# Plot the loss and accuracy curves for training and validation\nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['mae'], color='b', label=\"Training mae\")\nax[1].plot(history.history['val_mae'], color='r',label=\"Validation mae\")\nlegend = ax[1].legend(loc='best', shadow=True)","5691cb8b":"history = model_30.fit(X_train_30,Y_train_30,validation_split=.1,batch_size=64,epochs=50,callbacks=[LR_callback,EarlyStop_callback])\n","6b9ff9bb":"# Plot the loss and accuracy curves for training and validation\nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['mae'], color='b', label=\"Training mae\")\nax[1].plot(history.history['val_mae'], color='r',label=\"Validation mae\")\nlegend = ax[1].legend(loc='best', shadow=True)","1b052385":"images = test_data.Image.values\nx = []\nfor i in tqdm(images):\n    q=[int(j) for j in i.split()]\n    x.append(q)\nx=np.array(x)\nx=x.reshape(-1,96,96,1)\nx=x\/255.0","9c0be23c":"#Pridect points for each image using 2 different model.\ny_hat_30 = model_30.predict(x) \ny_hat_8 = model_8.predict(x)\nprint('Predictions shape', y_hat_30.shape)\nprint('Predictions shape', y_hat_8.shape)","6c34d698":"feature_8_index=[0,1,2,3,20,21,28,29]\nfor i in range(8):\n    y_hat_30[:,feature_8_index[i]] = y_hat_8[:,i]","64a78b6a":"def plot_face_pts(img, pts):\n    plt.imshow(img[:,:,0], cmap='gray')\n    for i in range(1,31,2):\n        plt.plot(pts[i-1], pts[i], 'b.')","fc3b9e3f":"#Display samples of the dataset.\nfig = plt.figure(figsize=(10, 7))\nfig.subplots_adjust(\n    left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\nfor i, f in enumerate(range(34,40)):\n    ax = fig.add_subplot(2, 3, i + 1, xticks=[], yticks=[])\n    plot_face_pts(x[f], y_hat_30[f])\n\nplt.show()","31e54a92":"#All required features in order.\nrequired_features = list(lookid_data['FeatureName'])\n#All images nmber in order.\nimageID = list(lookid_data['ImageId']-1)\n#Generate Directory to map feature name 'Str' into int from 0 to 29.\nfeature_to_num = dict(zip(required_features[0:30], range(30)))","7ca23343":"#Generate list of required features encoded into ints.\nfeature_ind = []\nfor f in required_features:\n    feature_ind.append(feature_to_num[f])","a5c8555c":"#Pick only the required predictions from y_hat_30 (filteration).\nrequired_pred = []\nfor x,y in zip(imageID,feature_ind):\n    required_pred.append(y_hat_30[x, y])","866aa445":"#Submit\nrowid = lookid_data['RowId']\nloc30 = pd.Series(required_pred,name = 'Location')\nsubmission = pd.concat([rowid,loc30],axis = 1)\nsubmission.to_csv('Predictions.csv',index = False)","253da0b2":"# Plotting the output images","01f5d764":"# Importing Necessary Packages","13a38349":"**Since the data is available in string form ,it needs to be converted into integer form for obtaining the pixel values of Image Input .\nThe function given below converts the string value to integer value and returns the required training sets after converting them to suitable forms.\n**","43b44dd6":"**Now the last step is to create our submission file keeping in mind the required format. \nThere should be two columns :- \n1. RowId \n2. Location\n\nThe Location column values should be filled according to the LookupTable provided as IdLookupTable.csv .**","c7b31822":"# Reading Inputs to Pandas Dataframes","edac29d8":"**Test data preparation**","af5aaf94":"**Result Prediction on both the models**","f50cc47d":"**Objective** : To predict keypoint positions on face images .\n\n**Given** : \n1. training.csv - It contains (x,y) coordinates of 30 facial keypoints(both left and right) and pixel values of Images.\n2. test.csv - It contains pixel values of images\n3. IdLookupTable.csv - It contains required Feature Names along with ImageId for submission.\n\n**Important Points** : \nI have read in one of the forums that the dataset is actually collected by merging 2 datasets together, the first one contains 7000+ samples with 8 features (4 keypoints) for each image, the second one contains 2000+ images that actually belongs to the first dataset but with 30 features (15 keypoints).\n\n**Simple Trick** :\nThe trick to reduce loss for this problem is to train 2 different models and merge the predictions on test set for final submission . \n\n**Model_1** will be trained on 8 features with 7000+ Images.\n\n**Model_2** will be trained on 30 features with 2000+ Images.\n\nImage Data Augmentation can be performed after splitting the data for better performance.\n\n**Kindly Upvote if you find this Notebook helpful**","d2edfea4":"**Thank You for going through the notebook. Please do provide your valuable feedback . \nFor any further queries feel free to comment in comment section.\n**","78ba1deb":"# Preparing the training data","b194a180":"**Splitting the data into two different training sets . The first set will be having 8 features while the second one will be having 30 features.**","021ef20b":"# Exploring Data","519ace72":"****","dfe438fd":"# Facial Keypoints Detection ","48d0381a":"**Having obtained the training set for both the models ,let's create 2 different models to handle 2 different datasets**","a6a2e825":"# Creating Final Submission","c31e3c2e":" **Merging the outputs of y_hat_8 to y_hat_30 by replacing each column in y_hat_8 with the corresponding column in y_hat_30 to prepare final output .**","00a94f47":"**Checking the count of null values in each of the columns .**","9fab1339":"# Model Training"}}