{"cell_type":{"9184509d":"code","d2f8f7c1":"code","b4e28a82":"code","e74a2a9a":"code","c2b8f368":"code","18d98480":"code","35debaf3":"code","e007213a":"code","9201db81":"code","f6966bb5":"code","1582916b":"code","a25114b4":"code","05b24b53":"code","cef9aedb":"code","b861c60b":"code","e573fb25":"code","07e836cc":"code","0ce679b0":"code","73b7243e":"code","78d93767":"code","99bdd49d":"code","892fc6b1":"code","c8a51e61":"code","1e652dd2":"code","c1c66568":"code","5e083c0d":"code","eed15d4f":"code","57f01284":"code","40c73fe7":"code","fca5c9bc":"code","3d69fb54":"code","0cfa567e":"code","0ef19446":"code","33768c74":"code","4cc44908":"markdown","c97de658":"markdown","23de7c42":"markdown","4e155889":"markdown","ac79f6aa":"markdown","103aa3ed":"markdown","5ac64011":"markdown","a79b4f21":"markdown","226ff01c":"markdown","ba2bc905":"markdown","1c1cbf18":"markdown","092c6d00":"markdown","5b1afcec":"markdown","199fbf7c":"markdown","5aed2b55":"markdown","bb467dde":"markdown","dcbadbf3":"markdown"},"source":{"9184509d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2f8f7c1":"import re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\n# from sklearn.cross_validation import KFold","b4e28a82":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\nPassengerId=test['PassengerId']\ntrain.head(3)","e74a2a9a":"PassengerId","c2b8f368":"print(type(train['Cabin'].iloc[0]))","18d98480":"full_data=[train,test]\ntrain['Name_length']=train['Name'].apply(len)\ntest['Name_length']=test['Name'].apply(len)\n\n#type(train['Cabin'].iloc[0]) ==> float\ntrain['Has_Cabin']=train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin']=test['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\n\nfor dataset in full_data:\n    dataset['FamilySize']=dataset['SibSp']+dataset['Parch']+1\n\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] ==1, 'IsAlone']=1\n    \nfor dataset in full_data:\n    dataset['Embarked']=dataset['Embarked'].fillna('S')\n    \nfor dataset in full_data:\n    dataset['Fare']=dataset['Fare'].fillna(train['Fare'].median())\n    \ntrain['CategoricalFare']=pd.qcut(train['Fare'], 4)\n\nfor dataset in full_data:\n    age_avg=dataset['Age'].mean()\n    age_std=dataset['Age'].std()\n    age_null_count=dataset['Age'].isnull().sum()\n    age_null_random_list=np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])]=age_null_random_list\n    dataset['Age']=dataset['Age'].astype(int)\n    \ntrain['CategoricalAge']=pd.cut(train['Age'],5)","35debaf3":"def get_title(name):\n    title_search=re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"\nfor dataset in full_data:\n    dataset['Title']=dataset['Name'].apply(get_title)\n    \nfor dataset in full_data:\n    dataset['Title']=dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'],'Rare')\n\ndataset['Title']=dataset['Title'].replace('Mlle', 'Miss')\ndataset['Title']=dataset['Title'].replace('Ms', 'Miss')\ndataset['Title']=dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    dataset['Sex']=dataset['Sex'].map({'female':int(0), 'male':int(1)})\n#     dataset['Sex']=dataset['Sex'].map({'female':0, 'male':1})\n#     dataset['Sex']=dataset['Sex'].astype(int)\n    title_mapping={'Mr':1, 'Miss':2, 'Mrs':3,'Master':4, 'Rare':5}\n    dataset['Title']=dataset['Title'].map(title_mapping)\n    dataset['Title']=dataset['Title'].fillna(0)\n    dataset['Embarked']=dataset['Embarked'].map({'S':0,'C':1,'Q':2}).astype(int)\n    \n    dataset.loc[dataset['Fare']<=7.91, 'Fare']=0\n    dataset.loc[(dataset['Fare']>7.91) & (dataset['Fare']<=14.454), 'Fare']=1\n    dataset.loc[(dataset['Fare']>14.454) & (dataset['Fare']<=31), 'Fare']=2\n    dataset.loc[(dataset['Fare']>31),'Fare']=3\n    dataset['Fare']=dataset['Fare'].astype(int)\n    \n    dataset.loc[dataset['Age']<=16,'Age']=0\n    dataset.loc[(dataset['Age'])>16 & (dataset['Age']<=32), 'Age']=1\n    dataset.loc[(dataset['Age'])>32 & (dataset['Age']<=48), 'Age']=2\n    dataset.loc[(dataset['Age'])>48 & (dataset['Age']<=64), 'Age']=3\n    dataset.loc[(dataset['Age'])>64, 'Age']=4","e007213a":"drop_elements=['PassengerId','Name','Ticket','Cabin', 'SibSp']\ntrain=train.drop(drop_elements,axis=1)\ntrain=train.drop(['CategoricalAge', 'CategoricalFare'],axis=1)\ntest=test.drop(drop_elements, axis=1)","9201db81":"train.head(10)","f6966bb5":"colormap=plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(), linewidths=0.1, vmax=1.0,square=True, cmap=colormap, linecolor='white', annot=True)","1582916b":"g= sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette='seismic', size=1.2, diag_kind='kde', diag_kws=dict(shade=True), plot_kws=dict(s=10))\ng.set(xticklabels=[])","a25114b4":"ntrain=train.shape[0]\nntest=test.shape[0]\nSEED=0\nNFOLDS=5\n\n# kf=KFold(ntrain, n_folds=NFOLDS, random_state=SEED)\nkf = KFold(n_splits= NFOLDS, random_state=SEED)","05b24b53":"class SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state']=seed\n        self.clf= clf(**params)\n    \n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n    def predict(self, x):\n        return self.clf.predict(x)\n    def fit(self, x,y):\n        return self.clf.fit(x,y)\n    def feature_importances(self,x,y):\n        return self.clf.fit(x,y).feature_importances_","cef9aedb":"def get_oof(clf, x_train,y_train, x_test):\n    oof_train=np.zeros((ntrain,))\n    oof_test=np.zeros((ntest,))\n    oof_test_skf=np.empty((NFOLDS, ntest))\n    \n    for i, (train_index, test_index) in enumerate(kf.split(train)):\n        x_tr=x_train[train_index]\n        y_tr=y_train[train_index]\n        x_te=x_train[test_index]\n        \n        clf.train(x_tr, y_tr)\n        \n        oof_train[test_index]=clf.predict(x_te)\n        oof_test_skf[i,:]=clf.predict(x_test)\n        \n    oof_test[:]=oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1,1),oof_test.reshape(-1,1)","b861c60b":"#random forest\nrf_params={'n_jobs':-1,\n          'n_estimators':500,\n          'warm_start':True,\n          'max_depth':6,\n          'min_samples_leaf':2,\n          'max_features':'sqrt',\n          'verbose':0}\n\n#extra tree\net_params={\n    'n_jobs':-1,\n    'n_estimators':500,\n    'max_depth':8,\n    'min_samples_leaf':2,\n    'verbose':0}\n\n#adaboost\nada_params={'n_estimators':500,\n           'learning_rate':0.75}\n\n#Gradient boosting\ngb_params={'n_estimators':500,\n          'max_depth':5,\n          'min_samples_leaf':2,\n          'verbose':0}\n\n# support vector machine\nsvm_params={'kernel':'linear',\n           'C':0.025}","e573fb25":"rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net= SklearnHelper(clf=ExtraTreesClassifier, seed= SEED, params=et_params)\nada=SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb=SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc=SklearnHelper(clf=SVC, seed=SEED, params=svm_params)","07e836cc":"#to numpy\ny_train=train['Survived'].ravel()\ntrain=train.drop(['Survived'],axis=1)\nx_train=train.values\nx_test=test.values","0ce679b0":"rf_oof_train, rf_oof_test=get_oof(rf, x_train,y_train, x_test)\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test)\nada_oof_train, ada_oof_test=get_oof(ada, x_train,y_train, x_test)\ngb_oof_train, gb_oof_test = get_oof(gb, x_train, y_train, x_test)\nsvc_oof_train, svc_oof_test=get_oof(svc, x_train,y_train, x_test)\n\nprint(\"training is completed\")","73b7243e":"rf_features=list(rf.feature_importances(x_train,y_train))\net_features=list(et.feature_importances(x_train,y_train))\nada_features=list(ada.feature_importances(x_train,y_train))\ngb_features=list(gb.feature_importances(x_train,y_train))","78d93767":"cols=train.columns.values\n\nfeature_dataframe=pd.DataFrame(\n    {\n        'features':cols,\n        'Random Forest feature importances': rf_features,\n        'Extra Trees feature importances':et_features,\n        'Adaboost feature importances':ada_features,\n        'Gradient Boost feature importances':gb_features\n    })","99bdd49d":"trace=go.Scatter(\n    y=feature_dataframe['Random Forest feature importances'].values,\n    x=feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        sizeref=1,\n        size = 25,\n        color=feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True),\n    text=feature_dataframe['features'].values\n)\n\ndata=[trace]\n\nlayout=go.Layout(\n    autosize=True,\n    title='Random Forest Feature Importance',\n    hovermode='closest',\n    yaxis=dict(\n        title='Feature Importance',\n        ticklen=5,\n        gridwidth=2),\n    showlegend=False\n)\nfig=go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","892fc6b1":"trace = go.Scatter(\n    y = feature_dataframe['Extra Trees feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Extra Trees feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Extra Trees feature importance',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","c8a51e61":"trace = go.Scatter(\n    y = feature_dataframe['Adaboost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Adaboost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'AdaBoost Feature Importance',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","1e652dd2":"trace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Feature Importance',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","c1c66568":"feature_dataframe.head(3)","5e083c0d":"feature_dataframe['mean'] = feature_dataframe.mean(axis=1)\nfeature_dataframe.head(3)","eed15d4f":"y=feature_dataframe['mean'].values\nx=feature_dataframe['features'].values\ndata=[go.Bar(x=x, y=y, width=0.5, marker=dict(color=feature_dataframe['mean'].values,\n                                             colorscale='Portland',\n                                             showscale=True,\n                                             reversescale=False),\n            opacity=0.6)]\n\nlayout=go.Layout(\nautosize=True,\ntitle='Barplots of Mean Feature Importance',\nhovermode='closest',\nyaxis=dict(title='Feature Importance',\n          ticklen=5,\n          gridwidth=2),\nshowlegend=False)\n\nfig=go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","57f01284":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n                                       'ExtraTrees': et_oof_train.ravel(),\n                                       'AdaBoost': ada_oof_train.ravel(),\n                                       'GradientBoost':gb_oof_train.ravel()})\nbase_predictions_train.head()","40c73fe7":"data = [\n    go.Heatmap(\n    z=base_predictions_train.astype(float).corr().values,\n    x=base_predictions_train.columns.values,\n    y=base_predictions_train.columns.values,\n    colorscale='Viridis',\n    showscale=True,\n    reversescale=True)\n]\npy.iplot(data, filename='labelled-heatmap')","fca5c9bc":"x_train=np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train,svc_oof_train), axis=1)\nx_test= np.concatenate((et_oof_test, rf_oof_test, ada_oof_test,gb_oof_test, svc_oof_test), axis=1)","3d69fb54":"et_oof_train[:10]","0cfa567e":"x_train[:3]","0ef19446":"gbm = xgb.XGBClassifier(\n    n_estimators=2000,\n    max_depth=4,\n    min_child_weight=2,\n    gamma=0.9,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective='binary:logistic',\n    nthread=-1,\n    scale_pos_weight=1).fit(x_train, y_train)\n\npredictions=gbm.predict(x_test)","33768c74":"StackingSubmission=pd.DataFrame({'PassengerId': PassengerId,\n                                'Survived': predictions})\nStackingSubmission.to_csv(\"StackingSubmission.csv\",index=False)","4cc44908":"### Gradient Boosting","c97de658":"## Out-of-Fold Predictions","23de7c42":"# Ensembel stacking model","4e155889":"## visualization","ac79f6aa":"Pearson Correlation plot","103aa3ed":"# Feature Exploration","5ac64011":"1 level classifier","a79b4f21":"I changed some codes and rest of codes are same as \n\"Introduction to Ensembling\/Stacking in Python\" notebook by Anisotropic\nhttps:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\/notebook\n\nbut KFold is moved to model_selection in scikit learn and I encountered a few errors about this.\nWhile I copying the notebook, I decided to turn this notebook to public for beginner.","226ff01c":"### First-level output as new features","ba2bc905":"### Adaboost","1c1cbf18":"## output of first level predictions","092c6d00":"### Random Forest","5b1afcec":"### Correlation Heatmap of the Second Level Training set","199fbf7c":"# Second-level Predictions from the first-level output","5aed2b55":"### Second level learning model via XGBoost","bb467dde":"### Extra trees","dcbadbf3":"* import \n> from sklearn.model_selection import KFold\n* n_split -> n_splits\n* kf.split\n* get_oof\n> def feature_importances(self,x,y):<br>\n>         return self.clf.fit(x,y).feature_importances_\n        \n* feature_importances\n> rf_features=list(rf.feature_importances(x_train,y_train)) \n<br>\n+)And some more details "}}