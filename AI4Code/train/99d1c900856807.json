{"cell_type":{"8a4a7b24":"code","b8756fbf":"code","a3f81c34":"code","8b626532":"code","439ab7ac":"code","415680d7":"code","4ae5703e":"code","324effb7":"code","748bd972":"code","9d83d0a5":"code","a00171c9":"code","782ec1f5":"code","5ea3312c":"code","3639e977":"code","a57d6939":"code","5be4d5cd":"code","9c0e8eec":"code","cf6e66f2":"code","634d785f":"code","517efff6":"code","a108ebed":"code","240a7a38":"code","39430f79":"code","f82bac26":"code","7480d2f7":"code","e68703f0":"code","c592239d":"code","6106abc3":"markdown","4a4f3d82":"markdown","a35570f7":"markdown","ea5631ab":"markdown","afea61d8":"markdown","b75d84c6":"markdown","3fd38b7e":"markdown","4fc71964":"markdown","98302122":"markdown","d510119b":"markdown","e2efe6ee":"markdown","fb2ca142":"markdown","2d945e84":"markdown","d60c6054":"markdown","086deda4":"markdown","48b46b3f":"markdown","ac74a705":"markdown","ae61e759":"markdown","91483251":"markdown"},"source":{"8a4a7b24":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, FunctionTransformer\nfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import StackingClassifier, RandomForestClassifier\nfrom xgboost import XGBClassifier","b8756fbf":"df = pd.read_csv('Data - Parkinsons')\ndf.head()","a3f81c34":"df.shape","8b626532":"df.info()","439ab7ac":"df.describe().T","415680d7":"df.status = df.status.astype('category')\ndf.status.value_counts()","4ae5703e":"df.describe()\ncorr = df.drop(['name', 'status'], axis = 1).corr()\nfig, ax = plt.subplots(figsize = [23,10])\n\nsns.heatmap(corr, annot = True, vmin = -1, vmax = 1, center = 0, cmap=\"coolwarm\")","324effb7":"corr_pos = corr.abs()\nmask = (corr_pos < 0.8 ) \nfig, ax = plt.subplots(figsize = [23,10])\nsns.heatmap(corr, annot = True, vmin = -1, vmax = 1, center = 0, mask = mask, cmap=\"coolwarm\")","748bd972":"f = ['MDVP:Jitter(%)',\t'MDVP:Jitter(Abs)',\t'MDVP:RAP',\t'MDVP:PPQ',\t'Jitter:DDP', 'NHR', \n     'MDVP:Shimmer',\t'MDVP:Shimmer(dB)',\t'Shimmer:APQ3',\t'Shimmer:APQ5',\t'MDVP:APQ',\t'Shimmer:DDA', 'HNR', 'spread1', 'PPE']\nfig, ax = plt.subplots(nrows = 4, ncols = 4, figsize = [23, 13])\nfor f, ax in zip(f, ax.flatten()):\n  sns.distplot(df[f], ax =ax)\n  mean = df[f].mean()\n  median = df[f].median()\n  ax.axvline(mean, color='r', linestyle='--')\n  ax.axvline(median, color='b', linestyle='-')\n  ax.legend({'Mean':mean, 'Median':median})\n","9d83d0a5":"features = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'NHR', 'HNR', 'RPDE', 'DFA',  'spread2', 'D2', 'PPE']\nfig, ax = plt.subplots(nrows = 2, ncols = 5, figsize = [23, 10])\nfor feature, ax in zip(features, ax.flatten()):\n  sns.distplot(df[feature], ax =ax)\n  mean = df[feature].mean()\n  median = df[feature].median()\n  ax.axvline(mean, color='r', linestyle='--')\n  ax.axvline(median, color='b', linestyle='-')\n  ax.legend({'Mean':mean, 'Median':median})\n","a00171c9":"fig, ax = plt.subplots(nrows = 2, ncols = 5, figsize = [23, 10])\nfor feature, ax in zip(features, ax.flatten()):\n  sns.boxplot(df[feature], ax =ax, orient = 'v')\n  ax.set_title(feature)\n","782ec1f5":"fig, ax = plt.subplots(nrows = 2, ncols = 5, figsize = [23, 10])\nfor feature, ax in zip(features, ax.flatten()):\n  sns.boxplot( x= 'status', y  = feature, data = df, ax =ax)","5ea3312c":"p_NHR = np.percentile(df.NHR, [25,75])\np_Jitter = np.percentile(df['MDVP:Jitter(%)'], [25,75])\niqr_NHR = p_NHR[1] = p_NHR[0]\niqr_Jitter = p_Jitter[1] = p_Jitter[0]\ncorrected_NHR = df.NHR.clip(p_NHR[0]-1.5*iqr_NHR, p_NHR[1]+1.5*iqr_NHR)\ncorrected_Jitter = df['MDVP:Jitter(%)'].clip(p_Jitter[0]-1.5*iqr_Jitter, p_Jitter[1]+1.5*iqr_Jitter)\ncorr = np.corrcoef(corrected_Jitter,corrected_NHR)\ncorr","3639e977":"p_HNR = np.percentile(df.HNR, [25,75])\np_Shimmer = np.percentile(df['MDVP:Shimmer'], [25,75])\niqr_HNR = p_HNR[1] = p_HNR[0]\niqr_Shimmer = p_Shimmer[1] = p_Shimmer[0]\ncorrected_HNR = df.NHR.clip(p_HNR[0]-1.5*iqr_HNR, p_HNR[1]+1.5*iqr_HNR)\ncorrected_Shimmer = df['MDVP:Shimmer'].clip(p_Shimmer[0]-1.5*iqr_Shimmer, p_Shimmer[1]+1.5*iqr_Shimmer)\ncorr = np.corrcoef(corrected_Shimmer,corrected_HNR)\ncorr","a57d6939":"#funection to treat outliers in the features\ndef outlier_correction(df):\n  outlier_features = ['MDVP:Fo(Hz)', 'RPDE', 'DFA',  'spread2', 'D2', 'PPE']\n  for col in outlier_features:\n    p = np.percentile(df[col], [25, 75])\n    iqr = p[1]-p[0]\n    df.loc[:, col].clip(lower = p[0]-1.5*iqr, upper = p[1]+1.5*iqr, inplace = True)\n  return df","5be4d5cd":"#Removing all highly correlated features and selecting only these features\nfeatures = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'NHR', 'HNR', 'RPDE', 'DFA',  'spread2', 'D2', 'PPE']\n#f = [ i for i in df.columns if i not in  ['name', 'status', 'spread1']]\ncolumn = ColumnTransformer( \n                              transformers = [('encoding', MinMaxScaler(), features)]\n                      )\npipeline_model = Pipeline(\n    [('outliers', FunctionTransformer(outlier_correction)),\n     ('scaling', column),\n     ('model', LogisticRegression())\n     ]\n)","9c0e8eec":"seed = 4\n#Seperating input and target features\nx = df.drop('status', axis =1)\ny = df['status']\n#splitting into train test split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, stratify = y, random_state = seed)","cf6e66f2":"#Creating many possible set of paramters for hyperparameter tuning for logistic regression\nparams_logistic = [ {\n    'scaling__encoding' : [MinMaxScaler(), StandardScaler(), RobustScaler()],\n    'model':[LogisticRegression()],\n    'model__penalty' : ['l1', 'l2'],\n    'model__solver' : ['liblinear'],\n    'model__C' : np.logspace(0, 4, 10),\n    'model__random_state' : [seed]\n},\n{\n    'scaling__encoding' : [MinMaxScaler(), StandardScaler(), RobustScaler()],\n    'model':[LogisticRegression()],\n    'model__penalty' : ['l2'],\n    'model__solver' : ['newton-cg', 'sag' , 'saga'  'ibfgs'],\n    'model__C' : np.logspace(0, 4, 10),\n    'model__random_state' : [seed]\n},\n{\n    'scaling__encoding' : [MinMaxScaler(), StandardScaler(), RobustScaler()],\n    'model':[LogisticRegression()],\n    'model__penalty' : ['elasticnet'],\n    'model__solver' : ['saga'],\n    'model__C' : np.logspace(0, 4, 10),\n    'model__random_state' : [seed]\n}\n]","634d785f":"#Creating many possible set of paramters for hyperparameter tuning for KNN\nparams_knn = {\n    'scaling__encoding' : [MinMaxScaler(), StandardScaler(), RobustScaler()],\n    'model': [KNeighborsClassifier()],\n    'model__n_neighbors' : [i for i in range(5, 15, 2)],\n    'model__leaf_size' : [i for i in range(3, 50, 5)],\n    # As we saw some outliers helped in classification and also we havent removed that outliers.\n    # Making the weight uniform make sure that model gives uniform weightage to outliers too.\n    'model__weights' : ['uniform'] \n}","517efff6":"#Creating many possible set of paramters for hyperparameter tuning for SVM\nparams_svm = {\n    'scaling__encoding' : [MinMaxScaler(), StandardScaler(), RobustScaler()],\n    'model' : [SVC(probability = True)],\n    'model__C': [i for i in range(1, 30, 1)],\n    'model__kernel' : ['linear']\n            }","a108ebed":"#Creating a dictionary of the parameters of all models\nparams = {\n          'Logistic Regression': params_logistic,\n          'KNN': params_knn,\n          'SVM': params_svm\n          }\n#Creating a array to store the best estimate\nmodel =[]\n#Creating DataFrame to store the performance metrics of each model.\nperformance = pd.DataFrame(columns = ['Model', 'Train Accuracy', 'Test Accuracy', 'Mean Cross Validation Accuracy',\n                                   '+\/- Deviation in Cross validation accuracy'])\n#Iterating throught each model\nfor key in params:\n  #Using StratifiedKFold to make sure that the both class of target is evenly distributed\n  stf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed)\n  gridSearch = GridSearchCV(pipeline_model, params[key], n_jobs= -1, cv = stf)\n  gridSearch.fit(x_train, y_train)\n  #Storing the tuple of best estimate of each model which can be fed as input to ensemble classifier.\n  model.append((key, gridSearch.best_estimator_))\n  cv = cross_val_score(gridSearch.best_estimator_, x, y, cv = stf)\n  performance = performance.append({'Model': key, \n                            'Train Accuracy':  np.round(gridSearch.score(x_train, y_train)*100, decimals = 2),\n                            'Test Accuracy': np.round(gridSearch.score(x_test, y_test)*100, decimals = 2),\n                            'Mean Cross Validation Accuracy': np.round(cv.mean()*100, decimals = 2),\n                            '+\/- Deviation in Cross validation accuracy': np.round(cv.std()*2*100, decimals = 2)}, ignore_index = True) ","240a7a38":"performance","39430f79":"#giving all the best estimators in list of tuples\n#It chooses Logistic Regression as a metaclassifier\nsk = StackingClassifier( estimators = model)\nsk.fit(x_train, y_train)\ncv_sk = cross_val_score(sk, x, y, cv = stf)\nperformance = performance.append({'Model': 'StackingClassifier', \n                            'Train Accuracy':  np.round(sk.score(x_train, y_train)*100, decimals = 2),\n                            'Test Accuracy': np.round(sk.score(x_test, y_test)*100, decimals = 2),\n                            'Mean Cross Validation Accuracy': np.round(cv_sk.mean()*100, decimals = 2),\n                            '+\/- Deviation in Cross validation accuracy': np.round(cv_sk.std()*2*100, decimals = 2)}, ignore_index = True) \nperformance","f82bac26":"import sklearn\nsklearn.__version__","7480d2f7":"params_ramdomforest = {\n    'model': [RandomForestClassifier(n_estimators = 300,  ccp_alpha = 0.02,\n                                     max_features = 0.4,  random_state = seed)],\n    'model__criterion': ['gini', 'entropy']\n}\nramdomforest = GridSearchCV(pipeline_model, params_ramdomforest, n_jobs= -1)\nramdomforest.fit(x_train, y_train)\ncv_ramdomforest = cross_val_score(ramdomforest.best_estimator_, x, y, cv = stf)\nperformance = performance.append({'Model': 'Random Forest', \n                          'Train Accuracy':  np.round(ramdomforest.score(x_train, y_train)*100, decimals = 2),\n                          'Test Accuracy': np.round(ramdomforest.score(x_test, y_test)*100, decimals = 2),\n                          'Mean Cross Validation Accuracy': np.round(cv_ramdomforest.mean()*100, decimals = 2),\n                          '+\/- Deviation in Cross validation accuracy': np.round(cv_ramdomforest.std()*2*100, decimals = 2)}, ignore_index = True) ","e68703f0":"params_xgboost = {\n    'model': [XGBClassifier(random_state= seed)],\n    'model__booster': ['gbtree', 'gblinear'],\n    'model__gamma': [ 1.5, 2, 3, 4],\n    'model__subsample': [0.6, 0.8, 1.0],\n    'model__colsample_bytree': [0.6, 0.8, 1.0],\n    'model__max_depth': [3, 4, 5]\n}\nxgboost = GridSearchCV(pipeline_model, params_xgboost, n_jobs= -1)\nxgboost.fit(x_train, y_train)\ncr_xgboost = cross_val_score(xgboost.best_estimator_, x, y, cv = stf)\nperformance = performance.append({'Model': 'XGBoost', \n                          'Train Accuracy':  np.round(xgboost.score(x_train, y_train)*100, decimals = 2),\n                          'Test Accuracy': np.round(xgboost.score(x_test, y_test)*100, decimals = 2),\n                          'Mean Cross Validation Accuracy': np.round(cr_xgboost.mean()*100, decimals = 2),\n                          '+\/- Deviation in Cross validation accuracy': np.round(cr_xgboost.std()*2*100, decimals = 2)}, ignore_index = True) ","c592239d":"performance","6106abc3":"##### Distribution of Target column\n\n* The target column is imbalanced as we have more dieseased people in dataset.","4a4f3d82":"##### Checking Outliers\n* There are many outliers in positive side of MDVP:Fhi(Hz), MDVP:Flo(Hz), NHR columns. Treating these outliers may affect the nature of the distribution as it is not normally distributed.\n* For all other attributes we can replace with 25th and 75th percentile for lower and higher outliers respectively.","a35570f7":"##### Distribution of selected features\n* Distribution of attributes (MDVP:Fo(Hz), HNR, RPDE, DFA, spread2, D2, PPE) are fairly normal.\n* Attributes (MDVP:Fhi(Hz), MDVP:Flo(Hz), NHR) are  right skewed.\n* Removing the outliers of these highly skewed features may affect the nature of distribution.","ea5631ab":"##### Checking the outliers of HNR\n* Similar to NHR even here removing the outliers of highly correlated columns decreasing the correlation.\n* We are not removing the outliers of these columns.","afea61d8":"##### Meta Classifier\n* As we saw there may be huge variation in output if use a single model to predict the output.\n* So here we are combining the output of all the 3 models and predicting the final output from the 3 models using another model called as Meta Classifier.\n* We are using logistic regression as a Meta classifier here.\n* We could see that mean accuracy and deviation of accuracy in cross validation is much better than all the models individually.\n* So collectively these models provided a better performance. ","b75d84c6":"##### Filtering Highly Correlated columns\n* From plot we can see that attributes (MDVP:Jitter(%),\tMDVP:Jitter(Abs),\tMDVP:RAP,\tMDVP:PPQ,\tJitter:DDP) are highly correlated to NHR.\n* Also attributes (MDVP:Shimmer, MDVP:Shimmer(dB),\tShimmer:APQ3,\tShimmer:APQ5,\tMDVP:APQ,\tShimmer:DDA) are highly correlated to HNR.\n* Then Speard1 and PPE are highly correlated.\n* So it is better to replace all these feature with 3 features NHR, HNR and PPE since adding highly correlated variables will not improve model performance but increase complexity. ","3fd38b7e":"* As per the below information, we can see that there is no direct null values in the dataset","4fc71964":"* The scale of all the attributes are very different from each other. So we have to scale the attributes before proceeding with model building.","98302122":"##### Boosting and Bagging Classifiers\n* Random Forest and XGBoost are powerfull Bagging and Boosting Classifiers respectively.\n* These models produce many weak learners and combine the output of these weak learners to give a best result. \n* We can see that both the models provided best train, test accuracy and also provided best performance in cross validation. ","d510119b":"##### Importing the Dataset as pandas dataframe","e2efe6ee":"##### Correlation Matrix\n* We can see that there is high corelation between many features in the dataset.","fb2ca142":"##### Checking with outliers of NHR\n* We know that the attributes (MDVP:Jitter(%),\tMDVP:Jitter(Abs),\tMDVP:RAP,\tMDVP:PPQ,\tJitter:DDP, NHR) are highly correlated to each other.\n* Lets treat the outliers for NHR and one column MDVP:Jitter(%).\n* On removing outliers we can see that the correlation between attributes is dropped.\n* So removing outliers is affecting the nature of the information. \n","2d945e84":"##### Shape of Dataset\n* No of columns - 24\n* No of rows - 195\n","d60c6054":"#### Pipeline\n\n* We are using Pipeline feature available in Scikit Learn to build our model.\n* Pipelines helps to bulid a better readable organized model.\n* It helps to process the train, test dataset seperately which refrains the model from data leakage.\n\n---\n#### Planned Steps:-\n\n1. Preprocessing\n>* Selection only these features and removing all highly correlated features. \n>*  (MDVP:Fo(Hz), MDVP:Fhi(Hz), MDVP:Flo(Hz), NHR, HNR, RPDE, DFA, spread2, D2, PPE)\n>* Removing the outliers of (MDVP:Fo(Hz), RPDE, DFA, spread2, D2, PPE)\n\n\n2. Scaling\n>* Scaling all the input features.\n\n3. ML Model\n>* Fitting the date to machine Learning Algorithm.","086deda4":"##### Observations\n\n* Here we could see that all the models performed well in both training and test data but there is a high variation in cross validation.\n* The range of accuracy is widely spread in cross validation, so there is a chances of getting least accuracy in this range for new data. \n* Dataset size is very small and also the target column is not equally distributed.\n* So a single model may not perform well.  ","48b46b3f":"##### Challenges\n* We could see that the size of sample is very small, so there many be more chances to over fit the data.\n* The target column is not balanced which may affect the performance of the model.\n* There are many input features. We have to perform EDA and select the most optimal features required for the model.","ac74a705":"##### Distribution of independent features with respect to target column\n* There is a non overlapping inter quartile range between 2 classes of target  for spread2 and PPE columns. So these features may have more weightage in prediction. \n* The outlier of MDVP:Fhi(Hz) and NHR is very high for diseased people which shows some unusuall values for diseased people. \n* Outliers of HNR  distributed in opposite direction between healthy and diseased people which may give some more information on classification.","ae61e759":"##### Choosing Best model\n\n* From above result we could see that both Random Forest and XGBoost gave best output than anyother models.\n* The variation in accuracy is least of all for Random Forest.\n* Being very narrow in deviation of accuracy, Random Forest is most stable model here.\n* So can pickup Random Forest as best model. ","91483251":"##### Nature of distribution of features to be removed\n* All the attributes which are correlated to NHR and HNR almost has very highly skewed on one side. \n* It do not follows normal distribution. So replacing the outliers with may affect the nature of distribution.\n* PPE and spread1 are normally distributed, so the outliers can be treated here. "}}