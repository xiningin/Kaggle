{"cell_type":{"bfb64d98":"code","cbc04eec":"code","b55f482b":"code","3b28837b":"code","22e3f726":"code","8630067c":"code","d915e945":"code","43b6d910":"code","02c45cd1":"code","66cec7de":"markdown","aabcd740":"markdown","de46fb40":"markdown","17db60b9":"markdown","21639f2f":"markdown","4e2bfc78":"markdown","3723cb63":"markdown","66029db0":"markdown","e4f98c1b":"markdown","f2148b9f":"markdown"},"source":{"bfb64d98":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport tensorflow.keras as keras","cbc04eec":"#reading both files\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nval_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","b55f482b":"NUM_CLASS = 10\n\n#making one hot encoding for the label\nlabel = data['label']\nlabel_one_hot = np.zeros((label.size, NUM_CLASS))\nfor i in range(label.size):\n    label_one_hot[i,label[i]] = 1\n#remove the label column, so the remaining 784 columns can form a 28*28 photo\ndel data['label']\n\n#changing data from DataFrame object to a numpy array, cause I know numpy better :p\ndata = data.to_numpy()\nprint(data.shape)\n\n#making data to 28*28 photo\ndata = data.reshape(-1,28,28,1)","3b28837b":"#checking out data shape\nprint(' data shape: {} \\n one hot lable shape: {}'.format(\n    data.shape, label_one_hot.shape))","22e3f726":"#simple CNN model with Keras\nimport keras\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution2D, MaxPooling2D, BatchNormalization\nfrom keras.layers import Dense, Flatten, Activation\n\nmodel = Sequential([\n    \n    Convolution2D(filters = 5, kernel_size = (3,3), activation = 'relu', input_shape=(28,28,1)),\n    MaxPooling2D(pool_size=(2,2)),\n    BatchNormalization(),\n    #do a drop out here if interested\n    \n    Convolution2D(filters = 25, kernel_size = (3,3), activation = 'relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    BatchNormalization(),\n    #do a drop out here if interested\n    \n    Flatten(),\n    Dense(10),\n    Activation('softmax')\n])","8630067c":"model.compile('adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy']\n             )","d915e945":"#Starts training the model over the data 10 times.\n#Here nothing fancy added for keeping it really really simple.\n\nhistory = model.fit(data, label_one_hot, epochs = 10, validation_split = 0.1)","43b6d910":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.show()","02c45cd1":"#we read the csv before, but just read it again here.\nval_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\n#the same way to process the training data after seperating the label\nval_data = val_data.to_numpy()\nval_data = val_data.reshape(-1,28,28,1)\n\n#here we ask the model to predict what the class is\nraw_result = model.predict(val_data)\n\n#note: model.predict will return the confidence level for all 10 class,\n#      therefore we want to pick the most confident one and return it as the final prediction\nresult = np.argmax(raw_result, axis = 1)\n\n#generating the output, remember to submit the result to the competition afterward for your final score.\nsubmission = pd.DataFrame({'ImageId':range(1,len(val_data) + 1), 'Label':np.argmax(raw_result, axis = 1)})\nsubmission.to_csv('SimpleCnnSubmission.csv', index=False)","66cec7de":"# Model\n\n## it's magic \n\njoking, personally I think there are blog post that can explain way better and deeper than I can.\n\nHere is my attempt to explain CNN (convolutional nueral net)\n\nso everthing after the `flatten()` is 1 layer nueral net, trying to reduce all the features into confident level for each number.\n\n### Convolution layer\nthis layer uses a 3 x 3 kernal (`kernal_size = (3,3)`) that scans through the photo overlappingly and extract 5 different feature maps (`filters = 5`). A 28 x 28 x 1 photo goes in will come out as a 26 x 26 x 5 feature map. (26 x 26 is becuse a 3 x 3 filter can only have 26 position in a 28 pixel edge)\n\n<span style=\"color:brown\"> \nMain take away: it extracts features\n\n### MaxPooling layer\nthis layer uses a 2 x 2 kernal (`pool_size = (2,2)`) that scans through the photo non-overlappingly, selects the largest value and output it to the next level. So a 26 x 26 x 5 feature map from the last layer will become 13 x 13 x 5 for the next layer (it doesn't reduce the z axis). It's main function is to reduce the input size for the next layer with some other benifit of like less sensitive to local variation, etc, etc.\n\n<span style=\"color:brown\"> \nMain take away: it reduce the size. smaller the photo, trains faster.\n    \n### Batch Normalization layer\nit Normalize the photo by the mean and variance of the photo. Making it more uniform for the next layer to work on. Usually performed after the pooling layers (or any upscale or downscale layers).\n\n<span style=\"color:brown\"> \nMain take away: it conditions the photo. Making it easier for the next layer to work on the data.\n    \n### Flatten layer\nthe layer squizes everything into a 1 dimension array. for example, a 13 x 13 x 5 feature map will turn into a array of 845. for preperation of the regular neural net.","aabcd740":"## Training model\n\nHere we provide the model with training set (data and label_one_hot), and we ask it to go through the set 10 times (epochs = 10).\nBut we ask the model to only train on 90% of the data, and the remaining 10% as a validation data set. (validation_split = 0.1)\n\n<span style=\"color:brown\"> \nfun note: validation_split is like reserving some question as a mid-term or quiz, it's used as a rough estimation of real world performce for us to see the model's traning progress.\n    \nIf the validation data accuracy starts to decrease, while the training accuracy still increase. It is a good indication of over-fitting occuring and should reduce the epoches.\n<span style=\"color:blue\">\nThere's a automatic way to do so. It's called early stopping, you should check it out ;)\n\n<span style=\"color:brown\"> \nfun note: Over-fitting is like when the student did the same question sets for too many times and kinda remembers the solution instead of learning.","de46fb40":"Okay, Here we finish preparing our data\n\nOn the cell above, we can have data and label_one_hot, where one is the question (28 * 28 photo) and one is the solution\n\nand it also shows that we have 42000 photos and solution pairs to work with.","17db60b9":"## Compile model\n\nHere we can see we complieled it with adam optimizer and a loss function of catagorical crossentropy.\n\nThe metrics is optional, we added accuracy here for use extra information.\n\n<span style=\"color:brown\"> \nfun note: basically telling what the model how it should do better next time (optimizer) and how performance is measured (loss function). Also asking it to show up some other performance metrics during training for us to see (metrics).","21639f2f":"Hi all, hope this will help you in your ML journey\n\nSo a bearbone ML flow having this components\n1. data\n2. model\n3. prediction\n\nExplaination alone the way\n\nthis notebook won't have anything fancy, but hopefully get you interested.","4e2bfc78":"# End of traning\n\nhere is a graph on how the model did for each epoches. Just some visulization for how the model performs, but it can also a critical part to see further improve your model.","3723cb63":"# Prediction\n\nThe model are given a set of 28 * 28 photos that it haven't seen before and are ask to give a prediction on what number it is.\n\n<span style=\"color:brown\"> \nfun note: Well, it's the final exam for the model. Wish the model luck and wish you luck too.","66029db0":"It's a fun ride, hope you learn something here and there :)","e4f98c1b":"## processing the data before feeding it into the model\n\n1. seperating the label and the actual data in train.csv\n2. change the label from 0 - 9 to **One Hot Encoding**\n\nOne Hot encoding with 10 classes\n```\nsolution -> One Hot version of the solution\n0 -> [1,0,0,0,0, 0,0,0,0,0]\n1 -> [0,1,0,0,0, 0,0,0,0,0]\n2 -> [0,0,1,0,0, 0,0,0,0,0]\n3 -> [0,0,0,1,0, 0,0,0,0,0]\n4 -> [0,0,0,0,1, 0,0,0,0,0]\n5 -> [0,0,0,0,0, 1,0,0,0,0]\n6 -> [0,0,0,0,0, 0,1,0,0,0]\n7 -> [0,0,0,0,0, 0,0,1,0,0]\n8 -> [0,0,0,0,0, 0,0,0,1,0]\n9 -> [0,0,0,0,0, 0,0,0,0,1]\n```\n\nI think you see what one hot encoding is here. Only one bit are \"hot\" down there, and each bit represents a possible answer (0 to 9 in our case)\n\n<span style=\"color:brown\">  \nwhy One Hot Encoding?\n<span style=\"color:brown\">  \nmainly an easy way to let the model know how wrong it's prediction are. Think about confident level for each solution that the model returns. We reward confidents level for the correct solution and pushish any confidence on the incorrect solutions.\n<\/span>\n\n<span style=\"color:brown\"> \nlet's say you gave the model a hand-writen photo of 0, and it returns it's confident level for each class.\n```\nconfident level we got back from the model:\nnumber 0  , 1  , 2  , 3  , 4  , 5  , 6  , 7  , 8  , 9\n      [0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.3]\n```\n<span style=\"color:blue\"> \n(what I am gonna talk about the main idea of loss function, in actuality there's so many different better way to do so).\n    \n<span style=\"color:brown\"> \nHere we got some pretty nice prediction, where the model is 90% sure the solution is a 0, so we reward the model 0.9 points, but we don't like the fact that the model is hassitaing that it might be a 6 or 9. Eventhough they might look similar, we would still like the model to be more confident on it's answer, so we will take off 0.2 points and 0.3 points respectively. The Final score for this prediction is 0.9 - (0.2+0.3) = **0.4**\n\n<span style=\"color:brown\"> \nSo the model have 2 ways to improve the score, one is to increase the condifence level of the correct solution, another is to reduce the confidentce level of other numbers. The model and go back and adjust it's nural network to achieve it.","f2148b9f":"Explaination for the data we get\n\n## data\n### train.csv\nIn this case we have a 28 * 28 photo of a hand writing digit, which is represented by 784 columns for each pixel.\nIn additional of the 784 columns, the first column (named \"label\"), holds the correct solution of the prediction.\n<span style=\"color:blue\">        \nnote: The first column (\"label\" column) needed to be seperated before we feed it into the model, but it's a simple task.\n<\/span>\n\n<span style=\"color:brown\">  \nfun note: so this is like the homework for the model, where it have the question and solution. The model does the homework over and over again, knowing how well it did and improve itself next time.\n<\/span>\n        \n### test.csv\nThis section contains only the 28 * 28 photos, with no label columns. Feed the photos to the model and submit the models prediction to the competion. \n\n<span style=\"color:brown\">\nfun note: so this is the final exam for the model, only questions the model never saw and will be a good estimate on how well they perform in the real world.\n<\/span>"}}