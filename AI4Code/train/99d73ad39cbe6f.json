{"cell_type":{"7c659d92":"code","e944e7d4":"code","bae0b288":"code","b95e1aea":"code","a9697657":"code","c24b9f85":"code","e795efd9":"code","257e50be":"code","25f0188e":"code","1349ac01":"code","fd6c62d5":"code","7061e93c":"code","b0101b7a":"code","a4c15e98":"code","60bc244c":"code","583ac79e":"code","a212459d":"code","5fa082df":"code","500431bb":"code","a4b10a03":"code","9291cbe3":"code","35bf780e":"code","50367a88":"code","078ca3a6":"code","fbab4c8f":"markdown","c6328ad2":"markdown","dfe4a230":"markdown","ef65ea0b":"markdown","64e43d2e":"markdown","48e35b3d":"markdown","d4a07bec":"markdown","e99547c6":"markdown","aba63b4a":"markdown","0568d5a5":"markdown","20d60739":"markdown","bc413507":"markdown","74bf5f95":"markdown","af9c69cf":"markdown","fce22868":"markdown","85711499":"markdown","8572e608":"markdown","22e80914":"markdown","3239ce49":"markdown","14aa91ab":"markdown","bc3dcf91":"markdown","a7ebb992":"markdown","a29d6b63":"markdown","44a2b4c1":"markdown"},"source":{"7c659d92":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e944e7d4":"df = pd.read_csv('..\/input\/ml-challenge\/checkins.csv')\ndf.count()","bae0b288":"df.head(20)","b95e1aea":"df.tail(20)","a9697657":"df.isna().sum()","c24b9f85":"df.isin([0]).sum()","e795efd9":"df.nunique()","257e50be":"duplicateRowsDF = df[df.duplicated(['user_id', 'venue_id','created_at'])]\n\nduplicateRowsDF.count()","25f0188e":"noDupdf = df.drop_duplicates(subset=['user_id', 'venue_id','created_at'], inplace=False)\nnoDupdf.count()","1349ac01":"noDupdf['user_id'].value_counts().head(15)","fd6c62d5":"top10df = noDupdf['user_id'].value_counts().head(14).rename_axis('user_id').reset_index(name='checkin_counts')","7061e93c":"userdf = pd.read_csv('..\/input\/ml-challenge\/users.csv')\nuserdf.head()","b0101b7a":"mergeuserdf = pd.merge(left=top10df, right=userdf, left_on='user_id', right_on='id')\nmergeuserdf","a4c15e98":"import folium\nfrom folium import Choropleth, Circle, Marker\nlat = mergeuserdf.iloc[0]['latitude']\nlong = mergeuserdf.iloc[0]['longitude']\nusermap = folium.Map(location=[lat, long], tiles='cartodbpositron', zoom_start=3)\n# Add a bubble map to the base map\nfor i in range(0,len(mergeuserdf)):\n      Circle(\n        location=[mergeuserdf.iloc[i]['latitude'], mergeuserdf.iloc[i]['longitude']],\n        radius=20,\n        color='darkred').add_to(usermap)\nusermap","60bc244c":"mergeuserdf = mergeuserdf.drop('id', axis=1)\nmergeuserdf.rename(columns = {'latitude':'user_latitude', 'longitude':'user_longitude'}, inplace = True)\nmergeuserdf.to_csv('top10checkins.csv', index=False)","583ac79e":"ratingsdf = pd.read_csv('..\/input\/ml-challenge\/ratings.csv')\npd.options.display.float_format = \"{:.2f}\".format\nratingsdf.describe()","a212459d":"dupRatingsDF = ratingsdf[ratingsdf.duplicated(['user_id', 'venue_id'])]\n\ndupRatingsDF.count()","5fa082df":"avgratingsdf = ratingsdf.groupby(['user_id','venue_id']).mean().reset_index()\navgratingsdf.head(10)","500431bb":"avgratingsdf.shape","a4b10a03":"avgratingsdf.to_csv('avgratings.csv', index=False)","9291cbe3":"venuesdf = pd.read_csv('..\/input\/ml-challenge\/venues.csv')\nvenuesdf.describe()","35bf780e":"print(venuesdf[venuesdf.latitude > 90])","50367a88":"venuesdf = venuesdf[venuesdf['latitude'] <= 90]\nvenuesdf.describe()","078ca3a6":"venuesdf.to_csv('correct_venues.csv', index=False)","fbab4c8f":"Now let's check the venues table:","c6328ad2":"There are duplicate ratings submitted by user for the same venue. Let's use the average rating","dfe4a230":"Now lets look at the ratings table.","ef65ea0b":"Let's proceed to do a join with the userdf & top10df frames with id as key","64e43d2e":"Hmm..looking at the venues file, there are some locations which have latitude > 90, which can't be true, so we have to remove them from the data..let's see how many venues are affected","48e35b3d":"Let's write this corrected venues to file for later use","d4a07bec":"Let's keep this list of 14 users for now","e99547c6":"and last 20 rows:","aba63b4a":"There are 6 columns in the checkins table, with total 1021966 rows.\nLet's check on the 1st 20 rows:","0568d5a5":"Right now, let's see where these 14 users are located. For that I will need to get the info in the users table","20d60739":"It seems the 1st 15 rows are 'missing', the field 'id' starts from 16 and ends at 1021981.","bc413507":"Let's save this info to a file","74bf5f95":"Ok..so there's only 1 venue which has a latitude that cant't exist..let's proceed to remove this venue","af9c69cf":"Ok..so there is actually a tie for 10th spot based on no. of checkins -> There are 5 users sharing 10th place.","fce22868":"There are 2809580 rows, ratings range from 2 to 5. Let's check for duplicate ratings.","85711499":"Ok..there are 625,332 geo coordinates which are all zero. How about the unique entries in this file?","8572e608":"Now lets get the top 15 user_ids","22e80914":"Let's check for any null values in the table:","3239ce49":"Ok..so there's plenty of duplicate values. We need to clean this mess up.","14aa91ab":"It looks like all the users are located in USA, some on the west coast, some on the east coast.","bc3dcf91":"All latitude and longtitude info are there, so now let's create some maps!","a7ebb992":"Let's write this avg ratings to file for later use","a29d6b63":"It looks like there are 485,381 unique user_ids in the checksin table, and 83,999 unique venue_ids. How about duplicate entries?","44a2b4c1":"Ok...so no null values, how about zero values?"}}