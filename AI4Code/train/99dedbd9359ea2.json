{"cell_type":{"cc6bf171":"code","75932a06":"code","134746cd":"code","462d28d6":"code","91196a76":"code","cf61ba34":"code","4c529b34":"code","9e16fae6":"code","908e3f66":"code","443b232f":"code","2e8a8c99":"code","6fbdfad9":"code","cc20e2ad":"code","fac1a501":"code","8991b8e2":"code","246492df":"code","233878e1":"code","dbda02f1":"code","61404373":"code","ef700074":"code","ac9bd867":"code","104202a0":"code","7586d0b2":"code","b2c196e0":"code","fcee2723":"code","ed05aeac":"code","d5ec3858":"code","bf048cf4":"code","815f7f94":"code","0418a2aa":"code","9c37336e":"code","d7e82338":"code","e512d2d3":"code","40a05a64":"code","30e54d56":"code","6a65305d":"code","58c1192f":"code","464a41b5":"code","a70b48fd":"code","7d458dd0":"code","f881ea74":"code","0f4aecad":"code","29ac2842":"code","7872ed77":"code","6a8ad80c":"code","9d94e790":"code","891a0896":"code","ab2ff3fb":"code","4e9f00c7":"code","976b8a78":"code","248d324b":"code","0def3095":"code","1fb51804":"code","fa0a152b":"code","b8d2836b":"code","4cf89fe9":"code","48df446a":"code","0b7ac9be":"code","4a721c3a":"code","7fdf13a5":"code","db43dc40":"markdown","2e85b945":"markdown","5b57fcd5":"markdown","32bb2480":"markdown","08d2d547":"markdown","7fc8c678":"markdown","6b663a21":"markdown","6d80c0eb":"markdown","d13a622e":"markdown","5eec9454":"markdown","928f8405":"markdown","e508c7c9":"markdown","9c766f97":"markdown","756d502e":"markdown","a09f4c57":"markdown","86fb65a9":"markdown","5e7d7643":"markdown","f44c3a34":"markdown","22de3b7e":"markdown","174d0349":"markdown","4a6a3d1a":"markdown"},"source":{"cc6bf171":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75932a06":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","134746cd":"import os\nprint(os.listdir(\"..\/input\/bank-loan-modelling\"))","462d28d6":"df = pd.read_excel('..\/input\/bank-loan-modelling\/Bank_Personal_Loan_Modelling.xlsx', 'Data')\ndf.columns = [\"ID\",\"Age\",\"Experience\",\"Income\",\"ZIPCode\",\"Family\",\"CCAvg\",\"Education\",\"Mortgage\",\"PersonalLoan\",\"SecuritiesAccount\",\"CDAccount\",\"Online\",\"CreditCard\"]","91196a76":"df.head(4) #to check first 4 rows of data set.","cf61ba34":"df.info()\n","4c529b34":"listitem=[]\nfor col in df.columns:\n    listitem.append([col,df[col].dtypes,df[col].isna().sum(),round((df[col].isna().sum()\/len(df[col]))*100,2),df[col].nunique(),df[col].unique()])\ndfdesc=pd.DataFrame(columns=['features','dtype','Null value count','Null value percentage','Unique count','Unique items'],data=listitem)\ndfdesc","9e16fae6":"df.shape #to check no of rows and column","908e3f66":"pd.set_option(\"display.float\",\"{:.2f}\".format)\ndf.describe()","443b232f":"df.PersonalLoan.value_counts()","2e8a8c99":"plt.figure(figsize=(5,5))\ndf.PersonalLoan.value_counts().plot(kind=\"bar\",color=['salmon','lightblue'])","6fbdfad9":"categorical_val=[]\ncontinuous_val=[]\nfor column in df.columns:\n    print('=================')\n    print(f\"{column} : {df[column].unique()}\")\n    if len(df[column].unique()) <= 10:\n        categorical_val.append(column)\n    else:\n        continuous_val.append(column)","cc20e2ad":"categorical_val","fac1a501":"continuous_val","8991b8e2":"plt.figure(figsize=(17,17))\nfor i , column in enumerate(categorical_val,1):\n    plt.subplot(3,3,i)\n    df[df[\"PersonalLoan\"]==0][column].hist(bins=35,color='red',label='Have Personal Loan = No')\n    df[df[\"PersonalLoan\"]==1][column].hist(bins=35,color='Blue',label=\"Have Personal Loan = Yes\")\n    plt.legend()\n    plt.xlabel(column)","246492df":"df[continuous_val].plot(kind='box',subplots=True, layout=(3,3), fontsize=10, figsize=(14,14))","233878e1":"sns.pairplot(data=df)","dbda02f1":"plt.figure(figsize=(7,7))\nplt.scatter(x='Age',y='Experience',data=df)","61404373":"df[\"Age\"].value_counts().plot.bar(figsize=(20,6))","ef700074":"df.describe()['Experience']","ac9bd867":"df[df['Experience']<0].count()","104202a0":"# Let us replace all the negative Experience data points by absolute value.\ndf['Experience']=df['Experience'].apply(abs)","7586d0b2":"df[df['Experience']<0].count()","b2c196e0":"Outlier = ['Income', 'CCAvg', 'Mortgage']\nQ1=df[Outlier].quantile(0.25)\nQ3=df[Outlier].quantile(0.75)\nIQR=Q3-Q1\nLL,UL = Q1-(IQR*1.5),Q3+(IQR*1.5)\n\nfor i in Outlier:\n    df[i][df[i]>UL[i]]=UL[i];df[i][df[i]<LL[i]]=LL[i] ","fcee2723":"df[continuous_val].plot(kind='box',subplots=True, layout=(3,3), fontsize=10, figsize=(14,14))","ed05aeac":"corr=df.corr()\nfig,ax=plt.subplots(figsize=(12,12))\nax=sns.heatmap(corr,annot=True,square=True,fmt=\".2f\",cmap=\"YlGnBu\")\n","d5ec3858":"categorical_val.remove('PersonalLoan')\nprint(categorical_val)","bf048cf4":"dataset = df.copy() # Let us create new dataset","815f7f94":"from sklearn.preprocessing import LabelEncoder\nencode=LabelEncoder()\nlabel1=encode.fit_transform(df['Family'])\nlabel2=encode.fit_transform(df['Education'])\ndataset['Family']=label1\ndataset['Education']=label2","0418a2aa":"dataset","9c37336e":"dataset.drop(['ID','Experience','ZIPCode'],axis=1,inplace=True)","d7e82338":"from sklearn.preprocessing import StandardScaler\nssc=StandardScaler()\ncol_to_scale=['Age','Income','CCAvg','Mortgage']\ndataset[col_to_scale] = ssc.fit_transform(dataset[col_to_scale])","e512d2d3":"X = dataset.drop('PersonalLoan', axis=1)\ny = dataset.PersonalLoan","40a05a64":"PersonalLoan1=dataset[dataset['PersonalLoan']==1]\nPersonalLoan0=dataset[dataset['PersonalLoan']==0]\nprint(PersonalLoan1.shape,PersonalLoan0.shape)","30e54d56":"## RandomOverSampler to handle imbalanced data\nfrom imblearn.over_sampling import RandomOverSampler\nos =  RandomOverSampler(random_state=101)\nX_ros,y_ros=os.fit_sample(X,y)","6a65305d":"from collections import Counter\nprint('Original dataset shape {}'.format(Counter(y)))\nprint('Resampled dataset shape {}'.format(Counter(y_ros)))","58c1192f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size= 0.3, random_state=42)","464a41b5":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_ros, y_ros)\ny_pred=log_reg.predict(X_test)\n","a70b48fd":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","7d458dd0":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test,y_pred)","f881ea74":"from sklearn.metrics import (accuracy_score , f1_score , precision_score , recall_score)\nprint(\"Accuracy:\" , accuracy_score(y_test,y_pred))\nprint(\"Precision:\",precision_score(y_test , y_pred))\nprint(\"Recall:\",recall_score(y_test,y_pred))\nprint(\"F1:\", f1_score(y_test,y_pred))","0f4aecad":"data = pd.read_excel('..\/input\/bank-loan-modelling\/Bank_Personal_Loan_Modelling.xlsx', 'Data')\ndata.columns = [\"ID\",\"Age\",\"Experience\",\"Income\",\"ZIPCode\",\"Family\",\"CCAvg\",\"Education\",\"Mortgage\",\"PersonalLoan\",\"SecuritiesAccount\",\"CDAccount\",\"Online\",\"CreditCard\"]","29ac2842":"data.head()","7872ed77":"categorical_val=[]\ncontinuous_val=[]\nfor column in data.columns:\n    print('=================')\n    print(f\"{column} : {data[column].unique()}\")\n    if len(data[column].unique()) <= 10:\n        categorical_val.append(column)\n    else:\n        continuous_val.append(column)","6a8ad80c":"data[continuous_val].plot(kind='box',subplots=True, layout=(3,3), fontsize=10, figsize=(14,14))","9d94e790":"# Capping Method\n\nOutlier = ['Income', 'CCAvg', 'Mortgage']\nQ1=data[Outlier].quantile(0.25)\nQ3=data[Outlier].quantile(0.75)\nIQR=Q3-Q1\nLL,UL = Q1-(IQR*1.5),Q3+(IQR*1.5)\n\nfor i in Outlier:\n    data[i][data[i]>UL[i]]=UL[i];data[i][data[i]<LL[i]]=LL[i] ","891a0896":"data[continuous_val].plot(kind='box',subplots=True, layout=(3,3), fontsize=10, figsize=(14,14))","ab2ff3fb":"# Standardizing the variables\nfrom sklearn.preprocessing import StandardScaler\nssc=StandardScaler()\ncol_to_scale=['Age','Income','CCAvg','Mortgage']\ndata[col_to_scale] = ssc.fit_transform(data[col_to_scale])","4e9f00c7":"data.drop(['ID','Experience','ZIPCode'],axis=1,inplace=True)\ndata","976b8a78":"cat=['Family','Education','SecuritiesAccount','CDAccount','Online','CreditCard']","248d324b":"target_col='PersonalLoan'\nX= df.loc[:,df.columns!=target_col]\ny=df.loc[:,target_col]","0def3095":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","1fb51804":"features=list(X_train.columns)","fa0a152b":"# Importing Library\n!pip install catboost\nfrom catboost import CatBoostClassifier","b8d2836b":"model_cb=CatBoostClassifier(iterations=1000, learning_rate=0.01, loss_function= 'Logloss', eval_metric='AUC',use_best_model=True,random_seed=42) # use_best_model params will make the model prevent overfitting","4cf89fe9":"model_cb.fit(X_train,y_train,cat_features=cat,eval_set=(X_test,y_test))\n","48df446a":"y_pred1 =model_cb.predict(X_test)","0b7ac9be":"from sklearn.metrics import classification_report\nprint(classification_report(y_test , y_pred1))","4a721c3a":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test , y_pred1))","7fdf13a5":"print(\"Accuracy:\",accuracy_score(y_test , y_pred1))\nprint(\"Precision:\",precision_score(y_test,y_pred1))\nprint(\"Recall:\",recall_score(y_test,y_pred1))\nprint(\"F1-score:\",f1_score(y_test,y_pred1))","db43dc40":"Now we had outlier in our data set.To treat them we will be replacing all those data points whole value less than equal to LL=(Q1-1.5*IQR) and greater than equal to UL=(Q3+1.5*IQR) by LL and UL.This is known as Capping Method","2e85b945":"Now we can see that now we do not have any outlier in data set.","5b57fcd5":"I will be doing the Model Deployment Soon using Flask and Heroku.\nIf you think there needs some impovement let me know in comment section.","32bb2480":"# **Exploratory Data Analysis.**","08d2d547":"From the pair plot we can see that.\n1. Age and Experience both have high correlation which each other. \n2. Income,CCAvg,Mortage show positive skewness.","7fc8c678":"The data contain some negative Experience data point as we see min is -3.\nLet us see the count of negative data points.","6b663a21":"As we can see Family and Education are Ordinal Variables so we do label endcoing.","6d80c0eb":"Personal Loan feature which is target variable has imblance data set which have more count of persoanl loan 0 than personal loan 1.  i.e. 9:1.\nWe Use Over sampling in Feature Engineering to make it balance data set.","d13a622e":"Exploratory Data Analysis is a approach to analyzing the data set to summarize their main characteristics  often with visual method . Here Visualizing means we will plot different charts and graph to get valuable information about  the data set. Every Machine project starts with Eda and Exploratory data analysis is the most important part of Machine learning project.","5eec9454":"Importing the data set and renaming the columns.","928f8405":"As we can see Age and Experience both have high correlation between each other. We have to remove any one of them in feature Engineering.","e508c7c9":"1. 1. # **Feature Enginerring**","9c766f97":"Let us try to use Catboost Classifier Algorithm","756d502e":"Conclusion\nAs we can see by Using CatBoost Classifier we have gained 99% Accuracy. I think Catboost Classifier give us maximum accuracy.\n","a09f4c57":" We have to make dataset balance. We will be using SMOTHE Method ","86fb65a9":"From look at above Heatmap\nWe can consider to remove Experience,ID & ZIPCODE.","5e7d7643":"Income, CCAvg , Mortgage have Outlier we will deal with this in Feature Engineering.","f44c3a34":"Feature engineering is the process of using domain knowledge to extract features from raw data set. Having a good knowledge of feature engineering  will help us get more accurate representation of underlying structure of the data and help us to improve the performance of machine learning algorithm. In Feature Engineering we have several topics like how to deal with missing values, how to deal with outliers , how do we convert categorical variables to numerical values so that model can read values easily.","22de3b7e":"To see all the summary of data let us make one DataFrame that shows feature,dtype,Null value count,Unique count, Unique item.","174d0349":"Now we don't have any negative data points in Experience.","4a6a3d1a":"Form the above histogram chart we can see that.\n1. Family size of 3 and 4 members are tending to take Personal Loan.\n2. Customer that belong to Education category 2 and 3 i.e. Graduate and Professional have taken more Persoanl Loan then the Undergraduate class.\n3. Customer who does not have Security Account have taken Personal Loan .\n4. Customer who does not CDAcount in this higher number of customer don't have Personal Loan . We can see that customer who have CDAcount most of them had taken Personal Loan. Here CDAccount means Certificate of Deposit.\n5. Customer how use Internet Bank service also have higher count of Personal Loan then those who does not use Online Service.\n6. Customer who don't have excess to Credit Card for Universal Bank are more likely to apply for PersonaL Loan.\n\n"}}