{"cell_type":{"3159278b":"code","712e2ba3":"code","431b7401":"code","6ec463ae":"code","44155e5d":"code","f5f39b73":"code","8e1a3cc0":"code","9abe64f3":"code","8d559da4":"code","93d1ac4f":"code","c3b17788":"code","56e57648":"code","4cd55229":"code","e56bf044":"code","b803e42c":"code","1802dce1":"code","02236570":"code","7b5e7a4c":"code","7a946273":"code","69f05173":"markdown","043d3b0b":"markdown","f8b3819f":"markdown","4482fe02":"markdown","49e376cc":"markdown","6f744773":"markdown","0b44a485":"markdown","2713ea51":"markdown","4ad1535f":"markdown","cf760893":"markdown","2c964b3a":"markdown","8732f7a4":"markdown","38dfc1d5":"markdown","ddc9314a":"markdown","02bcc41a":"markdown","57bafe26":"markdown","296a4ee8":"markdown","444ccf36":"markdown","1a6215eb":"markdown","974a0966":"markdown","a54e020c":"markdown","bf0bb5c8":"markdown","d2decc90":"markdown","e64c3434":"markdown","fb13b941":"markdown","eaf46986":"markdown","39a93d6d":"markdown","771a0444":"markdown"},"source":{"3159278b":"import os\nimport random\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.optimizers import *\nfrom keras.utils.vis_utils import plot_model\n\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.metrics import confusion_matrix, classification_report","712e2ba3":"### Cargar training ###\ntrain_df = pd.read_csv('\/kaggle\/input\/vida-util\/1. Training.txt', sep=\" \", header=None)\n\ntrain_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n\ntrain_df.columns = ['id',\n                    'ciclo',\n                    'p1', 'p2', 'p3',\n                    's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10',\n                    's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20',\n                    's21']\n\ntrain_df = train_df.sort_values(['id','ciclo'])\n\nprint('Cantidad de m\u00e1quinas (ids \u00fanicos):', len(train_df.id.unique()))\nprint('Cantidad de filas y columnas:', train_df.shape)\nprint('Los primeros 5 registros de Training:')\ntrain_df.head(5)","431b7401":"### Cargar testing ###\ntest_df = pd.read_csv('\/kaggle\/input\/vida-util\/2. Testing.txt', sep=\" \", header=None)\n\ntest_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n\ntest_df.columns = ['id',\n                   'ciclo',\n                   'p1', 'p2', 'p3',\n                   's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10',\n                   's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20',\n                   's21']\n\ntest_df = test_df.sort_values(['id','ciclo'])\n\nprint('Cantidad de m\u00e1quinas (ids \u00fanicos):', len(test_df.id.unique()))\nprint('Cantidad de filas y columnas:', test_df.shape)\nprint('Los primeros 5 registros de Testing:')\ntest_df.head(5)","6ec463ae":"### Cargar testing - resultados ###\ntest_vu_df = pd.read_csv('\/kaggle\/input\/vida-util\/2. Testing - VU.txt', sep=\" \", header=None)\n\ntest_vu_df.drop(test_vu_df.columns[[1]], axis=1, inplace=True)\ntest_vu_df.columns = ['vu']\ntest_vu_df = test_vu_df.set_index(test_vu_df.index + 1)\n\nprint('Cantidad de filas y columnas:', test_vu_df.shape)\nprint('Los primeros 5 registros de Testing - Resultados Vida Util:')\ntest_vu_df.head(5)","44155e5d":"plt.figure(figsize=(20,6))\ntrain_df.id.value_counts().plot.bar()\nprint(\"Mediana:\", train_df.id.value_counts().mean())\nprint(\"M\u00e1ximo:\", train_df.id.value_counts().max())\nprint(\"M\u00ednimo:\", train_df.id.value_counts().min())","f5f39b73":"CONST_MAQUINA_A_VISUALIZAR = 37\n\nmaquina_id = train_df[train_df['id'] == CONST_MAQUINA_A_VISUALIZAR]\nmaquina_id[train_df.columns[2:]].plot(subplots=True, sharex=True, figsize=(20,30))","8e1a3cc0":"### Funci\u00f3n para iniciar con la misma semilla todos los valores aleatorios ###\ndef set_seed(seed):\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\nset_seed(40)\n\n### Funci\u00f3n para escalar datos ###\n### Convierte cualquier rango de datos a valores comprendidos entre 0..1 ###\ndef escalar(df):\n    return (df - df.min()) \/ (df.max() - df.min())\n    \n","9abe64f3":"CONST_CATEGORIA_UMBRAL_2 = 15\nCONST_CATEGORIA_UMBRAL_1 = 45\n\n### DATOS TRAINING ###\n\n### Calcular ciclos hasta vida util ###\n### Se crea una nueva columna cvu con los ciclos que faltan hasta la falla ###\ntrain_df['cvu'] = train_df.groupby(['id'])['ciclo'].transform(max) - train_df['ciclo']\n\n### Agregar Categoria 0\/1\/2 ###\ntrain_df['cat1'] = np.where(train_df['cvu'] <= CONST_CATEGORIA_UMBRAL_1, 1, 0 )\ntrain_df['cat2'] = train_df['cat1']\ntrain_df.loc[train_df['cvu'] <= CONST_CATEGORIA_UMBRAL_2, 'cat2'] = 2\n\n### Escalar ###\nfor col in train_df.columns:\n    if (col[0] == 'p') or (col[0] == 's'):\n        train_df[col] = escalar(train_df[col])\n\ntrain_df = train_df.dropna(axis=1)\n#train_df.head(50)\n\n\n### DATOS TESTING ###\n\n### Calcular ciclos hasta vida util ###\n### Se crea una nueva columna cvu con los ciclos que faltan hasta la falla ###\ntest_vu_df['max'] = test_df.groupby('id')['ciclo'].max() + test_vu_df['vu']\ntest_df['cvu'] = [test_vu_df['max'][i] for i in test_df.id] - test_df['ciclo']\n\n### Agregar Categoria 0\/1\/2 ###\ntest_df['cat1'] = np.where(test_df['cvu'] <= CONST_CATEGORIA_UMBRAL_1, 1, 0 )\ntest_df['cat2'] = test_df['cat1']\ntest_df.loc[test_df['cvu'] <= CONST_CATEGORIA_UMBRAL_2, 'cat2'] = 2\n\n### Escalar ###\nfor col in test_df.columns:\n    if (col[0] == 'p') or (col[0] == 's'):\n        test_df[col] = escalar(test_df[col])\n        \ntest_df = test_df.dropna(axis=1)\n#test_df.head()","8d559da4":"CONST_SECUENCIA_LONGITUD = 50\n\n### Funci\u00f3n para armar ventanas\/secuencias ###\ndef gen_sequence(id_df, seq_cols):\n    data_matrix = id_df[seq_cols].values\n    num_elements = data_matrix.shape[0]\n    for start, stop in zip(range(0, num_elements-CONST_SECUENCIA_LONGITUD), range(CONST_SECUENCIA_LONGITUD, num_elements)):\n        yield data_matrix[start:stop, :]\n        \ndef gen_labels(id_df, label):\n    data_matrix = id_df[label].values\n    num_elements = data_matrix.shape[0]\n    return data_matrix[CONST_SECUENCIA_LONGITUD:num_elements, :]\n\n\n### Secuenciar las columnas de par\u00e1metros y sensores ###\nsecuencia_cols = []\n\nfor col in train_df.columns:\n    if (col[0] == 'p') or (col[0] == 's'):\n        secuencia_cols.append(col)\n        \n# print(secuencia_cols)","93d1ac4f":"### Generamos las entradas del algoritmo (X), training y test ###\nx_train, x_test = [], []\n\nfor maquina_id in train_df.id.unique():\n    for secuencia in gen_sequence(train_df[train_df.id==maquina_id], secuencia_cols):\n        x_train.append(secuencia)\n    for secuencia in gen_sequence(test_df[test_df.id==maquina_id], secuencia_cols):\n        x_test.append(secuencia)\n\nx_train = np.asarray(x_train)\nx_test = np.asarray(x_test)\n\n#print(\"x_train shape:\", x_train.shape)\n#print(\"x_test shape:\", x_test.shape)","c3b17788":"### Generamos las salidas del algoritmo (Y), training y test ###\ny_train, y_test = [], []\n\nfor maquina_id in train_df.id.unique():\n    for etiqueta in gen_labels(train_df[train_df.id==maquina_id], ['cat2']):\n        y_train.append(etiqueta)\n    for label in gen_labels(test_df[test_df.id==maquina_id], ['cat2']):\n        y_test.append(label)\n    \ny_train = np.asarray(y_train).reshape(-1,1)\ny_test = np.asarray(y_test).reshape(-1,1)\n\n#print(\"y_train shape:\", y_train.shape)\n#print(\"y_test shape:\", y_test.shape)\n\n### Convierte una columna de datos enteros en una matriz binaria, esto es una matriz con tantas columnas como n\u00fameros enteros hay ###\ny_train = tf.keras.utils.to_categorical(y_train)\ny_test = tf.keras.utils.to_categorical(y_test)\n\n#print(y_train.shape)\n#print(y_test.shape)","56e57648":"CONST_MAQUINA_A_VISUALIZAR_2D = 37\n\ndef rec_plot(s, eps=0.10, steps=10):\n    d = pdist(s[:,None])\n    d = np.floor(d\/eps)\n    d[d>steps] = steps\n    sf = squareform(d)\n    return sf\n\nplt.figure(figsize=(20,20))\n\nfor i in range(0,17):\n    plt.subplot(6, 3, i+1)    \n    rec = rec_plot(x_train[CONST_MAQUINA_A_VISUALIZAR_2D,:,i])\n    plt.imshow(rec)\n    plt.title(secuencia_cols[i])\n    \nplt.show()","4cd55229":"### Transformar los datos en im\u00e1genes, training y test ###\n\nx_train_img = np.apply_along_axis(rec_plot, 1, x_train).astype('float16')\n\nx_test_img = np.apply_along_axis(rec_plot, 1, x_test).astype('float16')\n\n#print(x_train_img.shape)\n#print(x_test_img.shape)","e56bf044":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 17)))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n#print(model.summary())","b803e42c":"plot_model(model, show_shapes=True, show_layer_names=True)","1802dce1":"es = EarlyStopping(monitor='val_accuracy', mode='auto', restore_best_weights=True, verbose=1, patience=20)\n\nhistory = model.fit(x_train_img, y_train, batch_size=512, epochs=50, callbacks=[es], validation_split=0.1, verbose=2)","02236570":"#-----------------------------------------------------------\n# Traer resultados del entrenamiento\n#-----------------------------------------------------------\nacc      = history.history['accuracy']\nloss     = history.history['loss']\nval_acc  = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nepochs   = range(len(acc)) # Get number of epochs\n#------------------------------------------------\n# Graficar Training (azul) & Validaci\u00f3n (naranja)\n#------------------------------------------------\nplt.plot  (epochs, acc)\nplt.plot  (epochs, val_acc)\nplt.title ('Acurracy: Training (azul) & Validacion (naranja)')\nplt.figure()\n\n#------------------------------------------------------\n# Graficar Loss: Training (azul) & Validaci\u00f3n (naranja)\n#------------------------------------------------------\nplt.plot  (epochs, loss )\nplt.plot  (epochs, val_loss )\nplt.title ('Loss: Training (azul) & Validacion (naranja)')","7b5e7a4c":"model.evaluate(x_test_img, y_test, verbose=2)","7a946273":"def plot_confusion_matrix(cm, classes, title='Matriz de Confusion', cmap=plt.cm.Blues):\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=25)\n    #plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=15)\n    plt.yticks(tick_marks, classes, fontsize=15)\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 14)\n    plt.ylabel('categor\u00eda Real', fontsize=20)\n    plt.xlabel('categor\u00eda Predecida', fontsize=20)\n    \n    \n#cnf_matrix = confusion_matrix(np.where(y_test != 0)[1], model.predict_classes(x_test_img))\ncnf_matrix = confusion_matrix(np.where(y_test != 0)[1], np.argmax(model.predict(x_test_img), axis=-1))\nplt.figure(figsize=(7,7))\nplot_confusion_matrix(cnf_matrix, classes=np.unique(np.where(y_test != 0)[1]), title=\"Resultado\")\nplt.show()","69f05173":"# \u00a1Bienvenidos!\nKaggle es la plataforma de Data Science y Machine Learning m\u00e1s grande del mundo, es excelente para aprender y realizar proyectos colaborativos.\nY si bien ofrece de manera gratuita este entorno (Jupyter Notebooks) y procesamiento de datos (CPU+GPU), lo m\u00e1s sorprendente es la comunidad de usuario, que aportan DataSets reales, C\u00f3digo, Algoritmos, Competencias con Premios y Conocimientos para todos los niveles y expectativas de uso, seas una persona nueva que est\u00e1 aprendiendo, un entusiasta del mundo corporativo en data science o un investigador con a\u00f1os de trayectoria en machine learning.\n\nVideo: [\u00bfQu\u00e9 es Kaggle?](https:\/\/www.youtube.com\/watch?v=NhHTWGIglRI)\n\nVideo: [Kaggle - Todo lo que debes saber](https:\/\/www.youtube.com\/watch?v=Qt-pN9CqJeo)","043d3b0b":"# 6. Evaluar\nHabiendo entrenado el modelos con los datos de training, el siguiente paso es evaluar su precisi\u00f3n con datos que el modelo NO HAYA visto nunca hasta ahora.\n\nPor eso recurrimos a los datos de test (recuerden los archivos \"2. Testing.txt\" y \"2. Testing - VU.txt\").","f8b3819f":"## 3.2 Evoluci\u00f3n de los par\u00e1metros y sensores\nCon el c\u00f3digo que sigue, vamos a mostrar visualmente los datos de los sensores. \n\nEl ejemplo usa la m\u00e1quina 37, pero puedes cambiar a cualquier otra en la primera l\u00ednea donde dice CONST_MAQUINA_A_VISUALIZAR, por ejemplo para ver la m\u00e1quina 55 se podr\u00eda usar:\n\n> CONST_MAQUINA_A_VISUALIZAR = 55\n\nIMPORTANTE: observe como cada sensor tiene una escala diferente y tambi\u00e9n observe detenidamente como hacia el final de los gr\u00e1ficos, el comportamiento de los sensores s1..s21 cambia la pendiente indicando su deterioro.","4482fe02":"## 2.3 Datos de Testing ","49e376cc":"# RECUERDEN SIEMPRE: EL MODELO VA A SER TAN BUENO C\u00d3MO LOS DATOS USADOS","6f744773":"# 1. Conocer los datos\nTe invito a que bajes los datos con los que vamos a trabajar, yendo a *Toggle sidebar visibility*, luego descargando los 3 archivos con extensi\u00f3n *.txt*. \n\nPodes explorarlos con el visor nativo de Kaggle, o abrirlos con Excel y separando los campos en columnas, o con alguna aplicaci\u00f3n (por ejemplo Notepad++)\n\n![image.png](attachment:498566f4-31cb-43de-8880-10b8ac83ffe1.png)\n\n\n\n\n## Datasets\n\n### Archivo: *1. Training.txt*\nEste archivo contiene los datos con los cuales entrenaremos el algoritmo.\n\nEs un archivo de tipo CSV (d\u00f3nde los datos est\u00e1n separados por un espacio).\n\nHay 20.631 l\u00edneas (o filas o registros).\n\nPor cada fila hay 26 campos (o columnas o atributos). \n\n\nEstructura de los campos:\n* id: ID de m\u00e1quina (de la 1 a la 100)\n* ciclo: Ciclo operativo de monitoreo (el \u00faltimo ciclo ES EL CICLO d\u00f3nde la m\u00e1quina se rompe)\n* p1..p3: los 3 par\u00e1metros de configuraci\u00f3n que tiene la m\u00e1quina en ese ciclo de monitoreo \n* s1..s21: los datos registrados por cada uno de los 21 sensores en ese ciclo de monitoreo \n\n![image.png](attachment:ee944c54-2d99-4f9c-af70-508a1bf70ad5.png)\n\n\n### Archivo: *2. Testing.txt*\nEste archivo contiene los datos con los cuales *probaremos* la precisi\u00f3n del algoritmo una vez entrenado.\n\nHay 13.096 registros (aprox 40% del total de datos se separ\u00f3 para probar el algoritmo).\n\nIMPORTANTE: estos casos son diferentes a los de training, tienen menos ciclos, hay un gap (FALTANTE) de datos\n\nLos datos que faltan, son sobre los que se tiene que predecir\n\nEl ciclo de da\u00f1o est\u00e1 indicado en el pr\u00f3ximo archivo\n\n\n### Archivo: *2. Testing - VU.txt*\nEste archivo contiene el ciclo en el cual fall\u00f3 efectivamente cada una de las m\u00e1quinas de Testing\n\nHay 100 registros que corresponden a las 100 m\u00e1quinas de prueba.","0b44a485":"IMPORTANTE: no te olvides de hacer Shift+Enter en cada bloque de c\u00f3digo, para ir ejecut\u00e1ndolo.\nVas a ver c\u00f3mo la barra superior cambia de estado.\n\n\n![image.png](attachment:d4784f17-6b53-468d-81b5-2c2545a9f0c8.png)\n\nY espere a que termine de procesar para pasar al siguiente bloque de c\u00f3digo.\n\n\n\nProcesando...\n\n![image.png](attachment:0e76c381-3df1-41aa-8b32-84410ee64a28.png)\n\n\n\nListo!\n\n![image.png](attachment:bca4862a-e2bb-4503-83e7-4a66c999623d.png)\n\n","2713ea51":"# Estructura del Ejercicio\nVamos a seguir los siguientes pasos:\n1. Conocer los datos\n2. Cargar los datos\n3. Preparar los datos (y conocerlos m\u00e1s)\n4. Definir el modelo\n5. Compilar el modelo\n6. Evaluar el modelo\n\nEstos pasos son basicamente los mismos que se usan la mayor\u00eda de los proyectos de machine learning.\n\nNo estamos considerando un bloque adicional de pasos, que ser\u00eda por ejemplo armar una aplicaci\u00f3n web o una plataforma que ejecute todo esto de manera autom\u00e1tica y pueda predecir en tiempo real una falla.\n\nEn los proyectos de Machine Learning, con estos pasos ya consideramos la viabilidad de la POC\/MVP y podemos gestionar la construcci\u00f3n de una aplicacion.\n","4ad1535f":"# 5. Entrenar\nYa contamos con los datos preparados y el modelo armado.\n\nEl siguiente paso es entrenar el modelo con los datos de entrada (training data, recuerden el archivo \"1. Training.txt\").\n\nDefinimos 50 epochs (ejecuciones de entrenamiento). Tambi\u00e9n definimos una condici\u00f3n de escape para terminar antes si no hay progreso significativo (en t\u00e9rminos coloquiales: una paciencia de 20 epochs sin mejoras considerables en la precisi\u00f3n o divergencia, se elige la \u00faltima mejor).\n\n**Este modelo deber\u00eda entrenarse aproximadamente entre 20 y 40 epochs tomando las consideraciones previas. Como cada epoch insume 20 segundos aproximadamente, debemos esperar entre 7 y 15 minutos a que termine de entrenar el modelo.**\n\n**IMPORTANTE: Si se activa GPU, el tiempo total se reduce a 40 segundos o menos**","cf760893":"## 2.2 Datos de Training ","2c964b3a":"# Primeros Pasos en Jupyter Notebooks\nEstas visualizando un **Notebook**:\n* Los Notebooks se componen de **Celdas**\n* Una Celda puede **ser C\u00f3digo (code)** o **Comentario (markdown)**\n* Esta celda es de tipo comentario\n* Cuando veas un bloque con c\u00f3digo, te paras sobre el misma y vas a tener que ejecutarla usando **Shift+Enter** \n* Respet\u00e1 siempre el orden de ejecutar el c\u00f3digo, en orden secuencial desde arriba hacia abajo de la pantalla. Las computadoras son lineales y no les gusta cuando mezclamos el orden de pasos.\n* Si sos curioso, las celdas se editan haciendo **doble-click**. No te preocupes si haces cambios y algo se rompe, podes cerrar el navegaodor y volver a ingresar desde el inicio\n","8732f7a4":"## 3.4 Categorizar los datos\n\nEs una t\u00e9cnica muy conocida usar CATEGOR\u00cdAS en vez de valores lineales para mejorar la precisi\u00f3n de los resultados.\n\n*Por ejemplo, si estuvi\u00e9semos entrenando un algoritmo para el mercado inmobiliario, en vez de tener casas de 80m2, 50m2, 180m2, 75m2, 120m2, etc, podr\u00edamos categorizarlas en: muy chicas, chicas, medianas, grandes, muy grandes. Este agrupamiento reduce los ruidos generados por valores lineales y la salida se termina ajustando mejor a la entrada.*\n\nPor eso, vamos a CATEGORIZAR cada uno de los ciclos para cada uno de los sensores, agregando una columna que indica cuanto falta hasta la pr\u00f3xima falla:\n* Categor\u00eda 2: corresponde al ciclo de la falla y sus 15 ciclos previos\n* Categor\u00eda 1: entre 16 y 45 ciclos previos a la falla\n* Categor\u00eda 0: faltan 45 o m\u00e1s ciclos hasta la falla","38dfc1d5":"## 2.4 Datos de Testing - Vida Util ","ddc9314a":"Con este bloque de c\u00f3digo leemos el primer archivo, indicando que un es archivo separado por espacios. Como la primer fila no tiene el nombre de las columnas, los definimos con c\u00f3digo para que sea m\u00e1s f\u00e1cil manipularlas luego.","02bcc41a":"## 3.5 Secuenciar los datos (Data Augmentation)\n\nVamos a crear SECUENCIAS de datos con Ventanas Corredizas de los ciclos. Esta tambi\u00e9n es una t\u00e9cnica muy conocida para aumentar los datos.\n\nSi 1 m\u00e1quina tiene 1 set de datos con 192 ciclos, creamos ventanas de 50 ciclos de duraci\u00f3n. Entonces AUMENTAMOS los datos 143 veces.\n\nEjemplo antes de aumentar los datos:\n* ventana 1: ciclo 1..192\n\nEjemplo luego de aumentar los datos:\n* ventana 1: ciclo 1..50\n* ventana 2: ciclo 2..51\n* ventana 3: ciclo 3..52\n* ...\n* ventana 143: ciclo 143..192","57bafe26":"Note como se dispara el consumo de procesador y memoria en el notebook\n\n![image.png](attachment:83d5fe65-6460-48aa-8a6e-17f66e88ff18.png)","296a4ee8":"# 2. Cargar datos","444ccf36":"## 3.3 Escalar los datos\nEn los sensores previos, se puede observar que hay mucha dispersi\u00f3n en las escalas. Algunos sensores trabajan en el rango 23.00..24.00, otros en 38.5..39.5, otros en 520..525, 9030..9050, etc.\n\nLo que vamos a hacer es ajustar la ESCALA en todos los datos a un rango de n\u00fameros reales, los decimales comprendido entre 0 y 1. Esto tiene la particularidad de equalizar las entradas, funciona mejor si los datos son dispersos y al hacer operaciones matem\u00e1ticas evitamos el desbordamiento de datos (overflow).\n\nEn algunos algoritmos, no es este ejercicio el caso, en la salida se podr\u00eda reconstruir el valor de salida a la escala inicial de entrada si fuese necesario.\n","1a6215eb":"# 4.0 Definir el Modelo\nA continuaci\u00f3n definimos el modelo y luego vamos a visualizarlo.\n\nImportante: notar c\u00f3mo el modelo se define con muy pocas lineas de c\u00f3digo en comparaci\u00f3n a todo el esfuerzo que se hizo para preparar los datos. Tambi\u00e9n notar la cantidad de hiper-par\u00e1metros que se est\u00e1n definiendo en cada una de las layers. Y las funciones de activaci\u00f3n usadas: relu y softmax.","974a0966":"El valor importante es **accuracy**, donde nos indica la precisi\u00f3n del modelo para predecir datos nuevos. Como es un valor entre 0..1 recuerde multiplicar por 100 (ej. accuracy: 0.8995 = precisi\u00f3n 89,95%).\n","a54e020c":"# 3. Preparar los datos (y conocerlos m\u00e1s)","bf0bb5c8":"## 3.7 Preparar todo\nEsto es mucho c\u00f3digo, as\u00ed que para no complicarnos hacemos varios pasos juntos.\n\nPueden ejecutarlos sin tratar de entender la programaci\u00f3n de los 4 conceptos anteriores para reestructurar y aumentar los datos.\n","d2decc90":"# Objetivo\n## Mantenimiento Predictivo: Estimaci\u00f3n de Vida \u00datil \nPara este ejercicio pr\u00e1ctico, contamos con datos reales generados por un grupo de motores, esto incluye los par\u00e1metros operativos de los mismos y los sensores internos que contiene.\n\nVamos a leer los datos, conocerlos, prepararlos y mediante un algoritmo de machine learning (\u00a1y algunas t\u00e9cnicas intermedias!) vamos a aprender a enteder las se\u00f1ales de estos sensores y predecir con una precisi\u00f3n mayor al 80% el potencial momento de falla de cada motor.\n\n## Escenario\nEn este escenario estamos trabajando con un tipo de motor que tiene 3 configuraciones operativas y un conjunto de 21 sensores de monitoreo.\n\nAsi mismo, contamos con un conjunto de datos de 100 motores.\n\n\n## La Pregunta de Negocio que nos hacemos es ...\n\u00bfCu\u00e1nto tiempo falta hasta la pr\u00f3xima falla?\n\nY vamos a poder responderla bas\u00e1ndonos en tres categor\u00edas que equivalen a: no hay evidencia que vaya a ocurrir una falla, en mediano plazo, en corto plazo.","e64c3434":"## 3.1 Histograma de Ciclos hasta  Falla (visualizando los datos de training)\nRecordar que estamos viendo los datos de training y el \u00faltimo ciclo es el de la falla.\n\nEn este gr\u00e1fico podemos visualizar que la m\u00e1quina que tuvo la vida \u00fatil m\u00e1s prolongada fue la 69 reportando un total 362 ciclos.\n\nOpuestamente, la m\u00e1quina con menor vida \u00fatil fue la 39 con 128 ciclos.","fb13b941":"## 2.1 Preparar el ambiente de trabajo\nEstos son *imports* de c\u00f3digo, indican que frameworks y librer\u00edas se van a estar usando\nTodo este ejemplo lo hacemos en lenguaje Python con TensorFlow (es la librer\u00eda de Machine Learning de Google)\n\nSi te genera m\u00e1s curiosidad:\n* OS: para acceder a archivos\n* numpy\n* pandas\n* matPlotLib\n* TensorFlor\n* Keras","eaf46986":"## 3.6 Espectrogramas\nVamos a convertir los datos que est\u00e1n expresados como una serie de tiempo (1 dimensi\u00f3n: Tiempo, todos de 50 ciclos de duraci\u00f3n) en una imagen (2 dimensiones: Frecuencia y Tiempo).\n\nEsto tiene PROS y CONTRAS, pero es interesante ver c\u00f3mo podemos traducir datos 1D en 2D.","39a93d6d":"# 7. Analizar\nY lo \u00faltimo que hacemos es crear una matriz de confusi\u00f3n. \n\nEsta matriz nos permite visualizar la precisi\u00f3n del modelo, entre el VALOR PREDECIDO por el algoritmo y el VALOR REAL.\n\nEs muy importante para evitar falsos positivos y falsos negativos.\n\nEn este algoritmo que entrenamos, tenemos una muy buena precisi\u00f3n para detectar una falla en los \u00faltimos ciclos (predecido=2 y real=2).\n\n**Con lo cual cumplimos satisfactoriamente la pregunta del negocio que nos hicimos al comenzar el ejercicio.**","771a0444":"**Observe c\u00f3mo la primer layer es de 42.500 neuronas artificiales (50 x 50 x 17), el modelo va sintetizando la informaci\u00f3n en las capas intermedias y la \u00faltima layer s\u00f3lo tiene 3 neuronas: cada una responde con una funci\u00f3n de activaci\u00f3n SoftMax y corresponde a cada una de las categor\u00edas definidas (categor\u00eda 0, categor\u00eda 1, categor\u00eda 2).**"}}