{"cell_type":{"d4bee0ad":"code","02123980":"code","f7db4fd5":"code","10861b91":"code","77333453":"code","9fc231e1":"code","fcd970df":"code","34052ff4":"code","df6bd665":"code","bfa6a68c":"code","9d089885":"code","36ffd0f8":"code","2ea5392d":"code","d0e9ffcd":"code","161ba95c":"code","37856859":"code","336d5fef":"code","72e247d7":"code","929609be":"code","0be91ac7":"code","f02a4f2f":"code","603cb5fa":"code","2220773c":"code","6b7a0331":"markdown","c2b46c6b":"markdown","20438bed":"markdown"},"source":{"d4bee0ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","02123980":"import h5py\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport math\nfrom tensorflow.python.framework import ops\n","f7db4fd5":"training_data = h5py.File('..\/input\/train_catvnoncat.h5','r')\ntesting_data = h5py.File('..\/input\/test_catvnoncat.h5','r')","10861b91":"train_data = np.array(training_data)\ntest_data = np.array(testing_data)","77333453":"print(\"Training data files \",train_data)\nprint(\"Testing data files \" ,test_data)","9fc231e1":"training_data['train_set_x']","fcd970df":"X_train_orig = np.array(training_data['train_set_x'][:])\nY_train_orig = np.array(training_data['train_set_y'][:])\nX_test_orig = np.array(testing_data['test_set_x'][:])\nY_test_orig = np.array(testing_data['test_set_y'][:])\nclasses = np.array(testing_data['list_classes'])","34052ff4":"X_train = X_train_orig.reshape(X_train_orig.shape[0], -1).T\nX_test = X_test_orig.reshape(X_test_orig.shape[0],-1).T\nY_train = Y_train_orig.reshape((1,Y_train_orig.shape[0]))\nY_test = Y_test_orig.reshape((1,Y_test_orig.shape[0]))","df6bd665":"print(\"shape of training images data : \", X_train.shape)\nprint(\"shape of testing images data : \", X_test.shape)\nprint(\"shape of training labels images data : \", Y_train.shape)\nprint(\"shape of testing  labels images data : \", Y_test.shape)\nprint(\"Total  Classes = \", len(classes), \" = \", classes)","bfa6a68c":"index = 11\nplt.imshow(X_train_orig[index])\nprint(\"This is \", classes[Y_train_orig[index]])","9d089885":"# INITIALIZING THE PARAMETERS\nregularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\ndef initialize_parameters():\n    W1 = tf.get_variable(name='W1',shape=[25,12288],initializer=tf.contrib.layers.xavier_initializer(seed =1),regularizer=regularizer)\n    b1 = tf.get_variable(name='b1',shape=[25,1],initializer=tf.zeros_initializer())\n    W2 = tf.get_variable(name='W2',shape=[12,25],initializer=tf.contrib.layers.xavier_initializer(seed =1),regularizer=regularizer)\n    b2 = tf.get_variable(name='b2',shape=[12,1],initializer=tf.zeros_initializer())\n    W3 = tf.get_variable(name='W3',shape=[1,12],initializer=tf.contrib.layers.xavier_initializer(seed =1),regularizer=regularizer)\n    b3 = tf.get_variable(name='b3',shape=[1,1],initializer=tf.zeros_initializer())\n    parameters = {'W1':W1,\n                  'b1':b1,\n                  'W2':W2,\n                  'b2':b2,\n                  'W3':W3,\n                  'b3':b3}\n    return parameters\n    ","36ffd0f8":"#SIGMOID FUNCTION\ndef sigmoid(Z):\n    x = tf.placeholder(tf.float32,name='x')\n    sigmoid = tf.sigmoid(x)\n    with tf.Session() as sess:\n        result = sess.run(sigmoid,feed_dict = {x:Z})\n    return result","2ea5392d":"#FORWARD PROPOGATION\ndef forward_propagation(X,parameters):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    Z1 = tf.add(tf.matmul(W1,X),b1)\n    A1 = tf.nn.relu(Z1)\n    Z2 = tf.add(tf.matmul(W2,A1),b2)\n    A2 = tf.nn.relu(Z2)\n    Z3 = tf.add(tf.matmul(W3,A2),b3)\n    \n    return Z3\n    ","d0e9ffcd":"def create_placeholders(n_x,n_h):\n    X = tf.placeholder(tf.float32,shape=[n_x,None],name='X')\n    Y = tf.placeholder(tf.float32,shape = [n_h,None],name= 'Y')\n    return X,Y","161ba95c":"def compute_cost(Z3, Y):\n    regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n    \n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n   \n    cost_1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n    reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n    reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n    cost = tf.reduce_mean(cost_1+reg_term)\n    \n    return cost","37856859":"##BACKPROPOGATION + WHOLE MODEL\ndef model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n          num_epochs = 2000, print_cost = True):\n    \"\"\"\n    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n    \n    Arguments:\n    X_train -- training set, of shape (input size = 12288, number of training examples = 209)\n    Y_train -- test set, of shape (output size = 1, number of training examples = 209)\n    X_test -- training set, of shape (input size = 12288, number of training examples = 50)\n    Y_test -- test set, of shape (output size = 1, number of test examples = 50)\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    print_cost -- True to print the cost every 100 epochs\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep consistent results\n    seed = 3                                          # to keep consistent results\n    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n\n    X, Y = create_placeholders(n_x, n_y)\n\n    parameters = initialize_parameters()\n    \n    Z3 = forward_propagation(X, parameters)\n    \n    cost = compute_cost(Z3, Y)\n   \n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    \n    init = tf.global_variables_initializer()\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0. # Defines a cost related to an epoch\n            \n            seed = seed + 1\n            #minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            \n           \n            _ , Cost = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train})\n            if print_cost == True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, Cost))\n            if print_cost == True and epoch % 5 == 0:\n                costs.append(Cost)\n                \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print(\"Parameters have been trained!\")\n\n    \n        return parameters\n","336d5fef":"parameters = model(X_train, Y_train, X_test, Y_test)","72e247d7":"def forward_propagation_for_predict(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n                  the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    \n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3'] \n                                                           # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n    A3 = tf.nn.sigmoid(Z3)\n    return A3\n    ","929609be":"def predict(X, parameters):\n    \n    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n    \n    params = {\"W1\": W1,\n              \"b1\": b1,\n              \"W2\": W2,\n              \"b2\": b2,\n              \"W3\": W3,\n              \"b3\": b3}\n    \n    x = tf.placeholder(\"float\", [12288, X.shape[1]])\n    \n    A3 = forward_propagation_for_predict(x, params)\n    \n    \n    sess = tf.Session()\n    prediction = sess.run(A3, feed_dict = {x: X})\n        \n    return prediction\n\n    ","0be91ac7":"def accuracy(label,pred):\n    for i in range(0, label.shape[1]):\n        if prediction[0,i] > 0.5:\n            prediction[0,i] = 1\n        else:\n            prediction[0,i] = 0\n\n    print(\"Accuracy: \"  + str(np.sum((prediction == label)\/(label.shape[1]))))\n    \n        ","f02a4f2f":"prediction = predict(X_test, parameters)","603cb5fa":"for i in range(0, prediction.shape[1]):\n    if prediction[0,i] > 0.5:\n        prediction[0,i] = 1\n    else:\n        prediction[0,i] = 0\naccuracy(Y_test,prediction)","2220773c":"training_prediction = predict(X_train, parameters)\nfor i in range(0, training_prediction.shape[1]):\n    if training_prediction[0,i] > 0.5:\n        training_prediction[0,i] = 1\n    else:\n        training_prediction[0,i] = 0\nfor i in range(0, 209):\n        if training_prediction[0,i] > 0.5:\n            training_prediction[0,i] = 1\n        else:\n            training_prediction[0,i] = 0\nprint(\" Training Accuracy: \"  + str(np.sum((training_prediction == Y_train)\/209)))","6b7a0331":"**GETTING DATA FROM STRORED FILES (h5py) FORMAT**","c2b46c6b":"**IMPORT  PACKAGES**","20438bed":"***DETAILS OF DATA***\n"}}