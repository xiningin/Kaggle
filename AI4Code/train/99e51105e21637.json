{"cell_type":{"f66ce62e":"code","20ad77f9":"code","2e792d1a":"code","d8b420b1":"code","e47f9a51":"code","d626fbfc":"code","386ae15f":"code","1602da9c":"code","4911c35d":"code","155682fd":"code","e71ad275":"code","3e349f87":"code","61848c93":"code","f80f3fa5":"code","9cd22f8e":"code","1f9d518e":"code","55c560e1":"code","c19804b6":"code","f94bb719":"code","2ba80e84":"code","289ee06f":"code","a369b969":"code","92043461":"code","910b8d69":"code","f4037302":"code","237be731":"code","d25e52a7":"code","14e9799c":"code","051305fa":"code","7cf280c2":"code","1243a94b":"code","33564e50":"code","c58d55ba":"code","0927824c":"code","a5f1cc4f":"code","1de524e0":"code","16658818":"code","7c03e91f":"code","dfd8823f":"code","186d0fde":"code","67d690d9":"code","cb9f7a9f":"code","23927032":"code","056f2048":"code","2a3a38d1":"code","a210ebb9":"code","d4ce46d1":"code","442ee02f":"code","ca1e685a":"code","0708ae08":"code","51454bad":"code","7300031c":"code","d5884595":"code","6c37f6cd":"markdown","bf731586":"markdown","7618fa2a":"markdown","3937c6c3":"markdown","87540b7d":"markdown","44dfa3f3":"markdown","a516e55d":"markdown","1da10433":"markdown","d5245386":"markdown","a3c9fe56":"markdown","3b6f41b7":"markdown","56ea39b7":"markdown","309d2b62":"markdown","d283f41f":"markdown","12caf5df":"markdown","68ccf446":"markdown","43076a91":"markdown","b1c37a98":"markdown","4e6dd047":"markdown","88eab40f":"markdown","b0505443":"markdown","79f31332":"markdown","2e9718de":"markdown","2c672d66":"markdown","440e3525":"markdown","ee9a736d":"markdown","68fc2ee6":"markdown","6b506c13":"markdown","c9fa590e":"markdown","7c9fdc10":"markdown","aa866184":"markdown","e0d78374":"markdown","8b6d7a26":"markdown","76cce60b":"markdown","64d96195":"markdown","429945ca":"markdown","3673c090":"markdown","b5d896d0":"markdown","c5efc963":"markdown","f36091e2":"markdown","d9b60957":"markdown","336c1632":"markdown","d0ae441e":"markdown","d6e4ae8d":"markdown","d9896a2f":"markdown","dbe2a94a":"markdown","7c7a3ec7":"markdown","6735c70b":"markdown","bfd77502":"markdown","6a01de2d":"markdown","3620fe13":"markdown","aab9b0f4":"markdown","1c32ede7":"markdown","6a59318d":"markdown","e3b973c4":"markdown","c9d546d2":"markdown","8cacf687":"markdown","23499e63":"markdown","b5f78e6c":"markdown","13c4dfdc":"markdown","9a901d80":"markdown","82f5d445":"markdown","d40d6f60":"markdown"},"source":{"f66ce62e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport re\nimport string\nimport collections\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import MiniBatchKMeans\n\nfrom time import time\n%matplotlib inline\nimport os\nimport pandas as pd\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport spacy\nimport spacy.cli\nfrom spacy.matcher import Matcher \nfrom spacy.matcher import PhraseMatcher\n\nspacy.cli.download(\"en\")\nspacy.cli.download(\"en_core_web_lg\")\nnlp = spacy.load('en_core_web_lg')","20ad77f9":"## set english loanguage\nstop_words = set(stopwords.words('english'))\n\n## declaration of Porter stemmer.\nporter=PorterStemmer()\n\n## Clean Null Record in dataframe\ndef cleanEmptyData(columnName,df):\n    return df[df[columnName].notnull()]\n\n## Remove Punctuation\ndef remove_punctuation(columnName,df):\n    return df.loc[:,columnName].apply(lambda x: re.sub('[^a-zA-z\\s]','',x))\n\n## Convert To Lower Case\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str  \n\n## Remove duplicate item in the dataframe\ndef removeDuplicate(df,list):\n    df.drop_duplicates(list, inplace=True)    \n\n## Remove nlp stop words    \ndef remove_stop_words(columnName,df):\n  return df.loc[:,columnName].apply(lambda x: [word for word in x.split() if word not in stop_words])\n\n##Remove single character from the sentence\ndef remove_one_character_word(columnName,df):\n  return df.loc[:,columnName].apply(lambda x: [i for i in x if len(i) > 1])\n\n## Join as a single text with seperator\ndef join_seperator(columnName,df):\n  seperator = ', '\n  return df.loc[:,columnName].apply(lambda x: seperator.join(x))\n\n## apply stemmer to data frame fields\ndef apply_stemmer(columnName,df):\n  return df.loc[:,columnName].apply(lambda x: [porter.stem(word) for word in x])\n\n## Data Cleaning Process function\ndef dataCleaningProcess(dataFrame):\n    ## remove duplicate records\n    removeDuplicate(dataFrame,['abstract', 'text_body'])\n    \n    ## clean null value records\n    clean_data = cleanEmptyData('text_body',dataFrame)\n    clean_data.loc[:,'text_body_clean'] = clean_data.loc[:,'text_body'].apply(lambda x: lower_case(x))\n    \n    ## removing punctuation \n    clean_data.loc[:,'text_body_clean'] = remove_punctuation('text_body_clean',clean_data)\n    \n    ## apply stop words\n    clean_data.loc[:,'text_body_clean'] = remove_stop_words('text_body_clean',clean_data)\n    \n    ## apply stemmer for each tokens\n    clean_data.loc[:,'text_body_clean'] = apply_stemmer('text_body_clean',clean_data)\n    \n    ## removing single charter word in the sentence\n    clean_data.loc[:,'text_body_clean'] = remove_one_character_word('text_body_clean',clean_data)\n    \n    ## join as a single text from words token\n    clean_data.loc[:,'text_body_clean'] = join_seperator('text_body_clean',clean_data)\n    \n    ## remove coma after join\n    clean_data.loc[:,'text_body_clean'] = remove_punctuation('text_body_clean',clean_data)\n    \n    return clean_data","2e792d1a":"## get words token from text\ndef getWordsFromText(_text):\n    words = []\n    for i in range(0,len(_text)):\n        words.append(str(_text.iloc[i]['text_body']).split(\" \"))\n    return words\n\n# Read Excel data as Data Frame\ndef readExcelToDataFrame(path):\n    research_dataframe = pd.read_csv(path,index_col=False)\n    research_dataframe.drop(research_dataframe.columns[research_dataframe.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n    return research_dataframe\n\n## basic scatter plot\ndef showScatterPlot(_X,title):\n    # sns settings\n    sns.set(rc={'figure.figsize':(15,15)})\n    # colors\n    palette = sns.color_palette(\"bright\", 1)\n    # plot\n    sns.scatterplot(_X[:,0], _X[:,1], palette=palette)\n    plt.title(title)\n    # plt.savefig(\"plots\/t-sne_covid19.png\")\n    plt.show()\n\n## scatter plot with cluster\ndef showClusterScatterPlot(_X, _y_pred, title):\n    # sns settings\n    sns.set(rc={'figure.figsize':(10,10)})\n    # colors\n    palette = sns.color_palette(\"bright\", len(set(_y_pred)))\n    # plot\n    sns.scatterplot(_X[:,0], _X[:,1], hue=_y_pred, legend='full', palette=palette)\n    plt.title(title)\n    # plt.savefig(\"plots\/t-sne_covid19_label.png\")\n    plt.show()\n\n\n## drop clumns\ndef getTargetData(dataFrame):\n    text_body = dataFrame.drop([\"doc_id\", \"source\", \"title\", \"abstract\"], axis=1)\n    return getWordsFromText(text_body)\n\n## train model for tSNE clustering visualization\ndef trainEmbededData(_perplexity,dataFrame,total_cluster, _n_iter):\n    ## convert text to word frequency vectors\n    vectorizer = TfidfVectorizer(max_features=2**12)\n    \n    ## training the data and returning term-document matrix.\n    _X = vectorizer.fit_transform(dataFrame['text_body_clean'].values)\n    \n    ## tsne declartion\n    tsne = TSNE(verbose=1, perplexity=_perplexity,learning_rate=200, random_state=0, n_iter=_n_iter)\n    _X_embeded = tsne.fit_transform(_X.toarray())\n    \n    ## clusterring for tsne\n    _kmeans = MiniBatchKMeans(n_clusters=total_cluster)\n    return _X_embeded,_kmeans,_X\n\n## predicting cluster centers and predict cluster index for each sample\ndef predict(_kmeans,_X):\n    return _kmeans.fit_predict(_X)\n\n## reusable fucntion for TSNE K-Mean Clustering with TF-IDF\ndef analyse(pplexity,data_frame,cluster,iter):\n    ## train model for tSNE clustering visualization\n    embeded,kmeans,x = trainEmbededData(pplexity,data_frame,cluster,iter)\n    pred = predict(kmeans,x)\n    ## visualized the scatter plot\n    showClusterScatterPlot(embeded,pred,'t-SNE Covid-19 - Clustered(K-Means) - Tf-idf with Plain Text')\n    return embeded,kmeans,x","d8b420b1":"research_dataframe = readExcelToDataFrame('\/kaggle\/input\/covid-data\/data.csv')\nresearch_dataframe.head()","e47f9a51":"clean_data =dataCleaningProcess(research_dataframe)","d626fbfc":"clean_data.head()\nclean_process_data = clean_data.drop([\"doc_id\", \"source\", \"title\", \"abstract\"], axis=1)\nclean_process_data.head(20)","386ae15f":"meta_data = readExcelToDataFrame('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\nmeta_data.head()\n","1602da9c":"def prepare_search_data(_meta_data_frame,research_dataframe):\n    ## add a field doc_id\n    _meta_data_frame[\"doc_id\"] = _meta_data_frame[\"sha\"]\n    \n    ## clean NUll record\n    _meta_data_frame = cleanEmptyData('doc_id', _meta_data_frame)\n    _meta_data_frame = cleanEmptyData('publish_time', _meta_data_frame)\n    \n    ## select only 2019 & 2020 published records\n    meta_data_filter = _meta_data_frame[_meta_data_frame['publish_time'].str.contains('2019') | _meta_data_frame['publish_time'].str.contains('2020')]  \n    \n     ## clean NUll record\n    research_dataframe_clean = cleanEmptyData('doc_id', research_dataframe)\n    research_dataframe_clean = cleanEmptyData('text_body', research_dataframe_clean)\n    \n    ## merging of Research data and meta data on doc_id\n    tmp_data_frame  = research_dataframe_clean.merge(meta_data_filter, on='doc_id', how='right')\n    \n    ## remove un used fields\n    clean_process_data = tmp_data_frame.drop([\"source\", \"abstract_x\", \"cord_uid\", \"abstract_x\",\"sha\",\"source_x\",\"title_y\",\"pmcid\",\"pubmed_id\",\"license\",\"abstract_y\",\"journal\",\"Microsoft Academic Paper ID\",\"WHO #Covidence\",\"has_pdf_parse\",\"has_pmc_xml_parse\",\"full_text_file\"], axis=1)\n    \n    ## clean NUll record\n    clean_process_data = cleanEmptyData('text_body', clean_process_data)\n    clean_process_data = clean_process_data.rename(columns={'title_x': 'title'}) \n    \n    # reordering the column index\n    columns = [\"doc_id\",\"doi\", \"publish_time\", \"authors\",\"url\",\"title\", \"text_body\"]\n    clean_process_data = clean_process_data.reindex(columns=columns)\n    \n    return clean_process_data","4911c35d":"def process_title(x):\n  if not str(x['title_x']).lower() =='nan':\n    return str(x['title_x']) + ' (' +  str(x['url']) + ')'\n  else:\n    return str(x['url'])","155682fd":"filter_data = prepare_search_data(meta_data,research_dataframe)\nfilter_data.head()","e71ad275":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n","3e349f87":"def show_WordCloud(filter_data):\n    comment_words = ' '\n    stopwords = set(STOPWORDS) \n  \n# iterate through the csv file \n    for val in filter_data: \n\n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n        #print(val) \n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        for words in tokens: \n         comment_words = comment_words + words + ' '\n         #print(comment_words)\n\n    wordcloud = WordCloud(width = 800, height = 800, \n                    background_color ='white',\n                    max_words = 200, \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(comment_words) \n\n    # plot the WordCloud image                        \n    plt.figure(figsize = (10, 10), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show()\n\n","61848c93":"show_WordCloud(filter_data.title)","f80f3fa5":"embeded,kmeans,x = analyse(5000,clean_process_data,10,15000)","9cd22f8e":"papers = research_dataframe['text_body'].astype('str')\nlen(papers)\npapers.head()\n#meta_data_filter\u00a0=\u00a0_meta_data_frame[_meta_data_frame['publish_time'].str.contains('2019')\u00a0|\u00a0_meta_data_frame['publish_time'].str.contains('2020')]\u00a0\u00a0 ","1f9d518e":"%%time\nimport nltk\nimport tqdm\nnltk.download('wordnet')\n\nstop_words = nltk.corpus.stopwords.words('english')\nwtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\nwnl = nltk.stem.wordnet.WordNetLemmatizer()\n\ndef normalize_corpus(papers):\n    norm_papers = []\n    for paper in tqdm.tqdm(papers):\n        paper = paper.lower()\n        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n        paper_tokens = list(filter(None, paper_tokens))\n        #if paper_tokens:\n        norm_papers.append(paper_tokens)\n            \n    return norm_papers\n    \nnorm_papers = normalize_corpus(papers)\nprint(len(norm_papers))","55c560e1":"import gensim\n\nbigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_') # higher threshold fewer phrases.\nbigram_model = gensim.models.phrases.Phraser(bigram)\n\nprint(bigram_model[norm_papers[0]][:50])","c19804b6":"print(bigram_model[norm_papers[1]][:50])","f94bb719":"norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n\n# Create a dictionary representation of the documents.\ndictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\nprint('Sample word to number mappings:', list(dictionary.items())[:15])\nprint('Total Vocabulary Size:', len(dictionary))","2ba80e84":"# Filter out words that occur less than 20 documents, or more than 60% of the documents.\ndictionary.filter_extremes(no_below=20, no_above=0.6)\nprint('Total Vocabulary Size:', len(dictionary))","289ee06f":"bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\nprint(bow_corpus[1][:50])","a369b969":"print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])","92043461":"print('Total number of papers:', len(bow_corpus))","910b8d69":"import joblib\nlda_model = joblib.load('\/kaggle\/input\/coviddata\/lda_model.jl')","f4037302":"topics_assigned = lda_model[bow_corpus]","237be731":"len(topics_assigned)","d25e52a7":"b= pd.DataFrame(topics_assigned,columns = ['T0','T1','T2','T3','T4','T5','T6','T7','T8','T9'])","14e9799c":"d= pd.concat([research_dataframe['text_body'],b],axis=1)","051305fa":"d.to_csv(\"Topic_paper_07042020_v4.csv\")","7cf280c2":"for topic_id, topic in lda_model.print_topics(num_topics=50, num_words=20):\n    print('Topic #'+str(topic_id+1)+':')\n    print(topic)\n    print()","1243a94b":"import numpy as np\ntopics_coherences = lda_model.top_topics(bow_corpus, topn=20)\navg_coherence_score = np.mean([item[1] for item in topics_coherences])\nprint('Avg. Coherence Score:', avg_coherence_score)","33564e50":"topics_with_wts = [item[0] for item in topics_coherences]\nprint('LDA Topics with Weights')\nprint('='*50)\nfor idx, topic in enumerate(topics_with_wts):\n    print('Topic #'+str(idx+1)+':')\n    print([(term, round(wt, 3)) for wt, term in topic])\n    print()","c58d55ba":"print('LDA Topics without Weights')\nprint('='*50)\nfor idx, topic in enumerate(topics_with_wts):\n    print('Topic #'+str(idx+1)+':')\n    print([term for wt, term in topic])\n    print()","0927824c":"cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n                                                      texts=norm_corpus_bigrams,\n                                                      dictionary=dictionary, \n                                                      coherence='c_v')\navg_coherence_cv = cv_coherence_model_lda.get_coherence()\n\numass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n                                                         texts=norm_corpus_bigrams,\n                                                         dictionary=dictionary, \n                                                         coherence='u_mass')\navg_coherence_umass = umass_coherence_model_lda.get_coherence()\n\nperplexity = lda_model.log_perplexity(bow_corpus)\n\nprint('Avg. Coherence Score (Cv):', avg_coherence_cv)\nprint('Avg. Coherence Score (UMass):', avg_coherence_umass)\nprint('Model Perplexity:', perplexity)","a5f1cc4f":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nstopwords = set(STOPWORDS)\n\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=2000,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","1de524e0":"with open(\"\/kaggle\/input\/coviddata\/Topic_paper_07042020_v4.csv\",encoding = 'utf8', errors='ignore') as f:\n    df_topic = pd.read_csv(f)\n    \n#df = df.replace(\"\\n\",\" \").dropna()","16658818":"show_WordCloud(df_topic.loc[df_topic['Dominant_topic'] == 0]['text_body'])\n#show_WordCloud(filter_data.title)","7c03e91f":"show_WordCloud(df_topic.loc[df_topic['Dominant_topic'] == 1]['text_body'])","dfd8823f":"\nfrom __future__ import print_function\n\n__author__ = 'maxim'\n\nimport numpy as np\nimport gensim\nimport string\nfrom gensim.models import Word2Vec\nfrom keras.callbacks import LambdaCallback\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, Activation\nfrom keras.models import Sequential\nfrom keras.utils.data_utils import get_file\nfrom gensim.models import Word2Vec\nimport pandas as pd\nimport numpy as np\nimport re\nimport json\nimport pandas as pd\nfrom gensim.models.fasttext import FastText\nfrom os import listdir\nfrom os.path import isfile, join\n\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nimport re\n\nimport gensim\nimport nltk","186d0fde":"print('loading model')\n\nword_model = Word2Vec.load(\"\/kaggle\/input\/covidmodel\/word2vec_1000ITR.model\")\n\nprint('model loaded')","67d690d9":"print(word_model.most_similar(positive=['covid19'])) ","cb9f7a9f":"print(word_model.most_similar(positive=['treatment','options' ])) ","23927032":"\narr_most_similar = [['coronavirus','incubation','period','7day','2day','14day'],['covid19','fomites','airborne','nonporous','handrails','desks','floor','fomite']\n      ,['covid19','ppe','masks'],['covid19','diagnostics','tools','advancement','automation'],['covid19','methods','detect','virus']]\narr_predict=[['covid19','season'],['covid19','surfaces','nonporous','handrails','desks','floor','fomite']]     \n\n#Question: 9     \n#Public health mitigation measures that could be effective for control\n#Intial input fed to word2vec  model to extract related words that could lead to the answers of this question\n#['public','health','mitigation','measures',  'effective', 'control','covid19','disease']]\n#RESULT from first Query: [('interventions', 0.7447171807289124), ('prevention', 0.7438710927963257), ('preventive', 0.7241473197937012), ('policies', 0.7131460905075073), ('intervention', 0.7103219628334045), ('management', 0.7056314945220947), ('implementing', 0.6939526200294495), ('policy', 0.6897070407867432), ('quarantine', 0.6833001971244812), ('planning', 0.6516326069831848), ('implementation', 0.641217827796936), ('awareness', 0.6297034025192261), ('containment', 0.6278428435325623), ('preparedness', 0.622099757194519), ('epidemic', 0.6220937967300415), ('timely', 0.6204564571380615), ('community', 0.6184203624725342), ('government', 0.6134730577468872), ('outbreak', 0.6104072332382202), ('pandemic', 0.6079082489013672)]]\n#Updated input taken from first querying of word2vec model after choosing relevant keywords\n#['mitigation','measures','control','covid19','quarantine','containment','awareness','policies']]\n#RESULT from second Query: [[('interventions', 0.7805880308151245), ('intervention', 0.7159266471862793), ('policy', 0.690924346446991), ('preventive', 0.6865450143814087), ('implementing', 0.6757345795631409), ('implementation', 0.6531403064727783), ('planning', 0.6507831811904907), ('practices', 0.6400246620178223), ('prevention', 0.6383914947509766), ('management', 0.6378570199012756), ('government', 0.6369956731796265), ('preparedness', 0.6348874568939209), ('restrictions', 0.6139740943908691), ('campaigns', 0.6061151027679443), ('behaviors', 0.5989149808883667), ('plans', 0.5976110696792603), ('decisions', 0.5968020558357239), ('timely', 0.591980516910553), ('governmental', 0.5905696153640747), ('biosecurity', 0.5887465476989746)]]\n\n#Most Similar Keywrods Detection \n\nlen(arr_most_similar)\narrans=[]\nprint(len(arr_most_similar))\ncount=0\nfor i in arr_most_similar:\n    print('--------->',i)\n    answers=word_model.most_similar(positive=i,topn=30)\n    \n    arrans.append(answers)\n    count +=1\n    print('=========',count)\nprint(arrans)\nprint(len(arrans))\n\n\n#Predicted Keywords Detection\n\narr_predict\nlen(arr_predict)\narrans_arr_predict=[]\nprint(len(arr_predict))\ncount=0\nfor j in arr_predict:\n    print('--------->',j)\n    answers1=word_model.predict_output_word(j,topn=30)\n    \n    arrans_arr_predict.append(answers1)\n    count +=1\n    print('=========',count)\nprint(arrans_arr_predict)\nprint(len(arrans_arr_predict))\n\n","056f2048":"for q in arrans:\n    print(q)","2a3a38d1":"#arrans\nfor k in arrans_arr_predict:\n    print(k)","a210ebb9":"## constant for spliting sentence\nalphabets= \"([A-Za-z])\"\nprefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\nsuffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\nstarters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\nacronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\nwebsites = \"[.](com|net|org|io|gov)\"","d4ce46d1":"\n \n## spliting to sentence from text\ndef split_into_sentences(text):\n    text = \" \" + text + \"  \"\n    text = text.replace(\"\\n\",\" \")\n    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n    text = re.sub(websites,\"<prd>\\\\1\",text)\n    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n    if \"\u201d\" in text: text = text.replace(\".\u201d\",\"\u201d.\")\n    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n    text = text.replace(\".\",\".<stop>\")\n    text = text.replace(\"?\",\"?<stop>\")\n    text = text.replace(\"!\",\"!<stop>\")\n    text = text.replace(\"<prd>\",\".\")\n    sentences = text.split(\"<stop>\")\n    sentences = sentences[:-1]\n    sentences = [s.strip() for s in sentences]\n    return sentences\n\n## search inference text by key words and return all the matches sentence\ndef search_inference_keys(text, keywords):\n    sentences = split_into_sentences(text)\n    txt = ''\n    for sent in sentences:\n        r = re.compile(keywords,flags=re.IGNORECASE)\n        if len(r.findall(sent))>0:         \n            txt = str(txt) + str(sent)\n    return txt\n\n\n## check key words exist or not in a sentence\ndef check_exist_multiple_keywords(text, keywords):\n    r = re.compile(keywords, flags=re.IGNORECASE)\n    if len(r.findall(text))>0:    \n        return True\n    else:\n        return False\n\n## Search Inference and download results as excel \ndef searc_by_keys_as_excel(keyword, src_data_frame):\n    data_frame = src_data_frame\n    ## check exist to slice down the related contents\n    data_frame['search_key_status'] =data_frame.loc[:,'text_body'].apply(lambda x: check_exist_multiple_keywords(x,keyword))\n    ## select only target data\n    process_data_frame = data_frame.query('search_key_status == True')\n  \n    ## filter on corona and covid 19 related data\n    process_data_frame['search_covid_content'] =process_data_frame.loc[:,'text_body'].apply(lambda x: check_exist_multiple_keywords(x,'covid-19|sars-cov-2|2019-ncov|ncov-19|coronavirus'))\n  \n    ## get only covid-19|sars-cov-2|2019-ncov|ncov-19|coronavirus data\n    process_data_frame = process_data_frame.query('search_covid_content == True')\n    process_data_frame.loc[:,'inference'] = process_data_frame.loc[:,'text_body'].apply( lambda x: search_inference_keys(x,keyword))\n    \n    ## remove unused fields\n    final_data = process_data_frame.drop([\"search_key_status\",\"text_body\"], axis=1)\n    ## download as excel\n    final_data.to_excel(str(keyword) + '_result.xlsx', sheet_name='keyword')\n  \n    return final_data","442ee02f":"# Search Inference for \"incubation period\" and download results as excel \nsearch_data = prepare_search_data(meta_data,research_dataframe)","ca1e685a":"final_data =searc_by_keys_as_excel('incubation period|2day|7day|14day',search_data)\n\n\nfinal_data.head()","0708ae08":"final_data =searc_by_keys_as_excel('winter season',search_data)\n","51454bad":"final_data =searc_by_keys_as_excel('fomites|airborne|nonporous|handrails|desks|floor|fomite|hands|toilets|door|bedrails',search_data)","7300031c":"final_data =searc_by_keys_as_excel('masks|ppe|respirators|protective equipments|gowns|facemasks|effectiveness|n95',search_data)","d5884595":"final_data =searc_by_keys_as_excel('covid19|diagnostics|tools|advancement|automation|serology',search_data)","6c37f6cd":"**Robust Word2Vec Model with Gensim**\n\nThe __`gensim`__ framework, created by Radim \u0158eh\u016f\u0159ek consists of a robust, efficient and scalable implementation of the Word2Vec model. We will leverage the same on our covid-19 corpus. In our workflow, we will tokenize our normalized corpus and then focus on the following four parameters in the Word2Vec model to build it.\n\n- __`size`:__ The word embedding dimensionality : 100\n- __`window`:__ The context window size : 20\n- __`min_count`:__ The minimum word count : 1\n- __`iter`:__ Iteration : 1000 \n- __`sg`:__ Training model, 1 for skip-gram otherwise CBOW : CBOW\n\nWe have build a  Word2Vec model on the corpus. ","bf731586":"The Visualization clearly talks about the : **coronavirus, transmission, infection, vaccine, ourbreak etc.. **\nGiving a clear picture of the terminalogies and informations that can be retrived from the research papers.","7618fa2a":"Data Preparation included the below process:\n\n* Load Research & Meta Data\n* Meta Data filter for published time from 2019 to 2020 on doc_id\n* Remove unused fields for the search inference.","3937c6c3":"**Evaluating topic model:**\n\nQuality We can use perplexity and coherence scores as measures to evaluate the topic model. Typically, lower the perplexity, the better the model. Similarly, the lower the UMass score and the higher the Cv score in coherence, the better the model.","87540b7d":"Import libraries","44dfa3f3":"EXTRACT CSV","a516e55d":"1. **Incubation Period :**\n\nKey HighLight in Search\n\n* Mean Incubation Period is 5days across various papers\n* Range of Incubation Period is upto 14 days\n* Median Incubation Period is reported to be 3 days\n* There are evidences where incubation period is mentioned as 20 days\n\n1. Mean Incubation Period is 5days across various papers\n\n2019-11-07 AlRuthia, Yazed; Somily, Ali M; Alkhamali, Amal S; Bahari, Ohud H; AlJuhani, Raneem J; Alsenaidy, Mohammad; Balkhi, Bander https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6844224\/ Estimation Of Direct Medical Costs Of Middle East Respiratory Syndrome Coronavirus Infection: A Single-Center Retrospective Chart Review Study\n\nAfter an average incubation period of approximately 5 days, 8 patients typically present with symptoms of lower respiratory tract infection, such as cough and shortness of breath often combined with nonspecific signs of the disease, such as fatigue, myalgia, fever, rhinorrhea, headaches, sore throat, vomiting, or diarrhea.\n\n2020 Wu, Yi-Chi; Chen, Ching-Sung; Chan, Yu-Jiun https:\/\/doi.org\/10.1097\/jcma.0000000000000270 The outbreak of COVID-19: An overview COVID-19 has a mean incubation period of 5.Clinicians should consider the possibility of 2019-nCoV virus infection in persons with travel or exposure history with compatible incubation period and presenting symptoms.\n\n2020-02-05 Julie Spencer; Deborah P Shutt; Sarah K Moser; Hannah Clegg; Helen J Wearing; Harshini Mukundan; Carrie A Manore https:\/\/doi.org\/10.1101\/2020.02.04.20020404 Epidemiological parameter review and comparative dynamics of influenza, respiratory syncytial virus, rhinovirus, human coronavirus, and adenovirus\n\nResults of our review for human coronavirus include the following mean values: an incubation period of 5.We assume here that the latent period equals the incubation period, or the mean period of time between exposure to the virus and the onset of symptoms.Second, the assumption that the latent period equals the incubation period may result in an overestimation of the latent period.This is because the beginning of the true infectious period may occur before the onset of symptoms; however, this is difficult to measure and is not generally reported in the studies that report values for the incubation period.Incubation periods of acute respiratory viral infections: a systematic review.\n\n2. Range of Incubation Period is upto 14 days\n\n2020-01-28 Natalie M. Linton; Tetsuro Kobayashi; Yichi Yang; Katsuma Hayashi; Andrei R. Akhmetzhanov; Sung-mok Jung; Baoyin Yuan; Ryo Kinoshita; Hiroshi Nishiura https:\/\/doi.org\/10.1101\/2020.01.26.20018754 Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data\n\nThe incubation period is defined as the time from infection to illness onset.Knowledge of the incubation period of a directly transmitted infectious disease is critical to determine the time period required for monitoring and restricting the movement of healthy individuals (i.e., the quarantine period) [5, 6] .The incubation period also aids in understanding the relative infectiousness of COVID-19 and can be used to estimate the epidemic size [7] .Using publicly available data from the ongoing epidemic of COVID-19 with known event dates, the present study aimed to estimate the incubation period and other time intervals that govern the interpretation of epidemiological dynamics of COVID-19 infections.We thus estimated the incubation period by (i) excluding Wuhan residents and (ii) including Wuhan residents.We used the dates of three critical points in the course of infection-symptom onset, hospital admission, and death-to calculate four time intervals: the time from (a) exposure to illness onset (i.e., incubation period), (b) illness onset to hospital admission, (c) illness onset to death, and (d) hospital admission to death.) is the PDF of the incubation period independent of g(.To address the selection bias in the dataset due to the continued growth of the outbreak (i.e., cases with shorter incubation periods are more likely to be included in the dataset), we also accounted for .For the incubation period estimates, the lognormal distribution provided the best fit to the data, both when excluding and including Wuhan residents.The mean incubation period was estimated at 5.The mean incubation period was 5.Figure 2 shows the cumulative distribution function of the incubation period with and without right truncation.The present study advances the public discussion on COVID-19 infections by presenting explicit estimations of the incubation period and other epidemiologic characteristics using publicly available data.Our estimated mean incubation period of approximately 5 days is comparable to known mean values of the incubation period for severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS) [9, [16] [17] [18] , as well as other recent estimates of the incubation period for COVID-19 [18] .In addition to empirically showing the comparability of COVID-19 to other disease-causing coronaviruses, the present study has also shown that the 95th percentile of the incubation period is around 10-14 days, indicating that a 14-day quarantine period would largely ensure the absence of disease among healthy .[18] noted a similar finding in their analysis of the incubation period for 88 cases (including 63 Wuhan residents).From the 95th percentile estimate of the incubation period we found that the length of quarantine should be at least 14 days, and we stress that the 17-24-day time delay from illness onset to death must be addressed when estimating COVID-19 case fatality risk.\n\n2020-02-11 Lin, Xiaoqi; Gong, Zhenyu; Xiao, Zuke; Xiong, Jingliang; Fan, Bing; Liu, Jiaqi https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7039714\/\n\nThe incubation period of 2019-nCoV is generally 3-7 days but no longer than 14 days, and the virus is infective during the incubation period.\n\n3. Median Incubation Period is reported to be 3 days\n\n2020 Xu, Xiao-Wei; Wu, Xiao-Xin; Jiang, Xian-Gao; Xu, Kai-Jin; Ying, Ling-Jun; Ma, Chun-Lian; Li, Shi-Bo; Wang, Hua-Ying; Zhang, Sheng; Gao, Hai-Nv; Sheng, Ji-Fang; Cai, Hong-Liu; Qiu, Yun-Qing; Li, Lan-Juan https:\/\/doi.org\/10.1136\/bmj.m606 Clinical findings in a group of patients infected with the 2019 novel coronavirus (SARS-Cov-2) outside of Wuhan, China: retrospective case series\n\nThe incubation period was defined as the time from exposure to the onset of illness, which was estimated among patients who could provide the exact date of close contact with individuals from Wuhan with confirmed or suspected SARS-Cov-2 infection.Among 56 patients who could provide the exact date of close contact with someone with confirmed or suspected SARS-Cov-2 infection, the median incubation period from exposure to symptoms was 4 days (interquartile range 3-5 days).Among the 33 patients who had symptoms for more than 10 days after illness onset, the median incubation period from exposure to symptoms was 3 days (interquartile range 3-4 days).It is possible that an even greater number of infected patients exist without a diagnosis because their symptoms were less severe and because of the incubation period.\n\n2020-02-29 Shi Zhao; Peihua Cao; Daozhou Gao; Zian Zhuang; Marc Chong; Yongli Cai; Jinjun Ran; Kai Wang; Yijun Lou; Weiming Wang; Lin Yang; Daihai He; Maggie H Wang https:\/\/doi.org\/10.1101\/2020.02.26.20028449 Title: Modelling the coronavirus disease (COVID-19) outbreak on the Diamond Princess ship using\n\nNote that a recent study on the 1099 patients found that the median of incubation period is only 3 days, thus a short effective SI is also possible [15] .\n\nThere are evidences where incubation period is mentioned as 20 days 10.1101\/2020.02.19.20025023 2020-02-23 Jinwei Ai; Junwen Chen; Yong Wang; Xiaoyun Liu; Wufeng Fan; Gaojing Qu; Meiling Zhang; Shengduo Polo Pei; Bowen Tang; Shuai Yuan; Yang Li; Lisha Wang; Guoxin Huang; Bin Pei https:\/\/doi.org\/10.1101\/2020.02.19.20025023 participated in study designimage.png\nIn the analysis of 44 cases with clear contact time, incubation periods ranged from one to twenty days with a mean of 8.In our analysis of 44 patients with clear contact history, we found that the mean incubation period of COVID-19 was 8.Compared with previous studies [9, 10] , the incubation period of our patients varied more greatly with maximum of 20 days.The prolonged incubation period will increase the risk of virus transmission.The rate of severe illness and death were low, whereas some patients had longer incubation period.\n\nprint(word_model.predict_output_word(['covid19','season'],topn=20))\n\n2. **Seasonality of Transmission :**\n\nKey Point\n\nCorona Viruses show seasonal transmission during various pandemic occured before. A detailed study is available in the below paper. However, Covid-19 is not yet proven the seasonality as it has affected even tropical climates in Singapore. Hence it is not yet seasonal. It can be observed in the future global transmission.\n\n2020-02-17 Richard A Neher; Robert Dyrdak; Valentin Druelle; Emma B Hodcroft; Jan Albert https:\/\/doi.org\/10.1101\/2020.02.13.20022806 Potential impact of seasonal forcing on a SARS-CoV-2 pandemic\n\n1.Seasonal forcing on SARS-CoV-2 should thus be taken into account in the further monitoring of the global transmission 2.The seasonal CoVs show a strong and consistent seasonal variation, and modeling suggests that this requires strong variation in transmissibility throughout the year. It should be noted, however, that SARS-CoV-2 does seem to transmit in tropical climates like Singapore, and so winter is not a necessary condition of SARS-CoV-2 spread\n\n3. **Transmission Surfaces :**\n\nCovid-19 looks to be airborne generating from the aerosols in medical procedure of patients There are evidences that the toilets and ICUs are tested positive for airborne in hospitals Studies suspect that there are possibilities are virus shedding through stools for patients with diahrea\n\nCovid-19 looks to be airborne generating from the aerosols in medical procedure of patients.\n2020 Peng, Xian; Xu, Xin; Li, Yuqing; Cheng, Lei; Zhou, Xuedong; Ren, Biao https:\/\/doi.org\/10.1038\/s41368-020-0075-9 Transmission routes of 2019-nCoV and controls in dental practice\n\nStudies have suggested that 2019-nCoV may be airborne through aerosols formed during medical procedures 46 .Since 2019-nCoV can be passed directly from person to person by respiratory droplets, emerging evidence suggested that it may also be transmitted through contact and fomites 43, 48 .The pathogenic microorganisms can be transmitted in dental settings through inhalation of airborne microorganisms that can remain suspended in the air for long periods 51 , direct contact with blood, oral fluids, or other patient materials 52 , contact of conjunctival, nasal, or oral mucosa with droplets and aerosols containing microorganisms generated from an infected individual and propelled a short distance by coughing and talking without a mask 53, 54 , and indirect contact with contaminated instruments and\/or environmental surfaces 50 .The airborne spread of SARS-Cov (severe acute respiratory syndrome coronavirus) is well-reported in many literatures.Particles of droplets and aerosols are small enough to stay airborne for an extended period before they settle on environmental surfaces or enter the respiratory tract.Specifically, the oral professionals should wash their hands before patient examination, before dental procedures, after touching the patient, after touching the surroundings and equipment without disinfection, and after touching the oral mucosa, damaged skin or wound, blood, body fluid, secretion, and excreta.Since airborne droplet transmission of infection is considered as the main route of spread, particularly in dental clinics and hospitals, barrier-protection equipment, including protective eyewear, masks, gloves, caps, face shields, and protective outwear, is strongly recommended for all healthcare givers in the clinic\/hospital settings during the epidemic period of 2019-nCoV.It has been reported that the use of rubber dam could significantly reduce airborne particles in~3-foot diameter of the operational field by 70% 58 .We have summarized the possible transmission routes of 2019-nCov in stomatology, such as the airborne spread, contact spread, and contaminated surface spread.\n\nThere are evidences that the toilets and ICUs are tested positive for airborne in hospitals\n\n2020-03-10 Yuan Liu; Zhi Ning; Yu Chen; Ming Guo; Yingle Liu; Nirmal Kumar Gali; Li Sun; Yusen Duan; Jing Cai; Dane Westerdahl; Xinjin Liu; Kin-fai Ho; Haidong Kan; Qingyan Fu; Ke Lan https:\/\/doi.org\/10.1101\/2020.03.08.982637 Aerodynamic Characteristics and RNA Concentration of SARS-CoV-2 Aerosol in Wuhan Hospitals during COVID-19 Outbreak\n\nGenerally undetectable or very low concentrations of airborne SARS-CoV-2 were found in most PAA inside the two hospitals in Wuhan.The negative pressure ventilation and high air exchange rate inside ICU, CCU and ward room of Renmin Hospital are effective in minimizing airborne SARS-CoV-2.This study also recorded an elevated airborne SARS-CoV-2 concentration inside the patient mobile toilet of Fangcang Hospital.has found the wipe samples from room surfaces of toilets used by SARS-CoV-2 patients tested positive.\n\nStudies suspect that there are possibilities are virus shedding through stools for patients with diahrea 2020-03-08 Roman Woelfel; Victor Max Corman; Wolfgang Guggemos; Michael Seilmaier; Sabine Zange; Marcel A Mueller; Daniela Niemeyer; Patrick Vollmar; Camilla Rothe; Michael Hoelscher; Tobias Bleicker; Sebastian Bruenink; Julia Schneider; Rosina Ehmann; Katrin Zwirglmaier; Christian Drosten; Clemens Wendtner https:\/\/doi.org\/10.1101\/2020.03.05.20030502 Virological assessment of hospitalized cases of coronavirus disease 2019 *equal contribution **senior authors with equal contribution\n\nThe present study shows that COVID-19 can often present as a common cold-like illness. SARS-CoV-2 can actively replicate in the upper respiratory tract, and is shed for a prolonged time after symptoms end, including in stool\n\n4. **Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings Effectiveness of Personal Protective Equipments.: **\n\nPersonal Protective Equipments have proven very effective in the control of Covid-19 in China. The below article and study retrieved through our keywords indicates there was a model derived which has proved the usage of self-protection equipments as effective.\n\n2020-03-06 Hui Wan; Jing-an Cui; Guo-Jing Yang https:\/\/doi.org\/10.1101\/2020.03.01.20029629 Risk estimation and prediction by modeling the transmission of the novel coronavirus (COVID-19) in mainland China excluding Hubei province\n\n**Results**: The estimation outcomes indicate that the control reproduction number is 3.36 (95% CI 3.20-3.64) and Re(t) has dropped below 1 since January 31st, 2020, which implies that the containment strategies implemented by the Chinese government in mainland China excluding Hubei province are indeed effective and magnificently suppressed COVID-19 transmission. Moreover, our results show that relieving personal protection too early may lead to the spread of disease for a longer time and more people would be infected, and may even cause epidemic or outbreak again. By calculating the effective reproduction ratio, we prove that the contact rate should be kept at least less than 30% of the normal level by April, 2020. Conclusions: To ensure the epidemic ending rapidly, it is necessary to maintain the current integrated restrict interventions and self-protection measures, including travel restriction, quarantine of entry, contact tracing followed by quarantine and isolation and reduction of contact, like wearing masks, etc. People should be fully aware of the real-time epidemic situation and keep sufficient personal protection until April. If all the above conditions are met, the outbreak is expected to be ended by April in mainland China apart from Hubei province","1da10433":"**LDA TOPIC WITH WEIGHTS**","d5245386":"Cleaning of corpus helper functions ","a3c9fe56":"# VISUALIZATION","3b6f41b7":"*The Keywords extracted through embedding are passed on to the below functions to extarct the list of research articles to further manually pick the articles relevant to the questions to answer the questions.*\n\n**Following are the keywords picked question wise:**\n\n* Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n**--> incubation period|2day|7day|14day**\n* Seasonality of transmission.\n**--> winter season**\n* Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n**--> fomites|airborne|nonporous|handrails|desks|floor|fomite|hands|toilets|door|bedrails**\n* Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\n**--> masks|ppe|respirators|protective equipments|gowns|facemasks|effectiveness|n95**\n* Implementation of diagnostics and products to improve clinical processes\n**--> covid19|diagnostics|tools|advancement|automation|serology**\n\n\nSeperate Excel is created in the Output section for each questions with its possible research articles that can prove the inference made using the word2vec answering approach.","56ea39b7":"# MODEL BUILDING AND INFERENCE","309d2b62":"**Topic 1**","d283f41f":"**WORD CLOUD FOR THE DOMINANT TOPIC(S) **\n\nExample: Topic 1 can have 20 documents while Topic 2 may have 200 documents in it.Hence a word cloud is drawn to visualize the intersting vocabs involving each Topic","12caf5df":"In the above Questions, Questions 1, 3 ,6 ,8 and 13 are attempted using a combination of word2vec embedding to extract the keywords and search algorithm to extract the research articles related to it.\n\nWord2vec embedding provides the keywords that are closest to answer the questions","68ccf446":"**Topic 2**","43076a91":"We tried understanding documents with different clusters with various perplexity and identified the optimal perplexity where the convergence of documents took place. ","b1c37a98":"Question 4: --> Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings --> masks|ppe|respirators|protective equipments|gowns|facemasks|effectiveness|n95","4e6dd047":"> In the above word cloud for the topics were mostly about infection samples and detection of virus in a patient","88eab40f":"# EDA\n\nWORD CLOUD FOR THE TITLE OF ARTICLES FROM 2019 - 2020 ","b0505443":"Question 2 and 3:\n\nKeywords sorted using prediction from the below result\n\n* Q2: winter season\n* Q3: fomites|airborne|nonporous|handrails|desks|floor|fomite|hands|toilets|door|bedrails","79f31332":"**Topic Models with Latent Dirichlet Allocation (LDA)**","2e9718de":"We perform some basic text wrangling or preprocessing before diving into topic modeling. We keep things simple here","2c672d66":"**Model Evaluation:**\n\nExample 1:\n\n**\" COVID-19 \"** keyword when tested against our word embedding model gave the following results.\n\n[('ncp', 0.7167163491249084), ('wuhan', 0.7067078351974487), ('2019ncov', 0.7065909504890442), ('sarscov2', 0.6876667737960815), ('mers', 0.6260266304016113), ('sars', 0.6118236780166626), ('2020', 0.6101416349411011), ('hubei', 0.5763685703277588), ('mainland', 0.5460340976715088), ('china', 0.531872034072876)]\n\nnotable keywords from the word2vec output are: 'wuhan','sars','china' relating to the orgin of the virus\n\n\nExample 2:\n\nFor Multiple keywords **'treatment','option'** keyword when tested against our word embedding model gave the following results.\n\n[('treatments', 0.88392174243927), ('therapies', 0.7969704866409302), ('therapy', 0.7752498984336853), ('drugs', 0.7263898253440857), ('medications', 0.7093261480331421), ('antivirals', 0.6795921325683594), ('regimens', 0.6594418287277222), ('prophylaxis', 0.653971254825592), ('medication', 0.6348516941070557), ('antibiotics', 0.6138178706169128)]\n\nnotable keywords from the word2vec outpur are : 'prophylaxis','therapies' relating to preventive medications . \n\nAs shown below:","440e3525":"Filtered Data to be used to visualize the word cloud in the upcoming cells to understand the importance of topics and vocabs used.","ee9a736d":"Re usable Helper functions","68fc2ee6":"***Building model**\n\n> %%time\n> \n> > TOTAL_TOPICS = 10\n> \n> lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, \n>                                    alpha='auto', eta='auto', random_state=42,\n>                                    iterations=500, num_topics=TOTAL_TOPICS, \n>                                    passes=20, eval_every=None)*","6b506c13":"# PROS and CONS\n\nPROS:\n\n1. Good understanding and iference of the research papers through TSNE and LDA models\n2. Well Trained word embedding models\n3. Accurate keyword extraction with emamples demonstrated with results\n\nCONS:\n\n1. Manual intervention to establish the relationship of search algorithm to derive answers to the questions.","c9fa590e":"**WORD2VEC MODEL **\n\nWORD2VEC EMBEDDING IS USED TO PREDICT THE TARGET WORD(S) FORM THE CONTEXT WORDS(Questions from task) USING CBOW \n\nIn this COVID-19 challenge the questions form the context words, we leverage the context words from the questions to predict the target words to form the keywords, which inturn will lead us to the answers for the respective context words(questions). we are using the CBOW approach in word2vec to acheive this.","7c9fdc10":"Question 5: --> Implementation of diagnostics and products to improve clinical processes --> covid19|diagnostics|tools|advancement|automation|serology","aa866184":"word2vec model loading, we are using the training model by loading it.","e0d78374":"LOAD the Research Dataset","8b6d7a26":"Data Cleaning","76cce60b":"# SOLUTION APPROACH\n\n![covid.JPG](attachment:covid.JPG)","64d96195":"**Inference of some topics **\n\n* Topic 1&5: mainly deals with the virus structure and genome \n* Topic 3: deals with the origin and history \n* Topic 4&10: For public health risk information \n* Topic 9: Has lab research on various subjects","429945ca":"# CONCLUSION","3673c090":"Import libraries","b5d896d0":"**LDA TOPIC WITHOUT WEIGHTS**","c5efc963":"**SEARCH ALGORITHM **\n\nTo extract the relevant research documents from the keywords extarcted ","f36091e2":"**Transforming corpus into bag of words vectors**\n\nWe can now perform feature engineering by leveraging a simple Bag of Words model.","d9b60957":"Import libraries ","336c1632":"# COVID-19 Open Research Dataset (CORD-19) Analysis\n\n![CORD-19.png](attachment:CORD-19.png)\n\n\n*An example of result snippet is shown below, For each tasks a sperate notebook is created with answers to each questions in a task added to a excel*\n\n\n","d0ae441e":"> TOPIC CSV CREATION","d6e4ae8d":"**KEYWORDS EXTRACTION:**","d9896a2f":"As shown in the above steps, the context of the context words 'treatment','options' are well understood by the word2vec model in predicting the target words 'therapy','antivirals','regimens','prophylaxis' which are very relavant in finding the answers to the questions.","dbe2a94a":"In the above word cloud for the topics were mostly about genome of the covid-19 and possibilties of other corona virus genomes and carriers.\n","7c7a3ec7":"Question 1: -->**Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery. --> incubation period|2day|7day|14day****","6735c70b":"**WORD CLOUD VISUALIZATION **\n\nWe have filtered 2019-2020 research papers to understand COVID-19 data through the word cloud visualization.","bfd77502":"Question 2: --> Seasonality of transmission. --> winter season","6a01de2d":"**Creation of Word Cloud Topic wise**","3620fe13":"Read the meta data from meta_data CSV","aab9b0f4":"**Loading data(a)-Complete data by reading uploaded CSV** | CSV file is created by parsing the JSON data","1c32ede7":"Visualise Cleaned Data and removing columns","6a59318d":"Question 3: --> Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic). --> fomites|airborne|nonporous|handrails|desks|floor|fomite|hands|toilets|door|bedrails","e3b973c4":"Build a Bi-gram Phrase Model","c9d546d2":"**Dimensionality Reduction with t-SNE**\n\nUsing t-SNE we can reduce our high dimensional features vector to 2 dimensions. By using the 2 dimensions as x,y coordinates, the text_body_clean can be plotted. t-SNE will attempt to preserve the relations of the higher dimensional data as closely as possible when shrunk to 2D\n\nAnalyse with perplexity of 5000, cluster 10, iteration : 15000\n\nThe optimal cluster parameter is decided after clustering the data from different preplexity and clusters and visualized.","8cacf687":"**TASK 1:**\n\n**Task Details**\nWhat is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?*\n\nSpecifically, we want to know what the literature reports about:\n\n\n1. Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n2. Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\n3. Seasonality of transmission.\n4. Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\n5. Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\n6. Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n7. Natural history of the virus and shedding of it from an infected person\n8. Implementation of diagnostics and products to improve clinical processes\n9. Disease models, including animal models for infection, disease and transmission\n10. Tools and studies to monitor phenotypic change and potential adaptation of the virus\n11. Immune response and immunity\n12. Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\n13. Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\n14. Role of the environment in transmission","23499e63":"**LDA TOPICS WITH TOPIC ID** ","b5f78e6c":"As shown in the above steps, the context of the context words 'covid19' are well understood by the word2vec model in predicting the target words 'wuhan','sars','china' relating to the orgin of the virus which are very relavant in approaching the answers to the questions. ","13c4dfdc":"Looks like we have a lot of unique phrases in our corpus of research papers, based on the preceding output. Several of these terms are not very useful since they are specific to a paper. Hence, we will prune our vocabulary and start removing terms.","9a901d80":"Question 1,3,4 and 5:\n\nKeywords sorted using most similar words from the below result \n\n* Q1- incubation period|2day|7day|14day \n* Q3- fomites|airborne|nonporous|handrails|desks|floor|fomite|hands|toilets|door|bedrails\n* Q4- masks|ppe|respirators|protective equipments|gowns|facemasks|effectiveness|n95\n* Q5- covid19|diagnostics|tools|advancement|automation|serology","82f5d445":"**Answering Approach word embedding method**\n \nWord embedding for the following questions are extracted by passing a set of words from the question to the word embedding model and closest keywords are extracted to further find the closest keywords as shown in the example below:\n\n\n1. Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n2. Seasonality of transmission.\n3. Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n4. Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\n5. Implementation of diagnostics and products to improve clinical processes\n\n**Example:**\n\n**Question 3 : Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).**\n\n***Embedding Step to extract keywords* **\n\n> print(word_model.predict_output_word(['covid19','surfaces'],topn=30))\n> print(word_model.predict_output_word(['covid19','surfaces','nonporous','handrails','desks','floor','fomite'],topn=30))\n> print(word_model.most_similar(['covid19','fomites','airborne','nonporous','handrails','desks','floor','fomite'],topn=30))\n\n***Search Algorithm to extracted related research articles:***\n\n> searc_by_keys_as_excel('fomites|airborne|nonporous|handrails|desks|floor|fomite|hands|toilets|door|bedrails',search_data)\n> \n","d40d6f60":"**Load the LDA Model **"}}