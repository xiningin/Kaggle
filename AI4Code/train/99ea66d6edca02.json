{"cell_type":{"da513669":"code","8274fe24":"code","0a93cc30":"code","b37ce98a":"code","935ccdc9":"code","debf2fa4":"code","2a1e5dbc":"code","9f99adc4":"code","686bebe2":"code","d960b308":"code","741ca7b4":"code","3e036edb":"code","75eca428":"code","27f3c228":"code","5a565b0b":"code","a6ea1145":"code","1055d51f":"code","d7a279a2":"code","af178ec6":"code","10603dc3":"code","e9058fa1":"code","fd12427a":"code","2d974d4c":"code","55e92a55":"code","94da25d5":"code","2a3e7869":"code","d147d99d":"code","f0cd4a40":"code","7542ca3b":"code","5194473f":"code","435aea7c":"code","15dcc818":"code","f90e25cf":"code","24c9c606":"code","9d5f02f4":"code","331142f7":"markdown","a9171795":"markdown","9e8d6b07":"markdown","b81d59d8":"markdown","d8009486":"markdown","959983a9":"markdown","dab7868c":"markdown","fdfdf0e0":"markdown","d04eb3e6":"markdown","085f820a":"markdown","7e7d8abf":"markdown","ac090546":"markdown","51e19d97":"markdown","41572107":"markdown","f187b23d":"markdown","e548a2c2":"markdown","4fe59f98":"markdown","b5eeae74":"markdown"},"source":{"da513669":"import numpy as np\nimport pandas as pd","8274fe24":"data = pd.read_csv(\"\/kaggle\/input\/judul-artikel-online-dengan-label-clickbait\/primary-dataset.csv\")","0a93cc30":"data[\"length\"] = [len(text.split()) for text in data.text]\ndata.head()","b37ce98a":"print(len(data.index))","935ccdc9":"data.label.value_counts().plot.bar()","debf2fa4":"import seaborn as sns\nsns.kdeplot(data.sort_values(by=\"length\", ascending=False).length, shade=True)","2a1e5dbc":"from nltk import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer","9f99adc4":"stopwords_id = pd.read_csv(\"\/kaggle\/input\/indonesian-stoplist\/stopwordbahasa.csv\", header=None)\nstopwords_id.columns = [\"Words\"]","686bebe2":"def preprocess(data):\n    token = word_tokenize(data)\n    token = [text.lower() for text in token]\n    token = [text for text in token if text.isalpha()]\n    token = [text for text in token if not stopwords_id.Words.eq(text).any()]\n    return token","d960b308":"def clean_text(data):\n    token = preprocess(data)\n    words = token[0]\n    for num in range(1,len(token)):\n        words = words + (\" \" + token[num])\n    return words","741ca7b4":"data[\"text_clean\"] = [clean_text(text) for text in data.text]","3e036edb":"count_vec = CountVectorizer(ngram_range=(1,2), min_df = 2)\ntoken = count_vec.fit_transform(data.text_clean)","75eca428":"token","27f3c228":"def most_freq_terms(min_len, max_len):\n    most_freq_vec = CountVectorizer(ngram_range=(min_len,max_len))\n    most_freq_mat = most_freq_vec.fit_transform(data.text_clean)\n    terms = most_freq_vec.get_feature_names()\n    freq = most_freq_mat.toarray().sum(axis=0)\n    df = pd.DataFrame(freq, terms)\n    df.columns = [\"Terms\"]\n    df = df.sort_values(by = \"Terms\", ascending = False)\n    df.head(10).plot.bar()","5a565b0b":"most_freq_terms(1,1)","a6ea1145":"most_freq_terms(2,2)","1055d51f":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB","d7a279a2":"train_x, test_x, train_y, test_y = train_test_split(token, data.label, test_size = 0.2, random_state = 42)","af178ec6":"print(\"Train Size :\", train_x.shape)\nprint(\"Test Size :\", test_x.shape)","10603dc3":"nb = MultinomialNB()\nnb.fit(train_x, train_y)","e9058fa1":"nb.score(train_x, train_y)","fd12427a":"nb.score(test_x, test_y)","2d974d4c":"from keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense","55e92a55":"x_train, x_test, y_train, y_test = train_test_split(data.text, data.label, test_size = 0.2, random_state = 42)","94da25d5":"VOCAB_SIZE = 2000\nMAX_LEN = 50\ntkz = Tokenizer(num_words=VOCAB_SIZE)\ntkz.fit_on_texts(x_train)\nsequences = tkz.texts_to_sequences(x_train)\nsequences = sequence.pad_sequences(sequences, maxlen=MAX_LEN)","2a3e7869":"from tensorflow.random import set_seed","d147d99d":"np.random.seed(42)\nset_seed(42)\nmodel = Sequential()\nmodel.add(Embedding(VOCAB_SIZE, 50, input_length = MAX_LEN))\nmodel.add(LSTM(64))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","f0cd4a40":"model.summary()","7542ca3b":"model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])","5194473f":"model.fit(sequences, y_train, batch_size=128, epochs=10)","435aea7c":"sequences_test = tkz.texts_to_sequences(x_test)\nsequences_test = sequence.pad_sequences(sequences_test, maxlen=MAX_LEN)","15dcc818":"model.evaluate(sequences_test, y_test)","f90e25cf":"def interactive(title):\n    print(\"Headline :\",title)\n    title = tkz.texts_to_sequences([title])\n    title = sequence.pad_sequences(title, maxlen=MAX_LEN)\n    label = model.predict_classes(title)\n    if label[0][0]==0:\n        print(\"This news is not a clickbait\")\n    else:\n        print(\"This news is a clickbait\")","24c9c606":"interactive(\"Berikut 5 fakta mengenai Enzy Storia, nomor 4 sempat menuai kontroversi\")","9d5f02f4":"interactive(\"Kevin De Bruyne cetak 2 gol ke gawang Arsenal\")","331142f7":"Even a simple LSTM model (3 layers and 10 epochs) has already done well. With a small data like this, 76% is a decent accuracy. Surely we can still improve this model by adding more layers, using different batch size, do more epochs, etc.","a9171795":"There are 4599 terms which appear at least in 2 documents, and these terms could be a single word or two words (bigram).","9e8d6b07":"## 6. Summary\n\n1. There are a lot of clickbait headlines in Indonesia\n2. The last few months, the most occuring terms in Indonesian headlines are virus corona, new normal, and pandemi\n3. We can predict a headline is clickbait or not with a 76% accuracy.","b81d59d8":"There are less clickbait contents\/headlines than non-clickbait one, but it's closer to balance.","d8009486":"Let's add a new column called `length`, which is simply the number of words on a text.","959983a9":"The length of the headlines has a normal distribution.","dab7868c":"## 3. Most Frequent Terms\n\nAfter tokenizing the text, we can now see which terms appear the most.","fdfdf0e0":"## 1. Exploratory Data Analysis\n\nFirst let's see the distribution around the data","d04eb3e6":"## Demo\n\nYou can copy this code snippet for interactive demo","085f820a":"## 5. Modelling : Neural Networks\n\nIn most cases, for NLP, you'd prefer to use neural networks rather than models like Naive-Bayes simply because it is a much better algorithm. First we have to do pad sequencing, which is a step of giving index to each words (sequencing), and then normalizing the text length by using padding method.","7e7d8abf":"Here we set the vocabulary size to 2000 and maximum sequence\/length for each text to be 50 words.","ac090546":"## 2. Text Pre-processing\n\nBefore building our NLP model, we have to clean the text first through some steps.\n\n1. Lower-casing\n2. Remove numbers and punctuations\n3. Remove stopwords\n4. Tokenizing","51e19d97":"87.6% Accuracy for train dataset and only 71,7% for test dataset. Not really a good performance.","41572107":"Terms like **new normal**, **virus corona**, and **pandemi** appear a lot since the data was scraped from the last few months.","f187b23d":"Then we can simply fit our RNN model","e548a2c2":"## 4. Modelling : Naive-Bayes\n\nWe can now fit the data into the model. First we will try a simple model aka Naive-Bayes.","4fe59f98":"## Detecting Clickbait Headlines in Indonesia\n\nIn this notebook, we will try to predict whether a headline in Indonesian News is a clickbait or not.","b5eeae74":"There are 3237 texts\/headlines in this data"}}