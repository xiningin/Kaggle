{"cell_type":{"e7b49964":"code","d9739e0f":"code","77275925":"code","b49b5aea":"code","4254d3c8":"code","dba47534":"code","460cfd83":"code","6bd8ffe9":"code","f2764aac":"code","b2f29920":"code","4cffb716":"code","a4d8e4e1":"code","bff48bdf":"code","8ceca9e8":"code","4108393a":"code","fda34334":"code","ee35e0fb":"code","d7e6a1c5":"code","77919a2b":"code","c45bd33b":"code","84923862":"code","79b6135a":"code","0da1fd84":"code","11657866":"code","a8242359":"code","f8a2614d":"code","5fbad17b":"code","f7e72fa1":"code","1070dbbe":"code","4d55c0dc":"code","6038f012":"code","c6b1c974":"code","5a375cd3":"code","4a2c7d2e":"code","99bb8132":"code","841d3699":"code","768d1859":"code","e1f11ce4":"code","947cb22f":"code","ba78c736":"code","66099fba":"code","34305f1c":"code","dd99e0b2":"code","c734fe86":"code","85f29748":"code","4d2fd4fd":"code","6174a527":"code","eecad039":"code","ff55e7f2":"code","976a4d01":"code","01f51972":"code","f5394030":"code","1acb3a20":"code","31d0be47":"code","81f8db87":"code","5e5c8a2d":"code","361fc3e4":"code","1ef30d5b":"code","6d0f721d":"markdown","5e406f47":"markdown","d6cd7ccb":"markdown","186bfeec":"markdown","b6d406da":"markdown","6765503d":"markdown","280e1398":"markdown","a4425e8a":"markdown","655e172f":"markdown","adedeaf3":"markdown","7fc51b96":"markdown","dbd5ee3e":"markdown","7a32636a":"markdown","3e54a688":"markdown","c588be65":"markdown","5af465a2":"markdown","ba0b5e3c":"markdown","dd6a4350":"markdown","82d191e4":"markdown","431726fc":"markdown","67b6dd7c":"markdown"},"source":{"e7b49964":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are aavailable in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d9739e0f":"# install pyspark\n!pip install pyspark","77275925":"# list the files in the present working directory i.e. in which this code is present\n!ls","b49b5aea":"# print the full path of present working directory\n!pwd","4254d3c8":"# SparkContext is used to create and initialize Pyspark\nfrom pyspark import SparkContext\n# provide the appName\nsc = SparkContext(appName='covid19anaysis')\n# if something is printed like the version of the Spark; it indicates that the spark successfully initiated\nsc","dba47534":"# we can also run SQL queries with same performance as that of using Pyspark modules\/functions\n# so to enable such functionalities and to use the pyspark functions we initialize it\nfrom pyspark.sql import SQLContext\n# initializing SQLContext\nsqlContext = SQLContext(sc)\n# to check if it got initialized or not\nsqlContext","460cfd83":"# we will use some of the functions present in pyspark like sum, min, max\n# we import them using alias and not like - from pyspark.sql.functions import *\n# the reason being; there are already functions sum, min, max, etc. present in python and it will override them\nimport pyspark.sql.functions as F\n# this contains the data-types avilable in the pyspark\nimport pyspark.sql.types as T","6bd8ffe9":"# load the data using pandas\npdf = pd.read_csv('\/kaggle\/input\/us-counties-covid-19-dataset\/us-counties.csv')\npdf.head(2)","f2764aac":"# using createDataFrame we can covert pandas dataframe into pyspark\nsdf = sqlContext.createDataFrame(pdf)\n# similar to head we use show in pyspark\n# it takes two inputs; first (int) the number of rows to display and second (boolean) to truncate the data or not\nsdf.show(5)","b2f29920":"# as mentioned above we can write SQL queries in pyspark\n# and to achieve so, we first have to save its schema into temporary tables (just an alias)\n# this creates a table with name 'covid_data'\nsdf.registerTempTable('covid_data')","4cffb716":"# to check the schema (columns & their data types)\nsdf.printSchema()","a4d8e4e1":"# to print the number of rows in the dataset\nsdf.count()","bff48bdf":"# to get the latest date available in the dataset\n# we use agg function to achieve the same\nlatest_date = sdf.agg(F.max('date').alias('max_data'))\nlatest_date.show()","8ceca9e8":"# we can get the same results using the sql queries\nlatest_date_sql = sqlContext.sql(\n    \"\"\" SELECT max(date) as max_data FROM covid_data\"\"\"\n)\nlatest_date_sql.show()","4108393a":"# both ways we are returned a spark dataframe\ntype(latest_date) ","fda34334":"type(latest_date_sql) ","ee35e0fb":"# to extract the value we use collect function\n# BUT, collect() should be only used on small data sizes as all the data is collected on the master node\n# since master nodes don't usually have such memory; it will fail (crash)\nlatest_date = latest_date.collect()\n# the data returned is in the format of lists in lists to be more specific list of namedTuples\n# namedTuples are available in collections and we can access the values inside it by providing parameter-name\nlatest_date","d7e6a1c5":"# there is only single item in the list and we will refer it using index - 0\n# also, as mentioned above we can extract values from named tuple using the names of parameters\/columns\nlatest_date = latest_date[0]['max_data']\nlatest_date","77919a2b":"# so the latest date avilable is 2020-04-13\n# now, we fill filter the data where this date is present\nsdf_filtered = sdf.where(\n    \"date = '{}'\".format(latest_date)\n)\n\nsdf_filtered.show(2)","c45bd33b":"# now lets check the number of records in the filtered data\nsdf_filtered.count()","84923862":"# I will be drawing parallels between the sql and pyspark for easy understanding\n# we can achieve the same using SQL and it is as follows\nlatest_date_sql = sqlContext.sql(\n    \"\"\" SELECT * FROM covid_data WHERE date = '{}'\"\"\".format(latest_date)\n)\nlatest_date_sql.show(10)","79b6135a":"# checking the count of the records\nlatest_date_sql.count()","0da1fd84":"# since we did not mention any grouping level this will return the statistics at overall level\noverall_stats = sdf_filtered.agg(\n    F.sum(\"cases\").alias(\"total_cases\"), # to sum the values in column 'cases'\n    F.sum(\"deaths\").alias(\"total_deaths\"),\n    F.count(\"*\").alias(\"number_of_records\"), # to count the number of records in the dataset\n    F.countDistinct(\"county\").alias(\"number_of_counties\"), # to get the distinct count of counties in column 'county'\n    F.countDistinct(\"state\").alias(\"number_of_states\")\n)\n\noverall_stats.show(1, False)","11657866":"# sort the data at column-'county' and show top 10 records\nsdf_filtered.orderBy(\"county\").show(10, False)","a8242359":"# register the dataframe as a table\nsdf_filtered.registerTempTable(\"covid19_20200413\")","f8a2614d":"# achiving the same using SQL queries\nsqlContext.sql(\n    \"\"\"\n        SElECT\n            SUM(cases) as number_of_cases,\n            SUM(deaths) as number_of_deaths,\n            COUNT(*) as number_of_records,\n            COUNT(DISTINCT county) as number_of_counties,\n            COUNT(DISTINCT state) as number_of_states\n        FROM\n            covid19_20200413\n    \"\"\"\n).show(1, False)","5fbad17b":"# here, we are grouping the data at 'county' level and this will take sum of cases, states and others\n# and summarize it at 'county' level \ncounty_summary = sdf_filtered.groupBy(\n    \"county\"\n).agg(\n    F.sum(\"cases\").alias(\"total_cases\"),\n    F.sum(\"deaths\").alias(\"total_deaths\"),\n    F.count(\"*\").alias(\"number_of_records\"),\n    F.countDistinct(\"state\").alias(\"number_of_states\")\n)\n\n# order the county in alphabetical order and show top 20 records\ncounty_summary.orderBy(\"county\").show(20, False)","f7e72fa1":"# ideally for each county there should be no state with same name\n# so, here we are comparing the 'number_of_records' and 'number_of_states'\n# and filtering only those records where both does not match\nfiltered_county_rec = county_summary.where(\n    F.col(\"number_of_records\") != F.col(\"number_of_states\")\n)\n\n# checking the count of filtered records\n# here count is zero - indicating that there are no duplicates\nfiltered_county_rec.count()","1070dbbe":"# so the dataframe returned is empty - containing no rows\nfiltered_county_rec.show()","4d55c0dc":"# achieving the same using SQL queries \nsqlContext.sql(\n    \"\"\"\n        SElECT\n            county,\n            SUM(cases) as number_of_cases,\n            SUM(deaths) as number_of_deaths,\n            COUNT(*) as number_of_records,\n            COUNT(DISTINCT state) as number_of_states\n        FROM\n            covid19_20200413\n        GROUP BY\n            county\n        ORDER BY\n            county\n    \"\"\"\n).show(10, False)","6038f012":"# filter data for county - Adair & Addison and prepare our left_table\nleft_table = county_summary.where(\n    \"county in ('Adair', 'Addison')\"\n)\nleft_table.show(5, False)","c6b1c974":"# filter data for county - Ada & Accomack and prepare our right_table\nright_table = county_summary.where(\n    \"county in ('Ada', 'Accomack')\"\n)\nright_table.show(5, False)","5a375cd3":"# applying inner join\n# county_summary contains all the counties and their statistics\ninner_table = county_summary.join(\n    left_table,\n    on=[\"county\"],\n    how=\"inner\"\n)\n\ninner_table.show(100, False)","4a2c7d2e":"# applying left join\nleft_table_joined = left_table.join(\n    right_table,\n    on=[\"county\"],\n    how=\"left\"\n)\n\nleft_table_joined.show(100, False)","99bb8132":"# applying right join\nright_table_joined = left_table.join(\n    right_table,\n    on=[\"county\"],\n    how=\"right\"\n)\n\nright_table_joined.show(100, False)","841d3699":"# applying outer join or full join\nouter_table_joined = left_table.join(\n    right_table,\n    on=[\"county\"],\n    how=\"outer\"\n)\n\nouter_table_joined.show(100, False)","768d1859":"# get percentage # of cases for each state in Adair\n# filter data for county - Adair\nadir_overall = county_summary.where(\n    \"county in ('Adair')\"\n)\nadir_overall.show(5, False)","e1f11ce4":"# join the Adair summary to its states\nperc_cases_statewise = sdf_filtered.join(\n    adir_overall,\n    on=\"county\",\n    how=\"inner\"\n)\n\nperc_cases_statewise.show(10, False)","947cb22f":"# calculate percentage of cases and deaths\nperc_cases_statewise = perc_cases_statewise.withColumn(\n    \"perc_cases\",\n    F.col(\"cases\")\/F.col(\"total_cases\")\n).withColumn(\n    \"perc_deaths\",\n    F.col(\"deaths\")\/F.col(\"total_deaths\")\n)\n\nperc_cases_statewise.show(10, False)","ba78c736":"# this is required to apply parition by clause\nfrom pyspark.sql.window import Window","66099fba":"# let's print out few records to check the structure of the dataframe\ncounty_summary.show(2, False)","34305f1c":"# we will create a new column and store the new dataframe as county_summary_ranked\n# withColumn is used to create new columns; where first parameter is the column name and second the values for the column\ncounty_summary_ranked = county_summary.withColumn(\n    \"rank\", # column name\n    # rank function assigns a rank starting from 1 and orderBy condition mentions that rank will be based on the column - total_cases\n    # by default the sorting is done in ascending order\n    F.rank().over(Window.orderBy(\"total_cases\")) \n)\n\n# print out some of the records\ncounty_summary_ranked.show(5, False)","dd99e0b2":"county_summary_ranked = county_summary_ranked.withColumn(\n    \"rank_desc\",\n    # if we want high end values to be assigned the lower rank, we sort the data in descending order using F.desc function\n    F.rank().over(Window.orderBy(F.desc(\"total_cases\")))\n)\n\n# also, let's order the data by total_cases in descending order\ncounty_summary_ranked.orderBy(\"total_cases\", ascending=False).show(5, False)","c734fe86":"# to achieve this we will use the state level dataset containing the data for the latest date\nsdf_filtered.show(2, False)\n\n# also, let's check the count\nsdf_filtered.count()","85f29748":"# we will create a new dataframe - ranked_states\nranked_states = sdf_filtered.withColumn(\n    \"state_rank\",\n    # now we will also specify the partition clause - telling it to create rank for each of those county seperately\n    F.rank().over(Window.partitionBy(\"county\").orderBy(F.desc(\"cases\")))\n)\n\n# show top 30 records ordered on county and state_rank\nranked_states.orderBy(\"county\", \"state_rank\").show(30, False)","4d2fd4fd":"# now, let's filter out only the top state for each of the county\nranked_states_filtered = ranked_states.filter(\n    # since, the top state has been assigned rank-1, we can filter states where state_rank is 1\n    \"state_rank = 1\"\n)\n\n# let's check the count which should be equal to the number of counties in the dataset\nranked_states_filtered.count()","6174a527":"# also, let's check the results\nranked_states_filtered.orderBy(\"county\", \"state_rank\").show(50, False)","eecad039":"# assigning the parition by clause in a variable - w\npartition_clause = Window.partitionBy(\"county\")","ff55e7f2":"# let's now get the total, average and maximum county cases across each of their respective states\n# we can use .withColumn in a chain format, that is one after the other and all are executed in a sequential order\nranked_states = ranked_states.withColumn(\n    \"country_total_cases\",\n    # we will use the parition_clause we created above\n    F.sum(\"cases\").over(partition_clause)\n).withColumn(\n    # here we are calculating the % of cases in a state i.e. state_cases\/respective_county_cases\n    \"perc_total_cases\",\n    F.col(\"cases\")\/F.col(\"country_total_cases\")\n).withColumn(\n    \"country_avg_cases\",\n    # get county average cases\n    F.avg(\"cases\").over(partition_clause)\n).withColumn(\n    # get county max caaes\n    \"country_max_cases\",\n    F.max(\"cases\").over(partition_clause)\n)\n\n# now, let's print out the results\nranked_states.orderBy(\"county\", \"perc_total_cases\").show(50, False)","976a4d01":"# register the dataframe as a table\nranked_states.registerTempTable(\"ranked_states\")","01f51972":"# SQL query to get the Minimum number of cases for each county\nranked_states_sql = sqlContext.sql(\"\"\"\n    SELECT\n        *,\n        MIN(cases) OVER (PARTITION BY county) as country_min_cases\n    FROM\n        ranked_states\n\"\"\")\n\n# we can remove a column from a dataframe using drop() function\nranked_states_sql = ranked_states_sql.drop('country_total_cases')\n\n# let's print out the results\nranked_states_sql.orderBy(\"county\", \"perc_total_cases\").show(50, False)","f5394030":"# here we will define a class and will create its object to access its methods which will return the summaries at various levels\nclass Covid19():\n    \n    # constructor: this will be the first to execute, when an object of the class is created\n    # self: is the default parameter and will be there in all the methods of the class. It points to the class itself\n    # other parameters are sqlContext and filename. Since these does not take any default values, they should be passed while,\n    # creating an object of the class\n    def __init__(self, sqlContext, filename):\n        \n        # input the spark context\n        self.sqlContext = sqlContext\n        # input the file\n        self.filename = filename\n        \n        # create the spark dataframe from the file\n        pdf = pd.read_csv(self.filename)\n        # convert pandas dataframe in pyspark datafram and save it in a class variable\n        self.covid19_dataset = self.sqlContext.createDataFrame(pdf)\n    \n    \n    def get_date(self, min_max=\"max\"):\n        \"\"\"\n            Return either the minimum or maximum date from the dataset.\n            \n                Parameter:\n                    min_max (string): Takes either 'min' or 'max' as input\n                \n                Return:\n                    date (string): returns the date in the format 'yyyy-mm-dd'\n        \"\"\"\n        \n        # ensure the value is either 'min' or 'max'\n        assert min_max in ('min', 'max')\n        \n        if min_max == 'min':\n            return self.covid19_dataset.agg(F.min(\"date\").alias(\"min_date\")).collect()[0]['min_date']\n        else:\n            return self.covid19_dataset.agg(F.max(\"date\").alias(\"max_date\")).collect()[0]['max_date']\n    \n    \n    def get_overall_numbers(self, date='max'):\n        \"\"\"\n            This will return the overall summary that is the number of cases, deaths and distinct number of counties and states.\n                \n                Parameters:\n                    date (string): Takes one of the following as input 'max', 'min', date in format 'yyyy-mm-dd' or None\n                    \n                Returns:\n                    overall_summary(spark dataframe): Returns the summary according to the date provided\n        \"\"\"\n        \n        # get the max\/min date\n        if date == \"max\" or date == 'min':\n            date = self.get_date(date)\n            \n        # if no date is provided then filter on date is not required else filter on the date\n        if date == None:\n            filtered_data = self.covid19_dataset\n        else:\n            # filter the data\n            filtered_data = self.covid19_dataset.filter(F.col(\"date\") == date)\n            \n        # group on date and calculate the metrics\n        overall_summary = filtered_data.groupBy(\n                                \"date\"\n                            ).agg(\n                                F.sum(\"cases\").alias(\"total_cases\"),\n                                F.sum(\"deaths\").alias(\"total_deaths\"),\n                                F.countDistinct(\"county\").alias(\"num_of_counties\"),\n                                F.countDistinct(\"state\").alias(\"num_of_states\"),\n                            )\n        \n        # return the summary dataframe\n        return overall_summary\n    \n    \n    def get_county_numbers(self, date='max', county=None):\n        \"\"\"\n            This will return the overall summary that is the number of cases, deaths and distinct number of counties and states.\n                \n                Parameters:\n                    date (string): Takes one of the following as input 'max', 'min', date in format 'yyyy-mm-dd' or None\n                    county (string): Take either None or the name of a county\n                    \n                Returns:\n                    county_summary(spark dataframe): If county is not provided then data for all the counties will be \n                                                        returned else for the provided county\n        \"\"\"\n        \n        # get the max\/min date\n        if date == \"max\" or date == 'min':\n            date = self.get_date(date)\n        \n        # if no date is provided then filter on date is not required else filter on the date\n        if date == None:\n            filtered_data = self.covid19_dataset\n        else:\n            filtered_data = self.covid19_dataset.filter(F.col(\"date\") == date)\n            \n            \n        # filter data for a county if provided else no filtering on data is required\n        if county == None:\n            pass\n        else:\n            filtered_data = filtered_data.filter(F.col(\"county\") == county)\n            \n        # now group on both date and county. This will return the metrics for each county for every date\n        county_summary = filtered_data.groupBy(\n                                \"date\", \"county\"\n                            ).agg(\n                                F.sum(\"cases\").alias(\"total_cases\"),\n                                F.sum(\"deaths\").alias(\"total_deaths\"),\n                                F.countDistinct(\"state\").alias(\"num_of_states\")\n                            )\n        \n        # return the summary\n        return county_summary","1acb3a20":"# now create an object of the class\n# pass the required parameters\ncovid19_class_obj = Covid19(\n                        sqlContext= sqlContext, \n                        filename='\/kaggle\/input\/us-counties-covid-19-dataset\/us-counties.csv')\n\n# now let's check the type of the object\ntype(covid19_class_obj)","31d0be47":"# let's now access the spark dataframe using the class object\ncovid19_class_obj.covid19_dataset.show(2, False)","81f8db87":"# also let's use the method-get_overall_numbers() and get the metrics for the latest date (max date)\ncovid19_class_obj.get_overall_numbers(date='max').show()","5e5c8a2d":"# also, let's see the output if we provide None in the date\ncovid19_class_obj.get_overall_numbers(date=None).orderBy(F.desc(\"date\")).show(5, False)","361fc3e4":"# let's now get the cases for all the counties for the latest date\ncovid19_class_obj.get_county_numbers(date='max', county=None).show(10, False)","1ef30d5b":"# and lastly for all the countuies for all the dates\ncovid19_class_obj.get_county_numbers(date=None, county=None).orderBy(\"county\", \"date\").show(100, False)","6d0f721d":"## Setup","5e406f47":"## Partition By Clause in SQL","d6cd7ccb":"## Summary at County level","186bfeec":"# Load the Dataset","b6d406da":"# Groupby and Aggregation","6765503d":"Note: quick insight here is that though Kentucky has the highest % of cases (47%), it has 50% of the deaths","280e1398":"## [This notebook has been created for a youtube series - Introduction to Pyspark (click to visit)](https:\/\/www.youtube.com\/channel\/UCHpga4QsQPB9PUpasfyf8cg?sub_confirmation=1)\n\n\n### Following topics have been covered here:\n1. Setting up the environment\n2. Loading the dataset into Pyspark\n3. Applying Filters\n4. Group by and Aggregation\n5. Joins  (left, right, inner, full)\n6. Partition By & Window Function\n7. Automating the code","a4425e8a":"## Overall Statistics","655e172f":"**Hope you enjoyed learning and mainly learnt through this series. I will shortly also cover, how to apply machine learning techniques using Pyspark.**<br><br>\n**If you find this valuable, please like my vidoes, subscribe to my channel and also share it with your friends.**","adedeaf3":"# Filters","7fc51b96":"**Now, we will create the rank for the states for each of the county seperately** <br>\nThis means, we will seperate the data for each county and will run the rank function like above for each of those partitions seperately. So, in each county there will be a state given rank-1","dbd5ee3e":"**We will be giving a rank to the Counties based on the total number of cases they have**","7a32636a":"*Source: https:\/\/dzone.com\/articles\/how-to-perform-joins-in-apache-hive\n\n![image.png](attachment:image.png)","3e54a688":"# Joins","c588be65":"# End Note","5af465a2":"* Overall Numbers\n* Overall Daily Numbers\n* County Overall Numbers\n* County % cases and cumulative numbers\n* Particular County Daaily numbers\n* Category Top states \n<br>\nmany others...","ba0b5e3c":"**As we can see above state having the highest cases in each county has been given the lowest rank (1)**","dd6a4350":"# Parition By & Window Function","82d191e4":"# Import and Install required packages","431726fc":"# Automation","67b6dd7c":"**We can also parameterize the window statements and use it where-ever required, instead of re-writing them again and again**"}}