{"cell_type":{"25efa50d":"code","55df74e8":"code","4778bcb6":"code","2ed4ddec":"code","eb62dd28":"code","b776d076":"code","19a05d59":"code","d2e158ea":"code","15ee3d25":"code","84ffeb3a":"code","11577aeb":"code","ef51b085":"code","cc3a219c":"code","f3aa9d05":"code","586d1b74":"code","165edc39":"code","2e157ca6":"code","797dc3ac":"code","e2edb2d3":"code","fe1e7bdb":"code","18a02a27":"markdown","9504e2d9":"markdown","d8c874c5":"markdown","561cc89c":"markdown","afa44e05":"markdown","9fbae6d8":"markdown","4cae9143":"markdown","8bc2ddc5":"markdown","ab1e9af7":"markdown","39fae56b":"markdown","fa20c7c1":"markdown","f26d70a8":"markdown","8b7bfd99":"markdown","8f0d6adb":"markdown","1262830e":"markdown","8bfd45a0":"markdown","50769cf8":"markdown","8becd1d0":"markdown","e8f5c09c":"markdown"},"source":{"25efa50d":"#Importing required libraries\n\nimport numpy as np\nimport pandas as pd\nfrom random import seed\nfrom random import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport inspect\nfrom bokeh.io import curdoc, show\nfrom bokeh.models import ColumnDataSource, Grid, LabelSet, LinearAxis, Plot, Wedge, CategoricalColorMapper\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import row\nfrom matplotlib.colors import rgb2hex as rgb2hex\nimport matplotlib.cm as cm\n\noutput_notebook()","55df74e8":"#Some helpers for inspecting variables\ndef print_var(x):    \n    print(retrieve_name(x) + ': ' + str(x))\n\ndef retrieve_name(var):\n    for fi in reversed(inspect.stack()):\n        names = [var_name for var_name, var_val in fi.frame.f_locals.items() if var_val is var]\n        if len(names) > 0:\n            return names[0]    ","4778bcb6":"def generate_P(N = 3):\n    '''\n    Generates a stochastic matrix with the specified size\n    '''\n    P = np.random.rand(N, N)   \n    #Divide each row by sum of the entire row to normalize row to 1\n    P = P\/P.sum(axis=1)[:,None]\n    return P\n\ndef noisify(base_p, noise_factor = 0.05):\n    '''\n    Adds random noise to given matrix\n    '''\n    (M,N) = base_p.shape\n    P = np.random.rand(N, N)   \n    #Divide each row by sum of the entire row to normalize row to 1\n    sampled = base_p + P * noise_factor\n    sampled = sampled\/sampled.sum(axis=1)[:,None]\n    return sampled","2ed4ddec":"#Let's take a peek at an example stochastic matrix\nP = generate_P()\nnoisy = noisify(P)\nprint(P)\nprint(noisy)\nprint(noisy.sum(axis=1))#Make sure every row sums up to 1","eb62dd28":"def iterate_process(current_state, P):\n    x,y = P.shape\n    if(x != y):\n        raise ValueError('State transition matrix needs to be a square matrix')\n        \n    if(current_state >= x):\n        raise ValueError(f\"Current state should be a in {range(x)}\")\n    \n    if(current_state < 0):\n        raise ValueError('Current state should be a positive integer')\n    \n    #Get the row with index of the current state, mask other rows out\n    #That row is the multinomial  probability mass function, that lists the \n    #discrete probability values for transitioning to target states from the current state\n    \n    pmf = np.ma.masked_values((P[current_state]), 0.0) \n    \n    \n    cdf = np.cumsum(pmf) #Compute cdf from pmf\n    cdf = np.insert(cdf, 0, 0) #Insert 0 to the 0th position\n        \n    r = np.random.uniform(0.0, 1.0) #Sample from uniform [0,1]\n    m = (np.where(cdf < r))[0] #Get indices of elements with value less than 'r'\n    next_state=m[len(m)-1] #Next state is the \n    return next_state","b776d076":"def generate_sequence(P, l=1000):\n    state = 0\n    sequence = []\n    \n    for i in range(l):\n        #Appending each new sample to the array to obtain a time-series\n        sequence.append(state)\n        state = iterate_process(state, P)\n        \n    return sequence","19a05d59":"#Define some functions to be used for visualizing a single sequence along with it's stochastic matrix\n\ndef get_sample_figure(samples, h, w):\n    '''\n    Generates a heatmap-like figure for visualizing sequences\n    '''\n    plot_samples = figure(\n    title=\"# of Vehicles vs Time\",\n    x_axis_label=\"Sample Index\",\n    width=w,\n    height=h)    \n    \n    palette=['#1f59b9','#f9b749','#9528c7']\n    colors = [palette[i] for i in samples]\n    index = range(len(samples))\n    plot_samples.rect(index, y=0.3, width=1,\n       height=1, color=colors)\n    plot_samples.yaxis.visible=False\n    \n    return plot_samples\n\ndef get_matrix_figure(P, h, title = 'Stochastic Matrix'):\n    '''\n    Plots a matrix as a heatmap\n    '''\n    map_range = ['s0', 's1', 's2']\n    \n    plot_matrix= figure(\n        title=title, width=h,\n        height=h, toolbar_location=None,\n        x_range=map_range, y_range=map_range\n    )\n    x = [0.5,1.5,2.5,0.5,1.5,2.5,0.5,1.5,2.5]\n    y = [0.5,0.5,0.5,1.5,1.5,1.5,2.5,2.5,2.5]\n    \n    cmap=cm.jet\n    matrix_colors = [rgb2hex(cmap(c)) for c in P.flatten()]\n    plot_matrix.rect(x, y, color=matrix_colors, width=1, height=1)\n\n\n    x_annot = [0,0,0,1,1,1,2,2,2]\n    y_annot = [0,1,2,0,1,2,0,1,2]\n    \n    label_source = ColumnDataSource(data=dict(x=x, y=y, vals= np.round(P.flatten(),3)))   \n    labels = LabelSet(x='x', y='y', text='vals',\n                      x_offset=-12, y_offset=-5, source=label_source,\n                      render_mode='css',text_font_size='1em', text_color='white')\n\n\n    plot_matrix.add_layout(labels)\n    #plot_matrix.add_tools(HoverTool(tooltips=None))\n    return plot_matrix\n\ndef plot_sequence(P, samples, h=175, w=1000):\n    \n    plot_samples = get_sample_figure(samples, h, w)\n    plot_matrix = get_matrix_figure(P, h)\n\n    show(row(plot_matrix, plot_samples))\n    ","d2e158ea":"#Generate a sequence to simulate # of cars passing through a road with 3 lanes at any successive measurements\n\nN = 3 # |StateSpace|\nL=250 #Length of sequence (# of measurements)\n\nnp.random.seed(2) # random seed for reproducability\nP = generate_P(N) #distribution for markov chain\n\ntraffic = generate_sequence(P,L)\n\nplot_sequence(P, traffic)","15ee3d25":"N = 3 # |StateSpace| \nobservations = []\nsequence = []\nP_s = []\ntraffic_states = []\n\n\np_quiet = [ #Calm-traffic\n    0.85, 0.1, 0.05,\n    0.95, 0.04, 0.01,\n    0.95, 0.03, 0.02]\n\np_usual = [ # casually-active\n    0.2, 0.6, 0.2,\n    0.1, 0.7, 0.3,\n    0.15, 0.55, 0.3]\n\np_busy =[ # extremely busy\/congestion\n    0.05, 0.25, 0.7,\n    0.05, 0.35, 0.6,\n    0.01, 0.09, 0.9]\n\nregimes = [\n    np.array(p_quiet).reshape(3,3),\n    np.array(p_usual).reshape(3,3),\n    np.array(p_busy).reshape(3,3)\n]\n\nfor i in range(100):\n    regime = np.random.choice([0, 1, 2])  #pick a random regime\n    P = noisify(regimes[regime], noise_factor=0.2)\n    P_s.append(P)\n    traffic_states.append(regime)\n    sequence = generate_sequence(P, l=1000) \n    observations.append(sequence)\n    \n    \nplt.figure(figsize=(25, 6))\ncmap = sns.color_palette(\"prism\", N)\n\n#visualize first 150 samples of each sequence\nsns.heatmap(np.asarray(observations)[:25,:150], cmap=cmap, annot=False)","84ffeb3a":"#Select 2 sequences from each regime and visualize\ntraffic_states = np.array(traffic_states)\nquiet_observations = np.where(traffic_states == 0)[0][:2] #time series with quite traffic profile\nusual_observations = np.where(traffic_states == 1)[0][:2] #usual traffic\nbusy_observations = np.where(traffic_states == 2)[0][:2]  #busy traffic","11577aeb":"index0 = quiet_observations[0]\nindex1 = quiet_observations[1]\n#plot_sequence(P,samples)\nplot_sequence(P_s[index0], observations[index0][:150])\nplot_sequence(P_s[index1], observations[index1][:150])","ef51b085":"index0 = usual_observations[0]\nindex1 = usual_observations[1]\n\nplot_sequence(P_s[index0], observations[index0][:150])\nplot_sequence(P_s[index1], observations[index1][:150])","cc3a219c":"index0 = busy_observations[0]\nindex1 = busy_observations[1]\n\nplot_sequence(P_s[index0], observations[index0][:150])\nplot_sequence(P_s[index1], observations[index1][:150])","f3aa9d05":"def estimate_p(sequence, L=0):\n    '''\n    Function for estimating the stochastic matrix of a given Markov chain realization (observations)\n    Optional parameter L can be used for specifying the number of observations to use in estimation\n    '''\n    #make sure the length to be used in estimation does not exceed the available length\n    #use entire data available if L is not specified\n    Ls = len(sequence)    \n    if L==0 or L > Ls:\n        L = Ls\n    \n    #get the number of distinct states in the sequence (State Space)\n    N = np.unique(seq).shape[0]\n    \n    p_hat = np.zeros((N,N))\n    \n    #loop through the sequence and count the state transitions\n    previous_state = sequence[0]\n    for state in sequence[1:L]:\n        current_state = state\n        p_hat[previous_state, current_state] +=1\n        previous_state = current_state\n    \n    p_hat = p_hat\/L\n    p_hat = p_hat\/p_hat.sum(axis=1)[:,None]\n    \n    return p_hat","586d1b74":"#Let's try the estimation function\n\np = noisify(regimes[0], noise_factor=0.2)\nseq = generate_sequence(p, l=5000)\n\n#Let's try the estimation function \n\np_hat = estimate_p(seq)\n\np_figure = get_matrix_figure(p, h=175, title='Original')\np_hat_figure = get_matrix_figure(p_hat, h=175, title='Estimated')\nshow(row(p_figure, p_hat_figure))","165edc39":"#different values for the parameter to try\nlengths = [50, 100, 200, 500, 1000, 2000, 3000, 5000]\n\np_hats = []\nerrors = []\n\nfor L in lengths:\n    p_hat = estimate_p(seq, L)\n    p_hats.append(p_hat)\n    \n    #error matrix\n    p_diff = p - p_hat\n    \n    #Sqrt of sum of all components of the error matrix\n    error = np.sqrt(np.sum(p_diff**2))\n    errors.append(error)\n\n\nfig = figure(title=\"Estimation Error\", x_axis_label='# of Observations Used (L)', y_axis_label='Sum of Squared Error')\nfig.line(x=lengths, y=errors)\nshow(fig)","2e157ca6":"L=1000 \np_hats = []\n\nfor observation in observations:\n    p_hat = estimate_p(observation, L)    \n    p_hats.append(p_hat.flatten())\n    \nX = np.array(p_hats)","797dc3ac":"#Create a dataframe for investigating the values\nx_df = pd.DataFrame(X, columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9'])\nx_df.head()","e2edb2d3":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\n\np_components = pca.fit_transform(X)\n\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)","fe1e7bdb":"cmap={0:'navy',1:'green',2:'red'} #color coding the original traffic states\ncolors = [cmap[c] for c in traffic_states]\n\np = figure()\np.circle(p_components[:,0], p_components[:,1], size=5, color=colors, alpha=0.5)\n\nshow(p)","18a02a27":"\n<a id='simulate-markov-chain'><\/a>\n# Simulating Markov Chains","9504e2d9":"# Using Markov Chain Transition Probabilities as Features for Machine Learning\n\nThis notebook demonstrates the estimation of Markov chain transition probabilities and using the estimated values as a feature vector for further processing and machine learning tasks. \n\n\n### Table of Contents\n* [Simulating Markov Chain](#simulate-markov-chain)\n    * [Generating Samples from a Model](#generate-sequence-from-p)\n    * [Visualize Sequence](#visualize-sequence)\n    * [Visualizing Multiple Sequences and Inspecting Similarities](#visualize-multiple-sequences)\n    * [Visualizing Similarities Between Pairs of Time Series](#visualize-similarities)\n* [Estimating the Stochastic Matrix and Feature Vector for Time Series](#estimating-matrix)\n    * [Estimating the Stochastic Matrix From Observations](#estimation-from-observation)\n    * [Estimation Accuracy](#estimation-accuracy)\n    * [Calculating Features for Time Series](#calculating-features)\n    * [Calculating the Principal Components for Visualizing the Clusters](#pca-and-visualization)\n    \n### Introduction\n\nAs majority of the machine learning algorithms require the input data in a tabular, fixed length normalized vector format, performing ML tasks on arbitrary length time series data is often not straightforward. This kernel demonstrates the use of Markov chain modelling to convert an arbitrary-length observed sequence into a fixed length feature vector by capturing the most of the information available in the observed sequence. Once the feature vector is computed, it can be used in many different data science tasks and classical, highly explainable ML algorithms for tasks such as, visualization, clustering, classification etc.\n\n\nAccording to Wikipedia definition;\n> A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. \n\nA Markov chain is useful for modelling sequences of observations\/measurements\/events or any other stochastic process with Markov assumption, meaning that, the probability distribution of the next value in the sequence, only depends on the previous value and does not depend on the past history, as can be seen in below equation:\n\n$$P(X_n=x_n|X_{n-1} = x_{n-1}, X_{n-2}=x_{n-2}{\\dotsb}X_0=x_0)=P(X_n=x_n|X_{n-1}=x_{n-1})$$\n\n\nMarkov chains can be used for modelling a broad range of phenomena such as;\n\n - Evolution of state of a system  \n - Successive actions taken by a user\n - Gene and protein sequences\n \n\nA discrete time, finite state Markov chain can be modeled by its ___Stochastic matrix___, that is a square matrix used to describe the 'transition probabilities' of the chain between pairs of states. Assuming that our random process can be in either one of two states, \n$$a_1,a_2$$\n\nand has the stochastic matrix shown below:\n\n$$P=\n\\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n$$\n\nThis matrix can be interpreted as;\n- When the system is in state $a_1$, the next state of the system will be $a_1$ with probability $a_{11}$\n- When the system is in state $a_1$, the next state of the system will be $a_2$ with probability $a_{12}$\n- When the system is in state $a_2$, the next state of the system will be $a_1$ with probability $a_{21}$\n- When the system is in state $a_2$, the next state of the system will be $a_2$ with probability $a_{22}$\n\n\n$$P=\n\\begin{pmatrix}\n0.1 & 0.9 \\\\\n0.7 & 0.5\n\\end{pmatrix}\n$$\n\n\nNow that we have the basics, let's get to the code!","d8c874c5":"As can be seen in the figure above, the estimation error decreases as more observations are taken into account for estimation. In the next cell, we will estimate the stochastic matrices for previously generated 100 observations. For a reasonable balance between estimation accuracy and duration, we will move forward with 1000 samples for estimation.\n","561cc89c":"<a id='visualize-sequence'><\/a>\n### Visualizing Sequences and Stochastic Matrices\n\nAs the next step, we can visualize the results to gain some insigth into the random process we have synthesized. \n\nFor concreteness of the example let's assume that these sequences are generated by a real case as described below:\n\nAssume that there's an IoT device installed on a road with 2 lanes, that observes how many vehicles are passing in front of it and reports this information to a cloud service, at regular time intervals. Since the road has 2 lanes, the number of vehicles that can appear in device's observation area can be either 0, 1 or 2 at most. \n\nAfter collecting these measurements, the resulting data would look something like below:\n\n$$[0,1,0,1,2,1,1,0,0,{\\dotsb}2,1,1,2,1,2,2,1,2]$$\n\nSeveral remarks can be made by inspecting the sequence.\n","afa44e05":"#### Two sequences generated by a busy traffic profile","9fbae6d8":"<a id='generate-sequence-from-p'><\/a>\n### Generating Samples from a Model\n\nIn the next cell, we will define a function to iterate a markov chain, that takes current state of the chain and the stochastic matrix as inputs, and generates the next sample accordingly.\n\nGiven the current state, we first slice the stochastic matrix to obtain the row with state transition probabilities that are originating from the current state. \n\nAs an example, if the current state is ___'2'___, the associated row of the stochastic matrix would be:\n\n$$\n\\begin{pmatrix}\np_{21} & p_{22} & p_{23}\n\\end{pmatrix}\n$$\n\nTo obtain the next state of the chain, we can use these values as probability mass function of a multinomial distribution and sample from that distribution. \n\nThe function below is heavily commented, so please check in case you are looking to understand the steps. Please note that below implementation is probably less efficient than a library implementation of multinomial random number generation such as numpy, however is beneficial for understand the inner workings of the algorithm.","4cae9143":"Assuming that our Markov chain has state space:\n$$(1,2,3)$$\n\nThe chain can be in one of these states at any given time, and the stochastic matrix needs to be in the form $3x3$ such as;\n\n$$P=\n\\begin{pmatrix}\np_{11} & p_{12} & p_{13} \\\\\np_{21} & p_{22} & p_{23} \\\\\np_{31} & p_{32} & p_{33}\n\\end{pmatrix}\n$$\n\nIn order to be used as a stochastic matrix, each row of the matrix need to sum up to 1: \n\n$$p_{11}+p_{12}+p_{13}=1$$ \n\nThis implies that the total probability of transitioning to any one of the states (including itself), from a state should be ___1___ (certain event). In the cell below, we are defining a function for generating random NxN matrices to be used as state transition probabilities for Markov chains.\n\n","8bc2ddc5":"Now that we have defined methods for generating the stochastic matrix and iterating the Markov chain, we can go ahead and put these together to generate a sequence of samples.\nBelow function generates a realization of the random process with specified length","ab1e9af7":"<a id='visualize-similarities'><\/a>\n### Visualize Pairs of Observations and Investigate Similarities","39fae56b":"<a id='pca-and-visualization'><\/a>\n### Calculating the Principal Components for Visualizing the Clusters\n\nNow that the features vectors have been computed from the observed time series, we can go ahead and calculate PCA to visualize the 2D projection  of the dataset. \n\nSince our dataset is quite small (100 observations x 9 features) computing the principal components is straightforward thanks to the scikit-learn implementation. ","fa20c7c1":"<a id='estimation-accuracy'><\/a>\n    \n### Estimation Accuracy\n    \nAlthough studying the estimation accuracy is not the goal of this notebook, we can play with the parameter `L` and investigate how it affects the estimation error, which can help us tune the _number of observations_ used for estimating the Markov chain. To find a good balance between  accuracy and performance, we will run the estimation using some values of hyperparameter L.  ","f26d70a8":"#### Two sequences generated by a usual traffic profile","8b7bfd99":"<a id='calculating-features'><\/a>\n### Calculating Features for Time Series\n\nIn the cell below, we will estimate the stochastic matrices for previously generated 100 observations. Since our goal is to use this matrix as a feature vector, we will then flatten the matrix and inspect the values. ___The result of the following cell is a dataset of i.i.d observations with 9 dimentions.___ \n\nFor better visualization and investigation of natural groupings in the extracted features, we will run principal component analisys (PCA) to select and visualize the fIrst two principal components of the feature dataset, with the expectation of being able to observe the initial traffic regimes as clearly separated clusters of points.  \n","8f0d6adb":"<a id='estimating-matrix'><\/a>\n# Estimating the Stochastic Matrix and Feature Vector for Time Series\n\nLet's get back to the original objective of this exercise which is,  ___given a sequence of observations, estimating the stochastic matrix and using this matrix as a feature vector.___ This feature vector can later be used by different machine learning algorithms for various tasks such as understanding the natural groupings and different traffic regimes (dimensionality reduction, visualization, clustering) or predicting the traffic density based on observations of the road.\n\nGenerally, machine learning algorithms require tabular data with a fixed format, unlike unbounded time series data, therefore, fitting a Markov chain as a preprocessing step in an ML pipeline can be useful for converting arbitrary length time series observations into a fixed length vectore, that summarizes the dynamic behavior of the underlying process.\n\nUsing Markov chains become tricky, due to the strong assumption of _Markov property_, and the underlying process that have generated the data may not necessarily be a good fit, which will result in a model that is not perfect, and may only be applicable under certain conditions, however, as long as the model serves the objectives of the task, we can use it by acknowledging its limitations and being prepared to mitigate the drawbacks. As stated above, as long as the Markov chain model can capture interesting aspects of the dynamic behavior of the observed time series, we can use it for creating a feature vector out of observations.\n\nIn the cell below, we are defining a function which takes a random sequence as an argument and returns the estimated stochastic matrix. The process of estimating the state transition probabilities of a Markov chain is as simple as counting the observed state transitions and calculating state transition frequencies.\n\nBelow function implementation is perhaps highly inefficient, however simple and commented enough to understand the estimation process.","1262830e":"<a id='estimation-from-observation'><\/a>\n### Estimating the Stochastic Matrix From Observations","8bfd45a0":"Each row of the above heatmap visualizes one of the realizations of the random process (sequence of observations). The regime of traffic has been randomly selected for each sequence, and resembles either one of the traffic states defined (quiet, usual, busy).\n\nAs can be seen in the heatmap, some of the rows are mostly yellow, where others are either mostly blue or, mostly purple. Althgough the sequences are generated completely randomly and none of the rows are identical, sequences generated by a particular regime generate similar visualizations. \n\nIn the visualization above, rows can be interpreted by their dominant color such as;\n\n>  * Mostly blue (0) if traffic is in __quite__ regime\n>  * Mostly yellow (1) if traffic is in __usual__ regime\n>  * Mostly purple (2) if traffic is in __busy__ regime\n\n\nAs you can see in the next cell, sequences generated by similar stochastic matrices look similar. This result confirms that the similarity between these random time series is also reflected in their corresponding stochastic matrices, and the stochastic matrix can capture some aspect of the similarities of observed time series. \n\nIn following parts of this kernel, we are going to work the other way around and try to identify the traffic regimes underlying the observed time series by their estimated stochastic matrices.","50769cf8":"### Visualizing the 1st and 2nd Principal Components\n\nIn the cell below, we have plotted the computed first two principal components of our dataset. For identifying the underlying traffic regimes for each sample, we have color coded the individual points in accordance with their real traffic states.\n\nAs expected, we can clearly observe 3 distinct linearly-separable clusters of points in the scatter plot.","8becd1d0":"#### Two sequences generated by a quiet traffic profile\nAs can be seen in below visualizations, similar stochastic matrices generate similar sequences.","e8f5c09c":"<a id='visualize-multiple-sequences'><\/a>\n### Generate and Visualize Multiple Sequences\n\nIn the cell below, we generate 3 different stochastic matrices, each one representing a different __regime__ of traffic. \n\nThe 1st stochastic matrix corresponds to a _quite_ traffic regime. The state is 0 most of the time, in other words, most of the measurements yield 0 cars passing through the road, with occasional 1 or 2 cars.\n\n$$P_{quite}=\n\\begin{pmatrix}\n0.85 & 0.10 & 0.05 \\\\\n0.95 & 0.04 & 0.01 \\\\\n0.95 & 0.03 & 0.02\n\\end{pmatrix}\n$$\n\nThe 2nd stochastic matrix corresponds to a usual traffic regime, with the most likely measurement being 1. As can be seen in below matrix, the transition probabilities targeting state 1 is highest in all rows. (Regardless of the current state, the most likely next step is 1)\n\n$$P_{usual}=\n\\begin{pmatrix}\n0.20 & 0.60 & 0.20 \\\\\n0.10 & 0.70 & 0.30 \\\\\n0.15 & 0.55 & 0.30\n\\end{pmatrix}\n$$\n\nFinally, the 3rd stochastic matrix represents a _busy_ traffic regime, with the state '2 cars' being the most likely.\n\n$$P_{busy}=\n\\begin{pmatrix}\n0.05 & 0.25 & 0.70 \\\\\n0.05 & 0.35 & 0.60 \\\\\n0.01 & 0.09 & 0.90\n\\end{pmatrix}\n$$\n\n\nIn the cell below, we take these 3 matrices as bases for different regimes of traffic and, generate 30 different observation sequences. We can consider these sequences as measurements taken during different 5 minute intervals of the same day. In each one of the measurement intervals, the traffic is in either one of 3 defined regimes, i.e the stochastic matrix is ___close to___ either one of the matrices. We have generated the stochastic matrices for these 30 observation sequences adding random noise to the base regime matrices.\n\nThe notion of ___closeness___ is particularly important in this exercise since the objective is to __estimate the stochastic matrix of given sequences and using this matrix as a feature vector for clustering algorithms__, in which the concept of distance and closeness is central.\n\n"}}