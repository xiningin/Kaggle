{"cell_type":{"fb71fa1f":"code","6f174c8e":"code","ea5f12dd":"code","240d54a0":"code","288ed61d":"code","e9d61725":"code","eb82cef5":"code","c48f2de9":"code","58aadf8a":"code","58526c49":"code","ab48d882":"code","268b8fff":"code","9abe0c14":"code","a0857e78":"code","6e302918":"code","a37b562a":"code","c22ce4b8":"code","13126346":"code","0027930d":"code","4e4ee6b4":"code","e3cadc04":"code","8976a9e1":"code","044e3efc":"code","bb068d26":"code","af157cf2":"code","e1a24682":"code","01b6e4e1":"code","31bdbf17":"code","1ad37324":"code","5bd1950a":"code","c7bb13ec":"code","fd7e2158":"code","6ac55466":"code","b6f1ae6e":"code","3bb71c73":"code","557ab8ad":"markdown","b012e469":"markdown","77951174":"markdown","0b7958ee":"markdown","8c5d8971":"markdown","2071d2ec":"markdown","c085b177":"markdown"},"source":{"fb71fa1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom os import listdir\nfrom os.path import isfile, join\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6f174c8e":"!pip install h5py","ea5f12dd":"!pip install keras-resnet","240d54a0":"!pip install tensorflow","288ed61d":"!pip install imageai","e9d61725":"import matplotlib.pyplot as plt;\nfrom tensorflow.keras.layers import BatchNormalization\nimport matplotlib.image as mpimg\nimport os\nimport warnings \nfrom imageai.Detection import ObjectDetection\nfrom os import listdir\nfrom os.path import isfile, join\nwarnings.filterwarnings('ignore')\nfrom imageai.Detection import ObjectDetection","eb82cef5":"execution_path = '\/kaggle\/input\/object-detection'\n\ndetector = ObjectDetection()\ndetector.setModelTypeAsRetinaNet()\ndetector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.1.0.h5\"))\ndetector.loadModel()\n\n    \nfiles_path = \"\/data\"\nimage_files = [f for f in listdir(execution_path+'\/'+files_path) if isfile(join(execution_path+'\/'+files_path, f))]\nresults = '\/kaggle\/working\/results'\nif os.path.isdir(results) == True:\n    print('Folder Exists!')\nelif os.path.isdir(results) == False:\n    os.mkdir(results)","c48f2de9":"for i in image_files:\n    \n    detections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path+'\/'+files_path , i), output_image_path=os.path.join(results, i.split('.')[0]+'_new.'+i.split('.')[1]))\n\n    for eachObject in detections:\n        print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"] )\n    print('-------------------------------------------------')","58aadf8a":"results_path = r\"\/kaggle\/working\/results\"\nresult_files = [f for f in listdir(results_path) if isfile(join(results_path, f))]\nfor i in result_files:  \n    fig=plt.figure(figsize=(10,8))\n    img = mpimg.imread(results_path+'\/'+i)\n    imgplot = plt.imshow(img)\n    fig.show()","58526c49":"!pip install opencv-python","ab48d882":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time","268b8fff":"names=open(r\"\/kaggle\/input\/object-detection\/coco.names\").read()\nprint(names)","9abe0c14":"names = names.strip().split(\"\\n\")\nprint(names)","a0857e78":"print(len(names))","6e302918":"configuration_path = r\"\/kaggle\/input\/object-detection\/yolov3.cfg\"\nweights_path = r\"\/kaggle\/input\/object-detection\/yolov3.weights\"\npro_min = 0.5 \nthreshold = 0.2","a37b562a":"net = cv2.dnn.readNetFromDarknet(configuration_path,weights_path)\n\n# Getting names of all layers\nlayers = net.getLayerNames()  # list of layers' names\n\n# # Check point\nprint(layers)","c22ce4b8":"for i in net.getUnconnectedOutLayers():\n    #print(type(i))\n    print(layers[i-1])","13126346":"output_layers=[layers[i - 1] for i in net.getUnconnectedOutLayers()]","0027930d":"output_layers","4e4ee6b4":"image=cv2.imread(r\"\/kaggle\/input\/object-detection\/data\/senior-g8bf177f88_1920.jpg\")","e3cadc04":"print(image.shape)","8976a9e1":"plt.rcParams['figure.figsize'] = (8,8)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()","044e3efc":"blob = cv2.dnn.blobFromImage(image, 1 \/ 255.0, (400,400), swapRB=True, crop=False)\n# blobfromimage returns a 4 dimensional bolb\n\n# Check point\nprint(image.shape)  \nprint(blob.shape)  \n# Resulted shape has number of images, number of channels, width and height\n# So (1,3,400,400)","bb068d26":"# Slicing blob and transposing to make channels come at the end\nblob_to_show = blob[0, :, :, :].transpose(1, 2,0)\nprint(blob_to_show.shape) \n\nplt.rcParams['figure.figsize'] = (5, 5)\nplt.imshow(blob_to_show)\nplt.show()","af157cf2":"print(blob.shape)","e1a24682":"blob = cv2.dnn.blobFromImage(image, 1 \/ 255.0, (416,416), swapRB=True, crop=False)\n# blobfromimage returns a 4 dimensional bLob\n\n# Check point\nprint(image.shape)  \nprint(blob.shape)  \n# Resulted shape has number of images, number of channels, width and height","01b6e4e1":"# Slicing blob and transposing to make channels come at the end\nblob_to_show = blob[0, :, :, :].transpose(1, 2,0)\nprint(blob_to_show.shape) \n\nplt.rcParams['figure.figsize'] = (5, 5)\nplt.imshow(blob_to_show)\nplt.show()","31bdbf17":"net.setInput(blob) # giving blob as input to our YOLO Network.\nt1=time.time()\noutput = net.forward(output_layers)\nt2 = time.time()\n\n# Showing spent time for forward pass\nprint('YOLO took {:.2f} seconds'.format(t2-t1))","1ad37324":"colours = np.random.randint(0, 255, size=(len(names), 3), dtype='uint8') # randint(low, high=None, size=None, dtype='l')","5bd1950a":"classes = []\nconfidences = []\nboxes = []","c7bb13ec":"Height = image.shape[0]\nWidth = image.shape[1]","fd7e2158":"for out in output:\n    for res in out:\n        scores = res[5:]\n        class_current = np.argmax(scores) # returning indices with max score and that would be our class as that will be 1 and rest will be 0\n\n        # Getting the probability for current object by accessing the indices returned by argmax.\n        confidence_current = scores[class_current]\n\n        # Eliminating the weak predictions that is with minimum probability and this loop will only be encountered when an object will be there\n        if confidence_current > 0.5:\n            \n            # Scaling bounding box coordinates to the initial image size\n            # YOLO data format just keeps center of detected box and its width and height\n            #that is why we are multiplying them elemwnt wise by width and height\n            box = res[0:4] * np.array([Width, Height, Width, Height])  #In the first 4 indices only contains \n            #the output consisting of the coordinates.\n            print(res[0:4])\n            print(box)\n\n            # From current box with YOLO format getting top left corner coordinates\n            # that are x and y\n            x, y, w, h = box.astype('int')\n            x = int(x - (w \/ 2))\n            y = int(y - (h \/ 2))\n            \n\n            # Adding results into the lists\n            boxes.append([x, y, int(w), int(h)]) ## appending all the boxes.\n            confidences.append(float(confidence_current)) ## appending all the confidences\n            classes.append(class_current) ## appending all the classes ","6ac55466":"results = cv2.dnn.NMSBoxes(boxes, confidences,0.2,0.4)\n\n# Showing labels of the detected objects\nfor i in range(len(classes)):\n    print(names[int(classes[i])])","b6f1ae6e":"if len(results) > 0:\n\n    for i in results.flatten():\n        \n        # Getting current bounding box coordinates\n        x, y = boxes[i][0],boxes[i][1]\n        width, height = boxes[i][2], boxes[i][3]\n        \n        colour_box_current = [int(j) for j in colours[classes[i]]]\n\n        # Drawing bounding box on the original image\n        cv2.rectangle(image, (x, y), (x + width, y + height),\n                      colour_box_current,7 )\n\n        # Preparing text with label and confidence \n        text_box_current = '{}: {:.4f}'.format(names[int(classes[i])], confidences[i])\n\n        # Putting text with label and confidence\n        cv2.putText(image, text_box_current, (x+2, y+2), cv2.FONT_HERSHEY_TRIPLEX, 1.5,(0,0,255))","3bb71c73":"plt.rcParams['figure.figsize'] = (10,10)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()","557ab8ad":"# OBJECT DETECTION\n<br\/><br\/>\n![1_EYtn2YE7b6MTzMQyD2R3nA.jpeg](attachment:04144e26-3fc7-437a-a300-9d11ac284b4c.jpeg)","b012e469":"Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Object detection, unlike computer vision and image processing, involves finding the coordinates of the detected object on the image. With the coordinates found, the area where the object will be enclosed with a frame is also determined.\n\nWe'll do object detection in two different ways. I'm going to use some random photos for this.","77951174":"There is a file called coco.names that has the list of 80 object class that the model will be able to detect. The model has been trained only on these 80 object classes.","0b7958ee":"## Object Detection with YOLOv3\n<br\/><br\/>![2.jpg](attachment:6251f3c3-910e-4372-b64c-7ff9e10ec8c7.jpg)","8c5d8971":"When we compare the two different ways, we see that Yolo gives better results. However, the first way has had quite a bit of success with less code.","2071d2ec":"YOLO is one of the fast object detection algorithms. We will use YOLOv3 in this notebook.","c085b177":"The neural network model architecture is stored in the yolov3.cfg file, and the pre-trained weights of the neural network are stored in yolov3.weights."}}