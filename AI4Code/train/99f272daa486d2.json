{"cell_type":{"8aa4e0bc":"code","245792ff":"code","da980fa6":"code","7195122c":"code","1e43d121":"code","954597aa":"code","5004caa5":"code","f62d76e9":"code","f3df9d5c":"code","9136fa7e":"code","b1754d3f":"code","c1e530cd":"code","757ff1cc":"code","d28b0f41":"code","184bfc39":"code","c7c112a2":"markdown","97596c56":"markdown","0049599a":"markdown","7e3b7a3a":"markdown","9cba0153":"markdown","260a7ba6":"markdown","9a92aa1b":"markdown","a463c2d2":"markdown","2eb97c63":"markdown","1f84ac5c":"markdown","09d602f9":"markdown","85714b0f":"markdown","9911f500":"markdown","875f1931":"markdown","5e58d085":"markdown","fa877235":"markdown","117fc6f7":"markdown","43f5f0a5":"markdown","63a81eb7":"markdown","fec8ad28":"markdown","c9039d7a":"markdown","2f2265fd":"markdown","085b7999":"markdown"},"source":{"8aa4e0bc":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna\nimport warnings\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import QuantileTransformer\nimport shap\nimport umap\nimport umap.plot\nimport joblib\n\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\").drop(\"id\", axis=1).fillna(0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\").drop(\"id\", axis=1).fillna(0)\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\n\nX, y = train.drop(\"loss\", axis=1), train[['loss']]\n\nplt.style.use(\"ggplot\")\nwarnings.filterwarnings(\"ignore\")","245792ff":"import umap, umap.plot\n\nsample = train.sample(30000, replace=False)","da980fa6":"# Project to 2d\nsample_X, sample_y = sample.iloc[:, :-1], sample.iloc[:, -1]\n\nmapper_2d = umap.UMAP(n_neighbors=1000).fit(sample_X, sample_y)\n\n# Plot\numap.plot.points(mapper_2d, labels=sample_y, theme='fire');","7195122c":"%%time\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale\nsample_X.iloc[:, :] = StandardScaler().fit_transform(sample_X)\n\n# Create a new embedding\nmapper_2d = umap.UMAP(n_neighbors=1000).fit(sample_X, sample_y)\n\n# Plot\numap.plot.points(mapper_2d, labels=sample_y, theme='fire');","1e43d121":"from sklearn.preprocessing import QuantileTransformer\n\n# Define the objective function\ndef objective(trial, X, y):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.3, random_state=1121218)\n    qt = QuantileTransformer(random_state=1121218)\n    X_train.iloc[:, :] = qt.fit_transform(X_train)\n    X_valid.iloc[:, :] = qt.transform(X_valid)\n    param = {\n        \"tree_method\": \"gpu_hist\",\n        'n_estimators': trial.suggest_int('n_estimators', 500, 2000, 100),\n        \"booster\": 'gbtree',\n        \"reg_lambda\": trial.suggest_int(\"reg_lambda\", 1, 100),\n        \"reg_alpha\": trial.suggest_int(\"reg_alpha\", 1, 100),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0, step=0.1),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0, step=0.1),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 2, 10),\n        \"learning_rate\": 0.01,\n        \"gamma\": trial.suggest_float(\"gamma\", 0, 20)\n    }\n    # Set up the CV\n    eval_set = [(X_valid, y_valid)]\n    fit_params = dict(eval_set=eval_set, eval_metric='rmse', \n                      early_stopping_rounds=100, verbose=False)\n    xgb_reg = xgb.XGBRegressor(**param)\n    # Fit\/predict\n    _ = xgb_reg.fit(X_train, y_train)\n    preds = xgb_reg.predict(X_valid)\n    # Compute rmse\n    rmse = np.sqrt(mean_squared_error(y_valid, preds))\n    \n    return rmse","954597aa":"# Callback function to print log messages when the best trial is updated\ndef logging_callback(study, frozen_trial):\n    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n    if previous_best_value != study.best_value:\n        study.set_user_attr(\"previous_best_value\", study.best_value)\n        print(\n            \"Trial {} finished with best value: {}. \".format(\n            frozen_trial.number,\n            frozen_trial.value\n            )\n        )","5004caa5":"%%time\n\nfrom optuna.samplers import TPESampler\nfrom sklearn.model_selection import KFold\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nstudy = optuna.create_study(sampler=TPESampler(seed=1121218), direction='minimize', study_name='xgb')\nfunc = lambda trial: objective(trial, X, y)\n\nstudy.optimize(func, timeout=60*30, callbacks=[logging_callback])","f62d76e9":"print(\"Best trial: 17\")\nprint(f\"\\twith value: {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\nfor key, value in study.best_params.items():\n    print(f\"\\t\\t{key}: {value}\")","f3df9d5c":"# Save the study\nimport joblib\n\njoblib.dump(study, \"xgb_study.pkl\")","9136fa7e":"study = joblib.load(\"..\/input\/new-insights-from-umap-optuna-shap\/xgb_study.pkl\")\n\n# New regressor with the optimal parameters\nfinal_xgb = xgb.XGBRegressor(\n    **study.best_params, tree_method='gpu_hist', learning_rate=0.01, booster='gbtree'\n)\n\n# Extract 5% of the training data for early stopping\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.05, random_state=1121218)\n\n# Apply the scaler to the sets\nqt = QuantileTransformer()\nX_train_scaled = qt.fit_transform(X_train)\nX_valid_scaled = qt.transform(X_valid)\ntest_scaled = qt.transform(test)\n\neval_set = [(X_valid_scaled, y_valid)]\n\n# Train \/ predict\n_ = final_xgb.fit(\n    X_train_scaled, \n    y_train, eval_metric='rmse', \n    eval_set=eval_set, \n    early_stopping_rounds=100, \n    verbose=False\n)\n\npreds = final_xgb.predict(test_scaled)\n\n# Submit\nfinal_sub = pd.DataFrame({\"id\": submission.id, \"loss\": preds})\nfinal_sub.to_csv(\"submission.csv\", index=False)","b1754d3f":"from optuna.visualization.matplotlib import plot_optimization_history\n\nplot_optimization_history(study)\nfig = plt.gcf()\nfig.set_size_inches(10, 6)","c1e530cd":"from matplotlib import rcParams\nfrom optuna.visualization.matplotlib import plot_param_importances\n\nrcParams['figure.figsize'] = 10, 6\nplot_param_importances(study);","757ff1cc":"%%time\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(final_xgb, random_state=1).fit(X_valid_scaled, y_valid)\neli5.show_weights(perm, feature_names = X.columns.tolist())","d28b0f41":"import shap\n\n# Choose a smaller subset of the validation data\nsmall_valid = X_valid_scaled[:200]\n\n# Create the explainer\nexplainer = shap.TreeExplainer(final_xgb)\nshap_values = explainer.shap_values(small_valid)\n\nshap.summary_plot(shap_values, small_valid);","184bfc39":"# Select a random row like 17\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[17, :], small_valid[17, :], feature_names=X.columns.tolist())","c7c112a2":"For the prediction of row 17, feature 25, 9, 3 had the most positive impact while 84, 51, 46 had the most negative. ","97596c56":"Since we have 100 features, it will be useful to see which features influence the predictions the most.\n\nFor that, we will be using a permutation importance (PI) plot, because it gives a more robust information than simple feature importances or coefficients that the model came up with:","0049599a":"Here is how to interpret this plot:\n- Each dot represents a single row from the data\n- The Y axis determines which feature the dot belongs to\n- The X axis determines the Shapley value for that point and how much it influenced the prediction\n- The color represents actual value of the point, as it appears in the dataset\n\nThis summary plot reveals an interesting global trend for each feature. When some features like the bottom 4, f13, f28, f39 increase, they increasingly have more positive impact on the model output. In contrast, an increase in others like 52, 3, 58 works against the model output. In other words, an increase\/decrease in the values of the feature either postively or negatively influences the model's decision. There are no overlaps.\n\nThe only incosistent feature with this trend is feature 81 because it has some outliers (red dots farthest away from the rest).\n\nNow, for the sake of completeness, let's select a random prediction and look at the features that most influenced the model's decision to produce that particular output:","7e3b7a3a":"# Summary","9cba0153":"# Model Explainability with ELI5 and SHAP","260a7ba6":"I guess we can all agree that this month's TPS is a bit boring. I think that is mainly because of the low prospects of doing effective feature engineering to imrpove the score. We are mostly left with hyperparameter tuning, which honestly makes this whole competition a matter of who has got the most time and compute. \n\nI have been doodling around recently with UMAP and I wanted to share with you my insights in this notebook. Though my experiments didn't spark substantial ideas for me, maybe they can aid you in coming up with some ideas.","9a92aa1b":"We have retrieved the best set of parameters with a best score of 7.8395. These parameters will the base for what will be doing in model explainability section with SHAP and ELI5.","a463c2d2":"Well, this did the trick. We can see that the biggest clusters correspond to the low vlaues in the target. In fact, from 0 to about 20 `loss` values, the clusters are pretty distinct. The higher the `loss`, the less grouped the points are.\n\nTo see if this new-found insight from the visualization translated to a score improvement, I tried projecting the whole data. Unfortunately, the kernel ran out of RAM every time I tried, so I leave this experiment for those with a machine with a larger RAM. ","2eb97c63":"Honestly, I went into this section expecting more. I wanted to generate a few potential ideas for feature engineering but I didn't get any.\n\nMaybe, I misinterpreted the plots and missed something. Other than this, I currently don't see any other way we could go around the dataset and come up with something that does not involve pure model and tuning. Maybe I would have gotten somewhere with UMAP if not for the hardware limitations.\n\nLet me know what you guys think and if you did experiments with UMAP or SHAP, or you know, anyhting new.","1f84ac5c":"## Setup","09d602f9":"# Hyperparameter tuning with Optuna","85714b0f":"On the second thought, a more useful plot would be the hyperpamater importances:","9911f500":"According to this plot, `colsample_bytree` and `min_child_weight` had little influence over the objective function of the study. For future reference and tuning, you may get faster and better results by excluding those hyperparameters from the search and giving a larger serach interval to more important ones.","875f1931":"Here is a simple explanation of how PI works:\n1. One feature is chosen and its values are shuffled while others are left fixed\n2. This new set of features is given to the already fitted model\n3. Model makes new predictions on these new features\n4. The new predictions are compared to predictions made with the original set of features. For more info, check out the Kaggle course [here](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance).\n5. If a shuffled feature is important, then it will have a significant impact on models predictions because shuffling it makes it a useless feature for the model, decreasing its predictive power.\n\nFrom the above PI plot, we can see that there aren't specific features that have higher influence than others. If I had to pick though, I guess, the top 5 - f81, f52, f69, f77, f25 would be my choices. Shuffling these features would hurt the accuracy of the predictions  slightly more than other features. \n\nWe can confirm this by computing the Shapley values and plotting them:","5e58d085":"# Exploring the Optuna study for more insight into hyperparameters","fa877235":"# Dimensionality Reduction w. UMAP, Hyperparameter Tuning w. Optuna, XAI with SHAP\n![](https:\/\/images.pexels.com\/photos\/2859169\/pexels-photo-2859169.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/www.pexels.com\/@andrew?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Andrew Neel<\/a>\n        on \n        <a href='https:\/\/www.pexels.com\/photo\/assorted-map-pieces-2859169\/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels.<\/a>\n    <\/strong>\n<\/figcaption>","117fc6f7":"Even though a bit beautiful, this plot does not tell much. The plot is a long thread of the data points dominated by the small values, especially 0s. If you you pay a little more attention, you can see some discontinuity in the thread on the left side. \n\nIn simple terms, UMAP uses linear distance between the points which makes it highly sensitive to feature scales. Let's try the same operation by scaling all features with `StandardScaler`:","43f5f0a5":"# Submitting predictions with the found parameters","63a81eb7":"# Reduction with UMAP","fec8ad28":"In this section, we will peprform XGBoost hyperparameter tuning with Optuna and plot the search history to explore hyperparameter importances. \n\nI will be using `QuantileTransformer` as mentioned in this [notebook](https:\/\/www.kaggle.com\/oxzplvifi\/tabular-denoising-residual-network). It is found to work best with those bimodal\/trimodal and skewed features observed in my previous [EDA notebook](https:\/\/www.kaggle.com\/bextuychiev\/relevant-eda-xgboost):","c9039d7a":"Let's try UMAP with 1000 neighbors (this is the number I ended up with after playing around a bit). Essentially, `n_neigbors` in UMAP controls the zoom level of the projection. ","2f2265fd":"Let's plot the optimization history first:","085b7999":"To make computations less time consuming, we will take 30k samples and try to project and plot them in 2D with UMAP:"}}