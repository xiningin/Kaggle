{"cell_type":{"c4916f21":"code","adbf29dc":"code","2f8fa5d8":"code","e5f947f0":"code","d2fd2678":"code","7aef4a04":"code","cd54695d":"code","3e9f2b03":"code","93421a13":"markdown","22e74f38":"markdown","7bad36db":"markdown","0d727dc6":"markdown","f4f4bef5":"markdown","737d5d68":"markdown","1fd2436a":"markdown","86e47ee1":"markdown","fac9f4cb":"markdown","a1e305e1":"markdown","361d46b4":"markdown","bfbb2ade":"markdown"},"source":{"c4916f21":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.animation import FuncAnimation","adbf29dc":"# np.random.seed(5)\nx = 2 * np.random.rand(100)\ny = 4 + x + np.random.rand(100)\nplt.scatter(x, y)\nplt.show()","2f8fa5d8":"# theta0, theta1, size of training dataset\nt0 = 0\nt1 = 0\nm = len(x)","e5f947f0":"def fun(t0, t1, x):\n    return t0 + t1 * x","d2fd2678":"def cost_fun(t0, t1, m, x, y):\n    suma = 0\n    for i in range(len(x)):\n        suma += (fun(t0, t1, x[i]) - y[i]) ** 2\n    return suma * (1 \/ 2 * m)\n","7aef4a04":"def gradient(alfa, m, x, y, t0, t1):\n    suma1, suma2 = 0, 0\n    for i in range(len(x)):\n        suma1 += fun(t0, t1, x[i]) - y[i]\n        suma2 += (fun(t0, t1, x[i]) - y[i]) * x[i]\n    temp1 = t0 - alfa * (1 \/ m) * suma1\n    temp2 = t1 - alfa * (1 \/ m) * suma2\n    return temp1, temp2","cd54695d":"costs = [cost_fun(t0, t1, m, x, y)]\nnum_of_iter = 1\nxx = np.linspace(sorted(x)[0], sorted(x)[-1], 100)\n\ndef animate(i):\n    global t0, t1, num_of_iter\n    yy = t0 + xx * t1 # regression line\n    plt.cla()\n    plt.plot(xx, yy, color='red', label = 'Regression line')\n    plt.scatter(x, y, label = 'data')\n    plt.legend()\n    t0, t1 = gradient(0.4, m, x, y, t0, t1)\n    num_of_iter += 1\n    costs.append(cost_fun(t0, t1, m, x, y))\n    \n    if costs[-2] - costs[-1] < 10 ** (-3): #automatic convergence test\n        print(\"osi\u0105gnieto zbie\u017cno\u015b\u0107\")\n        print(t0, t1)\n        ani.event_source.stop()\n\n\nani = FuncAnimation(plt.gcf(), animate, interval=50)\nplt.show()\n","3e9f2b03":"costs.pop(0)\nnum_of_iter -= 1\nplt.title(\"Cost function values\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Value\")\nplt.plot(np.linspace(0, num_of_iter, num_of_iter), costs[:num_of_iter])","93421a13":"![costfun.png](attachment:b98d0a0a-fdc6-4351-90a3-22f729201d5f.png)","22e74f38":"# Cost function\n![cost.jpg](attachment:63823914-f2c1-4575-aede-50992b3df4bf.jpg)","7bad36db":"**Uppdating theta1 and theta 0 until convergence**","0d727dc6":"![Figure_1.png](attachment:56991a7a-9b87-4ae0-ba4e-f9671a0cde46.png)","f4f4bef5":"![test.gif](attachment:11a3de70-619e-4c17-9018-703e9323c65c.gif)","737d5d68":"# \ud83d\udce4 Import Libraries","1fd2436a":"# Ploting values of cost function","86e47ee1":"# Gradient Descent Algorithm","fac9f4cb":"# Getting random data","a1e305e1":"# Welcome to my first project. \ud83d\udc4b \n# I implemented a simple algorithm that adjusts a linear function for the randomly selected data. Here I am using the gradient descent algorithm to minimize the cost function (Mean squared error).\n# I was able to visualize the regression line adjustments nicely with FuncAnimation - enjoy and pls share your comments and upvote ;)\n","361d46b4":"# Hypothesis h\n![hhh.jpg](attachment:26ac55d9-f49b-4bbb-9f4c-7d488827268e.jpg)","bfbb2ade":"![grad.jpg](attachment:1a054f51-3e7a-469d-86b7-116b2fdb38c9.jpg)"}}