{"cell_type":{"ce07f989":"code","dc9bbb99":"code","860f2c61":"code","e4173046":"code","d29151ec":"code","b52821c7":"code","3ad2fe27":"code","e43e8ae9":"code","f9dcf5cb":"code","1820cdce":"code","236fafef":"code","6edd66c0":"code","cbfd38a0":"code","e6d80feb":"code","950048bd":"code","199c04ba":"code","c7b4ccfa":"code","cd192b1d":"code","2cb613a4":"code","07ec1237":"code","248dee89":"code","ff9a4a5a":"code","9800d392":"code","60fb0d1f":"code","e7e75905":"code","79ccf762":"code","4cbad3f6":"code","0071043c":"code","6b7dc4ac":"code","f63e3b73":"code","accbbddd":"code","1057e5d7":"code","b23a7eb2":"code","39ab205c":"code","10002124":"code","93d52542":"code","43bfd955":"code","e2d18c7d":"code","85e495d8":"code","fcb2f5c2":"code","522d1c4f":"code","064b6308":"code","ec921543":"code","10eadc8a":"code","b4bda120":"code","a6e80167":"code","e13381e5":"code","38491dae":"code","f43a8b82":"code","a7a8f39f":"code","9dc3b197":"code","bbd27367":"code","63912a4b":"code","dafa44c9":"code","1169a89c":"code","b2fc1761":"markdown","f739444b":"markdown","5dd55832":"markdown","ef680afa":"markdown","598890a8":"markdown","421d32cf":"markdown","6a64248e":"markdown","4f807cfa":"markdown","b24b4556":"markdown","a239fccb":"markdown","ec79f251":"markdown","9d6f288d":"markdown","bf13086f":"markdown","1a468f97":"markdown","a84cd4f1":"markdown","8c9251d7":"markdown","5d198c53":"markdown","0c0d6388":"markdown","1c225126":"markdown","e9786734":"markdown","be97cc5b":"markdown","3e20a632":"markdown","a3fa8a69":"markdown","4332329c":"markdown","66e6fc18":"markdown","004971ce":"markdown","83edee9c":"markdown"},"source":{"ce07f989":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dc9bbb99":"# Lets import the basic libraries first then we will proceed to the others as the need arises\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","860f2c61":"import warnings\nwarnings.filterwarnings('ignore')","e4173046":"# reading the test and train data\ntrain_original  = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_original = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')","d29151ec":"# reading the sample\nsample = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')\nsample.head()","b52821c7":"# i like to do it this way i.e. preserve the original data.... just in case if i went wrong somewhere.\n# it can also lead to confusion and multiplicity of variable so one has to be careful\ntrain = train_original.copy()\ntest = test_original.copy()","3ad2fe27":"# displaying the train data\ntrain.head()","e43e8ae9":"# lets see the test data as well\ntest.head()","f9dcf5cb":"train.shape","1820cdce":"# lets check the null values if any\ntrain.isnull().sum()","236fafef":"# lets check if there are any duplicates\ntrain.duplicated().sum()","6edd66c0":"# lets check the test data\ntest.isnull().sum()","cbfd38a0":"test.shape","e6d80feb":"test.duplicated().sum()","950048bd":"train.info()","199c04ba":"# lets drop the id column. Else it will interfere with the regression results. \ntrain.drop(['id'], axis=1,inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","c7b4ccfa":"# lets see the stats for train dataset\ntrain.describe().T","cd192b1d":"figure, ax = plt.subplots(7,2,figsize=(15,30))\nc=1\nfor i in train.drop(['target'],axis=1).columns:\n    plt.subplot(7,2,c)\n    sns.distplot(train[i],color = 'blue', label='train')\n    sns.distplot(test[i],color = 'red', label='test')\n    c=c+1\n    plt.xlabel(i, fontsize=9)\n    plt.legend()\nplt.show()","2cb613a4":"# lets checkhow variables vary with respect to the other. This will help us identify any multicollinearity\nsns.pairplot(train)","07ec1237":"# lets draw the correlation heatmap \nplt.figure(figsize=(20,20))\nsns.heatmap(train.corr(),annot=True, cmap = 'coolwarm')","248dee89":"# lets check for the outliers\nplt.figure(figsize=(20,10))\nsns.boxplot(data=train.drop(['target'],axis=1))\nplt.title('The boxplot to study outliers')\nplt.xlabel('Variables that predict the Target')\nplt.ylabel('Values')","ff9a4a5a":"# The target variable should be normally distributed all the values of the independent variables.\n# lets check if the target variable is normally distributed\n# lets check with the distplot first\n\nsns.distplot(train['target'],color = 'blue', label='train')","9800d392":"from statsmodels.graphics.gofplots import qqplot\nqqplot(train['target'], line='s')\nplt.show()","60fb0d1f":"# importing the libraries\nfrom sklearn.model_selection import train_test_split\nX = train.drop(['target'],axis=1)\ny = train['target']\nX_train, X_test,y_train, y_test = train_test_split(X,y, train_size=0.75, random_state=42) ","e7e75905":"X_train.head()","79ccf762":"X_test.head()","4cbad3f6":"y_train.head()","0071043c":"y_test.head()","6b7dc4ac":"# import linear regression library\n\nfrom sklearn.linear_model import LinearRegression","f63e3b73":"# defining a variable to store the linear regression function for ease of use\n\nreg_model = LinearRegression()","accbbddd":"reg_model.fit(X_train,y_train)","1057e5d7":"for i, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for\",col_name, \"is\", reg_model.coef_[i])","b23a7eb2":"# Let us check the intercept for the model\n\nintercept = reg_model.intercept_\n\nprint(\"The intercept for our model is\", intercept)","39ab205c":"# R^2 value of the train dataset\nreg_model.score(X_train, y_train)","10002124":"# R^2 value of the test dataset\nreg_model.score(X_test, y_test)","93d52542":"data_train = pd.concat([X_train, y_train], axis=1)\ndata_train.head()","43bfd955":"# forming the regression equation\n\nreg_expression = 'target ~ cont1+cont2+cont3+cont4+cont5+cont6+cont7+cont8+cont9+cont10+cont11+cont12+cont13+cont14'","e2d18c7d":"import statsmodels.formula.api as smf\nmodel1 = smf.ols(formula=reg_expression, data=data_train).fit()\n# displaying first 5 parameters\nmodel1.params.head()","85e495d8":"print(model1.summary())","fcb2f5c2":"from statsmodels.regression.linear_model import RegressionResults\nnp.sqrt(model1.mse_resid)","522d1c4f":"model1.rsquared_adj","064b6308":"# forming the second regression equation\n\nreg_expression_2 = 'target ~ cont1+cont2+cont3+cont4+cont5+cont6+cont7+cont8+cont9+cont10+cont11+cont12+cont13'","ec921543":"model2 = smf.ols(formula=reg_expression_2, data=data_train).fit()\n# displaying first 5 parameters\nmodel2.params.head()","10eadc8a":"print(model2.summary())","b4bda120":"np.sqrt(model2.mse_resid)","a6e80167":"model2.rsquared_adj","e13381e5":"# another way of finding RSME\nfrom sklearn.metrics import mean_squared_error\nprint(\"model1 train RMSE:\", np.sqrt(mean_squared_error(y_train, model1.predict(X_train))))\nprint(\"model1 test RMSE:\", np.sqrt(mean_squared_error(y_test, model1.predict(X_test))))\nprint(\"model2 train RMSE:\", np.sqrt(mean_squared_error(y_train, model2.predict(X_train))))\nprint(\"model2 test RMSE:\", np.sqrt(mean_squared_error(y_test, model2.predict(X_test))))","38491dae":"# Import linear models\nfrom sklearn import linear_model\n# Create lasso and ridge objects\nlasso = linear_model.Lasso()\nridge = linear_model.Ridge()\n# Fit the models\nlasso.fit(X_train, y_train)\nridge.fit(X_train, y_train)\n# Print scores, MSE, and coefficients\nprint(\"R^2_lasso score:\", lasso.score(X_train, y_train))\nprint(\"R^2_ridge score:\",ridge.score(X_train, y_train))\nprint(\"lasso RMSE:\", np.sqrt(mean_squared_error(y_test, lasso.predict(X_test))))\nprint(\"ridge RMSE:\", np.sqrt(mean_squared_error(y_test, ridge.predict(X_test))))\nprint(\"lasso coef:\", lasso.coef_)\nprint(\"ridge coef:\", ridge.coef_)","f43a8b82":"# lets compare the RSME values of all the models that we have built\nprint(\"model1 test RMSE:\", np.sqrt(mean_squared_error(y_test, model1.predict(X_test))))\nprint(\"model2 test RMSE:\", np.sqrt(mean_squared_error(y_test, model2.predict(X_test))))\nprint(\"lasso test RMSE:\", np.sqrt(mean_squared_error(y_test, lasso.predict(X_test))))\nprint(\"ridge test RMSE:\", np.sqrt(mean_squared_error(y_test, ridge.predict(X_test))))","a7a8f39f":"test_predicted_ols = model2.predict(test)\ntest_predicted_ols","9dc3b197":"submission = test_original['id']\ntest_pred = pd.DataFrame(test_predicted_ols)\nsubmission = pd.concat([submission,test_predicted_ols],axis=1)\nsubmission.rename({0:'target'},axis=1,inplace=True)","bbd27367":"submission.head()\n","63912a4b":"# lets check the distribution of the predicted values of the v\/s original target values","dafa44c9":"sns.distplot(train['target'],color = 'blue', label='train')\nsns.distplot(submission['target'],color = 'red', label='test')","1169a89c":"submission.to_csv(\"result.csv\", index = False, header = True)\n","b2fc1761":"Good no duplicates...","f739444b":"First lets check the distribution of the variables, and see how test and train datasets vary in this ","5dd55832":"Here we can see that the R square value is very low as expected. There is a lot of noise in the train as well as test datasets. Typically we would see some pattern in the data while plotting the correlation heatmap and the scatter plots.\n\nFor a good model the R^2 and adjusted R^2 values would be close to 1","ef680afa":"Well even though the model RSME was 0.73, we can see above, that the prediction might not be very good. Lets submit and check whats the score. ","598890a8":"The train test function gives 4 values: \n1. first is the train dataframe (without target)\n1. second value is the test dataframe(without target)\n1. third is the training dataframe of target variable\n1. fourth is the test dataframe of target variable","421d32cf":"# Linear Regression using Statsmodels","6a64248e":"The scale of all the variables look similar. Hence there is no need to perform the scaling.\n\nNo outlier treatment is required since the test and the train data are absolutely identical.\n\nVariables such as cont2, count4, cont5, cont8 are skewed and cont 11, con12, cont 14 have double peaks in the data. \n\nTypically these might represent the different clusters. \n","4f807cfa":"# Train Test Split","b24b4556":"No missing values","a239fccb":"300000 rows and 16 columns","ec79f251":"Heatmap above does show quite a few variables correlated. But again, it is difficult to ascertain that there is a correlation. Hence we will keep these variables in the dataset. Else I would have remove the correlated variables.\n\nAlso we should note that none of the independent variables show any correlation with the target. there seems to be too much noise in the dataset. We should expect low R-Square values in the Regression model.","9d6f288d":"R^2 is not a reliable metric as it always increases with addition of more attributes even if the attributes have no influence on the predicted variable.\n\nInstead we use adjusted R^2 which removes the statistical chance that improves R^2.\n\nScikit does not provide a facility for adjusted R^2, so we use statsmodel, a library that gives results similar to what you obtain in R language. This library expects the X and Y to be given in one single dataframe\n\nfurther we can immprove the model by backward elimination","bf13086f":"From qq plot, it seems that the target is normally distributed.","1a468f97":"Hmmm..... Independent variables do not show any specific correlation with each other. Also the target variable doesnot seem to be correlated with any of the independent variables.\n\nWe will further check for correlation in the heatmap. But even if any of the independent variables show any correlation there, it will be difficult to conclude that there is actually a correlation between them. To me the dataset variables seem like randomly generated numbers. ","a84cd4f1":"# Analysing the target variable","8c9251d7":"# Ridge and Lasso","5d198c53":"# Building Linear Regression Model","0c0d6388":"# Checking the Basic data hygiene and preprocessing","1c225126":"The target variable is double peaked. Can't say if it is normally distributed. If it not we will need to transform it so that it becomes normal","e9786734":"No null values here... so one less thing to deal with","be97cc5b":"# EDA 1.0\n\nlets explore the datasets. if there's any processing required, we will do it and perform the EDA again. Hence this is EDA 1.0","3e20a632":"1. both the test and train datasets have the same distribution. So we can say that the test and train datasets are quite identical\n2. Most variables are not normally distributed. (Not that they are required to be)\n3. there could be some outliers","a3fa8a69":"out of all, model2 performs the best. \n\nwith lasso since all the coefficients are 0, we will omit the lasso","4332329c":"No duplicates","66e6fc18":"200000 rows and 15 columns","004971ce":"No categorical variables. Hence no encoding of variables will be required later","83edee9c":"Same case with the test data, the R square value is very low"}}