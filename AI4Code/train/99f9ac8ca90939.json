{"cell_type":{"b675dc98":"code","efe94bc5":"code","9bb7a4df":"code","770be3dd":"code","fef56c13":"code","550eb7e5":"code","371adf58":"code","890ad09d":"code","b65eb99a":"code","b47050d3":"code","633f7f42":"code","73db6d3a":"code","5e3ba2bf":"code","90d7ae9b":"code","50cbf6a3":"code","938480f5":"code","03457e02":"code","29631cbc":"code","db7ffc8d":"code","fa46a7dc":"code","549925ca":"code","1e1de8e9":"code","3eb2fbf2":"code","3fa563c1":"code","3821d9e3":"code","2aa8813e":"code","9ed107da":"code","28651b62":"code","b4c34419":"code","741a5d47":"code","40927d22":"code","41c20699":"code","8613d7b2":"code","70c0da1b":"code","525ee1f2":"code","b1a8f94b":"code","b209a089":"code","370f47b7":"code","bbdcd084":"code","7bfb3466":"code","b50deec6":"code","7dbc7f66":"code","b0b8a1a8":"code","0d28d44d":"code","4be1d68f":"code","86a55eba":"code","6af72997":"code","c5ab2fed":"markdown"},"source":{"b675dc98":"# Models\n# --------\n# FBProphet\n# Loader\/fast aggregrations from: https:\/\/www.kaggle.com\/christoffer\/pandas-multi-indices-for-hts-fast-loading-etc\n\n# FPBTD_v1 Level 9, LB=0.603\n# yearly_seasonality=2 weekly_seasonality=1, n_changepoints = 50, LRMSE=12.128\n\n# FPBTD_v2 - Levels 1 to 9\n# yearly_seasonality=10 weekly_seasonality=3, n_changepoints = 50, US holidays added, LRMSE=11.824\n# lag365, lag28_roll28 min and max\n\n# FPBTD_v3 - Levels 1 to 5\n# Same as previous with quarter_fourier_order=4, L6-L6=0.614\n\n# FPBTD_v4 - Levels 1 to 9\n# quarter_fourier_order=8 + cumulated price, L6-LB=0.616\n\n# FPBTD_v5 - Levels 1 to 9\n# quarter_fourier_order=6, outliers removed, L6-LB=0.637\n\n# FPBTD_v6 - Levels 1 to 9\n# quarter_fourier_order=6, outliers removed + christmas, L6-LB=0.642\n\n# FPBTD_v7 - Levels 1 to 9\n# 2013, quarter_fourier_order=6, outliers removed + christmas, L6-LB=0.61x\n\n# FPBTD_v8 - Levels 1 to 9\n# 2014, quarter_fourier_order=6, outliers removed + christmas, L6-LB=0.63x\n\n# FPBTD_v9 - Levels 1 to 9\n# 2012, quarter_fourier_order=6, outliers removed + christmas, L6-LB=0.618\n\n# FPBTD_v10 - Levels 1 to 9\n# 2012, quarter_fourier_order=6, additional lags, L6-LB=","efe94bc5":"import os, sys, random, gc, math, glob, time\nimport numpy as np\nimport pandas as pd\nimport io, timeit, os, gc, pickle, psutil\nimport joblib\nfrom matplotlib import cm\nfrom datetime import datetime, timedelta\nimport warnings\nfrom tqdm.notebook import tqdm\nfrom multiprocessing import Pool, cpu_count\nfrom joblib import Parallel, delayed\nfrom functools import partial\nfrom collections import OrderedDict\n# from tqdm.contrib.concurrent import process_map\n\n# warnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 4000)","9bb7a4df":"seed = 2020\nrandom.seed(seed)\nnp.random.seed(seed)","770be3dd":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nDEFAULT_FIG_WIDTH = 20\nsns.set_context(\"paper\", font_scale=1.2) ","fef56c13":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, TimeSeriesSplit, KFold, GroupKFold, ShuffleSplit\nfrom sklearn.metrics import accuracy_score, precision_score, cohen_kappa_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nimport csv\nfrom collections import defaultdict\n\n#import lightgbm as lgb\n\nprint('Python    : ' + sys.version.split('\\n')[0])\nprint('Numpy     : ' + np.__version__)\nprint('Pandas    : ' + pd.__version__)\n#print('LightGBM  : ' + lgb.__version__)","550eb7e5":"# !pip install fbprophet --upgrade","371adf58":"import fbprophet\nfrom fbprophet import Prophet\nprint('Prophet  : ' + fbprophet.__version__)","890ad09d":"HOME =  \".\/\"\nDATA_HOME = \"\/kaggle\/input\/m5-forecasting-accuracy\/\"\nTRAIN_DATA_HOME = DATA_HOME\n\nCALENDAR = DATA_HOME + \"calendar.csv\"\nSALES = DATA_HOME + \"sales_train_validation.csv\"\nPRICES = DATA_HOME + \"sell_prices.csv\"\n\nMODELS_DIR = \"models\"\nif not os.path.exists(MODELS_DIR):\n    os.makedirs(MODELS_DIR)\n            \nNUM_SERIES = 30490\nNUM_TRAINING = 1913\nNUM_TEST = NUM_TRAINING + 2 * 28\nMAX_LEVEL = 9 #9","b65eb99a":"# Load data\nseries_ids = np.empty(NUM_SERIES, dtype=object)\nitem_ids = np.empty(NUM_SERIES, dtype=object)\ndept_ids = np.empty(NUM_SERIES, dtype=object)\ncat_ids = np.empty(NUM_SERIES, dtype=object)\nstore_ids = np.empty(NUM_SERIES, dtype=object)\nstate_ids = np.empty(NUM_SERIES, dtype=object)\n\nqties = np.zeros((NUM_TRAINING, NUM_SERIES), dtype=float)\nsell_prices = np.zeros((NUM_TEST, NUM_SERIES), dtype=float)","b47050d3":"# Sales\nid_idx = {}\nwith open(SALES, \"r\", newline='') as f:\n    is_header = True\n    i = 0\n    for row in csv.reader(f):\n        if is_header:\n            is_header = False\n            continue\n        series_id, item_id, dept_id, cat_id, store_id, state_id = row[0:6]\n        # Remove '_validation\/_evaluation' at end by regenerating series_id\n        series_id = f\"{item_id}_{store_id}\"\n\n        qty = np.array(row[6:], dtype=float)\n\n        series_ids[i] = series_id\n\n        item_ids[i] = item_id\n        dept_ids[i] = dept_id\n        cat_ids[i] = cat_id\n        store_ids[i] = store_id\n        state_ids[i] = state_id\n\n        qties[:, i] = qty\n\n        id_idx[series_id] = i\n\n        i += 1","633f7f42":"# Calendar\nwm_yr_wk_idx = defaultdict(list)  # map wmyrwk to d:s\nwith open(CALENDAR, \"r\", newline='') as f:\n    for row in csv.DictReader(f):\n        d = int(row['d'][2:])\n        wm_yr_wk_idx[row['wm_yr_wk']].append(d)","73db6d3a":"# Price\nwith open(PRICES, \"r\", newline='') as f:\n    is_header = True\n    for row in csv.reader(f):\n        if is_header:\n            is_header = False\n            continue\n        store_id, item_id, wm_yr_wk, sell_price = row\n        series_id = f\"{item_id}_{store_id}\"\n        series_idx = id_idx[series_id]\n        for d in wm_yr_wk_idx[wm_yr_wk]:\n            sell_prices[d - 1, series_idx] = float(sell_price)","5e3ba2bf":"# Aggregations - Levels\nqty_ts = pd.DataFrame(qties,\n                      index=range(1, NUM_TRAINING + 1),\n                      columns=[state_ids, store_ids,\n                               cat_ids, dept_ids, item_ids])\n\nqty_ts.index.names = ['d']\nqty_ts.columns.names = ['state_id', 'store_id',\n                        'cat_id', 'dept_id', 'item_id']\n\nprice_ts = pd.DataFrame(sell_prices,\n                        index=range(1, NUM_TEST + 1),\n                        columns=[state_ids, store_ids,\n                                 cat_ids, dept_ids, item_ids])\nprice_ts.index.names = ['d']\nprice_ts.columns.names = ['state_id', 'store_id',\n                          'cat_id', 'dept_id', 'item_id']","90d7ae9b":"LEVELS = {\n    1: [],\n    2: ['state_id'],\n    3: ['store_id'],\n    4: ['cat_id'],\n    5: ['dept_id'],\n    6: ['state_id', 'cat_id'],\n    7: ['state_id', 'dept_id'],\n    8: ['store_id', 'cat_id'],\n    9: ['store_id', 'dept_id'],\n    10: ['item_id'],\n    11: ['state_id', 'item_id'],\n    12: ['item_id', 'store_id']\n}\n\nCOARSER = {\n    'state_id': [],\n    'store_id': ['state_id'],\n    'cat_id': [],\n    'dept_id': ['cat_id'],\n    'item_id': ['cat_id', 'dept_id']\n}","50cbf6a3":"def aggregate_all_levels(df):\n    levels = []\n    for i in range(1, max(LEVELS.keys()) + 1):\n        level = aggregate_groupings(df, i, *LEVELS[i])\n        levels.append(level)\n    return pd.concat(levels, axis=1)\n\ndef aggregate_groupings(df, level_id, grouping_a=None, grouping_b=None):\n    \"\"\"Aggregate time series by summing over optional levels\n\n    New columns are named according to the m5 competition.\n\n    :param df: Time series as columns\n    :param level_id: Numeric ID of level\n    :param grouping_a: Grouping to aggregate over, if any\n    :param grouping_b: Additional grouping to aggregate over, if any\n    :return: Aggregated DataFrame with columns as series id:s\n    \"\"\"\n    if grouping_a is None and grouping_b is None:\n        new_df = df.sum(axis=1).to_frame()\n    elif grouping_b is None:\n        new_df = df.groupby(COARSER[grouping_a] + [grouping_a], axis=1).sum()\n    else:\n        assert grouping_a is not None\n        new_df = df.groupby(COARSER[grouping_a] + COARSER[grouping_b] +\n                            [grouping_a, grouping_b], axis=1).sum()\n\n    new_df.columns = _restore_columns(df.columns, new_df.columns, level_id,\n                                      grouping_a, grouping_b)\n    return new_df","938480f5":"def _restore_columns(original_index, new_index, level_id, grouping_a, grouping_b):\n    original_df = original_index.to_frame()\n    new_df = new_index.to_frame()\n    for column in original_df.columns:\n        if column not in new_df.columns:\n            new_df[column] = None\n\n    # Set up `level` column\n    new_df['level'] = level_id\n\n    # Set up `id` column\n    if grouping_a is None and grouping_b is None:\n        new_df['id'] = 'Total_X'\n    elif grouping_b is None:\n        new_df['id'] = new_df[grouping_a] + '_X'\n    else:\n        assert grouping_a is not None\n        new_df['id'] = new_df[grouping_a] + '_' + new_df[grouping_b]\n\n    new_index = pd.MultiIndex.from_frame(new_df)\n    # Remove \"unnamed\" level if no grouping\n    if grouping_a is None and grouping_b is None:\n        new_index = new_index.droplevel(0)\n    new_levels = ['level'] + original_index.names + ['id']\n    return new_index.reorder_levels(new_levels)","03457e02":"agg_pd = aggregate_all_levels(qty_ts)\nagg_pd.head()","29631cbc":"# All levels\ndf_train = agg_pd.T.reset_index()\ndf_train = df_train.set_index(\"id\") # id as index\nrename_dict = {}\nfor c in df_train.columns:\n    if c not in ['level', 'state_id', 'store_id',   'cat_id',  'dept_id',  'item_id']:\n        rename_dict[c] = \"d_%s\" % c\ndf_train.rename(columns=rename_dict, inplace=True)\nday_cols = pd.Series([c for c in df_train.columns if c not in ['level', 'state_id', 'store_id',   'cat_id',  'dept_id',  'item_id']]) # d_1 to d_1913\nprint(df_train.shape)\ndf_train.head()","db7ffc8d":"# agg_price_pd = aggregate_all_levels(price_ts)\n# agg_price_pd.head()","fa46a7dc":"# df_train_price = agg_price_pd.T.reset_index()\n# df_train_price = df_train_price.set_index(\"id\") # id as index\n# rename_dict = {}\n# for c in df_train_price.columns:\n#     if c not in ['level', 'state_id', 'store_id',   'cat_id',  'dept_id',  'item_id']:\n#         rename_dict[c] = \"d_%s\" % c\n# df_train_price.rename(columns=rename_dict, inplace=True)\n# day_prices_cols = pd.Series([c for c in df_train_price.columns if c not in ['level', 'state_id', 'store_id',   'cat_id',  'dept_id',  'item_id']]) # d_1 to d_1913+\n# print(df_train_price.shape)\n# df_train_price.head()","549925ca":"# Level 12 only\ndf_sale = df_train[df_train[\"level\"] == 12]\ndf_sale.shape","1e1de8e9":"# Levels <= 9\ndf_train = df_train[df_train[\"level\"] <= MAX_LEVEL]\nprint(df_train.shape)","3eb2fbf2":"# df_train_price = df_train_price[df_train_price[\"level\"] <= MAX_LEVEL]\n# print(df_train_price.shape)","3fa563c1":"# Prepare calendar columns\ndf_calendar = pd.read_csv(CALENDAR)\ndf_calendar.index = df_calendar['d'].values # d_xxx as index\ndf_calendar['ds'] = pd.to_datetime(df_calendar['date']) # move date as datetime in \"ds\" column\ndf_calendar['quarter'] = df_calendar['ds'].dt.quarter # add quarter feature\ndf_calendar.head()","3821d9e3":"# Generate holidays ds\nevents1 = pd.Series(df_calendar['event_name_1'].values, index=df_calendar['ds'].values).dropna()\nevents2 = pd.Series(df_calendar['event_name_2'].values, index=df_calendar['ds'].values).dropna()\nholidays = pd.DataFrame(pd.concat([events1, events2], axis=0))\nholidays['ds'] = holidays.index.values\nholidays.rename({0: 'holiday'}, axis=1, inplace=True)\nholidays.reset_index(drop=True, inplace=True)\ndel events1, events2\nholidays.head()","2aa8813e":"# Clean data: remove leading zeros and outliers\ndef clean_data(df_train, day_cols, indx):\n    t = df_train.loc[indx].copy()\n    t.loc[day_cols[((t.loc[day_cols]>0).cumsum()==0).values]] = np.nan\n    q1 = t.loc[day_cols].quantile(0.25)\n    q3 = t.loc[day_cols].quantile(0.75)\n    iqr = q3-q1\n    qm = (q3+1.5*iqr)\n    t.loc[day_cols][t.loc[day_cols]>qm] = qm\n    return t","9ed107da":"future_preds = 28\nday_fit_cols = day_cols","28651b62":"# Remove noticeable dates that are not in evaluation\nignore_dates = [\"2011-12-25\", \"2012-12-25\", \"2013-12-25\", \"2014-12-25\", \"2015-12-25\",\n                \"2011-12-24\", \"2012-12-24\", \"2013-12-24\", \"2014-12-24\", \"2015-12-24\",\n                \"2011-12-31\", \"2012-12-31\", \"2013-12-31\", \"2014-12-31\", \"2015-12-31\",\n                \"2012-01-01\", \"2013-01-01\", \"2014-01-01\", \"2015-01-01\", \"2016-01-01\"]\nignore_days = [] # df_calendar[df_calendar['ds'].isin(ignore_dates)][\"d\"].apply(lambda x: int(x[2:])).values","b4c34419":"FIRST = 338 # 1069 # 704 # 1 # 704\nday_fit_cols = [\"d_%d\"%c for c in range(FIRST, 1914) if c not in ignore_days]\ndf_train = df_train[['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'level'] + day_fit_cols]\ndf_calendar = df_calendar[df_calendar[\"d\"].isin([\"d_%d\"%c for c in range(FIRST, 1942) if c not in ignore_days])]\n#df_prices = df_prices[[\"d_%d\"%c for c in range(FIRST, 1942) if c not in ignore_days]]","741a5d47":"# df_train: \"id\" [\"d_1\", \"d_2\", ...]\n# holidays: [\"holiday\", \"ds\"]\n# df_calendar d [\"date\", \"ds\", \"weekday\", \"wday\", \"month\", \"year\", \"quarter\" ...]\n# df_prices: \"id\" [\"d_1\", \"d_2\", ...]\ndef make_prediction(indx, model_columns = 'yhat', ret_columns = 'yhat', full_predict = True):\n    global df_train, holidays, df_calendar # df_train_price, df_prices\n    # full_predict = True\n    # Return either series or dataframe\n    # model_columns = 'yhat' # [\"yhat\", \"yhat_lower\", \"yhat_upper\"]  # 'yhat' [\"yhat\"]\n    # ret_columns = model_columns # + [\"ds\", \"y\"]\n    changepoints=list()\n    uncertainty_samples=False # False (True to include yhat_upper ...)\n    changepoint_prior_scale=0.1\n    changepoint_range=0.9\n    n_changepoints=50\n    holidays_prior_scale=10\n    yearly_seasonality=10 #2\n    weekly_seasonality=3 #1\n    daily_seasonality=False\n    monthly_fourier_order=8\n    quarter_fourier_order=6 # 6 #None\n    seasonality_prior_scale=10\n    seasonality_mode = 'multiplicative'  # 'additive'\n    \n    target = df_train.loc[indx, day_fit_cols] # sales for one time series\n    # target_price = df_train_price.loc[indx, day_prices_cols] # day_fit_cols\n\n    snap_state_id = str(df_train.loc[indx, 'state_id'])\n    cols = ['ds', 'month', 'wday', 'quarter']\n    if snap_state_id in [\"CA\", \"TX\", \"WI\"]:\n        cols = cols + ['snap_'+snap_state_id]\n\n    # Create temporary dataframe for prediction from 2011-01-29\tto 2016-05-22 (d_1941) initialized with NaN for values to predict\n    # [\"ds\", \"y\", \"prices\", \"month\", \"wday\", \"quarter\", \"snap_xx\"] (snap matching to state related to id)\n    df = df_calendar.iloc[:target.shape[0]+future_preds][cols].copy()\n    df['y'] = target    \n    \n    # Clip outliers in aggregated time series\n    #q1 = df['y'].quantile(0.25)\n    #q3 = df['y'].quantile(0.75)\n    #iqr = q3-q1\n    #qm_up = (q3+1.5*iqr)\n    #qm_dw = (q1-1.5*iqr)\n    #df.loc[df[\"y\"] > qm_up, \"y\"] = qm_up\n    #df.loc[df[\"y\"] < qm_dw, \"y\"] = qm_dw\n    \n    #df['ft_lag365'] = df['y'].shift(365)\n    #df[\"ft_lag365\"].fillna(method ='bfill', inplace = True)\n    df['ft_lag28'] = df['y'].shift(28)\n    df[\"ft_lag28\"].fillna(method ='bfill', inplace = True)\n    df['ft_lag35'] = df['y'].shift(35)\n    df[\"ft_lag35\"].fillna(method ='bfill', inplace = True)\n    df['ft_lag42'] = df['y'].shift(42)\n    df[\"ft_lag42\"].fillna(method ='bfill', inplace = True)\n    df['ft_lag28_roll28_std'] = df['y'].shift(28).rolling(28).std()\n    df[\"ft_lag28_roll28_std\"].fillna(method ='bfill', inplace = True)\n    df['ft_lag28_roll28_max'] = df['y'].shift(28).rolling(28).max()\n    df[\"ft_lag28_roll28_max\"].fillna(method ='bfill', inplace = True)\n    df['ft_lag28_roll28_min'] = df['y'].shift(28).rolling(28).min()\n    df[\"ft_lag28_roll28_min\"].fillna(method ='bfill', inplace = True)\n    # Add prices\n    #df['prices'] = target_price.astype(np.float32)\n    #df[\"prices\"].fillna(method ='bfill', inplace = True)\n    #df['ft_prices_roll7_std'] = df['prices'].rolling(7).std()\n    #df[\"ft_prices_roll7_std\"].fillna(method ='bfill', inplace = True)    \n\n    GROWTH = 'linear' #'logistic' #'linear'\n    m = Prophet(growth=GROWTH, uncertainty_samples=uncertainty_samples, changepoint_prior_scale=changepoint_prior_scale, changepoint_range=changepoint_range,\n                n_changepoints = n_changepoints,\n                holidays_prior_scale=holidays_prior_scale, yearly_seasonality=yearly_seasonality,\n                daily_seasonality=daily_seasonality, weekly_seasonality=weekly_seasonality,\n                holidays=holidays, seasonality_mode=seasonality_mode, seasonality_prior_scale=seasonality_prior_scale)\n    \n    m.add_country_holidays(country_name='US')\n\n    if not monthly_fourier_order is None:\n        m.add_seasonality(name='monthly', period=365.25\/12, fourier_order=monthly_fourier_order)\n    if not quarter_fourier_order is None:\n        m.add_seasonality(name='quarterly', period=365.25\/4, fourier_order=quarter_fourier_order)\n\n    # Add regressor for month, wday, quarter (snap_XX, prices)\n    for reg in df.columns:\n        if reg!='ds' and reg!='y':\n            m.add_regressor(reg)\n\n    target_first_valid_index = target.first_valid_index()\n\n    if GROWTH == \"logistic\":\n        df[\"cap\"] = df[\"y\"].max()\n        df[\"floor\"] = df[\"y\"].min()\n        \n    # Fit on existing data (first_valid_index = Return index for first non-NA\/null value.)\n    m.fit(df.loc[target.loc[target_first_valid_index:].index])\n\n    # Remove target\n    if 'y' not in ret_columns:\n        df.drop(['y'], axis=1, inplace=True)\n\n    res = None\n    if full_predict == True:\n        forecast = m.predict(df.loc[target_first_valid_index:]) # For all days with valid data\n        forecast[\"yhat\"] = forecast[\"yhat\"].astype(np.float32)\n        res = forecast[model_columns]\n        # Update prediction from first valid index from 2016-05-22\n        res.index = df.loc[target_first_valid_index:].index.values\n        res = df.merge(res, left_index=True, right_index=True, how=\"left\")[ret_columns]\n\n    else:\n        forecast = m.predict(df.iloc[-future_preds:]) # for last 28 days (2016-04-25 to 2016-05-22)\n        forecast[\"yhat\"] = forecast[\"yhat\"].astype(np.float32)\n        res = forecast[model_columns]\n        # Update prediction index from d_1914 to d_1941\n        res.index = df.iloc[-future_preds:].index.values\n\n    return (indx, res)","40927d22":"# Basic EDA and hyperparameters tuning\n# model_columns = ['yhat']\n# ret_columns = model_columns + [\"ds\", \"y\"]\n\ntrain_indxs = [\"Total_X\"] # L1\ntrain_indxs = train_indxs + [\"CA_X\", \"TX_X\", \"WI_X\"] # L2\ntrain_indxs = train_indxs + ['CA_1_X', 'CA_2_X', 'CA_3_X','CA_4_X', 'TX_1_X', 'TX_2_X', 'TX_3_X', 'WI_1_X', 'WI_2_X', 'WI_3_X'] # L3\ntrain_indxs = train_indxs + ['HOBBIES_X', 'FOODS_X', 'HOUSEHOLD_X'] # L4\ntrain_indxs = train_indxs + ['HOBBIES_1_X', 'HOBBIES_2_X', 'FOODS_1_X', 'FOODS_2_X', 'FOODS_3_X', 'HOUSEHOLD_1_X', 'HOUSEHOLD_2_X'] # L5\n\nm_score = 0\nfor train_ind in train_indxs:\n    (_, pred) = make_prediction(train_ind, model_columns = ['yhat'], ret_columns = ['yhat', 'ds', 'y']) # 'prices'\n    fig, ax = plt.subplots(figsize=(DEFAULT_FIG_WIDTH, 4))\n    d = pred.set_index(\"ds\").plot(kind=\"line\", y=[\"y\", \"yhat\"], ax=ax, linestyle='-', linewidth=0.8) # 'prices'\n    score = np.log1p(mean_squared_error(pred[\"y\"][:-28], pred[\"yhat\"][:-28]))\n    m_score = m_score + (score \/ len(train_indxs))\n    plt.title(\"%s LRMSE=%.3f\" % (train_ind, score))\n    plt.show()\n    #break\nprint(\"Average LRMSE=%.3f\" % m_score)","41c20699":"print('Predicting...', flush=True)\nstart_time = time.time()\n#pool = Pool()\n\ntrain_indxs = df_train.index #[0:10]\n\n# Memory crash with multiple CPU on Kaggle\n# res = pool.map(make_prediction, train_indxs)\n# res = process_map(make_prediction, train_indxs, chunksize=1)\nres = []\nfor train_indx in tqdm(train_indxs):\n    r = make_prediction(train_indx)\n    res.append(r)\n    \n#pool.close()\n#pool.join()\nend_time = time.time()\nprint('Exec speed=%.2f' %((end_time-start_time)\/train_indxs.shape[0]))","8613d7b2":"# Convert back to initial format\ntmp = pd.DataFrame()\nfor result in res:\n    uid = result[0]\n    ret = result[1].rename(uid)\n    tmp = pd.concat([tmp, ret], axis=1)\nfbp_pd = tmp.T\nfbp_pd.index.name = \"id\"\nfbp_pd.head(10)","70c0da1b":"fbp_pd.to_pickle(\"model_fit.pkl.gz\", compression=\"gzip\")","525ee1f2":"yhat_df = fbp_pd.reset_index()\nprint(yhat_df.shape)\nyhat_df.head()","b1a8f94b":"df_sale = df_sale.reset_index()\ndf_sale.head()","b209a089":"# Merge back to get levels info\nyhat_df = pd.merge(yhat_df, df_train.reset_index()[['id', 'level', 'state_id', 'store_id',   'cat_id',  'dept_id',  'item_id']], on=[\"id\"], how=\"left\")\nprint(yhat_df.shape)\nyhat_df.head()","370f47b7":"print(yhat_df[\"level\"].unique())","bbdcd084":"level_coef_dict = {\n    12: [\"id\"], # L12 x30490\n    10: [\"item_id\"], # L10 x3049\n    5: [\"dept_id\"], # L5 x7\n    4: [\"cat_id\"], # L4 x3\n    3: [\"store_id\"], # L3 x10\n    2: [\"state_id\"],  # L2 x3\n    1: [\"all\"],  # L1 x1\n    11: [\"state_id\", \"item_id\"], # L11 x9167\n    7: [\"state_id\", \"dept_id\"],  # L7 x21\n    9: [\"store_id\",\"dept_id\"],  # L9 x70\n    6: [\"state_id\", \"cat_id\"], # L6 x 9\n    8: [\"store_id\",\"cat_id\"], # L8 x30\n}","7bfb3466":"# Top-Down prediction\ndef predict_topdown(df, df_level12, items):\n    df_level_forecast = pd.DataFrame()\n    for idx, row in df.iterrows():\n        item1 = items[0]\n\n        if item1 is not \"all\":\n            item1_id = row[item1]\n        if len(items) == 2:\n            item2 = items[1]\n            item2_id = row[items[1]]\n\n        if item1 is not \"all\":\n            # Find all level 12 items for the dept_id, store_id pair\n            if len(items) == 2:\n                df_item = df_level12.loc[(df_level12[item1] == item1_id) & (df_level12[item2] == item2_id)][['id']]\n            else:\n                df_item = df_level12.loc[df_level12[item1] == item1_id][['id']]\n        else:\n            df_item = df_level12[['id']]\n        #print(df_item.shape)\n        #display(df_item.head())\n\n        # Sum sales from last 28 days in level 12 training\n        if item1 is not \"all\":\n            if len(items) == 2:\n                df_item['val'] = df_level12[(df_level12[item1] == item1_id) & (df_level12[item2] == item2_id)].iloc[:, np.r_[0,-28:0]].sum(axis = 1)\n            else:\n                df_item['val'] = df_level12[df_level12[item1] == item1_id].iloc[:, np.r_[0,-28:0]].sum(axis = 1)\n        else:\n            df_item['val'] = df_level12.iloc[:, np.r_[0,-28:0]].sum(axis = 1)\n        #display(df_item.head())\n\n        # Back to per id prediction\n        for i in range(1,29):\n            col = \"d_%d\" % (1913 + i)\n            p_col = \"F%d\" % i\n            df_item[p_col] = (df_item['val'] * float(row[col]) \/ df_item['val'].sum())\n        #display(df_item.head())\n\n        df_level_forecast = pd.concat([df_level_forecast, df_item])\n    return df_level_forecast.drop(columns=[\"val\"])","b50deec6":"for key, value in level_coef_dict.items():\n    if (key <= MAX_LEVEL) and (key >= 1):\n        predictions = yhat_df[yhat_df[\"level\"] == key]\n        df_levelx_forecast = predict_topdown(predictions, df_sale, value)\n        print(\"Top-Down prediction for level %s, %s, items=%s into %s\" % (key, value, predictions.shape[0], df_levelx_forecast.shape[0]))\n        df_levelx_forecast.to_pickle(MODELS_DIR + \"\/level_%d.pkl.gz\" % key, compression=\"gzip\")\n        # display(df_levelx_forecast.head())\nprint(df_levelx_forecast.shape)\ndf_levelx_forecast.head()","7dbc7f66":"tmp_pd1 = df_levelx_forecast[df_levelx_forecast[\"id\"].isin([\"FOODS_1_001_CA_1\"])]\ntmp_pd1 = pd.melt(frame = tmp_pd1, \n                  id_vars = ['id'],\n                  var_name = \"F\",\n                  value_vars = [c for c in tmp_pd1.columns if \"F\" in c],\n                  value_name = \"yhat\")\nd = tmp_pd1.plot(kind=\"line\", x=\"F\", y=\"yhat\")","b0b8a1a8":"SAMPLE_SUBMISSION = TRAIN_DATA_HOME + \"sample_submission.csv\"\nsub_ids = pd.read_csv(SAMPLE_SUBMISSION)[['id']]\nsub_ids = sub_ids[sub_ids.id.str.endswith(\"validation\")]","0d28d44d":"for key, value in level_coef_dict.items():\n    if (key <= MAX_LEVEL) and (key >= 1):\n        predictions = yhat_df[yhat_df[\"level\"] == key]\n        df_levelx_forecast = predict_topdown(predictions, df_sale, value)\n        print(\"Top-Down prediction for level %s, %s, items=%s into %s\" % (key, value, predictions.shape[0], df_levelx_forecast.shape[0]))\n        df_levelx_forecast[\"id\"] = df_levelx_forecast[\"id\"].astype(str) + \"_validation\"\n        part1 = pd.merge(sub_ids, df_levelx_forecast, on=\"id\", how=\"left\")\n        part2 = part1.copy()\n        part2[\"id\"] = part2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n        sub = pd.concat([part1, part2])\n        MODE_LEVEL_PATH = MODELS_DIR + \"\/level_%d\" % key\n        if not os.path.exists(MODE_LEVEL_PATH):\n            os.makedirs(MODE_LEVEL_PATH)\n        sub.to_csv(MODE_LEVEL_PATH + \"\/submission.csv.gz\", compression=\"gzip\", index=False)","4be1d68f":"print(sub.shape)","86a55eba":"sub.head()","6af72997":"sub.tail()","c5ab2fed":"Prepare data for Prophet"}}