{"cell_type":{"4a06fd22":"code","367d175d":"code","5e9fe7c9":"code","f7a1340e":"code","1bb3d6c6":"code","9ed8d583":"code","b213bba9":"code","0360a5b5":"code","7913a3cf":"code","93c5f8f8":"code","265d681f":"code","d7ec8add":"code","18414921":"code","7ac54c46":"code","994a3715":"markdown","4aa68d22":"markdown","b05924b0":"markdown","7988ec85":"markdown","dad0e2c2":"markdown"},"source":{"4a06fd22":"#import required packages..\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom dateutil import relativedelta\nimport tensorflow as tf","367d175d":"df = pd.read_excel('..\/input\/timeseries-data\/Data.xlsx')","5e9fe7c9":"df=df[['Date','Key','Volume','avg_T','precipitation']]","f7a1340e":"df.Date = pd.to_datetime(df.Date,format='%d-%m-%Y')\n#Brand_list = df.Key.unique()\n#Brand_list = ['B']\nBrand_list = ['B','C']","1bb3d6c6":"from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n\nfinal = pd.DataFrame()\n\nfor brand_name in Brand_list:\n    print(brand_name)\n    brand_df = df.loc[df.Key == brand_name]\n    brand_df.set_index('Date',inplace=True)\n    brand_df = brand_df[:'2019-12-01']\n    tmp = []\n    forecast = pd.DataFrame()\n    Actuals = pd.DataFrame()\n    Actual_backup = brand_df[['Volume']]\n    for i in range(7,0,-1):\n        train_date = brand_df.index.max() - relativedelta.relativedelta(months=i)\n        test_date = train_date + relativedelta.relativedelta(months=1)\n       \n        x = brand_df.drop(columns=['Volume','Key'])\n        y = brand_df[['Volume']]\n        \n        X_st = StandardScaler()\n        x_transformed = X_st.fit_transform(x)\n        \n        \n        y_st = StandardScaler()\n        y_tranformed = y_st.fit_transform(y)\n        \n        x_final = pd.DataFrame(x_transformed,index=brand_df.index,columns=x.columns)\n        y_final = pd.DataFrame(y_tranformed,index=brand_df.index,columns=y.columns)\n        \n        train_x = x_final[:train_date]\n        train_y = y_final[:train_date][['Volume']]\n        test_x = x_final[test_date:test_date]\n        test_y = y_final[test_date:test_date][['Volume']]\n        \n        \n        # define generator\n        n_input = 24\n        n_features = len(df.columns)-3\n        generator = TimeseriesGenerator(np.array(train_x), np.array(train_y), length=n_input, batch_size=1)\n\n        \n        \n        \n        \n        \n#         #apply scaler transformation before LSTM here standard scaler applied...\n#         X_st = StandardScaler()\n#         train_x = X_st.fit_transform(train_x)\n\n#         y_st = StandardScaler()\n#         train_y = y_st.fit_transform(train_y)\n\n#         X_test_st = StandardScaler()\n#         test_x = X_test_st.fit_transform(test_x)\n\n#         y_test_st = StandardScaler()\n#         test_y = y_test_st.fit_transform(test_y)\n        \n#         train_x = np.reshape(np.array(train_x), (train_x.shape[0], 1, train_x.shape[1]))\n#         test_x = np.reshape(np.array(test_x), (test_x.shape[0], 1, test_x.shape[1]))\n        \n\n        \n        simple_lstm_model = tf.keras.models.Sequential([\n        tf.keras.layers.LSTM(64,return_sequences=True,input_shape=(n_input, n_features)),\n        tf.keras.layers.Dropout(0.2),   #apply dropout for increase in performance..\n        tf.keras.layers.LSTM(32,return_sequences=True),\n        tf.keras.layers.Dropout(0.2),   #apply dropout for increase in performance..\n        tf.keras.layers.LSTM(16,return_sequences=True),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1)])\n        simple_lstm_model.compile(optimizer='adam', loss='mae')\n        simple_lstm_model.fit_generator(generator,epochs=100)\n        \n        first_eval_batch = np.array(train_x[-n_input:])\n        current_batch = first_eval_batch.reshape((1, n_input, n_features))\n        forecast[str(brand_name)+str('_')+str(test_date.month)+str(\"_\")+str(test_date.year)] = y_st.inverse_transform(simple_lstm_model.predict(current_batch))[0][0]\n        Actuals[str(brand_name)+str('_')+str(test_date.month)+str(\"_\")+str(test_date.year)] =  Actual_backup[test_date:test_date].Volume.values\n        \n\n    if ((len(forecast)>0) & (len(Actuals)>0)):\n        forecast=forecast.T.reset_index()\n        forecast.columns=[\"Brand\",\"Forecast_values\"]\n        Actuals=Actuals.T.reset_index()\n        Actuals.columns=[\"Brand\",\"Actual_values\"]\n        brand_wise_merge = forecast.merge(Actuals,on=\"Brand\",how=\"left\")\n        final = final.append(brand_wise_merge,ignore_index=True)\n    else:\n        print(\"No forecast\")","9ed8d583":"df[df.Key == 'B'].tail(20)","b213bba9":"plt.figure(figsize=(15,8))\nplt.plot(final.Actual_values,label='Actual value')\nplt.plot(final.Forecast_values,label='Forecasted value')\nplt.legend()\nplt.show()","0360a5b5":"final","7913a3cf":"final['Error']=np.abs(final['Actual_values']-final['Forecast_values'])\nfinal[['Brandname','leMonth','leYear']] = final.Brand.str.split(\"_\",expand=True)","93c5f8f8":"Agg_accuracy = final.groupby(by=['leMonth','leYear']).sum()[['Error','Actual_values','Forecast_values']]\nAgg_accuracy['Accuracy'] = np.round((1-(Agg_accuracy['Error']\/Agg_accuracy['Actual_values']))*100,2)","265d681f":"Agg_accuracy_brand = final.groupby(by=['Brandname']).sum()[['Error','Actual_values','Forecast_values']]\nAgg_accuracy_brand['Accuracy'] = np.round((1-(Agg_accuracy_brand['Error']\/Agg_accuracy_brand['Actual_values']))*100,2)","d7ec8add":"Agg_accuracy","18414921":"Agg_accuracy_brand","7ac54c46":"final.to_excel('LSTM_Forecast.xlsx')","994a3715":"## Vanishing\/Exploding gradient in RNNs\n\nVanishing gradient problem in neural networks and it is very similar in RNNs. The Gradient exponentially decays as its backpropagated.\n\nAs explained before RNN work on sequences or time dependent data. If we are feeding sentences as input to the RNN, at each time step one word is fed into network. Thus, one can consider this process is equivalent to a single layer of neural network. \n\n\nThus, unlike traditional neural network, the number of time steps increases the size of RNN as shown in figure above. The larger the number of layers, the higher chances of vanishing gradient. \n\n\n## Inability to learn long term dependencies\n\nDue to vanishing gradient, the weights in the starting of network will observe very small changes. Thus, RNN becomes unable to learn long term dependencies. In order to handle long term dependencies, LSTM and GRU models are used.\nE.g. \n\nA hungry cat is looking for something to eat. <br>\nShe sees a little grey mouse sitting near his house. <br>\nI want to catch that little mouse, - says the hungry cat.<br>\nShe sits down and begins to cry \"mew, mew, mew\". <br>\n\n__Note__\n1. Language can have a very long term dependencies.\n2. From the e.g., we can see tha a word tha appeared very early in the text\/sentence can affect a word that come very late in the sentence.\n3. RNN's are bad at this. They forget the long term past easily.\n","4aa68d22":"![image.png](attachment:image.png)","b05924b0":"# *LSTM Cell (Long-Short Term Memory Cell)*\n\n* Like GRUs, LSTMs remembers long term depedencies and eliminates the problem of vanishing gradients.\n\n\nE.g. We've placed no constraints on how our model updates, so its knowledge can change pretty chaotically: at one frame it thinks the characters are in the US, at the next frame it sees the characters eating sushi and thinks they're in Japan, and at the next frame it sees polar bears and thinks they're on Hydra Island. <br>\n\nThis chaos means information quickly transforms and vanishes, and it's difficult for the model to keep a long-term memory. So what you\u2019d like is for the network to learn how to update its beliefs (scenes without Bob shouldn't change Bob-related information, scenes with Alice should focus on gathering details about her), in a way that its knowledge of the world evolves more gently.\n\n__Figure. Vanilla RNN (The repeating module in a standard RNN contains a single layer)__\n\nLSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.\n\n\n\n__Figure. LSTMs (The repeating module in an LSTM contains four interacting layers.)__\n\n\nLSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.\n\nThe key to LSTMs is the __cell state__, the horizontal line running through the top of the diagram.\n\nThe cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It\u2019s very easy for information to just flow along it unchanged.\n\n\n\n__Figure. Memory state__\n\n\nGates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.\n\n\n\n__Figure.Gate__\n\n\nThe sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means \u201clet nothing through,\u201d while a value of one means \u201clet everything through!\u201d","7988ec85":"# *Recurrent Neural Network*\n## Introduction\n\n- A class of artificial neural network\n- Operates on sequence of vectors like sentences, time series data instead of fixed sized vectors like images\n- Used when the output is dependent on the context of input data\n\nExample:\n1. Suppose you want to predict the next scene in a movie. For that you need to know what happened in the movie before. Only when you have the context then can you relate everything in the movie.\n\n2. You have to predict the next word in a sentence. We can do that only if we have information on previous words.For example, Can you predict the word that will come after \"Eat\"? or would it be easier to predict the word that comes after \"Do you want to eat the pie that I\"?\n\n\nRNN remembers previous inputs and the relations between them to predict the output unlike other neural networks where all inputs are independent of each other.","dad0e2c2":"# For modelling we follow differnt steps:\n1. iterate over all brands and take out single brand each time.\n2. provide train and test date based on requirement.\n3. divide train and test data as le cycle works for whole year. ex apr is start so (3+9,4+8 etc..) each time upto december we have to complete, start date could be any.\n4. Very first step in any LSTM model to scale the data at 1 scale, otherwise it will create Noise and hence forecasting will not be upto the mark\n5. Define Timeseries Generator which generates timeseries data in input output format. suppose last 3 days sales is 100,110,145 then it will create a array [100,110,145] here 145 would be target and 100,110 as independent regressor\n6. Once Data ready apply the LSTM layer followed by Drop out layer and at the end output layer as Dense layer.Remember its always good to start with less nmber of neuron, but based on use case can be changed\n7. Here we are following the Teacher Forcing technique.Teacher forcing is a strategy for training recurrent neural networks that uses model output from a prior time step as an input. Models that have recurrent connections from their outputs leading back into the model\n8. save the result and iterate till end."}}