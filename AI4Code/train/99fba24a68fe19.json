{"cell_type":{"c0e5ae45":"code","bca067ab":"code","01303ef2":"code","aa79e628":"code","6cfe2265":"code","31576f48":"code","454b8806":"code","f085cf9b":"code","ef19ff88":"code","b689de04":"code","ddf1ef96":"code","b8d45d1e":"code","be9ff517":"code","278959d6":"code","4421e178":"code","f018b653":"code","89073206":"code","f1afbf97":"code","df4180a7":"code","b0161558":"code","ddf53997":"code","9a7b1fcf":"code","5a988a6d":"code","62ec56f8":"code","1b75cbc4":"code","c3d217d7":"code","17684617":"code","3c783ee8":"code","51f19f02":"code","e33554cc":"code","83a14dab":"code","0fb30d49":"code","e8ae48e7":"code","c267eef5":"code","ad341d50":"code","c7674313":"code","db9419f0":"code","77d5d61e":"code","ce0ef33b":"code","34d4acfa":"code","0315abe9":"code","8afaaea0":"code","cf631729":"markdown","f6d2e3ba":"markdown","53de88d7":"markdown","74937599":"markdown","f8595d98":"markdown","82bc6b7d":"markdown","1af14025":"markdown","83854cd1":"markdown","bb3b95f5":"markdown","8519e5cb":"markdown","ff36f1ee":"markdown","4d82929b":"markdown","2e906e2a":"markdown","76d93e32":"markdown","f49b808f":"markdown","4397a203":"markdown","48b64ed1":"markdown","06f1fef9":"markdown","d92d053d":"markdown"},"source":{"c0e5ae45":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\nfrom math import ceil\n\nfrom sklearn.preprocessing import LabelEncoder\n\nwarnings.filterwarnings('ignore')","bca067ab":"## Simple \"Memory profilers\" to see memory usage\ndef get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]\/2.**30, 2) \n        \ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)","01303ef2":"## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","aa79e628":"## Merging by concat to not lose dtypes\ndef merge_by_concat(df1, df2, merge_on):\n    merged_gf = df1[merge_on]\n    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n    return df1","6cfe2265":"########################### Vars\n#################################################################################\nTARGET = 'sales'         # Our main target\n#END_TRAIN = 1913         # Last day in train set\nEND_TRAIN = 1941         # Last day in train set\nMAIN_INDEX = ['id','d']  # We can identify item by these columns","31576f48":"########################### Load Data\n#################################################################################\nprint('Load Main Data')\n\n# Here are reafing all our data \n# without any limitations and dtype modification\n#train_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ntrain_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\nprices_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ncalendar_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv')","454b8806":"train_df.head()","f085cf9b":"# 'sales_train_evaluation.csv' has extra 28 days, for memory exhaust, drop 1st XXXdays (dropping 28days was not enouth)\ntrain_df.drop(train_df.columns[6:6+300], axis=1, inplace=True)\ntrain_df","ef19ff88":"########################### Make Grid\n#################################################################################\nprint('Create Grid')\n\n# We can tranform horizontal representation \n# to vertical \"view\"\n# Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n# and labels are 'd_' coulmns\n\nindex_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\ngrid_df = pd.melt(train_df, \n                  id_vars = index_columns, \n                  var_name = 'd', \n                  value_name = TARGET)\n\n","b689de04":"grid_df.head()","ddf1ef96":"# If we look on train_df we se that \n# we don't have a lot of traning rows\n# but each day can provide more train data\nprint('Train rows:', len(train_df), len(grid_df))\n\n# To be able to make predictions\n# we need to add \"test set\" to our grid\nadd_grid = pd.DataFrame()\nfor i in range(1,29):\n    ## \u4e0a\u304b\u3089\u53c2\u7167\u7528\u306b\u30b3\u30d4\u30fc\n    ## index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n    ## inde_columns\u3060\u3051\u3067df\u3092\u4f5c\u308b\u3002\n    temp_df = train_df[index_columns]\n    temp_df = temp_df.drop_duplicates()\n    ## \u4e0a\u304b\u3089\u53c2\u7167\u7528\u306b\u30b3\u30d4\u30fc\n    ## END_TRAIN = 1941   \n    ## \"d\"column\u306b\u3001d_1942,d_1943\u3092\u8ffd\u52a0\n    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n    ## TARGET = 'sales' sales\u306e\u304b\u3089\u3092\u6e96\u5099 \n    temp_df[TARGET] = np.nan\n    ## \u3064\u307e\u308a\u3001d_1942,d_1943\u3001\u3001\u3001\u3067\u3001sales\u304c\u7a7a\u306e\u3082\u306e\u3092\u4f5c\u3063\u3066\u3044\u308b\u3002\n    add_grid = pd.concat([add_grid,temp_df])\n\ngrid_df = pd.concat([grid_df,add_grid])\ngrid_df = grid_df.reset_index(drop=True)\n\n","b8d45d1e":"grid_df.tail()","be9ff517":"# Remove some temoprary DFs\ndel temp_df, add_grid\n\n# We will not need original train_df\n# anymore and can remove it\ndel train_df\n\n# You don't have to use df = df construction\n# you can use inplace=True instead.\n# like this\n# grid_df.reset_index(drop=True, inplace=True)\n\n# Let's check our memory usage\nprint(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n\n# We can free some memory \n# by converting \"strings\" to categorical\n# it will not affect merging and \n# we will not lose any valuable data\nfor col in index_columns:\n    grid_df[col] = grid_df[col].astype('category')\n\n# Let's check again memory usage\nprint(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))","278959d6":"prices_df.head(10)","4421e178":"########################### Product Release date\n#################################################################################\nprint('Release week')\n\n# It seems that leadings zero values\n# in each train_df item row\n# are not real 0 sales but mean\n# absence for the item in the store\n# we can safe some memory by removing\n# such zeros\n\n# Prices are set by week\n# so it we will have not very accurate release week \nrelease_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n","f018b653":"release_df.head()","89073206":"release_df.columns = ['store_id','item_id','release']\n\n","f1afbf97":"release_df.head()","df4180a7":"# Now we can merge release_df\ngrid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\ndel release_df\n","b0161558":"grid_df.head()","ddf53997":"# We want to remove some \"zeros\" rows\n# from grid_df \n# to do it we need wm_yr_wk column\n# let's merge partly calendar_df to have it\ngrid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n                      \n","9a7b1fcf":"grid_df.head()","5a988a6d":"# Now we can cutoff some rows \n# and safe memory \ngrid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\ngrid_df = grid_df.reset_index(drop=True)\n\n","62ec56f8":"grid_df.head()","1b75cbc4":"# Let's check our memory usage\nprint(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n\n# Should we keep release week \n# as one of the features?\n# Only good CV can give the answer.\n# Let's minify the release values.\n# Min transformation will not help here \n# as int16 -> Integer (-32768 to 32767)\n# and our grid_df['release'].max() serves for int16\n# but we have have an idea how to transform \n# other columns in case we will need it\ngrid_df['release'] = grid_df['release'] - grid_df['release'].min()\ngrid_df['release'] = grid_df['release'].astype(np.int16)\n\n# Let's check again memory usage\nprint(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))","c3d217d7":"########################### Save part 1\n#################################################################################\nprint('Save Part 1')\n\n# We have our BASE grid ready\n# and can save it as pickle file\n# for future use (model training)\ngrid_df.to_pickle('grid_part_1.pkl')\n\nprint('Size:', grid_df.shape)","17684617":"########################### Prices\n#################################################################################\nprint('Prices')\n\n# We can do some basic aggregations\nprices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\nprices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\nprices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\nprices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n\n# and do price normalization (min\/max scaling)\nprices_df['price_norm'] = prices_df['sell_price']\/prices_df['price_max']\n\n# Some items are can be inflation dependent\n# and some items are very \"stable\"\nprices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\nprices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n\n","3c783ee8":"calendar_df.head()","51f19f02":"# I would like some \"rolling\" aggregations\n# but would like months and years as \"window\"\ncalendar_prices = calendar_df[['wm_yr_wk','month','year']]\ncalendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\nprices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n\n\n","e33554cc":"calendar_prices.head()","83a14dab":"del calendar_prices","0fb30d49":"# Now we can add price \"momentum\" (some sort of)\n# Shifted by week \n# by month mean\n# by year mean\nprices_df['price_momentum'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\nprices_df['price_momentum_m'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\nprices_df['price_momentum_y'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n\ndel prices_df['month'], prices_df['year']","e8ae48e7":"prices_df.head()","c267eef5":"########################### Merge prices and save part 2\n#################################################################################\nprint('Merge prices and save part 2')\n\n# Merge Prices\noriginal_columns = list(grid_df)\ngrid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n","ad341d50":"grid_df.head()","c7674313":"keep_columns = [col for col in list(grid_df) if col not in original_columns]\ngrid_df = grid_df[MAIN_INDEX+keep_columns]\ngrid_df = reduce_mem_usage(grid_df)\n\n# Safe part 2\ngrid_df.to_pickle('grid_part_2.pkl')\nprint('Size:', grid_df.shape)\n\n# We don't need prices_df anymore\ndel prices_df\n\n# We can remove new columns\n# or just load part_1\ngrid_df = pd.read_pickle('grid_part_1.pkl')","db9419f0":"########################### Merge calendar\n#################################################################################\ngrid_df = grid_df[MAIN_INDEX]\n\n# Merge calendar partly\nicols = ['date',\n         'd',\n         'event_name_1',\n         'event_type_1',\n         'event_name_2',\n         'event_type_2',\n         'snap_CA',\n         'snap_TX',\n         'snap_WI']\n\ngrid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n\n# Minify data\n# 'snap_' columns we can convert to bool or int8\nicols = ['event_name_1',\n         'event_type_1',\n         'event_name_2',\n         'event_type_2',\n         'snap_CA',\n         'snap_TX',\n         'snap_WI']\nfor col in icols:\n    grid_df[col] = grid_df[col].astype('category')\n\n","77d5d61e":"# Convert to DateTime\ngrid_df['date'] = pd.to_datetime(grid_df['date'])\n\n# Make some features from date\ngrid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\ngrid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\ngrid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\ngrid_df['tm_y'] = grid_df['date'].dt.year\ngrid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\ngrid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x\/7)).astype(np.int8)\n\ngrid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\ngrid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n\n# Remove date\ndel grid_df['date']","ce0ef33b":"########################### Save part 3 (Dates)\n#################################################################################\nprint('Save part 3')\n\n# Safe part 3\ngrid_df.to_pickle('grid_part_3.pkl')\nprint('Size:', grid_df.shape)\n\n# We don't need calendar_df anymore\ndel calendar_df\ndel grid_df","34d4acfa":"########################### Some additional cleaning\n#################################################################################\n\n## Part 1\n# Convert 'd' to int\ngrid_df = pd.read_pickle('grid_part_1.pkl')\ngrid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n\n# Remove 'wm_yr_wk'\n# as test values are not in train set\ndel grid_df['wm_yr_wk']\ngrid_df.to_pickle('grid_part_1.pkl')\n\ndel grid_df","0315abe9":"########################### Summary\n#################################################################################\n\n# Now we have 3 sets of features\ngrid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n                     axis=1)\n                     \n# Let's check again memory usage\nprint(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\nprint('Size:', grid_df.shape)\n\n# 2.5GiB + is is still too big to train our model\n# (on kaggle with its memory limits)\n# and we don't have lag features yet\n# But what if we can train by state_id or shop_id?\nstate_id = 'CA'\ngrid_df = grid_df[grid_df['state_id']==state_id]\nprint(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n#           Full Grid:   1.2GiB\n\nstore_id = 'CA_1'\ngrid_df = grid_df[grid_df['store_id']==store_id]\nprint(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n#           Full Grid: 321.2MiB\n\n# Seems its good enough now\n# In other kernel we will talk about LAGS features\n# Thank you.","8afaaea0":"########################### Final list of features\n#################################################################################\ngrid_df.info()","cf631729":"\u3010\u30e1\u30e2\u3011<br>\ngrid_df\u306e\u5185\u5bb9\u3092\u4e00\u65e6\u9162\u6f2c\u3051\u3002<br>\n\u524d\u306b\u9162\u6f2c\u3051\u3057\u305f\u3082\u306e\u3092\u8aad\u307f\u76f4\u3057\u3066\u3044\u308b<br>\n\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u3092\u6e1b\u3089\u3059\u65bd\u7b56\u304b\u306a\u3002\u30e1\u30e2\u30ea\u304c\u8c4a\u5bcc\u306a\u3089\u3001\uff12\u3064\u306edataframe\u3092\u4f7f\u3048\u3070\u3044\u3044\u3060\u3051\u3002","f6d2e3ba":"\u3010\u30e1\u30e2\u3011<br>\n\u4e0b\u306emerge_by_concat\u306f\u3001grid_df\u3068release_df\u3067[\"store_id\",\"item_id\"]\u304c\u540c\u3058\u884c\u3092\u63a2\u3057\u3001release_df\u306e\u884c\u3092\u3001grid_df\u306b\u30de\u30fc\u30b8\u3057\u3066\u3044\u308b\u3002","53de88d7":"\u3010\u30e1\u30e2\u3011<br>\n\u4e0b\u306egroupby\u306f\u3001\u307e\u3068\u307e\u308a\u3054\u3068\u306b\u306a\u306b\u304b\u51e6\u7406\uff08\u3053\u306e\u5834\u5408\u306f\u3001agg([\"min\"])\uff09\u3092\u884c\u3046\u3068\u304d\u306b\u4f7f\u3046\u3002groupby\u3068\u4e00\u7dd2\u306b\u3088\u304f\u4f7f\u3046\u306e\u304cmean()<br>\ngroupby\u306f\u3053\u3053\u304c\u308f\u304b\u308a\u3084\u3059\u3044\u3002<br>\nhttps:\/\/deepage.net\/features\/pandas-groupby.html <br>\n.agg\u306b\u3064\u3044\u3066\u306f\u3001\u3053\u3061\u3089\u3002\u95a2\u6570\u3092\u5b9f\u884c\u3057\u3064\u3064\u3001column\u3092\u4ed8\u3051\u52a0\u3048\u308b\u3088\u3046\u306a\u52d5\u304d\uff1f <br>\nhttp:\/\/sinhrks.hatenablog.com\/entry\/2014\/10\/13\/005327","74937599":"\u3010\u30e1\u30e2\u3011<br>\nwm_yr_wk\u3068release\u3092\u6bd4\u8f03\u3057\u3001wm_yr_wk\u306e\u307b\u3046\u304c\u5927\u304d\u3044\u884c\u3060\u3051\u629c\u304d\u51fa\u3057\u3066\u3001\u65b0\u3057\u3044grid_df\u3092\u4f5c\u3063\u3066\u3044\u308b\u3002","f8595d98":"\u3010\u30e1\u30e2\u3011<br>\ncalendar_prices = calendar_df[['wm_yr_wk','month','year']]\u306f\u3001wm_yr_wk , month , year column\u3060\u3051\u629c\u304d\u51fa\u3057\u3001\n\u65b0\u3057\u3044dataframe calendar_prices\u306b\u3057\u3066\u3044\u308b\u3002<br>\n\u9805\u76ee\u6e1b\u3089\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u884c\u5358\u4f4d\u3067\u91cd\u8907\u304c\u767a\u751f\u3059\u308b\u305f\u3081\u3001drop_duplicates\u3067\u91cd\u8907\u524a\u9664\u3002<br>\n\u76f4\u5f8c\u306b\u3001prices_df\u3078\u30de\u30fc\u30b8\u3002<br>\nmerge_by_concat\u3092\u4f7f\u3063\u3066\u3044\u306a\u3044\u304c\u3001prices_df\u3068calender_prices\u3067\u3001'wm_yr_wk'\u304c\u540c\u3058\u884c\u3092prices_df\u306b\u8ffd\u52a0\u3057\u3066\u3044\u308b\u3002<br>\n\u8ffd\u52a0\u3057\u3066\u3044\u308b\u306e\u306f\u3001calenter_prices\u306e'wm_yr_wk','month','year'\u3060\u3051\u3002<br>\nmerge\u306f\uff12\u3064\u306etable\u3092\u7d50\u5408\u3057\u3066\u3001\u597d\u304d\u306atable\u306b\u3059\u308b\u3068\u304d\u306b\u3088\u304f\u4f7f\u3046\u3002<br>\n\u4ed6\u306b\u306f\u3001\u6b63\u898f\u5316\u3055\u308c\u305fdb(SQL\u7b49)\u304b\u3089\u3001\u610f\u56f3\u3057\u305ftable\u3092\u4f5c\u308b\u3068\u304d\u306b\u3082\u4f7f\u3046\u3002<br>","82bc6b7d":"\u3010\u30e1\u30e2\u3011<br>\nto_datetime\u306f\u3001\u65e5\u4ed8\u8868\u793a\u3092datetime\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u3057\u3066\u3044\u308b\u3002<br>\ndatetime\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306f\uff12\u3064\u306e\u65e5\u6642\u306e\u6f14\u7b97\u304c\u3067\u304d\u305f\u308a\u6bd4\u8f03\u304c\u3067\u304d\u308b\u306e\u3067\u3001\u4fbf\u5229\u3067\u826f\u304f\u4f7f\u3044\u307e\u3059\u3002<br>","1af14025":"\u3010\u30e1\u30e2\u3011<br>\n\u6b21\u3082\u3001\u65b0\u3057\u3044column(\u7279\u5fb4\u91cf)\u3092\u8ffd\u52a0\u3057\u3066\u3044\u308b\u3002<br>\ntransform\u306b\u3064\u3044\u3066\u306f\u3053\u3061\u3089\u3002<br>\nhttps:\/\/qiita.com\/greenteabiscuit\/items\/132e0f9b1479926e07e0","83854cd1":"\u3010\u30e1\u30e2\u3011<br>\n\u30e1\u30e2\u30ea\u304c\u8db3\u308a\u306a\u304f\u306a\u308b\u306e\u3067\u3001\u53e4\u3044\uff13\uff10\uff10\u65e5\u5206\u306e\u30c7\u30fc\u30bf\u3092\u843d\u3068\u3057\u3066\u3044\u308b\u3002<br>\n\u4e0a\u306ehead\u3092\u898b\u308b\u3068\u308f\u304b\u308b\u304c\u3001\u5de6\u304b\u3089\u53f3\u3078\uff16\u500b\u5206\u306f\u5fc5\u8981\u306a\u3082\u306e\u306a\u306e\u3067\u3001\u305d\u308c\u3092Skip\u3057\u3066\u3001\uff13\uff10\uff10\u65e5\u5206\u30c7\u30fc\u30bf\u524a\u9664\u3002<br>\n<br>\n\u5b9f\u52d9\u306e\u30c7\u30fc\u30bf\u51e6\u7406\u3067\u3082\u4f7f\u3044\u305d\u3046\u306a\u6280\u8853\u3067\u3059****<br>","bb3b95f5":"\u3010\u30e1\u30e2\u3011<br>\n\u4e0b\u306emelt\u306f\u3053\u3061\u3089\u304c\u308f\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002<br>\nhttps:\/\/qiita.com\/ishida330\/items\/922caa7acb73c1540e28\n","8519e5cb":"\u3010\u30e1\u30e2\u3011<br>\nto_pickle\u306f\u3001\u8efd\u91cfDB\u5316\u3057\u3066\u4fdd\u5b58\u3057\u3066\u3044\u308b\u3002\u8907\u6570\u5f62\u306epickles\u306f\u30d4\u30af\u30eb\u30b9\u306e\u3053\u3068\u3002<br>\u3088\u304f\u9162\u6f2c\u3051\u3068\u304b\u8a00\u308f\u308c\u308b\u3002","ff36f1ee":"\u3010\u30e1\u30e2\u3011<br>\nprices_df\u3092\u6271\u3046\u306e\u3067\u3001\u307e\u305a\u306f\u3058\u3081\u306b\u5f62\u3092\u78ba\u8a8d\u3002","4d82929b":"**python\u306b\u4e0d\u6163\u308c\u305f\u4eba\u306b\u3082\u308f\u304b\u308a\u3084\u3059\u3044\u3088\u3046\u306b\u30e1\u30e2\u3092\u8ffd\u52a0\u3057\u3066\u3044\u307e\u3059\u3002**<br>\n<br>\n\u3053\u3053\u3067\u306f\u3001\u3053\u306e\u5f8c\u306etraining\u7b49\u3067\u4f7f\u3044\u305f\u3044table\u3092\u3064\u304f\u308b\u3053\u3068\u3060\u3051\u3057\u3066\u3044\u307e\u3059\u3002\n\n","2e906e2a":"\u3010\u30e1\u30e2\u3011<br>\nprices_df['price_max']\u306a\u3069\u306f\u3001\u65b0\u3057\u3044column(\u7279\u5fb4\u91cf feature)\u3092\u8ffd\u52a0\u3057\u3066\u3044\u308b\u3002\ngroupby\u3092\u4f7f\u3063\u3066\u3044\u308b\u306e\u3067\u3001\"store_id\"\u3068\"item_id\"\u304c\u540c\u3058\u3082\u306e\u306emax\/min\/std\/mean\u3092\u3001\u65b0\u3057\u3044column\u306b\u8ffd\u52a0\u3057\u3066\u3044\u308b\u3002","76d93e32":"\u3010\u30e1\u30e2\u3011<br>\n\u9593\u306bgrid_df.head()\u3092\u5165\u308c\u308b\u305f\u3081\u3001\u3082\u3068\u306f\u4e00\u3064\u3060\u3063\u305fCell\u3092\uff13\u3064\u306b\u3076\u3063\u305f\u5207\u308a\u307e\u3057\u305f\u3002<br>\n**\u7dad\u6301\u3057\u305f\u3044Index\u3092\u305d\u306e\u307e\u307e\u306b\u3001row\u3092\u5897\u3084\u3059\u6280\u8853\u306f\u5b9f\u52d9\u3067\u3082\u6709\u52b9\u3068\u601d\u3044\u307e\u3059\u3002**","f49b808f":"\u3010\u30e1\u30e2\u3011<br>\nevaluation\u304c\u51fa\u3066\u304d\u305f\u306e\u3067\u3001END_TRAIN\u306e\u6570\u5b57\u304c\u5927\u304d\u304f\u306a\u3063\u3066\u3044\u308b\u3002\n1913\u65e5\u5206\u304b\u3089\uff11\uff19\uff14\uff11\u65e5\u5206","4397a203":"\u3010\u30e1\u30e2\u3011<br>\n\u3053\u3053\u304b\u3089\u3001\u30c7\u30fc\u30bf\u3092\u6574\u3048\u3066\u3044\u307e\u3059\u3002\nastype\u3067\u3001dataframe\u306e\u30c7\u30fc\u30bf\u578b\u3092column\u3054\u3068\u306b\u6307\u5b9a\u3057\u3001memory\u4f7f\u7528\u91cf\u3092\u6e1b\u3089\u3057\u3066\u3044\u307e\u3059\u3002","48b64ed1":"\u3010\u30e1\u30e2\u3011<br>\n\u4e0a\u3067\u3001grid_df\u306e\u4e0b\u5074\u306b\u3001d_1942\u4ee5\u964d\u306e\u3082\u306e\u304c\u3001sales\u304cNaN\u3067****\u8ffd\u52a0\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002","06f1fef9":"[col for col ...]\u306f\u3001original_columns\u3068grid_df\u306e\u30b3\u30e9\u30e0\u306eand\u3092\u3068\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u308b\u3002<br>\n\u3068\u3082\u306bset()\u306b\u3057\u3066And\u3057\u305f\u307b\u3046\u304c\u308f\u304b\u308a\u3084\u3059\u305d\u3046\u3002","d92d053d":"\u3010\u30e1\u30e2\u3011<br>\n\u4e0a\u3067\u3001id,item_id,dept_id,cat_id,store_id,state_id\u306f\u7dad\u6301\u3057\u305f\u307e\u307e\u3001d_301\u304b\u3089\u30b9\u30bf\u30fc\u30c8\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002"}}