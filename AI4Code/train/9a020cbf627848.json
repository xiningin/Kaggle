{"cell_type":{"db2d47d5":"code","74cfb7e1":"code","4451de68":"code","a2dd0713":"code","423460f1":"code","bf15a614":"code","fd01c66b":"code","76be13fa":"code","8d2ccadb":"code","f8d45878":"code","34addd7b":"code","f36ae7b4":"code","d8a29a03":"code","e369974a":"code","542e4229":"code","a297439d":"code","f47a4f70":"code","6a09df98":"code","3ac272ba":"code","b6841894":"code","d3845d0f":"code","e10d08df":"code","3fccede0":"code","644c0ccf":"code","fccb9f59":"code","2385a040":"code","ced817eb":"code","2a58d550":"code","3dc4c81d":"code","a3a64379":"code","507a6450":"code","b9f7b908":"code","81f1b899":"code","622a0635":"code","d40d826b":"code","aba34698":"code","4d55cd99":"code","d5de4732":"code","fad5349d":"code","db3e327e":"code","6555a894":"code","56291eb3":"code","9f2c6ad2":"code","4329d3c3":"code","0d9e291e":"code","019dab99":"code","a30ed2fb":"code","266a55f8":"code","187933bc":"code","ee3e54ba":"code","79a1da93":"code","8d2ad93d":"code","32c451cb":"code","afaf869d":"markdown","2caa6015":"markdown","5e1ae88f":"markdown","d6b4a614":"markdown","02d91254":"markdown","b4afb419":"markdown","4d14a308":"markdown","61661aa1":"markdown","989667cc":"markdown","4029dc1b":"markdown","202d19ae":"markdown","9cc6b5d3":"markdown","0e6dc630":"markdown","595682cd":"markdown","b6abaee9":"markdown","7c0dfc6b":"markdown","01a1d98b":"markdown","70dea201":"markdown","a02ef6fb":"markdown","1fd8a17d":"markdown","3c35b58b":"markdown","d0743069":"markdown","e2867a6a":"markdown","7ba23f50":"markdown","7118a5bd":"markdown","f722f1cb":"markdown"},"source":{"db2d47d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","74cfb7e1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline","4451de68":"raw_data = pd.read_csv('..\/input\/Absenteeism_at_work.csv')\nraw_data.head()","a2dd0713":"#Below code it just to let Ipython to show all the columns and rows\npd.options.display.max_columns = None\npd.options.display.max_rows = None","423460f1":"df = raw_data.copy()\ndf.info() # this will give us number of rows and data types at each column \n#There are no missing values in our table","bf15a614":"df.describe()","fd01c66b":"# we already said there is no missing value another way to do this is to plot heatmap graph df.isnull()\nsns.heatmap(df.isnull(),cbar=False, yticklabels=False, cmap='plasma')","76be13fa":"# Let's get rid of unnecessary columns\ndf.drop(['ID'], axis=1, inplace=True)","8d2ccadb":"# Let's how many reasons for absence are there\n# We already know from descriptive statistics that min is 0 and max is 28\ndf['Reason for absence'].unique()","f8d45878":"sorted(df['Reason for absence'].unique())","34addd7b":"reasons = pd.get_dummies(df['Reason for absence'], drop_first=True)\nreasons.head()","f36ae7b4":"reason_type1 = reasons.iloc[:, 0:14].max(axis=1)\nreason_type2 = reasons.iloc[:, 15:17].max(axis=1)\nreason_type3 = reasons.iloc[:, 18:21].max(axis=1)\nreason_type4 = reasons.iloc[:, 22:28].max(axis=1)","d8a29a03":"reason_type1.head(10)","e369974a":"df = pd.concat([df, reason_type1, reason_type2, reason_type3, reason_type4], axis=1)\ndf.head()","542e4229":"# drop 'Reason for absence' column\ndf.drop('Reason for absence', axis=1, inplace=True)","a297439d":"df.columns.values","f47a4f70":"column_names = ['Month of absence', 'Day of the week',\n       'Seasons', 'Transportation expense',\n       'Distance from Residence to Work', 'Service time', 'Age',\n       'Work load Average\/day ', 'Hit target', 'Disciplinary failure',\n       'Education', 'Body mass index', 'Absenteeism time in hours', 'reason_1',\n       'reason_2', 'reason_3', 'reason_4']","6a09df98":"df.columns = column_names\ndf.head()","3ac272ba":"df = df[['reason_1', 'reason_2','reason_3', 'reason_4', 'Month of absence', 'Day of the week',\n       'Seasons', 'Transportation expense',\n       'Distance from Residence to Work', 'Service time', 'Age',\n       'Work load Average\/day ', 'Hit target', 'Disciplinary failure',\n       'Education', 'Body mass index', 'Absenteeism time in hours']]\ndf.head()","b6841894":"df_reason_modified = df.copy()","d3845d0f":"df_reason_modified['Education'].unique()","e10d08df":"df_reason_modified['Education'].value_counts()","3fccede0":"df_reason_modified['Education'] = df_reason_modified['Education'].map({1:0, 2:1, 3:1, 4:1})\ndf_reason_modified['Education'].unique()","644c0ccf":"df_reason_modified.drop(['Hit target'], axis=1, inplace=True)","fccb9f59":"df_preprocessed = df_reason_modified.copy()\ndf_preprocessed.head(10)","2385a040":"median = df_preprocessed['Absenteeism time in hours'].median()\nmedian","ced817eb":"targets = np.where(df_preprocessed['Absenteeism time in hours']>median, 1,0)","2a58d550":"targets[:10]","3dc4c81d":"# let's check the ratio\ntargets.sum()\/targets.shape[0]\n","a3a64379":"df_preprocessed['Excessive Absenteeism'] = targets\ndf_preprocessed.head()","507a6450":"# data_with_targets = df_preprocessed.drop(['Absenteeism time in hours', 'reason_4', 'Body mass index', 'Age', 'Day of the week', 'Distance from Residence to Work'], axis=1)\ndata_with_targets = df_preprocessed.drop(['Absenteeism time in hours'], axis=1)","b9f7b908":"data_with_targets.head()","81f1b899":"unscaled_inputs = data_with_targets.iloc[:,:-1]\nunscaled_inputs.head()","622a0635":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\nclass CustomScaler(BaseEstimator, TransformerMixin):\n    def __init__(self, columns, copy=True, with_mean=True, with_std=True):\n        self.scaler = StandardScaler(copy, with_mean, with_std)\n        self.columns = columns\n        self.mean_ = None\n        self.std_ = None\n    \n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.std_ = np.std(X[self.columns])\n        return self\n    \n    def transform(self, X, y=None, copy=None):\n        init_col_order = X.columns\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        X_not_scaled = X.loc[:, ~X.columns.isin(self.columns)]\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]\n        ","d40d826b":"unscaled_inputs.columns.values","aba34698":"columns_to_omit = ['reason_1', 'reason_2', 'reason_3', 'reason_4','Disciplinary failure', 'Education',]\ncolumns_to_scale = [x for x in unscaled_inputs if x not in columns_to_omit]","4d55cd99":"absenteeism_scaler = CustomScaler(columns_to_scale)","d5de4732":"absenteeism_scaler.fit(unscaled_inputs)","fad5349d":"scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)","db3e327e":"scaled_inputs.head()","6555a894":"scaled_inputs.shape","56291eb3":"from sklearn.model_selection import train_test_split","9f2c6ad2":"X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, targets, test_size=0.2, \n                                                   random_state=42)","4329d3c3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","0d9e291e":"lr_model = LogisticRegression(solver='lbfgs')\nlr_model.fit(X_train, y_train)","019dab99":"lr_model.score(X_train, y_train)","a30ed2fb":"# or we could calculate accuracy manually like this\noutputs = lr_model.predict(X_train)","266a55f8":"outputs","187933bc":"print('total corretly predicted', np.sum(outputs == y_train))\nprint('accuracy', np.sum(outputs == y_train)\/ outputs.shape[0])","ee3e54ba":"summary_table = pd.DataFrame(columns=['Feature'], data=unscaled_inputs.columns.values)\nsummary_table['Coefficients'] = lr_model.coef_.T # Takiign the transpose\nsummary_table","79a1da93":"# Let's add intercept as well\nsummary_table.index = summary_table.index+1\nsummary_table.loc[0] = ['Intercept', lr_model.intercept_[0]]\nsummary_table = summary_table.sort_index()\nsummary_table","8d2ad93d":"summary_table.sort_values('Coefficients', ascending=False)","32c451cb":"lr_model.score(X_test, y_test)","afaf869d":"# Education","2caa6015":"Lastly, 'Transportation expense', 'Distance from Residence to Work', 'Service time', 'Age', 'Work load Average\/day ', 'Disciplinary failure','Body mass index' are important indicator, therefore I am not going to leave them intact. However, 'Hit target' is closely related with 'Work load Average\/day ' therefore I am going to drop it.","5e1ae88f":"# Training Data","d6b4a614":"Let's concatenate these new columns we created and drop reasons for absence column.","02d91254":"Reason 20 has never been used as an excuse","b4afb419":"These values repressent high school, graduate, post-graduate and master-Phd. Majority of employees are high school graduate and around 100 of them have college or higher degree. We will map high school to 0 and rest to 1. Are we losing any information here? Probably we do, but I doubt that will make a big change on our model's prediction. ","4d14a308":"column names 0, 1, 2 and 3 doesn't look that good. Let's rename them and reorder the columns. We want to see the reasons at the beginnign of the table","61661aa1":"# Descriptive Statistics","989667cc":"Great our dataset is balanced. Around 46% of the targets are 1s and 54% are zeros. That will work for logistic regression. For logistic regression usually 40 to 60 will work as well but that is not true for other algorithm such as NN. A balance of 45-55 is almost sufficient for NN.","4029dc1b":"How do we interpret coefficients? The closer a coefficient to zero, the less its predictive power. In logistic regression we take the log meaning coefficients are odds. So if an odd is close to zero that means for one standard deviation increase (not unit because we stadardized variables) it is close to zero times as likely a person will be absent. So in this case, \nService time, reason_4, Body mass index, Age, Day of the week, Distance from Residence to Work are not nessary. \nIf you go up all the way to data_with_targets check point and drop these variables and and rerun all the notebook you will get similar result. \n ","202d19ae":"## Checkpoint","9cc6b5d3":"As you can see all the dummies are reamined untouched","0e6dc630":"# Create a CheckPoint\nAt this point we have done a lot of preprocession. We would like to make a copy of the dataframe to reduce risk of losing important data at later stage. Gave a name that will give you some information about what you have done until this point","595682cd":"In this tutorial we will try predict whether an employee is absent more than median time (or acceptable) or not using absentee data. There is not much informtion about the dataset on Kaggle. You can familirize yourself with the data here: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Absenteeism+at+work <br>\nIf you like this tutorial please upvote it and let me know if you have any questions. Thanks","b6abaee9":"Having 27 dummy is not desirable, instead we can group reason for absence columns. If you look at the data we can group reason for absence into 4 categories\ndisease related 0-14, pregnancy related 15-17,external 18-21, visit 22-28. \nCode below requires a little explanation. When we get dummies at each row just one column is equal to 1 the rest are zeros. So instead of assigningn reasons.iloc[:, 1:14] to reason_type1 we assign the maximum value at each row, this way if value at column between 1 and 14 is 1 that means that employeee was absent for disease related excuse, if all are zero we will know that it was not disease related so just add zero to reason_type1","7c0dfc6b":"Test accuracy almost always is lower than training accuracy due overfitting. This model definitely can be improved but that is it from me, it has been an exhaustive work. Feel free to imporve it. Don't forget to up vote it, if you like it.","01a1d98b":"We get exactly the same number.\nNext let's get the coefficients and intercept to interpret our model","70dea201":"That is not bad!","a02ef6fb":"# Split the data into train and test","1fd8a17d":"We will turn 'Reason for absence' column into dummy variable becuase although you might think this columns is numeric it is actually categorical. Each number represent different type of excuse.\nAgain check https:\/\/archive.ics.uci.edu\/ml\/datasets\/Absenteeism+at+work  to get familiar with dataset\nLastly, I am  going to drop first column here to avoid multicoliniearity. reason 0 is actually represent no reason is given","3c35b58b":"# Testing the Model\nTraingin accuracy doesn't show anything we should test our model on unseen data meaning we should run our model on X_test ","d0743069":"# Standardize the data\nWe will standardize the data by subtracting the mean and dividing by standard deviation columnwise. However, we do not want to scale dummy variables otherwise they will lose their meanings. Along with dummies we created we are not going to scale Education and Disciplinary failure either since they are just zeros and ones. Unfortunately we can't use standard scaler instead we will create a custom scaler.","e2867a6a":"# Preprocessing","7ba23f50":"# Create Targets\nzero absence is kind of extreme, everyone once in a while can't make it to work. So we will take the median of the 'Absenteeism in hours'. ","7118a5bd":"If an observation has been absent more than 3 hours we will assign them to 1, otherwise to 0. Easy way to do that is to use numpy where function. <br>\nNote: This way we will have balanced dataset, roughly the half of target will be zero and other half will be 1. This will also prevent our model from learning to output only 0s or 1s. If you have used  mean you wouldn't get the same thing unless you were lucky.","f722f1cb":"It is always good practice to make a copy of original dataset so that we can make changes to the copy. We will make multiple copy of the data along the way at each step we make changes to the dataset so that we can go back and forth"}}