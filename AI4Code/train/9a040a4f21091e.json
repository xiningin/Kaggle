{"cell_type":{"75210bae":"code","a2a4e16a":"code","ce6e9a5e":"code","838fb867":"code","e9b8e04d":"code","fd82db1c":"code","3b1f3b72":"code","c31ce03a":"code","97d08652":"code","168e1783":"code","3087dbfb":"code","beb50098":"code","9753279c":"code","79d7d151":"code","8b9d0c1d":"code","1520c357":"code","35a14082":"code","a1af3324":"code","0f28624c":"code","a01c0380":"code","4e4c161e":"code","a81f8aa1":"code","518b27af":"markdown","0c2d3121":"markdown","50d7a0a7":"markdown","7efcc1a7":"markdown","93a8a247":"markdown","6772b66d":"markdown","7991c774":"markdown","20f42691":"markdown","00238466":"markdown","2ebf1a0e":"markdown","f59eb9e2":"markdown","26cc1425":"markdown","01ded7fe":"markdown","f84b51cf":"markdown","e95675af":"markdown","2b892818":"markdown","1f978755":"markdown","985f76b2":"markdown"},"source":{"75210bae":"\nimport numpy as np\nimport pandas as pd\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filename == \"toxic_test.csv\":\n            test_df = pd.read_csv(os.path.join(dirname,filename))\n        elif filename == \"toxic_train.csv\":\n            train_df = pd.read_csv(os.path.join(dirname,filename))\n","a2a4e16a":"\nprint(train_df[train_df[\"toxic\"] == 0][\"comment_text\"].values[0])\nprint(\"\\n\")\nprint(train_df[train_df[\"toxic\"] == 0][\"comment_text\"].values[1])\nprint(\"\\n\")\nprint(train_df[train_df[\"toxic\"] == 0][\"comment_text\"].values[2])\nprint(\"\\n\")\nprint(train_df[train_df[\"toxic\"] == 0][\"comment_text\"].values[3])\nprint(\"\\n\")\nprint(train_df[train_df[\"toxic\"] == 0][\"comment_text\"].values[4])","ce6e9a5e":"print(train_df[train_df[\"toxic\"] == 1][\"comment_text\"].values[0])\nprint(\"\\n\")\nprint(train_df[train_df[\"toxic\"] == 1][\"comment_text\"].values[1])\nprint(\"\\n\")\nprint(train_df[train_df[\"toxic\"] == 1][\"comment_text\"].values[2])\nprint(\"\\n\")\nprint(train_df[train_df[\"toxic\"] == 1][\"comment_text\"].values[3])\nprint(\"\\n\")\nprint(train_df[train_df[\"toxic\"] == 1][\"comment_text\"].values[4])","838fb867":"from matplotlib import pyplot as plt\n\nplt.style.use('fivethirtyeight')\n\ntot = train_df.shape[0]\nnum_toxic = train_df[train_df.toxic == 1].shape[0]\n\nslices = [num_toxic\/tot,(tot - num_toxic)\/tot]\nlabeling = ['Toxic','Non-Toxic']\nexplode = [0.2,0]\nplt.pie(slices,explode=explode,shadow=True,autopct='%1.1f%%',labels=labeling,wedgeprops={'edgecolor':'black'})\nplt.title('Number of Toxic vs. Non-Toxic Text Samples')\nplt.tight_layout()\nplt.show()\n\n","e9b8e04d":"from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n\ncv1 = feature_extraction.text.CountVectorizer(max_df=0.25)\ncv1.fit_transform(train_df[\"comment_text\"])\n\nprint(\"%i words discarded for occurring too frequently:\" %(len(cv1.stop_words_)))\nprint(cv1.stop_words_)\n\ncv2 = feature_extraction.text.CountVectorizer(min_df=5)\ncv2.fit_transform(train_df[\"comment_text\"])\n\nprint(\"%i words discarded for occurring too infrequently:\" %(len(cv2.stop_words_)))\nprint(list(cv2.stop_words_)[0:20])\n\ncount_vectorizer = feature_extraction.text.CountVectorizer(max_df=0.25, min_df=5)\ntrain_vectors = count_vectorizer.fit_transform(train_df[\"comment_text\"])\n\nprint(\"There are %i words in this corpus\" %(train_vectors.shape[1]))\n","fd82db1c":"from sklearn.metrics import f1_score\n\nclf = linear_model.RidgeClassifier().fit(train_vectors, train_df[\"toxic\"])\nprint(\"Percent correctly labeled comments by Ridge Classifier:\")\nprint(clf.score(train_vectors, train_df[\"toxic\"]))\n\nprint(\"f1 score for non-toxic comments:\")\nprint(f1_score(train_df[\"toxic\"],clf.predict(train_vectors),pos_label=0))\n\nprint(\"f1 score for toxic comments:\")\nprint(f1_score(train_df[\"toxic\"],clf.predict(train_vectors),pos_label=1))\n","3b1f3b72":"predict_toxic = clf.predict(train_vectors)\nerror1 = []\nerror2 = []\nfor i in range(len(predict_toxic)):\n    prediction = predict_toxic[i]\n    actual = train_df.iloc[i, 2]\n    if prediction == 0 and actual == 1 and len(error1) < 5:\n        error1.append(train_df.iloc[i,1])\n    elif prediction == 1 and actual == 0 and len(error2) < 5:\n        error2.append(train_df.iloc[i,1])\n\nprint(\"Toxic comments incorrectly labeled as Non-Toxic comments: \")\nprint(\"\\n\")\nfor comment in error1:\n    print(comment)\n    print(\"-\"*120)\n    \nprint(\"Non-Toxic comments incorrectly labeled as Toxic comments: \")\nprint(\"\\n\")\nfor comment in error2:\n    print(comment)\n    print(\"-\"*120)\n    ","c31ce03a":"n = 25\nidx_max = (-clf.coef_).argsort()\n\nprint(\"Most 'toxic' words:\")\nfor i in range(n):\n    num = idx_max[0][i]\n    print(count_vectorizer.get_feature_names()[num])\n\n    \nidx_min = (clf.coef_).argsort()\nprint(\"\\n\")\nprint(\"Most 'non-toxic' words:\")\nfor i in range(n):\n    num = idx_min[0][i]\n    print(count_vectorizer.get_feature_names()[num])\n    ","97d08652":"\nplt.hist(clf.coef_[0,:],bins=500,range=[-0.5,0.5])\nplt.title(\"Distribution of Ridge Classifier Coefficient Values\")\nplt.show()\n\npos = len(clf.coef_[clf.coef_ > 0])\n\ntot = len(clf.coef_[0,:])\n\ntoxic_leaning = pos\/tot\nnon_toxic_leaning = 1 - toxic_leaning\n\nprint(\"Percentage of words with a positive (leaning towards toxic) coefficient:\")\nprint(toxic_leaning)\n\nprint(\"Percentage of words with a negative (leaning towards non-toxic) coefficient:\")\nprint(non_toxic_leaning)","168e1783":"\ncount_vectorizer = feature_extraction.text.TfidfVectorizer(max_df=0.25, min_df=5)\ntrain_vectors = count_vectorizer.fit_transform(train_df[\"comment_text\"])\n\nprint(\"There are %i words in this corpus\" %(train_vectors.shape[1]))\n\nclf = linear_model.RidgeClassifier().fit(train_vectors, train_df[\"toxic\"])\nprint(\"Percent correctly labeled comments by Ridge Classifier:\")\nprint(clf.score(train_vectors, train_df[\"toxic\"]))\n\nprint(\"f1 score for non-toxic comments:\")\nprint(f1_score(train_df[\"toxic\"],clf.predict(train_vectors),pos_label=0))\n\nprint(\"f1 score for toxic comments:\")\nprint(f1_score(train_df[\"toxic\"],clf.predict(train_vectors),pos_label=1))","3087dbfb":"predict_toxic = clf.predict(train_vectors)\nerror1 = []\nerror2 = []\nfor i in range(len(predict_toxic)):\n    prediction = predict_toxic[i]\n    actual = train_df.iloc[i, 2]\n    if prediction == 0 and actual == 1 and len(error1) < 5:\n        error1.append(train_df.iloc[i,1])\n    elif prediction == 1 and actual == 0 and len(error2) < 5:\n        error2.append(train_df.iloc[i,1])\n\nprint(\"Toxic comments incorrectly labeled as Non-Toxic comments: \")\nprint(\"\\n\")\nfor comment in error1:\n    print(comment)\n    print(\"-\"*120)\n    \nprint(\"Non-Toxic comments incorrectly labeled as Toxic comments: \")\nprint(\"\\n\")\nfor comment in error2:\n    print(comment)\n    print(\"-\"*120)","beb50098":"n = 25\nidx_max = (-clf.coef_).argsort()\n\nprint(\"Most 'toxic' words:\")\nfor i in range(n):\n    num = idx_max[0][i]\n    print(count_vectorizer.get_feature_names()[num])\n\n    \nidx_min = (clf.coef_).argsort()\nprint(\"\\n\")\nprint(\"Most 'non-toxic' words:\")\nfor i in range(n):\n    num = idx_min[0][i]\n    print(count_vectorizer.get_feature_names()[num])","9753279c":"\nplt.hist(clf.coef_[0,:],bins=500,range=[-0.5,0.5])\nplt.title(\"Distribution of Ridge Classifier Coefficient Values\")\nplt.show()\n\npos = len(clf.coef_[clf.coef_ > 0])\n\ntot = len(clf.coef_[0,:])\n\ntoxic_leaning = pos\/tot\nnon_toxic_leaning = 1 - toxic_leaning\n\nprint(\"Percentage of words with a positive (leaning towards toxic) coefficient:\")\nprint(toxic_leaning)\n\nprint(\"Percentage of words with a negative (leaning towards non-toxic) coefficient:\")\nprint(non_toxic_leaning)","79d7d151":"import torch   \nfrom torchtext import data\n\ntrain_path = '\/kaggle\/input\/hate-speech-detection\/toxic_train.csv'\ntest_path = '\/kaggle\/input\/hate-speech-detection\/toxic_test.csv'\n\nTEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\nLABEL = data.LabelField(dtype = torch.float,batch_first=True)\n\nfields = [('', None), ('comment_text',TEXT),('toxic', LABEL)]\n\ntraining_data=data.TabularDataset(path = train_path,format = 'csv',fields = fields,skip_header = True)\nprint(vars(training_data.examples[0]))\n","8b9d0c1d":"import random\ntrain_data, valid_data = training_data.split(split_ratio=0.7)\n\n#initialize glove embeddings\nTEXT.build_vocab(train_data,min_freq=3)  \nLABEL.build_vocab(train_data)\n\n#No. of unique tokens in text\nprint(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n\n#No. of unique tokens in label\nprint(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n\n#Commonly used words\nprint(TEXT.vocab.freqs.most_common(10))  \n\n#Word dictionary\n#print(TEXT.vocab.stoi)  ","1520c357":"#check whether cuda is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \nprint(device)\n\n#set batch size\nBATCH_SIZE = 400\n\n#Load an iterator\ntrain_iterator, valid_iterator = data.BucketIterator.splits(\n    (train_data, valid_data), \n    batch_size = BATCH_SIZE,\n    sort_key = lambda x: len(x.comment_text),\n    sort_within_batch=True,\n    device = device)","35a14082":"import torch.nn as nn\n\nclass classifier(nn.Module):\n    \n    #define all the layers used in model\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout):\n        \n        #Constructor\n        super().__init__()          \n        \n        #embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        #lstm layer\n        self.lstm = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout,\n                           batch_first=True)\n        \n        #dense layer\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        \n        #activation function\n        self.act = nn.Sigmoid()\n        \n    def forward(self, text, text_lengths):\n        \n        #text = [batch size,sent_length]\n        embedded = self.embedding(text)\n        #embedded = [batch size, sent_len, emb dim]\n      \n        #packed sequence\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n        \n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        #hidden = [batch size, num layers * num directions,hid dim]\n        #cell = [batch size, num layers * num directions,hid dim]\n        \n        #concat the final forward and backward hidden state\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n                \n        #hidden = [batch size, hid dim * num directions]\n        dense_outputs=self.fc(hidden)\n\n        #Final activation function\n        outputs=self.act(dense_outputs)\n        \n        return outputs","a1af3324":"#define hyperparameters\nsize_of_vocab = len(TEXT.vocab)\nembedding_dim = 100\nnum_hidden_nodes = 32\nnum_output_nodes = 1\nnum_layers = 2\nbidirection = True\ndropout = 0.2\n\n#instantiate the model\nmodel = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n                   bidirectional = True, dropout = dropout)","0f28624c":"import torch.optim as optim\n\n#define optimizer and loss\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.BCELoss()\n\n#define metric\ndef binary_accuracy(preds, y):\n    #round predictions to the closest integer\n    rounded_preds = torch.round(preds)\n    \n    correct = (rounded_preds == y).float() \n    acc = correct.sum() \/ len(correct)\n    return acc\n    \n#push to cuda if available\nmodel = model.to(device)\ncriterion = criterion.to(device)","a01c0380":"from tqdm import tqdm_notebook as tqdm\n\ndef train(model, iterator, optimizer, criterion):\n    \n    #initialize every epoch \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    accs = []\n    losses = []\n    batches = []\n    \n    #set the model in training phase\n    model.train()  \n    \n    batch_num = 0\n    \n    for batch in tqdm(iterator):\n        \n        #resets the gradients after every batch\n        optimizer.zero_grad()   \n        \n        #retrieve text and no. of words\n        text, text_lengths = batch.comment_text   \n        \n        #convert to 1D tensor\n        predictions = model(text, text_lengths).squeeze()  \n        \n        #compute the loss\n        loss = criterion(predictions, batch.toxic)        \n        \n        #compute the binary accuracy\n        acc = binary_accuracy(predictions, batch.toxic)   \n        \n        #backpropage the loss and compute the gradients\n        loss.backward()       \n        \n        #update the weights\n        optimizer.step()      \n        \n        #loss and accuracy\n        epoch_loss += loss.item()  \n        epoch_acc += acc.item()\n        \n        losses.append(loss.item())\n        accs.append(acc.item())\n        batches.append(batch_num)\n        \n        batch_num += 1\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator), losses, accs, batches","4e4c161e":"def evaluate(model, iterator, criterion):\n    \n    #initialize every epoch\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    losses = []\n    accs = []\n    batches = []\n\n    #deactivating dropout layers\n    model.eval()\n    \n    #deactivates autograd\n    with torch.no_grad():\n        \n        batch_num = 0\n    \n        for batch in iterator:\n            \n        \n            #retrieve text and no. of words\n            text, text_lengths = batch.comment_text\n            \n            #convert to 1d tensor\n            predictions = model(text, text_lengths).squeeze()\n            \n            #compute loss and accuracy\n            loss = criterion(predictions, batch.toxic)\n            acc = binary_accuracy(predictions, batch.toxic)\n            \n            #keep track of loss and accuracy\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n            \n            losses.append(loss.item())\n            accs.append(acc.item())\n            batches.append(batch_num)\n            batch_num += 1\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator), losses, accs, batches","a81f8aa1":"N_EPOCHS = 5\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n     \n    #train the model\n    train_loss, train_acc, lossess, accs, batches = train(model, train_iterator, optimizer, criterion)\n    \n    plt.plot(batches, losses)\n    plt.title(\"Loss during epoch %i\" %(epoch))\n    plt.show()\n    \n    plt.plot(batches, accs)\n    plt.title(\"Accuracy during epoch %i\" %(epoch))\n    plt.show()\n    \n    #evaluate the model\n    valid_loss, valid_acc, losses, accs, batches = evaluate(model, valid_iterator, criterion)\n    \n    plt.plot(batches, losses)\n    plt.title(\"Loss during epoch %i\" %(epoch))\n    plt.show()\n    \n    plt.plot(batches, accs)\n    plt.title(\"Accuracy during epoch %i\" %(epoch))\n    plt.show()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        #torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","518b27af":"Now, let's take a step back and look at some trends throughout the training dataset:","0c2d3121":"Handling some imports and creating the train and test pandas dataframes:","50d7a0a7":"So the dataset is clearly biased towards non-toxic comments. Since I'm not quite sure where this data came from, it does make me wonder whether this proportion is close to reality. I'd imagine it varies widely depending on the source; for example, I would guess that 4chan and Pinterest vary widely in their proportion of toxic vs. non-toxic text.","7efcc1a7":"I feel pretty good about the words we eliminated for occurring too frequently, but had some trouble figuring out the threshold for words that occurr too infrequently. ","93a8a247":"We also see that the most and least toxic words make much more sense in this case. ","6772b66d":"Markedly better than CountVectorizer. Let's see which comments it gets wrong...","7991c774":"# Vectorizing Text\nWe'll use scikit-learn's CountVectorizer to vectorize each comment, whereby each word gets assigned to a numerical index. We'll throw away any word that occurs in more than 25% of the comments as well as any word that occurs less than 5 times","20f42691":"The mis-classified toxic as non-toxic speech is about what I expected. For example, the only thing that makes the fourth comment toxic is the \"with your mom\" comment at the end, which can easily be non-toxic depending on the context. \nHowever, the mis-classified non-toxic speech as toxic was surprising. For starters, I definitely would consider at least two of the comments to be toxic, even though in the data they are labeled as non-toxic. This poses serious issues for the analysis going forward if the dataset itself is mislabeled.\n\nLet's take a look at which words contribute to the classifier labeling toxic vs. non-toxic comments:","00238466":"What sticks out to me is that there is a pretty stark gradation within the examples designated toxic speech. I suppose the third example (\"Bye! Don't look, come or think of comming back! Tosser.\") fits into toxic speech, but certainly not hate speech, unless I am totally misinformed as to the definition of the word Tosser. Urban Dictionary says its basically on the same level as Wanker, which I am pretty sure is not hate speech. The first example surely qualified as toxic for the profanity used, although other than that it is not nearly as noxious as the remaining examples, which make references to white power, anti-semitism, and rape. So I'm curiouse to see which comments whatever classifiers we build struggle with due to the wide range of text considered toxic speech here.","2ebf1a0e":"**Toxic Text Examples:**","f59eb9e2":"We can see that now the classifier is more \"eager\" to classify comments as toxic. Again, we have some comments I personally would classify as toxic that the dataset disagrees with. Very curious","26cc1425":"> # TF-IDF\n\nNow, let's do a very similar classification, but instead of CountVectorizer we will be constructing a matrix of Tf-Idf features.\n","01ded7fe":"So why did the percentage of words with a toxic coefficient actually decrease? Observe how the tail for the positive end of the distribution has grown fatter; this means that this model has actually found more words that surely must be toxic, as opposed to before where the vast majority of words were somewhat toxic and somewhat non-toxic. ","f84b51cf":"So we can see that some non-toxic words snuck into the toxic word list and vice versa.","e95675af":"**Non-Toxic Text Examples:**","2b892818":"# Basic EDA\nFirst, let's check out a few examples of non-toxic and toxic speech.","1f978755":"# RNN\nCredit to https:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/first-text-classification-in-pytorch\/ for most of the meat and the main skeleton of this code\n","985f76b2":"Although 93.6% correctly labeled might look good, keep in mind that the most naive classifier we could build (one that says all comments are non-toxic) would have 90% accuracy. So we've got a lot of work to do, evidenced by the f1 score for both classes.ds"}}