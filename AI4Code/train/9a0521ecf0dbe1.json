{"cell_type":{"ba87921c":"code","67829af3":"code","88826e47":"code","3045d707":"code","42e22e24":"code","e90aa1aa":"code","4eb62f75":"code","45e37ef6":"code","2ea08c48":"code","fb098b93":"code","d151ae08":"code","a61a44de":"code","9c51fe8f":"code","29a4e873":"code","274a4fe6":"code","31d34e3e":"code","8c11df5b":"code","e9b2ae92":"code","80fe2409":"code","66448ccb":"code","92bb6402":"code","dd2998e2":"code","dfa8b8c8":"code","93116dac":"code","4df8ea31":"code","f84d3f8b":"code","19a3020b":"code","90accf22":"code","56e625eb":"code","3b1f3c50":"code","a6c6b3c4":"code","6a43af84":"code","ff3c1e8d":"code","04048e5a":"code","ef35d4aa":"code","1aff6569":"code","ff8d89ea":"code","049b908c":"code","4e06e885":"code","4fed43e1":"markdown","990497e4":"markdown","304b56a2":"markdown","7f3c8d5e":"markdown","6c1ddb17":"markdown","1643105b":"markdown","0cd9ce38":"markdown","0d18c983":"markdown","c91af5ea":"markdown","f3c3a0f2":"markdown","9769dafc":"markdown","2c8badf7":"markdown","e5d5958a":"markdown","183e57db":"markdown","e144add3":"markdown","ba5d2f59":"markdown","ea42d54d":"markdown","be78242d":"markdown","97438ff0":"markdown","bdf0522c":"markdown","f579301c":"markdown","1792fb82":"markdown","cfe46a33":"markdown","38ece1e0":"markdown","471bee0f":"markdown"},"source":{"ba87921c":"import numpy as np\nimport pandas as pd\nimport nltk\n# nltk.download()\ndir(nltk)","67829af3":"raw_data = open('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding='ISO-8859-1').read()\nraw_data[0:500]\n","88826e47":"parsed_data = raw_data.replace('\\t','\\n').split('\\n')\nparsed_data[0:10]","3045d707":"label_list = parsed_data[0::2]\nmsg_list = parsed_data[1::2]\nprint(label_list[0:5])\nprint(msg_list[0:5])","42e22e24":"print(len(label_list))\nprint(len(msg_list))\n\nprint(label_list[-3:])\n\ncombined_df = pd.DataFrame({\n    'label': label_list[:-1],\n    'sms': msg_list\n})\n\n\ncombined_df.head()","e90aa1aa":"dataset = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv', sep=\",\",encoding='ISO-8859-1')\ndataset.head()","4eb62f75":"dataset.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\ndataset.info()","45e37ef6":"dataset.columns=['label','sms']\ndataset.head()","2ea08c48":"dataset.tail()","fb098b93":"print(f'Inpute data has {len(dataset)} rows, {len(dataset.columns)} columns')","d151ae08":"print(f'ham = {len(dataset[dataset[\"label\"] == \"ham\"])}')\nprint(f'spam = {len(dataset[dataset[\"label\"] == \"spam\"])}')","a61a44de":"\nprint(f\"Numbers of missing label = {dataset['label'].isnull().sum()}\")\nprint(f\"Numbers of missing msg = {dataset['sms'].isnull().sum()}\")","9c51fe8f":"import string\nstring.punctuation","29a4e873":"def remove_punctuation(txt):\n    txt_nopunct = \"\".join([c for c in txt if c not in string.punctuation])\n    return txt_nopunct","274a4fe6":"dataset['msg_clean'] = dataset['sms'].apply(lambda x: remove_punctuation(x))\ndataset.head()","31d34e3e":"import re\n\ndef tokenize(txt):\n    tokens = re.split('\\W+', txt)\n    return tokens\n\n\ndataset['msg_clean_tokenized'] = dataset['msg_clean'].apply(lambda x: tokenize(x.lower()))\n\ndataset.head()","8c11df5b":"stopwords = nltk.corpus.stopwords.words('english')\nstopwords[0:10]","e9b2ae92":"def remove_stopwords(txt_tokenized):\n    txt_clean = [word for word in txt_tokenized if word not in stopwords]\n    return txt_clean\n\ndataset['msg_no_sw'] = dataset['msg_clean_tokenized'].apply(lambda x: remove_stopwords(x))\ndataset.head()","80fe2409":"from nltk.stem.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\n# dir(porter_stemmer)","66448ccb":"print(porter_stemmer.stem('programer'))\nprint(porter_stemmer.stem('programming'))\nprint(porter_stemmer.stem('program'))","92bb6402":"print(porter_stemmer.stem('run'))\nprint(porter_stemmer.stem('running'))","dd2998e2":"def stemming(tokenized_text):\n    text = [porter_stemmer.stem(word) for word in tokenized_text]\n    return text","dfa8b8c8":"dataset['msg_stemmed'] = dataset['msg_no_sw'].apply(lambda x: stemming(x))\ndataset.head()","93116dac":"# WordNet lexical database for lemmatization\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\nprint(wordnet_lemmatizer.lemmatize('goose'))\nprint(wordnet_lemmatizer.lemmatize('geese'))","4df8ea31":"def lemmatization(token_txt):\n    text = [wordnet_lemmatizer.lemmatize(word) for word in token_txt]\n    return text","f84d3f8b":"dataset['msg_lemmatized'] = dataset['msg_no_sw'].apply(lambda x : lemmatization(x))\ndataset.head()","19a3020b":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\n\ncorpus = [\"This is a sentence is\",\n         \"This is another sentence\",\n         \"third document is here\"]\n\n\nX = cv.fit(corpus)\nprint(X.vocabulary_)\nprint(cv.get_feature_names())","90accf22":"X = cv.transform(corpus)\n#X = cv.fit_transform(corpus)\nprint(X.shape)\nprint(X)\nprint(X.toarray())\n\ndf = pd.DataFrame(X.toarray(), columns = cv.get_feature_names())\nprint(df)","56e625eb":"def clean_text(txt):\n    txt = \"\".join([c for c in txt if c not in string.punctuation])\n    tokens = re.split('\\W+', txt)\n    txt = [porter_stemmer.stem(word) for word in tokens if word not in stopwords]\n    return txt\n\ncv1 = CountVectorizer(analyzer=clean_text)\n\nX = cv1.fit_transform(dataset['sms'])\nprint(X.shape)\n","3b1f3c50":"print(cv1.get_feature_names())","a6c6b3c4":"data_sample = dataset[0:10]\ncv2 = CountVectorizer(analyzer=clean_text)\n\nX = cv2.fit_transform(data_sample['sms'])\nprint(X.shape)\n\ndf = pd.DataFrame(X.toarray(), columns=cv2.get_feature_names())\ndf.head(10)","6a43af84":"def clean_text(txt):\n    txt = \"\".join([c for c in txt if c not in string.punctuation])\n    tokens = re.split('\\W+', txt)\n    txt = \" \".join([porter_stemmer.stem(word) for word in tokens if word not in stopwords])\n    return txt\n\ndataset['sms_clean'] = dataset['sms'].apply(lambda x: clean_text(x))\ndataset.head()","ff3c1e8d":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range=(2,3))\n\ncorpus = [\"This is a sentence is\",\n         \"This is another sentence\",\n         \"third document is here\"]\n\n#X = cv.fit(corpus)\n#print(X.vocabulary_)\n#print(cv.get_feature_names())\n\n#X = cv.transform(corpus)\nX = cv.fit_transform(corpus)\nprint(X.shape)\n#print(X)\n#print(X.toarray())\n\ndf = pd.DataFrame(X.toarray(), columns = cv.get_feature_names())\nprint(df)","04048e5a":"cv1 = CountVectorizer(ngram_range=(2,2))\n\nX = cv1.fit_transform(dataset['sms_clean'])\nprint(X.shape)","ef35d4aa":"print(cv1.get_feature_names())\ndata_sample = dataset[0:10]\ncv2 = CountVectorizer(ngram_range=(2,2))\n\nX = cv2.fit_transform(data_sample['sms_clean'])\nprint(X.shape)","1aff6569":"df = pd.DataFrame(X.toarray(), columns=cv2.get_feature_names())\ndf.head(10)","ff8d89ea":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vect = TfidfVectorizer()\n\ncorpus = [\"This is a sentence is\",\n         \"This is another sentence\",\n         \"third document is here\"]\n\n\nX = tfidf_vect.fit(corpus)\nprint(X.vocabulary_)\nprint(tfidf_vect.get_feature_names())\n\nX = tfidf_vect.transform(corpus)\n#X = cv.fit_transform(corpus)\nprint(X.shape)\nprint(X)\nprint(X.toarray())\n\ndf = pd.DataFrame(X.toarray(), columns = tfidf_vect.get_feature_names())\nprint(df)","049b908c":"data_sample = dataset[0:10]\ntfidf2 = TfidfVectorizer(analyzer=clean_text)\n\nX = tfidf2.fit_transform(data_sample['sms'])\nprint(X.shape)","4e06e885":"df = pd.DataFrame(X.toarray(), columns=tfidf2.get_feature_names())\ndf.head(10)","4fed43e1":"## What is Natural Language Processing?\nNatural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that concerned with the ability of a computer to understand, analyze, manipulate and potentially generate human language.\n\nNatural Language Processing or NLP is a very popular field and has lots of applications in our daily life. From typing a message to auto-classification of mails as Spam or not-spam NLP is everywhere.","990497e4":"**Removing Punctuation**","304b56a2":"**Remove stop words**","7f3c8d5e":"**CountVectorizer**","6c1ddb17":"### Stemming and Lemmatization","1643105b":"## Contents\n\n- Introduction to NLP and NLTK\n\n- NLP Pipeline\n\n- Reading raw data\n\n- Cleaning and Pre-processing\n\n- Tokenization\n\n- Vectorization\n\n- Feature Engineering\n\n- Training ML Algorithm for Classifying Spam and non-spam messages","0cd9ce38":"**TFIDF Vectorizer**","0d18c983":"### Import Packages","c91af5ea":"### NLP Pipeline for Text Data\n![Screenshot%20from%202020-06-30%2018-31-26.png](attachment:Screenshot%20from%202020-06-30%2018-31-26.png)","f3c3a0f2":"\n\nFeature Engineering\n\n\nFeature Engineering - Introduction\n\n\nFeature Creation\n\n\nFeature Evaluation\n\n\nPower Transformations - Box CoxTransformation\n\nBuilding Machine Learning Classifier\n\n\nEvaluation Metrics - Accuracy,Precision and Recall\n\n\nK-Fold Cross-Validation\n\n\nRandom Forest - Introduction\n\n\nBuilding a basic Random Forest\n\n\nRandom Forest with holdout test\n","9769dafc":"### Exploring the Data","2c8badf7":"## Vectorization\nProcess of encoding text as integers to create Feature Vectors.\n\n**Feature Vector:** vector of numerical features that represent an object\n\n### Types of Vectorization\n- Count Vectorization\n- N-grams\n- TF-IDF","e5d5958a":"**Method 2: read_csv()**","183e57db":"###  Reading Text data","e144add3":"**ham\/spam**","ba5d2f59":"**Shape of data**","ea42d54d":"**Lemmatization** is similar to stemming but it brings context to the words.So it goes a steps further by linking words with similar meaning to one word.","be78242d":"## Areas of NLP\n- Sentiment Analysis\n- Topic Modeling\n- Text Classification\n- parts-of-speech Tagging \/ Sentence Segmentation\n\nCore component of NLP is extracting all the information\nfrom a block of text that is relevant to computer\nunderstanding the language [task-specific]","97438ff0":"**Porter Stemmer**","bdf0522c":"**Method 1: open()**","f579301c":"> ## Natural Language Processing is the technology used to aid computers to understand the human\u2019s natural language.","1792fb82":"## Reading and Cleaning Data","cfe46a33":"**Missing data**","38ece1e0":"**Tokenization**","471bee0f":" **WordNetLemmatizer**"}}