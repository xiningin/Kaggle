{"cell_type":{"658bc20a":"code","5d578c70":"code","8375b92f":"code","dbea9b2d":"code","1ecfdacf":"code","d831884d":"code","c4c660c0":"code","c12541fb":"code","42b54035":"code","7db878f4":"code","1ec7e867":"code","279628b3":"code","8d7db94f":"code","69e52dec":"code","bdedfa6f":"code","1ec57ae0":"code","13541b4b":"code","8a617bb2":"code","2ea692b4":"code","9e2d54fd":"code","ffdd9388":"code","56f3a61d":"code","269d1484":"code","13a7e4b0":"code","b1d3e24b":"code","f309ce70":"code","556f0c27":"code","52afb919":"code","1d645952":"code","b8e54aac":"code","d1382846":"code","c0d6d1d3":"code","51286066":"code","1740725f":"code","3b257a63":"code","94c7064e":"code","2abc101e":"code","a8f4489c":"code","fb73a2d3":"code","f31ea6db":"code","94728287":"code","9d3b02d5":"code","09c344a4":"code","c2aa84b7":"code","82715e08":"code","b43645d9":"code","83121dca":"code","a0fc30a4":"code","02646317":"code","ca7632f6":"code","38158069":"code","0d573fad":"code","6ce4ac7d":"code","fd07f7b7":"code","7968bf35":"code","e7ebee25":"code","428b7fe6":"code","1a3fde54":"code","572c9837":"code","8c25636c":"code","22a7e1a0":"code","f78aea1b":"code","329f1689":"code","a21f9ab2":"code","f437ea8f":"code","0c79c508":"markdown","d28a97e2":"markdown","df9eb0c4":"markdown","32e85fa9":"markdown","67a71f01":"markdown","2298006c":"markdown","9db7eb8c":"markdown","f44d1a7a":"markdown","45f6707f":"markdown","f712ba0b":"markdown","b5efbe79":"markdown","714ad624":"markdown","38154683":"markdown","e16bc85e":"markdown","67e38a08":"markdown","70ecd5aa":"markdown","f32bec9a":"markdown","3abe455b":"markdown","3c7b9544":"markdown","c1eb6697":"markdown","54a66caa":"markdown","1ee016aa":"markdown","e07048c9":"markdown","c8c68b37":"markdown","a75eb819":"markdown","566f9fda":"markdown","2dfbc776":"markdown","ea364d17":"markdown","c6e2c9fa":"markdown","41d5b59d":"markdown","8e7acf61":"markdown","560473ef":"markdown","b17fac0f":"markdown","c21f494e":"markdown","1b7cc636":"markdown","bc7537e6":"markdown","a1fe5925":"markdown","0b3dcd4d":"markdown","6d217b14":"markdown","3bee36fd":"markdown","409b3a16":"markdown","65a8e2c5":"markdown","36370088":"markdown","fa192674":"markdown","5af117e2":"markdown","00a5d2be":"markdown","aa76606e":"markdown","f42cbedf":"markdown","699e86f4":"markdown","3b09ac86":"markdown","2a60251b":"markdown","a3569566":"markdown","1cfbb6a2":"markdown","136539fd":"markdown","26ac9280":"markdown","dc196d0e":"markdown","36b7c501":"markdown","7feb3b56":"markdown","e402cd49":"markdown","0be4112c":"markdown","07673134":"markdown","096a3127":"markdown","6ec1f886":"markdown","084f015e":"markdown","c014033a":"markdown","27c029e0":"markdown","dcaf4bba":"markdown","785e31ea":"markdown","a6c94d4b":"markdown","a0a50b51":"markdown","d36af6fd":"markdown","6e22ee0a":"markdown","bdba2ab2":"markdown","80d45ebc":"markdown","8e59452f":"markdown","dae7bcc8":"markdown","6d2e55ca":"markdown","b8dc65bf":"markdown","c0060cd0":"markdown","9ff1b0dd":"markdown","ce9c226d":"markdown"},"source":{"658bc20a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5d578c70":"df=pd.read_csv(\"\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv\")","8375b92f":"df.head()","dbea9b2d":"df.info(verbose=True)","1ecfdacf":"df.describe(include='all').transpose()","d831884d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10,10))\nsns.scatterplot(x='longitude', y='latitude', hue='neighbourhood_group',size='price', data=df)","c4c660c0":"# import Geopandas\nimport geopandas as gpd\n\n# import street map\nstreet_map = gpd.read_file('\/kaggle\/input\/nyc-shapefile\/geo_export_cd000852-2825-4046-9168-5b498fd4c7ca.shp')","c12541fb":"# create figure and axes, assign to subplot\nfig, ax = plt.subplots(1,1,figsize=(10,10))\n\n#plotting the price as a function of longitude and latitude\ng=df.plot(x='longitude',y='latitude', c='price',kind='scatter',cmap='jet',ax=ax,zorder=5)\n\n# add .shp mapfile to axes\nstreet_map.plot(ax=ax,alpha=0.6,color='black')\nplt.show()","42b54035":"sns.distplot(df['price'],kde = False)\nplt.show()","7db878f4":"#adding a small constant to the price column to prevent log(0) issue and assign a new column\ndf['log_price'] = np.log(df['price']+1)","1ec7e867":"sns.distplot(df['log_price'],kde = False)\nplt.show()","279628b3":"# create figure and axes, assign to subplot\nfig, ax = plt.subplots(1,1,figsize=(10,10))\n\n#plotting the price as a function of longitude and latitude\ndf.plot(x='longitude',y='latitude', c='log_price',kind='scatter',cmap='jet',ax=ax,zorder=5)\n\n# add .shp mapfile to axes\nstreet_map.plot(ax=ax,alpha=0.6,color='black',legend=True)\nplt.show()","8d7db94f":"plt.figure(figsize=(15,10))\n\n#utilizing Sebaorn boxplot. Categorizing log_price into neighbourhood_group for differnt room types\nsns.boxplot(x=\"room_type\", y=\"log_price\",hue=\"neighbourhood_group\",data=df)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","69e52dec":"#creating bins\nbins = [-np.inf, 100, 200, 300, 400, np.inf]\n\n#assigning names to each bin\nnames = ['0 to 100', '100 to 200', '200 to 300', '300 to 500', '500 and above']\n\n#creating a new column containing bin information\ndf['num_review_cat'] = pd.cut(df['number_of_reviews'], bins, labels=names)","bdedfa6f":"g=sns.catplot(x=\"num_review_cat\", y=\"log_price\", hue=\"neighbourhood_group\", data=df, kind=\"violin\")\ng.fig.set_size_inches(10,8)","1ec57ae0":"corr_matrix=df.corr()\n#creating a mask to block the upper triangle of coorelation matrix\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)]= True\n\n#plotting correlation\nf, ax = plt.subplots(figsize=(10, 10)) \nheatmap = sns.heatmap(corr_matrix, \n                      mask = mask,\n                      square = True,\n                      linewidths = .5,\n                      cmap = 'coolwarm',\n                      cbar_kws = {'shrink': .4, \n                                'ticks' : [-1, -.5, 0, 0.5, 1]},\n                      vmin = -1, \n                      vmax = 1,\n                      annot = True,\n                      annot_kws = {'size': 12})\n\n#add the column names as labels\nax.set_yticklabels(corr_matrix.columns, rotation = 0)\nax.set_xticklabels(corr_matrix.columns)\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True})","13541b4b":"sns.regplot(x='longitude',y='log_price',data=df, scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})","8a617bb2":"fig,ax=plt.subplots(2,figsize=(9,9))\n#Creating a histogram for availablity\nsns.distplot(df[\"availability_365\"], kde=False,ax=ax[0])\n\n#Creating a box plot for availablity\nsns.boxplot(\"availability_365\",data=df,ax=ax[1])","2ea692b4":"#first making the df['name'] series lower-case. Then, apply 'str.split' to split each sentence in each row by whitespace.\n#After that drop any NaN values in the series and then covert the series to a list.\n\nsplit_name_list=df['name'].str.lower().str.split().dropna().tolist()","9e2d54fd":"#see the first 10 elements in the list\nsplit_name_list[:10]","ffdd9388":"flatten_name_list = [val for sublist in split_name_list for val in sublist]","56f3a61d":"#show the first 10 elements in the flattened list\nflatten_name_list[:10]","269d1484":"#importing nltk library to use stopwords\nimport nltk\n#need to first download the stopwords using the follwoing link: nltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))","13a7e4b0":"stop_words","b1d3e24b":"filtered_sentence = [w for w in flatten_name_list if not w in stop_words]","f309ce70":"#showing the first 10 elements of the filtered element.\nfiltered_sentence[0:10]","556f0c27":"#import counter from collections package\nfrom collections import Counter\nc = Counter(filtered_sentence)\nmost_common_word=c.most_common(10)\nprint(dict(most_common_word))\nplt.figure(figsize=(10,10))\nplt.bar(dict(most_common_word).keys(), dict(most_common_word).values())","52afb919":"df.head()","1d645952":"#getting rid of unncessary columns from dataframe\n#we are going to use log_price instead of price\ndf=df.drop([\"price\",\"num_review_cat\"],axis=1)","b8e54aac":"#creating a new dataframe for the hypothesis check\nnew_df=df[['name','host_name','last_review','log_price']]\n\n#melting the dataframe to easily visualize the above mentioned features\n#variables: features, values: corresponding value for each feature. id_vars: log_price\nnew_df_melt=pd.melt(new_df,id_vars=['log_price'])\n\n#sampling 20% records from dataframe\nnew_dfSample = new_df_melt.sample(int(len(new_df)*0.2))","d1382846":"plt.figure(figsize=(20,10))\n#creating a scatterplot. Hue is for each variable (feature).\ng=sns.scatterplot(x=\"value\", y=\"log_price\", hue=\"variable\",data=new_dfSample)\n#not showing any xticklables because they are all different for each feature\ng.set(xticklabels=[])","c0d6d1d3":"df=df.drop([\"id\",\"host_id\",\"last_review\",\"host_name\",\"name\"],axis=1)","51286066":"df.head()","1740725f":"#separating labels and predictors\nX=df.drop('log_price',axis=1)\ny=df['log_price'].values\n\n#splitting train (75%) and test set (25%)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","3b257a63":"X_train.head()","94c7064e":"#Selecting numerical dataframe in train set\nX_train_num=X_train.select_dtypes(include=np.number)\n\n#Selecting categorical dataframe in train set\nX_train_cat=df.select_dtypes(exclude=['number'])","2abc101e":"X_train_num.head()","a8f4489c":"X_train_cat.head()","fb73a2d3":"#importing necessary modules from sklearn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\n#creating a pipeline for numerical attribute. Pipeline: median imputer + MinMaxScaler\nnum_pipeline=Pipeline([('imputer',SimpleImputer(strategy='median')),('mm_scaler',MinMaxScaler()),])","f31ea6db":"#importing label encoder\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\n\n#applying label encoding for categorical features in train set\nX_train_cat=X_train_cat.apply(LabelEncoder().fit_transform)","94728287":"X_train_cat.head()","9d3b02d5":"#importing ColumnTransformer and OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n#separating numerical and categorical attributes in the train set\nnum_attribs = list (X_train_num)\ncat_attribs = list (X_train_cat)\n\n#creating a full pipeline: numerical + categorical\nfull_pipeline = ColumnTransformer([(\"num\",num_pipeline,num_attribs),(\"cat\",OneHotEncoder(handle_unknown='ignore'),cat_attribs)])\n\n#fit and transform the train set using the full pipeline\nX_train_prepd = full_pipeline.fit_transform(X_train)","09c344a4":"X_train_prepd","c2aa84b7":"#importing linear regression model and mean_squared_error metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n#definiing linear regressor\nlin_reg=LinearRegression()\n\n#feeding in X_train and y_train for model fitting\nlin_reg.fit(X_train_prepd,y_train)\n\n#making predictions on train set\nlin_predictions = lin_reg.predict(X_train_prepd)\n\n#getting MSE and RMSE values\nlin_mse=mean_squared_error(y_train,lin_predictions)\nlin_rmse=np.sqrt(lin_mse)\n\nprint(\"Mean squared error: %.3f\" % lin_mse)\nprint(\"Root mean squared error: %.3f\" % lin_rmse)","82715e08":"#importing cross_val_score to perform k-fold validation\nfrom sklearn.model_selection import cross_val_score\n\n#performing 10-fold validation\nscores=cross_val_score(lin_reg, X_train_prepd, y_train, scoring = \"neg_mean_squared_error\",cv=10)\n\n# skleanr's cross validation expect a utility function, so the greater the better. \n# That is why putting a negative sign\nlin_rmse_scores=np.sqrt(-scores)\nprint(\"Scores:\", lin_rmse_scores)\nprint(\"Mean: %.3f\" % lin_rmse_scores.mean())\nprint(\"STD: %.3f\" % lin_rmse_scores.std())","b43645d9":"plt.scatter(y_train,lin_predictions, label='Predictions')\nplt.plot(y_train,y_train,'r',label='Perfect prediction line')\nplt.xlabel(\"Log_price in the train set\")\nplt.ylabel(\"Predicted log_price\")\nplt.legend()","83121dca":"#importing decision tree model\nfrom sklearn.tree import DecisionTreeRegressor\n\n#defining decision tree regressor\ntree_reg=DecisionTreeRegressor()\n#feeding X_train and y_train into the regressor\ntree_reg.fit(X_train_prepd,y_train)\n\n#making predictions on train set\ntree_predictions = tree_reg.predict(X_train_prepd)\n\n#getting MSE and RMSE values\ntree_mse=mean_squared_error(y_train,tree_predictions)\ntree_rmse=np.sqrt(tree_mse)\n\nprint(\"Mean squared error: %.3f\" % tree_mse)\nprint(\"Root mean squared error: %.3f\" % tree_rmse)","a0fc30a4":"#performing 10-fold validation\nscores=cross_val_score(tree_reg, X_train_prepd, y_train, scoring = \"neg_mean_squared_error\",cv=10)\n\n# skleanr's cross validation expect a utility function, so the greater the better. \n# That is why putting a negative sign\ntree_rmse_scores=np.sqrt(-scores)\nprint(\"Scores:\", tree_rmse_scores)\nprint(\"Mean: %.3f\" % tree_rmse_scores.mean())\nprint(\"STD: %.3f\" %  tree_rmse_scores.std())","02646317":"#adding max_depth of 15 limitation\ntree_reg=DecisionTreeRegressor(max_depth=15)\n#fitting the train set\ntree_reg.fit(X_train_prepd,y_train)\n\n#making predictions\ntree_predictions = tree_reg.predict(X_train_prepd)\n\n#getting MSE and RMSE values\ntree_mse=mean_squared_error(y_train,tree_predictions)\ntree_rmse=np.sqrt(tree_mse)\n\nprint(\"Mean squared error: %.3f\" % tree_mse)\nprint(\"Root mean squared error: %.3f\" % tree_rmse)","ca7632f6":"plt.scatter(y_train,tree_predictions, label='Predictions')\nplt.plot(y_train,y_train,'r',label='Perfect prediction line')\nplt.xlabel(\"Log_price in the train set\")\nplt.ylabel(\"Predicted log_price\")\nplt.legend()","38158069":"#setting environment path to correctly run graphviz in Jupyter notebook\nos.environ[\"PATH\"] += os.pathsep + 'C:\/Users\/jeong\/Anaconda3\/Library\/bin\/graphviz\/bin\/'","0d573fad":"dum_X_train=pd.get_dummies(X_train,columns=cat_attribs)\n#skipping the sclaing part for easier understanding.\ndum_X_train = dum_X_train.fillna(dum_X_train.median())","6ce4ac7d":"#limiting the maximum tree depth to 3\ntree_reg_vis=DecisionTreeRegressor(max_depth=3)\n#refitting the train set\ntree_reg_vis.fit(dum_X_train,y_train)","fd07f7b7":"from sklearn.tree import export_graphviz\nfrom sklearn import tree\nimport graphviz\n\n#export_graphviz(tree_reg, out_file = \"d1.dot\",impurity =False, filled=True)\noutfile = tree.export_graphviz(tree_reg_vis, out_file = 'tree.dot',feature_names=dum_X_train.columns.tolist())\n\nfrom subprocess import call","7968bf35":"#converting the dot file to png image file\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')","e7ebee25":"#importing random forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n#choose 30 number of trees\nforest_reg=RandomForestRegressor(n_estimators = 30, random_state = 42)\n\n#fitting the training set\nforest_reg.fit(X_train_prepd,y_train)\n\n#making predictions\nforest_predictions=forest_reg.predict(X_train_prepd)\n\n#getting MSE and RMSE values\nforest_mse=mean_squared_error(y_train,forest_predictions)\nforest_rmse=np.sqrt(forest_mse)\n\nprint(\"Mean squared error: %.3f\" % forest_mse)\nprint(\"Root mean squared error: %.3f\" % forest_rmse)","428b7fe6":"plt.scatter(y_train,forest_predictions, label='Predictions')\nplt.plot(y_train,y_train,'r',label='Perfect prediction line')\nplt.xlabel(\"Log_price in the train set\")\nplt.ylabel(\"Predicted log_price\")\nplt.legend()","1a3fde54":"#performing 5fold validation\nscores=cross_val_score(forest_reg, X_train_prepd, y_train, scoring = \"neg_mean_squared_error\",cv=5)\n\n# skleanr's cross validation expect a utility function, so the greater the better. \n# That is why putting a negative sign\nforest_rmse_scores=np.sqrt(-scores)\nprint(\"Scores:\", forest_rmse_scores)\nprint(\"Mean: %.3f\" % forest_rmse_scores.mean())\nprint(\"STD: %.3f\" % forest_rmse_scores.std())","572c9837":"#to show the pair of tuples in feature_importances\nfor feature in zip(X_train.columns, forest_reg.feature_importances_):\n    print(feature)\n\nfeats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X_train.columns, forest_reg.feature_importances_):\n    feats[feature] = importance #add the name\/value pair ","8c25636c":"#creating a dataframe from 'feats' dict. Setting dict keys as an index and renaming name of the importance value column\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Feature importance'})\n#sort the importance value by an descending order and create a bar plot\nimportances.sort_values(by='Feature importance',ascending=False).plot(kind='bar')","22a7e1a0":"#Do NOT fit the test data. Only transform using the pipeline\nX_test_prepd=full_pipeline.transform(X_test)","f78aea1b":"#Making predictions using random forest model trained above\nfinal_predictions = forest_reg.predict(X_test_prepd)\n\n#getting mse and rmse values\nfinal_mse = mean_squared_error(y_test,final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n\nprint(\"Mean squared error: %.3f\" % final_mse)\nprint(\"Root mean squared error: %.3f\" % final_rmse)","329f1689":"plt.scatter(y_test,final_predictions, label='Predictions')\nplt.plot(y_test,y_test,'r',label='Perfect prediction line')\nplt.xlabel(\"Log_price in the test set\")\nplt.ylabel(\"Predicted log_price\")\nplt.legend()","a21f9ab2":"#making predictions on the test set using the Decision tree model with pruning\nfinal_predictions_tree = tree_reg.predict(X_test_prepd)\n\n#getting mse and rmse values\nfinal_mse_tree = mean_squared_error(y_test,final_predictions_tree)\nfinal_rmse_tree = np.sqrt(final_mse_tree)\n\nprint(\"Mean squared error: %.3f\" % final_mse_tree)\nprint(\"Root mean squared error: %.3f\" % final_rmse_tree)","f437ea8f":"plt.scatter(y_test,final_predictions_tree, label='Predictions')\nplt.plot(y_test,y_test,'r',label='Perfect prediction line')\nplt.xlabel(\"Log_price in the test set\")\nplt.ylabel(\"Predicted log_price\")\nplt.legend()","0c79c508":"Wow! rmse of almost 0 seems too low. We are clearly overfitting.","d28a97e2":"It seems this produced a nested list. Let's flatten this. I used nested list comprehension for this.","df9eb0c4":"If we focus on the log_price row in the correlation map (last row), we can see that longitude has strong negative correlation. \"Price\" is essentialy the same as log_price so we can ignore it. Let's take a look at the Seaborn regression plot between the longigute and log_price.","32e85fa9":"Let's use cross-validation to check if we are overfitting or not. I will use 10 folds for the decision tree model and get the scores, mean of the scores and standard devaition of the scores.","67a71f01":"We can clearly see that the rmse value of training set is much smaller than that of validation set (mean: 0.61), which clearly shows significant overfitting with DecisionTree model. Let'as add some pruning and see how much we can get better.","2298006c":"## 2.2  Data visualization - Part 1","9db7eb8c":"Let's first try linear regression","f44d1a7a":"Now I want to take a look at how each feature affected random forest modeling, a.k.a feature importances.","45f6707f":"We can see that the names, host names and last_review do not really have a strong effect on prices. Let's drop these attributes altogether for easier machine learning model development.","f712ba0b":"We can see that we have some columns that we do not need anymore. For example, the \"num_review_cat\" was generated by us for easy plotting. Finally, since we decide to use log_price not price, I can simply drop the price column too.","b5efbe79":"## 4.3 Random Forest regression","714ad624":"Visualizing definitely helps understanding :)","38154683":"We can see that the difference between training set RMSE and validation set RMSE got much smaller than when we had used DecisionTree model WITHOUT pruning but still the training set RMSE is much smaller than the validation set RMSE (still overfitting). ","e16bc85e":"Let's generate descriptive statistics for our dataframe.","67e38a08":"## 4.1 Linear regression","70ecd5aa":"We can see that the model performed not as great as it did on the training set.","f32bec9a":"Let's use cross-validation to check.","3abe455b":"Let's actually look at the histogram of the prices to see how skewed our dataset is.","3c7b9544":"In this figure, the colors represent the neighborhoods and the marker sizes represent the price ranges. We can clearly differntiate the neighbourhoods but hard to see the price information. Let's focus on price information only and actually overlay the NYC map on top of this. \n\nFor the overlaying map, I downloaded the shapefiles from https:\/\/data.cityofnewyork.us\/City-Government\/Borough-Boundaries\/tqmj-j8zmand uploaded them to this direcotry. We will use Geopandas for plotting the shapefile.","c1eb6697":"We can see that the neighbourhood and neighbourhood_group were the two most important features in random forest modeling. The number_of_reivews feature was the 3rd highest although our violin plot in the EDA section for this feature did not reveal significant importance of it.","54a66caa":"In conclusion, the random forest model was better than liner regression and decision tree model in terms of predicting on both train and test sets. Decision tree model without pruining definitely overfitted too much. However even the random forest model could not make outstanding predictions across all datasets mainly because (1) the data was highly skewed so that we don't have enough low and high price AirBnB data and (2) hyperparameter tuning was not performed. At least visualization showed us that the neighbourhood was a importnat factor in setting a price. Other models such as support vector machine or neural networks could be worth trying. Hope you enjoyed my project.","1ee016aa":"Much better in terms of overfitting when we require the model to stop buildling the trees at the depth of 15. Let's take a look at how our predictions on the training set look like.","e07048c9":"What about the DecisionTreeModel WITH pruning? (original pruning of max depth = 15)","c8c68b37":"Ah, we are not crazily overfitting with the RandomForest model. The rmse is not also very bad. Let's compare the predictions and the labels in the train set.","a75eb819":"Definitely better than liner regression, but again not so great.","566f9fda":"Let's take a look at our dataframe one more time.","2dfbc776":"## 2.4  Data visualization - Part 2","ea364d17":"From this, we can see that the listings stayed available for booking for ~50 days (median), which means most of them did not remain available that long.","c6e2c9fa":"Ok now let's filter out these stop words from our flatten_name_list. I will use list comprehension again because we eventually need a list to feed into Counter class in Collection module.","41d5b59d":"Ok. Now let's look at some plots to visually understand our data. It seems we have latitude and longitude data, meaning we can generate a 'map' for the prices. I will use a scatter plot to map the prices.","8e7acf61":"Finally, I would like to see what typical words the hosts used when listing. As we saw above the listing names were something descriptive like \"Clean&quiet apt home by the park\". There might be something interesting to note.","560473ef":"Any other additional features to drop? Well the id and host_id are not really required to train our model as they are simply assigned to each property and host arbitrarily.\n\nAlso what about something like names (AirBnB listing names)? or the host names? First of all, listing_names should not signicantly affect prices as many people will use attractive words or descriptions. The host_name also should not matter. Also the date when the last_review was created should have a less effect on price thatn the number or the review scores. Let's check on my hypothesis by plotting price as a fucntion of these attributes from the orignal dataframe, df. \n\nBecause it takes quite a while to plot the all ~49,000 records in the original dataset, I will just choose 20% random samples for each category and plot.","b17fac0f":"The predictions are great for the middle prices but started getting worse for the high price data. This might have happened because we had a very skewed dataset to begin with. It seems the skewedness in the dataset even after logarithmic normalization still hurts us.","c21f494e":"As we expected because we were overfitting our training dataset with the RandomForest model, the predictions on the test dataset are not great and the rmse got much larger. Let's compare the predictions and test set labels.","1b7cc636":"Let's try DecisionTreeRegressor next. I am not going to use any pruning first.","bc7537e6":"Let's take a look at the correlation plot to see how numerical features are related to our target, the price. I wil ignore the cateogrical attributes for now.","a1fe5925":"Now we can clearly see that the most of expensive ones are in the Manhattan island, which is not very surprising.","0b3dcd4d":"Now we can see the categorical features have been succesfully converted to numerical features. However, these numbers do not really hold 'correct' meaning. For example. the value of 1 (private room) in the room_type feature does not really mean a private room is greater in magintude or degree than 0 (Entire home). For this reason, we need to apply OneHotEncoding to denote that they numerically belong to different classes but without any order or magnitude. I could apply OneHotEncoding to the LabelEncoded features above, but I will rely on sklearn's ColumnTransformer to take care of both numerical and categorical features at the same time through pipelines. Moreover, from scikit-learn 0.20, OneHotEncoder accepts strings, so we don't really need a LabelEncoder before it anymore.","6d217b14":"Let's finally try RandomForestRegressor. I chose hyperparameters myself to be simple.","3bee36fd":"This is not super bad given that our log_price mean was ~\\\\$5.","409b3a16":"# 2. EDA","65a8e2c5":"## 4.2 Decision Tree Regression","36370088":"We can see from the plot above that generally the number of reviews do not affect the prices in all neighbourhoods.","fa192674":"In this project, I am going to explore the NYC AirBnb dataset through visualization and try to engineer multiple features to optimze them for our machine learning models so that the price of the AirBnb properties can be predicted quite accurately.","5af117e2":"Ok now we are good to go to train our machine learning models.\n","00a5d2be":"# 3. Feature engineering","aa76606e":"Now let's separate numerical and categorical attributes to go through different transormation pipelines.","f42cbedf":"Ah I can see the NaN values in some columns already. Let's take a look at the dataframe info.","699e86f4":"## 2.1 Data loading and understanding","3b09ac86":"Let's foucs on the price information as that is going to be our target. I did not know that there are some $0 AirBnB (look at the min of price column). Also there is a \\\\$10,000 AirBnB! Something to note is the mean of the price, \\\\$152.","2a60251b":"Let's quickly look at the data.","a3569566":"We can try keep tuning the hyperparameters to make this better but let's move on for now and evaluate our random forest system on the test set.","1cfbb6a2":"Ok. Now I want to remove some \"stopwords\" such as articles (a, the) or connecting words (and, but). The nltk library already has common stopwords defined.","136539fd":"## 1.1 Data source\nhttps:\/\/www.kaggle.com\/dgomonov\/new-york-city-airbnb-open-data\n\n### Context\nSince 2008, guests and hosts have used Airbnb to expand on traveling possibilities and present more unique, personalized way of experiencing the world. This dataset describes the listing activity and metrics in NYC, NY for 2019.\n\n### Content\nThis data file includes all needed information to find out more about hosts, geographical availability, necessary metrics to make predictions and draw conclusions.\n\n### Acknowledgements\nThis public dataset is part of Airbnb, and the original source can be found on this website: http:\/\/insideairbnb.com\/\n\n## 1.2 Data definitions\nid: listing ID  \nname: name of the listing  \nhost_id: host ID  \nhost_name: name of the host  \nneighbourhood_group: location  \nneighbourhood: area  \nlatitude: latitude coordinates  \nlongitude: longitude coordinates  \nroom_type: listing space type  \nprice: price in dollars  \nminimum_nights: amount of nights minimum  \nnumber_of_reviews: number of reviews  \nlast_review: latest review  \nreviews_per_month: number of reviews per month  \ncalculated_host_listings_count : amount of listing per host  \navailability_365: number of days when listing is available for booking  ","26ac9280":"I will limit the depth of the tree to be 3 at maximum to properly visualize decision tree regression process.\nThis visualization relies on external graphviz module but to preoprly display the feature names we need to feed in the dataframes, not arrays. Thus, I will re-creaete one-hot encoded train set dataframe using Pandas get_dummies.","dc196d0e":"# 1. Introduction","36b7c501":"As we saw in the map plot above, the Manhattan AirBnb prices were higher than the rest of neighborhood's for all types of rooms. What about # of reviews? Would more reviews mean more 'luxurious' ones?","7feb3b56":"We also have 3 categorical features remaining in our dataframe. For ML regression modeling, we should convert them to numercal values. I can use sklearn's LabelEncoder for this.","e402cd49":"### 2.3 Data normalization - log transformation","0be4112c":"We can further visualize this by creating Seaborn boxplots for the neighbourhood_group feature.","07673134":"Let's take a look at our dataframe to see our work until now.","096a3127":"Not surprisingly, people used 'room' the most in their listings. Something like private or cozy are also noteworthy. Alright, let's move on to fetaure engineering in preparation of our price prediction.","6ec1f886":"Let's replot the histogram after this logarithmic transfomration.","084f015e":"Let's get the cross validation score on the random forest model to see if we are overfitting again. This time cross validation happeend across 5 folds to reduce run time.","c014033a":"# 5. Conclusion","27c029e0":"Hmm. It is still worse than the random forest model.","dcaf4bba":"Let's transform the test set. One thing to be careful is that we should NOT fit_transform the test set because the whole point of ML is to train the model so that it can make good predictions on test set. We don't want the model to cheat by optimizing the model to the test set.","785e31ea":"Ok. Before we jump into feature engineering before modeling, I would also like to see how long the AirBnB listing typically had lasted.","a6c94d4b":"The 3rd row of reviews_per_month column is showing a null value, which reminds me again that we need to take care of these missing values for proper modeling. But I prefer to take care of it through sklearn's pipelines after we split train and test datasets. Let's split now before I forget.","a0a50b51":"Rather than actually plotting for each number of reviews, I will categorize them into 5 bins using panda's \"cut\", so that we can quickly visualize this information with Seaborn's catplot.","d36af6fd":"### 4.3.1 Evaluate on the test set","6e22ee0a":"Ok. Let's use Counter to count the most 10 common words.","bdba2ab2":"The mean RMSE in the validation set is about the same as training set, which means we are not overly overfitting. Let's plot the predictions against the actual labels in the training set.","80d45ebc":"From this we can see that we have 48,895 entries and 16 columns but some columns (name, host_name, etc) have less non-null values which means there are NaN values.","8e59452f":"Now I will create pipelines for numerical features basically to fill in the missng values (imputing) and scale the numerical features. For this, I am going to first impute the missing values with a median of each column using sklearn's SimpleImputer and then use MinMaxScaler for feature scaling","dae7bcc8":"Let's explore our data! First up, data loading","6d2e55ca":"This is a cool image but we can see that most of makrers are shown in blue because the outliers (pricey AirBnB's) overwhelemed the rest of data.","b8dc65bf":"Not really looking great. We were doing ok with the middle price datasets but not great with the low and high datasets.\nLet's skip on modeling the test set with this model as training set itself looks not ideal.","c0060cd0":"Now it looks much more like our typical normal distribution. :) \n\nHowever, we have to be careful with logarihmic transformation. Leigh Metcalf et al. said in their \"Introduction to data analysis\" book published in 2016 that \"a process that de-emphasizes them (outliers)is not useful\" if we want to analyze the outliers and \"if the distance between each variable is important, then taking the log of the variable skews the distance\". \n\nFortunately, these do not apply to this project so let's replot the price map one more time with log_price.","9ff1b0dd":"# 4. Modeling","ce9c226d":"We can see that our data is significantly skewed towards the left. This is why we did not see any details in the price maps above.\nLet's correct this by normalizing our dataset. I am going to apply a log scale for normalization.\n\nOne thing we need to note though is that since there are some \\\\$0 price tags in our dataframe, which will cause a problem because log(0) cannot be defined.\n\nTo resolve this issue while preserving the dataframe's feature as much as possible, we can add a very small constant to the price column. I will add \\\\$1 to the price in the dataframe and then apply logarthmic function. This actually does not modify the nature of price dataset too much as log(1) = 0."}}