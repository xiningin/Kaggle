{"cell_type":{"f4bc5d69":"code","9318bf74":"code","05ac5207":"code","a22f4e97":"code","fe74cfbf":"code","52599b70":"code","97b4cc55":"code","f72b6308":"code","b904b38d":"code","ec4a64c5":"code","403781e5":"code","f9ec5d1e":"code","54d7d850":"code","170aad55":"code","26887d39":"code","99864815":"code","390ee42c":"code","66a1552c":"code","75d7bd5f":"code","826e1e44":"code","a2f510cf":"markdown","0cf1c706":"markdown","96fb3c6e":"markdown","6e32466f":"markdown"},"source":{"f4bc5d69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9318bf74":"!pip install --upgrade pip\n!pip install --upgrade allennlp\n!pip install transformers==4.0.1","05ac5207":"# for TPU\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","a22f4e97":"import transformers\nimport pandas as pd\nimport torch","fe74cfbf":"# for TPU\nimport torch_xla\nimport torch_xla.core.xla_model as xm","52599b70":"print('Transformers version: ', transformers.__version__)\nprint('Pytorch version: ', torch.__version__)","97b4cc55":"data_dir = '\/kaggle\/input\/contradictory-my-dear-watson\/'\ntrain_df = pd.read_csv(data_dir+'train.csv').sample(frac=1, random_state=100)\ntest_df = pd.read_csv(data_dir+'test.csv')\nprint(train_df['label'].value_counts())\ntrain_df.head(5)","f72b6308":"#PRE_TRAINED_MODEL = 'bert-base-multilingual-cased'\nPRE_TRAINED_MODEL = 'xlm-roberta-large'\ntokenizer = transformers.AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL)","b904b38d":"MAX_LEN = 84\nsplit_idx = int(train_df.shape[0] * 0.8)","ec4a64c5":"def get_encode (data):\n    tokenized_data=tokenizer(\n        text=list(data['premise']), text_pair=list(data['hypothesis']),\n                                            max_length=MAX_LEN,\n                                            pad_to_max_length=True,\n                                            add_special_tokens=True,\n                                            truncation=True, \n                                            return_attention_mask=True, \n                                            return_token_type_ids=True,\n                                             return_tensors='pt')\n    return tokenized_data\n    ","403781e5":"#Data Preprocessing and tensor generation\nseed=2\n\ntokenized_train=get_encode(train_df[:split_idx])\nlabels_train=torch.tensor(train_df.label.values[:split_idx])\n\ntokenized_valid=get_encode(train_df[split_idx:])\nlabels_valid=torch.tensor(train_df.label.values[split_idx:])\n\ntokenized_test=get_encode(test_df)","f9ec5d1e":"print (tokenized_train['input_ids'][0])\nprint (tokenized_train['token_type_ids'][0])\nprint (tokenized_train['attention_mask'][0])","54d7d850":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nbatch_size=64\n\ntrain_data=TensorDataset(torch.tensor(tokenized_train['input_ids']),\n                         torch.tensor(tokenized_train['token_type_ids']),torch.tensor(tokenized_train['attention_mask'])\n                         ,labels_train)\ntrain_sampler=RandomSampler(train_data)\ntrain_dataloader=DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalid_data=TensorDataset(torch.tensor(tokenized_valid['input_ids']),\n                         torch.tensor(tokenized_valid['token_type_ids']),\n                         torch.tensor(tokenized_valid['attention_mask'])\n                         ,labels_valid)\nvalid_sampler=SequentialSampler(valid_data)\nvalid_dataloader=DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n\ntest_data=TensorDataset(torch.tensor(tokenized_test['input_ids']),\n                        torch.tensor(tokenized_test['token_type_ids']),\n                        torch.tensor(tokenized_test['attention_mask']))\ntest_dataloader=DataLoader(test_data, batch_size=batch_size)","170aad55":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL,\n                                                      num_labels = 3,\n                                                      output_attentions = False,\n                                                      output_hidden_states = False)\n","26887d39":"# for GPU \/ CPU\n'''\nif torch.cuda.is_available():\n    print(model.cuda())\nelse :\n    print(model.cpu())\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print ('%d GPU(s) available' % torch.cuda.device_count())\nelse:\n    device = torch.device(\"cpu\")\n    print ('No GPU avaailable, using CPU.')\n'''\n    \n# for TPU\ndevice = xm.xla_device()\ntorch.set_default_tensor_type('torch.FloatTensor')\nprint(model.to(device))\nprint ('TPU available')\n","99864815":"def accuracy(predictions, labels):\n    prediction_flat = np.argmax(predictions, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(prediction_flat == labels_flat) \/ len(labels_flat)","390ee42c":"from transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5\n                 )\nepochs = 20\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)","66a1552c":"import datetime\nimport random\n\nrandom.seed(10)\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed_all(10)\n\nlosses = []\n\nfor i in range(0, epochs):\n    print ('Epoch {:} of {:} Training...'.format(i+1, epochs))\n    \n    total_loss = 0\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        if step % 10 == 0 and step != 0:\n            print ('[{}] Batch {:>5,} of {:>5,}'\n               .format(datetime.datetime.now().strftime('%H:%M:%S'), step, len(train_dataloader)))\n        train_batch_input = batch[0].to(device)\n        train_batch_input_types = batch[1].to(device)\n        train_batch_mask = batch[2].to(device)\n        train_batch_label = batch[3].to(device)\n        \n        model.zero_grad()\n        outputs = model(train_batch_input, token_type_ids = train_batch_input_types, \n                        attention_mask = train_batch_mask, labels = train_batch_label)\n        loss = outputs[0]        \n        \n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # for GPU \/ CPU\n        #optimizer.step()\n        # for TPU\n        xm.optimizer_step(optimizer, barrier=True)\n        scheduler.step()\n    \n    average_train_loss = total_loss \/ len(train_dataloader)\n    losses.append(average_train_loss)\n    print ('Training loss={:.2f}'.format(average_train_loss))\n    \n    model.eval()\n    eval_accuracy = 0\n    eval_count = 0\n    \n    for batch in valid_dataloader:\n        valid_batch_input = batch[0].to(device)\n        valid_batch_input_types = batch[1].to(device)\n        valid_batch_mask = batch[2].to(device)\n        valid_batch_labels = batch[3].to(device)\n        \n        with torch.no_grad():\n            outputs = model(valid_batch_input, token_type_ids = valid_batch_input_types,\n                           attention_mask = valid_batch_mask)\n        \n        logits = outputs[0]\n        logits = logits.detach().cpu().numpy()\n        label_ids = valid_batch_labels.to('cpu').numpy()\n        batch_accuracy = accuracy(logits, label_ids)\n        eval_accuracy += batch_accuracy\n        eval_count += 1\n\n    print ('Validation accuracy={:.2f}'.format(eval_accuracy \/ eval_count) )\n    print ('')\n    \n    if i == 4:\n        break\n\nprint (\"Training done\")\n    ","75d7bd5f":"model.eval()\nsubmissions = []\nfor batch in test_dataloader:\n    test_batch_input = batch[0].to(device)\n    test_batch_input_types = batch[1].to(device)\n    test_batch_mask = batch[2].to(device)\n    \n    with torch.no_grad():\n        outputs = model(test_batch_input, token_type_ids = test_batch_input_types,\n                       attention_mask = test_batch_mask)\n        \n    logits = outputs[0]\n    submissions.extend(np.argmax(logits.detach().cpu().numpy(), axis=1).flatten())\n","826e1e44":"output = pd.DataFrame({'id': test_df.id,\n                       'prediction': submissions})\noutput.to_csv('submission.csv', index=False)","a2f510cf":"Make submission","0cf1c706":"# Import Data","96fb3c6e":"# Tokenization & Make input","6e32466f":"# Model Fine Tuning"}}