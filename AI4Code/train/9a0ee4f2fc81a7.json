{"cell_type":{"c1041f7f":"code","9a41d55d":"code","d264c86e":"code","a85bc49f":"code","9c805f9c":"code","f46743d2":"code","8a5e774d":"code","a9edd253":"code","0d218a48":"code","ad514eb1":"code","ba1d6fbb":"code","31bdb21d":"code","38f2bf07":"code","af0e97e3":"code","6df2c469":"code","4f97f7cf":"code","b605ae3c":"code","557153e7":"code","3b198250":"code","84d3c217":"code","b444ff42":"code","e18d0f08":"code","acbb3e7a":"code","b60e205f":"code","727c341a":"code","099cc6eb":"code","4ee09006":"code","d0dcd624":"code","89393b41":"code","ff51bcf5":"code","5a98300c":"markdown"},"source":{"c1041f7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a41d55d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","d264c86e":"#----------------Read_data----------------\ntrain_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\") ","a85bc49f":"train_text_len = [len(row.split())for row in train_data['text']]\ntest_text_len = [len(row.split())for row in test_data['text']]","9c805f9c":"train_text_len = np.array(train_text_len)\/max(train_text_len)\ntest_text_len = np.array(test_text_len)\/max(test_text_len)","f46743d2":"print(\"Train_data_dim:{}\".format(train_data.shape))\nprint(\"Test data_dim:{}\".format(test_data.shape))","8a5e774d":"#--------Sample_train_data---------------\ntrain_data.head()","a9edd253":"#-----------Sample_test_data------------------\ntest_data.head()","0d218a48":"#-------------Check_class_imbalance-------------------\nsns.countplot(x ='target',data = train_data)","ad514eb1":"target = train_data[\"target\"]\ntrain_data = train_data['text']","ba1d6fbb":"train_data","31bdb21d":"import nltk\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nimport string","38f2bf07":"def text_preprocessor(text):\n    #-------------Remove_stop_words----------------\n    stop_free_text = \" \".join([word.lower() for word in word_tokenize(text) if word not in stop_words])\n    #------------Remove_numeric_values--------------\n    digit_free_text = res = \"\".join(filter(lambda x: not x.isdigit(), stop_free_text))\n    #-------------Remove_punctuations---------------\n    punct_free_text = \" \".join([word for word in word_tokenize(digit_free_text) if word not in list(string.punctuation)])\n    #-------------Lemmatize_text--------------------\n    lemmatised_text = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(punct_free_text)])\n    return lemmatised_text\n                               ","af0e97e3":"train_df = pd.DataFrame([])\nprocessed_text = []\nfor text_data in train_data:\n    processed_text.append(text_preprocessor(text_data))\n    \ntrain_df['text'] = processed_text\ntrain_df['text_len'] = train_text_len","6df2c469":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(train_df,target,test_size=0.20,random_state=42)\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","4f97f7cf":"from sklearn.feature_extraction.text import TfidfVectorizer","b605ae3c":"vectorizer = TfidfVectorizer(max_features=500,min_df=1)\ntrain_X = vectorizer.fit_transform(X_train['text'].tolist())\ntest_X = vectorizer.transform(X_test['text'].tolist())","557153e7":"train_X = pd.DataFrame(train_X.toarray())\ntest_X = pd.DataFrame(test_X.toarray())\ntrain_X['text_len'] = list(X_train['text_len'])\ntest_X['text_len'] = list(X_test['text_len'])","3b198250":"print(vectorizer.get_feature_names())","84d3c217":"#--------------Train_ML_model---------------------\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = MultinomialNB()\nmodel = RandomForestClassifier(random_state = 42)\nmodel.fit(train_X,y_train)\nmodel","b444ff42":"#-----------Generate_predictions---------------\npredictions = model.predict(test_X)","e18d0f08":"from sklearn.metrics import classification_report,accuracy_score\nprint(\"Accuracy:{}\".format(accuracy_score(y_test,predictions)))\nprint(classification_report(y_test,predictions))","acbb3e7a":"#--------Predict_classes_for_test_dataset--------------\ntest_text = test_data['text']","b60e205f":"test_df = pd.DataFrame([])\nprocessed_text = []\nfor text_data in test_text:\n    processed_text.append(text_preprocessor(text_data))\n    \ntest_df['text'] = processed_text","727c341a":"test_data = vectorizer.transform(test_df['text'].tolist())\ntest_data = pd.DataFrame(test_data.toarray())\ntest_data['test_len'] = test_text_len","099cc6eb":"#-------------generate_test_predictions-----------------\ntest_preds = model.predict(test_data)","4ee09006":"temp = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","d0dcd624":"temp['target'] = list(test_preds)","89393b41":"temp.columns","ff51bcf5":"temp.to_csv('Submission_file_v7.csv',index=False)","5a98300c":"The above graph shows that, the distribution of comments across each class is balanced.\nImbalance handling process is not required."}}