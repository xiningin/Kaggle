{"cell_type":{"3df7bdc6":"code","4b790f65":"code","99359f22":"code","fd1fb151":"code","c62897d7":"code","3a3a6596":"code","d6002a06":"code","70ea385a":"code","b81d178d":"code","4d1f2665":"code","3e579db7":"code","b2c59bf9":"code","e8fa0941":"code","7e42aaf9":"code","faee207d":"code","43a05c2f":"code","cd89748a":"code","84e37476":"code","4f323b37":"code","5b9c1496":"code","d9904137":"code","6c458fe6":"code","37d0d18b":"code","8a661eef":"code","067c7437":"code","0b0de2e2":"code","d34c4d68":"code","46fca9ff":"code","eda26ba1":"code","65e15f8f":"code","068aa642":"code","fb127ea1":"code","350cafbc":"code","852c64ef":"code","4f7a2ca9":"code","15e361fe":"code","02b95ed1":"code","1eb02d0a":"code","2e504e0c":"code","c0202d52":"code","c2d0f234":"code","6a236b33":"code","69cd21e0":"code","b397ab22":"code","c661674d":"markdown","567a8df7":"markdown","a7accb8b":"markdown","49bd5b77":"markdown","4d4817ed":"markdown","cad4afe3":"markdown"},"source":{"3df7bdc6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\n\n\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b790f65":"# loading the dataset\ndataset = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","99359f22":"dataset.head()","fd1fb151":"# analyzing basic information from dataset\ndataset.info()","c62897d7":"## Analyzing the statistical information from dataset\ndataset.describe(include='all')","3a3a6596":"# Finding the missing values\ndataset.isnull().sum()","d6002a06":"# we find that 'bmi' has null values we will replace the null values with mean()\ndataset['bmi'].fillna(dataset['bmi'].mean(),inplace=True)","70ea385a":"# checking weather we succesfully replaced the null values\ndataset.isnull().sum()","b81d178d":"# Lets drop the 'id' column which will not make much sence in the analyizing the dataset\ndataset.drop('id',axis=1,inplace=True)\n","4d1f2665":"dataset.head()","3e579db7":"df_value_stroke = dataset['stroke'].value_counts()\nprint('Non Stroke:{}'.format(df_value_stroke[0]))\nprint('Stroke:{}'.format(df_value_stroke[1]))\ndataset['stroke'].value_counts().plot.bar()\nplt.title('Stroke Analysis')\nplt.show()","b2c59bf9":"# Numerical Features\ndataset_numerical = [feature for feature in dataset.columns if dataset[feature].dtype != 'O']\ndataset_numerical","e8fa0941":"# Categorical Features\ndataset_categorical = [feature for feature in dataset.columns if dataset[feature].dtype == 'O']\ndataset_categorical","7e42aaf9":"# Lets find the relationship between categorical_features and Stroke\nfor feature in dataset_categorical:\n    data = dataset.copy()\n    sns.countplot(x = data[feature], hue = data[\"stroke\"])\n    plt.xlabel(feature)\n    plt.title(feature)\n    plt.show()","faee207d":"#Lets Check weather data is normally distributed or not\n\nfor feature in dataset_numerical:\n    data = dataset.copy()\n    sns.distplot(x=data[feature])\n    plt.xlabel(feature)\n    plt.title(feature)\n    plt.show()\n","43a05c2f":"for feature in dataset_categorical:\n    data = dataset.copy()\n    sns.histplot(x=data[feature],bins=50)\n    plt.xlabel(feature)\n    plt.title(feature)\n    plt.show()","cd89748a":"# Finding the outliers with the boxplot\nfor feature in dataset_numerical:\n    data = dataset.copy()\n    data.boxplot(column=feature)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()","84e37476":"# Lets find  all the unique values in dataset\nfor feature in dataset.columns[:]:\n    print(feature,':',len(dataset[feature].unique()))","4f323b37":"# One Hot Encoding\ndata = pd.get_dummies(dataset, columns = [\"gender\", \"work_type\"], drop_first = True)","5b9c1496":"data","d9904137":"# Replacing Categorical Features with Numerical Values\never_married_map = data['ever_married'].value_counts().to_dict()\never_married_map","6c458fe6":"ordinal_label = {k:i for i,k in enumerate(ever_married_map,0)}\ndata['ever_married'] = data['ever_married'].map(ordinal_label)","37d0d18b":"Residence_type_map = data['Residence_type'].value_counts().to_dict()\nResidence_type_map","8a661eef":"ordinal_label_1 = {k:i for i,k in enumerate(Residence_type_map,0)}\ndata['Residence_type'] = data['Residence_type'].map(ordinal_label_1)","067c7437":"ord_encoder = OrdinalEncoder()\ndata[\"smoking_status\"] = ord_encoder.fit_transform(data[\"smoking_status\"].values.reshape(-1, 1))\ndata.smoking_status.value_counts()","0b0de2e2":"data.head()","d34c4d68":"data.info()","46fca9ff":"# Checking weather data is normally distributed or not\nfor feature in data.columns[:]:\n    sns.distplot(data[feature])\n    plt.title(feature)\n    plt.show()","eda26ba1":"fig = data.boxplot(column='avg_glucose_level')","65e15f8f":"data['avg_glucose_level'].describe()","068aa642":"## lets compute the interquantile range to calculate the boundaries\nIQR = data.avg_glucose_level.quantile(0.75) - data.avg_glucose_level.quantile(0.25)\n# extreme outlires\n\nlower_bridge = data['avg_glucose_level'].quantile(0.25) - (IQR*3)\nupper_bridge = data['avg_glucose_level'].quantile(0.75) + (IQR*3)\n\nprint(lower_bridge),print(upper_bridge)","fb127ea1":"data.loc[data['avg_glucose_level']>165,'avg_glucose_level']=165","350cafbc":"data.boxplot(column='avg_glucose_level')","852c64ef":"## lets compute the interquantile range to calculate the boundaries\nIQR = data.bmi.quantile(0.75) - data.bmi.quantile(0.25)\n# extreme outlires\nlower_bridge = data['bmi'].quantile(0.25) - (IQR*3)\nupper_bridge = data['bmi'].quantile(0.75) + (IQR*3)\nprint(lower_bridge),print(upper_bridge)","4f7a2ca9":"data.boxplot(column='bmi')","15e361fe":"data.loc[data['bmi']>45,'bmi']=45","02b95ed1":"data.boxplot(column='bmi')","1eb02d0a":"figure = data.bmi.hist(bins=50)\nfigure.set_title('bmi')\nfigure.set_xlabel('bmi')\n","2e504e0c":"\nfigure = data.avg_glucose_level.hist(bins=50)\nfigure.set_title('avg_glucose_level')\nfigure.set_xlabel('avg_glucose_level')\n","c0202d52":"X = data.drop(columns = [\"stroke\"])\ny = data[\"stroke\"]","c2d0f234":"# Spliting the data into train and test to avoid the data leakage\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 24)\nprint(f\"Train Data: {X_train.shape}, {y_train.shape}\")\nprint(f\"Test Data: {X_test.shape}, {y_test.shape}\")","6a236b33":"std_scaler  = StandardScaler()\nX_train = std_scaler.fit_transform(X_train)\nX_test = std_scaler.transform(X_test)","69cd21e0":"# using Logistic Regression \nfrom sklearn.linear_model import LogisticRegression\nclassifer = LogisticRegression()\nclassifer.fit(X_train,y_train)\ny_pred = classifer.predict(X_test)\n","b397ab22":"# checking accuracy of the model\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nprint(\"Accuaracy-score:{}\".format(accuracy_score(y_test,y_pred)))\nprint((confusion_matrix(y_test,y_pred)))","c661674d":"# Removing Outliers","567a8df7":"# # Univariate Analysis","a7accb8b":"# Scaling the Data","49bd5b77":"# # EDA","4d4817ed":"# Feature Engineering","cad4afe3":"# Outliers "}}