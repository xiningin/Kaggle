{"cell_type":{"2e2bbf28":"code","c5cf609d":"code","128b5f57":"code","3f1f1906":"code","aaa9e918":"code","4580404b":"code","8f581d46":"code","4431a7f8":"code","2b7ae271":"code","01c5512e":"code","3b96495e":"code","5652d8ea":"code","817f1572":"code","70a4b7c8":"code","c79eb56c":"code","f67414f0":"code","86d6e1a4":"code","13222284":"code","27aec832":"code","1a3ba572":"code","1e17e853":"code","2bafdff2":"code","db6d674a":"code","52491e18":"code","4cb6e325":"code","b082c4c1":"code","c05d38a0":"code","67d69d92":"code","99330ef8":"code","21179dd9":"code","50a30ed7":"code","c4caa048":"code","b165b0b3":"code","1d81cf8c":"code","9c47de87":"code","b4bfcfcb":"code","64bdf6ce":"code","7ef89046":"code","f38f3854":"code","e2fed498":"code","dc354d09":"code","8b5ee98b":"code","378f1bed":"code","7cb925fb":"code","dfaf2c71":"code","945504e7":"code","828b5d39":"code","0a957be4":"code","ab71b400":"code","22bf5fb6":"code","0a507506":"code","9c635a4b":"code","b5374454":"code","dcb0ac81":"code","a6f47430":"code","8ac7f3f3":"code","968d842d":"code","4665c6c0":"code","87856826":"code","80f58571":"code","2db17f7d":"code","66d8e4c1":"code","473d0cbe":"code","90d21bd4":"markdown","6eafffaa":"markdown","fb4c967e":"markdown","479e16b6":"markdown","1bd60ab5":"markdown","8bc8e0bc":"markdown","ccf6a0b3":"markdown","0799c7f5":"markdown","0de8d793":"markdown","2ea1aeb4":"markdown","0777192a":"markdown","05c1cbbf":"markdown","109d5412":"markdown","efe9ad64":"markdown","aa1fdb36":"markdown","8ecc3731":"markdown","9612f772":"markdown","01a60f3a":"markdown","aedc37fc":"markdown","e956f6fe":"markdown","e8f1ccdf":"markdown","81b99dcd":"markdown","92cbe80a":"markdown","8674b544":"markdown","11cd6b89":"markdown","9298895b":"markdown","8e8de17f":"markdown","fd0e6b85":"markdown","e9580052":"markdown","0dd8394e":"markdown","767fa56b":"markdown","dec32919":"markdown","2ca445d2":"markdown","ebb2bb83":"markdown","ea679d72":"markdown","832fc447":"markdown","82281c7d":"markdown","7b88e6a5":"markdown","d4188f08":"markdown","2466832d":"markdown","3d8b96d1":"markdown","0502415e":"markdown","fdca22e5":"markdown","4ea0e7e5":"markdown","c8d2b0ec":"markdown","0e4d0107":"markdown","6a00c953":"markdown","49ec3c61":"markdown","e472a711":"markdown"},"source":{"2e2bbf28":"# Importing neccesary packages.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n#\n\nfrom scipy import interp\nimport math\nfrom scipy.stats import norm\nfrom scipy import stats\n\n#\n\nimport warnings\nwarnings.filterwarnings('ignore') # Disabling warnings for clearer outputs.\npd.options.display.max_columns = 50 # Pandas option to increase max number of columns to display.\nplt.style.use('ggplot') # Setting default plot style.\n","c5cf609d":"# Read train and test data from csv files for visualization:\n\nv_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nv_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\nidx = len(v_train)","128b5f57":"# Checking train and test sets\ndisplay(v_train.sample(3))\ndisplay(v_test.sample(3))","3f1f1906":"# Merging visualization datasets.\n\nv_train.drop('PassengerId', axis=1, inplace=True)\nv_test.drop('PassengerId', axis=1, inplace=True)\nv_merged = pd.concat([v_train, v_test], sort=False).reset_index(drop=True)","aaa9e918":"# Checking merged shape:\n\ndisplay(v_merged.shape)","4580404b":"# Checking features and target columns:\n\ndisplay(v_merged.columns)","8f581d46":"# Checking dtypes:\n\ndisplay(v_merged.info())","4431a7f8":"# Selecting categorical data for univariate analysis:\n\ncats = ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n\n\ndef plotFrequency(cats):\n    '''A plot for visualize categorical data, showing both absolute and relative frequencies'''\n    \n    fig, axes = plt.subplots(math.ceil(len(cats) \/ 3), 3, figsize=(20, 12))\n    axes = axes.flatten()\n\n    for ax, cat in zip(axes, cats):\n        if cat == 'Survived':\n            total = float(len(v_train[cat]))\n        else:\n            total = float(len(v_merged[cat]))\n        sns.countplot(v_merged[cat], palette='plasma', ax=ax)\n\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x() + p.get_width() \/ 2.,\n                    height + 10,\n                    '{:1.2f}%'.format((height \/ total) * 100),\n                    ha=\"center\")\n\n        plt.ylabel('Count', fontsize=15, weight='bold')\n","2b7ae271":"plotFrequency(cats)","01c5512e":"def plotsurvival(cats, data):\n    \n    '''A plot for bivariate analysis.'''\n    \n    fig, axes = plt.subplots(math.ceil(len(cats) \/ 3), 3, figsize=(20, 12))\n    axes = axes.flatten()\n\n    for ax, cat in zip(axes, cats):\n        if cat == 'Survived':\n            sns.countplot(v_train[cat], palette='plasma', ax=ax)\n\n        else:\n\n            sns.countplot(x=cat,\n                          data=data,\n                          hue='Survived',\n                          palette='plasma',\n                          ax=ax)\n            ax.legend(title='Survived?',\n                      loc='upper right',\n                      labels=['No', 'Yes'])\n\n        plt.ylabel('Count', fontsize=15, weight='bold')\n","3b96495e":"plotsurvival(cats, v_train)","5652d8ea":"def plot_3chart(df, feature):\n    import matplotlib.gridspec as gridspec\n    from matplotlib.ticker import MaxNLocator\n    \n    # Creating a customized chart. and giving in figsize and everything.\n    \n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n    \n    # Creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    \n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # Plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.legend(labels=['Normal', 'Actual'])\n\n    # Customizing the QQ_plot.\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    \n    stats.probplot(df.loc[:, feature].fillna(np.mean(df.loc[:, feature])),\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n\n    # Customizing the Box Plot.\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    \n    # Set title.\n    \n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(df.loc[:, feature], orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{feature}', fontsize=24)","817f1572":"plot_3chart(v_train, 'Age')\nplot_3chart(v_train, 'Fare')","70a4b7c8":"# Listing most related continious values to target.\n\ntrain_corr = v_train[['Survived', 'Age', 'Fare'\n                      ]].corr(method='spearman').abs().unstack().sort_values(\n                          kind='quicksort', ascending=False).reset_index()\ntrain_corr.rename(columns={\n    'level_0': 'Feature A',\n    'level_1': 'Feature B',\n    0: 'Correlation Coefficient'\n},\n                  inplace=True)\ntrain_corr[(train_corr['Feature A'] == 'Survived')].style.background_gradient(\n    cmap='summer_r')","c79eb56c":"# Detecting missing data:\n\nfig, ax = plt.subplots(ncols=2, figsize=(20, 6))\nsns.heatmap(v_train.isnull(),\n            yticklabels=False,\n            cbar=False,\n            cmap='magma',\n            ax=ax[0])\nsns.heatmap(v_test.isnull(),\n            yticklabels=False,\n            cbar=False,\n            cmap='magma',\n            ax=ax[1])\n\nax[0].set_title('Train Data Missing Values')\nax[1].set_title('Test Data Missing Values')\n\nplt.xticks(rotation=90)\nplt.show()","f67414f0":"def status(feature):\n    print('Processing', feature, ': DONE')\n    print(f'Shape after processing {combined.shape}')\n    print('*' * 40)","86d6e1a4":"def get_combined_data():\n    # Reading train data:\n    train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n    # Reading test data:\n    test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n    # Extracting the targets from the training data:\n    targets = train.Survived\n\n    # Merging train data and test data for future feature engineering:\n    combined = train.append(test)\n    combined.reset_index(inplace=True, drop=True)\n\n    return combined","13222284":"def process_family():\n\n    global combined\n    # Introducing a new feature : The size of families (Including the passenger):\n    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n\n    # Introducing another feature based on the family size:\n    \n    combined['Alone'] = combined['FamilySize'].map(lambda s: 1\n                                                   if s == 1 else 0)\n\n    # These two below are optional, it didn't help with the model...\n    \n    #combined['SmallFamily'] = combined['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n    #combined['LargeFamily'] = combined['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n\n    status('Family')\n    return combined","27aec832":"def family_survival():\n    global combined\n\n    ''' A function working on family survival rate using last names and ticket features: '''\n\n    # Extracting surnames:\n    \n    combined['Last_Name'] = combined['Name'].apply(\n        lambda x: str.split(x, \",\")[0])\n\n    # Adding new feature: 'Survived'\n    \n    default_survival_rate = 0.5\n    combined['Family_Survival'] = default_survival_rate\n\n    for grp, grp_df in combined[[\n            'Survived', 'Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n            'SibSp', 'Parch', 'Age', 'Cabin'\n    ]].groupby(['Last_Name', 'Fare']):\n\n        if (len(grp_df) != 1):\n            \n            # A Family group is found.\n            \n            for ind, row in grp_df.iterrows():\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    combined.loc[combined['PassengerId'] ==\n                                 passID, 'Family_Survival'] = 1\n                elif (smin == 0.0):\n                    combined.loc[combined['PassengerId'] ==\n                                 passID, 'Family_Survival'] = 0\n\n    for _, grp_df in combined.groupby('Ticket'):\n        if (len(grp_df) != 1):\n            for ind, row in grp_df.iterrows():\n                if (row['Family_Survival'] == 0) | (\n                        row['Family_Survival'] == 0.5):\n                    smax = grp_df.drop(ind)['Survived'].max()\n                    smin = grp_df.drop(ind)['Survived'].min()\n                    passID = row['PassengerId']\n                    if (smax == 1.0):\n                        combined.loc[combined['PassengerId'] ==\n                                     passID, 'Family_Survival'] = 1\n                    elif (smin == 0.0):\n                        combined.loc[combined['PassengerId'] ==\n                                     passID, 'Family_Survival'] = 0\n\n    status('FamilySurvival')\n    \n    return combined","1a3ba572":"def get_titles():\n\n    title_dictionary = {\n        'Capt': 'Dr\/Clergy\/Mil',\n        'Col': 'Dr\/Clergy\/Mil',\n        'Major': 'Dr\/Clergy\/Mil',\n        'Jonkheer': 'Honorific',\n        'Don': 'Honorific',\n        'Dona': 'Honorific',\n        'Sir': 'Honorific',\n        'Dr': 'Dr\/Clergy\/Mil',\n        'Rev': 'Dr\/Clergy\/Mil',\n        'the Countess': 'Honorific',\n        'Mme': 'Mrs',\n        'Mlle': 'Miss',\n        'Ms': 'Mrs',\n        'Mr': 'Mr',\n        'Mrs': 'Mrs',\n        'Miss': 'Miss',\n        'Master': 'Master',\n        'Lady': 'Honorific'\n    }\n\n    # Extract the title from names\n    combined['Title'] = combined['Name'].map(\n        lambda name: name.split(',')[1].split('.')[0].strip())\n\n    # Mapping titles\n    combined['Title'] = combined.Title.map(title_dictionary)\n    status('Title')\n    return combined","1e17e853":"def process_names():\n    \n    global combined\n    \n    # Cleaning the Name variable:\n    \n    combined.drop('Name', axis=1, inplace=True)\n\n    # Encoding names as dummy variables:\n    \n    titles_dummies = pd.get_dummies(combined['Title'], prefix='Title')\n    combined = pd.concat([combined, titles_dummies], axis=1)\n\n    # Removing the title variable after getting dummies:\n    \n    combined.drop('Title', axis=1, inplace=True)\n\n    status('names')\n    return combined","2bafdff2":"def process_age():\n    global combined\n    \n    # A function that fills the missing values of the Age variable:\n    \n    combined['Age'] = combined.groupby(\n        ['Pclass', 'Sex'])['Age'].apply(lambda x: x.fillna(x.median()))\n    status('Age')\n    return combined","db6d674a":"def age_binner():\n    global combined\n    # Ranging and grouping Ages\n    \n    #bins = [0, 2, 18, 35, 65, np.inf]\n    \n    names = ['less2', '2-18', '18-35', '35-65', '65plus']\n\n    #combined['AgeBin'] = pd.cut(combined['Age'], bins, labels=names)\n    \n    combined['AgeBin'] = pd.qcut(combined['Age'],q = 5, labels = names)\n    age_dummies = pd.get_dummies(combined['AgeBin'], prefix='AgeBin')\n    combined = pd.concat([combined, age_dummies], axis=1)\n    combined.drop('AgeBin', inplace=True, axis=1)\n    combined.drop('Age', inplace=True, axis=1)\n    status('Age Bins')\n    \n    return combined","52491e18":"def process_fares():\n    global combined\n\n    # Filling missing values in fare:\n\n    combined['Fare'] = combined.groupby(\n        ['Pclass', 'Sex'])['Fare'].apply(lambda x: x.fillna(x.median()))\n    status('fare')\n    return combined","4cb6e325":"def process_fare_bin(onehot='None'):\n\n    global combined\n    \n    # Ranging and grouping Fare using historical data\n    \n    bins = [-1, 7.91, 14.454, 31, 99, 250, np.inf]\n    names = [0, 1, 2, 3, 4, 5]\n\n    combined['FareBin'] = pd.cut(combined['Fare'], bins,\n                                 labels=names).astype('int')\n    if onehot == 'yes':\n        farebin_dummies = pd.get_dummies(combined['FareBin'], prefix='FareBin')\n        combined = pd.concat([combined, farebin_dummies], axis=1)\n        combined.drop('FareBin', inplace=True, axis=1)\n        combined.drop('Fare', inplace=True, axis=1)\n    elif onehot == 'both':\n        farebin_dummies = pd.get_dummies(combined['FareBin'], prefix='FareBin')\n        combined = pd.concat([combined, farebin_dummies], axis=1)\n        combined.drop('FareBin', inplace=True, axis=1)\n    else:\n        combined.drop('Fare', inplace=True, axis=1)\n\n    status('FareBin')\n    \n    return combined","b082c4c1":"def scale_fare():\n\n    # A definition for scaling fare values:\n\n    from scipy.stats import skew, boxcox_normmax, norm\n    from scipy.special import boxcox1p\n    global combined\n    combined['Fare'] = boxcox1p(combined['Fare'],\n                                boxcox_normmax(combined['Fare'] + 1))\n    status('NFareBin')\n    return combined","c05d38a0":"def process_embarked():\n    \n    global combined\n    \n    # Filling missing embarked values with the most frequent one:\n    \n    combined.Embarked.fillna(combined.Embarked.mode()[0], inplace=True)\n    \n    # One hot encoding.\n    \n    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')\n    combined = pd.concat([combined, embarked_dummies], axis=1)\n    combined.drop('Embarked', axis=1, inplace=True)\n    status('Embarked')\n    \n    return combined","67d69d92":"def process_cabin():\n    global combined\n    \n    # Replacing missing cabins with M (for Missing).\n    \n    combined['Cabin_Informed'] = [\n        1 if pd.notnull(cab) else 0 for cab in combined['Cabin']\n    ]\n    combined.Cabin.fillna('M', inplace=True)\n\n    # Mapping each Cabin value with the cabin letter.\n    \n    combined['Deck'] = combined['Cabin'].map(lambda c: c[0])\n\n    combined['Deck'].replace('T', 'A', inplace=True)\n\n    # One hot encoding...\n    \n    cabin_dummies = pd.get_dummies(combined['Deck'], prefix='Deck')\n    combined = pd.concat([combined, cabin_dummies], axis=1)\n\n    combined.drop('Cabin', axis=1, inplace=True)\n    combined.drop('Deck', axis=1, inplace=True)\n    status('Cabin')\n    return combined","99330ef8":"def process_sex():\n    global combined\n    \n    # Mapping string values with numerical ones.\n    \n    combined['Sex'] = combined['Sex'].map({'male': 1, 'female': 0})\n    status('Sex')\n    return combined","21179dd9":"def process_pclass():\n\n    global combined\n    # One hot encoding pclass into 3 categories:\n    \n    pclass_dummies = pd.get_dummies(combined['Pclass'], prefix='Pclass')\n\n    # Adding dummy variables to main set.\n    \n    combined = pd.concat([combined, pclass_dummies], axis=1)\n\n    # Removing redundant 'Pclass'.\n    \n    combined.drop('Pclass', axis=1, inplace=True)\n\n    status('Pclass')\n    return combined","50a30ed7":"def process_ticket():\n\n    global combined\n\n    ''' A function that extracts each prefix of the ticket, returns 'Unknown' if no prefix (i.e the ticket is a digit). '''\n    \n    def cleanTicket(ticket):\n        ticket = ticket.replace('.', '')\n        ticket = ticket.replace('\/', '')\n        ticket = ticket.split()\n        ticket = map(lambda t: t.strip(), ticket)\n        ticket = list(filter(lambda t: not t.isdigit(), ticket))\n        if len(ticket) > 0:\n            return ticket[0]\n        else:\n            return 'Unknown'\n\n    # Extracting dummy variables from tickets:\n\n    combined['Ticket'] = combined['Ticket'].map(cleanTicket)\n    tickets_dummies = pd.get_dummies(combined['Ticket'], prefix='Ticket')\n    combined = pd.concat([combined, tickets_dummies], axis=1)\n    combined.drop('Ticket', inplace=True, axis=1)\n\n    status('Ticket')\n    return combined","c4caa048":"def dropper():\n    \n    global combined\n\n    combined.drop('Cabin', axis=1, inplace=True)\n    combined.drop('PassengerId', inplace=True, axis=1)\n    combined.drop('Last_Name', inplace=True, axis=1)\n    combined.drop('Survived', inplace=True, axis=1)\n    combined.drop('Ticket', inplace=True, axis=1)\n\n    return combined","b165b0b3":"# Executing definitions to process data.\ncombined = get_combined_data()  # For merging train test data.\ncombined = family_survival()  # For creating family survival feature.\ncombined = process_family()  # For creating Family size feature.\ncombined = get_titles()  # For extracting titles.\ncombined = process_names()  # For one hot encoding titles.\ncombined = process_age()  # For imputing missing age values.\ncombined = age_binner()  # For grouping and hot encoding age ranges.\ncombined = process_fares()  # For imputing fares.\n# For grouping and label encoding fares, can use 'both' for keeping age with dummies or yes for just one hot encoding.\ncombined = process_fare_bin(onehot='no')\n# combined =scale_fare() # For scaling age values.\ncombined = process_embarked()  # For imputing embarked and one hot encoding.\n# combined = process_cabin() # For extracting deck info from cabins.\ncombined = process_sex()  # For label encoding sex.\n# combined = process_pclass() # For one hot encoding pclass.\n# combined = process_ticket() # For extracting ticket info.\ncombined = dropper()  # For dropping not needed features.\n\nprint(\n    f'Processed everything. Missing values left: {combined.isna().sum().sum()}'\n)","1d81cf8c":"# Assigning engineered data to inspection variable.\n\nv_merged = combined.copy()\nv_merged['Survived'] = v_train['Survived']\nv_merged.head()","9c47de87":"v_merged.describe()","b4bfcfcb":"# Display numerical correlations between features on heatmap.\n\nsns.set(font_scale=1.1)\ncorrelation_train = v_merged.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(18, 15))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1)\n\nplt.show()","64bdf6ce":"# Importing packages for modelling.\n\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, learning_curve, cross_validate, train_test_split, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, plot_roc_curve, auc\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom mlxtend.plotting import plot_decision_regions","7ef89046":"def recover_train_test_target():\n    global combined\n\n    y = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', usecols=['Survived'])['Survived']\n    X = combined.iloc[:idx]\n    X_test = combined.iloc[idx:]\n\n    return X, X_test, y\n\n\nX, X_test, y = recover_train_test_target()","f38f3854":"print(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint(f'X_test shape: {X_test.shape}')","e2fed498":"# Models\n\ncv = StratifiedKFold(10, shuffle=True, random_state=42)\n\nrf = RandomForestClassifier(criterion='gini',\n                            n_estimators=1750,\n                            max_depth=7,\n                            min_samples_split=6,\n                            min_samples_leaf=6,\n                            max_features='auto',\n                            oob_score=True,\n                            random_state=42,\n                            n_jobs=-1,\n                            verbose=0)\n\nlg = lgb.LGBMClassifier(max_bin=4,\n                        num_iterations=550,\n                        learning_rate=0.0114,\n                        max_depth=3,\n                        num_leaves=7,\n                        colsample_bytree=0.35,\n                        random_state=42,\n                        n_jobs=-1)\n\nxg = xgb.XGBClassifier(\n    n_estimators=2800,\n    min_child_weight=0.1,\n    learning_rate=0.002,\n    max_depth=2,\n    subsample=0.47,\n    colsample_bytree=0.35,\n    gamma=0.4,\n    reg_lambda=0.4,\n    random_state=42,\n    n_jobs=-1,\n)\n\nsv = SVC(probability=True)\n\nlogreg = LogisticRegression(n_jobs=-1, solver='newton-cg')\n\ngb = GradientBoostingClassifier(random_state=42)\n\ngnb = GaussianNB()\n\nmlp = MLPClassifier(random_state=42)","dc354d09":"estimators = [rf, lg, xg, gb, sv, logreg, gnb, mlp]","8b5ee98b":"def model_check(X, y, estimators, cv):\n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est in estimators:\n\n        MLA_name = est.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n        #    model_table.loc[row_index, 'MLA Parameters'] = str(est.get_params())\n\n        cv_results = cross_validate(\n            est,\n            X,\n            y,\n            cv=cv,\n            scoring='accuracy',\n            return_train_score=True,\n            n_jobs=-1\n        )\n\n        model_table.loc[row_index, 'Train Accuracy Mean'] = cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test Accuracy Mean'] = cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test Accuracy Mean'],\n                            ascending=False,\n                            inplace=True)\n\n    return model_table","378f1bed":"raw_models = model_check(X, y, estimators, cv)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","7cb925fb":"# Plotting model performances\ndef model_barplot(models, bins):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    g = sns.barplot('Test Accuracy Mean',\n                    'Model Name',\n                    data=models,\n                    palette='plasma',\n                    orient='h',\n                    **{'xerr': models['Test Std']})\n    g.set_xlabel('Test Mean Accuracy')\n    g = g.set_title('Cross validation scores')\n    ax.xaxis.set_major_locator(MaxNLocator(nbins=bins))","dfaf2c71":"# Plotting model performances\nmodel_barplot(raw_models, 32)","945504e7":"def m_roc(estimators, cv, X, y):\n\n    fig, axes = plt.subplots(math.ceil(len(estimators) \/ 2),\n                             2,\n                             figsize=(25, 50))\n    axes = axes.flatten()\n\n    for ax, estimator in zip(axes, estimators):\n        tprs = []\n        aucs = []\n        mean_fpr = np.linspace(0, 1, 100)\n\n        for i, (train, test) in enumerate(cv.split(X, y)):\n            estimator.fit(X.loc[train], y.loc[train])\n            viz = plot_roc_curve(estimator,\n                                 X.loc[test],\n                                 y.loc[test],\n                                 name='ROC fold {}'.format(i),\n                                 alpha=0.3,\n                                 lw=1,\n                                 ax=ax)\n            interp_tpr = interp(mean_fpr, viz.fpr, viz.tpr)\n            interp_tpr[0] = 0.0\n            tprs.append(interp_tpr)\n            aucs.append(viz.roc_auc)\n\n        ax.plot([0, 1], [0, 1],\n                linestyle='--',\n                lw=2,\n                color='r',\n                label='Chance',\n                alpha=.8)\n\n        mean_tpr = np.mean(tprs, axis=0)\n        mean_tpr[-1] = 1.0\n        mean_auc = auc(mean_fpr, mean_tpr)\n        std_auc = np.std(aucs)\n        ax.plot(mean_fpr,\n                mean_tpr,\n                color='b',\n                label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' %\n                (mean_auc, std_auc),\n                lw=2,\n                alpha=.8)\n\n        std_tpr = np.std(tprs, axis=0)\n        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n        ax.fill_between(mean_fpr,\n                        tprs_lower,\n                        tprs_upper,\n                        color='grey',\n                        alpha=.2,\n                        label=r'$\\pm$ 1 std. dev.')\n\n        ax.set(xlim=[-0.02, 1.02],\n               ylim=[-0.02, 1.02],\n               title=f'{estimator.__class__.__name__} ROC')\n        ax.legend(loc='lower right', prop={'size': 18})\n    plt.show()","828b5d39":"m_roc(estimators, cv, X, y)","0a957be4":"def plot_learning_curve(estimators,\n                        X,\n                        y,\n                        ylim=None,\n                        cv=None,\n                        n_jobs=None,\n                        train_sizes=np.linspace(.1, 1.0, 5)):\n\n    fig, axes = plt.subplots(math.ceil(len(estimators) \/ 2),\n                             2,\n                             figsize=(25, 50))\n    axes = axes.flatten()\n\n    for ax, estimator in zip(axes, estimators):\n\n        ax.set_title(f'{estimator.__class__.__name__} Learning Curve')\n        if ylim is not None:\n            ax.set_ylim(*ylim)\n        ax.set_xlabel('Training examples')\n        ax.set_ylabel('Score')\n\n        train_sizes, train_scores, test_scores, fit_times, _ = \\\n            learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                           train_sizes=train_sizes,\n                           return_times=True)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n\n        # Plot learning curve\n\n        ax.fill_between(train_sizes,\n                        train_scores_mean - train_scores_std,\n                        train_scores_mean + train_scores_std,\n                        alpha=0.1,\n                        color='r')\n        ax.fill_between(train_sizes,\n                        test_scores_mean - test_scores_std,\n                        test_scores_mean + test_scores_std,\n                        alpha=0.1,\n                        color='g')\n        ax.plot(train_sizes,\n                train_scores_mean,\n                'o-',\n                color='r',\n                label='Training score')\n        ax.plot(train_sizes,\n                test_scores_mean,\n                'o-',\n                color='g',\n                label='Cross-validation score')\n        ax.legend(loc='best')\n        ax.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.show()","ab71b400":"plot_learning_curve(estimators,\n                    X,\n                    y,\n                    ylim=None,\n                    cv=cv,\n                    n_jobs=-1,\n                    train_sizes=np.linspace(.1, 1.0, 10))","22bf5fb6":"def f_imp(estimators, X, y, bins):\n    \n    fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n    axes = axes.flatten()\n\n    for ax, estimator in zip(axes, estimators):\n\n        try:\n            estimator.fit(X, y)\n            feature_imp = pd.DataFrame(sorted(\n                zip(estimator.feature_importances_, X.columns)),\n                                       columns=['Value', 'Feature'])\n\n            sns.barplot(x=\"Value\",\n                        y=\"Feature\",\n                        data=feature_imp.sort_values(by=\"Value\",\n                                                     ascending=False),\n                        ax=ax,\n                        palette='plasma')\n            plt.title('Features')\n            plt.tight_layout()\n            ax.set(title=f'{estimator.__class__.__name__} Feature Impotances')\n            ax.xaxis.set_major_locator(MaxNLocator(nbins=bins))\n        except:\n            continue\n    plt.show()","0a507506":"f_imp(estimators, X, y, 14)","9c635a4b":"# Train\nrf.fit(X, y)\n# Extract first tree from random forest\nestimator = rf.estimators_[0]\n\nfrom sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = X.columns,\n                class_names = ['Not Survived','Survived'],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n# Convert to png\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in python\nplt.figure(figsize = (40, 20))\nplt.imshow(plt.imread('tree.png'))\nplt.axis('off');\nplt.show();","b5374454":"def f_selector(X, y, est, features):\n    X_train, X_valid, y_train, y_valid = train_test_split(X,\n                                                          y,\n                                                          test_size=0.4,\n                                                          random_state=42)\n\n    # Create the RFE with a LogisticRegression estimator and 4 features to select\n    rfe = RFE(estimator=est, n_features_to_select=features, verbose=1)\n\n    # Fit the eliminator to the data\n    rfe.fit(X_train, y_train)\n\n    # Print the features and their ranking (high = dropped early on)\n    print(dict(zip(X.columns, rfe.ranking_)))\n\n    # Print the features that are not eliminated\n    print(X.columns[rfe.support_])\n\n    # Calculates the test set accuracy\n    acc = accuracy_score(y_valid, rfe.predict(X_valid))\n    print(\"{0:.1%} accuracy on test set.\".format(acc))\n    X_red = X[X_train.columns[rfe.support_].to_list()]\n    X_te_red = X_test[X_train.columns[rfe.support_].to_list()]\n    return X_red, X_te_red","dcb0ac81":"X_sel, X_test_sel = f_selector(X, y, rf, 11)","a6f47430":"# Build the pipeline\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=2)),\n])\n\n# Fit the pipeline to the training data\nX_sel_red = pipe.fit_transform(X_sel)\nX_test_sel_red = pipe.transform(X_test_sel)","8ac7f3f3":"def dec_regs(X, y, estimators):\n    fig, axes = plt.subplots(math.ceil(len(estimators) \/ 2),\n                             2,\n                             figsize=(20, 40))\n\n    axes = axes.flatten()\n\n    for ax, estimator in zip(axes, estimators):\n        estimator.fit(X, y.values)\n        plot_decision_regions(X, y.values, clf=estimator,colors='#e00d14,#3ca02c',markers='x+', ax=ax)\n        ax.set(title=f'Reg {estimator.__class__.__name__}')\n\n        ax.set_xlabel('PCA 1')\n        ax.set_ylabel('PCA 2')\n\n        handles, labels = ax.get_legend_handles_labels()\n        ax.legend(handles,\n                  ['Not Survived', 'Survived'],\n                  framealpha=0.3, scatterpoints=1)\n\n    plt.show()","968d842d":"def prob_reg(X, y):\n    from matplotlib.colors import ListedColormap\n    figure = plt.figure(figsize=(20, 40))\n    h = .02\n    i = 1\n\n    # preprocess dataset, split into training and test part\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.4, random_state=42)\n\n    x_min, x_max = X_sel_red[:, 0].min() - .5, X_sel_red[:, 0].max() + .5\n    y_min, y_max = X_sel_red[:, 1].min() - .5, X_sel_red[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Just plot the dataset first\n    cm = plt.cm.RdYlGn\n    cm_bright = ListedColormap(['#e00d14', '#3ca02c'])\n    ax = plt.subplot(5, 2, i)\n\n    # Iterate over estimators\n    for clf in estimators:\n        ax = plt.subplot(math.ceil(len(estimators) \/ 2), 2, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        g = ax.scatter(X_train[:, 0],\n                       X_train[:, 1],\n                       c=y_train,\n                       cmap=cm_bright,\n                       edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0],\n                   X_test[:, 1],\n                   c=y_test,\n                   cmap=cm_bright,\n                   edgecolors='k',\n                   alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n\n        ax.set_title(clf.__class__.__name__)\n        #ax.text(xx.max() - .3,\n        #        yy.min() + .3, (f'Score:{score:.2f}'),\n        #        size=15,\n        #        horizontalalignment='right')\n\n        ax.set_xlabel('PCA 1')\n        ax.set_ylabel('PCA 2')\n        plt.legend(handles=g.legend_elements()[0],\n                   labels=['Not Survived', 'Survived'],\n                   framealpha=0.3,\n                   scatterpoints=1)\n\n        i += 1\n\n    plt.tight_layout()\n    plt.show()","4665c6c0":"dec_regs(X_sel_red, y, estimators)","87856826":"prob_reg(X_sel_red, y)","80f58571":"pca_models = model_check(X_sel_red, y, estimators, cv)\ndisplay(pca_models.style.background_gradient(cmap='summer_r'))","2db17f7d":"# Plotting model performances\nmodel_barplot(pca_models, 24)","66d8e4c1":"rand_model_full_data = rf.fit(X, y)\nprint(accuracy_score(y, rand_model_full_data.predict(X)))\ny_pred = rand_model_full_data.predict(X_test)","473d0cbe":"test_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = test_df['PassengerId']\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submission.csv', header=True, index=False)\nsubmission_df.head(10)","90d21bd4":"<a id=\"sectioncv\"><\/a>\n## Reduced Dimension Model Results with Cross-Validation\n\nHmm, it seems our feature selection\/extractions reduced our overall accuracy around 5% with some overfitting models like GradientBoostingClassifier and RandomForestClassifier. I might try to tune them again but it's seems not needed for now...\n\n### [Back To Table of Contents](#toc_section)","6eafffaa":"<a id=\"section26\"><\/a>\n# Modelling\n\nSince preprocessing done we are ready for training our models. We start with loading packages and splitting our transformed data so we have 22 features and and 891 observations to train our estimators. Our test set has 418 observations to make predictions.\n\n### [Back To Table of Contents](#toc_section)","fb4c967e":"<a id=\"section4\"><\/a>\n## Numerical Features","479e16b6":"<a id=\"section10\"><\/a>\n## Title Extractor\n\nThis function extracts prefixes from passenger names and groups them in more generalized titles. We know male survival is low but we also know women and children first, right? So we can extract 'Master' title which is given for young males and their survival ratio might differ.\n\n### [Back To Table of Contents](#toc_section)","1bd60ab5":"<a id=\"section24\"><\/a>\n# Double Check\n\nOk, this is the part where we're going make sure our data is ready for the modelling. So, let's start with checking how's our data looking. Everyting seems in order...\n\n### [Back To Table of Contents](#toc_section)","8bc8e0bc":"<a id=\"section36\"><\/a>\n# Plotting Decision Boundaries\n\nI just wanted to visualise how different models act on given 2D data. I can say that good fitting models produced similar results with small differences. I take this as a good theoretical information...\n\n### [Back To Table of Contents](#toc_section)","ccf6a0b3":"<a id=\"section30\"><\/a>\n# ROC'S of the Models\n\nOk I'm going with the parts are that not much needed for this competition but I still want to apply them for the sake of learning and improving. Here are the results:\n\n### [Back To Table of Contents](#toc_section)","0799c7f5":"<a id=\"section8\"><\/a>\n## Family Assembler\n\n- Combining Parch and SibSp and passenger can give us new indicator that shows how big the family that person belongs to. \n\n- Since being alone might have an effect on survival we creating another feature for it.\n\n- We can also categorize family size manually \n\n### [Back To Table of Contents](#toc_section)","0de8d793":"<a id=\"top_section\"><\/a>\n# Introduction\n\nWhen I started doing this analysis my main goal was getting experience. I'm still learning and trying to improve my skills, so there might be some areas can be improved.\n\n\n### My main objectives on this project are:\n\n* Explorating and visualising the data, trying to get some insights about our dataset,\n* Getting data in better shape by feature engineering to help us in building better models,\n* Building and tuning couple regression models to get some stable results on predicting Titanic disaster outcome.\n\n### In this notebook we are going to try explore the data we have and going try answer questions like:\n\n- What is the survival rate for specific groups?\n- Is there any relation between given info of passengers and their survival?\n- Was women and children first policy in order?\n- Having higher social status in helped people getting in life boats?\n- The mustering order from captain was highly dependent on the class of the passengers, can we spot this  effect between pclass and the survival rates?\n- What are the effects of being alone or with family?\n\n- Can we predict if a passenger survived from the disaster with using machine learning techniques?\n- What can our predictions achieve with different approaches?\n- How are the ROC and Learning curves looks like for different models and can they help with selecting right one?\n- If we decrease dimensionality by feature selection and extraction, what would be the effects on the models?\n- How are the decision regions looks like for different estimators?\n\n\n\n#### If you liked this kernel feel free to upvote and leave feedback, thanks!\n","2ea1aeb4":"<a id=\"section12\"><\/a>\n## Age Filler\n\nThis function is for filling missing values in Age feature using most revelant features. Tried different approaches(like giving random value etc.) but they don't change model performance much so I selected simplier one.\n\n### [Back To Table of Contents](#toc_section)","0777192a":"<a id=\"section20\"><\/a>\n## Pclass sorter\nAgain a function for getting dummy variables for passenger classes.\n\n### [Back To Table of Contents](#toc_section)","05c1cbbf":"<a id=\"section31\"><\/a>\n# Learning Curves of the Models\n\nLearning curves are good indicators for showing if our models are overfitting or underfitting. It's also showing if we need more data or not but it's not the case here since we can't increase our numbers. After inspecting these graphs I'm thinking our top models are looking decent.\n\n### [Back To Table of Contents](#toc_section)","109d5412":"<a id=\"section22\"><\/a>\n## Housekeeping\n\nThis function is for cleaning the redundant features after extracting useful information from them. The cabin, passengerId, last name, ticket and ofc our target survived is going to be dropped. These features did their job on feature engineering and we can leave them in peace now.\n\n### [Back To Table of Contents](#toc_section)","efe9ad64":"<a id=\"section14\"><\/a>\n## Fare Imputer\n\nAgain a basic function for filling missing fare values.\n\n### [Back To Table of Contents](#toc_section)","aa1fdb36":"<a id=\"section15\"><\/a>\n## Fare Encoder\n\nSince there are multiple outliers I just wanted to group and categorize them. I used historical ticket prices for grouping, maybe using pcut would be better choice but I just went with historical grouping.\n\n### [Back To Table of Contents](#toc_section)","8ecc3731":"- The code below is edited version of [this example from sklearn official page](https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py) I really enjoyed while tinkering it, kudos to creators!","9612f772":"<a id=\"section13\"><\/a>\n## Age Grouper\n\nA function for categorizing the age values and setting dummy variables for each one.\n\nUpdated this one with the feedback from @Ayaan, replaced old custom grouping with pd.qcut This approach increased our score by 0.005'ish... Thanks for the feedback!\n\n### [Back To Table of Contents](#toc_section)","01a60f3a":"<a id=\"section2\"><\/a>\n# Exploring the Data","aedc37fc":"# Some last words\n\n**I'm still a beginner and want to improve myself in every way I can. So if you have any ideas to feedback please let me know in comments, and if you liked my work please don't forget to vote, thank you!**\n\n### [Back To Table of Contents](#toc_section)\n","e956f6fe":"**Observations:**\n\n- Sadly most of the train set passengers are not survived, around 62 percent.\n- While having highest portion of the passengers on the ship, 3rd class also having highest casuality ratio. Meanwhile first class have more survival rate. So this kinda confirms our hypothesis that mustering orders from captain (1>2>3) might have effect on survival.\n- Again most of the passengers on the ship are males(around two third of the passengers) they also have much higher ratio of casualities too. This confirms women part of the \"Women and children first policy\" had an effect in our Titanic case.\n- Most of our passengers are travelling alone, they also have high casuality rate. So being married or with family has some positive effect on survival?\n- Most of the passenger are embarked from Southampton. Survival rates differ between ports but I'm not sure if it's important observation or just random. We can relate this to families, groups to get something out of it or just use as it is.\n\nOk these are my rough observations from inspecting some categorical data, let's continue with numerical ones...\n\n### [Back To Table of Contents](#toc_section)","e8f1ccdf":"<a id=\"section25\"><\/a>\n## Correlation Matrix\n\nAlright one last table I'm going to check is correlation matrix. So we can spot linear relations between features. Especially the ones which effects survival rate. It seems Sex, the Mr. title and family survival ratio is most related features to survival.\n\nWell... That's it then let's continue with modelling now!\n\n### [Back To Table of Contents](#toc_section)","81b99dcd":"- **Age:** Well an obvious feature that indicates age of the passenger.\n- **Fare:** Again obvious one that shows amount of the money spent on the ticket.","92cbe80a":"<a id=\"section18\"><\/a>\n## Deck Finder\n\nThis approach is trying to find something out of Cabin values and assignin it to newly created Deck column. I tried this approach on models but didn't get much, other than adding complexity.\n\n### [Back To Table of Contents](#toc_section)","8674b544":"<a id=\"section3\"><\/a>\n## Categorical Features\n\n- **Survived:** Indicates that if particular passenger survived(1) or not(0)\n- **Pclass:** Shows classes for the passenger, 1 for first, 2 for second and 3 for third.\n- **Sex:** Indicates gender of the passenger. Might be crucial indicator for our model since historical records show women were first to save in ship accidents.\n- **SibSp:** The number of siblings and spouses on the ship, might be useful for extracting family ties.\n- **Parch:** The number of parents and children on the ship, migt have similar use with SibSp.\n- **Embarked:** Flag for the where the passenger embarked from, C for Cherbourg, Q for Queenstown, S for Southampton\n\n### [Back To Table of Contents](#toc_section)","11cd6b89":"**It seems, fare and age has no strong linear relation with survival outcomes even with Spearman correlation. We're going to bin these variables to get better results...**","9298895b":"<a id=\"section31.1\"><\/a>\n# Feature Selection","8e8de17f":"<a id=\"sectionlst\"><\/a>\n#  Submission","fd0e6b85":"<a id=\"section17\"><\/a>\n## Embarked Processor\n\nA function for filling missing values and then hot encoding for each value.\n\n### [Back To Table of Contents](#toc_section)","e9580052":"<a id=\"section23\"><\/a>\n## Feeding the Machine\n\nOk this is our command room, this is where I control the switches for feature engineering(aka commenting some functions out or combine them :D) Well, this was pretty trial and error methodology for me. Tried couple combinations of feature engineering and tried to find best fit on the models. Here is what I came up with.\n\n### [Back To Table of Contents](#toc_section)","0dd8394e":"<a id=\"section6\"><\/a>\n# Building the Feature Engineering Machine\n\nHere comes to part where we are going to play with the data we have. I wanted to build functions instead of moving one by one, I feel like with this way I can change my build up for the model more easily. Since there are some changes might get better results but there are some changes might badly effect outcome of the model by increasing complexity. This is a trial and error process so with this approach I feel like I can control them easily.\n\nI called this part machine but if you are more traditional you can say this is our toolbox too :)\n\n### [Back To Table of Contents](#toc_section)","767fa56b":"<a id=\"section37\"><\/a>\n# Plotting Decision Regions\n\nWanted to add this version including probabilities too, this one adds new vision to decision regions and worth inspecting. We can see how sure are the models on deciding and also giving hints about how complex model is. This one still experimental for me and may need to be improved, I'd be grateful for your feedback!\n\n### [Back To Table of Contents](#toc_section)","dec32919":"<a id=\"toc_section\"><\/a>\n## Table of Contents\n* [Introduction](#top_section)\n    - [Well... What do we have here?](#section1)\n* [Exploring the Data](#section2)\n    - [Categorical Features](#section3)\n    - [Numerical Features](#section4)\n    - [Missing Values](#section5)    \n* [Building the Feature Engineering Machine](#section6)\n    - [Data Merger](#section7)\n    - [Family Assembler](#section8)\n    - [Family Survival Detector](#section9)   \n    - [Title Extractor](#section10)\n    - [Title Encoder](#section11)\n    - [Age Filler](#section12)\n    - [Age Grouper](#section13)\n    - [Fare Imputer](#section14)\n    - [Fare Encoder](#section15)\n    - [Scaler](#section16)\n    - [Embarked Processor](#section17)\n    - [Deck Finder](#section18)\n    - [Gender Mapper](#section19)\n    - [Pclass Sorter](#section20)\n    - [Ticket Cleaner](#section21)\n    - [Housekeeping](#section22)\n    - [Feeding the Machine](#section23)\n* [Double Check](#section24)\n    - [Correlation Matrix](#section25)\n* [Modelling](#section26)\n    - [Model Selection](#section27)\n    - [Cross-Validate Models](#section28)\n    - [Model Results](#section29)\n    - [ROC'S of the Models](#section30)\n    - [Learning Curves of the Models](#section31)\n* [Feature Selection](#section31.1)\n    - [Feature Importances](#section32)\n    - [Decision Trees](#section33)    \n    - [Feature Selection by Recursive Feature Elimination](#section34)\n    - [Dimension Reduction by Principal Component Analysis](#section35)\n    - [Reduced Dimension Model Results with Cross-Validation](#sectioncv)\n* [Plotting Decision Boundaries](#section36)\n* [Plotting Decision Regions](#section37)\n* [Submission & Some Last Words](#sectionlst)\n\n\n","2ca445d2":"<a id=\"section1\"><\/a>\n# Well... What do we have here?\n\nOk we have two sets(train and test) data and in total 1301 observations 11 features. Our target is Survived column which is not present on the test set(duh!)... With basic inspection I'd say 'PassengerId' has effect on survival, but... Maybe we can use it on feature engineering part? For the rest we gonna inspect them individually soon but  generally speaking they look mostly categorical data with some continuous values like Fare and Age.\n\nI'm going to use different sets of variables for visualizing and feature engineering so let's start with assigning visualizing ones. I'm also saving idx variable for future use.\n\n### [Back To Table of Contents](#toc_section)\n","ebb2bb83":"<a id=\"section29\"><\/a>\n## Model Results\n\nHmm... It seems our top classifiers are getting similar results and they are mostly ensemble models. Our basic Logistic Regression did pretty well too! Gradient Boosting Classifier seems effected by high variance, we might try to fix it by hyperparameter tuning but I'd say we already have good number of decent estimators so we can continue with them.\n\n### [Back To Table of Contents](#toc_section)","ea679d72":"<a id=\"section27\"><\/a>\n## Model Selection\n\nI choose couple models to try, mostly ensemble models with boosting but we also have some linear models like  Logistic Regression. I tuned some these models with Optuna but I'm not going to include these meta parts on this notebook.\n\n### [Back To Table of Contents](#toc_section)","832fc447":"<a id=\"section19\"><\/a>\n## Gender Mapper\n\nThis is a function for assigning numerical values for each gender.\n\n### [Back To Table of Contents](#toc_section)","82281c7d":"<a id=\"section7\"><\/a>\n## Data Merger\n\n- This function merges our train and test sets so we can transform them together.\n\n### [Back To Table of Contents](#toc_section)","7b88e6a5":"<a id=\"section35\"><\/a>\n# Dimension Reduction by Principal Component Analysis\n\nGiven a collection of points in two, three, or higher dimensional space, a \"best fitting\" line can be defined as one that minimizes the average squared distance from a point to the line. The next best-fitting line can be similarly chosen from directions perpendicular to the first. Repeating this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. These basis vectors are called principal components, and several related procedures principal component analysis (PCA).\n\n**Again with this we're reducing our models dimensions to 2, so we can plot decision regions. Like previous steps I'm doing this to get experience what can we do with PCA.**\n\n### [Back To Table of Contents](#toc_section)","d4188f08":"**Here we pruned half of the features we have, what left there is more important features like:**\n\n'Pclass', 'Sex', 'SibSp', 'Family_Survival', 'FamilySize', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'FareBin'","2466832d":"<a id=\"section32\"><\/a>\n## Feature Importances\n\nAlright we have couple features and they seem working well. But I feel like some of them are not that important, I also want to reduce dimensions(features) in next chapters to gain new insights anyways. Luckly we have couple decision trees based models so we can use them to visualize feature importances. So... Let's get going!\n\n### [Back To Table of Contents](#toc_section)","3d8b96d1":"**Observations:**\n\n- Seems our Age values distrubuted well, little right skewed with high number of young adults. With the median age of 28 there are little bit of ouliers but things look ordinal.\n\n- Fare distrubution doesn't look good, not fitting normal distrubution at all. While most of the passenger fares are below 30, there are some crazy outliers lying there like 500+ spent on tickets. We might need to scale our data for the models or assigning bins to categorize them to get better results...\n\n### [Back To Table of Contents](#toc_section)","0502415e":"<a id=\"section16\"><\/a>\n## Scaler\n\nAnother function for getting rid of outliers in Fare values with using boxcox. I wasn't sure about if I use this or binning, but model did little better at the end with binning.\n\n### [Back To Table of Contents](#toc_section)","fdca22e5":"<a id=\"section33\"><\/a>\n# Decision Trees\nSince we were talking about decision tree based models I wanted to visualize how they work. For that I'm going to choose random forest classifier. RF has multiple decision trees in it, we clas display them all but I think first one should be enough for this notebook. You can learn more about decision tree plotting in this great notebook [here](https:\/\/www.kaggle.com\/willkoehrsen\/visualize-a-decision-tree-w-python-scikit-learn). \n\nYou can see the tree is somewhat matching our feature importances ranking just above, again this is just for gaining some knowledge and you might not need it for the competition...\n\n### [Back To Table of Contents](#toc_section)","4ea0e7e5":"<a id=\"section21\"><\/a>\n## Ticket Cleaner\n\nThis function extract prefixes from ticket and assign them to specific group based on their prefix. Again I found this approach adding complexity only.\n\n### [Back To Table of Contents](#toc_section)","c8d2b0ec":"<a id=\"section34\"><\/a>\n# Feature Selection by Recursive Feature Elimination\n\nAgain a part where my main motivation is learning and applying what I know. Here I'm gonna reduce dimensionality with two sklearn tools. First one is recursive feature elimination (RFE). The goal of RFE is to select features by recursively considering smaller and smaller sets of features. I'm going to prune half of the features we have, again this is just for experience and might not be needed for Titanic competition.\n\n### [Back To Table of Contents](#toc_section)","0e4d0107":"<a id=\"section11\"><\/a>\n## Title Encoder\n\nThis function is for getting dummy variables for titles, might or might not use it.\n\n### [Back To Table of Contents](#toc_section)","6a00c953":"<a id=\"section5\"><\/a>\n## Missing Values\n\nOn both datasets Cabin feature is missing a lot, it looks this feature not useful for modelling but we might give it a chance with feature engineering later.\n\nAgain, Age feature has many missing values, we can impute them with some logical way to use later...\n\nThere are little number of missing values on Embarked and Fare, I think we can impute them without taking much risk.\n\n### [Back To Table of Contents](#toc_section)","49ec3c61":"<a id=\"section9\"><\/a>\n## Family Survival Detector\n\nOn this approach I got insprided from S.Xu's work with families. The family information is extracted from name, fare and ticket number through examing of the data. I found this feature pretty useful on models.\n\n### [Back To Table of Contents](#toc_section)","e472a711":"<a id=\"section28\"><\/a>\n# Cross-Validate Models\n\nWe gonna shuffle and fold 10 times to cross validate our models on the training set. With this way we can get some strong indicators to select which estimator is best fit.\n\n### [Back To Table of Contents](#toc_section)"}}