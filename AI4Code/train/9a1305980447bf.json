{"cell_type":{"43651071":"code","d34c23b8":"code","355272bd":"code","0e4cd55c":"code","5429043c":"code","9131bd82":"code","8a19324d":"code","8ebbcf7f":"code","52838329":"code","1a1fca45":"code","201e458e":"code","2e80f7c3":"code","235602fa":"code","20ae8aea":"code","fcfa4627":"code","ca3afe61":"code","06571d5b":"code","cc75a100":"code","caf89ddd":"code","e028022c":"code","39b1a99c":"code","b10ce394":"code","b53f068d":"code","a5c5a0af":"code","3539227f":"code","5b68b4c1":"code","7d831a2b":"code","f40ebf27":"code","4cc3f0e8":"code","8147488b":"code","7d7f0ac6":"code","2ad02719":"code","d7b092eb":"code","fafaf6e4":"code","03d146a4":"code","d0e4f70f":"code","a351a5b6":"code","5284a7c5":"markdown","613f5627":"markdown","32246d8a":"markdown","bd5fb7b2":"markdown","9ee97486":"markdown","c11c3e12":"markdown","6094bd4f":"markdown"},"source":{"43651071":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d34c23b8":"dataset = pd.read_csv('\/kaggle\/input\/dataset1\/unpaddy.csv.txt',index_col=[0])","355272bd":"dataset","0e4cd55c":"X = dataset.iloc[:, :-1]\nY= dataset.iloc[:,-1]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)\n","5429043c":"y_test","9131bd82":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nscaler.fit_transform(X_train)","8a19324d":"x_train_scaled = scaler.transform(X_train)\nx_test_scaled = scaler.transform(X_test)","8ebbcf7f":"pd.DataFrame(x_train_scaled)","52838329":"from tensorflow import keras\nfrom tensorflow.keras import layers","1a1fca45":"model = keras.Sequential([ \n    layers.Dense(units=512, activation='relu', input_shape=[6]),\n    layers.Dropout(0.2),\n    layers.Dense(units=256, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(units=512, activation='relu'),\n    layers.Dense(units=1, activation=\"linear\")\n    # une seule sortie pour la regression \n    \n])","201e458e":"model.compile(\noptimizer='adam',\nloss='mse',\nmetrics=['mae']\n)\nmodel.summary()","2e80f7c3":"history = model.fit(x_train_scaled,y_train,batch_size=64,epochs=1000,verbose=1,validation_split=0.2)","235602fa":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[5:, ['loss','val_loss']].plot();\nhistory_df['loss'].min()","20ae8aea":"predictions = model.predict(x_test_scaled)\n\nrandomlist = []\nfor i in range(0,8):\n    n = np.random.randint(0,X_test.shape[0])\n    randomlist.append(n)\nprint(randomlist)","fcfa4627":"print('les donn\u00e9es pr\u00e9dits par notre modele:')\nfor x in randomlist:\n    print(predictions[x])\n    ","ca3afe61":"print('les donn\u00e9e r\u00e9el:')\nfor x in randomlist:\n     print(y_test.iloc[x])","06571d5b":"# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nresults = model.evaluate(x_test_scaled, y_test)\nmse_neural,mae_neural=results\nprint(\"l'erreure moyenne quadratique(MSE),l'erreur moyenne absolue(MAE):\", results)","cc75a100":"model.save('\/kaggle\/working\/model0') ","caf89ddd":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmodelf = RandomForestRegressor(n_estimators = 10)\njk=modelf.fit(x_train_scaled, y_train)\n\ny_pred_RF = modelf.predict(x_test_scaled)\n\nmse_RF = mean_squared_error(y_test, y_pred_RF)\nmae_RF = mean_absolute_error(y_test, y_pred_RF)\nprint('l\\'erreure moyenne quadratique(MSE) du Random Forest: ', mse_RF)\nprint('l\\'erreure moyenne absolue(MAE) du Random Forest:', mae_RF)","e028022c":"### Decision tree\nfrom sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeRegressor()\ntree.fit(x_train_scaled, y_train)\ny_pred_tree = tree.predict(x_test_scaled)\nmse_dt = mean_squared_error(y_test, y_pred_tree)\nmae_dt = mean_absolute_error(y_test, y_pred_tree)\nprint('l\\'erreure moyenne quadratique(MSE)  de l\\'arbre de decision:  ', mse_dt)\nprint('l\\'erreure moyenne absolue(MAE) de l\\'arbre de decision: ', mae_dt)\n\n##############################################","39b1a99c":"import matplotlib.pyplot as plt\n%matplotlib inline","b10ce394":"MSE=[mse_dt,mse_RF,mse_neural]\nMAE=[mae_dt,mae_RF,mae_neural]","b53f068d":"abssices=['Arbre de decission','Foret al\u00e9atoire','R\u00e9seau de neuronne profond']\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.set_title('MSE des algorithmes de l\\'AA')\nbar1=ax.bar(abssices,MSE,width=0.35, color='royalblue')\nfig.savefig('MSE')","a5c5a0af":"fig0= plt.figure()\nax2 = fig0.add_axes([0,0,1,1])\nax2.set_title('MAE des algorithmes de l\\'AA')\nbar2=ax2.bar(abssices,MAE,width=0.35, color='seagreen')\nfig0.savefig('MAE')","3539227f":"import seaborn as sns\nsns.displot(dataset['paddy.yield'],kde=True,kind='hist',bins=30)","5b68b4c1":"sns.pairplot(dataset,palette='coolwarm')","7d831a2b":"sns.violinplot(x=\"paddy.season\", y=\"paddy.yield\", data=dataset,palette='rainbow')\n","f40ebf27":"sns.countplot(x='paddy.season',data=dataset)","4cc3f0e8":"sns.barplot(x='paddy.season',y='paddy.yield',data=dataset)","8147488b":"dataset.corr()","7d7f0ac6":"sns.heatmap(dataset.corr())","2ad02719":"sns.lmplot(x='paddy.slevel',y='paddy.yield',data=dataset,scatter=False)","d7b092eb":"sns.lmplot(x='paddy.temp',y='paddy.yield',data=dataset,scatter=False)","fafaf6e4":"sns.lmplot(x='paddy.ph',y='paddy.yield',data=dataset,scatter=False)","03d146a4":"sns.set()","d0e4f70f":"figura, axes = plt.subplots(nrows=2,ncols=3,figsize=(12,8))","a351a5b6":"sns.regplot(ax=axes[0][0],x='paddy.slevel',y='paddy.yield',data=dataset,scatter=False)\nsns.regplot(ax=axes[0][1],x='paddy.temp',y='paddy.yield',data=dataset,scatter=False)\nsns.regplot(ax=axes[0][2],x='paddy.ph',y='paddy.yield',data=dataset,scatter=False)\nsns.regplot(ax=axes[1][0],x='paddy.season',y='paddy.yield',data=dataset,scatter=False)\nsns.regplot(ax=axes[1][1],x='paddy.humidity',y='paddy.yield',data=dataset,scatter=False)\nsns.regplot(ax=axes[1][2],x='paddy.variation',y='paddy.yield',data=dataset,scatter=False)\n","5284a7c5":"# **DATA PRE PROCESSING**","613f5627":"# **Random forrest**","32246d8a":"**SAUVGARDER LE MODELE**","bd5fb7b2":"# **AFFICHAGE summary CHANGEMENT DE PERTE ET DE LA PRECISION**","9ee97486":"# **nural network**","c11c3e12":"# **CREATION DU RESEAU DE NEURONE PROFOND**","6094bd4f":"# **PREDICTION DES RESULTATS**"}}