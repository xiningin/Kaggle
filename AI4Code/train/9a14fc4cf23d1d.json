{"cell_type":{"cb045b4d":"code","e85200c6":"code","1f289679":"code","178e41c1":"code","b630c079":"code","55cfe2f3":"code","f90a8d63":"code","0ffca073":"code","9671c762":"code","58b022e3":"code","2c9da5f5":"code","87657ba6":"code","a426b191":"code","d90da0d2":"code","1445f7f2":"code","ffd06106":"code","d58182fc":"code","8a0af2bf":"code","aecb0336":"code","dadd6a1d":"code","9076cd93":"markdown","3cf5ce1d":"markdown","54c7eff5":"markdown","85f45d80":"markdown","2bfea1dc":"markdown","3a423228":"markdown","2dc201f5":"markdown","e6aabf4b":"markdown"},"source":{"cb045b4d":"import pandas as pd\nimport os\nimport numpy as np\nimport pandas as pd\nimport zipfile\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport sys\nimport datetime","e85200c6":"#downloading weights and cofiguration file for the model\n!wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip","1f289679":"repo = 'model_repo'\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(repo)","178e41c1":"!ls 'model_repo\/uncased_L-12_H-768_A-12'","b630c079":"!ls 'model_repo'","55cfe2f3":"!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/optimization.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/run_squad.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/tokenization.py ","f90a8d63":"# Available pretrained model checkpoints:\n#   uncased_L-12_H-768_A-12: uncased BERT base model\n#   uncased_L-24_H-1024_A-16: uncased BERT large model\n#   cased_L-12_H-768_A-12: cased BERT large model\n#We will use the most basic of all of them\nBERT_MODEL = 'uncased_L-12_H-768_A-12'\nBERT_PRETRAINED_DIR = f'{repo}\/uncased_L-12_H-768_A-12'\nOUTPUT_DIR = f'{repo}\/outputs'\nprint(f'***** Model output directory: {OUTPUT_DIR} *****')\nprint(f'***** BERT pretrained directory: {BERT_PRETRAINED_DIR} *****')\n","0ffca073":"!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-development.tsv\n!ls","9671c762":"def is_whitespace(c):\n    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n        return True\n    return False\ndef get_squad_attributes(row):\n    paragraph_text = row['Text']\n    is_impossible = row['is_impossible'] \n    doc_tokens = []\n    char_to_word_offset = []\n    prev_is_whitespace = True\n    for c in paragraph_text:\n        if is_whitespace(c):\n            prev_is_whitespace = True\n        else:\n            if prev_is_whitespace:\n                doc_tokens.append(c)\n            else:\n                doc_tokens[-1] += c\n            prev_is_whitespace = False\n        char_to_word_offset.append(len(doc_tokens) - 1)\n        \n    if not is_impossible:\n        if row['A-coref']:\n            orig_answer_text = row['A']\n            answer_offset = row['A-offset']\n        else:\n            orig_answer_text = row['B']\n            answer_offset = row['B-offset']\n            \n        answer_length = len(orig_answer_text)\n        start_position = char_to_word_offset[answer_offset]\n        end_position = char_to_word_offset[answer_offset + answer_length -\n                                       1]\n        # Only add answers where the text can be exactly recovered from the\n        # document. If this CAN'T happen it's likely due to weird Unicode\n        # stuff so we will just skip the example.\n        #\n        # Note that this means for training mode, every example is NOT\n        # guaranteed to be preserved.\n        actual_text = \" \".join(\n            doc_tokens[start_position:(end_position + 1)])\n        cleaned_answer_text = \" \".join(\n            tokenization.whitespace_tokenize(orig_answer_text))\n\n        if actual_text.find(cleaned_answer_text) == -1:\n            tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n                                 actual_text, cleaned_answer_text)\n#             continue\n    else:\n        start_position = -1\n        end_position = -1\n        orig_answer_text = \"\"\n    return pd.Series({'doc_tokens':doc_tokens, \n                      'char_to_word_offset':char_to_word_offset,\n                      'orig_answer_text':orig_answer_text,\n                      'start_position': start_position,\n                      'end_position': end_position,\n                     })","58b022e3":"from sklearn.model_selection import train_test_split\nimport run_squad\nimport modeling\nimport optimization\nimport tokenization\nimport tensorflow as tf\n\ntrain_df =  pd.read_csv('gap-development.tsv', sep='\\t')\ntrain_df = train_df.sample(2000)\nprint(train_df.columns)\n# add is_impossible\ntrain_df['is_impossible'] = ~train_df['A-coref'] & ~train_df['B-coref']\n# add doc tokens\n# add start word number of answer text\n# add end word number of answer text\ntrain_df_full = train_df.merge(train_df.apply(lambda row: get_squad_attributes(row), axis=1),\n                             left_index=True, right_index=True)\n\ntrain, test = train_test_split(train_df_full, test_size = 0.3, random_state=42)\n\ntrain.head()","2c9da5f5":"import tensorflow as tf\nif tf.test.gpu_device_name():\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nelse:\n    print(\"Please install GPU version of TF\")","87657ba6":"\n\ndef create_squad_example(row):\n    return run_squad.SquadExample(\n        qas_id=row['ID'],\n        question_text=\"Which noun does \"+ row['Pronoun'] + \" refer to?\",\n        doc_tokens=row['doc_tokens'],\n        orig_answer_text=row['orig_answer_text'],\n        start_position=row['start_position'],\n        end_position=row['end_position'],\n        is_impossible=row['is_impossible']\n    )\n\n\ndef create_examples(rows, set_type):\n#Generate data for the BERT model\n    guid = f'{set_type}'\n    examples = []\n    if guid == 'train':\n        for row in rows:\n            examples.append(run_squad.SquadExample(\n                    qas_id=row['ID'], # ID\n                    question_text=question_text, #TBD\n                    doc_tokens=doc_tokens, #doc_tokens\n                    orig_answer_text=orig_answer_text, # \n                    start_position=start_position,\n                    end_position=end_position,\n                    is_impossible=is_impossible)\n                           )\n    else:\n        for line in lines:\n            text_a = line\n            label = '0'\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n# Model Hyper Parameters\nTRAIN_BATCH_SIZE = 32\nEVAL_BATCH_SIZE = 8\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 3.0\nWARMUP_PROPORTION = 0.1\nMAX_SEQ_LENGTH = 128\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 1000 #if you wish to finetune a model on a larger dataset, use larger interval\n# each checpoint weights about 1,5gb\nITERATIONS_PER_LOOP = 1000\nNUM_TPU_CORES = 8\nVOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\nINIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nDO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\ntrain_examples = train.apply(lambda row:create_squad_example(row), axis=1)\n\ntpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=OUTPUT_DIR,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=ITERATIONS_PER_LOOP,\n        num_shards=NUM_TPU_CORES,\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n\nnum_train_steps = int(\n    len(train_examples) \/ TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\nmodel_fn = run_squad.model_fn_builder(\n    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n    init_checkpoint=INIT_CHECKPOINT,\n    learning_rate=LEARNING_RATE,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_warmup_steps,\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n    use_one_hot_embeddings=True)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE,\n    predict_batch_size=EVAL_BATCH_SIZE,\n    eval_batch_size=EVAL_BATCH_SIZE)","a426b191":"\"\"\"\nNote: You might see a message 'Running train on CPU'. \nThis really just means that it's running on something other than a Cloud TPU, which includes a GPU.\n\"\"\"\n!mkdir model_repo\/outputs\n\n# Train the model.\nprint('Please wait...')\ntrain_writer = run_squad.FeatureWriter(\n        filename=os.path.join(OUTPUT_DIR, \"train.tf_record\"),\n        is_training=True)\ntrain_features = run_squad.convert_examples_to_features(\n    train_examples, tokenizer, MAX_SEQ_LENGTH, 128, 64, True, train_writer.process_feature)\ntrain_writer.close()\nprint('***** Started training at {} *****'.format(datetime.datetime.now()))\nprint('  Num examples = {}'.format(len(train_examples)))\nprint('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\ntf.logging.info(\"  Num steps = %d\", num_train_steps)\ntrain_input_fn = run_squad.input_fn_builder(\n    input_file=train_writer.filename,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=True)\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint('***** Finished training at {} *****'.format(datetime.datetime.now()))","d90da0d2":"def create_test_squad_example(row):\n    return run_squad.SquadExample(\n        qas_id=row['ID'],\n        question_text=\"Which noun does \"+ row['Pronoun'] + \" refer to?\",\n        doc_tokens=row['doc_tokens'],\n        orig_answer_text=\"\",\n        start_position=-1,\n        end_position=-1,\n        is_impossible=row['is_impossible']\n    )\n\neval_examples = test.apply(lambda row:create_test_squad_example(row), axis=1)\n\neval_writer = run_squad.FeatureWriter(\n    filename=os.path.join(OUTPUT_DIR, \"eval.tf_record\"),\n    is_training=False)\n\neval_features = []\n\ndef append_feature(feature):\n    eval_features.append(feature)\n    eval_writer.process_feature(feature)\n    \nrun_squad.convert_examples_to_features(\n        examples=predict_examples,\n        tokenizer=tokenizer,\n        max_seq_length=MAX_SEQ_LENGTH,\n        doc_stride=128,\n        max_query_length=64,\n        is_training=False,\n        output_fn=append_feature)\n\neval_writer.close()\n\ntf.logging.info(\"***** Running predictions *****\")\ntf.logging.info(\"  Num orig examples = %d\", len(predict_examples))\ntf.logging.info(\"  Num features = %d\", len(eval_features))\n\npredict_input_fn = run_squad.input_fn_builder(\n    input_file=eval_writer.filename,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)","1445f7f2":"all_results = []\nfor result in estimator.predict(predict_input_fn, yield_single_examples=True):\n    if len(all_results) % 1000 == 0:\n        tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n    unique_id = int(result[\"unique_ids\"])\n    start_logits = [float(x) for x in result[\"start_logits\"].flat]\n    end_logits = [float(x) for x in result[\"end_logits\"].flat]\n    all_results.append(\n          run_squad.RawResult(\n              unique_id=unique_id,\n              start_logits=start_logits,\n              end_logits=end_logits))\n\noutput_prediction_file = os.path.join(OUTPUT_DIR, \"predictions.json\")\noutput_nbest_file = os.path.join(OUTPUT_DIR, \"nbest_predictions.json\")\noutput_null_log_odds_file = os.path.join(OUTPUT_DIR, \"null_odds.json\")\n","ffd06106":"\ndef write_predictions(all_examples, all_features, all_results, n_best_size,\n                      max_answer_length, do_lower_case, output_prediction_file,\n                      output_nbest_file, output_null_log_odds_file):\n  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n  tf.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n  tf.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n\n  example_index_to_features = collections.defaultdict(list)\n  for feature in all_features:\n    example_index_to_features[feature.example_index].append(feature)\n\n  unique_id_to_result = {}\n  for result in all_results:\n    unique_id_to_result[result.unique_id] = result\n\n  _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n      \"PrelimPrediction\",\n      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n\n  all_predictions = collections.OrderedDict()\n  all_nbest_json = collections.OrderedDict()\n  scores_diff_json = collections.OrderedDict()\n\n  for (example_index, example) in enumerate(all_examples):\n    features = example_index_to_features[example_index]\n\n    prelim_predictions = []\n    # keep track of the minimum score of null start+end of position 0\n    score_null = 1000000  # large and positive\n    min_null_feature_index = 0  # the paragraph slice with min mull score\n    null_start_logit = 0  # the start logit at the slice with min null score\n    null_end_logit = 0  # the end logit at the slice with min null score\n    for (feature_index, feature) in enumerate(features):\n      result = unique_id_to_result[feature.unique_id]\n      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n      # if we could have irrelevant answers, get the min score of irrelevant\n      for start_index in start_indexes:\n        for end_index in end_indexes:\n          # We could hypothetically create invalid predictions, e.g., predict\n          # that the start of the span is in the question. We throw out all\n          # invalid predictions.\n          if start_index >= len(feature.tokens):\n            continue\n          if end_index >= len(feature.tokens):\n            continue\n          if start_index not in feature.token_to_orig_map:\n            continue\n          if end_index not in feature.token_to_orig_map:\n            continue\n          if not feature.token_is_max_context.get(start_index, False):\n            continue\n          if end_index < start_index:\n            continue\n          length = end_index - start_index + 1\n          if length > max_answer_length:\n            continue\n          prelim_predictions.append(\n              _PrelimPrediction(\n                  feature_index=feature_index,\n                  start_index=start_index,\n                  end_index=end_index,\n                  start_logit=result.start_logits[start_index],\n                  end_logit=result.end_logits[end_index]))\n\n    prelim_predictions = sorted(\n        prelim_predictions,\n        key=lambda x: (x.start_logit + x.end_logit),\n        reverse=True)\n\n    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n\n    seen_predictions = {}\n    nbest = []\n    for pred in prelim_predictions:\n      if len(nbest) >= n_best_size:\n        break\n      feature = features[pred.feature_index]\n      if pred.start_index > 0:  # this is a non-null prediction\n        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n        tok_text = \" \".join(tok_tokens)\n\n        # De-tokenize WordPieces that have been split off.\n        tok_text = tok_text.replace(\" ##\", \"\")\n        tok_text = tok_text.replace(\"##\", \"\")\n\n        # Clean whitespace\n        tok_text = tok_text.strip()\n        tok_text = \" \".join(tok_text.split())\n        orig_text = \" \".join(orig_tokens)\n\n        final_text = run_squad.get_final_text(tok_text, orig_text, do_lower_case)\n        if final_text in seen_predictions:\n          continue\n\n        seen_predictions[final_text] = True\n      else:\n        final_text = \"\"\n        seen_predictions[final_text] = True\n\n      nbest.append(\n          _NbestPrediction(\n              text=final_text,\n              start_logit=pred.start_logit,\n              end_logit=pred.end_logit))\n\n    # In very rare edge cases we could have no valid predictions. So we\n    # just create a nonce prediction in this case to avoid failure.\n    if not nbest:\n      nbest.append(\n          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n\n    assert len(nbest) >= 1\n\n    total_scores = []\n    best_non_null_entry = None\n    for entry in nbest:\n      total_scores.append(entry.start_logit + entry.end_logit)\n      if not best_non_null_entry:\n        if entry.text:\n          best_non_null_entry = entry\n\n    probs = run_squad._compute_softmax(total_scores)\n\n    nbest_json = []\n    for (i, entry) in enumerate(nbest):\n      output = collections.OrderedDict()\n      output[\"text\"] = entry.text\n      output[\"probability\"] = probs[i]\n      output[\"start_logit\"] = entry.start_logit\n      output[\"end_logit\"] = entry.end_logit\n      nbest_json.append(output)\n\n    assert len(nbest_json) >= 1\n\n    all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n\n    all_nbest_json[example.qas_id] = nbest_json\n\n  with tf.gfile.GFile(output_prediction_file, \"w\") as writer:\n    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n\n  with tf.gfile.GFile(output_nbest_file, \"w\") as writer:\n    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n    \n\ndef _get_best_indexes(logits, n_best_size):\n  \"\"\"Get the n-best logits from a list.\"\"\"\n  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n  best_indexes = []\n  for i in range(len(index_and_score)):\n    if i >= n_best_size:\n      break\n    best_indexes.append(index_and_score[i][0])\n  return best_indexes\n\n","d58182fc":"import collections\nimport json\n\nwrite_predictions(eval_examples, eval_features, all_results,\n                      20, 30,\n                      True, output_prediction_file,\n                      output_nbest_file, output_null_log_odds_file)\n\n","8a0af2bf":"train_df_full[train_df_full.ID == \"development-1446\"]","aecb0336":"!head model_repo\/outputs\/predictions.json","dadd6a1d":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nprint(accuracy_score(np.array(results), test_labels))\nprint(f1_score(np.array(results), test_labels))","9076cd93":"<img src=\"https:\/\/image.ibb.co\/gGCZ0A\/image.jpg\" alt=\"image\" border=\"0\">","3cf5ce1d":"There are several downsides for BERT at this moment:\n\n- Training is expensive. All results on the paper were fine-tuned on a single Cloud TPU, which has 64GB of RAM. It is currently not possible to re-produce most of the BERT-Large results on the paper using a GPU with 12GB - 16GB of RAM, because the maximum batch size that can fit in memory is too small. \n\n- At the moment BERT supports only English, though addition of other languages is expected.\n\n","54c7eff5":"Example below is done on preprocessing code, similar to **SQUAD**:","85f45d80":"# Competition test\n","2bfea1dc":"I've run a test with 30% of Quora data on free colab cloud TPU and achieved f1 score of 0.684.(11th place at the moment)\n\n**You can't use BERT in the competition, the notebook will fail when it comes to real testing.\nI apologies for littering the leaderboard, but I couldn't believe that local test where so high, I had to check.**\n\nTraining took about 30-40 minutes.\nResults are really amazing, espetially because it's a raw model on random 30% sample with no optimization or ensamble, using the simlest of 3 released models.\nI didn't even have to preprocess anything, model does it for you.\n","3a423228":"# Downloading all necessary dependencies\nYou will have to turn on internet for that.\n\nThis code is slightly modefied version of this colab notebook https:\/\/colab.research.google.com\/github\/tensorflow\/tpu\/blob\/master\/tools\/colab\/bert_finetuning_with_cloud_tpus.ipynb","2dc201f5":"#                                                                                 BERT\n\nBERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.\n\nAcademic paper which describes BERT in detail and provides full results on a number of tasks can be found here: https:\/\/arxiv.org\/abs\/1810.04805.\n\nGithub account for the paper can be found here: https:\/\/github.com\/google-research\/bert\n\nBERT is a method of pre-training language representations, meaning training of a general-purpose \"language understanding\" model on a large text corpus (like Wikipedia), and then using that model for downstream NLP tasks (like question answering). BERT outperforms previous methods because it is the first *unsupervised, deeply bidirectional *system for pre-training NLP.","e6aabf4b":"![](https:\/\/www.lyrn.ai\/wp-content\/uploads\/2018\/11\/transformer.png)"}}