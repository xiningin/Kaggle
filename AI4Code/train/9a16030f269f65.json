{"cell_type":{"8abc60f4":"code","7bacda16":"code","6041a57b":"code","4617fe83":"code","eaafea05":"code","6328d72e":"code","e2375356":"code","2c46c1bf":"code","ae6d4e53":"code","ea8bc363":"code","2f1fcde8":"code","f14fae20":"code","b95805af":"code","0919a86b":"code","375b4aee":"code","aa31e6a5":"markdown","c3232ca1":"markdown","39fe3506":"markdown","65d6dcbc":"markdown","f6a6e788":"markdown","488c4b3c":"markdown","13926e39":"markdown","45916b75":"markdown","19bc5180":"markdown"},"source":{"8abc60f4":"from sklearn.datasets import fetch_20newsgroups\nfrom nltk.tokenize import word_tokenize #Used to extract words from documents\nfrom nltk.stem import WordNetLemmatizer #Used to lemmatize words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans\n\nimport sys\nfrom time import time\n\nimport pandas as pd\nimport numpy as np","7bacda16":"# Selected 3 categories from the 20 newsgroups dataset\n\ncategories = [\n    'talk.religion.misc',\n    'comp.graphics',\n    'sci.space',\n]\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)","6041a57b":"df = fetch_20newsgroups(subset='all', categories=categories, \n                             shuffle=False, remove=('headers', 'footers', 'quotes'))","4617fe83":"labels = df.target\ntrue_k = len(np.unique(labels)) ## This should be 3 in this example\nprint(true_k)","eaafea05":"lemmatizer = WordNetLemmatizer()\nfor i in range(len(df.data)):\n    word_list = word_tokenize(df.data[i])\n    lemmatized_doc = \"\"\n    for word in word_list:\n        lemmatized_doc = lemmatized_doc + \" \" + lemmatizer.lemmatize(word)\n    df.data[i] = lemmatized_doc  ","6328d72e":"print(df.data[1])","e2375356":"vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', min_df=2) ## Corpus is in English\nX = vectorizer.fit_transform(df.data)","2c46c1bf":"print(X.shape)","ae6d4e53":"km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100)\nt0 = time()\nkm.fit(X)\nprint(\"done in %0.3fs\" % (time() - t0))","ea8bc363":"print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\nprint(\"Adjusted Rand-Index: %.3f\"\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, km.labels_, sample_size=1000))","2f1fcde8":"centroids = km.cluster_centers_.argsort()[:, ::-1] ## Indices of largest centroids' entries in descending order\nterms = vectorizer.get_feature_names()\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i, end='')\n    for ind in centroids[i, :10]:\n        print(' %s' % terms[ind], end='')\n    print()","f14fae20":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt","b95805af":"def frequencies_dict(cluster_index):\n    if cluster_index > true_k - 1:\n        return\n    term_frequencies = km.cluster_centers_[cluster_index]\n    sorted_terms = centroids[cluster_index]\n    frequencies = {terms[i]: term_frequencies[i] for i in sorted_terms}\n    return frequencies","0919a86b":"def makeImage(frequencies):\n\n    wc = WordCloud(background_color=\"white\", max_words=50)\n    # generate word cloud\n    wc.generate_from_frequencies(frequencies)\n\n    # show\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","375b4aee":"for i in range(true_k):\n    freq = frequencies_dict(i)\n    makeImage(freq)\n    print()","aa31e6a5":"### Visualization","c3232ca1":"In this notebook, it explained that the case of text document clustering , showing how the `scikit-learn` package can be used to perform clustering. Doing this, you will review:\n1. How a document is converted into a vector of features. In particular, it considered that tf-idf vectorization.\n2. How k-means clustering can be applied to perform unsupervised clustering of the documents.\n3. How this is done in practice using the `scikit-learn` package.","39fe3506":"We next convert our corpus into tf-idf vectors. We remove common stop words, terms with very low document frequency (many of them are numbers or misspells), accents. ","65d6dcbc":"### Perform Lemmatization","f6a6e788":"### Identify the 10 most relevant terms in each cluster","488c4b3c":"<h1 style=\"background-color:#a83299;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 20px 20px;font-family:cursive\">Clustering text documents using k-means<\/h1>","13926e39":"### Clustering using standard k-means","45916b75":"We first cluster documents using the standard k-means algorithm (actually, a refined variant called k-means++), without any further date preprocessing. The key parameter of choice when performing k-means is $k$. Alas, there really is no principled way to choose an initial value for $k$. Essentially we have two options:\n\n1. We choose a value that reflects our knowledge about the data, as in this case\n2. We may try several value, possibly in increasing order. We proceed this way as long as the quality of the resulting clustering (as measured by one or more quality indices) increases and stop when it starts decreasing. \n\nIn this specific case, we set $k = 3$ of course","19bc5180":"### Standard measures of cluster quality"}}