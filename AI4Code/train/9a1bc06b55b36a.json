{"cell_type":{"163b25f1":"code","6ed17871":"code","93601ee0":"code","9673caff":"code","7bb9edf9":"code","ce5ef441":"code","1b7492b8":"code","58ab2b31":"code","0ebc5bef":"code","cdfe57fc":"code","2d126ad0":"code","f5087c83":"code","efbe6c84":"markdown","b97fcb3b":"markdown","3fa67334":"markdown","3891eb03":"markdown","faad4cd1":"markdown","c07eb6ff":"markdown","477c4efb":"markdown","b005629c":"markdown","987b48d7":"markdown"},"source":{"163b25f1":"import numpy as np\nimport pandas as pd\nimport spacy as sp\n\nnlp_lg = sp.load('en_core_web_lg')\nnlp_lg","6ed17871":"# get spacy vector\nlgword = nlp_lg(\"and\")\nlgvec =   \",\".join(lgword.vector[0:10].round(5).astype(str))\n\n# get glove vector\nglv = pd.read_csv('..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt', header=None, sep=' ', skiprows=2, nrows=5, index_col=[0])\nglvec = glv.loc['and', 0:10].round(5).astype(str).str.cat(sep=' ')\n\nprint(lgword.vector.shape[0], \"\\n\",\n      lgvec, \"\\n\",\n      glv.shape[1], \"\\n\",\n      glvec)","93601ee0":"nlp_sm = sp.load('en_core_web_sm')\nsmword = nlp_sm(\"and\")\nsmvec = \",\".join(smword.vector[0:10].round(5).astype(str))\n\nprint(smword.vector.shape[0], \"\\n\",\n       smvec)","9673caff":"e = nlp_lg('Why are aliens so smart?')\n\nprint(e.vector.shape, \"\\n\",\n       e.vector[0:10])","7bb9edf9":"c = nlp_sm('What capital city is the prettiest?') \nd = nlp_sm('Which country has the nicest people?')\ne = nlp_sm('Why are aliens so smart?')\n\nprint(\"\\n\", c.similarity(d),\n        c.similarity(e))","ce5ef441":"df = pd.DataFrame({\"text\": [tokens.text for tokens in d], \n                   \"lemmatized\": [tokens.lemma_ for tokens in d],\n                   \"part of speech\": [tokens.pos_ for tokens in d],\n                  \"stop word\": [tokens.is_stop for tokens in d]})\ndisplay(df)                 \nsp.displacy.render(d, style='dep', jupyter=True, options={'compact':60})","1b7492b8":"#%% import\nimport time\nimport numpy as np\nimport pandas as pd\nimport spacy as sp\nnlp_lg = sp.load('en_core_web_lg')\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom tqdm import tqdm\n\n\n# get train data\ntrain = pd.read_csv('..\/input\/train.csv', nrows=30_000)  #limiting the data for time's sake\ntrain['question_text'] = train.question_text.str.replace('?', ' ?')\ntrain['question_text'] = train.question_text.str.replace('.', ' .')","58ab2b31":"tstacked = pd.DataFrame(train.question_text.str.split(expand=True).stack(), \n                columns=['token'])\n\ntlist = tstacked.token.unique().tolist()\nvlist = [nlp_lg(str).vector for str in tqdm(tlist)]\nlookup = dict(zip(tlist, vlist))\n\ntstacked['vec'] = tstacked.token.map(lookup)\n\ncolnames = ['t'+str(i) for i in range(300)]\ntstacked[colnames] = pd.DataFrame(tstacked.vec.values.tolist(), \n                            index=tstacked.index)\ntstacked.drop(['token', 'vec'], axis=1, inplace=True)\n\ndel tlist\ndel vlist\ndel lookup\ntagg = tstacked.groupby(level=0).apply(np.mean)\ndel tstacked\n\nX_vecs = tagg.values\ny = train.target.values\ndel tagg","0ebc5bef":"# Logistic Regression\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=911)\ntrain_pred = np.zeros(train.shape[0])\nfor train_idx, val_idx in skf.split(X_vecs, y):\n    X_train, y_train  = X_vecs[train_idx], y[train_idx]\n    X_val, y_val = X_vecs[val_idx], y[val_idx]\n    model = LogisticRegression(solver='saga', class_weight='balanced', \n                                    C=0.5, max_iter=250, verbose=1, n_jobs=-1) #seed not set\n    model.fit(X_train, y_train)\n    val_pred = model.predict_proba(X_val)\n    train_pred[val_idx] = val_pred[:,1]\n    \n\nprint(\"finding best threshold\")\nbest_thresh = 0.0\nbest_score = 0.0\nfor thresh in np.arange(0, 1, 0.01):\n    score = f1_score(y, train_pred > thresh)\n    if score > best_score:\n        best_thresh = thresh\n        best_score = score\nprint(best_thresh, best_score)","cdfe57fc":"print(best_thresh, best_score)","2d126ad0":"# predict on test set\ntest = pd.read_csv('..\/input\/test.csv', index_col=['qid'])\ntest.head()\nX_test = test.question_text.tolist()\nX_testvecs = np.array([nlp_lg(text).vector for text in tqdm(X_test)])\n\ntrounds = 3\npreds_test = np.zeros(len(X_test))\nfor i in range(trounds):\n    model = LogisticRegression(solver='saga', class_weight='balanced', \n                                    C=0.5, max_iter=250, verbose=1, n_jobs=-1, random_state=40*i)\n    model.fit(X_vecs, y)\n    preds_test += lgr.predict_proba(X_testvecs)[:, 1] \/ trounds\n\n    \n# submit\nsub = pd.read_csv('..\/input\/sample_submission.csv', index_col=['qid'])\nsub['prediction'] = preds_test > best_thresh\nsub.to_csv('submission.csv')","f5087c83":"#%% import\nimport numpy as np\nimport pandas as pd\nimport spacy as sp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n\n# get data and make json format for spacy\ntrain = pd.read_csv('..\/input\/train.csv', nrows=10_000)  ## using part of the data again\ntexts = train.question_text.tolist()\ncats = train.target.apply(lambda t: {'cats': {'Insincere': t == 1}}).tolist()\ntrain_texts, dev_texts, train_cats, dev_cats = train_test_split(texts, cats, \n        test_size=0.2, random_state=90)\ntrain_data = list(zip(train_texts, train_cats))\nprint(\"Example format \\n\", train_data[0:10])\n\n\n#%% set up the pipeline\nnlp_bl = sp.blank('en') \nnlp_bl.vocab.vectors.name = 'spacy_pretrained_vectors'\ntextcat = nlp_bl.create_pipe('textcat')\nnlp_bl.add_pipe(textcat, last=True)\ntextcat.add_label('Insincere')\n\n\n# train\nn_iter = 10\nother_pipes = [pipe for pipe in nlp_bl.pipe_names if pipe != 'textcat']\nwith nlp_bl.disable_pipes(*other_pipes):  #only train textcat\n    optimizer = nlp_bl.begin_training()\n    print(\"Training the model...\")\n    for i in range(n_iter):\n        losses = {}\n        batches = sp.util.minibatch(train_data, size=sp.util.compounding(4., 32., 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp_bl.update(texts, annotations, sgd=optimizer, drop=0.2,\n                        losses=losses)\n        print(\"iter {} loss: {:4f}\".format(i, losses['textcat']))\n\n        \n# evaluate model\npreds = []\ndocs = (nlp_bl(text) for text in dev_texts)\nfor doc in docs:\n    pred = doc.cats['Insincere']\n    preds.append(pred)\n    \ntruths = [val['Insincere'] for val in [dc['cats'] for dc in dev_cats]]\n\n#%% find best threshold\nbest_thresh = 0.0\nbest_score = 0.0\nfor thresh in np.arange(0, 1, 0.01):\n    score = f1_score(truths, preds > thresh)\n    if score > best_score:\n        best_thresh = thresh\n        best_score = score\nprint(best_thresh, best_score)","efbe6c84":"## Language Features\n\nspaCy has a host of other language features. You can use a built-in similarity function to compare questions. If I remember correctly, it's a shorthand function for cosine similarity.","b97fcb3b":"Vectors are the same for the word \"and\" as well as other words I checked. Oh well, no new information here. \n\nLet's check the small model, which \"only includes context-sensitive tensors\". The docs say that the small models don't work as well. Maybe they can be helpful anyway as an additional source of information. ","3fa67334":"The GloVe vector and spaCy vector (or rank1 tensor if you insist) are indeed different. The model may be a useful addition to other vectors.\n\nspaCy will also calculate vectors for an entire question. The model tokenizes the string according to its own rules, gets vectors for each word, and averages them to get a single vector. ","3891eb03":"## A Simple Model\nHere's a simple model to get average vectors for each question and train a logistic regression model. The vectors are the same as the GloVe vectors we're given, except there are fewer words available. \n\nCalculating vectors for each question is time consuming. It's 4-5 times faster to get vectors for each unique token and manually average them.","faad4cd1":"Every once in a while I mess around with [spaCy](https:\/\/spacy.io\/) to see what it can do. It comes with a rich set of features, including it's own pretrained language models. Some of these models include word embeddings like the ones we're given. A spaCy model might be useful here as a way to bring in additional vectors and dictionaries. Let's see what we have.","c07eb6ff":"## spaCy Vectors\n\nFirst let's look at spaCy's \"large model\". The documentation says the model uses GloVe vectors trained on Common Crawl. We are already given the 300d vectors as a text file. I'll compare a vector from spaCy with a vector in the text file to see if there's a difference.","477c4efb":"This is a basic model trained on part of the data. So far the results have not been as good as logistic regression with tf-idf features. I think using the other annotations (parts of speech, etc.) as meta-features might be the best way to use spaCy.\n\nAlternately, you can get vectors for each word in a question and assemble them for a Keras model. See https:\/\/www.kaggle.com\/enerrio\/scary-nlp-with-spacy-and-keras for an example.\n\n\n\n## spaCy's CNN\nspaCy also has it's own CNN for text classification. I haven't dug into it very much, but it seems to work at a basic level. Here is an example of how to format the data and train a classifier from scratch. You can also run the code (with modifications) on a GPU for better speed. ","b005629c":"The model can also lemmatize, assign parts of speech, find dependencies and otherwise annotate text.","987b48d7":"Again, this model needs to run longer on more data to seee what it can do. Hope to see some clever uses of spaCy in other kernels!"}}