{"cell_type":{"bf054b2e":"code","fdfaea41":"code","a17d0757":"code","2a437d92":"code","8b97c4fe":"code","da2ffd8f":"code","684ae7f8":"code","3f9d19f9":"code","85472b75":"code","dfbb2a3a":"code","7548f380":"code","6defd676":"code","842cbf55":"code","8b808ca5":"markdown","e6194d87":"markdown","0e5c3407":"markdown","4dea0d2c":"markdown","a88371ff":"markdown","cc8cef16":"markdown","81b51a03":"markdown","bcfa2068":"markdown"},"source":{"bf054b2e":"import time\nimport gc\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom numba import jit","fdfaea41":"N_SPLITS = 4","a17d0757":"src_data_path = \"..\/input\/porto-seguro-safe-driver-prediction\"\ndf_train = pd.read_csv(os.path.join(src_data_path, \"train.csv\"))\ndf_test = pd.read_csv(os.path.join(src_data_path, \"test.csv\"))","2a437d92":"id_test = df_test['id'].values\ntarget_train = df_train['target'].values\n\ndf_train = df_train.drop(['target', 'id'], axis=1)\ndf_test = df_test.drop(['id'], axis=1)","8b97c4fe":"import missingno as msno\ndf_copy = df_train.copy()\ndf_copy = df_copy.replace(-1, np.nan)\n\n# print in order of number of NaN.\nprint(df_copy.count().sort_values()[0:5])\nmsno.matrix(df=df_copy.iloc[:, 2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))","da2ffd8f":"incomplete_columns = ['ps_car_03_cat', 'ps_car_05_cat']\ndf_train = df_train.drop(incomplete_columns, axis=1)\ndf_test = df_test.drop(incomplete_columns, axis=1)","684ae7f8":"df_train = df_train.replace(-1, np.nan)\ndf_test = df_test.replace(-1, np.nan)","3f9d19f9":"cat_features = [a for a in df_train.columns if a.endswith('cat')]\n\ndef preprocess_cat_feature(df, cat_feats):\n    for column in cat_feats:\n        temp = pd.get_dummies(pd.Series(df[column]), prefix=column)        \n        df = pd.concat([df, temp],axis=1)\n        df = df.drop([column],axis=1)\n    return df\n\ndf_train = preprocess_cat_feature(df_train, cat_features)\ndf_test = preprocess_cat_feature(df_test, cat_features)","85472b75":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# check feature importances\nmodel = lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.1, max_depth=-1, min_data_in_leaf = 1, min_sum_hessian_in_leaf = 1.0)\n\nX = df_train\ny = target_train\nX = X.replace(-1, np.nan)\nmodel.fit(X, y)\n\nfeatures_imp = pd.DataFrame(sorted(zip(model.feature_importances_, X.columns)), columns=[\"Value\", \"Feature\"])\nplt.figure(figsize=(16, 50))\nsns.barplot(x=\"Value\", y=\"Feature\", data=features_imp.sort_values(by=\"Value\", ascending=False))","dfbb2a3a":"sorted_columns_by_fi = sorted(zip(model.feature_importances_, X.columns))\ncol_to_drop = [x[1] for x in sorted_columns_by_fi if x[0] < 100]\n\ndf_train = df_train.drop(col_to_drop, axis=1)\ndf_test = df_test.drop(col_to_drop, axis=1)","7548f380":"# col_to_drop = df_train.columns[df_train.columns.str.startswith('ps_calc_')]\n# df_train = df_train.drop(col_to_drop, axis=1)\n# df_test = df_test.drop(col_to_drop, axis=1)","6defd676":"import warnings\nfrom sklearn.linear_model import LogisticRegression\n\n\nclass model_builder(object):\n    def __init__(self, n_splits):\n        self.n_splits = n_splits\n        self.stacker = LogisticRegression()\n        pass\n    \n    def fit_predict(self, X, y, T, params_list):\n    \n        # shape\ub9cc \uac00\uc838\uc628\ub2e4.\n        y_valid_pred = 0 * y\n        y_submit_pred = 0\n        \n        X = np.array(X)\n        y = np.array(y)\n        T = np.array(T)\n        \n        stack_train = np.zeros((X.shape[0], len(params_list)))\n        stack_test = np.zeros((T.shape[0], len(params_list)))\n\n        for param_index, params in enumerate(params_list):\n            # set up folds\n            kfold = KFold(n_splits = self.n_splits, random_state = 1, shuffle = True)\n\n            model = lgb.LGBMClassifier(**params)\n            \n            stack_test_i = np.zeros((T.shape[0], self.n_splits))            \n\n            for fold_index, (train_index, val_index) in enumerate(kfold.split(X)):                \n                print(\"model \", param_index, \" fold \", fold_index)\n\n                # create data for this fold\n                X_train = X[train_index]\n                y_train = y[train_index]\n                X_valid = X[val_index]\n\n                model.fit(X_train, y_train, verbose=True)\n\n                # inference validation data with trained model\n                # this will be used as train data for ensemble model(stacker)\n                y_pred = model.predict_proba(X_valid)[:, 1]\n                stack_train[val_index, param_index] = y_pred\n                \n                # inference submit data with trained model\n                # this will be used as input of ensemble model(stacker)\n                pred = model.predict_proba(T)\n                stack_test_i[:, fold_index] = pred[:, 1]\n            stack_test[:, param_index] = stack_test_i.mean(axis=1) # fold model \ud3c9\uade0\uac12\uc744 \uc800\uc7a5\n        \n        # train ensemble model with stacked train data([?, len(params)])\n        self.stacker.fit(stack_train, y)\n        \n        # inference with stacked submit data\n        pred = self.stacker.predict_proba(stack_test)[:, 1]\n        return pred\n\n\nparams_list = [\n    {\n        'learning_rate' : 0.02,\n        'n_estimators' : 650,\n        'max_bin' : 10,\n        'subsample' : 0.8,\n        'subsample_freq' : 10,\n        'colsample_bytree' : 0.8,\n        'min_child_samples' : 500,\n        'seed' : 99\n    },\n    {\n        'learning_rate' : 0.02,\n        'n_estimators' : 1100,        \n        'subsample' : 0.7,\n        'subsample_freq' : 2,\n        'colsample_bytree' : 0.3,        \n        'num_leaves' : 16,\n        'seed' : 99\n    },\n    {\n        'n_estimators' : 1100,\n        'max_depth' : 4,\n        'learning_rate' : 0.02,\n        'seed' : 99\n    },\n]\n\nbuilder = model_builder(n_splits=N_SPLITS)\ny_test_pred = builder.fit_predict(X=df_train, y=target_train, T=df_test, params_list=params_list)","842cbf55":"sub = pd.DataFrame()\nsub['id'] = id_test\nsub['target'] = y_test_pred\nsub.to_csv('lgb_submit.csv', float_format='%.6f', index=False)","8b808ca5":"## Train + Predict\n\nCreate models with different sets of hyperparameters and use K-Fold cross validation for each model. Inferences from valid fold is used to train the ensemble model.<br>\nSubmit data also stack inference result for each model and pass to ensemble model.<br>\n<br>\nOverall process can be seen in the image below.\n\n\n<img src='http:\/\/drive.google.com\/uc?export=view&id=1EI7Nt-TjbeL0hYvD-wugl0AyJPRgUU1m' \/><br>\n<br>\n\nshape of train data for ensemble model:<br>\n\n    (len(train_data) , len(models))\n\nshape of submit data for ensemble model:<br>\n    \n    (len(submit_data) , len(models))","e6194d87":"## Preprocessing\n\nLoad Data","0e5c3407":"Remove columns data is too much missing.","4dea0d2c":"## Porto Seguro\u2019s Safe Driver Prediction : LightGBM with stacking\n\n\nFrom : https:\/\/www.kaggle.com\/yekenot\/simple-stacker-lb-0-284   ---  very helpful!\n\n","a88371ff":"Most of other notebooks drop columns starts with 'ps_calc_' and get good result.<br>\nI don't know why dropping 'ps_calc_XXX' columns results in better result.<br>\nMaybe there's a problem of calculating feature importances.<br>","cc8cef16":"Check missing data(-1 means data is missing.)","81b51a03":"Calculate feature importance and drop less-important features.","bcfa2068":"Do one-hot encoding conversion on each categorical features(features name ends with 'cat')"}}