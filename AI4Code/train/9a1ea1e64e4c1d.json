{"cell_type":{"a66d6e77":"code","8b826be8":"code","904e2500":"code","297cc742":"code","ee0ed382":"code","05533eef":"code","928d64a5":"code","5ecf8618":"code","ee89704a":"code","8a4f7526":"code","0a72f670":"code","6bdb9e09":"code","e49028a3":"code","59c72f47":"code","82eee578":"code","209e50a0":"code","f2768fdf":"code","61b36421":"code","b1ead781":"code","ba115069":"code","5cb3ec8d":"code","ecf0f830":"code","bd245700":"code","3a21846c":"code","0d1a7533":"code","c8095031":"code","5508cbf2":"code","90f19b46":"code","13564eaf":"code","4657cb89":"code","666ecc2c":"code","6b096904":"code","9552c0e0":"code","b5fef043":"code","63f80043":"code","31d26423":"code","479c9c63":"code","9f1ba0fb":"code","4632993d":"markdown","3097b81c":"markdown","c12a4213":"markdown","4c477ede":"markdown","b7abf434":"markdown"},"source":{"a66d6e77":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","8b826be8":"import tensorflow\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding,Bidirectional,LSTM,Dense,Dropout\nfrom tensorflow.keras.utils import to_categorical","904e2500":"df=pd.read_csv('..\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Reddit_Data.csv')\ndf.head(5)","297cc742":"dist=list(df.category)\npp=[0,0,0]\nfor i in dist:\n    if i==-1:\n        pp[0]+=1\n    elif i==0:\n        pp[1]+=1\n    else:\n        pp[2]+=1\nprint(pp)\n        \n","ee0ed382":"labels=['Negative','Neutral','Positive']\nsns.barplot(x=labels,y=pp)\nplt.show()","05533eef":"comment=list(df.clean_comment.astype(str))\nsentiment=list(df.category)\nreddit_dict=dict(zip(comment,sentiment))","928d64a5":"print(list(reddit_dict.items())[:5])","5ecf8618":"Neg_list=[]\nPos_list=[]\nNeutral_list=[]\nfor i,j in reddit_dict.items():\n    if j==-1:\n        Neg_list.append(i)\n    elif j==0:\n        Neutral_list.append(i)\n    else:\n        Pos_list.append(i)    ","ee89704a":"print(Neg_list[:2],'\\n',Neutral_list[:2],'\\n',Pos_list[:2])","8a4f7526":"pos_len=[]\nfor i in Pos_list:\n    pos_len.append(len(i))","0a72f670":"neg_len=[]\nfor i in Neg_list:\n    neg_len.append(len(i))","6bdb9e09":"Neutral_len=[]\nfor i in Neutral_list:\n    Neutral_len.append(len(i))","e49028a3":"plt.subplots(figsize=(20,8))\nplt.title(\"Word Length Variation\")\nplt.plot(Neutral_len[:250],c='b',label='Neutral')\nplt.plot(neg_len[:250],c='r',label='Negative')\nplt.plot(pos_len[:250],c='g',label='Positive')\nplt.legend(loc='upper left')\nplt.show()","59c72f47":"pos_mean=sum(pos_len)\/\/len(pos_len)\nneg_mean=sum(neg_len)\/\/len(neg_len)\nneutral_mean=sum(Neutral_len)\/\/len(Neutral_len)\ncombined_mean=(sum(pos_len)+sum(neg_len)+sum(Neutral_len))\/\/(len(pos_len)+len(neg_len)+len(Neutral_len))","82eee578":"plt.title(\"Average Word Length\")\nsns.barplot(x=['Negative','Neutral','Positive','Combined'],y=[neg_mean,neutral_mean,pos_mean,combined_mean])\nplt.show()","209e50a0":"X=df['clean_comment'].astype('str')\nX[:5]","f2768fdf":"lp=\"\"\nfor i in X:\n    lp+=i+\" \"\nprint(lp[:100])","61b36421":"st=lp.split(' ')\ndict_len=len(set(st))","b1ead781":"dict_len,len(st)","ba115069":"tokenizer=Tokenizer(num_words=dict_len,lower=True,oov_token=\"OOV\")\ntokenizer.fit_on_texts(X)","5cb3ec8d":"len(tokenizer.word_index)\n","ecf0f830":"X_train=tokenizer.texts_to_sequences(X)\nX_train_padded=pad_sequences(X_train,maxlen=175,padding='post',truncating='post')","bd245700":"X_train[:2]","3a21846c":"df['category']=df['category'].replace({-1:2})","0d1a7533":"mp={0:\"Neutral\",1:\"Positve\",2:\"Negative\"}","c8095031":"Y=df['category'].values","5508cbf2":"Y_hot=to_categorical(Y)","90f19b46":"print(Y_hot[:3])","13564eaf":"model=Sequential()\nmodel.add(Embedding(dict_len,64,input_length=175))\nmodel.add(Dropout(0.3))\nmodel.add(Bidirectional(LSTM(175,return_sequences=True)))\nmodel.add(Dropout(0.3))\nmodel.add(Bidirectional(LSTM(350,return_sequences=True)))\nmodel.add(Dropout(0.3))\nmodel.add(Bidirectional(LSTM(700)))\nmodel.add(Dense(3,activation='softmax'))\nprint(model.summary())","4657cb89":"model.compile(optimizer='adam',metrics=['accuracy'],loss='categorical_crossentropy')\n","666ecc2c":"hist=model.fit(X_train_padded,Y_hot,epochs=5,validation_split=0.2)","6b096904":"plt.plot(hist.history['accuracy'],c='b',label='Training')\nplt.plot(hist.history['val_accuracy'],c='r',label='Validation')\nplt.legend(loc='lower right')\nplt.show()","9552c0e0":"plt.plot(hist.history['loss'],c='b',label='Training')\nplt.plot(hist.history['val_loss'],c='r',label='Validation')\nplt.legend(loc='upper right')\nplt.show()","b5fef043":"def predict(s):\n    X_tes=[]\n    X_tes.append(s)\n    X_test=tokenizer.texts_to_sequences(X_tes)\n    X_test_padded=pad_sequences(X_test,maxlen=175,padding='post',truncating='post')\n    sent=int(model.predict_classes(X_test_padded))\n    print(\"The Predicted Sentiment is \",mp[sent])","63f80043":"pol=\"The article is good but its not great moreover i would say you have done a decent job\"\npredict(pol)","31d26423":"lop=\"You have done a stupid mistake which made you lose all the progress you made\"\npredict(lop)","479c9c63":"com=\"It aint hard work but its honest work\"\npredict(com)","9f1ba0fb":"ppp=\"Lets Find out what this is going to be classified as\"\npredict(ppp)","4632993d":"# The Model","3097b81c":"# Tokeinzer\n","c12a4213":"# Exploratory Data Analysis","4c477ede":"# Check For Your Own Data","b7abf434":"# One Hot Encode The Sentiment Values"}}