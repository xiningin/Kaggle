{"cell_type":{"89df3b1e":"code","e3140062":"code","1c8c3056":"code","1884c2b0":"code","434d0812":"code","deeff1f6":"code","274d4f85":"code","873ec8ca":"code","0f983200":"code","1be36ac8":"code","a1218e35":"code","52a73127":"code","6a7f3502":"code","b921a6b1":"code","4da12564":"markdown","ca84cc1c":"markdown","10c7ed12":"markdown","c3f22871":"markdown","8449edbe":"markdown","ab7cf35c":"markdown","b52d35c5":"markdown","da1ffe89":"markdown","f6590f3f":"markdown","df0201d3":"markdown","10e1d743":"markdown","95b71598":"markdown","5b216e9b":"markdown","f6978213":"markdown","324f6f7b":"markdown","eb5741c7":"markdown","bd3f1c86":"markdown","3fc84afa":"markdown","f6514973":"markdown","713ae3ee":"markdown","068125dd":"markdown","c773f76e":"markdown"},"source":{"89df3b1e":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e3140062":"df_cancer = pd.read_csv('..\/input\/kag_risk_factors_cervical_cancer.csv')\ndf_cancer = df_cancer.replace('?', np.nan)\n# df_cancer = df_cancer.drop(DROP_COLUMNS, axis = 1)\ndf_cancer = df_cancer.rename(columns={'Biopsy': 'Cancer'})\ndf_cancer = df_cancer.apply(pd.to_numeric)\ndf_cancer = df_cancer.fillna(df_cancer.mean().to_dict())\n\nX = df_cancer.drop('Cancer', axis=1)\ny = df_cancer['Cancer']\n\ndf_cancer.head()","1c8c3056":"X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size = 0.25,random_state=2019)\nrandom_forest = RandomForestClassifier(n_estimators=500, random_state=2019).fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\nprint('Accuracy score: ' + str(accuracy_score(y_test, y_pred)))\n","1884c2b0":"X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X[['IUD']], y, shuffle=True, test_size = 0.25,random_state=2019)\nrandom_forest = RandomForestClassifier(n_estimators=500, random_state=2019).fit(X_train_2, y_train_2)\ny_pred_2 = random_forest.predict(X_test_2)\nprint('Accuracy score: ' + str(accuracy_score(y_test_2, y_pred_2)))","434d0812":"from sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\nrecall_score(y_test, y_pred)","deeff1f6":"from imblearn.combine import SMOTETomek\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size = 0.25,random_state=2019)\n\ncc = SMOTETomek(random_state=2019)\nX_res, y_res = cc.fit_resample(X_train, y_train)\n\nrandom_forest = RandomForestClassifier(n_estimators=500, random_state=2019).fit(X_res, y_res)\ny_pred = random_forest.predict(X_test)\nprint('Accuracy score: ' + str(accuracy_score(y_test, y_pred)))\nprint('Recall score: ' + str(recall_score(y_test, y_pred)))","274d4f85":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\npermumtation_impor = PermutationImportance(random_forest, random_state=2019).fit(X_test, y_test)\neli5.show_weights(permumtation_impor, feature_names = X_test.columns.tolist())","873ec8ca":"from pdpbox import pdp, get_dataset, info_plots\n\ndef pdpplot( feature_to_plot, pdp_model = random_forest, pdp_dataset = X_test, pdp_model_features = list(X)):\n    pdp_cancer = pdp.pdp_isolate(model=pdp_model, dataset=pdp_dataset, model_features=pdp_model_features, feature=feature_to_plot)\n    fig, axes = pdp.pdp_plot(pdp_cancer, feature_to_plot, figsize = (10, 5),plot_params={})\n#     _ = axes['pdp_ax'].set_ylabel('Probability of Cancer')\n    \npdpplot('Schiller')\npdpplot('First sexual intercourse')\npdpplot('Num of pregnancies')\n\nplt.show()","0f983200":"row_to_show = 10\ndata_for_prediction = X_test.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n\nrandom_forest.predict_proba(data_for_prediction_array)","1be36ac8":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(random_forest)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)","a1218e35":"shap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0], data_for_prediction)","52a73127":"row_to_show = 94\ndata_for_prediction = X_test.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n\nrandom_forest.predict_proba(data_for_prediction_array)","6a7f3502":"# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(random_forest)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)","b921a6b1":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","4da12564":"For `patient 94`, the model predicted that she has cancer. But why? Let's see! ","ca84cc1c":"# Why my model predicted that\nIn the previous section, we already have seen how our model works, which features are important and how individuals feature affect the prediction of our model. But now we want to know why an individual prediction is made. Why our model made such a prediction? We will know that by using a method named '**SHAP Values**'. \n\n### SHAP Values\nSHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the impact of each feature [2]. It explains why a model made a certain prediction. Previously, we see the explanation of the model after training and how each feature will affect the prediction of the model. Now we are going to look at the individual prediction that our model made. Meaning we will now look at our model after it has made a prediction. \n\n\nFor example, in our cancer prediction model, if our model predicts someone has cancer, based on certain features, we want to know how each feature together contribute to the prediction. Here contribute means how each value of a feature affecting the prediction of cancer. Does it increase the chance of prediction or decreasing the chance of cancer. For instances, let's consider the age variable. How much a prediction is driven by the factor of age 40 instead of some baseline value of age?  And we do these for each feature. The rules for calculating SHAP values is [2]: \n\n`sum(SHAP values for all features) = pred_for_patient - pred_for_baseline_values`\n","10c7ed12":"Let's understand SHAP by a story[3]. Consider the following scenario: a group of people are playing a game. As a result of playing this game, they receive a certain reward; how can they divide this reward between themselves in a way which reflects each of their contributions?\n\nThere are a few things which everyone can agree on; meeting the following conditions will mean the game is \u2018fair\u2019 according to Shapley values:\n\n- The sum of what everyone receives should equal the total reward\n- If two people contributed the same value, then they should receive the same amount from the reward\n- Someone who contributed no value should receive nothing\n- If the group plays two games, then an individual\u2019s reward from both games should equal their reward from their first game plus their reward from the second game\n\nIf we consider game reward is the model outcome and the participants are features, then this is how SHAP values calculated. \n\n\n\n","c3f22871":"Now we know how to explain a model. But these notebooks introduce you to the basic and popular proven concept in the area of model explainability. Remember, model explainability is an ongoing field of active research. There is a lot of other methods for ml explainability. If you want to enhance your knowledge in this area then check out the references below. \n\n\n# Thanks! \n","8449edbe":"Wow! As you can see our model recall score is improved significantly after applying some sampling technique. Though still our recall score is not so high, still it is lot better than the previous model. \n\nKey ideas: \n* Always choose evaluation metrics carefully \n* In medical use model, you should always check recall. It will helps us to make sure how our model is performing for minority target values. ","ab7cf35c":"As expected, our model is significantly worse at predicting someone has cancer. How do we improve recall score? Well, the dataset we are using is heavily imbalanced. So we need to apply some sampling technique to solve this problem. We will sample our dataset in a way that each sample represent our target with balance. ","b52d35c5":"# Load data and build our model\nWe will use cervical cancer dataset from Kaggle dataset. It contains information of patients describing who has cervical cancer or not based on some criteria. We will use those criteria as features of our model. ","da1ffe89":"Key things to remember:\n```\n- SHAP Values explains why the model predicted something\n- It calculates how the value of a feature affected the predicted outcome instead of some baseline value\n- And it does it for every feature and shows the contribution of each feature for the outcome\n```\n\nLet's use SHAP Value to explain why our model predicted someone has cancer or not. First, we will choose a random row from our test data representing as a patient, then we will predict whether the patient has cancer or not. Then we will use SHAP Value to understand why our model predicted that. ","f6590f3f":"* Now We have our model that can successfully predict a patient has cervical cancer or not based on some features. \n* But, currently our model is acting as a black box. If it predicts that a patient has cancer, we will not know why the model predicts that. \n* Currently, we are only calculating accuracy score as an indicator of model performance. But sometimes it can be misleading.  \n* How do we improve our model? \n\nFor all these questions, the answer to all these is model explainability. Let's together explore our model inside. ","df0201d3":"What are we seeing? Our model predicts the same accuracy score with just one feature column. What's happening! Well, the problem is that our model learned nothing. Because over 95% of the patients in our dataset doesn't have cancer. So our model got biased towards it. And in the test set, the model just output everybody doesn't have cancer and it got 95% accuracy. \n\nHow do we solve this problem? The problem is that in this type of imbalanced dataset the accuracy as a matric is not a good choice, in fact, it is a very bad choice in a model that will be used in medical work. The accuracy metric is calculated `accuracy = number of correct predictions \/ total number of predictions` using this equation. As you noticed in an imbalanced dataset the accuracy metric is not very useful, because `number of correct prediction` will be higher for model bias prediction. \n\nFor solving this problem, we need to learn some important topics. And that is Sensitivity and Specificity. \n\nSensitivity or True Positive Rate (TPR), also known as recall, is the proportion of people that tested positive and are positive (True Positive, TP) of all the people that actually are positive. It can be seen as the probability that the test is positive given that the patient is sick. With higher sensitivity, fewer actual cases of disease go undetected (or, in the case of the cancer model fewer patient those have cancer is undetected) [5]. Also, specificity is the opposite of that. \n\nFor our model, we need to calculate sensitivity also known as recall. Because we don't want to misclassify a patient who has cancer. This will cause very damage to the patient. On the other hand, if our model has predicted wrong someone has cancer we can make sure from another test. So the higher the recall is the less chance will be that our model will wrong classify someone has cancer. ","10e1d743":"References:\n\n[1] [Kaggle Survey Analysis by Graeme Keleher](https:\/\/www.kaggle.com\/graeme16161\/importance-of-interpretability)\n\n[2] [Machine Learning Explainability by DanB](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability)\n\n[3] [Interpreting complex models with SHAP values](https:\/\/medium.com\/@gabrieltseng\/interpreting-complex-models-with-shap-values-1c187db6ec83)\n\n[4] [Interpretable Machine Learning by Christoph Molnar](https:\/\/christophm.github.io\/interpretable-ml-book\/)\n\n[5] [Sensitivity and specificity](https:\/\/en.wikipedia.org\/wiki\/Evaluation_of_binary_classifiers)","95b71598":"In this case, we are seeing that the `Number of sexual partners = 4`, `Schiller = 1` and `Cytology = 1` have affected the outcome of the model. That makes sense. Because we previously saw from PDP that cancer probability is increased with `Schiller` and `Cytology` being 1. Also, we are seeing that `Hinselmann = 0` and `Num of pregnancies = 1` decreased the model outcome. ","5b216e9b":"Explanation: \n\nFeature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect. Feature values decreasing the prediction are in blue [2]. \n\nWe are seeing that `Schiller = 0` has the biggest impact on the outcome of the model. Also, the `Cytology = 1` has a very strong effect on decreasing the outcome. That's meaning is that `Schiller = 0` increased the chances of not cancer and `Cytology = 1` increased the chance of cancer. \n\nCongratulations! Now we can explain why our model made certain predictions. Let's look at something different prediction. In the above example, our model has predicted the `patient(row 10)` doesn't have cancer. But now we want to look at a patient who has cancer. For that, we have to choose `row 94`.","f6978213":"Explanation: \n* In the plot, the most important feature is at the top and most less important features is at the bottom. \n* We can see that most important features are  \"**Schiller, First Sexual Intercourse, Number of pregnancies**\" etc. \n* We are also seeing the less important features at the bottom of the feature like STDs. \n\nNow we know a lot of inside in our model by using feature importance. We can use this information when collecting dataset and pay more attention to the important features. ","324f6f7b":"# Introduction \nWe, modern people, use technology extensively. In the era of machine learning, we are constantly using the machine for our decision making. Nowadays, we use a machine to predict whether a person will pay back the loan or a person has certain types of disease. Self-driving cars already becoming reality. In every field, machine learning is used heavily. \n* Finance - Use machine learning for predicting the stock market and lot more. \n* Medical - Use machine learning to identify diseases, affected cells, diagnosis and more. \n* Politics - Identify how certain policy will affect society and more. \n\nAn endless example can be given on the use of machine learning in the modern world. But with the blessing of machine learning, we are also facing some drawbacks. One of the biggest drawbacks is '**Lack of Machine learning model explainability**'. \n\n\n## Machine learning model explainability\nWhat is the Machine learning model explainability? Suppose, we have a model that predicts the price of a product on an e-commerce site. It takes some data as input and output the result (price). But do you know why the model output that price? **What if the model output the wrong price?** Well, you can say that we could just trust the model. Well, let's see another scenario, let's say we have a model that predicts a patient has certain types of cancer or not? If the model output is wrong it can lead to serious damage and issue! **We need to know why our model output\/predicted certain value**. And that's called machine learning model explainability. \n\n\nIn the 2018 Kaggle survey, Kaggle asked data scientist how they view about Machine learning model explainability. [1]Here is the result of that question\n![](https:\/\/i.imgur.com\/sdCNPv8.jpg)\nAccording to the survey result, we find that \n* Model explainability is very important\n* Many people have skills to explain the outcome of many models but not all\n* Unfortunately, there are also many people who think that expert can only explain the outcome of a model and \n* Many of them think machine learning model as a black box\n\n\n\n## Importance of model explainability\n* We humans are a very rational being. We always like to know why happen something? As machine learning is a part of life, we want to know why our model makes such a prediction. This will build trust and overall use case of the model. \n* Our model can pick biases from training data. This can turn your machine learning models into racists that discriminate against protected groups. By identifying model explainability we will be better solving the bias problem [4]. \n* We can improve our model by using model explainability. \n* Machine learning models take on real-world tasks that require safety measures and testing. Like self-driving cars. We want to make sure it identifies roadblocks and cars and other things carefully. One single mistake can cause huge damage. Model explainability helps us in this kind of scenario [4]. \n\n\n## What we will do in this kernel\nWe already know what is model explainability, the importance of it and why should we learn about it. In this kernel, I will try my best to explain a model which is used in medical for predicting cervical cancer. We will use a random forest classifier as an ML algorithm. We will together explore the world of model explainability.\n","eb5741c7":"**1. Made prediction**","bd3f1c86":"We are seeing our model predicted **72.6%** probability that given patient has no cancer and **27.4%** that the given patient has cancer. Let's see why our model predicted that. \n\n**2. Explain the prediction using SHAP Values**","3fc84afa":"# How a single feature affect our prediction\nPreviously, we saw the most important features of our model. It's a good inside. But **we don't know how each feature affecting accuracy**. For example, let's use the features 'Age'. We know that age is an important feature. But we don't know that the chance of cancer is increasing with age or decreasing. For knowing how single features affect our prediction we need to use a different technique called \"**Partial Dependence Plots**\". \n\nPartial Dependence Plots or PDP is also a very popular method. PDP is calculated after the model is fitted. We then use a single row from test data to predict the outcome. Instead of predicting one prediction, we repeatedly alter one variable of the row to make a series of prediction. For example, for our cervical cancer model, we take on a row from test data and repeatedly alter a single variable value like age, and then make a series of prediction. And we do these for multiple rows, then plot average predicted the outcome on vertical axes. \n\nKey things to remember for PDP:\n* PDP calculated after a model is fitted\n* Use single row for prediction\n* Repeatedly alter a variable value to make a series prediction\n* Do that for multiple rows and plot the average prediction on the vertical axes\n\nIn this notebook, we will use PDPBox library for Partial Dependence Plots. Let's see! ","f6514973":"# Most important feature\nOur cervical cancer model uses a lot of features. But how do we know which features are crucial and which is less important for predicting cervical cancer? The answer will be \"**Feature importance**\". \n\nFeature importance is a very classic and popular method. Feature importance is actually what it sounds like. According to the definition, feature importance is *\"The importance of a feature is the increase in the prediction error of the model after we permuted the feature\u2019s values, which breaks the relationship between the feature and the true outcome.\" [4]* \n\nFeature importance is calculated after the model has fitted. In the features columns, it randomly shuffles\/permutate each column without touching the target and other feature column and calculates how it affects the accuracy of the prediction of new shuffled data. \n\nKey things to remember\n* Feature importance applied after the model has been fitted\n* It selects each feature column one at a time and shuffles\/permutate that column randomly\n* Then it calculates how that affect the prediction\/accuracy of the model\n* If changing value of a feature column affect the accuracy of the model heavily then the importance of that column feature is higher. In this way, feature importance is calculated. \n\nThere are many ways you can calculate feature importance. In this notebook, we will use '**Permutation importance**' by the eli5 library. ","713ae3ee":"# Put it all together\nWow! We have now known a lot about model explainability. In this notebook, we have seen how we can explain a cervical cancer prediction model using some popular explainability method. A quick recap: \n* Firstly, we talk about what is model explainability and what is the importance of it. \n* Then, we create a model that will predict someone has cervical cancer or not based on some features \n* We explain how to evaluate a model successfully. \n* The focus of this notebook is model explainability, so we didn't invest too much time in the model building. Also, the dataset is very small. \n* But we have uncovered some of the world most popular and best model explainability method. \n* We saw what is the most important features, how each feature affects the accuracy and most importantly we saw how to explain a prediction made by our model. ","068125dd":"Explanation: \n* The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value. \nA blue shaded area indicates level of confidence [2].\n* We can see that probability of cancer is increasing with Schiller. \n* In the second plot, we can see that probability of cancer decreases with the **first sexual intercourse**. \n* Lastly, we see that cancer probability increases after** Number of pregnancies** hits 3. \n\nNow we know how some important features are affecting our prediction. We have unveiled a lot of inside of our model. We can already feel some sense of how our model is working. But in the next section, we will learn another interesting method for explaining our prediction. ","c773f76e":"# Is Accuracy Score telling the truth\nWe can see that our model accuracy score is over 95%. But is it really trustworthy? Let's see what is the model accuracy when we use only one feature for predicting cancer. This will drastically decrease the accuracy of the model. "}}