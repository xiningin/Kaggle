{"cell_type":{"bd10d15e":"code","bb773b16":"code","efb40533":"code","f8bedc44":"code","0654e251":"code","26ecc904":"code","e79148ef":"code","bbeadcdf":"code","a0ba4d82":"code","fab04635":"code","84521d23":"code","3712eac5":"code","fd252ca1":"code","12b50306":"code","43285d01":"code","92f634b0":"code","d585c217":"code","eac81644":"code","d052b79c":"code","29c9ec24":"code","c6c59184":"code","27d1cef6":"code","1b3e54ec":"code","becc90e8":"code","f6da81bc":"code","5d0212b6":"code","f8e6b657":"code","dd7c8895":"code","850419a6":"code","985b54f0":"code","ec7ca07f":"code","de4d9dea":"code","0f993e7c":"code","8588e8ba":"code","fe90a5b0":"code","3261bd69":"code","4873137d":"code","36f88d78":"code","14202885":"code","526cbfa0":"code","7fbf2db2":"code","421f58cc":"code","02675c55":"code","aef111f3":"code","81473d99":"code","1df7feab":"code","5c0a89b4":"code","0c606f3d":"code","b9fdbaba":"code","42c678a1":"code","8cc7846e":"code","70009630":"code","c17c1ab0":"code","a9464a78":"code","47e9c860":"code","72979f53":"code","b6f0f85a":"code","d1ef82ea":"code","e48b0627":"code","9ba74ab7":"code","a79ad9a6":"code","49d0c2b1":"code","a0094c7f":"code","475391ec":"code","b2d70ab7":"code","123ef3fe":"code","f1ce1d74":"code","2175fdd5":"code","4c7e7852":"code","2e31e0ee":"code","83c39ec5":"code","e200a8e0":"code","faf91ff0":"code","f1373e38":"code","becf4778":"code","d15017b4":"code","7e20e41c":"markdown","d3758d2d":"markdown","81fe9af5":"markdown","2cbef7e6":"markdown","e86172ba":"markdown","d3388b2d":"markdown","619940ca":"markdown","83ee2bea":"markdown","aaf97794":"markdown","a0a2f6cc":"markdown","7a545097":"markdown","172c01f0":"markdown","2c32324c":"markdown","2a74e807":"markdown","f6b91f3b":"markdown","a597a597":"markdown","875b4500":"markdown","169453da":"markdown","f1ecb341":"markdown","9b3157f3":"markdown","0f2f83e9":"markdown","5772ce8e":"markdown","66e58281":"markdown","1e0c26c8":"markdown","efbae84d":"markdown","b0053abe":"markdown","51a248bf":"markdown","97c580a8":"markdown"},"source":{"bd10d15e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb773b16":"## i have implemented mice and KNN imputer to get good results\n","efb40533":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n!pip install impyute","f8bedc44":"data = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')","0654e251":"data.head(10)","26ecc904":"test.head()","e79148ef":"test.shape","bbeadcdf":"sns.countplot(data['song_popularity'], palette='Set3')","a0ba4d82":"pp = data['song_popularity'].sum()\/len(data['song_popularity'])*100\nprint(\" the percentage of popular songs is {}\".format(pp))\nprint('the percentage of un-popoular songs is {}'.format(100-pp))","fab04635":"sum(data.isnull().sum())  ## total no. of missing values in the whole data set","84521d23":"data.info()","3712eac5":"test.info()","fd252ca1":"#data.describe()  ","12b50306":"#plt.scatter(data['loudness'],data['danceability'])\n#plt.show()\n","43285d01":"#plt.scatter(data['loudness'],data['speechiness'])\n#plt.show()","92f634b0":"#plt.scatter(data['liveness'],data['speechiness'])\n#plt.show()","d585c217":"data.shape","eac81644":"missing_cols = [feature for feature in data.columns if data[feature].isnull().sum().any()>0]","d052b79c":"print(\"the no. of columns which have missing columns are {}\".format(len(missing_cols)))\nprint(\"The columns which have missing values are \\n\\n {}\".format(missing_cols))\n# this means out of 15 features (including 1 target variable) we have more than half of the dataset having mising values","29c9ec24":"def missing_value_count(feature):\n    number = data[feature].isnull().sum()\n    return number","c6c59184":"\n\n\n\ndef missing_value_percent(feature):\n    percent = data[feature].isnull().sum() \/ (len(data[feature]))\n    return percent","27d1cef6":"missing_values = data.isnull().sum()\nmissing_values = missing_values[missing_values > 0]\nmissing_values.sort_values(inplace=True)\nmissing_values","1b3e54ec":"for feature in missing_cols:\n    print( \"the total missing values in {} is equal to {}\".format(feature, missing_value_count(feature)))\n    print(\"the mising values in {} accounts for {} % of total value\\n\\n\\n\".format(feature, missing_value_percent(feature)))","becc90e8":"#Top skewed columns\nfrom scipy.stats import skew\n\ncolumns = data.columns\nskewed_features = data[columns].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[abs(skewed_features) > 0.5]\nprint(high_skew)\n\n## audio_mode takes either 0 or 1  implies categorical variable\n## similarly for song_popularity","f6da81bc":"features = data.columns.values[0:14]\ni = 0\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(10,10,figsize=(18,22))\n\nfor feature in features:\n    i += 1\n    plt.subplot(7,2,i)\n    sns.kdeplot(data[feature], label='train')\n    sns.kdeplot(test[feature], label='test')\n    plt.xlabel(feature, fontsize=9)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n    plt.tick_params(axis='y', which='major', labelsize=6)\nplt.show();","5d0212b6":"high_skew = ['speechiness','tempo']\ndata[high_skew]=np.log1p(data[high_skew])\ntest[high_skew]=np.log1p(test[high_skew])","f8e6b657":"features = data.columns.values[0:14]\ni = 0\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(10,10,figsize=(18,22))\n\nfor feature in features:\n    i += 1\n    plt.subplot(7,2,i)\n    sns.kdeplot(data[feature], label='train')\n    sns.kdeplot(test[feature], label='test')\n    plt.xlabel(feature, fontsize=9)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n    plt.tick_params(axis='y', which='major', labelsize=6)\nplt.show();","dd7c8895":"def feature_engineering(df):\n    df['NaN_row'] = df.isna().sum(axis=1)\n    return df\n\ndata_2 = feature_engineering(data)\ntest_2 = feature_engineering(test)\n","850419a6":"data_2.head()","985b54f0":"test_2.head()","ec7ca07f":"## data features\nX =data_2.drop(['song_popularity'],axis=1)\n\n## target variable\ny = data_2['song_popularity']","de4d9dea":"X=X.drop(['id'],axis=1)\ntest_2 = test_2.drop(['id'],axis=1)","0f993e7c":"X.head()","8588e8ba":"test_2.head()","fe90a5b0":"## imputing training data\nimport sys\nfrom impyute.imputation.cs import fast_knn\nsys.setrecursionlimit(100000) #Increase the recursion limit of the OS\n\n# start the KNN training\nimputed_X = pd.DataFrame(fast_knn(X.values,k=30))\n\n# Imputation removed column names; put them back\nimputed_X.columns = X.columns\n\nimputed_X.head()","3261bd69":"##imputing test data\nimputed_test_2 = pd.DataFrame(fast_knn(test_2.values,k=30))\nimputed_test_2.columns = test_2.columns\n\n","4873137d":"imputed_test_2.head()","36f88d78":"## let us transform the variables using standard scaler","14202885":"from sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()","526cbfa0":"imputed_scaled_X = pd.DataFrame(stdsc.fit_transform(imputed_X))\nimputed_scaled_test = pd.DataFrame(stdsc.transform(imputed_test_2))","7fbf2db2":"imputed_scaled_X.columns = imputed_X.columns\nimputed_scaled_test.columns= imputed_test_2.columns","421f58cc":"imputed_scaled_X.head()","02675c55":"print(sum(imputed_scaled_X.isnull().sum()))\n## so now no null values are present in the data \/set","aef111f3":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc","81473d99":"params = {  'objective': 'binary', \n            'n_estimators': 20000,\n            'max_depth': 7,\n            'learning_rate':  0.01, \n            'min_child_weight': 256,\n            'min_child_samples': 15, \n            'reg_alpha': 10, \n            'reg_lambda': 0.1, \n            'subsample': 0.6, \n            'subsample_freq': 1, \n            'colsample_bytree': 0.4,}","1df7feab":"%%time\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=121)\n\npreds = []\nscores = []\nfeature_importance_df = pd.DataFrame()\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(imputed_scaled_X, y)):\n    imp_X_train, y_train = imputed_scaled_X.iloc[idx_train], y.iloc[idx_train]\n    imp_X_valid, y_valid = imputed_scaled_X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = LGBMClassifier(**params)\n    \n    model.fit(imp_X_train, y_train,\n              eval_set = [(imp_X_valid, y_valid)],\n              verbose = False,\n              early_stopping_rounds = 300)\n    \n    pred_valid = model.predict_proba(imp_X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = X.columns\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\" \"\\n\")\n    print('||'*40, \"\\n\")\n    \n    test_preds = model.predict_proba(imputed_scaled_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","5c0a89b4":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:107].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(7,14))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()    \nplt.savefig('FI.png')","0c606f3d":"ss    = pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')","b9fdbaba":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['song_popularity'] = predictions   \nss.to_csv('submission_knn_lgbm_notuning.csv', index=False)\nss.head()","42c678a1":"from impyute.imputation.cs import mice\n\n# start the KNN training\nmice_X = pd.DataFrame(mice(X.values))\n\n# Imputation removed column names; put them back\nmice_X.columns = X.columns\n\nmice_X.head()","8cc7846e":"##imputing test data\nmice_test_2 = pd.DataFrame(mice(test_2.values))\nmice_test_2.columns = test_2.columns\n\n","70009630":"mice_test_2.head()","c17c1ab0":"from sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\n\nmice_scaled_X = pd.DataFrame(stdsc.fit_transform(mice_X))\nmice_scaled_test = pd.DataFrame(stdsc.transform(mice_test_2))\n\nmice_scaled_X.columns = mice_X.columns\nmice_scaled_test.columns= mice_test_2.columns  ","a9464a78":"mice_scaled_X.head()","47e9c860":"mice_scaled_test.head()","72979f53":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc","b6f0f85a":"params = {  'objective': 'binary', \n            'n_estimators': 20000,\n            'max_depth': 7,\n            'learning_rate':  0.01, \n            'min_child_weight': 256,\n            'min_child_samples': 15, \n            'reg_alpha': 10, \n            'reg_lambda': 0.1, \n            'subsample': 0.6, \n            'subsample_freq': 1, \n            'colsample_bytree': 0.4,}","d1ef82ea":"%%time\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=121)\n\npreds = []\nscores = []\nfeature_importance_df = pd.DataFrame()\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(mice_scaled_X, y)):\n    mice_X_train, y_train = mice_scaled_X.iloc[idx_train], y.iloc[idx_train]\n    mice_X_valid, y_valid = mice_scaled_X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = LGBMClassifier(**params)\n    \n    model.fit(mice_X_train, y_train,\n              eval_set = [(mice_X_valid, y_valid)],\n              verbose = False,\n              early_stopping_rounds = 300)\n    \n    pred_valid = model.predict_proba(mice_X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = X.columns\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\" \"\\n\")\n    print('||'*40, \"\\n\")\n    \n    test_preds = model.predict_proba(mice_scaled_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","e48b0627":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()      \n        .sort_values(by=\"importance\", ascending=False)[:107].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(7,14))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","9ba74ab7":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['song_popularity'] = predictions\nss.to_csv('submission_mice_lgbm_notuning.csv', index=False)\nss.head()","a79ad9a6":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.model_selection import train_test_split","49d0c2b1":"def objective(trial, data=imputed_scaled_X, target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(imputed_scaled_X, y, test_size=0.2,random_state=121)\n    \n    params = {\n        'max_depth': trial.suggest_int('max_depth', 6, 15),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 20000, 500),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4),\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n        'predictor': \"gpu_predictor\",\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic',\n         }\n    \n    model = XGBClassifier(**params, tree_method='gpu_hist', random_state=121, use_label_encoder=False)\n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)], early_stopping_rounds=100, verbose=False)\n    \n    preds = model.predict_proba(test_x)[:,1]\n    fpr, tpr, _ = roc_curve(test_y, preds)\n    score = auc(fpr, tpr)\n    \n    return score\n    ","a0094c7f":"import optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=150)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","475391ec":"optuna.visualization.plot_optimization_history(study)\n","b2d70ab7":"optuna.visualization.plot_param_importances(study)","123ef3fe":"params=study.best_params  \nprint(params)   ","f1ce1d74":"%%time\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\npreds = []\nscores = []\nfeature_importance_df = pd.DataFrame()\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(imputed_scaled_X, y)):\n    imp_X_train, y_train = imputed_scaled_X.iloc[idx_train], y.iloc[idx_train]\n    imp_X_valid, y_valid = imputed_scaled_X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = XGBClassifier(**params,\n                            booster= 'gbtree',\n                            eval_metric = 'auc',\n                            tree_method= 'gpu_hist',\n                            predictor=\"gpu_predictor\",\n                            use_label_encoder=False)\n    \n    model.fit(imp_X_train, y_train,   \n              eval_set = [(imp_X_valid, y_valid)],\n              verbose = False,\n              early_stopping_rounds = 300)    \n    \n    pred_valid = model.predict_proba(imp_X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = X.columns\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\" \"\\n\")\n    print('||'*40, \"\\n\")\n    \n    test_preds = model.predict_proba(imputed_scaled_test)[:,1]\n    preds.append(test_preds)\nprint(f\"Overall Validation Score: {np.mean(scores)}\")","2175fdd5":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:107].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(7,14))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","4c7e7852":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['song_popularity'] = predictions\nss.to_csv('submission_xgb_knn_tuning.csv', index=False)\nss.head()   ","2e31e0ee":"def objective(trial, data=mice_scaled_X, target=y):  \n    \n    train_x, test_x, train_y, test_y = train_test_split(mice_scaled_X, y, test_size=0.2,random_state=121)\n    \n    params = {\n        'max_depth': trial.suggest_int('max_depth', 6, 15),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 20000, 500),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4),\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n        'predictor': \"gpu_predictor\",\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic',\n         }\n    \n    model = XGBClassifier(**params, tree_method='gpu_hist', random_state=121, use_label_encoder=False)\n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)], early_stopping_rounds=100, verbose=False)\n    \n    preds = model.predict_proba(test_x)[:,1]\n    fpr, tpr, _ = roc_curve(test_y, preds)\n    score = auc(fpr, tpr)\n    \n    return score\n    ","83c39ec5":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=150)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","e200a8e0":"optuna.visualization.plot_optimization_history(study)\n\n\noptuna.visualization.plot_param_importances(study)","faf91ff0":"params=study.best_params\nprint(params)","f1373e38":"%%time\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\npreds = []\nscores = []   \nfeature_importance_df = pd.DataFrame()\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(mice_scaled_X, y)):\n    mice_X_train, y_train = mice_scaled_X.iloc[idx_train], y.iloc[idx_train]\n    mice_X_valid, y_valid = mice_scaled_X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = XGBClassifier(**params,\n                            booster= 'gbtree',\n                            eval_metric = 'auc',\n                            tree_method= 'gpu_hist',\n                            predictor=\"gpu_predictor\",\n                            use_label_encoder=False)\n    \n    \n    model.fit(mice_X_train, y_train,  \n              eval_set = [(mice_X_valid, y_valid)],\n              verbose = False,\n              early_stopping_rounds = 300)\n    \n    pred_valid = model.predict_proba(mice_X_valid)[:,1]   \n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = X.columns\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\" \"\\n\")\n    print('||'*40, \"\\n\")\n        \n    test_preds = model.predict_proba(mice_scaled_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")\n\n","becf4778":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:107].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(7,14))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","d15017b4":"predictions = np.mean(np.column_stack(preds),axis=1) \n\nss['song_popularity'] = predictions\nss.to_csv('submission_xgb_mice_tuning.csv', index=False)\nss.head()","7e20e41c":"## let us try to see how to impute the missing values","d3758d2d":"## 1. Imputation using KNN","81fe9af5":"# Please upvote if you find this helpful\n","2cbef7e6":"## final submission in this notebook","e86172ba":"### As indicated by rob mulla in this session here (https:\/\/www.youtube.com\/watch?v=EYySNJU8qR0) on youtube we can impute both the training and test data set to get a good score on the leader board. I just tried to do the same. I would like to add generally it is not considered a good technique for the real world data sets.\n","d3388b2d":"#### Optuna is a software framework for automating the optimization process of these hyperparameters. It automatically finds optimal hyperparameter values by making use of different samplers such as grid search, random, bayesian, and evolutionary algorithms","619940ca":"## 2. Imputation Using Multivariate Imputation by Chained Equation (MICE)\nThis type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a single imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns. For more information on the algorithm mechanics, you can refer to the Research Paper\n\n#### Time consuming\n\n","83ee2bea":"## please make sure to go through the notebook completely to get better idea of the techniques used and the data set ","aaf97794":" ## this is just an experimental notbeook and the aim was to see what position on leaderboard can be achieved using the MICE method and KNN imputer method without fit\/transform method.","a0a2f6cc":"## let us count the total no. of missing values and the percentage of missing values in each column","7a545097":"## A ) USING LGBM classifier","172c01f0":"# 2. Imputation using MICE","2c32324c":"## please upvote this notebook if you find it helpful. I will try to make more of my notebooks public!","2a74e807":"from impyute.imputation.cs import mice\n\n    start the MICE training\nimputed_training=mice(train.values)","f6b91f3b":"#### we will have to use stratified split ","a597a597":"we can see here that the missing values are not a lot in number as they account only for the 1% of the total data ","875b4500":"#### song popularity 0 indicates that the song is not popular and 1 indicates that the song is popular","169453da":"### we can clearly see all are numerical values as all the features have dtype as int64 or float 64","f1ecb341":"## B) Using XGBoost Classifier and HyperParameter usage!\n","9b3157f3":"since we can see from data above and as well as from skew function the speechiness and  temp are right-skewed so we perform log transformation for them","0f2f83e9":"missing_values = missing_values.to_frame()\nmissing_values.columns = ['count']\nmissing_values.index.names = ['Name']\nmissing_values['Name'] = missing_values.index\n#missing_values\nplt.figure(figsize = (15, 10))\nsns.barplot(x=missing_values['Name'], y=missing_values['count'], data=missing_values)\nplt.xticks(rotation = 90)\nplt.show()","5772ce8e":"## the best practice is to always fit_transform (or first fit and then transform) the trainig data set and then to apply the transform on test data set.","66e58281":"we will be using two methods here for imputing the training data set:\n### 1. Imputation using KNN\n### 2. Imputation Using Multivariate Imputation by Chained Equation (MICE)\n","1e0c26c8":"# Let us see the accuracy using different Classifiers","efbae84d":"clearly there are missing values in the features\/columns!","b0053abe":"## let us import the important libraries first\n","51a248bf":"### distribution plots","97c580a8":"## 1. Imputation using KNN\n    Pros:\nCan be much more accurate than the mean, median or most frequent imputation methods (It depends on the dataset).\n    \n    Cons:\nComputationally expensive. KNN works by storing the whole training dataset in memory.\nK-NN is quite sensitive to outliers in the data (unlike SVM)"}}