{"cell_type":{"ceba59a5":"code","59241220":"code","77e3095d":"code","85bc71b0":"code","6812c46e":"code","a8e6933e":"code","0d663555":"code","cd5c1546":"code","6ff59196":"code","ce264e83":"code","bad3d18a":"code","e6235c11":"code","038cbb60":"code","6a301129":"code","bc8edb52":"code","d9bd9d18":"markdown","f2f46844":"markdown","eec2bc2e":"markdown","86f5af67":"markdown","faaf89fa":"markdown"},"source":{"ceba59a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.metrics import roc_auc_score \n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\n\nfrom eli5.sklearn import PermutationImportance\nfrom eli5.permutation_importance import get_score_importances\n\nimport tensorflow as tf\n\nimport shap\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59241220":"raw_data = pd.read_csv('\/kaggle\/input\/room-occupancy-detection-data-iot-sensor\/Occupancy.csv')\n\nprint(\"Data Set First Five Values\")\nprint(60 * '=')\nprint(raw_data.head())\nprint('\\n\\n')\n\nprint(\"Data Set Info\")\nprint(60 * '=')\nprint(raw_data.info())\nprint('\\n\\n')\n\nprint(\"Data Set Stats\")\nprint(60 * '=')\nprint(raw_data.describe())\nprint('\\n\\n')\n\nprint('Missing Values')\nprint('=' * 60)\nprint(raw_data.isna().sum())","77e3095d":"fig = px.box(raw_data, x='Occupancy', y='Temperature')\nfig.update_layout(\n    xaxis_title_text='Occupancy'\n)\nfig.show()","85bc71b0":"fig = px.box(raw_data, x='Occupancy', y='CO2')\nfig.update_layout(\n    xaxis_title_text='Occupancy',\n)\nfig.show()","6812c46e":"fig = px.box(raw_data, x='Occupancy', y='CO2')\nfig.update_layout(\n    xaxis_title_text='Occupancy'\n)\nfig.show()","a8e6933e":"fig = px.box(raw_data, x='Occupancy', y='Humidity')\nfig.update_layout(\n    xaxis_title_text='Occupancy'\n)\nfig.show()","0d663555":"fig = px.box(raw_data, x='Occupancy', y='Light')\nfig.update_layout(\n    xaxis_title_text='Occupancy'\n)\nfig.show()","cd5c1546":"\ny = raw_data['Occupancy']\nX = raw_data.drop(['date', 'Occupancy'], axis=1\n                )\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.33, random_state = 31)\ninput_shape = [X_train.shape[1]]\n\ninput_shape_test = [X_test.shape[1]]\n\n\nprint(f'Train Shape: {input_shape}, Test Shape: {input_shape_test}')","6ff59196":"model = keras.Sequential ([\n    layers.BatchNormalization(input_shape=input_shape),\n    \n    layers.Dense(255, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.3),\n    \n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    \n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile (\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping],\n)","ce264e83":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")","bad3d18a":"y_pred = np.around(model.predict(X_test))\ncm = confusion_matrix(y_test, y_pred)\nlabels = ['Occupied' , 'Not']\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","e6235c11":"print('Reciever Operating Characteristic Curve (ROC AUC)')\nprint(roc_auc_score(y_test, y_pred))\n\nprint('Raw Accuracy Score')\nprint(accuracy_score(y_test, y_pred))\n\nprint('Balanced Accuracy Score')\nprint(balanced_accuracy_score(y_test, y_pred)) ","038cbb60":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)\n\ndisplay = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n                          estimator_name = 'Deep Learning Model')\ndisplay.plot()\n\nplt.show()","6a301129":"converter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the model.\nwith open('model.tflite', 'wb') as f:\n  f.write(tflite_model)\n","bc8edb52":"\nexplainer = shap.KernelExplainer(model.predict,X_train)\nshap_values = explainer.shap_values(X_test,nsamples=100)\nshap.summary_plot(shap_values,X_test,feature_names=features)","d9bd9d18":"# Model Developement","f2f46844":"# Exploratory Data Analysis\n","eec2bc2e":"# Running Tests on our outputted data","86f5af67":"# Conclusion\n\nThis goals of the prototype proof of concept model was complete. It was observed that a model was able to be used utilizing the IOT data. This model can be said to have a generalized accuracy of about 99% as all the accuracy evaluation metrics fell above this accuracy. In a very naive way we were able to observe that in theory it would be possible to perform person detection using a conventional nerual network trained on environmentally sensed data. The model was able to predict with a high degree of accuracy considering the proof of concept data was pretty crude. One additional, consideration is that the data is most likely from an office environment or some other commercial environment as from the exploratory data analysis shows that light in the room without an occpant is significantly high that one with an occupant. As such, we have to have cautious optimism about the translatability of the results to a vehicle however if the correct sensors were selected one predicts that the results could potentially be translated to other environments.\n\nAdditionally, the model was able to be exported as a tensor flow lite model which can be deployed either in an embedded linux environment or exported to a C style array which can be used in either an RTOS or bare-metal embedded environment. ","faaf89fa":"# Introduction\n\nThe purpose of this invesigation is to be a proof of concept project to investagate whether person detection can be accomplished in an embedded IOT device without the use of a computer vision solution. Such a solution would bridge the demand for saftey solutions for vehicles without the use of a camera. Cameras historically have been viewed as encroaching on the privacy of the user. Cameras typically bring the connotation that Big Brother is watching. However, the is also the market need of vehicular saftey solutions which check whether an operator is present. In the age of semi-autonomous vehicles, current legislation dictates that all vehicles which can semi-autonomously drive must maintain an operator behind the wheel in the case the autonomous system makes a mistake. There exists an operator who is ready to take over in the event something goes wrong. Even with mainstream production of semi-autonomous vehicles (such as cars, tractors, truck, ext...) current legislation is still a few years away from anything being considered higher that Level 2 automation. Not because of the capability of the technology, but because the security or lack thereof. Current operator detection techniques have fallen short of the crafty uses which attempt to bypass current saftey measures and unlock the full autonomous capabilities of their vehicle. Simple seat switches are easily bypassed by the simple Weekend at Bernie's method of using a weighted object. As such there is a market need for operater detection in semi-autonomous systems which don't use a camera.\n\nThe following POC project is meant to explore the possiblity of training a Tensor Flow deep learning model which can detect the presence of an operator using data such as the temperature, humidity, light and CO2 contained in a room. Such a model has the potential to be deployed on a device such as an embedded linux embedded system such as a Rasperry Pi or a Beagle Bone. In addition, manufacturing costs can be reduces significantly using the Tensor Flow ecosystem and leveraging the Tensor Flow lite for microcontrollers model cross-compiler which can be deployed on a microcontroller such as an ESP32."}}