{"cell_type":{"668f4b9b":"code","93c73442":"code","eb3bf353":"code","e29d1101":"code","06e4e382":"code","e149ac09":"code","7b819f31":"code","11a6af51":"code","348081b4":"code","12d6ea2d":"code","858d65ef":"code","0f171921":"code","051d88ac":"code","52eca807":"code","d20f0eef":"code","18ac8475":"code","433d4b58":"markdown","0899675f":"markdown","bf59129f":"markdown","3e72de00":"markdown","4662cc4a":"markdown"},"source":{"668f4b9b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport math\nimport os\nimport re\nimport time\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nfrom kaggle_datasets import KaggleDatasets","93c73442":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","eb3bf353":"AUTO = tf.data.experimental.AUTOTUNE\n\nEPOCHS = 10\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 200\n\nMAX_FEATURES = 100000\nEMBED_SIZE = 300","e29d1101":"EMBEDDING_FILE = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n\ntrain = pd.read_csv('\/kaggle\/input\/toxic-comment-classification\/train.csv')\n\nvalid = pd.read_csv('\/kaggle\/input\/toxic-comment-classification\/validation.csv')\n\ntest = pd.read_csv('\/kaggle\/input\/toxic-comment-classification\/test.csv')","06e4e382":"x_train = train['comment_text'].str.lower()\n\ny_train = train['toxic'].values\n\nx_valid = valid['comment_text'].str.lower()\n\ny_valid = valid['toxic'].values\n\nx_test = test['comment_text'].str.lower()","e149ac09":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","7b819f31":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.x_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.x_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","11a6af51":"%%time\n\ntok=text.Tokenizer(num_words=MAX_FEATURES,lower=True)\ntok.fit_on_texts(list(x_train)+list(x_test))\n\nx_train=tok.texts_to_sequences(x_train)\nx_test=tok.texts_to_sequences(x_test)\n\nx_train=sequence.pad_sequences(x_train,maxlen=MAX_LEN)\nx_test=sequence.pad_sequences(x_test,maxlen=MAX_LEN)","348081b4":"%%time\n\nembeddings_index = {}\nwith open(EMBEDDING_FILE,encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs","12d6ea2d":"word_index = tok.word_index\n\nnum_words = min(MAX_FEATURES, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, EMBED_SIZE))\nfor word, i in word_index.items():\n    if i >= MAX_FEATURES:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","858d65ef":"with strategy.scope():\n\n    sequence_input = Input(shape=(MAX_LEN, ))\n    \n    x = Embedding(MAX_FEATURES, EMBED_SIZE, weights=[embedding_matrix],trainable = False)(sequence_input)\n\n    x = SpatialDropout1D(0.1)(x)\n\n    x = Bidirectional(GRU(200, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n\n    x = Conv1D(200, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n\n    x = concatenate([avg_pool, max_pool])\n\n    x = Dense(100, activation='relu')(x)\n    x = Dropout(0.1)(x)\n\n    preds = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-5),metrics=['accuracy'])\n\nmodel.summary()","0f171921":"filepath = \"bilstm_cnn.h5\"\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n\nearly = EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=5)","051d88ac":"%%time\n\nn_steps = x_train.shape[0] \/\/ BATCH_SIZE\n\nmodel.fit(train_dataset, steps_per_epoch=n_steps, epochs=EPOCHS, validation_data=valid_dataset, callbacks=[early, checkpoint], verbose=1)","52eca807":"%%time\n\ny_true = test['toxic'].values\n\ny_pred = []\nfor i in x_test:\n    y_pred.append(model.predict(np.array([i])).tolist()[0][0])\n    \ny_pred = np.array(y_pred)","d20f0eef":"roc_auc_score(y_true, y_pred)","18ac8475":"fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=0)\nplt.plot(tpr,fpr)\n\nplt.show()","433d4b58":"## Tokenizer","0899675f":"## Train Model","bf59129f":"## TPU Configs","3e72de00":"## Metrics","4662cc4a":"## Import libraries"}}