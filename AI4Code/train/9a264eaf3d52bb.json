{"cell_type":{"eecfb933":"code","3788ee2c":"code","a08b4444":"code","4ad163f6":"code","d1147ea2":"code","816d1a2b":"code","90a8c3b5":"code","ab0ee269":"code","9a23e51b":"code","67ffac68":"code","719d208d":"code","2eeb7392":"code","84ded34f":"code","1c218732":"code","4ca95d70":"code","80464b9a":"code","dcde3fb0":"code","7d4c5899":"markdown","67ff9d23":"markdown","6c323981":"markdown"},"source":{"eecfb933":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3788ee2c":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nimport os\nimport glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset\nfrom torchvision.datasets import ImageFolder","a08b4444":"base_dir =  r'\/kaggle\/input\/multiclass-weather-dataset\/Multi-class Weather Dataset'\nfolders = os.listdir(base_dir)\nprint(folders)","4ad163f6":"# labeling files with Image Data Generator\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(\n    base_dir,\n    target_size=(250 , 250),\n    batch_size= 32,\n    class_mode='categorical',\n    subset='training') # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(\n    base_dir, # same directory as training data\n    target_size=(250, 250),\n    batch_size= 32,\n    class_mode='categorical',\n    subset='validation') # set as validation data\n\n","d1147ea2":"# Generated Labels\nlabels = (train_generator.class_indices)\nlabels = dict((v , k) for k , v in labels.items())\nprint(labels)\n","816d1a2b":"# setting seed and clearing session\ntf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)","90a8c3b5":"num_classes = 4\n\n# defining model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (5,5), input_shape = (250 , 250 , 3), activation = 'relu'),\n    tf.keras.layers.MaxPool2D(3,3),\n    tf.keras.layers.Conv2D(64, (5,5) , activation = 'relu'),\n    tf.keras.layers.MaxPool2D((3,3)),\n    tf.keras.layers.Conv2D(128 , (5,5) , activation = 'relu'),\n    tf.keras.layers.MaxPool2D(3,3),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5, seed = 5),\n    tf.keras.layers.Dense(128 , activation = 'relu'),\n    tf.keras.layers.Dense(num_classes , activation = 'softmax')])","ab0ee269":"# getting model summary after compiling\nmodel.compile(loss = 'categorical_crossentropy' , optimizer = tf.keras.optimizers.Adam(lr = 0.001) , metrics = 'accuracy')\n\nmodel.summary()","9a23e51b":"# defining callback for early stopping\nclass My_callback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self , epochs , logs = {}):\n        if(logs.get('accuracy') > 0.85 and logs.get('val_accuracy') > 0.85):\n            print('\\n stopping training')\n            self.model.stop_training = True\n            \ncallbacks = My_callback()","67ffac68":"# training model\nhistory = model.fit(train_generator ,\n          validation_data = validation_generator ,\n          epochs = 15 ,\n          steps_per_epoch = 901\/32, # train_files\/batch_size\n          validation_steps = 224\/32, # valid_files\/batch_size \n          callbacks = [callbacks]) ","719d208d":"# displaying graphs\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.title(string)\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","2eeb7392":"def prediction(test_path):\n    img = image.load_img(test_path , target_size = (250 , 250))\n    img = image.img_to_array(img, dtype=np.uint8)\n    img = np.array(img)\/255.0\n        \n    plt.title('Image')\n    plt.axis('off')\n    plt.imshow(img.squeeze())\n        \n    predict = model.predict(img[np.newaxis , ...])\n    predicted_class = labels[np.argmax(predict[0] , axis = -1)]\n        \n    print('Prediction Value: ' , np.max(predict[0] , axis = -1))\n    print(\"Classified:\",predicted_class)","84ded34f":"test_path = r'..\/input\/test-dataset\/Clouds.jpg'\nprediction(test_path)","1c218732":"test_path = r'..\/input\/test-dataset\/Rainy.jpg'\nprediction(test_path)","4ca95d70":"test_path = r'..\/input\/test-dataset\/Sunrise.jpg'\nprediction(test_path)","80464b9a":"test_img = r'..\/input\/test-dataset\/Clouds_2.jpg'\nprediction(test_path)","dcde3fb0":"print('Thank You , Uploading New Version Soon')","7d4c5899":"As can be seen , being a cloudy image , it states image is of shine , may be its due to that shiny sun , So it clearly indicates overfit ,  \n","67ff9d23":"**Clearly As Can Be Seen loss < val_loss , maybe model overfits**","6c323981":"# **QUICK AND DIRTY ON(1st Attempt)**"}}