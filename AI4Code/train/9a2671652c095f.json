{"cell_type":{"92f30949":"code","c81bc9c0":"code","b9e69a30":"code","3c949440":"code","8df860e3":"code","9e08fcf3":"code","58d46cd9":"code","a3ffe8ea":"code","cab8c8d7":"code","fff4dcc2":"code","29330df3":"markdown","da779e46":"markdown","fb44aa90":"markdown","eb8f6b6e":"markdown","2f60a9f7":"markdown","259abceb":"markdown","9383e905":"markdown","e1f94008":"markdown","3bc9512b":"markdown"},"source":{"92f30949":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom PIL import Image, ImageDraw\nfrom itertools import chain\n\nplt.style.use('fivethirtyeight')","c81bc9c0":"pjme = pd.read_csv('..\/input\/PJME_hourly.csv', index_col=[0], parse_dates=[0])","b9e69a30":"print(pjme.head())\n\ncolor_pal = [\"#F8766D\", \"#D39200\", \"#93AA00\", \"#00BA38\", \"#00C19F\", \"#00B9E3\", \"#619CFF\", \"#DB72FB\"]\n_ = pjme.plot(style='.', figsize=(15,5), color=color_pal[0], title='PJM East')","3c949440":"split_date = '01-Jan-2015'\npjme_train = pjme.loc[pjme.index <= split_date].copy()\npjme_test = pjme.loc[pjme.index > split_date].copy()","8df860e3":"_ = pjme_test \\\n    .rename(columns={'PJME_MW': 'TEST SET'}) \\\n    .join(pjme_train.rename(columns={'PJME_MW': 'TRAINING SET'}), how='outer') \\\n    .plot(figsize=(15,5), title='PJM East', style='.')","9e08fcf3":"def create_features(df, label=None):\n    \"\"\"\n    Creates time series features from datetime index\n    \"\"\"\n    df['date'] = df.index\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    X = df[['hour','dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear']]\n    if label:\n        y = df[label]\n        return X, y\n    return X","58d46cd9":"X_train, y_train = create_features(pjme_train, label='PJME_MW')\nX_test, y_test = create_features(pjme_test, label='PJME_MW')","a3ffe8ea":"frames = []\n\nx1=0\n\nfor n in chain(range(1,50+1),range(60,300+1,10),range(325,1000+1,25)):    \n    y=n-x1\n    x1=n\n    reg = xgb.XGBRegressor(n_estimators=y)\n    if n==1:\n        model = reg.fit(X_train, y_train,\n        eval_set=[(X_train, y_train), (X_test, y_test)],\n        verbose=False)\n        model.save_model('model.model')\n        \n        pjme_test['MW_Prediction'] = model.predict(X_test)\n        pjme_all = pd.concat([pjme_test, pjme_train], sort=False)\n        \n        mae_test = mean_absolute_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test['MW_Prediction'])\n        mae_train = model.evals_result()['validation_0']['rmse'][0]\n        \n        mae_plot = pd.DataFrame({'mae_test': [mae_test], \n                                 'mae_train': [mae_train],\n                                 'n': [n]})\n        \n        plot1 = pjme_all[['MW_Prediction','PJME_MW']].plot(style=['.','-'],figsize=(14,6))\n        plot1.set_ylim(0, 60000)\n        plot1.set_xbound(lower='06-01-2016', upper='06-08-2016')\n        plt.suptitle('First week of August 2016, Actual vs Predicted')\n        plt.annotate(s='# of rounds: '+str(n), xy=(0.6, 0.2), xycoords='axes fraction',\n            fontsize=30)\n        plt.annotate(s='MAE train: '+str(round(mae_train,1)), xy=(0.4, 0.9), xycoords='axes fraction',\n            fontsize=20)\n        plt.annotate(s='MAE test: '+str(round(mae_test,1)), xy=(0.4, 0.8), xycoords='axes fraction',\n            fontsize=20)\n        plt.savefig('plot'+str(n))\n        new_frame = Image.open('plot'+str(n)+'.png')\n        frames.append(new_frame)\n        \n    else:\n        model = reg.fit(X_train, y_train,\n        eval_set=[(X_train, y_train), (X_test, y_test)],\n        verbose=False, xgb_model='model.model')\n        model.save_model('model.model')\n        \n        pjme_test['MW_Prediction'] = model.predict(X_test)\n        pjme_all = pd.concat([pjme_test, pjme_train], sort=False)\n        \n        mae_test = mean_absolute_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test['MW_Prediction'])\n        mae_train = model.evals_result()['validation_0']['rmse'][0]\n        \n        mae_plot = mae_plot.append(pd.DataFrame({'mae_test': [mae_test], \n                                                 'mae_train': [mae_train],\n                                                 'n': [n]}))\n        plot1 = pjme_all[['MW_Prediction','PJME_MW']].plot(style=['.','-'],figsize=(14,6))\n        plot1.set_ylim(0, 60000)\n        plot1.set_xbound(lower='06-01-2016', upper='06-08-2016')\n        plt.suptitle('First week of August 2016, Actual vs Predicted')\n        plt.annotate(s='# of rounds: '+str(n), xy=(0.6, 0.2), xycoords='axes fraction',\n            fontsize=30)\n        plt.annotate(s='MAE train: '+str(round(mae_train,1)), xy=(0.4, 0.9), xycoords='axes fraction',\n            fontsize=20)\n        plt.annotate(s='MAE test: '+str(round(mae_test,1)), xy=(0.4, 0.8), xycoords='axes fraction',\n            fontsize=20)\n        \n        plt.savefig('plot'+str(n))\n        new_frame = Image.open('plot'+str(n)+'.png')\n        frames.append(new_frame)\n    print(n)\n;","cab8c8d7":"frames[0].save('png_to_gif.gif', format = 'GIF',\n              append_images = frames[1:],\n              save_all=True,\n              duration=100,loop=0)","fff4dcc2":"plot3 = mae_plot.plot(x='n',figsize=(12,6))\nplot3.set_ylim(2000,4000)\nplt.suptitle('MAE of train vs test set over the number of iterations')","29330df3":"The following function adds time features to the data based on the **Datetime** column.","da779e46":"### Overfitting can be a difficult concept to wrap one's head around. Below is a visual demonstration of how a XGBoost regressor model output changes for the same input with more training iterations.\n![](png_to_gif.gif)\n\n### The rest of the kernel showcases how to get this visualisation.\n#### We are going to be using a regressor model on the Energy Consumption dataset, trying to predict nominal energy consumption levels.","fb44aa90":"![](png_to_gif.gif)","eb8f6b6e":"First of all, it helps to visualize the data we are working with.\n\nPJME dataset is an hourly account of energy consumption on the East Coast of the US, in the years 2002-2019.","2f60a9f7":"Making the GIF animation:","259abceb":"**The above graph showcases the gradual disparity between the train and test MAE. In this specific case, the prediction accuracy peaks at around 150 iterations.**","9383e905":"**As can be observed in the animation, the model fits the data better up to a certain point (measured by Test MAE), but gradually deteriorates after.**\n\n**This can be observed in the oddly specific shapes that the dotted line starts morphing into after approximately 300 rounds. The model begins to learn shapes that, although fit the training data better, are too specific for predicting future data.**","e1f94008":"The following loop:\n* Trains the model, 1 iteration at a time\n* Saves the predicted set and the MAE (mean absolute error) of both the training set and the testing set\n* Saves a snapshot of a graph of the predicted vs actual values for 1 week of data to later make a GIF","3bc9512b":"Train\/test split:\n\n* Train: 2002-2014\n* Test: 2015-2019"}}