{"cell_type":{"dae2d095":"code","314bc5b5":"code","0045f7ca":"code","54148916":"code","63470e16":"code","4c6fec9d":"code","dedfecd2":"code","92969aea":"code","2e1e847b":"code","9c36ea6a":"code","5a95e398":"code","c147310e":"code","292796ed":"code","e62c75f2":"code","65218589":"code","8a454027":"code","4ed64d5b":"code","26d4e711":"code","5a6ab5f7":"code","d1e8c8d2":"code","4bb52630":"code","57d2812d":"code","91573e2e":"markdown","3a3ebfce":"markdown","da7d6bd2":"markdown","41a560ac":"markdown","e9a51e1e":"markdown","993381bb":"markdown","41068f70":"markdown","1bb325cb":"markdown","f71eb105":"markdown","c1c350bd":"markdown","1b7c1f9a":"markdown","5a84729a":"markdown","93f6af2e":"markdown","3392e3c3":"markdown","23ddf237":"markdown","58b295cc":"markdown","f0d77b0d":"markdown","59e0ac4f":"markdown","01407ca7":"markdown","e9e7e69a":"markdown","7c09c559":"markdown","d09238af":"markdown","10990026":"markdown","ad39ae74":"markdown","e434061f":"markdown","9b89fdd4":"markdown"},"source":{"dae2d095":"# Librerie\nimport gc\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","314bc5b5":"def load_dataset():\n    # Carichiamo le immagini in memoria\n    images = []\n    labels = []\n\n    # Esploriamo la cartella con i dati\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n\n        # Le sottocartelle con i files sono quelle con l'etichetta\n        # come ultima sottocartella.\n        # Otteniamo il nome dell'ultima sottocartella.\n        dirlabel = dirname.split('\/')[-1]\n\n        # Carichiamo le immagini\n        if dirlabel in ['mountain', 'street', 'buildings', 'sea', 'forest', 'glacier']:\n            for filename in filenames:\n                labels.append(dirlabel)\n                images.append(\n                                tf.keras.preprocessing.image.load_img(os.path.join(dirname,\n                                                                                   filename),\n                                                                      color_mode=\"rgb\",\n                                                                      target_size=(150, 150))\n                             )\n    # Mescoliamo il dataset\n    dataset = list(zip(labels, images))\n    random.seed(1604)\n    random.shuffle(dataset)\n    labels, images = zip(*dataset)\n    del dataset\n\n    # Trasformiamo le immagini in arrays di numpy\n    x_data = np.zeros((len(images), 150, 150, 3), dtype=np.float32)\n    for index, item in enumerate(images):\n        x_data[index,:,:,:] = tf.keras.preprocessing.image.img_to_array(item,\n                                                                        dtype=np.float32) \\\n                                                                        \/ 255.0\n    del images\n\n    # Trasformiamo le labels in array di numpy\n    y_data = np.zeros(len(labels))\n    for index, item in enumerate(labels):\n        if item == 'mountain':\n            y_data[index] = 0\n        elif item == 'street':\n            y_data[index] = 1\n        elif item == 'buildings':\n            y_data[index] = 2\n        elif item == 'sea':\n            y_data[index] = 3\n        elif item == 'forest':\n            y_data[index] = 4\n        elif item == 'glacier':\n            y_data[index] = 5\n        else:\n            raise ValueError('Etichetta non riconosciuta!')\n    del labels\n\n    # Separiamo il dataset in training and test set\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.25,\n                                                        random_state=2021)\n    del x_data, y_data\n    \n    return x_train, x_test, y_train, y_test","0045f7ca":"%%time\n\n# Carichiamo il dataset in memoria\nx_train, x_test, y_train, y_test = load_dataset()","54148916":"# Visualizziamo 15 immagini con Matplotlib\nfig, axes = plt.subplots(3,5, figsize=(25, 15))\nfor index1 in range(3):\n    for index2 in range(5):\n        axes[index1, index2].imshow(x_train[index1*5 + index2, :, :, :])\n        axes[index1, index2].axis(\"off\")\nfig.suptitle(\"Images included in the Intel Image Classification Dataset\")\nplt.show()\nplt.close()","63470e16":"# Appiattiamo le immagini\nx_train_flat = x_train.reshape(x_train.shape[0], -1)\nx_test_flat = x_test.reshape(x_test.shape[0], -1)\nprint(f\"Shape of training design matrix: {x_train_flat.shape}\")\nprint(f\"Shape of test design matrix: {x_test_flat.shape}\")\n\n# Liberiamo memoria che \u00e8 poca\ndel x_train, x_test","4c6fec9d":"%%time\n# Algoritmo PCA\npca = PCA(n_components=3600, random_state=2021)\npca.fit(x_train_flat)\nx_train_mini = pca.transform(x_train_flat)\nx_test_mini = pca.transform(x_test_flat)\nprint(f'Number of components: {pca.n_components_}')\nprint(f'Explained variance ratio: {np.sum(pca.explained_variance_ratio_)}')","dedfecd2":"# Riportiamo i dati ridotti alla loro dimensionalit\u00e0 originale (con perdita di informazione)\nx_train_lossy = pca.inverse_transform(x_train_mini[0:5,:])\nx_train_lossy = x_train_lossy.reshape(5, 150, 150, 3)\n\n# Recuperiamo i dati originali nella loro shape corretta\nx_train_lossless = x_train_flat[0:5,:].reshape(5, 150, 150, 3)\n\n# Visualizziamo le immagini\nfig, axes = plt.subplots(5,3, figsize=(12, 25))\nfor index in range(5):\n    # Immagine originale\n    axes[index, 0].imshow(x_train_lossless[index, :, :, :])\n    axes[index, 0].axis(\"off\")\n    # Immagine compressa con PCA\n    axes[index, 1].imshow(x_train_lossy[index, :, :, :])\n    axes[index, 1].axis(\"off\")\n    # Immagine compressa con un semplice resize\n    axes[index, 2].imshow(tf.image.resize(x_train_flat[index,:].reshape(150, 150, 3),\n                                          size=(35,35), antialias=True))\n    axes[index, 2].axis(\"off\")\nfig.suptitle(\"From left to right: original image, compressed with PCA, compressed with downsampling.\")\nplt.tight_layout()\nplt.show()\nplt.close()","92969aea":"%%time\n\n# Regressione logistica\nregr_log = LogisticRegression(multi_class=\"ovr\", max_iter=1000)\nregr_log.fit(x_train_mini, y_train)\n\n# Valutazione delle performance\ntrain_accuracy = regr_log.score(x_train_mini, y_train)\ntest_accuracy = regr_log.score(x_test_mini, y_test)\n\n# Stampa a schermo risultati\nprint(f\"Training accuracy: {train_accuracy}\")\nprint(f\"Test accuracy: {test_accuracy}\")","2e1e847b":"# Create a simple NN model\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(1000, activation=\"relu\", input_shape=(3600,)))\nmodel.add(tf.keras.layers.Dense(250, activation=\"relu\"))\nmodel.add(tf.keras.layers.Dense(6, activation=\"softmax\"))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\nmodel.summary()\n\n# Train the network\ny_train_one_hot = tf.keras.utils.to_categorical(y_train, 6)\ny_test_one_hot = tf.keras.utils.to_categorical(y_test, 6)\nhistory = model.fit(x_train_mini, y_train_one_hot, epochs=50, batch_size=32, verbose=1)\naccuracy = model.evaluate(x_test_mini, y_test_one_hot, batch_size=32)[1]\nprint(f\"Training accuracy: {history.history['accuracy'][-1]}\")\nprint(f\"Test accuracy: {accuracy}\")","9c36ea6a":"# Recupera i dati originale nella dimensione corretta e rimuovi quelli flattened\nx_train = x_train_flat.reshape(x_train_flat.shape[0], 150, 150, 3)\nx_test = x_test_flat.reshape(x_test_flat.shape[0], 150, 150, 3)\ndel x_train_flat, x_test_flat\n\n# Libera memoria che \u00e8 poca\ndel pca, regr_log\ntf.keras.backend.clear_session()\ngc.collect() # chiama il garbage collector per sicurezza\n\n# Crea una convolutional neural network basilare\ncnn = tf.keras.Sequential()\ncnn.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", input_shape=(150, 150, 3)))\ncnn.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 4)))\ncnn.add(tf.keras.layers.Conv2D(64, kernel_size=(5, 5), activation=\"relu\"))\ncnn.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 4)))\ncnn.add(tf.keras.layers.Flatten())\ncnn.add(tf.keras.layers.Dense(6, activation=\"softmax\"))\ncnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\ncnn.summary()\n\n# Alleniamo il modello e testiamolo sul test set\nhistory = cnn.fit(x_train, y_train_one_hot, epochs=25, batch_size=32, verbose=1)\naccuracy = cnn.evaluate(x_test, y_test_one_hot, batch_size=32)[1]\nprint(f\"Training accuracy: {history.history['accuracy'][-1]}\")\nprint(f\"Test accuracy: {accuracy}\")","5a95e398":"# Libera memoria che \u00e8 poca\ntf.keras.backend.clear_session()\ngc.collect() # chiama il garbage collector per sicurezza\n\n# Crea una convolutional neural network un po' pi\u00f9 elaborata\ncnn = tf.keras.Sequential()\ncnn.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", input_shape=(150, 150, 3)))\ncnn.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 4)))\ncnn.add(tf.keras.layers.Conv2D(64, kernel_size=(5, 5), activation=\"relu\"))\ncnn.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 4)))\ncnn.add(tf.keras.layers.Flatten())\ncnn.add(tf.keras.layers.Dropout(0.5))\ncnn.add(tf.keras.layers.Dense(6, activation=\"softmax\"))\ncnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\ncnn.summary()\n\n# Prepara il generatore dei minibatches per il training\ny_train_one_hot = tf.keras.utils.to_categorical(y_train, 6)\ny_test_one_hot = tf.keras.utils.to_categorical(y_test, 6)\ngenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n)\n\n\n# Alleniamo il modello e testiamolo sul training set\nhistory = cnn.fit(generator.flow(x_train, y_train_one_hot, batch_size=32),\n                  steps_per_epoch=len(x_train) \/ 32, epochs=50, verbose=1)\ntraining_accuracy_without_dropout = cnn.evaluate(x_train, y_train_one_hot, batch_size=32)[1]\naccuracy = cnn.evaluate(x_test, y_test_one_hot, batch_size=32)[1]\nprint(f\"Training accuracy (with Dropout): {history.history['accuracy'][-1]}\")\nprint(f\"Training accuracy (without Dropout): {training_accuracy_without_dropout}\")\nprint(f\"Test accuracy: {accuracy}\")","c147310e":"# Visualizziamo i filtri con matplotlib\nfilters = cnn.layers[0].get_weights()[0]\nfig, axes = plt.subplots(32, 3, figsize=(9,96))\nfor index in range(32):\n    for channel in range(3):\n        axes[index, channel].imshow(filters[:,:,channel,index], cmap=\"gray\")\n        axes[index, channel].axis(\"off\")\nplt.show()","292796ed":"# Creiamo il modello (senza MaxPooling per avere una risoluzione migliore)\nfirst_layer = tf.keras.Sequential()\nfirst_layer.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", input_shape=(150, 150, 3)))\nfirst_layer.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# Ora impostiamo i pesi del modello precedente\nfirst_layer.layers[0].set_weights(cnn.layers[0].get_weights())\n\n# Diamo in pasto alla rete 5 immagini\nactivations = first_layer.predict(x_train[0:5,:,:,:])","e62c75f2":"# Visualizziamo ora le cinque immagini e gli output della rete dopo il layer Conv2D\nfor i in range(5):\n    # Plottiamo l'immagine originale\n    plt.figure(figsize=(5,5))\n    plt.imshow(x_train[i])\n    plt.title(\"Original image\")\n    plt.show()\n    plt.close()\n    \n    # Plottiamo gli output\n    fig, axes = plt.subplots(8, 4, figsize=(24, 48))\n    for index in range(8):\n        for channel in range(4):\n            axes[index, channel].imshow(activations[i,:,:,index*4+channel], cmap=\"gray\")\n            axes[index, channel].axis(\"off\")\n    plt.show()\n    plt.close()","65218589":"# Proseguiamo vedendo l'attivazione sul secondo layer (senza l'ultimo MaxPooling per avere una migliore risoluzione)\nsecond_layer = tf.keras.Sequential()\nsecond_layer.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", input_shape=(150, 150, 3)))\nsecond_layer.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 4)))\nsecond_layer.add(tf.keras.layers.Conv2D(64, kernel_size=(5, 5), activation=\"relu\"))\nsecond_layer.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# Ora impostiamo i pesi del modello precedente\nsecond_layer.layers[0].set_weights(cnn.layers[0].get_weights())\nsecond_layer.layers[2].set_weights(cnn.layers[2].get_weights())\n\n# Diamo in pasto alla rete le 5 immagini di prima\nactivations_second = second_layer.predict(x_train[0:5,:,:,:])","8a454027":"# Visualizziamo ora le cinque immagini e gli output della rete dopo il secondo layer Conv2D\nfor i in range(2):\n    # Plottiamo l'immagine originale\n    plt.figure(figsize=(5,5))\n    plt.imshow(x_train[i])\n    plt.title(\"Original image\")\n    plt.show()\n    plt.close()\n    \n    # Plottiamo gli output\n    fig, axes = plt.subplots(16, 4, figsize=(24, 96))\n    for index in range(16):\n        for channel in range(4):\n            axes[index, channel].imshow(activations_second[i,:,:,index*4+channel], cmap=\"gray\")\n            axes[index, channel].axis(\"off\")\n    plt.show()\n    plt.close()","4ed64d5b":"# Definiamo una funzione che riceve come input dei parametri che regolano la complessit\u00e0\n# di una CNN, esegue il training e restituisce (num_params, training_accuracy, test_accuracy)\ndef build_cnn_incremental(num_layers=1, filter_multiplier=1):\n    \n    # Costruisci e allena la rete\n    tf.keras.backend.clear_session()\n    cnn = tf.keras.Sequential()\n    cnn.add(tf.keras.layers.Input(shape=(150, 150, 3)))\n    for index in range(num_layers):\n        cnn.add(tf.keras.layers.Conv2D(4*(index+1)*filter_multiplier, kernel_size=(5, 5), activation=\"relu\"))\n        cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 4)))\n    cnn.add(tf.keras.layers.Flatten())\n    cnn.add(tf.keras.layers.Dense(6, activation=\"softmax\"))\n    cnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    #cnn.summary()\n    history = cnn.fit(x_train, y_train_one_hot, epochs=40, batch_size=32, verbose=0)\n    \n    # Calcola i risultati\n    num_params = cnn.count_params()\n    training_accuracy = history.history['accuracy'][-1]\n    test_accuracy = cnn.evaluate(x_test, y_test_one_hot, batch_size=32, verbose=0)[1]\n    \n    return num_params, training_accuracy, test_accuracy","26d4e711":"# Alleniamo e misuriamo le performance delle reti neurali al variare della complessit\u00e0 del modello.\nparams_1 = np.zeros(7)\nt_acc_1 = np.zeros(7)\nacc_1 = np.zeros(7)\nfor index in range(7): \n    print(f\"Test {index+1}\")\n    params_temp, t_acc_temp, acc_temp = build_cnn_incremental(num_layers=1, filter_multiplier=2**index)\n    gc.collect()\n    params_1[index]= params_temp\n    t_acc_1[index] = t_acc_temp\n    acc_1[index]= acc_temp\n    print(f\"Number of parameters: {params_temp}\")\n    print(f\"Training accuracy:    {t_acc_temp}\")\n    print(f\"Test accuracy:        {acc_temp}\")","5a6ab5f7":"# Alleniamo e misuriamo le performance delle reti neurali al variare della complessit\u00e0 del modello.\nparams_2 = np.zeros(7)\nt_acc_2 = np.zeros(7)\nacc_2 = np.zeros(7)\nfor index in range(7):\n    print(f\"Test {index+1}\")\n    params_temp, t_acc_temp, acc_temp = build_cnn_incremental(num_layers=2, filter_multiplier=2**index)\n    gc.collect()\n    params_2[index]= params_temp\n    t_acc_2[index] = t_acc_temp\n    acc_2[index]= acc_temp\n    print(f\"Number of parameters: {params_temp}\")\n    print(f\"Training accuracy:    {t_acc_temp}\")\n    print(f\"Test accuracy:        {acc_temp}\")","d1e8c8d2":"# Alleniamo e misuriamo le performance delle reti neurali al variare della complessit\u00e0 del modello.\nparams_3 = np.zeros(7)\nt_acc_3 = np.zeros(7)\nacc_3 = np.zeros(7)\nfor index in range(7):\n    print(f\"Test {index+1}\")\n    params_temp, t_acc_temp, acc_temp = build_cnn_incremental(num_layers=3, filter_multiplier=2**index)\n    gc.collect()\n    params_3[index]= params_temp\n    t_acc_3[index] = t_acc_temp\n    acc_3[index]= acc_temp\n    print(f\"Number of parameters: {params_temp}\")\n    print(f\"Training accuracy:    {t_acc_temp}\")\n    print(f\"Test accuracy:        {acc_temp}\")","4bb52630":"# Plottiamo ora i risultati ottenuti\nimport matplotlib\nmatplotlib.rcParams.update({'font.size': 16})\nfig = plt.figure(figsize=(16,9))\nplt.title(\"Accuracy vs number of parameters\")\nplt.plot(params_1, t_acc_1, label=\"Training accuracy (depth 1)\", color='k', ls='--')\nplt.plot(params_1, acc_1, label=\"Test accuracy (depth 1)\", color='k')\nplt.plot(params_2, t_acc_2, label=\"Training accuracy (depth 2)\", color='r', ls='--')\nplt.plot(params_2, acc_2, label=\"Test accuracy (depth 2)\", color='r')\nplt.plot(params_3, t_acc_3, label=\"Training accuracy (depth 3)\", color='b', ls='--')\nplt.plot(params_3, acc_3, label=\"Test accuracy (depth 3)\", color='b')\nplt.legend()\nplt.xscale('log')\nplt.xlabel(\"Number of parameters\")\nplt.ylabel(\"Accuracy\")\nplt.grid()\nplt.show()\nplt.close()","57d2812d":"y_train_one_hot = tf.keras.utils.to_categorical(y_train, 6)\ny_test_one_hot = tf.keras.utils.to_categorical(y_test, 6)\n\n# Libera memoria che \u00e8 poca\ntf.keras.backend.clear_session()\ngc.collect() # chiama il garbage collector per sicurezza\n\nvgg16_model = tf.keras.applications.VGG16(\n    include_top=False,\n    weights=\"imagenet\",\n    input_shape=(150,150,3)\n)\nfor layer in vgg16_model.layers:\n     layer.trainable = False\nlast_layer = tf.keras.layers.Flatten()(vgg16_model.layers[-1].output)\nlast_layer = tf.keras.layers.Dense(1024, activation=\"relu\")(last_layer)\nlast_layer = tf.keras.layers.Dropout(0.2)(last_layer)\nlast_layer = tf.keras.layers.Dense(6, activation=\"softmax\")(last_layer)\nmodel = tf.keras.Model(vgg16_model.input, last_layer)\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()\n\n# Eliminiamo il modello precedente\ndel vgg16_model\ngc.collect()\n\n# Alleniamo il modello e testiamolo sul training set\nhistory = model.fit(x_train, y_train_one_hot, epochs=25, batch_size=32, verbose=1)\naccuracy = model.evaluate(x_test, y_test_one_hot, batch_size=32)[1]\nprint(f\"Training accuracy: {history.history['accuracy'][-1]}\")\nprint(f\"Test accuracy: {accuracy}\")","91573e2e":"### 2. Riduzione della dimensionalit\u00e0 delle immagini con la tecnica Principal Component Analysis.\n<div style=\"text-align: justify\">\nLe immagini sono relativamente grandi (150x150x3) per cui \u00e8 interessante provare ad applicare la <strong>Principal Component Analysis (PCA)<\/strong> [5] per ridurre la dimensionalit\u00e0 nella speranza di semplificare l'analisi successiva senza perdere troppa informazione riguardo al dataset originale.\nCi aspettiamo infatti che non tutti i predittori siano ugualmente importanti.\nAd esempio, i bordi dell'immagine potrebbero non contenere informazioni sul soggetto.\nInoltre ci si aspetta che i pixel vicini abbiano valori correlati.<br>\nLa PCA \u00e8 una tecnica di riduzione della dimensionalit\u00e0 basata sul proiettare i dati su un sottospazio dello spazio dei predittori in maniera tale da conservare la maggior varianza possibile dei dati.\nAll'atto pratico, dati $N$ campionamenti di un dataset con $P$ predittori, si ottengono gli stessi $N$ campionamenti descritti con un numero $p<P$ di predittori.\nUtilizzando il formalismo della matrice di design, data la matrice $X$ di dimensione $N \\times P$, la si comprime in una matrice $N \\times p$ con $p < P$.\nLa compressione \u00e8 ottimale rispetto alla varianza totale dei dati, ovvero alla somma della varianza delle colonne di $X$.<br><br>\nLa procedura \u00e8 la seguente:\n<ol>\n    <li>Traslare i valori di ciascun predittore in maniera che il suo valor medio rispetto ai campionamenti sia 0. In pratica occorre sottrarre a ciascuna colonna di $X$ il suo valor medio.<\/li>\n    <li>Calcolare la decomposizione ai valori singolari di $X$, ovvero $X = U \\Sigma V^T$.<\/li>\n    <li>Prendere le prime $p$ colonne di V, e costruire con esse la matrice $\\tilde{V}$.<\/li>\n    <li>Eseguire la trasformazione $X_{PCA}=X\\tilde{V}$.<\/li>\n<\/ol>\nUna figura di merito che possiamo usare per valutare la qualit\u00e0 della compressione \u00e8 l'<strong>explained variance ratio<\/strong>, ovvero il rapporto tra la somma della varianza dei nuovi $p$ predittori e la somma della varianza dei $P$ predittori originali. In pratica, si calcola il rapporto tra la somma dei quadrati dei primi $p$ valori singolari di $X$ e la somma dei quadrati di tutti i $P$ valori singolari.<br><br>\nOccupiamoci ora dei dettagli di codice.\n    La prima cose che dobbiamo fare \u00e8 \"appiattire\" (in inglese <em>Flatten<\/em>) le matrici con le immagini, in maniera tale che la matrice di design sia effettivamente una matrice e non un array quadridimensionale.\n<\/div>\n","3a3ebfce":"<div style=\"text-align: justify\">\n    <strong>Commento:<\/strong> abbiamo ottenuto un'accuratezza sul test set di circa 55% - 60%.\n    Le performance sono migliorate rispetto alla regressione logistica!\n    Osserviamo per\u00f2 che anche in questo caso la rete overfitta molto, nel senso che la rete sembra imparare praticamente a memoria il training set ma senza imparare ad estrarre le features che caratterizzano veramente le diverse categorie di immagini.\n<\/div>","da7d6bd2":"<div style=\"text-align: justify\">\n    <strong>Commento:<\/strong> come possiamo vedere, l'accuratezza di test \u00e8 migliorata e la differenza tra l'accuratezza di training e di test si \u00e8 ridotta, segno che abbiamo ridotto l'overfitting.\n    Notiamo che il training in questo caso \u00e8 molto pi\u00f9 lento di prima.\n    Durante l'esecuzione si pu\u00f2 notare che l'occupazione della GPU \u00e8 molto bassa, segno che la Data Augmentation in tempo reale crea un collo di bottiglia e impedisce alla GPU di esprimere il suo potenziale. Magari questa \u00e8 una conseguenza del fatto che le immagini che stiamo usando sono modestamente grandi, mentre il nostro modello \u00e8 piccolo in confronto alle architetture pi\u00f9 famose e la GPU in uso \u00e8 decisamente sproporzionata. Per fare considerazioni pi\u00f9 precise servirebbe pi\u00f9 esperienza.<br><br>\n    Ora che abbiamo ottenuto un modello discretamente funzionante, \u00e8 interessante provare a visualizzare i filtri dei nostri layer convoluzionali, per quanto sia possibile dato il numero di essi e la loro dimensionalit\u00e0.\n    L'idea sarebbe quella di provare a dare un'occhiata \"under the hood\" e provare a capire come effettivamente funziona la rete.\n    Il primo layer convoluzionale ha filtri di dimensione 5x5x3, dato che 5x5 \u00e8 la dimensione spaziale e bisogna operare sui 3 canali di colore. Di conseguenza, i filtri saranno 32x3 immagini 5x5, plottate qui sotto.\n    Per quanto riguarda il secondo layer convoluzionale, ciascun filtro sar\u00e0 di dimensione 5x5x32, dove 5x5 \u00e8 la dimensione spaziale e bisogna operare sui 32 canali del layer precedente.\n    Per motivi pratici non conviene plottare questi filtri.\n<\/div>","41a560ac":"<div style=\"text-align: justify\">\n    Facciamo alcune osservazioni:\n    <ul><li>L'accuratezza di test in questo caso non si potr\u00e0 pi\u00f9 utilizzare per fare <strong>model assessment<\/strong>, ma solo per fare <strong>model selection<\/strong> [6]. Per calcolare la performance finale del modello servir\u00e0 un altro dataset indipendente; abbiamo un dataset ulteriore di 7000 immagini ma le etichette non sono disponibili perch\u00e8 veniva utilizzato nella competizione originale per valutare i modelli migliori evitando bari. Se uno era interessato alle performance del modello migliore, e non semplicemente a studiare la dipendenza delle performance dall'architettura, poteva effettuare le misure su un <strong>validation set<\/strong> e poi valutare il modello migliore sul test set.<\/li>\n        <li>L'accuratezza sul training set aumenta all'aumentare del numero di parametri, in accordo con il fatto che all'aumentare della complessit\u00e0 del modello aumenta anche la sua capacit\u00e0 espressiva. In pratica diminuisce il bias del modello rispetto alla funziona esatta sottostante al processo di etichettamento.<\/li>\n        <li>Se consideriamo l'accuratezza sul test set a profondit\u00e0 fissata, vediamo che non \u00e8 monotona con il numero di parametri. In generale aumenta nella fase iniziale, e poi sembra abbastanza costante o decresce leggermente.<\/li>\n        <li>L'accuratezza sul test set aumenta all'aumentare della profondit\u00e0 del modello.<\/li>\n        <li>Il \"generalization gap\", cio\u00e8 la differenza tra la figura di merito sul training set e sul test set, aumenta all'aumentare del numero di parametri, partendo da un valore basso e poi aumentando mano a mano che aumenta la complessit\u00e0 del modello. In pratica, aumentando la complessit\u00e0 del modello aumentiamo di poco la performance sul test set mentre la performance sul training set raggiunge valori tra il 95% e il 100%, indicando che c'\u00e8 molto overfitting e i parametri aggiuntivi vengono usati per apprendere \"a memoria\" il training set e non per apprendere le vere features che caratterizzano i soggetti nelle immagini.<\/li><\/ul>\n<\/div>","e9a51e1e":"### 1. Lettura e preprocessing base del dataset\n<div style=\"text-align: justify\">\nIn questo paragrafo ci occupiamo di leggere il dataset dal disco, e di occuparci di tutti i dettagli di micromanagement che permettono poi di lavorare agevolmente per il resto del notebook.<br>\nIn particolare, il dataset \u00e8 salvato in una serie di cartelle sotto <code>\/kaggle\/input\/<\/code>.\nIl nome dell'ultima cartella rappresenta la categoria che l'immagine rappresenta, e le immagini sono salvate come file JPEG.\nNoi vogliamo leggere tutte le immagini, attribuendo ad ognuna la giusta categoria, e poi salvarle come <strong>matrice di design<\/strong> $X$, che in questo caso sar\u00e0 un array 4D con dimensione <code>(num_images, width, height, RGB_channels) = (num_images, 150, 150, 3)<\/code>.\nPer essere precisi, l'elemento $X_{ijkl}$ rappresenter\u00e0 l'intensit\u00e0 del canale del colore con indice $l$ del pixel in posizione $(j,k)$ dell'immagine numero $i$.\nL'intensit\u00e0 andr\u00e0 divisa per 255 per fare s\u00ec che $X_{ijkl} \\in [0,1]$.\nA fianco della matrice di design $X$ servir\u00e0 anche il vettore $y$ di dimensione <code>(num_images)<\/code> con le categorie delle immagini codificate in un numero intero.<br><br>\n<strong>Dettagli implementativi:<\/strong> per fare questo abbiamo usato le funzioni di <strong>Keras<\/strong> [3] <code>tf.keras.preprocessing.image.load_img<\/code>, che legge l'immagine in JPEG e la restitusce come immagine PIL (definita nella libreria <a href=https:\/\/pillow.readthedocs.io\/en\/stable\/index.html>Pillow<\/a>), e <code>tf.keras.preprocessing.image.img_to_array<\/code> che la trasforma in un array di NumPy con i valori grezzi dei pixel.<br><br>\nInfine, abbiamo separato il dataset in training e test set in proporzione 75% - 25%, utilizzando la funzione <code>sklearn.model_selection.train_test_split<\/code> di <strong>Scikit-learn<\/strong> [2]. Il training set verr\u00e0 utilizzato per allenare i modelli, mentre il test set verr\u00e0 utilizzato per valutare la performance dei modelli.<br><br>\n    <strong>Commento:<\/strong> nonostante il dataset in formato JPEG fosse relativamente piccolo (circa 350 MB), una volta trasformato nella matrice di design (contenente i valori grezzi delle intensit\u00e0 dei pixel) diventa molto voluminoso, arrivando a consumare gran parte dei 13 GB di RAM a disposizione. Questo fa apprezzare quanto sia utile la codifica delle immagini, ma ci fa capire che dovremo stare attenti a non sprecare memoria in futuro.\n<\/div>","993381bb":"### Appendice: classificazione con VGG16 pre-allenato su ImageNet\nPer concludere, mostriamo come si comporta il modello VGG16 preallenato sul dataset ImageNet e riallenato parzialmente sul dataset Intel. In particolare abbiamo sostituito e riallenato i layer fully-connected presenti alla fine del modello.\n\nFonte: https:\/\/www.kaggle.com\/janvichokshi\/transfer-learning-cnn-resnet-vgg16-iceptionv3 <br>\nDescrizione VGG16: https:\/\/cs231n.github.io\/convolutional-networks\/ [8]<br>\nDataset ImageNet: http:\/\/www.image-net.org\/","41068f70":"<div style=\"text-align: justify\">\n    <strong>Commento:<\/strong> abbiamo ottenuto un'accuratezza di circa 80%, che \u00e8 decisamente migliore dei risultati ottenuti precedentemente. Vediamo comunque che l'accuratezza di training continua a essere molto maggiore di quella di test: proviamo quindi a complicare un po' il modello utilizzando delle tecniche di regolarizzazione, ovvero tecniche per ridurre l'overfitting e migliorare quindi la generalizzazione dei modelli. Utilizzeremo il <strong>Dropout<\/strong> e la <strong>Data Augmentation<\/strong>.<br><br>\n    Un layer di <a href=https:\/\/keras.io\/api\/layers\/regularization_layers\/dropout\/><strong>Dropout<\/strong><\/a> [7] \u00e8 un layer che azzera randomicamente e indipendentemente i suoi input secondo un determinato rate $r$. Gli input rimanenti vengolo riscalati di $1 \/ (1-r)$ in maniera tale che la somma su tutti gli input rimanga invariata. Il layer di Dropout \u00e8 attivo solo durante il training, e non durante la valutazione e l'uso dei modelli. Questo layer \u00e8 implementato nella classe <code>tf.keras.layers.Dropout<\/code>.<br><br>\n    Vediamo adesso il metodo di Data Augmentation [7] (nel caso di un dataset di immagini).\n    Ci sono alcune trasformazioni dei dati di input sotto le quali il nostro problema \u00e8 invariante.\n    Ad esempio, nel nostro caso ci aspettiamo che delle piccole rotazioni, traslazioni e zoom non debbano cambiare il soggetto inquadrato.\n    La stessa cosa si pu\u00f2 dire a riguardo dei ribaltamenti orizzontali.\n    Allora, il metodo di Data Augmentation consiste nell'applicare in maniera casuale queste trasformazioni ai dati di input durante la procedura di training, in maniera tale da avere un dataset \"efficacie\" pi\u00f9 grande e ottenere un modello che generalizza meglio. \n    Possiamo accedere a questo metodo usando la classe di Keras <code>tf.keras.preprocessing.image.ImageDataGenerator<\/code>.\n<\/div>","1bb325cb":"Partiamo studiando reti di profondit\u00e0 1 e aumentando i filtri:","f71eb105":"<div style=\"text-align: justify\">\n    <strong>Commento:<\/strong> l'accuratezza ottenuta \u00e8 di circa 40%-45%, che \u00e8 maggiore di un classificatore random 17%, ma sembra comunque abbastanza deludente. Osserviamo inoltre che l'accuratezza ottenuta sul training set \u00e8 90%, di conseguenza la rete overfitta tantissimo (cio\u00e8 apprende bene il training set ma non \u00e8 in grado di generalizzare a nuovi esempi). Inoltre, osserviamo che nonostante abbiamo aumentato di un ordine di grandezza le iterazioni massime per l'algoritmo di ottimizzazione, quest'ultimo non sia comunque arrivato a convergenza. Scikit-Learn non supporta l'accelerazione tramite GPU, quindi non \u00e8 possibile sfruttarla per ridurre i tempi di calcolo.\n<\/div>","c1c350bd":"<div style=\"text-align: justify\">\nAbbiamo quindi ridotto la dimensionalit\u00e0 degli input da 67500 a 3600 con un <em>explained variance ratio<\/em> di circa 0.95. Potremmo chiederci quanto questa compressione sia buona. Dato che sono immagini, \u00e8 possibile effettuare il confronto visivo tra le immagini originali e le immagini compresse e poi riportate nello spazio originale. La trasformazione inversa \u00e8 $$X_{\\mbox{lossy}} = X_{PCA}\\tilde{V}^T + <X>$$ dove $<X>$ \u00e8 la matrice sottratta alla matrice originale per centrare i valori medi delle colonne. La trasformazione inversa pu\u00f2 essere effettuata con il metodo <code>inverse_transform<\/code>.<br>\n    <strong>Curiosit\u00e0:<\/strong> Un modo alternativo per ridurre la dimensionalit\u00e0 delle immagini \u00e8 fare un resize dell'immagine, cio\u00e8 ridurre il numero di pixel con un apposito algoritmo di downscaling. Riducendo l'immagine da 150x150 a 35x35 otteniamo all'incirca lo stesso fattore di compressione, quindi vediamo qual \u00e8 il risultato!\n<\/div>","1b7c1f9a":"### 5. Classificazione utilizzando convolutional neural networks.\n<div style=\"text-align: justify\">\n    A questo punto proviamo ad utilizzare le <strong>reti neurali convoluzionali (CNN)<\/strong> [7].\n    Ci aspettiamo che le performance siano migliori, perch\u00e8 le CNN sono specializzate nel trattamento di immagini (anche se sono utili anche in altri ambiti).<br><br>\n    Le reti neurali convoluzionali sono ispirate al funzionamento della corteccia visiva degli animali. I neuroni nella corteccia visiva reagiscono esclusivamente a stimoli presenti in una specifica porzione del campo visivo (chiamata receptive field), e, a seconda della profondit\u00e0 in cui sono situati, rispondono a pattern specifici e.g. linee in una determinata direzione (si veda ad esempio l'<a href=https:\/\/www.youtube.com\/watch?v=8VdFf3egwfg>esperimento di Hubel & Wiesel<\/a>, premi Nobel per la medicina 1981). La loro azione pu\u00f2 essere approssimata matematicamente da un'operazione di convoluzione.<br><br>\n    Le reti neurali convoluzionali sono simili alle reti neurali fully-connected, ma con importanti differenze.\n    Innanzitutto, i singoli dati di input e gli output corrispondenti a ciascun layer sono array multidimensionali, ad esempio per un immagine in input avremo array di dimensione $\\mbox{height} \\times \\mbox{width} \\times \\mbox{channels}$ dove i diversi canali indicano l'intensit\u00e0 dei vari colori.\n    Inoltre, l'operazione lineare che viene eseguita in ciascun layer non \u00e8 pi\u00f9 una generica trasformazione affine bens\u00ec una convoluzione discreta lungo le dimensioni spaziali con un altro array multidimensionale chiamato <strong>filtro<\/strong>.\n    Il filtro \u00e8 solitamente piccolo lungo le dimensioni spaziali, ma si estende lungo tutta la dimensione dei canali (e.g. $5 \\times 5 \\times 3$ se l'input \u00e8 un'immagine a colori).\n    Questo operazione pu\u00f2 venire effettuata in parallelo un certo numero di volte (questo numero \u00e8 detto numero dei filtri di quel layer) con filtri parametrizzati indipendentemente.\n    Di conseguenza l'output di questa operazione \u00e8 un altro array multidimensionale di dimensione $\\mbox{new height} \\times \\mbox{new width} \\times \\mbox{no. of filters}$.\n    Successivamente una funzione di attivazione non lineare viene applicata element-wise.<br><br>\n    Solitamente vengono applicati anche dei layer di <strong>pooling<\/strong>, nei quali viene effettuato un downsampling spaziale dell'output del layer precedente. Il downsampling pu\u00f2 venire effettuato prendendo il valore massimo da una certa pool size (in tal caso si chiama Max Pooling) oppure prendendo la media (Average Pooling). Alla fine solitamente vengono applicati dei layer fully-connected come quelli descritti in precedenza.<br><br>\n    Tutte le operazioni sopra elencate introducono molti dettagli che non abbiamo esplicitato, e che producono molte diverse varianti nell'architettura di una CNN. Una descrizione molto pi\u00f9 dettagliata si trova al seguente link: <a href=https:\/\/cs231n.github.io\/convolutional-networks\/>https:\/\/cs231n.github.io\/convolutional-networks\/<\/a> [8]. Tuttavia, le CNN si posizionano nello stesso framework delle reti neurali \"vanilla\", per cui tutte le considerazioni fatte in precedenza su funzioni di costo, ottimizzazione, struttura dell'output valgono anche in questo caso.<br><br>\n    <strong>Dettagli implementativi:<\/strong> anche in questo caso abbiamo usato <strong>Keras<\/strong> con backend <strong>TensorFlow<\/strong>.\n    Dato che abbiamo a disposizione una GPU NVIDIA Tesla P100 (con 16 GB di RAM) possiamo permetterci di applicare la rete all'immagine originale non compressa.\n    Oltre alle classi e ai metodi descritti sopra, abbiamo utilizzato tre nuovi layers:\n    <ol><li>Il layer <code>tf.keras.layers.Conv2D<\/code> \u00e8 un convolutional layer con numero di filtri, dimensione dei filtri e funzione di attivazione definita dall'utente.<\/li>\n        <li>Il layer <code>tf.keras.layers.MaxPooling2D<\/code> effettua un max pooling con una pool size definita dall'utente.<\/li>\n        <li>Il layer <code>tf.keras.layers.Flatten<\/code> appiattisce l'output del layer precedente in maniera che possa venire tranquillamente applicato ad un layer fully-connected.<\/li><\/ol>\nGli iperparametri della rete e l'architettura sono scelti seguendo gli esempi presenti nella pagina ufficiale di <strong>Keras<\/strong>.\n<\/div>","5a84729a":"### 6. Studio delle performance delle convolutional neural networks al variare della complessit\u00e0.\n<div style=\"text-align: justify\">\n    In questo paragrafo studieremo come variano le performance delle convolutional neural networks al variare della loro complessit\u00e0.\n    Per essere precisi, prenderemo una CNN modulare, e cambieremo il numero di layers Conv2D+MaxPooling e il numero di filtri, mantenendo invariati tutti gli altri iperparametri.\n    Per ognuna di queste reti effettueremo il training e la misura dell'accuratezza sul test set, e alla fine mostreremo un grafico con i risultati complessivi.<br><br>\n    <strong>Dettagli implementativi:<\/strong> per fare questo \u00e8 stata costruita una funzione <code>build_cnn_incremental<\/code> che prende come input il numero di layer Conv2D+MaxPooling e un moltiplicatore che va ad agire sul numero dei filtri, costruisce e allena una CNN con l'architettura corrispondente, infine calcola il numero di parametri e l'accuratezza sul training e test set. L'architettura che abbiamo scelto \u00e8 quella base senza Dropout+DataAugmentation per poter effettuare lo studio in tempi gestibili.\n<\/div>","93f6af2e":"<div style=\"text-align: justify\">\nUna volta caricato il dataset, conviene visualizzare alcune di queste immagini per verificare di aver implementato tutto correttamente e per poter osservare in pratica il compito che l'algoritmo di classificazione dovr\u00e0 eseguire.\n<\/div>","3392e3c3":"<div style=\"text-align: justify\">\n    A questo punto usiamo l'algoritmo <strong>PCA<\/strong> implementato in <strong>Scikit-Learn<\/strong> per ridurre la dimensionalit\u00e0 del campione.\n    La scelta di tenere 3600 predittori \u00e8 stata fatta per tentativi cercando di ottenere contemporaneamente un numero tondo e un'explained variance ratio di circa 0.95.<br><br>\n    <strong>Dettagli implementativi e commenti:<\/strong> abbiamo usato la classe <code>sklearn.decomposition.PCA<\/code> per eseguire la PCA.\n    Il metodo <code>fit<\/code> si occupa di calcolare la SVD e di calcolare quindi la matrice di trasformazione $\\tilde{V}$.\n    Il metodo <code>transform<\/code> si occupa poi di calcolare la matrice di design ridotta $X_{PCA}=X \\tilde{V}$.\n    Non occorre occuparsi di traslare le colonne di $X$ perch\u00e8 se ne occupa la classe.\n    Per essere precisi, nel nostro caso l'algoritmo non calcola la SVD completa, bens\u00ec utilizza un algoritmo stocastico per trovare velocemente un'approssimazione delle prime componenti principali (questo spiega anche il parametro <code>random_state<\/code>). La classe si occupa autonomamente di scegliere quale tecnica usare in base a determinate condizioni descritte nella documentazione di Scikit-Learn. E' possibile forzare il calcolo della SVD completa, ma il costo computazionale \u00e8 eccessivo.\n<\/div>","23ddf237":"<div style=\"text-align: justify\">\n    Anche in questo caso \u00e8 complicato intuire quello che sta succedendo.\n    Si vede che alcuni filtri estraggono i contorni lungo una determinata direzione, ma \u00e8 difficile andare oltre.<br><br>\n    Facciamo ora la stessa identica cosa con il secondo layer (ma con meno immagini per ragioni di spazio):\n<\/div>","58b295cc":"### 7. Conclusioni\n<div style=\"text-align: justify\">\n    Abbiamo applicato tre strategie diverse per la classificazione delle immagini contenute nel dataset Intel Image Classification.\n    La prima \u00e8 consistita nell'applicare la Principal Component Analysis per ridurre la dimensionalit\u00e0 del problema e successivamente applicare la regressione logistica.\n    La seconda \u00e8 stata una variante della prima con una rete neurale fully-connected al posto della regressione logistica.\n    L'ultima, invece, \u00e8 consistita nell'utilizzare una rete neurale convoluzionale.<br><br>\n    Le convolutional neural networks sono state i modelli pi\u00f9 accurati.\n    Abbiamo quindi verificato in un caso specifico come esse siano molto efficaci nel trattamento delle immagini.\n    Inoltre, questo esercizio ha messo in evidenza l'importanza di avere un'implementazione efficiente che sfrutti hardware parallelo.\n    Infatti, le sezioni di codice pi\u00f9 lente sono risultate quelle eseguite su CPU, in particolare PCA e regressione logistica.\n    Al contrario, le reti neurali implementate con Keras sono state eseguite su GPU, rivelandosi molto pi\u00f9 adeguate nel gestire dataset discretamente grandi.\n    In particolare, la combinazione PCA+RegressioneLogistica \u00e8 risultata molto pi\u00f9 lenta del primo modello di CNN presentato (15 minuti contro qualche minuto) nonostante quest'ultimo fosse ben pi\u00f9 accurato.\n    Un discorso a parte va fatto per il modello di CNN con Dropout+DataAugmentation, che \u00e8 risultato comunque lento a causa di colli di bottiglia che hanno quasi azzerato l'occupazione media della GPU (tra lo 0% e il 4%). Questo insegna che non serve avere hardware costoso se i vari carichi di lavoro su tutte le componenti hardware non sono proporzionati.<br><br>\n    Questo esercizio ha inoltre messo in evidenza i limiti di memoria RAM a cui si va incontro quando si lavora con dataset voluminosi.\n    In particolare, quando si lavora con immagini di dimensione modesta (per gli standard di un utente normale), ma occorre lavorare con i valori raw dei pixel, la quantit\u00e0 di RAM utilizzata diventa elevata.\n    Se poi a questo si aggiungono i modelli, la quantit\u00e0 di RAM finisce in fretta.\n    In questo caso la memoria \u00e8 stata appena sufficiente, e se non fosse bastata avremmo dovuto evitare di caricare l'intero dataset in memoria ma utilizzare delle procedure apposite implementate in Keras, con l'eventualit\u00e0 di aggiungere colli di bottiglia legati alla lettura da disco.<br><br>\n    Ci sono sicuramente margini di miglioramento per questo esercizio, per esempio effettuando una procedura di <strong>hyperparameter optimization<\/strong>.\n    In questo esercizio non abbiamo fatto una ricerca sistematica degli iperparametri e dell'architettura migliore.\n    Nel caso della PCA+RegressioneLogistica, potevamo provare vari numeri di componenti (PCA) e varie intensit\u00e0 di regolarizzazione (regressione logistica).\n    Nel caso delle reti neurali, potevamo provare diverse architetture, diverse tecniche di regolarizzazione ($L^2$, Early Stopping, Noise Layers), diversi ottimizzatori, vari learning rate etc.\n    Abbiamo fatto esclusivamente un breve studio sull'architettura della CNN.\n    Se uno vuole veramente studiare a fondo questi modelli, pu\u00f2 implementare un'hyperparameter search con librerie open-source tipo <a href=https:\/\/github.com\/hyperopt\/hyperopt>HyperOpt<\/a> e molto probabilmente otterr\u00e0 modelli migliori di quelli presenti in questo esercizio.\n    Per effettuare questa operazione in tempi brevi, per\u00f2, servono notevoli risorse computazionali.\n    Un'alternativa a questa procedura \u00e8 quella di utilizzare delle architetture di computer vision conosciute e gi\u00e0 allenate su altri dataset, come consigliato anche in <a href=https:\/\/cs231n.github.io\/convolutional-networks\/>https:\/\/cs231n.github.io\/convolutional-networks\/<\/a> [8].\n    Questo \u00e8 l'approccio usato anche in molte soluzioni pubblicate qui su Kaggle, e un esempio \u00e8 mostrato in appendice.\n<\/div>","f0d77b0d":"<div style=\"text-align: justify\">\n    <strong>Commento:<\/strong> interpretare il significato di questi filtri non \u00e8 per niente banale, e pi\u00f9 il modello diventa complicato pi\u00f9 diventa difficile cercare di capire come funziona in questo modo.<br><br>\n    Magari \u00e8 pi\u00f9 semplice capire cosa stanno facendo questi filtri applicandoli a delle immagini e vedendo cosa succede.\n    Quindi, prendiamo il primo layer del modello gi\u00e0 allenato e vediamo i 32 canali ottenuti dopo la sua applicazione.\n    Per fare ci\u00f2 abbiamo utilizzato i metodi <code>get_weights<\/code> e <code>set_weights<\/code>, applicati rispettivamente al modello gi\u00e0 allenato e al modello parziale da reinizializzare.\n<\/div>","59e0ac4f":"<div style=\"text-align: justify\">\n    Al secondo layer \u00e8 ancora pi\u00f9 difficile dare un'interpretazione di quello che sta succedendo.\n    E' tuttavia interessante la possibilit\u00e0 di osservare in qualche modo una CNN in azione.\n<\/div>","01407ca7":"Visualizziamo adesso i risultati ottenuti:","e9e7e69a":"Riusciamo ad ottenere una accuracy di test di circa 87-88%, che \u00e8 la migliore di tutto l'esercizio!","7c09c559":"# Intel Image Classification Exercise\n(This is an exercise in Italian for a machine learning course at my university)\n### Abstract\n<div style=\"text-align: justify\">\nI progressi recenti nell'ambito del machine learning, e del deep learning in particolare, hanno trovato una moltitudine di applicazioni rilevanti in moltissimi settori, sia nell'ambito della ricerca accademica che nel settore industriale\/servizi.\nIl notevole interesse a riguardo ha alimentato lo sviluppo di hardware specifici e software <em>open-source user-friendly<\/em> che permettono di implementare e allenare modelli avanzati anche con un investimento ridotto\/nullo.\nIn questo progetto impiegheremo il dataset <a href=https:\/\/www.kaggle.com\/puneet6060\/intel-image-classification)>Intel Image Classification Dataset<\/a> [1] per costruire e allenare un modello di computer vision adibito al task di <strong>riconoscimento e classificazione di immagini<\/strong>.\nPer fare ci\u00f2 utilizzeremo vari tipi di modelli e di strategie, e sfrutteremo hardware specifici quali la GPU <a href=https:\/\/www.nvidia.com\/it-it\/data-center\/tesla-p100\/>NVIDIA Tesla P100<\/a>.\n<\/div><br><br>\n\n### Introduzione\n<div style=\"text-align: justify\">\nQuesto notebook presenta l'applicazione di algoritmi di classificazione al dataset <a href=https:\/\/www.kaggle.com\/puneet6060\/intel-image-classification)>Intel Image Classification Dataset<\/a> [1].\nQuesto dataset contiene circa 25000 immagini JPEG a colori di dimensione 150x150.\nIl soggetto delle immagini sono paesaggi naturali o immagini urbane.\nIn particolare le immagini sono suddivise in 6 categorie: edifici, foreste, ghiacciai, montagne, mari e strade.\nCirca 7000 delle 25000 immagini a disposizione non sono etichettate, in quanto il dataset \u00e8 stato usato per una competizione di machine learning e di conseguenza le etichette del dataset di test finale non sono fornite.\nAi fini dei nostri esperimenti verrano usate soltanto le restanti immagini etichettate.\nL'obiettivo \u00e8 sfruttare questo dataset per ottenere degli algoritmi di classificazione che possano classificare nuove immagini raffiguranti le stesse categorie di soggetti (con lo stesso formato 150x150 a meno di preprocessing).\n<\/div><br>\n\nIl notebook \u00e8 strutturato nei seguenti paragrafi:\n1. Lettura e preprocessing base del dataset.\n2. Riduzione della dimensionalit\u00e0 delle immagini con l'algoritmo Principal Component Analysis.\n3. Classificazione utilizzando la regressione logistica.\n4. Classificazione utilizzando reti neurali fully-connected.\n5. Classificazione utilizzando convolutional neural networks.\n6. Studio delle performance delle convolutional neural networks al variare della complessit\u00e0.\n7. Conclusioni\n8. Appendice: classificazione con VGG16 pre-allenato su ImageNet\n\n##### Riferimenti\n[1] Intel Image Classification Dataset, https:\/\/www.kaggle.com\/puneet6060\/intel-image-classification.<br>\n[2] **Scikit-learn**, https:\/\/scikit-learn.org\/stable\/.<br>\n[3] **Keras**, Fran\u00e7ois Chollet and others, https:\/\/keras.io\/.<br>\n[4] **TensorFlow**, https:\/\/www.tensorflow.org\/.<br>\n[5] Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow, Aur\u00e9lien Geron, 2nd Edition, 2019.<br>\n[6] The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Hastie et al, 2nd Edition, 2009.<br>\n[7] **Deep learning**, Ian Goodfellow and Yoshua Bengio and Aaron Courville, 2016, http:\/\/www.deeplearningbook.org.<br>\n[8] CS231n: Convolutional Neural Networks for Visual Recognition, Stanford University, https:\/\/cs231n.github.io\/convolutional-networks\/\n\nInnanzitutto importiamo tutte le librerie e le funzioni necessarie per il resto del notebook:","d09238af":"Infine facciamo la stessa cosa con reti di profondit\u00e0 3.","10990026":"### 3. Classificazione utilizzando la regressione logistica\n\n<div style=\"text-align: justify\">\nOra che abbiamo ridotto la dimensionalit\u00e0 dei dati di input, possiamo fare un tentativo di classificazione utilizzando la <strong>regressione logistica<\/strong> [6]. \nLa regressione logistica si applica a problemi di classificazione, come quello in esame, in cui l'obiettivo \u00e8 modellizzare la probabilit\u00e0 di avere una categoria $k \\in {1,2,...,K}$ dato un campione $\\mathbf{x} \\in \\mathbb{R}^{p}$.\nIn particolare, si assumono delle relazioni lineari tra i logaritmi delle probabilit\u00e0 condizionate rispetto a una classe di riferimento, che per convenzione \u00e8 la classe $K$:<br>\n$$ \\log \\left( \\frac{P(k|\\mathbf{x})}{P(K|\\mathbf{x})} \\right) =  \\beta_{k0} + \\beta_{k}^T \\mathbf{x} $$\nDopo alcune manipolazione, si ottiene il seguente risultato per le $P(k|\\mathbf{x})$:\n$$ P(k|\\mathbf{x}) = \\frac{\\exp{\\left( \\beta_{k0} + \\beta_{k}^T \\mathbf{x} \\right)}}{1 + \\sum_{i=1}^{K-1}\\exp{\\left( \\beta_{i0} + \\beta_{i}^T \\mathbf{x} \\right) }}, \\;\\; k \\neq K$$\n$$ P(K|\\mathbf{x}) = \\frac{1}{1 + \\sum_{i=1}^{K-1}\\exp{\\left( \\beta_{i0} + \\beta_{i}^T \\mathbf{x} \\right) }}.$$\nIn questo modo le probabilit\u00e0 sono tutte positive e sommano a uno. Il numero totale di parametri del modello \u00e8 $(p+1)(K-1)$.\nFacciamo adesso l'assunzione esplicita di avere un problema di classificazione binaria.\nIndichiamo il set complessivo di parametri con $\\theta \\in \\mathbb{R}^{(p+1)}$ e utilizziamo come funzione costo la <strong>negative log-likelihood<\/strong>\n$$ C( \\mathbf{y}, X, \\theta) = - \\sum_{i=1}^{N} \\left[ y_i \\log{(p_i)} + (1 - y_i) \\log{(1 - p_i)} \\right] $$\ndove $y_i$ \u00e8 l'etichetta binaria del campionamento i-esimo, che vale 1 se $k=1$ e vale 0 altrimenti, e $p_i = P(k=1|\\mathbf{x}_i)$ \u00e8 l'unico termine che dipende da $\\theta$.\nI parametri $\\theta$ migliori si trovando minimizzando la funzione costo. Dato che la negative log-likelihood \u00e8 monotona con la likelihood questo equivale a utilizzare la stima di massima verosimiglianza. Per minimizzare la funzione costo \u00e8 possibile calcolare il suo gradiente rispetto ai parametri:\n$$\\nabla_{\\theta}C( \\mathbf{y}, X, \\theta) = - X^T (\\mathbf{y} - \\mathbf{p})$$  \ndove $\\mathbf{p}$ \u00e8 il vettore con gli elementi $p_i$ descritti sopra. Se l'algoritmo lo richiede \u00e8 possibile calcolare anche la derivata seconda.<br><br>\n<strong>Dettagli implementativi:<\/strong> nel seguente codice \u00e8 stata usata la regressione logistica implementata in Scikit-Learn.\nDato che il problema di classificazione non \u00e8 binario, \u00e8 stata usata la strategia \"One versus Rest\" che consiste nel fittare un classificatore binario per ogni classe.\nSono stati mantenuti tutti i parametri di default (a parte il numero di iterazioni massime impiegabili per il fit).\nIn particolare l'algoritmo di minimizzazione \u00e8 il metodo L-BFGS [7]. Inoltre, viene applicata di default una regolarizzazione $L^2$ che consiste nell'aggiungere alla funzione costo un termine proporzionale al modulo quadro del vettore $\\beta$ contenente tutti i parametri del modello.\n<\/div>","ad39ae74":"<div style=\"text-align: justify\">\nNotiamo comunque che, anche se all'atto pratico entrambe le tecniche comprimono un immagine, lo scopo \u00e8 molto diverso.\nIl rescaling si applica a una singola immagine: l'obiettivo \u00e8 ottenere un'altra immagine con una risoluzione minore ma possibilmente fedele a quella originale.\nAl contrario, la PCA si applica ad un intero dataset: l'obiettivo \u00e8 ridurre la dimensionalit\u00e0 dei singoli campionamenti ma mantenendo il pi\u00f9 possibile la varianza dei predittori all'interno del dataset. Il confronto \u00e8 comunque interessante perch\u00e8 mostra come nel nostro caso la PCA conservi molti pi\u00f9 dettagli di un semplice riscalamento, anche se ovviamente per riottenere un'immagine occorre riportare il dato compresso nel suo spazio originale.\n<\/div>","e434061f":"Facciamo adesso la stessa cosa con reti di profondit\u00e0 2:","9b89fdd4":"### 4. Classificazione utilizzando reti neurali fully-connected.\n<div style=\"text-align: justify\">\nLe performance non molto soddisfacenti della regressione logistica su questo dataset ci portano a provare dei modelli pi\u00f9 sofisticati. Proviamo quindi utilizzando una <strong>rete neurale fully-connected<\/strong> [7]. Dal punto di vista matematico, una rete neurale fully-connected \u00e8 una sequenza di trasformazioni affini dell'input intermezzate da una funzione non lineare applicata element-wise (detta funzione di attivazione).<br><br>\n$$ N(\\mathbf{x}) = f^{(n)} \\circ W_n \\circ f^{(n-1)} \\circ W_{n-1} \\circ \\; ... \\; \\circ f^{(1)} \\circ W_1 ( \\mathbf{x} )$$<br>\nCiascuna coppia trasformazione affine\/funzione di attivazione \u00e8 detta layer. La dimensione dello spazio di input e di output \u00e8 fissata dal problema, mentre tutto il resto \u00e8 configurabile dall'utente, ad esempio la dimensione dello spazio di output delle mappe affini intermedie (ovvero la dimensione dei layer intermedi) e le funzioni di attivazione.\nLe reti neurali godono di una propriet\u00e0 fondamentale, ovvero il poter approssimare funzioni continue in maniera arbitrariamente precisa, a condizione di essere \"abbastanza complicate\" (questo risultato va sotto il nome di <strong>universal approximation theorem<\/strong>).\nEsiste una ricca fenomenologia di varianti di questo teorema, tuttavia, ai fini di questo esercizio, \"abbastanza complicate\" significa con una sufficiente dimensione dei layer intermedi e\/o un sufficiente numero di layer. Questi due parametri sono comunemente chiamati \"larghezza\" e \"profondit\u00e0\" della rete.<br><br>\nLe reti neurali vengono utilizzate come modelli parametrici per problemi di regressione o classificazione.\nEsiste una ricca zoologia di funzioni di attivazione, funzioni di costo, algoritmi di ottimizzazione che permettono di sfruttare al meglio le potenzialit\u00e0 di questi modelli. Ad esempio, noi siamo interessati a usarle per costruire un classificatore.\nSappiamo dalla struttura del nostro dataset che la dimensione di input del primo layer sar\u00e0 3600, mentre quella di output sar\u00e0 6 (il numero delle classi).\nLa larghezza dei layer intermedi, il numero di essi e le funzioni di attivazione sono scelti a tentativi o in base all'esperienza dell'utente.\nLa funzione di attivazione dell'ultimo layer \u00e8 guidata dal fatto che vogliamo ottenere come output una distribuzione di probabilit\u00e0 sulle 6 classi, e di conseguenza scegliamo la funzione <strong>softmax<\/strong>:<br><br>\n$$f_i(\\mathbf{x}) = \\frac{\\exp{\\left( x_i \\right)}}{\\sum_{j=0}^{K-1} \\exp{\\left( x_j \\right)}}, \\;\\; i=0,...,5$$<br>\nin modo tale che i 6 valori di output siano positivi e normalizzati.\nPrima di definire la funzione costo, facciamo un cambio di notazione.\nFino ad ora per rappresentare le etichette del dataset abbiamo utilizzato il vettore $\\mathbf{y}$ il cui elemento $y_i \\in \\{0,1,2,3,4,5 \\}$ indica la categoria codificata in un numero intero.\n    Ora cambiamo rappresentazione: utilizzaremo la <strong>matrice di indice<\/strong> $Y$ di dimensione $N \\times K$ la cui riga i-esima contiene tutti 0 tranne un 1 nella posizione j-esima corrispondente alla classe dell'immagine i-esima $y_i$.\nDetto questo, la funzione di costo scelta \u00e8 la <strong>cross-entropy<\/strong> tra la distribuzione empirica del dataset e la distribuzione modellizzata dalla rete neurale:<br><br>\n$$C(Y,X,\\theta) = - \\sum_{i=0}^{N-1} \\sum_{j=0}^{K-1} y_{ij} \\log{\\left( N_j(\\mathbf{X_{i*}}, \\theta) \\right)} $$<br>\nche generalizza il caso binario visto sopra nel paragrafo sulla regressione logistica.\nE' possibile calcolare in maniera efficiente il gradiente della funzione costo rispetto ad uno qualunque dei parametri della rete (i parametri delle trasformazioni affini) utilizzando un algoritmo chiamato <strong>back-propagation<\/strong>.<br><br>\n<strong>Dettagli implementativi:<\/strong> le reti neurali sono state implementate in maniera efficiente e pratica in numerose librerie <em>open-source<\/em> come <strong>TensorFlow<\/strong>, <strong>pyTorch<\/strong>, <strong>Theano<\/strong>, etc. In questo esercizio \u00e8 stato utilizzato <strong>Keras<\/strong>, con backend <strong>TensorFlow<\/strong> [4]. Sono state utilizzate le seguenti classi e metodi:\n<ol><li>La classe <code>tf.keras.Sequential<\/code> \u00e8 adatta per costruire modelli basilari in cui i layers vengono impilati uno dopo l'altro dall'input all'output, esattamente come nel nostro caso.<\/li>\n    <li>Il layer <code>tf.keras.layer.Dense<\/code> \u00e8 un layer fully-connected con numero di unit\u00e0 (ovvero dimensione dello spazio di output) passato come primo argomento dall'utente. Le altre opzioni usate sono la funzione di attivazione e l'input_shape. L'input_shape nei layer successivi al primo viene calcolata automaticamente.<\/li>\n    <li>Il metodo <code>compile<\/code> permette di scegliere l'ottimizzatore (in questo caso Adam), la loss function (descritta sopra), e eventuali metriche di performance (in questo caso l'accuratezza della classificazione).<\/li>\n    <li>La funzione <code>tf.keras.utils.to_categorical<\/code> permette di trasformare il vettore delle etichette $\\mathbf{y}$ nella matrice di indice $Y$ descritta sopra.<\/li>\n    <li>Il metodo <code>fit<\/code> allena la rete sul training set che gli viene passato con argomento. L'utente pu\u00f2 scegliere il numero di epoche, la dimensione dei minibatch per il training, e tanti altri parametri che qui non abbiamo usato.<\/li>\n    <li>Il metodo <code>evaluate<\/code> calcola la loss function e le metriche sul dataset fornito dall'utente, in questo caso il test set.<\/li><\/ol>\nLe librerie usate possono sfruttare l'accelerazione GPU per ridurre i tempi di esecuzione.<\/div>"}}