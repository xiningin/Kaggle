{"cell_type":{"d48ea717":"code","a3875ced":"code","9e170253":"code","1143a0ca":"code","283b2c16":"code","4146cdbd":"code","dc248fe2":"code","70605d74":"code","0d9edecd":"code","9a53a8c7":"code","28e59c31":"code","80a06519":"code","b2ab9d4e":"code","296a9cdf":"code","309368ff":"code","a72f3401":"code","d3041d51":"code","e45ba210":"code","71759b93":"code","a6613c2f":"code","3dc3cd40":"code","ddd196ae":"code","95652eea":"code","7306f617":"code","75dffcca":"code","59891dbd":"code","45354529":"code","6cc89bf9":"code","1848c7b8":"code","ad73e9e9":"code","714c045b":"markdown","3f48a9f7":"markdown","6e58c227":"markdown","f0b2e043":"markdown"},"source":{"d48ea717":"# import libraries\nimport gzip\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import tree\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor","a3875ced":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9e170253":"#!kaggle competitions download -c commonlitreadabilityprize","1143a0ca":"# get the train data\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntrain.head()","283b2c16":"# get the test data\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest.head()","4146cdbd":"# define X, y and test\nX = train['excerpt']\ny = train['target']\ntest_text = test[\"excerpt\"]","dc248fe2":"# lower the text\nX = X.str.lower()\ntest_text = test_text.str.lower()","70605d74":"# apply Porter Stemmer\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nX = X.apply(ps.stem)\ntest_text = test_text.apply(ps.stem)","0d9edecd":"# lemmatize the text\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\n#nltk.download('wordnet')\n\nwnl = WordNetLemmatizer()\nX = X.apply(wnl.lemmatize)\ntest_text = test_text.apply(wnl.lemmatize)","9a53a8c7":"import nltk\n#nltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\nX = X.apply(lambda text: remove_stopwords(text))\ntest_text = test_text.apply(lambda text: remove_stopwords(text))","28e59c31":"X_new = []\nfor index, value in X.items():\n    X_new.append(value)","80a06519":"y_new = []\nfor index, value in y.items():\n    y_new.append(value)","b2ab9d4e":"real_test = []\nfor index, value in test_text.items():\n    real_test.append(value)","296a9cdf":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.20)","309368ff":"EMBEDDING_FILE = '..\/input\/glove6b100dtxt\/glove.6B.100d.txt'\n\nembeddings = {}\nfor o in open(EMBEDDING_FILE):\n    word = o.split(\" \")[0]\n    # print(word)\n    embd = o.split(\" \")[1:]\n    embd = np.asarray(embd, dtype='float32')\n    # print(embd)\n    embeddings[word] = embd","a72f3401":"# convert features into vectors \n# I took this function from this notebook \n# https:\/\/www.kaggle.com\/prajittr\/commonlit-readability-glove-xgb-nn-rf-ensemble\n\ndef get_feature_vectors(sentence):\n    words = sentence.split()\n    feature_vec = np.zeros((100,),dtype=\"float32\")\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, embeddings.get(word))\n        except:\n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","d3041d51":"train_vectors = np.array([get_feature_vectors(sentence) for sentence in X_train])\ntest_vectors = np.array([get_feature_vectors(sentence) for sentence in X_test])\nreal_test_vectors = np.array([get_feature_vectors(sentence) for sentence in real_test])\n\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nprint(train_vectors.shape, y_train.shape)\nprint(test_vectors.shape, y_test.shape)\nprint(real_test_vectors.shape)","e45ba210":"vectors_full = np.array([get_feature_vectors(sentence) for sentence in X_new])\ny_full = np.array(y_new)","71759b93":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n","a6613c2f":"def create_model():\n    \n    model = keras.Sequential([\n        layers.Dense(units=512, kernel_initializer='normal', activation='linear', input_shape=[100]),\n        layers.Dense(units=256, kernel_initializer='normal', activation='linear'),\n        layers.Dense(units=128, kernel_initializer='normal', activation='linear'),\n        layers.Dropout(0.25),\n        # the linear output layer \n        layers.Dense(units=1, kernel_initializer='normal', activation='linear'),\n    ])\n    \n    model.compile(optimizer = 'adam', loss='mean_squared_error')\n    \n    return model","3dc3cd40":"model = create_model()\nmodel.summary()","ddd196ae":"from keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\n\n# Create a KerasClassifier\n#model_KR = KerasRegressor(build_fn = create_model)\n\n#define the parameters to try out\n#params = {'batch_size':[16, 32, 128], 'epochs':[10, 20, 50]}\n\n#define RandomizedSearchCV\n#random_searcher = RandomizedSearchCV(model_KR, param_distributions = params, cv = KFold(5))\n\n#fit the model\n#random_searcher.fit(vectors_full, y_full)\n\n#take a look a the results\n#print(random_searcher.best_params_)\n#print(random_searcher.best_score_)\n\n#get the mean accuracy\n#print('The mean accuracy:', kfolds.mean())","95652eea":"# Create a KerasClassifier with best parameters\nmodel_KR = KerasRegressor(build_fn = create_model, batch_size = 16, epochs = 50)\n\n# Calculate the accuracy score for each fold\nkfolds = cross_val_score(model_KR, vectors_full, y_full, cv = 10)\n\n#get the accuracy\nprint(kfolds.mean())\nprint('The mean accuracy:', kfolds.mean())","7306f617":"#use callbacks\nfrom keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(\"\", monitor=\"val_loss\", verbose=1, save_best_only=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5, min_lr=1e-6, verbose=1)\nearly_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, mode='auto', restore_best_weights=True)","75dffcca":"history = model_KR.fit(\n    vectors_full, y_full,\n    validation_split=0.4,\n    batch_size=16,\n    epochs=50,\n    callbacks = [early_stop, checkpoint, reduce_lr]\n)\n\n#callbacks=[reduce_lr, early_stop]","59891dbd":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[1:, ['loss', 'val_loss']].plot()","45354529":"history_df.head()","6cc89bf9":"# predict \npred_test = model_KR.predict(real_test_vectors)","1848c7b8":"pred_test_list = [i for i in pred_test]\npred_test_list","ad73e9e9":"# create submission file\nsubmission = pd.DataFrame({'id' : test['id'], 'target' : pred_test_list})\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)\nsubmission.head(7)","714c045b":"**Model**","3f48a9f7":"**Preprocessing**","6e58c227":"## CommonLit Readability with KerasRegressor","f0b2e043":"In this notebook we will use only excerpt (X) and target (y) columns. So at this stage we don't need other columns."}}