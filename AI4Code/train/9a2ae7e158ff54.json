{"cell_type":{"fcd10295":"code","0187c734":"code","089b3139":"code","c9f291f0":"code","20ea3a94":"code","62b9b2ed":"code","1a0d0c59":"code","579990d5":"code","1f9db531":"code","db818a5c":"code","71e58498":"code","7c5cdaf4":"code","fcb46d63":"code","cd1816a0":"markdown","b99aa934":"markdown","848b38b6":"markdown","8b39abd0":"markdown","8f6b2ae1":"markdown","68cb6fa8":"markdown","9c87aa4b":"markdown"},"source":{"fcd10295":"# !pip install transformers[sentencepiece]","0187c734":"import torch\nprint(torch.cuda.is_available())\nprint(torch.cuda.device_count())\n# print(torch.cuda.current_device())\n# print(torch.cuda.get_device_name(torch.cuda.current_device()))","089b3139":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9f291f0":"TRAINING_FILE = pd.read_csv(\"\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\")\nTEST_FILE = pd.read_csv(\"\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")\nSUBMISSION_FILE = pd.read_csv(\"\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/sample_submission.csv\")\n# change it to True if you want to use GPU\nuse_gpu = False","20ea3a94":"import transformers\nfrom pprint import pprint\nfrom torch.utils.data import Dataset, DataLoader\n# import torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.optim import Adam,AdamW\n\nfrom transformers import logging\n\nlogging.set_verbosity_error()","62b9b2ed":"tokenizer = AutoTokenizer.from_pretrained(\"deepset\/xlm-roberta-large-squad2\")","1a0d0c59":"\"\"\" Code adopted from reference link \"\"\"\nclass ChaiiDataset(Dataset):\n    \n    def __init__(self,df,max_len=356,doc_stride=128):\n        \n        self.df = df\n        self.max_len = max_len \n        self.doc_stride = doc_stride\n        self.labelled = 'answer_text' in df\n        self.tokenizer = AutoTokenizer.from_pretrained(\"deepset\/xlm-roberta-large-squad2\",add_special_tokens=True)        \n        self.tokenized_samples = self.tokenizer(\n                                self.df['context'].values.tolist(),\n                                self.df['question'].values.tolist(),\n                                truncation=\"only_first\",\n                                max_length=self.max_len,\n                                stride=self.doc_stride,\n                                return_overflowing_tokens=True,\n                                return_offsets_mapping=True,\n                                padding=\"max_length\")\n        \n    def __getitem__(self,idx):\n        \n        data = {}\n        ids,mask,offset = self.tokenized_samples['input_ids'][idx],\\\n                        self.tokenized_samples['attention_mask'][idx],\\\n                        self.tokenized_samples['offset_mapping'][idx]\n        \n        data['index'] = idx\n        data['ids'] = torch.tensor(ids)\n        data['mask'] = torch.tensor(mask)\n        data['offset'] = offset\n        if self.labelled:\n            \n            answer_text,start,end = self.get_targets(idx)\n            data['answer_text'] = answer_text\n            data['start'] = torch.tensor(start)\n            data['end'] = torch.tensor(end)\n            \n        \n        return data\n    \n    def get_targets(self,idx):\n        \n        df_index = self.tokenized_samples['overflow_to_sample_mapping'][idx]\n        start_char = (self.df.iloc[df_index]['answer_start'])\n        end_char = start_char + len(self.df.iloc[df_index]['answer_text'])\n        offset = self.tokenized_samples['offset_mapping'][idx]\n        sequence_ids = self.tokenized_samples.sequence_ids(idx)\n        end_offset = len(self.tokenized_samples['input_ids'][idx])-1\n        start_offset = 1\n        while sequence_ids[end_offset] != 0:\n            end_offset -= 1\n            \n            \n        start_idx = 0;end_idx=0\n        ## answer not in context\n        if (start_char > offset[end_offset][0] or end_char < offset[start_offset][0]):\n            #print(\"In first loop\")\n            start_idx = 0;end_idx=0\n            answer_text=\"\"\n        \n            \n        ## answer partially in context\n        elif ((start_char <= offset[end_offset][0]) and (end_char >  offset[end_offset][0])):\n            #print(\"in second loop\")\n            start_idx = 0;end_idx=0\n            answer_text = \"\"\n        \n        ## answer fully inside context\n        else:\n            #print(\"In third loop\")\n            i=0\n            while (start_idx < len(offset) and offset[i][0]<=start_char and offset[i][1]<start_char):\n                start_idx+=1\n                i+=1\n            end_idx = i\n            while (end_idx < len(offset) and offset[i][1]<end_char):\n                end_idx+=1\n                i+=1\n            answer_text = self.df.iloc[df_index]['answer_text'].strip()\n            \n        \n        return answer_text,start_idx, end_idx \n    \n    def __len__(self):\n        return len(self.tokenized_samples['overflow_to_sample_mapping'])\n","579990d5":"class ChaiiModel(torch.nn.Module):\n    def __init__(self):\n        # create neural network architecture here\n        super(ChaiiModel,self).__init__()\n        config = transformers.AutoConfig.from_pretrained(\"deepset\/xlm-roberta-large-squad2\")\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.xlmrob = AutoModel.from_pretrained(\"deepset\/xlm-roberta-large-squad2\", config=config)\n        self.l0 = torch.nn.Linear(1024, 2)\n        \n    def forward(self, ids, attention_mask):\n        # feed forward execution happens here\n        xlmrob_out = self.xlmrob(ids, attention_mask)\n        sequence_output = xlmrob_out[0]\n        logits = self.l0(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n    \n        return start_logits, end_logits","1f9db531":"\"\"\" Adopted some code from  reference link \"\"\"\ndef loss_fn(o1, o2, t1, t2):\n    l1 = torch.nn.BCEWithLogitsLoss()(o1, t1)\n    l2 = torch.nn.BCEWithLogitsLoss()(o2, t2)\n    return l1 + l2\n\ndef safe_div(x,y):\n    if y == 0:\n        return 1\n    return x \/ y\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return safe_div(float(len(c)) , (len(a) + len(b) - len(c)))\n\ndef get_jaccard_score(y_true,y_pred):\n    assert len(y_true)==len(y_pred)\n    score=0.0\n    for i in range(len(y_true)):\n        score += jaccard(y_true[i], y_pred[i])\n        \n    return score","db818a5c":"class AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","71e58498":"from tqdm import tqdm\n\ndef train_fn(data_loader, model, optimizer):\n    if use_gpu == True:\n        model = model.to(device=\"cuda\")\n    model.train()\n    losses = AverageMeter()\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        attention_mask = d[\"mask\"]\n        targets_start = d[\"start\"]\n        targets_end = d[\"end\"]\n        if use_gpu == True:\n            ids = d[\"ids\"].to(device=\"cuda\")\n            attention_mask = d[\"mask\"].to(device=\"cuda\")\n            targets_start = d[\"start\"].to(device=\"cuda\")\n            targets_end = d[\"end\"].to(device=\"cuda\")\n\n        \n        optimizer.zero_grad()\n        o1, o2 = model(\n            ids = ids,\n            attention_mask = attention_mask\n        )\n        \n        loss = loss_fn(o1, o2, targets_start, targets_end)\n        loss.backward()\n        optimizer.step()\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg)\n        ","7c5cdaf4":"from sklearn import model_selection\nEPOCH = 1\n\ndef run():\n    dfx = pd.read_csv(TRAINING_FILE)\n    \n    df_train, df_valid = model_selection.train_test_split(\n        dfx,\n        test_size = 0.1,\n        random_state = 42,\n    )\n    \n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n    \n    # pass it to dataset class and data loader - train df\n    train_dataset = ChaiiDataset(df_train)\n    train_dataloader =  DataLoader(train_dataset, batch_size=16, shuffle=True)\n    # pass it to dataset class and data loader - valid df\n    valid_dataset = ChaiiDataset(df_valid)\n    valid_dataloader =  DataLoader(valid_dataset, batch_size=16, shuffle=True)\n\n    \n    # initialize model\n    model = ChaiiModel()\n    # initialize optimizer\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.01,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters,lr=4e-5)\n    # calculate number of steps\n    steps = (len(df_train)*EPOCH)\/\/16\n    # loop over epochs - train_fn\n    for epoch in range(EPOCH):\n        train_fn(train_dataloader, model, optimizer)\n    ","fcb46d63":"# Invoke this function to start training\n# run()","cd1816a0":"## Training","b99aa934":"## Loss and Eval Function","848b38b6":"**Refernces ::** https:\/\/www.kaggle.com\/shahules\/chaii-xlm-custom-qa-baseline-train-infer","8b39abd0":"## Data Preparation","8f6b2ae1":"## Imports","68cb6fa8":"## Model","9c87aa4b":"I have used pretrained Roberta model with SQUAD dataset to fine tune for our use case. \n\nPretrained model available at :: https:\/\/huggingface.co\/deepset\/xlm-roberta-large-squad2"}}