{"cell_type":{"a0a9acc4":"code","5c86c1d1":"code","9d02a3cb":"code","a3afcb5e":"code","38c0f6ec":"code","336385cd":"code","22b71742":"code","cc95d269":"code","629df935":"code","1878fbf5":"code","b9f19eab":"code","598f82ef":"code","69ac60d9":"code","a1eba037":"code","b4b99013":"code","ed62f116":"code","cd733a68":"code","20a8bb81":"code","2375bec0":"code","90b51c54":"code","5942b4d7":"code","9fac2898":"code","c2c10b18":"code","200c0b83":"code","41971a03":"markdown","f51aaf50":"markdown","1abe8b1e":"markdown","173c5e19":"markdown","e2609e05":"markdown","81c8a7c3":"markdown","6374f68f":"markdown","a7a649b3":"markdown","c8571d36":"markdown","b7c7d2c1":"markdown","f5d4a747":"markdown","cc96a49c":"markdown","a42f07ff":"markdown","63d2346e":"markdown","4f11446b":"markdown","458cd710":"markdown","ee32cdba":"markdown","97c04b56":"markdown","70282833":"markdown","86265763":"markdown","7148d5ad":"markdown","914eedbc":"markdown","633d9f8a":"markdown","e6f2812d":"markdown","412c978e":"markdown","aca9aac5":"markdown","fa8be33d":"markdown"},"source":{"a0a9acc4":"#importing libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nseed = 0","5c86c1d1":"df = pd.read_csv(\"\/kaggle\/input\/mall-customers\/Mall_Customers.csv\")\ndf.head()","9d02a3cb":"df.info()","a3afcb5e":"df.describe()","38c0f6ec":"X = df.loc[:, ['Annual Income (k$)', 'Spending Score (1-100)']]","336385cd":"plt.figure(figsize=(10,5))\nplt.scatter(x= X['Annual Income (k$)'], y=X['Spending Score (1-100)'])\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)');","22b71742":"from sklearn.cluster import KMeans","cc95d269":"from sklearn.cluster import KMeans\nkm = KMeans(n_clusters = 3)\nkm.fit(X)","629df935":"plt.figure(figsize=(10,5))\nplt.scatter(x= X.iloc[:, 0], y=X.iloc[:, 1], c= km.labels_)\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)');","1878fbf5":"y_pred = km.predict(X)\ny_pred","b9f19eab":"km.labels_","598f82ef":"km.predict([[100, 30]]) # This customer his annual Income (k$) is 100 and his spending score is 30\n","69ac60d9":"fig, ax = plt.subplots(1, 3, gridspec_kw={'wspace': 0.3}, figsize=(15,5))\nfor i in range(3):\n    km = KMeans(n_clusters = 3, init='random', n_init=1, random_state=i)\n    km.fit(X)\n    ax[i].scatter(x= X.iloc[:, 0], y=X.iloc[:, 1], c= km.labels_);","a1eba037":"fig, ax = plt.subplots(1, 3, gridspec_kw={'wspace': 0.3}, figsize=(15,5))\nfor i in range(3):\n    km = KMeans(n_clusters = 3, init='random', n_init=1, random_state=i)\n    km.fit(X)\n    ax[i].scatter(x= X.iloc[:, 0], y=X.iloc[:, 1], c= km.labels_)\n    ax[i].set_title(f\"Inertia = {round(km.inertia_, 2)}\");","b4b99013":"wcss = [] #Within cluster sum squares\nfor i in range(2,15):\n    km = KMeans(n_clusters= i, random_state=seed)\n    km.fit(X)\n    wcss.append(km.inertia_)","ed62f116":"wcss","cd733a68":"km = KMeans(n_clusters = len(X))\nkm.fit(X)\nkm.inertia_","20a8bb81":"plt.plot(range(2,15), wcss, 'og-')\nplt.annotate('Elbow', xy=(5, 50000), xytext=(6, 100000), arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xlabel(\"K\")\nplt.ylabel(\"Inertia\");","2375bec0":"km = KMeans(n_clusters = 5, random_state=seed)\nkm.fit(X)\n\nplt.figure(figsize=(10,5))\nscatter = plt.scatter(x= X.iloc[:, 0], y=X.iloc[:, 1], c= km.labels_)\n#plt.scatter(x= X['Annual Income (k$)'], y=X['Spending Score (1-100)'], c= km.labels_)\n#plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], marker=\"X\", color='r', s=300) \nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend(handles=scatter.legend_elements()[0], labels=[0, 1, 2, 3, 4]);","90b51c54":"from sklearn.metrics import silhouette_score\n\nkm = KMeans(n_clusters = 5)\nkm.fit(X)\nsilhouette_score(X, km.labels_)","5942b4d7":"sil_scores = [] \nfor i in range(2,15):\n    km = KMeans(n_clusters= i, random_state=seed)\n    km.fit(X)\n    sil_scores.append(silhouette_score(X, km.labels_))","9fac2898":"plt.plot(range(2,15), sil_scores, 'og-')\nplt.annotate('Elbow', xy=(5, 50000), xytext=(6, 100000), arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xlabel(\"K\")\nplt.ylabel(\"Silhouette score\");","c2c10b18":"df['cluster'] = km.labels_","200c0b83":"df[df['cluster'] == 2].Genre.value_counts(normalize=True)","41971a03":"As you can see, this visualization is much richer than the previous one: in particular,\nalthough it confirms that k=5 is a very good choice, it also underlines the fact that\nk=6 is quite good as well, and much better than k=7 or 7. This was not visible when\ncomparing inertias.","f51aaf50":"Let's apply it on our dataset. ","1abe8b1e":"<a id='intro'><\/a>\n## Introduction to K-Means Clustering","173c5e19":"So we are now interested to segment our customers (separating customers into groups based on their sales, needs, etc..). This segmentation will improve the relationship between the company and customers.","e2609e05":"For simplicity, I will use only two features.","81c8a7c3":"As you see. If we run the K-Means multiple times, it will give different results (remember the first step, it pick k centroids **randomly**). \n\nFrom the three clusters that we get. I think the third is the best. But wait, How can we evaluate our clustering? It was easy for us to choose the third one here as we are working in two dimensions (with two features only). What if we are working with 5, 10, or more?\n\nWe have a performance metric. it's called inertia: it's the mean squared distance between each instance and its closest centroid.\n\nLet's calculate the inertia for each model. It's accessible via `.inertia_` attribute.","6374f68f":"We will use K-Means to segment these customers.\n\nK-Means proceeds like this: \n\n1. Pick k random points as \u201ccluster center\u201d\n2. Assign each data point to its closest cluster center.\n3. Recompute cluster centers as the mean of the assigned points.\n4. Repeat steps 2 and 3 until converge (converge means the assignment of the points to the clusters doesn't change anymore).","a7a649b3":"Kmeans saved a copy during training. it's accessible via `.labels_` attribute.","c8571d36":"<img src=\"https:\/\/imgur.com\/jZelYRN.jpg\" width=600>","b7c7d2c1":"\n# Intro to K-Means Clustering with sklearn\n\n## Table of Contents\n<ul>\n<li><a href=\"#intro\">Introduction to K-Means Clustering<\/a><\/li>\n<li><a href=\"#k\">Finding the Optimal Number of Clusters<\/a><\/li>\n<\/ul>","f5d4a747":"A more precise approach (but also more computationally expensive) is to use the silhouette score.\n\nThe silhouette coefficient can vary between -1 and +1: a coefficient close to\n+1 means that the instance is well inside its own cluster and far from other clusters,\nwhile a coefficient close to 0 means that it is close to a cluster boundary, and finally a\ncoefficient close to -1 means that the instance may have been assigned to the wrong cluster. \n[This video has a nice visualization for silhouette score](https:\/\/www.youtube.com\/watch?v=AtxQ0rvdQIA&t=313s).","cc96a49c":"I will explain these concepts using the Mall customers dataset. So let's start first by importing libs and taking a quick look on the dataset.","a42f07ff":"Note: I took some ideas and definitions from [Aur\u00e9lien Geron's book](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646). I highly recommend reading the unsupervised chapter to get deeper understanding.","63d2346e":"Let's plot the inertia","4f11446b":"Let\u2019s compare the silhouette scores for different numbers of clusters","458cd710":"But `predict` method can work too with new instances.","ee32cdba":"Now we have finished clustering. It's easy to interpret the above clustering and see the behavior of each cluster.\n\nWhen we have more than two or three dimensions. It's harder to interpret the results.\nSo we do EDA on each cluster to get more about it (why these customers are in the same segment\/cluster).\n\nFor example, Let's see if cluster two (the one with high income and high spending score) has more males than females or is balanced.","97c04b56":"Let's assume we don't have domain experience. So let's try different K.","70282833":"Should we pick the model with the lowest inertia? It's not that easy.\n\nAs you see, when we increase k, we get lower inertia.  Inertia will be zero if we choose k = number of observations (Each observation is a cluster).","86265763":"You can control how many times do you want to run your K-Means via `n_init`. Sklearn runs it 10 times by default and return the model with the best score. ","7148d5ad":"Yeah, the third one has the lowest inertia.","914eedbc":"As you can see, the inertia drops very quickly as we increase k up to 5, but then it\ndecreases much more slowly as we keep increasing k. \n\nLet's do clustering with k=5","633d9f8a":"It's not easy to choose K, and the result might be quite bad if you set it to the wrong value.\n\nIn general, there are two ways to choose K:\n\n    1- By domain experience \n    2- Exploring different Ks ","e6f2812d":"<a id='k'><\/a>\n## Finding the Optimal Number of Clusters","412c978e":"We can get the cluster of each point using `predict` method.","aca9aac5":"For example, if we choose k = 3","fa8be33d":"**Note:** \n\n- *K-Means is sensitive to outliers as we take the mean to calculate centroids. You can use median instead if you have outliers.*\n- *It's important to scale your data before feeding it to K-Means. If you didn't the feature with a larger range will affect distance more than the others.*"}}