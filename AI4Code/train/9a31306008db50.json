{"cell_type":{"eb03cdb8":"code","c86c3457":"code","03e41954":"code","b21a2421":"code","0f117336":"code","0f4ad6b9":"code","fa15af37":"code","3228076b":"code","41e65e7c":"code","94842c36":"code","f630e362":"code","e51b7bed":"code","1d543180":"code","3cc1a860":"code","aeee730a":"code","12c8badd":"code","b7fdb7db":"code","6641a19b":"code","22e53d59":"code","1875e35f":"code","ff8c31f3":"code","1de826e6":"code","0f53e70e":"code","2bd186af":"code","6a34b727":"code","ee032542":"code","5be51236":"code","e5a86c0e":"code","0baed6dc":"markdown","4e0431ad":"markdown","f569183e":"markdown","0516dd8a":"markdown","3d0785a5":"markdown","afec2690":"markdown","93fc50e2":"markdown","3746f467":"markdown","997e64f0":"markdown","67b82085":"markdown"},"source":{"eb03cdb8":"import numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c86c3457":"# Importing data visualization libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","03e41954":"df = pd.read_csv('\/kaggle\/input\/diamonds\/diamonds.csv')\ndf.head()","b21a2421":"# The Unnamed:0 column does not add any useful information. Let's simply delete it.\n\ndf.drop('Unnamed: 0', axis=1,inplace=True)","0f117336":"# High level information about the dataset\n\ndf.info()","0f4ad6b9":"# Let's get some high level insight about our numerical features\n\ndf.describe()","fa15af37":"# Let's see how many rows (instances) has either x, y or z values equal to zero\n\nlen(df[(df['x']==0) | (df['y']==0) | (df['z']==0)])","3228076b":"df = df[(df['x']!=0) & (df['y']!=0) & (df['z']!=0)]","41e65e7c":"# Now let's get some high level insight about our categorical features\n\ndf.describe(exclude=[np.number])","94842c36":"# Let's find out all the unique categories of the categorical features\n\ncategory_list = ['cut','color', 'clarity']\nfor cat in category_list:\n    print(f\"Unique values of {cat} column: {df[cat].unique()}\\n\")","f630e362":"# Pay attention that the order of categories are from the worst to the best.\n\ncut = pd.Categorical(df['cut'], categories=['Fair','Good','Very Good','Premium','Ideal'], ordered=True)\nlabels_cut, unique = pd.factorize(cut, sort=True)\ndf['cut'] = labels_cut\n\ncolor = pd.Categorical(df['color'], categories=['J','I','H','G','F','E','D'], ordered=True)\nlabels_color, unique = pd.factorize(color, sort=True)\ndf['color'] = labels_color\n\nclarity = pd.Categorical(df['clarity'], categories=['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF'], ordered=True)\nlabels_clarity, unique = pd.factorize(clarity, sort=True)\ndf['clarity'] = labels_clarity","e51b7bed":"# Let's take a look at our data frame now\n\ndf.head()","1d543180":"# Alright way better now. Time to check the corrolations between the features\n\nplt.figure(figsize=(12,12))\nsns.heatmap(df.corr(),annot=True,square=True,cmap='RdYlGn')","3cc1a860":"# How's the distribution of the target variale\n\nplt.figure(figsize=(12,5))\nsns.distplot(df['price'],bins=50)","aeee730a":"# Seems like there are lots of outliers in the price variable, for better visualization, I will use the boxplot:\n\nplt.figure(figsize=(10,3))\nsns.boxplot(data=df,x=df['price'])","12c8badd":"# Let's calculate the max in the boxplot above \n\niqr = df['price'].quantile(q=0.75) - df['price'].quantile(q=0.25)\nmaximum = df['price'].quantile(q=0.75) + 1.5*iqr\nmaximum","b7fdb7db":"# percentage of the instances above the maximum\n\nlen(df[df['price']>maximum])\/len(df)*100","6641a19b":"# Separating features form the target variable\n# Then, spliting the data into train and test datasets prior to feature scaling to avoid data leakage.\n# I chose the test_size=0.1 since the number of instances are big enough.\n\nX = df.drop('price',axis=1)\ny = df['price']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","22e53d59":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","1875e35f":"# Importing all the required libraries\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings('ignore')","ff8c31f3":"# Instanciating all the models that we are going to apply\n\nlr = LinearRegression()\nlasso = Lasso()\nridge = Ridge()\nsvr = SVR()\nknr = KNeighborsRegressor()\ndtree = DecisionTreeRegressor()\nrfr = RandomForestRegressor()\nabr = AdaBoostRegressor(n_estimators=1000)\nmlpr = MLPRegressor()\nxgb = xgboost.XGBRegressor()","1de826e6":"# For the sake of automation, let's create a function to train the model and generate the variance score \n\ndef R2_function(regressor,X_train,y_train,X_test,y_test):\n    regressor.fit(X_train,y_train)\n    predictions = regressor.predict(X_test)\n    return (metrics.explained_variance_score(y_test,predictions))","0f53e70e":"models_list = [lr, lasso, ridge, svr, knr, dtree, rfr, abr, mlpr, xgb]\n\nfor model in models_list:\n    print(f'{model} R2 score is: {R2_function(model,X_train,y_train,X_test,y_test)} \\n')","2bd186af":"from sklearn.model_selection import GridSearchCV","6a34b727":"# Random Forest GridSearch\n\nparams_dict = {'n_estimators':[20,40,60,80,100], 'n_jobs':[-1],'max_features':['auto','sqrt','log2']}\nrfr_GridSearch = GridSearchCV(estimator=RandomForestRegressor(), param_grid=params_dict,scoring='r2')\nrfr_GridSearch.fit(X_train,y_train)","ee032542":"rfr_GridSearch.best_params_","5be51236":"rfr_GridSearch_BestParam = RandomForestRegressor(max_features='auto',n_estimators=100,n_jobs=-1)\n\nrfr_GridSearch_BestParam.fit(X_train,y_train)\npredictions = rfr_GridSearch_BestParam.predict(X_test)\nprint(f\"R2 score: {metrics.explained_variance_score(y_test,predictions)}\")\nprint(f\"Mean absolute error: {metrics.mean_absolute_error(y_test,predictions)}\")\nprint(f\"Mean squared error: {metrics.mean_squared_error(y_test,predictions)}\")\nprint(f\"Root Mean squared error: {np.sqrt(metrics.mean_squared_error(y_test,predictions))}\")","e5a86c0e":"# Residual Histogram\n\nsns.distplot((y_test-predictions),bins=100)","0baed6dc":"The top three models are **RandomForestRegressor(R2=0.9846)**, **XGBRegressor(R2=0.9830)** and **KNeighborsRegressor(R2=0.9717)**.  \n\nNow it is time to perform **Hyperparameter Tuning** by using **GridSearchCV**.","4e0431ad":"According to the description of the features, **cut**, **color** and **clarity** are all Oridinal categorical features. There are various ways to convert Ordinal features into Numerical. OrdinalEncoder() is the most famous one. However, I am going to use factorize method (You can check out the following article to figure out why: [https:\/\/towardsdatascience.com\/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9](http:\/\/)).  \nIf there were any Nominal category, I would have used pd.get_dummies or OneHotEncoding (just a reminder). ","f569183e":"The target variable price has the highest correlation with **carat(0.92)**. \n\nprice is highly correlated to **x(0.89)**, **y(0.87)** and **z(0.87)** and also to themselves. At this point, there are two different approaches one can take. Either keep x, y and z as separate features, or drop them all and add ***xyz or volume*** as a new feature to the dataframe. I have tried both methods and it turned out keeping x, y and z, results in better performance.\n\nprice has surprisingly negative correlation with **cut(-0.053)**, **color(-0.17)**, **clarity(-0.15)** and **depth(-0.011)**.","0516dd8a":"**6.55%** is relatively a considerable number. Therefore, for now we will keep these instances in the dataframe rather than dropping them.  ","3d0785a5":"The next step is to perform feature scaling.  \nThere are different methods to choose between, **StandardScaler**, **MinMax Scaler**, **MaxAbs Scaler** and **Robust Scaler**.  \nI chose ***StandardScaler*** since it returned better results (you can try other ones and play with them yourself).","afec2690":"Just 20 rows out of 53,940 rows have either x, y or z values equal to zero.  \nDropping all of them should not be harmful to our final results.","93fc50e2":"**Thanks for reading and agian, I would appreciate your comments for improvement purposes.**","3746f467":"**There is something fishy here!  \nThe min for x, y and z are zero which does not make sense.  \nOn top of that, the min for depth (which is multiplication of x, y and z) is not zero!  \nWe need to take care of this issue, for sure.**","997e64f0":"# **The highest *Variance Score* I could achieve (0.9848)**\n\nIn this Kernel, I am going to share my thoughts and findings for the Diamonds dataset.  \nI would be greatful if you leave your comments about any sections you like or dislike\/disagree.   \n\n**General Steps**:\n1. [**Exploratory Data Analysis (EDA)**](#there_you_go_1)\n2. [**Data Cleaning**](#there_you_go_2)\n3. [**Preprocessing and Feature Engineering**](#there_you_go_3)\n4. [**Training Machine Learning Algorithms**](#there_you_go_4)\n5. [**Hyperparameter Tuning**](#there_you_go_5)","67b82085":"**53940 rows and 10 columns (9 features & 1 target).  \nAs you can see above, there are 3 columns with Categorical data type: cut, color and clarity.  \nWe will deal with them in a bit.  \nAlso, it looks like there is no missing value at all (*superficially though*).**"}}