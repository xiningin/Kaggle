{"cell_type":{"445ce3b8":"code","897698ab":"code","a902cd97":"code","85f585d1":"code","88b8da84":"code","99a298b3":"code","43e6e9d7":"code","9b9376df":"code","3a992c27":"code","c75e7873":"code","bc8c079b":"code","705d843b":"code","21da0463":"code","96e9a3c7":"code","8c6a4bc4":"code","bce0f19e":"code","31114a79":"code","4c077f5a":"code","a32d5b48":"code","216b573e":"markdown","650211f7":"markdown","c33108c1":"markdown","768872e3":"markdown","d4623235":"markdown","25f4b04a":"markdown","00a880b0":"markdown","3e215d6e":"markdown","296bc667":"markdown","85608259":"markdown","6e84b714":"markdown","cfcd29bd":"markdown","f0986253":"markdown","e681f7dd":"markdown","cdbb83af":"markdown"},"source":{"445ce3b8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re","897698ab":"os.chdir('..\/input')\nos.getcwd()","a902cd97":"toy_rev = pd.read_csv('..\/input\/Scrapped_Car_Reviews_Toyota.csv',engine='python',index_col=False)\ntoy_rev.head()","85f585d1":"toy_rev['review']=toy_rev['Review_Title']+toy_rev['Review']","88b8da84":"import spacy\nfrom tqdm import tqdm\nnlp = spacy.load('en_core_web_lg', parse=True, tag=True, entity=True)","99a298b3":"txt = 'Great car and has long range'\ndoc = nlp(txt)\nspacy.displacy.render(doc,style='dep',jupyter=True)","43e6e9d7":"txt = 'Drives well has great handling'\ndoc = nlp(txt)\nspacy.displacy.render(doc,style='dep',jupyter=True)","9b9376df":"txt =  \"wonderful to drive the camry \"\ndoc = nlp(txt)\nspacy.displacy.render(doc,style='dep',jupyter=True)","3a992c27":"txt =  \"not wonderful to drive the camry \"\ndoc = nlp(txt)\nspacy.displacy.render(doc,style='dep',jupyter=True)","c75e7873":"competitors = ['Chevy','chevy','Ford','ford','Nissan','nissan','Honda','honda','Chevrolet','chevrolet','Volkswagen','volkswagen','benz','Benz','Mercedes','mercedes','subaru','Subaru','VW']","bc8c079b":"aspect_terms = []\ncomp_terms = []\neaspect_terms = []\necomp_terms = []\nenemy = []\nfor x in tqdm(range(len(toy_rev['review']))):\n    amod_pairs = []\n    advmod_pairs = []\n    compound_pairs = []\n    xcomp_pairs = []\n    neg_pairs = []\n    eamod_pairs = []\n    eadvmod_pairs = []\n    ecompound_pairs = []\n    eneg_pairs = []\n    excomp_pairs = []\n    enemlist = []\n    if len(str(toy_rev['review'][x])) != 0:\n        lines = str(toy_rev['review'][x]).replace('*',' ').replace('-',' ').replace('so ',' ').replace('be ',' ').replace('are ',' ').replace('just ',' ').replace('get ','').replace('were ',' ').replace('When ','').replace('when ','').replace('again ',' ').replace('where ','').replace('how ',' ').replace('has ',' ').replace('Here ',' ').replace('here ',' ').replace('now ',' ').replace('see ',' ').replace('why ',' ').split('.')       \n        for line in lines:\n            enem_list = []\n            for eny in competitors:\n                enem = re.search(eny,line)\n                if enem is not None:\n                    enem_list.append(enem.group())\n            if len(enem_list)==0:\n                doc = nlp(line)\n                str1=''\n                str2=''\n                for token in doc:\n                    if token.pos_ is 'NOUN':\n                        for j in token.lefts:\n                            if j.dep_ == 'compound':\n                                compound_pairs.append((j.text+' '+token.text,token.text))\n                            if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n                                str1 = j.text+' '+token.text\n                                amod_pairs.append(j.text+' '+token.text)\n                                for k in j.lefts:\n                                    if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n                                        str2 = k.text+' '+j.text+' '+token.text\n                                        amod_pairs.append(k.text+' '+j.text+' '+token.text)\n                                mtch = re.search(re.escape(str1),re.escape(str2))\n                                if mtch is not None:\n                                    amod_pairs.remove(str1)\n                    if token.pos_ is 'VERB':\n                        for j in token.lefts:\n                            if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n                                advmod_pairs.append(j.text+' '+token.text)\n                            if j.dep_ is 'neg' and j.pos_ is 'ADV':\n                                neg_pairs.append(j.text+' '+token.text)\n                        for j in token.rights:\n                            if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n                                advmod_pairs.append(token.text+' '+j.text)\n                    if token.pos_ is 'ADJ':\n                        for j,h in zip(token.rights,token.lefts):\n                            if j.dep_ is 'xcomp' and h.dep_ is not 'neg':\n                                for k in j.lefts:\n                                    if k.dep_ is 'aux':\n                                        xcomp_pairs.append(token.text+' '+k.text+' '+j.text)\n                            elif j.dep_ is 'xcomp' and h.dep_ is 'neg':\n                                if k.dep_ is 'aux':\n                                        neg_pairs.append(h.text +' '+token.text+' '+k.text+' '+j.text)\n            \n            else:\n                enemlist.append(enem_list)\n                doc = nlp(line)\n                str1=''\n                str2=''\n                for token in doc:\n                    if token.pos_ is 'NOUN':\n                        for j in token.lefts:\n                            if j.dep_ == 'compound':\n                                ecompound_pairs.append((j.text+' '+token.text,token.text))\n                            if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n                                str1 = j.text+' '+token.text\n                                eamod_pairs.append(j.text+' '+token.text)\n                                for k in j.lefts:\n                                    if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n                                        str2 = k.text+' '+j.text+' '+token.text\n                                        eamod_pairs.append(k.text+' '+j.text+' '+token.text)\n                                mtch = re.search(re.escape(str1),re.escape(str2))\n                                if mtch is not None:\n                                    eamod_pairs.remove(str1)\n                    if token.pos_ is 'VERB':\n                        for j in token.lefts:\n                            if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n                                eadvmod_pairs.append(j.text+' '+token.text)\n                            if j.dep_ is 'neg' and j.pos_ is 'ADV':\n                                eneg_pairs.append(j.text+' '+token.text)\n                        for j in token.rights:\n                            if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n                                eadvmod_pairs.append(token.text+' '+j.text)\n                    if token.pos_ is 'ADJ':\n                        for j in token.rights:\n                            if j.dep_ is 'xcomp':\n                                for k in j.lefts:\n                                    if k.dep_ is 'aux':\n                                        excomp_pairs.append(token.text+' '+k.text+' '+j.text)\n        pairs = list(set(amod_pairs+advmod_pairs+neg_pairs+xcomp_pairs))\n        epairs = list(set(eamod_pairs+eadvmod_pairs+eneg_pairs+excomp_pairs))\n        for i in range(len(pairs)):\n            if len(compound_pairs)!=0:\n                for comp in compound_pairs:\n                    mtch = re.search(re.escape(comp[1]),re.escape(pairs[i]))\n                    if mtch is not None:\n                        pairs[i] = pairs[i].replace(mtch.group(),comp[0])\n        for i in range(len(epairs)):\n            if len(ecompound_pairs)!=0:\n                for comp in ecompound_pairs:\n                    mtch = re.search(re.escape(comp[1]),re.escape(epairs[i]))\n                    if mtch is not None:\n                        epairs[i] = epairs[i].replace(mtch.group(),comp[0])\n            \n    aspect_terms.append(pairs)\n    comp_terms.append(compound_pairs)\n    easpect_terms.append(epairs)\n    ecomp_terms.append(ecompound_pairs)\n    enemy.append(enemlist)\ntoy_rev['compound_nouns'] = comp_terms\ntoy_rev['aspect_keywords'] = aspect_terms\ntoy_rev['competition'] = enemy\ntoy_rev['competition_comp_nouns'] = ecomp_terms\ntoy_rev['competition_aspects'] = easpect_terms\ntoy_rev.head()","705d843b":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()","21da0463":"import operator\nsentiment = []\nfor i in range(len(toy_rev)):\n    score_dict={'pos':0,'neg':0,'neu':0}\n    if len(toy_rev['aspect_keywords'][i])!=0: \n        for aspects in toy_rev['aspect_keywords'][i]:\n            sent = analyser.polarity_scores(aspects)\n            score_dict['neg'] += sent['neg']\n            score_dict['pos'] += sent['pos']\n        #score_dict['neu'] += sent['neu']\n        sentiment.append(max(score_dict.items(), key=operator.itemgetter(1))[0])\n    else:\n        sentiment.append('NaN')\ntoy_rev['sentiment'] = sentiment\ntoy_rev.head()","96e9a3c7":"int_sent = []\nfor sent in toy_rev['sentiment']:\n    if sent is 'NaN':\n        int_sent.append('NaN')\n    elif sent is 'pos':\n        int_sent.append('1')\n    else:\n        int_sent.append('0')\ntoy_rev['int_sent'] = int_sent\ntoy_rev.head()","8c6a4bc4":"import math\npos = []\nfor i in range(len(toy_rev)):\n    if not math.isnan(toy_rev['Rating'][i]):\n        if int(toy_rev['Rating'][i])>3:\n            pos.append('1')\n        else:\n            pos.append('0')\n    else:\n        pos.append('0')\ntoy_rev['Positive Review'] = pos\ntoy_rev.head()","bce0f19e":"d = {'sent':toy_rev['Positive Review'],'sent_pred':toy_rev['int_sent']}\nmetric_df = pd.DataFrame(data=d)\nmetric_df.head()","31114a79":"len(metric_df.sent)","4c077f5a":"metric_df = metric_df[metric_df.sent_pred != 'NaN']\nlen(metric_df.sent)","a32d5b48":"from sklearn.metrics import accuracy_score,auc,f1_score,recall_score,precision_score\nprint('accuracy')\nprint(accuracy_score(metric_df.sent, metric_df.sent_pred))\nprint('f1 score')\nprint(f1_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))\nprint('recall')\nprint(recall_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))\nprint('precision')\nprint(precision_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))","216b573e":"## Possible improvements that can be made\n*  Tricky situation of removing stopwords to reduce unwanted extractions of non-aspects but this can also affect spaCy's dependency parsing. Same goes with noun chunk merging as well. If someone can think of a better way to remove stopwords and still retain spaCy's dependency goodness it can greatly improve the accuracy\n\n* This is not a ML task per se since we do more of parsing than ML. Although Bi-Directional LSTM have been very good at ABSA tasks in the past, unlike semeval tasks we do not have a fixed topic for our aspects to fall into. If someone can use the parsing aspect of the code to implement BLSTM in this case, that would be great\n\n* better alternatives to vaderSentiment if available (unsupervised\/ semi-supervised methods might be better here I think)\n\n* The very definition of aspects can be a bit vague at times hence we do not have a valid metric to measure the aspect extraction's accuracy\n","650211f7":"### ADVMOD - adverb modifier\n#### An adverb modifier of a word is a (non-clausal) adverb or adverb-headed phrase that serves to modify\n#### the meaning of the word\n### ex - 'Drives --advmod--> well'","c33108c1":"## **Combining the review title and review body for the text corpus****","768872e3":"### COMPOUND WORDS\n#### Generally from a review standpoint, compound words often do not offer us sentiments per se, hence my code looks for possible compound word pairs and then checks with the aspect words extracted if it can add more detail to the extracted aspects - ex Outstanding passenger van gives *more context* than Outstanding van (which is what my code would have extracted without the compound word search) while the compound word search will identify passenger van as a compound word","d4623235":"### NEG - self explanatory\n### ex - not <--neg-- wonderful","25f4b04a":"### Here we have arbitarily taken ratings greater than 3 as positive and everything else as negative","00a880b0":"## P.S This is my first Kaggle Kernel and I am fairly new to python programming as well, hence my non usage of list comprehensions and functions might be evident. I highly encourage everyone to fork my code and add your own twists to increase the accuracy of both aspect extractions and sentiment analysis.","3e215d6e":"# Aspect Based Sentiment Analysis on Car Reviews\n## Taking Toyota Cars as an example","296bc667":"**from https:\/\/nlp.stanford.edu\/software\/dependencies_manual.pdf**\n### AMOD - adjectival modifier\n#### An adjectival modifier of a Noun is any adjectival phrase that serves to modify the meaning of the Noun\n### ex - 'Great <--amod-- Car', 'Long <--amod-- range'","85608259":"## **Using spaCy for dependency parsing which forms the crux of aspect extraction**","6e84b714":"## We use vaderSentiment for sentiment analysis because of it's speed and simplicity. It offers 3 types of polarity -  positive, negative and neutral. As a result we can filter all aspects which have high neutral scores hence minimizing errors caused due to wrong extraction of aspects and stopwords","cfcd29bd":"### XCOMP -  open clausal complement\n#### An open clausal complement (xcomp) of a verb or an adjective is a predicative or clausal complement without its own subject\n### ex - 'wonderful --xcomp--> drive'\n","f0986253":"**Reason for using competitor name list is to remove potential misleading aspects-sentiments, since we are interested to acquire aspect info about Toyota and not any other brand. This is because a reviewer might be comparing a Benz saying it has superior handling when compared to the car the person is reviewing and this can lead to misclassifications******","e681f7dd":" ## **Using spaCy's awesome displacy module to show the dependency relations**","cdbb83af":"## Removing NaN values in the sentiment predictions"}}