{"cell_type":{"fb99080f":"code","8b381fa2":"code","d68a2f36":"code","04f50427":"code","2f5095e7":"code","f0c8f8c1":"code","913e7d56":"code","30128ae2":"code","f12a4736":"code","b3e1cae3":"code","9543e401":"code","ddb70dff":"code","ad1ae0f0":"code","4164b4e9":"code","6ea1f03e":"code","4baee126":"code","b173877d":"code","db03a6a2":"code","5ef5f4b4":"code","91d0fb21":"code","4c607d1d":"markdown","e648a4ec":"markdown","9638da29":"markdown","d9e14929":"markdown","b4a18fe7":"markdown","b089dbbe":"markdown","f386781c":"markdown","3505b710":"markdown","38c1a412":"markdown"},"source":{"fb99080f":"# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nimport matplotlib.font_manager as fm\n","8b381fa2":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport cv2\nimport numpy as np\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport random\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.utils import to_categorical, Sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom tqdm.auto import tqdm\nfrom collections import Counter\nfrom PIL import Image\nfrom itertools import groupby\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom ast import literal_eval\n%matplotlib inline\ntqdm.pandas()","d68a2f36":"data_folder = \"\/kaggle\/input\/banglasymbols\/\"\nprop = fm.FontProperties(fname=os.path.join(data_folder,'bangla.ttf'))\ntrain_folder  = os.path.join(data_folder,\"data\",\"data\",'train')\ntest_folder   = os.path.join(data_folder,\"data\",\"data\",'test')\n\ntrain_df=pd.read_csv(os.path.join(data_folder,'train.csv'))\ntest_df=pd.read_csv(os.path.join(data_folder,'test.csv'))\n# shuffle randomly\ntrain_df=train_df.sample(frac=1)\ntrain_df.dropna(inplace=True)\ntest_df.dropna(inplace=True)\ntrain_df","04f50427":"train_df.grapheme=train_df.grapheme.progress_apply(lambda x: literal_eval(x))\ntest_df.grapheme=test_df.grapheme.progress_apply(lambda x: literal_eval(x))\n","2f5095e7":"vocab=[]\nfor grapmeme_list in tqdm(train_df.grapheme):\n    vocab+=grapmeme_list\nfor grapmeme_list in tqdm(test_df.grapheme):\n    vocab+=grapmeme_list\n\nvocab=sorted(list(set(vocab)))\nlen(vocab)","f0c8f8c1":"train_df[\"length\"]=train_df.grapheme.progress_apply(lambda x: len(x))\ntest_df[\"length\"]=test_df.grapheme.progress_apply(lambda x: len(x))\nmax_label_len=max(train_df.length.max(),test_df.length.max())\nmax_label_len","913e7d56":"def get_label_idxes(glist):\n    data=[]\n    for g in glist:\n        data.append(vocab.index(g))\n    return pad_sequences([data], maxlen=max_label_len, padding='post', value=len(vocab))[0]\n\ntrain_df[\"encoded_label\"]=train_df.grapheme.progress_apply(lambda x: get_label_idxes(x))\ntest_df[\"encoded_label\"]=test_df.grapheme.progress_apply(lambda x: get_label_idxes(x))\ntrain_df","30128ae2":"train_df[\"img_path\"]=train_df.image.progress_apply(lambda x: os.path.join(train_folder,x))\ntest_df[\"img_path\"]=test_df.image.progress_apply(lambda x: os.path.join(test_folder,x))\ntrain_df","f12a4736":"img_width = 200\nimg_height = 50\n\ndef process_single_sample(img_path, label):\n\n    # 1. Read image\n    img = tf.io.read_file(img_path)\n\n    # 2. Decode and convert to grayscale\n    img = tf.io.decode_png(img, channels=1)\n\n    # 3. Convert to float32 in [0, 1] range\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    # 4. Resize to the desired size\n    img = tf.image.resize(img, [img_height, img_width])\n    \n    # 5. Transpose the image because we want the time\n    # dimension to correspond to the width of the image.\n    img = tf.transpose(img, perm=[1, 0, 2])\n\n    return {\"image\": img, \"label\": label}","b3e1cae3":"batch_size = 256\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_df.img_path.tolist(),train_df.encoded_label.tolist()))\ntest_dataset   = tf.data.Dataset.from_tensor_slices((test_df.img_path.tolist(),test_df.encoded_label.tolist()))\n\ntrain_dataset = (\n    train_dataset.map(\n        process_single_sample, \n        num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    .batch(batch_size)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n)\n\ntest_dataset = (\n    test_dataset.map(\n        process_single_sample, \n        num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    .batch(batch_size)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n)","9543e401":"# Mapping characters to integers\nchar_to_num = layers.experimental.preprocessing.StringLookup(\n    vocabulary=vocab, num_oov_indices=0, mask_token=None\n)\n\n# Mapping integers back to original characters\nnum_to_char = layers.experimental.preprocessing.StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n)\n\ntrain_data_fig, ax = plt.subplots(4, 4, figsize=(15, 10))\ntrain_data_fig.suptitle('Training data', weight='bold', size=18)\n\nfor batch in train_dataset.take(1):\n    images = batch[\"image\"]\n    labels = batch[\"label\"]\n    for i in range(16):\n        img = (images[i] * 255).numpy().astype(\"uint8\")\n        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n        label = label.replace('[UNK]', '')\n        \n        ax[i \/\/ 4, i % 4].imshow(img[:, :, 0], cmap=\"gray\")\n        ax[i \/\/ 4, i % 4].set_title(label,fontproperties=prop,fontsize=32)\n        ax[i \/\/ 4, i % 4].axis(\"off\")\n    \nplt.show()\n\n\n","ddb70dff":"char_list=sorted(vocab)","ad1ae0f0":"\n## Ref: https:\/\/keras.io\/examples\/vision\/captcha_ocr\/\n\nclass CTCLayer(layers.Layer):\n\n    def __init__(self, name=None):\n\n        super().__init__(name=name)\n        self.loss_fn = keras.backend.ctc_batch_cost\n\n    def call(self, y_true, y_pred):\n        # Compute the training-time loss value and add it\n        # to the layer using `self.add_loss()`.\n\n        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n        self.add_loss(loss)\n\n        # At test time, just return the computed predictions\n        return y_pred","4164b4e9":"\ndef ctc_decoder(predictions):\n    '''\n    input: given batch of predictions from text rec model\n    output: return lists of raw extracted text\n\n    '''\n    text_list = []\n    \n    pred_indcies = np.argmax(predictions, axis=2)\n    \n    for i in range(pred_indcies.shape[0]):\n        ans = \"\"\n        \n        ## merge repeats\n        merged_list = [k for k,_ in groupby(pred_indcies[i])]\n        \n        ## remove blanks\n        for p in merged_list:\n            if p != len(char_list):\n                ans += char_list[int(p)]\n        \n        text_list.append(ans)\n        \n    return text_list","6ea1f03e":"\nclass PlotPredictions(tf.keras.callbacks.Callback):\n\n    def __init__(self, frequency=1):\n        self.frequency = frequency\n        super(PlotPredictions, self).__init__()\n\n        batch = test_dataset.take(1)\n        self.batch_images = list(batch.as_numpy_iterator())[0][\"image\"]\n        self.batch_labels = list(batch.as_numpy_iterator())[0][\"label\"]\n\n    def plot_predictions(self, epoch):\n\n        prediction_model = keras.models.Model(\n            self.model.get_layer(name=\"image\").input, \n            self.model.get_layer(name=\"dense2\").output\n        )\n        \n        preds = prediction_model.predict(self.batch_images)\n        pred_texts = ctc_decoder(preds)\n\n        orig_texts = []\n\n        for label in self.batch_labels:\n            orig_texts.append(\"\".join([char_list[int(char_ind)] for char_ind in label if not(char_ind == len(char_list))]))\n\n        fig , ax = plt.subplots(4, 4, figsize=(15, 5))\n        fig.suptitle('Epoch: '+str(epoch), weight='bold', size=14)\n\n        for i in range(16):\n\n            img = (self.batch_images[i, :, :, 0] * 255).astype(np.uint8)\n            title = f\"Prediction: {pred_texts[i]}\"\n            ax[i \/\/ 4, i % 4].imshow(img, cmap=\"gray\")\n            ax[i \/\/ 4, i % 4].set_title(title,fontproperties=prop,fontsize=32)\n            ax[i \/\/ 4, i % 4].axis(\"off\")\n        \n        plt.show()\n        #plt.savefig(\"predictions_epoch_\"+ str(epoch)+'.png', bbox_inches = 'tight', pad_inches = 0)\n        \n       \n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % self.frequency == 0:\n            self.plot_predictions(epoch)","4baee126":"def get_model():\n    # Inputs to the model\n    input_img = layers.Input(\n        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n    )\n    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n\n    # First conv block\n    x = layers.Conv2D(\n        32,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv1\",\n    )(input_img)\n    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n\n    # Second conv block\n    x = layers.Conv2D(\n        64,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv2\",\n    )(x)\n    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n\n    # We have used two max pool with pool size and strides 2.\n    # Hence, downsampled feature maps are 4x smaller. The number of\n    # filters in the last layer is 64. Reshape accordingly before\n    # passing the output to the RNN part of the model\n    new_shape = ((img_width \/\/ 4), (img_height \/\/ 4) * 64)\n    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n    x = layers.Dropout(0.2)(x)\n\n    # RNNs\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n\n    # Output layer\n    x = layers.Dense(len(char_list) + 1, activation=\"softmax\", name=\"dense2\")(x)\n\n    # Add CTC layer for calculating CTC loss at each step\n    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n\n    # Define the model\n    model = keras.models.Model(\n        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n    )\n    # Optimizer\n    opt = keras.optimizers.Adam(lr = 0.001)\n    # Compile the model and return\n    model.compile(optimizer=opt)\n\n    return model","b173877d":"def train(model, epochs):\n    \n    file_path = \"C_LSTM_best.h5\"\n    \n    checkpoint = ModelCheckpoint(filepath=file_path, \n                                monitor='val_loss', \n                                verbose=1, \n                                save_best_only=True, \n                                mode='min')\n\n    callbacks_list = [checkpoint, \n#                       PlotPredictions(frequency=1),\n                      EarlyStopping(patience=3, verbose=1)\n                     ]\n\n    history = model.fit(train_dataset, \n                        epochs = epochs,\n                        validation_data=test_dataset,\n                        verbose = 1,\n                        callbacks = callbacks_list,\n                        shuffle=True)\n    \n    return model","db03a6a2":"model = get_model()","5ef5f4b4":"model.summary()","91d0fb21":"train(model, epochs=30)","4c607d1d":"* Can set Bangla Labels in title","e648a4ec":"# Vocabulary","9638da29":"# Imports","d9e14929":"# Data","b4a18fe7":"# plot callback","b089dbbe":"# Model training and callbacks","f386781c":"# CTC Base","3505b710":"* Loading time is a problem","38c1a412":"# Dataset"}}