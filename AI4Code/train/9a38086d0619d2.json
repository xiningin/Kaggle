{"cell_type":{"0e962c0c":"code","690fb187":"code","865f3878":"code","8c0c3e6a":"code","282be42e":"code","829ed886":"code","d962d4b6":"code","8db48618":"code","fa2afef3":"markdown"},"source":{"0e962c0c":"import pandas as pd\nimport numpy as np","690fb187":"data_train_raw_df = pd.read_csv('..\/input\/genre-classification-dataset-imdb\/Genre Classification Dataset\/train_data.txt', delimiter = \" ::: \", header=None, index_col=0,engine='python')\ndata_train_raw_df.columns=['Title','Genre','Synopsis']\ntemp = pd.get_dummies(data_train_raw_df.Genre)\ndata_train_raw_df = pd.concat([data_train_raw_df, temp], axis=1)\nprint('Shape : ',data_train_raw_df.shape)\ndata_train_raw_df.head()","865f3878":"CLASS_NAME = list(set(data_train_raw_df.Genre))\nCLASS_NAME","8c0c3e6a":"data_xytest_raw_df = pd.read_csv('..\/input\/genre-classification-dataset-imdb\/Genre Classification Dataset\/test_data_solution.txt', delimiter = \" ::: \", header=None, index_col=0,engine='python')\ndata_xytest_raw_df.columns=['Title','Genre','Synopsis']\ntemp = pd.get_dummies(data_xytest_raw_df.Genre)\ndata_xytest_raw_df = pd.concat([data_xytest_raw_df, temp], axis=1)\nprint('Shape : ',data_xytest_raw_df.shape)\ndata_xytest_raw_df.head()","282be42e":"import pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nimport os\n\nprint(\"tensorflow version : \", tf.__version__)\nprint(\"tensorflow_hub version : \", hub.__version__)","829ed886":"EPOCH = 5\nBATCH = 32\nSAMEPLE_SIZE = 20000","d962d4b6":"data_train_raw_df = data_train_raw_df.iloc[0:SAMEPLE_SIZE,:]","8db48618":"#######################################\n### -------- Load libraries ------- ###\n\n# Load Huggingface transformers\nfrom transformers import TFBertModel,  BertConfig, BertTokenizerFast\n\n# Then what you need from tensorflow.keras\nfrom tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n\n# And pandas for data import + sklearn because you allways need sklearn\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\n#######################################\n### --------- Import data --------- ###\n\n# Import data from csv\ndata = data_train_raw_df.copy()\n\n# Select required columns\ndata = data[['Synopsis','Genre']]\n\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndata['Genre'] = le.fit_transform(data.Genre)\n# Remove a row if any of the three remaining columns are missing\ndata = data.dropna()\n\n\n\n# Split into train and test - stratify over Issue\ndata, data_test = train_test_split(data, test_size = 0.2, stratify = data['Genre'])\n\ny_test = le.inverse_transform(data_test['Genre'])\n\n\n#######################################\n### --------- Setup BERT ---------- ###\n\n# Name of the BERT model to use\nmodel_name = 'bert-base-uncased'\n\n# Max length of tokens\nmax_length = 200\n\n# Load transformers config and set output_hidden_states to False\nconfig = BertConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the Transformers BERT model\ntransformer_model = TFBertModel.from_pretrained(model_name, config = config)\n\n\n#######################################\n### ------- Build the model ------- ###\n\n# TF Keras documentation: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model\n\n# Load the MainLayer\nbert = transformer_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n# attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32') \n# inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers BERT model as a layer in a Keras model\nbert_model = bert(inputs)[1]\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(bert_model, training=False)\n\n# Then build your model output\nGenre = Dense(units=len(data.Genre.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='Genre')(pooled_output)\n\noutputs = {'Genre': Genre}\n\n# And combine it all in a model object\nmodel = Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n\n# Take a look at the model\nmodel.summary()\n\n\n#######################################\n### ------- Train the model ------- ###\n\n# Set an optimizer\noptimizer = Adam(\n    learning_rate=5e-05,\n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'Genre': CategoricalCrossentropy(from_logits = True)}\nmetric = {'Genre': CategoricalAccuracy('accuracy')}\n\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)\n\n# Ready output data for the model\ny_Genre = to_categorical(data['Genre'])\n\n\n# Tokenize the input (takes some time)\nx = tokenizer(\n    text=data['Synopsis'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\n# Fit the model\nhistory = model.fit(\n    # x={'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']},\n    x={'input_ids': x['input_ids']},\n    y={'Genre': y_Genre},\n    validation_split=0.2,\n    batch_size=BATCH,\n    epochs=EPOCH)\n\n\n#######################################\n### ----- Evaluate the model ------ ###\n\n# Ready test data\ntest_y_Genre = to_categorical(data_test['Genre'])\n\ntest_x = tokenizer(\n    text=data_test['Synopsis'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = True)\n\n# Run evaluation\nmodel.predict(x={'input_ids': test_x['input_ids']})\ny_predicted_raw = model.predict(x={'input_ids': test_x['input_ids']})\n\ny_predicted = le.inverse_transform([np.argmax(y_predicted_raw['Genre'][index,:], axis=None, out=None) for index in np.arange(len(y_predicted_raw['Genre']))])\ny_predicted\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predicted, target_names=CLASS_NAME))","fa2afef3":"https:\/\/www.kaggle.com\/abhinand05\/bert-for-humans-tutorial-baseline\nhttps:\/\/medium.com\/analytics-vidhya\/bert-in-keras-tensorflow-2-0-using-tfhub-huggingface-81c08c5f81d8\n\n\nhttps:\/\/towardsdatascience.com\/multi-label-multi-class-text-classification-with-bert-transformer-and-keras-c6355eccb63a"}}