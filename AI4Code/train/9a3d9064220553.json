{"cell_type":{"fee47b48":"code","464b42af":"code","fa430c8e":"code","ab4c8f19":"code","531b7a76":"code","930bbfe0":"markdown","b7e8e92a":"markdown","0017b6c3":"markdown","630ea9ce":"markdown","f39f8a7a":"markdown","091744eb":"markdown","d24c6875":"markdown","ae880536":"markdown","95f3b3e6":"markdown","7fc71c6a":"markdown"},"source":{"fee47b48":"import torch\n\nimport matplotlib.pyplot as plt","464b42af":"#### Defining Helper function to visualize tensors\ndef tensor_status(**kwargs):\n    print(\"\\n\")\n    for arg_name in kwargs:\n        t= kwargs[arg_name]\n        t_name=arg_name\n    print(\"Status of tensor \"+t_name+\":\\n\")\n    print(\"==============\")\n    print(\"Data: \"+str(t.data))\n    print(\"grad: \"+str(t.grad) if t.is_leaf else str(None))\n    print(\"grad_fn: \"+str(t.grad_fn))\n    print(\"is_leaf: \"+str(t.is_leaf))\n    print(\"requires_grad:\"+str(t.requires_grad))\n    print(\"==============\\n\")\n    return \"\"","fa430c8e":"#Building the Neural Net\n#======================\n#Inputs\nx=torch.Tensor([5,\n                3])\nprint(\"Inputs:\\n\",x,\"\\n\")\n\n#Randomly initialized weights\nW_x_z=torch.randn(2,2,requires_grad=True)\nprint(\"Weights X -> Z:\\n\",W_x_z,\"\\n\")\n\nW_z_y=torch.randn(2,1,requires_grad=True)\nprint(\"Weights Z -> Y:\\n\",W_z_y,\"\\n\")\n\n#Define output\ny=torch.Tensor([7.0])\n#======================\n\n\n#Loss Function\nloss_fn=torch.nn.MSELoss()\n\n\n#FORWARD PROPAGATION\n#======================\nprint(\"\\nForward Propagation:\\n\")\n\nz = torch.matmul(x , W_x_z )\nprint(\"Hidden Layer:\\n\",z,\"\\n\")\n\ny_pred = torch.matmul(z,W_z_y)\nprint(\"Output:\\n\",y_pred,\"\\n\\nExpected Value :\\n\",y,\"\\n\\n\")\n#======================\n\n#Calculate Error(loss)\nloss=loss_fn(y_pred,y)\nprint(\"Mean Squared Error : \" + str(loss.item()))\n\n#BACKPROPAGATION\nloss.backward()","ab4c8f19":"print(loss.grad_fn)\nprint(\"next_functions : \\n\",loss.grad_fn.next_functions)\n\nprint(\"\\t|\\n\\t|\\n\\t|\\n\\t|\\n\\t|\")\nprint(loss.grad_fn.next_functions[0][0])\nprint(\"next_functions : \\n\",loss.grad_fn.next_functions[0][0].next_functions)\n\nprint(\"\\t|\\n\\t|\\n\\t|\\n\\t|\\n\\t|\")\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0])\nprint(\"next_functions : \\n\",loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n\nprint(\"\\t|\\n\\t|\\n\\t|\\n\\t|\\n\\t|\")\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0])\nprint(\"next_functions : \\n\",loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n\nprint(\"\\t|\\n\\t|\\n\\t|\\n\\t|\\n\\t|\")\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0])\nprint(\"next_functions : \\n\",loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n\nprint(\"\\t|\\n\\t|\\n\\t|\\n\\t|\\n\\t|\")\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0])\nprint(\"next_functions : \\n\",loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)","531b7a76":"tensor_status(W_x_z=W_x_z)\ntensor_status(W_z_y=W_z_y)","930bbfe0":"## $W_{i}\\rightarrow W_{i} - \\alpha \\frac{\\partial Loss}{\\partial W_{i}}$","b7e8e92a":"## Example :\n\n#### We're going to build a simple Perceptron,as in the figure :\n\n![](https:\/\/i.stack.imgur.com\/RClFY.png)","0017b6c3":"### Strcture of a tensor :\n* `Data : Data of the tensor`\n* `grad :  value of the accumulated gradient calculated after a call to backward()`\n* `grad_fn :  function \"handle\", giving access to the applicable gradient function`\n* `is_leaf : Boolean specifying if the tensor is a leaf (not a node) in the computational graph, by default only leaf nodes will get their gradients computed`\n* `requires_grad : Boolean specifying wheter Pytorch needs to track and calculate gradients for this tensor.`","630ea9ce":"### Under the hood, Pytorch had built a **Backwards graph**, that keeps track of the *backward* functions to use to calculate gradients :","f39f8a7a":"#### *Keep in mind that these computations are implemented in C++ and made available in Python through Python bindings,so a lot of processes aren't visible from the frontend*","091744eb":"### After calling `loss.backward()` Pytorch sets the `grad` attribute of all tensors with `requires_grad=True` in the computational graph of which `loss` will be the last component","d24c6875":"### Graph of the computation :\n![](https:\/\/i.postimg.cc\/wv0V45nY\/Untitled-Diagram-drawio-2.png)","ae880536":"#### The output above shows the structure of such graph,that can now be used by Pytorch to traverse the computations backwards,propagating the weights ,and, in some instances, a call to `AccumulateGrad`, that will update the `grad` value of the tensors involved in that specific calculation:","95f3b3e6":"#### As you can see, gradients have been calculated, and those values can now be used to update the weights,like so:","7fc71c6a":"# Autograd\n\n#### *Autograd* is PyTorch's automatic differentiation engine used to calculate gradients of the model parameters (weights) during the neural network's backpropagation phase"}}