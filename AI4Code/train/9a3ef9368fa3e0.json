{"cell_type":{"d99527c6":"code","dde0e810":"code","7b82c362":"code","f7e4a666":"code","1207421b":"code","0f9e3d37":"code","e0033964":"code","ad11edd0":"code","2cd74971":"code","9ed7b897":"code","ab68e5bd":"code","1557ef0e":"code","f348bba2":"code","4037fc62":"code","06d25fb0":"code","89cef35f":"code","19876731":"code","543995a8":"code","dee5ae4e":"code","4f9baf8c":"code","1ca5514e":"code","16eeb12b":"code","a19e915b":"code","a0cecd71":"code","089287d9":"code","3cc5a7d3":"code","20ef9ada":"code","66c43916":"code","45be2558":"code","70cce136":"code","83d8b4ca":"code","403f6a53":"code","299fd562":"code","164b3fda":"code","337c33c3":"code","948d66ad":"code","08ea5bc7":"code","8011db3d":"code","3b0d1c18":"code","43e58eda":"code","5c3cf1de":"code","d61eacb4":"code","1d95ccd6":"code","9e47b15d":"code","b05d512a":"code","543a5fff":"markdown","3a6d1db8":"markdown","bf903ea3":"markdown","bbdca5ce":"markdown","84fe2811":"markdown","d341b6b9":"markdown","17dba304":"markdown","48a07093":"markdown","d7307c6a":"markdown","62669fb1":"markdown","ca4bc806":"markdown","cd376127":"markdown","4fbcc1fc":"markdown","27dd8da1":"markdown","88abaaf5":"markdown","405935a6":"markdown","74f50ea4":"markdown","530a7897":"markdown","c6490c24":"markdown","35aa473c":"markdown","a7af1dae":"markdown","ee941d45":"markdown","0950b5df":"markdown","ac5102e5":"markdown","eb055106":"markdown","a1d98e10":"markdown"},"source":{"d99527c6":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\n# statistics tools\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# ML tools\nimport h2o\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator","dde0e810":"# load data\ndf = pd.read_csv('..\/input\/telecom-users-dataset\/telecom_users.csv')\ndf.head()","7b82c362":"# conversions\ndf.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce') # has some string values, force them to NaN\ndf.SeniorCitizen = df.SeniorCitizen.astype('object')\n\n# drop first column\ndf = df.drop('Unnamed: 0', axis=1)","f7e4a666":"# show structure of data frame\ndf.info()","1207421b":"df[df.TotalCharges.isna()]","0f9e3d37":"df.TotalCharges = df.TotalCharges.fillna(0)","e0033964":"# define numerical features\nfeatures_num = ['tenure','MonthlyCharges','TotalCharges']","ad11edd0":"# define categorical features\nfeatures_cat = ['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n                'PhoneService', 'MultipleLines', 'InternetService',\n                'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n                'TechSupport','StreamingTV', 'StreamingMovies',\n                'Contract', 'PaperlessBilling','PaymentMethod']","2cd74971":"# plot numerical distributions\nfor f in features_num:\n    df[f].plot(kind='hist', bins=20)\n    plt.title(f)\n    plt.grid()\n    plt.show()","9ed7b897":"# pairwise scatter plot\nsns.pairplot(df[features_num], \n             kind='reg', \n             plot_kws={'line_kws':{'color':'magenta'}, 'scatter_kws': {'alpha': 0.05}})\nplt.show()","ab68e5bd":"# Pearson (linear) correlation\ncorr_pearson = df[features_num].corr(method='pearson')\n\nfig = plt.figure(figsize = (5,4))\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()","1557ef0e":"# Spearman (Rank) correlation\ncorr_spearman = df[features_num].corr(method='spearman')\n\nfig = plt.figure(figsize = (5,4))\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","f348bba2":"# plot categorical distributions\nfor f in features_cat:\n    df[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","4037fc62":"target = 'Churn'","06d25fb0":"# eval frequencies\nprint(df[target].value_counts())\n# and plot\ndf[target].value_counts().plot(kind='bar', color='darkred')\nplt.title('Target = Churn')\nplt.grid()\nplt.show()","89cef35f":"# add binned version of numerical features for plotting\n\n# quantile based:\ndf['BIN_tenure'] = pd.qcut(df['tenure'], q=10, precision=1)\ndf['BIN_MonthlyCharges'] = pd.qcut(df['MonthlyCharges'], q=10, precision=1)\ndf['BIN_TotalCharges'] = pd.qcut(df['TotalCharges'], q=10, precision=1)","19876731":"# plot target vs features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_num:\n    f_bin = 'BIN_' + f\n    plt.rcParams['figure.figsize'] = (14,5) # increase plot size for mosaics\n    mosaic(df, [f_bin, target], title='Target vs ' + f + ' [binned]')\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","543995a8":"# plot target vs features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_cat:\n    plt.rcParams['figure.figsize'] = (10,5) # increase plot size for mosaics\n    mosaic(df, [f, target], title='Target vs ' + f)\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","dee5ae4e":"# select predictors\npredictors = features_num + features_cat\nprint('Number of predictors: ', len(predictors))\nprint(predictors)","4f9baf8c":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","1ca5514e":"# upload data frame in H2O environment\ndf_hex = h2o.H2OFrame(df)\n\n# train \/ test split (70\/30)\ntrain_hex, test_hex = df_hex.split_frame(ratios=[0.7], seed=999)","16eeb12b":"# define (distributed) Random Forest model\nn_CV = 5 # cross validations\n\nfit_1 = H2ORandomForestEstimator(ntrees=50,\n                                   max_depth=15,\n                                   min_rows=5,\n                                   nfolds=n_CV,\n                                   model_id='DRF_1',\n                                   seed=999)","a19e915b":"# train model\nt1 = time.time()\nfit_1.train(x=predictors,\n            y=target,\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","a0cecd71":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","089287d9":"# show scoring history - training vs cross validations\nfor i in range(n_CV):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [AUC]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_auc, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_auc, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylim(0.75,1)\n    plt.legend()\n    plt.grid()\n    plt.show()","3cc5a7d3":"# calc performance on TRAINING data\nperf_train_1 = fit_1.model_performance(train_hex)\n# ROC curve\nperf_train_1.plot()","20ef9ada":"# predict on training data\npred_train_1 = fit_1.predict(train_hex)\n# and check calibration (using class probabilities)\nprint('Predicted frequency:', pred_train_1['Yes'].sum())\nprint('Actual frequency   :', train_hex[target].sum())","66c43916":"# confusion matrix using manual threshold\n# => our goal is to approximately match actual and predicted values\ntt_1 = 0.4165\nconf_train_1 = perf_train_1.confusion_matrix(thresholds=tt_1)\nconf_train_1.show()","45be2558":"# show standard variable importance\nfit_1.varimp_plot(-1)","70cce136":"# variable importance using SHAP values\n# => see direction as well as severity of feature impact\nt1 = time.time()\nfit_1.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","83d8b4ca":"# calc performance on TEST test\nperf_test_1 = fit_1.model_performance(test_hex)\n# ROC curve\nperf_test_1.plot()","403f6a53":"# confusion matrix using manual threshold (from training)\nconf_test_1 = perf_test_1.confusion_matrix(thresholds=tt_1)\nconf_test_1.show()","299fd562":"# define Gradient Boosting model\nfit_2 = H2OGradientBoostingEstimator(ntrees = 50,\n                                     max_depth=4,\n                                     min_rows=5,\n                                     learn_rate=0.05,\n                                     sample_rate=1,\n                                     col_sample_rate=0.7,\n                                     nfolds=n_CV,\n                                     model_id='GBM_1',\n                                     seed=999)","164b3fda":"# train model\nt1 = time.time()\nfit_2.train(x=predictors,\n            y=target,\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","337c33c3":"# show cross validation metrics\nfit_2.cross_validation_metrics_summary()","948d66ad":"# show scoring history - training vs cross validations\nfor i in range(n_CV):\n    cv_model_temp = fit_2.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [AUC]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_auc, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_auc, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.ylim(0.75,1)\n    plt.xlabel('Number of Trees')\n    plt.legend()\n    plt.grid()\n    plt.show()","08ea5bc7":"# predict on training data\npred_train_2 = fit_2.predict(train_hex)\n# and check calibration\nprint('Predicted frequency:', pred_train_2['Yes'].sum())\nprint('Actual frequency   :', train_hex[target].sum())","8011db3d":"# calc performance on TRAINING data\nperf_train_2 = fit_2.model_performance(train_hex)\n# ROC curve\nperf_train_2.plot()","3b0d1c18":"# confusion matrix using manual threshold\n# => our goal is to approximately match actual and predicted values\ntt_2 = 0.417\nconf_train_2 = perf_train_2.confusion_matrix(thresholds=tt_2)\nconf_train_2.show()","43e58eda":"# show standard variable importance\nfit_2.varimp_plot(-1)","5c3cf1de":"# calc performance on TEST test\nperf_test_2 = fit_2.model_performance(test_hex)\n# ROC Curve\nperf_test_2.plot()","d61eacb4":"# confusion matrix using our manual threshold\nconf_test_2 = perf_test_2.confusion_matrix(thresholds=tt_2)\nconf_test_2.show()","1d95ccd6":"# pull test set from H2O environment in pandas world\ndf_test = test_hex.as_data_frame()\n\n# cosmetics: remove binned versions of numerical features\ndf_test = df_test.drop(['BIN_tenure','BIN_MonthlyCharges','BIN_TotalCharges'], axis=1)\n\n# predict on test set (both models)\npred_test_1 = fit_1.predict(test_hex).as_data_frame()\npred_test_2 = fit_2.predict(test_hex).as_data_frame()\n\n# and add prediction to test data frame\ndf_test['pred_churn_RF'] = pred_test_1.Yes\ndf_test['pred_churn_GB'] = pred_test_2.Yes\n\n# add also blend of both models\ndf_test['pred_churn_blend'] = 0.5*pred_test_1.Yes + 0.5*pred_test_2.Yes\n\n# show preview\ndf_test.head()","9e47b15d":"# compare predictions of both models\nsns.jointplot(data=df_test, x='pred_churn_RF', y='pred_churn_GB',\n              kind='reg',\n              joint_kws = {'scatter_kws' : {'alpha' : 0.25}, \n                           'line_kws' : {'color' : 'magenta'}},\n              xlim=(0,1),ylim=(0,1))\nplt.show()","b05d512a":"# our top 10 risks (in the test set) according to blend of models\ntop10 = df_test.nlargest(10, columns='pred_churn_blend')\ntop10","543a5fff":"### Some initial clean up first","3a6d1db8":"#### => Slightly better Test AUC than Random Forest model...","bf903ea3":"#### Interpretations: Churn probability decreases with tenure and (more or less) increases with monthly charge.","bbdca5ce":"#### Now the real check on the test set:","84fe2811":"<a id='4'><\/a>\n# Model - Random Forest","d341b6b9":"#### However, the results on the test set are a little bit more skewed: 466 predicted churns vs. 478 actual ones...","17dba304":"<a id='2'><\/a>\n# Explore feature distributions ","48a07093":"### We focus on the test set in the following!","d7307c6a":"<a id='5'><\/a>\n# Model - Gradient Boosting","62669fb1":"# Explore telecom users dataset and build predictive models for churn\n\n## Table of Contents\n* [Import and Preprocessing](#1)\n* [Explore feature distributions](#2)\n* [Explore Target and Impact of each Feature](#3)\n* [Model - Random Forest](#4)\n* [Model - Gradient Boosting](#5)\n* [Individuals with highest likelihood to churn](#6)","ca4bc806":"#### Nicely calibrated: 1111 predicted churns vs. 1109 actual ones...","cd376127":"### Target vs Categorical Features","4fbcc1fc":"#### A few interpretations: Gender does not really seem to matter, whereas \"SeniorCitizen\", \"Partner\" and \"Dependents\" do make a difference. No internet services seem to imply a low churn risk. High impact of month-to-month payments and electronic check payment etc.","27dd8da1":"<a id='1'><\/a>\n# Import and Preprocessing","88abaaf5":"### Numerical Features","405935a6":"#### Nicely calibrated: 1107 predicted churns vs. 1109 actual ones...","74f50ea4":"### Categorical Features","530a7897":"<a id='3'><\/a>\n# Explore Target and Impact of each Feature","c6490c24":"#### Now again the real check on the test set:","35aa473c":"#### Again our mosaic plots","a7af1dae":"#### Unfortunately there are 10 missings for \"TotalCharges\". Let's check those guys.","ee941d45":"### Target vs Numerical Features","0950b5df":"<a id='6'><\/a>\n# Individuals with highest likelihood to churn","ac5102e5":"#### Ok, this is easier than expected. In all those cases tenure = 0, so we can set the TotalCharges to 0 as well.","eb055106":"#### Let's use mosaic plots to show the impact of the binned features on the target:","a1d98e10":"#### Still nice fit also on test set: 474 predicted churns vs 478 actual ones."}}