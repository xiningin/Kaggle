{"cell_type":{"1c5054cb":"code","05676d02":"code","f4dc2cc8":"code","16fa3d32":"code","a8ce3cd2":"code","320c796e":"code","88344b74":"code","bfa24367":"code","5af3243a":"code","11dea1fd":"code","e6495cee":"code","b8bbe368":"code","0e883ce3":"code","6a03a2f3":"code","2313d9b9":"code","6f0a3434":"code","fee533c4":"code","597c697c":"code","8f97bfa6":"code","c06c92c8":"code","68bba013":"code","9992423e":"code","3d478aad":"code","1512bf42":"code","14d2daab":"code","4a0be5df":"code","09d10ff5":"code","a89551f3":"code","76ec9b1a":"code","17485926":"code","9d2d55a3":"code","d230a881":"code","bb3d726f":"code","e79c3352":"code","1b966c12":"code","c7bf0274":"code","be95fc41":"code","e75d6e95":"code","fd68f35b":"code","ef69fe20":"code","dc00504a":"code","b02af309":"code","79249782":"code","70d8513f":"code","15a25342":"code","c5f0f97c":"code","8236fbee":"code","942682f2":"code","a32cffb8":"code","43eb7781":"code","864ac829":"code","1e884b24":"code","612527ca":"code","b2fe7a64":"code","bae9b7df":"code","71decd3d":"code","13429db6":"code","1da8b4d9":"code","097d7c3f":"code","30bf57f2":"code","5d6519a6":"code","9f51960b":"code","7d74c5b4":"code","13042d97":"code","7121f48b":"code","34a67385":"code","235e65cb":"code","f379a878":"code","8922417f":"code","41f2f3c9":"code","6c64b5ae":"code","599000f8":"code","9e530fea":"code","4fc8a1d4":"code","d88abbd8":"code","937b4969":"code","c574cded":"code","bf8da6d6":"code","b7db33a1":"code","57609e13":"code","87620ca7":"code","82f5b05b":"markdown","320e2adf":"markdown","91a7a511":"markdown","68d46687":"markdown","fc781341":"markdown","302336bb":"markdown","c951b817":"markdown","6c234721":"markdown","4322fce7":"markdown"},"source":{"1c5054cb":"import numpy as np \nimport pandas as pd \nimport sklearn\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom pylab import rcParams\nimport plotly\nimport plotly.graph_objs as go\nimport plotly\nimport plotly.figure_factory as ff\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","05676d02":"dataset = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndataset.shape","f4dc2cc8":"dataset.columns","16fa3d32":"dataset.isnull().values.any()","a8ce3cd2":"dataset.describe().T","320c796e":"dataset.info()","88344b74":"dataset.head()","bfa24367":"dataset.shape","5af3243a":"import plotly.express as px\ndf = pd.value_counts(dataset['Class'], sort = True).sort_index()\nfig = px.bar(df)\nfig.show()","11dea1fd":"import plotly.express as px\ndf = dataset\nfig = px.histogram(df, x=dataset.Amount, color=dataset.Class)\nfig.show()","e6495cee":"import plotly.express as px\ndf = dataset\nfig = px.histogram(df, x=dataset.Time, color=dataset.Class)\nfig.show()","b8bbe368":"f,ax = plt.subplots(figsize=(25, 15))\nsns.heatmap(dataset.corr(), annot=True, linewidths=.3, fmt= '.2f',ax=ax, cmap='viridis')\nplt.savefig(\"corr.png\")\nplt.show()","0e883ce3":"fig, ax = plt.subplots(1, 2, figsize=(30,7))\n\namount = dataset['Amount'].values\ntime = dataset['Time'].values\n\nsns.distplot(amount, ax=ax[0], color='green')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount), max(amount)])\n\nsns.distplot(time, ax=ax[1], color='yellow')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time), max(time)])\n\n\nplt.savefig(\"dis\")\nplt.show()","6a03a2f3":"import plotly.express as px\ndf = dataset\nfig = px.histogram(df, x=dataset.Time, y=dataset.Amount, color=dataset.Class,\n                   marginal=\"box\", # or violin, rug\n                )\nfig.show()","2313d9b9":"dataset.hist(figsize=(20,20))\nplt.show()","6f0a3434":"import plotly.express as px\ndf = dataset\nfig = px.box(df, x=dataset.Class, y=dataset.Amount, points=\"all\")\nfig.show()","fee533c4":"import plotly.express as px\ndf = dataset\nfig = px.box(df, x=dataset.Class, y=dataset.Time, points=\"all\")\nfig.show()","597c697c":"Fraud = dataset[dataset['Class']==1]\nNormal = dataset[dataset['Class']==0]","8f97bfa6":"pd.concat([Normal.Amount.describe(), Normal.Time.describe()],  axis=1)","c06c92c8":"pd.concat([Fraud.Amount.describe(), Fraud.Time.describe()],  axis=1)","68bba013":"print('Fraud Shape:\\t', Fraud.shape)\nprint('Normal Shape:\\t', Normal.shape)","9992423e":"f, (axes1, axes2) = plt.subplots(1,2,sharey=True)\nf.set_figheight(15)\nf.set_figwidth(40)\nsns.heatmap(Fraud.corr(), annot=False, linewidths=.3, fmt= '.2f',ax=axes1, cmap='viridis')\naxes1.title.set_text('Fraud Correlation')\nsns.heatmap(Normal.corr(), annot=False, linewidths=.3, fmt= '.2f',ax=axes2, cmap='viridis')\naxes2.title.set_text('Normal Correlation')\nplt.savefig(\"corr_fr_nr.png\")\nplt.show()","3d478aad":"import plotly.express as px\ndf = Fraud\nfig = px.histogram(df,Fraud.Amount,\n                   title='Amount of Transaction (Fraud)',\n                   opacity=0.8,\n                   color_discrete_sequence=['red'] # color of histogram bars\n                   )\nfig.show()","1512bf42":"import plotly.express as px\ndf = Normal\nfig = px.histogram(df,Normal.Amount,\n                   title='Amount of Transactions (Normal)',\n                   opacity=0.8,\n                   color_discrete_sequence=['green'] # color of histogram bars\n                   )\nfig.show()","14d2daab":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=2, cols=1)\n\n\ntrace1 = go.Scatter(x=Fraud.Time, y=Fraud.Amount,\n                    mode='markers',\n                    name='Fraud',\n                    )\ntrace2 = go.Scatter(x=Normal.Time,y=Normal.Amount,\n                    mode='markers',\n                    name='Normal')\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 2, 1)\n\nfig.show()","4a0be5df":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndataset['amount_scale'] = rob_scaler.fit_transform(dataset['Amount'].values.reshape(-1,1))\ndataset['time_scale'] = rob_scaler.fit_transform(dataset['Time'].values.reshape(-1,1))\n\ndataset.drop(['Time','Amount'], axis=1, inplace=True)","09d10ff5":"amount_scale = dataset['amount_scale']\ntime_scale = dataset['time_scale']\n\ndataset.drop(['amount_scale', 'time_scale'], axis=1, inplace=True)\ndataset.insert(0, 'amount_scale', amount_scale)\ndataset.insert(1, 'time_scale', time_scale)\n\ndataset.head()","a89551f3":"dataset = dataset.sample(frac=1)\n\nfraud = dataset.loc[dataset['Class'] == 1]\nnormal = dataset.loc[dataset['Class'] == 0][:492]\n\nnormal_distributed_data = pd.concat([fraud, normal])\n\nsample_data = normal_distributed_data.sample(frac=1, random_state=42)\n\nsample_data.head()","76ec9b1a":"sample_data.shape","17485926":"f,ax = plt.subplots(figsize=(25, 15))\nsns.heatmap(sample_data.corr(), annot=True, linewidths=.3, fmt= '.2f',ax=ax, cmap='viridis')\nplt.savefig(\"corr_sample.png\")\nplt.show()","9d2d55a3":"f, axes = plt.subplots(nrows=2, ncols=5, figsize=(30,20))\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=sample_data, ax=axes[0][0])\naxes[0][0].set_title('V2 vs Class Pozitif Correlation')\nsns.boxplot(x=\"Class\", y=\"V4\", data=sample_data, ax=axes[0][1])\naxes[0][1].set_title('V4 vs Class Pozitif Correlation')\nsns.boxplot(x=\"Class\", y=\"V11\", data=sample_data, ax=axes[0][2])\naxes[0][2].set_title('V11 vs Class Pozitif Correlation')\nsns.boxplot(x=\"Class\", y=\"V3\", data=sample_data, ax=axes[0][3])\naxes[0][3].set_title('V3 vs Class Negatif Correlation')\nsns.boxplot(x=\"Class\", y=\"V9\", data=sample_data, ax=axes[0][4])\naxes[0][4].set_title('V9 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=sample_data, ax=axes[1][0])\naxes[1][0].set_title('V10 vs Class Negative Correlation')\nsns.boxplot(x=\"Class\", y=\"V12\", data=sample_data, ax=axes[1][1])\naxes[1][1].set_title('V12 vs Class Negatif Correlation')\nsns.boxplot(x=\"Class\", y=\"V14\", data=sample_data, ax=axes[1][2])\naxes[1][2].set_title('V14 vs Class Negative Correlation')\nsns.boxplot(x=\"Class\", y=\"V16\", data=sample_data, ax=axes[1][3])\naxes[1][3].set_title('V16 vs Class Negative Correlation')\nsns.boxplot(x=\"Class\", y=\"V17\", data=sample_data, ax=axes[1][4])\naxes[1][4].set_title('V17 vs Class Negative Correlation')\n\n\nplt.show()","d230a881":"X = sample_data.drop('Class', axis=1)\ny = sample_data['Class']","bb3d726f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=21)","e79c3352":"from sklearn.svm import SVC\nsvm_model = SVC()","1b966c12":"svm_params = {\"C\": np.arange(1,10), \"kernel\":[\"linear\", \"rbf\"]}","c7bf0274":"from sklearn.model_selection import GridSearchCV\nsvm_cv_model = GridSearchCV(svm_model, svm_params, cv=7, n_jobs=-1, verbose=7).fit(X_train, y_train)","be95fc41":"svm_cv_model.best_score_","e75d6e95":"best_params = svm_cv_model.best_params_\nprint(best_params)","fd68f35b":"svm = SVC(C = best_params['C'], kernel=best_params['kernel'], probability=True).fit(X_train, y_train)","ef69fe20":"y_pred_svm = svm.predict(X_test)","dc00504a":"accuracy_score(y_test, y_pred_svm)","b02af309":"from sklearn.model_selection import cross_val_score\ncross_val_score(svm, X_test, y_test, cv=21).mean()","79249782":"print(classification_report(y_test, y_pred_svm))","70d8513f":"from imblearn.metrics import classification_report_imbalanced, sensitivity_specificity_support\nprint('sensitivity and specificity:', sensitivity_specificity_support(y_test, y_pred_svm, average='micro', labels=pd.unique(dataset.Class)))\nprint(classification_report_imbalanced(y_test, y_pred_svm))","15a25342":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_svm)\nsns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)\nplt.title('SVC Confusion Matrix')\nplt.savefig('svc_con_mat')\nplt.show()","c5f0f97c":"from sklearn.metrics import roc_auc_score, roc_curve\nsvm_roc_auc = roc_auc_score(y_test, svm.predict(X_test))\nfpr , tpr, thresholds = roc_curve(y_test, svm.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='AUC (area = %0.2f)' % svm_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.legend(loc='lower right')\nplt.show()","8236fbee":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=21)","942682f2":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()","a32cffb8":"rf_params = {'n_estimators': [100,200,500],\n            'max_features': [3,5,7],\n            'min_samples_split':[5,10,20]}","43eb7781":"rf_cv_model = GridSearchCV(rf, rf_params, cv=7, n_jobs=-1, verbose=1).fit(X_train, y_train)","864ac829":"rf_cv_model","1e884b24":"best_params = rf_cv_model.best_params_\nprint(best_params)","612527ca":"rf = RandomForestClassifier(max_features=best_params['max_features'], min_samples_split=best_params['min_samples_split'], n_estimators=best_params['n_estimators']).fit(X_train, y_train)","b2fe7a64":"y_pred_rf = rf.predict(X_test)","bae9b7df":"accuracy_score(y_test, y_pred_rf)","71decd3d":"rf.feature_importances_","13429db6":"feature_imp = pd.Series(rf.feature_importances_,\n                       index=X_train.columns).sort_values(ascending=False)\nplt.figure(figsize=(15, 13))\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('De\u011fisken \u00d6nem Skorlar\u0131')\nplt.ylabel('De\u011fi\u015fkenler')\nplt.title('De\u011fi\u015fken \u00d6nem D\u00fczeyleri')\nplt.show()","1da8b4d9":"cross_val_score(rf, X_test, y_test, cv=21).mean()","097d7c3f":"print(classification_report(y_test, y_pred_rf))","30bf57f2":"from imblearn.metrics import classification_report_imbalanced, sensitivity_specificity_support\nprint('sensitivity and specificity:', sensitivity_specificity_support(y_test, y_pred_rf, average='micro', labels=pd.unique(dataset.Class)))\nprint(classification_report_imbalanced(y_test, y_pred_rf))","5d6519a6":"cm = confusion_matrix(y_test, y_pred_rf)\nsns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)\nplt.title('RF Confusion Matrix')\nplt.savefig('rf_con_mat')\nplt.show()","9f51960b":"rf_roc_auc = roc_auc_score(y_test, rf.predict(X_test))\nfpr , tpr, thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='AUC (area = %0.2f)' % rf_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.legend(loc='lower right')\nplt.show()","7d74c5b4":"from sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n\nx_orjinal_train , x_orjinal_test, y_orjinal_train, y_orjinal_test = train_test_split(X, y, test_size=0.33,random_state=21)\n\ny_train = y_orjinal_train\ny_test = y_orjinal_test\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x_orjinal_train)\nx_train = pd.DataFrame(x_scaled)\nx_scaled1 = min_max_scaler.fit_transform(x_orjinal_test)\nx_test = pd.DataFrame(x_scaled1)\n\n\nx_train = x_train.values\nx_test = x_test.values\n\nx_train = np.asarray(x_train)\nx_test = np.asarray(x_test)\n\nx_train_mean = np.mean(x_train)\nx_train_std = np.std(x_train)\n\nx_test_mean = np.mean(x_test)\nx_test_std = np.std(x_test)\n\nx_train = (x_train - x_train_mean)\/x_train_std\nx_test = (x_test - x_test_mean)\/x_test_std\n\nprint(x_train.shape)\n\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))","13042d97":"x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.33, random_state = 21)","7121f48b":"x_train.shape","34a67385":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard\n\ncnn = Sequential()\ncnn.add(Conv1D(32, 2, activation='relu', input_shape=(30,1)))\ncnn.add(Dropout(0.1))\n\ncnn.add(Conv1D(64, 2, activation='relu'))\ncnn.add(Dropout(0.2))\n\n\ncnn.add(Flatten())\ncnn.add(Dropout(0.4))\ncnn.add(Dense(64, activation='relu'))\ncnn.add(Dropout(0.5))\n\ncnn.add(Dense(1, activation='sigmoid'))\n\ncnn.summary()\n\ncnn.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n\nepochs = 7\nbatch_size = 10\nhistory = cnn.fit(x_train , y_train , verbose=1 , batch_size=batch_size , epochs=epochs ,validation_data=(x_test, y_test) )\n\n\n","235e65cb":"from keras.utils import plot_model\nplot_model(cnn)","f379a878":"loss, accuracy = cnn.evaluate(x_test, y_test, verbose=1)\nloss_v, accuracy_v = cnn.evaluate(x_validate, y_validate, verbose=1)\nprint(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\nprint(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))","8922417f":"fig, ax1 = plt.subplots(figsize= (10, 5) )\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"acc.png\")\nplt.show()","41f2f3c9":"fig, ax1 = plt.subplots(figsize= (10, 5) )\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"loss.png\")\nplt.show()","6c64b5ae":"from sklearn.metrics import confusion_matrix\ny_pred_cnn = cnn.predict_classes(x_validate)\ncm = confusion_matrix(y_validate, y_pred_cnn)\nsns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)\nplt.title('CNN Confusion Matrix')\nplt.savefig('cnn_con_mat')\nplt.show()","599000f8":"accuracy_score(y_validate, y_pred_cnn)","9e530fea":"print(classification_report(y_validate, y_pred_cnn))","4fc8a1d4":"from imblearn.metrics import classification_report_imbalanced, sensitivity_specificity_support\nprint('sensitivity and specificity:', sensitivity_specificity_support(y_validate, y_pred_cnn, average='micro', labels=pd.unique(dataset.Class)))\nprint(classification_report_imbalanced(y_validate, y_pred_cnn))","d88abbd8":"dataset = dataset.sample(frac=0.5)\n\nfraud = dataset.loc[dataset['Class'] == 1]\nnormal = dataset.loc[dataset['Class'] == 0][:492]\n\nnormal_distributed_data = pd.concat([fraud, normal])\n\nsample_data = normal_distributed_data.sample(frac=0.5, random_state=42)\n\nsample_data.head()","937b4969":"X = sample_data.drop('Class', axis=1)\ny = sample_data['Class']","c574cded":"min_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(X)\nx = pd.DataFrame(x_scaled)\nx = x.values\nx = np.asarray(x)\nx_mean = np.mean(x)\nx_std = np.std(x)\nx = (x - x_mean)\/x_std\nprint(x.shape)\n\nx = np.reshape(x, (x.shape[0], x.shape[1], 1))\n","bf8da6d6":"from sklearn.metrics import confusion_matrix\n\ny_pred_svm = svm.predict(X)\ny_pred_rf = rf.predict(X)\ny_pred_cnn = cnn.predict_classes(x)\n\nf, (axes1, axes2, axes3) = plt.subplots(1,3,sharey=True)\nf.set_figheight(7)\nf.set_figwidth(35)\nsns.heatmap(confusion_matrix(y, y_pred_svm), annot=True, linewidths=.3, fmt= 'd',ax=axes1, cmap='viridis', cbar=False)\naxes1.title.set_text('SVC Confusion Matrix')\nsns.heatmap(confusion_matrix(y, y_pred_rf), annot=True, linewidths=.3, fmt= 'd',ax=axes2, cmap='viridis', cbar=False)\naxes2.title.set_text('Random Forest Confusion Matrix')\nsns.heatmap(confusion_matrix(y, y_pred_cnn), annot=True, linewidths=.3, fmt= 'd',ax=axes3, cmap='viridis', cbar=False)\naxes3.title.set_text('CNN Confusion Matrix')\nplt.savefig(\"cm_all.png\")\nplt.show()","b7db33a1":"models = [svm, rf, cnn]\nresult = []\nresults = pd.DataFrame(columns=['Models', \"Accuracy\"])\n\nfor model in models:\n    names = model.__class__.__name__\n    print(names)\n    if names == 'Sequential':\n        y_pred = model.predict_classes(x)\n    else:\n        y_pred = model.predict(X)\n    acc = accuracy_score(y, y_pred)\n    result = pd.DataFrame([[names, acc*100]], columns=['Models', 'Accuracy'])\n    results = results.append(result)","57609e13":"plt.figure(figsize=(15,5))\nsns.barplot(x='Accuracy', y='Models', data=results, color='purple')\nplt.xlabel('Accuracy %')\nplt.title('Modellerin Do\u011fruluk Oranlar\u0131');","87620ca7":"results","82f5b05b":"# Models","320e2adf":"# SVM","91a7a511":"# Test and Conclusion","68d46687":"# Data Preprocessing and Data Visualization","fc781341":"___","302336bb":"## Normal and Fraud Dataset","c951b817":"# CNN","6c234721":"### Sample Selection","4322fce7":"# Random Forest"}}