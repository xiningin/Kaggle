{"cell_type":{"db3f88c2":"code","66fe8536":"code","5c66f3ba":"code","27e813ff":"code","ab86ee78":"code","f9fac66c":"code","1261b861":"code","16febe1e":"code","ddc80283":"code","985853c8":"code","77d95738":"code","e7472c54":"code","fd49a5c4":"code","f4ab985c":"code","7fe16ccf":"code","b132053d":"code","f08caf99":"code","cfd9bd62":"code","9e2aa8e7":"code","4ec16c44":"code","b1ded0f1":"code","58a36fda":"code","b311df8d":"code","c23e79e1":"code","7eaa3382":"code","28eb6ae8":"code","88ccb52c":"code","cd663603":"code","e2b44962":"code","26e68fd3":"markdown","7b178d49":"markdown","452495d6":"markdown","dd4d31c2":"markdown","7fc92561":"markdown","4770a09b":"markdown","c3567ff4":"markdown","fbd4d8b7":"markdown","2f005cb5":"markdown","7f8b8f86":"markdown","984100a8":"markdown","fb0d5a54":"markdown","57714724":"markdown","0bc77b6e":"markdown","5b666864":"markdown","50d98ba7":"markdown","da7bc182":"markdown","23ca03f8":"markdown","121cb78d":"markdown","537f8a1c":"markdown","1f7293db":"markdown","a14d0715":"markdown","b142af29":"markdown","5d0e9f1e":"markdown","d63f341a":"markdown","4e8df6dc":"markdown","bf8b6ce1":"markdown","f527bc69":"markdown","818de558":"markdown"},"source":{"db3f88c2":"from IPython.display import Image\ndisplay(Image('..\/input\/machine-learning-everywhere-memes\/machine-learning-everywhere.jpg', width=500, unconfined=True))","66fe8536":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5c66f3ba":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","27e813ff":"train.Survived.value_counts()","ab86ee78":"train.head()","f9fac66c":"test.head()","1261b861":"train.info()","16febe1e":"train.isna().sum()","ddc80283":"train.loc[train.Age.isna(), 'Age'] = train[~train.Age.isna()].Age.mean()","985853c8":"train.loc[train.Cabin.isna(),'Cabin'] = \"No Cabin\"","77d95738":"print(train.Embarked.value_counts())\ntrain.loc[train.Embarked.isna(),'Embarked'] = \"S\"","e7472c54":"train.isna().sum()","fd49a5c4":"fig,axes = plt.subplots(1, 2,figsize=(25,8))\nprint(axes)\n\nsns.boxplot(x='Pclass',y='Age',data=train, palette='viridis',ax=axes[0])\n\n# We now need to check for outliers (values that seem irregular compared to the others)\nsns.boxplot(x='Pclass',y='Fare',data=train, palette='viridis',ax=axes[1])\n\nplt.show()","f4ab985c":"train.loc[train.Fare > 200]","7fe16ccf":"\nnumerical_column = ['int64','float64'] #select only numerical features to find correlation\nplt.figure(figsize=(10,10))\nsns.heatmap(\n    train.select_dtypes(include=numerical_column).corr(),\n    cmap=plt.cm.RdBu,\n    vmax=1.0,\n    linewidths=0.1,\n    linecolor='white',\n    square=True,\n    annot=True\n)","b132053d":"plt.figure(figsize=(10,10))\nsns.pairplot(train.select_dtypes(include=numerical_column), hue = 'Survived')","f08caf99":"train.Sex.value_counts()","cfd9bd62":"# thanks to the open-source world, we do not need to waste that much time here ! LabelEncore from sckit-learn allow us\n# to convert this text categorical data into numbers ! :O\n\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ntrain['Sex'] = labelencoder.fit_transform(train['Sex'])\ntrain.Sex.value_counts()","9e2aa8e7":"#Let's now print the Age distrubtion regarding who survived\n\npalette ={1:\"g\", 0:\"r\"}\nsns.countplot(x='Sex',data=train,hue=\"Survived\", palette=palette)","4ec16c44":"def features_engineering(df):\n    df.loc[df.Age.isna(), 'Age'] = df[~df.Age.isna()].Age.mean()\n    df.loc[df.Cabin.isna(),'Cabin'] = \"No Cabin\"\n    df.loc[df.Embarked.isna(),'Embarked'] = \"S\"\n    df['persons_abroad_size'] = (df['Parch']+df['SibSp']).astype(int)\n    df['alone'] = np.where(df['Parch']==0,1,0)\n    df['Embarked'] = df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    df['Sex'] = df['Sex'].map( {'male': 1, 'female': 2} ).astype(int)\n    df['log_fare'] = df['Fare'].apply(np.log)\n    df['Room'] = (df['Cabin']\n                    .str.slice(1,5).str.extract('([0-9]+)', expand=False)\n                    .fillna(0)\n                    .astype(int))\n    df['RoomBand'] = 0\n    df.loc[(df.Room > 0) & (df.Room <= 20), 'RoomBand'] = 1\n    df.loc[(df.Room > 20) & (df.Room <= 40), 'RoomBand'] = 2\n    df.loc[(df.Room > 40) & (df.Room <= 80), 'RoomBand'] = 3\n    df.loc[df.Room > 80, 'RoomBand'] = 4\n    df_id = df.PassengerId\n    df = df.drop('PassengerId', axis=1)\n    return df,df_id","b1ded0f1":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain,train_id = features_engineering(train)\ntest,test_id = features_engineering(test)","58a36fda":"train.info()","b311df8d":"import xgboost as xgb\nfrom sklearn import model_selection\nX_train = train.drop('Survived',axis=1).select_dtypes(include=['int32','int64','float64'])\ny_train = train['Survived']\nX_test = test.select_dtypes(include=['int32','int64','float64'])\n\nxg_boost = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.65, gamma=2, learning_rate=0.3, max_delta_step=1,\n       max_depth=4, min_child_weight=2, missing=None, n_estimators=280,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)","c23e79e1":"xg_boost.fit(X_train, y_train)","7eaa3382":"print(xg_boost.score(X_train, y_train))\n\nscores = model_selection.cross_val_score(xg_boost, X_train, y_train, cv=5, scoring='accuracy')\nprint(scores)\nprint(\"Kfold on XGBClassifier: %0.4f (+\/- %0.4f)\" % (scores.mean(), scores.std()))","28eb6ae8":"xgb.plot_importance(xg_boost)\nplt.show()","88ccb52c":"Y_pred = xg_boost.predict(X_test)","cd663603":"submission = pd.DataFrame({\n    \"PassengerId\": test_id, \n    \"Survived\": Y_pred \n})\nsubmission.head(10)","e2b44962":"submission.to_csv('submission.csv', index=False)","26e68fd3":"## Do you understand what we have to do ?\nHere we have to determine if a passenger will survive or not the Titanic tragedy. As you can notice below, the column Survived can only be two numbers : 1 = Survived , 0 = Died. So it seems to be a classification problem. We will have to keep in mind this information to adapt our exploration and modeling strategy.\nIn this case, it is a supervised problem, as we have a column of the label that we want to predict.","7b178d49":"## Pairplot \/ scatter matrix\nAnother comomon thing to do in a Classification is to plot a pair plot, which is a figure that allows you to see the distribution of each data compared to others, in different colors regarding your label column (here Survived). You can already notice the different distribution of Pclass.\n\nIt seems that the cheapeast class has a lowest chance of survival... It seems also that people with no parent\/child aboard the titanic (regarding Parch) has the highest change to survive... hum... let's keep this in mind this for later.","452495d6":"Train your model with your trained dataset :","dd4d31c2":"82% of accuracy during a cross validation is a correct score for the first shot in a binary classification. Try to improve this ! :) \nNow let's predict our testing value.","7fc92561":"## Few other checks\nWe can notice, regarding the Age distrubition that children have been (most of the time) saved !\n'Save women and children in first' seems at last to really be true... but wait ! We don't have Sex ditrbution here\nas it's an object column, and pairplot do not support \"object\" columns. Let's change that.","4770a09b":"Here we can do an evaluation of our model :","c3567ff4":"## STEP 3) Model and save your prediction\nHere we will construct our model. We will use all numerical columns and the XGboost algorithm. XGB is one of the most famous algorithm you may see one the Kaggle platform by its capacity to automatize a large number of process.","fbd4d8b7":"The Embarked feature has only two values empty, it may not really be smart to fill those value with \"empty\" values that will create a new class and make your future model more complex. \n\nLet's use the more representative port here, which is S (Southampton).","2f005cb5":"## Fill empty values\nNULL values ? Empty values?\nOne of the first thing to do is to check for empty cells, that can be a common source of error when your run an algorithm !","7f8b8f86":"You can notice that the column \"Survived' is not present in the test dataframe. It's because it is our \"label\"\/\"target\" column that we will need to predict. :-)","984100a8":"Have a preview of the training dataframe. The training dataframe is made to be able to learn from it, as a development environement.","fb0d5a54":"Let's check if all nan value have been filled. Indeed, everything seems okay now :","57714724":"Your file with the prediction with the testing dataframe is ready to be saved !  Congratulations you just made your first machine learning project !","0bc77b6e":"#### Fill Age\n\nFill the Age with the mean value to make it not empty. We may do a more sophisticated filling, as for example taking the mean for each Pclass and fill the value based on this feature... you can try to do it !","5b666864":"It seems that only a few tickets have been really expensive compared to the others ! Let's take a closer look at those.","50d98ba7":"As we can notice : some have several Cabins, but some not, and they get only one but really expensive Cabin.\n\nLine 258 : Ward Anna seems to have paid 512$ ! As we know in the Titanic some Cabin can be really expensive compared to others - perhaps it was the best Cabin ever made !\n\nLine 679 & Line 737 : We can also notice that Mr Thomas Drake Martinez and Mr Gustance had the same Fare, with the same number Ticket, perhaps they are traveling together ?\n\nYou can try to change those outliers Fares, and manage to do some nice modifications ! The idea is to have an intuition about the veracity of the values, and implement it.","da7bc182":"Indeed, our assumption was correct. We can notice that indeed women have a better chance of surviving during this tragedy.","23ca03f8":"## STEP 2) Features Engineering\nThis step is made to be able to get a maximum of information from the data, by, for example, creating new columns or changing something in a column as we just already did with the \"Sex\" column.\nI recommend to create a features_engineering function to have a common modification between your \"training\" and \"testing\", through just one function :-).\n\nI will be really simple here to give you the opportunity to create and add some new features and test it in your model. Do not forget to add here all features modification you might already have done during the previous step.\n### !! Be sure to never include your target\/label column in a features engineering process or an error will occur during the feature engineering process of your testing dataframe !!","121cb78d":"Fortunately we made this check ! We got 177 empty values for Age, 687 for Cabin and 2 for Embarked ! You will notice along your data science journey that every dataset has to be cleaned before use ;) let's deal with those issues.","537f8a1c":"### Import all libraries required\n* pandas is used for data manipulation (including reading data)\n* numpy is used for mathematacial functions\n* matplotlib and seaborn are for Data Visualisation","1f7293db":"## Correlation matrix\nA common way to find relationships between variables is to plot a correlation matrix. \nHere you can already see that the Survived feature is higly dependent on the Ticket Class (Pclass), interesting, isn't it ?","a14d0715":"Knowing the type of your data is important. Some algorithms heavily depend on the type and you may obtain an Error if you use an unadapted type.\nHere you can notice that PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare are numerical columns. And Name, Sex, Ticket,Cabin, Embarked are object columns (in this case, that means strings).","b142af29":"# Your first ML Project\nThis notebook has been made to help you to go trough, in a really simple way, a machine learning project. In a few minutes you will be able to run your first ML algorithm :-) .","5d0e9f1e":"## STEP 1) Data Exploration\nThe first step of any ML project is to understand your data. Let's do it ! :-)","d63f341a":"### Trying to find why a person would survive, or not\nDo you remember that it is a classification problem ? We will try to find what is different between the two categories of person using the training dataframe.","4e8df6dc":"Have the preview of the testing dataframe. The testing dataframe is the dataframe that we will have to use to make our prediction in the production environement.","bf8b6ce1":"#### Fill Cabin\n\nCabin being empty... Perhaps they do not have a Cabin number ? Let make this assumption !\n","f527bc69":"## Outliers\nWe now need to check for outliers. Outliers are values that are very different compared to what we could expect from all other values.\nHere we will try to verify some assumptions. The first one we could make is that older people have more money (and thus a better Class).\n\nWe will use a boxplot. What is a boxplot ? Check this link : https:\/\/towardsdatascience.com\/understanding-boxplots-5e2df7bcbd51","818de558":"### Read data\nPandas allow us to read data directly from the file. Here we have a csv file :"}}