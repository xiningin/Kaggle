{"cell_type":{"d4812b2f":"code","529b11e2":"code","3f147deb":"code","2ad3c4ed":"code","2e44438f":"code","d516eae8":"code","2498d522":"code","ed3ec543":"code","a3a66200":"code","98e14fa5":"code","242ea4e1":"code","d6eb9819":"code","999beb09":"code","998bdc9d":"code","50f6f98c":"code","5ab5e2b0":"code","63a94051":"code","e43f971f":"code","d5c76dda":"code","92449509":"code","c931f3cf":"code","ac7054a0":"code","3d0b5574":"code","5209f7f7":"code","a57e0c72":"code","d56b16af":"code","c093034e":"code","5cf8b377":"code","0a87d91a":"code","9572182d":"code","224bafb6":"code","0ad69ac4":"code","e58cd5e0":"code","7c9f1659":"code","568698a9":"code","f0117a80":"code","87054955":"code","67364891":"code","ca2c5182":"code","39580c4b":"code","3eb78147":"code","7279d268":"code","2698346f":"code","cdf34b2b":"code","54243b27":"code","c18aa399":"code","f4a0f788":"code","415f8fb5":"code","20028874":"code","8884b54a":"code","d54d8366":"code","97a8914a":"code","aff71e1b":"markdown","1df6ec1c":"markdown","434a294b":"markdown","c8ca07be":"markdown","d6ae40be":"markdown","57e94eaa":"markdown","51150c61":"markdown","11c25192":"markdown","0cf9a708":"markdown","efaf46dc":"markdown","2ec1258d":"markdown","ef7326e2":"markdown"},"source":{"d4812b2f":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport pyarrow.parquet as pq\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import matthews_corrcoef\nfrom scipy import signal\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nfrom numba import jit, int32","529b11e2":"INIT_DIR = '..\/input'\nSIZE = 1024","3f147deb":"os.listdir(INIT_DIR)","2ad3c4ed":"meta = pd.read_csv(os.path.join(INIT_DIR, 'metadata_train.csv'))","2e44438f":"train = pq.read_pandas(os.path.join(INIT_DIR, 'train.parquet'), columns=[str(i) for i in range(1000)]).to_pandas()","d516eae8":"meta.describe()","2498d522":"meta.corr()","ed3ec543":"positive_mid = np.unique(meta.loc[meta.target == 1, 'id_measurement'].values)\nnegative_mid = np.unique(meta.loc[meta.target == 0, 'id_measurement'].values)","a3a66200":"pid = meta.loc[meta.id_measurement == positive_mid[0], 'signal_id']\nnid = meta.loc[meta.id_measurement == negative_mid[0], 'signal_id']","98e14fa5":"positive_sample = train.iloc[:, pid]\nnegative_sample = train.iloc[:, nid]","242ea4e1":"@jit('float32(float32[:,:], int32, int32)')\ndef flatiron(x, alpha=50., beta=1):\n    new_x = np.zeros_like(x)\n    zero = x[0]\n    for i in range(1, len(x)):\n        zero = zero*(alpha-beta)\/alpha + beta*x[i]\/alpha\n        new_x[i] =  x[i] - zero\n    return new_x","d6eb9819":"plt.figure(figsize=(24, 8))\nplt.plot(positive_sample, alpha=0.8);","999beb09":"x_filt = flatiron(positive_sample.values)\nplt.figure(figsize=(24, 8))\nplt.plot(x_filt, alpha=0.5);","998bdc9d":"plt.figure(figsize=(24, 8))\nplt.plot(negative_sample, alpha=0.7);","50f6f98c":"x_filt = flatiron(negative_sample.values)\nplt.figure(figsize=(24, 8))\nplt.plot(x_filt, alpha=0.5);","5ab5e2b0":"@jit('float32(float32[:,:], int32, int32)')\ndef feature_extractor(x, n_part=1000, n_dim=3):\n    lenght = len(x)\n    pool = np.int32(np.ceil(lenght\/n_part))\n    output = np.zeros((n_part, n_dim))\n    for j, i in enumerate(range(0,lenght, pool)):\n        if i+pool < lenght:\n            k = x[i:i+pool]\n        else:\n            k = x[i:]\n        output[j] = np.max(k, axis=0) - np.min(k, axis=0)\n    return output\n\n@jit('float32(float32[:,:])')\ndef basic_feature_extractor(x):\n    return [np.max(x, axis=0), np.min(x, axis=0), np.std(x, axis=0), np.sum(x, axis=0)\/len(x)]","63a94051":"x_train = []\nbasic = []\ny_basic = []\ny_train = []\nmid = np.unique(meta.id_measurement.values)\nfor b in tqdm(range(4)):\n    start = b*len(meta)\/\/12\n    if len(meta)\/\/3 - start < len(meta)\/\/12:\n        end = -1\n    else:\n        end = start + len(meta)\/\/12\n    \n    columns = []\n    for i in mid[start:end]:\n        columns.extend(meta.loc[meta.id_measurement==i, 'signal_id'].values.tolist())\n    train = pq.read_pandas(os.path.join(INIT_DIR, 'train.parquet'), columns=[str(i) for i in columns]).to_pandas()\n    \n    for i in range(len(train.columns)):\n        train.iloc[:, i] = flatiron(train.iloc[:, i].values)\n        \n    for i in mid[start:end]:\n            idx = meta.loc[meta.id_measurement==i, 'signal_id'].values\n            x_train.append(abs(feature_extractor(train.loc[:, [str(kj) for kj in idx]].values, n_part=SIZE)))\n            basic.extend([basic_feature_extractor(train.loc[:, str(kj)].values) for kj in idx])\n            y_basic.extend([meta.loc[meta.signal_id==kj, 'target'].values for kj in idx])\n            y_train.append(meta.loc[meta.id_measurement==i, 'target'].values)","e43f971f":"del train;gc.collect()","d5c76dda":"np.shape(basic)","92449509":"x_base = np.array(basic)","c931f3cf":"from sklearn.decomposition import PCA","ac7054a0":"pca = PCA(n_components=2)\nx_pca = pca.fit_transform(x_base)","3d0b5574":"y_basic = np.array(y_basic)","5209f7f7":"x_pos, x_neg = [], []\nfor _x, _y in zip(x_pca, y_basic):\n    if _y == 1:\n        x_pos.append(_x)\n    else:\n        x_neg.append(_x)","a57e0c72":"x_pos, x_neg = np.array(x_pos), np.array(x_neg)","d56b16af":"x_pos.shape","c093034e":"plt.figure(figsize=(10, 10))\nplt.scatter(x_neg[:, 0], x_neg[:, 1])\nplt.scatter(x_pos[:, 0], x_pos[:, 1])","5cf8b377":"np.unique(y_train)","0a87d91a":"x_train = np.array(x_train)\ny_train = np.array(y_train)","9572182d":"print(np.shape(x_train), np.shape(y_train))","224bafb6":"csum = np.sum(y_train, axis=-1)\nnp.unique(csum)","0ad69ac4":"pos_index = np.where(csum>0)[0]\nneg_index = np.where(csum==0)[0]","e58cd5e0":"figure, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(24,8))\nsns.heatmap(x_train[pos_index[0], :, :], ax=ax1)\nsns.heatmap(x_train[pos_index[10], :, :], ax=ax2)\nsns.heatmap(x_train[pos_index[20], :, :], ax=ax3)\nax1.set_axis_off()\nax2.set_axis_off()\nax3.set_axis_off();","7c9f1659":"figure, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(24,8))\nsns.heatmap(x_train[neg_index[0], :, :], ax=ax1)\nsns.heatmap(x_train[neg_index[10], :, :], ax=ax2)\nsns.heatmap(x_train[neg_index[20], :, :], ax=ax3)\nax1.set_axis_off()\nax2.set_axis_off()\nax3.set_axis_off();","568698a9":"from keras.layers import *\nfrom keras import Model\nfrom keras.optimizers import Nadam\nfrom keras.utils import Sequence\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.constraints import max_norm\nimport keras.backend as K","f0117a80":"class DataGenerator(Sequence):\n    def __init__(self, x, y, batch_size=64):\n        self._x = x\n        self._y = y\n        self._batch_size = batch_size\n        \n    def __getitem__(self, index):\n        index = np.random.choice([i for i in range(len(self._y))], size=(self._batch_size))\n        \n        x_batch, y_batch, w = [], [], []\n        for _x, _y in zip(self._x[index], self._y[index]):\n            _x = self.cyclic_shift(_x)\n            #_x, _y = self.phase_permutation(_x, _y)\n            x_batch.append(_x)\n            y_batch.append(_y)\n            #w.append(np.sum(_y)*100+1)\n        return np.array(x_batch), np.array(y_batch)#, np.array(w)\n    \n    def __len__(self):\n        return len(self._y)\/\/self._batch_size\n    \n    @staticmethod\n    def cyclic_shift(x, alpha=0.5):\n        s = np.random.uniform(0, alpha)\n        part = int(len(x)*s)\n        x_ = x[:part, :]\n        _x = x[-len(x)+part:, :]\n        return np.concatenate([_x, x_], axis=0)\n    \n    @staticmethod\n    def phase_permutation(x, y):\n        phase = np.random.permutation([0,1,2])\n        out_x, out_y = [], []\n        for indx in phase:\n            out_x.append(x[..., indx])\n            out_y.append(y[indx])\n        return np.stack(out_x, axis=-1), np.array(out_y)","87054955":"def matthews_corr_coeff(y_true, y_pred):\n    y_pos_pred = K.round(K.clip(y_pred, 0, 1))\n    y_pos_true = K.round(K.clip(y_true, 0, 1))\n    \n    y_neg_pred = 1 - y_pos_pred\n    y_neg_true = 1 - y_pos_true\n\n    tp = K.sum(y_pos_true * y_pos_pred)\n    tn = K.sum(y_neg_true * y_neg_pred)\n    fp = K.sum(y_neg_true * y_pos_pred)\n    fn = K.sum(y_pos_true * y_neg_pred)\n    return (tp * tn - fp * fn) \/ (K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) + K.epsilon())","67364891":"def get_model(inp_shape=(SIZE, 3)):\n    inp = Input(inp_shape)\n    # 256\n    x = Conv1D(32, kernel_size=3, dilation_rate=3,use_bias=False,kernel_constraint=max_norm(2.))(inp)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 250\n    x = Conv1D(32, kernel_size=3, dilation_rate=2,use_bias=False,kernel_constraint=max_norm(2.))(x)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 245\n    x = MaxPooling1D(pool_size=3, strides=2)(x)\n    \n    #  122\n    x = Conv1D(64, kernel_size=3, dilation_rate=3,use_bias=False,kernel_constraint=max_norm(2.))(x)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 116\n    x = Conv1D(64, kernel_size=3, dilation_rate=2,use_bias=False,kernel_constraint=max_norm(2.))(x)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 112\n    x = Conv1D(64, kernel_size=3, dilation_rate=1,use_bias=False,kernel_constraint=max_norm(2.))(x)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 110\n    x = MaxPooling1D(pool_size=3, strides=2)(x)\n    \n    # 54\n    x = Conv1D(128, kernel_size=3, dilation_rate=3,use_bias=False,kernel_constraint=max_norm(2.))(x)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 48\n    x = Conv1D(128, kernel_size=3, dilation_rate=2,use_bias=False,kernel_constraint=max_norm(2.))(x)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 44\n    x = Conv1D(128, kernel_size=3, dilation_rate=1,use_bias=False,kernel_constraint=max_norm(2.))(x)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 42\n    x = MaxPooling1D(pool_size=3, strides=2)(x)\n    \n    #  20\n    x = Conv1D(256, kernel_size=3, dilation_rate=3,use_bias=False,kernel_constraint=max_norm(2.))(x)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 15\n    x = Conv1D(256, kernel_size=3, dilation_rate=2,use_bias=False,kernel_constraint=max_norm(2.))(x)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 10\n    x = Conv1D(256, kernel_size=3, dilation_rate=1,use_bias=False,kernel_constraint=max_norm(2.))(x)\n    x = BatchNormalization()(x)\n    x = Activation('selu')(x)\n    # 8\n    x = GlobalMaxPooling1D()(x)\n    \n    x = Dropout(0.75)(x)\n    \n    max_out = []\n    for _ in range(5):\n        max_out.append(Dense(128,use_bias=False,kernel_constraint=max_norm(2.))(x))\n    x = Maximum()(max_out)\n    x = BatchNormalization()(x)\n    \n    out = Dense(3, activation='sigmoid',kernel_constraint=max_norm(2.))(x)\n    return Model(inp, out)","ca2c5182":"mcp = ModelCheckpoint('model.h5',monitor='val_matthews_corr_coeff', mode='max')","39580c4b":"from sklearn.model_selection import train_test_split, KFold, StratifiedKFold","3eb78147":"x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, shuffle=True, train_size=0.75, random_state=28, stratify=csum)","7279d268":"tr_gen = DataGenerator(x_tr, y_tr)\nvl_gen = DataGenerator(x_val, y_val)","2698346f":"model = get_model()\nprint(model.summary())\nmodel.compile(optimizer=Nadam(4*1e-3, schedule_decay=1e-7),loss='binary_crossentropy', metrics=['accuracy', matthews_corr_coeff])","cdf34b2b":"model.fit_generator(tr_gen, steps_per_epoch=1000, epochs=10, callbacks=[mcp], validation_data=vl_gen, validation_steps=400)\nmodel.load_weights('model.h5')","54243b27":"best_thr = 0.01\nbest_metric = 0\ny_val = y_val.flatten()\nproba = model.predict(x_val)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\nfor i in tqdm(np.linspace(0.01, 0.9999, 30)):\n    for j in np.linspace(0.01, 0.9999, 30):\n        for k in np.linspace(0.01, 0.9999, 30):\n            y_pred = np.int32(np.stack([proba[:, 0] > i, proba[:, 1] > j, proba[:, 2] > k], axis=-1)).flatten()\n            m = matthews_corrcoef(y_val, y_pred)\n            if m > best_metric:\n                best_thr = (i, j, k)\n                best_metric= m\nprint('Best threshold: ',best_thr, ' ; Best metric: ',best_metric)","c18aa399":"meta = pd.read_csv(os.path.join(INIT_DIR, 'metadata_test.csv'))\nsubmission = pd.read_csv(os.path.join(INIT_DIR, 'sample_submission.csv'))","f4a0f788":"meta.corr()","415f8fb5":"len(meta.id_measurement.unique())*3","20028874":"len(meta.signal_id)","8884b54a":"for b in tqdm(range(0, len(meta), 3000)):\n    idx = []\n    if b+3000 < len(meta):\n        idx = meta.signal_id[b:b+3000].values\n    else:\n        idx = meta.signal_id[b:].values\n    subset_test = pq.read_pandas(os.path.join(INIT_DIR, 'test.parquet'), columns=[str(j) for j in idx]).to_pandas()\n    x_batch = []\n    for i in range(0, len(idx)\/\/3):\n        _x  = []\n        for j in range(0, 3):\n            _x.append(flatiron(subset_test.iloc[:, i*3+j].values))\n        _x = np.concatenate(_x, axis=-1)\n        x_batch.append(feature_extractor(_x, n_part=SIZE))\n    y_batch = model.predict(np.array(x_batch), verbose=0)\n    pred = []\n    for yj in y_batch:\n        for j, yi in enumerate(yj):\n            pred.append(np.int32(yi > best_thr[j]))\n    for jdx, iy in zip(idx, pred):\n        submission.loc[submission.signal_id == jdx, 'target'] = iy","d54d8366":"submission.to_csv('submission.csv', index=False)","97a8914a":"submission.head()","aff71e1b":"In cell below we can see, that for one measurement various number channels can be fault.","1df6ec1c":"Data contain 3 phase signal for each mesuarment. From table above we can see that target independant from phase and id_mesurment.","434a294b":"https:\/\/www.kaggle.com\/ashishpatel26\/transfer-learning-in-basic-nn","c8ca07be":"At the plots below we can see features(amplitude), which extracted from signal, for positive and negative case.","d6ae40be":"<center> **Predict**","57e94eaa":"Signal with phase, for my mind, will not be useful for CNN or RNN model.  For this case I'm apply filter like HPF for signal flatten. And thus I can more easier extract specific noise and anomaly feature.  ","51150c61":"For time series very useful use dilation and  for CNN useful apply in architecture residual connection.","11c25192":"<center>  **Preprocessing**","0cf9a708":"Unsolved problems:\n* apply stratifiedkfold","efaf46dc":"It's kernel investigate problems of the use CNN for line fault detection.","2ec1258d":"<center> **CNN**","ef7326e2":"!!! Numba is very useful tool for situation like this !!!"}}