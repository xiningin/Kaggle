{"cell_type":{"8239e76b":"code","24c5fdbc":"code","74264b1c":"code","21fa22a6":"code","2b1964f5":"code","c93c60de":"code","d1efdcad":"code","ac0f27b2":"code","dd7d1012":"code","947b203d":"code","d4cb4592":"code","d1bf5c6b":"markdown","1625e408":"markdown","5bed323c":"markdown","71a3d0c2":"markdown","c396dda5":"markdown","4769a576":"markdown","f672b404":"markdown","5ab5383f":"markdown","e00ff7d8":"markdown","db91e594":"markdown","1cf7875f":"markdown","f3313d57":"markdown"},"source":{"8239e76b":"# Imports\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n","24c5fdbc":"imgs_path = \"..\/input\/dont-stop-until-you-drop\/images\/train_images\/\" # this path contains images,\n\ntest_path = \"..\/input\/dont-stop-until-you-drop\/images\/test_images\/\" #contains paths of images for testing\n\npaths_labels = pd.read_csv(\"..\/input\/dont-stop-until-you-drop\/train.csv\") #contain the train image paths and classes\n\nimgs_data_list = np.asarray(os.listdir(imgs_path)) #converting it to an array to be able to access the paths\n\ntest_data_list = np.asarray(os.listdir(test_path)) # the test images as list, we should use this to predict classes and submit our result\n\n# os.listdir() method in python is used to get the list of all files and directories in the specified directory\n# numpy.asarray(a, dtype=None, order=None, *, like=None) Convert the input to an array\n# a: array_like\n#Input data, in any form that can be converted to an array.\n#This includes lists, lists of tuples, tuples, tuples of tuples, tuples of lists and ndarrays\n\n# here we just want to see the size, some sample data and images\n\nprint('----csv file shape----')\nprint(paths_labels.shape)\nprint('----csv file head----')\nprint(paths_labels.head())\n\nprint('----images len----')\nprint(len(imgs_data_list))\n\nprint('----images sample----')\nprint(imgs_data_list[0:5])\n\nprint('----test len----')\nprint(len(test_data_list))\n\nprint('----test sample----')\nprint(test_data_list[0:5])\n\n\nprint('-------------------------------')\n\nimport cv2\n# cv2 is a libary that enables us to show an image here\n# Image_Path_ToS_how = cv2.imread(path of image,setting)\n# by using this libary plt can show the image by its path\n\n# use labe.iloc[row][column] to catch a sample image in a orbitary row and column 0 (since image path is in labe's column zero)\n\n# to find each pose first image location in label array,\n# use np.where to find which indexes of column \"class_6\" in label array\n# np.where(condition)[item index where condition is true][item index where condition is false]\n\n# to see images of the poses for each class of i == 1,2,3,4,5,6\n# use a for loop to go through each 6 positions\n\nclass_names = list(paths_labels['class_6'].unique())\n\nfor i in class_names:\n    pose = cv2.imread(imgs_path + paths_labels.iloc[np.where(paths_labels[\"class_6\"]==i)[0][0]][0], cv2.IMREAD_COLOR )\n    print('------ pose: ' + str(i) + '----------')\n    plt.imshow(pose)\n    plt.show()\n    ","74264b1c":"from sklearn.model_selection import train_test_split\n\n# train_test_split get our data and will return two data set,\n#by divding main set randomly into 1-test_size and test_size percentage\n\n# if we want to run our model in the faster pace to checking it is working,\n#then we can first get a small sample and then splliting it.\n# to doing so, uncomment the following line and replace paths_labels with sample_data\n# in the train_test_split\n\n#sample_data = paths_labels.sample(frac=0.25, replace=True, random_state=1)\n\n#print(paths_labels.shape)\n#print(sample_data.shape)\n\ndataset_train, dataset_valid = train_test_split(\n    paths_labels,\n    test_size=0.25, random_state=5\n)\n\nprint(dataset_train.shape)\nprint(dataset_valid.shape)","21fa22a6":"import shutil\n\n# creating the empty folders for each category in temporary memory\nfor i in class_names:\n    os.makedirs(os.path.join('..\/kaggle\/temp\/train_', str(i)))\n    os.makedirs(os.path.join('..\/kaggle\/temp\/valid_', str(i)))\n\n# copying each image in their respective temporary folders for train data\nfor c in class_names:\n    for i in list(dataset_train[dataset_train['class_6']==c].image_id):\n        get_image = os.path.join(imgs_path, i) # Path to Images\n        move_image_to_cat = shutil.copy(get_image, '..\/kaggle\/temp\/train_\/'+str(c))\n\n# copying each image in their respective temporary folders for train data\nfor c in class_names:\n    for i in list(dataset_valid[dataset_valid['class_6']==c].image_id):\n        get_image = os.path.join(imgs_path, i) # Path to Images\n        move_image_to_cat = shutil.copy(get_image, '..\/kaggle\/temp\/valid_\/'+str(c))","2b1964f5":"from tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '..\/kaggle\/temp\/train_\/',\n    labels='inferred',\n    label_mode='categorical',\n    image_size=[128, 128], # images have to have same size, here it will resize all to 128 (for better accuracy set it to 256)\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\n\nds_valid_ = image_dataset_from_directory(\n    '..\/kaggle\/temp\/valid_\/',\n    labels='inferred',\n    label_mode='categorical',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n\n# Data Pipeline for converting images to tensorflow float32 formar\n\n# function to do the reformatting\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\n# automatically tune by tensorflow\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# here is the pipeline for do all the steps of re-formatting images to numbers\n# one for train data and one for validation data\n# these are data sets that we will feed in to out model\n\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\n\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)","c93c60de":"pretrained_base = tf.keras.models.load_model(\n    '..\/input\/cv-course-models\/cv-course-models\/inceptionv1',\n)\npretrained_base.trainable = False","d1efdcad":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    pretrained_base,\n    layers.Flatten(),\n    layers.Dense(6, activation='relu'),\n    layers.Dense(6, activation='sigmoid'), # last layer we have to set 6 for our out puts, because of having 6 classes\n])","ac0f27b2":"model.compile(\n    optimizer='adam',\n    loss='CategoricalCrossentropy',\n    metrics=['categorical_accuracy'],\n)\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=30,\n    verbose=1, # use verbos 1 if you want to see the progress bar, if not set it to 0\n)","dd7d1012":"#import pandas as pd\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot();","947b203d":"# we also need to conver our test images to float numbers, however, when reading the test images\n# since there is no defined category for them, we have to set labels and label_mode to None\n# and then reading our file, resizing it and converting it to float32\n\nds_test_ = image_dataset_from_directory(\n    '..\/input\/dont-stop-until-you-drop\/images\/test_images',\n    labels=None,\n    label_mode=None,\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\ndef convert_test_to_float(image):\n    # we don't have labes here so a new function is defined\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n\nds_test = (\n    ds_test_\n    .map(convert_test_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\n\n# now we can predict test images\n\npredictions = model.predict(ds_test)\n\n# here our prediction will have 6 columns for each image representing the predicted probability to be belong\n# to each class. So we assume the maximum value would be our best choice as the class prediction for that image\n\n#argmax will return the index of maximum value in a column (when setting axis = 1) \n# and that index is equal to our category number 0,1,2,3,4,5\n\nscore = np.argmax(predictions,axis=1) \n\nresults = pd.DataFrame({'image_id': test_data_list,'class_6':score})\n\n#results.set_index(\"image_id\", inplace=True)\n\nresults.head()","d4cb4592":"# and saving it in the csv file format\nresults.to_csv('outputfile.csv', index=False)","d1bf5c6b":"# Introduction\n\nWhen the [competition](https:\/\/www.kaggle.com\/c\/dont-stop-until-you-drop\/overview) starts, I tried to implement what I have learned in kaggle courses (and other notes) in the topic of image classification. I'm biginner in this area and trying to level up my-self. However, I couldn't even reach to the point of submitting the answers. So here I tried to reach a solution by using the [Computer Vision](https:\/\/www.kaggle.com\/learn\/computer-vision) and others notebook, especially from the particiapnts in this competition. (thanks Bjorn for your amazing work).\nI hope this be useful for other beginners like me.","1625e408":"In the following cell I get help from notebook of [Bjorn](https:\/\/www.kaggle.com\/bjoernjostein) in his notebook of [Kaggle Days Shanghai 2021 - Accuracy 88.07%](https:\/\/www.kaggle.com\/bjoernjostein\/kaggle-days-shanghai-2021-accuracy-88-07)","5bed323c":"Based on [computer vision course - The Convolutional Classifier](https:\/\/www.kaggle.com\/ryanholbrook\/the-convolutional-classifier), we use a pretrained model. By experimenting some, I find out inceptionv1 would be better choice. However, you can try others as well.\n\nFor adding the pretrained models (which is cv-course-models in kaggle public data set), you can take following steps.\n\n1. click on + button for adding data\n2. search for cv-course-models\n3. add the data base\n\n![pretrained models.JPG](attachment:cb29f8be-ab37-4aff-bea1-a1d6ecaf3c7e.JPG)","71a3d0c2":"Note that, there are many other ways to prepare the data for model, but I find this easer and more understandable for beginners. I used [IAIN CRUICKSHANK](https:\/\/www.kaggle.com\/ijcrook) notebook([Kaggle Days x Z by HP Shanghai Submission](https:\/\/www.kaggle.com\/ijcrook\/kaggle-days-x-z-by-hp-shanghai-submission)) for the following cell.","c396dda5":"Now we can compile our model by setting it optimizer to adam (or other optimizer to test which is better), the loss should be CategoricalCrossentropy because of having more than 1 classes, and using categorical_accuracy as metics. You can refer to tenserflow documention for other [optimizers](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers), [losses](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses), and [metrices](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/metrics) inputs.\n\nAfter that, we fit our model with train data and validate it with validation data and store the losses and metrices data in the history variable. so that we could chartting it to see how our model progressed.","4769a576":"# Step 1: Loading and feeling the data\nIn this section we just want to be able to locate the data, and see what we have.\n\nData paths can be found and copied in the data section at top right position.\n\n![data paths.JPG](attachment:d8a209ad-98bd-4246-a5cf-b695bd20e548.JPG)\n\n","f672b404":"One important thing that we have to note in the charts, is seeing where and when the lines of train and valid are separated from each other. this will tell us where we start to have overfitting.","5ab5383f":"# Step 5: predict and generate submission file\n\nNow that we have to generate a csv file that contains images of test data and their respective predicted classes. Here we have to note that the image id should be set as the index of the csv file and we have one column named class_6 representing each image class.","e00ff7d8":"# Step 3: make data ready for tenserflow.keras\nNow, in order to have our data set ready for keras, we need to place each category's images in one folder (so we need to have 6 folders for each category for train as well as validation data. In this way we can read images of categories and preparing them for our model.\nTo creat empty folders, we have to use os.makedirs function and we can use the temporary memory in kaggle to save them (please check the answer [Guoxiang Zu](https:\/\/www.kaggle.com\/guoxiangzu) in this [link](https:\/\/www.kaggle.com\/questions-and-answers\/250701)).\n\nFor copying images of each category we first construct its path by joining the main path of the images data set and  image name (imageName.jpg), which is in our csv file. Afterward, by using [shutil](https:\/\/www.geeksforgeeks.org\/python-shutil-copy-method\/) library, we make a copy of that image and will put it in our temporary filders.","db91e594":"# Step 4: constructing a basic CNN model\n\nNow here we are ready to construct our model. Please note that, we use a very simple model here and you can make it more professional by going through the  [computer vision course](https:\/\/www.kaggle.com\/learn\/computer-vision), which is very amazing course. thanks to [Ryan Holbrook](https:\/\/www.kaggle.com\/ryanholbrook)\n\nNote than, in the last layer we have to set 6 for our out puts, because of having 6 classes.\n\nAlso, if you don't want to wait hours to get results, its better to turn on the GPU. take following steps:\n1. click on the settings to open up the options\n2. click on Accelerator\n3. select GPU\n\nNotet that your notebook will re started and you need to run all the codes again\n\n\n![accelerator.JPG](attachment:cdef0ae3-46b6-440d-a691-b522eddae19d.JPG)\n\n\nThe better option for running a very fast model, is to use TPU, howver, it need some settings that you can find at the end of computer vision course in the notes: [Create Your First Submission](https:\/\/www.kaggle.com\/ryanholbrook\/create-your-first-submission) and Getting Started: [TPUs + Cassava Leaf Disease](https:\/\/www.kaggle.com\/jessemostipak\/getting-started-tpus-cassava-leaf-disease). ","1cf7875f":"# Step 2: Spliting data to have training and validation\nIn order to use the data in the keras (based on the [computer vision course](https:\/\/www.kaggle.com\/learn\/computer-vision)), we need to have train and validation sets (validation set for checking the accuracy of model and improving it). In the following cell, we use the train_test_split function from sklearn library to doing so.","f3313d57":"By using [image_dataset_from_directory](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/image_dataset_from_directory) function, we can now read our images in each category and make it understandable for model. Note that, image_dataset_from_directory function can only understand categories when images are divided in each category.\n\nAfter that, we need to convert or images to numbers ([float32](https:\/\/en.wikipedia.org\/wiki\/Single-precision_floating-point_format)), in order to feeding it in our model."}}