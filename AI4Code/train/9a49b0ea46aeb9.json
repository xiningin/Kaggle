{"cell_type":{"5557fd33":"code","3d2e7494":"code","790dd207":"code","2a0615ea":"code","e56ddf27":"code","bd2e0525":"code","6c4abf05":"code","8e7d03e1":"code","01eaf5bb":"code","69ea3a80":"code","2538b38b":"code","a67cc13f":"code","51956b38":"code","a1141635":"code","9c3ad987":"code","a3305e6e":"code","3d8d8f7d":"code","dca379ef":"code","fea1d12e":"code","7c244720":"code","7c098aa2":"code","e1cb5cc4":"code","0c2148c2":"code","b1845b4b":"code","f83be9f7":"code","9b109fa4":"code","76f0ed35":"code","f5a98ace":"code","737e53ea":"code","fd3ccb68":"code","9fe5ef87":"markdown","c17b365a":"markdown","df607f1c":"markdown","260f6f8c":"markdown","1e6b60e1":"markdown","04100223":"markdown","8b50dd4e":"markdown","1927203d":"markdown","d804713d":"markdown","a50b1beb":"markdown","8532c8a3":"markdown","14c0dad6":"markdown","fb5dd2fe":"markdown","5fed012a":"markdown","dd25ca77":"markdown","75cf06bb":"markdown","965ee9e1":"markdown","94cacbfd":"markdown","6f539ac2":"markdown","c4248026":"markdown","7f8adeab":"markdown","2de2f701":"markdown","8be8e16f":"markdown","f48c1d63":"markdown","ecbf2158":"markdown"},"source":{"5557fd33":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3d2e7494":"#!pip install -U imbalanced-learn\ntraindf=pd.read_csv('..\/input\/trainingSetToMatchCustomTestSet111518.csv')\nprint(traindf.shape)\nprint(traindf.head())\n\ndef dropUselessFeatures(df):\n    print(df.shape)\n    df=df.drop(['Unnamed: 0','hephs','hepos','hepts','lephs','lepos','lepts',\n                  'hmphs','hmpos','hmpts','lmphs','lmpos','lmpts',\n                  'hlphs','hlpos','hlpts','llphs','llpos','llpts'], axis=1)\n    \n    #these boolean columns got rolled into the outlierScore\n    df=df.drop(['highEnergy_transitory_1.5_TF',\n       'lowEnergy_transitory_1.0_TF', 'lowEnergy_transitory_1.5_TF'], axis=1)\n    \n    \n    print(df.shape)\n    return df\n\ntraindf=dropUselessFeatures(traindf)\n\ntraindf.loc[:,'target']=traindf.loc[:,'target'].astype(str)\n\n#from stacy's code\n# move target to end\ntraindf = traindf[[c for c in traindf if c not in ['target']] + ['target']]\ntraindf.head()\n\n#df[1].fillna(0, inplace=True)\ntraindf['deltaDetect'].fillna(0,inplace=True)\ntigdf=traindf[traindf['hostgal_photoz']==0]\ntegdf=traindf[traindf['hostgal_photoz']!=0]\n\nprint(tigdf.shape)\nprint(tegdf.shape)\n\ntigdf=tigdf.drop(['hostgal_specz', 'hostgal_photoz', 'distmod', 'hostgal_photoz_err'], axis=1)\nprint(tigdf.shape)","790dd207":"tigdf.describe()","2a0615ea":"print('inter-galactic')\nfor theClass in tigdf.loc[:,'target'].unique():\n    print('class ' + str(theClass) + ':')\n    trueFilter=tigdf['target']==theClass\n    print(trueFilter.sum())","e56ddf27":"print('extra-galactic')\nfor theClass in tegdf.loc[:,'target'].unique():\n    print('class ' + str(theClass) + ':')\n    trueFilter=tegdf['target']==theClass\n    print(trueFilter.sum())","bd2e0525":"#https:\/\/www.kaggle.com\/qianchao\/smote-with-imbalance-data\n#from sklearn.preprocessing import StandardScaler\nXig = np.array(tigdf.iloc[:, tigdf.columns != 'target'])\nyig = np.array(tigdf.iloc[:, tigdf.columns == 'target'])\nprint('Shape of X: {}'.format(Xig.shape))\nprint('Shape of y: {}'.format(yig.shape))","6c4abf05":"\nXeg = np.array(tegdf.iloc[:, tegdf.columns != 'target'])\nyeg = np.array(tegdf.iloc[:, tegdf.columns == 'target'])\nprint('Shape of X: {}'.format(Xeg.shape))\nprint('Shape of y: {}'.format(yeg.shape))","8e7d03e1":"from imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import train_test_split\n\nXig_train, Xig_test, yig_train, yig_test = train_test_split(Xig, yig, test_size=0.3, random_state=0)\n\nprint(\"Number transactions X_train dataset: \", Xig_train.shape)\nprint(\"Number transactions y_train dataset: \", yig_train.shape)\nprint(\"Number transactions X_test dataset: \", Xig_test.shape)\nprint(\"Number transactions y_test dataset: \", yig_test.shape)","01eaf5bb":"print(\"Before OverSampling, counts of label '92': {}\".format(sum(yig_train=='92')))\nprint(\"Before OverSampling, counts of label '65': {} \\n\".format(sum(yig_train=='65')))\nprint(\"Before OverSampling, counts of label '16': {}\".format(sum(yig_train=='16')))\nprint(\"Before OverSampling, counts of label '6': {} \\n\".format(sum(yig_train=='6')))\nprint(\"Before OverSampling, counts of label '53': {}\".format(sum(yig_train=='53')))\n\nsm = SMOTE(random_state=2)\nXig_train_res, yig_train_res = sm.fit_sample(Xig_train, yig_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(Xig_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(yig_train_res.shape))\n\nprint(\"After OverSampling, counts of label '92': {}\".format(sum(yig_train_res=='92')))\nprint(\"After OverSampling, counts of label '65': {}\".format(sum(yig_train_res=='65')))\nprint(\"After OverSampling, counts of label '16': {}\".format(sum(yig_train_res=='16')))\nprint(\"After OverSampling, counts of label '6': {}\".format(sum(yig_train_res=='6')))\nprint(\"After OverSampling, counts of label '53': {}\".format(sum(yig_train_res=='53')))\n","69ea3a80":"def smoteAdataset(Xig, yig, test_size=0.2, random_state=0):\n    \n    Xig_train, Xig_test, yig_train, yig_test = train_test_split(Xig, yig, test_size=test_size, random_state=random_state)\n    print(\"Number transactions X_train dataset: \", Xig_train.shape)\n    print(\"Number transactions y_train dataset: \", yig_train.shape)\n    print(\"Number transactions X_test dataset: \", Xig_test.shape)\n    print(\"Number transactions y_test dataset: \", yig_test.shape)\n\n    classes=[]\n    for i in np.unique(yig):\n        classes.append(i)\n        print(\"Before OverSampling, counts of label \" + str(i) + \": {}\".format(sum(yig_train==i)))\n        \n    sm=SMOTE(random_state=2)\n    Xig_train_res, yig_train_res = sm.fit_sample(Xig_train, yig_train.ravel())\n\n    print('After OverSampling, the shape of train_X: {}'.format(Xig_train_res.shape))\n    print('After OverSampling, the shape of train_y: {} \\n'.format(yig_train_res.shape))\n    \n    for eachClass in classes:\n        print(\"After OverSampling, counts of label \" + str(eachClass) + \": {}\".format(sum(yig_train_res==eachClass)))\n        \n    return Xig_train_res, yig_train_res\n\nXeg_train_res, yeg_train_res=smoteAdataset(Xeg, yeg)\n","2538b38b":"%matplotlib inline","a67cc13f":"# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(__doc__)\n\nrng = np.random.RandomState(18)\n\nf, ax = plt.subplots(1, 1, figsize=(8, 8))\n\n# generate some data points\ny = np.array([3.65284, 3.52623, 3.51468, 3.22199, 3.21])\nz = np.array([0.43, 0.45, 0.6, 0.4, 0.211])\ny_2 = np.array([3.3, 3.6])\nz_2 = np.array([0.58, 0.34])\n\n# plot the majority and minority samples\nax.scatter(z, y, label='Minority class', s=100)\nax.scatter(z_2, y_2, label='Majority class', s=100)\n\nidx = rng.randint(len(y), size=2)\nannotation = [r'$x_i$', r'$x_{zi}$']\n\nfor a, i in zip(annotation, idx):\n    ax.annotate(a, (z[i], y[i]),\n                xytext=tuple([z[i] + 0.01, y[i] + 0.005]),\n                fontsize=15)\n\n# draw the circle in which the new sample will generated\nradius = np.sqrt((z[idx[0]] - z[idx[1]]) ** 2 + (y[idx[0]] - y[idx[1]]) ** 2)\ncircle = plt.Circle((z[idx[0]], y[idx[0]]), radius=radius, alpha=0.2)\nax.add_artist(circle)\n\n# plot the line on which the sample will be generated\nax.plot(z[idx], y[idx], '--', alpha=0.5)\n\n# create and plot the new sample\nstep = rng.uniform()\ny_gen = y[idx[0]] + step * (y[idx[1]] - y[idx[0]])\nz_gen = z[idx[0]] + step * (z[idx[1]] - z[idx[0]])\n\nax.scatter(z_gen, y_gen, s=100)\nax.annotate(r'$x_{new}$', (z_gen, y_gen),\n            xytext=tuple([z_gen + 0.01, y_gen + 0.005]),\n            fontsize=15)\n\n# make the plot nicer with legend and label\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.get_xaxis().tick_bottom()\nax.get_yaxis().tick_left()\nax.spines['left'].set_position(('outward', 10))\nax.spines['bottom'].set_position(('outward', 10))\nax.set_xlim([0.2, 0.7])\nax.set_ylim([3.2, 3.7])\nplt.xlabel(r'$X_1$')\nplt.ylabel(r'$X_2$')\nplt.legend()\nplt.tight_layout()\nplt.show()","51956b38":"# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import LinearSVC\n\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.base import BaseSampler\n\nprint(__doc__)","a1141635":"def create_dataset(n_samples=1000, weights=(0.01, 0.01, 0.98), n_classes=3,\n                   class_sep=0.8, n_clusters=1):\n    return make_classification(n_samples=n_samples, n_features=2,\n                               n_informative=2, n_redundant=0, n_repeated=0,\n                               n_classes=n_classes,\n                               n_clusters_per_class=n_clusters,\n                               weights=list(weights),\n                               class_sep=class_sep, random_state=0)","9c3ad987":"def plot_resampling(X, y, sampling, ax):\n    X_res, y_res = sampling.fit_resample(X, y)\n    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n    # make nice plotting\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n    return Counter(y_res)","a3305e6e":"def plot_decision_function(X, y, clf, ax):\n    plot_step = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.4)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')","3d8d8f7d":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\nX, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94))\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title('Linear SVC with y={}'.format(Counter(y)))\npipe = make_pipeline(RandomOverSampler(random_state=0), LinearSVC())\npipe.fit(X, y)\nplot_decision_function(X, y, pipe, ax2)\nax2.set_title('Decision function for RandomOverSampler')\nfig.tight_layout()","dca379ef":"# Make an identity sampler\nclass FakeSampler(BaseSampler):\n\n    _sampling_type = 'bypass'\n\n    def _fit_resample(self, X, y):\n        return X, y\n\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15))\nX, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94))\nsampler = FakeSampler()\nclf = make_pipeline(sampler, LinearSVC())\nplot_resampling(X, y, sampler, ax1)\nax1.set_title('Original data - y={}'.format(Counter(y)))\n\nax_arr = (ax2, ax3, ax4)\nfor ax, sampler in zip(ax_arr, (RandomOverSampler(random_state=0),\n                                SMOTE(random_state=0),\n                                ADASYN(random_state=0))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_resampling(X, y, sampler, ax)\n    ax.set_title('Resampling using {}'.format(sampler.__class__.__name__))\nfig.tight_layout()","fea1d12e":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94),\n                      class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4))\nfor ax, sampler in zip(ax_arr, (SMOTE(random_state=0),\n                                ADASYN(random_state=0))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}'.format(\n        sampler.__class__.__name__))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}'.format(\n        sampler.__class__.__name__))\nfig.tight_layout()","7c244720":"fig, ((ax1, ax2), (ax3, ax4),\n      (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(15, 30))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94),\n                      class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8))\nfor ax, sampler in zip(ax_arr,\n                       (SMOTE(random_state=0),\n                        BorderlineSMOTE(random_state=0, kind='borderline-1'),\n                        BorderlineSMOTE(random_state=0, kind='borderline-2'),\n                        SVMSMOTE(random_state=0))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}'.format(\n        sampler.__class__.__name__))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}'.format(sampler.__class__.__name__))\nfig.tight_layout()","7c098aa2":"# create a synthetic data set with continuous and categorical features\nrng = np.random.RandomState(42)\nn_samples = 50\nX = np.empty((n_samples, 3), dtype=object)\nX[:, 0] = rng.choice(['A', 'B', 'C'], size=n_samples).astype(object)\nX[:, 1] = rng.randn(n_samples)\nX[:, 2] = rng.randint(3, size=n_samples)\ny = np.array([0] * 20 + [1] * 30)\n\nprint('The original imbalanced dataset')\nprint(sorted(Counter(y).items()))\nprint('The first and last columns are containing categorical features:')\nprint(X[:5])\n\nsmote_nc = SMOTENC(categorical_features=[0, 2], random_state=0)\nX_resampled, y_resampled = smote_nc.fit_resample(X, y)\nprint('Dataset after resampling:')\nprint(sorted(Counter(y_resampled).items()))\nprint('SMOTE-NC will generate categories for the categorical features:')\nprint(X_resampled[-5:])\n\nplt.show()","e1cb5cc4":"# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\n\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import (ClusterCentroids, RandomUnderSampler,\n                                     NearMiss,\n                                     InstanceHardnessThreshold,\n                                     CondensedNearestNeighbour,\n                                     EditedNearestNeighbours,\n                                     RepeatedEditedNearestNeighbours,\n                                     AllKNN,\n                                     NeighbourhoodCleaningRule,\n                                     OneSidedSelection)\nprint(__doc__)","0c2148c2":"def plot_resampling(X, y, sampling, ax):\n    X_res, y_res = sampling.fit_resample(X, y)\n    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n    # make nice plotting\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n    return Counter(y_res)","b1845b4b":"def plot_decision_function(X, y, clf, ax):\n    plot_step = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.4)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')","f83be9f7":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94),\n                      class_sep=0.8)\n\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title('Linear SVC with y={}'.format(Counter(y)))\nsampler = ClusterCentroids(random_state=0)\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax2)\nax2.set_title('Decision function for {}'.format(sampler.__class__.__name__))\nplot_resampling(X, y, sampler, ax3)\nax3.set_title('Resampling using {}'.format(sampler.__class__.__name__))\nfig.tight_layout()","9b109fa4":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94),\n                      class_sep=0.8)\n\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title('Linear SVC with y={}'.format(Counter(y)))\nsampler = RandomUnderSampler(random_state=0)\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax2)\nax2.set_title('Decision function for {}'.format(sampler.__class__.__name__))\nplot_resampling(X, y, sampler, ax3)\nax3.set_title('Resampling using {}'.format(sampler.__class__.__name__))\nfig.tight_layout()","76f0ed35":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2,\n                                                         figsize=(15, 25))\nX, y = create_dataset(n_samples=5000, weights=(0.1, 0.2, 0.7), class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6))\nfor ax, sampler in zip(ax_arr, (NearMiss(version=1),\n                                NearMiss(version=2),\n                                NearMiss(version=3))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}-{}'.format(\n        sampler.__class__.__name__, sampler.version))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}-{}'.format(\n        sampler.__class__.__name__, sampler.version))\nfig.tight_layout()","f5a98ace":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2,\n                                                         figsize=(15, 25))\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6))\nfor ax, sampler in zip(ax_arr, (\n        EditedNearestNeighbours(),\n        RepeatedEditedNearestNeighbours(),\n        AllKNN(allow_minority=True))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}'.format(\n        sampler.__class__.__name__))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}'.format(\n        sampler.__class__.__name__))\nfig.tight_layout()","737e53ea":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2,\n                                                         figsize=(15, 25))\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6))\nfor ax, sampler in zip(ax_arr, (\n        CondensedNearestNeighbour(random_state=0),\n        OneSidedSelection(random_state=0),\n        NeighbourhoodCleaningRule())):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}'.format(\n        sampler.__class__.__name__))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}'.format(\n        sampler.__class__.__name__))\nfig.tight_layout()","fd3ccb68":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94),\n                      class_sep=0.8)\n\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title('Linear SVC with y={}'.format(Counter(y)))\nsampler = InstanceHardnessThreshold(\n    random_state=0, estimator=LogisticRegression(solver='lbfgs',\n                                                 multi_class='auto'))\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax2)\nax2.set_title('Decision function for {}'.format(sampler.__class__.__name__))\nplot_resampling(X, y, sampler, ax3)\nax3.set_title('Resampling using {}'.format(sampler.__class__.__name__))\nfig.tight_layout()\n\nplt.show()","9fe5ef87":"## The first time I did it manually\n- I copied [qianchao's](https:\/\/www.kaggle.com\/qianchao) [kernel](https:\/\/www.kaggle.com\/qianchao\/smote-with-imbalance-data) and then modified the classes","c17b365a":"# NOTHING BELOW THIS POINT IS MY WORK\n- URLs given at the beginning of each (oversampling, undersampling) section\n- I left it in this kernel because its a handy reference","df607f1c":"## Make sure you've dealt with NaNs before this point","260f6f8c":"Oversamping sample code from https:\/\/imbalanced-learn.org\/en\/stable\/auto_examples\/index.html","1e6b60e1":"``InstanceHardnessThreshold`` uses the prediction of classifier to exclude\nsamples. All samples which are classified with a low probability will be\nremoved.\n\n","04100223":"## How badly is the data imbalanced?  Intergalactic:","8b50dd4e":"### This kernel expands on **[this work](https:\/\/www.kaggle.com\/qianchao\/smote-with-imbalance-data)**\n- in addition to handling multiclass, it creates a method that can be called repeatedly\n- which is important because [SMOTE should be done AFTER cross validation splits](https:\/\/www.marcoaltini.com\/blog\/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation)","1927203d":"We (they) will first illustrate the influence of the balancing ratio on some toy\ndata using a linear SVM classifier. Greater is the difference between the\nnumber of samples in each class, poorer are the classfication results.\n","d804713d":"Due to those sampling particularities, it can give rise to some specific\nissues as illustrated below.\n\n","a50b1beb":"SMOTE proposes several variants by identifying specific samples to consider\nduring the resampling. The borderline version will detect which point to\nselect which are in the border between two classes. The SVM version will use\nthe support vectors found using an SVM algorithm to create new samples.\n\n","8532c8a3":"The following plot illustrate the difference between ADASYN and SMOTE. ADASYN\nwill focus on the samples which are difficult to classify with a\nnearest-neighbors rule while regular SMOTE will not make any distinction.\nTherefore, the decision function depending of the algorithm.\n\n","14c0dad6":"## Get tigdf and tegdf (train intergalactic data frame, train extragalactic data frame)","fb5dd2fe":"Undersamping sample code from https:\/\/imbalanced-learn.org\/en\/stable\/auto_examples\/index.html","5fed012a":"## We're ready to start training models\n- We have oversampling training sets for both inter-galactic and extra-galactic objects","dd25ca77":"## Repeat for the extra-galactic set","75cf06bb":"``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a\nsample should be kept in a dataset or not. The issue is that\n``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy\nsamples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to\nremove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a\n``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3\nnearest-neighbors to remove samples which do not agree with this rule.\n\n","965ee9e1":"When dealing with a mixed of continuous and categorical features, SMOTE-NC\nis the only method which can handle this case.\n\n","94cacbfd":"## How badly is the data imbalanced?  Extragalactic:","6f539ac2":"### I'm using [qianchao's](https:\/\/www.kaggle.com\/qianchao) kernel as a starting point","c4248026":"# Everything below this point is for reference only","7f8adeab":"``ClusterCentroids`` under-samples by replacing the original samples by the\ncentroids of the cluster found.\n\n","2de2f701":"The algorithm performing prototype selection can be subdivided into two\ngroups: (i) the controlled under-sampling methods and (ii) the cleaning\nunder-sampling methods.\n\nWith the controlled under-sampling methods, the number of samples to be\nselected can be specified. ``RandomUnderSampler`` is the most naive way of\nperforming such selection by randomly selecting a given number of samples by\nthe targetted class.\n\n","8be8e16f":"## We really need to make this a method\n- We're going to have to do it over and over again because SMOTE should be done AFTER cross validation splitting\n- and there's no guarantee every class will be in every cross validated sample","f48c1d63":"``NearMiss`` algorithms implement some heuristic rules in order to select\nsamples. NearMiss-1 selects samples from the majority class for which the\naverage distance of the $k$` nearest samples of the minority class is\nthe smallest. NearMiss-2 selects the samples from the majority class for\nwhich the average distance to the farthest samples of the negative class is\nthe smallest. NearMiss-3 is a 2-step algorithm: first, for each minority\nsample, their :$m$ nearest-neighbors will be kept; then, the majority\nsamples selected are the on for which the average distance to the $k$\nnearest neighbors is the largest.\n\n","ecbf2158":"``EditedNearestNeighbours`` removes samples of the majority class for which\ntheir class differ from the one of their nearest-neighbors. This sieve can be\nrepeated which is the principle of the\n``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from\nthe ``RepeatedEditedNearestNeighbours`` by changing the $k$ parameter\nof the internal nearest neighors algorithm, increasing it at each iteration.\n\n"}}