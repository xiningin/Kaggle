{"cell_type":{"bb1ae2cb":"code","584977ae":"code","3289bd8e":"code","92e769b7":"code","87461759":"code","ae2797e6":"code","0191c8bd":"code","e2175a55":"code","12ddbb86":"code","9271cf29":"code","38b3668e":"markdown","ef506d7c":"markdown","f0a15d0d":"markdown","b530173d":"markdown","d0e9657e":"markdown"},"source":{"bb1ae2cb":"# import\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","584977ae":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntrain","3289bd8e":"# data preparation\n\n# combine the dataset\ncombined = pd.concat([train['excerpt'], test['excerpt']])\n# set vocab count\nvocab = 10000\ntokenizer = Tokenizer(num_words=vocab, oov_token=0)\ntokenizer.fit_on_texts(combined) \nsequence_combined = tokenizer.texts_to_sequences(combined)\nmax_len = max([len(x) for x in sequence_combined])\nsequences = tokenizer.texts_to_sequences(train['excerpt'])\npadded_seq = pad_sequences(sequences, maxlen=max_len, dtype='int32', padding='pre',truncating='pre', value=0)","92e769b7":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Bidirectional, Embedding","87461759":"# mode\nmodel = Sequential()\n# encoder\nmodel.add(keras.Input(shape=(padded_seq.shape[1], )))\nmodel.add(Embedding(vocab, 300))\nmodel.add(Bidirectional(LSTM(256)))\n# decoder\nmodel.add(Dense(256, kernel_initializer='normal', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, kernel_initializer='normal', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, kernel_initializer='normal', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, kernel_initializer='normal', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16, kernel_initializer='normal', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(8, kernel_initializer='normal', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='linear'))\n# summary\nmodel.summary()","ae2797e6":"# callbacks\nearlystopping = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n\n# compile\nmodel.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n\n# fit\nhistory = model.fit(padded_seq, train['target'], epochs=100, batch_size=32, verbose=2)","0191c8bd":"import matplotlib.pyplot as plt\nprint(history.history.keys())\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","e2175a55":"test_sequences = tokenizer.texts_to_sequences(test['excerpt'])\ntest_pad_sequences = pad_sequences(test_sequences, maxlen=max_len, dtype='int32', padding='pre',truncating='pre', value=0)\ny_pred = model.predict(test_pad_sequences)","12ddbb86":"sub = test[['id']].copy()","9271cf29":"sub['target'] = y_pred\nsub.to_csv(\"submission.csv\", index=False)","38b3668e":"# Commonlit | biLSTM Sentence encoder\n\n","ef506d7c":"## Submission","f0a15d0d":"## Test data","b530173d":"## biLSTM based Regression Model\n\n- Sentence encoder: biLSTM\n- Target decoder: Regression model","d0e9657e":"## Data preparation"}}