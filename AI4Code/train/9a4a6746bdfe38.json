{"cell_type":{"b92a868d":"code","7f88204c":"code","28cbea34":"code","27cebd95":"code","0d43a712":"code","5085e906":"code","c88ae396":"code","7a805cb8":"code","ce1e5959":"code","65b9c518":"code","4bead776":"code","998afb4f":"code","538c9c5a":"code","b1b288f5":"code","5635ffd9":"markdown","704685ca":"markdown","2701d74e":"markdown","53fd2796":"markdown","1dc18f37":"markdown","4b43c1b2":"markdown","b27d409e":"markdown","b7253205":"markdown","d7128633":"markdown","6033f02c":"markdown","d8eebc13":"markdown","619ea540":"markdown","b0c09224":"markdown","c17021b3":"markdown","6f995eee":"markdown","2e09f8f5":"markdown","b94b2483":"markdown","1ed4345e":"markdown","1f8d5006":"markdown"},"source":{"b92a868d":"# Importing required moduless\nimport numpy as np \nimport pandas as pd \nimport pprint,random\n\nfrom scipy.ndimage.interpolation import shift\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.optimizers import SGD","7f88204c":"class tic_tac_toe_game(object):\n    def __init__(self):\n        self.board=np.full((3,3),2)\n\n    def toss(self):\n        \"\"\"Function to simulate a toss and decide which player goes first\n\n        Args:\n\n        Returns:\n        Returns 1 if player assigned mark 1 has won, or 0 if his opponent won\n\n        \"\"\"\n        turn=np.random.randint(0,2,size=1)\n        if turn.mean()==0:\n            self.turn_monitor=0\n        elif turn.mean()==1:\n            self.turn_monitor=1\n        return self.turn_monitor\n\n    def move(self,player,coord):\n        \"\"\"Function to perform the action of placing a mark on the tic tac toe board\n        After performing the action, this function flips the value of the turn_monitor to \n        the next player\n\n        Args:\n        player: 1 if player who is assigned the mark 1 is performing the action, \n        0 if his opponent is performing the action\n        coord: The coordinate where the 1 or 0 is to be placed on the \n        tic-tac-toe board (numpy array)\n\n        Returns:\n        game_status(): Calls the game status function and returns its value\n        board: Returns the new board state after making the move\n\n        \"\"\"\n        if self.board[coord]!=2 or self.game_status()!=\"In Progress\" or self.turn_monitor!=player:\n            raise ValueError(\"Invalid move\")\n        self.board[coord]=player\n        self.turn_monitor=1-player\n        return self.game_status(),self.board\n\n\n    def game_status(self):\n        \"\"\"Function to check the current status of the game, \n        whether the game has been won, drawn or is in progress\n\n        Args:\n\n        Returns:\n        \"Won\" if the game has been won, \"Drawn\" if the \n        game has been drawn, or \"In Progress\", if the game is still in progress\n\n        \"\"\"\n        #check for a win along rows\n        for i in range(self.board.shape[0]):\n            if 2 not in self.board[i,:] and len(set(self.board[i,:]))==1:\n                return \"Won\"\n        #check for a win along columns\n        for j in range(self.board.shape[1]):\n            if 2 not in self.board[:,j] and len(set(self.board[:,j]))==1:\n                return \"Won\"\n        # check for a win along diagonals\n        if 2 not in np.diag(self.board) and len(set(np.diag(self.board)))==1:\n            return \"Won\"\n        if 2 not in np.diag(np.fliplr(self.board)) and len(set(np.diag(np.fliplr(self.board))))==1:\n            return \"Won\"\n        # check for a Draw\n        if not 2 in self.board:\n            return \"Drawn\"\n        else:\n            return \"In Progress\"","28cbea34":"# create an object of the class tick_tac_toe_game\ngame=tic_tac_toe_game()\n# toss to decide which player goes first\ngame.toss()\nprint(\"Player \",game.turn_monitor,\" has won the toss\")\n# make the first move\nprint(\"Initial board state \\n\",game.board)\nprint(\"Let first player place their mark on 0,0\")\ngame_status,board=game.move(game.turn_monitor,(0,0))\nprint(\"New Board State: \\n\",board)\nprint(\"Let second player place their mark on 0,1\")\ngame_status,board=game.move(game.turn_monitor,(0,1))\nprint(\"New Board State: \\n\",board)\nprint(\"Let first player place their mark on 1,1\")\ngame_status,board=game.move(game.turn_monitor,(1,1))\nprint(\"New Board State: \\n\",board)\nprint(\"Let second player place their mark on 0,2\")\ngame_status,board=game.move(game.turn_monitor,(0,2))\nprint(\"New Board State: \\n\",board)\nprint(\"Let first player place their mark on 2,2\")\ngame_status,board=game.move(game.turn_monitor,(2,2))\nprint(\"New Board State: \\n\",board)\nprint(\"Player \",1-game.turn_monitor,\" Has \",game_status)","27cebd95":"def legal_moves_generator(current_board_state,turn_monitor):\n    \"\"\"Function that returns the set of all possible legal moves and resulting board states, \n    for a given input board state and player\n\n    Args:\n    current_board_state: The current board state\n    turn_monitor: 1 if it's the player who places the mark 1's turn to play, 0 if its his opponent's turn\n\n    Returns:\n    legal_moves_dict: A dictionary of a list of possible next coordinate-resulting board state pairs\n    The resulting board state is flattened to 1 d array\n\n    \"\"\"\n    legal_moves_dict={}\n    for i in range(current_board_state.shape[0]):\n        for j in range(current_board_state.shape[1]):\n            if current_board_state[i,j]==2:\n                board_state_copy=current_board_state.copy()\n                board_state_copy[i,j]=turn_monitor\n                legal_moves_dict[(i,j)]=board_state_copy.flatten()\n    return legal_moves_dict","0d43a712":"game=tic_tac_toe_game()\ngame.toss()\nprint(\"Player \",game.turn_monitor,\" has won the toss\")\nprint(\"Current board state \\n\",game.board)\nlegal_moves_dict=legal_moves_generator(game.board,game.turn_monitor)\nprint(\"Dictionary of Possible Next Legal Moves:\")\npprint.pprint(legal_moves_dict)","5085e906":"model = Sequential()\nmodel.add(Dense(18, input_dim=9,kernel_initializer='normal', activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(9, kernel_initializer='normal',activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1,kernel_initializer='normal'))\n\nlearning_rate = 0.001\nmomentum = 0.8\n\nsgd = SGD(lr=learning_rate, momentum=momentum,nesterov=False)\nmodel.compile(loss='mean_squared_error', optimizer=sgd)\nmodel.summary()","c88ae396":"def move_selector(model,current_board_state,turn_monitor):\n    \"\"\"Function that selects the next move to make from a set of possible legal moves\n\n    Args:\n    model: The Evaluator function to use to evaluate each possible next board state\n    turn_monitor: 1 if it's the player who places the mark 1's turn to play, 0 if its his opponent's turn\n\n    Returns:\n    selected_move: The numpy array coordinates where the player should place thier mark\n    new_board_state: The flattened new board state resulting from performing above selected move\n    score: The score that was assigned to the above selected_move by the Evaluator (model)\n\n    \"\"\"\n    tracker={}\n    legal_moves_dict=legal_moves_generator(current_board_state,turn_monitor)\n    for legal_move_coord in legal_moves_dict:\n        score=model.predict(legal_moves_dict[legal_move_coord].reshape(1,9))\n        tracker[legal_move_coord]=score\n    selected_move=max(tracker, key=tracker.get)\n    new_board_state=legal_moves_dict[selected_move]\n    score=tracker[selected_move]\n    return selected_move,new_board_state,score","7a805cb8":"# new game\ngame=tic_tac_toe_game()\n# toss\ngame.toss()\n# choose the first move\nprint(\"Player assigned mark 1\",game.turn_monitor,\" won the toss\")\nprint(\"Initial board state:\")\nprint(game.board)\nselected_move,new_board_state,score=move_selector(model,game.board,game.turn_monitor)\nprint(\"Selected move: \",selected_move)\nprint(\"Resulting new board state: \",new_board_state)\nprint(\"Score assigned to above board state by Evaluator(model): \", score)","ce1e5959":"def row_winning_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan rowwise and identify coordinate amongst the legal coordinates that will\n    result in a winning board state\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n    \n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will lead to win for the opponent\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        #check for a win along rows\n        for i in range(current_board_state_copy.shape[0]):\n            if 2 not in current_board_state_copy[i,:] and len(set(current_board_state_copy[i,:]))==1:\n                selected_move=legal_move_coord\n                return selected_move\n            \ndef column_winning_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan column wise and identify coordinate amongst the legal coordinates that will\n    result in a winning board state\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n\n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will lead to win for the opponent\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        for j in range(current_board_state_copy.shape[1]):\n                    if 2 not in current_board_state_copy[:,j] and len(set(current_board_state_copy[:,j]))==1:\n                        selected_move=legal_move_coord\n                        return selected_move\n\ndef diag1_winning_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan diagonal and identify coordinate amongst the legal coordinates that will\n    result in a winning board state\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n\n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will lead to win for the opponent\n\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        if 2 not in np.diag(current_board_state_copy) and len(set(np.diag(current_board_state_copy)))==1:\n            selected_move=legal_move_coord\n            return selected_move\n            \ndef diag2_winning_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan second diagonal and identify coordinate amongst the legal coordinates that will\n    result in a winning board state\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n\n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will lead to win for the opponent\n\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        if 2 not in np.diag(np.fliplr(current_board_state_copy)) and len(set(np.diag(np.fliplr(current_board_state_copy))))==1:\n            selected_move=legal_move_coord\n            return selected_move\n            \n#------------#\n\ndef row_block_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan rowwise and identify coordinate amongst the legal coordinates \n    that will prevent the program \n    from winning\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n    \n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will block 1 from winning\n\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        for i in range(current_board_state_copy.shape[0]):\n            if 2 not in current_board_state_copy[i,:] and (current_board_state_copy[i,:]==1).sum()==2:\n                if not (2 not in current_board_state[i,:] and (current_board_state[i,:]==1).sum()==2):\n                    selected_move=legal_move_coord\n                    return selected_move\n            \ndef column_block_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan column wise and identify coordinate amongst the legal coordinates that will prevent 1 \n    from winning\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n    \n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will block 1 from winning\n\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        \n        for j in range(current_board_state_copy.shape[1]):\n                    if 2 not in current_board_state_copy[:,j] and (current_board_state_copy[:,j]==1).sum()==2:\n                        if not (2 not in current_board_state[:,j] and (current_board_state[:,j]==1).sum()==2):\n                            selected_move=legal_move_coord\n                            return selected_move\n\ndef diag1_block_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan diagonal 1 and identify coordinate amongst the legal coordinates that will prevent 1 \n    from winning\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n    \n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will block 1 from winning\n\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor    \n        if 2 not in np.diag(current_board_state_copy) and (np.diag(current_board_state_copy)==1).sum()==2:\n                if not (2 not in np.diag(current_board_state) and (np.diag(current_board_state)==1).sum()==2):\n                    selected_move=legal_move_coord\n                    return selected_move\n            \ndef diag2_block_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan second diagonal wise and identify coordinate amongst the legal coordinates that will\n    result in a column having only 0s\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n    \n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will lead to two 0s being there (and no 1s)\n\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        if 2 not in np.diag(np.fliplr(current_board_state_copy)) and (np.diag(np.fliplr(current_board_state_copy))==1).sum()==2:\n            if not (2 not in np.diag(np.fliplr(current_board_state)) and (np.diag(np.fliplr(current_board_state))==1).sum()==2):\n                selected_move=legal_move_coord\n                return selected_move\n\n#---------------#\ndef row_second_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan rowwise and identify coordinate amongst the legal coordinates that will\n    result in a row having two 0s and no 1s\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n    \n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will lead to two 0s being there (and no 1s)\n\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        \n        for i in range(current_board_state_copy.shape[0]):\n            if 1 not in current_board_state_copy[i,:] and (current_board_state_copy[i,:]==0).sum()==2:\n                if not (1 not in current_board_state[i,:] and (current_board_state[i,:]==0).sum()==2):\n                    selected_move=legal_move_coord\n                    return selected_move\n            \ndef column_second_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan column wise and identify coordinate amongst the legal coordinates that will\n    result in a column having two 0s and no 1s\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n    \n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will lead to two 0s being there (and no 1s)\n\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        \n        for j in range(current_board_state_copy.shape[1]):\n                    if 1 not in current_board_state_copy[:,j] and (current_board_state_copy[:,j]==0).sum()==2:\n                        if not (1 not in current_board_state[:,j] and (current_board_state[:,j]==0).sum()==2):\n                            selected_move=legal_move_coord\n                            return selected_move\n\ndef diag1_second_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan diagonal wise and identify coordinate amongst the legal coordinates that will\n    result in a column having two 0s and no 1s\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n    \n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will lead to two 0s being there (and no 1s)\n\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        if 1 not in np.diag(current_board_state_copy) and (np.diag(current_board_state_copy)==0).sum()==2:\n            if not (1 not in np.diag(current_board_state) and (np.diag(current_board_state)==0).sum()==2):\n                selected_move=legal_move_coord\n                return selected_move\n            \ndef diag2_second_move_check(current_board_state,legal_moves_dict,turn_monitor):\n    \"\"\"Function to scan second diagonal wise and identify coordinate amongst \n    the legal coordinates that will result in a column having two 0s and no 1s\n\n    Args:\n    legal_moves_dict: Dictionary of legal next moves\n    turn_monitor: whose turn it is to move\n    \n    Returns:\n    selected_move: The coordinates of numpy array where opponent places their mark\n\n    \"\"\" \n    legal_move_coords =  list(legal_moves_dict.keys())\n    random.shuffle(legal_move_coords)\n    for legal_move_coord in legal_move_coords:\n        current_board_state_copy=current_board_state.copy()\n        current_board_state_copy[legal_move_coord]=turn_monitor\n        if 1 not in np.diag(np.fliplr(current_board_state_copy)) and (np.diag(np.fliplr(current_board_state_copy))==0).sum()==2:\n            if not (1 not in np.diag(np.fliplr(current_board_state)) and (np.diag(np.fliplr(current_board_state))==0).sum()==2):\n                selected_move=legal_move_coord\n                return selected_move\n    \ndef opponent_move_selector(current_board_state,turn_monitor,mode):\n    \"\"\"Function that picks a legal move for the opponent\n\n    Args:\n    current_board_state: Current board state\n    turn_monitor: whose turn it is to move\n    mode: whether hard or easy mode\n\n    Returns:\n    selected_move: The coordinates of numpy array where placing the 0 will lead to two 0s being there (and no 1s)\n\n    \"\"\" \n    legal_moves_dict=legal_moves_generator(current_board_state,turn_monitor)\n    \n    winning_move_checks=[row_winning_move_check,column_winning_move_check,diag1_winning_move_check,diag2_winning_move_check]\n    block_move_checks=[row_block_move_check,column_block_move_check,diag1_block_move_check,diag2_block_move_check]\n    second_move_checks=[row_second_move_check,column_second_move_check,diag1_second_move_check,diag2_second_move_check]\n\n    if mode==\"Hard\":\n        random.shuffle(winning_move_checks)\n        random.shuffle(block_move_checks)\n        random.shuffle(second_move_checks)        \n        \n        for fn in winning_move_checks:\n            if fn(current_board_state,legal_moves_dict,turn_monitor):\n                return fn(current_board_state,legal_moves_dict,turn_monitor)\n            \n        for fn in block_move_checks:\n            if fn(current_board_state,legal_moves_dict,turn_monitor):\n                return fn(current_board_state,legal_moves_dict,turn_monitor)\n            \n        for fn in second_move_checks:\n            if fn(current_board_state,legal_moves_dict,turn_monitor):\n                return fn(current_board_state,legal_moves_dict,turn_monitor)\n            \n        selected_move=random.choice(list(legal_moves_dict.keys()))\n        return selected_move\n    \n    elif mode==\"Easy\":\n        legal_moves_dict=legal_moves_generator(current_board_state,turn_monitor)\n        selected_move=random.choice(list(legal_moves_dict.keys()))\n        return selected_move","65b9c518":"def train(model,mode,print_progress=False):\n    \"\"\"Function trains the Evaluator (model) by playing a game against an opponent \n    playing random moves, and updates the weights of the model after the game\n    \n    Note that the model weights are updated using SGD with a batch size of 1\n\n    Args:\n    model: The Evaluator function being trained\n\n    Returns:\n    model: The model updated using SGD\n    y: The corrected scores\n\n    \"\"\" \n    # start the game\n    if print_progress==True:\n        print(\"___________________________________________________________________\")\n        print(\"Starting a new game\")\n    game=tic_tac_toe_game()\n    game.toss()\n    scores_list=[]\n    corrected_scores_list=[]\n    new_board_states_list=[]\n    \n    while(1):\n        if game.game_status()==\"In Progress\" and game.turn_monitor==1:\n            # If its the program's turn, use the Move Selector function to select the next move\n            selected_move,new_board_state,score=move_selector(model,game.board,game.turn_monitor)\n            scores_list.append(score[0][0])\n            new_board_states_list.append(new_board_state)\n            # Make the next move\n            game_status,board=game.move(game.turn_monitor,selected_move)\n            if print_progress==True:\n                print(\"Program's Move\")\n                print(board)\n                print(\"\\n\")\n        elif game.game_status()==\"In Progress\" and game.turn_monitor==0:\n            selected_move=opponent_move_selector(game.board,game.turn_monitor,mode=mode)\n        \n            # Make the next move\n            game_status,board=game.move(game.turn_monitor,selected_move)\n            if print_progress==True:\n                print(\"Opponent's Move\")\n                print(board)\n                print(\"\\n\")\n        else:\n            break\n\n    \n    # Correct the scores, assigning 1\/0\/-1 to the winning\/drawn\/losing final board state, \n    # and assigning the other previous board states the score of their next board state\n    new_board_states_list=tuple(new_board_states_list)\n    new_board_states_list=np.vstack(new_board_states_list)\n    if game_status==\"Won\" and (1-game.turn_monitor)==1: \n        corrected_scores_list=shift(scores_list,-1,cval=1.0)\n        result=\"Won\"\n    if game_status==\"Won\" and (1-game.turn_monitor)!=1:\n        corrected_scores_list=shift(scores_list,-1,cval=-1.0)\n        result=\"Lost\"\n    if game_status==\"Drawn\":\n        corrected_scores_list=shift(scores_list,-1,cval=0.0)\n        result=\"Drawn\"\n    if print_progress==True:\n        print(\"Program has \",result)\n        print(\"\\n Correcting the Scores and Updating the model weights:\")\n        print(\"___________________________________________________________________\\n\")\n        \n    x=new_board_states_list\n    y=corrected_scores_list\n    \n    def unison_shuffled_copies(a, b):\n        assert len(a) == len(b)\n        p = np.random.permutation(len(a))\n        return a[p], b[p]\n    \n    # shuffle x and y in unison\n    x,y=unison_shuffled_copies(x,y)\n    x=x.reshape(-1,9) \n    \n    # update the weights of the model, one record at a time\n    model.fit(x,y,epochs=1,batch_size=1,verbose=0)\n    return model,y,result","4bead776":"updated_model,y,result=train(model,mode=\"Hard\",print_progress=True)","998afb4f":"game_counter=1\ndata_for_graph=pd.DataFrame()\n\nmode_list=[\"Easy\",\"Hard\"]\n\nwhile(game_counter<=200000):\n    mode_selected=np.random.choice(mode_list, 1, p=[0.5,0.5])\n    model,y,result=train(model,mode=mode_selected[0],print_progress=False)\n    data_for_graph=data_for_graph.append({\"game_counter\":game_counter,\"result\":result},ignore_index=True)\n    if game_counter % 10000 == 0:\n        print(\"Game#: \",game_counter)\n        print(\"Mode: \",mode_selected[0])\n    game_counter+=1","538c9c5a":"bins = np.arange(1, game_counter\/10000) * 10000\ndata_for_graph['game_counter_bins'] = np.digitize(data_for_graph[\"game_counter\"], bins, right=True)\ncounts = data_for_graph.groupby(['game_counter_bins', 'result']).game_counter.count().unstack()\nax=counts.plot(kind='bar', stacked=True,figsize=(17,5))\nax.set_xlabel(\"Count of Games in Bins of 10,000s\")\nax.set_ylabel(\"Counts of Draws\/Losses\/Wins\")\nax.set_title('Distribution of Results Vs Count of Games Played')","b1b288f5":"model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'","5635ffd9":"## Getting Started","704685ca":"# A Self Learning Tic-Tac-Toe Program\n\n**Note:\nThis kernel is an implementation of chapter 1 of the  book [Machine Learning](http:\/\/www.cs.cmu.edu\/afs\/cs.cmu.edu\/user\/mitchell\/ftp\/mlbook.html)  by [Tom M Mitchell](http:\/\/www.cs.cmu.edu\/~tom\/). In the section **\"*Design of a learning system*\"**, Mitchell describes the set up of a self learning checkers program. This is an attempt at trying to implement the approach for a tic-tac-toe game.**\n\n\n## Objective:\n* \tTo design a program that learns to play tic-tac-toe on its own\n* \tThe program is expected to learn by playing a large number of iterations of the game\n* \tThe program is not provided prior information about the game, the only information supplied to it is whether the game was won\/lost\/drawn\n\n## Methodology:\n\nTo break down the problem into simpler parts, let us define some terminologies. \n\n**The self learning Program**:\n\nThe self learning program is the system that is expected to learn to play tic-tac-toe. It consists of the following components:\n* \t**Legal Moves Generator:**\n    * For a given board state, the Legal Moves Generator outputs the set of all possible legal next board states for a player\n    \n* \t**Evaluator (Neural Network Model):**\n    * For a given board state, the evaluator predicts a score\n    \n*   **Program Move Selector:**\n    * For a given board state, the Program Move Selector selects the next move for the Program using the following approach:\n    \n        *     Use the **Legal Moves Generator** to obtain the set of all possible legal next board states\n        *     For each of the possible legal next board states above, use the **Evaluator** and predict a score\n        *     The possible next board state with the highest predicted score is chosen for the next move\n        \n\n\n**Thus, the task of \"learning\" to play tic-tac-toe, can be reduced to the task of training the Evaluator to assign higher scores to board states favourable to the Program, and lower scores to board states that are unfavourable to the Program**\n\nBelow is a visual representation of the program:\n\n\n![](https:\/\/image.ibb.co\/dMXjKq\/program-diagram-kaggle-v2.png)\n   \n   \n\n\n**Training the Program**\n*         The Program is trained by playing several iterations of games against a coded opponent\n*         **The \"Evaluator\" mentioned above, is a neural network model**\n*         During the first game, the neural network (Evaluator) is initialized with random weights\n*         After the program plays the first game, the scores output by the neural network are corrected as follows:\n    *         If the game was won by the program, assign the last board state a score of 1. Similarly, assign a score of -1 if the game was lost, and 0 if the game was drawn\n    *         For each of the previous board states, assign the score that was predicted to the next board state\n    *         For example, the second last board state is assigned the score that was predicted for the last board state, and so on\n*         Update the weights of the neural network (using SGD) by fitting it to the board states and the corrected scores\n*         Repeat the above process over and over, over a large number of games, updating the neural network after every game\n*         The logic of the approach is that, for a player, if the final board state was a winning state, then the board states leading up to the final state would also be favourable\n*         As the weight update process repeats over games, the neural network learns to assign a higher score to favourable board states, and lower scores to unfavourable board states\n* Below is a diagram of the how the weights of the model would update after each game:\n\n\n![](https:\/\/image.ibb.co\/cf8oUq\/training-kaggle-v3.png)\n  \n### Table of contents\n1. Getting Started\n2. Creating a Tic Tac Toe Game\n3. Testing the Tic Tac Toe game\n4. Creating the legal moves generator\n5. Testing the legal moves generator\n6. Creating the Evaluator (model)\n7. Creating the Program Move Selector\n8. Testing the Program Move Selector\n9. Training the Program\n10. A sample single training game\n11. Training by playing a large number of games\n12. Plotting the # Wins, Draws and Losses with respect to the number of games played above\n13. Saving the model\n14. Evaluating the Program\n15. References","2701d74e":"## Creating the Program Move Selector\n\nThe Program Move Selector selects the next move for a player given the current board state using the following approach:\n* Use the Legal Moves Generator to get all the possible next legal board states\n* Use the Evaluator (model) to score each possible next board state\n* The next board state that is assigned the highest score is selected as the next board state\n\n\n![](https:\/\/image.ibb.co\/dMXjKq\/program-diagram-kaggle-v2.png)","53fd2796":"### Testing the Legal Moves Generator\n\nLet us test the above created Legal Moves Generator.\n\nWe start a dummy game, and do the toss. We then pass the current board state and toss winner to the Legal Moves Generator,\n\nIt returns the dictionary of legal moves that it returns\n\nThe dictionary is of the form \"possible next legal move coordinate\":\"flattened resulting board state\"","1dc18f37":"## Creating a Tic Tac Toe Game\n\nIn order for a program to learn how to play Tic-Tac-Toe, we first need to create the Tic-Tac-Toe game\n\nBelow, we create a simple tic tac toe game using Numpy\n\nBelow are the details of the Numpy Tic Tac Toe Game:\n* The game board is a 3 * 3 numpy array\n* The below is the denotation used:\n    * 2: placeholder\/empty cell\n    * 0: 'O'\n    * 1: 'X'\n* By default, the array is filled with the number 2 as placeholder\n* The number 0 denotes an 'O' and the number 1 denotes  an 'X'\n* Players take turns in choosing the coordinates of the numpy array where they will place 0 or 1 respectively\n* The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game\n* For simplicity, the program always plays with the mark 1 and the opponent plays with 0\n\nTo get you used to the numpy tic tac toe board, below is a diagram\n![](https:\/\/image.ibb.co\/dDFa20\/equivalent-kaggle.png)","4b43c1b2":"## Training the Program\n\nTraining the progam is essentially training the Evaluator(model) to assign higher scores to board states favourable to the program, and lower scores to board states unfavourable to the program.\n\nThe progam is expected to learn by playing games against the Opponent.\n\nThe Evaluator (model) is trained in the following way:\n* At first the weights of the model are randomly assigned\n* Using this model, the Move Selector function selects the next move to make for the program\n* The program and the opponent make thier moves until the game ends\n* After the game is completed, the scores are corrected as follows:\n    * If the game was Won by the program, assign the score 1 to the final board state, or assign -1 if lost or 0 if drawn\n    * Shift the remaining scores so that they become the scores of the previous board states\n    * Update the weights of the model using the board state and corrected score\n* The above process is repeated over 100s of thousands of games\n* It is expected that eventually, the Evaluator (model) learns to assign higher scores to more favourable board states and lower scores to less favourable board states\n\n\n\n![](https:\/\/image.ibb.co\/cf8oUq\/training-kaggle-v3.png)\n\n\n\n#### An intuitive explanation of the above logic:\n\nhttps:\/\/cs.stackexchange.com\/questions\/3296\/why-does-the-experience-propagation-rule-for-checkers-work-in-tom-mitchells-boo\n\n* Remember that the only information provided to the model is whether the game was won, lost or drawn\n* If a game was won by the Program, it means that the final board state was a winning board state. Furthermore, it means that the second last board state, which was only 1 move away from winning, was a very favourable board state for the Program. The same goes for the third last, fourth last and so on\n* At the end of each game, depending on whether the Progam won\/lost\/drawn, the score for the final board state is corrected by setting it to 1 or -1 or 0 respectively. The scores for the other board states are corrected by assigning the score predicted for the successor board state. The model weights are then updated by fitting it to the board state and corrected score\n* Over several iterations of the games, as the experience propogrates backwards, it is expected that the model learns to predict higher scores board states favourable to the Program, and lower scores to board states unfavourable to the Program\n\nMitchell states in his book that this approach has been found to be surprisingly effective for the checkers player, and has been known to converge to the target function well.","b27d409e":"## References\n\nTom M. Mitchell (1997). Machine Learning  (2013 ed.). McGraw-Hill Education ([link](http:\/\/www.cs.cmu.edu\/afs\/cs.cmu.edu\/user\/mitchell\/ftp\/mlbook.html))","b7253205":"### Plotting the number of Wins, Draws and Losses Vs count of games played","d7128633":"### Testing the Program Move Selector \n\nLet us play a dummy game and use the above created Program Move Selector function to select the next move","6033f02c":"### Testing the Tic Tac Toe game\n\nTo simply test the tic-tac-toe game created above, let us play a dummy game. We make the moves for both the players.","d8eebc13":"### Saving the model","619ea540":"Please note that since the Evaluator (model) has not yet been trained and has been assigned random weights, hence the score above is a random number","b0c09224":"## Creating the Legal Moves Generator\n\nThe legal moves genator accepts the board state and player as input, and provides the set of all possible legal moves as output.\n\nBasically, for a given board state and player, it tells us where all the player can place his mark legally (the player can only place the mark where there is no opponent's mark)","c17021b3":"You can observe above that the Legal Moves Generator returns a dictionary with all the possible legal coordinates and their resulting flattened numpy board states","6f995eee":"### A sample single training game\n\nNote that the program will always play the mark '1' and the opponent will always play mark '0'.\n\nbelow, we run a single iteration of training, or in other words, a single game of tic-tac-toe between the program and opponent.","2e09f8f5":"## Creating the opponent (The Opponent Move Selector)\n\nFor our program to self learn, it needs to play several games of tic-tac-toe.\n\nBut with who will it play with?\n\nIn the book \"[Machine Learning](http:\/\/www.cs.cmu.edu\/afs\/cs.cmu.edu\/user\/mitchell\/ftp\/mlbook.html)\", Mitchell describes the self learning checkers program learning by playing games against itself.\n\nIn older versions of this kernel, I did attempt to set it up so that the program was used to make the moves for both players.\n\nHowever, the problem is that Tic-Tac-Toe is a simple game, and after about 10-15 games, I would always find that the program and the opponent program fell into a sync and played the same game (making the same moves) over and over, since they were using the same model.\n\nMaybe this would work better for a more complex game like checkers.\n\nHence in this case, the set up is that the program learns by playing games against a coded opponent. \n\nThe opponent is coded to do the following:\n* **If playing in \"Easy\" mode:**\n    * When it is your turn, use the Legal Moves Generator to get all possible legal next moves\n    * Pick a legal move at random\n    * Repeat above until the end of the game\n  \n* **If playing in \"Hard\" mode:**\n    * When it is your turn, use the Legal Moves Generator to get all possible legal next moves\n    * Of the above legal moves, if any legal move makes you win (along row\/column\/diagonal), pick that legal move\n    * Else, if any legal move prevents your opponent (program) from winning when it is one move away from a win, pick that move\n    * Else, if any legal move leads to a row\/diagonal\/column having two 0's and no 1's (program's mark) , pick that legal move\n    * If none of the above conditions exist, pick a legal move at random\n\n\nThe Program is trained by repeatedly playing games against the above opponent and updating the weights of the model at the end of each game.\n\nEach game is played at either \"Easy\" or \"Hard\" mode, which is picked at random at the start of each game.\n\nThe \"Easy\" mode helps the Program explore the tic-tac-toe space. The \"Hard\" mode helps the Program learn how to play with a real opponent. \n\nI arrived at the above approach for the opponent after different iterations in previous versions of this kernel , and can be further improved, I welcome suggestions with regards to that.","b94b2483":"## Training by playing a large number of games\n\nWe play 200,000 games of tic-tac-toe between the program and the opponent. We record the result of each game and plot the distribution of wins\/losses\/draws by the program over fixed intervals of games","1ed4345e":"## Creating the Evaluator (Neural Network Model)\n\nAs defined previously, the Evaluator function maps board states to scores. \n\n\n![](https:\/\/i.ibb.co\/MGbnKxd\/evaluator.png)\n\n\nIt is expected that after training, the Evaluator learns to assign higher scores to board states that are favourable to the program.\n\nIn our kernel, we use a neural network as an Evaluator Function. \n\n**Do note that, in the book Machine Learning by Tom M Mitchell, he describes using a linear model**. I used a neural network here. I plan on replacing this with a linear model to check if I can get similar results in future versions of the kernel.\n\nBelow, we create the neural network. The weights of the neural network are intialized randomly. \n\nThe architecture\/hyperparameters of the neural network were picked through trial and error, and can be optimized further.\n\nThe neural network takes the flattened tic-tac-toe board numpy array as input and outputs a score.","1f8d5006":"#### Observations:\n* It can be observed in the above graph, that as the Program played more and more games against the opponent, its win rate increased and its loss rate reduced\n* Due to the noisy nature of the training, I would not be able to provide more specific observations, since the above graph is going to look different each time this kernel is run"}}