{"cell_type":{"fd0f6fd7":"code","b442993e":"code","777d37ce":"code","d0a0b7ab":"code","835b365f":"code","c9c606c6":"code","eb437d27":"code","173bb05a":"code","83df7249":"code","5c372d73":"code","db690860":"code","48554d91":"markdown","460c7b98":"markdown","dabde8f4":"markdown","d8b59440":"markdown","9e58a262":"markdown","8bd4afd7":"markdown","a9cc9572":"markdown","ad5012f1":"markdown"},"source":{"fd0f6fd7":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport datetime\nimport gc\nimport os\nfrom scipy.stats import spearmanr, pearsonr\nfrom math import floor, ceil\n\nnp.set_printoptions(suppress=True)","b442993e":"PATH = '..\/input\/google-quest-challenge\/'\nBERT_PATH = '..\/input\/bert-base-from-tfhub\/bert_en_uncased_L-12_H-768_A-12'\ntokenizer = tokenization.FullTokenizer(BERT_PATH+'\/assets\/vocab.txt', True)\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_sub.columns[1:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","777d37ce":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows(), total = len(df[columns])):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","d0a0b7ab":"def rho(y_true, y_pred): \n    rhos = tf.constant(0, dtype='float32') \n    for ind in range(30): \n        a = tf.slice(y_true, [0, ind], [-1, 1]) \n        a = tf.reshape(a, [-1]) \n        b = tf.slice(y_pred, [0, ind], [-1, 1]) \n        b = tf.reshape(b, [-1]) \n        rhos = tf.cond(tf.equal(tf.argmax(a), tf.argmin(a)), \n                       lambda: tf.add(rhos, tf.metrics.binary_crossentropy(a, b)), \n                       lambda: tf.add(rhos, tf.py_function(spearmanr, [a, b], Tout=tf.float32))) \n    return tf.divide(rhos, tf.constant(30, 'float32'))","835b365f":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, train_data, valid_data, test_data, batch_size=16, fold=None):\n        \n        self.train_inputs = train_data[0] #\n        self.train_outputs = train_data[1] #\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n    \n    def on_train_begin(self, logs={}):\n        self.train_predictions = [] #\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.train_predictions.append(self.model.predict(self.train_inputs, batch_size=self.batch_size)) #\n        rho_train = compute_spearmanr(self.train_outputs, np.average(self.train_predictions, axis=0)) #\n        \n        self.valid_predictions.append(self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        rho_val = compute_spearmanr(self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        print(f\"\\ntrain rho: %.4f, validation rho: %.4f\" % (rho_train, rho_val))\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size))\n\ndef bert_model(output_dim):\n    \n    input_word_ids = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n    \n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n    \n    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(output_dim, activation=\"sigmoid\", name=\"dense_output\")(x)\n\n    model = tf.keras.models.Model(inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n    \n    return model    \n        \ndef train_and_predict(model, train_data, valid_data, test_data, \n                      learning_rate, epochs, batch_size, loss_function, fold):\n        \n    custom_callback = CustomCallback(train_data=(train_data[0], train_data[1]), #\n                                     valid_data=(valid_data[0], valid_data[1]), \n                                     test_data=test_data,\n                                     batch_size=batch_size,\n                                     fold=None)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(loss=loss_function, optimizer=optimizer, metrics=[rho])\n    history = model.fit(x=train_data[0], y=train_data[1], epochs=epochs, \n                        batch_size=batch_size, validation_data=(valid_data[0], valid_data[1]), callbacks=[custom_callback])\n    \n    return custom_callback, history","c9c606c6":"MAX_SEQUENCE_LENGTH = 512\ncv = 5\n\ngkf = GroupKFold(n_splits=cv).split(X=df_train.question_body, groups=df_train.question_body) \n\noutputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","eb437d27":"epochs = 6\nbatch_size = 6\ncomp_folds = cv+1\n\ncustom_callback_histories = []\nhistories = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    # will actually only do 3 folds (out of 5) to manage < 2h\n    if fold < comp_folds:\n        K.clear_session()\n        model = bert_model(outputs.shape[1])\n\n        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n        train_outputs = outputs[train_idx]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n\n        # custom_callback_history contains two lists of valid and test preds respectively:\n        # [valid_predictions_{fold}, test_predictions_{fold}]\n        custom_callback_history, history = train_and_predict(model, \n                                                             train_data=(train_inputs, train_outputs), \n                                                             valid_data=(valid_inputs, valid_outputs),\n                                                             test_data=test_inputs,\n                                                             learning_rate=3e-5, epochs=epochs, batch_size=batch_size,\n                                                             loss_function='binary_crossentropy', fold=fold)\n        \n        histories.append(history)\n        custom_callback_histories.append(custom_callback_history)","173bb05a":"def metricsplot(histories, metric):\n    fig = plt.figure(figsize=(7*2, 4*ceil(len(histories)\/2)))\n    fig.set_facecolor(\"#F3F3F3\")\n    for n in range(len(histories)):\n        qx = plt.subplot(ceil(len(histories)\/2), 2, n+1)\n        plt.plot(list(range(1, len(histories[n].history[metric])+1)), histories[n].history[metric], 'b', linestyle = \"dotted\", linewidth = 2, \n                 label='Training '+metric)\n        plt.plot(list(range(1, len(histories[n].history['val_'+metric])+1)), histories[n].history['val_'+metric], 'b', linewidth = 2, label='Validation '+metric)\n        plt.legend(prop = {\"size\" : 12})\n        plt.grid(True, alpha = .15)\n        plt.title('Training and validation '+ metric + ' on fold_{%d}' %(n))\n        plt.xlabel('Epochs')\n        plt.ylabel(metric.title())\n        plt.xticks(list(range(1, len(histories[n].history[metric])+1)))","83df7249":"metricsplot(histories, 'loss')","5c372d73":"metricsplot(histories, 'rho')","db690860":"test_predictions = [custom_callback_histories[i].test_predictions for i in range(len(custom_callback_histories))]\ntest_predictions = [np.average(test_predictions[i], axis=0) for i in range(len(test_predictions))]\ntest_predictions = np.mean(test_predictions, axis=0)\n\ndf_sub.iloc[:, 1:] = test_predictions\n\ndf_sub.to_csv('submission.csv', index=False)","48554d91":"# Acknowledgements\n\nOriginal kernels: \n* https:\/\/www.kaggle.com\/akensert\/bert-base-tf2-0-minimalistic \n* https:\/\/www.kaggle.com\/khoongweihao\/bert-base-tf2-0-minimalistic-iii","460c7b98":"#### 3. Create model\n\n`CustomCallback()` is a class which inherits from `tf.keras.callbacks.Callback` and will compute and append validation score and validation\/test predictions respectively, after each epoch.\n<br><br>\n`bert_model()` contains the actual architecture that will be used to finetune BERT to our dataset. It's simple, just taking the sequence_output of the bert_layer and pass it to an AveragePooling layer and finally to an output layer of 30 units (30 classes that we have to predict)\n<br><br>\n`train_and_predict()` this function will be run to train and obtain predictions\n\n`compute_spearmanr()` is used to compute the competition metric for the training and validation sets. However, the main function to compute the competition metric will be the `rho()`.  <br><br>\n\nThe following `rho()` metric function uses the spearman correlation and the binary crossentropy. The final metric is the average of all the targets. The basic idea is, if the target has only one unique value, binary crosessentropy will be used (to avoid `nan` in spearman), otherwise, spearman will be used. <br><br>","dabde8f4":"#### 2. Preprocessing functions\n\nThese are some functions that will be used to preprocess the raw text data into useable Bert inputs.","d8b59440":"### Bert-base TensorFlow 2.0\n\nThis kernel does not explore the data. For that you could check out some of the great EDA kernels: [introduction](https:\/\/www.kaggle.com\/corochann\/google-quest-first-data-introduction), [getting started](https:\/\/www.kaggle.com\/phoenix9032\/get-started-with-your-questions-eda-model-nn) & [another getting started](https:\/\/www.kaggle.com\/hamditarek\/get-started-with-nlp-lda-lsa). This kernel is an example of a TensorFlow 2.0 Bert-base implementation, using TensorFow Hub. <br><br>\n\nIn this kernel, I added a metric function `rho()` which calculates the competition metric (Spearman's correlation coefficient) in order to see the behavior of the model over the epochs.\n\nThe objective is to plot and compare the validation metrics to the training metrics.\n* If both metrics are moving in the same direction, everything is fine.\n* If the validation metric begins to stagnate while the training metric continues to improve, you are probably close to overfitting.\n* If the validation metric is going in the wrong direction, the model is clearly overfitting.","9e58a262":"#### 6. Process and submit test predictions\n\nFirst the test predictions are read from the list of lists of `custom_callback_histories`. Then each test prediction list (in lists) is averaged. Then a mean of the averages is computed to get a single prediction for each data point. Finally, this is saved to `submission.csv`","8bd4afd7":"#### 5. Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for the number of epochs --- with a learning rate of 1e-5 and a batch_size. A simple binary crossentropy is used as the objective-\/loss-function. ","a9cc9572":"#### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)","ad5012f1":"#### 4. Obtain inputs and targets, as well as the indices of the train\/validation splits"}}