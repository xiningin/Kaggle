{"cell_type":{"bd12ac2e":"code","f6cce774":"code","62a67d0e":"code","46a18210":"code","c81c1cba":"code","398a4d33":"code","e285b6c9":"code","3e0f39fb":"code","1973901c":"code","ed7c2a45":"code","b75c1992":"code","e7ae98f0":"markdown","5bfba11f":"markdown","5b2b863a":"markdown","3a2534ba":"markdown","0bdb492a":"markdown","f6a6e9dd":"markdown","93fb8186":"markdown","8c9af488":"markdown","4a352bc4":"markdown"},"source":{"bd12ac2e":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap","f6cce774":"dfTrain = pd.read_csv('..\/input\/ClassificationDataTraining.csv')   #Training Dataset\ndfTrain.head()","62a67d0e":"DistinctClasses=np.array(dfTrain['Y'].unique())\nprint(DistinctClasses)","46a18210":"pd.plotting.scatter_matrix(dfTrain, alpha=1, diagonal='kde',color='r')\nplt.show()","c81c1cba":"cmap = ListedColormap(['blue', 'red']) \n\n\n\nplt.scatter(dfTrain.loc[:,['X1']].values,dfTrain.loc[:,['X2']].values, c=dfTrain.loc[:,['Y']].values, cmap=cmap)\nplt.show()","398a4d33":"def extractFeatures(df):\n    df_Features=df.iloc[:,0:2]\n    df_Label=df.iloc[:,2:3]\n    X=df_Features.values\n    Y=df_Label.values\n    return X,Y","e285b6c9":"X,Y=extractFeatures(dfTrain)","3e0f39fb":"def predict(inputX,trainX,trainY,K):\n    trainBatchSize=trainX.shape[0]\n    predictBatchSize=inputX.shape[0]\n    pY=np.zeros((inputX.shape[0],1))\n    distanceList=np.zeros(trainY.shape)\n    for i in range(predictBatchSize): \n        \n        #Step1:Calculate Distances\n        distanceValues=np.linalg.norm(inputX[i,:]-trainX,axis=1)\n        distanceList=np.column_stack((distanceValues,trainY))\n          \n        #Step2: Sort Distances\n        sortedList=distanceList[distanceList[:,0].argsort()]\n        \n       \n        #Step3: Pick top K\n        topKList=sortedList[:K,:]\n                \n        \n        \n        #Step4: GetMost voted class top K    \n        DistinctClassesWithCount=np.column_stack((DistinctClasses,np.zeros(DistinctClasses.shape)))\n        \n        for cls in DistinctClasses:           \n            DistinctClassesWithCount[cls,1]= len(topKList[np.where(topKList[:,1]==cls)])\n            \n    \n        \n        mostVoted=np.argmax(np.max(DistinctClassesWithCount, axis=1))\n        \n        pY[i]=mostVoted\n            \n                    \n    \n    return pY","1973901c":"def accurracy(Y1,Y2):\n    m=np.mean(np.where(Y1==Y2,1,0))    \n    return m*100","ed7c2a45":"K=25\npY=predict(X,X,Y,K) \nprint(accurracy(Y, pY))","b75c1992":"plt.scatter(X[:,0],X[:,1], c=Y[:,0], cmap=cmap) \n###########################################################################\n#Predict for each X1 and X2 in Grid \nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nu = np.linspace(x_min, x_max, 20) \nv = np.linspace(y_min, y_max, 20) \n\nU,V=np.meshgrid(u,v)\nUV=np.column_stack((U.flatten(),V.flatten())) \nW=predict(UV, X,Y,K) \nplt.scatter(U.flatten(), V.flatten(),  c=W.flatten(), cmap=cmap,marker='.', alpha=0.1)\n\n###########################################################################\n#Exact Decision Boundry can be plot with contour\nz = np.zeros(( len(u), len(v) )) \nfor i in range(len(u)): \n    for j in range(len(v)): \n        uv= np.column_stack((np.array([[u[i]]]),np.array([[v[j]]])))               \n        z[i,j] =predict(uv, X,Y,K) \nz = np.transpose(z) \nplt.contour(u, v, z)\n###########################################################################\nplt.show()","e7ae98f0":"<H1>Read Data from CSV","5bfba11f":"<h5>Accurracy on Training Data","5b2b863a":"<h2>KNN Algorithm <\/h2>\n<p>Nearest neighbors is a simple algorithm predict the numerical target based on similarity using distance functions. It is also called a non-parametric technique.\n\n<p>An implementation of KNN regression is to calculate the average of the numerical target of the K nearest neighbors.  Alternativly we may use an inverse distance weighted average of the K nearest neighbors. KNN regression uses the same distance functions as KNN classification.\n\n<P><h2>Distance Functions (Continuous Data) <\/h2>\n    <p>Ecludian Distance $=  \\displaystyle \\sqrt{\\sum _{i=1}^n (X_i- Y_i)^2}$\n      <p>Manhattan Distance$ =  \\displaystyle \\sum _{i=1}^n \\left|X_i- Y_i\\right|$\n  <p>Minikowski Distance$ =  \\displaystyle \\left[\\sum _{i=1}^n \\left|X_i- Y_i\\right|^q\\right]^{\\frac{-1}{q}}$\n  \n  <P><h2>Distance Function (Categorical Data) <\/h2>\n     <p>Hamming Distance$ =  \\displaystyle \\sum _{i=1}^n \\left|X_i- Y_i\\right|$ \n       <p>   $\\hspace{20mm}X = Y \\hspace{10mm} then \\hspace{10mm}  D = 0 $\n      <p>    $\\hspace{20mm}X \\neq Y \\hspace{10mm} then \\hspace{10mm} D = 1$\n  \n  <p><h2>Standardize\/ Normalize Data<\/h2>\nCalculating distance measures directly from the training set values could be biased in the case where variables have different measurement scales or there is a mixture of numerical and categorical variables. For example, if one variable is based on annual income in dollars, and the other is based on age in years then income will have a much higher influence on the distance calculated. One solution is to Normalize the training set using Min-Max or Mean-Std.Dev method.\n \n  <p> Standardization\n <p>$X =  \\displaystyle \\frac{X-X_{min}}{X_{max}-X_{min}}$\n  <p> Normalization\n <p>$X =  \\displaystyle \\frac{X-\\mu}{\\sigma}$   where  $\\mu$ is mean and $\\sigma$ is standard deviation\n  \n<P><h2>Pesudo Code<\/h2>\n\n<p>Step 1: To classify a new data point $(x_a,x_b)$,\n     calculate distance from each training(given) data point $(x_1^i,x_2^i)$\n        \nStep 2: Sort the distances in ascending order.\n    \nStep 3: Choose top K data points\n    \nStep 4: Pick the Class  having most items in Top K data point chosen \n\nStep 5: Predict data point $(x_a,x_b)$ with highest voting class\n    ","3a2534ba":"<h1>KNN Classification <\/h1>","0bdb492a":"<h3>Predict using K-Neighbors","f6a6e9dd":"<h2>Extract Input Feature to <b>X <\/b>and Label to <b>Y<\/b><\/h2>\n<h5>X=(X1 &amp; X2 in DS) and Y(Class in DS)   ","93fb8186":"<h1> Prediction\/Accuracy Evaluation","8c9af488":"<h5> Visualize Data","4a352bc4":"<h1>Plotting Hypothesis"}}