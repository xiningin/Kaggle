{"cell_type":{"00281c32":"code","6f1a2479":"code","3b828e79":"code","fe4021f2":"code","4c5f044e":"code","3beb9658":"code","a903c3f9":"code","adbe175e":"code","e939359b":"code","5b250932":"code","d512eacd":"code","63703cfb":"code","3223e2c1":"code","1fbbeb30":"code","1fb2e9ea":"code","072e2955":"code","9e1d3dd5":"code","9bc8508c":"code","e84c8845":"code","bfed08c2":"code","fd9def3d":"code","a9c803cd":"code","588182fd":"code","d6b50b90":"code","b012c01c":"code","9117688c":"code","ae1eb1c0":"code","2c8cec4a":"code","47272f9e":"code","43e4d800":"code","733da0cf":"code","d2abda6e":"code","4c4f689c":"markdown"},"source":{"00281c32":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f1a2479":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","3b828e79":"def reduce_mem_usage(df):\n    #iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    #create a dataframe and optimize its memory usage\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","fe4021f2":"train_fp = '..\/input\/tabular-playground-series-oct-2021\/train.csv'\ntrain_df = import_data(train_fp)\n\ntest_fp = '..\/input\/tabular-playground-series-oct-2021\/test.csv'\ntest_df = import_data(test_fp)\n","4c5f044e":"train_df","3beb9658":"train_df.info()","a903c3f9":"train_df.dtypes","adbe175e":"print(train_df.isnull().values.any())\nprint(' ')\nprint(train_df.isnull().sum())","e939359b":"train = train_df.drop(['id'], axis=1)","5b250932":"test_df","d512eacd":"print(test_df.isnull().values.any())\nprint(' ')\nprint(test_df.isnull().sum())","63703cfb":"test_df = test_df.drop(['id'], axis=1)","3223e2c1":"train_corr = train_df.corr()\nmask = np.triu(np.ones_like(train_corr, dtype=np.bool))\n\nfig = plt.figure(figsize=(16,13))\nsns.heatmap(train_corr, mask=mask)\nplt.title('Correlation between features')","1fbbeb30":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\ntrain_copy = train.copy()\n\nX = train_copy.drop(['target'], axis=1)\ny = train_copy.target\n","1fb2e9ea":"X","072e2955":"#scaler = MinMaxScaler()\n\ntest = test_df.copy()\n\n#X_scaled = scaler.fit_transform(X)","9e1d3dd5":"#X_scaled = pd.DataFrame(X_scaled)","9bc8508c":"#X_scaled","e84c8845":"X['std'] = X.std(axis=1)\nX['min'] = X.min(axis=1)\nX['max'] = X.max(axis=1)\n\ntest['std'] = test.std(axis=1)\ntest['min'] = test.min(axis=1)\ntest['max'] = test.max(axis=1)","bfed08c2":"display(X)","fd9def3d":"display(test)","a9c803cd":"'''\nimport optuna\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import log_loss, accuracy_score, mean_absolute_error, r2_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n\ndef objective(trial):\n    \n    xtrain,xvalid,ytrain,yvalid = train_test_split(X,y,test_size=0.2,random_state=0)\n    \n    learning_rate = trial.suggest_loguniform('learning_rate', 0.00001, 10.0)\n    max_depth = trial.suggest_int('max_depth', 4, 10)\n    l1_reg = trial.suggest_loguniform('l1_reg', 0.00001, 10.0)\n    l2_reg = trial.suggest_loguniform('l2_reg', 0.00001, 10.0)\n    \n    params = {    \n        'learning_rate': learning_rate,\n        'max_depth': max_depth,\n        'l1_reg': l1_reg,\n        'l2_reg': l2_reg\n    }\n    \n    model = XGBRegressor(**params,\n                        tree_method='gpu_hist',\n                        gpu_id=0,\n                        predictor='gpu_predictor'\n                        )\n\n    model.fit(\n        xtrain, \n        ytrain,\n        early_stopping_rounds=10,\n        eval_set=[(xvalid, yvalid)], \n        verbose=False\n    )\n\n    yhat = model.predict(xvalid)\n    return roc_auc_score(yvalid, yhat)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=100)\nprint(study.best_params)\n'''","588182fd":"import optuna\nfrom optuna.samplers import TPESampler\nfrom hyperopt import STATUS_OK,Trials,fmin,hp,tpe\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import log_loss, accuracy_score, mean_absolute_error, r2_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom optuna.integration import XGBoostPruningCallback\n\ndef objective(trial):\n    xtrain,xvalid,ytrain,yvalid = train_test_split(X,y,test_size=0.2,random_state=0)\n    \n    params = {\n        'max_depth': trial.suggest_int('max_depth', 6, 10),\n        #'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400), \n        'eta': trial.suggest_float('eta', 0.007, 0.013), \n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), \n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4), \n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n        'eval_metric' : 'auc',\n        'objective' : 'binary:logistic',\n        'gpu_id': 0,\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor'\n            }\n    \n    model = XGBClassifier(**params,\n                         use_label_encoder=False)\n\n    model.fit(\n        xtrain, \n        ytrain,\n        early_stopping_rounds=100,\n        eval_set=[(xvalid, yvalid)], \n        verbose=False\n    )\n\n    yhat = model.predict(xvalid)\n    return roc_auc_score(yvalid, yhat)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=100)\nprint(study.best_params)","d6b50b90":"print(\"Best value (RMSE): {:.5f}\".format(study.best_value))\nprint(\"Best params:\")\n\nfor key, value in study.best_params.items():\n    print(\"{}: {}\".format(key, value))","b012c01c":"xgb_params = study.best_params\nxgb_params","9117688c":"xtrain,xvalid,ytrain,yvalid = train_test_split(X,y,test_size=0.2,random_state=0)","ae1eb1c0":"xgb = XGBClassifier(**xgb_params,\n                    learning_rate=0.02,\n          tree_method='gpu_hist', \n          predictor='gpu_predictor', \n          eval_metric='auc',\n          gpu_id=0,\n         random_state=0,\n                   use_label_encoder=False)\n\nxgb.fit(\n    xtrain, \n    ytrain,\n    early_stopping_rounds=100,\n    eval_set=[(xvalid, yvalid)], \n    verbose=False\n    )","2c8cec4a":"predictions = xgb.predict(test)","47272f9e":"final_output = np.mean(np.column_stack(predictions), axis=0)","43e4d800":"sample_fp = '..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv'\n\nsample = pd.read_csv(sample_fp)","733da0cf":"submission = sample.copy()\n\nsubmission['target'] = final_output\n\nsubmission\n","d2abda6e":"submission.to_csv('submission.csv', index=False)","4c4f689c":"<center><h1>Tabular Playground - October 2021<\/h1><center>\n    <center><h2>EDA, Optuna\/XGB Implementation<\/h2><center>\n        <center><h3>By Tariq Hussain<\/center><\/h3>"}}