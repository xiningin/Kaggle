{"cell_type":{"90418d83":"code","0212ffd5":"code","c0c79403":"code","9a01ab9b":"code","5c5004ed":"code","dc61f1f9":"code","c449b3ce":"code","2d6ecdc2":"code","934479c0":"code","a7535312":"code","924a285c":"code","7df1b1ac":"code","0466297d":"code","303f0083":"code","5e093018":"code","f66924ec":"code","e6860718":"code","86730db9":"code","37902016":"code","332f3996":"code","93003b80":"code","47bf7a85":"code","02b67e0c":"code","7a0e0982":"code","5118a000":"code","ca1ab97b":"code","44a82d1a":"code","8508a4c1":"code","c13f6f50":"code","a4eacf00":"code","e155052f":"code","a3f538aa":"code","19d7d5a2":"code","d1068a80":"code","d3062f37":"code","7d96ae9a":"code","d62feca2":"code","d3bfb89b":"code","fc06effe":"code","f922b1fa":"code","2e638614":"code","8283093a":"code","de19b9e2":"code","22b046d5":"code","c17debf1":"code","52f7ddfd":"code","b7cecbc1":"code","3120ae45":"code","43f1bac6":"code","0c73f123":"code","a219a912":"code","bcd31c35":"code","a7da1db5":"code","6ce2d6aa":"code","cf773774":"code","39832259":"code","35c438a9":"code","1258f14e":"code","0bfbd5a0":"code","735f1de3":"code","ee2c89e4":"code","0f43a779":"code","b6a4ee5a":"code","b54ec1ac":"code","e86199ea":"code","8a0dd829":"code","e7b158a5":"code","606272f8":"code","64ea06fb":"code","c38bd466":"code","ce5c2abe":"code","e6fe4be3":"code","71db559f":"code","1474ee52":"code","debedcfc":"code","eaf8fd36":"code","57196051":"code","bdcf7702":"code","23b057a6":"code","96f54390":"code","4d37623f":"code","6bc091a9":"code","46b0e073":"code","1876cc8b":"code","40eda4a9":"code","15fe7e95":"code","f1a478da":"code","e3ca5118":"code","cbf5a07a":"code","01756a10":"code","89201c13":"code","e0b7cdd1":"code","69815677":"code","029a3ab8":"code","79460ef1":"code","3fa2734d":"code","b40a84ee":"code","095274ca":"code","a0a22a7a":"code","d093c3d0":"code","75253786":"code","d8422ce9":"code","898ea7c1":"code","2043a43d":"code","a0b8d8c0":"code","a8502375":"code","fad8fb29":"code","db5fee23":"code","36decc2f":"code","53dccd94":"code","8184b6f3":"code","c0cf2c82":"code","96423d05":"code","48f35ea1":"code","f08e4652":"code","073faf64":"code","af713309":"code","aee35889":"code","a3b97ba6":"code","5a9780fb":"code","15651b8e":"code","764dbd1e":"code","b45879f2":"code","b3ffb1b2":"code","924a5d8d":"code","190832ae":"code","f7a1d3c8":"code","6da75204":"code","c3c5279b":"code","db731370":"code","d7c7ed24":"code","ddf16be3":"code","605fb39c":"code","cef1db40":"code","4d00dd83":"code","b9cf43fd":"code","b46836b6":"code","20e0efed":"code","84dacd9a":"code","bed77cb0":"code","d17da606":"code","fcb0e0b9":"code","9a52eb52":"code","2b4dafc2":"code","26e2573c":"code","6588d722":"code","b078c6ea":"code","a8326012":"code","2eb08f1e":"code","640da797":"code","1ee70f92":"code","6f1d22a6":"code","5fbafa7b":"code","089932b9":"code","6fa4c31a":"code","3f4c107c":"code","77a05a3d":"code","802f50ed":"code","49e9a41a":"code","98baa774":"code","d03d28eb":"code","6fcebd52":"code","213f4460":"code","73f1fc53":"code","aae53bca":"code","f8cb7493":"code","527309af":"code","ba93136b":"code","0787753d":"code","fc803da6":"code","d29828a6":"code","45653ea5":"code","b3d16e12":"code","d0669113":"code","c4f05531":"code","0f5824a4":"code","49136b4a":"code","f3b7f87d":"code","1387e961":"code","344b5c5b":"code","8905aef2":"code","5182c4db":"code","8eaed6a4":"code","cb6ba844":"code","da1c8b14":"code","58194845":"code","8530980a":"code","73cc6113":"code","251b7ac4":"code","6c1f7616":"code","7e19db45":"code","84ed73c5":"code","3c2b1201":"code","5e14dec7":"code","dc7261ae":"code","4378e74c":"code","869efc09":"code","d6a0b34d":"code","e086d0b9":"code","374eaad2":"code","656e134c":"code","0a71ab7f":"code","2c8ace66":"code","d629bd38":"code","fbb3aa70":"code","77bbb31e":"code","81d56edf":"code","d6b019b8":"code","67b182e5":"code","e3ef4671":"code","8dcfa5ae":"code","637334d8":"code","bd4110e6":"code","47f687a7":"code","697ca832":"code","955d0c48":"code","5b247ffd":"code","0dc09a84":"code","78c60b1f":"code","a2dcb291":"code","2ea7d385":"code","0486c165":"code","05236d0a":"code","4d743da1":"code","1cca9975":"code","a1e028e4":"code","7e2cd6b8":"code","7e685332":"code","ddf8e2cd":"code","43190ca5":"code","ebb1822f":"code","e52e1508":"code","2f5e6cc4":"code","0455da2f":"code","206d6461":"code","cdd78b35":"code","2e46f395":"code","618f4c03":"code","5fd19914":"code","9acd82ec":"code","5b99069b":"code","0b306a0a":"code","14e1b6c7":"code","62d5da40":"code","636edb81":"code","a5d7f8ab":"code","6709e2d9":"code","de35795c":"code","441f1a00":"code","4fc186c0":"code","2b2b1e20":"code","3ff9a91a":"code","868569c6":"code","282df798":"code","a66638ec":"code","2aac53b4":"code","79f84874":"code","eb697d9b":"code","1092663f":"code","b0712c0a":"code","5e7aa026":"code","69aae970":"code","9105a791":"code","cbd669e8":"code","57c5f1e6":"code","cef499c2":"code","10ea292a":"code","a0b24d42":"code","7ee5a458":"code","cda9150d":"code","1bd03410":"code","54843cfc":"code","7f05fede":"code","261f0438":"code","c224ee6e":"code","70451568":"code","0b5825dd":"code","c37b10f6":"code","7772d5be":"code","08ac83cf":"code","dc1be353":"code","f6c6c644":"code","b9eb9308":"code","a6d4c661":"code","2c21bb24":"code","4e550e31":"code","b840c43b":"code","30a68541":"code","e3a21a0d":"code","c5a36925":"code","e9ff149f":"code","f9b84c65":"code","e6430f17":"code","c8873c9c":"code","3eca39f7":"code","1052f105":"code","eb61c4b4":"code","961bad71":"code","b8c7b681":"code","e4d452e3":"code","db7aa4cf":"code","03b78dcb":"code","43d6f368":"code","5a5c58d2":"code","90b3fcdc":"code","d4c36b85":"code","1f6ecb98":"code","bb7a788a":"code","beaf6740":"code","c1c6f56e":"code","cfe07efc":"code","13b95b64":"code","a407bfc5":"code","5d6316e7":"code","cbffdc81":"code","5207f123":"code","184948f1":"code","67dedb65":"code","00972bfe":"code","9bcd21f7":"code","1af39465":"code","32910275":"code","46129fa4":"code","2baac3ed":"code","9cc47c77":"code","1fbe0385":"code","391ee7f7":"code","0b72dbe0":"code","3b99ec3b":"code","a8b9d4e8":"code","4cc38c39":"code","1a825f4a":"code","65fb2529":"code","635cf6b1":"code","0cf11861":"code","ccd87e40":"code","b11fb7f3":"code","a48db24a":"code","8248b15b":"code","02994ef1":"code","29dec413":"code","6b0234a5":"code","033c7724":"code","7e47df40":"code","3658640f":"code","9abcec16":"code","0f209fe4":"code","f76dab54":"code","dc3be5ab":"code","13fb2365":"code","357f0570":"code","cb99c413":"code","9160487f":"markdown","4a9b39fc":"markdown","e26cd289":"markdown","28963d26":"markdown","91d17031":"markdown","7bc45884":"markdown","f683c133":"markdown","628cad89":"markdown","be7dcd44":"markdown","3e57c7fd":"markdown","968bbff3":"markdown","2cfccb20":"markdown","5b2f800e":"markdown","a6854cf6":"markdown","5dbede48":"markdown","a72984b5":"markdown","b878bced":"markdown","8b1504b8":"markdown","b44ddc3c":"markdown","21e0bdd2":"markdown","2677c9c1":"markdown","6733368c":"markdown","799fbe57":"markdown","d06c4d1a":"markdown","1eedc636":"markdown","ba92e379":"markdown","4e6fd759":"markdown","e497e88c":"markdown","9776e462":"markdown","6f87275a":"markdown","c7c38bf9":"markdown","830eb12b":"markdown","9f697f76":"markdown","f0bdd6b1":"markdown","2394c9ab":"markdown","af7a331a":"markdown","ff3d1e04":"markdown","f13a7cba":"markdown","7bf01425":"markdown","4f3a579b":"markdown","77c825f2":"markdown","285641c5":"markdown","1886e2ef":"markdown","4ee9bfe6":"markdown","12b946df":"markdown","7e943699":"markdown","521e072f":"markdown","d7daf955":"markdown","aa9bcab9":"markdown","1be914c5":"markdown","739d6e32":"markdown","73ec736c":"markdown","440acfdf":"markdown","902e1701":"markdown","a8eafdc0":"markdown","6884d9fd":"markdown","caef9fa8":"markdown","953d3d81":"markdown","190bc6fa":"markdown","17f9df0a":"markdown","ec699b47":"markdown","2f6ffbe5":"markdown","b8d2e635":"markdown","e0d90b93":"markdown","6d3b9a85":"markdown","a57ea80a":"markdown","9805c952":"markdown","695f0203":"markdown","92a39fee":"markdown","26ae08b0":"markdown","44a63a3d":"markdown","85e08307":"markdown","afdc7aaf":"markdown","7511f149":"markdown","8e1a14e4":"markdown","da616a50":"markdown","74eba470":"markdown","a03cccb5":"markdown","b06d16fa":"markdown","744cc759":"markdown","8af54fb6":"markdown","796f4f48":"markdown","6859de6f":"markdown","931e9327":"markdown","32bbf769":"markdown","7a7032a2":"markdown","c4d4e6d8":"markdown","7024ebda":"markdown","237fea17":"markdown","62eebaf3":"markdown","20cd20f8":"markdown"},"source":{"90418d83":"%matplotlib inline","0212ffd5":"import numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score  #Deciding Performance Measure\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble.forest import RandomForestClassifier\nimport os\nimport seaborn as sns\nprint(os.listdir(\"..\/input\"))\n","c0c79403":"import re","9a01ab9b":"from sklearn.impute import SimpleImputer","5c5004ed":"from sklearn.preprocessing import LabelEncoder","dc61f1f9":"data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\ngender_data = pd.read_csv('..\/input\/gender_submission.csv')","c449b3ce":"data.head()","2d6ecdc2":"data.describe()","934479c0":"data.info()","a7535312":"test_data.head()","924a285c":"test_data.info()","7df1b1ac":"test_data.describe()","0466297d":"test_data['Survived'] = gender_data['Survived']","303f0083":"tree = DecisionTreeClassifier(criterion='entropy')  #Entropy and Gini both doesn't give much of the difference in the tree structure\nX = np.array(data['Fare']).reshape((-1,1))  #taking input as fare checking if high fare means rich people are affected or not\ny = np.array(data['Survived']).reshape((-1,1))\ntree.fit(X, y)","5e093018":"tree.predict_proba([[512]])","f66924ec":"tree.predict_proba([[150]])","e6860718":"test_X = np.array(test_data['Fare']).reshape((-1,1))\ntest_y = np.array(gender_data['Survived'])","86730db9":"nan_index = np.where(np.isnan(test_X))  ","37902016":"test_X[152] = 0  #test_X contains NaN value which is causing error, later imputer will be used","332f3996":"y_pred = tree.predict(test_X)  #test_X contains NaN value which is causing error","93003b80":"np.mean(np.double(y_pred!=test_y)) #Error","47bf7a85":"train_errors = []\ntest_errors = []\nfor m in range(1, len(X)):\n    tree.fit(X[:m], y[:m])\n    y_train_predict = tree.predict(y[:m])\n#     train_error = np.mean(np.double(y_train_predict!=y[:m]))\n    train_accuracy = accuracy_score(y[:m], y_train_predict)\n#     train_error = mean_squared_error(y[:m], y_train_predict)\n    train_errors.append(train_accuracy)\n    y_test_predict = tree.predict(test_X)\n#     test_error = np.mean(np.double(y_test_predict!=test_y))\n    test_accuracy = accuracy_score(test_y, y_test_predict)\n#     test_error = mean_squared_error(test_y, y_test_predict)\n    test_errors.append(test_accuracy)","02b67e0c":"plt.plot(train_errors, 'b-', label='Train Error')\nplt.plot(test_errors, 'r-', label='Test Error')   #Either I plot accuracy_score or mean_squared error it plots same with different interpretetion\nplt.legend()","7a0e0982":"Pclass_df = data[['Pclass', 'Survived']]","5118a000":"Pclass_df.head(10)","ca1ab97b":"Pclass_df.info() #Checking for missing values","44a82d1a":"pd.crosstab(index=data['Pclass'], columns=data['Survived']).plot(kind='bar')","8508a4c1":"combined_data = [data, test_data]","c13f6f50":"data.columns","a4eacf00":"test_data.columns","e155052f":"for dataset in combined_data:\n    dataset.drop(labels=['PassengerId', 'Cabin', 'Ticket'], axis=1, inplace=True)","a3f538aa":"data.columns  # cross checking","19d7d5a2":"test_data.columns","d1068a80":"names_train = data['Name']\nnames_test = test_data['Name']","d3062f37":"def extract_titles(names):\n    titles = []\n    for name in names:\n        title = re.sub('(.*, )|(\\..*)', '', name)\n        titles.append(title)\n    return titles","7d96ae9a":"titles_train = extract_titles(names_train)\ntitles_test = extract_titles(names_test)","d62feca2":"titles_df_train = pd.DataFrame(titles_train, columns=['Titles'])\ntitles_df_test = pd.DataFrame(titles_test, columns=['Titles'])\ntitles_df_train['Sex'] = data['Sex']","d3bfb89b":"titles_df_train['Survived'] = data['Survived']","fc06effe":"pd.unique(titles_df_train['Titles'])  #Checking all the unique titles from train data","f922b1fa":"pd.unique(titles_df_test['Titles'])","2e638614":"titles_df_train['Titles'].value_counts()","8283093a":"titles_df_test['Titles'].value_counts()","de19b9e2":"titles_df_train.head()","22b046d5":"pd.crosstab(titles_df_train['Sex'], [titles_df_train['Titles']], rownames=['Sex'], colnames=['Titles'])","c17debf1":"rare_titles = ['the Countess', 'Sir', 'Major', 'Jonkheer', 'Don', 'Col', 'Capt', 'Dr', 'Rev', 'Dona']","52f7ddfd":"titles_df_train.loc[titles_df_train['Titles'].isin(rare_titles), 'Titles'] = 'Rare'\ntitles_df_test.loc[titles_df_test['Titles'].isin(rare_titles), 'Titles'] = 'Rare'","b7cecbc1":"ms_titles = ['Mlle', 'Ms']\nmrs_titles = ['Mme', 'Lady']","3120ae45":"titles_df_train.loc[titles_df_train['Titles'].isin(ms_titles), 'Titles'] = 'Miss'\ntitles_df_test.loc[titles_df_test['Titles'].isin(ms_titles), 'Titles'] = 'Miss'","43f1bac6":"titles_df_train.loc[titles_df_train['Titles'].isin(mrs_titles), 'Titles'] = 'Mrs'\ntitles_df_test.loc[titles_df_test['Titles'].isin(mrs_titles), 'Titles'] = 'Mrs'","0c73f123":"pd.unique(titles_df_train['Titles'])","a219a912":"pd.unique(titles_df_test['Titles'])","bcd31c35":"pd.crosstab(titles_df_train['Sex'], [titles_df_train['Titles'], titles_df_train['Survived']], rownames=['Sex'], colnames=['Titles', 'Survived'])","a7da1db5":"data.info()","6ce2d6aa":"for dataset in combined_data:\n    dataset['Sex'] =pd.factorize(dataset['Sex'])[0]","cf773774":"data.head()","39832259":"test_data.head()","35c438a9":"for dataset in combined_data:\n    dataset.drop(labels='Name', axis=1, inplace=True)","1258f14e":"data['Titles'] = titles_df_train['Titles']\ntest_data['Titles'] = titles_df_test['Titles']","0bfbd5a0":"data.head()","735f1de3":"guess_age = np.zeros((2,3))","ee2c89e4":"median_ages = data.groupby(by=['Sex', 'Pclass'])['Age'].median()","0f43a779":"median_ages","b6a4ee5a":"median_ages.loc[0,1]","b54ec1ac":"for i in range(0, 2):\n    for j in range(0, 3):\n        for dataset in combined_data:\n            dataset.loc[(data['Sex']==i) & (dataset['Pclass']==j+1) & (dataset['Age'].isna()), 'Age'] = median_ages.loc[i, j+1]","e86199ea":"data.Age.isnull().any()","8a0dd829":"test_data.Age.isnull().any()","e7b158a5":"data['Age'].isna().sum()","606272f8":"def fill_missing_ages(data, columns):\n    median_ages = data.groupby(by=columns)['Age'].median()\n    for i in range(0, 2):\n        for j in range(0, 3):\n            data.loc[(data['Sex']==i) & (data['Pclass']==j+1) & (data['Age'].isna()), 'Age'] = median_ages.loc[i, j+1]","64ea06fb":"data['Age'].dtype","c38bd466":"plt.boxplot(data['Age'])","ce5c2abe":"data['AgeBand'] = pd.cut(data['Age'],5)\ntest_data['AgeBand'] = pd.cut(test_data['Age'],5)","e6fe4be3":"pd.unique(data['AgeBand'])","71db559f":"pd.unique(test_data['AgeBand'])","1474ee52":"data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='Survived', ascending=False)","debedcfc":"for dataset in combined_data:\n    dataset.loc[dataset['Age']<=16, 'Age'] =0\n    dataset.loc[(dataset['Age']>16) & (dataset['Age']<=32), 'Age'] =1\n    dataset.loc[(dataset['Age']>32) & (dataset['Age']<=48), 'Age'] =2\n    dataset.loc[(dataset['Age']>48) & (dataset['Age']<=64), 'Age'] =3\n    dataset.loc[(dataset['Age']>64) , 'Age'] =4","eaf8fd36":"data.drop(labels='AgeBand', axis=1, inplace=True)\ntest_data.drop(labels='AgeBand', axis=1, inplace=True)","57196051":"data.head()","bdcf7702":"test_data.head()","23b057a6":"for dataset in combined_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","96f54390":"pd.unique(data['FamilySize'])","4d37623f":"pd.unique(test_data['FamilySize'])","6bc091a9":"test_data.loc[test_data['Parch']==test_data['Parch'].max()]","46b0e073":"data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","1876cc8b":"for dataset in combined_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize']==1, 'IsAlone'] = 1\ndata[['IsAlone', 'Survived']].groupby('IsAlone', as_index=False).mean().sort_values(by='Survived', ascending=False)","40eda4a9":"pd.crosstab(index=data['FamilySize'], columns=data['Survived']).plot.bar()","15fe7e95":"data.head()","f1a478da":"test_data.head()","e3ca5118":"data.loc[data['Embarked'].isna()]","cbf5a07a":"test_data.isna().sum()","01756a10":"data.isna().sum()","89201c13":"for dataset in combined_data:\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)","e0b7cdd1":"data.isnull().any()","69815677":"test_data.isnull().any()","029a3ab8":"pd.crosstab(index=data['Embarked'], columns=data['Survived']).plot.bar()","79460ef1":"pd.crosstab(index=data['Pclass'], columns=data['Survived']).plot.bar()","3fa2734d":"imputer = SimpleImputer(strategy='median')","b40a84ee":"imputer.fit(np.array(data['Fare']).reshape((-1,1)))","095274ca":"data['Fare'] = imputer.transform(np.array(data['Fare']).reshape((-1,1)))\ntest_data['Fare'] = imputer.transform(np.array(test_data['Fare']).reshape((-1, 1)))","a0a22a7a":"data.isna().sum()","d093c3d0":"test_data.isna().sum()","75253786":"data[['Survived', 'Embarked', 'Pclass']].groupby(by=['Embarked', 'Pclass'], as_index=False)\\\n['Pclass', 'Survived'].mean().sort_values(by='Survived', ascending=False)","d8422ce9":"data.head()","898ea7c1":"data['Embarked'] = pd.factorize(data['Embarked'])[0]","2043a43d":"test_data['Embarked'] = pd.factorize(test_data['Embarked'])[0]","a0b8d8c0":"test_data.head()","a8502375":"data.head()","fad8fb29":"label_encoder = LabelEncoder()","db5fee23":"label_encoder.fit(data['Embarked'])","36decc2f":"data ['Embarked'] = label_encoder.transform(data['Embarked']) + 1\ntest_data['Embarked'] = label_encoder.transform(test_data['Embarked']) + 1","53dccd94":"pd.qcut(data['Fare'], 4).unique()","8184b6f3":"pd.qcut(test_data['Fare'], 4).unique()","c0cf2c82":"data['FareBand'] = pd.qcut(data['Fare'], 4)","96423d05":"data[['FareBand', 'Survived']].groupby(by='FareBand', as_index=False).mean().sort_values(by='Survived', ascending=False)","48f35ea1":"data[['FareBand', 'Survived']].groupby(by='FareBand', as_index=False).count()","f08e4652":"for dataset in combined_data:\n    dataset.loc[dataset['Fare']<8, 'Fare'] = 0\n    dataset.loc[(dataset['Fare']>=8) & (dataset['Fare']<14), 'Fare'] = 1\n    dataset.loc[(dataset['Fare']>=14) & (dataset['Fare']<31), 'Fare'] = 2\n    dataset.loc[(dataset['Fare']>=31), 'Fare'] = 3","073faf64":"data.head()","af713309":"test_data.head()","aee35889":"data.drop(labels='FareBand', axis=1, inplace=True)","a3b97ba6":"label_encoder_titles = LabelEncoder()","5a9780fb":"label_encoder_titles.fit(data['Titles'])","15651b8e":"data['Titles'] = label_encoder_titles.transform(data['Titles']) + 1\ntest_data['Titles'] = label_encoder_titles.transform(test_data['Titles']) + 1","764dbd1e":"data.head()","b45879f2":"test_data.head()","b3ffb1b2":"for dataset in combined_data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset['Fare'] = dataset['Fare'].astype(int)","924a5d8d":"data.head()","190832ae":"test_data.head()","f7a1d3c8":"from sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble.forest import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier","6da75204":"xgb = XGBClassifier()\nsv_linear = SVC(kernel='linear', gamma='auto')\nsv_rbf = SVC(kernel='rbf', gamma='auto')\nrf = RandomForestClassifier(n_estimators=10)\nnb = MultinomialNB()\nlr = LogisticRegression(solver='lbfgs')","c3c5279b":"X_train = data.drop(labels='Survived', axis=1, inplace=False)\ny_train = data['Survived']","db731370":"X_test = test_data.drop(labels='Survived', axis=1, inplace=False)","d7c7ed24":"xgb.fit(X_train, y_train)\nsv_linear.fit(X_train, y_train)\nsv_rbf.fit(X_train, y_train)\nrf.fit(X_train, y_train)\nnb.fit(X_train, y_train)\nlr.fit(X_train, y_train)","ddf16be3":"xgb_score_train = np.mean(np.double(xgb.predict(X_train) == y_train))\nsvc_linear_score_train = np.mean(np.double(sv_linear.predict(X_train) == y_train))\nsvc_rbf_score_train = np.mean(np.double(sv_rbf.predict(X_train) == y_train))\nrf_score_train = np.mean(np.double(rf.predict(X_train) == y_train))\nnb_score_train = np.mean(np.double(nb.predict(X_train) == y_train))\nlr_score_train = np.mean(np.double(lr.predict(X_train) == y_train))","605fb39c":"print(\"XGB Scores train: {0}\".format(xgb_score_train))","cef1db40":"print(\"SVC_Linear Scores train: {0}\".format(svc_linear_score_train))","4d00dd83":"print(\"SVC_RBF Scores train: {0}\".format(svc_rbf_score_train))","b9cf43fd":"print(\"Random Forest Scores train: {0}\".format(rf_score_train))","b46836b6":"print(\"Naive Bayes Scores train: {0}\".format(nb_score_train))","20e0efed":"print(\"Logistic Regression Scores train: {0}\".format(lr_score_train))","84dacd9a":"train_accuracies = []\n# test_accuracies = []\nfor m in range(1, len(X_train)):\n    x_train = X_train.iloc[:m]\n    y = y_train.iloc[:m]\n    xgb.fit(x_train, y)\n    train_accuracy = np.mean(np.double(xgb.predict(x_train) == y))\n    train_accuracies.append(train_accuracy)\n#     test_accuracy = np.mean(np.double(xgb.predict(X_test) == y_test))\n#     test_accuracies.append(test_accuracy)","bed77cb0":"plt.plot(train_accuracies, 'b-', label='Train Accuracy')\n# plt.plot(test_accuracies, 'r-', label='Test Accuracy')   #Either I plot accuracy_score or mean_squared error it plots same with different interpretetion\nplt.legend()","d17da606":"feature_names = list(X_train.columns)","fcb0e0b9":"sv_linear.coef_","9a52eb52":"feature_names","2b4dafc2":"lr.coef_","26e2573c":"xgb.feature_importances_","6588d722":"rf.feature_importances_","b078c6ea":"feature_importance = np.vstack((sv_linear.coef_[0], lr.coef_[0],  xgb.feature_importances_,  rf.feature_importances_))","a8326012":"index_names = ['SVC Linear', 'Logistic Regression', 'XGB', 'Random Forest']","2eb08f1e":"feature_importance_df = pd.DataFrame(feature_importance, index=index_names, columns=feature_names)","640da797":"feature_importance_df","1ee70f92":"data.corr()","6f1d22a6":"data[['Embarked', 'Pclass', 'Survived']].head()","5fbafa7b":"new_data = data.copy()","089932b9":"random_data = pd.read_csv('.\/..\/input\/train.csv')","6fa4c31a":"new_data['Survived'] = random_data['Survived']  #random_data just created for test purpose","3f4c107c":"new_data[['Embarked', 'Pclass', 'Survived']].groupby(by=['Embarked', 'Pclass'], as_index=False).mean().\\\nsort_values(by='Survived', ascending=False)","77a05a3d":"pd.crosstab([new_data['Embarked'], new_data['Pclass']], new_data['Survived'], \\\n            rownames=['Embarked', 'Pclass'], colnames=['Survived'])","802f50ed":"new_data['Embarked_Pclass'] = 1","49e9a41a":"new_data.loc[(new_data['Embarked']==1) & (new_data['Pclass']==3), 'Embarked_Pclass'] = 0","98baa774":"pd.crosstab([new_data['Sex'], new_data['Titles']], new_data['Survived'], \\\n            rownames=['Sex', 'Titles'], colnames=['Survived'])","d03d28eb":"label_encoder_titles.inverse_transform([0, 1, 2, 3, 4])","6fcebd52":"pd.crosstab([new_data['Sex'], new_data['Titles'], new_data['Pclass']], new_data['Survived'], \\\n            rownames=['Sex', 'Titles', 'Pclass'], colnames=['Survived'])","213f4460":"new_data['Sex_Titles_Pclass'] = 1","73f1fc53":"new_data.loc[(new_data['Sex']==0) & (new_data['Titles']==3) & new_data['Pclass']==3] = 0 ","aae53bca":"new_data.head()","f8cb7493":"new_data['Sex_Titles'] = 1","527309af":"new_data.head()","ba93136b":"new_data.loc[(new_data['Sex']==0) & (new_data['Titles']==3), 'Sex_Titles'] = 0","0787753d":"new_X_train = new_data.drop(labels='Survived', axis=1)\nnew_y_train = y_train","fc803da6":"xgb = XGBClassifier()\nsv_linear = SVC(kernel='linear', gamma='auto')\nsv_rbf = SVC(kernel='rbf', gamma='auto')\nrf = RandomForestClassifier(n_estimators=10)\nnb = MultinomialNB()\nlr = LogisticRegression(solver='lbfgs')","d29828a6":"xgb.fit(new_X_train, y_train)\nsv_linear.fit(new_X_train, y_train)\nsv_rbf.fit(new_X_train, y_train)\nrf.fit(new_X_train, y_train)\nnb.fit(new_X_train, y_train)\nlr.fit(new_X_train, y_train)","45653ea5":"xgb_score_train = np.mean(np.double(xgb.predict(new_X_train) == y_train))\nsvc_linear_score_train = np.mean(np.double(sv_linear.predict(new_X_train) == y_train))\nsvc_rbf_score_train = np.mean(np.double(sv_rbf.predict(new_X_train) == y_train))\nrf_score_train = np.mean(np.double(rf.predict(new_X_train) == y_train))\nnb_score_train = np.mean(np.double(nb.predict(new_X_train) == y_train))\nlr_score_train = np.mean(np.double(lr.predict(new_X_train) == y_train))","b3d16e12":"print(\"XGB Scores train: {0}\".format(xgb_score_train))","d0669113":"print(\"SVC_Linear Scores train: {0}\".format(svc_linear_score_train))","c4f05531":"print(\"SVC_RBF Scores train: {0}\".format(svc_rbf_score_train))","0f5824a4":"print(\"Random Forest Scores train: {0}\".format(rf_score_train))","49136b4a":"print(\"Naive Bayes Scores train: {0}\".format(nb_score_train))","f3b7f87d":"print(\"Logistic Regression Scores train: {0}\".format(lr_score_train))","1387e961":"feature_importance = np.vstack((sv_linear.coef_[0], lr.coef_[0],  xgb.feature_importances_,  rf.feature_importances_))","344b5c5b":"index_names = ['SVC Linear', 'Logistic Regression', 'XGB', 'Random Forest']","8905aef2":"feature_importance_df = pd.DataFrame(feature_importance, index=index_names, columns=feature_names + ['Embarked_Pclass', 'Sex_Titles_Pclass', 'Sex_Titles'])","5182c4db":"feature_importance_df","8eaed6a4":"feature_importance_df #Old importance for comparing","cb6ba844":"new_data.drop('Sex_Titles_Pclass', axis=1, inplace=True)","da1c8b14":"new_data['Sex_Titles'] = 1","58194845":"new_data.head()","8530980a":"new_data.loc[(new_data['Sex']==0) & (new_data['Titles']==3), 'Sex_Titles'] = 0","73cc6113":"new_X_train = new_data.drop(labels='Survived', axis=1)\nnew_y_train = y_train","251b7ac4":"xgb = XGBClassifier()\nsv_linear = SVC(kernel='linear', gamma='auto')\nsv_rbf = SVC(kernel='rbf', gamma='auto')\nrf = RandomForestClassifier(n_estimators=10)\nnb = MultinomialNB()\nlr = LogisticRegression(solver='lbfgs')","6c1f7616":"xgb.fit(new_X_train, y_train)\nsv_linear.fit(new_X_train, y_train)\nsv_rbf.fit(new_X_train, y_train)\nrf.fit(new_X_train, y_train)\nnb.fit(new_X_train, y_train)\nlr.fit(new_X_train, y_train)","7e19db45":"xgb_score_train = np.mean(np.double(xgb.predict(new_X_train) == y_train))\nsvc_linear_score_train = np.mean(np.double(sv_linear.predict(new_X_train) == y_train))\nsvc_rbf_score_train = np.mean(np.double(sv_rbf.predict(new_X_train) == y_train))\nrf_score_train = np.mean(np.double(rf.predict(new_X_train) == y_train))\nnb_score_train = np.mean(np.double(nb.predict(new_X_train) == y_train))\nlr_score_train = np.mean(np.double(lr.predict(new_X_train) == y_train))","84ed73c5":"print(\"XGB Scores train: {0}\".format(xgb_score_train))","3c2b1201":"print(\"SVC_Linear Scores train: {0}\".format(svc_linear_score_train))","5e14dec7":"print(\"SVC_RBF Scores train: {0}\".format(svc_rbf_score_train))","dc7261ae":"print(\"Random Forest Scores train: {0}\".format(rf_score_train))","4378e74c":"print(\"Naive Bayes Scores train: {0}\".format(nb_score_train))","869efc09":"print(\"Logistic Regression Scores train: {0}\".format(lr_score_train))","d6a0b34d":"feature_importance = np.vstack((sv_linear.coef_[0], lr.coef_[0],  xgb.feature_importances_,  rf.feature_importances_))","e086d0b9":"index_names = ['SVC Linear', 'Logistic Regression', 'XGB', 'Random Forest']","374eaad2":"feature_importance_df = pd.DataFrame(feature_importance, index=index_names, columns=feature_names + ['Embarked_Pclass', 'Sex_Titles'])","656e134c":"feature_importance_df","0a71ab7f":"data = new_data.copy()","2c8ace66":"test_data['Embarked_Pclass'] = 1\ntest_data['Sex_Titles'] = 1","d629bd38":"test_data.loc[(new_data['Embarked']==1) & (new_data['Pclass']==3), 'Embarked_Pclass'] = 0\ntest_data.loc[(test_data['Sex']==0) & (test_data['Titles']==3), 'Sex_Titles'] = 0","fbb3aa70":"new_test_data = test_data.copy()","77bbb31e":"new_data.head()","81d56edf":"new_X_train = new_data.drop(labels='Survived', axis=1)\nnew_X_test = new_test_data.drop(labels='Survived', axis=1)","d6b019b8":"xgb = XGBClassifier()\nsv_linear = SVC(kernel='linear', gamma='auto')\nsv_rbf = SVC(kernel='rbf', gamma='auto')\nrf = RandomForestClassifier(n_estimators=10)\nnb = MultinomialNB()\nlr = LogisticRegression(solver='lbfgs')","67b182e5":"xgb.fit(new_X_train, y_train)\nsv_linear.fit(new_X_train, y_train)\nsv_rbf.fit(new_X_train, y_train)\nrf.fit(new_X_train, y_train)\nnb.fit(new_X_train, y_train)\nlr.fit(new_X_train, y_train)","e3ef4671":"xgb_score_train = np.mean(np.double(xgb.predict(new_X_train) == y_train))\nsvc_linear_score_train = np.mean(np.double(sv_linear.predict(new_X_train) == y_train))\nsvc_rbf_score_train = np.mean(np.double(sv_rbf.predict(new_X_train) == y_train))\nrf_score_train = np.mean(np.double(rf.predict(new_X_train) == y_train))\nnb_score_train = np.mean(np.double(nb.predict(new_X_train) == y_train))\nlr_score_train = np.mean(np.double(lr.predict(new_X_train) == y_train))","8dcfa5ae":"print(\"XGB Scores train: {0}\".format(xgb_score_train))","637334d8":"print(\"SVC_Linear Scores train: {0}\".format(svc_linear_score_train))","bd4110e6":"print(\"SVC_RBF Scores train: {0}\".format(svc_rbf_score_train))","47f687a7":"print(\"Random Forest Scores train: {0}\".format(rf_score_train))","697ca832":"print(\"Naive Bayes Scores train: {0}\".format(nb_score_train))","955d0c48":"print(\"Logistic Regression Scores train: {0}\".format(lr_score_train))","5b247ffd":"feature_importance = np.vstack((sv_linear.coef_[0], lr.coef_[0],  xgb.feature_importances_,  rf.feature_importances_))","0dc09a84":"index_names = ['SVC Linear', 'Logistic Regression', 'XGB', 'Random Forest']","78c60b1f":"feature_importance_df = pd.DataFrame(feature_importance, index=index_names, columns=feature_names + ['Embarked_Pclass', 'Sex_Titles'])","a2dcb291":"feature_importance_df.abs()","2ea7d385":"train_data = data.copy()\nnew_test_data = test_data.copy()","0486c165":"train_data = pd.concat((train_data,pd.get_dummies(train_data['FamilySize'], prefix='FamilySize')), axis=1)\ntrain_data.drop(labels='FamilySize', axis=1, inplace=True)\nnew_test_data = pd.concat((new_test_data,pd.get_dummies(new_test_data['FamilySize'], prefix='FamilySize')), axis=1)\nnew_test_data.drop(labels='FamilySize', axis=1, inplace=True)\ntrain_data = pd.concat((train_data,pd.get_dummies(train_data['Titles'], prefix='Titles')), axis=1)\ntrain_data.drop(labels='Titles', axis=1, inplace=True)\nnew_test_data = pd.concat((new_test_data,pd.get_dummies(new_test_data['Titles'], prefix='Titles')), axis=1)\nnew_test_data.drop(labels='Titles', axis=1, inplace=True)","05236d0a":"new_X_train = train_data.drop(labels='Survived', axis=1)\nnew_X_test = test_data.drop(labels='Survived', axis=1)","4d743da1":"xgb = XGBClassifier()\nsv_linear = SVC(kernel='linear', gamma='auto')\nsv_rbf = SVC(kernel='rbf', gamma='auto')\nrf = RandomForestClassifier(n_estimators=10)\nnb = MultinomialNB()\nlr = LogisticRegression(solver='lbfgs')","1cca9975":"xgb.fit(new_X_train, y_train)\nsv_linear.fit(new_X_train, y_train)\nsv_rbf.fit(new_X_train, y_train)\nrf.fit(new_X_train, y_train)\nnb.fit(new_X_train, y_train)\nlr.fit(new_X_train, y_train)","a1e028e4":"xgb_score_train = np.mean(np.double(xgb.predict(new_X_train) == y_train))\nsvc_linear_score_train = np.mean(np.double(sv_linear.predict(new_X_train) == y_train))\nsvc_rbf_score_train = np.mean(np.double(sv_rbf.predict(new_X_train) == y_train))\nrf_score_train = np.mean(np.double(rf.predict(new_X_train) == y_train))\nnb_score_train = np.mean(np.double(nb.predict(new_X_train) == y_train))\nlr_score_train = np.mean(np.double(lr.predict(new_X_train) == y_train))","7e2cd6b8":"print(\"XGB Scores train: {0}\".format(xgb_score_train))","7e685332":"print(\"SVC_Linear Scores train: {0}\".format(svc_linear_score_train))","ddf8e2cd":"print(\"SVC_RBF Scores train: {0}\".format(svc_rbf_score_train))","43190ca5":"print(\"Random Forest Scores train: {0}\".format(rf_score_train))","ebb1822f":"print(\"Naive Bayes Scores train: {0}\".format(nb_score_train))","e52e1508":"print(\"Logistic Regression Scores train: {0}\".format(lr_score_train))","2f5e6cc4":"feature_importance = np.vstack((sv_linear.coef_[0], lr.coef_[0],  xgb.feature_importances_,  rf.feature_importances_))","0455da2f":"index_names = ['SVC Linear', 'Logistic Regression', 'XGB', 'Random Forest']","206d6461":"feature_names_new = list(train_data.columns)\nfeature_names_new.remove('Survived')","cdd78b35":"feature_importance_df = pd.DataFrame(feature_importance, index=index_names, columns=feature_names_new)","2e46f395":"feature_importance_df","618f4c03":"label_encoder_familysize = LabelEncoder()","5fd19914":"label_encoder_familysize.fit(data['FamilySize'])","9acd82ec":"data['FamilySize'] = X_train['FamilySize']","5b99069b":"data_length = X_train.shape[0]","0b306a0a":"random_indices = np.random.randint(data_length, size=int(0.20*data_length))","14e1b6c7":"new_X_train = new_data.drop(labels='Survived', axis=1)\nnew_X_test = test_data.drop(labels='Survived', axis=1)","62d5da40":"X_cv = X_train.iloc[random_indices]\ny_cv = y_train.iloc[random_indices]\nX_train_cv = X_train.iloc[(X_train.index.isin(random_indices)==False)]\ny_train_cv = y_train.iloc[(y_train.index.isin(random_indices)==False)]","636edb81":"train_accuracies = []\ncv_accuracies = []\nfor m in range(10, len(X_train_cv)):\n    sv_rbf.fit(X_train_cv.iloc[:m], y_train_cv.iloc[:m])\n    train_accuracy = accuracy_score(y_train_cv, sv_rbf.predict(X_train_cv))\n    test_accuracy = accuracy_score(y_cv, sv_rbf.predict(X_cv))\n    train_accuracies.append(train_accuracy)\n    cv_accuracies.append(test_accuracy)","a5d7f8ab":"plt.plot(train_accuracies, 'b-', label= 'Train Accuracies')\nplt.plot(cv_accuracies, 'r-',label = 'CV Accuracies')\nplt.legend()","6709e2d9":"xgb = XGBClassifier()\nsv_linear = SVC(kernel='linear', gamma='auto')\nsv_rbf = SVC(kernel='rbf', gamma='auto')\nrf = RandomForestClassifier(n_estimators=10)\nnb = MultinomialNB()\nlr = LogisticRegression(solver='lbfgs')","de35795c":"param_grid_xgb = {'max_depth': [3, 5, 10, 100], 'n_estimators': [100, 200, 500, 1000], 'learning_rate': [0.1, 0.01, 0.2] }\nparam_grid_sv_linear = {'C': [0.1, 1, 10, 100]}\nparam_grid_sv_rbf = {'gamma': [0.1, 1, 10, 100], 'C': [0.1, 1, 10, 100]}\nparam_grid_rf = {'n_estimators': [10, 100, 1000], 'max_depth': [3, 5, 10]}\nparam_grid_nb = {'alpha': [0.1, 0.01, 0.2]}\nparam_grid_lr = {'C': [0.1, 1, 10, 100]}","441f1a00":"from sklearn.model_selection import GridSearchCV","4fc186c0":"grid_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, verbose=True)\ngrid_sv_linear = GridSearchCV(estimator=sv_linear, param_grid=param_grid_sv_linear, cv=5, verbose=True)\ngrid_sv_rbf = GridSearchCV(estimator=sv_rbf, param_grid=param_grid_sv_rbf, cv=5, verbose=True)\ngrid_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, verbose=True)\ngrid_nb = GridSearchCV(estimator=nb, param_grid=param_grid_nb, cv=5, verbose=True)\ngrid_lr = GridSearchCV(estimator=lr, param_grid=param_grid_lr, cv=5, verbose=True)","2b2b1e20":"new_X_train = new_data.drop(labels='Survived', axis=1)\nnew_y_train = y_train","3ff9a91a":"%%time\ngrid_xgb.fit(new_X_train, new_y_train)","868569c6":"%%time\ngrid_sv_linear.fit(new_X_train, new_y_train)","282df798":"%%time\ngrid_sv_rbf.fit(new_X_train, new_y_train)","a66638ec":"%%time\ngrid_rf.fit(new_X_train, new_y_train)","2aac53b4":"%%time\ngrid_nb.fit(new_X_train, new_y_train)","79f84874":"%%time\ngrid_lr.fit(new_X_train, new_y_train)","eb697d9b":"grid_xgb_score_train = np.mean(np.double(grid_xgb.predict(new_X_train) == y_train))\ngrid_svc_linear_score_train = np.mean(np.double(grid_sv_linear.predict(new_X_train) == y_train))\ngrid_svc_rbf_score_train = np.mean(np.double(grid_sv_rbf.predict(new_X_train) == y_train))\ngrid_rf_score_train = np.mean(np.double(grid_rf.predict(new_X_train) == y_train))\ngrid_nb_score_train = np.mean(np.double(grid_nb.predict(new_X_train) == y_train))\ngrid_lr_score_train = np.mean(np.double(grid_lr.predict(new_X_train) == y_train))","1092663f":"print(\"XGB Scores train: {0}\".format(grid_xgb_score_train))","b0712c0a":"print(\"SVC_Linear Scores train: {0}\".format(grid_svc_linear_score_train))","5e7aa026":"print(\"SVC_RBF Scores train: {0}\".format(grid_svc_rbf_score_train))","69aae970":"print(\"Random Forest Scores train: {0}\".format(grid_rf_score_train))","9105a791":"print(\"Naive Bayes Scores train: {0}\".format(grid_nb_score_train))","cbd669e8":"print(\"Logistic Regression Scores train: {0}\".format(grid_lr_score_train))","57c5f1e6":"submission_df = pd.read_csv('.\/..\/input\/test.csv', usecols=['PassengerId'])","cef499c2":"submission_df['Survived'] = grid_sv_rbf.predict(new_X_test)","10ea292a":"submission_df.set_index(keys='PassengerId', inplace=True)","a0b24d42":"submission_df.to_csv('submission.csv')  # The score was 78.9% not that much good","7ee5a458":"plt.boxplot(data['FamilySize'], showmeans = True, meanline = True)\nplt.show()","cda9150d":"data.head()","1bd03410":"plt.scatter(data['Fare'], data['Survived'], alpha=0.1 )","54843cfc":"plt.plot(data['Fare'], data['Survived'], 'o', alpha=0.1)","7f05fede":"# Earlier Approach for comparing Pclass and Survived\ncond1 = Pclass_df['Pclass']==3  #Comparing the survival\ncond2 = Pclass_df['Survived']==1\ncond3 = Pclass_df['Pclass']==2\ncond4 = Pclass_df['Pclass']==1\nPclass_survival_df = Pclass_df[(cond1 & cond2) | (cond3 & cond2) | (cond4 & cond2)]  #we cannot use and, or, not and parenthesis is a must use","261f0438":"#Earlier Approach\nlen(Pclass_survival_df)","c224ee6e":"#Earlier Approach\nPclass_survival_df.plot(y='Pclass', kind='hist')  #since all are comparabale hence not helping much","70451568":"# Earlier Approach\nclass_1_survival_per = 100*len(Pclass_survival_df[Pclass_survival_df['Pclass']==1])\/len(Pclass_survival_df)\nclass_2_survival_per = 100*len(Pclass_survival_df[Pclass_survival_df['Pclass']==2])\/len(Pclass_survival_df)\nclass_3_survival_per = 100*len(Pclass_survival_df[Pclass_survival_df['Pclass']==3])\/len(Pclass_survival_df)","0b5825dd":"#Earlier Approach\ncond1 = Pclass_df['Pclass']==3\ncond2 = Pclass_df['Survived']==0\ncond3 = Pclass_df['Pclass']==2\ncond4 = Pclass_df['Pclass']==1\nPclass_non_survival_df = Pclass_df[(cond1 & cond2) | (cond3 & cond2) | (cond4 & cond2)]","c37b10f6":"#Earlier Approach\nPclass_non_survival_df.plot(y='Pclass', kind='hist')  #class 3 has more chances of non-surviving shows the differences clearly","7772d5be":"#Earlier Approach\nclass_1_non_per = 100*len(Pclass_non_survival_df[Pclass_non_survival_df['Pclass']==1])\/len(Pclass_non_survival_df)\nclass_2_non_per = 100*len(Pclass_non_survival_df[Pclass_non_survival_df['Pclass']==2])\/len(Pclass_non_survival_df)\nclass_3_non_per = 100*len(Pclass_non_survival_df[Pclass_non_survival_df['Pclass']==3])\/len(Pclass_non_survival_df)","08ac83cf":"#Earlier Approach\nclass_1_non_per","dc1be353":"#Earlier Approach\nclass_2_non_per","f6c6c644":"#Earlier Approach\nclass_3_non_per","b9eb9308":"corr_matrix = data.corr()","a6d4c661":"corr_matrix  #SibSp has high correlation with Parch ","2c21bb24":"plt.scatter(x=data['Fare'], y=data['Survived'])","4e550e31":"plt.scatter(x=data['SibSp'], y=data['Parch'])","b840c43b":"corr_matrix_survived = corr_matrix['Survived']","30a68541":"corr_matrix_survived.sort_values(ascending=False)  #Only Numerical Data will considered for features","e3a21a0d":"train_data = data.drop(labels=['PassengerId', 'Ticket', 'Cabin'], inplace=False, axis=1)","c5a36925":"train_data.head(5)","e9ff149f":"from sklearn.preprocessing import LabelBinarizer","f9b84c65":"sex_encoder = LabelBinarizer()\nembarked_encoder = LabelBinarizer()","e6430f17":"sex_cat = train_data['Sex']\nembarked_cat = train_data['Embarked']","c8873c9c":"sex_encoder.fit(sex_cat)\nembarked_encoder.fit(embarked_cat.astype(str)) # Since embarked has null values","3eca39f7":"sex_cat_1hot = sex_encoder.transform(sex_cat)\nembarked_cat_1hot = embarked_encoder.transform(embarked_cat.astype(str))","1052f105":"embarked_cat_1hot.shape","eb61c4b4":"#Earlier Approach\nsex_dict = {'male':1, 'female':0}\nembarked_dict = {'S':1 , 'C':2, 'Q':3}","961bad71":"train_data = train_data.replace(to_replace={'male':1, 'female':0, 'S':1, 'C':2, 'Q': 3}, value=None, inplace=False)","b8c7b681":"train_data.head(5)","e4d452e3":"from sklearn.impute import SimpleImputer","db7aa4cf":"imputer = SimpleImputer(strategy='median')","03b78dcb":"data_num = train_data.drop('Name', axis=1, inplace=False)","43d6f368":"imputer.fit(data_num)   #it calculates the median of each column","5a5c58d2":"imputer.statistics_","90b3fcdc":"x = imputer.transform(data_num) # returns plain numpy array thus, DF needs to be reconstructed","d4c36b85":"data_num_train = pd.DataFrame(x, columns=data_num.columns)","1f6ecb98":"data_num_train.info()","bb7a788a":"from sklearn.metrics.cluster import contingency_matrix","beaf6740":"sex_survived_matrix = contingency_matrix(labels_true=data_num_train['Survived'], labels_pred=data_num_train['Sex'])","c1c6f56e":"contingency_matrix_sex = pd.DataFrame(sex_survived_matrix, index= ['Not Survived', 'Survived'], columns=['Female', 'Male'])","cfe07efc":"contingency_matrix_sex['Total'] = contingency_matrix_sex.sum(axis=1, numeric_only=True)","13b95b64":"contingency_matrix_sex","a407bfc5":"total_female = len(data_num_train[data_num_train['Sex']==0])\ntotal_male = len(data_num_train[data_num_train['Sex']==1])\ntotal = len(data)","5d6316e7":"contingency_matrix_sex.loc['Total'] = [total_female, total_male, total]","cbffdc81":"contingency_matrix_sex","5207f123":"print(len(data_num_train[(data_num_train['Sex']==0) & (data_num_train['Survived']==1)])) #female survival count\nprint(len(data_num_train[(data_num_train['Sex']==0) & (data_num_train['Survived']==0)])) #female non-survival count\nprint(len(data_num_train[(data_num_train['Sex']==1) & (data_num_train['Survived']==1)])) #male survival count\nprint(len(data_num_train[(data_num_train['Sex']==0) & (data_num_train['Survived']==1)])) #male non-survival count","184948f1":"data_num_train.head()","67dedb65":"a = pd.read_csv('.\/..\/input\/train.csv')","00972bfe":"pd.get_dummies(a['Sex'])","9bcd21f7":"data1 = data.copy()","1af39465":"y = data1['Survived']","32910275":"data1.drop(labels=['Survived'], axis=1, inplace=True)","46129fa4":"X = data1","2baac3ed":"from xgboost import XGBClassifier","9cc47c77":"xgb = XGBClassifier()","1fbe0385":"xgb.fit(X, y)","391ee7f7":"np.mean(xgb.predict(X) == y)","0b72dbe0":"from sklearn.linear_model import LogisticRegression","3b99ec3b":"lr = LogisticRegression(solver='lbfgs', C=10)","a8b9d4e8":"lr.fit(X, y)","4cc38c39":"np.mean(np.double(lr.predict(X) ==y))*100","1a825f4a":"from sklearn.svm import SVC","65fb2529":"sv_linear = SVC(kernel='linear')","635cf6b1":"sv_linear.fit(X, y)","0cf11861":"np.mean(np.double(sv_linear.predict(X) ==y))*100","ccd87e40":"sv_rbf = SVC()","b11fb7f3":"sv","a48db24a":"np.mean(np.double(sv.predict() ==y))*100","8248b15b":"test_data1 = test_data.copy()","02994ef1":"y_test = test_data1['Survived']","29dec413":"test_data1.drop(labels='Survived', axis=1, inplace=True)","6b0234a5":"X_test = test_data1","033c7724":"np.mean(np.double(xgb.predict(X_test) ==y_test))*100","7e47df40":"xgb.feature_importances_","3658640f":"X_test.head()","9abcec16":"X.head()","0f209fe4":"lr.classes_","f76dab54":"lr.decision_function","dc3be5ab":"lr.intercept_","13fb2365":"lr.coef_","357f0570":"sv.coef0","cb99c413":"sv.coef_","9160487f":"**Hence no null values**","4a9b39fc":"Earlier I used simple replace method of pandas dataframe for changing text categories to numerical values","e26cd289":"Name could be one of the interesting features, as it directly doesn't provide any value but it opens up the scope for feature engineering from it","28963d26":"Out of all the numerical data only two of them are continous :- Fare and Age which can be scaled","91d17031":"Exploring different Features one-by-one while appyling all different things on them for the analysis","7bc45884":"Before combining features all features needs to be changed to numeric types","f683c133":"**These improved accuracies from the basic features**","628cad89":"the Countess, Sir, Major, Johnkheer, Don, Col, Capt ->Rare Titles\nMs, Mlle -> Miss\nMme, Lady -> Mrs\nRev, Dr -> Rare Titles","be7dcd44":"### Creating Submission File","3e57c7fd":"Embarked has null value","968bbff3":"Fare has null value","2cfccb20":"I don't think that using test_data for filling missing values is correct because test data is something that we haven't seen yet","5b2f800e":"# EDA ","a6854cf6":"This means that we need to have some better or more features to fit the model","5dbede48":"#### Sex Vs Survived Exploration","a72984b5":"Male is Less likely to survive than female","b878bced":"But other than that pd.get_dummies() and LabelBinarizer() are good approaches","8b1504b8":"###  Changing Text Categorical Data to Numeric Categories","b44ddc3c":"Using Bar Chart for Categorical Data","21e0bdd2":"Pclass is a categorical input, valus being provided already in the data. It mainly tells the class to which passenger belongs","2677c9c1":"Sex - Male\/Female as 1\/0\nEmbarked - S\/C\/Q as 1\/2\/3","6733368c":"**Fare as expected has higher correlation out of all, but still that correlation is lower so it can be combined to form better features**","799fbe57":"## Rough Prediction","d06c4d1a":"S->0, C->1, Q->2","1eedc636":"**Combining Titles and Sex only**","ba92e379":"Predicting Ages based on grouping by Sex and Pclass","4e6fd759":"### Exploring Fare","e497e88c":"categorizing starting from 1","9776e462":"### Removing Less Important Features","6f87275a":"**Note:-** Since Dependent Variable is a categorical value therefore, the Pearson's Correlation Coefficient is not a good measure for correlation","c7c38bf9":"Categorizing the Age values","830eb12b":"**Training Accuracy Decreasing over time and Validation Accuracy is reaching Plateau and is low=> Underfitting**","9f697f76":"#### Combining Sex, Titles, Pclass","f0bdd6b1":"The training accuracy is decreasing and reaching a plateau means Underfitting and Testing accuracy is not at all improving","2394c9ab":"Features that can be combined:-\n1. Pclass, Embarked\n2. Sex, Title, Pclass","af7a331a":"Pclass=1, Embarked=C -> high chances of survival <br>\nPclass=3, Embarked=S -> Lowest Chances of survival","ff3d1e04":"**It is a high bias problem** Means increasing number of traning examples will not help much adding extra features and making model more complex should be done","f13a7cba":"Based on this the data preprocessing must be handled in a sequence\n\n1. Missing values\n2. Polynomial features\n3. Categorical features\n4. Numerical features\n5. Custom transformations\n6. Feature scaling\n7. Normalization","7bf01425":"### Let's Try Groupby","4f3a579b":"Using Comparative Bar graph","77c825f2":"### Exploring Embarked vs Survival","285641c5":"https:\/\/medium.com\/@outside2SDs\/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365","1886e2ef":"Using Sklearn contingency matrix","4ee9bfe6":"#### Checking Feature Importance","12b946df":"#### Combining Pclass and Embarked","7e943699":"*Lady, the countess, Mme ->Mrs,   Ms, Mlle ->Miss,  Johnkheer,Don,Capt,,Major,Col,Rev ->Rare,  Sir->Mr*","521e072f":"**No Null Entries**","d7daf955":"### Plotting Train vs Validation Curve","aa9bcab9":"taking 20% as CV set","1be914c5":"Calculating Percentage","739d6e32":"Changing the DataType of Age and Fare to int","73ec736c":"#### Practicing Random Stuff","440acfdf":"Pearson's Correlation Coefficient - http:\/\/learntech.uwe.ac.uk\/da\/Default.aspx?pageid=1442","902e1701":"Two Approaches\n1. Missing data should be removed (row-wise)\n2. Missing value must be filled in with Median Value\n\nIn this case since our test data also have missing values so instead of removing missing value rows we should replace it with Mean\/Median of train data at both time during testing and during predicting also. However, if one of the Target value would be missing then dropping that record would be helpful in learning","a8eafdc0":"### Checking Results on this Data","6884d9fd":"Binning The Age","caef9fa8":"For class 1&2 survival& non-survival is comparable and doesn't give much insights, but for class 3 the non-survival rate is much higher than usual.","953d3d81":"Null-Hypothesis :- Sex does\nAssuming alpha for null-hypothesis to be 10% or 0.10","190bc6fa":"**Categorical-Categorical Correlation Check**\nhttps:\/\/medium.com\/@outside2SDs\/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365","17f9df0a":"### Now Missing Values must be handled","ec699b47":"### Dropping Unecessary Data","2f6ffbe5":"**Sex=Male, Title=Mr, Pclass=3 ->lowest chances of survival**","b8d2e635":"### Combining Features","e0d90b93":"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.15.2\/indexing.html#boolean-indexing","6d3b9a85":"Since median can only be computed on Numerical Values so we need to drop the Name for numerical data","a57ea80a":"#### Checking the correlation between attributes and Survived","9805c952":"The competition expects to use any combination of features to predict the Survived part","695f0203":"We will use Imputer from sklearn for the median values","92a39fee":"Creating a very rough DecisionTree classifier using Pclass as feature to see how is this affecting it","26ae08b0":"###  Exploring the Pclass vs Survival","44a63a3d":"### Model Tuning","85e08307":"Checking the frequency of Survival based on class","afdc7aaf":"Handling Missing values in numerical columns - can be handled by imputer\nHandling Missing values in text columns - ","7511f149":"##  Exploring Name vs Survived","8e1a14e4":"Sex=0, Titles = 3 i.e. 'Mr' died most","da616a50":"Insight is that chances of survival of class_3 people are very less","74eba470":"The training data results are not so good let's try other approaches","a03cccb5":"#### Cross-Validation Plotting using CV set","b06d16fa":"The test accuracy didn't improve so need to try more things like onehotencoding","744cc759":"### Removing Some Useless Columns ","8af54fb6":"Females have less count but high frequency of living","796f4f48":"**Predicting Missing Ages**","6859de6f":"### Fare Band","931e9327":"BoxPlot - http:\/\/www.physics.csbsju.edu\/stats\/box2.html","32bbf769":"**Sex, Title & Pclasss combination is not helpful**","7a7032a2":"## Checking Size of Family","c4d4e6d8":"Frequency of Survival of people from Class 1 is more than other two","7024ebda":"Importing different models","237fea17":"The shape came as 891x4 because embarked has nan values which were considered as fourth category","62eebaf3":"On this Contingency Matrix we can do some statistical tests like **Chi-Square Test**","20cd20f8":"This shows that even on very less training data (891 records only) the accuracy is very low"}}