{"cell_type":{"4ad4a0bc":"code","bb56e8a1":"code","51f9a8d4":"code","25aad155":"code","754f4b73":"code","f53a7e4b":"code","a057d5de":"code","7f4594e7":"code","09b00eef":"code","783607f6":"code","339e21aa":"code","fd46ec10":"code","23990a88":"code","6383432e":"code","8f44b906":"code","9d1fef53":"code","265fbadc":"code","78b111a1":"code","153586f8":"code","5dd4719d":"code","29d0d463":"code","8d5d07a2":"code","f55c6925":"code","e9968938":"code","ebf2cedb":"code","d2f5c0f7":"code","b4ea3844":"code","5e899b02":"code","155291c5":"code","2ed3157d":"code","3b601713":"code","630fa84e":"code","18358020":"code","11f155be":"code","e24fef6d":"code","dbdc7072":"code","88147052":"code","b8384456":"code","c12f72dc":"markdown","134b3cd1":"markdown","49202dec":"markdown","879c2fa7":"markdown","5cd010d3":"markdown","4640026a":"markdown","cf4499d7":"markdown"},"source":{"4ad4a0bc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline ","bb56e8a1":"df=pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head(5)","51f9a8d4":"df.info()","25aad155":"df.isna().sum()","754f4b73":"df.describe()","f53a7e4b":"sns.histplot(df['Residence_type'])","a057d5de":"sns.histplot(df['work_type'])","7f4594e7":"sns.histplot(df['ever_married'])","09b00eef":"sns.histplot(df['gender'])","783607f6":"sns.histplot(df['stroke'])\nprint(df['stroke'].value_counts())","339e21aa":"sns.distplot(df['age'], color='b', kde=True,bins=70)","fd46ec10":"sns.histplot(df, x=\"heart_disease\", bins=6)\nprint(print(df['heart_disease'].value_counts()))","23990a88":"sns.histplot(df, x=\"bmi\", color=\"b\",kde=True)\n","6383432e":"sns.histplot(df, x=\"avg_glucose_level\", color=\"b\",kde=True)\n","8f44b906":"sns.histplot(df, x=\"hypertension\", color=\"b\")","9d1fef53":"sns.countplot(df['work_type'],hue=df['stroke'])","265fbadc":"sns.countplot(df['Residence_type'],hue=df['stroke'])","78b111a1":"labels=['no stroke','stroke']\ncolors = [\"cyan\",\"red\"]\nplt.pie(df['stroke'].value_counts(),labels=labels,colors=colors,\n        autopct='%1.2f%%', shadow=True, startangle=140) \nplt.show()\n","153586f8":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True);","5dd4719d":"sns.scatterplot(data=df, x=\"bmi\", y=\"avg_glucose_level\")\nplt.show()","29d0d463":"sns.scatterplot(data=df, x=\"age\", y=\"avg_glucose_level\")\nplt.show()","8d5d07a2":"sns.scatterplot(data=df, x=\"age\", y=\"bmi\")\nplt.show()","f55c6925":"summary_df = df[['work_type','gender','Residence_type','smoking_status','stroke']]\nsummary = pd.concat([pd.crosstab(cat_df[x], cat_df.stroke) for x in cat_df.columns[:-1]], keys=cat_df.columns[:-1])\nsummary","e9968938":"for i in df:\n    if df[i].dtype == 'object':\n        print(i,df[i].unique())","ebf2cedb":"from sklearn.impute import SimpleImputer\nimport numpy as np\nfor i in df:\n    if df[i].isna().sum()>0:\n        imr=SimpleImputer(missing_values=np.nan,strategy='mean')\n        imr=imr.fit(df[[i]])\n        imputed_data=imr.transform(df[[i]])\n        df[i]=imputed_data","d2f5c0f7":"from sklearn.preprocessing import LabelEncoder\nfor c in df.columns:\n    le = LabelEncoder()\n    if df.dtypes[c] == object:\n        le.fit(df[c].astype(str))\n        df[c] = le.transform(df[c].astype(str))","b4ea3844":"y=df['stroke']\nX=df.drop(['stroke','id'],axis=1)\nprint(y.value_counts())","5e899b02":"from sklearn import preprocessing\nnorm = preprocessing.StandardScaler()\nndf=norm.fit_transform(X)\nX = pd.DataFrame(ndf, index=X.index, columns=X.columns)\nX.head(10)","155291c5":"from imblearn.over_sampling import ADASYN \nX_resampled, y_resampled = ADASYN().fit_resample(X, y)","2ed3157d":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test =train_test_split(X_resampled,y_resampled,train_size=0.7, random_state=42)","3b601713":"from sklearn.model_selection import RandomizedSearchCV\nimport lightgbm as lgb\nparams = {\n    'learning_rate': [0.05,0.01,0.0001],\n    'num_leaves': [90,140,200],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['binary'],\n    'max_depth' : [3,4,5,6,7,8],\n    'random_state' : [42], \n    'colsample_bytree' : [0.5,0.6,0.7,0.8,1.0],\n    'subsample' : [0.5,0.6,0.7,0.8,1.0],\n    'min_split_gain' : [0.01],\n    'min_data_in_leaf':[10],\n    'metric':['auc']\n    }\nclf = lgb.LGBMClassifier()\nRSCV = RandomizedSearchCV(clf,params,verbose=3,cv=10,n_jobs = -1,n_iter=10)\nRSCV.fit(X_train,y_train)","630fa84e":"y_pred=RSCV.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","18358020":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred)","11f155be":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(RSCV,X_test,y_test)","e24fef6d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = { \n    'n_estimators': [100, 200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nrfc=RandomForestClassifier()\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=3,n_jobs=-1,verbose=True)\nCV_rfc.fit(X_train, y_train)\ny_predict = CV_rfc.predict(X_test)\n\nprint(classification_report(y_test, y_predict))\n\n","dbdc7072":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(CV_rfc,X_test,y_test)","88147052":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nxgb = XGBClassifier(learning_rate=0.05,n_estimators=10000,seed=2019,reg_alpha=5,eval_metric='auc',tree_method='hist',\n                    objective='binary:logistic',\n                    silent=True, nthread=1)\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\nfolds = 3\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\nrandom_search.fit(X_train, y_train)\n","b8384456":"from sklearn.ensemble import RandomForestClassifier\n\ny_predict = random_search.predict(X_test)\n\nprint(classification_report(y_test, y_predict))\n\nplot_confusion_matrix(random_search,X_test,y_test)","c12f72dc":"I decided to easily and simply fill in the missing values on mean. And also convert categorical data using Label Encoder. And also get rid of the imbalance with ADASYN. The following classification algorithms were selected: XGBClassifier, LGBMClassifier, and RandomForestClassifier","134b3cd1":"In total, we have 9 variables and 1 target. The fact of a stroke is affected by the following parameters:\n1) Gender\n2) Average glucose level\n3) BMI\n4) Smoking\n5) Type of work\n6) Was the person married (amazing, isn't it?)\n7) Where does the person live\n8) Does a person suffer from hypertension\n9) Does the patient have a history of heart disease.\nLet's do a simple visual analysis of our data.","49202dec":"From the work done, we can conclude that the Random Forest Classifier algorithm did the worst. XGBClassifier and LGBMClassifier are approximately equal. I would prefer LGBMClassifier, because it is worse at detecting sick people.","879c2fa7":"Let's import the main libraries and see what kind of data we have.","5cd010d3":"Stroke \u2014 an acute violation of the blood supply to the brain, characterized by a sudden (within a few minutes, hours) the appearance of focal and \/ or general cerebral neurological symptoms that persist for more than 24 hours or lead to the death of the patient in a shorter period of time due to cerebrovascular pathology.","4640026a":"We were able to understand the distribution of our data. We also have an imbalance in observations: very few people have heart problems, hypertension, and in principle, few people with a stroke in our data. We also found that there is a relationship between age and average glucose levels. And also between age and bmi. Let's also take a closer look at who is more susceptible to stroke.","cf4499d7":"![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)"}}