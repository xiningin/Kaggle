{"cell_type":{"06640f98":"code","c979c669":"code","f2890542":"code","f2acfe4d":"code","5cb95c80":"code","53932c2e":"code","b0e6dda2":"code","5388e92a":"code","bb711a0a":"code","bb0136ee":"code","159bea76":"code","50430cab":"code","1bdb301f":"code","12451687":"code","ba631f63":"code","1e78fe53":"code","6b1b0497":"code","70681dfe":"code","d9fc361d":"code","a3a920c3":"code","7a821114":"code","e0bed19f":"code","5408e7a5":"code","2ba553a5":"code","44d240a1":"code","d72c852f":"code","97dfe67c":"code","0b74ee98":"code","73e41c4b":"code","97972f56":"code","91885a87":"code","73eceb73":"code","32c66320":"code","496d9981":"code","a348916f":"code","981d8075":"code","eb32f800":"code","274877e4":"code","eba64d0e":"code","a3b10d77":"code","3f02e87c":"code","6fa90129":"code","5ebce93e":"code","3b3da87b":"code","50815cd3":"code","67fb252f":"code","6b33cc56":"code","a0122870":"code","12299563":"code","a38557b2":"code","ba68546e":"markdown","0ff453d5":"markdown","688404b1":"markdown","7f16c870":"markdown","818b9641":"markdown","f98c03c7":"markdown","199a6b6d":"markdown","59aa38e5":"markdown","7914af16":"markdown","1dfa4c56":"markdown","b1e3249a":"markdown","17370777":"markdown","4e66833d":"markdown","29466b6c":"markdown","5e94e8ec":"markdown","65076fa8":"markdown","815e97d1":"markdown","418aaa2a":"markdown","c824b7fb":"markdown","5ad1545f":"markdown"},"source":{"06640f98":"from IPython.display import Image\nImage(\"..\/input\/hov-alvin\/hov_alvin_1.jpg\")","c979c669":"# Importing necessary libraries:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\nimport requests\nimport datetime\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","f2890542":"# Defining the url where the data is stored:\nurl_list = ['http:\/\/dsg.whoi.edu\/divelog.nsf\/By%20Pilot%20Name?OpenView&Start=1',\n            'http:\/\/dsg.whoi.edu\/divelog.nsf\/By%20Pilot%20Name?OpenView&Start=5065&Count=6000']\n\n# Create an empty list for storage:\ndata1 = []\n\n# Using for loops to collects columns of data from the urls:\nfor each in url_list:\n    r = requests.get(each)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    \n    for tr in soup.find_all('tr'):\n        col = 0\n        for td in tr.find_all('td'):\n            td_text = td.get_text().strip()\n            if col == 0:\n                pilot = td_text\n            if col == 1:\n                dive_nu = td_text\n            if col == 2:\n                date = td_text\n            if col == 3:\n                op_area = td_text\n            if col == 4:\n                lat = td_text\n            if col == 5:\n                lon = td_text\n            if col == 6:\n                depth = td_text\n            if col == 7:\n                obs1 = td_text\n            if col == 8:\n                obs2 = td_text\n            if col == 9:\n                dive_time = td_text\n            if col == 10:\n                bottom_time = td_text if td_text else np.nan\n        \n                data1.append({'pilot': pilot,\n                             'dive_nu': dive_nu,\n                             'date': date,\n                             'op_area': op_area,\n                             'lat': lat,\n                             'lon': lon,\n                             'depth': depth,\n                             'obs1': obs1,\n                             'obs2': obs2,\n                             'dive_time': dive_time,\n                             'bottom_time': bottom_time})\n            col += 1\n\n# Turn collected data into Pandas Dataframe: \ndf1 = pd.DataFrame(data1, columns=['pilot', 'dive_nu', 'date', 'op_area', \n                                   'lat', 'lon', 'depth', 'obs1', 'obs2', 'dive_time', 'bottom_time'])","f2acfe4d":"# Erasing duplicated values:\ndf1.drop_duplicates(subset='dive_nu', keep='first' , inplace=True)\nprint('Is there any duplicated value? ', df1.duplicated().sum())\n\n# Let's see what we have collected:\ndf1.head()","5cb95c80":"# Urls for the summary table:\nurl_list = ['http:\/\/dsg.whoi.edu\/divelog.nsf\/Summary?OpenView&Start=1', \n            'http:\/\/dsg.whoi.edu\/divelog.nsf\/Summary?OpenView&Start=5065']\n\n# Create an empty list for storage:\ndata2 = []\n\n# Collect data by for loops:\nfor each in url_list:\n    r = requests.get(each)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    \n    for tr in soup.find_all('tr'):\n        col = 0\n        for td in tr.find_all('td'):\n            td_text = td.get_text().strip()\n            if col == 0:\n                date = td_text\n            if col == 1:\n                dive_nu = td_text\n            if col == 2:\n                cruise = td_text\n            if col == 3:\n                leg = td_text\n            if col == 4:\n                chief_sci = td_text\n                \n                data2.append({'date': date,\n                              'dive_nu': dive_nu,\n                              'cruise': cruise,\n                              'leg': leg,\n                              'chief_sci': chief_sci})\n            col += 1\n\n# Convert the data into Pandas Dataframe:\ndf2 = pd.DataFrame(data2, columns=['date', 'dive_nu', 'cruise', 'leg', 'chief_sci'])\n\n# Erase the first row from the dataset which is the website menu items: \ndf2 = df2[1:5066]\n\n# Let's see what we have collected:\ndf2.head()","53932c2e":"# Urls for the summary table:\nurl_list = ['http:\/\/dsg.whoi.edu\/divelog.nsf\/By%20Dive%20Number\/Date?OpenView',\n            'http:\/\/dsg.whoi.edu\/divelog.nsf\/By%20Dive%20Number\/Date?OpenView&Start=5065']\n\ndata3 = []\n\n# Collecting data from the web site:\nfor each in url_list:\n    r = requests.get(each)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    \n    for tr in soup.find_all('tr'):\n        col = 0\n        for td in tr.find_all('td'):\n            td_text = td.get_text().strip()\n            if col == 0:\n                dive_nu = td_text\n            if col == 1:\n                date = td_text\n            if col == 2:\n                purpose = td_text\n                \n                data3.append({'dive_nu': dive_nu, 'date': date, 'purpose': purpose})\n            col += 1\n\n# Converting data into Pandas Dataframe:            \ndf3 = pd.DataFrame(data3, columns=['dive_nu', 'date', 'purpose'])\n\n# Erase website menu items from the dataset \ndf3 = df3[1:5066]","b0e6dda2":"# Let's see our three datasets:\ndisplay(df1.head(2))\ndisplay(df2.head(2))\ndisplay(df3.head(2))","5388e92a":"# Merging the datasets:\ndata_raw = pd.merge(df1, df2, on=['date', 'dive_nu'], how='outer')\ndata_raw = pd.merge(data_raw, df3, on=['date', 'dive_nu'], how='outer')\n\n# Changing the order of the columns:\ndata_raw = data_raw[['dive_nu', 'date', 'op_area', 'lat', 'lon', 'cruise', \n                     'leg', 'purpose', 'depth', 'dive_time', 'bottom_time',\n                     'chief_sci', 'pilot', 'obs1', 'obs2']]\n\ndisplay(data_raw.head(3))","bb711a0a":"# data_raw.to_csv(r'\/Users\/melihakdag\/Desktop\/Data Science\/alvin_dive_logs\/alvin_data_raw.csv')","bb0136ee":"data_raw.info()","159bea76":"# Changing 'date' column type into datetime:\ndata_raw['date'] = pd.to_datetime(data_raw['date'])\ndata_raw.tail(3)","50430cab":"# Define the function:\ndef change_year(x):\n    if x.year > 2060:\n        year = x.year - 100\n        \n    else:\n        year = x.year\n\n    return datetime.date(year,x.month,x.day)\n\n# Apply the function:\ndata_raw['date'] = data_raw['date'].apply(change_year)\n\n# Let's see if the dates are correct now:\ndata_raw.tail(3)","1bdb301f":"# Changing 'dive_time' and 'bottom_time' column types into minutes:\ndef minutes(x):\n    if type(x) == str:\n        return int(x[:-3])*60 + int(x[-2:])\n    else:\n        return np.nan\n        \ndata_raw['bottom_time'] = data_raw['bottom_time'].apply(minutes)\ndata_raw['dive_time'] = data_raw['dive_time'].apply(minutes)","12451687":"# Changing 'dive_nu', 'depth' columns' types into integer:\nto_integer = lambda x: int(x)\n\ncolumns = ['dive_nu', 'depth']\n\nfor each in columns:\n    data_raw[each] = data_raw[each].apply(to_integer)","ba631f63":"# Splitting latitude and longitude strings by '-':\ndata_raw[['lat01', 'lat02']] = data_raw['lat'].str.split(pat=\"-\", expand=True)\ndel data_raw['lat']\n\ndata_raw[['long01', 'long02']] = data_raw['lon'].str.split(pat='-', expand=True)\ndel data_raw['lon']","1e78fe53":"data_raw.head(2)","6b1b0497":"# Let's see if there is any missing values in the new features:\ndata_raw[['lat01', 'lat02', 'long01', 'long02']].isnull().values.any()","70681dfe":"# Which columns have the missing values? \nprint('lat01 any NaN : ', data_raw['lat01'].isnull().values.any())\nprint('lat02 any NaN : ', data_raw['lat02'].isnull().values.any())\nprint('long01 any NaN: ', data_raw['long01'].isnull().values.any())\nprint('long02 any NaN: ', data_raw['long02'].isnull().values.any())","d9fc361d":"# Find those missing values in long02 column:\ndata_raw[data_raw['long02'].isnull() == True]","a3a920c3":"# It looks like the missing values are caused by a typo. Let's fix the typos: \ndata_raw.iloc[3508, 15] = data_raw.iloc[3508, 15].replace('95.28.5W', '95')\ndata_raw.iloc[3508, 16] = '28.5W'\n\ndata_raw.iloc[3514, 15] = data_raw.iloc[3514, 15].replace('95.33.0W', '95')\ndata_raw.iloc[3514, 16] = '33.0W'\n\ndata_raw.iloc[[3508, 3514]]","7a821114":"# Changing latidude and longitude strings into numbers:\ndata_raw['lat01'] = data_raw['lat01'].apply(lambda x: float(x))\ndata_raw['long01'] = data_raw['long01'].apply(lambda x: float(x))","e0bed19f":"# Changing latitude and longitude degree signs according to being on the Southern hemisphere or having a Western longitude:\ndef change_sign(x, y):\n    if y[-1] == 'S' or y[-1] == 'W':\n        return x*-1\n    else:\n        return x\n    \ndata_raw['lat01'] = data_raw.apply(lambda x: change_sign(x.lat01, x.lat02), axis=1)\ndata_raw['long01'] = data_raw.apply(lambda x: change_sign(x.long01, x.long02), axis=1)\n\ndata_raw.head(3)","5408e7a5":"# Changing Decimal Minutes to Decimal Degrees:\ndef to_DD(x):\n    if x[-1:] == 'N':\n        return float(x[:-1])\/60\n    if x[-1:] == 'S':\n        return float(x[:-1])\/-60\n    if x[-1:] == 'E':\n        return float(x[:-1])\/60\n    if x[-1:] == 'W':\n        return float(x[:-1])\/-60\n\n# Finding Decimal Degrees:\ndata_raw['lat02'] = data_raw['lat02'].apply(to_DD)\ndata_raw['long02'] = data_raw['long02'].apply(to_DD)\n\n# Saving the DD latidude and longitudes and deleting other coordinate datas:\ndata_raw['lat(DD)'] = data_raw['lat01'] + data_raw['lat02']\ndata_raw['long(DD)'] = data_raw['long01'] + data_raw['long02']\ndata_raw.drop(['lat01', 'lat02', 'long01', 'long02'], axis=1, inplace=True)","2ba553a5":"# Data inputs are written in different styles:\ndata_raw[['op_area','purpose', 'chief_sci']].sample(5)","44d240a1":"# Let's make the string styles all the same:\ndata_raw[['op_area','purpose', 'chief_sci']] = data_raw[['op_area','purpose', 'chief_sci']].apply(lambda x: x.str.title())","d72c852f":"# Create a copy of the final data set:\nalvin_dives = data_raw.copy()\n\n# Finally we can export this cleaned dataset:\n#alvin_dives.to_csv(r'\/Users\/melihakdag\/Desktop\/Data Science\/alvin_dive_logs\/alvin_data_cleaned.csv')\n\nalvin_dives.head()","97dfe67c":"import plotly.express as px\nimport folium\nfrom folium import Circle","0b74ee98":"# Defining the borders of our map:\nsw = alvin_dives[['lat(DD)', 'long(DD)']].min().values.tolist()\nne = alvin_dives[['lat(DD)', 'long(DD)']].max().values.tolist()\n\n# Creating the basemap:\nworld_map = folium.Map(zoom_start = 9)\n# Default zoom setting with the boundaries:\nworld_map.fit_bounds([sw, ne])\n\n# Creating circle marks on the basemap:\nfor lat, long, date, depth in zip(alvin_dives['lat(DD)'], alvin_dives['long(DD)'], \n                                       alvin_dives['date'], alvin_dives['depth']): \n    folium.Circle(location = [lat, long], \n                  radius = 20).add_child(folium.Popup(str(date.year) + ', ' + str(depth) + ' m')).add_to(world_map)\n    \nworld_map","73e41c4b":"# We can see total number of dives for each year.\n# Total dives for the first ten years:\ndive_years = pd.DataFrame(alvin_dives.groupby(alvin_dives['date'].map(lambda x: x.year)).dive_nu.count())\ndive_years.reset_index(inplace=True)\ndive_years.rename(columns={'date':'year', 'dive_nu':'total_dive_nu'}, inplace=True)\ndive_years.head()","97972f56":"fig = px.line(dive_years, \n              x=\"year\", \n              y=\"total_dive_nu\", \n              title='Total Number of Dives in Years')\n\nfig.show()","91885a87":"# How many different purposes are there?\nprint('Alvin have dived for %d different purposes.' %len(alvin_dives['purpose'].unique()))","73eceb73":"# Let's see the distribution of the purposes:\npurpose_count = pd.DataFrame(alvin_dives['purpose'].value_counts())\npurpose_count.reset_index(inplace=True)\npurpose_count.rename(columns = {'index':'purpose', 'purpose':'total_nu'}, inplace=True)\npurpose_count.head(10)","32c66320":"fig = px.bar(purpose_count.head(20),\n             x='purpose',\n             y='total_nu',\n             title='Top 20 Purposes of the Dives')\n\nfig.update_traces(marker=dict(color=\"RoyalBlue\"))\n\nfig.show()","496d9981":"dive_depths = alvin_dives[['date', 'dive_nu', 'depth', 'dive_time']]\ndive_depths['depth'] = dive_depths['depth'].map(lambda x: -(x))","a348916f":"fig = px.scatter(dive_depths,\n                 x = 'date', \n                 y = 'depth',\n                 title = \"Alvin's Dive Dates & Depths & Times\",\n                 color='dive_time',\n                 hover_data = ['dive_nu'])\nfig.show()","981d8075":"# Group by purpose and find the average depth for each purpose:\npurpose_avg_depth = pd.DataFrame(alvin_dives.groupby(['purpose']).depth.median())\npurpose_avg_depth.reset_index(inplace=True)\npurpose_avg_depth.rename(columns={'depth':'avg_depth'}, inplace=True)","eb32f800":"# Combine the purpose count and the average depth tables:\npurpose_depth_count = pd.merge(purpose_count, purpose_avg_depth, on=['purpose'])\npurpose_depth_count['avg_depth'] = purpose_depth_count['avg_depth'].map(lambda x: -(x))\npurpose_depth_count.head(10)","274877e4":"fig = px.scatter(purpose_depth_count.head(20), \n                 x = 'purpose',\n                 y = 'avg_depth',\n                 size = 'total_nu',\n                 title = 'Average Depths for Purposes')   \nfig.show()","eba64d0e":"max_dive_time = pd.DataFrame(alvin_dives.groupby(['depth'])['dive_time'].max())\nmax_dive_time.reset_index(inplace=True)\nmax_dive_time.rename(columns={'dive_time': 'max_dive_time'}, inplace=True)\nmax_dive_time['depth'] = max_dive_time['depth'].map(lambda x: -(x)) \nmax_dive_time.tail()","a3b10d77":"fig = px.scatter(max_dive_time, \n                 x = 'max_dive_time',\n                 y = 'depth',\n                 color = 'depth',\n                 title = 'Maximum Dive Times for Depths')\nfig.show()","3f02e87c":"# Let's extract the pilots, chief scientists, observers,\ndivers = alvin_dives[['pilot', 'chief_sci', 'obs1', 'obs2', 'date', 'dive_nu', 'purpose']]\ndivers['date'] = divers['date'].map(lambda x: x.year)","6fa90129":"print('%d talented people had chance to work as a pilot for ALVIN since 1964.' %len(divers['pilot'].unique()))","5ebce93e":"divers_dives = pd.DataFrame(divers.groupby(['pilot', 'date']).dive_nu.count())\ndivers_dives.reset_index(inplace=True)\ndivers_dives.rename(columns={'date': 'year', 'dive_nu': 'total_dives'}, inplace=True)\ndivers_dives.sort_values(by='year', inplace=True)\ndivers_dives.head()","3b3da87b":"fig = px.bar(divers_dives, \n             y = 'pilot',\n             x = 'total_dives',\n             color = 'year',\n             title = 'Pilots and Total Dive Numbers')\nfig.show()","50815cd3":"fig = px.scatter(divers_dives, \n                 y = 'pilot',\n                 x = 'year',\n                 color = 'year',\n                 title = 'Pilots and Years')\nfig.show()","67fb252f":"chief_scientist = pd.DataFrame(divers.groupby(['chief_sci', 'date']).dive_nu.count())\nchief_scientist.reset_index(inplace=True)\nchief_scientist.rename(columns={'date': 'year', 'dive_nu': 'total_dives'}, inplace=True)\nchief_scientist.sort_values(by='year', ascending=True, inplace=True)","6b33cc56":"chief_scientist.head()","a0122870":"fig = px.scatter(chief_scientist,\n                 x = 'year',\n                 y = 'chief_sci',\n                 color = 'year',\n                 size = 'total_dives',\n                 title = 'Chief Scientists and Research Years')\nfig.show()","12299563":"# Cruises and total number of dives:\ncruise_dives = pd.DataFrame(alvin_dives.groupby('cruise').dive_nu.count().sort_values(ascending=False))\ncruise_dives.rename(columns={'dive_nu':'total_dives'}, inplace=True)\n\n# Getting the cruises which have more than 50 dives:\ncruise_dives50 = cruise_dives[cruise_dives.values > 50]","a38557b2":"fig = px.pie(cruise_dives50,\n             names = cruise_dives50.index,\n             values = 'total_dives',\n             title = 'Cruises With More Than 50 Dives')\nfig.show()","ba68546e":"Now I am going to save the raw data before any data preparation. So one can use it to practice for data preparation and cleaning skills.","0ff453d5":"# HOV ALVIN DIVE LOGS\n\nWoods Hole Institute's (WHOI) Human Occupied Vehicle HOV ALVIN allows in-situ data collection and observation by two scientists of the seafloor and water column to depths reaching 4,500 meters on dives lasting up to ten hours (https:\/\/ndsf.whoi.edu\/alvin\/).\n\nIn 2018, when I've started to learn data science through online courses, I used Woods Hole Institute's (WHOI) Human Occupied Vehicle (HOV) Alvin's dive logs for exploratory data analysis (EDA) and visualisation practices.\n\nThe data which I studied back then has Alvin's dive logs up to 2017. One can reach that data set through: https:\/\/www.kaggle.com\/sauuyer\/alvin-dives\n\nSo I decided to use my **web scraping** and **data preparation** skills to update the data set.\n\nOne can examine WHOI's data through this link: https:\/\/ndsf.whoi.edu\/alvin\/dive-log\/\n\n## SKILLS USED IN THIS NOTEBOOK:\n\n* Web scraping; which means using Python and Beautiful Soup library to collect the data from a web site,\n* Data cleaning and data preparation skills,\n* Data visualization skills for graphs and analyze the data,\n* Geospatial visualization skills to demonstrate dive destinations on the map.\n\n### Check my dashboard created by the final data:\nhttps:\/\/public.tableau.com\/profile\/melih.akdag#!\/vizhome\/HOVALVINDiveData\/ALVINDives","688404b1":"### 5.3. Scientific purposes of the dives: ","7f16c870":"## 3. BY DIVE NUMBER\/DATE TABLE:\n\nIn this table we can find the information about purpose of the each dive.","818b9641":"### 5.5. Depths and the purposes of the dives:","f98c03c7":"### 5.9. Cruises and Dive Numbers:","199a6b6d":"### 5.1. ALVIN's dive destinations:","59aa38e5":"### 5.2. ALVIN's dive history (Dives & Dates Distribution):","7914af16":"### 5.8. Chief Scientists:","1dfa4c56":"Strings in the columns are written in different formats like upper case, lower case, all capitals.","b1e3249a":"## 1. ALVIN'S DIVE LOG BY PILOTS TABLE:\n\nFirst I am going to scrape the dives by pilot table.","17370777":"Now it's time to prepare the latitude and longitude columns:","4e66833d":"### 5.4. Depths of the dives:","29466b6c":"### 5.7. ALVIN's pilots:","5e94e8ec":"# 5. VISUALIZATION \n\nNow we have a clean and an updated ALVIN dive log dataset, we can visualize and examine the dataset in order to extract some insights. ","65076fa8":"### 5.6. Depths and Maximum Dive times:","815e97d1":"## 2. ALVIN'S DIVE LOG SUMMARY TABLE:\n\nIn the summary table we can find the information about the chief scientists, cruise and cruise leg numbers.","418aaa2a":"As seen from above the date 64 (1964) converted into 2064. \n\nSo we need to use a function to correct the years:","c824b7fb":"## 4. COMBINING TABLES:\n\nI am going to merge three tables on the date of the dive and the dive number columns, in order to create my final dataset.","5ad1545f":"## 4. DATA CLEANING AND PREPARATION:\n\nNow I have collected my data. But I need to examine my data for data types and prepare some of the columns for later analysis.\n\nFor example I want my latitude and longitude columns in decimal degrees for geospatial visualization. I want the dive_time column as minutes instead of hour & minutes style. I want the dive number and depth column types as integer."}}