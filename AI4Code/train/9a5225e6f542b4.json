{"cell_type":{"7e4f905d":"code","ce1a5f73":"code","b24dafe2":"code","8d92d1ae":"code","e4fcf9c8":"code","d3a42d3a":"code","96bea92d":"code","60879c1a":"markdown","5dbc1e2c":"markdown","116f2c51":"markdown"},"source":{"7e4f905d":"N_EPOCHS = 1\n\nmname = \"vinai\/bertweet-large\"\nCFG = {\"lr\": 1e-5, \"batch_size\": 4, \\\n       \"grad_acc_steps\": 8, \"fp16\": False, \\\n       \"eval_steps\": 250, \"wandb_logging_steps\": 30}\n\nrun_id = \"bertweet_lg_better_baseline\"\nkaggle_source = \"jdoesv\/baseline-model-bertweet-regression, V2\"","ce1a5f73":"import pandas as pd\n\n#Sampled in source NB. TODO: Remove that and use weighted DL here. \ntraindf = pd.read_csv(\"..\/input\/baseline-data-prep\/cleaned_train_actual.csv\")\ntraindf[\"normed_text\"].fillna(\"\", inplace=True)\ntraindf = traindf.sample(frac=1).reset_index(drop=True)\ntraindf = traindf[[\"normed_text\", \"new_wscore\"]] #Now we need to sample\ntraindf.rename(columns={\"normed_text\": \"sentence\", \"new_wscore\": \"label\"}, inplace=True)\ndisplay(traindf[\"label\"].describe())\n\nvaliddf = pd.read_csv(\"..\/input\/wranged-validation-data\/eval_regression.csv\")\nvaliddf = validdf[[\"tclean\", \"reg_rank\"]]\nvaliddf[\"reg_rank\"] = (validdf[\"reg_rank\"]-validdf[\"reg_rank\"].min())\/(validdf[\"reg_rank\"].max()-validdf[\"reg_rank\"].min())\nvaliddf.rename(columns={\"tclean\": \"sentence\", \"reg_rank\": \"label\"}, inplace=True)\ndisplay(validdf[\"label\"].describe())","b24dafe2":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(mname)\nmodel = AutoModelForSequenceClassification.from_pretrained(mname, num_labels=1)","8d92d1ae":"from datasets import Dataset\n\ntrain_ds = Dataset.from_pandas(traindf)\nvalid_ds = Dataset.from_pandas(validdf)\n\nkey1, key2 = \"sentence\", None\ndef preprocess_function(examples):\n    if key2 is None:\n        return tokenizer(examples[key1], padding=True, truncation=True, max_length=512) \n    return tokenizer(examples[key1], examples[key2], padding=True, truncation=True, max_length=512) #We allow truncation. So encoded ds and actual ds should share the same size. \n\ntrain_ds = train_ds.map(preprocess_function, batched=True)\nvalid_ds = valid_ds.map(preprocess_function, batched=True)","e4fcf9c8":"from kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nwandbkey = user_secrets.get_secret(\"wandbkey\")\nwandb.login(key=wandbkey)","d3a42d3a":"wandb_kwargs = {\"project\":\"kaggle_jigsaw\", \"tags\": [\"bertweet-lg\", \"baseline\"], \"name\": run_id, \"reinit\": True, \"notes\": kaggle_source}","96bea92d":"from transformers import TrainingArguments, Trainer, default_data_collator\ndata_collator = default_data_collator\n\nrun = wandb.init(**wandb_kwargs)\n\nargs = TrainingArguments(\n    run_id,\n    evaluation_strategy=\"steps\",\n    eval_steps=CFG[\"eval_steps\"],\n    save_strategy = \"steps\",\n    save_steps=CFG[\"eval_steps\"],\n    learning_rate=CFG[\"lr\"],\n    per_device_train_batch_size=CFG[\"batch_size\"],\n    gradient_accumulation_steps=CFG[\"grad_acc_steps\"],\n    per_device_eval_batch_size=64,\n    num_train_epochs=N_EPOCHS,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    report_to=[\"wandb\"],\n    run_name=run_id,\n    logging_steps=CFG[\"wandb_logging_steps\"],\n    #seed=4142\n)\n\ntrainer = Trainer(\n    args=args,\n    model=model,\n    train_dataset=train_ds,\n    eval_dataset=valid_ds,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\nrun.finish() ","60879c1a":"# Approach\n\nWill work to evaluate the model. Last competition's public NB that uses combined submission also has a binary output, and a score of 0.97. We'll use those probability scores with weights for each comment type to generate a final regressor score. Bertweet model will get preprocessed inputs (except url and twitter handle preprocessing) and a regressor head on top of its model is finetuned.\n\nThis is Roberta. So we can easily squeeze in 5 checkpoints in 20GB. ","5dbc1e2c":"## Training prep\n\nPitfalls\n1. Token length > 512. For now, adopt truncation\n2. Other language data. For now, ignore. \n3. Not a rankable. TODO: Custom loss function: Take 10 pivot elements (1 per decimal range). Contrastive loss against each of these pivot and aggregate to generate loss fn. Should teach the model a measure of relative-toxicity. With 50 steps itself, model is at a 0.03 MSE loss (=== 0.17 RMSE) already. For now ignore.","116f2c51":"## Data prep\n- Training data: Weighted scores from that prediction notebook\n- Validation data: Jigsaw validation data min-max scaled.\n- Both have sentence, label as inputs"}}