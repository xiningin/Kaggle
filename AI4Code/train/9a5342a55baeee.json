{"cell_type":{"d2ec14fe":"code","9ea3222f":"code","4d01346c":"code","e385c68a":"code","e70bc858":"code","47913419":"code","7aba8262":"code","4a190d88":"code","d9f83bce":"code","e9b45d7d":"code","1c899d01":"code","635e3421":"code","df03f0ca":"code","f51231b5":"code","dc37f11d":"code","e30aec71":"code","16582597":"code","b3806b18":"code","eb5bc87e":"code","2778e09b":"code","395deb09":"markdown","fb8c2e6a":"markdown","a0b32d5e":"markdown","00f76dcb":"markdown","b565fc39":"markdown","2e2e4c94":"markdown","591b5ee6":"markdown","0e737b75":"markdown","5881d8c2":"markdown","fb72177b":"markdown","c4f21c69":"markdown","92fe3bd1":"markdown","f98fb5c4":"markdown","1cbe843a":"markdown","ac273ab8":"markdown","20a63f54":"markdown","454ece9e":"markdown","2aa124c1":"markdown","3b327be1":"markdown","98945a8f":"markdown","74314668":"markdown","fe22c5fd":"markdown","746fd568":"markdown","ec7b5ed7":"markdown","8257256b":"markdown","c0eefaca":"markdown"},"source":{"d2ec14fe":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nwarnings.simplefilter(\"ignore\")","9ea3222f":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        data = pd.read_csv(os.path.join(dirname, filename))\ndata.head()","4d01346c":"df = data.copy()\ndata.describe().T.style.bar()","e385c68a":"data.info()","e70bc858":"print('We have', data.shape[0], 'Rows and', data.shape[1], 'features')","47913419":"data.columns","7aba8262":"plt.figure(figsize=(23, 3))\nsns.heatmap(data.isnull(), yticklabels=False, cbar=True)","4a190d88":"num_col = data._get_numeric_data().columns.tolist()\nprint('numeric features:', num_col)","d9f83bce":"cat_col = set(data.columns) - set(num_col)\nprint('categorical features:',cat_col)","e9b45d7d":"for i in cat_col:\n    le = LabelEncoder()\n    n = str(i) + '_n'\n    df[n] = le.fit_transform(df[i])\n    del df[i]\ndf.head()","1c899d01":"plt.figure(figsize=(10, 5))\nfor i, j in enumerate(df.keys()):\n    plt.subplot(2, 2+1, i+1)\n    plt.boxplot(df[j], 0,'o',showbox=True,\n            showfliers=True, showcaps=True, showmeans=True)\n    plt.title(j + ' - box plot')","635e3421":"X = (df.drop(['Drug_n'], axis=1)).values\ny = (df.Drug_n).values\nclass_n = np.unique(y)\nprint('X shape:', X.shape, 'y shape:', y.shape, 'class labels:', class_n)","df03f0ca":"fig = plt.figure(figsize=(20, 3))\nax = plt.axes()\nplt.title('class Distribution')\nsns.histplot(y, kde=True, color='gray')\nplt.xlabel('Drug Type')\nplt.ylabel('Numbers')\nplt.xticks(class_n)\nplt.savefig('hist.jpg', dpi=300)","f51231b5":"y.shape","dc37f11d":"K = 10\nN = 100\n\nerr_mart = np.zeros((K, N))\npred_mart = np.zeros((y.shape[0], 100))\npred_t = np.zeros_like(y)\nacc = []\n\nkfold = StratifiedKFold(n_splits=K, shuffle=True, random_state=1)\n\nfor k, (train_index, test_index) in enumerate(kfold.split(X, y)):\n    x_train, x_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    mart = GradientBoostingClassifier(max_depth=2,\n                                      subsample=0.75,\n                                      max_features=\"sqrt\",\n                                      learning_rate=0.025,\n                                      random_state=1,\n                                      n_estimators=100)\n    pipe = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", mart)])\n    pipe.fit(x_train, y_train)\n    pred_t[test_index] = pipe.predict(x_test)\n    acc.append(accuracy_score(y_test, pipe.predict(x_test)))\n    \n    mart.fit(x_train, y_train)\n    \n\n    for i, pred in enumerate(mart.staged_predict(x_test)):\n        pred_mart[test_index, i] = pred","e30aec71":"print('Model average accuracy is:', '{0:.2f}%'.format(np.mean(acc, axis=0)))","16582597":"test_score_mart = np.empty((100))\nfor i in range(mart.n_estimators_):\n    test_score_mart[i] = accuracy_score(y, pred_mart[:, i])\n\n    \nplt.plot(test_score_mart, '-', label='Accuracy', linewidth=3, color='black')\nplt.xlabel('Boosting Iteration')\nplt.ylabel('accuracy')\nplt.legend(loc=0)\nplt.title('Base learners performance')\nplt.grid(True)\nplt.show()","b3806b18":"plt.plot(mart.train_score_, '-', label='Loss', linewidth=3, color='black')\nplt.xlabel('Boosting Iteration')\nplt.ylabel('loss')\nplt.legend(loc=0)\nplt.title('loss curve')\nplt.grid(True)\nplt.show()","eb5bc87e":"plt.plot(mart.train_score_, '-', label='Loss', linewidth=3, color='blue')\nplt.plot(test_score_mart, '-', label='Accuracy', linewidth=3, color='red')\nplt.xlabel('Boosting Iteration')\nplt.legend(loc=0)\nplt.title('Model performance')\nplt.grid(True)\nplt.show()","2778e09b":"cf = confusion_matrix(y, pred_t)\nsns.heatmap(cf, cmap='PuBu', annot=True, fmt='0.1f')\nplt.xlabel('Predicted values')\nplt.ylabel('True labels')\nplt.title('MART')","395deb09":"<h4> Returning the numeric features <\/h4>","fb8c2e6a":"### 3.4.3 loss curve\n<h4> in train loss for each boosting iteration on the in-bag sample. <\/h4>","a0b32d5e":"<h4>For sampling and splitting the dataset, I used the stratified method to produce the test\/train indices to guarantee the same distribution of samples and built ten-folds for training the model.\nMoreover, the random seed is constant for re-producing the same result.\n<\/h4>","00f76dcb":"## 3.4. Evaluation","b565fc39":"\n<h3> Dataset <\/h3>\nIn the following notebook, I tried to analyze the attached dataset.\n\n<h3> Classification problem <\/h3>\nClassification problem\nTo classify the class labels I used the gradient Boosting model from Friedman's work and applied different metrics and evaluation methods to check the model performance.\n\n<h4> Metrics <\/h4>\nThe metric I used to measure the model performance is the accuracy of the classifier.\nThe followings are the evaluation methods;\n<ol>\n    <li> Accuracy <\/li>\n    <li> Staged Predict <\/li>\n    <li> Confusion matrix <\/li>\n<\/ol>\n\n<h3>splitting method<\/h3>\nK-Fold cross validation\n\n<\/br>\n\n<img src=\"https:\/\/cdn.mdedge.com\/files\/s3fs-public\/Image\/August-2018\/pills_520225198_web.jpg\" alt=\"Travel\" width=\"500\" height=\"600\">\n\n<hr>\nI tried to explain each cell in a markdown cell above.\n\n\n<h5>If you are interested in this problem and detailed analysis, you can copy this Notebook as follows<\/h5>\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1101107%2F8187a9b84c9dde4921900f794c6c6ff9%2FScreenshot%202020-06-28%20at%201.51.53%20AM.png?generation=1593289404499991&alt=media\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">","2e2e4c94":"## 1.3. Check the missing values","591b5ee6":"## 2.1. Identifying datatype","0e737b75":"<a id='Feature_engineering' > <\/a>\n# 2. Feature engineering ","5881d8c2":"<h4> Returning the categorical features <\/h4>","fb72177b":"### Model performance","c4f21c69":"### 3.4.4. Confusion Matrix","92fe3bd1":"## 2.2. One hot encoding\n<h4>Converting categorical features and class labels<\/h4>","f98fb5c4":"### 3.4.1. Model Accuracy","1cbe843a":"<a id=\u2019lib\u2019><\/a>\n# Import Libraries","ac273ab8":"## 1.1. Importing the dataset","20a63f54":"## 3.3. Training the model","454ece9e":"<a id='dataset'> <\/a>\n# 1. Dataset","2aa124c1":"## 3.1. Introducing the dependant and independent variables","3b327be1":"## 1.2. Data info","98945a8f":"### 3.4.2. Base learner accuracy\n<h4> Check the performance of each base learner in the ensemble model.\n <\/h4>","74314668":"<h4> As we can see, the Na_to_K might has outliers, but I will skip the outlier treatment in this notebook <\/h4>","fe22c5fd":"# About this Notebook\n## Author: Seyedsaman Emami\n\n<hr>","746fd568":"<a id='modeling'><\/a>\n# 3. Modeling","ec7b5ed7":"# Table of Contents\n* [Importing Libs](#lib)\n* [Exploring dataset](#dataset)\n* [Feature engineering](#Feature_engineering)\n* [Modeling](#modeling)\n","8257256b":"Hopefully, there is no Null value in the mentioned dataset.","c0eefaca":"## 3.2. Label histogram"}}