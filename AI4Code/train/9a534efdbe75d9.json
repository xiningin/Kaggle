{"cell_type":{"f9293954":"code","30b8f6c9":"code","108f4db5":"code","77a11d53":"code","f99223da":"code","e8995344":"code","11b0a1a3":"code","507cf5a6":"code","0b4c696c":"code","ffcc20c3":"code","0ce57670":"code","13a3eee0":"code","252a1a5d":"code","e28a380a":"code","64e61394":"code","aba38114":"code","580a1b14":"code","19716dd0":"code","feec63c8":"code","e69b0dfc":"code","af7dba2d":"code","5cd4b297":"code","e02d524a":"code","9b2a8746":"code","3bf32c5d":"code","a1246435":"code","d516d4ed":"code","258029de":"code","74dc9438":"code","5ff7f7e5":"markdown","c3311584":"markdown","e546bc5b":"markdown","f14dc048":"markdown","ee9bfb39":"markdown","7efbe5b8":"markdown","ed20cef4":"markdown","9e586155":"markdown","06b158da":"markdown","4b0b5043":"markdown","f76e5b9b":"markdown","0846c85f":"markdown","f6215c5e":"markdown","c6209d35":"markdown","d9d7abb2":"markdown","a4270f81":"markdown","6a15b261":"markdown","71d852bb":"markdown"},"source":{"f9293954":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Modeling and Prediction\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import confusion_matrix","30b8f6c9":"# Download training data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')","108f4db5":"# Display the first 5 rows of the training dataframe.\ntrain.head()","77a11d53":"# Display basic information about training data\ntrain.info()","f99223da":"# Download test data\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.tail(7)  # Display the 7 last rows of the training dataframe","e8995344":"# Display basic information about the test data\ntest.info()","11b0a1a3":"# Download submission sample file\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","507cf5a6":"# Display the 10 first rows of the submission dataframe\nsubmission.head(10)","0b4c696c":"def highlight(value):\n    # The painting of the cell in different colors depending on the value: \n    # >= 0.5 - palegreen, < 0.5 - pink\n    \n    if value >= 0.5:\n        style = 'background-color: palegreen'\n    else:\n        style = 'background-color: pink'\n    return style","ffcc20c3":"# Pivot table for input features 'Sex' and output feature 'Survived'\npd.pivot_table(train, values='Survived', index=['Sex']).style.applymap(highlight)","0ce57670":"# Pivot table for input features 'Pclass' and output feature 'Survived'\npd.pivot_table(train, values='Survived', index=['Pclass']).style.applymap(highlight)","13a3eee0":"# Replace missing embarked values to S\ntrain['Embarked'] = train['Embarked'].fillna('S')","252a1a5d":"# Pivot table for input features 'Embarked' and output feature 'Survived'\npd.pivot_table(train, values='Survived', index=['Embarked']).style.applymap(highlight)","e28a380a":"# Decision trees work with numbers, not words, so we must encode feature \"Sex\" by numbers\ntrain['Sex'].replace({'male': 0, 'female': 1}).head()","64e61394":"# Like at the previous sptep we must encode feature \"Embarked\" by numbers\ntrain['Embarked'].replace({'C': 0, 'Q': 1, 'S': 2}).head()","aba38114":"# Pivot table for input features 'Sex', 'Pclass', 'Embarked' and output feature 'Survived'\npd.pivot_table(train, values='Survived', index=['Sex', 'Pclass', 'Embarked']).style.applymap(highlight)","580a1b14":"# Copy commands (see above) to this function to calculate new features or process existing ones\n\ndef df_transform(df):\n    # FE for df\n    \n    # Encoding feature \"Sex\" by numbers\n    df['Sex'] = df['Sex'].replace({'male': 0, 'female': 1})\n    \n    # Encoding feature \"Embarked\" by numbers\n    df['Embarked'] = df['Embarked'].replace({'C': 0, 'Q': 1, 'S': 2})\n\n    # Select the main features\n    df = df[['Sex', 'Pclass', 'Embarked']]\n    \n    return df","19716dd0":"# Selecting a target featute and removing it from training dataset\ntarget = train.pop('Survived')","feec63c8":"# FE to training dataset\ntrain = df_transform(train)\n\n# Statistics of training dataset\ntrain.info()","e69b0dfc":"train","af7dba2d":"# FE to test dataset\ntest = df_transform(test)\ntest.info()","5cd4b297":"test","e02d524a":"# Select model as Decision Tree Classifier \n# \"Classifier\" because target has limited (integer) number of classes, in this case 2 classes = [0, 1] or [\"No Survived\", \"Survived\"]\n# For a small amount of data, it is better to choose a smaller parameter max_depth - from 3 to 5, let give 4\n# at a more complex Level we will teach the program to calculate it automatically\nmodel = DecisionTreeClassifier(max_depth=1, random_state=12)\n\n# Training model\nmodel.fit(train, target)","9b2a8746":"# Visualization - build a plot with Decision Tree\nplt.figure(figsize=(18,9))\nplot_tree(model, filled=True, rounded=True, class_names=[\"No Survived\", \"Survived\"], feature_names=train.columns) ","3bf32c5d":"# Prediction for training data\ny_train = model.predict(train).astype(int)","a1246435":"confusion_matrix(target, y_train)","d516d4ed":"# Prediction of target for test data\ny_pred = model.predict(test).astype(int)","258029de":"# Saving the result into submission file\nsubmission[\"Survived\"] = y_pred\nsubmission.to_csv('submission.csv', index=False) # Competition rules require that no index number be saved\n\n# Building the Histogram of predicted target values for test data\nsubmission['Survived'].hist()","74dc9438":"# Calculation of the mean value of forecasting data\nsubmission['Survived'].mean()","5ff7f7e5":"### Feature \"Pclass\"","c3311584":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA & FE](#3)\n1. [Modeling](#4)\n1. [Prediction & Submission](#5)","e546bc5b":"### Feature \"Embarked\"","f14dc048":"### My upgrade:\n* replaced feature \"Family_size\" to \"Pclass\";\n* replaced feature \"Age\" to \"Embarked\";\n* model tuning (changed parameter \"max_depth\" & \"random_state\");","ee9bfb39":"**It is important to make sure** that all features in the training and test datasets:\n* do not have missing values (number of non-null values = number of entries of index) \n* all features have a numeric data type (int8, int16, int32, int64 or float16, float32, float64).","7efbe5b8":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","ed20cef4":"The result of FE is usually combined into one function so that it is convenient to apply to different datasets","9e586155":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","06b158da":"### Feature \"Sex\"","4b0b5043":"**Thanks to [AI-ML-DS Training. L1A: Titanic - Decision Tree](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1a-titanic-decision-tree)**","f76e5b9b":"### Confusion matrix","0846c85f":"## 5. Prediction & Submission<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","f6215c5e":"Let's make the following assumptions:\n- **Women** have a better chance of surviving ==> feature \"**Sex**\" is important\n- Bigger **Pclass** have a better chance of surviving ==> the **Pclasss** is important\n- Diffetert **Embarked** have a diffetert chance of survival ==> **Embarked** is important","c6209d35":"## Competition [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic)","d9d7abb2":"## 3. EDA & FE<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","a4270f81":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","6a15b261":"### Decision Tree Visualization","71d852bb":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)"}}