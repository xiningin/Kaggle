{"cell_type":{"46b97f71":"code","9c8a0a42":"code","849658bc":"code","cb66ba19":"code","fae3247e":"code","3f29683f":"code","c14abce1":"code","11da6ba1":"code","5087980f":"code","7afe7fdf":"code","7ba14b6f":"code","bd81354b":"code","7984d869":"code","ca5e668c":"code","b3671380":"code","ae44e624":"code","0e3ad9ad":"code","6b8db238":"code","9c5ecb6b":"code","ca3d7b30":"code","7b6cb518":"code","c5e037a5":"code","38230275":"code","858ea9d5":"code","330080af":"code","1e74c604":"code","ae3feb40":"code","b2e0be95":"code","1f79edcc":"code","a91f2a7f":"code","733e50ab":"code","5f3807bf":"code","fc5f2dca":"markdown","f2f2039a":"markdown","f1862b2c":"markdown","07671405":"markdown","42efa86f":"markdown","d068ca1b":"markdown","3f3a002e":"markdown","b58b0d21":"markdown","2abf8b68":"markdown","89c15acb":"markdown","1af60e6f":"markdown","5d5e80a4":"markdown","a44b8234":"markdown"},"source":{"46b97f71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c8a0a42":"df = pd.read_csv(\"\/kaggle\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")","849658bc":"df.head()","cb66ba19":"df.describe()","fae3247e":"df.info()","3f29683f":"df.rename(columns = {\"Annual Income (k$)\": \"Income\", \n                     \"Spending Score (1-100)\":\"SpendingScore\"}, \n                                 inplace = True) ","c14abce1":"df.head()","11da6ba1":"f,ax1 = plt.subplots(figsize =(30,10))\nsns.pointplot(x='CustomerID',y='Income',data=df,color='lime',alpha=0.8)","5087980f":"g= sns.jointplot(\"SpendingScore\",\"Income\",data=df,size=5,ratio=3,color=\"r\")","7afe7fdf":"sns.catplot(x=\"Gender\",y=\"Income\",data=df, kind = \"bar\", height = 6)\nplt.show()","7ba14b6f":"sns.catplot(x=\"Gender\",y=\"SpendingScore\",data=df, kind = \"bar\", height = 6)\nplt.show()","bd81354b":"plt.hist(df[\"Age\"],bins=10)\nplt.title(\"Age by Histogram\")\nplt.show()","7984d869":"g= sns.jointplot(\"Age\",\"Income\",data=df,height=5,ratio=3,color=\"r\")","ca5e668c":"g = sns.jointplot(df.Age, df.SpendingScore, kind=\"kde\",height=7)\nplt.show()","b3671380":"sns.pairplot(df.iloc[:,1:])\nplt.show()","ae44e624":"sns.heatmap(df.iloc[:,1:].corr(),annot=True,fmt = \".2f\")\nplt.show","0e3ad9ad":"df_unsupervised = df.iloc[:,3:]","6b8db238":"from sklearn.cluster import KMeans\nwcss=[]\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(df_unsupervised)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1,15),wcss)\nplt.show()","9c5ecb6b":"kmeans2 = KMeans(n_clusters=5)\n\nclusters = kmeans2.fit_predict(df_unsupervised)","ca3d7b30":"plt.scatter(df_unsupervised.Income[clusters == 0],df_unsupervised.SpendingScore[clusters == 0], c = 'red', label = 'Cluster 1')\nplt.scatter(df_unsupervised.Income[clusters == 1], df_unsupervised.SpendingScore[clusters == 1], c = 'blue', label = 'Cluster 2')\nplt.scatter(df_unsupervised.Income[clusters == 2], df_unsupervised.SpendingScore[clusters == 2],c = 'green', label = 'Cluster 3')\nplt.scatter(df_unsupervised.Income[clusters == 3], df_unsupervised.SpendingScore[clusters == 3], c = 'cyan', label = 'Cluster 4')\nplt.scatter(df_unsupervised.Income[clusters == 4], df_unsupervised.SpendingScore[clusters == 4], c = 'magenta', label = 'Cluster 5')\nplt.scatter(kmeans2.cluster_centers_[:, 0], kmeans2.cluster_centers_[:, 1],  c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","7b6cb518":"from scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(df_unsupervised.iloc[:100,:],method=\"ward\")\n\ndendrogram(merg,leaf_rotation = 90)\n\nplt.xlabel(\"data points\")\n\nplt.ylabel(\"ceuclidean distance\")\n\nplt.show()","c5e037a5":"\nfrom sklearn.cluster import AgglomerativeClustering\n\nhiyertical_cluster = AgglomerativeClustering(n_clusters =3,affinity=\"euclidean\",linkage=\"ward\")\n\ncluster = hiyertical_cluster.fit_predict(df_unsupervised)\n\ndf_unsupervised[\"label\"]=cluster\n\n\nplt.scatter(df_unsupervised.Income[df_unsupervised[\"label\"] == 0],df_unsupervised.SpendingScore[df_unsupervised[\"label\"] == 0], c = 'red', label = 'Cluster 1')\nplt.scatter(df_unsupervised.Income[df_unsupervised[\"label\"] == 1], df_unsupervised.SpendingScore[df_unsupervised[\"label\"] == 1], c = 'blue', label = 'Cluster 2')\nplt.scatter(df_unsupervised.Income[df_unsupervised[\"label\"] == 2], df_unsupervised.SpendingScore[df_unsupervised[\"label\"] == 2],c = 'green', label = 'Cluster 3')\nplt.scatter(kmeans2.cluster_centers_[:, 0], kmeans2.cluster_centers_[:, 1],  c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()\nplt.show()","38230275":"df_supervised= df.drop(labels=[\"CustomerID\"],axis=1)\ndf_supervised[\"Gender\"] = [1 if i==\"Male\" else 0 for i in df_supervised[\"Gender\"]]","858ea9d5":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB","330080af":"df_supervised.head()","1e74c604":"x = df_supervised.iloc[:,1:].values\ny = df_supervised.iloc[:,0].values\nprint(x.shape)\nprint(y.shape)","ae3feb40":"x = (x-np.min(x))\/(np.max(x)-np.min(x))","b2e0be95":"x_train , x_test, y_train , y_test = train_test_split(x,y,test_size=0.1,random_state=42)\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","1f79edcc":"random_state=42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier(),\n             GaussianNB()]\n             \ndt_param_grid={\"min_samples_split\":range(10,500,20),\n              \"max_depth\":range(1,20,2)}\n\nsvc_param_grid ={\"kernel\":[\"rbf\"],\n               \"gamma\":[0.001,0.01,0.1,1],\n               \"C\":[1,10,50,100,200,300,1000]}\n\nrf_param_grid={\"max_features\":[1,3,10],\n              \"min_samples_split\":[2,3,10],\n              \"min_samples_leaf\":[1,3,10],\n              \"bootstrap\":[False],\n              \"n_estimators\":[100,300],\n              \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\ngauss_grid = {}\n\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid,\n                    gauss_grid\n                   ]","a91f2a7f":"cv_results = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(x_train,y_train)\n    cv_results.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_results[i])","733e50ab":"nb = GaussianNB()\nfrom sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator=nb,X = x_train,y = y_train,cv=10)\n\nprint(\"mean =\", np.mean(accuracies))","5f3807bf":"cv_result = pd.DataFrame({\"Cross Validation Means\":cv_results, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\",\n             \"Gaussian\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_result)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","fc5f2dca":"<a id=\"2\">\n\n# Data Visualization","f2f2039a":"<a id = \"1\" >\n    \n# Load and Check Data","f1862b2c":"## Hierarcial Clustering","07671405":"* Our accuracy isn't satisfying because number of data is really low.","42efa86f":"* Elbow is existed for 5","d068ca1b":"1. [Load and Check Data](#1)\n2. [Data Visualization](#2)\n3. [Machine Learning Models](#3)\n4. [Unsupervised Learning Part](#3)\n5. [SuperVised Part](#4)\n","3f3a002e":"<a id=\"3\" >\n    \n# Machine Learning Models","b58b0d21":"## K-means","2abf8b68":"* for this denogram ounr n_cluster is 3 because of longest distance without intersection by other lines","89c15acb":"<a id =\"5\" >\n    \n# SuperVised Part","1af60e6f":"<a id=\"4\" >\n\n# Unsupervised Learning Part","5d5e80a4":"* Define labels and features\n             - We want to find customer's gender by looking other features,so our label is gender.","a44b8234":"* Normalize Data"}}