{"cell_type":{"1c370c40":"code","96e12e17":"code","a121a4c3":"code","6647dd94":"code","198a32a7":"code","5ea3c63d":"code","21d989e7":"code","ca6a613e":"code","6b31dfb5":"code","c808bec2":"code","82025ad4":"code","9818745d":"code","07a928ac":"code","82140bcf":"code","cfbcae36":"code","fe87b242":"code","c659e17e":"code","45049b5e":"code","c93d4595":"code","ac4d2493":"code","4b281692":"code","20cd09e6":"code","60891c74":"code","de33234a":"code","f2e4d479":"code","5ad549ed":"code","fe5aae5b":"code","c3aea3f9":"code","b86d10ff":"code","9571c61b":"code","687545e1":"code","340fe29f":"code","08b0c22f":"code","93f691b3":"code","746e43c7":"code","13f224a6":"code","676f6597":"code","e6415d83":"code","181f4bd7":"code","b522c5a7":"code","215b1dbd":"code","0b3e395c":"code","aa7ad96f":"code","66f0619f":"code","23726770":"code","3e421318":"code","f0d93ae0":"code","789fa36d":"code","d757a72b":"code","b4f9fbd2":"code","03e7ff32":"code","16147712":"code","e1633a71":"code","30040ef7":"code","29373e8d":"code","d33389f7":"code","17fe9ad0":"code","24189f98":"code","76640f26":"code","3d5d27db":"markdown","bb0089dc":"markdown","23124dcf":"markdown","415e6885":"markdown","89f99b57":"markdown","29caff90":"markdown","7238d2de":"markdown","3ebc43c5":"markdown","d408f56e":"markdown","b9b35f7f":"markdown","d3a3e129":"markdown","14eb6862":"markdown"},"source":{"1c370c40":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\nfrom sklearn.base import BaseEstimator","96e12e17":"x = tf.Variable(3, name='x')\ny = tf.Variable(4, name='y')\nf = x * x * y + y + 2","a121a4c3":"sess = tf.Session()\nsess.run(x.initializer)\nsess.run(y.initializer)\nresult = sess.run(f)\nprint(result)","6647dd94":"sess.close()","198a32a7":"with tf.Session() as sess:  # with this method, you don't need to call \"session.close\" explicitly \n    x.initializer.run()\n    y.initializer.run()\n    result = f.eval()","5ea3c63d":"print(result)","21d989e7":"init = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    init.run()\n    result = f.eval()","ca6a613e":"print(result)","6b31dfb5":"init = tf.global_variables_initializer() # sess.run\nwith tf.Session() as sess:\n    init.run()\n    print(x.eval())\n    print(y.eval())\n    print(f.eval())\n    result = sess.run(f, feed_dict={x:1, y:0}) # through feed_dict: change the previouly set values for x and y","c808bec2":"print(result)","82025ad4":"\ndef plot_decision_boundary(X, model):\n    h = .02 \n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                       np.arange(y_min, y_max, h))\n\n\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n\n# linear function to be predicated\/learned\ndef f(X):\n    \"\"\"\n    input: x\n    output: y = 3x + 4\n    \"\"\"\n    return 3*X + 4","9818745d":"N = 40\nnoise_level = 0.8\ntrainX = np.linspace(-4.0, 4.0, N) # from -4 to 4, N=40 datapoints\nnp.random.shuffle(trainX)\ntrainY = f(trainX) + np.random.randn(N) * noise_level # generate training labels by adding some noise. randn  \u9ad8\u65af\u5206\u5e03\n\nlearning_rate = 0.01\ntraining_epochs = 1000 # run 1000 time against all training data\ndisplay_step = 50 # show logs every 50 epochs ","07a928ac":"plt.scatter(trainX, trainY)","82140bcf":"np.random.randn()","cfbcae36":"class LinearRegressionTF(BaseEstimator):\n    def __init__(self, learning_rate, training_epochs, display_step, annotate=False):\n        self.annotate = annotate\n        self.sess = tf.Session() # Tensflow session instance\n        self.training_epochs = training_epochs\n        self.learning_rate = learning_rate\n        self.display_step = display_step\n        \n        \n    def fit(self, trainX,trainY):\n        N = trainX.shape[0] # see how many training data i.e. row number\n        # \u56fe\u7684\u8f93\u5165\n        self.X = tf.placeholder(\"float\")\n        self.Y = tf.placeholder(\"float\")\n        \n        # \u53c2\u6570\u7684\u5b9a\u4e49, also give intial value to those parameters.\u4e4b\u6240\u4ee5\u5b9a\u4e49\u4e3a\u53d8\u91cf\uff0c\u56e0\u4e3a\u5728train\u7684\u8fc7\u7a0b\u4e2d\u4f1a\u53d8\u5316\n        #  y = 3 * X + 4\n        self.W = tf.Variable(np.random.randn(), name=\"weight\") # randomly initialize a value for weight\n        self.b = tf.Variable(np.random.randn(), name=\"bias\") # randomly initialize a value for bias\n        \n        # \u7ebf\u6027\u6a21\u578b i.e. Y = W * X + b. \u8fd9\u91cc\u4e5f\u53ef\u4ee5\u76f4\u63a5\u7528+\u53f7\uff0ctf.add\u662f\u53ef\u4ee5\u5728tensorboard\u4e0a\u663e\u793a\u7684\uff0cdebug\u7684\u65f6\u5019\u80fd\u770b\u89c1\n        self.pred = tf.add(tf.multiply(self.X, self.W), self.b) \n        \n        # Mean Squre Error \u5206\u6bcd\u662f2N \u6216 N \u90fd\u53ef\u4ee5\n        cost = tf.reduce_sum(tf.pow(self.pred-self.Y, 2))\/(2*N)\n        \n        # \u53cd\u5411\u68af\u5ea6\u4f18\u5316\u65f6\u7684\u4f18\u5316\u5668\n        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(cost)\n        \n        # \u521d\u59cb\u5316\u6240\u6709\u7684\u53c2\u6570\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        # intial graph before trainning data to fit \n        if self.annotate:\n            plt.plot(trainX, trainY, 'ro', label='Original data')\n            #self.sess.run(self.W) can view\/monitor each individual variable value\n            plt.plot(trainX, self.sess.run(self.W) * trainX + self.sess.run(self.b), label='Fitted line')\n            plt.legend()\n            plt.title(\"This is where model starts to learn!!\")\n            plt.show()\n            \n        # \u8bad\u7ec3\u5f00\u59cb\n        for epoch in range(self.training_epochs):\n            for (x, y) in zip(trainX, trainY): # go through each training data sample in one epoch\n                # regular one: result = sess.run(f, feed_dict={x:1, y:0})\n                # pass in optimizer function wihch is defined ealier, and each trian sample data points\n                self.sess.run(optimizer, feed_dict={self.X: x, self.Y: y}) # set self.X = x, self.Y = y\n\n            #\u5c55\u793a\u8bad\u7ec3\u7ed3\u679c for each epoch \n            if (epoch+1) % display_step == 0: # because epoch start with 0, so here epoch +1. i.e. epoch =49 means 50 epoch\n                # pass in cost function, which is defined earlier. When calculate cost, we need all data points..that's why self.X = trainX\n                c = self.sess.run(cost, feed_dict={self.X: trainX, self.Y:trainY}) # set self.X = trainX, self.Y = trainY\n                print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n            \"W=\", self.sess.run(self.W), \"b=\", self.sess.run(self.b))      # \\ means change line, \n                \n            #\u663e\u793a\u62df\u5408\u7684\u76f4\u7ebf for each epoch, \u548cinitial graph\u4f1a\u4e0d\u540c\uff0c\u56e0\u4e3a\u53d8\u91cf W \u548c b \u662f\u4e00\u76f4\u6539\u53d8\u7684\n                if self.annotate:\n                    plt.plot(trainX, trainY, 'ro', label='Original data')\n                    plt.plot(trainX, self.sess.run(self.W) * trainX + self.sess.run(self.b), label='Fitted line')\n                    plt.legend()\n                    plt.show()\n                #plt.pause(0.5)\n\n        print(\"Optimization Finished!\")\n        # \u8bad\u7ec3\u7ed3\u675f\uff0c\u770b\u4e00\u4e0bcost. self.sess.run \u53ef\u4ee5\u663e\u793a\u4e00\u4e2a\u7279\u5b9a\u53d8\u91cf\uff0ccost\u53d8\u91cf\u8ba1\u7b97\u9700\u8981\u8f93\u5165trainX, trainY\u6570\u636e\n        training_cost = self.sess.run(cost, feed_dict={self.X: trainX, self.Y: trainY})\n        print(\"Training cost=\", training_cost, \"W=\", self.sess.run(self.W), \"b=\", self.sess.run(self.b), '\\n')\n\n        \n    def predict(self, testX):\n        #self.pred is defined in fit function: self.pred = tf.add(tf.multiply(self.X, self.W), self.b)\n        # \u867d\u7136self.pred \u7528\u5230\uff0cX, W,b \u7531\u4e8e\u8bad\u7ec3\u7ed3\u675f\u540e\uff0cW, b\u90fd\u662f\u5df2\u77e5\u7684\u4e86\uff0c\u6240\u4ee5\u53ea\u9700\u8981\u4f20\u5165textX \u5c31\u53ef\u4ee5\u4e86\n        prediction = self.sess.run(self.pred,feed_dict={self.X: testX}) # just pass in real testing data i.e. testX\uff0c\n        return prediction\n    \n    def score(self, testX, testY):\n        result = self.predict(testX)\n        return r2_score(testY, result) # score (actual lable, predicated value)\n        \n    ","fe87b242":"lr = LinearRegressionTF(learning_rate, 1000, display_step, annotate=False) # False OR True\nlr.fit(trainX, trainY)","c659e17e":"from sklearn.model_selection import cross_val_score\ncross_val_score(lr, trainX, trainY, cv=2).mean()","45049b5e":"tf.reset_default_graph() # have to reset it after previous tensflow run","c93d4595":"N = 100\nD = 2\n# trainX.shape: (100 * 2), also it follows standard normal distribution\ntrainX = np.random.randn(N, D) # Return a samples from the \u201cstandard normal\u201d distribution, N rows, D columns\n\n# add some noise to the trainning data\ndelta = 1.75\ntrainX[:N\/\/2] += np.array([delta, delta]) # add delta for each 2 columns for first 50 rows\ntrainX[N\/\/2:] += np.array([-delta, -delta])\n\ntrainY = np.array([0] * (N\/\/2) + [1] * (N\/\/2))\nplt.scatter(trainX[:,0], trainX[:,1], s=100, c=trainY, alpha=0.5)\nplt.show()","ac4d2493":"# 100 lables, generate 50 ZERO & 50 ONE\noriginal_label = np.array([0] * (N\/\/2) + [1] * (N\/\/2)) # 5\/\/2 =2 5\/2 = 2.5\nprint(original_label)\nprint(len(original_label))","4b281692":"from sklearn.metrics import accuracy_score\nclass LogisticRegressionTF(BaseEstimator):\n    def __init__(self, learning_rate, training_epochs, display_step, annotate=False):\n        self.annotate = annotate\n        self.sess = tf.Session() # like linear regression, always createa session instance in __init__ method\n        self.training_epochs = training_epochs\n        self.learning_rate = learning_rate\n        self.display_step = display_step\n        \n        \n    def fit(self, trainX,trainY):\n        N, D = trainX.shape # \u5728\u6211\u4eec\u7684\u4f8b\u5b50\u91ccD=2,\u4e5f\u5c31\u8868\u793a\u6bcf\u4e2aX \u67092\u4e2afeatures\n        _, c = trainY.shape # c \u591a\u5c11\u4e2a\u7c7b\u522b.\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u91cc c=2.\u7531\u4e8eone hot encoded, \u75282\u5217\u8868\u793a\n        # \u56fe\u7684\u8f93\u5165. here \"None\" means I'm not sure how many rows\/training data\n        self.X = tf.placeholder(tf.float64, shape=[None, D]) # D\u5217features\n        self.Y = tf.placeholder(tf.float64, shape=[None, c]) # c = trainY.shape defined earlier.\u901a\u5e38\u6765\u8bf4label\u5c311\u5217\uff0c\u4f46\u662f\u7531\u4e8e\u505a\u4e86one hot encoding, label\u591a\u5c11\u4e2a\u79cd\u7c7b\u5c31\u6709\u591a\u5c11\u5217\n        \n        # \u53c2\u6570\u7684\u5b9a\u4e49 and also assign initial values to those newly defined variables\/parameters\n        # \u89e3\u91ca\u4e3a\u4ec0\u4e48\u5b9a\u4e49\u8fd9\u4e2arandn(D,c)shape. trainX(N * D) * W (D * c) -> label (N * c) \u6240\u4ee5\u8fd9\u91cc\u9700\u8981\u5b9a\u4e49W \u4e3a\uff08D, c\uff09shape\n        self.W = tf.Variable(np.random.randn(D,c), name=\"weight\") # D\uff1a trainX\u7684feature columns \u5217\n        self.b = tf.Variable(np.random.randn(c), name=\"bias\") # label (N * c), \u6bcf\u4e00\u5217\u90fd\u9700\u8981\u4e00\u4e2abias,\u6240\u4ee5bias shape\u4e3a c\n        \n        # logistic prediction\n        #self.pred = tf.sigmoid(tf.add(tf.matmul(self.X, self.W), self.b))\n        # matmul VS multiply: matmul: MATRIX multiplication AND multiple: element-wise multiplication which is used in above linear regression,\u53ea\u80fd\u7528\u4e8e\u5355\u884c \u6216\u5355\u5217\u77e9\u9635\u76f8\u4e58\n        # https:\/\/stackoverflow.com\/questions\/47583501\/tf-multiply-vs-tf-matmul-to-calculate-the-dot-product\n        output_logits = tf.add(tf.matmul(self.X, self.W), self.b) # \u8fd9\u91cc X, W \u987a\u5e8f\u4e0d\u80fd\u53d8\u6362\n        # different from linear regression, here we have to go through sigmoid function to get the probability \n        #\u8fd9\u91cc\u8fd4\u56de\u7684\u5c31\u662f2\u4e2a\u503c\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e00\u7c7b\u7684\u6982\u7387 \uff08prediction function \u91cc\u9762\u7528\u4e86np.argmax(prediction, axis=1) \u8868\u793a\u6bcf\u4e00\u884c\u7ed3\u679c2\u4e2a\u503c\uff1f\uff09\n        self.pred = tf.sigmoid(output_logits)   # turn logits to probability through sigmoid function\n        \n        # \u4ea4\u53c9\u71b5loss (differnt from linear regressin's MSE: mean squred error)\n        #cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n        #??? \n        cost= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_logits, labels=self.Y))\n        \n        # \u4f18\u5316\u5668\n        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(cost)\n        \n        # \u521d\u59cb\u5316\u6240\u6709\u7684\u53c2\u6570\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        \n        \n        # \u53ef\u89c6\u5316\u521d\u59cb\u5316\u7684\u6a21\u578b\u8fb9\u754c\n        if self.annotate:\n            assert len(trainX.shape) == 2, \"Only 2d points are allowed!!\"\n\n            plt.scatter(trainX[:,0], trainX[:,1], s=100, c=original_label, alpha=0.5) \n\n            h = .02 \n            x_min, x_max = trainX[:, 0].min() - 1, trainX[:, 0].max() + 1\n            y_min, y_max = trainX[:, 1].min() - 1, trainX[:, 1].max() + 1\n            xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n\n            Z = self.predict(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n            plt.title(\"This is where model starts to learn!!\")\n            plt.show()\n\n        \n\n        # \u8bad\u7ec3\u5f00\u59cb\n        for epoch in range(self.training_epochs):\n            for (x, y) in zip(trainX, trainY):\n                # have to call np.asmatrix(x) ?, in above linear regression, there is no asmatrix,\u56e0\u4e3a\u6ca1\u6709 shape\u7684\u95ee\u9898\n                # \u9700\u8981\u6307\u660eshape\u7684\u95ee\u9898\uff0c\u6240\u4ee5\u9700\u8981\u8f6c\u5316\u4e3amatrix\n                self.sess.run(optimizer, feed_dict={self.X: np.asmatrix(x), self.Y: np.asmatrix(y)})\n\n            #\u5c55\u793a\u8bad\u7ec3\u7ed3\u679c for each epoch\n            if (epoch+1) % display_step == 0:\n                c = self.sess.run(cost, feed_dict={self.X: trainX, self.Y:trainY})\n                print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n            \"W=\", self.sess.run(self.W), \"b=\", self.sess.run(self.b))\n                \n            #\u663e\u793a\u62df\u5408\u7684\u76f4\u7ebf for each epoch. it's actualy just the \"plot_decision_boundaryg\" function in below link:\n            # https:\/\/www.kaggle.com\/jungan\/ensemble-bagging-tree-plot-decision-boundary\n                if self.annotate:\n                    assert len(trainX.shape) == 2, \"Only 2d points are allowed!!\"\n\n                    plt.scatter(trainX[:,0], trainX[:,1], s=100, c=original_label, alpha=0.5) \n             \n                    h = .02 \n                    x_min, x_max = trainX[:, 0].min() - 1, trainX[:, 0].max() + 1\n                    y_min, y_max = trainX[:, 1].min() - 1, trainX[:, 1].max() + 1\n                    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n                    # in plot_decision_boundary function, model.predcit where model is passed into this function\n                    Z = self.predict(np.c_[xx.ravel(), yy.ravel()])\n                    Z = Z.reshape(xx.shape)\n                    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n                    plt.show()\n\n\n\n        print(\"Optimization Finished!\")\n        training_cost = self.sess.run(cost, feed_dict={self.X: trainX, self.Y: trainY})\n        print(\"Training cost=\", training_cost, \"W=\", self.sess.run(self.W), \"b=\", self.sess.run(self.b), '\\n')\n\n        \n    def predict(self, testX):\n        # 0.4\uff0c 0.6 -> 1 \u53bb\u6982\u7387\u503c\u5927\u7684\u5217\u53f7\n        # 0.7\uff0c 0.3 -> 0   \n        # \u4e3a\u4ec0\u4e48prediction\u8fd4\u56de\u4e24\u5217\uff1f \u662f\u6211\u4eectensorflow\u7684\u8fd9\u4e2agraph\u5b9a\u4e49\u7684\uff0c\u4e4b\u524d\u6211\u4eectrain\u7684\u65f6\u5019\u7528\u7684\u6570\u636e\u7684label\u662f2\u5217\u7684\uff0c\u90a3\u4e48\u5728predict\u7684\u65f6\u5019\u8fd4\u56de\u7684\u5c31\u662f\u4e00\u6837\u5927\u5c0f\u7684\u503c\n        prediction = self.sess.run(self.pred,feed_dict={self.X: testX}) # prediction \u6709\u4e24\u5217\uff0c\u6bcf\u4e00\u4e2a\u79cd\u7c7b\u7684\u6982\u7387\n        # https:\/\/docs.scipy.org\/doc\/numpy-1.15.1\/reference\/generated\/numpy.argmax.html\n        # return the max value's index in each row (note: not the actual value...instead just index)\n        return np.argmax(prediction, axis=1)\n    \n    def score(self, testX, testY):\n        #suppose the testY has been one hot encoded. testY = label of the training data\n        #eg:#0: [1,0]  -> 0\n            #1: [1,0]  -> 0\n            #2: [0,1]  -> 1\n        _ , true_result = np.where(testY == 1) # which index = 1, e.g. 0, 1 -> index =1, class1, 1,0 -> index =0, class 0\n        result = self.predict(testX)\n        return accuracy_score(true_result, result)","20cd09e6":"from sklearn.preprocessing import OneHotEncoder","60891c74":"trainY\n","de33234a":"\nle = OneHotEncoder()\n# below fit and transform can be done in one line\nle.fit(trainY.reshape(N,-1)) # -1 means not sure how many columns and columsn will be inferred from another dimension\ntrainY = le.transform(trainY.reshape(N,-1)).toarray() # toarray()  \u53d8\u6210 numpy \u7684ndarray type\ntrainY\n\n# lalel=0 -> [1, 0] \n# label=1 -> [0, 1]","f2e4d479":"logisticTF = LogisticRegressionTF(learning_rate, 1000, display_step, annotate=False)\nlogisticTF.fit(trainX, trainY)","5ad549ed":"from sklearn.model_selection import cross_val_score\ncross_val_score(logisticTF, trainX, trainY, cv=2).mean() # will run 5 times. each time will 1000 epochs","fe5aae5b":"tf.reset_default_graph()","c3aea3f9":"#from tensorflow.examples.tutorials.mnist import input_data\n#mnist = input_data.read_data_sets(\"\/tmp\/data\/\", one_hot=True)\n#trainX, trainY = mnist.train.next_batch(5000) #5000\u4e2a\u6570\u636e\u4f5c\u4e3a\u8fd1\u90bb\u96c6\u5408\n#testX, testY = mnist.test.next_batch(200) #200\u4e2a\u6570\u636e\u7528\u4e8e\u6d4b\u8bd5","b86d10ff":"import os\ndata_folder = \"..\/input\/ninechapterdigitsub\"\n#data_folder = \"data\"\ntrainX = np.genfromtxt(os.path.join(data_folder, \"digit_mnist_trainx.csv\"), delimiter=',')\ntrainY = np.genfromtxt(os.path.join(data_folder, \"digit_mnist_trainy.csv\"), delimiter=',')\ntestX = np.genfromtxt(os.path.join(data_folder, \"digit_mnist_testx.csv\"), delimiter=',')\ntestY = np.genfromtxt(os.path.join(data_folder, \"digit_mnist_testy.csv\"), delimiter=',')","9571c61b":"trainX.shape, trainY.shape, testX.shape, testY.shape, type(trainX)\n#\u4e3a\u4ec0\u4e48label \u670910 cloumns,  \u56e0\u4e3alabel \u505a\u4e86one hot encodingl\u4e86 0\uff0c 1\uff0c2\uff0c3\uff0c4\uff0c5\uff0c6\uff0c7\uff0c8\uff0c9 \u5177\u4f53\u662f\u54ea\u4e2a\u6570\u5b57\uff0c\u90a3\u4e2a\u5bf9\u5e94\u4f4d\u7f6e\u7684\u503c\u5c31\u662f1","687545e1":"trainY[:2]","340fe29f":"xtr = tf.placeholder(\"float\", [None, 784]) # \u524d\u9762\u7528\u6cd5\uff1aself.X = tf.placeholder(tf.float64, shape=[None, D])\nxte = tf.placeholder(\"float\", [784]) # \u5bf9\u4e8e\u6d4b\u8bd5\u6570\u636e\uff0c\u80af\u5b9a\u53ea\u67091\u884c\uff0c\u6240\u4ee5\u4e0d\u7528None","08b0c22f":"# xtr: (5000, 784) xte(200, 784)\ndistance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(xtr, xte)), reduction_indices=1)) # \u8bbe\u7f6e\u7ef4\u5ea6\u4e3a1\n\n# example\uff1a \u6bcf\u4e2atest data \u53bb\u548c\u6240\u6709\u7684training data \u8ba1\u7b97\u8ddd\u79bb\uff0c \u56e0\u4e3a\u57fa\u4e8e\u4e0a\u9762\u7684\u5b9a\u4e49\uff1axte = tf.placeholder(\"float\", [784])\uff0c xte \u5c31\u8868\u793a1\u7ef4\u6570\u636e\uff0c\u4e5f\u5c311\u884c\u6570\u636e\n# train 0: [1,...1]\n# train 1: [0,...0]\n# test : [1,...1]\n# tf.subtract(xtr, xte):\n#  0: [0,...0]\n#  1: [-1,...-1]\n# tf.square:\n#  0: [0,...0]\n#  1: [1,...1]\n#  tf.reduce_sum(tf.square(tf.subtract(xtr, xte)), reduction_indices=1):\n#  0: [0]\n#  1: [784]","93f691b3":"# \u56e0\u4e3a\u662ftopk\u5927\u7684\u503c\uff0c\u8fd9\u91ccdistance\u53d6\u8d1f\u53f7, \u8868\u793a\u53d6\u4e3e\u4f8b\u6700\u5c0f\u7684K\u4e2a\nKVALUE = 2\n# based on the https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/top_k,  tf.nn.top_k \u51fd\u6570 \u8fd4\u56de values  and index \u4e24\u4e2a\u503c\npred= tf.nn.top_k(-distance, k=KVALUE) #\u4e0a\u9762\u5176\u4ed6\u51e0\u4e2a\u6a21\u578b\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u7531\u4e8e\u5b9a\u4e49\u4e86init, fit, precit functions, \u6240\u4ee5\u4ed6\u4eec\u90fd\u653e\u5728\u4e86predict \u51fd\u6570\u91cc\u4e86","746e43c7":"from collections import Counter\naccuracy = 0.\n\n# \u521d\u59cb\u5316\u53c2\u6570\ninit = tf.global_variables_initializer()\n\n# \u5f00\u59cb\u8bad\u7ec3 \u91c7\u7528\u4e86\u4e0a\u9762\"intialize Variable - Method 4 - Recommened Way\"\nwith tf.Session() as sess:\n    sess.run(init) # \u4e5f\u53ef\u4ee5\u7528init.run\n\n    # \u9884\u6d4b\u6d4b\u8bd5\u6570\u636e\u7684\u6807\u7b7e (passive learner)\n    for i in range(len(testX)):\n        # \u6700\u8fd1\u90bb\u7684\u5e8f\u53f7\n        # \u4e3a\u4ec0\u4e48\u77e5\u9053\u8fd9\u91cc\u80fd\u8fd4\u56de\u54ea\u4e9b\u4e1c\u897f\uff0c\u6709\u7684\u65f6\u5019\u662f1\u4e2a\u503c\uff0c\u6709\u7684\u65f6\u50192\u4e2a\u503c, \u6839\u636e\u5c31\u662ftensorflow\u5177\u4f53API \u7684\u51fd\u6570\u7684\u7528\u6cd5\n        # \u6839\u636e\u4e0a\u9762pred\u7684\u5b9a\u4e49\uff1a top_k\u8fd4\u56de2\u4e2a\u503c\uff1a https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/top_k\n        # \u8fd9\u91cc\u7684feed_dict\u53d8\u91cf\u540d\u5b57\u8981\u548c\u524d\u9762\u5b9a\u4e49\u7684 \u5bf9\u5e94 i.e.xtrde = tf.placeholder(\"float\", [None, 784]) \n        values, knn_index = sess.run(pred, feed_dict={xtr: trainX, xte: testX[i, :]}) # testX\u7684i\u884c\uff0c\u6240\u6709\u5217\n\n        # \u62ff\u5230k\u4e2a\u90bb\u5c45\u540e\u505a\u5168\u6c11\u516c\u6295\uff0c\u5f97\u7968\u6700\u591a\u7684\u4e3a\u9884\u6d4b\u6807\u7b7e\uff0c\n        # e.g y =1 [0, 1,0, 0] np.argmax -> 1\n        #     y =0 [1, 0, 0, 0] np.argmax -> 0\n        #     y =3 [0, 0, 0, 1] np.argmax -> 3\n        # \u56e0\u4e3alable,\u505a\u4e86oneHotEncoding, \u53ea\u67090\uff0c1 \u4e24\u4e2a\u503c\uff0cnp.argmax(trainY[knn_index]\uff0c\u5c31\u53ef\u4ee5\u77e5\u90531\u5bf9\u5e94\u7684\u4f4d\u7f6e\uff0c\u4e5f\u5c31\u662f\u5b9e\u9645\u7684\u6570\u5b57\uff0c\u56e0\u4e3a\u6570\u5b57\u5c31\u662f\u4ece0\u5f00\u59cb \u52309\n        c = Counter(np.argmax(trainY[knn_index], axis=1)) # e.g. K=2\u65f6\uff0csample output: Counter({7: 1, 1: 1})\u76f8\u5f53\u4e8edict\u7c7b\u578b\n        # \u5bf9\u4e8eCounter({7: 1, 1: 1})\uff0c Counter.most_commmon(2) \u8fd4\u56de\u6570\u7ec4\u7c7b\u578bCounter [(7, 1), (1, 1)]\uff0c \u8fd9\u6837 \u52a0\u4e0a[0][0] \u5c31\u5a36\u5230\u5b9e\u9645\u7684\u503c 7\u4e86\n        # another exmaple: {0:2, 1:4}, after most_common((1, 4), (0,2))\n        result = c.most_common(KVALUE)[0][0]\n        # \u8ba1\u7b97\u6700\u8fd1\u90bb\u7684\u6807\u7b7e\u548c\u771f\u5b9e\u6807\u7b7e\u503c\n        print(\"Test\", i, \"Prediction:\", result, \\\n            \"True Class:\", np.argmax(testY[i]))\n        # \u6b63\u786e\u7387\n        if result == np.argmax(testY[i]):\n            accuracy += 1.\/len(testX)\n            \n    print(\"Done!\")\n    print(\"Accuracy:\", accuracy)","13f224a6":"tf.reset_default_graph()","676f6597":"N = 120\nD = 2\ntrainX = np.random.randn(N, D) # generate 120 samples , each sample has 2 features\n\ndelta = 2\n#trainX[:N\/\/3] += np.array([delta, delta])\n#trainX[N\/\/3:N*2\/\/3] += np.array([-delta, delta])\n#trainX[N*2\/\/3:] += np.array([0, -delta])\n\ndelta = 1.75\ntrainX[:N\/\/2] += np.array([delta, delta])\ntrainX[N\/\/2:] += np.array([-delta, -delta])\n\ntrainY = np.array([0] * (N\/\/2) + [1] * (N\/\/2))\nplt.scatter(trainX[:,0], trainX[:,1], s=100, c=trainY, alpha=0.5)\nplt.show()","e6415d83":"from sklearn.metrics import accuracy_score","181f4bd7":"from matplotlib import colors\nfrom sklearn.utils.fixes import logsumexp\n\nclass NaiveBayesTF(BaseEstimator):\n    \n    def __init__(self):\n        self.dist = None\n        self.sess = tf.Session() # as always, initialize Tensforflow session instance in __init__\n\n    def fit(self, trainX, trainY):\n        # Separate training points by class (nb_classes * nb_samples * nb_features)\n        unique_classes = np.unique(trainY) # check how many unique classess in labels\n        # points_by_class shape: (2, 60, 2) i.e. (nb_classes * nb_samples * nb_features), 2 classess, each class has 60 samples, each sample has 2 features\n        points_by_class = np.array([\n            [x for x, y in zip(trainX, trainY) if y == c]\n            for c in unique_classes])\n        input_x = tf.placeholder(tf.float64, shape=points_by_class.shape) #points_by_class shape: (2, 60, 2)\n        # \u4f30\u8ba1\u6bcf\u4e2a\u7c7b\u5e95\u4e0b\u6bcf\u4e00\u79cdfeature\u7684\u5747\u503c\u548c\u65b9\u5dee\n        # shape: num_classes * nb_features\n        \n        #???  how to determin axes=[1] for 2 * 60 * 2 dimensions\n        # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/moments\n        moments = tf.nn.moments(input_x, axes=[1]) # tf.nn.moments return: mean and variance,  axes=[1]:each column\/feature of input_x\n        mean, var = self.sess.run(moments, feed_dict={input_x:points_by_class})\n        # print(mean.shape) (2, 2) \u6bcf\u4e2a\u7c7b\u5e95\u4e0b\u6bcf\u4e00\u79cdfeature\u7684\u5747\u503c\u548c\u65b9\u5dee -> 2 * 2\n        # print(var.shape) (2, 2)\n        \n        # \u70b9\u96c6\u5b9e\u9a8c\u91cc\u4e3a2\u7c7b\uff0c\u6bcf\u4e2a\u6570\u636e\u70b9\u67092\u4e2a\u7279\u5f81 \n        # known mean and variance. https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distributions\/Normal\n        self.dist = tf.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n        # print(self.dist.scale.shape) (2, 2)\n        # print(self.dist.scale) -> Tensor(\"Normal_8\/scale:0\", shape=(2, 2), dtype=float64)\n\n    def predict(self, testX):\n        assert self.dist is not None\n        num_classes, num_features = map(int, self.dist.scale.shape)\n\n        # \u6761\u4ef6\u6982\u7387 log P(x|c)\n        # (nb_samples, nb_classes)\n        # ??? how to determin:  axis=2\n        cond_probs = tf.reduce_sum(\n            # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distributions\/Normal#log_prob\n            # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/tile\n            # self.dist is normal distrubtion\n            self.dist.log_prob(\n                # tf.tile(testX, [1, num_classes]), 1 means along x asix, duplicate num_classes times\n                tf.reshape(\n                    tf.tile(testX, [1, num_classes]), [-1, num_classes, num_features])), # -1 means: not sure the dimension\n            axis=2)\n        \n        # \u7b2c\u4e00\u4e2a\u70b9: 2.0,3.5\n        # \u7b2c\u4e8c\u4e2a\u70b9: 0.5,1.4\n        \n        # tf.tile (num_classes = 2): \n        # ===>\n        # \u7b2c\u4e00\u4e2a\u70b9: 2.0,3.5,2.0,3.5\n        # \u7b2c\u4e8c\u4e2a\u70b9: 0.5,1.4,0.5,1.4\n        \n        # tf.reshape:\n        # ===>\n        # \u7b2c\u4e00\u4e2a\u70b9: 2.0,3.5 \n        #         2.0,3.5\n        # \u7b2c\u4e8c\u4e2a\u70b9\uff1a0.5,1.4\n        #         0.5,1.4\n        \n        # e.g: point1, c = 0  log(P(x0=2|c=0)) + log(P(x1=3.5|c=0))\n        #      point1, c = 1  log(P(x0=2|c=1)) + log(P(x1=3.5|c=1))\n        # bascially, dupliate each row, then it's easier to calculate prob for each row for each class  \n        \n        # P(C) \u5747\u5300\u5206\u5e03  # priors array([0.5, 0.5]), i.e each class has 50% in all samples\n        priors = np.log(np.array([1. \/ num_classes] * num_classes))\n\n        # \u540e\u9a8c\u6982\u7387\u53d6log, log P(C) + log P(x|C)\n        posterior = tf.add(priors, cond_probs)\n        \n        # \u53d6\u6982\u7387\u6700\u5927\u7684\u90a3\u4e00\u4e2a\n        # \u7b2c\u4e00\u4e2a\u70b9: 2.0,3.5 \n        #         2.0,3.5\n        # each data point has two posterior, i.e. each row (each class) has one.\n        # axis=1 means, max index for for each row\n        result = self.sess.run(tf.argmax(posterior, axis=1)) # axis=1, \u8868\u793a\u987a\u7740y\u8f74\uff0c \u5bf9\u6bcf\u4e00\u884c\u6c42max\n\n        return result\n    \n    \n    def score(self, testX, testY):\n        result = self.predict(testX)\n        return accuracy_score(testY, result) # accuracy_score(label, predicated values)\n","b522c5a7":"np.array([1. \/ 2] * 2)","215b1dbd":"tf_nb = NaiveBayesTF()\ntf_nb.fit(trainX, trainY)","0b3e395c":"x_min, x_max = trainX[:, 0].min() - .5, trainX[:, 0].max() + .5\ny_min, y_max = trainX[:, 1].min() - .5, trainX[:, 1].max() + .5\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 30),\n                     np.linspace(y_min, y_max, 30))","aa7ad96f":"Z = tf_nb.predict(np.c_[xx.ravel(), yy.ravel()])","66f0619f":"Z = Z.reshape(xx.shape)","23726770":"plt.scatter(trainX[:,0], trainX[:,1], s=100, c=trainY, alpha=0.5)\nplt.contour(xx, yy, Z, cmap=plt.cm.Paired)\nplt.show()","3e421318":"tf_nb.score(trainX, trainY)","f0d93ae0":"from tensorflow.contrib.tensor_forest.python import tensor_forest\nfrom tensorflow.python.ops import resources\nimport pandas as pd\nimport numpy as np\nimport os","789fa36d":"data_folder = \"..\/input\/fashionmnist\"\n#data_folder = \".\/data\"\ntrain_data = pd.read_csv(os.path.join(data_folder, \"fashion-mnist_train.csv\"))\ntest_data = pd.read_csv(os.path.join(data_folder, \"fashion-mnist_test.csv\"))","d757a72b":"train_data.head()","b4f9fbd2":"test_data.head()","03e7ff32":"trainX = np.array(train_data.iloc[:, 1:])\ntrainY = np.array(train_data.iloc[:, 0])\ntestX = np.array(test_data.iloc[:, 1:])\ntestY = np.array(test_data.iloc[:, 0])","16147712":"IMAGE_CLASSES = {\n    0: 'T-shirt\/top',\n    1: 'Trouser',\n    2: 'Pullover',\n    3: 'Dress',\n    4: 'Coat',\n    5: 'Sandal',\n    6: 'Shirt',\n    7: 'Sneaker',\n    8: 'Bag',\n    9: 'Ankle boot'\n}","e1633a71":"import matplotlib.pyplot as plt\nimg_size = 28 # 28 * 28 = 784 which is he feature number\nfor img, label in zip(trainX[:10], trainY[:10]):   # take a look at at first 10 images in the train set\n    plt.imshow(img.reshape(img_size,img_size),cmap='gray')\n    plt.title(IMAGE_CLASSES[label])\n    plt.show()","30040ef7":"# \u53c2\u6570\u8bbe\u5b9a\n#The 10 categories\n#784 Each image is 28x28 pixels\nnum_steps = 100# Total steps to train \nbatch_size = 1024 # The number of samples per batch \nnum_trees = 10\nmax_nodes = 1000","29373e8d":"tf.reset_default_graph() # as always, reset the tensorflow","d33389f7":"class RandomForestTF(BaseEstimator):\n    \n    # why not define  self.sess = tf.Session() wihtin __init__\n    def __init__(self, num_trees):\n        self.num_trees = num_trees # \n        \n    def fit(self, X, Y, num_steps, batch_size,max_nodes):\n        num_classes = 10   #len(IMAGE_CLASSES)\n        num_data = X.shape[0] # how many samples\n        num_features = X.shape[1] # how many features\n        \n        self.X = tf.placeholder(tf.float32, shape=[None, num_features]) # None means not sure how many samples\n        self.Y = tf.placeholder(tf.int32, shape=[None])\n        \n        # \u968f\u673a\u68ee\u6797\u7684\u53c2\u6570\n        hparams = tensor_forest.ForestHParams(num_classes=num_classes,\n                                      num_features=num_features,\n                                      num_trees=self.num_trees,\n                                      max_nodes=max_nodes).fill()\n        \n        # \u968f\u673a\u68ee\u6797\u7684\u8ba1\u7b97\u56fe\n        forest_graph = tensor_forest.RandomForestGraphs(hparams)\n        \n        train_operation = forest_graph.training_graph(self.X, self.Y)\n        loss_operation = forest_graph.training_loss(self.X, self.Y)\n        \n        # inference_graph will return probabilities, decision path and variance\n        self.infer_op, _, _ = forest_graph.inference_graph(self.X) # inference_graph API tell you return data\n        # infer_op includes inrerred prob for each type, pick the max prob type. then check against with the original label\n        # tf.equal: https:\/\/www.dotnetperls.com\/equal-tensorflow\n        correct_prediction = tf.equal(tf.argmax(self.infer_op, 1), tf.cast(self.Y, tf.int64))\n        print(\"correct_prediction\")\n        print(correct_prediction)\n        # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/reduce_mean\n        self.accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n        \n        \n        # \u5c06\u521d\u59cb\u5316\u7684\u64cd\u4f5c\u548c\u6811\u7684\u53c2\u6570\u521d\u59cb\u5316 \u4f5c\u4e3a\u4e00\u4e2a\u6574\u4f53\u64cd\u4f5c\n        init_vars = tf.group(tf.global_variables_initializer(),\n                   resources.initialize_resources(resources.shared_resources()))\n        #init_vars = tf.global_variables_initializer()\n        \n        self.sess = tf.Session()\n        self.sess.run(init_vars)\n\n        # \u5f00\u59cb\u8bad\u7ec3\n        cnt = 0\n        for i in range(1, num_steps + 1): # step = epoch\n            # Prepare Data\n            # \u6bcf\u6b21\u5b66\u4e60\u4e00\u4e2abatch\u7684MNIST data\n            #batch_x, batch_y = training_set.next_batch(batch_size)\n            start, end = ((i-1) * batch_size) % num_data, (i * batch_size) % num_data\n            \n            batch_x, batch_y = X[start:end], Y[start:end]\n            _, l = self.sess.run([train_operation, loss_operation], feed_dict={self.X: batch_x, self.Y: batch_y})\n            if i % 50 == 0 or i == 1:\n                acc = self.sess.run(self.accuracy_op, feed_dict={self.X: batch_x, self.Y: batch_y})\n                print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))\n                \n    def predict(self, testX):\n        results = self.sess.run(self.infer_op, feed_dict={self.X:testX})\n        return np.argmax(results, axis=1)\n    \n    def score(self, testX, testY):\n        accuracy = self.sess.run(self.accuracy_op, feed_dict={self.X: testX, self.Y: testY})\n        return accuracy","17fe9ad0":"rftf = RandomForestTF(num_trees)","24189f98":"rftf.fit(trainX, trainY, num_steps, batch_size, max_nodes)","76640f26":"rftf.score(testX, testY)","3d5d27db":"### Tensorflow_Random Forest","bb0089dc":"### **Tensorflow - KNN** ","23124dcf":"### **Tensorflow - Linear Regression**","415e6885":"**Plot decision boundary function (use tf_nb to  instead of passed in model)**","89f99b57":"**Intialize Variable - Method 1**","29caff90":"### define your computing graph","7238d2de":"**Intialize Variable - Method 4 - Recommened Way**","3ebc43c5":"**Intialize Variable - Method 3**","d408f56e":"****1. python assert function\n2. genreate half 0 lables, half 1 lables: np.array([0] * (N\/\/2) + [1] * (N\/\/2))\n3. np.argmax(prediction, axis=1)\n4. load csv file with delimiter and generate txt file: np.genfromtxt(os.path.join(data_folder, \"digit_mnist_trainx.csv\"), delimiter=',')\n5. np.unique(trainY)****","b9b35f7f":"1. ![](http:\/\/)### Tensorflow - Logistic Regression","d3a3e129":"**Intialize Variable - Method 2**","14eb6862":"### **Tensorflow - Naive Bayes**"}}