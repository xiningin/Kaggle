{"cell_type":{"f9409796":"code","2d328ae0":"code","e4301300":"code","0fb69752":"code","f03f5db6":"code","bcd4799c":"code","cf0d6b22":"code","fda38b90":"code","ad530da0":"code","8d540ed3":"code","0339ad28":"code","1dbebcc7":"code","4621f267":"code","1e465fc2":"code","a15a20c2":"code","e6ca7101":"code","8885631e":"code","a3ef4c6e":"code","da5cebf2":"code","1c176571":"code","555adc2e":"code","605ed35c":"code","6a687906":"code","8c2a3812":"code","39ea272b":"code","173cb884":"code","af466857":"code","dcaa907b":"code","0cd5e4d7":"code","9aef1c6a":"code","82095cb1":"code","f2858013":"code","ea2b9cf2":"code","4febd918":"code","7188daf2":"code","a0661bcf":"code","dbccb04a":"code","0f9471bd":"code","1369b89c":"code","5d96029e":"code","a5d056c6":"code","5f98176b":"code","3d925a60":"code","83d46f5c":"code","0d6b889b":"code","46579bc2":"code","4447e705":"code","ecd62e52":"code","81a5b84e":"code","662451a4":"code","e7d371cd":"code","1ef848b2":"code","92263910":"code","8464403b":"code","eb6fb7bd":"code","cd9ea598":"code","c5abce11":"code","e6ce8c43":"code","cf5d8f13":"code","cf0bd576":"code","d96e5384":"code","32bf218b":"code","68787f40":"code","8c7d1700":"code","89154a43":"code","24f4529d":"code","8a1f0a2e":"code","31005e62":"code","4e2ed669":"code","7e557046":"code","d2ac6bff":"code","bae9ffe0":"code","d0aa901b":"markdown","9ea25e19":"markdown","967c469e":"markdown","930829ce":"markdown","de1e39f2":"markdown","9dada75a":"markdown","cdb12afa":"markdown","a4a6aef1":"markdown","db11f41d":"markdown","6c51f463":"markdown","ea93e0b7":"markdown","7e86b44a":"markdown","ca8ba955":"markdown","4a40f0fa":"markdown","9f350f50":"markdown","4c377f0e":"markdown","63bea563":"markdown","3a8ddf3b":"markdown","24a657cc":"markdown","78889e21":"markdown","31162bac":"markdown","197cc625":"markdown","d2624c49":"markdown","e965b515":"markdown","277f638c":"markdown","e1792e5c":"markdown","3e3cdc5c":"markdown","9917836d":"markdown","bdb0d336":"markdown","48b2687c":"markdown","fd81327d":"markdown","81a1b1eb":"markdown","2ed2bea4":"markdown","7466f1a7":"markdown","4327d196":"markdown","4b3f67bf":"markdown","07c318ed":"markdown","d29a2d38":"markdown","7211d1e0":"markdown","aeec43e1":"markdown","e9ada04a":"markdown","f16d9288":"markdown","2d123985":"markdown","f187b996":"markdown","f7e5a18f":"markdown","7f6cf90e":"markdown","80d17617":"markdown","a277b113":"markdown","42ba8786":"markdown","8971adf1":"markdown","9d4fe8e8":"markdown","7f0e5547":"markdown","73724385":"markdown","d7933703":"markdown","5e3e296e":"markdown","41c01126":"markdown","b6d262d2":"markdown","8e284958":"markdown","663e6bd1":"markdown","dbf0e077":"markdown","662b62ca":"markdown","4f58efcf":"markdown","809757da":"markdown","0c56e0e2":"markdown","4d4cb4bb":"markdown","b8ab3b09":"markdown","09cbe965":"markdown","fce2f350":"markdown","d7668c5f":"markdown","44f6dd9b":"markdown","abd59bdc":"markdown","14713f17":"markdown","ecf38e34":"markdown","8fcb2531":"markdown","64161d00":"markdown","f230cef8":"markdown","3b4dbc0c":"markdown","91bc7fff":"markdown"},"source":{"f9409796":"# Importing filterwarnings to ignore warning messages\nimport warnings\nwarnings.filterwarnings('ignore')","2d328ae0":"# Importing the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e4301300":"# Reading the bikes rental  dataset into 'bikes' dataframe\n\nbikes=pd.read_csv(\"..\/input\/boom-bikes-data\/day.csv\")\nbikes.head()","0fb69752":"# looking at the shape of the bikes dataset\nbikes.shape","f03f5db6":"bikes.describe()","bcd4799c":"#Finding out the datatype of the columns in the bikes dataset:\nbikes.info()","cf0d6b22":"# To check if there are any missing values in the dataset\n\nimport missingno as mn\nmn.matrix(bikes)","fda38b90":"# Dropping the columns : instant,dteday,casual,registered\n\ncols=[\"instant\",\"dteday\",\"casual\",\"registered\"]\n\nbikes=bikes.drop(columns=cols,axis=1)","ad530da0":"# Renaming some columns for more clearity \n\nbikes.rename(columns={'hum':'humidity','cnt':'count','mnth':'month','yr':'year'},inplace=True)","8d540ed3":"# Mapping the categorical column : season into its categories\n\nseason_cat={1:\"spring\",2:\"summer\",3:\"fall\",4:'winter'}\n\nbikes.season=[season_cat[item] for item in bikes.season]","0339ad28":"# Mapping the categorical column : weathersit into its categories\n\nweather_cat={1:\"clear\",2:\"mist & cloudy\",3:\"light rain & snow\",4:'heavy rain & snow'}\n\nbikes.weathersit=[weather_cat[item] for item in bikes.weathersit]","1dbebcc7":"# Mapping the categorical column : month into its categories\n\nmonth_cat={1: 'Jan' , 2: 'Feb' , 3: 'Mar' , 4: 'Apr' , 5: 'May' , 6: 'Jun' , 7: 'Jul' , 8: 'Aug' , 9: 'Sep' , 10: 'Oct' , 11: 'Nov' , 12: 'Dec'}\n\nbikes.month=[month_cat[item] for item in bikes.month]","4621f267":"# Mapping the categorical column : weekday into its categories\n\nwkday_cat={0: 'Sunday',1: 'Monday',2: 'Tuesday',3: 'Wednesday',4: 'Thursday',5: 'Friday',6: 'Saturday'}\n\nbikes.weekday=[wkday_cat[item] for item in bikes.weekday]","1e465fc2":"# Mapping the categorical column : Year into its categories\n\nyr_cat={0: '2018',1: '2019'}\n\nbikes.year=[yr_cat[item] for item in bikes.year]","a15a20c2":"# Analysing the demand in various seasons\nsns.barplot(x='season',y='count',data=bikes)","e6ca7101":"# Analysing the demand in year 2018 and 2019\nsns.set_style('whitegrid')\nplt.figure(figsize=(6,4))\nsns.barplot(x='year',y='count',data=bikes)","8885631e":"# Analysing the demand in various months\nsns.set_style('whitegrid')\nplt.figure(figsize=(12,6))\nsns.barplot(x='month',y='count',data=bikes,hue='year',palette='ocean')","a3ef4c6e":"# Analysing the demand in various weathers\nsns.set_style('whitegrid')\nplt.figure(figsize=(9,4))\nsns.barplot(x='weathersit',y='count',data=bikes)","da5cebf2":"# Analysing the demand in various weekdays\nsns.set_style('whitegrid')\nplt.figure(figsize=(9,4))\nsns.barplot(x='weekday',y='count',data=bikes)","1c176571":"# Analysing the demand based on workingday or not a workingday\nsns.set_style('whitegrid')\n# plt.figure(figsize=(9,4))\nsns.barplot(x='workingday',y='count',data=bikes)","555adc2e":"sns.pairplot(bikes, x_vars=['temp','atemp','humidity','windspeed'], y_vars='count',size=4, aspect=1 )\nplt.show()","605ed35c":"plt.figure(figsize=(11,7.5))\nsns.heatmap(bikes.corr(),annot=True,cmap='YlGnBu')","6a687906":"# Dropping the variable 'atemp' \nbikes=bikes.drop(\"atemp\",axis=1)","8c2a3812":"# Creating the dummy variables for the variables month,season,weathersit,weekday and storing them \n# in new variable 'months',seasons','weather' and 'weekdays' respectively and \n# dropping the first column from these variables using 'drop_first = True'\n\nmonths= pd.get_dummies(bikes['month'],drop_first=True,prefix='month')\n\nseasons = pd.get_dummies(bikes['season'],drop_first=True,prefix='season')\n\nweather= pd.get_dummies(bikes['weathersit'],drop_first=True,prefix='weather')\n\nweekdays= pd.get_dummies(bikes['weekday'],drop_first=True,prefix='day')\n\nyears= pd.get_dummies(bikes['year'],drop_first=True,prefix='year')","39ea272b":"# Add the above created dummy variables to the original bikes dataframe\nbikes = pd.concat([bikes,months,seasons,weather,weekdays,years], axis = 1)\n\n# Looking at the top rows of our dataframe.\nbikes.head()","173cb884":"# As we have created dummy variables for the categorical variables , now we will drop those categorical variables .\n\nbikes.drop(['season','weathersit','weekday','month','year'],axis=1,inplace=True)","af466857":"# Looking at the shape of dataframe after dropping the above variables\nbikes.shape","dcaa907b":"from sklearn.model_selection import train_test_split\n\nbikes_train, bikes_test = train_test_split(bikes, train_size = 0.7, test_size = 0.3, random_state = 100)","0cd5e4d7":"#Looking at the shape of the train dataset.\nbikes_train.shape","9aef1c6a":"#Looking at the shape of the test dataset.\nbikes_test.shape","82095cb1":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","f2858013":"# Applying Scaling on the continuous columns : 'windspeed' , 'temp' , 'humidity' , 'count'\nvars = ['windspeed' , 'temp' , 'humidity','count']\n\nbikes_train[vars] = scaler.fit_transform(bikes_train[vars])\n\nbikes_train.head()","ea2b9cf2":"y_train = bikes_train.pop('count')\nX_train = bikes_train","4febd918":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","7188daf2":"# Running RFE with selecting 15 variables \nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nnp.random.seed(0)\nrfe = RFE(lm, 15)             # running RFE,15 is the number of variables we want RFE to select\nrfe = rfe.fit(X_train, y_train)","a0661bcf":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))  \n\n# rfe_support_ : tells whether RFE selected the variable or not\n# rfe.ranking_ : tells the next best variable to be selected and ranks accordingly , The numbers \n#                 beside the variables indicate the importance of that variable.","dbccb04a":"# Looking at the cols that RFE selected\ncol = X_train.columns[rfe.support_]\ncol","0f9471bd":"# Creating X_train_rfe dataframe with RFE selected variables\nX_train_rfe = X_train[col]","1369b89c":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)","5d96029e":"# Running the linear model\nlm = sm.OLS(y_train,X_train_rfe).fit()   ","a5d056c6":"#Looking at the summary of our linear model\nlm.summary()","5f98176b":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3d925a60":"# Dropping the const variable\nX_train_new = X_train_rfe.drop([\"const\"], axis = 1)","83d46f5c":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\n\n#Running the linear model\nlm = sm.OLS(y_train,X_train_lm).fit()\n\n##Looking at the summary of our linear model\nlm.summary()","0d6b889b":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","46579bc2":"# Dropping the 'humidity' variable\nX_train_new = X_train_new.drop([\"humidity\"], axis = 1)","4447e705":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\n\n#Running the linear model\nlm = sm.OLS(y_train,X_train_lm).fit()\n\n##Looking at the summary of our linear model\nlm.summary()","ecd62e52":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","81a5b84e":"# Dropping the 'workingday' variable\nX_train_new = X_train_new.drop([\"workingday\"], axis = 1)","662451a4":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\n\n#Running the linear model\nlm = sm.OLS(y_train,X_train_lm).fit()\n\n##Looking at the summary of our linear model\nlm.summary()","e7d371cd":"# Dropping the 'day_Saturday' variable\nX_train_new = X_train_new.drop([\"day_Saturday\"], axis = 1)","1ef848b2":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\n\n#Running the linear model\nlm = sm.OLS(y_train,X_train_lm).fit()\n\n##Looking at the summary of our linear model\nlm.summary()","92263910":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8464403b":"# Dropping the 'month_Jan' variable\nX_train_new = X_train_new.drop([\"month_Jan\"], axis = 1)","eb6fb7bd":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\n\n#Running the linear model\nlm = sm.OLS(y_train,X_train_lm).fit()\n\n##Looking at the summary of our linear model\nlm.summary()","cd9ea598":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c5abce11":"# Dropping the 'month_Sep' variable\nX_train_new = X_train_new.drop([\"month_Sep\"], axis = 1)","e6ce8c43":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\n\n#Running the linear model\nlm = sm.OLS(y_train,X_train_lm).fit()\n\n##Looking at the summary of our linear model\nlm.summary()","cf5d8f13":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","cf0bd576":"# Dropping the 'season_summer' variable\nX_train_new = X_train_new.drop([\"season_summer\"], axis = 1)","d96e5384":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\n\n#Running the linear model\nlm = sm.OLS(y_train,X_train_lm).fit()\n\n##Looking at the summary of our linear model\nlm.summary()","32bf218b":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","68787f40":"y_train_count = lm.predict(X_train_lm)","8c7d1700":"# Plotting the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_count), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                   \nplt.xlabel('Errors', fontsize = 18)      ","89154a43":"# Applying Scaling on the continuous columns : 'windspeed' , 'temp' , 'humidity'\nvars = ['windspeed' , 'temp' , 'humidity','count']\n\nbikes_test[vars] = scaler.transform(bikes_test[vars])","24f4529d":"y_test = bikes_test.pop('count')\nX_test = bikes_test","8a1f0a2e":"# Using our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","31005e62":"# Making predictions\ny_pred = lm.predict(X_test_new)","4e2ed669":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16) ","7e557046":"from sklearn.metrics import r2_score\nr2=r2_score(y_test, y_pred)\nr2","d2ac6bff":"n = X_test_new.shape[0]      # n is number of rows in X_test_new\n\np = X_test_new.shape[1]     # p= Number of features\/predictors which is number of columns in X_test_new\n\n# Calculating Adjusted R-squared value using the formula\n\nadjusted_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\nadjusted_r2","bae9ffe0":"# calculating the Mean Squared Error , Root Mean Squared Error and Mean Absolute error\nfrom sklearn import metrics\n\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","d0aa901b":"### Reporting the Final Model:","9ea25e19":"#### 5. Weekday :","967c469e":"#### 2. Year :\n* 0: 2018\n* 1: 2019","930829ce":"## Residual Analysis of the training set\n\nNow we will try to check one of the major assumptions of the linear regression ie., checking whether the error terms are normally distributed by plotting a histogram of the error terms and see what it looks like.","de1e39f2":"We see that the VIF value for the 'const' is very high , so we will drop the const.","9dada75a":"#### Observation: Bikes rented are more in the Clear weather .","cdb12afa":"#### 4.Weekday column mapping as:\n* 0: Sunday\n* 1: Monday \n* 2: Tuesday \n* 3: Wednesday\n* 4: Thursday\n* 5: Friday\n* 6: Saturday","a4a6aef1":"### Calculating Adj. R-squared value for Test set","db11f41d":"Model 4","6c51f463":"### Mapping the Categorical columns to their actual category names :","ea93e0b7":"## Exploratory Data Analysis :","7e86b44a":"**From the coefficients that we get for the variables, our `linear regression equation` becomes:**\n\n**Count** =  0.2505 + 0.4693 X *`temp`* - 0.1482 X *`windspeed`* + 0.2335 X *`year_2019`* - 0.0845 X *`month_Jul`*  -              0.1122 X *`season_spring`* + 0.0464 X *`season_winter`* - 0.2810 X *`weather_light rain & snow`* - 0.0794 X            *`weather_mist & cloudy`* - 0.0926 X *`holiday`* ","ca8ba955":"The predicted values have a linear relationship with actual values.","4a40f0fa":"## DATA PREPARATION :","9f350f50":"## Data Modelling and Evaluation\n\n#### We will be using the `Mixed Approach` for model building ie., firstly we will select 15 variables by using the `Automated Approach of RFE` and then using `Manual Approach for removing variables one by one based on the Pvalues and VIF values.` \n\nWe will be using the **LinearRegression function from SciKit Learn** for its compatibility with RFE (Recursive Feature Elimination which is a utility from sklearn)","4c377f0e":"#### Observation: Bikes rented are more on Saturday and Friday .","63bea563":"#### Thanks for Reading !!!\n\n#### Please provide feedback in comments section . ","3a8ddf3b":"#### Observation: Bikes rented are more during the Fall season.","24a657cc":"### Model - 1","78889e21":"**The three most significant variables affecting the demand for shared bikes are :**\n* **temperature** \n* **year**\n* **season winter** \n\nas these features are having positive coefficients and an increase in them is going to result into an increase in the demand for shared bikes .","31162bac":"#### Renaming columns :","197cc625":"Model 3","d2624c49":"In the above model , according to the pvalues , all the variables are significant. Now we will have a look at the VIF of the variables to find if any multicollinearity exists betweeen any variable.","e965b515":"Rebuilding the model without humidity","277f638c":"\n**From the coefficients that we get for the variables, our `linear regression equation` becomes:**\n\n\n**Count** =  0.2505 + 0.4693 X *`temp`* - 0.1482 X *`windspeed`* + 0.2335 X *`year_2019`* - 0.0845 X *`month_Jul`*  -              0.1122 X *`season_spring`* + 0.0464 X *`season_winter`* - 0.2810 X *`weather_light rain & snow`* - 0.0794 X            *`weather_mist & cloudy`* - 0.0926 X *`holiday`* ","e1792e5c":"#### 6. Working day :  \n* 0 : holiday or weekend\n* 1 : neither holiday nor weekend ie., working day","3e3cdc5c":"In the above model , according to the pvalues , all the variables are significant. Now we will have a look at the VIF of the variables to find if any multicollinearity exists betweeen any variable.","9917836d":"### F-Statistics\n\n`F-Statistics is used for testing the overall significance of the Model.`\n\nHigh value for the F-Statistics means the model is more significant and vice-versa.\n\n* `F-statistic: 268.3\n* `Prob (F-statistic):  4.56e-185\n\n\n#### The F-Statistics value of 268.3 (greater than 1) and the Prob(F-statistic) of '~0.000'(very low) states that the overall model is significant.\n\n#### So we can conclude it as our final model with 9 variables .","bdb0d336":"#### Observation: Bikes rented are more in the year 2019 as compared to 2018.","48b2687c":"## Making Predictions","fd81327d":"### Problem Statement:\n\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\n* **Which variables are significant in predicting the demand for shared bikes.**\n* **How well those variables describe the bike demands**\n\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. ","81a1b1eb":"### Dividing into X_test and y_test","2ed2bea4":"#### Observation: \n* **Bike rental counts show a `positive correlation` with `temp` and `atemp`** and therefore **bike rental counts increase at higher temperatures and vice-versa**\n\n* **Bike rental counts show a `negative correlation` with `humidity`** and therefore **bike rental counts are less at high humidity levels and vice-versa** \n\n* **Bike rental counts show a `negative correlation` with `windspeed`** and therefore **bike rental counts are less at high windspeed and vice-versa .**\n","7466f1a7":"#### 1. Season :","4327d196":"### Creating Dummy Variables for the Categorical variables :\n<font color=blue>month , season , weathersit , weekday<\/font>","4b3f67bf":"Rebuilding the model without workingday","07c318ed":"#### Observation : It is seen that the variables 'temp' and 'atemp'  have a high correlation value of 0.99 , ie., they are highly correlated to each other , therefore we need to drop one of them . ","d29a2d38":"### Comparison between the results on Train and Test datasets:\n\n* `R-squared Value`:<br><br>\n    * **Train set : <font color=blue>82.8% <\/font>** \n    * **Test set : <font color=blue>80.5% <\/font>**\n<br><br>\n\n* `Adj R-squared Value`:<br><br>\n     * **Train set : <font color=blue>82.5%<\/font>**\n     * **Test set : <font color=blue>79.6%<\/font>**","7211d1e0":"### Splitting the Data into Training and Testing Sets","aeec43e1":"### Analysis of various variables w.r.t  'count' ie., target variable through Visualization:","e9ada04a":"* As we can see that the **difference between the R-squared value for the train and test dataset is not more than 5% , therefore we can say that this is a good model** . \n\n* As we can see that the **difference between the Adj R-squared value for the train and test dataset is not more than 5% , therefore we can say that this is a good model** . ","f16d9288":"Checking for VIFs again","2d123985":"Model 5 ","f187b996":"#### In our bikes dataset , we have the following types of columns:\n* **Categorical columns: <font color=blue>season , month , weathersit , weekday<\/font>**\n* **Continuous\/Numerical columns : <font color=blue>windspeed , hum , temp , atemp , count<\/font>**\n* **Binary\/Dichotomous columns:<font color=blue> yr , holiday , workingday<\/font>**","f7e5a18f":"#### 3 . Month column mapping as:<br>\n* 1: Jan , 2: Feb , 3: Mar , 4: Apr , 5: May , 6: Jun , 7: Jul , 8: Aug , 9: Sep , 10: Oct , 11: Nov , 12: Dec","7f6cf90e":"From the regression model above , we have the following variables and their coefficients which are significant in predicting the demand for shared bikes:\n\n* temp = 0.4693\n* windspeed = -0.1482 \n* year_2019 = 0.2335\n* month_Jul = -0.0845\n* season_spring = -0.1122\n* season_winter = 0.0464\n* weather_light rain or snow = -0.2810 \n* weather_mist and cloudy = -0.0794\n* holiday = -0.0926","80d17617":"**R-squared = 82.8 which means that 82.8 % of the variance for the target variable ie., '`count`'  is explained by the predictor variables , and hence we say that it is a good model.**","a277b113":"In the above model, the variable 'workingday' is having high vif value , therefore we drop workingday.","42ba8786":"#### Observation: Bikes rented are more in the month of Sep 2019.","8971adf1":"## Multiple Linear Regression Model \n\n## <font color=blue>Prediction of Demand for Shared Bikes","9d4fe8e8":"### 7. Visualizing the continuous variables 'temp','atemp','humidity','windspeed' w.r.t 'count ' variable through a pairplot.","7f0e5547":"#### 3. Month :","73724385":"#### 2 . Weathersit column mapping as:<br>\n* 1: clear \n* 2: mist & cloudy \n* 3: light rain & snow \n* 4: heavy rain & snow","d7933703":"## Model Evaluation","5e3e296e":"#### 1 . Season column mapping as:<br>\n* 1: spring \n* 2: summer \n* 3: fall \n* 4: winter","41c01126":"### Building model using statsmodel, for the detailed statistics","b6d262d2":"We see there are no missing values in our bikes dataset.","8e284958":"**Since the errors terms are normally distributed with mean=0, thus our assumption is satisfied in our model.**","663e6bd1":"#### 5.Year column mapping as:\n* 0: 2018\n* 1: 2019 ","dbf0e077":"We see that the VIF value for the variable 'humidity' is very high so we are going to drop `humidity` variable.","662b62ca":"Rebuilding the model without const","4f58efcf":"### Dividing into X and Y sets for the model building :","809757da":"#### Model 8 is our final model with :\n#### * `R-squared for training set` : <font color= blue>       0.828 ( 82.8 %)<\/font>\n#### * `Adj R-squared for training set`: <font color= blue> 0.825 ( 82.5 %)<\/font>","0c56e0e2":"#### In our bikes dataset,we have some unnecessary columns like:<font color=blue> instant , dteday , casual , registered <\/font>, so we are going to drop these columns. ","4d4cb4bb":"### Calculating R-squared value for Test set","b8ab3b09":"#### 4. Weather :","09cbe965":"Model 2","fce2f350":"#### Observation: Bikes rented are more on working days .","d7668c5f":"#### From the above model ,we see that the Pvalues of all the 9 variables are very low(approx equal to 0) and VIF values are also less than 5 , which is acceptable.\n\n##### We will now see the F-Statistic value .","44f6dd9b":"### Checking the Correlations among variables:","abd59bdc":"### Business Goal:\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","14713f17":"Model 6","ecf38e34":"### Applying the scaling on the test set also","8fcb2531":"**VIF (Variable Inflation Factor) Calculation**","64161d00":"### Scaling the Features \n\nFor Scaling the features in our bikes dataset , we will use `MinMax scaling`.","f230cef8":"### RFE\n`Recursive feature elimination`","3b4dbc0c":"Model 8","91bc7fff":"Model 7"}}