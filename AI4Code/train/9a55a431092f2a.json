{"cell_type":{"103e9bd4":"code","55afbcbc":"code","11697747":"code","9f30c3d6":"code","85c6fc68":"code","481d6108":"code","d21fa310":"code","86350744":"code","f346fe0f":"code","88430007":"code","b8c0eac3":"code","8ce4083e":"code","745eb342":"code","39cda0c7":"code","32eb7200":"code","7a1a30ba":"code","3d9ea28b":"code","3cf1cdb2":"code","65ece5da":"code","3cf7e15f":"code","38914cce":"code","fa3fa785":"code","3e8ad4e1":"code","ef6f008f":"code","4c0b3ad6":"code","93763c15":"code","6d159ea3":"code","1f3c8b81":"code","a9ed2dd6":"code","d232b044":"code","be76c86a":"code","acc497e6":"code","1918c2dd":"code","91dc55f5":"code","e90efaf8":"code","c5d3592a":"code","27c4af01":"code","121e7b49":"code","f0ce9f9e":"code","ca12f4e5":"code","45195f35":"code","bb580451":"code","9b141055":"code","20cd4b68":"code","d69fc6d7":"markdown","f161a914":"markdown","2e57cca5":"markdown","187986a2":"markdown","ba5064ec":"markdown","7bd9f402":"markdown","086c71f0":"markdown","d19a18fe":"markdown","410a2441":"markdown","156c6743":"markdown","082f72da":"markdown","f080b9f9":"markdown","30cf11d5":"markdown","fa59c17f":"markdown","445f2701":"markdown","75c7a04a":"markdown","c64f6c83":"markdown","29965f9d":"markdown","6ec4da8a":"markdown","2e2ff0b5":"markdown","8a966cf2":"markdown","ed23735f":"markdown","a42a3807":"markdown","c55a4130":"markdown","19b102a3":"markdown","ed970f42":"markdown","919e6fa8":"markdown","b332393d":"markdown","483ab3ee":"markdown","6b0143cb":"markdown","91640def":"markdown","596bf21a":"markdown","38b6c1fc":"markdown","dc76b9ca":"markdown"},"source":{"103e9bd4":"# import libraries\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport sklearn","55afbcbc":"# load data\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\n\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","11697747":"print(train_data.shape) ","9f30c3d6":"train_data.info()","85c6fc68":"# Find missing values\ntrain_data.isnull().sum()","481d6108":"test_data.isnull().sum()","d21fa310":"# Visualize the first rows of the table\ntrain_data.head()","86350744":"# compute and display summary statistics\ntrain_data.describe()","f346fe0f":"print(sum(train_data.Survived)) # 342 out of 891 passenger survived","88430007":"sns.countplot(x='Survived', hue='Sex', data=train_data)","b8c0eac3":"women = train_data.loc[train_data.Sex == 'female']['Survived']\nwomen_survived = sum(women)\ntot_women = len(women)\nrate_women = women_survived\/tot_women * 100\nprint(women_survived,'women survived of the',tot_women,'women on board','({:.2f}'.format(rate_women),'%)')","8ce4083e":"men = train_data.loc[train_data.Sex == 'male']['Survived']\nmen_survived = sum(men)\ntot_men = len(men)\nrate_men = men_survived\/tot_men * 100\nprint(men_survived,'men survived of the',tot_men,'men on board','({:.2f}'.format(rate_men),'%)')","745eb342":"train_data.groupby(['Sex','Pclass']).mean()","39cda0c7":"plt.figure(figsize=(8,8))\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data = train_data)\nplt.show()","32eb7200":"fig, axes = plt.subplots(nrows=1,ncols=2,figsize=(10, 4))\nwomen = train_data[train_data['Sex']=='female']\nmen = train_data[train_data['Sex']=='male']\n\nax = sns.histplot(women[women['Survived']==1].Age.dropna(), ax=axes[0], bins=range(0, 80, 5), label = 'survived', color='blue', edgecolor='blue', alpha=0.1)\nax = sns.histplot(women[women['Survived']==0].Age.dropna(), ax=axes[0], bins=range(0, 80, 5), label = 'died', color='red', edgecolor='red', alpha=0.1)\n\nax.set_title('Female')\n\nax = sns.histplot(men[men['Survived']==1].Age.dropna(), ax=axes[1], bins=range(0, 80, 5), label = 'survived',  color='blue', edgecolor='blue', alpha=0.2)\nax = sns.histplot(men[men['Survived']==0].Age.dropna(), ax=axes[1], bins=range(0, 80, 5), label = 'died', color='red', edgecolor='red', alpha=0.2)\nax.legend()\n_ = ax.set_title('Male')","7a1a30ba":"train_data[train_data['Age']<18].groupby(['Sex','Pclass']).mean()","3d9ea28b":"fig1, ax1 = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\ntrain_data.groupby('Sex')['Survived'].mean().plot.bar(ax=ax1[0], rot=0, title='Female or Male', edgecolor=\"k\", xlabel='')\ntrain_data.groupby('Pclass')['Survived'].mean().plot.bar(ax=ax1[1], rot=0, title='Passenger Class', edgecolor=\"k\", xlabel='')\ntrain_data.groupby('Embarked')['Survived'].mean().plot.bar(ax=ax1[2], rot=0, title='Port of Embarkation', edgecolor=\"k\", xlabel='')\n\nplt.tight_layout()\nplt.show()","3cf1cdb2":"fig2, ax2 = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\nax = sns.countplot(x='Embarked', hue='Pclass', data=train_data, ax=ax2[0])\nax = sns.countplot(x='Embarked', hue='Sex', data=train_data, ax=ax2[1])","65ece5da":"train_data[[\"Fare\"]].describe()","3cf7e15f":"print('Mean:\\n',train_data.groupby(['Sex', 'Pclass'])['Fare'].mean(),'\\n')\nprint('Median:\\n',train_data.groupby(['Sex', 'Pclass'])['Fare'].median())","38914cce":"sns.catplot(x='Fare', y='Survived', row='Pclass', kind='box',\n            orient='h', height=1.5, aspect=4, data=train_data)","fa3fa785":"women = train_data[(train_data['Sex'] == 'female')]\nmen = train_data[(train_data['Sex'] == 'male')]\n\nfig, axes = plt.subplots(nrows=1,ncols=2,figsize=(10, 10))\nplt.figure(figsize=(8,8))\nax = sns.boxplot(data=women, x=\"Pclass\", y=\"Fare\", hue=\"Survived\", palette=\"Set3\", ax=axes[0]).set_title('Females')\n\nax = sns.boxplot(data=men, x=\"Pclass\", y=\"Fare\", hue=\"Survived\", palette=\"Set3\", ax=axes[1]).set_title('Males')","3e8ad4e1":"# Extract and then remove the targets from the training data \ntargets = train_data.Survived\ntrain_data.drop(['Survived'], axis=1, inplace=True)\n\n# Merge train and test data for feature engineering\ncombined = train_data.append(test_data) \ncombined.reset_index(inplace=True)\n\n# Remove non informative features\ncombined.drop(['index','PassengerId','Name','Ticket','Cabin','Fare','Embarked'], axis=1, inplace=True) ","ef6f008f":"print(combined.shape)","4c0b3ad6":"combined.head()","93763c15":"print(combined.iloc[:891].Age.isnull().sum()) # Number of missing ages in train set","6d159ea3":"print(combined.iloc[891:].Age.isnull().sum()) # Number of missing ages in test set","1f3c8b81":"grouped_train = combined.iloc[:891].groupby(['Sex','Pclass'])\ngrouped_median_train = grouped_train.median()\ngrouped_median_train = grouped_median_train.reset_index()[['Sex', 'Pclass', 'Age']]","a9ed2dd6":"grouped_median_train.head()","d232b044":"def fill_age(row):\n    condition = (\n        (grouped_median_train['Sex'] == row['Sex']) & \n        (grouped_median_train['Pclass'] == row['Pclass'])\n    ) \n    return grouped_median_train[condition]['Age'].values[0]\n\n\ndef process_age():\n    global combined\n    # a function that fills the missing values of the Age variable\n    combined['Age'] = combined.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n    return combined","be76c86a":"combined = process_age()\ncombined.head()","acc497e6":"def process_age_group():\n    \n    global combined\n    combined['Child'] = combined['Age'].map(lambda s: 1 if s <= 12 else 0)\n    combined['Teenager'] = combined['Age'].map(lambda s: 1 if 13 <= s <= 19 else 0)\n    combined['Adult'] = combined['Age'].map(lambda s: 1 if s >= 20 else 0)\n    \n    # removing \"Age\"\n    combined.drop('Age',axis=1,inplace=True)\n    \n    return combined","1918c2dd":"combined = process_age_group()\ncombined.head()","91dc55f5":"def process_sex():\n    global combined\n    # mapping string values to numerical one \n    combined['Sex'] = combined['Sex'].map({'male':1, 'female':0})\n    \n    return combined","e90efaf8":"combined = process_sex()\ncombined.head()","c5d3592a":"def process_pclass():\n    \n    global combined\n    # encoding into 3 categories:\n    pclass_dummies = pd.get_dummies(combined['Pclass'], prefix='Pclass')\n    \n    # adding dummy variable\n    combined = pd.concat([combined, pclass_dummies],axis=1)\n    \n    # removing \"Pclass\"\n    combined.drop('Pclass',axis=1,inplace=True)\n    \n    return combined","27c4af01":"combined = process_pclass()\ncombined.head()","121e7b49":"def process_family():\n    \n    global combined\n    # introducing a new feature : the size of families (including the passenger)\n    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n    \n    # introducing other features based on the family size\n    combined['Singleton'] = combined['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    combined['SmallFamily'] = combined['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n    combined['LargeFamily'] = combined['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n    \n    # removing Parch and SibSp\n    combined.drop('Parch',axis=1,inplace=True)\n    combined.drop('SibSp',axis=1,inplace=True)\n    combined.drop('FamilySize',axis=1,inplace=True)\n    \n    return combined","f0ce9f9e":"combined = process_family()\ncombined.head()","ca12f4e5":"from sklearn.linear_model import LogisticRegression","45195f35":"def recover_train_test_target():\n    global combined\n    \n    targets = pd.read_csv('..\/input\/titanic\/train.csv', usecols=['Survived'])['Survived'].values\n    train = combined.iloc[:891]\n    test = combined.iloc[891:]\n    \n    return train, test, targets\n\ntrain, test, targets = recover_train_test_target()","bb580451":"clf = LogisticRegression(solver='liblinear')\nclf = clf.fit(train, targets)","9b141055":"output = clf.predict(test).astype(int)\ndf_output = pd.DataFrame()\naux = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_output['PassengerId'] = aux['PassengerId']\ndf_output['Survived'] = output\ndf_output[['PassengerId','Survived']].to_csv('logReg.csv', index=False)","20cd4b68":"print(df_output)","d69fc6d7":"# Titanic challenge: simple logistic regression model \nThis notebook shows how to solve the Titanic challenge with a simple logistic regression model. The notebook is divided in three parts:\n- Data exploration\n- Feature engineering\n- Classification: logistic regression\n\nThe goal is to predict the survival or the death of a given passenger based on a set of available variables. Having more features (in this case, using more variables) is not always advantageous, quite the opposite! Especially in logistic regression, we aim at the simpler model that can explain the data. ","f161a914":"## Data exploration","2e57cca5":"### PClass variable\nThis function encodes the values of Pclass (1,2,3) using a dummy encoding.","187986a2":"Based on this data we can say:\n- 38% of the passengers survived\n- the oldest person was 80 years old and the yougest person was less than one year old\n- More than 50% of people did not come with any siblings or spouses\n- The highest fare was 512.3 and the lowest was 0","ba5064ec":"Now the predictions contained in the file logReg.csv can be submitted to the competition!","7bd9f402":"### Family variable","086c71f0":"The Sex of the passenger is an important predictor: 74.20 % of the women onboard survived with respect of only 18.89 % of the men. Therefore a very simple model that predicts the survival on the basis of the variable Sex only (if passenger is female predict survival, otherwise death) would have an accuracy of about 74.20 %.","d19a18fe":"A large number of values is missing for the variables Cabin (77%) and Age (20 %). Only 2 values are missing for the variable Embarked. We will drop the variable Cabin since the proportion of missing values is too large, whereas we will fill in the missing variables for Age and Embarked. ","410a2441":"For modeling the effect that age has on the chance of survival of a passenger, we introduce a categorical variable \"AgeGroup\" with levels Child (0-12 y.o.), Teenager (13-19 y.o.), Adult(>= 20 y.o.)","156c6743":"Let's explore the relationship between the ticket price paid by a passenger (Fare) and their chance of survival. Does the variable Fare provide additional information than only Pclass?","082f72da":"#### Missing values","f080b9f9":"The comparison of mean and median values shows that there are certainly some outliers. ","30cf11d5":"### Looking deeper into differences between females and males statistics","fa59c17f":"It seems that passenger embarked in Cherbourg have higher chance of survival then those embarked in Queenstown and Southampton. Let's look at the sex and class of the passenger embarked in each port. ","445f2701":"### Children\nNow we will look at how the age of the passenger affected their chance of survival.","75c7a04a":"At first it seems that the variable Fare may add additional information: both first and second class passengers who survived are likely to have paid a higher Fare than the other passengers of the same class. However, in each class females paid a higher fee than males, so it is likely that difference chance of survival related to the paid fare is due to the sex of the passenger. ","c64f6c83":"### Port of embarkation\n\nWe have seen that sex, class of the passenger and age are variables affecting the survival chance of a passenger. Now we'll look at the effect of the port of embarkation. ","29965f9d":"### Sex variable\nThis function maps the string values male and female to 1 and 0 respectively.","6ec4da8a":"## Classification: logistic regression","2e2ff0b5":"The training data consists of 891 observations of 12 variables.","8a966cf2":"Interestingly, we notice that:\n- the proportion of first class passengers embarked in Cherbourg is much higher with respect to the other two ports\n- A higher proportion of females has embarked both in Chenbourg and Queenstown than in Southampton. \n\nSince sex and class are the reasons behind the highest survival rate of passengers embarked in Chenbourg and Queenstown with respect to those embarked in Southampton, including the variable Embarked in our model would cause redundancy. Why would the port of embarkation make any difference? ","ed23735f":"#### Summary statistics","a42a3807":"The Fare value can range between 0 and 512. Let's see the mean and median values grouped by class and sex.","c55a4130":"### Model based only on gender \n\nHow the 38 %  passenger who survived are distributed among male and females?","19b102a3":"This function introduces 4 new features:\n- FamilySize : the total number of relatives including the passenger (him\/her)self.\n- Sigleton : a boolean variable that describes families of size = 1\n- SmallFamily : a boolean variable that describes families of 2 <= size <= 4\n- LargeFamily : a boolean variable that describes families of size > 5","ed970f42":"Children have a higher chance of survival, especially in the first and second class. This confirms the known fact that, during the slow sinking of the Titanic, the crew enforced the order of saving women and children first. ","919e6fa8":"There are many missing values for the variables Cabin and Age also in the test data, plus one missing value for the Fare variable. ","b332393d":"### Age variable\nThe Age variable is missing a large number of values. Simply replacing them with the mean or the median age is not the best solution since the age differs by categories of passengers. \nTherefore, we will fill in the missing values on the base of both Sex and Class of the passenger.\n\nTo avoid data leakage from the test set, we fill in missing ages in the train using the train set and we fill in ages in the test set using values calculated from the train set as well.","483ab3ee":"Grouping passengers based on Sex and Ticket class (Pclass) we can notice other difference between survival rates:\n- Women in the first and second class are highly likely to survive, while women in the third class have 50 % chance of survival.\n- Men in the first class are almost 3-times more likely to survive than men in the third class.\n\nWe can confirm this information also with barplots. Note that by default, the barplot() function draws error bars in the plot with 95% confidence interval.","6b0143cb":"### Fare","91640def":"### Explore variables","596bf21a":"First we create a function that fills in the missing age based on Sex and Class.","38b6c1fc":"The Survived column is the target variable, taking value of 1 if the passenger survived, 0 if the passenger died.\n\nThe other variables (possible predictors) are:\n\n- PassengerId: id of the passenger\n- Pclass: the passenger's class, which has possible values 1,2,3 (first, second and third class)\n- Name of the passenger\n- Sex\n- Age\n- SibSp: number of siblings and spouse traveling with the passenger \n- Parch: number of parents and children traveling with the passenger\n- Ticket number\n- Ticket Fare\n- Cabin number \n- Embarkation: Port of Embarkation, one of C (Cherbourg), Q (Queenstown), S (Southampton)","dc76b9ca":"## Feature engineering\n\nHaving more features is not always advantageous, quite the opposite! Redundant and correlated features affects negatively the performance of the model. For example the variable \"Fare\" is highly correlated with the class and gender of the passenger, and has therefore a prective power, but it becomes redundant in a model already cointaing \"class\" and \"sex\". Especially in logistic regression, we aim at the simpler model that can explain the data. Therefore here we will limit our predictors to the following most informative variables:\n\n- sex\n- age\n- class\n- family size\n\nWe will create categorical variables to better represent these factors."}}