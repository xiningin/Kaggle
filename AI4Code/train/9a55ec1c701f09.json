{"cell_type":{"620404ae":"code","d9b42680":"code","651abc4c":"code","ac8d8362":"code","66ebe572":"code","93823252":"code","82c6980e":"code","4d79a957":"code","5252c6e4":"code","b54233d2":"code","a1925db6":"code","517b928f":"code","9a26543b":"code","3cee45d5":"code","e29c6f2f":"code","f5802a71":"code","c7752365":"code","223df3dd":"code","39e5bd54":"code","d0d73f81":"code","62f067cb":"code","10001ca6":"code","6082a5a4":"code","ee4edfde":"code","4009b0d8":"code","947055f9":"code","8e9489e9":"code","f6a4e5b0":"code","01eaf948":"code","35c5824b":"code","8377546d":"code","1639f8cd":"code","1f67ee22":"code","23465279":"code","7ee249c8":"code","7268030f":"code","d2915680":"code","ceef6ad5":"code","aa4b55d3":"markdown","509d2e1f":"markdown","777af6e5":"markdown","ecfd1d8f":"markdown","f8f7de72":"markdown","81dc2577":"markdown","c0d0e2d2":"markdown","bf12920b":"markdown","0c8e1957":"markdown","9d878e7f":"markdown","93be4e43":"markdown"},"source":{"620404ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9b42680":"df = pd.read_csv('..\/input\/predict-test-scores-of-students\/test_scores.csv')\ndf.head()","651abc4c":"df.info()","ac8d8362":"fig, ax = plt.subplots()\nax.scatter(df.index, df['posttest']);","66ebe572":"sns.histplot(df['posttest']);","93823252":"df.columns","82c6980e":"print('Student_data_shape:',df.shape)\nprint('--------Unique records-----------------')\nprint(df.nunique())\nprint('----------Null records--------------')\nprint( df.isnull().sum())\nprint('-------Data type----------------')\nprint(df.dtypes)","4d79a957":"#Check for duplicate rows\ndf_duplicate=df[df.duplicated()]\ndf_duplicate.size","5252c6e4":"df.describe()","b54233d2":"gender_order = df['gender'].value_counts().index\np = sns.FacetGrid(df, row='gender', row_order=gender_order,  height=2, aspect=3,)\np.map(sns.kdeplot, \"posttest\");","a1925db6":"sns.catplot(x='school_setting',y='posttest',hue='gender', kind=\"box\", data=df);","517b928f":"p=sns.FacetGrid(df, col='school_setting', height=4, aspect=1)\np.map(sns.barplot, 'gender', 'posttest', order=['Male','Female']);","9a26543b":"p=sns.FacetGrid(df, col='school', col_wrap=4,height=4, aspect=1)\np.map(sns.barplot, 'gender','posttest',  order=['Male','Female']);","3cee45d5":"#school_type\np=sns.FacetGrid(df, col='school_type', height=4, aspect=1)\np.map(sns.barplot, 'gender', 'posttest', order=['Male','Female']);","e29c6f2f":"sns.catplot(x='school_type',y='posttest', kind=\"bar\", data=df);","f5802a71":"sns.catplot(x='school_setting',y='posttest', kind=\"bar\", data=df);","c7752365":"fig, axes = plt.subplots(1, 2, figsize=(15, 10))\nsns.regplot(ax=axes[0,], data=df[df['gender']=='Male'], x='pretest', y='posttest')\nsns.regplot(ax=axes[1], data=df[df['gender']=='Female'],color='red', x='pretest', y='posttest');","223df3dd":"# These columns contain strings\nfor label, content in df.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","39e5bd54":"# Turn categorical variables into numbers\nfor label, content in df.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        df[label] = pd.Categorical(content).codes  ","d0d73f81":"df.info()","62f067cb":"df.head().T","10001ca6":"# A quick check\ndf.gender.value_counts()","6082a5a4":"X=df.drop(['posttest','pretest','student_id'],axis='columns')\ny=df['posttest']","ee4edfde":"# Random seed for reproducibility\nnp.random.seed(100)\n\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","4009b0d8":"X_train.shape, y_train.shape, X_test.shape,y_test.shape","947055f9":"# Put models in a dictionary\nmodels = {'Lasso':Lasso(), \n          'Ridge': Ridge(),\n          'ElasticNet':ElasticNet() ,\n          'Random Forest': RandomForestRegressor(),\n          'Ada Boost': AdaBoostRegressor(),\n          'Gradient Boosting': GradientBoostingRegressor()\n         }\n\n# Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","8e9489e9":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","f6a4e5b0":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot.bar();","01eaf948":"# Different RandomForestRegressor hyperparameters\nrf_grid = {'n_estimators': np.arange(10, 1000, 50),\n           'max_depth': [6, 8, 10, 12],\n           'min_samples_split': np.arange(2, 20, 2),\n           'min_samples_leaf': np.arange(1, 20, 2),\n           'max_features':['sqrt']   }","35c5824b":"print(rf_grid)","8377546d":"%%time\n# Setup random seed\nnp.random.seed(10)\n\n# Setup random hyperparameter search for RandomForestRegressor\nrs_rf = RandomizedSearchCV(RandomForestRegressor(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model\nrs_rf.fit(X_train, y_train);","1639f8cd":"print(rs_rf.best_params_)\nprint('model_score:',rs_rf.score(X_test, y_test))\nprint('Model_MSE:',MSE(y_test,rs_rf.predict(X_test)))","1f67ee22":"# Different GradientBoostingRegressor hyperparameters\ngb_grid = {'n_estimators': np.arange(100,500,100),\n           'subsample': np.arange(.4,1,0.2),\n            'max_features':np.arange(.4,1,0.2)  }","23465279":"%%time\n# Setup random seed\nnp.random.seed(10)\n\n# Setup random hyperparameter search for GradientBoostingRegressor\nrs_gb = RandomizedSearchCV(GradientBoostingRegressor(),\n                           param_distributions=gb_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model\nrs_gb.fit(X_train, y_train);","7ee249c8":"print(rs_gb.best_params_)\nprint('model_score:',rs_gb.score(X_test, y_test))\nprint('Model MSE:',MSE(y_test,rs_gb.predict(X_test)))","7268030f":"# Model with tuned hyperparameres\nmd1=GradientBoostingRegressor(subsample=0.8,n_estimators=400, max_features=0.6)\nmd1.fit(X_train, y_train)\nprint('model_score:',md1.score(X_test, y_test))\nprint('Model MSE:',MSE(y_test,md1.predict(X_test)))","d2915680":"# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importance\": importances})\n          .sort_values(\"feature_importance\", ascending=False)\n          .reset_index(drop=True))\n    \n    sns.barplot(x=\"feature_importance\",\n                y=\"features\",\n                data=df[:n],\n                orient=\"h\")","ceef6ad5":"plot_features(X_train.columns, md1.feature_importances_)","aa4b55d3":"## Data preparation for models\n### convert string to numerical (categorical) variables","509d2e1f":" #### Remove features from the dataset\n \n 'student_id' is not needed for the modeling \n \n 'pretest' is also unknown variable for the modeling purpose so it is also dropped","777af6e5":"## Model Comparison\n\nSince we've saved our models scores to a dictionary, we can plot them by first converting them to a DataFrame.","ecfd1d8f":"### Outline of this code:\n<ol>\n<li> Data Load <\/li>\n<li> Data summary <\/li>\n<li> Plots <\/li>\n<!-- <li> Hypothesis testing <\/li> -->\n<li> Data preparation of ML <\/li>\n<li> Models  <\/li>   \n    <li> Hyper-parameter tuning <\/li> \n    \n    \n<\/ol>","f8f7de72":"### Plots to understand the distribution of data","81dc2577":"### Hyperparameter tuning for the best selected model\nBased on the result, Random Forest and Gradient Boosting is performing better than rest of the models. For these two models, hypreparameter tuning will be performed and then best model will be selected","c0d0e2d2":"### Finally to find the varaiable of importance of the Gradient boosting model","bf12920b":"###  Models to be evaluated\n1. Lasso regression\n2. Ridge regression\n3. Elastic net\n4. Random forest\n5. Ada boost\n6. Gradient boosting\n ","0c8e1957":"### Data Summary\nBased on the given data total # of records are 2133 and columns 11\n* posttest is a target variable (continuous)\n* n_student and pretest are continuous variables\n* rest 8 are caegorical variables\n* There are <b><i>no missing<\/i><\/b> values in the data","9d878e7f":"### This concludes that gradient boosting outperforms as compared to random forest.\nmodel_score: 0.9455\n\nModel MSE: 11.28","93be4e43":"### To identify the relationship between pretest and posttest scores"}}