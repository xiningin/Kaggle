{"cell_type":{"d9bfa8e4":"code","5f0df204":"code","8179440b":"code","d59c1d15":"code","d212fa64":"code","02410435":"code","3ddfbb0a":"code","6901fa67":"code","227881df":"code","95c7ef38":"code","611e3423":"code","b68267ec":"code","548d79b0":"code","46806946":"code","b9085559":"code","50f5ddbc":"code","1d5a1bc0":"code","ca26278f":"code","f2fc1f97":"code","de599a6a":"code","32532040":"code","4d67631a":"code","ea27a151":"code","5a587cc9":"markdown","3e85a7c5":"markdown","1118969c":"markdown","04716090":"markdown","aaac0893":"markdown"},"source":{"d9bfa8e4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams[\"figure.figsize\"] = (15, 7)\nplt.style.use(\"ggplot\")","5f0df204":"FILE_PATH = \"..\/input\/human-activity-recognition-with-smartphones\"\ndf_train = pd.read_csv(FILE_PATH + \"\/train.csv\")\ndf_test = pd.read_csv(FILE_PATH + \"\/test.csv\")","8179440b":"df_train.head()","d59c1d15":"df_train.info()","d212fa64":"df_train[\"Activity\"].value_counts(ascending=False).plot.barh()\n_ = plt.title(\"Distribution of activities\")","02410435":"from sklearn.decomposition import PCA\n\npca = PCA(2, random_state=42)\nX_datavis = pca.fit_transform(df_train.values[:, :-2])\n_ = sns.scatterplot(X_datavis[:, 0], X_datavis[:, 1], hue=df_train[\"Activity\"])","3ddfbb0a":"# How much of the variance did we capture? \npca.explained_variance_ratio_.sum()","6901fa67":"# What is the lowest number of components we need to explain 95% of the variance? \npca_2 = PCA(n_components=0.95)\npca_2.fit(df_train.values[:, :-2])\npca_2.n_components_","227881df":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(df_train.iloc[:, :-2], df_train.iloc[:, -1], train_size=0.8, \n                                                      random_state=42, stratify=df_train.iloc[:, -1])\n\nX_test, y_test = df_test.iloc[:, :-2], df_test.iloc[:, -1]","95c7ef38":"cls = XGBClassifier(objective=\"logistic\")\ncls.fit(X_train, y_train)","611e3423":"cls.score(X_valid, y_valid)","b68267ec":"cls.score(X_test, y_test)","548d79b0":"from sklearn.metrics import confusion_matrix\n\nlabels = df_train[\"Activity\"].unique()\ncm = confusion_matrix(cls.predict(X_valid), y_valid, normalize=\"true\", labels=labels)\nplt.imshow(cm)\nplt.grid(False)\nplt.ylabel(\"truth\"), plt.xlabel(\"predicted\"), plt.title(\"Confusion matrix for validation\") \n_ = plt.xticks(range(0, len(labels)),labels, rotation = 90), plt.yticks(range(0, len(labels)), labels)","46806946":"cm = confusion_matrix(cls.predict(X_test), y_test, normalize=\"true\")\nplt.imshow(cm)\nplt.grid(False)\nplt.ylabel(\"truth\"), plt.xlabel(\"predicted\"), plt.title(\"Confusion matrix for test\") \n_ = plt.xticks(range(0, len(labels)),labels, rotation = 90), plt.yticks(range(0, len(labels)), labels)","b9085559":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","50f5ddbc":"rfc = RandomForestClassifier(n_jobs=-1, random_state=42)\nrfc.fit(X_train, y_train)","1d5a1bc0":"# In decending order of highest to lowest, top 10\nmost_important = rfc.feature_importances_.argsort()[:-10:-1]\nimps = rfc.feature_importances_[most_important]\nvars_ = df_train.columns[most_important]\n_ = [print(f\"Feature: {var}, Importance: {imp}\") for var, imp in zip(vars_, imps)]","ca26278f":"most_important","f2fc1f97":"# Method 2\nfrom sklearn.linear_model import RidgeClassifier\nfrom tqdm.auto import tqdm\n\n# Ridge Classifier was chose due to its speed. \nbase = RidgeClassifier()\nbase.fit(X_train, y_train)\nbase_acc = base.score(X_valid, y_valid)\nN_features = 561\nfeature_imp = []\n\nfor feature in tqdm(range(N_features)): \n    X_valid_new = X_valid.copy().values\n    shuffled = np.random.permutation(X_valid_new[:, feature])\n    X_valid_new[:, feature] = shuffled\n    feature_imp.append(base_acc - base.score(X_valid_new, y_valid))","de599a6a":"feature_imp = np.array(feature_imp)\n# Descending Order of most importance\nmost_important = feature_imp.argsort()[:-10:-1]\nimps = feature_imp[most_important]\nvars_ = df_train.columns[most_important]\n_ = [print(f\"Feature: {var}, Importance: {imp}\") for var, imp in zip(vars_, imps)]","32532040":"# It's also provided by sklearn\nfrom sklearn.inspection import permutation_importance\n\nresult = permutation_importance(base, X_valid, y_valid, n_repeats=1, n_jobs=-1)","4d67631a":"most_important = result[\"importances_mean\"].argsort()[:-10:-1]\nimps = result[\"importances_mean\"][most_important]\nvars_ = df_train.columns[most_important]\n_ = [print(f\"Feature: {var}, Importance: {imp}\") for var, imp in zip(vars_, imps)]","ea27a151":"imp1, imp2 = most_important[:2]\n\nsns.scatterplot(X_train.iloc[:, imp1], X_train.iloc[:, imp2], hue=y_train)\n_ = plt.title(\"Scatter plot with the two most important features\")","5a587cc9":"### Visualization with only the top two most important features","3e85a7c5":"## Feature Importance\n\nHere I'll use two different methods to determine feature importance.\n\nThe first is to use the feature importances vector from a decision tree based classifier, which is based on impurity.\n\nThe second is described as follows: First, we create, fit and score a baseline model. Then, for every feature, we permute it, and get the difference between the baseline accuracy and the score of the model on the permuted dataset. The intuition here is that if that feature really is important, then shuffling it should cause the greatest decrease in accuracy. (Of course, you can choose any other metric other than accuracy).  ","1118969c":"### Simple XGBoost classification","04716090":"### Quick EDA","aaac0893":"From the graph above, we can see why they are considered important: we can clearly see that we can easily divide 'laying' and the other activities from there two metrics alone. "}}