{"cell_type":{"21fb378c":"code","fca9d70b":"code","2d715f31":"code","fd69fe28":"code","514e3db5":"code","a1b84a5a":"code","d41ef38f":"code","d5e38c9e":"code","bc79bca1":"code","1f76a84d":"code","0be42db3":"code","fa797468":"code","b79a3195":"code","b4d8a9de":"code","ce5e1533":"code","f43cd540":"code","2943a8f3":"code","321854c3":"code","a6436270":"code","5b0a8a0b":"code","3910d380":"code","9a3489a3":"code","844b1d6f":"code","b6086e11":"code","3c094e95":"code","009f5340":"code","e269fe0e":"code","b12895bc":"code","a3dbfeb9":"code","907aab24":"code","a9111c5b":"code","c8aa5226":"code","55b67009":"code","8930be71":"code","cb8e4e1d":"code","c7052756":"code","de53800a":"code","e4a67733":"code","66be036a":"code","03738689":"code","cfaf6536":"code","4f89ba13":"code","da045057":"code","8119e847":"code","1274aebd":"code","d0fb3ed5":"code","bb674856":"code","288f8d31":"code","931813c4":"code","cf4393a8":"code","6fd04e38":"code","772be394":"code","868aeed4":"code","d98a4fa3":"code","cb79f1d8":"code","3025493f":"code","2de214eb":"code","39fa3c66":"code","1e15d2d0":"code","87d7c2d0":"code","c81ebaf1":"code","2229a85c":"code","c442a09c":"code","1481aca9":"code","2b191281":"code","0ea79006":"code","215b6c80":"code","80f609ff":"code","4d179f78":"code","6b721aab":"code","da932ab9":"code","ef7a1ada":"code","3be110f7":"code","11658a43":"code","73c1171f":"code","9f4605a1":"code","b6eb280c":"code","4b8cdf33":"code","de6f46a3":"code","fb3066f0":"code","30ff92cb":"code","1dae90cb":"code","2133e87f":"code","072c1ac9":"code","84dda79c":"code","0d9a3231":"code","7701b1b4":"code","61c807ea":"code","454798fd":"code","f1fc9047":"code","3f770bbe":"code","174c0118":"code","ab1a0c44":"code","44618d55":"code","93831b66":"code","4cab54fb":"code","554c93a3":"code","8d522d16":"code","7acfe23d":"markdown","d2a05bc6":"markdown","05a805d9":"markdown","61bd40a1":"markdown","9977f5f2":"markdown","c15d323d":"markdown","903e011b":"markdown","5a6a8dbe":"markdown","04869c82":"markdown","a4342c8e":"markdown"},"source":{"21fb378c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fca9d70b":"# import library\nimport ggplot\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px ","2d715f31":"# Import data from CSV files\n\ndf_train=pd.read_csv('\/kaggle\/input\/titanic-machine-learning-from-disaster\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/titanic-machine-learning-from-disaster\/test.csv')\ndf_train.head()","fd69fe28":"# Number of survivors of the titanic accident\ntotal_Passengers= df_train['Survived'].value_counts()[1] + df_train['Survived'].value_counts()[0]\nprint(\"Total Passengers in Titanic : \",total_Passengers)\nSurvived_passengers=df_train['Survived'].value_counts()[1]\nprint(\"Survived Passengers in Titanic : \",Survived_passengers)","514e3db5":"df_test.head()  # to have a look to data","a1b84a5a":"df_train.info()","d41ef38f":"df_train.describe()","d5e38c9e":"# let's find out the missing values and fill them with appropriate values\n\ndf_train.isnull().sum()","bc79bca1":"df_train.isnull()","1f76a84d":"# Draw heat map to check numm values in train data\nsns.heatmap(df_train.isnull())","0be42db3":"# Draw heat map to check numm values in test data\nsns.heatmap(df_test.isnull())","fa797468":"# check No. of males and females travelled\n\nprint(\"No of males tavelled in Titanic : \",df_train['Sex'].value_counts()['male'])\nprint(\"No of Females travelled in Titanic : \",df_train['Sex'].value_counts()['female'])","b79a3195":"# check No. of males and females survived\n\ndf_train['Survived'].value_counts()[df_train['Sex']=='male']","b4d8a9de":"df_train['Survived'].value_counts()[df_train['Sex']=='female']","ce5e1533":"# we can also visualise the survival rate in gender category using count plot\n\nplt.subplot(1,2,1)\nsns.countplot(x='Sex',data=df_train)","f43cd540":"# plot to visualize Survived stats\nplt.subplot(1,2,1)\nsns.countplot(data=df_train,x='Survived')","2943a8f3":"# check survived count as per sex\n\nsns.countplot(x='Survived',data=df_train,palette='rainbow',hue='Sex')","321854c3":"# check survived count as per Pclass\n\nsns.countplot(x='Survived',data=df_train,palette='rainbow',hue='Pclass')\n# people who were living in better passenger classes survived people living in lower class were remained dead","a6436270":"# check survived count as per Embarked\n\nsns.countplot(x='Survived',data=df_train,palette='rainbow',hue='Embarked')","5b0a8a0b":"# check survived count as per Parch\n\nsns.countplot(x='Survived',data=df_train,palette='rainbow',hue='Parch')","3910d380":"df_train['Fare']\/\/10","9a3489a3":"# check survived count as per Fare\nsns.countplot(data=df_train,x='Survived',hue=df_train['Fare'],palette='rainbow')","844b1d6f":"# from above missing observations we learnt that around 20% of age values were missing.from the distribution large groups of passengers are of 15 to 35 years\nsns.set_style('whitegrid')\nsns.distplot(df_train['Age'],kde=False,bins=20,color='g')","b6086e11":"# Survived people vs Siblings\/Spouses aboard the Titanic\ndf_train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean().sort_values(ascending=False,by='SibSp')","3c094e95":"# Visualize a count plot between Survived & Sibsp\nsns.countplot(data=df_train,x='Survived',hue='SibSp',palette='rainbow')","009f5340":"# Draw scatter plot between Survived & SibSp\nimport matplotlib.pyplot as plt\nx=df_train['SibSp']\ny=df_train['Survived']\nfig,Axes=plt.subplots()\nplt.suptitle('SibSp vs Survived')\nplt.subplot(1,3,1)\nplt.scatter(x,y,marker='*',color='r',linewidth=5,s=25,edgecolor='g')\nAxes.set_title('using scatterplot')\n","e269fe0e":"# Draw different plot between Survived & SibSp\nplt.subplot(1,3,2)\nplt.xlabel('SibSp')\nplt.ylabel('Survived')\nAxes.set_title('using plot')\nplt.plot(x,y,'g*',linestyle='dashdot',linewidth=2,markersize=10)\nplt.subplot(1,3,3)\nplt.bar(x,y,align='center',color='black')\nAxes.set_title('using bar')\nplt.xlabel('SibSp')\nplt.ylabel('Survived')","b12895bc":"# Survived based on their gender\n\ndf_train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean().sort_values(by='Sex',ascending=False)","a3dbfeb9":"# cleansing of the data\nsns.boxplot(x='SibSp',y='Age',data=df_train)","907aab24":"# From the above observation we can map an estimated age to the null values in comparision with SibSp\n\ndef fill_age(cols):\n    SibSp = cols[0]\n    Age =cols[1]\n    if pd.isnull(Age):\n        if SibSp==0:\n            return 29\n        if SibSp==1:\n            return 30\n        if SibSp==2:\n            return 25\n        if SibSp==3:\n            return 21\n        if SibSp==4:\n            return 17\n        if SibSp==5:\n            return 11\n        else:\n            return df_train('ffill')\n    else:\n            return Age","a9111c5b":"df_train['Age']=df_train[[\"Age\",\"SibSp\"]].apply(fill_age,axis=1)","c8aa5226":"# Draw heat map to display null values\nsns.heatmap(df_train.isnull())","55b67009":"df_train.isnull().sum()","8930be71":"# From the above heatmap we can see all the missing values are resolved but if we see the missing values using isnull().sum() there are still some missing values in Embarked columns\n# so we will fill missing Embarked values with backward or forward fill\n\ndf_train['Embarked'].fillna('bfill',inplace=True)","cb8e4e1d":"df_train.isnull().sum()","c7052756":"# so we now resolved all the missing values in the dataset, we can use the data provided efficiently only if the data is categorical format\n\ndf_train.info()","de53800a":"# we can represent the given values except Name,Sex,Embarked ,Ticket\n# so we will convert object datatype into categorical values if possible or we will drop the unnecessary columns\npd.get_dummies(df_train)","e4a67733":"# so we will create a copy of train_df and proceed accordingly\ntrain_copy=df_train.copy()\ntrain_copy\n","66be036a":"# now we will drop name and ticket columns because they can't be converted into valid categorical columns\ntrain_copy.drop(['Name','Ticket'],inplace=True,axis=1)","03738689":"train_copy","cfaf6536":"# so we will convert the Embarked and Sex to categorical values using get_dummies()\nSex_category=pd.get_dummies(train_copy['Sex'],drop_first=True)\nEmbarked_category=pd.get_dummies(train_copy['Embarked'],drop_first=True)","4f89ba13":"# drop Sex and Embarked\ntrain_copy.drop(['Sex','Embarked'],axis=1,inplace=True)","da045057":"# now we will add Sex_category and Embarked_category into the train_copy DataFrame\ntrain=pd.concat([train_copy,Sex_category,Embarked_category],axis=1)","8119e847":"train.head()","1274aebd":"train.drop(['bfill'],axis=1,inplace=True)","d0fb3ed5":"train.info()","bb674856":"# Visualise using some seaborn plotting techniques\nsns.rugplot(train['Age'].isnull())\n\n# the below figure depicts that there are no missing values present in Age column","288f8d31":"sns.jointplot(data=train,x=train['Survived'],y=train['Pclass'],kind='kde')","931813c4":"# create pair plot for visibility across data\nsns.pairplot(train)","cf4393a8":"sns.distplot(train[['Survived','Pclass']],kde=True,bins=10)","6fd04e38":"sns.jointplot(x=train['male'],y=train['Pclass'],kind='kde')","772be394":"# correlation of the train data\nsns.heatmap(train.corr())","868aeed4":"# visualize through categorical plottings\nsns.boxplot(x='male',y='Pclass',data=train,color='r')\nsns.boxenplot(x='male',y='Pclass',data=train,color='g')","d98a4fa3":"# Plot swarm plot between male & Pclass\nsns.swarmplot(x='male',y='Pclass',data=train,color='k')\n","cb79f1d8":"# Plot violine plot between male & Pclass\nsns.violinplot(x='male',y='Pclass',data=train,color='g')","3025493f":"# Plot strip plot between male & Pclass\nsns.stripplot(x='male',y='Pclass',data=train,color='r')","2de214eb":"# Using all the categorical plotting in a single figure\n\nsns.stripplot(x='Survived',y='SibSp',data=train,color='b')","39fa3c66":"# Plot swarm plot between Survived & SibSp\nsns.swarmplot(x='Survived',y='SibSp',data=train,color='k')","1e15d2d0":"# Plot violine plot between Survived & SibSp\nsns.violinplot(x='Survived',y='SibSp',data=train,palette='rainbow')","87d7c2d0":"# Plot boxen plot between Survived & SibSp\nsns.boxenplot(data=train,x='Survived',y='SibSp',color='m')","c81ebaf1":"# Plot bar plot between Survived & SibSp\nsns.barplot(data=train,y='SibSp',x='Survived',color='y')","2229a85c":"# Plot box plot between Survived & SibSp\nsns.boxplot(x='Survived',y='SibSp',data=train,palette='dark')","c442a09c":"# Plot count plot between SibSp\nsns.countplot(data=train,y='SibSp',color='red')","1481aca9":"# Plot factor plot between Pclass & SibSp\nsns.factorplot(x='Pclass',y='SibSp',data=train)","2b191281":"# We have explored the data to a great extent.Now we can use machine learning techiniques to predict who survived or died on the Titanc.We can use different algorithms to predict this.Also we will try to quantify which algorithm gives us the highest accuracy.\n\n# #importing all the required ML packages\n\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier  #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier   # KNN\nfrom sklearn.tree import DecisionTreeClassifier   # Decision tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","0ea79006":"f,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked',data=df_train,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=df_train,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=df_train,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=df_train,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","215b6c80":"# Factor Plot based on Class,Survival,Sex and Embarked\nsns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=df_train)\nplt.show()","80f609ff":"data=pd.read_csv('\/kaggle\/input\/titanic-machine-learning-from-disaster\/train.csv')\ndata.head()","4d179f78":"data.isnull().sum()","6b721aab":"# Filing Age Missing Values\n# We can fill the missing value with the mean of the Age column.This is because we can fill the age of a child (5 years) with the mean age ( Around 30 years).This would increase the error in the data.But we can see that in the Name column we have titles like Mr,Mrs menioned.We have to get the mean age of each group in the titles and then replace the missing age with the mean values of the titles.\n\ndata['Title']=0\nfor i in data:\n    data['Title']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations","da932ab9":"# Here we are using the Regex: [A-Za-z]+).. So what it does is, it looks for strings which lie between A-Z or a-z and followed by a .(dot). So we successfully extract the Initials from the Name.\npd.crosstab(data.Title,data.Sex).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex","ef7a1ada":"# In the dataset initials like Mlle or Mme stand for Miss.We will replace them with Miss.\ndata['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","3be110f7":"pd.crosstab(data.Title,data.Sex).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex","11658a43":"# We have converted the Titles in four categories like Master,Miss,Mr,Mrs and Other.\ndata.groupby('Title')['Age'].mean() #lets check the average age by Initials","73c1171f":"# Now we have got the mean Age of the different groups in the Title.Now we can use this values to imput the missing values in the Age Column in our Titanic Dataset.\n\ndata.loc[(data.Age.isnull())&(data.Title=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Title=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Title=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Title=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Title=='Other'),'Age']=46","9f4605a1":"data.Age.isnull().any() #So no null values left finally ","b6eb280c":"# Filling Embarked NaN\n# We can see that the Embarked column has two missing values.In the dataset most people embarked Titanic from the port S.So we can replace the two missing value with S\ndata['Embarked'].fillna('S',inplace=True)","4b8cdf33":"data.Embarked.isnull().any()# Finally No NaN values","de6f46a3":"# Fare Bin\n# Here we will use qcut this will split the data into bins based on the number of bins requested by us.\ndata['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","fb3066f0":"# We can clearly see that as the fare increased the the survival percentage increases.\ndata['Fare_Group']=0\ndata.loc[data['Fare']<=7.91,'Fare_Group']=0\ndata.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_Group']=1\ndata.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_Group']=2\ndata.loc[(data['Fare']>31)&(data['Fare']<=513),'Fare_Group']=3","30ff92cb":"data","1dae90cb":"# Age Binning\n# In case of a continous variable like Age we can get better insight by making bins of the Age data.\n\ndata['Age_band']=0\ndata.loc[data['Age']<=16,'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\ndata.loc[data['Age']>64,'Age_band']=4\ndata.head(10)","2133e87f":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')#checking the number of passenegers in each band","072c1ac9":"# Sibling and Spouse\nf,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked',data=data,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","84dda79c":"data['Sex'] = data['Sex'].astype(str)\ndata['Embarked'] = data['Embarked'].astype(str)\ndata['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndata['Title'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)\ndata.head(2)","0d9a3231":"df=data.copy()\ndf.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)\nsns.heatmap(df.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","7701b1b4":"df.drop(['SibSp','Parch'],axis=1,inplace=True)\ndf.head()","61c807ea":"df.isnull().sum()","454798fd":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nimport cv2\nimport glob\nimport os\nimport matplotlib.pyplot as plt\nimport string\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom mlxtend.plotting import plot_decision_regions\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.metrics import plot_confusion_matrix","f1fc9047":"X = df.drop(labels='Survived',axis=1)\ny = df['Survived']","3f770bbe":"# Test Train Split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state= 1234,stratify=y)\n\nprint('Training Set:',len(X_train))\nprint('Test Set:',len(X_test))\nprint('Training labels:',len(y_train))\nprint('Test labels:',len(y_test))","174c0118":"model=svm.SVC(kernel='linear',C=0.2,gamma=0.1)\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint('Accuracy for linear SVM is',metrics.accuracy_score(prediction1,y_test))\n","ab1a0c44":"model=svm.SVC(kernel='rbf',C=0.8,gamma=0.4)\nmodel.fit(X_train,y_train)\nprediction2=model.predict(X_test)\nprint('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction2,y_test))","44618d55":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf')]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","93831b66":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()\npass","4cab54fb":"new_models_dataframe2['CV Mean'].plot.bar(width=0.5)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(9,6)\nplt.show()","554c93a3":"# Decision tree\nfrom sklearn import tree\n# Create Decision Tree with max_depth = 3\ndecision_tree = tree.DecisionTreeClassifier(max_depth = 3)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprint(y_pred)","8d522d16":"acc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","7acfe23d":"<a id=\"section-two\"><\/a>\n# Section  2 - Exploratory Data Analysis","d2a05bc6":"# Table of Contents\n\n* [Section  1 - Importing Library & Data](#section-one)  \n* [Section 2 - Exploratory Data Analysis](#section-two)\n* [Section 3 - Visualizing Data via Graphs](#section-three)\n* [Section 4 - Linear Support Vector Machine(Linear-SVM)](#section-four)\n* [Section 5 - Radial Support Vector Machine(Radial-SVM)](#section-five)\n* [Section 6 - K Fold Cross Validation](#section-six)\n* [Section 7 - Box Plot of Accuracy](#section-seven)\n* [Section 8 - Bar Plot of Accuracy](#section-eight)\n* [Section 9 - Decision tree](#section-nine)\n","05a805d9":"<a id=\"section-seven\"><\/a>\n# Section  7 - Box Plot of Accuracy\n\n","61bd40a1":"<a id=\"section-one\"><\/a>\n# Section  1 - Importing Library & Data","9977f5f2":"<a id=\"section-eight\"><\/a>\n# Section  8 - Bar Plot of Accuracy\n","c15d323d":"<a id=\"section-four\"><\/a>\n# Section  4 - Linear Support Vector Machine(Linear-SVM)","903e011b":"<a id=\"section-six\"><\/a>\n# Section  6 - K Fold Cross Validation\n\n","5a6a8dbe":"<a id=\"section-five\"><\/a>\n# Section  5 - Radial Support Vector Machine(Radial-SVM)","04869c82":"<a id=\"section-nine\"><\/a>\n# Section  9 - Decision tree","a4342c8e":"<a id=\"section-three\"><\/a>\n# Section  3 - Visualizing Data via Graphs"}}