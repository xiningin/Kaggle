{"cell_type":{"bc6c743a":"code","652e7acd":"code","5e617d93":"code","b0e043de":"code","ccd58850":"code","f848333c":"code","788fc20d":"code","a0a7018d":"code","59870867":"code","0d455108":"code","92a71150":"code","c6434e2a":"code","cae7faed":"code","f19391a7":"code","676b97f4":"code","dfd06533":"code","68f98737":"code","c6b7a154":"code","15a69852":"code","00e1dfd5":"code","c4990834":"code","840d74f5":"code","f44913d8":"code","2a035cc3":"code","4ecca3e3":"code","1f09a290":"code","9057714d":"code","8ed02018":"code","ed4170e6":"code","c6c5308e":"code","dfe61edd":"code","a2f710cc":"code","fa91717f":"code","0a7f6091":"code","2d2d0b52":"code","fad02842":"code","afcb26a7":"code","07fbdc55":"code","d08d6994":"code","98aa4fbd":"code","5e8c26cb":"code","53e2a50b":"code","cadc5bdc":"code","d0666e72":"code","30c5ae0d":"code","a82f5ac4":"code","49f57f03":"code","e4443bc9":"code","d222f856":"code","2a29cf34":"code","902c7a32":"code","944a7901":"code","5b3a2bef":"code","8b14ab84":"code","2149ad55":"code","c9e4d3dc":"code","a9bcd819":"code","dbd7e8eb":"code","8f9d00ec":"code","0bcf93cb":"code","420cf940":"code","c126001d":"code","390890c2":"code","842990a9":"code","74196102":"code","8231c5e5":"code","749e1b80":"code","a01dcebd":"code","aaa67789":"code","23bf9475":"code","566e4327":"code","4ca613fb":"code","dd7d9f75":"code","2565ccd8":"code","f74958ba":"code","5079e6a5":"code","da24bb60":"code","afc23396":"code","b26b6781":"code","fd27a4d8":"code","41a9c8f2":"code","35988016":"code","9b091e65":"code","068e310c":"code","e8bd4806":"code","786aef71":"code","22add021":"code","a57685f6":"code","177c62e2":"code","c2f358db":"code","448b5909":"code","f9cf9887":"code","f4310bdc":"code","6c224d4c":"code","9f30e6ae":"code","dabf05ae":"markdown","aad43281":"markdown","0304f76c":"markdown","a5ff7fc6":"markdown","32ccaeb0":"markdown","1029dfe3":"markdown","f098a7b7":"markdown","67136fe8":"markdown","8b2a9732":"markdown","a718bc03":"markdown","3465f81e":"markdown","a467a2a2":"markdown","a167f55b":"markdown","4a29bb78":"markdown","f157b415":"markdown","f8f1459f":"markdown","d8f12403":"markdown","61e750dd":"markdown","e7b6d19a":"markdown","09c9559e":"markdown","7dd2071c":"markdown","7cf95bed":"markdown","6b10185b":"markdown","dab89a81":"markdown","a1b8106e":"markdown","122d8eba":"markdown","37f11f8f":"markdown","84f30a86":"markdown","89021d16":"markdown","4e512843":"markdown","020d9a23":"markdown","1a36ce84":"markdown"},"source":{"bc6c743a":"# Warning Libraries :\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Scientific and Data Manipulation Libraries :\nimport pandas as pd\nimport numpy as np\nimport math\nimport gc\nimport os\n\n# ML Libraries :\nfrom sklearn.preprocessing            import LabelEncoder, OneHotEncoder \nfrom sklearn.preprocessing            import StandardScaler, MinMaxScaler, Normalizer, RobustScaler, MaxAbsScaler\nfrom sklearn.model_selection          import KFold, StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.tree                     import DecisionTreeClassifier\nfrom sklearn.ensemble                 import VotingClassifier, RandomForestClassifier\nfrom sklearn.metrics                  import f1_score, confusion_matrix, classification_report\n\n                    \n# Data Visualization Libraries :\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px","652e7acd":"rs=1331 ##random_state","5e617d93":"train= pd.read_csv('..\/input\/hranalysis\/train.csv')\ntest= pd.read_csv('..\/input\/hranalysis\/test.csv')","b0e043de":"test.head()","ccd58850":"train.head()","f848333c":"print(train.columns)\nprint(\"*\"*100)\nprint(test.columns)","788fc20d":"print(\"Train data shape\",train.shape)\nprint(\"Test data shape\",test.shape)","a0a7018d":"train.info()","59870867":"train.describe(include='all')","0d455108":"test.info()","92a71150":"test.describe(include='all')","c6434e2a":"train.isna().sum()","cae7faed":"#Using missingno to visualize null values in train data\nimport missingno as msno\nmsno.bar(train, color = '#6389df', figsize = (10,8))  \n","f19391a7":"test.isna().sum()","676b97f4":"#Using missingno to visualize null values in test data\nmsno.bar(test, color = '#6389df', figsize = (10,8))  \n","dfd06533":"corr=train.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr,square=True,annot=True)","68f98737":"train.dtypes","c6b7a154":"# Let\u2019s plot the distribution of each feature\ndef plot_distribution(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(dataset.shape[1]) \/ cols)\n    for i, column in enumerate(dataset.columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        if dataset.dtypes[column] == np.object:\n            g = sns.countplot(y=column, data=dataset)\n            substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n            g.set(yticklabels=substrings)\n            plt.xticks(rotation=25)\n        else:\n            g = sns.distplot(dataset[column])\n            plt.xticks(rotation=25)\n    \nplot_distribution(train,cols=2, width=30, height=60, hspace=0.45, wspace=0.5)","15a69852":"\ndef plot_bivariate_bar(dataset, hue, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    dataset = dataset.select_dtypes(include=[np.object])\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(dataset.shape[1]) \/ cols)\n    for i, column in enumerate(dataset.columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        if dataset.dtypes[column] == np.object:\n            g = sns.countplot(y=column, hue=hue, data=dataset)\n            substrings = [s.get_text()[:10] for s in g.get_yticklabels()]\n            g.set(yticklabels=substrings)\n            \n            \n            \nplot_bivariate_bar(train, hue=train.is_promoted, cols=1, width=10, height=35, hspace=0.4, wspace=0.5)","00e1dfd5":"#unique value in education feature\ntrain.education.value_counts()","c4990834":"#plotting a pie chart\nsize = [36669,14925,805]\nlabel=[\"Bachelor's\",\"Master's & above\",'Below Secondary']\ncolor=['#6389df','#1f2b6c','#a3ccf4']\nexplode = [0.1, 0.2 , 0.3]\nplt.figure(figsize=(8,8))\nplt.pie(size,labels=label,colors=color,explode=explode,shadow=True,autopct=\"%.1f%%\")\nplt.title(\"Pie Chart of the Employees Degrees\", fontsize = 20)\nplt.axis('off')\nplt.legend(title='Education Degrees')\nplt.show()","840d74f5":"#unique value in gender feature\ntrain.gender.value_counts()","f44913d8":"#plotting a pie chart\nsize = [38496,16312]\nlabel=[\"Male\",\"Female\"]\ncolor=['#6389df','#1f2b6c']\nexplode = [0.1, 0.2 ]\nplt.figure(figsize=(8,8))\nplt.pie(size,labels=label,colors=color,explode=explode,shadow=True,autopct=\"%.1f%%\")\nplt.title(\"Pie Chart of the GenderGap\", fontsize = 20)\nplt.axis('off')\nplt.legend(title='Gender')\nplt.show()","2a035cc3":"plt.subplots(figsize=(15,5))\nsns.countplot(x = 'education', data = train, hue = 'gender', palette = 'Paired')\nplt.title('Showing Degree & Gender ratio', fontsize = 20)\nplt.show()","4ecca3e3":"train['recruitment_channel'].value_counts()","1f09a290":"size=[30446,23220,1142]\nlabel=[\"Other\",\"Sourcing\",'Referred']\ncolor=['#6389df','#1f2b6c','#a3ccf4']\nexplode=[.05,.05,.05]\nplt.figure(figsize=(8,8))\nplt.pie(size,labels=label,colors=color, startangle=90,shadow=True,autopct=\"%.2f%%\",pctdistance=.85)\n\ncenter_circle=plt.Circle((0,0),.7,fc='white')\nfig=plt.gcf()\nfig.gca().add_artist(center_circle)\n\nplt.title('A Pie Chart Representing Recruitment_Channel', fontsize = 30)\nplt.axis('off')\nplt.legend()\nplt.show()\n","9057714d":"#### Check most popular department\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopword = set(STOPWORDS)\n\nwordcloud = WordCloud(stopwords = stopword).generate(str(train['department']))\n\nplt.rcParams['figure.figsize'] = (15, 8)\nprint(wordcloud)\nplt.imshow(wordcloud)\nplt.title('Most Popular Departments', fontsize = 30)\nplt.axis('off')\nplt.show()","8ed02018":"plt.figure(figsize=(15,5))\nsns.distplot(train['age'],color='#6389df')\nplt.title('Distribution of Age of Employees', fontsize = 30)\nplt.grid(axis='both')\nplt.show()","ed4170e6":"#pie chart for the KPIs_met\ntrain['KPIs_met >80%'].value_counts()","c6c5308e":"size = [35517, 19291]\nlabels = \"Not Met KPI > 80%\", \"Met KPI > 80%\"\ncolor=['#6389df','#a3ccf4']\nexplode = [0, 0.1]\nplt.figure(figsize=(8,8))\nplt.pie(size, labels = labels, colors = color, explode = explode, shadow = True, autopct = \"%.2f%%\")\nplt.title('A Pie Chart Representing Gap in Employees in terms of KPI', fontsize = 30)\nplt.axis('off')\nplt.legend()\nplt.show()","dfe61edd":"train['awards_won?'].value_counts()","a2f710cc":"size = [53538,1270]\nlabels = \"Awards Won\", \"NO Awards Won\"\ncolor=['#6389df','#a3ccf4']\nexplode = [0, 0.1]\n\nmy_circle = plt.Circle((0, 0), 0.7, color = 'white')\n\nplt.figure(figsize=(8,8))\nplt.pie(size, labels = labels, colors = color, explode = explode, shadow = True, autopct = \"%.2f%%\")\nplt.title('A Pie Chart Representing Gap in Employees in terms of KPI', fontsize = 30)\np = plt.gcf()\np.gca().add_artist(my_circle)\nplt.legend()\nplt.show()","fa91717f":"size = [50140, 4668]\nlabels = \"NOT Promoted \", \"Promoted \"\ncolor=['#a3ccf4','#6389df']\nexplode = [0, 0.1]\n\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.rcParams['figure.figsize'] = (8, 8)\nplt.pie(size, labels = labels, colors = color, explode = explode, shadow =False, autopct = \"%.2f%%\",startangle=180)\nplt.title('Showing a Percentage of employees who Promoted ' , fontsize = 20)\nplt.axis('off')\nplt.legend()\nplt.show()","0a7f6091":"## BIVARIATE FEATURE ANALYSIS","2d2d0b52":"train.columns","fad02842":"a=['department', 'region', 'education', 'gender', 'no_of_trainings', 'age', 'previous_year_rating',\n       'length_of_service', 'KPIs_met >80%', 'awards_won?']\nfor i in a:\n  data = pd.crosstab(train[i],train['is_promoted'])\n  data.div(data.sum(1).astype('float'), axis = 0).plot(kind = 'bar', stacked = True, figsize = (15, 5), color = ['#a3ccf4','#6389df'])\n\nplt.legend()\nplt.show()","afcb26a7":"import plotly.express as px\nfig = px.parallel_categories(train[['department','education','gender','previous_year_rating','KPIs_met >80%',\n                                    'recruitment_channel',\n                                   'is_promoted']], \n                             color=\"is_promoted\", \n                             color_continuous_scale=px.colors.sequential.Aggrnyl  )\nfig.show()","07fbdc55":"#  Removes Data Duplicates while Retaining the First one - Similar to SQL DISTINCT :\ndef remove_duplicate(data):\n    \n    print(\"BEFORE REMOVING DUPLICATES - No. of Rows = \",data.shape[0])\n    data.drop_duplicates(keep=\"first\", inplace=True) \n    print(\"AFTER REMOVING DUPLICATES  - No. of Rows = \",data.shape[0])\n    return \"Checked Duplicates\"\n# Remove Duplicates from \"train\" data :\nremove_duplicate(train)\n# No Duplicates at all !!!","d08d6994":"##missing value function which return an dataframe with total null values and percentage\ndef missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(train)","98aa4fbd":"missing_data(test)","5e8c26cb":"train.previous_year_rating=train.previous_year_rating.fillna(0)\ntest.previous_year_rating=test.previous_year_rating.fillna(0)","53e2a50b":"train['Fresher']=train['previous_year_rating'].apply(lambda x: 'Fresher' if x==0 else 'Experienced')\ntest['Fresher']=test['previous_year_rating'].apply(lambda x: 'Fresher' if x==0 else 'Experienced')","cadc5bdc":"train['education']=train['education'].ffill(axis=0)\ntrain['education']=train['education'].bfill(axis=0)\n\ntest['education']=test['education'].ffill(axis=0)\ntest['education']=test['education'].bfill(axis=0)","d0666e72":"display(missing_data(train))\ndisplay(missing_data(test))\n","30c5ae0d":"#BINNING THE AGE FEATURE IN 20-29 , 29-39 , 39-49 \nsns.distplot(train['age'])\n\ntrain['age'] = pd.cut( x=train['age'], bins=[20, 29, 39, 49], labels=['20', '30', '40'] )\ntest['age']  = pd.cut( x=test['age'], bins=[20, 29, 39, 49],  labels=['20', '30', '40'] )","a82f5ac4":"train.age.value_counts(dropna=False)","49f57f03":"train.drop(['employee_id'],axis=1,inplace=True)\ntest_d=test","e4443bc9":"test_d.drop(['employee_id'],axis=1,inplace=True)","d222f856":"def data_encoding( encoding_strategy , encoding_data , encoding_columns ):\n    \n    if encoding_strategy == \"LabelEncoding\":\n        print(\"IF LabelEncoding\")\n        Encoder = LabelEncoder()\n        for column in encoding_columns :\n            print(\"column\",column )\n            encoding_data[ column ] = Encoder.fit_transform(tuple(encoding_data[ column ]))\n        \n    elif encoding_strategy == \"OneHotEncoding\":\n        print(\"ELIF OneHotEncoding\")\n        encoding_data = pd.get_dummies(encoding_data)\n        \n    dtypes_list =['float64','float32','int64','int32']\n    encoding_data.astype( dtypes_list[0] ).dtypes\n    \n    return encoding_data","2a29cf34":"encoding_columns  = [ \"region\", \"age\",\"department\", \"education\", \"gender\", \"recruitment_channel\" ]\nencoding_strategy = [ \"LabelEncoding\", \"OneHotEncoding\"]\n\ntrain_encode = data_encoding( encoding_strategy[1] , train , encoding_columns )\ntest_encode =  data_encoding( encoding_strategy[1] , test  , encoding_columns )","902c7a32":"test_encode.head()","944a7901":"train_encode.head()","5b3a2bef":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler, MaxAbsScaler\ndef data_scaling( scaling_strategy , scaling_data , scaling_columns ):\n    \n    if    scaling_strategy ==\"RobustScaler\" :\n        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"StandardScaler\" :\n        scaling_data[scaling_columns] = StandardScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"MinMaxScaler\" :\n        scaling_data[scaling_columns] = MinMaxScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"MaxAbsScaler\" :\n        scaling_data[scaling_columns] = MaxAbsScaler().fit_transform(scaling_data[scaling_columns])\n        \n    else :  # If any other scaling send by mistake still perform Robust Scalar\n        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n    \n    return scaling_data","8b14ab84":"scaling_st=[\"RobustScaler\" ,\"StandardScaler\",\"MinMaxScaler\",\"MaxAbsScaler\"]\n\ntrain_scale=data_scaling(scaling_st[0],train_encode,train_encode.columns)\ntest_scale=data_scaling(scaling_st[0],test_encode,test_encode.columns)","2149ad55":"X=train_scale.drop(['is_promoted'],axis=1)\nY=train.is_promoted","c9e4d3dc":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y)","a9bcd819":"Y_train.value_counts(normalize=True)*100","dbd7e8eb":"from imblearn.combine import SMOTETomek\ndef oversample(X,Y):\n    over_sample = SMOTETomek(random_state=rs)\n    X_over,Y_over = over_sample.fit_resample(X,Y)\n    return X_over,Y_over","8f9d00ec":"X_train_os,Y_train_os=oversample(X_train,Y_train)","0bcf93cb":"clf = RandomForestClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train, Y_train)\n\n### FEATURE SCORES \nfeature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nprint(feature_scores)\n\n### PLOT TO VISUALIZE\nsns.barplot(x=feature_scores, y=feature_scores.index)\n# Add labels to the graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\n# Add title to the graph\nplt.title(\"Visualizing Important Features\")\n# Visualize the graph\nplt.show()\n","420cf940":"from sklearn.ensemble          import RandomForestClassifier\nfrom sklearn.tree              import DecisionTreeClassifier\n\nfrom sklearn.metrics           import accuracy_score\nfrom sklearn.metrics           import classification_report\nfrom sklearn.metrics           import confusion_matrix\n\nfrom sklearn.model_selection   import RandomizedSearchCV\nfrom sklearn.model_selection   import KFold,cross_val_score\n","c126001d":"## PASSING THE TRAIN DATA IN THE CROSS VALIDATION\nkf=KFold(n_splits=5,random_state=rs,shuffle=True)\ncnt = 1\n# split()  method generate indices to split data into training and test set.\nfor train_index, test_index in kf.split(X_train_os, Y_train_os):\n    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n    cnt+=1","390890c2":"score = cross_val_score(RandomForestClassifier(random_state= rs), X_train_os, Y_train_os, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","842990a9":"rfc=RandomForestClassifier(random_state=rs)\nrfc.fit(X_train_os,Y_train_os)\ny_pred_rf=rfc.predict(X_train_os)\nprint(accuracy_score(y_pred_rf,Y_train_os))","74196102":"cm = confusion_matrix(Y_train_os, y_pred_rf)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","8231c5e5":"y_pred_test=rfc.predict(X_test)\nprint(accuracy_score(y_pred_test,Y_test))","749e1b80":"cm = confusion_matrix(Y_test, y_pred_test)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","a01dcebd":"print(classification_report(Y_test,y_pred_test))","aaa67789":"#Randomized Search CV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","23bf9475":"random_grid={'n_estimators': n_estimators,\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf}","566e4327":"rf=RandomForestClassifier()","4ca613fb":"rf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,\n                                scoring='f1',\n                                n_iter = 10, cv = 5,\n                                verbose=2, random_state=rs,\n                                n_jobs = 1)\nrf_random.fit(X_train_os,Y_train_os)","dd7d9f75":"rf_random.best_params_","2565ccd8":"rfc= RandomForestClassifier(random_state=rs,\n                            n_estimators=1100,\n                            min_samples_split=5,\n                            max_features='auto',\n                            min_samples_leaf= 1,\n                            max_depth=20,oob_score=True)\nrfc.fit(X_train_os,Y_train_os)","f74958ba":"y_pred_rf_ht=rfc.predict(X_train_os)\nprint(accuracy_score(y_pred_rf_ht,Y_train_os))\ncm = confusion_matrix(Y_train_os, y_pred_rf_ht)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","5079e6a5":"y_pred_test_ht=rfc.predict(X_test)\nprint(accuracy_score(y_pred_test_ht,Y_test))","da24bb60":"cm = confusion_matrix(Y_test, y_pred_test_ht)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","afc23396":"dtc=DecisionTreeClassifier(random_state=rs)\nscore = cross_val_score(dtc, X_train_os, Y_train_os, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","b26b6781":"def hyperparameter_tuning(X,Y,rf):\n#Randomized Search CV\n# Number of features to consider at every split\n    max_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\n    min_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\n    min_samples_leaf = [1, 2, 5, 10]\n\n    random_grid={\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf}\n\n    rf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,\n                                scoring='f1', \n                                n_iter = 10, cv = 5,\n                                verbose=0, random_state=rs,\n                                n_jobs = 1)\n    rf_random.fit(X,Y)\n    return rf_random.best_params_","fd27a4d8":"param_dt=hyperparameter_tuning(X_train_os,Y_train_os,dtc)","41a9c8f2":"## Printing the best parameters obtained after randomizesearch CV or hyperparameter tuning\nprint(param_dt)","35988016":"dtc=DecisionTreeClassifier(random_state=rs,\n                           min_samples_split=10,\n                           min_samples_leaf=2,\n                           max_features='auto',\n                           max_depth=25)\ndtc.fit(X_train_os,Y_train_os)\ny_pred_train_dc_ht=dtc.predict(X_train_os)\ny_pred_test_dt_ht=dtc.predict(X_test)\nprint('Test Accuracy',accuracy_score(y_pred_test_dt_ht,Y_test))\nprint('Train Accuracy',accuracy_score(y_pred_train_dc_ht,Y_train_os))","9b091e65":"# Boosting Algorithms :\nfrom xgboost                          import XGBClassifier\nfrom catboost                         import CatBoostClassifier\nfrom lightgbm                         import LGBMClassifier\n\nfrom scipy.stats                      import randint","068e310c":"mod= CatBoostClassifier(random_state=rs)\n\npar={'max_depth':[5,10,None],\n              'n_estimators':[200,300,400,500,600],'learning_rate':[0.1,0.01,0.001]}\ndef hyperparameter_tuning(mod,param_d,p,q):\n    rdmsearch=  RandomizedSearchCV(mod, param_distributions=param_d,n_jobs=-1,cv=9,scoring='roc_auc')\n    rdmsearch.fit(p,q)\n    ht_params = rdmsearch.best_params_\n    ht_score = rdmsearch.best_score_\n    return ht_params, ht_score\n\n\nrf_parameters, rf_ht_score = hyperparameter_tuning(mod, par,  X_train_os,Y_train_os)\n","e8bd4806":"print(rf_parameters, rf_ht_score)","786aef71":"mod=XGBClassifier(random_state=rs)\nparam_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10], \n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500]\n                }\nrf_parameters_xgb, rf_ht_score_xgb = hyperparameter_tuning(mod, param_tuning,  X_train_os,Y_train_os)","22add021":"print(rf_parameters_xgb, rf_ht_score_xgb)","a57685f6":"lgb = LGBMClassifier()\nlgb.fit(X_train_os,Y_train_os)\n\nlgb_pred = lgb.predict(X_test)\n\nprint(\"Training Accuracy :\", lgb.score(X_train_os, Y_train_os))","177c62e2":"# Create a Dictionary (Key->Value Pairs) for \"ML Model Name\"-> \"ML Model Functions with Hyper-Parameters\" :\n\nClassifiers = {'0.XGBoost' : XGBClassifier(learning_rate =0.1, \n                                           n_estimators=394, \n                                           max_depth=10, \n                                           subsample = 0.50, \n                                           verbosity = 0,\n                                           scale_pos_weight = 2.5,\n                                           updater =\"grow_histmaker\",\n                                           base_score  = 0.2,\n                                          min_child_weight=1),\n                            \n               '1.CatBoost' : CatBoostClassifier(learning_rate=0.1, \n                                                 n_estimators=300, \n                                                 subsample=0.085, \n                                                 max_depth=10, \n                                                 scale_pos_weight=2.5),\n               \n               '2.LightGBM' : LGBMClassifier(subsample_freq = 2, \n                                             objective =\"binary\",\n                                             importance_type = \"gain\",\n                                             verbosity = -1, \n                                             max_bin = 60,\n                                             num_leaves = 300,\n                                             boosting_type = 'dart',\n                                             learning_rate=0.10, \n                                             n_estimators=494,\n                                             max_depth=10, \n                                             scale_pos_weight=2.5)\n }\n\nprint( list(Classifiers.keys()) )\nprint(\"--#--\"*25)\nprint( list(Classifiers.values()) )","c2f358db":"from sklearn.ensemble import VotingClassifier\nvoting_model = VotingClassifier(estimators=[\n                                              ('XGBoost_Best', list(Classifiers.values())[0]), \n                                              ('CatBoost_Best', list(Classifiers.values())[1]),\n                                              ('LightGBM_Best', list(Classifiers.values())[2]),\n                                             ], \n                                              voting='soft',weights=[5,5,5.2])\n\nvoting_model.fit(X_train_os,Y_train_os) \n\npredictions_of_voting = voting_model.predict_proba( test_encode )[::,1]","448b5909":"predictions_of_voting\n","f9cf9887":"y_pred_class = [int(round(value)) for value in predictions_of_voting]","f4310bdc":"### Final ensembel model after hyperparameter tuning ","6c224d4c":"# Data Visualization Libraries :\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px","9f30e6ae":"Classifiers = {'0.XGBoost' : XGBClassifier(learning_rate =0.1, \n                                           n_estimators=394, \n                                           max_depth=5,\n                                           subsample = 0.70, \n                                           verbosity = 0,\n                                           scale_pos_weight = 2.5,\n                                           updater =\"grow_histmaker\",\n                                           base_score  = 0.2),\n               \n            \n               '1.CatBoost' : CatBoostClassifier(learning_rate=0.15, \n                                                 n_estimators=300, \n                                                 max_depth=5, \n                                                 scale_pos_weight=2.5,\n                                                verbose=False),\n               \n               '2.LightGBM' : LGBMClassifier(learning_rate=0.15, \n                                             n_estimators=494,\n                                             subsample_freq = 2, \n                                             objective =\"binary\",\n                                             importance_type = \"gain\",\n                                             verbosity = -1, \n                                             max_bin = 60,\n                                             num_leaves = 300,\n                                             boosting_type = 'dart',                                            \n                                             max_depth=5, \n                                             scale_pos_weight=2.5)\n                }\n\nprint( list(Classifiers.keys()) )\n\nclf1 = list(Classifiers.values())[0]\nclf2 =list(Classifiers.values())[1]\nclf3 = list(Classifiers.values())[2]\nX = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\ny = np.array([1, 1, 2, 2])\n\neclf = VotingClassifier(estimators=[('xgboost', clf1), \n                                    ('catboost', clf2), \n                                    ('lgbm', clf3)],\n                        voting='soft',\n                        weights=[5, 5, 5.2])\n\n# predict class probabilities for all classifiers\nprobas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\n\n# get class probabilities for the first sample in the dataset\nclass1_1 = [pr[0, 0] for pr in probas]\nclass2_1 = [pr[0, 1] for pr in probas]\n\n# plotting\n\nN = 4  # number of groups\nind = np.arange(N)  # group positions\nwidth = 0.35  # bar width\n\nfig, ax = plt.subplots()\n\n# bars for classifier 1-3\np1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width,\n            color='green', edgecolor='k')\np2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width,\n            color='lightgreen', edgecolor='k')\n\n# bars for VotingClassifier\np3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width,\n            color='blue', edgecolor='k')\np4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width,\n            color='steelblue', edgecolor='k')\n\n# plot annotations\nplt.axvline(2.8, color='k', linestyle='dashed')\nax.set_xticks(ind + width)\nax.set_xticklabels(['XGBoost\\nweight 5',\n                    'CatBoost\\nweight 5',\n                    'LightGBM\\nweight 5.2',\n                    'VotingClassifier\\n(average probabilities)'],\n                   rotation=40,\n                   ha='right')\nplt.ylim([0, 1])\nplt.title('Class probabilities for sample 1 by different classifiers')\nplt.legend([p1[0], p2[0]], ['is_promoted=No', 'is_promoted=Yes'], loc='upper right')\nplt.tight_layout()\nplt.show()","dabf05ae":"- {'n_estimators': 1100,\n- 'min_samples_split': 5,\n- 'min_samples_leaf': 1,\n- 'max_features': 'auto',\n- 'max_depth': 20}","aad43281":"# If you like this kernel please upvote and make this kernel reach more people","0304f76c":"## UNIVARIATE ANALYSIS ","a5ff7fc6":"#### Here i have used Multiple Algoriths starting from \n#### 1.Randomforest \n#### 2.Decision tree\n#### 3.CatBoost\n#### 4.XG Boost \n#### 5.LGBM ","32ccaeb0":"## Test train split","1029dfe3":"## Split target variable and predictors","f098a7b7":"# <font size=\"+2\" color=red ><b> <center><u>Using CAT BOOST XGBOOST and LGBM <\/u><\/center><\/b><\/font><br><a id=\"top\"><\/a>","67136fe8":"### FEATURE SELECTION","8b2a9732":"### Correlation between features through Heatmap","a718bc03":"## Model Building ","3465f81e":"## Encoding \n`Converting the categorical features into binary or numerical counterparts`","a467a2a2":"### Hyperparameter Tuning \n- Decision Tree CLASSIFIER\n","a167f55b":"## 2. DECISION TREE\n","4a29bb78":"### Checking Missing Value","f157b415":"### Hyperparameter Tuning \n- CatBoostClassifier\n","f8f1459f":"## BIVARIATE ANALYSIS","d8f12403":"## 3 DATA CLEANING ","61e750dd":"## FEATURE SCALING","e7b6d19a":"###  Cleary we can see data is unbalanced with only 8% of 1s in it so we have to use over Sampling on the data so as to make it balanced\n### using Smote Over Sampling Method ","09c9559e":"- {'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 25}","7dd2071c":"# Loading the Data","7cf95bed":"# <font size=\"+3\" color=Blue ><b> <center><u>HR Analysis, Prediction and Visualization and Ensemble Model <\/u><\/center><\/b><\/font><br><a id=\"top\"><\/a>","6b10185b":"## Missing value Analysis ","dab89a81":"## OVERSAMPLING\n### Checking of the data is unbalanced or balanced ","a1b8106e":"## FEATURE ENGINEERING","122d8eba":"### CHECKING DUPLICATES AND REMOVAL","37f11f8f":"**Used Voting classifier** -A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\n\n **Soft Voting** -In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier.\n","84f30a86":"![](https:\/\/blog.walkme.com\/wp-content\/uploads\/2019\/07\/2.jpg)","89021d16":"## 1.RANDOMFOREST CLASSIFIER","4e512843":"# Boosting algorithms","020d9a23":"* `ffill is used to forward fill the missing values in the dataset - https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-ffill\/`\n\n* `bfill is used to backward fill the missing values in the dataset - https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-bfill\/`","1a36ce84":"### Hyperparameter Tuning \n- RANDOM FOREST CLASSIFIER\n"}}