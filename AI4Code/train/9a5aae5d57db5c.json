{"cell_type":{"7ee86aaf":"code","5b3f85dd":"code","697dc953":"code","6e538220":"code","3bb61d26":"code","fa6831f7":"code","3060ce2a":"code","e35ee65d":"code","5a666e30":"code","d528e990":"code","3d410555":"code","c661da29":"code","edde69ff":"code","39fd340b":"code","f30d085f":"code","511567fe":"code","936b9036":"code","1bad7e9c":"code","74a881e1":"code","19375ba1":"code","07485cf2":"code","2324c8b3":"markdown","c8fe3075":"markdown","b3a7678f":"markdown","dcefb327":"markdown","c8268293":"markdown","76cdf5ed":"markdown","4b4c7995":"markdown","b41f6238":"markdown","e94e792e":"markdown","bf4d97f0":"markdown","d0372e71":"markdown","d438f02f":"markdown","98ce741a":"markdown","f0a3ca99":"markdown","50561e69":"markdown","1243597a":"markdown","3e81ca27":"markdown","10c4b06a":"markdown"},"source":{"7ee86aaf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5b3f85dd":"df = pd.read_csv(\"\/kaggle\/input\/iris-flower-dataset\/IRIS.csv\")","697dc953":"df","6e538220":"df.describe()","3bb61d26":"sns.pairplot(df, hue = 'species')","fa6831f7":"# We're seperating the species column\nspecies = df[\"species\"].tolist()\nX = df.drop(\"species\", 1)","3060ce2a":"# Standardize the data\nX = (X - X.mean()) \/ X.std(ddof=0)","e35ee65d":"# Calculating the correlation matrix of the data\nX_corr = (1 \/ 150) * X.T.dot(X)","5a666e30":"# Plotting the correlation matrix\nplt.figure(figsize=(10,10))\nsns.heatmap(X_corr, vmax=1, square=True,annot=True)\nplt.title('Correlation matrix')","d528e990":"# method1\nu,s,v = np.linalg.svd(X_corr)\neig_values, eig_vectors = s, u\neig_values, eig_vectors","3d410555":"# method2\nnp.linalg.eig(X_corr)","c661da29":"np.sum(eig_values)","edde69ff":"# plotting the variance explained by each PC \nexplained_variance=(eig_values \/ np.sum(eig_values))*100\nplt.figure(figsize=(8,4))\nplt.bar(range(4), explained_variance, alpha=0.6)\nplt.ylabel('Percentage of explained variance')\nplt.xlabel('Dimensions')","39fd340b":"# calculating our new axis\npc1 = X.dot(eig_vectors[:,0])\npc2 = X.dot(eig_vectors[:,1])","f30d085f":"# plotting in 2D\ndef plot_scatter(pc1, pc2):\n    fig, ax = plt.subplots(figsize=(15, 8))\n    \n    species_unique = list(set(species))\n    species_colors = [\"r\",\"b\",\"g\"]\n    \n    for i, spec in enumerate(species):\n        plt.scatter(pc1[i], pc2[i], label = spec, s = 20, c=species_colors[species_unique.index(spec)])\n        ax.annotate(str(i+1), (pc1[i],pc2[i]))\n    \n    from collections import OrderedDict\n    handles, labels = plt.gca().get_legend_handles_labels()\n    by_label = OrderedDict(zip(labels, handles))\n    plt.legend(by_label.values(), by_label.keys(), prop={'size': 15}, loc=4)\n    \n    ax.set_xlabel('Principal Component 1', fontsize = 15)\n    ax.set_ylabel('Principal Component 2', fontsize = 15)\n    ax.axhline(y=0, color=\"grey\", linestyle=\"--\")\n    ax.axvline(x=0, color=\"grey\", linestyle=\"--\")\n    \n    plt.grid()\n    plt.axis([-4, 4, -3, 3])\n    plt.show()\n    \nplot_scatter(pc1, pc2)","511567fe":"def plot_correlation_circle(pc1, pc2):    \n    fig, ax = plt.subplots(figsize=(16, 10))\n\n    for i in range(X.shape[1]):\n        x = np.corrcoef(pc1,X[X.columns[i]])[0,1]\n        y = np.corrcoef(pc2,X[X.columns[i]])[0,1]\n        ax.annotate(\"\", xy= (x,y), xytext=(0, 0),arrowprops=dict(arrowstyle=\"->\"))\n        ax.annotate(X.columns[i], (x+0.02,y+0.02), size=12)\n\n\n    ax.set_title('Correlation circle')\n    ax.axhline(y=0, color=\"grey\", linestyle=\"--\")\n    ax.axvline(x=0, color=\"grey\", linestyle=\"--\")\n\n    an = np.linspace(0, 2 * np.pi, 100)\n    plt.plot(np.cos(an), np.sin(an))\n    plt.axis('equal')\n    plt.show()\n    \nplot_correlation_circle(pc1,pc2)","936b9036":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","1bad7e9c":"X = df.drop(\"species\", 1)\nX = StandardScaler().fit_transform(X)\npca = PCA()\nresult = pca.fit_transform(X)\n# Remember what we said about the sign of eigen vectors that might change ?\npc1 = - result[:,0]\npc2 = - result[:,1]\nplot_scatter(pc1, pc2)","74a881e1":"import plotly.express as px","19375ba1":"pc3 = result[:,2]","07485cf2":"pcs = pd.DataFrame(list(zip(pc1, pc2, pc3, species)),columns =['pc1', 'pc2', 'pc3', 'species']) \nfig = px.scatter_3d(pcs, x='pc1', y='pc2', z='pc3',color='species')\nfig.show()","2324c8b3":"These are the libraries we need, nothing special \u00af\\\\\\_(\u30c4)\\_\/\u00af","c8fe3075":"# 4. PCA with sklearn <a name=\"sklearn\"><\/a>","b3a7678f":"1. [Notebook goal](#notebookgoal)\n2. [PC What ?](#pca)\n3. [Let's code](#letscode)\n4. [PCA with sklearn](#sklearn)\n5. [Bonus](#bonus)","dcefb327":"The next step is to extract the eign values and their respective eigen vectors.\n(\nIt's important to have the sorted list of eigen values \/ eigen vectors) .\n\nWe'll try to calculate them using two methods :","c8268293":"This Free DLC consists of plotting the data in 3D.\n\nRemember that in our case plotting the data in 2D is sufficient.","76cdf5ed":"In this section we'll use the famous sklearn library instead","4b4c7995":"Well it seems that:\n* petal length & petal width are the main things that characterizes the data\n* virginica has relatively bigger petals while setosa has the smallest\n* a small group of setosa have a relatively big sepal_width","b41f6238":"# 2. Let's code <a name=\"letscode\"><\/a> ","e94e792e":"**Note**: instead of standardizing the data manually, we can use sklearns StandardScaler \n> StandardScaler().fit_transform(X)","bf4d97f0":"Notice how the direction of some vectors is not the same in the two outputs. Despite that,our analysis will [still be valid](https:\/\/stats.stackexchange.com\/questions\/88880\/does-the-sign-of-scores-or-of-loadings-in-pca-or-fa-have-a-meaning-may-i-revers)","d0372e71":"# 2. PC What ? <a name=\"pca\"><\/a>\nPrincipal component analysis (PCA) allows us to summarize and to visualize the information in a data set containing individuals\/observations described by multiple inter-correlated quantitative variables. Each variable could be considered as a different dimension. If you have more than 3 variables in your data sets, it could be very difficult to visualize a multi-dimensional hyperspace.\n\nPrincipal component analysis is used to extract the important information from a multivariate data table and to express this information as a set of few new variables called principal components. These new variables correspond to a linear combination of the originals. The number of principal components is less than or equal to the number of original variables.\n\nThe information in a given data set corresponds to the total variation it contains. The goal of PCA is to identify directions (or principal components) along which the variation in the data is maximal.\n\nIn other words, PCA reduces the dimensionality of a multivariate data to two or three principal components, that can be visualized graphically, with minimal loss of information [[sthda](http:\/\/www.sthda.com\/english\/articles\/31-principal-component-methods-in-r-practical-guide\/112-pca-principal-component-analysis-essentials\/)].","d438f02f":"This bar chart shows us that the first two dimensions will be enough to represent the data.\n\nSo let's find these principal components:","98ce741a":"Wooooooo, the sum of the eigen values is equal to number of variables. we're going in the right direction ;)","f0a3ca99":"**Note**: instead of calculating the correlation matrix manually we can just use \n> X.corr()","50561e69":"The same plot, isn't it ?","1243597a":"# 1. Notebook goal <a name=\"notebookgoal\"><\/a>\nIn this notebook we'll focus on implementing PCA  with little help possible from sklearn; You'll see how easy it is; And don't worry we'll compare the results ;)","3e81ca27":"That's it, i'll be happy to receive your feedbacks :D","10c4b06a":"# 4. Bonus <a name=\"bonus\"><\/a>"}}