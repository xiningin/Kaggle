{"cell_type":{"85228a73":"code","19545b8c":"code","0ea91141":"code","62efa7bb":"code","acaeecad":"code","09279c0e":"code","671b32c1":"code","0220a0b5":"code","b7c1964c":"code","ecbd09bc":"code","ea72c531":"code","4b8f725d":"code","789fc5d7":"code","aa018d91":"code","9531a743":"code","448f1c91":"code","3685873f":"code","4d21ba7b":"code","e5e8e317":"code","ddb80af6":"code","6b07b6c8":"code","5dc52125":"code","f98e03de":"markdown","bdc11d08":"markdown","c34869fb":"markdown","386de27c":"markdown","39a9e367":"markdown","0739602f":"markdown","7c92b00b":"markdown","8f62459b":"markdown","abd21aa0":"markdown","9b936191":"markdown","4382520e":"markdown","00986ca8":"markdown","98831511":"markdown","62dc978c":"markdown","7c7f75a1":"markdown","bd5e128d":"markdown","2ae220e7":"markdown","91c80421":"markdown","c6471e0e":"markdown"},"source":{"85228a73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","19545b8c":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\",index_col='PassengerId')\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\", index_col='PassengerId')","0ea91141":"train_data.isnull().sum()","62efa7bb":"test_data.isnull().sum()","acaeecad":"train_data['Sex'].replace({'male':1 , 'female':0},inplace=True)\ntrain_data['Embarked'].replace({'C': 1, 'S': 2 ,'Q': 3},inplace=True)\ntrain_data['Embarked'].fillna(1,inplace=True)\ntest_data['Sex'].replace({'male':1 , 'female':0},inplace=True)\ntest_data['Embarked'].replace({'C': 1, 'S': 2 ,'Q': 3},inplace=True)","09279c0e":"train_data['Age'].fillna(train_data['Age'].median(), inplace = True)\ntest_data['Age'].fillna(test_data['Age'].median(),inplace = True )\ntest_data['Fare'].fillna(test_data['Fare'].mean(), inplace = True)","671b32c1":"train_data.isnull().sum()","0220a0b5":"test_data.isnull().sum()","b7c1964c":"sns.barplot(x = train_data['Pclass'] , y = 1 - train_data['Survived'])\nplt.title(\"No.of deaths according to Pclass\")\nplt.ylabel('Deaths')\n\n# It can be observed that as the people from the 3rd class suffered the highest deaths","ecbd09bc":"sns.barplot(x = train_data['Sex'] , y = train_data['Survived'])\nplt.title(\"No.of survivals according to Sex\")\n\n#It is observed that most of the survivors were female","ea72c531":"sns.barplot(x = train_data['SibSp'] , y = train_data['Survived'])\nplt.title(\"Effect of sibling(s) and spouse on survival\")\n\n# It can be observed that people with 0,1,2 spouse\/sibling(s) had the highest survival rate","4b8f725d":"sns.barplot(x = train_data['Parch'] , y = train_data['Survived'])\nplt.title(\"Effect of parent and children on survival\")\n\n# It can be observed that parents with 0,1,2,3 children had the highest survival rate","789fc5d7":"sns.barplot(x = train_data['Embarked'] , y = train_data['Survived'])\nplt.title(\"Ports Embarked and Survival\")\n\n# It can be observed that people who Embarked from Cherbourg had the highest survival rate","aa018d91":"#Creating age groups and plotting it\n\nage_labels = ['Kids','Teenager','Adult','Middle aged','Retired','Old']\nage_bins = [0,10,18,35,60,75,100]\ntrain_data['binned_age'] = pd.cut(train_data['Age'], bins=age_bins, labels=age_labels)\ntrain_data['binned_age'].replace({ 'Kids' : 1,'Teenager' : 2,'Adult' : 3,'Middle aged' : 4,'Retired' : 5,'Old' : 6 }, inplace= True)\n\ntest_data['binned_age'] = pd.cut(test_data['Age'], bins=age_bins, labels=age_labels)\ntest_data['binned_age'].replace({ 'Kids' : 1,'Teenager' : 2,'Adult' : 3,'Middle aged' : 4,'Retired' : 5,'Old' : 6 }, inplace= True)\n\nsns.barplot(x = train_data['binned_age'] , y = train_data['Survived'])\nplt.title(\"Survival according to age groups\")\n\n#It can observed that old people \/ group 6 survived the most.","9531a743":"#Creating Fare groups and plotting it\n\ntrain_data['Fare'].value_counts()\n\nfare_labels = ['Base Class','Express Class','Gold Class','Platinum Class']\nfare_bins = [0,15,30,100,1000]\ntrain_data['binned_fare'] = pd.cut(train_data['Fare'], bins=fare_bins, labels=fare_labels)\ntrain_data['binned_fare'].replace({ 'Base Class' : 1,'Express Class' : 2,'Gold Class' : 3,'Platinum Class' : 4}, inplace= True)\n\ntest_data['binned_fare'] = pd.cut(test_data['Fare'], bins=fare_bins, labels=fare_labels)\ntest_data['binned_fare'].replace({ 'Base Class' : 1,'Express Class' : 2,'Gold Class' : 3,'Platinum Class' : 4}, inplace= True)\n\nsns.barplot(x = train_data['binned_fare'] , y = train_data['Survived'])\nplt.title(\"Survival according to fare groups\")\n\n#The plot shows that people travelling via the platinum class \/ group 4 survived a lot more than the other class of people","448f1c91":"train_data['binned_fare'].fillna(4,inplace = True)\ntest_data['binned_fare'].fillna(1,inplace=True)","3685873f":"features = ['Pclass','Sex','binned_age','SibSp','Parch','Embarked','binned_fare']\nX = train_data[features]\ny = train_data.Survived\nX_test = test_data[features]","4d21ba7b":"X_test.isnull().sum()\nX.isnull().sum()","e5e8e317":"from sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n#n_est = 400 , depth = 10 , obtained by tuning\n\nRfc_model = RandomForestClassifier(n_estimators = 400,max_depth=10,n_jobs= -1,random_state=33)\nscores = cross_val_score(Rfc_model,X,y,cv=5,scoring='f1')\nprint(round(scores.mean(),5)*100)","ddb80af6":"from sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#n_neighbours = 5 , leaf_size = 50 has been obtained after tuning \n\nknn_model = KNeighborsClassifier(n_neighbors=5,leaf_size = 50,n_jobs=-1)\nscores = cross_val_score(knn_model,X,y,cv=5,scoring='f1')\nprint(round(scores.mean(),5)*100)","6b07b6c8":"from sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nLoReg_model = LogisticRegression(random_state = 3,n_jobs = -1)\nscores = cross_val_score(LoReg_model,X,y,cv=5,scoring='f1')\nprint(round(scores.mean(),5)*100)","5dc52125":"#Fitting the data\n\nRfc_model.fit(X,y)\npred_RFC = Rfc_model.predict(X_test)\noutput = pd.DataFrame({'PassengerId': X_test.index, 'Survived': pred_RFC})\noutput.to_csv('Sub_RFC.csv', index=False)\n\nknn_model.fit(X,y)\npred_knn = knn_model.predict(X_test)\noutput = pd.DataFrame({'PassengerId': X_test.index, 'Survived': pred_knn})\noutput.to_csv('Sub_knn.csv', index=False)\n\nLoReg_model.fit(X,y)\npred_LoReg = LoReg_model.predict(X_test)\noutput = pd.DataFrame({'PassengerId': X_test.index, 'Survived': pred_LoReg})\noutput.to_csv('Sub_LoReg.csv', index=False)\n\n\nprint(\"Saved!\")","f98e03de":"Using Random Forest Classification has an accuracy of 73.13%","bdc11d08":"Defining features and creating the test and train data","c34869fb":"# ***Exploratory Data Analysis***","386de27c":"###### Choosing the right model - Since all the three models have almost similar accuracy(70,73,74 %) approx , we can choose by directly submitting the result(s) and see which gives better result","39a9e367":"Using Logistic Regression gives an accuracy of 70.58%","0739602f":"Final check to see if any null values exist","7c92b00b":"It can be seen that only the Cabin column now has missing values.\nSince it won't be used as a feature, it can be ignored","8f62459b":"KNN","abd21aa0":"Filling missing values in Age and Fare with their median and mean respectively","9b936191":"Choosing Model and determining accuracy via Cross Validation and f1 score","4382520e":"##### Random Forest Classification","00986ca8":"##### The submission results show that the model results were -  \n* RandomForestClassification with score of 0.77033 \n* Logistic Regression with score of 0.7655\n* KNN with score of 0.74162","98831511":"Creating Age Group and Fare Group","62dc978c":"Note - creating bins has resulted in some missing values in the binned_fare column. Replcing the missing values with the most frequent value","7c7f75a1":"Futher improvements to the model can be made via Feature Engineering, but since I am still unknown to that field, I will eventually delve into it. \nFor any suggestions, pls comment. I appreciate any and all suggestions.","bd5e128d":"Logistic Regression","2ae220e7":"Using K-Nearest Neighbours has an accuracy of 74.52%","91c80421":"Loading the data and checking for missing values","c6471e0e":"Encoding categorical values into numeric ones "}}