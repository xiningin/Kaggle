{"cell_type":{"b6f859ad":"code","ccfa0fa0":"code","84d6fc48":"code","2a9e411b":"code","ba3446a7":"code","c80a745c":"code","e279b476":"code","91bf3ade":"code","652ab23a":"code","f0de2b4e":"code","ce66d31d":"code","ef2cb933":"code","3173a11d":"code","cd304fa9":"code","79f5df83":"code","21010c21":"code","139a568b":"code","10a0f3bc":"code","a77e604b":"code","0e15b239":"code","6b908e1a":"code","4eb121a4":"code","b8b9b40a":"code","30ef90e5":"code","efebc354":"code","bd222732":"code","8e8a0a25":"code","e7faf142":"code","66a8006a":"code","174d36a2":"markdown","c7c7b38e":"markdown","30a47d67":"markdown","6a645637":"markdown","110cc86c":"markdown","d193ab3f":"markdown","ca6fde9b":"markdown","66daf259":"markdown","41b68bb2":"markdown","c7301605":"markdown","0727f3f4":"markdown","348a946e":"markdown","9e866fac":"markdown","b38e5ef9":"markdown","b6739cb4":"markdown","7c46cedb":"markdown","03e8b087":"markdown","b7ec8f52":"markdown","cc35bd3c":"markdown","19eff290":"markdown"},"source":{"b6f859ad":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ccfa0fa0":"train_df = pd.read_csv(\"..\/input\/widsdatathon2022\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/widsdatathon2022\/test.csv\")\n\ntrain_df.head()","84d6fc48":"train_df.columns","2a9e411b":"test_submission = pd.DataFrame(test_df[\"id\"])\ntrain_df.drop(columns=[\"id\"], axis=1, inplace=True)\ntest_df.drop(columns=[\"id\"], axis=1, inplace=True)","ba3446a7":"print(f'Training dataset shape: {train_df.shape}')\nprint(f'Training dataset shape: {test_df.shape}')","c80a745c":"l0 = train_df.shape[0]\ntrain_df.drop_duplicates(inplace=True)\nl1 = train_df.shape[0]\nprint(f'{l0-l1} rows were dropped.')","e279b476":"def count_missing_values(dataframe):\n    \n    total = dataframe.isnull().sum()\n    percent = (dataframe.isnull().sum())*100\/(len(dataframe))\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    missing_data = missing_data[missing_data['Total']>0]\n    missing_data.sort_values('Total', ascending=False, inplace=True)\n    print(missing_data)\n    \n    return \n\ncount_missing_values(train_df)","91bf3ade":"count_missing_values(test_df)","652ab23a":"to_drop = [\"days_with_fog\", \"direction_max_wind_speed\", \"direction_peak_wind_speed\", \"max_wind_speed\"]\ntrain_df.drop(columns=to_drop, axis=1, inplace=True)\ntest_df.drop(columns=to_drop, axis=1, inplace=True)","f0de2b4e":"f = plt.figure(figsize=(10, 10))\nplt.matshow(train_df.corr(), fignum=f.number)\nplt.colorbar()\nplt.show()","ce66d31d":"def correlation(dataframe, threshold):\n    high_corr = set() # Set of all the names of deleted columns\n    corr_matrix = dataframe.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and ((corr_matrix.columns[j], corr_matrix.columns[i]) not in high_corr):\n                columns = (corr_matrix.columns[i], corr_matrix.columns[j]) # getting the name of the columns\n                high_corr.add(columns)\n\n    return high_corr\n\nhigh_corr = correlation(train_df, 0.95)\nprint(high_corr)","ef2cb933":"to_drop = ['january_min_temp']\ntrain_df.drop(columns=to_drop, axis=1, inplace=True)\ntest_df.drop(columns=to_drop, axis=1, inplace=True)\n\nprint(f'Training dataset shape: {train_df.shape}')\nprint(f'Training dataset shape: {test_df.shape}')","3173a11d":"train_y = train_df[\"site_eui\"]\ntrain_df.drop(\"site_eui\", axis=1, inplace=True)","cd304fa9":"cat_columns = train_df.dtypes[train_df.dtypes == \"object\"].index.tolist()\nnumerical_columns = train_df.dtypes[train_df.dtypes != \"object\"].index.tolist()\nprint(cat_columns)","79f5df83":"def plot_distbn(column, column_name, num):\n    plt.subplot(num)\n    column.value_counts(normalize=True).plot(kind='bar')\n    plt.title(column_name)\n    plt.ylabel('Relative frequency')\n    plt.tight_layout()\n    plt.grid(True)\n    \n    return column.value_counts(normalize=True)\n\n\nplt.rcParams['axes.axisbelow'] = True\nplt.figure(figsize=(15,6))\nrel_freqs1 = plot_distbn(train_df[cat_columns[0]], cat_columns[0], 311)\nrel_freqs2 = plot_distbn(train_df[cat_columns[1]], cat_columns[1], 312)\nrel_freqs3 = plot_distbn(train_df[cat_columns[2]], cat_columns[2], 313)","21010c21":"rel_freqs3","139a568b":"sum(rel_freqs3[:6])","10a0f3bc":"significant = rel_freqs3.index[:6].tolist()\nprint(significant)","a77e604b":"train_df['facility_type'].loc[~train_df['facility_type'].isin(significant)] = 'Others'\ntest_df['facility_type'].loc[~test_df['facility_type'].isin(significant)] = 'Others'\ntrain_df[:10]","0e15b239":"for column in cat_columns:\n    # Training data\n    dummies = pd.get_dummies(train_df[column], prefix=column, drop_first=True)\n    train_df = pd.concat([train_df, dummies],axis=1)\n    train_df.drop(column, axis=1, inplace=True)\n    # Test data\n    dummies = pd.get_dummies(test_df[column], prefix=column, drop_first=True)\n    test_df = pd.concat([test_df, dummies],axis=1)\n    test_df.drop(column, axis=1, inplace=True)\n    \n# Get missing columns in the training test\nmissing_cols = set(train_df.columns) - set(test_df.columns)\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    test_df[c] = 0\n    \n# Ensure the order of columns in the test set is in the same order than in train set\ntest_df = test_df[train_df.columns]","6b908e1a":"train_df.columns","4eb121a4":"len(numerical_columns)","b8b9b40a":"train_df[numerical_columns] = train_df[numerical_columns].fillna(train_df[numerical_columns].median())\ntest_df[numerical_columns] = test_df[numerical_columns].fillna(test_df[numerical_columns].median())","30ef90e5":"\nscaler = preprocessing.StandardScaler().fit(train_df[numerical_columns])\ntrain_df[numerical_columns] = scaler.transform(train_df[numerical_columns])\ntest_df[numerical_columns] = scaler.transform(test_df[numerical_columns])","efebc354":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# List of algorithms to implement\nmodels = []\nmodels.append(('Linear regression', LinearRegression()))\nmodels.append(('k-Nearest neighbor', KNeighborsRegressor()))\nmodels.append(('Decision tree', DecisionTreeRegressor()))\nmodels.append(('Random forest', RandomForestRegressor()))\nmodels.append(('XGBoost classifier', XGBRegressor()))","bd222732":"# Create a dataframe to store metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\ncol = ['Algorithm', 'Mean RMSE', 'S.D. RMSE']\ndf_results = pd.DataFrame(columns=col)\ni = 0\n\n# Fit the models one by one\nfor name, model in models:\n    print(f'Fitting {name} model...')\n    kfold = KFold(n_splits=5, shuffle=True, random_state=10)  # 5-fold cross-validation\n    cv_rmse_results = cross_val_score(model, train_df, train_y, cv=5, scoring=\"neg_root_mean_squared_error\")\n    cv_rmse_results *= -1\n    df_results.loc[i] = [name,\n                         cv_rmse_results.mean(),\n                         cv_rmse_results.std(),\n                         ]\n    i += 1\ndf_results.sort_values(by=['Mean RMSE'])","8e8a0a25":"model = RandomForestRegressor()\nrf = model.fit(train_df, train_y)\npred = rf.predict(test_df)","e7faf142":"test_submission[\"site_eui\"] = pred\ntest_submission","66a8006a":"test_submission.to_csv(\"submission.csv\",index=False)","174d36a2":"# Import libraries","c7c7b38e":"There are 54 numerical features, out of which energy_star_rating and year_built has missing values. Let us impute these values using the median of the respective features.","30a47d67":"### Scale numerical features","6a645637":"The first pair makes sense, since average and minimum temperatures in January are bound to be correlated. But the second pair's correlation seems a little coincidental. Let us drop one of the first pair, but keep both of the second.","110cc86c":"### Dataset dimensions","d193ab3f":"# Basic EDA + data pre-processing","ca6fde9b":"### Load data","66daf259":"### Compute pair-wise correlation between numeric columns and drop one if this is greater than a threshold","41b68bb2":"More than 85% of values in the columns days_with_fog, direction_max_wind_speed, direction_peak_wind_speed and max_wind_speed are missing in the test set, so they will not be very useful as features. We can drop them.","c7301605":"### Separate features and labels","0727f3f4":"Let us look at the distribution of unique values for each categorical feature in the dataset.","348a946e":"# Model building and evaluation","9e866fac":"* state_factor seems to have an even spread (except for state_6 which contains >50% of the buildings). We will use one-hot encoding for this feature.\n* Building class is a binary variable. \n* The distribution of facility_type has a long tail. In fact the top 6 values claim >80% of the data points. We can bin all other values into one value, and do one-hot encoding with these 7 values for this feature.","b38e5ef9":"The purpose of this notebook is to do basic EDA and try out 5 simple regression models with default hyperparameters using 5-fold cross-validation on the given dataset. We will then use the best default model for submitting predictions.","b6739cb4":"### Drop building id","7c46cedb":"### Observations","03e8b087":"### Separate categorical and numerical features","b7ec8f52":"### Drop duplicate rows","cc35bd3c":"Random forest has the lowest RMSE with default hyperparameters.","19eff290":"### Check proportion of missing values in each column"}}