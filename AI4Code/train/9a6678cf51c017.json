{"cell_type":{"3eadb520":"code","1ec9d7d0":"code","cb510f62":"code","71123913":"code","e561416d":"code","82869a1e":"code","d7f5849d":"code","731d7618":"code","ca4b5326":"code","4a475261":"code","1f46e772":"code","515e1188":"code","e06e4792":"code","75c1918e":"code","9ea599f6":"code","bc519d3f":"code","aa7a69c6":"code","71230346":"code","95f44923":"code","a36d0b34":"code","3c767580":"code","f83781bd":"code","c63a871e":"code","2e662808":"code","524f4bf8":"code","cb14cb03":"code","5253fefa":"code","9c047ef6":"markdown","0ad368e4":"markdown","8dd39482":"markdown","84d74b64":"markdown","29082103":"markdown","91c6c862":"markdown","02b70bb4":"markdown","fd95e070":"markdown","42129345":"markdown","b29c4cf4":"markdown","38d39aae":"markdown","8a8d0613":"markdown","340dbdde":"markdown","340f1322":"markdown","3a7c71d9":"markdown","6c617c99":"markdown","e4aad682":"markdown","d4f58725":"markdown","05b592fd":"markdown","d4beb6a5":"markdown","ab4f9953":"markdown","ac298450":"markdown","cbb335b9":"markdown","a9f44239":"markdown","2ac26445":"markdown","05f5d72d":"markdown","ec32544f":"markdown"},"source":{"3eadb520":"import random\n\nimport pandas as pd\nimport numpy as np \nfrom scipy.special import softmax\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (roc_auc_score, classification_report, \n                             confusion_matrix)\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nfrom transformers import TFBertForSequenceClassification\nfrom transformers import AutoConfig\n","1ec9d7d0":"# The name of the BERT model used\nPRETRAINED_MODEL_NAME = 'bert-base-uncased'\n# The number of labels of the target variable\nLABELS_NUMBER = 2\n\n# The max lenght of text can be up to 512 for BERT\nMAX_LENGHT = 512\n\nBATCH_SIZE = 6\nLEARNING_RATE = 2e-5\nEPOCHS_NUMBER = 1\n\nN_PREDICTIONS_TO_SHOW = 10","cb510f62":"train_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nprint(train_data.shape)\ntrain_data.head(3)","71123913":"# load test dataset\ntest_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nprint(test_data.shape)\ntest_data.head(3)","e561416d":"for tweet_index in range(1,30,5):\n    print(f'Text of the tweet: {train_data[\"text\"][tweet_index]}')\n    print(f'Target: {\"Real disaster\" if train_data[\"target\"][tweet_index]==1 else \"Not real disaster\"}\\n')","82869a1e":"sns.countplot(train_data[\"target\"])","d7f5849d":"# Get the Bert tokenizer\ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME, \n                                          do_lower_case=True)","731d7618":"# Print some words of the vocabulary\nvocabulary = tokenizer.get_vocab()\nprint(f'Size of the vocabulary: {len(vocabulary)}')\nprint(f'Some tokens of the vocabulary: {list(vocabulary.keys())[5000:5010]}')","ca4b5326":"def prepare_sequence(text):\n    \"\"\"\n    Tokenize and prepare a sequence for the model. It tokenizes the text sequence\n    adding special tokens ([CLS], [SEP]), padding  to the max length and truncate \n    reviews longer than the max length.\n    Return the token IDs, the segment IDs and the mask IDs.\n    \"\"\"\n\n    prepared_sequence = tokenizer.encode_plus(\n                            text, \n                            add_special_tokens = True, \n                            max_length = MAX_LENGHT, \n                            padding = 'max_length',\n                            return_attention_mask = True\n                            )\n    return prepared_sequence","4a475261":"# Prepare a test sentence\ntest_sentence = 'Is this jacksonville?'\ntest_sentence_encoded = prepare_sequence(test_sentence)\ntoken_ids = test_sentence_encoded[\"input_ids\"]\nprint(f'Test sentence:   {test_sentence}')\nprint(f'Keys:            {test_sentence_encoded.keys()}')\nprint(f'Tokens:          {tokenizer.convert_ids_to_tokens(token_ids)[:12]}')\nprint(f'Token IDs:       {token_ids[:12]}')\nprint(f'Segment IDs:     {test_sentence_encoded[\"token_type_ids\"][:12]}')\nprint(f'Mask IDs         {test_sentence_encoded[\"attention_mask\"][:12]}')\nprint(f'Input dimension: {len(token_ids)}')","1f46e772":"def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n    \"\"\"\n    Map to the expected input to TFBertForSequenceClassification.\n    \"\"\"\n    mapped_example = {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"attention_mask\": attention_masks,\n    }\n    return mapped_example, label \n\ndef encode_examples(texts_and_labels):\n    \"\"\"\n    Prepare all sequences of text and build TF dataset.\n    \"\"\"\n\n    input_ids_list = []\n    token_type_ids_list = []\n    attention_mask_list = []\n    label_list = []\n        \n    for text, label in texts_and_labels:\n\n        bert_input = prepare_sequence(text)\n\n        input_ids_list.append(bert_input['input_ids'])\n        token_type_ids_list.append(bert_input['token_type_ids'])\n        attention_mask_list.append(bert_input['attention_mask'])\n        label_list.append([label])\n\n    # Create TF dataset\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (input_ids_list, attention_mask_list, token_type_ids_list,\n         label_list)\n    )\n    # Map to the expected input to TFBertForSequenceClassification\n    dataset_mapped = dataset.map(map_example_to_dict)\n    return dataset_mapped","515e1188":"X = train_data[\"text\"]\ny = train_data[\"target\"]","e06e4792":"# Split the training dataset for training and test\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, \n                                                    random_state=1)","75c1918e":"n_training_examples = X_train.shape[0]\nn_positive_training_examples = y_train.value_counts()[1]\nn_negative_training_examples = y_train.value_counts()[0]\nprint(f'Number examples in training dataset: {n_training_examples}')\nprint(f'Number of positive examples in training dataset: {n_positive_training_examples}')\nprint(f'Number of negative examples in training dataset: {n_negative_training_examples}')","9ea599f6":"train_dataset = list(zip(X_train, y_train))\nval_dataset = list(zip(X_val, y_val))","bc519d3f":"# Prepare sequences of text and build TF train dataset\nds_train_encoded = encode_examples(train_dataset).shuffle(10000).batch(BATCH_SIZE)\n\n# Prepare sequences of text and build TF validation dataset\nds_val_encoded = encode_examples(val_dataset).batch(BATCH_SIZE)","aa7a69c6":"def get_model():\n    # Define the configuration of the model\n    config = AutoConfig.from_pretrained(PRETRAINED_MODEL_NAME,\n                                        hidden_dropout_prob=0.2,\n                                        num_labels=LABELS_NUMBER)\n    # Model initialization\n    model = TFBertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, \n                                                            config=config)\n    return model","71230346":"# Model initialization\nmodel = get_model()\n\n# Define the optimizer, the loss function and metrics\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n# Compile the model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])","95f44923":"# Scaling by total\/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\nweight_for_0 = (1 \/ n_negative_training_examples)*(n_training_examples)\/2.0 \nweight_for_1 = (1 \/ n_positive_training_examples)*(n_training_examples)\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","a36d0b34":"# Train the model\nmodel.fit(ds_train_encoded, epochs=EPOCHS_NUMBER, validation_data=ds_val_encoded,\n          class_weight = class_weight)","3c767580":"# Get predictions in the validation dataset\nval_predictions = model.predict(ds_val_encoded)\nval_probabilities = softmax(val_predictions[0], axis=1)\ny_val_predictions = np.argmax(val_probabilities, axis=1).flatten()","f83781bd":"# Compute metrics to evaluate the model\nclassification_metrics = classification_report(y_val, y_val_predictions)\n# Compute the area under the ROC curve\narea_under_the_curve = roc_auc_score(y_val, val_probabilities[:,1:2], multi_class=\"ovr\")\n# Compute the confusion matrix\nerror_matrix = confusion_matrix(y_val, y_val_predictions)\nprint(f'Area under the ROC curve: {area_under_the_curve}')\nprint(f'Classification metrics:\\n{classification_metrics}')\n# Plot the confusion matrix\nax = plt.axes()\nsns.heatmap(error_matrix, annot=True, fmt=\"d\")\nax.set_title('Confusion matrix Validation set')","c63a871e":"# Show some predictions in the validation dataset\nfor i in random.sample(range(len(val_dataset)), k=N_PREDICTIONS_TO_SHOW):\n    print(f'\\nText:       {X_test.values[i]}')\n    print(f'Ground truth: {\"Real disaster\" if y_val.values[i]==1 else \"Not real disaster\"}')\n    print(f'Predicted:    {\"Real disaster\" if y_val_predictions[i]==1 else \"Not real disaster\"}')","2e662808":"def encode_test_examples(texts):\n    \"\"\"\n    Prepare all sequences of text and build TF dataset.\n    \"\"\"\n\n    input_ids_list = []\n    token_type_ids_list = []\n    attention_mask_list = []\n        \n    for text in texts:\n\n        bert_input = prepare_sequence(text)\n\n        input_ids_list.append(bert_input['input_ids'])\n        token_type_ids_list.append(bert_input['token_type_ids'])\n        attention_mask_list.append(bert_input['attention_mask'])\n\n    # Create TF dataset\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (input_ids_list, attention_mask_list, token_type_ids_list)\n    )\n    # Map to the expected input to TFBertForSequenceClassification\n    dataset_mapped = dataset.map(map_test_example_to_dict)\n    return dataset_mapped\n\ndef map_test_example_to_dict(input_ids, attention_masks, token_type_ids):\n    \"\"\"\n    Map to the expected input to TFBertForSequenceClassification.\n    \"\"\"\n    mapped_example = {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"attention_mask\": attention_masks,\n    }\n    return mapped_example","524f4bf8":"X_test = test_data[\"text\"]\ntest_dataset = list(X_test)\nds_test_encoded = encode_test_examples(test_dataset).batch(BATCH_SIZE)","cb14cb03":"test_predictions = model.predict(ds_test_encoded)\ntest_probabilities = softmax(test_predictions[0], axis=1)\ny_test_predictions = np.argmax(test_probabilities, axis=1).flatten()","5253fefa":"# Copy the results to a pandas dataframe with an \"id\" column and a \"target\" column\nfinal_submission = pd.DataFrame( data={\"id\":test_data[\"id\"], \"target\":y_test_predictions})\n# Save the submission file\nfinal_submission.to_csv(\"submissionTweets.csv\", index=False)","9c047ef6":"## Define constants","0ad368e4":"## Model training","8dd39482":"## Model","84d74b64":"## References","29082103":"We build the test dataset.","91c6c862":"## Data exploration","02b70bb4":"Data is provided as two separate files, one with texts for training the model and another one for testing.","fd95e070":"Let's see how a few tweets look like in the training dataset.","42129345":"Let's see how a test sentence is prepared.","b29c4cf4":"## Load data","38d39aae":"Let's evaluate the model in the validation dataset.\n\nOnce the model is trained we can obtain the model predictions with the `predict` method. Each prediction is a list of unnormalized probabilities for each class. To obtain actual probabilities we need to apply a `softmax` function, which enforces each probability to take a value in the range `[0, 1]`, and also that all probabilities sum up to 1. Finally, apply the `argmax` numpy function to get the labels predicted.","8a8d0613":"We define the model, the optimizer, the loss function and the metrics used. Then we compile the model.","340dbdde":"The training dataset is slightly imbalanced. There are a few more tweets with label 0 (no disaster) than label 1 (disaster tweets).\n","340f1322":"- [BERT documentation Transformers](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html)\n\n- [Classify text with bert TensorFlow](https:\/\/www.tensorflow.org\/tutorials\/text\/classify_text_with_bert)\n\n- [Imbalanced data TensorFlow](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data)","3a7c71d9":"## Tokenization","6c617c99":"Let's check if the training dataset is balanced looking at the distribution of the target variable:","e4aad682":"## Import modules","d4f58725":"We build the training and validation datasets in the format expected by the `TFBertForSequenceClassification` model.","05b592fd":"## Introduction","d4beb6a5":"The goal is to identify tweets about disasters, but we don't have as many positive samples as negative ones, so we would want to have the classifier heavily weight the few examples that are available. We can do this by passing weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class.","ab4f9953":"In this notebook, we build a machine learning model that predicts which Tweets are about real disasters and which ones aren\u2019t. The description of the task can be found [here](https:\/\/www.kaggle.com\/c\/nlp-getting-started). We are going to fine-tune a pre-trained Transformer-based model (BERT) to build our classification model. A review of different transfer learning techniques, including BERT, is available [here](https:\/\/nbviewer.jupyter.org\/github\/victorviro\/Deep_learning_python\/blob\/master\/Transfer_learning_in_NLP.ipynb).\n\nWe use [Transformers](https:\/\/github.com\/huggingface\/transformers), a state-of-the-art NLP library for PyTorch and TensorFlow 2.0 that provides implementations of many language models like BERT, GPT-2, and many more. It also allows making use of pre-trained versions of these models.","ac298450":"The first step is to tokenize the data (see [Introduction to tokenization](https:\/\/huggingface.co\/transformers\/tokenizer_summary.html)). In particular, BERT uses WordPiece tokenization.","cbb335b9":"We get the predictions in the test dataset.","a9f44239":"The training dataset includes a column `text` with the text we must classify and an additional column `target` which tells us if the tweet is about a real disaster (1-> real disaster). The test dataset looks similar.","2ac26445":"## Predictions in test dataset","05f5d72d":"## Model evaluation","ec32544f":"## Data preparation"}}