{"cell_type":{"68b25cef":"code","d34eff2c":"code","58acd236":"code","1574c99e":"code","bd53566c":"code","2e9a4caa":"code","5c5559e9":"code","0aa6fd3e":"code","54001fe2":"code","76e2b67e":"code","9dff63ec":"code","4640d3de":"code","ff56ce6c":"code","364e3176":"code","41a33144":"code","83b9bee3":"code","9de45c4f":"code","7bc8b04f":"code","e6a844f6":"code","8d836054":"code","556730c2":"code","1c23a4c1":"code","68e7c20f":"code","be5f4d13":"code","2c3270d1":"code","2634c034":"code","7661ea93":"code","b97f9025":"markdown","7a2a3176":"markdown","cc512bd4":"markdown","ef609333":"markdown","adf809b3":"markdown","09c99a29":"markdown","9196c1fe":"markdown","cff424f9":"markdown","38ae6345":"markdown","d2eb25f1":"markdown","fba55ad9":"markdown","30f0c0aa":"markdown","6079b8f7":"markdown","89310c96":"markdown","66dc45d7":"markdown","e030ed5b":"markdown","fd20040e":"markdown","4e43e6ef":"markdown","41da370e":"markdown"},"source":{"68b25cef":"# Import essential libraries\nimport os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom numpy.random import rand\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport random\nfrom random import randrange\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xg\n\n\n# setting up options\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n# import datasets\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n\n# converting column without decimal to integer\nfor col in train_df.columns:\n    if np.sum((train_df[col] - train_df[col].astype('int'))) == 0:\n        train_df[col] = train_df[col].astype('int')\n        \nfor col in test_df.columns:\n    if np.sum((test_df[col] - test_df[col].astype('int'))) == 0:\n        test_df[col] = test_df[col].astype('int')","d34eff2c":"train_df.head()","58acd236":"print(f'Number of rows: {train_df.shape[0]};  Number of columns: {train_df.shape[1]}; No of missing values: {sum(train_df.isna().sum())}')","1574c99e":"train_df.dtypes","bd53566c":"train_df.describe()","2e9a4caa":"test_df.head()","5c5559e9":"print(f'Number of rows: {test_df.shape[0]};  Number of columns: {test_df.shape[1]}; No of missing values: {sum(test_df.isna().sum())}')","0aa6fd3e":"test_df.dtypes","54001fe2":"submission.head()","76e2b67e":"X_train = train_df.drop(['id','loss'], axis=1).values\ny_train = train_df['loss'].values","9dff63ec":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)","4640d3de":"print(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of y_train:\", y_train.shape)","ff56ce6c":"# error rate\ndef error_rate(xtrain, ytrain, x, opts):\n    # parameters\n    fold = opts['fold']\n    xt = fold['xt']\n    yt = fold['yt']\n    xv = fold['xv']\n    yv = fold['yv']\n    # number of instances\n    num_train = np.size(xt, 0)\n    num_valid = np.size(xv, 0)\n    # Define selected features\n    xtrain = xt[:, x == 1]\n    ytrain = yt.reshape(num_train)\n    xvalid = xv[:, x == 1]\n    yvalid = yv.reshape(num_valid)\n    # Training\n    mdl     = LinearRegression()\n    mdl.fit(xtrain, ytrain)\n    # Prediction\n    ypred   = mdl.predict(xvalid)\n    error   = mean_squared_error(yvalid, ypred, squared=False)\n    \n    return error","364e3176":"# Error rate & Feature size\ndef Fun(xtrain, ytrain, x, opts):\n    # parameters\n    alpha = 0.99\n    beta = 1 - alpha\n    # original feature size\n    max_feat = len(x)\n    # Number of selected features\n    num_feat = np.sum(x == 1)\n    # Solve if no feature selected\n    if num_feat == 0:\n        cost = 1\n    else:\n        # Get error rate\n        error = error_rate(xtrain, ytrain, x, opts)\n        # Objective function\n        cost = alpha * error + beta * (num_feat \/ max_feat)\n        \n    return cost","41a33144":"def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    \n    return X","83b9bee3":"def init_velocity(lb, ub, N, dim):\n    V    = np.zeros([N, dim], dtype='float')\n    Vmax = np.zeros([1, dim], dtype='float')\n    Vmin = np.zeros([1, dim], dtype='float')\n    # Maximum & minimum velocity\n    for d in range(dim):\n        Vmax[0,d] = (ub[0,d] - lb[0,d]) \/ 2\n        Vmin[0,d] = -Vmax[0,d]\n        \n    for i in range(N):\n        for d in range(dim):\n            V[i,d] = Vmin[0,d] + (Vmax[0,d] - Vmin[0,d]) * rand()\n        \n    return V, Vmax, Vmin","9de45c4f":"def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    \n    return Xbin","7bc8b04f":"def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    \n    return x","e6a844f6":"def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0\n    thres = 0.5\n    w     = 0.9    # inertia weight\n    c1    = 2      # acceleration factor\n    c2    = 2      # acceleration factor\n    \n    N        = opts['N']\n    max_iter = opts['T']\n    if 'w' in opts:\n        w    = opts['w']\n    if 'c1' in opts:\n        c1   = opts['c1']\n    if 'c2' in opts:\n        c2   = opts['c2'] \n    \n    # Dimension\n    dim = np.size(xtrain, 1)\n    if np.size(lb) == 1:\n        ub = ub * np.ones([1, dim], dtype='float')\n        lb = lb * np.ones([1, dim], dtype='float')\n        \n    # Initialize position & velocity\n    X             = init_position(lb, ub, N, dim)\n    V, Vmax, Vmin = init_velocity(lb, ub, N, dim) \n    \n    # Pre\n    fit   = np.zeros([N, 1], dtype='float')\n    Xgb   = np.zeros([1, dim], dtype='float')\n    fitG  = float('inf')\n    Xpb   = np.zeros([N, dim], dtype='float')\n    fitP  = float('inf') * np.ones([N, 1], dtype='float')\n    curve = np.zeros([1, max_iter], dtype='float') \n    t     = 0\n    \n    while t < max_iter:\n        # Binary conversion\n        Xbin = binary_conversion(X, thres, N, dim)\n        \n        # Fitness\n        for i in range(N):\n            fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n            if fit[i,0] < fitP[i,0]:\n                Xpb[i,:]  = X[i,:]\n                fitP[i,0] = fit[i,0]\n            if fitP[i,0] < fitG:\n                Xgb[0,:]  = Xpb[i,:]\n                fitG      = fitP[i,0]\n        \n        # Store result\n        curve[0,t] = fitG.copy()\n        print(\"Iteration:\", t + 1)\n        print(\"Best (PSO):\", curve[0,t])\n        t += 1\n        \n        for i in range(N):\n            for d in range(dim):\n                # Update velocity\n                r1     = rand()\n                r2     = rand()\n                V[i,d] = w * V[i,d] + c1 * r1 * (Xpb[i,d] - X[i,d]) + c2 * r2 * (Xgb[0,d] - X[i,d]) \n                # Boundary\n                V[i,d] = boundary(V[i,d], Vmin[0,d], Vmax[0,d])\n                # Update position\n                X[i,d] = X[i,d] + V[i,d]\n                # Boundary\n                X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n    \n                \n    # Best feature subset\n    Gbin       = binary_conversion(Xgb, thres, 1, dim) \n    Gbin       = Gbin.reshape(dim)\n    pos        = np.asarray(range(0, dim))    \n    sel_index  = pos[Gbin == 1]\n    num_feat   = len(sel_index)\n    # Create dictionary\n    pso_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n    \n    return pso_data","8d836054":"xtrain, xtest, ytrain, ytest = train_test_split(X_train, y_train, test_size=0.3, shuffle=True)\nfold = {'xt':xtrain, 'yt':ytrain, 'xv':xtest, 'yv':ytest}","556730c2":"c1  = 2         # cognitive factor\nc2  = 2         # social factor \nw   = 0.9       # inertia weight\nk     = 5     # k-value in KNN\nN     = 20    # number of population\nT     = 100   # maximum number of iterations\nopts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'w':w, 'c1':c1, 'c2':c2}","1c23a4c1":"# perform feature selection\nstart_time = time.time()\nfmdl  = jfs(X_train, y_train, opts)\nprint(\"Run Time --- %s seconds ---\" % (time.time() - start_time))\n\nsf    = fmdl['sf']\n\n# model with selected features\nnum_train = np.size(xtrain, 0)\nnum_valid = np.size(xtest, 0)\nx_train   = xtrain[:, sf]\ny_train   = ytrain.reshape(num_train)  # Solve bug\nx_valid   = xtest[:, sf]\ny_valid   = ytest.reshape(num_valid)  # Solve bug\n\nmdl       = LinearRegression()\nmdl.fit(x_train, y_train)\n\n# accuracy\ny_pred    = mdl.predict(x_valid)\nRMSE       = mean_squared_error(y_valid, y_pred, squared=False)\nprint(\"RMSE:\", RMSE)\n\n# number of selected features\nnum_feat = fmdl['nf']\nprint(\"Feature Size:\", num_feat)\n\n# plot convergence\ncurve   = fmdl['c']\ncurve   = curve.reshape(np.size(curve,1))\nx       = np.arange(0, opts['T'], 1.0) + 1.0\n\nfig, ax = plt.subplots()\nax.plot(x, curve, 'o-')\nax.set_xlabel('Number of Iterations')\nax.set_ylabel('Fitness')\nax.set_title('PSO')\nax.grid()\nplt.show()","68e7c20f":"fmdl['sf']","be5f4d13":"X_test = test_df.drop(['id'], axis=1).values\nX_test   = X_test[:, sf]\nsc = StandardScaler()\nX_test = sc.fit_transform(X_test)","2c3270d1":"y_test_pred = mdl.predict(X_test)\ny_test_pred","2634c034":"my_submission = pd.DataFrame({'id': test_df.id, 'loss': y_test_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","7661ea93":"my_submission.head()","b97f9025":"This varible shows the index of the selected features, \n\nwhich we can use it to select these columns in the test dataframe","7a2a3176":"### 4.3. The beginning of the PSO Algorithm\n\n* **Introduction**\n\nAt First, I'm going to give you a quik and basic introduction about this algorithm.\n\nParticle Swarm optimization is first attributed by Kennedy, Eberhar and Shi in their 1995 paper 'Particle Swarm Optimization'. It locates the minimum of a function by creating a number of 'particles'. These particles store their best position as well as also storing the global best position. It is this combination of local and global information that gives rise to 'swarm intelligence'.\n\nWithin an iteration, a particle will update it's position slightly towards both the swarm best and slightly towards it's personal best. With eventually the particles converging on (hopefully) the global minimum.\n\nMathematically this position update is defined as follows:\n\n$ v_i^{t + 1}=\\omega v_i^t + \\phi_br_b(x_{i_b}-x_i) + \\phi_gr_g(g_b-x_i) $\n\n$ x_i^{t + 1}=x_i^t + v_i^t $\n\nInitially every particle is given a random velocity *vi*, and the function is evaluated for every particle. Each particle is now 'aware' of it's previous best position as well as the global best position. On the first iteration it's previous best position is obviously it's current position so this term doesn't come into play until the second iteration.\n\nIt's current velocity is first scaled by a factor of w in order to ensure particle velocities don't grow exponentially over each iteration.\n\nThe term $ \\phi_br_b(x_{i_b}-x_i) $ then represents the vector from the particles position, towards it's previous best position. It is scaled by a constant $ \\phi_b $ (Here I've referred to this as c1) and a random value $ r_b $ between 0 and 1 (uniformly distributed). This random scaling provides the stochastic element of this optimization scheme.\n\nLikewise $ \\phi_gr_g(g_b-x_i) $ represents the vector from the particles position towards the swarms best position, again scaled by a constant $ \\phi_g $ and a random variable $ r_g $. Varying these phi (or c) parameters effect how locally or globally the particle explores the search space. A higher value will provide a larger vector and therefore the particle will take into account that aspect more than another.\n\nNow in the next part I'm going to explain how we can generalize this algorithm for selecting features.\n\n* **PSO Algorithm for Feature Selection**\n\nI will break down this implementation for better understanding.\n\nSo in this implementation we have 7 following functions:\n\n**1. Error Rate Function:**\n\nThis is a simple function for measuring the error for a subset of selected features.\nwe have four inputs: \n1. xtrain \n2. ytrain \n3. x: \n\nx is an array equal to number of features, this array have value of 0 and 1, 0 indicating that features are not selected based on the indexs and 1 indicating that features are selected. you can see an example of this array below:\n\nX: [1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1\n 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0\n 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0]\n \n 4. opts:\n is a dictionary contaning required variables and parameters which I will explain further.\n \nSo, in general in this function we are going to create xtrain and ytrain based on the subset of features (x) and then we going to train a model with this features and measure its error (for Regression and Classification) and accuracy (Classification). This competition is based on error so I measured the error.\n\n**2. Fun Function:**\n\nIn general this function get error based on the selected features using the error function and then it calculates the objective function.\n\n**3. init position Function:**\n\nAs I mentioned above every index of feature has the value of zero (0) or one (1) (selected or not selected). So, how do we decide which one to set 1 (as selected) and which to set 0 (as not selected)? randomly, Of course!  (Based on the definition of the Algorithm that I've given above)\n\nSo, in this function, we are going to build N (number of population) array with the size of the features, and then, we going to initialize it with a random value between 0-1.\n\nDon't worry as we move forward it makes more sense.\n\n**4. init velocity Function:**\n\nVelocity in the Particle Swarm Optimization algorithm (PSO) is one of its major features, as it is the mechanism used to move (evolve) the position of a particle to search for optimal solutions. ... This velocity regulation aims to achieve a balance between exploration and exploitation.\n\nSo, in this function we are going to initialize vlocity of each particle randomly between [-Vmax, Vmax]\nWe choose maximum and minimum velocity based on the our lower and upper bound which it is zero (0) or one (1) (selected or not selected) for selecting features.\n\n**5. binary conversion Function:**\n\nIf you remember in the init position function we randomly put value between 0-1 for each dimension of particles (N particles), as you can see the index of each feature should the value of zero or one, according to this we are going to convert these random values to 0 and 1 by seting a threshold (I set thresh to 0.5).\n\n**6. boundary Function:**\n\nIn this function we are going to check that every index value not be greater than our upper bound (which is 1) smaller than our lower bound (which is 0). (for index features)\nand the same function is also going to use for vlocity values (upper: Vmax, lower: Vmin)\n\n**7. jfs Function:**\n\nSo, lets put all of these together and run the algoritm using above functions.\nI've put comment in each line that you can see whats happening.","cc512bd4":"# 2. Importing essential libraries and csv files","ef609333":"[back to top](#table-of-contents)\n<a id=\"3.3\"><\/a>\n## 3.3 Submission\nThe submission file is expected to have an `id` and `loss` columns.\n\nBelow is the first 5 rows of submission file:","adf809b3":"## 6.1 Regression\nModel that will be evaluated is `Linear Regression`","09c99a29":"### 4.2. Scaling features to unit variance\n\nThen we are going to use StandardScaler which removes the mean and scales each feature\/variable to unit variance. \n\nThis operation is performed feature-wise in an independent way.\n\nThe idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.\n\nIn case of multivariate data, this is done feature-wise (in other words independently for each column of the data).\n\nGiven the distribution of the data, each value in the dataset will have the mean value subtracted, and then divided by the standard deviation of the whole dataset (or feature in the multivariate case).\n\nReferences and LearnMore about it:\n1. https:\/\/stackoverflow.com\/questions\/40758562\/can-anyone-explain-me-standardscaler\n2. https:\/\/towardsdatascience.com\/how-and-why-to-standardize-your-data-996926c2c832#:~:text=StandardScaler%20removes%20the%20mean%20and,standard%20deviation%20of%20each%20feature.","9196c1fe":"## 6.1.1 Linear Regression","cff424f9":"Then, we are going to check our data shape and see everything is OK!","38ae6345":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n## 4. Feature Selection using Particle Swarm Optimization","d2eb25f1":"### 3.1.2 Data types\nExcept for column `id`, `f1`, `f16`, `f27`, `f55`, `f60`, `f86` and `loss` column which are in `int64` type, other columns are in `float64`. *(to see the details, please expand)*","fba55ad9":"# 1. References \nThis notebook heavily uses concepts and implementation of:\n* https:\/\/github.com\/JingweiToo\/Wrapper-Feature-Selection-Toolbox-Python\n* https:\/\/www.kaggle.com\/dwin183287\/tps-august-2021-eda-base-model\n\nThanks for making such great implementations","30f0c0aa":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n# 3 Dataset Overview\nThe intend of the overview is to get a feel of the data and its structure in train, test and submission file. An overview on train and test datasets will include a quick analysis on missing values and basic statistics, while sample submission will be loaded to see the expected submission.\n\n<a id=\"3.1\"><\/a>\n## 3.1 Train dataset\nAs stated before, train dataset is mainly used to train predictive model as there is an available target variable in this set. This dataset is also used to explore more on the data itself including find a relation between each predictors and the target variable.\n\n**Observations:**\n- **target**\n    - `loss` column is the target variable which is only available in the `train` dataset.\n    - The interesting part with the `loss` column is in `int64` type. **Is this a classification task with a regression evaluation metrics?**\n- **features**\n    - There are `100` features which start from `f0` to `f99`.\n    - `train` dataset contain of `250,000` observations without any missing values with total of `102` columns.\n    - Only features `id`, `f1`, `f16`, `f27`, `f55`, `f60`, and `f86` are in `int64` type, other features are in `float64`.\n    - `f31`, `f36`, `f46`, `f78` mean are quite close with target variable mean.\n\n### 3.1.1 Quick view\nBelow is the first 5 rows of train dataset:","6079b8f7":"### 3.2.2 Data types\nExcept for column `id`, `f1`, `f16`, `f27`, `f55`, `f60`, `f86` and `loss` column which are in `int64` type, other columns are in `float64` which is consistent with the train dataset. *(to see the details, please expand)*","89310c96":"The dimension and number of missing values in the train dataset is as below:","66dc45d7":"[back to top](#table-of-contents)\n<a id=\"3.2\"><\/a>\n## 3.2 Test dataset\nTest dataset is used to make a prediction based on the model that has previously trained. Exploration in this dataset is also needed to see how the data is structured and especially on it\u2019s similiarity with the train dataset.\n\n**Observations:**\n\nFeatures column in `test` dataset are similar with `train` with details as follow:\n- There are `100` features which start from `f0` to `f99`.\n- `test` dataset contain of `150,000` observations without any missing values with total of `101` columns.\n- Only features `id`, `f1`, `f16`, `f27`, `f55`, `f60` and `f86` are in `int64` type, other features are in `float64`.\n\n### 3.2.1 Quick view\nBelow is the first 5 rows of test dataset:","e030ed5b":"### 3.1.3 Basic statistics\nBelow is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.","fd20040e":"Now, this is the interesting part of the story!!","4e43e6ef":"**Thanks for reading my notebook!**\n\n**I hope that you use it for your task and do great works!**\n\n**I've implemented this alogrithm not just for TPS competition you can use it in any task that you want (Image Processing, Signal Processing, ...).**\n\n**Your support gives me motivation to implement more enhanced algorithms (ISSA, ISCA, Jaya, ...) in various task.**","41da370e":"### 4.1. Creating an array of X (Features) and y (Targets)\n\n* At first we are going to put features values in to X_train by droping 'id' and 'loss' columnes.\n* Then, we put the loss values into 'y_train'"}}