{"cell_type":{"5c21e7e6":"code","412e4325":"code","5676dd24":"code","44d12f79":"code","1479c2fb":"code","380d36f5":"code","9bd2bb06":"code","2255aeb0":"code","f283ad81":"code","c9d57286":"code","29da6c0b":"code","cae47c9a":"code","90b70d87":"code","7ed0a87c":"code","e8482805":"code","e78bb41a":"code","c7d15c88":"code","a848bc02":"code","f3cfe1bd":"code","6e443990":"code","7d925bfa":"code","06dbb99c":"code","a5624a3f":"markdown","a028ef3a":"markdown","698444b6":"markdown","95d9e97f":"markdown","9c575a76":"markdown","36cf3a6a":"markdown","0459da27":"markdown","1f1d43f2":"markdown","20aee0ff":"markdown","8517396b":"markdown","9c8b1e2d":"markdown","06ac2983":"markdown","a9901c9b":"markdown","210f2744":"markdown","d9862f54":"markdown","f3d218ac":"markdown","be104d02":"markdown","c5c5c663":"markdown","3db8b54d":"markdown"},"source":{"5c21e7e6":"import os\nimport glob\nimport numpy as np\nfrom random import seed\nfrom random import randint\n\nfrom tqdm import tqdm\n# Fix seeds\nfrom numpy.random import seed\nseed(639)\n\nfrom scipy.signal import hilbert\nfrom scipy.signal import convolve\nfrom scipy.signal import hann\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n                              AdaBoostRegressor)\n#from sklearn.ensemble import AdaBoostRegressor\n\nfrom sklearn.ensemble import VotingRegressor\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\n\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.precision = 15","412e4325":"# Load training data\nINPATH = '..\/input\/LANL-Earthquake-Prediction\/'\ntrain_df = pd.read_csv(os.path.join( INPATH, 'train.csv'),\\\n                       dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n","5676dd24":"# sta\/lta function from obspy: https:\/\/docs.obspy.org\/_modules\/obspy\/signal\/trigger.html\ndef recursive_sta_lta(a, nsta, nlta):\n    '''\n    a: seismic trace\n    nsta: Length of short time average window in samples\n    nlta: Length of long time average window in samples\n    '''\n    try:\n        a = a.tolist()\n    except Exception:\n        pass\n    ndat = len(a)\n    # compute the short time average (STA) and long time average (LTA)\n    # given by Evans and Allen\n    csta = 1. \/ nsta\n    clta = 1. \/ nlta\n    sta = 0.\n    lta = 1e-99  # avoid zero division\n    charfct = [0.0] * len(a)\n    icsta = 1 - csta\n    iclta = 1 - clta\n    for i in range(1, ndat):\n        sq = a[i] ** 2\n        sta = csta * sq + icsta * sta\n        lta = clta * sq + iclta * lta\n        charfct[i] = sta \/ lta\n        if i < nlta:\n            charfct[i] = 0.\n    return np.array(charfct)","44d12f79":"def add_linear_trend(arr):\n    idx = np.array(range(len(arr)))\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]","1479c2fb":"#https:\/\/www.kaggle.com\/gpreda\/lanl-earthquake-eda-and-prediction\ndef create_features(seg_id, seg, X):\n    xc = pd.Series(seg['acoustic_data'].values)\n    zc = np.fft.fft(xc)\n    \n    X.loc[seg_id, 'mean'] = xc.mean()\n    X.loc[seg_id, 'std'] = xc.std()\n    X.loc[seg_id, 'max'] = xc.max()\n    X.loc[seg_id, 'min'] = xc.min()\n    \n    X.loc[seg_id, 'mad'] = xc.mad()\n    X.loc[seg_id, 'kurt'] = xc.kurtosis()\n    X.loc[seg_id, 'skew'] = xc.skew()\n    X.loc[seg_id, 'med'] = xc.median()\n    \n    #FFT transform values\n    realFFT = np.real(zc)\n    imagFFT = np.imag(zc)\n    X.loc[seg_id, 'Rmean'] = realFFT.mean()\n    X.loc[seg_id, 'Rstd'] = realFFT.std()\n    X.loc[seg_id, 'Rmax'] = realFFT.max()\n    X.loc[seg_id, 'Rmin'] = realFFT.min()\n    X.loc[seg_id, 'Imean'] = imagFFT.mean()\n    X.loc[seg_id, 'Istd'] = imagFFT.std()\n    X.loc[seg_id, 'Imax'] = imagFFT.max()\n    X.loc[seg_id, 'Imin'] = imagFFT.min()\n    \n    X.loc[seg_id, 'mean_change_abs'] = np.mean(np.diff(xc))\n       \n    X.loc[seg_id, 'abs_max'] = np.abs(xc).max()\n    X.loc[seg_id, 'abs_min'] = np.abs(xc).min()\n    X.loc[seg_id, 'abs_std'] = np.abs(xc).std()\n    X.loc[seg_id, 'abs_mean'] = np.abs(xc).mean()\n    \n    for value in [10000, 50000]:\n        #value = randint(10000, 150000)\n        X.loc[seg_id, 'std_first_%s' %value] = xc[:value].std()\n        X.loc[seg_id, 'std_last_%s' %value] = xc[-value:].std()\n        \n        X.loc[seg_id, 'mean_first_%s' %value] = xc[:value].mean()\n        X.loc[seg_id, 'mean_last_%s' %value] = xc[-value:].mean()\n       \n        X.loc[seg_id, 'min_first_%s' %value] = xc[:value].min()\n        X.loc[seg_id, 'min_last_%s' %value] = xc[-value:].min()\n        \n        X.loc[seg_id, 'max_first_%s' %value] = xc[:value].max()\n        X.loc[seg_id, 'max_last_%s' %value] = xc[-value:].max()\n        \n        #X.loc[seg_id, 'mean_change_rate_last_%s' %value] = np.mean(np.nonzero((np.diff(xc[-value:]) \/ xc[-value:][:-1]))[0])\n        #X.loc[seg_id, 'mean_change_rate_first_%s' %value] = np.mean(np.nonzero((np.diff(xc[:value]) \/ xc[:value][:-1]))[0])\n        \n    X.loc[seg_id, 'max_to_min'] = xc.max() \/ np.abs(xc.min())\n    X.loc[seg_id, 'max_min_diff'] = xc.max() - np.abs(xc.min())\n    X.loc[seg_id, 'sum'] = xc.sum()\n    X.loc[seg_id, 'mean_change_abs'] = np.mean(np.diff(xc))\n    \n    X.loc[seg_id, 'q95'] = np.quantile(xc, 0.95)\n    X.loc[seg_id, 'q80'] = np.quantile(xc, 0.80)\n    X.loc[seg_id, 'q05'] = np.quantile(xc, 0.05)\n    X.loc[seg_id, 'q20'] = np.quantile(xc, 0.20)\n    #interquartile rang\n    X.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(xc, [75, 25]))\n    X.loc[seg_id, 'q999'] = np.quantile(xc,0.999)\n    X.loc[seg_id, 'q001'] = np.quantile(xc,0.001)\n    \n    X.loc[seg_id, 'trend'] = add_linear_trend(xc)\n  # recursive_sta_lta\n    df = 1\/(np.sort(realFFT))\n    X.loc[seg_id, 'recursive_sta_lta1_mean'] = recursive_sta_lta(xc, 500, 10000).mean()\n    X.loc[seg_id, 'recursive_sta_lta2_mean'] = recursive_sta_lta(xc, 5000, 100000).mean()\n    #X.loc[seg_id, 'recursive_sta_lta3_mean'] = recursive_sta_lta(xc, df[0]*50, df[0]*100).mean()\n    #X.loc[seg_id, 'recursive_sta_lta4_mean'] = recursive_sta_lta(xc, df[1]*100, df[1]*200).mean()\n    \n    X.loc[seg_id, 'Hann_window_mean'] = (convolve(xc, hann(150), mode='same') \/ sum(hann(150))).mean()\n    X.loc[seg_id, 'Hilbert_mean'] = np.abs(hilbert(xc)).mean()\n    \n    # windowing\n    for winlen in [10, 100, 1000]:\n        x_rollwindow_std = xc.rolling(window=winlen, win_type='cosine').std().dropna().values\n        X.loc[seg_id, 'ave_roll_std_' + str(winlen)] = x_rollwindow_std.mean()    \n        X.loc[seg_id, 'std_roll_std_' + str(winlen)] = x_rollwindow_std.std()\n        X.loc[seg_id, 'max_roll_std_' + str(winlen)] = x_rollwindow_std.max()   \n        X.loc[seg_id, 'min_roll_std_' + str(winlen)] = x_rollwindow_std.min()    \n        X.loc[seg_id, 'q01_roll_std_' + str(winlen)] = np.quantile(x_rollwindow_std, 0.01)\n        X.loc[seg_id, 'q05_roll_std_' + str(winlen)] = np.quantile(x_rollwindow_std, 0.05)\n        X.loc[seg_id, 'q95_roll_std_' + str(winlen)] = np.quantile(x_rollwindow_std, 0.95)\n        X.loc[seg_id, 'q99_roll_std_' + str(winlen)] = np.quantile(x_rollwindow_std, 0.99)\n        X.loc[seg_id, 'ave_change_abs_roll_std_' + str(winlen)] = np.mean(np.diff(x_rollwindow_std))\n        \n        x_rollwindow_mean = xc.rolling(window=winlen, win_type='cosine').mean().dropna().values\n        X.loc[seg_id, 'ave_roll_mean_' + str(winlen)] = x_rollwindow_mean.mean()\n        X.loc[seg_id, 'std_roll_mean_' + str(winlen)] = x_rollwindow_mean.std()\n        X.loc[seg_id, 'max_roll_mean_' + str(winlen)] = x_rollwindow_mean.max()\n        X.loc[seg_id, 'min_roll_mean_' + str(winlen)] = x_rollwindow_mean.min()\n        X.loc[seg_id, 'q01_roll_mean_' + str(winlen)] = np.quantile(x_rollwindow_mean, 0.01)\n        X.loc[seg_id, 'q05_roll_mean_' + str(winlen)] = np.quantile(x_rollwindow_mean, 0.05)\n        X.loc[seg_id, 'q95_roll_mean_' + str(winlen)] = np.quantile(x_rollwindow_mean, 0.95)\n        X.loc[seg_id, 'q99_roll_mean_' + str(winlen)] = np.quantile(x_rollwindow_mean, 0.99)\n        X.loc[seg_id, 'ave_change_abs_roll_mean_' + str(winlen)] = np.mean(np.diff(x_rollwindow_mean))\n        \n    for winlen in [500, 1000]:\n        #winlen = randint(500, 10000)\n        X.loc[seg_id, 'Moving_average_%s_mean' %winlen] = xc.rolling(window=winlen).mean().mean(skipna=True)\n        X.loc[seg_id, 'MA_%s_std_mean' %winlen] = xc.rolling(window=winlen).std().mean()\n        X.loc[seg_id,'MA_%s_BB_high_mean' %winlen] = (X.loc[seg_id, 'Moving_average_%s_mean' %winlen] + \\\n                                                      2 * X.loc[seg_id, 'MA_%s_std_mean' %winlen]).mean()\n        X.loc[seg_id,'MA_%s_BB_low_mean' %winlen] = (X.loc[seg_id, 'Moving_average_%s_mean' %winlen] - \\\n                                                      2 * X.loc[seg_id, 'MA_%s_std_mean' %winlen]).mean()\n","380d36f5":"print('Train dataset has {} rows and {} columns.'.format(train_df.shape[0], train_df.shape[1]))\nprint(\"There are {} files in the test folder.\".format(len(os.listdir(os.path.join(INPATH, 'test' )))))\nprint('Each test segment has {} rows.'.format(\\\n    pd.read_csv(glob.glob(os.path.join(INPATH, 'test', '*.csv'))[0]).shape[0]))","9bd2bb06":"train_segments = int(np.round(train_df.shape[0] \/ 150000))\nprint(\"Number of segments: \", train_segments)\n\ntrain_X = pd.DataFrame(index=range(train_segments), dtype=np.float32)\ntrain_y = pd.DataFrame(index=range(train_segments), dtype=np.float32, columns=['time_to_failure'])","2255aeb0":"# Feature engineering\n# Split the train data into segments of the same dimension as the test files.\nseg_length = 150000\n# Iterate over all segments\nfor seg_id in tqdm(range(train_segments)):\n    seg = train_df.iloc[seg_id*seg_length:(seg_id+1)*seg_length]\n    create_features(seg_id, seg, train_X)\n    train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]","f283ad81":"# Scale the train data\nscaler = StandardScaler()\nscaler.fit(train_X)\nscaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)\n#scaled_train_X.head(4)\nscaled_train_X.info()","c9d57286":"np.any(np.isnan(train_X))\nfor col in train_y:\n    if train_y[col].isnull().values.any():\n        print(col)\n","29da6c0b":"# Check the shape of the submission file\nsubmission = pd.read_csv(os.path.join(INPATH,'sample_submission.csv'))\nsubmission.shape","cae47c9a":"X_test = pd.DataFrame(index=range(submission.shape[0]), dtype=np.float32)\n\n# Load files located in the test directory\ntest_files = glob.glob(os.path.join( INPATH, 'test', '*_*.*'))\n\n# Feature engineering for the test set\n# Iterate over all files in the test directory and\n# create the test dataframe with new features\nall_test_files = len(test_files)\n#all_test_files =600\nfor seg_id in tqdm(range(all_test_files)):\n    seg = pd.read_csv(test_files[seg_id])\n    create_features(seg_id, seg, X_test)\n    \n","90b70d87":"Y_train = train_y['time_to_failure'].values\n\nselection = list(scaled_train_X.columns)\nX_train = scaled_train_X[selection].values\n\nx_test = X_test[selection].values\n","7ed0a87c":"scaled_train_X[selection].describe()","e8482805":"#Build the Models\nrandom_state = 1\n\n# Step 1: create a list containing all estimators with their default parameters\nmodel_list = [LinearRegression(), Ridge(), Lasso(),\n          KNeighborsRegressor(), DecisionTreeRegressor(),\n          RandomForestRegressor(), GradientBoostingRegressor(), \n          AdaBoostRegressor()]\n\n\n# Step 2: calculate the cross-validation mean and standard deviation for the estimators\ncv_mean, cv_std = [], []\n\n \nfor mdl in model_list: \n    print('-----------------------------', mdl)\n    cv = cross_val_score(mdl, X_train, y = Y_train, scoring='neg_mean_squared_error', cv = 7, n_jobs = -1)\n    \n    cv_mean.append(abs(cv.mean()))\n    cv_std.append(cv.std())\n\n        \n# Step 3: create a dataframe and plot  means with error bars\ncv_total = pd.DataFrame({'Algorithm': ['Linear Regression',  'Ridge', 'Lasso Regression',\n         'K Neighbors Regressor', 'Decision Tree Regressor', \n         'Random Forest Regressor', 'Gradient Boosting Regressor',\n         'Adaboost Regressor'],\n                         'CV-Means': cv_mean, \n                         'CV-Errors': cv_std})\n\nsns.barplot(x='CV-Means', y='Algorithm', data = cv_total, palette = 'Set1', orient = 'h',\\\n            **{'xerr': cv_std})\nplt.xlabel('Mean Squared Error')\nplt.title('Cross Validation Scores')\n","e78bb41a":"cv_total","c7d15c88":"#Hyperparameter search for Ridge\nridge = Ridge()\nparam_grid = {'alpha': [0.05, 0.5, 0.7]\n             }\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)                                 \ngrid_ridge = GridSearchCV(ridge, param_grid = param_grid, cv=cv , scoring='neg_mean_squared_error', verbose = True, n_jobs = -1);\ngrid_ridge_result = grid_ridge.fit(X_train, Y_train);\n# summarize results for rf\nprint('-------------------------------------------\\n')\nprint(\"\\nBest Performance: MSE= %f using \\n%s\" % (grid_ridge_result.best_score_, grid_ridge_result.best_params_));\n\nmeans = grid_ridge_result.cv_results_['mean_test_score']\nstds = grid_ridge_result.cv_results_['std_test_score']\nparams = grid_ridge_result.cv_results_['params']\n#for mean, stdev, param in zip(means, stds, params):#\n#    print(\"%f (%f) with: %r\" % (mean, stdev, param))","a848bc02":"#Hyperparameter search for Randome Forest\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html\nrf = RandomForestRegressor()\nparam_grid = {'n_estimators': [100, 200],\n              'bootstrap': [True],\n              'max_depth': [3, 5],\n              #'max_features': ['auto','sqrt'],\n              'min_samples_leaf': [2, 3],\n              'min_samples_split': [2, 3]}\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=random_state)                                   \ngrid_rf = GridSearchCV(rf, param_grid = param_grid, cv = 7, scoring='neg_mean_squared_error', verbose = True, n_jobs = -1);\ngrid_rf_result = grid_rf.fit(X_train, Y_train);\n# summarize results for rf\nprint('-------------------------------------------\\n')\nprint(\"\\nBest Performance : MSE= %f using \\n%s\" % (grid_rf_result.best_score_, grid_rf_result.best_params_));\n","f3cfe1bd":"param_dist = {\n 'n_estimators': [50, 100],\n 'learning_rate' : [0.03,0.05,0.1,0.3],\n 'loss' : ['linear', 'square', 'exponential']\n }\n\n#cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=random_state)  \nrand_ada = RandomizedSearchCV(AdaBoostRegressor(), param_distributions = param_dist,\n                            cv=10, n_iter = 7, n_jobs=-1, scoring='neg_mean_squared_error')\nrand_ada_result = rand_ada.fit(X_train, Y_train)\n\nprint('-------------------------------------------\\n')\nprint(\"\\nBest Performance: MSE= %f using \\n%s\" % (rand_ada_result.best_score_, rand_ada_result.best_params_));\n","6e443990":"estimators = [('rf', grid_rf_result.best_estimator_),\\\n             ('Ridge', grid_ridge_result.best_estimator_),\n             ('AdaBoost', rand_ada_result.best_estimator_)]\n                \ntuned_voting = VotingRegressor(estimators = estimators, n_jobs = -1)\n\ntuned_voting.fit(X_train, Y_train)\n\n\ncv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=random_state)  \n\ncv_voting = cross_val_score(tuned_voting, X_train, Y_train, cv = 10, scoring='neg_mean_squared_error')\n\nprint ('Tuned Models - Ensemble\\n-----------------------')\nprint ('Voting: {}%'.format(np.round(cv_voting.mean(), 2)))\n","7d925bfa":"#y_pred_ensemble = tuned_voting.predict(x_test)\ny_pred_ridge = grid_ridge.predict(x_test)","06dbb99c":"submission.time_to_failure = y_pred_ridge\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head(4)","a5624a3f":"### 3.2. Process the Train Dataset\n","a028ef3a":"## 6.1. Submission","698444b6":"# 4. Evaluating Some Models\n<a id='sec4'><\/a>","95d9e97f":"# 3. Data Processing\n<a id='sec3'><\/a>","9c575a76":"### 3.3. Process the Test Dataset\n<a id='sec3.3'><\/a>","36cf3a6a":"# 7. Prediction\n<a id='sec7'><\/a>\nI'll use the Ridge model for the final prediction. ","0459da27":"# 1. Introduction\n<a id='sec1'><\/a>\nThis competition aims to use seismic signals to predict the timing of laboratory earthquakes. The data is from an experimental set-up. The acoustic_data input signal is used to predict the time remaining before the next laboratory earthquake (time_to_failure).<br>\nThe training data is a single continuous segment of experimental data. The test folder contains many small segments, which do not represent a continuous segment of the experiment. Predictions cannot be assumed to follow the same regular pattern seen in the training file.","1f1d43f2":"### 3.4 Data Selection","20aee0ff":"# 5. Hyperparameter Tuning\n<a id='sec5'><\/a>\nWe're optimising parameters for those models that have the highest accuracy.","8517396b":"<h1 align=\"center\"> LANL Earthquake Prediction<\/h1>","9c8b1e2d":"# Table of Contents\n[1. Introduction](#sec1) <br>\n[2. Import Libraries](#sec2)<br>\n[3. Data Processing](#sec3)<br>\n[4. Evaluating Some Models](#sec4)<br>\n[5. Hyperparameter Tuning](#sec5)<br>\n[6. Building Final Models](#sec6)<br>\n[7. Prediction](#sec7)\n","06ac2983":"### 3.1. Useful Functions\nWe'll use the following functions for data processing and feature engineering.","a9901c9b":"## References\n[Gabriel Preda](https:\/\/www.kaggle.com\/gpreda\/lanl-earthquake-eda-and-prediction)<br>\n[obspy](https:\/\/docs.obspy.org\/_modules\/obspy\/signal\/trigger.html)","210f2744":"<img style=\"margin: 0 auto;\" src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/LANL\/nik-shuliahin-585307-unsplash.jpg\" width=\"450\" height=\"200\" style=\"align:center\"\/>","d9862f54":"# 2. Import Libraries\n<a id='sec2'><\/a>","f3d218ac":"#### Random Forest Regressor","be104d02":"#### Ridge","c5c5c663":"#### AdaBoost Regressor","3db8b54d":"# 6. Building Final Models\n<a id='sec6'><\/a>"}}