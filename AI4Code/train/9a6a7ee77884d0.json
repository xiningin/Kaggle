{"cell_type":{"d6e7ecb6":"code","a81e9ca9":"code","587520c8":"code","61adb877":"code","09cebea1":"code","9f030733":"code","36ae2d7c":"code","57699d31":"code","c68f68f3":"code","1f75326d":"code","0da2b3db":"code","d3e27e7c":"code","bd3ffa61":"code","7e46f0e2":"code","b6e3b9cd":"code","167a36bb":"code","4cfa5a05":"code","7a008d44":"code","7913fe56":"code","87687910":"code","2f389561":"code","4a05976e":"code","f4af8bb9":"code","f57e6bc9":"code","5d905c4a":"code","fb21a333":"code","7009a8d0":"code","5d0635b0":"code","c35b610b":"code","cb9a1048":"code","984cb691":"code","4e86ccb1":"code","1d150a3e":"code","61c9a746":"code","97c82eb9":"code","dafbd53f":"code","94cd7c44":"code","713e01af":"code","5ed3a941":"code","9593279d":"code","b8b0a2f5":"code","f56c73bc":"code","681d109a":"code","3a635f69":"code","71651a58":"code","b0110700":"code","57e7092d":"code","b1f21478":"code","6a00512d":"code","756aae05":"code","b96819cb":"code","ef46e597":"code","ccf89676":"code","2101cc19":"code","db6d4b5d":"code","d903699f":"code","f0e69284":"code","56626895":"code","16b442d4":"code","f4398710":"code","1585ad64":"code","98ab13bf":"code","500195af":"code","3da23402":"code","62b40de0":"code","df759bbd":"code","5cb14d85":"code","1e60e121":"code","b8c63d7d":"code","0ee1cf33":"code","e9962413":"code","3f69816a":"code","65bb0229":"code","9b607f61":"code","f569e183":"code","35e7acac":"code","641543a3":"code","101fcd97":"code","c4fe8e8d":"code","dd50334a":"code","19e332ef":"markdown","02cf9c5d":"markdown","ccb0360c":"markdown","a360f628":"markdown","370cefc8":"markdown","0c8e1ad0":"markdown","ad36034f":"markdown","703a5a14":"markdown","328dc7d3":"markdown","ca8c35c7":"markdown","3cfea5d0":"markdown","46f0cac5":"markdown","851fb903":"markdown","9095b1c9":"markdown","373a8740":"markdown","72c85f3a":"markdown","66be7342":"markdown","311fb7b6":"markdown","22afb593":"markdown","fe156367":"markdown","51533ce9":"markdown","1d9f3b45":"markdown","b3ed85c8":"markdown","7228b8cd":"markdown","e2ef5065":"markdown","42b986dd":"markdown","a01ace0e":"markdown","23436cbd":"markdown","e8866ba3":"markdown","05f05f03":"markdown","7655f822":"markdown","2b53fd39":"markdown","bdcf1508":"markdown","591f4dc1":"markdown","fde5b0a0":"markdown","7315b9ef":"markdown","e5b8ef5e":"markdown","d72ace47":"markdown","64225694":"markdown","f9052d2e":"markdown","3686f294":"markdown","27bcd328":"markdown","9cb8a8c3":"markdown","3484820f":"markdown","c912bf76":"markdown","d4592504":"markdown","24a610d5":"markdown","e5860cc7":"markdown","9e263352":"markdown","4094de74":"markdown","1c83e8c4":"markdown","cb2f9f6b":"markdown","2914178a":"markdown","8d5ff235":"markdown","6973440c":"markdown","5263ed16":"markdown","6acd08e7":"markdown","ced57619":"markdown","ae80e548":"markdown","6729695f":"markdown","ac203d54":"markdown","f3897a9a":"markdown","c8ff26d7":"markdown","ea7bf47e":"markdown","3299b7aa":"markdown","329f62f0":"markdown","b2f7e4ce":"markdown","617a98b4":"markdown","c81e1e21":"markdown","3eeafb99":"markdown","4a4e8594":"markdown","855f7a77":"markdown","27d1d8f4":"markdown","5c51f2cb":"markdown","8c22e915":"markdown","48ea55d2":"markdown","9df61d9c":"markdown","22839167":"markdown","4cea74ae":"markdown","a196c587":"markdown","3caad0d6":"markdown","a8aad4c8":"markdown"},"source":{"d6e7ecb6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","a81e9ca9":"df = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')","587520c8":"df.head()","61adb877":"df.info()","09cebea1":"df.describe()","9f030733":"df['class'].value_counts()","36ae2d7c":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\nfor c in df.columns:\n    df[c]=label.fit_transform(df[c])","57699d31":"df.head()","c68f68f3":"x = df.drop('class', axis=1)\ny = df['class']","1f75326d":"x = pd.get_dummies(x,columns=x.columns ,drop_first=True)","0da2b3db":"x.head()","d3e27e7c":"corr = []\nfor i in range(x.shape[1]):\n    c = np.corrcoef(x.iloc[:,i],y)\n    corr.append(abs(c[0][1]))","bd3ffa61":"corr_data = pd.DataFrame({'correlation': corr}, index=x.columns)","7e46f0e2":"corr_data","b6e3b9cd":"plt.figure(figsize=(20,9))\nsns.barplot(x=corr_data.index, y = corr_data['correlation'])\nplt.xticks(rotation=90)","167a36bb":"corr_data = corr_data.sort_values(by = 'correlation', ascending=False)","4cfa5a05":"corr_imp = corr_data[corr_data['correlation'] >= 0.5]","7a008d44":"corr_imp","7913fe56":"corr_X = x[corr_imp.index]","87687910":"corr_X","2f389561":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(corr_X, y, test_size=0.33, random_state=42)","4a05976e":"from sklearn.linear_model import LogisticRegression","f4af8bb9":"classifier = LogisticRegression(n_jobs=-1)\nclassifier.fit(X_train, y_train)","f57e6bc9":"predictions1 = classifier.predict(X_test)","5d905c4a":"from sklearn.metrics import classification_report\nprint(classification_report(predictions1,y_test))","fb21a333":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(predictions1,y_test))","7009a8d0":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","5d0635b0":"X_indices = np.arange(x.shape[-1])","c35b610b":"from sklearn.feature_selection import SelectKBest, chi2","cb9a1048":"selector = SelectKBest(chi2, k=20)\nselector.fit(X_train, y_train)\nscores = selector.scores_\/1000\n\nplt.figure(figsize=(50,10))\nsns.barplot(data=pd.DataFrame({'Feature':x.columns, 'Scores': scores}),x='Feature',y='Scores',ci=None)\nplt.xticks(rotation=90)","984cb691":"scores_data = pd.DataFrame({'Feature':x.columns, 'Scores': scores})","4e86ccb1":"plt.figure(figsize=(12,8))\nsns.distplot(scores_data['Scores'])","1d150a3e":"scores_data = scores_data.sort_values(by = 'Scores',ascending=False)","61c9a746":"scores_x = scores_data.head(20)","97c82eb9":"scores_x = x[scores_x['Feature']]","dafbd53f":"scores_x","94cd7c44":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(scores_x, y, test_size=0.33, random_state=42)","713e01af":"from sklearn.linear_model import LogisticRegression","5ed3a941":"classifier = LogisticRegression(n_jobs=-1)\nclassifier.fit(X_train, y_train)","9593279d":"predictions = classifier.predict(X_test)","b8b0a2f5":"from sklearn.metrics import classification_report\nprint(classification_report(predictions,y_test))","f56c73bc":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(predictions,y_test))","681d109a":"from sklearn.feature_selection import RFE","3a635f69":"estimator = LogisticRegression(n_jobs=-1)","71651a58":"d = {}\nfor k in range(2, 25,2):  \n    selector = RFE(estimator, n_features_to_select=k, step=2)\n    selector = selector.fit(x, y)\n    selector.support_\n    selector.ranking_\n\n    sel_fea  = [i for i,j in zip(x.columns,selector.ranking_) if j==1]\n\n    x_new = x[sel_fea]\n\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(x_new, y, test_size=0.33, random_state=42)\n    from sklearn.linear_model import LogisticRegression\n    classifier = LogisticRegression(n_jobs=-1)\n    classifier.fit(X_train, y_train)\n    y_pred1 = classifier.predict(X_test)\n\n    from sklearn.metrics import accuracy_score\n    acc = accuracy_score(y_pred1,y_test)\n    print(\"features: %s\"%k, \" Accuracy: %f\"%acc)\n    d[str(k)]=acc","b0110700":"selector = RFE(estimator, n_features_to_select=20, step=2)\nselector = selector.fit(x, y)\nselector.support_\nselector.ranking_\n","57e7092d":"sel_fea  = [i for i,j in zip(x.columns,selector.ranking_) if j==1]","b1f21478":"x_new = x[sel_fea]","6a00512d":"x_new","756aae05":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x_new, y, test_size=0.33, random_state=42)","b96819cb":"from sklearn.linear_model import LogisticRegression","ef46e597":"classifier = LogisticRegression(n_jobs=-1)\nclassifier.fit(X_train, y_train)","ccf89676":"y_pred1 = classifier.predict(X_test)","2101cc19":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_pred1,y_test))","db6d4b5d":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_pred1,y_test))","d903699f":"from sklearn.metrics import accuracy_score\nprint('correlation :')\nprint(accuracy_score(predictions1,y_test))\nprint('selectKBest :')\nprint(accuracy_score(predictions,y_test))\nprint('RFE :' )\nprint(accuracy_score(y_pred1,y_test))","f0e69284":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x_new, y, test_size=0.33, random_state=42)","56626895":"from sklearn.linear_model import LogisticRegression","16b442d4":"classifier = LogisticRegression(n_jobs=-1)","f4398710":"classifier.fit(X_train, y_train)","1585ad64":"y_pred1 = classifier.predict(X_test)","98ab13bf":"from sklearn.metrics import classification_report\nprint(classification_report(y_pred1,y_test))","500195af":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_pred1,y_test))","3da23402":"from sklearn.neighbors import KNeighborsClassifier","62b40de0":"classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\nclassifier.fit(X_train,y_train)","df759bbd":"y_pred2 = classifier.predict(X_test)","5cb14d85":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_pred2))\nprint(confusion_matrix(y_test,y_pred2))","1e60e121":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train,y_train)","b8c63d7d":"y_pred3 = classifier.predict(X_test)","0ee1cf33":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_pred3))\nprint(confusion_matrix(y_test,y_pred3))","e9962413":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)","3f69816a":"y_pred4 = classifier.predict(X_test)","65bb0229":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_pred4))\nprint(confusion_matrix(y_test,y_pred4))","9b607f61":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=10,criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)","f569e183":"y_pred5 = classifier.predict(X_test)","35e7acac":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_pred5))\nprint(confusion_matrix(y_test,y_pred5))","641543a3":"df = pd.DataFrame({'y_test': y_test,'logistic_reg': y_pred1, 'KNN': y_pred2, 'Naive_Bayes': y_pred3\n                  , 'Decision Tree': y_pred4, 'Random Forest': y_pred5})","101fcd97":"df","c4fe8e8d":"from sklearn.metrics import accuracy_score","dd50334a":"for i in df.columns[1:]:\n    print(i+': ',accuracy_score(df['y_test'], df[i]))","19e332ef":"### Importing RFE","02cf9c5d":"## 3. Naive Bayes","ccb0360c":"### Making classification report and confusion matrix","a360f628":"### Importing Logistic Regression","370cefc8":"### Predicting the test set results","0c8e1ad0":"### Splitting the Dataset into X_train, X_test, y_train and y_test","ad36034f":"Now, we have 95 features, let's try some feature selection techniques to extract useful features.","703a5a14":"As we can see above classes are not imbalanced. To use such discrete features we'll first encode these to natural numbers using LabelEncoder then One-Hot-Encoding will be applied.","328dc7d3":"### Making classification report and confusion matrix","ca8c35c7":"## 3. Recurssive feature elimination (RFE)","3cfea5d0":"### Making Classification report and Confusion Matrix","46f0cac5":"### Separating dependent and independent variables","851fb903":"## 5. Random Forest Classification","9095b1c9":"## 1 = p , 0 = e","373a8740":"### Importing KNN Classifier","72c85f3a":"# Applying 5 classification models","66be7342":"## Accuracy scores of Logistic Regression with above mentioned feature selection techniques.","311fb7b6":"## 2. Univariate feature selection","22afb593":"### Fitting the Random Forest Classification","fe156367":"### Fitting Logistic Regression Model","51533ce9":"### Fitting Logistic Regression Model","1d9f3b45":"### Calculating Accuracy Scores of above mentioned Classification models.","b3ed85c8":"### One Hot Encoding","7228b8cd":"## 2. KNN Classifier","e2ef5065":"### Fitting Logistic Regression Model","42b986dd":"## Applying Logistic Regression Model (Independent variable = corr_x)","a01ace0e":"## 1. Logistic Regression","23436cbd":"### Predicting the test set results","e8866ba3":"## 1. Selecting features with highest correlation with independent variable (y)","05f05f03":"### Splitting the Dataset into X_train, X_test, y_train and y_test","7655f822":"### Label Encoding","2b53fd39":"### Getting information of the data","bdcf1508":"### Making Predictions","591f4dc1":"### Splitting the Dataset into X_train, X_test, y_train and y_test","fde5b0a0":"### Read dataset","7315b9ef":"### Importing Logistic Regression","e5b8ef5e":"### Applying RFE with 20 features","d72ace47":"### Making DataFrame of correaltion values","64225694":"### Making Predictions","f9052d2e":"### Defining new DataFrame of selected independent variables (x)","3686f294":"### Importing Logistic Regression","27bcd328":"### Making Predictions","9cb8a8c3":"Hi, I'm going to apply 5 supervised machine learning classification models on the given dataset to classify mushrooms as poisonous or edible.\n1. Logistic Regression\n2. K-Nearest Neighbours(K-NN)\n3. Naive Bayes classifier\n4. Decision Tree Classifier\n5. Random Forest Classifier","3484820f":"# Applying Feature Selection Techniques","c912bf76":"### Choosing features with correlation values greater than 0.5.","d4592504":"### Visualizing scores ","24a610d5":"### Making Classification report and Confusion Matrix","e5860cc7":"### Making Predictions","9e263352":"### Making classification report and confusion matrix","4094de74":"### Defining new DataFrame of selected independent variables (x)","1c83e8c4":"### Fitting KNN Classifier","cb2f9f6b":"### Checking whether the data is equally distributed between poisonous (p) and edible (e)\n","2914178a":"### Getting Accuracy score and confusion matrix","8d5ff235":"### Visualization of corr DataFrame","6973440c":"### Predicting the test set results","5263ed16":"## 4. Decision Tree classification","6acd08e7":"### So, we conclude that on this dataset RFE performed best among different feature selection techniques and successfully reduced number of variables without hampering accuracy.\nIf you like my work, an upvote will motivate me to persue this never ending ML\/Data Science journey.\nI am new to this field, if you feel I made some mistakes or have any suggestions please comment. I trust this community will help me to hone my skills.","ced57619":"### Fitting the Decision Tree classification","ae80e548":"### Making DataFrame of all the predictions made by 5 models with recpect to actual target value (y_test)","6729695f":"This dataset contains discrete values for each variable. So, standardization\/normalization should not be applied on this.","ac203d54":"### Predicting the test set results","f3897a9a":"### Selecting 20 Scores with highest value","c8ff26d7":"## Applying Logistic Regression Model (Independent variable = scores_x)","ea7bf47e":"### Appying SelectKBest with k = 20","3299b7aa":"### Splitting the Dataset into X_train, X_test, y_train and y_test","329f62f0":"### Making classification report and confusion matrix","b2f7e4ce":"## Let's check how many features to preserve with RFE","617a98b4":"### Fitting Logistic Regression Model","c81e1e21":"### Making list of correlation values","3eeafb99":"### Importing Logistic Regression","4a4e8594":"### Making Classification report and Confusion Matrix","855f7a77":"From the above graph we can conclude that there are only a few number of features which have higher correlation than most of the features with respect to target feature.","27d1d8f4":"### Importing SelectKBest method ","5c51f2cb":"### Importing Libraries","8c22e915":"I'll proceed by converting categorical variables into dummy\/indicator variables, then applying 3 feature selection techniques to reduce 23 categorical variables (which will become 95 variables after conversion to dummy variables) to only 20 variables and choose the best feature elemination technique for given dataset. Then training different classification models over these 20 features. Here  the goal is to choose best feature selection technique for such datasets with optimum accuracy.\n","48ea55d2":"### Splitting the Dataset into X_train, X_test, y_train and y_test","9df61d9c":"### Fitting the naive bayes model","22839167":"### Describing the data","4cea74ae":"## Applying Logistic Regression Model (Independent variable = scores_x)","a196c587":"## Encoding Categorical Data","3caad0d6":"### Class is dependent variable and rest are independent variables","a8aad4c8":"### Defining new DataFrame of selected independent variables (x)"}}