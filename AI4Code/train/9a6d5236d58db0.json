{"cell_type":{"18d37b2a":"code","f314c568":"code","d8295774":"code","9cbaa74d":"code","8c8cc813":"code","d997de29":"code","3f37f104":"code","9939865d":"code","415e682a":"code","79b7aeae":"code","7b3edbf7":"code","04d30d4c":"code","94a6df38":"code","9982faf4":"markdown","9d7ca317":"markdown","bda3e1ef":"markdown","40844811":"markdown","23c3d18e":"markdown","20c07b0b":"markdown","9cca7486":"markdown","49362093":"markdown","bb9b8422":"markdown","fa8bcfbf":"markdown","cc117f2b":"markdown"},"source":{"18d37b2a":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f314c568":"train_data = pd.read_csv(\"..\/input\/summeranalytics2020\/train.csv\",index_col= \"Id\")\ntest_data = pd.read_csv(\"..\/input\/summeranalytics2020\/test.csv\",index_col= \"Id\")\n\nX = train_data.drop(['Attrition'],axis='columns').copy()\ny = train_data[\"Attrition\"].copy()\nX_test = test_data.copy()","d8295774":"X.head()","9cbaa74d":"from sklearn.model_selection import train_test_split\n\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.3,random_state=0)","8c8cc813":"# Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical columns:\")\nprint(object_cols)\n\n# Get list of numerical variables\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\nprint(\"\\nNumerical columns:\")\nprint((numerical_cols))","d997de29":"X_train.nunique()","3f37f104":"X_train.drop(['Behaviour'],inplace=True,axis='columns')\nX_valid.drop(['Behaviour'],inplace=True,axis='columns')\nX_test.drop(['Behaviour'],inplace=True,axis='columns')\n\nnumerical_cols.remove(\"Behaviour\")\nprint(\"Numerical Columns:\")\nprint(numerical_cols)","9939865d":"import seaborn as sns\nfrom matplotlib import pyplot as plt \n%matplotlib inline\n\n#all numerical columns except performance rating because there was some error in producing it graph\n\ncols = ['Age', 'DistanceFromHome', 'Education', 'EmployeeNumber', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobSatisfaction', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'CommunicationSkill']\n\nfor col in cols:\n    fig,ax = plt.subplots(1,3)\n    sns.distplot(X_train[col],ax=ax[0])\n    sns.distplot(np.sqrt(X_train[col]),ax=ax[1])\n    sns.distplot(np.log(X_train[col]+1),ax=ax[2])","415e682a":"sqrt_col = ['DistanceFromHome','TotalWorkingYears','NumCompaniesWorked','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']\nlog_col = ['MonthlyIncome','PercentSalaryHike']\n\nX_train[sqrt_col] = np.sqrt(X_train[sqrt_col])\nX_train[log_col] = np.log(np.log(X_train[log_col]))\n\nX_valid[sqrt_col] = np.sqrt(X_valid[sqrt_col])\nX_valid[log_col] = np.log(np.log(X_valid[log_col]))\n\nX_test[sqrt_col] = np.sqrt(X_test[sqrt_col])\nX_test[log_col] = np.log(np.log(X_test[log_col]))","79b7aeae":"cols = ['Age', 'DistanceFromHome', 'Education', 'EmployeeNumber', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobSatisfaction', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'CommunicationSkill']\n\nfor col in cols:\n    fig,ax = plt.subplots(1,2)\n    sns.distplot(X_train[col],ax=ax[0])\n    sns.distplot(X_test[col],ax=ax[1])","7b3edbf7":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.naive_bayes import BernoulliNB,GaussianNB\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Preprocessing for numerical data\nnumerical_transformer = StandardScaler()\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore',sparse=False)),\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, object_cols),\n    ])\n\nmodel1 = LogisticRegression(random_state=0)\nlrm = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model',model1)\n])\ngrid_val1 = [\n    {'model__penalty':['l2'],'model__tol':[0.001],'model__C':[1],'model__solver':['liblinear','saga','newton-cg','lbfgs'],'model__max_iter':[100]},\n    {'model__penalty':['l1'],'model__tol':[0.001],'model__C':[1],'model__solver':['liblinear','saga'],'model__max_iter':[100]},\n    {'model__penalty':['elasticnet'],'model__tol':[0.001],'model__C':[0.1],'model__solver':['saga'],'model__max_iter':[100],'model__l1_ratio':[0.25,0.5,0.75]},\n]\n\n'''\nmodel2 = DecisionTreeClassifier(random_state=0)\ndtm = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model',model2)\n])\ngrid_val2 = [\n    {'model__criterion':['gini','entropy'],'model__max_depth':[10],'model__max_features':['sqrt','log2','none']}\n]\n\nmodel3 = RandomForestClassifier(random_state=0)\nrfcm = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model',model3)\n])\ngrid_val3 = [\n    {'model__criterion':['gini'],'model__max_depth':[6],'model__n_estimators':[20]}\n]\n\nmodel4 = SVC(random_state=0,probability=True)\nsvcm = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model',model4)\n])\ngrid_val4 = [\n    {'model__C':[0.1],'model__kernel':['poly'],'model__degree':[2],'model__gamma':[0.1],'model__tol':[0.01]}\n]\n\nmodel5 = XGBClassifier(random_state=0)\nxgbcm = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model',model5)\n])\ngrid_val5 = [\n    {'model__max_depth':[5],'model__objective':['binary:logistic'],'model__eval_metric':['auc'],'model__booster':['gblinear'],'model__learning_rate':[0.2]}\n]\n\nmodel6 = GaussianNB()\nnbcm = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model',model6)\n])\ngrid_val6 = [\n    {'model__var_smoothing':[1e-09,1e-10,1e-08]}\n]\n\nmodel7 = CalibratedClassifierCV(LinearSVC(max_iter=1000000,random_state=0,loss='hinge'))\nlsvcm = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model',model7)\n])\ngrid_val7 =[\n    {'model__method':['sigmoid']}\n]\n'''\n\nfm = GridSearchCV(lrm,param_grid = grid_val1,scoring = 'roc_auc')\n\nfm.fit(X_train,y_train)\n\nprint(fm.best_params_)\n\ny_valid_pred = fm.predict_proba(X_valid)[:,1]\ny_train_pred = fm.predict_proba(X_train)[:,1]\n\nprint(roc_auc_score(y_train,y_train_pred))\nprint(roc_auc_score(y_valid,y_valid_pred))\n\ny_test_pred = fm.predict_proba(X_test)[:,1]","04d30d4c":"output = pd.DataFrame({'Id': X_test.index,\n                       'Attrition': y_test_pred})\noutput.to_csv('submission.csv',index=False)","94a6df38":"#Checking if the output is in the desired format\n\noutput.head()","9982faf4":"**Importing files**\n\nAdding the input data for the comeptetition and importing the most neccesary libraries.","9d7ca317":"Depending upon the skewness of the data and a few iterations a found that the following transformations are best suited.\n\nSquare root transformation for ['DistanceFromHome','TotalWorkingYears','NumCompaniesWorked','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']\n\nLog transformation for ['MonthlyIncome','PercentSalaryHike']","bda3e1ef":"Hence it is obvious that \"Behaviour\" is redundant column and so removing it. The copy warnings can be ignored.","40844811":"Checking distribution of both the train(1st column) and test(2nd column) data and found them almost same.","23c3d18e":"**Spliting the data to create validation set**\n\nTest size is suggested to be between 0.2-0.3 .","20c07b0b":"**Employee Attrition Submission**\n\nFirstly thanking Consulting and Analytics Club IITG for organising the Summer Analytics Course and this Kaggle competition. This being my first completely attempted Kaggle competition was really insightful and tought me a lot. I have tried to explain my notebook at my best and tried to keep the code simple and clear. I am open to any suggestions you have and would love to learn from you.","9cca7486":"**Loading Data**\n\nNote: Your current directory always remains \"\/kaggle\/working\" and can be checked by using !pwd command, and so the paths to input files are written accordingly.","49362093":"**Visualization of Data**\n\nSince there were no missing values in the data now checking for the skewness of the data to apply appropriate transformations. \n\nFirst column-No transformations, Second column-Sqrt Transformation, Third column-Log Transformation ","bb9b8422":"**Defining models and Data preprocessing**\n\nTo make the data clean and clear Pipeline is used.\nMultiple models are defined with multiple grids appropriately and GridSearchCV is used for hyperparameter tuning.\nAfter testing multiple models, LogisticRegression seem to fit great so used it as final model.","fa8bcfbf":"**Creating the output file**","cc117f2b":"**Checking for redundant columns**"}}