{"cell_type":{"ca46d365":"code","27896225":"code","546c8468":"code","acad9c60":"code","3a8b5021":"code","7064c9cc":"code","80aa356d":"code","8129a156":"code","1010d134":"code","1b39911e":"code","a307ac80":"code","20699de1":"code","34cbb1da":"code","2466d92b":"code","a0e40e90":"code","dbcea86a":"code","7f6a3a95":"code","164c612b":"code","34c10ce4":"code","10c04a0c":"code","c66a3fce":"code","b507c9d9":"code","79a381d2":"code","fb137ff3":"code","8b8c93e7":"code","3cb9bb1a":"code","836f6f3c":"code","0985e1ed":"code","c7a19bc5":"code","b10a7168":"code","916b29ab":"code","1e80ead7":"code","f424e40a":"code","bd163526":"code","3d6b0dc1":"code","7757a66f":"code","a0c2ab5a":"code","bca446df":"code","c2c9110e":"code","351dff0d":"code","900c1686":"code","511ccf60":"code","bb865a78":"code","f1ec7d56":"code","14f449e1":"code","27238ee7":"code","8da9dba5":"code","b7310381":"code","2bab45c3":"code","d3ad132f":"code","bec42d98":"code","eed004c4":"code","6b031445":"code","ad86f318":"code","a789d735":"code","bbd09679":"code","19dfe631":"code","b0ce73db":"code","16efb585":"code","0062ffde":"code","e9706307":"code","14e87bba":"markdown","697250f6":"markdown","7e3072bc":"markdown","caefc48f":"markdown","bf7f6ed4":"markdown","636be1d2":"markdown","280b0cd7":"markdown","3649c2bf":"markdown","cd61d71f":"markdown","89add335":"markdown","470abab6":"markdown","dd1ed3a1":"markdown","06d9122b":"markdown","7010ae95":"markdown","d364aee3":"markdown","5a9dafd6":"markdown","c2d5103f":"markdown","6e771429":"markdown","b7d809ac":"markdown","f91cb914":"markdown","7f45a458":"markdown","0d379664":"markdown","561a645d":"markdown","0a283527":"markdown","26feb021":"markdown","a5aace89":"markdown","cfad33f5":"markdown","c6b0df63":"markdown","81b38d66":"markdown","edec494a":"markdown","6e6a0597":"markdown","8d66ec44":"markdown","65b88076":"markdown","d906dc97":"markdown","10c9dfe6":"markdown","70907716":"markdown","5ff24e72":"markdown","7c889a23":"markdown","1a9807b6":"markdown","ae58b997":"markdown","c972d2ca":"markdown","1c170ba6":"markdown","dffe57fb":"markdown","e7ff7d0c":"markdown","aa66d26d":"markdown","c4345563":"markdown","cfbb454f":"markdown","9e61bd06":"markdown","bcf96612":"markdown","95e4fc36":"markdown","665fdc94":"markdown","8b14aafb":"markdown","9e76b6d2":"markdown","286c2eb2":"markdown","2ea01997":"markdown","ddcd63b4":"markdown","5f3c8d63":"markdown","53aaee07":"markdown","b7b53d38":"markdown","d45090ad":"markdown","8d8c537f":"markdown","3832b893":"markdown","51c10784":"markdown","86ee5131":"markdown","b8ac3c88":"markdown","0634f559":"markdown","82ab12bc":"markdown","f0635f1b":"markdown","455bdffb":"markdown","7cb577b7":"markdown"},"source":{"ca46d365":"# Import Library\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image","27896225":"# Reading the data file\ndata= pd.read_csv('..\/input\/tvradionewspaperadvertising\/Advertising.csv') ","546c8468":"data.head() # Display first five rows from the dataset","acad9c60":"data.shape","3a8b5021":"data.info() #summary of the dataframe","7064c9cc":"data.isna().sum() # finding the count of missing values","80aa356d":"# visualize the relationship between the features and the response using scatterplots\nfig, axs = plt.subplots(1, 3, sharey=True)\ndata.plot(kind='scatter', x='TV', y='Sales', ax=axs[0], figsize=(10, 8))\ndata.plot(kind='scatter', x='Radio', y='Sales', ax=axs[1])\ndata.plot(kind='scatter', x='Newspaper', y='Sales', ax=axs[2])","8129a156":"# create X and y\nfeature_cols = ['TV']\nX = data[feature_cols]\ny = data.Sales\n\n# follow the usual sklearn pattern: import, instantiate, fit\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X, y)\n\n# print intercept and coefficients\nprint(lm.intercept_)\nprint(lm.coef_)","1010d134":"#calculate the prediction\n6.974821 + 0.0554647*50","1b39911e":"#  Let's create a DataFrame since the model expects it\nX_new = pd.DataFrame({'TV': [50]})\nX_new.head()","a307ac80":"# use the model to make predictions on a new value\nlm.predict(X_new)","20699de1":"# create a DataFrame with the minimum and maximum values of TV\nX_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})\nX_new.head()","34cbb1da":"# make predictions for those x values and store them\npreds = lm.predict(X_new)\npreds","2466d92b":"# first, plot the observed data\ndata.plot(kind='scatter', x='TV', y='Sales')\n\n# then, plot the least squares line\nplt.plot(X_new, preds, c='red', linewidth=2)","a0e40e90":"#constant is automatically added to your data and intercept id fitted whereas in statsmodelsapi we have to add constant\nimport statsmodels.formula.api as smf\nlm = smf.ols(formula='Sales ~ TV', data=data).fit()\nlm.conf_int()\n\n#a = lm.summary()\n#print(a)","dbcea86a":"# print the p-values for the model coefficients\nlm.pvalues","7f6a3a95":"# print the R-squared value for the model\nlm.rsquared","164c612b":"# create X and y\nfeature_cols = ['TV', 'Radio', 'Newspaper']\nX = data[feature_cols]\ny = data.Sales\n\nlm = LinearRegression()\nlm.fit(X, y)\n\n# print intercept and coefficients\nprint(lm.intercept_)\nprint(lm.coef_)","34c10ce4":"lm = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\nlm.conf_int()\nlm.summary()","10c04a0c":"# only include TV and Radio in the model\nlm = smf.ols(formula='Sales ~ TV + Radio', data=data).fit()\nlm.rsquared","c66a3fce":"# add Newspaper to the model (which we believe has no association with Sales)\nlm = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\nlm.rsquared","b507c9d9":"import numpy as np\n\n# set a seed for reproducibility\nnp.random.seed(12345)\n\n# create a Series of booleans in which roughly half are True\nnums = np.random.rand(len(data))\nmask_large = nums > 0.5\n# initially set Size to small, then change roughly half to be large\ndata['Scale'] = 'small'\ndata.loc[mask_large, 'Scale'] = 'large'\ndata.head()","79a381d2":"# create a new Series called IsLarge\ndata['IsLarge'] = data.Scale.map({'small':0, 'large':1})\ndata.head()","fb137ff3":"# create X and y\nfeature_cols = ['TV', 'Radio', 'Newspaper', 'IsLarge']\nX = data[feature_cols]\ny = data.Sales\n\n# instantiate, fit\nlm = LinearRegression()\nlm.fit(X, y)\n\n# print coefficients\ni=0\nfor col in feature_cols:\n    print('The Coefficient of ',col, ' is: ',lm.coef_[i])\n    i=i+1","8b8c93e7":"# set a seed for reproducibility\nnp.random.seed(123456)\n\n# assign roughly one third of observations to each group\nnums = np.random.rand(len(data))\nmask_suburban = (nums > 0.33) & (nums < 0.66)\nmask_urban = nums > 0.66\ndata['Targeted Geography'] = 'rural'\ndata.loc[mask_suburban, 'Targeted Geography'] = 'suburban'\ndata.loc[mask_urban, 'Targeted Geography'] = 'urban'\ndata.head()","3cb9bb1a":"# create three dummy variables using get_dummies, then exclude the first dummy column\narea_dummies = pd.get_dummies(data['Targeted Geography'], prefix='Targeted Geography').iloc[:, 1:]\n\n# concatenate the dummy variable columns onto the original DataFrame (axis=0 means rows, axis=1 means columns)\ndata = pd.concat([data, area_dummies], axis=1)\ndata.head()","836f6f3c":"# create X and y\nfeature_cols = ['TV', 'Radio', 'Newspaper', 'IsLarge', 'Targeted Geography_suburban', 'Targeted Geography_urban']\nX = data[feature_cols]\ny = data.Sales\n\n# instantiate, fit\nlm = LinearRegression()\nlm.fit(X, y)\n\n# print coefficients\nprint(feature_cols, lm.coef_)","0985e1ed":"#Let's start with importing necessary libraries\n\nimport pandas as pd \nimport numpy as np \nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model  import Ridge,Lasso,RidgeCV, LassoCV, ElasticNet, ElasticNetCV, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","c7a19bc5":"# Let's create a function to create adjusted R-Squared\ndef adj_r2(x,y):\n    r2 = regression.score(x,y)\n    n = x.shape[0]\n    p = x.shape[1]\n    adjusted_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\n    return adjusted_r2","b10a7168":"data =pd.read_csv('..\/input\/admission-prediction\/Admission_Prediction.csv')\ndata.head()","916b29ab":"data.describe(include='all')","1e80ead7":"data['University Rating'] = data['University Rating'].fillna(data['University Rating'].mode()[0])\ndata['TOEFL Score'] = data['TOEFL Score'].fillna(data['TOEFL Score'].mean())\ndata['GRE Score']  = data['GRE Score'].fillna(data['GRE Score'].mean())","f424e40a":"data.describe()","bd163526":"data= data.drop(columns = ['Serial No.'])\ndata.head()","3d6b0dc1":"# let's see how data is distributed for every column\nplt.figure(figsize=(10,10), facecolor='white')\nplotnumber = 1\n\nfor column in data:\n    if plotnumber<=16 :\n        ax = plt.subplot(4,4,plotnumber)\n        sns.distplot(data[column])\n        plt.xlabel(column,fontsize=10)\n    plotnumber+=1\nplt.tight_layout()","7757a66f":"y = data['Chance of Admit']\nX =data.drop(columns = ['Chance of Admit'])","a0c2ab5a":"plt.figure(figsize=(10,10), facecolor='white')\nplotnumber = 1\n\nfor column in X:\n    if plotnumber<=15 :\n        ax = plt.subplot(5,3,plotnumber)\n        plt.scatter(X[column],y)\n        plt.xlabel(column,fontsize=10)\n        plt.ylabel('Chance of Admit',fontsize=10)\n    plotnumber+=1\nplt.tight_layout()","bca446df":"scaler =StandardScaler()\n\nX_scaled = scaler.fit_transform(X)","c2c9110e":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvariables = X_scaled\n\n# we create a new data frame which will include all the VIFs\n# note that each variable has its own variance inflation factor as this measure is variable specific (not model specific)\n# we do not include categorical values for mulitcollinearity as they do not provide much information as numerical ones do\nvif = pd.DataFrame()\n\n# here we make use of the variance_inflation_factor, which will basically output the respective VIFs \nvif[\"VIF\"] = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n# Finally, I like to include names so it is easier to explore the result\nvif[\"Features\"] = X.columns","351dff0d":"vif","900c1686":"x_train,x_test,y_train,y_test = train_test_split(X_scaled,y,test_size = 0.25,random_state=355)","511ccf60":"y_train","bb865a78":"regression = LinearRegression()\n\nregression.fit(x_train,y_train)","f1ec7d56":"regression.score(x_train,y_train)","14f449e1":"adj_r2(x_train,y_train)","27238ee7":"regression.score(x_test,y_test)","8da9dba5":"adj_r2(x_test,y_test)","b7310381":"# Lasso Regularization\n# LassoCV will return best alpha and coefficients after performing 10 cross validations\nlasscv = LassoCV(alphas = None,cv =10, max_iter = 100000, normalize = True)\nlasscv.fit(x_train, y_train)","2bab45c3":"# best alpha parameter\nalpha = lasscv.alpha_\nalpha","d3ad132f":"#now that we have best parameter, let's use Lasso regression and see how well our data has fitted before\n\nlasso_reg = Lasso(alpha)\nlasso_reg.fit(x_train, y_train)","bec42d98":"lasso_reg.score(x_test, y_test)","eed004c4":"# Using Ridge regression model\n# RidgeCV will return best alpha and coefficients after performing 10 cross validations. \n# We will pass an array of random numbers for ridgeCV to select best alpha from them\n\nalphas = np.random.uniform(low=0, high=10, size=(50,))\nridgecv = RidgeCV(alphas = alphas,cv=10,normalize = True)\nridgecv.fit(x_train, y_train)","6b031445":"ridgecv.alpha_","ad86f318":"ridge_model = Ridge(alpha=ridgecv.alpha_)\nridge_model.fit(x_train, y_train)","a789d735":"ridge_model.score(x_test, y_test)","bbd09679":"# Elastic net\n\nelasticCV = ElasticNetCV(alphas = None, cv =10)\n\nelasticCV.fit(x_train, y_train)","19dfe631":"elasticCV.alpha_","b0ce73db":"# l1_ration gives how close the model is to L1 regularization, below value indicates we are giving equal\n#preference to L1 and L2\nelasticCV.l1_ratio","16efb585":"elasticnet_reg = ElasticNet(alpha = elasticCV.alpha_,l1_ratio=0.5)\nelasticnet_reg.fit(x_train, y_train)","0062ffde":"elasticnet_reg.score(x_test, y_test)","e9706307":"# saving the model to the local file system\nimport pickle\nfilename = 'LR.pickle'\npickle.dump(regression, open(filename, 'wb'))","14e87bba":"The mathematical flow for multicollinearity can be shown as:\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fmulticollinearity_flow.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601013945&Signature=jgqXKBFfR641EWFjSwygAaZKFESrQRRK%2FK7SqVblgfyfm2wUq2tyDpHImbvwrFF8MBXClI7SeXI8pzBmEhQqkYLQLwZf1gAe5KjQ91eASeYK5Pz4P8YDgL3qRytGCZrKp%2Fh7PB5PITRS35DoAfV%2FdSkOdbAffDJsOyyrtjZSeTaWYkNXgf1J1Tue7ph0Le6JsetLofauHhKaECD3AwIIt4%2Fj4zepHISEZzQV0BEeQ6dJtcTOnbKHMxjFIwWT7HvvHIEga%2B8UYsRSSY46c7ihWCvbUGWIZrhPV9cNFPwjr%2B3PMVV22Zc0tLzyGH1EYeqnh67tfnDDR1M7jHMHyxmVEQ%3D%3D\" width=\"500\">","697250f6":"Now, let\u2019s understand how to check, how well the model fits our data.","7e3072bc":"\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1510123%2Fsl.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601041260&Signature=CUeqjJZLLjcr8cO5izVEDebiVPm4JvxKUpvOJ%2FvECYQ0DFTDlycMkk7zgAml29LNe3iUn2VLS906iXTo1FWHJYOQy5W2%2BHtZHI2ZyzrvY3krlf3k%2BMBuBz4tLrpMrkUDB5CZ3KDNuGc6ZJMykNMWELc4DhpYsLgoQlHqFdX6%2BWM%2B3RuOoHg1mmDmItzbrT5yYyebFEuJCn6AYRfsCLnwVIp%2Bfd0EAdkRBOh6%2BfysPn64w1Sr%2Fr7bqdvU4CV5UUWP%2FqOcc%2FVBoqHXprv%2BVZ2ezvbVj2B%2Bs70P%2Bukn2JOuuqLCds4wu%2BJAfw%2BzP7%2F2cUHEJuLgWaWvrcrMRk6DxempMw%3D%3D\" width= \"300\">\n\n\n**Hence, we can build a model using the Linear Regression Algorithm.**","caefc48f":"# <a>Linear Regression<\/a>\nLinear Regression is one of the most fundamental and widely known Machine Learning Algorithms. Building blocks of a Linear \n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1511964%2Flinreg.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601466571&Signature=tk246AwYMMNxcBISsnGlqYe9ovQT%2BWGtRP5NY4hyLPrLCLq4AMP0VrK8ow9p%2FuOFBG1j7GmDkk8geYTyhVYYt1iIXwHsw89EsIe8XFwFAsGDDkwJ95GyRr4%2BF6VpTlvTfPj%2BBADYeP41bbpGsIceeVvQP4yl1jsXDIJfAfCxkKNDZk3kzPy4whAwFOpR4sT50blv2fpxdrlzeVGnpRzMdFkkp78eKg98PkrU5jpn0eDiAZA1uHKzxLCjc4hgApc6xzb%2B8H9YUw728Uo2xkIQHCkUOcOp%2F1VMMv1r7MM17DJ6quYPghB6PqJOOGqYGisx1zaIDs0m9PFcFRCR1GZiJw%3D%3D\" width = \"400\">\n          \nRegression Model are:\n\nDiscret\/continuous independent variables\n\nA best-fit regression line\n\nContinuous dependent variable. i.e., A Linear Regression model predicts the dependent variable using a regression line based on the independent variables. The equation of the Linear Regression is:\n\n                                    Y=a+b*X + e \nWhere, a is the intercept, b is the slope of the line, and e is the error term. The equation above is used to predict the value of the target variable based on the given predictor variable(s).   \n\n","bf7f6ed4":"The slope indicates the steepness of a line and the intercept indicates the location where it intersects an axis. The slope and the intercept define the linear relationship between two variables, and can be used to estimate an average rate of change. The greater the magnitude of the slope, the steeper the line and the greater the rate of change.\n\n![image.png](attachment:image.png)","636be1d2":"The new values for 'slope' and 'intercept' are caluclated as follows:\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fnew_m.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012756&Signature=q30o0BkvffKA4DoQ4%2BunV6H439M6oQwLN%2BCnlpzd%2B4ht%2BWY2i4KIgvR4yRyG66l6WFdeUs78%2Fg7EM0YwtQxHJA0nsutP5RNzfoed2BUcYQ9XoSeYmUiTt0Usdz4WNu9aVFWuqKZcQ2uudhaL1LsmDUz5V0w7CxyM%2FEscAJtj2Xr2VEd0%2FZkBCYqRH1SEFwWEjmuNMwvlpPKZAHvnBb%2BbsXEx3VuE%2BdcUMctGOJ2NdS0057wJGUqRoFIivMNLSRMLwnkLmgMp1uotUyz3riO6rJHIxc5GhKfy%2BsHuQfU3dqtrxnAgGTdK5ozUdQrAe1945Uom77QSSNsmv%2FSN8c7WeA%3D%3D\" width=\"300\">\n\nwhere, $\\theta_0$ is 'intercept' , $\\theta_1$ is the slope, $\\alpha$ is the learning rate, m is the total number of observations and the term after the $\\sum$ sign is the loss. Google Tensor board recommends a Learning rate between 0.00001 and 10. Generally a smaller learning rate is recommended to avoid overshooting while creating a mode","280b0cd7":"**When should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net?**\n\nAccording to the Hands-on Machine Learning book, it is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features\u2019 weights down to zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of\ntraining instances or when several features are strongly correlated.\n\nNo let's see the Python Implementation of these concepts:\n","3649c2bf":"How do we interpret the coefficients?\n- If all other columns are constant, the  _suburban_  geography , _urban_  geographyis associated with an average **decrease** of  7.45, 8.10  widgets in sales for $1000 spent. \n\n\n**A final note about dummy encoding:** If we have categories that can be ranked (i.e., worst, bad, good, better, best), we can potentially represent them numerically  as (1, 2, 3, 4, 5) using a single dummy column","cd61d71f":"# $R^2$ statistics\n\nThe R-squared statistic provides a measure of fit. It takes the form of a proportion\u2014the proportion of variance\nexplained\u2014and so it always takes on a value between 0 and 1. \nIn simple words, it represents how much of our data is being explained by our model. \nFor example,  $R^2$ statistic = 0.75, it says that our model fits 75 % of the total data set.\nSimilarly, if it is 0, it means none of the data points is being explained and a value of 1 represents 100% data explanation.\nMathematically $R^2$ statistic is calculated as :\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FRSquared.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012863&Signature=msWg0hnW7RfmgBq6JT3ylvZ0KWflkGCa7KCIokSVZqRRQSZ0oJp5aRmgUVc31C6v2qh3WqpHdA32UbqaQByn4EVP%2BPNYB6%2BaI2DxDrIX%2Fl8Fso8H55Hw05q30fzYZdt%2Fjy2%2F7CDwaSBlkDzpdzPjqjqUbG6jZGQU18lCMdsIRIAP02pUnf%2FP0ZlE05EBZe5XbLSfitoZjW99rCofSP0UZ9QYfv%2BgfoFIK5C1cBEMKczFT3s71mNkf8j5lGhuGxYScysOMd%2FmowSEBAbU6iwOUc41gXK42dUukm0r9AbVEaQVTQN5ppQKoCi1SfWvdF9ehKoQ4XnOrSO0SeGlkfF4Qg%3D%3D\" width=\"300\">\n                                              \nWhere RSS:  is  the Residual Sum of squares and is given as :\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FRSS.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012884&Signature=J3oP5W3w%2FZ9rxaHiDnyIxMxUd5ob1M%2FcRnZgbTH4RvLBh76Y4zw206FbEUTIy5VZ%2BAct%2BiXamy%2FpiHC1ARrfnCcPmXM8bZcI%2F5WQblRjhXvL0HdJ7wMRnFoI8Ja%2ByuXnaJrEB0ndViEOkzABkM80kkVySCxvH8BuNzMypRWzZkbKOa6e4sVLWFavfEobI3toT1QRL%2FICBvoYl7GjxfoLu%2FTXr1WsJv1JcsKe11Be%2FYksiavoO6yNYrVBrF3w5rsWbdZbWm7BF2xVPmq29W8RgV0fcjv4Sy4F%2FzWlFrjB6db7Bnpmp0GLVXQ0rlfIjPpHV966Ok5BApkw60ptnR59%2BQ%3D%3D\">\n                                                                \nRSS is the residual(error) term we have been talking about so far.\nAnd, TSS:  is  the Total sum of squares and given as :\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FTSS.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012902&Signature=rTJqhilWpXGwPfcJThxn40rFY8NflxYLU%2FrpeV2qN53hAphgvU8NDwFSJZyJBhQL4B%2F2bImReWoNJ81lg313KjkyY5akF2qY5SLnm3kJYkJxuG2th0usbshZnrNpUeKcT3M4jCCyf2hRsxyXcvWyKuyT4chwolzbjR6ZaYFpJkFTjIQCsq9kqYq7ueDBGCfl%2Ff48OH2HdpO%2FKNFAYYkWWb13bX5zl%2FUbnw6b5gDw1f7AkACLGUGClWK2fWC3Ivle8BPervY3eCMGHAEBc%2BiZK5gZY3P1COcg7fFkVfIB0F%2BpLv%2F54volG1XfoyV0FRX7AEFN4%2B0IUbIEsCU1j3BTAQ%3D%3D\">\n                                                             \nTSS is calculated when we consider the line passing through the mean value of y, to be the best fit line.\nJust like RSS, we calculate the error term when the best fit line is the line passing through the mean value of y and we get the value of TSS.\n    <img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FTSS2.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012923&Signature=dnLO9khms6kbybif%2BdfUroa0%2BsWcwse5BWXhq1b6b4qGpZ1d5rT%2BpLWgo6T7yL5%2F2Pv8gLAgMo22U7Sk3x%2Bnml6fQ%2BoTzSzwWTbY%2Fxuyjk5Iz8jHV7Kh8cMyEPhkcdJj0yU75xqaNBLU3M8iK2HXWHa9DSIKtasRibrpnRu5OF7C0fUSt7PoNri647VsRDc%2BIjCI4nRMjdqrziiFfAfLDCF%2FLZVljQpzh1QERsPnUh1c3LXswOiZGrSXmeVhTBUwsciOZy%2BhBBSUzxlx3kJLHCcQrPgnYCzgtqx%2BgHDbRDPpwGAdQXWHdkn0Htd4cfhTRGZWDAPc6xtWmiYlvJQw4Q%3D%3D\">\n    \nThe closer the value of R2 is to 1 the better the model fits our data. If R2 comes below 0(which is a possibility) that means the model is so bad that it is performing even worse than the average best fit line.","89add335":"# Simple Linear Regression\n\nSimple Linear regression is a method for predicting a **quantitative response** using a **single feature** (\"input variable\"). The mathematical equation is:\n\n$y = \\beta_0 + \\beta_1x$\n\nWhat do terms represent?\n- $y$ is the response or the target variable\n- $x$ is the feature\n- $\\beta_1$ is the coefficient of x\n- $\\beta_0$ is the intercept\n\n$\\beta_0$ and $\\beta_1$ are the **model coefficients**. To create a model, we must \"learn\" the values of these coefficients. And once we have the value of these coefficients, we can use the model to predict the Sales!","470abab6":"# Prediction  using the model\n\nIf the expense on TV ad is $50000, what will be the sales prediction for that market?\n\n$$y = \\beta_0 + \\beta_1x$$\n$$y = 6.974821 + 0.0554647 \\times 50$$","dd1ed3a1":"**In our example:****\n\n$y = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Radio + \\beta_3 \\times Newspaper$\n\nLet's use Statsmodels to estimate these coefficients","06d9122b":"Keep in mind that we only have a **single sample of data**, and not the **entire population of data**. The \"true\" coefficient is either within this interval or it isn't, but there's no way actually to know. We estimate the coefficient with the data we do have, and we show uncertainty about that estimate by giving a range that the coefficient is **probably** within.\n\nNote that using 95% confidence intervals is just a convention. You can create 90% confidence intervals (which will be more narrow), 99% confidence intervals (which will be wider), or whatever intervals you like.\n","7010ae95":"Let's redo the multiple linear regression problem and include the **IsLarge** predictor:","d364aee3":"<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1510153%2Fresss.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601041971&Signature=A6j5Ww4qGslp3RBTZkaNORzCHoIXmd8JYzs5sDcL%2BeNITyy4N0eNCSVgaQ6rmctkz4HGEBPt00d9QOkj%2FEF8LLy3xGA6Cs%2Bq%2FVNVdJe84uCjpSEtX9xtbvsLQlU9XYfKj8uJzzkS1Ww0NIWP6JBzuCv4sQBK3WzTyL214sFB9E1IgVTZjwLIn0PmP4Xk9uRp4dScJabN%2Fy6EKYflyRteMdtXu9bs%2Fs9%2BuK%2FF4QvC4nIlDtFW4l78bxICdNGgwWs8ETCbHov2OlTxF1eBs9OIYH2tLo10%2FtVjmM8k874%2FBvlaRSmloXlnBPNu6%2BM7N%2FrxmWMu7wNLM2zEYVsMeeQz7g%3D%3D\" width= \"500\">\n\nIn the diagram above:\n\nThe black dots are the observed values of x and y.\nThe blue line is our least squares line.\nThe red lines are the residuals, which are the vertical distances between the observed values and the least squares line.","5a9dafd6":"our r2_score for test data (75.34%) comes same as before using regularization. So, it is fair to say our OLS model did not overfit the data.","c2d5103f":" # Elastic Net\n\nAccording to the Hands-on Machine Learning book, elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso\u2019s regularization terms, and you can control the mix ratio \u03b1. \n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FelasticNet.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601014343&Signature=Ga3%2B5NtSrOJP7IGS%2FP9ZF1E6iGDG3gP315V0qhN0v7jEslXurTtXNZ6WXphdlhfsdoAmT9aId1VgqEi7EO72JC6K4HlmyNk8JnXQXhWb%2BHEOlzwfTNwXEvivhoj2%2BDI8%2FDXdIZG%2BsgGcYyBBKo5rqAe4m%2FYIMleiGBnrVpjdMpqEEMK0BJfS%2BuR96vjhvkaYzS4YMR2cf2mYrzkkEoDfjbfOkiKb4rhXHX7RINhJi67Z5e5X1vkRV4HiKFxWMoYZyFEaPVTDSBC23eQEEzixspb%2Ft5%2FClxPhSyLjGrO0nI0NRhsQL7FVajMZrBGs%2BMl646G5f4r4%2BIlKstY8fG8cUg%3D%3D\" width=\"300\">\nwhere \u03b1 is the mixing parameter between ridge (\u03b1\u2004=\u20040) and lasso (\u03b1\u2004=\u20041).","6e771429":"# Ridge Regression (L2 Form)\nRidge regression penalizes the model based on the sum of squares of magnitude of the coefficients. The regularization term is given by\n\n regularization=$ \\lambda *\\sum  |\\beta_j ^ 2| $\n\nWhere, \u03bb is the shrinkage factor.\n\nand hence the formula for loss after regularization is:\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fridge.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601014228&Signature=e5Ye%2F9HwnZQ2QmdSN%2BsKI45LKIQt3Vnqz%2Bz4A%2FVrlLfUCFNsbnmjAwGi3ADNyyogdghfSy3j%2B93AlpulxPuMRJ%2F672fyGBRaI0t7v2KZlq2vemlGt9ZU1ztlV6MHxFOUbfXVXGaiUbsCv%2B%2BaCbY1r3pVuSwrSbHMojED7a1vRGZW9vPCoKJ3bcTlr6HgEDZGROnt74gjr%2BoEH7ajlDTy2R0K%2B774AF6GZqYi7nUm2Jpb9rUcmZbvTgYtL0wy0SSRVRjbTny7QC7K8A5T0gbF5xeYldnwz%2BWP3tPqKSeYnNTcyeQfcXdqfzllvE5CGEwZ74UgJ2lMcjxnKoFCr5dhbw%3D%3D\" width=\"300\">\n\nThis value of lambda can be anything and should be calculated by cross validation as to what suits the model.\n\nLet\u2019s consider $\\beta_1$ and $\\beta_2$ be coefficients of a linear regression and \u03bb = 1:\n\nFor Lasso, $\\beta_1$ + $\\beta_2$ <= s  \n\nFor Ridge, $\\beta_1^2$ + $\\beta_2^2$  <= s  \n\nWhere s is the maximum value the equations can achieve\n.\nIf we plot both the above equations, we get the following graph:\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fridge_vs_lasso.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601014263&Signature=sN%2Fywtq1jlGIMJVOeWqBugpOk2EA88UAmqf0cbDs5XCsnYTexe6XST6IkcYbAljLYuxbTy3uu37BlbNQV%2FMW76k9%2BXsAVFwuK85eDZO%2F3asebVXSrW%2BYnk%2F48oc4pRLtsFACm88zuLdjJRJ9h9eXEsVbrx9u%2FVjleIypol%2FQQwtq%2FhBaurJzL0ij1zZcsitH4AzYytfqNJ%2FPiuJkEx6I9dKC6YxgsoTEDUsb1JpCHETaUXePWAmSptjEvBAll4inXlZJQqhOAQpMbGD85kkhG3eEsRA%2BgYMOGD6metxVJjZSakdmnokIxmMS%2Bgrt%2BD%2BHBCSUMtqR6Z%2BVA57nd12c1A%3D%3D\" width=\"300\">\n\nThe red ellipse represents the cost function of the model, whereas the square (left side) represents the Lasso regression and the circle (right side) represents the Ridge regression.","b7d809ac":"From the relationship diagrams above, it can be observed that there seems to be a linear relationship between the features TV ad,  Radio ad and the sales is almost a linear one. A linear relationship typically looks like:","f91cb914":"# <a>What is Regression Analysis?<\/a>\nRegression in statistics is the process of predicting a Label(or Dependent Variable) based on the features(Independent Variables). Regression is used for time series modelling and finding the causal effect relationship between the variables and forecasting. For example, the relationship between the stock prices of the company and various factors like customer reputation and company annual performance etc. can be studied using regression.\n\nRegression analysis is an important tool for analysing and modelling data. Here, we fit a curve\/line to the data points, in such a manner that the differences between the distance of the actual data points from the plotted curve\/line is minimum.\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1509677%2FRegression.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601028526&Signature=l424XY4eTT%2FPbygl7sz3R7iZCj81l7D2DpeRT%2BnYl6hT5Qpp5KVK7iaBOE0URgoDFwXrSuVaFzWhrxymr5a0fGCTT7govG8tLBYmbR21d9bOC2Oi6RTEDtflxZrVlza9jOvOMRm9qr3PecLSs00OSW%2B04Zkdt3NeGIdzPYL%2BI0v0kvKeu9pRPWqtRMKS2043cOzA06MV9rZT2KZhcHrql1Nxq1WYs%2BR8lNVdCSPFcnieKd4XSSeUvVU0DfSO1wAaczJvpfrVs3XUZcSJLMiH%2Bk3bDyRXECv4sWiioCtl4QOqPpsGGYq5kNgQjJ3T2IN3MnwkfixoJl36FwoCfN25Bg%3D%3D\" width= \"300\">","7f45a458":"We need to represent the \u2018Targeted Geography\u2019 column numerically. But mapping urban=0, suburban=1 and rural=2 will mean that rural is two times suburban which is not the case.\nHence, we\u2019ll create another **dummy variable**:","0d379664":"# <a>The use of Regression<\/a>\nRegression analyses the relationship between two or more features. Let\u2019s take an example:\n\nLet\u2019s suppose we want to make an application which predicts the chances of admission a student to a foreign university. \n\nThe benefits of using Regression analysis are as follows:\n\n* It shows the significant relationships between the Label (dependent variable) and the features(independent variable).\n* It shows the extent of the impact of multiple independent variables on the dependent variable. \n* It can also measure these effects even if the variables are on a different scale. \n\nThese features enable the data scientists to find the best set of independent variables for predictions.","561a645d":"# Hypothesis Testing and p-values\n\n**Hypothesis testing** is Closely related to confidence intervals. We start with a **null hypothesis** and an **alternate hypothesis** (that is opposite to the null). Then, we check whether the data **rejects the null hypothesis** or **fails to reject the null hypothesis**.\n\n(\"Failing to reject\" the null hypothesis does not mean \"accepting\" the null hypothesis. The alternative hypothesis might indeed be true, but that we just don't have enough data to prove that.)\n\nThe conventional hypothesis test is as follows:\n- **Null hypothesis:** No relationship exists between TV advertisements and Sales (and hence $\\beta_1$ equals zero).\n- **Alternative hypothesis:** There exists a relationship between TV advertisements and Sales (and hence, $\\beta_1$ is not equal to zero).\n\nHow do we test this? We reject the null hypothesis (and thus believe the alternative hypothesis) if the 95% confidence interval **does not include zero**. The **p-value** represents the probability of the coefficient actually being zero.","0a283527":"# <a>Handling Categorical Predictors with Two Categories<\/a>\n\nscikit-learn expects all features to be numeric. \n\n1. Ordered categories: transform them to sensible numeric values (example: small=1, medium=2, large=3)\n2. Unordered categories: use dummy encoding (0\/1)\n\nTill now, all the predictors have been numeric. What if one of the predictors is categorical?\n\nWe\u2019ll create a new feature called **Scale**, and shall randomly assign observations as **small or large*","26feb021":"#  <a>Advantages and Disdvantages<\/a>\n\n******Advantages of linear regression:\n\n- Simple to explain\n- Highly interpretable\n- Model training and prediction are fast\n- No tuning is required (excluding regularization)\n- Features don't need scaling\n- Can perform well with a small number of observations\n- Well-understood\n\n******Disadvantages of linear regression:\n\n- Presumes a linear relationship between the features and the response\n- Performance is (generally) not competitive with the best supervised learning methods due to high bias\n- Can't automatically learn feature interactions","a5aace89":"# Multiple Linear Regression\n\nTill now, we have created the model based on only one feature. Now, we\u2019ll include multiple features and create a model to see the relationship between those features and the label column.\nThis is called **Multiple Linear Regression**.\n\n\ud835\udc66=\ud835\udefd0+\ud835\udefd1\ud835\udc651+\ud835\udefd2\ud835\udc652+...+\ud835\udefd\ud835\udc5b\ud835\udc65\ud835\udc5b \n\ud835\udc66  is the response\n\ud835\udefd0  is the intercept\n\ud835\udefd1  is the coefficient for  \ud835\udc651  (the first feature)\n\ud835\udefd\ud835\udc5b  is the coefficient for  \ud835\udc65\ud835\udc5b  (the nth feature)\n\nThe  \ud835\udefd  values are called the model coefficients:\n\n* These values are estimated (or \"learned\") during the model fitting process using the least squares criterion.\n* Specifically, we are going to find the line (mathematically) which minimizes the sum of squared residuals (or \"sum of squared errors\").\n* And once we've learned these coefficients, we can use the model to predict the response.\n \n","cfad33f5":"# Interpreting the model\n\nHow do we interpret the coefficient for spends on TV ad ($\\beta_1$)?\n- A \"unit\" increase in spends on a TV ad is **associated with** a 0.0556477 \"unit\" increase in Sales.\n- Or, an additional $1,000  on TV ads is **translated to** an increase in sales by 55.47 Dollars.\n\nAs an increase in TV ad expenditure is associated with a **decrease** in sales, $\\beta_1$ would be **negative**.","c6b0df63":"# Why Should We Care About Multi-Collinearity?\n* The coefficients in a Linear Regression model represent the extent of change in Y when a certain x (amongst X1,X2,X3\u2026) is changed keeping others constant. But, if x1 and x2 are dependent, then this assumption itself is wrong that we are changing one variable keeping others constant as the dependent variable will also be changed. It means that our model itself becomes a bit flawed.\n* We have a redundancy in our model as two variables (or more than two) are trying to convey the same information.\n* As the extent of the collinearity increases, there is a chance that we might produce an overfitted model. An overfitted model works well with the test data but its accuracy fluctuates when exposed to other data sets. \n* Can result in a Dummy Variable Trap.","81b38d66":"Let's visualize the data and analyze the relationship between independent and dependent variables:","edec494a":"Thus, we would predict Sales of **9,748 widgets** in that market.\n\nLet's do the same thing using code.","6e6a0597":"# LASSO(Least Absolute Shrinkage and Selection Operator) Regression (L1 Form)\nLASSO regression penalizes the model based on the sum of magnitude of the coefficients. The regularization term is given by\n\n regularization=$ \\lambda *\\sum  |\\beta_j| $\n\nWhere, \u03bb is the shrinkage factor.\n\nand hence the formula for loss after regularization is:\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FL1.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601011090&Signature=PMwc%2BvMPBskPSStA5WK%2B0bC6dsj3wQWgBRjw9ED6MoqaD46E4VpAlx3qgwFGUpItarfiaMnY1Gu9M5gMT76oAzGODdZVyWFbo%2FkqJZdnFtKLcxkyQJVRDWkjPE2zjbCBqb3uMpcbIVznjc1sx4RATMpoI4edoN4FX1UoPOy2Ii4O5EBYi5mIRNwF5y%2FJzTn7OpsWyPxLSvnnw4%2BOT9Jxg1nSE3ip2BEj%2BSy9D%2B2uwZPB4mQP9LSa9PiSc6x912iUNI9AgTId9Rc593dPJY0NwH5g%2FVOePFnaLN9dQ6xX6Eh09HFHkYuE1Y%2Fp59V0qiqPLNCGmdOjOoqX7kV4p2iA8w%3D%3D\" width=\"300\">","8d66ec44":"Our r2 score is 84.15% and adj r2 is 83.85% for our training et., so looks like we are not being penalized by use of any feature\n\nLet's check how well model fits the test data.\n\nNow let's check if our model is overfitting our data using regularization.","65b88076":"**Definition:** The purpose of executing a Linear Regression is to predict the value of a \tdependent variable based on certain independent variables.\n\nSo, when we perform a Linear Regression, we want our dataset to have variables which are independent i.e., we should not be able to define an \tindependent variable with the help of another independent variable  \tbecause now in our model we have two variables which can be defined based \ton a certain set of independent variables which defeats the entire purpose.\n\n* Multi-collinearity is the statistical term to represent this type of a relation amongst the independent variable- when the independent variables are not so independent\ud83d\ude0a.\n* We can define multi-collinearity as the situation where the independent variables (or the predictors) have strong correlation amongst themselves.\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fmulticollinearity_pic.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601013907&Signature=k427Kv9IbSfPbJKqwlOrQvF6KbH8HYKjYpTVDML%2BlF%2Bfg4S9xKAREN2uInNiliU%2F4BHOgjXSujxzoEIjpRdIJdwYt9GPRnlosm7xOgEVrlRmhi5c95y7CZFYnskaG8qqlt1XEvnEiK00zQBA759cy9TBKcQAUpJeKNH0yZzgCnYGEnEJCLoxqsx3Vp9oXQu%2F2WM7GRSxtanhdulXIT3atyEC7SGB5DOI9AJd1e%2F0%2BlWqY04XxWAGJah4mYQOWmeQy8zaVEgs3ylqbwhg8nhQvs9RfIgtpkvpGtthBC33P1bFy9NxW%2F3z5YHZYd2szYNEe8Ea4lSWHFucPbCP5VSDKw%3D%3D\" width=\"500\">","d906dc97":"# Regularization \nWhen we use regression models to train some data, there is a good chance that the model will overfit the given training data set.  Regularization helps sort this overfitting problem by restricting the degrees of freedom of a given equation i.e. simply reducing the number of degrees of a polynomial function by reducing their corresponding weights.  \nIn a linear equation, we do not want huge weights\/coefficients as a small change in weight can make a large difference for the dependent variable (Y). So, regularization constraints the weights of such features to avoid overfitting. Simple linear regression is given as:\n\n$y = \\beta_0 + \\beta_1x1+ \\beta_2x2 +\\beta_3x3+...+\\beta_PxP$\n\nUsing the OLS method, we try to minimize the cost function given as:\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FRSS_reg.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012881&Signature=kt22nZ82XpmiYR2RncRN7EeUg4bSr09Id%2BYCGyvm68xe5ifIjNWkVC3ehKmYZpHplFliXuuHOEVJmrqvROUI%2BnzUe2uGbhm6tOhAFfhRTARYN40n%2FicNPHO%2FfI2OJDRPajn5tt2JUiCHrudbScQCKRFZyVBIXJp5AhZMMxb0D7IjtjrYTqYJTrwOGevD5xozYgUDnTs69G2XVIa2jb%2BhyY5oeJ4pb27vt7%2BSR6cf4N1nZ2z4WvWke12aymUwGZiHuDHWn9EblXvYu8sUhjOCXxTL4pwlJbNxSvJKjEn%2FvajOLe1Nm1pGxPSchij3UABWQblGvIL%2F7J3favtDOA25Gg%3D%3D\" width=\"300\">\n\nTo regularize the model, a Shrinkage penalty is added to the cost function.\nLet\u2019s see different types of regularizations in regression:","10c9dfe6":"Selecting the model with the highest value of R-squared is not a correct approach as the value of R-squared shall always increase whenever a new feature is taken for consideration even if the feature is unrelated to the response.\n\nThe alternative is to use **adjusted R-squared** which penalises the model complexity (to control overfitting), but this again generally [under-penalizes complexity](http:\/\/scott.fortmann-roe.com\/docs\/MeasuringError.html).\n\na better approach to feature selection is**Cross-validation.** It provides a more reliable way to choose which of the created models will best **generalise** as it better estimates of out-of-sample error. An advantage is that the cross-validation method can be applied to any machine learning model and the scikit-learn package provides extensive functionality for that.","70907716":"# <a> R squared vs Adjusted R Squared<\/a>\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1509734%2FRvsAdjjR.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601029923&Signature=LRqYprHKIJU1w%2BCLMeK2TjB%2B5fjazEZtAcZqitaM%2BJXfSy9c7b1nXKFboXk1xHjHsAm3w3xifTTz%2Bx0EnbEhHzmCl8V%2Fcm5upkZz1bFep9XepfXS23fcGDgfexb8bHsVQvQbCXEDhPfrfUCUhfq3uQGjsQAp41EjpiPxFUDFF1p8nH4LtRtClW%2BHackM%2BRRy5SCJGOVcNOe%2F0Z795%2BlRfLNaY9LnMg4RpjeHkOoBW8mRDdOLa6bx2EbJgTzGkwT2toAMHLE%2Ft2cv5WlKDq0hkSx%2FphDLbJ0w0epRpwoBVAbcDBdKjLLhsuVjPRRYANYwKDzsBl5oHUoXJiIUOaSaAQ%3D%3D\">","5ff24e72":"Here, we have the correlation values for all the features. As a thumb rule, a VIF value greater than 5 means a very severe multicollinearity. We don't any VIF greater than 5 , so we are good to go. \n\nGreat. Let's go ahead and use linear regression and see how good it fits our data.\nBut first. let's split our data in train and test.","7c889a23":"# Bias Variance trade-off\n\nVariance is a measure of the degree of the spread of the predicted results. Bias, on the other hand, is a measure of the polarity of the results i.e., the results are favored towards a specific outcome.  The problem with Bias and Variance can be depicted as follows:\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fbias_variance.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601014058&Signature=k%2B9NcmdAkhv3AXLeUBWsp8M2%2BNyFVRE%2FJtke8VQOalxVzo0DQEGtFXVro4TxW508Wciz0PNwoFp%2BCQG8po2a00ORFb29xmCXixWPJ2mh2adbR%2BGTYiVavkG0qT%2FobOirKKJwvUR%2B97JQQAY%2Fg%2BiDyXzIfaVZHNxraO8qksQtwC4SgLhBWIPm7F%2FeQL3zgGCp05PhtJt43RuoF15FE0aTMV5yVZzj%2Bkn58w4qIz6vIw7heQtov2T7ZjRFMEK6R3%2FOALIUQPfE8PgaCWRlCgaiF2pPRfzKYtKwzP2c0HEn3o2G0Fg%2FiAdPDhU1CCkBkWPmqJsSij%2FATjcNWxB%2FlXqkDA%3D%3D\" width=\"500\">\n \n As depicted in the figure, our goal is to minimize the Variance as well as Bias to get to accurate results.\nIf there are lesser number of predictors in the data set that too having big coefficients, it is a possibility that the model obtained is favored towards those predictors i.e., a small change in those predictors lead to a big change in the predicted result. In other words, that predictor is driving our model, or our model is biased for that predictor. An easy way to overcome this is to introduce more no. of predictors which contributes towards the end result of the prediction. This increases the complexity of the model or in other words, the variance in the model increases.\nMathematically,\n\nTotal Error = $Bias^2 + Variance + Irreducible Error$\n\nIrreducible error is the error due to noise in the data.\n\nGraphically it looks like:\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Ftradeoff.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601014078&Signature=EsBi6NcZzoYkH0hPWG%2F1d7T25FW%2BNArFWtQZzAzz%2Fzhn0lU6dHdbFaIPBxq3HcaSnfGZ2kjY%2BS8oU7W3Jjro8N%2F9qi7Z8DF0Ek9E90aIoR0KlrkRSIFWr396m0F%2BfIRriqh3YFlqf5mYuX3%2BmF839imEr9dthEFF0OB6Sji7bD6buYYOhRpiAVGb4wdpA44tLhspeEblmXuwlymCObuw4pXb%2BIovoivGa9JhME6BvnYBv%2FPtn6Ej%2F7%2BS1AFfocYQhoNq%2FIj%2FDezGIf9ShwyeBUNS%2BqQpwUobGkuYR21qwo9glV3S8U9bxrnlgkr7zEuf%2FVIHevxtjh6875MGyNBIpw%3D%3D\" width=\"500\">\n\nIt can be seen that as the bias decreases, the variance increases.\n So, we need to find a balance so that we get a model which has low variance as well as low bias. This is called **bias-variance tradeoff.**\n\nBias can be minimized by training with more data and variance can be reduced by using regularization methods which we are going to discuss.","1a9807b6":"[](http:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FLinearGraph.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601010898&Signature=to1kJX6cC5aaUh2zG%2BvqKBXwgcvcFSDyZRL%2BXm%2BQnFsKQTHHQgUkP%2Fv6%2BYQo7KDyc0xmY7fGLeI5i%2F4YJnAfvc%2BWI7cdkE5245PsamtEBCr6YeTlvMJF5F%2FqiOFpxhtzaB3aOOygJy5WZqnU7j46lft6gl2mHElAi4CwtouV06o2q8Jd3p4IzEcjGuEHOu%2F24qEPkpJ%2FpJys7DOWeiv3EFVkYegDuttHyeyvdS%2F%2BP1wdvR5h2dmK1uJ8H%2BqCFNMrMHRzLzn5tF1XRmB%2FnO5Li%2F7BP1QxdC0dUuxXyoIkeu3QN05w%2Fb9TI5ag8yqYE%2FKu6a2bE7kVhJ8vR%2BR%2B8hXRUQ%3D%3D)","ae58b997":"Is it a \"good\" R-squared value? Now, that\u2019s hard to say. In reality, the domain to which the data belongs to plays a significant role in deciding the threshold for the R-squared value. Therefore, it's a tool for **comparing different models**.","c972d2ca":"Great, the relationship between the dependent and independent variables look fairly linear.\nThus, our linearity assumption is satisfied.\n\nLet's move ahead and check for multicollinearity.","1c170ba6":"# Feature Selection\n\n**How do we choose which features to include in the model?**\n\nWe're going to use train\/test split (and eventually cross-validation).\n\nWhy not use of p-values or R-squared for feature selection?\n\n* Linear models rely upon a lot of assumptions (such as the features being independent), and if those assumptions are violated, p-values and R-squared are less reliable. Train\/test split relies on fewer assumptions.\n* Features that are unrelated to the response can still have significant p-values.\n* Adding features to your model that are unrelated to the response will always increase the R-squared value, and adjusted R-squared does not sufficiently account for this.\n* p-values and R-squared are proxies for our goal of generalization, whereas train\/test split and cross-validation attempt to directly estimate how well the model will generalize to out-of-sample data.","dffe57fb":"* <a>__Variance Inflation Factor:__<\/a> Regression of one X variable against other X variables.\n\n     VIF=$\\frac {1}{(1-R squared)}$\n\n            The VIF factor, if greater than 10 shows extreme correlation between the variables and then we need to take care of the correlation.","e7ff7d0c":"# Remedies for Multicollinearity\n\n* **Do Nothing:** If the Correlation is not that extreme, we can ignore it. If the correlated variables are not used in solving our business question, they can be ignored.\n* **Remove One Variable**: Like in dummy variable trap\n* **Combine the correlated variables:** Like creating a seniority score based on Age and Years of experience\n* Principal Component Analysis","aa66d26d":"# Handling Categorical variables with More than Two Categories\n\nLet's create a new column called **Targeted Geography**, and randomly assign observations to be **rural, suburban, or urban**:","c4345563":"Now the data looks good and there are no missing values. Also, the first cloumn is just serial numbers, so we don' need that column. Let's drop it from data and make it more clean.","cfbb454f":"we got the same r2 square using Ridge regression as well. So, it's safe to say there is no overfitting.","9e61bd06":"# <a>The Problem statement:<\/a>\n\nThis data is about the amount spent on advertising through different channels like TV, Radio and Newspaper. The goal is to predict how the expense on each channel affects the sales and is there a way to optimise that sale?","bcf96612":"# Difference between Ridge and Lasso\nRidge regression shrinks the coefficients for those predictors which contribute very less in the model but have huge weights, very close to zero. But it never makes them exactly zero. Thus, the final model will still contain all those predictors, though with less weights. This doesn\u2019t help in interpreting the model very well. This is where Lasso regression differs with Ridge regression. In Lasso, the L1 penalty does reduce some coefficients exactly to zero when we use a sufficiently large tuning parameter \u03bb. So, in addition to regularizing, lasso also performs feature selection.","95e4fc36":"# <a>Questions about the data<\/a>\nA generic question shall be: How the company should optimise the spends on advertising to maximise the sales?\n\nThese general questions might lead you to more specific questions:\n\n* What\u2019s the relationship between ads and sales?\n* How prominent is that relationship?\n* Which ad types contribute to sales?\n* How each ad contributes to sales?\n* Can sales be predicted based on the expense of the advertisement?\n\nWe will explore these questions below!","665fdc94":"The data distribution looks decent enough and there doesn't seem to be any skewness. Great let's go ahead!\n\nLet's observe the relationship between independent variables and dependent variable.","8b14aafb":"If the 95% confidence interval **includes zero**, the p-value for that coefficient will be **greater than 0.05**. If the 95% confidence interval **does not include zero**, the p-value will be **less than 0.05**. \n\nThus, a p-value of less than 0.05 is a way to decide whether there is any relationship between the feature in consideration and the response or not. Using 0.05 as the cutoff is just a convention.\n\nIn this case, the p-value for TV ads is way less than 0.05, and so we **believe** that there is a relationship between TV advertisements and Sales.\n\nNote that we generally ignore the p-value for the intercept.","9e76b6d2":"For the scikit-learn library, all data must be represented **numerically**. If the feature only has two categories, we can simply create a **dummy variable** that represents the categories as a combination of binary value:","286c2eb2":"# <a>Estimating (\"Learning\") Model Coefficients<\/a>\n\nThe coefficients are estimated using the **least-squares criterion**,  i.e., the best fit line has to be calculated that minimizes the **sum of squared residuals** (or \"sum of squared errors\").","2ea01997":"# Why use Regularization?\nRegularization helps to reduce the variance of the model, without a substantial increase in the bias. If there is variance in the model that means that the model won\u2019t fit well for dataset different that training data. The tuning parameter \u03bb controls this bias and variance tradeoff. When the value of \u03bb is increased up to a certain limit, it reduces the variance without losing any important properties in the data. But after a certain limit, the model will start losing some important properties which will increase the bias in the data. Thus, the selection of good value of \u03bb is the key.\nThe value of \u03bb is selected using cross-validation methods. A set of \u03bb is selected and cross-validation error is calculated for each value of \u03bb and that value of \u03bb is selected for which the cross-validation error is minimum.","ddcd63b4":"# <a>Let's understand the relationship between the feature and target column<\/a>","5f3c8d63":"# Adjusted $R^2$ statistics\nAs we increase the number of independent variables in our equation, the R2 increases as well. But that doesn\u2019t mean that the new independent variables have any correlation with the output variable. In other words, even with the addition of new features in our model, it is not necessary that our model will yield better results but R2 value will increase. To rectify this problem, we use Adjusted R2 value which penalises excessive use of such features which do not correlate with the output data.\nLet\u2019s understand this with an example:\n \n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fadjr.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012969&Signature=XVbzzwSrKtbVJ%2BoQGhUWTuDNmmjGAPcL4Z6CURenY4Zdf%2BBv%2FOQw08zelIe1r4jP7%2FPzh3pJp0EKGC%2BFQYrQiQ6ERu11fe3Q4b%2B%2B%2FLjVvd33fwG0ZRhfZUewXmaAeIX8tiV8XB8yAbrW%2BDPwZBh5PaZggZfRPYJa2KtD6Tzu5iT8ybFYdRhWzwwrhfDSziKJ7%2BO6Xz4RjCHFAzaBhFMIMqELk9yAiojWq%2Bomg7azf8Tae8umUMtjzXi1GyDvjTvjaA2VI0g20wrq8v4I%2BKtXFkro%2Fu6LDudgC62G4jfgScpU94APsTWEs69Kf1%2BWAvj3WZwKz%2FsiCkA56po6FwWaEA%3D%3D\" width=\"300\">\nWe can see that R2 always increases with an increase in the number of independent variables.\nThus, it doesn\u2019t give a better picture and so we need Adjusted R2 value to keep this in check.\nMathematically, it is calculated as:\n                                        <img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fadjr2.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012987&Signature=IPfkYIb9NIlhCU5qyQe3oDl1HDT9RPIEMN0P3%2FUV58%2Fl%2BE7XCMqXoEL%2FwydGJAKb8l5d0%2BWctq%2BAD3KVnLECxZanw3%2FSKcGLIvuTk9bgm1UwIm250S%2Fua1FRSuf9Z2a19dzhBpqvdhwNdG%2Fbdfn3McZj0POectJpQWARXCBvAl9uz96CPiaAIx6zjgBJ9%2B7Ud%2BBBEi5xJShvxFpjunBQPBotxYi%2BVVqSxDr2JDmD1V9n1gjka2AvhwyBXdyYIA4DRgELLqQXeutefALx%2Bov1b7ANu8akL4gsw9kYA0JztYjnCLQclXzJb0BWXprh%2B%2BJZ0r5vAy9wntoAfZkwzdWh0A%3D%3D\">\nIn the equation above, when p = 0, we can see that adjusted R2 becomes equal to R2.\nThus, adjusted R2  will always be less than or equal to R2, and it penalises the excess of independent variables which do not affect the dependent variable.","53aaee07":"How do we interpret these coefficients? \nIf we look at the coefficients, in our case all are positive\n\nBut **<a>for assumption<a>** if the coefficient for the newspaper spends is negative. It means that the money spent for newspaper advertisements is not contributing in a positive way to the sales.\n\nA lot of the information we have been reviewing piece-by-piece is available in the model summary output:","b7b53d38":"What does the encoding say?\n* rural is encoded as Targeted Geography_suburban=0 and Targeted Geography_urban=0\n* suburban is encoded as Targeted Geography_suburban=1 and Targeted Geography_urban=0\n* urban is encoded as Targeted Geography_suburban=0 and Targeted Geography_urban=1\n\nNow the question is: **Why have we used two dummy columns instead of three?**\n\nBecause using only two dummy columns, we can capture the information of all the 3 columns. For example, if the value for Targeted Geography_urban as well as Targeted Geography_rural is 0, it automatically means that the data belongs to Targeted Geography_suburban.\n\nThis is called handling the **dummy variable trap**. If there are N dummy variable columns, then the same information can be conveyed by N-1 columns.\nLet's include the two new dummy variables in the model:","d45090ad":"# <a>Model Confidence<\/a>\n\n**Question:** Is linear regression a low bias\/high variance model or a high bias\/low variance model?\n\n**Answer:** It's a High bias\/low variance model. Even after repeated sampling, the best fit line will stay roughly in the same position (low variance), but the average of the models created after repeated sampling won't do a great job in capturing the perfect relationship (high bias). Low variance is helpful when we don't have less training data! \n\nIf the model has calculated a 95% confidence for our model coefficients, it can be interpreted as follows: \nIf the population from which this sample is drawn, is **sampled 100 times**, then approximately **95 (out of 100) of those confidence intervals** shall contain the \"true\" coefficients.","8d8c537f":"**What are the features?**\n\n* TV: Advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n* Radio: Advertising dollars spent on Radio\n* Newspaper: Advertising dollars spent on Newspaper\n\n**What is the response?**\n\nSales: sales of a single product in a given market (in thousands of widgets)","3832b893":"The general equation of a straight line is:$$\ud835\udc66={mx+b}$$\nIt means that if we have the value of m and b, we can predict all the values of y for corresponding x.\nDuring construction of a Linear Regression Model, the computer tries to calculate the values of m and b to get a straight line.\nBut the question is:\n###### How Do you Know this is the best fit line?\nThe best fit line is obtained by minimizing the _residual_.\nResidual is the distance between the observed value of dependent variable and predicted value, as shown below:\n![image](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1509778%2Fresi1.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601031108&Signature=dDV5%2F%2F%2BuLaDtG5uxQQq1acODOl5D9DX%2F56KFvvfdu0q6dkG9p45tAokxhM5yPj7RQoFvOsGw8roriRAXzm3BSyWB7EZ3VXcjH5AU1Vl%2FUXXxCV7Um5KRljj1BulULy9Tjnmgnyz4V19WHmErYjzh9Q5Db9WZuKeaTbqL%2B0h%2BzP6cQ3y2CAQ5ODRapNYD724GmafH5AAGKeAV87w5FBYmMeQkWeFfESDdDP1DH3RCBXuIEdLDjaoOHxTxDNVhag%2BzmB6F1l62dHdhaR0UCTLDrgnoAYQxkiiyPD8dz8gfeQVnVSHpRo5tdUkldSCmirJ6ICvcWxWGBFIdIOw9HZz6AA%3D%3D)\n\nMathematically, Residual is: $$r={y-(mx+b)}$$\nHence, the sum of the square of residuals is:\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FsumOfResiduals.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012182&Signature=Fy%2FNScajJHY3JLTk0zA2Ad%2F9CXt9lU%2BW7zhAwo6nwm6%2BmJIXH1C9VR02QfPbZNJpGHeMiS%2BAtQOJK2NqIHVv6GWDl5o%2BlEKpeD%2B6tV7JS7MVpA%2FnQCJvXLgt2ErzLs8j%2FaevbBKU%2F96Ta7qkeL5EWzPGl0ucN%2FSiSDFc9o5EzAcjG5aA9SsxkaF9Dh1%2B1QWQUQDpxZjItRXYf%2FmEWpAwYQuvxNyck6qTafqFLeUkhpEjGaqzik984HFfDiUQ6pzX%2B5gRaWZQVJPu3fF%2Bj8%2FYGM7PBrIiPKSAbsDzcFkQ4tvSNLXNlRFHqTTbrK2rZT14gWOtL5cqRiztE0dSu3S%2Fzg%3D%3D\">\n\nAs we can see that the residual is both a function of m and b, so differentiating partially with respect to m and b will give us:\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FpartialDerivatives.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012204&Signature=ISZiTAmeZxNErI1l8C3TMxI62TwhtVSoxHBGVIPVfF6H4YYiryoBpd9XNqduBoYZCnin3dAqaQy0jQRXHQ3%2FYTQBo7A0WzwNR6%2Fszo1eMlWuJ8gUao88vBooDM%2BfiHj3DlQLKDQFqIP8krK4t4gf73oT9VR3AkXJxT%2Bm1RTvqISOb2KnRIgJRf%2FQ4AjLONNqSIv637ItvQmHtZ2wLz224zMQINadJw1U59DE1PQeQ0KcPaalXjV96jVrq39meX1tN9chXfMbfy7jay7PVCccKH6QdTORFbXESAwFNeURsdkg3tBpiLMMFwp1ogI%2BXer2srD41cl1skI4IKxL%2Fglhug%3D%3D\">\n\nFor getting the best fit line, residual should be minimum. The minima of a function occurs where the derivative=0. So, equating our corresponding derivatives to 0, we get:\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fminima.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012225&Signature=IH8h3wRFG8Byg20GVKLIYTdmfPy1bcANxnYCpujL3V925q9DqCfylOUyhaeDgDSZ9m7%2FsqP%2FeMhyhMQYL1Vw09S1BxcFN4FmEOrQrwXT25uTqVlNQ9p7WNXV88svwu9sIDyufGszumq73EcOwLa%2By9j%2FxQEQSWFye71LiYr4v1OyCq%2BYD0M0FFCZ1ibmutT9d7KMje3ZNxt8A7PkXkt7Ry5538K5c9ZcU3cQ6dnaByzNQ8VIQHAGlgKf20HN3%2BauPbYAvnzstKnoJveZjszDKk9JPAWPy5Y0uUuWHJa0SfKbJqH0BQ9iFk9O9iMKiF7uB%2BM7JTUB9lnDxvAdwVnS%2BQ%3D%3D\">\n\nThis same equation can be written in matrix form as:\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fmatrix1.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012241&Signature=eEBOOHAS3iT8lrlfbkMPgBc2KZPW1lrCqRZqMILEn5jMcdsH5TLjQDlRgzP%2FOzCh%2B3kfA%2BfGnevRZ0DhW5ihbIbRhOlYJll4ou2p%2FkAskY%2BDJ8r%2B6vz%2F6FTBlyX7yCTZy%2FH%2FSY2hLudkBB96aHgRXjgRaVL0IJIXqyLN9Rti0zNmURQPsXpy8SeM5rSOWlFhfG4Jedqw4uROHBD8Q%2FinC9UTE5KXqT9nyKNjh62C0wvQO4p0LWF1u8ryIXY7JX%2BQaeTDDixphSDKqJPEOHFfeP8TppMf%2BoxvLxSZYS24UFGgKGb5G4frCkLANJ2dfV0nbn2yDSSFrTHAVuy%2FL5WiVA%3D%3D\">\n\nIdeally, if we'd have an equation of one dependent and one independent variable the minima will look as follows:\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2Fminima2.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601012253&Signature=MaV3ggJB7knQmH7DJUOfzgYh%2FweP68dZd0aK9QmuVlBzXKaC27GqUSVd9gWybmDH1jn5la1FeNG7xDvVvm0%2ByWBMb6IWdM4s%2B4UfZdxWfhryIn55YoEOQ3RnrOaIdTIfYaC%2FQnr3tPmibUb55Czy8Mems5e1eNm1%2FVt3qNHYHvWUN7FkDf%2BmpU%2Bk0U6KlqXXoxnUfw%2BSMWHBsZ33ft0TPxv6CIcnCQSLYHvgHc4tJJq%2BVDpnqt1ngXCGUawxrtlxN2mpz0%2BtYrFT3n%2FEA5a8Kv8p0KNJklHQwH3WyWwJb6BGSguBE%2FmBYjEPyy1108PlYbRmNsOmNsS5ts5zjaF50w%3D%3D\">\n\nBut as the residual's minima is dependent on two variables m and b, it becomes a _Paraboloid_ and the appropriate m and b are calculated using _*Gradient Descent*_ as shown below:\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1508405%2FGradientDescent.gif?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601010889&Signature=szxNJqa%2FZipKhnsvxAgUh23JRG8Bvrg9QxEgZNafTarSb8tHRThJgSbvRfVtuv0vaQFhkpFbdMNRTn9Bn9ofa6GIT0F5BxMWk5JYgVsk0%2B0dYcKcKoKsyOExb65K46mL8WIw3Toz5CQD2%2FWVZK3ZdGPKA8q0m7u4TPHeW2uW5BkuUAkKdbtaiz0u6hdz%2F9lDqLzZxouoyhgbt8T9A%2B9V%2BO3aNh4WDl1n29Ae44ModXECNl13W6NeGNlDVnhflqg1i9IrDXARQRInyDzx6QfMKAiVxv4NtAdImzmzGDX6T%2BHd771JBNDggIDJacd8wiQNLvsc9q%2FMr5L2jj3VyS9%2Blg%3D%3D\"> \n\n","51c10784":"What are the things to be learnt from this summary?\n\n- TV and Radio have positive **p-values**, whereas Newspaper has a negative one. Hence, we can reject the null hypothesis for TV and Radio that there is no relation between those features and Sales, but we fail to reject the null hypothesis for Newspaper that there is no relationship between newspaper spends and sales.\n- The expenses on bot TV ,Radio and Newspaper ads are**positively associated** with Sales.\n- This model has a higher value of **R-squared** (0.903) than the previous model, which means that this model explains more variance and provides a better fit to the data than a model that only includes the TV","86ee5131":"### <a>****Latest version updated with Regularization part****<\/a>","b8ac3c88":"How do we interpret the coefficient for **IsLarge**? For a given TV\/Radio\/Newspaper ad expenditure if the average sales decreases by 27.02 widgets, it\u2019s considered as a large market.\n\nWhat if the 0\/1encoding is reversed? Still, the value of the coefficient shall be same, the only difference being the sign. It\u2019ll be a postive number instead of negative.","0634f559":"# Multi- Collinearity\n\n**Origin of the word:** The word multi-collinearity consists of two words:Multi, meaning multiple, and Collinear, meaning being linearly dependent on each other. \n\nFor e.g.,\n\t\tLet\u2019s consider this equation\n\t\t$ a+b=1  => b=1-a $\n        \nIt means that \u2018b\u2019 can be represented in terms of \u2018a\u2019 i.e., if the value of \u2018a\u2019 \t\tchanges, automatically the value of \u2018b\u2019 will also change. This equation \t\tdenotes a simple linear relationship among two variable","82ab12bc":"# How Well Does the Model Fit the data?\n\nOne of the most generic way to evaluate the fit of a linear model is by computing the **R-squared** value. R-squared explains the**proportion of variance**, i.e., the proportion of variance in the observed data which the model explains, or the reduction in error over the **null model**. (A null model only predicts the mean of all the observed responses, and thus it only has an intercept and no slope.)\n\nThe value of R-squared lies between 0 and 1. A value closer to 1 is better as it means that more variance is explained by the model.","f0635f1b":"So it looks like our model r2 score is less on the test data.\n\nLet's see if our model is overfitting our training data.","455bdffb":"# Detection\n* __Correlation Matrices and Plots:__ for correlation between all the X variables.\n \n <a>Pearson Corelation heatmap<\/a>\n        This plot shows the extent of correlation between the independent variable. Generally, a correlation greater than 0.9 or less than -0.9 is to be avoided.\n    <img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1510176%2Fhm.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601042318&Signature=e4b6PC0EihHfLUxRnyJarVsY6M999dg1u5hn8Xut7wCkOkDqSKPppBl9oXsWTVnulGHCz8HheeI8JFF8F0wKc%2BcXndwEX0hUfoNETVOaDwLh4Ms5Wdoo0GW%2FwzeKScY3G0RRrb6hTvkfqwIP4o5IDVUJbGVdP%2FFAvrifxvQZe5oEDiXlfJaWMlLnMrVYfrWy8OrO8QmqYDj7JCIUM31SFseltH%2F75SBq3%2FYiWby%2BhqlC8d5qe2PwAAchvoq%2BESi6A4mei49jzCzDSe66dd3YT1nHpXEiKKbHY4G574%2BUSxOmm3TscHkUf9dgf2o3U3%2FCa%2FL5s%2BiYpOeUDtpzp%2FN9RA%3D%3D\" width=\"500\">\n    \n\n\n","7cb577b7":"<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F888192%2F1511964%2Fgeneqn.jpeg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601466452&Signature=YgS9kQam5Wr4uSsfLi1BLVP4gvdAwUatDG3axoVOonHP43MlkpSTUn7aYMaF2rnRvP0UfuOCUzLdjop60NUm1fyZE4M4k7EFUpoflje5%2Ffcz%2BgLIiYEuVKObzajy3CF8IUGhjbkWJ0TiDxa6uIg0QqwhMt5KZ0BT%2BYv7%2Bc48Sj9IA1ox%2BucrrOUkpXH4NQID%2FQKZWgmsg1yg8%2BzqEpxVW1mYmM1%2FvhLjbbgh56E8sv8fyEEo%2BzNjWj%2BZzXYbIesD7WEBe4saHL2SH3DglIXU16%2FvozG6bRiq6PHwkQeGQlX3V7sBTCxenIHhf9QvKebV591t94ltLrFJNffoU%2Flqqw%3D%3D\">"}}