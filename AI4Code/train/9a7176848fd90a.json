{"cell_type":{"ec7fe393":"code","f790fd17":"code","5091a69b":"code","d1641d13":"code","b33bd480":"code","d6b0b001":"code","b9899358":"code","1bb2af73":"code","28be2f38":"code","15053be2":"code","255ff88f":"code","b5ee7211":"code","cecdf39f":"code","063690b0":"code","d52dcad0":"markdown","45892eb7":"markdown","5b5f112b":"markdown","08e1aac5":"markdown","2665666d":"markdown","015f6db2":"markdown","939a506e":"markdown"},"source":{"ec7fe393":"import os\nimport pandas as pd\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nos.listdir('..\/input')","f790fd17":"print('We have {} files in dataset'.format(len(os.listdir('..\/input\/understanding_cloud_organization\/train_images\/'))))","5091a69b":"# Reading the training dataset\ndf = pd.read_csv('..\/input\/understanding_cloud_organization\/train.csv')\ndf.tail()","d1641d13":"# Split the labels from the Images ID\nnew = df.Image_Label.str.split('_', expand=True).rename(columns={0:'id',1:'labels'})\ndf['id']=new['id']\ndf['labels']=new['labels']\ndf.head()","b33bd480":"# All individual labels\nlabels_counts = df.labels.value_counts()\nlabels_counts","d6b0b001":"print('We have {} NaN classes'.format(df.EncodedPixels.isna().sum()))","b9899358":"# Plotting the nan class occurance\nvalue_count = df[df.EncodedPixels.isna()]['labels'].value_counts()\nvalue_count.plot.bar()","1bb2af73":"# Plotting the classes occurances\nnon_nan_labels = labels_counts - value_count\nnon_nan_labels.plot.bar()","28be2f38":"def np_resize(img, input_shape):\n    \"\"\"\n    Reshape a numpy array, which is input_shape=(height, width), \n    as opposed to input_shape=(width, height) for cv2\n    \"\"\"\n    height, width = input_shape\n    return cv2.resize(img, (width, height))\n    \ndef mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle2mask(rle, input_shape):\n    width, height = input_shape[:2]\n    \n    mask= np.zeros( width*height ).astype(np.uint8)\n    \n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]\n    lengths = array[1::2]\n\n    current_position = 0\n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n        current_position += lengths[index]\n        \n    return mask.reshape(height, width).T\n\ndef build_masks(rles, input_shape, reshape=None):\n    depth = len(rles)\n    if reshape is None:\n        masks = np.zeros((*input_shape, depth))\n    else:\n        masks = np.zeros((*reshape, depth))\n    \n    for i, rle in enumerate(rles):\n        if type(rle) is str:\n            if reshape is None:\n                masks[:, :, i] = rle2mask(rle, input_shape)\n            else:\n                mask = rle2mask(rle, input_shape)\n                reshaped_mask = np_resize(mask, reshape)\n                masks[:, :, i] = reshaped_mask\n    \n    return masks\n\ndef build_rles(masks, reshape=None):\n    width, height, depth = masks.shape\n    \n    rles = []\n    \n    for i in range(depth):\n        mask = masks[:, :, i]\n        \n        if reshape:\n            mask = mask.astype(np.float32)\n            mask = np_resize(mask, reshape).astype(np.int64)\n        \n        rle = mask2rle(mask)\n        rles.append(rle)\n        \n    return rles","15053be2":"os.listdir('..\/input\/understanding_cloud_organization\/train_images\/')[:10]","255ff88f":"sample_filename = '8db703a.jpg'\nsample_image_df = df[df['id'] == sample_filename]\nsample_path = f\"..\/input\/understanding_cloud_organization\/train_images\/{sample_image_df['id'].iloc[0]}\"\nsample_img = cv2.imread(sample_path)\nsample_rles = sample_image_df['EncodedPixels'].values\nsample_masks = build_masks(sample_rles, input_shape=(1400, 2100))\n\nfig, axs = plt.subplots(5, figsize=(12, 12))\naxs[0].imshow(sample_img, cmap='gray')\naxs[0].axis('off')\n\nfor i in range(4):\n    axs[i+1].imshow(sample_masks[:, :, i])\n#     axs[i+1].axis('off')","b5ee7211":"maskid=2\nymin = sample_masks[:,:,maskid].argmax(axis=1).argmax()\nxmin = sample_masks[:,:,maskid].argmax(axis=0).argmax()\nymax = sample_masks[ymin:,xmin:,maskid].argmin(axis=1).argmin()+ymin\nxmax = sample_masks[ymin:,xmin:,maskid].argmin(axis=0).argmin()+xmin\n\nprint(xmin, ymin, xmax, ymax)","cecdf39f":"sample_masks[ymin:,:,maskid].argmin(axis=1).shape","063690b0":"# (sample_img[ymin:ymax, xmin:xmax], cmap='gray')\n# cv2.rectangle(sample_img, (xmin, ymin), (xmax, ymax), (0,255,0), 5)\nplt.imshow(sample_img[ymin:ymax,xmin:xmax], cmap='gray')\nplt.axis('off')","d52dcad0":"# Basic EDA of Satellite clouds\n\n![Competition label](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/MaxPlanck\/Teaser_AnimationwLabels.gif)\n\nHere I am performing a simple _Exploratory data analysis_ on [Understanding Clouds from Satellite Images](https:\/\/www.kaggle.com\/c\/understanding_cloud_organization) dataset provided with the same contest on [Kaggle](https:\/\/www.kaggle.com\/).\n> The images were downloaded from [NASA Worldview](https:\/\/worldview.earthdata.nasa.gov\/). Three regions, spanning 21 degrees longitude and 14 degrees latitude, were chosen. The true-color images were taken from two polar-orbiting satellites, TERRA and AQUA, each of which pass a specific region once a day. Due to the small footprint of the imager (MODIS) on board these satellites, an image might be stitched together from two orbits. The remaining area, which has not been covered by two succeeding orbits, is marked black.","45892eb7":"Now lets verify our preposition here","5b5f112b":"## What's under those masks\n\nLets now decode the masks provided in the training dataset and view what's beneath those masks.\n\nFor this I would like to thank [xhlulu](https:\/\/www.kaggle.com\/xhlulu) for making his awesome kernels publically available. \n","08e1aac5":"This is all that I thought would be required for understanding the dataset. But if you would like to help improve the kernel, you are most welcome to contribute to this kernel. If there's anything  else you would want me to add, do comment.","2665666d":"We can clearly see that the dataset contains the images has been mapped with all the labels and each image has been discretely provided us all the segmetation maps of the images\nThe masks of the images have been encoded and then fed into the training file. For this we do not have seperate mask images. So images without some specific class has _NaN_ in its place. ","015f6db2":"We see 4 seperate classes:\n1. Sugar\n2. Fish\n3. Gravel\n4. Flower\n\nAnd as each image has been given all the labels, we have 5546 labels for each classes","939a506e":"## Exploring files in dataset\n\nSo we have the data split into the training and the testing dataset. Lets now explore the dataset more with numbers"}}