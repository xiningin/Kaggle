{"cell_type":{"7bffd2f6":"code","a254b173":"code","43fa03cb":"code","a0938955":"code","d8f6d5a8":"code","053da3c9":"code","491f29e6":"code","f045eddc":"code","6bd5b218":"code","b4df9abe":"code","a4830990":"code","30da768b":"code","cad94980":"code","45cd154e":"code","473a9d87":"code","1b74afa3":"code","1baa8ccd":"code","196e03d7":"code","c819b5a9":"code","0be898eb":"code","93f5e324":"code","7b78ea3e":"code","b804010b":"code","fcd34ab4":"code","7f19daee":"code","31cb4332":"code","4545344b":"code","d2d0352b":"code","94752008":"code","d2a02bec":"code","49a83ad7":"code","ee7771d3":"markdown","d0dbc83a":"markdown","6e4c1b39":"markdown","8911db89":"markdown","1684b405":"markdown","d3ea740f":"markdown","c3a82f9f":"markdown","0a29b896":"markdown","d5be9f66":"markdown","4f8d9160":"markdown","a7142d95":"markdown","08f9d695":"markdown","08378d45":"markdown","1eaa11c4":"markdown","678cee1a":"markdown","e74b4ea6":"markdown","e4a10ccc":"markdown","855d334a":"markdown","887e1b25":"markdown","f808992f":"markdown","c30cf8cf":"markdown","072bb1da":"markdown","71c2987e":"markdown","363c6918":"markdown","3df59e51":"markdown","2403d33c":"markdown","fe718ba5":"markdown","f69a9cae":"markdown","0548ac32":"markdown","78065424":"markdown","70278f8e":"markdown","c027d4ac":"markdown","d5cbf63c":"markdown","27bae8df":"markdown","6a281826":"markdown","dd369ca6":"markdown","7cfd6e2c":"markdown","7d087541":"markdown","08469ce8":"markdown"},"source":{"7bffd2f6":"import random, os, warnings, math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import optimizers, losses, metrics, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenizer\n\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 0\nseed_everything(seed)\nsns.set(style='whitegrid')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', 150)","a254b173":"# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","43fa03cb":"train_filepath = '\/kaggle\/input\/commonlitreadabilityprize\/train.csv'\ntest_filepath = '\/kaggle\/input\/commonlitreadabilityprize\/test.csv'\n\ntrain = pd.read_csv(train_filepath)\ntest = pd.read_csv(test_filepath)\n\nprint(f'Train samples: {len(train)}')\ndisplay(train.head())\n\nprint(f'Test samples: {len(test)}')\ndisplay(test.head())\n\n# removing unused columns\ntrain.drop(['url_legal', 'license'], axis=1, inplace=True)\ntest.drop(['url_legal', 'license'], axis=1, inplace=True)","a0938955":"BATCH_SIZE = 8 * REPLICAS\nLEARNING_RATE = 1e-5 * REPLICAS\nEPOCHS = 35\nES_PATIENCE = 7\nPATIENCE = 2\nN_FOLDS = 6\nSEQ_LEN = 256 #300\nBASE_MODEL = '\/kaggle\/input\/huggingface-roberta\/roberta-base\/' #bert\u30e2\u30c7\u30eb","d8f6d5a8":"# Datasets utility functions\ndef custom_standardization(text): #@01\n    text = text.lower() # if encoder is uncased\n    text = text.strip()\n    return text\n\n\ndef sample_target(features, target):\n    mean, stddev = target\n    sampled_target = tf.random.normal([], mean=tf.cast(mean, dtype=tf.float32), \n                                      stddev=tf.cast(stddev, dtype=tf.float32), dtype=tf.float32)\n    \n    return (features, sampled_target)\n    \n\ndef get_dataset(pandas_df, tokenizer, labeled=True, ordered=False, repeated=False,  #train\u4e2d\u306b\u547c\u3070\u308c\u308b\u3002\n                is_sampled=False, batch_size=32, seq_len=128):\n    \"\"\"\n       \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u307e\u305f\u306f\u63a8\u8ad6\u306e\u6e96\u5099\u304c\u3067\u304d\u3066\u3044\u308bTensorflow\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8fd4\u3057\u307e\u3059\u3002\n    \"\"\"\n    text = [custom_standardization(text) for text in pandas_df['excerpt']] #@01\n    \n    # Tokenize inputs\n    tokenized_inputs = tokenizer(text, max_length=seq_len, truncation=True, \n                                 padding='max_length', return_tensors='tf')\n    \n    if labeled:\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']}, \n                                                      (pandas_df['target'], pandas_df['standard_error'])))\n        if is_sampled:\n            dataset = dataset.map(sample_target, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']})\n        \n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n\n\ndef plot_metrics(history):\n    metric_list = list(history.keys())\n    size = len(metric_list)\/\/2\n    fig, axes = plt.subplots(size, 1, sharex='col', figsize=(20, size * 5))\n    axes = axes.flatten()\n    \n    for index in range(len(metric_list)\/\/2):\n        metric_name = metric_list[index]\n        val_metric_name = metric_list[index+size]\n        axes[index].plot(history[metric_name], label='Train %s' % metric_name)\n        axes[index].plot(history[val_metric_name], label='Validation %s' % metric_name)\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric_name)\n\n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()","053da3c9":"display(train.head())","491f29e6":"display(train.sort_values(by=['target']).head())","f045eddc":"display(train.sort_values(by=['target'], ascending=False).head())","6bd5b218":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['target'], ax=ax)\nplt.show()","b4df9abe":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['standard_error'], ax=ax)\nplt.show()","a4830990":"print(f\"standard_error values >= than 0.4: {len(train[train['standard_error'] >= 0.4])}\")\nprint(f\"standard_error values < than 0.4: {len(train[train['standard_error'] < 0.4])}\")","30da768b":"fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nsns.scatterplot(x=train['target'], y=train['standard_error'], s=10, color=\".15\")\nsns.kdeplot(x=train['target'], y=train['standard_error'], levels=5, color=\"r\", linewidths=1)\nplt.ylim([0.4, None])\nplt.show()","cad94980":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL) #@02\n#\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306fAutoTokenizer.from_pretrained()\u3067\u547c\u3073\u51fa\u305b\u307e\u3059\u3002bert-base-japanese\u306fWikipedia\u306e\u65e5\u672c\u8a9e\u7248\u3067\u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\u306e\u30b5\u30d6\u30ef\u30fc\u30c9\u8f9e\u66f8\u3068\u306a\u308a\u307e\u3059\n#\u82f1\u8a9e\u306a\u306e\u3067roberta-base\u3092\u3064\u304b\u3063\u3066\u3044\u308b\n\ntrain['excerpt_len'] = train['excerpt'].apply(lambda x : len(x))\ntrain['excerpt_wordCnt'] = train['excerpt'].apply(lambda x : len(x.split(' ')))\ntrain['excerpt_tokenCnt'] = train['excerpt'].apply(lambda x : len(tokenizer.encode(x, add_special_tokens=False)))\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['excerpt_len'], ax=ax).set_title('Excerpt length')\nplt.show()","45cd154e":"train['excerpt_wordCnt'] #\u7a7a\u767d\u3067\u533a\u5207\u3063\u3066\u307f\u305f\u5834\u5408\u306e\u5358\u8a9e\u6570","473a9d87":"train['excerpt_tokenCnt'] #tokenizer\u30a8\u30f3\u30b3\u30fc\u30c9\u3057\u305f\u5834\u5408\u306e\u6570\u3002\u524d\u8005\u3088\u308a\u591a\u3044\u306a\u3042\u3002","1b74afa3":"seq_len=80\ntext ='When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landsca'\ntokenized_inputs = tokenizer(text, max_length=seq_len, truncation=True, \n                                 padding='max_length', return_tensors='tf')\ntokenized_inputs","1baa8ccd":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['excerpt_wordCnt'], ax=ax).set_title('Excerpt word count')\nplt.show()","196e03d7":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['excerpt_tokenCnt'], ax=ax).set_title('Excerpt token count')\nplt.show()","c819b5a9":"def model_fn(encoder, seq_len=256):\n    #tensorflow.keras.layers as L\n    input_ids = L.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n    input_attention_mask = L.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    outputs = encoder({'input_ids': input_ids, \n                       'attention_mask': input_attention_mask})\n    \n    model = Model(inputs=[input_ids, input_attention_mask], outputs=outputs) #input\u306f2\u3064\u3067\u3044\u3044\u306e\u304b\u3002\n\n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=optimizer, \n                  loss=losses.MeanSquaredError(),  #\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\n                  metrics=[metrics.RootMeanSquaredError()]) #y_true\u3068y_pred\u306e\u9593\u306e\u4e8c\u4e57\u5e73\u5747\u5e73\u65b9\u6839\u8aa4\u5dee\u30e1\u30c8\u30ea\u30c3\u30af\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\n    \n    return model\n\n\nwith strategy.scope():  \n    encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n    model = model_fn(encoder, SEQ_LEN) #bert\u30e2\u30c7\u30eb\u3092\u4f5c\u3063\u3066\u3044\u308b\u3002\n    \nmodel.summary()","0be898eb":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nskf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\noof_pred = []; oof_labels = []; history_list = []; test_pred = []\n\nfor fold,(idxT, idxV) in enumerate(skf.split(train)):\n    if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    print(f'\\nFOLD: {fold+1}')\n    print(f'TRAIN: {len(idxT)} VALID: {len(idxV)}')\n\n    #import tensorflow.keras.backend as K\n    K.clear_session()\n    with strategy.scope():\n        encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1) #TFAutoModelForSequenceClassification\u306f\u3001\u4e0d\u660e\u3060\u304cencoder\u306b\u3057\u3066\u3044\u308b\n        model = model_fn(encoder, SEQ_LEN)\n\n#\u8b0e\u3067\u3059\u3002\n#from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n#from transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenize\n\n    model_path = f'model_{fold}.h5'\n    es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', \n                       patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n    checkpoint = ModelCheckpoint(model_path, monitor='val_root_mean_squared_error', mode='min', \n                                 save_best_only=True, save_weights_only=True)\n\n    # Train\n    history = model.fit(x=get_dataset(train.loc[idxT], tokenizer, repeated=True, is_sampled=True, #get_dataset\u3092\u547c\u3093\u3067\u3044\u308b\u3002\u305d\u306e\u4e2d\u3067excerpt\u3092\u62bd\u51fa\u3057\u3066\u3044\u308b\u3002\n                                      batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        validation_data=get_dataset(train.loc[idxV], tokenizer, ordered=True, \n                                                    batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        steps_per_epoch=50, \n                        callbacks=[es, checkpoint], \n                        epochs=EPOCHS,  \n                        verbose=2).history\n      \n    history_list.append(history)\n    # Save last model weights\n    model.load_weights(model_path)\n    \n    # Results\n    print(f\"#### FOLD {fold+1} OOF RMSE = {np.min(history['val_root_mean_squared_error']):.4f}\")\n\n    # OOF predictions\n    valid_ds = get_dataset(train.loc[idxV], tokenizer, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    oof_labels.append([target[0].numpy() for sample, target in iter(valid_ds.unbatch())])\n    x_oof = valid_ds.map(lambda sample, target: sample)\n    oof_pred.append(model.predict(x_oof)['logits']) #predict\u306eoutput\u306eligits\u3060\u3051\u3092\u62bd\u51fa\n\n    # Test predictions\n    test_ds = get_dataset(test, tokenizer, labeled=False, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    x_test = test_ds.map(lambda sample: sample)\n    test_pred.append(model.predict(x_test)['logits'])","93f5e324":"TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1) ","7b78ea3e":"get_dataset(train.loc[idxT], tokenizer, repeated=True, is_sampled=True, #\n                                      batch_size=BATCH_SIZE, seq_len=SEQ_LEN),","b804010b":"get_dataset(train.loc[idxV], tokenizer, ordered=True, \n                                                    batch_size=BATCH_SIZE, seq_len=SEQ_LEN)","fcd34ab4":"model.predict(x_oof)[0][0]","7f19daee":"x_oof #valid_ds.map(lambda sample, target: sample)","31cb4332":"test_ds","4545344b":"for fold, history in enumerate(history_list):\n    print(f'\\nFOLD: {fold+1}')\n    plot_metrics(history)","d2d0352b":"y_true = np.concatenate(oof_labels)\ny_preds = np.concatenate(oof_pred)\n\n\nfor fold, history in enumerate(history_list):\n    print(f\"FOLD {fold+1} RMSE: {np.min(history['val_root_mean_squared_error']):.4f}\")\n    \nprint(f'OOF RMSE: {mean_squared_error(y_true, y_preds, squared=False):.4f}')","94752008":"preds_df = pd.DataFrame({'Label': y_true, 'Prediction': y_preds[:,0]})\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(preds_df['Label'], ax=ax, label='Label')\nsns.distplot(preds_df['Prediction'], ax=ax, label='Prediction')\nax.legend()\nplt.show()","d2a02bec":"sns.jointplot(data=preds_df, x='Label', y='Prediction', kind='reg', height=10)\nplt.show()","49a83ad7":"submission = test[['id']]\nsubmission['target'] = np.mean(test_pred, axis=0)\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission.head(10))","ee7771d3":"## \u30e9\u30d9\u30eb\u306e\u914d\u5e03","d0dbc83a":"# \u4e88\u6e2c","6e4c1b39":"![image.png](attachment:733eaeb8-3f65-4aad-adf9-fc1e53985a95.png)\n\n","8911db89":"### Hardware configuration","1684b405":"### 5 \u3064\u306e\u6700\u3082\u9ad8\u3044\u300c\u30bf\u30fc\u30b2\u30c3\u30c8\u300d\u5024\u306e\u4f8b","d3ea740f":"\u4f55\u304c\u3069\u3046\u306a\u308b\u3068\u3053\u306e\u6570\u5b57\u306b\u306a\u308b\u306e\u304b\u3002\u3002\u3002","c3a82f9f":"\u3053\u306e\u30b3\u30fc\u30c9\u304c\u308f\u304b\u3063\u305f\u3053\u308d\u306b\u306f\u3001\u30b3\u30f3\u30da\u304c\u7d42\u308f\u3063\u3066\u3044\u308b\u6c17\u304c\u3059\u308b\u3002\n\n![image.png](attachment:59f74cad-70c8-4308-95d2-32a689d61e15.png)","0a29b896":"\u6b63\u898f\u5206\u5e03\u3068\u306f\u8a00\u3048\u307e\u305b\u3093\u304c\u3001\uff11\uff13\uff10\uff5e\uff12\uff10\uff10\u3050\u3089\u3044\u306b\u307b\u307c\u53ce\u307e\u3063\u3066\u3044\u307e\u3059\u306d\u3002","d5be9f66":"# EDA\n\n### \u5c11\u3057\u30b5\u30f3\u30d7\u30eb\u3092\u307f\u3066\u307f\u308b","4f8d9160":"\u300c\u30bf\u30fc\u30b2\u30c3\u30c8\u300d\u5217\u306f\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3044\u307e\u3059\u304c\u3001\u6b63\u306e\u30b9\u30b3\u30a2\u3088\u308a\u3082\u8ca0\u306e\u30b9\u30b3\u30a2\u306e\u30b5\u30f3\u30d7\u30eb\u304c\u306f\u308b\u304b\u306b\u591a\u3044\u3067\u3059\u3002\u8ca0\u306e\u5024\u306f\u300c-4\u300d\u306b\u8fd1\u3065\u304d\u3001\u6b63\u306e\u5024\u306f\u300c2\u300d\u307e\u3067\u3057\u304b\u4e0a\u6607\u3057\u307e\u305b\u3093\u3002 \u3067\u3082\u3001\u306a\u306b\u3092\u610f\u5473\u3059\u308b\u306e\u304b\u3002","a7142d95":"# Training","08f9d695":"# \u30c7\u30a3\u30b9\u30ab\u30c3\u30b7\u30e7\u30f3\u30e1\u30e2\u3000\u307b\u307c\u8b0e\n\u30fb\u73fe\u5728\u3001\u307b\u3068\u3093\u3069\u306e\u6559\u80b2\u30c6\u30ad\u30b9\u30c8\u306f\u3001\u5f93\u6765\u306e\u8aad\u307f\u3084\u3059\u3055\u306e\u65b9\u6cd5\u307e\u305f\u306f\u5e02\u8ca9\u306e\u516c\u5f0f\u3092\u4f7f\u7528\u3057\u3066\u8aad\u8005\u3068\u7167\u5408\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u305f\u3060\u3057\u3001\u305d\u308c\u305e\u308c\u306b\u554f\u984c\u304c\u3042\u308a\u307e\u3059\u3002 Flesch-Kincaid Grade Level\u306e\u3088\u3046\u306a\u30c4\u30fc\u30eb\u306f\u3001\u30c6\u30ad\u30b9\u30c8\u30c7\u30b3\u30fc\u30c9\u306e\u5f31\u3044\u30d7\u30ed\u30ad\u30b7\uff08\u3064\u307e\u308a\u3001\u5358\u8a9e\u3054\u3068\u306e\u6587\u5b57\u307e\u305f\u306f\u97f3\u7bc0\uff09\u3068\u69cb\u6587\u306e\u8907\u96d1\u3055\uff08\u3064\u307e\u308a\u3001\u6587\u3054\u3068\u306e\u6570\u307e\u305f\u306f\u5358\u8a9e\uff09\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002 \u7d50\u679c\u3068\u3057\u3066\u3001\u305d\u308c\u3089\u306f\u69cb\u6210\u6982\u5ff5\u3068\u7406\u8ad6\u7684\u59a5\u5f53\u6027\u3092\u6b20\u3044\u3066\u3044\u307e\u3059\u3002 \u540c\u6642\u306b\u3001Lexile\u306a\u3069\u306e\u5e02\u8ca9\u306e\u6570\u5f0f\u306f\u3001\u30b3\u30b9\u30c8\u304c\u9ad8\u304f\u3001\u9069\u5207\u306a\u691c\u8a3c\u7814\u7a76\u304c\u4e0d\u8db3\u3057\u3066\u304a\u308a\u3001\u6570\u5f0f\u306e\u6a5f\u80fd\u304c\u516c\u958b\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u900f\u660e\u6027\u306e\u554f\u984c\u306b\u60a9\u307e\u3055\u308c\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002<br>\n\u30fb\u53c2\u52a0\u8005\u306f\u307e\u305f\u30012\u3064\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u3069\u3061\u3089\u304c\u7406\u89e3\u3057\u3084\u3059\u3044\u304b\uff08\u3064\u307e\u308a\u3001\u7406\u89e3\uff09\u30012\u3064\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u3069\u3061\u3089\u304c\u3088\u308a\u901f\u304f\u8aad\u3093\u3060\u304b\uff08\u3064\u307e\u308a\u3001\u53c2\u52a0\u8005\u304c\u30c6\u30ad\u30b9\u30c8\u3092\u8aad\u3080\u306e\u306b\u3069\u308c\u3060\u3051\u306e\u51e6\u7406\u52aa\u529b\u3092\u8cbb\u3084\u3057\u305f\u3068\u611f\u3058\u305f\u304b\uff09\u3092\u9078\u629e\u3059\u308b\u3088\u3046\u306b\u6c42\u3081\u3089\u308c\u307e\u3057\u305f\u3002 \u5f7c\u3089\u304c\u3088\u308a\u3088\u304f\u77e5\u3063\u3066\u3044\u305f2\u3064\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u3046\u3061\u3002 \u6700\u521d\u306e2\u3064\u306e\u6bd4\u8f03\u306f\u3001\u305d\u306e\u5f8c\u306e\u8aad\u307f\u3084\u3059\u3055\u306e\u5206\u6790\u3067\u4f7f\u7528\u3055\u308c\u308b\u30c7\u30fc\u30bf\u3092\u5f62\u6210\u3057\u307e\u3057\u305f\u3002<br>\n\u30fb\u8aad\u307f\u3084\u3059\u3055\u306e\u30b9\u30b3\u30a2\u3068\u306f\u4f55\u3067\u3059\u304b\uff1f\u7d20\u6674\u3089\u3057\u3044\u8cea\u554f\u3067\u3059\uff01 \u8aad\u307f\u3084\u3059\u3055\u306e\u30b9\u30b3\u30a2\u306f\u3001\u8ab0\u304b\u304c\u7279\u5b9a\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u8aad\u3080\u306e\u304c\u3069\u308c\u307b\u3069\u7c21\u5358\u304b\u3092\u793a\u3059\u6570\u5024\u3067\u3059\u3002<br>\n\u30fb\u300c\u4ed6\u306e\u591a\u304f\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306b\u306f\u3001\u6d3b\u7528\u3067\u304d\u308b\u512a\u308c\u305f\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u3042\u308a\u307e\u3059\u3002\u6295\u7968\u6570\u3068\u30d5\u30a9\u30fc\u30af\u6570\u304c\u5c11\u306a\u3044\u306e\u306b\u5f79\u7acb\u3064\u30b3\u30fc\u30c9\u3082\u3042\u308a\u307e\u3059\u304c\u3001\u5f79\u7acb\u3064\u30a2\u30a4\u30c7\u30a2\u3082\u3042\u308a\u307e\u3059\u3002\u73fe\u5728\u8003\u3048\u3066\u3044\u308b\u3082\u306e\u3068\u306f\u7570\u306a\u308b\u65b9\u6cd5\u3067\u8003\u3048\u308b\u306e\u306b\u5f79\u7acb\u3064\u30b3\u30fc\u30c9\u3082\u3042\u308a\u307e\u3059\u304b\uff1f\u300d<br>\n\u30fb\u5805\u7262\u306a\u691c\u8a3c\u30bb\u30c3\u30c8\u306e\u69cb\u7bc9\u306b\u7126\u70b9\u3092\u5f53\u3066\u308b\uff1a\u91cd\u8981\u6027\u3092\u3053\u308c\u4ee5\u4e0a\u5f37\u8abf\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u307e\u305b\u3093\u3002 \u591a\u304f\u306e\u5834\u5408\u306b\u767a\u751f\u3059\u308b\u30b7\u30a7\u30a4\u30af\u30a2\u30c3\u30d7\u3068\u6226\u3046\u305f\u3081\u306b\u3001\u5805\u7262\u306a\u691c\u8a3c\u30bb\u30c3\u30c8\u304c\u5fc5\u8981\u3067\u3059\u3002 \u6700\u826f\u306e\u65b9\u6cd5\u306f\u3001\u7af6\u6280\u306e\u65e9\u3044\u6bb5\u968e\u3067\u30d5\u30a9\u30fc\u30eb\u30c9\u3092\u751f\u6210\u3057\u3001\u305d\u308c\u3089\u3092\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u3053\u3068\u3067\u3059\u3002<br>\n\u30fbAutoNLP\u306f\u3001Hugging Face\u306e\u65b0\u3057\u3044\uff08\u6709\u6599\uff09\u30b5\u30fc\u30d3\u30b9\u3067\u3042\u308a\u3001\u7279\u5b9a\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u6570\u767e\u306e\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3067\u304d\u307e\u3059\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u30a4\u30f3\u30d5\u30e9\u30b9\u30c8\u30e9\u30af\u30c1\u30e3\u3001\u30e2\u30c7\u30eb\u306e\u9078\u629e\u3001\u5fae\u8abf\u6574\u3001\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6700\u9069\u5316\uff08\u307e\u305f\u306f\u5c55\u958b\uff09\u306b\u3064\u3044\u3066\u5fc3\u914d\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002 AutoNLP\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u3053\u3053\u3067\u30c1\u30a7\u30c3\u30af\u3057\u3066\u304f\u3060\u3055\u3044\uff1ahttps\uff1a\/\/github.com\/huggingface\/autonlp<br>\n\u30fbAutoML\u304c\u3053\u3053\u3067\u3069\u306e\u3088\u3046\u306b\u6a5f\u80fd\u3057\u3066\u3044\u308b\u304b\u3092\u5b8c\u5168\u306b\u628a\u63e1\u3059\u308b\u3053\u3068\u306f\u307e\u3060\u3067\u304d\u307e\u305b\u3093\u3002 \u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306e\u69cb\u6210\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u308f\u304b\u308b\u9650\u308a\u3001RobertaForSequenceClassification\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u30b5\u30a4\u30ba\u306f\u3001\u3059\u3079\u3066\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u69cb\u6210\u3092\u5099\u3048\u305fRoberta-Base\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u540c\u3058\u3053\u3068\u3092\u624b\u52d5\u3067\u884c\u3046\u3068\u30015\u500d\u307e\u305f\u306f6\u500d\u306e\u540c\u3058\u30e2\u30c7\u30eb\u30670.52 + rmse\u3092\u8d85\u3048\u3066\u3044\u307e\u3059\u3002 \u305d\u308c\u3067\u3001\u79c1\u306e\u5834\u5408\u306b\u3088\u308a\u6709\u671b\u3067\u3042\u308b\u304c\u3001\u3042\u306a\u305f\u306e\u30b9\u30b3\u30a2\u306b\u306f\u307b\u3069\u9060\u3044\u306e\u3067\u3001\u79c1\u306fDeBERTa\u30d9\u30fc\u30b9\u306b\u5207\u308a\u66ff\u3048\u307e\u3057\u305f\u3002<br>\n\u30fbBERT\u306f\u3001\u4eca\u65e5\u3055\u307e\u3056\u307e\u306aNLP\u30bf\u30b9\u30af\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306b\u5e83\u304f\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u6700\u3082\u30db\u30c3\u30c8\u306a\u30e2\u30c7\u30eb\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e1\u3064\u3067\u3059\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u3053\u306e\u30e2\u30c7\u30eb\u306b\u3064\u3044\u3066\u306e\u6df1\u3044\u77e5\u8b58\u3092\u5f97\u308b\u3053\u3068\u304c\u975e\u5e38\u306b\u91cd\u8981\u3067\u3059\u3002 BERT\u306b\u306f\u3001\u3044\u304f\u3064\u304b\u306e\u30d6\u30ed\u30b0\u3001\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306a\u3069\u304c\u3042\u308a\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u5b8c\u5168\u306a\u30e2\u30c7\u30eb\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u5b8c\u5168\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u306f\u975e\u5e38\u306b\u307e\u308c\u3067\u3059\u3002<br>\n\u30fb\u3053\u306e\u7af6\u4e89\u306f\u4e00\u898b\u7c21\u5358\u306b\u601d\u3048\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u305d\u308c\u3060\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u79c1\u306e\u4fee\u58eb\u8ab2\u7a0b\u3067\u306f\u3001\u4eba\u9593\u306b\u3088\u3063\u3066\u4f5c\u6210\u304a\u3088\u3073\u8a55\u4fa1\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u3044\u304f\u3064\u304b\u306e\u5074\u9762\u3092\u7814\u7a76\u3057\u307e\u3057\u305f\u3002\u3053\u308c\u306f\u901a\u5e38\u3001\u4e3b\u89b3\u7684\u306a\u65b9\u6cd5\u3067\u53ce\u96c6\u3055\u308c\u305f\u60c5\u5831\u3067\u3059\u3002<br>\n\u30fb\u7af6\u4e89\u306f\u53b3\u3057\u3044\u3067\u3059\u304c\u3001\u3042\u306a\u305f\u304c\u8a00\u53ca\u3057\u305f\u7406\u7531\u306e\u305f\u3081\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u30e9\u30d9\u30eb\u306f\u78ba\u304b\u306b\u9a12\u3005\u3057\u3044\u3067\u3059\u304c\u3001\u3042\u306a\u305f\u306e\u30e2\u30c7\u30eb\u306f\u3068\u306b\u304b\u304f\u6700\u826f\u306e\u4e2d\u9593\u70b9\u3092\u898b\u3064\u3051\u308b\u306e\u306b\u304b\u306a\u308a\u512a\u308c\u3066\u3044\u307e\u3059\u3001\u305d\u3057\u3066\u305d\u308c\u304b\u3089\u554f\u984c\u5916\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306b\u305d\u308c\u3089\u306e\u6570\u5341\u304c\u3042\u308a\u307e\u3059\u3002\u7406\u7531\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059\u3002\u554f\u984c\u3068\u30e1\u30c8\u30ea\u30af\u30b9\u306e\u4e21\u65b9\u304c\u5358\u7d14\u3059\u304e\u3066\u3001\u30c7\u30fc\u30bf\u3092\u4efb\u610f\u306e\u30cf\u30b0\u30d5\u30a7\u30a4\u30b9\u30e2\u30c7\u30eb\u306b\u5165\u529b\u3059\u308b\u3060\u3051\u3067\u3001\u3059\u3067\u306b\u9069\u5207\u306a\u7d50\u679c\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\u5de7\u5999\u306a\u30e2\u30c7\u30ea\u30f3\u30b0\u3084\u5f8c\u51e6\u7406\u306f\u5fc5\u8981\u306a\u304f\u3001\u53ef\u80fd\u6027\u3082\u3042\u308a\u307e\u305b\u3093\u3002\n\u3053\u308c\u306f\u3001\u6a5f\u80fd\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u306e\u5fc5\u8981\u6027\u306b\u3064\u306a\u304c\u308a\u307e\u3059\u3002 \u3053\u308c\u306f\u8208\u5473\u6df1\u3044\u90e8\u5206\u3067\u3059\u3002\n\u3069\u306e\u3088\u3046\u306a\u6a5f\u80fd\u304c\u5fc5\u8981\u3067\u3059\u304b\uff1f \u305d\u308c\u306f\u3042\u306a\u305f\u306e\u5e73\u5747\u7684\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u304c\u305d\u308c\u81ea\u4f53\u3067\u7406\u89e3\u3057\u3066\u3044\u306a\u3044\u3082\u306e\u3067\u3042\u308b\u306b\u9055\u3044\u3042\u308a\u307e\u305b\u3093\u3001\u305d\u3057\u3066\u79c1\u305f\u3061\u306f\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u304c\u4f55\u3092\u77e5\u3063\u3066\u3044\u308b\u306e\u304b\u672c\u5f53\u306b\u77e5\u308a\u307e\u305b\u3093\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u7a81\u7834\u53e3\u3092\u958b\u304f\u306b\u306f\u3001\u4e00\u6b69\u4e0b\u304c\u3063\u3066\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u30b9\u3092\u3055\u3089\u306b\u8abf\u3079\u3001\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u30b9\u306b\u4f55\u304c\u542b\u307e\u308c\u3066\u3044\u306a\u3044\u304b\u3092\u78ba\u8a8d\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u305d\u308c\u304c\u52dd\u3064\u65b9\u6cd5\u3067\u3059\u3002","08378d45":"![image.png](attachment:aabdb9e1-d38e-4d30-899c-ec28cb461555.png)","1eaa11c4":"# Model parameters","678cee1a":"### `\u30c8\u30fc\u30af\u30f3\u6570\uff08\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u4f7f\u7528\u5f8c\uff09","e74b4ea6":"BASE_MODEL\u306f\u5b66\u7fd2\u6e08\u30e2\u30c7\u30eb\u3002<br>\n\u30de\u30b9\u30af\u3055\u308c\u305f\u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0 (MLM) \u306e\u76ee\u7684\u3092\u4f7f\u7528\u3057\u305f\u82f1\u8a9e\u306e\u4e8b\u524d\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u6e08\u307f\u30e2\u30c7\u30eb\u3002 \u3053\u306e\u30db\u30ef\u30a4\u30c8\u30da\u30fc\u30d1\u30fc\u3067\u7d39\u4ecb\u3055\u308c\u3001\u3053\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3067\u6700\u521d\u306b\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u307e\u3057\u305f\u3002 \u3053\u306e\u30e2\u30c7\u30eb\u306f\u5927\u6587\u5b57\u3068\u5c0f\u6587\u5b57\u3092\u533a\u5225\u3057\u307e\u3059\u3002\nRoBERTa \u3092\u30ea\u30ea\u30fc\u30b9\u3059\u308b\u30c1\u30fc\u30e0\u306f\u3053\u306e\u30e2\u30c7\u30eb\u306e\u30e2\u30c7\u30eb\u30ab\u30fc\u30c9\u3092\u4f5c\u6210\u3057\u306a\u304b\u3063\u305f\u305f\u3081\u3001\u3053\u306e\u30e2\u30c7\u30eb\u30ab\u30fc\u30c9\u306f Hugging Face \u30c1\u30fc\u30e0\u306b\u3088\u3063\u3066\u4f5c\u6210\u3055\u308c\u307e\u3057\u305f\u3068\u3042\u308a\u307e\u3059\u3002<br>","e4a10ccc":"# Model","855d334a":"## \u88dc\u52a9\u6a5f\u80fd","887e1b25":"# \u5b9f\u9a13","f808992f":"# Load data","c30cf8cf":"\u3053\u3053\u3067\u666e\u901a\u306fx,y\u3092\u6e21\u3057\u3066\u5b66\u7fd2\u3055\u305b\u3066\u3044\u308b\u306f\u305a\u306a\u3093\u3067\u3059\u3051\u3069\u3002\u3053\u3053\u304c\u89e3\u3051\u308c\u3070\u308f\u304b\u308b\u306f\u305a\u3002\n\n![image.png](attachment:10eda786-9fe3-4331-b01d-7ab08cb66b45.png)","072bb1da":"\u7d50\u5c40\u3001encorder\u306b\u4f55\u3092\u3044\u308c\u3066\u3044\u308b\u306e\u304b\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u304c\u3001All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\u304c\u7b54\u3048\u304b\u306a\u3002","71c2987e":"OOF \u4e88\u6e2c\u3067\u30e2\u30c7\u30eb\u3092\u8a55\u4fa1\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f Out Of Fold \u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002K-Fold \u3092\u4f7f\u7528\u3057\u3066\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u30e2\u30c7\u30eb\u306f\u3059\u3079\u3066\u306e\u30c7\u30fc\u30bf\u3092\u8868\u793a\u3057\u3066\u3044\u307e\u3059\u3002","363c6918":"\u201d\u300cstandard_error\u300d\u5217\u306b\u306f\u3001\u300c0.4\u300d\u3088\u308a\u3082\u4f4e\u3044\u5024\u306e\u5916\u308c\u5024\u304c\u3044\u304f\u3064\u304b\u3042\u308b\u3088\u3046\u3067\u3059\u3002\u201d\u3068\u3042\u308a\u307e\u3059\u304c\u3001\u5916\u308c\u5024\u306a\u306e\u304b\u306a\u3042\u3002 ","3df59e51":"# \u5b9f\u9a13","2403d33c":"![image.png](attachment:f44b8077-ee35-4128-9dc7-0e4baef179ac.png)","fe718ba5":"##  `\u30c6\u30ad\u30b9\u30c8\u914d\u5e03\n\n### `\u9577\u3055","f69a9cae":"\u201d\u3053\u308c\u3089\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u898b\u308b\u3060\u3051\u3067\u306f\u3001\u305d\u308c\u3089\u306e\u300c\u30bf\u30fc\u30b2\u30c3\u30c8\u300d\u30b9\u30b3\u30a2\u3092\u5224\u65ad\u3059\u308b\u306e\u306f\u5c11\u3057\u96e3\u3057\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u300c\u30bf\u30fc\u30b2\u30c3\u30c8\u300d\u30b9\u30b3\u30a2\u304c\u6700\u3082\u4f4e\u3044\u30b5\u30f3\u30d7\u30eb\u306b\u306f\u3001\u6587\u6cd5\u3001\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u3001\u53e5\u8aad\u70b9\u306e\u30a8\u30e9\u30fc\u304c\u6df7\u5728\u3057\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002 \u8aad\u3093\u3060\u308a\u7406\u89e3\u3057\u305f\u308a\u3059\u308b\u306e\u3092\u96e3\u3057\u304f\u3057\u307e\u3059\u3002\u201d\u3068\u306e\u3053\u3068\u3067\u3059\u304c\u3001\u3046\u30fc\u3093\u3001\u305d\u3046\u304b\u306a\u3042\u3002\u65e5\u672c\u4eba\u306b\u306f\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u3002","0548ac32":"\u6b63\u898f\u5206\u5e03\u306b\u8fd1\u304f\u306a\u3063\u305f\u3002\uff11\uff15\uff10\uff5e\uff13\uff10\uff10\u306b\u307b\u307c\u53ce\u307e\u3063\u3066\u307e\u3059\u3002","78065424":"### \u6b21\u306b\u30015 \u3064\u306e\u6700\u3082\u4f4e\u3044\u300c\u30bf\u30fc\u30b2\u30c3\u30c8\u300d\u5024\u306e\u4f8b","70278f8e":"## Dependencies","c027d4ac":"\u3053\u306e\u76ee\u7684\u95a2\u6570\u306etarget\u304c\u306a\u3093\u306e\u3053\u3068\u304b\u5168\u304f\u308f\u304b\u3089\u306a\u3044\u3002standard_error\u3082\u3002\n\n![image.png](attachment:0491dca0-e13b-4954-a46d-4868fc92ae39.png)","d5cbf63c":"![image.png](attachment:2ac145a7-2b8d-4c7e-b52d-adbf48558fbe.png)","27bae8df":"### `excerpt` word count","6a281826":"### **Error analysis**, label x prediction distribution\n\n\u3053\u3053\u3067\u3001\u30e9\u30d9\u30eb\u304b\u3089\u306e\u5206\u5e03\u3068\u4e88\u6e2c\u5024\u3092\u6bd4\u8f03\u3057\u3066\u3044\u307e\u3059\u3002\u5b8c\u5168\u306a\u30b7\u30ca\u30ea\u30aa\u3067\u306f\u3001\u305d\u308c\u3089\u306f\u4e00\u81f4\u3059\u308b\u306f\u305a\u3002\u5927\u4f53\u4e00\u81f4\u3057\u3066\u3044\u308b\u3088\u3046\u306a\u3002","dd369ca6":"target\u3068standard_error\u306e\u76f8\u95a2\u95a2\u4fc2\u3002\u6975\u7aef\u306a\u300c\u30bf\u30fc\u30b2\u30c3\u30c8\u300d\u5024\uff08\u300c-4\u300d\u3068\u300c2\u300d\u306b\u8fd1\u3044\uff09\u3092\u6301\u3064\u30b5\u30f3\u30d7\u30eb\u306f\u3001standard_error\u306e\u5024\u304c\u9ad8\u304f\u306a\u3063\u3066\u3044\u308b\u3002<br>\n\u3067\u3001\u3069\u3046\u3057\u308d\u3068\uff1f","7cfd6e2c":"TPU\u3067\u5b9f\u884c\u3059\u308b\u3068\u30012\u56de\u76ee\u306efold\u3067KeyError: 'logits'\u3000\u3068\u306a\u3063\u3066\u3053\u3051\u308b\u3002\u306a\u3093\u3067\u304b\u308f\u304b\u3093\u306a\u3044\u3002GPU\u3067\u52d5\u304b\u3059\u3057\u304b\u306a\u3044\u3088\u3046\u3067\u3059\u3002","7d087541":"## \u30e2\u30c7\u30eb\u306e\u640d\u5931\u3068\u30e1\u30c8\u30ea\u30c3\u30af\u306e\u30b0\u30e9\u30d5","08469ce8":"\u3053\u306e\u30b3\u30f3\u30da\u3067\u91cd\u8981\u306a\u306e\u306f\u3001\u201d3 \u5e74\u751f\u304b\u3089 12 \u5e74\u751f\u306e\u6559\u5ba4\u3067\u4f7f\u7528\u3059\u308b\u6587\u7bc0\u306e\u8907\u96d1\u3055\u3092\u8a55\u4fa1\u3059\u308b\u201d\u3053\u3068\u3060\u305d\u3046\u3067\u3059\u3002<br>\n\u4eca\u306e\u3061\u304b\u3089\u3067\u306f\u3069\u3046\u3059\u308c\u3070\u3044\u3044\u306e\u304b\u308f\u304b\u308a\u307e\u305b\u3093\u304c\u3002"}}