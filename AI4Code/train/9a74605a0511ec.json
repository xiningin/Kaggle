{"cell_type":{"6c9b474d":"code","4644b51f":"code","aaf4e51f":"code","dcb0209a":"code","b2634143":"code","7576f6b9":"code","f1161a22":"code","6befedf3":"code","7a828706":"code","94ce3128":"code","ca1d91a2":"code","5c909306":"markdown","2aceb97b":"markdown","4a819da2":"markdown","62f2e1fb":"markdown","06814ff9":"markdown","7085c708":"markdown","6c44babd":"markdown","422a2feb":"markdown","a58c0048":"markdown","3bf3f421":"markdown","c4836584":"markdown","52100027":"markdown","8714ec42":"markdown"},"source":{"6c9b474d":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4644b51f":"#this is the same dataset as the TF.dataset from TFhub, but it is easier to explore using pandas\ndf = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf","aaf4e51f":"#Splitting the dataset into training, validation, and test datasets\ntrain_data, validation_data, test_data = tfds.load(\n    name=\"imdb_reviews\", \n    split=('train[:75%]', 'train[75%:]', 'test'),\n    as_supervised=True)\n\nprint(\"Training Data: {}\".format(len(train_data)))\nprint(\"Validation Data: {}\".format(len(validation_data)))\nprint(\"Test Data: {}\".format(len(test_data)))","dcb0209a":"train_examples_batch, train_labels_batch = next(iter(train_data.batch(5)))\nprint(train_examples_batch, \"\\n\\n\", train_labels_batch)","b2634143":"#downloading and defining the embedding layer from tfhub\nembedding = \"https:\/\/tfhub.dev\/google\/nnlm-en-dim50\/2\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[], #input shape is a list\n                           dtype=tf.string, trainable=True)\n\n#prints the output when passing three examples through the layer.\nhub_layer(train_examples_batch[:3])","7576f6b9":"model = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(16, activation='relu')) #note relu maps all negative values to zero\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid')) #sigmoid maps values between 0-1\n\n#printing model structure\nmodel.summary()","f1161a22":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model.fit(train_data.shuffle(10000).batch(512),\n                    epochs=10,\n                    validation_data=validation_data.batch(512),\n                    verbose=1)","6befedf3":"dataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3) # or with batch shuffle --- dataset = dataset.shuffle(3).batch(3)\nlist(dataset.as_numpy_iterator())","7a828706":"results = model.evaluate(test_data.batch(64), verbose=1)\n\nfor name, value in zip(model.metrics_names, results):\n    print(\"%s: %.3f\" % (name, value))","94ce3128":"examples = [\n    'this is such an amazing movie!',  # this is the same sentence tried earlier\n    'The movie was great!',\n    'The movie was decent.',\n    'The movie was okish.',\n    'The movie was so awful and terrible...'\n]\n\n#tf.sigmoid maps values between 0-1\n#tf.constant creates a tensor from a tensor-like object\noriginal_results = model(tf.constant(examples))\n\nfor (x, y) in zip(original_results, examples) :\n    print(\"Input: {} --- {:.2f}\".format(y, x[0]))","ca1d91a2":"model.save_weights(\"keras_NLP_basic_weights.h5\")","5c909306":"In the next cell we are getting a batch of labels and data from the tf.dataset to view. We can see that there are two possible labels, either a 0 or 1. 0 represents negative reviews, and 1 represents positive reviews.","2aceb97b":"We can see that the model performed pretty well on the test_data and scores ~85%. Thats pretty good! Hopefully with some improvements in the model structure, and the embedding layer we can make a model that scores >90%!\n\n---\n\nFinally, we have a trained model and can pass unseen data into the model to make predictions. As the final layer of the model uses a sigmoid activation function the output will be mapped between 0-1.\n\nThe output of the model is a prediction on the likelihood that the text is a positive review.","4a819da2":"### IMDB Movie Review Sentiment Analysis using TFHub and TFDatasets\n\nIn the first cell we are downloading the dataset from TFDatasets to be trained using tf.keras, a high-level API to build and train models in TensorFlow.\n\nThe purpose of the model is to determine wether a movie review is either positive or negative. ","62f2e1fb":"# Natural Language Processing (NLP) Tutorial\n\n**Notes**\n\n- This notebook is built in conjunction with Tensorflow's tutorial: [Text classification with TensorFlow Hub: Movie reviews](https:\/\/www.tensorflow.org\/tutorials\/keras\/text_classification_with_hub). This notebook is a blend of what I have learned so far in NLP(Natural Language Processing), and contains useful comments throughout to better understand the training process.\n\n\n**Useful Links\/Notebooks**\n\n[Tensorflow RNN Tutorial](https:\/\/www.tensorflow.org\/text\/tutorials\/text_classification_rnn)\n\n[Tensorflow Tokenizer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer)\n\n[getting-started-with-text-preprocessing](https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing)","06814ff9":"Finally we compile and train the model. \n\nWe are using ADAM optimizer, binary crossentropy loss function (as we are predicting labels either 0 or 1), and evaluting the model performance based on its accuracy on the labels.","7085c708":"In this example we are using a pre-trained embedding layer from tensoflow hub called [google\/nnlm-en-dim50\/2](https:\/\/tfhub.dev\/google\/nnlm-en-dim50\/2). This layer will map a string of any length to 50 floating point values. \n\nThere are many other pretrained embedding layers that can be used in this case, but they all have pros\/cons. \n\n- For example, we could use [nnlm-en-dim128](https:\/\/tfhub.dev\/google\/nnlm-en-dim128\/2), which is trained with the same data as google\/nnlm-en-dim50\/2, yet it maps a string to 128 floating point values rather than 50. This may make the model more accurate, but will take longer to train and make predictions.\n\n- We could also use [nnlm-en-dim128-with-normalization](https:\/\/tfhub.dev\/google\/nnlm-en-dim128-with-normalization\/2) which is built w\/ additional feautures that normalize text. Ie. removing punctuation\/additional characters.\n\nSee more text_embedding_models here --> [TF-Hub text-embedding Models](https:\/\/tfhub.dev\/s?module-type=text-embedding)","6c44babd":"Evaluating model performance on the test dataset.","422a2feb":"### Loading Data\n\n- 50,000 rows, equal distribution of negative and positive reviews.","a58c0048":"Next we define the model structure.\n\nThe first layer is the embedding layer, followed by a dense\/linear layer with 16 hidden nodes, and then a final dense layer with 1 node.\n\nThe beauty of Keras is that is requires very little code to create powerful Neural Networks.","3bf3f421":"### Closing Thoughts\n\nI would like to experiment with Tensorflow using a lower level API, and not relying on the higher level API in Keras. \n\nThat being said, Keras is amazing at prototyping and getting started on a wide range of machine learning problems. I think it is beneficial to start with keras to get a sense of the problem you are working with, and then work down from there. I think it would be interesting to mess with other embedding layers, create a custom embedding layer, experiment with different model structures, and training parameters. \n\nI also recently created a blog_application using Flask, and would like to incorporate an NLP model into that. Feel fre to check out the code for that here --> [flask_blog_application](https:\/\/github.com\/brendanartley\/Flask_Blog_Application)\n\n--- \n\nAny thoughts\/suggestions on this notebook would be greatly appreciated!","c4836584":"Saving the model weights so we can use the model in the future.","52100027":"![The kaggle logo][1]\n\n[1]: https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/6\/69\/IMDB_Logo_2016.svg\/2560px-IMDB_Logo_2016.svg.png","8714ec42":"I had to look into the tf.data.Dataset documentation to understand what was going on when we batched a tf dataset, as I thought that it was only evaluating 512 test_data examples and not all the test data. \n\n---\n\nWe can see from the following example how tf datasets are arranged when the batch method is called. \n\nIf the dataset can not be split evenly, the final batch will be smaller than the rest. If for any reason we need to ensure the batch size stays the same we can pass `drop_remainder=True` into the batch method to drop the last batch.\n\nTo get a better understanding of tf.data.datsets check out the docs here --> [tf\/data\/Dataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset)"}}