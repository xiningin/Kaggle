{"cell_type":{"2cc9bdc7":"code","12ef460b":"code","45be0187":"code","d44dbe69":"code","90f9a9ee":"code","f3616be6":"code","5c056980":"code","e2e2cb18":"code","c76e4010":"code","51322178":"markdown"},"source":{"2cc9bdc7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport spacy # NLP\nimport plac\nimport random\nfrom pathlib import Path\nimport spacy\nfrom tqdm import tqdm\nimport json\nimport logging\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","12ef460b":"train_data = pd.read_csv(\"\/kaggle\/input\/scl-2021-ds\/train.csv\")\ntrain_data.head()","45be0187":"\n# creating dataset for test and train\n\n\n# for each row, find the part of raw address that needs is label\n# and find using regex how far the label is\n\n# # Add two columns of index of POI and Index of street\n# for index, row in train_data.head().iterrows():\n#     address = row['raw_address']\n#     label_element = row['POI\/street'].split(\"\/\")\n#     label_POI = label_element[0]\n#     label_street = label_element[1]\n    \n#     POI_index = [m.start() for m in re.finditer(label_POI, address)]\n#     print(POI_index)\n# #     print(row['raw_address'], row['POI\/street'])\n\nwords = []\ntags = []\n\n# Intutive solution using NER\nfor index, row in train_data.iterrows():\n    label_element = row['POI\/street'].split(\"\/\")\n    label_POI = label_element[0]\n    label_street = label_element[1]\n    if label_POI !=\"\":\n        words.append(label_POI)\n        tags.append(\"POI\")\n    if label_street != \"\":\n        words.append(label_street)\n        tags.append(\"STREET\")\n","d44dbe69":"TRAIN_DATA = []\n\nfor i in range(len(words)):\n    TRAIN_DATA.append((words[i],{'entities':[(0,len(words[i])-1,tags[i])]}))\n\nprint(TRAIN_DATA)\n\n# gold_dict = {\"tags\":tags}\n# TRAIN_DATA = [(words[i],tags[i]) for i in range(len(words))]\n# TRAIN_DATA = [words,gold_dict]\n# from spacy.tokens import Doc, DocBin\n# doc = Doc(nlp.vocab,words=words,ents=tags)\n# docbin.add(doc)\n\n","90f9a9ee":"# Setup\n\n# spacy.prefer_gpu() # or spacy.require_gpu()\nspacy.require_gpu()\n\n# variables required for the training model to be processed\nmodel = None\noutput_dir=Path(\"output\")\nn_iter=1\n\n#load the model\n\nif model is not None:\n    nlp = spacy.load(model)  \n    print(\"Loaded model '%s'\" % model)\nelse:\n    nlp = spacy.blank('en')  \n    print(\"Created blank 'en' model\")\n\n#set up the pipeline\n\nif 'ner' not in nlp.pipe_names:\n    ner = nlp.create_pipe('ner')\n    nlp.add_pipe(ner, last=True)\nelse:\n    ner = nlp.get_pipe('ner')\n    \nfor _, annotations in TRAIN_DATA:\n    for ent in annotations.get('entities'):\n        ner.add_label(ent[2])","f3616be6":"# Training\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n     # show warnings for misaligned entity spans once \n    optimizer = nlp.begin_training()\n    for itn in range(n_iter):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        for text, annotations in tqdm(TRAIN_DATA):\n            nlp.update(\n                [text],  \n                [annotations],  \n                drop=0.5,  \n                sgd=optimizer,\n                losses=losses)\n        print(losses)","5c056980":"nlp.to_disk('\/tmp\/my_model', disable=['tokenizer'])","e2e2cb18":"def save_model(output_dir, nlp, new_model_name):\n    output_dir = f'..\/working\/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\nsave_model('\/tmp2', nlp, 'st_ner')\n","c76e4010":"# # save it into a pickle\n\n# import pickle\n\n# with open('words.pkl', 'wb') as f:\n#     pickle.dump(words, f)\n\n# with open('TRAIN_DATA.pkl', 'wb') as f:\n#     pickle.dump(TRAIN_DATA, f)\n\n# words = []\n# tags = []\n# TRAIN_DATA = []\n# with open('words.pkl', 'wb') as f:\n#     pickle.load(words, f)\n\n# with open('tags.pkl', 'wb') as f:\n#     pickle.load(tags, f)\n\n# with open('train_data.pkl', 'wb') as f:\n#     pickle.load(TRAIN_DATA, f)","51322178":"## Appendix"}}