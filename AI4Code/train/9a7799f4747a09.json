{"cell_type":{"9d2becf7":"code","94f9d75a":"code","28fdee39":"code","595bb234":"code","05234395":"code","d50444b7":"code","fa8c691a":"code","aac7c007":"code","0f6b1bff":"code","37c85f2f":"code","bf524f28":"code","d883718f":"code","330f669a":"code","093947ba":"code","e5ed51d9":"code","4050bd04":"markdown","dde686f8":"markdown","42c741a2":"markdown","2eede868":"markdown","6015fa23":"markdown","a233ba51":"markdown","202366ef":"markdown","e432fe5a":"markdown","c8a89279":"markdown","8a4d0d4f":"markdown","573756d8":"markdown","6fbd022c":"markdown","78e53590":"markdown","967fc362":"markdown","8f64acc2":"markdown","ecb24b58":"markdown"},"source":{"9d2becf7":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf \nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Model \nfrom tensorflow.keras.layers import Conv2D, concatenate, Activation, MaxPooling2D, UpSampling2D, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport cv2\nimport matplotlib.pyplot as plt\nimport random\nimport os","94f9d75a":"# Create Lists of images and masks paths\npath = '\/kaggle\/input\/lyft-udacity-challenge\/'\nimages_path = [path + 'data' + i + '\/' + 'data' + i + '\/' + 'CameraRGB' for i in ['A', 'B', 'C', 'D', 'E']]\nmasks_path = [path + 'data' + i + '\/' + 'data' + i + '\/' + 'CameraSeg' for i in ['A', 'B', 'C', 'D', 'E']]","28fdee39":"# Defining a function that take the parent path and return a list of all images in that path\ndef return_paths(path):\n    X = []\n    for i in sorted(path):\n        g = sorted(os.listdir(i))\n        for j in g:\n            X.append(i + '\/' + j)\n    return X","595bb234":"# Creating two lists of the images and masks pathes respectively.\nX = return_paths(images_path)\nY = return_paths(masks_path)","05234395":"# Splitting the data into train, validation and test sets\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2)\nX_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size = 0.2)","d50444b7":"# This function will load and resize images and return arrays to the generators\ndef read_imageMask(image_path, mask_path):\n    \n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_png(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, (256, 256), method='nearest')\n    \n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=3)\n    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n    mask = tf.image.resize(mask, (256, 256), method='nearest')\n    \n    return img, mask","fa8c691a":"# Defining our custom generator\ndef data_generator(image_paths, mask_paths, batch_size):\n    images = tf.constant(image_paths)\n    masks = tf.constant(mask_paths)\n    dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n    dataset = dataset.map(read_imageMask, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.cache().shuffle(500).batch(batch_size)\n    \n    return dataset","aac7c007":"# Now we are ready to create the train, validation and test generators\ntrain_generator = data_generator(X_train, Y_train, 32)\nval_generator = data_generator(X_val, Y_val, 32)\ntest_generator = data_generator(X_test, Y_test, 32)","0f6b1bff":"def unet(num_classes = 13, image_shape = (256, 256, 3)):\n    # Input\n    inputs = Input(image_shape)\n    # Encoder Path\n    conv1 = Conv2D(64, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(inputs)\n    conv1 = Conv2D(64, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv1)\n    pool1 = MaxPooling2D((2,2))(conv1)\n    \n    conv2 = Conv2D(128, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool1)\n    conv2 = Conv2D(128, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv2)\n    pool2 = MaxPooling2D((2,2))(conv2)\n\n    conv3 = Conv2D(256, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool2)\n    conv3 = Conv2D(256, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv3)\n    pool3 = MaxPooling2D((2,2))(conv3)\n    \n    conv4 = Conv2D(512, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool3)\n    conv4 = Conv2D(512, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D((2,2))(drop4)\n    \n    conv5 = Conv2D(1024, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool4)\n    conv5 = Conv2D(1024, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n    \n    # Decoder Path\n    up6 = Conv2D(512, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(drop5))\n    merge6 = concatenate([up6, conv4], axis = 3)\n    conv6 = Conv2D(512, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge6)\n    conv6 = Conv2D(512, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv6)\n    \n    up7 = Conv2D(256, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(conv6))\n    merge7 = concatenate([up7, conv3], axis = 3)\n    conv7 = Conv2D(256, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge7)\n    conv7 = Conv2D(256, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv7)\n    \n    up8 = Conv2D(128, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(conv7))\n    merge8 = concatenate([up8, conv2], axis = 3)\n    conv8 = Conv2D(128, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge8)\n    conv8 = Conv2D(128, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv8)\n    \n    up9 = Conv2D(64, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(conv8))\n    merge9 = concatenate([up9, conv1], axis = 3)\n    conv9 = Conv2D(64, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge9)\n    conv9 = Conv2D(64, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv9)\n    \n    conv10 = Conv2D(num_classes, (1, 1), padding='same', activation='softmax')(conv9)\n    \n    model = Model(inputs, conv10)\n    \n    return model","37c85f2f":"# Create The Model\nmodel = unet()\nmodel.summary()","bf524f28":"# Complile The Model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Define the Callbacks we will use during training\nearlystopping = EarlyStopping(monitor='val_loss', patience= 20)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=1e-1, patience=5, verbose=1, min_lr = 2e-6)","d883718f":"# Train the Model\nhistory = model.fit(train_generator, validation_data=val_generator,epochs=25,\n                    verbose = 1, batch_size=16,\n                    callbacks=[earlystopping, reduce_lr])","330f669a":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","093947ba":"train_loss, train_accuracy = model.evaluate(train_generator, batch_size = 32)\nvalidation_loss, validation_accuracy = model.evaluate(val_generator, batch_size = 32)\ntest_loss, test_accuracy = model.evaluate(test_generator, batch_size = 32)","e5ed51d9":"for image, mask in test_generator.take(6):\n    pred_mask = model.predict(image)\n    \n    plt.figure(figsize=(20, 20))\n    plt.subplot(1, 3, 1)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(image[0]))\n    plt.subplot(1, 3, 2)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(mask[0]))\n    plt.subplot(1, 3, 3)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(tf.expand_dims(tf.argmax(pred_mask[0], axis = -1), axis = -1)))","4050bd04":"Now that we have two lists containing all the images and masks we have, We are ready to split our data into training, validation and test sets.","dde686f8":"Evaluate The Model","42c741a2":"In this notebook, we are trying to do semantic segmentation for self driving cars.\n\nThe [dataset](https:\/\/www.kaggle.com\/kumaresanmanickavelu\/lyft-udacity-challenge) provides data images and labeled semantic segmentations captured via CARLA self-driving car simulator.","2eede868":"# ***Brief Introduction***","6015fa23":"# ***Splitting the data***","a233ba51":"Now that we have reached this point, Let's define our model.\n\nFor this Task We will use the U-net architecture","202366ef":"Let's Now Visualize the Results","e432fe5a":"# ***Evaluation***","c8a89279":"Because The data in our dataset is split across five folders, and for other reasons we will do some data preprcessing","8a4d0d4f":"# ***Data Preparation***","573756d8":"# ***Model***","6fbd022c":"# ***Compling and Training***","78e53590":"First let's import the dependencies","967fc362":"Visualize The results of our Model on test data","8f64acc2":"Now that we have prepared and splitted our data, we need to define our Generators that are responsible for loading batches of the data and feed them to the model.\nBefore this step, we will need to defind a function that will be reponsible for loading, and resizing images","ecb24b58":"# ***Custom Data Generators***"}}