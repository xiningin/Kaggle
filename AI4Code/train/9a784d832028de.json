{"cell_type":{"21f9c0bf":"code","b820a79f":"code","0544e0b4":"code","8435cc6a":"code","12e37f71":"code","b7e2eb10":"code","1f3827e8":"code","3c27bdd5":"code","423643fa":"code","888d2dbd":"code","a129d01c":"code","0023ace6":"code","bceede8f":"code","64038650":"code","91434cdf":"code","3057ce8d":"code","ddfe9d72":"code","7a489d53":"code","48c3ad81":"code","93338936":"code","669bdac5":"code","fc0aeafc":"code","a78c8b5b":"code","d707f0c3":"markdown","b884d523":"markdown","9d7dbc1c":"markdown","266a304b":"markdown","9847b432":"markdown","1c67460e":"markdown","6b4ae37c":"markdown"},"source":{"21f9c0bf":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom nltk.stem.porter import PorterStemmer","b820a79f":"df = pd.read_csv(\"..\/input\/fake-news\/train.csv\")","0544e0b4":"df.head()","8435cc6a":"df = df.fillna('')\n\ndf[\"total\"] = df['title'] + \" \" + df[\"author\"]\n\ndf.head()","12e37f71":"X = df.drop(\"label\", axis = 1)\ny = df[\"label\"]\n\nX.shape, y.shape","b7e2eb10":"VOCAB_SIZE = 5000\ntext = X.copy()","1f3827e8":"def text_cleaning(length):\n    ps = PorterStemmer()\n    corpus = []\n    for i in range(length):\n        m = re.sub(\"[^a-zA-Z]\",\" \",text[\"total\"][i])\n        m = m.lower()\n        m = m.split()\n        m = [ps.stem(word) for word in m if not word in stopwords.words('english')]\n        clean_text = \" \".join(m)\n        corpus.append(clean_text)\n        \n    return corpus","3c27bdd5":"%%time\nclean_text_corpus = text_cleaning(len(text))","423643fa":"clean_text_corpus[0]","888d2dbd":"# to one hot\nonehot_text = [one_hot(words, VOCAB_SIZE) for words in clean_text_corpus]","a129d01c":"# padding sequences\npadded_doc = pad_sequences(onehot_text, padding=\"pre\", maxlen = 25)","0023ace6":"x_train = np.array(padded_doc)\ny_train = np.array(y)","bceede8f":"def build_model():\n    # model structure\n    model = tf.keras.Sequential([\n        L.Embedding(VOCAB_SIZE, 40, input_length = 25),\n        L.Dropout(0.3),\n        L.LSTM(100),\n        L.Dropout(0.3),\n        L.Dense(64, activation = \"relu\"),\n        L.Dropout(0.3),\n        L.Dense(1, activation = \"sigmoid\")\n    ])\n    \n    model.compile(optimizer=\"adam\", loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                 metrics = tf.metrics.BinaryAccuracy()\n                 )\n    \n    return model","64038650":"news_classifier = build_model()\nnews_classifier.summary()","91434cdf":"tf.keras.utils.plot_model(news_classifier)","3057ce8d":"my_callbacks = [tf.keras.callbacks.ModelCheckpoint(\"news_classifier.h5\", monitor = \"val_loss\", save_best_only=True),\n                tf.keras.callbacks.CSVLogger(\"training.log\"),\n                tf.keras.callbacks.EarlyStopping(patience=5),\n                tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n               ]","ddfe9d72":"history = news_classifier.fit(\n    x_train,\n    y_train,\n    epochs = 25,\n    batch_size = 128,\n    validation_split = 0.2,\n    callbacks = my_callbacks\n)","7a489d53":"test_df = pd.read_csv(\"..\/input\/fake-news\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/fake-news\/submit.csv\")\n\nsample_submission.shape","48c3ad81":"test_df.head()","93338936":"test_df = test_df.fillna('')\ntest_df[\"total\"] = test_df[\"title\"] + \" \" + test_df[\"author\"]\n\ntext_test = test_df.copy()\n\ndef text_cleaning_test(length):\n    ps = PorterStemmer()\n    corpus_test = []\n    for i in range(length):\n        m = re.sub(\"[^a-zA-Z]\",\" \",text_test[\"total\"][i])\n        m = m.lower()\n        m = m.split()\n        m = [ps.stem(word) for word in m if not word in stopwords.words('english')]\n        clean_text = \" \".join(m)\n        corpus_test.append(clean_text)\n        \n    return corpus_test\n\nclean_test_corpus = text_cleaning_test(len(text_test))\n\n# one hot encoder\nonehot_text_test = [one_hot(words, VOCAB_SIZE) for words in clean_test_corpus]\n\n# padding sequences\npadded_doc_test = pad_sequences(onehot_text_test, padding=\"pre\", maxlen = 25)\n\nx_test = np.array(padded_doc_test)","669bdac5":"# making predictions\npredictions = news_classifier.predict(x_test)\npredictions = [int(np.round(pred[0])) for pred in predictions]","fc0aeafc":"submission = pd.DataFrame({\n    \"id\" : test_df[\"id\"],\n    \"label\" : predictions\n})\nsubmission","a78c8b5b":"# submission\nsubmission.to_csv(\"submission.csv\", index = False)","d707f0c3":"## Model","b884d523":"## Fake news classifier using LSTM","9d7dbc1c":"## Pre-Processing","266a304b":"## Training","9847b432":"### Setup","1c67460e":"### Data","6b4ae37c":"## Making Predictions"}}