{"cell_type":{"39a09f75":"code","4d611395":"code","874f03a6":"code","419d2bdf":"markdown","083ef17f":"markdown"},"source":{"39a09f75":"import pandas as pd\nimport numpy as np\nimport time","4d611395":"class CFG:\n    TRAIN_SIZE = 0.2\n    MIN_WEIGHT = 0\n    NROWS = None\n    FILL_NA = -999","874f03a6":"# -75% of time space took by train.csv so we can train faster models\n# and avoid RAM errors\n\nfeatures_columns = [\"feature_%d\" % i for i in range(130)]\ncolumns_dtypes = {}\nfor column in features_columns:\n    columns_dtypes[column] = \"float16\"\ncolumns_dtypes[\"resp_1\"] = \"float16\"\ncolumns_dtypes[\"resp_2\"] = \"float16\"\ncolumns_dtypes[\"resp_3\"] = \"float16\"\ncolumns_dtypes[\"resp_4\"] = \"float16\"\ncolumns_dtypes[\"resp\"] = \"float16\"\n\nprint(\"Loading dataset...\")\ndataset = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/train.csv\", delimiter=\",\", nrows=CFG.NROWS, dtype=columns_dtypes)\ndataset = dataset[dataset.weight > CFG.MIN_WEIGHT]\nprint(\"Done!\")\n\nprint(\"Splitting train\/test dataset...\")\ntrain_number_items = int(dataset.shape[0] * CFG.TRAIN_SIZE)\ntrain = dataset[:train_number_items]\ntest = dataset[train_number_items + 1:]\nprint(\"Done!\")\n\nprint(\"Filling NaN values...\")\ntrain = train.fillna(CFG.FILL_NA)\ntest = test.fillna(CFG.FILL_NA)\nprint(\"Done.\")\n\nprint(\"Preparing X and y...\")\nX_train = train[features_columns].to_numpy()\nX_test = test[features_columns].to_numpy()\ny_train = np.where(train[\"resp\"] > 0, 1, 0)\ny_test = np.where(test[\"resp\"] > 0, 1, 0)\nd_train = train[\"date\"].to_numpy()\nd_test = test[\"date\"].to_numpy()\nw_train = train[\"weight\"].to_numpy()\nw_test = test[\"weight\"].to_numpy()\nr_train = train[\"resp\"].to_numpy()\nr_test = test[\"resp\"].to_numpy()\nresp_train = train[[\"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]].to_numpy()\nresp_test = test[[\"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]].to_numpy()\n\nresp_label_train = np.where(resp_train > 0, 1, 0)\nresp_label_test = np.where(resp_test > 0, 1, 0)\n\nprint(\"Done!\")\n\nprint(\"Deleting unused variables...\")\ndel dataset\ndel train\ndel test\nprint(\"Done!\")\n\nprint(\"Train\/test sizes: %d\/%d\" % (len(X_train), len(X_test)))","419d2bdf":"# Memory trick to load a lot of data\n\nIf you ever faced to memory problems on this challenge or another, this notebook should help you to have a better understanding on how to deal with Big Data.\n\nEvery dataset you load is stored into memory, which is limited.\nWhen using pandas, every float (decimal numbers) is casted as float64.\n\nFloat64 uses 64 bits (8 bytes) to store a single value and range between -1.79e+308 and 1.74e+308.\nBut on the Jane Street dataset, never use such huge numbers!\n\nThe float16 consumes 16 bits (2 bytes) of memory and range between -32768 and 32767.\nIf we use them, you can save 75% of the memory.\n\nMoreover, the arithmetical operations on the float16 will be really cheaper than the float64 operations.\nThen, your training and predicting will be better!","083ef17f":"Now, here is the code I use to load the dataset:"}}