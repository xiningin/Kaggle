{"cell_type":{"541a7b78":"code","c8f45db8":"code","6d0c5809":"code","6d2e6774":"code","78aeac7e":"code","8ef5b3e3":"code","81d54c68":"code","fcc1b41c":"code","97209922":"code","229aaaac":"code","1744b4a3":"code","c50ec97b":"code","34f01124":"code","c61643e8":"code","b6a0b206":"code","8e5b0ade":"code","7f70841f":"code","b855b369":"code","054880a3":"code","413b5dc8":"markdown","6058b3e7":"markdown"},"source":{"541a7b78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('display.max_rows', 104)\npd.set_option('display.max_columns',104)\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_style('whitegrid')\n%matplotlib inline\n","c8f45db8":"# Load the library with the CountVectorizer method\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer \nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report, precision_recall_fscore_support, precision_score, recall_score, f1_score\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dense, Embedding,Dropout, Layer ,LSTM, SpatialDropout1D,Bidirectional,GlobalMaxPool1D, Input\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping \nimport keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nimport keras\nimport multiprocessing\nnthreads = multiprocessing.cpu_count()\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nimport string\npunctuations = string.punctuation\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n","6d0c5809":"df = pd.read_csv('\/kaggle\/input\/bonding\/hm_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/bonding\/hm_test.csv')\ndf.tail(3)","6d2e6774":"stop_words = set(stopwords.words(\"english\"))\ndef cleanup_text(doc):\n    #print(1)\n    texts = []\n    #doc = re.sub(r\"\\$\\s[0-9]+\", \"moneyvalue\", doc)        \n    doc = nlp(doc, disable=['parser', 'ner'])\n    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n\n    tokens = [tok for tok in tokens if tok not in stop_words and tok not in punctuations and not tok.isdigit()]\n    tokens = ' '.join(tokens)\n\n    texts.append(tokens)\n    return pd.Series(texts)","78aeac7e":"%%time\nwith multiprocessing.Manager().Pool(nthreads) as pool:\n    res = pool.map_async(cleanup_text, df['cleaned_hm'].values)\n    pool.close()\n    pool.join()\n    \ndf['c'] = pd.concat(res.get(), axis=0, ignore_index=True)","8ef5b3e3":"label_binarizer = LabelBinarizer()\ny = label_binarizer.fit_transform(df.predicted_category)\nlabels = label_binarizer.classes_\nprint(labels)","81d54c68":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 150000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 250\n# This is fixed.\nEMBEDDING_DIM = 100\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['c'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","fcc1b41c":"X = tokenizer.texts_to_sequences(df['c'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","97209922":"x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.10, random_state = 42)\nx_train.shape, y_train.shape","229aaaac":"# model = Sequential()\n# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n# model.add(Bidirectional(LSTM(64)))\n# model.add(Dense(100, activation='relu'))\n# model.add(Dropout(rate=0.4))\n# model.add(Dense(100, activation='relu'))\n# model.add(Dense(len(labels), activation=\"softmax\"))\n# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","1744b4a3":"# epochs = 5\n# batch_size = 128\n\n# history = model.fit(x_train, y_train, epochs=epochs,\n#                     batch_size=batch_size,validation_split=0.2,\n#                    verbose =1\n#                    )","c50ec97b":"# def my_func(a):\n#     n = a.argmax()\n#     for i in range(len(a)):\n#         if i==n:\n#             a[i] = 1\n#         else:\n#             a[i] = 0\n#     return a\n\n# y_pred = model.predict(x_test)\n# y_pr = np.apply_along_axis(my_func, 1, y_pred)\n# print('accuracy score -->',accuracy_score(y_pr, y_test))      \n# print('F1 score -->',f1_score(y_pr,y_test, average = 'weighted'))\n# print('prescision score -->',precision_score(y_test, y_pr, average='macro'))\n# print('recall score -->',recall_score(y_test, y_pr, average='macro'))\n#classification_report(y_test, y_pr).split('\\n')","34f01124":"# %%time\n# with multiprocessing.Manager().Pool(nthreads) as pool:\n#     res = pool.map_async(cleanup_text, test['cleaned_hm'].values)\n#     pool.close()\n#     pool.join()\n    \n# test['c'] = pd.concat(res.get(), axis=0, ignore_index=True)\n# X = tokenizer.texts_to_sequences(test['c'].values)\n# X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n# print('Shape of data tensor:', X.shape)\n# y_pred = model.predict(X)\n# y_pred = np.apply_along_axis(my_func, 1, y_pred)\n# test['predicted_category'] = label_binarizer.inverse_transform(y_pred, threshold=None)\n# test[['hmid','predicted_category']].to_csv('submission.csv', index=False)","c61643e8":"len_mat=[]\nfor i in range(len(X)):\n    len_mat.append(len(X[i]))","b6a0b206":"class attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(attention,self).get_config()","8e5b0ade":"inputs=Input((X.shape[1],))\nx=Embedding(input_dim=MAX_NB_WORDS+1,output_dim=EMBEDDING_DIM,input_length=X.shape[1],\\\n            embeddings_regularizer=keras.regularizers.l2(.001))(inputs)\natt_in=Bidirectional(LSTM(64, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\natt_in = Dense(100, activation=\"relu\")(att_in)\natt_in = Dropout(0.4)(att_in)\natt_in = Dense(100, activation=\"relu\")(att_in)\natt_out=attention()(att_in)\noutputs=Dense(7,activation='softmax',trainable=True)(att_out)\nmodel=Model(inputs,outputs)\nmodel.summary()","7f70841f":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nepochs = 16\nbatch_size = 128\n\nhistory = model.fit(x_train, y_train, epochs=epochs,\n                    batch_size=batch_size,validation_split=0.2,\n                   verbose =1\n                   )","b855b369":"def my_func(a):\n    n = a.argmax()\n    for i in range(len(a)):\n        if i==n:\n            a[i] = 1\n        else:\n            a[i] = 0\n    return a\n\ny_pred = model.predict(x_test)\ny_pr = np.apply_along_axis(my_func, 1, y_pred)\nprint('accuracy score -->',accuracy_score(y_pr, y_test))      \nprint('F1 score -->',f1_score(y_pr,y_test, average = 'weighted'))\nprint('prescision score -->',precision_score(y_test, y_pr, average='macro'))\nprint('recall score -->',recall_score(y_test, y_pr, average='macro'))","054880a3":"%%time\nwith multiprocessing.Manager().Pool(nthreads) as pool:\n    res = pool.map_async(cleanup_text, test['cleaned_hm'].values)\n    pool.close()\n    pool.join()\n    \ntest['c'] = pd.concat(res.get(), axis=0, ignore_index=True)\nX = tokenizer.texts_to_sequences(test['c'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\ny_pred = model.predict(X)\ny_pred = np.apply_along_axis(my_func, 1, y_pred)\ntest['predicted_category'] = label_binarizer.inverse_transform(y_pred, threshold=None)\ntest[['hmid','predicted_category']].to_csv('submission_attn_8808.csv', index=False)","413b5dc8":"# Attention Model","6058b3e7":"# BiLSTM Model"}}