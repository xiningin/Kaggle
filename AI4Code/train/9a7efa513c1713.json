{"cell_type":{"bb88eee5":"code","d26e9d69":"code","b13bf893":"code","c9a9d201":"code","59107bd5":"code","0a2e50e6":"code","899ebe37":"code","1ca2413a":"code","78520c84":"code","be03fdba":"code","77d6dbaa":"code","b42ae0cb":"code","8645b1f4":"code","c8debc7f":"code","4167e5ae":"code","e69be319":"code","7c8b970b":"code","28f166b8":"code","214b0247":"code","ba333e35":"code","10ee61b6":"code","545e664f":"code","2e7a47c6":"code","531238a0":"code","e1dfc70f":"code","7704f4c4":"code","5f3f1897":"code","61df990a":"code","01316ece":"code","f3bdca44":"code","84faf334":"code","b5390b18":"code","860042cf":"code","66d6a2a2":"code","ecb54612":"code","71110b8d":"code","eab761a7":"code","0e68a820":"code","79f81472":"code","55e8c70c":"code","c60f3123":"code","5052b3ca":"code","13915289":"code","544b5463":"markdown","3f428756":"markdown","31bf5f3b":"markdown","9173619e":"markdown","be443ae0":"markdown","3d582a1f":"markdown","5a8fa172":"markdown","a2783ad0":"markdown","aec4385c":"markdown","accad2ac":"markdown","c99ccbdc":"markdown","dd938991":"markdown","5a495169":"markdown","887cdcaa":"markdown","c2b1c927":"markdown","0d41b49b":"markdown","5866f2b8":"markdown","20eb1ce9":"markdown","2da90ead":"markdown","1fe2d3e0":"markdown"},"source":{"bb88eee5":"import pandas as pd\nimport numpy as np","d26e9d69":"books = pd.read_csv(\"..\/input\/books.csv\")","b13bf893":"books.head(3)","c9a9d201":"books_sub = books[[\"book_id\",\"title\"]].copy()\nbooks_sub","59107bd5":"# Count Vectorizer - converts words into vectors.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# initialize vectorizer\nvect = CountVectorizer(analyzer='word',ngram_range=(1,2),stop_words='english', min_df = 0.001)\n\nvect.fit(books_sub['title'])\ntitle_matrix = vect.transform(books_sub['title'])","0a2e50e6":"title_matrix.shape","899ebe37":"# Find vocabulary\nfeatures = vect.get_feature_names()\nfeatures","1ca2413a":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_sim_titles = cosine_similarity(title_matrix, title_matrix)\ncosine_sim_titles.shape","78520c84":"title_id = 3\nbooks_sub['title'].iloc[title_id]","be03fdba":"top_n_idx = np.flip(np.argsort(cosine_sim_titles[title_id,])[-10:])\ntop_n_sim_values = cosine_sim_titles[title_id, top_n_idx]\ntop_n_sim_values","77d6dbaa":"# find top n with values > 0\ntop_n_idx = top_n_idx[top_n_sim_values > 0]","b42ae0cb":"# Matching books\nbooks_sub['title'].iloc[top_n_idx]","8645b1f4":"# lets wrap the above code in a function\ndef return_sim_books(title_id, title_matrix, vectorizer, top_n = 10):\n    \n    # generate sim matrix\n    sim_matrix = cosine_similarity(title_matrix, title_matrix)\n    features = vectorizer.get_feature_names()\n\n    top_n_idx = np.flip(np.argsort(sim_matrix[title_id,])[-top_n:])\n    top_n_sim_values = sim_matrix[title_id, top_n_idx]\n    \n    # find top n with values > 0\n    top_n_idx = top_n_idx[top_n_sim_values > 0]\n    scores = top_n_sim_values[top_n_sim_values > 0]\n    \n    # find features from the vectorized matrix\n    sim_books_idx = books_sub['title'].iloc[top_n_idx].index\n    words = []\n    for book_idx in sim_books_idx:\n        try:\n            feature_array = np.squeeze(title_matrix[book_idx,].toarray())\n        except:\n            feature_array = np.squeeze(title_matrix[book_idx,])\n        idx = np.where(feature_array > 0)\n        words.append([\" , \".join([features[i] for i in idx[0]])])\n        \n    # collate results\n    res = pd.DataFrame({\"book_title\" : books_sub['title'].iloc[title_id],\n           \"sim_books\": books_sub['title'].iloc[top_n_idx].values,\"words\":words,\n           \"scores\":scores}, columns = [\"book_title\",\"sim_books\",\"scores\",\"words\"])\n    \n    \n    return res\n","c8debc7f":"vect = CountVectorizer(analyzer='word',ngram_range=(1,2),stop_words='english', min_df = 0.001)\nvect.fit(books_sub['title'])\ntitle_matrix = vect.transform(books_sub['title'])\nreturn_sim_books(3,title_matrix,vect,top_n=20)","4167e5ae":"# Consider recommendation for the following book - \n#...\"The Seven Spiritual Laws of Success: A Practical Guide to the Fulfillment of Your Dreams\" ..\n\nbooks[books[\"title\"] == \"The Seven Spiritual Laws of Success: A Practical Guide to the Fulfillment of Your Dreams\"].index\n\nreturn_sim_books(1854,title_matrix,vect,top_n=20)","e69be319":"# Number of times \"guide\" appears vs \"success\" in book titles\n\nbooks_sub[\"title\"].str.contains(\"\\\\bsuccess\\\\b\", case=False).sum()","7c8b970b":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(analyzer='word',ngram_range=(1,2),stop_words='english', min_df = 0.001)\nvect.fit(books_sub['title'])\ntitle_matrix = vect.transform(books_sub[\"title\"])","28f166b8":"import re\nfrom nltk import word_tokenize          \nfrom nltk.stem import PorterStemmer \nclass PorterStem(object):\n    def __init__(self):\n        self.stm = PorterStemmer()\n    def __call__(self, doc):\n        return [self.stm.stem(t) for t in word_tokenize(doc) if re.match('[a-zA-Z0-9*.]',t)]\n\nvect = TfidfVectorizer(tokenizer=PorterStem(),analyzer='word',ngram_range=(1,2),stop_words='english', min_df = 0.001)  ","214b0247":"vect.fit(books_sub['title'])\ntitle_matrix = vect.transform(books_sub[\"title\"])\nreturn_sim_books(1854,title_matrix,vect,top_n=20)","ba333e35":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline","10ee61b6":"vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,2),stop_words='english', min_df = 0.001)\n\n# Build the tfidf vectorizer from the training data (\"fit\"), and apply it \n# (\"transform\").\nvect_matrix = vectorizer.fit_transform(books[\"title\"])","545e664f":"vect_matrix.shape","2e7a47c6":"# Apply SVD\nsvd_mod = TruncatedSVD(100)\nlsa = make_pipeline(svd_mod, Normalizer(copy=False))\n\n# Run SVD on the training data, then project the training data.\nvect_matrix_svd = lsa.fit_transform(vect_matrix)","531238a0":"vect_matrix_svd.shape","e1dfc70f":"return_sim_books(1854,vect_matrix_svd,vectorizer,top_n=20)","7704f4c4":"svd_comp = pd.DataFrame(svd_mod.components_)\nsvd_comp.columns = vectorizer.get_feature_names()\n\n#svd_comp.to_csv(\"svd_components.csv\")\n\nsvd_comp","5f3f1897":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(analyzer='word',ngram_range=(1,2),stop_words='english', min_df = 0.0005)\nvect.fit(books_sub['title'])\n\ntitle_matrix = vect.fit_transform(books[\"title\"])","61df990a":"title_matrix=title_matrix.toarray() ","01316ece":"title_matrix.shape","f3bdca44":"words = []\nfeatures = vect.get_feature_names()\nfor i in range(0,title_matrix.shape[0]):\n    feature_array = np.squeeze(title_matrix[i,])\n    idx = np.where(feature_array > 0)\n    words.append([\" , \".join([features[i] for i in idx[0]])])","84faf334":"books_sub[\"words\"] = words","b5390b18":"# To use use word2vec install gensim\n# GloVe means Global Vectors for word representation. Is an unsupervised learning algorithm\n\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nglove2word2vec(glove_input_file=\"..\/input\/gensim_glove.6B.50d.txt\", word2vec_output_file=\"gensim_glove.6B.50d.txt\")","860042cf":"from gensim.models.keyedvectors import KeyedVectors\nglove_model = KeyedVectors.load_word2vec_format(\"..\/input\/gensim_glove.6B.50d.txt\", binary=False)","66d6a2a2":"a = glove_model.word_vec(\"rich\")\nb = glove_model.word_vec(\"money\")","ecb54612":"from scipy.stats.stats import pearsonr\n\n# Returns (Pearson\u2019s correlation coefficient, 2-tailed p-value)\npearsonr(a,b)[0]","71110b8d":"words = books_sub[\"words\"].iloc[0][0].split(\",\")\nwords = [x.strip() for x in words]","eab761a7":"words","0e68a820":"chk = [glove_model.word_vec(x) for x in words if x in glove_model.vocab]","79f81472":"np.array(chk).mean(axis=0).shape","55e8c70c":"\nfrom tqdm import tqdm\ntitle_vec = np.zeros((10000,50))\nfor i in tqdm(range(0,books_sub.shape[0])):\n    words = books_sub[\"words\"].iloc[i][0].split(\",\")\n    words = [x.strip() for x in words]\n    ind_word_vecs = [glove_model.word_vec(x) for x in words if x in glove_model.vocab]\n    title_vec[i] = np.array(ind_word_vecs).mean(axis=0)","c60f3123":"title_vec.shape","5052b3ca":"title_vec = np.nan_to_num(title_vec)","13915289":"return_sim_books(1854,title_vec,vect,top_n=20)","544b5463":"Features in machine learning are basically numerical attributes from which anyone can perform some mathematical operations such as matrix factorization, dot product etc. \nHowever there are scenarios where the dataset given doesn't contain numerical attributes (customer reviews, sentiment analysis, recommendations etc)\nConversion of these types of features into numerical values is called featurization.\nTo convert string data into numerical data we may use any of the following methods: \n - Bag of Words (CountVectorizer)\n - TF-IDF\n - Word2Vec\n\n","3f428756":"### Find out what features have been considered by the vectorizer for a given title?","31bf5f3b":"A popular application of SVD is for dimensionality reduction.\n\nData with a large number of features, such as more features (columns) than observations (rows) may be reduced to a smaller subset of features that are most relevant to the prediction problem.\n\nThe result is a matrix with a lower rank that is said to approximate the original matrix.","9173619e":"# Get unique books with title","be443ae0":"### Let's get books similiar to a given title","3d582a1f":"## Lets see what happens when we change the min_df value? What is this argument controlling?\n","5a8fa172":"# TF-IDF Vectorization\n- TF-IDF basically tells the importance of word in the corpus or dataset.\n- TF - Term Frequency > How frequently the word appears. \n- IDF - Inverse Document Frequency -> importance of the word\n\nTF-IDF is a matrix multiplication of the matrices of TF and IDF","a2783ad0":"### Using Word2Vec Model : A method of clustering words\n- Is a deep learning technique with a two layer network\n- Clusters words\n- Maps meaningful words as well as grammar","aec4385c":"### Consider the book - \"To Kill a Mockingbird\" (location_id - 3). What are the most similiar books? ","accad2ac":"# SVD on the Vectorizer Matrix\n\nA = U . Sigma . V^T\n\nWhere \n- A is the real m x n matrix that we wish to decompose\n- U is an m x m matrix, Sigma (often represented by the uppercase Greek letter Sigma) is an m x n diagonal matrix\n- V^T is the  transpose of an n x n matrix where T is a superscript.","c99ccbdc":"## Find Similarity Between Items","dd938991":"# Reduce Variability using Stemming","5a495169":"### A content based recommender works with data that the user provides, either explicitly (eg: rating) or implicitly (eg: clicking on a link). \n\n### Based on that data, a user profile is generated, which is then used to make suggestions to the user. As the user provides more inputs or takes actions on the recommendations, the engine becomes more and more robust and accurate.","887cdcaa":"### What difference do you see in the results? ","c2b1c927":"# Content based Recommendation using NLP","0d41b49b":"# Featurization","5866f2b8":"# Count Vectorizer","20eb1ce9":"\nmax_df - removing terms that appear too frequently (corpus specific stop words)\n\n\nmin_df - ignore terms that appear in less than **n** documents\n- min_df = 0.01 means ignore terms that appear in less than 1% of documents\n- min_df = 5 means ignore terms that appear in less than 5 documents\n         ","2da90ead":"Cosine Similarity - Find similar words (related) or test the orthogonality (unrelated)\n\nFind the value of theta. \n- Cos (0)   - Vectors are similar\n- Cos (90)  - Vectors are orthogonal (not related)\n- Cos (180) - Vectors are opposite to each other\n\n\nCosine similarity not only finds out similarity between vectors but also ignores frequency count of words. ","1fe2d3e0":"\n### Get the most similiar books using the TF-IDF aproach for the title \"The Seven Spiritual Laws of Success..\" book (title id: 1854)"}}