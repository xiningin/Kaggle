{"cell_type":{"83437a2d":"code","f73b14fd":"code","a14aeaff":"code","c805c9ad":"code","39578c93":"code","c1a78449":"code","44440aa7":"code","24b80510":"code","2270afcc":"code","60a81ec0":"code","4dc9cac1":"code","b4861d9d":"code","6ef8dd08":"code","ed3e59e7":"code","44e26da0":"code","15507cf3":"code","2f9b8edd":"code","edf234a3":"code","8968ceb7":"code","77a5d32a":"code","ab3274c9":"code","1eedf676":"code","041c95c9":"code","f156c4bf":"code","eb248933":"code","c17d4729":"code","a5d2cfea":"code","3c63c34c":"code","72a8c45d":"code","f50bab38":"code","cf9f6223":"code","30a36f30":"code","4146de90":"code","785a314f":"code","9e7b135a":"code","724687e4":"markdown","f4d16203":"markdown","a8f3a96d":"markdown","3ea89772":"markdown","96051b15":"markdown","02bdc7d3":"markdown","68d9a8ba":"markdown","c1526799":"markdown","d593e8b0":"markdown","c42ed53d":"markdown","151cd3da":"markdown","8508acf0":"markdown","c7eb8ccc":"markdown","ea82ad26":"markdown","e67d1b35":"markdown","41526a9c":"markdown"},"source":{"83437a2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f73b14fd":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a14aeaff":"data = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')","c805c9ad":"data.head(10)","39578c93":"data.shape","c1a78449":"data.info()","44440aa7":"data.describe().T","24b80510":"# Explore Missing Data \ndata.isnull().sum().sort_values(ascending = False)","2270afcc":"#check frequency values in MINIMUM_PAYMENTS\ndata['MINIMUM_PAYMENTS'].value_counts()","60a81ec0":"#copy data without cust_id beacause it is object not numeric to make visualize\ndf = data.copy()\ndf.drop(columns=['CUST_ID'] , axis=1 , inplace=True)\n\n ","4dc9cac1":"for col in df:\n    df[[col]].hist()","b4861d9d":"fig = plt.figure(figsize=(20,20))\nfor col in range(len(df.columns)) :\n    fig.add_subplot(6,3,col+1)\n    sns.boxplot(x=df.iloc[ : , col])\nplt.show()","6ef8dd08":"fig = plt.figure(figsize=(12,10))\nsns.heatmap(data.corr() , annot=True)","ed3e59e7":"data['CREDIT_LIMIT']=data['CREDIT_LIMIT'].fillna(data['CREDIT_LIMIT'].mean())\ndata['MINIMUM_PAYMENTS'] = data['MINIMUM_PAYMENTS'].fillna(data['MINIMUM_PAYMENTS'].median())","44e26da0":"data.isnull().sum().sort_values(ascending = False)","15507cf3":"data = data.drop(data[data['PURCHASES'] > 4500].index)\ndata = data.drop(data[data['ONEOFF_PURCHASES'] > 3000].index)\ndata = data.drop(data[data['INSTALLMENTS_PURCHASES'] > 1800].index)\ndata = data.drop(data[data['CASH_ADVANCE'] > 3500].index) \ndata = data.drop(data[data['CASH_ADVANCE_FREQUENCY'] > 1.3].index)\ndata = data.drop(data[data['CASH_ADVANCE_TRX'] > 55].index)\ndata = data.drop(data[data['PAYMENTS']>3500].index)\ndata = data.drop(data[data['MINIMUM_PAYMENTS'] > 4000].index)\n\n'''\ndata.where(data['PURCHASES'] < 4500 , inplace=True)\ndata.where(data['ONEOFF_PURCHASES'] < 3000 , inplace=True)\ndata.where(data['INSTALLMENTS_PURCHASES'] < 1800 , inplace= True)\ndata.where(data['CASH_ADVANCE'] < 3500 ,inplace= True)\ndata.where(data['CASH_ADVANCE_FREQUENCY'] < 1.3 ,inplace=True)\ndata.where(data['CASH_ADVANCE_TRX'] < 55 , inplace=True)\ndata.where(data['PAYMENTS'] < 3500 , inplace=True)\ndata.where(data['MINIMUM_PAYMENTS'] < 4000 , inplace=True)\n'''\n","2f9b8edd":"data.drop(['CUST_ID'] , axis = 1 , inplace= True)","edf234a3":"data.head()","8968ceb7":"data.dtypes","77a5d32a":"data.shape","ab3274c9":"data.isnull().sum().sort_values(ascending = False)","1eedf676":"#from sklearn.preprocessing import StandardScaler\n#sc = StandardScaler()\n#from sklearn.preprocessing import RobustScaler\n#sc = RobustScaler()\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\ndata = sc.fit_transform(data)","041c95c9":"from sklearn.cluster import KMeans\nwcss =[]\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i , init='k-means++' ,random_state = 42)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,11) , wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Cluster')\nplt.ylabel('wcss')\nplt.legend()\nplt.show()","f156c4bf":"kmeans = KMeans(n_clusters=4 , init='k-means++' , random_state=42)\nkmeans = kmeans.fit(data)\nclusters = kmeans.predict(data)","eb248933":"data = pd.DataFrame(data)","c17d4729":"data.head()","a5d2cfea":"from sklearn.decomposition import PCA \npca = PCA(n_components = 2)\nreduced_data = pca.fit_transform(data)\nexplained_varience = pca.explained_variance_ratio_","3c63c34c":"reduced_data.shape  , data.shape , clusters.shape","72a8c45d":"reduced_data=pd.DataFrame(reduced_data, columns=['PC1', 'PC2'])\nred_data_2 = reduced_data.copy()\nred_data_2 = pd.DataFrame(red_data_2 , columns=['PC1' , 'PC2'])\nreduced_data.head()","f50bab38":"reduced_data['clusters'] = clusters\nreduced_data.head()","cf9f6223":"plt.figure(figsize=(8,6))\nplt.scatter(reduced_data.loc[reduced_data['clusters'] == 0 , 'PC1'] , reduced_data.loc[reduced_data['clusters'] ==0 , 'PC2'] , c='r' , label='cluster 0')\nplt.scatter(reduced_data.loc[reduced_data['clusters'] == 1 , 'PC1'] , reduced_data.loc[reduced_data['clusters'] ==1 , 'PC2'] , c='b' ,label= 'Cluster 1')\nplt.scatter(reduced_data.loc[reduced_data['clusters'] == 2 , 'PC1'] , reduced_data.loc[reduced_data['clusters'] ==2 , 'PC2'] , c='g' , label='cluster 2')\nplt.scatter(reduced_data.loc[reduced_data['clusters'] == 3 , 'PC1'] , reduced_data.loc[reduced_data['clusters'] ==3 , 'PC2'] , c='cyan' ,label= 'Cluster 3')\nplt.title('Credit Card Segmentation')\nplt.legend()\nplt.show()\n\n","30a36f30":"import scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(data, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distances')\nplt.show()\n\n","4146de90":"# Fitting Hierarchical Clustering to the dataset\nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(data)","785a314f":"red_data_2['clusters'] = y_hc","9e7b135a":"plt.figure(figsize=(8,6))\nplt.scatter(red_data_2.loc[red_data_2['clusters'] == 0 , 'PC1'] , red_data_2.loc[red_data_2['clusters'] ==0 , 'PC2'] , c='r' , label='cluster 0')\nplt.scatter(red_data_2.loc[red_data_2['clusters'] == 1 , 'PC1'] , red_data_2.loc[red_data_2['clusters'] ==1 , 'PC2'] , c='b' ,label= 'Cluster 1')\nplt.scatter(red_data_2.loc[red_data_2['clusters'] == 2 , 'PC1'] , red_data_2.loc[red_data_2['clusters'] ==2 , 'PC2'] , c='g' ,label= 'Cluster 2')\nplt.title('Credit Card Segmentation')\nplt.legend()\nplt.show()","724687e4":"**Handle Outliers**","f4d16203":"**Feature Engineering**<br>\nDrop CUST_ID","a8f3a96d":"using Dendogram to find optimal cluster","3ea89772":"**Handle missing values**<hr>\nthere is missing values in (credit_limit , minimum[](http:\/\/)_payment)","96051b15":"there is outlier in (Purchase , one-off-purchase , installments_purchase , Cash_advanced)","02bdc7d3":"**<h4>Explore Outliers<\/h4>**","68d9a8ba":"explore correlation between features ","c1526799":"Read Data ","d593e8b0":"**<h2>preprocessing<\/h2>**","c42ed53d":"**Explore Data**","151cd3da":"import libraries ","8508acf0":"there is missing values in (Minimum_payments and credit_limit)","c7eb8ccc":"Visualize Clusters","ea82ad26":"**Build Model**","e67d1b35":"**Feature Scaling**","41526a9c":"<h2>Using Hierarchical Clustering<\/h2>"}}