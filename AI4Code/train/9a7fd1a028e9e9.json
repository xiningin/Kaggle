{"cell_type":{"13216f26":"code","e5db04dc":"code","1abb67b8":"code","f00a44a9":"code","96d6b69a":"code","9f3ea39e":"code","a735bfd4":"code","9ebf3896":"code","dc1917ef":"code","9032d9ee":"code","f50196ee":"code","9b2b5842":"code","70435617":"code","93e7b6f8":"code","78e6327f":"code","79429c76":"markdown","a62fac6b":"markdown","56405ac0":"markdown","a3e6ac4d":"markdown","869fa0f1":"markdown","363259f1":"markdown"},"source":{"13216f26":"import logging\nfrom nltk import word_tokenize\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.metrics import f1_score\nfrom torch import optim\nimport torchtext\nimport random","e5db04dc":"text = torchtext.data.Field(lower=True, batch_first=True, tokenize=word_tokenize)\nqid = torchtext.data.Field()\ntarget = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\ntrain = torchtext.data.TabularDataset(path='..\/input\/train.csv', format='csv',\n                                      fields={'question_text': ('text',text),\n                                              'target': ('target',target)})\ntest = torchtext.data.TabularDataset(path='..\/input\/test.csv', format='csv',\n                                     fields={'qid': ('qid', qid),\n                                             'question_text': ('text', text)})","1abb67b8":"text.build_vocab(train, test, min_freq=3)\nqid.build_vocab(test)\ntext.vocab.load_vectors(torchtext.vocab.Vectors('..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'))\nprint(text.vocab.vectors.shape)","f00a44a9":"random.seed(2018)\ntrain, val = train.split(split_ratio=0.9, random_state=random.getstate())","96d6b69a":"class BiLSTM(nn.Module):\n    def __init__(self, pretrained_lm, padding_idx, static=True, hidden_dim=128, lstm_layer=2, dropout=0.2):\n        super(BiLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout = nn.Dropout(p=dropout)\n        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n        self.embedding.padding_idx = padding_idx\n        if static:\n            self.embedding.weight.requires_grad = False\n        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n                            hidden_size=hidden_dim,\n                            num_layers=lstm_layer, \n                            dropout = dropout,\n                            bidirectional=True)\n        self.hidden2label = nn.Linear(hidden_dim*lstm_layer*2, 1)\n    \n    def forward(self, sents):\n        x = self.embedding(sents)\n        x = torch.transpose(x, dim0=1, dim1=0)\n        lstm_out, (h_n, c_n) = self.lstm(x)\n        y = self.hidden2label(self.dropout(torch.cat([c_n[i,:, :] for i in range(c_n.shape[0])], dim=1)))\n        return y","9f3ea39e":"def training(epoch, model, eval_every, loss_func, optimizer, train_iter, val_iter, early_stop=1, warmup_epoch=2):\n    \n    step = 0\n    max_loss = 1e5\n    no_improve_epoch = 0\n    no_improve_in_previous_epoch = False\n    fine_tuning = False\n    train_record = []\n    val_record = []\n    losses = []\n    \n    for e in range(epoch):\n        if e >= warmup_epoch:\n            if no_improve_in_previous_epoch:\n                no_improve_epoch += 1\n                if no_improve_epoch >= early_stop:\n                    break\n            else:\n                no_improve_epoch = 0\n            no_improve_in_previous_epoch = True\n        if not fine_tuning and e >= warmup_epoch:\n            model.embedding.weight.requires_grad = True\n            fine_tuning = True\n        train_iter.init_epoch()\n        for train_batch in iter(train_iter):\n            step += 1\n            model.train()\n            x = train_batch.text.cuda()\n            y = train_batch.target.type(torch.Tensor).cuda()\n            model.zero_grad()\n            pred = model.forward(x).view(-1)\n            loss = loss_function(pred, y)\n            losses.append(loss.cpu().data.numpy())\n            train_record.append(loss.cpu().data.numpy())\n            loss.backward()\n            optimizer.step()\n            if step % eval_every == 0:\n                model.eval()\n                model.zero_grad()\n                val_loss = []\n                for val_batch in iter(val_iter):\n                    val_x = val_batch.text.cuda()\n                    val_y = val_batch.target.type(torch.Tensor).cuda()\n                    val_pred = model.forward(val_x).view(-1)\n                    val_loss.append(loss_function(val_pred, val_y).cpu().data.numpy())\n                val_record.append({'step': step, 'loss': np.mean(val_loss)})\n                print('epcoh {:02} - step {:06} - train_loss {:.4f} - val_loss {:.4f} '.format(\n                            e, step, np.mean(losses), val_record[-1]['loss']))\n                if e >= warmup_epoch:\n                    if val_record[-1]['loss'] <= max_loss:\n                        save(m=model, info={'step': step, 'epoch': e, 'train_loss': np.mean(losses),\n                                            'val_loss': val_record[-1]['loss']})\n                        max_loss = val_record[-1]['loss']\n                        no_improve_in_previous_epoch = False\n    \n\ndef save(m, info):\n    torch.save(info, 'best_model.info')\n    torch.save(m, 'best_model.m')\n    \ndef load():\n    m = torch.load('best_model.m')\n    info = torch.load('best_model.info')\n    return m, info\n\n                ","a735bfd4":"batch_size = 128\ntrain_iter = torchtext.data.BucketIterator(dataset=train,\n                                               batch_size=batch_size,\n                                               sort_key=lambda x: x.text.__len__(),\n                                               shuffle=True,\n                                               sort=False)\nval_iter = torchtext.data.BucketIterator(dataset=val,\n                                             batch_size=batch_size,\n                                             sort_key=lambda x: x.text.__len__(),\n                                             train=False,\n                                             sort=False)\nmodel = BiLSTM(text.vocab.vectors, lstm_layer=2, padding_idx=text.vocab.stoi[text.pad_token], hidden_dim=128).cuda()\n# loss_function = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([pos_w]).cuda())\nloss_function = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n                    lr=1e-3)","9ebf3896":"training(model=model, epoch=20, eval_every=500,\n         loss_func=loss_function, optimizer=optimizer, train_iter=train_iter,\n        val_iter=val_iter, warmup_epoch=3, early_stop=2)","dc1917ef":"model, m_info = load()\nm_info","9032d9ee":"model.lstm.flatten_parameters()","f50196ee":"model.eval()\nval_pred = []\nval_true = []\nval_iter.init_epoch()\nfor val_batch in iter(val_iter):\n    val_x = val_batch.text.cuda()\n    val_true += val_batch.target.data.numpy().tolist()\n    val_pred += torch.sigmoid(model.forward(val_x).view(-1)).cpu().data.numpy().tolist()","9b2b5842":"tmp = [0,0,0] # idx, cur, max\ndelta = 0\nfor tmp[0] in np.arange(0.1, 0.501, 0.01):\n    tmp[1] = f1_score(val_true, np.array(val_pred)>tmp[0])\n    if tmp[1] > tmp[2]:\n        delta = tmp[0]\n        tmp[2] = tmp[1]\nprint('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))","70435617":"model.eval()\nmodel.zero_grad()\ntest_iter = torchtext.data.BucketIterator(dataset=test,\n                                    batch_size=batch_size,\n                                    sort_key=lambda x: x.text.__len__(),\n                                    sort=True)\ntest_pred = []\ntest_id = []\n\nfor test_batch in iter(test_iter):\n    test_x = test_batch.text.cuda()\n    test_pred += torch.sigmoid(model.forward(test_x).view(-1)).cpu().data.numpy().tolist()\n    test_id += test_batch.qid.view(-1).data.numpy().tolist()\n    ","93e7b6f8":"sub_df =pd.DataFrame()\nsub_df['qid'] = [qid.vocab.itos[i] for i in test_id]\nsub_df['prediction'] = (np.array(test_pred) >= delta).astype(int)","78e6327f":"sub_df.to_csv(\"submission.csv\", index=False)","79429c76":"## Model Define","a62fac6b":"## model train","56405ac0":"##  Prediction","a3e6ac4d":"## Training","869fa0f1":"## train\/validate split","363259f1":"## Build Vocabulary"}}