{"cell_type":{"86e599dd":"code","e841d015":"code","e092b07d":"code","8e01e282":"code","aeb87959":"code","341f5155":"code","af24e783":"code","9b065a10":"code","427fdcdc":"code","a29d477a":"code","5a30bd86":"code","58ce6680":"code","4f99ad63":"code","991b4162":"code","e4d2c121":"code","e9b4f827":"code","fbaf51e3":"code","7a49adfb":"code","e083c4b5":"code","08fc63b8":"code","f9e75346":"markdown","ed4798a8":"markdown","a01b8499":"markdown","1e6498ec":"markdown","e7cfe6ba":"markdown","6f2f4af9":"markdown","f949a965":"markdown","3ae2bf68":"markdown","2899b52a":"markdown","1bb262ac":"markdown"},"source":{"86e599dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e841d015":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","e092b07d":"train['text'].iloc[2]","8e01e282":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","aeb87959":"stop_words = set(stopwords.words('english'))","341f5155":"word_tokens = word_tokenize(train['text'].iloc[2])\nprint(word_tokens)","af24e783":"filtered_tweet = [w for w in word_tokens if not w in stop_words]\nprint(filtered_tweet)","9b065a10":"from sklearn.feature_extraction import text","427fdcdc":"count_vectorizer = text.CountVectorizer()\n\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test['text'])\n\ntrain_vectors","a29d477a":"print(train_vectors[0].todense().shape)\nprint(train_vectors[0].todense())","5a30bd86":"def remove_stopwords(df):\n    \n    for i in range(len(df)):\n        \n        word_tokens = word_tokenize(df['text'].loc[i])\n        filtered_set = []\n        \n        for w in word_tokens:\n            if w not in stop_words:\n                filtered_set.append(w)\n                \n        filtered_sentence = ' '.join(filtered_set)\n        df['text'].iloc[i] = filtered_sentence","58ce6680":"remove_stopwords(train)\nremove_stopwords(test)","4f99ad63":"count_vectorizer = text.CountVectorizer()\n\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test['text'])\n\nprint(train_vectors[0].shape)","991b4162":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB","e4d2c121":"clf = RidgeClassifier()\nlogreg = LogisticRegression()\nsgd = SGDClassifier()\nsvc = LinearSVC()\nmnb = MultinomialNB()","e9b4f827":"clf_scores = cross_val_score(clf, train_vectors, train['target'], cv=10, scoring='f1')\nlogreg_scores = cross_val_score(logreg, train_vectors, train['target'], cv=10, scoring='f1')\nsgd_scores = cross_val_score(sgd, train_vectors, train['target'], cv=10, scoring='f1')\nsvc_scores = cross_val_score(svc, train_vectors, train['target'], cv=10, scoring='f1')\nmnb_scores = cross_val_score(mnb, train_vectors, train['target'], cv=10, scoring='f1')","fbaf51e3":"print(\"Ridge Classifier: \", np.mean(clf_scores))\nprint(\"Logistic Regression: \", np.mean(logreg_scores))\nprint(\"Stochastic Gradient Descent Classifier: \", np.mean(sgd_scores))\nprint(\"Support Vector Classifier: \", np.mean(svc_scores))\nprint(\"Multinomial Naive Bayes: \", np.mean(mnb_scores))","7a49adfb":"mnb.fit(train_vectors, train['target'])","e083c4b5":"preds = mnb.predict(test_vectors)\nresult = pd.DataFrame({'id':test['id'], 'target':preds})","08fc63b8":"result.to_csv(\"submission.csv\", index=False)","f9e75346":"# Implementing the above techniques:","ed4798a8":"**Pretty cool, isn't it? You can separate sentences using word_tokenize!**","a01b8499":"# Removing Stopwords(EXAMPLE): ","1e6498ec":"**If you use fit_transform for the test_vectors here, it returns the number of unique words in the test set, which is less than the train set. We want to use the unique words from the train set. So, we use transform to just convert the words to numbers, without returning the unique words.**","e7cfe6ba":"**There you go! We remove unnecessary words just like that.**","6f2f4af9":"**A small difference of 21637-21617 = 20 words, but each small change counts.**","f949a965":"**To fit a model to anything, be it words or images, we gotta convert them to numbers, to work with them!**","3ae2bf68":"**Boy, there are 21,637 unique tokens in the tweets!**","2899b52a":"# Converting words to numbers(EXAMPLE):","1bb262ac":"**Clearly, the best model to use here is the Multinomial Naive Bayes classifier.**"}}