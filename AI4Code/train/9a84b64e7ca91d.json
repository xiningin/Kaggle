{"cell_type":{"5f72cc35":"code","d62c9384":"code","15e138a4":"code","82254b62":"code","fecfc056":"code","a21b5641":"code","7e3ec045":"code","55ddd9c3":"code","dcdba7e1":"code","41c20b74":"code","f7c2e31f":"code","e7ba73a6":"code","bcd3f374":"code","d50a4c52":"code","4c096cbc":"markdown","c3e3a0c9":"markdown","e7f006d7":"markdown","5b80a852":"markdown","88a4bf89":"markdown","184f673e":"markdown"},"source":{"5f72cc35":"#This librarys is to work with matrices\nimport pandas as pd \n# This librarys is to work with vectors\nimport numpy as np\n# This library is to create some graphics algorithmn\nimport seaborn as sns\n# to render the graphs\nimport matplotlib.pyplot as plt\n#This library use for building ANN model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nimport keras\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input\n#This library use for data preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n# This function makes the plot directly on browser\n%matplotlib inline","d62c9384":"import optuna\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nimport kerastuner as kt\nfrom sklearn.metrics import log_loss","15e138a4":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","82254b62":"for i in range(50):\n    mean, std = train[f'feature_{i}'].mean(), train[f'feature_{i}'].std()\n    train[f'feature_{i}'] = train[f'feature_{i}'].apply(lambda x : (x-mean)\/std)\n    test[f'feature_{i}'] = test[f'feature_{i}'].apply(lambda x : (x-mean)\/std)\nlabel = {var:index for index, var in enumerate(sorted(train['target'].unique()))}\ntrain['target'] = train['target'].map(label)\n\ntarget = train['target']\ntrain.drop(['target'], inplace=True, axis=1)\ntrain = train.values\ntarget = target.values\ntarget =  to_categorical(target)\nX_train, X_val, y_train, y_val = train_test_split(train, target, test_size = 0.1, random_state = 2, stratify=target)","fecfc056":"def create_model1(hp, num_columns, num_labels):\n    model = Sequential()\n    model.add(Dense(hp.Int('hidden_units_1', 9, 99, 3), activation='relu',input_dim = num_columns,kernel_initializer='uniform'))\n    model.add(Dropout(hp.Float(\"dropout_rate_1\", 0.01, 0.5)))\n    model.add(Dense(hp.Int('hidden_units_2', 9, 99, 3), kernel_initializer = 'uniform',activation ='relu'))\n    model.add(Dropout(hp.Float(\"dropout_rate_2\", 0.01, 0.5)))\n    model.add(Dense(hp.Int('hidden_units_3', 9, 150, 3), kernel_initializer = 'uniform',activation ='relu'))\n    model.add(Dropout(hp.Float(\"dropout_rate_3\", 0.01, 0.5)))\n    model.add(Dense(num_labels, kernel_initializer='uniform', \n                    activation = hp.Choice('out_activation',['softmax','sigmoid'])))\n    model.compile(optimizer = 'sgd', \n                       loss = 'categorical_crossentropy', \n                       metrics = ['accuracy'])\n    return model","a21b5641":"params = {'num_columns': 50, 'num_labels': 4}\nEPOCHS = 3\nMAX_TRIAL = 100\nmodel_fn = lambda hp: create_model1(hp, **params)\ntuner = kt.tuners.BayesianOptimization(model_fn, kt.Objective('accuracy', direction='max'), MAX_TRIAL, seed = 2020)\ntuner.search(X_train, y_train, epochs=EPOCHS, validation_data = (X_val, y_val))\ntuner.results_summary()","7e3ec045":"model1 = tuner.get_best_models()[0]\nmodel1.fit(X_train, y_train, epochs = 30, verbose = 0)","55ddd9c3":"scores = model1.evaluate(X_train, y_train, batch_size=30)\nprint(\"%s: %.2f%%\" % (model1.metrics_names[1], scores[1]*100))","dcdba7e1":"sample_submission[['Class_1','Class_2', 'Class_3', 'Class_4']] = model1.predict(test)\nsample_submission.to_csv('BO.csv',index=False)\nsample_submission","41c20b74":"def create_model2(num_columns, num_labels, hidden_units_1, hidden_units_2, hidden_units_3,\n                dropout_rate_1, dropout_rate_2, dropout_rate_3, out_activation):\n    hidden_units = [hidden_units_1, hidden_units_2, hidden_units_3]\n    dropout_rates = [dropout_rate_1, dropout_rate_2, dropout_rate_3]\n    model = Sequential()\n    model.add(Dense(hidden_units[0], activation='relu',input_dim = num_columns, kernel_initializer='uniform'))\n    model.add(Dropout(dropout_rates[0]))\n    model.add(Dense(hidden_units[1], kernel_initializer = 'uniform',activation ='relu'))\n    model.add(Dropout(dropout_rates[1]))\n    model.add(Dense(hidden_units[2], kernel_initializer = 'uniform',activation ='relu'))\n    model.add(Dropout(dropout_rates[2]))\n    model.add(Dense(num_labels, kernel_initializer='uniform', \n                    activation = out_activation))\n    model.compile(optimizer = 'sgd', \n                       loss = 'categorical_crossentropy', \n                       metrics = ['accuracy'])\n    return model\ndef objective(trial , X = X_train , y = y_train):   \n    params = {'num_columns': 50, \n              'num_labels': 4, \n          'hidden_units_1': trial.suggest_int('hidden_units_1' ,9 , 99), \n          'hidden_units_2': trial.suggest_int('hidden_units_2' ,9 , 99), \n          'hidden_units_3': trial.suggest_int('hidden_units_3' ,9 , 150),\n          'dropout_rate_1': trial.suggest_uniform('dropout_rate_1' ,0.01 , 0.5), \n          'dropout_rate_2': trial.suggest_uniform('dropout_rate_2' ,0.01 , 0.5), \n          'dropout_rate_3': trial.suggest_uniform('dropout_rate_3' ,0.01 , 0.5),        \n          'out_activation': trial.suggest_categorical('out_activation' , ['softmax','sigmoid'])}\n    # pruning_callback = optuna.integration.KerasPruningCallback(trial, monitor = 'accuracy')\n    model2 = create_model2(**params)\n    model2.fit(X_train, y_train, epochs = 3, verbose = 0\n               #, callbacks = [pruning_callback]\n              )\n    y_pred = model2.predict(X_val)\n    ll = log_loss(y_val, y_pred)\n    return ll\n              \nstudy = optuna.create_study(direction = 'minimize' , study_name = 'Optuna NN'\n                        #    , pruner = optuna.pruners.HyperbandPruner()\n                           )\nstudy.optimize(objective, n_trials = 100)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)","f7c2e31f":"model2 = create_model2(**study.best_trial.params, num_columns = 50, num_labels = 4)\nmodel2.fit(X_train, y_train, epochs = 30, verbose = 0)","e7ba73a6":"scores = model2.evaluate(X_train, y_train, batch_size=30)\nprint(\"%s: %.2f%%\" % (model2.metrics_names[1], scores[1]*100))","bcd3f374":"sample_submission[['Class_1','Class_2', 'Class_3', 'Class_4']] = model2.predict(test)\nsample_submission.to_csv('Optuna.csv',index=False)\nsample_submission","d50a4c52":"sample_submission[['Class_1','Class_2', 'Class_3', 'Class_4']] = (model1.predict(test) + model2.predict(test)) \/ 2\nsample_submission.to_csv('avg.csv',index=False)\nsample_submission","4c096cbc":"**Evaluating the model**","c3e3a0c9":"reference: https:\/\/www.kaggle.com\/harunshimanto\/tps-2021-eda-build-an-artificial-neural-network by Harun-Ur-Rashid","e7f006d7":"# Tuning packages","5b80a852":"# Optuna","88a4bf89":"# Bayesian Optimization","184f673e":"optimizers list\noptimizers['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']"}}