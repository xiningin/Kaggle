{"cell_type":{"08d7fb8f":"code","fcec04a0":"code","d0f8b582":"code","02972c70":"code","93363aa7":"code","4c11554c":"code","19098ee1":"code","4c6b145b":"code","f7c92317":"code","8eeb750d":"code","7da3db2d":"markdown","c724cf0d":"markdown","9b3cca0a":"markdown","3fc9e696":"markdown","23be8206":"markdown","b4a290cf":"markdown","681f67ee":"markdown","344f6002":"markdown","6b3cbed1":"markdown"},"source":{"08d7fb8f":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam, RMSprop, Adagrad\nfrom keras.layers import BatchNormalization\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport tensorflow as tf\nimport cv2\nimport os\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","fcec04a0":"labels = ['Negative', 'Positive']\nimg_size = 120\ndef read_images(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size)) \n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)\n\nDataset = read_images('..\/input\/surface-crack-detection')","d0f8b582":"Im = []\nfor i in Dataset:\n    if(i[1] == 0):\n        Im.append(\"Negative\")\n    elif(i[1] == 1):\n        Im.append(\"Positive\")\n\nplt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nsns.set_style('darkgrid')\naxl = sns.countplot(Im)\naxl.set_title(\"Number of Images\")","02972c70":"x = []\ny = []\n\nfor feature, label in Dataset:\n    x.append(feature)\n    y.append(label)\n\nx = np.array(x).reshape(-1, img_size, img_size, 1)\nx = x \/ 255\ny = np.array(y)","93363aa7":"plt.subplot(1, 2, 1)\nplt.imshow(x[1000].reshape(img_size, img_size), cmap='gray')\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(x[30000].reshape(img_size, img_size), cmap='gray')\nplt.axis('off')","4c11554c":"model = Sequential()\nmodel.add(Conv2D(64,3,padding=\"same\", activation=\"relu\", input_shape = x.shape[1:]))\nmodel.add(MaxPool2D())\n\nmodel.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D())\n\nmodel.add(Conv2D(128, 3, padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D())\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(2, activation=\"softmax\"))\n\nmodel.summary()","19098ee1":"opt = Adam(lr=1e-5)\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]) \n\nhistory = model.fit(x, y, epochs = 15, batch_size = 128, validation_split = 0.25, verbose=1)","4c6b145b":"print(history.history.keys())","f7c92317":"plt.figure(figsize=(12, 12))\nplt.style.use('ggplot')\nplt.subplot(2,2,1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Accuracy of the Model')\nplt.ylabel('Accuracy', fontsize=12)\nplt.xlabel('Epoch', fontsize=12)\nplt.legend(['train accuracy', 'validation accuracy'], loc='lower right', prop={'size': 12})\n\nplt.subplot(2,2,2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss of the Model')\nplt.ylabel('Loss', fontsize=12)\nplt.xlabel('Epoch', fontsize=12)\nplt.legend(['train loss', 'validation loss'], loc='best', prop={'size': 12})","8eeb750d":"from sklearn.metrics import classification_report,confusion_matrix\n\npredictions = model.predict_classes(x)\npredictions = predictions.reshape(1,-1)[0]\nprint(classification_report(y, predictions, target_names = ['Negative','Positive']))","7da3db2d":"<a id = \"5\"><\/a><br>\n# **Model Training**\n\n* Accuracy can be increased by increasing the number of epochs.\n* Learning rate is also important here. Better results can be achieved by training at different learning rates. The best learning rate can be found by trying. Or grid search method can be used. But this increases the training time considerably.\n\nIf a low learning rate is selected; the training speed slows down, if the high learning rate is selected; the training will speed up, but the accuracy will decrease.","c724cf0d":"<a id = \"2\"><\/a><br>\n# Visualizing the Dataset","9b3cca0a":"<a id = \"1\"><\/a><br>\n# Load Dataset","3fc9e696":"<a id = \"6\"><\/a><br>\n# **Graphs**","23be8206":"<a id = \"4\"><\/a><br>\n# CNN Model\n\n\nConvolutional neural networks (CNN) are a neural network method that can include multiple arrays created to process 2-dimensional arrays of three color components (Lecun et al., 2010). In images, edges called local features form patterns and these patterns combine into pieces, then pieces form objects (Bengio & Lecun, 1997). These properties are obtained in the convolution layer and these properties are brought into a format that can be semantically combined and processed in the pooling layer.\n\n\nCNN; It is a type of algorithm that consists of an input, an output, and many hidden layers. The hidden layer includes convolution layer, pooling layer, rectified linear unit layer (ReLu), fully connected layer and classification sections (Lecun et al., 2010).\n\n* **Convolution layer** is used in convolutional neural networks to perform convolution process in multi-dimensional processes. This layer enables the adjustment of the neurons in the image matrix, which is defined as the input called feature map, and enables the learning of the properties.\n\n* **ReLu** fulfills the task of flattening the feature map that emerges after the convolution process. By converting negative values to zero, it produces output between zero and positive infinite values.\n\n* **Pooling layer** performs the size reduction operation by performing the function operation defined as subsampling (Kalchbrenner et al., 2014). In addition, thanks to this layer, excessive memorization is prevented.\n\nRegularization in the training phase in CNN. Data augmentation is an important element for regularization of weights and batch normalization (Srivastava et al., 2014). For this reason, the method called Dropout is used. Its main purpose is to *prevent overfitting*.","b4a290cf":"<a id = \"8\"><\/a><br>\n# Result\n\n* As a result, 99% success has been achieved. \n* The accuracy and loss function can change by changing the learning rates or changing the number of epoch.\n\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#F12C3B;\n           font-size:200%;\n           font-family:Cambria;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 20px;\n              color:white;\">Thank you\n<\/p>\n<\/div>","681f67ee":"\n<font color = 'blue'>\nContent: \n\n1. [Load Dataset](#1)\n1. [Visualizing the Dataset](#2)\n1. [Normalization of image data](#3)\n1. [Convolutional Neural Network (CNN) Model](#4)\n    * [Model Training](#5)\n    * [Accuracy and Loss Graphs](#6) \n    * [Classification Report](#7)  \n1. [Result](#8)\n","344f6002":"<a id = \"3\"><\/a><br>\n# Normalization of image data","6b3cbed1":"<a id = \"7\"><\/a><br>\n# Classification Report\n\n* The classification_report function builds a text report showing the main classification metrics. \n\n* **Precision** for each class, it is defined as the ratio of true positives to the sum of true and false positives.\n\n* **Recall** for each class, it is defined as the ratio of true positives to the sum of true positives and false negatives.\n* **F1 scores** are lower than accuracy measures as they embed precision and recall into their computation."}}