{"cell_type":{"7336dbec":"code","8d6bdfe2":"code","eda12f14":"code","5c337490":"code","58b415e4":"code","d6b4e522":"code","6d54ac40":"code","be4542df":"code","eb27173c":"code","17de1390":"code","37049d66":"code","d26225b2":"code","f41e67c7":"code","13b25253":"code","bf21c8fc":"code","f4777b10":"code","51e1e17a":"code","9ac74a27":"code","ba9471eb":"code","ee9636d1":"code","c4cde736":"code","cd17a391":"code","a15d9d97":"code","4399cdc7":"code","e76eb4db":"code","4a037ac1":"code","4b9735cf":"code","1f6f03ad":"code","c78e1ef8":"code","232665b0":"code","b63b8e41":"code","3febd4c5":"code","ea5ea232":"code","b13420ec":"code","e18f50a2":"code","42ecaf3a":"code","c750acef":"code","c4e8cc1b":"code","eb2a835a":"code","39733d67":"code","cdbb96eb":"code","9e6d999a":"code","a72b0167":"code","eaad7612":"code","74a5180d":"code","3678f36b":"code","130d6a27":"code","297378b3":"code","32d25fee":"code","ee0cacac":"code","86e4db23":"code","e95b6a30":"code","0a8ce2e7":"code","bb27746a":"code","92ef3a07":"code","b47dd0b3":"code","53b4f3a6":"code","0ee93682":"code","8efa57f8":"code","c4d3eae4":"code","bfa3204a":"code","a1f7a5a0":"code","af61a30d":"code","d5a03354":"code","bb083b85":"code","11b890a8":"code","92f3eaee":"code","64ddd40d":"markdown","fb45e734":"markdown","14034b6d":"markdown","9dc2b4d4":"markdown","493f6526":"markdown","0873164a":"markdown","58ad35b4":"markdown","da84a53d":"markdown","1f1b1240":"markdown","c344f09b":"markdown","ce265a44":"markdown","503075c4":"markdown","cbf1f44a":"markdown","7fe98cf1":"markdown","69657f3a":"markdown","e92b8d5c":"markdown","6b63de0b":"markdown","1bfab2ef":"markdown","84b0bee9":"markdown","04fa9b9f":"markdown","352e2567":"markdown","ee4c680a":"markdown","153c5fd6":"markdown","55623ec9":"markdown","cf7eb87f":"markdown","c52ddfb4":"markdown","852ce7f4":"markdown","1552656f":"markdown","45b6b6ea":"markdown","53e0cfae":"markdown","247bff2d":"markdown","f1a53a82":"markdown","8c14be71":"markdown","0223cdb5":"markdown","14585a24":"markdown","6852297d":"markdown","dfccbe00":"markdown","7349bbbb":"markdown","32c4106e":"markdown","710a80a3":"markdown","120b60c6":"markdown","2c1b0253":"markdown","da77e681":"markdown","0a748aee":"markdown","65a70d70":"markdown","44564e88":"markdown","e7e74cbd":"markdown","745203a4":"markdown","2b4b2990":"markdown","0eeb35e4":"markdown","f94aac31":"markdown","025ccdfc":"markdown","0e9846c6":"markdown","c70de3fa":"markdown","5cd8702a":"markdown","9d7d1221":"markdown","520160b2":"markdown","8e471a1e":"markdown","97b5b648":"markdown","d7394b88":"markdown","eebe1d13":"markdown","28ed6dd7":"markdown"},"source":{"7336dbec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D , Dropout, Activation, Flatten, Input, MaxPooling2D\nfrom keras.utils import to_categorical, plot_model\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8d6bdfe2":"numbers_df=pd.read_csv('\/kaggle\/input\/emnist\/emnist-mnist-train.csv',header=None)\ntest_df=pd.read_csv('\/kaggle\/input\/emnist\/emnist-mnist-test.csv',header=None)\nnumbers_df.head()","eda12f14":"numbers_df.shape","5c337490":"sns.countplot(data=numbers_df,x=0,palette='pastel')\nsns.countplot(data=test_df,x=0)\nplt.show()","58b415e4":"def vis(dataset,nr_samples,label_col,legend=False,cmap='gray_r',cbar=False,transpose=True,prediction=False,dict_name=None):\n    samples=dataset.iloc[np.random.randint(0,dataset.shape[0],size=(1,nr_samples))[0]].reset_index(drop=True) # Picks n random samples from the dataset\n    labels=samples[label_col].values\n    samples.drop(label_col,axis=1,inplace=True)\n    if prediction==True:\n        preds=samples['Prediction'].values\n        samples.drop('Prediction',axis=1,inplace=True)\n    fig, ax = plt.subplots(2,nr_samples\/\/2,sharey=True,sharex=True)\n    for i in range(nr_samples):\n        pixels=samples.iloc[i].values\n        pixels=pixels.reshape((28,28))\n        if transpose==True:\n            pixels=pixels.transpose()\n        sns.heatmap(pixels,cmap=cmap,cbar=cbar,ax=ax.flatten()[i])\n        ax.flatten()[i].axes.get_xaxis().set_visible(False)\n        ax.flatten()[i].axes.get_yaxis().set_visible(False)\n    plt.tight_layout()\n    plt.show()\n    if legend==True:\n        if dict_name!=None:\n            labels=[dict_name[i] for i in labels]\n        if prediction==True:\n            print('The images represent items with labels {} which were predicted to be {}.'.format(labels,preds))\n        else:\n            print('The images represent items with labels {}.'.format(labels))","d6b4e522":"vis(numbers_df,10,label_col=0,legend=True)","6d54ac40":"features_df=numbers_df.drop(0,axis=1)\nlabels_df=numbers_df[0]\nscaler=MinMaxScaler(feature_range=(0,1))\nscaler.fit(features_df)\nfeatures_df=pd.DataFrame(scaler.transform(features_df),columns=features_df.columns)\nfeatures_df['Label']=labels_df\nfeatures_df.head()\n","be4542df":"vis(features_df,10,label_col='Label',legend=True)","eb27173c":"test_features_df=test_df.drop(0,axis=1)\ntest_labels_df=test_df[0]\ntest_features_df=pd.DataFrame(scaler.transform(test_features_df),columns=test_features_df.columns)\ntest_features_df['Label']=test_labels_df","17de1390":"def MLP(input_shape,nodes=[128,32],dropout_chance=0.4,num_classes=10,produce_output=True):\n    # If a 0 is inserted into the nodes, that means a Dropout layer is to be added\n    model=Sequential()\n    model.add(Dense(nodes[0],input_shape=input_shape,activation='relu'))\n    num_hidd_layers=len(nodes)\n    for i in range(num_hidd_layers-1):\n        if nodes[i+1]==0:\n            model.add(Dropout(dropout_chance))\n        else:\n            model.add(Dense(nodes[i+1],activation='relu'))\n    if produce_output==True:\n        model.add(Dense(num_classes,activation='softmax'))\n    return model","37049d66":"# Define the model\nmodel=MLP((784,),nodes=[256,128,32])\n\n# Compile and fit the model\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nhistory = model.fit(features_df.drop('Label',axis=1),to_categorical(features_df['Label']),nb_epoch=25,validation_split=0.2,batch_size=128,verbose=1)","d26225b2":"def learning_curve(history,titles,legend,metrics=['accuracy','loss']):\n    nplots=len(metrics)\n    fig , ax = plt.subplots(1,nplots,sharex=True)\n    fig.set_figheight(5)\n    fig.set_figwidth(15)\n    for i in range(nplots):\n        ax[i].plot(history.history[metrics[i]])\n        ax[i].plot(history.history['val_{}'.format(metrics[i])])\n        ax[i].set(xlabel='Epoch', ylabel=metrics[i])\n        ax[i].legend(legend)\n        ax[i].title.set_text(titles[i])\n    plt.tight_layout()\n    plt.show()\n    ","f41e67c7":"learning_curve(history,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])","13b25253":"test_preds = model.predict_classes(test_features_df.drop('Label',axis=1))\nscore, acc = model.evaluate(test_features_df.drop('Label',axis=1),to_categorical(test_features_df['Label']))\nprint('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score, acc))","bf21c8fc":"test_preds_df=pd.DataFrame()\ntest_preds_df['Prediction']=test_preds\ntest_preds_df['Label']=test_features_df['Label']\nmissclassified=test_preds_df[test_preds_df.Prediction!=test_preds_df.Label]\nmissclassified_index=missclassified.index.to_list()\nprint('{} images (out of 10 000) were missclassified!'.format(len(missclassified_index)))","f4777b10":"missclassified_features_df=test_features_df.iloc[missclassified_index].copy()\nmissclassified_features_df['Prediction']=missclassified['Prediction']\nmissclassified_features_df.reset_index(inplace=True,drop=True)\nmissclassified_features_df.head()","51e1e17a":"vis(missclassified_features_df,10,'Label',legend=True,prediction=True)","9ac74a27":"def CNN(input_shape,num_kernels=[20,20],kernel_shapes=[(3,3),(3,3)],dense_nodes=[128],dropout_chance=0.4,num_classes=10,produce_output=True):\n    # A 0 inserted either in num_kernels or in dense_nodes means a Dropout layer is to be inserted at that point\n    # If it is inserted in the convolutional layers, then some value must be adde in the corresponding place in kernel_shapes\n    model = Sequential()\n    model.add(Conv2D(num_kernels[0],kernel_size=kernel_shapes[0],activation='relu',input_shape=input_shape))\n    num_conv_layers = len(num_kernels)\n    for i in range(num_conv_layers-1):\n        if num_kernels[i+1]==0:\n            model.add(Dropout(dropout_chance))\n        else:\n            model.add(Conv2D(num_kernels[i+1],kernel_size=kernel_shapes[i+1],activation='relu'))\n            model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n    num_dense_layers = len(dense_nodes)\n    model.add(Flatten())\n    for j in range(num_dense_layers):\n        if dense_nodes[j]==0:\n            model.add(Dropout(dropout_chance))\n        else:\n            model.add(Dense(dense_nodes[j],activation='relu'))\n    if produce_output==True:\n        model.add(Dense(num_classes,activation='softmax'))\n    return model\n    ","ba9471eb":"num_images=features_df.shape[0]\nX=features_df.drop('Label',axis=1).values.reshape(num_images,28,28,1)\ny=features_df['Label'].values","ee9636d1":"# Define the model\ncnn_model = CNN((28,28,1),num_kernels=[20,30],kernel_shapes=[(3,3),(4,4)],dense_nodes=[256,128,32])\n\n# Compile and fit the model\ncnn_model.compile(loss = 'categorical_crossentropy',optimizer = 'rmsprop',metrics = ['accuracy'])\ncnn_history = cnn_model.fit(X,to_categorical(y),nb_epoch = 25,validation_split = 0.2,batch_size = 128,verbose = 1)","c4cde736":"learning_curve(cnn_history,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])","cd17a391":"test_images=test_features_df.shape[0]\nX_test=test_features_df.drop('Label',axis=1).values.reshape(test_images,28,28,1)\ny_test=test_features_df['Label']","a15d9d97":"cnn_test_preds = cnn_model.predict_classes(X_test)\nscore, acc = cnn_model.evaluate(X_test,to_categorical(y_test))\nprint('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score, acc))","4399cdc7":"cnn_test_preds_df=pd.DataFrame()\ncnn_test_preds_df['Prediction']=cnn_test_preds\ncnn_test_preds_df['Label']=test_features_df['Label']\ncnn_missclassified=cnn_test_preds_df[cnn_test_preds_df.Prediction!=cnn_test_preds_df.Label]\ncnn_missclassified_index=cnn_missclassified.index.to_list()\nprint('{} images (out of 10 000) were missclassified!'.format(len(cnn_missclassified_index)))","e76eb4db":"cnn_missclassified_features_df=test_features_df.iloc[cnn_missclassified_index].copy()\ncnn_missclassified_features_df['Prediction']=cnn_missclassified['Prediction']\ncnn_missclassified_features_df.reset_index(inplace=True,drop=True)","4a037ac1":"vis(cnn_missclassified_features_df,10,'Label',legend=True,prediction=True)","4b9735cf":"# Define model\nmodel_do=MLP((784,),nodes=[256,0,128,32])\n\n# Compile and fit model\nmodel_do.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nhistory_do = model_do.fit(features_df.drop('Label',axis=1),to_categorical(features_df['Label']),nb_epoch=25,validation_split=0.2,batch_size=128,verbose=1)","1f6f03ad":"learning_curve(history_do,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])","c78e1ef8":"test_preds = model_do.predict_classes(test_features_df.drop('Label',axis=1))\nscore, acc = model_do.evaluate(test_features_df.drop('Label',axis=1),to_categorical(test_features_df['Label']))\nprint('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score, acc))","232665b0":"test_preds_df=pd.DataFrame()\ntest_preds_df['Prediction']=test_preds\ntest_preds_df['Label']=test_features_df['Label']\nmissclassified=test_preds_df[test_preds_df.Prediction!=test_preds_df.Label]\nmissclassified_index=missclassified.index.to_list()\nprint('{} images (out of 10 000) were missclassified!'.format(len(missclassified_index)))","b63b8e41":"# Define model\ncnn_model_do = CNN((28,28,1),num_kernels=[20,30],kernel_shapes=[(3,3),(4,4)],dense_nodes=[0,256,0,128,32])\n\n# Compile and fit model\ncnn_model_do.compile(loss = 'categorical_crossentropy',optimizer = 'rmsprop',metrics = ['accuracy'])\ncnn_history_do = cnn_model_do.fit(X,to_categorical(y),nb_epoch = 25,validation_split = 0.2,batch_size = 128,verbose = 1)","3febd4c5":"learning_curve(cnn_history_do,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])","ea5ea232":"cnn_test_preds = cnn_model_do.predict_classes(X_test)\nscore, acc = cnn_model_do.evaluate(X_test,to_categorical(y_test))\nprint('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score, acc))","b13420ec":"cnn_test_preds_df=pd.DataFrame()\ncnn_test_preds_df['Prediction']=cnn_test_preds\ncnn_test_preds_df['Label']=test_features_df['Label']\ncnn_missclassified=cnn_test_preds_df[cnn_test_preds_df.Prediction!=cnn_test_preds_df.Label]\ncnn_missclassified_index=cnn_missclassified.index.to_list()\nprint('{} images (out of 10 000) were missclassified!'.format(len(cnn_missclassified_index)))","e18f50a2":"char_df = pd.read_csv('\/kaggle\/input\/emnist\/emnist-letters-train.csv',header=None)\ntest_char_df = pd.read_csv('\/kaggle\/input\/emnist\/emnist-letters-test.csv',header=None)","42ecaf3a":"# Normalization and changing the label column name to 'Label'\nlabels=char_df[0].tolist()\nchar_df.drop(0,axis=1,inplace=True)\nchar_df=char_df\/255\nchar_df['Label']=labels\n\ntest_labels=test_char_df[0].tolist()\ntest_char_df.drop(0,axis=1,inplace=True)\ntest_char_df=test_char_df\/255\ntest_char_df['Label']=test_labels\ntest_char_df.head()","c750acef":"fig, ax = plt.subplots(2)\nsns.countplot(data=char_df,x='Label',ax=ax[0])\nax[0].title.set_text('Training set')\nsns.countplot(data=test_char_df,x='Label',ax=ax[1])\nax[1].title.set_text('Test set')\nplt.tight_layout()\nplt.show()","c4e8cc1b":"vis(char_df,10,'Label',legend=True)","eb2a835a":"# Vision part of the model\ninputs=Input((28,28,1))\nconv1=Conv2D(20,(3,3),activation='relu')(inputs)\nconv2=Conv2D(30,(4,4),activation='relu')(conv1)\noutput1=Flatten()(conv2)\n\nvision_model=Model(inputs,output1)\n\ninputs2=vision_model(inputs)\n\n# Specific part of the model for digits\n\ndropout1=Dropout(0.4)(inputs2)\ndense1=Dense(256,activation='relu')(dropout1)\ndropout2=Dropout(0.4)(dense1)\ndense2=Dense(128,activation='relu')(dropout2)\ndense3=Dense(32,activation='relu')(dense2)\noutput2=Dense(10,activation='softmax')(dense3)\n\n\n# Specific part of the model for characters\n\ndropout1_c=Dropout(0.4)(inputs2)\ndense1_c=Dense(256,activation='relu')(dropout1_c)\ndropout2_c=Dropout(0.4)(dense1_c)\ndense2_c=Dense(128,activation='relu')(dropout2_c)\ndense3_c=Dense(32,activation='relu')(dense2_c)\noutput2_c=Dense(27,activation='softmax')(dense3_c)\n\n# Specific part of the model for fashion MNIST\n\ndropout1_f=Dropout(0.4)(inputs2)\ndense1_f=Dense(256,activation='relu')(dropout1_f)\ndropout2_f=Dropout(0.4)(dense1_f)\ndense2_f=Dense(128,activation='relu')(dropout2_f)\ndense3_f=Dense(32,activation='relu')(dense2_f)\noutput2_f=Dense(10,activation='softmax')(dense3_f)","39733d67":"# Define the model (same as previous CNN with Dropout)\ntraining_model=Model(inputs,outputs=output2)\n\n# Compile and fit the model\ntraining_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\ntraining_history = training_model.fit(X,to_categorical(y),nb_epoch = 25,validation_split = 0.2,batch_size = 128,verbose = 1)","cdbb96eb":"learning_curve(training_history,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])","9e6d999a":"digits_test_probs = training_model.predict(X_test)\ndigits_test_preds = [np.argmax(np.asarray(i)) for i in digits_test_probs]\nscore, acc = training_model.evaluate(X_test,to_categorical(y_test))\nprint('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score, acc))","a72b0167":"digits_test_preds_df=pd.DataFrame()\ndigits_test_preds_df['Prediction']=digits_test_preds\ndigits_test_preds_df['Label']=test_features_df['Label']\ndigits_missclassified=digits_test_preds_df[digits_test_preds_df.Prediction!=digits_test_preds_df.Label]\ndigits_missclassified_index=digits_missclassified.index.to_list()\nprint('{} images (out of 10 000) were missclassified!'.format(len(digits_missclassified_index)))","eaad7612":"num_chars=char_df.shape[0]\nX_char=char_df.drop('Label',axis=1).values.reshape(num_chars,28,28,1)\ny_char=char_df['Label'].values\n\nnum_test_chars=test_char_df.shape[0]\ntest_X_char=test_char_df.drop('Label',axis=1).values.reshape(num_test_chars,28,28,1)\ntest_y_char=test_char_df['Label'].values","74a5180d":"conv1.trainable = False\nconv2.trainable = False\noutput1.trainable = False\n\nchar_model=Model(inputs,output2_c)\nplot_model(char_model,show_shapes=True,show_layer_names=True,expand_nested=True)\nchar_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\nchar_history = char_model.fit(X_char,to_categorical(y_char),nb_epoch = 25,validation_split = 0.2,batch_size = 256,verbose = 1)","3678f36b":"learning_curve(char_history,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])","130d6a27":"char_test_probs = char_model.predict(test_X_char)\nchar_test_preds = [np.argmax(np.asarray(i)) for i in char_test_probs]\nfiller=[0.0,0.0,0.0,0.0,0.0,0.0,0.0] #adds 0s to the labels from 21 to 27 because there are no samples with tese labels in the test set\ny_vals=np.asarray([list(i)+filler for i in to_categorical(test_y_char)])\nscore_char, acc_char = char_model.evaluate(test_X_char,y_vals)\nprint('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score_char, acc_char))","297378b3":"char_test_preds_df=pd.DataFrame()\nchar_test_preds_df['Prediction']=char_test_preds\nchar_test_preds_df['Label']=test_char_df['Label']\nchar_missclassified=char_test_preds_df[char_test_preds_df.Prediction!=char_test_preds_df.Label]\nchar_missclassified_index=char_missclassified.index.to_list()\nprint('{} images (out of {}) were missclassified!'.format(len(char_missclassified_index),test_char_df.shape[0]))","32d25fee":"# Import data\nfashion_df = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest_fashion_df = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')","ee0cacac":"fashion_df.head()","86e4db23":"# Change format\nlabels=fashion_df['label'].tolist()\nfashion_df.drop('label',axis=1,inplace=True)\nfashion_df=fashion_df\/255\nfashion_df['Label']=labels\n\ntest_labels_f=test_fashion_df['label'].tolist()\ntest_fashion_df.drop('label',axis=1,inplace=True)\ntest_fashion_df=test_fashion_df\/255\ntest_fashion_df['Label']=test_labels_f\nfashion_df.head()","e95b6a30":"fig, ax = plt.subplots(2)\nsns.countplot(data=fashion_df,x='Label',ax=ax[0])\nax[0].title.set_text('Training set')\nsns.countplot(data=test_fashion_df,x='Label',ax=ax[1])\nax[1].title.set_text('Test set')\nplt.tight_layout()\nplt.show()","0a8ce2e7":"num_items=fashion_df.shape[0]\nX_fashion=fashion_df.drop('Label',axis=1).values.reshape(num_items,28,28,1)\ny_fashion=fashion_df['Label'].values\n\nnum_test_items=test_fashion_df.shape[0]\ntest_X_fashion=test_fashion_df.drop('Label',axis=1).values.reshape(num_test_items,28,28,1)\ntest_y_fashion=test_fashion_df['Label'].values","bb27746a":"# Dictionary for converting items into names.\n\nfashion_dict={0: 'T-shirt\/top', 1:'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}","92ef3a07":"vis(fashion_df,10,'Label',legend=True,transpose=False,dict_name=fashion_dict)","b47dd0b3":"conv1.trainable = False\nconv2.trainable = False\noutput1.trainable = False\n\nfashion_model=Model(inputs,output2_f)\nplot_model(fashion_model,show_shapes=True,show_layer_names=True,expand_nested=True)\nfashion_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\nfashion_history = fashion_model.fit(X_fashion,to_categorical(y_fashion),nb_epoch = 25,validation_split = 0.2,batch_size = 128,verbose = 1)","53b4f3a6":"learning_curve(fashion_history,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])","0ee93682":"fashion_test_probs = fashion_model.predict(test_X_fashion)\nfashion_test_preds = [np.argmax(np.asarray(i)) for i in fashion_test_probs]\nscore_fashion, acc_fashion = fashion_model.evaluate(test_X_fashion,to_categorical(test_y_fashion))\nprint('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score_fashion, acc_fashion))","8efa57f8":"fashion_test_preds_df=pd.DataFrame()\nfashion_test_preds_df['Prediction']=fashion_test_preds\nfashion_test_preds_df['Label']=test_fashion_df['Label']\nfashion_missclassified=fashion_test_preds_df[fashion_test_preds_df.Prediction!=fashion_test_preds_df.Label]\nfashion_missclassified_index=fashion_missclassified.index.to_list()\nprint('{} images (out of {}) were missclassified!'.format(len(fashion_missclassified_index),test_fashion_df.shape[0]))","c4d3eae4":"# Specific part of the model for characters 2.0\n\ndropout1_c2=Dropout(0.4)(inputs2)\ndense1_c2=Dense(512,activation='relu')(dropout1_c2)\ndropout2_c2=Dropout(0.4)(dense1_c2)\ndense2_c2=Dense(256,activation='relu')(dropout2_c2)\ndropout3_c2=Dropout(0.4)(dense2_c2)\ndense3_c2=Dense(64,activation='relu')(dropout3_c2)\noutput2_c2=Dense(27,activation='softmax')(dense3_c2)\n\n# Specific part of the model for fashion MNIST 2.0\n\ndropout1_f2=Dropout(0.4)(inputs2)\ndense1_f2=Dense(512,activation='relu')(dropout1_f2)\ndropout2_f2=Dropout(0.4)(dense1_f2)\ndense2_f2=Dense(256,activation='relu')(dropout2_f2)\ndropout3_f2=Dropout(0.4)(dense2_f2)\ndense3_f2=Dense(64,activation='relu')(dropout3_f2)\noutput2_f2=Dense(10,activation='softmax')(dense3_f2)","bfa3204a":"Let us test the improved Letter recognition model.","a1f7a5a0":"conv1.trainable = False\nconv2.trainable = False\noutput1.trainable = False\n\nchar_model_2=Model(inputs,output2_c2)\nplot_model(char_model_2,show_shapes=True,show_layer_names=True,expand_nested=True)\nchar_model_2.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\nchar_history_2 = char_model_2.fit(X_char,to_categorical(y_char),nb_epoch = 25,validation_split = 0.2,batch_size = 128,verbose = 1)","af61a30d":"learning_curve(char_history_2,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])","d5a03354":"char_test_probs = char_model_2.predict(test_X_char)\nchar_test_preds = [np.argmax(np.asarray(i)) for i in char_test_probs]\nfiller=[0.0,0.0,0.0,0.0,0.0,0.0,0.0] #adds 0s to the labels from 21 to 27 because there are no samples with tese labels in the test set\ny_vals=np.asarray([list(i)+filler for i in to_categorical(test_y_char)])\nscore_char, acc_char = char_model_2.evaluate(test_X_char,y_vals)\nprint('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score_char, acc_char))","bb083b85":"char_test_preds_df=pd.DataFrame()\nchar_test_preds_df['Prediction']=char_test_preds\nchar_test_preds_df['Label']=test_char_df['Label']\nchar_missclassified=char_test_preds_df[char_test_preds_df.Prediction!=char_test_preds_df.Label]\nchar_missclassified_index=char_missclassified.index.to_list()\nprint('{} images (out of {}) were missclassified!'.format(len(char_missclassified_index),test_char_df.shape[0]))","11b890a8":"conv1.trainable = False\nconv2.trainable = False\noutput1.trainable = False\n\nfashion_model_2=Model(inputs,output2_f2)\nplot_model(fashion_model_2,show_shapes=True,show_layer_names=True,expand_nested=True)\nfashion_model_2.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\nfashion_history_2 = fashion_model_2.fit(X_fashion,to_categorical(y_fashion),nb_epoch = 25,validation_split = 0.2,batch_size = 32,verbose = 1)","92f3eaee":"learning_curve(fashion_history_2,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])","64ddd40d":"Preprocessing character image data.","fb45e734":"We shoul try to visualize some of these digits. Let us make a function for that purpose, since we may want to reuse it.","14034b6d":"There are 6000 of each class in the training set and 1000 of each in the test set.\n\n!! Carefull !! The picture above is not a stacked countplot!","9dc2b4d4":"Now we define and fit the specific part of the model.","493f6526":"Here, we define the building of the MLP model using a function. This helps in testing different architectures in a a faster and more efficient way. It will also allow for the specification of dropout layers and control the number of classes in an easy way, by specifying function arguments.","0873164a":"### Import and analyse standard MNIST data\n\nLet us start by importing the standard MNIST dataset, comprised of 60 000 train samples, with balanced data, for digits from 1 to 10.","58ad35b4":"The model layers are now created. Let us define the model for traning on the digits dataset.","da84a53d":"The scaling was done correctly, as the numbers didn't get distorted.\nWe now use the scaler which was fitted to the training set to scale the test set as well.","1f1b1240":"We can now see the representation of some samples from the numbers_df dataframe.","c344f09b":"Checking the learning curve.","ce265a44":"### MLP with Dropout","503075c4":"How about the model performance on the test set?","cbf1f44a":"Let us modify slightly the model specific to each one of the latter datasets.","7fe98cf1":"As expected, 60 000 samples, with 784 (28x28) features. The first column in the dataset is the label for the training.","69657f3a":"** <font size=\"5\">Data Processing<\/font>**","e92b8d5c":"### Letters MNIST data","6b63de0b":"### Fashion MNIST Data","1bfab2ef":"## Improving Letter and Fashion classification","84b0bee9":"## Convolutional Neural Network (CNN)","04fa9b9f":"Here, both sets have the same labesl, so there is no need to complete the one-hot encoded vectors for any of them.","352e2567":"## Model definition","ee4c680a":"# Adaptability","153c5fd6":"Having visualised the dataset, and checked some digits, we should start processing our data. The first step is to normalize the values of the geyscale. As can be seen in the scale of the previous Figure, values are ranging from 0 (for black) to 255 (for white). We'll normalize them, so they are kept in the interval $\\left[0,1\\right]$.\n\nFor this instance in particular, we will use a MinMaxScaler fit on the train set and then fit it to the testing set as well. This could be done instead dividing by 255, but this procedure is more general. Further down in the notebook, we will just divide by 255.","55623ec9":"In the following model, we introduce some Dropout layers, to see if we can minimize the overfitting effect we have in the standard MLP model above.","cf7eb87f":"Now we try to use the same convolution layers as trained for the digit dataset to try and make predictions about the Fashion MNIST data.","c52ddfb4":"#### Preprocessing","852ce7f4":"Now it seems that both curves are converging, unlike what happened without Dropout layers. Furthermore, this comes at virtually no cost for the accuracy of the model and a slight decrease in loss. \n\nLet's see how is the performace on the test set with this model.","1552656f":"Now that the model is fitted to the training data, we'll test it in the test data.","45b6b6ea":"Then we compile and fit the model with the training data.","53e0cfae":"Let us see the learning curve for this model. \n(As this will be used often, we'll define a function for it.)","247bff2d":"As seen in the validation scores, we get a similar result, but now with a far more consistent learning through the several epochs of training, with the validation accuracy and loss converging to that of the training ones.","f1a53a82":"Let's start by importing the alphabet dataset.","8c14be71":"How many of each digit class is there in the training ans test sets?","0223cdb5":"Having done this scaling, we are ready to use this to predict the handwriten digits!","14585a24":"Let's check performance and missed samples.","6852297d":"We have some improvements! The CNN is able to classify correctly more samples than the simple MLP. Let's check which were missed by the CNN.","dfccbe00":"We have some wrongly classified samples. Let's see some of them in order to try and understand why they occurred.","7349bbbb":"First, just like in the MLP, we create the model. Again, let's make it a function.","32c4106e":"# The Goal\n\nThe goal of this notebook is to see how MLPs compare to CNNs in terms of digit recognition. \n\nAdditionally we will see the  generalization capabilities of CNNs. For this last step, we will train the CNN in data for digits and see how they generalize after taking the same networks to be tested on the alphabet characters and clothing items from the \"Fashion MNIST\" dataset.","710a80a3":"### Training on MNIST Data","120b60c6":"By analysing the learning curves of the letters and fashion models, one is able to notice the existence of overfitting. Additionally, it is possible to see that there is some room for improvement, even in the training set for these models. In the next section this issue will be addressed by incresing the complexity of the Dense layers of the models.","2c1b0253":"By looking at some sample images, it is possible to notice that the images in which the CNN fails to classify the digit correctly are much more prone to be wrongly classified by humans too: the prediction and the label don't mach but many times the drawn digit resembles the prediction in some way. This does not happen so much with the MLP.","da77e681":"### Letter Recognition","0a748aee":"Let's check the performance on the test set.","65a70d70":"### Fashion Recognition","44564e88":"### CNN with Dropout","e7e74cbd":"Now, we prepare the data for training.","745203a4":"Before proceeding, one should check for the possibility of overfitting.","2b4b2990":"Let us try to visualize the numbers with the rescaled features.","0eeb35e4":"Since the set of labels for the training set and the test set is not the same (there are some labels missing from the test set), we will have to increase the length of the one-hot encoded vectors for the test set before applying our model to the them. This will be done at some later point in the notebook.","f94aac31":"We now use a model which has the same structure in the dense part as the former MLP, but as a first step it encodes the features found by using convolutions.","025ccdfc":"Let's visualize some of these images.","0e9846c6":"Additionally, we can notice some overfitting by looking at the learning curve of both models. This can be countered with the addition of Dropout layers. These were purposefully not added so one can get a sense of their effect.\n\nLet us now add these Dropout layers. ","c70de3fa":"As we can se, some of the missclassified digits are not difficult to recognize by human standards. Let's see if a CNN can achieve better results through it's pattern recognition.","5cd8702a":"How many digits were missclassified with the CNN?","9d7d1221":"We should look at the learning curves now.","520160b2":"We will now check how adaptable CNN are. For that, we will train these networks on the digits dataset and then take the learned weights and build a model for classifying alphabet characters with them.\nFor this effect, using the Sequential() model from Keras is not the ideal. We will therefore proceed to use Functional API model.","8e471a1e":"Now we define the model. It will be a four-layer model. The first hidden layer has 256 nodes, with 'ReLu' as activation function, the second has 128 nodes, also with 'ReLu' and the third has 32 nodes. The output layer has 10 nodes and it uses softmax as activation, since it allows for a probabilistic interpretation of the outputs.","97b5b648":"Let's do the model for classifying the characters, without training the convolutional part of the network.","d7394b88":"As in the case of the MLP, the introduction of Dropout layers was able to solve the overfitting problem, as can be seen in the learning curves for the above model. In doing this, it improved the performance of our model on the test set as well, since it is now able to generalize consistently to unseen data.","eebe1d13":"How many misslabeled samples do we have?","28ed6dd7":"## Multi-Layer Perceptron (MLP)"}}