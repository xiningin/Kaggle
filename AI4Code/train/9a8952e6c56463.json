{"cell_type":{"0e5f3c27":"code","6b6d3800":"code","8001ae94":"code","9bc26b7a":"code","a7399130":"code","9ab38374":"code","d10448df":"code","7d5c9de6":"code","977f9b6f":"code","21cb40b2":"code","35c56024":"code","e7b6fde5":"code","48b9386c":"code","1d5cd271":"code","bd68abc3":"code","27f3182f":"code","26802c29":"code","48e39703":"code","01000d69":"code","4737705f":"code","2fd2e80a":"code","0d1dcfd3":"code","66ca17c8":"code","5ff549a3":"code","71786be7":"code","6bd1b8a7":"code","b388880c":"code","7c391cb3":"code","fdba88f0":"code","3ca9adef":"code","6d5b65f3":"code","476afdb5":"code","7313d3e0":"code","ea40d0de":"code","f423495f":"code","1db8e70a":"code","c198e88f":"code","9806ac40":"code","609a5932":"code","25fd1dc4":"code","b63ea0b8":"code","a7ca5321":"code","40069a02":"code","0130f6fa":"code","566cd096":"code","16306cc2":"markdown","68b9f965":"markdown","346ddcfa":"markdown","c53f78cb":"markdown","50a29eec":"markdown","00e31282":"markdown","b425b975":"markdown","be092b61":"markdown","29ac29b6":"markdown","4e50c70a":"markdown","eae4cc3a":"markdown","a7144863":"markdown","77707380":"markdown","213078ae":"markdown","c2cf5010":"markdown","1eeabc71":"markdown","2564705e":"markdown"},"source":{"0e5f3c27":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6b6d3800":"## LOADING LIBRARIES \nimport os\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input","8001ae94":"#### Loading the data\ndata = pd.read_csv(\"..\/input\/BackOrders.csv\",header=0)\ndata.head()","9bc26b7a":"# Understand the Data  See the No. row and columns\ndata.shape","a7399130":"# Display the columns\ndata.columns","9ab38374":"# Display the index\ndata.index","d10448df":"#Shows a quick statistic summary of your data using describe\ndata.describe(include='all')","7d5c9de6":"# Display data type of each variable\ndata.dtypes","977f9b6f":"for col in ['sku', 'potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']:\n    data[col] = data[col].astype('category')","21cb40b2":"# Cheeking the Variable int \ndata.dtypes","35c56024":"# Delete sku attribute by cheeking the count of it\nnp.size(np.unique(data.sku, return_counts=True)[0])\n","e7b6fde5":"data.drop('sku', axis=1, inplace=True)","48b9386c":"data.isnull().sum()","1d5cd271":"# Observing the number of records before and after missing value records removal\nprint(data.shape)","bd68abc3":"# Since the number of missing values is about 5%. For initial analysis we ignore all these records\ndata = data.dropna(axis=0)\n","27f3182f":"data.isnull().sum()\n","26802c29":"categorical_Attributes = data.select_dtypes(include=['category']).columns","48e39703":"\ndata = pd.get_dummies(columns=categorical_Attributes, data=data, prefix=categorical_Attributes, prefix_sep=\"_\",drop_first=True)","01000d69":"pd.value_counts(data['went_on_backorder_Yes'].values)","4737705f":"#Performing train test split on the data\nX, y = data.loc[:,data.columns!='went_on_backorder_Yes'].values, data.loc[:,'went_on_backorder_Yes'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)","2fd2e80a":"#To get the distribution in the target in train and test\nprint(pd.value_counts(y_train))\nprint(pd.value_counts(y_test))","0d1dcfd3":"perceptron_model = Sequential()\nperceptron_model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid', kernel_initializer='normal'))","66ca17c8":"perceptron_model.compile(loss='binary_crossentropy', optimizer='adam')","5ff549a3":"perceptron_model.fit(X_train, y_train, epochs=100)","71786be7":"test_pred=perceptron_model.predict_classes(X_test)\ntrain_pred=perceptron_model.predict_classes(X_train)","6bd1b8a7":"confusion_matrix_test = confusion_matrix(y_test, test_pred)\nconfusion_matrix_train = confusion_matrix(y_train, train_pred)\n\nprint(confusion_matrix_train)\nprint(confusion_matrix_test)","b388880c":"TNR_Train = confusion_matrix_train[0,0]\/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\nTPR_Train = confusion_matrix_train[1,1]\/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\nprint(\"Train TNR: \",TNR_Train)\nprint(\"Train TPR: \",TPR_Train)\n","7c391cb3":"TNR_Test = confusion_matrix_test[0,0]\/(confusion_matrix_test[0,0] +confusion_matrix_test[0,1])\nTPR_Test = confusion_matrix_test[1,1]\/(confusion_matrix_test[1,0] +confusion_matrix_test[1,1])\n\nprint(\"Test TNR: \",TNR_Test)\nprint(\"Test TPR: \",TPR_Test)","fdba88f0":"# The size of encoded and actual representations\nencoding_dim = 16 \nactual_dim = X_train.shape[1]","3ca9adef":"# Input placeholder\ninput_attrs = Input(shape=(actual_dim,))\n\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu')(input_attrs)\n\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(actual_dim, activation='sigmoid')(encoded)","6d5b65f3":"# this model maps an input to its reconstruction\nautoencoder = Model(input_attrs, decoded)","476afdb5":"print(autoencoder.summary())","7313d3e0":"autoencoder.compile(optimizer='adam', loss='binary_crossentropy')","ea40d0de":"autoencoder.fit(X_train, X_train, epochs=100)","f423495f":"## Create a separate encoder model\n# this model maps an input to its encoded representation\nencoder = Model(input_attrs, encoded)","1db8e70a":"print(encoder.summary())","c198e88f":"X_train_nonLinear_features = encoder.predict(X_train)\nX_test_nonLinear_features = encoder.predict(X_test)","9806ac40":"X_train = np.concatenate((X_train, X_train_nonLinear_features), axis=1)\nX_test = np.concatenate((X_test, X_test_nonLinear_features), axis=1)","609a5932":"perceptron_model = Sequential()\n\nperceptron_model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))","25fd1dc4":"perceptron_model.compile(loss='binary_crossentropy', optimizer='adam')","b63ea0b8":"perceptron_model.fit(X_train, y_train, epochs=10)","a7ca5321":"test_pred=perceptron_model.predict_classes(X_test)\ntrain_pred=perceptron_model.predict_classes(X_train)","40069a02":"confusion_matrix_test = confusion_matrix(y_test, test_pred)\nconfusion_matrix_train = confusion_matrix(y_train, train_pred)\n\nprint(confusion_matrix_train)\nprint(confusion_matrix_test)","0130f6fa":"Accuracy_Train=(confusion_matrix_train[0,0]+confusion_matrix_train[1,1])\/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\nTNR_Train= confusion_matrix_train[0,0]\/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\nTPR_Train= confusion_matrix_train[1,1]\/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n\nprint(\"Train TNR: \",TNR_Train)\nprint(\"Train TPR: \",TPR_Train)\nprint(\"Train Accuracy: \",Accuracy_Train)","566cd096":"Accuracy_Test=(confusion_matrix_test[0,0]+confusion_matrix_test[1,1])\/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\nTNR_Test= confusion_matrix_test[0,0]\/(confusion_matrix_test[0,0] +confusion_matrix_test[0,1])\nTPR_Test= confusion_matrix_test[1,1]\/(confusion_matrix_test[1,0] +confusion_matrix_test[1,1])\n\nprint(\"Test TNR: \",TNR_Test)\nprint(\"Test TPR: \",TPR_Test)\nprint(\"Test Accuracy: \",Accuracy_Test)","16306cc2":"**## evaluation metrics and evaluating model performance**","68b9f965":"> **Observations\n* sku is Categorical but is interpreted as int64 \n* Like As potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, and went_on_backorder are also Categorical but  it is interpreted as object**\n\n* Convert all the attributes to appropriate type\n###  Data type conversion\n\n**Using astype('category') to convert \n* potential_issue,\n* deck_risk,\n* oe_constraint, \n* ppap_risk,\n* stop_auto_buy,\n* rev_stop,\n* and went_on_backorder\n* [](http:\/\/)attributes to categorical attributes.**\n","346ddcfa":"*Converting Categorical to Numeric\nFor some of the models all the independent attribute should be of type numeric and Linear Regression model is one among them. But this data set has some categorial attributes.\n\n'pandas.get_dummies' To convert convert categorical variable into dummy\/indicator variables*\n\n* Creating dummy variables.\n\nIf we have k levels in a category, then we create k-1 dummy variables as the last one would be redundant. So we use the parameter drop_first in pd.get_dummies function that drops the first level in each of the category \n","c53f78cb":"**Split the data in to train and test\nsklearn.model_selection.train_test_split\n\nSplit arrays or matrices into random train and test subsets**","50a29eec":"#### Perceptron Model Building with both actual and non-linear features","00e31282":"> #### Calculate Accuracy, True Positive Rate and True Negative Rates","b425b975":"## Pill To Remaind\nWhen Ever you are working on the retail domain data set,you are suppose to keep orCreate engineer the Variables you just have to think mainly in 3 aspects\n\n* Recently\n* Frequency\n* Monitoring\nWhen I say Recently How many transaction Happende in a one week or month duration of time\nIf I Say Frequency mean How Many Item They Purchased Time Also into the compenent Monitoring Also How much valu or product that revenue generates\n\nSo,this is called RFM Analysis Can Helpful in Building A better Model IN A Deep LEarn Prospective The Hirearchy of the features","be092b61":"**Deriving now  new non-linear features using autoencoder\n**","29ac29b6":"Confusion Matrix","4e50c70a":"> Missing Data\nMissing value analysis and dropping the records with missing values","eae4cc3a":"** Predictions **","a7144863":"**Calculate Accuracy, True Positive Rate and True Negative Rates**","77707380":"**Combining new non-linear features to X_train and X_test respectively**","213078ae":"** derive new non-linear features**","c2cf5010":"**Target attribute distribution**","1eeabc71":"**Carrying forward the journey of exploring data sets on Kaggle to continue my learning, I came across another challenge that can be categorized as anomaly detection. This dataset is related to parts from a warehouse. It is an imbalanced class \u2014 the dataset consists of a few thousand SKUs and in a small percentage of these SKUs there were stockouts or backorders.**\n\n###  Problem Statement \nIs to identify products at risk of backorder before the event occurs so the business has time to react. \n\n![](https:\/\/miro.medium.com\/max\/800\/1*xhf_63WiTSNirnaBPzAtmA.jpeg)\n\n### Data\nData file contains the historical data for the 8 weeks prior to the week we are trying to predict. The data was taken as weekly snapshots at the start of each week. Columns are defined as follows:\n\n* sku - Random ID for the product   # Shopping Keeping Unit \n\n* national_inv - Current inventory level for the part\n\n* lead_time - Transit time for product (if available)\n\n* in_transit_qty - Amount of product in transit from source\n\n* forecast_3_month - Forecast sales for the next 3 months\n\n* forecast_6_month - Forecast sales for the next 6 months\n\n* forecast_9_month - Forecast sales for the next 9 months\n\n* sales_1_month - Sales quantity for the prior 1 month time period\n\n* sales_3_month - Sales quantity for the prior 3 month time period\n\n* sales_6_month - Sales quantity for the prior 6 month time period\n\n* sales_9_month - Sales quantity for the prior 9 month time period\n\n* min_bank - Minimum recommend amount to stock\n\n* potential_issue - Source issue for part identified\n\n* pieces_past_due - Parts overdue from source\n\n* perf_6_month_avg - Source performance for prior 6 month period\n\n* perf_12_month_avg - Source performance for prior 12 month period\n\n* local_bo_qty - Amount of stock orders overdue\n\n* deck_risk - Part risk flag\n\n* oe_constraint - Part risk flag\n\n* ppap_risk - Part risk flag\n\n* stop_auto_buy - Part risk flag\n\n* rev_stop - Part risk flag\n\n* went_on_backorder - Product actually went on backorder. (TARGET VARIABLE)\n\n\n\n**Identify Right Error Metrics**\n\n* Based on the businees have to identify right error metrics.","2564705e":"### Perceptron Model Building"}}