{"cell_type":{"5bf13cba":"code","06bee0c9":"code","f59260e5":"code","b3b31c0b":"code","eca1ba67":"code","a31d99cd":"code","d341fa0b":"code","5881a254":"code","862f6add":"code","b594e77a":"markdown","6c9705ca":"markdown","e9470bf6":"markdown","658c991c":"markdown","97132bcb":"markdown","b0166d30":"markdown","ac5b9cc3":"markdown","c52345e8":"markdown"},"source":{"5bf13cba":"#Imports\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Activation\nfrom tensorflow.python.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.python.keras.optimizers import RMSprop, Adam\nfrom tensorflow.python.keras.activations import relu\nfrom tensorflow.python.keras.utils import plot_model\nfrom tensorflow.python.keras.losses import categorical_crossentropy","06bee0c9":"#Read Data\ndigit_data = pd.read_csv('..\/input\/train.csv')\ndigit_data.head(5)","f59260e5":"img_rows, img_cols = 28,28\nnum_classes = 10\n\n#Preparing the training data\ndef data_prep_train(raw,val_frac):\n    \"\"\"\n    Prepares the training data for our model\n    inputs:\n        raw: raw dataset\n        val_frac: integer between 0 and 1. Fraction of the data to be used for validation.\n    \n    Outputs:\n        X_train: training examples\n        X_val: validation examples\n        y_train: training lables\n        y_val: validation lables\n    \"\"\"\n    num_images = int(raw.shape[0])    \n    \n    y_full = keras.utils.to_categorical(raw.label, num_classes)\n    \n    X_as_array = raw.values[:,1:]\n    X_shaped_array = X_as_array.reshape(num_images, img_rows, img_cols, 1)\n    X_full = X_shaped_array \/ 255\n    \n    X_train, X_val, y_train, y_val = train_test_split(X_full, y_full, test_size=val_frac)\n    return X_train, X_val, y_train, y_val\n\n#Preparing the test data\n    \"\"\"\n    Prepares the test data for our model\n    inputs:\n        raw: raw dataset\n    \n    Outputs:\n        X: test examples\n    \"\"\"\ndef data_prep_predict(raw):\n    num_images = int(raw.shape[0])\n    X_as_array = raw.values\n    X_shaped_array = X_as_array.reshape(num_images, img_rows, img_cols, 1)\n    X = X_shaped_array \/ 255\n    return X","b3b31c0b":"def build_model(layer_sizes=[32, 32, 64, 64, 256], kernel_sizes=[5,5,3,3], activation = 'relu'):\n    \"\"\"\n    building a CNN with 4+2 layers.\n    inputs:\n        layer_sizes: list of length 5 containing the number of hidden units in each layer\n        kernel_sizes: list of length 4 containing the size of the kernels in the Conv Layers\n        activation: The activation function, string or function.\n        \n    output:\n        model: The finished model\n    \"\"\"\n    model = Sequential()\n    \n    model.add(Conv2D(layer_sizes[0], kernel_size=kernel_sizes[0], padding = 'same', input_shape=(img_rows, img_cols, 1)))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Conv2D(layer_sizes[1], kernel_size=kernel_sizes[1], padding = 'same'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2)))\n    model.add(Dropout(rate=0.25))\n\n    model.add(Conv2D(layer_sizes[2], kernel_size=kernel_sizes[2], padding = 'same'))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Conv2D(layer_sizes[3], kernel_size=kernel_sizes[3], padding = 'same'))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(MaxPool2D(pool_size=(2,2)))\n    model.add(Dropout(rate=0.25))\n    \n    model.add(Flatten())\n    model.add(Dense(layer_sizes[4]))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Dropout(rate=0.5))\n\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    return model\n\nmy_model = build_model()\nplot_model(my_model, to_file='my_model.png', show_shapes=True, show_layer_names=True)\nImage('my_model.png')","eca1ba67":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False,)  # randomly flip images","a31d99cd":"def train_model(model, optimizer='adam', batch_size=64, epochs=1, verbose=1, callbacks=[]):\n    \"\"\"\n    Trains the model.\n    Outputs:\n        history: dictionary containing information about the training process like training and validation accuracy\n    \"\"\"\n    model.compile(loss=categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n    \n    history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n                            epochs=epochs,\n                            verbose=verbose,\n                            validation_data=(X_val,y_val),\n                            callbacks=callbacks)\n    return history","d341fa0b":"# leaky_relu = lambda x: relu(x, alpha=0.1)\n# X_train, X_val, y_train, y_val = data_prep_train(digit_data,0.2)\n\n# learning_rates = [0,0015,0.003,0.006]\n\n# histories = []\n# for lr in learning_rates:\n#     optimizer = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n#     lr_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.000001)\n#     my_model = build_model(activation=leaky_relu)\n#     histories.append(train_model(my_model, optimizer=optimizer, epochs=35, batch_size = 64, verbose=2, callbacks=[lr_reduction]))\n\n# colors = ['red', 'blue', 'green', 'purple', 'grey', 'yellow']\n\n# plt.figure(figsize=(20,9))\n# for i, lr in enumerate(learning_rates):\n#     plt.plot(range(25,36), histories[i].history['val_acc'][24:], color=colors[i],label='learning rate: '+str(lr))\n# legend = plt.legend(loc='best', shadow=True)\n# plt.show()\n","5881a254":"X_train, X_val, y_train, y_val = data_prep_train(digit_data,0.1)\n\nleaky_relu = lambda x: relu(x, alpha=0.1)\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nlr_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\nmy_model = build_model(activation=leaky_relu)\nhistory = train_model(my_model, optimizer=optimizer, epochs=40, batch_size = 128, verbose=1, callbacks=[lr_reduction])\n\nplt.figure(figsize=(20,9))\nplt.plot(range(20,41),history.history['val_acc'][19:], color='red', label='validation accuracy')\nplt.plot(range(20,41),history.history['acc'][19:], color='blue', label='accuracy')\nlegend = plt.legend(loc='best', shadow=True)\nplt.show()","862f6add":"subm_examples = pd.read_csv('..\/input\/test.csv')\nX_subm = data_prep_predict(subm_examples)\ny_subm = my_model.predict(X_subm)\nn_rows = y_subm.shape[0]\ny_subm = [np.argmax(y_subm[row,:]) for row in range(n_rows)]\noutput = pd.DataFrame({'ImageId': range(1,n_rows+1), 'Label': y_subm})\noutput.to_csv('submission.csv', index=False)","b594e77a":"The following function defines the training process.\n\nBy default, we are using the Adam optimizer which is a combination of gradient descent with momentum and RMSProp. See [this video](https:\/\/www.youtube.com\/watch?v=JXQT_vxqwIs) from a coursera course for a detailed explanation. I tried some other optimizers but achieved the best results with Adam.\n\n","6c9705ca":"The key problem of this task is that there is not enough data. We therefore generate new data from our existing data by randomly rotate, zoom and shift the images.\nThis is explained in all of the ressources I mentioned above. In particular, I use the same settings as in [this kernel](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6).","e9470bf6":"Let's now load the data and have a look.","658c991c":"Let us train our final model. I am using as much data as possible here, the only reason for the validation set is the learning rate reduction. Therefore, the validation accuracy is very unreliable here and I wouldn't pay too much attention to it.","97132bcb":"Now, we want to tune the hyperparamters. This process takes quite long as we really need to train for 30-40 epochs to see which model will do best in the end. It is not necessarily the model which converges fastest to a decent accuracy which will give the best results.\n\nIn order to have reasonable results, I suggest using at least 20% of the data for validation. This is only for the tuning, later on we will train on almost all of the data to get the most out of it.\n\nIt is important to reduce the learning rate in the process in order to keep improving the model. Otherwise, the model tends to jump too far once we are getting close to the minimum. Adam has a parameter 'decay' which does this automatically. I had better results though decreasing the learning_rate whenever the validation accuracy does not improve anymore. This is done with the Keras function ReduceLROnPlateau. I use similar settings as in [this kernel](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6).\n\nI have commented out this section in order to save time.","b0166d30":"Resources:\n\n* The [Kaggle Course on Deep Learning](https:\/\/www.kaggle.com\/learn\/deep-learning)\n* The Coursera Deep Learning specialisation (you can access the course material for free)\n* This [Kaggle Kernel](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6). My solution is based on this kernel with a few extras to reduce the error rate even further.","ac5b9cc3":"The next function preps the data for our model and splits the data into a training set and a validation set.\n\nEach row in our data_set starts with the label followed by 784 integers between 0 and 255 representing the pixel values on a grayscale. In order for CNNs to work appropriately we need to convert each picture to a 28x28 array. We will also scale the pixel values to be between 0 and 1.\n","c52345e8":"Next, we are going to build our model. The basic structure is Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> Dense -> Dense. For a picture, see below.\n\nThe convolutional layers are the basic building block of this model. The Kaggle Course explains nvery nicely how they work. I use the architecture from [this kernel](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6).\n\nThe Batch normalization layer scales the input to a certain mean and variance (which are parameters of the model). This seems to give the model a little edge over many other models out there. There is some discussion if this should be done before or after the activation, but I did not notice significant differences.\n\nThe goal of the pooling layer is to reduce the complexity of the model whithout loosing too much information. This is explained pretty well in the Coursera Courses.\n\nThe standard relu activation works very well. I saw small improvements by using a leaky relu (this function is like relu but has a small slope below 0 to speed up convergence).\n\nThe Dropout layer randomly ignores some units with a given probability. This helps the model to avoid overfitting by forcing it not to rely on single units too much.\n\nFinally, we will use a dense layer to make predictions."}}