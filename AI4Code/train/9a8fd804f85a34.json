{"cell_type":{"211d60af":"code","1009e3f4":"code","977692cc":"code","515416ca":"code","d1f42ccf":"code","2522df09":"code","efd0a6ac":"code","9d49a2c5":"code","9d406e18":"code","f4f405a6":"code","f01893d9":"code","542b9e0a":"code","099e1b22":"code","bdb4b569":"code","9c4802a9":"code","54b2312a":"code","1df608b9":"code","0a6cb6d7":"code","50d02687":"code","82a35031":"code","9050ab0e":"code","1109b04d":"code","de13beee":"code","556bc1d0":"markdown","ef355adf":"markdown","dc83cd69":"markdown","23f5c7ba":"markdown","045962bc":"markdown","01b9d827":"markdown","e3981ac1":"markdown","82bf54ab":"markdown","decb5ff2":"markdown","299455ab":"markdown","a8bb1c28":"markdown"},"source":{"211d60af":"!pip install -qq git+https:\/\/www.github.com\/keras-team\/keras-contrib.git","1009e3f4":"from IPython.display import clear_output\nimport scipy\nfrom keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nimport datetime\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom skimage.io import imread\nimport os\nfrom IPython.display import Image\nfrom keras.utils.vis_utils import model_to_dot\nfrom tqdm import tqdm_notebook\nimport h5py\nclear_output()","977692cc":"with h5py.File('..\/input\/packaging-images-and-features\/out_cartoons.h5', 'r') as h:\n    cartoon_dict = {k: h[k][()] for k in h.keys()}\n    cartoon_dict['image'] = cartoon_dict['image'][:, 16:-16, 16:-16]","515416ca":"base_path = Path('..') \/ 'input' \/ 'celeba-dataset'\nall_images_df = pd.DataFrame({'path': list(base_path.glob('*\/*\/*.jpg'))})\nall_images_df['image_path'] = all_images_df['path'].map(str)\nprint(all_images_df.shape[0])\nall_images_df.sample(3)","d1f42ccf":"fig, (a_axs, b_axs) = plt.subplots(2, 4, figsize=(20, 5))\nfor a_ax, b_ax, (i, c_row) in  zip(a_axs, b_axs, all_images_df.sample(20).iterrows()):\n    a_img = imread(c_row['path'])\n    a_ax.imshow(a_img)\n    b_ax.imshow(cartoon_dict['image'][i % cartoon_dict['image'].shape[0]])","2522df09":"IMAGE_SIZE = 96, 96\nIMAGE_A_CHANNELS = 3 # Celebrity\nIMAGE_B_CHANNELS = 3 # Cartoon","efd0a6ac":"from keras import backend as K\nx0 = 1-np.abs(np.linspace(-1, 1, IMAGE_SIZE[0]).reshape((1, -1)))\nradial_map = np.matmul(x0.T, x0)\nplt.imshow(radial_map)\ndef radial_mse(x, y, dim=IMAGE_SIZE[0]):\n    x0 = 1-np.abs(np.linspace(-1, 1, dim).reshape((1, -1)))\n    radial_map = np.tile(np.expand_dims(np.matmul(x0.T, x0),0), (64,1,1))\n    return radial_map*K.mean(K.square(x-y), axis=[ 3])","9d49a2c5":"from keras import layers\nclass CycleGAN():\n    def __init__(self,\n                 img_rows,\n                 img_cols,\n                 channels_A,\n                 channels_B,\n                 parallel_channels=False,\n                 ):\n        \"\"\"\n        args:\n          parallel_channels: process each input and output channel on its own\n        \"\"\"\n        # Input shape\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.channels_A = channels_A\n        self.channels_B = channels_B\n        self.parallel_channels = parallel_channels\n        self.img_shape_A = (self.img_rows, self.img_cols, self.channels_A)\n        self.img_shape_B = (self.img_rows, self.img_cols, self.channels_B)\n        # Calculate output shape of D (PatchGAN)\n        patch_r = int(self.img_rows \/ 2**4)\n        patch_c = int(self.img_cols \/ 2**4)\n        self.disc_patch = (patch_r, patch_c, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = 32\n        self.df = 64\n\n        # Loss weights\n        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n\n        optimizer = Adam(0.0002, 0.5)\n\n        # Build and compile the discriminators\n        self.d_A = self.build_discriminator(self.img_shape_A, suffix='A')\n        self.d_B = self.build_discriminator(self.img_shape_B, suffix='B')\n        self.d_A.compile(loss='mse',\n                         optimizer=optimizer,\n                         metrics=['accuracy'])\n        self.d_B.compile(loss='mse',\n                         optimizer=optimizer,\n                         metrics=['accuracy'])\n\n        # -------------------------\n        # Construct Computational\n        #   Graph of Generators\n        # -------------------------\n\n        # Build the generators\n        self.g_AB = self.build_generator(\n            self.img_shape_A, self.img_shape_B, suffix='AB')\n        self.g_BA = self.build_generator(\n            self.img_shape_B, self.img_shape_A, suffix='BA')\n\n        # Input images from both domains\n        img_A = Input(shape=self.img_shape_A, name='ImageA')\n        img_B = Input(shape=self.img_shape_B, name='ImageB')\n\n        # Translate images to the other domain\n        fake_B = self.g_AB(img_A)\n        fake_A = self.g_BA(img_B)\n        # Translate images back to original domain\n        reconstr_A = self.g_BA(fake_B)\n        reconstr_B = self.g_AB(fake_A)\n        # Identity mapping of images\n        img_A_id = self.g_BA(img_B)\n        img_B_id = self.g_AB(img_A)\n\n        # For the combined model we will only train the generators\n        self.d_A.trainable = False\n        self.d_B.trainable = False\n\n        # Discriminators determines validity of translated images\n        valid_A = self.d_A(fake_A)\n        valid_B = self.d_B(fake_B)\n\n        # Combined model trains generators to fool discriminators\n        self.combined = Model(inputs=[img_A, img_B],\n                              outputs=[valid_A, valid_B,\n                                       reconstr_A, reconstr_B,\n                                       img_A_id, img_B_id])\n        self.combined.compile(loss=['mse', 'mse',\n                                    radial_mse, radial_mse,\n                                    radial_mse, radial_mse],\n                              loss_weights=[1, 1,\n                                            self.lambda_cycle, self.lambda_cycle,\n                                            self.lambda_id, self.lambda_id],\n                              optimizer=optimizer)\n\n    def build_generator(self, in_img_shape, out_img_shape, suffix=''):\n        \"\"\"U-Net Generator\"\"\"\n        in_chan = in_img_shape[-1]\n        out_chan = out_img_shape[-1]\n        gf_down = self.gf \/\/ in_chan if self.parallel_channels else self.gf\n        gf_up = self.gf \/\/ in_chan if self.parallel_channels else self.gf\n\n        def conv2d(layer_input, filters, f_size=4):\n            \"\"\"Layers used during downsampling\"\"\"\n            d = Conv2D(filters, kernel_size=f_size,\n                       strides=2, padding='same')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            d = InstanceNormalization()(d)\n            return d\n\n        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n            \"\"\"Layers used during upsampling\"\"\"\n            u = UpSampling2D(size=2)(layer_input)\n            u = Conv2D(filters, kernel_size=f_size, strides=1,\n                       padding='same', activation='relu')(u)\n            if dropout_rate:\n                u = Dropout(dropout_rate)(u)\n            u = InstanceNormalization()(u)\n            u = Concatenate()([u, skip_input])\n            return u\n\n        # Image input\n        d0 = Input(shape=in_img_shape)\n        # Downsampling\n\n        def _ds_pipe(in_d0):\n            d1 = conv2d(in_d0, gf_down)\n            d2 = conv2d(d1, gf_down*2)\n            d3 = conv2d(d2, gf_down*4)\n            d4 = conv2d(d3, gf_down*8)\n            return d1, d2, d3, d4\n\n        if (in_chan > 1) and self.parallel_channels:\n            chan_list = []\n            for i in range(in_chan):\n                c_d0 = layers.Lambda(\n                    lambda x: x[:, :, :, i:(i+1)],\n                    name='InSel{}_{}'.format(suffix, i))(d0)\n                chan_list.append(_ds_pipe(c_d0))\n            d1, d2, d3, d4 = [\n                layers.concatenate(list(c_outs))\n                for c_outs in zip(*chan_list)]\n        else:\n            d1, d2, d3, d4 = _ds_pipe(d0)\n\n        # Upsampling\n        def _us_pipe(chan_count, d1, d2, d3, d4):\n            u1 = deconv2d(d4, d3, gf_up*4)\n            u2 = deconv2d(u1, d2, gf_up*2)\n            u3 = deconv2d(u2, d1, gf_up)\n\n            u4 = UpSampling2D(size=2)(u3)\n            output_img = Conv2D(chan_count, kernel_size=4, strides=1,\n                            padding='same', activation='sigmoid')(u4)\n            return output_img\n        \n        \n        if (out_chan > 1) and self.parallel_channels:\n            chan_list = []\n            for i in range(out_chan):\n                chan_list.append(_us_pipe(1, d1, d2, d3, d4))\n            output_img = layers.concatenate(chan_list)\n        else:\n            output_img = _us_pipe(out_chan, d1, d2, d3, d4)\n        \n\n        return Model(d0, output_img, name='Gen{}_{}_{}_{}-{}'.format(suffix, *in_img_shape, out_img_shape[-1]))\n\n    def build_discriminator(self, img_shape, suffix=''):\n        in_chan = img_shape[-1]\n        df = self.df \/\/ in_chan if self.parallel_channels else self.df\n\n        def d_layer(layer_input, filters, f_size=4, normalization=True):\n            \"\"\"Discriminator layer\"\"\"\n            d = Conv2D(filters, kernel_size=f_size,\n                       strides=2, padding='same')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if normalization:\n                d = InstanceNormalization()(d)\n            return d\n\n        img = Input(shape=img_shape)\n\n        def _disc_block(in_img):\n            d1 = d_layer(in_img, df, normalization=False)\n            d2 = d_layer(d1, df*2)\n            d3 = d_layer(d2, df*4)\n            d4 = d_layer(d3, df*8)\n            return d4\n\n        if (in_chan > 1) and self.parallel_channels:\n            chan_list = []\n            for i in range(in_chan):\n                c_img = layers.Lambda(\n                    lambda x: x[:, :, :, i:(i+1)],\n                    name='DiscSelect{}_{}'.format(suffix, i))(img)\n                chan_list.append(_disc_block(c_img))\n            d4 = layers.concatenate(chan_list)\n        else:\n            d4 = _disc_block(img)\n\n        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n\n        return Model(img, validity, name='Disc{}_{}_{}_{}'.format(suffix, *img_shape))","9d406e18":"cg = CycleGAN(IMAGE_SIZE[0], IMAGE_SIZE[1], IMAGE_A_CHANNELS, IMAGE_B_CHANNELS)","f4f405a6":"Image(model_to_dot(cg.combined, show_shapes=True).create_png())","f01893d9":"Image(model_to_dot(cg.g_AB, show_shapes=True).create_png())","542b9e0a":"Image(model_to_dot(cg.d_A, show_shapes=True).create_png())","099e1b22":"from keras.preprocessing.image import ImageDataGenerator\ndg_args = dict(featurewise_center = False, \n                  rotation_range = 5, \n                  width_shift_range = 0.05, \n                  height_shift_range = 0.05, \n                  shear_range = 0.01,\n                  zoom_range = [0.95, 1.05],  \n                  brightness_range = [0.9, 1.1],\n                  horizontal_flip = True, \n                  vertical_flip = False,\n                  fill_mode = 'reflect',\n                   data_format = 'channels_last',\n              preprocessing_function=lambda x: x\/127.0-1)\n\ncore_idg = ImageDataGenerator(**dg_args)","bdb4b569":"from sklearn.model_selection import train_test_split\n\nclass loader_class():\n    def __init__(self, a_df, b_array, goal_size):\n        self.goal_size = goal_size\n        self.a_df = a_df\n        self.b_array = b_array\n        self._a_gen = None\n        self._b_gen = None\n        self.n_batches = 0\n\n    def _make_gen(self, batch_size):\n        a_gen = core_idg.flow_from_dataframe(self.a_df,\n                                               x_col='image_path',\n                                               y_col='image_path',\n                                               class_mode=None,\n                                               target_size=self.goal_size,\n                                               color_mode='rgb',\n                                               batch_size=batch_size)\n        def b_gen_func():\n            while True:\n                c_idx = np.random.permutation(np.arange(self.b_array.shape[0]))\n                for i in range(0, self.b_array.shape[0]\/\/batch_size):\n                    yield self.b_array[c_idx[i*batch_size:(i+1)*batch_size]]\/127.0-1\n        \n        return a_gen, b_gen_func()\n\n    def load_batch(self, batch_size):\n        a_gen, b_gen = self._make_gen(batch_size)\n        for a_x, b_x in zip(a_gen, b_gen):\n            if (a_x.shape[0]==batch_size) and (b_x.shape[0]==batch_size):\n                yield a_x, b_x\n            else:\n                yield None, None\n\n    def load_data(self, domain=\"A\", batch_size=1, is_testing=False):\n        a_gen, b_gen = self._make_gen(batch_size)\n        if domain == \"A\":\n            return next(a_gen)\n        elif domain == \"B\":\n            return next(b_gen)\n        else:\n            raise ValueError(\"Unknown domain\")\n\n\nloader_obj = loader_class(\n    a_df=all_images_df.sample(10000), \n    b_array=cartoon_dict['image'], \n    goal_size=IMAGE_SIZE\n)\nloader_obj.load_data(domain=\"A\", batch_size=1, is_testing=True).shape","9c4802a9":"# sanity check on the tool\nfor _, (a, b) in zip(range(2), loader_obj.load_batch(8)):\n    print(a.shape, b.shape)","54b2312a":"def sample_images(cyc_gan, data_loader, epoch, batch_i):\n    plt.close('all')\n    r, c = 2, 3\n\n    imgs_A = data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n    imgs_B = data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n\n    # Translate images to the other domain\n    fake_B = cyc_gan.g_AB.predict(imgs_A)\n    fake_A = cyc_gan.g_BA.predict(imgs_B)\n    # Translate back to original domain\n    reconstr_A = cyc_gan.g_BA.predict(fake_B)\n    reconstr_B = cyc_gan.g_AB.predict(fake_A)\n\n    gen_imgs = [imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B]\n\n    titles = ['Original', 'Translated', 'Reconstructed']\n    fig, axs = plt.subplots(r, c, figsize=(10, 5))\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            c_img = np.clip(0.5 * gen_imgs[cnt][0]+0.5, 0, 1)\n            axs[i,j].imshow(c_img.squeeze(), cmap='gray', vmin=0, vmax=1)\n            axs[i, j].set_title('{} {}'.format(titles[j], 'A' if i==0 else 'B'))\n            axs[i,j].axis('off')\n            cnt += 1\n    fig.savefig(\"{:03d}_{:03d}.png\".format(epoch, batch_i))\nsample_images(cg, loader_obj, 0, 0)","1df608b9":"BATCH_SIZE = 64\nEPOCHS = 30","0a6cb6d7":"start_time = datetime.datetime.now()\n\n# Adversarial loss ground truths\nvalid = np.ones((BATCH_SIZE,) + cg.disc_patch)\nfake = np.zeros((BATCH_SIZE,) + cg.disc_patch)\n\nfor epoch in tqdm_notebook(range(EPOCHS), desc='Epochs'):\n    for batch_i, (imgs_A, imgs_B) in tqdm_notebook(enumerate(loader_obj.load_batch(BATCH_SIZE)), desc='Batch'):\n        #  Train Discriminators\n        if imgs_A is None:\n            break\n        # Translate images to opposite domain\n        fake_B = cg.g_AB.predict(imgs_A)\n        fake_A = cg.g_BA.predict(imgs_B)\n\n        # Train the discriminators (original images = real \/ translated = Fake)\n        dA_loss_real = cg.d_A.train_on_batch(imgs_A, valid)\n        dA_loss_fake = cg.d_A.train_on_batch(fake_A, fake)\n        dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n\n        dB_loss_real = cg.d_B.train_on_batch(imgs_B, valid)\n        dB_loss_fake = cg.d_B.train_on_batch(fake_B, fake)\n        dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n\n        # Total disciminator loss\n        d_loss = 0.5 * np.add(dA_loss, dB_loss)\n        \n        #  Train Generators\n        # Train the generators\n        g_loss = cg.combined.train_on_batch([imgs_A, imgs_B],\n                                                [valid, valid,\n                                                imgs_A, imgs_B,\n                                                imgs_A, imgs_B])\n\n        elapsed_time = datetime.datetime.now() - start_time\n    \n    # Plot the progress at each epoch\n    print (\"[Epoch %d\/%d] [Batch %d\/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n                                                            % ( epoch, EPOCHS,\n                                                                batch_i, loader_obj.n_batches,\n                                                                d_loss[0], 100*d_loss[1],\n                                                                g_loss[0],\n                                                                np.mean(g_loss[1:3]),\n                                                                np.mean(g_loss[3:5]),\n                                                                np.mean(g_loss[5:6]),\n                                                                elapsed_time))\n    \n    clear_output()\n    sample_images(cg, loader_obj, epoch, 0)\n","50d02687":"sample_images(cg, loader_obj, EPOCHS, 1)","82a35031":"sample_images(cg, loader_obj, EPOCHS, 2)","9050ab0e":"sample_images(cg, loader_obj, EPOCHS, 3)","1109b04d":"sample_images(cg, loader_obj, EPOCHS, 4)","de13beee":"sample_images(cg, loader_obj, EPOCHS, 5)","556bc1d0":"# Model Parameters","ef355adf":"## Setup and Data","dc83cd69":"# Build Models","23f5c7ba":"# Train Model","045962bc":"## Preview Model Output","01b9d827":"# Setup CycleGAN Code\nThe code below has been lightly adapted from the code at https:\/\/github.com\/eriklindernoren\/Keras-GAN\/tree\/master\/cyclegan by @eriklindernoren","e3981ac1":"# Data Loaders\nHere we setup the data-loaders","82bf54ab":"## Generators\nWe just show one and the other can be inferred","decb5ff2":"### Focus on the center\nWe make a special loss function that focuses on the center and allows more flexibility at the edges","299455ab":"# Goal\nHere we try an experiment to see if we can take unpaired celebrity and cartoon images and create models that go from celebrity to cartoon and from cartoon to celebrity using the CycleGAN approach. The basic idea is we have generators for both of the mappings with a U-Net style architecture (which forces them to learn something from the original pixels). We then have discriminators which determine if the images we have created are real or fake. Finally we have the cycle-consistent loss of using both the forward and backward back-to-back which should give us the original images. \n\nIf this works well it could be applied to lots of issues where paired training data are not available. It should also produce more 'realistic' cartoons since the discriminator is actively trying to determine if the image output is discernible real data.","a8bb1c28":"## Discriminator"}}