{"cell_type":{"ca2e6c02":"code","16c960bd":"code","7b65dc58":"code","dc936303":"code","2308df62":"code","b89179f9":"code","70bb5f46":"code","00b50989":"code","c43ce2c6":"code","12b13e1a":"code","516d6efe":"code","6d125b05":"code","43eeb5b8":"code","7f8f4784":"code","42346265":"code","a01bf373":"code","e5ef3212":"code","a206fb27":"code","0d06ebc7":"code","7d9c7c9e":"code","beefac33":"code","261f15b5":"code","60b5c8e0":"code","6d6e9755":"code","41117fd4":"code","45612fe8":"code","83e797d1":"code","2f636d64":"code","fe541166":"code","c6852e6e":"code","7e2823fe":"code","6b61098b":"code","f846bc10":"code","453343c2":"code","a223e7de":"code","ea11a5cf":"code","442b0212":"code","6ce1ddaa":"code","cf996f51":"code","8e9462a2":"code","ed2028a5":"code","5958d2dd":"markdown","f3b934c2":"markdown","c1bc0936":"markdown","587f41e9":"markdown","cbaa04b4":"markdown","ad7ef3b2":"markdown","adc35790":"markdown"},"source":{"ca2e6c02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16c960bd":"!pip install swifter","7b65dc58":"import pandas as pd \nimport cv2\nimport numpy as np\nimport os\nfrom glob import glob\nimport math\nimport matplotlib.pyplot as plt\n\n#import swifter\nimport re\nimport html\nimport string\nimport unicodedata\nfrom nltk.tokenize import word_tokenize","dc936303":"df  =pd.read_csv(\"\/kaggle\/input\/chest-xrays-indiana-university\/indiana_reports.csv\")\ndf.head()","2308df62":"df['findings'].iloc[0:10].tolist()","b89179f9":"df.shape","70bb5f46":"df['impression'].unique().shape","00b50989":"df['MeSH'].unique().tolist()[:20]","c43ce2c6":"img = cv2.imread('\/kaggle\/input\/chest-xrays-indiana-university\/images\/images_normalized\/1_IM-0001-3001.dcm.png')","12b13e1a":"plt.imshow(img)\nplt.show()","516d6efe":"df2 = pd.read_csv(\"\/kaggle\/input\/chest-xrays-indiana-university\/indiana_projections.csv\")\ndf2.head()","6d125b05":"df2.projection.unique()","43eeb5b8":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow","7f8f4784":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br \/>', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    #words = text2words(text)\n    #stop_words = stopwords.words('english')\n    #words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    #words = lemmatize_words(words)\n    #words = lemmatize_verbs(words)\n\n    return text\n  \ndef normalize_corpus(corpus):\n    return [normalize_text(t) for t in corpus]\n  ","42346265":"df['report'] = df[df.columns[1:]].apply(\n    lambda x: ','.join(x.astype(str)),\n    axis=1\n)\ndf['report'].head()","a01bf373":"df['report'] = df['report'].apply(normalize_text)","e5ef3212":"df['report'] = '<start> '+df['report']+' <end>'","a206fb27":"num_words = []\nfor row in df['report'].tolist():\n    num_words.append(len(word_tokenize(row)))","0d06ebc7":"num_words= np.array(num_words)\nprint(\"min length             : \", num_words.min())\nprint(\"max length             : \", num_words.max())\nprint(\"50th percentile length : \", np.percentile(num_words,50))\nprint(\"75th percentile length : \", np.percentile(num_words,75))\nprint(\"90th percentile length : \", np.percentile(num_words,90))\nprint(\"95th percentile length : \", np.percentile(num_words,95))\nprint(\"98th percentile length : \", np.percentile(num_words,98))\nprint(\"98th percentile length : \", np.percentile(num_words,99))\n","7d9c7c9e":"vocab_size = 10000\nmax_len = 260\n\ntok = Tokenizer(num_words=vocab_size,  oov_token='UNK', )\ntok.fit_on_texts(df['report'].tolist())","beefac33":"df2 = df2[df2['projection']=='Frontal']","261f15b5":"df  =pd.merge(df,df2,  on=['uid'])","60b5c8e0":"df.head()","6d6e9755":"\nclass det_gen(tensorflow.keras.utils.Sequence):\n    'Generates data from a Dataframe'\n    def __init__(self,df, tok, max_len,images_path, dim=(256,256), batch_size=8):\n        self.df=df\n        self.dim = dim\n        self.images_path = images_path\n        self.tok= tok\n        self.max_len = max_len\n        self.batch_size = batch_size\n        self.nb_iteration = math.ceil((self.df.shape[0])\/self.batch_size)\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return self.nb_iteration\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.df=self.df.sample(frac=1)\n    \n    def load_img(self, img_path):\n        \n        img = cv2.imread(img_path)\n        img =cv2.resize(img,(self.dim))\n        \n        \n        return img\n        \n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        \n        indicies = list(range(index*self.batch_size, min((index*self.batch_size)+self.batch_size ,(self.df.shape[0]))))\n        \n        images = []\n        for img_path in self.df['filename'].iloc[indicies].tolist():\n            img = self.load_img(os.path.join(self.images_path,img_path))\n            images.append(img)\n            \n            \n        \n        \n        x_batch = self.df['report'].iloc[indicies].tolist()\n        \n        x_batch_input = [sample[:-len(\" <end>\")] for sample in x_batch]\n        \n        x_batch_gt = [sample[len(\" <start>\"): ] for sample in x_batch]\n        \n        \n        x_batch_input = np.array(pad_sequences( self.tok.texts_to_sequences (x_batch_input),\n                          maxlen=self.max_len-1 ,\n                          padding='post',\n                          truncating='post'))\n        \n        x_batch_gt = np.array(pad_sequences( self.tok.texts_to_sequences (x_batch_gt),\n                          maxlen=self.max_len-1 ,\n                          padding='post',\n                          truncating='post'))\n        \n        \n        \n        \n        \n        \n        return [np.array(images), np.array(x_batch_input)] , np.array(x_batch_gt)   ","41117fd4":"validation_split= 0.2\ndf = df.sample(frac=1)\ndf_train = df.iloc[:-int(df.shape[0]*validation_split)]\ndf_val   = df.iloc[-int(df.shape[0]*validation_split):]","45612fe8":"images_path = \"\/kaggle\/input\/chest-xrays-indiana-university\/images\/images_normalized\/\"\ntrain_dataloader =  det_gen(df_train, tok, max_len,images_path)\nval_dataloader =  det_gen(df_val, tok, max_len,images_path)","83e797d1":"[X_img,X_report] ,Y = next(enumerate(train_dataloader))[1]","2f636d64":"plt.imshow(X_img[0])\nplt.show()","fe541166":"print(X_report[0])\nprint(\"====================\")\nprint(Y[0])","c6852e6e":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3","7e2823fe":"## Input layers\nimg_input = layers.Input(shape= (256,256,3)) \nreport_input= layers.Input(shape= (max_len-1,))\n\n## Encoder ######################\n\nDensenet_model = tf.keras.applications.DenseNet121(\n            include_top=False,\n            weights=None,\n            input_shape=(256,256,3),\n        )\nnumber_of_encoder_layers=  len(Densenet_model.layers)\n\nencoder_output = Densenet_model(img_input)\nencoder_output = layers.Flatten()(encoder_output)\nencoder_output = layers.Dropout(0.2)(encoder_output)\nencoder_output = layers.Dense(512,activation='relu')(encoder_output)\n\n##decoder ########################\n\n#layers\ngru_layer =  layers.GRU(512, return_sequences=True)\ndense_layer= layers.Dense(vocab_size,activation='softmax')\nembedding_layer = layers.Embedding(vocab_size, 300, mask_zero=True)\ndropout = layers.Dropout(0.2)\n\n# decoder model\nembedding_output = embedding_layer(report_input)\ngru_output = gru_layer(embedding_output, initial_state=encoder_output )\ngru_output = dropout(gru_output)\noutput  = dense_layer(gru_output)\nmodel = Model([img_input,report_input ],output)\n\n##Inference models ################\n\n#encoder_inference model\nencoder_model = Model(img_input,encoder_output)\n\n#decoder_inference model\nprev_hidden_state= layers.Input(shape= (512))\nreport_input2 = layers.Input(shape= (1,))\nembedding_output2= embedding_layer(report_input2)\ngru_output2 = gru_layer(embedding_output2, initial_state=prev_hidden_state )\ngru_output2 = dropout(gru_output2)\noutput2 = dense_layer(gru_output2)\ndecoder_model = Model([report_input2,prev_hidden_state],[output2,gru_output2])","6b61098b":"model.summary()","f846bc10":"plot_model(model)","453343c2":"epochs =5\nlr=1e-3\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr))","a223e7de":"hist = model.fit_generator( train_dataloader,\n                    validation_data = val_dataloader,\n                    epochs = epochs\n                )","ea11a5cf":"import numpy as np\nfrom nltk.translate.bleu_score import corpus_bleu","442b0212":"def tokens_to_text(tokens,tok,end_token='end'):\n    sentence=\"\"\n    for token in tokens:\n        if token ==0:\n            break\n        \n        word = tok.index_word[token]\n        \n        if word==end_token:\n            break\n            \n        sentence+= word+\" \"\n        \n    sentence = sentence.strip()\n    \n    return sentence\n\n\ndef greedy_inference(input_img, tok,encoder_model, decoder_model,max_len,start_token=\"start\",end_token='end',decoder_type=\"GRU\"):\n    if decoder_type=='LSTM':\n        a0,c0  =encoder_model(np.expand_dims(input_img,axis=0))\n    elif decoder_type=='GRU': \n        hidden_layer  =encoder_model(np.expand_dims(input_img,axis=0))\n        \n    word = tok.word_index[start_token]\n    \n    words = []\n    \n    for index in range(max_len):\n        if decoder_type=='LSTM':\n            word_probs , a0,c0 = decoder_model.predict([[np.array([word]),a0,c0]])\n        elif decoder_type=='GRU': \n            word_probs , hidden_layer = decoder_model.predict([[np.array([word]),hidden_layer]])\n            hidden_layer=hidden_layer[0]\n        \n        word = np.argmax(word_probs)\n        \n        try:\n            if tok.index_word[word]==end_token:\n                break\n        except:\n            pass\n        \n        words.append(word)\n        \n    words = tokens_to_text(words,tok)\n    return words","6ce1ddaa":"def get_predictions_from_data_loader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=\"start\"\n                                     ,end_token='end', inference_type='greedy',decoder_type='GRU'):\n    \n    data_loader_iterator = data_loader.__iter__()\n    \n    pred_sentences = []\n    Gt_sentences = []\n    for index, (X,Y) in enumerate(data_loader_iterator):\n        for img,_,sample_y in zip(X[0],X[1],Y):\n            \n            if inference_type=='greedy':\n                pred_sentence = greedy_inference(img, tok,encoder_model, decoder_model,max_len,\n                                                 start_token=\"start\",end_token='end',decoder_type='GRU')\n            \n            GT_sentence   = tokens_to_text(sample_y,tok)\n            \n            pred_sentences.append(pred_sentence)\n            Gt_sentences.append(GT_sentence)\n        \n        if index == data_loader.nb_iteration -1:\n            break\n        print(\"Done with batch number: {} \", index)\n        \n    return Gt_sentences, pred_sentences","cf996f51":"def calculate_bleu_evaluation(GT_sentences, predicted_sentences):\n    BLEU_1 = corpus_bleu(GT_sentences, predicted_sentences, weights=(1.0, 0, 0, 0))\n    BLEU_2 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.5, 0.5, 0, 0))\n    BLEU_3 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.3, 0.3, 0.3, 0))\n    BLEU_4 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.25, 0.25, 0.25, 0.25))\n    \n    return BLEU_1,BLEU_2,BLEU_3,BLEU_4","8e9462a2":"def evaluate_from_dataloader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=\"start\",end_token='end', inference_type='greedy',decoder_type=\"GRU\"):\n    Gt_sentences, pred_sentences = get_predictions_from_data_loader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=start_token,end_token=end_token, inference_type=inference_type,decoder_type=decoder_type)\n    BLEU_1,BLEU_2,BLEU_3,BLEU_4 = calculate_bleu_evaluation(Gt_sentences, pred_sentences)\n    \n    return BLEU_1,BLEU_2,BLEU_3,BLEU_4","ed2028a5":"BLEU_1,BLEU_2,BLEU_3,BLEU_4 =  evaluate_from_dataloader(val_dataloader,tok,encoder_model, decoder_model,max_len)\nprint(BLEU_1)\nprint('-------')\nprint(BLEU_2)\nprint('-------')\nprint(BLEU_3)\nprint('-------')\nprint(BLEU_4)","5958d2dd":"**Text cleaner**","f3b934c2":"# Download necessary packages","c1bc0936":"# Data loader","587f41e9":"# Merge Images Path","cbaa04b4":"# Build Model","ad7ef3b2":"# Build vocab","adc35790":"# Explore dataset"}}