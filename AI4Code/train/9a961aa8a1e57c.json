{"cell_type":{"e59eedd2":"code","2bb50b90":"code","d64cbe65":"code","23c817b7":"code","ecf73c93":"code","3d8fc572":"code","ddac7b1e":"code","a33ed37e":"code","64bab9c0":"code","e25a995c":"code","0da69961":"code","640fedf3":"code","0b388227":"code","19529386":"code","8bcf0338":"code","c82ccf15":"code","e31c8c7b":"code","1bec31d2":"code","59362185":"code","b16dd664":"code","9e60997b":"code","e4591f67":"code","5f61dbb7":"code","68f7a727":"code","cfd478ec":"code","d6be8b57":"code","010a37c8":"code","09ca54d2":"code","40385103":"code","f4bd0bb1":"code","f04ed024":"code","820a16fd":"code","bdc7a843":"code","c2d7a646":"code","d9daacdb":"code","03a4dc5c":"code","236be97d":"code","1495f394":"code","232e9dfc":"code","4f3ed960":"code","45e1a68e":"code","2271c51c":"code","cf7ba462":"code","572d38df":"code","0795c976":"code","eb863b58":"code","5917d67f":"code","baed563b":"code","dead8f6e":"code","16dcbcd1":"code","fe83d8a1":"code","b10f7874":"code","7f62b588":"code","2ffacd4f":"code","87e8c3c7":"code","7eb913bd":"code","718425a1":"code","e1fca998":"code","55314643":"code","47a3c240":"code","84676fb3":"code","45e043e1":"code","0f1978bc":"code","dc4416ab":"code","86b5bdfc":"code","271b2b56":"code","6b0878a1":"code","2b1ba3d6":"code","eff48d9c":"code","8acb6262":"code","8c4da939":"code","75269dcc":"code","06c029ca":"code","35dc9d59":"code","0febffe5":"code","97af005a":"code","72025ded":"code","e07909a7":"code","95017a3b":"code","32ffcfed":"code","3dae20aa":"code","682823d0":"code","cf6fc433":"code","fb8bb4f9":"code","6f575102":"code","2f83a848":"code","b4a3d802":"code","441ef891":"code","ce7d9246":"code","fd19a897":"code","9a651a71":"code","90b8aab0":"code","e211ad6a":"code","3268aea8":"code","839626f3":"code","b9c0dbba":"code","9893c377":"code","62f75654":"code","db9c0a56":"code","bac495ed":"code","7b0067f6":"code","4e35618f":"code","57232fda":"code","3131ecaf":"code","329b3dcc":"code","2a5db8ed":"code","87166eee":"code","4af3aba0":"code","5e25938a":"code","13bbdd8a":"code","109f25e0":"code","fbd15299":"code","eaeb7663":"code","1271ee1e":"code","d4c8768c":"code","043b1ad3":"code","aa444b57":"code","9e04b5be":"code","415c9c32":"code","213d756e":"code","3f20a81e":"code","a3c1e211":"code","abb5947d":"code","71cb7dc0":"code","48f29bb1":"code","a0e70d8a":"code","5608c828":"code","1f06254a":"code","782f04b4":"code","a50cc4e2":"code","e0aaf5c7":"code","30c32167":"code","1c0477a4":"code","218f8c67":"code","ab419eed":"code","d8e39cfe":"code","a2f073ca":"code","76518dd5":"code","8707f305":"code","d08213ec":"code","884226ad":"markdown","0c1f0e7d":"markdown","66503e42":"markdown","77f50dd3":"markdown","d7fee34f":"markdown","b70a88ed":"markdown","6528fbb7":"markdown","475caf76":"markdown","9a993ce8":"markdown","ff95f783":"markdown","ff36999e":"markdown","1441b535":"markdown","64203607":"markdown","0fb952fd":"markdown","f2a250a8":"markdown","3d58618a":"markdown","48365634":"markdown","9e2d8e74":"markdown","83ac0e6e":"markdown","5b76febc":"markdown","f7ef2f56":"markdown","186df149":"markdown","860fda6a":"markdown","2dd2e595":"markdown","b176b5c6":"markdown","0ce7285b":"markdown","cdac6b7f":"markdown","caf4c68e":"markdown","60d1299b":"markdown","33600587":"markdown","49dc1b7e":"markdown","4b1f1fbf":"markdown","bec07013":"markdown"},"source":{"e59eedd2":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_columns',100)\npd.set_option('display.max_rows', 500)","2bb50b90":"#Reading previous application file\nprev=pd.read_csv('..\/input\/home-credit-default-risk\/previous_application.csv')","d64cbe65":"#Reading application_data file \napp=pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')","23c817b7":"app.shape","ecf73c93":"app.head()","3d8fc572":"app.describe()","ddac7b1e":"#column-wise null count in the application data\n100*round(app.isnull().sum()\/len(app),4)","a33ed37e":"#retaining only those columns where the null percentage is less than 50\napp=app.loc[:,100*round(app.isnull().sum()\/len(app),4)<50]","64bab9c0":"#inspecting\napp.shape","e25a995c":"#checking null percentages again \n100*round(app.isnull().sum()\/len(app),4)","0da69961":"#Checking what these columns are, in which we can safely impute values\ncols_to_impute = list(app.loc[:,(100*round(app.isnull().sum()\/len(app),4) > 0) & (100*round(app.isnull().sum()\/len(app),4) <14)].columns)\ncols_to_impute","640fedf3":"#Checking what these columns look like, and what data they hold\nfor i in enumerate(cols_to_impute):\n    print(i[1],'\\n')\n    print((app[i[1]].describe()))\n    print('\\n')","0b388227":"#We create another list of columns with only the numerical variables that we wish to impute null values for\nnum_cols_to_impute=cols_to_impute.copy()\nnum_cols_to_impute.remove('NAME_TYPE_SUITE')","19529386":"plt.figure(figsize=(15,15))\n\nfor i in enumerate(num_cols_to_impute):\n    plt.subplot(3,4,i[0]+1)\n    sns.boxplot(y=i[1],data=app)\n\nplt.show()","8bcf0338":"#Let's also visualize the categorical variable and see what the spread is like\nsns.countplot(y='NAME_TYPE_SUITE',data=app)\nplt.show()","c82ccf15":"#Let's check if any numerical columns have negative values which don't make sense (for example, negative age)\napp.min()","e31c8c7b":"#Inspecting the data types\napp.dtypes","1bec31d2":"#Fixing data types of columns which appear to be incorrectly formatted\napp['DAYS_REGISTRATION'] = app['DAYS_REGISTRATION'].astype('int64')\napp.CNT_FAM_MEMBERS = pd.to_numeric(app.CNT_FAM_MEMBERS, errors = 'coerce')\napp['CNT_FAM_MEMBERS'] = app['CNT_FAM_MEMBERS'].fillna(0).astype('int64')\n\n","59362185":"app.DAYS_BIRTH.describe()","b16dd664":"#Let's see what this looks like with absolute values\napp.DAYS_BIRTH.apply(lambda x: abs(x)).describe()","9e60997b":"# Converting 'DAYS_BIRTH' to 'AGE'.\n\napp['AGE']=pd.to_timedelta(abs(app['DAYS_BIRTH']), unit = 'days')\napp['AGE']= round((app['AGE']\/np.timedelta64(1,'Y')))","e4591f67":"app['AGE'] = app['AGE'].astype('int64')","5f61dbb7":"#Inspecting again to ensure the calcualtion was correct\napp.AGE.describe()","68f7a727":"#We can drop the original days column for age\napp.drop('DAYS_BIRTH',axis=1,inplace=True)","cfd478ec":"#Now, we'll change the rest of the days columns to positive values.\n\ndays_columns = ['DAYS_ID_PUBLISH','DAYS_LAST_PHONE_CHANGE','DAYS_REGISTRATION']\nfor i in days_columns:\n    app[i]=app[i].apply(lambda x:abs(x))\n   ","d6be8b57":"app.AMT_ANNUITY.isna().sum()","010a37c8":"#Changing AMT_ANNUITY and AMT_CREDIT units to thousand \n#there are some NaNs in AMT_ANNUITY. We will be dropping them\napp.dropna(subset=['AMT_ANNUITY'],inplace=True)\napp.AMT_ANNUITY = app.AMT_ANNUITY.apply(lambda x:round(x\/1000))\napp.AMT_CREDIT = app.AMT_CREDIT.apply(lambda x: round(x\/1000))","09ca54d2":"#Changing the units of client's income to thousands\n\napp['AMT_INCOME_TOTAL_original']=app['AMT_INCOME_TOTAL'] #backing up the column if needed in future\napp['AMT_INCOME_TOTAL']=app['AMT_INCOME_TOTAL'].apply(lambda x:round(x\/1000))","40385103":"app['AMT_INCOME_TOTAL'].describe()","f4bd0bb1":"#The box plot can be used to identify the outliers easily in the income column.\nsns.boxplot(app.AMT_INCOME_TOTAL)\nplt.show()","f04ed024":"#The income seems to spread towards the lower end, indicating a lot of loan seekers belong to the lower income bracket (Mainly middle class folks)\nsns.distplot(app['AMT_INCOME_TOTAL'], hist=False)\nplt.show()","820a16fd":"#This is the outlier that skews our data to the higher end\napp['AMT_INCOME_TOTAL'].max()   ","bdc7a843":"#We can bin the values in the income column to deal with the outliers\n# bins = pd.IntervalIndex.from_tuples([(0, 50), (50, 120), (120, 250),(250,500),(500,5000),(5000,120000)])\napp['INCOME_CATEGORY']=pd.cut(app['AMT_INCOME_TOTAL'], bins=[0,50,120,250,500,5000,120000], labels = ['Lower','LowerMiddle','UpperMiddle', 'Upper','Rich','UberRich'])","c2d7a646":"sns.countplot(y=app.INCOME_CATEGORY)","d9daacdb":"app.INCOME_CATEGORY.value_counts()","03a4dc5c":"# Binning the age column\napp['AGE_CAT']=pd.cut(app['AGE'], bins=[0,30,40,50,60,70], labels = ['0 - 30','30 - 40', '40 - 50','50 - 60','60 +'])","236be97d":"app.AGE_CAT.value_counts()","1495f394":"#The days employed column has some negative values. We need to fix this\n#Also, the days as such would be of little value to us during the analysis. Instead, we can convert this column to years\napp.DAYS_EMPLOYED.describe()","232e9dfc":"#Let's see the spread of data when viewed as years\nax=sns.boxplot(app['DAYS_EMPLOYED'].apply(lambda x: abs(x)).apply(lambda x:x\/\/365))\nax.set(xlabel='Years Employed')\nplt.show()","4f3ed960":"#Here, we can abserve that there is not one, but 55374 such rows which are causing outliers in the column. Since this is a lot of rows, we will not delete them. Instead, let's change these to NaN\napp.loc[app['DAYS_EMPLOYED'].abs()==365243,'DAYS_EMPLOYED']=np.NaN","45e1a68e":"#Binning the days employed column into YEARS_EMPLOYED\napp['YEARS_EMPLOYED']=pd.cut(app['DAYS_EMPLOYED'].apply(lambda x: abs(x)), bins=[0,365,5*365,10*365,25*365,45*365,1001*365], labels = ['Upto 1 Year','1 - 5 Years','5 - 10 Years', '10 - 25 Years','25 - 45 Years','45+ Years'])\napp['YEARS_EMPLOYED'].value_counts()","2271c51c":"#Let's visualize the results for a better view\nplt.figure(figsize=(10,5))\nax = sns.countplot(app['YEARS_EMPLOYED'])\nax.set(xlabel='Years Employed',ylabel='Number of Applicants')\nplt.show()","cf7ba462":"#Let's convert the AMT_GOODS_PRICE to thousands as well, for ease of analysis \napp['AMT_GOODS_PRICE']=app['AMT_GOODS_PRICE'].apply(lambda x:(x\/\/1000))","572d38df":"#Let's see what the spread of the goods value looks like\nsns.distplot(app.AMT_GOODS_PRICE.dropna(),hist=False)\nplt.show()","0795c976":"#We can spot some outliers in the goods price column\nplt.figure(figsize=(15,5))\nsns.boxplot(app['AMT_GOODS_PRICE'])\nplt.show()","eb863b58":"#Upon inspecting the spread of the data using qcut, we can see that high values may not be outliers after all, since there are quite a few values located in the top 10% range.\npd.qcut(app['AMT_GOODS_PRICE'],q=[0,0.2,0.4,0.6,0.8,0.9,1]).value_counts()","5917d67f":"app.AMT_GOODS_PRICE.describe()","baed563b":"#Let's see how the family member count column looks like. We'll be excluding the zero value here since that was used to fill NANs previously\nplt.figure(figsize=(15,5))\nsns.boxplot(app.loc[app['CNT_FAM_MEMBERS']>0].CNT_FAM_MEMBERS)\nplt.show()","dead8f6e":"#Let's now see how the credit amount is spread out\nplt.figure(figsize=(15,5))\nsns.boxplot(app.AMT_CREDIT.apply(lambda x: round(x\/1000)))\nplt.show()","16dcbcd1":"#Observing outliers in the annuity\nplt.figure(figsize=(15,5))\nsns.boxplot(app.AMT_ANNUITY)\nplt.show()","fe83d8a1":"app.info()","b10f7874":"#Checking the imbalance percentage of the dataset\napp['TARGET'].value_counts(normalize = True)*100","7f62b588":"#Separting the data into two data frames based on the target values 0 and 1\napp1 = app[app['TARGET']==1]\napp0 = app[app['TARGET']==0]","2ffacd4f":"#Looking for trends in the column NAME_INCOME_TYPE against the target variable. The spread seems to be even across both.\nplt.figure(figsize=(12,5))\nsns.countplot(y = 'NAME_INCOME_TYPE', data = app, hue='TARGET' )\nplt.show()","87e8c3c7":"# Analysis for continuous variables\ncont_var = ['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE']\n\nplt.figure(figsize=(10,10))\n\nfor i in enumerate(cont_var):\n    plt.subplot(2,2,i[0]+1)\n    sns.boxplot(y = i[1],x='TARGET',  data = app)\n\nplt.show()","7eb913bd":"plt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app.loc[app['TARGET'] == 0, 'AGE'], label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app.loc[app['TARGET'] == 1, 'AGE'], label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');\nplt.savefig('ages')","718425a1":"#We can see some outliers in the AMT_INCOME_TOTAL. Let's visualizing this using the binner variable we created.\n## The spread seems to quite similar here.\nplt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nsns.countplot('INCOME_CATEGORY', data=app0)\nplt.title('NOT DEFAULTED')\nplt.subplot(1,2,2)\nsns.countplot('INCOME_CATEGORY', data=app1)\nplt.title('DEFAULTED')\nplt.show()","e1fca998":"#Checking the spread of the total credit amount against the target variable \nplt.figure(figsize=(20,6))\nsns.distplot(app0.AMT_CREDIT,hist=False,color='green')\nsns.distplot(app1.AMT_CREDIT,hist=False,color='red')\nplt.title('Distribution of income of applicants, Defaulted in Red, Non-Defaulted in Green')\nplt.show()","55314643":"#Checking the income distrubution for target 0 vs 1; it turns out to be similar\nplt.figure(figsize=(10,10))\nplt.subplot(2,1,1)\nplt.title('Non Defaulted Applications')\nsns.countplot(y='INCOME_CATEGORY',data=app0)\nplt.subplot(2,1,2)\nplt.title('Defaulted Applications')\nsns.countplot(y='INCOME_CATEGORY',data=app1)","47a3c240":"app.info()","84676fb3":"# Analysis of categorical variables\ncat_list = ['NAME_CONTRACT_TYPE','CODE_GENDER','NAME_TYPE_SUITE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS',\n            'NAME_HOUSING_TYPE','WEEKDAY_APPR_PROCESS_START','OCCUPATION_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n            'ORGANIZATION_TYPE', 'AGE_CAT'\n           ]\n\nplt.figure(figsize=(20,100))\nfor i in enumerate(cat_list):\n    plt.subplot(13,2,2*(i[0]+1)-1)\n    plt.title('TARGET = 0')\n    plt.xticks(rotation = 90)\n    sns.countplot(x= i [1], data = app0.sort_values(by=i [1]))\n    plt.subplot(13,2,2*(i[0]+1))\n    plt.title('TARGET = 1')\n    plt.xticks(rotation = 90)\n    sns.countplot(x= i [1], data = app1.sort_values(by=i [1]))\n    #plt.save\nplt.show()","45e043e1":"app['CODE_GENDER'].value_counts()","0f1978bc":"app.loc[app['CODE_GENDER'] == 'XNA','CODE_GENDER'] = 'F'\napp['CODE_GENDER'].value_counts()","dc4416ab":"#Observing the documents submitted by those who did not default vs. those who defaulted\nflag_list = ['FLAG_DOCUMENT_2','FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5','FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7',\n             'FLAG_DOCUMENT_8','FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13',\n             'FLAG_DOCUMENT_14','FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17','FLAG_DOCUMENT_18', \n             'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']\n\napp0.loc[:, flag_list].sum(axis=0).plot.barh()\nplt.show()\napp1.loc[:, flag_list].sum(axis=0).plot.barh()\nplt.show()","86b5bdfc":"#The spread is almost similar, let's combine these flags into a single column\napp.loc[:,'TOTAL_DOCS']=app.loc[:, flag_list].sum(axis=1)\napp0.loc[:,'TOTAL_DOCS']=app0.loc[:, flag_list].sum(axis=1)\napp1.loc[:,'TOTAL_DOCS']=app1.loc[:, flag_list].sum(axis=1)","271b2b56":"# Now dropping the FLAG_DOCUMENT columns as we have already created a column for that.\napp.drop(flag_list, axis=1, inplace = True)\napp0.drop(flag_list, axis=1, inplace = True)\napp1.drop(flag_list, axis=1, inplace = True)","6b0878a1":"#There is some data from external sources present in the dataset. These seem to be ratings given by credit rating agencies.\n#Let's see how the data links to these ratings.\nplt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate([ 'EXT_SOURCE_2', 'EXT_SOURCE_3']): #EXT_SOURCE_1 was dropped since it had several null values\n    \n    # create a new subplot for each source\n    plt.subplot(2, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app.loc[app['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app.loc[app['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","2b1ba3d6":"plt.figure(figsize=(15,6))\nsns.kdeplot(app.loc[app['TARGET'] == 0, 'DAYS_EMPLOYED'].abs()\/365, label = 'target == 0')\nsns.kdeplot(app.loc[app['TARGET'] == 1, 'DAYS_EMPLOYED'].abs()\/365, label = 'target == 1')\nplt.title('Years of employment of applicant compared against whether the applicant defaulted or not')\nplt.show()","eff48d9c":"app.info()","8acb6262":"#Plotting education level and work experience \nplt.figure(figsize=(8,8))\nplt.subplot(211)\nsns.countplot(y='NAME_EDUCATION_TYPE',hue='TARGET',data=app)\nplt.ylabel('Education Level')\nplt.legend(title='Target',loc='lower right')\nplt.subplot(212)\nsns.countplot(y='YEARS_EMPLOYED',hue='TARGET',data=app)\nplt.ylabel('Years in Employment')\nplt.legend(title='Target',loc='lower right')\nplt.show()","8c4da939":"app.info()","75269dcc":"#Checking the income vs credit amount against the target variable \nplt.figure(figsize=(12,6))\nsns.scatterplot(x='AMT_INCOME_TOTAL_original', y='AMT_CREDIT', data=app.loc[app.AMT_INCOME_TOTAL < 2500], hue='TARGET',alpha=0.3)\nplt.xlabel('Total Income of the Applicant')\nplt.ylabel('Total Credit Amount')\nplt.title('Both DEFAULTED and NOT DEFAULTED')\nplt.show()","06c029ca":"#Checking the income vs credit amount against the target variable \nplt.figure(figsize=(12,6))\nsns.scatterplot(x='AMT_INCOME_TOTAL_original', y='AMT_CREDIT', data=app1.loc[app1.AMT_INCOME_TOTAL < 2500], alpha=0.3,color='orange')\nplt.xlabel('Total Income of the Applicant')\nplt.title('DEFAULTED')\nplt.ylabel('Total Credit Amount')\nplt.show()","35dc9d59":"#Let's also obserrve the same using the binned column we created\nsns.scatterplot(x='INCOME_CATEGORY', y='AMT_CREDIT', data=app, hue='TARGET',alpha=0.3)","0febffe5":"#Let's check the correlation matrix for the two cases, one where target variable is 1, and second where target variable is 0\ncorr0=app0[['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',\n       'AMT_GOODS_PRICE',\n       'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'CNT_FAM_MEMBERS',\n       'REGION_RATING_CLIENT',\n       'HOUR_APPR_PROCESS_START',\n       'LIVE_CITY_NOT_WORK_CITY', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'TOTALAREA_MODE',\n       'AMT_REQ_CREDIT_BUREAU_HOUR',\n       'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n       'AMT_REQ_CREDIT_BUREAU_YEAR', 'AGE',]].corr()\ncorr1=app1[['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',\n       'AMT_GOODS_PRICE',\n       'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'CNT_FAM_MEMBERS',\n       'REGION_RATING_CLIENT',\n       'HOUR_APPR_PROCESS_START',\n       'LIVE_CITY_NOT_WORK_CITY', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'TOTALAREA_MODE',\n       'AMT_REQ_CREDIT_BUREAU_HOUR',\n       'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n       'AMT_REQ_CREDIT_BUREAU_YEAR', 'AGE',]].corr()","97af005a":"corr0 = corr0.where(np.triu(np.ones(corr0.shape), k=1).astype(np.bool))\ncorrdf0 = corr0.unstack().reset_index()\ncorrdf0.head()","72025ded":"corrdf0.columns = ['VAR1', 'VAR2', 'Correlation']\ncorrdf0.dropna(subset = ['Correlation'], inplace = True)\ncorrdf0['Correlation'] = round(corrdf0['Correlation'], 2)\n# We will be using absolute value of the correlation coefficiaents since we are only interested in seeing the absolute value.\n# The dirction in which the entities are correlated is currently not of concern\ncorrdf0['Correlation'] = corrdf0['Correlation'].abs()","e07909a7":"corr1 = corr1.where(np.triu(np.ones(corr1.shape), k=1).astype(np.bool))\ncorrdf1 = corr1.unstack().reset_index()\ncorrdf1.head()","95017a3b":"corrdf1.columns = ['VAR1', 'VAR2', 'Correlation']\ncorrdf1.dropna(subset = ['Correlation'], inplace = True)\ncorrdf1['Correlation'] = round(corrdf1['Correlation'], 2)\n# We will be using absolute value of the correlation coefficiaents since we are only interested in seeing the absolute value.\n# The dirction in which the entities are correlated is currently not of concern\ncorrdf1['Correlation'] = corrdf1['Correlation'].abs()","32ffcfed":"#Inspecintg the TOP 10 correlated variables in the app1 dataframe, which indicates applicants who defaulted\ncorrdf1.sort_values(by = 'Correlation', ascending = False).head(10)","3dae20aa":"#Inspecintg the TOP 10 correlated variables in the app0 dataframe, which indicates applicants who did not default\ncorrdf0.sort_values(by = 'Correlation', ascending = False).head(10)","682823d0":"#Income vs Annuity\n#Here we observe that the income and annuity observe a low|low trend for the defaulters. \n#High income earners who get loans for higher annuity values end up defaulting much lesser than those..\n#...in the low income bracket, even when they opt for lower annuity\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nplt.title('DID NOT DEFAULT')\nsns.scatterplot(x='AMT_ANNUITY',y='AMT_INCOME_TOTAL',data=app0,alpha=0.1,color='green')\nplt.ylabel('Income in Thousands')\nplt.xlabel('Annuity in Thousands')\nplt.subplot(1,2,2)\nplt.title('DEFAULTED')\nsns.scatterplot(x='AMT_ANNUITY',y='AMT_INCOME_TOTAL',data=app1[app1.AMT_INCOME_TOTAL < 20000],alpha=0.1,color='red')\nplt.ylabel('Income in Thousands')\nplt.xlabel('Annuity in Thousands')\nplt.show()","cf6fc433":"#AMT_GOODS_PRICE vs AMT_ANNUITY\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nplt.title('DID NOT DEFAULT')\nsns.scatterplot(x='AMT_ANNUITY',y='AMT_GOODS_PRICE',data=app0)\nplt.subplot(1,2,2)\nplt.title('DEFAULTED')\nsns.scatterplot(x='AMT_ANNUITY',y='AMT_GOODS_PRICE',data=app1)\nplt.show()","fb8bb4f9":"#AMT_GOODS_RPICE vs AMT_TOTAL_INCOME\n#Here we observe that low income earning applicants who seek to buy high value goods are more likely to default on payments\nplt.figure(figsize=(11,10))\nplt.subplot(211)\nplt.title('DID NOT DEFAULT')\nsns.scatterplot(x='AMT_GOODS_PRICE',y='AMT_INCOME_TOTAL',data=app0[app0.AMT_INCOME_TOTAL < 10000],alpha=0.1, color='green')\nplt.xlabel('Goods Price in Thousands')\nplt.ylabel('Income in Thousands')\nplt.subplot(212)\nplt.title('DEFAULTED')\nsns.scatterplot(x='AMT_GOODS_PRICE',y='AMT_INCOME_TOTAL',data=app1[app1.AMT_INCOME_TOTAL < 10000],alpha=0.1, color='red')\nplt.xlabel('Goods Price in Thousands')\nplt.ylabel('Income in Thousands')\nplt.savefig('appllesss')\nplt.show()","6f575102":"#Let's check the pairplot for amount variables once, to spot any trends\namt = app[[ 'AMT_INCOME_TOTAL','AMT_CREDIT',\n                         'AMT_ANNUITY', 'AMT_GOODS_PRICE',\"TARGET\"]]\namt = amt[(amt[\"AMT_GOODS_PRICE\"].notnull()) & (amt[\"AMT_ANNUITY\"].notnull())]\nsns.pairplot(amt,hue=\"TARGET\",palette=[\"b\",\"r\"])\nplt.show()","2f83a848":"app.head()","b4a3d802":"prev.head()","441ef891":"prev.shape","ce7d9246":"prev.describe()","fd19a897":"#Inspecing null values\n100*round(prev.isnull().sum()\/len(prev),4)","9a651a71":"#Retaining only those columns with less than 50% null values\nprev=prev.loc[:,100*round(prev.isnull().sum()\/len(prev),4)<50]","90b8aab0":"100*round(prev.isnull().sum()\/len(prev),4)","e211ad6a":"#Dropping columns that we'll not be using \nprev.drop(['DAYS_FIRST_DRAWING','DAYS_FIRST_DUE','DAYS_LAST_DUE_1ST_VERSION','DAYS_LAST_DUE','DAYS_TERMINATION'],axis=1,inplace=True)","3268aea8":"prev.shape","839626f3":"#Some columns can be imputed\n#Checking what these columns are, in which we can safely impute values\ncols_to_impute = list(prev.loc[:,(100*round(prev.isnull().sum()\/len(app),4) > 0) & (100*round(prev.isnull().sum()\/len(prev),4) <25)].columns)\ncols_to_impute","b9c0dbba":"#Checking what these columns look like, and what data they hold\nfor i in enumerate(cols_to_impute):\n    print(i[1],'\\n')\n    print((prev[i[1]].describe()))\n    print('\\n')","9893c377":"#We create another list of columns with only the numerical variables that we wish to impute null values for\nnum_cols_to_impute=cols_to_impute.copy()\nnum_cols_to_impute.remove('PRODUCT_COMBINATION')","62f75654":"plt.figure(figsize=(15,8))\n\nfor i in enumerate(num_cols_to_impute):\n    plt.subplot(1,3,i[0]+1)\n    sns.boxplot(y=i[1],data=prev)\n\nplt.show()","db9c0a56":"#Let's also visualize the categorical variable and see what the spread is like\nsns.countplot(y='PRODUCT_COMBINATION',data=prev)\nplt.show()","bac495ed":"#Let's check if any numerical columns have negative values which don't make sense (for example, negative age)\nprev.min()","7b0067f6":"prev.info()","4e35618f":"#DAYS_DECISION is signifying number of days when was the decision about previous application made\n#this cannot be negative, let's change it to posiitve\nprev['DAYS_DECISION']=prev.DAYS_DECISION.apply(lambda x:abs(x))","57232fda":"#Let's see drill-down of each type of loan by the status\nax = pd.crosstab(prev[\"NAME_CONTRACT_TYPE\"],prev[\"NAME_CONTRACT_STATUS\"]).plot(kind=\"barh\",figsize=(10,7),stacked=True)\nplt.xticks(rotation =0)\nplt.ylabel(\"count\")\nplt.title(\"Count of application status by application type\")\nplt.show()","3131ecaf":"#Let's see what is the exact ype of cash loans that get cancelled\nplt.figure(figsize=(12,8))\nsns.countplot(y='NAME_CASH_LOAN_PURPOSE',data=prev[(prev['NAME_CONTRACT_STATUS']=='Canceled') & (prev['NAME_CONTRACT_TYPE']=='Cash loans') & (prev['NAME_CASH_LOAN_PURPOSE'] != 'XNA')])\nplt.xlabel('Number of Cancelled Loans')\nplt.ylabel('Purpose of Loan')\nplt.savefig('CancelledPurposeCashLoan')\nplt.title('Cancelled Cash Loans - A Breakdown by purpose')\nplt.show()","329b3dcc":"#Let's see the count of previous loans available for the current loans we have in the app dataframe\nx = prev.groupby(\"SK_ID_CURR\")[\"SK_ID_PREV\"].count().reset_index()\nplt.figure(figsize=(13,7))\nax = sns.distplot(x[\"SK_ID_PREV\"],color=\"red\")\nplt.title(\"Current loan ID having previous loan applications\")\nplt.show()","2a5db8ed":"#It's also observed that the loan application amount, and the actual amount credited was not the same.\n#We see in the plots that these two factors differed by both positive and negative values\n#This implies that while there were instances when the bank granted a loan for an amount less than the application amount, \n#there were also instances when the loan was granted for an amount higher than application amount\nplt.figure(figsize=(12,13))\nplt.subplot(211)\nax = sns.kdeplot(prev[\"AMT_APPLICATION\"],color=\"b\",linewidth=3)\nax = sns.kdeplot(prev[prev[\"AMT_CREDIT\"].notnull()][\"AMT_CREDIT\"],color=\"r\",linewidth=3)\nplt.title(\"Previous loan amounts applied and loan amounts credited.\")\n\nplt.subplot(212)\ndiff = (prev[\"AMT_CREDIT\"] - prev[\"AMT_APPLICATION\"]).reset_index()\ndiff = diff[diff[0].notnull()]\nax1 = sns.kdeplot(diff[0],color=\"g\",linewidth=3,label = \"difference in amount requested by client and amount credited\")\nplt.title(\"difference in amount requested by client and amount credited\")\nplt.axvline(0,color=\"black\",linestyle=\"dashed\",label = \"Zero\")","87166eee":"prev.NAME_CONTRACT_STATUS.value_counts()","4af3aba0":"#Analysis of difference in credit amount and application amount seen against the application status\nplt.figure(figsize=(12,13))\na1=sns.kdeplot(prev[prev.NAME_CONTRACT_STATUS == 'Approved']['AMT_CREDIT'] -\n               prev[prev.NAME_CONTRACT_STATUS == 'Approved']['AMT_APPLICATION'],\n               label = 'Approved', linewidth= 1)\na2=sns.kdeplot(prev[prev.NAME_CONTRACT_STATUS == 'Canceled']['AMT_CREDIT'] -\n               prev[prev.NAME_CONTRACT_STATUS == 'Canceled']['AMT_APPLICATION'], \n               label = 'Canceled', linewidth= 1)\na3=sns.kdeplot(prev[prev.NAME_CONTRACT_STATUS == 'Refused']['AMT_CREDIT'] -\n               prev[prev.NAME_CONTRACT_STATUS == 'Refused']['AMT_APPLICATION'], \n               label = 'Refused', linewidth= 1)\na4=sns.kdeplot(prev[prev.NAME_CONTRACT_STATUS == 'Unused offer']['AMT_CREDIT'] -\n               prev[prev.NAME_CONTRACT_STATUS == 'Unused offer']['AMT_APPLICATION'], \n               label = 'Unused offer', linewidth= 1)\nplt.axvline(0,color=\"black\",linestyle=\"dashed\",label = \"Zero\")\nplt.title('Difference between credit and application amount against status of contract')","5e25938a":"plt.figure(figsize=(12,13))\na1=sns.kdeplot(prev[prev.NAME_CONTRACT_STATUS == 'Approved']['AMT_CREDIT'], label = 'Approved', linewidth=3)\na2=sns.kdeplot(prev[prev.NAME_CONTRACT_STATUS == 'Canceled']['AMT_CREDIT'], label = 'Canceled', linewidth=3)\na3=sns.kdeplot(prev[prev.NAME_CONTRACT_STATUS == 'Refused']['AMT_CREDIT'], label = 'Refused', linewidth=3)\na4=sns.kdeplot(prev[prev.NAME_CONTRACT_STATUS == 'Unused offer']['AMT_CREDIT'], label = 'Unused offer', linewidth=3)\nplt.title('Amount of Credit compared against contract status')","13bbdd8a":"prev.info()","109f25e0":"#let's see how the bank is treating its different types of clients\nplt.figure(figsize=(10,6))\nsns.countplot(y='NAME_CLIENT_TYPE',data=prev,hue='NAME_CONTRACT_STATUS')\nplt.xlabel('Number of loans')\nplt.ylabel('Type of Client')\nplt.title('Status of each loan vs Type of Client (From Previous Data)')\nplt.legend(title='Contract Status', loc= 'lower right')\nplt.show()","fbd15299":"#We now try to see how these approved loans turned out in each category.\nnewapprovals=prev[(prev['NAME_CONTRACT_STATUS']=='Approved')][['SK_ID_CURR','NAME_CLIENT_TYPE']]","eaeb7663":"newapprovals.head()","1271ee1e":"merged_new_approvals = newapprovals.merge(app, how='left', left_on='SK_ID_CURR', right_on='SK_ID_CURR')","d4c8768c":"#Merging selected data from previous with application data (current)\nnew_approvals_by_default = merged_new_approvals[['SK_ID_CURR','NAME_CLIENT_TYPE','TARGET']]","043b1ad3":"#This plot shows all the loans which were marked as \"Approved\" in the previous applications, \n#and their statuses in the current applications.\nplt.figure(figsize=(10,6))\nax=sns.countplot(y='NAME_CLIENT_TYPE',data=new_approvals_by_default,hue='TARGET')\nplt.title('Previous Approved Loans - Current Status (Defaulted or Not)')\nplt.ylabel('Type of Client')\nplt.xlabel('Number of occurences')\nplt.legend(title='Target Variable',loc='lower right')\nplt.show()","aa444b57":"#Checking how the category of goods being bought is impacting the loan approval status \nplt.figure(figsize=(10,12))\nsns.countplot(y='NAME_GOODS_CATEGORY',hue='NAME_CONTRACT_STATUS',data=prev[prev.NAME_GOODS_CATEGORY!='XNA'])","9e04b5be":"#Let's see the pair plots between same numerical variables in the previous application data\namtp = prev[[ 'AMT_ANNUITY','AMT_APPLICATION','AMT_CREDIT', 'AMT_GOODS_PRICE', 'NAME_CONTRACT_STATUS']]\namtp = amtp[(amtp[\"AMT_GOODS_PRICE\"].notnull()) & (amtp[\"AMT_ANNUITY\"].notnull())]\nsns.pairplot(amtp,hue=\"NAME_CONTRACT_STATUS\")\nplt.show()","415c9c32":"#Let's check the correlation matrix for the previous application dataset\ncorr=prev[['AMT_ANNUITY','AMT_APPLICATION','AMT_CREDIT','AMT_GOODS_PRICE','HOUR_APPR_PROCESS_START','NFLAG_LAST_APPL_IN_DAY',\n           'DAYS_DECISION','SELLERPLACE_AREA','CNT_PAYMENT','NFLAG_INSURED_ON_APPROVAL']].corr()\n","213d756e":"corr = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\ncorrdf = corr.unstack().reset_index()\ncorrdf.head()","3f20a81e":"corrdf.columns = ['VAR1', 'VAR2', 'Correlation']\ncorrdf.dropna(subset = ['Correlation'], inplace = True)\ncorrdf['Correlation'] = round(corrdf['Correlation'], 2)\n# We will be using absolute value of the correlation coefficiaents since we are only interested in seeing the absolute value.\n# The direction in which the entities are correlated is currently not of concern\ncorrdf['Correlation'] = corrdf['Correlation'].abs()","a3c1e211":"#Inspecting the top-10 correlated entities\ncorrdf.sort_values(by = 'Correlation', ascending = False).head(10)","abb5947d":"#AMT_APPLICATION vs AMT_ANNUITY\n#Here we can notice a steady rice in the annuity amount with a rise in the application amount\nplt.figure(figsize=(10,5))\nsns.scatterplot(x='AMT_APPLICATION',y='AMT_ANNUITY',data=prev)\nplt.show()","71cb7dc0":"#AMT_ANNUITY vs AMT_CREDIT\n#We see an almost proportional increase in annuity amount with an increase in the credit amount \nplt.figure(figsize=(10,5))\nsns.scatterplot(y='AMT_ANNUITY',x='AMT_CREDIT',data=prev)\nplt.show()","48f29bb1":"prev.columns","a0e70d8a":"#Let's check the Amount of credit requested in various goods_categories\nplt.figure(figsize =(15,10))\nsns.barplot(x ='AMT_CREDIT', y=\"NAME_GOODS_CATEGORY\", data = prev )\nplt.show()","5608c828":"#Let's the see the number of loans in this category and the status of these loans\nsns.countplot('NAME_CONTRACT_STATUS', data = prev[prev.NAME_GOODS_CATEGORY == 'House Construction'])","1f06254a":"prev[prev.NAME_GOODS_CATEGORY=='House Construction']","782f04b4":"prev['PRODUCT_COMBINATION'].value_counts().plot(kind = 'barh')","a50cc4e2":"#Cash loans are seeing the most cancellations, while almost all the unused offers lie in the 'POS mobile with interest' combo\nplt.figure(figsize=(8,12))\nsns.countplot(y='PRODUCT_COMBINATION', data = prev, hue = 'NAME_CONTRACT_STATUS')","e0aaf5c7":"prev['CNT_PAYMENT'].value_counts()","30c32167":"#We will bin the CNT_PAYMENT variable to analyze it categorically\nprev['CNT_PAYMENT_BINNED']=pd.cut(prev['CNT_PAYMENT'], bins=[0,10,20,30,40,50,60,70,80,90], \n                         labels = ['0 - 10','10 - 20', '20 - 30','30 - 40','40 - 50','50 - 60','60 - 70','70 - 80','80 - 90'])","1c0477a4":"prev['CNT_PAYMENT_BINNED'].value_counts()","218f8c67":"#We see an interesting insight here when analyzing the term of previous credit against the contract statuses in previous application datatset\nax = pd.crosstab(prev[\"CNT_PAYMENT_BINNED\"],prev[\"NAME_CONTRACT_STATUS\"]).plot(kind=\"barh\",figsize=(12,5),stacked=True)\nplt.ylabel(\"Term of Previous Credit\")\nplt.xlabel('Number of applications')\nplt.title('Distrubution of applications against contract status vs term of previous credit')\nplt.show()","ab419eed":"#We try to cross-reference this column against the application dataset to see how these approved loans are performing","d8e39cfe":"newapprovals=prev[(prev['NAME_CONTRACT_STATUS']=='Approved')][['SK_ID_CURR','CNT_PAYMENT_BINNED']]","a2f073ca":"newapprovals.head()","76518dd5":"merged_new_approvals = newapprovals.merge(app, how='left', left_on='SK_ID_CURR', right_on='SK_ID_CURR')","8707f305":"#Merging selected data from previous with application data (current)\nnew_approvals_by_default = merged_new_approvals[['SK_ID_CURR','CNT_PAYMENT_BINNED','TARGET']]","d08213ec":"#Now, we see the current status of the approver loans in against the term of previous\nplt.figure(figsize=(12,5))\nsns.countplot('CNT_PAYMENT_BINNED', hue='TARGET', data=new_approvals_by_default)\nplt.xlabel('Term of previous credit')\nplt.ylabel('Number of applications')\nplt.title('Approved applications and their default status vs Term of Previous Credit')","884226ad":"## Observations\n* The **outlier(s)** seem to be in sync with those in the AMT_GOODS_PRICE column\n* This makes sense, since a client that purchases high value goods would certainly need a high value loan, hence causing the outliers in the in the credit amount column too","0c1f0e7d":"### We can make some observations here\n* Name_Type_Suite is a categorical variable and cannot be imputed using numerical analysis \n* Rest of the columns are numerical in nature, and some appear to have outliers. \n\nTo further study the presence of these outliers, we'll use box plots","66503e42":"### We can take the following observations from the visual analysis:\n* All the discussed numerical variables have a large number of outliers except for EXT_SOURCE_2. We can use the mean for imputing the null values in this column since the spread is even. For the rest, we'll have to use median. Let's take a look at some of these columns: \n    * 'OBS_30_CNT_SOCIAL_CIRCLE' - For this column, we see that the spread is heavily concentrated near the lower end. If we look at the description of the column above, the 50th percentile is at 0, 75th percentile at 2 and the max value is very high, 348. This will lead to a skewed mean, hence we can impute with the median which is 0.\n    * Same as above, for the columns 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', we see a similar spread of the data. A very huge volume of the data is at zero, and few outliers span to higher values. We can impute the null values with 0 for these as well. \n    * For the column 'AMT_REQ_CREDIT_BUREAU_YEAR', however, imputing with zero won't be wise. This is because the data is more spread out in this column, and there are many occurences of values greater than zero, as can be seen from the box plot. The median of the values in this column is occuring at 1, and we can use 1 as imputing value while filling the nulls in this column.\n* For the one categorical column we looked at (NAME_TYPE_SUITE), the cateogry \"Unaccompanied\" is a clear winner for the imputation process. \n    * The categorical variable can be imputed using the concept of mode. The mode of the data is the data point with the highest frequency of occurence. We can easily identify from the countplot that \"Unaccompanied\" has the highest frequency of occurence and can be used for this impuation. \n* For the column 'AMT_GOODS_PRICE' we will need to proceed with caution since this variable would be critical towards the analysis. If we impute a wrong value here, it could lead to heavily skewed results.","77f50dd3":"### Planning\nDue to the high imbalance in the data, using the 'hue' paramter while plotting may not yeild discernible results at all. Hence, we can make use of the subplot functionality to view the plots side by side, for target value 0 vs target value 1","d7fee34f":"### Observation\n* A large number of cancelled loans lie in the low value loans","b70a88ed":"## **Objective**: \n\nThis analysis aims to identify patterns which indicate if a client has difficulty paying their installments which may be used for taking actions such as denying the loan, reducing the amount of loan, lending (to risky applicants) at a higher interest rate, etc. This will ensure that the consumers capable of repaying the loan are not rejected. Identification of such applicants using EDA is the aim of this case study.\n\nIn other words, the company wants to understand the driving factors (or driver variables) behind loan default, i.e. the variables which are strong indicators of default.  The company can utilise this knowledge for its portfolio and risk assessment.\n\nWe will be looking at the process for cleaning the data, and visualizing several parameters so we can gain an understanding of the driving factors. We will be documenting all the inferences we can make based on our observations.","6528fbb7":"### Observation\n* An applicant was found to be more likely to default on the payment in the smaller income bracket.\n* In the lower middle class applicants, the default percentage is seen to be higher when they apply for high value loans with a credit amount of >2000k","475caf76":"### Observation:\n \n There are several columns like 'DAYS_BIRTH','DAYS_EMPLOYED','DAYS_ID_PUBLISH','DAYS_LAST_PHONE_CHANGE','DAYS_REGISTRATION' which contain a value representing number of days. These cannot be negative and need to be fixed. We can also change the 'DAYS_BIRTH' column to years, as years is a good measure for age of a person, not days. ","9a993ce8":"### Observation\n* We have a very major outlier here! Home Counstruction is outperforming every other category in terms of credit","ff95f783":"### Observations\n* Females tend to default more than men do\n* The labourer class has a high number of defaults","ff36999e":"### Observation:\n* We can note here that EXT_SOURCE_3 seems to be realitvely better at rating a client's repayment tendency.\n* The plot for defaulted payments skewed towards the left hand side, indicating that a low credit score can be used an indicator to represent repayment failure tendency","1441b535":"### Observation\nWe can observe quite few outliers in the annuity column. These would again be due to high value goods purchased by clients, needing high value credit, in turn causing a high annuity amount for the loan.","64203607":"### Observation\n* A lot of defaulters were concentrated in the low income region .\n* The annuity amount for the loans issued to these defaulters was also low, yet they defaulted on the payments.","0fb952fd":"### Observation\nThere are some **outliers** in the number of family members, these indicate some extraordinarily large families, and may be one-off cases.","f2a250a8":"### Starting with anlyzing previous application file now","3d58618a":"### Observation\nThere is definitely something wrong going on here. We see there is an applicant which has an employment experience of around 1000 years! This is definitely an error and we need to drop this value so it doesn't impact the analysis. Let's see what this row is.","48365634":"### Observations\n* If we wish to impute, we'll need to use the median for the columns AMT_ANNUITY and AMT_GOODS_PRICE since there are a significant number of outliers in the data which would skew the mean\n* For imputing null values in the categorical varaible PRODUCT_COMBINATION, we can go with the mode of the data, which is 'Cash'\n* CNT_PAYMENT has an even spread, and we can choose mean for imputing values in this column","9e2d8e74":"### Observation:\n* We see a hug enumber of canceled loans in the cash loans sector, and the same sector also attracts a lot of refused loans\n* The highest number of approved loans lie in the consumer loans sector","83ac0e6e":"### Observation\n* The data in 3 of these columns is numerical, and 1 is categorical wiht 17 unique values.","5b76febc":"### Observation:\n* We can note that the largest number of loan applicants fall in the range of 1-5 years in terms of work experience. There is also a significant chunk of people in the 45+ years experience range who apply for loans\n* We have successfully dealt with the outliers in the AMT_INCOME_TOTAL and DAYS_EMPLOYED using the method of binning","f7ef2f56":"#### After dropping the columns where more than half the rows contained null values, we are left with 81 columns (we started with 122)","186df149":"# STARTING ANALYSIS PART","860fda6a":"### We can now try imputing the missing values for columns where the null percentage is less than 14%","2dd2e595":"### Observation\n* We can see that the defaulting applicatns are skewed highly towards the left end, indicating that less experiences people with < 10 years of work experience are much more likely to default on a loan payment","b176b5c6":"### Observation\n* We get only one record, and as expected, the bank has refused this loan with a credit request of an obnoxiously large amount\n* This is an outlier in the data, but seems genuine since it was rejected.","0ce7285b":"### Observations\n* We have a lot of crowding of loan application from people in middle class, specifically upper middle class with an income in the range of 120-250 thousand. \n* There's clearly a few **outliers** in this data, we have binned them in the \"UberRich\" category ","cdac6b7f":"## Inspecting the dataframe","caf4c68e":"We can easily observe that the value of goods against which a loan is acquired ranges from 40k to 4050k","60d1299b":"### Cleaning the data in previous application file","33600587":"### Observation\n* Around 8% of the records have the target variable as 1, and the rest 92% have the target variable as 0\n* Hence, our data is **highly imbalanced**","49dc1b7e":"### Observation\n* We see that the highest number of loan approvals are occuring for repeat clients, while loans are being very rarely refused to new customers. \n* Repeat Clients are also defaulting on payments the most.\n* There is also a significant chunk of \"New\" clients with defaulted payments","4b1f1fbf":"### Observation: \n* Younger applicants are a little more likely to default a payment than the older ones","bec07013":"### Observation:\n* Both the data sets are showing a similar set of variables with a high correlation value"}}