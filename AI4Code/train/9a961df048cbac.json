{"cell_type":{"87abd8f4":"code","9747836d":"code","69d14470":"code","e4e7a7f7":"code","d0428031":"code","ed82bdca":"code","189966c7":"code","ad47d881":"code","4fab496e":"markdown","69acd605":"markdown","b4b89cd6":"markdown","5593e368":"markdown","5ed78bfa":"markdown","62cf0143":"markdown","5b4be04b":"markdown"},"source":{"87abd8f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9747836d":"import pandas as pd\nimport numpy as np\n\nX_train_csv = pd.read_csv(\"..\/input\/churn-model-data-set-competition-form\/X_train.csv\")\ny_train_csv = pd.read_csv(\"..\/input\/churn-model-data-set-competition-form\/y_train.csv\")\nX_test_csv  = pd.read_csv(\"..\/input\/churn-model-data-set-competition-form\/X_test.csv\")\ny_test_csv  = pd.read_csv(\"..\/input\/churn-model-data-set-competition-form\/test_label\/y_test.csv\")\n\ndef EDA(name, df):\n    print(\"== ===================================================\")\n    print(\"== EDA : \", name)\n    print(\"== ===================================================\")\n    \n    print(\">>> all column info\")\n    print(df.columns)\n    \n    print(\">>> include object column info\")\n    print(df.select_dtypes(include=object).columns)\n    for col in df.select_dtypes(include=object).columns :\n        print(\"-- ---------------------------------------------------\")\n        print(col, \"-\", df[col].nunique(), \" : \", df[col].unique())\n        print(df[col].value_counts())\n        print(\"null check : \", df[col].isnull().sum())\n    \n    print(\">>> exclude object column info\")\n    print(df.select_dtypes(exclude=object).columns)\n    for col in df.select_dtypes(exclude=object).columns :\n        print(\"-- ---------------------------------------------------\")\n        print(col, \"-\", df[col].unique()[:10])\n        print(df[col].describe())\n        print(\"null check : \", df[col].isnull().sum())\n    \n    print(\"== ===================================================\")\n    print(df.isnull().sum())\n    print(df.info())\n    print(df.describe())    \n    print(\"== ===================================================\\n\\n\")\n    \nEDA(\"X_train_csv\", X_train_csv)\nEDA(\"y_train_csv\", y_train_csv)\nEDA(\"X_test_csv\", X_test_csv)","69d14470":"## User Defined Variable\ntarget_col = \"Exited\"\ndrop_col = [\"CustomerId\", \"Surname\"]\nis_scaled = True\n\n### strategy\n\"\"\"\nIndex(['CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age',\n       'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n       'EstimatedSalary'],\n      dtype='object')\nGender : Male, Female \ub85c \uc77c\uce58\ud654\n\n\"\"\"\n\ndef data_preprocessing(df_train, df_test):\n    split_len = df_train.shape[0]\n    \n    df_train[\"Gender\"] = df_train[\"Gender\"].replace(\" male\", \"Male\").replace(\"female\",\"Female\")\n#     df_train[\"Gender\"] = df_train[\"Gender\"].replace(\"female\",\"Female\")\n    df_test[\"Gender\"] = df_test[\"Gender\"].replace(\" male\", \"Male\").replace(\"female\",\"Female\")\n#     df_test[\"Gender\"] = df_test[\"Gender\"].replace(\"female\",\"Female\")\n    \n    print(\" > Gender replace check ---------------------------------\")\n    print(df_train[\"Gender\"].value_counts())\n    print(df_test[\"Gender\"].value_counts())\n    \n    print(\" > One-Hot encoding pre count check ---------------------\")\n    print(df_train.shape)\n    print(df_test.shape)\n    df_dum = pd.get_dummies(pd.concat([df_train, df_test], axis=0))\n    \n    print(\" > One-Hot encoding shape check -------------------------\")\n    print(df_dum.shape)\n    \n    df_train = df_dum[:split_len]\n    df_test  = df_dum[split_len:]\n    \n    print(\" > One-Hot encoding split count check -------------------\")\n    print(df_train.shape)\n    print(df_test.shape)\n    \n    print(\" > One-Hot encoding type check --------------------------\")\n    print(df_train.info())\n    print(df_test.info())\n    \n    return df_train, df_test\n\nX = X_train_csv.drop(drop_col, axis=1)\ny = y_train_csv[target_col]\nX_sub = X_test_csv.drop(drop_col, axis=1)\n\nX, X_sub = data_preprocessing(X, X_sub)","e4e7a7f7":"## Holdout\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42)\n\n## Regulation\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_scaled_train = X_train\nX_scaled_test = X_test\nX_scaled_sub = X_sub\n\nif is_scaled :\n    X_scaled_train = scaler.transform(X_train)\n    X_scaled_test = scaler.transform(X_test)\n    X_scaled_sub = scaler.transform(X_sub)","d0428031":"## model - RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_scaled_train, y_train)\n\n## train predict\npred_train = model.predict(X_scaled_train)\nprob_train = model.predict_proba(X_scaled_train)\n## test predict\npred_test = model.predict(X_scaled_test)\nprob_test = model.predict_proba(X_scaled_test)\n## submission predict\npred_sub = model.predict(X_scaled_sub)\nprob_sub = model.predict_proba(X_scaled_sub)\n\n\n## evaluation - f1_score, roc_auc_score, accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score, accuracy_score\nprint(\"\\n\\n>>> train -----------------------------------------------\")\nprint(\"accuracy_score : \", accuracy_score(y_train, pred_train))\nprint(\"f1_score       : \", f1_score(y_train, pred_train))\nprint(\"roc_auc_score  : \", roc_auc_score(y_train, prob_train[:, 1]))\nprint(confusion_matrix(y_train, pred_train))\nprint(classification_report(y_train, pred_train))\n\nprint(\"\\n\\n>>> test ------------------------------------------------\")\nprint(\"accuracy_score : \", accuracy_score(y_test, pred_test))\nprint(\"f1_score       : \", f1_score(y_test, pred_test))\nprint(\"roc_auc_score  : \", roc_auc_score(y_test, prob_test[:,1]))\nprint(confusion_matrix(y_test, pred_test))\nprint(classification_report(y_test, pred_test))\n\nprint(\"\\n\\n>>> sub creation ----------------------------------------\")\ndf_sub = pd.DataFrame({\"CustomerId\":X_test_csv['CustomerId'], \"Exited\":prob_sub[:,1], \"Exited.pred\":pred_sub})\ndf_sub.to_csv(\"y_submission.csv\", index=False)\n\ndf_subr = pd.read_csv(\".\/y_submission.csv\")\ndisplay(df_subr.head())\n\ny_sub = y_test_csv[\"Exited\"]\nprob_subr = df_subr[\"Exited\"]\npred_subr = df_subr[\"Exited.pred\"]\n\nprint(\"\\n\\n>>> sub -------------------------------------------------\")\nprint(\"accuracy_score : \", accuracy_score(y_sub, pred_subr))\nprint(\"f1_score       : \", f1_score(y_sub, pred_subr))\nprint(\"roc_auc_score  : \", roc_auc_score(y_sub, prob_subr))\nprint(confusion_matrix(y_sub, pred_subr))\nprint(classification_report(y_sub, pred_subr))","ed82bdca":"## Enclosure.01 Cross_val\n## cross valid \nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold\nkfold = KFold(n_splits=10)\nscore = cross_val_score(RandomForestClassifier(random_state=42), X_scaled_train, y_train, cv=kfold, scoring=\"roc_auc\")\nprint(score.mean())\nprint(score)\n# help(GridSearchCV)","189966c7":"## Enclosure.02 GridSearchCV\n## GridSearchCV\n\"\"\"\ngrid_param = {\"n_estimators\":range(100, 1000, 100), \"min_samples_split\":range(1, 10, 1), \"min_samples_leaf\":range(1,5,1), \"max_features\":[\"log2\", \"sqrt\", \"auto\"]}\ngrid_search = GridSearchCV(model, grid_param)\ngrid_search.fit(X_scaled_train, y_train)\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.score(X_scaled_test, y_test))\n\"\"\"","ad47d881":"## Enclosure.03 FeatureImportances\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nftr_importances_values = model.feature_importances_\nftr_importances = pd.Series(ftr_importances_values, index = X_train.columns)\nftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n\nplt.figure(figsize=(8,6))\nplt.title(\"Top 20 Feature Importances\")\nsns.barplot(x=ftr_top20, y=ftr_top20.index)\nplt.show()","4fab496e":"## Enclosure.01 Cross_val","69acd605":"## Part.04 Data Modeling & Evaluation","b4b89cd6":"## Part.01 Data Load & EDA","5593e368":"## Part.02 Data Preprocessing","5ed78bfa":"## Part.03 Data Holdout & Data Normalization","62cf0143":"## Enclosure.03 FeatureImportances","5b4be04b":"## Enclosure.02 GridSearchCV"}}