{"cell_type":{"edcfd0a1":"code","24028853":"code","30fa8f03":"markdown","6907deab":"markdown","e9dc03a5":"markdown","75157ed3":"markdown"},"source":{"edcfd0a1":"def get_model(cfg):\n    model_input = tf.keras.Input(shape=(cfg['net_size'], cfg['net_size'], 3), name='imgIn')\n\n    dummy = tf.keras.layers.Lambda(lambda x:x)(model_input)\n    \n    outputs = []    \n    for i in range(cfg['net_count']):\n        constructor = getattr(efn, f'EfficientNetB{i}')\n        \n        x = constructor(include_top=False, weights='imagenet', \n                        input_shape=(cfg['net_size'], cfg['net_size'], 3), \n                        pooling='avg')(dummy)\n        dense = []\n        FC = tf.keras.layers.Dense(32, activation='relu')\n        for p in np.linspace(0.1,0.5, 5):\n            x_ = tf.keras.layers.Dropout(p)(x)\n            x_ = FC(x_)\n            x_ = tf.keras.layers.Dense(1, activation='sigmoid')(x_)\n            dense.append(x_)\n        x = tf.keras.layers.Average()(dense)\n        outputs.append(x)\n        \n    model = tf.keras.Model(model_input, outputs, name='aNetwork')\n    model.summary()\n    return model","24028853":"class multilabel_dropout():\n    # Multisample Dropout: https:\/\/arxiv.org\/abs\/1905.09788\n    def __init__(self, HIGH_DROPOUT, HIDDEN_SIZE):\n        self.high_dropout = torch.nn.Dropout(config.HIGH_DROPOUT)\n        self.classifier = torch.nn.Linear(config.HIDDEN_SIZE * 2, 2)\n    def forward(self, out):\n        return torch.mean(torch.stack([\n            self.classifier(self.high_dropout(p))\n            for p in np.linspace(0.1,0.5, 5)\n        ], dim=0), dim=0)","30fa8f03":"Dropout parameters change 0.1 - 0.5. You can customer this for betters model.","6907deab":"## Multi-Sample Dropout\nDropout is an efficient regularization instrument for avoiding overfitting of deep neural networks. It works very simply randomly discarding a portion of neurons during training; as a result, a generalization occurs because in this way neurons depend no more on each other.\nIn this post, I try to reproduce the results presented in this paper; which introduced a technique called Multi-Sample Dropout. As declared by the author, its scopes are:\n* accelerate training and improve generalization over the original dropout;\n* reduce computational cost because most of the computation time is consumed in the layers below (often convolutional or recurrent) and the weights in the layers at the top are shared;\n* achieve lower error rates and losses.\n![](https:\/\/miro.medium.com\/max\/700\/1*mdBXp3-D7G7mTcKDZHui8g.png)","e9dc03a5":"## For Keras","75157ed3":"## For pytorch"}}