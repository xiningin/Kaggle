{"cell_type":{"1572a6a3":"code","e3e6d242":"code","6ef21751":"code","f11b23c1":"code","30ea3d17":"code","6d6ad442":"code","f19b8cc4":"code","ae0ae4bf":"code","1bbb26ae":"code","781bbc1d":"code","e213ec7f":"code","04dca7f7":"code","2ca48905":"markdown","3c05b8ed":"markdown","3dfcc11b":"markdown"},"source":{"1572a6a3":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport transformers\nfrom transformers import AlbertTokenizer, AlbertModel#Bert\u304c\u91cd\u304b\u3063\u305f\u306e\u3067\u3001albert\u306b\u5909\u66f4\nfrom tqdm import tqdm\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nimport umap#\nimport matplotlib.pyplot as plt\nimport time\ntqdm.pandas()","e3e6d242":"device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\ntrain = pd.read_csv(\"..\/input\/data-science-winter-osaka2\/train.csv\")\ntest = pd.read_csv(\"..\/input\/data-science-winter-osaka2\/test.csv\")","6ef21751":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"albert-base-v1\", num_labels=3).to(device)\nmodel.load_state_dict(torch.load(\"..\/input\/description-embedding-with-tuned-bert-1-training\/tuned_albert.pth\"))#\u5b66\u7fd2\u6e08\u307fmodel\u306eload","f11b23c1":"#\u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u4ed8\u3051\u66ff\u3048\ntrain[\"user_reviews_int\"] = train[\"user_reviews\"].map({'c0':0, 'c1':1, 'c2':2})\ntrain['description'] = train['description'].fillna(\"NaN\")\ntest['description'] = test['description'].fillna(\"NaN\")\n\nprint(train.shape)\nprint(test.shape)","30ea3d17":"#\u63a8\u8ad6\u7528\u306edataloader\nclass Description_dataset_valid(Dataset):\n    def __init__(self, df, \n                 features=\"description\",\n                 model_name = 'albert-base-v1',\n                 max_len=512#\u9577\u6587\u306e\u7d39\u4ecb\u3082\u591a\u304b\u3063\u305f\u306e\u3067\u3001\u30e2\u30c7\u30eb\u306e\u6700\u5927\u307e\u3067token\u306e\u9577\u3055\u3092\u62e1\u5f35(\u3057\u305f\u305f\u3081\u306b\u30e1\u30e2\u30ea\u306e\u6d88\u8cbb\u304c\u6fc0\u3057\u304f\u306a\u308a\u307e\u3057\u305f\u3002)\n                ):\n        self.features_values = df[features].values\n        self.tokenizer = AlbertTokenizer.from_pretrained(model_name)\n        self.max_len = max_len\n\n    # len()\u3092\u4f7f\u7528\u3059\u308b\u3068\u547c\u3070\u308c\u308b\n    def __len__(self):\n        return len(self.features_values)\n\n    # \u8981\u7d20\u3092\u53c2\u7167\u3059\u308b\u3068\u547c\u3070\u308c\u308b\u95a2\u6570    \n    def __getitem__(self, index):\n        text = self.features_values[index]#text\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\n        inputs = self.tokenizer.encode_plus(\n              text,\n              add_special_tokens=True,\n              max_length=self.max_len,\n              padding='max_length',\n              truncation=True\n            )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        \n        return {\n          'ids': torch.LongTensor(ids),\n          'mask': torch.LongTensor(mask)\n        }","6d6ad442":"dataset_valid = Description_dataset_valid(train)\nvalid_loader = DataLoader(dataset_valid, batch_size=8, shuffle=False)\n\n\nembedings = []\nfor batch in tqdm(valid_loader):\n    bert_out = model.albert(batch[\"ids\"].to(device), \n                            batch[\"mask\"].to(device))\n    res = bert_out[\"last_hidden_state\"][:, 0, :] #\u5404\u30b5\u30f3\u30d7\u30eb\u306e[CLS] token\u306e\u307f\u3092\u53d6\u308a\u51fa\u3059(dim:768)\n    if torch.cuda.is_available():    \n        res = res.cpu().detach().numpy() # 0\u756a\u76ee\u306f [CLS] token, 768 dim \u306e\u6587\u7ae0\u7279\u5fb4\u91cf\n    else:\n        res =  res.detach().numpy()\n    embedings.append(res)\n    \nres = np.concatenate(embedings)\nnp.save('train_embed_pretrain.npy', res)","f19b8cc4":"dataset_valid_test = Description_dataset_valid(test)\nvalid_loader_test = DataLoader(dataset_valid_test, batch_size=8, shuffle=False)\n\n\nembedings_test = []\nfor batch in tqdm(valid_loader_test):\n    bert_out = model.albert(batch[\"ids\"].to(device), \n                            batch[\"mask\"].to(device))\n    res_test = bert_out[\"last_hidden_state\"][:, 0, :] #\u5404\u30b5\u30f3\u30d7\u30eb\u306e[CLS] token\u306e\u307f\u3092\u53d6\u308a\u51fa\u3059(dim:768)\n    if torch.cuda.is_available():    \n        res_test = res_test.cpu().detach().numpy() # 0\u756a\u76ee\u306f [CLS] token, 768 dim \u306e\u6587\u7ae0\u7279\u5fb4\u91cf\n    else:\n        res_test =  res_test.detach().numpy()\n    embedings_test.append(res_test)\n    \nres_test = np.concatenate(embedings_test)\nnp.save('test_embed_pretrain.npy', res_test)","ae0ae4bf":"#\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u307f\u3067mapper\u3092\u4f5c\u6210\nmapper = umap.UMAP(random_state=0)\nembedding_train = mapper.fit_transform(res)\nembedding_train = pd.DataFrame(embedding_train)\nembedding_train[\"grade\"] = train[\"user_reviews_int\"]","1bbb26ae":"embedding_test = mapper.transform(res_test)","781bbc1d":"#\u8a13\u7df4\u30c7\u30fc\u30bf\u5168\u3066\nplt.figure(figsize=[10, 10])\nplt.scatter(embedding_train.iloc[:,0], \n            embedding_train.iloc[:,1], \n            c=embedding_train.iloc[:,2], \n            alpha=0.3, s=3)\nplt.colorbar()","e213ec7f":"#\u30ec\u30d3\u30e5\u30fc\u304c\u3064\u3044\u305f\u30c7\u30fc\u30bf\u306e\u307f\nembedding_labeled = embedding_train[embedding_train.iloc[:,2]!=2]\nplt.figure(figsize=[10, 10])\nplt.scatter(embedding_labeled.iloc[:,0], \n            embedding_labeled.iloc[:,1], \n            c=embedding_labeled.iloc[:,2], \n            alpha=0.3, s=3)\nplt.colorbar()","04dca7f7":"plt.figure(figsize=[10, 10])\nplt.scatter(embedding_train.iloc[:,0], \n            embedding_train.iloc[:,1], \n            alpha=0.3, s=3, label=\"train\")\nplt.scatter(embedding_test[:,0], \n            embedding_test[:,1], \n            alpha=0.3, s=3, label=\"test\")\nplt.legend()","2ca48905":"- \u4ee5\u4e0b\u306eNotebook\u306b\u3066fintune\u3057\u305f\u91cd\u307f\u3092\u5229\u7528\u3057\u3066\u3001**description**\u3092embedding\u3057\u307e\u3059\u3002\n\nhttps:\/\/www.kaggle.com\/ryoichi0917\/training-1-finetuned-bert-description-embedding\n\n- \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3067\u306f\u7dba\u9e97\u306b\u3092embedding\u7d50\u679c\u304c\u5206\u304b\u308c\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u3001model\u3092finetune\u3059\u308c\u3070\u826f\u304f\u306a\u308b\u306e\u3067\u306f\uff1f\u3068\u601d\u3044\u5b9f\u9a13\u3057\u307e\u3057\u305f\u3002","3c05b8ed":"pretrain\u306e\u91cd\u307f\u3092\u5229\u7528\u3057\u305f\u6642\u3068\u6bd4\u8f03\u3057\u3066\u3001\u6539\u5584\u3057\u3066\u3044\u307e\u3059\u3002(\u7dba\u9e97\u306b\u5206\u3051\u3089\u308c\u305d\u3046\u3002)\n\n\u3010\u53c2\u8003\u30fbpretrain\u306e\u91cd\u307f\u3092\u5229\u7528\u3057\u305f\u5834\u5408\u3011\nhttps:\/\/www.kaggle.com\/ryoichi0917\/description-embedding-with-bert-batch-ver","3dfcc11b":"# FineTune\u3057\u305fBERT(ALBERT)\u306b\u3088\u308bdescription\u306eembeding"}}