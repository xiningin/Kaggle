{"cell_type":{"4eb5ffc9":"code","d14331b2":"code","42234663":"code","3028834d":"code","7ca358c0":"code","ea199f47":"code","60c58c34":"code","a52dd36c":"code","dd6f99d0":"code","77aae0ba":"code","35bc9db1":"code","141c9994":"code","b68322c6":"code","191f044c":"code","218394ad":"code","09950e5b":"code","9fba0b6e":"code","e4e022bd":"code","711f54c1":"code","3fff82f6":"code","4b09e1c8":"code","7c75c7f3":"code","c30cc2ec":"code","459f9744":"code","5cfbaf1a":"code","d3aa8b1c":"code","6ec622b8":"code","1f9aa851":"code","649a7366":"code","1c10e650":"code","338e8a94":"code","966a733d":"code","b66c6249":"code","5e95dc30":"code","82f86102":"code","8c5096ee":"code","795df299":"code","4e2a7114":"code","41e66504":"markdown","ad69d7f3":"markdown","2d552c66":"markdown","438bd698":"markdown","0370f3e6":"markdown","349ae683":"markdown","e4e55444":"markdown","a9655330":"markdown","a2199fcc":"markdown","99cece07":"markdown","08b74d19":"markdown"},"source":{"4eb5ffc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d14331b2":"import pandas as pd\nimport numpy as np\nfrom time import time\n\nimport matplotlib.pyplot as plt","42234663":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head()","3028834d":"train = train.drop(['Alley','PoolQC','Fence','MiscFeature'], axis=1)\ntrain.head()\ntrain.shape","7ca358c0":"for i in train.columns:\n    if 'SalePrice' not in i:\n        if 'object' in str(train[str(i)].dtype):\n            train[str(i)] = train[str(i)].fillna(train[str(i)].mode().index[0])\n            \nfor i in train.columns:\n    if 'SalePrice' not in i:\n        if 'object' not in str(train[str(i)].dtype):\n            train[str(i)] = train[str(i)].fillna(train[str(i)].mean())\n            \ntrain.head()","ea199f47":"# multiple column label encoding\nfor i in train.columns:\n    if 'SalePrice' not in i:\n        if 'object' in str(train[str(i)].dtype):\n            train[str(i)]=train[str(i)].astype('category').cat.codes\n\ntrain.head()","60c58c34":"train['SalePrice'] = np.log1p(train['SalePrice'])","a52dd36c":"train.info()","dd6f99d0":"x = train.drop(\"SalePrice\", axis = 1)\ny = train[\"SalePrice\"]","77aae0ba":"x_train, x_test, y_train, y_test = train_test_split(x,y, train_size = 0.8 , random_state = 27)","35bc9db1":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators= 500,\n min_samples_split= 2,\n min_samples_leaf= 1,\n max_features = 'sqrt',\n max_depth = 15\n)","141c9994":"rf.fit(x_train, y_train)\npred_rf = rf.predict(x_test)","b68322c6":"import seaborn as sns\nsns.distplot(y_test-pred_rf)","191f044c":"import xgboost\nxgb = xgboost.XGBRegressor(subsample = 0.6, n_estimators = 1100, min_child_weight = 3, max_depth = 5, learning_rate =  0.005)\nxgb.fit(x_train, y_train)\npred_xgb = xgb.predict(x_test)","218394ad":"sns.distplot(y_test-pred_xgb)","09950e5b":"import lightgbm\nlgbm = lightgbm.LGBMRegressor(bagging_fraction=0.7, bagging_freq=10, boosting_type='gbdt', class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,\n       importance_type='split', learning_rate=0.005, max_bin=512,\n       max_depth=8, metric=['l2', 'auc'], min_child_samples=20,\n       min_child_weight=0.001, min_split_gain=0.0, n_estimators=1000,\n       n_jobs=-1, num_iterations=100000, num_leaves=128,\n       objective='regression', random_state=None, reg_alpha=0.0,\n       reg_lambda=0.0, silent=True, subsample=1.0,\n       subsample_for_bin=200000, subsample_freq=0, task='train', verbose=0)\nlgbm.fit(x_train, y_train)\npred_lgbm = lgbm.predict(x_test)","9fba0b6e":"sns.distplot(y_test-pred_lgbm)","e4e022bd":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\n#score = cross_val_score(rf, x,y ,  cv = 10)","711f54c1":"#print(score)","3fff82f6":"#score.mean()","4b09e1c8":"#from sklearn.model_selection import KFold, cross_val_score, train_test_split\n#score1 = cross_val_score(xgb, x,y ,  cv = 10)","7c75c7f3":"#print(score1)","c30cc2ec":"#score1.mean()","459f9744":"lgbm = lightgbm.LGBMRegressor(bagging_fraction=0.7, bagging_freq=10, boosting_type='gbdt', class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,\n       importance_type='split', learning_rate=0.005, max_bin=512,\n       max_depth=8, metric=['l2', 'auc'], min_child_samples=20,\n       min_child_weight=0.001, min_split_gain=0.0, n_estimators=1000,\n       n_jobs=-1, num_iterations=100000, num_leaves=128,\n       objective='regression', random_state=None, reg_alpha=0.0,\n       reg_lambda=0.0, silent=True, subsample=1.0,\n       subsample_for_bin=200000, subsample_freq=0, task='train', verbose=0)\nlgbm.fit(x_train, y_train)","5cfbaf1a":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nscore2 = cross_val_score(lgbm, x,y ,  cv = 10)","d3aa8b1c":"print(score2)","6ec622b8":"score2.mean()","1f9aa851":"#from sklearn.model_selection import RandomizedSearchCV\n #Randomized Search CV\n\n# Number of trees in random forest\n#n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Various learning rate parameters\n#learning_rate = ['0.005','0.01', '0.05','0.1','0.2','0.3', '0.4', '0.5']\n#num_leaves = [int(x) for x in np.linspace(start = 10, stop = 100, num = 2)]\n# Maximum umber of levels in tree\n#max_depth = [int(x) for x in np.linspace(20, 60, num = 6)]\n# max_depth.append(None)\n#Subssample parameter values\n#subsample=[0.7,0.6,0.8,1,1.2,1.3,1.4,1.5]\n# Minimum child weight parameters\n#min_child_weight=[int(x) for x in np.linspace(0.00001, 10, num = 10)]\n","649a7366":"#random_grid = {'n_estimators': n_estimators,\n#               'learning_rate': learning_rate,\n#               #'max_depth': max_depth,\n#               'num_leaves': num_leaves,\n#               'subsample': subsample,\n#               'min_child_weight': min_child_weight}\n#\n#print(random_grid)","1c10e650":"#lgbm_random = RandomizedSearchCV(estimator = lgbm, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = 1)","338e8a94":"#lgbm_random.fit(x_train, y_train)","966a733d":"#lgbm_random.best_params_","b66c6249":"#lgbm_random.best_score_","5e95dc30":"#pred_lgbm_random = lgbm_random.predict(x_test)","82f86102":"#score4 = cross_val_score(lgbm_random, x,y ,  cv = 10)","8c5096ee":"#target = pd.DataFrame(columns=['actual', 'rf_pred', 'xgb_pred', 'lgb_pred'])","795df299":"#target['actual']=y_test\n#target['rf_pred']=pred_rf\n#target['xgb_pred']=pred_xgb\n#target['lgb_pred']=pred_lgbm_random","4e2a7114":"#target","41e66504":"## LGBM Hyperparameter","ad69d7f3":"## LGBM HYPER CV","2d552c66":"#Split","438bd698":"# Ensemble all models (Bagging)","0370f3e6":"## LGBM CV","349ae683":"# RF Model","e4e55444":"## XGB CV","a9655330":"# LGBM Model","a2199fcc":"## RF CV","99cece07":"# CV with K fold","08b74d19":"# XGB Model"}}