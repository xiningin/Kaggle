{"cell_type":{"59a271df":"code","214a8c48":"code","30990256":"code","67db84ac":"code","ac30bbf2":"code","d29c31ab":"code","37d781a0":"code","4091e3be":"code","3b8df557":"code","752a50fe":"code","4796e293":"code","9c43b13b":"code","878ad64f":"code","97997fff":"code","59df9661":"code","b2ed0835":"code","ff8b088d":"code","9879378b":"code","24ccaca4":"code","55162364":"code","8739053d":"code","be87188f":"code","4de2d670":"code","331d44ee":"code","96031212":"code","54134e81":"code","7c8634ea":"code","f73a76e4":"code","2be50e60":"code","1be350f7":"code","726acbf2":"code","ca5ba91b":"code","860b4573":"code","69eb3b99":"code","0da79a2a":"code","e49590e6":"code","ba3a4574":"code","ae7546ab":"code","096d2ce9":"code","57a93947":"code","9546757f":"markdown","d30607c4":"markdown","850a0b7a":"markdown","1b363a56":"markdown","f6f2030b":"markdown","47311663":"markdown","814de87d":"markdown","26da0cb9":"markdown","046f682e":"markdown","f5d4143c":"markdown","bb4cac73":"markdown","db0251d3":"markdown","d4574eca":"markdown","4d4b1350":"markdown","b814be89":"markdown"},"source":{"59a271df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","214a8c48":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nprint('Setup complete')","30990256":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint(\"No. of Row, Column of train: \", train.shape)\ntrain.head()","67db84ac":"train.columns","ac30bbf2":"train.drop('Id', axis=1, inplace=True)","d29c31ab":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest.drop('Id', axis=1, inplace=True)\nprint(\"No. of Row, Column of test: \", test.shape)\ntest.head()","37d781a0":"test.columns","4091e3be":"train.describe()","3b8df557":"# Target SalePrie\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\ndef feature_distribution(data):\n  fig = plt.figure(figsize=(12,5))\n  plt.subplot(1,2,1)\n  sns.distplot(data , fit=norm);\n  # Get the fitted parameters used by the function\n  (mu, sigma) = norm.fit(data)\n  print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n  plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n              loc='best')\n  plt.ylabel('Frequency')\n  plt.title('SalePrice distribution')\n  plt.subplot(1,2,2)\n  res = stats.probplot(data, plot=plt)\n\nfeature_distribution(train['SalePrice'])","752a50fe":"# log(1+x) transform\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\nfeature_distribution(train['SalePrice'])","4796e293":"# Split features and labels\ntrain_labels = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape","9c43b13b":"#Here we will check the percentage of nan values present in each feature\n# function return percent of missing value in each feature\ndef features_null(df):\n    features_with_na = [x for x in df.columns if df[x].isnull().sum()>0]\n    print('No. features_with_na = ' + str(len(features_with_na)))\n    print('Total NaN = ' + str(df.isnull().values.sum()))\n\n    total_miss = df[features_with_na].isnull().sum()\n    percent_miss = total_miss*100\/len(df)\n    features_with_na_df = pd.DataFrame({'Total': total_miss,\n                                  '%': percent_miss})\n    features_with_na_df = features_with_na_df.sort_values('%',ascending=False)\n    return features_with_na_df\n\nfeatures_null(all_features)\n","878ad64f":"# Checking Numerical Data\n# Ph\u00e2n lo\u1ea1i numerical v\u00e0 non_numerical\ndef classify_numerical_feature(df):\n    numerical = df.select_dtypes('number').columns\n    non_numerical = df.select_dtypes(['category', 'object']).columns\n    print('No. numerical_features = ' + str(len(numerical)))\n    print('No. non_numerical_features = ' + str(len(non_numerical)))\n    return numerical, non_numerical\n\nnumerical_features, non_numerical_features = classify_numerical_feature(all_features)  \n\nall_features[numerical_features].head()","97997fff":"# Non_numerical feature\nall_features[non_numerical_features].head()","59df9661":"# Correlation\ncorr = np.abs(train.corr())\nplt.figure(figsize=(30,12))\nsns.heatmap(corr,cmap='coolwarm',annot = True)\nplt.show()","b2ed0835":"top_feature = corr.index[abs(corr['SalePrice']>0.5)]\nplt.subplots(figsize=(12, 8))\ntop_corr = train[top_feature].corr()\nsns.heatmap(top_corr, cmap='coolwarm',annot=True)\nplt.show()","ff8b088d":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","9879378b":"# Numerical feature\n# Function show scatter\ndef scatter_with_feature(df, features, feature_target, figures_per_time = 4):\n  count = 0\n  for feature in features:\n    plt.figure(count\/\/figures_per_time,figsize=(25,5))\n    plt.subplot(1,figures_per_time,np.mod(count,figures_per_time)+1)\n    plt.scatter(x= feature, y=feature_target, data = df)\n    plt.xlabel(feature)\n    count += 1\n\nscatter_with_feature(train, numerical_features, 'SalePrice')","24ccaca4":"# Non_numerical_feature\ndef boxplot_with_features(df, features, target,figures_per_time = 4):\n  count = 0\n  for feature in features:\n    plt.figure(count\/\/figures_per_time,figsize=(25,5))\n    plt.subplot(1,figures_per_time,np.mod(count,figures_per_time)+1)\n\n    sns.boxplot(x=feature, y=target, data=df)\n    plt.xticks(\n      rotation=45, \n      horizontalalignment='right',\n      fontweight='light',\n      fontsize='medium'  \n      )\n    count += 1\n\nboxplot_with_features(train, non_numerical_features, 'SalePrice')","55162364":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","8739053d":"def clean_null(df):\n\n    # Replace 'LotFrontage' base on Neighborhood\n    df['LotFrontage'] = df.groupby('Neighborhood')[\"LotFrontage\"].transform(\n        lambda x: x.fillna(x.median()))\n\n    # GarageYrBlt replace with 0\n    df['GarageYrBlt'].fillna(0, inplace=True)\n    \n    #MasVnrArea : replace with 0\n    df['MasVnrArea'].fillna(0, inplace = True)\n\n    df['Electrical'].fillna(df['Electrical'].mode()[0], inplace=True)\n    \n    numerical = df.select_dtypes('number').columns\n    non_numerical = df.select_dtypes(['category', 'object']).columns\n    \n    # missing value in non_numerical_columns\n    features_with_na = [x for x in df.columns if df[x].isnull().sum()>0]  \n    non_numerical_features_na = [x for x in features_with_na if x in non_numerical]   \n    # Missing data filling None\n    df[non_numerical_features_na] = df[non_numerical_features_na].fillna('None')\n    \n    numerical_features_na = [x for x in features_with_na if x in numerical]   \n    for x in numerical_features_na:\n        df[x].fillna(0, inplace = True)\n    \nclean_null(all_features)\n","be87188f":"features_null(all_features)","4de2d670":"# Transfrom categorical_feature\n\ndef replace_categorical_feature(df):\n    df['LotShape'].replace({'Reg':1, 'IR1':2, 'IR2':3, 'IR3':4}, inplace=True)\n    df['BldgType'].replace({'1Fam':1, '2fmCon':1, 'Duplex':3, 'TwnhsE':4, 'Twnhs':5}, inplace=True)\n    df['LandContour'].replace({ 'Low':1, 'HLS':2, 'Bnk':3, 'Lvl':4}, inplace=True)\n    df['Utilities'].replace({'ELO':1, 'NoSeWa':2, 'NoSewr':3, 'AllPub':4}, inplace=True)\n    df['LandSlope'].replace({'Sev':1, 'Mod':2, 'Gtl':3}, inplace=True)\n    df['ExterQual'].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    df['ExterCond'].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    df['BsmtQual'].replace({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    df['BsmtCond'].replace({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    df['BsmtExposure'].replace({'None':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4}, inplace=True)\n    df['BsmtFinType1'].replace({'None':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}, inplace=True)\n    df['BsmtFinType2'].replace({'None':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}, inplace=True)\n    df['HeatingQC'].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    df['KitchenQual'].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    df['Functional'].replace({'Sal':1, 'Sev':2, 'Maj2':3, 'Maj1':4, 'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8}, inplace=True)\n    df['FireplaceQu'].replace({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    df['GarageFinish'].replace({'None':0, 'Unf':1, 'RFn':2, 'Fin':3}, inplace=True)\n    df['GarageQual'].replace({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    df['GarageCond'].replace({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    df['PoolQC'].replace({'None':0, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    df['PavedDrive'].replace({'N':1, 'P':2, 'Y':3}, inplace=True)\n\n    df['HasGarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['HasBasement'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n\nreplace_categorical_feature(all_features)\n\n","331d44ee":"classify_numerical_feature(all_features)","96031212":"# Create interesting features\n\nall_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\nall_features['HasWoodDeck'] = (all_features['WoodDeckSF'] == 0) * 1\nall_features['HasOpenPorch'] = (all_features['OpenPorchSF'] == 0) * 1\nall_features['HasEnclosedPorch'] = (all_features['EnclosedPorch'] == 0) * 1\nall_features['Has3SsnPorch'] = (all_features['3SsnPorch'] == 0) * 1\nall_features['HasScreenPorch'] = (all_features['ScreenPorch'] == 0) * 1\nall_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\nall_features = all_features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\nall_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\n\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\nall_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nall_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['GarageCars'] = all_features['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\nall_features['LotFrontage'] = all_features['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\nall_features['MasVnrArea'] = all_features['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\nall_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\nall_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","54134e81":"# Feature transformations\ndef logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\nall_features = logs(all_features, log_features)","7c8634ea":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n              'GarageCars_log', 'GarageArea_log']\nall_features = squares(all_features, squared_features)","f73a76e4":"# Encode categorical features\nall_features = pd.get_dummies(all_features).reset_index(drop=True)\n\nprint('Final data shape:', all_features.shape)\n\nall_features.head()","2be50e60":"X = all_features.iloc[:len(train_labels), :]\ny = train_labels\ntest = all_features.iloc[len(train_labels):, :]\nX.shape, y.shape, test.shape","1be350f7":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\nprint('Shape')\nprint('X_train: ' + str(X_train.shape))\nprint('X_test: ' + str(X_test.shape))\nprint('y_train: ' + str(y_train.shape))\nprint('y_test: ' + str(y_test.shape))","726acbf2":"#Train the model\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score","ca5ba91b":"#Train the model\nfrom sklearn import linear_model\nlr = linear_model.LinearRegression()\n\n#Fit the model\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n\n#Score\/Accuracy\nprint(\"Accuracy --> \", lr.score(X_test, y_test)*100)","860b4573":"def plot_test_predict(test, predict):\n  plt.figure(figsize=(10,10))\n  plt.scatter(test, predict, c='crimson')\n  plt.yscale('log')\n  plt.xscale('log')\n\n  p1 = max(max(predict), max(test))\n  p2 = min(min(predict), min(test))\n  plt.plot([p1, p2], [p1, p2], 'b-')\n  plt.xlabel('True Values', fontsize=15)\n  plt.ylabel('Predictions', fontsize=15)\n  plt.axis('equal')\n  plt.show()","69eb3b99":"plot_test_predict(y_test, y_pred_lr)","0da79a2a":"#Train the model\nfrom sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor(n_estimators=250, max_depth=3)\n\n#Fit\nGBR.fit(X_train, y_train)\ny_pred_GBR = GBR.predict(X_test)\nprint(\"Accuracy --> \", GBR.score(X_test, y_test)*100)","e49590e6":"plot_test_predict(y_test, y_pred_GBR)","ba3a4574":"submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission.shape","ae7546ab":"test.shape","096d2ce9":"\n# D\u1ef1 b\u00e1o v\u1edbi t\u1ea1p test\npredict_df = GBR.predict(test)\nmy_submission = pd.DataFrame({'Id': submission['Id'], 'SalePrice': predict_df})\nmy_submission.head()","57a93947":"my_submission.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)","9546757f":"### Feature selecting","d30607c4":"In this initial investigations on data will be performed to to develop an understanding of the data, discover patterns and spot anomalies.","850a0b7a":"we can see the most corelated parameters in numerical values above plotting. And we can pick these as features for our macine learning model.","1b363a56":"PoolQC, MiscFeature, Alley, Fence has more 80% missing value. It mean no pool, fence...","f6f2030b":"1. Exploratory Data Analysis\n\nTo Find out the below stuff\n- 1.1.  Missing Values\n- 1.2. All The Numerical Variables\n- 1.3. Distribution of the Numerical Variables\n- 1.4. Categorical Variables\n- 1.5. Cardinality of Categorical Variables\n- 1.6. Outliers\n- 1.7. Relationship between independent and dependent feature(SalePrice)","47311663":"## Data cleaning","814de87d":"## 1.2 Numerical Variables","26da0cb9":"The SalePrice is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) tranform to fix the skew.","046f682e":"## Gradient Boosting Regressor","f5d4143c":"https:\/\/github.com\/krishnaik06\/Advanced-House-Price-Prediction-\/blob\/master\/Exploratory%20Data%20Analysis%20Part%201.ipynb","bb4cac73":"### Predict with test.csv","db0251d3":"- LotFrontage has 16% null\n- Other numerical_feature fill 0","d4574eca":"- Alley: NA: No alley access\n- FireplaceQu: NA: No Fireplace\n- GarageType: NA: No Garage\n- GarageFinish: NA: No Garage\n- GarageQual: NA: No Garage\n- GarageCond: NA: No Garage\n- PoolQC: NA: No Pool\n- Fence: NA: No Fence\n- MiscFeature: NA: None\n- Bsmt: NA: No Basement\n","4d4b1350":"## 1.1. Missing values","b814be89":"## Linear Regression"}}