{"cell_type":{"0a87a4e1":"code","d932d39f":"code","3f15b2a9":"code","c0f2872d":"code","879dcdf5":"code","43a83ab4":"code","2a8d1599":"code","6d7ff135":"code","1fd9082d":"code","d0d54967":"code","eff9e6f4":"code","5ec6a257":"code","fe4025d4":"code","c84b1611":"code","edd8b359":"code","fda37d35":"code","8c412ca7":"code","7f8c3d34":"code","5fe322e6":"code","0e191fc0":"code","760ac5e3":"code","43a35d55":"code","00fa9d5a":"code","1ed196ac":"code","6b4a4028":"code","bc503ccb":"markdown","1852955c":"markdown","9ff2d35d":"markdown","a724eecc":"markdown","60fe6bc7":"markdown","52824d9c":"markdown","6050a4a7":"markdown","c853a9b7":"markdown","5f57857f":"markdown","8f2a824e":"markdown","6f570e84":"markdown"},"source":{"0a87a4e1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nimport pandas_profiling\n%matplotlib inline\n\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\nimport sklearn.metrics as metrics\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d932d39f":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv')","3f15b2a9":"train.head()","c0f2872d":"cont = [c for c in train.columns if \"cont\" in c]\ntrain_cont = train[cont]","879dcdf5":"fig = plt.figure(figsize=(18,16))\n\nfor index,col in enumerate(train_cont):\n    plt.subplot(5,3,index+1)\n    sns.distplot(train_cont.loc[:,col], kde=False)\nfig.tight_layout(pad=1.0)","43a83ab4":"cat = [ca for ca in train.columns if \"cat\" in ca]\ntrain_cat = train[cat]","2a8d1599":"fig = plt.figure(figsize=(18,16))\n\nfor index,col in enumerate(train_cat):\n    plt.subplot(5,3,index+1)\n    sns.countplot(y=col, data=train)\nfig.tight_layout(pad=1.0)","6d7ff135":"for c in train_cont.columns:\n        fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n        sns.boxplot(y=c, data=train_cont, ax=axs[0]) # 1\n\n        sns.violinplot(y=c, data=train_cont, ax=axs[1]) # 2\n\n        sns.stripplot(y=c, data=train_cont, size=4, color=\".3\", linewidth=0, ax=axs[2]) # 3\n\n\n        fig.suptitle(c, fontsize=15, y=1.1)\n        axs[0].set_title('Box Plot')\n        axs[1].set_title('Violin Plot')\n        axs[2].set_title('Strip Plot')\n\n        plt.tight_layout()\n        plt.show()","1fd9082d":"from sklearn.preprocessing import StandardScaler\ns = StandardScaler() \ntrain_scaled = s.fit_transform(train[cont])\ntest_scaled = s.transform(test[cont])","d0d54967":"X_scaled_df = pd.DataFrame(train_scaled,columns=train[cont].columns)\nX_scaled_test_df = pd.DataFrame(test_scaled,columns=test[cont].columns)","eff9e6f4":"from sklearn.mixture import GaussianMixture\n\ndef k_selection(data,feat):\n    name = str(feat)\n    print(name)\n    gms_per_k = [GaussianMixture(n_components=k, n_init=7, random_state=42).fit(data.values.reshape(-1, 1))\n             for k in range(1, 9)]\n    \n    bics = [model.bic(data.values.reshape(-1, 1)) for model in gms_per_k]\n    aics = [model.aic(data.values.reshape(-1, 1)) for model in gms_per_k]\n    \n    #silhouette_scores = [silhouette_score(X, model.labels_)\n    #                 for model in gms_per_k[1:]]\n\n\n    plt.figure(figsize=(8, 3))\n    plt.plot(range(1, 9), bics, \"bo-\", label=\"BIC\")\n    plt.plot(range(1, 9), aics, \"go--\", label=\"AIC\")\n    plt.xlabel(\"$k$\", fontsize=14)\n    plt.ylabel(\"Information Criterion\", fontsize=14)\n    plt.axis([1, 9.5, np.min(aics) - 50, np.max(aics) + 50])\n    \n    plt.legend()\n    plt.savefig(f\"aic_bic_vs_k_plot_{name}.png\")\n    plt.show()","5ec6a257":"#%%time\n#for col in cont:\n#    k_selection(X_scaled_df[col],col)","fe4025d4":"from sklearn.mixture import GaussianMixture\ndef get_gmm_class_feature(feat, n):\n    gmm = GaussianMixture(n_components=n, random_state=42)\n\n    gmm.fit(X_scaled_df[feat].values.reshape(-1, 1))\n\n    train[f'{str(feat)}_class'] = gmm.predict(X_scaled_df[feat].values.reshape(-1, 1))\n    test[f'{str(feat)}_class'] = gmm.predict(X_scaled_test_df[feat].values.reshape(-1, 1))\n\nfor col in cont:\n    get_gmm_class_feature(col,8)","c84b1611":"from sklearn.preprocessing import LabelEncoder\n\nfor col in train_cat:\n    le = LabelEncoder()\n    le.fit(train[col])\n    train[col] = le.transform(train[col])\n    test[col] = le.transform(test[col])","edd8b359":"X_train = train.drop(['id','target'],axis=1)\ny_train = train['target']","fda37d35":"import optuna\nfrom sklearn.model_selection  import KFold\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\n\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n        'max_depth': trial.suggest_int('max_depth', 6, 13),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.10),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 0.05),\n        'objective':'reg:squarederror',\n        'eval_metric' : 'rmse',\n        'tree_method':'gpu_hist',\n       }\n        \n    clf = xgb.XGBRegressor(**params)\n    rmse_scores = []\n    X_train_k = X_train.values\n    y_train_k = y_train.values\n    skf = KFold(n_splits=5,shuffle=True, random_state=2001)\n    for train_idx, valid_idx in skf.split(X_train_k,y_train_k):\n        \n        clf.fit(X_train_k[train_idx, :], y_train_k[train_idx],eval_set=[(X_train_k[train_idx, :], y_train_k[train_idx]), (X_train_k[valid_idx, :], y_train_k[valid_idx])], eval_metric='rmse',\n            verbose=100, early_stopping_rounds=100)\n        pred = clf.predict(X_train_k[valid_idx, :])\n        rmse = np.sqrt(mean_squared_error(y_train_k[valid_idx],pred))\n        rmse_scores.append(rmse)\n    print(f'Trial done: Accuracy values on folds: {rmse_scores}')\n    return np.average(rmse_scores)\n#  Just for lesser time I've used less trials,Please do increase the trials \nn_trials = 10\n\nFIT_XGB = False\n\nif FIT_XGB:\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=n_trials)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","8c412ca7":"best_param_1 = {'n_estimators': 674, 'max_depth': 6, 'learning_rate': 0.0834557625148096,\n 'subsample': 0.7835176160094526, 'colsample_bytree': 0.889939455423845, 'gamma': 0}\nbest_param_2 = {'n_estimators': 442, 'max_depth': 6, 'learning_rate': 0.09530143944404941, \n                'subsample': 0.8592488449341857, \n                'colsample_bytree': 0.7800448158708977, 'gamma': 0,\n                'objective':'reg:squarederror',\n                'eval_metric' : 'rmse',\n                'tree_method':'gpu_hist' }","7f8c3d34":"X = X_train\ny = y_train \ncolumns = X_train.columns\nmodels = []\nfeature_importance = pd.DataFrame()\nscores = []\nimport time","5fe322e6":"folds = KFold(n_splits=10, shuffle=True, random_state=2001)\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n    print(f'Fold {fold_n} started at {time.ctime()}')\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    model = xgb.XGBRegressor(**best_param_2, n_jobs = -1)\n    model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n            verbose=100, early_stopping_rounds=200)\n    pred = model.predict(X_valid)\n    score = np.sqrt(mean_squared_error(y_valid,pred))\n    \n    models.append(model)\n    scores.append(score)\n\n    fold_importance = pd.DataFrame()\n    fold_importance[\"feature\"] = columns\n    fold_importance[\"importance\"] = model.feature_importances_\n    fold_importance[\"fold\"] = fold_n + 1\n    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)","0e191fc0":"feature_importance[\"importance\"] \/= 1\ncols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\nplt.figure(figsize=(16, 12));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\nplt.title('LGB Features (avg over folds)');","760ac5e3":"lgb_params = {\n    \"objective\": \"regression\",\n    \"metric\": \"root_mean_squared_error\",\n    \"verbosity\": -1,\n    \"learning_rate\": 0.001,\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0 \n}","43a35d55":"#import optuna.integration.lightgbm as lgb\n\n#lgb_data = lgb.Dataset(X, y)\n\n#folds = KFold(n_splits=10, shuffle=True, random_state=2001)\n#LGB_train = False\n\n#if LGB_train:\n#    tuner_cv = lgb.LightGBMTunerCV(lgb_params,lgb_data, num_boost_round=1000, early_stopping_rounds=100, folds=folds, verbose_eval=100)\n#    tuner_cv.run()","00fa9d5a":"prediction = pd.DataFrame(columns=['target'])\nprediction['target'] = np.zeros(len(test))","1ed196ac":"for model in models[-9:]:\n    prediction['target'] = prediction['target'] + model.predict(test.drop('id',axis=1))\nprediction['target'] \/= len(models) - 1","6b4a4028":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\":prediction['target']\n    })\nsubmission.to_csv('my_submission.csv', index=False)","bc503ccb":"# LGBM Updating","1852955c":"## Outliers search","9ff2d35d":"# Tuning","a724eecc":"# Encoding","60fe6bc7":"# Categorical","52824d9c":"# Gaussian Mixture (FeatureEng)","6050a4a7":"# Scaling ","c853a9b7":"# Continuous","5f57857f":"# Training","8f2a824e":"# Data","6f570e84":"# Submission"}}