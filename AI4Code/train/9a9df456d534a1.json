{"cell_type":{"2a572640":"code","cfb7a0e4":"code","15e770e9":"code","00af45f6":"code","43959d21":"code","71cb5c9c":"code","c5038ba7":"code","ae1e9b65":"code","1192f5a5":"markdown","66cedd79":"markdown","72794879":"markdown","8183c275":"markdown","7d4ee087":"markdown","3ed81ac1":"markdown","67505f02":"markdown","6b83f54b":"markdown"},"source":{"2a572640":"import pandas as pd\n\n# Read the data\nX = pd.read_csv('..\/input\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/test.csv', index_col='Id')","cfb7a0e4":"X.head()","15e770e9":"X.describe()","00af45f6":"X.columns","43959d21":"from sklearn.model_selection import train_test_split\n\n# Remove rows with missing target, separate target from predictions\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n#Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y,\n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)","71cb5c9c":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and\n                       X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","c5038ba7":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Define the model\nmodel = XGBRegressor(random_state=0, n_estimators=1000, learning_rate=0.05)\n\n# Fit the model\nmodel.fit(X_train, y_train,\n            early_stopping_rounds=5,\n            eval_set=[(X_valid, y_valid)],\n            verbose=False)\n\n# Get predictions\npredictions = model.predict(X_valid)\n\n# Calculate MAE\nmae = mean_absolute_error(predictions, y_valid)\n\nprint(\"Mean Absolute Error:\", mae)","ae1e9b65":"# make predictions which I will submit. \ntest_preds = model.predict(X_test)\n\n# save predictions in format used for competition scoring\n\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","1192f5a5":"That's actually a lot of data. We got over 80 columns. Since the name doesn't explain that much it makes sence to have a look at the Data Description the dataset luckily provides us with. This is relevant in case for the next steps when I'll feed the model with the prediction features.","66cedd79":"### Extreme Gradient Boosting (XGB)\nWe will use now the XGBoost Library. XGBoost stands for extreme gradient boosting.\n\n**Gradient boosting** is \"a method that goes through cycles to iteratively add models into an ensemble\".\nFirst it uses a naive model which it uses to make predictions. Then it calculates the loss, trains the new model and adds this new model to the ensemble before starting the circle ones again.","72794879":"In the next step, we change the dataset to a set of low cardinality cols as well as numericals in order for our model to be able to work with it. After we got those columns we will one-hot encode them. Remember: One-Hot encoding expands the dataset, that's why it's necessary to limit it to low cardinality cols.","8183c275":"# House Prices - Advanced Regression Techniques\n\nThis machine learning model is based on the Kaggle \"Intro to Machine Learning\" and \"Intermediate Machine Learning\" courses. It's highly recommended for newbies like me.\n\nMy goal in this competition is to predict the final price for each home in Ames, Iowa (US). For that I have a dataset with \"79 explanatory variables describing (almost) every aspect of residential homes\".\n\nLet's get it on!","7d4ee087":"The results look pretty good! Let's continue.\n\n# Make Predictions\nAt the beginning I inserted the train.csv. Now I'm gonna using the test.csv data and apply our model to make predictions on the complete test data.","3ed81ac1":"## Build model\nSince in the ML Kaggle course we learned about Decision Trees with finding the best leaf_nodes as well as Random Forests and Extreme Gradient Boosting (XGB), I'll use the latest two.","67505f02":"## Basic Data Exploration\nBefore jumping in building the model, I actually want to get to know it better.","6b83f54b":"## Get dataset ready\nIn order to do so, we will split our target from the columns we'll use for predicting. Afterwards we'll split our train data into test and validation. "}}