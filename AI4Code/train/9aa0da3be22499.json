{"cell_type":{"ee869b70":"code","95ff3e62":"code","f8296a2a":"code","22007bb9":"code","9ea5f2ba":"code","fc90f29b":"code","758f0c8f":"code","6e84f8ca":"code","f0486158":"code","6d2eb17e":"code","aa76da25":"code","3ca0da2e":"code","0db8c743":"code","d66e3696":"code","673a7785":"code","28a6a83a":"code","05efdca3":"code","2bd068a6":"code","08bf5fa5":"code","54494c13":"code","e70a81fa":"code","bb369597":"code","67542576":"code","f5ab7f29":"code","569f51ff":"code","ff7f48b4":"code","b75cc850":"code","475826f7":"code","90056bc1":"code","d2c4f505":"code","d78fdb6d":"code","6d60134d":"code","0ac4edea":"code","52528d4c":"code","f89f1f29":"code","adba67e1":"markdown","9b639eb1":"markdown","e7387913":"markdown","f6c16ede":"markdown","9b1c9b42":"markdown","2b82b0a5":"markdown","f1756770":"markdown","49291b50":"markdown","d8b03779":"markdown","18d05c36":"markdown","969accf4":"markdown","24fd79c8":"markdown","577fcd4a":"markdown","9f9c926e":"markdown","04031d01":"markdown","4b578a1c":"markdown","aed8bc1a":"markdown","7e7c8397":"markdown"},"source":{"ee869b70":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing,svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom collections import Counter","95ff3e62":"train=pd.read_csv('\/kaggle\/input\/loan-eligibility\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/loan-eligibility\/test.csv')","f8296a2a":"train.head(20)","22007bb9":"train.info()","9ea5f2ba":"train.describe()","fc90f29b":"print(Counter(train['Loan_Status']))","758f0c8f":"train=train.drop('Loan_ID',axis=1)\ntest=test.drop('Loan_ID',axis=1)\ntrain.head()","6e84f8ca":"cat_features=[feature for feature in train.columns if train[feature].dtype == 'object']\ncat_features.remove('Loan_Status')\ncat_features","f0486158":"num_features=[feature for feature in train.columns if train[feature].dtype != 'O']\nnum_features","6d2eb17e":"con_features = ['ApplicantIncome','CoapplicantIncome','LoanAmount']\ndiscrete_features = ['Loan_Amount_Term','Credit_History']","aa76da25":"for feature in ['ApplicantIncome','CoapplicantIncome','LoanAmount']:\n    print(feature,'\\nMean \\n',train.groupby('Loan_Status')[feature].mean(),'\\n')\n    print('Median \\n',train.groupby('Loan_Status')[feature].median(),'\\n')","3ca0da2e":"for i,feature in enumerate(cat_features):\n    print(train.groupby([feature,'Loan_Status'])['Loan_Status'].count())","0db8c743":"for i,feature in enumerate(cat_features):\n    plt.figure(i)\n    sns.countplot(x=feature,data=train,hue='Loan_Status')","d66e3696":"for i,feature in enumerate(con_features):\n    plt.figure(i)\n    #plt.hist(train[feature])\n    sns.boxplot(train[feature])","673a7785":"for feature in cat_features:\n    train[feature]=train[feature].fillna(train[feature].mode()[0])\n    test[feature]=test[feature].fillna(test[feature].mode()[0])   ","28a6a83a":"for feature in num_features:\n    train[feature]=train[feature].fillna(train[feature].mean())\n    test[feature]=test[feature].fillna(test[feature].mean())","05efdca3":"train.info()","2bd068a6":"#### Encoding categrical Features: ##########\n\ndict_1 = {'Urban':3, 'Semiurban':2 , 'Rural':1}\ndict_2 = {'0':0,'1':1,'2':2,'3+':3}\n\n\ntrain['Property_Area'] = train['Property_Area'].map(dict_1)\ntest['Property_Area'] = test['Property_Area'].map(dict_1)\n\ntrain['Dependents'] = train['Dependents'].map(dict_2)\ntest['Dependents'] = test['Dependents'].map(dict_2)\n\ntrain['Education'] = train['Education'].map({'Graduate':1,'Not Graduate':0})\ntest['Education'] = test['Education'].map({'Graduate':1,'Not Graduate':0})\n\ntrain = pd.get_dummies(train,drop_first=True)\ntest = pd.get_dummies(test,drop_first=True)\n\ntrain['Total_income'] = train['ApplicantIncome']+train['CoapplicantIncome']\ntest['Total_income'] = test['ApplicantIncome']+test['CoapplicantIncome']","08bf5fa5":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif,chi2","54494c13":"X=train.copy()\ny=X['Loan_Status_Y']\nX.drop(['Loan_Status_Y'],axis=1,inplace=True)\nX.head()","e70a81fa":"train['LoanAmount'] = np.log(train['LoanAmount'])\ntest['LoanAmount'] = np.log(test['LoanAmount'])","bb369597":"X= X[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n       'Loan_Amount_Term', 'Credit_History', 'Education',\n       'Total_income','Dependents','Property_Area']]\ntest= test[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n       'Loan_Amount_Term', 'Credit_History', 'Education',\n       'Total_income','Dependents','Property_Area']]","67542576":"scale=preprocessing.StandardScaler()\nX=scale.fit_transform(X)\ntest=scale.transform(test)","f5ab7f29":"X_train,X_eval,y_train,y_eval=train_test_split(X,y,test_size=0.2,random_state=7,stratify =y)","569f51ff":"svc=svm.SVC(probability=True)\nsvc.fit(X_train,y_train)\nacc=svc.score(X_eval,y_eval)\nacc1=svc.score(X_train,y_train)\nprint(acc1,acc)\ntrain_preds=svc.predict(X_train)\ntest_preds=svc.predict(X_eval)\nprint('log loss\\n', metrics.log_loss(y_eval,test_preds))\nprint(\"cost of training model\\n\",metrics.confusion_matrix(y_train,train_preds))\nprint(\"cost of testing model\\n\",metrics.confusion_matrix(y_eval,test_preds))\nprint(\"cost of training model\\n\",metrics.classification_report(y_train,train_preds))\nprint(\"cost of testing model\\n\",metrics.classification_report(y_eval,test_preds))\ny_test1=svc.predict(test)","ff7f48b4":"y_eval_probs = svc.predict_proba(X_eval)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_eval , y_eval_probs)\nauc = metrics.roc_auc_score(y_eval, y_eval_probs)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","b75cc850":"ran=RandomForestClassifier(n_estimators=25,min_samples_split=5)\nran.fit(X_train,y_train)\nacc=ran.score(X_eval,y_eval)\nacc1=ran.score(X_train,y_train)\nprint(acc1,acc)\ntrain_preds=ran.predict(X_train)\ntest_preds=ran.predict(X_eval)\nprint('log loss\\n', metrics.log_loss(y_eval,test_preds))\nprint(\"cost of training model\\n\",metrics.confusion_matrix(y_train,train_preds))\nprint(\"cost of testing model\\n\",metrics.confusion_matrix(y_eval,test_preds))\nprint(\"cost of training model\\n\",metrics.classification_report(y_train,train_preds))\nprint(\"cost of testing model\\n\",metrics.classification_report(y_eval,test_preds))\ny_test1=ran.predict(test)","475826f7":"y_eval_probs = ran.predict_proba(X_eval)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_eval , y_eval_probs)\nauc = metrics.roc_auc_score(y_eval, y_eval_probs)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","90056bc1":"clf=LogisticRegression()\nclf.fit(X_train,y_train)\neval_score=clf.score(X_eval,y_eval)\ntrain_score=clf.score(X_train,y_train)\nprint('train data score\\n',train_score,'\\ntest data score\\n',eval_score)\n\ntrain_preds=clf.predict(X_train)\ntest_preds=clf.predict(X_eval)\nprint(\"confusion matrix of training data\\n\",metrics.confusion_matrix(y_train,train_preds))\nprint(\"confusion matrix of testing data\\n\",metrics.confusion_matrix(y_eval,test_preds))\nprint(\"classification report of training data\\n\",metrics.classification_report(y_train,train_preds))\nprint(\"classification report of testing data\\n\",metrics.classification_report(y_eval,test_preds))\ny_test1=clf.predict(test)","d2c4f505":"svm_params={'C':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,25],\n        'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n           'gamma' : ['scale','auto']}\nsvc=svm.SVC()\nsvm_tun=RandomizedSearchCV(svc,svm_params,scoring='roc_auc',cv=5)\nsvm_tun.fit(X_train,y_train)\nprint(svm_tun.best_params_)\nprint(svm_tun.best_score_)","d78fdb6d":"svc=svm.SVC(kernel= svm_tun.best_params_['kernel'], gamma= 'auto', C= svm_tun.best_params_['C'],probability=True)\nsvc.fit(X_train,y_train)\nacc=svc.score(X_eval,y_eval)\nacc1=svc.score(X_train,y_train)\nprint(acc1,acc)\ntrain_preds=svc.predict(X_train)\ntest_preds=svc.predict(X_eval)\nprint('log loss\\n', metrics.log_loss(y_eval,test_preds))\nprint(\"cost of training model\\n\",metrics.confusion_matrix(y_train,train_preds))\nprint(\"cost of testing model\\n\",metrics.confusion_matrix(y_eval,test_preds))\nprint(\"cost of training model\\n\",metrics.classification_report(y_train,train_preds))\nprint(\"cost of testing model\\n\",metrics.classification_report(y_eval,test_preds))\ny_test1=svc.predict(test)","6d60134d":"y_eval_probs = svc.predict_proba(X_eval)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_eval , y_eval_probs)\nauc = metrics.roc_auc_score(y_eval, y_eval_probs)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","0ac4edea":"ran_params={'n_estimators':[100,75,50,150,125,175,50,200,250,40],\n        'max_depth':[15,18,20,25,30,35,40,45],\n        'min_samples_split':[5,3,4,6,7,8,9,11,4,12],\n           'min_samples_leaf' : [2,3,4,5,6,7,8]}\nran=RandomForestClassifier()\nran_tun=RandomizedSearchCV(ran,ran_params,scoring='roc_auc',cv=5)\nran_tun.fit(X_train,y_train)\nprint(ran_tun.best_params_)\nprint(ran_tun.best_score_)","52528d4c":"ran=RandomForestClassifier(n_estimators=ran_tun.best_params_['n_estimators'],\n                           min_samples_split= ran_tun.best_params_['min_samples_split'],\n                           min_samples_leaf= ran_tun.best_params_['min_samples_leaf'],\n                           max_depth=ran_tun.best_params_['max_depth'])\nran.fit(X_train,y_train)\nacc=ran.score(X_eval,y_eval)\nacc1=ran.score(X_train,y_train)\nprint(acc1,acc)\nran_train_preds=ran.predict(X_train)\nran_test_preds=ran.predict(X_eval)\n\nprint('log loss\\n', metrics.log_loss(y_eval,ran_test_preds))\nprint(\"cost of training model\\n\",metrics.confusion_matrix(y_train,ran_train_preds))\nprint(\"cost of testing model\\n\",metrics.confusion_matrix(y_eval,ran_test_preds))\nprint(\"cost of training model\\n\",metrics.classification_report(y_train,ran_train_preds))\nprint(\"cost of testing model\\n\",metrics.classification_report(y_eval,ran_test_preds))\ny_test1=ran.predict(test)","f89f1f29":"y_eval_probs = ran.predict_proba(X_eval)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_eval , y_eval_probs)\nauc = metrics.roc_auc_score(y_eval, y_eval_probs)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","adba67e1":"Filling categorical features with mode.","9b639eb1":"## Conculsion\nRandom Forest Classifer with hyperparameter tuning has better metrics of all.","e7387913":"Continuous and decrete features","f6c16ede":"A list of categorical features","9b1c9b42":"Exploring each categorical feature and understanding the relationship between each feature and Loan status.                                                                                                                                                                                 \n\n**Observations:**\n\n1) Loan approval is more for Males.\n\n2) There are more married applicants and the percentage of approval seems to be more for married applicants.\n\n3) Applicants with 0 dependents are more and the percentage of approval seems to be higher for applicants with 2 dependents.  \n\n\n4) Non graduate applicants are less compared to graduate applicants and  percentage of approval is less for non graduate applicants.  \n\n\n5) Major fraction of the applicants are not self-employed.                                                                               \n\n\n6) More number of applicants are from semi-urban areas. Percentage of approval is highest for applicants from semi-urban areas and is   least for appicants from rural areas.","2b82b0a5":"## Feature Selection","f1756770":"## Feature Engineering","49291b50":"Se if the target variable is balanced or not.\nDataset is not balanced, we can perform some sampling techniques to balance the dataset. But sampling doesn't always give a better model. In this case we have to perform oversampling and might lead to overfitting.","d8b03779":"Dropping Loan Id as all the values are different which doesn't add any value to the model","18d05c36":"Analyzing loan status for each categorical feature","969accf4":"After using kbest technique and correlation with target, following are best features.","24fd79c8":"Preprocessing the data","577fcd4a":"## Hyperparameter Tuning","9f9c926e":"Even though there are outliers in numerical features, it's better  to fill nulls with mean rather than median. This would preserve the distribution on the data and would work perfectly fine even if a new input is an outlier. If we fill nulls with median the model would not work well for inputs which actually contain outliers.","04031d01":"A list of numerical features","4b578a1c":"Encoding ordinal and nominal features with label encoding and one hot encoding respectively.","aed8bc1a":"## **Data Analysis**","7e7c8397":"Analyzing mean and median applicant income,coapplicant income  and loan amount and how it effects the loan status."}}