{"cell_type":{"d8f60094":"code","ada7da65":"code","ab81ad36":"code","1e6ad828":"code","66feab93":"code","cea6bdf2":"code","9bd6d7fd":"code","93cfcf7f":"code","a9ee0689":"code","153a4cae":"code","8176dc6e":"code","b2889ca2":"code","f4044aaf":"code","d9342817":"code","88fbca93":"code","78ef3a62":"code","d6925640":"code","f8e1b9f3":"code","19767e75":"code","22b0daba":"code","ae5941ea":"code","3e77b3ed":"code","6c2e5155":"code","c953f8b4":"code","eb4ce12a":"code","3f2cd41c":"code","34d5e8c7":"code","98955aac":"code","6938e237":"code","a96c77a0":"code","8bb364b3":"code","5357f19c":"code","1af7466e":"code","9d8b25ff":"code","bc979014":"code","e2827832":"code","a3677b6a":"code","c9468ead":"code","38829dec":"code","46efa84c":"code","7136ffb9":"code","9b8346a5":"code","5823f8c1":"code","968051ff":"code","c46df41c":"code","8e31384e":"code","54475eb3":"code","e36b2be9":"code","816405f6":"code","6a244984":"code","0e5f6ad9":"code","c8b20828":"code","2b5e58c6":"markdown","70b05ec8":"markdown","491cc2e5":"markdown","b3071d59":"markdown","afc77f67":"markdown","614e832d":"markdown"},"source":{"d8f60094":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.image import imread","ada7da65":"data_dir='..\/input\/pedestrian-no-pedestrian\/data'","ab81ad36":"os.listdir(data_dir)","1e6ad828":"#Set Train and Validation directories\ntrain_path=data_dir+'\/train\/'\nvalidation_path=data_dir+'\/validation\/'","66feab93":"validation_path","cea6bdf2":"os.listdir(validation_path)","9bd6d7fd":"os.listdir(train_path)","93cfcf7f":"os.listdir(train_path+'pedestrian')[5]","a9ee0689":"pedestrian=train_path+'pedestrian\/'+'pic_073.jpg'\nimread(pedestrian).shape","153a4cae":"# First two numbers shows the size of the matrix and the number 3 signifies it has combination of 3 colors (Red,Green,Blue)\n# Let us visualise a sample image\nplt.imshow(imread(pedestrian))","8176dc6e":"# Now let us see one data from no pedestiran dataset\nos.listdir(train_path+'no pedestrian')[5]","b2889ca2":"no_pedestrian=train_path+'no pedestrian\/'+'train (612).jpg'","f4044aaf":"plt.imshow(imread(no_pedestrian))","d9342817":"# let us check the number of images in dataset\nlen(os.listdir(train_path+'pedestrian'))","88fbca93":"len(os.listdir(train_path+'no pedestrian'))","78ef3a62":"len(os.listdir(validation_path+'pedestrian'))\nlen(os.listdir(validation_path+'no pedestrian'))","d6925640":"# So we have 631 images in our train data and 177 images in the validation data\n# Now let us visualaise the average size of the images and try to make all the images in the dimensions \n#for the validation data pedestrians, with a simple for loop we can extract the dimensions\ndim1=[]\ndim2=[]\n\nfor image_filename in os.listdir(validation_path+'pedestrian'):\n    \n    img=imread(validation_path+'pedestrian\/'+image_filename)\n    d1,d2,colors=img.shape\n    dim1.append(d1)\n    dim2.append(d2)","f8e1b9f3":"dim1[0:10]","19767e75":"#we have different image sizes, let us visualise more with a joint plot\nsns.jointplot(dim1,dim2)","22b0daba":"# we have a wide varity of image files are here\nnp.mean(dim1)","ae5941ea":"np.mean(dim2)","3e77b3ed":"# let us explore train data also\ndim1=[]\ndim2=[]\n\nfor image_filename in os.listdir(train_path+'pedestrian'):\n    \n    img=imread(train_path+'pedestrian\/'+image_filename)\n    d1,d2,colors=img.shape\n    dim1.append(d1)\n    dim2.append(d2)","6c2e5155":"sns.jointplot(dim1,dim2)","c953f8b4":"np.mean(dim1)","eb4ce12a":"np.mean(dim2)","3f2cd41c":"# The image sizes are ranging from 180 to 270, to do a Deep learning network we need to standardize the images\n# Here I am going to standardize images to (200,200)\nimage_shape=(200,200,3)","34d5e8c7":"# we can use image generator to manipulate the images and make it ready for our network\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","98955aac":"imread(pedestrian).max()","6938e237":"#maximum value in the cells greater than 1, so we will reshape it in range 0 to 1\n# Here we are not doing much since rotation and flippings will not useful for this datatype\nimage_gen=ImageDataGenerator(rescale=1\/255,shear_range=0.1,zoom_range=0.1,fill_mode='nearest')","a96c77a0":"image_gen.flow_from_directory(train_path)","8bb364b3":"image_gen.flow_from_directory(validation_path)","5357f19c":"# We will stop preprocessing for now, here we can try so much if we want to improve the accurcy of the model","1af7466e":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Dropout,Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping","9d8b25ff":"# Now let us build our model\nmodel=Sequential()\n\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=image_shape,activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),input_shape=image_shape,activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),input_shape=image_shape,activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128,activation='relu')) #same as adding activation in 2nd para\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","bc979014":"model.summary()","e2827832":"early_stop=EarlyStopping(monitor='val_loss',patience=2)","a3677b6a":"# Batch size is selected to specify how many images are flowing through this network at a time\nbatch_size=16","c9468ead":"train_image_gen = image_gen.flow_from_directory(train_path,target_size=image_shape[:2],color_mode='rgb',\n                                                batch_size=batch_size,class_mode='binary')","38829dec":"val_image_gen = image_gen.flow_from_directory(validation_path,target_size=image_shape[:2],color_mode='rgb',\n                                               batch_size=batch_size,class_mode='binary',shuffle=False)","46efa84c":"train_image_gen.class_indices","7136ffb9":"results=model.fit_generator(train_image_gen,epochs=15,validation_data=val_image_gen,callbacks=[early_stop])","9b8346a5":"# Let us visualise the performance\nlosses=pd.DataFrame(model.history.history)\nlosses.plot()","5823f8c1":"# We can see that our accuracy goes up with each epoch and loss come down, and our early callback functioned before overfitting the data","968051ff":"# Our Final Accuracy is\nmodel.evaluate_generator(val_image_gen)","c46df41c":"# Let us predict the images\npred=model.predict_generator(val_image_gen)","8e31384e":"# our pred will be probability values\npred[:5]","54475eb3":"# Let us convert this to predict classes, here I assume if the probability is greater than 0.5 it will be in pedestrian \n# If the probability is less than 0.5 it will be no pedestrian\npredictions=pred>0.5","e36b2be9":"#predictions will be a boolean array\npredictions[:5]","816405f6":"#our validation data looks like this\nval_image_gen.classes","6a244984":"# Let us evaluate our model\nfrom sklearn.metrics import classification_report,confusion_matrix","0e5f6ad9":"print(classification_report(val_image_gen.classes,predictions))","c8b20828":"confusion_matrix(val_image_gen.classes,predictions)","2b5e58c6":"## Visualisation","70b05ec8":"## Model Creation","491cc2e5":"With a relatively simple CNN method we got around 90% accuracy, we can try changing parameter or increase the probabilty threshold to get \nan even better result\nThanks all for reading, if you have any suggetions, please let me know in the comments.","b3071d59":"## Model Evaluation","afc77f67":"## Data Preprocessing","614e832d":"## Importing Libraries"}}