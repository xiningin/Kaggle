{"cell_type":{"344d3012":"code","3a93d90a":"code","b1e6f9ba":"code","753225b2":"code","2f322b27":"code","0f7c9824":"code","35552d85":"code","33b2fc11":"code","fa1d0ebb":"code","e63243bd":"code","367fce41":"code","ba825bd5":"code","be5e7b15":"code","6d6607c4":"code","fa611963":"code","cf69c0d6":"code","05ed4184":"code","d328c785":"code","ff8c2546":"code","7b4c888b":"code","5ea1c141":"code","23f5ae58":"code","eb065f6a":"code","f9e81284":"code","a2632948":"code","acd2379f":"code","4f10904f":"code","8dd807ea":"code","5103a82b":"code","fb80fb92":"code","2e8867a5":"code","62c33c17":"code","a82cd9f5":"code","bb591532":"code","ed66b681":"code","5dc8a338":"code","641d8dc6":"code","df09f213":"code","101095d5":"code","50984c5b":"code","68ea3098":"code","7926201f":"code","840938fc":"code","0b78e239":"code","e9a186f6":"code","d1c6a687":"code","4a7a0d55":"code","0f13f4aa":"code","6e578068":"code","063989ac":"code","5207c1ad":"code","5648aec8":"code","68180795":"code","16ec3e29":"code","5c1e902d":"code","5a696c82":"code","69d9153f":"code","2019769a":"code","acbc062c":"code","b48477a2":"code","0c4b1b10":"code","16b1520e":"code","2ccb8b25":"code","5eccd62e":"code","3e686b4d":"code","e343f257":"code","cba88606":"code","bffb124f":"code","2d770218":"code","91194d70":"code","bf0331f5":"code","8bc06da1":"code","7fa285a1":"code","8c1a9f4c":"code","562961e7":"code","74e2f311":"code","7c081a2c":"code","c4c24e3f":"code","c347e4e6":"code","bac47614":"code","246105eb":"code","50eb9197":"code","375a2117":"code","c69c5b8a":"code","67fae0cd":"code","2a0333ed":"code","4f11cd8b":"code","d16f18e4":"code","9d2d65ec":"code","5d55f837":"code","88027fa7":"code","19dac008":"code","059ed9df":"code","41f141d9":"code","d48ab17c":"code","c23129b5":"markdown","ec1fc280":"markdown","d0d098c7":"markdown","8361f449":"markdown","1c4b88ba":"markdown","d7e7c031":"markdown","acdadd90":"markdown","6d884ee4":"markdown","7d8a80fa":"markdown","85dfc0cc":"markdown","9aef9352":"markdown","b44c646b":"markdown","35cb80c9":"markdown"},"source":{"344d3012":"pip install pydotplus","3a93d90a":"import os\nimport csv, json\n\nimport numpy as np\nimport pandas as pd\nimport featuretools as ft\n\nimport networkx as nx\nimport pydotplus\nimport graphviz\n\npd.set_option('display.max_rows', 690)\npd.set_option('display.max_columns', 690)\npd.set_option('display.width', 6969)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom itertools import combinations","b1e6f9ba":"import warnings\n\nwarnings.filterwarnings('ignore', message=\"^Columns.*\")","753225b2":"from difflib import SequenceMatcher\n\ndef compare_strings(a: str, b: str) -> float:\n    len_a, len_b = len(a), len(b)\n    max_len = max(len_a, len_b)\n    min_len = min(len_a, len_b)\n    \n    sim_full = SequenceMatcher(None, a, b).ratio()\n    sim_half_first = SequenceMatcher(None, a[:min_len], b[:min_len]).ratio()\n    sim_half_last = SequenceMatcher(None, a[-min_len:], b[-min_len:]).ratio()\n    return np.average([sim_full, sim_half_first, sim_half_last])","2f322b27":"def describe_missing_values(df):\n    miss_val = df.isnull().sum()\n    miss_val_percent = 100 * df.isnull().sum() \/ len(df)\n    miss_val_table = pd.concat([miss_val, miss_val_percent], axis=1)\n    miss_val_table_ren_columns = miss_val_table.rename(\n        columns = {\n            0: 'Missing Values', \n            1: '% of Total Values',\n        }\n    )\n    miss_val_table_ren_columns = miss_val_table_ren_columns[\n        miss_val_table_ren_columns.iloc[:,1] != 0\n    ].sort_values('% of Total Values', ascending=False).round(1)\n    \n    print(f\"Your selected dataframe has {df.shape[1]} columns,\")\n    print(f\"\\t{miss_val_table_ren_columns.shape[0]} columns that have missing values.\")\n\n    return miss_val_table_ren_columns","0f7c9824":"def visualize_distribution_of_missing_values(df, df_name: str='df'):\n    df_nan_check = df.isna().sum().sort_values()\n    df_nan_check = df_nan_check.to_dict()\n    df_not_nan = []\n\n    nan_cols = 0\n\n    for key, value in df_nan_check.items():\n        df_nan_check[key] = int(value\/len(df)*100)\n        if df_nan_check[key] >= 80:\n            nan_cols += 1\n        else:\n            df_not_nan.append(key)\n\n    # Visualize\n    plt.figure(figsize=(9, 6))\n    plt.suptitle('Distribution of Empty Values', fontsize=19)\n    plt.bar(df_nan_check.keys(), df_nan_check.values())\n    plt.xticks(rotation=69)\n    # plt.savefig(df_name+'.png', dpi=7*len(df_nan_check.keys()))\n    plt.show()","35552d85":"def save_dict(dictionary: dict, dictionary_name: str='dict'):\n    with open(dictionary_name+'.json', 'w', encoding='utf-8') as writer:\n        writer.write(json.dumps(dictionary, indent=4))","33b2fc11":"def correct_mispell(text: str, \n                    vocab: list or tuple, \n                    threshold: float=0.69, \n                    oov: str='unknown') -> str:\n    for word in vocab:\n        sim = compare_strings(text, word)\n        if sim > threshold:\n            return word\n    return oov","fa1d0ebb":"def clean_data(df: pd.DataFrame, \n               processes: list=[('remove_null', 0.69), \n                                ('remove_single_val', False), \n                                ('remove_low_info',)]) -> pd.DataFrame:\n    print('Original:\\n', df.columns.values.tolist())\n    for process in processes:\n        print('-'*69)\n        if process[0] == 'remove_null':\n            df = ft.selection.remove_highly_null_features(df, pct_null_threshold=process[1])\n        elif process[0] == 'remove_single_val':\n            df = ft.selection.remove_single_value_features(df, count_nan_as_value=process[1])\n        elif process[0] == 'remove_low_info':\n            df = ft.selection.remove_low_information_features(df)\n        else:\n            continue\n        print(f'After {process[0]}:\\n', df.columns.values.tolist())\n    return df","e63243bd":"datetime_milestone = '2000-01-01'\n\ndef datetime2unix(date_df, dt_format: str):\n    date_df = pd.to_datetime(date_df, format=dt_format, errors='coerce')\n    # date_df = (date_df - pd.Timestamp(datetime_milestone)) \/\/ pd.Timedelta('1s')\n    # date_df.fillna(value=-1, inplace=True)\n    return date_df","367fce41":"def category2numerical(table: str, col: str, oov: str='unknown'):\n    DFs[table][col] = DFs[table][col].str.lower()\n    display(DFs[table][col].value_counts())\n    \n    unique_values = DFs[table][col].unique().tolist()\n    unique_values_code = {\n        val: val_id for val_id, val in enumerate(unique_values)\n    }\n    unique_values_code[oov] = len(unique_values_code)\n    \n    print(unique_values_code)\n    save_dict(unique_values_code, f'{table}-{col}')\n    \n    DFs[table][col] = DFs[table][col].apply(\n        lambda x: unique_values_code[x] if x in unique_values_code.keys() else unique_values_code[oov]\n    )\n    DFs[table][col].fillna(value=unique_values_code[oov], inplace=True)","ba825bd5":"prefix = '..\/input\/sii-fcda\/'\nsuffix = '_25112020_0'\nfilenames = [\n    'Final_GroundTruth',\n    'claim_listing', 'claim_submission', 'claimant_listing',\n    'insured', 'intermediary', 'policy'\n]\n\nDFs = dict()\nfor f_i in range(len(filenames)):\n    if f_i >= 1:\n        fn = prefix + filenames[f_i] + suffix + '.csv'\n        DFs[filenames[f_i]] = pd.read_csv(fn, delimiter='|', index_col=False)\n    else:\n        fn = prefix + filenames[f_i] + '.csv'\n        DFs[filenames[f_i]] = pd.read_csv(fn, index_col=False)\n    display(DFs[filenames[f_i]].sample(n=11))","be5e7b15":"for df_name, df in DFs.items():\n    if df_name == 'claim_submission':\n        continue\n    print(df_name)\n    display(describe_missing_values(df))\n    visualize_distribution_of_missing_values(df, df_name)","6d6607c4":"display(DFs['claim_listing'].sample(n=7))\ndisplay(DFs['claim_listing'].describe())","fa611963":"DFs['claim_listing'] = clean_data(DFs['claim_listing'])","cf69c0d6":"# DFs['claim_listing']['claim_closed_yn'].unique() # unique values: ['Y', 'N'] -> binarize\nDFs['claim_listing']['claim_closed_yn'] = np.where(DFs['claim_listing']['claim_closed_yn']=='Y', 1, 0)","05ed4184":"datetime_cols = ['claim_loss_date', 'claim_report_date', 'claim_created_date', 'claim_closed_date']\nfor col in datetime_cols:\n    DFs['claim_listing'][col] = datetime2unix(DFs['claim_listing'][col], dt_format='%d-%m-%Y %H:%M')\n\ndisplay(DFs['claim_listing'][datetime_cols].sample(7))","d328c785":"DFs['claim_listing']['claim_accident_location'] = DFs['claim_listing']['claim_accident_location'].str.lower()\nlocs = sorted(DFs['claim_listing']['claim_accident_location'].unique().tolist())\n\nloc_count = DFs['claim_listing']['claim_accident_location'].value_counts()\ndisplay(loc_count)\n\nrare_locs = loc_count[loc_count<2].index.values.tolist()\nfor rare_loc in rare_locs:\n    DFs['claim_listing']['claim_accident_location'][\n        DFs['claim_listing']['claim_accident_location'] == rare_loc\n    ] = 'unknown'\n\nDFs['claim_listing']['claim_accident_location'].hist(bins=69)","ff8c2546":"locs = DFs['claim_listing']['claim_accident_location'].unique().tolist()\nlocation_code, current_code = dict(), 0\nfor loc in locs:\n    # initialize\n    if len(location_code.keys()) == 0:\n        location_code[loc] = current_code\n        current_code += 1\n        continue\n        \n    # check similar location encoded\n    for loc_, loc_id_ in location_code.items():\n        sim = compare_strings(loc, loc_)\n        if sim > 0.79:\n            location_code[loc] = loc_id_\n            break\n            \n    # if no location is similar, assign new code\n    if loc not in location_code.keys():\n        location_code[loc] = current_code\n        current_code += 1\n\nsave_dict(location_code, 'claim_listing-claim_accident_location')\n# print(location_code)\n\nDFs['claim_listing']['claim_accident_location'] = DFs['claim_listing']['claim_accident_location'].apply(\n    lambda x: location_code[x] if x in location_code.keys() else location_code['unknown']\n)","7b4c888b":"DFs['claimant_listing'] = clean_data(DFs['claimant_listing'])","5ea1c141":"display(DFs['claimant_listing'].sample(n=7))","23f5ae58":"DFs['claimant_listing']['benefit_desc'] = DFs['claimant_listing']['benefit_desc'].str.lower()\n\nbenefit_count = DFs['claimant_listing']['benefit_desc'].value_counts()\ndisplay(benefit_count)\n\nunknown_benefits = benefit_count[benefit_count<2].index.values.tolist()\nfor unk_benefit in unknown_benefits:\n    DFs['claimant_listing']['benefit_desc'][\n        DFs['claimant_listing']['benefit_desc'] == unk_benefit\n    ] = 'unknown'\n\nDFs['claimant_listing']['benefit_desc'].fillna(value='unknown', inplace=True)\ndisplay(DFs['claimant_listing']['benefit_desc'].value_counts())","eb065f6a":"benefits = DFs['claimant_listing']['benefit_desc'].unique().tolist()\nbenefit_code = {\n    benefit: benefit_id for benefit_id, benefit in enumerate(benefits)\n}\nprint(json.dumps(benefit_code, indent=4))\nsave_dict(benefit_code, 'claimant_listing-benefit_desc')\n\nDFs['claimant_listing']['benefit_desc'] = DFs['claimant_listing']['benefit_desc'].apply(\n    lambda x: benefit_code[x] if x in benefit_code.keys() else benefit_code['unknown']\n)","f9e81284":"print('loss_type_desc:', DFs['claimant_listing']['loss_type_desc'].nunique())\nprint('reserve_desc:', DFs['claimant_listing']['reserve_desc'].nunique())\nprint('payee_name_encrypted:', DFs['claimant_listing']['payee_name_encrypted'].nunique())","a2632948":"col = 'reserve_date'\ndisplay(DFs['claimant_listing'][col].sample(n=7))\n\nDFs['claimant_listing'][col] = datetime2unix(DFs['claimant_listing'][col], dt_format='%d-%m-%Y')\ndisplay(DFs['claimant_listing'][col].sample(n=7))","acd2379f":"display(DFs['claimant_listing']['reserve_amount'].describe())","4f10904f":"DFs['claimant_listing'].describe()","8dd807ea":"DFs['claimant_listing'].isna().sum()","5103a82b":"DFs['claimant_listing']['benefit_sum_insured'].fillna(value=-1, inplace=True)","fb80fb92":"DFs['claimant_listing'].dropna(subset=['claimant_id_encrypted'], inplace=True)","2e8867a5":"DFs['claimant_listing'].sample(n=7)","62c33c17":"DFs['policy'] = clean_data(DFs['policy'])\ndisplay(DFs['policy'].sample(n=11))","a82cd9f5":"DFs['policy']['intermediary_classification'].value_counts()","bb591532":"for col in DFs['policy'].columns:\n    if col in ['policy_number_encrypted', 'intermediary_code']:\n        continue\n    \n    # check Not-A-Number\n    nan_ratio = DFs['policy'][col].isna().sum() \/ len(DFs['policy'])\n        \n    # check distinctness\n    distinctness = DFs['policy'][col].nunique()\n    \n    if nan_ratio > 0.69 or distinctness < 2:\n        DFs['policy'].drop(labels=[col], axis='columns', inplace=True)\n\nDFs['policy'].sample(n=7)","ed66b681":"features_bin = ['policy_cancellation_yn', 'policy_renewable_yn', 'policy_renewed_yn']\nfor col in features_bin:\n    DFs['policy'][col] = np.where(DFs['policy'][col]=='Y', 1, 0)\n    DFs['policy'][col].fillna(value=0, inplace=True)\n\nDFs['policy'].sample(n=7)","5dc8a338":"policy_datetime_cols = ['policy_issue_date', 'policy_from_date', 'policy_to_date']\nfor col in policy_datetime_cols:\n    DFs['policy'][col] = datetime2unix(DFs['policy'][col], dt_format='%d-%m-%Y %H:%M')\n    \ndisplay(DFs['policy'][policy_datetime_cols].sample(n=7))","641d8dc6":"display(DFs['policy']['sales_platform'].value_counts())\ncategory2numerical(table='policy', col='sales_platform')\n\ndisplay(DFs['policy'].sample(n=7))","df09f213":"display(DFs['policy']['product_name'].value_counts())\ncategory2numerical(table='policy', col='product_name')\n\ndisplay(DFs['policy'].sample(n=7))","101095d5":"display(DFs['policy']['product_code'].value_counts()) # the same as `product_name` -> drop\nDFs['policy'].drop(\n    labels=['product_code'], \n    axis='columns',\n    inplace=True\n)","50984c5b":"print(DFs['policy']['policyholder_code'].nunique(), '\/', len(DFs['policy']))\n\n# almost records are unique -> drop\n# DFs['policy'].drop(\n#     labels=['policyholder_code'], \n#     axis='columns',\n#     inplace=True\n# )","68ea3098":"DFs['policy']['policyholder_id_type'] = DFs['policy']['policyholder_id_type'].str.lower()\nDFs['policy']['policyholder_id_type'].fillna(value='unknown', inplace=True)\ndisplay(DFs['policy']['policyholder_id_type'].value_counts())\n\npolicyholder_ids = ['passport', 'ktp', 'driving license', 'kitas', 'unknown', 'idcard', 'id']\npolicyholder_id_code = {\n    policy_id: id_ for id_, policy_id in enumerate(policyholder_ids)\n}\npolicyholder_id_code['id'] = policyholder_id_code['idcard']\n\nsave_dict(policyholder_id_code, 'policy-policyholder_id_type')\nprint(policyholder_id_code)\n\n# Correct mispelled type\nDFs['policy']['policyholder_id_type'] = DFs['policy']['policyholder_id_type'].apply(\n    lambda x: correct_mispell(text=x, vocab=policyholder_id_code, threshold=0.69, oov='unknown')\n)\ndisplay(DFs['policy']['policyholder_id_type'].value_counts())\n\nDFs['policy']['policyholder_id_type'] = DFs['policy']['policyholder_id_type'].apply(\n    lambda x: policyholder_id_code[x] if x in policyholder_id_code.keys() else policyholder_id_code['unknown']\n)","7926201f":"display(DFs['policy']['policyholder_id_no_encrypted'].value_counts())\nprint(DFs['policy']['policyholder_id_no_encrypted'].nunique(), '\/', len(DFs['policy']))\n\n# DFs['policy'].drop(\n#     labels=['policyholder_id_no_encrypted'], \n#     axis='columns',\n#     inplace=True\n# )","840938fc":"print(DFs['policy']['policyholder_date_of_birth'].isna().sum() \/ len(DFs['policy']))\n\nDFs['policy'].drop(\n    labels=['policyholder_date_of_birth'], \n    axis='columns',\n    inplace=True\n)","0b78e239":"print(DFs['policy']['policyholder_age'].isna().sum() \/ len(DFs['policy']))\n\ndisplay(DFs['policy']['policyholder_age'].hist(bins=69))\n\npolicyholder_age_distribution = DFs['policy']['policyholder_age'].value_counts(normalize=True, sort=True)\n# display(policyholder_age_distribution)\n\n# Fill NaN by distribution\npolicyholder_ages = policyholder_age_distribution.index.tolist()\npolicyholder_age_probs = policyholder_age_distribution.values.tolist()\nDFs['policy']['policyholder_age'] = DFs['policy']['policyholder_age'].apply(\n    lambda x: x if not pd.isna(x) else np.random.choice(policyholder_ages, p=policyholder_age_probs)\n)","e9a186f6":"feature_name = 'policyholder_gender'\nDFs['policy'][feature_name] = DFs['policy'][feature_name].str.lower()\nDFs['policy'][feature_name] = DFs['policy'][feature_name].apply(\n    lambda x: 1 if x=='male' else 0\n)\nDFs['policy'][feature_name].fillna(value=-1, inplace=True)\ndisplay(DFs['policy'][feature_name].value_counts())\n\ndisplay(DFs['policy'].sample(n=7))","d1c6a687":"feature_name = 'policy_premium_current_year'\ndisplay(DFs['policy'][feature_name].describe())","4a7a0d55":"DFs['policy']['policyholder_id_no_encrypted'].fillna(value='unknown', inplace=True)","0f13f4aa":"DFs['insured'] = clean_data(DFs['insured'])","6e578068":"for col in DFs['insured'].columns:\n    if col in ['policy_number_encrypted']:\n        continue\n    \n    # check Not-A-Number\n    nan_ratio = DFs['insured'][col].isna().sum() \/ len(DFs['insured'])\n        \n    # check distinctness\n    distinctness = DFs['insured'][col].nunique()\n    \n    if nan_ratio > 0.69 or distinctness < 2:\n        DFs['insured'].drop(labels=[col], axis='columns', inplace=True)\n\nDFs['insured'].sample(n=7)","063989ac":"display(DFs['insured']['product_plan_name'].value_counts())\ncategory2numerical(table='insured', col='product_plan_name')\n\ndisplay(DFs['insured'].sample(n=7))","5207c1ad":"feature_name = 'insured_gender'\nDFs['insured'][feature_name] = DFs['insured'][feature_name].str.lower()\nDFs['insured'][feature_name] = DFs['insured'][feature_name].apply(\n    lambda x: 1 if x=='male' else 0\n)\nDFs['insured'][feature_name].fillna(value=-1, inplace=True)\ndisplay(DFs['insured'][feature_name].value_counts())\n\ndisplay(DFs['insured'].sample(n=7))","5648aec8":"DFs['insured']['insured_id_type'] = DFs['insured']['insured_id_type'].str.lower()\nDFs['insured']['insured_id_type'].fillna(value='unknown', inplace=True)\ndisplay(DFs['insured']['insured_id_type'].value_counts())\n\nsave_dict(policyholder_id_code, 'insured-insured_id_type')\n\n# Correct mispelled type\nDFs['insured']['insured_id_type'] = DFs['insured']['insured_id_type'].apply(\n    lambda x: correct_mispell(text=x, vocab=policyholder_id_code, threshold=0.69, oov='unknown')\n)\ndisplay(DFs['insured']['insured_id_type'].value_counts())\n\nDFs['insured']['insured_id_type'] = DFs['insured']['insured_id_type'].apply(\n    lambda x: policyholder_id_code[x] if x in policyholder_id_code.keys() else policyholder_id_code['unknown']\n)","68180795":"DFs['insured'].drop(\n    labels=['insured_date_of_birth'], \n    axis='columns',\n    inplace=True\n)","16ec3e29":"print(DFs['insured']['insured_age'].isna().sum() \/ len(DFs['insured']))\n\ndisplay(DFs['insured']['insured_age'].hist(bins=69))\n\npolicyholder_age_distribution = DFs['insured']['insured_age'].value_counts(normalize=True, sort=True)\n# display(policyholder_age_distribution)\n\n# Fill NaN by distribution\npolicyholder_ages = policyholder_age_distribution.index.tolist()\npolicyholder_age_probs = policyholder_age_distribution.values.tolist()\nDFs['insured']['insured_age'] = DFs['insured']['insured_age'].apply(\n    lambda x: x if not pd.isna(x) else np.random.choice(policyholder_ages, p=policyholder_age_probs)\n)","5c1e902d":"DFs['insured']['insured_id_number_encrypted'].fillna(value='unknown', inplace=True)","5a696c82":"DFs['intermediary'] = clean_data(DFs['intermediary'])\ndisplay(DFs['intermediary'].sample(n=7))","69d9153f":"DFs['intermediary']['tax_applicable_yn'] = np.where(DFs['intermediary']['tax_applicable_yn']=='Y', 1, 0)\nDFs['intermediary']['tax_applicable_yn'].fillna(value=-1, inplace=True)","2019769a":"display(DFs['intermediary'].sample(n=7))","acbc062c":"col = 'intermediary_joint_date'    \nDFs['intermediary'][col] = datetime2unix(DFs['intermediary'][col], dt_format='%d-%m-%Y %H:%M')\ndisplay(DFs['intermediary'].sample(n=7))","b48477a2":"sets = [\n    ('claim_listing', 'claimant_listing'),\n    ('claim_listing', 'policy'),\n    ('policy', 'insured'),\n    ('policy', 'intermediary'),    \n]\nfor ss in combinations(list(DFs.keys()), 2):\n    cols_1 = set(DFs[ss[0]].columns.values.tolist())\n    cols_2 = set(DFs[ss[1]].columns.values.tolist())\n    print(f'{ss[0]} U {ss[1]} =', set.intersection(cols_1, cols_2))","0c4b1b10":"claim_df = DFs['claim_listing'].merge(DFs['claimant_listing'], how='inner', on='claim_no_encrypted')\ndisplay(claim_df.sample(n=11))\nprint(len(claim_df))","16b1520e":"dfs_joined = ['policy', 'insured', 'intermediary']\ncols_joined = ['policy_number_encrypted', 'policy_number_encrypted', ['intermediary_code', 'intermediary_classification']]\nfor df_name, col in zip(dfs_joined, cols_joined):\n    n_records_old = len(claim_df)\n    claim_df = claim_df.merge(DFs[df_name], how='left', on=col)\n    n_records_new = len(claim_df)\n    print(f'{n_records_old} --> {n_records_new} by LEFT JOIN {df_name} ON {col}')\n    \nclaim_df.sample(n=11)","2ccb8b25":"claim_df.duplicated(\n    subset=['claim_no_encrypted', 'claimant_id_encrypted'],\n    keep='first'\n).sum()","5eccd62e":"claim_df.drop_duplicates(\n    subset=['claim_no_encrypted', 'claimant_id_encrypted'],\n    keep='first', \n    inplace=True\n)\nlen(claim_df)","3e686b4d":"claim_df.describe()","e343f257":"describe_missing_values(claim_df)","cba88606":"claim_df.dropna(axis='index', \n                how='any', \n                subset=['claim_no_encrypted', 'claimant_id_encrypted'], \n                inplace=True)","bffb124f":"claim_df['intermediary_joint_date'].sample(3)","2d770218":"claim_df['tax_applicable_yn'].fillna(value=-1, inplace=True)\nclaim_df['intermediary_joint_date'].fillna(value=pd.to_datetime('2021-01-01', format='%Y-%m-%d', errors='coerce'), inplace=True)\n\ndescribe_missing_values(claim_df)","91194d70":"display(claim_df['insured_id_number_encrypted'].describe())\ndisplay(claim_df['insured_id_number_encrypted'].sample(n=11))\n\nclaim_df.drop(\n    labels=['insured_id_number_encrypted'], \n    axis='columns',\n    inplace=True\n)","bf0331f5":"DFs['Final_GroundTruth'].sample(n=7)","8bc06da1":"claim_df[['claim_no_encrypted', 'claimant_id_encrypted']].sample(n=7)","7fa285a1":"labeled_df = DFs['Final_GroundTruth'].copy()\nlabeled_df.sample(n=7)","8c1a9f4c":"labeled_df = labeled_df.merge(claim_df, on=['claim_no_encrypted', 'claimant_id_encrypted'], how='left')\nlabeled_df.sample(n=11)","562961e7":"describe_missing_values(labeled_df)","74e2f311":"print('Before drop NA:', len(labeled_df))\nlabeled_df.dropna(axis='index', how='any', inplace=True)\nprint('After drop NA:', len(labeled_df))","7c081a2c":"labeled_df['class'] = np.where(labeled_df['FinalLabel']=='Y', 1, 0)\nlabeled_df.drop(\n    labels=['FinalLabel', 'claim_no_encrypted', 'claimant_id_encrypted', 'policy_number_encrypted'], \n    axis='columns',\n    inplace=True\n)","c4c24e3f":"labeled_df.columns.tolist()","c347e4e6":"col = 'intermediary_code'\noov = 'unknown'\nlabeled_df[col] = labeled_df[col].str.lower()\n# display(labeled_df[col].value_counts())\n\nunique_values = labeled_df[col].unique().tolist()\nunique_values_code = {\n    val: val_id for val_id, val in enumerate(unique_values)\n}\nunique_values_code[oov] = len(unique_values_code)\n\n# print(unique_values_code)\nsave_dict(unique_values_code, 'intermediary_code')\n\nlabeled_df[col] = labeled_df[col].apply(\n    lambda x: unique_values_code[x] if x in unique_values_code.keys() else unique_values_code[oov]\n)\nlabeled_df[col].fillna(value=unique_values_code[oov], inplace=True)","bac47614":"col = 'intermediary_classification'\ndisplay(labeled_df[col].value_counts())\nintermediary_classes = labeled_df[col].unique().tolist()\nintermediary_class_code = {\n    intermediary_class: class_id for class_id, intermediary_class in enumerate(intermediary_classes)\n}\nintermediary_class_code['unknown'] = len(intermediary_class_code)\nsave_dict(intermediary_class_code, 'intermediary_classification')\n\nlabeled_df[col] = labeled_df[col].apply(\n    lambda x: unique_values_code[x] if x in unique_values_code.keys() else unique_values_code[oov]\n)\nlabeled_df[col].fillna(value=unique_values_code[oov], inplace=True)","246105eb":"labeled_df.describe()","50eb9197":"labeled_df.dtypes","375a2117":"labeled_df.to_csv('sii_fraud.csv', index=False)","c69c5b8a":"labeled_df['class'].sum() \/ len(labeled_df)","67fae0cd":"labeled_df.describe()","2a0333ed":"entity_set = ft.EntitySet(id=\"fraudulent_claims\")","4f11cd8b":"display(labeled_df.sample(n=3))\ndisplay(labeled_df.dtypes)","d16f18e4":"entity_set = entity_set.entity_from_dataframe(\n    entity_id='fraudulent_claims',\n    dataframe=labeled_df,\n    make_index=True, \n    index='claim_id',\n    variable_types={\n        'class': ft.variable_types.Boolean,\n        \n        # claim_listing\n        'claim_closed_yn': ft.variable_types.Boolean,\n        'claim_loss_date': ft.variable_types.Datetime,\n        'claim_report_date': ft.variable_types.Datetime,\n        'claim_closed_date': ft.variable_types.Datetime,\n        'claim_created_date': ft.variable_types.Datetime,\n        'claim_outstanding_amount': ft.variable_types.Numeric,\n        'claim_payment_amount': ft.variable_types.Numeric,\n        'claim_recovery_amount': ft.variable_types.Numeric,\n        'claim_total_incurred': ft.variable_types.Numeric,\n        'claim_accident_location': ft.variable_types.Categorical,\n        'claim_description': ft.variable_types.NaturalLanguage,\n        \n        # claimant_listing\n        'benefit_desc': ft.variable_types.Categorical,\n        'benefit_sum_insured': ft.variable_types.Numeric,\n        'loss_type_desc': ft.variable_types.NaturalLanguage,\n        'reserve_desc': ft.variable_types.NaturalLanguage,\n        'reserve_amount': ft.variable_types.Numeric,\n        'reserve_date': ft.variable_types.Datetime,\n        'payee_name_encrypted': ft.variable_types.Categorical,\n        \n        # insured\n        'product_plan_name': ft.variable_types.Categorical,\n        'insured_id_type': ft.variable_types.Categorical,\n        'insured_age': ft.variable_types.Numeric,\n        'insured_gender': ft.variable_types.Categorical,\n        'tax_applicable_yn': ft.variable_types.Boolean,\n        \n        # policy\n        'policy_issue_date': ft.variable_types.Datetime,\n        'policy_from_date': ft.variable_types.Datetime,        \n        'policy_to_date': ft.variable_types.Datetime,        \n        'sales_platform': ft.variable_types.Categorical,\n        'product_name': ft.variable_types.Categorical,\n        'policyholder_code': ft.variable_types.Categorical,\n        'policyholder_id_type': ft.variable_types.Categorical,\n        'policyholder_id_no_encrypted': ft.variable_types.Categorical,\n        'policyholder_age': ft.variable_types.Numeric,\n        'policyholder_gender': ft.variable_types.Categorical,\n        'policy_premium_current_year': ft.variable_types.Numeric,\n        'policy_cancellation_yn': ft.variable_types.Boolean,\n        'policy_renewable_yn': ft.variable_types.Boolean,\n        'policy_renewed_yn': ft.variable_types.Boolean,\n        \n        # intermediary\n        'intermediary_classification': ft.variable_types.Categorical,\n        'intermediary_code': ft.variable_types.Categorical,\n        'intermediary_joint_date': ft.variable_types.Datetime,\n    }\n)","9d2d65ec":"default_aggregation_primitives = [\n    \"sum\", \"max\", \"min\", \"mode\", \"mean\", \"std\", \"skew\",\n    \"count\", \"percent_true\", \"num_unique\",\n]\n\ndefault_transformation_primitives = [\n    # Numeric\n    \"add_numeric\", \"multiply_numeric\", \"subtract_numeric\", \n    \n    # Datetime\n    \"day\", \"year\", \"month\", \"weekday\", \"is_weekend\", \n\n    # LatLong\n    \"haversine\", \n\n    # NaturalLanguage\n    \"num_words\", \"num_characters\", \n]","5d55f837":"from featuretools.primitives import *\nfrom featuretools.variable_types import *\nfrom featuretools.utils.gen_utils import Library\n\n\nclass GreaterThanMean(TransformPrimitive):\n    \"\"\"\n    Determines if values are greater than MEAN of distribution.\n\n    Description:\n        Given a list of values and a constant scalar, determine\n        whether each of the values is greater than MEAN of that list.\n        If a value is equal to the MEAN, return `False`.\n    \"\"\"\n    name = \"greater_than_mean\"\n    input_types = [[Numeric], [Datetime], [Ordinal]]\n    return_type = Boolean\n    compatibility = [Library.PANDAS, Library.DASK, Library.KOALAS]\n\n    def __init__(self):\n        self.description_template = \"whether {{}} is greater than MEAN value\"\n\n    def get_function(self):\n        def greater_than_mean(vals: pd.Series):\n            self.mean = np.mean(vals.values)\n            return vals > self.mean\n        return greater_than_mean","88027fa7":"feature_matrix, feature_descriptions = ft.dfs(\n    entityset=entity_set, \n    target_entity=\"fraudulent_claims\",\n    agg_primitives=default_aggregation_primitives,\n    trans_primitives=default_transformation_primitives+[GreaterThanMean],\n    max_depth=11\n)","19dac008":"feature_matrix, feature_descriptions = ft.encode_features(feature_matrix, feature_descriptions, include_unknown=False)","059ed9df":"feature_matrix.sample(n=7)","41f141d9":"for description in feature_descriptions:\n    print(description)\n    print(ft.describe_feature(description))\n    feature_graph = ft.graph_feature(description)\n    display(graphviz.Source(feature_graph).view())","d48ab17c":"ft.graph_feature(description)","c23129b5":"# **Feature Engineering**","ec1fc280":"# **Analysis on Outlier**","d0d098c7":"# *claim_listing*","8361f449":"# **Functions**","1c4b88ba":"# **Load data**","d7e7c031":"# *policy*","acdadd90":"# *claimant_listing*","6d884ee4":"# *Ground Truth*","7d8a80fa":"# **Merge**","85dfc0cc":"# *intermediary*","9aef9352":"# *insured*","b44c646b":"# **Fill missing values**","35cb80c9":"# **Libraries**"}}