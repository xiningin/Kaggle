{"cell_type":{"7571029e":"code","a1b129b5":"code","151d7a2f":"code","f6b11291":"code","a69317e5":"code","183e358b":"code","c39ef369":"code","64e1d40f":"code","acd34dc2":"code","15c07375":"code","b4afe2bc":"code","9c389719":"code","74fb154b":"code","711c473c":"code","03e0c455":"code","5aa1fe1b":"code","6bf781e3":"markdown","7b3ea606":"markdown","66da6cd3":"markdown","077e77ce":"markdown","179f35f3":"markdown","e474d487":"markdown","9a9ce95d":"markdown","f2b5d1f1":"markdown","223d0293":"markdown","40f592e4":"markdown","7b091521":"markdown"},"source":{"7571029e":"import os\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport pandas as pd\nimport librosa\nimport numpy as np\n\nfrom sklearn.utils import shuffle\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom pathlib import Path\n# Global vars\nRANDOM_SEED = 1337\nSAMPLE_RATE = 32000\nSIGNAL_LENGTH = 5 # seconds\nSPEC_SHAPE = (48, 128) # height x width\nFMIN = 500\nFMAX = 12500\n#MAX_AUDIO_FILES = 1500","a1b129b5":"# Code adapted from: \n# https:\/\/www.kaggle.com\/frlemarchand\/bird-song-classification-using-an-efficientnet\n# Make sure to check out the entire notebook.\n\n# Load metadata file\ntrain = pd.read_csv('..\/input\/birdclef-2021\/train_metadata.csv',)\n\n# Second, assume that birds with the most training samples are also the most common\n# A species needs at least 200 recordings with a rating above 4 to be considered common\nbirds_count = {}\nfor bird_species, count in zip(train.primary_label.unique(), \n                               train.groupby('primary_label')['primary_label'].count().values):\n    birds_count[bird_species] = count\nmost_represented_birds = [key for key,value in birds_count.items()] \n\nTRAIN = train.query('primary_label in @most_represented_birds')\nLABELS = sorted(TRAIN.primary_label.unique())\n\n# Let's see how many species and samples we have left\nprint('NUMBER OF SPECIES IN TRAIN DATA:', len(LABELS))\nprint('NUMBER OF SAMPLES IN TRAIN DATA:', len(TRAIN))\nprint('LABELS:', most_represented_birds)","151d7a2f":"pd.read_csv('..\/input\/birdclef-2021\/test.csv')","f6b11291":"train","a69317e5":"# Shuffle the training data and limit the number of audio files to MAX_AUDIO_FILES\nTRAIN = shuffle(TRAIN, random_state=RANDOM_SEED)\n\n# Define a function that splits an audio file, \n# extracts spectrograms and saves them in a working directory\ndef get_spectrograms(filepath, primary_label, output_dir):\n    \n    # Open the file with librosa (limited to the first 15 seconds)\n    sig, rate = librosa.load(filepath, sr=SAMPLE_RATE, offset=None, duration=15)\n    \n    # Split signal into five second chunks\n    sig_splits = []\n    for i in range(0, len(sig), int(SIGNAL_LENGTH * SAMPLE_RATE)):\n        split = sig[i:i + int(SIGNAL_LENGTH * SAMPLE_RATE)]\n\n        # End of signal?\n        if len(split) < int(SIGNAL_LENGTH * SAMPLE_RATE):\n            break\n        \n        sig_splits.append(split)\n        \n    # Extract mel spectrograms for each audio chunk\n    s_cnt = 0\n    saved_samples = []\n    for chunk in sig_splits:\n        \n        hop_length = int(SIGNAL_LENGTH * SAMPLE_RATE \/ (SPEC_SHAPE[1] - 1))\n        mel_spec = librosa.feature.melspectrogram(y=chunk, \n                                                  sr=SAMPLE_RATE, \n                                                  n_fft=1024, \n                                                  hop_length=hop_length, \n                                                  n_mels=SPEC_SHAPE[0], \n                                                  fmin=FMIN, \n                                                  fmax=FMAX)\n    \n        mel_spec = librosa.power_to_db(mel_spec, ref=np.max) \n        \n        # Normalize\n        mel_spec -= mel_spec.min()\n        mel_spec \/= mel_spec.max()\n        \n        # Save as image file\n        save_dir = os.path.join(output_dir, primary_label)\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        save_path = os.path.join(save_dir, filepath.rsplit(os.sep, 1)[-1].rsplit('.', 1)[0] + \n                                 '_' + str(s_cnt) + '.png')\n        im = Image.fromarray(mel_spec * 255.0).convert(\"L\")\n        im.save(save_path)\n        \n        saved_samples.append(save_path)\n        s_cnt += 1\n        \n        \n    return saved_samples\n\nprint('FINAL NUMBER OF AUDIO FILES IN TRAINING DATA:', len(TRAIN))","183e358b":"# Parse audio files and extract training samples\ninput_dir = '..\/input\/birdclef-2021\/train_short_audio\/'\noutput_dir = '..\/working\/melspectrogram_dataset\/'\nsamples = []\nwith tqdm(total=len(TRAIN)) as pbar:\n    for idx, row in TRAIN.iterrows():\n        pbar.update(1)\n        \n        if row.primary_label in most_represented_birds:\n            audio_file_path = os.path.join(input_dir, row.primary_label, row.filename)\n            samples += get_spectrograms(audio_file_path, row.primary_label, output_dir)\n            \nTRAIN_SPECS = shuffle(samples, random_state=RANDOM_SEED)\nprint('SUCCESSFULLY EXTRACTED {} SPECTROGRAMS'.format(len(TRAIN_SPECS)))","c39ef369":"# Plot the first 12 spectrograms of TRAIN_SPECS\nplt.figure(figsize=(15, 7))\nfor i in range(12):\n    spec = Image.open(TRAIN_SPECS[i])\n    plt.subplot(3, 4, i + 1)\n    plt.title(TRAIN_SPECS[i].split(os.sep)[-1])\n    plt.imshow(spec, origin='lower')","64e1d40f":"def data_generator(batch_size):\n    \n    while True:\n        \n        for offset in range(0, len(TRAIN_SPECS), batch_size):\n            train_specs, train_labels = [], []\n            current_paths = TRAIN_SPECS[offset:offset + batch_size]\n            for path in current_paths:\n                # Open image\n                spec = Image.open(path)\n\n                # Convert to numpy array\n                spec = np.array(spec, dtype='float32')\n\n                # Normalize between 0.0 and 1.0\n                # and exclude samples with nan \n                spec -= spec.min()\n                spec \/= spec.max()\n                if not spec.max() == 1.0 or not spec.min() == 0.0:\n                    continue\n\n                # Add channel axis to 2D array\n                spec = np.expand_dims(spec, -1)\n\n                # Add new dimension for batch size\n                spec = np.expand_dims(spec, 0)\n\n                # Add to train data\n                if len(train_specs) == 0:\n                    train_specs = spec\n                else:\n                    train_specs = np.vstack((train_specs, spec))\n\n                # Add to label data\n                target = np.zeros((len(LABELS)), dtype='float32')\n                bird = path.split(os.sep)[-2]\n                target[LABELS.index(bird)] = 1.0\n                if len(train_labels) == 0:\n                    train_labels = target\n                else:\n                    train_labels = np.vstack((train_labels, target))\n            yield train_specs, train_labels","acd34dc2":"# Make sure your experiments are reproducible\ntf.random.set_seed(RANDOM_SEED)\n\n# Build a simple model as a sequence of  convolutional blocks.\n# Each block has the sequence CONV --> RELU --> BNORM --> MAXPOOL.\n# Finally, perform global average pooling and add 2 dense layers.\n# The last layer is our classification layer and is softmax activated.\n# (Well it's a multi-label task so sigmoid might actually be a better choice)\nmodel = tf.keras.Sequential([\n    \n    # First conv block\n    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', \n                           input_shape=(SPEC_SHAPE[0], SPEC_SHAPE[1], 1)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    \n    # Second conv block\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D((2, 2)), \n    \n    # Third conv block\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D((2, 2)), \n    \n    # Fourth conv block\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    \n    # Global pooling instead of flatten()\n    tf.keras.layers.GlobalAveragePooling2D(), \n    \n    # Dense block\n    tf.keras.layers.Dense(256, activation='relu'),   \n    tf.keras.layers.Dropout(0.5),  \n    tf.keras.layers.Dense(256, activation='relu'),   \n    tf.keras.layers.Dropout(0.5),\n    \n    # Classification layer\n    tf.keras.layers.Dense(len(LABELS), activation='softmax')\n])\nprint('MODEL HAS {} PARAMETERS.'.format(model.count_params()))","15c07375":"# Compile the model and specify optimizer, loss and metric\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.01),\n              metrics=['accuracy', tf.keras.metrics.AUC()])","b4afe2bc":"# Let's train the model for a few epochs\ntrain_gen = data_generator(32)\n#next(train_gen)[1].shape\nmodel.fit_generator(train_gen,\n                    steps_per_epoch=len(TRAIN_SPECS) \/\/ 32,\n                    epochs=40)","9c389719":"model.save('.\/best_model.h5')","74fb154b":"TEST = (len(list(Path(\"..\/input\/birdclef-2021\/test_soundscapes\/\").glob(\"*.ogg\"))) != 0)\nif TEST:\n    DATADIR = list(Path(\"..\/input\/birdclef-2021\/test_soundscapes\/\").glob(\"*.ogg\"))\nelse:\n    DATADIR = list(Path(\"..\/input\/birdclef-2021\/train_soundscapes\/\").glob(\"*.ogg\"))","711c473c":"# Load the best checkpoint\n#model = tf.keras.models.load_model('best_model.h5')\n\n# Pick a soundscape\n\nprint(DATADIR)\n\nmain_df = []\nfor s_path in DATADIR:\n    soundscape_path = str(s_path)\n    # Open it with librosa\n    sig, rate = librosa.load(soundscape_path, sr=SAMPLE_RATE)\n\n    # Store results so that we can analyze them later\n\n\n    # Split signal into 5-second chunks\n    # Just like we did before (well, this could actually be a seperate function)\n    sig_splits = []\n    for i in range(0, len(sig), int(SIGNAL_LENGTH * SAMPLE_RATE)):\n        split = sig[i:i + int(SIGNAL_LENGTH * SAMPLE_RATE)]\n\n        # End of signal?\n        if len(split) < int(SIGNAL_LENGTH * SAMPLE_RATE):\n            break\n\n        sig_splits.append(split)\n\n    # Get the spectrograms and run inference on each of them\n    # This should be the exact same process as we used to\n    # generate training samples!\n    seconds, scnt = 0, 0\n\n    for chunk in sig_splits:\n        data = {}\n        # Keep track of the end time of each chunk\n        seconds += 5\n\n        # Get the spectrogram\n        hop_length = int(SIGNAL_LENGTH * SAMPLE_RATE \/ (SPEC_SHAPE[1] - 1))\n        mel_spec = librosa.feature.melspectrogram(y=chunk, \n                                                  sr=SAMPLE_RATE, \n                                                  n_fft=1024, \n                                                  hop_length=hop_length, \n                                                  n_mels=SPEC_SHAPE[0], \n                                                  fmin=FMIN, \n                                                  fmax=FMAX)\n\n        mel_spec = librosa.power_to_db(mel_spec, ref=np.max) \n\n        # Normalize to match the value range we used during training.\n        # That's something you should always double check!\n        mel_spec -= mel_spec.min()\n        mel_spec \/= mel_spec.max()\n\n        # Add channel axis to 2D array\n        mel_spec = np.expand_dims(mel_spec, -1)\n\n        # Add new dimension for batch size\n        mel_spec = np.expand_dims(mel_spec, 0)\n\n        # Predict\n        p = model.predict(mel_spec)[0]\n\n        # Get highest scoring species\n        idx = p.argmax()\n        species = LABELS[idx]\n        score = p[idx]\n\n        # Prepare submission entry\n        data[\"row_id\"] = soundscape_path.split(os.sep)[-1].rsplit('_', 1)[0] + '_' + str(seconds)\n\n        # Decide if it's a \"nocall\" or a species by applying a threshold\n        if score > 0.30:\n            data[\"birds\"] = species\n            scnt += 1\n        else:\n            data[\"birds\"] = 'nocall'\n        main_df.append(data)\n    \n        # Add the confidence score as well\n        #data['score'].append(score)\n\nprediction_df  = pd.DataFrame(main_df)\n\nprint('SOUNSCAPE ANALYSIS DONE. FOUND {} BIRDS.'.format(scnt))","03e0c455":"prediction_df.to_csv(\"submission.csv\", index=False)","5aa1fe1b":"pd.read_csv(\"submission.csv\")","6bf781e3":"Nice! These are good samples. Notice how some of them only contain a fraction of a bird call? That's an issue we won't deal with in this tutorial. We will simply ignore the fact that samples might not contain any bird sounds.\n\n# 4. Load training samples\n\nFor now, our spectrograms reside in a working directory. If we want to train a model, we have to load them into memory. Yet, with potentially hundreds of thousands of extracted spectrograms, an in-memory dataset is not a good idea. But for now, loading samples from disk and combining them into a large NumPy array is fine. It\u2019s the easiest way to use these data for training with Keras.","7b3ea606":"# 5. Build a simple model\n\nAlright, our dataset is ready, now we need to define a model architecture. In this tutorial, we\u2019ll use a very simplistic, AlexNet-like design with four convolutional layers and three dense layers. It might make sense to choose an off-the-shelve TF model that was pre-trained on audio data, but we would need to adjust the inputs (i.e., the resolution of our spectrograms) to fit the external model. So we keep it simple and build our own model.","66da6cd3":"# Feel free to use this code and if you liked it please upvote.\n# Thank you.","077e77ce":"**Save prediction to csv**","179f35f3":"Callbacks make our life easier, the three that we're adding will take care of saving the best checkpoint, they will reduce the learning rate whenever the training process stalls, and they will stop the training if the model is overfitting.","e474d487":"This is not a huge CNN, it only has ~200,000 parameters.\n\nNext, we need to specify an optimzer, initial learning rate, a loss function and a metric.","9a9ce95d":"# Model training\n\nIn this notebook, we will train our first model and apply this model to a soundscape. We will keep the amount of training samples, species and soundscapes to a minimum to keep the execution time as short as possible. Remember, this is only a sample implementation, feel free to explore your own workflow.\n\nThese are the steps that we will cover:\n\n\n* select audio files we want to use for training  \n* extract spectrograms from those files and save them in a working directory  \n* load selected samples into a large in-memory dataset  \n* build a simple beginners CNN  \n* train the model  \n* apply the model to a selected soundscape and look at the results \n\n# 1. Settings and imports\n\nLet\u2019s begin with imports and a few basic settings.","f2b5d1f1":"# Save Model","223d0293":"# 2. Data preparation\n\nThe training data for this competition contains tens of thousands of audio files for 397 species.","40f592e4":"Alright, we got training spectrograms.Let's make sure the spectrograms look right and show the first 12.","7b091521":"\n# 3. Extract training samples\n\nWe need to define a function that extracts spectrograms for a given audio file. This function needs to load a file with Librosa, extract mel spectrograms and save each spectrogram as PNG image in a working directory for later access."}}