{"cell_type":{"ee7f2fbb":"code","42f73c01":"code","9f3b17dd":"code","ce373fe2":"code","963c7456":"code","0fdbcd1a":"code","9e046d82":"code","0e26666a":"code","f7de0759":"code","57b590ca":"code","6d385d41":"code","a4e7e8cb":"code","98c3ee8a":"code","113bb5c0":"code","6fce1ff4":"code","1a831eba":"code","0f889f08":"markdown","9bd68d27":"markdown","d40af3f9":"markdown","09d2d387":"markdown","e2b05704":"markdown","a454722d":"markdown","efe87cbb":"markdown","9a3c8433":"markdown","a40b58f8":"markdown","fd5fcfd9":"markdown","a3aca5e1":"markdown","909328b5":"markdown","42dee860":"markdown","57cc6b1b":"markdown","4fd4ddfc":"markdown","44bbf45b":"markdown"},"source":{"ee7f2fbb":"from tensorflow import keras\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","42f73c01":"\ndef readucr(filename):\n    data = np.loadtxt(filename, delimiter=\"\\t\")\n    y = data[:, 0]\n    x = data[:, 1:]\n    return x, y.astype(int)\n\n\nroot_url = \"https:\/\/raw.githubusercontent.com\/hfawaz\/cd-diagram\/master\/FordA\/\"\n\nx_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\nx_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")","9f3b17dd":"df = pd.DataFrame(x_train,columns=['fea_'+str(i) for i in range(500)])\ndf['target'] = y_train","ce373fe2":"df.head()","963c7456":"classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n\nplt.figure()\nfor c in classes:\n    c_x_train = x_train[y_train == c]\n    plt.plot(c_x_train[0], label=\"class \" + str(c))\nplt.legend(loc=\"best\")\nplt.show()\nplt.close()","0fdbcd1a":"x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\nx_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))","9e046d82":"num_classes = len(np.unique(y_train))","0e26666a":"idx = np.random.permutation(len(x_train))\nx_train = x_train[idx]\ny_train = y_train[idx]","f7de0759":"y_train[y_train == -1] = 0\ny_test[y_test == -1] = 0","57b590ca":"def make_model(input_shape):\n    input_layer = tf.keras.layers.Input(input_shape)\n\n    conv1 = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding=\"same\")(input_layer)\n    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n    conv1 = tf.keras.layers.ReLU()(conv1)\n\n    conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n    conv2 = tf.keras.layers.ReLU()(conv2)\n\n    lstm1 = tf.keras.layers.LSTM(64, return_sequences=True)(conv2)\n    lstm2 = tf.keras.layers.LSTM(64, return_sequences=True)(lstm1)\n    lstm3 = tf.keras.layers.LSTM(64, return_sequences=True)(lstm2)\n\n    gap = tf.keras.layers.GlobalAveragePooling1D()(lstm3)\n    \n    d1 = tf.keras.layers.Dense(100)(gap)\n    d1 = tf.keras.layers.BatchNormalization()(d1)\n    d1 = tf.keras.layers.ReLU()(d1)\n    \n    d2 = tf.keras.layers.Dense(50, activation=\"relu\")(d1)\n    d2 = tf.keras.layers.BatchNormalization()(d2)\n    d2 = tf.keras.layers.ReLU()(d2)\n\n    output_layer = tf.keras.layers.Dense(2, activation=\"softmax\")(d2)\n\n    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n\n\nmodel = make_model(input_shape=x_train.shape[1:])\ntf.keras.utils.plot_model(model, show_shapes = True )","6d385d41":"epochs = 500\nbatch_size = 32\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n    ),\n    keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n    ),\n    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n]\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"sparse_categorical_accuracy\"],\n)\nhistory = model.fit(\n    x_train,\n    y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    validation_split=0.2,\n    verbose=1,\n)","a4e7e8cb":"model = keras.models.load_model(\"best_model.h5\")\n\ntest_loss, test_acc = model.evaluate(x_test, y_test)\n\nprint(\"Test accuracy\", test_acc)\nprint(\"Test loss\", test_loss)","98c3ee8a":"from sklearn.metrics import roc_auc_score,roc_curve,auc","113bb5c0":"preds = model.predict(x_test)[:,1]","6fce1ff4":"fpr, tpr, threshold = roc_curve(y_test, preds)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","1a831eba":"metric = \"sparse_categorical_accuracy\"\nplt.figure()\nplt.plot(history.history[metric])\nplt.plot(history.history[\"val_\" + metric])\nplt.title(\"model \" + metric)\nplt.ylabel(metric, fontsize=\"large\")\nplt.xlabel(\"epoch\", fontsize=\"large\")\nplt.legend([\"train\", \"val\"], loc=\"best\")\nplt.show()\nplt.close()","0f889f08":"## Load the data: the FordA dataset\n\n### Dataset description\n\nThe dataset we are using here is called FordA.\nThe data comes from the UCR archive.\nThe dataset contains 3601 training instances and another 1320 testing instances.\nEach timeseries corresponds to a measurement of engine noise captured by a motor sensor.\nFor this task, the goal is to automatically detect the presence of a specific issue with\nthe engine. The problem is a balanced binary classification task. The full description of\nthis dataset can be found [here](http:\/\/www.j-wichard.de\/publications\/FordPaper.pdf).\n\n### Read the TSV data\n\nWe will use the `FordA_TRAIN` file for training and the\n`FordA_TEST` file for testing. The simplicity of this dataset\nallows us to demonstrate effectively how to use ConvNets , lstms, and dense for timeseries classification.\nIn this file, the first column corresponds to the label.","9bd68d27":"## Standardize the data","d40af3f9":"## Build a model\n* 2 Conv1d layers\n* 3 lstm layers\n* dense layers","09d2d387":"<center><h1> Timeseries classification from scratch<\/h1><\/center>\n\n<center>Description: Training a timeseries classifier from scratch on the FordA dataset from the UCR\/UEA archive.<center>","e2b05704":"## Setup","a454722d":"## Train the model","efe87cbb":"## Plot the model's training and validation loss","9a3c8433":"Standardize the labels to positive integers.\nThe expected labels will then be 0 and 1.","a40b58f8":"<center><h2 style=\"color:red\">If you like please Upvote it motivates me to write quality content<\/h2><\/center>\n","fd5fcfd9":"## Lets Plot ROC curve","a3aca5e1":"## Visualize the data\n\nHere we visualize one timeseries example for each class in the dataset.","909328b5":"## Evaluate model on test data","42dee860":"## Introduction\n\nThis example shows how to do timeseries classification from scratch, starting from raw\nCSV timeseries files on disk. We demonstrate the workflow on the FordA dataset from the\n[UCR\/UEA archive](https:\/\/www.cs.ucr.edu\/%7Eeamonn\/time_series_data_2018\/).","57cc6b1b":"<center><h2 style=\"color:red\">If you like please Upvote it motivates me to write quality content<\/h2><\/center>","4fd4ddfc":"Finally, in order to use `sparse_categorical_crossentropy`, we will have to count\nthe number of classes beforehand.","44bbf45b":"Now we shuffle the training set because we will be using the `validation_split` option\nlater when training."}}