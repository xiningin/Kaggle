{"cell_type":{"10d4bc33":"code","1af322ee":"code","a3adfc02":"code","1f5546a0":"code","c1fafac3":"code","e82a5019":"code","cc5fdddd":"code","98b6e510":"code","2ec29964":"code","722b7e87":"code","438b66b1":"code","b5cc8ef6":"code","0011c5f8":"code","eb8462f3":"code","418e0624":"code","4bf343c3":"code","22f1ee9f":"code","c502b478":"code","299e0221":"code","eeb7ad96":"code","de43e6d3":"code","77a18e84":"code","066bc322":"code","74c6d813":"code","787a6216":"code","401f3044":"code","67d38082":"code","8a59dbf3":"code","07e11b01":"code","5abe182c":"code","6a301e2c":"code","68462a27":"code","ae9b4f98":"code","8af77487":"code","543a0e5e":"code","6c6a3745":"code","411832fc":"code","4507ad66":"code","997ae8de":"code","c85644dd":"markdown","4018d960":"markdown","7b56a569":"markdown","f39e848c":"markdown","64e4fc16":"markdown","b7c3c2d2":"markdown","3d9b5f7a":"markdown","b72e1f45":"markdown","6825211b":"markdown","8eb541ca":"markdown","88dfe3f3":"markdown","68999bd7":"markdown","0dd35cf0":"markdown"},"source":{"10d4bc33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom warnings import filterwarnings as filt \n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.style.use('fivethirtyeight')\nfilt('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1af322ee":"# !pip install wandb\n# !pip install albumentations ","a3adfc02":"def get_row_num(num, col = 3):\n    if num % col  == 0:\n        return num \/\/ col\n    return (num \/\/ col) + 1\n\ndef show_img(base_path, num_img = 5, n_cols = 3):\n    n_rows = get_row_num(num_img, col = n_cols)\n    fig = plt.figure(figsize=(20, n_cols * n_rows))\n    images = np.random.choice(os.listdir(base_path), num_img)\n    for ind, img in enumerate(images):\n        fig.add_subplot(n_rows, n_cols, ind + 1)\n        path = os.path.join(base_path, img)\n        image = plt.imread(path)\n        plt.imshow(image)\n        plt.title(image.shape)\n        plt.axis('off')","1f5546a0":"base_train_path = '..\/input\/cat-and-dog\/training_set\/training_set'\nbase_cat = os.path.join(base_train_path, 'cats')\nbase_dog = os.path.join(base_train_path, 'dogs')","c1fafac3":"show_img(base_cat, 9, 3)","e82a5019":"show_img(base_dog, 9, 3)","cc5fdddd":"len(os.listdir(base_cat)), len(os.listdir(base_dog))","98b6e510":"from keras.preprocessing.image import ImageDataGenerator\nfrom albumentations import transforms \nimport albumentations as A\nfrom PIL import Image\nimport cv2 as cv\nimport PIL ","2ec29964":"def alb_transformation(x):\n    transform = A.Compose([\n#         A.RandomBrightnessContrast(p=0.3),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Rotate(limit=60),\n        A.Perspective(p=0.8),\n        A.GaussianBlur(p=0.4),\n        A.CoarseDropout(max_holes = 15),\n    ])\n    \n    agumented_imgs = transform(image = x)['image']\n    return agumented_imgs\n\ndef img_gen(datagen_args):\n    return ImageDataGenerator(**datagen_args)\n\ndef create_datagen(base_path, batch_size = 16, shuffle = False, datagen_args = { 'resize': 1.\/255 }):\n    datagen = img_gen(datagen_args)\n    return datagen.flow_from_directory(\n        base_path,\n        target_size = (200, 200),\n        batch_size = batch_size,\n        shuffle = shuffle,\n        class_model = 'binary'\n    )","722b7e87":"iname = os.path.join(base_cat, 'cat.1.jpg')\ni = cv.imread(iname)\ni = cv.cvtColor(i, cv.COLOR_BGR2RGB)\nai = alb_transformation(i)\n\nplt.subplot(1, 2, 1)\nplt.imshow(ai)\nplt.axis('off');\n\nplt.subplot(1, 2, 2)\nplt.imshow(i)\nplt.axis('off')","438b66b1":"BATCH_SIZE = 32\nIMG_SIZE   = 250\n\ndargs = {\n    'rescale'         : 1.\/255,\n    'validation_split': 0.2,\n    'preprocessing_function' : alb_transformation\n}\n\nimageGen = img_gen(dargs)\ntrainGen = imageGen.flow_from_directory(\n    base_train_path,\n    target_size = (IMG_SIZE, IMG_SIZE),\n    batch_size = BATCH_SIZE, \n    shuffle = True,\n    subset = 'training',\n    class_mode = 'binary',\n    classes = ['cats', 'dogs']\n)\n\nvalidGen = imageGen.flow_from_directory(\n    base_train_path,\n    target_size = (IMG_SIZE, IMG_SIZE),\n    batch_size = BATCH_SIZE, \n    shuffle = False,\n    subset = 'validation',\n    class_mode = 'binary',\n    classes = ['cats', 'dogs']\n)","b5cc8ef6":"plt.imshow(trainGen[0][0][9])\nplt.title(f'{\"cat\" if trainGen[0][1][9] == 0 else \"dog\"} | {trainGen[0][0][9].shape}')\nplt.axis('off');","0011c5f8":"from keras.layers import Dense, Conv2D, MaxPool2D, BatchNormalization, Dropout, Input, Flatten\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import metrics as kmetrics\nfrom tensorflow.keras import optimizers\nfrom keras import Sequential\n\ndef CNN(ip, op):\n    model = Sequential([\n        Input(shape = ip),\n        \n        Conv2D(filters = 16, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu'),\n        Conv2D(filters = 16, kernel_size = (5,5), strides = 1, padding = 'same', activation = 'relu'),\n        MaxPool2D(),\n        \n        Conv2D(filters = 32, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu'),\n        Conv2D(filters = 32, kernel_size = (5,5), strides = 1, padding = 'same', activation = 'relu'),\n        MaxPool2D(),\n        \n        Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu'),\n        Conv2D(filters = 64, kernel_size = (5,5), strides = 1, padding = 'same', activation = 'relu'),\n        MaxPool2D(),\n        \n        Flatten(),\n        \n        Dense(1024, activation = 'relu'),\n        Dense(512, activation = 'relu'),\n        Dense(64, activation = 'relu'),\n        \n        Dense(op, activation = 'sigmoid' if op < 2 else 'softmax')\n    ])\n    print(f\"Output activation : {'sigmoid' if op < 2 else 'softmax'}\")\n    return model","eb8462f3":"import wandb \nfrom wandb.keras import WandbCallback\n\nwandb.login()","418e0624":"wandb.init(\n    project=\"my-test-project\", \n    entity=\"jaabir\",\n    config={\n        \"learning_rate\": 0.001,\n        \"epochs\"       : 10,\n        \"loss\"         : 'binary_crossentropy',\n        \"optimizer\"    : 'adam',\n        'batch_size'   : BATCH_SIZE,\n        'architecture' : 'VANILLA-CNN'\n    }\n)\n\nconfig = wandb.config\n\nmodel1 = CNN((250,250, 3), 1)\n\nmodel1.compile(\n    loss = config.loss,\n    optimizer = optimizers.Adam(learning_rate = config.learning_rate),\n    metrics = ['accuracy']\n)\n\nmodel1.summary()","4bf343c3":"save_model = ModelCheckpoint(\n    filepath = 'model.h5',\n    monitor = 'val_loss',\n    verbose = 1,\n    mode = 'min',\n    save_best_only = True\n)\n\ner_stopping = EarlyStopping(\n    monitor = 'val_loss',\n    patience = 4,\n    verbose = 1,\n    mode = 'min',\n    restore_best_weights = True\n)\n\nhis = model1.fit_generator(\n    generator = trainGen,\n    epochs = config.epochs,\n    validation_data = validGen,\n    callbacks = [save_model, WandbCallback(), er_stopping]\n)","22f1ee9f":"def AlexNet(ip, op):\n    model = Sequential([\n        Input(shape = ip),\n        \n        Conv2D(filters = 96, kernel_size = (11,11), strides = 4, padding = 'valid', activation = 'relu'),\n        MaxPool2D(pool_size=(3, 3), strides = 2),\n        \n        Conv2D(filters = 256, kernel_size = (5,5), strides = 1, padding = 'same', activation = 'relu'),\n        MaxPool2D(pool_size=(3, 3), strides = 2),\n        \n        Conv2D(filters = 384, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu'),\n        \n        Conv2D(filters = 384, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu'),\n        \n        Conv2D(filters = 256, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu'),\n        MaxPool2D(pool_size=(3, 3), strides = 2),\n        \n        Flatten(),\n        \n        Dense(4096, activation = 'relu'),\n        Dropout(0.3),\n        \n        Dense(4096, activation = 'relu'),\n        Dropout(0.3),\n        \n        Dense(op, activation = 'sigmoid' if op < 2 else 'softmax')\n    ])\n    print(f\"Output activation : {'sigmoid' if op < 2 else 'softmax'}\")\n    return model","c502b478":"BATCH_SIZE = 32\nIMG_SIZE   = 227\n\ndargs = {\n    'rescale'         : 1.\/255,\n    'validation_split': 0.2,\n    'preprocessing_function' : alb_transformation\n}\n\nimageGen2 = img_gen(dargs)\ntrainGen2 = imageGen2.flow_from_directory(\n    base_train_path,\n    target_size = (IMG_SIZE, IMG_SIZE),\n    batch_size = BATCH_SIZE, \n    shuffle = True,\n    subset = 'training',\n    class_mode = 'categorical',\n    classes = ['cats', 'dogs']\n)\n\nvalidGen2 = imageGen2.flow_from_directory(\n    base_train_path,\n    target_size = (IMG_SIZE, IMG_SIZE),\n    batch_size = BATCH_SIZE, \n    shuffle = False,\n    subset = 'validation',\n    class_mode = 'categorical',\n    classes = ['cats', 'dogs']\n)","299e0221":"trainGen2[0][1][0], validGen2[0][1][0]","eeb7ad96":"wandb.init(\n    project=\"my-test-project\", \n    entity=\"jaabir\",\n    config={\n        \"learning_rate\": 0.001,\n        \"epochs\"       : 10,\n        \"loss\"         : 'categorical_crossentropy',\n        \"optimizer\"    : 'adam',\n        'batch_size'   : BATCH_SIZE,\n        'architecture' : 'ALEX-NET'\n    }\n)\n\nconfig = wandb.config\n\nmodel2 = AlexNet((227,227, 3), 2)\n\nmodel2.compile(\n    loss = config.loss,\n    optimizer = optimizers.Adam(learning_rate = config.learning_rate),\n    metrics = ['accuracy']\n)\n\nmodel2.summary()","de43e6d3":"save_model = ModelCheckpoint(\n    filepath = 'model.h5',\n    monitor = 'val_loss',\n    verbose = 1,\n    mode = 'min',\n    save_best_only = True\n)\n\ner_stopping = EarlyStopping(\n    monitor = 'val_loss',\n    patience = 4,\n    verbose = 1,\n    mode = 'min',\n    restore_best_weights = True\n)\n\nhis2 = model2.fit_generator(\n    generator = trainGen2,\n    epochs = config.epochs,\n    validation_data = validGen2,\n    callbacks = [save_model, WandbCallback(), er_stopping]\n)","77a18e84":"from keras.applications.vgg16 import VGG16 as vgg\n\ndef VGG(op, trainable = False):\n    model = Sequential()\n    vmodel = vgg()\n    for layer in vmodel.layers[:-1]:\n        model.add(layer)\n    \n    if not trainable:\n        for layer in model.layers:\n            layer.trainable = False\n        \n    model.add(Dense(op, activation = 'softmax', name = 'prediction'))\n    \n    return model\n\nmodel3 = VGG(2)\nmodel3.summary()","066bc322":"BATCH_SIZE = 32\nIMG_SIZE   = 224\n\ndargs = {\n    'rescale'         : 1.\/255,\n    'validation_split': 0.2,\n    'preprocessing_function' : alb_transformation\n}\n\nimageGen3 = img_gen(dargs)\ntrainGen3 = imageGen3.flow_from_directory(\n    base_train_path,\n    target_size = (IMG_SIZE, IMG_SIZE),\n    batch_size = BATCH_SIZE, \n    shuffle = True,\n    subset = 'training',\n    class_mode = 'categorical',\n    classes = ['cats', 'dogs']\n)\n\nvalidGen3 = imageGen3.flow_from_directory(\n    base_train_path,\n    target_size = (IMG_SIZE, IMG_SIZE),\n    batch_size = BATCH_SIZE, \n    shuffle = False,\n    subset = 'validation',\n    class_mode = 'categorical',\n    classes = ['cats', 'dogs']\n)","74c6d813":"trainGen3[0][1][0], validGen3[0][1][0]","787a6216":"wandb.init(\n    project=\"my-test-project\", \n    entity=\"jaabir\",\n    config={\n        \"learning_rate\": 0.001,\n        \"epochs\"       : 10,\n        \"loss\"         : 'categorical_crossentropy',\n        \"optimizer\"    : 'adam',\n        'batch_size'   : BATCH_SIZE,\n        'architecture' : 'VGG-16',\n        'transfer-learning' : True\n    }\n)\n\nconfig = wandb.config\n\nmodel3.compile(\n    loss = config.loss,\n    optimizer = optimizers.Adam(learning_rate = config.learning_rate),\n    metrics = ['accuracy']\n)","401f3044":"save_model = ModelCheckpoint(\n    filepath = 'model3.h5',\n    monitor = 'val_loss',\n    verbose = 1,\n    mode = 'min',\n    save_best_only = True\n)\n\ner_stopping = EarlyStopping(\n    monitor = 'val_loss',\n    patience = 4,\n    verbose = 1,\n    mode = 'min',\n    restore_best_weights = True\n)\n\nhis3 = model3.fit_generator(\n    generator = trainGen3,\n    epochs = config.epochs,\n    validation_data = validGen3,\n    callbacks = [save_model, WandbCallback(), er_stopping]\n)","67d38082":"model4 = VGG(2, True)\n\nwandb.init(\n    project=\"my-test-project\", \n    entity=\"jaabir\",\n    config={\n        \"learning_rate\": 0.001,\n        \"epochs\"       : 10,\n        \"loss\"         : 'categorical_crossentropy',\n        \"optimizer\"    : 'adam',\n        'batch_size'   : BATCH_SIZE,\n        'architecture' : 'VGG-16',\n        'transfer-learning' : False\n    }\n)\n\nconfig = wandb.config\n\nmodel4.compile(\n    loss = config.loss,\n    optimizer = optimizers.Adam(learning_rate = config.learning_rate),\n    metrics = ['accuracy']\n)\n\n\nmodel4.summary()","8a59dbf3":"save_model = ModelCheckpoint(\n    filepath = 'model4.h5',\n    monitor = 'val_loss',\n    verbose = 1,\n    mode = 'min',\n    save_best_only = True\n)\n\ner_stopping = EarlyStopping(\n    monitor = 'val_loss',\n    patience = 4,\n    verbose = 1,\n    mode = 'min',\n    restore_best_weights = True\n)\n\nhis4 = model4.fit_generator(\n    generator = trainGen3,\n    epochs = config.epochs,\n    validation_data = validGen3,\n    callbacks = [save_model, WandbCallback(), er_stopping]\n)","07e11b01":"from keras.applications.resnet import ResNet50\nfrom keras.models import Model\n\ndef RNet(op, fc_hidden = False):\n    if not fc_hidden:\n        model = ResNet50(classes = op, weights = None)\n    else:\n        base_model = ResNet50(include_top = False, weights = None, pooling = 'avg', input_shape = (224, 224, 3))\n        dp = Dropout(0.3)(base_model.output)\n        \n        hd1 = Dense(1024, activation = 'relu')(dp)\n        bn1 = BatchNormalization()(hd1)\n        dp1 = Dropout(0.2)(bn1)\n        \n        hd2 = Dense(512, activation = 'relu')(dp1)\n        bn2 = BatchNormalization()(hd2)\n        dp2 = Dropout(0.15)(bn2)\n        \n        op  = Dense(op, activation = 'softmax')(dp2)\n        \n        model = Model(inputs = base_model.input, outputs = op)\n    \n    return model \n\n\nmodel5 = RNet(2, True)\nmodel5.summary()","5abe182c":"BATCH_SIZE = 32\nIMG_SIZE   = 224\n\ndargs = {\n    'rescale'         : 1.\/255,\n    'validation_split': 0.2,\n    'preprocessing_function' : alb_transformation\n}\n\nimageGen5 = img_gen(dargs)\ntrainGen5 = imageGen5.flow_from_directory(\n    base_train_path,\n    target_size = (IMG_SIZE, IMG_SIZE),\n    batch_size = BATCH_SIZE, \n    shuffle = True,\n    subset = 'training',\n    class_mode = 'categorical',\n    classes = ['cats', 'dogs']\n)\n\nvalidGen5 = imageGen5.flow_from_directory(\n    base_train_path,\n    target_size = (IMG_SIZE, IMG_SIZE),\n    batch_size = BATCH_SIZE, \n    shuffle = False,\n    subset = 'validation',\n    class_mode = 'categorical',\n    classes = ['cats', 'dogs']\n)","6a301e2c":"wandb.init(\n    project=\"my-test-project\", \n    entity=\"jaabir\",\n    config={\n        \"learning_rate\": 0.001,\n        \"epochs\"       : 10,\n        \"loss\"         : 'categorical_crossentropy',\n        \"optimizer\"    : 'adam',\n        'batch_size'   : BATCH_SIZE,\n        'architecture' : 'RES-NET',\n    }\n)\n\nconfig = wandb.config\n\nmodel5.compile(\n    loss = config.loss,\n    optimizer = optimizers.Adam(learning_rate = config.learning_rate),\n    metrics = ['accuracy']\n)","68462a27":"save_model = ModelCheckpoint(\n    filepath = 'model5.h5',\n    monitor = 'val_loss',\n    verbose = 1,\n    mode = 'min',\n    save_best_only = True\n)\n\ner_stopping = EarlyStopping(\n    monitor = 'val_loss',\n    patience = 4,\n    verbose = 1,\n    mode = 'min',\n    restore_best_weights = True\n)\n\nhis5 = model5.fit_generator(\n    generator = trainGen5,\n    epochs = config.epochs,\n    validation_data = validGen5,\n    callbacks = [save_model, WandbCallback(), er_stopping]\n)","ae9b4f98":"from keras.applications.inception_v3 import InceptionV3\n\ndef Inception(op, fc_hidden = False):\n    if not fc_hidden:\n        model = InceptionV3(classes = op, weights = None)\n    else:\n        base_model = InceptionV3(include_top = False, weights = None, pooling = 'max', input_shape = (299, 299, 3))\n        dp = Dropout(0.3)(base_model.output)\n        \n        hd1 = Dense(1024, activation = 'relu')(dp)\n        bn1 = BatchNormalization()(hd1)\n        dp1 = Dropout(0.2)(bn1)\n        \n        hd2 = Dense(512, activation = 'relu')(dp1)\n        bn2 = BatchNormalization()(hd2)\n        dp2 = Dropout(0.15)(bn2)\n        \n        op  = Dense(op, activation = 'softmax')(dp2)\n        \n        model = Model(inputs = base_model.input, outputs = op)\n        \n    return model\n\nmodel6 = Inception(2, True)\nmodel6.summary()","8af77487":"BATCH_SIZE = 32\nIMG_SIZE   = 299\n\ndargs = {\n    'rescale'         : 1.\/255,\n    'validation_split': 0.2,\n    'preprocessing_function' : alb_transformation\n}\n\nimageGen6 = img_gen(dargs)\ntrainGen6 = imageGen6.flow_from_directory(\n    base_train_path,\n    target_size = (IMG_SIZE, IMG_SIZE),\n    batch_size = BATCH_SIZE, \n    shuffle = True,\n    subset = 'training',\n    class_mode = 'categorical',\n    classes = ['cats', 'dogs']\n)\n\nvalidGen6 = imageGen6.flow_from_directory(\n    base_train_path,\n    target_size = (IMG_SIZE, IMG_SIZE),\n    batch_size = BATCH_SIZE, \n    shuffle = False,\n    subset = 'validation',\n    class_mode = 'categorical',\n    classes = ['cats', 'dogs']\n)","543a0e5e":"wandb.init(\n    project=\"my-test-project\", \n    entity=\"jaabir\",\n    config={\n        \"learning_rate\": 0.001,\n        \"epochs\"       : 10,\n        \"loss\"         : 'categorical_crossentropy',\n        \"optimizer\"    : 'adam',\n        'batch_size'   : BATCH_SIZE,\n        'architecture' : 'Inception-NET',\n        'attempt'      : 2\n    }\n)\n\nconfig = wandb.config\n\nmodel6.compile(\n    loss = config.loss,\n    optimizer = optimizers.Adam(learning_rate = config.learning_rate),\n    metrics = ['accuracy']\n)","6c6a3745":"save_model = ModelCheckpoint(\n    filepath = 'model5.h5',\n    monitor = 'val_loss',\n    verbose = 1,\n    mode = 'min',\n    save_best_only = True\n)\n\ner_stopping = EarlyStopping(\n    monitor = 'val_loss',\n    patience = 4,\n    verbose = 1,\n    mode = 'min',\n    restore_best_weights = True\n)\n\nhis6 = model6.fit_generator(\n    generator = trainGen6,\n    epochs = config.epochs,\n    validation_data = validGen6,\n    callbacks = [save_model, WandbCallback(), er_stopping]\n)","411832fc":"from keras.applications.mobilenet_v3 import MobileNetV3Large\n\ndef MobileNet(op, fc_hidden = False):\n    if not fc_hidden:\n        model = MobileNetV3Large(classes = op, weights = None, input_shape = (299, 299, 3))\n    else:\n        base_model = Inceptionv3(include_top = False, weights = None, pooling = 'avg', input_shape = (299, 299, 3))\n        dp = Dropout(0.3)(base_model.output)\n        \n        hd1 = Dense(1024, activation = 'relu')(dp)\n        bn1 = BatchNormalization()(hd1)\n        dp1 = Dropout(0.2)(bn1)\n        \n        hd2 = Dense(512, activation = 'relu')(dp1)\n        bn2 = BatchNormalization()(hd2)\n        dp2 = Dropout(0.15)(bn2)\n        \n        op  = Dense(op, activation = 'softmax')(dp2)\n        \n        model = Model(inputs = base_model.input, outputs = op)\n        \n    return model\n\nmodel7 = MobileNet(2)\nmodel7.summary()","4507ad66":"wandb.init(\n    project=\"my-test-project\", \n    entity=\"jaabir\",\n    config={\n        \"learning_rate\": 0.001,\n        \"epochs\"       : 10,\n        \"loss\"         : 'categorical_crossentropy',\n        \"optimizer\"    : 'adam',\n        'batch_size'   : BATCH_SIZE,\n        'architecture' : 'MobileNet',\n    }\n)\n\nconfig = wandb.config\n\nmodel7.compile(\n    loss = config.loss,\n    optimizer = optimizers.Adam(learning_rate = config.learning_rate),\n    metrics = ['accuracy']\n)","997ae8de":"save_model = ModelCheckpoint(\n    filepath = 'model5.h5',\n    monitor = 'val_loss',\n    verbose = 1,\n    mode = 'min',\n    save_best_only = True\n)\n\ner_stopping = EarlyStopping(\n    monitor = 'val_loss',\n    patience = 4,\n    verbose = 1,\n    mode = 'min',\n    restore_best_weights = True\n)\n\nhis7 = model7.fit_generator(\n    generator = trainGen6,\n    epochs = config.epochs,\n    validation_data = validGen6,\n    callbacks = [save_model, WandbCallback(), er_stopping]\n)","c85644dd":"### VGG - 16","4018d960":"cat images ","7b56a569":"#### With Transfer learning ","f39e848c":"all the 3 classic CNNs gave around 50% validation accuracy on average for 10 epochs ( except transfer learned vgg-16 )","64e4fc16":"### plain CNN","b7c3c2d2":"## Linear CNNs","3d9b5f7a":"### RES-NET","b72e1f45":"## Non linear CNNs","6825211b":"i am gonna try transfer learning only on vgg - 16 just to see how good it works and omit it for rest of the models ","8eb541ca":"#### without transfer learning","88dfe3f3":"dog images","68999bd7":"**you can view all the metrics report here** : [REPORT]('https:\/\/wandb.ai\/jaabir\/my-test-project?workspace=user-jaabir')\n<br>\nInceptionV3 with extra FC layers did better.","0dd35cf0":"### AlexNet - 8 "}}