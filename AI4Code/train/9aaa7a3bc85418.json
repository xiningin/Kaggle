{"cell_type":{"711e5fce":"code","c66df2d3":"code","2272dc49":"code","5a682521":"code","ad94465c":"code","8cf62d28":"code","b6fc2dce":"code","e82270fb":"code","fa83db15":"code","7955fab7":"code","9352b9eb":"code","4b66b30e":"code","6fb26c41":"code","27ad1d1b":"code","0cfe0165":"code","3f5ba2c4":"code","f10d0447":"markdown","a5fee96f":"markdown","9f834408":"markdown","28a4c6ca":"markdown","d72e4854":"markdown","3a31948b":"markdown","4dd573d1":"markdown","2b4cf9fb":"markdown","99ad21ac":"markdown","5a65b5f9":"markdown","5da82f8a":"markdown","e34f8413":"markdown","7d3fddd6":"markdown","21945b41":"markdown","61207551":"markdown","903a268c":"markdown","4a426778":"markdown"},"source":{"711e5fce":"import tensorflow as tf\nfrom tensorflow import keras","c66df2d3":"# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport math\n\nimport numpy as np ","2272dc49":"(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full = X_train_full \/ 255.0\nX_test = X_test \/ 255.0\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]","5a682521":"n_epochs = 25\nlearning_rate = 0.01  #initial learning rate\ndecay = 1e-4\nbatch_size = 32\nn_steps_per_epoch = math.ceil(len(X_train) \/ batch_size)\nepochs = np.arange(n_epochs)\nlrs = learning_rate \/ (1 + decay * epochs * n_steps_per_epoch)\n\nplt.plot(epochs, lrs,  \"o-\")\nplt.axis([0, n_epochs - 1, 0, 0.01])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Power Scheduling\", fontsize=14)\nplt.grid(True)\nplt.show()","ad94465c":"def exponential_decay_fn(epoch):\n    return 0.01 * 0.1**(epoch \/ 20)\ndef exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1**(epoch \/ s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(lr0=0.01, s=20)","8cf62d28":"model = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nn_epochs = 25","b6fc2dce":"lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\nhistory = model.fit(X_train, y_train, epochs=n_epochs,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[lr_scheduler])","e82270fb":"plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\nplt.axis([0, n_epochs - 1, 0, 0.011])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Exponential Scheduling\", fontsize=14)\nplt.grid(True)\nplt.show()","fa83db15":"def piecewise_constant_fn(epoch):\n    if epoch < 5:\n        return 0.01\n    elif epoch < 15:\n        return 0.005\n    else:\n        return 0.001\ndef piecewise_constant(boundaries, values):\n    boundaries = np.array([0] + boundaries)\n    values = np.array(values)\n    def piecewise_constant_fn(epoch):\n        return values[np.argmax(boundaries > epoch) - 1]\n    return piecewise_constant_fn\n\npiecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n\nlr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nn_epochs = 25\nhistory = model.fit(X_train, y_train, epochs=n_epochs,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[lr_scheduler])","7955fab7":"plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\nplt.axis([0, n_epochs - 1, 0, 0.011])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Piecewise Constant Scheduling\", fontsize=14)\nplt.grid(True)\nplt.show()","9352b9eb":"lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\noptimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nn_epochs = 25\nhistory = model.fit(X_train, y_train, epochs=n_epochs,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[lr_scheduler])","4b66b30e":"plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\", color='b')\nplt.tick_params('y', colors='b')\nplt.gca().set_xlim(0, n_epochs - 1)\nplt.grid(True)\n\nax2 = plt.gca().twinx()\nax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\nax2.set_ylabel('Validation Loss', color='r')\nax2.tick_params('y', colors='r')\n\nplt.title(\"Reduce LR on Plateau\", fontsize=14)\nplt.show()","6fb26c41":"K = keras.backend\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\ndef find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n    init_weights = model.get_weights()\n    iterations = math.ceil(len(X) \/ batch_size) * epochs\n    factor = np.exp(np.log(max_rate \/ min_rate) \/ iterations)\n    init_lr = K.get_value(model.optimizer.learning_rate)\n    K.set_value(model.optimizer.learning_rate, min_rate)\n    exp_lr = ExponentialLearningRate(factor)\n    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n                        callbacks=[exp_lr])\n    K.set_value(model.optimizer.learning_rate, init_lr)\n    model.set_weights(init_weights)\n    return exp_lr.rates, exp_lr.losses\n","27ad1d1b":"def plot_lr_vs_loss(rates, losses):\n    plt.plot(rates, losses)\n    plt.gca().set_xscale('log')\n    plt.hlines(min(losses), min(rates), max(rates))\n    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) \/ 2])\n    plt.xlabel(\"Learning rate\")\n    plt.ylabel(\"Loss\")","0cfe0165":"tf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])","3f5ba2c4":"batch_size = 128\nrates, losses = find_learning_rate(model, X_train, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)","f10d0447":"<font size=\"4.5\">**Concluson**\n\n    \n<font size=\"4\">This technique requires fidling around to figure out the right sequence of learning rates and how long to use each of them.    \n","a5fee96f":"<font size=\"4\">\ud83d\udc80Finding a good learning rate is very important.If too high training may diverge.If too low training will converge but take a very long time.\n    \n<font size=\"4\"> \ud83d\udc7bWe can find a good learning rate by training the\nmodel for a few hundred iterations, exponentially increasing the learning rate from a\nvery small value to a very large value, and then looking at the learning curve and\npicking a learning rate slightly lower than the one at which the learning curve starts\nshooting back up.  <img src=\"https:\/\/i.stack.imgur.com\/hhwyC.png\" width=\"400\"\/>\n        ","9f834408":"<font size=\"4\">**Hope you like this ....**\n","28a4c6ca":"# Learning Rate Scheduling","d72e4854":"<font size=\"4\">\ud83d\udc7dDeep learning neural networks can be trained using the\n* Gradient Descent.\n* Stochastic Gradient Descent.\n* Adagrad.\n* Adadelta.\n* RMSprop.\n* Adam. algorithm.   \n<font size=\"4\">\ud83d\udc7e These optimization algorithm estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the back-propagation of errors algorithm, referred to as simply backpropagation.   \n<font size=\"4\">\ud83e\udd16 The amount that the weights are updated during training is referred to as the step size or the \u201clearning rate.\u201d","3a31948b":"<html> \n \n<body> \n    <h1 style=\"color:green;\">Introduction<\/h1> \n    <font size=\"4\">\ud83e\udd81Learning rate hyperparameter is one the  most important hyperparameter which we need to take care while traing an neural network. As it controls how much a model is to be changed in response to the error each time while updating the weights.  \n        \n  \ud83d\udc2fIf the learnig rate is too small the algorithm will have to go through many iterations to converge,which will take a long time.\n    <img src=\"https:\/\/static.wixstatic.com\/media\/0cef7a_e11ad46aef61451091fa20c77c8d14a1~mv2.png\/v1\/fill\/w_360,h_192,al_c,q_95\/0cef7a_e11ad46aef61451091fa20c77c8d14a1~mv2.webp\" width=\"400\"\/>\n       \n  \ud83d\udc36But if the learnig rate is too high,the algorithm diverges and fail to find a good solution.Because when the learnig rate is high,you might jump across the valley and end up on the other side, possibly even higher up than you were before.\n        <img src=\"https:\/\/static.wixstatic.com\/media\/0cef7a_6a1e7529716344cd8fb9451281c39f21~mv2.png\/v1\/fill\/w_360,h_194,al_c,q_95\/0cef7a_6a1e7529716344cd8fb9451281c39f21~mv2.webp\" width=\"400\"\/>\n        \n<\/body> \n  \n<\/html>","4dd573d1":"<font size=\"4\">The most commonly used learnig schedules are as following:\n * Power scheduling\n * Exponential scheduling\n * Piecewise constant scheduling \n * Performance scheduling \n * 1cycle scheduling   \n \n\n <img src=\"https:\/\/imagehost7.online-image-editor.com\/oie_upload\/images\/8744142cuv65Gsl\/wMA77fBrQRhW.png\"\/>\n","2b4cf9fb":"<html> \n \n<body> \n    <h1 style=\"color:green;\">5.1cycle scheduling<\/h1> \n     <font size=\"4\">\ud83e\udd96This approach contrary to other approaches starts by increasing the initial learnig rate \u03b70, growing linearly up a learning rate \u03b71.      \n         \n\ud83e\udd95Then it decreases the learning rate linearly to the initial learning rate \u03b70 again during the second half of training.\n         \n\ud83d\udc09The maximum learning rate \u03b71 is chosen using the same approach we used to find the optimal\nlearning rate.\n\n\ud83e\udd8eThe initial learning rate \u03b70 is chosen to be roughly 10 times lower.","99ad21ac":"# Thank you for reading if u like please upvote\ud83d\ude01","5a65b5f9":"<html> \n \n<body> \n      <h1 style=\"color:green;\">Concluding all<\/h1> \n    \n<font size=\"4\">A 2013 paper by Andrew Senior et al. compared the performance of some of the\nmost popular learning schedules when using momentum optimization to train deep\nneural networks for speech recognition.\n    \n<font size=\"4\">The authors concluded that, in this setting,both performance scheduling and exponential scheduling performed well.\n    \n<font size=\"4\">They favored exponential scheduling because it was easy to tune and it converged slightly faster to the optimal solution.\n    \n<font size=\"4\">But the 1cycle approach seems to perform even better.\n    \n    \nNote: In keras the easiest option is to implement power scheduling(just set the decay hyprparameter)\n    \n    \n    \n    \n    \n    ","5da82f8a":"<font size=\"4.5\">**Concluson**\n\n<font size=\"4\">From the above digram we can see that\n * In this scheduling technique learning rate first drops quickly,then more and more slowly.     \n * It requires tuning of initial learning rate and steps hyper perameter","e34f8413":"<html> \n \n<body> \n    <h1 style=\"color:green;\">4. Performance scheduling<\/h1> \n     <font size=\"4\">\ud83e\udd86This technique measures the validation error every N steps and reduce the learning rate by a factor of lemda when the error stop dropping. <\/font> ","7d3fddd6":"<font size=\"4\">\ud83d\udc25But we can do better than a constant learning rate : ;if we start with a large learnig rate and then reduce it once traing stops making fst progress, you can reach a good solution faster than with the optimal constant learnig rate.  \n<font size=\"4\">\ud83d\udc23There are many differnt strategies to reduce the learnig rate during training.\nThese strategies are called **Learning schedules** ","21945b41":"<html> \n \n<body> \n    <h1 style=\"color:green;\">2.Exponential scheduling<\/h1> \n     <font size=\"4\">\ud83e\udd86In this technique we set the learning rate to  number. <\/font> \n    \n\n<font size=\"4\">**Formula**\n>  <font size=\"4\"> lr = lr0 * 0.1**(epoch \/ s)  \n>     here,  \n>           lr0 ---->  is the initial learning rate  \n>           s   -----> steps     \n \n <font size=\"4\"> \ud83e\udda2The learning rate drops gradually by a factor of 10 every s steps.","61207551":"<html> \n \n<body> \n    <h1 style=\"color:green;\">1.Power scheduling<\/h1> \n     <font size=\"4\">\ud83e\udd86In this technique we set the learning rate to a function of the iteration number. <\/font> \n    \n\n<font size=\"4\">**Formula**\n>  <font size=\"4\"> lr = lr0 \/ (1 + steps \/ s)**c  \n>     here,  \n>           lr0 ---->  is the initial learning rate  \n>           c   -----> power(typically set to 1)  \n>           s   -----> steps     \n   \n <font size=\"4\">\ud83e\udda2The learning rate drops at each step. After ssteps, it is down to lr0\/2 then lr0\/3 after some time lr0\/5 so on.\n     \n     ","903a268c":"<font size=\"4.5\">**Concluson**\n\n    \n<font size=\"4\">From the above digram we can see that\n * In this scheduling technique learning rate  drops by factor of 10 as no. of epochs increase.    \n","4a426778":"<html> \n \n<body> \n    <h1 style=\"color:green;\">3. Piecewise constant scheduling<\/h1> \n     <font size=\"4\">\ud83e\udd86In this technique constant learning rate is used for a number of epoch then a smaller learning rate is used for another number of epochs and so on. <\/font> \n    \n"}}