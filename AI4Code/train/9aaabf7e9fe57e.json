{"cell_type":{"4ee90786":"code","8c7c29a4":"code","47d47f6e":"code","780a6705":"code","9e81b75b":"code","c667244f":"code","ef07f644":"code","474d7231":"code","3c13c7be":"code","6d1910e6":"code","9d18eedc":"code","47ccf812":"code","93720821":"code","7889364f":"code","e4065ccb":"code","c0465d51":"code","cce9d300":"markdown"},"source":{"4ee90786":"!wget \"https:\/\/raw.githubusercontent.com\/aladdinpersson\/Machine-Learning-Collection\/master\/ML\/Pytorch\/more_advanced\/torchtext\/mydata\/train.csv\"\n!wget \"https:\/\/raw.githubusercontent.com\/aladdinpersson\/Machine-Learning-Collection\/master\/ML\/Pytorch\/more_advanced\/torchtext\/mydata\/test.csv\"","8c7c29a4":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport spacy\nfrom torchtext.data import Field, TabularDataset, BucketIterator","47d47f6e":"# steps\n\n# using preprocessing on the data > field\n# load dataset -> TabularDataset\n# iterator for batching and padding > BucketIterator\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nspacy_en = spacy.load(\"en\")","780a6705":"def tokenize(text):\n    return [tok.text for tok in spacy_en.tokenizer(text)]","9e81b75b":"quote = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\nscore = Field(sequential=False, use_vocab=False)\n\nfields = {\"quote\": (\"q\", quote),\"score\": (\"s\",score)}","c667244f":"train_data, test_data = TabularDataset.splits(path=\".\/\", train=\"train.csv\", test=\"test.csv\", format=\"csv\", fields=fields)","ef07f644":"for batch in train_data:\n    print(batch.q)\n    print(batch.s)","474d7231":"quote.build_vocab(train_data, max_size=10000, min_freq=1, vectors=\"glove.6B.100d\")\n\ntrain_iterator, test_iterator = BucketIterator.splits((train_data, test_data), batch_size=2, device=device)","3c13c7be":"for batch in train_iterator:\n    print(batch.q)\n    print(batch.s)\n    break","6d1910e6":"class simpleLSTM(nn.Module):\n    def __init__(self,input_size,embed_size, hidden_size, num_layers):\n        super(simpleLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(input_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n        self.fc1 = nn.Linear(hidden_size,1)\n        \n    def forward(self, x):\n        \n        h0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device=device)\n        c0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device=device)\n        \n        out = self.embedding(x)\n        out, _ = self.lstm(out, (h0, c0))\n        out = self.fc1(out[-1,:,:])\n        \n        return out","9d18eedc":"num_epochs = 5\nlearning_rate = 0.005\ninput_size = len(quote.vocab)\nhidden_size = 256\nnum_layers = 2\nembed_size = 100","47ccf812":"model = simpleLSTM(input_size,embed_size,hidden_size,num_layers).to(device=device)","93720821":"pretrained_embedding = quote.vocab.vectors\nmodel.embedding.weight.data.copy_(pretrained_embedding)","7889364f":"optimizer = optim.Adam(model.parameters(),lr=learning_rate)\nloss_criterion = nn.BCEWithLogitsLoss()","e4065ccb":"for epoch in range(num_epochs):\n    \n    for batch_idx, batch in enumerate(train_iterator):\n        \n        data = batch.q.to(device=device)\n        target = batch.s.to(device=device)\n        \n        preds = model(data)\n        loss = loss_criterion(preds.squeeze(1), target.type_as(preds))\n        \n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n    print(f\"For epoch: {epoch}, loss : {loss}\")","c0465d51":"for batch in train_iterator:\n    print(f\"For input {batch.q}\")\n    preds = model(batch.q.to(device=device))\n    \n    for p in preds.squeeze(0):\n        print(quote.vocab[p])\n        \n    print(f\"Output {preds}\")","cce9d300":"Taken from : wonderful tutorial by Alladin perrson: https:\/\/www.youtube.com\/watch?v=KRgq4VnCr7I&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=34"}}