{"cell_type":{"3cb0d0c7":"code","f1979225":"code","d638f9bd":"code","b6ecf778":"code","7f1c0b1d":"code","6906f690":"code","9a90040f":"code","4f1366f9":"code","84393ac8":"code","25a4b60e":"code","b9b68f30":"code","c56777cc":"code","2f3bea42":"code","f165fda7":"code","61d9a38b":"code","30ab2cff":"code","d5323e61":"code","b3b4b8ec":"code","1e0b743d":"code","3428905b":"code","f0520fe9":"code","0c6cb528":"code","cf31069d":"code","4ddf8bd8":"code","08e8a27b":"code","9ce04add":"code","7b71cdd5":"code","1261c730":"code","92cbc13c":"code","1d4849f6":"code","59f2cf2e":"code","f613d1c7":"code","38c54dbc":"code","38f233c3":"code","b707d692":"code","f1d8716b":"markdown","6b3a6685":"markdown","52614267":"markdown","7ce0d8df":"markdown","3e3c4cfb":"markdown","681c468e":"markdown","0f8badd1":"markdown","9953b382":"markdown","91e9e31e":"markdown","ff5615a8":"markdown","24196070":"markdown","7ab27add":"markdown","97dfacbd":"markdown","cc62cc05":"markdown","4c3bee76":"markdown","9194b2ad":"markdown","fd5afb52":"markdown"},"source":{"3cb0d0c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # visualization\n!pip install seaborn as sns -q --upgrade pip # visualization with seaborn v0.11.1\nimport seaborn as sns # visualization\nimport missingno as msno # missing values pattern visualization\n#set seed\nimport random\nseed = 41\n!pip install dabl -q --upgrade pip\nimport dabl # quick exploration and model assessment\n\n\n\nimport warnings # supress warnings\nwarnings.filterwarnings('ignore')\n\n# set pandas display option\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n\n\n\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f1979225":"# Load the data \ntrain_df = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntest_df = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')\ndata_dictionary_df = pd.read_csv(\"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")\n\n# Drop first column because it is identical to index\ntrain_df.drop(columns = ['Unnamed: 0', 'encounter_id', 'hospital_id'], axis = 1, inplace = True)\ntest_df.drop(['Unnamed: 0', 'encounter_id', 'hospital_id'], axis = 1, inplace = True)\n\n# display the dataset\ntrain_df.head().style.set_caption('Sample of training data')","d638f9bd":"train_df.shape, test_df.shape","b6ecf778":"# let's inspect the variable values\nfor var in train_df.columns:\n    print(var, train_df[var].unique()[0:20], '\\n')","7f1c0b1d":"# make list of variables  types for train set\n\n# numerical: discrete vs continuous\n\ncontinuous = [var for var in train_df.columns if train_df[var].dtype!='object' and var!='diabetes_mellitus' and train_df[var].nunique()>10]\n\n# categorical\ncategorical = [var for var in train_df.columns if train_df[var].dtype =='object' or train_df[var].nunique()<=3 and var!='diabetes_mellitus']\n\ndiscrete = [var for var in train_df.columns if train_df[var].dtype!='object' and train_df[var].nunique()>3 and var not in continuous ]\n\n\nprint('There are {} discrete variables'.format(len(discrete)))\nprint('There are {} continuous variables'.format(len(continuous)))\nprint('There are {} categorical variables'.format(len(categorical)))\n","6906f690":"# make list of variables  types for test set\n\n# numerical: discrete vs continuous\n\ncont = [var for var in test_df.columns if test_df[var].dtype!='object' and var!='diabetes_mellitus' and test_df[var].nunique()>10]\n\n# categorical\ncat = [var for var in test_df.columns if test_df[var].dtype =='object' or test_df[var].nunique()<=3 and var!='diabetes_mellitus']\n\ndis = [var for var in test_df.columns if test_df[var].dtype!='object' and test_df[var].nunique()>3 and var not in cont ]\n\n\nprint('There are {} discrete variables'.format(len(dis)))\nprint('There are {} continuous variables'.format(len(cont)))\nprint('There are {} categorical variables'.format(len(cat)))","9a90040f":"def checker(lis, df1, df2):\n    for var in lis:\n        unique_to_df1 = [\n            x for x in df1[var].unique() if x not in df2[var].unique()\n        ]\n    print(f'There are {len(unique_to_df1)} categorical values unique to train set') \n    return unique_to_df1","4f1366f9":"unique_to_train = checker(cat, train_df, test_df)\nunique_to_train","84393ac8":"cat","25a4b60e":"shallow_copy = train_df.copy()","b9b68f30":"def missing_zero_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = round(df.isnull().mean().mul(100), 2)\n        mz_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mz_table = mz_table.rename(\n        columns = {df.index.name:'col_name', 0 : 'Missing Values', 1 : '% of Total Values'})\n        mz_table['Data_type'] = df.dtypes\n        mz_table = mz_table[mz_table.iloc[:,1] != 0 ].sort_values('% of Total Values', ascending=False)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n There are \" + str(mz_table.shape[0]) + \" columns that have missing values.\")\n        return mz_table.reset_index()\nmissing = missing_zero_values_table(shallow_copy[categorical])\nmissing.style.background_gradient(cmap='Reds')","c56777cc":"missing_tr = missing_zero_values_table(train_df[categorical])\ncol_to_drop = list(missing_tr[missing_tr['% of Total Values'] > 50]['index'])\ncol_to_drop","2f3bea42":"missing = missing_zero_values_table(test_df[categorical])\nmissing.style.background_gradient(cmap='Reds')","f165fda7":"# plot number of categories per categorical variable\n\nfig = test_df[categorical].nunique().plot.bar(figsize=(10,6))\n# plot a threshold of categories that has only one sub-category\nfig.axhline(y=1, color='red')\nplt.title('CARDINALITY: Number of categories in categorical variables in the test set')\nplt.xlabel('Categorical variables')\nplt.ylabel('Number of different categories')","61d9a38b":"def fill_na(df):\n    for col in categorical:\n        if df[col].dtypes == 'object': \n            df[col].fillna('missing', inplace = True)\n        elif df[col].dtypes != 'object': \n            df[col] = df[col].astype('str')\n            df[col].fillna('missing', inplace = True)\n    return df","30ab2cff":"group_miss = fill_na(shallow_copy)","d5323e61":"missing = missing_zero_values_table(group_miss[categorical])\nmissing.style.background_gradient(cmap='Reds')","b3b4b8ec":"# for each categorical variable\nfig = plt.figure(figsize=(5, 7))\nfor col in categorical:\n    t = sns.catplot(data = group_miss, x = col, hue = 'diabetes_mellitus', kind = 'count')\n    t.set_xticklabels(rotation=90)\n    plt.show()","1e0b743d":"# plot number of categories per categorical variable\n\nfig = group_miss[categorical].nunique().plot.bar(figsize=(10,6))\n# plot a threshold of categories that has only one sub-category\nfig.axhline(y=1, color='red')\nplt.title('CARDINALITY: Number of categories in categorical variables after adding missing label')\nplt.xlabel('Categorical variables')\nplt.ylabel('Number of different categories')","3428905b":"total_labels = len(group_miss)\n\n# for each categorical variable\nfor col in categorical:\n    # calculate how many times the label is repeated in the whole dataset\n    temp_df = pd.Series(group_miss[col].value_counts() \/ total_labels)\n\n    # make plot with the above percentages\n    fig = temp_df.sort_values(ascending=False).plot.bar()\n    fig.set_xlabel(col)\n\n    # add a line at 5 % to flag the threshold for rare categories\n    fig.axhline(y=0.10, color='red')\n    fig.set_ylabel(f'Percentage of {col}')\n    plt.show()","f0520fe9":"# Number of unique classes in each object column\ngroup_miss[categorical].apply(pd.Series.nunique, axis = 0)","0c6cb528":"# I will replace all the labels that appear in less than 2-time\n# of the train_df by the label 'rare'\n\n\ndef group_rare_labels(df, var, tresh):\n\n    total_obs = len(df)\n\n    # first I calculate the % of obs for each category\n    temp_df = pd.Series(df[var].value_counts() \/ total_obs)\n\n    # now I create a dictionary to replace the rare labels with the\n    # string 'rare' if they are present in less than 5% of obs\n\n    grouping_dict = {\n        k: ('rare' if k not in temp_df[temp_df >= tresh].index else k)\n        for k in temp_df.index\n    }\n\n    # now I replace the rare categories\n    tmp = df[var].map(grouping_dict)\n\n    return tmp","cf31069d":"def group_rar(df):\n    temp = df.copy()\n    for col in categorical:\n        temp[col] = group_rare_labels(temp, col, 0.10)\n    return temp","4ddf8bd8":"# for each categorical variable\ngroup_rare = group_rar(group_miss)","08e8a27b":"group_rare[categorical].apply(pd.Series.nunique, axis = 0)","9ce04add":"group_rare.head()","7b71cdd5":"\nfig = group_rare[categorical].nunique().plot.bar(figsize=(10,6))\n# plot a threshold of categories that has only one sub-category\nfig.axhline(y=1, color='red')\nplt.title('CARDINALITY: Number of categories in categorical variables')\nplt.xlabel('Categorical variables')\nplt.ylabel('Number of different categories')","1261c730":"total_labels = len(group_rare)\n\n# for each categorical variable\nfor col in categorical:\n\n    temp_df = pd.Series(group_rare[col].value_counts() \/ total_labels)\n\n    # make plot with the above percentages\n    fig = temp_df.sort_values(ascending=False).plot.bar()\n    fig.set_xlabel(col)\n\n    # add a line at 5 % to flag the threshold for rare categories\n    fig.axhline(y=0.10, color='red')\n    fig.set_ylabel(f'Percentage of {col}')\n    plt.show()","92cbc13c":"# Number of unique classes in each object column\ngroup_rare[categorical].apply(pd.Series.nunique, axis = 0)","1d4849f6":"def find_category_mappings(df, variable, target):\n\n    # copy of the original dataframe, so we do not accidentally\n    # modify it\n    tmp = df.copy()\n\n    # total positive class\n    total_pos = df[target].sum()\n\n    # total negative class\n    total_neg = len(df) - df[target].sum()\n\n    # non target\n    tmp['non-target'] = 1 - tmp[target]\n\n    # % of positive class per category, respect to total positive class\n    pos_perc = tmp.groupby([variable])[target].sum() \/ total_pos\n\n    # % of negative class per category, respect to total negative class\n    neg_perc = tmp.groupby([variable])['non-target'].sum() \/ total_neg\n\n    # let's concatenate\n    prob_tmp = pd.concat([pos_perc, neg_perc], axis=1)\n\n    # let's calculate the Weight of Evidence\n    prob_tmp['woe'] = np.log(prob_tmp[target]\/prob_tmp['non-target'])\n    \n    return prob_tmp['woe'].to_dict()\n","59f2cf2e":"def apply_encode(df):\n    temp = df.copy()\n    for variable in categorical:\n\n        mappings = find_category_mappings(temp, variable, 'diabetes_mellitus')\n\n        temp[variable] = temp[variable].map(mappings)\n    return temp\n\nwoe_encode = apply_encode(group_rare)\nwoe_encode.head()","f613d1c7":"# first in the train set\nfor var in categorical:\n\n    fig = plt.figure()\n    fig = woe_encode.groupby([var])['diabetes_mellitus'].mean().plot()\n    fig.set_title('Monotonic relationship between {} and diabetes_mellitus'.format(var))\n    fig.set_ylabel('Mean positive diabetes')\n    plt.show()","38c54dbc":"def get_IV(df, feature, target):\n    lst = []\n\n    # optional\n    df[feature] = df[feature].fillna(\"NULL\")\n\n    unique_values = df[feature].unique()\n    for val in unique_values:\n        lst.append([feature,                                                        # Feature name\n                    val,                                                            # Value of a feature (unique)\n                    df[(df[feature] == val) & (df[target] == 0)].count()[feature],  # Good (diabetes_mellitus == 0)\n                    df[(df[feature] == val) & (df[target] == 1)].count()[feature]   # Bad  (diabetes_mellitus == 1)\n                   ])\n \n    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'Good', 'Bad'])\n\n        \n    total_bad = df[df[target] == 1].count()[feature]\n    total_good = df.shape[0] - total_bad\n    \n    data['Distribution Good'] = data['Good']\/ total_good\n    data['Distribution Bad'] = data['Bad'] \/ total_bad\n    data['WoE'] = np.log(data['Distribution Good'] \/ data['Distribution Bad'])\n\n    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n\n    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])\n\n    data = data.sort_values(by=['Variable', 'Value'], ascending=[True, True])\n    data.index = range(len(data.index))\n\n    iv = data['IV'].sum()\n\n    return data","38f233c3":"test_woe = group_rare.copy()","b707d692":"\nt = [get_IV(test_woe, col, 'diabetes_mellitus') for col in categorical]\n\nt","f1d8716b":"Now let's see how much sub-categories per category we got","6b3a6685":"<hr>\n<hr>","52614267":"# Encode categorical variables\nI will use the WOE encoding to preserve a monotonic relationship with the target variable, as I am intended to logistic regression algorithm family. \nWOE has the benefit that it encode the categorical variables based on their good\/bad events.\n\n### The advantages of WOE transformation are\n* Handles missing values\n* Handles outliers\n* The transformation is based on logarithmic value of distributions. This is aligned with the logistic regression output function\n* No need for dummy variables\n* By using proper binning technique, it can establish monotonic relationship (either increase or decrease) between the independent and dependent variable","7ce0d8df":"There are some labels that might be considered to drop to reduce cardinality of the categorical variables. Also, There are some variables that has only one label, we may consider to drop them too. However, will decide later when we compare algorithms after fully process the data and split them into train and test!","3e3c4cfb":"We need to figure out a way to calculate the `IV` to see what vars are significant to the target variable","681c468e":"# Preprocessing steps for categorical variables\n\nThe aim from this notebook is to infer some insights out of the categorical features by trying different techiques that may enhance the predictive performance. \nThis step may seem tedious, long, and tedious, but it would save plenty of times when we start building the model based on different algorithms. \n\nIn this notebook, we will do the following:\n1. Explore train and test set, number of labels per each dataset, check cardinality, check labels similarity\n2. Extract categorical, discrete, and continuous features for easy processing\n3. Fix categorical types (int64 -> obj) \n4. Add missing indicators for the categorical variables\n5. Group rare labels under one united label to reduce cardinality\n6. Encode categorical variables using weight of evidence WOE\n7. Extract the information value IV per each category to select most important features. \n\nThis notebook is only exploring the potential methods that may have potential to enhance the performance. In later notebookss, I will use the `train_test_split` to split the datasets before making any processing. \n","0f8badd1":"# Work in progress ","9953b382":"For each of the categorical variables, some labels appear in less than 5% of and many appear in less than 5% These are infrequent labels or Rare Values and could cause over-fitting. Also, missing values in some of the variables have a significant number.","91e9e31e":"`hospital_admit_source` and `ethnicity` have dramatically decreased!!let's viisualize them again","ff5615a8":"--------\n<hr>","24196070":"# Reduce cardinality\nGroup rare labels into one category - apply to train, then will do again to test data when finalize the preprocess with the pipeline step.","7ab27add":"Same missing categories in the test set too","97dfacbd":"If data are not missing at random, it is a good idea to replace missing observations by the mean \/ median \/ mode AND flag those missing observations as well with a Missing Indicator. A Missing Indicator is an additional binary variable, which indicates whether the data was missing for an observation (1) or not (0).","cc62cc05":"# Group missing values\nArbitrary value imputation for categorical variables\nThis is the most widely used method of missing data imputation for categorical variables. This method consists in treating missing data as an additional label or category of the variable. All the missing observations are grouped in the newly created label 'Missing'.\n\nThis is in essence, the equivalent of replacing by an arbitrary value for numerical variables.\n\nThe beauty of this technique resides on the fact that it does not assume anything about the fact that the data is missing. It is very well suited when the number of missing data is high.\n\nAdvantages\nEasy to implement\nFast way of obtaining complete datasets\nCan be integrated in production (during model deployment)\nCaptures the importance of \"missingness\" if there is one\nNo assumption made on the data\nLimitations\nIf the number of NA is small, creating an additional category may cause trees to over-fit\nFor categorical variables this is the method of choice, as it treats missing values as a separate category, without making any assumption on the variable or the reasons why data could be missing. ","4c3bee76":"Check if there are any unique values in train that doesn't exist in test","9194b2ad":"since all the categorical variables have a value less than 0.02, which means that those variables would no useful to the prediction model.  [source](https:\/\/www.listendata.com\/2019\/08\/WOE-IV-Continuous-Dependent.html) ","fd5afb52":"<hr>\n<hr>"}}