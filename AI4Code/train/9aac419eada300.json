{"cell_type":{"e126ae6e":"code","1d43c84a":"code","efa6c56c":"code","21f74125":"code","fedc9dae":"code","67a0aad5":"code","3e765162":"code","bc46b6aa":"code","b1ea8904":"code","7648688e":"code","872c2a69":"code","3ce8edf3":"code","dc9e0678":"code","01947c5b":"code","18c05bfc":"code","031ca942":"code","1043288f":"code","f5c8e557":"code","03a7de48":"code","4cf96297":"code","c0e5f9ff":"code","f3487055":"code","91ce7ddc":"code","fe2a087c":"code","d8b48572":"code","41b73559":"markdown","d90d9f50":"markdown","f8a0d317":"markdown","34e308c4":"markdown","c3c98b42":"markdown","b9dad59d":"markdown","6fce51bc":"markdown","9d859260":"markdown","7b8a9e2f":"markdown","489ddd63":"markdown","b8dff569":"markdown","b8c8c5c6":"markdown","1b3c0096":"markdown","0f2ad0d0":"markdown","0b670811":"markdown","75fd97b6":"markdown","d58e173c":"markdown","74f6d0f9":"markdown","9fba9ee9":"markdown","2bc88d14":"markdown","43cbc862":"markdown","b4f08a46":"markdown","f43e3fbc":"markdown","79dcb4aa":"markdown","3724b14f":"markdown","92d05e84":"markdown","a4259026":"markdown","d4bb347c":"markdown","39d6e99c":"markdown","86271d91":"markdown","36a3a802":"markdown","4f22187a":"markdown"},"source":{"e126ae6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport regex as re\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1d43c84a":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","efa6c56c":"df.head()","21f74125":"print(df.isnull().sum())\nsns.countplot(df['target'])\n","fedc9dae":"key1 = df[df['target'] == 1]['keyword'].value_counts()\nf, ax = plt.subplots(figsize=(6, 55))\nsns.barplot(key1.values,key1.index,\n            label=\"Keyword\", color=\"b\")\nsns.despine(left=True, bottom=True)","67a0aad5":"df[df['target'] == 1]['keyword'].isnull().sum()","3e765162":"key2 = df[df['target'] == 0]['keyword'].value_counts()\nf, ax = plt.subplots(figsize=(6, 55))\nsns.barplot(key2.values,key2.index,\n            label=\"Keyword\", color=\"g\")\nsns.despine(left=True, bottom=True)","bc46b6aa":"df['text'] = df['text'].replace('http:\/\/\\w+.\\w+\/\\w+',' ',regex = True)\n\ndf['text'] = df['text'].replace('https:\/\/\\w+.\\w+\/\\w+',' ',regex = True)\n\ndf['text'] = df['text'].replace('http:\/\/\\w*.?\\w*\/?\\w+',' ',regex = True)\n\ndf['text'] = df['text'].replace('@\\w+',' ',regex = True)\n\ndf['text'] = df['text'].replace('\\d+',' ',regex = True)\n\ndf['text'] = df['text'].replace('[!@#$%^&*()_+\\-\":;.,\/?\\=}{}\"<>~`]',' ',regex = True)\n\ndf['text'] = df['text'].replace(r'\\s{2:}',' ',regex = True)","b1ea8904":"df['text'].head()","7648688e":"target1 = df[df['target'] == 1]\ntarget0 = df[df['target'] == 0]","872c2a69":"string = ' '.join(list(target1['text'].values))\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color=\"white\",max_words =200,stopwords = stopwords,random_state=42,max_font_size = 100)\nwordcloud.generate(string)\nplt.figure(figsize=(24,29))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout()","3ce8edf3":"string = ' '.join(list(target0['text'].values))\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color=\"white\",max_words =200,stopwords = stopwords,random_state=42)\nwordcloud.generate(string)\nplt.figure(figsize=(24,16))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout()","dc9e0678":"df_main = df[['text','target']] \nX = df_main['text']\ny = df_main['target']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.15,random_state = 42)","01947c5b":"classifiers = [\n    KNeighborsClassifier(3),\n    LinearSVC(),\n    SVC(kernel=\"rbf\"),\n    NuSVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    MultinomialNB()\n    ]","18c05bfc":"for classifier in classifiers:\n    text_clf = Pipeline([('tfidf',TfidfVectorizer()),('clf',classifier)])\n    text_clf.fit(X_train,y_train)\n    predictions = text_clf.predict(X_test)\n    print(classifier)\n    print(accuracy_score(y_test,predictions))","031ca942":"parameters = {\n    'tfidf__stop_words':('english',None),\n    'tfidf__max_df':(0.5,0.75,1),\n    'tfidf__max_features': (None, 5000, 10000, 50000),\n    'clf__max_iter':(800,1000,2000,4000),\n    'clf__C':(0.1,0.5,1,2)\n    \n}","1043288f":"pipeline = Pipeline([('tfidf',TfidfVectorizer()),('clf',LinearSVC())])","f5c8e557":"grid = GridSearchCV(pipeline,parameters,n_jobs=-1, verbose=1)\ngrid.fit(X_train, y_train)","03a7de48":"grid.best_estimator_.get_params()","4cf96297":"text_clf = Pipeline([('tfidf',TfidfVectorizer(stop_words=None,max_df=0.5,max_features = 10000)),('clf',LinearSVC(max_iter=800,C=0.1))])","c0e5f9ff":"text_clf.fit(X_train,y_train)","f3487055":"predictions = text_clf.predict(X_test)","91ce7ddc":"print(\"Confusion matrix\\n\",confusion_matrix(y_test,predictions))","fe2a087c":"print(\"Classification Report\\n\",classification_report(y_test,predictions))","d8b48572":"print(\"Accuracy Score\\n\",accuracy_score(y_test,predictions))","41b73559":"# What pattern keywords help us to find? <a id='section5'><\/a>","d90d9f50":"From the parameters we provided the best selected are as under: <br><br>\n<h6>For tfidfvectorizer<\/h6>\n<li>stop_words=None<\/li>\n<li>max_df=0.5<\/li>\n<li>max_features = 10000<\/li>\n\n<h6>For our Model LinearSVC<\/h6>\n<li>max_iter=800<\/li>\n<li>C=0.1<\/li>\n\n","f8a0d317":"We have removed urls, symbols, numbers, tags and other unwanted things from raw tweets as they provide no value to us. ","34e308c4":"# Performing EDA","c3c98b42":"* [What you will learn? ](#section1)\n* [Importing ](#section2)\n* [Performing EDA ](#section3)\n* [Which type of tweets are more & how much? ](#section4)\n* [What pattern keywords help us to find? ](#section5)\n* [Cleaning Tweets ](#section6)\n* [Finding most used words in disaster & non-disaster tweets using Wordcloud ](#section7)\n* [Splitting the dataset into train & test set ](#section8)\n* [Model Selection ](#section9)\n* [Hyperparameter-tuning ](#section10)\n* [Fitting tuned model ](#section11)\n* [Prediction ](#section12)\n* [Complete Evaluation ](#section13)","b9dad59d":"Highest Accuracy is given by LinearSVC algorithm without any hyperparameter tuning. So we will move forward with it. Now let's tune it's hyperparameter and make our accuracy better.","6fce51bc":"# Importing <a id='section2'><\/a>","9d859260":"# Hyperparameter-tuning <a id='section10'><\/a>","7b8a9e2f":"# Loading Dataset <a id='section3'><\/a>","489ddd63":"# Prediction <a id='section12'><\/a>","b8dff569":"# Model Selection <a id='section9'><\/a>","b8c8c5c6":"Finding most used keywords to label non-disaster tweets.","1b3c0096":"# Complete Evaluation <a id='section13'><\/a>","0f2ad0d0":"# What you will learn? <a id='section1'><\/a>","0b670811":"<h1 style=\"color:red\">Don't forget to upvote my notebook if you found it useful<\/h1>","75fd97b6":"# Splitting the dataset into train & test set. <a id='section8'><\/a>","d58e173c":"This shows the number of times different keywords are used to label the disaster-tweet. Keywords like outbreak, suicude_bomb are used more number of times. Which shows that such disaster events may had happen more than other.","74f6d0f9":"**NOTE: You can try more by tuning other parameters. I have just tuned few to show you how to do hyperparameter tuning for increasing the accuracy.**","9fba9ee9":"This notebook will help you to learn:\n\n<li>How to use Pipeline in ML specially in NLP to make your work too easy and simple<\/li>\n<li>Basic EDA performed while analysing the text<\/li>\n<li>How to remove unwanted words, symbols and other things from text using regular expression<\/li>\n<li>How to use Wordcloud to make our search for most used words easy while performing analysing.<\/li>\n<li>Hyperparameter tuning with Pipeline<\/li>\n<li>How to check accuracy of different Machine Learning Algoritms (models) simultaneously and that too fast with Pipeline.<\/li>","2bc88d14":"We tested multiple options on the selected model. Now it's time to see the best hyperparameters that fit our model for highest accuracy.","43cbc862":"# Thanks Giving <a id='section13'><\/a>","b4f08a46":"NOTE: Check what's the accuracy of the same model without tuning hyperparameters when we selected our model. This shows that tuning hyperparameters is must.","f43e3fbc":"***Congratulations! You learned alot many things to make your project accurate.***","79dcb4aa":"This shows that most of the disasters may have happened because of fire, storm, suicide, car accidents, flood and much more as this words are highly used in many tweets. And one more pattern it brings is most of may have happened in California.","3724b14f":"# Finding most used words in disaster & non-disaster tweets using Wordcloud <a id='section7'><\/a>","92d05e84":"# Which type of tweets are more & how much? <a id='section4'><\/a>","a4259026":"Learning from others work is the best tool for mastering anything.\n<h3>Your learning tour begins!<\/h3>","d4bb347c":"Defining the pipeline. First the data will get pass into tfidvectorizer whose output will be fitted in the model.","39d6e99c":"# Cleaning tweets <a id='section6'><\/a>","86271d91":"Thanks for learning from my notebook. Much Appreciated!","36a3a802":"Their are most Non-Disastrous tweets and thats intuitive. Disasters are rarely prevailed. If not that it's a point to be considered.","4f22187a":"# Fitting in the tuned model <a id='section11'><\/a>"}}