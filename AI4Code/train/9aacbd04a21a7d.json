{"cell_type":{"0beeaf70":"code","3ac33120":"code","904ac59a":"code","3c3af5a5":"code","7cb19cb5":"code","3afbb0e4":"code","9fa36be7":"code","62b5a233":"code","e6bcedcf":"code","299c275b":"code","a77b5230":"code","5a2b0ab6":"code","fca8501d":"code","f3ed42d0":"code","a3788140":"code","a07653e8":"code","166976eb":"code","6e7e9da9":"code","2e72e86c":"code","17a02e4f":"code","68c9615f":"code","d0282dda":"code","81872a0f":"code","8b31c4c6":"code","9b985c4a":"code","dcd73beb":"code","b7200c10":"code","b24d490b":"code","13b64076":"code","f5925dc6":"code","189fd648":"code","b48cf5ab":"code","c54aabee":"code","7a6c680a":"code","4c2ce0ac":"code","b61fe6f1":"code","94936498":"code","44afc30e":"code","5687da06":"code","957b9469":"code","fc26c4fd":"code","dc6a4367":"code","3fe4a45f":"code","b8f67b68":"code","126676e4":"code","b4909869":"code","79f8d255":"code","340ab753":"code","8dbaacb2":"code","684c992a":"code","be28925c":"code","9836883b":"code","42666312":"code","48dd33c3":"code","1249e5cc":"code","6aa9c74e":"code","1fc76afb":"code","8cfe0fa7":"code","5d717bb0":"code","86fc2d58":"code","4bb97125":"code","da600184":"code","9e17965e":"code","93acfe00":"code","c51c0148":"code","4fa039e7":"code","94e01008":"code","fb830351":"code","be1636db":"code","62e8ca08":"code","205ed86a":"code","fc3cef5f":"code","10f8fea4":"code","35e15fd0":"code","3803ccc6":"code","925ce67f":"code","fcb3808b":"code","7454eb61":"code","c50462e4":"code","80863a66":"code","3ba75935":"code","fefc31db":"code","ec1e3e30":"code","7ee1dad4":"code","80ff1058":"code","f55f13f1":"code","7b7be082":"code","070d1fa6":"code","eb9b506a":"code","b9a51853":"code","e20751e3":"markdown","b5cd638b":"markdown","f6e8a0ff":"markdown","c3e56eaf":"markdown","d39a51db":"markdown","5be06a45":"markdown","8b4a64d8":"markdown","1e156c76":"markdown","ee426f49":"markdown","a91a0771":"markdown","33a4e5ca":"markdown","9c105ef9":"markdown","e16b61de":"markdown","48a5108d":"markdown","29b4ff7b":"markdown","0f88592e":"markdown","8002118d":"markdown","06924c51":"markdown","d31935e8":"markdown","c597591f":"markdown","09713ed1":"markdown","07a9f373":"markdown","bc1e640a":"markdown","4aa40e63":"markdown","48debb06":"markdown","d88ac9cf":"markdown","01182d05":"markdown","26a0b1b4":"markdown"},"source":{"0beeaf70":"#--DATA CLEANING --#\n#refer to Indexed report for Commentary\n# conda create --name workshop\n# conda activate workshop\n# conda install folium \n# conda install seaborn\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n","3ac33120":"\ndf_0 = pd.read_csv('..\/input\/barcelonaairbnbgeojson\/listings.csv')\nnum_rows = len(df_0['id'])\ndf_0.head(5)\n\n# trouble with only displaying some columns or rows?\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)\n\n\n# scraped on sept19, 2019 from insideairbnb.com","904ac59a":"display(df_0.describe())","3c3af5a5":"display(df_0.isnull().sum())","7cb19cb5":"#Remove Majority Null Columns\n\ncolsToDrop = []\nfor col in df_0.columns:\n    if df_0[col].isnull().sum() > (.6 * num_rows):\n        colsToDrop.append(col)  \nprint(f'Number of columns to be dropped: {len(colsToDrop)}')\nfor col in colsToDrop:\n    print(col)\ndf_0.shape","3afbb0e4":"df_1 = df_0.drop(colsToDrop, axis=1)\n\ndf_1.shape","9fa36be7":"# Remove Columns with One Unique Value\n# Why? For example, country. If all data is in the same country, we don't need 20404 rows that say Spain\ncolsToDrop = []\n\nfor col in df_1.columns:\n    if df_1[col].nunique() == 1:\n        colsToDrop.append(col)\n        \nfor col in colsToDrop:\n    print(col)\ndf_1.shape\n","62b5a233":"# Reassign to df_2 variable\n\ndf_2 = df_1.drop(colsToDrop, axis = 1)\ndf_2.shape","e6bcedcf":"# what data types make up our dataframe?\n# object == string\n\ndf_2.dtypes","299c275b":"# As previously seen, pandas' nunique function counts the number of unique values in a column\ndisplay(df_2.nunique())","a77b5230":"# Bulk Removal of Redundant Columns and String Columns\n\n# Some of these string columns may be helpful for categorical analysis and language processing, \n# but for the purpose of this workshop we will leave them out.\n# \n\n\ndf_3 = df_2.drop(['listing_url', 'last_scraped', 'name', 'summary', 'space', 'description',\n                 'neighborhood_overview', 'notes', 'transit', 'access', 'interaction', 'house_rules', 'picture_url',\n                 'host_url', 'host_name', 'host_since', 'host_location', 'host_about', 'host_thumbnail_url', \n                  'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', 'host_response_time', \n                  'host_response_rate', 'street', 'host_verifications', 'host_picture_url', 'amenities', 'calendar_updated', \n                  'calendar_last_scraped', 'availability_30', 'availability_90', 'availability_60','neighbourhood', \n                  'smart_location', 'is_location_exact', 'first_review', 'last_review', 'license','minimum_minimum_nights',\n                  'maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights','minimum_nights_avg_ntm',\n                  'maximum_nights_avg_ntm','calculated_host_listings_count_entire_homes',\n                  'calculated_host_listings_count_shared_rooms','calculated_host_listings_count_private_rooms'], axis =1)\nprint(f'Before removing redundant values: {df_2.shape}')\nprint(f'After removing redundant values:  {df_3.shape}')","5a2b0ab6":"# Look at what data remains, along with how many unique values there are\ndf_3.nunique()","fca8501d":"df_3 = df_3.set_index('id')","f3ed42d0":"df_3.head(20)","a3788140":"# Why? To visualize the proportions of data relative to each other\ndf_3.room_type.value_counts(normalize=True)","a07653e8":"df_3.cancellation_policy.value_counts(normalize=True)\n\n","166976eb":"df_3.neighbourhood_group_cleansed.value_counts(normalize=True)\n        ","6e7e9da9":"df_3.bed_type.value_counts(normalize=True)","2e72e86c":"# Almost everybody has a real bed, let's remove this variable\ndf_4 = df_3.drop(['bed_type', 'review_scores_accuracy',\n'review_scores_cleanliness',\n'review_scores_checkin',\n'review_scores_communication',\n'review_scores_location',\n'review_scores_value',\n'state',\n'zipcode',\n'market',\n], axis = 1)","17a02e4f":"df_4.isnull().sum()\n#must parse strings and change variable type later\n\n","68c9615f":"# Returns every row where the 'availability_365' column equals 0.\ndf_4.loc[df_4['availability_365'] == 0].head()","d0282dda":"# Removal of Inactive Listings\n\ndf_4 = df_4[df_4['availability_365'] != 0] \n","81872a0f":"#Parsing Floats from Price Columns\n\ndf_5 = df_4\ndf_6 = df_5\ndf_6['price'] = df_6['price'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_6['security_deposit'] = df_6['security_deposit'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_6['cleaning_fee'] = df_6['cleaning_fee'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_6['extra_people'] = df_6['extra_people'].str.replace('$', '').str.replace(',', '').astype(float)","8b31c4c6":"df_6['security_deposit'].fillna(0, inplace = True)\ndf_6['cleaning_fee'].fillna(0, inplace = True)\ndf_6['extra_people'].fillna(0, inplace = True)\ndf_6['review_scores_rating'].fillna(0, inplace = True)\ndf_6['reviews_per_month'].fillna(0, inplace = True)","9b985c4a":"# Review the remaining data\ndf_6.head()","dcd73beb":"df_7 = df_6.drop(['review_scores_rating', 'number_of_reviews', 'number_of_reviews_ltm'], axis=1)\n","b7200c10":"df_7.shape","b24d490b":"#PRICE LIMIT SETTING AND PRICE DISTRIBUTION\ndf_7.price.describe()","13b64076":"\n\n\ndf_7[df_7.price > 1000].head()","f5925dc6":"df_8 = df_7.drop(df_7[df_7.price > 650].index, axis=0)\ndf_8 = df_8.drop('maximum_nights', axis=1)\ndf_8.price.describe()\n\n#df_7 has prices above 500, df_8 does not. there are about 500 listings priced above 500","189fd648":"import seaborn as sns\nplt.figure(figsize=(10,10))\nsns.violinplot(x=df_8.price, palette = \"Set3\").set_title(\"Price Distribution (removed listings $650 and above)\", size=16)\nsns.despine","b48cf5ab":"df_8.shape","c54aabee":"df_8.isnull().sum()\n#because there are so few, we will investigate the cases where there is null","7a6c680a":"null_cases = df_8[df_8.isnull().any(axis=1)]\nnull_cases.head(25)","4c2ce0ac":"null_cases.info()","b61fe6f1":"df_9 = df_8.dropna()\ndf_9.shape","94936498":"df_9.head()","44afc30e":"df_9.isnull().sum()","5687da06":"df_9.nunique()","957b9469":"# Explain the parts of the violin chart:\n# Curve = Normal Disribution\n# Bottom of veritcal line = Smallest value\n# Top of vertical line = Highest value\n# Bottom of the middle box = First quartile\n# Top of the middle box = Third quartile\n# White dot = mean\nplt.figure(figsize=(20,10))\nsorted_room = df_9.groupby(['room_type'])['price'].median().sort_values()\nsns.violinplot(x=df_9.room_type, y=df_9.price, order=list(sorted_room.index)).set_title(\"Price by Room Type\", size=16)\n#sns.despine","fc26c4fd":"\nplt.figure(figsize=(20,10))\n\nsorted_room = df_9.groupby(['room_type'])['reviews_per_month'].median().sort_values()\nsns.violinplot(x=df_9.room_type, y=df_9.reviews_per_month, order=list(sorted_room.index)).set_title(\"Reviews Per Month by Room Type\", size=16)\nsns.despine","dc6a4367":"# Explain skew\ndf_9.skew()","3fe4a45f":"df_9['minimum_nights'][df_9['minimum_nights'] > 365] = 366","b8f67b68":"#Predictor variables and Skewed Distribution\n\n\nf, axes = plt.subplots(2, 4, figsize=(15, 15))\nsns.despine(left=True)\n\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.price, ax = axes[0,0])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.minimum_nights, ax = axes[1,0])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.calculated_host_listings_count, ax = axes[0,1])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.accommodates, ax = axes[1,1])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.reviews_per_month, ax = axes[0,2])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.bedrooms, ax = axes[0,3])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.bathrooms, ax = axes[1,3])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.extra_people, ax = axes[1,2])\n\n\ndf_9.skew(axis=0)","126676e4":"df_9.nunique()\n","b4909869":"df_9.describe()","79f8d255":"df_9.minimum_nights.describe()","340ab753":"df_9.calculated_host_listings_count.describe()","8dbaacb2":"# BINNING VARIABLES (drop beds because we have bedrooms)\n\ndf_10 = df_9.drop(['beds', 'accommodates'], axis=1)\ndf_10['binned_min_nights'] = pd.cut(df_9['minimum_nights'], bins=[0, 1, 2, 10, 1900],\n                                                labels = ['oneNight', '2to3','4to10', '11+']\n                                                )\ndf_10.binned_min_nights.value_counts()\n","684c992a":"df_10.skew()","be28925c":"\n\ndf_10.calculated_host_listings_count.describe()","9836883b":"#log transforming price, sec deposit, cleaning fee, extra people, reviews_per_month variables\n\ndf_10['log_price'] = np.log(df_10['price']*10 + 1)\ndf_10['log_security'] = np.log(df_10['security_deposit'] + 1)\ndf_10['log_cleaning'] = np.log(df_10['cleaning_fee']*10 + 1)\n\ndf_10['log_extra_people'] = np.log(df_10['extra_people']*10 + 1)\ndf_10['log_reviews_pm'] = np.log((df_10['reviews_per_month']+ 1) * 10)\ndf_10['log_bedrooms'] = np.log(df_10['bedrooms'] + 1)\ndf_10['log_bathrooms'] = np.log(df_10['bathrooms']*10 + 1)\ndf_10['log_guests_included'] = np.log(df_10['guests_included']*100 + 1)\ndf_10['log_listings_count'] = np.log(df_10['calculated_host_listings_count']*10 + 1)\n\ndf_10.skew()\n","42666312":"# Predictor variables and Skewed Distribution\n\nsns.set({'xtick.labelsize': 10, 'ytick.labelsize': 10})\n\nf, axes = plt.subplots(2,1, figsize=(5, 10))\nsns.despine(left=True)\n\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.reviews_per_month, ax = axes[0]).set_title(\"Log transformed Reviews Distribution\")\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.log_reviews_pm, ax = axes[1])\n\n\n\ndf_10.skew(axis=0)","48dd33c3":"# Predictor variables and Skewed Distribution\n\n\nf, axes = plt.subplots(2,1, figsize=(18, 9))\nsns.despine(left=True)\n\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.price, ax = axes[0]).set_title(\"Log transformed Price Distribution\")\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.log_price, ax = axes[1])\n\n\n\ndf_10.skew(axis=0)","1249e5cc":"df_10.room_type.value_counts()","6aa9c74e":"plt.figure(figsize=(15,20))\nsns.set({'xtick.labelsize': 20, 'ytick.labelsize': 20})\nsorted_hood = df_10.groupby(['neighbourhood_group_cleansed'])['log_price'].median().sort_values()\nsns.boxplot(palette = 'Set3', y=df_10.neighbourhood_group_cleansed, x=df_10.log_price, order=list(sorted_hood.index)).set_title(\"Log(Price) by Neighbourhood\", size=20)\nsns.despine","1fc76afb":"plt.figure(figsize=(20,20))\nsns.heatmap(df_10.corr(), annot=True, cmap='coolwarm', linewidth=.2)","8cfe0fa7":"f, axes = plt.subplots(2, 2, figsize=(10, 10))\nsns.set()\n\nsns.distplot(df_10.bedrooms, ax = axes[0,0])\nsns.distplot(df_10.log_bedrooms, ax = axes[1,0])\nsns.distplot(df_10.log_reviews_pm, ax = axes[1,1])\nsns.distplot(df_10.reviews_per_month, ax = axes[0,1])\n","5d717bb0":"import descartes\nimport geopandas as gpd\n\ngjsonFile = \"..\/input\/barcelonaairbnbgeojson\/neighbourhoods.geojson\"\nbarc_hoods = gpd.read_file(gjsonFile)\n\nbarc_hoods.plot(figsize=(10,10), column=\"neighbourhood_group\", cmap = \"tab10\")","86fc2d58":"barc_hoods['neighbourhood_group'].value_counts()","4bb97125":"barc_hoods.plot(figsize=(20,20), column=\"neighbourhood_group\", cmap='tab10', alpha = .5)\n#plt.figure(figsize=(20,20))\n\nsns.scatterplot(x='longitude', \n                y = 'latitude', \n                hue='price', \n                size = 'price', \n                sizes= (20, 600),\n                alpha = .8,\n                marker=\".\",\n                data = df_10,\n                )","da600184":"barc_hoods.plot(figsize=(10,10), alpha = .5)\n#plt.figure(figsize=(20,20))\n#here we add lat and longitude lines to plot for context\n\n\nsns.set({'font.size' : 10})\nsns.set_style(\"whitegrid\")\n#plot by reviews per month\nsns.scatterplot(x='longitude', \n                y = 'latitude', \n                hue='neighbourhood_group_cleansed', \n                alpha = .5,\n                marker=\"o\",\n                data = df_10,\n                cmap='Set3')\n                #size = 15)","9e17965e":"import folium\nfrom folium.plugins import HeatMap, MarkerCluster\n\nmapp = folium.Map(location=[41.40,2.15], zoom_start=12, figsize=(20,20))\ncluster = MarkerCluster().add_to(mapp)\n# add a marker for every record in the filtered data, use a clustered view\nfor each in df_10.iterrows():\n    folium.Marker(\n        location = [each[1]['latitude'],each[1]['longitude']], \n        clustered_marker = True).add_to(cluster)\n  \nmapp.save(outfile='map.html')\ndisplay(mapp)","93acfe00":"\n\nmax_price_map = df_10['price'].max() #this should be 650\nbarc_map = folium.Map(location=[41.40, 2.15], zoom_start=12, )\n\nheatmap = HeatMap( list(zip(df_10.latitude, df_10.longitude, df_10.price)),\n                 min_opacity = .3,\n                 max_val = max_price_map, \n                 radius = 3,\n                 blur = 2,\n                 max_zoom=1)\n\nfolium.GeoJson(barc_hoods).add_to(barc_map)\nbarc_map.add_child(heatmap)\n\nbarc_map.save(outfile=\"mapp.html\")","c51c0148":"\nimport folium\nfrom folium.plugins import HeatMap\n\n\nmax_reviews_pm = df_10['reviews_per_month'].max() \nbarc_map = folium.Map(location=[41.40, 2.15], zoom_start=12, )\n\nheatmap = HeatMap( list(zip(df_10.latitude, df_10.longitude, df_10.reviews_per_month)),\n                 min_opacity = .3,\n                 max_val = max_reviews_pm, \n                 radius = 3,\n                 blur = 2,\n                 max_zoom=1)\n\nfolium.GeoJson(barc_hoods).add_to(barc_map)\nbarc_map.add_child(heatmap)\n\n","4fa039e7":"sns.set()\nplt.figure(figsize=(20,10))\nsorted_hood_by_price = df_10.groupby(['neighbourhood_group_cleansed'])['log_price'].median().sort_values()\n\nsns.boxplot(x=\"neighbourhood_group_cleansed\",y=\"log_price\",data=df_10, palette=\"tab10\", order = list(sorted_hood_by_price.index)).set_title(\"Log(Price) by neighbourhood\", size=14)\n","94e01008":"\nplt.figure(figsize=(20,10))\nsorted_hood_by_price = df_10.groupby(['neighbourhood_group_cleansed'])['reviews_per_month'].median().sort_values()\n\nsns.boxplot(x=\"neighbourhood_group_cleansed\",y=\"log_reviews_pm\",\n            data=df_10, palette=\"tab10\", order = list(sorted_hood_by_price.index)\n           ).set_title(\"Log(ReviewsPerMonth) by neighbourhood\", size=14)\n","fb830351":"\nplt.figure(figsize=(20,10))\nsorted_hood_by_price = df_10.groupby(['room_type'])['log_price'].median().sort_values()\n\nsns.boxplot(x=\"room_type\",\n            y=\"log_price\",\n            data=df_10, \n            palette=\"tab10\", \n            order = list(sorted_hood_by_price.index)\n           ).set_title(\"Log(Price) by Room Type\", size=15)\n","be1636db":"plt.figure(figsize=(20,10))\n\nsorted_hood_by_activity = df_10.groupby(['neighbourhood_group_cleansed'])['log_price'].median().sort_values()\nsns.violinplot(x=df_10.neighbourhood_group_cleansed, y=df_10.log_price, order=list(sorted_hood_by_activity.index))\nsns.despine","62e8ca08":"plt.figure(figsize=(20,10))\n\nsorted_hood_by_activity = df_10.groupby(['room_type'])['log_reviews_pm'].median().sort_values()\nsns.violinplot(x=df_10.room_type, y=df_10.log_reviews_pm, order=list(sorted_hood_by_activity.index))\nsns.despine\n#private rooms getting reviewed the most","205ed86a":"# UGLY, but potentially helpful plot\n\n# why may we want to do a scatterplot for categorical data??\n\n\nf, axes = plt.subplots(1, 1, figsize=(10, 15))\nsns.despine\n\nsns.scatterplot(x='log_guests_included', \n                y = 'log_price', \n                hue='room_type',\n                size = 'log_reviews_pm', \n                sizes= (5, 400),\n                alpha = .8,\n                palette = \"tab10\",\n                data = df_10).set_title(\"Price by Bedrooms (weighted by Reviews)\")","fc3cef5f":"#appears that reviews_per_month is an important activity heuristic, \n#we can see there is a potential relationship between price and bedrooms, especially in active airbnbs","10f8fea4":"df_10.info()","35e15fd0":"df_10.nunique()","3803ccc6":"#Removing Log Transformed Variables\n\ndf_lg = df_10.drop(['price', \n                    'security_deposit', \n                    'cleaning_fee', \n                    'extra_people', \n                    'reviews_per_month', \n                    'bedrooms', \n                    'bathrooms', \n                    'guests_included', \n                    'calculated_host_listings_count'], axis = 1)","925ce67f":"df_lg.nunique()","fcb3808b":"df_lg.head()","7454eb61":"df_lg2 = df_lg.drop(['host_id', 'neighbourhood_cleansed', 'city', 'property_type'], axis=1)","c50462e4":"df_lg2.nunique()","80863a66":"df_lg2.skew()","3ba75935":"df_lg2.dtypes","fefc31db":"#MODELING STARTS\nX = df_lg2\nX = pd.get_dummies(data=X, drop_first = True)\n\nY = X['log_price']\nX2 = X.drop('log_price', axis = 1)\nX = X2\n\nX.nunique()","ec1e3e30":"#save cleaned X and Y for further analysis in R or elsewhere\n\nX.to_csv(\".\/Xcleaned.csv\")\nY.to_csv(\".\/Ycleaned.csv\")","7ee1dad4":"#BASELINE MULTIVARIATE REGRESSION MODEL\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .2, random_state = 40)\n\nest = sm.OLS(Y_train, X_train).fit()\ndisplay(est.summary())\n\n","80ff1058":"# explore Variance inflation factor of each variable in regression model\n\n# big VIF == bad\n\n# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n\nvif[\"features\"] = X_train.columns\n\nvif.round(1)","f55f13f1":"from sklearn.metrics import mean_squared_error, r2_score\n\n#EVALUATION OF MODEL\n\npredicted = est.predict(X_test)\n\nprint(\"MSE of model when comparing Y_test and predicted is %lf\" %mean_squared_error(Y_test, predicted))\n\n    \nfig, ax = plt.subplots(figsize=(7,7))\nax.scatter(Y_test, predicted)\nax.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'k--', lw=4, alpha=.5)\nax.set_xlabel('measured')\nax.set_ylabel('predicted')\nax.set_title(\"Baseline Train Predictions\")\nplt.show()","7b7be082":"plot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(5)\nplot_lm_1.set_figwidth(14)\n\nplot_lm_1.axes[0] = sns.residplot(est.fittedvalues, y=Y_train, data=X_train, \n                          lowess=True, \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')","070d1fa6":"\nmodel_fitted_y = est.fittedvalues\n# model residuals\nmodel_residuals = est.resid\n# normalized residuals\nmodel_norm_residuals = est.get_influence().resid_studentized_internal\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n# leverage, from statsmodels internals\nmodel_leverage = est.get_influence().hat_matrix_diag\n\n","eb9b506a":"QQ = sm.ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n\nplot_lm_2.set_figheight(6)\nplot_lm_2.set_figwidth(6)\n\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\n\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i, \n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));","b9a51853":"coef = pd.Series(est.params, index = X_train.columns)\n\n\nimp_coef = pd.concat([coef.sort_values().head(18),\n                     coef.sort_values().tail(18)])\nplt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Baseline Model\")","e20751e3":"Distributions will (most likely) not be perfectly symmetric, so we want to look at the skew of each column of data.","b5cd638b":"When `availability_365` is 0, the listing is not active. We take those out too.","f6e8a0ff":"# Seaborn - Violin Plot\nFirst of the plots, we have the violin plot. It displays the density of data. \nThe distribution is display, along with a boxplot found in the middle.","c3e56eaf":"Viewing more distributions after log","d39a51db":"`bedrooms`, `beds`, `accommodates` are very similar, better to limit to one column.\nAlso limiting the number of unique values on `minimum_nights` by grouping together.","5be06a45":"Removing columns that were effected by the log transformations.","8b4a64d8":"With `distplot` we can view our distributions to visualize skew.","1e156c76":"In order to have an approximately normal distribution, we can try to apply a log transformation to our skewed data.","ee426f49":"Dropping all nulls for `df_9`","a91a0771":"# Heatmaps\nThe denser the color, the higher the correlation. A red hue means a positive correlation, a blue hue means a negative correlation.","33a4e5ca":"# Checking Data Types\nOur data includes many Strings, which are very useful for any categorical analysis. We will be removing some columns as we don't need.","9c105ef9":"Dropping more columns","e16b61de":"Through these two distplots, we can see how the log function effected our distribution and skew.","48a5108d":"# descartes and geopandas\n","29b4ff7b":"# Boxplots\nA part of the violin plot. Although we cannot see our distribution in this graph, it is helpful in visualizing outliers and where the majority of data lies.","0f88592e":"Converting strings for price into floats","8002118d":"Using boxplot to view relationships between price and other columns.","06924c51":"`df.describe()` displays a quick summary of the data with some statistics per column","d31935e8":"# Scatterplot\nScatterplots are great for categorical data, allowing you to see a relationship.","c597591f":"# Visualizing Proportions of Data\nLooking at how the data is distributed can help when working through a dataset.","09713ed1":"# Libraries","07a9f373":"# Checking for Missing \/ Null Values\n`df.isnull()` returns a table of booleans. Use `sum()` to get the total number of `null` values.","bc1e640a":"Removing reviews","4aa40e63":"As you may have noticed, everytime we display the data the ID has been its own column. Set that as the index.","48debb06":"And remove the unnecessary columns","d88ac9cf":"# Loading In and Reading Data","01182d05":"Here we want to remove the columns that have greater than 60% of values missing.","26a0b1b4":"# Modeling"}}