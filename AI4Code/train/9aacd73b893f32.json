{"cell_type":{"e76f580b":"code","a4090005":"code","52378b51":"code","f18f94a5":"code","e49d3ba4":"code","0088854a":"code","06fc3631":"code","3e76d8f6":"code","e489075e":"code","02996553":"code","492b7821":"code","f8a698ee":"code","a8744ffe":"code","70da9d23":"code","ad0932d3":"code","fbb55e26":"code","a3a71917":"code","125c989a":"code","8e3e658d":"code","dbcc9554":"code","ccb93c90":"code","bf866e06":"code","dcd731bf":"code","240ccb8b":"code","d5005308":"code","b7c175d9":"code","77f098d0":"code","8bf1cc84":"code","d3faeaae":"code","623f69e4":"code","00491c60":"code","208b6882":"code","87c3649d":"code","879a0fe6":"code","9ab98a88":"code","2defc24e":"code","7e3dee45":"code","22a80fd3":"code","a7b44bba":"markdown","22640783":"markdown","05695428":"markdown","05cca3c6":"markdown","f0f9b44e":"markdown","6e76d9e2":"markdown","bb2d112b":"markdown","34dccbec":"markdown","cdcdb1a8":"markdown"},"source":{"e76f580b":"from pathlib import Path\nimport os.path\nfrom IPython.display import Image, display\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.cm as cm\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport random\nimport os","a4090005":"image_dir = Path('..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset')\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**\/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Drop GT images\nimage_df = image_df[image_df['Label'].apply(lambda x: x[-2:] != 'GT')]","52378b51":"#Shuffle the DataFrame and reset index\nimage_df = image_df.sample(frac=1).reset_index(drop = True)","f18f94a5":"# Display 25 picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=5, ncols=5, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(image_df.Filepath[i]))\n    ax.set_title(image_df.Label[i])\nplt.tight_layout()\nplt.show()","e49d3ba4":"# Separate in train and test data\ntrain_df, test_df = train_test_split(image_df, train_size=0.8, shuffle=True, random_state=1)","0088854a":"len(train_df)","06fc3631":"div = ['Train', 'Test']\n\nvalues = [len(train_df), len(test_df)]\n\nfig = plt.figure(figsize = (15, 8)) \n  \n# creating the bar plot \nplt.bar(div, values, color ='blue',  \n        width = 0.4) \nplt.ylim(0, 8000, 100)  \nplt.xlabel(\"Train-Test\") \nplt.ylabel(\"Number of images in each split\") \nplt.show() ","3e76d8f6":"train_generator = ImageDataGenerator(\n    preprocessing_function= tf.keras.applications.xception.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = ImageDataGenerator(\n    preprocessing_function= tf.keras.applications.xception.preprocess_input\n)\n\ntrain_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(299, 299),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=16,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(299, 299),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=16,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(299, 299),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=16,\n    shuffle=False\n)","e489075e":"# Create the base model from the pre-trained model MobileNet V2\nIMG_SIZE = (299, 299)\n\nIMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.Xception(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","02996553":"image_batch, label_batch = next(iter(train_images))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","492b7821":"base_model.trainable = False","f8a698ee":"# Let's take a look at the base model architecture\nbase_model.summary()","a8744ffe":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","70da9d23":"prediction_layer = tf.keras.layers.Dense(9)\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","ad0932d3":"inputs = tf.keras.Input(shape=(299, 299, 3))\nx = base_model(inputs, training=False)\nx = global_average_layer(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)","fbb55e26":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","a3a71917":"model.summary()","125c989a":"len(model.trainable_variables)","8e3e658d":"%load_ext tensorboard","dbcc9554":"import datetime\n# Clear any logs from previous runs\n!rm -rf .\/logs\/ ","ccb93c90":"log_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)","bf866e06":"initial_epochs = 10\n\nloss0, accuracy0 = model.evaluate(val_images)","dcd731bf":"print(\"initial loss: {:.2f}\".format(loss0))\nprint(\"initial accuracy: {:.2f}\".format(accuracy0))","240ccb8b":"history = model.fit(train_images,\n                    epochs=initial_epochs,\n                    validation_data=val_images)\n                    #callbacks = [tensorboard_callback])","d5005308":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')","b7c175d9":"plt.figure(figsize= (8, 8))\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","77f098d0":"base_model.trainable = True","8bf1cc84":"# Let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the base model: \", len(base_model.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = 100\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n  layer.trainable =  False","d3faeaae":"#base_learning_rate = 1e-5\n\nmodel.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate\/10),\n              metrics=['accuracy'])","623f69e4":"model.summary()","00491c60":"len(model.trainable_variables)","208b6882":"es = EarlyStopping(\n    monitor='val_loss', patience=5, verbose=0, mode='min'\n)","87c3649d":"mc = ModelCheckpoint('best_xception_fish_classifier.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)","879a0fe6":"#initial_epochs = 50\nfine_tune_epochs = 10\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_fine = model.fit(train_images,\n                         epochs=total_epochs,\n                         initial_epoch=history.epoch[-1],\n                         validation_data=val_images,\n                         callbacks = [es, mc, tensorboard_callback])\n\n#model.save('\/content\/mobilenet_v2')","9ab98a88":"acc += history_fine.history['accuracy']\nval_acc += history_fine.history['val_accuracy']\n\nloss += history_fine.history['loss']\nval_loss += history_fine.history['val_loss']","2defc24e":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')","7e3dee45":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","22a80fd3":"%tensorboard --logdir logs\/fit","a7b44bba":"# Training","22640783":"## Import Packages\n","05695428":"## Data Preprocessing","05cca3c6":"## Explore the Data","f0f9b44e":"## Tensorboard","6e76d9e2":"## Building Model","bb2d112b":"## Dataset","34dccbec":"## Evaluating Accuracy and Loss for the Model","cdcdb1a8":"# Fine Tuning"}}