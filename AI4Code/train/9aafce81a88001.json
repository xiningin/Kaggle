{"cell_type":{"227bb930":"code","1f623e5d":"code","a5261eeb":"code","8b080d3d":"code","a64eeb2c":"code","00806bf9":"code","2257d109":"code","3db9e552":"code","48b144e7":"code","c7bfda29":"code","31baa148":"code","c3fedd9d":"code","c49cbd08":"code","aa3b7304":"code","8de746e2":"code","ffd3c42f":"code","b2fee723":"code","ee87888c":"code","8092e218":"code","e691b7bc":"code","043eb080":"code","eb822997":"code","72c026c9":"code","faa0d40c":"code","015adfcd":"code","13956540":"code","18ec5e09":"code","63a25ce4":"code","5e81eeec":"code","d0b77961":"code","003b943e":"code","1fe05d08":"code","305ccd31":"code","ee891d79":"code","84fefcdb":"code","a1fa34c6":"code","82c5823f":"markdown","e2669a57":"markdown","0ccc02e1":"markdown","4ab4ec0e":"markdown","5776d4d4":"markdown","37090d29":"markdown","3f1dab82":"markdown","afd9d3b1":"markdown","6138e257":"markdown","bf24bf1a":"markdown","2dc52b9f":"markdown","1349e46e":"markdown","83d4fc25":"markdown","0482743f":"markdown","8f4b90c0":"markdown","1b46212c":"markdown","741e2faa":"markdown"},"source":{"227bb930":"# library imports\nimport numpy as np \nimport pandas as pd\nimport random as rn\nimport cv2 as cv \nimport os\nimport sys\nimport skimage.io\nfrom pathlib import Path\n\n# neural network wizardry\nimport tensorflow as tf\n\n# visuals\nfrom matplotlib import pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# for reproducibility\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.set_random_seed(RANDOM_SEED)\nrn.seed(RANDOM_SEED)\n\n# paths\nimg_train_folder = Path('\/kaggle\/input\/train_images\/')\nimg_test_folder = Path('\/kaggle\/input\/test_images\/')","1f623e5d":"# imagine a 3*3 image with a diagional line across\nX = np.eye(3,3, dtype=np.uint8)\nY = np.eye(3,3, dtype=np.uint8)\n\n# we change one pixel\nX[1,1] = 0","a5261eeb":"print(X)\nprint('')\nprint(Y)","8b080d3d":"def dice_coefficient(X, y):\n    \n    # convert the pixel\/mask matrix to a one-dimensional series\n    predicted = X.flatten()\n    truth = y.flatten()\n    \n    # our masks will consist of ones and zeros\n    # summing the result of their product gives us the cross section\n    overlap = np.sum(predicted * truth)\n    total_surface_area = np.sum(predicted + truth)\n    \n    # passing our calculated values to the formula\n    return 2 * overlap \/ total_surface_area","a64eeb2c":"print(f'The dice coefficient for 1 wrongly labeled pixel in a 3*3 image is: {dice_coefficient(X, Y)}')\nprint('(2 * 2 overlapping \"1\" pixels \/ 5 total \"1\" surface area)')","00806bf9":"# a more elaborate version of kaggle.com\/paulorzp\/rle-functions-run-lenght-encode-decode\n# note that we will transpose the incoming array outside of the function, \n# as I find this a clearer illustration\n\ndef mask_to_rle(mask):\n    \"\"\"\n    params:  mask - numpy array\n    returns: run-length encoding string (pairs of start & length of encoding)\n    \"\"\"\n    \n    # turn a n-dimensional array into a 1-dimensional series of pixels\n    # for example:\n    #     [[1. 1. 0.]\n    #      [0. 0. 0.]   --> [1. 1. 0. 0. 0. 0. 1. 0. 0.]\n    #      [1. 0. 0.]]\n    flat = mask.flatten()\n    \n    # we find consecutive sequences by overlaying the mask\n    # on a version of itself that is displaced by 1 pixel\n    # for that, we add some padding before slicing\n    padded = np.concatenate([[0], flat, [0]])\n    \n    # this returns the indeces where the sliced arrays differ\n    runs = np.where(padded[1:] != padded[:-1])[0] \n    # indexes start at 0, pixel numbers start at 1\n    runs += 1\n\n    # every uneven element represents the start of a new sequence\n    # every even element is where the run comes to a stop\n    # subtract the former from the latter to get the length of the run\n    runs[1::2] -= runs[0::2]\n \n    # convert the array to a string\n    return ' '.join(str(x) for x in runs)","2257d109":"rle_example = mask_to_rle(X)\nprint(f'The run-length encoding for our example would be: \"{rle_example}\"')","3db9e552":"def rle_to_mask(lre, shape=(1600,256)):\n    '''\n    params:  rle   - run-length encoding string (pairs of start & length of encoding)\n             shape - (width,height) of numpy array to return \n    \n    returns: numpy array with dimensions of shape parameter\n    '''    \n    # the incoming string is space-delimited\n    runs = np.asarray([int(run) for run in lre.split(' ')])\n    \n    # we do the same operation with the even and uneven elements, but this time with addition\n    runs[1::2] += runs[0::2]\n    # pixel numbers start at 1, indexes start at 0\n    runs -= 1\n    \n    # extract the starting and ending indeces at even and uneven intervals, respectively\n    run_starts, run_ends = runs[0::2], runs[1::2]\n    \n    # build the mask\n    h, w = shape\n    mask = np.zeros(h*w, dtype=np.uint8)\n    for start, end in zip(run_starts, run_ends):\n        mask[start:end] = 1\n    \n    # transform the numpy array from flat to the original image shape\n    return mask.reshape(shape)","48b144e7":"print(f'The mask reconstructed from the run-length encoding (\"{rle_example}\") \\\nfor our example would be:\\n{rle_to_mask(rle_example, shape=(3,3))}')","c7bfda29":"# reading in the training set\ndata = pd.read_csv('\/kaggle\/input\/train.csv')\ndata['ClassId'] = data['ClassId'].astype(np.uint8)\n\ndata.info()\ndata.head()","31baa148":"# keep only the images with labels\nsquashed = data.dropna(subset=['EncodedPixels'], axis='rows', inplace=True)\n\n# squash multiple rows per image into a list\nsquashed = (\n    data[['ImageId', 'EncodedPixels', 'ClassId']]\n        .groupby('ImageId', as_index=False) \n        .agg(list) \n)\n\n# count the amount of class labels per image\nsquashed['DistinctDefectTypes'] = squashed['ClassId'].apply(lambda x: len(x))\n\n# display first ten to show new structure\nsquashed.head(10)","c3fedd9d":"print(f\"\"\"The training set now consists of {len(squashed):,} distinct images,\nfor a total of {squashed[\"DistinctDefectTypes\"].sum():,} labeled mask instances.\"\"\")","c49cbd08":"\"\"\" use a consistent color palette per label throughout the notebook \"\"\"\nimport colorlover as cl\n\n# see: https:\/\/plot.ly\/ipython-notebooks\/color-scales\/\ncolors = cl.scales['4']['qual']['Set3']\nlabels = np.array(range(1,5))\n\n# combining into a dictionary\npalette = dict(zip(labels, np.array(cl.to_numeric(colors))))","aa3b7304":"# we want counts & frequency of the labels\nclasses = (\n    data.groupby(by='ClassId', as_index=False)\n        .agg({'ImageId':'count'})\n        .rename(columns={'ImageId':'Count'})\n)\n\nclasses['Frequency'] = round(classes['Count'] \/ classes['Count'].sum() * 100, 2) \nclasses['Frequency'] = classes['Frequency'].astype(str) + '%'\n\n# plotly for interactive graphs\nfig = go.Figure(\n    \n    data=go.Bar(\n        orientation='h',\n        x=classes.Count,\n        y=classes.ClassId,\n        hovertext=classes.Frequency,\n        text=classes.Count,\n        textposition='auto',\n        marker_color=colors),\n    \n    layout=go.Layout(\n        title='Defect Type: Count & Frequency',\n        showlegend=False,\n        xaxis=go.layout.XAxis(showticklabels=False),\n        yaxis=go.layout.YAxis(autorange='reversed'),\n        width=750, height=400\n    )\n)\n\n# display\nfig.show()","8de746e2":"# we want counts of the possible combinations of labels\npermutations = pd.DataFrame(data=squashed.ClassId.astype(str).value_counts())\n\n# and their frequency\npermutations['Frequency'] = round(permutations['ClassId'] \/ permutations['ClassId'].sum() * 100, 2)\npermutations['Frequency'] = permutations['Frequency'].astype(str) + '%'\n\n# plotly for interactive graphs\nfig = go.Figure(\n    \n    data=go.Bar(\n        orientation='h',\n        x=permutations.ClassId,\n        y=permutations.index,\n        hovertext=permutations.Frequency,\n        text=permutations.ClassId,\n        textposition='auto'),\n    \n    layout=go.Layout(\n        title='Count of Distinct Defect Combinations in Images',\n        showlegend=False,\n        xaxis=go.layout.XAxis(showticklabels=False),\n        yaxis=go.layout.YAxis(autorange='reversed'),\n        width=750, height=500\n    )\n)\n\n# display\nfig.show()","ffd3c42f":"def build_mask(encodings, labels):\n    \"\"\" takes a pair of lists of encodings and labels, \n        and turns them into a 3d numpy array of shape (256, 1600, 4) \n    \"\"\"\n    \n    # initialise an empty numpy array \n    mask = np.zeros((256,1600,4), dtype=np.uint8)\n   \n    # building the masks\n    for rle, label in zip(encodings, labels):\n        \n        # classes are [1, 2, 3, 4], corresponding indeces are [0, 1, 2, 3]\n        index = label - 1\n        \n        # fit the mask into the correct layer\n        # note we need to transpose the matrix to account for \n        # numpy and openCV handling width and height in reverse order \n        mask[:,:,index] = rle_to_mask(rle).T\n    \n    return mask","b2fee723":"def mask_to_contours(image, mask_layer, color):\n    \"\"\" converts a mask to contours using OpenCV and draws it on the image\n    \"\"\"\n\n    # https:\/\/docs.opencv.org\/4.1.0\/d4\/d73\/tutorial_py_contours_begin.html\n    contours, hierarchy = cv.findContours(mask_layer, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n    image = cv.drawContours(image, contours, -1, color, 2)\n        \n    return image","ee87888c":"def visualise_mask(file_name, mask):\n    \"\"\" open an image and draws clear masks, so we don't lose sight of the \n        interesting features hiding underneath \n    \"\"\"\n    \n    # reading in the image\n    image = cv.imread(f'{img_train_folder}\/{file_name}')\n\n    # going through the 4 layers in the last dimension \n    # of our mask with shape (256, 1600, 4)\n    for index in range(mask.shape[-1]):\n        \n        # indeces are [0, 1, 2, 3], corresponding classes are [1, 2, 3, 4]\n        label = index + 1\n        \n        # add the contours, layer per layer \n        image = mask_to_contours(image, mask[:,:,index], color=palette[label])   \n        \n    return image","8092e218":"# the images we want to see\nconditions = [\n    squashed['ClassId'].astype(str)=='[1]',\n    squashed['ClassId'].astype(str)=='[2]',\n    squashed['ClassId'].astype(str)=='[3]',\n    squashed['ClassId'].astype(str)=='[4]',\n    squashed['DistinctDefectTypes']==2,\n    squashed['DistinctDefectTypes']==3\n]\n\n# max 2 due to limited population of [squashed['Distinct Defect Types']==3]\n# remove that condition if you wish to increase the sample size, \n# or add replace=True to the .sample() method\nsample_size = 2\n\n# looping over the different combinations of labels \nfor condition in conditions:\n    \n    # isolate from dataset and draw a sample\n    sample = squashed[condition].sample(sample_size) \n    \n    # make a subplot+\n    fig, axes = plt.subplots(sample_size, 1, figsize=(16, sample_size*3))\n    fig.tight_layout()\n    \n    # looping over sample\n    for i, (index, row) in enumerate(sample.iterrows()):\n        \n        # current ax\n        ax = axes[i,]\n        \n        # build the mask\n        mask = build_mask(encodings=row.EncodedPixels, labels=row.ClassId)\n\n        # fetch the image and draw the contours\n        image = visualise_mask(file_name=row.ImageId, mask=mask)\n        \n        # display\n        ax.set_title(f'{row.ImageId}: {row.ClassId}')\n        ax.axis('off')\n        ax.imshow(image);","e691b7bc":"WORKING_DIR = '\/kaggle\/working'\nLOGS_DIR = os.path.join(WORKING_DIR, \"logs\")\nMASK_RCNN_DIR = os.path.join(WORKING_DIR, 'Mask_RCNN-master')","043eb080":"# !git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git \n# results in Commit Error (too many nested subdirectories)\n\n\"\"\" Credit to Simon Walker, whose method helped me to \n    circumvent the commit error. Check out his kernel at \n    https:\/\/www.kaggle.com\/srwalker101\/mask-rcnn-model\n\"\"\"\n!pip install git+https:\/\/github.com\/rteuwens\/Mask_RCNN","eb822997":"from mrcnn.utils import Dataset\nfrom mrcnn.config import Config\nfrom mrcnn.model import MaskRCNN","72c026c9":"class SeverstalConfig(Config):\n\n    # Give the configuration a recognizable name\n    NAME = \"severstal\"\n\n    # We use a GPU with 12GB memory, which can fit two images.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 2\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 4  # background + steel defects\n\n    # Number of training steps per epoch\n    STEPS_PER_EPOCH = 100\n\n    # Skip detections with < 90% confidence\n    DETECTION_MIN_CONFIDENCE = 0.9\n    \n    # Discard inferior model weights\n    SAVE_BEST_ONLY = True\n    \n# instantiating \nseverstal_config = SeverstalConfig()","faa0d40c":"# super class can be found here:\n# https:\/\/github.com\/matterport\/Mask_RCNN\/blob\/v2.1\/utils.py\n\nclass SeverstalDataset(Dataset):\n    \n    def __init__(self, dataframe):\n        \n        # https:\/\/rhettinger.wordpress.com\/2011\/05\/26\/super-considered-super\/\n        super().__init__(self)\n        \n        # needs to be in the format of our squashed df, \n        # i.e. image id and list of rle plus their respective label on a single row\n        self.dataframe = dataframe\n        \n    def load_dataset(self, subset='train'):\n        \"\"\" takes:\n                - pandas df containing \n                    1) file names of our images \n                       (which we will append to the directory to find our images)\n                    2) a list of rle for each image \n                       (which will be fed to our build_mask() \n                       function we also used in the eda section)         \n            does:\n                adds images to the dataset with the utils.Dataset's add_image() metho\n        \"\"\"\n        \n        # input hygiene\n        assert subset in ['train', 'test'], f'\"{subset}\" is not a valid value.'\n        img_folder = img_train_folder if subset=='train' else img_test_folder\n        \n        # add our four classes\n        for i in range(1,5):\n            self.add_class(source='', class_id=i, class_name=f'defect_{i}')\n        \n        # add the image to our utils.Dataset class\n        for index, row in self.dataframe.iterrows():\n            file_name = row.ImageId\n            file_path = f'{img_folder}\/{file_name}'\n            \n            assert os.path.isfile(file_path), 'File doesn\\'t exist.'\n            self.add_image(source='', \n                           image_id=file_name, \n                           path=file_path)\n    \n    def load_mask(self, image_id):\n        \"\"\"As found in: \n            https:\/\/github.com\/matterport\/Mask_RCNN\/blob\/master\/samples\/coco\/coco.py\n        \n        Load instance masks for the given image\n        \n        This function converts the different mask format to one format\n        in the form of a bitmap [height, width, instances]\n        \n        Returns:\n            - masks    : A bool array of shape [height, width, instance count] with\n                         one mask per instance\n            - class_ids: a 1D array of class IDs of the instance masks\n        \"\"\"\n        \n        # find the image in the dataframe\n        row = self.dataframe.iloc[image_id]\n        \n        # extract function arguments\n        rle = row['EncodedPixels']\n        labels = row['ClassId']\n        \n        # create our numpy array mask\n        mask = build_mask(encodings=rle, labels=labels)\n        \n        # we're actually doing semantic segmentation, so our second return value is a bit awkward\n        # we have one layer per class, rather than per instance... so it will always just be \n        # 1, 2, 3, 4. See the section on Data Shapes for the Labels.\n        return mask.astype(np.bool), np.array([1, 2, 3, 4], dtype=np.int32)","015adfcd":"from sklearn.model_selection import train_test_split\n\n# stratified split to maintain the same class balance in both sets\ntrain, validate = train_test_split(squashed, test_size=0.2, random_state=RANDOM_SEED)","13956540":"print(train['ClassId'].astype(str).value_counts(normalize=True))\nprint('')\nprint(validate['ClassId'].astype(str).value_counts(normalize=True))","18ec5e09":"%%time\n\n# instantiating training set\ndataset_train = SeverstalDataset(dataframe=train)\ndataset_train.load_dataset()\ndataset_train.prepare()\n\n# instantiating validation set\ndataset_validate = SeverstalDataset(dataframe=validate)\ndataset_validate.load_dataset()\ndataset_validate.prepare()","63a25ce4":"!curl -LO https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.0\/mask_rcnn_coco.h5","5e81eeec":"# configuration\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n# session stuff\nsession = tf.Session(config=config)\nsession.run(tf.global_variables_initializer())\nsession.run(tf.local_variables_initializer())\n\n# initialiazing model\nmodel = MaskRCNN(mode='training', config=severstal_config, model_dir='modeldir')\n\n# we will retrain starting with the coco weights\nmodel.load_weights('mask_rcnn_coco.h5', \n                   by_name=True, \n                   exclude=['mrcnn_bbox_fc',\n                            'mrcnn_class_logits', \n                            'mrcnn_mask',\n                            'mrcnn_bbox'])","d0b77961":"%%time \n\n# ignore UserWarnongs\nimport warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\n\n# training at last\nmodel.train(dataset_train,\n            dataset_validate,\n            epochs=1,\n            layers='heads',\n            learning_rate=severstal_config.LEARNING_RATE*2)\n\nhistory = model.keras_model.history.history\n\n\n\n","003b943e":"%%time\n\n# ignore UserWarnongs\nimport warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\n\nmodel.train(dataset_train,\n            dataset_validate,\n            epochs=1,\n            layers='all',\n            learning_rate=severstal_config.LEARNING_RATE*2)\n\n\nhistory2 = model.keras_model.history.history\n#for i in history2: history[i] = history[i] + history2[i]","1fe05d08":"%%time\n\nmodel.train(dataset_train,\n            dataset_validate,\n            epochs=1,\n            layers='all',\n            learning_rate=severstal_config.LEARNING_RATE\/3)\n\nhistory3 = model.keras_model.history.history\n#for i in history3: history[i] = history[i] + history3[i]","305ccd31":"class InferenceConfig(SeverstalConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    \ninference_config = InferenceConfig()\n\nmodel= modellib.MaskRCNN(mode='inference',\n                        config=inference_config,\n                        model_dir=ROOT_DIR)\n\nassert model_path != '', \"path trained weights\"\n\nmodel.load_weights(model_path, by_name=True)\n    ","ee891d79":"#load sample submission\nsample_df = pd.read_csv('\/kaggle\/input\/sample_submission.csv')\nsample_df.head()","84fefcdb":"#convert data to run-length encoding\ndef conversion(bits):\n    conv = []\n    pos = 0\n    for bit, group in itertools.grooupby(bits):\n        group_list = list(group)\n        if bit:\n            conv.extend([pos, sum(group_list)])\n        pos += len(group_list)\n    return conv","a1fa34c6":"#refine masks to prevent overlapping\n\ndef refine_masks(masks, rois):\n    areas = np.sum(masks.reshape(-1, masks.shape[-1]), axis=0)\n    mask_index = np.argsort(areas)\n    union_mask = np.zeros(masks.shape[:-1], dtype=bool)\n    for m in mask_index:\n        masks[:, :, m] = np.logical_and(masks[:, :, m], np.logical_not(union_mask))\n        union_mask = np.logical_or(masks[:, :, m], union_mask)\n    for m in range(masks.shape[-1]):\n        mask_pos = np.where(masks[:, :, m]==True)\n        if np.any(mask_pos):\n            y1, x1 = np.min(mask_pos, axis=1)\n            y2, x2 = np.max(mask_pos, axis=1)\n            rois[m, :] = [y1, x1, y2, x2]\n    return masks, rois","82c5823f":"An overwhelming amount of the observations is for class 3. Hopefully we can balance this out at least a little with some data augmentation later.\n\nLet's see what the distributions are if we consider all possible combinations, inccluding multi-class instances.","e2669a57":"### *Mean* Dice Coefficient\nThe dataset's original format (one row per *imageId:classId* pair) points at the fact that we will have to run this dice coefficient function over every layer in our mask and take the average. If we train multiple images at a time, we will have to take the average across a batch. More about this in the **Data Shapes** chapter.","0ccc02e1":"<a id=\"4\"><\/a> <br>\n# Visualising the Masks\nLet's take a look at some examples of each class, and of some of the images containing multiple classes.","4ab4ec0e":"# Foreword\nThe main goal of this notebook is to make the ideas presented easy to understand. I try my best to be as verbose as possible, but if you have remaining questions, I'll happily answer them in the comments.\n\nCarefully illustrating ideas takes a lot more time than just writing up code, so upvotes are much appreciated.\n\n# Kernel Structure\n* [Competition Information](#1)\n<br><span style=\"font-size:10px\">We discuss the format required from us by the competition, as well as the loss function of choice and the type of encoding used.<\/span>\n* [Exploratory Data Analysis & Baseline](#2)\n<br><span style=\"font-size:10px\">There is a class imbalance that will have to be dealt with. We also show accuracy is the wrong metric due to the majority class (no defects) being the majority class.<\/span>\n* [Input\/Output Data Shapes](#3)\n<br><span style=\"font-size:10px\">Understanding the dimensions of the in- and output that the neural network will be expecting.<\/span>\n* [A First Look: Visualising the Masks](#4)\n<br><span style=\"font-size:10px\">We define some utility functions and show some instances of defects.<\/span>\n* [Formulating the Problem: Semantic Segmentation](#5)\n<br><span style=\"font-size:10px\">We define what task is expected of us.<\/span>\n* [Setting up our ML model: Mask R-CNN](#6)\n<br><span style=\"font-size:10px\">Cloning and configuring Matterport's implementation of Mask RCNN.<\/span>\n* [Training](#7)\n<br><span style=\"font-size:10px\">Currently configured to run a single epoch. This should allow you to get started!<\/span>","5776d4d4":"<a id=\"2\"><\/a> <br>\n# Exploratory Data Analysis\n\n## Class Imbalances\nA huge imbalance quickly becomes apparent when looking at the training set description:\n\n![class_imbalance](https:\/\/i.imgur.com\/B4Dsxur.png)\n\nOnly 7095 pictures will be of any use to us when training... (you wouldn't train a pedestrian detector on a dataset of empty streets, either).\n\n## Baseline\nIf we'd train on the entire dataset, the risk is substantial that our model will simply learn the **majority class** (no defect, ever).\nIndeed, if we simply upload the sample submission, we have a score of:\n\n![baseline_code](https:\/\/i.imgur.com\/zAxYg4g.png)\n![baseline_score](https:\/\/i.imgur.com\/AXaygTV.png)\n\nThis is the benchmark to beat. \n\n## Class Imbalances (continued)\nLet's see how often each class appears, as well as how the class distribution is inside images.","37090d29":"## Pre-Trained Weights","3f1dab82":"# To-Do\n* Split kernels into training & inference\n* \"Squashed\" DataFrame for inference on the competition set\n* Improving the model\n\nHave fun!","afd9d3b1":"## Run-Length Encoding\n> In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit **pairs of values** that contain a **start position and a run length**. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n>\n>The competition format requires a **space delimited list of pairs**. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are **sorted, positive, and the decoded pixel values are not duplicated**. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\nSo, if we were to encode something like our example above, we would have to write it as follows:","6138e257":"We now have a validation set that has the same class distribution as the training set.","bf24bf1a":"**Prediction**","2dc52b9f":"## Configuring","1349e46e":"<a id=\"1\"><\/a> <br>\n# Competition Information\n\n## Prediction Output Format\nFrom the competition's [data](https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/data) page:\n> Each image may have no defects, a defect of a single class, or defects of multiple classes. For each image you must segment defects of each class ```(ClassId = [1, 2, 3, 4])```.\n\nThe submission format requires us to make the classifications for each respective class on a separate row:\n![format](https:\/\/i.imgur.com\/x3rWaJP.png)\n\n## Loss Function\n\n### Dice Coefficient\nFrom the [evaluation](https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/overview\/evaluation) page:\n\n> This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n>\n>$$Dice(X,Y) = \\frac{2\u2217|X\u2229Y|}{|X|+|Y|}$$\n>\n>\n>where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each ```<ImageId, ClassId>``` pair in the test set.\n\nOr if you prefer a visual illustration:\n![dice_viz](https:\/\/i.imgur.com\/zl2W0xQ.png)\n\n\nTo get a better understanding, let's demonstrate with a quick toy example as we write the function:","83d4fc25":"It looks like combinations of two labels in a single image are reasonably frequent, too. In fact, 3 & 4 appear together more often than 2 does on its own!\n\n<a id=\"3\"><\/a> <br>\n# Data Shapes\n\n## Images\nThe input shape will be an image we convert to a three-dimensional array with shape ```(256, 1600, 3)```, for height, width, and the three colour channels (RGB), respectively.\n\n## Labels\nNaturally, the masks will share the same width and height, but the third dimension will be as large as there are labels ```(256, 1600, 4)```, with each class occupying a different layer. Somewhat like this:\n![label shape](https:\/\/i.imgur.com\/PePSemo.png)\n\n## Batch Size\nTo leverage the parellel computation a GPU offers, we will feed the images and their labels to the algorithm in batches. Consequently, our array dimensions will be expanded to ```(batch size, 256, 1600, 3)``` and ```(batch size, 256, 1600, 4)```, respectively.\n\nIf you remember our dice coefficient, we will have to calculate it for evey layer in every mask, and take the average over the entire batch.","0482743f":"<a id=\"7\"><\/a> <br>\n# Training","8f4b90c0":"## Train\/Test Split","1b46212c":"<a id=\"5\"><\/a> <br>\n# Semantic Segmentation\nBelow are some of the common tasks in the field of Machine Vision:\n![semantic_segmentation](https:\/\/miro.medium.com\/max\/1838\/1*Tb3CvTONAA4IVL-HciJscw.jpeg)\n\nWe are dealing with the problem of semantic segmentation: predicting a pixel-by-pixel mask of distinct classes.\nCheck out Priya Dwivedi's [excellent blogpost](https:\/\/towardsdatascience.com\/semantic-segmentation-popular-architectures-dff0a75f39d0) on the topic if you want to read more.\n\n<a id=\"6\"><\/a> <br>\n# Mask R-CNN\nMask R-CNN falls under the category of meta-algorithms, rather than purely a neural network architecture. In fact, it builds on the faster R-CNN architecture, so you even have a choice of what neural net 'backbone' you want it to use.\n\n![maskrcnn-framework](https:\/\/miro.medium.com\/max\/1285\/1*IWWOPIYLqqF9i_gXPmBk3g.png)\n\n\nThe most important aspects of this algorithm are:\n* **FPN (feature pyramid network)** - A fully convolutional neural architecture designed to extract features.\n* **RPN (region proposal network)** - A lightweight neural network that scans over the FPN features to suggest ROI (regions of interest)\n* **ROIAlign** - a novel way to pass the object to the classifier an mask generator. Contrary to the ROIpool mechanism that was the standard, this one uses [bilinear interpolation](https:\/\/www.quora.com\/How-does-ROTAlign-work-in-Mask-RCNN) to improve performance significantly.\n* **Classifier & Bounding Box Regressor**. \n* **Mask Generator** - A convolutional network that takes the regions selected by the ROI classifier and generates soft masks for them. \n\nRead more on [Matterport's official blog](https:\/\/engineering.matterport.com\/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46) <br>\nYou can also check out the original paper's authors presenting the Mask R-CNN [on YouTube](https:\/\/youtu.be\/g7z4mkfRjI4)\n\n## Importing\nFor instructions on how to import models into a Kaggle kernel, check out the following Medium article: [Setting Up Mask-RCNN on Kaggle](https:\/\/medium.com\/@umdfirecoml\/setting-up-mask-rcnn-on-kaggle-34b656140b5e)\n\n2019\/10\/10: <br> \nThis method currently produces an error message when committing. <br> \n```Output path '\/Mask_RCNN\/.git\/logs\/refs\/remotes\/origin\/HEAD' contains too many nested subdirectories (max 6)```. <br> \nI resort to Simon Walker's method to get around this.\n\n2019\/10\/17: <br>\nA Keras update means the model now produces an error [documented in issue #1754](https:\/\/github.com\/matterport\/Mask_RCNN\/issues\/1754). <br>\nI therefore cloned the repo and applied a small fix. <br>\n```changing: self.keras_model.metrics_tensors.append(loss)\nto: self.keras_model.add_metric(loss, name)````\n","741e2faa":"## Instantiating the Model"}}