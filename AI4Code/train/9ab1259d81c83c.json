{"cell_type":{"a759d718":"code","95dffcca":"code","eea12a0b":"code","25f33776":"code","e7fc4287":"code","956e72c5":"code","179869a7":"code","09b63839":"code","ea721fe2":"code","5eee5b55":"code","a53f6e72":"code","7baa341c":"code","0be3b87e":"code","a20b5234":"code","437fb9ef":"code","01579e8e":"code","3ea32d40":"code","7ee6812f":"code","2d88cb3d":"code","da63cd80":"code","1ce8ea7c":"code","18b5a2d3":"markdown","b5cb298a":"markdown","57334ea4":"markdown","c2e1f7cb":"markdown","47d0c2ff":"markdown","6b8b10b5":"markdown","f2d824a3":"markdown","8d640179":"markdown","09ed9e43":"markdown","547284a6":"markdown","6150d38c":"markdown","5f6ab854":"markdown","f26f1d2c":"markdown","673b4ee8":"markdown","5f58cd9d":"markdown","c4f79b0f":"markdown","64874dd7":"markdown","c52c5cc7":"markdown","703d53c8":"markdown","22ba90f3":"markdown","3611474b":"markdown","353777d0":"markdown","d7618124":"markdown","5486ec6a":"markdown","8dcd4fd4":"markdown","32c89981":"markdown","b425da74":"markdown","a63636e7":"markdown","e8a0868f":"markdown","531e02cc":"markdown"},"source":{"a759d718":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","95dffcca":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv')","eea12a0b":"#calculating CPM\n#calculating the value that the Advertisers Bid for the month of June\n# CPM(the value which was the winning bid value) = \n#((revenue of the publisher*100)\/revenue_share_percentage)\/measurable_impressions)*1000\n\ndf['CPM'] = np.where(df.measurable_impressions > 0, df.total_revenue * 100 \/ df.measurable_impressions * 1000, 0)","25f33776":"# cleanse df because there're some outliers\ndf = df[~(df.CPM < 0.0)]","e7fc4287":"# we aren't going to use total_revenue to predict CPM, because we used it to calculate it\ndf.drop(['total_revenue'], axis=1, inplace=True)\n\n#we can remove integration type as it has only one value and revenue share percent as that we have already used and \n#is only one single value as well\ndf.drop(['integration_type_id' , 'revenue_share_percent'], axis = 1, inplace=True)\n\n# we can remove total impressions as well as that is account the same information as measurable impressions \ndf.drop([ 'total_impressions'], axis = 1, inplace=True)","956e72c5":"df.nunique()","179869a7":"gb = df.groupby('os_id')['CPM'].mean().reset_index()\ngb","09b63839":"class ProcessingDataset:\n    CAT_FEATS =  ['site_id', 'ad_type_id', 'geo_id',\n                  'device_category_id', 'advertiser_id', 'order_id',\n                  'line_item_type_id', 'os_id',\n                  'monetization_channel_id', 'ad_unit_id']\n    __ids_transform_dict = {}\n    \n    def fit(self, X, y):\n        X = X.assign(target = lambda x: y.to_numpy())\n        for col in self.CAT_FEATS:\n            gb = X.groupby(col)['target'].mean().reset_index()\n            gb.rename(columns = {'target' : col + '_avg_score'}, inplace=True)\n            self.__ids_transform_dict[col] = gb\n        X.drop(['target'], axis=1, inplace=True)\n        return self\n    \n    def transform(self, X):\n        for col in self.CAT_FEATS:\n            X = pd.merge(X, self.__ids_transform_dict[col], how='left', on=col)\n        X.drop(self.CAT_FEATS, axis=1, inplace=True)\n        X['View\/measurable'] = np.where(\n            X.measurable_impressions > 0,\n            X['viewable_impressions'] \/ X['measurable_impressions'], 0)\n        \n        #X['date'] = X.date.apply(lambda x: int(x[8:10]))\n        #X['is_weekend'] = np.where(X.date.isin((1, 2, 8, 9, 15, 16, 22, 23, 29, 30)), 1, 0)\n        \n        X.drop(['date'], axis=1, inplace=True)\n        X.fillna(0, inplace=True)\n        return X","ea721fe2":"train = df[df.date < '2019-06-22'].sort_values(by='date')\ntest = df[df.date > '2019-06-21'].sort_values(by='date')\n\ntest = test[test.CPM < test.CPM.quantile(0.95)]\ntarget_test = test.CPM.copy()\ntest.drop(['CPM'], axis=1, inplace=True)\n\n# so there's no reason to not doing it for train\ntrain = train[train.CPM < train.CPM.quantile(0.95)]\ntarget_train = train.CPM.copy()\ntrain.drop(['CPM'], axis=1, inplace=True)","5eee5b55":"processing = ProcessingDataset()\nprocessing.fit(train, target_train)\nx = processing.transform(train)\nx.head()","a53f6e72":"corr = (x.assign(y = target_train)).corr()\nplt.figure(figsize=(12,9))\nsns.heatmap(data=np.abs(corr), cmap=\"YlGnBu\",  square=True, annot= True)\nplt.show()","7baa341c":"from sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split, KFold","0be3b87e":"X_train, X_valid, y_train, y_valid = train_test_split(train, target_train, test_size=0.25,random_state=42)","a20b5234":"pipe = Pipeline([('fe', ProcessingDataset()), ('scaler', MinMaxScaler()), ('model', LinearRegression())])\npipe.fit(X_train, y_train)\n\ny_preds = pipe.predict(X_valid)\n# replace zeros with negatives\ny_preds = np.where(y_preds > 0, y_preds, 0)\nmean_squared_error(y_valid, y_preds)","437fb9ef":"y_preds = pipe.predict(test)\ny_preds = np.where(y_preds > 0, y_preds, 0)\nmean_squared_error(target_test, y_preds)","01579e8e":"params = {\n    'n_estimators': 400,\n    'learning_rate': 0.075,\n    'num_leaves': 255,\n    'metric': 'mse'\n}\nmodel = LGBMRegressor(**params)\n\npipe = Pipeline([('fe', ProcessingDataset()), ('scaler', MinMaxScaler()), ('model', model)])\npipe.fit(X_train, y_train);","3ea32d40":"y_preds = pipe.predict(X_valid)\ny_preds = np.where(y_preds > 0, y_preds, 0)\nmean_squared_error(y_valid, y_preds)","7ee6812f":"y_preds = pipe.predict(test)\ny_preds = np.where(y_preds > 0, y_preds, 0)\nmean_squared_error(target_test, y_preds)","2d88cb3d":"params = {\n    'n_estimators': 400,\n    'learning_rate': 0.075,\n    'num_leaves': 255,\n    'metric': 'mse'\n}\nmodel = LGBMRegressor(**params)\n\npipe = Pipeline([('fe', ProcessingDataset()), ('scaler', MinMaxScaler()), ('model', model)])","da63cd80":"n_splits = 5\nkf = KFold(n_splits=n_splits)\n\nvalid_ans = np.zeros(y_valid.shape[0])\ntest_ans = np.zeros(target_test.shape[0])\ncurrent_fold = 0\nfor train_index, test_index in kf.split(X_train):\n    current_fold += 1\n    print(\"Fold #\", current_fold, \"initiate...\")\n    X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n    pipe.fit(X_train_cv, y_train_cv)\n    y_pred_fold = pipe.predict(X_test_cv)\n    print(\"Fold MSE: \", mean_squared_error(y_test_cv, y_pred_fold))\n    valid_ans += pipe.predict(X_valid)\n    test_ans += pipe.predict(test)\nprint(\"Valid prediction: \", mean_squared_error(y_valid, valid_ans \/ n_splits))","1ce8ea7c":"test_preds = test_ans \/ n_splits\ntest_preds = np.where(test_preds > 0, test_preds, 0)\nmean_squared_error(target_test, test_preds)","18b5a2d3":"We approached different models for given data, and tried a bit different from baseline notebook feature engineering. Given baseline for this work was set at 4850, so `hochu zachet` condition should be assumed as completed :) Thanks for your attention.","b5cb298a":"Valid score is not fine, but test score is even bigger :)","57334ea4":"## Feature engineering","c2e1f7cb":"We will start with a simple baseline. Note, that if our prediction is less than 0, there's no reason to return negative value instead of zero.","47d0c2ff":"Validation:","6b8b10b5":"## Linear model","f2d824a3":"Fit CV model:","8d640179":"Drop some uninformative features which we aren't gonna use:","09ed9e43":"Lets try something harder. So we'll use regressor based on decision trees.","547284a6":"## KFolds GBDT","6150d38c":"We see that revenue is different for different OS type. I will use idea to replace id's with their values with mean cpm for previous period (i'll define it later).\n\nSo next transformer will be used:","5f6ab854":"In this model, we will predict CPM in the third part of June (2019-06-22 and later). We also drop outliers based on 95% quantile for test data.","f26f1d2c":"Some features has strong correlation (what is logically acceptable), but anyway they improve final model a bit more.","673b4ee8":"Now let's analyze what do we exactly have (especially with categorial features):","5f58cd9d":"Lets create valid part of dataset to fine-tune parameters and verify quality of the model.","c4f79b0f":"Read the dataset:","64874dd7":"Here, as it was told before, i replaced categorical features with their \"scores\". In addition, i added views to measurable ratio (as in baseline notebook). I tried to use date column assuming that revenue in weekend might be higher, but it worked bad in model.","c52c5cc7":"While valid score for CV model looks stable, our test score is again about 20% higher than it, what might be a consequence of covariance shift.","703d53c8":"Build test results:","22ba90f3":"Take a look at how our data will look like after engineering:","3611474b":"## Create datasets","353777d0":"## Modelling","d7618124":"## GBDT","5486ec6a":"We have some negative values in CPM columns, what is weird, so lets drop it:","8dcd4fd4":"In previous parts, our test result was about 20% higher than validation. To justify that a reason is not a deviation, lets build cross-validation model.","32c89981":"## Conclusion","b425da74":"In this notebook we'll try to predict reserve price (here it's roughly CPM) based on given dataset.\nhttps:\/\/www.kaggle.com\/akshaypaliwal709\/predicting-the-reserve-price-base-cpm was used as an inspiration of the problem.","a63636e7":"Test score:","e8a0868f":"Let's compute CPM in a same way (just bit modified to increase computing time):","531e02cc":"If we try to group values by CPM (here - roughly at full dataset, just to show basic idea):"}}