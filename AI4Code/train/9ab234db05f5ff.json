{"cell_type":{"94a8c74a":"code","d14d72cd":"code","a06454dd":"code","a401bc6d":"code","4ebb23f7":"code","2fd6ee82":"code","c560b371":"code","cb5b7cf2":"code","84091ad0":"code","d3d66db4":"code","ee9425cb":"code","cbd69cd4":"code","3ff9ea46":"code","50c7a458":"code","87ae0821":"code","e0f7e630":"code","fa6b9ad9":"code","98ef14c3":"code","605d004d":"code","731fd550":"code","5b6ad423":"code","dbb42743":"code","ce0f486a":"code","c750c1c8":"code","5ed2a1be":"code","dddd4ff4":"code","99aa8bf9":"code","3ac4c760":"code","853a3ed6":"code","cf8185e8":"code","6d0088c5":"code","2ab8c322":"code","df8fc3a0":"code","2d20f67c":"code","46f57f2e":"code","b93113e9":"code","3e20c4db":"code","6d23f1e9":"code","701f9810":"code","e05b855b":"code","5f106798":"code","6a092558":"code","6e020807":"code","c75fe69c":"code","ace18183":"code","fb291520":"code","35909b3e":"code","e4d542a8":"code","c60e6d86":"code","c0fdfce0":"code","7a657f97":"code","59de10bc":"code","dbd63426":"code","6591bcce":"code","630beb7f":"code","e1914138":"code","d0d1d978":"code","6d31027c":"code","570e6f67":"code","58af0626":"markdown","1a050023":"markdown","c702dd07":"markdown","88231c60":"markdown","6be7cc3e":"markdown","2efd5e9e":"markdown","9030e98c":"markdown","536b7a28":"markdown","c70d7bc1":"markdown","c1545b02":"markdown","c0b7dbf8":"markdown","eea3f29a":"markdown","68631e5a":"markdown","5bc957bb":"markdown","ef5e7589":"markdown","7576901f":"markdown","27e26cf9":"markdown","09a00750":"markdown","1d0cdef8":"markdown","210486ff":"markdown","9d161b7e":"markdown","6dfc4f58":"markdown","0de5e263":"markdown","04bf165c":"markdown","7f20b648":"markdown","3346fca6":"markdown","8b649734":"markdown","19d567fc":"markdown","dce1222a":"markdown","340a90fc":"markdown","70a27f9b":"markdown","f302a8c9":"markdown","f55abf8d":"markdown","0094fa3f":"markdown","5f00a6b9":"markdown","14893119":"markdown","57d02f50":"markdown","326ccaee":"markdown","31711fac":"markdown","813e8618":"markdown","b2ac4e35":"markdown","4654c082":"markdown","633472f4":"markdown","10620ec3":"markdown","f0760677":"markdown","60f61653":"markdown","214ffdb2":"markdown","8b12b577":"markdown"},"source":{"94a8c74a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d14d72cd":"train = pd.read_csv('..\/input\/titanic\/train.csv')","a06454dd":"train.head()","a401bc6d":"train.info()","4ebb23f7":"train.describe()","2fd6ee82":"plt.figure(figsize=(15,3))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","c560b371":"sns.set_style('whitegrid')\nsns.set_palette('bright')\n\nsns.countplot(x='Survived',data=train)","cb5b7cf2":"sns.countplot(x='Survived',hue='Sex',data=train)","84091ad0":"sns.countplot(x='Survived',hue='Pclass',data=train)","d3d66db4":"plt.figure(figsize=(13,5))\nsns.distplot(train['Age'].dropna(),kde=False,bins=30)","ee9425cb":"g = sns.FacetGrid(train, hue=\"Sex\", height=6, aspect=2, palette='coolwarm')\ng = g.map(plt.hist, \"Age\", bins=20, alpha=0.8)\ng.add_legend()","cbd69cd4":"plt.figure(figsize=(13,5))\nsns.countplot(x='SibSp',data=train)","3ff9ea46":"plt.figure(figsize=(13,5))\nsns.distplot(train['Fare'].dropna(),kde=False,bins=40)","50c7a458":"# ploting bar plot for Embarked vs Survived\nsns.barplot(x=\"Embarked\",y=\"Survived\",data= train)","87ae0821":"plt.figure(figsize=(13, 5))\nsns.boxplot(x='Pclass',y='Age',data=train,palette='winter')","e0f7e630":"train.groupby('Pclass')['Age'].mean()","fa6b9ad9":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 38\n\n        elif Pclass == 2:\n            return 30\n\n        else:\n            return 25\n\n    else:\n        return Age","98ef14c3":"train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)","605d004d":"plt.figure(figsize=(15,3))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","731fd550":"train.drop('Cabin',axis=1,inplace=True)","5b6ad423":"train.head()","dbb42743":"train.dropna(inplace=True)","ce0f486a":"train.info()","c750c1c8":"sex = pd.get_dummies(train['Sex'],drop_first=True)\nembark = pd.get_dummies(train['Embarked'],drop_first=True)","5ed2a1be":"# sex\n# embark","dddd4ff4":"train.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)","99aa8bf9":"train = pd.concat([train,sex,embark],axis=1)","3ac4c760":"train.drop('PassengerId',axis=1,inplace=True)","853a3ed6":"train.head()","cf8185e8":"X=train.drop('Survived',axis=1)\ny=train['Survived']","6d0088c5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","2ab8c322":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n\ndef print_validation_report(y_true, y_pred):\n    print(\"Classification Report\")\n    print(classification_report(y_true, y_pred))\n    acc_sc = accuracy_score(y_true, y_pred)\n    f1_sc=f1_score(y_true,y_pred)\n    print(\"Accuracy Score : \"+ str(acc_sc))\n    print(\"F1 Score : \"+ str(f1_sc))\n\ndef plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", cbar=False)\n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","df8fc3a0":"from sklearn.linear_model import LogisticRegression\n\nclf_log = LogisticRegression(max_iter=1000)\nclf_log.fit(X_train,y_train)\npred_log = clf_log.predict(X_test)","2d20f67c":"print_validation_report(y_test,pred_log)\nacc_log=accuracy_score(y_test,pred_log)","46f57f2e":"plot_confusion_matrix(y_test,pred_log)","b93113e9":"# FEATURE IMPORTANCE\n\nimportance = abs(clf_log.coef_[0])\ncoeffecients = pd.DataFrame(importance, X_train.columns)\ncoeffecients.columns = ['Coeffecient']\nplt.figure(figsize=(15,4))\nplt.bar(X_train.columns,importance)\nplt.show()","3e20c4db":"from sklearn.neighbors import KNeighborsClassifier\n\nclf_knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None,n_neighbors=7, p=2, weights='uniform')\nclf_knn.fit(X_train,y_train)\npred_knn = clf_knn.predict(X_test)","6d23f1e9":"print_validation_report(y_test,pred_knn)\nacc_knn=accuracy_score(y_test,pred_knn)","701f9810":"plot_confusion_matrix(y_test,pred_knn)","e05b855b":"error_rate = []\naccuracy_scs =[]\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    accuracy_sc=accuracy_score(y_test,pred_i)\n    accuracy_scs.append(accuracy_sc)","5f106798":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","6a092558":"plt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),accuracy_scs,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Accuracy Score vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Accuracy Score')","6e020807":"from sklearn.svm import SVC\n\nclf_svc = SVC(kernel=\"linear\")\nclf_svc.fit(X_train,y_train)\npred_svc = clf_svc.predict(X_test)","c75fe69c":"print_validation_report(y_test,pred_svc)\nacc_svc=accuracy_score(y_test,pred_svc)","ace18183":"plot_confusion_matrix(y_test,pred_svc)","fb291520":"from sklearn.naive_bayes import GaussianNB\nclf_nb = GaussianNB()\nclf_nb.fit(X_train, y_train)\npred_nb = clf_nb.predict(X_test)","35909b3e":"print_validation_report(y_test,pred_nb)\nacc_nb=accuracy_score(y_test,pred_nb)","e4d542a8":"plot_confusion_matrix(y_test,pred_nb)","c60e6d86":"from sklearn.ensemble import RandomForestClassifier\nclf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\npred_rf = clf_rf.predict(X_test)","c0fdfce0":"print_validation_report(y_test,pred_rf)\nacc_rf=accuracy_score(y_test,pred_rf)","7a657f97":"plot_confusion_matrix(y_test,pred_nb)","59de10bc":"models = pd.DataFrame({\n    'Model': [\"LOGISTIC REGRESSION\",\"K NEAREST NEIGHBORS\",\"SUPPORT VECTOR MACHINE\",\"NAIVE BAYES\",\"RANDOM FOREST\"],\n    'Score': [acc_log,acc_knn,acc_svc,acc_nb,acc_rf\n              ]})\nmodels.sort_values(by='Score', ascending=False)","dbd63426":"test= pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest.info()","6591bcce":"test['Fare'].fillna(test['Fare'].mean(),inplace=True)","630beb7f":"test.info()","e1914138":"test['Age'] = test[['Age','Pclass']].apply(impute_age,axis=1)\ntest.drop('Cabin',axis=1,inplace=True)\ntest.dropna(inplace=True)\nsex = pd.get_dummies(test['Sex'],drop_first=True)\nembark = pd.get_dummies(test['Embarked'],drop_first=True)\ntest.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)\ntest = pd.concat([test,sex,embark],axis=1)\npassengerID = test['PassengerId']\ntest.drop('PassengerId',axis=1,inplace=True)","d0d1d978":"test.head()","6d31027c":"test_copy=test.copy()","570e6f67":"test= pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\nprediction = clf_log.predict(test_copy)\n\noutput = pd.DataFrame({ 'PassengerId' : passengerID, 'Survived': prediction })\noutput.to_csv('submission.csv', index=False)\nprint(output)","58af0626":"### 3.5 Random Forest","1a050023":"Does the embarkment port matter in the survival chance?","c702dd07":"We can see that the males are older on average.","88231c60":"Let's see the survival ratio, our prediction on the test data should aim for around the same %. Let's add some hues in the survival ratio, like gender and ticket class etc, to find any initial correlations.","6be7cc3e":"Evaluation using predefined functions:","2efd5e9e":"# 2. Data Pre-proceesing","9030e98c":"# 3. Models","536b7a28":"The average age for each class is:","c70d7bc1":"### Data Cleaning (Filling)\n\nThe cabin data is mostly missing, so it is likely that it will be ignored in the final model.\nThe age data can be dealt with in 2 ways:\n- imputation, replace missing values (NaN) with the mean age on the ship (not really accurate)\n- calculate the mean Age for every Pclass and replace that in the missing fields (a lot more accurate, as older people are expected to be in Class 1, as opposed to younger people in Class 3. Let's check this with a boxplot:","c1545b02":"**Observation:** Older = higher class, as expected.","c0b7dbf8":"Loading the Titanic dataset @ Kaggle:","eea3f29a":"# 1. Exploratory Data Analysis\n\n\n### Missing Data:","68631e5a":"Let's drop the Sex, Embarked, Name and Ticket columns. **Feature engineering could be done on the Name column (i.e. extracting the title and encoding it with LabelEncoder).**","5bc957bb":"Fare distribution:","ef5e7589":"Accuracy score, F1 score, confusion matrix and classification report.","7576901f":"### Initial Analysis on the dataset:","27e26cf9":"Not so bad results! Suggestions on futher **feature engineering** for even more accuracy:\n* Name column: grab title with NLP and encode it\n* Cabin column: letter could be a feature","09a00750":"Siblings count:","1d0cdef8":"Optimize value of K:","210486ff":"### Performance Functions are predefined as such:","9d161b7e":"Evaluation usign predefined functions:","6dfc4f58":"Only 38% of all people survived, and mean age is around 29 years, mean Fare is 32.2 dollars (no inflation back then)!","0de5e263":"Data columns are:\n- **Target label**: survival - Survival (0 = No; 1 = Yes)\n- class - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- name - Name\n- sex - Sex\n- age - Age\n- sibsp - Number of Siblings\/Spouses Aboard\n- parch - Number of Parents\/Children Aboard\n- ticket - Ticket Number\n- fare - Passenger Fare\n- cabin - Cabin\n- embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)","04bf165c":"Let's check that heat map again to see if the missing Age data is there:","7f20b648":"### Converting Features (categorical ones)","3346fca6":"### 3.4 Naive-Bayes Gaussian","8b649734":"We can now apply different models to the training set and compare their performance.","19d567fc":"I will use seaborn to create a simple heatmap that displays missing data:","dce1222a":"### 3.2 KNN Classifier","340a90fc":"**Feature importance in Logistic Regression**:","70a27f9b":"Will use the entire train dataset for both training+testing; however the test.csv dataset can be cleaned identically and used for testing only. Splitting data into train and test:","f302a8c9":"### Train-test split:","f55abf8d":"Evaluation usign predefined functions:","0094fa3f":"As expected, the Sex and Class of the passangers matters the most.","5f00a6b9":"K=7 seems a good choice.","14893119":"Evaluation using predefined functions:","57d02f50":"This is the final DF that can be used for training. Survived column can be dropped to train-test split.","326ccaee":"Same pre-processing as for the training dataset:","31711fac":"**Observation:** a bi-normal distribution leaning towards younger people (i.e. 20 years old).","813e8618":"Drop the Cabin column and the 2 rows in Embarked that are missing (NaN).","b2ac4e35":"# Classification Project: Machine Learning for Titanic survival prediction\n\n![image.png](attachment:image.png)","4654c082":"The SEX and EMBARK columns are categorical, so they need encoding with either methods:\n- dummy variables and drop first column\n- LabelEncoding (automatic in pandas)","633472f4":"Importing Python packages:","10620ec3":"## Submission:","f0760677":"### 3.1 Logistic Regression Model","60f61653":"### 3.3 SVM Classifier","214ffdb2":"## Performance comparison","8b12b577":"- roughly 30% of age data is missing, so the missing values could potentially be replaced with the median age\n- a lot of the cabin data is missing, so this column will probably be ignored in the training model section (this column could be replaced with a binary 1\/0 if the cabin is known\/unknown)"}}