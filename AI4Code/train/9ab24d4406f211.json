{"cell_type":{"ea3b7924":"code","ae1e97a9":"code","b2efbdcf":"code","67c12472":"code","cb42d647":"code","1a5f5c35":"code","e9ff7cb0":"code","9747ec17":"code","5359ca91":"code","942d1993":"code","828c00a2":"code","c3fe6ab4":"code","c26fed88":"code","5fc80945":"code","dc0e8eeb":"code","0e6c049e":"markdown","19a2321c":"markdown","dc624f13":"markdown"},"source":{"ea3b7924":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ae1e97a9":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF Version : ',tf.__version__)","b2efbdcf":"def read_train():\n    train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\n    train['text'] = train['text'].astype(str)\n    train['selected_text'] = train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n    test['text'] = test['text'].astype(str)\n    return test\n\ndef read_submission():\n    submission = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n    return submission\n\ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","67c12472":"train_df.head()","cb42d647":"submission_df.head()","1a5f5c35":"#Implementing Jaccard score\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","e9ff7cb0":"MAX_LEN = 96\nPATH  = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file = PATH + 'vocab-roberta-base.json',\n    merges_file = PATH + 'merges-roberta-base.txt',\n    lowercase = True,\n    add_prefix_space = True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","9747ec17":"pd.options.display.max_rows = None\npd.options.display.max_columns = None","5359ca91":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN), dtype='int32')\nattention_mask = np.zeros((ct, MAX_LEN), dtype = 'int32')\ntoken_type_ids = np.zeros((ct, MAX_LEN), dtype = 'int32')\nstart_tokens = np.zeros((ct, MAX_LEN), dtype = 'int32')\nend_tokens = np.zeros((ct, MAX_LEN), dtype = 'int32')\n\nfor k in range(train_df.shape[0]):\n    \n    #Find overlap\n    text1 = \" \" + \" \".join(train_df.loc[k, 'text'].split())\n    text2 = \" \".join(train_df.loc[k, 'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros(len(text1))\n    chars[idx : idx + len(text2)] = 1\n    if text1[idx - 1] == ' ':\n        chars[idx-1] == 1\n    enc = tokenizer.encode(text1)\n    #print('Text 1 :', text1, '\\n Text 2 :', text2, '\\n Match Index :', idx, '\\n Character array :', chars, '\\n')\n    \n    #ID Offsets\n    idx = 0\n    offsets = []\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n        #print(w)\n        #print(offsets)\n        \n    #Start End Tokens\n    toks = []\n    for i, (a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm > 0:\n            toks.append(i)\n            \n    s_tok = sentiment_id[train_df.loc[k, 'sentiment']]\n    input_ids[k, :len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2] \n    #print(input_ids[0])\n    attention_mask[k, :len(enc.ids)+5] = 1\n    if len(toks) > 0:\n        start_tokens[k, toks[0] + 1] = 1\n        end_tokens[k, toks[-1] + 1] = 1","942d1993":"#Test dataset preprocessing\nct = test_df.shape[0]\ninput_ids_t = np.ones((ct, MAX_LEN), dtype = 'int32')\nattention_mask_t = np.zeros((ct, MAX_LEN), dtype = 'int32')\ntoken_type_ids_t = np.zeros((ct, MAX_LEN), dtype = 'int32')\n\nfor k in range(ct):\n    #Input IDs\n    text1  = \" \" + \" \".join(test_df.loc[k, 'text'].split())\n    enc = tokenizer.encode(text1)\n    s_tok = sentiment_id[test_df.loc[k, 'sentiment']]\n    input_ids_t[k, :len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k, :len(enc.ids)+5] = 1","828c00a2":"def scheduler(epoch):\n    return 3e-5 * 0.2**epoch","c3fe6ab4":"#Model Building\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN, ), dtype='int32')\n    att = tf.keras.layers.Input((MAX_LEN, ), dtype='int32')\n    tok = tf.keras.layers.Input((MAX_LEN, ), dtype='int32')\n    \n    config = RobertaConfig.from_pretrained(PATH + 'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH + 'pretrained-roberta-base.h5', config = config)\n    \n    x = bert_model(ids, attention_mask = att, token_type_ids = tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n    x1 = tf.keras.layers.Conv1D(128, 2, padding='same')(x1)\n    x1 = tf.keras.layers.ReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2, padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0])\n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.ReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n    \n    model = tf.keras.models.Model(inputs = [ids, att, tok], outputs = [x1, x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='binary_crossentropy', optimizer = optimizer)\n    \n    return model","c26fed88":"#Inference\nn_splits = 5\npreds_start = np.zeros((input_ids_t.shape[0], MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0], MAX_LEN))\n\nfor i in range(5):\n    print('#'*25)\n    print('Model %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('..\/input\/model4\/v4-roberta-%i.h5'%i)\n    \n    print('Predicting Text...')\n    preds = model.predict([input_ids_t, attention_mask_t, token_type_ids_t], verbose = 1)\n    preds_start += preds[0] \/ n_splits\n    preds_end += preds[0] \/ n_splits ","5fc80945":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_df.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","dc0e8eeb":"#Result\ntest_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)","0e6c049e":"**Data Preprocessing**","19a2321c":"Model","dc624f13":"We will skip the training stage and use the pretrained model"}}