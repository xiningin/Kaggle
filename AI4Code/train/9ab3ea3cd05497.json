{"cell_type":{"8f5cf79e":"code","ceb8950b":"code","389c7a1b":"code","77e768aa":"code","f46dea0e":"code","5f9d163c":"code","177f39d0":"code","6964b467":"code","c91f971b":"code","c2a4302e":"code","464cb132":"code","33124883":"code","3e1ce0bf":"markdown","c5ee6f7c":"markdown","7503e77a":"markdown","560db76b":"markdown","c795508a":"markdown","6e6b6bd7":"markdown","75ecb612":"markdown","6931ec0c":"markdown","a686c227":"markdown","5d71630d":"markdown","3b73c857":"markdown"},"source":{"8f5cf79e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"..\/input\"))","ceb8950b":"# Multiple Columns Label Encoder\nfrom sklearn.preprocessing import LabelEncoder\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns \n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X):\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)","389c7a1b":"# Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nChurn_Modelling = pd.read_csv(\"..\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv\")\nX = Churn_Modelling.iloc[:,3:-1]\ny = Churn_Modelling.iloc[:,-1]\nX = MultiColumnLabelEncoder(columns = ['Geography','Gender']).fit_transform(pd.DataFrame(X))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nsc = MinMaxScaler(feature_range=(0,1))\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX","77e768aa":"# LSTM Implementation\nimport keras\nfrom subprocess import check_output\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nimport time\ntrainX = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\ntestX = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))","f46dea0e":"from numpy import newaxis\nmodel = Sequential()\n\nmodel.add(LSTM(input_shape=(1,10),units=6,return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(32,return_sequences=True))\nmodel.add(LSTM(32))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(activation=\"sigmoid\", units=1))\n\nstart = time.time()\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\nprint ('compilation time : ', time.time() - start)","5f9d163c":"history=model.fit(trainX,y_train,batch_size=500,epochs=1000,validation_split=0.1)","177f39d0":"trainPredict = model.predict(trainX)\nprint(trainPredict)\nprint(model.summary())","6964b467":"plt.plot(np.array(history.history['accuracy']) * 100)\nplt.plot(np.array(history.history['val_accuracy']) * 100)\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.legend(['train', 'validation'])\nplt.title('Accuracy over epochs')\nplt.show()","c91f971b":"y_pred = model.predict(testX)\nprint(y_pred[:5])","c2a4302e":"y_pred = (y_pred > 0.5).astype(int)\nprint(y_pred[:5])","464cb132":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","33124883":"print (((cm[0][0]+cm[1][1])*100)\/(len(y_test)), '% of testing data was classified correctly')","3e1ce0bf":"**Data Preprocessing**\nafter, we pass to the preprocessing phase, in this part, we separate training data and test, and we standardize the data with the *MinMaxScaler* function instead of the *StandardScaler* method because the range of values must be between 0 and 1.","c5ee6f7c":"The output network should converge to an accuracy of around 86%","7503e77a":"**Making the Confusion Matrix**","560db76b":"**Significance of the confusion matrix value:**\n\nThis means that we should have about  *(1547+184)=1731*  correct classifications out of our total testing data size of  2000 . This means that our accuracy for this trial was  *1731\u00f72000=0.8655* , which matches the classifier's prediction","c795508a":"**Fitting the RNN**\n\nThis is where we will be fitting the RNN to our training set.\n\nThe breakdown of the inputs for compiling is as follows:\n**trainX** The independent variable portion of the data which needs to be fitted with the model.\n\n**y_train** The output portion of the data which the model needs to produce after fitting.\n\n**batch_size:** How often we want to back-propogate the error values so that individual node weights can be adjusted.\n\n**epochs:** The number of times we want to run the entire test data over again to tune the weights. This is like the fuel of the algorithm.\n\n**validation_split:** 0.1 The fraction of data to use for validation data.","6e6b6bd7":"**Customer Classification**\n\nWe have a dataset consisting of Bank Customer information, so we build a classifier which will tell us if a customer will exit the bank or not.","75ecb612":"**Testing the RNN**\nPredicting the Test set results\nThis shows the probability of a customer leaving given the testing data. Each row in X_test corresponds to a row in Y_test","6931ec0c":"We start by encoding the categorical values:","a686c227":"To use the confusion Matrix, we need to convert the probabilities that a customer will leave the bank into the form true or false. So we will use the cutoff value 0.5 to indicate whether they are likely to exit or not.","5d71630d":"**Making the RNN (LSTM)**","3b73c857":"A hurestic tip is that the amount of nodes (dimensions) in your hidden layer should be the average of your input and output layers, which means that since we have **11** dimensions and we are looking for a binary output, we calculate this to be  **(11+1)\u00f72=6** .\n\n**The breakdown of the inputs for the output layer is as follows:**\n\n**optimizer:** *adam* The algorithm we want to use to find the optimal set of weights in the neural networks. Adam is a very efficeint variation of Stochastic Gradient Descent.\n\n**loss:** *binary_crossentropy* This is the loss function used within adam. This should be the logarthmic loss. If our dependent (output variable) is Binary, it is binary_crossentropy. If Categorical, then it is called categorical_crossentropy\n\n**metrics:** *[accuracy]* The accuracy metrics which will be evaluated(minimized) by the model. Used as accuracy criteria to imporve model performance."}}