{"cell_type":{"96610492":"code","8c60e4ad":"code","2e164274":"code","d9c4b343":"code","981b53db":"code","d732ee49":"code","063787aa":"code","66759de0":"code","fbf1bd03":"code","bd055fc2":"code","bb0bd245":"code","77280f04":"code","215b2eec":"code","e40adc81":"code","07862fe6":"code","a54912b2":"code","15f83286":"code","2528e101":"code","e779b272":"code","265a0d0a":"code","8ead2844":"code","a1dc74c8":"code","f978298b":"code","dc301604":"code","33429e05":"code","6457df34":"code","fde3a68f":"code","9b8520f9":"code","5b6b4127":"code","3b3df7a5":"code","1e79c578":"code","df23724c":"code","9e011d32":"code","1a36993b":"code","beaf8bb7":"code","28a7780f":"code","2ad7b773":"code","4e5767ee":"code","61109704":"code","7931d936":"code","f7630c69":"code","ac3a846b":"code","5f734cbe":"code","347e9ba2":"code","34177675":"code","b4095e6d":"code","dda512d8":"code","f150f73f":"code","33b508b1":"code","a9b1ab4a":"code","6ce8b837":"code","6028eb34":"code","68a42930":"code","3041f602":"code","ee7fe43d":"code","ec396301":"code","6b43dd66":"code","d9ba652c":"code","c05abe7b":"code","db70b1fb":"code","10bd9eb5":"code","4b649719":"code","4f64d67c":"code","215ef8b9":"code","df4df884":"code","ac4161c6":"code","45e7208b":"code","66c128ca":"code","306d36cd":"code","92fff573":"code","8497c5dc":"code","ca6f2802":"code","625b71f7":"code","fada1737":"code","5c556b83":"code","9d49f0a1":"code","f9986c9d":"code","9579f06a":"code","5ea80cd4":"code","32b1c2f9":"code","43ff1185":"code","1a16ef9e":"code","1263aca7":"code","cbffa406":"code","6d9ad57e":"code","c8e05e9f":"code","94e77f35":"code","802ef31f":"code","642fce1c":"code","6fda8de0":"code","c8a85e99":"code","db82aa7f":"code","bf8026c5":"code","ac3549b5":"code","3f16f95e":"code","be994e8f":"code","f597ff99":"code","1ac35ea9":"code","fc262420":"code","a5b22ce6":"code","a5afb30a":"code","8c4b19c0":"code","a726a311":"code","ac6ae25b":"code","c4786f00":"code","d1f7d9d5":"code","6a5765ea":"markdown","d714fd95":"markdown","22152a3f":"markdown","3ffcafa5":"markdown","5f65dfa9":"markdown","88fc5cf1":"markdown","7c4ee7fb":"markdown","03bcdb6d":"markdown","d6849b7d":"markdown","0581544c":"markdown","b0fec07c":"markdown","62b31ca3":"markdown","34e0f871":"markdown","af2b6809":"markdown","94210cf5":"markdown","303775c4":"markdown","4eb7c4f1":"markdown","a04f5f89":"markdown","ce33c6bf":"markdown","6426884b":"markdown","0d77a63c":"markdown","7bab39ad":"markdown","233eaff7":"markdown","7016a329":"markdown","7ea3db58":"markdown","3107ce99":"markdown","abfc5704":"markdown","55274458":"markdown","43cfcbde":"markdown","0744beee":"markdown","d0b31be8":"markdown","105670c3":"markdown","26d40ec3":"markdown","cd366c6e":"markdown","cffbf83f":"markdown","165ed438":"markdown","68351160":"markdown","3513f83c":"markdown","c5d65d23":"markdown","79fcf39a":"markdown","5f072386":"markdown","95c74766":"markdown","77a354ef":"markdown","85259236":"markdown","6613f6c5":"markdown","51203485":"markdown","69f6aa1a":"markdown","707608cf":"markdown","8a5d5d09":"markdown","5c083a0a":"markdown","37f09516":"markdown","607370b5":"markdown","5bb67a77":"markdown","50c98a2a":"markdown","46a66bb0":"markdown","bbfc0385":"markdown","637d848c":"markdown","76cade1e":"markdown","7366175e":"markdown","a77a3960":"markdown","99c17477":"markdown","472f255f":"markdown","c870c850":"markdown","cc5b2d32":"markdown","d7131069":"markdown","96d06c18":"markdown","78c91a1b":"markdown","f6572d40":"markdown","6844876b":"markdown","6a15d37c":"markdown","ed75d1ff":"markdown","591ce31f":"markdown","900819d2":"markdown","fd281db4":"markdown","8aba2e73":"markdown","932f2676":"markdown","cdd7a2af":"markdown","eb2e2c55":"markdown","1cde3ed3":"markdown","0397aaac":"markdown","f69cf0f9":"markdown","2650b5ba":"markdown","0d80903c":"markdown","ab9753d1":"markdown","f785245f":"markdown","47f1ad82":"markdown","19df8442":"markdown","ec1f43e0":"markdown","20c27ee2":"markdown","7b0a8be9":"markdown","997c9cea":"markdown","835a599f":"markdown","8d692d93":"markdown","34896d54":"markdown","8906eab7":"markdown","3271c6f3":"markdown","61229a6b":"markdown","384dc0e8":"markdown","23b67896":"markdown","11b9edeb":"markdown","ec3b891d":"markdown","feb5e332":"markdown","2b53bceb":"markdown","2e96ae25":"markdown","f0b47d49":"markdown","b31d2526":"markdown","09629e13":"markdown","6c2aa9b5":"markdown","2a8adebb":"markdown","cf2cc79d":"markdown","2263050c":"markdown","6ee9b86c":"markdown","f4bb5bd7":"markdown","f47bbfd2":"markdown","d3d2fc24":"markdown","6bb2d1c8":"markdown","3fb8d876":"markdown","e39d3e13":"markdown"},"source":{"96610492":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.manifold import TSNE\n# from sklearn.manifold import TSNE\n# import matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\nimport time\n\nfrom plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()\nfrom collections import Counter\n# merge two sparse matrices: https:\/\/stackoverflow.com\/a\/19710648\/4084039\nfrom scipy.sparse import hstack\nimport scipy\nprint(\"DONE LOADING-------\")","8c60e4ad":"project_data = pd.read_csv('..\/input\/train_data.csv')\nresource_data = pd.read_csv('..\/input\/resources.csv')","2e164274":"# Taking radom samples for less memory machines\n# -> https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-sample\/\n# -> https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.sample.html\n# project_data= project_data1.sample(n = 5000) \n# project_data= project_data1.sample(n = 5000) ","d9c4b343":"print(\"Number of data points in train data\", project_data.shape)\nprint('-'*50)\nprint(\"The attributes of data :\", project_data.columns.values)","981b53db":"print(\"Number of data points in train data\", resource_data.shape)\nprint(resource_data.columns.values)\nresource_data.head(2)","d732ee49":"prefixlist=project_data['teacher_prefix'].values\nprefixlist=list(prefixlist)\ncleanedPrefixList = [x for x in project_data['teacher_prefix'] if x != float('nan')] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\nlen(cleanedPrefixList)\n# print(len(prefixlist))","063787aa":"## Converting to Nan and Droping -> https:\/\/stackoverflow.com\/a\/29314880\/4433839\n\n# df[df['B'].str.strip().astype(bool)] \/\/ for deleting EMPTY STRINGS.\nproject_data.dropna(subset=['teacher_prefix'], inplace=True)\nproject_data.shape","66759de0":"# PROVIDE CITATIONS TO YOUR CODE IF YOU TAKE IT FROM ANOTHER WEBSITE.\n# https:\/\/matplotlib.org\/gallery\/pie_and_polar_charts\/pie_and_donut_labels.html#sphx-glr-gallery-pie-and-polar-charts-pie-and-donut-labels-py\n\n\ny_value_counts = project_data['project_is_approved'].value_counts()\nprint(\"Number of projects thar are approved for funding \", y_value_counts[1], \", (\", (y_value_counts[1]\/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\nprint(\"Number of projects thar are not approved for funding \", y_value_counts[0], \", (\", (y_value_counts[0]\/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect=\"equal\"))\nrecipe = [\"Accepted\", \"Not Accepted\"]\n\ndata = [y_value_counts[1], y_value_counts[0]]\n\nwedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)\n\nbbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\nkw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\n\nfor i, p in enumerate(wedges):\n    ang = (p.theta2 - p.theta1)\/2. + p.theta1\n    y = np.sin(np.deg2rad(ang))\n    x = np.cos(np.deg2rad(ang))\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                 horizontalalignment=horizontalalignment, **kw)\n\nax.set_title(\"Nmber of projects that are Accepted and not accepted\")\n\nplt.show()","fbf1bd03":"# Pandas dataframe groupby count, mean: https:\/\/stackoverflow.com\/a\/19385591\/4084039\n\ntemp = pd.DataFrame(project_data.groupby(\"school_state\")[\"project_is_approved\"].apply(np.mean)).reset_index()\n# if you have data which contain only 0 and 1, then the mean = percentage (think about it)\ntemp.columns = ['state_code', 'num_proposals']\n\n'''# How to plot US state heatmap: https:\/\/datascience.stackexchange.com\/a\/9620\n\nscl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = temp['state_code'],\n        z = temp['num_proposals'].astype(float),\n        locationmode = 'USA-states',\n        text = temp['state_code'],\n        marker = dict(line = dict (color = 'rgb(255,255,255)',width = 2)),\n        colorbar = dict(title = \"% of pro\")\n    ) ]\n\nlayout = dict(\n        title = 'Project Proposals % of Acceptance Rate by US States',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)',\n        ),\n    )\n\nfig = go.Figure(data=data, layout=layout)\noffline.iplot(fig, filename='us-map-heat-map')\n'''","bd055fc2":"# https:\/\/www.csi.cuny.edu\/sites\/default\/files\/pdf\/administration\/ops\/2letterstabbrev.pdf\ntemp.sort_values(by=['num_proposals'], inplace=True)\nprint(\"States with lowest % approvals\")\nprint(temp.head(5))\nprint('='*50)\nprint(\"States with highest % approvals\")\nprint(temp.tail(5))","bb0bd245":"#stacked bar plots matplotlib: https:\/\/matplotlib.org\/gallery\/lines_bars_and_markers\/bar_stacked.html\ndef stack_plot(data, xtick, col2='project_is_approved', col3='total'):\n    ind = np.arange(data.shape[0])\n    \n    plt.figure(figsize=(20,5))\n    p1 = plt.bar(ind, data[col3].values)\n    p2 = plt.bar(ind, data[col2].values)\n\n    plt.ylabel('Projects')\n    plt.title('Number of projects aproved vs rejected')\n    plt.xticks(ind, list(data[xtick].values))\n    plt.legend((p1[0], p2[0]), ('total', 'accepted'))\n    plt.show()","77280f04":"def univariate_barplots(data, col1, col2='project_is_approved', top=False):\n    # Count number of zeros in dataframe python: https:\/\/stackoverflow.com\/a\/51540521\/4084039\n    temp = pd.DataFrame(project_data.groupby(col1)[col2].agg(lambda x: x.eq(1).sum())).reset_index()\n\n    # Pandas dataframe grouby count: https:\/\/stackoverflow.com\/a\/19385591\/4084039\n    temp['total'] = pd.DataFrame(project_data.groupby(col1)[col2].agg({'total':'count'})).reset_index()['total']\n    temp['Avg'] = pd.DataFrame(project_data.groupby(col1)[col2].agg({'Avg':'mean'})).reset_index()['Avg']\n    \n    temp.sort_values(by=['total'],inplace=True, ascending=False)\n    \n    if top:\n        temp = temp[0:top]\n    \n    stack_plot(temp, xtick=col1, col2=col2, col3='total')\n    print(temp.head(5))\n    print(\"=\"*50)\n    print(temp.tail(5))","215b2eec":"univariate_barplots(project_data, 'school_state', 'project_is_approved', False)","e40adc81":"univariate_barplots(project_data, 'teacher_prefix', 'project_is_approved' , top=False)","07862fe6":"univariate_barplots(project_data, 'project_grade_category', 'project_is_approved', top=False)","a54912b2":"catogories = list(project_data['project_subject_categories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\ncat_list = []\nfor i in catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_') # we are replacing the & value into \n    cat_list.append(temp.strip())","15f83286":"project_data['clean_categories'] = cat_list\nproject_data.drop(['project_subject_categories'], axis=1, inplace=True)\nproject_data.head(2)","2528e101":"univariate_barplots(project_data, 'clean_categories', 'project_is_approved', top=20)","e779b272":"# count of all the words in corpus python: https:\/\/stackoverflow.com\/a\/22898595\/4084039\nfrom collections import Counter\nmy_counter = Counter()\nfor word in project_data['clean_categories'].values:\n    my_counter.update(word.split())","265a0d0a":"# dict sort by value python: https:\/\/stackoverflow.com\/a\/613218\/4084039\ncat_dict = dict(my_counter)\nsorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(sorted_cat_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(sorted_cat_dict.values()))\n\nplt.ylabel('Projects')\nplt.title('% of projects aproved category wise')\nplt.xticks(ind, list(sorted_cat_dict.keys()))\nplt.show()","8ead2844":"for i, j in sorted_cat_dict.items():\n    print(\"{:20} :{:10}\".format(i,j))","a1dc74c8":"sub_catogories = list(project_data['project_subject_subcategories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\n\nsub_cat_list = []\nfor i in sub_catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_')\n    sub_cat_list.append(temp.strip())","f978298b":"project_data['clean_subcategories'] = sub_cat_list\nproject_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\nproject_data.head(2)","dc301604":"univariate_barplots(project_data, 'clean_subcategories', 'project_is_approved', top=50)","33429e05":"# count of all the words in corpus python: https:\/\/stackoverflow.com\/a\/22898595\/4084039\nfrom collections import Counter\nmy_counter = Counter()\nfor word in project_data['clean_subcategories'].values:\n    my_counter.update(word.split())","6457df34":"# dict sort by value python: https:\/\/stackoverflow.com\/a\/613218\/4084039\nsub_cat_dict = dict(my_counter)\nsorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(sorted_sub_cat_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(sorted_sub_cat_dict.values()))\n\nplt.ylabel('Projects')\nplt.title('% of projects aproved state wise')\nplt.xticks(ind, list(sorted_sub_cat_dict.keys()))\nplt.show()","fde3a68f":"for i, j in sorted_sub_cat_dict.items():\n    print(\"{:20} :{:10}\".format(i,j))","9b8520f9":"#How to calculate number of words in a string in DataFrame: https:\/\/stackoverflow.com\/a\/37483537\/4084039\nword_count = project_data['project_title'].str.split().apply(len).value_counts()\nword_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(word_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(word_dict.values()))\n\nplt.ylabel('Numeber of projects')\nplt.xlabel('Numeber words in project title')\nplt.title('Words for each title of the project')\nplt.xticks(ind, list(word_dict.keys()))\nplt.show()","5b6b4127":"approved_title_word_count = project_data[project_data['project_is_approved']==1]['project_title'].str.split().apply(len)\napproved_title_word_count = approved_title_word_count.values\n\nrejected_title_word_count = project_data[project_data['project_is_approved']==0]['project_title'].str.split().apply(len)\nrejected_title_word_count = rejected_title_word_count.values","3b3df7a5":"# https:\/\/glowingpython.blogspot.com\/2012\/09\/boxplot-with-matplotlib.html\nplt.boxplot([approved_title_word_count, rejected_title_word_count])\nplt.xticks([1,2],('Approved Projects','Rejected Projects'))\nplt.ylabel('Words in project title')\nplt.grid()\nplt.show()","1e79c578":"plt.figure(figsize=(10,3))\nsns.kdeplot(approved_title_word_count,label=\"Approved Projects\", bw=0.6)\nsns.kdeplot(rejected_title_word_count,label=\"Not Approved Projects\", bw=0.6)\nplt.legend()\nplt.show()","df23724c":"# merge two column text dataframe: \nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","9e011d32":"approved_word_count = project_data[project_data['project_is_approved']==1]['essay'].str.split().apply(len)\napproved_word_count = approved_word_count.values\n\nrejected_word_count = project_data[project_data['project_is_approved']==0]['essay'].str.split().apply(len)\nrejected_word_count = rejected_word_count.values","1a36993b":"# https:\/\/glowingpython.blogspot.com\/2012\/09\/boxplot-with-matplotlib.html\nplt.boxplot([approved_word_count, rejected_word_count])\nplt.title('Words for each essay of the project')\nplt.xticks([1,2],('Approved Projects','Rejected Projects'))\nplt.ylabel('Words in project essays')\nplt.grid()\nplt.show()","beaf8bb7":"plt.figure(figsize=(10,3))\nsns.distplot(approved_word_count, hist=False, label=\"Approved Projects\")\nsns.distplot(rejected_word_count, hist=False, label=\"Not Approved Projects\")\nplt.title('Words for each essay of the project')\nplt.xlabel('Number of words in each eassay')\nplt.legend()\nplt.show()","28a7780f":"# we get the cost of the project using resource.csv file\nresource_data.head(2)","2ad7b773":"# https:\/\/stackoverflow.com\/questions\/22407798\/how-to-reset-a-dataframes-indexes-for-all-groups-in-one-step\nprice_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nprice_data.head(2)","4e5767ee":"# join two dataframes in python: \nproject_data = pd.merge(project_data, price_data, on='id', how='left')","61109704":"approved_price = project_data[project_data['project_is_approved']==1]['price'].values\n\nrejected_price = project_data[project_data['project_is_approved']==0]['price'].values","7931d936":"# https:\/\/glowingpython.blogspot.com\/2012\/09\/boxplot-with-matplotlib.html\nplt.boxplot([approved_price, rejected_price])\nplt.title('Box Plots of Cost per approved and not approved Projects')\nplt.xticks([1,2],('Approved Projects','Rejected Projects'))\nplt.ylabel('Price')\nplt.grid()\nplt.show()","f7630c69":"plt.figure(figsize=(10,3))\nsns.distplot(approved_price, hist=False, label=\"Approved Projects\")\nsns.distplot(rejected_price, hist=False, label=\"Not Approved Projects\")\nplt.title('Cost per approved and not approved Projects')\nplt.xlabel('Cost of a project')\nplt.legend()\nplt.show()","ac3a846b":"# http:\/\/zetcode.com\/python\/prettytable\/\nfrom prettytable import PrettyTable\n\n#If you get a ModuleNotFoundError error , install prettytable using: pip3 install prettytable\n\nx = PrettyTable()\nx.field_names = [\"Percentile\", \"Approved Projects\", \"Not Approved Projects\"]\n\nfor i in range(0,101,5):\n    x.add_row([i,np.round(np.percentile(approved_price,i), 3), np.round(np.percentile(rejected_price,i), 3)])\nprint(x)","5f734cbe":"plt.figure(figsize=(10,3))\nsns.distplot(project_data[\"teacher_number_of_previously_posted_projects\"], hist=False, label=\"Previous Projects\")\nplt.title('PDF For Teacher with previous projects')\nplt.xlabel('Number of a previous project')\nplt.legend()\nplt.show()","347e9ba2":"univariate_barplots(project_data, 'teacher_number_of_previously_posted_projects', 'project_is_approved', False)","34177675":"#How to calculate number of words in a string in DataFrame: https:\/\/stackoverflow.com\/a\/37483537\/4084039\n\nword_count = project_data[\"teacher_number_of_previously_posted_projects\"].value_counts()\nword_count=word_count[:100]\nind = np.arange(len(word_count))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(word_count))\n\nplt.ylabel('Count Of Teachers')\nplt.xlabel('Number of previous projects')\nplt.title('Words for each title of the project')\n\nplt.show()","b4095e6d":"##nikhil\ntotal=project_data.shape[0]\ncounts=project_data[\"teacher_number_of_previously_posted_projects\"].value_counts()\nzeros=counts[0]\nzeroProjectApprovedCount=0\nfor i in range(0,total):\n    if project_data[\"teacher_number_of_previously_posted_projects\"][i]==0 and  project_data[\"project_is_approved\"][i]==1:\n        zeroProjectApprovedCount+=1\nprint(\"Teacher with 0 previous projects:\",zeros,\"out of:\",total,\" ie. \",round(zeros\/total*100,2),\"%\")\nprint(\"Accepted: \", zeroProjectApprovedCount,\" ie. \",round(zeroProjectApprovedCount\/zeros*100,2),\"%\")\nprint(\"Rejected: \", zeros-zeroProjectApprovedCount,\" ie. \",round((zeros-zeroProjectApprovedCount)\/zeros*100,2),\"%\")\nprint(\"-\"*90)\n","dda512d8":"##nikhil\ncounts=project_data[\"teacher_number_of_previously_posted_projects\"].value_counts()\nfor j in range(0,11):\n\n    zeros=counts[j]\n    zeroProjectApprovedCount=0\n    for i in range(0,total):\n        if project_data[\"teacher_number_of_previously_posted_projects\"][i]==j and  project_data[\"project_is_approved\"][i]==1:\n            zeroProjectApprovedCount+=1\n    print(\"Teacher with \",j,\" previous projects:\",zeros,\"out of:\",total,\" ie. \",round(zeros\/total*100,2),\"%\")\n    print(\"Accepted: \", zeroProjectApprovedCount,\" ie. \",round(zeroProjectApprovedCount\/zeros*100,2),\"%\")\n    print(\"Rejected: \", zeros-zeroProjectApprovedCount,\" ie. \",round((zeros-zeroProjectApprovedCount)\/zeros*100,2),\"%\")\n    print(\"-\"*90)\n","f150f73f":"## BOX PLOT\nplt.boxplot(project_data[\"teacher_number_of_previously_posted_projects\"])\nplt.title('Box Plots of Number of previous projects of teachers')\n\nplt.ylabel('Number of Projects')\nplt.grid()\nplt.show()","33b508b1":"## HISTOGRAM WITH PDF\nsns.FacetGrid(project_data,hue=\"project_is_approved\",height=7)\\\n    .map(sns.distplot,\"teacher_number_of_previously_posted_projects\")\\\n    .add_legend();\n\nplt.show();","a9b1ab4a":"## PDF - CDF\n\nplt.figure(figsize=(20,6))\nplt.subplot(131) ##(1=no. of rows, 3= no. of columns, 1=1st figure,2,3,4 boxes)\ncounts,bin_edges=np.histogram(project_data[\"teacher_number_of_previously_posted_projects\"],bins=15,density=True)\npdf=counts\/sum(counts)\ncdf=np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf,linewidth=3.0)\nplt.plot(bin_edges[1:],cdf,linewidth=3.0)\nplt.ylabel(\"COUNT\")\nplt.xlabel('Previous Projects')\nplt.title('PDF-CDF of Teachers with their total previously posted projects')\nplt.legend(['PDF-Prev. Projects', 'CDF-Prev. Projects'], loc = 5,prop={'size': 16})","6ce8b837":"#How to calculate number of words in a string in DataFrame: https:\/\/stackoverflow.com\/a\/37483537\/4084039\nproj_res_summ_word_count = project_data['project_resource_summary'].str.split().apply(len).value_counts()\nproj_res_summ_word_dict = dict(proj_res_summ_word_count)\nproj_res_summ_word_dict = dict(sorted(proj_res_summ_word_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(proj_res_summ_word_dict))\nplt.figure(figsize=(20,7))\np1 = plt.bar(ind, list(proj_res_summ_word_dict.values()))\n\nplt.ylabel('Number of projects')\nplt.xlabel('Number words in project resource summary')\nplt.title('Words for each project resource summary')\nplt.xticks(ind, list(proj_res_summ_word_dict.keys()))\nplt.show()","6028eb34":"approved_proj_resource_summary_word_count = project_data[project_data['project_is_approved']==1]['project_resource_summary'].str.split().apply(len)\napproved_proj_resource_summary_word_count = approved_proj_resource_summary_word_count.values\n\nrejected_proj_resource_summary_word_count = project_data[project_data['project_is_approved']==0]['project_resource_summary'].str.split().apply(len)\nrejected_proj_resource_summary_word_count = rejected_proj_resource_summary_word_count.values","68a42930":"plt.figure(figsize=(20,7))\n\nsns.distplot(approved_proj_resource_summary_word_count, hist=False, label=\"Approved Projects\")\nsns.distplot(rejected_proj_resource_summary_word_count, hist=False, label=\"Not Approved Projects\")\nplt.title('Words for each project resource summary')\nplt.xlabel('Number of words in each project resource summary')\nplt.legend()\nplt.show()\n\n# # ALTERNATE CODE FOR KDE\n# plt.figure(figsize=(10,3))\n# sns.kdeplot(approved_proj_resource_summary_word_count,label=\"Approved Projects\", bw=0.6)\n# sns.kdeplot(rejected_proj_resource_summary_word_count,label=\"Not Approved Projects\", bw=0.6)\n# plt.legend()\n# plt.show()","3041f602":"\n\n## BOX PLOT\nplt.boxplot([approved_proj_resource_summary_word_count, rejected_proj_resource_summary_word_count])\nplt.title('Words for each essay of the project')\nplt.xticks([1,2],('Approved Projects','Rejected Projects'))\nplt.ylabel('Words in project essays')\nplt.grid()\nplt.show()","ee7fe43d":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n","ec396301":"## FUNCTION TO CHECK NUMERIC VALUE IN A SENTENCE\n#https:\/\/stackoverflow.com\/questions\/19859282\/check-if-a-string-contains-a-number\/31861306\n\nimport re\ndef hasNumbers(inputString):\n    return bool(re.search(r'\\d', inputString))","6b43dd66":"# Combining all the above statements --> functions reused as provided in the iPython notebook from AppliedAI\n\nfrom tqdm import tqdm\npreprocessed_summary = []\npreprocessed_summary_with_num=[]\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['project_resource_summary'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_summary.append(sent.lower().strip())\n    if hasNumbers(sent):\n        preprocessed_summary_with_num.append(1)\n    else:\n        preprocessed_summary_with_num.append(0)\n    ","d9ba652c":"##counting number of summary with numbers\nsummcountwithnum=0\nfor i in range(0,len(preprocessed_summary_with_num)):\n    if preprocessed_summary_with_num[i]==1:\n        summcountwithnum+=1\nprint(\"Total number of resource summaries with NUMERIC value in it: \",summcountwithnum)","c05abe7b":"## Counting Total Approved application with and without NUMERIC value in resource summary data\nsumnumapp=0\nfor i in range(0,total):\n    if preprocessed_summary_with_num[i]==1 and project_data['project_is_approved'][i]==1:\n        sumnumapp+=1\nprint(\"Total Approved application with NUMERIC value in resource summary:\",sumnumapp)\nprojWithoutNumberSummary=total-summcountwithnum\nprint(\"Total number of resource summaries WITHOUT NUMERIC value in it: \",projWithoutNumberSummary)","db70b1fb":"## Printing a detailed report\ntotal= total=project_data.shape[0]#109248\ntotalApp=y_value_counts[1] ## 92706\ntotalRejected=y_value_counts[0] ## 16542\n\nprint(\"\\n\\n----------DETAILED OBSERVATION----------\\n\")\n\nprint(\"PROJECT SUMMARY WITH VS WITHOUR NUMBERIC DIGITS %\")\nprint(\"Total || With Number || Without Number || Percentage\")\nprint(total,\"    \",summcountwithnum,\"        \",total-summcountwithnum,\"         \",round(summcountwithnum\/total*100,2),\"%\")\nprint(\"-\"*50)\nprint()\nprint(\"PROJECT DETAILS\")\nprint(\"Total || Approved || Rejected || Approval Percentage\")\nprint(total,\"    \",totalApp,\"    \",totalRejected,\"      \",round(totalApp\/total*100,2),\"%\")\nprint(\"-\"*50)\nprint()\n\nprint(\"PROJECT SUMMARY WITH NUMERIC VALUE DETAILS\")\nprint(\"Total || Approved || Rejected || Approval Percentage\")\nprint(summcountwithnum,\"    \",sumnumapp,\"    \",summcountwithnum-sumnumapp,\"      \",round(sumnumapp\/summcountwithnum*100,2),\"%\")\nprint(\"-\"*50)\nprint()\n\nprint(\"PROJECT SUMMARY WITHOUT NUMERIC VALUE DETAILS\")\nprint(\"Total || Approved || Rejected || Approval Percentage\")\nprint(total-summcountwithnum,\"    \",totalApp-sumnumapp,\"    \",totalRejected-(summcountwithnum-sumnumapp),\"      \",round((totalApp-sumnumapp)\/(total-summcountwithnum)*100,2),\"%\")","10bd9eb5":"project_data.head(2)\n","4b649719":"# printing some random essays.\nprint(project_data['essay'].values[0])\nprint(\"=\"*50)\nprint(project_data['essay'].values[150])\nprint(\"=\"*50)\nprint(project_data['essay'].values[1000])\nprint(\"=\"*50)\nprint(project_data['essay'].values[20000])\nprint(\"=\"*50)\nprint(project_data['essay'].values[99999])\nprint(\"=\"*50)\n","4f64d67c":"sent = decontracted(project_data['essay'].values[2000])\nprint(sent)\nprint(\"=\"*50)","215ef8b9":"# \\r \\n \\t remove from string python: http:\/\/texthandler.com\/info\/remove-line-breaks-python\/\nsent = sent.replace('\\\\r', ' ')\nsent = sent.replace('\\\\\"', ' ')\nsent = sent.replace('\\\\n', ' ')\nprint(sent)","df4df884":"#remove spacial character: https:\/\/stackoverflow.com\/a\/5843547\/4084039\nsent = re.sub('[^A-Za-z0-9]+', ' ', sent)\nprint(sent)","ac4161c6":"# Combining all the above statemennts \nfrom tqdm import tqdm\npreprocessed_essays = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['essay'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_essays.append(sent.lower().strip())","45e7208b":"# after preprocesing\npreprocessed_essays[2000]","66c128ca":"# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\nfrom tqdm import tqdm\npreprocessed_titles = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['project_title'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_titles.append(sent.lower().strip())","306d36cd":"print(\"BEFORE -->  \",project_data['project_title'][7],\"     NOW --> \",preprocessed_titles [7])","92fff573":"project_data.columns","8497c5dc":"# we use count vectorizer to convert the values into one hot encoded features\n# You can use it as follows:\n# Create an instance of the CountVectorizer class.\n# Call the fit() function in order to learn a vocabulary from one or more documents.\n# Call the transform() function on one or more documents as needed to encode each as a vector.\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=list(sorted_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer.fit(project_data['clean_categories'].values)\nprint(vectorizer.get_feature_names())\n\n\ncategories_one_hot = vectorizer.transform(project_data['clean_categories'].values)\nprint(\"Shape of matrix after one hot encodig \",categories_one_hot.shape)\ncategories_one_hot.toarray()\n","ca6f2802":"# we use count vectorizer to convert the values into one hot encoded features\nvectorizer = CountVectorizer(vocabulary=list(sorted_sub_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer.fit(project_data['clean_subcategories'].values)\nprint(vectorizer.get_feature_names())\n\n\nsub_categories_one_hot = vectorizer.transform(project_data['clean_subcategories'].values)\nprint(\"Shape of matrix after one hot encodig \",sub_categories_one_hot.shape)\nprint(sub_categories_one_hot.toarray())","625b71f7":"from scipy import sparse ## Exporting Sparse Matrix to NPZ File -> https:\/\/stackoverflow.com\/questions\/8955448\/save-load-scipy-sparse-csr-matrix-in-portable-data-format\nstatelist=list(project_data['school_state'].values)\nvectorizer = CountVectorizer(vocabulary=set(statelist), lowercase=False, binary=True)\nvectorizer.fit(statelist)\nprint(vectorizer.get_feature_names())\n\n\nschool_one_hot = vectorizer.transform(statelist)\nprint(\"Shape of matrix after one hot encodig \",school_one_hot.shape)\nprint(type(school_one_hot))\nsparse.save_npz(\"school_one_hot_export.npz\", school_one_hot) \nprint(school_one_hot.toarray())","fada1737":"prefixlist=project_data['teacher_prefix'].values\nprefixlist=list(prefixlist)\ncleanedPrefixList = [x for x in prefixlist if x == x] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\n## preprocessing the prefix to remove the SPACES,- else the vectors will be just 0's. Try adding - and see\nprefix_nospace_list = []\nfor i in cleanedPrefixList:\n    temp = \"\"\n    i = i.replace('.','') # we are placeing all the '.'(dot) with ''(empty) ex:\"Mr.\"=>\"Mr\"\n    temp +=i.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n    prefix_nospace_list.append(temp.strip())\n\ncleanedPrefixList=prefix_nospace_list\n\nvectorizer = CountVectorizer(vocabulary=set(cleanedPrefixList), lowercase=False, binary=True)\nvectorizer.fit(cleanedPrefixList)\nprint(vectorizer.get_feature_names())\nprefix_one_hot = vectorizer.transform(cleanedPrefixList)\nprint(\"Shape of matrix after one hot encodig \",prefix_one_hot.shape)\nprefix_one_hot_ar=prefix_one_hot.todense()\n\n##code to export to csv -> https:\/\/stackoverflow.com\/a\/54637996\/4433839\n# prefixcsv=pd.DataFrame(prefix_one_hot.toarray())\n# prefixcsv.to_csv('prefix.csv', index=None,header=None)","5c556b83":"print(type(prefix_one_hot_ar))","9d49f0a1":"gradelist=project_data['project_grade_category'].values\ngradelist=list(gradelist)\n\n## preprocessing the grades to remove the SPACES,- else the vectors will be just 0's. Try adding - and see\ngrade_nospace_list = []\nfor i in gradelist:\n    temp = \"\"\n    i = i.replace(' ','_') # we are placeing all the ' '(space) with ''(empty) ex:\"Grades 3-5\"=>\"Grades_3-5\"\n    i = i.replace('-','_')\n    temp +=i.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n    grade_nospace_list.append(temp.strip())\n\nvectorizer = CountVectorizer(vocabulary=set(grade_nospace_list), lowercase=False, binary=True)\nvectorizer.fit(grade_nospace_list)\nprint(vectorizer.get_feature_names())\ngrade_one_hot = vectorizer.transform(grade_nospace_list)\nprint(\"Shape of matrix after one hot encodig \",grade_one_hot.shape)\nprint(type(grade_one_hot))\ngrade_one_hot.toarray()\n\n##code to export to csv -> https:\/\/stackoverflow.com\/a\/54637996\/4433839\n# gradecsv=pd.DataFrame(grade_one_hot.toarray())\n# gradecsv.to_csv('grades.csv', index=None,header=None)","f9986c9d":"tt=grade_one_hot.todense()\nprint(type(tt))","9579f06a":"grade_one_hot","5ea80cd4":"# We are considering only the words which appeared in at least 10 documents(rows or projects).\nvectorizer = CountVectorizer(min_df=10)\ntext_bow = vectorizer.fit_transform(preprocessed_essays)\n# text_bow = vectorizer.fit(preprocessed_essays)\nprint(\"Shape of matrix after one hot encodig \",text_bow.shape)\n\n# TO CHECK A VECTOR\n# https:\/\/towardsdatascience.com\/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e\n# v0 = vectorizer.transform([preprocessed_essays[0]]).toarray()[0] \n# print(v0)\n\n# # write one of the vector to CSV to confirm wether we are obtaining meaning full results or just 0's.\n# text_bow=pd.DataFrame(v0)\n# text_bow.to_csv('text_bow.csv', index=None,header=None)\n\n# count number of non zero values in the vector\n# count=0\n# for i in range(0,len(v0)):\n#     if v0[i]>0:\n#         count=count+1\n# print(count)\n# v0.shape","32b1c2f9":"# you can vectorize the title also \n# before you vectorize the title make sure you preprocess it\nvectorizer = CountVectorizer(min_df=10)\ntext_title_bow = vectorizer.fit_transform(preprocessed_titles)\nprint(\"Shape of matrix after one hot encodig \",text_title_bow.shape)\n\n\n# TO CHECK A VECTOR\n# https:\/\/towardsdatascience.com\/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e\n\n# v0 = vectorizer.transform([preprocessed_titles[0]]).toarray()[0]\n\n# write one of the vector to CSV to confirm wether we are obtaining meaning full results or just 0's.\n\n# text_title_bow=pd.DataFrame(v0)\n# text_title_bow.to_csv('text_title_bow.csv', index=None,header=None)\n\n# count number of non zero values in the vector\n# count=0\n# for i in range(0,len(v0)):\n#     if v0[i]>0:\n#         count=count+1\n# print(count)\n# print(v0.shape)","43ff1185":"text_title_bow_ar=text_title_bow.todense()\nprint(type(text_title_bow_ar))\ntext_title_bow_ar","1a16ef9e":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\ntext_tfidf = vectorizer.fit_transform(preprocessed_essays)\nprint(\"Shape of matrix after one hot encodig \",text_tfidf.shape)","1263aca7":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\ntext_titles_tfidf = vectorizer.fit_transform(preprocessed_titles)\nprint(\"Shape of matrix after one hot encodig \",text_titles_tfidf.shape)\n\n# Code for testing and checking the generated vectors\n# v1 = vectorizer.transform([preprocessed_titles[0]]).toarray()[0]\n# text_title_tfidf=pd.DataFrame(v1)\n# text_title_tfidf.to_csv('text_title_tfidf.csv', index=None,header=None)","cbffa406":"'''\n# Reading glove vectors in python: https:\/\/stackoverflow.com\/a\/38230349\/4084039\ndef loadGloveModel(gloveFile):\n    print (\"Loading Glove Model\")\n    f = open(gloveFile,'r', encoding=\"utf8\")\n    model = {}\n    for line in tqdm(f):\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = np.array([float(val) for val in splitLine[1:]])\n        model[word] = embedding\n    print (\"Done.\",len(model),\" words loaded!\")\n    return model\nmodel = loadGloveModel('glove.42B.300d.txt')\n\n# ============================\nOutput:\n    \nLoading Glove Model\n1917495it [06:32, 4879.69it\/s]\nDone. 1917495  words loaded!\n\n# ============================\n\nwords = []\nfor i in preproced_texts:\n    words.extend(i.split(' '))\n\nfor i in preproced_titles:\n    words.extend(i.split(' '))\nprint(\"all the words in the coupus\", len(words))\nwords = set(words)\nprint(\"the unique words in the coupus\", len(words))\n\ninter_words = set(model.keys()).intersection(words)\nprint(\"The number of words that are present in both glove vectors and our coupus\", \\\n      len(inter_words),\"(\",np.round(len(inter_words)\/len(words)*100,3),\"%)\")\n\nwords_courpus = {}\nwords_glove = set(model.keys())\nfor i in words:\n    if i in words_glove:\n        words_courpus[i] = model[i]\nprint(\"word 2 vec length\", len(words_courpus))\n\n\n# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n\nimport pickle\nwith open('glove_vectors', 'wb') as f:\n    pickle.dump(words_courpus, f)\n\n\n'''","6d9ad57e":"# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n# make sure you have the glove_vectors file\nwith open('..\/input\/glove_vectors\/glove_vectors', 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())\n# print(kajsdf)","c8e05e9f":"# average Word2Vec\n# compute average word2vec for each review.\navg_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm(preprocessed_essays): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    avg_w2v_vectors.append(vector)\n\nprint(\"Length of avg_w2v_vectors: \",len(avg_w2v_vectors))\nprint(\"Length of avg_w2v_vectors[0]: \",len(avg_w2v_vectors[0]))","94e77f35":"# Similarly you can vectorize for title also\n# average Word2Vec\n# compute average word2vec for each review.\navg_w2v_title_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm(preprocessed_titles): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    avg_w2v_title_vectors.append(vector)\n\nprint(\"length of avg_w2v_title_vectors: \",len(avg_w2v_title_vectors))\nprint(\"length avg_w2v_title_vectors[0]:  \",len(avg_w2v_title_vectors[0]))\n# print(avg_w2v_title_vectors[0]) ## Checking the generated vector","802ef31f":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\ntfidf_model = TfidfVectorizer()\ntfidf_model.fit(preprocessed_essays)\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())","642fce1c":"# average Word2Vec\n# compute average word2vec for each review.\ntfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm(preprocessed_essays): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    tfidf_w2v_vectors.append(vector)\n\nprint(\"length of tfidf_w2v_vectors: \",len(tfidf_w2v_vectors))\nprint(\"length of tfidf_w2v_vectors[0]: \",len(tfidf_w2v_vectors[0]))","6fda8de0":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\ntfidf_title_model = TfidfVectorizer()\ntfidf_title_model.fit(preprocessed_titles)\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_title_model.get_feature_names(), list(tfidf_title_model.idf_)))\ntfidf_title_words = set(tfidf_title_model.get_feature_names())","c8a85e99":"# average Word2Vec\n# compute average word2vec for each title.\ntfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\nfor sentence in preprocessed_titles: # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_title_weight =0; # num of words with a valid vector in the title\n    for word in sentence.split(): # for each word in a title\n        if (word in glove_words) and (word in tfidf_title_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_title_weight += tf_idf\n    if tf_idf_title_weight != 0:\n        vector \/= tf_idf_title_weight\n    tfidf_w2v_title_vectors.append(vector)\n\nprint(\"Length of tfidf_w2v_title_vectors: \",len(tfidf_w2v_title_vectors))\nprint(\"Length of  tfidf_w2v_title_vectors[0]: \",len(tfidf_w2v_title_vectors[0]))\n# print(tfidf_w2v_title_vectors[0])","db82aa7f":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nprice_scalar = StandardScaler()\nprice_scalar.fit(project_data['price'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nprice_standardized = price_scalar.transform(project_data['price'].values.reshape(-1, 1))","bf8026c5":"price_standardized","ac3549b5":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nteacher_previous_proj_scalar = StandardScaler()\nteacher_previous_proj_scalar.fit(project_data['teacher_number_of_previously_posted_projects'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {teacher_previous_proj_scalar.mean_[0]}, Standard deviation : {np.sqrt(teacher_previous_proj_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nteacher_previous_proj_standardized = teacher_previous_proj_scalar.transform(project_data['teacher_number_of_previously_posted_projects'].values.reshape(-1, 1))","3f16f95e":"\nteacher_previous_proj_standardized.shape","be994e8f":"avg_w2v_vectors=np.asarray(avg_w2v_vectors)\navg_w2v_title_vectors=np.asarray(avg_w2v_title_vectors)\n# tfidf_w2v_vectors=np.asarray(tfidf_w2v_vectors)\ntfidf_w2v_title_vectors=np.asarray(tfidf_w2v_title_vectors)\n\nprint(\"SCHOOL STATE: -> \",school_one_hot.shape)\nprint(\"categories_one_hot -> \",categories_one_hot.shape)\nprint(\"sub_categories_one_hot -> \",sub_categories_one_hot.shape)\nprint(\"TEACHER PREFIX -> \", prefix_one_hot.shape)\nprint(\"PROJECT GRADE -> \",grade_one_hot.shape)\n\nprint(\"price_standardized -> \",price_standardized.shape)\nprint(\"TEACHER PREVIOUS POSTED PROJECT -> \",teacher_previous_proj_standardized.shape)\n\nprint(\"text_title_bow -> \",text_title_bow.shape)\nprint(\"text_titles_tfidf -> \",text_titles_tfidf.shape)\nprint(\"avg_w2v_title_vectors -> \",avg_w2v_title_vectors.shape)\nprint(\"tfidf_w2v_title_vectors -> \",tfidf_w2v_title_vectors.shape)","f597ff99":"## save csr to npz python -> https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.sparse.save_npz.html","1ac35ea9":"from scipy.sparse import hstack\nprint(\"H-STACKING the required features\")\nCAT_NUM_BOW_vec_stack=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, price_standardized, teacher_previous_proj_standardized, text_title_bow))\nCAT_NUM_TFIDF_vec_stack=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, price_standardized, teacher_previous_proj_standardized, text_titles_tfidf))\nCAT_NUM_Avg_W2V_vec_stack=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, price_standardized, teacher_previous_proj_standardized, avg_w2v_title_vectors))\nCAT_NUM_TFIDF_W2V_vec_stack=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, price_standardized, teacher_previous_proj_standardized, tfidf_w2v_title_vectors))\nALL_vec_stack=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, text_title_bow, text_titles_tfidf, avg_w2v_title_vectors, tfidf_w2v_title_vectors, price_standardized, teacher_previous_proj_standardized))\nprint(\"H-STACKING Completed----------------------------------\\n\")\n\nprint(\"TYPE: CAT_NUM_BOW_vec_stack: \",type(CAT_NUM_BOW_vec_stack))\nprint(\"TYPE: CAT_NUM_TFIDF_vec_stack: \",type(CAT_NUM_TFIDF_vec_stack))\nprint(\"TYPE: CAT_NUM_Avg_W2V_vec_stack: \",type(CAT_NUM_Avg_W2V_vec_stack))\nprint(\"TYPE: CAT_NUM_TFIDF_W2V_vec_stack: \",type(CAT_NUM_TFIDF_W2V_vec_stack))\nprint(\"TYPE: ALL_vec_stack: \",type(ALL_vec_stack))\n\nprint(\"SHAPE: CAT_NUM_BOW_vec_stack: \",CAT_NUM_BOW_vec_stack.shape)\nprint(\"SHAPE: CAT_NUM_TFIDF_vec_stack: \",CAT_NUM_TFIDF_vec_stack.shape)\nprint(\"SHAPE: CAT_NUM_Avg_W2V_vec_stack: \",CAT_NUM_Avg_W2V_vec_stack.shape)\nprint(\"SHAPE: CAT_NUM_TFIDF_W2V_vec_stack: \",CAT_NUM_TFIDF_W2V_vec_stack.shape)\nprint(\"SHAPE: ALL_vec_stack: \",ALL_vec_stack.shape)\n\nprint(\"============================================================\\n\\n\")\nprint(\"Converting to CSR\")\nCAT_NUM_BOW_vec_stack_tocsr=CAT_NUM_BOW_vec_stack.tocsr()\nCAT_NUM_TFIDF_vec_stack_tocsr=CAT_NUM_TFIDF_vec_stack.tocsr()\nCAT_NUM_Avg_W2V_vec_stack_tocsr=CAT_NUM_Avg_W2V_vec_stack.tocsr()\nCAT_NUM_TFIDF_W2V_vec_stack_tocsr=CAT_NUM_TFIDF_W2V_vec_stack.tocsr()\nALL_vec_stack_tocsr=ALL_vec_stack.tocsr()\nprint(\"Converted to CSR --------------------------------------\\n\")\n\n\nprint(\"TYPE: CAT_NUM_BOW_vec_stack_tocsr: \",type(CAT_NUM_BOW_vec_stack_tocsr))\nprint(\"TYPE: CAT_NUM_TFIDF_vec_stack_tocsr: \",type(CAT_NUM_TFIDF_vec_stack_tocsr))\nprint(\"TYPE: CAT_NUM_Avg_W2V_vec_stack_tocsr: \",type(CAT_NUM_Avg_W2V_vec_stack_tocsr))\nprint(\"TYPE: CAT_NUM_TFIDF_W2V_vec_stack_tocsr: \",type(CAT_NUM_TFIDF_W2V_vec_stack_tocsr))\nprint(\"TYPE: ALL_vec_stack_tocsr: \",type(CAT_NUM_TFIDF_W2V_vec_stack_tocsr))\n\n\nprint(\"SHAPE: CAT_NUM_BOW_vec_stack_tocsr: \",CAT_NUM_BOW_vec_stack_tocsr.shape)\nprint(\"SHAPE: CAT_NUM_TFIDF_vec_stack_tocsr: \",CAT_NUM_TFIDF_vec_stack_tocsr.shape)\nprint(\"SHAPE: CAT_NUM_Avg_W2V_vec_stack_tocsr: \",CAT_NUM_Avg_W2V_vec_stack_tocsr.shape)\nprint(\"SHAPE: CAT_NUM_TFIDF_W2V_vec_stack_tocsr: \",CAT_NUM_TFIDF_W2V_vec_stack_tocsr.shape)\nprint(\"SHAPE: ALL_vec_stack_tocsr: \",ALL_vec_stack_tocsr.shape)\n\nprint(\"============================================================\\n\\n\")\nprint(\"Saving to NPZ\\n\")\nscipy.sparse.save_npz('CAT_NUM_BOW_vec_stack_csr_sparse_matrix.npz',CAT_NUM_BOW_vec_stack_tocsr)\nscipy.sparse.save_npz('CAT_NUM_TFIDF_vec_stack_csr_sparse_matrix.npz',CAT_NUM_TFIDF_vec_stack_tocsr)\nscipy.sparse.save_npz('CAT_NUM_Avg_W2V_vec_stack_csr_sparse_matrix.npz',CAT_NUM_Avg_W2V_vec_stack_tocsr)\nscipy.sparse.save_npz('CAT_NUM_TFIDF_W2V_vec_stack_csr_sparse_matrix.npz',CAT_NUM_TFIDF_W2V_vec_stack_tocsr)\nscipy.sparse.save_npz('ALL_vec_stack_csr_sparse_matrix.npz',ALL_vec_stack_tocsr)\nprint(\"Saved to NPZ--------------------------------------\\n\\n\")\nprint(\"============================================================\\n\\n\")\n","fc262420":"print(\"LOADING THE NPZ\")\nCAT_NUM_BOW_vec_tocsr=scipy.sparse.load_npz('CAT_NUM_BOW_vec_stack_csr_sparse_matrix.npz')\nCAT_NUM_TFIDF_vec_tocsr=scipy.sparse.load_npz('CAT_NUM_TFIDF_vec_stack_csr_sparse_matrix.npz')\nCAT_NUM_Avg_W2V_vec_tocsr=scipy.sparse.load_npz('CAT_NUM_Avg_W2V_vec_stack_csr_sparse_matrix.npz')\nCAT_NUM_TFIDF_W2V_vec_tocsr=scipy.sparse.load_npz('CAT_NUM_TFIDF_W2V_vec_stack_csr_sparse_matrix.npz')\nALL_vec_tocsr=scipy.sparse.load_npz('ALL_vec_stack_csr_sparse_matrix.npz')\n\nprint(\"LOADING Completed ----------------------------------\\n\")\n\nprint(\"TYPE: CAT_NUM_BOW_vec_tocsr: \",type(CAT_NUM_BOW_vec_tocsr))\nprint(\"TYPE: CAT_NUM_TFIDF_vec_tocsr: \",type(CAT_NUM_TFIDF_vec_tocsr))\nprint(\"TYPE: CAT_NUM_Avg_W2V_vec_tocsr: \",type(CAT_NUM_Avg_W2V_vec_tocsr))\nprint(\"TYPE: CAT_NUM_TFIDF_W2V_vec_tocsr: \",type(CAT_NUM_TFIDF_W2V_vec_tocsr))\nprint(\"TYPE: ALL_vec_tocsr: \",type(ALL_vec_tocsr))\n\n\nprint(\"SHAPE: CAT_NUM_BOW_vec_tocsr: \",CAT_NUM_BOW_vec_tocsr.shape)\nprint(\"SHAPE: CAT_NUM_TFIDF_vec_tocsr: \",CAT_NUM_TFIDF_vec_tocsr.shape)\nprint(\"SHAPE: CAT_NUM_Avg_W2V_vec_tocsr: \",CAT_NUM_Avg_W2V_vec_tocsr.shape)\nprint(\"SHAPE: CAT_NUM_TFIDF_W2V_vec_tocsr: \",CAT_NUM_TFIDF_W2V_vec_tocsr.shape)\nprint(\"SHAPE: ALL_vec_tocsr: \",ALL_vec_tocsr.shape)\nprint(\"=========================END===================================\\n\\n\")","a5b22ce6":"project_data = pd.read_csv('..\/input\/train_data.csv')","a5afb30a":"sampleSize=6000","8c4b19c0":"## CAT_NUM_BOW Dataset\n# from sklearn.manifold import TSNE\n\nprint(\"SAMPLE SIZE: \",sampleSize,\"\\n\\n\")\nstart_time = time.time()\n# # CAT_NUM_BOW_vec=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, price_standardized, teacher_previous_proj_standardized, text_title_bow))\n# print(\"TYPE: CAT_NUM_BOW_vec: \",type(CAT_NUM_BOW_vec))\n# print(\"SHAPE: CAT_NUM_BOW_vec: \",CAT_NUM_BOW_vec.shape)\n# #<class 'scipy.sparse.coo.coo_matrix'>\n# print(\"\\n\\n\")\n\n# CAT_NUM_BOW_vec_tocsr=CAT_NUM_BOW_vec.tocsr()\nprint(\"CSR VECTORS ARE LOADED FROM DISK FOR SAVING MEMORY BY NOT REQUIRING ABOVE VECTOR GENERATION CODE TO BE RUN AGAIN AND ALSO SAVES RAM BY NOT LOADING THE GLOVE VECTORS IN MEMORY\")\nCAT_NUM_BOW_vec_n_samples = CAT_NUM_BOW_vec_tocsr[0:sampleSize,:]\n\nprint(\"TYPE: CAT_NUM_BOW_vec_n_samples: \",type(CAT_NUM_BOW_vec_n_samples))\n# <class 'scipy.sparse.csr.csr_matrix'>\n\nCAT_NUM_BOW_vec_n_samples_toarray=CAT_NUM_BOW_vec_n_samples.toarray()\nprint(\"TYPE: CAT_NUM_BOW_vec_n_samples_toarray: \",type(CAT_NUM_BOW_vec_n_samples_toarray))\nprint(\"SHAPE: CAT_NUM_BOW_vec_n_samples_toarray: \",CAT_NUM_BOW_vec_n_samples_toarray.shape)\n# <class 'numpy.ndarray'>\nprint(\"\\n\\n\")\n\ntsne_model = TSNE(n_components = 2, perplexity = 100.0, random_state = 0)\ntsne_data = tsne_model.fit_transform(CAT_NUM_BOW_vec_n_samples_toarray)\n\n\nprint(\"TYPE: tsne_data: \",type(tsne_data))\nprint(\"SHAPE: tsne_data: \",tsne_data.shape)\nprint(\"\\n\\n\")\n\nCAT_NUM_BOW_Labels=project_data[\"project_is_approved\"]\nCAT_NUM_BOW_Labels_n_samples = CAT_NUM_BOW_Labels[0: sampleSize]\nprint(\"LEN: CAT_NUM_BOW_Labels_n_samples: \",len(CAT_NUM_BOW_Labels_n_samples))\nprint(\"\\n\\n\")\n\ntsne_data = np.vstack((tsne_data.T, CAT_NUM_BOW_Labels_n_samples)).T\n# tsne_df_b = pd.DataFrame(tsne_data_b, columns = (\"1st_Dim\",\"2nd_Dim\",\"Labels\"))\ntsne_data_df = pd.DataFrame(tsne_data, columns = [\"Dimension_x\",\"Dimension_y\",\"Score\"])\n\n\nprint(\"TYPE: tsne_data-AfterStacking: \",type(tsne_data))\nprint(\"TYPE: tsne_data_df: \",type(tsne_data_df))\nprint(\"SHAPE: tsne_data_df: \",tsne_data_df.shape)\nprint(\"\\n\\n\")\n# colors = {0:'red', 1:'blue', 2:'green'}\n# plt.scatter(tsne_data_df['Dimension_x'], tsne_data_df['Dimension_y'], c=tsne_data_df['Score'].apply(lambda x: colors[x]))\n# plt.show()\n\nimport matplotlib.pyplot as plt\nsns.FacetGrid(tsne_data_df, hue = \"Score\", size = 8).map(plt.scatter, \"Dimension_x\", \"Dimension_y\").add_legend()\nplt.suptitle(\"TSNE WITH BOW ENCODING OF PROJECT TITLE FEATURE \")\nplt.show()\nprint(\"PROCESSING TOOK--- %s seconds ---\" % (time.time() - start_time))\n","a726a311":"# please write all the code with proper documentation, and proper titles for each subsection\n# when you plot any graph make sure you use \n    # a. Title, that describes your plot, this will be very helpful to the reader\n    # b. Legends if needed\n    # c. X-axis label\n    # d. Y-axis label\n\n# sampleSize=10000\nprint(\"SAMPLE SIZE: \",sampleSize,\"\\n\\n\")\nstart_time = time.time()\n# CAT_NUM_TFIDF_vec=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, price_standardized, teacher_previous_proj_standardized, text_titles_tfidf))\n# print(\"TYPE: CAT_NUM_TFIDF_vec: \",type(CAT_NUM_TFIDF_vec))\n# print(\"SHAPE: CAT_NUM_TFIDF_vec: \",CAT_NUM_TFIDF_vec.shape)\n# #<class 'scipy.sparse.coo.coo_matrix'>\n\n# CAT_NUM_TFIDF_vec_tocsr=CAT_NUM_TFIDF_vec.tocsr()\nprint(\"CSR VECTORS ARE LOADED FROM DISK FOR SAVING MEMORY BY NOT REQUIRING ABOVE VECTOR GENERATION CODE TO BE RUN AGAIN AND ALSO SAVES RAM BY NOT LOADING THE GLOVE VECTORS IN MEMORY\")\nCAT_NUM_TFIDF_vec_n_samples = CAT_NUM_TFIDF_vec_tocsr[0:sampleSize,:]\n\nprint(\"TYPE: CAT_NUM_TFIDF_vec_n_samples: \",type(CAT_NUM_TFIDF_vec_n_samples))\n# <class 'scipy.sparse.csr.csr_matrix'>\n\nCAT_NUM_TFIDF_vec_n_samples_toarray=CAT_NUM_TFIDF_vec_n_samples.toarray()\nprint(\"TYPE: CAT_NUM_TFIDF_vec_n_samples_toarray: \",type(CAT_NUM_TFIDF_vec_n_samples_toarray))\nprint(\"SHAPE: CAT_NUM_TFIDF_vec_n_samples_toarray: \",CAT_NUM_TFIDF_vec_n_samples_toarray.shape)\n# <class 'numpy.ndarray'>\n\n\ntsne_model = TSNE(n_components = 2, perplexity = 100.0, random_state = 0)\ntsne_data = tsne_model.fit_transform(CAT_NUM_TFIDF_vec_n_samples_toarray)\n\n\nprint(\"TYPE: tsne_data: \",type(tsne_data))\nprint(\"SHAPE: tsne_data: \",tsne_data.shape)\n\n\nCAT_NUM_TFIDF_Labels=project_data[\"project_is_approved\"]\nCAT_NUM_TFIDF_Labels_n_samples = CAT_NUM_TFIDF_Labels[0: sampleSize]\nprint(\"LEN: CAT_NUM_TFIDF_Labels_n_samples: \",len(CAT_NUM_TFIDF_Labels_n_samples))\n\n\ntsne_data = np.vstack((tsne_data.T, CAT_NUM_TFIDF_Labels_n_samples)).T\n# tsne_df_b = pd.DataFrame(tsne_data_b, columns = (\"1st_Dim\",\"2nd_Dim\",\"Labels\"))\ntsne_data_df = pd.DataFrame(tsne_data, columns = [\"Dimension_x\",\"Dimension_y\",\"Score\"])\n\n\nprint(\"TYPE: tsne_data-AfterStacking: \",type(tsne_data))\nprint(\"TYPE: tsne_data_df: \",type(tsne_data_df))\nprint(\"SHAPE: tsne_data_df: \",tsne_data_df.shape)\n\n# colors = {0:'red', 1:'blue', 2:'green'}\n# plt.scatter(tsne_data_df['Dimension_x'], tsne_data_df['Dimension_y'], c=tsne_data_df['Score'].apply(lambda x: colors[x]))\n# plt.show()\n\nimport matplotlib.pyplot as plt\nsns.FacetGrid(tsne_data_df, hue = \"Score\", size = 8).map(plt.scatter, \"Dimension_x\", \"Dimension_y\").add_legend()\nplt.suptitle(\"TSNE WITH TF-IDF ENCODING OF PROJECT TITLE FEATURE \")\nplt.show()\nprint(\"PROCESSING TOOK--- %s seconds ---\" % (time.time() - start_time))\n\n\n# sns.FacetGrid(tsne_df_b, hue = \"Score\", size = 10).map(plt.scatter, \"Dimension_x\", \"Dimension_y\").add_legend()\n# # nd().fig.suptitle(\"TSNE WITH BOW ENCODING OF PROJECT TITLE FEATURE \")\n# plt.show()\n    ","ac6ae25b":"# please write all the code with proper documentation, and proper titles for each subsection\n# when you plot any graph make sure you use \n    # a. Title, that describes your plot, this will be very helpful to the reader\n    # b. Legends if needed\n    # c. X-axis label\n    # d. Y-axis label\n\nsampleSize=15000\nprint(\"SAMPLE SIZE: \",sampleSize,\"\\n\\n\")\nstart_time = time.time()\n# CAT_NUM_Avg_W2V_vec=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, price_standardized, teacher_previous_proj_standardized, avg_w2v_title_vectors))\n# print(\"TYPE: CAT_NUM_Avg_W2V_vec: \",type(CAT_NUM_Avg_W2V_vec))\n# print(\"SHAPE: CAT_NUM_Avg_W2V_vec: \",CAT_NUM_Avg_W2V_vec.shape)\n# #<class 'scipy.sparse.coo.coo_matrix'>\n\n# CAT_NUM_Avg_W2V_vec_tocsr=CAT_NUM_Avg_W2V_vec.tocsr()\nprint(\"CSR VECTORS ARE LOADED FROM DISK FOR SAVING MEMORY BY NOT REQUIRING ABOVE VECTOR GENERATION CODE TO BE RUN AGAIN AND ALSO SAVES RAM BY NOT LOADING THE GLOVE VECTORS IN MEMORY\")\nCAT_NUM_Avg_W2V_vec_n_samples = CAT_NUM_Avg_W2V_vec_tocsr[0:sampleSize,:]\n\nprint(\"TYPE: CAT_NUM_Avg_W2V_vec_n_samples: \",type(CAT_NUM_Avg_W2V_vec_n_samples))\n# <class 'scipy.sparse.csr.csr_matrix'>\n\nCAT_NUM_Avg_W2V_vec_n_samples_toarray=CAT_NUM_Avg_W2V_vec_n_samples.toarray()\nprint(\"TYPE: CAT_NUM_Avg_W2V_vec_n_samples_toarray: \",type(CAT_NUM_Avg_W2V_vec_n_samples_toarray))\nprint(\"SHAPE: CAT_NUM_Avg_W2V_vec_n_samples_toarray: \",CAT_NUM_Avg_W2V_vec_n_samples_toarray.shape)\n# <class 'numpy.ndarray'>\n\n\ntsne_model = TSNE(n_components = 2, perplexity = 100.0, random_state = 0)\ntsne_data = tsne_model.fit_transform(CAT_NUM_Avg_W2V_vec_n_samples_toarray)\n\n\nprint(\"TYPE: tsne_data: \",type(tsne_data))\nprint(\"SHAPE: tsne_data: \",tsne_data.shape)\n\n\nCAT_NUM_Avg_W2V_Labels=project_data[\"project_is_approved\"]\nCAT_NUM_Avg_W2V_Labels_n_samples = CAT_NUM_Avg_W2V_Labels[0: sampleSize]\nprint(\"LEN: CAT_NUM_Avg_W2V_Labels_n_samples: \",len(CAT_NUM_Avg_W2V_Labels_n_samples))\n\n\ntsne_data = np.vstack((tsne_data.T, CAT_NUM_Avg_W2V_Labels_n_samples)).T\n# tsne_df_b = pd.DataFrame(tsne_data_b, columns = (\"1st_Dim\",\"2nd_Dim\",\"Labels\"))\ntsne_data_df = pd.DataFrame(tsne_data, columns = [\"Dimension_x\",\"Dimension_y\",\"Score\"])\n\n\nprint(\"TYPE: tsne_data-AfterStacking: \",type(tsne_data))\nprint(\"TYPE: tsne_data_df: \",type(tsne_data_df))\nprint(\"SHAPE: tsne_data_df: \",tsne_data_df.shape)\n\n# colors = {0:'red', 1:'blue', 2:'green'}\n# plt.scatter(tsne_data_df['Dimension_x'], tsne_data_df['Dimension_y'], c=tsne_data_df['Score'].apply(lambda x: colors[x]))\n# plt.show()\n\nimport matplotlib.pyplot as plt\nsns.FacetGrid(tsne_data_df, hue = \"Score\", size = 8).map(plt.scatter, \"Dimension_x\", \"Dimension_y\").add_legend()\nplt.suptitle(\"TSNE WITH AVERAGE WORD2VEC ENCODING OF PROJECT TITLE FEATURE \")\nplt.show()\nprint(\"PROCESSING TOOK--- %s seconds ---\" % (time.time() - start_time))\n    ","c4786f00":"# please write all the code with proper documentation, and proper titles for each subsection\n# when you plot any graph make sure you use \n    # a. Title, that describes your plot, this will be very helpful to the reader\n    # b. Legends if needed\n    # c. X-axis label\n    # d. Y-axis label\n    \nsampleSize=20000\nprint(\"SAMPLE SIZE: \",sampleSize,\"\\n\\n\")\nstart_time = time.time()\n# CAT_NUM_TFIDF_W2V_vec=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, price_standardized, teacher_previous_proj_standardized, tfidf_w2v_title_vectors))\n# print(\"TYPE: CAT_NUM_TFIDF_W2V_vec: \",type(CAT_NUM_TFIDF_W2V_vec))\n# print(\"SHAPE: CAT_NUM_TFIDF_W2V_vec: \",CAT_NUM_TFIDF_W2V_vec.shape)\n# #<class 'scipy.sparse.coo.coo_matrix'>\n\n# CAT_NUM_TFIDF_W2V_vec_tocsr=CAT_NUM_TFIDF_W2V_vec.tocsr()\nprint(\"CSR VECTORS ARE LOADED FROM DISK FOR SAVING MEMORY BY NOT REQUIRING ABOVE VECTOR GENERATION CODE TO BE RUN AGAIN AND ALSO SAVES RAM BY NOT LOADING THE GLOVE VECTORS IN MEMORY\")\nCAT_NUM_TFIDF_W2V_vec_n_samples = CAT_NUM_TFIDF_W2V_vec_tocsr[0:sampleSize,:]\n\nprint(\"TYPE: CAT_NUM_TFIDF_W2V_vec_n_samples: \",type(CAT_NUM_TFIDF_W2V_vec_n_samples))\n# <class 'scipy.sparse.csr.csr_matrix'>\n\nCAT_NUM_TFIDF_W2V_vec_n_samples_toarray=CAT_NUM_TFIDF_W2V_vec_n_samples.toarray()\nprint(\"TYPE: CAT_NUM_TFIDF_W2V_vec_n_samples_toarray: \",type(CAT_NUM_TFIDF_W2V_vec_n_samples_toarray))\nprint(\"SHAPE: CAT_NUM_TFIDF_W2V_vec_n_samples_toarray: \",CAT_NUM_TFIDF_W2V_vec_n_samples_toarray.shape)\n# <class 'numpy.ndarray'>\n\n\ntsne_model = TSNE(n_components = 2, perplexity = 100.0, random_state = 0)\ntsne_data = tsne_model.fit_transform(CAT_NUM_TFIDF_W2V_vec_n_samples_toarray)\n\n\nprint(\"TYPE: tsne_data: \",type(tsne_data))\nprint(\"SHAPE: tsne_data: \",tsne_data.shape)\n\n\nCAT_NUM_TFIDF_W2V_Labels=project_data[\"project_is_approved\"]\nCAT_NUM_TFIDF_W2V_Labels_n_samples = CAT_NUM_TFIDF_W2V_Labels[0: sampleSize]\nprint(\"LEN: CAT_NUM_TFIDF_W2V_Labels_n_samples: \",len(CAT_NUM_TFIDF_W2V_Labels_n_samples))\n\n\ntsne_data = np.vstack((tsne_data.T, CAT_NUM_TFIDF_W2V_Labels_n_samples)).T\n# tsne_df_b = pd.DataFrame(tsne_data_b, columns = (\"1st_Dim\",\"2nd_Dim\",\"Labels\"))\ntsne_data_df = pd.DataFrame(tsne_data, columns = [\"Dimension_x\",\"Dimension_y\",\"Score\"])\n\n\nprint(\"TYPE: tsne_data-AfterStacking: \",type(tsne_data))\nprint(\"TYPE: tsne_data_df: \",type(tsne_data_df))\nprint(\"SHAPE: tsne_data_df: \",tsne_data_df.shape)\n\n# colors = {0:'red', 1:'blue', 2:'green'}\n# plt.scatter(tsne_data_df['Dimension_x'], tsne_data_df['Dimension_y'], c=tsne_data_df['Score'].apply(lambda x: colors[x]))\n# plt.show()\n\nimport matplotlib.pyplot as plt\nsns.FacetGrid(tsne_data_df, hue = \"Score\", size = 8).map(plt.scatter, \"Dimension_x\", \"Dimension_y\").add_legend()\nplt.suptitle(\"TSNE WITH TF-IDF WEIGHTED WORD2VEC ENCODING OF PROJECT TITLE FEATURE \")\nplt.show()\nprint(\"PROCESSING TOOK--- %s seconds ---\" % (time.time() - start_time))\n\n    ","d1f7d9d5":"## ALL\nsampleSize=7000\nprint(\"SAMPLE SIZE: \",sampleSize,\"\\n\\n\")\nstart_time = time.time()\n# # ALL_vec=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, price_standardized, teacher_previous_proj_standardized, tfidf_w2v_title_vectors))\n# ALL_vec=hstack((school_one_hot, categories_one_hot, sub_categories_one_hot, prefix_one_hot, grade_one_hot, text_title_bow, text_titles_tfidf, avg_w2v_title_vectors, tfidf_w2v_title_vectors, price_standardized, teacher_previous_proj_standardized))\n# print(\"TYPE: ALL_vec: \",type(ALL_vec))\n# print(\"SHAPE: ALL_vec: \",ALL_vec.shape)\n# #<class 'scipy.sparse.coo.coo_matrix'>\n# ALL_vec_tocsr=ALL_vec.tocsr()\n\nprint(\"CSR VECTORS ARE LOADED FROM DISK FOR SAVING MEMORY BY NOT REQUIRING ABOVE VECTOR GENERATION CODE TO BE RUN AGAIN AND ALSO SAVES RAM BY NOT LOADING THE GLOVE VECTORS IN MEMORY\")\nALL_vec_n_samples = ALL_vec_tocsr[0:sampleSize,:]\n\nprint(\"TYPE: ALL_vec_n_samples: \",type(ALL_vec_n_samples))\n# <class 'scipy.sparse.csr.csr_matrix'>\n\nALL_vec_n_samples_toarray=ALL_vec_n_samples.toarray()\nprint(\"TYPE: ALL_vec_n_samples_toarray: \",type(ALL_vec_n_samples_toarray))\nprint(\"SHAPE: ALL_vec_n_samples_toarray: \",ALL_vec_n_samples_toarray.shape)\n# <class 'numpy.ndarray'>\n\n\ntsne_model = TSNE(n_components = 2, perplexity = 100.0, random_state = 0)\ntsne_data = tsne_model.fit_transform(ALL_vec_n_samples_toarray)\n\n\nprint(\"TYPE: tsne_data: \",type(tsne_data))\nprint(\"SHAPE: tsne_data: \",tsne_data.shape)\n\n\nALL_Labels=project_data[\"project_is_approved\"]\nALL_Labels_n_samples = ALL_Labels[0: sampleSize]\nprint(\"LEN: ALL_Labels_n_samples: \",len(ALL_Labels_n_samples))\n\n\ntsne_data = np.vstack((tsne_data.T, ALL_Labels_n_samples)).T\n# tsne_df_b = pd.DataFrame(tsne_data_b, columns = (\"1st_Dim\",\"2nd_Dim\",\"Labels\"))\ntsne_data_df = pd.DataFrame(tsne_data, columns = [\"Dimension_x\",\"Dimension_y\",\"Score\"])\n\n\nprint(\"TYPE: tsne_data-AfterStacking: \",type(tsne_data))\nprint(\"TYPE: tsne_data_df: \",type(tsne_data_df))\nprint(\"SHAPE: tsne_data_df: \",tsne_data_df.shape)\n\n# colors = {0:'red', 1:'blue', 2:'green'}\n# plt.scatter(tsne_data_df['Dimension_x'], tsne_data_df['Dimension_y'], c=tsne_data_df['Score'].apply(lambda x: colors[x]))\n# plt.show()\n\nimport matplotlib.pyplot as plt\nsns.FacetGrid(tsne_data_df, hue = \"Score\", size = 8).map(plt.scatter, \"Dimension_x\", \"Dimension_y\").add_legend()\nplt.suptitle(\"TSNE WITH BOW, TF-IDF, Avg W2C and TFIDF Weighted W2V ENCODING OF PROJECT TITLE FEATURE \")\nplt.show()\nprint(\"PROCESSING TOOK--- %s seconds ---\" % (time.time() - start_time))","6a5765ea":"**Observation:**\n 1. Every state has greater than 80% success rate in approval.\n 1. CA (California) state has the maximum number of the project proposals ie 15387.\n 1. There is 50% reduction in the 2nd highest state for project proposals. ie TX (Texas) with 6014.\n 1. VT (Vermont) has the minimum number of project submissions.\n 1. High variation is observed in the state statistics.","d714fd95":"## BAR GRAPH FOR WORDS IN SUMMARY","22152a3f":"#### 1.4.2.4 TFIDF Vectorizer on `project_title`","3ffcafa5":"**Observation:**\nRoughly 28% of the teachers have applied for the 1st time. Interestingly, 82% of those are accepted considering that they dont have any previous applications.","5f65dfa9":"# DonorsChoose: Project Title Text Preprocessing","88fc5cf1":"### SAMPLE SIZE = 15,000","7c4ee7fb":"## BOXPLOT FOR TEACHERS WITH PREVIOUS PROJECTS","03bcdb6d":"<p>\nDonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the DonorsChoose.org website.\n<\/p>\n<p>\n    Next year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they need to solve:\n<ul>\n<li>\n    How to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and as efficiently as possible<\/li>\n    <li>How to increase the consistency of project vetting across different volunteers to improve the experience for teachers<\/li>\n    <li>How to focus volunteer time on the applications that need the most assistance<\/li>\n    <\/ul>\n<\/p>    \n<p>\nThe goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval.\n<\/p>","d6849b7d":"### PREFIX","0581544c":"### SAMPLE SIZE = 20,000","b0fec07c":"\n**Observation:**\n1. The Median of the Approved Projects is slightly higher than the Median of the Rejected Projects, we can infer that, Approved Projects have higher number of words in the essay.\n1. Sufficiently large essays are considered as outlier in both Approved and Rejected Projects.","62b31ca3":"## MAXIMUM COUNT ANALYSIS","34e0f871":"## HISTOGRAM WITH PDF","af2b6809":"- https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/handling-categorical-and-numerical-features\/","94210cf5":"#### 1.4.2.6 Using Pretrained Models: AVG W2V on `project_title`","303775c4":"## Sample size is varied to observe the behaviour of tSNE on different size of data. Below are the configuration details\n\n1. BOW = 6000\n1. TF-IDF = 10,000\n1. Avg W2V = 15,000\n1. TF-IDF W2V= 20,000\n1. ALL ABOVE = 7000\n\nA max of 6000 datapoints were supported in an i5\/8gb system, however I tried the approach of saving the vectors in NPZ format and then loading it anytime without need to load the glove vectors and run the vector generation code, this allowed me to save RAM space. This further helped me in loading datapoints upto 20,000 in a normal machine.","4eb7c4f1":"### SAVING VECTORS","a04f5f89":"**Observation:**\n1. Literacy has the highest approved projects.\n1. Economics has the least approved projects.","ce33c6bf":"**Observations:**\n1. Unstructured random smaller and larger clusters are visible but with high overlapping of points.\n1. Unable to draw any concrete conlusion.","6426884b":"1. 3 cells in the Teacher_Prefix column had NaN values and those rows are not considered, thus the number of rows reduced from 109248 to 109245.\n1. Number of projects thar are approved for funding  92703 , ( 84.85788823287108 %)\n1. Number of projects thar are not approved for funding  16542 , ( 15.14211176712893 %)\n1. Every state has greater than 80% success rate in approval.\n1. DE (Delaware) has the Maximimum approval rate of 89.79 %\n1. VT (Vermont) has the Minimum approval rate of 80% then followed by DC (District of Columbia).\n1. Every state has greater than 80% success rate in approval.\n1. CA (California) state has the maximum number of the project proposals ie 15387.\n1. There is 50% reduction in the 2nd highest state for project proposals. ie TX (Texas) with 6014.\n1. VT (Vermont) has the minimum number of project submissions.\n1. High variation is observed in the state statistics.\n1. Female teachers have proposed more number of projects than Male teachers.\n1. Approval rate of Married Female teacher with prefix Mrs. is higher than Male Teachers\n1. Teacher having Dr. prefix has proposed very less projects\n1. Interstingly teachers with the highest qualification ie Dr. have lesser approval rate.\n1. Grades PreK-2 and 3-5 have large number of project proposals.\n1. Grades 3-5 has the highest approval rate then followed by Grade PreK-2.\n1. Number of project proposals reduces 4 times from Lower most grade to highest grade. That is, number of project proposal reduces as the grades increases.\n1. Literacy_Language is the Single most category having highest approval rate of 86%.\n1. If Literacy_Language is clubed with History_Civics, the approval rate rises from 86% to 89%.\n1. Math_Science alone has approval rate of around 82%, when clubbed with Literacy_Language, approval rate rises to 86%; when clubbed with AppliedLearning, rises to ~84%, but when clubbed with AppliedLearning, the approval rate reduced to 81%.\n1. Warmth and Care_Hunger has highest approval rate of ~93%. Thus from this we can conclude that, Non-Science categories have higher approval rate than compared to Science categories including Maths.\n1. Literacy_Language has the highest number of project than followed by Math_Science.\n1. Warmth and Care Hunger has the least number of projects.\n1. Interestingly, from the previous plot, we observed that Warmth and Care_Hunger when clubbed together has the highest approval rate.\n1. There is high variance between the 2nd and 3rd most number of project categories counts. ie Math_Science (41419) and Health_Sports (14223)\n1. Projects with sub-category Literacy has the highest number of projects.\n1. Also Literacy has the highest approval rate of 88%.\n1. Interestingly, when Literacy is clubbed with any other sub-category, the approval rate is reduced.\n1. Mathematics alone has lower approval rate than compared to Mathematics clubbed with any other category.\n1. AppliedSciences College_CareerPrep has least number of project posted and also the with least approval rate.\n1. Literacy has the highest approved projects.\n1. Economics has the least approved projects.\n1. Maximum projects have 4 words, then followed by 5 and 3.\n1. There are extremely less number of projects that have titles of 1 word and > 10 words.\n1. In Approved Projects, 25th Quartile lies at 4 words. Median at 5 words and 75th Quartile at 7 words.\n1. In Rejected Projects, 25th Quartile lies at 3 words. Median at 5 words and 75th Quartile at 6 words.\n1. In Approved Projects, the gap between the Median and 75th Quartile is large, where as exactly inverse scenario of a larger gap between 25th and Median is observed in Rejected Projects.\n1. In Approved Projects, titles with more than 11 words are considered as outliers, where as in Rejected Projects, titles with more than 10 words are considered as outliers.\n1. The number of Approved Projects have a slightly more words in the Title when compared to the Rejected Projects.\n1. The Median of the Approved Projects is slightly higher than the Median of the Rejected Projects, we can infer that, Approved Projects have higher number of words in the essay.\n1. Sufficiently large essays are considered as outlier in both Approved and Rejected Projects.\n1. The PDF of the approved projects is denser for words around 240 to around 470. From this, we can again say that, Approved Projects have higher number of words in the essay.\n1. The box plots for Price seems very much identical for Approved and Rejected Projects considering the 25th, 50th and 75th Quartiles.\n1. Interestingly, the Minimum value of the both the box plots for price is very close to 0, and maximum roughly at 800.\n1. Both the box plots for price have considered higher price as outliers.\n1. The PDFs of Price are very much similar and overlapping. Thus not much can be understood.\n1. To some extent, from the PDFs of Price, we can say Projects with higher price are generally not approved.\n1. It may not be , but at first glance, the distribution of the Teacher's number of previous project seems to look at LOG DISTRIBUTION.\n1. Minimum Previous Project : 0 - - - - - -Count: 30014\n1. Maximum Previous Projects : 451 - - - Count: 1\n1. Roughly 28% of the teachers have applied for the 1st time. Interestingly, 82% of those are accepted considering that they dont have any previous applications.\n1. We can observe from the box plot of previous projects, that the Median extremely close to 0\n1. The 75th Percentile of the data is roughly near 20.\n1. A large section of the data is beyond the 75th Percentile, hence considered as outlier by the plot.\n1. The PDF of the Approved -> teacher_number_of_previously_posted_projects, is slightly forward than compared to the PDF of Rejected, thus this gives a slight inference that, projects whose Teacher have previous projects will have a slighly approval rate higher than those who didnt.\n1. Teachers who had roughly 12 previous project were highly approved. ie mean seems to be around 12.\n1. From the CDF, we can say that teacher had Maximum project between 0-100.\n1. After roughly 100 project counts, the CDF\/PDF seems stable\n1. A Maximum of around 11,000 summaries is composed of 11 words.\n1. 2000 and above summaries is comprised of words ranging between 11 - 31.\n1. Interestingly we can observe that, if the summary word count is NOMINALLY large then Approval rate is higher. However beyond nominal count, ie extremely large summaries have lower chance of approval.\n1. Maximum project with summary having words between around 11 to 20 are approved.\n1. Number of Summaries with word counts in the range of around 20-30 is less than compared to 30-40\n1. Projects with summary having words less than roughly 11 and more than 40 are rejected.\n1. Extremely large sized summaries which can be seen as outliers are approved. Contrary to what we observed in the above PDF Plot Point no. 1\n1. The 25th, Median and 75th quartiles are almost similar in both the plots.\n1. The IQR range cover the summaries with the word count roughly between 11 to 25.\n1. Only a small portion of 14% have numeric values in the summary.\n1. Around 89% of the summaries having numeric values are approved than compared to the 84% approval rate of the summaries without the numeric values.\n1. It is therefore recommended to have a numeric value in summary to increase the chance of approval.\n1. Visualisation of TSNE with Bag of Words, TF-IDF, Avg Word2Vec, TF-IDF Weighted Word2Vec does not seem to provide an meaningfull observation. Clusters formed didnt provide us any useful insights. High overlapping of datapoints were observed in all the 5 visualization. Hence any other method needs to implemented for understanding this data.","0d77a63c":"**Observation:**\n1. The PDFs of Price are very much similar and overlapping. Thus not much can be understood.\n1. To some extent, from the PDFs of Price, we can say Projects with higher price are generally not approved.","7bab39ad":"## PDF - CDF","233eaff7":"<h2> 2.5 Summary <\/h2>","7016a329":"### Checking whether the presence of the numerical digits in the project_resource_summary effects the acceptance of the project or not.","7ea3db58":"tSNE with `BOW` encoding of `project_title` feature with the dataset size of 1200, 3200, 4200, 6000 for comparison purpose.","3107ce99":"## BOX PLOT FOR SUMMARY","abfc5704":"### 1.2.10 Univariate Analysis: project_resource_summary","55274458":"**Observation**\n1. Interestingly we can observe that, if the summary word count is NOMINALLY large then Approval rate is higher. However beyond nominal count, ie extremely large summaries have lower chance of approval.\n2. Maximum project with summary having words between around 11 to 20 are approved.\n3. Number of Summaries with word counts in the range of around 20-30 is less than compared to 30-40\n4. Projects with summary having words less than roughly 11 and more than 40 are rejected.","43cfcbde":"**Observation:**\n1. Literacy_Language is the Single most category having highest approval rate of 86%.\n1. If Literacy_Language is clubed with History_Civics, the approval rate rises from 86% to 89%.\n1. Math_Science alone has approval rate of around 82%, when clubbed with Literacy_Language, approval rate rises to 86%;  when clubbed with AppliedLearning, rises to ~84%, but when clubbed with AppliedLearning, the approval rate reduced to 81%.\n1. Warmth and  Care_Hunger has highest approval rate of ~93%. Thus from this we can conclude that, Non-Science categories have higher approval rate than compared to Science categories including Maths.","0744beee":"### SAMPLE SIZE = 7000","d0b31be8":"<h2> 2.4 TSNE with `TFIDF Weighted W2V` encoding of `project_title` feature <\/h2>","105670c3":"### 1.2.3 Univariate Analysis: project_grade_category","26d40ec3":"## HISTOGRAM FOR TEACHERS WITH PREVIOUS PROJECTS","cd366c6e":"**Observation**\n1. Extremely large sized summaries which can be seen as outliers are approved. Contrary to what we observed in the above PDF Plot Point no. 1\n2. The 25th, Median and 75th quartiles are almost similar in both the plots.\n3. The IQR range cover the summaries with the word count roughly between 11 to 25.","cffbf83f":"## 1.1 Reading Data","165ed438":"<h2> 2.2 TSNE with `TFIDF` encoding of `project_title` feature <\/h2>","68351160":"**Observation**:\n1. The PDF of the Approved -> teacher_number_of_previously_posted_projects, is slightly forward than compared to the PDF of Rejected, thus this gives a slight inference that, projects whose Teacher have previous projects will have a slighly approval rate higher than those who didnt.\n1. Teachers who had roughly 12 previous project were highly approved. ie mean seems to be around 12.\n\n","3513f83c":"#### Lets take a close look at the pattern of the top 100","c5d65d23":"### 1.2.7 Univariate Analysis: Text features (Project Essay's)","79fcf39a":"#### TEXT PROCESSING FUNCTIONS","5f072386":" <font color=#F4274F>If you are using any code snippet from the internet, you have to provide the reference\/citations, as we did in the above cells. Otherwise, it will be treated as plagiarism without citations.<\/font>","95c74766":"**Observation:** 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245. <br>\n**Action:** We can safely delete these, as 3 is very very small and its deletion wont impact as the original dataset is very large. <br>\nStep1: Convert all the empty strings with Nan \/\/ Not required as its NaN not empty string <br> \nStep2: Drop rows having NaN values","77a354ef":"### SAMPLE SIZE = 6000","85259236":"### 1.4.3.2 Standardizing Teacher's Previous Projects","6613f6c5":"### 1.2.4 Univariate Analysis: project_subject_categories","51203485":"**Observation:**\n1. The PDF of the approved projects is denser for words around 240 to around 470. From this, we can again say that, Approved Projects have higher number of words in the essay.","69f6aa1a":"**Observation:**\n 1. Every state has greater than 80% success rate in approval.\n 1. DE (Delaware) has the Maximimum approval rate of 89.79 %\n 1. VT (Vermont) has the Minimum approval rate of 80% then followed by DC (District of Columbia)\n    ","707608cf":"### 1.2.8 Univariate Analysis: Cost per project","8a5d5d09":"- we need to merge all the numerical vectors i.e catogorical, text, numerical vectors","5c083a0a":"**Observation:**\n 1. The count of teachers with more previous project reduces sharply.\n    ","37f09516":"<h2> 2.1 TSNE with `BOW` encoding of `project_title` feature <\/h2>","607370b5":"**Observation:**\n1. The box plots for Price seems very much identical for Approved and Rejected Projects considering the 25th, 50th and 75th Quartiles.\n1. Interestingly, the Minimum value of the both the box plots for price is very close to 0, and maximum roughly at 800.\n1. Both the box plots for price have considered higher price as outliers.","5bb67a77":"### 1.2.1 Univariate Analysis: School State","50c98a2a":"**Observation:**\n1. The number of Approved Projects have a slightly more words in the Title when compared to the Rejected Projects.","46a66bb0":"**Observations:**\n1. Unstructured large cluster is observed but with high overlapping of points.\n1. Unable to draw any concrete conlusion.","bbfc0385":"#### 1.4.2.9 Using Pretrained Models: TFIDF weighted W2V on `project_title`","637d848c":"we are going to consider\n\n       - school_state : categorical data\n       - clean_categories : categorical data\n       - clean_subcategories : categorical data\n       - project_grade_category : categorical data\n       - teacher_prefix : categorical data\n       \n       - project_title : text data\n       - text : text data\n       - project_resource_summary: text data\n       \n       - quantity : numerical\n       - teacher_number_of_previously_posted_projects : numerical\n       - price : numerical","76cade1e":"### 1.2.9 Univariate Analysis: teacher_number_of_previously_posted_projects","7366175e":"# DonorsChoose Dataset: EDA and TNSE","a77a3960":"## PDF FOR SUMMARY","99c17477":"### 1.2.6 Univariate Analysis: Text features (Title)","472f255f":"## 1. 4 Preparing data for models","c870c850":"**Observation:**\n1. Literacy_Language has the highest number of project than followed by Math_Science.\n1. Warmth and Care Hunger has the least number of projects.\n1. Interestingly, from the previous plot, we observed that Warmth and Care_Hunger when clubbed together has the highest approval rate.\n1. There is high variance between the 2nd and 3rd  most number of project categories counts. ie Math_Science (41419) and Health_Sports (14223)","cc5b2d32":"**Observation:**\n 1. Female teachers have proposed more number of projects than Male teachers.\n 1. Approval rate of Married Female teacher with prefix Mrs. is higher than Male Teachers\n 1. Teacher having Dr. prefix has proposed very less projects\n 1. Interstingly teachers with the highest qualification ie Dr. have lesser approval rate.","d7131069":"### 1.2.5 Univariate Analysis: project_subject_subcategories","96d06c18":"**Observation:**\n1. Grades PreK-2 and 3-5 have large number of project proposals.\n1. Grades 3-5 has the highest approval rate then followed by Grade PreK-2\n1. Number of project proposals reduces 4 times from Lower most grade to highest grade. That is, number of project proposal reduces as the grades increases.","78c91a1b":"**Observation:**\n1. Projects with sub-category Literacy has the highest number of projects.\n1. Also Literacy has the highest approval rate of 88%.\n1. Interestingly, when Literacy is clubbed with any other sub-category, the approval rate is reduced.\n1. Mathematics alone has lower approval rate than compared to Mathematics clubbed with any other category.\n1. AppliedSciences College_CareerPrep has least number of project posted and also the with least approval rate.","f6572d40":"**Please ignore the above Scipy Error, its resolved by importing it. Since it is taking a long time to run, I have avoided to re-run it completly.**","6844876b":"## BAR PLOTS FOR TEACHERS WITH PREVIOUS PROJETCS","6a15d37c":"### 1.4.3 Vectorizing Numerical features","ed75d1ff":"### GRADE","591ce31f":"**Observation:** \n1. 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245.","900819d2":"### 1.2.2 Univariate Analysis: teacher_prefix","fd281db4":"**Teacher Prefix has NAN values, that needs to be cleaned.\nRef: https:\/\/stackoverflow.com\/a\/50297200\/4433839**","8aba2e73":"#### 1.4.2.1 Bag of words: ESSAYS","932f2676":"#### Checking total number of enteries with NaN values","cdd7a2af":"<ol> \n    <li> In the above cells we have plotted and analyzed many features. Please observe the plots and write the observations in markdown cells below every plot.<\/li>\n    <li> EDA: Please complete the analysis of the feature: teacher_number_of_previously_posted_projects<\/li>\n    <li>\n        <ul>Build the data matrix using these features \n            <li>school_state : categorical data (one hot encoding)<\/li>\n            <li>clean_categories : categorical data (one hot encoding)<\/li>\n            <li>clean_subcategories : categorical data (one hot encoding)<\/li>\n            <li>teacher_prefix : categorical data (one hot encoding)<\/li>\n            <li>project_grade_category : categorical data (one hot encoding)<\/li>\n            <li>project_title : text data (BOW, TFIDF, AVG W2V, TFIDF W2V)<\/li>\n            <li>price : numerical<\/li>\n            <li>teacher_number_of_previously_posted_projects : numerical<\/li>\n         <\/ul>\n    <\/li>\n    <li> Now, plot FOUR t-SNE plots with each of these feature sets.\n        <ol>\n            <li>categorical, numerical features + project_title(BOW)<\/li>\n            <li>categorical, numerical features + project_title(TFIDF)<\/li>\n            <li>categorical, numerical features + project_title(AVG W2V)<\/li>\n            <li>categorical, numerical features + project_title(TFIDF W2V)<\/li>\n        <\/ol>\n    <\/li>\n    <li> Concatenate all the features and Apply TNSE on the final data matrix <\/li>\n    <li> <font color='blue'>Note 1: The TSNE accepts only dense matrices<\/font><\/li>\n    <li> <font color='blue'>Note 2: Consider only 5k to 6k data points to avoid memory issues. If you run into memory error issues, reduce the number of data points but clearly state the number of datat-poins you are using<\/font><\/li>\n<\/ol>","eb2e2c55":"#### 1.4.2.5 Using Pretrained Models: Avg W2V","1cde3ed3":"### SAMPLE SIZE = 10,000","0397aaac":"### 1.4.4 Merging all the above features","f69cf0f9":"### 1.3.2 Project title Text","2650b5ba":"**Observation:**\n1. We can observe from the box plot of previous projects, that the Median extremely close to 0\n2. The 75th Percentile of the data is roughly near 20.\n3. A large section of the data is beyond the 75th Percentile, hence considered as outlier by the plot.","0d80903c":"**Observation**\n1. Only a small portion of 14% have numeric values in the summary.\n1. Around 89% of the summaries having numeric values are approved than compared to the 84% approval rate of the summaries without the numeric values.\n1. It is therefore recommended to have a numeric value in summary to increase the chance of approval.","ab9753d1":"#### Checking the shape for final merging.","f785245f":"## PDF FOR TEACHERS WITH PREVIOUS PROJECTS","47f1ad82":"**Observation:**\n1. It may not be , but at first glance, the distribution of the Teacher's number of previous project seems to look at LOG DISTRIBUTION.","19df8442":"### STATE","ec1f43e0":"**Observation**:\n1. From the CDF, we can say that teacher had Maximum project between 0-100.\n2. After roughly 100 project counts, the CDF\/PDF seems stable\n    ","20c27ee2":"### 1.4.3.1 Standardizing Price","7b0a8be9":"**Observation:**\n\nMinimum Previous Project  : 0 - - - - - -Count: 30014   \nMaximum Previous Projects : 451 - - - Count: 1","997c9cea":"## 1.3 Text preprocessing","835a599f":"### 1.4.2 Vectorizing Text data","8d692d93":"**Observation**\n1. A Maximum of around 11,000 summaries is composed of 11 words.\n2. 2000 and above summaries is comprised of words ranging between 11 - 31.","34896d54":"**Observation:**\n1. In Approved Projects, 25th Quartile lies at 4 words. Median at 5 words and 75th Quartile at 7 words.\n1. In Rejected Projects, 25th Quartile lies at 3 words. Median at 5 words and 75th Quartile at 6 words.\n1. In Approved Projects, the gap between the  Median and 75th Quartile is large, where as exactly inverse scenario of a larger gap between 25th and Median is observed in Rejected Projects.\n1. In Approved Projects, titles with more than 11 words are considered as outliers, where as in Rejected Projects, titles with more than 10 words are considered as outliers.","8906eab7":"### Notes on the Essay Data\n\n<ul>\nPrior to May 17, 2016, the prompts for the essays were as follows:\n<li>__project_essay_1:__ \"Introduce us to your classroom\"<\/li>\n<li>__project_essay_2:__ \"Tell us more about your students\"<\/li>\n<li>__project_essay_3:__ \"Describe how your students will use the materials you're requesting\"<\/li>\n<li>__project_essay_4:__ \"Close by sharing why your project will make a difference\"<\/li>\n<\/ul>\n\n\n<ul>\nStarting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following:<br>\n<li>__project_essay_1:__ \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"<\/li>\n<li>__project_essay_2:__ \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"<\/li>\n<br>For all projects with project_submitted_datetime of 2016-05-17 and later, the values of project_essay_3 and project_essay_4 will be NaN.\n<\/ul>\n","3271c6f3":"<h4> 1.4.2.2 Bag of Words: PROJECT TITLES<\/h4>","61229a6b":"**Observations:**\n1. Unstructured large cluster is observed but with high overlapping of points.\n1. Unable to draw any concrete conlusion.","384dc0e8":"### As it is clearly metioned in the dataset details that TEACHER_PREFIX has NaN values, we need to handle this at the very beginning to avoid any problems in our future analysis","23b67896":"![..\/input\/allBows.PNG](attachment:allBows.PNG)","11b9edeb":"### 1.4.1 Vectorizing Categorical data","ec3b891d":"Please do this on your own based on the data analysis that was done in the above cells\n\nCheck if the `presence of the numerical digits` in the `project_resource_summary` effects the acceptance of the project or not. If you observe that `presence of the numerical digits` is helpful in the classification, please include it for further process or you can ignore it.","feb5e332":"# SAVING AND LOADING MATRIX FOR EFFICIENT RUNNING","2b53bceb":"### LOADING VECTORS","2e96ae25":"<h2> 2.4 TSNE with `BOW, TF-IDF, Avg W2C and TFIDF Weighted W2V` encoding of `project_title` feature <\/h2>","f0b47d49":"## TOP 10 ANALYSIS","b31d2526":"## About the DonorsChoose Data Set\n\nThe `train.csv` data set provided by DonorsChoose contains the following features:\n\nFeature | Description \n----------|---------------\n**`project_id`** | A unique identifier for the proposed project. **Example:** `p036502`   \n**`project_title`**    | Title of the project. **Examples:**<br><ul><li><code>Art Will Make You Happy!<\/code><\/li><li><code>First Grade Fun<\/code><\/li><\/ul> \n**`project_grade_category`** | Grade level of students for which the project is targeted. One of the following enumerated values: <br\/><ul><li><code>Grades PreK-2<\/code><\/li><li><code>Grades 3-5<\/code><\/li><li><code>Grades 6-8<\/code><\/li><li><code>Grades 9-12<\/code><\/li><\/ul>  \n **`project_subject_categories`** | One or more (comma-separated) subject categories for the project from the following enumerated list of values:  <br\/><ul><li><code>Applied Learning<\/code><\/li><li><code>Care &amp; Hunger<\/code><\/li><li><code>Health &amp; Sports<\/code><\/li><li><code>History &amp; Civics<\/code><\/li><li><code>Literacy &amp; Language<\/code><\/li><li><code>Math &amp; Science<\/code><\/li><li><code>Music &amp; The Arts<\/code><\/li><li><code>Special Needs<\/code><\/li><li><code>Warmth<\/code><\/li><\/ul><br\/> **Examples:** <br\/><ul><li><code>Music &amp; The Arts<\/code><\/li><li><code>Literacy &amp; Language, Math &amp; Science<\/code><\/li>  \n  **`school_state`** | State where school is located ([Two-letter U.S. postal code](https:\/\/en.wikipedia.org\/wiki\/List_of_U.S._state_abbreviations#Postal_codes)). **Example:** `WY`\n**`project_subject_subcategories`** | One or more (comma-separated) subject subcategories for the project. **Examples:** <br\/><ul><li><code>Literacy<\/code><\/li><li><code>Literature &amp; Writing, Social Sciences<\/code><\/li><\/ul> \n**`project_resource_summary`** | An explanation of the resources needed for the project. **Example:** <br\/><ul><li><code>My students need hands on literacy materials to manage sensory needs!<\/code<\/li><\/ul> \n**`project_essay_1`**    | First application essay<sup>*<\/sup>  \n**`project_essay_2`**    | Second application essay<sup>*<\/sup> \n**`project_essay_3`**    | Third application essay<sup>*<\/sup> \n**`project_essay_4`**    | Fourth application essay<sup>*<\/sup> \n**`project_submitted_datetime`** | Datetime when project application was submitted. **Example:** `2016-04-28 12:43:56.245`   \n**`teacher_id`** | A unique identifier for the teacher of the proposed project. **Example:** `bdf8baa8fedef6bfeec7ae4ff1c15c56`  \n**`teacher_prefix`** | Teacher's title. One of the following enumerated values: <br\/><ul><li><code>nan<\/code><\/li><li><code>Dr.<\/code><\/li><li><code>Mr.<\/code><\/li><li><code>Mrs.<\/code><\/li><li><code>Ms.<\/code><\/li><li><code>Teacher.<\/code><\/li><\/ul>  \n**`teacher_number_of_previously_posted_projects`** | Number of project applications previously submitted by the same teacher. **Example:** `2` \n\n<sup>*<\/sup> See the section <b>Notes on the Essay Data<\/b> for more details about these features.\n\nAdditionally, the `resources.csv` data set provides more data about the resources required for each project. Each line in this file represents a resource required by a project:\n\nFeature | Description \n----------|---------------\n**`id`** | A `project_id` value from the `train.csv` file.  **Example:** `p036502`   \n**`description`** | Desciption of the resource. **Example:** `Tenor Saxophone Reeds, Box of 25`   \n**`quantity`** | Quantity of the resource required. **Example:** `3`   \n**`price`** | Price of the resource required. **Example:** `9.95`   \n\n**Note:** Many projects require multiple resources. The `id` value corresponds to a `project_id` in train.csv, so you use it as a key to retrieve all resources needed for a project:\n\nThe data set contains the following label (the value you will attempt to predict):\n\nLabel | Description\n----------|---------------\n`project_is_approved` | A binary flag indicating whether DonorsChoose approved the project. A value of `0` indicates the project was not approved, and a value of `1` indicates the project was approved.","09629e13":"To better understand the Top 5 result from the above plot, we calculate more information about the top 10 results as below:","6c2aa9b5":"**Observations:**\n1. Unstructured random smaller and larger clusters are visible.\n1. Data points are very much overlapping.\n1. Unable to draw any concrete conlusion.","2a8adebb":"### 1.3.1 Essay Text","cf2cc79d":"**Observation:**\n1. The points are scattered in dataset of size 1200\n1. With 1200 data points, hardly any clusters form are visible. Just a large cloud is visible.\n1. As the datapoints are increased, we can see the lighter clusters are now turning into solid cluster, but any pattern is not observed.","2263050c":"#### 1.4.2.7 Using Pretrained Models: TFIDF weighted W2V","6ee9b86c":"# 1.2 Data Analysis","f4bb5bd7":"#### 1.4.2.3 TFIDF vectorizer","f47bbfd2":"**Conclusion:** Now the number of rows reduced from 109248 to 109245 in project_data.","d3d2fc24":"<h1><font color='red'>Assignment 2: Apply TSNE<font><\/h1>","6bb2d1c8":"**Observations:**\n1. Unstructured large cluster is observed but with high overlapping of points.\n1. Unable to draw any concrete conlusion.","3fb8d876":"**Observation:**\n1. Maximum projects have 4 words, then followed by 5 and 3.\n1. There are extremely less number of projects that have titles of 1 word and > 10 words.","e39d3e13":"<h2> 2.3 TSNE with `AVG W2V` encoding of `project_title` feature <\/h2>"}}