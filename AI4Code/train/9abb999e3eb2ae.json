{"cell_type":{"1e64b832":"code","6f32d630":"code","08dbb4fc":"code","3038a1cb":"code","401aa379":"code","97b17400":"code","cdd642db":"code","e1c2a6d2":"code","0d6eca3c":"code","df88124c":"code","688a3abd":"code","4b680a55":"code","22e5bf94":"code","8730165e":"code","cfe9fedb":"code","ca4d87da":"code","b74b247a":"code","a86203a4":"code","75bf8f9d":"code","65fc73ac":"code","fa617b5d":"code","b59e7c3c":"code","82b5caee":"code","be97de93":"code","4086dd26":"code","579289ac":"code","6ce10aa8":"code","5061439f":"code","201fd56f":"code","30aa0af4":"code","ba0c3ad9":"code","be107c93":"code","37b58d10":"code","5006ad77":"code","0515b326":"code","b0514784":"code","3d7277d9":"code","ceb005b7":"code","638fd65b":"code","93c712ab":"code","d7c838dd":"code","5a816a70":"code","124df4c3":"code","82043d95":"code","bd786e24":"code","ff757834":"code","2e2a6b30":"code","32769bdc":"code","3aeab0b2":"code","de08b492":"code","6c495042":"code","f4905bc2":"code","ed9b42ab":"code","4dcb987f":"code","58a5755e":"code","8c7d18c7":"code","9972cf31":"code","a7d80cf7":"code","d587a704":"markdown","8631ad89":"markdown","bedb39f3":"markdown","03f2a2b1":"markdown","202d7d7a":"markdown","088ea219":"markdown","d33d11ef":"markdown","778cc1ec":"markdown","6ecba517":"markdown","9a2c627d":"markdown","08e2c674":"markdown","d5d9a411":"markdown","8c2310e5":"markdown","1018838d":"markdown","7e9b2130":"markdown","9ee450c4":"markdown"},"source":{"1e64b832":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Configs\npd.options.display.float_format = '{:,.4f}'.format\nsns.set(style=\"whitegrid\")\nplt.style.use('seaborn')\nseed = 42\nnp.random.seed(seed)","6f32d630":"file_path = '\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv'\ndf = pd.read_csv(file_path)\nprint(\"DataSet = {:,d} rows and {} columns\".format(df.shape[0], df.shape[1]))\n\nprint(\"\\nAll Columns:\\n=>\", df.columns.tolist())\n\nquantitative = [f for f in df.columns if df.dtypes[f] != 'object']\nqualitative = [f for f in df.columns if df.dtypes[f] == 'object']\n\nprint(\"\\nStrings Variables:\\n=>\", qualitative,\n      \"\\n\\nNumerics Variables:\\n=>\", quantitative)\n\ndf.head(3)","08dbb4fc":"from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, f1_score\n\nthis_labels = ['No Churn','Churn']\nscoress = {}\n\ndef class_report(y_real, y_my_preds, name=\"\", labels=this_labels):\n    if(name != ''):\n        print(name,\"\\n\")\n    print(confusion_matrix(y_real, y_my_preds), '\\n')\n    print(classification_report(y_real, y_my_preds, target_names=labels))\n    scoress[name] = [accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='macro')]","3038a1cb":"import time\n\ndef time_spent(time0):\n    t = time.time() - time0\n    t_int = int(t) \/\/ 60\n    t_min = t % 60\n    if(t_int != 0):\n        return '{} min {:.3f} s'.format(t_int, t_min)\n    else:\n        return '{:.3f} s'.format(t_min)","401aa379":"# statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew, boxcox_normmax #for some statistics\nfrom scipy.special import boxcox1p\n\ndef test_normal_distribution(serie, series_name='series', thershold=0.4):\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 6), sharex=False)\n    f.suptitle('{} is a Normal Distribution?'.format(series_name), fontsize=18)\n    ax1.set_title(\"Histogram to \" + series_name)\n    ax2.set_title(\"Q-Q-Plot to \"+ series_name)\n    mu, sigma = norm.fit(serie)\n    print('Normal dist. (mu= {:,.2f} and sigma= {:,.2f} )'.format(mu, sigma))\n    skewness = serie.skew()\n    kurtoise = serie.kurt()\n    print(\"Skewness: {:,.2f} | Kurtosis: {:,.2f}\".format(skewness, kurtoise))\n    pre_text = '\\t=> '\n    if(skewness < 0):\n        text = pre_text + 'negatively skewed or left-skewed'\n    else:\n        text =  pre_text + 'positively skewed or right-skewed\\n'\n        text += pre_text + 'in case of positive skewness, log transformations usually works well.\\n'\n        text += pre_text + 'np.log(), np.log1(), boxcox1p()'\n    if(skewness < -1 or skewness > 1):\n        print(\"Evaluate skewness: highly skewed\")\n        print(text)\n    if( (skewness <= -0.5 and skewness > -1) or (skewness >= 0.5 and skewness < 1)):\n        print(\"Evaluate skewness: moderately skewed\")\n        print(text)\n    if(skewness >= -0.5 and skewness <= 0.5):\n        print('Evaluate skewness: approximately symmetric')\n    print('evaluate kurtoise')\n    if(kurtoise > 3 + thershold):\n        print(pre_text + 'Leptokurtic: anormal: Peak is higher')\n    elif(kurtoise < 3 - thershold):\n        print(pre_text + 'Platykurtic: anormal: The peak is lower')\n    else:\n        print(pre_text + 'Mesokurtic: normal: the peack is normal')\n    sns.distplot(serie , fit=norm, ax=ax1)\n    ax1.legend(['Normal dist. ($\\mu=$ {:,.2f} and $\\sigma=$ {:,.2f} )'.format(mu, sigma)],\n            loc='best')\n    ax1.set_ylabel('Frequency')\n    stats.probplot(serie, plot=ax2)\n    plt.show()","97b17400":"def plot_top_bottom_rank_correlation(my_df, column_target, top_rank=5, title=''):\n    corr_matrix = my_df.corr()\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 7), sharex=False)\n    if(title):\n        f.suptitle(title)\n\n    ax1.set_title('Top {} Positive Corr to {}'.format(top_rank, column_target))\n    ax2.set_title('Top {} Negative Corr to {}'.format(top_rank, column_target))\n    \n    cols_top = corr_matrix.nlargest(top_rank+1, column_target)[column_target].index\n    cm = np.corrcoef(my_df[cols_top].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 11}, yticklabels=cols_top.values,\n                     xticklabels=cols_top.values, mask=mask, ax=ax1)\n    \n    cols_bot = corr_matrix.nsmallest(top_rank, column_target)[column_target].index\n    cols_bot  = cols_bot.insert(0, column_target)\n    cm = np.corrcoef(my_df[cols_bot].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 11}, yticklabels=cols_bot.values,\n                     xticklabels=cols_bot.values, mask=mask, ax=ax2)\n    \n    plt.show()","cdd642db":"def check_balanced_train_test_binary(x_train, y_train, x_test, y_test, original_size, labels):\n    \"\"\" To binary classification\n    each paramethes is pandas.core.frame.DataFrame\n    @total_size = len(X) before split\n    @labels = labels in ordem [0,1 ...]\n    \"\"\"\n    train_unique_label, train_counts_label = np.unique(y_train, return_counts=True)\n    test_unique_label, test_counts_label = np.unique(y_test, return_counts=True)\n\n    prop_train = train_counts_label\/ len(y_train)\n    prop_test = test_counts_label\/ len(y_test)\n\n    print(\"Original Size:\", '{:,d}'.format(original_size))\n    print(\"\\nTrain: must be 80% of dataset:\\n\", \n          \"the train dataset has {:,d} rows\".format(len(x_train)),\n          'this is ({:.2%}) of original dataset'.format(len(x_train)\/original_size),\n                \"\\n => Classe 0 ({}):\".format(labels[0]), train_counts_label[0], '({:.2%})'.format(prop_train[0]), \n                \"\\n => Classe 1 ({}):\".format(labels[1]), train_counts_label[1], '({:.2%})'.format(prop_train[1]),\n          \"\\n\\nTest: must be 20% of dataset:\\n\",\n          \"the test dataset has {:,d} rows\".format(len(x_test)),\n          'this is ({:.2%}) of original dataset'.format(len(x_test)\/original_size),\n                  \"\\n => Classe 0 ({}):\".format(labels[0]), test_counts_label[0], '({:.2%})'.format(prop_test[0]),\n                  \"\\n => Classe 1 ({}):\".format(labels[1]),test_counts_label[1], '({:.2%})'.format(prop_test[1])\n         )","e1c2a6d2":"def eda_categ_feat_desc_plot(series_categorical, title = \"\", fix_labels=False):\n    \"\"\"Generate 2 plots: barplot with quantity and pieplot with percentage. \n       @series_categorical: categorical series\n       @title: optional\n       @fix_labels: The labes plot in barplot in sorted by values, some times its bugs cuz axis ticks is alphabethic\n           if this happens, pass True in fix_labels\n       @bar_format: pass {:,.0f} to int\n    \"\"\"\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (12,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    if(fix_labels):\n        val_concat = val_concat.sort_values(series_name).reset_index()\n    \n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], '{:,d}'.format(int(row['quantity'])), color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","0d6eca3c":"def eda_numerical_feat(series, title=\"\", with_label=True, number_format=\"\", show_describe=False, size_labels=10):\n    # Use 'series_remove_outiliers' to filter outiliers\n    \"\"\" Generate series.describe(), bosplot and displot to a series\n    @with_label: show labels in boxplot\n    @number_format: \n        integer: \n            '{:d}'.format(42) => '42'\n            '{:,d}'.format(12855787591251) => '12,855,787,591,251'\n        float:\n            '{:.0f}'.format(91.00000) => '91' # no decimal places\n            '{:.2f}'.format(42.7668)  => '42.77' # two decimal places and round\n            '{:,.4f}'.format(1285591251.78) => '1,285,591,251.7800'\n            '{:.2%}'.format(0.09) => '9.00%' # Percentage Format\n        string:\n            ab = '$ {:,.4f}'.format(651.78) => '$ 651.7800'\n    def swap(string, v1, v2):\n        return string.replace(v1, \"!\").replace(v2, v1).replace('!',v2)\n    # Using\n        swap(ab, ',', '.')\n    \"\"\"\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 5), sharex=False)\n    if(show_describe):\n        print(series.describe())\n    if(title != \"\"):\n        f.suptitle(title, fontsize=18)\n    sns.distplot(series, ax=ax1)\n    sns.boxplot(series, ax=ax2)\n    if(with_label):\n        describe = series.describe()\n        labels = { 'min': describe.loc['min'], 'max': describe.loc['max'], \n              'Q1': describe.loc['25%'], 'Q2': describe.loc['50%'],\n              'Q3': describe.loc['75%']}\n        if(number_format != \"\"):\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + number_format.format(v), ha='center', va='center', fontweight='bold',\n                         size=size_labels, color='white', bbox=dict(facecolor='#445A64'))\n        else:\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + str(v), ha='center', va='center', fontweight='bold',\n                     size=size_labels, color='white', bbox=dict(facecolor='#445A64'))\n    plt.show()","df88124c":"sns.heatmap(df.isnull(), cbar=False, yticklabels=False)","688a3abd":"df.duplicated().sum()\n","4b680a55":"df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')","22e5bf94":"df.head()","8730165e":"eda_categ_feat_desc_plot(df['gender'], title = \"gender distribution\")","cfe9fedb":"eda_categ_feat_desc_plot(df['SeniorCitizen'], title = \"SeniorCitizen distribution\")","ca4d87da":"eda_categ_feat_desc_plot(df['Partner'], title = \"Partner distribution\")","b74b247a":"# PhoneService\neda_categ_feat_desc_plot(df['PhoneService'], title = \"PhoneService distribution\")","a86203a4":"# PhoneService\neda_categ_feat_desc_plot(df['MultipleLines'], title = \"MultipleLines distribution\")","75bf8f9d":"# PhoneService\neda_categ_feat_desc_plot(df['InternetService'], title = \"InternetService distribution\")","65fc73ac":"# PhoneService\neda_categ_feat_desc_plot(df['OnlineSecurity'], title = \"OnlineSecurity distribution\")","fa617b5d":"# InternetService\n","b59e7c3c":"# InternetService\neda_categ_feat_desc_plot(df['InternetService'], title = \"InternetService distribution\")","82b5caee":"eda_numerical_feat(df['MonthlyCharges'], title=\"MonthlyCharges distribution\")","be97de93":"eda_categ_feat_desc_plot(df['Churn'], title = \"Churn distribution\")","4086dd26":"df['StreamingMovies'].value_counts()","579289ac":"yes_no = {'No':0, 'Yes': 1}\ngender = {'Female':0, 'Male':1}\n\ndf1 = df.copy().drop(['customerID'], axis=1)\n\ndf1['Churn'] = df1['Churn'].replace(yes_no)\ndf1['PaperlessBilling'] = df1['PaperlessBilling'].replace(yes_no)\ndf1['Partner'] = df1['Partner'].replace(yes_no)\ndf1['Dependents'] = df1['Dependents'].replace(yes_no)\ndf1['PhoneService'] = df1['PhoneService'].replace(yes_no)\n\ndf1['gender'] = df1['gender'].replace(gender)\n\nmultiple_lines = pd.get_dummies(df1['MultipleLines'], prefix='ML')\ninternet_service = pd.get_dummies(df1['InternetService'], prefix='IS')\nonline_security = pd.get_dummies(df1['OnlineSecurity'], prefix='OS')\nonline_backup = pd.get_dummies(df1['OnlineBackup'], prefix='OB')\n\ndevice_protection = pd.get_dummies(df1['DeviceProtection'], prefix='DP')\ntech_support = pd.get_dummies(df1['TechSupport'], prefix='TS')\nstreaming_tv = pd.get_dummies(df1['StreamingTV'], prefix='ST')\nstreaming_movies = pd.get_dummies(df1['StreamingMovies'], prefix='SM')\n\ncontract = pd.get_dummies(df1['Contract'], prefix='Contr')\npayment_method = pd.get_dummies(df1['PaymentMethod'], prefix='PM')\n\ndummies_columns = [multiple_lines, internet_service, online_security, online_backup,\n                  device_protection, tech_support, streaming_tv, streaming_movies,\n                  contract, payment_method]\n\ndf1['TotalCharges'] = df1['TotalCharges'].replace(\" \", 0).astype('float32')\n\ndf1 = pd.concat([df1, *dummies_columns], axis=1)\n\ndf1 = df1.drop(['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n               'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n               'Contract', 'PaymentMethod'], axis=1)\n\ndf1","6ce10aa8":"abc = plot_top_bottom_rank_correlation(df1, 'Churn', top_rank=12, title='Top Cors')","5061439f":"df1","201fd56f":"from sklearn.model_selection import train_test_split\n\nX = df1.drop(['Churn'], axis=1)\n\ny = df1['Churn']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y.values, test_size=0.20, random_state=42)\n\ncheck_balanced_train_test_binary(x_train, y_train, x_test, y_test, len(df), ['Response 0', 'Response 1'])","30aa0af4":"from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, SVMSMOTE, BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\nfrom imblearn.combine import SMOTEENN, SMOTETomek # over and under sampling\nfrom imblearn.metrics import classification_report_imbalanced\n\nimb_models = {\n    'ADASYN': ADASYN(random_state=42),\n    'SMOTE': SMOTE(random_state=42),\n    'SMOTEENN': SMOTEENN(\"minority\", random_state=42),\n    'SMOTETomek': SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=42),\n    'RandomUnderSampler': RandomUnderSampler(random_state=42)\n}\n\nimb_strategy = \"None\"\n\nif(imb_strategy != \"None\"):\n    before = x_train.shape[0]\n    imb_tranformer = imb_models[imb_strategy]\n    x_train, y_train = imb_tranformer.fit_sample(x_train, y_train)\n    print(\"train dataset before: {:,d}\\nimbalanced_strategy: {}\".format(before, imb_strategy),\n          \"\\ntrain dataset after: {:,d}\\ngenerate: {:,d}\".format(x_train.shape[0], x_train.shape[0] - before))\nelse:\n    print(\"Dont correct unbalanced dataset\")","ba0c3ad9":"# Classifier Libraries\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# Ensemble Classifiers\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\n# Others Linear Classifiers\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\nfrom sklearn.linear_model import Perceptron, PassiveAggressiveClassifier\n\n# xboost\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# scores\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n\n# neural net of sklearn\nfrom sklearn.neural_network import MLPClassifier\n\n# others\nimport time\nimport operator","be107c93":"all_classifiers = {\n    \"NaiveBayes\": GaussianNB(),\n    \"Ridge\": RidgeClassifier(),\n    \"Perceptron\": Perceptron(),\n    \"PassiveAggr\": PassiveAggressiveClassifier(),\n    \"XGBoost\": XGBClassifier(),\n    \"LightGB\": LGBMClassifier(boosting_type='gbdt',n_estimators=500,depth=10,learning_rate=0.04,objective='binary',\n                 colsample_bytree=0.5,reg_lambda=2,reg_alpha=2,random_state=294,n_jobs=-1),\n    \"SVM\": SVC(),\n    \"LogisiticR\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n#     \"DecisionTree\": DecisionTreeClassifier(),\n    \"AdaBoost\": AdaBoostClassifier(), # All 100 features: 48min\n    # \"SGDC\": SGDClassifier(),\n    \"GBoost\": GradientBoostingClassifier(),\n#     \"Bagging\": BaggingClassifier(),\n    \"RandomForest\": RandomForestClassifier(),\n    \"ExtraTree\": ExtraTreesClassifier()\n}","37b58d10":"metrics = { 'cv_acc': {}, 'acc_test': {}, 'f1_test': {} }\nm = list(metrics.keys())\ntime_start = time.time()\nprint('CrossValidation, Fitting and Testing')\n\n# Cross Validation, Fit and Test\nfor name, model in all_classifiers.items():\n    print('{:15}'.format(name), end='')\n    t0 = time.time()\n    # Cross Validation\n    training_score = cross_val_score(model, x_train, y_train, scoring=\"accuracy\", cv=4)\n    # Fitting\n    all_classifiers[name] = model.fit(x_train, y_train) \n    # Testing\n    y_pred = all_classifiers[name].predict(x_test)\n    t1 = time.time()\n    # Save metrics\n    metrics[m[0]][name] = training_score.mean()\n    metrics[m[1]][name] = accuracy_score(y_test, y_pred)\n    metrics[m[2]][name] = f1_score(y_test, y_pred, average=\"macro\") \n    # Show metrics\n    print('| {}: {:6,.4f} | {}: {:6,.4f} | {}: {:6.4f} | took: {:>15} |'.format(\n        m[0], metrics[m[0]][name], m[1], metrics[m[1]][name],\n        m[2], metrics[m[2]][name], time_spent(t0) ))\n        \nprint(\"\\nDone in {}\".format(time_spent(time_start)))","5006ad77":"print(\"Best cv acc  :\", max( metrics[m[0]].items(), key=operator.itemgetter(1) ))\nprint(\"Best acc test:\", max( metrics[m[1]].items(), key=operator.itemgetter(1) ))\nprint(\"Best f1 test :\", max( metrics[m[2]].items(), key=operator.itemgetter(1) ))\n\ndf_metrics = pd.DataFrame(data = [list(metrics[m[0]].values()),\n                                  list(metrics[m[1]].values()),\n                                  list(metrics[m[2]].values())],\n                          index = ['cv_acc', 'acc_test', 'f1_test' ],\n                          columns = metrics[m[0]].keys() ).T.sort_values(by=m[2], ascending=False)\ndf_metrics","0515b326":"from catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\nname = 'CatBoost'\ncatb = CatBoostClassifier()\n\nt0 = time.time()\n# Fitting\ncatb = catb.fit(x_train, y_train, eval_set=(x_test, y_test), plot=False, early_stopping_rounds=30,verbose=0)\n# catb = catb.fit(x_train, y_train, cat_features=cat_col, eval_set=(x_test, y_test), plot=False, early_stopping_rounds=30,verbose=0) \n# Testing\ny_pred = catb.predict(x_test)\nt1 = time.time()\n# Save metrics\nmetrics[m[0]][name] = 0.0\nmetrics[m[1]][name] = accuracy_score(y_test, y_pred)\nmetrics[m[2]][name] = f1_score(y_test, y_pred, average=\"macro\") \n\n# Show metrics\nprint('{:15} | {}: {:6,.4f} | {}: {:6.4f} | took: {:>15} |'.format(\n    name, m[1], metrics[m[1]][name],\n    m[2], metrics[m[2]][name], time_spent(t0) ))","b0514784":"feat_importances = pd.Series(catb.feature_importances_, index=X.columns)\nfeat_importances.nlargest(25).plot(kind='barh')\nplt.show()","3d7277d9":"from sklearn.model_selection import GridSearchCV\n\ndef optimize_logistic_r(mx_train, my_train, my_hyper_params, hyper_to_search, hyper_search_name, cv=4, scoring='accuracy'):\n    \"\"\"search best param to unic one hyper param\n    @mx_train, @my_train = x_train, y_train of dataset\n    @my_hyper_params: dict with actuals best_params: start like: {}\n      => will be accumulated and modified with each optimization iteration\n      => example stater: best_hyper_params = {'random_state': 42, 'n_jobs': -1}\n    @hyper_to_search: dict with key @hyper_search_name and list of values to gridSearch:\n    @hyper_search_name: name of hyperparam\n    \"\"\"\n    if(hyper_search_name in my_hyper_params.keys()):\n        del my_hyper_params[hyper_search_name]\n    if(hyper_search_name not in hyper_to_search.keys()):\n        raise Exception('\"hyper_to_search\" dont have {} in dict'.format(hyper_search_name))\n        \n    t0 = time.time()\n        \n    rf = LogisticRegression(**my_hyper_params)\n    \n    grid_search = GridSearchCV(estimator = rf, param_grid = hyper_to_search, \n      scoring = scoring, n_jobs = -1, cv = cv)\n    grid_search.fit(mx_train, my_train)\n    \n    print('took', time_spent(t0))\n    \n    data_frame_results = pd.DataFrame(\n        data={'mean_fit_time': grid_search.cv_results_['mean_fit_time'],\n        'mean_test_score_'+scoring: grid_search.cv_results_['mean_test_score'],\n        'ranking': grid_search.cv_results_['rank_test_score']\n         },\n        index=grid_search.cv_results_['params']).sort_values(by='ranking')\n    \n    print('The Best HyperParam to \"{}\" is {} with {} in {}'.format(\n        hyper_search_name, grid_search.best_params_[hyper_search_name], grid_search.best_score_, scoring))\n    \n    my_hyper_params[hyper_search_name] = grid_search.best_params_[hyper_search_name]\n    \n    \"\"\"\n    @@my_hyper_params: my_hyper_params appends best param find to @hyper_search_name\n    @@data_frame_results: dataframe with statistics of gridsearch: time, score and ranking\n    @@grid_search: grid serach object if it's necessary\n    \"\"\"\n    return my_hyper_params, data_frame_results, grid_search","ceb005b7":"best_hyper_params = {'random_state': 42, 'n_jobs': -1} # Stater Hyper Params","638fd65b":"search_hyper = {'penalty': ['l1', 'l2', 'elasticnet', 'none']}\n\nbest_hyper_params, results, last_grid_search = optimize_logistic_r(\n    x_train, y_train, best_hyper_params, search_hyper, 'penalty')","93c712ab":"search_hyper = {'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0 , 4.0, 8.0, 16.0, 32.0, 64.0]}\n\nbest_hyper_params, results, last_grid_search = optimize_logistic_r(\n    x_train, y_train, best_hyper_params, search_hyper, 'C')","d7c838dd":"search_hyper = {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nbest_hyper_params, results, last_grid_search = optimize_logistic_r(\n    x_train, y_train, best_hyper_params, search_hyper, 'solver')","5a816a70":"# last_grid_search\n\ny_pred = all_classifiers['LogisiticR'].predict(x_test)\nprint(accuracy_score(y_test, y_pred))\nclass_report(y_test, y_pred, name=\"LogisiticR\")\n\ny_pred = last_grid_search.predict(x_test)\nprint(accuracy_score(y_test, y_pred))\nclass_report(y_test, y_pred, name=\"LogisiticR0\")","124df4c3":"# all_classifiers['LogisiticR'].get_params()","82043d95":"from sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom mlens.ensemble import SuperLearner\n \n# create a list of base-models\ndef get_models():\n    models = list()\n    models.append(LogisticRegression(**best_hyper_params))\n    models.append(DecisionTreeClassifier())\n    models.append(XGBClassifier())\n    models.append(AdaBoostClassifier())\n    models.append(CatBoostClassifier(verbose=0))\n    models.append(RandomForestClassifier())\n    models.append(LGBMClassifier())\n    return models\n \n# create the super learner\ndef get_super_learner(X):\n    ensemble = SuperLearner(scorer=accuracy_score, folds=5, shuffle=True, sample_size=len(X), verbose=0)\n    # add base models\n    models = get_models()\n    ensemble.add(models)\n    # add the meta model\n    ensemble.add_meta(LogisticRegression(**best_hyper_params))\n    return ensemble","bd786e24":"import time\nt0 = time.time()\n\n# create the super learner\nensemble = get_super_learner(x_train.values)\n\n# fit the super learner\nensemble.fit(x_train.values, y_train)\n\n# summarize base learners\nprint(ensemble.data)\n\n# make predictions on hold out set\ny_pred = ensemble.predict(x_test.values)\n\nprint(\"took \", time_spent(t0))\nclass_report(y_test, y_pred, name=\"SuperLeaner\")\n\n# y_probs = ensemble.predict_proba(x_test.values)\n\n# roc_auc_score(y_test, y_probs)","ff757834":"y_pred = all_classifiers['LogisiticR'].predict(x_test)\nprint(accuracy_score(y_test, y_pred))\nclass_report(y_test, y_pred, name=\"LogisiticR\")","2e2a6b30":"!pip install pycaret","32769bdc":"from pycaret.classification import *","3aeab0b2":"df_pycaret = df.copy().drop(['customerID'],axis=1)\n\ndf_pycaret['Churn'] = df_pycaret['Churn'].replace(yes_no)\ndf_pycaret['SeniorCitizen'] = df_pycaret['SeniorCitizen'].replace({0: 'No', 1: 'Yes'})\ndf_pycaret.head(1)","de08b492":"categorical_features = [f for f in df_pycaret.columns if df_pycaret.dtypes[f] == 'object']\n# categorical_features","6c495042":"# from sklearn.model_selection import train_test_split\n\n# X = df1.drop(['Churn'], axis=1)\n\n# y = df1['Churn']\n\n# x_train, x_test, y_train, y_test = train_test_split(X, y.values, test_size=0.20, random_state=42)\n\n# # https:\/\/pycaret.org\/classification\/","f4905bc2":"df_pycaret_setup = setup(data = df_pycaret,\n                         target = 'Churn',\n                         numeric_imputation = 'mean',\n                         categorical_features = categorical_features, \n                         train_size = 0.80,\n                         session_id = 42,\n                         silent = True)","ed9b42ab":"compare_models()","4dcb987f":"lr_pycaret  = create_model('lr')     ","58a5755e":"plot_model(estimator = lr_pycaret, plot = 'learning')","8c7d18c7":"plot_model(estimator = lr_pycaret, plot = 'auc')","9972cf31":"plot_model(estimator = lr_pycaret, plot = 'confusion_matrix')","a7d80cf7":"plot_model(estimator = lr_pycaret, plot = 'feature')","d587a704":"## Problem Description\n\nhttps:\/\/www.kaggle.com\/blastchar\/telco-customer-churn","8631ad89":"## Split Train and Test","bedb39f3":"### Churn by other features","03f2a2b1":"## Table Of Content (TOC) <a id=\"top\"><\/a>","202d7d7a":"## EDA\n\n### Each feature individually","088ea219":"## Data Cleaning","d33d11ef":"## Conclusion\n\nThis kernel is not finished. If you think that it's useful, votes up the kernel","778cc1ec":"### Hyper Tuning Logistic R","6ecba517":"## Snippets","9a2c627d":"## Correlations","08e2c674":"## Handle Unbalanced DataSet","d5d9a411":"## pyCaret\n\nhttps:\/\/www.kaggle.com\/frtgnn\/pycaret-introduction-classification-regression","8c2310e5":"````\nAll Dumies\nCrossValidation, Fitting and Testing :: Done in 29.003 s\nNaiveBayes     | cv_acc: 0.6876 | acc_test: 0.6977 | f1_test: 0.6798 | took:         0.061 s |\nRidge          | cv_acc: 0.7993 | acc_test: 0.8233 | f1_test: 0.7575 | took:         0.068 s |\nPerceptron     | cv_acc: 0.7501 | acc_test: 0.4627 | f1_test: 0.4612 | took:         0.091 s |\nPassiveAggr    | cv_acc: 0.6734 | acc_test: 0.7473 | f1_test: 0.7162 | took:         0.110 s |\nXGBoost        | cv_acc: 0.7806 | acc_test: 0.7956 | f1_test: 0.7203 | took:         2.424 s |\nLightGB        | cv_acc: 0.7922 | acc_test: 0.8084 | f1_test: 0.7357 | took:         2.910 s |\nSVM            | cv_acc: 0.7345 | acc_test: 0.7353 | f1_test: 0.4237 | took:         7.705 s |\nLogisiticR     | cv_acc: 0.8023 | acc_test: 0.8226 | f1_test: 0.7620 | took:         0.503 s |\nKNearest       | cv_acc: 0.7597 | acc_test: 0.7771 | f1_test: 0.6945 | took:         0.515 s |\nAdaBoost       | cv_acc: 0.8000 | acc_test: 0.8141 | f1_test: 0.7476 | took:         1.960 s |\nGBoost         | cv_acc: 0.8008 | acc_test: 0.8091 | f1_test: 0.7380 | took:         4.865 s |\nRandomForest   | cv_acc: 0.7840 | acc_test: 0.7999 | f1_test: 0.7180 | took:         3.814 s |\nExtraTree      | cv_acc: 0.7758 | acc_test: 0.7871 | f1_test: 0.7013 | took:         3.974 s |\n\n\n````","1018838d":"<h1 align=\"center\"> Telcom Churn: Classify <\/h1>\n\n<img src=\"https:\/\/austindatascience.files.wordpress.com\/2017\/11\/screen-shot-2017-11-19-at-4-00-11-pm.png\" width=\"50%\" \/>\n\nCreated: 2020-09-18\n\nLast updated: 2020-09-19\n\nKaggle Kernel made by \ud83d\ude80 <a href=\"https:\/\/www.kaggle.com\/rafanthx13\"> Rafael Morais de Assis<\/a>","7e9b2130":"### Super Leaner","9ee450c4":"## Best Model"}}