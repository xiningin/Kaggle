{"cell_type":{"57949fda":"code","18fd724a":"code","41a3f6d7":"code","04363164":"code","14258a34":"code","6ea8e44a":"code","a50be6a5":"code","45e697b6":"code","12115f4c":"code","2fcfdc2f":"code","70d983ca":"code","a4c5047f":"code","fe702d40":"code","fb9a4c1e":"code","d80f3392":"code","e446d9ca":"code","d9688507":"code","fdcaa940":"code","43616c60":"code","36ec2668":"code","c9a23d80":"code","4de73db9":"code","5980d105":"code","2dbe889a":"code","89248931":"code","82744c2e":"markdown","fdc324fa":"markdown","b79114bd":"markdown","c3c740c7":"markdown","bd20f48c":"markdown","33713d2e":"markdown"},"source":{"57949fda":"import os\nimport time\nimport shutil\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom tqdm.notebook import tqdm\n\nimport albumentations\nfrom albumentations import *\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\nimport math\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\nimport seaborn as sns\n%matplotlib inline\nprint('Ready...')","18fd724a":"DATA_DIR  = '..\/input\/global-wheat-detection\/train\/'\nTEST_DIR  = '..\/input\/global-wheat-detection\/test\/'\ntrain_df_path = '..\/input\/global-wheat-detection\/train.csv'\ntest_df_path = '..\/input\/global-wheat-detection\/sample_submission.csv'\nList_Data_dir = os.listdir(DATA_DIR)","41a3f6d7":"raw = pd.read_csv(train_df_path)\nraw","04363164":"raw.describe()\n\n# all images have resolution 1024 x 1024","14258a34":"print(f'Total number of train images: {raw.image_id.nunique()}')\nprint(f'Total number of test images: {len(os.listdir(TEST_DIR))}')","6ea8e44a":"plt.figure(figsize=(15,8))\nplt.title('Wheat Distribution', fontsize= 20)\nsns.countplot(x=\"source\", data=raw)\n\n# based on the chart, there are seven types of wheat from images data, with the most types 'ethz_1' and the least is 'inrae_1'","a50be6a5":"# Extract bbox column to xmin, ymin, width, height, then create xmax, ymax, and area columns\n\nraw[['xmin','ymin','w','h']] = pd.DataFrame(raw.bbox.str.strip('[]').str.split(',').tolist()).astype(float)\nraw['xmax'], raw['ymax'], raw['area'] = raw['xmin'] + raw['w'], raw['ymin'] + raw['h'], raw['w'] * raw['h']\nraw","45e697b6":"def show_image(image_id):\n    \n    fig, ax = plt.subplots(1, 2, figsize = (24, 24))\n    ax = ax.flatten()\n    \n    bbox = raw[raw['image_id'] == image_id ]\n    img_path = os.path.join(DATA_DIR, image_id + '.jpg')\n    \n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n    image \/= 255.0\n    image2 = image\n    \n    ax[0].set_title('Original Image')\n    ax[0].imshow(image)\n    \n    for idx, row in bbox.iterrows():\n        x1 = row['xmin']\n        y1 = row['ymin']\n        x2 = row['xmax']\n        y2 = row['ymax']\n        label = row['source']\n        \n        cv2.rectangle(image2, (int(x1),int(y1)), (int(x2),int(y2)), (255,255,255), 2)\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        cv2.putText(image2, label, (int(x1),int(y1-10)), font, 1, (255,255,255), 2)\n    \n    ax[1].set_title('Image with Bondary Box')\n    ax[1].imshow(image2)\n\n    plt.show()","12115f4c":"show_image(raw.image_id.unique()[91])","2fcfdc2f":"show_image(raw.image_id.unique()[1231])","70d983ca":"show_image(raw.image_id.unique()[3121])","a4c5047f":"def get_bboxes(bboxes, col, bbox_format = 'pascal_voc', color='white'):\n    for i in range(len(bboxes)):\n        x_min = bboxes[i][0]\n        y_min = bboxes[i][1]\n        x_max = bboxes[i][2]\n        y_max = bboxes[i][3]\n        width = x_max - x_min\n        height = y_max - y_min\n        rect = patches.Rectangle((x_min, y_min), \n                                 width, height, \n                                 linewidth=2, \n                                 edgecolor=color, \n                                 facecolor='none')\n        col.add_patch(rect)","fe702d40":"def augmented_images(image, augment):\n    \n    fig, ax = plt.subplots(1, 2, figsize = (24, 24))\n    ax = ax.flatten()\n    \n    image_data = raw[raw['image_id'] == image]\n    bbox = image_data[['xmin', 'ymin', 'xmax', 'ymax']].astype(np.int32).values\n    labels = np.ones((len(bbox), ))\n\n    image = cv2.imread(os.path.join(DATA_DIR + '\/{}.jpg').format(image), cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n    image \/= 255.0\n    \n    get_bboxes(bbox, ax[0], color='white')\n    \n    ax[0].set_title('Original Image with Bounding Boxes')\n    ax[0].imshow(image)\n    \n    aug = albumentations.Compose([augment], \n                         bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\n    \n    aug_result = aug(image=image, bboxes=bbox, labels=labels)\n\n    aug_image = aug_result['image']\n    get_bboxes(aug_result['bboxes'], ax[1], color='red')\n    \n    ax[1].set_title('Augmented Image with Bounding Boxes')\n    ax[1].imshow(aug_image)\n    \n    plt.show()","fb9a4c1e":"# HorizontalFlip Augmentation\naugmented_images(raw.image_id.unique()[1230], albumentations.HorizontalFlip(p=1))","d80f3392":"# VerticalFlip Augmentation\naugmented_images(raw.image_id.unique()[2110], albumentations.VerticalFlip(p=1))","e446d9ca":"# Change Color to gray\naugmented_images(raw.image_id.unique()[1212], albumentations.ToGray(p=1))","d9688507":"# Random Change Brightness Contrast\naugmented_images(raw.image_id.unique()[1230], albumentations.RandomBrightnessContrast(p=1))","fdcaa940":"class wheatdataset_train(Dataset):\n       \n    def __init__(self, dataframe, data_dir, transforms=None):\n        super().__init__()\n        self.df = dataframe \n        self.image_list = list(self.df['image_id'].unique())\n        self.image_dir = data_dir\n        self.transforms = transforms\n    \n    def __len__(self):\n        return len(self.image_list)\n        \n    def __getitem__(self, idx):\n        \n        image_id = self.image_list[idx]\n        image_data = self.df.loc[self.df['image_id'] == image_id]\n        boxes = torch.as_tensor(np.array(image_data[['xmin','ymin','xmax','ymax']]), \n                                dtype=torch.float32)\n        area = torch.tensor(np.array(image_data['area']), dtype=torch.int64) \n        labels = torch.ones((image_data.shape[0],), dtype=torch.int64)\n        iscrowd = torch.zeros((image_data.shape[0],), dtype=torch.uint8)\n         \n        target = {}\n        target['boxes'] = boxes\n        target['area'] = area\n        target['labels'] = labels\n        target['iscrowd'] = iscrowd\n        \n        image = cv2.imread((self.image_dir + '\/' + image_id + '.jpg'), cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        \n        if self.transforms:\n            \n            image_transforms = {\n                                'image': image,\n                                'bboxes': target['boxes'],\n                                'labels': labels\n                                 }\n            \n            image_transforms = self.transforms(**image_transforms)\n            image = image_transforms['image']\n            \n            target['boxes'] = torch.as_tensor(image_transforms['bboxes'], dtype=torch.float32)\n                 \n        return image, target","43616c60":"# Albumentations\n\ndef get_train_transform():\n    return albumentations.Compose([\n        #albumentations.Resize(p=1, height=512, width=512),\n        albumentations.ToGray(p=0.5),\n        albumentations.Flip(p=0.5),\n        albumentations.RandomBrightnessContrast(p=0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n\ndef get_test_transform():\n    return albumentations.Compose([\n        ToTensorV2(p=1.0)\n    ])\n\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))","36ec2668":"train_data = wheatdataset_train(raw, DATA_DIR, get_train_transform())\ntrain_dataloader = DataLoader(train_data, batch_size=16,shuffle=True, num_workers=4,collate_fn=collate_fn)","c9a23d80":"len(train_data)","4de73db9":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ntorch.cuda.empty_cache()\nprint(device)","5980d105":"def train_model():\n    \n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    num_classes = 2  \n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    \n    return model\n\ndef train(data_loader, epoch):\n        \n    model = train_model()\n    model.to(device)\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n    model.parameters\n\n\n    total_train_loss = []\n    itr = 1\n    train_loss_threshold = math.inf\n\n    for epoch in tqdm(range(epoch)):\n        \n        print(f'Epoch :{epoch + 1}')\n        start_time = time.time()\n        train_loss = []\n        model.train()\n        for images, targets in tqdm(data_loader):\n            \n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            loss_dict = model(images, targets)\n\n            losses = sum(loss for loss in loss_dict.values())\n            \n            loss_value = losses.item()\n            \n            train_loss.append(losses.item())\n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n            \n            if itr % 50 == 0:\n                print(f\"Iteration #{itr} loss: {loss_value:.4f}\")\n\n            itr += 1\n    \n        \n        epoch_train_loss = np.mean(train_loss)\n        total_train_loss.append(epoch_train_loss)\n        print(f'Epoch train loss is {epoch_train_loss:.4f}')\n        time_elapsed = time.time() - start_time\n        print('{:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n        if epoch_train_loss < train_loss_threshold:\n            train_loss_threshold = epoch_train_loss\n            torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn_{0:.3f}.pth'.format(epoch_train_loss))\n\n    #visualize\n    plt.figure(figsize=(12,6))\n    plt.title('Train Loss', fontsize= 20)\n    plt.plot(total_train_loss)\n    plt.xlabel('epochs')\n    plt.ylabel('loss')\n    plt.show()","2dbe889a":"num_epochs = 17\ntrain(train_dataloader, num_epochs)","89248931":"# total = 0\n#     sum_loss = 0\n#     correct = 0 \n#     for images_val, targets_val, image_ids_val in valid_data_loader:\n#         images_val = list(image.to(device) for image in images_val)\n#         targets_val = [{k: v.to(device) for k, v in t.items()} for t in targets_val]\n\n#         loss_dict_val = model(images_val, targets_val)\n\n#         losses_val = sum(loss for loss in loss_dict_val.values())\n#         val_loss = losses_val.item()\n#     print(\"val_loss %.5f\"%(val_loss))\n#     if val_loss < pre_valid_loss:\n#         pre_valid_loss = val_loss\n#         torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn_{0:.3f}.pth'.format(val_loss))","82744c2e":"## Augmentations\n\n\n<p style=\"text-align:justify;\">Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. \nIn image data, augmentation can range from basic image manipulation like color variation, Fliping, resize, or rotate image, and data augmentation can also reduce overfitting.<p\/>","fdc324fa":"## Create a model and training","b79114bd":"## Data Preprocessing (Train Data)","c3c740c7":"## References\n\n\nEDA - Augmentations\n\n* https:\/\/github.com\/albumentations-team\/albumentations_examples\n\n* https:\/\/link.springer.com\/article\/10.1186\/s40537-019-0197-0\n\n\nPytorch - Model\n\n* https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html\n              \n* https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train\n \n* https:\/\/www.kaggle.com\/arunmohan003\/fasterrcnn-using-pytorch-baseline","bd20f48c":"## Load Data and Simple EDA","33713d2e":"**Let's look at some random images with boundary boxes**"}}