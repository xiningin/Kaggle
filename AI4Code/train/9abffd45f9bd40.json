{"cell_type":{"f044b912":"code","6d6119ca":"code","6369a219":"code","0544d6f5":"code","076f3df3":"code","9016912b":"code","863ac146":"code","c39acbfd":"code","ebe4b8c8":"code","2707b4fc":"code","99d8768c":"code","ad0c5509":"markdown","5a517bac":"markdown"},"source":{"f044b912":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d6119ca":"from sklearn.neighbors import kneighbors_graph\nimport time\nimport matplotlib.pyplot as plt\n\n_t00 = time.time()\n\n","6369a219":"try:\n    import cudf\n    from cuml.neighbors import NearestNeighbors\n    #from cuml.datasets import make_blobs    \nexcept:\n    print('GPU library cannot be imported. Turn ON GPU')","0544d6f5":"\n#distribution_type = 'Gauss' # 'uniform'\nverbose = 0\n\n\nplt.style.use('ggplot')\n\nfig = plt.figure(figsize=(20,12))\n\n#ax2 =     fig.add_subplot(2,1,2)\n\nt00=time.time()\n\ndf_stat = pd.DataFrame()\ndf_stat2 = pd.DataFrame()\n\nlist_save_results = []\nn_sample0 = 1e4\ni = 0\n#for dim , n_neighbors, n_sample  in [(2,5,1e4), (5,5,1e4),   (8,5,1e4),   (10,5,1e4), (50,5,1e4), (500,5,1e4)]: #  , (100,10,1e4)  , (10,5,1e4) , (10,5,1e5) ]:  \nfor distribution_type in ['Gauss', 'Uniform', 'Sphere', 'Torus' ]: #  ['Gauss']: # \n    i += 1\n    #ax1 =     \n    #fig.add_subplot(2,1,i)\n    fig = plt.figure(figsize=(20,8))\n\n    for dim , n_neighbors, n_sample  in [(100,5,n_sample0), (1000,5,n_sample0), (1000,5,10*n_sample0),  (10000,5,n_sample0),  ]: # , (50,10,10*n_sample0) , (50,10,100*n_sample0) ]: # ,(500,5,n_sample0),]:# [(2,5,n_sample0),     (10,5,n_sample0), (50,5,n_sample0),(500,5,n_sample0)]: \n                                          #  , (5,5,n_sample0) , (100,10,1e4)  , (10,5,1e4) , (10,5,1e5) ]:  \n        n_sample = int(n_sample)\n\n        df_stat_over_trials = pd.DataFrame()\n        for trial in range(100):\n            \n            if distribution_type == 'Gauss':\n                X = np.random.randn(n_sample,dim)\n                if verbose >= 100:\n                    print('Gauss')\n            elif distribution_type == 'Sphere':\n                X = np.random.randn(n_sample,dim+1)\n                s = np.sqrt( (X*X).sum(axis=1))\n                X = X \/ s[:,np.newaxis]\n                if verbose >= 100:\n                    print('Sphere')\n            elif distribution_type == 'Torus':\n                X = ( np.random.rand(n_sample,dim) )\n                X1 = np.sin(2*np.pi*X)\n                X2 = np.cos(2*np.pi*X)\n                X = np.concatenate((X1,X2),axis=1)\n                if verbose >= 100:\n                    print('Torus')\n            else:\n                X = np.random.uniform(-1,1, size = (n_sample,dim) )\n                if verbose >= 100:\n                    print('Uniform')    \n\n            for method in ['GPU']: # 'sklearn',  'GPU'\n\n                t0=time.time()\n\n                if method in ['GPU']:\n                    X_cudf = cudf.DataFrame(X)\n                    model = NearestNeighbors(n_neighbors=n_neighbors+1)\n                    model.fit(X)\n                    # get 3 nearest neighbors\n                    m = model.kneighbors_graph(X_cudf)#, include_self=False) #  no include_self\n                    vec_degs = np.sum(m,axis = 0 )\n                    vec_degs = vec_degs.get()\n\n                    vec_degs = np.squeeze(np.array(vec_degs.ravel()))\n                    vec_degs -= np.ones_like(vec_degs ) # no include_self - so that is a way round\n\n                else: #if method in ['sklearn']:\n                    m = kneighbors_graph(X, n_neighbors= n_neighbors, mode='connectivity' , include_self=False) #'distance' mode=  'connectivity'\n                    vec_degs = np.sum(m,axis = 0 )\n                    vec_degs = np.squeeze(np.array(vec_degs.ravel()))\n\n                #print(m.toarray())\n\n                bins = np.arange(200)# np.max(vec_degs))\n                h = np.histogram(vec_degs,bins = bins, density= True)\n\n                #label = method +' '+ distribution_type + ' dim'+str(dim) +' n_neighbors'+str(n_neighbors) + ' n_sample'+str(n_sample)\n                label = distribution_type + ' dim'+str(dim) +' n_neighbors'+str(n_neighbors) + ' n_sample'+str(n_sample)\n\n                list_save_results.append( (label, h) )\n\n                #plt.plot(h[0],'*-',label = label)\n                #plt.loglog(h[0],'*-',label = label)\n                #plt.legend(fontsize = 13)\n                #plt.xlabel('Degree')\n                #plt.ylabel('Probability')\n\n                degs = bins[:-1]\n                probs = h[0]\n                # Cut suitable subset to make linear interpolation (i.e. avoid noisy part) and take logs:   \n                m = (degs > 2*n_neighbors ) # & (degs < 90)\n                y = (probs[m])\n                x = (degs[m])\n                for t in range(len(y) ):\n                    if y[t] == 0:\n                        break\n                t = t-1\n                y = y[:t]\n                x = x[:t]\n                y = np.log10(y)\n                x = np.log10(x)\n\n                coefs_polyfit = np.polyfit(x, y, 1)\n                x2 =  np.log10(degs[degs > 0])\n                vec_line_intepolation_result = np.poly1d(coefs_polyfit)(x2)\n                if verbose >= 100:\n                    print(label, 'coefficients of line interpolation: ', coefs_polyfit )\n                m = h[0] > 1e-12\n                if trial <= 1:\n                    #plt.loglog(h[1][:-1][m], h[0][m],'*-', label = label, linewidth=4)\n                    plt.loglog( h[0][m],'*-', label = label, linewidth=4)\n                    plt.loglog(np.power(10,x2), np.power(10, vec_line_intepolation_result ), label = 'linear approx' )#  ,  label = label+' Approx', linewidth=4)\n                    plt.title('LogLog plot')\n                    plt.xlabel('degree')\n                    #plt.ylabel('Node count')\n                    plt.legend( fontsize = 12 )\n                    #plt.show()\n\n                    #ax1.grid()\n\n                    #ax2.loglog(h[0],'*-',label = label)\n                    #ax2.legend()\n                    #ax2.grid()\n\n                st = pd.Series(vec_degs).describe( percentiles = [0.05,0.1, .25, .5, .75, 0.9,0.95,0.99 ])\n                st.name = label + ' trial' + str(trial)\n                st['Linear slope'] = coefs_polyfit[0]\n                st['Linear constant'] = coefs_polyfit[1]\n                st['Seconds passed'] = np.round( time.time()-t0, 2)\n                df_stat = df_stat.join( st, how = 'outer'  )\n                df_stat_over_trials = df_stat_over_trials.join( st, how = 'outer'  )\n                \n                if verbose >= 100:\n                    print(label,  np.round(time.time()-t0,1),  np.round(time.time()-t00,1),'seconds passed trial, total')\n                \n        label = distribution_type + ' dim'+str(dim) +' n_neighbors'+str(n_neighbors) + ' n_sample'+str(n_sample)\n        st = df_stat_over_trials.mean(axis = 1)\n        st.name = label \n        df_stat2 = df_stat2.join( st, how = 'outer'  ) \n        st = df_stat_over_trials.std(axis = 1)\n        st.name = label + ' std' \n        df_stat2 = df_stat2.join( st, how = 'outer'  ) \n\nplt.show()\n    \nseconds_passed_total = time.time()-t00\nprint( np.round(seconds_passed_total,1),  np.round(seconds_passed_total\/60,1), np.round(seconds_passed_total\/3600,1), \n      'seconds, minutes, hours passed')\n\n#print(df_stat)\ndf_stat2","076f3df3":"print( np.round(seconds_passed_total,1),  np.round(seconds_passed_total\/60,1), np.round(seconds_passed_total\/3600,1), \n      'seconds, minutes, hours passed')\n","9016912b":"df_stat2","863ac146":"df_stat","c39acbfd":"df_stat2.to_csv('df_stat2.csv')\ndf_stat.to_csv('df_stat.csv')\n","ebe4b8c8":"if 0:\n    fig = plt.figure(figsize=(20,12))\n\n    for label,h in list_save_results:\n        plt.plot(h[0][h[0]>0],'*-', label = label)\n\n    plt.legend()\n    #plt.grid()\n    plt.show()","2707b4fc":"if 0:\n    fig = plt.figure(figsize=(20,12))\n\n    for label,h in list_save_results:\n        plt.loglog(h[0][h[0]>1e-12],'*-', label = label)\n\n    plt.legend()\n    #plt.grid()\n    plt.show()","99d8768c":"print(np.round(time.time()- _t00,1), np.round( (time.time()-_t00)\/60,1),  np.round( (time.time()-_t00)\/3600,1), 'seconds minutes hours total passed')\n","ad0c5509":"dim10 n_neighbors5 n_sample1000 0.1 0.1 seconds passed trial, total\n\ndim10 n_neighbors5 n_sample10000 1.5 1.6 seconds passed trial, total\n\ndim10 n_neighbors5 n_sample100000 66.1 67.7 seconds passed trial, total","5a517bac":"# What is about \n\nStudy degree distribution of KNN graphs for model data clouds - Gauss, uniform, sphere.\nFor high dimensions it seems to be  power law like. We also see drastical difference between sphere-torus vs Gauss-Uniform.\nSee discussion in : \nhttps:\/\/cstheory.stackexchange.com\/questions\/47957\/power-law-for-degree-distribution-of-random-knn-graphs\nsee also https:\/\/stats.stackexchange.com\/questions\/500250\/recognize-distribution-on-integers-0-mean-5-median-2-percentiles-75-5\n\nV20 - same as 18,19 - but simulate Gauss, Uniform, Sphere, Torus \n\nV19 - same as 18, but trials increase to 100, and dim 1000 repeated twice with n_sample 1e4,1e5,\n\nV18 - high dims explore: dim 100,1000,10000, K =5 n_sample = 1e4, Gauss \n\nV17 uniform , dim = 50, K = 10 \n\nV16 same with , dim = 10, K = 20  strange thing  - mean sometimes not K for big sample size - is not a bug in cuml ? \nfor dim 10 we see mean equals to median , while for dim 50 we saw mean was = percentile 75 \n\nV15 same with , dim = 10, K = 10\n\nV14 same with , dim = 10, K = 5\n\nV13: K = 20\n\nV12: repeat V11 for K = 10\n\nV11 - study distribution in details for particular case - K=5, dim = 50 , change n_samples, look at percentiles, power law etc..  Findings summarized here: https:\/\/stats.stackexchange.com\/questions\/500250\/recognize-distribution-on-integers-0-mean-5-median-2-percentiles-75-5\n\nV10 - same as V9 - increase samples to 10_000_000 - crashed by memory after processing first case \n\nV9 - same as V8 - increase samples to 1_000_000 - 150 seconds \n\nV8 - simulations for Sphere and Torus added. See drastical difference for sphere-torus VS Gauss-uniform. 100_000 samples - 7 seconds \n\n\nV7 - same as V6 , but n_sample = 1e6 - run out of time 9 hours\n\n\nV6 - linear interplotation of loglog plot - estimate exponent of power law of the distibution \n\nV5 - same as V4 but compare GPU and sklearn implementations - should coincide - indeed coincide \n\n\nV4 - pdf of degrees K=5, various dimensions , n_sample = 1e4\n\nV3: plot pdf for :\n\nGauss dim50 n_neighbors3 n_sample100000 2276.7 2276.7 seconds passed trial, total\n\nGauss dim50 n_neighbors10 n_sample100000 2294.3 4571.1 seconds passed trial, total\n\nV2: plot pdf for :\n\nGauss dim10 n_neighbors5 n_sample100000 73.2 73.2 seconds passed trial, total\n\nGauss dim50 n_neighbors5 n_sample100000 2313.5 2386.7 seconds passed trial, total\n\nV1 - fast calculation:\n\nGauss dim10 n_neighbors5 n_sample10000 1.5 1.5 seconds passed trial, total\n\nGauss dim50 n_neighbors5 n_sample10000 11.9 13.4 seconds passed trial, total\n"}}