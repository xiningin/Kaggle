{"cell_type":{"1bb55e82":"code","d13f3c71":"code","1b8b8c63":"code","2da893aa":"code","ada4dead":"code","5f96564e":"code","bb0b56e8":"code","a64e8da9":"code","acb5d930":"code","89f46c99":"code","5127266a":"code","760d201b":"code","a9a061f5":"code","dfe993b8":"code","5fe031b9":"code","30eba51a":"code","5624d904":"code","ef836acf":"code","387f6327":"code","54bd2661":"code","d942bc7c":"code","194a10be":"code","4ae47ba8":"code","1cfc6c00":"code","8328a25e":"code","c9b6e927":"code","d2afabe8":"code","e16b67f9":"code","c218ccad":"code","8012af54":"code","1a7bbf7a":"code","d124d80d":"code","45cbfa9f":"code","7cab1f27":"code","34a6b501":"code","ea5134a5":"code","08bfb2d6":"code","604f2d6f":"code","efe0d926":"code","b6d49c2c":"code","a096b55d":"code","0cec4a39":"code","534ca834":"code","5e68a4cc":"code","0670ed9d":"code","bf7c694c":"code","07812a20":"code","8fe9e564":"code","19b97564":"code","3097ab8b":"code","65a2fd88":"code","4e949b4e":"code","b3bb03ef":"code","c5a6ce4c":"code","9a986501":"code","183adce5":"code","538e53e4":"code","edc5d464":"code","ae347f19":"code","f6f3cf9c":"code","c0ca9552":"code","9f2fd8ad":"code","af59dcde":"code","260a6dfa":"code","fa454eb9":"code","f3129351":"code","eeb9c895":"code","4c6b2aa9":"code","e1c8e81b":"code","4ae1b132":"code","9d705282":"code","a9b12d78":"code","8810bca8":"code","f66c1565":"code","5dc9336c":"code","46294b1b":"code","80837fea":"code","4bfa56a2":"code","95effbc2":"code","fc778947":"code","77640abc":"markdown","e13abbd3":"markdown","9701fc82":"markdown","a11c9948":"markdown","f4ea4d9b":"markdown","a6793ed8":"markdown","b3c677d0":"markdown","45ed9bdd":"markdown","9814366d":"markdown","36fbdb87":"markdown","ceb706e6":"markdown","15d42932":"markdown","bb27f68f":"markdown","5339100e":"markdown","69171164":"markdown","f0c78f15":"markdown","a5555091":"markdown","5354fdc1":"markdown","8c33e3df":"markdown","fc658760":"markdown","dce78e38":"markdown","763b5dfc":"markdown","c2060848":"markdown","0b2e2c88":"markdown","f4681b21":"markdown","a946cf11":"markdown","b165d8af":"markdown","89f0d223":"markdown","e43b8b43":"markdown","9e6894bf":"markdown","d60eaea4":"markdown","e1682be0":"markdown","ede7d19e":"markdown","4a08614f":"markdown","da0a39a2":"markdown","6229af5b":"markdown","fa1c30bb":"markdown","80837992":"markdown","7f095471":"markdown","f6528ed5":"markdown","d8603a86":"markdown","2cfe2485":"markdown","9efa39dc":"markdown","a5feaad1":"markdown","ed349fc9":"markdown","d938a47e":"markdown","283e4245":"markdown","f177ce45":"markdown","1016bba7":"markdown","bef321e3":"markdown","c84bf945":"markdown","e00d26ba":"markdown","7812abe5":"markdown","af83624c":"markdown","533684e4":"markdown","126bf6e9":"markdown","49212f09":"markdown","33e7ed3d":"markdown","5777f4f9":"markdown","ddec2de9":"markdown","32f809ce":"markdown","10ce7106":"markdown","a719304d":"markdown","9f1e1867":"markdown","05e2ddeb":"markdown","dc9adc0e":"markdown","0ae9df27":"markdown","f28ad0fe":"markdown","5c5c5559":"markdown","faf9f590":"markdown","db9395b2":"markdown","f32fa31a":"markdown","b637616c":"markdown","d0d9a3cf":"markdown","78d6f324":"markdown","8b33e411":"markdown","8dbc4e28":"markdown","86f33d23":"markdown","5a5e7187":"markdown","fb8b3950":"markdown","90531098":"markdown","febad13b":"markdown","9c28970f":"markdown","2f0df74c":"markdown","51a0adb2":"markdown","2309635e":"markdown","d204238a":"markdown","61e58ccd":"markdown","21291287":"markdown","a0fd3c08":"markdown","b9880615":"markdown","fc300f74":"markdown","e2ad0920":"markdown","339833a6":"markdown","8c7cd49b":"markdown","40d21d89":"markdown","0848fb3f":"markdown","28a9ba4e":"markdown"},"source":{"1bb55e82":"import matplotlib.pyplot as plt # visualization\nimport numpy as np\nimport pandas as pd\nimport random\nimport seaborn as sns # visualization\nfrom statsmodels.graphics.gofplots import qqplot # visualization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# just to jupyter notebook be capable of execute matplotlib\n%matplotlib inline\n\nfrom pandas.plotting import register_matplotlib_converters # avoid erros between pandas and matplotlib\nregister_matplotlib_converters()","d13f3c71":"# Importing dataset from UCI Machine Learning Repository\ncars = pd.read_csv('..\/input\/automobile-dataset\/Automobile_data.csv')\n\n# Seeing the first 5 rows\ncars.head()","1b8b8c63":"# Seeing dataset structure\ncars.info()","2da893aa":"# seeing dataset shape\ncars.shape","ada4dead":"# Seeing columns\ncars.describe()","5f96564e":"# Looking for total missing values in each column\ncars.isna().sum()","bb0b56e8":"# Creating a list with the possible characters that are replacing missing values\nmissing_values = ['?','--','-','??','.']\n\n# Importing dataset from UCI Machine Learning Repository\ncars = pd.read_csv('..\/input\/automobile-dataset\/Automobile_data.csv',na_values = missing_values)\n\n# Seeing the first 5 rows\ncars.head()","a64e8da9":"# Verifying continuos variables\ncars.describe()","acb5d930":"# Ploting missing values percentage in each dataset\nplt.subplots(0,0, figsize = (18,5))\nax = (cars.isnull().sum()).sort_values(ascending = False).plot.bar(color = 'blue')\nplt.title('Missing values per column', fontsize = 20);","89f46c99":"# Interpolating a linear regression to replace missing values in continuos variables\ncars['normalized-losses'] = cars['normalized-losses'].interpolate(method = \"linear\"\n                                      ,limit_direction = \"both\")\n\ncars['price'] = cars['price'].interpolate(method = \"linear\"\n                                      ,limit_direction = \"both\")\n\ncars['stroke'] = cars['stroke'].interpolate(method = \"linear\"\n                                      ,limit_direction = \"both\")\n\ncars['bore'] = cars['bore'].interpolate(method = \"linear\"\n                                      ,limit_direction = \"both\")\n\ncars['peak-rpm'] = cars['peak-rpm'].interpolate(method = \"linear\"\n                                      ,limit_direction = \"both\")\n\ncars['horsepower'] = cars['horsepower'].interpolate(method = \"linear\"\n                                      ,limit_direction = \"both\")","5127266a":"# Counting number of missing values in num_of_doors\ncars['num-of-doors'].isna().sum()","760d201b":"# Looking what body_style and make our missing values have\ncars[['make','body-style']][cars['num-of-doors'].isnull()==True]","a9a061f5":"# Seeing how many doors a mazda sedan have\ncars['num-of-doors'][(cars['body-style']=='sedan') & (cars['make']=='mazda')]","dfe993b8":"# Seeing how many doors a dodge sedan have\ncars['num-of-doors'][(cars['body-style']=='sedan') & (cars['make']=='dodge')]","5fe031b9":"# Replacing missing values into num_of_doors\ncars['num-of-doors'] = cars['num-of-doors'].fillna('four')","30eba51a":"# Verifying missing alues\ncars.isna().sum()","5624d904":"# Plotting a histogram of our feature price\nplt.figure(figsize=(8,6)) # creating the figure\nplt.hist(cars['price'] # plotting the histogram\n         ,bins=30 # defyning number of bars\n         ,label='price' # add legend\n        ,color='blue') # defyning the color\n\nplt.xlabel('price') # add xlabel\nplt.ylabel('frequency') # add ylabel\nplt.legend()\nplt.title('price distribution');","ef836acf":"# Saving numerical features\nnum_var = ['symboling','normalized-losses','wheel-base','length'\n          ,'width','height','curb-weight','engine-size','bore'\n           ,'stroke','compression-ratio','horsepower','peak-rpm'\n           ,'city-mpg','highway-mpg']\n\n# plotting a histogram for each feature\ncars[num_var].hist(bins=10\n                   , figsize=(50,30)\n                   , layout=(4,4));","387f6327":"# Numerical variables correlation\ncorr = cars.corr() # creting the correlation matrix\n\nplt.figure(figsize=(12,12)) # creating the and difyning figure size\nax = sns.heatmap( # plotting correlation matrix\n    corr,vmin=-1, vmax=1, center=0,\n    annot=True,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels( # adding axes values\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","54bd2661":"# Plotting a scatter plot of relation between engine_size and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['engine-size'] # plotting\n         ,cars['price'],'.'\n        ,color='blue')\n\nplt.xlabel('engine-size') # add xlabel\nplt.ylabel('price') # add ylabel\nplt.title('Relation between engine-size and price');","d942bc7c":"# Plotting a scatter plot of relation between horsepower and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['horsepower'] # plotting\n         ,cars['price'],'.'\n        ,color='blue')\n\nplt.xlabel('horsepower') # add xlabel\nplt.ylabel('price') # add ylabel\nplt.title('Relation between horsepower and price');","194a10be":"# Plotting a scatter plot of relation between curb_weight and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['curb-weight'] # plotting\n         ,cars['price'],'.'\n        ,color='blue')\n\nplt.xlabel('curb-weight')\nplt.ylabel('price')\nplt.title('Relation between curb-weight and price');","4ae47ba8":"# Plotting a scatter plot of relation between length and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['length'] # plotting\n         ,cars['price'],'.'\n        ,color='blue')\n\nplt.xlabel('length')\nplt.ylabel('price')\nplt.title('Relation between length and price');","1cfc6c00":"# Plotting a scatter plot of relation between width and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['width'] # plotting\n         ,cars['price'],'.'\n        ,color='blue')\n\nplt.xlabel('width')\nplt.ylabel('price')\nplt.title('Relation between width and price');","8328a25e":"# Plotting Distribution of maker into price\nplt.figure(figsize=(24,8))\nsns.boxplot(x='make',y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('Distribution of make into price');","c9b6e927":"# Plotting Distribution of num_of_cylinders into price\nplt.figure(figsize=(15,6))\nsns.boxplot(x='num-of-cylinders',y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('Distribution of num-of-cylinders into price');","d2afabe8":"# Plotting Distribution of fuel_system into price\nplt.figure(figsize=(15,6))\nsns.boxplot(x='fuel-system',y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('Distribution of fuel-system into price');","e16b67f9":"# Plotting Distribution of engine_type into price\nplt.figure(figsize=(15,6))\nsns.boxplot(x='engine-type',y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('Distribution of engine-type into price');","c218ccad":"# Plotting Distribution of body_style into price\nplt.figure(figsize=(12,6))\nsns.boxplot(x='body-style',y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('Distribution of body-style into price');","8012af54":"# Plotting Distribution of drive_wheels into price\nplt.figure(figsize=(12,6))\nsns.boxplot(x='drive-wheels',y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('Distribution of drive-wheels into price');","1a7bbf7a":"# Plotting Distribution of engine_location into price\nplt.figure(figsize=(10,6))\nsns.boxplot(x='engine-location',y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('Distribution of engine-location into price');","d124d80d":"# Plotting Distribution of fuel_type into price\nplt.figure(figsize=(10,6))\nsns.boxplot(x='fuel-type',y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('Distribution of fuel-type into price');","45cbfa9f":"# Plotting Distribution of aspiration into price\nplt.figure(figsize=(10,6))\nsns.boxplot(x='aspiration',y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('Distribution of aspiration into price');","7cab1f27":"# Plotting Distribution of num_of_doors into price\nplt.figure(figsize=(10,6))\nsns.boxplot(x='num-of-doors',y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('Distribution of num-of-doors into price');","34a6b501":"# Plotting boxplots to numeric features\nnum_var = ['normalized-losses','wheel-base','length'\n          ,'width','height','engine-size','bore'\n           ,'stroke','compression-ratio','horsepower'\n           ,'city-mpg','highway-mpg']\n\nplt.figure(figsize=(20,10))\nsns.boxplot(data=cars[num_var], \n                 palette=\"colorblind\")\nplt.title('Numerical features outlayers');","ea5134a5":"# creating a for to replace outlayers using boxplot method\nfor i in num_var:\n    # taking quantiles\n    Q1 = cars[i].quantile(0.25)\n    Q3 = cars[i].quantile(0.75)\n    IQR = Q3 - Q1 # calculating IQR\n    cars[i] = np.where(cars[i]>(Q3+1.5*IQR),(Q3+1.5*IQR),cars[i]) # removing outlayers\n    cars[i] = np.where(cars[i]<(Q1-1.5*IQR),(Q1-1.5*IQR),cars[i]) # removing outlayers","08bfb2d6":"# Plotting boxplots to numeric features\nnum_var = ['normalized-losses','wheel-base','length'\n          ,'width','height','engine-size','bore'\n           ,'stroke','compression-ratio','horsepower'\n           ,'city-mpg','highway-mpg']\n\nplt.figure(figsize=(20,10))\nsns.boxplot(data=cars[num_var], \n                 palette=\"colorblind\")\nplt.title('Numerical features outlayers');","604f2d6f":"# Plotting a boxplot of our target to visualize outlayers\nplt.figure(figsize=(6,6))\nsns.boxplot(y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('price outlayers');","efe0d926":"# Replace outlayers using boxplot method\nQ1 = cars['price'].quantile(0.25) # taking Q1\nQ3 = cars['price'].quantile(0.75) # taking Q3\nIQR = Q3 - Q1 # calculating IQR\ncars['price'] = np.where(cars['price']>(Q3+1.5*IQR),(Q3+1.5*IQR),cars['price']) # removing outlayers\ncars['price'] = np.where(cars['price']<(Q1-1.5*IQR),(Q1-1.5*IQR),cars['price']) # removing outlayers","b6d49c2c":"# Visualizing if outlayers was removed\nplt.figure(figsize=(6,6))\nsns.boxplot(y='price',data=cars, \n                 palette=\"colorblind\")\nplt.title('price outlayers');","a096b55d":"# Quantile-Quantile Plot to virify normal distributions\nqqplot(cars['price'], line='s')\nplt.title('Quantile-Quantile Plot')\nplt.show()","0cec4a39":"# Log-transformation of the target variable\ncars['price'] = np.log1p(cars['price'])\n\n# Quantile-Quantile Plot to virify normal distributions\nqqplot(cars['price'], line='s')\nplt.title('Quantile-Quantile Plot')\nplt.show()","534ca834":"# Converting num_of_cylinders into a continuos variable\ncars['num-of-cylinders'][cars['num-of-cylinders']=='two'] = 2\ncars['num-of-cylinders'][cars['num-of-cylinders']=='three'] = 3\ncars['num-of-cylinders'][cars['num-of-cylinders']=='four'] = 4\ncars['num-of-cylinders'][cars['num-of-cylinders']=='five'] = 5\ncars['num-of-cylinders'][cars['num-of-cylinders']=='six'] = 6\ncars['num-of-cylinders'][cars['num-of-cylinders']=='eight'] = 8\ncars['num-of-cylinders'][cars['num-of-cylinders']=='twelve'] = 12\n\n# converting into integer\ncars['num-of-cylinders'] = cars['num-of-cylinders'].astype('int64')","5e68a4cc":"# Plotting a scatter plot of relation between num_of_cylinders and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['num-of-cylinders'] # plotting\n         ,cars['price'],'.'\n        ,color='blue')\n\nplt.xlabel('num-of-cylinders')\nplt.ylabel('price')\nplt.title('Relation between num-of-cylinders and price');","0670ed9d":"# taking numerical features to log trasformation\nnum_var = ['normalized-losses','wheel-base','length'\n          ,'width','height','curb-weight','engine-size','bore'\n           ,'stroke','compression-ratio','horsepower','peak-rpm'\n           ,'city-mpg','highway-mpg','num-of-cylinders']\n\ncars[num_var] = np.log1p(cars[num_var]) # log transformation","bf7c694c":"# plotting a histogram for each feature\ncars[num_var].hist(bins=10\n                   , figsize=(50,30)\n                   , layout=(4,4));","07812a20":"# relation between length and width\ncars['len_wid'] = cars['length']\/cars['width']\n\n# relation between wheel_base and curb_weight\ncars['whb_c_wght'] = cars['wheel-base']\/cars['curb-weight']\n\n# relation between horsepower and engine_size\ncars['hpw_eng_size'] = cars['horsepower']\/cars['engine-size']\n\n# relation between highway_mpg and city_mpg\ncars['hway_cit_mpg'] = cars['highway-mpg']\/cars['city-mpg']","8fe9e564":"# Saving new features\nnew_feat1 = ['len_wid','whb_c_wght','hpw_eng_size','hway_cit_mpg']\n\n# plotting a histogram for each feature\ncars[new_feat1].hist(bins=15\n                   , figsize=(20,4)\n                   , layout=(1,4));","19b97564":"# creating a feature to represent the mean mpg\ncars['mean_mpg'] = (cars['highway-mpg']+cars['city-mpg'])\/2\n\n# creating a feature to represent the horsepower per cylinders\ncars['hpw_cylinders'] = cars['horsepower']\/cars['num-of-cylinders']\n\n# creating a feature to represent the mean mpg per horsepower\ncars['mean_mpg_hpw'] = cars['mean_mpg']\/cars['horsepower']","3097ab8b":"# Saving new features\nnew_feat2 = ['mean_mpg','hpw_cylinders','mean_mpg_hpw']\n\n# plotting a histogram for each feature\ncars[new_feat2].hist(bins=15\n                   , figsize=(20,5)\n                   , layout=(1,3));","65a2fd88":"# Converting engine_location into binary\ncars['engine-location'] = np.where(cars['engine-location']=='front',2,1)\n\n# Converting fuel_type into binary\ncars['fuel-type'] = np.where(cars['fuel-type']=='gas',2,1)\n\n# Converting aspiration into binary\ncars['aspiration'] = np.where(cars['aspiration']=='std',2,1)\n\n# Converting num_of_doors into binary\ncars['num-of-doors'] = np.where(cars['num-of-doors']=='two',2,1)","4e949b4e":"# converting maker to continuos based on boxplots\ncars['make'][cars['make']=='chevrolet'] = 1\ncars['make'][cars['make']=='renault'] = 2\ncars['make'][cars['make']=='isuzu'] = 3\ncars['make'][cars['make']=='subaru'] = 4\ncars['make'][cars['make']=='plymouth'] = 5\ncars['make'][cars['make']=='dodge'] = 6\ncars['make'][cars['make']=='honda'] = 7\ncars['make'][cars['make']=='volkswagen'] = 8\ncars['make'][cars['make']=='mitsubishi'] = 9\ncars['make'][cars['make']=='alfa-romero'] = 10\ncars['make'][cars['make']=='mercury'] = 11\ncars['make'][cars['make']=='toyota'] = 12\ncars['make'][cars['make']=='peugot'] = 13\ncars['make'][cars['make']=='mazda'] = 14\ncars['make'][cars['make']=='saab'] = 15\ncars['make'][cars['make']=='nissan'] = 16\ncars['make'][cars['make']=='volvo'] = 17\ncars['make'][cars['make']=='audi'] = 18\ncars['make'][cars['make']=='jaguar'] = 19\ncars['make'][cars['make']=='porsche'] = 20\ncars['make'][cars['make']=='bmw'] = 21\ncars['make'][cars['make']=='mercedes-benz'] = 22\ncars['make']=cars['make'].astype('int64')","b3bb03ef":"# Plotting a scatter plot of relation between width and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['make'] # plotting\n         ,cars['price'],'.'\n        ,color='purple')\n\nplt.xlabel('make')\nplt.ylabel('price')\nplt.title('Relation between make and price');","c5a6ce4c":"# converting fuel_system to continuos based on boxplots\ncars['fuel-system'][cars['fuel-system']=='1bbl'] = 1\ncars['fuel-system'][cars['fuel-system']=='spfi'] = 2\ncars['fuel-system'][cars['fuel-system']=='2bbl'] = 3\ncars['fuel-system'][cars['fuel-system']=='mfi'] = 4\ncars['fuel-system'][cars['fuel-system']=='4bbl'] = 5\ncars['fuel-system'][cars['fuel-system']=='spdi'] = 6\ncars['fuel-system'][cars['fuel-system']=='idi'] = 7\ncars['fuel-system'][cars['fuel-system']=='mpfi'] = 8\ncars['fuel-system']=cars['fuel-system'].astype('int64')","9a986501":"# Plotting a scatter plot of relation between width and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['fuel-system'] # plotting\n         ,cars['price'],'.'\n        ,color='purple')\n\nplt.xlabel('fuel-system')\nplt.ylabel('price')\nplt.title('Relation between fuel-system and price');","183adce5":"# converting engine_type to continuos based on boxplots\ncars['engine-type'][cars['engine-type']=='rotor'] = 1\ncars['engine-type'][cars['engine-type']=='l'] = 2\ncars['engine-type'][cars['engine-type']=='dohcv'] = 3\ncars['engine-type'][cars['engine-type']=='dohc'] = 4\ncars['engine-type'][cars['engine-type']=='ohcf'] = 5\ncars['engine-type'][cars['engine-type']=='ohc'] = 6\ncars['engine-type'][cars['engine-type']=='ohcv'] = 7\ncars['engine-type']=cars['engine-type'].astype('int64')","538e53e4":"# Plotting a scatter plot of relation between width and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['engine-type'] # plotting\n         ,cars['price'],'.'\n        ,color='purple')\n\nplt.xlabel('engine-type')\nplt.ylabel('price')\nplt.title('Relation between engine_type and price');","edc5d464":"# converting body_style to continuos based on boxplots\ncars['body-style'][cars['body-style']=='hatchback'] = 1\ncars['body-style'][cars['body-style']=='wagon'] = 2\ncars['body-style'][cars['body-style']=='convertible'] = 4\ncars['body-style'][cars['body-style']=='sedan'] = 3\ncars['body-style'][cars['body-style']=='hardtop'] = 5\ncars['body-style']=cars['body-style'].astype('int64')","ae347f19":"# Plotting a scatter plot of relation between width and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['body-style'] # plotting\n         ,cars['price'],'.'\n        ,color='purple')\n\nplt.xlabel('body-style')\nplt.ylabel('price')\nplt.title('Relation between body-style and price');","f6f3cf9c":"# converting drive_wheels to continuos based on boxplots\ncars['drive-wheels'][cars['drive-wheels']=='4wd'] = 1\ncars['drive-wheels'][cars['drive-wheels']=='fwd'] = 2\ncars['drive-wheels'][cars['drive-wheels']=='rwd'] = 3\ncars['drive-wheels']=cars['drive-wheels'].astype('int64')","c0ca9552":"# Plotting a scatter plot of relation between width and price\nplt.figure(figsize=(10,6)) # creating the figure\nplt.plot(cars['drive-wheels'] # plotting\n         ,cars['price'],'.'\n        ,color='purple')\n\nplt.xlabel('drive-wheels')\nplt.ylabel('price')\nplt.title('Relation between drive-wheels and price');","9f2fd8ad":"# Plotting boxplots to numeric features\nnum_var = ['mean_mpg','hpw_cylinders','mean_mpg_hpw'\n           ,'len_wid','whb_c_wght','hpw_eng_size','hway_cit_mpg']\n\nplt.figure(figsize=(20,10))\nsns.boxplot(data=cars[num_var], \n                 palette=\"colorblind\")\nplt.title('Numerical features outlayers');","af59dcde":"# creating a for to replace outlayers using boxplot method\nfor i in num_var:\n    Q1 = cars[i].quantile(0.25)\n    Q3 = cars[i].quantile(0.75)\n    IQR = Q3 - Q1\n    cars[i] = np.where(cars[i]>(Q3+1.5*IQR),(Q3+1.5*IQR),cars[i])\n    cars[i] = np.where(cars[i]<(Q1-1.5*IQR),(Q1-1.5*IQR),cars[i])","260a6dfa":"# Plotting boxplots to numeric features\nnum_var = ['mean_mpg','hpw_cylinders','mean_mpg_hpw'\n           ,'len_wid','whb_c_wght','hpw_eng_size','hway_cit_mpg']\n\nplt.figure(figsize=(20,10))\nsns.boxplot(data=cars[num_var], \n                 palette=\"colorblind\")\nplt.title('Numerical features outlayers');","fa454eb9":"# taking numerical features to log trasformation\nnum_var = ['mean_mpg','hpw_cylinders','mean_mpg_hpw'\n           ,'len_wid','whb_c_wght','hpw_eng_size','hway_cit_mpg'\n          ,'make','drive-wheels','body-style','engine-type','fuel-system'\n          ,'engine-location','fuel-type','aspiration','num-of-doors']\n\ncars[num_var] = np.log1p(cars[num_var]) # log transformation","f3129351":"# Numerical variables correlation\ncars_noprice = cars.drop('price',axis=1) # removing price column\n\ncorr = cars_noprice.corr() # creting the correlation matrix\n\nplt.figure(figsize=(20,20)) # creating the and difyning figure size\nax = sns.heatmap( # plotting correlation matrix\n    corr,vmin=-1, vmax=1, center=0,\n    annot = True,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels( # adding axes values\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","eeb9c895":"# creating the correlation matrix\ncorr_matrix = cars_noprice.corr().abs()\n\n# creating a mask to apply to our correlation matrix and filter high correlations\nmask = np.triu(np.ones_like(corr_matrix,dtype=bool))\n\n# replacing low correlations with NA's\ntri_df = corr_matrix.mask(mask)\n\n# selecting features to dropp that have correlation with each other above 0.80\nto_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.8)]\n\n# dropping high correlated features\ncars = cars.drop(to_drop,axis=1)","4c6b2aa9":"price = cars['price'] #saving prices\ncars = cars.drop('price',axis=1) # dropping prices from cars dataset\ncars['price'] = price # joing prices into our dataset again","e1c8e81b":"# Numerical variables correlation\ncorr = cars.corr() # creting the correlation matrix\n\nplt.figure(figsize=(20,20)) # creating the and difyning figure size\nax = sns.heatmap( # plotting correlation matrix\n    corr,vmin=-1, vmax=1, center=0,\n    annot = True,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels( # adding axes values\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","4ae1b132":"price = cars['price'] #saving prices\ncars = cars.drop('price',axis=1) # dropping prices from cars dataset\ncars['price'] = price # joing prices into our dataset again","9d705282":"# splitting the data with our target into y1 and the rest of data into x1\nx1 = cars.drop('price', axis=1)\ny1 = cars['price']","a9b12d78":"# splitting our dataset into train and target\nx_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size=0.7,random_state=42) # 70% train and 30% test","8810bca8":"# training the model\nregression = LinearRegression()\nregression.fit(x_train1, y_train1)","f66c1565":"# predicting on train dataset\ny_pred_train1 = regression.predict(x_train1)\ny_pred_train1 = np.exp(y_pred_train1)","5dc9336c":"# predicting on test dataset\ny_pred_test1 = regression.predict(x_test1)\ny_pred_test1 = np.exp(y_pred_test1)","46294b1b":"# removing log transformation from our target\ny_train1_exped = np.exp(y_train1)\ny_test1_exped = np.exp(y_test1)","80837fea":"# model quality metrics in train dataset prediction\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_train1_exped, y_pred_train1))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_train1_exped, y_pred_train1)))","4bfa56a2":"# model quality metrics in test dataset prediction\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test1_exped, y_pred_test1))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test1_exped, y_pred_test1)))","95effbc2":"print(\"The mean price of our test dataset is: \")\nprint(y_test1_exped.mean()) # calculating the mean of our car prices\nprint()\nprint(\"The MAE percentage in relation to mean price of our test dataset is: \")\nprint(round(metrics.mean_absolute_error(y_test1_exped, y_pred_test1)\/y_test1_exped.mean()*100,2)) # calculating mae ratio","fc778947":"# draw the figure to recieve a plot\nplt.figure(figsize=(14,8))\n\n# creating a scatter plot with seaborn\nsns.scatterplot(x=y_pred_test1, y=y_test1_exped,hue=y_test1_exped);","77640abc":"As we can see some continuos features are converted to strings, let's deal with that latter","e13abbd3":"As we can see in our exploratory analysis many of our features have a linear relation with our target.\nThinking in that, let's create our first model using **Linear Regression**.","9701fc82":"Apparently everything it's okay with missing values, let's plot a bar graph with na's percentage","a11c9948":"Now let's verify if missing values was replaced correctly","f4ea4d9b":"### Exploratory Analysis","a6793ed8":"Clearly our target have not a normal distribution, let's deal with that latter in feature engginering, let's take a look into all numerical distributions","b3c677d0":"num_of_cylinders are represented as categorys but it's a continuos feature, let's convert","45ed9bdd":"###### Normalizing our numerical features wigh Log-transformation","9814366d":"### About Dataset:","36fbdb87":"### Modeling","ceb706e6":"This dataset it's a popular dataset to study regression tecnics from UCI Machine Learning Repository that can be found here: http:\/\/archive.ics.uci.edu\/ml\/datasets\/Automobile\n\nThis data set consists of three types of entities: \n\n(a) the specification of an auto in terms of various characteristics, \n\n(b) its assigned insurance risk rating, \n\n(c) its normalized losses in use as compared to other cars.\n\nThe second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process \"symboling\". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n\nThe third factor is the relative average loss payment per insured vehicle year.  This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports\/speciality, etc...), and represents the average loss per car per year.","15d42932":"First of all let's try to remove some of high correlations creating new features with relation between high correlated features","bb27f68f":"<img src='https:\/\/i.imgur.com\/btxdIWH.png' style='width:1000px;height:550px'\/>","5339100e":"### Dealing with missing values","69171164":"The feature 'num_of_doors' represent the number of doors(two, four) but are described as a string object, let's investigate more to replace missing values in this feature","f0c78f15":"engine_size, horsepower, curb_weight, length and width have a high correlation with car prices, let's make a scatter plot for each of this features to understand better this relation","a5555091":"Cars with fwd and 4wd drive wheels tends to be more sheep and rwd cars more expensive","5354fdc1":"### Importing and cleaning our dataset","8c33e3df":"First of all let's treat our target 'price'","fc658760":"### Loading Required Librarys","dce78e38":"Let's see distributions of this new features","763b5dfc":"###### Creating new features","c2060848":"A good distribution but we have also some outlayers, let's see now curb_weight vs price","0b2e2c88":"Here if we exclude the outlayers we can clarly see that cars with more high prices have ohcv or dohcv engines","f4681b21":"Let's take a look into length feature vs prices","a946cf11":"### Conclusion","b165d8af":"Only 2 missing values, let's make a simple analysis by cars make and body_style and try to infer how many doors this cars have","89f0d223":"The features above seems to be okay, let's verify missing values:","e43b8b43":"We can create a boxplot for each numerical feature to visualize outlayers and use this boxplots to remove then using Boxplot Interquartile Range(IQR) method.","9e6894bf":"Apparently a great result with MAE and RMSE, now let's see if our model was capable of generalize in test dataset","d60eaea4":"Here I use boxplots of categorical features in Exploratory analysis step to convert categorical features into continuos features using boxplot distribution, i use range of prices that this categorys was distributed to give then a level of importance, lower levels of importance means that this category can predict low prices more accurattly and high levels of importance means that this category can predict high prices more accurattly, the importance levels was distributed like as 1,2,3,4,5...","e1682be0":"<img src='https:\/\/i.imgur.com\/PKSoO8R.png' style='width:600px;height:350px'\/>","ede7d19e":"Linear regression consists of finding the best-fitting straight line through the points. The best-fitting line is called a regression line. The blue lineis  the regression line and consists of the predicted score on weight for each height. The vertical lines from the points to the regression line represent the errors of prediction. As you can see, the black points is very near the regression line; its error of prediction is small.\n\nWith a linear regression model we can use this regression line that is given by y= a + b * x to make predictions of new weights based on already knew height values.\n\nIn this case we have a simple linear regression, in our model i'll use a multiple linear regression(that have more then 1 variable in x axes) to predict car prices.","4a08614f":"Now that we have removed high correlated features we have less features to process and we this we also can remove chances of overfitting","da0a39a2":"As we can see many or our numerical features have outlayers, let's remove then","6229af5b":"Cars with diesel have high prices but exist some outlayers cars with gas that have a high price and we can work with that in feature engineering","fa1c30bb":"Now let's use log trasformation to solve some problemas with features that are not normal distributed","80837992":"Now that we already cleaned our data and treated missing values, let's explore our dataset to understand and get insights from it","7f095471":"###### Normalizing new features wigh log transformation","f6528ed5":"Perfect, was we can see hpw_cylinders and another features have outlayes, let's remove then using IQR boxplot method","d8603a86":"Now let's do a correlation analysis again to remove high correlated features","2cfe2485":"None of our numerical features have a normal distribution, let's deal with that latter in feature engginering, now let's a look in each numerical feature correlation with our target and with each other","9efa39dc":"Now let's put our target in the last column and go to modeling","a5feaad1":"###### But how linear regression works?","ed349fc9":"IQR is simple the diference between Q3(25th percentile) and Q1(75th percentile), values above (Q3+1.5 * IQR) and values below (Q1-1.5 * IQR) are considered outlayers, in a boxplot we can find all this values and use then to remove outlayers from our data.","d938a47e":"Now let's predict car prices in train dataset and latter in test dataset to verify model stabilitty","283e4245":"###### Model Selection","f177ce45":"Clearly the makers mercedes-benz, porsche, jaguar and bmw have the high prices, with this information we can create a new feature with this makers","1016bba7":"<img src='https:\/\/i.imgur.com\/QA8v36F.png' style='width:1000px;height:550px'\/>","bef321e3":"We was right, the missing values was replaced by another characters, let's solve that importing our\ndataset again passing now the argument \"na_values\" inside pd.read_csv() with the possible characters that\nare replacing missing values\n\nWhen we remove this characters from our dataset the problem with continuos as strings will be solved","c84bf945":"Now let's see this feature relation with prices","e00d26ba":"###### Dealing with outlayers in new features","7812abe5":"Now let's see how this our new correlation matrix but first let's but our target 'price' in the last column","af83624c":"We will convert this feature to continuos latter but we already can see that much more cylinders, more high prices, cars with eight and twelve cylinders shows that in boxplot","533684e4":"Now, to evaluate our model we need to use to matrics: Mean Absolute Error(MAE) and Root Mean Squared Error(RMSE), let's see this metrics in train and test dataset predictions","126bf6e9":"Now let's create some features that have a create relation with each other, like mpg and horsepower","49212f09":"Maker and fuel_system seens to have a good 'linear' relation with our target, latter let's do a correlation analysis to verify this","33e7ed3d":"Dealing with outlayers in our target","5777f4f9":"Let's visualize the new numerical distributions","ddec2de9":"Seeing new features distributions","32f809ce":"Here we can see that cars prices above 30000 are outlayers, let's remove then","10ce7106":"Not perfect, but much better, now we have values above 10 and below 9 more close to the central line, let's treat another features with the same tecnics","a719304d":"As we can see we have a car that are a sedan from dodge make and we have a sedan from mazda make, let's see how many doors each of this car types have","9f1e1867":"Perfect! Now let's do a log transformation with the new features","05e2ddeb":"We clearly have some features that are highly correlated with each other, now we have a great number of features, let's select and remove then using a high correlation matrix","dc9adc0e":"###### Evaluating our model with Mae Ratio metrics","0ae9df27":"Now let's convert some binary categorical features into continuos using boxplot distribution related to car prices","f28ad0fe":"###### But what is IQR?","5c5c5559":"A beautiful distribution, with linear relation","faf9f590":"Verifying if outlayers was replaced","db9395b2":"As we can see this type of cars have four doors, let's replace missing values in this feature with 'four'","f32fa31a":"Very similar with all another high correlated features, i'll need to remove some of this features latter do avoid overfitting  in our model","b637616c":"First of all let's take a look into our target: 'price'","d0d9a3cf":"Not a great diference from train to test predictions, to be sure that our model are good let's look at the Mae Ratio","78d6f324":"Now let's verifying if outlayers was removed","8b33e411":"Now we can see clearly the missing values proportion, normalized_losses have more missing values, let's deal with this feature first:","8dbc4e28":"###### Removing high correlated features","86f33d23":"Here we need to be sure that new features will have not outlayers, let's do a boxplot for each feature to see this","5a5e7187":"Apparently this dataset have \"?\" in place of missing values, how to solve that? Let's investigate!","fb8b3950":"Not soo linear, latter in feature engginering i'll deal with unnormal distribution of target to solve some problems","90531098":"###### Dealing with outlayers in numeric features","febad13b":"# How much cost a car?","9c28970f":"Now with the first outlayers removed, target treated and our first continuos features normalized let's create new features, treat outlayers, do log transformation and do a correlation analysis to remove high correlated features","2f0df74c":"Now its perfect, let's normalize our numerical features using log transformation","51a0adb2":"We have high prices cars with two and four doors, represented into boxplot as outlayers","2309635e":"###### Let's make a bivariate a analysis of each categorical feature with our target to understand better this features","d204238a":"Here we can see that some outlayers cars with mpfi fuel system have high prices but all other fuel system types are all custing less then 25000","61e58ccd":"Mae Ratio is take the MAE percentage in relation to mean price of our test dataset, this metrics it's called MAE RATIO, with this metric we can measure how far our predictions was from the mean of our target, the lower our MAE RATIO is, better our model is, the best scenario it's a model with a MAE RATIO with less then 10.","21291287":"First of all let's **split our dataset into train and test**, we need to do this do avoid overfitting in our model and evaluate our model performance, to do this we need to separaty and target from rest of features","a0fd3c08":"Cars with turbo tends to have more high prices but exist outlayers with std with high prices too","b9880615":"To answer our question in this notebook title, let's take a look into scatter plot below, most of our cars in our dataset cost between 5000 and 2000 dollars, only a feel cost between 20000 and 30000 dollars, our model seems to be more accuraty to predict prices of sheep cars, as we can see cars with values above 20000 are more scattered in our plot.","fc300f74":"After cleaning, explore and modelling our data we are finelly here, the conclusion! The model has a Mae Ratio of 15.26, not perfect but a good model, with a medium error and can be used to evaluate cars with a good accuracy, our target have not a good distribution and even after outlayers removal and log transformation still not good enough,certainly we can archieve a better results working more on feature engineering to create feature with more prediction power. Lastly we can also test another regression models like gradient boosting and try to improve results without overfitting.","e2ad0920":"### Approach:\n\nHow much cost a car? Determined to answer this question let's performe the following steps to understand our data and be capable of create a regression model to evaluate cars of all makers and models.\n\n- Loading Required Librarys\n- Importing and cleaning our dataset\n- Dealing with missing values\n- Exploratory Analysis\n- Feature Engineering\n- Modeling\n- Conclusion","339833a6":"Clearly we have more sheep cars that are hatchback, sedan or wagon, only a feel outlayers sedan cars have high prices","8c7cd49b":"### Feature Engineering","40d21d89":"Clearly we have not a normal distribution in our target, let's use the numpy fuction log1p which  applies log(1+x) to all elements of the column to handdle with that","0848fb3f":"As we already see above in boxplots, cars with more cylinders aparently tends to be more expensive","28a9ba4e":"All cars with the engine located into rear have high prices"}}