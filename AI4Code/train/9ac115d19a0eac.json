{"cell_type":{"84f0f649":"code","3e3f2f55":"code","2f979a26":"code","73aba37d":"code","8d7c0405":"code","76a5d9c3":"code","5ce83184":"code","f17d9792":"code","c2f26223":"code","f83877c3":"code","225407be":"code","752eb124":"markdown","8ec4ee63":"markdown","2438fd50":"markdown","6f87e67d":"markdown","e9e78c9d":"markdown","bdeed8c1":"markdown","6c45c499":"markdown","27c609be":"markdown","66dce3d6":"markdown","01713130":"markdown"},"source":{"84f0f649":"import numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display as display_fn\nfrom IPython.display import Image, clear_output","3e3f2f55":"def tensor_to_image(tensor):\n    \"\"\"converts a tensor to an image\"\"\"\n    tensor_shape = tf.shape(tensor)\n    number_elem_shape = tf.shape(tensor_shape)\n    if number_elem_shape > 3:\n        assert tensor_shape[0] == 1\n        tensor = tensor[0]\n    return tf.keras.preprocessing.image.array_to_img(tensor)\n\n\ndef load_img(path_to_img):\n    \"\"\"loads an image as a tensor and scales it to 512 pixels\"\"\"\n    max_dim = 512\n    image = tf.io.read_file(path_to_img)\n    image = tf.image.decode_jpeg(image)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    shape = tf.shape(image)[:-1]\n    shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    image = tf.image.resize(image, new_shape)\n    image = image[tf.newaxis, :]\n    image = tf.image.convert_image_dtype(image, tf.uint8)\n\n    return image\n\n\ndef load_images(content_path, style_path):\n    \"\"\"loads the content and path images as tensors\"\"\"\n    content_image = load_img(\"{}\".format(content_path))\n    style_image = load_img(\"{}\".format(style_path))\n\n    return content_image, style_image\n\n\ndef imshow(image, title=None):\n    \"\"\"displays an image with a corresponding title\"\"\"\n    if len(image.shape) > 3:\n        image = tf.squeeze(image, axis=0)\n\n    plt.imshow(image)\n    if title:\n        plt.title(title)\n\n\ndef show_images_with_objects(images, titles=[]):\n    \"\"\"displays a row of images with corresponding titles\"\"\"\n    if len(images) != len(titles):\n        return\n\n    plt.figure(figsize=(20, 12))\n    for idx, (image, title) in enumerate(zip(images, titles)):\n        plt.subplot(1, len(images), idx + 1)\n        plt.xticks([])\n        plt.yticks([])\n        imshow(image, title)\n\n\ndef clip_image_values(image, min_value=0.0, max_value=255.0):\n    \"\"\"clips the image pixel values by the given min and max\"\"\"\n    return tf.clip_by_value(image, clip_value_min=min_value, clip_value_max=max_value)\n\n\ndef preprocess_image(image):\n    \"\"\"centers the pixel values of a given image to use with VGG-19\"\"\"\n    image = tf.cast(image, dtype=tf.float32)\n    image = tf.keras.applications.vgg19.preprocess_input(image)\n\n    return image","2f979a26":"content_path = '..\/input\/simpsons-faces\/cropped\/1.png'\nstyle_path = '..\/input\/abstract-art-gallery\/Abstract_gallery\/Abstract_gallery\/Abstract_image_100.jpg'\n\ncontent_image, style_image = load_images(content_path, style_path)\nshow_images_with_objects([content_image, style_image], titles=['content image', 'style image'])","73aba37d":"style_layers = [\n    \"block1_conv1\",\n    \"block2_conv1\",\n    \"block3_conv1\",\n    \"block4_conv1\",\n    \"block5_conv1\",\n]\ncontent_layers = [\"block5_conv2\"]\noutput_layers = style_layers + content_layers\n\nNUM_CONTENT_LAYERS = len(content_layers)\nNUM_STYLE_LAYERS = len(style_layers)","8d7c0405":"def nst_model(layer_names):\n    \"\"\"Creates a vgg model that outputs the style and content layer activations.\n\n    Args:\n      layer_names: a list of strings, representing the names of the desired content and style layers\n\n    Returns:\n      A model that takes the regular vgg19 input and outputs just the content and style layers.\n\n    \"\"\"\n    vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights=\"imagenet\")\n    vgg.trainable = False\n    outputs = [vgg.get_layer(name).output for name in layer_names]\n    model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n\n    return model\n","76a5d9c3":"K.clear_session()\nmodel = nst_model(output_layers)\nmodel.summary()","5ce83184":"tf.keras.utils.plot_model(model, expand_nested=True, show_shapes=True)","f17d9792":"def get_style_loss(features, targets):\n    \"\"\"Expects two images of dimension h, w, c\n\n    Args:\n      features: tensor with shape: (height, width, channels)\n      targets: tensor with shape: (height, width, channels)\n\n    Returns:\n      style loss (scalar)\n    \"\"\"\n    style_loss = tf.reduce_mean(tf.square(features - targets))\n\n    return style_loss\n\n\ndef get_content_loss(features, targets):\n    \"\"\"Expects two images of dimension h, w, c\n\n    Args:\n      features: tensor with shape: (height, width, channels)\n      targets: tensor with shape: (height, width, channels)\n\n    Returns:\n      content loss (scalar)\n    \"\"\"\n    content_loss = 0.5 * tf.reduce_sum(tf.square(features - targets))\n\n    return content_loss\n\ndef gram_matrix(input_tensor):\n    \"\"\"Calculates the gram matrix and divides by the number of locations\n    Args:\n      input_tensor: tensor of shape (batch, height, width, channels)\n\n    Returns:\n      scaled_gram: gram matrix divided by the number of locations\n    \"\"\"\n\n    gram = tf.linalg.einsum(\"bijc,bijd->bcd\", input_tensor, input_tensor)\n    input_shape = tf.shape(input_tensor)\n    height = input_shape[1]\n    width = input_shape[2]\n    num_locations = tf.cast(height * width, tf.float32)\n    scaled_gram = gram \/ num_locations\n\n    return scaled_gram\n\n\ndef get_style_content_loss(\n    style_targets,\n    style_outputs,\n    content_targets,\n    content_outputs,\n    style_weight,\n    content_weight,\n):\n    \"\"\"Combine the style and content loss\n\n    Args:\n      style_targets: style features of the style image\n      style_outputs: style features of the generated image\n      content_targets: content features of the content image\n      content_outputs: content features of the generated image\n      style_weight: weight given to the style loss\n      content_weight: weight given to the content loss\n\n    Returns:\n      total_loss: the combined style and content loss\n\n    \"\"\"\n    style_loss = tf.add_n(\n        [\n            get_style_loss(style_output, style_target)\n            for style_output, style_target in zip(style_outputs, style_targets)\n        ]\n    )\n    content_loss = tf.add_n(\n        [\n            get_content_loss(content_output, content_target)\n            for content_output, content_target in zip(content_outputs, content_targets)\n        ]\n    )\n    style_loss = style_loss * style_weight \/ NUM_STYLE_LAYERS\n    content_loss = content_loss * content_weight \/ NUM_CONTENT_LAYERS\n    total_loss = style_loss + content_loss\n\n    return total_loss\n","c2f26223":"def get_style_image_features(image, model):\n    \"\"\"Get the style image features\n\n    Args:\n      image: an input image\n\n    Returns:\n      gram_style_features: the style features as gram matrices\n    \"\"\"\n    preprocessed_style_image = preprocess_image(image)\n    outputs = model(preprocessed_style_image)\n    style_outputs = outputs[:NUM_STYLE_LAYERS]\n    gram_style_features = [gram_matrix(style_layer) for style_layer in style_outputs]\n\n    return gram_style_features\n\n\ndef get_content_image_features(image, model):\n    \"\"\"Get the content image features\n\n    Args:\n      image: an input image\n\n    Returns:\n      content_outputs: the content features of the image\n    \"\"\"\n    preprocessed_content_image = preprocess_image(image)\n    outputs = model(preprocessed_content_image)\n    content_outputs = outputs[NUM_STYLE_LAYERS:]\n    return content_outputs\n","f83877c3":"def update_image_with_style(\n    image,\n    style_targets,\n    content_targets,\n    style_weight,\n    var_weight,\n    content_weight,\n    optimizer,\n):\n    \"\"\"\n    Args:\n      image: generated image\n      style_targets: style features of the style image\n      content_targets: content features of the content image\n      style_weight: weight given to the style loss\n      content_weight: weight given to the content loss\n      var_weight: weight given to the total variation loss\n      optimizer: optimizer for updating the input image\n    \"\"\"\n    gradients = calculate_gradients(\n        image, style_targets, content_targets, style_weight, content_weight, var_weight\n    )\n    optimizer.apply_gradients([(gradients, image)])\n    image.assign(clip_image_values(image, min_value=0.0, max_value=255.0))\n\n\ndef fit_style_transfer(\n    style_image,\n    content_image,\n    style_weight=1e-2,\n    content_weight=1e-4,\n    var_weight=0,\n    optimizer=\"adam\",\n    epochs=1,\n    steps_per_epoch=1,\n):\n    \"\"\"Performs neural style transfer.\n    Args:\n      style_image: image to get style features from\n      content_image: image to stylize\n      style_targets: style features of the style image\n      content_targets: content features of the content image\n      style_weight: weight given to the style loss\n      content_weight: weight given to the content loss\n      var_weight: weight given to the total variation loss\n      optimizer: optimizer for updating the input image\n      epochs: number of epochs\n      steps_per_epoch = steps per epoch\n\n    Returns:\n      generated_image: generated image at final epoch\n      images: collection of generated images per epoch\n    \"\"\"\n\n    images = []\n    step = 0\n    style_targets = get_style_image_features(style_image, model)\n    content_targets = get_content_image_features(content_image, model)\n    generated_image = tf.cast(content_image, dtype=tf.float32)\n    generated_image = tf.Variable(generated_image)\n    images.append(content_image)\n    for n in range(epochs):\n        for m in range(steps_per_epoch):\n            step += 1\n            update_image_with_style(\n                generated_image,\n                style_targets,\n                content_targets,\n                style_weight,\n                content_weight,\n                var_weight,\n                optimizer,\n            )\n\n            print(\".\", end=\"\")\n\n            if (m + 1) % 10 == 0:\n                images.append(generated_image)\n\n        clear_output(wait=True)\n        display_image = tensor_to_image(generated_image)\n        display_fn(display_image)\n\n        images.append(generated_image)\n        print(\"Train step: {}\".format(step))\n    generated_image = tf.cast(generated_image, dtype=tf.uint8)\n\n    return generated_image, images\n\n\ndef calculate_gradients(\n    image, style_targets, content_targets, style_weight, content_weight, var_weight\n):\n    \"\"\"Calculate the gradients of the loss with respect to the generated image\n    Args:\n      image: generated image\n      style_targets: style features of the style image\n      content_targets: content features of the content image\n      style_weight: weight given to the style loss\n      content_weight: weight given to the content loss\n      var_weight: weight given to the total variation loss\n\n    Returns:\n      gradients: gradients of the loss with respect to the input image\n    \"\"\"\n    with tf.GradientTape() as tape:\n        style_features = get_style_image_features(image, model)\n        content_features = get_content_image_features(image, model)\n        loss = get_style_content_loss(\n            style_targets,\n            style_features,\n            content_targets,\n            content_features,\n            style_weight,\n            content_weight,\n        )\n        loss += var_weight * tf.image.total_variation(image)\n    gradients = tape.gradient(loss, image)\n\n    return gradients\n","225407be":"style_weight = 1e-4\ncontent_weight = 1e-32\nvar_weight = 1e-2\n\nadam = tf.optimizers.Adam(\n    tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=30.0, decay_steps=100, decay_rate=0.90\n    )\n)\n\nstylized_image_reg, display_images_reg = fit_style_transfer(\n    style_image=style_image,\n    content_image=content_image,\n    style_weight=style_weight,\n    content_weight=content_weight,\n    var_weight=var_weight,\n    optimizer=adam,\n    epochs=10,\n    steps_per_epoch=100,\n)","752eb124":"# Generate the Styled Images","8ec4ee63":"# Define the loss functions\n\nWe will define functions to compute the losses required for generating the new image. These would be the:\n\n* Style loss: the average of the squared differences between the features and targets\n* Content loss: the sum of the squared error between the features and targets, then multiplied by a scaling factor (0.5)\n* Total loss: combination of style and content loss $L_{total} = \\beta L_{style} + \\alpha L_{content}$","2438fd50":"# Reference\n[A Neural Algorithm of Artistic Style](https:\/\/arxiv.org\/pdf\/1508.06576.pdf)","6f87e67d":"<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1xNii3cDPob5cX8QpXPu3S3ps8s9O5X15\" width=\"75%\" height=\"75%\"\/>","e9e78c9d":"## Utilities","bdeed8c1":"# Neural Style Transfer","6c45c499":"# Build the model \n\nWe will be using the VGG-19 model as the feature extractor. We will feed in the style and content image and depending on the computed losses, a new image will be generated which has elements of both the content and style image.\n\n- For the style layers, we will use the first layer of each convolutional block.\n\n- For the content layer, we will use the second convolutional layer of the last convolutional block (just one layer)\n\nWe will define a model to take the same input as the standard VGG-19 model, and output just the selected content and style layers.","27c609be":"# Style Transfer","66dce3d6":"## Images","01713130":"# Get Content and Style Features"}}