{"cell_type":{"082853f7":"code","ae13ec64":"code","1208e4cb":"code","9a827a1a":"code","ce1f6a21":"code","9f77d733":"code","01767d60":"code","1291bc90":"code","10ebc523":"code","7543c7e1":"code","1675de5c":"code","0512c349":"code","3fd98f5f":"code","55db8b07":"code","8c6ee1e6":"code","c416f53f":"code","0cd9d4c3":"code","14fceb60":"code","c74c7528":"code","e662c3f5":"code","bba17bb3":"code","46512406":"code","980cfc67":"code","8f47e781":"code","b38c5ae0":"code","b7975630":"code","5b15fcf9":"code","02e49427":"code","2c0e8b71":"code","68ac483c":"code","1b6d768e":"code","239f3f51":"code","cc27f440":"code","5ee64cc0":"code","a8ae8c3e":"code","c184fac6":"code","3ed2f82c":"code","6be640f2":"code","ac432d68":"code","29720dae":"code","885c4ee7":"code","589eddbb":"code","9258b192":"code","c661abdf":"code","89834c8a":"code","b4487b33":"code","7d74e140":"code","5cfcec93":"code","ae693352":"code","76f77c37":"code","92013e22":"code","a6c32c39":"code","e17b6b13":"code","2916211a":"code","79b6aed6":"code","156ad468":"code","a9736d90":"code","f7c4b544":"code","327c37df":"code","cdf5b105":"code","6c05f3e3":"code","a6dd981d":"code","ab2f58c8":"code","1f8df743":"markdown","75010ff5":"markdown","c07f99bf":"markdown","b0a3badc":"markdown","ba0198c0":"markdown","623f7650":"markdown","83f5654f":"markdown"},"source":{"082853f7":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport datetime\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ae13ec64":"from sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin","1208e4cb":"#!pip install xgboost","9a827a1a":"# read in files\nraw_train_identity  = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\nraw_train_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")\nraw_test_identity  = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_identity.csv\")\nraw_test_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_transaction.csv\")","ce1f6a21":"# reduce file memory of train_identity, train_transaction, test_identity, and test_transaction \n# credit to @Tharindu Gangoda: https:\/\/www.kaggle.com\/tharug\/ieee-fraud-detection\ndef memory_reduction(df):\n    # check original dataframe usage\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    # reduce all numeric columns to a smaller data type\n    for col in df.columns:\n        col_type = df[col].dtype.name\n        if col_type not in ('object', 'category'):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n        else:\n            pass\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","9f77d733":"# reduce original data memory\nraw_test_identity = memory_reduction(raw_test_identity)\nraw_test_transaction = memory_reduction(raw_test_transaction)\nraw_train_transaction = memory_reduction(raw_train_transaction)\nraw_train_identity = memory_reduction(raw_train_identity)","01767d60":"# preview raw_train_transaction \nraw_train_transaction.head()","1291bc90":"# create a bar graph to visualize the distribution of fraud vs. non fraud transactions\nplt.figure(figsize=(12,6))\nplt.title('Fraud Transaction Distribution')\nax=sns.countplot(x='isFraud', data =raw_train_transaction)\nplt.ylabel(\"Transaction Count\")\nfor p in ax.patches:\n             ax.annotate(str(round(p.get_height()\/len(raw_train_transaction)*100,2))+\"%\", (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                 ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),\n                 textcoords='offset points')\n","10ebc523":"# we examine whether the continous variable TransactionAMT has normal distribution or not by plotting a histogram \nplt.figure(figsize=(12,6))\nsns.distplot(raw_train_transaction['TransactionAmt'])\nplt.title(\"TransactionAmt Distribution \")\nplt.ylabel(\"Probability Density\")\nplt.show()","7543c7e1":"plt.figure(figsize=(12,6))\nsns.distplot(raw_train_transaction['TransactionAmt'].apply(np.log))\nplt.title(\"Log Transformation of TransactionAmt Distribution \")\nplt.ylabel(\"Probability Density\")\nplt.show()","1675de5c":"# defind a function to visualize the percentage of fraud transactions in each categorical variables \ndef find_percentage(df,variable_name,fig_size,rotate):\n    # create empty list to sore percentage \n    find_percent=[]\n    # go through all the columns \n    for i in variable_name:\n        # create a dictionary\n        percent=dict()\n        # fill na with 'NA'\n        df[i] = df[i].fillna(\"NA\")\n        # go through each category\n        for j in sorted(df[i].unique()):\n            # fraud number\n            fraud_num = len(df[(df[i]==j) & (df['isFraud']==1)])\n            # not fraud number\n            total_num = len(df[df[i]==j])\n            # round to 2 decimal\n            temp = dict([(j,round((fraud_num\/total_num*100),2))])\n            # add new key &value\n            percent.update(temp)\n        plt.figure(figsize=fig_size)\n        plt.title(\"Number of Transactions for \" + str(i)) \n        ax=sns.countplot(x=i,data=df,order=sorted(df[i].unique()))\n        plt.ylabel(\"Transaction Count\")\n        # add value label\n        for p in ax.patches:\n            ax.annotate(str(round(p.get_height()\/len(raw_train_transaction)*100,2))+\"%\", (p.get_x() + p.get_width() \/ 2., p.get_height()),\n            ha='center', va='center', fontsize=11, color='black', xytext=(0, 5), textcoords='offset points')\n        plt.xticks(rotation=rotate)\n        plt.figure(figsize=fig_size)\n        plt.title(\"Fraud Transaction Percentage for \" + str(i))\n        ax=sns.barplot(x=list(percent.keys()),\n            y=list(percent.values()))        \n        plt.xticks(range(len(percent)), list(percent.keys()))\n        plt.xticks(rotation=rotate)\n        plt.xlabel(i)\n        plt.ylabel(\"Fraud Transaction Percentage of Each Category\")\n        # add value label\n        for p in ax.patches:\n            ax.annotate(str(p.get_height())+'%', (p.get_x() + p.get_width() \/ 2., p.get_height()),\n            ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),\n            textcoords='offset points')\n        find_percent.append(percent)\n","0512c349":"# find out the number of transactions for each category of ProductCD variable\n# find out the fraud transaction percentage of total transactions for each category of ProductCD variable\nfig_size =(12,6)\nrotation=0\nfind_percentage(raw_train_transaction,['ProductCD'],fig_size,rotation)","3fd98f5f":"# check the distribution for continuous card1 variable \nplt.figure(figsize=(12,6))\nsns.distplot(raw_train_transaction[raw_train_transaction['isFraud']==0]['card1'])\nsns.distplot(raw_train_transaction[raw_train_transaction['isFraud']==1]['card1'])\nplt.legend(labels=['Not Fraud','Fraud'])\nplt.show()","55db8b07":"# find out the number of transactions for each category of card4 variable\n# find out the fraud transaction percentage of total transactions for each category of card4 variable\nfind_percentage(raw_train_transaction,['card4'],fig_size,rotation)","8c6ee1e6":"# find out the number of transactions for each category of card6 variable\n# find out the fraud transaction percentage of total transactions for each category of card6 variable\nfind_percentage(raw_train_transaction,['card6'],fig_size,rotation)","c416f53f":"# create list to store M1-M9 variable names\nM_cols = [\"M\"+str(i) for i in np.arange(1, 10, 1)]","0cd9d4c3":"# find out the number of transactions for each category of M1 to M9 variables\n# find out the fraud transaction percentage of total transactions for each category of M1 to M9 variables\nfind_percentage(raw_train_transaction,M_cols,fig_size,rotation)","14fceb60":"# visualize the distribution of each addr1 category\nplt.figure(figsize=(12,6))\nraw_train_transaction['addr1'].plot(kind='hist',bins=80)\nplt.xticks(np.arange(min(raw_train_transaction['addr1']), max(raw_train_transaction['addr1'])+1, 20))\nplt.show()","c74c7528":"# visualize the distribution of each addr2 category\nplt.figure(figsize=(12,6))\nraw_train_transaction['addr2'].plot(kind='hist',bins=80)\nplt.xticks(np.arange(min(raw_train_transaction['addr2']), max(raw_train_transaction['addr2'])+1, 5))\nplt.show()","e662c3f5":"# In order to visualize more deailed insights of add1 variable, we select the top 20 transaction count add1 categories \ntop20_addr1 = raw_train_transaction[\"addr1\"].value_counts().head(20).index\ntop20_addr1 = raw_train_transaction[raw_train_transaction['addr1'].isin(top20_addr1)]","bba17bb3":"# find out the number of transactions for addr1 categories with the top 20 transaction count \n# find out the fraud transaction percentage of total transactions for addr1 categories with the top 20 transaction count\nfig_size=(22,6)\nfind_percentage(top20_addr1,[\"addr1\"],fig_size,rotation)","46512406":"# In order to visualize more deailed insights of add1 variable, we selecte the top 20 transaction count add2 categories \ntop20_addr2 = raw_train_transaction[\"addr2\"].value_counts().head(20)\ntop20_addr2 = raw_train_transaction[raw_train_transaction['addr2'].isin(top20_addr2)]","980cfc67":"# find out the number of transactions for addr2 categories with the top 20 transaction count \n# find out the fraud transaction percentage of total transactions for addr2 categories with the top 20 transaction count \nfind_percentage(top20_addr2,[\"addr2\"],fig_size,rotation)","8f47e781":"# select the P_emaildomain categories with the top 20 transaction count\ntop20_P_email = raw_train_transaction[\"P_emaildomain\"].value_counts().head(20).index\ntop20_P_email = raw_train_transaction[raw_train_transaction['P_emaildomain'].isin(top20_P_email)]","b38c5ae0":"# find out the number of transactions for P_emaildomain categories with the top 20 transaction count \n# find out the fraud transaction percentage of total transactions for P_emaildomain categories with the top 20 transaction count \nfig_size=(24,6)\nfind_percentage(top20_P_email,['P_emaildomain'],fig_size,rotation)","b7975630":"# select the R_emaildomain categories with the top 20 transaction count\ntop20_R_email = raw_train_transaction[\"R_emaildomain\"].value_counts().head(20).index\ntop20_R_email = raw_train_transaction[raw_train_transaction['R_emaildomain'].isin(top20_R_email)]","5b15fcf9":"# find out the number of transactions for R_emaildomain categories with the top 20 transaction count \n# find out the fraud transaction percentage of total transactions for R_emaildomain categories with the top 20 transaction count \nfind_percentage(top20_R_email,['R_emaildomain'],fig_size,rotation)","02e49427":"# create a function to convert TransactionDT variable into a time format\ndef convert_time (df):\n    START_DATE = '2017-01-01'\n    startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n    df[\"Date\"] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n    df['Weekdays'] = df['Date'].dt.dayofweek\n    df['Hours'] = df['Date'].dt.hour\n    df['Days'] = df['Date'].dt.day\n    df['Month'] = df['Date'].dt.month\n    return df","2c0e8b71":"# convert both train and test transaction's TransactionDT variable into a time format\nraw_train_transaction = convert_time(raw_train_transaction)\nraw_test_transaction =  convert_time(raw_test_transaction)","68ac483c":"# plot bar graphs to visualize the number of transaction and fraud transaction percentage for each hour\nfig_size=(16,6)\nfind_percentage(raw_train_transaction,['Hours'],fig_size,rotation)","1b6d768e":"# plot bar graphs to visualize the number of transaction and fraud transaction percentage for each week day\nfind_percentage(raw_train_transaction,['Weekdays'],fig_size,rotation)","239f3f51":"# plot bar graphs to visualize the number of transaction and fraud transaction percentage for each month\nfind_percentage(raw_train_transaction,['Month'],fig_size,rotation)","cc27f440":"# create a new dataframe to store the number of fraud for each day\nfraud_record=raw_train_transaction[raw_train_transaction['isFraud']==1]\ndaily_fraud=fraud_record.groupby(by=fraud_record['Date'].dt.date).size().reset_index()\ndaily_fraud.columns=['Date','Number of Fraud']","5ee64cc0":"# create a line graph to visualize the changes in the number of fraud transaction for each day from 2017-01-01 to 2017-07-02\nplt.figure(figsize=(16,6))\nplt.title(\"Number of Fraud Transaction per Day\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Transaction Count\")\nplt.plot(daily_fraud['Date'],daily_fraud['Number of Fraud'])\nplt.show()","a8ae8c3e":"# preview of identify file\nraw_train_identity.head()","c184fac6":"merged_df = raw_train_transaction.merge(raw_train_identity,how='left', left_index=True, right_index=True)","3ed2f82c":"# select the devices with top 10 transaction count \ntop10_Device_Type = merged_df[\"DeviceType\"].value_counts().index\ntop10_Device_Type  = merged_df[merged_df['DeviceType'].isin(top10_Device_Type)]","6be640f2":"# select more detailed devices with top 10 transaction count \ntop10_Device_Info = merged_df[\"DeviceInfo\"].value_counts().head(10).index\ntop10_Device_Info = merged_df[merged_df['DeviceInfo'].isin(top10_Device_Info)]","ac432d68":"# plot bar graphs to visualize the number of transaction and fraud transaction percentage for each device  \nfind_percentage(top10_Device_Type,['DeviceType'],fig_size,rotation)","29720dae":"# plot bar graphs to visualize the number of transaction and fraud transaction percentage for the top 10 devices \nrotation=45\nfind_percentage(top10_Device_Info,['DeviceInfo'],fig_size,rotation)","885c4ee7":"# plot bar graphs to visualize other categorical variables in the identity table\nrotation=0\nid_list= ['id_12','id_15','id_16','id_23','id_27','id_28','id_29','DeviceType']\nfind_percentage(merged_df,id_list,fig_size,rotation)","589eddbb":"# store raw files into a new variable for modification\ntrain_transaction = raw_train_transaction\ntest_transaction = raw_test_transaction","9258b192":"# log transformation on TransactionAMT variable \ntrain_transaction['TransactionAmt'] = np.log(raw_train_transaction['TransactionAmt'])\ntest_transaction['TransactionAmt'] = np.log(raw_test_transaction['TransactionAmt'])","c661abdf":"# merge transaction file with identity file \nmerged_train_df = train_transaction.merge(raw_train_identity,how='left', left_index=True, right_index=True)\nmerged_test_df = test_transaction.merge(raw_test_identity,how='left', left_index=True, right_index=True)","89834c8a":"# create lists to store categorical variables\nv_features = [\"V\"+str(i) for i in np.arange(1, 340, 1)]\nC_cols = [\"C\"+str(i) for i in np.arange(1, 15, 1)]\ncard_cols = [\"card\"+str(i) for i in np.arange(1, 7, 1)]\nD_cols = [\"D\"+str(i) for i in np.arange(1, 16, 1)]\naddr_cols = [\"addr\"+str(i) for i in np.arange(1, 3, 1)]\nid_cols = [\"id_\"+str(i) for i in np.arange(12, 39, 1)]","b4487b33":"# create a function to perform pca transformation to reduce the number of variables\ndef PCA_transform(df, cols,prefix, n_features):\n    pca = PCA(n_components = n_features, random_state=101)\n    pca_model = pca.fit_transform(df[cols])\n    pca_df = pd.DataFrame(pca_model)\n    df.drop(cols, axis=1, inplace=True)\n    pca_df.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n    df = pd.concat([df, pca_df], axis=1)\n    return df","7d74e140":"# since pca does not accept NA values, we will fill na with -1 \n# before pca transformation the data need to be scaled from 0 to 1 \ndef fill_na_features (df,features):\n    for col in features:\n        df[col] = df[col].fillna((df[col].min() - 1))\n        df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n    return df","5cfcec93":"# prepare pca transformation\nmerged_train_df = fill_na_features(merged_train_df,v_features)\nmerged_test_df = fill_na_features(merged_test_df,v_features)","ae693352":"# perform pca transformation which holds 95% of variance of v_features\nmerged_train_df = PCA_transform(merged_train_df, v_features, 'PCA_V',20)\nmerged_test_df = PCA_transform(merged_test_df, v_features, 'PCA_V',20)","76f77c37":"cat_cols1 = [card_cols,addr_cols,M_cols,id_cols]\ncat_cols2 = ['ProductCD','P_emaildomain','R_emaildomain','DeviceType','DeviceInfo']","92013e22":"# create a function to convert the categorical variable's categories into numbers\ndef convert_cat_label1(df):\n    for i in range(len(cat_cols1)):\n        for col in cat_cols1[i]:\n            # avoid nan\n            if df[col].dtype=='object':\n                le = preprocessing.LabelEncoder()\n                le.fit(list(df[col].values) + list(df[col].values))\n                df[col] = le.transform(list(df[col].values))\n    return df","a6c32c39":"# create a function to convert the categorical variable's categories into numbers\ndef convert_cat_label2(df):\n    for col in cat_cols2:\n        if col in df.columns:\n            le = preprocessing.LabelEncoder()\n            le.fit(list(df[col].values) + list(df[col].values))\n            df[col] = le.transform(list(df[col].values))\n    return df","e17b6b13":"# convert categorical variables's categories into numbers \nmerged_train_df = convert_cat_label1(merged_train_df)\nmerged_train_df = convert_cat_label2(merged_train_df)\nmerged_test_df = convert_cat_label1(merged_test_df)\nmerged_test_df = convert_cat_label2(merged_test_df)","2916211a":"# collect all the garbabge variables to reduce memory\ngc.collect()","79b6aed6":"# assign indedepnt variables to X, and depdent variable isFraud to y\nX= merged_train_df.drop(['TransactionID_x','TransactionDT','isFraud','Date'],axis=1)\ny=merged_train_df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nX_submission = merged_test_df.drop(['TransactionID_x','TransactionDT','Date'],axis=1)","156ad468":"# use hypteropt to optimize the parameters of xgb classifier \n'''\ndef objective(params):\n    params = {\n        'max_depth': int(params['max_depth']),\n        'subsample':'{:.3f}'.format(params['subsample']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n    }\n    \n    clf = xgb.XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        n_jobs=4,\n        random_state=101\n        **params\n    )\n\n    #score = cross_val_score(clf, X_test, y_test, scoring='roc_auc', cv=3)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n    clf.fit(X_train, y_train)\n    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n\n    print(\"auc {:.3f} params {}\".format(auc, params))\n    return auc\n\nspace = {\n    'max_depth': hp.quniform('max_depth', 3, 8, 1),\n    'subsample': hp.uniform('subsample', 0.6, 0.9),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 0.9),\n}\n'''","a9736d90":"'''\n%%time\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5)\nprint(\"Hyperopt estimated optimum {}\".format(best))\n'''","f7c4b544":"# use xgboost to classify whether each transaction is fraud or not\nimport xgboost as xgb\nclf = xgb.XGBClassifier( n_estimators=500,\n    max_depth=7,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=101)","327c37df":"# fit the model with x label and y label and predict X_test\nclf.fit(X_train,y_train)\ny_preds = clf.predict_proba(X_test)","cdf5b105":"# accuracy on y_test \nauc = roc_auc_score(y_test, y_preds[:,1])\nprint('AUC: %.3f' % auc)","6c05f3e3":"# predict the probability of each transaction is fraud or not\ny_preds = clf.predict_proba(X_submission)","a6dd981d":"# merge prediction results with test transactions\nsample_submission = pd.read_csv(\"..\/input\/ieee-fraud-detection\/sample_submission.csv\")\nsample_submission['isFraud']=y_preds[:,1]\nsample_submission.head(10)\nsample_submission.to_csv('final_result.csv', index=False)","ab2f58c8":"# display the first 10 rows\nsample_submission.head(10)","1f8df743":"## Exploratory Data Analysis","75010ff5":"<h3><center>Transaction Table<\/center><\/h3> \n<center>reference from https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203<\/center>\n<center>It contains money transfer and also other gifting goods and service, like you booked a ticket for others, etc<\/center>\n\n- <em>TransactionDT<\/em>: timedelta from a given reference datetime (not an actual timestamp). TransactionDT first value is 86400, which corresponds to the number of seconds in a day (60 * 60 * 24 = 86400) so I think the unit is seconds. Using this, we know the data spans 6 months, as the maximum value is 15811131, which would correspond to day 183\n\n- <em>TransactionAMT<\/em>:  transaction payment amount in USD\n\n- <em>ProductCD (Categorical)<\/em>: product code, the product for each transaction \n\n- <em>card1 - card6 (Categorical)<\/em>: payment card information, such as card type, card category, issue bank, country, etc.\n\n- <em>addr (Categorical)<\/em>: purchaser address\naddr1 as billing region\naddr2 as billing country\n\n- <em>dist<\/em>: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\u201d\n\n- <em>P_ and R_emaildomain (Categorical)<\/em>: purchaser and recipient email domain. Certain transactions don't need recipient, so R_emaildomain is null\n\n- <em>C1-C14 <\/em>: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked\n\n- <em>D1-D15<\/em>: timedelta, such as days between previous transaction, etc\n\n- <em>M1-M9 (Categorical)<\/em>: match, such as names on card and address, etc\n\n- <em>Vxxx<\/em>: Vesta engineered rich features, including ranking, counting, and other entity relations\n","c07f99bf":"## Feature Engineering","b0a3badc":"<h3><center>Key findings through exploratory data analysis:<\/center><\/h3>\n\n- 96.5% of transactions were not fraud and 3.5% of transactions were fraud transactions\n- The values in TransactionAmt variable were heavily skewed to the right, therefore apply log transformation on TransactionAMT column will give the values a normal distribution\n- In variable ProductCD category W had the most transactions, but category C had the highest chance to be a fraud transaction\n- Transactions with card1 value around 10000 were more likely to have fraud transactions, whereas card1 value around 8000, and 12500 were more likely to have non fraud transactions\n- 65.16% of the transactions were from visa credit card company, but discover credit card company was more likely to have a fraud transaction than other credit card types\n- 74.5% of the transactions were debit type, however credit transactions were more likely to have fraud transactions\n- The top 3 transactions were from 200, 300, and 320 addr1 variable\n- Most of the transactions were from 87 addr2 variable\n- Although there were only 1.4% of transactions were from 512 add1 variable, it had the highest chance to have a fraud transaction \n- Purchaser and receipt\u2019s email domain from outlook had the highest chance to be a fraud transaction\n- Around 30% of fraud transactions occurred during 07:00 \u2013 09:00\n- Most of the transactions occurred in early March (March break), and transactions in July had the highest chance to be fraud transactions\n- The most used device was on Windows but the device with the highest probabilty to have a fraud transaction was SM-G531H Build\/LMY48B\n","ba0198c0":"## Data Preprocessing","623f7650":"<h3><center>Identity Table<\/center><\/h3> \n<center>reference from https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203<\/center>\n\nVariables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.\n\n- <em>id01 - id38<\/em>: features for identity, which is collected by Vesta and security partners such as device rating, ip_domain rating, proxy rating, etc. Also it recorded behavioral fingerprint like account login times\/failed to login times, how long an account stayed on the page, etc. All of these are not able to elaborate due to security partner T&C\n- <em>id12 - id38 (Categorical)<\/em>\n \n- <em>DeviceType (Categorical)<\/em>: device type of the transaction occurred\n- <em>DeviceInfo (Categorical)<\/em>: More detailed device type of the transaction occurred  \n","83f5654f":"## Prediction Model"}}