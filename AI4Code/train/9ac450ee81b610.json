{"cell_type":{"ba24f8a3":"code","53355c99":"code","d8cb3852":"code","2a5631f6":"code","3d309ffc":"code","6021dfb1":"code","ac26e796":"code","bd613bcd":"code","43c820a1":"code","437f32bc":"code","1593b140":"code","f9faeea4":"code","83a12491":"code","94eed073":"code","11f26580":"code","46177d7d":"code","8ff65599":"markdown"},"source":{"ba24f8a3":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport keras\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error","53355c99":"data = pd.read_csv('..\/input\/Concrete_Data_Yeh.csv')","d8cb3852":"data.head()","2a5631f6":"data.describe()","3d309ffc":"print(\"Null values check \", data.columns[data.isna().any()].tolist())","6021dfb1":"print('Data Size ', data.shape)","ac26e796":"## print(data.columns)\nfor coloumn in data.columns:\n    print(coloumn + ' Distribution ', data[coloumn].plot( style='o', figsize = (15,12)))","bd613bcd":"training_features = ['cement','slag', 'flyash', 'water', 'superplasticizer', 'coarseaggregate',  'fineaggregate', 'age' ]\nlabel_feature = ['csMPa']\nselected_feature_data = data","43c820a1":"m = data.shape[0]\nn = len(training_features)\npercentage_of_training = 80\nnumber_example_in_training = int((percentage_of_training * m)\/100)\nnumber_example_in_test = int(m - number_example_in_training)\n\nprint('number_example_in_training', number_example_in_training)\nprint('number_example_in_test', number_example_in_test)\n\ntraining_data_features = selected_feature_data.head(number_example_in_training)[training_features]\ntraining_data_labels = selected_feature_data.head(number_example_in_training)[label_feature]\n\ntest_data_features = selected_feature_data.head(number_example_in_test)[training_features]\ntest_data_labels = selected_feature_data.head(number_example_in_test)[label_feature]\n\n\nprint('shape of traing data features', training_data_features.shape)\nprint('shape of traing data labels', training_data_labels.shape)\nprint('shape of test data features', test_data_features.shape)\nprint('shape of test data features', test_data_labels.shape)","437f32bc":"def build_model(learning_rate):\n\n  model = keras.models.Sequential([\n    keras.layers.Dense(19, activation=tf.nn.relu,kernel_regularizer= keras.regularizers.l2(0.01),\n                       input_shape=(training_data_features.shape[1],)),\n    keras.layers.Dense(13, activation=tf.nn.relu, kernel_regularizer= keras.regularizers.l2(0.01)),  \n    keras.layers.Dense(7, activation=tf.nn.relu,kernel_regularizer= keras.regularizers.l2(0.01)),\n    keras.layers.Dense(1)\n  ])\n\n  optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n\n  model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae'])\n  return model","1593b140":"\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 1000 == 0: print('epoch ', epoch,)\n\ndef plot_history(history):\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Abs Error [1000$]')\n  plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n           label='Train Loss')\n  plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n           label = 'Val loss')\n  plt.legend()\n  plt.ylim([0, 5])\n\n# Store training stats\n","f9faeea4":"\n\nlearning_rates = [0.0001, 0.0003, 0.0009, 0.001, 0.003, 0.006, 0.009,0.01,0.06,0.1,0.3]\n\nmse = [None] * len(learning_rates)\ncounter = 0\nfor learning_rate in learning_rates:\n    model = build_model(learning_rate=learning_rate)\n#    model.summary()\n    EPOCHS = 1000\n        \n    history = model.fit(training_data_features, training_data_labels, epochs=EPOCHS,\n                    validation_split=0.2, verbose=0,\n                    callbacks=[PrintDot()])        \n    print('Learning Rate ', learning_rate)\n    print(model.evaluate(test_data_features, test_data_labels, verbose=0))\n    print(plot_history(history))\n    \n    predicted_value = model.predict(test_data_features)\n    MSE = mean_squared_error(np.asmatrix(test_data_labels), predicted_value)\n    print(\"Mean Square Error \", MSE)\n    mse[counter] = MSE\n    counter = counter +1\n    for i in range(0,5):\n        print(predicted_value[i], \"--\", np.asmatrix(test_data_labels)[i])\n    \n","83a12491":"plt.plot(learning_rates, mse)\n","94eed073":"learning_rate = 0.0003\nepochs = [500,1000,1500,2000,3000,5000]\nmse = [None] * len(epochs)\ncounter = 0\n\nfor epoch in epochs:\n    model = build_model(learning_rate=learning_rate)\n#    model.summary()\n    EPOCHS = epoch\n    history = model.fit(training_data_features, training_data_labels, epochs=EPOCHS,\n                    validation_split=0.2, verbose=0,\n                    callbacks=[PrintDot()])        \n    print('Learning Rate ', learning_rate)\n    print(model.evaluate(test_data_features, test_data_labels, verbose=0))\n    print(plot_history(history))\n    \n    predicted_value = model.predict(test_data_features)\n    MSE = mean_squared_error(np.asmatrix(test_data_labels), predicted_value)\n    print(\"Mean Square Error \", MSE)\n    mse[counter] = MSE\n    counter = counter +1\n    for i in range(0,5):\n        print(predicted_value[i], \"--\", np.asmatrix(test_data_labels)[i])\n    \n","11f26580":"plt.plot(epochs, mse)","46177d7d":"learning_rate = 0.0003\nepochs = 3000\n\n\nmodel = build_model(learning_rate=learning_rate)\nhistory = model.fit(training_data_features, training_data_labels, epochs=epochs,\n                    validation_split=0.2, verbose=0,\n                    callbacks=[PrintDot()])        \nprint('Learning Rate ', learning_rate)\nprint(model.evaluate(test_data_features, test_data_labels, verbose=0))\nprint(plot_history(history))\n    \npredicted_value = model.predict(test_data_features)\nMSE = mean_squared_error(np.asmatrix(test_data_labels), predicted_value)\nprint(\"Mean Square Error \", MSE)\nfor i in range(0,5):\n    print(predicted_value[i], \"--\", np.asmatrix(test_data_labels)[i])\n\n","8ff65599":"Till now we have find the learning rate of 0.0003 to be desent learning rate with 3000 epochs\nlet freez the learning rate and epochs for not.\nTry two differet things , normalized the input data and chnage with model design."}}