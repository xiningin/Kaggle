{"cell_type":{"4b9deaf9":"code","c8458d12":"code","c06e6844":"code","301ef322":"code","22456c47":"code","1b06d31c":"code","ea3dc76f":"code","6317af8e":"code","6d4b648b":"code","1ce855e1":"code","5cf3cbe5":"code","c38d09b0":"code","cbf1b7e8":"code","df376526":"code","4134fc53":"code","fd94bbec":"code","6234b6d6":"code","574b3de6":"code","a0a62252":"code","f47be020":"code","dbc774f6":"code","b6dfe0c4":"code","bfda8b41":"code","068106dd":"code","4e598cd0":"code","a15fe57b":"code","16022c8c":"code","97e9ec49":"code","f76f8688":"code","cba94e0c":"code","c9757370":"code","dabb8eae":"code","5c513a27":"code","6cc6cc31":"code","59caa41a":"code","4c674f48":"code","0c58f2d7":"code","83686b8c":"code","8017d7e4":"code","84745c9f":"code","f9224aff":"code","7742aec7":"code","190f9921":"code","f0437e92":"code","6c408c54":"code","4f4571c4":"code","d2fff513":"code","c56d4277":"code","9536b0cc":"code","96d4d766":"code","56685d6b":"code","44e134fe":"code","e0d0a95d":"code","230b9164":"code","2ee3d86b":"code","572e552f":"code","c652e1a3":"code","24bd0956":"code","07f3f941":"code","0c4bea5c":"code","e63fdc37":"code","9fb3a0a5":"code","86e8c0bc":"code","282c62e0":"code","fe850f97":"code","3da289bf":"code","a0c64979":"code","1219d888":"code","ed92654d":"code","320aefac":"code","62e179a9":"code","4019f96c":"code","f42c7275":"code","3bba308a":"code","71a61e97":"code","3f97d12f":"code","c1e85d00":"code","102c193c":"code","0ba6b89c":"code","3c268657":"code","9106a735":"code","786c1f5c":"code","7a01a678":"code","29e2e727":"code","3d0dac40":"code","cd681c9e":"code","b29162b7":"code","8b791f22":"code","9b7e4260":"code","2f723f22":"code","817185b5":"code","2b8edafc":"code","181f752f":"code","c52a85e7":"code","0d51d09f":"code","b3eeef4c":"code","42d0c9d2":"code","6f46d634":"code","5912e5a4":"markdown","2a4e64e8":"markdown","29450bf3":"markdown","917543f4":"markdown","5ad698d4":"markdown","5dbe1af4":"markdown","a0e38eb8":"markdown","1c75bc88":"markdown","5a9e19df":"markdown","d6de386a":"markdown","44280e0c":"markdown","da1f2ad5":"markdown","a3431f30":"markdown","040ed48e":"markdown","3b98abfa":"markdown","f2e732a5":"markdown","912bb1a8":"markdown","639a144a":"markdown","d2ac1d6b":"markdown","4bef0bac":"markdown","cd188403":"markdown","004914f6":"markdown","33136b9f":"markdown","770a54da":"markdown","13bc486b":"markdown","625f29c3":"markdown","ab5b01f4":"markdown","61265c65":"markdown","a7f5c6c4":"markdown","541b1737":"markdown","5aee53b8":"markdown","2c3a565d":"markdown","b692dc4b":"markdown","99731c96":"markdown","68a0719b":"markdown","53ea5b02":"markdown","48cdbd0f":"markdown","b61f371c":"markdown","0a3ead7a":"markdown","88dd09b0":"markdown","6c6294f3":"markdown","cb309c79":"markdown","021fa1f6":"markdown","ca63931c":"markdown","2590f0b8":"markdown","a96d230c":"markdown","7e6cb13a":"markdown","17ade439":"markdown","85660c96":"markdown","b8071d9e":"markdown","7414ae12":"markdown","0c764b3b":"markdown","a0825e2d":"markdown","c9b6359d":"markdown","c819cbda":"markdown","20ebea6c":"markdown"},"source":{"4b9deaf9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c8458d12":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVR\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","c06e6844":"covid_data = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')\ncovid_data.head()","301ef322":"covid_data.info()\n\n# We have 111 columns\n# 5644 instances or rows","22456c47":"numericals = covid_data.select_dtypes(include=['int', 'float']).columns[4:]\nplt.figure(figsize=(10, 7))\nfor cols in numericals:\n    data = covid_data[cols].dropna()\n    try:\n        sns.distplot(data, label=cols)\n    except:\n        continue\nplt.xlabel(None)","1b06d31c":"print('Number of rows: {}'.format(len(covid_data)))\nprint('Number of columns: {}'.format(len(covid_data.columns)))","ea3dc76f":"# Visuzalizing Abscence of data\nplt.figure(figsize=(20, 7))\nsns.heatmap(covid_data.isnull(), cmap='Reds', cbar=False)\nplt.yticks([], [])","6317af8e":"import missingno as msno\nmsno.matrix(covid_data, sparkline=False)","6d4b648b":"msno.matrix(covid_data[covid_data.columns[0:50]], sparkline=False)\nplt.xticks(rotation='vertical')","1ce855e1":"msno.matrix(covid_data[covid_data.columns[50:100]], sparkline=False)\nplt.xticks(rotation='vertical')","5cf3cbe5":"msno.matrix(covid_data[covid_data.columns[100:]], sparkline=False)\nplt.xticks(rotation='vertical')","c38d09b0":"msno.bar(covid_data[covid_data.columns[0:50]])\nplt.xticks(rotation='vertical')","cbf1b7e8":"msno.bar(covid_data[covid_data.columns[50:100]])\nplt.xticks(rotation='vertical')","df376526":"msno.bar(covid_data[covid_data.columns[100:]])\nplt.xticks(rotation='vertical')","4134fc53":"sns.countplot(x='SARS-Cov-2 exam result', data=covid_data)","fd94bbec":"num_positive_cases = len(covid_data[covid_data['SARS-Cov-2 exam result'] == 'positive'])\nnum_negative_cases = len(covid_data[covid_data['SARS-Cov-2 exam result'] == 'negative'])\n\nprint('Number of positive cases for Covid-19: {}'.format(num_positive_cases))\nprint('Number of negative cases for Covid-19: {}'.format(num_negative_cases))","6234b6d6":"positive_covid_data = covid_data[covid_data['SARS-Cov-2 exam result'] == 'positive']\nnegative_covid_data = covid_data[covid_data['SARS-Cov-2 exam result'] == 'negative']\n\n#Exporting the splited data just in case\npositive_covid_data.to_csv('covid-positive.csv', index=False)\nnegative_covid_data.to_csv('covid-negative.csv', index=False)","574b3de6":"positive_covid_data.head()","a0a62252":"positive_covid_data.count()","f47be020":"# Filtering columns that have the amount of significant data less than 10% than the total amount\n# of positive cases of covid which are 558\ncols_indexes = np.where(~ (positive_covid_data.count() < 0.1 * len(positive_covid_data)))[0]\nrepr_pos_covid_data = positive_covid_data.iloc[:, cols_indexes].copy()\n\nprint('Amount of columns after processing: {}'.format(len(repr_pos_covid_data.columns)))\nprint(repr_pos_covid_data.columns)\nprint('Number of instances after droping rows: {}'.format(len(repr_pos_covid_data.dropna())))","dbc774f6":"# Visuzalizing Abscence of data\nplt.figure(figsize=(20, 7))\nsns.heatmap(repr_pos_covid_data.isnull(), cmap='Reds', cbar=False)\nplt.yticks([], [])","b6dfe0c4":"import missingno as msno\nmsno.heatmap(repr_pos_covid_data)","bfda8b41":"covid_data = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')\ncovid_tests = covid_data.copy()\ncovid_data.head()","068106dd":"print('Number of columns: {}'.format(len(covid_tests.columns)))\nprint(covid_tests.columns)","4e598cd0":"covid_tests.dropna(axis='columns', how='all', inplace=True)","a15fe57b":"print('Number of columns: {}'.format(len(covid_tests.columns)))\nprint(covid_tests.columns)","16022c8c":"covid_data.iloc[:, np.where(covid_data.isnull().all())[0]].columns","97e9ec49":"categorical_data = covid_tests.select_dtypes(exclude=['int', 'float']).copy()\ncategorical_data.info()","f76f8688":"categorical_data.head()","cba94e0c":"categorical_data.drop(labels=[\"Patient ID\", \"SARS-Cov-2 exam result\"], axis='columns', inplace=True)\ncategorical_data.head()","c9757370":"single_value_cols = []\nmore_than_one = []\nfor col_name in categorical_data.columns:\n    unique_values = list(categorical_data[col_name].unique())\n    unique_values.remove(np.nan)\n    print(\"{}: {} - Unique values {}\".format(col_name, unique_values, len(unique_values)))\n    \n    if len(unique_values) == 1:\n        single_value_cols.append(col_name)\n    else:\n        more_than_one.append(col_name)","dabb8eae":"print(single_value_cols)","5c513a27":"print('Number of columns: {}'.format(len(covid_tests.columns)))\ncovid_tests.drop(labels=single_value_cols, axis='columns', inplace=True)\nprint('Number of after columns: {}'.format(len(covid_tests.columns)))","6cc6cc31":"covid_tests.head()","59caa41a":"print(\"Remaining categorical values to analyze: {}\".format(len(more_than_one)))","4c674f48":"fig, ax = plt.subplots(nrows=6, ncols=5, figsize=(20, 15))\naxis = ax.ravel()\nfor idx, col in enumerate(more_than_one):\n    plt.title(col)\n    sns.countplot(x=col, data=covid_tests, ax=axis[idx])\nfig.tight_layout()","0c58f2d7":"mapping_cols = dict()\nfor col_name in more_than_one:\n    unique_values = list(covid_tests[col_name].unique())\n    unique_values.remove(np.nan)\n    mapping_cols[col_name] = (len(unique_values), unique_values)\n    \nto_drop = []\nfor key, val in mapping_cols.items():\n    if val[0] <= 2:\n        total_val = covid_tests[key].value_counts().sum()\n        mappg_val = covid_tests[key].value_counts().to_dict()\n        pct = np.array([mappg_val[k] \/ total_val for k in mappg_val])\n        if np.any(pct < 0.1):\n            to_drop.append(key)\n\nprint(to_drop)","83686b8c":"print('Total number of catergorical columns: {}'.format(len(more_than_one)))\nprint('Total number of catergorical columns: {}'.format(len(to_drop)))\nprint('Remaining number of catergorical columns: {}'.format(len(more_than_one) - len(to_drop)))","8017d7e4":"print('Total number of columns of current data: {}'.format(len(covid_tests.columns)))\ncovid_tests.drop(labels=to_drop, axis='columns', inplace=True)\nprint('Total number of columns of current data after vertical elimination: {}'.format(len(covid_tests.columns)))","84745c9f":"covid_tests.head()","f9224aff":"# Visuzalizing Abscence of data\nplt.figure(figsize=(20, 7))\nsns.heatmap(covid_tests.isnull(), cmap='Reds', cbar=False)\nplt.yticks([], [])","7742aec7":"len(covid_tests.columns[5:])","190f9921":"fig, ax = plt.subplots(nrows=8, ncols=5, figsize=(20, 15))\naxis = ax.ravel()\nfor idx, col in enumerate(covid_tests.columns[5:][:40]):\n    plt.title(col)\n    indexes = covid_tests[col].dropna().index\n    covid_tests.iloc[indexes, :]['SARS-Cov-2 exam result'].value_counts().plot(kind='bar', color=['b', 'r'], ax=axis[idx], title=col)\nfig.tight_layout()","f0437e92":"fig, ax = plt.subplots(nrows=8, ncols=5, figsize=(20, 15))\naxis = ax.ravel()\nfor idx, col in enumerate(covid_tests.columns[5:][40:]):\n    plt.title(col)\n    indexes = covid_tests[col].dropna().index\n    covid_tests.iloc[indexes, :]['SARS-Cov-2 exam result'].value_counts().plot(kind='bar', color=['b', 'r'], ax=axis[idx], title=col)\n\nfor ax in axis[-6:]:\n    ax.set_visible(False)\nfig.tight_layout()","6c408c54":"print('Total number of columns of current data: {}'.format(len(covid_tests.columns)))\ncovid_tests.drop(labels=['Albumin', 'Vitamin B12', 'Fio2 (venous blood gas analysis)'], axis='columns', inplace=True)\nprint('Total number of columns of current data after vertical elimination: {}'.format(len(covid_tests.columns)))","4f4571c4":"# Visuzalizing Abscence of data\nplt.figure(figsize=(20, 7))\nsns.heatmap(covid_tests.isnull(), cmap='Reds', cbar=False)\nplt.yticks([], [])","d2fff513":"from sklearn.impute import SimpleImputer","c56d4277":"covid_input = covid_tests.drop(labels=covid_tests.columns[:6], axis='columns')\ncovid_targt = covid_tests['SARS-Cov-2 exam result']","9536b0cc":"categorical_columns = covid_input.select_dtypes(exclude=['int', 'float']).columns\nnumerical_columns = covid_input.select_dtypes(include=['int', 'float']).columns","96d4d766":"numerical_columns","56685d6b":"categorical_columns","44e134fe":"covid_input[numerical_columns] = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(covid_input[numerical_columns])\ncovid_input[numerical_columns].head()","e0d0a95d":"for col in categorical_columns:\n    categories = dict(enumerate(covid_input[col].dropna().unique()))\n    categories = {val:key for key, val in categories.items()}\n    covid_input[col] = covid_input[col].map(categories)","230b9164":"covid_input[categorical_columns] = SimpleImputer(missing_values=np.nan, strategy='most_frequent').fit_transform(covid_input[categorical_columns])\ncovid_input[categorical_columns].head()","2ee3d86b":"covid_input.head()","572e552f":"def experiment_train_kfold(data, estimator, param_grid, scaler, metric='accuracy'):\n    X_train, X_test, Y_train, Y_test = data\n    std = scaler()\n    \n    X_train = std.fit_transform(X_train)\n    X_test = std.transform(X_test)\n\n    kfold = KFold(n_splits=10)\n    grid = GridSearchCV(estimator=estimator, param_grid=param_grid,\n                        scoring=metric, cv=kfold, verbose=1, n_jobs=-1, error_score='raise')\n    grid.fit(X_train, Y_train)\n    return grid.best_estimator_","c652e1a3":"covid_tmp = pd.concat([covid_input, covid_targt], axis='columns')\npositive_cases = covid_tmp[covid_tmp['SARS-Cov-2 exam result'] == 'positive'].copy()\nnegative_cases = covid_tmp[covid_tmp['SARS-Cov-2 exam result'] == 'negative'].copy()","24bd0956":"print('Number of instance: {}'.format(len(positive_cases)))","07f3f941":"print('Number of instance: {}'.format(len(negative_cases)))","0c4bea5c":"# Sampling negative cases\nnegative_cases = negative_cases.sample(n=len(positive_cases), replace=False).reset_index(drop=True).copy()\nprint('Number of instance: {}'.format(len(negative_cases)))","e63fdc37":"covid_tmp = positive_cases.append(negative_cases).copy()\nprint('Number of instances: {}'.format(len(covid_tmp)))","9fb3a0a5":"covid_input = covid_tmp.drop(labels=['SARS-Cov-2 exam result'], axis='columns')\ncovid_targt = covid_tmp['SARS-Cov-2 exam result']","86e8c0bc":"mlp = MLPClassifier()\nn_dim = 30\nparam_grid_mlp = {'hidden_layer_sizes': [\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim,\n                                          n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim), #18\n    \n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim,\n                                          n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim), #14\n    \n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim)\n                                        ],\n                  'max_iter':[5000],\n                  'activation': ['tanh', 'relu', 'logistic'],\n                  'solver': ['lbfgs', 'sgd', 'adam'],\n                  'learning_rate' : ['constant'],\n                  'learning_rate_init': [0.001, 0.01, 0.1]}\n\nsplited_data = train_test_split(covid_input, covid_targt, test_size=0.15, stratify=covid_targt, shuffle=True)\nbest_estimator = experiment_train_kfold(data=splited_data,\n                                        estimator=mlp,\n                                        param_grid=param_grid_mlp,\n                                        scaler=StandardScaler,\n                                        metric='roc_auc')","282c62e0":"X_train, X_test, Y_train, Y_test = splited_data\nstd = StandardScaler()\nX_train = std.fit_transform(X_train)\nX_test = std.transform(X_test)\npreds = best_estimator.predict(X_test)\n\nacc = best_estimator.score(X_test, Y_test)\nprint('Accuracy of the training model : {:.2f}%'.format(acc * 100))","fe850f97":"print(classification_report(Y_test, preds))","3da289bf":"cfm = confusion_matrix(Y_test, preds)\nplt.figure(figsize=(10, 7))\nsns.heatmap(cfm, annot=True, cbar=False, cmap='Blues', fmt='g')","a0c64979":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import plot_roc_curve\n\n\nmapping_dict = dict(enumerate(np.unique(Y_test)))\nmapping_dict = {val: key for key, val in mapping_dict.items()}\n\ntest_vals = list(map(lambda x: mapping_dict[x], Y_test))\n\nprobs = best_estimator.predict_proba(X_test)\npreds = probs[:, 1]\n\nfpr, tpr, threshold = roc_curve(test_vals, preds)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic for Positive cases')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","1219d888":"positive_cases = covid_tests[covid_tests['SARS-Cov-2 exam result'] == 'positive'].copy()\nnegative_cases = covid_tests[covid_tests['SARS-Cov-2 exam result'] == 'negative'].copy()\n\nprint('Number of positive samples: {}'.format(len(positive_cases)))\nprint('Number of negative samples: {}'.format(len(negative_cases)))\n\nnumerical_columns = covid_tests.select_dtypes(include=['int', 'float']).columns\ncategorical_columns = covid_tests.select_dtypes(exclude=['int', 'float']).columns\n\nfor col in categorical_columns:\n    categories = dict(enumerate(covid_tests[col].dropna().unique()))\n    categories = {val:key for key, val in categories.items()}\n    positive_cases[col] = positive_cases[col].map(categories)\n    negative_cases[col] = negative_cases[col].map(categories)","ed92654d":"positive_cases[numerical_columns] = SimpleImputer(missing_values=np.nan, strategy='mean') \\\n                                                .fit_transform(positive_cases[numerical_columns])\npositive_cases[categorical_columns] = SimpleImputer(missing_values=np.nan, strategy='most_frequent') \\\n                                                .fit_transform(positive_cases[categorical_columns])\n\nnegative_cases[numerical_columns] = SimpleImputer(missing_values=np.nan, strategy='mean') \\\n                                                .fit_transform(negative_cases[numerical_columns])\nnegative_cases[categorical_columns] = SimpleImputer(missing_values=np.nan, strategy='most_frequent') \\\n                                                .fit_transform(negative_cases[categorical_columns])","320aefac":"negative_cases = negative_cases.sample(n=len(positive_cases), replace=False)","62e179a9":"covid_trfm = positive_cases.append(negative_cases).copy()\ncovid_trfm.drop(labels=['Patient ID', 'Patient age quantile',\n                        'Patient addmited to regular ward (1=yes, 0=no)',\n                        'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                        'Patient addmited to intensive care unit (1=yes, 0=no)'], axis='columns', inplace=True)\ncovid_trfm.head()","4019f96c":"categorized_data = pd.get_dummies(data=covid_trfm, columns=categorical_columns[2:])\ncategorized_data.head()","f42c7275":"covid_input = categorized_data.drop(labels=['SARS-Cov-2 exam result'], axis='columns').copy()\ncovid_targt = categorized_data['SARS-Cov-2 exam result'].copy()","3bba308a":"X_train, X_test, y_train, y_test = train_test_split(covid_input, covid_targt, test_size=0.2, stratify=covid_targt)","71a61e97":"rf = RandomForestRegressor(n_estimators = 100,\n                           n_jobs = -1,\n                           oob_score = True,\n                           bootstrap = True)\nrf.fit(X_train, y_train)\nprint('Score: {:.2f}%'.format(rf.score(X_test, y_test) * 100))","3f97d12f":"preds = rf.predict(X_test)\npreds[preds < 0.5] = 0\npreds[preds > 0.5] = 1\n\ncfm = confusion_matrix(y_test, preds)\nplt.figure(figsize=(10, 7))\nsns.heatmap(cfm, annot=True, cbar=False, cmap='Blues', fmt='g')","c1e85d00":"plt.figure(figsize=(20, 7))\nplt.xticks(rotation='vertical')\nindx = np.argsort(rf.feature_importances_)[::-1]\nplt.bar(covid_input.columns[indx], rf.feature_importances_[indx])","102c193c":"plt.figure(figsize=(20, 7))\nplt.xticks(rotation='vertical')\nplt.bar(covid_input.columns[indx[:15]], rf.feature_importances_[indx[:15]])","0ba6b89c":"covid_input.columns[indx[:15]]","3c268657":"mlp = MLPClassifier()\nn_dim = 30\nparam_grid_mlp = {'hidden_layer_sizes': [\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim,\n                                          n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim), #18\n    \n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim,\n                                          n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim), #14\n    \n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim)\n                                        ],\n                  'max_iter':[5000],\n                  'activation': ['tanh', 'relu', 'logistic'],\n                  'solver': ['lbfgs', 'sgd', 'adam'],\n                  'learning_rate' : ['constant'],\n                  'learning_rate_init': [0.001, 0.01, 0.1]}\n\nsplited_data = train_test_split(covid_input[covid_input.columns[indx[:15]]],\n                                covid_targt, test_size=0.15, stratify=covid_targt, shuffle=True)\nbest_estimator = experiment_train_kfold(data=splited_data,\n                                        estimator=mlp,\n                                        param_grid=param_grid_mlp,\n                                        scaler=StandardScaler,\n                                        metric='f1')","9106a735":"X_train, X_test, Y_train, Y_test = splited_data\nstd = StandardScaler()\nX_train = std.fit_transform(X_train)\nX_test = std.transform(X_test)\npreds = best_estimator.predict(X_test)\n\nacc = best_estimator.score(X_test, Y_test)\nprint('Accuracy of the training model : {:.2f}%'.format(acc * 100))","786c1f5c":"print(classification_report(Y_test, preds))","7a01a678":"cfm = confusion_matrix(Y_test, preds)\nplt.figure(figsize=(10, 7))\nsns.heatmap(cfm, annot=True, cbar=False, cmap='Blues', fmt='g')","29e2e727":"positive_cases.head()","3d0dac40":"task2_input = positive_cases.drop(labels=['Patient ID',\n                                          'SARS-Cov-2 exam result',\n                                          'Patient addmited to regular ward (1=yes, 0=no)',\n                                          'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                                          'Patient addmited to intensive care unit (1=yes, 0=no)'], axis='columns')\n\ntask2_targt = positive_cases[['Patient addmited to regular ward (1=yes, 0=no)',\n                              'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                              'Patient addmited to intensive care unit (1=yes, 0=no)']]","cd681c9e":"task2_input.head()","b29162b7":"task2_targt.head()","8b791f22":"def experiment_train_kfold(data, estimator, param_grid, scaler, metric='accuracy'):\n    X_train, X_test, Y_train, Y_test = data\n    std = scaler()\n    \n    X_train = std.fit_transform(X_train)\n    X_test = std.transform(X_test)\n\n    kfold = KFold(n_splits=10)\n    grid = GridSearchCV(estimator=estimator, param_grid=param_grid,\n                        scoring=metric, cv=kfold, verbose=1, n_jobs=-1, error_score='raise')\n    grid.fit(X_train, Y_train)\n    return grid.best_estimator_","9b7e4260":"mlp = MLPClassifier()\nn_dim = 30\nparam_grid_mlp = {'hidden_layer_sizes': [\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim,\n                                          n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim), #18\n    \n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim,\n                                          n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim), #14\n    \n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),\n                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim)\n                                        ],\n                  'max_iter':[5000],\n                  'activation': ['tanh', 'relu', 'logistic'],\n                  'solver': ['lbfgs', 'sgd', 'adam'],\n                  'learning_rate' : ['constant'],\n                  'learning_rate_init': [0.001, 0.01, 0.1]}\n\nsplited_data = train_test_split(task2_input, task2_targt, test_size=0.15, stratify=task2_targt, shuffle=True)\nbest_estimator = experiment_train_kfold(data=splited_data,\n                                        estimator=mlp,\n                                        param_grid=param_grid_mlp,\n                                        scaler=StandardScaler,\n                                        metric='f1_micro')","2f723f22":"X_train, X_test, Y_train, Y_test = splited_data\nstd = StandardScaler()\nX_train = std.fit_transform(X_train)\nX_test = std.transform(X_test)\npreds = best_estimator.predict(X_test)\n\nacc = best_estimator.score(X_test, Y_test)\nprint('Accuracy of the training model : {:.2f}%'.format(acc * 100))","817185b5":"print(classification_report(Y_test, preds))","2b8edafc":"from sklearn.metrics import multilabel_confusion_matrix\n\ncfm = multilabel_confusion_matrix(Y_test, preds)\n\nfor idx, mat in enumerate(cfm):\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(mat, annot=True, cbar=False, cmap='Blues', fmt='g')\n    plt.title('Confusion Matrix for class: {}'.format(task2_targt.columns[idx]))","181f752f":"from IPython.display import Image\nImage(\"\/kaggle\/input\/imagesdata\/urine.png\")","c52a85e7":"from IPython.display import Image\nImage(\"\/kaggle\/input\/imagesdata\/blood1.png\")","0d51d09f":"from IPython.display import Image\nImage(\"\/kaggle\/input\/imagesdata\/blood2.png\")","b3eeef4c":"from IPython.display import Image\nImage(\"\/kaggle\/input\/imagesdata\/other-diseases.png\")","42d0c9d2":"from IPython.display import Image\nImage(\"\/kaggle\/input\/imagesdata\/breathing.png\")","6f46d634":"from IPython.display import Image\nImage(\"\/kaggle\/input\/imagesdata\/6-correlated.png\")","5912e5a4":"We can see that some o those last columns have more than 90% of missing data, since the number of positive cases is equal to 558. Let's try to filter out those columns and see what happens.","2a4e64e8":"We are going to categorize into number all of our categorical values, but first lets ger rid for once of these first 6 columns and begin to wokr olny with the data that is going to bed fed into our model","29450bf3":"### Urine\n<a id='urine'><\/a>\n\n- This group comprised only urinary examinations, considering the variables of physical chemistry and also sedimentscopy.\n- In these data, the greatest correlation between a variable and the result, occurred from the variable 'urine pH', which obtained a correlation of -0.47.\n- From this dataset, some other known correlations were also verified, such as density, appearance of the urine and color of the urine, both with a correlation of 0.39. In addition to the correlation between the amount of red blood cells per field and the amount of hemoglobin in the urine, being 0.35.","917543f4":"### Conclusions\n<a id='conclusions'><\/a>\n\n\n- Most of the correlations found were not classified at high levels (> 80%), but several were found at satisfactory levels (> 50%).\n- Correlations, even at low levels, are indicative of promising variables for monitoring patients and should be analyzed with greater criteria.\n- Due to the imbalance of the base, we cannot analyze the real scenario of the correlations, for that we would have to have a base with balanced positive and negative results, in addition to knowing the way the data were collected.\n- There was consistency between the correlations between most variables.\n- Correlations already known from the literature and which have already been described in patients positive for Covid-19, were found in this dataset and are described in the base, confirming trends in laboratory changes.","5ad698d4":"As our fears became true, a lot of these categorical columns are binary and very unexpressive, since the proportions of diferent values is pretty much imbalanced.\n\nLets change the previous chunk of code, detect those binary columns and eliminate all of them who have one othe values representing less tham 8% of the actual valid data","5dbe1af4":"The number of positive cases represents only 558 \/ (558 + 5086 = 5644) ~ 9.8% of the data, lets use those samples to filter the columns and try to make the most out the data","a0e38eb8":"### TASK 2 - Using the Positive cases only to predict the general ward, semi-intensive unit or intensive care unit\u00b6\n\n<a id='task2'> <\/a>\n\nNow that we have and MVP for the task 1, lets focus our efforts on the solutions for task 2. We are going to use only the positive cases for Covid-19 to predict when a patient will be allocated to War, semi-intensive or intesive care unit. This is another classification task, but we are going to take into account only the spectrum of the input variables that is correlated with the positive cases of covid-19.","1c75bc88":"### Blood Biochemical\n<a id='blood_biochem'><\/a>\n\n\n- This group comprised all the variables in the dataset that were related to biochemical tests and metabolic analyzes of the organism.\n- The highest correlation with the result was the variable Dosage of Lipase, in the value of 0.19, in addition to Ferritin, which correlated 0.18 with the aspect. An important point is that, correlation between Ferritin and serum lipase was -1, showing an inversely proportional correlation that, for these data, had maximum compatibility.\n- In addition to this, other correlations were confirmed, such as the influence of serum glucose on liver enzymes ALT and AST, with correlations of 0.41 and 0.48 respectively. In addition to these, serum glucose had an influence on ferritin, with a correlation of 0.9. Another important correlation was that of serum Glucose with the Dosage of Lipase, which obtained a value of -1, being inversely proportional. In other words, the positive results are directly related to the lipase dosage, and this varies inversely with the serum glucose value.\n- Urea and Creatinine, as expected, had a significant correlation, in the amount of 0.52. Urea also maintained a correlation with the total and direct Bilirubin indices, being 0.34 and 0.4 respectively, which is explained by the possible excretion of bilirubin by the kidneys. In addition, Urea obtained a strong correlation with Vitamin B12, having a value of 0.97.\n- Another significant correlation was that of the PCR variable with the Vitamin B12 and Ferritin aspects, with values \u200b\u200bof 0.63 and 0.78 respectively.\n- When dealing with the Vitamin B12 and Ferritin variables, they had significant correlations with almost all the biochemical parameters analyzed.\n- Other related indexes were LDH with the enzyme Phosphatase Alkaline, with a value of 0.56, which is explained because LDH is a marker found also in the liver and AF investigates the presence of liver diseases.","5a9e19df":"# Kaggle Challenge\n\n## Diagnosis of COVID-19 and its clinical spectrum - A.I.Life\n\nAI and Data Science supporting clinical decisions (from 28th Mar to 1st Apr)\n\n![title](https:\/\/img.olhardigital.com.br\/uploads\/acervo_imagens\/2020\/03\/r16x9\/20200330050709_1200_675_-_coronavirus_brasil.jpg)\n\nThe solutions, approaches and techniques here provided, were only possible by the cooperation of the members of A.I.Life team. A \nmultidisciplinary partnership made by PickCells team and several Academic Institutions.\n\n#### PickCells Team:\n\nPedro Buarque - buarque.pedro@pickcells.bio <br>\nAna Clara - ana.clara@pickcells.bio <br>\nDaniela Lopes - danielalopesfreire@gmail.com <br>\nRodrigo Paiva - rodrigo@pickcells.bio <br>\n\n\n#### Academic Partnerships\n\nCarmelo Filho (UPE - University of Pernambuco) - carmelo.filho@upe.br <br>\nAnthony Lins (Unicap - Catholic University of Pernambuco) - anthony.lins@unicap.br <br>\nDiego Pinheiro (UC Davis - University of California, Davis) - pinsilva@ucdavis.edu <br>","d6de386a":"We have 101 columns left ... okay ... but lets check the distribuition of the categorical values on the remainder columns ... that we have saved on the array more_than_one <br>\n\nOn these remaining categorical columns we have 30 variable, let take a look at them","44280e0c":"### Analysis of target distribuition\n<a id='targed_dist'> <\/a>","da1f2ad5":"Since the variables we standardized to have mean zero and standard deviation one, the distribuiton of all numerical variables must ve very likely close to a normal distribuition. lets vizualize if that is consistent","a3431f30":"For each numerical columns we are going to use the SimpleInputer to use the mean of the data","040ed48e":"### Loading essential libraries\u00b6\n<a id='loading_libs'><\/a>","3b98abfa":"Taking a look at this heatmap, I came to think that this entire dataset was build by the merging of other separated data, see how when some variables are present, the absence of other is almost certain. To verify that, lets use the missigno library and check the correlation of the missing data.","f2e732a5":"## A Second Approach to the Data\n<a id='second_app'> <\/a>\n\nLets take the entire that and perform a series of vertical eliminations based on the following rules and see of what we got\n\n\u2022 Step 1: Clean all columns that have only NaN values; <br>\n\u2022 Step 2: Clean all remaining columns that have only one value, or a very unproportional set o values (lets use 10% of its size); <br>\n\u2022 Step 3: Clean all remaining columns that cover only one spectrum of output target variable.","912bb1a8":"Note 1: Strong correlations are considerated values greater than 0.5, this value can be positive (explaining a positive correlation where the data are grows in the same direction, that is, when one grows the other also grows) or negative (explaining an inversely proportional correlation, that is , when one grows, the other decreases).\n\nNote 2: In the case of this specific dataset, we did not have much information about the actual results of the exams (all data were previously normalized) or how they were collected. In addition, we do not know the relationship between the result and the viral load, which directly influences the correlation values, since some variables change at the beginning of the infection and others may change only with the body's immune response or with the impairment of some body systems, in the face of coronavirus infection.","639a144a":"As we can see, there is a lot of missing data, let's take a close look at it using the missgno library","d2ac1d6b":"### General\n<a id='general'><\/a>\n\n- This group had different laboratory variables and mixed with the markers of possible changes, already known with the coronavirus, in addition to markers that are directly related to viral infections.\n- In the analysis of correlation with the result, the most relevant was with Platelets, in the value of -0.28 and Leukocytes, in the value of -0.29, indicating an indirectly proportional relationship and a change where, in cases of positivity for coronavirus, a drop in these values \u200b\u200boccurs, causing thrombocytopenia and lymphopenia.\n- The serum glucose variable had a correlation with the liver enzymes AST and ALT of 0.41 and 0.48 respectively. In addition, it was related to Dosage of Lipase and Albumin at -1 and -0.79 respectively, identifying correlations that are indirectly proportional and that elucidate associated hepatic and pancreatic impairment.\n- In the renal markers, Urea and Creatinine had a connection of 0.52. and Creatinine also had connections with the Dosage of Lipase, in the amount of 0.74.\n- Another interesting marker was Albumin, which had a correlation with LDH dosage at -1, indicating inversely proportional variations and a connection between the markers due to both indicating hepatic impairment, the organ for which Covid-19 has tropism. This compatibility was already known in positive infections for coronavirus where there is an increase in LDH levels and a decrease in albumin levels. Albumin also varied with CRP levels at -0.85 and neutrophil values \u200b\u200bat -1, which is compatible with coronavirus infections where we have a drop in albumin values \u200b\u200band an increase in the inflammatory marker PCR and the number of neutrophils.\n- A different and interesting finding was that the Lipase Dosage correlated with the number of Neutrophils at -1, showing a promising new marker that is correlated with one of the main points of alteration of the coronavirus, the increase in the number of neutrophils.\n- As expected, hematimetric indices correlated with each other, due to their dependencies and the whole set having the same variations.\n- Another expected correlation was that of Neutrophils and Lymphocytes, in the amount of -0.93, which confirms the neutrophilia and lymphopenia present in patients positive for coronavirus.","4bef0bac":"### Training a very first Naive Model\n<a id='naive_model'> <\/a>\n\nLets use this data now, to train in a very naive way ou very first model. First to automate things a little bit, lets define the following function to make and experiment using KFoldCrossValidation and GridSearch for parameter optimization","cd188403":"As we can see, there are a lot of columns with larges amount of missing data. Even if we try to calculate some sort of correlation between the values, we would have to drop those NaN values. So the correlations inferred may be imprecise.\n\nLet's take a closer look on the amount of missing data per columns, and check the viability to discard columns with more that 20% o missing data per row than the size of dataset","004914f6":"#### Numerical Columns","33136b9f":"Well we get rid of 5 columns which we completely full of NaN values, therefore, from an analytical and numerical perspective, those columns are useless, even if they have some meaning in a biological way, we just don't have any data on them. Just for the sake of curiosity lets see which columns are those, and report the so the data collectors can pay more attention.","770a54da":"First lets check which ones have a single values","13bc486b":"We can see that the last columns present a very small amount of usefull data, we could drop all of them and focus on eliminating the rows, but first lets take a look on the distribuition of positive and negative cases of Covid-19","625f29c3":"We see that after cleaning some columns, if we try to drop all rows, we get only 9 instances. Let's see if we can improve that somehow.","ab5b01f4":"Lets categorize into number the categorical columns","61265c65":"lets drop the colums \"Patient ID\" and \"SARS-Cov-2 exam result\", they are of no use in this analysis of categorical variables","a7f5c6c4":"### Visualizing the correlation among the missing data\n<a id='visu_mso'> <\/a>","541b1737":"Since 'Parainfluenza 2', 'Urine - Nitrite', 'Urine - Hyaline cylinders', 'Urine - Granular cylinders', 'Urine - Yeasts' have only a single value, let's drop those columns, with no mercy","5aee53b8":"### A second attempt with MLP\n<a id='mlp_2'> <\/a>\n\nHere we are goin to try once again, the Multi-Layer Perceptron classifier to solve the task 1, but this time, only using the features that were pre-selected by the Random Forets model that we have previously trained.","2c3a565d":"Here we are goin to use our previously inputed data, but only for the positive cases of Covid-19. We are going to drop the columns Patient ID and SARS-Cov-2 exam result, and use as target variables Patient addmited to regular ward, Patient addmited to semi-intensive unit (1=yes, 0=no) and Patient addmited to intensive care unit (1=yes, 0=no)","b692dc4b":"### Inputing by the mean values that segreate the classes\n<a id='mean_clss'> <\/a>","99731c96":"### Transforming the Nominal Variables in Dummies\n<a id='dummy'> <\/a>","68a0719b":"Our very first attempt will be with a neural network model. Since we have a problem of imbalanced data, lets deal with that first.\n\nTo solve a imbalanced data there are 2 ways basically: Generate synthetic data from the classes that have fewer samples, or decrease the amount of samples of thoses classes who has a large amount of data. Since we already generated synthetic values for the columns, we don't want to insert even more synthetic data. So we are goin with the second approach.\n\nWe are goint to split our data into positive and negative cases, a select a number of negative cases that is proportional to the number of positive cases and retrain our model.","53ea5b02":"###  Step 1: Clean all columns that have only NaN values;","48cdbd0f":"###  Step 3: Clean all remaining columns that cover only one spectrum of output target variable.","b61f371c":"### Pay very attention to that chart!\n\nClearly we have a problem of imbalanced data. We have to be VERY careful in the way we make vertical and horizontal elimintation, because we can make the problem of imbalanced data even worse. If we somehow end up with 1000 valid samples and 90% of them are NEGATIVE, its very obvious that any mid range classification algorithm will perform good! It will learn that the positive samples are just NOISE.","0a3ead7a":"In this matrix of correlation of missing data provided by missigno, a correlation of 1 or <1 mean a strong positive correlatio, and -1 a strong negative correlation. According to missigno documentation:\n\n\u2022 A value near -1 means if one variable appears then the other variable is very likely to be missing. <br>\n\u2022 A value near 0 means there is no dependence between the occurrence of missing values of two variables. <br>\n\u2022 A value near 1 means if one variable appears then the other variable is very likely to be present. <br>\n\nSince no negative values are present, our theory about the correlation is not checked, so we cannot have a clear track about the relationships between the missing data and the variables. But at least in a visual way we can see that some sort of pattern is present.","88dd09b0":"###  Step 2: Clean all remaining columns that have only one value, or a very unproportional set o values (lets use 10% of its size);\n\nWell, we know that we have some categorical varibales here right, like \"ParaInfluenza1\", \"Coronaviru HKU1\". Which have possible values (detected, not_detected). But are this values disposed in a proporional way? What if for example te column \"ParaInfluenza1\" only have \"not_detected\" values? In this case this column is useless from a machine learning perpective, because there is no usefull information that we can get from it. It would be that same that append a column full of zeros or a constant value. So let's check those columns","6c6294f3":"### Analysis of Data from the perspective of Biomedicine\n<a id='biomed_analysis'> <\/a>\n\n**Methodology:**\n\n1. Classification of variables. These variables were classified into 6 groups, analyzing the collection material, such as laboratory indications and groups of clinical information related to them, thus: Group 1: Urine; Group 2: Blood - Hematological; Group 3: Blood - Biochemical; Group 4: Correlated diseases; Group 5: Breathing patterns and Group 6: General (in this case, we had a mixture of several groups, such as those changes in cases of viral infections, are not caused by the coronavirus).\n2. Division of the database by the classification of the groups.\n3. Cleaning the database of each group:\n    a. exclusion of columns and rows that contain only null values;\n    b. exclude from columns with a single value.\n4. Conversion and normalization of strategic data. Categorical data were conversions to the numeric format and normalized by equation:\n$$ (variable value - mean (column)) \/ standard_deviation. (1) $$\n5. Generation of heat map with correlation between pair of variables.\n6. Analysis of possible correlations.","cb309c79":"Lets take a look how thing look lik now with a heatmap","021fa1f6":"### Imputing Features in our data\n<a id='imput'> <\/a>","ca63931c":"### Blood - Hematological\n<a id='blood_hemato'><\/a>\n\n\n- This group comprised all the variables in the dataset that had some relationship with the blood tests.\n- In this heat map, the greatest correlations between the variables and the result were the Platelets with the value of -0.28, and the Leukocytes with the value of -0.29, identifying inversely proportional correlations, where if the result is positive, these two variables will shift downward, causing thrombocytopenia and lymphocytopenia.\n- In addition to these, some correlations of two or more variables were found, elucidating the direct interferences of each other. The correlations confirmed from these data were hematocrit and hemoglobin directly, in addition to the two variables with the number of erythrocytes, which were 0.87 and 0.84. As expected and known, all hematimetric indices were correlated with each other.\n- Another interesting correlation was the number of platelets with the number of leukocytes, being directly proportional to 0.44 (that is, both will follow the same trend and, if one falls the other will also fall, if one goes up in number of blood, the other will follow the same pattern).\n- Another correlation in the dataset was between lymphocytes and neutrophils, this being an inversely proportional correlation, in the value of -0.93, indicating that one of the indices will rise and the other will fall during the infection. This aspect is well known in patients positive for Coronavirus, who generally have relevant neutrophilia and lymphopenia.\n- In addition to this, neutrophils and leukocytes obtained a directly proportional correlation of 0.4, which indicates the same pattern in both, either increase or decrease, a correlation that makes sense from the biological point of view where in positive cases we have an increase in both. indices, causing neutrophilia and leukocytosis. Lymphocytes and leukocytes were also correlated, in the value of -0.33, elucidating an inversely proportional aspect (which would explain lymphopenia and leukocytosis in positive patients).","2590f0b8":"# Table of Contents\n\n1. [Loading essential libraries](#loading_libs)\n2. [Loading initial raw data](#loading_data)\n3. [Data munging and preparation](#data_manip) \n4. [Analysis of target distribuition](#targed_dist)\n5. [Visualizing the correlation among the missing data](#visu_mso)\n6. [A Second Approach to the Data](#second_app)\n7. [Imputing Features in our data](#imput)\n8. [Training a very first Naive Model](#naive_model)\n9. [Inputing by the mean values that segreate the classes](#mean_clss)\n10. [Transforming the Nominal Variables in Dummies](#dummy)\n11. [Measuring Features Importances with Random Forests](#feature_import)\n12. [TASK 1 - A second attempt with MLP](#mlp_2)\n13. [TASK 2 - Using the Positive to predict the general ward, semi-intensive unit or intensive care unit](#task2)\n14. [Analysis of Data from the perspective of Biomedicine](#biomed_analysis)<br>\n    1. [Urine](#urine)<br>\n    2. [Blood - Hematological](#blood_hemato)<br>\n    3. [Blood - Biochemical](#blood_biochem)<br>\n    4. [Correlated diseases](#corr_diseas)<br>\n    5. [Breathing patterns](#breathing)<br>\n    6. [General](#general)<br>\n    7. [Conclusions](#conclusions)","a96d230c":"#### Categorical Columns","7e6cb13a":"### Data munging and preparation\n<a id='data_manip'> <\/a>\n\nLets take a closer look and analyze the data presented here, we don't want just to throw random values in a classification model an pay attention to an unrealistic value of accuray, let's analyze the nature and distribuition of our data.","17ade439":"### Correlated Diseases\n<a id='corr_diseas'><\/a>\n\n\n- In this group, the other pathologies that were present in the dataset were placed, which were tested in the patients in question. The vast majority cover respiratory infections.\n- None of the diseases tested had correlations with the result for Covid-19, all the correlations found were below 0.1, which explains that the result of the coronavirus test does not depend on the result of other diseases, it does not impact on the positivity for any other respiratory disease.\n- However, in addition to the result aspect, it was found that, with the exception of the variable Strepto A, all respiratory diseases had a high correlation with each other. Almost all of them were direct correlations, around 1 when they were positive (0.98 or 0.99). Which means that these respiratory diseases are related and that their positivity can appear together.\n- One aspect that drew attention was the variables of Rapid Tests for Influences A and B, which obtained negative and considerable correlations with all other respiratory diseases, around 0.65. This means that, in cases of acute infections by Influenza A or B, there is usually no infection by the other respiratory viruses analyzed in the dataset (this does not include the new Coronavirus, which is included in the parameter 'result').","85660c96":"Let's take a closer look at the feature names","b8071d9e":"### Breathing Patterns\n<a id='breathing'><\/a>\n\n- In this group, only the respiratory variables included in the dataset were separated, including venous and arterial measurements related to respiratory patterns.\n- Among the data included in the dataset, those that had the greatest correlation with the result were the variables pCO2 (arterial blood gas analysis), in the value of -0.32; pH (arterial blood gas analysis), in the value of -0.31; pO2 (arterial blood gas analysis), with a value of -0.31 and Arteiral Fio2, with a value of -0.31. These, therefore, would be the most altered variables when it comes to breathing patterns.\n- Even if the correlations are small, the results are relevant if we analyze that, for positive cases of coronavirus, these variables will be reduced, because the correlations are negative, therefore indirectly proportional. What is relevant is that, in positive patients, we have a decreased respiratory rate, which is characterized by the shortness of breath symptom strongly present in patients positive for coronavirus.\n- Regarding the direct correlations between the parameters, we had several correlated parameters, since they are from the same group and all are respiratory parameters and change together with codependency.","7414ae12":"We can see from the visual analysis that the columns 'Albumin', 'Vitamin B12', 'Fio2 (venous blood gas analysis)' appears only once. We can immediatelly delete those columns","0c764b3b":"Once more we are going to ignore the first six columns. let's try a visual aproach first","a0825e2d":"With the diffent approach for inputing by class, and using the categorization strategy we could achied an almost perfect score with the Random Forest. Lets use the features that we could see from the feature importances from the Random Forest to train again the MLP and check if another model can in fact use those particular features to learn and have an improved performance.","c9b6359d":"### Measuring Features Importances with Random Forests\n<a id='feature_import'> <\/a>","c819cbda":"### Loading initial raw data\n<a id='loading_data'><\/a>\n","20ebea6c":"Once more, lets try again ou old friend MLP, since the output variables are alredy in the ONE HOT ENCONDING format! Lets make a simple training and see what happens."}}