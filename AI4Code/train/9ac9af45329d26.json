{"cell_type":{"78abd5fe":"code","d28ead2e":"code","ec2d54da":"code","67bce008":"code","2462396b":"code","aa57868e":"code","5b6fe103":"code","912dcf05":"code","a4ff5673":"code","0bb26532":"code","16dd1fcd":"code","2041abff":"code","3186c1c6":"code","cff949c5":"code","bff638b6":"code","ea559ba1":"code","600c20be":"code","a2c80310":"code","50f45090":"code","589d366a":"code","bf56ed9f":"code","bbd13621":"code","a3391ee7":"code","da25a413":"code","87a6782f":"code","9444be28":"code","2004b52c":"code","8e04d1ff":"code","d87a06c2":"code","0abd2371":"code","19ab3294":"code","466bfdf5":"markdown"},"source":{"78abd5fe":"# Eda \u0130nan - Student ID: 20151708034\n# Rojda Becerikli - Student ID: 20151708024\n# Selin Filiz - Student ID: 20141708018","d28ead2e":"%%capture\n!pip install -U feature-engine\n!pip install -U imbalanced-learn","ec2d54da":"import pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport imblearn as imb\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom feature_engine.encoding import RareLabelEncoder\nfrom feature_engine.encoding import OneHotEncoder\nfrom feature_engine.imputation import MeanMedianImputer\nfrom feature_engine.imputation import CategoricalImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 500)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","67bce008":"train = pd.read_csv('..\/input\/it-405-dm-and-bi-2020\/train.csv')\ntest = pd.read_csv('..\/input\/it-405-dm-and-bi-2020\/test.csv')\nsample_submission = pd.read_csv('..\/input\/it-405-dm-and-bi-2020\/sample_submission.csv')\n\ntarget = train[\"Target\"]\ntrain = train.drop([\"Id\", \"Target\"], axis=1)\ntest.drop([\"Id\"], axis=1, inplace=True)","2462396b":"missing_rate = train.isna().sum() \/ len(train)\n\ntrain = train.loc[:, missing_rate < 0.75]\ntest = test.loc[:, missing_rate < 0.75]","aa57868e":"print('Train shape:      ',train.shape)\nprint('Target shape:     ',target.shape)\nprint('Test shape:       ',test.shape)\nprint('Submission shape: ',sample_submission.shape)","5b6fe103":"#Finding categorical and numerical columns\ncategorical_cols = train.select_dtypes(exclude=np.number).columns.to_list()\nnumerical_cols = train.select_dtypes(include=np.number).columns.to_list()","912dcf05":"#Writing which colums has nan values \ntrain_nan_cat_cols = []\nfor col in categorical_cols:\n    if train[col].isna().sum() > 0:\n        train_nan_cat_cols.append(col)\n        \ntrain_nan_num_cols = []\nfor col in numerical_cols:\n    if train[col].isna().sum() > 0:\n        train_nan_num_cols.append(col)          \n        \nprint('Categorical Columns containing nan values: \\n',train_nan_cat_cols)   \nprint('\\nNumber of Categorical Columns containing nan values: ',len(train_nan_cat_cols))\nprint('\\n\\nNumerical Columns containing nan values: \\n',train_nan_num_cols)   \nprint('\\nNumber of Numerical Columns containing nan values: ',len(train_nan_num_cols))","a4ff5673":"#Filling nan values with mode and median \ntrain[train_nan_cat_cols] = train[train_nan_cat_cols].fillna(train[train_nan_cat_cols].mode().iloc[0])\ntrain[train_nan_num_cols] = train[train_nan_num_cols].fillna(train[train_nan_num_cols].mean().iloc[0])\n\n#Last check, is there a column that contains nan values\nfor col in train.columns:\n    if train[col].isna().sum() > 0:\n        print(f'{col} is contain nan values')","0bb26532":"train[numerical_cols].describe().T","16dd1fcd":"corr_data = train.copy()\ncorr_data['Target'] = target\ncorrmat = corr_data.corr()\ncorrmat.style.background_gradient(cmap='coolwarm')","2041abff":"k = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'Target')['Target'].index\ncm = np.corrcoef(corrmat[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","3186c1c6":"def preprocess_pipeline(train):\n    numerical_col_median_imputer = MeanMedianImputer(imputation_method='mean', variables = numerical_cols)\n    numerical_col_median_imputer.fit(train)\n    x = numerical_col_median_imputer.transform(train)\n\n    categorical_col_imputer = CategoricalImputer(imputation_method = 'frequent', variables = categorical_cols)\n    categorical_col_imputer.fit(x)\n    x = categorical_col_imputer.transform(x)\n\n    cardinality_reduce = RareLabelEncoder(variables = categorical_cols)\n    cardinality_reduce.fit(x)\n    x = cardinality_reduce.transform(x)\n\n    categorical_to_ohe = OneHotEncoder(variables = categorical_cols)\n    categorical_to_ohe.fit(x)\n    x = categorical_to_ohe.transform(x)\n\n    normalizer = StandardScaler()\n    normalizer.fit(x)\n    x = normalizer.transform(x)\n    return x","cff949c5":"def optimumClassifier(x_train, y_train, x_test, y_test):\n    classifiers = [[XGBClassifier(),'XGB Classifier'], [GradientBoostingClassifier(), 'GradientBoosting Classifier'], [RandomForestClassifier(),'Random Forest'], \n    [KNeighborsClassifier(), 'K-Nearest Neighbours'], [SGDClassifier(),'SGD Classifier']]\n    score_list = []\n    cross_val_list = []\n    classifier_score_dict = {}\n    models = {}\n\n    for classifier in classifiers:\n        model = classifier[0]\n        model.fit(x_train, y_train)\n        model_name = classifier[1]\n\n        score = model.score(x_test, y_test)\n        cv = sk.model_selection.StratifiedKFold(n_splits=3, random_state=42,shuffle=True)\n        cross_val = sk.model_selection.cross_val_score(model, x_test, y_test, cv=cv, scoring='roc_auc').mean()\n\n        score_list.append(score)\n        cross_val_list.append(cross_val)\n\n        print(model_name, 'model score:     ' + str(round(score*100, 2)) + '%')\n        print(model_name, 'cross val roc_auc score: ' +str(round(cross_val*100, 2)) + '%')\n        \n        classifier_score_dict[model_name] = {'score':score,'roc_auc':cross_val}\n        models[model_name] = model\n        \n        if model_name != classifiers[-1][1]:\n            print('')\n    return classifier_score_dict, models","bff638b6":"# Tranforming training data \ntrain_ = preprocess_pipeline(train)\n\n# Spliting the data to train and test data\nx_train, x_test, y_train, y_test = sk.model_selection.train_test_split(train_, target, test_size=0.3, random_state=42)","ea559ba1":"classifier_score_dict, models = optimumClassifier(x_train, y_train,x_test, y_test)\nclassifier_df = pd.DataFrame(classifier_score_dict).T\nprint(classifier_df)","600c20be":"fig, ax =plt.subplots(1,2,figsize=(15,5))\na = sns.barplot(x=classifier_df.index, y=\"score\", data=classifier_df,ci=68,ax=ax[0])\na.set_xticklabels(a.get_xticklabels(), \n                          rotation=90, \n                          horizontalalignment='center')\na.set_title('Score')\nfor p in a.patches:\n    a.annotate(format(p.get_height(), '.4f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \nb = sns.barplot(x=classifier_df.index, y=\"roc_auc\", data=classifier_df,ci=68,ax=ax[1],)\nb.set_xticklabels(b.get_xticklabels(), \n                          rotation=90, \n                          horizontalalignment='center')\nb.set_title('Roc-Auc Score')\nfor p in b.patches:\n    b.annotate(format(p.get_height(), '.4f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')    \n    \nplt.show()","a2c80310":"xgb_model = models['XGB Classifier']\ngb_model = models['GradientBoosting Classifier']\n\nxgb_prediction = xgb_model.predict(x_test)\ngb_prediction = gb_model.predict(x_test)","50f45090":"title_model = [('XGBClassifier Confusion Matrix',xgb_prediction),\n                ('GradientBoostingClassifier  Confusion Matrix',gb_prediction)]\nfig, ax= plt.subplots(1,2,figsize=(15,5))\na = 0\nfor title,pred in title_model: \n    cm = sk.metrics.confusion_matrix(y_test, pred)\n    sns.heatmap(cm, annot=True, ax = ax[a],fmt='g') #annot=True to annotate cells\n    ax[a].set_xlabel('Predicted labels')\n    ax[a].set_ylabel('True labels') \n    ax[a].set_title(title) \n    ax[a].xaxis.set_ticklabels(['Class 0', 'Class 1'])\n    ax[a].yaxis.set_ticklabels(['Class 0', 'Class 1'])\n    a += 1","589d366a":"target_names = ['Class 0','Class 1']\nprint('XGBClassifier: \\n',sk.metrics.classification_report(y_test,xgb_prediction,target_names=target_names))\nprint('GradientBoostingClassifier : \\n',sk.metrics.classification_report(y_test,gb_prediction,target_names=target_names))","bf56ed9f":"def modelfit(alg, dtrain, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    import xgboost as xgb\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain, label=target.values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n                    metrics='auc', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain, target,eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain)\n    dtrain_predprob = alg.predict_proba(dtrain)[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % sk.metrics.accuracy_score(target, dtrain_predictions))\n    print (\"AUC Score (Train): %f\" % sk.metrics.roc_auc_score(target, dtrain_predprob))","bbd13621":"xgb_model = XGBClassifier(learning_rate =0.15,\n                     n_estimators=1000,\n                     max_depth=12,\n                     min_child_weight=1,\n                     gamma=0,\n                     subsample=0.7,\n                     colsample_bytree=0.7,\n                     objective= 'binary:logistic',\n                     nthread=4,\n                     scale_pos_weight=3,\n                     seed=27)\nmodelfit(xgb_model, train_, target)","a3391ee7":"param_test1 = {'n_estimators':range(20,81,10)}\ngsearch1 = sk.model_selection.GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.02, \n                                                               min_samples_split=500,\n                                                               min_samples_leaf=50,\n                                                               max_depth=8,\n                                                               max_features='sqrt',\n                                                               subsample=0.8,\n                                                               random_state=10), \n                        param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(train_,target)","da25a413":"gsearch1.best_params_ ,gsearch1.best_score_","87a6782f":"final_pipeline = Pipeline([\n     # replace NA by the mean in variables below, they are numerical\n     ('numerical_col_median_imputer', MeanMedianImputer(imputation_method='mean', variables = numerical_cols)),\n    \n     # replace NA by adding the most frequent value in categorical variables\n     ('categorical_col_imputer', CategoricalImputer(imputation_method = 'frequent', variables = categorical_cols)),\n    \n     # reduce the cardinality of categorical columns grouping rare values into \"Rare\" value\n    (\"cardinality_reduce\", RareLabelEncoder(variables = categorical_cols)),\n    \n    # Categorical value to One-hot encoding\n    ('categorical_to_ohe', OneHotEncoder(variables = categorical_cols)),\n    \n    # Normalize features\n    ('normalizer', StandardScaler()),\n    \n    # XGBoost for Classification Model\n    ('classifier', gsearch1)\n])","9444be28":"cv = sk.model_selection.StratifiedKFold(n_splits=10, random_state=42)\ncross_val_scores = sk.model_selection.cross_val_score(final_pipeline, train, target, cv=cv, scoring='roc_auc')","2004b52c":"print('GradientBoosting Classifier roc_auc Score: ', np.mean(cross_val_scores))","8e04d1ff":"final_pipeline.fit(train,target)\npred = final_pipeline.predict(train)","d87a06c2":"target_names = ['Class 0','Class 1']\n\nprint('Classification Report\\n\\n',sk.metrics.classification_report(target,pred,target_names=target_names))","0abd2371":"\nsample_submission['Target'] = final_pipeline.predict_proba(test)[:, 1]\nsample_submission.head()","19ab3294":"sample_submission.to_csv('eda_inan_final_submission.csv',index=False)","466bfdf5":"XGBClassifier and GradientBoosting have highest accuracy so we look these models confusion matrix"}}