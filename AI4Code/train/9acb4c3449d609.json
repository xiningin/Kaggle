{"cell_type":{"2cad3ea8":"code","3156622b":"code","2a7d1530":"code","6954abe3":"code","df663073":"code","d879161f":"code","727d8cc5":"code","3de1ef58":"code","b0618dc4":"code","bbae47a8":"code","543a5c94":"code","e0e76720":"code","9d74a0dc":"code","3d8e376c":"code","9e3a2838":"code","b5341dfb":"code","1b2448f9":"code","ca5f11b0":"code","ea833b92":"code","cee887b4":"code","a61fd0cf":"code","a138046b":"code","d2582170":"code","8df7a44e":"code","639ab20f":"code","4f61c8f8":"code","e29b10bb":"code","a43aaaf0":"code","4fb72b7d":"code","f3758c49":"code","5c27310e":"code","e20420cd":"code","2c891727":"code","efd31644":"markdown","1ab98857":"markdown","188c8912":"markdown","42290efe":"markdown","c35dea87":"markdown","6ccdc041":"markdown","0c915073":"markdown","c73d0dc3":"markdown"},"source":{"2cad3ea8":"!pip install dexplot -q\n!pip install datasist -q","3156622b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\nimport dexplot as dxp\nimport plotly.express as px\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport datasist as ds\n%matplotlib inline\n\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom scipy.stats import probplot,norm\nimport warnings\nwarnings.filterwarnings('ignore')\nDATA_PATH = '..\/input\/abzoobaassessmentdata\/'\n","2a7d1530":"train_df = pd.read_csv(DATA_PATH + 'train.csv')\ntest_df = pd.read_csv(DATA_PATH + 'test.csv')\nsubmission_data = pd.read_csv(DATA_PATH + 'sample_submission.csv')","6954abe3":"train_df.head(50)","df663073":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent = (train_df.isnull().sum()\/train_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","d879161f":"total = test_df.isnull().sum().sort_values(ascending=False)\npercent = (test_df.isnull().sum()\/test_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","727d8cc5":"for Columns in ['id','address','state','zipcode','longitude','latitude']:\n    train_df = train_df.drop(Columns,axis=1)\n    test_df = test_df.drop(Columns,axis=1)","3de1ef58":"def impute_nan_most_frequent_category(data,colname):\n    most_frequent_category=data[colname].mode()[0]\n    data[colname].fillna(most_frequent_category,inplace=True)","b0618dc4":"def impute_nan_create_category(data,colname):\n    data[colname]=np.where(data[colname].isnull(),\"Unknown\",data[colname])","bbae47a8":"for Columns in ['property_type']:\n    impute_nan_most_frequent_category(train_df,Columns)\n    impute_nan_most_frequent_category(test_df,Columns)","543a5c94":"for Columns in ['county']:\n    impute_nan_create_category(train_df,Columns)\n    impute_nan_create_category(test_df,Columns)","e0e76720":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent = (train_df.isnull().sum()\/train_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","9d74a0dc":"total = test_df.isnull().sum().sort_values(ascending=False)\npercent = (test_df.isnull().sum()\/test_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","3d8e376c":"train_df_chi = train_df.copy()\nX = train_df_chi.iloc[:,0:29] \ny = train_df_chi.iloc[:,-1]  ","9e3a2838":"for f in X.columns:\n    if X[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(X[f].values))\n        X[f] = lbl.transform(list(X[f].values))","b5341dfb":"bestfeatures = SelectKBest(score_func=chi2, k=8)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns) \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Features','Score']  \nprint(featureScores.nlargest(10,'Score'))\n","1b2448f9":"cols_to_drop = ['bath','bed']","ca5f11b0":"corrmat = train_df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(15,15))\ng=sns.heatmap(train_df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","ea833b92":"## Zoomed out correlation map\nk = 10\ncols = corrmat.nlargest(k, 'rent')['rent'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True,cmap = 'RdYlGn', fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","cee887b4":"print(train_df.shape)\nprint(test_df.shape)","a61fd0cf":"bin_labels_3 = ['Low', 'Medium', 'High']\ntrain_df['rent_bins'] = pd.cut(train_df['rent'],3,labels=bin_labels_3)\ntest_df['rent_bins'] = pd.cut(train_df['rent'],3,labels=bin_labels_3)\ntrain_df['income_bins'] = pd.cut(train_df['Census_MedianIncome'],3,labels=bin_labels_3)\ntest_df['income_bins'] = pd.cut(test_df['Census_MedianIncome'],3,labels=bin_labels_3)\ntrain_df.head()","a138046b":"test_df.head()","d2582170":"train_df['total_bathrooms'] = (train_df['bath'] + (0.5 * train_df['halfbath']))\ntest_df['total_bathrooms'] = (test_df['bath'] + (0.5 * test_df['halfbath']))","8df7a44e":"train_df['total_miles'] = (train_df['cemetery_dist_miles'] + (train_df['nationalhighway_miles']) + train_df['railline_miles'] +\n                          train_df['starbucks_miles'] + train_df['walmart_miles'] + train_df['hospital_miles'] + train_df['physician_dist_miles'] +\n                          +train_df['dentist_dist_miles'] + train_df['opt_dist_miles'] + train_df['vet_dist_miles'] + train_df['farmers_miles'] )\n\ntest_df['total_miles'] = (test_df['cemetery_dist_miles'] + (test_df['nationalhighway_miles']) + test_df['railline_miles'] +\n                          test_df['starbucks_miles'] + test_df['walmart_miles'] + test_df['hospital_miles'] + test_df['physician_dist_miles'] +\n                          +test_df['dentist_dist_miles'] + test_df['opt_dist_miles'] + test_df['vet_dist_miles'] + test_df['farmers_miles'] )\n\n","639ab20f":"print(train_df.shape)\nprint(test_df.shape)","4f61c8f8":"fig, axes = plt.subplots(1, 2,figsize = (15,5))\nsns.distplot(train_df['rent'], fit=norm,ax = axes[0])\nres = probplot(train_df['rent'], plot=axes[1])\nprint(\"Skewness: %f\" % train_df['rent'].skew())\nprint(\"Kurtosis: %f\" % train_df['rent'].kurt())","e29b10bb":"train_df['rent'] = np.log(train_df['rent'])\nfig, axes = plt.subplots(1, 2,figsize = (15,5))\nsns.distplot(train_df['rent'], fit=norm,ax = axes[0])\nres = probplot(train_df['rent'], plot=axes[1])\nprint(\"Skewness: %f\" % train_df['rent'].skew())\nprint(\"Kurtosis: %f\" % train_df['rent'].kurt())","a43aaaf0":"train_df = train_df.drop(cols_to_drop,1)\ntest_df = test_df.drop(cols_to_drop,1)","4fb72b7d":"encoder = LabelEncoder()\ntrain_df['county'] = encoder.fit_transform(train_df['county'])\ntrain_df['city'] = encoder.fit_transform(train_df['city'])\ntest_df['county'] = encoder.fit_transform(test_df['county'])\ntest_df['city'] = encoder.fit_transform(test_df['city'])","f3758c49":"preprocessed_train = pd.get_dummies(train_df,drop_first=True)\npreprocessed_test = pd.get_dummies(test_df,drop_first=True)","5c27310e":"preprocessed_train.sample(10)","e20420cd":"print(preprocessed_train.shape)\nprint(preprocessed_test.shape)","2c891727":"preprocessed_train.to_csv('preprocessed_train.csv',index=False)\npreprocessed_test.to_csv('preprocessed_test.csv',index=False)","efd31644":"#### --> Dropping Irrelevant Columns from data(train and test)\n* Dropping property_id,address and zip codes as they don't have any significant relationship to depenedent variable.\n* Dropping latitude and longitude as their relationship is highly non linear insteal will use city feature.\n\n\n#### --> Impute Missing Values (Using Mode and Imputing \"Unknown\" as category)","1ab98857":"### --> Performing Label Encoding and Dummies\n\n* Due to High cardinality for county and city label encoder is used.\n* Other Discrete variables encoded using get_dummies() method.","188c8912":"## END","42290efe":"### 2.2  Feature Selection \n#### --> Chi square and Correlation Heatmap","c35dea87":"#### Conclusion --> Since Bath & Bed Independent variables have less Chi Square Score hence it will be dropped after feature Engineering.","6ccdc041":"## Stage II.Data Preprocessing\n### 2.1 Data Preprocessing-Missing value Treatment","0c915073":"### 2.2 Feature Engineering\n#### --> Extracting possible features from data based on Domain Knowledge that will boost Regression model efficacy.","c73d0dc3":"###  2.3 Normality Tests\n#### Removing Skewness - Normalizing Dependent feature with Log Transformation( Histogram and scipy's Normal Prob Plot)"}}