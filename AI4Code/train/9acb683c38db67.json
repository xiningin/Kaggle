{"cell_type":{"035ad65c":"code","a0264b8d":"code","39390d4e":"code","7583a73c":"code","9c86d0bb":"code","14c32ac9":"code","2ae46a46":"code","60027116":"code","87841e73":"code","5720391f":"code","871e9125":"code","f1755549":"code","ad695321":"markdown","4a641f94":"markdown","12c1f0ca":"markdown","4f432681":"markdown","e3b14323":"markdown","46ffcedf":"markdown","e58a969b":"markdown","8e6c5f0f":"markdown","d7ebefed":"markdown","01209e2d":"markdown","8ff54b30":"markdown","af25182f":"markdown","9601ed43":"markdown","9aa8788a":"markdown","b0a4e7ad":"markdown","0aa72bfe":"markdown"},"source":{"035ad65c":"### Import the required packages\nimport scipy as sp\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\nimport matplotlib.transforms as transforms\nimport seaborn as sns\nimport math\n\n### Set theme\nplt.style.use('seaborn')\nsns.set_style(\"darkgrid\")\n\n### Define the colour scheme\nc1 = \"#173f5f\"\nc2 = \"#20639b\"\nc3 = \"#3caea3\"\nc4 = \"#f6d55c\"\nc5 = \"#ed553b\"\n\nprint(\"Imported the required packages successfully!\")\n\ndef confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n    \"\"\"\n    Create a plot of the covariance confidence ellipse of *x* and *y*.\n\n    Parameters\n    ----------\n    x, y : array-like, shape (n, )\n        Input data.\n\n    ax : matplotlib.axes.Axes\n        The axes object to draw the ellipse into.\n\n    n_std : float\n        The number of standard deviations to determine the ellipse's radiuses.\n\n    Returns\n    -------\n    matplotlib.patches.Ellipse\n\n    Other parameters\n    ----------------\n    kwargs : `~matplotlib.patches.Patch` properties\n    \"\"\"\n    if x.size != y.size:\n        raise ValueError(\"x and y must be the same size\")\n\n    cov = np.cov(x, y)\n    pearson = cov[0, 1]\/np.sqrt(cov[0, 0] * cov[1, 1])\n    # Using a special case to obtain the eigenvalues of this\n    # two-dimensionl dataset.\n    ell_radius_x = np.sqrt(1 + pearson)\n    ell_radius_y = np.sqrt(1 - pearson)\n    ellipse = Ellipse((0, 0),\n        width=ell_radius_x * 2,\n        height=ell_radius_y * 2,\n        facecolor=facecolor,\n        **kwargs)\n\n    # Calculating the stdandard deviation of x from\n    # the squareroot of the variance and multiplying\n    # with the given number of standard deviations.\n    scale_x = np.sqrt(cov[0, 0]) * n_std\n    mean_x = np.mean(x)\n\n    # calculating the stdandard deviation of y ...\n    scale_y = np.sqrt(cov[1, 1]) * n_std\n    mean_y = np.mean(y)\n\n    transf = transforms.Affine2D() \\\n        .rotate_deg(45) \\\n        .scale(scale_x, scale_y) \\\n        .translate(mean_x, mean_y)\n\n    ellipse.set_transform(transf + ax.transData)\n    return ax.add_patch(ellipse)","a0264b8d":"n = 1000\n\nX = sp.stats.norm.rvs(loc=0, scale=1, size=n, random_state=123)\nY = sp.stats.norm.rvs(loc=0, scale=1, size=n, random_state=321)\n\n### Emperical CDF\ncdf_y = np.arange(1, n+1) \/ n\nX_cdf_x = np.sort(X)\nY_cdf_x = np.sort(Y)\n\n### Correlation\ncorrelation_sample = np.corrcoef(X, Y)\nprint(\"Sample Correlation: {:.1%}\".format(correlation_sample[1,0]))\n\n### Expected Value\nX_mean = np.mean(X)\nY_mean = np.mean(Y)\n\nX2 = np.square(X)\nY2 = np.square(Y)\n\n### Variance\nvar = (np.mean(X2)*np.mean(Y2)) - ((np.mean(X)**2)*(np.mean(Y)**2))\n\nprint(\"Sample Expected Value: ({:,.2f},{:,.2f})\".format(X_mean, Y_mean))\nprint(\"Sample Variance: {:,.2f}\".format(var))\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\nax[0].step(X_cdf_x, cdf_y, where='post', color=c1, label=r\"Marginal Distribution $X$\")\nax[0].step(Y_cdf_x, cdf_y, where='post', color=c5, label=r\"Marginal Distribution $Y$\")\nax[0].set(title=\"Cumulative Distribution Function\", xlabel=\"Random Variates\", ylabel=\"Cumulative Probability\")\nax[0].legend()\n\nax[1].scatter(X, Y, color=c1, alpha=0.7, label=\"Joint Distribution\")\nconfidence_ellipse(X, Y, ax=ax[1],alpha=0.2, facecolor=c2, edgecolor=c2, zorder=0, label=\"3 Standard Deviations\",)\nax[1].set(title=\"Joint Distribution Scatter Plot\", xlabel=\"X\", ylabel=\"Y\")\nax[1].legend(bbox_to_anchor=(1,1), loc=\"upper left\")\nplt.show()","39390d4e":"### Variable Parameters\ncorrelation = 0.8\nx1 = 2\n\n### Fixed Parameters\nmu_X = 0\nstd_X = 1\nmu_Y = 0\nstd_Y = 1\n\n### Theoretical x1 PDF\nx1_pdf_x = np.linspace(-4, 4, 100)\nx1_pdf_y = sp.stats.norm.pdf(x=x1_pdf_x, loc=0, scale=1)\n\n### Emperical x1 PDF\nx1_pdf = sp.stats.norm.pdf(x=x1, loc=0, scale=1)\n\n### Expected Value and Standard Deviation of y1\nE_y1 = mu_Y + (correlation * std_Y * ( (x1 - mu_X)\/std_X) )\nstd_y1 = std_Y * math.sqrt(1 - correlation**2)\n\n### Theoretical y1 PDF\ny1_pdf_x = np.linspace(-4, 4, 100)\ny1_pdf_y = sp.stats.norm.pdf(x=y1_pdf_x, loc=E_y1, scale=std_y1)\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6))\n\nax.scatter(x1, x1_pdf, color=c1, s=100, label=\"_nolabel_\")\nax.plot(x1_pdf_x, x1_pdf_y, color=c1, label=r\"$X\\sim N(0,1)$\")\nax.plot(y1_pdf_x, y1_pdf_y, color=c5, label=r\"$(y_1|x_1={}) \\sim N({:,.1f},{:,.1f})$\".format(x1,E_y1,std_y1))\nax.annotate(r'$x_1={}$'.format(x1), xy=(x1+0.05, x1_pdf+0.01), color=c1)\n\nax.set(title=\"Probability Density Function\", xlabel=\"Random Variates\", ylabel=\"Density\")\nax.legend()\nplt.show()","7583a73c":"n = 1000\n\nZ_x = sp.stats.norm.rvs(loc=0, scale=1, size=n, random_state=123)\nZ_y = sp.stats.norm.rvs(loc=0, scale=1, size=n, random_state=321)\n\n### Emperical CDF\ncdf_y = np.arange(1, n+1) \/ n\nX_cdf_x = np.sort(Z_x)\nY_cdf_x = np.sort(Z_y)\n\n### Correlation\ncorrelation_sample = np.corrcoef(X, Y)\n\n### Expected Value\nX_mean = np.mean(Z_x)\nY_mean = np.mean(Z_y)\n\nX2 = np.square(Z_x)\nY2 = np.square(Z_y)\n\n### Variance\nvar = (np.mean(X2)*np.mean(Y2)) - ((np.mean(X)**2)*(np.mean(Y)**2))\n\nprint(\"Sample Expected Value: ({:,.2f},{:,.2f})\".format(X_mean, Y_mean))\nprint(\"Sample Variance: {:,.2f}\".format(var))\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\nax[0].step(X_cdf_x, cdf_y, where='post', color=c1, label=r\"Marginal Distribution $Z_X$\")\nax[0].step(Y_cdf_x, cdf_y, where='post', color=c5, label=r\"Marginal Distribution $Z_Y$\")\nax[0].set(title=\"Cumulative Distribution Function\", xlabel=\"Random Variates\", ylabel=\"Cumulative Probability\")\nax[0].legend()\n\nax[1].scatter(Z_x, Z_y, color=c1, alpha=0.7, label=\"Joint Distribution\")\nconfidence_ellipse(Z_x, Z_y, ax=ax[1],alpha=0.2, facecolor=c2, edgecolor=c2, zorder=0, label=\"3 Standard Deviations\",)\nax[1].set(title=\"Joint Distribution Scatter Plot\", xlabel=r\"$Z_X$\", ylabel=r\"$Z_Y$\")\nax[1].legend(bbox_to_anchor=(1,1), loc=\"upper left\")\nplt.show()","9c86d0bb":"correlation = 0.8\nn = 1000\n\nZ_x = sp.stats.norm.rvs(loc=0, scale=1, size=n, random_state=123)\nZ_y = sp.stats.norm.rvs(loc=0, scale=1, size=n, random_state=321)\n\n# Construct the correlation matrix and Cholesky Decomposition\nrho = np.matrix([[1, correlation], [correlation, 1]])\ncholesky = np.linalg.cholesky(rho)\n\nZ = np.matrix([Z_x, Z_y])\n\nZ_XY = cholesky * Z\n\nX = np.array(Z_XY[0,:]).flatten()\nY = np.array(Z_XY[1,:]).flatten()\n\n### Emperical CDF\ncdf_y = np.arange(1, n+1) \/ n\nX_cdf_x = np.sort(X)\nY_cdf_x = np.sort(Y)\n\n### Correlation\ncorrelation_sample = np.corrcoef(X, Y)\n\n### Expected Value\nX_mean = np.mean(X)\nY_mean = np.mean(Y)\n\nX2 = np.square(X)\nY2 = np.square(Y)\n\n### Variance\nvar = (np.mean(X2)*np.mean(Y2)) - ((np.mean(X)**2)*(np.mean(Y)**2))\n\nprint(\"Sample Expected Value: ({:,.2f},{:,.2f})\".format(X_mean, Y_mean))\nprint(\"Sample Variance: {:,.2f}\".format(var))\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\nax[0].step(X_cdf_x, cdf_y, where='post', color=c1, label=r\"Marginal Distribution $X$\")\nax[0].step(Y_cdf_x, cdf_y, where='post', color=c5, label=r\"Marginal Distribution $Y$\")\nax[0].set(title=\"Cumulative Distribution Function\", xlabel=\"Random Variates\", ylabel=\"Cumulative Probability\")\nax[0].legend()\n\nax[1].scatter(X, Y, color=c1, alpha=0.7, label=\"Joint Distribution\")\nconfidence_ellipse(X, Y, ax=ax[1],alpha=0.2, facecolor=c2, edgecolor=c2, zorder=0, label=\"3 Standard Deviations\",)\nax[1].set(title=\"Joint Distribution Scatter Plot\", xlabel=r\"$X$\", ylabel=r\"$Y$\")\nax[1].legend(bbox_to_anchor=(1,1), loc=\"upper left\")\nplt.show()","14c32ac9":"n = 1000\n\ncorrelation = 0.8\ndf = 5\n\n### Independed Standard Normal distributions\nZ_x = sp.stats.norm.rvs(loc=0, scale=1, size=n, random_state=123)\nZ_y = sp.stats.norm.rvs(loc=0, scale=1, size=n, random_state=321)\nZ = np.matrix([Z_x, Z_y])\n\n# Construct the correlation matrix and Cholesky Decomposition\nrho = np.matrix([[1, correlation], [correlation, 1]])\ncholesky = np.linalg.cholesky(rho)\n\n### Apply Cholesky Decomposition\nXY = cholesky * Z\n\n### Extract X and Y\nX = np.array(XY[0,:]).flatten()\nY = np.array(XY[1,:]).flatten()\n\n### Chi Squared Sample\nnp.random.seed(123)\nChiSquared = np.random.chisquare(df=df, size=n)\n\nt_X = X \/ (np.sqrt(ChiSquared \/ df))\nt_Y = Y \/ (np.sqrt(ChiSquared \/ df))\n\n### Emperical CDF\ncdf_y = np.arange(1, n+1) \/ n\nX_cdf_x = np.sort(t_X)\nY_cdf_x = np.sort(t_Y)\n\n### Correlation\ncorrelation_sample = np.corrcoef(t_X, t_Y)\n\n### Expected Value\nX_mean = np.mean(t_X)\nY_mean = np.mean(t_Y)\n\nX2 = np.square(t_X)\nY2 = np.square(t_Y)\n\n### Variance\nvar = (np.mean(X2)*np.mean(Y2)) - ((np.mean(t_X)**2)*(np.mean(t_Y)**2))\n\nprint(\"Sample Expected Value: ({:,.2f},{:,.2f})\".format(X_mean, Y_mean))\nprint(\"Sample Variance: {:,.2f}\".format(var))\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\nax[0].step(X_cdf_x, cdf_y, where='post', color=c1, label=r\"Marginal Distribution $t_X$\")\nax[0].step(Y_cdf_x, cdf_y, where='post', color=c5, label=r\"Marginal Distribution $t_Y$\")\nax[0].set(title=\"Cumulative Distribution Function\", xlabel=\"Random Variates\", ylabel=\"Cumulative Probability\")\nax[0].legend()\n\nax[1].scatter(t_X, t_Y, color=c1, alpha=0.7, label=\"Joint Distribution\")\nconfidence_ellipse(t_X, t_Y, ax=ax[1],alpha=0.2, facecolor=c2, edgecolor=c2, zorder=0, label=\"3 Standard Deviations\",)\nax[1].set(title=\"Joint Distribution Scatter Plot\", xlabel=r\"$t_X$\", ylabel=r\"$t_Y$\")\nax[1].legend(bbox_to_anchor=(1,1), loc=\"upper left\")\nplt.show()","2ae46a46":"def copula_gaussian(n, correlation, seed):\n    \n    ''' Normally distributed random variates X and Y with correlation 'p'\n    '''\n    \n    # Independed Normal distributions\n    Z_x = sp.stats.norm.rvs(loc=0, scale=1, size=n, random_state=seed)\n    Z_y = sp.stats.norm.rvs(loc=0, scale=1, size=n, random_state=seed*2)\n    Z = np.matrix([Z_x, Z_y])\n    \n    # Construct the correlation matrix and Cholesky Decomposition\n    rho = np.matrix([[1, correlation], [correlation, 1]])\n    cholesky = np.linalg.cholesky(rho)\n    \n    # Apply Cholesky and extract X and Y\n    Z_XY = cholesky * Z\n    X = np.array(Z_XY[0,:]).flatten()\n    Y = np.array(Z_XY[1,:]).flatten()\n    \n    # CDF\n    X_cdf = sp.stats.norm.cdf(X, loc=0, scale=1)\n    Y_cdf = sp.stats.norm.cdf(Y, loc=0, scale=1)\n    \n    return X, Y, X_cdf, Y_cdf\n\ndef copula_t(n, correlation, df, seed):\n    \n    ''' Student's t distributed random variates t_X and t_Y with correlation 'p'\n        and degrees of freedom 'df'\n    '''\n    \n    # Gaussian Copula\n    Zx, Zy, _, _ = copula_gaussian(n=n, correlation=correlation, seed=seed)\n    \n    # Chi Squared Sample\n    np.random.seed(seed)\n    ChiSquared = np.random.chisquare(df=df, size=n)\n\n    # Stident's t distributed random variables\n    X = Zx \/ (np.sqrt(ChiSquared \/ df))\n    Y = Zy \/ (np.sqrt(ChiSquared \/ df))\n    \n    # CDF\n    X_cdf = sp.stats.t.cdf(X, df=df, loc=0, scale=1)\n    Y_cdf = sp.stats.t.cdf(Y, df=df, loc=0, scale=1)\n    \n    return X, Y, X_cdf, Y_cdf","60027116":"n=10000\ncorrelation = 0\ndf = 1\n\nX, Y, X_cdf, Y_cdf  = copula_gaussian(n=n, correlation=correlation, seed=123)\nTx, Ty, Tx_cdf, Ty_cdf  = copula_t(n=n, correlation=correlation, df=df, seed=123)\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\nplt.suptitle(r\"Gaussian Copula: $\\rho={}$\".format(correlation), fontsize=16)\nax[0].scatter(X, Y, color=c1, alpha=0.2, label=\"Joint Distribution\")\nax[0].set(title=\"Joint Distribution Scatter Plot\", xlabel=r\"$X$\", ylabel=r\"$Y$\", xlim=[-6,6], ylim=[-6,6])\n\nax[1].scatter(X_cdf, Y_cdf, color=c1, alpha=0.2, label=\"Joint Distribution\")\nax[1].set(title=\"Joint Distribution CDF Scatter Plot\", xlabel=r\"CDF of X\", ylabel=r\"CDF of Y\")\nplt.show()\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\nplt.suptitle(r\"Student's t-Copula: $\\rho={}$, $df={}$\".format(correlation,df), fontsize=16)\nax[0].scatter(Tx, Ty, color=c1, alpha=0.2, label=\"Joint Distribution\")\nax[0].set(title=\"Joint Distribution Scatter Plot\", xlabel=r\"$X$\", ylabel=r\"$Y$\", xlim=[-6,6], ylim=[-6,6])\n\nax[1].scatter(Tx_cdf, Ty_cdf, color=c1, alpha=0.2, label=\"Joint Distribution\")\nax[1].set(title=\"Joint Distribution CDF Scatter Plot\", xlabel=r\"CDF of X\", ylabel=r\"CDF of Y\")\nplt.show()","87841e73":"n=10000\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=3)\n\n### Loop through different correlations\nfor ii, correlation in enumerate([-0.8, 0, 0.8]):\n    \n    # Determine joint Gaussian distribution\n    X, Y, X_cdf, Y_cdf  = copula_gaussian(n=n, correlation=correlation, seed=123)\n    \n    # Plot joint distribution CDF\n    ax[ii].scatter(X_cdf, Y_cdf, color=c1, alpha=0.1)\n    if ii == 0:\n        ax[ii].set(title=r\"$\\rho={:.0%}$%\".format(correlation), xlabel=r\"CDF of X\", ylabel=r\"CDF of Y\")\n    else:\n        ax[ii].set(title=r\"$\\rho={:.0%}$%\".format(correlation))\n\nplt.show()","5720391f":"n=10000\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,16), nrows=3, ncols=3)\n\n### Loop through different correlations\nfor ii, correlation in enumerate([-0.8, 0, 0.8]):\n    for jj, df in enumerate([1, 3, 5]):\n    \n        # Determine joint Gaussian distribution\n        X, Y, X_cdf, Y_cdf  = copula_t(n=n, correlation=correlation, df=df, seed=123)\n\n        # Plot joint distribution CDF\n        ax[jj, ii].scatter(X_cdf, Y_cdf, color=c1, alpha=0.1)\n        ax[jj, ii].set(title=r\"$\\rho={:.0%}$%, $\\nu={}$\".format(correlation,df))\n\nplt.show()","871e9125":"# n = 100000\n# seed = 123\n\n# BTC_return = sp.stats.norm.rvs(loc=0.2, scale=0.5, size=n, random_state=seed)\n# ETH_return = sp.stats.t.rvs(df=15, loc=1, scale=0.2, size=n, random_state=seed)\n\n# ### Emperical CDF\n# cdf = np.arange(1, n+1) \/ n\n# cdf_BTC = np.sort(BTC_return)\n# cdf_ETH = np.sort(ETH_return)\n\n# ### Percentiles\n# BTC_upside = np.percentile(BTC_return, 99)\n# BTC_downside = np.percentile(BTC_return, 1)\n# ETH_upside = np.percentile(ETH_return, 99)\n# ETH_downside = np.percentile(ETH_return, 1)\n\n# print(\"Bitcoin 1-in-100 Upside\/Downside: {:.1%} \/ {:.1%}\".format(BTC_upside, BTC_downside))\n# print(\"Etherium 1-in-100 Upside\/Downside: {:.1%} \/ {:.1%}\".format(ETH_upside, ETH_downside))\n\n# ### Plot the sampled PDF and CDF against the theoretical distribution\n# fig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\n# ax[0].step(cdf_BTC, cdf, where='post', color=c1, label=r\"Bitcoin Returns\")\n# ax[0].set(title=\"Cumulative Distribution Function\", xlabel=\"Daily Returns (%)\", ylabel=\"Cumulative Probability\")\n# ax[0].legend()\n\n# ax[1].step(cdf_ETH, cdf, where='post', color=c5, label=r\"Etherium Returns\")\n# ax[1].set(title=\"Cumulative Distribution Function\", xlabel=\"Daily Returns (%)\", ylabel=\"Cumulative Probability\")\n# ax[1].legend()\n# plt.show()","f1755549":"# correlation = 0.2\n# df = 5\n\n# # Determine joint Student's t Copula\n# X, Y, X_cdf, Y_cdf  = copula_t(n=n, correlation=correlation, df=df, seed=123)\n# copula = np.array([X_cdf, Y_cdf]) * 100\n\n# # Marginal returns\n# marginal_returns = np.array([np.sort(BTC_return), np.sort(ETH_return)])\n\n# # Apply Copula to determine the Total Returns\n# correlated_returns = np.zeros((investments, n))\n# for portfolio in range(2):\n#     correlated_returns[portfolio,:] = np.percentile(a = marginal_returns[portfolio,:], q = copula[portfolio,:], interpolation = 'linear')\n    \n# total_returns = correlated_returns.sum(axis=0)\n# cdf_total_returns = np.sort(total_returns)\n\n# # Uncorrelated\n# uncorrelated_total_returns = marginal_returns.sum(axis=0)\n# cdf_uncorrelated_total_returns = np.sort(uncorrelated_total_returns)\n\n# ### Percentiles\n# total_returns_upside = np.percentile(total_returns, 99)\n# total_returns_downside = np.percentile(total_returns, 1)\n# uncorrelated_total_returns_upside = np.percentile(uncorrelated_total_returns, 99)\n# uncorrelated_total_returns_downside = np.percentile(uncorrelated_total_returns, 1)\n\n# print(\"100% correlated portfolios 1-in-100 Upside\/Downside: {:.1%} \/ {:.1%}\".format(uncorrelated_total_returns_upside, uncorrelated_total_returns_downside))\n# print(\"20% correlated portfolios 1-in-100 Upside\/Downside: {:.1%} \/ {:.1%}\".format(total_returns_upside, total_returns_downside))\n\n# ### Plot the sampled PDF and CDF against the theoretical distribution\n# fig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\n# ax[0].step(cdf_BTC, cdf, where='post', color=c1, label=r\"Bitcoin Returns\")\n# ax[0].step(cdf_ETH, cdf, where='post', color=c5, label=r\"Etherium Returns\")\n# ax[0].set(title=\"Cumulative Distribution Function\", xlabel=\"Daily Returns (%)\", ylabel=\"Cumulative Probability\")\n# ax[0].legend()\n\n# ax[1].step(cdf_total_returns, cdf, where='post', color=c1, label=r\"Total Returns\")\n# ax[1].step(cdf_uncorrelated_total_returns, cdf, where='post', color=c5, label=r\"Uncorrelated Total Returns\")\n# ax[1].set(title=\"Cumulative Distribution Function\", xlabel=\"Daily Returns (%)\", ylabel=\"Cumulative Probability\")\n# ax[1].legend()\n# plt.show()","ad695321":"# 1. Introduction <a class=\"anchor\" id=\"Introduction\"><\/a>\n---\n\nCopula is a multivariate cumulative distribution function which describe the dependence between random variables. Copulas are often used in quantitative finance to model the tail-risk or returns of a set of correlated distributions (<a href=\"https:\/\/en.wikipedia.org\/wiki\/Marginal_distribution\">Marginal Distributions<\/a>). There are many types of Copula which all have different application. The purpose of this notebook is to derive two types of Copula (namely the Gaussian and t-Copula) from first principles, withou the use of specific Copula packages in Python. The reason for this is to understand the mathematics and principles behind the Copula and also provide a visual representation for each step in the process. \n\n## 1.1 Marginal Distributions \n\nBefore we can discuss how to construct a Copula, we will first define some termonology which will be used throughout this notebook. For a refresher on distributions, you can also refer to my notebook which introduces <a href=\"https:\/\/www.kaggle.com\/liamhealy\/distributions-in-python\">Distribution Analysis in Python<\/a>.\n\nConsider a distribution $X$ which we can simulate $n$ random variates {$x_1, x_2, \\dots, x_n$}. This is refered to as a marginal distribution because the $x_n$ are draws from the distribution $X$ <a href=\"https:\/\/en.wikipedia.org\/wiki\/Independence_(probability_theory)\">independently<\/a>. We will refer to the marginal distribution of $X$ as $f_X(x)$. In the case when we have $d$ distribution $X_1, X_2, \\dots, X_d$, then the marginal distributions are defined as $f_{X_1}(x_1),\\dots,f_{X_d}(x_d)$. The correlation between each of the marginal distributions is fundamental to the Copula and is defined as\n\n\\begin{equation}\n\\rho =   \n    \\begin{pmatrix}\n    \\rho_{X_1X_1} & \\rho_{X_1X_2} & \\cdots & \\rho_{X_1X_d} \\\\\n    \\rho_{X_2X_1} & \\rho_{X_2X_2} & \\cdots & \\rho_{X_2X_d} \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho_{X_dX_1} & \\rho_{X_dX_2} & \\cdots & \\rho_{X_dX_d} \n    \\end{pmatrix}\n\\end{equation}\n\n## 1.2 Joint Disributions (<a href=\"https:\/\/en.wikipedia.org\/wiki\/Independence_(probability_theory)\">Sklar's theorem<\/a>)\n\nSklar's theorem states that every multivariate cumulative density function of random vectors $X_1, X_2, \\dots, X_d$ can be expressed in terms of its marginal distributions $f_{X_1}(x_1),\\dots,f_{X_d}(x_d)$. Therefore the join distribution is\n\n\\begin{equation}\n\\underbrace{f_{X_1,\\dots,X_d}(x_1,\\dots,x_d)}_{\\text{Joint Distribution}} = C(\\underbrace{f_{X_1}(x_1) \\cdot f_{X_2}(x_2) \\cdots f_{X_d}(x_d)}_{\\text{Marginal Distributions}})\n\\end{equation}\n\nwhere $C$ is the Copula.\n\n## 1.3 Independenct vs. Dependence\n\nAnother important concept which which underpins the use of Copulas is the dependence structure between marginal distribution. When we refer to marginal distributions being independent, this means that the random variates from one distribution have no influence on the random variates of the other distribution. therefore, it is clear that their correlation would also be very low or zero. However, the same cannot be said in reverse. Just because two marginal distribution have zero correlation does not mean they are independent!\n\n\\begin{align}\\notag\n\\text{Independence} &\\Rightarrow \\text{No Correlation} \\\\\n\\text{No Correlation}  &\\nRightarrow \\text{Independence}\n\\end{align}\n\nThe reason this is the case is because correlation only refers to the linear relationship between the distributions while dependence is any relationship (i.e. non-linear included).\n\nIn the case when we have two marginal distribution which are independent, then their correlation will be zero ($\\rho_{X_1X_2}=0$) and their join distribution $f_{X_1X_2}(x_1,x_2)$ can be calculated directly without the use of a Copula.\n\n\\begin{equation}\n\\underbrace{f_{X_1,X_2}(x_1,x_2)}_{\\text{Joint Distribution}} = \\underbrace{f_{X_1}(x_1) \\cdot f_{X_2}(x_2)}_{\\text{Marginal Distributions}}\n\\end{equation}\n\n## 1.4 Types of Copulas\n\nThere are two types of Copulas which will be discussed in this notebook: the Gaussian Copula and the t-Copula. The Gaussian copula is the Standard Normal copula which does not have any particular weighting towards the different parts of the distribution. The t-Copula on the other hand follows a multivariate Student's t distribution and assigns more weight tot he tails of the distribution. t-Copula are considered more suitable in quanititative finance and risk management because they comtrol the depence in the tails of the distribution which is where large profits and losses can be made. The t-Copula is essentially an extension of the Gaussian copula as we will see within this notebook.\n\n## 1.5 Notebook Structure\n\nThe next couple of sections of the notebook discuss the construction of three different joint distributions:\n\n1. Constructing a joint distribution from independent marginal distribution without the use of a Copula;\n2. Constructing a joint distribution from dependent Normal distributions with a Gaussian Copula;\n3. Constructing a joint distribution from dependent Normal distributions with a t-Copula;\n4. Sensitivity analysis of the Copula; and\n5. Application of a t-Copula to determine the joint returns from a set of correlated investment portfolios.","4a641f94":"# 2. Setup <a class=\"anchor\" id=\"Setup\"><\/a>\n---\n\n\nThe Python packages we will be using in this notebook include <a href=\"https:\/\/numpy.org\/\">Numpy<\/a>, <a href=\"https:\/\/www.scipy.org\/\">SciPy<\/a> and <a href=\"https:\/\/matplotlib.org\/\">Matplotlib<\/a>. We will also make use of a function to assist with plotting which has been taken from <a href=\"https:\/\/matplotlib.org\/3.2.0\/gallery\/statistics\/confidence_ellipse.html#sphx-glr-gallery-statistics-confidence-ellipse-py\">confidence ellipse<\/a>.","12c1f0ca":"# 8. Conclusion <a class=\"anchor\" id=\"Section8\"><\/a>\n---\nThis notebook is still a work in progress so please comment if you would like to see anything added. I hope you have found this notebook useful!","4f432681":"# Table of Contents\n---\n**1. [Introduction](#Introduction)** \n\n**2. [Setup](#Setup)**\n\n**3. [Independent Marginal Distributions](#Section3)**\n\n**4. [Dependent Marginal Distributions: Gaussian Copula](#Section4)**\n\n**5. [Dependent Marginal Distributions: t-Copula](#Section5)**\n\n**6. [Copula Representation and Sensitivity](#Section6)**\n\n**7. [Example](#Section7)**\n\n**8. [Conclusion](#Section8)**","e3b14323":"## 4.2 Full Simulation\n\nThe process to calculate $X$ and $Y$ is as follows:\n\n1. Obtain random variates $Z_X, Z_Y \\sim N(0,1)$ assuming indepences ($\\rho = 0$), as we did in Section 1. Sampling Standard Normal distributions as a starting point is why we define this as a Gaussian Copula.\n2. Apply the Cholesky Decomposition to $Z_X$ and $Z_Y$ to get $X, Y \\sim N(0,1)$ such that they retain their linear dependence ($\\rho = 0.8$)","46ffcedf":"## 6.1 Copula Representation\n\nThe charts below compare the joint distibution scatter plots to the CDF scatter plots. In this example we have plotted the a Gaussian Copula with $\\rho_{XY}=0$ versus a Student's t-Copula with $\\rho_{XY}=0$ and 1 degree of freedom. Having a low degrees of fredom implies more tail dependence between the two marginal distributions, as shown by the darker regions on all four corners of the t-Copula CDF scatter plot.","e58a969b":"![Copula-min.png](attachment:Copula-min.png)\n\n<a href=\"https:\/\/linkedin.com\/in\/ljhealy1992\">Liam Healy<\/a> $\\cdot$ Feb 16, 2020 $\\cdot$ 10 min read\n\n---","8e6c5f0f":"# 3. Independent Marginal Distributions (No Copula) <a class=\"anchor\" id=\"Section3\"><\/a>\n--- \nConsider an example where we have the marginal distribution $X$ and $Y$ which are Standard Normal Distributions and are independent. \n\n$$X, Y \\sim N(0,1)$$\n\nThen the joint distirbution $f_{X,Y}(x,y)$ is simply the product of the marginal distribution (from Eq. 1)\n\n$$ f_{X,Y}(x,y) = f_{X}(x) \\cdot f_{Y}(y)$$\n\nWhere the Expected Value and Variance of $f_{X,Y}(x,y)$ can be simply calculated as follows:\n\n\\begin{equation}\\notag\nE[f_{X,Y}] = E[f_{X}(x)] \\cdot E[f_{Y}(y)]\n\\end{equation}\n\n\\begin{align}\\notag\nVar(f_{X,Y}) &= [E(X)]^2Var(Y)+[E(Y)]^2Var(X) + Var(X)Var(Y) \\\\\n&= E(X^2)E(Y^2) - [E(X)]^2[E(Y)]^2\n\\end{align}\n\nBy simulating 1,000 times from there distirbutions we construct the following joint distribution.","d7ebefed":"# 7. Joint Portfolio Returns Example <a class=\"anchor\" id=\"Section7\"><\/a>\n---\nTBC","01209e2d":"### Student's t-Copula Sensitivity ($\\rho$ & $\\nu$)\n","8ff54b30":"# 4. Dependent Marginal Distributions I: Gaussian Copula <a class=\"anchor\" id=\"Section4\"><\/a>\n---\nNow consider the example where we have the marginal distribution $X$ and $Y$ which are Standard Normal Distributions but have an element of dependence (Called a Bivariate Normal). For this example we also need to know the relationship between $X$ and $Y$ is order to determine the joint distribution.\n\n\\begin{equation}\\notag\nX, Y \\sim N(0,1)\\\\\n\\rho = \n    \\begin{bmatrix}\n    1 & \\rho_{XY} \\\\\n    \\rho_{XY} & 1 \n    \\end{bmatrix}\n\\end{equation}\n\nIn order to determine the joint distirbution $f_{X,Y}(x,y,\\rho)$ we need to do a bit more work than the first example. Lets say that we know one of the random variates in $X$, say $x_1$. Then what does that tell us about the distribution of the next random variate from $Y$? To answer this we need to answer two questions:\n\n1. We know the distribution of $Y$;\n2. What are the parameters (Expected Value and Variance) of $Y$.\n\nThe answer to the first question is Normal because we know both $X$ and $Y$ are originally Standard Normal distributions. The parameters of $Y$ however, are not known staight away and will need to be determined.\n\n**Expected Value:**\n\nThe expected value of $y_1$ given that we know $x_1$ is\n\n\\begin{equation} \\notag\nE(y_1|X=x_1) = \\mu_Y + \\underbrace{\\rho \\cdot \\sigma_Y \\cdot \\left( \\frac{x_1 - \\mu_X}{\\sigma_X} \\right)}_{\\text{Scaled Mean Shock}}\n\\end{equation}\n\nThis equation is basically saying that the expected value of $y_1$ given that we know $x_1$ is the original mean of $Y$ plus a shock to account for the fact that $x_1$ may be different than the original mean of $X$. Note that in the event that $x_1=\\mu_X$ then the expectected value of $y_1$ would be the original mean of $Y$ which is $\\mu_Y$.\n\n**Standard Deviation:**\n\nThe standard deviation of $y_1$ given that we know $x_1$ is\n\n\\begin{equation}\\notag\nSd(y_1|X=x_1)=\\sigma_Y \\cdot \\sqrt{1-\\rho^2}\n\\end{equation}\n\nThis equation tells us that the updated standard deviation of $y_1$, given that we know $x_1$ is a scaled version of the original Standard Deviation $\\sigma_Y$. In fact, the scaling factor $\\sqrt{1-\\rho^2}$ ensures that the conditional Standard Deviation is lower than the original $\\sigma_Y$. This makes sense because if we know more about $X$ means we should also know more about $Y$, given that they have some relationship $\\rho$.\n\n## 4.1 Example Simulation ($x_1=2$)\n\nLets consider the example where $X$ and $Y$ are dependent Standard Normal distribution with a pearson correlation of $\\rho_{XY} = 0.8$ (i.e. they are 80% linearly correlation). Lets also assume that we randomly sample $x_1=2$ from $X \\sim N(0,1)$. ","af25182f":"# 6. Copula Representation and Sensitivity <a class=\"anchor\" id=\"Section6\"><\/a>\n---\nWe can express copulas in either their PDF or CDF form. The CDF form is extremely useful because it gives us a better understanding of the correlation structure while it also be used to aggregate emperical distributions through Monte-Carlo simulations. In this section we will explore the different views of a Copula and also their sensitivity to changes in the correlation and degrees of freedom.\n\nWe have defined two function `copula_nor()` and `copula_t()` below, which follow the same logic that we have already discussed in this presentation.","9601ed43":"### Step 2: Apply Cholesky Decomposition and Determine $X$ and $Y$\nOnce we have drawn our $Z_X$ and $Z_Y$ from independent Standard Normal distribution, we now have to take into account the dependence structure between the two samples by applying Cholesky Decomposion.\n\nFor a two-dimentional correlation matrix $\\rho$ defined as\n\n\\begin{equation}\\notag\n\\rho = \n    \\begin{bmatrix}\n    1 & \\rho_{XY} \\\\\n    \\rho_{XY} & 1 \n    \\end{bmatrix}\n\\end{equation}\n\nThe Cholesky Decomposition is\n\n\\begin{equation}\\notag\nA =\n    \\begin{bmatrix}\n    1 & 0 \\\\\n    \\rho_{XY} & \\sqrt{1-\\rho_{XY}^2} \n    \\end{bmatrix}\n\\implies \\rho = AA^T\n\\end{equation}\n\nTherefore we can calculate $X$ and $Y$ as follows:\n\n\\begin{equation}\\notag\n    \\begin{bmatrix}\n    X \\\\\n    Y\n    \\end{bmatrix}=\n    \\begin{bmatrix}\n    1 & 0 \\\\\n    \\rho_{XY} & \\sqrt{1-\\rho_{XY}^2} \n    \\end{bmatrix}\n    \\begin{bmatrix}\n    Z_X \\\\\n    Z_Y \n    \\end{bmatrix} \\implies\n    \\begin{cases}\n      X &= Z_X\\\\\n      Y &= \\rho_{XY}Z_Y + \\sqrt{1-\\rho_{XY}^2}Z_Y\n    \\end{cases} \n\\end{equation}\n\nthe results below show that the marginal distributions of $X$ and $Y$ look the same as $Z_X$ and $Z_Y$, however the joint distribution is clearly shows the dependence structure. This demonstrates an example as to why we do not nessassarily know the join distribution given the marginal distributions. The Copula is also needed to understand the joint distribution.","9aa8788a":"## 6.2 Copula Sensitivity\n\nThe key sensitivities in the Gaussian and t-Copulas are:\n\n- **Gaussian Sensitivities:** Correlation structure $\\rho$\n- **Student's t-Copula:** Correlation structure $\\rho$ and the degrees of freedom $\\nu$\n\nThe charts below provide a visual representation of how changes in these parameters affect the joint distributions CDF.\n\n### Gaussian Copula Sensitivity ($\\rho$)\nIncreasing the degree of correlation between the marginal distributions results in the joint distribution concentrating around the line $y=x$ (i.e. linear correlation). Also notice that the points are also equally distributed accros the $y=x$ axis (i.e. no excessive tail dependence).","b0a4e7ad":"# 5. Dependent Marginal Distributions II: t-Copula <a class=\"anchor\" id=\"Section5\"><\/a>\n--- \nIn the previous section we discussed the Gaussian Copula which links Normal distributions to their Copula. However, in many field of quantitative finance, we are interested in the extreme tain events. In the even of an economic stress we tend to see more extreme losses with increasing correlation. These extreme and correlated losses are not very well captured by a Normal distribution because the tails of a normal distribution are thin. Thats why in practice, t-Copulas are much better at controling the correlation in thails of the distributions.\n\nSimulating a t-Copula is much the same as the Gaussian Copula but instead of selecting independend Standard Normal variates to multiple through the Cholesky Matrix, we instead need to simulate Student's t distributed variates. i.e \n\n\\begin{equation}\\notag\nX, Y \\sim T_\\nu\\\\\n\\rho = \n    \\begin{bmatrix}\n    1 & \\rho_{XY} \\\\\n    \\rho_{XY} & 1\n    \\end{bmatrix}\n\\end{equation}\n\nwhere $\\nu$ is the degrees of freedom. We can simulate $T_\\nu$ samples as a distribution of the test statistic\n\n\\begin{equation}\nT = \\frac{Z}{\\sqrt{U\/\\nu}}\n\\end{equation}\n\nwhere $Z$ is a Standard Normal distribution and $U \\sim \\chi_\\nu^2$ is a Chi-Squared distribution with degrees of freedom $\\nu$. \n\nThe process to sample independed t-distributed random variabes $T_X$, and $T_Y$ follows:\n\n1. Simulate $U \\sim \\chi_\\nu^2$ for the number of simulations\n2. Simulate $Z_X$, $Z_Y \\sim N(0,1)$ with correlation $\\rho_{XY}$ as done in Section 2\n3. Apply the t-test statistic formula \n\n\\begin{equation}\nT_X = \\frac{Z_X}{\\sqrt{U\/\\nu}} , \\qquad T_Y = \\frac{Z_Y}{\\sqrt{U\/\\nu}}\n\\end{equation}","0aa72bfe":"### Step 1: Obtain random variates $Z_X, Z_Y \\sim N(0,1)$ which are independent\nThe first step is to simply simulate $Z_X$ and $Z_Y$ from two Standard Normal Distributions independently. The charts below show that the CDFs for each distribution are similar and that there is no dependence structure due to the symetric nature of the variance."}}