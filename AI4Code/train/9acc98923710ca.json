{"cell_type":{"2d18783c":"code","c9702cdd":"code","8c586f3c":"code","7a3f4ac8":"code","8210c09e":"code","bc2cfc61":"code","13e08ad2":"code","66eb0b84":"code","a3350cc2":"code","23c4a30f":"code","84c47219":"code","a55dfb34":"code","d18b9cae":"code","4cac4b56":"code","45a61a94":"code","a5ed3afa":"code","790509a1":"code","e891c56b":"code","005c07f2":"code","9c250185":"code","82b9992e":"code","344bb1ae":"code","8745032f":"code","33bd3f62":"code","da04c452":"code","860c6023":"code","7c55e785":"code","44cc5eb3":"code","2a5449e4":"code","71494965":"code","8d45af7c":"code","371ec334":"code","6684dac5":"code","92304c28":"code","0c82a310":"code","ff13141b":"code","31acebba":"code","16146b28":"code","daa76900":"code","9db126e8":"code","6a7ea007":"code","b5785df3":"code","fb3f3452":"code","1b618a1e":"code","c854a3e3":"code","6026db20":"code","180a4bcf":"code","5c529141":"markdown","f081ced8":"markdown","a7fd6105":"markdown","e305c5a4":"markdown","680bf759":"markdown","4bfe5f06":"markdown","a65cb957":"markdown","2bee1e02":"markdown","2ee90110":"markdown","70c5345c":"markdown","72145dcc":"markdown","7955d760":"markdown","2fdc3e01":"markdown","d3f90ed3":"markdown","dcc14264":"markdown","728a25d4":"markdown","b5d2017b":"markdown","b8dc4f55":"markdown","31b90e32":"markdown","b2a20b9f":"markdown","f6806328":"markdown","8da68590":"markdown","bc8ae280":"markdown","0228f835":"markdown","7391f836":"markdown","772d137a":"markdown","cdd4a98d":"markdown","fd586271":"markdown","06203485":"markdown","c874dc5b":"markdown","d2b5f99d":"markdown"},"source":{"2d18783c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9702cdd":"training = pd.read_csv('\/kaggle\/input\/unsw-nb15\/UNSW_NB15_training-set.csv', sep=',', encoding='cp1252')\ntesting = pd.read_csv('\/kaggle\/input\/unsw-nb15\/UNSW_NB15_testing-set.csv', sep=',', encoding='cp1252')\n\ntrain_test = pd.concat([training, testing])\ntrain_test.shape","8c586f3c":"pd.set_option('display.max_columns', None)\n\ndf = train_test.drop(['\u00ef\u00bb\u00bfid', 'rate', 'attack_cat'], axis=1)\ndisplay(df.shape)\n\ndisplay(pd.DataFrame(df.isna().sum().values.reshape(1, df.shape[1]), columns=df.columns, index=['Number']))\nprint('Columns : ',df.columns, '\\n Duplicated Columns : ',df.duplicated().sum())","7a3f4ac8":"def part(n):\n    folder_partitions = '\/kaggle\/input\/unsw-nb15\/'\n    features = pd.read_csv(folder_partitions+'NUSW-NB15_features.csv', sep=',', encoding='cp1252')\n    features['Name'] = features['Name'].str.lower()\n    features['Name'] = features['Name'].str.replace(\" \", \"\")\n\n    part1 = pd.read_csv(folder_partitions+'UNSW-NB15_'+n+'.csv', sep=',', encoding='cp1252', low_memory=False)\n    part1.drop_duplicates(inplace=True)\n\n    part1 = part1.set_axis(features.Name, axis=1)\n\n    cols = ['dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes','dbytes', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sintpkt','dintpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt','synack', 'ackdat', 'smeansz', 'dmeansz', 'trans_depth','res_bdy_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm','ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm','is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm','ct_srv_dst', 'is_sm_ips_ports','label']\n\n    p1 = part1[cols]\n    # shuffle the dataframe\n    p1 = p1.sample(frac=1, random_state=42)\n    print(p1.shape)\n    return p1","8210c09e":"part1, part2, part3, part4 = part('1'),part('2'),part('3'),part('4')\n\nP = pd.concat([part1, part2, part3, part4])\nprint('Shape : ',P.shape,'\\n Columns : ', P.columns)\n\n# unifier les noms des colonnes\nP.columns = df.columns\nprint('\\n Columns unifi\u00e9es : ', P.columns)","bc2cfc61":"data_pur = pd.concat([P, df])\ndata_pur.shape","13e08ad2":"pd.set_option('display.max_columns', None)\n\nisna = pd.DataFrame(data_pur.isna().sum().values.reshape(1, 42), columns=data_pur.columns, index=['NA'])\nisnull = pd.DataFrame(data_pur.isnull().sum().values.reshape(1, 42), columns=data_pur.columns, index=['Null'])\n\npd.concat([isna, isnull])","66eb0b84":"data_pur.is_ftp_login.value_counts(), data_pur.is_ftp_login.isna().sum()","a3350cc2":"from collections import Counter \n\nCounter(data_pur.label), data_pur.label.value_counts().plot.bar()","23c4a30f":"cat_features = data_pur.columns[data_pur.dtypes == 'O']\ncat_features","84c47219":"from collections import Counter\nCounter(data_pur.ct_ftp_cmd)","a55dfb34":"data_pur.ct_ftp_cmd.replace({' ': None, '0':0, '1':1, '2':2, '4':4}, inplace=True)\ncat_features = data_pur.columns[data_pur.dtypes == 'O']\ncat_features","d18b9cae":"data = data_pur.dropna(axis=0)\n\ndisplay(data.shape)\nCounter(data.label), data.label.value_counts().plot.bar()","4cac4b56":"pd.set_option('display.max_columns', None)\n\nisna = pd.DataFrame(data.isna().sum().values.reshape(1, 42), columns=data.columns, index=['NA'])\nisnull = pd.DataFrame(data.isnull().sum().values.reshape(1, 42), columns=data.columns, index=['Null'])\n\npd.concat([isna, isnull])","45a61a94":"from sklearn.model_selection import train_test_split\n\nTesting_data, Construction_data = train_test_split(data, test_size=0.5, shuffle=True)\n\nTesting_data.shape, Construction_data.shape","a5ed3afa":"Testing_data.to_csv('Testing_data.csv', index=False)","790509a1":"def ordinal_encoding(dataset):\n    tmp_df = dataset.copy()\n    start = 0\n    cat_features = tmp_df.columns[tmp_df.dtypes == 'O']\n    for cat_ft in cat_features:\n        ordinal_mapping = {k:i for i, k in enumerate(tmp_df[cat_ft].unique(), start)}\n        tmp_df[cat_ft] = tmp_df[cat_ft].map(ordinal_mapping)\n        start += len(tmp_df[cat_ft].unique())\n    return tmp_df","e891c56b":"construction_data_encoded = ordinal_encoding(Construction_data)","005c07f2":"from collections import Counter \n\nCounter(construction_data_encoded.label), construction_data_encoded.label.value_counts().plot.bar()","9c250185":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\nprint('Before : ', Counter(construction_data_encoded.label))\n\nsmote = SMOTE()\n\nX_smote, Y_smote = smote.fit_resample(construction_data_encoded.drop(['label'], axis=1), construction_data_encoded.label)\nprint('After : ', Counter(Y_smote))\n\nY_smote.value_counts().plot.bar()","82b9992e":"construction_data_balanced = X_smote.copy()\nconstruction_data_balanced['label'] = Y_smote\n\nfrom collections import Counter\n\nprint('Duplicated data (after over_sapling 1) : ',construction_data_balanced.duplicated().sum())\nprint('Shape (after oversampling1) : ',construction_data_balanced.shape)\n\nconstruction_data_balanced.drop_duplicates(inplace=True)\n\nprint('\\n\\t Label : ', Counter(construction_data_balanced.label))\nprint('\\t Duplicated data (after droping) : ',construction_data_balanced.duplicated().sum())\nprint('\\tShape (after droping missing values) : ',construction_data_balanced.shape)\n\nconstruction_data_balanced.label.value_counts().plot.bar()","344bb1ae":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\nprint('Before : ', Counter(construction_data_balanced.label))\n\nsmote = SMOTE()\n\nX_smote, Y_smote = smote.fit_resample(construction_data_balanced.drop(['label'], axis=1), construction_data_balanced.label)\nprint('After : ', Counter(Y_smote))\n\nY_smote.value_counts().plot.bar()","8745032f":"construction_data_balanced2 = X_smote.copy()\nconstruction_data_balanced2['label'] = Y_smote\n\nfrom collections import Counter\n\nprint('Duplicated data (after over_sapling 1) : ',construction_data_balanced2.duplicated().sum())\nprint('Shape (after oversampling1) : ',construction_data_balanced2.shape)\n\nconstruction_data_balanced2.drop_duplicates(inplace=True)\n\nprint('\\n\\t Label : ', Counter(construction_data_balanced2.label))\nprint('\\t Duplicated data (after droping missing values) : ',construction_data_balanced2.duplicated().sum())\nprint('\\tShape (after oversampling2 - final) : ',construction_data_balanced2.shape)\n\nconstruction_data_balanced2.label.value_counts().plot.bar()","33bd3f62":"construction_data_balanced2.label.value_counts(normalize=True)","da04c452":"construction_data_balanced2.to_csv('Construction_data.csv', index=False)","860c6023":"construction_data_balanced2.head()","7c55e785":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nTRAIN, TEST = train_test_split(construction_data_balanced2, test_size=0.25, shuffle=True)\n\ndisplay(TRAIN.shape, TEST.shape)\nprint('Train label : ', Counter(TRAIN.label))\nprint('Test label : ', Counter(TEST.label))\n\n\ny_train = TRAIN.label\nX_train_ = TRAIN.drop(['label'], axis=1)\n\ny_test = TEST.label\nX_test_ = TEST.drop(['label'], axis=1)\n\nX_train_.shape, y_train.shape, X_test_.shape, y_test.shape","44cc5eb3":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train_)\nX_test = scaler.transform(X_test_)","2a5449e4":"data = [X_train, y_train, X_test, y_test]","71494965":"from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\nfrom sklearn.metrics import accuracy_score,precision_score, f1_score, recall_score, mean_absolute_error, roc_curve, roc_auc_score\n\ndef evaluation(model, data):\n    import time \n    X,Y, x,y = data[0],data[1],data[2],data[3]\n    \n    start = time.time()\n    model.fit(X,Y)\n    \n    ypred = model.predict(x)\n    \n    print(classification_report(y, ypred))\n    \n    plot_confusion_matrix(model, x, y)\n    \n    return model, ypred, time.time()-start\n\ndef get_performance(model, data, ypred,fit_time):\n    X,Y, x,y = data[0],data[1],data[2],data[3]\n    \n    tmp = {}\n    tmp['train_score'] = model.score(X,Y)\n    tmp['test_score'] = model.score(x,y)\n    tmp['Accuracy_score'] = accuracy_score(y, ypred)\n    tmp['Precision_score'] = precision_score(y, ypred)\n    tmp['Recall_score'] = recall_score(y, ypred)\n    tmp['f1_score'] = f1_score(y, ypred)\n    tmp['fit time'] = fit_time\n        \n    results = pd.DataFrame(tmp, index=['values'])\n    \n    return results","8d45af7c":"from sklearn.ensemble import BaggingClassifier\n\nbag_dt = BaggingClassifier(bootstrap=True, n_estimators=20, oob_score=True, random_state=1, n_jobs=-1)\n\nbag_dt, ypred_bag_dt, time_bag_dt = evaluation(bag_dt, data)\n\nperformances_bag_dt = get_performance(bag_dt, data, ypred_bag_dt, time_bag_dt)\ndisplay(performances_bag_dt)\n\nplot_roc_curve(bag_dt, X_test, y_test, name='Bagging DT based'), plot_precision_recall_curve(bag_dt, X_test, y_test)\n\nimport pickle\npickle.dump(bag_dt, open('Bagging', 'wb'))","371ec334":"validation_data_encoded = ordinal_encoding(Testing_data)\n\nY_real = validation_data_encoded.label\nX_real_ = validation_data_encoded.drop(['label'], axis=1)\n\nfrom sklearn.preprocessing import MinMaxScaler \n\nscaler = MinMaxScaler()\n\nX_real = scaler.fit_transform(X_real_)\n\nX_real.shape, Y_real.shape","6684dac5":"Y_PRED = bag_dt.predict(X_real)\n\nprint('Precision : ',precision_score(Y_real, Y_PRED))\nprint('Accuracy : ',accuracy_score(Y_real, Y_PRED))\nprint('Recall : ',recall_score(Y_real, Y_PRED))\nprint('F1 : ',f1_score(Y_real, Y_PRED))\nprint(classification_report(Y_real, Y_PRED))","92304c28":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nada_dt = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1, max_features='sqrt'),\n                            n_estimators=500, learning_rate=0.5)\n\nada_dt, ypred_ada_dt, time_ada_dt = evaluation(ada_dt, data)\n\nperformances_ada_dt = get_performance(ada_dt, data, ypred_ada_dt, time_ada_dt)\ndisplay(performances_ada_dt)\n\nplot_roc_curve(ada_dt, X_test, y_test), plot_precision_recall_curve(ada_dt, X_test, y_test)\n\nimport pickle\npickle.dump(ada_dt, open('AdaBoost', 'wb'))","0c82a310":"validation_data_encoded = ordinal_encoding(validation_data)\n\nY_real = validation_data_encoded.label\nX_real_ = validation_data_encoded.drop(['label'], axis=1)\n\nfrom sklearn.preprocessing import MinMaxScaler \n\nscaler = MinMaxScaler()\n\nX_real = scaler.fit_transform(X_real_)\n\nX_real.shape, Y_real.shape","ff13141b":"Y_PRED = ada_dt.predict(X_real)\n\nprint('Precision : ',precision_score(Y_real, Y_PRED))\nprint('Accuracy : ',accuracy_score(Y_real, Y_PRED))\nprint('Recall : ',recall_score(Y_real, Y_PRED))\nprint('F1 : ',f1_score(Y_real, Y_PRED))\nprint(classification_report(Y_real, Y_PRED))","31acebba":"from sklearn.ensemble import RandomForestClassifier\n\nrfc_dt = RandomForestClassifier(max_depth=30, bootstrap=True, \n                                oob_score=True, max_features='sqrt', \n                                n_jobs=-1, n_estimators=20)\n\nrfc_dt, ypred_rfc_dt, time_rfc_dt = evaluation(rfc_dt, data)\n\nperformances_rfc_dt = get_performance(rfc_dt, data, ypred_rfc_dt, time_rfc_dt)\ndisplay(performances_rfc_dt)\n\nplot_roc_curve(rfc_dt, X_test, y_test), plot_precision_recall_curve(rfc_dt, X_test, y_test)\n\nimport pickle\npickle.dump(rfc_dt, open('RandomForest', 'wb'))","16146b28":"validation_data_encoded = ordinal_encoding(validation_data)\n\nY_real = validation_data_encoded.label\nX_real_ = validation_data_encoded.drop(['label'], axis=1)\n\nfrom sklearn.preprocessing import MinMaxScaler \n\nscaler = MinMaxScaler()\n\nX_real = scaler.fit_transform(X_real_)\n\nX_real.shape, Y_real.shape","daa76900":"Y_PRED = rfc_dt.predict(X_real)\n\nprint('Precision : ',precision_score(Y_real, Y_PRED))\nprint('Accuracy : ',accuracy_score(Y_real, Y_PRED))\nprint('Recall : ',recall_score(Y_real, Y_PRED))\nprint('F1 : ',f1_score(Y_real, Y_PRED))\nprint(classification_report(Y_real, Y_PRED))","9db126e8":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=0)\ntree, ypred_tree, time_tree = evaluation(tree, data)\n\nperformances_tree = get_performance(tree, data, ypred_tree, time_tree)\ndisplay(performances_tree)","6a7ea007":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(solver='liblinear', random_state=0)\nlr, ypred_lr, time_lr = evaluation(lr, data)\n\nperformances_lr = get_performance(lr, data, ypred_lr, time_lr)\ndisplay(performances_lr)","b5785df3":"KNN_TRAIN_X, _, KNN_TRAIN_Y,_ = train_test_split(data[2], data[3], test_size=0.3, shuffle=True)\nKNN_TRAIN_X.shape, KNN_TRAIN_Y.shape\n\nX_train_knn, X_test_knn, y_train_knn,y_test_knn = train_test_split(KNN_TRAIN_X, KNN_TRAIN_Y, test_size=0.1, shuffle=True)\n\ndata_knn = [X_train_knn, y_train_knn, X_test_knn, y_test_knn]\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=10, n_jobs=-1)\nknn, ypred_knn, time_knn = evaluation(knn, data_knn)\n\nperformances_knn = get_performance(knn, data_knn, ypred_knn, time_knn)\ndisplay(performances_knn)","fb3f3452":"SVM_TRAIN_X, _, SVM_TRAIN_Y,_ = train_test_split(data[2], data[3], test_size=0.3, shuffle=True)\nSVM_TRAIN_X.shape, SVM_TRAIN_Y.shape\n\nX_train_svm, X_test_svm, y_train_svm,y_test_svm = train_test_split(KNN_TRAIN_X, KNN_TRAIN_Y, test_size=0.1, shuffle=True)\n\ndata_svm = [X_train_svm, y_train_svm, X_test_svm, y_test_svm]\n\n\nfrom sklearn.svm import SVC\n\nsvc = SVC(kernel='linear', probability=True)\nsvc, ypred_svc, time_svc = evaluation(svc, data_svm)\n\nperformances_svc = get_performance(svc, data_svm, ypred_svc, time_svc)\ndisplay(performances_svc)","1b618a1e":"from mlxtend.classifier import EnsembleVoteClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nml_h_voting = EnsembleVoteClassifier([tree,knn, lr, svc], voting='hard' ,fit_base_estimators=False)\n\nml_h_voting, ypred_ml_h_voting, fit_time_ml_h_voting = evaluation(ml_h_voting, data)\n\nperformances_ml_h_voting = get_performance(ml_h_voting, data, ypred_ml_h_voting, fit_time_ml_h_voting)\ndisplay(performances_ml_h_voting)\n\nplot_roc_curve(ml_h_voting, X_test, y_test), plot_precision_recall_curve(ml_h_voting, X_test, y_test)\n\nimport pickle\npickle.dump(ml_h_voting, open('ml_h_voting_with_ID', 'wb'))","c854a3e3":"Y_PRED = ml_h_voting.predict(X_real)\n\nprint('Precision : ',precision_score(Y_real, Y_PRED))\nprint('Accuracy : ',accuracy_score(Y_real, Y_PRED))\nprint('Recall : ',recall_score(Y_real, Y_PRED))\nprint('F1 : ',f1_score(Y_real, Y_PRED))\nprint(classification_report(Y_real, Y_PRED))","6026db20":"from mlxtend.classifier import EnsembleVoteClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nml_s_voting = EnsembleVoteClassifier([tree,knn, lr, svc], voting='soft' ,fit_base_estimators=False)\n\nml_s_voting, ypred_ml_s_voting, fit_time_ml_s_voting = evaluation(ml_s_voting, data)\n\nperformances_ml_s_voting = get_performance(ml_s_voting, data, ypred_ml_s_voting, fit_time_ml_s_voting)\ndisplay(performances_ml_s_voting)\n\nplot_roc_curve(ml_s_voting, X_test, y_test), plot_precision_recall_curve(ml_s_voting, X_test, y_test)\n\npickle.dump(ml_s_voting, open('ml_s_voting_with_ID', 'wb'))","180a4bcf":"Y_PRED = ml_s_voting.predict(X_real)\n\nprint('Precision : ',precision_score(Y_real, Y_PRED))\nprint('Accuracy : ',accuracy_score(Y_real, Y_PRED))\nprint('Recall : ',recall_score(Y_real, Y_PRED))\nprint('F1 : ',f1_score(Y_real, Y_PRED))\nprint(classification_report(Y_real, Y_PRED))","5c529141":"# parse ct_ftp_cmd","f081ced8":"# **Bagging**","a7fd6105":"> - UNSW_NB15_testing-set.csv\n> - UNSW_NB15_training-set.csv","e305c5a4":"# show Missing values \n> - isna() : missing (None or numpy.NaN)\n> - isnull() : null","680bf759":"---","4bfe5f06":"# **check duplicated values after oversampling**","a65cb957":"> ## **SVM**","2bee1e02":"## **make dataset from partitions**","2ee90110":"# **Data list for evaluation function**","70c5345c":"# **Split data**\n> - **Testing Data** => will be used to test models\n> - **Construction Data** => will be used de train and validate models ","72145dcc":"# **Basic Classifiers**","7955d760":"# **RandomForest**","2fdc3e01":"# **AdaBoost**","d3f90ed3":"# **Oversampling 1**","dcc14264":"# **encode production data**","728a25d4":"# **Voting Classifier**","b5d2017b":"> ## **Decision Tree**","b8dc4f55":"# **Evaluation function**","31b90e32":"# **test adaboost**","b2a20b9f":"# **Construct models**\n> - split data to train and validation set\n","f6806328":"> ## **KNN**","8da68590":"> ## **Soft Voting**","bc8ae280":"# **All data in one dataframe**","0228f835":"# **data without null (na) values**","7391f836":"## **Load data**\n> - UNSW_NB15_testing-set.csv\n> - UNSW_NB15_training-set.csv\n> - UNSW-NB15_1.csv\n> - UNSW-NB15_2.csv\n> - UNSW-NB15_3.csv\n> - UNSW-NB15_4.csv","772d137a":"# **test bagging in data_test**","cdd4a98d":"# oversampling 2","fd586271":"# **Data Scaling**","06203485":"> ## **Logistic Regression**","c874dc5b":"> - UNSW-NB15_1.csv\n> - UNSW-NB15_2.csv\n> - UNSW-NB15_3.csv\n> - UNSW-NB15_4.csv","d2b5f99d":"> ## **Hard Voting**"}}