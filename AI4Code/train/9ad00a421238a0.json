{"cell_type":{"e39194af":"code","468ddba8":"code","f1479f94":"code","4c545c2f":"code","e2c129cc":"code","6d454cfd":"code","720653a3":"code","c5857b16":"code","5221c28b":"code","2432b299":"code","453b8b12":"code","77e4782b":"code","bd0a1cd4":"code","1b5cd977":"code","e4e64833":"code","21d3d282":"code","6a1f816b":"code","2572c46a":"code","5d5a5e72":"code","95b05a04":"code","9ebedf1c":"code","48ac3a24":"code","3fa53cbf":"code","d93d85dd":"code","c2d91e8d":"code","ecdec9e4":"code","a1924423":"code","85d36967":"code","c511590f":"code","459567a0":"code","a1fde54b":"markdown","3556c30b":"markdown","ce184754":"markdown","b36fd5d1":"markdown"},"source":{"e39194af":"#Importing Libraries\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib as pyplot\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))","468ddba8":"pip install pyspellchecker","f1479f94":"#Importing Data\ndata = pd.read_excel('..\/input\/asapaeg\/training_set_rel3.xls')\ndata.head()\n#data.shape (12978,28)","4c545c2f":"data.describe() ","e2c129cc":"data.isnull().sum()","6d454cfd":"#Finding the number of records for each column for each of the eight essay sets, to know that data is consistent.\ndata.groupby('essay_set').agg('count')","720653a3":"data1 = data[['essay_set','essay','domain1_score']].copy()\ndata1.head()\n#print(data1)","c5857b16":"import nltk\nimport string\nimport re, collections\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom spellchecker import SpellChecker\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk import pos_tag","5221c28b":"string.punctuation","2432b299":"#Cleaning the text using regex function\ndef process_text(essay):\n    essay = str(essay)\n    result = re.sub(r'http[^\\s]*','',essay)  #removing url\n    result = re.sub('[0-9]+','', result).lower() # remove numbers and lowercase the text\n    result = re.sub('@[a-z0-9]+', '', result) #Eg: @caps1 will be removed\n    return re.sub('[%s]*' % string.punctuation, '',result) #remove punctuation\ndata1['clean_essay'] = data1['essay'].apply(process_text)","453b8b12":"#After cleaning the data\ndata1['clean_essay']","77e4782b":"#Here, we are using ascii encoding on the string, ignoring the ones that can't be converted and then again decoding it.\ndef decode_essay(essay):\n    return essay.encode('ascii', 'ignore').decode('ascii')","bd0a1cd4":"#Tokenizing the sentences to words\ndef convert_essay_to_wordlist(sentence):\n    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", sentence)\n    wordlist = nltk.word_tokenize(clean_sentence)\n    return wordlist\ndata1['clean_essay'] = data1['clean_essay'].apply(convert_essay_to_wordlist)","1b5cd977":"#For Splitting sentences in the paragraph using PunktSentenceTokenizer\ndef tokenize_essay(essay):\n    strip_essay = essay.strip()\n    tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n    raw_sentences = tokenizer.tokenize(strip_essay)\n    tokenized_sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:\n            tokenized_sentences.append(convert_essay_to_wordlist(raw_sentence))\n    return tokenized_sentences","e4e64833":"#Data After tokenizing\ndata1['clean_essay'].head()\n#data1.head()","21d3d282":"def remove_stopwords(text):\n    words = [word for word in text if word not in stopwords.words('english')]\n    return words\ndata1['clean_essay'] = data1['clean_essay'].apply(lambda x: remove_stopwords(x))","6a1f816b":"#Data after removing stopwords\ndata1.head()","2572c46a":"def clean_length(token):\n    return [i for i in token if len(i)>2]\ndata1['clean_essay'] = data1['clean_essay'].apply(clean_length)","5d5a5e72":"#After removing the words having length=1\ndata1['clean_essay'].head()","95b05a04":"# calculating number of words in an essay\ndef word_count(essay):    \n    clean_essay = re.sub(r'\\W',' ', essay)                 #equivalent to [^a-zA-Z0-9]\n    words = nltk.word_tokenize(clean_essay)    \n    return len(words)","9ebedf1c":"#Calculating the number of characters\ndef char_count(essay):    \n    clean_essay = re.sub(r'\\s','', str(essay).lower())     #matches a single whitespace character, space, newline, return, tab, form\n    return len(clean_essay)","48ac3a24":"#Number of sentences\ndef sent_count(essay):    \n    sentences = nltk.sent_tokenize(essay)                 #using sent_tokenize to convert paragraph into sentences\n    return len(sentences)","3fa53cbf":"#Average word length\ndef average_word_length(essay):    \n    clean_essay = re.sub(r'\\W', ' ', essay)\n    words = nltk.word_tokenize(clean_essay)    \n    return sum(len(word) for word in words) \/ len(words)","d93d85dd":"# Lemmatization using POS Tagging\n# Here, we are appending all the POS tags in the lemma\ndef count_lemmas(essay):\n    tokenized_sentences = tokenize_essay(essay)         #create list of tuples\n    lemmas = []\n    wordnet_lemmatizer = WordNetLemmatizer()            #Lemmatizing the wprds\n    \n    for sentence in tokenized_sentences:\n        tag_tokens = nltk.pos_tag(sentence)             #POS-tagging ('going',VB) \n        \n        for token_tuple in tag_tokens:\n        \n            pos_tag = token_tuple[1]                    #We only need POS_tags,not the word\n        \n            if pos_tag.startswith('N'): \n                pos = wordnet.NOUN\n                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n            elif pos_tag.startswith('J'):\n                pos = wordnet.ADJ\n                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n            elif pos_tag.startswith('V'):\n                pos = wordnet.VERB\n                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n            elif pos_tag.startswith('R'):\n                pos = wordnet.ADV\n                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n            else:\n                pos = wordnet.NOUN\n                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))   #the words that are neither of above,are by default tagged as NOUN.\n    \n    lemma_count = len(set(lemmas))\n    \n    return lemma_count","c2d91e8d":"def spell_count(essay):\n    spell = SpellChecker()\n    essay=essay.split()\n    misspelled = spell.unknown(essay)\n    return len(misspelled)","ecdec9e4":"#Calculating number of nouns, adjectives, verbs and adverbs in an essay, this will give the real count. Hence, count of lemmas is less than count of noun,adj,verb and adverb.\ndef count_pos(essay):\n    \n    tokenized_sentences = tokenize_essay(essay)           #create a list of tuples\n    \n    noun_count = 0\n    adj_count = 0\n    verb_count = 0\n    adv_count = 0\n    \n    for sentence in tokenized_sentences:\n        tagged_tokens = nltk.pos_tag(sentence)\n        \n        for token_tuple in tagged_tokens:\n            pos_tag = token_tuple[1]\n        \n            if pos_tag.startswith('N'): \n                noun_count += 1\n            elif pos_tag.startswith('J'):\n                adj_count += 1\n            elif pos_tag.startswith('V'):\n                verb_count += 1\n            elif pos_tag.startswith('R'):\n                adv_count += 1\n            \n    return noun_count, adj_count, verb_count, adv_count","a1924423":"def extract_features(data1):\n    \n    features = data1.copy()\n    \n    features['char_count'] = features['essay'].apply(char_count)\n    \n    features['word_count'] = features['essay'].apply(word_count)\n    \n    features['sent_count'] = features['essay'].apply(sent_count)\n    \n    features['average_word_length'] = features['essay'].apply(average_word_length)\n    \n    features['spell_count'] = features['essay'].apply(spell_count)\n    \n    features['lemma_count'] = features['essay'].apply(count_lemmas)\n    \n    features['noun_count'], features['adj_count'], features['verb_count'], features['adv_count'] = zip(*features['essay'].map(count_pos))\n    \n    return features","85d36967":"#Only for essay_set = 1\nfeatures_set1 = extract_features(data1[data1['essay_set'] == 1])\n#features_set2 = extract_features(data1[data1['essay_set'] == 2])\n#features_set3 = extract_features(data1[data1['essay_set'] == 3])\n#features_set4 = extract_features(data1[data1['essay_set'] == 4])\n#features_set5 = extract_features(data1[data1['essay_set'] == 5])\n#features_set6 = extract_features(data1[data1['essay_set'] == 6])\n#features_set7 = extract_features(data1[data1['essay_set'] == 7])\n#features_set8 = extract_features(data1[data1['essay_set'] == 8])\nprint(features_set1)","c511590f":"data1","459567a0":"data1.to_excel('data1.xls')","a1fde54b":"# Preprocessing the Data\n1. Cleaning the Data using regex function\n2. Tokenization (Word Tokenize and Sentence Tokenize)\n3. Remove Stopwords\n4. Lemmatization using POS_Tagging\n5. Numeric Features like word_count,char_count,sentence_count,etc.\n6. Orthography(Spelling Mistakes and Punctuation)","3556c30b":"### The Automated Student Assessment Prize\u2019s (ASAP) Dataset, sponsored by Hewitt-Packard comprised of 13,000 essays, 8 different datasets of different genre. Each dataset was a collection of responses to its own prompt.Each essay set has it's own rubric (marking scheme) to decide the final score of the essay. Each essay has one or more human scores and a final resolved score\n\n### Part 1 - Text-Preprocessing on the Data","ce184754":"### Currently, I am learning NLP by practicing it on various datasets. This is all I have done by far in text-preprocessing, If there are certain things that should be changed,updated or I have missed out, please comment and let me know. Also, If there could be some inputs on Semantic Analysis, like what should be the approach and algorithms that could be used for Semantic Analysis, please share.\n\n### Thankyou.\n","b36fd5d1":"#### A Regular Expression is a text string that describes a search pattern which can be used to match or replace patterns inside a string with a minimal amount of code."}}