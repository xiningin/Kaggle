{"cell_type":{"d4d11684":"code","9ce01fd1":"code","47e14a7e":"code","2bc4fc7e":"code","0ee86ebd":"code","4436583d":"code","4fe8f359":"code","c01eb169":"code","bd4dfe59":"code","30851487":"code","f257c7d6":"code","3721410c":"code","c2045c9a":"code","ad100a16":"code","4597ceb5":"code","9b988f3b":"code","0ba2ab77":"code","97db10d7":"code","b19134e8":"code","0bf62520":"code","8cdeb9cd":"code","e414063b":"code","d988d63b":"code","86f04a8d":"code","7a159ef2":"code","39df1471":"code","09297ee1":"code","85c39d09":"markdown","7d585f4e":"markdown","a46dc33b":"markdown","59c1a3ba":"markdown","c3b9d086":"markdown","fa7881e5":"markdown","74ace197":"markdown","b0e37299":"markdown","4aa21b28":"markdown"},"source":{"d4d11684":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style=\"white\") \nsns.set(style=\"whitegrid\", color_codes=True)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9ce01fd1":"files = os.listdir(\"..\/input\")\nprint(files)","47e14a7e":"df_creditCard = pd.read_csv('..\/input\/'+files[0])","2bc4fc7e":"df_creditCard.head(5)","0ee86ebd":"print(\"The maximum missing values = %d, which means there is no misisng values and we don't have pre-processing tasks :D\" %max(df_creditCard.isna().sum()))","4436583d":"print(\"There is %.2f%% of Frauds examples in the dataset\" %(100*df_creditCard[df_creditCard.Class == 0].shape[0]\/df_creditCard.shape[0]))\nprint(\"There is %.2f%% of non Frauds examples in the dataset\" %(100*df_creditCard[df_creditCard.Class == 1].shape[0]\/df_creditCard.shape[0]))","4fe8f359":"plt.figure(figsize=(15,8))\nsns.countplot('Class', data=df_creditCard)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)","c01eb169":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df_creditCard['Amount'].values\ntime_val = df_creditCard['Time'].values\n\nsns.distplot(amount_val, ax= ax[0], color = 'g')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1])\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])","bd4dfe59":"from sklearn.preprocessing import RobustScaler\n\nrob_scaler = RobustScaler()\n\ndf_creditCard.Amount = rob_scaler.fit_transform(df_creditCard.Amount.values.reshape(-1,1))\ndf_creditCard.Time   = rob_scaler.fit_transform(df_creditCard.Time.values.reshape(-1,1))","30851487":"df_creditCard.head(5)","f257c7d6":"from sklearn.model_selection import StratifiedKFold\n\nX = df_creditCard.drop('Class', axis=1)\ny = df_creditCard['Class']\n\n#This cross-validation object is a variation of KFold that returns stratified folds.\n#The folds are made by preserving the percentage of samples for each class.\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n    \noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","3721410c":"fraud_df = df_creditCard.loc[df_creditCard['Class'] == 1]\nnon_fraud_df = df_creditCard.loc[df_creditCard['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","c2045c9a":"print('Distribution of the Classes in the subsample dataset')\nprint(new_df['Class'].value_counts()\/len(new_df))\n\nsns.countplot('Class', data=new_df)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","ad100a16":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = df_creditCard.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","4597ceb5":"important_features = sub_sample_corr.Class[abs(sub_sample_corr.Class) >= 0.4].drop(['Time', 'Class'])","9b988f3b":"# Positive Correlation \npositive_corr = important_features[important_features > 0]\n\nf, axes = plt.subplots(ncols=3, figsize=(20,4))\n\nfor  i in range(len(positive_corr)):\n    sns.boxplot(x=\"Class\", y=positive_corr.index[i], data=new_df, ax=axes[i])\n    axes[i].set_title(str(positive_corr.index[i]) + ' vs Class Positive Correlation')\n    \nplt.show()","0ba2ab77":"# Negative Correlation\n\ndef trim_axs(axs, N):\n    \"\"\"little helper to massage the axs list to have correct length...\"\"\"\n    axs = axs.flat\n    for ax in axs[N:]:\n        ax.remove()\n    return axs[:N]\n\nnegative_corr = important_features[important_features < 0]\n\nf, axes = plt.subplots(nrows=4, ncols=3, figsize=(40,30))\naxes = trim_axs(axes,len(negative_corr))\n\nfor  i,axe in enumerate(axes):\n    sns.boxplot(x=\"Class\", y=negative_corr.index[i], data=new_df, ax=axe)\n    axe.set_title(str(negative_corr.index[i]) + ' vs Class Negative Correlation')\n    \nplt.show()","97db10d7":"from scipy.stats import norm\n\nimportant = ['V17', 'V12', 'V14', 'V16', 'V10']\ncolors = ['#ff5733', '#ffe933', '#c1ff33', '#33ffc1', '#d433ff']\nfigure, axes = plt.subplots(2, 3, figsize=(30,15))\naxes = trim_axs(axes, len(important))\n\nfor  i,axe in enumerate(axes):\n    sns.distplot(new_df[important[i]], ax=axe, fit=norm, color=colors[i])\n    axe.set_title('Distribution of '+str(important[i]))\n    \nplt.show()","b19134e8":"df_copy = new_df.copy()","0bf62520":"# Removing outlier from V17\nv17_fraud = new_df.V17[new_df.Class == 1].values\nq25, q75 = np.percentile(v17_fraud,25), np.percentile(v17_fraud, 75)\nprint('First quartile = %.2f, Third quartile = %.2f'%(q25, q75))\nv17_iqr = q75 - q25\nv17_cutOff = v17_iqr * 1.5\nv17_lower, v17_upper = q25 - v17_cutOff, q75 + v17_cutOff\noutliers = [x for x in v17_fraud if x < v17_lower or x > v17_upper]\nprint(\"numbers of outliers in the feature V17 = %d\"%len(outliers))\nnew_df.drop(new_df[(new_df.V17 > v17_upper) | (new_df.V17 < v17_lower)].index,inplace = True)\nprint('-'*40)\n\n# Removing outlier from V14\nv14_fraud = new_df.V14[new_df.Class == 1].values\nq25, q75 = np.percentile(v14_fraud,25), np.percentile(v14_fraud, 75)\nprint('First quartile = %.2f, Third quartile = %.2f'%(q25, q75))\nv14_iqr = q75 - q25\nv14_cutOff = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cutOff, q75 + v14_cutOff\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint(\"numbers of outliers in the feature V14 = %d\"%len(outliers))\nnew_df.drop(new_df[(new_df.V14 > v14_upper) | (new_df.V14 < v14_lower)].index,inplace = True)\nprint('-'*40)\n\n# Removing outlier from V12\nv12_fraud = new_df.V12[new_df.Class == 1].values\nq25, q75 = np.percentile(v12_fraud,25), np.percentile(v12_fraud, 75)\nprint('First quartile = %.2f, Third quartile = %.2f'%(q25, q75))\nv12_iqr = q75 - q25\nv12_cutOff = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cutOff, q75 + v12_cutOff\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint(\"numbers of outliers in the feature V12 = %d\"%len(outliers))\nnew_df.drop(new_df[(new_df.V12 > v12_upper) | (new_df.V12 < v12_lower)].index,inplace = True)\nprint('-'*40)\n\n# Removing outlier from V16\nv16_fraud = new_df.V16[new_df.Class == 1].values\nq25, q75 = np.percentile(v16_fraud,25), np.percentile(v16_fraud, 75)\nprint('First quartile = %.2f, Third quartile = %.2f'%(q25, q75))\nv16_iqr = q75 - q25\nv16_cutOff = v16_iqr * 1.5\nv16_lower, v16_upper = q25 - v16_cutOff, q75 + v16_cutOff\noutliers = [x for x in v16_fraud if x < v16_lower or x > v16_upper]\nprint(\"numbers of outliers in the feature V16 = %d\"%len(outliers))\nnew_df.drop(new_df[(new_df.V16 > v16_upper) | (new_df.V16 < v16_lower)].index,inplace = True)\nprint('-'*40)\n\n# Removing outlier from V10\nv10_fraud = new_df.V10[new_df.Class == 1].values\nq25, q75 = np.percentile(v10_fraud,25), np.percentile(v10_fraud, 75)\nprint('First quartile = %.2f, Third quartile = %.2f'%(q25, q75))\nv10_iqr = q75 - q25\nv10_cutOff = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cutOff, q75 + v10_cutOff\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint(\"numbers of outliers in the feature V10 = %d\"%len(outliers))\nnew_df.drop(new_df[(new_df.V10 > v10_upper) | (new_df.V10 < v10_lower)].index,inplace = True)","8cdeb9cd":"important = ['V17', 'V12', 'V14', 'V16', 'V10']\ncolors = ['#ff5733', '#ffe933', '#c1ff33', '#33ffc1', '#d433ff']\nfigure, axes = plt.subplots(2, 3, figsize=(30,15))\naxes = trim_axs(axes, len(important))\n\nfor  i,axe in enumerate(axes):\n    sns.distplot(new_df[important[i]], ax=axe, fit=norm, color=colors[i])\n    axe.set_title('Distribution of '+str(important[i]))\n    \nplt.show()","e414063b":"X = new_df.drop('Class', axis = 1)\ny = new_df.Class","d988d63b":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)","86f04a8d":"X_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","7a159ef2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"XgBoost\": XGBClassifier()\n}","39df1471":"from sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","09297ee1":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n\nlabels = ['No Fraud', 'Fraud']\nfor key, classifier in classifiers.items():\n    print(\"Classifiers: \", classifier.__class__.__name__)\n    print(classification_report(y_test,classifier.predict(X_test),target_names=labels))\n    print('-' * 40)","85c39d09":"#### Outlier removal","7d585f4e":"The dataset is so impbalanced, so using it without handling this aspect will lead to an overfitting. The final model will assum all the transcation as non Fraud, but out goal is to learn the pattern of the frauds in the transactions. <br>\nIn the following lines of code I will talk about how to handle the imbalanced data ","a46dc33b":"Using just the correlation matrix to decide which features are highly correlated with the fraud class can be missleading in some cases where there is a lot of outliers. <br>\nFor this reason we will call our savior the boxplot, for more information how we use it go to this link : <a href=\"https:\/\/towardsdatascience.com\/understanding-boxplots-5e2df7bcbd51\">Here<\/a> <br>\nFor the sake of simplicity and the beauty of the notebook, we will choose the most imporatante features to analyse, and fix some threasholds. We will take the features with the absolute value of the the correlation coefficient is greater than 0.4.","59c1a3ba":"Splitting data using undersampling ","c3b9d086":"### Splitting Data","fa7881e5":"#### Classifiers","74ace197":"# Importing train datasets","b0e37299":"As it is shown in the prvious boxplots, for some features the correlation coefficient is strongly affected by the outliers, eg : V1 <br>\nSo to sumup the following features are the selected ones :\n* Positive correlation : V4, V11\n* Negative correlation : V17, V12, V14, V16, V10","4aa21b28":"As the input dataset is normilized, we should normalize the other feartures :D"}}