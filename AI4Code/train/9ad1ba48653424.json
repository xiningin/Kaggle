{"cell_type":{"e0ba5599":"code","c74c51ff":"code","ca621983":"code","5879434b":"code","b4633219":"code","08a65f69":"code","fa1d028c":"code","32ffaa1f":"code","24388756":"code","67aae978":"code","27e36d1d":"code","dafcaa7b":"code","ef11bb83":"code","ab6d411e":"code","27bd1441":"code","08f3e7f8":"markdown","08c9b954":"markdown","4314d478":"markdown","5b59c430":"markdown","c1b1ff24":"markdown","dd6223cf":"markdown","a99ec9a8":"markdown","0929e29e":"markdown","f867fe8c":"markdown","6c89a8ad":"markdown","7af32ce7":"markdown"},"source":{"e0ba5599":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as mpatches\n","c74c51ff":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","ca621983":"df.describe()","5879434b":"df.isna().sum().any()","b4633219":"df.Class.value_counts()","08a65f69":"sns.countplot(x='Class', data=df)","fa1d028c":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\n\n\nplt.show()","32ffaa1f":"from sklearn.preprocessing import RobustScaler\n\n# We will use RobustScaler as it is less prone to outliers.\nrob_scaler = RobustScaler()\n\namount_ = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ntime_ = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)\n\ndf.insert(0, 'Amount_', amount_)\ndf.insert(1, 'Time_', time_)\n\n# Amount and Time are now scaled!\n\ndf.head()","24388756":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\n# Suffling\ndf = df.sample(frac=1)\n\n# Splitting\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nbalanced_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle again\nbalanced_df = balanced_df.sample(frac=1, random_state=42)\n\nbalanced_df.head()","67aae978":"sns.countplot(x='Class', data=balanced_df)","27e36d1d":"# Correlating\ncorr = balanced_df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20})","dafcaa7b":"# Box-plots\nvariables = balanced_df.columns.values[:-1]\n\ni = 0\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(5,6,figsize=(24,18))\n\nfor feature in variables:\n    i += 1\n    plt.subplot(5,6,i)\n    sns.boxplot(y=feature, x='Class', data=balanced_df)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","ef11bb83":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n# New_df is from the random undersample data (fewer instances)\nX = balanced_df.drop('Class', axis=1)\ny = balanced_df['Class']\n\n\n# T-SNE Implementation\nX_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n\n# PCA Implementation\nX_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n\n# TruncatedSVD\nX_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\n\n# Plotting decompositions\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_cluster = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_cluster = mpatches.Patch(color='#AF0000', label='Fraud')\n\n\n# t-SNE scatter plot\nax1.scatter(X_tsne[:,0], X_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_tsne[:,0], X_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_cluster, red_cluster])\n\n\n# PCA scatter plot\nax2.scatter(X_pca[:,0], X_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_pca[:,0], X_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.grid(True)\n\nax2.legend(handles=[blue_cluster, red_cluster])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_svd[:,0], X_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_svd[:,0], X_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\n\nax3.grid(True)\n\nax3.legend(handles=[blue_cluster, red_cluster])\n\nplt.show()","ab6d411e":"# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split \n\nimport collections\n\n# Let us use the balanced data, undersampling.\nX = balanced_df.drop('Class', axis=1)\ny = balanced_df['Class']\n\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Converting to arrays.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\n\n\n# Using the following four classifiers\nclassifiers = {\n    \"Logisitic Regression\": LogisticRegression(),\n    \"K Nearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"Decision Tree Classifier\": DecisionTreeClassifier()\n}\n\n# Validating the scores, ROC Cureves, Confusion Matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, f1_score, roc_curve, confusion_matrix\n\nresults = pd.DataFrame(columns=['Accuracy %', 'Cross Val %', 'F1-Score'])\nrocs = {}\nconfusion_matrices = {}\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    \n    cv_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    accuracy = accuracy_score(y_test,y_pred)\n    f1 = f1_score(y_test,y_pred)    \n    results.loc[key] = [100*round(accuracy,4), round(cv_score.mean(),4)*100, round(f1,2)]\n    \n    rocs[key] = roc_curve(y_test, y_pred)\n    confusion_matrices[key] = confusion_matrix(y_test, y_pred)\nresults","27bd1441":"fig, ax = plt.subplots(2, 2,figsize=(14,6))\nfig.suptitle('Comparing Confusion Matrices', fontsize = 18)\n\ni=0;\nfor k,cm in confusion_matrices.items():\n    i += 1\n    ax = plt.subplot(2,2,i)\n    sns.heatmap(cm, annot=True, cmap=plt.cm.Greens)\n    ax.set_title(k, fontsize=12)\n    ax.set_xticklabels(['', ''], fontsize=8, rotation=90)\n    ax.set_yticklabels(['', ''], fontsize=8, rotation=360)\n\nplt.show()","08f3e7f8":"\nHere, in the Confusion Matrix:\n- Upper Left Square: The correctly classified no fraud transactions.\n- Upper Right Square: The incorrectly classified transactions as fraud cases, but the actual label is no fraud .\n- Lower Left Square: The incorrectly classified transactions as no fraud cases, but the actual label is fraud .\n- Lower Right Square: The correctly classified fraud transactions.\n\n## Summary\nWe can see here, the model is able to predict clearly with 90%+ accuracy but still needs improvements. \n- We can remove outliers\n- consider using SMOTE to use full dataset.\n\nWe can see still 1 transaction is classified as fraud, yet it is non fraud and 10 transactions are classified non-frauds while they were fraud.","08c9b954":"## Exploratory Data Analysis","4314d478":"We can see that the classes seem separable in t-SNE, hence our models should be able to separate transactions.\n\n## Classifiers (Undersampling)\n\nWe will use four types of classifiers and will analyze them.","5b59c430":"Hence, there is **no missing values**  in dataset.","c1b1ff24":"### Random Under-sampling\nTo make balanced dataset","dd6223cf":"# Credit Card Frauds\nIn this exercise, we are trying to develop various models to find fraud transactions. We will compare the accuracy of models in classifying a transaction as fraud or non-fraud.\n\nWe will begin with \n- basic **exploratory data analysis** to gather sense of data, then \n- we will do **preprocessing of data**, next\n- we will do **exploratory data visualization** to understand more about the classes and data, then\n- we will do **dimentionality reduction** using t-SNE, then\n- use different **Classifier Models** to fit the data, lastly\n- we will compare the **accuracy** of models.\n\nLets get started!\n\n**About the dataset:** The dataset contains transactions made by credit cards in September 2013 by European cardholders. It has 31 columns in which we have Time, Class and Amount, rest contains only numerical input variables which are the result of a PCA transformation. 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","a99ec9a8":"# Confusion Matrices\n","0929e29e":"## Dimentionality Reduction t-SNE\n\nt-SNE algorithm can pretty accurately cluster of cases that were fraud and non-fraud in our dataset. This gives us an indication that predictive models will perform pretty well in separating classes.","f867fe8c":"Now that our data is **balanced** and is **scaled**, we will start,\n\n## Exploratory Data Visualization","6c89a8ad":"We can see that overall **K Nearest** is more accurate than other three in this sample of under fitting.\n","7af32ce7":"We have 492 frauds out of 284,807 transactions. Thus, the dataset is highly **unbalanced**, the positive class (frauds) account for 0.172% of all transactions. We will handle this later by down sampling the majority class, non-fraud transactions.\n\n## Pre-processing Data\n\nChecking the distribution of data by time and amount. These are unscaled.\n\nWe can see that the data is highly imbalanced. We need to balance the data.\n\nWe will create sub-sample of data to have 50-50 fraud vs Non-fraud cases.\n\nThis is required to avoid overfitting and wrong calculations.\n\nHence, we will:\n- **Scale** the amound and time columns\n- make sub-set with **equal number** of cases for fraud and non-fraud.\n\n### Scaling"}}