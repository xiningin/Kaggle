{"cell_type":{"942f15d2":"code","a1a1ed8d":"code","39aae09f":"code","c6656932":"code","dfbcd4b0":"code","3cfe40cf":"code","7f50117a":"code","a407ea4b":"code","c14c0d69":"code","8231e2a2":"code","6b92c224":"code","787501c6":"code","65800390":"markdown","3d906f56":"markdown","c01affb1":"markdown","be9e9497":"markdown","e202e6be":"markdown","8d5a8910":"markdown","5c9f78d7":"markdown","9a3ee60a":"markdown","2e5d5a61":"markdown","46a9a0ca":"markdown","f7b7a965":"markdown"},"source":{"942f15d2":"import numpy as np\n\nfrom time import time\nimport scipy.stats as stats\nfrom sklearn.utils.fixes import loguniform\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.datasets import load_digits\nfrom sklearn.linear_model import SGDClassifier\n","a1a1ed8d":"X, y = load_digits(return_X_y=True)","39aae09f":"X","c6656932":"y","dfbcd4b0":"clf = SGDClassifier(loss='hinge', penalty='elasticnet', fit_intercept=True)","3cfe40cf":"def report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n                  .format(results['mean_test_score'][candidate],\n                          results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n","7f50117a":"param_dist = {'average': [True, False],\n              'l1_ratio': stats.uniform(0, 1),\n              'alpha': loguniform(1e-4, 1e0)}\n","a407ea4b":"n_iter_search = 20\n\nrandom_search = RandomizedSearchCV(\n    clf, \n    param_distributions=param_dist,\n    n_iter=n_iter_search\n)","c14c0d69":"start = time()\nrandom_search.fit(X, y)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n      \" parameter settings.\" % ((time() - start), n_iter_search))\nreport(random_search.cv_results_)\n","8231e2a2":"param_grid = {'average': [True, False],\n              'l1_ratio': np.linspace(0, 1, num=10),\n              'alpha': np.power(10, np.arange(-4, 1, dtype=float))}\n\n\n","6b92c224":"grid_search = GridSearchCV(clf, param_grid=param_grid)","787501c6":"start = time()\ngrid_search.fit(X, y)\n\nprint(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n      % (time() - start, len(grid_search.cv_results_['params'])))\n\nreport(grid_search.cv_results_)","65800390":"## specify parameters and distributions to sample from","3d906f56":"## use a full grid over all parameters","c01affb1":"# Comparing randomized search and grid search for hyperparameter estimation","be9e9497":"# Import Libraries","e202e6be":"# Load Data","8d5a8910":"## run grid search","5c9f78d7":"# Build the classifier","9a3ee60a":"## run randomized search","2e5d5a61":"`Cross validation` is a method for evaluating models. One well known use case is to evaluate what set of hyper parameters to use in a model, such as a learning rate in gradient descents.\n\nTo find the optimal hyperparameters, we take a set of candidate hyper parameters, train models for all of these and compare their fitness via cross validation. Finally we select the hyperparameters that gave the best CV score.\n\n* `Randomized Search Cross Validation`\ndocumentation at: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html\n\n* `Grid Search Cross Validation`\ndocumentation at: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n","46a9a0ca":"# Utility function to report best scores","f7b7a965":"This is a direct example from https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_randomized_search.html"}}