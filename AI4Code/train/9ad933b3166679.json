{"cell_type":{"b546cf06":"code","0fb79b0c":"code","2408f450":"code","4f644e27":"code","6d887d7b":"code","b7f5927b":"code","5604fe66":"code","119156dd":"code","6b7eb031":"markdown","0dbf4dd1":"markdown","0c4699e9":"markdown","3460bd1e":"markdown","8129829e":"markdown"},"source":{"b546cf06":"# Data manipulation\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport xgboost\n\n# Models and metrics. Since the data is small, we will test many models\nfrom sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm, tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","0fb79b0c":"df = pd.read_csv('..\/input\/fish-market\/Fish.csv')\ndf.head()","2408f450":"df.describe(include='all')\ndf['Species'].value_counts()","4f644e27":"# Split the data into features and targets\nX = df[['Weight', 'Length1', 'Length2', 'Length3', 'Height', 'Width']]\ny = df['Species']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, shuffle=True)","6d887d7b":"plt.figure(figsize=(10,5))\nplt.hist(y, bins = [0,1,2,3,4,5,6,7], align='left', rwidth=0.8, color = 'green')\nplt.title('Distribution of Fish Species')\nplt.xlabel('Fish Species')\nplt.ylabel('Amount of Fish with Given Species')\nplt.grid(b=True, axis='y')","b7f5927b":"# Here we are creating all of our models that we want to test\n# This code is modified from [1]\n\nclassifiers = []\nmodel1 = xgboost.XGBClassifier()\nclassifiers.append(model1)\nmodel2 = svm.SVC()\nclassifiers.append(model2)\nmodel3 = tree.DecisionTreeClassifier(class_weight = 'balanced')\nclassifiers.append(model3)\nmodel4 = RandomForestClassifier(class_weight = 'balanced')\nclassifiers.append(model4)\nmodel5 = LogisticRegression(class_weight = 'balanced')\nclassifiers.append(model5)\nmodel6 = KNeighborsClassifier( n_neighbors=1)\nclassifiers.append(model6)\nmodel7 = KNeighborsClassifier( n_neighbors=2)\nclassifiers.append(model7)\nmodel8 = KNeighborsClassifier( n_neighbors=3)\nclassifiers.append(model8)\nmodel9 = KNeighborsClassifier( n_neighbors=4)\nclassifiers.append(model9)\nmodel10 = KNeighborsClassifier( n_neighbors=5)\nclassifiers.append(model10)\nmodel11 = GaussianNB()\nclassifiers.append(model11)","5604fe66":"# This code is modified from [1]\nmaxAccuracy = 0\nmaxCV = 0\nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    y_pred= clf.predict(X_test)\n    acc = balanced_accuracy_score(y_test, y_pred)\n    print(\"Accuracy of %s is %s\"%(clf, acc))\n    cm = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix of %s is %s\"%(clf, cm))\n    \n    # Get CV Score and max single score. Check if it is max for all models\n    cvScore = cross_val_score(clf, X, y, cv = 6)\n    cvMean = np.mean(cvScore)\n    maxScore = np.max(cvScore)\n    if maxAccuracy < acc:\n        maxAccuracy = acc\n        model = clf\n    if maxCV < cvMean:\n        maxCV = cvMean\n        modelCV = clf\n    print(cvScore)","119156dd":"print(\"Our best model was\", model, \"with a balanced accuracy of\", maxAccuracy, \".\")\nprint(\"Our best cross-validation model was\", modelCV, \"with the score,\", maxCV, \".\")","6b7eb031":"# Begin to Select Best Model\n\n[Source [1] Code Modified from here](https:\/\/medium.com\/datadriveninvestor\/choosing-the-best-algorithm-for-your-classification-model-7c632c78f38f)","0dbf4dd1":"# But First, Visualize Data","0c4699e9":"# Import Data and Split to Train and Test","3460bd1e":"With a small dataset, it may be difficult to make accurate predictions. In particular, Whitefish may be difficult with under 10 entries.","8129829e":"# Conclusion and Results\nOur highest accuracy model was Logistic Regression. After tuning parameters, setting class_weights to balanced improved accuracy as it helped address our small and uneven sample distribution as seen in our graph above. We used balanced accuracy due to this inbalance in data. \n\nAn accuracy of 90.7% is quite high and given the dataset size, is acceptable. With more data, we can expect the model to be a great predictor of fish species."}}