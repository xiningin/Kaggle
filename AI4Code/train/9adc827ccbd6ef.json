{"cell_type":{"73a86db3":"code","d65faddd":"code","048469e7":"code","8052cdd5":"code","5b426303":"code","815e58b8":"code","d9a8b3c6":"code","7fc15e53":"code","cd2b7bc1":"code","8a3a00f2":"code","56587de8":"code","acd3a78a":"code","e5aef2aa":"code","803654d0":"code","33b021c1":"code","c25a0c67":"code","3cdabd22":"code","669aaeae":"code","079513e8":"code","439e6225":"code","75ffd873":"code","e488eec1":"code","060267ad":"code","1956a5fd":"code","a78168a6":"code","2605b0f7":"code","93bac120":"code","94ff9ec8":"code","e7afb485":"code","96b8250f":"markdown","a28e26b6":"markdown","f94615ac":"markdown","3a7a8a33":"markdown","d8baf3aa":"markdown","c835f6ab":"markdown","caf9ff42":"markdown","81829fab":"markdown","c1f3f25d":"markdown"},"source":{"73a86db3":"import numpy as np \nimport math\nimport pandas as pd \nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom wordcloud import STOPWORDS\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport re\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport spacy\nfrom spacy import displacy\nfrom scipy.sparse import lil_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set();\nfrom numpy.linalg import norm\nfrom scipy.linalg import svd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nnlp = spacy.load('en_core_web_sm')  \nporter = PorterStemmer()","d65faddd":"#Load the dataset and remove rows containing null values\ndata = pd.read_csv('..\/input\/scrapped-lyrics-from-6-genres\/lyrics-data.csv')\nartists = pd.read_csv('..\/input\/scrapped-lyrics-from-6-genres\/artists-data.csv')\ndata.dropna(inplace=True)","048469e7":"#Keep only one song for each artist\nonly_one_per_author = True\n#Keep only one genre for each artist (to have a link between a song and a genre)\nonly_one_genre_per_author = True\n#Avoid duplicates between titles, in order to retrive a unique song by its title\nonly_one_song_per_title = True\n#Load the term document matrix instead of generating it\nload_termdocMatrix = True\n\nif only_one_per_author:\n    data = data.drop_duplicates(subset=['ALink'], keep='first')\nif only_one_genre_per_author:\n    artists = artists.drop_duplicates(subset=['Link'], keep='first')\nif only_one_song_per_title:\n    data = data.drop_duplicates(subset=['SName'], keep='first')\n\n#Keep only the songs in english    \ndata = data.drop(data[data['Idiom'] != 'ENGLISH'].index)\ndata.shape\n\n#The two tables are merged\ndataAll = pd.merge(left=data, right=artists, left_on='ALink', right_on='Link')\ndataAll.head(10)","8052cdd5":"def get_wordcloud(documents):\n    \n    \"\"\"Given a set of texts, plot the wordcoud containing the most common words \"\"\"\n    \n    text = \"\".join(str(c) for c in documents)\n    wc = WordCloud(background_color=\"white\", max_words=2000,\n                   stopwords=STOPWORDS, max_font_size=100,\n                   random_state=42, width=500, height=500)\n    wc.generate(text)\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()\n","5b426303":"def clean_text(text):\n    \n    \"\"\"Given a text, return the same text without words smaller than two letters, numbersd and punctuation. \n    Apply also stemming on the words of the text\"\"\"\n    \n    text = ' '.join([c.lower() for c in str(text).split() if len(c)>2])\n    numbers_patterns = re.compile('[0-9]+[\\w]*')\n    text = re.sub(numbers_patterns, '', text)\n    punctiation_pattern = re.compile('[!-_@#$%^''&*()?`<>;\\.,:\"]')\n    text = re.sub(punctiation_pattern, '', text)\n    text = ' '.join(porter.stem(word) for word in text.split()) \n    return text","815e58b8":"get_wordcloud(dataAll['Lyric'])","d9a8b3c6":"dataAll['Lyric'] = dataAll['Lyric'].apply(clean_text)\nget_wordcloud(dataAll['Lyric'])","7fc15e53":"def createDictionaryAndTermDocumentMatrixFreqency(dataAll):\n    \n    \"\"\"Given the dataset, build a dictionary (with term - list of documents containing the term) \n    and the term-document matrix\"\"\"\n    \n    dictionary = {}\n    for index, row in dataAll.iterrows():\n        tokens = [w.lemma_ for w in nlp(row['Lyric'])\n                  if not w.is_stop and w.pos_ != 'PUNCT']\n        for t in tokens:\n            dictionary[t] = dictionary.setdefault(t, [])\n            dictionary[t] += [index]\n    A = lil_matrix((len(dictionary.keys()), len(dataAll)), dtype=int)\n    for i, t in enumerate(dictionary.keys()):\n        for j in dictionary[t]:\n            A[i, j] = A[i, j]+1\n    tdm = pd.DataFrame(A.toarray(), index=dictionary.keys(), columns=dataAll['SName'])\n    tdm\n    return dictionary, tdm\n\ndef updateTermDocumentMatrixTF(tdm):\n    \n    \"\"\"Given the term-document matrix, \n     update the values with the term frequency\"\"\"\n    \n    for i in range(tdm.shape[1]): #iterate over cols\n        d=0\n        for j in range(tdm.shape[0]): #iterate over rows\n            d = d + tdm.iloc[j,i]\n        \n        tdm.iloc[i] = tdm.iloc[i].apply(lambda x: x\/float(d))\n            \n    return tdm\n\ndef updateTermDocumentMatrixIDF(tdm):\n    \n    \"\"\"Given the term-document matrix with term frequency, \n    update the values multipling them for the inverse document frequency\"\"\"\n    \n    for i in range(tdm.shape[0]): #iterate over rows\n        idf=0\n        for j in range(tdm.shape[1]): #iterate over cols\n            if(tdm.iloc[i,j] > 0):\n                idf = idf + 1\n        idf = math.log(tdm.shape[1]\/float(idf)   , 10)  \n        for j1 in range(tdm.shape[1]): #iterate over cols\n            tdm.iloc[i,j1] = tdm.iloc[i,j1] * idf\n            \n    return tdm","cd2b7bc1":"if load_termdocMatrix:\n    termDocMatrix = pd.read_csv('..\/input\/termdocmatrix\/termDocMatricTFIDF.csv')\n    dictionary, tdm = createDictionaryAndTermDocumentMatrixFreqency(dataAll)\nelse:\n    dictionary, termDocMatrix = createDictionaryAndTermDocumentMatrixFreqency(dataAll)\n    termDocMatrix = updateTermDocumentMatrixTF(termDocMatrix)\n    termDocMatrix.to_csv('termDocMatricTF.csv',index=False)\n    termDocMatrix = updateTermDocumentMatrixIDF(termDocMatrix)\n    termDocMatrix.to_csv('termDocMatricTFIDF.csv',index=False)","8a3a00f2":"tdm","56587de8":"termDocMatrix","acd3a78a":"def singularValueDecomposition(termDocMatrix, k, dictionary, dataAll):\n    \n    \"\"\"Given the term-document matrix, the number of pseudp-terms, the dictionary and the dataset, \n    compute the singular value decomposition\"\"\"\n    \n    u, s, vt = svd(termDocMatrix)                              \n    VT = vt[:k, :]\n    df_vt = pd.DataFrame(VT, columns=dataAll['SName'])\n    US = u[:, :k] @ np.diag(s[:k])\n    df_us = pd.DataFrame(US, index=dictionary.keys())\n    return US, df_vt, df_us\n\ndef retrieve(query, W, Vt):\n\n    \"\"\"Given a free text, retrieve the documents\n    relevant to the query. \"\"\"\n    \n    query = clean_text(query)\n    querytokns = [w.lemma_ for w in nlp(query)\n              if not w.is_stop and w.pos_ != 'PUNCT']\n    q = W.loc[querytokns, :].mean(axis=0)\n    nrm = norm(q)\n    q \/= nrm\n    sim = list()\n    i = 0\n    for column in df_vt:\n        sim.append((i , q.dot(df_vt.iloc[:,i])\/norm(q) \/ norm(df_vt.iloc[:,i]), Vt.columns[i] ))\n        i = i+1\n    sim.sort(key=lambda x:x[1], reverse=True)\n    \n    return sim","e5aef2aa":"k = 50\nUS, df_vt, df_us = singularValueDecomposition(termDocMatrix, k, dictionary, dataAll)","803654d0":"df_us.head(100)","33b021c1":"df_vt.head(10)","c25a0c67":"query1 = \"Back in black I hit the sack I've been too long, I'm glad to be back Yes, I'm let loose From the noose that's me hanging about I've been looking at the sky Cause it's gettin me high Forget the hearse cause I never die I got nine lives Cat's eyes Abusin' every one of them and running wild\"\nsim = retrieve(query1, df_us, df_vt)\nsim[:3]","3cdabd22":"k = 10\nUS, df_vt, df_us = singularValueDecomposition(termDocMatrix, k, dictionary, dataAll)\nsim = retrieve(query1, df_us, df_vt)\nsim[:40]","669aaeae":"k = 20\nUS, df_vt, df_us = singularValueDecomposition(termDocMatrix, k, dictionary, dataAll)\nsim = retrieve(query1, df_us, df_vt)\nsim[:10]","079513e8":"k = 60\nUS, df_vt, df_us = singularValueDecomposition(termDocMatrix, k, dictionary, dataAll)\nsim = retrieve(query1, df_us, df_vt)\nsim[:10]","439e6225":"term1 = df_us.iloc[:,56]\nterm1 = term1.sort_values(ascending=False)\ntop_5 = term1[:5]\nplt.title('Top terms along the axis of Latent concept 56')\nfig = sns.barplot(x=top_5.values, y=top_5.index)","75ffd873":"term1 = df_us.iloc[:,15]\nterm1 = term1.sort_values(ascending=False)\ntop_5 = term1[:5]\nplt.title('Top terms along the axis of Latent concept 56')\nfig = sns.barplot(x=top_5.values, y=top_5.index)","e488eec1":"#Compare two words to get the similarity between terms\nlove = df_us.loc['love', :].to_numpy()\ngirl = df_us.loc['girl', :].to_numpy()\npain = df_us.loc['pain', :].to_numpy()\ntonight = df_us.loc['tonight', :].to_numpy()\ntime = df_us.loc['time', :].to_numpy()\nbabi = df_us.loc['babi', :].to_numpy()\nboy = df_us.loc['boy', :].to_numpy()\n\n\nprint(\"Similarity love and pain = \", love.dot(pain) \/ norm(love) \/ norm(pain))\nprint(\"Similarity tonight and pain = \", pain.dot(tonight) \/ norm(pain) \/ norm(tonight))\nprint(\"Similarity love and time = \", love.dot(time) \/ norm(love) \/ norm(time))\nprint(\"Similarity time and tonight = \", tonight.dot(time) \/ norm(tonight) \/ norm(time))\nprint(\"Similarity girl and love = \", love.dot(girl) \/ norm(love) \/ norm(girl))\nprint(\"Similarity girl and babi = \", babi.dot(girl) \/ norm(babi) \/ norm(girl))\nprint(\"Similarity girl and boy = \", boy.dot(girl) \/ norm(boy) \/ norm(girl))","060267ad":"#Plot the term vectors projections on two axis (concept 15 and 56)\n\nbye = df_us.loc['bye', :].to_numpy()\nbad = df_us.loc['bad', :].to_numpy()\nmoonlight = df_us.loc['moonlight', :].to_numpy()\ntough = df_us.loc['tough', :].to_numpy()\nplayer = df_us.loc['player', :].to_numpy()\ndad = df_us.loc['dad', :].to_numpy()\nrock = df_us.loc['rock', :].to_numpy()\n\nfig = plt.figure(figsize=(10, 8)); ax = fig.gca()\nax.arrow(0, 0, tough[15], tough[56],       width=0.10, alpha=0.3,color='r')\nax.arrow(0, 0, bad[15], bad[56],       width=0.10, alpha=0.3,color='r')\nax.arrow(0, 0, moonlight[15], moonlight[56], width=0.10, alpha=0.3,color='r')\nax.arrow(0, 0, player[15], player[56], width=0.10, alpha=0.3,color='r')\nax.arrow(0, 0, dad[15], dad[56], width=0.10, alpha=0.3,color='r')\nax.arrow(0, 0, rock[15], rock[56], width=0.10, alpha=0.3,color='r')\nax.scatter(US[: , 15], US[: ,56], alpha=0.5)\nax.scatter(0, 0,  color='r', marker='*', s=150, alpha=0.6);\n\nax.annotate('bye', (tough[15], tough[56]))\nax.annotate('bad', (bad[15], bad[56]))\nax.annotate('pain', (moonlight[15], moonlight[56]))\nax.annotate('tonight', (player[15], player[56]))\nax.annotate('dad', (dad[15], dad[56]))\nax.annotate('rock', (rock[15], rock[56]))\n\nax.set_xlim((-3, 12)); ax.set_ylim((-10, 12));\nax.set_title('Term Vectors');","1956a5fd":"#Compare two songs to get the similarity between documents\nsong1 = df_vt.loc[:, 'More Than This'].to_numpy()\nsong2    = df_vt.loc[:, 'Back In Black'].to_numpy()\nsong3    = df_vt.loc[:, 'Fantasy'].to_numpy()\n\nprint(\"Similarity More Than This and Back In Black = \", song1.dot(song2)\/(norm(song1)*norm(song2)))\nprint(\"Similarity More Than This and Fantasy = \", song1.dot(song3)\/(norm(song1)*norm(song3)))\nprint(\"Similarity Back In Black and Fantasy = \", song2.dot(song3)\/(norm(song2)*norm(song3)))","a78168a6":"query2 = \"love heart\"\nsim2 = retrieve(query2, df_us, df_vt)\nsim2[:5]\n\n#Remembering Sunday -> relevant\n#Did You Enjoyed The Feeling? -> not relevant\n#Love Exists -> relevant\n#If It Means A Lot To You -> not relevant\n#I Can't Live Without Your Love -> relevant\n\n#precision: 3\/5 = 0.6","2605b0f7":"query2 = \"happy party smile\"\nsim2 = retrieve(query2, df_us, df_vt)\nsim2[:5]\n\n#Try -> relevant\n#Party For One -> relevant\n#You Can't Get Away From Me -> relevant\n#Everybody Know Me -> relevant\n#Girls Like You (Feat. Cardi B) -> not relevant\n\n#precision: 4\/5 = 0.8","93bac120":"query2 = \"dark night\"\nsim2 = retrieve(query2, df_us, df_vt)\nsim2[:5]\n\n#Whataya Want From Me -> not relevant\n#Too Little, Too Late -> relevant\n#Don't Break Me Down -> relevant\n#More Than This-> relevant\n#Hard Times Come Easy -> not relevant\n\n#precision: 3\/5 = 0.6","94ff9ec8":"query2 = \"rock monster bad\"\nsim2 = retrieve(query2, df_us, df_vt)\nsim2[:5]\n\n#I Was A Teenage Werewolf -> relevant\n#Drop The Mask-> relevant\n#Monster -> relevant\n#All Monsters -> relevant\n#Burning -> relevant\n\n#precision: 5\/5 = 1","e7afb485":"dataAll['Lyric'][777]","96b8250f":"**Using a rank k = 50 it can be noticed that, considering the full text of a song (in this case 'Back In Black') as the query, the song is not retrieved as the best match. So, different values of k are used to analyze the retrivial and the smallest value of k for which 'Back In Black' is considered the best match is chosen for the model.** ","a28e26b6":"**In the first part of the project, the dataset is analyzed. There are two tables, one containing the texts and the information about the songs, the other containing information about the artists. The tables are linked by the artist-link and can be merged by a join. \nIt is possible to visualize the most common words of the songs by a wordcloud. \nAfter that, the texts of the songs are cleaned, removing stop words and punctuation and performing stemming.**","f94615ac":"**It is also possible to compare different terms by the cosine similarity in the space of concepts. To visualize the terms, two pseudo-terms (the same of the previous barplots) are used as x-axis and y-axis and the terms are scattered**","3a7a8a33":"**So precision can be calculated as the average of the queries performed : (0.6 + 0.8 + 0.6 + 1)\/4 = 0.75**\n","d8baf3aa":"**Using the term-document matrix it is possible to compute a low-rank approximation (rank k), thanks to the singular value decomposition. Three matrices are obtained:**\n* **u term-concept matrix**\n* **s concept matrix**\n* **v document-concept**\n\n**These matrices can be used as a representation of the documents in term of concepts (pseudo-terms) for comparing different documents. Also a query can be represented in the space of concept in order to get the cosine similarity with each document.**","c835f6ab":"**It is possible to have an idea of the quality of the system by executing some queries and deciding if the retrieved documents are relevant or not. The decision is taken reading the lyrics and looking at the key words of the query. The \"precision at K\" method is used , with K equal to 5.**","caf9ff42":"**Each concept is represented by the terms and the weight that each term has on that concept. For example, the top 5 terms for concepts 56 and 15 are:**","81829fab":"**From the dataset, it is possible to build a dictionary with all the terms associated to a list of songs that contain that term.**\n\n**From the dictionary, a sparse matrix (called term-document matrix) is created with the songs as columns and the terms as rows. If a term *i* is present in the song *j*, then *M[i,j] = 1* otherwise *M[i,j] = 0***\n\n**This matrix is then updated with the TF-IDF values**","c1f3f25d":"**To compare two documents, the cosine similarity is used:**"}}