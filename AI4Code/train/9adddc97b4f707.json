{"cell_type":{"296c4c15":"code","c153c851":"code","3f8fc256":"code","58e63ab1":"code","9dea00ce":"code","3e88daf3":"code","3794dd4a":"code","0d4bc7d0":"code","c3c53ec1":"code","38353622":"code","6d01ab7e":"code","859d1d51":"code","abd727b2":"code","5cc519db":"code","f1376025":"code","31b7ca2c":"code","01fb24d6":"code","de1f8b33":"code","c9c89202":"code","1c099f6a":"code","10e793ad":"markdown"},"source":{"296c4c15":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport zipfile\nimport gc\nfrom tqdm import tqdm_notebook as tqdm\nimport re\nprint(os.listdir(\"..\/input\"))","c153c851":"!wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall()\n!ls 'uncased_L-12_H-768_A-12'","3f8fc256":"!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/extract_features.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/tokenization.py","58e63ab1":"import modeling\nimport extract_features\nimport tokenization\nimport tensorflow as tf\nimport spacy\nnlp = spacy.load('en_core_web_lg')","9dea00ce":"test_df  = pd.read_table('..\/input\/gap-coreference\/gap-development.tsv')\ntrain_df = pd.read_table('..\/input\/gap-coreference\/gap-test.tsv')\nval_df   = pd.read_table('..\/input\/gap-coreference\/gap-validation.tsv')\ntest_df.head()","3e88daf3":"#This code is referenced from \n#https:\/\/www.kaggle.com\/keyit92\/coref-by-mlp-cnn-coattention\n\ndef bs(lens, target):\n    low, high = 0, len(lens) - 1\n\n    while low < high:\n        mid = low + int((high - low) \/ 2)\n\n        if target > lens[mid]:\n            low = mid + 1\n        elif target < lens[mid]:\n            high = mid\n        else:\n            return mid + 1\n\n    return low\n\ndef bin_distance(dist):\n    \n    buckets = [1, 2, 3, 4, 5, 8, 16, 32, 64]  \n    low, high = 0, len(buckets)\n    while low < high:\n        mid = low + int((high-low) \/ 2)\n        if dist > buckets[mid]:\n            low = mid + 1\n        elif dist < buckets[mid]:\n            high = mid\n        else:\n            return mid\n\n    return low\n\ndef distance_features(P, A, B, char_offsetP, char_offsetA, char_offsetB, text, URL):\n    \n    doc = nlp(text)\n    \n    lens = [token.idx for token in doc]\n    mention_offsetP = bs(lens, char_offsetP) - 1\n    mention_offsetA = bs(lens, char_offsetA) - 1\n    mention_offsetB = bs(lens, char_offsetB) - 1\n    \n    mention_distA = mention_offsetP - mention_offsetA \n    mention_distB = mention_offsetP - mention_offsetB\n    \n    splited_A = A.split()[0].replace(\"*\", \"\")\n    splited_B = B.split()[0].replace(\"*\", \"\")\n    \n    if re.search(splited_A[0], str(URL)):\n        contains = 0\n    elif re.search(splited_B[0], str(URL)):\n        contains = 1\n    else:\n        contains = 2\n    \n    dist_binA = bin_distance(mention_distA)\n    dist_binB = bin_distance(mention_distB)\n    output =  [dist_binA, dist_binB, contains]\n    \n    return output\n\ndef extract_dist_features(df):\n    \n    index = df.index\n    columns = [\"D_PA\", \"D_PB\", \"IN_URL\"]\n    dist_df = pd.DataFrame(index = index, columns = columns)\n\n    for i in tqdm(range(len(df))):\n        \n        text = df.loc[i, 'Text']\n        P_offset = df.loc[i,'Pronoun-offset']\n        A_offset = df.loc[i, 'A-offset']\n        B_offset = df.loc[i, 'B-offset']\n        P, A, B  = df.loc[i,'Pronoun'], df.loc[i, 'A'], df.loc[i, 'B']\n        URL = df.loc[i, 'URL']\n        \n        dist_df.iloc[i] = distance_features(P, A, B, P_offset, A_offset, B_offset, text, URL)\n        \n    return dist_df","3794dd4a":"test_dist_df = extract_dist_features(test_df)\ntest_dist_df.to_csv('test_dist_df.csv', index=False)\nval_dist_df = extract_dist_features(val_df)\nval_dist_df.to_csv('val_dist_df.csv', index=False)\ntrain_dist_df = extract_dist_features(train_df)\ntrain_dist_df.to_csv('train_dist_df.csv', index=False)","0d4bc7d0":"def count_char(text, offset):   \n    count = 0\n    for pos in range(offset):\n        if text[pos] != \" \": count +=1\n    return count\n\ndef candidate_length(candidate):\n    count = 0\n    for i in range(len(candidate)):\n        if candidate[i] !=  \" \": count += 1\n    return count\n\ndef count_token_length_special(token):\n    count = 0\n    special_token = [\"#\", \" \"]\n    for i in range(len(token)):\n        if token[i] not in special_token: count+=1\n    return count\n\ndef embed_by_bert(df):\n    \n    text = df['Text']\n    text.to_csv('input.txt', index=False, header=False)\n    os.system(\"python3 extract_features.py \\\n               --input_file=input.txt \\\n               --output_file=output.jsonl \\\n               --vocab_file=uncased_L-12_H-768_A-12\/vocab.txt \\\n               --bert_config_file=uncased_L-12_H-768_A-12\/bert_config.json \\\n               --init_checkpoint=uncased_L-12_H-768_A-12\/bert_model.ckpt \\\n               --layers=-1 \\\n               --max_seq_length=256 \\\n               --batch_size=8\")\n    \n    bert_output = pd.read_json(\"output.jsonl\", lines = True)\n    bert_output.head()\n    \n    os.system(\"rm input.txt\")\n    os.system(\"rm output.jsonl\")\n    \n    index = df.index\n    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n    emb = pd.DataFrame(index = index, columns = columns)\n    emb.index.name = \"ID\"\n    \n    for i in tqdm(range(len(text))):\n        \n        features = bert_output.loc[i, \"features\"]\n        P_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'Pronoun-offset'])\n        A_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'A-offset'])\n        B_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'B-offset'])\n        A_length = candidate_length(df.loc[i, 'A'])\n        B_length = candidate_length(df.loc[i, 'B'])\n        \n        emb_A = np.zeros(768)\n        emb_B = np.zeros(768)\n        emb_P = np.zeros(768)\n        \n        char_count = 0\n        cnt_A, cnt_B = 0, 0\n        \n        for j in range(2, len(features)):\n            token = features[j][\"token\"]\n            token_length = count_token_length_special(token)\n            if char_count == P_char_start:\n                emb_P += np.asarray(features[j][\"layers\"][0]['values']) \n            if char_count in range(A_char_start, A_char_start+A_length):\n                emb_A += np.asarray(features[j][\"layers\"][0]['values'])\n                cnt_A += 1\n            if char_count in range(B_char_start, B_char_start+B_length):\n                emb_B += np.asarray(features[j][\"layers\"][0]['values'])\n                cnt_B += 1                \n            char_count += token_length\n        \n        emb_A \/= cnt_A\n        emb_B \/= cnt_B\n        \n        label = \"Neither\"\n        if (df.loc[i,\"A-coref\"] == True):\n            label = \"A\"\n        if (df.loc[i,\"B-coref\"] == True):\n            label = \"B\"\n\n        emb.iloc[i] = [emb_A, emb_B, emb_P, label]\n        \n    return emb     ","c3c53ec1":"test_emb = embed_by_bert(test_df)\ntest_emb.to_json(\"contextual_embeddings_gap_test.json\", orient = 'columns')\nvalidation_emb = embed_by_bert(val_df)\nvalidation_emb.to_json(\"contextual_embeddings_gap_validation.json\", orient = 'columns')\ntrain_emb = embed_by_bert(train_df)\ntrain_emb.to_json(\"contextual_embeddings_gap_train.json\", orient = 'columns')","38353622":"from keras.layers import *\nimport keras.backend as K\nfrom keras.models import *\nimport keras\nfrom keras import optimizers\nfrom keras import callbacks\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nclass End2End_NCR():\n    \n    def __init__(self, word_input_shape, dist_shape, embed_dim=20): \n        \n        self.word_input_shape = word_input_shape\n        self.dist_shape   = dist_shape\n        self.embed_dim    = embed_dim\n        self.buckets      = [1, 2, 3, 4, 5, 8, 16, 32, 64] \n        self.hidden_dim   = 150\n        \n    def build(self):\n        \n        A, B, P = Input((self.word_input_shape,)), Input((self.word_input_shape,)), Input((self.word_input_shape,))\n        dist1, dist2 = Input((self.dist_shape,)), Input((self.dist_shape,))\n        inputs = [A, B, P]\n        dist_inputs = [dist1, dist2]\n        \n        self.dist_embed = Embedding(len(self.buckets)+1, self.embed_dim)\n        self.ffnn       = Sequential([Dense(self.hidden_dim, use_bias=True),\n                                     Activation('relu'),\n                                     Dropout(rate=0.2, seed = 7),\n                                     Dense(1, activation='linear')])\n        \n        dist_embeds = [self.dist_embed(dist) for dist in dist_inputs]\n        dist_embeds = [Flatten()(dist_embed) for dist_embed in dist_embeds]\n        \n        #Scoring layer\n        #In https:\/\/www.aclweb.org\/anthology\/D17-1018, \n        #used feed forward network which measures if it is an entity mention using a score\n        #because we already know the word is mention.\n        #In here, I just focus on the pairwise score\n        PA = Multiply()([inputs[0], inputs[2]])\n        PB = Multiply()([inputs[1], inputs[2]])\n        #PairScore: sa(i,j) =wa\u00b7FFNNa([gi,gj,gi\u25e6gj,\u03c6(i,j)])\n        # gi is embedding of Pronoun\n        # gj is embedding of A or B\n        # gi\u25e6gj is element-wise multiplication\n        # \u03c6(i,j) is the distance embedding\n        PA = Concatenate(axis=-1)([P, A, PA, dist_embeds[0]])\n        PB = Concatenate(axis=-1)([P, B, PB, dist_embeds[1]])\n        PA_score = self.ffnn(PA)\n        PB_score = self.ffnn(PB)\n        # Fix the Neither to score 0.\n        score_e  = Lambda(lambda x: K.zeros_like(x))(PB_score)\n        \n        #Final Output\n        output = Concatenate(axis=-1)([PA_score, PB_score, score_e]) # [Pronoun and A score, Pronoun and B score, Neither Score]\n        output = Activation('softmax')(output)        \n        model = Model(inputs+dist_inputs, output)\n        \n        return model\n","6d01ab7e":"def create_input(embed_df, dist_df):\n    \n    assert len(embed_df) == len(dist_df)\n    all_P, all_A, all_B = [] ,[] ,[]\n    all_label = []\n    all_dist_PA, all_dist_PB = [], []\n    url_A, url_B = [], []\n    \n    for i in tqdm(range(len(embed_df))):\n        \n        all_P.append(embed_df.loc[i, \"emb_P\"])\n        all_A.append(embed_df.loc[i, \"emb_A\"])\n        all_B.append(embed_df.loc[i, \"emb_B\"])\n        all_dist_PA.append(dist_df.loc[i, \"D_PA\"])\n        all_dist_PB.append(dist_df.loc[i, \"D_PB\"])\n        \n        if dist_df.loc[i, \"IN_URL\"] == 0:\n            url_A.append(1)\n            url_B.append(0)\n        elif dist_df.loc[i, \"IN_URL\"] == 1:\n            url_A.append(0)\n            url_B.append(1)\n        else:\n            url_A.append(0)\n            url_B.append(0)\n        \n        label = embed_df.loc[i, \"label\"]\n        if label == \"A\": \n            all_label.append(0)\n        elif label == \"B\": \n            all_label.append(1)\n        else: \n            all_label.append(2)\n    \n    return [np.asarray(all_A), np.asarray(all_B), np.asarray(all_P),\n            np.expand_dims(np.asarray(all_dist_PA),axis=1),\n            np.expand_dims(np.asarray(all_dist_PB),axis=1)],all_label","859d1d51":"new_emb_df = pd.concat([train_emb, validation_emb])\nnew_emb_df = new_emb_df.reset_index(drop=True)\nnew_dist_df = pd.concat([train_dist_df, val_dist_df])\nnew_dist_df = new_dist_df.reset_index(drop=True)\n\nnew_emb_df.head()","abd727b2":"X_train, y_train = create_input(new_emb_df, new_dist_df)\nX_test, y_test = create_input(test_emb, test_dist_df)","5cc519db":"model = End2End_NCR(word_input_shape=X_train[0].shape[1], dist_shape=X_train[3].shape[1]).build()\nmodel.summary()","f1376025":"SVG(model_to_dot(model).create(prog='dot', format='svg'))","31b7ca2c":"min_loss = 1.0\nbest_model = 0\n# Use Kfold to get best model\n\nfrom sklearn.model_selection import KFold\nn_fold = 5\nkfold = KFold(n_splits=n_fold, shuffle=True, random_state=3)\nfor fold_n, (train_index, valid_index) in enumerate(kfold.split(X_train[0])):\n    \n    X_tr  = [inputs[train_index] for inputs in X_train]\n    X_val = [inputs[valid_index] for inputs in X_train]\n    y_tr  = np.asarray(y_train)[train_index]\n    y_val = np.asarray(y_train)[valid_index]\n    \n    model = End2End_NCR(word_input_shape=X_train[0].shape[1], dist_shape=X_train[3].shape[1]).build()\n    model.compile(optimizer=optimizers.Adam(lr=0.001), loss=\"sparse_categorical_crossentropy\")\n    file_path = \"best_model_{}.hdf5\".format(fold_n+1)\n    check_point = callbacks.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 0, save_best_only = True, mode = \"min\")\n    early_stop = callbacks.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=100)\n    hist = model.fit(X_tr, y_tr, batch_size=128, epochs=1000, validation_data=(X_val, y_val), verbose=0,\n              shuffle=True, callbacks = [check_point, early_stop])\n    \n    if min(hist.history['val_loss']) < min_loss:\n        min_loss = min(hist.history['val_loss'])\n        best_model = fold_n + 1","01fb24d6":"del model","de1f8b33":"#Use best model to predict\nmodel = End2End_NCR(word_input_shape=X_train[0].shape[1], dist_shape=X_train[3].shape[1]).build()\nmodel.load_weights(\".\/best_model_{}.hdf5\".format(best_model))\npred = model.predict(x = X_test, verbose = 0)\n\nsub_df_path = os.path.join('..\/input\/gendered-pronoun-resolution\/', 'sample_submission_stage_1.csv')\nsub_df = pd.read_csv(sub_df_path)\nsub_df.loc[:, 'A'] = pd.Series(pred[:, 0])\nsub_df.loc[:, 'B'] = pd.Series(pred[:, 1])\nsub_df.loc[:, 'NEITHER'] = pd.Series(pred[:, 2])\n\nsub_df.head(20)","c9c89202":"from sklearn.metrics import log_loss\ny_one_hot = np.zeros((2000, 3))\nfor i in range(len(y_test)):\n    y_one_hot[i, y_test[i]] = 1\nlog_loss(y_one_hot, pred) # Calculate the log loss ","1c099f6a":"sub_df.to_csv(\"submission.csv\", index=False)","10e793ad":"**In this kernel, I try to use Pretrain Bert Model and Feed Forword Network.\n\u203bI am just starter for deep learning.**\n**If there are some mistakes, please comment.**\n\n1. used code from the kernel below to get word Embedding from pretrain Bert mode.\nhttps:\/\/www.kaggle.com\/mateiionita\/taming-the-bert-a-baseline\n\n2. Inspired by https:\/\/arxiv.org\/pdf\/1805.04893v1.pdf and https:\/\/cs.stanford.edu\/people\/kevclark\/resources\/clark-manning-emnlp2016-deep.pdf.\nI assume that FFNN reduce A, B, Pronoun dimensions(from Bert)  and  only  keep  information relevant to coreference decisions. \n\n"}}