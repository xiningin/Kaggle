{"cell_type":{"d346c4bb":"code","9d7b679e":"code","5e6fbf21":"code","387abdf6":"code","217ff219":"code","9311bd11":"code","dac0a15f":"code","37082c95":"code","cf9f0ce5":"code","d75ba70d":"code","831bb141":"code","caebc1bd":"code","61d9c234":"code","d1d182de":"code","81453c1f":"code","0c41fe46":"code","0418d3d8":"code","d8c26d38":"code","3cd759e4":"code","46cda5d3":"code","3b10f492":"code","7e9dcd48":"code","c6d488bf":"code","9c4b9e38":"code","3b8532ff":"code","871dd052":"code","6399f837":"markdown"},"source":{"d346c4bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9d7b679e":"import pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom collections import Counter\nfrom sklearn import preprocessing \nfrom keras.utils import to_categorical\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nimport warnings\nwarnings.filterwarnings('ignore')","5e6fbf21":"titanic_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_gender= pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ntitanic_test= pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ncombine = [titanic_train, titanic_test]","387abdf6":"## Join train and test datasets to obtain the same number of features during categorical conversion\ntrain_len = len(titanic_train)\ntitanic_dataset =  pd.concat(objs=[titanic_train, titanic_test], axis=0).reset_index(drop=True)","217ff219":"# Fill empty and NaNs values with NaN\ntitanic_dataset = titanic_dataset.fillna(np.nan)\n\n# Check for Null values\ntitanic_dataset.isnull().sum()","9311bd11":"#Fill Fare missing values with the median value\ntitanic_dataset[\"Fare\"] = titanic_dataset[\"Fare\"].fillna(titanic_dataset[\"Fare\"].median())\n\n# As Fare distribution is skewed ,Applying log to Fare to reduce skewness distribution\ntitanic_dataset[\"Fare\"] = titanic_dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","dac0a15f":"titanic_train[[\"Sex\",\"Survived\"]].groupby('Sex').mean()","37082c95":"#Fill Embarked nan values of dataset set with 'S' most frequent value\ntitanic_dataset[\"Embarked\"] = titanic_dataset[\"Embarked\"].fillna(\"S\")","cf9f0ce5":"# Filling missing value of Age \n\n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(titanic_dataset[\"Age\"][titanic_dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = titanic_dataset[\"Age\"].median()\n    age_pred = titanic_dataset[\"Age\"][((titanic_dataset['SibSp'] == titanic_dataset.iloc[i][\"SibSp\"]) & \n                                       (titanic_dataset['Parch'] == titanic_dataset.iloc[i][\"Parch\"]) & \n                                       (titanic_dataset['Pclass'] == titanic_dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        titanic_dataset['Age'].iloc[i] = age_pred\n    else :\n        titanic_dataset['Age'].iloc[i] = age_med","d75ba70d":"#Binning Age,Fare\ntitanic_dataset['Age']= pd.qcut(titanic_dataset['Age'], q=10)\ntitanic_dataset['Fare'] = pd.qcut(titanic_dataset['Fare'], q=13)","831bb141":"# Eature Extraction: Get Title from Name\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in titanic_dataset[\"Name\"]]\ntitanic_dataset[\"Title\"] = pd.Series(dataset_title)\ntitanic_dataset[\"Title\"].head()","caebc1bd":"#Creating Rare & Mrs\ntitanic_dataset[\"Title\"] = titanic_dataset[\"Title\"].replace(\n    ['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', \n     'Dona'], 'Rare').replace(['Miss','Ms','Mme','Mlle','Mrs'], 'Mrs')","61d9c234":"# Create a family size descriptor from SibSp and Parch\ntitanic_dataset[\"Fsize\"] = titanic_dataset[\"SibSp\"] + titanic_dataset[\"Parch\"] + 1\n\n# Create new feature of family size\ntitanic_dataset['Single'] = titanic_dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ntitanic_dataset['FS_S'] = titanic_dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ntitanic_dataset['FS_M'] = titanic_dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntitanic_dataset['FS_L'] = titanic_dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","d1d182de":"# Replace the Cabin number by the type of cabin 'X' if not\ntitanic_dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in titanic_dataset['Cabin'] ])","81453c1f":"## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n\nTicket = []\nfor i in list(titanic_dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ntitanic_dataset[\"Ticket\"] = Ticket\ntitanic_dataset[\"Ticket\"].head()","0c41fe46":"#Label Encoding the Non-Numerical Features\n#LabelEncoder basically labels the classes from 0 to n.\n\nnon_numeric_features = ['Embarked', 'Sex', 'Cabin', 'Title', 'Age', 'Fare']\n\nlabel_encoder = preprocessing.LabelEncoder() \nfor feature in non_numeric_features:        \n    titanic_dataset[feature] = label_encoder.fit_transform(titanic_dataset[feature])","0418d3d8":"#titanic_dataset1 = pd.DataFrame(to_categorical(titanic_dataset[\"Pclass\"]))\ntitanic_dataset = pd.get_dummies(titanic_dataset, columns = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Title'])","d8c26d38":"'''cat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Title']\nencoded_features = []\n\nfor feature in cat_features:\n    encoded_feat = OneHotEncoder().fit_transform(titanic_dataset[feature].values.reshape(-1, 1)).toarray()\n    n = titanic_dataset[feature].nunique()\n    cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n    encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n    encoded_df.index = titanic_dataset.index\n    encoded_features.append(encoded_df)\ntitanic_dataset=pd.concat([titanic_dataset, *encoded_features], axis=1)\n'''","3cd759e4":"# Drop useless variables \ntitanic_dataset.drop(labels = [\"PassengerId\",\"Ticket\",\"Name\",\"Fsize\",\"SibSp\",\"Parch\"], \n                     axis = 1, inplace = True)","46cda5d3":"titanic_dataset.head()","3b10f492":"## Separate train dataset and test dataset\n\ntrain = titanic_dataset[:train_len]\ntest = titanic_dataset[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)","7e9dcd48":"## Separate train features and label \nX_train = train.drop(labels = [\"Survived\"],axis = 1)\nY_train = train['Survived'].values\nX_test = test\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('Y_train shape: {}'.format(Y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))","c6d488bf":"from keras.optimizers import SGD\n\nmodel = Sequential()\nmodel.add(Dense(27, input_dim=27, kernel_initializer= 'normal' , activation= 'relu' ))\nmodel.add(Dense(14, kernel_initializer= 'normal' , activation= 'relu' ))\nmodel.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' ))\n\n# Compile model\nsgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\nmodel.compile(loss= 'binary_crossentropy' , optimizer='sgd', metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train, Y_train, epochs=150, batch_size=10, verbose=0)","9c4b9e38":"predictions = model.predict(X_test)\npredict= pd.DataFrame(predictions)","3b8532ff":"predict[predict < 0.5] = 0\npredict[predict >= 0.5] = 1\npredict.columns = ['Survived']","871dd052":"#id=titanic_test['PassengerId'].reset_index(drop=True, inplace=True)\n#print(titanic_test['PassengerId'])\noutput = pd.concat([titanic_test['PassengerId'],predict['Survived'].astype('int') ], axis=1)\noutput.to_csv('titanic-predictions.csv', index = False)\noutput.head(100)","6399f837":"Build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc)"}}