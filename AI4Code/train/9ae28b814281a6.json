{"cell_type":{"1c59f929":"code","b023d4dd":"code","ac02320b":"code","d3682a9b":"code","d428614f":"code","a2b420c1":"code","7511890b":"code","be1e0d81":"code","046751f4":"code","f2ced2a3":"code","2fb291ad":"code","c0bf1604":"code","e9c6f86c":"code","1dcded54":"code","4e8a11cd":"code","29e83093":"code","2b13b389":"code","1c80b5c4":"code","03b8b221":"code","988ee597":"code","95c76efd":"code","3b773bb6":"code","c1f23ca9":"code","53e44a4b":"code","fcd054c8":"code","e8b79e23":"code","53ce9d56":"code","e297335f":"code","2dac20ee":"code","88c34525":"code","63ddc46d":"code","2a4a7431":"code","555d2136":"code","0c7249e5":"code","5f086ef3":"code","7d3ba303":"code","cc72ca64":"code","3fddb535":"code","1543c6de":"markdown"},"source":{"1c59f929":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b023d4dd":"import pandas as pd\nP39_Financial_Data = pd.read_csv(\"..\/input\/P39-Financial-Data.csv\")","ac02320b":"P39_Financial_Data ","d3682a9b":"# predict if esign or not\n\n#pay schedule - how often would be paid\n#home owner if 1\n#curr adress year- how many years at curr adress\n# personal account m y - months had personal account or years\n# amount requested\n# has debt -- 1 is true\n# bunch of risk scores - non normalized --> will userpays loan\n# then normalize risk scores","d428614f":"# EDA\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sn\n\n\ndataset = P39_Financial_Data ","a2b420c1":"dataset.head","7511890b":"dataset.columns","be1e0d81":"dataset.describe()","046751f4":"# clean the data - any columns have na? no, so no need to deal\ndataset.isna().any()","f2ced2a3":"\"\"\"\n\n# not workign great-can fix later\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sn\n\n\n\n# histograms - make sure columns are plottable - no categoricals (still can plot), and no NAs\ndataset2 = dataset.drop(columns = ['entry_id', 'pay_schedule', 'e_signed'])\n#dataset2.columns\n\n\n#fig = plt.figure(figsize = )\n\nplt.suptitle('histogram', fontsize=20)\n\n#iterate all features\nfor i in range(1, dataset2.shape[1] + 1):\n    \n    \n    # generate a subplot witha given index\n    plt.subplot(2,2,i)\n    \n    # what is this?!\n    f = plt.gca()\n    \n    #get name of column at a given value\n    f.set_title(dataset2.columns.values[i-1])\n    \n    # Why getting the unique values? \n    # so if vals>100, then vals = 100. cap number of bins\n    vals = np.size(dataset2.iloc[:, i-1].unique())\n    \n    # plot histogram where i subset data, all rows at index i-1 (why not name --> easier to use), and number of bins is equal to num of uniques\n    # is this a standard method?!\n    plt.hist(dataset2.iloc[:, i-1], bins=vals, color = '#3F5D7D')\n    \n    plt.show()\n    \n    \"\"\"","2fb291ad":"# #plt.subtitle('histogram', fontsize=20)\n# dataset2.shape[1] #get number of columns\n# dataset2.columns.values[1]\n# #np.size(dataset2.iloc[:, 1].unique())\n# dataset2.iloc[:, 2].unique() #get the array pandas of the column\n# np.size(dataset2.iloc[:, 3].unique()) #get number of unique values","c0bf1604":"\n\n# my version\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sn\n\n\n# drop some of the variables\n# histograms - make sure columns are plottable - no categoricals (still can plot), and no NAs\ndataset2 = dataset.drop(columns = ['entry_id', 'pay_schedule', 'e_signed'])\n#dataset2.columns\n\n\n# suptitle things\nplt.suptitle('histogram', fontsize=20)\n\n\n\n\n#iterate all features: from 1 to rest\nfor i in range(0, dataset2.shape[1]):\n    \n    \n    # so can iterate all the cols. now per each col do a histogram witha title that is pulled from it\n    #print column\n    print(dataset2.columns.values[i])\n    \n    #do histogram of column we are iterating\n    plt.hist(dataset2.iloc[:, i])\n    \n    plt.show()\n    \n    \n    ","e9c6f86c":"# corr plot and amtrix - do after!","1dcded54":"dataset.columns","4e8a11cd":"# data reprocessing \n\n#remove months employed-issue - some users 0 mistakenly\n#dataset = dataset.drop(columns = ['months_employed'])\n\n# feature engineering\ndataset['personal_account_months'] = dataset.personal_account_m + (dataset.personal_account_y * 12)\ndataset","29e83093":"dataset.columns","2b13b389":"# preprocess data - hot encode and scale data\n\n# one hot encode\ndataset2 = pd.get_dummies(dataset)\ndataset2.columns\n#dataset2\n#dataset = dataset.drop(columns = ['pay_schedule_semi-monthly'])","1c80b5c4":"dataset2","03b8b221":"dataset3.dtypes","988ee597":"# put aside\nresponse = dataset[\"e_signed\"]\nusers = dataset['entry_id']\n\n#drop reponse, pay_schedule, and the ids --> how to reconcile IDs?\ndataset3 = dataset.drop(columns = ['e_signed', 'entry_id', 'pay_schedule'])\ndataset3","95c76efd":"# split train test\nfrom sklearn.model_selection import train_test_split\n\n# have train and test -- and response variable is separate\nX_train, X_Test, y_train, y_test = train_test_split(dataset3, response, test_size = 0.2, random_state = 0)\n\n\n# feature scale\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\n\n#fit scaler to X_Train - column names lost - until i conert to df\nX_Train2 = pd.DataFrame(sc_X.fit_transform(X_train))\nX_Test2 = pd.DataFrame(sc_X.fit_transform(X_Test))\n\n#map it all back\nX_Train2.columns = X_train.columns.values\nX_Test2.columns = X_Test.columns.values\n\n# reset index - why?! can I not get it other way\nX_Train2.index = X_train.index.values\nX_Test2.index = X_Test.index.values\n\nX_Train2\n#X_Train2.columns = X_train.columns.values","3b773bb6":"from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n\n# function to get accuracy scores\n\ndef errors(y_test, y_pred, model_type):\n    \n    # accuracy\n    acc = accuracy_score(y_test, y_pred)\n\n    #precision - \n    prec = precision_score(y_test, y_pred)\n\n    #recall - biased model\n    rec = recall_score(y_test, y_pred)\n\n    #f1\n    f1 = f1_score(y_test, y_pred)\n\n    #put to pandas\n    errors = pd.DataFrame([[model_type, acc, prec, rec, f1]], columns = ['Model', 'accuracy', 'precision', 'recall', 'f1 score'])\n\n    return errors","c1f23ca9":"# model building\n\n# logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\n#lasso - penalizes if too much of a coefficient ona variable\nclassifier = LogisticRegression(random_state = 0, penalty = 'l1')\n\n# fit classifier to train data --> fits the classifier object\nclassifier.fit(X_Train2, y_train)\n\n# predicting test set - want to see accuracy\ny_pred = classifier.predict(X_Test2)\n\n\nerror_logistic = errors(y_test, y_pred, 'logistic')\nerror_logistic","53e44a4b":"\"\"\"\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n\n\n# accuracy\nacc = accuracy_score(y_test, y_pred)\n\n#precision - \nprec = precision_score(y_test, y_pred)\n\n#recall - biased model\nrec = recall_score(y_test, y_pred)\n\n#f1\nf1 = f1_score(y_test, y_pred)\n\n#put to pandas\nerrors = pd.DataFrame([['Linear Regression (lasso)', acc, prec, rec, f1]], columns = ['Model', 'accuracy', 'precision', 'recall', 'f1 score'])\n\n\"\"\"","fcd054c8":"# let's do more models and see which one works the best\n\n\n# model building\n\n# SVC\nfrom sklearn.svm import SVC\n\n#lasso - penalizes if too much of a coefficient ona variable\nclassifier = SVC(random_state = 0, kernel = 'linear')\n\n# fit classifier to train data --> fits the classifier object\nclassifier.fit(X_Train2, y_train)\n\n# predicting test set - want to see accuracy\ny_pred = classifier.predict(X_Test2)\n\nerror_svc = errors(y_test, y_pred, 'SVC')\nerror_svc","e8b79e23":"# model building\n\n# SVC\nfrom sklearn.svm import SVC\n\n#lasso - penalizes if too much of a coefficient ona variable\nclassifier = SVC(random_state = 0, kernel = 'rbf')\n\n# fit classifier to train data --> fits the classifier object\nclassifier.fit(X_Train2, y_train)\n\n# predicting test set - want to see accuracy\ny_pred = classifier.predict(X_Test2)\n\nerror_svc_rbf = errors(y_test, y_pred, 'SVC_rbf')\nerror_svc_rbf","53ce9d56":"# model building\n\n# random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n#lasso - penalizes if too much of a coefficient ona variable\nclassifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state=0)\n\n# fit classifier to train data --> fits the classifier object\nclassifier.fit(X_Train2, y_train)\n\n# predicting test set - want to see accuracy\ny_pred = classifier.predict(X_Test2)\n\nerror_rf = errors(y_test, y_pred, 'rf')\nerror_rf","e297335f":"# append for comparison, have to reset idnex too - can see svc bit better - rf is best one\nappended_error = error_svc.append([error_logistic, error_svc_rbf, error_rf], ignore_index=True)\nappended_error","2dac20ee":"# do cross validation - k fold --- can be used in grid search\nfrom sklearn.model_selection import cross_val_score\n\n# cross validate the classifier X_train\naccuracies = cross_val_score(estimator = classifier, X=X_train, y=y_train, cv=10)\n\n#average accuracy is 62% --> where we batch and try --> we guarantee that model is consistent\nprint(accuracies.mean())\nprint(accuracies.std() * 2)","88c34525":"# grid search\n\n","63ddc46d":"# aside - learning stuff","2a4a7431":"#matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.arange(1,5)\ny = x**3\n\n# plotting 2 arrays\nplt.plot([1,2,3,4], [1,4,9,16], \"go\", x, y, \"r^\")\nplt.title(\"First plot\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","555d2136":"x = np.arange(1,5)\ny = x**3\n\n# subplot 1 returned\nplt.subplot(1,2,1) #figure 1 returned\nplt.plot([1,2,3,4], [1,4,9,16], \"go\")  #plot attached to figure 1\n\nplt.subplot(1,2,2)\nplt.plot( x, y, \"r^\")","0c7249e5":"#bar chart\n# things are really layered on in a way\n\n\ndivisions = [\"a\", \"b\", \"c\"]\n\n# plotting stuff for a given coordinate - what will be what\ngirls = [1,2,3]\nboys = [5,6,7]\n\nindex = np.arange(3)\nwidth = 0.2\n\nplt.bar(index, girls, width)\nplt.bar(index + width, boys, width)\n\n#I give coordinates to all and divisions array are the xticks that will be used\nplt.xticks(index + width\/2, divisions)\n\n\nplt.show()","5f086ef3":"x = np.random.randn(1000)\n\nplt.title(\"x\")\nplt.hist(x)","7d3ba303":"# figure object created\nfig = plt.figure()\n\n# call suptitle method\nfig.suptitle('abc')\n\n#fig.show()\n\n# generate subplots\nplt.subplots(nrows = 2, ncols = 2)","cc72ca64":"import numpy as np\n\n\nt = np.arange(0, 5, 0.2)\nplt.plot(t,t, 'r--', t, t**2, 'bs', t, t**3, 'g^')\nplt.show()","3fddb535":"x = np.arange(0, 5, 0.2)\ny = np.arange(0, 5, 0.2)\n\n\n\n#working off one instance of pyplot - call methods on it which return visual object which are overlaid on top each other\n#plt.figure is implicitly created for line2D object\nplt.figure()\n\n# return line2D object - it is overlaid on result of previous method that was called\nplt.plot(x,y,linewidth=2.0, animated = False, color = 'red')\n#plt.show()","1543c6de":"start with likelyhood of esigning, then productionize model course then kaggle for it deconstruct. Then reconstruct in new notebook."}}