{"cell_type":{"57ea24e1":"code","69bf54b1":"code","2f39dfed":"code","d4d80f58":"code","8a67f818":"code","59dc4b9f":"code","dd1c3993":"code","a14b59da":"code","5ffd6c3e":"code","6efd871d":"code","c7ace874":"code","32a21946":"code","0b1327cd":"code","f6e8b60a":"code","1ba77a7b":"code","d7b56926":"code","ad9421ca":"code","348a8ece":"code","4b6b7dff":"code","f4364e29":"code","ad16669f":"code","c72913cc":"code","3a7fa149":"code","f3043645":"code","b780ab4d":"code","2d6d9325":"code","ed9c0a6c":"code","13c371ed":"code","3cef8eaa":"code","861828e9":"code","3c115599":"code","ba4f6d3f":"code","875efe86":"code","2568a311":"code","1e2a19ab":"code","8ddbb46a":"code","3be86c64":"code","50ac8421":"code","58595963":"code","b264b6c3":"code","2e3fa8f3":"code","cbb39c59":"code","7ba1c60d":"code","0f3018c7":"code","de2d9d2e":"code","54704427":"code","05444e6c":"code","0ffce826":"code","648755d4":"code","7cec4488":"code","679f2d82":"code","1cd3020f":"code","f6654b5d":"code","ba019992":"code","0500dc73":"code","393b7db0":"code","abcf35e9":"code","a9f458bd":"markdown","ad18c3f4":"markdown","de3059b4":"markdown","fa0140f3":"markdown","a6e2b322":"markdown","89bdb7de":"markdown","3fc40514":"markdown","1643a93a":"markdown","dca72944":"markdown","a49bffd1":"markdown","a5228979":"markdown","b236a61e":"markdown","bd6c99b1":"markdown","1878358b":"markdown","b21eba6b":"markdown","7db7cf34":"markdown","74cf4582":"markdown","534f5b0d":"markdown","8362c430":"markdown","9d6c4868":"markdown","9ba062bc":"markdown","67ba0cdb":"markdown","21a1a87b":"markdown","124c2ea1":"markdown","e0a99f50":"markdown","55252a1a":"markdown","bde64550":"markdown","6900f1b2":"markdown","71dd5b7e":"markdown","50752b1a":"markdown","22097175":"markdown","72311690":"markdown","8c771ed4":"markdown","95f68b8d":"markdown","16b66bb9":"markdown","a300f5ea":"markdown","3d9ba388":"markdown","57dd8efc":"markdown","824a5ead":"markdown","f50958ad":"markdown","caf7d717":"markdown","17165f70":"markdown","10a909bf":"markdown","51050a39":"markdown","359f0efc":"markdown","65c9c6a4":"markdown","ea9c3958":"markdown","f6e07354":"markdown","e6cfedc3":"markdown","e7f6a59a":"markdown","f745581d":"markdown","4629f2db":"markdown"},"source":{"57ea24e1":"# Load Libraries:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport plotly.offline as pyo \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n#\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n#\nfrom sklearn.preprocessing import RobustScaler,StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import clone\n#\nimport xgboost as xgb\n#\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","69bf54b1":"data = pd.read_csv(\"\/kaggle\/input\/autompg-dataset\/auto-mpg.csv\")\ndata.head()","2f39dfed":"# row\nprint(\"row count:\",data.shape[0])\n# columns\nprint(\"column count:\",data.shape[1])","d4d80f58":"data.info()","8a67f818":"data.describe().T","59dc4b9f":"# But there are \"?\", (Missing Attribute Values: horsepower has 6 missing values)\n#Just we did not see here\ndata.isnull().sum()","dd1c3993":"data.cov()","a14b59da":"data.corr()","5ffd6c3e":"sns.pairplot(data,markers=\"*\");\nplt.show()","6efd871d":"data[\"origin\"].value_counts()","c7ace874":"data[\"origin\"].value_counts(normalize=True)","32a21946":"colors = ['#f4cb42', '#cd7f32', '#a1a8b5'] #gold,bronze,silver\n#\norigin_counts = data[\"origin\"].value_counts(sort=True)\nlabels = [\"USA\", \"Europe\",\"Japan\"]\nvalues = origin_counts.values\n#\npie = go.Pie(labels=labels, values=values, marker=dict(colors=colors))\nlayout = go.Layout(title='Origin Distribution')\nfig = go.Figure(data=[pie], layout=layout)\npyo.iplot(fig)","0b1327cd":"trace0 = [go.Bar(x=data[\"model year\"]+1900,y=data[\"mpg\"],\n                   marker=dict(color=\"rgb(17,77,117)\"),\n                   name=\"Total\")]\n\nlayout = go.Layout(title=\"Consumption Gallon by Years\",barmode=\"stack\")\nfig = go.Figure(data=trace0,layout=layout)   \npyo.iplot(fig) ","f6e8b60a":"trace1 = [go.Scatter(x=data[\"horsepower\"], y=data[\"weight\"],\n                   text=data[\"car name\"],\n                   mode=\"markers\",\n                   marker=dict(size=2*data[\"cylinders\"],\n                               color=data[\"cylinders\"],\n                               showscale=True))]\n \nlayout = go.Layout(title=\"Relation Horse Power & Weight\",\n                   xaxis=dict(title=\"Horse Power\"),\n                   yaxis=dict(title=\"Weight\"),\n                   hovermode=\"closest\")\nfig = go.Figure(data=trace1,layout=layout)\npyo.iplot(fig)","1ba77a7b":"trace2 = [go.Histogram(x=data.mpg,\n                         xbins=dict(start=0,end=50))]\nlayout = go.Layout(title=\"MPG\")\n\nfig = go.Figure(data=trace2,layout=layout)\npyo.iplot(fig)","d7b56926":"trace3 = [go.Box(y=data[\"mpg\"],name=data.columns[0]),\n          go.Box(y=data[\"cylinders\"],name=data.columns[1]),\n          go.Box(y=data[\"displacement\"],name=data.columns[2]),\n          go.Box(y=data[\"horsepower\"],name=data.columns[3]),\n          go.Box(y=data[\"weight\"],name=data.columns[4]),\n          go.Box(y=data[\"acceleration\"],name=data.columns[5]),\n          go.Box(y=data[\"origin\"],name=data.columns[7])]\n\npyo.iplot(trace3)","ad9421ca":"# I choice Germany instead of Euro\ncountry_number = pd.DataFrame(index=[\"USA\",\"DEU\",\"JPN\"],columns=[\"number\",\"country\"])\ncountry_number[\"country\"] = [\"United States\",\"Germany\",\"Japan\"]\ncountry_number[\"number\"] = [249,79,70]","348a8ece":"country_number","4b6b7dff":"worldmap = [dict(type = 'choropleth', locations = country_number['country'], locationmode = 'country names',\n                 z = country_number['number'], autocolorscale = True, reversescale = False, \n                 marker = dict(line = dict(color = 'rgb(180,180,180)', width = 0.5)), \n                 colorbar = dict(autotick = False, title = 'Number of athletes'))]\n\nlayout = dict(title = 'Distribution of Data', geo = dict(showframe = False, showcoastlines = True, \n                                                                projection = dict(type = 'Mercator')))\n\nfig = dict(data=worldmap, layout=layout)\npyo.iplot(fig, validate=False)","f4364e29":"data[\"horsepower\"] = data[\"horsepower\"].replace(\"?\",\"100\")\ndata[\"horsepower\"] = data[\"horsepower\"].astype(float)","ad16669f":"threshoold       = 2\nhorsepower_desc  = data[\"horsepower\"].describe()\nq3_hp            = horsepower_desc[6]\nq1_hp            = horsepower_desc[4]\nIQR_hp           = q3_hp - q1_hp\ntop_limit_hp     = q3_hp + threshoold * IQR_hp\nbottom_limit_hp  = q1_hp - threshoold * IQR_hp\nfilter_hp_bottom = bottom_limit_hp < data[\"horsepower\"]\nfilter_hp_top    = data[\"horsepower\"] < top_limit_hp\nfilter_hp        = filter_hp_bottom & filter_hp_top","c72913cc":"data = data[filter_hp]\ndata.shape","3a7fa149":"data.columns","f3043645":"acceleration_desc  = data[\"acceleration\"].describe()\nq3_acc             = acceleration_desc[6]\nq1_acc             = acceleration_desc[4]\nIQR_acc            = q3_acc - q1_acc\ntop_limit_acc      = q3_acc + threshoold * IQR_acc\nbottom_limit_acc   = q1_acc - threshoold * IQR_acc\nfilter_acc_bottom  = bottom_limit_acc < data[\"acceleration\"]\nfilter_acc_top     = data[\"acceleration\"] < top_limit_acc\nfilter_acc         = filter_acc_bottom & filter_acc_top","b780ab4d":"data = data[filter_acc]\ndata.shape","2d6d9325":"f,ax = plt.subplots(figsize = (20,7))\nsns.distplot(data.mpg, fit=norm);\n# we see that, data[\"mpg\"] has positive skewness","ed9c0a6c":"(mu,sigma) = norm.fit(data[\"mpg\"])\nprint(\"mu:{},sigma:{}\".format(mu,sigma))","13c371ed":"# qq plot:\nplt.figure(figsize = (20,7))\nstats.probplot(data[\"mpg\"],plot=plt)\nplt.show\nprint(\"We expect that our data points will be on red line for gaussian distributin. We see dist tails\")","3cef8eaa":"data[\"mpg\"] = np.log1p(data[\"mpg\"])","861828e9":"f,ax = plt.subplots(figsize = (20,7))\nsns.distplot(data[\"mpg\"], fit=norm);","3c115599":"# Let's other skewness of features\n# if skew > 1  : positive skewness\n# if skew > -1 : negative skewness\n\nfeatures = ['cylinders', 'displacement', 'horsepower', 'weight','acceleration','origin']\nskew_list = []\nfor i in range(0,6):\n    skew_list.append(skew(data.iloc[:,i]))\n# So, features are good at skewness \nskew_list","ba4f6d3f":"data[\"origin\"] = data[\"origin\"].astype(str)\ndata[\"cylinders\"] = data[\"cylinders\"].astype(str)","875efe86":"data.drop([\"car name\"],axis=1,inplace=True)","2568a311":"# One Hot Encoding - 1\ndata = pd.get_dummies(data,drop_first=True)","1e2a19ab":"# I decide to use model year as a fuaure, so i will apply one hot encoding\ndata[\"model year\"] = data[\"model year\"].astype(str)","8ddbb46a":"# One Hot Encoding - 2\ndata = pd.get_dummies(data,drop_first=True)","3be86c64":"data.info()","50ac8421":"y = data[\"mpg\"]\nx = data.drop([\"mpg\"],axis=1)","58595963":"# Creating Train and Test Datasets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.90, random_state=42)","b264b6c3":"# Scale the data to be between -1 and 1\n# Mean= 0\n# Std = 1\nsc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.transform(x_test)","2e3fa8f3":"lr = LinearRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nprint(\"LR Coef:\",lr.intercept_)\nprint(\"LR Coef:\",lr.coef_)\nmse = mean_squared_error(y_test,y_pred)\nprint(\"MSE\",mse)","cbb39c59":"ridge = Ridge(random_state=42, max_iter=10000)\nalphas = np.logspace(-4,-0.5,30)\ntuned_parameters = dict(alpha=alphas)","7ba1c60d":"clf = GridSearchCV(ridge,tuned_parameters,cv=5,scoring=\"neg_mean_squared_error\", refit=True)\nclf.fit(X_train,y_train)\nscores = clf.cv_results_[\"mean_test_score\"]\nscores_std = clf.cv_results_[\"std_test_score\"]","0f3018c7":"print(\"Ridge Coef:\",clf.best_estimator_.coef_)\nridge = clf.best_estimator_\nprint(\"Ridge Best Estimator:\",ridge)","de2d9d2e":"y_pred_ridge = clf.predict(X_test)\nmse_ridge = mean_squared_error(y_test,y_pred_ridge)\nprint(\"Ridge MSE:\",mse_ridge)","54704427":"lasso = Lasso(random_state=42, max_iter=10000)\nalphas = np.logspace(-4,-0.5,30)\ntuned_parameters = dict(alpha=alphas)","05444e6c":"clf = GridSearchCV(lasso,tuned_parameters,cv=5,scoring=\"neg_mean_squared_error\", refit=True)\nclf.fit(X_train,y_train)\nscores = clf.cv_results_[\"mean_test_score\"]\nscores_std = clf.cv_results_[\"std_test_score\"]","0ffce826":"print(\"Lasso Coef:\",clf.best_estimator_.coef_)\nlasso = clf.best_estimator_\nprint(\"Lasso Best Estimator:\",lasso)\nprint(\"Put Zero for redundat features:\")","648755d4":"y_pred_lasso = clf.predict(X_test)\nmse_lasso = mean_squared_error(y_test,y_pred_lasso)\nprint(\"Lasso MSE:\",mse_lasso)","7cec4488":"parameters = dict(alpha=alphas,l1_ratio=np.arange(0.0,1,0.05))\neNet = ElasticNet(random_state=42, max_iter=10000)\nclf = GridSearchCV(eNet,tuned_parameters,cv=5,scoring=\"neg_mean_squared_error\", refit=True)\nclf.fit(X_train,y_train)","679f2d82":"print(\"Lasso Coef:\",clf.best_estimator_.coef_)\neNet = clf.best_estimator_\nprint(\"Lasso Best Estimator:\",eNet)","1cd3020f":"y_pred_eNet = clf.predict(X_test)\nmse_eNet = mean_squared_error(y_test,y_pred_eNet)\nprint(\"Lasso MSE:\",mse_eNet)","f6654b5d":"technices = [\"Ridge\",\"Lasso\",\"ElasticNet\"]\nresults   = [0.01822,0.01844,0.01813]","ba019992":"fig = go.Figure(data=[go.Bar(\n            x=technices, y=results,\n            text=results,\n            textposition='auto',)])\nfig.show()","0500dc73":"# objective: aim\n# n_estimator: number of trees\nmodel_xgb = xgb.XGBRegressor(objective=\"reg:linear\",max_depth=5,min_child_weight=4,subsample=0.7,n_estimator=1000,learning_rate=0.07)\nmodel_xgb.fit(X_train,y_train)","393b7db0":"y_pred_xgb = model_xgb.predict(X_test)\nmse_xgb = mean_squared_error(y_test,y_pred_xgb)\nprint(\"XGBOOST MSE:\",mse_xgb)","abcf35e9":"class AveragingModels():\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    # the reason of clone is avoiding affect the original base models\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]  \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([model.predict(X) for model in self.models_])\n        return np.mean(predictions, axis=1)\n\naveraged_models = AveragingModels(models = (model_xgb,lr))\naveraged_models.fit(X_train,y_train)\ny_pred_averaged_models = averaged_models.predict(X_test)\nmse_averaged_models = mean_squared_error(y_test,y_pred_averaged_models)\nprint(\"Averaging Models MSE:\",mse_averaged_models)","a9f458bd":"<a id = \"6\"><\/a><br>\n## Pie Chart\n* Display the percentages of Data Origin","ad18c3f4":"* mpg.mean = 23.514573\n* mpg.median(50%) = 23.0\n* So we can not say this is a \"Normal Distribution\"\n","de3059b4":"### What did happen?","fa0140f3":"<a id = \"4\"><\/a><br>\n## Descriptive Analysis","a6e2b322":"<a id = \"5\"><\/a><br>\n## Data Visualization","89bdb7de":"### Describe of Data\nWe explore :\n* Count\n* Mean\n* Std\n* Quartiles\n* Max","3fc40514":"<a id = \"12\"><\/a><br>\n## Detection Outliers","1643a93a":"* Attribute Information:\n\n* mpg: continuous\n* cylinders: multi-valued discrete (3-8)\n* displacement: continuous\n* horsepower: continuous\n* weight: continuous\n* acceleration: continuous\n* model year: multi-valued discrete (1970-1982)\n* origin: multi-valued discrete (USA - Japan - Euro)\n* car name: string (unique for each instance)","dca72944":"<a id = \"19\"><\/a><br>\n## Regularization Technics\n* This technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.\n* Minimize Sum Of Squared Errors","a49bffd1":"<a id = \"21\"><\/a><br>\n## 2- Lasso(L1)\n* This technic give zero for redundant features\n* Use this technic for feature selection","a5228979":"<a id = \"15\"><\/a><br>\n## Train and Test Split","b236a61e":"### How can we decide data is gaussian or not?\n*  1-Histogram\n*  2-We can understand Quantile Quatile plot ","bd6c99b1":"* I will apply One Hot Encoding for data[\"origin\"]  and data[\"cylinders\"]\n\n   because these features have categorical values.\n","1878358b":"<a id = \"18\"><\/a><br>\n## Linear Regression","b21eba6b":"<a id = \"7\"><\/a><br>\n## Bar Plot","7db7cf34":"## Covariance \n* Covariance is a measure of how much two random variables vary together.\n\n* If two variables are independent, their covariance is 0. However, a covariance of 0 does not imply that the variables are independent.\n* (+) : positive relation\n* (-) : negtive relation","74cf4582":"<a id = \"20\"><\/a><br>\n## 1- Ridge(L2)","534f5b0d":"First, convert to object","8362c430":"<a id = \"25\"><\/a><br>\n## Model Averaging by XGB and Linear Regression","9d6c4868":"<a id = \"23\"><\/a><br>\n## Compare Technics via Bar Plot\n* As we see eNet gives the best result","9ba062bc":"<a id = \"3\"><\/a><br>\n## Load Dataset","67ba0cdb":"### Info of Data\nWe explore :\n* Data Types\n* Row Count\n* Column Count\n* Is There Missing Values","21a1a87b":"## Observe for Dependent Feature ","124c2ea1":"<a id = \"2\"><\/a><br>\n## Load Libraries","e0a99f50":"## Correlation \n* Correlation is a standardized version of covariance.\n* Between -1 and 1\n* Zero = Not correlated\n* (+) : positive relation\n* (-) : negtive relation","55252a1a":"As Percentage","bde64550":"<a id = \"22\"><\/a><br>\n## ElasticNet\n* This technic takes positive ways of L1 and L2","6900f1b2":"## Distribution of Countries\n*  1 : USA\n*  2 : Eurepe\n*  3 : Japan","71dd5b7e":"<a id = \"16\"><\/a><br>\n## Standardization","50752b1a":"## Missing Value Control","22097175":"<a id = \"13\"><\/a><br>\n## Feature Engineering - 1","72311690":"<a id = \"10\"><\/a><br>\n## Box Plot\nWe will display statistics\n\n* A box plot is a graphical method to summarize a data set by visualizing the minimum value, 25th percentile, median, 75th percentile, the maximum value, and potential outliers. A percentile is the value below which a certain percentage of data fall. For example, if 75% of the observations have values lower than 685 in a data set, then 685 is the 75th percentile of the data. At the 50th percentile, or median, 50% of the values are lower and 50% are higher than that value.","8c771ed4":"<a id = \"17\"><\/a><br>\n## Regression Models","95f68b8d":"### Skewness\n* In the tails, our points can be outliers, so we should make them gaussian distirbution","16b66bb9":"### Shape of Data","a300f5ea":"### Summary Statistics\n* Mean : Balance Point\n* Median : Middle Value \"when ordered\"\n* Variance : The average of the squared distance of the mean\n* Standard Deviation : The square root of the variance\n* Skewness : A measure that describes the contrast of one tail versus the other tail. For example, if there are more high values in your distribution than low values then your distribution is 'skewed' towards the high values.\n* Kurtosis : A measure of how 'fat' the tails in the distribution are.","3d9ba388":"<a id = \"26\"><\/a><br>\n## Conclusion\n* Average Models gave the best score for our dataset.","57dd8efc":"## Log Transformations\n* The log transformation can be used to make highly skewed distributions less skewed.\n* http:\/\/onlinestatbook.com\/2\/transformations\/log.html","824a5ead":"![](https:\/\/i1.wp.com\/alevelmaths.co.uk\/wp-content\/uploads\/2019\/02\/Skewness_1.png?w=761&ssl=1)","f50958ad":"## Let's see in the plot\n* We see in the plot\n    * clinders and origin can be categorical feature\n    * and also explore about correlations between features","caf7d717":"### Outlier for Horsepower","17165f70":"<a id = \"11\"><\/a><br>\n## Choropleth\nLet's determine how many origins have ","10a909bf":"# Introduction[](http:\/\/)\n * This kernel contains Prediction of Fuel Efficiency\n * If you like my kernel, please upvote.\n \n<hr>\n * You will learn:\n   * Plotly\n   * Drop Outliers\n   * Regularization Technics\n   * Model Average\n   \n<hr>\n<br>\n<br>\n<font color = 'blue'>\n<b>Content: <\/b>\n\n1. [Prepare Problems](#1)\n    * [Load Libraries](#2)\n    * [Load Dataset](#3)    \n1. [Descriptive Analysis](#4)\n1. [Data Visualization](#5)\n    * [Pie Chart](#6)\n    * [Bar Plot](#7)\n    * [Bubble Plot](#8)\n    * [Histogram](#9)\n    * [Box Plot](#10)\n    * [Choropleth](#11)\n1. [Detection Outliers](#12)\n1. [Feature Engineering - 1](#13)\n1. [Feature Engineering - 2](#14)\n1. [Train and Test Split](#15)\n1. [Standardization](#16)\n1. [Regression Models](#17)\n    * [Linear Regression](#18)\n1. [Regularization Technics](#19)\n    * [Ridge(L2)](#20)\n    * [Lasso(L1)](#21)\n    * [ElasticNet](#22) \n    * [Compare Technis via Bar Plot](#23) \n1. [XGBOOST](#24)\n1. [Model Averaging by XGB and Linear Regression](#25)\n1. [Conclusion](#26)","51050a39":"### Outlier for Acceleration","359f0efc":"![](https:\/\/lumileds.studysixsigma.com\/wp-content\/uploads\/sites\/14\/2016\/03\/anat-1.png)","65c9c6a4":"<a id = \"9\"><\/a><br>\n## Histogram","ea9c3958":"IQR = Q3 - Q1","f6e07354":"<a id = \"14\"><\/a><br>\n## Feature Engineering - 2","e6cfedc3":"## Let's other features of Skewness\n* I will ignore if they are close to -1 or 1","e7f6a59a":"<a id = \"24\"><\/a><br>\n## XGBOOST","f745581d":"<a id = \"1\"><\/a><br>\n## Prepare Problems\nThis kernel contains Prediction of Fuel Efficiency","4629f2db":"<a id = \"8\"><\/a><br>\n## Bubble Plot\nWe will make circles of different sizes depending on the Cylinders"}}