{"cell_type":{"56766529":"code","4f60a596":"code","8a91b53f":"code","e867f741":"code","d34aef8f":"code","462e7a32":"code","4a6e79b0":"code","9e8b6b5c":"code","f29c6144":"code","f1529e84":"code","4317cde0":"code","11a8d27d":"code","9e927e1f":"code","d6e53fea":"code","28698ffb":"code","424e3c79":"code","bfc15b9e":"code","1b9da833":"code","afb41805":"code","12f3a5a7":"code","c4256192":"code","3e103c71":"code","7f2ef273":"code","eeee4dca":"code","7f880974":"code","0f6a2738":"code","3d64d4b4":"code","76e3c49f":"code","eaacfec9":"code","e42e6262":"code","e870e7b2":"code","426e545b":"code","bab67e37":"code","ff7c91c6":"code","22496c52":"code","87066516":"code","dee0365d":"code","a520692a":"code","9a062843":"code","ae6b015e":"code","3fffe4e4":"code","644be475":"code","640644f4":"code","18cc2689":"code","83e20357":"code","aeb94961":"code","f8330a43":"code","21a4ea36":"code","40deeb3d":"code","ee54066a":"code","d54e73b5":"code","84587e44":"code","a482be91":"code","2c18c127":"code","00215b69":"code","eeb2f638":"code","ff105cf0":"code","7fd6aa31":"code","f34b0619":"code","bf5fd7c7":"code","13361f1d":"code","53030908":"code","cac51045":"code","51c29968":"code","8731e73c":"code","3e1ff793":"code","dfbc57c6":"code","52d9b4aa":"code","91217a80":"code","5c609973":"code","0d149aae":"code","26bb2b98":"code","9aae7de1":"code","8b1c5031":"code","2c4b00b6":"code","f9ce6e91":"code","bf2fe8cb":"code","3fa73666":"code","38c75ddd":"code","d96eabdb":"code","a224aade":"code","ec17b4c9":"code","08950d36":"code","f80264bb":"code","c5e24397":"code","5492c0bf":"code","da5b0526":"code","dbeceac8":"markdown","3c75fc3e":"markdown","7fb1ab0b":"markdown","632fc3dd":"markdown","a761cf9e":"markdown","f79049cb":"markdown","319cb328":"markdown","bbfbda39":"markdown","86e3e806":"markdown","18fcfc11":"markdown","7f97a56e":"markdown","57e80ce7":"markdown","a0415800":"markdown","a3cfb5d2":"markdown","2b7db98d":"markdown","b0015c28":"markdown","9839615a":"markdown","437f437b":"markdown","70bef8a4":"markdown","a91390cd":"markdown","08e63721":"markdown","1b217c43":"markdown","1838dada":"markdown","66b18522":"markdown","bc495b14":"markdown","b2827eb2":"markdown","d7b7757e":"markdown"},"source":{"56766529":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","4f60a596":"#export\n# from exp.nb_01 import *\n\ndef get_data():\n    path = datasets.download_data(MNIST_URL, ext='.gz')\n    with gzip.open(path, 'rb') as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n    return map(tensor, (x_train,y_train,x_valid,y_valid))\n\ndef normalize(x, m, s): return (x-m)\/s  #since we know about boardcasting we make a funktion that our tensor(x) subrat with the mean(m) and divided with standard diviation(s)\ndef test_eq(a,b): test(a,b,operator.eq,'==')","8a91b53f":"#export test if the floats are neer since we cant just compare floats with eachother \ndef near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\ndef test_near(a,b): test(a,b,near)","e867f741":"#export\nfrom pathlib import Path\nfrom IPython.core.debugger import set_trace\nfrom fastai import datasets\nimport pickle, gzip, math, torch, matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom torch import tensor\n\nMNIST_URL='http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl'","d34aef8f":"path = datasets.download_data(MNIST_URL, ext='.gz'); path","462e7a32":"with gzip.open(path, 'rb') as f:\n    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')","4a6e79b0":"x_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nn,c = x_train.shape\nx_train, x_train.shape, y_train, y_train.shape, y_train.min(), y_train.max()","9e8b6b5c":"# get the mean and standard diviation (std()) for the normalize function\ntrain_mean,train_std = x_train.mean(),x_train.std()\ntrain_mean,train_std #note we want then to be 0 or 1 ","f29c6144":"x_train = normalize(x_train, train_mean, train_std)\n# NB: Use training, not validation mean for validation set\nx_valid = normalize(x_valid, train_mean, train_std)","f1529e84":"train_mean,train_std = x_train.mean(),x_train.std()\ntrain_mean,train_std #note that after the normalizer we get it really close to 0 and 1","4317cde0":"#export # to test if the mean is near 0 and std is near 0 \ndef test_near_zero(a,tol=1e-3): assert a.abs()<tol, f\"Near zero: {a}\"","11a8d27d":"test_near_zero(x_train.mean())\ntest_near_zero(1-x_train.std()) #note it is usally 1 so we subrat 1 from the std, so it should be 0 now, when we test","9e927e1f":"n,m = x_train.shape #the size og the training set\nc = y_train.max()+1 #the number of activations we will need in our model\nn,m,c","d6e53fea":"# num hidden\nnh = 50","28698ffb":"# the below code is a simplified kaiming method ( init \/ he init )\nw1 = torch.randn(m,nh)\/math.sqrt(m) #randn = normal random numbers of size m(784) by nh(50), \n# \/math.sqrt(m) we do this because we want t.mean(),t.std() between 0 and 1 \nb1 = torch.zeros(nh)\nw2 = torch.randn(nh,1)\/math.sqrt(nh) #randn = normal random numbers of size nh(50) by 1\nb2 = torch.zeros(1)","424e3c79":"test_near_zero(w1.mean())\ntest_near_zero(w1.std()-1\/math.sqrt(m))","bfc15b9e":"# This should be ~ (0,1) (mean,std)...\nx_valid.mean(),x_valid.std()","1b9da833":"def lin(x, w, b): return x@w + b #this is not how our first layer are defined because first layer will have a relu","afb41805":"t = lin(x_valid, w1, b1)","12f3a5a7":"#...so should this, because we used kaiming init, which is designed to do this\nt.mean(),t.std()","c4256192":"def relu(x): return x.clamp_min(0.) #this means take our data (x) and replace any negative data with 0 ","3e103c71":"t = relu(lin(x_valid, w1, b1)) #this is our first layer ","7f2ef273":"#but unfornunally this does not give us a mean on 0 and a std on 1 and therefor we have to use a different technic \nt.mean(),t.std()","eeee4dca":"# kaiming init \/ he init for relu\nw1 = torch.randn(m,nh)*math.sqrt(2\/m)","7f880974":"w1.mean(),w1.std()","0f6a2738":"t = relu(lin(x_valid, w1, b1))\nt.mean(),t.std()","3d64d4b4":"#export\nfrom torch.nn import init","76e3c49f":"w1 = torch.zeros(m,nh)\ninit.kaiming_normal_(w1, mode='fan_out')  #this is the same as the above formular though with a forwards pass\nt = relu(lin(x_valid, w1, b1))","eaacfec9":"init.kaiming_normal_??","e42e6262":"w1.mean(),w1.std()","e870e7b2":"t.mean(),t.std()","426e545b":"w1.shape","bab67e37":"import torch.nn","ff7c91c6":"torch.nn.Linear(m,nh).weight.shape #this goes to show that ...","22496c52":"torch.nn.Linear.forward??","87066516":"torch.nn.functional.linear??","dee0365d":"torch.nn.Conv2d??","a520692a":"torch.nn.modules.conv._ConvNd.reset_parameters??","9a062843":"# what if...?\ndef relu(x): return x.clamp_min(0.) - 0.5 #take the data set and set all the negative to 0, and then subrat 0.5 ","ae6b015e":"# kaiming init \/ he init for relu \nw1 = torch.randn(m,nh)*math.sqrt(2.\/m )\nt1 = relu(lin(x_valid, w1, b1))\nt1.mean(),t1.std() #this shows that the mean and std is much better and closer to 0 and 1 ","3fffe4e4":"# a model in pytoch can just be a function, so this model is the forward pass  \ndef model(xb):\n    l1 = lin(xb, w1, b1) #one linear layer\n    l2 = relu(l1) #one relu (activation) layer\n    l3 = lin(l2, w2, b2) #and one more linear layer\n    return l3","644be475":"%timeit -n 10 _=model(x_valid) #9.45 ms is fast enoth","640644f4":"assert model(x_valid).shape==torch.Size([x_valid.shape[0],1]) #check if the shape is still the same ","18cc2689":"model(x_valid).shape # we can see the model have to more then a singel vector, so we have to remove the unit axsis \n# we do this by using squeeze ","83e20357":"#export\ndef mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() # you can also write squeeze(1) wich is the same axsis in this chase\n#so we take our output and subrat from target (targ) take it to the power of 2 and find the mean of that. This is the MSE (mean qsure error)","aeb94961":"y_train,y_valid = y_train.float(),y_valid.float()  #to use the loss function the data has to be floats ","f8330a43":"preds = model(x_train) #make a prediction ","21a4ea36":"preds.shape #the shape of our prediction","40deeb3d":"mse(preds, y_train) #use the above funktion for mse","ee54066a":"def mse_grad(inp, targ): \n    # gradient of loss function with respect to output of previous layer\n    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) \/ inp.shape[0]\n    #the mse is just the input squared subratet with the target and the derevitive of that is 2 times input minus target \n    #we have to store the gradient somewhere since for the chain rule, we have to multiply all the these things together (see above picture)\n    #so we can store it in the dot g (.g) of the previous layer. So the input of the mse is the same as the output of the previous layer","d54e73b5":"def relu_grad(inp, out):\n    # gradient of relu with respect to input activations. \n    inp.g = (inp>0).float() * out.g\n    #the above picture shows relu activation, where the gradient is either 1 or 0, so we can write (inp>0) so the input is greater then 0\n    #.float() to make it a float. though we still need to use the chain rule, so we have to multiply it with the gradient of the next layer\n    #which we stored away in (.g). ","84587e44":"def lin_grad(inp, out, w, b):\n    # gradient of matmul with respect to input\n    inp.g = out.g @ w.t()  # gradient of output with respect to input\n    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0) # gradient of output with respect to weight \n    b.g = out.g.sum(0)  # gradient of output with respect to bias\n    \n    #inp.g = out.g @ w.t()  = we do the same thing for the linear layer but we do a gradient of a matrix product where is simply a matrix product with a transpos (.t())\n    ","a482be91":"def forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = inp @ w1 + b1\n    l2 = relu(l1)\n    out = l2 @ w2 + b2\n    # we don't actually need the loss in backward!\n    loss = mse(out, targ)\n    \n    # backward pass: #note we do it in \"omvendt\" order since we use the chain rule \n    mse_grad(out, targ)\n    lin_grad(l2, out, w2, b2)\n    relu_grad(l1, l2)\n    lin_grad(inp, l1, w1, b1)\n    \n    #backpropagation is just the chain rule where we save away the intermidet calulation (.g)so we dont have to calulate them again\n    ","2c18c127":"forward_and_backward(x_train, y_train)","00215b69":"# Save for testing against later\nw1g = w1.g.clone()\nw2g = w2.g.clone()\nb1g = b1.g.clone()\nb2g = b2.g.clone()\nig  = x_train.g.clone()","eeb2f638":"xt2 = x_train.clone().requires_grad_(True)\nw12 = w1.clone().requires_grad_(True)\nw22 = w2.clone().requires_grad_(True)\nb12 = b1.clone().requires_grad_(True)\nb22 = b2.clone().requires_grad_(True)","ff105cf0":"def forward(inp, targ):\n    # forward pass:\n    l1 = inp @ w12 + b12\n    l2 = relu(l1)\n    out = l2 @ w22 + b22\n    # we don't actually need the loss in backward!\n    return mse(out, targ)","7fd6aa31":"loss = forward(xt2, y_train)","f34b0619":"loss.backward()","bf5fd7c7":"# test_near(w22.grad, w2g)\n# test_near(b22.grad, b2g)\n# test_near(w12.grad, w1g)\n# test_near(b12.grad, b1g)\n# test_near(xt2.grad, ig )","13361f1d":"class Relu():\n    def __call__(self, inp): #__call__ means dondercall and it does so we can treat the Relu class as a function. so if you call the Relu() without any parameters it return __Call__ function\n        self.inp = inp  #save the input \n        self.out = inp.clamp_min(0.)-0.5  #save the output, where all negative numbers are turned to 0 and minus 0.5, see the code longer up in this notebook for deeper eksplination if needed\n        return self.out #and let us return the output \n    \n    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g #this was the activation in the backward pass and we save it in self.inp.g ","53030908":"class Lin():\n    def __init__(self, w, b): self.w,self.b = w,b #__init__ is a special Python method that is automatically called when memory is allocated for a new object. \n        #The sole purpose of __init__ is to initialize the values of instance members for the new object\n        \n    #forward pass \n    def __call__(self, inp):\n        self.inp = inp #save the input\n        self.out = inp@self.w + self.b #save the output, where the input is a matrix multiplication with the weight matrix added with the bias vector\n        return self.out #and let us return the output \n    \n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()  # gradient of output with respect to input\n        # Creating a giandient outer product, just to sum it, is inefficient!\n        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0) # gradient of output with respect to weight \n        self.b.g = self.out.g.sum(0) # gradient of output with respect to bias","cac51045":"class Mse():\n    def __call__(self, inp, targ):\n        self.inp = inp  #save the input \n        self.targ = targ #save the target\n        self.out = (inp.squeeze() - targ).pow(2).mean() #calulate the mean \"kvadratrod\" error\n        return self.out #return the output\n    \n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) \/ self.targ.shape[0] # and here are our gradient see code longer up in this notebook for a deeper explornation","51c29968":"class Model():\n    def __init__(self, w1, b1, w2, b2): \n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] #list of all our layers, note it calls an other classes defined above\n        self.loss = Mse() #define loss funcion \n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x) #call are gonna go through each layer x = l(x) here we call and the function on the result of the previuas thing \n        return self.loss(x, targ) #and then we call self.loss on that \n    \n    def backward(self):\n        self.loss.backward() #here we do the 'direkte modsatte' \n        for l in reversed(self.layers): l.backward() #here we go through the reversed layers and call backwards on each one an remember that the backward pass are gonna\n            # save the gradien away inside the (.g) -- se function above ","8731e73c":"w1.g,b1.g,w2.g,b2.g = [None]*4 #let save all our gradients to None, so we know we are not \"snyder\"\nmodel = Model(w1, b1, w2, b2) #then we create our model ","3e1ff793":"%time loss = model(x_train, y_train) #and we can call it as if it was a function because '__call__' ","dfbc57c6":"%time model.backward() ##call our backward","52d9b4aa":"# test_near(w2g, w2.g)\n# test_near(b2g, b2.g)\n# test_near(w1g, w1.g)\n# test_near(b1g, b1.g)\n# test_near(ig, x_train.g)","91217a80":"class Module():\n    def __call__(self, *args):\n        self.args = args \n        self.out = self.forward(*args) #have something that calls forward \n        return self.out #return output\n    \n    def forward(self): raise Exception('not implemented') #which we will set to 'not implemented' \n    def backward(self): self.bwd(self.out, *self.args) #backward are gonna take in the output we just saved ","5c609973":"class Relu(Module):\n    def forward(self, inp): return inp.clamp_min(0.)-0.5 #so relu have somethin called forward which uses the same code as before\n    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g # and backward just have this smaller version of the code as before ","0d149aae":"class Lin(Module):\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def forward(self, inp): return inp@self.w + self.b\n    \n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g) # we can rewrite the  self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0) with einsum\n        self.b.g = out.g.sum(0)","26bb2b98":"class Mse(Module):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) \/ targ.shape[0]","9aae7de1":"\nclass Model():\n    def __init__(self):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()","8b1c5031":"w1.g,b1.g,w2.g,b2.g = [None]*4\nmodel = Model()","2c4b00b6":"%time loss = model(x_train, y_train)","f9ce6e91":"%time model.backward()","bf2fe8cb":"# test_near(w2g, w2.g)\n# test_near(b2g, b2.g)\n# test_near(w1g, w1.g)\n# test_near(b1g, b1.g)\n# test_near(ig, x_train.g)","3fa73666":"class Lin(Module):\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def forward(self, inp): return inp@self.w + self.b\n    \n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        self.w.g = inp.t() @ out.g #but we can also rewrite this again from  self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g). Because it is realy jus the input.transpos matrix multiplied with the output gradient \n        self.b.g = out.g.sum(0)","38c75ddd":"w1.g,b1.g,w2.g,b2.g = [None]*4\nmodel = Model()","d96eabdb":"%time loss = model(x_train, y_train)","a224aade":"%time model.backward()","ec17b4c9":"# test_near(w2g, w2.g)\n# test_near(b2g, b2.g)\n# test_near(w1g, w1.g)\n# test_near(b1g, b1.g)\n# test_near(ig, x_train.g)","08950d36":"\n#export\nfrom torch import nn","f80264bb":"class Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n        self.loss = mse\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x.squeeze(), targ)","c5e24397":"model = Model(m, nh, 1)","5492c0bf":"%time loss = model(x_train, y_train)","da5b0526":"%time loss.backward()","dbeceac8":"# nn.Linear and nn.Module","3c75fc3e":"numpy. std() in Python. numpy. std(arr, axis = None) : Compute the standard deviation of the given data (array elements) along the specified axis(if any).. Standard Deviation (SD) is measured as the spread of data distribution in the given data set","7fb1ab0b":"\nFrom pytorch docs: a: the negative slope of the rectifier used after this layer (0 for ReLU by default)\n\n$$\\text{std} = \\sqrt{\\frac{2}{(1 + a^2) \\times \\text{fan_in}}}$$\nThis was introduced in the paper that described the Imagenet-winning approach from He et al: Delving Deep into Rectifiers, which was also the first paper that claimed \"super-human performance\" on Imagenet (and, most importantly, it introduced resnets!)","632fc3dd":"now we use the code from before and put it into classes. More directly we are taking each of our layers, and making them into classes, here is 'Relu' a activation and 'Lin' is the linear layer ","a761cf9e":"![image.png](attachment:image.png)","f79049cb":"## Basic architecture","319cb328":"# Module.forward()","bbfbda39":"# The forward and backward passes","86e3e806":"the graf above show the blue dots as being points in our dataset with a std of 1. Here we remove haf and this is a problem when we train models since we have half as much data to train on and as the number of runs it has the more the data is removed","18fcfc11":"The next we need is a loss function to check how well we are doing ","7f97a56e":"# Gradients and backward pass","57e80ce7":"# Loss function: MSE","a0415800":"![image.png](attachment:image.png)","a3cfb5d2":"# Refactor model","2b7db98d":"## Layers as classes","b0015c28":"We cheat a little bit and use PyTorch autograd to check our results.","9839615a":"The reason the above code wont be optimal is because that we remove every negative number under the actiovation. So efter a few runs most of the data will be gone. ","437f437b":"for the model there are to layers so we need two weight matrices and two bias vectors ","70bef8a4":"we can now rewrite the above code so it looks better and cleaner ","a91390cd":"There is one hidden layer, and usally we would wan 10 activations since there are 10 tensor a the output, but we will \nuse MAE, so it means we only need one activation a the end.","08e63721":"the above picture show to formulars, that tells the same thing. The thing it tells is how we go from input(x) though the model and to mse to make the target prediction. ","1b217c43":"## This is last part of fastai part 2 lesson 8, though it is mixed with my notes, eksperiments and what I found usefull, if you want the pure version, check fastai github or the following link: https:\/\/github.com\/fastai\/course-v3\/tree\/master\/nbs\/dl2","1838dada":"# Foundations version","66b18522":"# Without einsum","bc495b14":"the backward pass is the one of which that tells us how to update our parameters \nfor this we use gradients ","b2827eb2":"This is how Pytorch implement the same code as above ","d7b7757e":"![image.png](attachment:image.png)"}}