{"cell_type":{"41a9df6a":"code","36ac69b7":"code","b66a2682":"code","df638d2c":"code","0a528ba8":"code","9a1e220b":"code","80b57f53":"code","d0ae0103":"code","f4114e87":"code","9d19646e":"code","6f2b4931":"code","5aced1cd":"code","dae4d2db":"code","89b27290":"code","4c28fa22":"code","c0fdf05a":"code","1488b7bb":"code","15e727a5":"code","2a5bcb9e":"code","0253ef11":"code","d197b2ad":"code","9e42fc27":"code","0b469d41":"code","92a7f212":"code","29497dc9":"code","f5729e04":"code","fa5d9f15":"code","95677476":"code","d834f6b9":"code","027bab38":"code","304cce0f":"markdown","afbde5aa":"markdown","fb088cdc":"markdown","0ccb1988":"markdown","ef5a0f3d":"markdown","4ade104a":"markdown","3d6df0fb":"markdown","6ed58bd1":"markdown","bbdd3b8d":"markdown"},"source":{"41a9df6a":"#Import the dependencies we will require\nimport numpy as np \nimport pandas as pd \nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom PIL import Image\nimport tensorflow_addons as tfa","36ac69b7":"#Set the paths for the data\npath = '..\/input\/plant-pathology-2021-fgvc8\/'\ntrain_dir = path + 'train_images\/'\ntest_dir = path + 'test_images\/'","b66a2682":"#Read the dataframe\ndf = pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/train.csv')","df638d2c":"#Load the model. Replace this with your the path to you own model if you like\nmodel = keras.models.load_model('..\/input\/plant-pathology-basic-cnn\/best_model.h5')","0a528ba8":"# Converting to multi-label problem\n# This is required because I trained my CNN model as a multi-label problem. If you trained it as multi-class, \n# or followed any other labelling conventions, please modify this code (or ignore it all together)\n# (credits: https:\/\/www.kaggle.com\/shanmukh05\/plant-pathology-2k21-baseline-tpu-training)\n\ncount_dict = df.labels.value_counts()\nlabel2id = {\n    'scab': 0,\n    'frog_eye_leaf_spot' : 1,\n    'rust' : 2,\n    'complex' : 3,\n    'powdery_mildew' : 4,\n}\nNUM_CLASS = len(label2id)\nid2label = dict([(value, key) for key, value in label2id.items()])\ndf[\"labels\"] = df[\"labels\"].map(lambda x : [i for i in x.split(\" \") if i != \"healthy\"])\ndf[\"labels\"] = df[\"labels\"].map(lambda x : [label2id[i] for i in x])\ndf.head()","9a1e220b":"image_datagen = ImageDataGenerator(rescale = 1.\/255)\nimage_generator = image_datagen.flow_from_dataframe(dataframe = df,\n                                                   directory = train_dir,\n                                                   target_size = (256,256),\n                                                   x_col = 'image',\n                                                   y_col = 'labels',\n                                                   batch_size = 128,\n                                                   color_mode = 'rgb',\n                                                   class_mode = 'categorical',\n                                                   seed = 3)","80b57f53":"#Get the first batch of images\nx_batch, y_batch = next(image_generator)","d0ae0103":"# This is helper code required to convert my predictions back to their labels\ndef convert_to_text(pred):\n    labels = \"\"\n    indices = np.where(pred == 1)[0]\n    for index in indices:\n        labels = labels + id2label[index] + \" \"\n    if labels == \"\":\n        labels = \"healthy\"    \n    return labels \n\ndef convert_pred_to_text(pred):\n    labels = []\n    for row in pred:\n        text_pred = \"\"\n        if row != []:\n            for i,v in enumerate(row):\n                text_pred = text_pred + id2label[v] + \" \"\n        else:\n            text_pred = \"healthy\"\n        text_pred = text_pred.strip()\n        labels.append(text_pred)\n    return labels \n\nlabel2id = {\n    'scab': 0,\n    'frog_eye_leaf_spot' : 1,\n    'rust' : 2,\n    'complex' : 3,\n    'powdery_mildew' : 4,\n}\nNUM_CLASS = len(label2id)\nid2label = dict([(value, key) for key, value in label2id.items()])\n\ndef convert_image_to_tensor(img):\n    return np.expand_dims(keras.preprocessing.image.img_to_array(img), axis = 0)\n\ndef plot_batch(x_batch, y_batch, number_of_images):\n    for i in range (0, number_of_images):\n        image = x_batch[i]\n        plt.imshow(image)\n        plt.title(convert_to_text(y_batch[i]))\n        plt.show()","f4114e87":"# Let's plot the first 20 images in our batch\nplot_batch(x_batch, y_batch, 20)","9d19646e":"# Let's take a look at the architecture of our pre-trained CNN\nfrom keras.utils.vis_utils import plot_model\nplot_model(model, show_shapes = True, show_layer_names = True)","6f2b4931":"#Predictions helper code\ndef get_labels_from_pred(pred):\n    threshold = {0: 0.57,\n             1: 0.41,\n             2: 0.36,\n             3: 0.37,\n             4: 0.17}\n    labels = []\n    for row in pred:\n        row_label = []\n        for i,v in enumerate(row):\n            if v > threshold[i]:\n                row_label.append(i)\n        labels.append(row_label)\n    return labels\n","5aced1cd":"# Lets consider the first ten layers\nlayer_outputs = [layer.output for layer in model.layers[:10]] \nactivation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input","dae4d2db":"# Let's choose an image to predict\n# For this example I will choose the 3rd image in the batch\nimg = x_batch[5]\nimg_tensor = convert_image_to_tensor(x_batch[5])\npredicted_output = model.predict(img_tensor)\npredicted_labels = get_labels_from_pred(predicted_output)\nplt.imshow(img)\nprint(\"Actual label %s\" %(convert_to_text(y_batch[5])))\nprint(\"Predicted label %s\" %(convert_pred_to_text(predicted_labels)))","89b27290":"#Let's get run the image through the activation model that we have defined earlier. This will help us to visualize\n#the intermediate activations\nactivations = activation_model.predict(img_tensor) \n","4c28fa22":"#Lets visualize a single activation say of the layer conv2d\nfirst_layer_activation = activations[0]\nprint(first_layer_activation.shape)","c0fdf05a":"#Lets try to plot a few channels of the activation of the conv2d layer (first convolution layer) \n#I am choosing channels numbered 9, 10, and 11\nfig = plt.figure(figsize=(14,14))\n\nfig.add_subplot(1, 4, 1)\nplt.imshow(x_batch[5])\n\nfig.add_subplot(1, 4, 2)\nplt.imshow(first_layer_activation[0, :, :, 9])\nplt.title('Channel 8')\n\nfig.add_subplot(1, 4, 3)\nplt.imshow(first_layer_activation[0, :, :, 10])\nplt.title('Channel 9')\n\nfig.add_subplot(1, 4, 4)\nplt.imshow(first_layer_activation[0, :, :, 11])\nplt.title('Channel 10')","1488b7bb":"# Credits: https:\/\/github.com\/gabrielpierobon\/cnnshapes\/blob\/master\/README.md\nlayer_names = []\nfor layer in model.layers[:10]:\n    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n    \nimages_per_row = 16\nplt.imshow(x_batch[5])\nplt.title(convert_to_text(y_batch[5]))\nfor layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n    n_features = layer_activation.shape[-1] # Number of features in the feature map\n    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n    n_cols = n_features \/\/ images_per_row # Tiles the activation channels in this matrix\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size, # Displays the grid\n                         row * size : (row + 1) * size] = channel_image\n    scale = 2. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    \n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","15e727a5":"third_layer_activation = activations[2]\nfifth_layer_activation = activations[4]\nseventh_layer_activation = activations[6]\nfig = plt.figure(figsize=(14,14))\n\nfig.add_subplot(1, 4, 1)\nplt.imshow(x_batch[5])\n\nfig.add_subplot(1, 4, 2)\nplt.imshow(third_layer_activation[0, :, :, 16])\nplt.title('2nd Conv2d layer Channel 17')\n\nfig.add_subplot(1, 4, 3)\nplt.imshow(fifth_layer_activation[0, :, :, 17])\nplt.title('3rd Conv2d layer Channel 18')\n\nfig.add_subplot(1, 4, 4)\nplt.imshow(seventh_layer_activation[0, :, :, 8])\nplt.title('4th Conv2d layer Channel 9')","2a5bcb9e":"baseline = tf.zeros(shape=(256,256,3))\nm_steps=50\nalphas = tf.linspace(start=0.0, stop=1.0, num=m_steps+1)","0253ef11":"def interpolate_images(baseline,\n                       image,\n                       alphas):\n  alphas_x = alphas[:, tf.newaxis, tf.newaxis, tf.newaxis]\n  baseline_x = tf.expand_dims(baseline, axis=0)\n  input_x = tf.expand_dims(image, axis=0)\n  delta = input_x - baseline_x\n  images = baseline_x +  alphas_x * delta\n  return images","d197b2ad":"interpolated_images = interpolate_images(\n    baseline=baseline,\n    image=x_batch[5],\n    alphas=alphas)","9e42fc27":"fig = plt.figure(figsize=(20, 20))\n\ni = 0\nfor alpha, image in zip(alphas[0::10], interpolated_images[0::10]):\n  i += 1\n  plt.subplot(1, len(alphas[0::10]), i)\n  plt.title(f'alpha: {alpha:.1f}')\n  plt.imshow(image)\n  plt.axis('off')\n\nplt.tight_layout();","0b469d41":"def compute_gradients(images, target_class_idx):\n  with tf.GradientTape() as tape:\n    tape.watch(images)\n    logits = model(images)\n    probs = tf.nn.softmax(logits, axis=-1)[:, target_class_idx]\n  return tape.gradient(probs, images)","92a7f212":"def integral_approximation(gradients):\n  # riemann_trapezoidal\n  grads = (gradients[:-1] + gradients[1:]) \/ tf.constant(2.0)\n  integrated_gradients = tf.math.reduce_mean(grads, axis=0)\n  return integrated_gradients","29497dc9":"path_gradients = compute_gradients(\n    images=interpolated_images,\n    target_class_idx=2)","f5729e04":"ig = integral_approximation(\n    gradients=path_gradients)","fa5d9f15":"@tf.function\ndef integrated_gradients(baseline,\n                         image,\n                         target_class_idx,\n                         m_steps=50,\n                         batch_size=32):\n  # 1. Generate alphas.\n  alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps+1)\n\n  # Initialize TensorArray outside loop to collect gradients.    \n  gradient_batches = tf.TensorArray(tf.float32, size=m_steps+1)\n\n  # Iterate alphas range and batch computation for speed, memory efficiency, and scaling to larger m_steps.\n  for alpha in tf.range(0, len(alphas), batch_size):\n    from_ = alpha\n    to = tf.minimum(from_ + batch_size, len(alphas))\n    alpha_batch = alphas[from_:to]\n\n    # 2. Generate interpolated inputs between baseline and input.\n    interpolated_path_input_batch = interpolate_images(baseline=baseline,\n                                                       image=image,\n                                                       alphas=alpha_batch)\n\n    # 3. Compute gradients between model outputs and interpolated inputs.\n    gradient_batch = compute_gradients(images=interpolated_path_input_batch,\n                                       target_class_idx=target_class_idx)\n\n    # Write batch indices and gradients to extend TensorArray.\n    gradient_batches = gradient_batches.scatter(tf.range(from_, to), gradient_batch)    \n\n  # Stack path gradients together row-wise into single tensor.\n  total_gradients = gradient_batches.stack()\n\n  # 4. Integral approximation through averaging gradients.\n  avg_gradients = integral_approximation(gradients=total_gradients)\n\n  # 5. Scale integrated gradients with respect to input.\n  integrated_gradients = (image - baseline) * avg_gradients\n\n  return integrated_gradients","95677476":"ig_attributions = integrated_gradients(baseline=baseline,\n                                       image=x_batch[5],\n                                       target_class_idx=2,\n                                       m_steps=240)","d834f6b9":"def plot_img_attributions(baseline,\n                          image,\n                          target_class_idx,\n                          m_steps=50,\n                          cmap=None,\n                          overlay_alpha=0.4):\n\n  attributions = integrated_gradients(baseline=baseline,\n                                      image=image,\n                                      target_class_idx=target_class_idx,\n                                      m_steps=m_steps)\n\n  # Sum of the attributions across color channels for visualization.\n  # The attribution mask shape is a grayscale image with height and width\n  # equal to the original image.\n  attribution_mask = tf.reduce_sum(tf.math.abs(attributions), axis=-1)\n\n  fig, axs = plt.subplots(nrows=2, ncols=2, squeeze=False, figsize=(8, 8))\n\n  axs[0, 0].set_title('Baseline image')\n  axs[0, 0].imshow(baseline)\n  axs[0, 0].axis('off')\n\n  axs[0, 1].set_title('Original image')\n  axs[0, 1].imshow(image)\n  axs[0, 1].axis('off')\n\n  axs[1, 0].set_title('Attribution mask')\n  axs[1, 0].imshow(attribution_mask, cmap=cmap)\n  axs[1, 0].axis('off')\n\n  axs[1, 1].set_title('Overlay')\n  axs[1, 1].imshow(attribution_mask, cmap=cmap)\n  axs[1, 1].imshow(image, alpha=overlay_alpha)\n  axs[1, 1].axis('off')\n\n  plt.tight_layout()\n  return fig","027bab38":"_ = plot_img_attributions(image=x_batch[5],\n                          baseline=baseline,\n                          target_class_idx=2,\n                          m_steps=240,\n                          cmap=plt.cm.inferno,\n                          overlay_alpha=0.4)","304cce0f":"## References\n[1] Carvalho, Diogo V., Eduardo M. Pereira, and Jaime S. Cardoso. \"Machine learning \ninterpretability: A survey on methods and metrics.\" Electronics 8, no. 8 (2019): 832.\n\n[2] Rudin, Cynthia. \"Stop explaining black box machine learning models for high stakes \ndecisions and use interpretable models instead.\" Nature Machine Intelligence 1, no. 5 (2019): \n206-215.\n\n[3] Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. \"Axiomatic attribution for deep networks.\" In International Conference on Machine Learning, pp. 3319-3328. PMLR, 2017.","afbde5aa":"## Visualizing activations of intermediate convolutions ","fb088cdc":"##### It seems that the activations get increasingly abstract as we go deeper into the neural net. Manual inspection of these layers does not seem to be a very good idea for us and there are some other ways to interpret the neural network. However, just for fun, let's take a look at some of interesting activations ","0ccb1988":"By using integrated gradients, we are able to find an attribution mask for the features (pixels) that contribute towards our prediction of this image as rust. We can overlay this mask with the original image in order to provide a visual guidance to end users to understand the decisions taken by the deep neural network.","ef5a0f3d":"# Visualising decisions of a basic CNN \n\n## Motivation\nThe increased predictive accuracy of ML models has brought with it its own set of problems, \nthe most significant one being the lack of transparency in the process by which a model \narrives at a conclusion. These opaque or black-box models hide their inner workings from the \nuser thereby preventing technical or domain experts from being able to verify and understand \nthe reasoning of the system and how decisions are made [1]. This lack of transparency can \nlead to severe consequences when algorithms are used in situations like determining bail, air-quality measures, medicine, energy reliability, finance and in other domains [2]. Entrusting \nimportant decisions to a system that cannot explain itself and cannot be explained by humans \npresents evident dangers [1].\n\nFor this notebook, I will be using a basic pre-trained model. <br\/>\nThe training notebook is available at: <br\/>\nhttps:\/\/www.kaggle.com\/mreenav\/plant-pathology-basic-cnn\n\nYou are free to train your own models and visualize their decision boundaries! Just add your saved model as a dataset to this notebook and load it in. \n\n![image.png](attachment:7c229662-5120-450c-8744-83c1c1727fbc.png)\n\n![image.png](attachment:8b835b5b-76db-4fd6-9302-719e6896b0ef.png)","4ade104a":"[3] proposed a new method called Integrated Gradients that allows us to view the attributions of the features towards making a predictions while retaining sensitivity and implementation invariance. This method, called integrated gradients, is defined as the path integral of the gradients along the straight line path between the original input and the baseline input. For image processing, the baseline input can be considered as a black image.\n\nCode adapted from: https:\/\/www.tensorflow.org\/tutorials\/interpretability\/integrated_gradients","3d6df0fb":"**Version History**\n* Version 1: Intermediate Activations of Convolutions\n* Version 2: Integrated Gradients\n","6ed58bd1":"##### Channel 9 seems to have correctly identified the spots indicative of the rust while channel 8 has not learnt much useful information. Channel 10 is partially activated by the rust spot but parts of the background are similaryly activated.<br\/>\n##### As we go deeper into the CNN, we will see that the convolutions get better at understanding what causes the disease. <br\/>\n##### Let's take a look at all the layers and zoom in on any layer that we find interesting.","bbdd3b8d":"# Integrated Gradients\n"}}