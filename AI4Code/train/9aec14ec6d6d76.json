{"cell_type":{"87d8dbbf":"code","6bd7684c":"code","d7cd917b":"code","7ba7e6f4":"code","15610a4a":"code","4da5538f":"code","0ba454ad":"code","14fa4d62":"code","7f8ad6b3":"code","554faa00":"code","bd7cd92f":"code","259b973d":"code","6743cf84":"code","6f3c59ee":"code","6c37e5b9":"code","9095a68e":"code","554517e1":"markdown","2436b724":"markdown","4af58138":"markdown","95414f75":"markdown"},"source":{"87d8dbbf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bd7684c":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\n","d7cd917b":"train = pd.read_csv('..\/input\/leaf-classification\/train.csv.zip')\ntest = pd.read_csv('..\/input\/leaf-classification\/test.csv.zip')\n","7ba7e6f4":"def encode(train, test):\n    label_encoder = LabelEncoder().fit(train.species)\n    labels = label_encoder.transform(train.species)\n    classes = list(label_encoder.classes_)\n\n    train = train.drop(['species', 'id'], axis=1)\n    test_ids=test.id\n    test = test.drop('id', axis=1)\n\n    return train, labels, test, classes,test_ids","15610a4a":"train, labels, test, classes,test_ids = encode(train, test)\n","4da5538f":"scaler = StandardScaler().fit(train.values)\nscaled_train = scaler.transform(train.values)","0ba454ad":"sss = StratifiedShuffleSplit(test_size=0.1, random_state=23)\nfor train_index, valid_index in sss.split(scaled_train, labels):\n    X_train, X_valid = scaled_train[train_index], scaled_train[valid_index]\n    y_train, y_valid = labels[train_index], labels[valid_index]\n    ","14fa4d62":"nb_features = 64 # number of features per features type (shape, texture, margin)   \nnb_class = len(classes)","7f8ad6b3":"# reshape train data\nX_train_r = np.zeros((len(X_train), nb_features, 3))\nX_train_r[:, :, 0] = X_train[:, :nb_features]\nX_train_r[:, :, 1] = X_train[:, nb_features:128]\nX_train_r[:, :, 2] = X_train[:, 128:]\n\n# reshape validation data\nX_valid_r = np.zeros((len(X_valid), nb_features, 3))\nX_valid_r[:, :, 0] = X_valid[:, :nb_features]\nX_valid_r[:, :, 1] = X_valid[:, nb_features:128]\nX_valid_r[:, :, 2] = X_valid[:, 128:]\n","554faa00":"# Keras model with one Convolution1D layer\n# unfortunately more number of covnolutional layers, filters and filters lenght \n# don't give better accuracy\nmodel = Sequential()\nmodel.add(Convolution1D(512, 1, input_shape=(nb_features, 3)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(2048, activation='relu'))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dense(nb_class))\nmodel.add(Activation('softmax'))","bd7cd92f":"y_train = np_utils.to_categorical(y_train, nb_class)\ny_valid = np_utils.to_categorical(y_valid, nb_class)\n\nsgd = SGD(lr=0.01, nesterov=True, decay=1e-6, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n\nnb_epoch = 15\nmodel.fit(X_train_r, y_train, epochs=nb_epoch, validation_data=(X_valid_r, y_valid), batch_size=16)","259b973d":"scaler = StandardScaler().fit(test.values)\nscaled_test = scaler.transform(test.values)","6743cf84":"test_dataset = np.zeros((len(scaled_test), nb_features, 3))\ntest_dataset[:, :, 0] = scaled_test[:, :nb_features]\ntest_dataset[:, :, 1] = scaled_test[:, nb_features:128]\ntest_dataset[:, :, 2] = scaled_test[:, 128:]","6f3c59ee":"\npreds_test = model.predict_proba(test_dataset)\npreds_test","6c37e5b9":"submission = pd.DataFrame(preds_test, columns=classes)\nsubmission.insert(0, 'id', test_ids)\nsubmission","9095a68e":"submission.to_csv('submission.csv', index=False)\nprint('done!')","554517e1":"# split train data into train and validation","2436b724":"# Keras model ","4af58138":"# reshape","95414f75":"# standardize train features"}}