{"cell_type":{"4520768e":"code","cdcf6b7c":"code","e12be1f6":"code","97dd4e62":"code","d1c7551b":"code","aa64d12a":"code","2d554a2c":"code","f3cc2d9f":"code","c6c4fcd1":"code","b1990e25":"code","cdc1690f":"code","15ca9e67":"code","db47afea":"code","eecd390b":"code","d56e8dcb":"code","e8b2f450":"code","2cdc4cde":"code","d0922338":"markdown","aaf42fb2":"markdown","6a8f3e8f":"markdown","e5ad8266":"markdown","a5239ea5":"markdown","214dbd9b":"markdown","5236aff6":"markdown","38cba2d3":"markdown","a6fdf414":"markdown","a3b3c12c":"markdown","7059f8fe":"markdown","ebf86f26":"markdown","1088248b":"markdown","fcb009b1":"markdown","07b9c4ff":"markdown","cc361e1d":"markdown"},"source":{"4520768e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cdcf6b7c":"train_data = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv')\nsubmission_data = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv')","e12be1f6":"print(train_data.isna().sum(), '\\n', submission_data.isna().sum())","97dd4e62":"train_data = train_data.drop(columns=['id', 'title', 'console'], axis=1)\nsubmission_data = submission_data.drop(columns=['console'], axis=1)\n\nprint(train_data.shape, submission_data.shape)","d1c7551b":"train_data = train_data.drop(columns=['no_descriptors'], axis=1)\nsubmission_data = submission_data.drop(columns=['no_descriptors'], axis=1)\n\nprint(train_data.shape, submission_data.shape)","aa64d12a":"import matplotlib.pyplot as plt\n\nfor column in train_data.columns:\n    plt.hist(train_data[column])\n    plt.xlabel(column)\n    plt.show()","2d554a2c":"import seaborn as sns\n\nprint(train_data.sum())\n\nfor column in train_data.columns:\n    if column != 'esrb_rating':\n        sns.catplot(y=column, data=train_data, kind='count', col='esrb_rating')\n        plt.show()","f3cc2d9f":"e_rating_group = train_data[train_data['esrb_rating'] == 'E']\net_rating_group = train_data[train_data['esrb_rating'] == 'ET']\nt_rating_group = train_data[train_data['esrb_rating'] == 'T']\nm_rating_group = train_data[train_data['esrb_rating'] == 'M']\n\nprint('******************************** E Rating Sums ********************************\\n')\nprint(e_rating_group.sum())\nprint('\\n******************************** ET Rating Sums ********************************\\n')\nprint(et_rating_group.sum())\nprint('\\n******************************** T Rating Sums ********************************\\n')\nprint(t_rating_group.sum())\nprint('\\n******************************** M Rating Sums ********************************\\n')\nprint(m_rating_group.sum())","c6c4fcd1":"X = train_data.drop(columns=['esrb_rating'], axis=1)\ny = train_data['esrb_rating']\n\nprint(X.shape, y.shape)","b1990e25":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nlr_model = LogisticRegression(max_iter=200)\nparam_grid = {'C': np.linspace(1, 10, 20), 'solver': ['newton-cg', 'sag', 'saga', 'lbfgs']}\n\ngrid_search_object = GridSearchCV(\n        estimator = lr_model,\n        param_grid = param_grid,\n        scoring = 'accuracy',\n        n_jobs = -1,\n        cv = 100,\n        return_train_score = True\n)\n\ngrid_search_object.fit(X, y)\nprint(grid_search_object.best_estimator_, '\\n', grid_search_object.best_score_)\n\nlr_model = grid_search_object.best_estimator_\nscores = cross_val_score(estimator=lr_model, X=X, y=y, cv=100, scoring='accuracy')\n\nscores = pd.DataFrame(scores)\nprint(scores.describe())\nplt.plot(scores)\nplt.ylabel('Accuracy')\nplt.show()","cdc1690f":"from sklearn.svm import SVC\n\nsvm_model = SVC()\n\nparam_grid = {'C': np.linspace(1, 10, 40), 'gamma': ['scale', 'auto']}\n\ngrid_search_object = GridSearchCV(\n        estimator = svm_model,\n        param_grid = param_grid,\n        scoring = 'accuracy',\n        n_jobs = -1,\n        cv = 100,\n        return_train_score = True\n)\n\ngrid_search_object.fit(X, y)\nprint(grid_search_object.best_estimator_, '\\n', grid_search_object.best_score_)\n\nsvm_model = grid_search_object.best_estimator_\nscores = cross_val_score(estimator=svm_model, X=X, y=y, cv=100, scoring='accuracy')\n\nscores = pd.DataFrame(scores)\nprint(scores.describe())\nplt.plot(scores)\nplt.ylabel('Accuracy')\nplt.show()","15ca9e67":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\ntree_model = DecisionTreeClassifier()\n\nparam_grid = {'criterion': ['gini', 'entropy'], 'max_depth': np.arange(1, 100, 5), 'min_samples_split': np.arange(2, 50, 5), 'max_features': ['auto', 'sqrt', 'log2']}\n\nrandom_search_object = RandomizedSearchCV(\n        estimator = tree_model,\n        param_distributions = param_grid,\n        scoring = 'accuracy',\n        n_jobs = -1,\n        cv = 100,\n        return_train_score = True\n)\n\nrandom_search_object.fit(X, y)\nprint(random_search_object.best_estimator_, '\\n', random_search_object.best_score_)\n\ntree_model = random_search_object.best_estimator_\nscores = cross_val_score(estimator=tree_model, X=X, y=y, cv=100, scoring='accuracy')\n\nscores = pd.DataFrame(scores)\nprint(scores.describe())\nplt.plot(scores)\nplt.ylabel('Accuracy')\nplt.show()","db47afea":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier()\n\nparam_grid = {'n_estimators': np.arange(100, 500, 50), 'criterion': ['gini', 'entropy'], 'max_depth': np.arange(1, 100, 5), 'min_samples_split': np.arange(2, 50, 5), 'max_features': ['auto', 'sqrt', 'log2']}\n\nrandom_search_object = RandomizedSearchCV(\n        estimator = rf_model,\n        param_distributions = param_grid,\n        scoring = 'accuracy',\n        n_jobs = -1,\n        cv = 100,\n        return_train_score = True\n)\n\nrandom_search_object.fit(X, y)\nprint(random_search_object.best_estimator_, '\\n', random_search_object.best_score_)\n\nrf_model = random_search_object.best_estimator_\nscores = cross_val_score(estimator=rf_model, X=X, y=y, cv=100, scoring='accuracy')\n\nscores = pd.DataFrame(scores)\nprint(scores.describe())\nplt.plot(scores)\nplt.ylabel('Accuracy')\nplt.show()","eecd390b":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors': np.arange(2, 50, 2)}\n\ngrid_search_object = GridSearchCV(\n        estimator = knn_model,\n        param_grid = param_grid,\n        scoring = 'accuracy',\n        n_jobs = -1,\n        cv = 100,\n        return_train_score = True\n)\n\ngrid_search_object.fit(X, y)\nprint(grid_search_object.best_estimator_, '\\n', grid_search_object.best_score_)\n\nknn_model = grid_search_object.best_estimator_\nscores = cross_val_score(estimator=knn_model, X=X, y=y, cv=100, scoring='accuracy')\n\nscores = pd.DataFrame(scores)\nprint(scores.describe())\nplt.plot(scores)\nplt.ylabel('Accuracy')\nplt.show()","d56e8dcb":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\ny = label_encoder.fit_transform(y)\n\nprint(y)","e8b2f450":"import xgboost as xgb\n\nxg_cl = xgb.XGBClassifier(objective='multi:softmax', num_class=4, n_estimators=2000, use_label_encoder=False, eval_metric='merror')\n\nscores = cross_val_score(estimator=xg_cl, X=X, y=y, cv=100, scoring='accuracy')\n\nscores = pd.DataFrame(scores)\nprint(scores.describe())\nplt.plot(scores)\nplt.ylabel('Accuracy')\nplt.show()","2cdc4cde":"svm_model.fit(X, y)\nsubmission_pred = svm_model.predict(submission_data.drop(columns=['id']))\n\nsubmission_pred = label_encoder.inverse_transform(submission_pred)\n\noutput = pd.DataFrame({'id': submission_data['id'], 'esrb_rating': submission_pred})\noutput.to_csv('submission_csv', index=False)\nprint(output)\nprint('Your submission was successfully saved!')","d0922338":"# Check For Outliers Related to Actual Ratings","aaf42fb2":"# Create X\/y Data","6a8f3e8f":"# Build a Logistic Regression Model","e5ad8266":"# Remove No Descriptors Features\n\n*Not needed as all other features are 0 only when No Descriptors is 1*","a5239ea5":"# Load Data from CSV Files","214dbd9b":"# Build a K Nearest Neighbors Model","5236aff6":"# Check Distributions of Features for Outliers","38cba2d3":"# Encode Label Data For XGBoost Model","a6fdf414":"# Create Submission Data","a3b3c12c":"# Check for Missing Values","7059f8fe":"# Build a Support Vector Machine Model","ebf86f26":"# Build a Decision Tree Model","1088248b":"# Plot Histograms of all Individual Features against Categorical Ratings","fcb009b1":"# Remove Title\/ID\/Console as they are not Related to Rating","07b9c4ff":"# Build a Random Forest Model","cc361e1d":"# Create a XGBoost Model"}}