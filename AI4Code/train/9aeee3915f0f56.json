{"cell_type":{"e98b9598":"code","cc52219e":"code","189668f7":"code","9ae0de11":"code","59420236":"code","970c80b7":"code","b289e8d7":"code","160033c8":"code","16d359cd":"code","737063ed":"code","33349e38":"code","a2965225":"code","d065cb11":"code","b2fdf2a7":"code","ca65bbae":"code","62a119ba":"code","172e3c62":"code","26f76c6f":"code","b973cc9b":"code","69230c44":"code","4120cd1f":"code","03ce2fc1":"code","3fd45264":"code","81362b3a":"code","1b3da446":"code","ac0a6b69":"markdown","b1c7391c":"markdown","c1d5c3fc":"markdown","daa6f798":"markdown","47a03e49":"markdown","74b969d2":"markdown","c3e6ec87":"markdown","58acb3c7":"markdown","974b5ee8":"markdown","251180ee":"markdown","cf2baa9f":"markdown","0a8fe750":"markdown","4918b967":"markdown","e41125c2":"markdown","660a1283":"markdown","67f0f5a7":"markdown","355d5036":"markdown"},"source":{"e98b9598":"# Install and load relevant packages for this case study analysis.\n\n# Numerical Python Library\nimport numpy as np\n\n# Pandas Library\nimport pandas as pd\n\n# MatPlotLib Library\nimport matplotlib.pyplot as plt\n\n# Scientific Python Library\nimport scipy as sp","cc52219e":"# Loading the data set\n\ndata = pd.read_csv('..\/input\/marketing-campaign-data-set\/Marketing Campaign data.csv')\nmarketing_data = data.copy()\nmarketing_data.head()","189668f7":"# Check marketing_data type\nprint(type(marketing_data))\n\n# Data frame atrtibutes data types\nprint(marketing_data.dtypes)\n\n# Data frame shape\nprint(marketing_data.shape)\n\n# Assiging the target variable into an individual pandas series -> AFFINITY_CARD\ntarget_variable = marketing_data.loc[:, 'AFFINITY_CARD']\nprint(target_variable)","9ae0de11":"# Delete all attributes with low or no correlation with the target variable as well as the COMMENTS attribute\nprint(marketing_data.corr())\n\n# Eliminate the COMMENTS attribute from the data frame \ndel marketing_data['COMMENTS']\nmarketing_data.columns\n\n# Number of unique values of each attribute\nprint(marketing_data.loc[:, 'CUST_ID'].nunique())\nprint(marketing_data.loc[:, 'CUST_GENDER'].nunique())\nprint(marketing_data.loc[:, 'AGE'].nunique())\nprint(marketing_data.loc[:, 'CUST_MARITAL_STATUS'].nunique())\nprint(marketing_data.loc[:, 'COUNTRY_NAME'].nunique())\nprint(marketing_data.loc[:, 'CUST_INCOME_LEVEL'].nunique())\nprint(marketing_data.loc[:, 'EDUCATION'].nunique())\nprint(marketing_data.loc[:, 'OCCUPATION'].nunique())\nprint(marketing_data.loc[:, 'HOUSEHOLD_SIZE'].nunique())\nprint(marketing_data.loc[:, 'YRS_RESIDENCE'].nunique())\nprint(marketing_data.loc[:, 'AFFINITY_CARD'].nunique())\nprint(marketing_data.loc[:, 'BULK_PACK_DISKETTES'].nunique())\nprint(marketing_data.loc[:, 'FLAT_PANEL_MONITOR'].nunique())\nprint(marketing_data.loc[:, 'HOME_THEATER_PACKAGE'].nunique())\nprint(marketing_data.loc[:, 'BOOKKEEPING_APPLICATION'].nunique())\nprint(marketing_data.loc[:, 'PRINTER_SUPPLIES'].nunique())\nprint(marketing_data.loc[:, 'Y_BOX_GAMES'].nunique())\nprint(marketing_data.loc[:, 'OS_DOC_SET_KANJI'].nunique())\n\n# Unique values of each attribute\nprint(marketing_data.loc[:, 'CUST_GENDER'].unique())\nprint(marketing_data.loc[:, 'AGE'].unique())\nprint(marketing_data.loc[:, 'CUST_MARITAL_STATUS'].unique())\nprint(marketing_data.loc[:, 'COUNTRY_NAME'].unique())\nprint(marketing_data.loc[:, 'CUST_INCOME_LEVEL'].unique())\nprint(marketing_data.loc[:, 'EDUCATION'].unique())\nprint(marketing_data.loc[:, 'OCCUPATION'].unique())\nprint(marketing_data.loc[:, 'HOUSEHOLD_SIZE'].unique())\nprint(marketing_data.loc[:, 'YRS_RESIDENCE'].unique())\nprint(marketing_data.loc[:, 'AFFINITY_CARD'].unique())\nprint(marketing_data.loc[:, 'BULK_PACK_DISKETTES'].unique())\nprint(marketing_data.loc[:, 'FLAT_PANEL_MONITOR'].unique())\nprint(marketing_data.loc[:, 'HOME_THEATER_PACKAGE'].unique())\nprint(marketing_data.loc[:, 'BOOKKEEPING_APPLICATION'].unique())\nprint(marketing_data.loc[:, 'PRINTER_SUPPLIES'].unique())\nprint(marketing_data.loc[:, 'Y_BOX_GAMES'].unique())\nprint(marketing_data.loc[:, 'OS_DOC_SET_KANJI'].unique())\n\n# Drop the PRINTER_SUPPLIERS attribute since it has only one value of 1\ndel marketing_data['PRINTER_SUPPLIES']\nmarketing_data.columns\n\n# Change the CUST_ID Attribute to the Pandas Data Frame Index\nmarketing_data = marketing_data.set_index('CUST_ID')\nmarketing_data.head()","59420236":"# Cleanning the OCCUPATION attributes.\nprint(marketing_data.loc[:, 'OCCUPATION'].nunique())\nprint(marketing_data.loc[:, 'OCCUPATION'].unique())\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['?'], 'Other')\nprint(marketing_data.loc[:, 'OCCUPATION'].nunique())\nprint(marketing_data.loc[:, 'OCCUPATION'].unique())","970c80b7":"# CUST_GENDER into binary \n\n# F -> 0 \n# M -> 1\n\nmarketing_data['CUST_GENDER'] = marketing_data['CUST_GENDER'].replace(['F'], 0)\nmarketing_data['CUST_GENDER'] = marketing_data['CUST_GENDER'].replace(['M'], 1)\nprint(marketing_data.loc[:, 'CUST_GENDER'].nunique())\nprint(marketing_data.loc[:, 'CUST_GENDER'].unique())","b289e8d7":"# COUNTRY_NAME into ordinal number based on its frequence of occurance\n# in descending order\nprint(marketing_data.loc[:, 'COUNTRY_NAME'].nunique())\nprint(marketing_data.loc[:, 'COUNTRY_NAME'].unique())\n\n# Check COUNTRY_NAME values frequency of occurance\nprint(marketing_data['COUNTRY_NAME'].value_counts())\n\n# COUNTRY_NAME into ordinal number based on its frequence of occurance\n# in descending order\n\n# United States of America -> 19\n# Argentina -> 18                     \n# Italy -> 17                         \n# Brazil -> 16                        \n# Canada -> 15                        \n# Germany -> 14                       \n# Poland -> 13                         \n# United Kingdom -> 12                 \n# Saudi Arabia -> 11                  \n# Denmark -> 10                        \n# China -> 9                         \n# Singapore -> 8                      \n# New Zealand -> 7                   \n# Australia -> 6                     \n# Japan -> 5                         \n# Turkey -> 4                        \n# France -> 3                         \n# South Africa -> 2                  \n# Spain -> 1                         \n\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['United States of America'], 19)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Argentina'], 18)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Italy'], 17)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Brazil'], 16)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Canada'], 15)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Germany'], 14)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Poland'], 13)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['United Kingdom'], 12)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Saudi Arabia'], 11)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Denmark'], 10)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['China'], 9)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Singapore'], 8)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['New Zealand'], 7)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Australia'], 6)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Japan'], 5)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Turkey'], 4)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['France'], 3)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['South Africa'], 2)\nmarketing_data['COUNTRY_NAME'] = marketing_data['COUNTRY_NAME'].replace(['Spain'], 1)\n\n# Check new attribute values\nprint(marketing_data.loc[:, 'COUNTRY_NAME'].nunique())\nprint(marketing_data.loc[:, 'COUNTRY_NAME'].unique())\nprint(marketing_data['COUNTRY_NAME'].value_counts())","160033c8":"# CUST_INCOME_LEVEL into ordinal numbers from 1 to 12 accordingly\n\nprint(marketing_data.loc[:, 'CUST_INCOME_LEVEL'].nunique())\nprint(marketing_data.loc[:, 'CUST_INCOME_LEVEL'].unique())\nprint(marketing_data['CUST_INCOME_LEVEL'].value_counts())\n\n# CUST_INCOME_LEVEL into ordinal numbers from 1 to 12 accordingly\n\n# J: 190,000 - 249,999 -> 1    \n# L: 300,000 and above -> 2   \n# I: 170,000 - 189,999 -> 3  \n# K: 250,000 - 299,999 -> 4   \n# F: 110,000 - 129,999 -> 5   \n# G: 130,000 - 149,999 -> 6  \n# E: 90,000 - 109,999 -> 7    \n# H: 150,000 - 169,999 -> 8  \n# B: 30,000 - 49,999 -> 9   \n# C: 50,000 - 69,999 -> 10      \n# D: 70,000 - 89,999 -> 11      \n# A: Below 30,000 -> 12 \n\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['J: 190,000 - 249,999'], 1)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['L: 300,000 and above'], 2)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['I: 170,000 - 189,999'], 3)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['K: 250,000 - 299,999'], 4)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['F: 110,000 - 129,999'], 5)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['G: 130,000 - 149,999'], 6)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['E: 90,000 - 109,999'], 7)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['H: 150,000 - 169,999'], 8)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['B: 30,000 - 49,999'], 9)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['C: 50,000 - 69,999'], 10)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['D: 70,000 - 89,999'], 11)\nmarketing_data['CUST_INCOME_LEVEL'] = marketing_data['CUST_INCOME_LEVEL'].replace(['A: Below 30,000'], 12)\n\n# Check new attribute values\nprint(marketing_data.loc[:, 'CUST_INCOME_LEVEL'].nunique())\nprint(marketing_data.loc[:, 'CUST_INCOME_LEVEL'].unique())\nprint(marketing_data['CUST_INCOME_LEVEL'].value_counts())","16d359cd":"# EDUCATION into ordinal numbers based on USA education level in \n# descending order\nprint(marketing_data.loc[:, 'EDUCATION'].nunique())\nprint(marketing_data.loc[:, 'EDUCATION'].unique())\nprint(marketing_data['EDUCATION'].value_counts())\n\n# Oridnal numbers USA Education levels in descending order\n\n# Phd (Doctorate) -> 9\n# Masters (Masters) -> 8\n# Bach. (College Degree) -> 7\n# Assoc-V (Associate Degree) -> 6\n# Assoc-A (Associate Degree) -> 6\n# profsc (Associate Degree) -> 6\n# < Bach. (Less than College Degree) -> 5\n# HS-grad (High School Graduate) -> 5\n# 12th (Secondary School - High School) -> 4\n# 11th (Secondary School - High School) -> 4\n# 10th (Secondary School - High School) -> 4\n# 9th (Secondary School - High School) -> 4\n# 7th-8th (Secondary School - Junior High) -> 3\n# 5th-6th (Primary School) -> 2\n# 1st-4th (Primary School) -> 2\n# Presch. (Pre-School) -> 1\n\nmarketing_data['EDUCATION'] = marketing_data['EDUCATION'].replace(['PhD'], 9)\nmarketing_data['EDUCATION'] = marketing_data['EDUCATION'].replace(['Masters'], 8)\nmarketing_data['EDUCATION'] = marketing_data['EDUCATION'].replace(['Bach.'], 7)\nmarketing_data['EDUCATION'] = marketing_data['EDUCATION'].replace(['Assoc-V', 'Assoc-A', 'Profsc'], 6)\nmarketing_data['EDUCATION'] = marketing_data['EDUCATION'].replace(['< Bach.', 'HS-grad'], 5)\nmarketing_data['EDUCATION'] = marketing_data['EDUCATION'].replace(['12th', '11th', '10th', '9th'], 4)\nmarketing_data['EDUCATION'] = marketing_data['EDUCATION'].replace(['7th-8th'], 3)\nmarketing_data['EDUCATION'] = marketing_data['EDUCATION'].replace(['5th-6th', '1st-4th'], 2)\nmarketing_data['EDUCATION'] = marketing_data['EDUCATION'].replace(['Presch.'], 1)\n\n# Check new attribute values\nprint(marketing_data.loc[:, 'EDUCATION'].nunique())\nprint(marketing_data.loc[:, 'EDUCATION'].unique())\nprint(marketing_data['EDUCATION'].value_counts())","737063ed":"# HOUSEHOLD_SIZE into ordinal numbers based on number of rooms\nprint(marketing_data.loc[:, 'HOUSEHOLD_SIZE'].nunique())\nprint(marketing_data.loc[:, 'HOUSEHOLD_SIZE'].unique())\nprint(marketing_data['HOUSEHOLD_SIZE'].value_counts())\n\n# HOUSEHOLD_SIZE into ordinal numbers based on number of rooms\n\n# 1 -> 1\n# 2 -> 2\n# 3 -> 3\n# 4-5 -> 4\n# 6-8 -> 4\n# 9 -> 4\n# 9+ -> 4\n\nmarketing_data['HOUSEHOLD_SIZE'] = marketing_data['HOUSEHOLD_SIZE'].replace(['1'], 1)\nmarketing_data['HOUSEHOLD_SIZE'] = marketing_data['HOUSEHOLD_SIZE'].replace(['2'], 2)\nmarketing_data['HOUSEHOLD_SIZE'] = marketing_data['HOUSEHOLD_SIZE'].replace(['3'], 3)\nmarketing_data['HOUSEHOLD_SIZE'] = marketing_data['HOUSEHOLD_SIZE'].replace(['4-5'], 4)\nmarketing_data['HOUSEHOLD_SIZE'] = marketing_data['HOUSEHOLD_SIZE'].replace(['6-8'], 4)\nmarketing_data['HOUSEHOLD_SIZE'] = marketing_data['HOUSEHOLD_SIZE'].replace(['9'], 4)\nmarketing_data['HOUSEHOLD_SIZE'] = marketing_data['HOUSEHOLD_SIZE'].replace(['9+'], 4)\n\n# Check new attribute values\nprint(marketing_data.loc[:, 'HOUSEHOLD_SIZE'].nunique())\nprint(marketing_data.loc[:, 'HOUSEHOLD_SIZE'].unique())\nprint(marketing_data['HOUSEHOLD_SIZE'].value_counts())","33349e38":"# CUST_MARITAL STATUS into a bianry married or not married numeric variable\n\n# Married -> 1\n# Not Married -> 0\n\nprint(marketing_data.loc[:, 'CUST_MARITAL_STATUS'].nunique())\nprint(marketing_data.loc[:, 'CUST_MARITAL_STATUS'].unique())\nprint(marketing_data['CUST_MARITAL_STATUS'].value_counts())\n\n# CUST_MARITAL STATUS into a bianry married or not married numeric variable\n\n# Married -> 1\n# Not Married -> 0\n\n# Married -> 1\n# NeverM -> 0\n# Divorc. -> 0\n# Separ. -> 0\n# Widowed -> 0\n# Mabsent -> 0\n# Mar-AF -> 0\n\nmarketing_data['CUST_MARITAL_STATUS'] = marketing_data['CUST_MARITAL_STATUS'].replace(['Married'], 1)\nmarketing_data['CUST_MARITAL_STATUS'] = marketing_data['CUST_MARITAL_STATUS'].replace(['NeverM'], 0)\nmarketing_data['CUST_MARITAL_STATUS'] = marketing_data['CUST_MARITAL_STATUS'].replace(['Divorc.'], 0)\nmarketing_data['CUST_MARITAL_STATUS'] = marketing_data['CUST_MARITAL_STATUS'].replace(['Separ.'], 0)\nmarketing_data['CUST_MARITAL_STATUS'] = marketing_data['CUST_MARITAL_STATUS'].replace(['Widowed'], 0)\nmarketing_data['CUST_MARITAL_STATUS'] = marketing_data['CUST_MARITAL_STATUS'].replace(['Mabsent'],0)\nmarketing_data['CUST_MARITAL_STATUS'] = marketing_data['CUST_MARITAL_STATUS'].replace(['Mar-AF'],0)\n\n# Check new attribute values\nprint(marketing_data.loc[:, 'CUST_MARITAL_STATUS'].nunique())\nprint(marketing_data.loc[:, 'CUST_MARITAL_STATUS'].unique())\nprint(marketing_data['CUST_MARITAL_STATUS'].value_counts())","a2965225":"# OCCUPATION into ordinal numbers based on its frequency of occurance \n\nprint(marketing_data.loc[:, 'OCCUPATION'].nunique())\nprint(marketing_data.loc[:, 'OCCUPATION'].unique())\nprint(marketing_data['OCCUPATION'].value_counts())\n\n# OCCUPATION into ordinal numbers based on its frequency of occurance \n\n# Other -> 1      \n# Exec. -> 2      \n# Crafts -> 3     \n# Sales -> 4       \n# Cleric. -> 5    \n# Prof. -> 6        \n# Machine -> 7      \n# Transp. -> 8       \n# Handler -> 9      \n# TechSup -> 10     \n# Farming -> 11       \n# Protec. -> 12     \n# House-s -> 13       \n# Armed-F -> 14\n\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Other'], 1)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Exec.'], 2)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Crafts'], 3)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Sales'], 4)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Cleric.'], 5)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Prof.'], 6)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Machine'], 7)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Transp.'], 8)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Handler'], 9)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['TechSup'], 10)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Farming'], 11)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Protec.'], 12)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['House-s'], 13)\nmarketing_data['OCCUPATION'] = marketing_data['OCCUPATION'].replace(['Armed-F'], 14)\n\n# Check new attribute values\nprint(marketing_data.loc[:, 'OCCUPATION'].nunique())\nprint(marketing_data.loc[:, 'OCCUPATION'].unique())\nprint(marketing_data['OCCUPATION'].value_counts())","d065cb11":"print(marketing_data.head())\nprint(marketing_data.dtypes)","b2fdf2a7":"# General Description\nprint(marketing_data.describe())\n\n# Standard Deviation\nprint(marketing_data.std())\n\n# Sum\nprint(marketing_data.sum())\n\n# Skewness\n# If the skewness is between -0.5 and 0.5, the data is fairly symmetrical\n# If the skewness is between -1 and \u2014 0.5 or between 0.5 and 1, \n# the data is moderately skewed\n# If the skewness is less than -1 or greater than 1, the data is. highly skewed\nprint(marketing_data.skew())\n\n# Kurtosis\n# Kurtosis is one of the two measures that quantify shape of a distribution\n# kur.tosis determine the volume of the outlier and describes the peakedness \n# of the distribution\n# High kurtosis in a data set is an indicator that data has heavy outliers\n# Low kurtosis in a data set is an indicator that data has lack of outliers\nprint(marketing_data.kurtosis())","ca65bbae":"# Variable correlation between attributes of the data frame\nprint(marketing_data.corr())","62a119ba":"# AFFINITY_CARD attribute correlation\nmarketing_data[marketing_data.columns].corr()['AFFINITY_CARD']","172e3c62":"# 1-Variable Histogram Plot using a while loop, program ends if user \n# types 'exit'\n\n# Feel free to write the attributes in upper or lower case, but the exit \n# trigger should be written in lower-case\n\nuser_input = input('Please enter an attribute of the data frame \\\nto obtain its histogram, type \"exit\" to finish and observe all histograms: ')\n\nwhile user_input != 'exit':\n  for n in marketing_data.columns:\n    user_input = user_input.upper()\n    if user_input == n:\n      plt.hist(marketing_data.loc[:, n])\n      plt.xlabel(user_input)\n      plt.show()\n  user_input = input('Please enter another attribute of the data frame \\\nto obtain its histogram, remeber to type \"exit\" to finish and observe \\\nall histograms: ')","26f76c6f":"# 2-Variable Scatter Plot using a while loop, program ends if user \n# types 'exit' \n\n# Feel free to write the attributes in upper or lower case, but the exit \n# trigger should be written in lower-case and in both user_input_x and \n# user_input_y in order to display the scatter plots\n\nuser_input_x = input('Please enter an attribute of the data frame \\\nto show its scatter plot, type \"exit\" to finish: ') \n\nuser_input_y = input('Please enter another attribute of the data frame \\\nto show its scatter plot against the previous input, type \"exit\" to finish: ')\n\nwhile user_input_x != 'exit' or user_input_y != 'exit':\n  for x in marketing_data.columns:\n    for y in marketing_data.columns:\n      user_input_x = user_input_x.upper()\n      user_input_y = user_input_y.upper()\n      if user_input_x == x and user_input_y == y:\n        plt.scatter(marketing_data.loc[:, x], \n                    marketing_data.loc[:, y])\n        plt.xlabel(user_input_x)\n        plt.ylabel(user_input_y)\n        plt.show()\n  user_input_x = input('Please enter another attribute of the data frame \\\nto show its scatter plot, type \"exit\" to finish and obtain all the scatter \\\nplots: ') \n  user_input_y = input('Please enter another attribute of the data frame \\\nto show its scatter plot against the previous attribute, type \"exit\" to \\\nfinish and obtain all the scatter plots: ')","b973cc9b":"# Importing relevant libraries for data predictive modelling\n\n# Skit-learn preprocessing module\nfrom sklearn import preprocessing\n\n# Seaborn library\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes = True)\n\n# Target Variable Data Exploration \nprint(marketing_data['AFFINITY_CARD'].value_counts())\n\n# Exploratory Plot\nsns.countplot(x = 'AFFINITY_CARD',\n              data = marketing_data,\n              palette = 'hls')\nplt.show()\nplt.savefig('count_plt')\n\n# Percentage of affinity card holders (1) vs no-holders (0)\n# The classes of the AFFINITY_CARD attribute are imbalanced and the ratio \n# of no affinity card holders to affinity card holders is 75:25\ncount_no_sub = len(marketing_data[marketing_data['AFFINITY_CARD'] == 0])\ncount_sub = len(marketing_data[marketing_data['AFFINITY_CARD'] == 1])\npct_of_no_sub = count_no_sub \/ (count_no_sub + count_sub)\nprint(f'The percentage of no affinity card holders is {pct_of_no_sub * 100}')\npct_of_sub = count_sub \/ (count_no_sub + count_sub)\nprint(f'The percentage of affinity card holders is {pct_of_sub * 100}')","69230c44":"# Desired Outcome for target variable(AFFINITY_CARD) -> 1\n# The logistic regression model predicts y = 1 or 0 as a function of X\n\n# Importing logistic regression model from skit-learn\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn import metrics\n\n# Importing Dataset Division Algorithm \nfrom sklearn.model_selection import train_test_split\n\n# X -> Independent Variables \n# Y -> Target Variable \nX1 = marketing_data.loc[:, marketing_data.columns != 'AFFINITY_CARD']\ny1 = target_variable\n\n# Train \/ test data split\n# 70% training and 30% test\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, \n                                                        y1, \n                                                        test_size = 0.3, \n                                                        random_state = 42)\n\n# Scaling the data to fit the model\nscaler = preprocessing.StandardScaler().fit(X1_train)\nX1_scaled = scaler.transform(X1_train)\n                                                    \n# Training and fitting the model\nlr_model = LogisticRegression()\nlr_model.fit(X1_scaled, y1_train)\nlr_model","4120cd1f":"# Logistic regression model prediction results for \n# target variable: AFFINITY_CARD\n\n# 853 -> 0 (No affinity card holder)\n# 197 -> 1 (Affinity card holder)\n\nlr_prediction = lr_model.predict(X1_scaled)\nAFFINITY_CARD_LR_PREDICTION = pd.Series(lr_prediction)\nAFFINITY_CARD_LR_PREDICTION.value_counts()","03ce2fc1":"# Accuracy of the logistic regression model \n# 83% Accurate\n\nfrom sklearn import metrics\nprint(f'Accuracy:{metrics.accuracy_score(y1_train, lr_prediction)}')","3fd45264":"# Support Vector Machine model import \nfrom sklearn import svm\nfrom sklearn import metrics\n\n# X2 -> Independent Variables \n# y2 -> Target Variable\n\nX2 = marketing_data.loc[:, marketing_data.columns != 'AFFINITY_CARD']\ny2 = target_variable\n\n# Train \/ test data split\n# 70% training and 30% test\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, \n                                                    y2, \n                                                    test_size = 0.3, \n                                                    random_state = 42)\n\n# Support Vector Machine Classifier - Linear Kernel\nsvm_model = svm.SVC(kernel = 'linear') \n\n# Training and fitting the model \nsvm_model.fit(X2_train, \n              y2_train)","81362b3a":"# Support vector machine model prediction results for \n# target variable: AFFINITY_CARD\n\n# 372 -> 0 (No affinity card holder)\n# 78 -> 1 (Affinity card holder)\n\nsvm_prediction = svm_model.predict(X2_test)\nAFFINITY_CARD_SVM_PREDICTION = pd.Series(svm_prediction)\nAFFINITY_CARD_SVM_PREDICTION.value_counts()","1b3da446":"# Accuracy of the support vector machine model \n\n# 84% Accurate\n\nfrom sklearn import metrics\nprint(f'Accuracy:{metrics.accuracy_score(y2_test, svm_prediction)}')","ac0a6b69":"The data set contains 1500 customer records. Each record consists of 19 variables, which includes socio-demographic and product ownership information.","b1c7391c":"Correlation Between Variables and Target Variable","c1d5c3fc":"# **Marketing Campaign Case Study**\nBy Jorge Manuel Garcia Gallardo\\\nStudent ID: 20021250\n\n**London Metropolitan University**\\\n*MSc Data Analytics*\n\\\n\\\nThe case study is an individual assessment weighted 100% of the marks for the module CC7182 (Programming for data analytics). It is primarily an exercise in applying programming knowledge and skills to data analysis tasks, demonstrating my skills for problem-solving and critical thinking and evaluation. This case study involves the analysis of marketing campaign dataset based on data sets from a retailer company. All data sets were provided by the university, and will be uploaded here. The case study will consists of Python programs and technical report on data understanding, preparation, exploration, initial analysis and data mining.","daa6f798":"## Data Cleaning ##","47a03e49":"## Data Analysis ##\n\nStatistics Summary","74b969d2":"Predictive Model 1 -> **Logistic Regression**\n\nSince the target variable (AFFNITY_CARD) is a binary label (1 or 0) we will do a regression model. The models will be evaluated by the mean squared error and root mean squared error. The possible predictive values are zero or one. The model keeps predictions from being illogical in the sense of having probabilities below 0 or higher than 1.","c3e6ec87":"**Histogram**","58acb3c7":"**Target Variable Data Exploration**","974b5ee8":"The only attribuets eliminated from the dataset, are the CUST_ID, that was assigned as the data frame index, and the COMMENTS attribute since it has no influence on the target variable. All unique values and number of unique values will be analyzed and will be cleaned in the next section, as for errors, there was only a quesiton mark symbol error in the OCCUPATION attribute that will be re-assigned as 'other' variable in the cleaning data phase. All other attributes relating to the different products the customers have purchased have a direct connection to the target variable (AFFINITY_CARD) as well as the CUST_GENDER, AGE, CUST_MARITAL_STATUS, COUNTRY_NAME, CUST_INCOME_LEVEL and EDUCATION.","251180ee":"## Data Visualization ##","cf2baa9f":"## Data Exploration ##","0a8fe750":"**Data Tranformation Analysis**","4918b967":"## Data Transformation ##","e41125c2":"## Data Mining ##","660a1283":"Predictive Model 2 -> **Support Vector Machine**\n\\\n\\\nSuppervised learning model with more accuracy than logistic regression. The model works by separating data points using a hyperplane with the largest amount of margin. It looks for an optimal hyperplane which helps in classifying new data points, into either a 1 or a 0 for the AFFINITY_CARD target variable.","67f0f5a7":"**Scatter Plot**","355d5036":"## Data Preparation ##\nVariable reduction and selection based on its influence on the target variable (AFFINITY_CARD)"}}