{"cell_type":{"b342442f":"code","4dad5c61":"code","3511261f":"code","f71cc87e":"code","98cadfaa":"code","29dccaab":"code","7a8d9efa":"code","91632a2e":"code","7a96b461":"code","e9c00fa4":"code","3b034fc2":"code","68751322":"code","95734a09":"code","c5b40946":"markdown","c5671b74":"markdown","2e658eda":"markdown","d1066806":"markdown","9d6bcb0d":"markdown","6b2902fe":"markdown","7532d98e":"markdown","7cb6c084":"markdown","1e4bf2e3":"markdown","1d7cb2a7":"markdown","39b94545":"markdown","268d10f6":"markdown","0ffd2029":"markdown","14268488":"markdown","4571c981":"markdown"},"source":{"b342442f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder, label_binarize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix, roc_curve, auc, f1_score, precision_score,recall_score, roc_auc_score\nfrom keras.datasets import mnist\nimport warnings \nwarnings.filterwarnings('ignore')","4dad5c61":"df = pd.read_csv('..\/input\/breast-cancer-dataset\/breast_cancer.csv')\ndel df['Unnamed: 32']\nx = df.iloc[:,2:]\n\ny = df['diagnosis']\ny = y.map({'M': 0, 'B': 1})\ny = y.values\n\n\nsc = MinMaxScaler()\nX = sc.fit_transform(x)\nX = np.c_[np.ones(len(X)),X]\nx_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.7)\nx.head()","3511261f":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\ndef train_perceptron(X,Y,alpha = 0.1,iter = 2500):\n    theta = np.random.uniform(size=(X.shape[1], 1))\n    Y = Y.reshape(Y.shape[0], 1)\n    for i in range(iter):\n        z = X @ theta\n        Y_pred = sigmoid(z)\n        error = Y - Y_pred\n        temp = alpha * error\n        theta += X.T @ temp\n    return theta","f71cc87e":"def evaluateClassifier(x,y,y_pred,y_score):\n    cm = pd.DataFrame(\n        confusion_matrix(y, y_pred),\n        columns=['Predicted Benign', 'Predicted Malignant'],\n        index=['True Benign', 'True Malignant']\n    )\n    print('\\nConfusion Matrix: \\n')\n    sns.set(font_scale=1.4) # for label size\n    sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}) # font size\n    plt.show()\n\n    w1 = cm['Predicted Benign']['True Benign'] \/ (cm['Predicted Benign']['True Benign'] + cm['Predicted Malignant']['True Benign'])\n    w2 = cm['Predicted Malignant']['True Malignant'] \/ (cm['Predicted Benign']['True Malignant'] + cm['Predicted Malignant']['True Malignant'])\n    print('\\nClasswise accuracy: ')\n    print('\\nBenign: ',w1 * 100)\n    print('\\nMalignant: ',w2 * 100)\n    \n    indices = ['Accuracy','Precision','F1 score','Recall  score']\n    eval = pd.DataFrame([accuracy_score(y,y_pred) * 100,precision_score(y,y_pred,average = 'macro') * 100,f1_score(y,y_pred,average = 'macro') * 100,recall_score(y,y_pred,average = 'macro') * 100],columns=['Value'],index=indices)\n    eval.index.name = 'Metrics'\n    print('\\n',eval)\n  \n    fpr,tpr,_ = roc_curve(y, y_score)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)        \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.05])\n    plt.ylim([0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    plt.title('ROC curve')\n    plt.show()","98cadfaa":"weights = train_perceptron(x_train,y_train)\ny_pred = sigmoid(x_test @ weights)\n","29dccaab":"y_probs = y_pred\ny_pred[y_pred >=  0.5] = 1\ny_pred[y_pred <  0.5] = 0\nevaluateClassifier(x_test,y_test,y_pred,y_probs)","7a8d9efa":"(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train \/ 255\nx_test = x_test \/ 255\n\nnsamples, nx, ny = x_train.shape\nx_train = x_train.reshape((nsamples,nx*ny))\n\nnsamples, nx, ny = x_test.shape\nx_test = x_test.reshape((nsamples,nx*ny))\n","91632a2e":"training_acc = np.zeros(11)\ntesting_acc = np.zeros(11)\niter = 0\nfor i in range(30,41): \n    mlp = MLPClassifier(hidden_layer_sizes = (i), max_iter = 10, alpha=0.001, solver='sgd', verbose=False, learning_rate_init=0.01)\n    \n    mlp.fit(x_train, y_train)\n    training_acc[iter] = mlp.score(x_train, y_train)\n    \n    mlp.fit(x_test, y_test)\n    testing_acc[iter] = mlp.score(x_test, y_test)\n    iter += 1\n","7a96b461":"plt.plot(range(30,41),training_acc * 100,'b-')\nplt.xlabel('No. of hidden layer nodes')\nplt.ylabel('Accuracy')\nplt.title('Training accuracy v\/s no. of hidden nodes')\nplt.show()\ns = pd.Series(training_acc * 100,range(30,41))\ndf = pd.DataFrame({'No. of hidden nodes':s.index, 'Training Accuracy':s.values})\ndf\n","e9c00fa4":"plt.plot(range(30,41),testing_acc * 100,'b-')\nplt.xlabel('No. of hidden layer nodes')\nplt.ylabel('Accuracy')\nplt.title('Testing accuracy v\/s no. of hidden nodes')\nplt.show()\ntesting_acc * 100\n\ns = pd.Series(testing_acc * 100,range(30,41))\ndf = pd.DataFrame({'No. of hidden nodes':s.index, 'Testing Accuracy':s.values})\ndf\n","3b034fc2":"mlp = MLPClassifier(hidden_layer_sizes = (40), max_iter = 10, alpha=0.001, solver='sgd', verbose=False, learning_rate_init=0.01)\nmlp.fit(x_train,y_train)\ny_pred = mlp.predict(x_test)\ny_probs = mlp.predict_proba(x_test)\n","68751322":"indices = ['Accuracy','Precision','F1 score','Recall  score']\neval = pd.DataFrame([accuracy_score(y_test,y_pred) * 100,precision_score(y_test,y_pred,average = 'macro') * 100,f1_score(y_test,y_pred,average = 'macro') * 100,recall_score(y_test,y_pred,average = 'macro') * 100],columns=['Value'],index=indices)\neval.index.name = 'Metrics'\nprint('\\n',eval)","95734a09":"\nclasses = range(10)\nprobabs = y_probs\ny_test2 = label_binarize(y_test, classes)\nfor i in range(10):\n    preds = probabs[:,i]    \n    fpr, tpr, threshold = roc_curve(y_test2[:, i], preds)\n    roc_auc = auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'Class ' + str(i + 1))\n    plt.legend(loc = 'lower right')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\nplt.show()\n","c5b40946":"## Function to evaluate classifier ","c5671b74":"# Binary classification using Single Layer Perceptron","2e658eda":"## Plotting testing accuracy vs no. of hidden layers ","d1066806":"> Do upvote if you find this notebook useful. So let's get started.","9d6bcb0d":"## Calculating the predicted values","6b2902fe":"# In this notebook, you will find:\n* How to train a perceptron using sigmoid as activation function.\n* Using the perceptron for binary classification.\n* Digit classification using the famous MNIST dataset.\n* How to select the optimal number of hidden layers in a multi-layer perceptron\n* Evaluating the perceptrons trained on different classification metrics(Accuracy,Precision,ROC-AUC etc..)","7532d98e":"## Since the maximum testing accuracy is with 40 hidden nodes, we will use 40 nodes in the hidden layer for classification.","7cb6c084":"## Varying the number of hidden layer and calculating the training and testing accuracies","1e4bf2e3":"## Training the perceptron\n\n### I have used sigmoid as the activation function(you can also use the step function). The function *train_perceptron* returns the weight vector after iterating.\n\n### Some links providing good insights about perceptron and training rule:\n\n1. [https:\/\/appliedgo.net\/perceptron\/](http:\/\/)\n1. [https:\/\/sebastianraschka.com\/Articles\/2015_singlelayer_neurons.html](http:\/\/)","1d7cb2a7":"# Multi-class classification on MNIST dataset using Multi-layer Perceptron","39b94545":"## Plotting training accuracy vs no. of hidden layers ","268d10f6":"## Data Preparation","0ffd2029":"## Data Preparation ","14268488":"# Imports","4571c981":"### *Set the threshold value here(I have used 0.5)*"}}