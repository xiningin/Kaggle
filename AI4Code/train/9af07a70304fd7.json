{"cell_type":{"84cdbf06":"code","7572be17":"code","0f4a29d2":"code","9ae78e02":"code","9c60cf06":"code","0388b035":"code","a541db13":"code","e95f09cf":"code","3f743555":"code","85d087ce":"code","75108cb2":"code","2fc49c59":"code","978b1b39":"code","869c6b5e":"code","c0a52e62":"code","9ef76426":"code","bd12f737":"code","3328d991":"code","d6340304":"code","25be7597":"code","15ec717b":"code","136a2c7b":"code","eb0e1c18":"code","bd764a92":"code","c8e59e9c":"code","a6d85bd3":"code","378091df":"code","de51880f":"code","6d0ce4bb":"code","32b0f5b3":"code","4c7a403c":"code","00011179":"code","e8127bea":"code","ccc68193":"code","57019a0e":"code","8458499f":"code","311d4988":"code","a9898b47":"code","95438eda":"code","7805d05b":"code","053369aa":"code","10ffaf99":"code","fdacaa36":"code","947c0a2d":"code","f6b52b28":"code","b7561efc":"code","37200c44":"code","f80109fc":"code","5e3a0382":"code","b285ecfe":"code","5e695446":"code","f57e219a":"code","c5b9bb7c":"code","45ab8e06":"code","60704990":"code","36b58c97":"code","c5b71f10":"code","336e32c2":"code","c94d5a8e":"code","76a09df3":"code","ae455e46":"code","497e0ee1":"code","49dc1249":"code","2725fde3":"code","af104160":"code","ee15cafd":"code","83261015":"code","a9b0c311":"code","6468fff4":"code","9dcf8b04":"code","7674d0b6":"code","0652c462":"code","d6356c93":"code","a8ddfb69":"code","e357ef00":"code","604cd6da":"code","57063a8b":"code","33858510":"code","309ece69":"code","f6c7f805":"code","4beefa8e":"code","06564001":"code","01b7a955":"code","e487c034":"code","9ca27460":"code","52be315b":"code","73dfe4ef":"code","6866dd59":"code","db132bc1":"code","823d7a15":"code","eae6e452":"code","9abc580f":"code","e580b51c":"code","09168280":"code","4059bbc8":"code","5047c05f":"code","1946f261":"code","0634dec0":"code","8f65a0e7":"code","3da3acd2":"code","cc40cb93":"code","6c545809":"code","58a5da8a":"code","3ae71461":"code","a973c16e":"code","9cc0e56a":"code","752fd30c":"markdown","0ab4cb24":"markdown","498a37ae":"markdown","d0a34679":"markdown","f35d8646":"markdown","8d794310":"markdown","9a156b66":"markdown","9dbd9d78":"markdown","435f4a69":"markdown","46ee62e1":"markdown","3ef84e61":"markdown","c333bcbd":"markdown","e99269e9":"markdown","e3603de0":"markdown","33c3448c":"markdown","b1b5c2d1":"markdown","239eb055":"markdown","9f26ed76":"markdown","9e559157":"markdown","29c3877c":"markdown","05596399":"markdown","4f63ed2e":"markdown","e321079b":"markdown","657969b9":"markdown","c8207a79":"markdown","d6baafcc":"markdown","21c971ee":"markdown"},"source":{"84cdbf06":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sqlite3\nimport nltk\nimport string\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7572be17":"# Create a SQL connection to our SQLite database\n# con = sqlite3.connect('C:\/\/Users\/\/MUSKAN\/\/Downloads\/\/Project 5--__ Amazon Customers Data Analysis-20210331T135011Z-001\/\/Project 5--_ Amazon Customers Data Analysis\/\/database.sqlite')","0f4a29d2":"# type(con)","9ae78e02":"# pd.read_sql_query(\"SELECT * FROM Reviews\", con)","9c60cf06":"# df=pd.read_sql_query(\"SELECT * FROM Reviews\", con)","0388b035":"df=pd.read_csv(\"..\/input\/amazon-reviews\/Reviews.csv\")","a541db13":"# pd.read_sql_query(\"SELECT * FROM Reviews LIMIT 3\", con)","e95f09cf":"!pip install textblob","3f743555":"from textblob import TextBlob","85d087ce":"TextBlob(df['Summary'][0]).sentiment.polarity","75108cb2":"\npolarity=[] # list which will contain the polarity of the comments\n\nfor i in df['Summary']:\n    try:\n        polarity.append(TextBlob(i).sentiment.polarity)   \n    except:\n        polarity.append(0)","2fc49c59":"len(polarity)","978b1b39":"data=df.copy()","869c6b5e":"data['polarity']=polarity","c0a52e62":"data.head()","9ef76426":"data['polarity'].nunique()","bd12f737":"data_positive = data[data['polarity']>0]\ndata_positive.shape","3328d991":"from wordcloud import WordCloud, STOPWORDS","d6340304":"stopwords=set(STOPWORDS)","25be7597":"positive=data_positive[0:200000]","15ec717b":"\ntotal_text= (' '.join(data_positive['Summary']))\n","136a2c7b":"len(total_text)","eb0e1c18":"total_text[0:10000]","bd764a92":"import re\ntotal_text=re.sub('[^a-zA-Z]',' ',total_text)","c8e59e9c":"total_text","a6d85bd3":"## remove extra spaces\ntotal_text=re.sub(' +',' ',total_text)","378091df":"total_text[0:10000]","de51880f":"len(total_text)","6d0ce4bb":"wordcloud = WordCloud(width = 1000, height = 500,stopwords=stopwords).generate(total_text)\nplt.figure(figsize=(15,5))\nplt.imshow(wordcloud)\nplt.axis('off')","32b0f5b3":"data_negative = data[data['polarity']<0]\ndata_negative.shape","4c7a403c":"data_negative.head()","00011179":"total_negative= (' '.join(data_negative['Summary']))\n","e8127bea":"total_negative","ccc68193":"import re\ntotal_negative=re.sub('[^a-zA-Z]',' ',total_negative)","57019a0e":"len(total_negative)","8458499f":"total_negative","311d4988":"total_negative=re.sub(' +',' ',total_negative)","a9898b47":"len(total_negative)","95438eda":"\nwordcloud = WordCloud(width = 1000, height = 500,stopwords=stopwords).generate(total_negative)\nplt.figure(figsize=(15,5))\nplt.imshow(wordcloud)\nplt.axis('off')","7805d05b":"df['UserId'].shape","053369aa":"df['UserId'].nunique()","10ffaf99":"df.head()","fdacaa36":"raw=df.groupby(['UserId']).agg({'Summary':'count', 'Text':'count','Score':'mean','ProductId':'count'}).sort_values(by='Text',ascending=False)\nraw","947c0a2d":"raw.columns=['Number_of_summaries','num_text','Avg_score','Number_of_products_purchased']\nraw","f6b52b28":"user_10=raw.index[0:10]\nnumber_10=raw['Number_of_products_purchased'][0:10]\n\nplt.bar(user_10, number_10, label='java developer')\nplt.xlabel('User_Id')\nplt.ylabel('Number of Products Purchased')\nplt.xticks(rotation=60)","b7561efc":"df.head()","37200c44":"## picking a random sample\nfinal=df.sample(n=2000)","f80109fc":"final=df[0:2000]","5e3a0382":"final.isna().sum()","b285ecfe":"final.duplicated().sum()","5e695446":"final.head()","f57e219a":"len(final['Text'][0].split(' '))","c5b9bb7c":"final['Text'][0]","45ab8e06":"\ndef calc_len(text):\n    return (len(text.split(' ')))","60704990":"final['Text_length']=final['Text'].apply(calc_len)","36b58c97":"import plotly.express as px\npx.box(final, y=\"Text_length\")","c5b71f10":"sns.countplot(final['Score'], palette=\"plasma\")","336e32c2":"# converting the text to lower case","c94d5a8e":"final['Text'] =final['Text'].str.lower()\nfinal.head(10)","76a09df3":"final['Text'][164]\n# 164 index","ae455e46":"import re\nre.sub('[^a-zA-Z]',' ',final['Text'][164])","497e0ee1":"# define punctuation\npunctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n\ndata= final['Text'][164]\n\n# remove punctuation from the string\nno_punct = \"\"\nfor char in data:\n    if char not in punctuations:\n        no_punct = no_punct + char\n\n# display the unpunctuated string\nno_punct","49dc1249":"# # import string\n# string.punctuation\n# import nltk\n# nltk.download('stopwords')\n# from nltk.corpus import stopwords\n# stopwords.words('english') #another approach","2725fde3":"# # Let's define a pipeline to clean up all the messages \n# # The pipeline performs the following: (1) remove punctuation, (2) remove stopwords\n\n# def message_cleaning(message):\n#     Test_punc_removed = [char for char in message if char not in string.punctuation]\n#     Test_punc_removed_join = ''.join(Test_punc_removed)\n#     Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]\n#     return Test_punc_removed_join_clean","af104160":"# final['Text']=final['Text'].apply(message_cleaning)","ee15cafd":"final.head()","83261015":"print(final['Text'][1])","a9b0c311":"def remove_punc(review):\n    import string\n    punctuations =string.punctuation\n    # remove punctuation from the string\n    no_punct = \"\"\n    for char in review:\n        if char not in punctuations:\n            no_punct = no_punct + char\n    return no_punct","6468fff4":"final['Text'] =final['Text'].apply(remove_punc)","9dcf8b04":"final.head()","7674d0b6":"final['Text'][164]","0652c462":"import re\nfrom nltk.corpus import stopwords","d6356c93":"review='seriously this product was as tasteless as they come there are much better tasting products out there but at 100 calories its better than a special k bar or cookie snack pack you just have to season it or combine it with something else to share the flavor'","a8ddfb69":"re=[word for word in review.split(' ') if word not in set(stopwords.words('english'))]\nstr=''\nfor wd in re:\n    str=str+wd\n    str=str+' '\n#     including some space #instead of this we can use join as well\n\nstr","e357ef00":"re=[word for word in review.split(' ') if word not in set(stopwords.words('english'))]\n' '.join(re)","604cd6da":"def remove_stopwords(review):\n    return ' '.join([word for word in review.split(' ') if word not in set(stopwords.words('english'))])","57063a8b":"remove_stopwords(review)","33858510":"final.shape","309ece69":"final.columns","f6c7f805":"final['Text'] = final['Text'].apply(remove_stopwords)","4beefa8e":"final.head()","06564001":"final['Text'].str.contains('http?').sum()","01b7a955":"final['Text'].str.contains('http').sum()","e487c034":"pd.set_option('display.max_rows',2000)\n# to show all the 2000 rows otherwise it will display with the gap\nfinal['Text'].str.contains('http',regex=True)","9ca27460":"final['Text'][21]","52be315b":"final['Text'][21]","73dfe4ef":"review=final['Text'][21]\nreview","6866dd59":"import re","db132bc1":"url_pattern = re.compile(r'href|http.\\w+')\n# w indicate any A -Z and a-z\n#after http you have any number of character till space . if there is anything ,and w+ means www or similar way for another charachters\nurl_pattern.sub(r'', review)\n#just replace it with space","823d7a15":"import re\ndef remove_urls(review):\n    url_pattern = re.compile(r'href|http.\\w+')\n    return url_pattern.sub(r'', review)","eae6e452":"final['Text'] = final['Text'].apply(remove_urls)\n","9abc580f":"final.head()\n","e580b51c":"final['Text'].str.contains('http').sum()","09168280":"final['Text'][34]","4059bbc8":"final['Text'][34].replace('br','')\n","5047c05f":"for i in range(len(final['Text'])):\n    final['Text'][i]=final['Text'][i].replace('br','')","1946f261":"data2=final.copy()","0634dec0":"data2['Text'][34]","8f65a0e7":"data2.shape","3da3acd2":"data2.dtypes","cc40cb93":"from wordcloud import WordCloud, STOPWORDS ","6c545809":"stopwords = set(STOPWORDS) ","58a5da8a":"data2.head()","3ae71461":"comment_words = '' \nfor val in data2['Text']:\n    # typecaste each val to string\n    \n    # split the value \n    tokens = val.split() \n    \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n    comment_words=comment_words+ \" \".join(tokens)+\" \"\n    ","a973c16e":"wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) ","9cc0e56a":"# plot the WordCloud image                        \nplt.figure(figsize = (8, 8)) \nplt.imshow(wordcloud) \nplt.axis(\"off\") ","752fd30c":"#### logic to remove punctuations or all the special characters","0ab4cb24":"#### Amazon can recommend more products to only those who are going to buy more or to one who has a better conversion rate,so lets ready data according to this problem statement","498a37ae":"#### Conclusion-->>\n    Seems to have Almost 50 percent users are going to give their Feedback limited to 50 words whereas there are only few users who are going give Lengthy Feedbacks","d0a34679":"## Analyse to which set of users Amazon Can recommend more product","f35d8646":"#### Analyze Score","8d794310":"#### check missing values in dataset","9a156b66":"#### drawback of this re.sub in this use-case is, it will remove some numerical data too & may be that numerical values matters alot","9dbd9d78":"#### check if urls is present in Text column or not","435f4a69":"#### reading data from Sqlite database","46ee62e1":"#### reading some n number of rows, use LIMIT over ther","3ef84e61":"CSV file reading","c333bcbd":"#### Create function to remove punctuations in your review","e99269e9":"### Lets perform EDA for the Positve sentences\u00b6","e3603de0":"### What is sentiment analysis?\n    Sentiment analysis is the computational task of automatically determining what feelings a writer is expressing in text\n    Some examples of applications for sentiment analysis include:\n\n\n    Sentiment analysis is not perfect.It also cannot tell you why a writer is feeling a certain way. However, it can be useful to quickly summarize some qualities of text, especially if you have so much text that a human reader cannot analyze it.For this project,the goal is to to classify Food reviews based on customers' text.","33c3448c":"#### These are the Top 10 Users so we can recommend more & more Prodcuts to these Usser Id as there will be a high probability that these person are going to be buy more","b1b5c2d1":"####  Removal of urls","239eb055":"## Lets perform EDA for the Neagtive sentences","9f26ed76":"#### using join to convert list into string","9e559157":"The column or features in the dataset:\nId\nProductId \u2014 unique identifier for the product\nUserId \u2014 unqiue identifier for the user\nProfileName\nHelpfulnessNumerator \u2014 number of users who found the review helpful\nHelpfulnessDenominator \u2014 number of users who indicated whether they found the review helpful or not\nScore \u2014 rating between 1 and 5\nTime \u2014 timestamp for the review\nSummary \u2014 brief summary of the review\nText \u2014 text of the review","29c3877c":"**Sentiment analysis of product reviews, an application\nproblem, has recently become very popular in text mining\nand computational linguistics research.\nThis notebook is all about the data analysis on customer reviews text analysis ,based on the reviews they post on several amazon products\nSo here I have performed sentiment analysis,with the help of NLP  and re modules \nI also extracted the most frequent words which appear in negative as well as positive reviews given by the customers with the help of wordcloud \nAlso determined which set of customers amazon should target in order to increase profit** ","05596399":"##### as we will see we have lots of br in my data, let me remove wherever i have br","4f63ed2e":"### Text Pre-Processsing","e321079b":"#### Removing the Duplicates if any","657969b9":"#### we will observe we have some kind of URLs over here in the data that is definitely a kind of Dirtines in data, so we have to clean this data & make ready data for the analysis purpose","c8207a79":"#### Removal of Stopwords","d6baafcc":"### Analyse Length of Comments whether Customers are going to give Lengthy comments or short one","21c971ee":"Advantages of Word Clouds :\nAnalyzing customer and employee feedback.\nIdentifying new SEO keywords to target."}}