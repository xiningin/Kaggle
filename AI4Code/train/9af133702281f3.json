{"cell_type":{"ebc88692":"code","76cc835f":"code","65dd7d6f":"code","b23650f4":"code","fb0cdf7e":"code","93d896ff":"code","7c207fc0":"code","a7be042c":"code","5bfca68c":"code","774ae205":"code","61b25456":"code","f31290dd":"code","e40a4145":"code","d57ca00d":"code","bf1f4de0":"code","9254f5a4":"code","dd6efe45":"code","b0e9dcd5":"code","d1fa25a9":"code","0b0c019c":"code","4d0590cf":"code","23b12d4d":"code","3de8d467":"code","899e3169":"code","d4ee1d8c":"code","452f6edd":"code","f5cce484":"code","7747fb90":"code","9cf8cb16":"code","8df622ed":"code","e240b572":"code","7d67d9ef":"code","44666939":"code","dc285b31":"code","66e5bad5":"code","3ed6e17b":"code","b3a77b05":"code","ae539576":"code","d398fd90":"code","5c8bcb9c":"code","8b50816b":"code","0abcf59d":"code","0ffc65cd":"code","61a912d8":"code","904c6295":"code","cb480ae4":"code","87492b77":"code","417af461":"code","8a958b66":"code","7cac3da0":"code","04711d56":"code","aeef8ebf":"code","0290154c":"code","9bea9977":"code","ef932012":"code","44410c22":"code","28145dcf":"code","719967c5":"code","f912f4b3":"code","655d24a7":"code","4314fb92":"code","60495782":"code","4953a4f6":"code","00a7d3a3":"code","76fc2532":"code","b3d68bbf":"code","cf864ce4":"code","e7c570b6":"code","66a3fa85":"code","e4988390":"code","d2adc33b":"code","1edc990a":"code","bb6e160e":"code","0ac5a9eb":"code","c1e6ba42":"code","3ea480c7":"markdown","54adfe53":"markdown","45bc37c7":"markdown","1fd3c7ad":"markdown","10004a51":"markdown","bf228815":"markdown","6816accb":"markdown","90258759":"markdown","a9ed1f2e":"markdown","8040ed90":"markdown","c18b524a":"markdown","e8b011c8":"markdown","2262d482":"markdown","463d5ecb":"markdown","5fd3d59b":"markdown","c4dd81bd":"markdown","29841f59":"markdown","7afda011":"markdown","777627e6":"markdown","9893f0a8":"markdown","c28c0db6":"markdown","06143d95":"markdown","e2f8f05a":"markdown","0c96b1ce":"markdown","57eecc0c":"markdown","2a8e31f2":"markdown","ef07b372":"markdown","0be4c456":"markdown","f65dffeb":"markdown"},"source":{"ebc88692":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\npd.set_option('display.max_columns', None)\n\n# Visialisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Machine Learning\n# Utils\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn import preprocessing\n#Feature Selection\nfrom sklearn.feature_selection import chi2, f_classif, f_regression, mutual_info_classif\nfrom sklearn.feature_selection import mutual_info_regression, SelectKBest, SelectPercentile\nfrom sklearn.feature_selection import VarianceThreshold\n# Models\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier\n# Unsupervised Models\nfrom sklearn.cluster import KMeans\n#Metrics\nfrom sklearn.metrics import roc_auc_score\n\n# Fixing Seed\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything()","76cc835f":"data_dir = '..\/input\/jobathon-may-2021-credit-card-lead-prediction'\n\ntrain_file_path = os.path.join(data_dir, 'train.csv')\ntest_file_path = os.path.join(data_dir, 'test.csv')\nsample_sub_file_path = os.path.join(data_dir, 'sample_submission.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Train file: {test_file_path}')\nprint(f'Train file: {sample_sub_file_path}')","65dd7d6f":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","b23650f4":"train_df.sample(10)","fb0cdf7e":"train_df.columns","93d896ff":"train_df.describe().T","7c207fc0":"np.sum(train_df.isnull())","a7be042c":"nulls_train = np.sum(train_df.isnull())\nnullcols_train = nulls_train.loc[(nulls_train != 0)].sort_values(ascending=False)\nnullcols_train = nullcols_train.apply(lambda x: 100*x\/train_df.shape[0])\n\nbarplot_dim = (15, 8)\nax = plt.subplots(figsize=barplot_dim)\nsns.barplot(x=nullcols_train.index, y=nullcols_train)\nplt.ylabel(\"Null %\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.show()","5bfca68c":"nulls_train = np.sum(test_df.isnull())\nnullcols_train = nulls_train.loc[(nulls_train != 0)].sort_values(ascending=False)\nnullcols_train = nullcols_train.apply(lambda x: 100*x\/test_df.shape[0])\n\nbarplot_dim = (15, 8)\nax = plt.subplots(figsize=barplot_dim)\nsns.barplot(x=nullcols_train.index, y=nullcols_train)\nplt.ylabel(\"Null %\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.show()","774ae205":"train_df['Credit_Product'].fillna('Unk', inplace=True)\ntest_df['Credit_Product'].fillna('Unk', inplace=True)","61b25456":"train_df['Credit_Product_Known'] = train_df['Credit_Product'].apply(lambda x: 0 if x == 'Unk' else 1)\ntest_df['Credit_Product_Known'] = test_df['Credit_Product'].apply(lambda x: 0 if x == 'Unk' else 1)","f31290dd":"np.sum(train_df.isnull())","e40a4145":"np.sum(test_df.isnull())","d57ca00d":"ax = plt.subplots(figsize=(18, 6))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Is_Lead', data=train_df);\nplt.ylabel(\"No. of Observations\", size=20);\nplt.xlabel(\"Is Lead?\", size=20);","bf1f4de0":"imbalance_ratio = train_df[train_df['Is_Lead'] == 0].shape[0]\/train_df[train_df['Is_Lead'] == 1].shape[0]\nprint(f'Imbalance ratio: {imbalance_ratio}')","9254f5a4":"train_df.nunique()","dd6efe45":"train_df.shape","b0e9dcd5":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Gender', hue='Is_Lead', data=train_df);","d1fa25a9":"# Response Rate from Gender\nv = train_df.groupby('Gender').Is_Lead.value_counts().unstack()\nv['Ratio'] = v[1]\/(v[0] + v[1])\nv.reset_index(inplace=True)","0b0c019c":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='Gender', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","4d0590cf":"g = sns.catplot(x='Is_Lead', y='Age', kind='boxen', data=train_df);\ng.fig.set_size_inches(15,8)","23b12d4d":"g = sns.catplot(x='Gender', y='Age', kind='boxen', data=train_df);\ng.fig.set_size_inches(15,8)","3de8d467":"g = sns.catplot(x='Gender', y='Age', hue='Is_Lead', kind='box', data=train_df);\ng.fig.set_size_inches(15,8)","899e3169":"ax = plt.subplots(figsize=(30, 8))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Region_Code', hue='Is_Lead', data=train_df);","d4ee1d8c":"# Response Rate from Cities\nv = train_df.groupby('Region_Code').Is_Lead.value_counts().unstack()\nv['Ratio'] = v[1]\/(v[0] + v[1])\nv.reset_index(inplace=True)","452f6edd":"v['Ratio'].mean()","f5cce484":"ax = plt.subplots(figsize=(30, 8))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='Region_Code', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","7747fb90":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Occupation', hue='Is_Lead', data=train_df);","9cf8cb16":"# Response Rate from Occupation\nv = train_df.groupby('Occupation').Is_Lead.value_counts().unstack()\nv['Ratio'] = v[1]\/(v[0] + v[1])\nv.reset_index(inplace=True)","8df622ed":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='Occupation', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","e240b572":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Channel_Code', hue='Is_Lead', data=train_df);","7d67d9ef":"# Response Rate from Channel_Code\nv = train_df.groupby('Channel_Code').Is_Lead.value_counts().unstack()\nv['Ratio'] = v[1]\/(v[0] + v[1])\nv.reset_index(inplace=True)","44666939":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='Channel_Code', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","dc285b31":"g = sns.catplot(x='Is_Lead', y='Vintage', kind='boxen', data=train_df);\ng.fig.set_size_inches(15,8)","66e5bad5":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Credit_Product', hue='Is_Lead', data=train_df);","3ed6e17b":"# Response Rate from Credit_Product\nv = train_df.groupby('Credit_Product').Is_Lead.value_counts().unstack()\nv['Ratio'] = v[1]\/(v[0] + v[1])\nv.reset_index(inplace=True)","b3a77b05":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='Credit_Product', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","ae539576":"ax = plt.subplots(figsize=(8, 5))\nsns.histplot(data=train_df, x='Avg_Account_Balance');","d398fd90":"g = sns.catplot(x='Is_Lead', y='Avg_Account_Balance', kind='boxen', data=train_df);\ng.fig.set_size_inches(15,8)","5c8bcb9c":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Is_Active', hue='Is_Lead', data=train_df);","8b50816b":"# Response Rate from Is_Active\nv = train_df.groupby('Is_Active').Is_Lead.value_counts().unstack()\nv['Ratio'] = v[1]\/(v[0] + v[1])\nv.reset_index(inplace=True)","0abcf59d":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='Is_Active', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","0ffc65cd":"target = ['Is_Lead']\nnot_features = ['ID', 'Is_Lead']\ncols = list(train_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features]\nprint(features)","61a912d8":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_features = []\nnumerical_features = []\n\nfor i in features:\n    if train_df[i].dtype in numerics:\n        numerical_features.append(i)\n    else:\n        categorical_features.append(i)\n        \nprint(f'Numeric features: {numerical_features}')\nprint(f'Categorical features: {categorical_features}')","904c6295":"g = sns.pairplot(train_df[numerical_features + ['Is_Lead']], hue='Is_Lead')\ng.fig.set_size_inches(10,8)","cb480ae4":"train_df_cor_spear = train_df[numerical_features].corr(method='spearman')\nplt.figure(figsize=(10,8))\nsns.heatmap(train_df_cor_spear, square=True, cmap='coolwarm', annot=True);","87492b77":"NUM_SPLITS = 5\n\ntrain_df[\"kfold\"] = -1\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ny = train_df.Is_Lead.values\nkf = StratifiedKFold(n_splits=NUM_SPLITS)\nfor f, (t_, v_) in enumerate(kf.split(X=train_df, y=y)):\n    train_df.loc[v_, 'kfold'] = f\n    \ntrain_df.head()","417af461":"def label_enc(train_df, test_df, features):\n    lbl_enc = preprocessing.LabelEncoder()\n    full_data = pd.concat(\n        [train_df[features], test_df[features]],\n        axis=0\n    )\n    \n    for col in (features):\n        print(col)\n        if train_df[col].dtype == 'object':\n            lbl_enc.fit(full_data[col].values)\n            train_df[col] = lbl_enc.transform(train_df[col])\n            test_df[col] = lbl_enc.transform(test_df[col])\n            \n    return train_df, test_df","8a958b66":"mapping_dict = {'Yes': 1,\n                'No': 0,\n                'Unk': 0.5}","7cac3da0":"train_df['Credit_Product'] = train_df['Credit_Product'].map(mapping_dict)\ntest_df['Credit_Product'] = test_df['Credit_Product'].map(mapping_dict)\n\ntrain_df['Is_Active'] = train_df['Is_Active'].map(mapping_dict)\ntest_df['Is_Active'] = test_df['Is_Active'].map(mapping_dict)","04711d56":"train_df.head()","aeef8ebf":"target = ['Is_Lead']\nnot_features = ['ID', 'Is_Lead', 'kfold']\ncols = list(train_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features]\nprint(features)","0290154c":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_features = []\nnumerical_features = []\n\nfor i in features:\n    if train_df[i].dtype in numerics:\n        numerical_features.append(i)\n    else:\n        categorical_features.append(i)\n        \nprint(f'Numeric features: {numerical_features}')\nprint(f'Categorical features: {categorical_features}')","9bea9977":"train_df[numerical_features] = train_df[numerical_features].astype('float64')\ntest_df[numerical_features] = test_df[numerical_features].astype('float64')","ef932012":"train_df[target] = train_df[target].astype('float64')","44410c22":"cutoff = 10\nlow_cardinal_columns = []\nhigh_cardinal_columns = []\n\nfor i in categorical_features:\n    if train_df[i].nunique() > cutoff:\n        high_cardinal_columns.append(i)\n    else:\n        low_cardinal_columns.append(i)\n        \nprint(f'High Cardinality columns: {high_cardinal_columns}')\nprint(f'Low Cardinality columns: {low_cardinal_columns}')","28145dcf":"if len(low_cardinal_columns) > 0:\n    train_df, test_df = label_enc(train_df, test_df, low_cardinal_columns)","719967c5":"if len(high_cardinal_columns) > 0:\n    train_df, test_df = label_enc(train_df, test_df, high_cardinal_columns)","f912f4b3":"kmeans = KMeans(n_clusters=3, random_state=42, n_jobs=-1)\nkmeans.fit(train_df[features])\n\ntrain_df['cluster'] = kmeans.predict(train_df[features])\ntest_df['cluster'] = kmeans.predict(test_df[features])","655d24a7":"cols = list(train_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features+target]","4314fb92":"class UnivariateFeatureSelction:\n    def __init__(self, n_features, problem_type, scoring, return_cols=True):\n        \"\"\"\n        Custom univariate feature selection wrapper on\n        different univariate feature selection models from\n        scikit-learn.\n        :param n_features: SelectPercentile if float else SelectKBest\n        :param problem_type: classification or regression\n        :param scoring: scoring function, string\n        \"\"\"\n        self.n_features = n_features\n        \n        if problem_type == \"classification\":\n            valid_scoring = {\n                \"f_classif\": f_classif,\n                \"chi2\": chi2,\n                \"mutual_info_classif\": mutual_info_classif\n            }\n        else:\n            valid_scoring = {\n                \"f_regression\": f_regression,\n                \"mutual_info_regression\": mutual_info_regression\n            }\n        if scoring not in valid_scoring:\n            raise Exception(\"Invalid scoring function\")\n            \n        if isinstance(n_features, int):\n            self.selection = SelectKBest(\n                valid_scoring[scoring],\n                k=n_features\n            )\n        elif isinstance(n_features, float):\n            self.selection = SelectPercentile(\n                valid_scoring[scoring],\n                percentile=int(n_features * 100)\n            )\n        else:\n            raise Exception(\"Invalid type of feature\")\n    \n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n    \n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n    \n    def return_cols(self, X):\n        if isinstance(self.n_features, int):\n            mask = SelectKBest.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n                    \n        elif isinstance(self.n_features, float):\n            mask = SelectPercentile.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n        else:\n            raise Exception(\"Invalid type of feature\")\n        \n        return selected_features","60495782":"ufs = UnivariateFeatureSelction(\n    n_features=1.0,\n    problem_type=\"classification\",\n    scoring=\"f_classif\"\n)\n\nufs.fit(train_df[features], train_df[target].values.ravel())\nselected_features = ufs.return_cols(train_df[features])","4953a4f6":"print(f'{len(selected_features)} Features Selected')\nprint(selected_features)","00a7d3a3":"def get_models():\n    models = dict()\n    models['gauss'] = GaussianNB()\n    models['QDA'] = QuadraticDiscriminantAnalysis()\n    models['lr'] = LogisticRegression(solver='liblinear')\n    models['rf'] = RandomForestClassifier(class_weight='balanced_subsample',\n                                          random_state=42)\n    models['lgbm'] = LGBMClassifier(metric='binary_logloss',\n                                    objective='binary',\n                                    reg_alpha=2.945525898790487,\n                                    max_depth=13,\n                                    num_leaves=34,\n                                    seed=42,\n                                    learning_rate=0.0037601596530868493,\n                                    n_estimators=1913)\n    models['BalBag'] = BalancedBaggingClassifier()\n    models['BalRF'] = BalancedRandomForestClassifier()\n    \n    return models\n\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv,\n                             n_jobs=-1, error_score='raise')\n    return scores","76fc2532":"%%time\n\nX = train_df[selected_features].values\ny = train_df[target].values\n\nmodels = get_models()\nresults = []\nnames = []\n\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print(f'{name} : {round(np.mean(scores),6)} ({round(np.std(scores),3)})')","b3d68bbf":"ax = plt.subplots(figsize=(12, 6))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","cf864ce4":"mean_scores = []\nfor score in results:\n    mean_scores.append(round(np.mean(score),3))\nmin_index = mean_scores.index(max(mean_scores))\nmodel_name = names[min_index]","e7c570b6":"print(f'Best Score: {mean_scores[min_index]}')\nprint(f'Best Model: {model_name}')","66a3fa85":"%%time\n\nmodels = get_models()\nclf = models[model_name]\nX = train_df[selected_features]\ny = train_df[target]\nclf.fit(X, y)\n\npreds = clf.predict_proba(test_df[selected_features])\nsub = pd.DataFrame()\nsub['ID'] = test_df['ID']\nsub['Is_Lead'] = preds[:, 1]","e4988390":"sub.head()","d2adc33b":"sub.to_csv('Best_single_Model.csv', index=False)","1edc990a":"test_pred_all = None\n\nfor i in tqdm(range(NUM_SPLITS)):\n    print('#'*50)\n    print(f'{\"*\"*21} FOLD {i+1} {\"*\"*21}')\n    \n    train = train_df[train_df['kfold'] != i]\n    valid = train_df[train_df['kfold'] == i]\n    test = test_df\n    \n    clf = LGBMClassifier(metric='binary_logloss',\n                         objective='binary',\n                         reg_alpha=2.945525898790487,\n                         max_depth=13,\n                         num_leaves=34,\n                         seed=42,\n                         learning_rate=0.0037601596530868493,\n                         n_estimators=20000)\n    clf.fit(train[selected_features].values,train[target].values,\n           eval_set=(valid[selected_features].values,valid[target].values),\n           eval_metric='binary_logloss',\n           early_stopping_rounds=500,\n           verbose=1000)\n    \n    pred = clf.predict_proba(valid[selected_features])[:, 1]\n    roc = roc_auc_score(valid[target], pred)\n\n    test_pred = clf.predict_proba(test[selected_features])[:, 1]\n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n    \n    print(f'ROC: {roc}')\n    print('#'*50)\n    \ntest_pred_all \/= NUM_SPLITS","bb6e160e":"sub_2 = pd.DataFrame()\nsub_2['ID'] = test_df['ID']\nsub_2['Is_Lead'] = test_pred_all","0ac5a9eb":"sub_2.head()","c1e6ba42":"sub_2.to_csv('LGBM_5fold_Ensemble.csv', index=False)","3ea480c7":"* Customer acquired through X3 channel are most likely to respond positively and X1 are least likely.\n\n## 9. Vintage\nVintage for the Customer (In Months)","54adfe53":"We can get a naive idea about the type of variables form the definition itself and looking at the data makes it clearer.","45bc37c7":"We can see that there are some Null values in some columns. Which is not good for data ingestion into any model, so let's see the Null situation upfront:-  \n## 1. Null Values","1fd3c7ad":"## 7. Occupation\nOccupation Type for the customer","10004a51":"* Customers having a higher Vintage on an average are more likely to respond positively.\n\n## 10. Credit Product\nIf the Customer has any active credit product (Home loan, Personal loan, Credit Card etc.)","bf228815":"The situation is almost similar in both train set and test set; Around 12% of the values from 'Credit_Product' column is missing. And looking at the feature definition of the missing column we can assume that the NaN values in credit product means we do not know is the user has\/does not have any active credit product. So, we can replace Nan with 'Unknown' and create another binary column to track the same:-","6816accb":"* People already having some kind of credit product are way more likely to respond positively to offer as compared to non-credit users.\n\n## 11. Avg Account Balance\nAverage Account Balance for the Customer in last 12 Months","90258759":"## 13. Numerical Feature Interactions","a9ed1f2e":"* Positively responding customers have a slightly higher average account balance on an average as compared to negatively responding customers.\n\n## 12. Is Active\nIf the Customer is Active in last 3 Months","8040ed90":"# Things That Did Not Work\nBelow are some stuffs I tried but did not work out in the end or did not give any significant boost as comapred to the model above:-\n* Tabnet\n* Denoising Auto Encoders(DAE)\n* Semi Supervised Learning: Pseudo Labling\n* Entity Embeddings\n* Dense NN\n* Unsupervised Learning\n\n**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** \ud83d\ude0a","c18b524a":"Let's see what columns we have in the training data.","e8b011c8":"Okay, so it is an imbalanced set. We have to keep that in mind while modelling and choosing hyper-parameters later.  \n## 3. Feature Value Counts\nLet's see how manu unique values are there in each feature.","2262d482":"# Why this Competition?\nThis competition provides a unique opportunity for Data Science beginners to participate in a Hackathon style challenge for Data Science and compete for potential opportunities in many reputed companies.  \nIt also provides the unique opportunities for beginners to get their hands dirty and indulge is practical application of ML and do one of the basic tasks machine learning algorithms are capable of doing:- **Classification**.\n\n**If you find this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** \ud83d\ude0a\n\n# Problem Statement\n* Our client, Happy Customer Bank is a mid-sized private bank that deals in all kinds of banking products.\n* They also cross-sell products to its existing customers and to do so they use different kinds of communication like tele-calling, e-mails, recommendations on net banking, mobile banking, etc.\n* In this case, they want to cross sell its credit cards to its existing customers.\n* The bank has identified a set of customers that are eligible for taking these credit cards.\n\n**Given historic data and other data of the customers we have to identify which customers are most likely to accept our cross-sell offer.**\n\n## Data Description:-\nWe have the following information regarding the customer:\n* Customer details (gender, age, region etc.)\n* Details of his\/her relationship with the bank (Channel_Code,Vintage, 'Avg_Asset_Value etc.)\n\n## Expected Outcome:-\n* Build a model to predict whether the person will be interested in buying the Credit card offered by our client.\n* Grading Metric: **ROC_AUC_SCORE**\n\n## Problem Category:-\nFor the data and objective, it\u2019s evident that this is a **Binary Classification Problem** in the **Tabular Data** format.\n\nSo without further ado, let's now start with some basic imports to take us through this journey of Lead prediction:-","463d5ecb":"# KFolds Split\nBefore we move on to feature engineering, it is always a good idea to perform cross validation splits. In that way, we will not rix any data leakage and would be more certain of the validation set being aptly represenative of the real world unknown data.","5fd3d59b":"# Models Benchmarking\nFirst let's create a benchmarking function which finds the best single model for this dataset.","c4dd81bd":"* On an average, more aged people are likely to respond positively to our offer.\n* The average age of male in dataset is > average age of female.\n* If we split by gender, the average age of positively responding male is > average of positively responding Female.\n\n## 6. Region Code\nCode of the Region for the customers","29841f59":"From the column keys in problem statement we know the following information about each of the features:-\n\n Variable      | Definition         \n :-----------  |:------------\n ID | Unique Identifier for a row\n Gender | Gender of the Customer\n Age | Age of the Customer (in Years)\n Region_Code | Code of the Region for the customers\n Occupation | Occupation Type for the customer\n Channel_Code | Acquisition Channel Code for the Customer  (Encoded)\n Vintage | Vintage for the Customer (In Months)\n Credit_Product | If the Customer has any active credit product (Home loan, Personal loan, Credit Card etc.)\n Avg_Account_Balance | Average Account Balance for the Customer in last 12 Months\n Is_Active | If the Customer is Active in last 3 Months\n Is_Lead(Target) | 0 : Customer is not interested\n | 1 : Customer is interested","7afda011":"# Feature Encoding\n\nFirst let's convert all the categorical features to numbers. I have decided to use Label encoder after lot of experiments with other types of categorical encoders and decision is made based on CV ROC score.","777627e6":"# Solution-1\nPrediction using the best single model.","9893f0a8":"Through iterations it has been found that all features are important. Hence we are not doing any feature selection and thus the n_features parameter has value 1.0","c28c0db6":"* The customer base in training data are majorly Self-Employed or Salaried.\n* Entrepreneural customers are way more likely to respond positively to our Credit Card offer.\n* Salaried people are least likely to respond positively to our offer.\n\n## 8. Channel Code\nAcquisition Channel Code for the Customer  (Encoded)","06143d95":"* There are a mix of certain continuous, high cardinality categorical and low cardinality categorical features.\n\nNow let's look at each individual feature separately and undersatnd the data...\n## 4. Gender\nThis feature contains the Gender data of the customer","e2f8f05a":"Now that we have resolved all the null values, let's move on to EDA starting with class imabalance of the Dataset.\n## 2. Class Imbalance\nAs this is a classification problem, let's start from the population of each class in out training set.","0c96b1ce":"* There are more number of observations from Male customers as compared to Female customers in the training data.\n* According to the dataset, Male gender has better conversion ratio than Female gender.\n\n## 5. Age\nAge of the Customer (in Years)","57eecc0c":"# Feature Selection\n\nWe need to select only the important features for better performance of the model. As unnecessary in best case scenario will not add to any productive calculation of the algorithm or in worst case scenario 'confuse' the model.\n\nTo do the same let's create a wrapper class that has all the built in statistical tests required to perform feature selection and takes some basic inputs from user and spits out the required features.","2a8e31f2":"* Active Customers(in last 3 months) are slightly more likely to respond positively as compared to inactive customers.","ef07b372":"# Clustering\nWe can cluster the customers to some groups using the given features and use them as one of the features to identify what cluster the customer belongs to.","0be4c456":"# Final Solution\nCreating a custom ensemble using the best single model from above and training on folds while tracking the OOF scores.","f65dffeb":"# EDA\nLet's have a basic look around the data we have at hand first"}}