{"cell_type":{"ba57543a":"code","14839a6c":"code","3a0b18da":"code","c2f9c2f5":"code","c16abcb6":"code","7104eb07":"code","5cf77181":"code","16a3cb61":"code","a18bda94":"code","39d159fa":"code","f4cb2fd4":"code","0ac585ec":"code","eac78a61":"code","1b848fad":"code","091b522e":"code","47fb052f":"code","7ef898d4":"code","1b3a22aa":"code","bc4c8b26":"code","cdaa74d7":"code","e6bf6547":"code","4ecaab39":"code","b30e06df":"code","67e45e60":"code","6e5e4c4f":"code","487a4902":"code","a24f0c98":"code","e64333a0":"code","af37d79b":"code","4fc95fa2":"code","6e5182f9":"code","dcc0fa4c":"code","0e7dc67a":"code","fb98a7c6":"code","4de1f5da":"markdown","6339752f":"markdown","88e024fe":"markdown","cde8336c":"markdown","96e484ab":"markdown","ec5a6988":"markdown","efee3f99":"markdown","e8f264fb":"markdown","e49ffcc0":"markdown","f679b275":"markdown","d13001dd":"markdown","c2079157":"markdown","c77e8ab4":"markdown","497e948e":"markdown","e8775487":"markdown","1a034122":"markdown","fb4f5db0":"markdown","13d12519":"markdown","4b4b11bc":"markdown","a1f6336b":"markdown","e5d4ff77":"markdown","42bd140e":"markdown","6a4c01c5":"markdown","deec8854":"markdown","e6f920c5":"markdown","53cad8b1":"markdown","303cfb52":"markdown","068f70a4":"markdown","0444bca5":"markdown","588c43a4":"markdown","de474af7":"markdown","43a17411":"markdown","7199069b":"markdown"},"source":{"ba57543a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14839a6c":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error as mse\n\nfrom datetime import datetime\n\nimport re\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3a0b18da":"train_data = pd.read_csv(\"\/kaggle\/input\/assignment1\/train.csv\", dtype={'Year':'object', 'Week-Ending Date':'object'})\ntest_data = pd.read_csv(\"\/kaggle\/input\/assignment1\/test.csv\")\nuntouched_test_data = pd.read_csv(\"\/kaggle\/input\/assignment1\/test.csv\")","c2f9c2f5":"print(train_data.info())\nprint(train_data.describe())\nprint(train_data.shape)\nprint(train_data.columns.tolist())\ntrain_data.head()","c16abcb6":"print(test_data.info())\nprint(test_data.describe())\nprint(test_data.shape)\nprint(test_data.columns.tolist())\ntest_data.head()","7104eb07":"print(train_data.isnull().sum())","5cf77181":"msno.bar(train_data)\nmsno.matrix(train_data)","16a3cb61":"print(train_data['Footnote'].unique())\nprint(train_data['Group'].unique())\nprint(test_data['Group'].unique())","a18bda94":"# drop columns in question\ntrain_data = train_data.drop(['Month', 'Footnote'], axis=1)\ntest_data = test_data.drop('Month', axis=1)\n\n# confirm columns got dropped\nprint(train_data.columns.tolist())","39d159fa":"# Drop the rows that are not grouped by week\ntrain_data = train_data.drop(train_data[(train_data['Group']=='By Month')].index)\ntrain_data = train_data.drop(train_data[(train_data['Group']=='By Year')].index)\n\n# We must do the same to the test data\nprint(test_data['Group'].unique())\n# Test Data only has 'By Week', so we can leave it as is\n\n# Drop the column\ntrain_data = train_data.drop('Group', axis=1)\ntest_data = test_data.drop('Group', axis=1)\n\n# Confirm changes were made\nprint(train_data.shape)","f4cb2fd4":"print(train_data.isnull().sum())","0ac585ec":"# Drop all rows where 'COVID-19 Deaths' is missing\ntrain_data = train_data.drop(train_data[train_data['COVID-19 Deaths'].isna()].index)\n\n# Confirm it worked\nprint(train_data.isnull().sum())","eac78a61":"print(train_data['Data As Of'].unique())","1b848fad":"train_data = train_data.drop('Data As Of', axis=1)\ntest_data = test_data.drop('Data As Of', axis=1)\n\nprint(test_data.columns.tolist())","091b522e":"# Before we can compare these features, we should go ahead and convert them to DateTime Objects.\ntrain_data['Start Date'] = pd.to_datetime(train_data['Start Date'], format=\"%m\/%d\/%Y\")\ntrain_data['End Date'] = pd.to_datetime(train_data['End Date'], format=\"%Y-%m-%d\")\ntrain_data['Week-Ending Date'] = pd.to_datetime(train_data['Week-Ending Date'], format=\"%m\/%d\/%Y\")\n\ntest_data['Start Date'] = pd.to_datetime(test_data['Start Date'], format=\"%m\/%d\/%Y\")\ntest_data['End Date'] = pd.to_datetime(test_data['End Date'], format=\"%Y-%m-%d\")\ntest_data['Week-Ending Date'] = pd.to_datetime(test_data['Week-Ending Date'], format=\"%m\/%d\/%Y\")\n\ncomparison = train_data[train_data['End Date'] != train_data['Week-Ending Date']]\nprint(\"Number of rows that have differing End Dates and Week-Ending Dates\", comparison.shape)","47fb052f":"train_data = train_data.drop('Week-Ending Date', axis=1)\ntest_data = test_data.drop('Week-Ending Date', axis=1)","7ef898d4":"print(train_data.isnull().sum())","1b3a22aa":"train_data = train_data.drop('Year', axis=1)\ntest_data = test_data.drop('Year', axis=1)\n\ntrain_data = train_data.drop('End Date', axis=1)\ntest_data = test_data.drop('End Date', axis=1)\n\ntrain_data = train_data.drop('id', axis=1)\ntest_data = test_data.drop('id', axis=1)\n\nprint(train_data.isnull().sum())","bc4c8b26":"print(train_data['HHS Region'].unique())\nprint(train_data['Race and Hispanic Origin Group'].unique())\nprint(train_data['Age Group'].unique())\nprint(train_data['MMWR Week'].unique())\n\nprint(test_data['HHS Region'].unique())\nprint(test_data['Race and Hispanic Origin Group'].unique())\nprint(test_data['Age Group'].unique())\nprint(train_data['MMWR Week'].unique())","cdaa74d7":"train_data = train_data.drop(train_data[train_data['HHS Region'] != 'United States'].index)\nprint(train_data['HHS Region'].unique())\n\ntrain_data = train_data.drop('HHS Region', axis=1)\ntest_data = test_data.drop('HHS Region', axis=1)\n\n\nprint(train_data.isnull().sum())\nprint(train_data.shape)","e6bf6547":"# Plot the data by Race and Age using seaborn\n\nplot = sns.relplot(x=\"Start Date\", y=\"COVID-19 Deaths\", \n            data=train_data,\n            col=\"Age Group\",\n            col_wrap=3,\n            hue=\"Race and Hispanic Origin Group\",\n            kind=\"line\",\n            facet_kws={'sharey': False, 'sharex': True}\n            )\n\nplot.set_xticklabels(rotation=45)\n\nplt.show()","4ecaab39":"def weeksSinceCovid(currWeek):\n    diffDays = currWeek - datetime(2019, 12, 29)\n    return (diffDays.dt.days \/ 7).astype(int)","b30e06df":"def getMonth(currWeek):\n    return currWeek.dt.month.astype(int)\n","67e45e60":"train_data['Weeks Passed'] = weeksSinceCovid(train_data['Start Date'])\ntest_data['Weeks Passed'] = weeksSinceCovid(test_data['Start Date'])\n\ntrain_data['Month'] = getMonth(train_data['Start Date'])\ntest_data['Month'] = getMonth(test_data['Start Date'])\n\ntrain_data.head(100)","6e5e4c4f":"def parseFirstInt(ageGroup):\n    ret = re.search(r'\\d+', str(ageGroup)).group()\n    return int(ret)\n\nprint(parseFirstInt(train_data['Age Group'][100]))","487a4902":"le = LabelEncoder()\n\ntrain_data['Ages'] = train_data['Age Group'].apply(parseFirstInt)\ntest_data['Ages'] = test_data['Age Group'].apply(parseFirstInt)\n\ntrain_data['Ages'] = le.fit_transform(train_data['Age Group'])\ntest_data['Ages'] = le.fit_transform(test_data['Age Group'])\n\ntrain_data = train_data.drop('Age Group', axis=1)\ntest_data = test_data.drop('Age Group', axis=1)\n\ntrain_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)\n\n\n#train_WeekOfYear = pd.get_dummies(train_data['MMWR Week'], prefix='MMWR Week')\n#test_WeekOfYear = pd.get_dummies(test_data['MMWR Week'], prefix='MMWR Week')\n\n#train_data = train_data.drop('MMWR Week', axis=1)\n#test_data = test_data.drop('MMWR Week', axis=1)\n\n#train_data = pd.concat([train_data, train_WeekOfYear], axis=1)\n#test_data = pd.concat([test_data, test_WeekOfYear], axis=1)","a24f0c98":"train_data.head(100)","e64333a0":"X = train_data.drop(['COVID-19 Deaths','Start Date'], axis=1)\ny = train_data[['COVID-19 Deaths']]\n\n# Will be used to make final predictions\nX_final = test_data.drop(['Start Date'], axis=1)\n\n# Split the data by time series for testing\ntime_kfold = TimeSeriesSplit(n_splits=60)","af37d79b":"fold_metrics = []\n\nfor train_index, test_index in time_kfold.split(train_data):\n    cv_train_X, cv_train_y, cv_test_X,  cv_test_y = X.iloc[train_index], y.iloc[train_index], X.iloc[test_index], y.iloc[test_index]\n    # Create and Fit Model\n    lin = LinearRegression()\n    lin.fit(cv_train_X, cv_train_y)\n    # Make Predictions and Store the RSME of this iteration\n    predictions = lin.predict(cv_test_X)\n    score = mse(cv_test_y, predictions) ** (1\/2)\n    fold_metrics.append(score)\n    \n# Convert the list of rsmes into a dataframe to print statistics\nprint(fold_metrics)\nframe = pd.DataFrame(fold_metrics)\nprint(np.mean(fold_metrics))\nframe.describe()","4fc95fa2":"fold_metrics = []\n\nfor train_index, test_index in time_kfold.split(train_data):\n    cv_train_X, cv_train_y, cv_test_X,  cv_test_y = X.iloc[train_index], y.iloc[train_index], X.iloc[test_index], y.iloc[test_index]\n    rid = Ridge(alpha=0.5, random_state=55, normalize=True)\n    rid.fit(cv_train_X, cv_train_y)\n    predictions = rid.predict(cv_test_X)\n    score = mse(cv_test_y, predictions) ** (1\/2)\n    fold_metrics.append(score)\n    \nprint(fold_metrics)\nframe = pd.DataFrame(fold_metrics)\nprint(np.mean(fold_metrics))\nframe.describe()","6e5182f9":"fold_metrics = []\n\nfor train_index, test_index in time_kfold.split(train_data):\n    cv_train_X, cv_train_y, cv_test_X,  cv_test_y = X.iloc[train_index], y.iloc[train_index], X.iloc[test_index], y.iloc[test_index]\n    las = Lasso(alpha=0.4, random_state=42, normalize=True)\n    las.fit(cv_train_X, cv_train_y)\n    predictions = las.predict(cv_test_X)\n    score = mse(cv_test_y, predictions) ** (1\/2)\n    fold_metrics.append(score)\n    \nprint(fold_metrics)\nframe = pd.DataFrame(fold_metrics)\nprint(np.mean(fold_metrics))\nframe.describe()","dcc0fa4c":"fold_metrics = []\n\nfor train_index, test_index in time_kfold.split(train_data):\n    cv_train_X, cv_train_y, cv_test_X,  cv_test_y = X.iloc[train_index], y.iloc[train_index], X.iloc[test_index], y.iloc[test_index]\n    ela = ElasticNet(alpha=0.5, random_state=4, normalize=True)\n    ela.fit(cv_train_X, cv_train_y)\n    predictions = ela.predict(cv_test_X)\n    score = mse(cv_test_y, predictions) ** (1\/2)\n    fold_metrics.append(score)\n    \nprint(fold_metrics)\nframe = pd.DataFrame(fold_metrics)\nprint(np.mean(fold_metrics))\nframe.describe()","0e7dc67a":"predictions = las.predict(X_final)\n\noutput = pd.DataFrame({'id': untouched_test_data.id, 'COVID-19 Deaths': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","fb98a7c6":"# Final Submission\nprint(output.to_string())","4de1f5da":"**Imports**","6339752f":"**Elastic Net**","88e024fe":"A deeper look on the values in the remaining columns.","cde8336c":"**Cleaning the Data and Checking for Missing Data**","96e484ab":"Before we proceed with handling the Total Deaths column missing values, let's take a closer look at some of the the columns.","ec5a6988":"The Test Data only has one value for 'HHS Region', which is 'United States'. \n\nThe Train Data has the overall United States region as well as smaller regions which divide the United States into 10 equal parts. Because the Test Data does not have the regions seperated we can drop all the regions that aren't 'United States' from the train data, and then drop the column.","efee3f99":"**Loading the Data**","e8f264fb":"After testing the above 4 models, Lasso was the one that provided the best results.","e49ffcc0":"**Ridge Regression Model**","f679b275":"**Data Transformations and Feature Engineering**","d13001dd":"After the above observations, it is clear that the \"Week-Ending Date\" column is redundant. We will drop it.","c2079157":"First, let's get an initial look and understanding of the training DataFrame.","c77e8ab4":"This section consists of a few plots to visually observe the data and try to notice any trends.","497e948e":"# CAP 4611 - 2021 Fall - Assignment 1\n## COVID-19 Deaths","e8775487":"As seen above, Month and Footnote have the greatest number of missing values. Month has missing values for 82% of the rows, and Footnote doesn't seem particularly useful for what we're trying to accomplish. Due to these reasons, these columns will be dropped.","1a034122":"**Generating Submission Data**","fb4f5db0":"**Lasso Regression**","13d12519":"From the above graphs, there do not appear to be any outliers, but COVID Deaths appear to be cyclical.","4b4b11bc":"Some rows are missing are COVID-19 Deaths, ","a1f6336b":"We encode Age Group by order using label encoding, and then use get_dummies to one-hot encode the Race and Hispanic Origin","e5d4ff77":"**Ordinary Least Squares**","42bd140e":"A quick look at the Test Data","6a4c01c5":"This function serves to encode the age groups by increasing ages by grabbing the first integer","deec8854":"The 'Data As Of' column only has one unique value, therefore it is redundant to keep in the DataFrame. We will drop this column from both DataFrames now.","e6f920c5":"**Plotting the Data and Checking Outliers**","53cad8b1":"Using Missingno for a better visualization of the missing data.","303cfb52":"Because, COVID-19 Deaths is the target value, it doesn't make much sense to fill it with filler data. Dropping every row where this data is missing might be the optimal move.","068f70a4":"Upon inspecting the 'Group' column, I noticed that there are three unique values for that column in the training data, 'By Week', 'By Month', and 'By Year'. \n\nIt appears that working with the data on a weekly basis is preferable as the test data only has 'By Week' entries. To avoid duplicate data along with simplifying our initial test DataFrame, we will remove all rows that aren't grouped 'By Week'.\n\nThe justification for this is that I don't really see how including data grouped 'By Month' or 'By Year' can be beneficial in any way, on the contrary, I beieve it will only confuse our model.\n\nWe will remove said rows for now to try and keep the model simplified.","0444bca5":"'End Date' and 'Week-Ending Date' seem to be duplicated columns for the most part, but I will confirm this below just to make sure","588c43a4":"Create and Test the following models on the kfold split. ","de474af7":"The 'Year' column is redundant, as the 'Start Date' and 'End Date' already contain this information. It doesn't make sense to keep both the Start and End date, so we will only keep one of the two. The id column can go as well, as it doesn't offer much to our model.","43a17411":"As the pandemic progresses, the number of COVID-19 deaths seems to increase, it might be wise to engineer a feature that holds what week into the pandemic we are at.","7199069b":"**Exploratory Data Analysis**"}}