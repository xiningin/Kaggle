{"cell_type":{"67e3ecc3":"code","cf1375b5":"code","a70b1eb1":"code","4fbf9576":"code","16ff324c":"code","eb28f6a1":"code","090ffd0c":"code","1919c5de":"code","08b46537":"code","34ec40fe":"code","cf6b92ba":"code","6f94456c":"code","e50e05de":"code","12286586":"code","97c1c2ce":"code","4357accd":"code","301fd4eb":"code","24934783":"code","3f38a585":"code","d2703ab0":"markdown","c5e46781":"markdown","36c1d3b5":"markdown","6cb7e889":"markdown","ab39ea63":"markdown","6928cf02":"markdown","a87af1a5":"markdown","9b4625c6":"markdown","4c8155e2":"markdown"},"source":{"67e3ecc3":"import os\nimport datetime\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport scipy.signal\n\nimport matplotlib\nimport matplotlib.pyplot as plt\npd.options.display.max_columns = None    # disp all columns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\n\n# from lightgbm import LGBMRegressor\nimport lightgbm as lgb","cf1375b5":"### Kaggle or Local-PC ###\nKAGGLE = True       # <==== SET ============\n\nif KAGGLE:\n    DIR = '..\/input\/predict-volcanic-eruptions-ingv-oe'\nelse:              # local PC\n    DIR = '.\/predict-volcanic-eruptions-ingv-oe\/'","a70b1eb1":"train = pd.read_csv(os.path.join(DIR, 'train.csv'))\ntest = pd.read_csv(os.path.join(DIR, 'sample_submission.csv'))\n\ntrain","4fbf9576":"# Convert 'time_to_eruption'to hours:minutes:seconds (Just for reference)\ntrain['h:m:s'] = (train['time_to_eruption']\n                  .apply(lambda x:datetime.timedelta(seconds = x\/100)))\ntrain","16ff324c":"# plot utility function\ndef plot(ax, x, y, xlabel=None, ylabel=None, legend=None):\n    ax.plot(x, y, label = legend)\n    if xlabel != None:\n        ax.set_xlabel(xlabel)\n    if ylabel != None:\n        ax.set_ylabel(ylabel)\n    if legend != None:\n        ax.legend()\n    ax.grid(True)","eb28f6a1":"# Serect sample segment\nsample_df = (train.sort_values('time_to_eruption')\n             .reset_index()\n             .rename(columns={'index': 'train_id'}))\nsample_df = sample_df[sample_df.index % (len(train) \/\/ 5) == 5].reset_index(drop = True)\nsample_ids = sample_df['segment_id'].values\nsample_df","090ffd0c":"sensor = 4      #### 1 \uff5e 10\n\nfig, ax = plt.subplots(len(sample_ids), 1, figsize = (12, len(sample_ids)*2))\nfor i, segment_id in enumerate(sample_ids):\n    segment_df = pd.read_csv(os.path.join(DIR, f'train\/{segment_id}.csv')).fillna(0)\n    ax[i].plot(range(len(segment_df)), segment_df[f'sensor_{sensor}'])\n    ax[i].set_title(f'segment_id : {segment_id},  sensor : {sensor}')\n\nfig.tight_layout()","1919c5de":"fs = 100                # sampling frequency \nN = len(segment_df)     # data size\nn = 256                 # FFT segment size\n\nfig, ax = plt.subplots(len(sample_ids), 1, figsize = (12, len(sample_ids)*2))\nfor i, segment_id in enumerate(sample_ids):\n    segment_df = pd.read_csv(os.path.join(DIR, f'train\/{segment_id}.csv')).fillna(0)\n    \n    x = segment_df[f'sensor_{sensor}'][:N]\n    f, t, Z = scipy.signal.stft(x, fs = fs, window = 'hann', nperseg = n)\n    Z = np.abs(Z)\n\n    ax[i].pcolormesh(t, f, Z, vmin = 0, vmax = Z.mean()*10)\n    ax[i].set_ylim(0, 20)\n    ax[i].set_ylabel('Frequency [Hz]'); plt.xlabel('Time [s]')\n    ax[i].set_title(f'segment_id : {segment_id},  sensor : {sensor}')\nfig.tight_layout()","08b46537":"# STFT(Short Time Fourier Transform) Specifications\nfs = 100                # sampling frequency \nN = len(segment_df)     # data size\nn = 256                 # FFT segment size\nmax_f = 20              # \uff5e20Hz\n\ndelta_f = fs \/ n        # 0.39Hz\ndelta_t = n \/ fs \/ 2    # 1.28s","34ec40fe":"def make_features(tgt):\n    tgt_df = train if tgt == 'train' else test\n    feature_set = []\n    for segment_id in tqdm(tgt_df['segment_id']):\n        segment_df = pd.read_csv(os.path.join(DIR,f'{tgt}\/{segment_id}.csv'))\n        segment = [segment_id]\n        for sensor in segment_df.columns:\n            x = segment_df[sensor][:N]\n            if x.isna().sum() > 1000:     ##########\n                segment += ([np.NaN] * 10)\n                continue\n            f, t, Z = scipy.signal.stft(x.fillna(0), fs = fs, window = 'hann', nperseg = n)\n            f = f[:round(max_f\/delta_f)+1]\n            Z = np.abs(Z[:round(max_f\/delta_f)+1]).T    # \uff5emax_f, row:time,col:freq\n\n            th = Z.mean() * 1     ##########\n            Z_pow = Z.copy()\n            Z_pow[Z < th] = 0\n            Z_num = Z_pow.copy()\n            Z_num[Z >= th] = 1\n\n            Z_pow_sum = Z_pow.sum(axis = 0)\n            Z_num_sum = Z_num.sum(axis = 0)\n\n            A_pow = Z_pow_sum[round(10\/delta_f):].sum()\n            A_num = Z_num_sum[round(10\/delta_f):].sum()\n            BH_pow = Z_pow_sum[round(5\/delta_f):round(8\/delta_f)].sum()\n            BH_num = Z_num_sum[round(5\/delta_f):round(8\/delta_f)].sum()\n            BL_pow = Z_pow_sum[round(1.5\/delta_f):round(2.5\/delta_f)].sum()\n            BL_num = Z_num_sum[round(1.5\/delta_f):round(2.5\/delta_f)].sum()\n            C_pow = Z_pow_sum[round(0.6\/delta_f):round(1.2\/delta_f)].sum()\n            C_num = Z_num_sum[round(0.6\/delta_f):round(1.2\/delta_f)].sum()\n            D_pow = Z_pow_sum[round(2\/delta_f):round(4\/delta_f)].sum()\n            D_num = Z_num_sum[round(2\/delta_f):round(4\/delta_f)].sum()\n            segment += [A_pow, A_num, BH_pow, BH_num, BL_pow, BL_num, C_pow, C_num, D_pow, D_num]\n\n        feature_set.append(segment)\n\n    cols = ['segment_id']\n    for i in range(10):\n        for j in ['A_pow', 'A_num','BH_pow', 'BH_num','BL_pow', 'BL_num','C_pow', 'C_num','D_pow', 'D_num']:\n            cols += [f's{i+1}_{j}']\n    feature_df = pd.DataFrame(feature_set, columns = cols)\n    feature_df['segment_id'] = feature_df['segment_id'].astype('int')\n    return feature_df","cf6b92ba":"feature_df = make_features('train')\ntrain_set = pd.merge(train, feature_df, on = 'segment_id')\ntrain_set","6f94456c":"fig, ax = plt.subplots(2, 5, figsize = (12, 6))\nx = train_set['time_to_eruption']\nfor i,type in enumerate(['A_pow','A_num','BH_pow','BH_num','BL_pow','BL_num','C_pow','C_num','D_pow','D_num']):\n    y = np.zeros(len(x))\n    for j in range(10):\n        y += train_set[f's{j+1}_{type}']\n    y \/= 10\n    x1 = np.polyfit(x, y.fillna(y.mean()), 2)\n    y1 = np.poly1d(x1)(x)\n    ax[i%2, i\/\/2].plot(x, y,'.')\n    ax[i%2, i\/\/2].plot(x, y1,'.')\n    ax[i%2, i\/\/2].set_ylim(0,)\n    ax[i%2, i\/\/2].set_title(type)\nfig.tight_layout()","e50e05de":"df = train_set.drop(['segment_id', 'time_to_eruption','h:m:s'], axis=1)\ny = train_set['time_to_eruption']\n\nX_train, X_val, y_train, y_val = train_test_split(df, y,\n                                                  random_state = 42,\n                                                  test_size = 0.2,\n                                                  shuffle = True)\n\nfeatures = X_train.columns.tolist()\ncat_features = {}","12286586":"# lgb = LGBMRegressor(random_state = 42,\n#                     max_depth = 7,\n#                     n_estimators = 250,       ######### \n#                     learning_rate = 0.05)\n# lgb.fit(X_train, y_train)\n# preds = lgb.predict(X_val)\n\n# print('RMSE: ', np.sqrt(mse(y_val, preds)))","97c1c2ce":"def do_lgb(X_train, y_train, X_val, y_val):\n    params = {'objective': 'rmse',\n              'metric': 'rmse',\n              'max_depth':14,\n              'min_data_in_leaf':5,         # = min_child_samples\n              'num_leaves': 2**7 - 1,\n              'learning_rate': 0.05,\n              'feature_fraction': 0.7,      # = colsample_bytree\n              'bagging_fraction': 0.5,      # = subsample\n              'bagging_freq': 5,\n              'lambda_l1':80,               # = reg_alpha\n              'num_iterations': 10000,      # = n_estimators\n              'seed': 42,\n              'verbose': 1\n             }\n\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\n    evals_result = {}\n    model = lgb.train(\n        params,\n        lgb_train,\n        valid_sets = (lgb_train, lgb_eval), \n        feature_name = features,\n        categorical_feature = cat_features,\n        verbose_eval = 100,\n        evals_result = evals_result,\n        early_stopping_rounds = 200)\n\n    return model","4357accd":"lgb_model = do_lgb(X_train, y_train, X_val, y_val)","301fd4eb":"feature_df = make_features('test')\ntest_set = pd.merge(test, feature_df, on = 'segment_id')\ntest_set","24934783":"# Predict test data\npreds = lgb_model.predict(test_set.drop(['segment_id', 'time_to_eruption'], axis=1))\ntest['time_to_eruption'] = preds\ntest[['segment_id','time_to_eruption']]","3f38a585":"test[['segment_id','time_to_eruption']].to_csv('submission.csv', index=False)","d2703ab0":"# Features","c5e46781":"**Version 3**\n * At the beginning, briefly add the mechanism of earthquake and tremor\n * Corrected the unit of `time_to_eruption` from millisecond to centisecond (Thanks Alex V B)\n * Changed LightGBM parameters (Thanks [Dave E](https:\/\/www.kaggle.com\/davidedwards1\/volcano-stft-data-optimisation))","36c1d3b5":"### Time-Frequency Domain (STFT)\nSTFT : Short Time Fourier Transform","6cb7e889":"# Modeling and Predicting","ab39ea63":"# INGV Volcanic : Basic_solution (STFT)\n**Volcanic earthquakes** have various characteristics depending on the volcano, and there are various classifications, but the typical classifications can be classified as follows.\n \n**Volcanic earthquake**\n * type A (10Hz or higher): Earthquake caused by destruction of rocks surrounding magma chambers and conduits\n * type BH (5-8Hz) : Earthquake caused by magma intruding into the conduit and destroying the conduit and rocks around the conduit\n * type BL (1.5-2.5Hz) : An earthquake around the conduit due to gas etc. ejecting from the crater prior to the explosive eruption and reducing the pressure inside the conduit.\n\n**Volcanic tremor**\n * type C (0.5-1.2Hz) : Vibration due to increase in gas pressure in the cavity along with BH\n * type D (2-4Hz) : Vibration due to gas ejection along with BL\n \n> I am a complete amateur about volcanoes. The jargon may be wrong, but please forgive me.","6928cf02":"# Predict test data","a87af1a5":"# Observe sample data","9b4625c6":"### Time Domain","4c8155e2":"# Data"}}