{"cell_type":{"127273f6":"code","dcd2ead6":"code","028495c5":"code","fd37e1a6":"code","ea61bf74":"code","57f02a33":"code","8cc11d70":"code","19487948":"code","55294641":"code","0fe15fe2":"code","36f2e6ff":"code","1081f7b2":"code","bd5b7ca2":"code","f7cbefdc":"code","2d59509f":"code","eeda51ed":"code","5cc9bd6a":"code","a7458a26":"code","ee0a4a15":"code","0a469156":"code","04737609":"markdown","3a1b3960":"markdown","ee39eb21":"markdown"},"source":{"127273f6":"import pandas as pd\n\n# Read csv file into dataframe\ndf = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\n\ndf.head()","dcd2ead6":"Y = df['DEATH_EVENT']\nX = df[['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n       'ejection_fraction', 'high_blood_pressure', 'platelets',\n       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']]\n\nX.head(5)","028495c5":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Heatmap to Invertigate Correlation in Data\nsns.set()\nfig, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(df.corr(), linewidths=.5, ax=ax, cmap='Blues')\nplt.show()","fd37e1a6":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify = Y, test_size=0.2, random_state=52)\n\nprint('Shape of X_train:', X_train.shape)\nprint('Shape of X_test:', X_test.shape)\nprint('Shape of Y_train:', Y_train.shape)\nprint('Shape of Y_test:', Y_test.shape)\n","ea61bf74":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\n\nDecisionTreeClassifier(class_weight=None, criterion=\"gini\", \n                       max_depth=None,max_features=None, max_leaf_nodes=None,\n                       min_impurity_split=1e-07, min_samples_leaf=1,\n                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n                       presort=False, random_state=None, splitter=\"best\")\n\nY_pred = dt.predict(X_test)\nY_pred","57f02a33":"from sklearn.metrics import roc_curve, auc\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test,Y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","8cc11d70":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\n\nmax_depths = np.linspace(1, 32, 32, endpoint=True)\ntrain_results = []\ntest_results = []\n\nfor max_depth in max_depths:\n   dt = DecisionTreeClassifier(max_depth=max_depth)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   # Add auc score to previous train results\n   train_results.append(roc_auc)\n   Y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, Y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   # Add auc score to previous test results\n   test_results.append(roc_auc)\n    \nline1, = plt.plot(max_depths, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(max_depths, test_results, 'r', label=\"Test AUC\")\n\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"Tree depth\")\nplt.show()","19487948":"from matplotlib.legend_handler import HandlerLine2D\n\nmin_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\ntrain_results = []\ntest_results = []\n\nfor min_samples_split in min_samples_splits:\n   dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds =    roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   Y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, Y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\n    \n\nline1, = plt.plot(min_samples_splits, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(min_samples_splits, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"min samples split\")\nplt.show()","55294641":"from matplotlib.legend_handler import HandlerLine2D\n\nmin_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\ntrain_results = []\ntest_results = []\n\nfor min_samples_leaf in min_samples_leafs:\n   dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   Y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, Y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\n\nline1, = plt.plot(min_samples_leafs, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(min_samples_leafs, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"min samples leaf\")\nplt.show()","0fe15fe2":"from matplotlib.legend_handler import HandlerLine2D\n\nmax_features = list(range(1, 12))\ntrain_results = []\ntest_results = []\n\nfor max_feature in max_features:\n   dt = DecisionTreeClassifier(max_features=max_feature)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   Y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, Y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\n\nline1, = plt.plot(max_features, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(max_features, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"max features\")\nplt.show()","36f2e6ff":"from sklearn import decomposition, datasets\nfrom sklearn import tree\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler","1081f7b2":"std_slc = StandardScaler()\n\npca = decomposition.PCA()\n\ndec_tree = tree.DecisionTreeClassifier()","bd5b7ca2":"pipe = Pipeline(steps=[('std_slc', std_slc),\n                           ('pca', pca),\n                           ('dec_tree', dec_tree)])","f7cbefdc":"n_components = list(range(1,X.shape[1]+1,1))","2d59509f":"criterion = ['gini', 'entropy']\nmax_depth = [2,4,6,8,10,12]\nmax_features = [6,7,8,9,10,11,12]\n\nmin_samples_leaf = [0.10, 0.2, 0.3, 0.4, 0.5]\nmin_samples_split = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 ]","eeda51ed":"parameters = dict(pca__n_components=n_components,\n                dec_tree__criterion=criterion,\n                dec_tree__max_depth=max_depth,\n                dec_tree__min_samples_leaf = min_samples_leaf,\n                dec_tree__min_samples_split = min_samples_split)","5cc9bd6a":"clf_GS = GridSearchCV(pipe, parameters)\nclf_GS.fit(X, Y)","a7458a26":"print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\nprint('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\nprint('Best Number Of Components:', clf_GS.best_estimator_.get_params()['pca__n_components'])\n# print('Max_features:', clf_GS.best_estimator_.get_params()['dec_tree__max_features'])\nprint('min_samples_leaf:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_leaf'])\nprint('min_samples_split:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_split'])\nprint()\nprint(clf_GS.best_estimator_.get_params()['dec_tree'])","ee0a4a15":"import scikitplot as skplt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report \nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\n\nDecisionTreeClassifier(class_weight=None, criterion=\"gini\", \n                       max_depth=4, min_samples_leaf=0.1,\n                       min_samples_split=0.1)\n\nY_predict = dt.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=True,\n                                    cmap='Blues')\n\nplt.show()\n\nprint(classification_report(Y_test, Y_predict))","0a469156":"import scikitplot as skplt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report \n\ndt = DecisionTreeClassifier()\nmodel = dt.fit(X_train, Y_train)\nY_predict = model.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=True,\n                                    cmap='Blues')\n\nplt.show()\n\nprint(classification_report(Y_test, Y_predict))","04737609":"## Feature Selection","3a1b3960":"Athul Mathew Konoor - 20016  M-Tech AI and DS 19AI613 Machine Learning Project- Hyper Parameter Tuning.","ee39eb21":"## Heart Faliure Prediction using Descision Tree"}}