{"cell_type":{"780440a6":"code","a27de98e":"code","5e791802":"code","ddde14c3":"code","a113b491":"code","6b2bbe90":"code","87dc710f":"code","1d690e7d":"code","d22c59a6":"code","d4eb244c":"code","515553a2":"code","f01a3018":"code","97a75466":"code","6576567e":"code","6d1e0932":"code","43bf9856":"code","d1e251a9":"code","58014d78":"code","81b124e3":"code","5a89fada":"code","93490bd5":"code","013a7473":"code","7e2b68b0":"code","3b9fe9df":"code","ef6ea383":"code","13a6f64e":"markdown","abac984e":"markdown","b1624977":"markdown","33935fd5":"markdown","1ed3b49f":"markdown","e27c64a3":"markdown","365c578c":"markdown","c770235f":"markdown","55050e60":"markdown","b2420187":"markdown","6ad45f96":"markdown","a044ca85":"markdown","b3003fa7":"markdown","00fd492c":"markdown","a20d5bc0":"markdown","69e7599d":"markdown","31d29c89":"markdown","d0671695":"markdown","17be66dc":"markdown","fea7909d":"markdown","77f4ffe3":"markdown"},"source":{"780440a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a27de98e":"# Read data from CSV file\ntrainDF = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/train.csv\")\ntestDF = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/eval.csv\")","5e791802":"# Display the number of features and rows in the training and test dataset \nprint(trainDF.info())\nprint(\"-----------------\")\nprint(testDF.info())","ddde14c3":"# Display the head of the dataframe to observe the values of the dataset\nprint(trainDF.head())\nprint(\"---------------------\")\nprint(testDF.head())","a113b491":"print(trainDF.describe())\nprint(\"---------------------\")\nprint(testDF.describe())","6b2bbe90":"print(trainDF.isnull().sum())\nprint(\"---------------------\")\nprint(testDF.isnull().sum())","87dc710f":"y = trainDF[\"Eat\"]\nids = testDF[\"id\"]\nX = trainDF.drop([\"id\", \"Eat\"], axis=1)\nx = testDF.drop([\"id\"], axis=1)\nprint(X.describe())","1d690e7d":"X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size = 0.15, random_state = 27)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size = 0.15, random_state = 27)","d22c59a6":"print(X_train.describe())","d4eb244c":"# Baseline Sequential Neural Network (3 hidden, 1 output, 30 neurons)\ndef create_model():\n    \n    model = keras.models.Sequential()\n    model.add(keras.layers.InputLayer(input_shape=X_train.shape[1:]))\n    model.add(keras.layers.Dense(30, activation=\"relu\"))\n    model.add(keras.layers.Dense(30, activation=\"relu\"))\n    model.add(keras.layers.Dense(30, activation=\"relu\"))\n    model.add(keras.layers.Dense(1))\n    \n    return model","515553a2":"# Wide & Deep Neural Network (3 hidden, 1 output, 1 concatenation, 30 neurons)\ndef create_model2():\n    \n    input_ = keras.layers.Input(shape=X_train.shape[1:])\n    hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n    hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n    hidden3 = keras.layers.Dense(30, activation=\"relu\")(hidden2)\n    concat = keras.layers.Concatenate()([input_, hidden3])\n    output = keras.layers.Dense(1)(concat)\n    model = keras.Model(inputs=[input_], outputs=[output])\n    \n    return model","f01a3018":"# GridSearch Neural Network with dynamic parameters (Best Parameters: 4_hidden, 40 nerurons)\ndef build_model(n_hidden=1, n_neurons=30, input_shape=X_train.shape[1:]):\n    model = keras.models.Sequential()\n    model.add(keras.layers.InputLayer(input_shape=input_shape))\n    \n    for layer in range(n_hidden):\n        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n        \n    model.add(keras.layers.Dense(1))\n\n    return model","97a75466":"learning_rate = 0.001\nbatch_size = 60\nepochs = 300\n\ndef run_model(model):\n    hist = []\n    result = 1.00\n    \n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"mse\",\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    for i in range(10):  \n        print(\"Start training the model...\")\n        history = model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, y_valid))\n        print(\"Model training finished.\")\n        model.save(\"my_keras_model.h5\")\n        train_rmse = model.evaluate(X_train, y_train)\n        print(\"Train RMSE: \", train_rmse)\n\n        print(\"Evaluating model performance...\")\n        test_rmse = model.evaluate(X_test, y_test)\n        print(\"Test RMSE: \", test_rmse)\n        if test_rmse[1] < result:\n            result = test_rmse[1]\n            model.save(\"my_keras_model.h5\")\n            hist = history.history[\"val_root_mean_squared_error\"]\n    \n    prediction = model.predict(x)\n    prediction = prediction.reshape(3249)\n    print(\"Prediction: \", prediction)\n\n    return hist","6576567e":"model = create_model()\nhist = run_model(model)","6d1e0932":"valScores = pd.DataFrame(hist, columns = [\"RMSE\"])\n\nprint(valScores.describe())\nfig, ax = plt.subplots()\nax.hist(valScores)","43bf9856":"model2 = create_model2()\nhist2 = run_model(model2)","d1e251a9":"valScores = pd.DataFrame(hist2, columns = [\"RMSE\"])\n\nprint(valScores.describe())\nfig, ax = plt.subplots()\nax.hist(valScores)","58014d78":"keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)","81b124e3":"# Grid search hyperparameter tuning\nfrom scipy.stats import reciprocal\nfrom sklearn.model_selection import GridSearchCV\n\nparam_distribs = {\n\"n_hidden\": [3, 4],\n\"n_neurons\": [25, 30, 35, 40],\n\"learning_rate\": [0.001],\n}\n#rnd_search_cv = GridSearchCV(keras_reg, param_distribs, n_jobs=-1, cv=10)\n#rnd_search_cv.fit(X_train, y_train, batch_size=60, epochs=300,validation_data=(X_valid, y_valid))","5a89fada":"#rnd_search_cv.best_params_\n# Best parameters {n_neurons=40, n_hidden=4, learning_rate=0.001}","93490bd5":"model3 = build_model(n_neurons=40, n_hidden=4)\n#model.fit(X_train, y_train, batch_size=60, epochs=600,validation_data=(X_valid, y_valid))\nhist3 = run_model(model3)","013a7473":"valScores = pd.DataFrame(hist3, columns = [\"RMSE\"])\n\nprint(valScores.describe())\nfig, ax = plt.subplots()\nax.hist(valScores)","7e2b68b0":"model = keras.models.load_model(\"\/kaggle\/working\/my_keras_model.h5\")\nprint(model.evaluate(X_test, y_test))\nprediction = model.predict(x)\nprediction = prediction.reshape(3249)\nprint(\"Prediction: \", prediction)","3b9fe9df":"output = pd.DataFrame({'id': ids, 'Eat': prediction})\noutput.to_csv('submission.csv', index = False)\nprint(\"Your submission was successfully saved!\")","ef6ea383":"import csv\nwith open('\/kaggle\/working\/submission.csv', 'r') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)","13a6f64e":"## **Load data from CSV**","abac984e":"# **Cleaning Data**\n\n* **Drop All the redundant data.**","b1624977":"### **Inspect Missing & Null Values:**\n**Missing\/Null Features (0):**\n\n**There are no missing nor null values in both the training and testing data set.**","33935fd5":"## **Models**\n**Create three models: one base sequential model, one Wide & Deep Model, and a 4-deep model using Grid Search parameters**\n","1ed3b49f":"### **Run Third Model (BEST)** ","e27c64a3":"# **Modeling**","365c578c":"### **Run First Model**","c770235f":"#### **Distribution of Validation RMSE**","55050e60":"# **CONCLUSION**\n\n**The best model was the Third Model generated through GridSearch. A major factor that played in it being the best model is its depth. A minimum of 3 layers were required to produce any respectable results. If time permited, I would have liked to test a 5th hidden layer.**\n\n**Additionally, each model required a large epoch of at least 200 to generate adequate results. Otherwise, the model would consistently underfit. Experimenting with SGD as an optimizer resulted in very poor results. At times, the loss values generated while using SGD were \"nan.\" Switching to \"adam\" optimizer significantly improved the performance.**\n\n**The worst performing model was the Wide and Deep model. The model produces extremely high variance, most likely because the model generated was far too complex for the dataset.**","b2420187":"### **Head of Dataframe data:**\n* **ID** can reasonably be dropped as it offers no inferential value\n* **The rest of the features I am not  comfortable dropping.**","6ad45f96":"# **Objective:**\n\n**Predict the atomization energy of molecules using Multilayer Perceptrons (Neural Networks), given a sample data set.**","a044ca85":"#### **Distribution of Validation RMSE**","b3003fa7":"# **Explore & Analyze the Data**","00fd492c":"### **Run Second Model (WORST)** ","a20d5bc0":"### **Train-Test Split**\n**Split the training and evaluating dataframe into training (70%), testing (15%), and validation (15%) set.**\n","69e7599d":"### **Identify Any Correlations**\n\n**It is difficult to identify any correlations because the data currently lacks descriptive features and statistics.**\n\n**Feature engineering or unsupervised learning may identify groupings of data that are most appropriate.**","31d29c89":"### **Feature Data and Dataframe Structure:**\n* Seems to be 1276 features\n* About 12,993 data points in the training set (rows) per feature","d0671695":"### **Description of Dataframe data:**\n* Descriptive statistics do not seem to offer any meaningful or useful data.\n","17be66dc":"# **Feature Engineering**\n\n1) **In Neural Networks, the measure of similarity is learned essentially from the dataset and implicitly given by the mapping to the hidden layers**","fea7909d":"### **Identify Any Outliers**\n\n**Outliers in the current dataset are unclear.** \n\n**The data currently lacks descriptive features and statistics that could highlight values that deviate from the norm.**","77f4ffe3":"#### **Distribution of Validation RMSE**"}}