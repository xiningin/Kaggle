{"cell_type":{"33ca87bb":"code","9226d173":"code","bb778a83":"code","95f83ed3":"code","c57a29af":"code","3ad974ed":"code","d05c7eb8":"code","fcc941e5":"code","d0948d7e":"code","deeb67e8":"code","e89d78d3":"code","cec613d1":"code","1f8a78d5":"code","496686a3":"code","cefcf5f8":"code","64f30efe":"code","aaa1bef5":"code","5f6b2ac7":"code","ea157970":"code","98293918":"code","351b610a":"code","db301a95":"code","7be45a9d":"code","ed160db1":"code","725cf916":"code","681bc363":"code","26391025":"code","49ecded1":"code","d2f63517":"code","654530f9":"code","5d38775b":"code","9ee4e89f":"code","faf0b8d9":"code","0a239f27":"code","94be0836":"code","d99af182":"code","8e70b8fe":"markdown","2daba4f3":"markdown","6cd43bf1":"markdown","b1c9251e":"markdown","8428e395":"markdown","bfb35e21":"markdown","3aeae22c":"markdown","a032abbe":"markdown","1fe52b21":"markdown","08d29d09":"markdown","18c9d4a3":"markdown","a0f38885":"markdown","48af94cf":"markdown","a3b24614":"markdown","1eaf9f45":"markdown","814bedd8":"markdown","2454de74":"markdown","dfa1e4de":"markdown","f72e86c0":"markdown","e7c9cdfb":"markdown","b0bf2c58":"markdown","132d2c49":"markdown"},"source":{"33ca87bb":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\nwarnings.filterwarnings('ignore')","9226d173":"train = pd.read_csv(\"..\/input\/demand-forecasting-kernels-only\/train.csv\", parse_dates=[\"date\"])\ntest = pd.read_csv(\"..\/input\/demand-forecasting-kernels-only\/test.csv\", parse_dates=[\"date\"])\nsample_sub = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/sample_submission.csv')\ndf = pd.concat([train, test], sort=False)","bb778a83":"\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\n\ncheck_df(df)","95f83ed3":"print(train.shape, test.shape, df.shape)","c57a29af":"# Our time range of sales record.\ndf[\"date\"].min(), df[\"date\"].max()","3ad974ed":"# Summary Stats for each store\ndf.groupby([\"store\"]).agg({\"sales\": [\"count\", \"sum\", \"mean\", \"median\", \"std\", \"min\", \"max\"]})","d05c7eb8":"fig, axes = plt.subplots(2, 5, figsize=(20, 10))\nfor i in range(1, 11):\n    if i < 6:\n        train[train.store == i].sales.hist(ax=axes[0, i - 1])\n        axes[0, i - 1].set_title(\"Store \" + str(i), fontsize=15)\n\n    else:\n        train[train.store == i].sales.hist(ax=axes[1, i - 6])\n        axes[1, i - 6].set_title(\"Store \" + str(i), fontsize=15)\nplt.tight_layout(pad=4.5)\nplt.suptitle(\"Sales in Stores (Histogram)\");","fcc941e5":"def create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday \/\/ 4\n    df[\"quarter\"] = df.date.dt.quarter\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    df[\"season\"] = np.where(df.month.isin([12,1,2]), 0, 1)  # 0: Winter, 1: Spring\n    df[\"season\"] = np.where(df.month.isin([6,7,8]), 2, df[\"season\"]) # 2: Summer\n    df[\"season\"] = np.where(df.month.isin([9, 10, 11]), 3, df[\"season\"])  # 3: Fall\n    return df\n\ndf = create_date_features(df)","d0948d7e":"def random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))","deeb67e8":"# We don't want the values to be mixed sorted because I will derive a lag. That's why we're sorting it.\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)\n\n\ndef lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\n# The problem with my dataset was that we wanted to make an estimate for the next 3 months(90 days).\n# Therefore, since my forecast period is 3 months, I determine my new lag features accordingly.\n# We want to catch seasonality.\ndf = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])","e89d78d3":"def roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\n\ndf = roll_mean_features(df, [365, 546])","cec613d1":"def ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\n\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\ndf = ewm_features(df, alphas, lags)","1f8a78d5":"# df = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month']) \n# not needed for now","496686a3":"df['sales'] = np.log1p(df[\"sales\"].values)","cefcf5f8":"def smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num \/ denom)) \/ n\n    return smape_val\n\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False","64f30efe":"# I'm trying to make my reference point for validation look like Kaggle's test data scenario.\n# Dataset gave us till the first 3 months of 2018. For this reason, we do validation with the first 3 months of 2017.\n# Train until 2017, validation for the first 3 months of 2017.\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), ]\n\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\nY_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]\n\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape\n# There are 45000 values in my validation set. My test set also had 45000 values.","aaa1bef5":"#lgbm_params = {\"num_leaves\": [10,15,20,31],\n#               \"learning_rate\": [0.1, 0.05, 0.02],\n#               \"colsample_bytree\":[0.5, 0.8, 1.0],\n#               \"max_depth\": [-1, 5, 10, 20]}","5f6b2ac7":"# model = lgb.LGBMRegressor()\n# tscv = TimeSeriesSplit(n_splits=3)\n# rsearch = GridSearchCV(model, lgbm_params, cv=tscv, scoring=make_scorer(smape), verbose = True, n_jobs = -1).fit( X_train[cols], Y_train )","ea157970":"# print(rsearch.best_params_)","98293918":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.5,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 1000,\n              'early_stopping_rounds': 200,\n              'nthread': -1}","351b610a":"lgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n\n\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=100)","db301a95":"y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)","7be45a9d":"# We undo the logarithmic transformation we did. Then we calculated our smape error.\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))","ed160db1":"\ndef plot_lgb_importances(model, plot=False, num=10):\n\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n        return feat_imp\n\nplot_lgb_importances(model, num=30, plot=True)","725cf916":"feature_imp_df = plot_lgb_importances(model, num=50)\nfeature_imp_df.gain","681bc363":"cols = feature_imp_df[feature_imp_df.gain > 0.015].feature.tolist()\nprint(\"Independent Variables:\", len(cols))","26391025":"train_final = df.loc[~df.sales.isna()] \nY_train_final = train_final['sales']\nX_train_final = train_final[cols]\n\ntest_final = df.loc[df.sales.isna()]\nX_test_final = test_final[cols]","49ecded1":"\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.5,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}","d2f63517":"lgbtrain_all = lgb.Dataset(data=X_train_final, label=Y_train_final, feature_name=cols)","654530f9":"model = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)","5d38775b":"test_preds = model.predict(X_test_final, num_iteration=model.best_iteration)","9ee4e89f":"forecast = pd.DataFrame({\n    \"date\":test_final.date,\n    \"store\":test_final.store,\n    \"item\":test_final.item,\n    \"sales\":np.expm1(test_preds)\n})","faf0b8d9":"submission_df = test_final.loc[:, ['id', 'sales']]\nsubmission_df['sales'] = np.expm1(test_preds)\nsubmission_df['id'] = submission_df.id.astype(int)\nsubmission_df.to_csv('submission_demand3.csv', index=False)\nsubmission_df.head()","0a239f27":"submission_df[[\"sales\"]].describe([0.1, 0.75, 0.8, 0.9, 0.95, 0.99]).T","94be0836":"train_final[\"sales\"] = np.expm1(train_final[\"sales\"])","d99af182":"train_final[(train_final.store == 1) & (train_final.item == 1)].set_index(\"date\").sales.plot(figsize = (20,9),legend=True, label = \"Store 1 Item 1 Sales\")\nforecast[(forecast.store == 1) & (forecast.item == 1)].set_index(\"date\").sales.plot(legend=True, label = \"Store 1 Item 1 Forecast\", color =\"orange\");","8e70b8fe":"## Random noise\nWe will derive the shifted features with rolling mean.\nWhen we derive them, we add random noise to the data because it affects the generalizability ability\nwhen it is the same as the previous true value. In a way, we corrupt the data ourselves.\nIn the size of the data set with random normal distribution, we will add to the features we want.\nIt is very useful for time series problems.","2daba4f3":"Need to forecast item sales at different stories for following 3 months\n\nYou can find LightGBM features in [here](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Features.html).\nWe will use GBM method such as LightGBM. There will be some important steps before modeling.\nNormally LightGBM does not understand time series.\nBut we will put it in a way that it can understand.\nThere is such a thing as a trend, there is such a thing as stationary and seasonality.\nWe can use whatever ML method we want. However, the features we will extract must carry the pattern of our data.\n","6cd43bf1":"> [8700]\ttraining's l1: 0.12471\ttraining's SMAPE: 12.8089\tvalid_1's l1: 0.131423\tvalid_1's SMAPE: 13.5114","b1c9251e":"# Hyperparameter Tuning","8428e395":"### One-Hot Encoding","bfb35e21":"# Data Importing","3aeae22c":"# EDA\n\nWe discover dataset as looking shape, types of features, null values etc.\nDefined basic function you can use another projects also","a032abbe":"> {'colsample_bytree': 0.5, 'learning_rate': 0.02, 'max_depth': 5, 'num_leaves': 10}","1fe52b21":"## Custom Cost Function\n\n* MAE: mean absolute error\n* MAPE: mean absolute percentage error\n* SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)","08d29d09":"## Converting sales to log(1+sales)\nWe can do conversion to the dependent variable. We'll take that back when evaluating the error.\nWe got the logarithm of the dependent variable. Why?\nSince we will use LightGBM. We want to shorten the optimization time as it is based on GBM.\nI am interested in the dependent variable. We are in a regression problem.\nTherefore, we can the logarithm and reduce it and standardize it. But now I don't do it.","18c9d4a3":"## Rolling Mean Features\nBetter to shift first and then average. When deriving the rolling mean feature, take shift and avg.\n","a0f38885":"## Time Related Features (Date Features)","48af94cf":"# Feature Engineering\nThere may be seasonality in time units.\nWe derive date-based features by breaking as much as possible.\n\n* Time Related Features (Date Features)\n* Shifted Features\n* Rolling Mean Features (Moving Average Features)\n* Exponentially Weighted Mean Features","a3b24614":"## Time-Based Validation Sets\n\n\nWe have one train set, and we need to have one validation set. We have to look at our own fault with it.","1eaf9f45":"## Define Forecast","814bedd8":"## Exponentially Weighted Mean Features\nIt makes more sense to take the moving average instead of past average it if there is seasonality and trend\nThe weight we give to the closest value, which we call ewm. If it is high (alpha=0.99), it gives higher weight to the nearest period.\n","2454de74":"## Lag\/Shifted Features\n\nPast real values.\nSince the future is best influenced by the most recent last period values, we turn them into features.\nWe produce a feature for past sales values. We add previous sales features.\nIn other words, we create an independent variable using the dependent variable.\nThat's why we just added random noise.","dfa1e4de":"# Modeling","f72e86c0":"## Visualize Sales Forecast","e7c9cdfb":"# Final Model","b0bf2c58":"## Visualize sales in each stores","132d2c49":"## Submission"}}