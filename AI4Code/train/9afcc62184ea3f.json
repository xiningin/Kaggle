{"cell_type":{"5865ac8f":"code","77019448":"code","7388a777":"code","677150f7":"code","2136dc32":"code","c3734620":"code","11d194e0":"code","74293727":"code","a95e671a":"code","f6e2ae71":"code","669cb096":"code","0e9bc179":"code","143497c7":"code","edffb142":"code","f9004c26":"code","85559218":"code","248b2d31":"code","4c80458d":"code","14ab3fcd":"code","2d465305":"code","29b3b2ea":"code","3a0484d7":"code","1b824743":"code","88a9d42d":"code","8e57a062":"code","5032ade8":"code","6d985f6d":"code","5da225a3":"code","fb2a33cc":"code","b88dce65":"code","d6554d05":"code","af360b9e":"code","e51289a0":"code","d5265a8f":"code","ed91c9bb":"code","9f37d0fe":"code","55901842":"code","7b10ab47":"code","65d7eef6":"code","ce065114":"code","43d7dddd":"code","a0ca408e":"code","f59cf171":"code","839346c2":"code","0ce7c0b6":"markdown","d96d7c73":"markdown","8370a68b":"markdown","1785fbd3":"markdown","b50b28f6":"markdown","554d191c":"markdown","4f352bc9":"markdown","fc239bfd":"markdown","2ef2df38":"markdown","95dc7e49":"markdown","5c1bcff2":"markdown","ec781fdc":"markdown","85ccbbae":"markdown","b4184a70":"markdown","02b49ce6":"markdown","6e6327d0":"markdown","fb48f69f":"markdown","73249784":"markdown","628ba71a":"markdown","99bd3ab2":"markdown","cf359a39":"markdown","c0f34d9e":"markdown","711de7f5":"markdown","cbc43641":"markdown","645f0ae1":"markdown","d201e12e":"markdown","1bc379aa":"markdown","91df76d8":"markdown","2a79cce1":"markdown","c66b9e8c":"markdown","dcbb7a41":"markdown","e287f226":"markdown","89602bb9":"markdown","c0d35ab7":"markdown","02215d56":"markdown"},"source":{"5865ac8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77019448":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","7388a777":"df = pd.read_csv(\"..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv\")\ndf.head()","677150f7":"df.drop(['ID'], axis=1, inplace=True) # Deleting column ID","2136dc32":"df.isnull().sum() # No null values.","c3734620":"df.shape   # Shape of DataFrame.","11d194e0":"df.columns # Columns of DataFrame","74293727":"df.describe()","a95e671a":"corr = df.corr()\nplt.figure(figsize=(25,15))\nsns.heatmap(corr, annot=True)","f6e2ae71":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","669cb096":"x = df[['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2',\n       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]\ny = df['default.payment.next.month']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=3)\n\nxtrain_scaler = MinMaxScaler().fit_transform(x_train)\nxtest_scaler = MinMaxScaler().fit_transform(x_test)","0e9bc179":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix","143497c7":"i = 1\nk = np.arange(1, 30, 1)\nk_val_acc = []\nfor i in k:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train, y_train)              # Fitting model with x_train and y_train\n    y_pred = knn.predict(x_test)           # Predicting the results\n    k_val_acc.append(metrics.accuracy_score(y_test, y_pred))\n    print(\"Accuracy for K = {0} is: \".format(i),metrics.accuracy_score(y_test, y_pred))\n    \nm = max(k_val_acc)\n\nprint(\"We got max accuracy of {0} when K = {1}\". format(max(k_val_acc), [i+1 for i, j in enumerate(k_val_acc) if j == m]))","edffb142":"i = 1\nk = np.arange(1, 30, 1)\nk_val_acc_mms = []\nfor i in k:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(xtrain_scaler, y_train)              # Fitting model with xtrain_scaler and y_train\n    y_pred_mms = knn.predict(xtest_scaler)       # Predicting the results\n    k_val_acc_mms.append(metrics.accuracy_score(y_test, y_pred_mms))\n    print(\"Accuracy for K = {0} is: \".format(i),metrics.accuracy_score(y_test, y_pred_mms))\n    \nm = max(k_val_acc_mms)\n\nprint(\"We got max accuracy of {0} when K = {1}\". format(max(k_val_acc_mms), [i+1 for i, j in enumerate(k_val_acc_mms) if j == m]))","f9004c26":"plot_confusion_matrix(knn, xtest_scaler, y_test)\n\nconf_metr = metrics.confusion_matrix(y_test, y_pred_mms)\n\nprint(\"Confusion Matrix: \\n {}\".format(conf_metr))\nprint(metrics.classification_report(y_test,y_pred_mms))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_mms))\nprint(\"Recall\/Sensitivity\/True Positive Rate:\",metrics.recall_score(y_test, y_pred_mms))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred_mms))","85559218":"# ROC Curve:\nmetrics.plot_roc_curve(knn, xtest_scaler, y_test)","248b2d31":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV","4c80458d":"c_val = [0.001,0.01,0.1,0.5,1.0]\n\nlogreg = LogisticRegression(solver = 'liblinear')\nhyperParam = [{'C':c_val}]\n\ngsv = GridSearchCV(logreg,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(x_train, y_train)                      # Fitting model with x_train and y_train\nlogreg_pred = best_model.best_estimator_.predict(x_test)    # Predicting the results\n\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(\"Best Accuracy :\",best_model.score(x_test, y_test))","14ab3fcd":"c_val = [0.001,0.01,0.1,0.5,1.0]\n\nlogreg = LogisticRegression(solver = 'liblinear')\nhyperParam = [{'C':c_val}]\n\ngsv = GridSearchCV(logreg,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(xtrain_scaler, y_train)                       # Fitting model with xtrain_scaler and y_train\nlogreg_pred_mms = best_model.best_estimator_.predict(xtest_scaler) # Predicting the results\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(\"Best Accuracy :\",best_model.score(xtest_scaler, y_test))","2d465305":"plot_confusion_matrix(gsv,xtest_scaler, y_test)\n\nconf_metr = confusion_matrix(y_test, logreg_pred_mms)\n\nprint(\"Confusion Matrix: \\n {}\".format(conf_metr))\nprint(metrics.classification_report(y_test,logreg_pred_mms))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, logreg_pred_mms))\nprint(\"Recall\/Sensitivity\/True Positive Rate:\",metrics.recall_score(y_test, logreg_pred_mms))\nprint(\"Precision:\",metrics.precision_score(y_test, logreg_pred_mms))","29b3b2ea":"# ROC Curve:\nmetrics.plot_roc_curve(gsv, xtest_scaler, y_test)","3a0484d7":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","1b824743":"dtree_up = DecisionTreeClassifier()\ndtree_up.fit(x_train, y_train)                  # Fitting model with x_train and y_train\ndtree_pred_up = dtree_up.predict(x_test)        # Predicting the results\nprint(\"Accuracy is: \",metrics.accuracy_score(y_test, dtree_pred_up))","88a9d42d":"i = 1\nd = np.arange(1, 20, 1)\ndepth = []\nfor i in d:\n    dtree = DecisionTreeClassifier(max_depth=i)\n    dtree.fit(x_train, y_train)                # Fitting model with x_train and y_train\n    dtree_pred = dtree.predict(x_test)         # Predicting the results\n    depth.append(metrics.accuracy_score(y_test, dtree_pred))\n    print(\"Accuracy when max_depth = {0}: \".format(i),metrics.accuracy_score(y_test, dtree_pred))\n\nm = max(depth)\n\nprint(\"We got max accuracy of {0} when max_depth = {1}\". format(max(depth), [i+1 for i, j in enumerate(depth) if j == m]))","8e57a062":"i = 1\nd = np.arange(1, 20, 1)\ndepth_mms = []\nfor i in d:\n    dtree = DecisionTreeClassifier(max_depth=i)\n    dtree.fit(xtrain_scaler, y_train)             # Fitting model with xtrain_scaler and y_train\n    dtree_pred_mms = dtree.predict(xtest_scaler)  # Predicting the results\n    depth_mms.append(metrics.accuracy_score(y_test, dtree_pred_mms))\n    print(\"Accuracy when max_depth = {0}: \".format(i),metrics.accuracy_score(y_test, dtree_pred_mms))\n\nm = max(depth_mms)\n\nprint(\"We got max accuracy of {0} when max_depth = {1}\". format(max(depth_mms), [i+1 for i, j in enumerate(depth_mms) if j == m]))","5032ade8":"plot_confusion_matrix(dtree, xtest_scaler, y_test)\n\nconf_metr = metrics.confusion_matrix(y_test, dtree_pred_mms)\n\nprint(\"Confusion Matrix: \\n {}\".format(conf_metr))\nprint(metrics.classification_report(y_test,dtree_pred_mms))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, dtree_pred_mms))\nprint(\"Recall\/Sensitivity\/True Positive Rate:\",metrics.recall_score(y_test, dtree_pred_mms))\nprint(\"Precision:\",metrics.precision_score(y_test, dtree_pred_mms))","6d985f6d":"# ROC Curve:\nmetrics.plot_roc_curve(dtree, xtest_scaler, y_test)","5da225a3":"pip install pydotplus","fb2a33cc":"from sklearn.tree import export_graphviz\nfrom six import StringIO\nfrom IPython.display import Image  \nimport pydotplus","b88dce65":"feat = ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2',\n       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\ndot_data = StringIO()\nexport_graphviz(dtree, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feat,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('Credit_Card_Tree.png')\nImage(graph.create_png())","d6554d05":"from sklearn.ensemble import RandomForestClassifier","af360b9e":"rf = RandomForestClassifier()\nrf.fit(xtrain_scaler, y_train)             # Fitting model with xtrain_scaler and y_train\nrf_pred = rf.predict(xtest_scaler)         # Predicting the results\n#est_per.append(metrics.accuracy_score(y_test, rf_pred))\nprint(\"Accuracy: {0}\".format(metrics.accuracy_score(y_test, rf_pred)))","e51289a0":"estimators = [10,50,80,100,150,200,250,300]\n\nrf = RandomForestClassifier(max_depth=3,random_state=5)\nhyperParam = [{'n_estimators':estimators}]\n\ngsv = GridSearchCV(rf,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(xtrain_scaler, y_train)                       # Fitting model with xtrain_scaler and y_train\nrf_pred_mms = best_model.best_estimator_.predict(xtest_scaler)     # Predicting the results\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(\"Best Accuracy :\",best_model.score(xtest_scaler, y_test))","d5265a8f":"plot_confusion_matrix(gsv, xtest_scaler, y_test)\n\nconf_metr = confusion_matrix(y_test, rf_pred_mms)\n\nprint(\"Confusion Matrix: \\n {}\".format(conf_metr))\nprint(metrics.classification_report(y_test,rf_pred_mms))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, rf_pred_mms))\nprint(\"Recall\/Sensitivity\/True Positive Rate:\",metrics.recall_score(y_test, rf_pred_mms))\nprint(\"Precision:\",metrics.precision_score(y_test, rf_pred_mms))","ed91c9bb":"# ROC Curve:\nmetrics.plot_roc_curve(gsv, xtest_scaler, y_test)","9f37d0fe":"from sklearn.svm import SVC","55901842":"kernels = ['rbf','linear','poly','sigmoid']\n\nsvc = SVC()\nhyperParam = [{'kernel':kernels}]\n\ngsv = GridSearchCV(svc,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(xtrain_scaler, y_train)                       # Fitting model with xtrain_scaler and y_train\nsvc_pred_mms = best_model.best_estimator_.predict(xtest_scaler)    # Predicting the results\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(\"Best Accuracy :\",best_model.score(xtest_scaler, y_test))","7b10ab47":"plot_confusion_matrix(gsv, xtest_scaler, y_test)\n\nconf_metr = confusion_matrix(y_test, svc_pred_mms)\n\nprint(\"Confusion Matrix: \\n {}\".format(conf_metr))\nprint(metrics.classification_report(y_test,svc_pred_mms))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, svc_pred_mms))\nprint(\"Recall\/Sensitivity\/True Positive Rate:\",metrics.recall_score(y_test, svc_pred_mms))\nprint(\"Precision:\",metrics.precision_score(y_test, svc_pred_mms))","65d7eef6":"# ROC Curve:\nmetrics.plot_roc_curve(gsv, xtest_scaler, y_test)","ce065114":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier","43d7dddd":"# Creating classifiers\nknn = KNeighborsClassifier()\nlg = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nsvc = SVC()\n\nclf_array = [knn, lg, dt, rf,svc]\n\nfor clf in clf_array:\n    cc_scores = cross_val_score(clf, x, y, cv=10, n_jobs=-1)\n    bagging_clf = BaggingClassifier(clf, max_samples=0.25, max_features=10, random_state=3)\n    bagging_scores = cross_val_score(bagging_clf, x, y, cv=10, n_jobs=-1)\n    \n    print(\"Accuracy of: {1:.3f}, std: (+\/-) {2:.3f} [{0}]\".format(clf.__class__.__name__,cc_scores.mean(), cc_scores.std()))\n    print(\"Accuracy of: {1:.3f}, std: (+\/-) {2:.3f} [Bagging {0}]\\n\".format(clf.__class__.__name__,bagging_scores.mean(), bagging_scores.std()))","a0ca408e":"clf = [knn, lg, dt, rf,svc]\neclf = VotingClassifier(estimators=[('KNN', knn), ('Logistic Regression', lg), ('Decision Tree', dt), ('Random Forest', rf), ('SVC', svc)], voting='hard')\nfor clf, label in zip([knn, lg, dt, rf,svc, eclf], ['KNN', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'SVC', 'Ensemble']):\n    scores = cross_val_score(clf, x_train, y_train, cv=10, scoring='accuracy')\n    print(\"Accuracy: %0.3f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","f59cf171":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier","839346c2":"# Creating classifiers\nknn = KNeighborsClassifier()\nlg = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nsvc = SVC()\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier()\nxgb_boost = XGBClassifier()\nboost_array = [ada_boost, grad_boost, xgb_boost]\nclf = [knn, lg, dt, rf,svc]\neclf = EnsembleVoteClassifier(clfs=[ada_boost, grad_boost, xgb_boost], voting='hard')\nlabels = ['Ada Boost', 'Grad Boost', 'XG Boost', 'Ensemble']\nfor clf, label in zip([ada_boost, grad_boost, xgb_boost, eclf], labels):\n    scores = cross_val_score(clf, x, y, cv=10, scoring='accuracy')\n    print(\"Accuracy: {0:.3f}, std: (+\/-) {1:.3f} [{2}]\".format(scores.mean(), scores.std(), label))","0ce7c0b6":"# **5. SVC (Support Vector Classifier):**","d96d7c73":"# **Bagging with all classifiers using Cross Validation:**","8370a68b":"Lastly, we used boosting technique. This \nboosting is not random, and the current \nperformance of the model will depend on \nprevious models. We used Ada Boost \nClassifier, Gradient Boosting Classifier and XG \nBoost Classifier. We have used all 5 ML \nmodels (KNN, Logistic, Decision Tree, \nRandom Forest and SVM). We \nhyperparametered cross val score with all 5 ML \nmodels, cv = 10 and scoring = \u2018accuracy\u2019. We \nalso tuned \u2018EnsembleVoteClassifier\u2019 with voting = \u2018hard\u2019 and for all 3 boosters Ada boost, \nGradient boost and XG Boost.\nTo choose the best classifier, we will use \nSklearn\u2019s VotingClassifier, which will help us \nto combine different ML classifiers and will \nperform a vote on all classifiers.\nAccording to the results, Gradient boost came \nout to be best with 82.10 % accuracy and 0.011 \nstandard deviation.\n\nNote: Accuracy and Std deviation may change slightly after each run.","1785fbd3":"# **Boosting with all classifiers using Cross Validation:**","b50b28f6":"Next, Logistic Regression was used for this \ndataset. This model is tuned by giving \u2018C\u2019 \n[0.001,0.01,0.1,0.5,1.0] and assigning the\nsolver as \u2018liblinear\u2019. This model is \nhyperparameter tuned using sklearn\u2019s \nGridSearchCV. We performed this model two \ntimes, once with normal train and test sets and \na second time with transformed \n(MinMaxScaler) train and test sets.\n1. During our first model, we found results as:\n* C: 0.001\n* Accuracy: 78.81 %\n2. During our second model with transformed sets:\n* C: 1.0\n* Accuracy: 82.68 %\n3. After plotting confusion matrix with transformed sets, we were able to get:\n* Accuracy: 82.68 % \n* Precision: 68.74 %\n* Recall: 33.37 %\n\nShows ROC Curve and it can be observed that \u201cArea Under the Curve\u201d is 0.73. The ROC Curve can be used to compare with other models, it showsthe area under the curve. Large values on Y-Axis demonstrates lower false negatives and higher true positives.","554d191c":"**Confusion Matrix of Hyperparameter Tuned Random Forest model with transformed data:**","4f352bc9":"Next, Decision Tree is used for this dataset. We \nhave used 3 variants of models: 1st Full tree with \nnormal test and train sets, 2nd Pruned tree with \nnormal test and train sets, and 3rd Pruned tree \nwith transformed (MinMaxScaler) train and test \nsets.\n1. Decision Tree (Unpruned) with normal test and train sets:\n* Accuracy: 73.34 %\n2. Decision Tree \u2013 which was pruned using max_depth for 1 to 20 range and used normal train and test sets here.\n* Accuracy: 83.04 %\n* max_depth: 4\n2. Decision Tree \u2013 which was pruned using max_depth for 1 to 20 range and this time using transformed (MinMaxScaler) train and test sets.\n* Accuracy: 83.06 %\n* max_depth: 3\n3. After plotting confusion matrix (Fig. 6) with transformed sets, we were able to get:\n* Accuracy: 57.21 % \n* Precision: 24.61 %\n* Recall: 47.10 %\n\nShows ROC Curve and it can be observed that \u201cArea Under the Curve\u201d is 0.55. The ROC Curve can be used to compare with other models, it shows the area under the curve. We got not good results here, it is clearly visible in ROC that area under the curve is less.","fc239bfd":"Next, we used Random Forest. We used two \nmodels of Random Forest for the dataset. Both \ntimes, we used transformed (MinMaxScaler) \ntrain and test sets. 1st model is Simple Random \nForest and 2nd model is hyperparameter tuned \nRandom Forest model.\n1. During our first simple model, we found results as:\n* Accuracy: 79.44 %\n2. For second model, we have used hyperparameter tuning, for this we changed max_depth to 3 (because we had got depth 3 for decision tree which showed best accuracy), random state = 5 and n_estimators values as[10,50,80,100,150,200,250,300]. This model is hyperparameter tuned using sklearn\u2019s GridSearchCV.\n* Accuracy: 81.80 %\n* n_estimator: 10\n3. After plotting confusion matrix with transformed sets, we were able to get:\n* Accuracy: 81.80 % \n* Precision: 69.05 %\n* Recall: 25.44 %\n\nShows ROC Curve and it can be observed that \u201cArea Under the Curve\u201d is 0.77. The ROC Curve can be used to compare with other models, it shows the area under the curve. Large values on Y-Axis demonstrates lower false negatives and higher true positives.","2ef2df38":"# **Correlation Matrix Plot**\nWe can demonstrate that no variables \nare strongly correlated with the Target variable \n(default). The \u2018PAY_\u2019 variables have a strong \ncorrelation between them and have a weak \npositive correlation with the target \nvariable(default). All the \u2018BILL_AMT\u2019 \nvariables have a good positive correlation \nbetween them. Also, \u2018LIMIT_BAL\u2019 has a good \npositive correlation with \u2018BILL_AMT\u2019 \nvariables.","95dc7e49":"**Confusion Matrix of Pruned Tree model with transformed training and testing data:**","5c1bcff2":"**Visualizing Decision Tree**","ec781fdc":"> ***Tuned Model with transformed data:***","85ccbbae":"# **2. Logistic Regression:**","b4184a70":"Next, we used Support Vector Machine for this dataset. This model is tuned using 4 kernel values [\u2018rbf\u2019, \u2018linear\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019].\n1. Used this model with transformed (MinMaxScaler) train and test sets.\n* Accuracy: 81.64 %\n* Kernel: poly\n2. After plotting confusion matrix with transformed sets, we were able to get:\n* Accuracy: 81.64 % \n* Precision: 69.71 %\n* Recall: 23.48 %\n\nShows ROC Curve and it can be observed that the \u201cArea Under the Curve\u201d is 0.69. The ROC Curve which we can use to compare with other models, it shows the area under the curve. From the ROC curve, we can demonstrate that area under the curve is less.\n","02b49ce6":"> ***HyperParameter Tuned model with normal train and test data:***","6e6327d0":"> ***Pruned Tree after applying model with normal train and test data:***","fb48f69f":"> ***Checking for which K, model is generating more accuracy. Using transformed train and test data.***","73249784":"# **Conclusion:**\nThe first dataset is the Credit Card dataset, \nwhere we had records from Taiwan from April 2005 to September 2005. This dataset was \nhighly imbalanced and needed resampling or \ntransformation. We used MinMaxScaler to \ntransform our train and test data. After applying \n5 machine learning models with normal data \nand transformed data, it can be easily observed \nthat the performance of all models was good \nwith transformed train and test data. KNN, \nLogistic Regression and RandomForest gave \nthe best accuracy around 82 %. Logistic \nRegression was best with 82.68 % accuracy, \nrecall 84.27 % and 95.92 % recall. After \napplying bagging, it was observed that \nRandomForest came out to be the best with \n81.30 % accuracy and a very low 0.01 standard \ndeviation. In last, we used boosting for all 5 \nmachine learning models and applied Ada \nboost, Gradient boost and XG boost. Out of \nthese 3. Grad boost performed best when both \naccuracy and standard deviation were \ncompared with other boosting methods. For \nGrad boost, we got 82.10 % accuracy and 0.011 \nstandard deviation.\n\nNote: Accuracy and Std deviation may change slightly after each run.","628ba71a":"**Confusion Matrix Tuned Model with transformed data:**","99bd3ab2":"**Confusion Matrix of model with transformed data:**","cf359a39":"Next, we used the bagging method, and this will \ncreate all models using different data and a \nweighted average will be used to determine the \nresult. We have used all 5 ML models (KNN, \nLogistic, Decision Tree, Random Forest and \nSVM). We hyperparameter BaggingClassifier \nwith all 5 ML models, max_samples=0.25, \nmax_features=10, random_state=3. For cross \nval score, parameters are passed like cv = 10 \nand n_jobs = -1. We can see from the below \nthat for KNN, Logistic and Decision \nTree, accuracy is increasing, and the standard \ndeviation is decreasing for KNN, Decision Tree \nand Random Forest.\nTo choose the best classifier, we will use \nSklearn\u2019s VotingClassifier, which will help us \nto combine different ML classifiers and will \nperform a vote on all classifiers.\nShows results we can observe that \nRandomForest had the best accuracy 81.30 % \nwith a very low standard deviation of 0.01.\n\nNote: Accuracy and Std deviation may change slightly after each run.","c0f34d9e":"> ***HyperParameter Tuned model with transformed training and testing data:***","711de7f5":"> ***Unpruned Tree with normal train and test data:***","cbc43641":"# **Splitting data into train and test**\nWe have split this data into 75% and 25% for \ntrain and test sets respectively using \nsklearn.model_selection train_test_split. We \ncreated x_train, x_test, y_train and y_test.\nAlso, we have transformed the data using \nsklearn. preprocessing MinMaxScaler. Here, \nwe have used x_train and x_test to transform \nthem into xtrain_scaler and xtest_scalar \nbecause there are many observations with large \nranges such as \u2018TOTAL_PAY\u2019, \n\u2018LIMIT_BALANCE\u2019 and \u2018TOTAL_BILL\u2019. \nWe are using MinMaxScaler to scale our \nvariables and convert them in the range 0-1. \nSince all variables are transformed using the \nsame MinMaxScaler in the same range 0-1, so \nthe degree to which it affects our target variable \nwill become equal and will avoid variables \nfrom being biased because of large range \nvalues.","645f0ae1":"First, we checked for which K, our model has \nthe best accuracy. We performed KNN two \ntimes, once with normal train and test sets and \na second time with transformed \n(MinMaxScaler) train and test sets.\n1. During our first model, we found results as:\n* K: 28\n* Accuracy: 78.56 %\n2. During our second model of KNN with transformed sets:\n* K: 29\n* Accuracy: 81.78 %\n3. After plotting confusion matrix with transformed sets, we were able to get:\n* Accuracy: 81.78 % \n* Precision: 63.40 %\n* Recall: 33.06 %\n\nShows ROC Curve and it can be \nobserved that \u201cArea Under the Curve\u201d is 0.75. We can observe a decent ROC curve.","d201e12e":"# **Machine Learning Models:**\nThis project will use 5 Machine Learning models:\n# **1. KNN:**","1bc379aa":"# **4. Random Forest:**","91df76d8":"> ***Checking for which K, model is generating more accuracy. Using normal train and test data.***","2a79cce1":"> ***Simple Random Forest with transformed data:***","c66b9e8c":"> ***Pruned Tree applying model with transformed training and testing data:***","dcbb7a41":"**Confusion Matrix of model with transformed data:**","e287f226":"# **Data Wrangling:**","89602bb9":"# **3. Decision Tree:**","c0d35ab7":"> ***Hyperparameter Tuned Random Forest with transformed data:***","02215d56":"Firstly, Null values need to be checked, \nhowever, after checking this dataset, there are \nno null values. Then, column ID has been \nremoved from the data because it contains just \na normal serial number of observations."}}