{"cell_type":{"ee274873":"code","9efd2c77":"code","c859fa87":"code","a6eb6bb2":"code","1b6f4b76":"code","2bcbc4fb":"code","5c90447e":"code","ce217315":"code","1c3b8372":"code","c2330874":"code","876b98c4":"code","516e3cc7":"code","4615cf96":"code","fa365a3c":"code","0eff2beb":"code","cedab865":"code","9d243658":"code","8dda4689":"code","6c1bbac6":"code","7cd6fbd7":"code","52c0395a":"code","4a2dcdcb":"code","c32cbd6a":"code","4cbf26ca":"code","e33d2864":"code","2e7f2e37":"code","c85cd957":"code","c82181f2":"code","0194a66f":"code","39afc4d5":"code","92923d80":"code","972de0bc":"code","4df0dbf7":"code","cbfb53f5":"code","f3573a55":"code","4b097f3d":"code","bfd10c4c":"code","d3bbbb0f":"code","3394224c":"code","52486bf1":"markdown","c2875d79":"markdown","01ca9d93":"markdown","e02de967":"markdown","d2171f28":"markdown","4e030cb1":"markdown","bb5b3584":"markdown","a2d45150":"markdown","a861e4d6":"markdown","b5b39275":"markdown","5ba2122f":"markdown","6b08cc28":"markdown","0b4a68eb":"markdown","c8268b37":"markdown","ebf00512":"markdown","b777f181":"markdown","4a7f7580":"markdown","380f2ab4":"markdown","c72c188d":"markdown"},"source":{"ee274873":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","9efd2c77":"data = pd.read_csv('..\/input\/mice-protein-expression\/Data_Cortex_Nuclear.csv')\ndf = data.copy()\npd.set_option('display.max_row',df.shape[0])\npd.set_option('display.max_column',df.shape[1]) \ndf.head()","c859fa87":"df.dtypes.value_counts() # Compte les nombre de types de variables","a6eb6bb2":"print('There is' , df.shape[0] , 'rows')\nprint('There is' , df.shape[1] , 'columns')","1b6f4b76":"plt.figure(figsize=(10,10))\nsns.heatmap(df.isna(),cbar=False)\nplt.show()","2bcbc4fb":"(df.isna().sum()\/df.shape[0]*100).sort_values(ascending=False)","5c90447e":"exploitable = df.columns[df.isna().sum()\/df.shape[0]< 0.70 ] #Colonnes du dataframe o\u00f9 le pourcentage de NaN inf\u00e9rieur \u00e0 XXXXXXX %\ndf = df[exploitable]\ndf.head()","ce217315":"df['class'].value_counts(normalize=True) #Classes d\u00e9s\u00e9quilibr\u00e9es","1c3b8372":"for col in df.select_dtypes(include=['float64','int64']):\n    plt.figure()\n    sns.displot(df[col],kind='kde',height=3)\n    plt.show()","c2330874":"for col in df.select_dtypes(\"object\"):\n    plt.figure()\n    df[col].value_counts().plot.pie()\n    plt.show()","876b98c4":"for col in df.select_dtypes(\"object\"):\n    print(f'{col :-<50} {df[col].unique()}')","516e3cc7":"def encoding(df):\n    code = {'Control':1,\n            'Ts65Dn':0,\n            'Memantine':1,\n            'Saline':0,\n            'C\/S':0,\n            'S\/C':1,\n            'c-CS-m':0,\n            'c-SC-m':1,\n            'c-CS-s':2,\n            'c-SC-s':3,\n            't-CS-m':4,\n            't-SC-m':5,\n            't-CS-s':6,\n            't-SC-s':7,\n           }\n    for col in df.select_dtypes('object'):\n        df.loc[:,col]=df[col].map(code)\n        \n    return df\n\ndef imputation(df):\n    \n    #df = df.dropna(axis=0)\n    df = df.fillna(df.mean())\n    \n    return df\n\ndef feature_engineering(df):\n    useless_columns = ['MouseID']\n    for feature in useless_columns:\n        if feature in df:\n            df = df.drop(feature,axis=1)\n    return df","4615cf96":"def preprocessing(df):\n    df = encoding(df)\n    df = feature_engineering(df)\n    df = imputation(df)\n    \n    X = df.drop('class',axis=1)\n    y = df['class'].astype(int)\n      \n    return df,X,y","fa365a3c":"df=data.copy()\ndf,X,y = preprocessing(df)\ndf.head()","0eff2beb":"c_CS_m = df[y == 0]\nc_SC_m = df[y == 1]\nc_CS_s = df[y == 2]\nc_SC_s = df[y == 3]\nt_cs_m = df[y == 4]\nt_SC_m = df[y == 5]\nt_CS_s = df[y == 6]\nt_SC_s = df[y == 7]","cedab865":"corr = df.corr(method='pearson').abs()\n\nfig = plt.figure(figsize=(30,20))\nsns.heatmap(corr, annot=True, cmap='tab10', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()","9d243658":"df.corr()['class'].abs().sort_values()","8dda4689":"for col in df.columns:\n    plt.figure(figsize=(4,4))\n    sns.distplot(c_CS_m[col],label='c_CS_m')\n    sns.distplot(c_SC_m[col],label='c_SC_m')\n    sns.distplot(c_CS_s[col],label='c_CS_s')\n    sns.distplot(c_SC_s[col],label='c_SC_s')\n    sns.distplot(t_cs_m[col],label='t_cs_m')\n    sns.distplot(t_SC_m[col],label='t_SC_m')\n    sns.distplot(t_CS_s[col],label='t_CS_s')\n    sns.distplot(t_SC_s[col],label='t_SC_s')\n    plt.legend()\n    plt.show()","6c1bbac6":"from sklearn.model_selection import train_test_split\ndf = data.copy()\ntrainset, testset = train_test_split(df, test_size=0.2, random_state=0)\nprint(trainset['class'].value_counts())\nprint(testset['class'].value_counts())","7cd6fbd7":"_, X_train, y_train = preprocessing(trainset)\n_, X_test, y_test = preprocessing(testset)","52c0395a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA","4a2dcdcb":"preprocessor = make_pipeline(StandardScaler())\n\nPCAPipeline = make_pipeline(preprocessor, PCA(n_components=2,random_state=0))\n\nRandomPipeline = make_pipeline(preprocessor,RandomForestClassifier(random_state=0))\nAdaPipeline = make_pipeline(preprocessor,AdaBoostClassifier(random_state=0))\nSVMPipeline = make_pipeline(preprocessor,SVC(random_state=0,probability=True))\nKNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier())\nLRPipeline = make_pipeline(preprocessor,LogisticRegression(solver='sag'))","c32cbd6a":"PCA_df = pd.DataFrame(PCAPipeline.fit_transform(X))\nPCA_df = pd.concat([PCA_df, data['class']], axis=1)\nPCA_df.head()","4cbf26ca":"plt.figure(figsize=(8,8))\nsns.scatterplot(PCA_df[0],PCA_df[1],hue=PCA_df['class'],palette=sns.color_palette(\"Paired\", 8))\nplt.show()","e33d2864":"dict_of_models = {'RandomForest': RandomPipeline,\n'AdaBoost': AdaPipeline,\n'SVM': SVMPipeline,\n'KNN': KNNPipeline,\n'LR': LRPipeline}","2e7f2e37":"from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve\nfrom sklearn.model_selection import learning_curve, cross_val_score, GridSearchCV\n\ndef evaluation(model):\n    model.fit(X_train, y_train)\n    # calculating the probabilities\n    y_pred_proba = model.predict_proba(X_test)\n\n    # finding the predicted valued\n    y_pred = np.argmax(y_pred_proba,axis=1)\n    print('Accuracy = ', accuracy_score(y_test, y_pred))\n    print('-')\n    print(confusion_matrix(y_test,y_pred))\n    print('-')\n    print(classification_report(y_test,y_pred))\n    print('-')\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train, cv=4, scoring='accuracy', train_sizes=np.linspace(0.1,1,10))\n    \n    plt.figure(figsize=(8,6))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()","c85cd957":"for name, model in dict_of_models.items():\n    print('---------------------------------')\n    print(name)\n    evaluation(model)","c82181f2":"Control_df = data.loc[data['class'].str.startswith('c', na=False)]\nTrisomy_df = data.loc[data['class'].str.startswith('t', na=False)]\nprint(Trisomy_df['class'].unique())","0194a66f":"Control_df,X,y = preprocessing(Control_df)\ntrainset, testset = train_test_split(Control_df, test_size=0.2, random_state=0)\nprint(trainset['class'].value_counts())\nprint(testset['class'].value_counts())","39afc4d5":"_, X_train, y_train = preprocessing(trainset)\n_, X_test, y_test = preprocessing(testset)","92923d80":"PCA_df = pd.DataFrame(PCAPipeline.fit_transform(X))\nPCA_df.reset_index(drop=True, inplace=True)\ny.reset_index(drop=True, inplace=True)\nPCA_df = pd.concat([PCA_df, y], axis=1)\nPCA_df.head()","972de0bc":"plt.figure(figsize=(8,8))\nsns.scatterplot(PCA_df[0],PCA_df[1],hue=PCA_df['class'],palette=sns.color_palette(\"Paired\", 4))\nplt.show()","4df0dbf7":"for name, model in dict_of_models.items():\n    print('---------------------------------')\n    print(name)\n    evaluation(model)","cbfb53f5":"Trisomy_df,X,y = preprocessing(Trisomy_df)\ntrainset, testset = train_test_split(Trisomy_df, test_size=0.2, random_state=0)\nprint(trainset['class'].value_counts())\nprint(testset['class'].value_counts())","f3573a55":"_, X_train, y_train = preprocessing(trainset)\n_, X_test, y_test = preprocessing(testset)","4b097f3d":"y_train.head()","bfd10c4c":"PCA_df = pd.DataFrame(PCAPipeline.fit_transform(X))\nPCA_df.reset_index(drop=True, inplace=True)\ny.reset_index(drop=True, inplace=True)\nPCA_df = pd.concat([PCA_df, y], axis=1)\nPCA_df.head()","d3bbbb0f":"plt.figure(figsize=(8,8))\nsns.scatterplot(PCA_df[0],PCA_df[1],hue=PCA_df['class'],palette=sns.color_palette(\"Paired\", 4))\nplt.show()","3394224c":"for name, model in dict_of_models.items():\n    print('---------------------------------')\n    print(name)\n    evaluation(model)","52486bf1":"# Idea 2 : Separate the data in 4 groups : \"CS-M\" \"CS-S\" \"SC-M\" \"SC-S\"\n\n#### The idea here is to identify wheter the mouse is trisomic or not\n\n**(H0 : The mouse has been injected with saline)**\n\n**(H1 : The mouse is stimulated to learn)**","c2875d79":"# Exploratory Data Analysis\n\n## Aim :\n- Understand the data (\"A small step forward is better than a big one backwards\")\n- Begin to develop a modelling strategy\n\n## Target\nClasses:\n\nc-CS-s: control mice, stimulated to learn, injected with saline (9 mice)\n    \nc-CS-m: control mice, stimulated to learn, injected with memantine (10 mice)\n    \nc-SC-s: control mice, not stimulated to learn, injected with saline (9 mice)\n    \nc-SC-m: control mice, not stimulated to learn, injected with memantine (10 mice)\n    \nt-CS-s: trisomy mice, stimulated to learn, injected with saline (7 mice)\n    \nt-CS-m: trisomy mice, stimulated to learn, injected with memantine (9 mice)\n    \nt-SC-s: trisomy mice, not stimulated to learn, injected with saline (9 mice)\n    \nt-SC-m: trisomy mice, not stimulated to learn, injected with memantine (9 mice)\n\n## Features\n\n[1] Mouse ID\n\n[2:78] Values of expression levels of 77 proteins; the names of proteins are followed by N indicating that they were measured in the nuclear fraction. For example: DYRK1A_n\n\n[79] Genotype: control (c) or trisomy (t)\n\n[80] Treatment type: memantine (m) or saline (s)\n\n[81] Behavior: context-shock (CS) or shock-context (SC)\n\n[82] Class: c-CS-s, c-CS-m, c-SC-s, c-SC-m, t-CS-s, t-CS-m, t-SC-s, t-SC-m\n\n## Base Checklist\n#### Shape Analysis :\n- **target feature** : Class\n- **rows and columns** : 1080 , 82\n- **features types** : qualitatives : 5 , quantitatives : 77\n- **NaN analysis** :\n    - NaN (5 features > 15 % of NaN (all others < 5%))\n\n#### Columns Analysis :\n- **Target Analysis** :\n    - Balanced (Yes\/No) : Yes\n    - Percentages : ~12.5% for each class\n- **Categorical values**\n    - There is 4 categorical features (not inluding the target)","01ca9d93":"## Trisomy mice \"T-\" (Work in progress)","e02de967":"## Control mice \"C-\"","d2171f28":"# Modelling","4e030cb1":"### PCA Analysis","bb5b3584":"<h1><center>\ud83d\udc01Mice Trisomy Data Analysis\ud83d\udd0e<\/center><\/h1>\n<h3><center>\ud83e\uddec(Prediction at the end)\ud83d\udd2e<\/center><\/h3>\n<center><img src= \"https:\/\/www.jax.org\/~\/media\/JaxWeb\/images\/jax-mice-and-services\/mice\/datasheets\/001924\" alt =\"Titanic\" style='width: 600px;'><\/center>\n\n<h3>Context<\/h3>\n<p>\n\nExpression levels of 77 proteins measured in the cerebral cortex of 8 classes of control and Down syndrome mice exposed to context fear conditioning, a task used to assess associative learning.\n\n<\/p>\n\n<h3>Content<\/h3>\n<p>\n\nThe data set consists of the expression levels of 77 proteins\/protein modifications that produced detectable signals in the nuclear fraction of cortex. There are 38 control mice and 34 trisomic mice (Down syndrome), for a total of 72 mice. In the experiments, 15 measurements were registered of each protein per sample\/mouse. Therefore, for control mice, there are 38x15, or 570 measurements, and for trisomic mice, there are 34x15, or 510 measurements. The dataset contains a total of 1080 measurements per protein. Each measurement can be considered as an independent sample\/mouse.\n\nThe eight classes of mice are described based on features such as genotype, behavior and treatment. According to genotype, mice can be control or trisomic. According to behavior, some mice have been stimulated to learn (context-shock) and others have not (shock-context) and in order to assess the effect of the drug memantine in recovering the ability to learn in trisomic mice, some mice have been injected with the drug and others have not.\n<\/p>\n\n","a2d45150":"## Models evalutation","a861e4d6":"## Models evalutation","b5b39275":"# If you like please upvote !\n## Also check my other notebooks :\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83d\udc01Mice Trisomy (100% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-mice-100-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83c\udf97\ufe0fBreast Cancer Detection : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-breast-cancer-detection\n#### \ud83c\udf26\ud83c\udf21 Weather Forecasting \ud83d\udcc8 (98% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/weather-forecasting-98-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Heart Attack \ud83e\ude7a\ud83d\udc93 (90% Acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-heart-attack-90-accuracy-score\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Mobile price (95.5% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-95-5-acc-mobile-price\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83e\udde0 Stroke (74% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-stroke-74-acc","5ba2122f":"# Mid-conclusion : 100% Accuracy on most models\n\nFor the 5 models tested hereabove, here are the accuracies :\n- KNN : 98%\n- SVM \/ RandomForest \/ Adaboost \/ LogisticRegression : 100%\n","6b08cc28":"## Classification problem","0b4a68eb":"> Incoming","c8268b37":"# Detailed analysis","ebf00512":"# Idea 1 : Separate the data in 2 groups : Control mice \"C-\" and Trisomy mice \"T-\"\n\n#### The idea here is to identify what was injected to the mouse and wether it is stimulated to learn or not\n**(H0 : The mouse is not trisomic)**\n","b777f181":"## PCA Analysis","4a7f7580":"### PCA Analysis","380f2ab4":"## Examining target and features","c72c188d":"# A bit of data engineering ..."}}