{"cell_type":{"83c1281d":"code","8bf1cde2":"code","5e53eb5f":"code","25363066":"code","4afa6b71":"code","525944f2":"code","4754bddc":"code","f84eaf7c":"code","01bb7715":"code","f6d3430d":"code","4c6271a2":"code","673cd9d5":"code","bdc1f9a4":"code","4b1ec83d":"code","25c2c1f5":"code","ecf5eb4c":"code","3b9d06d2":"code","471b3231":"code","7de1e8c8":"code","8017bb10":"code","ecb696f2":"code","8b0c8add":"code","0bcb2f08":"code","7eebbaaf":"code","b13f820f":"code","20194e90":"code","814c39c8":"code","e4d982d6":"code","2873b67e":"code","d32c532a":"code","57b33450":"code","83a3dc14":"code","ed952e39":"code","ddc549f5":"code","f746503e":"code","e83f9d2c":"code","2d471bba":"code","0dc408e9":"code","02bd0b54":"code","39ab1d47":"code","9b0f93de":"code","5f99ffb5":"code","c490315f":"code","99db6812":"code","45f22817":"code","d1271d6e":"code","634ad3e1":"code","04b8beba":"code","885e1cbf":"code","9434a6b8":"code","5bf68ea2":"code","b12908b3":"code","f3d547e6":"code","aac04813":"code","a55256ee":"code","d6a910e2":"code","8354eaf0":"code","e1b1c6ef":"code","66f7df26":"code","b3caa9c6":"code","b58e15e8":"code","24cbd664":"code","7b9cd909":"code","689b76af":"code","6e9740e1":"code","86b43034":"code","098753fc":"markdown","e1ab6e66":"markdown","ac677b46":"markdown","ae1a5446":"markdown","61d08fa1":"markdown","4f97718b":"markdown","6833d0f8":"markdown","bfdebf10":"markdown","cd58afcb":"markdown","0a2b4578":"markdown","fba2f7ef":"markdown","ce7d9572":"markdown"},"source":{"83c1281d":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8bf1cde2":"#Data_train\ndf_train = pd.read_csv('train.csv')\n#Data_test\ndf_test = pd.read_csv('test.csv')\n\n#Data (Train + Test)\ndata = pd.concat((df_train, df_test)).reset_index(drop=True)\ndata.drop(['SalePrice'], axis=1, inplace=True)\n\ndf_train.head()","5e53eb5f":"#Different types of the features\ndf_train.dtypes","25363066":"#correlation matrix\ncorrmat = df_train.corr()\n\n#Plot a heatmap to visualize the correlations\nf, ax = plt.subplots(figsize=(30, 19))\nsns.set(font_scale=1.45)\nsns.heatmap(corrmat, square=True,cmap='coolwarm');","4afa6b71":"correlations = corrmat[\"SalePrice\"].sort_values(ascending=False)\nfeatures = correlations.index[0:10]\nfeatures","525944f2":"df_train.drop(['Id'], axis=1, inplace=True)\ndf_test.drop(['Id'], axis=1, inplace=True)\ndata.drop(['Id'], axis=1, inplace=True)","4754bddc":"training_null = pd.isnull(df_train).sum()\ntesting_null = pd.isnull(df_test).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])","f84eaf7c":"null","01bb7715":"#Based on the description data file provided, all the variables who have meaningfull Nan\n\nnull_with_meaning = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]","f6d3430d":"#Replacing every Nan value with \"None\"\n\nfor i in null_with_meaning:\n    df_train[i].fillna(\"None\", inplace=True)\n    df_test[i].fillna(\"None\", inplace=True)\n    data[i].fillna(\"None\", inplace=True)","4c6271a2":"null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #few missing values\nnull_many","673cd9d5":"df_train.drop(\"LotFrontage\", axis=1, inplace=True)\ndf_test.drop(\"LotFrontage\", axis=1, inplace=True)\ndata.drop(\"LotFrontage\", axis=1, inplace=True)","bdc1f9a4":"null_few","4b1ec83d":"#I chose to use the mean function for replacement\ndf_train[\"GarageYrBlt\"].fillna(df_train[\"GarageYrBlt\"].mean(), inplace=True)\ndf_test[\"GarageYrBlt\"].fillna(df_test[\"GarageYrBlt\"].mean(), inplace=True)\ndata[\"GarageYrBlt\"].fillna(data[\"GarageYrBlt\"].mean(), inplace=True)\ndf_train[\"MasVnrArea\"].fillna(df_train[\"MasVnrArea\"].mean(), inplace=True)\ndf_test[\"MasVnrArea\"].fillna(df_test[\"MasVnrArea\"].mean(), inplace=True)\ndata[\"MasVnrArea\"].fillna(data[\"MasVnrArea\"].mean(), inplace=True)\n\ndf_train[\"MasVnrType\"].fillna(\"None\", inplace=True)\ndf_test[\"MasVnrType\"].fillna(\"None\", inplace=True)\ndata[\"MasVnrType\"].fillna(\"None\", inplace=True)","25c2c1f5":"types_train = df_train.dtypes #type of each feature in data: int, float, object\nnum_train = types_train[(types_train == int) | (types_train == float)] #numerical values are either type int or float\ncat_train = types_train[types_train == object] #categorical values are type object\n\n#we do the same for the test set\ntypes_test = df_test.dtypes\nnum_test = types_test[(types_test == int) | (types_test == float)]\ncat_test = types_test[types_test == object]","ecf5eb4c":"#we should convert num_train and num_test to a list to make it easier to work with\nnumerical_values_train = list(num_train.index)\nnumerical_values_test = list(num_test.index)\nfill_num = numerical_values_train+numerical_values_test\n\nprint(fill_num)","3b9d06d2":"for i in fill_num:\n    df_train[i].fillna(df_train[i].mean(), inplace=True)\n    df_test[i].fillna(df_test[i].mean(), inplace=True)\n    data[i].fillna(data[i].mean(), inplace=True)","471b3231":"df_train.shape, df_test.shape","7de1e8c8":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)","8017bb10":"fill_cat = []\n\nfor i in categorical_values_train:\n    if i in list(null_few.index):\n        fill_cat.append(i)\nprint(fill_cat)","ecb696f2":"def most_common_term(lst):\n    lst = list(lst)\n    return max(set(lst), key=lst.count)\n#most_common_term finds the most common term in a series\n\nmost_common = []\n\nfor i in fill_cat:\n    most_common.append(most_common_term(data[i]))\n    \nmost_common","8b0c8add":"most_common_dictionary = {fill_cat[0]: [most_common[0]], fill_cat[1]: [most_common[1]], fill_cat[2]: [most_common[2]], fill_cat[3]: [most_common[3]],\n                          fill_cat[4]: [most_common[4]], fill_cat[5]: [most_common[5]], fill_cat[6]: [most_common[6]], fill_cat[7]: [most_common[7]],\n                          fill_cat[8]: [most_common[8]]}\nmost_common_dictionary","0bcb2f08":"k = 0\nfor i in fill_cat:  \n    df_train[i].fillna(most_common[k], inplace=True)\n    df_test[i].fillna(most_common[k], inplace=True)\n    data[i].fillna(most_common[k], inplace=True)\n    k += 1","7eebbaaf":"training_null = pd.isnull(df_train).sum()\ntesting_null = pd.isnull(df_test).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])\nnull[null.sum(axis=1) > 0]","b13f820f":"(np.log(df_train[\"SalePrice\"])).hist(bins = 40)","20194e90":"df_train[\"LogPrice\"] = np.log(df_train[\"SalePrice\"])\ndf_train.head()","814c39c8":"#Importing all the librairies we'll need\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, KFold","e4d982d6":"X_train = df_train_add.drop([\"SalePrice\",\"LogPrice\"], axis=1)\ny_train = df_train_add[\"LogPrice\"]","2873b67e":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","d32c532a":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_training,y_training)\nprint(lm)","57b33450":"# print the intercept\nprint(lm.intercept_)","83a3dc14":"print(lm.coef_)","ed952e39":"predictions = lm.predict(X_valid)\npredictions= predictions.reshape(-1,1)","ddc549f5":"submission_predictions = np.exp(predictions)","f746503e":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_valid, submission_predictions))\nprint('MSE:', metrics.mean_squared_error(y_valid, submission_predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, submission_predictions)))","e83f9d2c":"linreg = LinearRegression()\nparameters_lin = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False]}\ngrid_linreg = GridSearchCV(linreg, parameters_lin, verbose=1 , scoring = \"r2\")\ngrid_linreg.fit(X_training, y_training)\n\nprint(\"Best LinReg Model: \" + str(grid_linreg.best_estimator_))\nprint(\"Best Score: \" + str(grid_linreg.best_score_))","2d471bba":"linreg = grid_linreg.best_estimator_\nlinreg.fit(X_training, y_training)\nlin_pred = linreg.predict(X_valid)\nr2_lin = r2_score(y_valid, lin_pred)\nrmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\nprint(\"R^2 Score: \" + str(r2_lin))\nprint(\"RMSE Score: \" + str(rmse_lin))","0dc408e9":"scores_lin = cross_val_score(linreg, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lin)))","02bd0b54":"ridge = Ridge()\nparameters_ridge = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False], \"solver\" : [\"auto\"]}\ngrid_ridge = GridSearchCV(ridge, parameters_ridge, verbose=1, scoring=\"r2\")\ngrid_ridge.fit(X_training, y_training)\n\nprint(\"Best Ridge Model: \" + str(grid_ridge.best_estimator_))\nprint(\"Best Score: \" + str(grid_ridge.best_score_))","39ab1d47":"ridge = grid_ridge.best_estimator_\nridge.fit(X_training, y_training)\nridge_pred = ridge.predict(X_valid)\nr2_ridge = r2_score(y_valid, ridge_pred)\nrmse_ridge = np.sqrt(mean_squared_error(y_valid, ridge_pred))\nprint(\"R^2 Score: \" + str(r2_ridge))\nprint(\"RMSE Score: \" + str(rmse_ridge))","9b0f93de":"scores_ridge = cross_val_score(ridge, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_ridge)))","5f99ffb5":"from sklearn import ensemble","c490315f":"params = {'n_estimators': 20000, 'max_depth': 5, 'min_samples_split': 2,\n          'learning_rate': 0.05, 'loss': 'ls' , 'max_features' : 20}\nclf = ensemble.GradientBoostingRegressor(**params)\n\nclf.fit(X_training, y_training)","99db6812":"clf_pred=clf.predict(X_valid)\nclf_pred= clf_pred.reshape(-1,1)\nr2_clf = r2_score(y_valid, clf_pred)\nrmse_clf = np.sqrt(mean_squared_error(y_valid, clf_pred))\nprint(\"R^2 Score: \" + str(r2_clf))\nprint(\"RMSE Score: \" + str(rmse_clf))","45f22817":"scores_clf = cross_val_score(clf, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_clf)))","d1271d6e":"from sklearn.tree import DecisionTreeRegressor\ndtreg = DecisionTreeRegressor(random_state = 100)\nparameters_dtr = {\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"], \"splitter\" : [\"best\", \"random\"], \"min_samples_split\" : [2, 3, 5, 10], \n                  \"max_features\" : [\"auto\", \"log2\"]}\ngrid_dtr = GridSearchCV(dtreg, parameters_dtr, verbose=1, scoring=\"r2\")\ngrid_dtr.fit(X_training, y_training)\n\nprint(\"Best DecisionTreeRegressor Model: \" + str(grid_dtr.best_estimator_))\nprint(\"Best Score: \" + str(grid_dtr.best_score_))","634ad3e1":"dtr = grid_dtr.best_estimator_\ndtreg.fit(X_training, y_training)\ndtr_pred = dtreg.predict(X_valid)\nr2_dtr = r2_score(y_valid, dtr_pred)\nrmse_dtr = np.sqrt(mean_squared_error(y_valid, dtr_pred))\nprint(\"R^2 Score: \" + str(r2_dtr))\nprint(\"RMSE Score: \" + str(rmse_dtr))","04b8beba":"scores_dtr = cross_val_score(dtreg, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))","885e1cbf":"rfr = RandomForestRegressor()\nparemeters_rf = {\"n_estimators\" : [5, 10, 15, 20], \"criterion\" : [\"mse\" , \"mae\"], \"min_samples_split\" : [2, 3, 5, 10], \n                 \"max_features\" : [\"auto\", \"log2\"]}\ngrid_rf = GridSearchCV(rfr, paremeters_rf, verbose=1, scoring=\"r2\")\ngrid_rf.fit(X_training, y_training)\n\nprint(\"Best RandomForestRegressor Model: \" + str(grid_rf.best_estimator_))\nprint(\"Best Score: \" + str(grid_rf.best_score_))","9434a6b8":"rf = grid_rf.best_estimator_\nrfr.fit(X_training, y_training)\nrf_pred = rfr.predict(X_valid)\nr2_rf = r2_score(y_valid, rf_pred)\nrmse_rf = np.sqrt(mean_squared_error(y_valid, rf_pred))\nprint(\"R^2 Score: \" + str(r2_rf))\nprint(\"RMSE Score: \" + str(rmse_rf))","5bf68ea2":"scores_rf = cross_val_score(rfr, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_rf)))","b12908b3":"from xgboost import XGBRegressor\n\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=20000,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.006)\nxgb = xgboost.fit(X_training, y_training)","f3d547e6":"xgb_pred = xgb.predict(X_valid)\nr2_xgb = r2_score(y_valid, xgb_pred)\nrmse_xgb = np.sqrt(mean_squared_error(y_valid, xgb_pred))\nprint(\"R^2 Score: \" + str(r2_xgb))\nprint(\"RMSE Score: \" + str(rmse_xgb))","aac04813":"from lightgbm import LGBMRegressor\n\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=20000,\n                                       max_bin=2000, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\ngbm = lightgbm.fit(X_training, y_training)","a55256ee":"gbm_pred = gbm.predict(X_valid)\nr2_gbm = r2_score(y_valid, gbm_pred)\nrmse_gbm = np.sqrt(mean_squared_error(y_valid, gbm_pred))\nprint(\"R^2 Score: \" + str(r2_gbm))\nprint(\"RMSE Score: \" + str(rmse_gbm))","d6a910e2":"model_performances = pd.DataFrame({\n    \"Model\" : [\"Linear Regression\", \"Ridge\", \"Decision Tree Regressor\", \"Random Forest Regressor\",\"Gradient Boosting Regression\",\"XGBoost\",\"LGBM Regressor\"],\n    \"R Squared\" : [str(r2_lin)[0:5], str(r2_ridge)[0:5],  str(r2_dtr)[0:5], str(r2_rf)[0:5] , str(r2_clf)[0:5], str(r2_xgb)[0:5], str(r2_gbm)[0:5]],\n    \"RMSE\" : [str(rmse_lin)[0:8], str(rmse_ridge)[0:8],  str(rmse_dtr)[0:8], str(rmse_rf)[0:8], str(rmse_clf)[0:8], str(rmse_xgb)[0:8], str(rmse_gbm)[0:8]]\n})\nmodel_performances.round(4)\n\nprint(\"Sorted by R Squared:\")\nmodel_performances.sort_values(by=\"R Squared\", ascending=False)","8354eaf0":"print(\"Sorted by RMSE:\")\nmodel_performances.sort_values(by=\"RMSE\", ascending=True)","e1b1c6ef":"learning_rates = [0.75 ,0.5, 0.25, 0.1, 0.05, 0.01]\n\nr2_results = []\nrmse_results = []\n\nfor eta in learning_rates:\n    model = ensemble.GradientBoostingRegressor(learning_rate=eta)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(learning_rates, r2_results, 'b', label='R^2')\nline2, = plt.plot(learning_rates, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('learning_rates')\nplt.show()","66f7df26":"n_estimators = [1, 2, 16, 32, 64, 100, 200, 500]\nr2_results = []\nrmse_results = []\n\nfor estimator in n_estimators:\n    model = ensemble.GradientBoostingRegressor(n_estimators=estimator)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(n_estimators, r2_results, 'b', label='R^2')\nline2, = plt.plot(n_estimators, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('n_estimators')\nplt.show()","b3caa9c6":"max_depths = np.linspace(1, 10, 10, endpoint=True)\nr2_results = []\nrmse_results = []\n\nfor max_depth in max_depths:\n    model = ensemble.GradientBoostingRegressor(max_depth=max_depth)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, r2_results, 'b', label='R^2')\nline2, = plt.plot(max_depths, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('max_depths')\nplt.show()","b58e15e8":"min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\nr2_results = []\nrmse_results = []\n\nfor min_samples_split in min_samples_splits:\n    model = ensemble.GradientBoostingRegressor(min_samples_split=min_samples_split)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_splits, r2_results, 'b', label='R^2')\nline2, = plt.plot(min_samples_splits, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('min_samples_splits')\nplt.show()","24cbd664":"min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\nr2_results = []\nrmse_results = []\n\nfor min_samples_leaf in min_samples_leafs:\n    model = ensemble.GradientBoostingRegressor(min_samples_leaf=min_samples_leaf)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_leafs, r2_results, 'b', label='R^2')\nline2, = plt.plot(min_samples_leafs, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('min_samples_leafs')\nplt.show()","7b9cd909":"max_features = list(range(1,30))\nr2_results = []\nrmse_results = []\n\nfor max_feature in max_features:\n    model = ensemble.GradientBoostingRegressor(max_features=max_feature)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_features, r2_results, 'b', label='R^2')\nline2, = plt.plot(max_features, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('max_features')\nplt.show()","689b76af":"def blend_models_predict(X):\n    return ((0.05 * lm.predict(X)) + \\\n            (0.05 * linreg.predict(X)) + \\\n            (0.05 * ridge.predict(X)) + \\\n            (0.1 * clf.predict(X)) + \\\n            (0.2 * gbm.predict(X)) + \\\n            (0.15 * rfr.predict(X)) + \\\n            (0.4 * xgb.predict(X)))\n","6e9740e1":"submission_predictions = np.exp(blend_models_predict(df_test_add))\nprint(submission_predictions)","86b43034":"res=pd.DataFrame(columns = ['Id', 'SalePrice'])\nres['Id'] = df_test.index + 1461\nres['SalePrice'] = submission_predictions\nres.to_csv('submission1.csv',index=False)","098753fc":"## Decision Tree Regression","e1ab6e66":"### Splitting into Validation\n\nTry to split our training data again into validation sets. This will help us evaluate our model performance and maybe avoid overfitting.","ac677b46":"We can see that the feature LotFrontage has too many Null values, so it's better to just drop it.","ae1a5446":"**Adding the GridSearchCV function**","61d08fa1":"**Predictions from our Model** ","4f97718b":"## Random Forest Regression ","6833d0f8":"## Linear Regression Model","bfdebf10":"## Xgboost","cd58afcb":"## Ridge Model","0a2b4578":"**Model Evaluation**\nLet's evaluate the model by checking out it's coefficients and how we can interpret them.","fba2f7ef":"## Gradient Boosting Regression","ce7d9572":"### LGBM Regressor"}}