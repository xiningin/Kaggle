{"cell_type":{"a940766e":"code","64f45c38":"code","38fd4305":"code","9e6c9dcd":"code","53335a54":"code","64a9517a":"code","c6e2ec8c":"code","f3f37afa":"code","8555a198":"code","5084b92b":"code","39574207":"code","248e62a8":"code","f7d9e0a1":"code","05152d7f":"code","85b9f76b":"markdown","3b2b9b2b":"markdown","997df83b":"markdown","49a2d4f9":"markdown","66016afc":"markdown","e0d21059":"markdown","477487e2":"markdown","568afaf7":"markdown","a16df0c0":"markdown","8f9cc85a":"markdown"},"source":{"a940766e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64f45c38":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"openaq\" dataset\ndataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# List all the tables in the \"openaq\" dataset\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there's only one!)\nfor table in tables:  \n    print(table.table_id)","38fd4305":"# Construct a reference to the \"global_air_quality\" table\ntable_ref = dataset_ref.table(\"global_air_quality\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"global_air_quality\" table\nclient.list_rows(table, max_results = 5).to_dataframe()","9e6c9dcd":"query = \"\"\"\n        SELECT city \n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"","53335a54":"# Set up the query\nquery_request = client.query(query)","64a9517a":"# API request - run the query, and return a pandas DataFrame\nus_cities = query_request.to_dataframe()","c6e2ec8c":"print(us_cities)","f3f37afa":"# figuring out which cities have the most measurements\nus_cities.city.value_counts().head()","8555a198":"# creating a query to select multiple columns\nquery2 = \"\"\"\n         SELECT city, country\n         FROM `bigquery-public-data.openaq.global_air_quality`\n         WHERE country = 'US'\n         \"\"\"","5084b92b":"# creating a query to select all values using *\nquery3 = \"\"\"\n         SELECT *\n         FROM `bigquery-public-data.openaq.global_air_quality`\n         WHERE country = 'US'\n         \"\"\"","39574207":"# Query to get the score column from every row where the type column has value \"job\"\nquery = \"\"\"\n        SELECT score, title\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE type = \"job\" \n        \"\"\"\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run = True)\n\n# API request - dry run query to estimate costs\ndry_run_query_request = client.query(query, job_config = dry_run_config)\n\nprint(\"This query will process {} bytes.\".format(dry_run_query_request.total_bytes_processed))","248e62a8":"# Only run the query if it's less than a MB\none_mb = 1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed = one_mb)\n\n# Set up the query (will only run if it's less than 1 MB)\nsafe_query_job = client.query(query, job_config = safe_config)\n\n# API request - try to run the query, and return a pandas DataFrame\nsafe_query_job.to_dataframe()","f7d9e0a1":"# Only run the query if it's less than a GB\none_gb = 1000*1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed = one_gb)\n\n# Set up the query (will only run if it's less than 1 GB)\nsafe_query_job = client.query(query, job_config = safe_config)\n\n# API request - try to run the query, and return a pandas DataFrame\njob_post_scores = safe_query_job.to_dataframe()","05152d7f":"# Print average score for job posts\njob_post_scores.score.mean()","85b9f76b":"Simple `print` statement to take a look at some of the data points within the frame.","3b2b9b2b":"Now let's print the `mean` scores and play with the data a little!","997df83b":"These limit conditions can also be set within the *query* itself. We will need to create an **object** that specifies the limitation and that call on that object whil running the **query reference**.","49a2d4f9":"## SQL Practice 2\n\nJust some code to learn using SQL integrated within the *Kaggle environment*. ","66016afc":"Sometimes it's important to check how big the **size** of a query is before before running it. We can create a `query_job_config` and set the `dry_run` parameter to `TRUE`, which will give us an estimate. Let's use the very large `hacker_news` data set with the `full` table!","e0d21059":"Let's convert those results into a pandas data frame now!","477487e2":"Next, let's start *querying* the table itself to see if we can get any insights! Remember that **all queries** need to be names as objects within the notebook file so as to refer back to the *bigquery database*.","568afaf7":"Next we'll create a **table reference** for the specific table that we are trying to query, which will be `global_air_quality`.","a16df0c0":"The last query will **not** run since the size exceeded the 1 MB limitation, however we can increase the threshold to 1 GB to negate this issue!","8f9cc85a":"Next we have to create a *client* to reference back to *bigquery* and convert it into a `pandas` **data frame** right here in Kaggle."}}