{"cell_type":{"a84c4f99":"code","d0c5a7bc":"code","8dc77eed":"code","2448c7e6":"markdown","6c0980ac":"markdown"},"source":{"a84c4f99":"import torch","d0c5a7bc":"def dice_channel_torch(probability, truth, threshold):\n    batch_size = truth.shape[0]\n    channel_num = truth.shape[1]\n    mean_dice_channel = 0.\n    with torch.no_grad():\n        for i in range(batch_size):\n            for j in range(channel_num):\n                channel_dice = dice_single_channel(probability[i, j,:,:], truth[i, j, :, :], threshold)\n                mean_dice_channel += channel_dice\/(batch_size * channel_num)\n    return mean_dice_channel\n\n\ndef dice_single_channel(probability, truth, threshold, eps = 1E-9):\n    p = (probability.view(-1) > threshold).float()\n    t = (truth.view(-1) > 0.5).float()\n    dice = (2.0 * (p * t).sum() + eps)\/ (p.sum() + t.sum() + eps)\n    return dice","8dc77eed":"batchsize = 16\nchannel_num = 4\nprobability = torch.rand(batchsize,channel_num,256,1600)\ntruth = torch.ones(batchsize,channel_num,256,1600)\n\ndice = dice_channel_torch(probability, truth, 0.5)\n\nprint('Avg Dice score in this batch is {}'.format(dice))","2448c7e6":"Since there are many different dice calculation methods in Notebooks(a.k.a Kernels), I'm here to point out the right one.","6c0980ac":"This competition is evaluated on the **mean Dice coefficient**. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n\n$Dice(X,Y) = \\frac{2\u2217|X\\cap{Y}|}{|X|+|Y|}$\n\nwhere X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for **each** *<ImageId, ClassId>* pair in the test set.\n\nWhich means the seperate channel of each mask will be average to Dice score. So we have 1801x4 samples in test set, when we evaluate this dice score on a batch, we should calculate dice as follows:"}}