{"cell_type":{"b27e1fe9":"code","bddf166c":"code","d5aa8472":"code","04aeeaa4":"code","8d4f75fd":"code","590cca43":"code","6377ab32":"code","2a6a530b":"code","c857a065":"code","8710b309":"code","b6f5e74b":"code","2602ecb8":"code","816188c5":"code","f94b03fb":"code","fde5b95a":"code","0a7d2b27":"code","8f7e0094":"code","8e5cb06b":"code","a9c3d938":"code","7bb237d0":"code","2d49dbe5":"code","6da6baf7":"code","174506e1":"code","d0d97ad8":"code","7da4b2c0":"code","904dfb6d":"code","b1e56b66":"code","5b8d9fb3":"code","2c034f76":"markdown","4ea61915":"markdown","4894b9ca":"markdown","f685899f":"markdown","e0039d7e":"markdown","aae81470":"markdown","75572a60":"markdown","87cc3c57":"markdown","260bf738":"markdown","468b4df5":"markdown","fad7357b":"markdown","5d21b903":"markdown","b36320c6":"markdown","25993cd9":"markdown","075ba95f":"markdown","25bba7f4":"markdown","fa9819ac":"markdown","61bea1f0":"markdown","8c68964f":"markdown"},"source":{"b27e1fe9":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') \n%matplotlib inline\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom keras.optimizers import SGD\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional","bddf166c":"def plot(y_true, y_prediction):\n    plt.plot(y_true, color='red', label='Actual Stock Prices')\n    plt.plot(y_prediction, color='blue', label='Predicted Stock Prices')\n    plt.title(str.capitalize(key))\n    plt.xlabel('Time')\n    plt.ylabel('Stock Price')\n    plt.legend()\n    plt.show()","d5aa8472":"dataset = pd.read_csv(\"..\/input\/stock-time-series-20050101-to-20171231\/all_stocks_2006-01-01_to_2018-01-01.csv\", index_col=\"Date\")","04aeeaa4":"print(dataset.shape)","8d4f75fd":"print(dataset.Name.unique())","590cca43":"microsoft = dataset[dataset['Name']=='MSFT']\ngoogle = dataset[dataset['Name']=='GOOGL']\napple = dataset[dataset['Name']=='AAPL']\namazon = dataset[dataset['Name']=='AMZN']\nibm = dataset[dataset['Name']=='IBM']","6377ab32":"print(dataset.duplicated().sum())","2a6a530b":"stocks = [microsoft, google, apple, amazon, ibm]\nfor i in stocks:\n    print('\\033[1mNULL VALUES\\033[0m\\n'+ str(i.isnull().sum()))","c857a065":"ibm.fillna(method='ffill', inplace=True)","8710b309":"print(ibm.isnull().sum().any())","b6f5e74b":"microsoft.plot(subplots=True, figsize=(10,12))\nplt.suptitle('Microsoft stock values from 2006 to 2018', fontsize=15)\nplt.show()","2602ecb8":"google.plot(subplots=True, figsize=(10,12))\nplt.suptitle('Google stock values from 2006 to 2018', fontsize=15)\nplt.show()","816188c5":"apple.plot(subplots=True, figsize=(10,12))\nplt.suptitle('Apple stock values from 2006 to 2018', fontsize=15)\nplt.show()","f94b03fb":"amazon.plot(subplots=True, figsize=(10,12))\nplt.suptitle('Amazon stock values from 2006 to 2018', fontsize=15)\nplt.show()","fde5b95a":"ibm.plot(subplots=True, figsize=(10,12))\nplt.suptitle('Ibm stock values from 2006 to 2018', fontsize=15)\nplt.show()","0a7d2b27":"plt.figure(figsize=(25,25))\n\nplt.subplot(5,1,1)\nmicrosoft.High.pct_change().mul(100).plot()\nplt.title(\"Microsoft Daily Highest Price Variation in Stocks\")\n\nplt.subplot(5,1,2)\ngoogle.High.pct_change().mul(100).plot()\nplt.title(\"Google Daily Highest Price Variation in Stocks\")\n\nplt.subplot(5,1,3)\napple.High.pct_change().mul(100).plot()\nplt.title(\"Apple Daily Highest Price Variation in Stocks\")\n\nplt.subplot(5,1,4)\namazon.High.pct_change().mul(100).plot()\nplt.title(\"Amazon Daily Highest Price Variation in Stocks\")\n\nplt.subplot(5,1,5)\nibm.High.pct_change().mul(100).plot()\nplt.title(\"IBM Daily Highest Price Variation in Stocks\")\n\nplt.show()","8f7e0094":"microsoft[\"High\"].plot(figsize=(16,4),legend=True)\ngoogle[\"High\"].plot(figsize=(16,4),legend=True)\napple[\"High\"].plot(figsize=(16,4),legend=True)\namazon[\"High\"].plot(figsize=(16,4),legend=True)\nibm[\"High\"].plot(figsize=(16,4),legend=True)\nplt.legend(['Microsoft','Google', 'Apple', 'Amazon', 'IBM'])\nplt.show()","8e5cb06b":"microsoft[\"Low\"].plot(figsize=(16,4),legend=True)\ngoogle[\"Low\"].plot(figsize=(16,4),legend=True)\napple[\"Low\"].plot(figsize=(16,4),legend=True)\namazon[\"Low\"].plot(figsize=(16,4),legend=True)\nibm[\"Low\"].plot(figsize=(16,4),legend=True)\nplt.legend(['Microsoft','Google', 'Apple', 'Amazon', 'IBM'])\nplt.show()","a9c3d938":"def split(data):\n    return data[:'2016'].iloc[:, 1:2].values, data['2016':].iloc[:,1:2].values\n\nmicrosoft_train, microsoft_test = split(microsoft)\ngoogle_train, google_test = split(google)\napple_train, apple_test = split(apple)\namazon_train, amazon_test = split(amazon)\nibm_train, ibm_test = split(ibm)","7bb237d0":"print(\"\\033[1mShapes of train and test sets\\033[0m\\n\")\nprint(\"Microsoft train set:\", microsoft_train.shape, \"Microsoft test set:\", microsoft_test.shape)\nprint(\"Google train set:\", google_train.shape, \"Google test set:\", google_test.shape)\nprint(\"Apple train set:\", apple_train.shape, \"Apple test set:\", apple_test.shape)\nprint(\"Amazon train set:\", amazon_train.shape, \"Amazon test set:\", amazon_test.shape)\nprint(\"IBM train set:\", ibm_train.shape, \"IBM test set:\", ibm_test.shape)","2d49dbe5":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0, 1))\nmicrosoft_train_scaled, microsoft_test_scaled = sc.fit_transform(microsoft_train), sc.fit_transform(microsoft_test)\ngoogle_train_scaled, google_test_scaled = sc.fit_transform(google_train), sc.fit_transform(google_test)\napple_train_scaled, apple_test_scaled = sc.fit_transform(apple_train), sc.fit_transform(apple_test)\namazon_train_scaled, amazon_test_scaled = sc.fit_transform(amazon_train), sc.fit_transform(amazon_test)\nibm_train_scaled, ibm_test_scaled = sc.fit_transform(ibm_train), sc.fit_transform(ibm_test)","6da6baf7":"train_map = {\"microsoft\": microsoft_train_scaled, \"google\": google_train_scaled, \"apple\": apple_train_scaled, \"amazon\": amazon_train_scaled, \"ibm\": ibm_train_scaled}\ntrain_map_X = {\"microsoft\": [], \"google\": [], \"apple\": [], \"amazon\": [], \"ibm\": []}\ntrain_map_y = {\"microsoft\": [], \"google\": [], \"apple\": [], \"amazon\": [], \"ibm\": []}\n\nfor key in train_map.keys():\n    for i in range(60,2516):   \n        train_map_X[key].append(train_map[key][i-60:i,0])\n        train_map_y[key].append(train_map[key][i, 0])\n    train_map_X[key], train_map_y[key] = np.array(train_map_X[key]), np.array(train_map_y[key])\n    train_map_X[key] = np.reshape(train_map_X[key], (train_map_X[key].shape[0],train_map_X[key].shape[1],1))","174506e1":"for key in train_map_X.keys():\n    \n    # The LSTM architecture\n    lstm = Sequential()\n\n    # First LSTM layer with Dropout regularisation\n    lstm.add(LSTM(units=50, return_sequences=True, input_shape=(train_map_X[key].shape[1],1)))\n    lstm.add(Dropout(0.2))\n\n    # Second LSTM layer\n    lstm.add(LSTM(units=50, return_sequences=True))\n    lstm.add(Dropout(0.2))\n\n    # Third LSTM layer\n    lstm.add(LSTM(units=50, return_sequences=True))\n    lstm.add(Dropout(0.2))\n\n    # Fourth LSTM layer\n    lstm.add(LSTM(units=50))\n    lstm.add(Dropout(0.2))\n\n    # The output layer\n    lstm.add(Dense(units=1))\n\n    # Compiling the RNN\n    lstm.compile(optimizer='rmsprop',loss='mean_squared_error')\n    \n    # Fitting to the training set\n    print(\"Training:\", key)\n    lstm.fit(train_map_X[key],train_map_y[key],epochs=20,batch_size=32)\n","d0d97ad8":"test_map = {\"microsoft\": microsoft_test_scaled, \"google\": google_test_scaled, \"apple\": apple_test_scaled, \"amazon\": amazon_test_scaled, \"ibm\": ibm_test_scaled}\ntest_map_X = {\"microsoft\": [], \"google\": [], \"apple\": [], \"amazon\": [], \"ibm\": []}\ntest_map_y = {\"microsoft\": [], \"google\": [], \"apple\": [], \"amazon\": [], \"ibm\": []}\n\nfor key in test_map.keys():\n    for i in range(60,503):\n        test_map_X[key].append(test_map[key][i-60:i,0])\n        test_map_y[key].append(test_map[key][i, 0])\n    test_map_X[key], test_map_y[key]= np.array(test_map_X[key]), np.array(test_map_y[key])\n    test_map_X[key] = np.reshape(test_map_X[key], (test_map_X[key].shape[0], test_map_X[key].shape[1], 1))","7da4b2c0":"for key in test_map_X.keys():\n    y_true = sc.inverse_transform(test_map_y[key].reshape(-1,1)) \n    y_prediction = sc.inverse_transform(lstm.predict(test_map_X[key]))   \n    plot(y_true, y_prediction)\n    rmse = sqrt(mean_squared_error(y_true, y_prediction))\n    print(str.capitalize(key), \"Root Mean Squared Error:\", rmse)","904dfb6d":"for key in train_map_X.keys():\n    \n    # The GRU architecture\n    gru = Sequential()\n    \n    # First GRU layer with Dropout regularisation\n\n    gru.add(GRU(units=50, return_sequences=True, input_shape=(train_map_X[key].shape[1],1), activation='tanh'))\n    gru.add(Dropout(0.2))\n    \n    # Second GRU layer\n    gru.add(GRU(units=50, return_sequences=True, input_shape=(train_map_X[key].shape[1],1), activation='tanh'))\n    gru.add(Dropout(0.2))\n    \n    # Third GRU layer\n    gru.add(GRU(units=50, return_sequences=True, input_shape=(train_map_X[key].shape[1],1), activation='tanh'))\n    gru.add(Dropout(0.2))\n    \n    # Fourth GRU layer\n    gru.add(GRU(units=50, activation='tanh'))\n    gru.add(Dropout(0.2))\n    \n    # The output layer\n    gru.add(Dense(units=1))\n    \n    # Compiling the RNN\n    gru.compile(optimizer=SGD(),loss='mean_squared_error')\n    \n    # Fitting to the training set\n    print(\"Training:\", key)\n    gru.fit(train_map_X[key],train_map_y[key], epochs=20,batch_size=150)","b1e56b66":"test_map = {\"microsoft\": microsoft_test_scaled, \"google\": google_test_scaled, \"apple\": apple_test_scaled, \"amazon\": amazon_test_scaled, \"ibm\": ibm_test_scaled}\ntest_map_X = {\"microsoft\": [], \"google\": [], \"apple\": [], \"amazon\": [], \"ibm\": []}\ntest_map_y = {\"microsoft\": [], \"google\": [], \"apple\": [], \"amazon\": [], \"ibm\": []}\n\nfor key in test_map.keys():\n    for i in range(60,503):\n        test_map_X[key].append(test_map[key][i-60:i,0])\n        test_map_y[key].append(test_map[key][i, 0])\n    test_map_X[key], test_map_y[key]= np.array(test_map_X[key]), np.array(test_map_y[key])\n    test_map_X[key] = np.reshape(test_map_X[key], (test_map_X[key].shape[0], test_map_X[key].shape[1], 1))","5b8d9fb3":"for key in test_map_X.keys():\n    y_true = sc.inverse_transform(test_map_y[key].reshape(-1,1)) \n    y_prediction = sc.inverse_transform(gru.predict(test_map_X[key]))   \n    plot(y_true, y_prediction)\n    rmse = sqrt(mean_squared_error(y_true, y_prediction))\n    print(str.capitalize(key), \"Root Mean Squared Error:\", rmse)","2c034f76":"We will apply \"Many to One\" LSTM model. We will create a structure that contains 60 timesteps and 1 output. \n\nThe model expect the data to be in the shape \"samples, timesteps, features\". That's why we change the shape of training set.","4ea61915":"### Helper Functions","4894b9ca":"### Scaling the Data","f685899f":"There aren't any duplicated values in the dataset.","e0039d7e":"### Reading the Data","aae81470":"### Training with LSTM","75572a60":"We will split the dataset as train and test sets. Train set include the period that shows the highest stock prices until 2016. Test set include the period starting from 2016 and ends in 2018.","87cc3c57":"### Training with GRU","260bf738":"# 4. Data Preparation","468b4df5":"# 1. Reading and Understanding the Data","fad7357b":"There are some null values in ibm dataset. I will use ffill method. \u2018ffill\u2019 stands for \u2018forward fill\u2019 and will propagate last valid observation forward. In this way, last stock value will fill the null value.","5d21b903":"There are lots of companies in the dataset. I will just analyze five of them: Microsoft, Google, Apple, Amazon, Ibm.","b36320c6":"We have to scale our data for optimal performance. When evaluating stock prices, we\u2019ll use Scikit-Learn\u2019s MinMaxScaler and scale our dataset to numbers between zero and one.","25993cd9":"# 3. Exploratory Data Analysis","075ba95f":"The graph above show the price variation in stocks for all the companies comparing the price values for two consequtive days. There are high price changes during 2008 financial crisis period.","25bba7f4":"### Splitting the Dataset","fa9819ac":"GRU is computationally easier than LSTM since it has only 2 gates. GRU use less training parameters and therefore use less memory, execute faster and train faster than LSTM's whereas LSTM is more accurate on dataset using longer sequence. In short, if sequence is large or accuracy is very critical, LSTM is better whereas for less memory consumption and faster operation GRU is better.","61bea1f0":"Google and Amazon's stock prices are significantly higher than Apple, IBM, and Microsoft.","8c68964f":"# 2. Data Preprocessing "}}