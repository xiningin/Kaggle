{"cell_type":{"04a54b2f":"code","6732a150":"code","1cd1f651":"code","a67db99c":"code","a4f7da20":"code","0fec8d8c":"code","d27fe8c7":"code","6180cef9":"code","75dd31d5":"code","4b17b334":"code","f9183add":"code","73acd0ec":"code","7b388f07":"code","4a95b3e1":"code","a0839190":"code","3b0aa788":"code","4e13f83d":"code","b18bf16a":"code","cb458b06":"code","c6a77909":"code","45dae9ac":"code","f4360094":"code","b7b408a3":"code","b79d21c2":"code","32e6fb82":"code","810ab9b7":"code","b92e2110":"code","ffa11b89":"code","4ed0ee59":"code","cacc8acc":"code","ff30da90":"code","ab18d5e2":"code","d485d2f1":"code","a3bbe242":"code","a4b3d222":"code","fce8bdda":"code","0662bd22":"code","897c8663":"code","cf5b9ea0":"code","494c9ee3":"code","a630c701":"code","e91a2c39":"code","c94d7386":"code","87cc03c8":"code","e487dd56":"code","85ebda55":"code","cf06899b":"code","866fa441":"code","0a1f7ab5":"code","93d7847c":"code","9978d551":"code","b970538f":"code","6b4d14db":"code","ed86769a":"code","1044238f":"code","422b6d5b":"code","214e04dc":"code","207124e5":"code","c60e8f30":"code","04840c1d":"markdown","eb5e1a58":"markdown","0ac1208f":"markdown","9f433fa0":"markdown","0eac156e":"markdown","08a6ec85":"markdown","edc78e76":"markdown","c0670ecc":"markdown","342f1094":"markdown","89657e79":"markdown","4009b18c":"markdown","cb35e0a0":"markdown","6d550a0a":"markdown","43eb5deb":"markdown","5bb4213f":"markdown","a3bbeab8":"markdown","8e934442":"markdown","b9913adc":"markdown"},"source":{"04a54b2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6732a150":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngender_data = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","1cd1f651":"print(train_data.shape)\nprint(test_data.shape)\nprint(gender_data.shape)\nprint('train data columns are:')\nprint(train_data.columns)\nprint('test data columns are:')\nprint(test_data.columns)\nprint(gender_data.columns)","a67db99c":"from pandas_profiling import ProfileReport","a4f7da20":"#basic profiling\nprofile = ProfileReport(train_data,title='Pandas Profiling Report')\nprint(profile)","0fec8d8c":"profile","d27fe8c7":"train_data['Name'][100]","6180cef9":"def create_extra_features(data):\n    data['Is_fare_0'] = data['Fare'].apply(lambda x: (x==0)*1)\n    data['Age'] = data['Age'].fillna(120)\n    data['Age_less_10'] = data['Age'].apply(lambda x: (x<15)*1)\n    data['if_age_missing'] = data['Age'].apply(lambda x: (x==120)*1)\n    data['Age'] = data['Age'].replace(to_replace = 120, value = 30)#mean replacement\n    data['VIP_fare'] = data['Fare'].apply(lambda x: (x>112.07915)*1)\n    data['Cabin'] = data['Cabin'].fillna('NOCABIN')\n    data['has_Cabin'] = data['Cabin'].apply(lambda x: 1-(x=='NOCABIN')*1.0)\n    data['Name_length_space'] = data['Name'].apply(lambda x: len(x.split(\" \")))\n    data['Contains_Dr'] = data['Name'].apply(lambda x: 'Dr' in x)\n    data['Contains_Mr'] = data['Name'].apply(lambda x: 'Mr' in x)\n    data['Contains_Mrs'] = data['Name'].apply(lambda x: 'Mrs' in x)\n    data['Name_length_comma'] = data['Name'].apply(lambda x: len(x.split(\",\")))\n    return data","75dd31d5":"train_data = create_extra_features(train_data)\ntest_data = create_extra_features(test_data)","4b17b334":"train_data_mod = train_data.drop(['PassengerId', 'Name','Ticket', 'Fare'],axis =1)\ntest_data_mod = test_data.drop(['PassengerId', 'Name','Ticket', 'Fare'],axis =1)","f9183add":"print(train_data_mod.columns)\nprint(test_data_mod.columns)","73acd0ec":"train_data_mod.isna().sum().sum()","7b388f07":"train_data_mod['Embarked'] = train_data_mod['Embarked'].fillna('S')#median value filling","4a95b3e1":"train_data_mod.isna().sum().sum()","a0839190":"test_data_mod.isna().sum().sum()","3b0aa788":"def add_dummies(data,string_cols):\n    small_data = pd.DataFrame()\n    for col in string_cols:\n        curr_data = pd.get_dummies(data[col],prefix = col)\n        data = data.drop(col,axis = 1)\n        small_data = pd.concat([small_data,curr_data], axis = 1)\n        print(col,small_data.shape)\n    data = pd.concat([data,small_data],axis = 1)\n    return data","4e13f83d":"train_data_mod = add_dummies(train_data_mod,['Pclass','Sex','Embarked'] )\ntest_data_mod = add_dummies(test_data_mod,['Pclass','Sex','Embarked'])","b18bf16a":"train_data_mod.shape","cb458b06":"def add_more_features(data):\n    data['age_average'] = data['Age'].apply(lambda x: (x<40 and x>20)*1)\n    data['age_very_old'] = data['Age'].apply(lambda x: (x>70)*1)\n    data['age_very_small'] = data['Age'].apply(lambda x: (x<5)*1)\n    data['total_family'] = data['Parch'] + data['SibSp']\n    data['high_fare_best_class'] = data['VIP_fare'] * data['Pclass_1']\n    data['high_fare_middle_class'] = data['VIP_fare'] * data['Pclass_2']\n    data['old_and_poor'] = data['age_very_old'] * data['Pclass_3']\n    data['has_sp'] = data['SibSp'].apply(lambda x:(x>0)*1)\n    data['lady_with_husband'] = data['Sex_female']*data['has_sp']\n    return data","c6a77909":"train_data_mod = add_more_features(train_data_mod)\ntest_data_mod = add_more_features(test_data_mod)","45dae9ac":"!pip install lightautoml","f4360094":"from lightautoml.automl.base import AutoML\nfrom lightautoml.ml_algo.boost_lgbm import BoostLGBM\nfrom lightautoml.ml_algo.tuning.optuna import OptunaTuner\nfrom lightautoml.pipelines.features.lgb_pipeline import LGBSimpleFeatures\nfrom lightautoml.pipelines.ml.base import MLPipeline\nfrom lightautoml.pipelines.selection.importance_based import ImportanceCutoffSelector, ModelBasedImportanceEstimator\nfrom lightautoml.reader.base import PandasToPandasReader\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler\nfrom lightautoml.automl.blend import WeightedBlender","b7b408a3":"import logging\nimport os\nimport time\nlogging.basicConfig(format='[%(asctime)s] (%(levelname)s): %(message)s', level=logging.INFO)\n\n# Installed libraries\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport torch","b79d21c2":"N_THREADS = 8 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.1 # Test size for metric check\n\nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)\n\np = Profiler()\np.change_deco_settings({'enabled': True})","32e6fb82":"TARGET_NAME = 'Survived'# target column name","810ab9b7":"train_main_data, test_main_data = train_test_split(train_data_mod, \n                                                   test_size=TEST_SIZE, \n                                                   stratify=train_data[TARGET_NAME], \n                                                   random_state=RANDOM_STATE)\nlogging.info('Data splitted. Parts sizes: train_data = {}, test_data = {}'\n              .format(train_main_data.shape, test_main_data.shape))\n","b92e2110":"train_data_mod.head()","ffa11b89":"%%time\n\ntask = Task('binary')\nreader = PandasToPandasReader(task, cv=N_FOLDS, random_state=RANDOM_STATE)","4ed0ee59":"%%time\n\nmodel0 = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 64, 'seed': 42, 'num_threads': N_THREADS}\n)\npipe0 = LGBSimpleFeatures()\nmbie = ModelBasedImportanceEstimator()\nselector = ImportanceCutoffSelector(pipe0, model0, mbie, cutoff=0)","cacc8acc":"%%time \n\npipe = LGBSimpleFeatures()\n\nparams_tuner1 = OptunaTuner(n_trials=20, timeout=30) # stop after 20 iterations or after 30 seconds \nmodel1 = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 128, 'seed': 1, 'num_threads': N_THREADS}\n)\nmodel2 = BoostLGBM(\n    default_params={'learning_rate': 0.025, 'num_leaves': 64, 'seed': 2, 'num_threads': N_THREADS}\n)\n\npipeline_lvl1 = MLPipeline([\n    (model1, params_tuner1),\n    model2\n], pre_selection=selector, features_pipeline=pipe, post_selection=None)","ff30da90":"%%time\n\npipe1 = LGBSimpleFeatures()\n\nmodel = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 64, 'max_bin': 1024, 'seed': 3, 'num_threads': N_THREADS},\n    freeze_defaults=True\n)\n\npipeline_lvl2 = MLPipeline([model], pre_selection=None, features_pipeline=pipe1, post_selection=None)","ab18d5e2":"%%time \n\nautoml = AutoML(reader, [\n    [pipeline_lvl1],\n    [pipeline_lvl2],\n], skip_conn=False)","d485d2f1":"%%time \n\noof_pred = automl.fit_predict(train_main_data, roles={'target': TARGET_NAME})\nlogging.info('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","a3bbe242":"logging.info('Feature importances of selector:\\n{}'\n              .format(selector.get_features_score()))\nlogging.info('=' * 70)\n\nlogging.info('Feature importances of top level algorithm:\\n{}'\n              .format(automl.levels[-1][0].ml_algos[0].get_features_score()))\nlogging.info('=' * 70)\n\nlogging.info('Feature importances of lowest level algorithm - model 0:\\n{}'\n              .format(automl.levels[0][0].ml_algos[0].get_features_score()))\nlogging.info('=' * 70)\n\nlogging.info('Feature importances of lowest level algorithm - model 1:\\n{}'\n              .format(automl.levels[0][0].ml_algos[1].get_features_score()))\nlogging.info('=' * 70)","a4b3d222":"%%time\n\ntest_pred = automl.predict(test_main_data)\nlogging.info('Prediction for test data:\\n{}\\nShape = {}'\n              .format(test_pred, test_pred.shape))\n\nlogging.info('Check scores...')\nlogging.info('OOF score: {}'.format(roc_auc_score(train_main_data[TARGET_NAME].values, oof_pred.data[:, 0])))\nlogging.info('TEST score: {}'.format(roc_auc_score(test_main_data[TARGET_NAME].values, test_pred.data[:, 0])))","fce8bdda":"%%time\np.profile('my_report_profile.html')\nassert os.path.exists('my_report_profile.html'), 'Profile report failed to build'\n","0662bd22":"dir(automl)","897c8663":"test_res = automl.predict(test_data_mod)","cf5b9ea0":"array_lis = [test_res[i] for i in range(len(test_res))]","494c9ee3":"el = array_lis[0]","a630c701":"el.__format__","e91a2c39":"float(str(el.__repr__())[7:13])","c94d7386":"survival = [(float(str(el.__repr__())[7:13])>0.5)*1 for el in array_lis]","87cc03c8":"sum(survival)","e487dd56":"train_data['Survived'].value_counts()","85ebda55":"test_data.columns","cf06899b":"small_df = pd.DataFrame()\nsmall_df['PassengerId'] = test_data['PassengerId']\nsmall_df['Survived'] = survival\nsmall_df.to_csv('submission_automl_updated_more_features.csv',index = False)","866fa441":"X_train = train_data_mod.drop('Survived',axis = 1)\nY_train = train_data_mod['Survived']\nX_test = test_data_mod","0a1f7ab5":"X_train.columns","93d7847c":"X_train = X_train.drop('Cabin',axis=1)\nX_test = X_test.drop('Cabin',axis=1)","9978d551":"print(\"train data shape is:\",X_train.shape)\nprint(\"test data shape is:\",X_test.shape)\nprint(\"train label shape is:\",len(Y_train))","b970538f":"from sklearn.ensemble import RandomForestClassifier as rfclass\nfrom sklearn.metrics import mean_squared_error as rmse_1\nfrom sklearn.impute import SimpleImputer\nmy_imputer=SimpleImputer()\n\nX_train=my_imputer.fit_transform(X_train)\nX_test=my_imputer.fit_transform(X_test)\n\nlinclass=rfclass(n_estimators = 1000, max_depth = 4, min_samples_split = 2, max_features = 6,\n                 min_samples_leaf = 2, oob_score = True, n_jobs=-1)\nfitted_model = linclass.fit(X_train,Y_train)\npredictions=fitted_model.predict(X_test)","6b4d14db":"#pred_prob_train = fitted_model.predict_proba(X_train)","ed86769a":"#prob_1_array = []\n#for tup in pred_prob_train:\n#    prob_0,prob_1 = tup\n#    prob_1_array.append(prob_1)","1044238f":"#def threshold_based_prediction(prob_arr,threshold):\n#    pred = []\n#    for prob in prob_arr:\n#        if prob >= threshold:\n#            pred.append(1)\n#        else:\n#            pred.append(0)\n#    return pred        ","422b6d5b":"#pred_train = threshold_based_prediction(prob_1_array,threshold = 0.5)","214e04dc":"#pred_train = fitted_model.predict(X_train)\nprint(rmse_1(pred_train,Y_train)**0.5)\nfrom sklearn.metrics import classification_report as clreport\nprint(clreport(Y_train,pred_train))\nprint(fitted_model.oob_score_)","207124e5":"preds = list(predictions)","c60e8f30":"small_df = pd.DataFrame()\nsmall_df['PassengerId'] = test_data['PassengerId']\nsmall_df['Survived'] = predictions\nsmall_df.to_csv('submission_rf_almost_there.csv',index = False)","04840c1d":"\n## Step 3.2. Create 2nd level ML pipeline for AutoML\n\n### Our second level ML pipeline:\n\n*     Using simple features as well, but now it will be Out-Of-Fold (OOF) predictions of algos from 1st level\n*     Only one LGBM model without params tuning\n*     Without feature selection on this stage because we want to use all OOFs here\n","eb5e1a58":"#### this code patch is deprecated\nlinclass=rfclass(n_estimators = 100, max_depth = 8,min_samples_split = 30, max_features = 17,\n             oob_score=True,n_jobs=-1)\nfitted_model = linclass.fit(X_train,Y_train)\npredictions=fitted_model.predict(X_test)","0ac1208f":"The test_result gives an array type object which is called numpydataset. I couldn't find out ways to get the actual predicted probability from this array object; so this few coming blocks are ways to figure that out. Finally cooked a solution using __repr__ and so on; but would ask later on in the main source what is the correct way to figure it out.","9f433fa0":"## Step 6. Analyze fitted model\n\nBelow we analyze feature importances of different algos:\n","0eac156e":"#### this code patch is deprecated\nfrom sklearn.model_selection import RandomizedSearchCV\nrandomforest = rfreg(oob_score=True,n_jobs=-1)\ndistributions = dict(n_estimators = [100,200,500,1000], \n                     max_depth = [8,10,14,16,20],\n                     min_samples_split = [5,15,30], \n                     max_features = [17,50,80,145],\n                     )\nclf = RandomizedSearchCV(randomforest, distributions, random_state=0)\nsearch = clf.fit(X_train,Y_train)\nprint(search.best_params_)","08a6ec85":"# step 2: create feature selector ( if necessary)","edc78e76":"The automl achieved 0.77511 score in the board with small number of features without the add_more_features. On adding more features, the result comes down to 0.77033. Not bad, not so good either. This concludes our experiment therefore.","c0670ecc":"# exploratory data analysis\nWe will try the different variations of pandas profiling in this section. Pandas profiling is an extension to the normal Pandas.DataFrame.Describe() method, which gives you bunch of simple informations about the data. Pandas profiling is designed to dig deeper and explore more about the data. In the coming 3 code blocks,<br\/>\n(a) we will run just pandas_profiling.ProfileReport<br\/>\n(b) we will not run, but we can also do pandas_profiling ProfileReport with explorative = True, and note the differences.<br\/>\n(c) You can also try some of the [advanced usages](https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/master\/rtd\/pages\/advanced_usage.html).<br\/>\nLet's jump in!","342f1094":"Let's check the automl object now.","89657e79":"\n## Step 8. Profiling AutoML\n\nTo build report here, we must turn on decorators on step 0.4. Report is interactive and you can go as deep into functions call stack as you want:","4009b18c":"### Step 7. Predict to test data and check scores","cb35e0a0":"\n## Step 5. Train AutoML on loaded data\n\nIn cell below we train AutoML with target column TARGET to receive fitted model and OOF predictions:\n","6d550a0a":"So as you can see, pandas profiling results is truely **remarkable**. A lot of insights are right out generated in the profiling report; while some insights are subtle and would point here out:<br\/>\n1. from correlation matrix, it can be seen that passenger class i.e. pclass is a strong indicator of the fact that belonging to 3rd     or 2nd classes represented lower survival rates which is true from historical anecdotes.<br\/>\n2. age is also a slight negative marker of survival. Being higher age is slightly negatively correlated with survival so                representing lesser survival with increasing age.<br\/>\n3. Fare is again a strong indicator of survival; i.e. increasing fair has medium amount of correlation with survival.<br\/>\n4. Fare was not positive with every case. 15 people rode the ship with 0 fare. That maybe due to special treatment or people free-     riding the ship. We can create a feature **Is_fare_0** to check how that relates with survival. <br\/>\n5. Some people paid pretty high fares. I wonder how they were treated. We will create a feature called **VIP_fare** for people who     paid more than 112.07915; the 95th percentile.<br\/>\n6. I think extreme ages are also concernable. Historic anecdotes say that there have been preferences to women and children.           Creating a feature **age_less_10** which denotes if age is less than 10; makes sense. Age is missing in some cases. I wonder         what leads to that. We will create a feature **if_age_missing** and put 0 if age is present and 1 if age is missing.<br\/>\n7. parch and sibsp are interesting features too. parch refers to number of parents \/ children aboard the Titanic.<br\/>\n8. Cabin is not present for many passengers. Cabin is for sure a signal of lavishness therefore. We will check if having cabin is a     signal for survival.<br\/>\nLet's create the features and run one round of automl on the features.","43eb5deb":"Now that we have created a few extra features, completed null value filling. Let's try the automl library. We have copied the [example notebook](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/blob\/master\/Tutorial_1.%20Create%20your%20own%20pipeline.ipynb) in the repo for the most part and made necessary changes to fit our data.","5bb4213f":"\n## Step 4. Create AutoML pipeline\n\n### AutoML pipeline consist of:\n\n*     Reader for data preparation\n*     First level ML pipeline (as built in step 3.1)\n*     Second level ML pipeline (as built in step 3.2)\n*     Skip_conn = False equals here \"not to use initial features on the second level pipeline\"","a3bbeab8":"\n## Step 3.1. Create 1st level ML pipeline for AutoML\n\nOur first level ML pipeline:\n\n*     Simple features for gradient boosting built on selected features (using step 2)\n*     different models:\n *         LightGBM with params tuning (using OptunaTuner)\n *         LightGBM with heuristic params\n","8e934442":"The report my_report_profile.html will be downloaded at this step in your kaggle\/working\/. The report is interactive; and spreads to dependencies of the total time spent.<br\/>\nYou can download and check actually how much time is spent in each of the action we took.","b9913adc":"# Basic Goals:\nWe will train some automl models on the data and try achieve some baseline scores for this dataset. We are going to explore:<br\/>\n(1) [sberBank lightautoml](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)<br\/>\n(2) [pandas profiling](https:\/\/pandas-profiling.github.io\/pandas-profiling\/) for exploratory analysis<br\/>\n<br\/>\nI have written up the excerpt of this notebook in [this blog post](https:\/\/shyambhu20.blogspot.com\/2020\/12\/exploring-automl-and-pandas-profiling.html) which you can read.<br\/>"}}