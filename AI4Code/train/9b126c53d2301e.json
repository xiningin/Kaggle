{"cell_type":{"5652c079":"code","748df8f8":"code","4e822754":"code","18fa4656":"code","b10dc25b":"code","094c71f9":"code","6917fdcc":"code","040d6269":"code","16737c22":"code","6a81f764":"code","a9a7e5f3":"code","12d5940f":"code","27e1d3d5":"code","3e5fdf23":"code","44d6eafe":"code","5882d323":"code","0e89fc58":"code","ef223c77":"code","8eb15a2c":"code","ce880eaf":"code","69ce8e26":"code","4059d74e":"code","987d79fb":"code","97576b08":"code","40ae33a6":"code","7ad47bf0":"code","da36e7e0":"code","cd22dc47":"code","fd0f47fd":"code","86057d6b":"markdown","d58ab068":"markdown","92802698":"markdown","4f7705f7":"markdown","8d95aef8":"markdown","a021b794":"markdown","221128da":"markdown","9692b3db":"markdown","232898a1":"markdown","6f931495":"markdown","1052559c":"markdown","28abbb7a":"markdown","a8b33457":"markdown","5f06c6a6":"markdown"},"source":{"5652c079":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","748df8f8":"#the usual\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n#colored printing output\nfrom termcolor import colored\n\n#I\/O\nimport io\nimport os\nimport requests\n\n#pickle\nimport pickle\n\n#math\nimport math\n\n#scipy\nfrom scipy import stats\n\n#sk learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import preprocessing\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import learning_curve\nfrom itertools import combinations\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n#sns style\nimport seaborn as sns\n#sns.set_style(\"whitegrid\")\nsns.despine()\nsns.set_context(\"talk\") #larger display of plots axis labels etc..\nsns.set(style='darkgrid')","4e822754":"bank_data = pd.read_csv(\"..\/input\/Bank_Customers.csv\",index_col ='RowNumber')\nbank_data.head()","18fa4656":"bank_data.shape","b10dc25b":"bank_data.hist(figsize=(15,12),bins=15,color=\"purple\",grid=False,);","094c71f9":"sns.pairplot(bank_data,hue=\"Exited\",height=2)","6917fdcc":"fig = plt.figure(figsize = (15,15)); ax = fig.gca()\nsns.heatmap(bank_data.corr(), annot = True, vmin= -1, vmax = 1, ax=ax)","040d6269":"\nbank_data[\"Gender\"].value_counts().plot(kind='pie',figsize= (6,6));\n","16737c22":"def bar_chart(feature,input_df):\n    Exited = input_df[input_df['Exited']==1][feature].value_counts()\n    Stayed = input_df[input_df['Exited']==0][feature].value_counts()\n    df = pd.DataFrame([Exited,Stayed])\n    df.index = ['Exited','Stayed']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","6a81f764":"bar_chart(\"Geography\",bank_data)","a9a7e5f3":"bar_chart(\"IsActiveMember\",bank_data)","12d5940f":"bank_data.head()","27e1d3d5":"bank_data.describe()","3e5fdf23":"Bank_data = bank_data.drop(['CustomerId','Surname'],axis=1)\nBank_data.head()","44d6eafe":"Bank_data.shape","5882d323":"# https:\/\/www.kaggle.com\/shrutimechlearn\/types-of-regression-and-stats-in-depth\nGeo_bank = pd.get_dummies(prefix='Geo',data=Bank_data,columns=['Geography'])\nGeo_bank.head()","0e89fc58":"Gen_bank = Geo_bank.replace(to_replace={'Gender': {'Female': 1,'Male':0}})\nGen_bank.head()","ef223c77":"churn_bank = Gen_bank","8eb15a2c":"X = churn_bank.drop(['Exited'],axis=1)\ny = churn_bank.Exited","ce880eaf":"print(X.shape)\nprint(y.shape)","69ce8e26":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 0)","4059d74e":"# Feature Scaling because yes we don't want one independent variable dominating the other and it makes computations easy\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","987d79fb":"# sequential model to initialise our ann and dense module to build the layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense","97576b08":"classifier = Sequential()\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN | means applying SGD on the whole ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 100,verbose = 0)\n\nscore, acc = classifier.evaluate(X_train, y_train,\n                            batch_size=10)\nprint('Train score:', score)\nprint('Train accuracy:', acc)\n# Part 3 - Making predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n\nprint('*'*20)\nscore, acc = classifier.evaluate(X_test, y_test,\n                            batch_size=10)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","40ae33a6":"p = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","7ad47bf0":"#import classification_report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","da36e7e0":"from sklearn.metrics import roc_curve\ny_pred_proba = classifier.predict_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC curve')\nplt.show()","cd22dc47":"#Area under ROC curve\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_proba)","fd0f47fd":"# print('Best Parameters after tuning: {}'.format(best_parameters))\n# print('Best Accuracy after tuning: {}'.format(best_accuracy))","86057d6b":"## Improving ANN\n\n\n* from keras.layers import Dropout\n* classifier = Sequential()\n#### Adding the input layer and the first hidden layer\n* classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n* classifier.add(Dropout(rate = 0.1))\n\n#### Adding the second hidden layer\n* classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n* classifier.add(Dropout(rate = 0.1))\n\n#### Adding the output layer\n* classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n#### Compiling the ANN\n* classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n#### Fitting the ANN to the Training set\n* classifier.fit(X_train, y_train, batch_size = 10, epochs = 100,verbose = 0)\n\n#### Part 3 - Making predictions and evaluating the model\n\n* score, acc = classifier.evaluate(X_train, y_train,\n                            batch_size=10)\n* print('Train score:', score)\n* print('Train accuracy:', acc)\n#### Part 3 - Making predictions and evaluating the model\n\n#### Predicting the Test set results\n* y_pred = classifier.predict(X_test)\n* y_pred = (y_pred > 0.5)\n\n* print('*'*20)\n* score, acc = classifier.evaluate(X_test, y_test,\n                            batch_size=10)\n* print('Test score:', score)\n* print('Test accuracy:', acc)\n\n#### Making the Confusion Matrix\n* from sklearn.metrics import confusion_matrix\n* cm = confusion_matrix(y_test, y_pred)","d58ab068":"### Evaluation of Multiple Training Instances","92802698":"* p = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n* plt.title('Confusion matrix', y=1.1)\n* plt.ylabel('Actual label')\n* plt.xlabel('Predicted label')","4f7705f7":"<a id=\"2\"><\/a>\n##  Activation Function\n#### Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable.They introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.\n\n#### Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply a Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer.\n\n **popular types of Activation functions** \n* Unit-Step or Binary Step or Heaviside step function\n* Sign(Signum)\n* Linear \n* Piece- Wise Linear\n* Logistic (Sigmoid) \n* Hyberolic tangent(tanh) \n* ReLu (Rectified Linear)\n* Leaky ReLu \n\n**Unit-Step or Binary Step or Heaviside step function**\nIt usually denoted by H or \u03b8 (but sometimes u, 1 or \ud835\udfd9), is a discontinuous function,it represents a signal that switches on at a specified time and stays switched on indefinitely. Oliver Heaviside, who developed the operational calculus as a tool in the analysis of telegraphic communications, represented the function as 1.\n\n**Sign Activation**\nit is same as Binary Step but the limit are ranges in given below \n**Linear**\n The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. a(1) is the vectorized form of any linear function. Passing through Origin \n**Piece- Wise Linear**\nSuccessive linear transforms followed by nonlinear \"activation\" functions can approximate nonlinear functions to arbitrary precision given sufficient layers. The number of necessary layers is dependent on, in part, by the nature of the activation function.\n**Sigmoid or Logistic**\nIt is a activation function of form f(x) = 1 \/ 1 + exp(-x) . Its Range is between 0 and 1. It is a S\u200a\u2014\u200ashaped curve. It is easy to understand and apply but it has major reasons which have made it fall out of popularity -\n\n* Vanishing gradient problem\n \n \"\"\"Secondly , its output isn\u2019t zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\"\"\"\n* Sigmoids saturate and kill gradients.\n* Sigmoids have slow convergence.\n\n**Hyperbolic Tangent function- Tanh** : It\u2019s mathamatical formula is f(x) = 1\u200a\u2014\u200aexp(-2x) \/ 1 + exp(-2x). Now it\u2019s output is zero centered because its range in between -1 to 1 i.e -1 < output < 1 . Hence optimization is easier in this method hence in practice it is always preferred over Sigmoid function . But still it suffers from Vanishing gradient problem\n\n\n**ReLu- Rectified Linear units** : It has become very popular in the past couple of years. It was recently proved that it had 6 times improvement in convergence from Tanh function. It\u2019s just R(x) = max(0,x) i.e if x < 0 , R(x) = 0 and if x >= 0 , R(x) = x. Hence as seeing the mathamatical form of this function we can see that it is very simple and efficinent . A lot of times in Machine learning and computer science we notice that most simple and consistent techniques and methods are only preferred and are best. Hence it avoids and rectifies vanishing gradient problem . Almost all deep learning Models use ReLu nowadays.\n**Leaky-ReLu**\nWith a Leaky ReLU (LReLU), you won\u2019t face the \u201cdead ReLU\u201d problem which happens when your ReLU always have values under 0 - this completely blocks learning in the ReLU because of gradients of 0 in the negative part\n The derivative of the LReLU is 1 in the positive part, and is a small fraction in the negative part.\n*But its limitation is that it should only be used within Hidden layers of a Neural Network Model.\n\nHence for output layers we should use a Softmax function for a Classification problem to compute the probabilites for the classes , and for a regression problem it should simply use a linear function.*\n\n**Qucik View On Activation Function ** \n![](https:\/\/miro.medium.com\/max\/1005\/1*p_hyqAtyI8pbt2kEl6siOQ.png)\n","8d95aef8":"## Problem Statement\n\n* This is a classification problem where in we have a dataset in which there are details of a bank's customers and the target variable is ina  binary variable reflecting the fact whether the customer drop from the bank (closed his account) or he  is happy in process of transancation (happy customer)\n\n\n## Brain Stroming On Independent Variables \n\n1. I Feel the best push of dropping the   Customer Id, Surname,\n2. Males tend to stay a bit more, so do active members,no strong bias for geography\n3. As, We know that Age and Balance had a  Quite strong correlation \n4. One-hot encode with  Gender and Geograph\n## Visualizing\n* **Reading the Data** \n* **Plotting the main features of the columns** \n* **Visualize in the pair-wise plots for independent variables** \n* **Production of heatmap to know the Co-relation between variables **\n* **Count Plot for GENDER in Pie-Chart **\n* **Measuring the inflow and outflow of Customers Permenantly with bar-Chart of 3 Countries ** ","a021b794":"## **Evaluation Metric**","221128da":"* from sklearn.metrics import roc_curve\n* y_pred_proba = classifier.predict_proba(X_test)\n* fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n* plt.plot([0,1],[0,1],'k--')\n* plt.plot(fpr,tpr, label='ANN')\n* plt.xlabel('fpr')\n* plt.ylabel('tpr')\n* plt.title('ROC curve')\n* plt.show()","9692b3db":"# Tuning the ANN\n* from keras.wrappers.scikit_learn import KerasClassifier\n* from sklearn.model_selection import GridSearchCV\n* from keras.models import Sequential\n* from keras.layers import Dense\n* def build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n* classifier = KerasClassifier(build_fn = build_classifier)\n* parameters = {'batch_size': [25, 32],\n              'epochs': [100, 200],\n              'optimizer': ['adam', 'rmsprop']}\n* grid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10)\n* grid_search = grid_search.fit(X_train, y_train,verbose = 0)\n* best_parameters = grid_search.best_params_\n* best_accuracy = grid_search.best_score_","232898a1":"## Absences Of Activation function can Causes \nAbsences of Activation function cause the output signal would simply be a simple linear function.A linear function is just a polynomial of one degree. Now, a linear equation is easy to solve but they are limited in their complexity and have less power to learn complex functional mappings from data. A NN without Activation function would simply be a Linear regression Model, which has limited power and does not performs good most of the times. We are not desiring  Neural Network  not just to learn and compute a linear function but something more complicated than that. Also Absences of  activation function on our network would not be able to learn and  other complicated kinds of data such as images, videos , audio , speech etc. That is why we use Artificial Neural network techniques such as Deep learning to make sense of something complicated ,high dimensional,non-linear -big datasets, where the model has lots and lots of hidden layers in between and has a very complicated architecture which helps us to make sense and extract knowledge form such complicated big datasets.\n\n\n## Need & Limitation of Non-linearity \nIt simply mean that the output at any unit cannot be reproduced from a linear function of the input. I will try to talk why do we need the non-linearity and how this can be effectively achieved.\n**NEED**\nThis is always tried to ensure that the output at any unit should not be a linear combination of the inputs. Having so narrow downs the ability of network to fit non-linear boundaries because in this case it will simply behave as a linear least-squares model. Clearly this is not powerful and hence we want the output that cannot be written as a linear combination of the input.\n**Limitation**\nThis is achieved by using a proper activation function. It is different than to apply a threshold on the output, I will assume that you already know the use of bias in a neural network. Let\u2019s see the affect of some of the common activation functions and what kind of boundary they can fit:\n\n![](https:\/\/qphs.fs.quoracdn.net\/main-qimg-6a32a7b287a6806d5d7a9e96b225e4f9.webp)\n","6f931495":"### Improving ANN with Dropout layer\n**Dropout Regularization is used to reduce overfitting if needed.**\n\n* p is the fraction of input units to drop. If suppose there are ten neurons from a layer and p is 0.1 then one of the neurons would be disabled and its output would not be sent to the further layer.\n* It is advisable to start with p 0.1 and move to higher values when in case the overfitting problem persists. Also going over 0.5 is not advisable generally because it may cause underfitting as most of the neurons are disabled.","1052559c":" ##  Tuning the ANN","28abbb7a":"##  AGENDA \n* What are ANNs ?\n* Inspird by Brain, How? \n* About Perceptron?\n* Types of NNs?\n* In what situation does the algorithm fit best?\n* Work Process of ANN ?\n* Activation Function\n* What happens without activation function?\n* Need & Limitation Of Non-linearity \n* Business Problem and EDA\n* Evaluation Metrics\n* Evaluation of Multiple Training Instances\n* Improving the ANN with dropout layer\n* Tuning the ANN","a8b33457":"### ANN  \nANN stands for Artificial Neural Networks. Basically, it\u2019s a computational model. That is based on structures and functions of biological neural networks. Although, the structure of the ANN affected by a flow of information. Hence, neural network changes were based on input and output.\nBasically, we can consider ANN as nonlinear statistical data. That means complex relationship defines between input and output. As a result, we found different patterns. Also, we call the ANN as a neural network\n### Difference Between NN and ANN\n* NN\nThe term \u2018Neural\u2019 is derived from the human (animal) nervous system\u2019s basic functional unit \u2018neuron\u2019 or nerve cells which are present in the brain and other parts of the human (animal) body. Neural Network is a broad term that encompases various types of networks which were shown above.\n* ANN\nIs ANN one of the types? Well to understand this it is important to realise that neural network alone is not an algorithm but a framework which assists the algorithms to work. ANN is the most basic type of implementation of neurals. ANN was the term coined much earlier and nowadays the two terms are interchangeably used .Artificial Neural Networks are the computational models inspired by the human brain. Many of the recent advancements have been made in the field of Artificial Intelligence, including Voice Recognition, Image Recognition, Robotics using Artificial Neural Networks.\n### Brain Works \nHere, neurons, part of human brain. That was composed of 86 billion nerve cells.In,the cells there are Chemical gradients like(Potassium,Sodium,Chlorine) are to Conducting Signals from Ions. The Neurons are connected to other thousands of cells by Axons. Although, there are various inputs from sensory organs. That was accepted by dendrites. As a result, when chemical signal activates some neurons get activied(creates electric impulses,) .When, those neurons get activation strength of connection between dentrites becomes higher by the Ions get used repedenentively . This,is used to travel through the Artificial neural network.In 1949 Donald Hebb postulated one way for the network to learn.if a synapse is used more ,it gets strengthen -releases more Neurotransmitter.This causes that particular path through the network to get Stronger ,while others,not used,get weaker Thus,to handle the different issues, neuron send a message to another neuron.\n\nAs a result, we can say that ANNs are composed of multiple nodes. That imitate biological neurons of the human brain. Although, we connect these neurons by links. Also, they interact with each other. Although, nodes are used to take input data. Further, perform simple operations on the data. As a result, these operations are passed to other neurons. Also, output at each node is called its activation or node value.As each link is associated with weight. Also, they are capable of learning. That takes place by altering weight values.\n\n### Perceptron\n* Artificial Neuron\n\nAn artificial neuron is a mathematical function based on a model of biological neurons, where each neuron takes inputs, weighs them separately, sums them up and passes this sum through a nonlinear function to produce output.A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features or business intelligence in the input data.\n\n![](https:\/\/miro.medium.com\/max\/1290\/1*-JtN9TWuoZMz7z9QKbT85A.png)\n\nPerceptron was introduced by Frank Rosenblatt in 1957. He proposed a Perceptron learning rule based on the original MCP neuron.\nA Perceptron is an algorithm for supervised learning of binary classifiers. This algorithm enables neurons to learn and processes elements in the training set one at a time.\n\n* Perceptron Learning Rule\nPerceptron Learning Rule states that the algorithm would automatically learn the optimal weight coefficients. The input features are then multiplied with these weights to determine if a neuron fires or not.\n\nNeural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. ANNs have three layers that are interconnected. The first layer consists of input neurons. Those neurons send data on to the second layer, which in turn sends the output neurons to the third layer. ANNs are considered non-linear statistical data modeling tools where the complex relationships between inputs and outputs are modeled or patterns are found. Note that a neuron can also be referred to as a perceptron.\n\n### Types of NeuralNetwork \nThere are multiple types of neural network, each of which come with their own specific use cases and levels of complexity. The most basic type of neural net is something called a feedforward neural network, in which information travels in only one direction from input to output. A more widely used type of network is the recurrent neural network, in which data can flow in multiple directions. These neural networks possess greater learning abilities and are widely employed for more complex tasks such as learning handwriting or language recognition\n\n![](https:\/\/i.stack.imgur.com\/LgmYv.png)\n\n\n* Radial Basis Function Neural Network\n\nThe radial basis function neural network is applied extensively in power restoration systems. In recent decades, power systems have become bigger and more complex.This increases the risk of a blackout. This neural network is used in the power restoration systems in order to restore power in the shortest possible time\n\n### In what situation does the algorithm fit best?\n\n#### ANN is rarely used for predictive modelling. The reason being that Artificial Neural Networks (ANN) usually tries to over-fit the relationship. ANN is generally used in cases where what has happened in past is repeated almost exactly in same way. For example, say we are playing the game of Black Jack against a computer. An intelligent opponent based on ANN would be a very good opponent in this case (assuming they can manage to keep the computation time low). With time ANN will train itself for all possible cases of card flow. And given that we are not shuffling cards with a dealer, ANN will be able to memorize every single call. Hence, it is a kind of machine learning technique which has enormous memory. But it does not work well in case where scoring population is significantly different compared to training sample. For instance, if I plan to target customer for a campaign using their past response by an ANN. I will probably be using a wrong technique as it might have over-fitted the relationship between the response and other predictors.\n\n### Work Process of ANN ?\n\nNeural networks learning process is not very different from human learns from experience in lives while neural networks require data to gain experience and learn. Accuracy increases with the amount of data over time. Similarly, humans also perform the same task better and better by doing any task you do over and over. The underlying foundation of neural networks is a layer and layers of connections. The entire neural network model is based on a layered architecture. Each layer has its own responsibility. These networks are designed to make use of layers of \u201cneurons\u201d to process raw data, find patterns into it and objects which are usually hidden to naked eyes. To train a neural network, data scientist put their data in three different baskets.\n\nTraining data set \u2013 This helps networks to understand and know the various weights between nodes.\nValidation data set \u2013 To fine-tune the data sets.\nTest data set \u2013 To evaluate the accuracy and records margin of error.\n\nLayer takes input, extract feature and feed into the next layer i.e. each layer work as an input layer to another layer. This is to receive information and last layer job is to throw output of the required information. Hidden layers or core layers process all the information in between.\n\n![](https:\/\/i0.wp.com\/vinodsblog.com\/wp-content\/uploads\/2018\/10\/Artificial-Neural-Network-%E2%80%93-Dry-Run-png.png?w=2027&ssl=1)\n\n* Assign a random weight to all the links to start the algorithm.\n* Find links the activation rate of all hidden nodes by using the input and links.\n* Find the activation rate of output nodes with the activation rate of hidden nodes and link to output.\n* Errors are discovered at the output node and to recalibrate all the links between hidden & output nodes.\n* Using the weights and error at the output; cascade down errors to hidden & output nodes. Weights get applied on connections as     the best friend for neural networks.\n* Recalibrate & repeat the process of weights between hidden and input nodes until the convergence criteria are met.\n* Finally the output value of the predicted value or the sum of the three output values of each neuron. This is the output.\n* Patterns of information are fed into the network via the input units, which trigger the layers of hidden units, and these, in       turn, arrive at the output units.\n\nDeep Learning\u2019s most common model is \u201cThe 3-layer fully connected neural network\u201d. This has become the foundation for most of the others. The backpropagation algorithm is commonly used for improving the performance of neural network prediction accuracy. It\u2019s done by adjusting higher weight connections in an attempt to lower the cost function.\n\n### Behind the Scences Of Neural Network\n\nWe outline a few main algorithms with an overview to create our basic understanding and the big picture on behind the scene of this excellent networks. In neural networks almost every neuron influence and connected to each other as seen on the above picture.  Below 5 methods are commonly used in neural networks.\n\n* Feedforward algorithm\n* Sigmoid \u2013 A common activation algorithm\n* Cost function\n* Backpropagation\n* Gradient descent \u2013 Applying the learning rate\n\n![](https:\/\/i2.wp.com\/vinodsblog.com\/wp-content\/uploads\/2018\/12\/ANN-Picture.png?w=2031&ssl=1)","5f06c6a6":"# Takes Long time to run \n# Evaluating the ANN\n* from keras.wrappers.scikit_learn import KerasClassifier\n* from sklearn.model_selection import cross_val_score\n* from keras.models import Sequential\n* from keras.layers import Dense\n* def build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n* classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100,verbose=0)\n* accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n* mean = accuracies.mean()\n* "}}