{"cell_type":{"0fc0a37d":"code","8bf06564":"code","48253629":"code","71106700":"code","d3cdf69e":"code","5bd02871":"code","d278e16d":"code","ff36d550":"code","5d37a039":"code","1803a40c":"code","fc2cf44b":"code","f5e4a440":"code","deec961a":"code","84752016":"code","6f00334b":"code","b2da152d":"code","d3d2553e":"code","2fc70503":"code","67b6a804":"code","77dd805b":"code","21c14b08":"code","f3ad3f50":"code","d27440ca":"code","a6757212":"code","92d0ec1a":"code","1abd362d":"code","49baa348":"code","4f499eaf":"code","2d152a15":"code","8b271ea0":"code","cadaddd8":"code","3abe1610":"markdown","b53307c4":"markdown","55e8d5db":"markdown","3e32a8e9":"markdown","97843774":"markdown","7485867c":"markdown","fe7a6b35":"markdown","f0eb9b7f":"markdown","24805efc":"markdown","1e4fdb38":"markdown","a41e8e84":"markdown","5719d549":"markdown","63f0148a":"markdown","0883963b":"markdown","dc26ac88":"markdown","8ad71bc5":"markdown","88f4a260":"markdown","17020cf6":"markdown"},"source":{"0fc0a37d":"!pip install -q --upgrade nltk\n!pip install -q umap-learn\nimport nltk\nnltk.download('averaged_perceptron_tagger_ru')","8bf06564":"from itertools import product\n\nimport pandas as pd\nfrom IPython.display import display\nfrom textblob.utils import strip_punc\nfrom tqdm.auto import tqdm","48253629":"datadir = \"..\/input\/competitive-data-science-predict-future-sales\"","71106700":"# Get the name and category name of each item, along with it's average price.\ndf = (\n    pd.read_csv(f\"{datadir}\/sales_train.csv\")\n    .merge(pd.read_csv(f\"{datadir}\/items.csv\"))\n    .merge(pd.read_csv(f\"{datadir}\/item_categories.csv\"))\n    .groupby([\"item_id\", \"item_category_name\", \"item_name\"])[\"item_price\"]\n    .mean()\n    .reset_index()\n)\ndf","d3cdf69e":"def remove_punctuation(text):\n    return strip_punc(text, all=True)","5bd02871":"df[\"item_name\"] = df[\"item_name\"].apply(remove_punctuation)\ndf[\"item_category_name\"] = df[\"item_category_name\"].apply(remove_punctuation)\nwith pd.option_context(\"display.max_colwidth\", None):\n    display(df.sort_values([\"item_name\", \"item_category_name\"]))","d278e16d":"import gensim.downloader","ff36d550":"en_model = gensim.downloader.load(\"glove-wiki-gigaword-300\")","5d37a039":"en_model.similar_by_word(\"xbox\")","1803a40c":"df[\"description\"] = df[\"item_category_name\"] + \"\\n\" + df[\"item_name\"]","fc2cf44b":"import numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Remove any words that don't add meaning, such as \"the\", \"in\", \"of\", etc.\nstopwords = stopwords.words(\"english\")\n\n\ndef vectorize(row):\n    text = row[\"description\"]\n    tokens = [\n        word\n        for word in word_tokenize(strip_punc(text.lower(), all=True))\n        if word not in stopwords\n    ]\n    vecs = [en_model[tok] for tok in tokens if tok in en_model]\n    if vecs:\n        vector = np.mean(vecs, axis=0)\n    else:\n        vector = np.zeros_like(en_model.vectors[0])\n    return vector","f5e4a440":"vecs = pd.DataFrame(\n    data=np.array(df.apply(vectorize, axis=1).tolist()), index=df[\"description\"]\n)","deec961a":"import random\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom umap import UMAP  # UMAP is faster than t-SNE\n\nsns.set()\n\n\ndef plot_vectors(vectors, data=None, n_labels=0):\n    fig, ax = plt.subplots(figsize=(16, 9))\n    plt.close()\n    ax.axis(\"off\")\n    seed = 42\n\n    tsne = UMAP(n_components=2, random_state=seed)\n    reduced = tsne.fit_transform(vectors)\n    colours = np.log1p(data[\"item_price\"])\n    ax.scatter(reduced[:, 0], reduced[:, 1], c=colours, cmap=\"RdBu_r\", alpha=0.2)\n\n    random.seed(seed)\n    for idx in random.sample(range(len(reduced)), n_labels):\n        x, y = reduced[idx]\n        name = data.iloc[idx][\"description\"]\n        if len(name) > 37:\n            name = name[:37] + \"...\"\n\n        ax.annotate(\n            name,\n            (x, y),\n            xycoords=\"data\",\n            xytext=(random.randint(-100, 100), random.randint(-100, 100)),\n            horizontalalignment=\"right\" if x < 0 else \"left\",\n            textcoords=\"offset points\",\n            color=\"black\",\n            bbox=dict(boxstyle=\"round\", fc=(0.03, 0.85, 0.37, 0.45), ec=\"none\"),\n            arrowprops=dict(arrowstyle=\"simple\", linewidth=5, ec=\"none\",),\n        )\n\n    fig.tight_layout()\n    return fig","84752016":"from sklearn.neighbors import NearestNeighbors\n\ndisplay(plot_vectors(vecs, data=df, n_labels=15))\n\nnn = NearestNeighbors(n_neighbors=3, metric=\"cosine\", n_jobs=-1).fit(vecs)\n\n# Preview the nearest neighbours for the first few items.\nwith pd.option_context(\"display.max_colwidth\", None):\n    display(\n        pd.DataFrame(\n            df[\"description\"].values[\n                nn.kneighbors(vecs[:10], n_neighbors=3, return_distance=False)\n            ]\n        )\n    )","6f00334b":"cyrillic = set(\"\u0410\u0430\u0411\u0431\u0412\u0432\u0413\u0433\u0414\u0434\u0415\u0435\u0401\u0451\u0416\u0436\u0417\u0437\u0418\u0438\u0419\u0439\u041a\u043a\u041b\u043b\u041c\u043c\u041d\u043d\u041e\u043e\u041f\u043f\u0420\u0440\u0421\u0441\u0422\u0442\u0423\u0443\u0424\u0444\u0425\u0445\u0426\u0446\u0427\u0447\u0428\u0448\u0429\u0449\u042a\u044a\u042b\u044b\u042c\u044c\u042d\u044d\u042e\u044e\u042f\u044f\")\n\nen, ru = 0, 0\n\n\ndef count_langs(text):\n    global en, ru\n    tokens = [\n        set(word)\n        for word in word_tokenize(strip_punc(text.lower(), all=True))\n        if word not in stopwords\n    ]\n\n    for tok in tokens:\n        if tok & cyrillic:\n            ru += 1\n        else:\n            en += 1\n\n\ndf[\"description\"].apply(count_langs)\n\n# (Rough) percentage of words that are English\nen \/ (en + ru)","b2da152d":"# Note that this model was trained on POS (part-of-speech) tagged words.\n# This means that we have to append a POS tag to the end of any Russian words before we look them up in this model.\nru_model = gensim.downloader.load(\"word2vec-ruscorpora-300\")","d3d2553e":"print(en_model.similar_by_word(\"king\", 1))\n\n# The result for this should be '\u0446\u0430\u0440\u0438\u0446\u0430', which means 'queen'\nprint(ru_model.similar_by_word(\"\u0446\u0430\u0440\u044c_NOUN\", 1))","2fc70503":"tsar_vector = ru_model.get_vector(\"\u0446\u0430\u0440\u044c_NOUN\")\nru_model.similar_by_vector(tsar_vector, 2)","67b6a804":"king_vector = en_model.get_vector(\"king\")\nen_model.similar_by_vector(king_vector, 2)","77dd805b":"print(en_model.similar_by_vector(tsar_vector, 2))\nprint(ru_model.similar_by_vector(king_vector, 2))","21c14b08":"!pip install -q transvec","f3ad3f50":"from transvec.transformers import TranslationWordVectorizer\n\n# transvec also includes a tokenizer that deals with the Russian POS tags that the pre-trained Russian model uses.\nfrom transvec.tokenizers import EnRuTokenizer\n\n\nword_pairs = pd.read_csv(f\"{datadir}\/..\/enru-word-pairs\/ru_word_translations.csv\")[[\"en\", \"ru\"]]\n\n# The transvec model takes the target language model first, followed by any source languages (you can provide more if you have a mix of more than two languages).\nenru_model = TranslationWordVectorizer(\n    en_model, ru_model, alpha=1, missing=\"ignore\"\n).fit(word_pairs)","d27440ca":"# Our model can now automatically translate Russian words into a vector in English space.\n# It doesn't get it right every time, but we can see that 6 out of the top ten words for \"\u0446\u0430\u0440\u044c\" (\"tsar\"), are correctly related by meaning.\n# Note that if we provided an English word, the model would just default to the normal English-language vectors.\nprint(enru_model.similar_by_word(\"\u0446\u0430\u0440\u044c_NOUN\"))","a6757212":"tokenizer = EnRuTokenizer()\ntokens = df[\"description\"].apply(tokenizer.tokenize)\n\nitem_vectors = pd.DataFrame(\n    enru_model.transform(tokens), index=df[\"description\"]\n).fillna(0)","92d0ec1a":"display(plot_vectors(item_vectors, data=df, n_labels=7))\n\nnn = NearestNeighbors(n_neighbors=3, metric=\"cosine\", n_jobs=-1).fit(item_vectors)\n\n# Preview the nearest neighbours for the first few items.\nwith pd.option_context(\"display.max_colwidth\", None):\n    display(\n        pd.DataFrame(\n            df[\"description\"].values[\n                nn.kneighbors(item_vectors[:10], n_neighbors=3, return_distance=False)\n            ]\n        )\n    )","1abd362d":"# First, prepare the training data and test data into a single dataframe.\n\ndef denormalize(df):\n    return df.merge(pd.read_csv(f\"{datadir}\/items.csv\")).merge(\n        pd.read_csv(f\"{datadir}\/item_categories.csv\")\n    )\n\n\ntrain = (\n    # Take the mean item price and item count for each month\/shop\/item combo\n    pd.read_csv(f\"{datadir}\/sales_train.csv\")\n    .groupby([\"date_block_num\", \"shop_id\", \"item_id\"])\n    .agg({\"item_price\": \"mean\", \"item_cnt_day\": [\"mean\", \"sum\"]})\n    .reset_index()\n)\ntrain.columns = [\"_\".join([c for c in col if c]) for col in train.columns]\ntrain.rename(columns={\"item_cnt_day_sum\": \"item_cnt_month\"}, inplace=True)\ntrain = denormalize(train)\n\ntest = pd.read_csv(f\"{datadir}\/test.csv\").drop(\"ID\", axis=1)\ntest = denormalize(test)\ntest[\"date_block_num\"] = 34\n\ndata = pd.concat([train, test])\ndata","49baa348":"# Next, prepare a nearest neighbours lookup table to allow us to look up similar items quickly.\n\nitems = data.groupby(\"item_id\")[[\"item_category_name\", \"item_name\"]].first()\nitems[\"description\"] = items[\"item_category_name\"] + \"\\n\" + items[\"item_name\"]\nitems = items.drop(columns=[\"item_category_name\", \"item_name\"])\n\ntokenizer = EnRuTokenizer()\ntokens = items[\"description\"].apply(tokenizer.tokenize)\n\n# Index by item ID this time - it's easier to work with later on.\nitem_vectors = pd.DataFrame(enru_model.transform(tokens), index=items.index).fillna(0)\nnn = NearestNeighbors(n_neighbors=3, metric=\"cosine\", n_jobs=-1).fit(item_vectors)\n\n# Number of neighbours we want to calculate for each item.\nk = 3\n\nall_neighbours = pd.DataFrame(\n    nn.kneighbors(n_neighbors=k, return_distance=False), index=item_vectors.index,\n)\nall_neighbours","4f499eaf":"def add_nearest_neighbours(df, nns=all_neighbours):\n    \"Create a copy of df with extra columns containing the item IDs of the k most similar items (by description)\"\n\n    mergecol = \"item_id\"\n    nn_ids = (\n        nns.loc[df[mergecol]]\n        .astype(np.int16)\n        .rename(mapper=lambda x: f\"{mergecol}_nn_{x + 1}\", axis=\"columns\")\n    )\n\n    return pd.concat([df, nn_ids.set_index(df.index)], axis=1)","2d152a15":"# Add in the most similar item IDs to each row.\ndata = add_nearest_neighbours(data)\ndata","8b271ea0":"def lagjoin(df, groupon, features, lags, nns=[0], agg=\"mean\", dtype=None):\n    lagcols = pd.DataFrame(index=df.index)\n\n    if isinstance(groupon, str):\n        features = [groupon]\n    if isinstance(features, str):\n        features = [features]\n\n    for lag, nn in tqdm(list(product(lags, nns))):\n        # A lag of 0 means the current month. A nn of 0 means the current item, not a similar one.\n        \n        if not lag and not nn:\n            # Duplicate of original data.\n            continue\n\n        shifted = df[groupon + features].groupby(groupon).agg(agg).reset_index()\n        shifted[\"date_block_num\"] += lag\n\n        lgrpcols = [\n            col if col != \"item_id\" or not nn else f\"{col}_nn_{nn}\" for col in groupon\n        ]\n        rgrpcols = groupon\n\n        newfeatures = df.merge(\n            shifted, left_on=lgrpcols, right_on=rgrpcols, how=\"left\"\n        )[[f + \"_y\" if f in df.columns else f for f in features]]\n        newfeatures.columns = features\n\n        colnames = [fcol + f\"_lag_{lag}\" if lag else fcol for fcol in features]\n        if nn:\n            colnames = [fcol + f\"_nn_{nn}\" if nn else fcol for fcol in colnames]\n\n        if dtype is None:\n            newdata = newfeatures.values\n        else:\n            newdata = newfeatures.values.astype(dtype)\n\n        lagcols = pd.concat(\n            [lagcols, pd.DataFrame(newdata, columns=colnames, index=lagcols.index)],\n            axis=1,\n        )\n\n    return lagcols","cadaddd8":"newfeatures = lagjoin(\n    data,\n    \n    # Try different groupings, e.g. month, item and shop ID.\n    groupon=[\"date_block_num\", \"item_id\"],\n    \n    # Can also be a list of columns.\n    features=\"item_cnt_month\",\n    \n    # A lag of 0 lets us get data for similar items in the same month.\n    lags=[0, 1],\n    \n    # A nearest neighbour of 0 lets us get data for the same item in previous months.\n    nns=[0, 1, 2, 3],\n)\n\ndata = pd.concat([data, newfeatures], axis=1)\ndata","3abe1610":"## Using text descriptions to make predictions for new items\n\nThis notebook will cover how to learn new features from the text data, which may be used to improve your predictions: particularly for new items that first appear in the test set, meaning we have no direct history to train them on.\n\nThe competition description states that the test set can (and does) include some new items that have never been seen in the training data. How do we make predictions for them? Well, we can look at the sales of items from the same shop, or items from the same category. These are both valid options and will help you. But intuition tells us that if a new album by the band FooBar arrives in the shops, one of the best ways to predict the sales for it would be to look at the performance of other albums by the same band. Can we do this with the competition data?\n\n*(spoiler: the answer is yes)*\n\n**_Note:_** The following approach allows you to take advantage of data in multiple languages without needing to speak any of them :) It also doesn't depend on any kind of external translation services like Google Translate, except to produce an initial training set which has already been done for you, and can be downloaded at the bottom of this notebook.","b53307c4":"We get results that make no sense. This is because vectors produced by one model are meaningless in the vector space of a different model. If we mix vectors from two different models our item clustering will be completely useless. We need to find a better way of using the information from both languages.","55e8d5db":"We can now use this to produce item vectors that use the information from *both* languages! `transvec` models include a scikit-learn `Transformer` API to make this task easier:","3e32a8e9":"Straight away there are a few interesting points to note:\n\n1. For both items and categories, we can see common words that suggest similarity (e.g. the PC, PS3 and XBox versions of *007 Legends*).\n2. We can see a mix of both Russian and English text.\n\nThe first point is exactly what we were hoping for. This suggests we can use some text pre-processing to help our models learn which items are related. Let's look at the options:\n\n* **One-hot encoding (bag-of-words).** Tells us which words have been seen in each item with a simple true\/false flag. Doesn't account for the similarity of word meanings though, or the importance of each word in the description. It also can't give us a useful comparison between different pairs of items: difference is only measured by how many exact word matches there are in each description - the non-common words do not contribute to the similarity measurement.\n* **TF-IDF vectorisation.** Slightly better than one-hot encoding, as we now give each word a score based on its importance. Apart from this though, it has many of the same shortcomings as OHE.\n* **Word embeddings.** Word embeddings are the output of a neural network that has been trained to spot words that often appear in similar contexts. Word embeddings are a very powerful way of representing words and sentences. Not only can they tell us if the same words are appearing in two different item descriptions, but they can also measure how similar the *meanings* of the words are.\n\nWord embeddings would be a good option, but how do we create them? We could train our own word embedding model (or better still, a document embedding model which also accounts for how the words in our item descriptions are grouped), but to capture the full meaning of each word (and therefore spot any similarities between items) we need a large corpus of data for our model to learn the meanings. We will only spot similaries from our own training data if we keep seeing the exact same words appearing in similar items. These similarities could be identified with a simple one-hot or TF-IDF encoder, however we want to use word embeddings to identify much more subtle similarities, e.g. that *PS3* and *XBox* products are similar. This is where word embeddings come in handy.","97843774":"We now have data for both the item and similar items during the current month (for training data) and previous months (for training and test data). You can go further by imputing missing values, e.g. by taking averages across shop\/item groups, which will give you more data to work with for \"new\" items (items that are not present in the training data).","7485867c":"We have textual descriptions for everything, but for the items in the test set there is no price or sales information. If the items are similar to another item that was seen in the historic training data, we could use the pricing\/sales info for the similar item as a best guess.","fe7a6b35":"We can now see that items are being clustered based on both the English and the Russian words, and that there are more distinct clusters which do a better job of grouping items with similar prices.","f0eb9b7f":"Now we have the most similar items we can use their IDs for self-joins to get features for any new items we have no history for. To demonstrate this, let's generate a couple of new features: the average monthly price and total daily item count for the most similar items.\n\nWe'll define a new function to do this. As well as gathering data on nearest neighbours, it can also deal with time lags for us (get the data from X months ago) so we can generate historic features for both the item itself and its nearest neighbours.","24805efc":"As we can see, the model does a good job of recognising the similarity between the words \"PS3\" and \"XBox\":","1e4fdb38":"We can see from the plot that some clustering has taken place, and some of that clustering is identifying items with a similar price which is good. However, if we train a nearest neighbours model to show which items are closest to each other, we can actually see that the results are quite disappointing. Why is this? It's because our word embedding model was trained on *English* language text - it has no knowledge of Russian. This means that Russian words, which make up a significant amount of our text, aren't contributing any information to our similar items model. You can see this in the preview above: the only rows that have similar items are the ones with more English words in their descriptions.\n\nWe could use a Russian language model instead, but this would ignore the English words. To see which language would capture most of the information, let's do a simple count of how many Russian words there are vs. English words:","a41e8e84":"## Item and category descriptions\n\nFortunately there are just two fields that contain all of the information we know about an item: `item_name` and `item_category_name`. Unfortunately, they are unstructured text fields. Let's take a look:","5719d549":"A crude, but effective way of creating a single vector for an item description is to take the vector for each word in the sentence and average them. This will give us a single vector that we can then compare to the vectors of other items to see which ones are most similar by finding the items with the smallest cosine distance. A simple `NearestNeighbors` model from scikit-learn will do this for us efficiently.\n\nLet's try that for our item descriptions. We will combine both the category name and the item name to make an overall item description.\n\nThe vectors generated by this model have 300 dimensions, but we can use dimensionality reduction to get 2D vectors that we can visualise. t-SNE works well for this but is slow for the size of this dataset, so we will use UMAP (`pip install umap-learn`) instead which produces similar results much faster.","63f0148a":"So far so good. But what happens when we look up the vectors in different models?","0883963b":"## Putting it all together\n\nWe can now use our item vectors to find the most similar items for every record. We can add this in to our original train and test data, meaning that we can now add features to each record based on not just the history of the item in question, but also the history of similar items.","dc26ac88":"Oh dear: 40% of our words are English and 60% are Russian. Whichever model we choose, we are going to lose a lot of information. To get the most value out of our data, we need to find a way of using both languages.","8ad71bc5":"Fortunately, it's very easy to get hold of a word embedding model that's been pre-trained on a very large corpus of data. Let's use `gensim` to do this:","88f4a260":"### Aside: word vectors\n\nYou might now wonder if we can just use a Russian-language model to vectorise the Russian words, an Engligh-language model to vectorise the English words, and then take the mean of all the vectors as before to get an overall vector for the mixed-language text. Unfortunately this won't work. This is because word vectors only have meaning in the vector space defined by the model they came from. Comparing one word vector to another vector from a different model will give a meaningless answer. Let's illustrate with an example:","17020cf6":"## The `transvec` package\n\nFortunately there is a way around this. [Exploiting Similarities among Languages for Machine Translation Mikolov et. al, 2013.](https:\/\/arxiv.org\/pdf\/1309.4168.pdf) shows us that word vector spaces for different languages share some linear similarities in their structure. This means we can use a tranformation matrix to convert word vectors from a source space (e.g. Russian) into a target space (e.g. English), and the transformed vector should be close to vectors in the target space with a similar meaning.\n\nThe research paper shows that we can learn the transformation matrix from some training data using Ordinary Least Squares regression. The data we need is just pairs of Russian words with their English translations.\n\nA Russian\/English dictionary would make ideal training data, but we don't have one available so we'll have to do something with the data we have instead. A simple translation training set was produced by running all of the words in the category and item names through the Google translate API. This gave ~10,500 word pairs to learn a translation matrix from.\n\nTo make this technique reusable in other competitions, the code for producing these translation models was put into a separate package called `transvec` which is now available on PyPi. `transvec` allows you to convert word vectors from any number of source languages into the vector space of a single 'target' language. In this example, we will use English as the target language since this notebook is in English, although it would be just as valid to use Russian as the target language."}}