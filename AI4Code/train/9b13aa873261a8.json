{"cell_type":{"a93cd026":"code","bab02dc5":"code","41affdf8":"code","b220eaab":"code","b55053ec":"code","5ccb55dc":"code","151dc30b":"code","745ed10d":"code","af9831ae":"code","11b5a441":"code","3681f770":"code","316e1f00":"code","c45c9fc3":"code","f8eb026f":"code","3ab10bc3":"code","15a8d244":"code","3c5e9972":"code","c4c737ba":"code","78c2e4b3":"code","7d6760e8":"code","5dfaee4b":"code","9c7bcb00":"code","740f2155":"code","f4915cd2":"markdown","ad8889fd":"markdown","52fc9752":"markdown","52786f66":"markdown"},"source":{"a93cd026":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bab02dc5":"import re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D , LSTM\nfrom tensorflow.keras.optimizers import Adam\nfrom keras import optimizers, callbacks \nfrom sklearn.metrics import log_loss\nfrom keras.layers import Flatten\nimport matplotlib.pyplot as plt\n\n","41affdf8":"train_df = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ntrain_df.head(5)","b220eaab":"print(train_df.columns)\nprint(train_df['target'].value_counts())","b55053ec":"train_df['question_text'].str.len().hist()","5ccb55dc":"train_df['question_text'].str.split().map(lambda x : len(x)).hist(bins=64)\n","151dc30b":"max_length = 45","745ed10d":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how does\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\" u \": \" you \",\n\" ur \": \" your \",\n\" n \": \" and \"}","af9831ae":"all_text = ' '.join(train_df['question_text'])\nall_text = all_text.split()\nfrequence  = pd.Series(all_text).value_counts()\n\none_word = frequence[frequence.values == 1]\none_word[5:20]","11b5a441":"def clean_question(x):\n    if type(x) is str:\n        x = x.lower() # transformer to lower \n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key, value)\n            \n        x = re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x) # regex to remove to emails\n       \n        x = re.sub(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', '', x)   #regex to remove URLs     \n        x = re.sub( u\"\\s+\", u\" \", x ).strip() # remove multiple  espace and back line\n        x = ' '.join([t for t in x.split() if t not in one_word])  #combining all the text excluding rare words.\n        return x\n    else:\n        return x\n\ntrain_df['question_text'] = train_df['question_text'].apply(lambda x: clean_question(x))        \ntrain_df['question_text'] = train_df['question_text'].tolist()\n","3681f770":"print(train_df['question_text'][:6])","316e1f00":"\ntoken = Tokenizer() \ntoken.fit_on_texts(train_df['question_text'])\nvac = token.index_word\n","c45c9fc3":"dict(list(vac.items())[0:10]) ","f8eb026f":"#declaring the vocab_size\nvocab_size  = len(token.word_index) + 1\nvocab_size","3ab10bc3":"##### conversion to numerical formats\n\nencoded_text = token.texts_to_sequences(train_df['question_text'])\nprint(encoded_text[:3])\n\n#padding='post' means that we padding post the sentence(keeping values 0 if the tokens are not there)\n\nX = pad_sequences(encoded_text, maxlen=max_length, padding='post')\nprint(X[:2])","15a8d244":"\nglove_vectors = dict()\n\nfile = open('..\/input\/nlpword2vecembeddingspretrained\/glove.6B.200d.txt', encoding='utf-8')\n\nfor line in file:\n    values = line.split()\n    word = values[0]\n    #storing the word in the variable\n    vectors = np.asarray(values[1: ])\n    #storing the vector representation of the respective word in the dictionary\n    glove_vectors[word] = vectors\nfile.close()\n\n#printing length of glove vectors\nlen(glove_vectors)\n","3c5e9972":"glove_vectors.get('you').shape","c4c737ba":"word_vector_matrix = np.zeros((vocab_size, 200))\n\nfor word, index in token.word_index.items():\n    vector = glove_vectors.get(word)\n    if vector is not None:\n        word_vector_matrix[index] = vector","78c2e4b3":"word_vector_matrix[3]","7d6760e8":"y = train_df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2, stratify = y)\n","5dfaee4b":"vector_size = 200\nWEIGHTS_PATH = '.\/w0.h5'\nmc = callbacks.ModelCheckpoint( filepath=WEIGHTS_PATH, monitor='val_loss', mode='min', save_best_only=True )\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, vector_size, input_length=max_length, weights = [word_vector_matrix], trainable = False))\nmodel.add(LSTM(64))\nmodel.add(Dense(100, activation='relu'))\n\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=Adam(learning_rate = 0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])\nes = callbacks.EarlyStopping( patience=1 )\nhistory = model.fit(X_train, y_train, epochs = 2, validation_data = (X_test, y_test) , callbacks=[es , mc] , batch_size=2048 )\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['loss'])\nplt.legend( ['test', 'train'] )","9c7bcb00":"print(y_test[:10])","740f2155":"preds = model.predict( X_test , batch_size=2048)\n\nprint ( 'test ssscore : ', preds[:10] ,log_loss(y_test, preds, eps = 1e-7) )","f4915cd2":"Now we are creating a matrix for the tokens which we are having in our dataset and then storing their vector representation values in the matrix if it matches with glove_vectors words","ad8889fd":"rare word ","52fc9752":"all most questions with max word is 45","52786f66":"#  import library"}}