{"cell_type":{"cb411fb3":"code","66d0a1a8":"code","c3aa93ba":"code","375289e3":"code","5ccc09bb":"code","1269b193":"code","935effdc":"code","e94e9dd3":"code","0231ca83":"code","da3fce1b":"code","9e82d81d":"code","a04621ee":"code","22c93978":"code","65b52548":"code","851b6713":"code","53eb8890":"code","526b6f05":"code","aa4ae9a8":"code","e68abdb7":"code","9f07eceb":"code","921b3ff7":"code","d1ef033d":"code","c3513149":"code","7427acb2":"code","9c377f45":"code","e7ae67a0":"code","7e3a35d3":"code","dd9e16e0":"code","d3388417":"code","5e083090":"code","47cb331f":"code","e16c61f5":"code","8c4dcbc5":"code","5c48066a":"code","11cbd69c":"code","aa8f2fd5":"code","559dd65c":"code","84fabb10":"code","32443bc5":"code","9715329a":"markdown","7109c106":"markdown","258b7f44":"markdown","69519ed8":"markdown","cde606e9":"markdown","8ea04875":"markdown","8f5f97ab":"markdown","e4259c15":"markdown","5b60bd5c":"markdown","8e44b23e":"markdown","e54cc091":"markdown","814b7de5":"markdown","462051ba":"markdown","3fb12c7e":"markdown","b8b25b1d":"markdown","b9a004e6":"markdown","febf6bab":"markdown","75c09be6":"markdown","e5dc20f2":"markdown","68d77a62":"markdown","318298df":"markdown","04876964":"markdown","0cd854e8":"markdown"},"source":{"cb411fb3":"from pandas import read_csv\nimport pandas as pd\nfrom datetime import datetime\nimport missingno as msno\n\nfrom math import sqrt\nimport numpy as np\nfrom numpy.random import randn\n# this line makes numpy stop printing the mathematical errors (such as divide by zero)  \nnp.seterr(divide='ignore', invalid='ignore');\nimport matplotlib.pyplot as plt, seaborn as sns\n\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.gofplots import qqplot\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nimport os\nimport random\nimport time\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Dropout, GRU, LeakyReLU, BatchNormalization, LSTM, Conv2D, Conv2DTranspose, Flatten, Add, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import Sequential, callbacks, Model\nfrom tensorflow.keras.models import load_model\n\n!pip install pyts\nfrom pyts.image import GramianAngularField as GAF\nfrom mpl_toolkits.axes_grid1 import ImageGrid\n\n# load dataset\nseries = pd.read_csv(\"\/kaggle\/input\/wind-power-forecasting\/Turbine_Data.csv\")","66d0a1a8":"#the dataset shows 118224 observations for 22 features but some of the features have significant periods of missing data. We need to discard some periods and fill in the missing periods, otherwise our LSTM model will not converge or we may end up feeding it with garbage data.\n#the rightmost part of the msno matrix plot shows completess of data, it indicates that the last half of dataset is more complete. We will strive to use this part for training and testing.\nmsno.matrix(series)","c3aa93ba":"#correlation plot shows high positive correlation between a number of features, which will make them redundant. Discarting these features will help avoid the curse of dimensionality.\n#For this demonstration, we will only focus on the Target Y - \"ActivePower\", and Predictor feature - \"WindSpeed\". This further simplifies our task.\n#If you happen to have the mechanical engineering expertise knowledge about wind turbines, you will know that the wind power out is mostly dependent upon the weather conditions (e.g., wind speed), and you can therefore you can discard the rest of features.\nplt.subplots(figsize=(16, 16))\nsns.heatmap(series.corr(), annot=True, square=True)\nplt.show()","375289e3":"idxs = []\n\n# we first get rid of all features that have no correlation with the active power\nfor i, corr in enumerate(series.corr()['ActivePower']):\n    if corr > 0 and pd.notna(corr):\n        idxs.append(i)\n        \nidxs = idxs[1:]\nselected_features = series.corr()['ActivePower'][idxs]\n\n# now we can cualitatively eliminate some features that are redundant, i.e. they have high correlation \n# and they physically correspond to a similar event (gearbox bearing T and gearbox oil T)\nselected_features = selected_features.drop(labels=['GearboxOilTemperature', 'GeneratorWinding1Temperature', 'RotorRPM', 'NacellePosition'])\nsel_feature_names = [idx for idx, val in selected_features.iteritems()]\n#don't forget ActivePower\nsel_feature_names.insert(0, 'ActivePower')\nreduced_df = series[sel_feature_names]\nprint(f\"The selected features are:\\n{sel_feature_names}\")","5ccc09bb":"# we can see that there are still some big chunks of missing data among our selected features\nmsno.matrix(reduced_df, figsize=(10,5))\nplt.show()","1269b193":"'''pre-processing from the baseline'''\n\n#generate timestamp for the observations mimicking the \"unnamed:0\" feature\nrng = pd.date_range('2017-12-31', periods=118224, freq='10T')\ntime_df = pd.DataFrame(rng)\n#fill in missing values with zero using the fillforward function\nreduced_df = reduced_df.fillna(0).astype(float)\n#concatenate both the timestamp range and filled in features\nreduced_df = pd.concat((time_df, reduced_df), axis=1)\n#set up index\nreduced_df = reduced_df.set_index(0)\n#select a subset of data period from second half of original dataset, to ensure we have better quality signal (see completness comment of msno matrix plot above in the code)\nreduced_df = reduced_df.loc['2019-12-17':]","935effdc":"def forecast_accuracy(forecast, actual):\n    '''\n    (Recycled from notebook) Returns evaluation metrics given predicted and true values.\n    '''\n    forecast = np.array(forecast)\n    actual = np.array(actual)\n    \n    mape = np.mean(np.abs(forecast - actual)\/np.abs(actual))  # MAPE\n    \n    me = np.mean(forecast - actual)             # ME\n    \n    mae = np.mean(np.abs(forecast - actual))    # MAE\n    \n    mpe = np.mean((forecast - actual)\/actual)   # MPE\n    \n    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n    \n    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n    \n    mins = np.amin(np.hstack([forecast[:,None], actual[:,None]]), axis=1)\n    maxs = np.amax(np.hstack([forecast[:,None], actual[:,None]]), axis=1)\n    \n    minmax = 1 - np.mean(mins\/maxs)             # minmax\n    \n    return({'mape':mape, 'me':me, 'mae': mae, \n            'mpe': mpe, 'rmse':rmse, \n            'corr':corr, 'minmax':minmax})\n\n# scales back from normalized values\ndef invert_values(y_pred, y_test, scaler) -> tuple:\n    '''\n    De-normalizes the predicted and true values given a scaler. \n    '''\n    n_ahead = 144\n    rng = pd.date_range('2017-12-31', periods=118224, freq='10T')\n    time_df = pd.DataFrame(rng)\n\n    # Creating the predictions date range\n    days = time_df.values[-len(y_pred):-len(y_pred) + n_ahead]\n    days_df = pd.DataFrame(days)\n\n    pred_n_ahead = pd.DataFrame(y_pred)\n    actual_n_ahead = pd.DataFrame(y_test)\n\n    #repeat the column series n times (being n the number of features used), to make shape compatible for scale inversion\n    pr_p = pd.concat([pred_n_ahead]*len(sel_feature_names), axis=1)\n    ac_p = pd.concat([actual_n_ahead]*len(sel_feature_names), axis=1)\n\n    #inverse scale tranform the series back to kiloWatts of power\n    pred_power = scaler.inverse_transform(pr_p)[:,0]\n    actual_power = scaler.inverse_transform(ac_p)[:,0]\n    \n    return pred_power, actual_power\n\n\n\ndef plot_random_predictions(pred_power, actual_power, n_predictions=6):\n    '''\n    Plots the prediction of 6 randomly selected days (out of the 21 predicted)\n    The zeros that are filled in during pre-processing are marked in lime green\n    '''\n    if n_predictions > 21:\n        print('Error: there are only 21 days in the predicted set')\n        return\n    \n    # 21 days in total\n    days = np.arange(0,21)\n    np.random.shuffle(days)\n\n    # we assume hard zeros are the ones forced by pre-processing\n    cond_color = np.where(actual_power==0, 'lime', plt.rcParams['axes.prop_cycle'].by_key()['color'][0])\n\n    fig, ax = plt.subplots(3,2, figsize=(30, 20))\n\n    ax_h = 0\n    ax_v = 0\n    for i in range(0,n_predictions):\n        ax[ax_h,ax_v].plot(rng[days[i]*144:(days[i]+1)*144], actual_power[days[i]*144:(days[i]+1)*144], label=\"actual\")\n        ax[ax_h,ax_v].scatter(rng[days[i]*144:(days[i]+1)*144], actual_power[days[i]*144:(days[i]+1)*144], color=cond_color[days[i]*144:(days[i]+1)*144])\n        ax[ax_h,ax_v].plot(rng[days[i]*144:(days[i]+1)*144], pred_power[days[i]*144:(days[i]+1)*144], marker='o', label=\"prediction\")\n        ax[ax_h,ax_v].legend()\n        ax[ax_h,ax_v].set_title(f\"Predicted vs. Actual Power (Day {days[i]+1})\")\n        ax[ax_h,ax_v].set_xlabel(\"Time\")\n        ax[ax_h,ax_v].set_ylabel(\"Power (kW)\")\n        if i%2 == 0:\n            ax_v = 1\n        else:\n            ax_h += 1\n            ax_v = 0\n\n    plt.show()","e94e9dd3":"# Scaling data between 0 and 1\nscaler = MinMaxScaler()\nscaler.fit(reduced_df.values)\ndata_scaled = scaler.transform(reduced_df.values)","0231ca83":"# we split the data twice: for regression and for training of nn\n\ntest_split_days = 21 # 20%\nval_split_days = 10 # around 10%\n\ntest_split_idx = len(reduced_df)-144*test_split_days\nval_split_idx = test_split_idx-144*val_split_days\n\nprint(\"-- Regression --\")\ntrain_data = data_scaled[:test_split_idx]\ntest_data = data_scaled[test_split_idx:]\nprint(f\"Train data shape: {train_data.shape}\\nTest data shape: {test_data.shape}\\n\")\n\nprint(\"-- NN --\")\ntrain_data_nn = data_scaled[:val_split_idx]\nval_data_nn = data_scaled[val_split_idx:test_split_idx]\ntest_data_nn = data_scaled[test_split_idx:]\nprint(f\"Train data shape: {train_data_nn.shape}\\nVal data shape: {val_data_nn.shape}\\nTest data shape: {test_data_nn.shape}\")","da3fce1b":"print(\"-- Regression --\")\nx_train, y_train = train_data[:,1:], train_data[:,0]\nprint(f\"Train values shape: {x_train.shape}\\nTrain labels shape: {y_train.shape}\")\n\nx_test, y_test = test_data[:,1:], test_data[:,0]\nprint(f\"Test values shape: {x_train.shape}\\nTest labels shape: {y_train.shape}\\n\")\n\nprint(\"-- NN --\")\nx_train_nn, y_train_nn = train_data_nn[:,1:], train_data_nn[:,0]\nprint(f\"Train values shape: {x_train_nn.shape}\\nTrain labels shape: {y_train_nn.shape}\")\n\nx_val_nn, y_val_nn = val_data_nn[:,1:], val_data_nn[:,0]\nprint(f\"Val values shape: {x_val_nn.shape}\\nVal labels shape: {y_val_nn.shape}\")\n\nx_test_nn, y_test_nn = test_data_nn[:,1:], test_data_nn[:,0]\nprint(f\"Test values shape: {x_test_nn.shape}\\nTest labels shape: {y_test_nn.shape}\\n\")\n","9e82d81d":"'''cv_params = {'epsilon': np.linspace(0,0.1,9),'C': np.linspace(1,10,10)}\n# 84 folds would take a day per fold. Therefore we take lower multiples\ngrid = GridSearchCV(SVR(), cv_params, scoring='neg_mean_absolute_error', cv=21, n_jobs=4, verbose=1) # 4 days per fold\ngrid.fit(x_train, y_train)\nprint(f\"The best model has parameters C={grid.best_params_['C']} and epsilon={grid.best_params_['epsilon']}\")\n\n# predict using the best model\n#y_pred_svr = grid.best_estimator_.predict(x_test)''';","a04621ee":"svr_model = SVR(epsilon=0.0, C=2.0)\nsvr_model.fit(x_train, y_train)\n\n#predict\ny_pred_svr = svr_model.predict(x_test)\n\n# de-normalize values\npred_power_svr, actual_power_svr = invert_values(y_pred_svr, y_test, scaler)\n\n# get scores\nforecast_accuracy(pred_power_svr, actual_power_svr)","22c93978":"plot_random_predictions(pred_power_svr, actual_power_svr)","65b52548":"residuals_svr = pd.Series(pred_power_svr-actual_power_svr)\nresiduals_svr.describe()","851b6713":"fig, ax = plt.subplots(2,2, figsize=(20,10))\nresiduals_svr.plot(kind='hist', title=\"Residuals\", ax=ax[0,0])\nresiduals_svr.plot(kind='kde', title='Density', ax=ax[0,1])\nqqplot(residuals_svr, ax=ax[1,0])\nautocorrelation_plot(residuals_svr, ax=ax[1,1])\nplt.show()","53eb8890":"# reshape training data to feed days individually\nx_train_reshaped, x_val_reshaped, x_test_reshaped = x_train_nn.reshape(-1, 144, 9), x_val_nn.reshape(-1, 144, 9), x_test_nn.reshape(-1, 144, 9)\ny_train_reshaped, y_val_reshaped, y_test_reshaped = y_train_nn.reshape(-1, 144), y_val_nn.reshape(-1, 144), y_test_nn.reshape(-1, 144)","526b6f05":"## model parameters ##\ndepth = 2\n\n# create the GRU model\nmodel_gru = Sequential()\n\nfor i in range(1, depth+1):\n\n    # we leave the standard arguments for GPU speeding up\n    model_gru.add(GRU(units=64, input_shape=x_train_reshaped.shape[1:], return_sequences=True))\n    model_gru.add(BatchNormalization())\n    model_gru.add(Dropout(0.4))   \n\n# maps every neuron to a single output (predicted time series)\nmodel_gru.add(Dense(units=1))\n\nmodel_gru.summary()\n#plot_model(model)","aa4ae9a8":"# training parameters\nlr = 1e-3\nloss = 'mae'\nmetrics = 'mae'\n\n# compile and additional parameters\npatience = 5\nmin_delta_patience = 1e-4\ncheckpoint_path = 'best_gru.hdf5'\n\nweights_save = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=patience, min_delta=min_delta_patience)\nmodel_gru.compile(optimizer=Adam(learning_rate=lr), loss=loss, metrics=metrics)","e68abdb7":"# running parameters\nbatch_size = 1 # this is done because we want 1 step predictions, therefore we need to update for every step\nepochs = 50\n\nhistory_gru = model_gru.fit(x_train_reshaped,\n                    y_train_reshaped,\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    validation_data=(x_val_reshaped, y_val_reshaped),\n                    shuffle=False,\n                    callbacks=[early_stopping, weights_save])\n\nplt.plot(history_gru.history['loss'], label='Training loss')\nplt.plot(history_gru.history['val_loss'], label = 'Validation loss')\nplt.legend()\nplt.grid()\nplt.show()\n\nmodel_gru.load_weights(checkpoint_path)","9f07eceb":"y_pred_gru = model_gru.predict(x_test_reshaped)\npred_power_gru, actual_power_gru = invert_values(y_pred_gru.reshape(-1, ), y_test_nn, scaler)\n\nforecast_accuracy(pred_power_gru, actual_power_gru)","921b3ff7":"plot_random_predictions(pred_power_gru, actual_power_gru)","d1ef033d":"residuals_gru = pd.Series(pred_power_gru-actual_power_gru)\nresiduals_gru.describe()","c3513149":"fig, ax = plt.subplots(2,2, figsize=(20,10))\nresiduals_gru.plot(kind='hist', title=\"Residuals\", ax=ax[0,0])\nresiduals_gru.plot(kind='kde', title='Density', ax=ax[0,1])\nqqplot(residuals_gru, ax=ax[1,0])\nautocorrelation_plot(residuals_gru, ax=ax[1,1])\nplt.show()","7427acb2":"# model parameters\ndepth = 2\n\n#create the LSTM model\nmodel_lstm = Sequential()\n\nfor i in range(1, depth+1):\n\n    model_lstm.add(LSTM(units=64, input_shape=x_train_reshaped.shape[1:], return_sequences=True))\n    model_lstm.add(BatchNormalization())\n    model_lstm.add(Dropout(0.4))\n\n# maps every neuron to a single output (predicted time series)\nmodel_lstm.add(Dense(units=1))\n\nmodel_lstm.summary()\n#plot_model(model_lstm)","9c377f45":"# training parameters\nlr = 1e-3\nloss = 'mae'\nmetrics = 'mae'\n\n# compile and additional parameters\npatience = 5\nmin_delta_patience = 1e-4\ncheckpoint_path = 'best_lstm.hdf5'\n\nweights_save = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=patience, min_delta=min_delta_patience)\nmodel_lstm.compile(optimizer = Adam(learning_rate=lr), loss=loss, metrics=metrics)","e7ae67a0":"# running time\nbatch_size = 1 # this is done because we want 1 step predictions, therefore we need to update for every step\nepochs = 50\n\nhistory_lstm = model_lstm.fit(x_train_reshaped,\n                              y_train_reshaped,\n                              epochs=epochs,\n                              batch_size=batch_size,\n                              validation_data=(x_val_reshaped, y_val_reshaped),\n                              shuffle=False,\n                              callbacks=[early_stopping, weights_save])\n\nplt.plot(history_lstm.history['loss'], label='Training loss')\nplt.plot(history_lstm.history['val_loss'], label = 'Validation loss')\nplt.legend()\nplt.grid()\nplt.show()\n\nmodel_lstm.load_weights(checkpoint_path)","7e3a35d3":"y_pred_lstm = model_lstm.predict(x_test_reshaped)\npred_power_lstm, actual_power_lstm = invert_values(y_pred_lstm.reshape(-1, ), y_test_nn, scaler)\n\nforecast_accuracy(pred_power_lstm, actual_power_lstm)","dd9e16e0":"plot_random_predictions(pred_power_lstm, actual_power_lstm)","d3388417":"features_data = reduced_df.values[:,1:]\n\n# GAF does min max scaling already\ngadf_scaler = GAF(method='s', sample_range=(0,1))\ngadf_scaler.fit(features_data)\n\n# we have to reobtain the x_train, x_val and x_test, reshaping them to 4 dimensions to fit the input size\nx_train_cnn = gadf_scaler.transform(features_data[:val_split_idx])\nx_train_cnn = x_train_cnn.reshape(x_train_cnn.shape[0], x_train_cnn.shape[1], x_train_cnn.shape[2], 1)\n\nx_val_cnn = gadf_scaler.transform(features_data[val_split_idx:test_split_idx])\nx_val_cnn = x_val_cnn.reshape(x_val_cnn.shape[0], x_val_cnn.shape[1], x_val_cnn.shape[2], 1)\n\nx_test_cnn = gadf_scaler.transform(features_data[test_split_idx:])\nx_test_cnn = x_test_cnn.reshape(x_test_cnn.shape[0], x_test_cnn.shape[1], x_test_cnn.shape[2], 1)","5e083090":"### model parameters\ndepth = 3\nkernel_size = (3,3)\nlrelu_alpha = 0.2\ndropout = 0.4\nn_filters = 32\n\ninputs = Input(shape=x_train_cnn.shape[1:])\nb = inputs\nfor i in range(1, depth+1):\n\n    a = Conv2D(filters=n_filters*i, kernel_size=kernel_size, activation=None)(b)\n    x = LeakyReLU(alpha=lrelu_alpha)(a)\n    x = BatchNormalization()(x)\n    b = Dropout(dropout)(x)\n    \na = Conv2D(filters=n_filters*(i+1), kernel_size=kernel_size, activation=None)(x)\nx = LeakyReLU(alpha=lrelu_alpha)(a)\nx = BatchNormalization()(x)\nx = Flatten()(x)\nx = Dropout(dropout)(x)\n\noutputs = Dense(units=1, activation='sigmoid')(x)\n\nmodel_cnn = Model(inputs, outputs)\n\nmodel_cnn.summary()\n#plot_model(model_cnn)","47cb331f":"# compile parameters\nlr = 1e-4\n\n# callbacks parameters \npatience = 10\nmin_delta_patience = 1e-4\nloss = 'mae'\nmetrics = 'mae'\ncheckpoint_path = 'best_cnn.hdf5'\n\nweights_save = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=patience, min_delta=min_delta_patience)\nmodel_cnn.compile(optimizer = Adam(learning_rate=lr), loss=loss, metrics=metrics)","e16c61f5":"# running parameters \nepochs = 100\nbatch_size = 32\n\nhistory_cnn = model_cnn.fit(x_train_cnn,\n                            y_train_nn,\n                            epochs=epochs,\n                            batch_size=batch_size,\n                            validation_data=(x_val_cnn, y_val_nn),\n                            shuffle=False,\n                            callbacks=[early_stopping, weights_save])\n\nplt.plot(history_cnn.history['loss'], label='Training loss')\nplt.plot(history_cnn.history['val_loss'], label = 'Validation loss')\nplt.legend()\nplt.grid()\nplt.show()\n\nmodel_cnn.load_weights(checkpoint_path)","8c4dcbc5":"y_pred_cnn = model_cnn.predict(x_test_cnn)\npred_power_cnn, actual_power_cnn = invert_values(y_pred_cnn.reshape(-1, ), y_test_nn, scaler)\n\nforecast_accuracy(pred_power_cnn, actual_power_cnn)","5c48066a":"plot_random_predictions(pred_power_cnn, actual_power_cnn)","11cbd69c":"residuals_cnn = pd.Series(pred_power_cnn-actual_power_cnn)\nresiduals_cnn.describe()","aa8f2fd5":"\nfig, ax = plt.subplots(2,2, figsize=(20,10))\nresiduals_cnn.plot(kind='hist', title=\"Residuals\", ax=ax[0,0])\nresiduals_cnn.plot(kind='kde', title='Density', ax=ax[0,1])\nqqplot(residuals_cnn, ax=ax[1,0])\nautocorrelation_plot(residuals_cnn, ax=ax[1,1])\nplt.show()","559dd65c":"layer_outputs = [layer.output for layer in model_cnn.layers]\nfeature_map_model = Model(inputs=model_cnn.inputs, outputs=layer_outputs)\nfeature_maps = feature_map_model.predict(x_test_cnn)","84fabb10":"for i, layer in enumerate(model_cnn.layers):\n    if 'conv' in layer.name:\n        print(i)","32443bc5":"# convolutions at layers 1, 5, 9, ...\nshow_layer = 9\n\nfig = plt.figure(figsize=(10, 5))\n\ngrid = ImageGrid(fig, 111, nrows_ncols=(5, 10), axes_pad=0.1, share_all=True,\n                 cbar_mode='single')\nfor i, ax in enumerate(grid):\n    im = ax.imshow(feature_maps[show_layer][0, :, :, i], origin='lower', vmin=-1., vmax=1.)\n    \ngrid[0].get_yaxis().set_ticks([])\ngrid[0].get_xaxis().set_ticks([])\nplt.colorbar(im, cax=grid.cbar_axes[0])\nax.cax.toggle_label(True)\n\nfig.suptitle(f\"Feature maps for layer {model_cnn.layers[show_layer].name}\", y=0.92)\nplt.show()","9715329a":"### Visualize feature maps","7109c106":"#### Using the best parameters directly","258b7f44":"**LSTM isn't able to learn as easily as GRU, as it has more parameters and isn't that efficient. We can therefore discard it.**","69519ed8":"**By analysing the result, we can conclude that this method works quite well. The RMSE is drastically reduced, as well as the mean error, meaning that the prediction is better overall. The correlation metric shows that the predictions are very close to the actual values. The MAPE and MPE don't give useful result since the true power output is sometimes zero.**\n\n**Looking at the graphs with predictions vs. actual values, as well as at the residuals, we can see that the residuals have zero mean and a small variance, which is indeed a good sign since the error margins are minimal. However, The QQ plot indicates that the model has trouble handling extreme values. That can be confirmed in the previously mentioned plot, where the errors are located in zones containing those. The autocorrelation shows how the lagged information is well included in the predictions.**\n\n**Our interest now lies in finding a better model that is able to predict the extreme values as well as keeping the rest of prediction accurate**","cde606e9":"# CNN","8ea04875":"We want to check whether GRU indeed outperforms LSTM so that we can discard it","8f5f97ab":"### Residuals","e4259c15":"**After validation, the best architecture is a 2-layer with 64 units in each layer. Shallower models did not fit the curve well, while deep ones were overfitting and predicting completely wrong. This model captures well the data, but cannot reduce the error as well as the SVR model. the QQ plot shows how the extreme values are handled a bit better, but autocorrelation shows that the lagged values are integrated worse in the model.**\n\n\n**Looking at the training history, maybe a bit of overfitting can be observed in the validation curve since it oscillates slightly.**","5b60bd5c":"# GRU model\n","8e44b23e":"# LSTM","e54cc091":"# Visualisations and missing values","814b7de5":"**As it can be seen in the predictions and residuals, the CNN isn't learning well. Maybe the dimensionality of the input data is too low for a CNN, making the learning difficult for such a number of parameters. The intermediate layer do learn patterns properly, so it is working as intended. The problem is just too simple for this approach**","462051ba":"### Residuals analysis","3fb12c7e":"**Let's try with a GRU model, which is basically the same as LSTM, but a bit simpler and shown to be more efficient.**","b8b25b1d":"# Import Dataset and required python libraries","b9a004e6":"### Util functions","febf6bab":"### Visualize predictions","75c09be6":"#### This code was used to obtain the best parameters for SVR","e5dc20f2":"# First model: Support Vector Regression (SVR)","68d77a62":"**We're going to try a final resort for a neural network to perform well. Here we're using Gramian Angular Fields to convert the time series to image of size _nxn_ being _n_ the number of features. These images transform therefore every time point to a pixel in a non-cartesian way. Considering than CNNs are quite powerful, maybe they are able to improve the score over RNNs.**","318298df":"The are 105 days in total, so it's easier to split them by number of days instead of using a proportion which may cut a day if the resulting shape is not a multiple of 144","04876964":"We need to re-do the splitting, to count with a validation split. Reshaping is also necessary for inputting single days of data into the network","0cd854e8":"# Pre-processing "}}