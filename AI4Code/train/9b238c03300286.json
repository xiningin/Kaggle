{"cell_type":{"d2ceee09":"code","027c4c61":"code","9456f3c0":"code","cab9f01e":"code","c70777f7":"code","8601abb4":"code","8a34568f":"code","9eaeb624":"code","c67547db":"code","c939ca61":"code","afa0a7e9":"code","de13ddc4":"code","9f423a1d":"code","c0061175":"code","d2b47f05":"code","7b0d1bef":"code","4a2617a7":"code","40e21a82":"code","5850da73":"code","ef7fa1b8":"code","36e70ec7":"code","1af05af7":"code","385fd478":"code","6ce90b1c":"code","a6d413a9":"code","bf9c41d1":"code","5de0c068":"code","7cca42c8":"code","0b89a9f7":"code","1e9c129d":"code","4c13f043":"code","11b25407":"code","703ae09a":"code","6d7fa2a0":"code","33b66af5":"code","547f3d16":"code","8f091f3d":"code","1f699131":"code","7a543983":"code","487f9a83":"code","902de935":"code","eab9ce8e":"code","cc03ffbf":"code","ff754aed":"code","e68d54e9":"code","91f5984c":"code","5f60746b":"code","de41d1f0":"code","b36e5de5":"code","27edf011":"code","386eca98":"code","adfdd164":"code","0e7b0c55":"code","036ff5d5":"code","7abb098f":"code","43bc12ad":"code","3f5bc231":"code","8bd6c19d":"code","01c08e79":"code","03773760":"code","e193f06e":"code","df31bb15":"code","62a99517":"code","0f7500f9":"code","136fea55":"code","6a290126":"code","1dd05dc9":"code","66815969":"code","7171e176":"code","f74b36bc":"code","34842fc5":"code","da502062":"code","5721ecb2":"code","bec6b61d":"code","f00e5dfb":"code","3caed8b9":"code","6437b0af":"code","d3758e8a":"code","0bf9c045":"code","a5194aab":"code","5b5dd139":"code","6a55eee1":"code","62268bad":"code","7cbdbfd4":"code","5bf762a0":"code","94bbf27e":"markdown","fea54229":"markdown","f3e3064d":"markdown","5d39ef07":"markdown","2e329631":"markdown","a1a1e767":"markdown","af41eb1d":"markdown","7d6c1a35":"markdown","d9def1e0":"markdown","e0bafb15":"markdown","0e36342b":"markdown","1a9c38a8":"markdown","18a5a202":"markdown","59be0c02":"markdown","d228c7d0":"markdown","96b167bd":"markdown","d9c09928":"markdown","fde8b38e":"markdown","aadb666e":"markdown","035f3ef9":"markdown","5bc13073":"markdown","455cc0ad":"markdown","d06dd69c":"markdown","3f70a06c":"markdown","a7a297af":"markdown","4282c983":"markdown","c6ce3b8c":"markdown","29cefa2f":"markdown","c94e3e35":"markdown","cb804a22":"markdown","2fb99ab2":"markdown","ae9e0e75":"markdown","0a6161fd":"markdown","23a6767e":"markdown","b1f2fbee":"markdown","685a9f2c":"markdown","747a0650":"markdown","262b85a7":"markdown","a14ca335":"markdown","edbf987a":"markdown","9b6fcffb":"markdown","11d8410e":"markdown","400789a8":"markdown","0beb8211":"markdown","fd6b6bfb":"markdown","a062b568":"markdown","e4fc845d":"markdown","13c2e355":"markdown","d9163b66":"markdown","7772c12f":"markdown","ac3e1d3e":"markdown","46eaa2e2":"markdown","41b0c81a":"markdown","43d29ac2":"markdown","2664953d":"markdown","0e064f9d":"markdown","19cba5aa":"markdown","94fe1b46":"markdown","79edf3d6":"markdown","f609b3fe":"markdown","cb97bd70":"markdown","b44883f3":"markdown","9d52dfdb":"markdown","498c5e27":"markdown","1b8bad94":"markdown","5622ea45":"markdown","739aefa9":"markdown","bccbac25":"markdown","e42f6072":"markdown","41e94bd2":"markdown","8603319e":"markdown","5a8b1bbd":"markdown","3c7785b1":"markdown","e059a250":"markdown","61d2347a":"markdown","62731cc3":"markdown","6596356a":"markdown","df7aacd5":"markdown","53c80853":"markdown","f1ff71bf":"markdown","e17a3c83":"markdown","f493ed4d":"markdown","38e4dd9a":"markdown","fb28ff5e":"markdown","846a009c":"markdown","908efdbf":"markdown","7ed6d742":"markdown","82334737":"markdown","1d5bcb37":"markdown","9fcb08a6":"markdown","df42d0dc":"markdown","8d0428d6":"markdown","d98c855b":"markdown","7a51d95a":"markdown","ddd36559":"markdown","b0ada4cb":"markdown","0f7b0b2b":"markdown","fac69e77":"markdown","d071402e":"markdown","e5abf240":"markdown","95924114":"markdown","1a9e4e2b":"markdown","8b91b57c":"markdown","c1129752":"markdown","81a990fc":"markdown","af259cc3":"markdown","b7e5bf4e":"markdown","68bab97d":"markdown","7bb85a35":"markdown"},"source":{"d2ceee09":"# Importing necessary libraries\n\nimport time, os, psutil, operator, gc\nfrom IPython.core.display import display, HTML\nimport math\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nsns.set()\n\nimport json\nimport scipy\nfrom scipy import sparse\nfrom scipy.sparse import csr_matrix\n\nimport string\nimport re\nfrom string import punctuation\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer, WordPunctTokenizer, RegexpTokenizer\nfrom nltk.corpus import stopwords, wordnet\n!pip install num2words\nfrom num2words import num2words\n!pip install pyspellchecker\nfrom spellchecker import SpellChecker\nfrom nltk.stem.porter import PorterStemmer\nimport spacy\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nimport gensim","027c4c61":"# Recording the start time, which will be complemented with an end time check, to see the total runtime of the process\n\nstart = time.time()","9456f3c0":"# The dataset\n\nsample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ndata_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndata_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","cab9f01e":"print('The training set contains information on {} tweets.'.format(len(data_train)))\ndata_train[['id', 'text', 'target']]","c70777f7":"print('The test set contains information on {} tweets.'.format(len(data_test)))\ndata_test_target = data_test.copy()\ndata_test_target['target'] = '?'\ndata_test_target[['id', 'text', 'target']]","8601abb4":"logreg = LogisticRegression(penalty = 'l2', dual = False, tol = 0.0001, C = 1.0, fit_intercept = False, intercept_scaling = 1, class_weight = 'balanced', random_state = None, solver = 'saga', max_iter = 1000, multi_class = 'auto', verbose = 0, warm_start = False, n_jobs = -1, l1_ratio = None)\n\nknn = KNeighborsClassifier(n_neighbors = math.floor(math.sqrt(len(data_train))), weights = 'uniform', algorithm = 'auto', leaf_size = 30, p = 2, metric = 'minkowski', metric_params = None, n_jobs = -1)\n\ndt = DecisionTreeClassifier(criterion = 'entropy', splitter = 'best', max_depth = None, min_samples_split = 4, min_samples_leaf = 1, min_weight_fraction_leaf = 0.0, max_features = None, random_state = None, max_leaf_nodes = None, min_impurity_decrease = 0.0, class_weight = None, ccp_alpha = 0.0)\n\nsvm_linear = svm.SVC(C = 2.0, kernel = 'rbf', degree = 3, gamma = 'scale', coef0 = 0.0, shrinking = True, probability = False, tol = 0.0001, cache_size = 200, class_weight = 'balanced', verbose = False, max_iter = -1, decision_function_shape = 'ovr', break_ties = False, random_state = None)\n\nnb = GaussianNB()\n\nrf = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', max_depth = 10, min_samples_split = 10, min_samples_leaf = 5, min_weight_fraction_leaf = 0.0, max_features = 'auto', max_leaf_nodes = None, min_impurity_decrease = 0.0, bootstrap = True, oob_score = False, n_jobs = None, random_state = None, verbose = 0, warm_start = False, class_weight = 'balanced', ccp_alpha = 0.0, max_samples = None)\n\nlda = LinearDiscriminantAnalysis(solver = 'svd', shrinkage = None, priors = None, n_components = None, store_covariance = False, tol = 0.0001)\n\nsgd = SGDClassifier(loss = 'log', penalty = 'l2', alpha = 0.0001, l1_ratio = 0.15, fit_intercept = True, max_iter = 1000, tol = 0.001, shuffle = True, verbose = 0, epsilon = 0.1, n_jobs = None, random_state = None, learning_rate = 'optimal', eta0 = 0.0, power_t = 0.5, early_stopping = False, validation_fraction = 0.1, n_iter_no_change = 5, class_weight = None, warm_start = False, average = False) # loss = 'hinge'\n\nridge = RidgeClassifier(alpha = 2.0, fit_intercept = True, normalize = 'deprecated', copy_X = True, max_iter = None, tol = 0.001, class_weight = 'balanced', solver = 'auto', random_state = 1)\n\nxgb = XGBClassifier(use_label_encoder = False, eval_metric = 'logloss', max_depth = 3, learning_rate = 0.3, n_estimators = 100, base_score = 0.5, random_state = 1, objective = 'binary:logistic', booster = 'gbtree', n_jobs = -1, nthread = None, gamma = 0, min_child_weight = 1, max_delta_step = 0, subsample = 1, colsample_bytree = 1, colsample_bylevel = 1, reg_alpha = 0, reg_lambda = 1, scale_pos_weight = 1, seed = None) #, silent = True\n\nada = AdaBoostClassifier(base_estimator = None, n_estimators = 100, learning_rate = 1.0, algorithm = 'SAMME.R', random_state = None) # random_state = 0","8a34568f":"clf_list = [logreg, knn, dt, svm_linear, rf, sgd, ridge, xgb, ada] #, nb, lda\nclf_names = [\"Logistic Regression\", \"KNN Classifier\", \"Decision Tree\", \"Linear SVM\", \"Random Forest\", \"Stochastic Gradient Descent\", \"Ridge Classifier\", \"XGBoost Classifier\", \"AdaBoost Classifier\"] #, \"Naive Bayes\", \"Linear Discriminant Analysis\"","9eaeb624":"# Some Useful Functions\n\n# Function to create a list of unique elements of a given list (in order of first appearance)\n\ndef unique(lst):\n    list_unique = []\n    for x in lst:\n        if x not in list_unique:\n            list_unique.append(x)\n    return list_unique\n\n# Function to convert float nan values to string\n\ndef nan_type_conv(lst):\n    for i in range(len(lst)):\n        if str(lst[i]) == 'nan':\n            lst[i] = 'NaN'\n            \n# Word finder - Finding out if a specific word exists in a given list of words\n\ndef word_finder(word, lst):\n    count = 0\n    for x in lst:\n        if x == word:\n            count += 1\n            break\n    if count == 0:\n        return False\n    else:\n        return True\n\n# Word counter basic - Counting a specific word in a given list of words\n\ndef word_counter(word, lst):\n    lst_word = [x for x in lst if x == word]\n    return len(lst_word)\n\n# Word counter dictionary - Creating a dictionary of unique words in a given list with their frequencies\n\ndef word_counter_dict(word_list):\n    counter_dict = {}\n    for word in word_list:\n        if word not in counter_dict.keys():\n            counter_dict[word] = 1\n        else:\n            counter_dict[word] += 1\n    \n    counter_dict_sorted = dict(sorted(counter_dict.items(), key = operator.itemgetter(1), reverse = True))\n    return counter_dict_sorted\n\n# Word counter dictionary - Creating a dictionary of unique words in a given list with their relative frequencies\n\ndef word_counter_dict_relative(word_list):\n    counter_dict = {}\n    for word in word_list:\n        if word not in counter_dict.keys():\n            counter_dict[word] = 1\/len(word_list)\n        else:\n            counter_dict[word] += 1\/len(word_list)\n    \n    counter_dict_sorted = dict(sorted(counter_dict.items(), key = operator.itemgetter(1), reverse = True))\n    return counter_dict_sorted\n\n# Function to convert a given dictionary into a dataframe with given column names\n\ndef dict_to_df(dictionary, C1, C2):\n    df = pd.DataFrame(dictionary.items(), columns=[C1, C2])\n    return df\n\n# Word counter dataframe - Creating a dataframe of unique words in a given list with their frequencies\n\ndef word_counter_df(word_list):\n    return dict_to_df(word_counter_dict(word_list), \"Word\", \"Frequency\")\n\n# Word counter dataframe - Creating a dataframe of unique words in a given list with their relative frequencies\n\ndef word_counter_df_relative(word_list):\n    return dict_to_df(word_counter_dict_relative(word_list), \"Word\", \"Relative Frequency\")\n\n# Function to convert a given list of pairs into a dictionary\n\ndef list_to_dict(lst):\n    dct = {lst[i][0]: lst[i][1] for i in range(len(lst))}\n    return dct\n\n# RegexpTokenizer\n\nregexp = RegexpTokenizer(\"[\\w']+\")\n\n# List of punctuations except the apostrophe\n\npunctuation_list = [w for w in punctuation if w not in [\"'\"]]\n\n# Function to compute list of punctuations that are present in a given text\n\ndef text_punct(text):\n    x = word_tokenize(text)\n    punct_list = [w for w in x if w in punctuation_list]\n    return punct_list\n\n# Function to compute list of words that are present in a given list of texts\n\ndef text_list_words(text_list):\n    word_list = []\n    for text in text_list:\n        word_list = word_list + regexp.tokenize(text)\n    return word_list\n\n# Function to compute list of punctuations that are present in a given list of texts\n\ndef text_list_punct(text_list):\n    punct_list = []\n    for text in text_list:\n        punct_list = punct_list + text_punct(text)\n    return punct_list\n\n# Function to compute count per text of all unique words in a given list of texts\n\ndef word_count_per_text(text_list):\n    word_list = text_list_words(text_list) # list of words\n    word_count_dict = word_counter_dict(word_list) # dictionary of unique words and frequencies\n    for word in word_count_dict.keys():\n        word_count_dict[word] = word_count_dict[word]\/len(text_list) # converting frequency to count per sentence\n    return word_count_dict\n\n# Function to produce donutplot\n\ndef donutplot(value, label, color, title):\n    fig = plt.figure(figsize = (10, 8))\n    ax = fig.add_axes([0.0, 0.1, 1.0, 0.5], aspect = 1)\n    pie = ax.pie(value, colors = color, autopct = \"%1.1f%%\", startangle = 90) # labels = label\n    centre_circle = plt.Circle((0, 0), 0.8, fc = \"white\")\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n    fig.suptitle(title, y = 0.65, fontsize = 16)\n    plt.legend(pie[0], label, loc = \"upper left\")\n    plt.show()\n\n# Function to visualise classwise comparison - joint barplot\n\ndef classwise_comparison_barplot(df, n, feature, non_disaster, disaster, xlabel, ylabel, title):\n\n    labels = df.head(n).iloc[::-1][feature]\n    feature_non_disaster = df.head(n).iloc[::-1][non_disaster]\n    feature_disaster = df.head(n).iloc[::-1][disaster]\n\n    location = np.arange(len(labels)) # location points of the labels\n    width = 0.35 # width of the bars\n\n    fig, ax = plt.subplots()\n    fig = plt.gcf()\n    fig.set_size_inches(10, 10)\n    bar1 = ax.barh(location - (width \/ 2), feature_non_disaster, width, label = \"Non-disaster tweets\")\n    bar2 = ax.barh(location + (width \/ 2), feature_disaster, width, label = \"Disaster tweets\")\n\n    ax.set_xlabel(xlabel, fontsize = 14)\n    ax.set_ylabel(ylabel, fontsize = 14)\n    ax.set_title(title, fontsize = 16)\n    ax.set_yticks(location)\n    ax.set_yticklabels(labels)\n    ax.legend()\n    plt.setp(ax.xaxis.get_majorticklabels(), rotation = 0)\n\n    fig.tight_layout()\n    plt.show()\n\n# Function to visualise classwise comparison of feature distribution - histograms in two separate subplots\n\ndef classwise_comparison_subplot(feature_train_0, feature_train_1, binwidth, title_0, title_1, ylimit, xlabel, ylabel, suptitle):\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n\n    xmin = np.min([feature_train_0.min(), feature_train_1.min()])\n    xmax = np.max([feature_train_0.max(), feature_train_1.max()])\n\n    sns.histplot(feature_train_0, ax = ax1, color = \"green\", binwidth = binwidth)\n    ax1.set_title(title_0, fontsize = 14)\n    ax1.set_xlim([xmin - 0.5, xmax + 0.5])\n    ax1.set_ylim([0, ylimit])\n    ax1.set_xlabel(xlabel, fontsize = 14)\n    ax1.set_ylabel(ylabel, fontsize = 14)\n\n    sns.histplot(feature_train_1, ax = ax2, color = \"red\", binwidth = binwidth)\n    ax2.set_title(title_1, fontsize = 14)\n    ax2.set_xlim([xmin - 0.5, xmax + 0.5])\n    ax2.set_ylim([0, ylimit])\n    ax2.set_xlabel(xlabel, fontsize = 14)\n    ax2.set_ylabel(\"\")\n\n    fig.suptitle(suptitle, y = 1.0, fontsize = 16)\n    plt.show()\n\n# Visualization of embedding\n\ndef plot_embedding(test_data, test_labels): # savepath = \"filename.csv\"\n        truncated_SVD = TruncatedSVD(n_components = 2)\n        truncated_SVD.fit(test_data)\n        scores = truncated_SVD.transform(test_data)\n        color_mapper = {label:idx for idx, label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = [\"red\", \"blue\", \"blue\"]\n        \n        plt.scatter(scores[:, 0], scores[:, 1], s = 8, alpha = 0.8, c = test_labels,\n                        cmap = matplotlib.colors.ListedColormap(colors))\n        red_patch = mpatches.Patch(color = \"red\", label = \"Non-disaster tweet\")\n        green_patch = mpatches.Patch(color = \"blue\", label = \"Disaster tweet\")\n        plt.legend(handles=[red_patch, green_patch], prop={\"size\" : 12})\n\n# Confusion matrix\n\ndef confusion_matrix(y_test, y_pred):\n    class_names = [\"Non-disaster\", \"Disaster\"]\n    tick_marks_y = [0.5, 1.5]\n    tick_marks_x = [0.5, 1.5]\n    conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n    conf_matrix_df = pd.DataFrame(conf_matrix, range(2), range(2))\n    plt.figure(figsize = (6, 4.75))\n    sns.set(font_scale = 1.4) # label size\n    plt.title(\"Confusion Matrix\")\n    sns.heatmap(conf_matrix_df, annot = True, annot_kws = {\"size\" : 16}, fmt = 'd') # font size\n    plt.yticks(tick_marks_y, class_names, rotation = \"vertical\")\n    plt.xticks(tick_marks_x, class_names, rotation = \"horizontal\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.grid(False)\n    plt.show()\n\n# F1-score\n\ndef f1_score(y_test, y_pred):\n    conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n    TN = conf_matrix[0, 0]\n    FP = conf_matrix[0, 1]\n    FN = conf_matrix[1, 0]\n    TP = conf_matrix[1, 1]\n    F1 = TP\/(TP + (0.5*(FP + FN)))\n    return F1\n\n# Function to display dataframes side by side\n\ndef display_side_by_side(dfs:list, captions:list):\n    \"\"\"Display tables side by side to save vertical space\n    Input:\n        dfs: list of pandas.DataFrame\n        captions: list of table captions\n    \"\"\"\n    output = \"\"\n    combined = dict(zip(captions, dfs))\n    for caption, df in combined.items():\n        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n        output += \"\\xa0\\xa0\\xa0\"\n    display(HTML(output))\n    \ndef avg_f1_score_list(X, y):\n    cv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\n    cvs = [cross_val_score(clf, X, y, cv = cv, scoring = 'f1').mean() for clf in clf_list]\n    return cvs\n\n# def cv_f1_score_list(X, y):\n#     return [cross_val_score(clf, X, y, cv = 6, scoring = 'f1').std()\/cross_val_score(clf, X_fit_transform, y, cv = 6, scoring = 'f1_micro').mean() for clf in clf_list]\n    \nf1_score_max = []\ndef f1_score_df(X, y):\n    f1_df = pd.DataFrame()\n    f1_df[\"Classifier\"] = clf_names\n    f1_df[\"Average f1-score\"] = avg_f1_score_list(X, y)\n#     f1_df[\"Coefficient of variation\"] = cv_f1_score_list(X, y)\n    f1_score_max.append(max(f1_df[\"Average f1-score\"]))\n    return f1_df","c67547db":"# Splitting the training data by target\n\ndata_train_0 = data_train[data_train[\"target\"] == 0]\ndata_train_1 = data_train[data_train[\"target\"] == 1]\n\n# Class frequencies\n\nprint(\"Number of training tweets not indicating real disasters: {}\".format(len(data_train_0)))\nprint(\"Number of training tweets indicating real disasters: {}\".format(len(data_train_1)))","c939ca61":"# Visualization of class frequencies\n\ntarget_frequency = np.array([len(data_train_0), len(data_train_1)])\ntarget_label = [\"Not disaster tweets\", \"Disaster tweets\"]\ntarget_color = [\"green\", \"red\"]\ndonutplot(value = target_frequency, label = target_label, color = target_color, title = \"Frequency comparison of non-disaster tweets and disaster tweets\")","afa0a7e9":"# Keyword - main dataframe\n\nkeyword = list(data_train[\"keyword\"])\nnan_type_conv(keyword)\nkeyword_unique = unique(keyword)\nkeyword_unique_count = [word_counter(word, keyword) for word in keyword_unique]\n\nkeyword_0 = list(data_train_0[\"keyword\"])\nnan_type_conv(keyword_0)\nkeyword_0_unique_count = [word_counter(word, keyword_0) for word in keyword_unique]\n\nkeyword_1 = list(data_train_1[\"keyword\"])\nnan_type_conv(keyword_1)\nkeyword_1_unique_count = [word_counter(word, keyword_1) for word in keyword_unique]\n\nkeyword_df = pd.DataFrame()\nkeyword_df[\"keyword\"] = keyword_unique\nkeyword_df[\"count (all tweets)\"] = keyword_unique_count\nkeyword_df[\"proportion (all tweets)\"] = [count\/len(keyword) for count in keyword_unique_count]\nkeyword_df[\"count (non-disaster tweets)\"] = keyword_0_unique_count\nkeyword_df[\"proportion (non-disaster tweets)\"] = [count\/len(keyword_0) for count in keyword_0_unique_count]\nkeyword_df[\"count (disaster tweets)\"] = keyword_1_unique_count\nkeyword_df[\"proportion (disaster tweets)\"] = [count\/len(keyword_1) for count in keyword_1_unique_count]\nkeyword_df[\"absolute difference\"] = abs(keyword_df[\"proportion (disaster tweets)\"] - keyword_df[\"proportion (non-disaster tweets)\"])","de13ddc4":"# 'NaN' keywords\n\nnan_keyword_count = word_counter('NaN', keyword)\nkeyword_frequency = np.array([nan_keyword_count, len(keyword) - nan_keyword_count])\nkeyword_label = [\"Not NaN\", \"NaN\"]\nkeyword_color = [\"green\", \"red\"]\ndonutplot(value = keyword_frequency, label = keyword_label, color = keyword_color, title = \"Frequency comparison of training tweets withs non-NaN keywords and NaN keywords\")","9f423a1d":"# Classwise keyword-count\n\nkeyword_df_count = keyword_df[[\"keyword\", \"count (all tweets)\", \"count (non-disaster tweets)\", \"count (disaster tweets)\"]].sort_values(by = [\"count (all tweets)\"], ascending = False)\nkeyword_df_count.drop(0, axis = 0, inplace = True) # deleting the rows with keyword NaN\n\nclasswise_comparison_barplot(df = keyword_df_count,\n                             n = 20,\n                             feature = \"keyword\",\n                             non_disaster = \"count (non-disaster tweets)\",\n                             disaster = \"count (disaster tweets)\",\n                             xlabel = \"count of tweets\",\n                             ylabel = \"keyword\",\n                             title = \"Top 20 keyword-count (in decreasing order of total count)\"\n                            )","c0061175":"# Classwise keyword-proportion\n\nkeyword_df_proportion = keyword_df[[\"keyword\", \"proportion (non-disaster tweets)\", \"proportion (disaster tweets)\", \"absolute difference\"]].sort_values(by = [\"absolute difference\"], ascending = False)\nkeyword_df_proportion.drop(0, axis = 0, inplace = True) # deleting the rows with keyword NaN\n\nclasswise_comparison_barplot(df = keyword_df_proportion,\n                             n = 20,\n                             feature = \"keyword\",\n                             non_disaster = \"proportion (non-disaster tweets)\",\n                             disaster = \"proportion (disaster tweets)\",\n                             xlabel = \"keyword\",\n                             ylabel = \"proportion of tweets\",\n                             title = \"Top 20 keyword-proportion (in decreasing order of absolute difference)\"\n                            )","d2b47f05":"# 5 keywords with least absolute difference between proportion in non-disaster tweets and proportion in disaster tweets\n\nkeyword_df_proportion[\"keyword\"].tail(5).values.tolist()","7b0d1bef":"# Location - main dataframe\n\nlocation = list(data_train[\"location\"])\nnan_type_conv(location)\nlocation_unique = unique(location)\nlocation_unique_count = [word_counter(word, location) for word in location_unique]\n\nlocation_0 = list(data_train_0[\"location\"])\nnan_type_conv(location_0)\nlocation_0_unique_count = [word_counter(word, location_0) for word in location_unique]\n\nlocation_1 = list(data_train_1[\"location\"])\nnan_type_conv(location_1)\nlocation_1_unique_count = [word_counter(word, location_1) for word in location_unique]\n\nlocation_df = pd.DataFrame()\nlocation_df[\"location\"] = location_unique\nlocation_df[\"count (all tweets)\"] = location_unique_count\nlocation_df[\"proportion (all tweets)\"] = [count\/len(location) for count in location_unique_count]\nlocation_df[\"count (non-disaster tweets)\"] = location_0_unique_count\nlocation_df[\"proportion (non-disaster tweets)\"] = [count\/len(location_0) for count in location_0_unique_count]\nlocation_df[\"count (disaster tweets)\"] = location_1_unique_count\nlocation_df[\"proportion (disaster tweets)\"] = [count\/len(location_1) for count in location_1_unique_count]\nlocation_df[\"absolute difference\"] = abs(location_df[\"proportion (disaster tweets)\"] - location_df[\"proportion (non-disaster tweets)\"])","4a2617a7":"# 'NaN' locations\n\nnan_location_count = word_counter('NaN', location)\nlocation_frequency = np.array([nan_location_count, len(location) - nan_location_count])\nlocation_label = [\"Not NaN\", \"NaN\"]\nlocation_color = [\"green\", \"red\"]\ndonutplot(value = location_frequency, label = location_label, color = location_color, title = \"Frequency comparison of training tweets withs non-NaN locations and NaN locations\")","40e21a82":"# Classwise location-count\n\nlocation_df_count = location_df[[\"location\", \"count (all tweets)\", \"count (non-disaster tweets)\", \"count (disaster tweets)\"]].sort_values(by = [\"count (all tweets)\"], ascending = False)\nlocation_df_count.drop(0, axis = 0, inplace = True) # deleting the rows with location NaN\n\nclasswise_comparison_barplot(df = location_df_count,\n                             n = 20,\n                             feature = \"location\",\n                             non_disaster = \"count (non-disaster tweets)\",\n                             disaster = \"count (disaster tweets)\",\n                             xlabel = \"location\",\n                             ylabel = \"count of tweets\",\n                             title = \"Top 20 location-count (in decreasing order of total count)\"\n                            )","5850da73":"# Classwise location-proportion\n\nlocation_df_proportion = location_df[[\"location\", \"proportion (non-disaster tweets)\", \"proportion (disaster tweets)\", \"absolute difference\"]].sort_values(by = [\"absolute difference\"], ascending = False)\nlocation_df_proportion.drop(0, axis = 0, inplace = True) # deleting the rows with location NaN\n\nclasswise_comparison_barplot(df = location_df_proportion,\n                             n = 20,\n                             feature = \"location\",\n                             non_disaster = \"proportion (non-disaster tweets)\",\n                             disaster = \"proportion (disaster tweets)\",\n                             xlabel = \"location\",\n                             ylabel = \"proportion of tweets\",\n                             title = \"Top 20 location-proportion (in decreasing order of absolute difference)\"\n                            )","ef7fa1b8":"# Distribution of number of characters in tweets\n\ndata_train_0_char = data_train_0['text'].str.len()\ndata_train_1_char = data_train_1['text'].str.len()\n\nclasswise_comparison_subplot(feature_train_0 = data_train_0_char,\n                             feature_train_1 = data_train_1_char,\n                             binwidth = 5,\n                             title_0 = \"Tweets not indicating real disasters\",\n                             title_1 = \"Tweets indicating real disasters\",\n                             ylimit = 750,\n                             xlabel = \"Number of characters\",\n                             ylabel = \"Number of tweets\",\n                             suptitle = \"Distribution of number of characters in training tweets\"\n                            )","36e70ec7":"# Distribution of number of words in tweets\n\ndata_train_0_word = data_train_0[\"text\"].str.split().map(lambda x: len(x))\ndata_train_1_word = data_train_1[\"text\"].str.split().map(lambda x: len(x))\n\nclasswise_comparison_subplot(feature_train_0 = data_train_0_word,\n                             feature_train_1 = data_train_1_word,\n                             binwidth = 1,\n                             title_0 = \"Tweets not indicating real disasters\",\n                             title_1 = \"Tweets indicating real disasters\",\n                             ylimit = 300,\n                             xlabel = \"Number of words\",\n                             ylabel = \"Number of tweets\",\n                             suptitle = \"Distribution of number of words in training tweets\"\n                            )","1af05af7":"# Distribution of average word-length in tweets\n\ndata_train_0_avg = data_train_0[\"text\"].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\ndata_train_1_avg = data_train_1[\"text\"].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n\nclasswise_comparison_subplot(feature_train_0 = data_train_0_avg,\n                             feature_train_1 = data_train_1_avg,\n                             binwidth = 0.5,\n                             title_0 = \"Tweets not indicating real disasters\",\n                             title_1 = \"Tweets indicating real disasters\",\n                             ylimit = 700,\n                             xlabel = \"Number of words\",\n                             ylabel = \"Number of tweets\",\n                             suptitle = \"Distribution of length of words in training tweets\"\n                            )","385fd478":"# Distribution of number of urls in tweets\n\nurl_train_0_count = data_train_0[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\nurl_train_1_count = data_train_1[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\nclasswise_comparison_subplot(feature_train_0 = url_train_0_count,\n                             feature_train_1 = url_train_1_count,\n                             binwidth = 1,\n                             title_0 = \"Tweets not indicating real disasters\",\n                             title_1 = \"Tweets indicating real disasters\",\n                             ylimit = 3000,\n                             xlabel = \"Number of URLs\",\n                             ylabel = \"Number of tweets\",\n                             suptitle = \"Distribution of number of URLs in training tweets\"\n                            )","6ce90b1c":"# Distribution of number of hashtags in tweets\n\nhashtag_train_0_count = data_train_0[\"text\"].apply(lambda x: len([c for c in str(x) if c == '#']))\nhashtag_train_1_count = data_train_1[\"text\"].apply(lambda x: len([c for c in str(x) if c == '#']))\n\nclasswise_comparison_subplot(feature_train_0 = hashtag_train_0_count,\n                             feature_train_1 = hashtag_train_1_count,\n                             binwidth = 1,\n                             title_0 = \"Tweets not indicating real disasters\",\n                             title_1 = \"Tweets indicating real disasters\",\n                             ylimit = 3800,\n                             xlabel = \"Number of hashtags\",\n                             ylabel = \"Number of tweets\",\n                             suptitle = \"Distribution of number of hashtags in training tweets\"\n                            )","a6d413a9":"# Distribution of number of mentions in tweets\n\nmention_train_0_count = data_train_0[\"text\"].apply(lambda x: len([c for c in str(x) if c == '@']))\nmention_train_1_count = data_train_1[\"text\"].apply(lambda x: len([c for c in str(x) if c == '@']))\n\nclasswise_comparison_subplot(feature_train_0 = mention_train_0_count,\n                             feature_train_1 = mention_train_1_count,\n                             binwidth = 1,\n                             title_0 = \"Tweets not indicating real disasters\",\n                             title_1 = \"Tweets indicating real disasters\",\n                             ylimit = 3200,\n                             xlabel = \"Number of mentions\",\n                             ylabel = \"Number of tweets\",\n                             suptitle = \"Distribution of number of mentions in training tweets\"\n                            )","bf9c41d1":"punct_train = text_list_punct(data_train[\"text\"])\npunct_train_0 = text_list_punct(data_train_0[\"text\"])\npunct_train_1 = text_list_punct(data_train_1[\"text\"])\n\npunct_train_unique = unique(punct_train)","5de0c068":"# Classwise punctuation-count\n\npunct_count_non_disaster = [word_counter(punct, punct_train_0) for punct in punct_train_unique]\npunct_count_disaster = [word_counter(punct, punct_train_1) for punct in punct_train_unique]\n\npunct_count = pd.DataFrame()\npunct_count[\"punctuation\"] = punct_train_unique\npunct_count[\"count (non-disaster tweets)\"] = punct_count_non_disaster\npunct_count[\"count (disaster tweets)\"] = punct_count_disaster\npunct_count[\"count (all tweets)\"] = punct_count[\"count (non-disaster tweets)\"] + punct_count[\"count (disaster tweets)\"]\npunct_count.sort_values(by = [\"count (all tweets)\", \"count (non-disaster tweets)\", \"count (disaster tweets)\"], ascending = False, inplace = True)\n\nclasswise_comparison_barplot(df = punct_count,\n                             n = 20,\n                             feature = \"punctuation\",\n                             non_disaster = \"count (non-disaster tweets)\",\n                             disaster = \"count (disaster tweets)\",\n                             xlabel = \"punctuation\",\n                             ylabel = \"count\",\n                             title = \"Top 20 punctuation-count (in decreasing order of total count)\"\n                            )","7cca42c8":"# Classwise punctuation-count per tweet\n\npunct_count_per_non_disaster_tweet = [word_counter(punct, punct_train_0)\/len(data_train_0) for punct in punct_train_unique]\npunct_count_per_disaster_tweet = [word_counter(punct, punct_train_1)\/len(data_train_1) for punct in punct_train_unique]\n\npunct_count_per_tweet = pd.DataFrame()\npunct_count_per_tweet[\"punctuation\"] = punct_train_unique\npunct_count_per_tweet[\"count per non-disaster tweet\"] = punct_count_per_non_disaster_tweet\npunct_count_per_tweet[\"count per disaster tweet\"] = punct_count_per_disaster_tweet\npunct_count_per_tweet[\"absolute difference\"] = abs(punct_count_per_tweet[\"count per non-disaster tweet\"] - punct_count_per_tweet[\"count per disaster tweet\"])\npunct_count_per_tweet.sort_values(by = [\"absolute difference\", \"count per disaster tweet\", \"count per non-disaster tweet\"], ascending = False, inplace = True)\n\nclasswise_comparison_barplot(df = punct_count_per_tweet,\n                             n = 20,\n                             feature = \"punctuation\",\n                             non_disaster = \"count per non-disaster tweet\",\n                             disaster = \"count per disaster tweet\",\n                             xlabel = \"punctuation\",\n                             ylabel = \"count per tweet\",\n                             title = \"Top 20 punctuation-count per tweet (in decreasing order of absolute difference)\"\n                            )","0b89a9f7":"del keyword_df_count, keyword_df_proportion, keyword_df, location_df_count, location_df_proportion, location_df, punct_count, punct_count_per_tweet\ngc.collect()","1e9c129d":"# Converting to lowercase\n\ndef convert_to_lowercase(text):\n    return text.lower()\n\ntext = \"This is a FUNCTION that CoNvErTs a Text to lowercase\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(convert_to_lowercase(text)))","4c13f043":"# Removing whitespaces\n\ndef remove_whitespace(text):\n    return text.strip()\n\ntext = \" \\t This is a string \\t \"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_whitespace(text)))","11b25407":"# Removing punctuations\n\ndef remove_punctuation(text):\n    punct_str = string.punctuation\n    punct_str = punct_str.replace(\"'\", \"\") # discarding apostrophe from the string to keep the contractions intact\n    return text.translate(str.maketrans(\"\", \"\", punct_str))\n\ntext = \"Here's [an] example? {of} &a string. with.? punctuation!!!!\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_punctuation(text)))","703ae09a":"# Removing HTML tags\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\n\ntext = '<a href = \"https:\/\/www.kaggle.com\/c\/nlp-getting-started\/overview\"> Natural Language Processing with Disaster Tweets <\/a>'\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_html(text)))","6d7fa2a0":"# Removing emojis\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntext = \"Just happened a terrible car crash \ud83d\ude1f\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_emoji(text)))","33b66af5":"# Removing other unicode characters\n\ndef remove_http(text):\n    http = \"https?:\/\/\\S+|www\\.\\S+\" # matching strings beginning with http (but not just \"http\")\n    pattern = r\"({})\".format(http) # creating pattern\n    return re.sub(pattern, \"\", text)\n\ntext = \"It's a function that removes links starting with http: or https such as https:\/\/en.wikipedia.org\/wiki\/Unicode_symbols\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_http(text)))","547f3d16":"# Dictionary of acronyms\n\nwith open(\"..\/input\/english-acronyms\/english_acronyms_lowercase.json\") as json_file:\n    acronyms_dict = json.load(json_file)\n\nprint(\"Example: Original form of the acronym 'fyi' is '{}'\".format(acronyms_dict[\"fyi\"]))","8f091f3d":"# Dataframe of acronyms\n\ndict_to_df(acronyms_dict, \"acronym\", \"original\").head()","1f699131":"# List of acronyms\n\nacronyms_list = list(acronyms_dict.keys())","7a543983":"# Function to convert contractions in a text\n\ndef convert_acronyms(text):\n    words = []\n    for word in regexp.tokenize(text):\n        if word in acronyms_list:\n            words = words + acronyms_dict[word].split()\n        else:\n            words = words + word.split()\n    \n    text_converted = \" \".join(words)\n    return text_converted\n\ntext = \"btw you've to fill in the details including dob\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(convert_acronyms(text)))","487f9a83":"# Dictionary of contractions\n\nwith open(\"..\/input\/english-contractions\/english_contractions_lowercase.json\") as json_file:\n    contractions_dict = json.load(json_file)\n\nprint(\"Example: Original form of the contraction 'aren't' is '{}'\".format(contractions_dict[\"aren't\"]))","902de935":"# Dataframe of contractions\n\ndict_to_df(contractions_dict, \"contraction\", \"original\").head()","eab9ce8e":"# List of contractions\n\ncontractions_list = list(contractions_dict.keys())","cc03ffbf":"# Function to convert contractions in a text\n\ndef convert_contractions(text):\n    words = []\n    for word in regexp.tokenize(text):\n        if word in contractions_list:\n            words = words + contractions_dict[word].split()\n        else:\n            words = words + word.split()\n    \n    text_converted = \" \".join(words)\n    return text_converted\n\ntext = \"he's doin' fine\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(convert_contractions(text)))","ff754aed":"# Stopwords\n\nstops = stopwords.words(\"english\") # stopwords\naddstops = [\"among\", \"onto\", \"shall\", \"thrice\", \"thus\", \"twice\", \"unto\", \"us\", \"would\"] # additional stopwords\nallstops = stops + addstops\n\nprint(allstops)","e68d54e9":"# Function to remove stopwords from a list of texts\n\ndef remove_stopwords(text):\n    return \" \".join([word for word in regexp.tokenize(text) if word not in allstops])\n\ntext = \"This is a function that removes stopwords in a given text\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_stopwords(text)))","91f5984c":"# pyspellchecker\n\nspell = SpellChecker()\n\ndef pyspellchecker(text):\n    word_list = regexp.tokenize(text)\n    word_list_corrected = []\n    for word in word_list:\n        if word in spell.unknown(word_list):\n            word_list_corrected.append(spell.correction(word))\n        else:\n            word_list_corrected.append(word)\n    text_corrected = \" \".join(word_list_corrected)\n    return text_corrected\n\ntext = \"I'm goinng therre\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(pyspellchecker(text)))","5f60746b":"# Stemming\n\nstemmer = PorterStemmer()\ndef text_stemmer(text):\n    text_stem = \" \".join([stemmer.stem(word) for word in regexp.tokenize(text)])\n    return text_stem\n\ntext = \"Introducing lemmatization as an improvement over stemming\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(text_stemmer(text)))","de41d1f0":"# Lemmatization\n\nspacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n#lemmatizer = WordNetLemmatizer()\n\ndef text_lemmatizer(text):\n    text_spacy = \" \".join([token.lemma_ for token in spacy_lemmatizer(text)])\n    #text_wordnet = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]) # regexp.tokenize(text)\n    return text_spacy\n    #return text_wordnet\n\ntext = \"Introducing lemmatization as an improvement over stemming\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(text_lemmatizer(text)))","b36e5de5":"# Discardment of non-alphabetic words\n\ndef discard_non_alpha(text):\n    word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n    text_non_alpha = \" \".join(word_list_non_alpha)\n    return text_non_alpha\n\ntext = \"It is an ocean of thousands and 1000s of crowd\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(discard_non_alpha(text)))","27edf011":"def keep_pos(text):\n    tokens = regexp.tokenize(text)\n    tokens_tagged = nltk.pos_tag(tokens)\n    #keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW']\n    keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW', 'PRP', 'PRPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WPS', 'WRB']\n    keep_words = [x[0] for x in tokens_tagged if x[1] in keep_tags]\n    return \" \".join(keep_words)\n\ntext = \"He arrived at seven o'clock on Wednesday evening\"\nprint(\"Input: {}\".format(text))\ntokens = regexp.tokenize(text)\nprint(\"Tokens: {}\".format(tokens))\ntokens_tagged = nltk.pos_tag(tokens)\nprint(\"Tagged Tokens: {}\".format(tokens_tagged))\nprint(\"Output: {}\".format(keep_pos(text)))","386eca98":"# Additional stopwords\n\nalphabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\nprepositions = [\"about\", \"above\", \"across\", \"after\", \"against\", \"among\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beside\", \"between\", \"by\", \"down\", \"during\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"near\", \"of\", \"off\", \"on\", \"out\", \"over\", \"through\", \"to\", \"toward\", \"under\", \"up\", \"with\"]\nprepositions_less_common = [\"aboard\", \"along\", \"amid\", \"as\", \"beneath\", \"beyond\", \"but\", \"concerning\", \"considering\", \"despite\", \"except\", \"following\", \"like\", \"minus\", \"onto\", \"outside\", \"per\", \"plus\", \"regarding\", \"round\", \"since\", \"than\", \"till\", \"underneath\", \"unlike\", \"until\", \"upon\", \"versus\", \"via\", \"within\", \"without\"]\ncoordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\ncorrelative_conjunctions = [\"both\", \"and\", \"either\", \"or\", \"neither\", \"nor\", \"not\", \"only\", \"but\", \"whether\", \"or\"]\nsubordinating_conjunctions = [\"after\", \"although\", \"as\", \"as if\", \"as long as\", \"as much as\", \"as soon as\", \"as though\", \"because\", \"before\", \"by the time\", \"even if\", \"even though\", \"if\", \"in order that\", \"in case\", \"in the event that\", \"lest\", \"now that\", \"once\", \"only\", \"only if\", \"provided that\", \"since\", \"so\", \"supposing\", \"that\", \"than\", \"though\", \"till\", \"unless\", \"until\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"whether or not\", \"while\"]\nothers = [\"\u00e3\", \"\u00e5\", \"\u00ec\", \"\u00fb\", \"\u00fb\u00aam\", \"\u00fb\u00f3\", \"\u00fb\u00f2\", \"\u00ec\u00f1\", \"\u00fb\u00aare\", \"\u00fb\u00aave\", \"\u00fb\u00aa\", \"\u00fb\u00aas\", \"\u00fb\u00f3we\"]\nadditional_stops = alphabets + prepositions + prepositions_less_common + coordinating_conjunctions + correlative_conjunctions + subordinating_conjunctions + others\n\ndef remove_additional_stopwords(text):\n    return \" \".join([word for word in regexp.tokenize(text) if word not in additional_stops])","adfdd164":"def text_normalizer(text):\n    text = convert_to_lowercase(text)\n    text = remove_whitespace(text)\n    text = re.sub('\\n' , '', text) # converting text to one line\n    text = re.sub('\\[.*?\\]', '', text) # removing square brackets\n    text = remove_punctuation(text)\n    text = remove_html(text)\n    text = remove_emoji(text)\n    text = remove_http(text)\n    text = convert_acronyms(text)\n    text = convert_contractions(text)\n    text = remove_stopwords(text)\n    text = pyspellchecker(text)\n    text = text_lemmatizer(text) # text = text_stemmer(text)\n    text = discard_non_alpha(text)\n    text = keep_pos(text)\n    text = remove_additional_stopwords(text)\n    return text\n\ntext = \"We'll combine all into 1 SINGLE FUNCTION \ud83d\ude42 & apply on #training_tweets https:\/\/en.wikipedia.org\/wiki\/Text_normalization\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(text_normalizer(text)))","0e7b0c55":"data_train[\"tokens\"] = data_train[\"text\"].apply(lambda x: regexp.tokenize(x))\ndata_train[\"keyword plus\"] = data_train[\"keyword\"].fillna(\" \")\ndata_train[\"location plus\"] = data_train[\"location\"].fillna(\" \")\ndata_train[\"text plus\"] = data_train[\"keyword plus\"] + \" \" + data_train[\"location plus\"] + \" \" + data_train[\"text\"]\ndata_train[\"tokens plus\"] = data_train[\"text plus\"].apply(regexp.tokenize)\n\ndata_train[\"normalized text\"] = data_train[\"text\"].apply(text_normalizer) # implementing text normalization\ndata_train[\"normalized tokens\"] = data_train[\"normalized text\"].apply(lambda x: regexp.tokenize(x))\ndata_train[\"normalized text plus\"] = data_train[\"keyword plus\"] + \" \" + data_train[\"location plus\"] + \" \" + data_train[\"normalized text\"]\ndata_train[\"normalized tokens plus\"] = data_train[\"normalized text plus\"].apply(lambda x: regexp.tokenize(x))\n\ndata_train[[\"id\", \"keyword\", \"location\", \"text\", \"normalized text plus\", \"normalized tokens plus\", \"target\"]]","036ff5d5":"data_test[\"tokens\"] = data_test[\"text\"].apply(lambda x: regexp.tokenize(x))\ndata_test[\"keyword plus\"] = data_test[\"keyword\"].fillna(\" \")\ndata_test[\"location plus\"] = data_test[\"location\"].fillna(\" \")\ndata_test[\"text plus\"] = data_test[\"keyword plus\"] + \" \" + data_test[\"location plus\"] + \" \" + data_test[\"text\"]\ndata_test[\"tokens plus\"] = data_test[\"text plus\"].apply(lambda x: regexp.tokenize(x))\n\ndata_test[\"normalized text\"] = data_test[\"text\"].apply(text_normalizer) # implementing text normalization\ndata_test[\"normalized tokens\"] = data_test[\"normalized text\"].apply(lambda x: regexp.tokenize(x))\ndata_test[\"normalized text plus\"] = data_test[\"keyword plus\"] + \" \" + data_test[\"location plus\"] + \" \" + data_test[\"normalized text\"]\ndata_test[\"normalized tokens plus\"] = data_test[\"normalized text plus\"].apply(lambda x: regexp.tokenize(x))\n\ndata_test_target = data_test.copy()\ndata_test_target['target'] = '?'\ndata_test_target[[\"id\", \"keyword\", \"location\", \"text\", \"normalized text plus\", \"normalized tokens plus\", \"target\"]]","7abb098f":"X = data_train['normalized text'].tolist()\ny = data_train['target'].tolist()","43bc12ad":"del data_test_target\ngc.collect()","3f5bc231":"def count_words(text_list):\n    CountVec = CountVectorizer(ngram_range = (1, 1))\n    words = CountVec.fit_transform(text_list)\n    count_words_df = pd.DataFrame()\n    count_words_df['Words'] = CountVec.get_feature_names()\n    count_words_df['Frequency'] = words.toarray().sum(axis = 0)\n    count_words_df.sort_values(by = 'Frequency', ascending = False, inplace = True)\n    return count_words_df","8bd6c19d":"def count_bigrams(text_list):\n    CountVec = CountVectorizer(ngram_range = (2, 2))\n    bigrams = CountVec.fit_transform(text_list)\n    count_bigrams_df = pd.DataFrame()\n    count_bigrams_df['Bigrams'] = CountVec.get_feature_names()\n    count_bigrams_df['Frequency'] = bigrams.toarray().sum(axis = 0)\n    count_bigrams_df.sort_values(by = 'Frequency', ascending = False, inplace = True)\n    return count_bigrams_df","01c08e79":"def count_trigrams(text_list):\n    CountVec = CountVectorizer(ngram_range = (3, 3))\n    trigrams = CountVec.fit_transform(text_list)\n    count_trigrams_df = pd.DataFrame()\n    count_trigrams_df['Trigrams'] = CountVec.get_feature_names()\n    count_trigrams_df['Frequency'] = trigrams.toarray().sum(axis = 0)\n    count_trigrams_df.sort_values(by = 'Frequency', ascending = False, inplace = True)\n    return count_trigrams_df","03773760":"X_0 = data_train[data_train['target'] == 0]['normalized text'].tolist()\nX_1 = data_train[data_train['target'] == 1]['normalized text'].tolist()","e193f06e":"display_count_words = [count_words(X_0).head(10), count_words(X_1).head(10)]\ndisplay_title_words = [\"Words in non-disaster tweets\", \"Words in disaster tweets\"]\ndisplay_side_by_side(display_count_words, display_title_words)","df31bb15":"display_count_bigrams = [count_bigrams(X_0).head(10), count_bigrams(X_1).head(10)]\ndisplay_title_bigrams = [\"Bigrams in non-disaster tweets\", \"Bigrams in disaster tweets\"]\ndisplay_side_by_side(display_count_bigrams, display_title_bigrams)","62a99517":"display_count_trigrams = [count_trigrams(X_0).head(10), count_trigrams(X_1).head(10)]\ndisplay_title_trigrams = [\"Trigrams in non-disaster tweets\", \"Trigrams in disaster tweets\"]\ndisplay_side_by_side(display_count_trigrams, display_title_trigrams)","0f7500f9":"CountVec1 = CountVectorizer(ngram_range = (1, 1))\nX_fit_transform_1 = CountVec1.fit_transform(X)\nf1_score_df(X_fit_transform_1, y)","136fea55":"cutoff = 0.1 # The model considers top 10% features\n\nX_fit_transform_1_df = pd.DataFrame(X_fit_transform_1.toarray(), columns = CountVec1.get_feature_names())\nX_fit_transform_1_df_sorted = X_fit_transform_1_df.copy()\nX_fit_transform_1_df_sorted.loc[len(X_fit_transform_1_df_sorted.index)] = X_fit_transform_1.toarray().sum(axis = 0)\nX_fit_transform_1_df_sorted.sort_values(by = len(X_fit_transform_1_df_sorted.index)-1, axis = 1, ascending = False, inplace = True, kind = 'quicksort', na_position = 'last')\nX_fit_transform_1_df_sorted.drop(X_fit_transform_1_df_sorted.tail(1).index, inplace = True)\nselect = math.floor(cutoff*len(X_fit_transform_1_df_sorted.columns))\nX_fit_transform_1_df_selected = X_fit_transform_1_df_sorted.iloc[:, 0:select]\nX_fit_transform_1_selected = sparse.csr_matrix(X_fit_transform_1_df_selected.to_numpy())\n\nf1_score_df(X_fit_transform_1_selected, y)","6a290126":"CountVec2 = CountVectorizer(ngram_range = (2, 2))\nX_fit_transform_2 = CountVec2.fit_transform(X)\nf1_score_df(X_fit_transform_2, y)","1dd05dc9":"cutoff = 0.25\n\nX_fit_transform_2_df = pd.DataFrame(X_fit_transform_2.toarray(), columns = CountVec2.get_feature_names())\nX_fit_transform_2_df_sorted = X_fit_transform_2_df.copy()\nX_fit_transform_2_df_sorted.loc[len(X_fit_transform_2_df_sorted.index)] = X_fit_transform_2.toarray().sum(axis = 0)\nX_fit_transform_2_df_sorted.sort_values(by = len(X_fit_transform_2_df_sorted.index)-1, axis = 1, ascending = False, inplace = True, kind = 'quicksort', na_position = 'last')\nX_fit_transform_2_df_sorted.drop(X_fit_transform_2_df_sorted.tail(1).index, inplace = True)\nselect = math.floor(cutoff*len(X_fit_transform_2_df_sorted.columns))\nX_fit_transform_2_df_selected = X_fit_transform_2_df_sorted.iloc[:, 0:select]\nX_fit_transform_2_selected = sparse.csr_matrix(X_fit_transform_2_df_selected.to_numpy())\n\nf1_score_df(X_fit_transform_2_selected, y)","66815969":"X_fit_transform_merged_df = pd.concat([X_fit_transform_1_df, X_fit_transform_2_df], axis = 1)\nX_fit_transform_merged = sparse.csr_matrix(X_fit_transform_merged_df.to_numpy())\nf1_score_df(X_fit_transform_merged, y)","7171e176":"X_fit_transform_merged_df_selected = pd.concat([X_fit_transform_1_df_selected, X_fit_transform_2_df_selected], axis = 1)\nX_fit_transform_merged_selected = sparse.csr_matrix(X_fit_transform_merged_df_selected.to_numpy())\nf1_score_df(X_fit_transform_merged_selected, y)","f74b36bc":"del X_fit_transform_1_df, X_fit_transform_1_df_sorted, X_fit_transform_2_df, X_fit_transform_2_df_sorted, X_fit_transform_merged_df, X_fit_transform_1_df_selected, X_fit_transform_2_df_selected, X_fit_transform_merged_df_selected\ngc.collect()","34842fc5":"TfidfVec1 = TfidfVectorizer(ngram_range = (1, 1))\nX_fit_transform_tfidf_1 = TfidfVec1.fit_transform(X)\nf1_score_df(X_fit_transform_tfidf_1, y)","da502062":"cutoff = 0.1\n\nX_fit_transform_tfidf_1_df = pd.DataFrame(X_fit_transform_tfidf_1.toarray(), columns = TfidfVec1.get_feature_names())\nX_fit_transform_tfidf_1_df_sorted = X_fit_transform_tfidf_1_df.copy()\nX_fit_transform_tfidf_1_df_sorted.loc[len(X_fit_transform_tfidf_1_df_sorted.index)] = X_fit_transform_tfidf_1.toarray().sum(axis = 0)\nX_fit_transform_tfidf_1_df_sorted.sort_values(by = len(X_fit_transform_tfidf_1_df_sorted.index)-1, axis = 1, ascending = False, inplace = True, kind = 'quicksort', na_position = 'last')\nX_fit_transform_tfidf_1_df_sorted.drop(X_fit_transform_tfidf_1_df_sorted.tail(1).index, inplace = True)\nselect = math.floor(cutoff*len(X_fit_transform_tfidf_1_df_sorted.columns))\nX_fit_transform_tfidf_1_df_selected = X_fit_transform_tfidf_1_df_sorted.iloc[:, 0:select]\nX_fit_transform_tfidf_1_selected = sparse.csr_matrix(X_fit_transform_tfidf_1_df_selected.to_numpy())\n\nf1_score_df(X_fit_transform_tfidf_1_selected, y)","5721ecb2":"TfidfVec2 = TfidfVectorizer(ngram_range = (2, 2))\nX_fit_transform_tfidf_2 = TfidfVec2.fit_transform(X)\nf1_score_df(X_fit_transform_tfidf_2, y)","bec6b61d":"cutoff = 0.1\n\nX_fit_transform_tfidf_2_df = pd.DataFrame(X_fit_transform_tfidf_2.toarray(), columns = TfidfVec2.get_feature_names())\nX_fit_transform_tfidf_2_df_sorted = X_fit_transform_tfidf_2_df.copy()\nX_fit_transform_tfidf_2_df_sorted.loc[len(X_fit_transform_tfidf_2_df_sorted.index)] = X_fit_transform_tfidf_2.toarray().sum(axis = 0)\nX_fit_transform_tfidf_2_df_sorted.sort_values(by = len(X_fit_transform_tfidf_2_df_sorted.index)-1, axis = 1, ascending = False, inplace = True, kind = 'quicksort', na_position = 'last')\nX_fit_transform_tfidf_2_df_sorted.drop(X_fit_transform_tfidf_2_df_sorted.tail(1).index, inplace = True)\nselect = math.floor(cutoff*len(X_fit_transform_tfidf_2_df_sorted.columns))\nX_fit_transform_tfidf_2_df_selected = X_fit_transform_tfidf_2_df_sorted.iloc[:, 0:select]\nX_fit_transform_tfidf_2_selected = sparse.csr_matrix(X_fit_transform_tfidf_2_df_selected.to_numpy())\n\nf1_score_df(X_fit_transform_tfidf_2_selected, y)","f00e5dfb":"X_fit_transform_tfidf_merged_df = pd.concat([X_fit_transform_tfidf_1_df, X_fit_transform_tfidf_2_df], axis = 1)\nX_fit_transform_tfidf_merged = sparse.csr_matrix(X_fit_transform_tfidf_merged_df.to_numpy())\nf1_score_df(X_fit_transform_tfidf_merged, y)","3caed8b9":"X_fit_transform_tfidf_merged_df_selected = pd.concat([X_fit_transform_tfidf_1_df_selected, X_fit_transform_tfidf_2_df_selected], axis = 1)\nX_fit_transform_tfidf_merged_selected = sparse.csr_matrix(X_fit_transform_tfidf_merged_df_selected.to_numpy())\nf1_score_df(X_fit_transform_tfidf_merged_selected, y)","6437b0af":"del X_fit_transform_tfidf_1_df, X_fit_transform_tfidf_1_df_sorted, X_fit_transform_tfidf_2_df, X_fit_transform_tfidf_2_df_sorted, X_fit_transform_tfidf_merged_df, X_fit_transform_tfidf_1_df_selected, X_fit_transform_tfidf_2_df_selected, X_fit_transform_tfidf_merged_df_selected\ngc.collect()","d3758e8a":"word2vec_path = \"..\/input\/google-word2vec\/GoogleNews-vectors-negative300.bin\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary = True)","0bf9c045":"def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list) < 1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, tokens, generate_missing = False):\n    embeddings = tokens.apply(lambda x: get_average_word2vec(x, vectors, generate_missing = generate_missing))\n    return list(embeddings)","a5194aab":"data_train[\"corrected text\"] = data_train[\"text\"].apply(convert_to_lowercase).apply(convert_contractions)\ndata_train[\"corrected tokens\"] = data_train[\"corrected text\"].apply(regexp.tokenize)","5b5dd139":"X = get_word2vec_embeddings(word2vec, data_train['corrected tokens'])\ny = data_train['target'].tolist()\n\nfig = plt.figure(figsize=(8, 7))          \nplot_embedding(X, y)\nplt.show()","6a55eee1":"X_csr = scipy.sparse.csr_matrix(X)\nf1_score_df(X_csr, y)","62268bad":"print(\"Best score: {}\".format(max(f1_score_max)))","7cbdbfd4":"# Runtime and memory usage\n\nstop = time.time()\nprocess = psutil.Process(os.getpid())","5bf762a0":"print(\"Process runtime: %.2f seconds\" %float(stop - start))\nprint(\"Process memory usage: %.2f MB\" %float(process.memory_info()[0]\/(1024*1024)))","94bbf27e":"Now, we display the top words and bigrams for each class of tweets in the training datasets based on their frequency of occurance.","fea54229":"## Number of Words","f3e3064d":"# Basic Exploratory Data Analysis","5d39ef07":"## Removal of Additional Stopwords","2e329631":"## Mixture Model (Selected Features)","a1a1e767":"We observe that **logistic regression**, **linear SVM**, **stochastic gradient descent** and **ridge classifier** works well in this prediction scheme, compared to the other classifiers.","af41eb1d":"**What are contractions?** A contraction is a shortened form of a word or a phrase, obtained by dropping one or more letters.\n\nThese are commonly used in everyday speech, written dialogue, informal writing and in situations where space is limited or costly, such as advertisements. Usually the missing letters are indicated by an apostrophe, but there are exceptions. Examples: I'm = I am, let's = let us, won't = would not, howdy = how do you do.\n\nWe have compiled an extensive list of English contractions, which can be found in the attached .json file titled *english_contractions_lowercase*. The list is largely based on information obtained from the wikipedia page on *list of English contractions*. Note that the file only considers contractions in lowercase, i.e. it assumes that the textual data have already been transformed to lowercase before substituting the contractions. For example, the process will convert *i'll* to *i shall* but will leave *I'll* unchanged.","7d6c1a35":"Analyzing the data, we observe that several unnecessary words, which are not included in the ready-made set of **stopwords**, keep appearing in the text corpus. We discard these words to remove noise in the classification procedure.","d9def1e0":"## Bag of Bigrams Model (Selected Features)","e0bafb15":"## Removal of Unicode Characters","0e36342b":"## Bag of Words Model (Selected Features)","1a9c38a8":"We observe that **logistic regression**, **decision tree**, **linear SVM**, **stochastic gradient descent** and **ridge classifier** work moderately well for the bag of bigrams models but not as good as bag of words models.","18a5a202":"We examine the distribution of number of words per tweet for both the class of non-disaster tweets and the class of disaster tweets.","59be0c02":"# Introduction","d228c7d0":"The non-alphabetic words are not numerous and create unnecessary diversions in the context of classifying tweets into non-disaster and disaster categories. Hence we discard these words.","96b167bd":"We implement the text normalization on the test tweets as well.","d9c09928":"## Evaluation Metric","fde8b38e":"The function defined below, when fed with a text corpus, returns a dataframe consisting of all possible words along with their respective frequencies.","aadb666e":"Next we implement the text normalization on the training tweets.","035f3ef9":"## Project Objective","5bc13073":"We remove the unnecessary empty spaces from the tweets.","455cc0ad":"Next, we fit the same model, considering only the top $10\\%$ words as a feature and observe the mean f1-score resulting from cross-validations using different classifiers.","d06dd69c":"As in the model considering all words as features, **logistic regression**, **linear SVM**, **stochastic gradient descent** and **ridge classifier** works well in the model considering only the top layer of words, compared to the other classifiers.","3f70a06c":"## Removal of Stopwords","a7a297af":"## Substitution of Contractions","4282c983":"We examine the distribution of number of URLs per tweet for both the class of non-disaster tweets and the class of disaster tweets.","c6ce3b8c":"## Stemming and Lemmatization","29cefa2f":"## Bag of Bigrams Model (Selected Features)","c94e3e35":"## Parts of Speech","cb804a22":"Roughly speaking, **word embeddings** are vector representations of a particular word. It has the ability to capture the context of a particular word in a document, as well as identify semantic and syntactic similarity and other contextual relations with other words in the document.\n\n**Word2vec** is a specific word-embedding technique that uses a neural network model to learn word associations from a fairly large corpus of text. After the model is trained, it can detect similarity of words, as well as recommend words to complete a partial sentence. True to its name, word2vec maps each distinct word to a vector, which is assigned in such a way that the level of semantic similarity between words are indicated by a simple mathematical operation on the vectors that the words are mapped to (for instance, the cosine similarity between the vectors).","2fb99ab2":"The training tweets are typically sprinkled with emojis, URLs, punctuation and other symbols that do not contribute meaningfully to our analysis, but instead create noise in the learning procedure. Some of these symbols are unique, while the rest usually translate into unicode strings. We remove these irrelevant characters from the data using the regular expression module.","ae9e0e75":"## Length of Words","0a6161fd":"## Classifiers","23a6767e":"## Convertion to Lowercase","b1f2fbee":"We visualize the proportion of NaN values for the **location** feature, as well as the top keywords (both as per *total count* and *count per tweet*) for each class.","685a9f2c":"We now feed the corrected tokens to the word2vec embedder and plot the embedded observations, color-coded by their class (non-disaster or disaster).","747a0650":"# Contents\n\n- [Introduction](#Introduction)\n- [Basic Exploratory Data Analysis](#Basic-Exploratory-Data-Analysis)\n- [Text Normalization](#Text-Normalization)\n- [Bag of N-grams Model](#Bag-of-N-grams-Model)\n- [TF-IDF Model](#TF-IDF-Model)\n- [Word2Vec Model](#Word2Vec-Model)\n- [Acknowledgements](#Acknowledgements)\n- [References](#References)","262b85a7":"We shall not use trigrams in our models. Nonetheless we exhibit the top trigrams appearing in the non-disaster tweets as well as the disaster tweets.","a14ca335":"Several words, primarily pronouns, prepositions, modal verbs etc, are identified not to have much effect on the classification procedure. To get rid of the unwanted contamination effect, we remove these words.","edbf987a":"Finally we evaluate the model by considering $k$-fold cross-validation and compare the performances of different classifiers acting on it.","9b6fcffb":"We examine the distribution of number of hashtags per tweet for both the class of non-disaster tweets and the class of disaster tweets.","11d8410e":"The contractions do not always have a one-to-one mapping with the original words. For example **i'd** can come from both **i had** and **i would**. In the .json file only one the original words\/phrases are chosen. However, this does not affect our analysis since words like **had** and **would**, which do not have any meaningful contribution in achieving the objective of the project, will be discarded in the next subsection.","400789a8":"Since the size of the two classes are unequal, we cannot directly compare the count of a keyword in non-disaster tweets with the same in disaster tweets. To make valid comparison, we must scale these counts by respective class-sizes to obtain proportions of a keyword in non-disaster tweets and disaster tweets. In particular, the absolute difference of these two quantities can be considered as a measure of ability of a keyword to discriminate between non-disaster tweets and disaster tweets. For instance, if the absolute difference is close to 0, then we cannot infer anything on the status of the tweet based on keyword alone. On the other hand, a high value indicates that the keyword contributes towards classifying the tweet into a particular class.","0beb8211":"## Mentions (@)","fd6b6bfb":"We observe that the performances under mixture models are more or less similar to those under bag of words models.","a062b568":"Next we analyze the distribution of average word-length in tweets for both the class of non-disaster tweets and the class of disaster tweets.","e4fc845d":"We use the raw text except for converting to lowercase and converting the contractions to their respective expanded forms.","13c2e355":"The classification procedure cannot take mispellings into consideration and treats a word and its misspelt version as separate words. For this reason it is necessary to conduct spelling correction before feeding the data to the classification procedure.","d9163b66":"## URLs","7772c12f":"We convert all alphabetical characters of the tweets to lowercase so that the models do not differentiate identical words due to case-sensitivity. For example, without the normalization, *Sun* and *sun* would have been treated as two different words, which is not useful in the present context.","ac3e1d3e":"## Bag of Bigrams Model (All Features)","46eaa2e2":"We observe that **linear SVM** classifier acting on the embedded data obtained through **word2vec** algorithm produces the best result in terms of f1-score, given below.","41b0c81a":"**Note:** In the visualizations of classwise comparison of most features, including keyword and location, we produce only a few observations of the feature of interest due to the large number of distinct textual value taken by these features. The selection of these observations are done by considering certain attributes such as *total count* and choosing the top observations according to that attribute.","43d29ac2":"Too much false positives, where a model detects disaster in a tweet that does not indicate any such occurance, may be counterproductive and wasteful in terms of resources. Again, a false negative, where the model fails to detect a disaster from a tweet which actually indicates one, would delay disaster management and clearly costs too much. Observe that, in this problem, the class of tweets that indicate actual disasters (positive class) is more important than the class of tweets not indicating any disaster (negative class). Thus **the goal is to build a model that attempts to minimize the proportion of false positives in the predicted positive class (precision) and that of false negatives in the actual positive class (recall), assigning equal emphasis on both.** Let us denote\n\n**TP**: Number of true positives\n\n**TN**: Number of true negatives\n\n**FP**: Number of false positives\n\n**FN**: Number of false negatives\n\n**Precision** and **Recall** are universally accepted metrics to capture the performance of a model, when restricted respectively to the **predicted positive class** and the **actual positive class**. These are defined as\n\n$$\\text{Precision} = \\frac{TP}{TP + FP}.$$\n\n$$\\text{Recall} = \\frac{TP}{TP + FN}.$$\n\nThe **F1-score** provides a balanced measuring stick by considering the *harmonic mean* of the above two matrices.\n\n$$F_1\\text{-Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}.$$\n\nFor its equal emphasis on both *precision* and *recall*, *F1-score* is one of the most suitable metrics for evaluating the models in this project.","2664953d":"## Substitution of Acronyms","0e064f9d":"Now we consider mixture models by considering both words as well as bigrams.","19cba5aa":"## Mixture Model (All Features)","94fe1b46":"## Integration of the Processes","79edf3d6":"Similarly the next two functions return dataframes consisting of all possible bigrams and trigrams, respectively, along with their corresponding frequencies in the given text corpus.","f609b3fe":"## Bag of Words Model (Selected Features)","cb97bd70":"## Implementation on Training Tweets","b44883f3":"Mostly the punctuations do not play any role in predicting whether a particular tweet indicate disaster or not. Thus we prevent them from contaminating the classification procedures by removing them from the tweets. However, we keep **apostrophe** since most of the contractions contain this punctuation and will be automatically taken care of once we convert the contractions.","9d52dfdb":"We show the top punctuations (as per *total count* as well as *count per tweet*) for both the class of non-disaster tweets and the class of disaster tweets.","498c5e27":"We visualize the proportion of NaN values for the **keyword** feature, as well as the top keywords (both as per *total count* and *count per tweet*) for each class.","1b8bad94":"We examine the distribution of number of mentions per tweet for both the class of non-disaster tweets and the class of disaster tweets.","5622ea45":"## Removal of Whitespaces","739aefa9":"The **bag of words** model is a way of representing text data used in *natural language processing*. The model only considers multiplicity of the words and completely disregards the grammatical structure and ordering of the words. Here we fit the model, treating each word as a feature and observe the **average f1-score obtained from $5$ repetitions of $6$-fold cross-validation** using different classifiers.","bccbac25":"The **parts of speech** provide a great tool to select a subset of words that are more likely to contribute in the classification procedure and discard the rest to avoid noise. The idea is to select a number of parts of speech that are important to the context of the problem. Then we partition the words in a given text into several subsets corresponding to each part of speech and keep only those subsets corresponding to the selected parts of speech. ","e42f6072":"We consider a number of text normalization processes, namely [convertion to lowercase](#Convertion-to-Lowercase), [removal of unicode characters](#Removal-of-Unicode-Characters), [substitution of acronyms](#Substitution-of-Acronyms), [substitution of contractions](#Substitution-of-Contractions), [removal of stopwords](#Removal-of-Stopwords), [spelling correction](#Spelling-Correction), [stemming and lemmatization](#Stemming-and-Lemmatization), [integration of the processes](#Integration-of-the-Processes). At the end of the section, we combine all the processes into one single function and apply it on the training tweets.","41e94bd2":"**What are acronyms?** Acronyms are shortened forms of phrases, generally found in informal writings such as personal messages. For instance, *for your information* is written as *fyi* and *by the way* is written as *btw*. These time and effort-saving acronyms have received almost universal acceptance in social media platforms including twitter. For the sake of proper modeling, we convert the acronyms, appearing in the tweets, back to their respective original forms.","8603319e":"**Note:** A lot of keywords contain two words joined by *%20*, which is the URL-encoding of the *space* character.","5a8b1bbd":"**The objective of the project is to predict whether a particular tweet, of which the text (occasionally the keyword and the location as well) is provided, indicates a real disaster or not.**\n\nTwitter is one of the most active social media platform that many people use to share occurance of incidents including disasters. For example, if a fire breaks out in a building, many people around the particular location are likely to tweet about the incident. These tweets can send early alerts not only to people in the neighbourhood to evacuate, but also to the appropriate authority to take measures to minimize the loss, potentially saving lives. Thus the tweets indicating real disasters can be utilized for emergency disaster management to remarkable effect.","3c7785b1":"## Data","e059a250":"<h1><center> Natural Language Processing with Disaster Tweets <\/center><\/h1>\n<h2><center> Sugata Ghosh and Shyambhu Mukherjee <\/center><\/h2>","61d2347a":"## Class","62731cc3":"Next we list the classifiers, along with their specific hyperparameters, that are used in the notebook.","6596356a":"## Bag of Words Model (All Features)","df7aacd5":"## Location","53c80853":"We examine the distribution of number of characters per tweet for both the class of non-disaster tweets and the class of disaster tweets.","f1ff71bf":"## Bag of Bigrams Model (All Features)","e17a3c83":"## Hashtags (#)","f493ed4d":"Next we implement the **term frequency-inverse document frequency** (TFIDF) model.\n\nThe *term frequency* (TF) is the number of times a word appears in a text, divded by the total number of words appearing in the text. On the other hand, *inverse document frequency* (IDF) is the logarithm of the number of texts in the corpus, divided by the number of texts that contain the specific word. IDF determines the weight of rare words across all texts in the corpus. TF-IDF is the product of these two quantities. It objectively evaluates how relevant a word is to a text in a collection of texts, taking into consideration that some words appear more frequently in general.","38e4dd9a":"## Removal of Punctuations","fb28ff5e":"## Mixture Model (All Features)","846a009c":"## Spelling Correction","908efdbf":"# Word2Vec Model","7ed6d742":"## Bag of Words Model (All Features)","82334737":"Here we consider a mixture of features by considering top $10\\%$ words and top $25\\%$ bigrams.","1d5bcb37":"We observe that the results of the mixture models are more or less similar to that of the bag of words models. Also, the KNN classifier works poorly in all the prediction schemes described above.","9fcb08a6":"# References\n\nBelow is a list of references to resources that are mentioned in the notebook.\n\n- [List of English contractions (Wikipedia)](https:\/\/en.wikipedia.org\/wiki\/Wikipedia:List_of_English_contractions)\n- [Alphabetical list of part-of-speech tags used in the Penn Treebank Project](https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html)","df42d0dc":"Next we consider bag of bigrams (pair of consecutive words) model instead of bag of words model.","8d0428d6":"For an extensive list of part-of-speech tags, see [alphabetical list of part-of-speech tags used in the Penn Treebank Project](https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html).","d98c855b":"**Stemming** is the process of reducing the words to their root form or *stem*. It reduces related words to the same *stem* even if the stem is not a dictionary word. For example, the words *introducing*, *introduced*, *introduction* reduce to a common word *introduce*. However, the process often produces stems that are not actual words.","7a51d95a":"We observe that **logistic regression**, **linear SVM**, **stochastic gradient descent** and **ridge classifier** works well in this prediction scheme, compared to the other classifiers. In fact, logistic regression, the classifier returning the highest **f1-score**, has a slight improvement over the same model without TFIDF implementation.","ddd36559":"Next we consider the same model with the top quarter of bigrams.","b0ada4cb":"# Text Normalization","0f7b0b2b":"The stems *introduc*, *lemmat* and *improv* are not actual words. **Lemmatization** offers a more sophisticated approach by utilizing a corpus to match root forms of the words. Unlike stemming, it uses the context in which a word is being used.","fac69e77":"Source: https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data\n\nThe training dataset contains information on $7613$ tweets, each with a unique id, keyword (if available), location (if available), text and whether or not the tweet indicates a real disaster or not (expressed via a binary variable). The test dataset contains information on $3263$ tweets with the same features as above except the status of real disaster, which is to be predicted. The features of the dataset are described below.\n\n**id** : A unique identifier corresponding to the tweet\n\n**keyword** : A highlighting word from the tweet\n\n**location** : The location from where the tweet is sent\n\n**text**: The textual content of the tweet\n\n**target** : A binary variable, which is $0$ if the tweet does not indicate a real disaster and $1$ if it does\n\nNote that the features **keyword** and **location** may be blank for many tweets.","d071402e":"# Acknowledgements\n\nThe notebook contains ideas from the amazing works listed below. If this kernel helps you, please consider upvoting these works as well. Thank you.\n\n- [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove) by [Shahules](https:\/\/www.kaggle.com\/shahules)\n- [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert) by [Gunes Evitan](https:\/\/www.kaggle.com\/gunesevitan)\n- [Concrete solutions to real problems - An NLP workshop](https:\/\/github.com\/hundredblocks\/concrete_NLP_tutorial\/blob\/master\/NLP_notebook.ipynb) by [Emmanuel Ameisen](https:\/\/github.com\/hundredblocks)","e5abf240":"## Mixture Model (Selected Features)","95924114":"# TF-IDF Model","1a9e4e2b":"## Punctuations","8b91b57c":"**Observation:** The $5$ keywords with least absolute difference between their respective proportions in non-disaster tweets and disaster tweets are usually associated with occurances of disasters. Although these words are used in non-disastrous contexts, for example *landslide victory is an election* or *flood of joyful tears* etc, it is still surprising for these to qualify as keywords in the non-disaster tweets.","c1129752":"## Number of Characters","81a990fc":"## Discardment of Non-alphabetic Words","af259cc3":"## Keyword","b7e5bf4e":"Next, we construct the lists of normalized texts from both classes (non-disaster and disaster) of the training tweets.","68bab97d":"In order to compare the two classes (non-disaster tweets and disaster tweets), we split the training data based on the **target** feature.","7bb85a35":"# Bag of N-grams Model"}}