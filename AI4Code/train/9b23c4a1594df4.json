{"cell_type":{"c227bb31":"code","a995fe7a":"code","1ee6d26f":"code","66afc998":"code","611dbb96":"code","34f7015a":"code","dc9d3614":"code","1d14092e":"code","6544a0b8":"code","4a4f0d18":"code","4267d476":"code","d9fd9b5c":"code","cde9f9fa":"code","d8ec7fa9":"code","93cc53c3":"code","24a6c1b6":"code","b0b8cfe9":"code","f85b5ae4":"code","790abd9a":"code","f193a258":"code","576b48c0":"code","a95840c6":"code","594788b4":"code","7b115d61":"code","713f8bf8":"code","e484e84b":"code","45db88ff":"code","77836c5d":"markdown","17b868ab":"markdown","05c7b61c":"markdown","fa6257d9":"markdown","b7397d6b":"markdown","2ce97b67":"markdown","3c59594b":"markdown","8297c9db":"markdown","e7320381":"markdown","13837916":"markdown","a4dea124":"markdown","de3d57d6":"markdown","73bad6b7":"markdown","993d8764":"markdown","6ec5cedf":"markdown","edd32565":"markdown","f51227ca":"markdown","ba971c44":"markdown","aa65726b":"markdown","5b7d686d":"markdown","e430f988":"markdown"},"source":{"c227bb31":"import random\nrandom.seed(10)\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\n\n## setting the display style\nplt.style.use('fivethirtyeight') \n\n# settings to display all columns\npd.set_option(\"display.max_columns\", None)","a995fe7a":"from sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier","1ee6d26f":"train_df = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')","66afc998":"# function to reduce the memory usage\ndef reduce_mem_usage(train_data):\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n        \n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n\n    end_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return train_data","611dbb96":"## memory usage of train_df before memory usage reduction\nstart_mem = train_df.memory_usage().sum() \/ 1024**2\nprint('Memory usage of dataframe is {:.2f} MB'.format(start_mem))","34f7015a":"train_df = reduce_mem_usage(train_df)","dc9d3614":"train_df.info()","1d14092e":"train_df.describe()","6544a0b8":"train_df.head()","4a4f0d18":"print('Missing train data : {:d}'.format(train_df.isnull().values.sum()))","4267d476":"print('Missing test data : {:d}'.format(test_df.isnull().values.sum()))","d9fd9b5c":"train_df.head()","cde9f9fa":"## Listing the feature columns and ignoring the id column as it is just a unique identifier which shouldn't be used for model training\nfeatures = [col for col in train_df.columns if col not in ['target', 'id']]","d8ec7fa9":"%%time\ncat_features=[]\ncont_features=[]\nfor feature in features:\n    if train_df.dtypes[feature]=='int8':\n        cat_features.append(feature)\n    if train_df.dtypes[feature]=='float16':\n        cont_features.append(feature)\n    #print(test.dtypes[feature])\nprint('features obtained')\n\nplt.bar([1,2],[len(cat_features),len(cont_features)])\nplt.xticks([1,2],('Categorical','Continuous'))\nplt.show()","93cc53c3":"print('Categorical Features : {:d}'.format(len(cat_features)))\nprint('Continuous Features : {:d}'.format(len(cont_features)))","24a6c1b6":"train_df[cat_features].iloc[:,:44].hist(figsize=(32, 32),sharey=True)\nplt.show()","b0b8cfe9":"train_df[cont_features].iloc[:,:79].hist(figsize=(32, 32),sharey=True)\nplt.tight_layout()\nplt.show()","f85b5ae4":"train_df[cont_features].iloc[:,79:159].hist(figsize=(32, 32),sharey=True)\nplt.tight_layout()\nplt.show()","790abd9a":"train_df[cont_features].iloc[:,160:].hist(figsize=(32, 32),sharey=True)\nplt.tight_layout()\nplt.show()","f193a258":"target_df = train_df.target.value_counts() \/ len(train_df)\nlabels = ['1','0']\n\ntarget_df","576b48c0":"plt.bar([1,2],target_df)\nplt.xticks([1,2],('1','0'))\nplt.show()","a95840c6":"## Create validation set from training set\nX = train_df.copy()\ny = X.pop('target')\nX = X.drop('id',axis=1)","594788b4":"# stratify - making sure classes are evenly represented across train and validation set\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, stratify = y, test_size = 0.1)\ninput_shape = [X_train.shape[1]]","7b115d61":"eval_set = [(X_valid[features],y_valid)]\nlgbm_model = LGBMClassifier(objective=\"binary\")\nprint('LGBM parameters:\\n',lgbm_model.get_params())\n\nlgbm_model.fit(X[features], y,\n               eval_set = eval_set,\n               early_stopping_rounds=100,\n               eval_metric=\"binary_logloss\")","713f8bf8":"import seaborn as sns\nfeature_imp = pd.DataFrame(sorted(zip(lgbm_model.feature_importances_,X_train[features].columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(16, 44), tight_layout=True)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","e484e84b":"sub = lgbm_model.predict_proba(test_df[features])\nsub = pd.DataFrame(sub[:,1],columns=[\"target\"])\nsub[\"id\"]= test_df[\"id\"]\nsub = sub[['id', 'target']]\nsub.head()","45db88ff":"## saving the submission file\nsub.to_csv('.\/submission_01.csv', header=True, index=False)","77836c5d":"**f58** and **f69** are feature with highest importance and large number of feature has no importance in this model.","17b868ab":"# **<center style=\"font-family:poppins;color:#0456f3;\">5. Feature Distribution<\/center>**","05c7b61c":"From the above table we can see some features are continuous real numbers and  others are binary.","fa6257d9":"# **<center style=\"font-family:poppins;color:#0456f3;\">8. Define Light GBM Model<\/center>**\n\nHere, I train the Light GBM model using the ***X_train*** data with validation set and ***early_stopping_rounds*** of 100.","b7397d6b":"This data is very large and takes huge memory in kaggle. While training the model with this data, it will cross the maximum allocated RAM which is 13 GB for kaggle notebook. So, the data's size should be reduced to overcome this problem.","2ce97b67":"## **<center style=\"font-family:poppins;color:#0456f3;\">5.1 Distribution of Categorical Data<\/center>**","3c59594b":"\n\n1. This data is big  with  ***1 million rows*** and ***287*** columns.\n2. The target is ***highly balanced***.\n3. There are ***no missing values***  in train and test data.\n4. There are 45 categorical features and 240 continuous features. \n5. Didn't do ***feature engineering*** for my first run.\n6. Trained LightGBM model with no hyperparameter tunning for first run.\n7. ***f58*** has the highest feature importance.\n8. Large number of columns has no importance in this model.\n9. Got Score of ***0.84842***.\n\n## **<center style=\"font-family:poppins;color:#0456f3;\">10.1 Future Plans \ud83d\udcc5<\/center>**\n\n1. Will use the feature importances plot for selection of the features  for  future runs.\n2. Do feature engineering to improve the performance of model.\n5. Try XGB, CATBoost and Neural Networks and  see the performance of models.","8297c9db":"## **<center style=\"font-family:poppins;color:#0456f3;\">5.2 Distribution of Continuous Data<\/center>**","e7320381":"# **<center style=\"font-family:poppins;color:#0456f3;\">2. \ud83d\udcc9 Reduce Memory Usage<\/center>**","13837916":"The distribution of the target value **[0,1]** is equal.","a4dea124":"# **<center style=\"font-family:poppins;color:#0456f3;\">3. Basic Information \u2139\ufe0f about Data<\/center>**","de3d57d6":"# **<center style=\"font-family:poppins;color:#0456f3;\">1. Import Packages \ud83d\udce6<\/center>**","73bad6b7":"# **<center style=\"font-family:poppins;color:#0456f3;\">7. Training and Validation Data Set Split<\/center>**","993d8764":"## **<center style=\"font-family:poppins;color:#0456f3;\">4.1 Missing Values<\/center>**","6ec5cedf":"# **<center style=\"font-family:poppins;color:#0456f3;\">6. Distribution of the Target \ud83c\udfaf<\/center>**","edd32565":"# **<center style=\"font-family:poppins;color:#0456f3;\">9. Feature Importance<\/center>**\n\nLet's visualize the importance of features","f51227ca":"\n# **<center style=\"font-family:poppins;color:#0456f3;\">10. Conclusion \ud83d\udcd4<\/center>**","ba971c44":"\n## **<center style=\"font-family:poppins;color:#0456f3;\">10.2 References<\/center>**\n\n1. https:\/\/www.kaggle.com\/dwin183287\/tps-september-2021-eda\n2. https:\/\/www.kaggle.com\/subinium\/tps-oct-simple-eda\n3. https:\/\/www.kaggle.com\/questions-and-answers\/148011\n","aa65726b":"<center style=\"font-family:cursive; font-size:18px; color:#0456f3;\">Thank you \ud83d\ude4f for reading. If you have any feedback or find anything wrong, please let me know. I hope you enjoy it.Happy Learning \ud83d\ude4b\u200d\u2642\ufe0f<\/center>","5b7d686d":"Both the train_df and test_df has no missing values. So,no \ud83d\ude45\u200d\u2642\ufe0f need to worry about data imputation.  ","e430f988":"# **<center style=\"font-family:poppins;color:#0456f3;\">4. EDA \ud83d\udd0d<\/center>**"}}