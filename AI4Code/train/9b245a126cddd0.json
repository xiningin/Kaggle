{"cell_type":{"c6a5d8cd":"code","29a787cc":"code","91d22d96":"code","9884251e":"code","214ff406":"code","353ee362":"code","7aa642cf":"code","5a1ab130":"code","d56c6fe2":"code","d1f522f2":"code","6c186086":"code","24162711":"code","17346e65":"code","d9500b40":"code","a4be2e47":"code","38c717a1":"code","2254057b":"code","487b602a":"code","3861ba49":"markdown","cd137258":"markdown","fa88e678":"markdown","a680f9b0":"markdown","c896b04b":"markdown","fe16bf3a":"markdown","730e727e":"markdown","816e6fcd":"markdown","ecae91ed":"markdown","972809d4":"markdown","fdbddb7a":"markdown","660a7203":"markdown","df5e2759":"markdown","0b2455c1":"markdown","72995561":"markdown","c97db7d6":"markdown","f8844d62":"markdown","61f903eb":"markdown"},"source":{"c6a5d8cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29a787cc":"data=pd.read_csv('..\/input\/mnist-in-csv\/mnist_train.csv')","91d22d96":"data.head()","9884251e":"data.shape","214ff406":"label=data['label']","353ee362":"data=data.drop(['label'], axis=1) #axis 1 means doing operaion in columns","7aa642cf":"x=data.head(15000)\ny=label.head(15000)","5a1ab130":"from sklearn.preprocessing import StandardScaler\nstd_x=StandardScaler().fit_transform(x)\nstd_x.shape","d56c6fe2":"type(std_x)","d1f522f2":"from sklearn import decomposition\npca=decomposition.PCA()\npca.n_components=2\npca_x=pca.fit_transform(std_x)","6c186086":"pca_data=np.vstack((pca_x.T, y))\npca_df=pd.DataFrame(pca_data.T, columns=['first', 'second', 'label'])","24162711":"pca_df.head()","17346e65":"import matplotlib.pyplot as plt\nimport seaborn as sns","d9500b40":"ax=sns.FacetGrid(pca_df, hue='label', height=6).map(plt.scatter, 'first', 'second').add_legend()\nplt.show()","a4be2e47":"from sklearn.manifold import TSNE\ntsne=TSNE(n_components=2, random_state=0)\ntsne_x=tsne.fit_transform(std_x)\ntsne_data=np.vstack((tsne_x.T, y))\ntsne_df=pd.DataFrame(tsne_data.T, columns=['t_first', 't_second', 't_label'])","38c717a1":"tsne_df.head()","2254057b":"tsne_df.shape","487b602a":"ax=sns.FacetGrid(tsne_df, hue='t_label', height=6).map(plt.scatter, 't_first', 't_second').add_legend()\nplt.show()","3861ba49":"Now, joining 'label' data with our reduced dimensional dataset (each corresponding label shows actual digit value for related datapoint) and then again converting it to a dataframe.","cd137258":"**So why people use PCA?**\n\nBecause, TSNE is to place neighbors close to each other, (almost) completly ignoring the global structure.\nHence TSNE is excellent for visualization, because similar items can be plotted next to each other (and not on top of each other).\n\nPCA is quite the opposite. It tries to preserve the global properties (dimensions with high variance) while it may lose low-variance deviations between neighbors.\n\nAnd while working with this workbook, I personally experianced that TSNE is slower than PCA.","fa88e678":"Now, apply Standardization our dataset (to work our module efficiently)","a680f9b0":"**Looks like a beautiful mess, isn't it?**\n\nSee, a common logic we can think of that datapoints having same label value (actual digit value) will be having some internal relation between them.\n\nfor example, datapoints which we get by flattening the pixels of image having digit 0 will be similar in one way and datapoints which we get by flattening the pixels of image having digit 1 will be similar in another way.\n\nHence within a set of datapoints of same label value, there will be some similarity between those datapoints.\n\nSo, we can make a guess that in multi-dimension visualization, there will be clusters of different labels (0-9) made because of similarity within.\n\n**But PCA ruined that for us. It simply projected all the datapoints in 2 dimension without considering local similarity within points.**\n\nLets see what TSNE does now?","c896b04b":"Again, same proceedure. Training our TSNE and fitting it on our dataset.","fe16bf3a":"**How does PCA\/TSNE works?**\n\nIn simple terms, it projects our data into a new dimension having maximum variance or minimum distance of data points from that axis. Something like this:\n![](http:\/\/alexhwilliams.info\/itsneuronalblog\/img\/pca\/pca_two_views.png)\nimage source: http:\/\/alexhwilliams.info\/itsneuronalblog\/2016\/03\/27\/pca\/","730e727e":"Well, compare it with the one we get by using PCA.\n\nNoticed anything?\n\nOk I will try to explain. See that we are getting separate clusters of different colors (except some noise data points). Its showing the local relationship between datapoints having same label value which I was talking about right after I plotted the PCA visualization.\n","816e6fcd":"Now implementing PCA to our standardize dataset with components=2, which will convert\/reduce our dataset to 2 dimensions.","ecae91ed":"**But how PCA is different from TSNE?**\n\nFor that, I would like to take you through a simple dataset and visualize it after reducing its dimension and then show you (literally) how TSNE differs from PCA.","972809d4":"Taking only 15000 points so that it would take less time to apply pca\/tsne on small dataset.\n\n**Note:** number of features are unchanged. only datapoints (rows) are reduced.","fdbddb7a":"**IMPORTING DATASET:**","660a7203":"**CONCLUSION:**\n\nTSNE gives us the insight about our dataset by maintaining local structure\/relation of our datapoints. Which PCA is not able to do.","df5e2759":"Here, we see that our dataframe is been changed to numpy array.","0b2455c1":"**ABOUT THE DATASET**\n\nThis dataset is widely known as MNIST (Modified National Institute of Standards and Technology).\n\nIt has 60000 images of hand written digits from 0-9 in black and white format. Each image is of size (28\u00d728) pixel.\n\nWe used each image as a datapoint hence flattened it to get 784 features (each pixel as a feature).\nHence our dataset contains 60000 datapoints with 784 features, and one extra column ('label') is the actual digit in the image.\n\nHence combineD dataset is of shape (60000\u00d7785).\n","72995561":"**PCA** and **TSNE** both are dimensionality reduction techniques.\n\nBut the question is, why we reduce dimensionality (number of features) of our data as it's some precious information in one way or another? \n\nWell, there are mainly two reasons for doing this and to understand this, let our dataset is a (n\u00d7m) dataframe where n is number of datapoints and m is number of features. Then:\n\n**1.** When number of data points(n) are less than number of features(m), which leads to overfitting our training model. (OVERFIT MODEL: It gives almost 100% accurate results for train data but very poor result for our test data.)\n\n**2.** For visualization purpose: When we have to convert our m-dimensional dataset in 2 or 3 dimensions to plot it.","c97db7d6":"Removing 'label' column from our actual dataset because its kinda output to our main dataset.","f8844d62":"Finally, plotting our reduced dataset to visualize:","61f903eb":"**Disclaimer:** I started learning ML recently and it was hard to find notebooks based on basic problems which beginers face. Hence I decied to put some of my work which might help the beginers by providing detailed notebooks based on easy examples.\n\nSuggestions are most welcome as its my first notebook.\n\n**Thank you.**\n"}}