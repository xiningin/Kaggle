{"cell_type":{"528b6dde":"code","5dc5e6e2":"code","b540b122":"code","6dc5d996":"code","8055994a":"code","16ddc270":"code","4fa337d6":"code","f0a26056":"code","b8a59b19":"code","138efd33":"markdown","034b748c":"markdown","678ff087":"markdown","68c8b841":"markdown","eb1948a2":"markdown","68b9c688":"markdown","10db1c41":"markdown","ef2ceff1":"markdown","b7092bd4":"markdown","fdc153c7":"markdown","56d3da8a":"markdown","de861197":"markdown","d48ceda5":"markdown"},"source":{"528b6dde":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom IPython.display import display\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\nimport warnings\nwarnings.filterwarnings('ignore')","5dc5e6e2":"#import data into pandas dataframe\ndata = pd.read_csv(\"..\/input\/winequality-red.csv\")\n\n#display first 5 lines\ndata.head()\n\n#print data properties\nprint('Data Shape: {}'.format(data.shape))\n\ndisplay(data.describe())\ndisplay(data.info())\nsns.countplot(data['quality'],label=\"Count\")","b540b122":"plt.figure(figsize=(10,10))\ncorr_mat=sns.heatmap(data.corr(method='spearman'),annot=True,cbar=True,\n            cmap='viridis', vmax=1,vmin=-1,\n            xticklabels=data.columns,yticklabels=data.columns)\ncorr_mat.set_xticklabels(corr_mat.get_xticklabels(),rotation=90)","6dc5d996":"bins = (1, 6.5, 8.5)\nquality_level = ['low', 'high']\n\ndata['quality'] = pd.cut(data['quality'], bins = bins, labels = quality_level)\nsns.countplot(data['quality'])","8055994a":"#Extract data and label target\nX = data.iloc[:,0:11]\nle = LabelEncoder().fit(data['quality'])\ny = le.transform(data['quality'])\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,stratify=y)\n\n#Scale data\nscaler = MinMaxScaler()\nscaler.fit(X_train,y_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","16ddc270":"sns.pairplot(data, hue = \"quality\", diag_kind='kde')","4fa337d6":"kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nparam_grid = {'xgb__n_estimators': [500], \n              'xgb__max_depth': [2,3,4,5], \n              'xgb__alpha': [0.001,0.01,0.1,1],\n              'xgb__min_samples_leaf': [1,2,3]}\nxgb = XGBClassifier(random_state=0)\npipe = Pipeline([(\"scaler\",MinMaxScaler()), (\"xgb\",xgb)])\ngrid_xgb = GridSearchCV(pipe, param_grid=param_grid, cv=kfold, scoring='accuracy', n_jobs=-1)\ngrid_xgb.fit(X_train, y_train)\nprint(\"Best cross-validation accuracy: {:.3f}\".format(grid_xgb.best_score_))\nprint(\"Test set score: {:.3f}\".format(grid_xgb.score(X_test,y_test)))\nprint(\"Best parameters: {}\".format(grid_xgb.best_params_))\n\nconf_mat_xgb = confusion_matrix(y_test, grid_xgb.predict(X_test))\nsns.heatmap(conf_mat_xgb, annot=True, cbar=False, cmap=\"viridis_r\",\n            yticklabels=le.classes_, xticklabels=le.classes_)\n\n# Feature importance\nxgb = XGBClassifier(random_state=0, max_depth=grid_xgb.best_params_['xgb__max_depth'],\n                                 n_estimators=grid_xgb.best_params_['xgb__n_estimators'],\n                                 alpha=grid_xgb.best_params_['xgb__alpha'],\n                                 min_samples_leaf = grid_xgb.best_params_['xgb__min_samples_leaf'])\nxgb.fit(X_train_scaled,y_train)\nplt.figure()\nplt.bar(np.arange(X.shape[1]), xgb.feature_importances_)\nplt.xticks(np.arange(X.shape[1]), X.columns, rotation=90)\nplt.title('Feature Importance')\n\n# Classification Report\nprint(classification_report(y_test, xgb.predict(X_test_scaled), target_names=le.classes_))\n","f0a26056":"pipe = Pipeline([(\"scaler\",MinMaxScaler()), (\"svm\",SVC(random_state=0))])\nparam_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10],\n              'svm__gamma': [0.001, 0.01, 0.1, 1, 10],\n              'svm__kernel': ['linear', 'rbf']}\ngrid_svm = GridSearchCV(pipe, param_grid=param_grid, cv=kfold, scoring='accuracy',n_jobs=-1)\ngrid_svm.fit(X_train, y_train)\nprint(\"Best cross-validation accuracy: {:.3f}\".format(grid_svm.best_score_))\nprint(\"Test set score: {:.3f}\".format(grid_svm.score(X_test,y_test)))\nprint(\"Best parameters: {}\".format(grid_svm.best_params_))\n\n\n#SVM Cofusion matrix\nconf_mat_svm = confusion_matrix(y_test, grid_svm.predict(X_test_scaled))\nsns.heatmap(conf_mat_svm, annot=True, cbar=False, cmap=\"viridis_r\",\n            yticklabels=le.classes_, xticklabels=le.classes_)\n\n# Classification Report\nprint(classification_report(y_test, grid_svm.predict(X_test_scaled), target_names=le.classes_))\n","b8a59b19":"param = grid_svm.best_params_\nsvm = SVC(gamma = param[\"svm__gamma\"], C = param[\"svm__C\"], kernel=param[\"svm__kernel\"], probability=True, random_state=99)\n\nxgb = XGBClassifier(random_state=0, max_depth=grid_xgb.best_params_['xgb__max_depth'],\n                                 n_estimators=grid_xgb.best_params_['xgb__n_estimators'],\n                                 alpha=grid_xgb.best_params_['xgb__alpha'],\n                                 min_samples_leaf = grid_xgb.best_params_['xgb__min_samples_leaf'])\n\nensemble = VotingClassifier(estimators=[('clf1',svm), ('clf2',xgb)], voting='soft', weights=[1,1])\nensemble.fit(X_train_scaled, y_train)\nprint(\"Ensemble test score: {:.3f}\".format(ensemble.score(X_test_scaled, y_test)))\n\nconf_mat_ens = confusion_matrix(y_test, ensemble.predict(X_test_scaled))\nsns.heatmap(conf_mat_ens, annot=True, cbar=False, cmap=\"viridis_r\",\n            yticklabels=le.classes_, xticklabels=le.classes_)\n\ncv_score=np.mean(cross_val_score(ensemble,X_test,y_test,cv=kfold))\nprint(\"Cross Validation score: {:.3f}\".format(cv_score))","138efd33":"We see that the data consitsts of 11 features with no missing or NaN entries. The features vary in magnitude and scale so some feature scaling would be required.\n\nNote how most wines in the dataset are of mediocre quality (5-6), with a low population of the low quality wines (3-4).\nFor this analysis we will rearrange wine quality into low quality wines (<7) and high quality wines (>= 7).\n\n## Correlations","034b748c":"Feature extraction, scaling and splitting into train and test set. Note the use of stratify in the splitting. Since this is a highly unbalanced dataset, it is important to stratitfy the split i.e keep the proportion of classes the same in the splits. ","678ff087":"## Ensemble Learning with SVC\n\nIn this section we put togethor an ensemble model made up of the previous XGB and an SVC model.","68c8b841":"Note the following features are well correlated:\n-  citric acid and fixed acidity\n-  citric acid and volatile acidity\n-  density and fixed acidity\n-  pH and fixed acidity\n-  pH and citric acidity\n\nNotice that quality is most correlated with alcohol.\n\n\n## Preparing the Data\n\nRelabeling wine quality:","eb1948a2":"we can see some the correlations noted previously in the above pairplot. ","68b9c688":"Taking a look at the data","10db1c41":"As hoped, the combined model performs better on the test set than the XGB alone with an accuracy of 92%. However, this is probably just a lucky shuffle of the train and test sets. The cross validation score gives a more realistic result (88.2%).\n\n__Please upvote if you found this notebook helpful.__","ef2ceff1":"Notice how the SVC has classifed all the wines as low quality. Although this is useless on its own, combinig this model with the XGB might help increase the number of correctly classified low quality wines and increase total accuracy.\n\n## XGBoost & SVC","b7092bd4":"## Pairplot","fdc153c7":"## SVC","56d3da8a":"Although the accuracy of our model is high (91.7%), this does not reflect the face that the dataset is overwhelmed by low quality wine. Looking at the confusion matrix gives us better insight into the prediction of our model. We see that most of the low quality wine was classified correctly, however only about (63%) of high quality wine was correctly classified. This is just a sideeffect of poorly balanced datasets.\n\nLooking at the feature importance graph we see that the dominant feature when classifying wine quality is the alcohol content. This is expected from our observation of the correlation matrix.","de861197":"# Wine Quality with XGBoost\n\nThis notebook demonstrates the effectivnes of the XGBoost model on the red wine wuality dataset with minimal parameter tuning.\n\nComments and criticism welcome.\n\n__Please upvote if you found this notebook helpful.__","d48ceda5":"## XGBoost\n\nWith some parameter selection we get excellent results,"}}