{"cell_type":{"91ec4ccf":"code","dd209b48":"code","01a01c44":"code","cfb776d0":"code","02531a44":"code","1e9d4fa4":"code","fb007c3e":"code","d13c9e05":"code","c51a01f5":"code","4192a794":"code","09235849":"code","1b0313ed":"code","ad97e217":"code","7f930cce":"code","87a03cf8":"code","bf0a0c0c":"code","b2a01b6a":"code","1f423611":"code","93162396":"code","4e0b8d96":"code","43bd5937":"code","5fef950f":"code","1fee65d6":"code","1ac233f8":"code","cdb60ae0":"markdown","452c2808":"markdown","3dbd9c27":"markdown","d7c98742":"markdown"},"source":{"91ec4ccf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory # ref https:\/\/www.kaggle.com\/vikrishnan\/house-sales-price-using-regression\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dd209b48":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndata_df=pd.read_csv('\/kaggle\/input\/boston-house-prices\/housing.csv', header=None, delim_whitespace=True, names=column_names) #, delimiter=r\"\\s+\")","01a01c44":"data_df.head()","cfb776d0":"print(data_df.shape)","02531a44":"data_df.info()","1e9d4fa4":"data_df.isnull().sum()","fb007c3e":"data_df.duplicated().any()","d13c9e05":"data_df.describe()","c51a01f5":"data_df.corr(method='pearson')","4192a794":"data_df.hist(bins=12,figsize=(12,10),grid=False);","09235849":"y=data_df['MEDV']\nX=data_df.drop('MEDV',axis = 1)","1b0313ed":"X.head()","ad97e217":"y.head()","7f930cce":"from scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.distplot(y, hist=True);\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)","87a03cf8":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\nresults = []\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\npipeLR=Pipeline([('scaler', StandardScaler()), ('LR', LinearRegression())])\n\n# the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\ncv_results = cross_val_score(pipeLR, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n\npipeLR.fit(X_train, y_train)\n\n# this is the scaled LR\nprint(\"Score for scaledLR: \", pipeLR.score(X_test, y_test))\n# the mean result (10 data) of negative mean squared error\nprint(\"Score for scaledLR using cross_val_score: \", cv_results.mean())\n\n","bf0a0c0c":"import sklearn\nsklearn.metrics.SCORERS.keys()","b2a01b6a":"pipeLASSO=Pipeline([('scaler', StandardScaler()), ('LASSO', Lasso())])\n\n# the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\ncv_results = cross_val_score(pipeLASSO, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n\npipeLASSO.fit(X_train, y_train)\n\n# this is the scaled LR\nprint(\"Score for scaledLASSO: \", pipeLASSO.score(X_test, y_test))\n# the mean result (10 data) of negative mean squared error\nprint(\"Score for scaledLASSO using cross_val_score: \", cv_results.mean())","1f423611":"pipeEN=Pipeline([('scaler', StandardScaler()), ('ElasticNet', ElasticNet())])\n\n# the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\ncv_results = cross_val_score(pipeEN, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n\npipeEN.fit(X_train, y_train)\n\n# this is the scaled LR\nprint(\"Score for pipeEN: \", pipeEN.score(X_test, y_test))\n# the mean result (10 data) of negative mean squared error\nprint(\"Score for pipeEN using cross_val_score: \", cv_results.mean())","93162396":"pipeKNNReg=Pipeline([('scaler', StandardScaler()), ('KNeighborsRegressor', KNeighborsRegressor())])\n\n# the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\ncv_results = cross_val_score(pipeKNNReg, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n\npipeKNNReg.fit(X_train, y_train)\n\n# this is the scaled LR\nprint(\"Score for pipeKNNReg: \", pipeKNNReg.score(X_test, y_test))\n# the mean result (10 data) of negative mean squared error\nprint(\"Score for pipeKNNReg using cross_val_score: \", cv_results.mean())","4e0b8d96":"pipeGBM=Pipeline([('scaler', StandardScaler()), ('GradientBoostingRegressor', GradientBoostingRegressor())])\n\n# the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\ncv_results = cross_val_score(pipeGBM, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n\npipeGBM.fit(X_train, y_train)\n\n# this is the scaled LR\nprint(\"Score for pipeGBM: \", pipeGBM.score(X_test, y_test))\n# the mean result (10 data) of negative mean squared error\nprint(\"Score for pipeGBM using cross_val_score: \", cv_results.mean())","43bd5937":"pipeGBM=Pipeline([('scaler', StandardScaler()), ('GradientBoostingRegressor', GradientBoostingRegressor(random_state=0, n_estimators=400))])\n\n# the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\ncv_results = cross_val_score(pipeGBM, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n\npipeGBM.fit(X_train, y_train)\n\n#scaler = StandardScaler().fit(X_train)\n\n#X_test_scaled=scaler.transform(X_test)\n\npredictions = pipeGBM.predict(X_test)","5fef950f":"print(mean_squared_error(y_test, predictions))","1fee65d6":"pred_df=pd.DataFrame({\"Original Price of House\": y_test, \"Prediction Price of House\": predictions})","1ac233f8":"pred_df.head(10)","cdb60ae0":"The ideal of cross_val_score using neg_mean_squared_error scoring is zero. It means that the total error of all data on the regression line is as stated above.","452c2808":"# Summarize Data from Dataset","3dbd9c27":"CRIM per capita: crime rate by town\n\nZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n\nINDUS: proportion of non-retail business acres per town\n\nCHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\nNOX: nitric oxides concentration (parts per 10 million)\n\nRM: average number of rooms per dwelling\n\nAGE: proportion of owner-occupied units built prior to 1940\n\nDIS: weighted distances to five Boston employment centres\n\nRAD: index of accessibility to radial highways\n\nTAX: full-value property-tax rate per 10,000usd\n\nPTRATIO: pupil-teacher ratio by town\n\nB: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n\nLSTAT: % lower status of the population\n\nMEDV--> our resident value target","d7c98742":"GBM is good for this model. Now we are ready to use the model for data test"}}