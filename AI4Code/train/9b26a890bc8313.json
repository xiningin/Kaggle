{"cell_type":{"7d578e96":"code","8c1b3c0a":"code","b8b5cdbf":"code","97f621b5":"code","3a98f489":"code","65a2129f":"code","895535ff":"code","8b2f81c5":"code","68ff9f9b":"code","da275640":"code","e3596442":"code","01ce57ab":"code","7534ef0f":"code","65cb04c1":"code","6aa04cbb":"code","b1411c55":"code","0d361c7f":"code","3a6b5773":"code","8b0f04d6":"code","2feb7566":"code","38b4d9e4":"code","94d5c8ee":"code","205ece33":"code","95e82600":"code","fcb7a97c":"markdown","57c4a074":"markdown","a3dab710":"markdown","bb4850d9":"markdown","b2f66438":"markdown","c8e3f815":"markdown","c2ff3065":"markdown","06a263a4":"markdown","96cac8bb":"markdown","36887e1e":"markdown","c0700d19":"markdown","869333d4":"markdown"},"source":{"7d578e96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c1b3c0a":"# Library for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b8b5cdbf":"data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","97f621b5":"data.info()","3a98f489":"data.shape","65a2129f":"data.head()","895535ff":"data.describe()","8b2f81c5":"data.isna().sum()","68ff9f9b":"# LotFrontage: Linear feet of street connected to property\n# MasVnrArea: Masonry veneer area in square feet\n# GarageYrBlt: Year garage was built\n# Since, all the above fields have some \"NA\" value. So we need to first clean our data. Here we'll be taking average so that all NA will be compensated.\ndata.fillna(data.mean(), inplace = True)","da275640":"data.isna().sum()","e3596442":"# plotting correlation heatmap\nplt.figure(figsize=(12,10))\ndataplot = sns.heatmap(data.corr())\n  \n# displaying heatmap\nplt.show()","01ce57ab":"important_num_cols = list(data.corr()[(data.corr()>0.50) | (data.corr()<-0.50)].index)\n\ncategorical_cols = [\"MSZoning\", \"Utilities\",\"BldgType\",\"Heating\",\"KitchenQual\", \"SaleCondition\",\"LandSlope\"]\n\nimportant_cols = important_num_cols + categorical_cols\n\ndata = data[important_cols]","7534ef0f":"data","65cb04c1":"X = data.drop(\"SalePrice\" , axis =1)\ny = data[\"SalePrice\"]","6aa04cbb":"X","b1411c55":"X = pd.get_dummies(data)\nX","0d361c7f":"important_num_cols.remove(\"SalePrice\")\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX[important_num_cols] = scaler.fit_transform(X[important_num_cols])\n# Need to standardize only important_num_cols","3a6b5773":"X.head()","8b0f04d6":"from sklearn.model_selection import train_test_split, cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)","2feb7566":"print(\"X train shape - \", X_train.shape)\nprint(\"X test shape - \", X_test.shape)\nprint(\"y train shape - \", y_train.shape)\nprint(\"x test shape - \", y_test.shape)","38b4d9e4":"from sklearn.svm import SVR\nregressor = SVR(C=100000) #By-default kernel is \"rbf\"\nregressor.fit(X_train, y_train)","94d5c8ee":"predictions = regressor.predict(X_test)","205ece33":"def eval(y_hat, predictions):\n    mae = mean_absolute_error(y_hat, predictions)\n    mse = mean_squared_error(y_hat, predictions)\n    rmse = np.sqrt(mean_squared_error(y_hat, predictions))\n    r_squared = r2_score(y_hat, predictions)\n    return mae, mse, rmse, r_squared","95e82600":"from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\nmae, mse, rmse, r_squared = eval(y_test, predictions)\nprint(\"-\"*50)\nprint(\"MAE:\", mae)\nprint(\"MSE:\", mse)\nprint(\"RMSE:\", rmse)\nprint(\"R2 Score:\", r_squared)\nprint(\"-\"*50)\n\nnew_row = {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared}","fcb7a97c":"# **Finding the Correlation b\/w each features**","57c4a074":"**You can also use other Regression models like RandomForestClassifier, PolynomialRegression and compare them on basis of the accuracy based on same dataset.**","a3dab710":"# **Feature Selection**","bb4850d9":"# **Splitting the data into X and y**","b2f66438":"# **Check for missing values**","c8e3f815":"# **Model Preparation**","c2ff3065":"# **Train\/Test Split**","06a263a4":"# **Standardization**","96cac8bb":"# **Evaluation**","36887e1e":"# **One Hot Encoding**","c0700d19":"# **Exploring the data**","869333d4":"We are going to use \"corr()\" method by which we can determine the correlation between each of the variables."}}