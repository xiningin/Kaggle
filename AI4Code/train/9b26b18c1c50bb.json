{"cell_type":{"a26d46a5":"code","ae7bdfc2":"code","c0802e1f":"code","6bc70c7f":"code","d2224cc4":"code","fe7a3e8d":"code","d23b73f7":"code","69daff55":"code","33f1147c":"code","d79973aa":"code","2647c846":"code","477c6158":"code","0991c2ef":"code","36d83679":"code","b6f1e795":"code","ddac6ca3":"code","a00ed4dc":"code","ece91a7c":"code","de528802":"code","05552ced":"code","f5238c38":"code","6bc04dcb":"code","1ee139e3":"code","e0066668":"code","4d80a715":"code","e8b9e0a4":"code","3aeab7ce":"code","cfc7c165":"code","986d49cd":"code","d4f82cbc":"code","e60c7d76":"code","72e153d3":"code","df706177":"code","efc6d7a1":"code","3f1fd98e":"code","d823b161":"code","e1897b71":"code","54bf4a9a":"code","085bc1b3":"code","c94b8791":"code","c058063c":"code","54fb6697":"markdown","41a5ef48":"markdown","0007b128":"markdown","50199578":"markdown","f8985d5b":"markdown","1e9a6c34":"markdown","b6b8d962":"markdown","d70be4cc":"markdown","5129e557":"markdown","9a3256e0":"markdown","f6c986ec":"markdown","ab9d2f68":"markdown","b24dcc5d":"markdown","404dad5d":"markdown","02fc7d06":"markdown","de47ddb3":"markdown","61cfaf40":"markdown","bf0d5175":"markdown","ea9e7332":"markdown","564b7044":"markdown","137c4df2":"markdown","3c7225e4":"markdown","bac6c4fc":"markdown","90afc14c":"markdown"},"source":{"a26d46a5":"#Machine Learning Packages\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\n\n#Data Processing Packages\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\n\n#Data Visualization Packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#Metrics\nfrom sklearn import metrics\n\n#Data Analysis Packages\nimport pandas as pd\nimport numpy as np\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","ae7bdfc2":"#Loading the data sets\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntarget = train[\"Survived\"]\ndata = pd.concat([train.drop(\"Survived\", axis=1),test], axis=0).reset_index(drop=True)","c0802e1f":"data.head()","6bc70c7f":"data.info()","d2224cc4":"data.isnull().sum()","fe7a3e8d":"#Plotting the relations between Age and other features (Mean).\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12,12))\n\n#Age vs Pclass\nsns.barplot(x=\"Pclass\", y=\"Age\", data=data, ax=ax[0,0])\n#Age vs Sex\nsns.barplot(x=\"Sex\", y=\"Age\", data=data, ax=ax[0,1])\n#Age vs SibSp\nsns.barplot(x=\"SibSp\", y=\"Age\", data=data, ax=ax[0,2])\n#Age vs Parch\nsns.barplot(x=\"Parch\", y=\"Age\", data=data, ax=ax[1,0])\n#Age vs Family_size\nsns.barplot(x=(data[\"Parch\"] + data[\"SibSp\"]), y=\"Age\", data=data, ax=ax[1,1])\nax[1,1].set(xlabel='Family Size')\n#Age vs Embarked\nsns.barplot(x=\"Embarked\", y=\"Age\", data=data, ax=ax[1,2])","d23b73f7":"#Plotting relations between Age and other features (Median).\n\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12,12))\n#Age vs Pclass\nsns.boxplot(x=\"Pclass\", y=\"Age\", data=data, ax=ax[0,0])\n#Age vs Sex\nsns.boxplot(x=\"Sex\", y=\"Age\", data=data, ax=ax[0,1])\n#Age vs SibSp\nsns.boxplot(x=\"SibSp\", y=\"Age\", data=data, ax=ax[0,2])\n#Age vs Parch\nsns.boxplot(x=\"Parch\", y=\"Age\", data=data, ax=ax[1,0])\n#Age vs Family_size\nsns.boxplot(x=(data[\"Parch\"] + data[\"SibSp\"]), y=\"Age\", data=data, ax=ax[1,1])\nax[1,1].set(xlabel='Family Size')\n#Age vs Embarked\nsns.boxplot(x=\"Embarked\", y=\"Age\", data=data, ax=ax[1,2])","69daff55":"fig, ax = plt.subplots(figsize=(10,7))\n\n#Relation between Pclass and Embarked\nsns.countplot(x=\"Pclass\", data=data, hue=\"Embarked\")","33f1147c":"# We will use Pclass, Family Size and Embarked features to fill in the missing age values\n\n# First Lets create the feature Family_Size\ndata[\"Family_Size\"] = data[\"SibSp\"] + data[\"Parch\"]\n\n#Filling in the missing Age values\nmissing_age_value = data[data[\"Age\"].isnull()]\nfor index, row in missing_age_value.iterrows():\n    median = data[\"Age\"][(data[\"Pclass\"] == row[\"Pclass\"]) & (data[\"Embarked\"] == row[\"Embarked\"]) & (data[\"Family_Size\"] == row[\"Family_Size\"])].median()\n    if not np.isnan(median):\n        data[\"Age\"][index] = median\n    else:\n        data[\"Age\"][index] = np.median(data[\"Age\"].dropna())","d79973aa":"#Relation Between Fare and Pclass\nfig, ax = plt.subplots(figsize=(7,5))\nsns.boxplot(x=\"Pclass\", y=\"Fare\", data=data)\n\n#Since we have only 1 missing value for Fare we can just fill it according to Pclass feature\nprint(\"Pclass of the data point with missing Fare value:\", int(data[data[\"Fare\"].isnull()][\"Pclass\"]))\nmedian = data[data[\"Pclass\"] == 3][\"Fare\"].median()\ndata[\"Fare\"].fillna(median, inplace=True)","2647c846":"for index, rows in data.iterrows():\n    if pd.isnull(rows[\"Cabin\"]):\n        data[\"Cabin\"][index] = 'X'\n    else:\n        data[\"Cabin\"][index] = str(rows[\"Cabin\"])[0]","477c6158":"#Since we only have 2 missing Embarked values we will just fill the missing values with mode.\ndata[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)","0991c2ef":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n\n#Survived vs Pclass\nsns.barplot(x=\"Pclass\", y=target, data=data[:891], ax=ax[0])\n\n#Survived vs Sex\nsns.barplot(x=\"Sex\", y=target, data=data[:891], ax=ax[1])","36d83679":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n\n#Survived vs Family_Size\nsns.barplot(x=\"Family_Size\", y=target, data=data[:891], ax=ax[0])\n\n#Sex vs Single Passengers\nsns.countplot(x=\"Sex\", data=data[data[\"Family_Size\"] == 0], ax=ax[1])\n\n#Dividing Family_Size into 3 groups\ndata[\"Family_Size\"] = data[\"Family_Size\"].map({0:0, 1:1, 2:1, 3:1, 4:2, 5:2, 6:2, 7:2, 8:2, 9:2, 10:2})","b6f1e795":"data.head()","ddac6ca3":"plt.figure(figsize=(12,7))\n\n#Survived vs Cabin\nplt.subplot(121)\nsns.barplot(x=\"Cabin\", y=target, data=data[:891])\n\n#Survived vs Embarked\nplt.subplot(122)\nsns.barplot(x=\"Embarked\", y=target, data=data[:891])","a00ed4dc":"plt.figure(figsize=(12,7))\n\n#Plotting Kde for Fare\nplt.subplot(121)\nsns.kdeplot(data[\"Fare\"])\n\n#Plotting Kde for Fare with Survived as hue\nplt.subplot(122)\nsns.kdeplot(np.log(data[:891][target == 1][\"Fare\"]), color='blue', shade=True)\nsns.kdeplot(np.log(data[:891][target == 0][\"Fare\"]), color='red', shade=True)\nplt.legend([\"Survived\", \"Not Survived\"])\n\n#Since skewness can result in false conclusions we reduce skew for fare by taking log.\ndata[\"Fare\"] = np.log(data[\"Fare\"])\n\n#Dividing Fare into different categories\ndata[\"Fare\"] = pd.qcut(data[\"Fare\"], 5)","ece91a7c":"label = LabelEncoder()\ndata[\"Age\"] = label.fit_transform(pd.cut(data[\"Age\"].astype(int), 5))\nsns.barplot(x=\"Age\", y=target, data=data[0:891])","de528802":"data[\"Name\"] = data[\"Name\"].apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip())\ndata[\"Name\"] = data[\"Name\"].map({'Mr':1, 'Miss':2, 'Mrs':3, 'Ms':3, 'Mlle':3, 'Mme':3, 'Master':4, 'Dr':5, 'Rev':5, 'Col':5, \"Major\":5, \"Dona\":5, \"Sir\":5, \"Lady\":5, \"Jonkheer\":5, \"Don\":5, \"the Countess\":5, \"Capt\":5})\nsns.barplot(x=\"Name\", y=target, data=data[0:891])","05552ced":"data[\"Ticket\"] = data[\"Ticket\"].apply(lambda x: x.replace(\".\",\"\").replace('\/',\"\").strip().split(' ')[0] if not x.isdigit() else 'X')\ndata[\"Ticket\"] = label.fit_transform(data[\"Ticket\"])\nsns.barplot(x=\"Ticket\", y=target, data=data[0:891])","f5238c38":"#OneHot encoding with pd.get_dummies\ndata.drop([\"SibSp\", \"Parch\"], inplace=True, axis=1)\ndata = pd.get_dummies(data=data, columns=[\"Pclass\", \"Name\", \"Sex\", \"Age\", \"Cabin\", \"Embarked\", \"Family_Size\", \"Ticket\", \"Fare\"], drop_first=True)","6bc04dcb":"#Splitting into train and test again\ntrain = data[:891]\ntest = data[891:]","1ee139e3":"# Modeling step Test differents algorithms \ncv_split = model_selection.ShuffleSplit(n_splits=10, test_size=.3, train_size=.7, random_state=42)\n\nclassifiers = [\n    SVC(random_state=42),\n    DecisionTreeClassifier(random_state=42),\n    AdaBoostClassifier(DecisionTreeClassifier(random_state=42),random_state=42,learning_rate=0.1),\n    RandomForestClassifier(random_state=42),\n    ExtraTreesClassifier(random_state=42),\n    GradientBoostingClassifier(random_state=42),\n    MLPClassifier(random_state=42),\n    KNeighborsClassifier(),\n    LogisticRegression(random_state=42),\n    LinearDiscriminantAnalysis()\n]\n\ncv_train_mean = []\ncv_test_mean = []\ncv_score_time = []\ncv_fit_time = []\ncv_name = []\npredictions = []\n\nfor classifier in classifiers :\n    cv_results = model_selection.cross_validate(classifier, train.drop(['PassengerId'], axis=1), target, cv=cv_split, return_train_score=True)\n    cv_train_mean.append(cv_results['train_score'].mean())\n    cv_test_mean.append(cv_results['test_score'].mean())\n    cv_score_time.append(cv_results['score_time'].mean())\n    cv_fit_time.append(cv_results['fit_time'].mean())\n    cv_name.append(str(classifier.__class__.__name__))\n    classifier.fit(train.drop(['PassengerId'], axis=1), target)\n    predictions.append(classifier.predict(test.drop(['PassengerId'], axis=1)))\n    \n\nperformance_df = pd.DataFrame({\"Algorithm\":cv_name, \"Train Score\":cv_train_mean, \"Test Score\":cv_test_mean, 'Score Time':cv_score_time, 'Fit Time':cv_fit_time})\nperformance_df","e0066668":"#Plotting the performance on test set\nsns.barplot('Test Score', 'Algorithm', data=performance_df)","4d80a715":"#Plotting prediction correlation of the algorithms\nsns.heatmap(pd.DataFrame(predictions, index=cv_name).T.corr(), annot=True)","e8b9e0a4":"tuned_clf = {\n    'DecisionTreeClassifier':DecisionTreeClassifier(random_state=42),\n    'AdaBoostClassifier':AdaBoostClassifier(DecisionTreeClassifier(random_state=42),random_state=42,learning_rate=0.1),\n    'RandomForestClassifier':RandomForestClassifier(random_state=42),\n    'ExtraTreesClassifier':ExtraTreesClassifier(random_state=42),\n    \n    'GradientBoostingClassifier':GradientBoostingClassifier(random_state=42),\n    'MLPClassifier':MLPClassifier(random_state=42),\n    'LogisticRegression':LogisticRegression(random_state=42),\n    'LinearDiscriminantAnalysis':LinearDiscriminantAnalysis()\n}","3aeab7ce":"#DecisionTreeClassifier\ngrid = {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'max_depth': [2,4,6,8,10,None], \n        'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': [None, 'auto']}\n\ntune_model = model_selection.GridSearchCV(tuned_clf['DecisionTreeClassifier'], param_grid=grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)\ntune_model.fit(train.drop(['PassengerId'], axis=1), target)\nprint(\"Best Parameters:\")\nprint(tune_model.best_params_)\ntuned_clf['DecisionTreeClassifier'].set_params(**tune_model.best_params_)","cfc7c165":"#AdaBoostClassifier\ngrid = {'n_estimators': [10, 50, 100, 300], 'learning_rate': [.01, .03, .05, .1, .25], 'algorithm': ['SAMME', 'SAMME.R'] }\n\ntune_model = model_selection.GridSearchCV(tuned_clf['AdaBoostClassifier'], param_grid=grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)\ntune_model.fit(train.drop(['PassengerId'], axis=1), target)\nprint(\"Best Parameters:\")\nprint(tune_model.best_params_)\ntuned_clf['AdaBoostClassifier'].set_params(**tune_model.best_params_)","986d49cd":"#RandomForestClassifier\ngrid = {'n_estimators': [10, 50, 100, 300], 'criterion': ['gini', 'entropy'], 'max_depth': [2, 4, 6, 8, 10, None], \n        'oob_score': [True] }\n \ntune_model = model_selection.GridSearchCV(tuned_clf['RandomForestClassifier'], param_grid=grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)\ntune_model.fit(train.drop(['PassengerId'], axis=1), target)\nprint(\"Best Parameters:\")\nprint(tune_model.best_params_)\ntuned_clf['RandomForestClassifier'].set_params(**tune_model.best_params_)","d4f82cbc":"#ExtraTreesClassifier\ngrid = {'n_estimators': [10, 50, 100, 300], 'criterion': ['gini', 'entropy'], 'max_depth': [2, 4, 6, 8, 10, None]}\n \ntune_model = model_selection.GridSearchCV(tuned_clf['ExtraTreesClassifier'], param_grid=grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)\ntune_model.fit(train.drop(['PassengerId'], axis=1), target)\nprint(\"Best Parameters:\")\nprint(tune_model.best_params_)\ntuned_clf['ExtraTreesClassifier'].set_params(**tune_model.best_params_)","e60c7d76":"#GradientBoostingClassifier\ngrid = {#'loss': ['deviance', 'exponential'], 'learning_rate': [.01, .03, .05, .1, .25], \n        'n_estimators': [300],\n        #'criterion': ['friedman_mse', 'mse', 'mae'], \n        'max_depth': [4] }\n\ntune_model = model_selection.GridSearchCV(tuned_clf['GradientBoostingClassifier'], param_grid=grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)\ntune_model.fit(train.drop(['PassengerId'], axis=1), target)\nprint(\"Best Parameters:\")\nprint(tune_model.best_params_)\ntuned_clf['GradientBoostingClassifier'].set_params(**tune_model.best_params_)","72e153d3":"#MLPClassifier\ngrid = {'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"], 'alpha': 10.0 ** -np.arange(1, 7), 'activation': [\"logistic\", \"relu\", \"tanh\"]}\n\ntune_model = model_selection.GridSearchCV(tuned_clf['MLPClassifier'], param_grid=grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)\ntune_model.fit(train.drop(['PassengerId'], axis=1), target)\nprint(\"Best Parameters:\")\nprint(tune_model.best_params_)\ntuned_clf['MLPClassifier'].set_params(**tune_model.best_params_)","df706177":"#LogisticRegression\ngrid = {'fit_intercept': [True, False], 'penalty': ['l2'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n \ntune_model = model_selection.GridSearchCV(tuned_clf['LogisticRegression'], param_grid=grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)\ntune_model.fit(train.drop(['PassengerId'], axis=1), target)\nprint(\"Best Parameters:\")\nprint(tune_model.best_params_)\ntuned_clf['LogisticRegression'].set_params(**tune_model.best_params_)","efc6d7a1":"#LinearDiscriminantAnalysis\ngrid = {\"solver\" : [\"svd\"], \"tol\" : [0.0001,0.0002,0.0003]}\n \ntune_model = model_selection.GridSearchCV(tuned_clf['LinearDiscriminantAnalysis'], param_grid=grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)\ntune_model.fit(train.drop(['PassengerId'], axis=1), target)\nprint(\"Best Parameters:\")\nprint(tune_model.best_params_)\ntuned_clf['LinearDiscriminantAnalysis'].set_params(**tune_model.best_params_)","3f1fd98e":"#Evaluating the performance of our tuned models\n\ncv_train_mean = []\ncv_test_mean = []\ncv_score_time = []\ncv_fit_time = []\ncv_name = []\npredictions = []\n\nfor _, classifier in tuned_clf.items():\n    cv_results = model_selection.cross_validate(classifier, train.drop(['PassengerId'], axis=1), target, cv=cv_split, return_train_score=True)\n    cv_train_mean.append(cv_results['train_score'].mean())\n    cv_test_mean.append(cv_results['test_score'].mean())\n    cv_score_time.append(cv_results['score_time'].mean())\n    cv_fit_time.append(cv_results['fit_time'].mean())\n    cv_name.append(str(classifier.__class__.__name__))\n    classifier.fit(train.drop(['PassengerId'], axis=1), target)\n    predictions.append(classifier.predict(test.drop(['PassengerId'], axis=1)))\n    \n\nperformance_df = pd.DataFrame({\"Algorithm\":cv_name, \"Train Score\":cv_train_mean, \"Test Score\":cv_test_mean, 'Score Time':cv_score_time, 'Fit Time':cv_fit_time})\nperformance_df","d823b161":"sns.barplot('Test Score', 'Algorithm', data=performance_df)","e1897b71":"#voting_1 = VotingClassifier(estimators=[\n#    ('DecisionTreeClassifier', tuned_clf['DecisionTreeClassifier']), \n#    ('AdaBoostClassifier', tuned_clf['AdaBoostClassifier']),\n#    ('RandomForestClassifier', tuned_clf['RandomForestClassifier']), \n#    ('ExtraTreesClassifier',tuned_clf['ExtraTreesClassifier'])], voting='soft', n_jobs=4)\n\n#voting_1 = voting_1.fit(train.drop(['PassengerId'], axis=1), target)\n#pred_1 = voting_1.predict(test.drop(['PassengerId'], axis=1))","54bf4a9a":"#voting_2 = VotingClassifier(estimators=[\n#    ('GradientBoostingClassifier',tuned_clf['GradientBoostingClassifier']), \n#    ('MLPClassifier',tuned_clf['MLPClassifier']), \n#    ('LogisticRegression', tuned_clf['LogisticRegression']), \n#    ('LinearDiscriminantAnalysis', tuned_clf['LinearDiscriminantAnalysis'])], voting='soft', n_jobs=4)\n\n#voting_2 = voting_2.fit(train.drop(['PassengerId'], axis=1), target)\n#pred_2 = voting_2.predict(test.drop(['PassengerId'], axis=1))","085bc1b3":"voting_3 = VotingClassifier(estimators=[\n    ('DecisionTreeClassifier', tuned_clf['DecisionTreeClassifier']), \n    ('AdaBoostClassifier', tuned_clf['AdaBoostClassifier']),\n    ('RandomForestClassifier', tuned_clf['RandomForestClassifier']), \n    ('ExtraTreesClassifier',tuned_clf['ExtraTreesClassifier']),\n    ('GradientBoostingClassifier',tuned_clf['GradientBoostingClassifier']), \n    ('MLPClassifier',tuned_clf['MLPClassifier']), \n    ('LogisticRegression', tuned_clf['LogisticRegression']), \n    ('LinearDiscriminantAnalysis', tuned_clf['LinearDiscriminantAnalysis'])], voting='soft', n_jobs=4)\n\nvoting_3 = voting_3.fit(train.drop(['PassengerId'], axis=1), target)\npred_3 = voting_3.predict(test.drop(['PassengerId'], axis=1))","c94b8791":"#sol1 = pd.DataFrame(data=pred_1, columns=['Survived'], index=test['PassengerId'])\n#sol2 = pd.DataFrame(data=pred_2, columns=['Survived'], index=test['PassengerId'])\nsol3 = pd.DataFrame(data=pred_3, columns=['Survived'], index=test['PassengerId'])","c058063c":"#sol1.to_csv(\"sol1.csv\")\n#sol2.to_csv(\"sol2.csv\")\nsol3.to_csv(\"sol3.csv\")","54fb6697":"### Dealing with missing Embarked values","41a5ef48":"### Machine Learning\n\nWe will first see performance of different tuned models after that we will select the models with best performance and high diversity for ensemble modeling. ","0007b128":"### Ensemble Modeling\n\n* Now that we have performance of individual models we can select models for ensemble modeling on the following grounds:\n    \n    * The selected models should have high test score on cross_validation.\n    * The test set predictions of the selected models should be similar.\n\n* Condsidering the above statements we can create three ensembles which are:\n    \n    * Ensemble 1 --> DecisionTreeClassifier, AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\n    * Ensemble 2 --> LinearDiscriminantAnalysis, LogisticRegression, GradientBoostingClassifier, MLPClassifier \n    * Ensemble 3 --> Combining both Ensemble 1 and Ensemble 2\n","50199578":"Clearly Embarked is an important feature to determine the target feature. Also, we can see that the passengers whose cabin values were missing have a lower chance of survival. ","f8985d5b":"### Dealing with missing Fare value","1e9a6c34":"### Dealing with missing Age values","b6b8d962":"## Feature Engineering\n\nNow that we have cleaned our dataset it's now time to decide the important features and then to convert them into forms which our machine learning models could interpret.","d70be4cc":"By analysing the barplots and the boxplots we can conclude that mean and median follow the same trends for all the feature so we can use either to fill out the missing age values(here we will use median). We can draw the following conclusions from the below visualizations:\n\n* Age decreases with increasing Pclass. A valid argument to explain this trend is could be that old people have more money and hence can afford first class.\n\n* Male Passengers are comparitively older than the female passengers. A valid argument to explain this trend could be that for all the couples on-board wives would be younger than husbands in general.\n\n* Relation of Age with SibSp could be divided into two parts. The first part can be defined as SibSp in [0,1] and the second part could be defined as SibSp in [2,8]. \n    \n    * For the first part we can see that singles are younger than couples (without children) which is an acceptable trend.\n    \n    * For the second part we can see that age decreases with the increase in number of siblings on-board which is an acceptable trend because children are like to come with their brothers and sisters along with their parents.\n    \n* Relation of Age with Parch can also be divided in two parts. The first part can be defined as Parch in [0,2] and the second part can be defined as [3,9].\n\n    * For the first part we can see that age decreases with an increase in number of parents on-board which is an accepatble trend.\n    \n    * For the second part we can see that age increases with an increase in number of children on-board which is an acceptable trend.\n    \n* We can see that in general Age decrease with an increase in Family Size.\n\n* Finally we can see that passengers who embarked from 'C' are older than those who embarked from 'S' who are older than those who embarked from 'Q'. \n    \n\n","5129e557":"##### We can see that Age, Fare, Cabin, Embarked have null values. Since not all machine learning models can tackle Nan values we have to clean our dataset of such values.","9a3256e0":"###### Things to take note here:\n\n    1) There are 1309 data-points in train dataset.\n    2) Data types of all the columns.\n    3) Check which columns have null values.","f6c986ec":"### Dealing with Categorical Features\n\n* We will use OneHot encodings for Pclass, Name, Sex, Age, Cabin, Embarked, Ticket, Family_Size (We will drop SibSp and Parch).","ab9d2f68":"### Analysing Age feature\n\nSince we know that children were given priority when saving passengers and that old passengers have a lower chance of survivabilty it would be a good idea to divide Age into different categories.","b24dcc5d":"### Dealing with missing Cabin values\n\n* Since cabin has a lot of missing values we are going to update the Cabin column as follows:\n    * For the non-missing values we are going to update Cabin with the first character of Cabin. For eg: if Cabin is C13 we are going to change it to C\n    \n    * Replace all the missing Cabin values with X","404dad5d":"Lets have a peek at our dataset","02fc7d06":"### Analysing the Name feature\n\nEven though the name of the person plays no role in determining whether he\/she survived. The title in the name can provide useful insights like socio-economic status, age, gender, etc..\n\nHence we are going to extract title from each name.","de47ddb3":"The trend between Embarked and Age could be verified by the following plot. We can see that most of the passengers who embarked\nfrom 'C' went to first class since we know that people in first class are older we can verify that passengers who embarked from\n'C' are older. The same conclusions can be drawn for 'S' and 'Q'.","61cfaf40":"# Welcome to this beginners notebook on titanic dataset to achieve 80% accuracy.\n\nThis notebook will provide you with an overview of how you can approach a data science problem in order to achieve high prediction accuracy and to gain useful insights as well as conclusions from a given dataset.\n\nSo without further ado lets start by importing packages.\n\nCredits:\n\nThis notebook is inspired by ideas and code implemented in the following notebooks:\n\nhttps:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n\nhttps:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy","bf0d5175":"### Finally, we create a voting classifier for our final predictions!","ea9e7332":"* The following conclusions can be drawn from the plots below:\n    * More people survived in first class this can be due to the fact that they were given priority on the basis of their socio-economic status\n    \n    * More female passengers survived than male passengers again this is due to the fact that women and children are given priority over males when it comes to saving lives.","564b7044":"### Tuning ","137c4df2":"### Analysing Fare Feature\n\nWe can see that passengers who survived have a higher probability for paying a high fare as compared to those who did not survived. Hence people who survived were mostly from first class (a result we saw before as well).\n\nAgain, Fare is an important feature.","3c7225e4":"* The following conclusions can be drawn from the plots below:\n    \n    * Singles have a lower chance of survival. This can be verified by the fact that there are more 'male single' passengers than 'female single' passsengers\n    \n    * Family_Size of 1, 2, 3 has higher chances of survival. This is due to the fact that females and children were given priority over males \n    \n    * Survival probability drops again for Family_Size > 4. I don't know the reason for this trend but if you have any insights do feel to post in the comment section :)\n\n* On the basis of the above observations Family_Size could be divided into 3 groups which are:\n    \n    * Singles\n    * Family_Size -> [1,3]\n    * Family_Size >= 4","bac6c4fc":"### Analysing Ticket Feature\n\nAgain ticket as a whole may not be important but the prefix can provide some useful insights.","90afc14c":"Thank you for reading this notebook. If you have any doubts\/suggestions do feel free to post in the comment section!\nIf you liked this notebook dont forget to upvote :)"}}