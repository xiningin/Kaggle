{"cell_type":{"8aa649cd":"code","8e6abeee":"code","3d4aac45":"code","6c558e2e":"code","f7957dcd":"code","e824b1ce":"code","0491cd3f":"code","c0161ef7":"code","468a008c":"code","43970386":"code","9c422307":"code","6f47594d":"code","7c19d44a":"code","fd07bd6f":"code","3daefc10":"code","37cfd635":"code","6491f403":"code","dd6fc627":"code","aae84aaa":"markdown","f55d8e83":"markdown","35b5e10e":"markdown","4b219746":"markdown","48cd69e8":"markdown","bb6d4adb":"markdown","02adba1c":"markdown","33b97d34":"markdown","c28227b5":"markdown","a1ba3678":"markdown","5d5770d2":"markdown","03b79aec":"markdown","8fde2e89":"markdown","efe467cd":"markdown","868a08f6":"markdown","7dec47bf":"markdown","71ded8bd":"markdown"},"source":{"8aa649cd":"import os, pickle\nimport re, gc\nimport keras\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers","8e6abeee":"conf = {\n    \"kaggle\" : True,\n    \"embedding_file\" : '..\/input\/glove840b300dtxt\/glove.840B.300d.txt',\n    \"embedding_pickle\" : '\/data\/glove.840B.300d\/glove.840B.300d.pickle',\n    \"train_path\" : \"..\/input\/kuc-hackathon-winter-2018\/drugsComTrain_raw.csv\",\n    \"test_path\" : \"..\/input\/kuc-hackathon-winter-2018\/drugsComTest_raw.csv\",\n    \n    \n    \"max_features\" : 30000,\n    \"maxlen\" : 100,\n    \"embed_size\" : 300,\n    \n    \n    \"batch_size\":3000,\n    \"epochs\":500,\n    \"n_splits\":10,\n    \"random_state\":0\n}\n\nmodel_conf = {\n    \"SpDr\": 0.2,\n    \"GRU_Units\":128,\n    \"Conv_Units\":128,\n    \"conv_kernel\":1,\n    \"Dense1_Unit\":128,\n    \"Dr1\":0.2,\n    \"Dense2_Unit\":128,\n    \"Dr2\": 0.2,\n    \n    \n    \"lr\" : 1e-4,\n    \"loss\" : 'mean_absolute_error',\n    \"metrics\": ['mae']#list\n}","3d4aac45":"d_train = pd.read_csv(conf[\"train_path\"])\nd_test = pd.read_csv(conf[\"test_path\"])\n\nplt.figure(figsize=(8,8))\nsns.distplot(d_train['rating'])\n\nplt.xlabel('Rating')\nplt.ylabel('Dist')\nplt.title(\"Distribution of rating\")\nplt.show()","6c558e2e":"d_train['year'] = pd.to_datetime(d_train['date'], errors='coerce')\ncnt = d_train['year'].dt.year.value_counts()\ncnt = cnt.sort_index()\nplt.figure(figsize=(9,6))\nsns.barplot(cnt.index, cnt.values,color='blue',alpha=0.4)\nplt.xticks(rotation='vertical')\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Reviews per year\")\nplt.show()","f7957dcd":"d_train['month'] = pd.to_datetime(d_train['date'], errors='coerce')\ncnt = d_train['month'].dt.month.value_counts()\ncnt = cnt.sort_index()\nplt.figure(figsize=(9,6))\nsns.barplot(cnt.index, cnt.values,color='blue',alpha=0.4)\nplt.xticks(rotation='vertical')\nplt.xlabel('Month', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Reviews per month\")\nplt.show()","e824b1ce":"d_train['day'] = pd.to_datetime(d_train['date'], errors='coerce')\ncnt = d_train['day'].dt.day.value_counts()\ncnt = cnt.sort_index()\nplt.figure(figsize=(9,6))\nsns.barplot(cnt.index, cnt.values,color='blue',alpha=0.4)\nplt.xticks(rotation='vertical')\nplt.xlabel('Day', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Reviews per day\")\nplt.show()","0491cd3f":"drug = d_train['drugName'].value_counts()\n\nplt.figure(figsize=(20,6))\n\nsns.barplot(drug[:15].index, drug[:15],color='blue',alpha=0.4)\n\nplt.xlabel('Name of drug', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Top 10 popular drugs\")\nplt.xticks(rotation=90)\nplt.show()","c0161ef7":"plt.figure(figsize=(16,8))\n\nsns.distplot(d_train['usefulCount'].value_counts())\n\n\nplt.xlabel('Number of users who found review useful')\nplt.ylabel('Dist')\nplt.title(\"Distribution of usefulCount\")\n\n\nplt.show()","468a008c":"plt.figure(figsize=(8,6))\n\nsns.lineplot(x='rating',y='usefulCount',data=d_train)","43970386":"plt.figure(figsize=(9,9))\n\nsns.distplot(d_train['rating'])\nsns.distplot(d_test['rating'],color='violet')\n\nplt.xlabel('Rating')\nplt.ylabel('Dist')\nplt.title(\"Distribution of rating (train and test)\")\n\nplt.show()","9c422307":"train = pd.read_csv(conf[\"train_path\"])\ntest = pd.read_csv(conf[\"test_path\"])\n\nX_train = train[\"review\"]\ny_train = train[\"rating\"].values \/ 10\n\nX_test = test[\"review\"]\nsample_pred = np.zeros_like(test[\"rating\"], dtype=np.float32)\n\ntokenizer = text.Tokenizer(num_words=conf[\"max_features\"])\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train, maxlen=conf[\"maxlen\"])\nx_test = sequence.pad_sequences(X_test, maxlen=conf[\"maxlen\"])","6f47594d":"if conf[\"kaggle\"]: #tip: cause of config code style you can use same code of local experiments & for kaggle submission\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(conf[\"embedding_file\"],encoding=\"UTF-8\"))\nelse:\n    with open(conf[\"embedding_pickle\"], 'rb') as handle: #if you dont want to wait 6-9 minuts you can pickle your dict once\n        embeddings_index = pickle.load(handle) # and just unpickle next, it may take near 10-20 seconds\n    \nword_index = tokenizer.word_index\nnb_words = min(conf[\"max_features\"], len(word_index))\nembedding_matrix = np.zeros((nb_words, conf[\"embed_size\"]))\nfor word, i in word_index.items():\n    if i >= conf[\"max_features\"]: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \nconf[\"embedding_matrix\"] = embedding_matrix\n\ndel embeddings_index\ngc.collect()","7c19d44a":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    \n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n","fd07bd6f":"def model_lstm_atten(main_conf: dict, model_conf:dict):\n    inp = Input(shape=(main_conf[\"maxlen\"], ))\n    x = Embedding(main_conf[\"max_features\"], main_conf[\"embed_size\"], weights=[main_conf[\"embedding_matrix\"]],trainable = False)(inp)\n    x = Bidirectional(CuDNNLSTM(model_conf[\"LSTM_first_layer_units\"], return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(model_conf[\"LSTM_second_layer_units\"], return_sequences=True))(x)\n    x = Attention(main_conf[\"maxlen\"])(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(1, activation=\"relu\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss=model_conf[\"loss\"],optimizer=Adam(lr=model_conf[\"lr\"]),metrics=model_conf[\"metrics\"])\n    \n    return model","3daefc10":"def get_toxic_model(main_conf: dict, model_conf:dict):\n    sequence_input = Input(shape=(main_conf[\"maxlen\"], ))\n    x = Embedding(main_conf[\"max_features\"], main_conf[\"embed_size\"], weights=[main_conf[\"embedding_matrix\"]],trainable = False)(sequence_input)\n    x = SpatialDropout1D(model_conf[\"SpDr\"])(x)\n    x = Bidirectional(CuDNNGRU(model_conf[\"GRU_Units\"], return_sequences=True))(x)\n    x = Conv1D(model_conf[\"Conv_Units\"], kernel_size = model_conf[\"conv_kernel\"], padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    x = concatenate([avg_pool, max_pool])\n    x = Dense(model_conf[\"Dense1_Unit\"], activation='relu')(x)\n    x = Dropout(model_conf[\"Dr1\"])(x)\n    x = Dense(model_conf[\"Dense2_Unit\"], activation='relu')(x)\n    x = Dropout(model_conf[\"Dr2\"])(x)\n    preds = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(sequence_input, preds)\n    model.compile(loss=model_conf[\"loss\"],optimizer=Adam(lr=model_conf[\"lr\"]),metrics=model_conf[\"metrics\"])\n    return model","37cfd635":"kf = KFold(n_splits=conf[\"n_splits\"], shuffle=True, random_state=conf[\"random_state\"])\ncnt = 0\nmodel_conf[\"LSTM_first_layer_units\"] = 128\nmodel_conf[\"LSTM_second_layer_units\"] = 64\n\nfor train_index, test_index in kf.split(x_train, y_train):\n    model = model_lstm_atten(conf, model_conf)\n    es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n\n    hist = model.fit(x_train[train_index], y_train[train_index], batch_size=conf[\"batch_size\"], epochs=conf[\"epochs\"], \n                     validation_data=(x_train[test_index], y_train[test_index]),callbacks=[es])\n    model.save(f\"model_lstm_atten_cv_{cnt}.keras\")\n    \n    sample_pred +=  model.predict(x_test, conf[\"batch_size\"]*2).reshape(-1)\n    cnt+=1","6491f403":"kf = KFold(n_splits=conf[\"n_splits\"], shuffle=True, random_state=conf[\"random_state\"])\ncnt = 0\n\nfor train_index, test_index in kf.split(x_train, y_train):\n    model = get_toxic_model(conf, model_conf)\n    es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n\n    hist = model.fit(x_train[train_index], y_train[train_index], batch_size=conf[\"batch_size\"], epochs=conf[\"epochs\"], \n                     validation_data=(x_train[test_index], y_train[test_index]),callbacks=[es])\n    model.save(f\"model_gru_cnn_{cnt}.keras\")\n    \n    sample_pred +=  model.predict(x_test, conf[\"batch_size\"]*2).reshape(-1)\n    cnt+=1","dd6fc627":"print(mean_absolute_error(test[\"rating\"], 10 * sample_pred\/ conf[\"n_splits\"]\/2))\n","aae84aaa":"Let\u2019s prepare the data.\n\nTip: you can use \"config style\" for code reusability and faster experiments","f55d8e83":"The first thing we must check is the distribution of rating.This graph is bimodal distribution due to the reasons why customers write review in the first place.","35b5e10e":"Number of users who found review useful. Gamma distribution it is","4b219746":"We examine the total amount of rewiews made per year. Growth from year 2015 and later have reasons (maybe social) unimportant for the current goal","48cd69e8":"Writing attention mechanism.\n\nBecause word processing task can be described as sequence processing task we make two layered lstm with Attention to prevent overfitting and data augmentation\n\nActually stolen from: https:\/\/www.kaggle.com\/qqgeogor\/keras-lstm-attention-glove840b-lb-0-043","bb6d4adb":"**Team**: 5 top 100\n\n**Univesity**: PFUR\/RUDN \n\nThis kernel was made by:\n\n[Stanislav Prikhodko](http:\/\/www.kaggle.com\/stasian)\n    \n[Daniil Larionov](http:\/\/kaggle.com\/rexhaif)\n\n[Rustem Zalyalov](http:\/\/www.kaggle.com\/kaichou)\n\n[Katherine Lozhenko](http:\/\/www.kaggle.com\/notkappa)\n\n[Sergey Kuzmin](http:\/\/www.kaggle.com\/seekuu)","02adba1c":"Second is old but still attrective GRU->CNN->Dense, was used by us in Toxic Classification Challange and it gave us bronze medal:)\n\nThis model is more about catchaning structure of sentences. Also checkout this:\n\nI havent find original paper(cause it wasnt on arxiv(facepalm)) but you can look here: https:\/\/arxiv.org\/pdf\/1806.11316v1.pdf","33b97d34":"We examine the total amount of rewiews made per month. The graph is constant so it isn't really important.","c28227b5":"We chose to combine 2 kinds of models: \n\nFirst is really complicated 2 layer LSTM model with Attention layer for increasing information \"capability\", its model focusing on catching sequence information. Also checkout\n\nthis: https:\/\/arxiv.org\/pdf\/1709.00893v1.pdf\n\nand this: https:\/\/www.kaggle.com\/qqgeogor\/keras-lstm-attention-glove840b-lb-0-043  we changed alot, but base arcitecture was took","a1ba3678":"**Introduction**\nThis kernel was made to formalize reviews for the prescripted medicine and considers how such reviews might develop in the future. Medication feedback made by customers can help them find the most cost-effectiveness drug and for pharmacists there is good evidence that medication review improves process outcomes of prescribing and spot out specific problems early on. \nWe will use MAE for this because we don't need positive errors to cancel out negative ones or to give a relatively high weight to large errors.","5d5770d2":"Correlation between rating and users who found review useful. Positive linear correlation\n\nAcording to this we decided to use **only** text data for preventing major **data leak** and, you know, be more usefull in real world application","03b79aec":"For easier predictions we divide ratings by 10","8fde2e89":"Split reviews by words  and do embedding matrix\n\nWe choose Glove embedding, cause we earned best scores here","efe467cd":"Using out of fold predictions and  blending them","868a08f6":"We examine the total amount of rewiews made per day. As the previous one, the graph is constant .","7dec47bf":"Test the distribution of ratings in train and test dataset. Graphs are similar, the data was choosen correctly ","71ded8bd":"Top 10 popular drugs. Birth control and antidepressants are the most common drugs to see on top"}}