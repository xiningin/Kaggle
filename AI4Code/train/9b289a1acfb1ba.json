{"cell_type":{"ab09886a":"code","0b468a0d":"code","b56a87c6":"code","efdc1014":"code","0d6f25db":"code","bf22d709":"code","65a1d5f8":"code","d45e06c0":"code","2b5fec45":"code","12ffb46d":"code","0f928652":"code","d2cb7ef4":"code","69d870b9":"code","d9d6cfec":"code","ca253b9c":"code","980448da":"code","0815caa8":"code","48ede062":"markdown","7fc0beb4":"markdown","7897d676":"markdown","bdffe049":"markdown","3916f755":"markdown","0ea9ea72":"markdown","8655aa81":"markdown","fa347114":"markdown","875ac26a":"markdown","693270e4":"markdown","ce7af3bf":"markdown","9560a06c":"markdown","defaf7e9":"markdown"},"source":{"ab09886a":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom operator import itemgetter\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import (GradientBoostingRegressor, GradientBoostingClassifier)\nimport xgboost","0b468a0d":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","b56a87c6":"def find_missing_percent(data):\n    \"\"\"\n    Returns dataframe containing the total missing values and percentage of total\n    missing values of a column.\n    \"\"\"\n    miss_df = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n    for col in data.columns:\n        sum_miss_val = data[col].isnull().sum()\n        percent_miss_val = round((sum_miss_val\/data.shape[0])*100,2)\n        miss_df = miss_df.append(dict(zip(miss_df.columns,[col,sum_miss_val,percent_miss_val])),ignore_index=True)\n    return miss_df","efdc1014":"miss_df = find_missing_percent(train)\n'''Displays columns with missing values'''\ndisplay(miss_df[miss_df['PercentMissing']>0.0])\nprint(\"\\n\")\nprint(f\"Number of columns with missing values:{str(miss_df[miss_df['PercentMissing']>0.0].shape[0])}\")","0d6f25db":"drop_cols = miss_df[miss_df['PercentMissing'] >70.0].ColumnName.tolist()\nprint(f\"Number of columns with more than 70%: {len(drop_cols)}\")\ntrain = train.drop(drop_cols,axis=1)\ntest = test.drop(drop_cols,axis =1)\n\nmiss_df = miss_df[miss_df['ColumnName'].isin(train.columns)]\n'''Columns to Impute'''\nimpute_cols = miss_df[miss_df['TotalMissingVals']>0.0].ColumnName.tolist()\nmiss_df[miss_df['TotalMissingVals']>0.0]","bf22d709":"'''Segregate the numeric and categoric data'''\nnumeric_cols = train.select_dtypes(['float','int']).columns\ncategoric_cols = train.select_dtypes('object').columns\n\ntrain_numeric = train[numeric_cols[:-1]]\ntrain_categoric = train[categoric_cols]\n\ntest_numeric = test[numeric_cols[:-1]]\ntest_categoric = test[categoric_cols]\n\nnominal_cols = ['MSZoning', 'Street','LandContour','Neighborhood','Condition1','Condition2',\n                'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation',\n                'Heating','GarageType','SaleType','SaleCondition']\nordinal_cols = ['ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n                'BsmtFinType2','HeatingQC','CentralAir','Electrical','KitchenQual','Functional',\n                'FireplaceQu','GarageFinish','GarageQual','GarageCond','PavedDrive','LotShape',\n                'Utilities','LandSlope','BldgType','HouseStyle','LotConfig']","65a1d5f8":"def mice_imputation_numeric(train_numeric, test_numeric):\n    \"\"\"\n    Impute numeric data using MICE imputation with Gradient Boosting Regressor.\n    \"\"\"\n    iter_imp_numeric = IterativeImputer(GradientBoostingRegressor())\n    imputed_train = iter_imp_numeric.fit_transform(train_numeric)\n    imputed_test = iter_imp_numeric.transform(test_numeric)\n    train_numeric_imp = pd.DataFrame(imputed_train, columns = train_numeric.columns, index= train_numeric.index)\n    test_numeric_imp = pd.DataFrame(imputed_test, columns = test_numeric.columns, index = test_numeric.index)\n    return train_numeric_imp, test_numeric_imp\n\ndef mice_imputation_categoric(train_categoric, test_categoric):\n    \"\"\"\n    Impute categoric data using MICE imputation with Gradient Boosting Classifier.\n    Steps:\n    1. Ordinal Encode the non-null values\n    2. Use MICE imputation with Gradient Boosting Classifier to impute the ordinal encoded data\n    3. Inverse transform the ordinal encoded data.\n    \"\"\"\n    ordinal_dict={}\n    for col in train_categoric:\n        '''Ordinal encode train data'''\n        ordinal_dict[col] = OrdinalEncoder()\n        nn_vals = np.array(train_categoric[col][train_categoric[col].notnull()]).reshape(-1,1)\n        nn_vals_arr = np.array(ordinal_dict[col].fit_transform(nn_vals)).reshape(-1,)\n        train_categoric[col].loc[train_categoric[col].notnull()] = nn_vals_arr\n\n    for col in test_categoric:\n        '''Ordinal encode test data'''\n        nn_vals = np.array(test_categoric[col][test_categoric[col].notnull()]).reshape(-1,1)\n        nn_vals_arr = np.array(ordinal_dict[col].transform(nn_vals)).reshape(-1,)\n        test_categoric[col].loc[test_categoric[col].notnull()] = nn_vals_arr\n    \n    '''Impute the data using MICE with Gradient Boosting Classifier'''\n    iter_imp_categoric = IterativeImputer(GradientBoostingClassifier(), max_iter =5, initial_strategy='most_frequent')\n    imputed_train = iter_imp_categoric.fit_transform(train_categoric)\n    imputed_test = iter_imp_categoric.transform(test_categoric)\n    train_categoric_imp = pd.DataFrame(imputed_train, columns =train_categoric.columns,index = train_categoric.index).astype(int)\n    test_categoric_imp = pd.DataFrame(imputed_test, columns=test_categoric.columns,index =test_categoric.index).astype(int)\n    \n    '''Inverse Transform'''\n    for col in train_categoric_imp.columns:\n        oe = ordinal_dict[col]\n        train_arr= np.array(train_categoric_imp[col]).reshape(-1,1)\n        test_arr = np.array(test_categoric_imp[col]).reshape(-1,1)\n        train_categoric_imp[col] = oe.inverse_transform(train_arr)\n        test_categoric_imp[col] = oe.inverse_transform(test_arr)\n\n    return train_categoric_imp, test_categoric_imp\n","d45e06c0":"train_numeric_imp, test_numeric_imp = mice_imputation_numeric(train_numeric,test_numeric)\ntrain_categoric_imp, test_categoric_imp = mice_imputation_categoric(train_categoric, test_categoric)\n\n'''Concatenate Numeric and Categoric Training and Test set data '''\ntrain = pd.concat([train_numeric_imp, train_categoric_imp, train['SalePrice']], axis = 1)\ntest = pd.concat([test_numeric_imp, test_categoric_imp], axis =1)","2b5fec45":"def plot_histogram(train, col1, col2, cols_list, last_one =False):\n    \"\"\"\n    Plot the histogram for the numerical columns. The bin width\n    is calculated by Freedman Diaconis Rule and Sturges rule.\n    \n    Freedman-Diaconis Rule:\n    Freedman-Diaconis Rule is a rule to find the optimal number of bins.\n    Bin width: (2 * IQR)\/(N^1\/3)\n    N - Size of the data\n    Number of bins : (Range\/ bin-width)\n    \n    Disadvantage: The IQR might be zero for certain columns. In\n    that case the bin width might be equal to infinity. In that case \n    the actual range of the data is returned as bin width.\n    \n    Sturges Rule:\n    Sturges Rule is a rule to find the optimal number of bins.\n    Bin width: (Range\/ bin-width)\n    N - Size of the data\n    Number of bins : ceil(log2(N))+1\n    \n    \"\"\"\n    if(col1 in cols_list):\n        freq1, bin_edges1 = np.histogram(train[col1],bins='sturges')\n    else:\n        freq1, bin_edges1 = np.histogram(train[col1],bins='fd')\n    if(col2 in cols_list):\n        freq2, bin_edges2 = np.histogram(train[col2],bins='sturges')\n    else:\n        freq2, bin_edges2 = np.histogram(train[col2],bins='fd')\n        \n    if(last_one!=True):\n        plt.figure(figsize=(45,18))  \n        ax1 = plt.subplot(1,2,1)\n        ax1.set_title(col1,fontsize=45)\n        ax1.set_xlabel(col1,fontsize=40)\n        ax1.set_ylabel('Frequency',fontsize=40)\n        train[col1].hist(bins=bin_edges1,ax = ax1, xlabelsize=30, ylabelsize=30)\n        \n    else:\n        plt.figure(figsize=(20,10))\n        ax1 = plt.subplot(1,2,1)\n        ax1.set_title(col1,fontsize=25)\n        ax1.set_xlabel(col1,fontsize=20)\n        ax1.set_ylabel('Frequency',fontsize=20)\n        train[col1].hist(bins=bin_edges1,ax = ax1, xlabelsize=15, ylabelsize=15)\n    \n    if(last_one != True):\n        ax2 = plt.subplot(1,2,2)\n        ax2.set_title(col2,fontsize=45)\n        ax2.set_xlabel(col2,fontsize=40)\n        ax2.set_ylabel('Frequency',fontsize=40)\n        train[col2].hist(bins=bin_edges2, ax = ax2, xlabelsize=30, ylabelsize=30)\n","12ffb46d":"'''\nThese columns have IQR equal to zero. Freedman Diaconis Rule doesn't work significantly well for these columns. \nUse sturges rule to find the optimal number of bins for the columns.\n'''\ncols_list = ['LowQualFinSF','BsmtFinSF2','BsmtHalfBath','KitchenAbvGr',\n             'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']\n\n# Except ID\nhist_cols = numeric_cols[1:]\nfor i in range(0,len(hist_cols),2):\n    if(i == len(hist_cols)-1):\n        plot_histogram(train,hist_cols[i],hist_cols[i],cols_list,True)\n    else:\n        plot_histogram(train,hist_cols[i],hist_cols[i+1],cols_list)\n        ","0f928652":"def find_skewness(train, numeric_cols):\n    \"\"\"\n    Calculate the skewness of the columns and segregate the positive\n    and negative skewed data.\n    \"\"\"\n    skew_dict = {}\n    for col in numeric_cols:\n        skew_dict[col] = train[col].skew()\n\n    skew_dict = dict(sorted(skew_dict.items(),key=itemgetter(1)))\n    positive_skew_dict = {k:v for (k,v) in skew_dict.items() if v>0}\n    negative_skew_dict = {k:v for (k,v) in skew_dict.items() if v<0}\n    return skew_dict, positive_skew_dict, negative_skew_dict\n\ndef add_constant(data, highly_pos_skewed):\n    \"\"\"\n    Look for zeros in the columns. If zeros are present then the log(0) would result in -infinity.\n    So before transforming it we need to add it with some constant.\n    \"\"\"\n    C = 1\n    for col in highly_pos_skewed.keys():\n        if(col != 'SalePrice'):\n            if(len(data[data[col] == 0]) > 0):\n                data[col] = data[col] + C\n    return data\n\ndef log_transform(data, highly_pos_skewed):\n    \"\"\"\n    Log transformation of highly positively skewed columns.\n    \"\"\"\n    for col in highly_pos_skewed.keys():\n        if(col != 'SalePrice'):\n            data[col] = np.log10(data[col])\n    return data\n\ndef sqrt_transform(data, moderately_pos_skewed):\n    \"\"\"\n    Square root transformation of moderately skewed columns.\n    \"\"\"\n    for col in moderately_pos_skewed.keys():\n        if(col != 'SalePrice'):\n            data[col] = np.sqrt(data[col])\n    return data\n\ndef reflect_sqrt_transform(data, moderately_neg_skewed):\n    \"\"\"\n    Reflection and log transformation of highly negatively skewed \n    columns.\n    \"\"\"\n    for col in moderately_neg_skewed.keys():\n        if(col != 'SalePrice'):\n            K = max(data[col]) + 1\n            data[col] = np.sqrt(K - data[col])\n    return data","d2cb7ef4":"\"\"\"\nIf skewness is less than -1 or greater than 1, the distribution is highly skewed.\nIf skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\nIf skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n\"\"\"\nskew_dict, positive_skew_dict, negative_skew_dict = find_skewness(train, numeric_cols)\nmoderately_pos_skewed = {k:v for (k,v) in positive_skew_dict.items() if v>0.5 and v<=1}\nhighly_pos_skewed = {k:v for (k,v) in positive_skew_dict.items() if v>1}\nmoderately_neg_skewed = {k:v for (k,v) in negative_skew_dict.items() if v>-1 and v<=0.5}\nhighly_neg_skewed = {k:v for (k,v) in negative_skew_dict.items() if v<-1}\n\n'''Transform train data.'''\ntrain = add_constant(train, highly_pos_skewed)\ntrain = log_transform(train, highly_pos_skewed)\ntrain = sqrt_transform(train, moderately_pos_skewed)\ntrain = reflect_sqrt_transform(train, moderately_neg_skewed )\n'''Transform test data.'''\ntest = add_constant(test, highly_pos_skewed)\ntest = log_transform(test, highly_pos_skewed)\ntest = sqrt_transform(test, moderately_pos_skewed)\ntest = reflect_sqrt_transform(test, moderately_neg_skewed )","69d870b9":"ordinal_col_dicts = {\n    'ExterQual':{'TA': 3, 'Gd': 2, 'Ex': 1, 'Fa': 4,'Po':5},\n    'ExterCond': {'TA': 3, 'Gd': 2, 'Fa': 4, 'Ex': 1, 'Po': 5},\n    'BsmtQual': {'TA': 3, 'Gd': 2, 'Ex': 1, 'Fa': 4,'Po':5},\n    'BsmtCond': {'Fa': 4, 'Gd': 2, 'Po': 5, 'TA': 3,'Ex':1, 'NA':6},\n    'BsmtExposure': {'No': 4, 'Av': 2, 'Gd': 1, 'Mn': 3,'NA':5},\n    'BsmtFinType1': {'Unf': 6, 'GLQ': 1, 'ALQ': 2, 'BLQ': 3, 'Rec': 4, 'LwQ': 5, 'NA':7},\n    'BsmtFinType2': {'Unf': 6, 'Rec': 4, 'LwQ': 5, 'BLQ': 3, 'ALQ': 2, 'GLQ': 1, 'NA':7},\n    'HeatingQC': {'Ex': 1, 'TA': 3, 'Gd': 2, 'Fa': 4, 'Po': 5},\n    'CentralAir': {'Y': 1, 'N': 2},\n    'Electrical': {'SBrkr': 1, 'FuseA': 2, 'FuseF': 3, 'FuseP': 4, 'Mix': 5},\n    'KitchenQual': {'TA': 3, 'Gd': 2, 'Ex': 1, 'Fa': 4,'Po':5},\n    'Functional': {'Typ': 1, 'Min2': 3, 'Min1': 2, 'Mod': 4, 'Maj1': 5, 'Maj2': 6, 'Sev': 7, 'Sal':8},\n    'FireplaceQu': {'Gd': 2, 'TA': 3, 'Fa': 4, 'Ex': 1, 'Po': 5},\n    'GarageFinish': {'Unf': 3, 'RFn': 2, 'Fin': 1, 'NA':4},\n    'GarageQual': {'TA': 3, 'Fa': 4, 'Gd': 2, 'Ex': 1, 'Po': 5},\n    'GarageCond': {'TA': 3, 'Fa': 4, 'Gd': 2, 'Po': 5, 'Ex': 1},\n    'PavedDrive': {'Y': 1, 'N': 3, 'P': 2},\n    'LotShape': {'Reg': 1, 'IR1': 2, 'IR2': 3, 'IR3': 4},\n    'Utilities': {'AllPub': 1, 'NoSeWa': 3, 'NoSewr':2, 'ELO':4},\n    'LandSlope': {'Gtl': 1, 'Mod': 2, 'Sev': 3},\n    'BldgType': {'1Fam': 1, 'TwnhsE': 4, 'Duplex': 3, 'Twnhs': 5, '2fmCon': 2},\n    'HouseStyle': {'1Story': 1,  '2Story': 4, '1.5Fin': 2, 'SLvl': 8, 'SFoyer': 7, '1.5Unf': 3, '2.5Unf': 6, '2.5Fin': 5},\n    'LotConfig': {'Inside': 1, 'Corner': 2, 'CulDSac': 3, 'FR2': 4, 'FR3': 5}\n}\n\ndef ordinal_encode(data, ordinal_col_dicts): \n    \"\"\"\n    Ordinal encode the ordinal columns according to the values in \n    ordinal_col_dicts.\n    \"\"\"\n    for ord_col in ordinal_col_dicts:\n        ord_dict = ordinal_col_dicts[ord_col]\n        data[ord_col] = data[ord_col].map(ord_dict)\n    return data\n\ntrain = ordinal_encode(train, ordinal_col_dicts)\ntest = ordinal_encode(test, ordinal_col_dicts)","d9d6cfec":"def target_encode(train, test):\n    \"\"\"\n    Target encoding uses the mean of the target to encode\n    categorical data.\n    \"\"\"\n    target_enc = TargetEncoder()\n    x_train, y_train = train[train.columns[:-1]], train[train.columns[-1]]\n    x_train = target_enc.fit_transform(x_train,y_train)\n    test = target_enc.transform(test)\n    train = pd.concat([x_train, y_train], axis = 1)\n    return train, test\n\ntrain, test = target_encode(train, test)","ca253b9c":"def standard_scale(train, test):\n    \"\"\"\n    Built - in function to normalize data.\n    \"\"\"\n    ss = StandardScaler()\n    x_train, y_train = train[train.columns[:-1]], train[train.columns[-1]]\n    x_train = pd.DataFrame(ss.fit_transform(x_train),columns=x_train.columns,index=x_train.index)\n    test = pd.DataFrame(ss.transform(test),columns=test.columns,index=test.index)\n    return x_train, y_train, test\n\nx_train, y_train, test =standard_scale(train, test)","980448da":"def fit_model(x_train,y_train, model):\n    \"\"\"\n    Fits x_train to y_train for the given\n    model.\n    \"\"\"\n    model.fit(x_train,y_train)\n    return model\n\n'''Xtreme Gradient Boosting Regressor'''\nmodel = xgboost.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\nmodel = fit_model(x_train,y_train, model)\n'''Predict the outcomes'''\npredictions = model.predict(test)","0815caa8":"submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission['SalePrice'] = predictions\nsubmission.to_csv('submission.csv',index=False)","48ede062":"# 4. Data Modeling\nFit XGBoost Regressor model to the preprocessed data.\n","7fc0beb4":"# 3. Data Transformation\n### 3.1 Skewed data:\n![](https:\/\/miro.medium.com\/max\/1200\/1*nj-Ch3AUFmkd0JUSOW_bTQ.jpeg)\n\n* If skewness is less than -1 or greater than 1, the distribution is **highly skewed**.\n* If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is **moderately skewed**.\n* If skewness is between -0.5 and 0.5, the distribution is **approximately symmetric**.\n* If skewness is 0 the distribution is **symmetric**\n\n#### 3.1.1 **Positively skewed data:**\n* **Log transformation** (when the data is highly skewed)\n    * log(X) - if no zero values are present\n    * log(C + X) - if zero values are present\n        * C is a constant added so that the smallest value will be equal to 1.\n* **Square root transformation** (when the data is moderately skewed)\n    * sqrt(X)\n    \n#### 3.1.2 **Negatively skewed data:**\n* Reflect and Log transformation\n    * log(K - X) - K is a constant from which the values are subtracted so that the smallest value is 1.\n    * (K - X) makes the large number small and the small number large so the negatively skewed data becomes positively skewed.\n* Reflect and Square root transformation\n    * sqrt(K - X) \n\n","7897d676":"# 2. Data Visualization","bdffe049":"### 3.3 Normalization:\n* Normalization is also called as **Feature Scaling**. Normalization scales the values of features between a certain interval. Eg: [0,1]\n\n![](https:\/\/mathstat.co.ke\/wp-content\/uploads\/2020\/03\/z-score-formula.png)\n\n","3916f755":"### 1.2 Drop the columns which have more than 70% of missing values","0ea9ea72":"### Load the dataset for training and testing","8655aa81":"### 1.3 MICE (Multiple Imputation by Chained Equation)\nImputation of missing values can be done using two techniques,\n* **Single Imputation**\n    * Single imputation denotes that the missing value is replaced by a value only once.\n* **Multiple Imputation**\n    * In multiple imputation, the imputation process is repeated multiple times resulting in multiple imputed datasets.\n    \n#### MICE Algorithm:\nThe chained equation process can be broken down into four general steps:\n\n* **Step 1:** A simple imputation, such as imputing the mean, is performed for every missing value in the dataset. These mean imputations can be thought of as \u201cplace holders.\u201d\n* **Step 2:** The \u201cplace holder\u201d mean imputations for one variable (\u201cvar\u201d) are set back to missing.\n* **Step 3:** The observed values from the variable \u201cvar\u201d in Step 2 are regressed(can use any other regressors like Gradient Boosting Regressor or XGBoost Regressor for numeric data) on the other variables in the imputation model, which may or may not consist of all of the variables in the dataset. In other words, \u201cvar\u201d is the dependent variable in a regression model and all the other variables are independent variables in the regression model. These regression models operate under the same assumptions that one would make when performing linear, logistic, or Poison regression models outside of the context of imputing missing data.\n* **Step 4:** The missing values for \u201cvar\u201d are then replaced with predictions (imputations) from the regression model. When \u201cvar\u201d is subsequently used as an independent variable in the regression models for other variables, both the observed and these imputed values will be used.\n* **Step 5:** Steps 2\u20134 are then repeated for each variable that has missing data. The cycling through each of the variables constitutes one iteration or \u201ccycle.\u201d At the end of one cycle all of the missing values have been replaced with predictions from regressions that reflect the relationships observed in the data.\n* **Step 6:** Steps 2\u20134 are repeated for a number of cycles, with the imputations being updated at each cycle.\n\n**Reference:** https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3074241\/\n\n#### MICE Algorithm for Categorical data:\nBefore going through the steps 1 to 6 in MICE algorithm the following steps must be done in order to impute categorical data.\n* **Step 1:** Ordinal Encode the non-null values\n* **Step 2:** Use MICE imputation with Gradient Boosting Classifier to impute the ordinal encoded data\n* **Step 3:** Convert back from ordinal values to categorical values.\n* **Step 4:** Follow steps 1 to 6 in MICE Algorithm. Instead of using Mean imputation for initial strategy use **Mode imputation**.\n\n**Reference:** https:\/\/projector-video-pdf-converter.datacamp.com\/17404\/chapter4.pdf\n\n","fa347114":"# Import the required libraries","875ac26a":"### 3.2 Categorical Encoding\n    \n#### 3.2.1 Ordinal Encoding:\nOrdinal columns are the ones which have ordinality or inherent order in themselves. Example ratings and feedback like excellent, good, fair, poor.\n* Various **Ordinal Encoding techniques** are,\n    * Label Encoding\n    * Binary Encoding\n\n","693270e4":"# Data Preprocessing:\nData preprocessing is a predominant step in machine learning to yield highly accurate and insightful results. Greater the quality of data, greater is the reliance on the produced results. **Incomplete, noisy, and inconsistent data** are the properties of large real-world datasets. Data preprocessing helps in increasing the quality of data by filling in missing incomplete data, smoothing noise and resolving inconsistencies.\n\n* **Incomplete data** can occur for a number of reasons. Attributes of interest may not always be available, such as customer information for sales transaction data. Relevant data may not be recorded due to a misunderstanding, or because of equipment malfunctions.\n* There are many possible reasons for **noisy data** (having incorrect attribute values). The data collection instruments used may be faulty. There may have been human or computer errors occurring at data entry. Errors in data transmission can also occur. Incorrect data may also result from inconsistencies in naming conventions or data codes used, or inconsistent formats for input fields, such as date.\n\nThere are a number of data preprocessing techniques available such as,\n1. **Data Cleaning**\n2. **Data Integration**\n3. **Data Transformation**\n4. **Data Reduction**\n\n![Screen%20Shot%202020-07-10%20at%203.01.36%20PM.png](attachment:Screen%20Shot%202020-07-10%20at%203.01.36%20PM.png)\n\n* **Data cleaning** can be applied to filling in missing values, remove noise, resolving inconsistencies, identifying and removing outliers in the data. \n* **Data integration** merges data from multiple sources into a coherent data store, such as a data warehouse. \n* **Data transformations**, such as normalization, may be applied. For example, normalization may improve the accuracy and efficiency of mining algorithms involving distance measurements. \n* **Data reduction** can reduce the data size by eliminating redundant features, or clustering, for instance. \n\n**Reference**: Data Mining:Concepts and Techniques Second Edition, Jiawei Han, Micheline Kamber.\n\n**PS:** This is my first kaggle notebook contribution. Hope you like it!!","ce7af3bf":"#### 3.2.2 Nominal Encoding:\nNominal columns are the ones which does not have any ordinality or inherent order. Example country names, gender (male, female).\n* Various **Nominal Encoding techniques** available are,\n    * Frequency Encoding\n    * Target Encoding\n    * MEstimate Encoding\n    * Leave One Out Encoding\n    * One-Hot Encoding\n\n","9560a06c":"# 1. Data Cleaning\n### 1.1 Find the missing percentage of each columns in training set.\n","defaf7e9":" ####  3.2.2.1 Target Encoding:\n Target encoding is the process of replacing a categorical value with the mean of the target variable.\n \n\n "}}