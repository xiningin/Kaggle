{"cell_type":{"92333e67":"code","c0c7f746":"code","e578ac45":"code","ff507ba2":"code","2df4f44a":"code","b10a6c09":"code","80567fc6":"code","31db2b57":"code","39c64669":"code","8a192ac6":"code","331720bc":"code","573ddcd9":"code","b1aea2ba":"markdown","0cb18544":"markdown"},"source":{"92333e67":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nfrom wordcloud import WordCloud, STOPWORDS\nimport tensorflow as tf\nimport missingno as msno\nfrom collections import defaultdict\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport json\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nimport tensorflow.keras.backend as K\n\n%matplotlib inline","c0c7f746":"train = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\").fillna(\"\")\ntest = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv\")","e578ac45":"train[\"text\"] = train[\"text\"].apply(lambda x : x.strip())\ntest[\"text\"] = test[\"text\"].apply(lambda x : x.strip())","ff507ba2":"tf.__version__","2df4f44a":"MAX_LEN = 128\nPATH = \"..\/input\/tf-roberta\/\"\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file = PATH + \"vocab-roberta-base.json\",\n    merges_file = PATH + \"merges-roberta-base.txt\",\n    lowercase = True,\n    add_prefix_space = True\n)\n\nsentiment_id = {\"positive\": 1313, \"negative\": 2430, \"neutral\": 7974}\n","b10a6c09":"# Make tokens\n# Make input ids  0-start, 2-end, 1-pad\n# make attention masks\n# make start_token\n# make end token\nnum_rows = train.shape[0]\ntrain_input_ids = []\ntrain_attention_masks = []\ntrain_start_tokens = []\ntrain_end_tokens = []\n\nfor row in range(num_rows):\n    encoding = tokenizer.encode(train.iloc[row, 1])\n    text_tokens = encoding.tokens\n    senti = tokenizer.encode(train.iloc[row, 3])\n    padding = MAX_LEN - len(encoding.ids)\n    input_id = [0] + encoding.ids + [2, 2] + senti.ids + [2] + [1] * (padding - 5)\n    attention_mask = [1] * len([0] + encoding.ids + [2, 2] + senti.ids + [2]) + [0] * (padding - 5)\n    selected_tokens = tokenizer.encode(train.iloc[row, 2]).tokens\n    tok_start = 0\n    for tok_s in selected_tokens:\n        for i, tok_t in enumerate(text_tokens):\n            if tok_t == tok_s:\n                tok_start = i + 1\n                break  \n        break\n    tok_end = tok_start + len(selected_tokens) - 1\n    start_tok, end_tok = [0] * len(input_id), [0] * len(input_id)\n    start_tok[tok_start] = 1\n    end_tok[tok_end] = 1\n    \n    train_input_ids.append(input_id)\n    train_attention_masks.append(attention_mask)\n    train_start_tokens.append(start_tok)\n    train_end_tokens.append(end_tok)\n    \ntrain_input_ids = np.array(train_input_ids, dtype = \"int32\")\ntrain_attention_masks = np.array(train_attention_masks, dtype = \"int32\")\ntrain_start_tokens = np.array(train_start_tokens, dtype = \"int32\")\ntrain_end_tokens = np.array(train_end_tokens, dtype = \"int32\")\ntrain_token_type_ids = np.zeros((num_rows,MAX_LEN), dtype = \"int32\")\n","80567fc6":"\n\nnum_rows = test.shape[0]\ntest_input_ids = []\ntest_attention_masks = []\n\nfor row in range(num_rows):\n    encoding = tokenizer.encode(test.iloc[row, 1])\n    text_tokens = encoding.tokens\n    senti = tokenizer.encode(test.iloc[row, -1])\n    padding = MAX_LEN - len(encoding.ids)\n    input_id = [0] + encoding.ids + [2, 2] + senti.ids + [2] + [1] * (padding - 5)\n    attention_mask = [1] * len([0] + encoding.ids + [2, 2] + senti.ids + [2]) + [0] * (padding - 5)\n    test_input_ids.append(input_id)\n    test_attention_masks.append(attention_mask)\n    \ntest_input_ids = np.array(test_input_ids, dtype = \"int32\")\ntest_attention_mask = np.array(test_attention_masks, dtype = \"int32\")    \ntest_token_type_ids = np.zeros((num_rows,MAX_LEN), dtype = \"int32\")","31db2b57":"ids = tf.keras.layers.Input((MAX_LEN, ), dtype = tf.int32)\natt = tf.keras.layers.Input((MAX_LEN, ), dtype = tf.int32)\ntoken = tf.keras.layers.Input((MAX_LEN, ), dtype = tf.int32)\n\nconfig = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\nbert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n\nx = bert_model(ids,attention_mask=att,token_type_ids=token)\n# training for starting indices\nx1 = tf.keras.layers.Dropout(0.1)(x[0]) \nx1 = tf.keras.layers.Conv1D(128, 1,padding='same')(x1)\nx1 = tf.keras.layers.Conv1D(1,1)(x1)\nx1 = tf.keras.layers.Flatten()(x1)\nx1 = tf.keras.layers.Activation('softmax')(x1)\n# training for end indices\nx2 = tf.keras.layers.Dropout(0.1)(x[0]) \nx2 = tf.keras.layers.Conv1D(128, 1,padding='same')(x2)\nx2 = tf.keras.layers.Conv1D(1,1)(x2)\nx2 = tf.keras.layers.Flatten()(x2)\nx2 = tf.keras.layers.Activation('softmax')(x2)\n\nmodel = tf.keras.models.Model(inputs=[ids, att, token], outputs=[x1,x2])\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)","39c64669":"model.summary()","8a192ac6":"model.fit([train_input_ids.reshape(train.shape[0], 128), train_attention_masks.reshape(train.shape[0], 128), train_token_type_ids.reshape(train.shape[0], 128)], \n          [train_start_tokens.reshape(train.shape[0], 128), train_end_tokens.reshape(train.shape[0], 128)], epochs=3, batch_size=32) ","331720bc":"preds = model.predict([test_input_ids.reshape(test.shape[0], 128), test_attention_mask.reshape(test.shape[0], 128), test_token_type_ids.reshape(test.shape[0], 128)])","573ddcd9":"preds = []\nfor k in range(test_input_ids.shape[0]):\n    a = np.argmax(preds[0][k])\n    b = np.argmax(preds[1][k])\n    if a>b: \n        st = test.iloc[k,1]\n    else:\n        st = tokenizer.decode(test_input_ids[k][a:b + 1])\n    preds.append(st)","b1aea2ba":"# Preparing train data","0cb18544":"**1.1 Importing Libraries**"}}