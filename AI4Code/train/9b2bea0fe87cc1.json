{"cell_type":{"72cdf6fa":"code","b6e393b6":"code","308fd8c7":"code","4bf5eb15":"code","a675c237":"code","c583a025":"code","118994f0":"code","2ba87172":"code","851facb9":"code","535fecd0":"code","e3524304":"code","6876c673":"code","674e8d68":"code","b2199205":"code","9ec960d0":"code","773e8f09":"code","112123f6":"code","ccd34f9d":"code","d23eda09":"code","4340da37":"code","7a242ad4":"code","578bbf5a":"code","74752a47":"code","ca985c56":"code","03b3fa50":"code","06a3bf67":"code","9c3e3e7f":"code","f7675dd7":"code","138bf05f":"code","a449802b":"code","67e5a22f":"code","5808e15f":"code","0c80c27e":"code","77280f7a":"code","31ae81a5":"code","043006a7":"code","242f6c42":"code","b0e0e48f":"code","af54dab1":"code","300232b2":"code","30d778d7":"code","2baa9aef":"code","88c92f57":"code","6abdd20b":"code","b36f81c4":"code","14f8e6bb":"code","091853e9":"code","3808ed40":"code","f25feca5":"code","95646da5":"code","b2715721":"code","0976fca2":"code","6865be67":"code","9bd13ad2":"code","1187cf97":"code","5712b5c6":"code","c569f00d":"code","40ab3c69":"code","b526da85":"code","5762fe34":"code","6f7699c4":"code","2e8ed2e9":"code","b1d233a3":"code","48256716":"code","31923b8e":"code","2d894487":"code","ac8f4c0c":"code","cfde16a8":"code","ea2459e8":"code","f0fe567e":"code","9828ac5e":"code","e34f03e8":"code","0a33cd2c":"code","5ff4346a":"code","37ee65ec":"code","b97e5f16":"code","00fbf8e4":"code","23c875da":"code","05239680":"code","a46a09d2":"code","f4839761":"code","f726d3c7":"code","eced2597":"code","ae50763c":"code","8a1b94db":"code","b5391b53":"code","9aab1fb2":"code","4b2f43f4":"code","4ac5aab4":"code","59de4059":"code","9f8aaa0c":"code","d34b5eda":"code","1b72e0cd":"code","5038aaa7":"code","48af1131":"code","07745aaf":"code","88ac87cc":"code","efe02195":"code","4664236d":"code","5c3b46f4":"code","d0fc7842":"code","223d7197":"markdown","55819986":"markdown","4fea2318":"markdown","cdb23440":"markdown","73934e86":"markdown","a1f2f79b":"markdown","8361e260":"markdown","0ab2ed32":"markdown","5b0d5006":"markdown","5b5fae1b":"markdown","307051d6":"markdown","eca127dc":"markdown","efe5620b":"markdown","541aab38":"markdown","597f3091":"markdown","7d7ae5dd":"markdown","57fd0237":"markdown","7e510176":"markdown","f6ef1365":"markdown","4b4f58cd":"markdown","ccbe5723":"markdown","4ce20d76":"markdown","f5be359f":"markdown"},"source":{"72cdf6fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6e393b6":"import warnings\nwarnings.filterwarnings('ignore')","308fd8c7":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import LSTM, Embedding,BatchNormalization, Dense, TimeDistributed, Dropout, Bidirectional, Flatten, GlobalMaxPool1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report,accuracy_score","4bf5eb15":"# Defining the global variables for the color schemes we will incorporate\npblue = \"#496595\"\npb2 = \"#85a1c1\"\npb3 = \"#3f4d63\"\npg = \"#c6ccd8\"\npb = \"#202022\"\npbg = \"#f4f0ea\"\n\npgreen = px.colors.qualitative.Plotly[2]","a675c237":"df = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv')\ndf.head()","c583a025":"df.shape","118994f0":"df.isnull().sum()","2ba87172":"df.dtypes","851facb9":"df.isnull().sum()","535fecd0":"df.dropna(axis=1, inplace=True)\ndf.head()","e3524304":"df.rename(columns={\"v1\":\"label\", \"v2\":\"text\"}, inplace=True)\ndf.head()","6876c673":"# Finding maximum length of text message\n\nnp.max(df['text'].apply(lambda x: len(x.split())).values)","674e8d68":"# Checking balance of dataset\ngrouped_df = df.groupby('label').count().values.flatten()\ngrouped_df","b2199205":"fig = go.Figure()\n\nfig.add_trace(go.Bar(\n        x=['ham'],\n        y=[grouped_df[0]],\n        name='Safe',\n        text=[grouped_df[0]],\n        textposition='auto',\n        marker_color=pblue\n)\n             )\nfig.add_trace(go.Bar(\n        x=['spam'],\n        y=[grouped_df[1]],\n        name='Spam',\n        text=[grouped_df[1]],\n        textposition='auto',\n        marker_color=pg\n))\n\nfig.update_layout(\n    title='Class distribution in the dataset')\n\nfig.show()","9ec960d0":"# Creating series with length as index\n# Sorting the series by index i.e. length\nlen_df_ham = df[df['label']=='ham'].text.apply(lambda x: len(x.split())).value_counts().sort_index()\nlen_df_spam = df[df['label']=='spam'].text.apply(lambda x: len(x.split())).value_counts().sort_index()","773e8f09":"len_df_ham","112123f6":"len_df_spam","ccd34f9d":"# X-axis consists of the length of the msgs\n# Y-axis consists of the frequency of those lengths\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(\nx=len_df_ham.index,\ny=len_df_ham.values,\nname='Safe',\nfill='tozeroy',\nmarker_color=pblue))\n\nfig.add_trace(go.Scatter(\nx=len_df_spam.index,\ny=len_df_spam.values,\nname='Spam',\nfill='tozeroy',\nmarker_color=pg\n))\n\nfig.update_layout(\n    title='Frequency of SMS lengths')\nfig.update_xaxes(range=[0, 80])\nfig.show()","d23eda09":"def cleaning(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","4340da37":"df['text']","7a242ad4":"df['text'] = df['text'].apply(cleaning)\ndf['text']","578bbf5a":"# Removing stop words\nstop_words = stopwords.words('english')\nmore = ['u', 'im', 'c']\nstop_words = stop_words + more\n\n\ndef sw_rem(text):\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    return text\n\ndf['text'] = df['text'].apply(sw_rem)\ndf['text']","74752a47":"stems = nltk.SnowballStemmer('english')\n\ndef stemming(text):\n    text = ' '.join(stems.stem(word) for word in text.split())\n    return text","ca985c56":"df['text'] = df['text'].apply(stemming)\ndf.head()","03b3fa50":"# Creating a pipeline\n\ndef pipeline(text):\n    text = cleaning(text)\n    text = ' ' .join(word for word in text.split(' ') if word not in stop_words)\n    text = ' '.join(stems.stem(word) for word in text.split(' '))\n    return text","06a3bf67":"df['text'] = df['text'].apply(pipeline)\ndf.head()","9c3e3e7f":"# Encoding the categorical target variable\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(df['label'])\n\ndf['label_num'] = le.transform(df['label'])\ndf.head()","f7675dd7":"# This will combine all the text values for safe sms\n#' '.join(text for text in df[df['label']=='ham'].text)","138bf05f":"# Extracting the twitter word cloud mask\ntwitter_mask = np.array(Image.open('\/kaggle\/input\/masksforwordclouds\/twitter_mask3.jpg'))\n\nwc = WordCloud(background_color='white', max_words=200, mask=twitter_mask)\n\nwc.generate(' '.join(text for text in df[df['label']=='ham'].text))\nplt.figure(figsize=(15, 10))\nplt.title('Top words for safe messages', fontdict={'size':22})\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","a449802b":"# Extracting the twitter word cloud mask\nwc = WordCloud(background_color='white', max_words=200, mask=twitter_mask)\n\nwc.generate(' '.join(text for text in df[df['label']=='spam'].text))\nplt.figure(figsize=(15, 10))\nplt.title('Top words for Spam messages', fontdict={'size':22})\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","67e5a22f":"x = df['text']\ny = df['label_num']\n\nlen(x), len(y)","5808e15f":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y)\nprint(len(x_train), len(y_train))\nprint(len(x_test), len(y_test))","0c80c27e":"# First working with count vectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# instantiate the vectorizer\ncount = CountVectorizer()\ncount.fit(x)\n\nx_train_num = count.transform(x_train)\nx_test_num = count.transform(x_test)","77280f7a":"# Example of a tuned model\ncount_tuned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)","31ae81a5":"# Working with TF-IDF now\nfrom sklearn.feature_extraction.text import TfidfTransformer\n# We are using transformer here\n# If we use vectorizer, we can directly use the text\ntfidf = TfidfTransformer()\n\ntfidf.fit(x_train_num)\nx_train_tfidf = tfidf.transform(x_train_num)\n\nx_train_tfidf","043006a7":"text = df['text']\nlabel = df['label_num']","242f6c42":"# Calculating the total vocabulary\ntk = Tokenizer()\ntk.fit_on_texts(text)\n\nvocab = len(tk.word_index)+1\nvocab","b0e0e48f":"# Maximum length\nmax_len = np.max(df['text'].apply(lambda x: len(x.split())).values)\nmax_len","af54dab1":"text","300232b2":"def embedding(text):\n    return tk.texts_to_sequences(text)\n\ntrain_padded = pad_sequences(embedding(text), 80, padding='post')\ntrain_padded","30d778d7":"# Using our helper functions for GloVe\n\nembedding_dict = dict()\nembedding_dim = 100\n\n# Each word is represented in one line in the text file\n# Format - Word val1 val2 val3......val-n for n-dimension vector space\n\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt') as fp:\n    for line in fp.readlines():\n        records = line.split()\n        word = records[0]\n        vector = np.asarray(records[1:], dtype='float32')\n        embedding_dict[word] = vector","2baa9aef":"# Creating a matrix for each word as index (word numerical value extracted from tokenizer\n# with N-features (corresponding to GloVe)\n# We will replace the matrix elements by the words and their embeddings\n\n# Our embeddings will also consist embeddings for padding\nembedding_matrix = np.zeros((vocab, embedding_dim))\n\nfor word, index in tk.word_index.items():\n    embed_vector = embedding_dict.get(word)\n    if embed_vector is not None:\n        embedding_matrix[index] = embed_vector\n        \nembedding_matrix","88c92f57":"# We will be creating seaborn and plotly confusion matrices\nimport plotly.figure_factory as ff\nx_axes = ['Safe','Spam']\ny_axes = ['Spam', 'Safe']\n\ndef conf_matrix(z, x=x_axes, y=y_axes):\n    z = np.flip(z, 0)\n    # Change each element of z to string \n    # This allows them to be used as annotations\n    z_str = [[str(y) for y in x] for x in z]\n    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_str)\n    \n    fig.update_layout(title_text='Confusion matrix', xaxis=dict(title='Predicted Value'),\n                     yaxis=dict(title='Real value'))\n    \n    fig['data'][0]['showscale'] = True\n    return fig","6abdd20b":"from sklearn.metrics import confusion_matrix\ncategories=['Safe', 'Spam']\ndef seaborn_conf(y, ypred):\n    y_true = [\"Safe\", \"Spam\"]\n    y_pred = [\"Safe\", \"Spam\"]\n    cf = confusion_matrix(y, ypred)\n    df_cm = pd.DataFrame(cf, columns=np.unique(y_true), index = np.unique(y_true))\n    plt.figure(figsize=(8,6))\n    sns.heatmap(df_cm, annot=True, fmt='g')\n    plt.title('Confusion matrix')\n    plt.xlabel('Predicted value')\n    plt.ylabel('Real value')\n    plt.show()","b36f81c4":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n\n# Train the model - CountVectorizer model\nnb.fit(x_train_num, y_train)","14f8e6bb":"# Class and probability predictions\nyp_class = nb.predict(x_test_num)\nyp_prob = nb.predict_proba(x_test_num)[:, 1]","091853e9":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test, yp_class))\nseaborn_conf(y_test, yp_class)","3808ed40":"metrics.roc_auc_score(y_test, yp_prob)","f25feca5":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('bow', CountVectorizer()), \n                 ('tfid', TfidfTransformer()),  \n                 ('model', MultinomialNB())])","95646da5":"pipe.fit(x_train, y_train)\nyp_class = pipe.predict(x_test)\nprint(metrics.accuracy_score(y_test, yp_class))\nseaborn_conf(y_test, yp_class)","b2715721":"import xgboost as xgb\npipe = Pipeline([\n    ('bow', CountVectorizer()), \n    ('tfid', TfidfTransformer()),  \n    ('model', xgb.XGBClassifier(\n        learning_rate=0.1,\n        max_depth=6,\n        n_estimators=90,\n        use_label_encoder=False,\n        eval_metric='auc'\n    ))\n    ]\n)","0976fca2":"pipe.fit(x_train, y_train)\nyp_class_test = pipe.predict(x_test)\nyp_class_train = pipe.predict(x_train)\n\nprint('Training accuracy score: {}'.format(metrics.accuracy_score(y_train, yp_class_train)))\nprint('Testing accuracy score: {}'.format(metrics.accuracy_score(y_test, yp_class_test)))\n\nseaborn_conf(y_test, yp_class_test)","6865be67":"train_padded.shape","9bd13ad2":"x_train, x_test, y_train, y_test = train_test_split(train_padded, label, test_size=0.2)","1187cf97":"model = Sequential()\nmodel.add(Embedding(input_dim=embedding_matrix.shape[0], \n                   output_dim=embedding_matrix.shape[1],\n                   weights=[embedding_matrix],\n                   input_length=max_len\n                   )\n         )\nmodel.add(Bidirectional(LSTM(max_len, return_sequences=True, recurrent_dropout=0.15)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(max_len, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(max_len, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","5712b5c6":"# Defining Callbacks\n# Checkpoints in case our model stops training due to some circumstance - saving progress\ncheckpoints = ModelCheckpoint('ck_model.h5', monitor='val_loss', verbose=1, save_best_only=True)\n# Reducing the learning rate if no improvement in validation loss over 5 epochs\n# This is to train the model better\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', vactor=0.1, verbose=1, patience=5, min_lr=0.0001)","c569f00d":"history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test), verbose=1, callbacks=[reduce_lr, checkpoints])","40ab3c69":"# Plotting the results\ndef learning_curve(history, arr):\n    fig, ax=plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]])\n        ax[idx].set_xlabel('Epochs')\n        ax[idx].set_ylabel('Value')\n        ax[idx].set_title(arr[idx][0]+' X '+ arr[idx][1])","b526da85":"learning_curve(history, [['loss', 'val_loss'], ['accuracy', 'val_accuracy']])","5762fe34":"yp = model.predict(x_test)\nyp","6f7699c4":"yp = (model.predict(x_test)>0.5).astype('int32')\nyp","2e8ed2e9":"seaborn_conf(y_test, yp)","b1d233a3":"!pip install transformers","48256716":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer","31923b8e":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\ndef bert_encode(data, maximum_length):\n    input_ids=[]\n    attention_masks=[]\n    for text in data:\n        encoded = tokenizer.encode_plus(text, add_special_tokens=True, max_length = maximum_length, pad_to_max_length=True, return_attention_mask=True)\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids), np.array(attention_masks)","2d894487":"np.max(df['text'].apply(lambda x: len(x.split())).values)","ac8f4c0c":"bt_text = df['text']\nbt_label = df['label_num']\n\nbt_ids, bt_masks = bert_encode(bt_text, 80)","cfde16a8":"from transformers import TFBertModel\ndef create_model(bert_model):\n    \n    input_ids = tf.keras.Input(shape=(80,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(80,),dtype='int32')\n\n    output = bert_model([input_ids,attention_masks])\n    output = output[1]\n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","ea2459e8":"bert_model = TFBertModel.from_pretrained('bert-base-uncased')","f0fe567e":"model = create_model(bert_model)\nmodel.summary()","9828ac5e":"history = model.fit([bt_ids, bt_masks], bt_label, validation_split=0.25, epochs=3, batch_size=10)","e34f03e8":"learning_curve(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","0a33cd2c":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n\ndf_train = df_train.dropna(axis=1)\ndf_train.head()","5ff4346a":"df.groupby('target').count()","37ee65ec":"# We can use .agg('count').values also\nclass_counts = df.groupby('target').id.count().values\nclass_counts","b97e5f16":"fig = go.Figure()\n\nfig.add_trace(go.Bar(\n        x=['Fake disaster'],\n        y=[class_counts[0]],\n        name='Fake',\n        text=[class_counts[0]],\n        textposition='auto',\n        marker_color=pblue\n)\n             )\nfig.add_trace(go.Bar(\n        x=['Real disaster'],\n        y=[class_counts[1]],\n        name='Real',\n        text=[class_counts[1]],\n        textposition='auto',\n        marker_color=pg\n))\n\nfig.update_layout(\n    title='Class distribution in the dataset')\n\nfig.show()","00fbf8e4":"len_real = df[df['target']==1].text.apply(lambda x: len(x.split())).value_counts().sort_index()\nlen_fake = df[df['target']==0].text.apply(lambda x: len(x.split())).value_counts().sort_index()","23c875da":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=len_real.index,\n    y=len_real.values,\n    name='Real disaster',\n    fill='tozeroy',\n    marker_color=pblue,\n))\nfig.add_trace(go.Scatter(\n    x=len_fake.index,\n    y=len_fake.values,\n    name='Fake disaster',\n    fill='tozeroy',\n    marker_color=pg,\n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Data Roles in Different Fields<\/span>'\n)\nfig.show()","05239680":"def remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\n        'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n        '', \n        text\n    )\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    text = remove_url(text)\n    text = remove_emoji(text)\n    text = remove_html(text)\n    \n    return text","a46a09d2":"# Testing the function\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","f4839761":"stopw = stopwords.words('english')\nmore = ['u', 'im', 'c']\nstopw = stopw + more\n\nstemmer = nltk.SnowballStemmer('english')\n\ndef data_cleaning(text):\n    text = clean_text(text)\n    text = ' '.join(stemmer.stem(word) for word in text.split(' ') if word not in stopw)\n    return text","f726d3c7":"df['cleaned_text'] = df['text'].apply(data_cleaning)\ndf_test['cleaned_text'] = df_test['text'].apply(data_cleaning)","eced2597":"df.head()","ae50763c":"def corpus(df, label):\n    corpus=[]\n    for x in df[df['target']==label]['cleaned_text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","8a1b94db":"corpus_reald = corpus(df, 1)\ndic = defaultdict(int)\n\n# Creating a dictionary with frequency of words\nfor word in corpus_reald:\n    dic[word]+=1\n    \n# Sorting words by descending frequency\ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\ntop","b5391b53":"twitter_mask = np.array(Image.open('\/kaggle\/input\/masksforwordclouds\/twitter_mask3.jpg'))\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=twitter_mask,\n)\nwc.generate(' '.join(text for text in df.loc[df['target'] == 1, 'cleaned_text']))\nplt.figure(figsize=(18,10))\nplt.title('Wordcloud for real disasters', \n          fontdict={'size': 22,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","9aab1fb2":"corpus_faked = corpus(df, 0)\ndic = defaultdict(int)\n\n# Creating a dictionary with frequency of words\nfor word in corpus_faked:\n    dic[word]+=1\n    \n# Sorting words by descending frequency\ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\ntop","4b2f43f4":"twitter_mask = np.array(Image.open('\/kaggle\/input\/masksforwordclouds\/twitter_mask3.jpg'))\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=twitter_mask,\n)\nwc.generate(' '.join(text for text in df.loc[df['target'] == 0, 'cleaned_text']))\nplt.figure(figsize=(18,10))\nplt.title('Wordcloud for fake disasters', \n          fontdict={'size': 22,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","4ac5aab4":"x = df['cleaned_text']\ny = df['target']\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y)\nlen(x_train), len(y_train), len(x_test), len(y_test)","59de4059":"pipe = Pipeline([\n    ('bow', CountVectorizer()), \n    ('tfid', TfidfTransformer()),  \n    ('model', xgb.XGBClassifier(\n        use_label_encoder=False,\n        eval_metric='auc',\n    ))\n])\n\npipe.fit(x_train, y_train)\nyp_test = pipe.predict(x_test)\nyp_train = pipe.predict(x_train)\n\nprint('Training accuracy: {}'.format(metrics.accuracy_score(y_train, yp_train)))\nprint('Testing accuracy: {}'.format(metrics.accuracy_score(y_test, yp_test)))\n\nseaborn_conf(y_test, yp_test)","9f8aaa0c":"training = df['cleaned_text'].values\ntesting = df_test['cleaned_text'].values\n# Target labels\nlabels = df['target'].values","d34b5eda":"# Word tokenizer\ntk = Tokenizer()\ntk.fit_on_texts(training)\n\nvocab = len(tk.word_index)+1\nvocab","1b72e0cd":"def metric_calculation(y_test, y_pred):\n    print(\"F1-score: \", f1_score(y_pred, y_test))\n    print(\"Precision: \", precision_score(y_pred, y_test))\n    print(\"Recall: \", recall_score(y_pred, y_test))\n    print(\"Acuracy: \", accuracy_score(y_pred, y_test))\n    print(\"-\"*50)\n    print(classification_report(y_pred, y_test))\n    \ndef embeddings(corpus): \n    return tk.texts_to_sequences(corpus)","5038aaa7":"len_train = np.max(df['cleaned_text'].apply(lambda x: len(x)))\nlen_train","48af1131":"train_padded_sentences = pad_sequences(\n    embeddings(training), \n    len_train, \n    padding='post'\n)\ntest_padded_sentences = pad_sequences(\n    embeddings(testing), \n    len_train,\n    padding='post'\n)\n\ntrain_padded_sentences","07745aaf":"# As we've already created a GloVe dictionary in the SMS-dataset, we will start with the matrix\n\nembedding_matrix = np.zeros((vocab, embedding_dim))\n\nfor word, index in tk.word_index.items():\n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        \nembedding_matrix","88ac87cc":"x_train, x_test, y_train, y_test = train_test_split(train_padded_sentences, labels, test_size=0.20)","efe02195":"model = Sequential()\nmodel.add(Embedding(input_dim=embedding_matrix.shape[0], \n                   output_dim=embedding_matrix.shape[1],\n                   weights=[embedding_matrix],\n                   input_length=max_len\n                   )\n         )\nmodel.add(Bidirectional(LSTM(max_len, return_sequences=True, recurrent_dropout=0.15)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(max_len, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(max_len, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","4664236d":"# Callbacks\ncheckpoint = ModelCheckpoint('model.h5', monitor = 'val_loss', verbose = 1, save_best_only = True)\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, verbose = 1, patience = 5,                        min_lr = 0.001)\n\nhistory = model.fit(x_train, y_train, epochs = 7,batch_size = 32,validation_data = (x_test, y_test),verbose = 1,callbacks = [reduce_lr, checkpoint])","5c3b46f4":"learning_curve(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","d0fc7842":"preds = model.predict_classes(x_test)\nmetric_calculation(preds, y_test)","223d7197":"The 'text' property is a string and must be specified as:\n\n      - A string\n      - A number that will be converted to a string\n      - A tuple, list, or one-dimensional numpy array \n      \nThe 'x' and 'y' property is an array that may be specified as a tuple,\n    list, numpy array, or pandas Series","55819986":"<h2>XGBoost<\/h2>","4fea2318":"<h2>Model creation and prediction<\/h2>\n\nWe will first start with the **naive bayes classifier** which comes from a family of simple \"probabilistic classifiers\" based on application of Bayes theroem with strong independent assumptions between features.\n\nThe model is highly scalable, with number of parameters being linear with number of variables. ","cdb23440":"<h2>Data pre-processing and cleaning<\/h2>","73934e86":"# Introduction\n\nThis notebook will focus on text-classification and sentiment analysis. We will go through all major NLP and dat analysis techniques, some of which include:\n\n* LSTMs\n* Transformers (such as BERT)\n* Naive Bayes\n* XGBoost \n\nand much more...\n\nThe first half of the notebook is focused on cleaning and pre-processing the data, while the second half builds and compares different models with the techniques mentioned above.","a1f2f79b":"<h2>Working with Naive Bayes + TF-IDF<\/h2>","8361e260":"<h2>GloVe Embeddings<\/h2>\n\nThese embeddings are based on the principle that we can derive sematic relationships between words from their co-occurence matrix. This embedding focuses on words co-occurrences over the whole corpus. \n\nThey are a form of word representation that try to merge human understanding of languages into their structure. They have a learned representation in an n-dimension space, where words with similar meanings have similar embeddings. Two similar words are represented by almost similar vectors that are at a small distance in the vector space.\n\nWhen using a vector space, all the words are represented as vectors in a predefined N-dimension vector space. Each word is mapped to a vector and the vector values are learned in a way that resembles a neural network.","0ab2ed32":"<h3>GloVE - LSTM<\/h2>","5b0d5006":"<h2>Working with Embeddings - GloVe<\/h2>","5b5fae1b":"The CountVectorizer model can be tuned in a variety of ways:\n\n* Stop words - Extremely common words can be omitted by the model by setting this parameter to the language corresponding to the text.\n\n* ngram_range - It pairs up words together as features. If we consider bigrams and we have a sentence \"I am happy\", we will have two features - [\"I am\", \"am happy\"]. We can define a range of ngrams, so if we have the same sentence with a range from 1 to 2, our features will be:  `[\"I\", \"am\", \"happy\", \"I am\", \"am happy\"]`. This increase is features helps to fine tune the model.\n\n* min_df, max_df - Minimum and maximum frequencies of words of n-grams that can be used as features. If either of the conditions are not met, the feature will be omitted.\n\n* max_features - Choose the most frequent words and drop everything else.","307051d6":"<h2>Transformers - BERT<\/h2>","eca127dc":"We can see that the safe SMS messages are much shorter than the spam messages.","efe5620b":"<h2>Visualizing tokens<\/h2>","541aab38":"<h2>Data preprocessing<\/h2>","597f3091":"<h2>Vectorization<\/h2>\n\nWe currently have each text record in string format. We need to convert each of those records into a vector that our models can work with. We will first do this using the bag-of-words model.\n\nWe will use two major approaches here\n\n* **CountVectorizer** - Working on frequency of each word in the given string.\n\n* **Term frequency-inverse document frqeuency TFIDF** - Works on frequency divided by the appearance of the given word in the total documents.","7d7ae5dd":"BERT has revolutionized the world of NLP by providing state-of-the-art results on many NLP tasks. BERT stands for Bidirectional Encoder Representation from Transformer. It is the state-of-the-art embedding model published by Google. It has created a major breakthrough in the field of NLP by providing greater results in many NLP tasks, such as question answering, text generation, sentence classification, and many more besides. One of the major reasons for the success of BERT is that it is a context-based embedding model, unlike other popular embedding models, such as word2vec, which are context-free.","57fd0237":"<h2>LSTMs and GloVE embeddings<\/h2>","7e510176":"<h2>Exploratory data analysis<\/h2>","f6ef1365":"<h2>Stemming and lemmatization<\/h2>\n\nDocuments and other forms of text use different forms of the same words, such as play, playing, played. There are families of derivationally related words that have similar meanings. Our main task with stemming and lemmatization is to reduce all these derived words into the parent\/family word, therefore reducing the total vocabulary while retaining information.\n\n* **Stemming** - Omits the ends of words to achieve the goal correctly, this works **most of the times** and can also remove the derivational suffix\n\n* **Lemmatization** - Working with a vocabulary and morphological analysis of wrods, removing inflectional endings only and returning the base and dictionary form of a word.\n\nAs we do not require much emphasis on words, we will focus more on stemming than lemmatization,.\n\n<h3>Stemming algorithms<\/h3>\n\nWe have multiple algorithms to achieve our stemming goals, some of them are as follows:\n\n* PorterStemmer - Fast and efficient. Strips off the end (suffix) to produce the stems. It does not follow linguistics but rather a set of 05 rules for diferent cases. \n\n* SnowballStemmer - Generate a set of rules for any language. These are useful for non-english stemming tasks.\n\n* LancasterStemmer - Iterative algorithm, uses about 120 rules, it tries to find an applicable rule by the last character of each word. The last character may be omitted or replaced.","4b4f58cd":"Now we will proceed with converting the text to numerical values and also padding the vectors so each of them are of equal length. ","ccbe5723":"<h2>Model creation and testing<\/h2>","4ce20d76":"<h2>WordCloud analysis<\/h2>","f5be359f":"<h2>Working with disaster tweets - Dataset<\/h2>"}}