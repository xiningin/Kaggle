{"cell_type":{"5e64c5f8":"code","b7062e81":"code","ce4bac4c":"code","cd7b4c26":"code","3c57bb69":"code","f5a3b9b1":"code","8b4a2495":"code","ba64763a":"code","b1ceaf86":"code","1d11770d":"code","d7728cbc":"code","78b6060d":"code","d40dd784":"code","657d1b21":"code","524fe626":"code","45d40a49":"code","1cb84693":"code","a0efff71":"code","d9f859c8":"code","958f87dc":"code","6bae6595":"code","4af3d9d7":"code","518e8b44":"code","82b0b825":"code","962de572":"code","3711ab43":"code","9506fca9":"code","dc3fdf62":"code","8c4568a5":"code","d235d557":"code","bfa65547":"code","918157b7":"code","96d04ab7":"code","64c09b28":"code","87d31286":"code","7231a813":"code","fc1ac736":"code","a2086fd6":"markdown","83d1f56a":"markdown","3a84732c":"markdown","19ac4701":"markdown","18c48e27":"markdown","a09f11e8":"markdown","98f03b5d":"markdown","7598d03c":"markdown","b03a0017":"markdown","c382fd1d":"markdown","103c67b2":"markdown","d19942ca":"markdown","b66ef845":"markdown","dd61a565":"markdown","d41cbf94":"markdown","34939583":"markdown","da52f853":"markdown","b92f880f":"markdown","caf31436":"markdown"},"source":{"5e64c5f8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b7062e81":"ratings = pd.read_csv(\"\/kaggle\/input\/ratings-pca\/Ratings.csv\")\nratings","ce4bac4c":"ratings.info()","cd7b4c26":"ratings.describe()","3c57bb69":"plt.figure(figsize=(7,5))\nsns.heatmap(ratings.corr(),annot=True,cmap='coolwarm')\nplt.show()","f5a3b9b1":"# Step1: Covariance Matrix Computation\ncovv = np.cov(ratings.T)\ncovv\nsns.heatmap(covv)","8b4a2495":"plt.figure(figsize=(7,5))\nsns.heatmap(ratings.corr(),annot=True,cmap='coolwarm')\nplt.title(\"Correlation\")\nplt.show()\nplt.figure(figsize=(7,5))\nsns.heatmap(covv,annot=True,cmap='coolwarm')\nplt.title(\"Covariance\")\nplt.show()","ba64763a":"# Step 2. Eigendecomposition of the Covariance Matrix\n# Then you performed the eigendecomposition of the covariance matrix  to obtain the eigenvalues and the eigenvectors \n\neigenvalues , eigenvectors = np.linalg.eig(covv)\neigenvalues","b1ceaf86":"eigenvectors","1d11770d":"# Step 3: Sort the eigenvectors on the basis of the eigenvalues.\nidx = eigenvalues.argsort()[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:,idx]","d7728cbc":"# Step 4. These eigenvectors are the principal components of the original matrix.\n\neigenvectors","78b6060d":"# Step 5: The eigenvalues denote the amount of variance explained by the eigenvectors.\n# Higher the eigenvalues, higher is the variance explained by the corresponding eigenvector.\neigenvalues","d40dd784":"# Step 6: These eigenvectors are orthonormal,i.e. they're unit vectors and are perpendicular to each other.\n\nnew_rating = np.linalg.inv(eigenvectors) @ ratings.T\nnew_rating = pd.DataFrame(new_rating.T)\nnew_rating","657d1b21":"100 * np.var(new_rating) \/ sum(np.var(new_rating))","524fe626":"#### Let's verify the results using PCA\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)\npca.fit(ratings)\n#Let's check the components\nprint(\"------------------------------------------------------------------------------------------------------------------\")\nprint(\"|Note that the columns in the eigenvector matrix are to be compared with the rows of pca.components_ matrix.\")\nprint(\"|Also the directions are reversed for the second axis.\")\nprint(\"|This wouldn't make a difference as even though they're antiparallel, they would represent the same 2-D space.\")\nprint(\"|For example, X\/Y and X\/-Y both cover the entire 2-D plane\")\nprint(\"------------------------------------------------------------------------------------------------------------------\")\nprint(pca.components_)\n\n\n# Let's check the variance explained\npca.explained_variance_ratio_","45d40a49":"# Importing data\n\n# del train,test,sample\ntrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/sample_submission.csv\")\n\ntrain.head()","1cb84693":"# train.info(verbose=True)\n\nif len(train.select_dtypes('int').columns) == train.shape[1]:\n    print(\"All int values!\")\nelse:\n    print(\"Few non-int values!\")","a0efff71":"# Great news!\n\nif len(train.isnull().sum() == 0) == train.shape[1]:\n    print(\"No null values!\")\nelse:\n    print(\"Null values!\")","d9f859c8":"print(\"Number of Observations are {0} while number of features are {1} \".format(train.shape[0],train.shape[1]))","958f87dc":"# Step1 : To find the covvariance matrix\ncovv = np.cov(train.T)","6bae6595":"covv.shape","4af3d9d7":"# Step2: To find eigenvalues and eigenvectors from covariance matrix\n# eigenvalues and eigenvectors\neigenvalues,eigenvectors = np.linalg.eig(covv)","518e8b44":"idx = eigenvalues.argsort()[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[idx]","82b0b825":"eigenvalues.shape","962de572":"eigenvalues[:2]","3711ab43":"eigenvectors.shape","9506fca9":"# eigenvectors[0,:]","dc3fdf62":"variance_explained = []\nfor i in eigenvalues:\n     variance_explained.append((i\/sum(eigenvalues))*100)\n        \n# print(variance_explained)","8c4568a5":"cumulative_variance_explained = np.cumsum(variance_explained)\nprint(cumulative_variance_explained[:1])\n# cumulative_variance_explained.shape","d235d557":"# Visualizing the eigenvalues and finding the \"elbow\" in the graphic\nplt.plot(cumulative_variance_explained)\nplt.ylim([80,101])\nplt.grid()\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Cumulative explained variance\")\nplt.title(\"Explained variance vs Number of components\")","bfa65547":"digit = np.linalg.inv(eigenvectors) @ train.T\ndigit = pd.DataFrame(digit.T)\ndigit.head()","918157b7":"100 * np.var(digit) \/ sum(np.var(digit))\n# 100 * np.var(new_rating) \/ sum(np.var(new_rating))","96d04ab7":"from sklearn.decomposition import PCA","64c09b28":"pca = PCA( random_state=42)\npca","87d31286":"pca.fit(train)","7231a813":"pca.components_","fc1ac736":"pca.explained_variance_ratio_[0]","a2086fd6":"These eigenvectors( along the columns) denote the Principal components of the original dataset. They're ordered according to their corresponding eigenvalues. \n\nSo higher is the eigenvalue, higher is the variance explained by the eigenvector\/PC and hence it is ordered higher than the eigenvector having a lower eigenvalue.\n\nSignificance of the Spectral Theorem: \n* So as you saw in the above code the reason why following this procedure of eigendecomposition of the covariance matrix hands us the principal components in a platter is due to the `spectral theorem`.\n* Now going through the proof of this theorem would be beyond the scope of the this code. However, a basic understanding of the main algorithm is sufficient for our purposes. So you only have to memorise the following nuances of the Spectral Theorem.","83d1f56a":"**Calculating the covariance matrix**\n- Now I will find the covariance matrix of the dataset by multiplying the matrix of features by its transpose. It is a measure of how much each of the dimensions varies from the mean with respect to each other. Covariance matrices, like correlation matrices, contain information about the amount of variance shared between pairs of variables.\n<img src=\"https:\/\/miro.medium.com\/max\/473\/0*Nw5vyMDOnUm4JJCj\">","3a84732c":"# Sample Data to build PCA from scratch\n\n### PCA - Algorithm\n    In this segment, you'll get to learn about the algorithm through which PCA works. Originally PCA used the eigendecomposition route in\n    finding the principal components. However, much faster algorithms like SVD have come up which are predominantly used nowadays.\n    However, one thing to note here is that SVD is actually a generalized procedure of eigendecomposition. Therefore, both of them will\n    be having some key similarities which we'll see in the later segments.","19ac4701":"#### This is what PCA does. It doesn't change the total variance of the dataset. It only rearranges them in the direction of maximum variances","18c48e27":"## 2. Importing Ratings Dataset","a09f11e8":"### Spectral theorem\n\n**Keypoints** : Thus the spectral theorem states that\n\n1. When you do the eigendecomposition of the covariance matrix, the corresponding eigenvectors would be the principal components of the original matrix.\n\n2. These eigenvectors would be orthonormal to each other and hence they follow the property that PCs need to be perpendicular to each other.\n\n3. They would also be in an ordered fashion - the eigenvalues would dictate the variance explained by that principal component and hence ordering the matrix according to the eigenvalues would give us the resultant principal component matrix.\n\n4. These eigenvectors would also be the linear combinations of the original variables as well.\n\nLink: https:\/\/stats.stackexchange.com\/questions\/2691\/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues.","98f03b5d":"**Eigenvectors are the principal components.**\n- The first principal component is the first column with values of 0.000802331972,0.000499446269. The second principal component is the second column and so on. Each Eigenvector will correspond to an Eigenvalue, each eigenvector can be scaled of its eigenvalue, whose magnitude indicates how much of the data\u2019s variability is explained by its eigenvector.","7598d03c":"### Eigenvectors and Eigendecomposition\n\nA special kind of linear transformation that can happen to a vector is that when a matrix is multiplied to it, it only manages to stretch the vector by a certain scalar magnitude. These vectors are known as eigenvectors for that particular matrix and that scalar magnitude is known as the corresponding eigenvalue. Formally, they can be written as follows,\n\nAv = \u03bbv , where A is the original matrix \/ linear transformation, v is the eigenvector and \u03bb is the corresponding eigenvalue.","b03a0017":"**`Covariance` indicates the direction of the linear relationship between variables.\n`Correlation` on the other hand measures both the strength and direction of the linear relationship between two variables.**\n`Correlation is a function of the covariance`\n\nNote: Correlation is the standardized value of covariance","c382fd1d":"**Because Spectral Theorem exists! Because of this theorem eigendecomposition of the covariance matrix will always:**\n1. Yield the eigenvectors which are perpendicular to each other \n2. Have maximum variances allocated to them in an ordered way depending on the magnitude of the eigenvalues.","103c67b2":"# I am creating an index for easy reference to the topics:\n\n**Baic concepts included**\n\n### Definition of PCA\n\n1. Importing modules.\n2. Importing Ratings data.\n3. Implementing PCA using numpy on Ratings dataset.\n4. Implementing PCA using numpy on Digit Recognizer.\n5. Implementing PCA using scikit-learn Algorithm on Digit Recognizer.\n\n**Exercise: Few Questions\/Answer to test PCA skills**","d19942ca":"## 3. Implementing PCA using numpy on Ratings dataset.","b66ef845":"## 1. Importing Modules","dd61a565":"**Give some examples where PCA implementation is highly useful?**\n- Image segmentation,  Recommender Systems, etc. can be explained here.\n\n**What is the first step you do before applying PCA and why(assume that the data is clean and all the values are numerical in nature)?**\n- Scale and standardise the data so that the variables of differing magnitudes fall in the same range.\n\n**Explain the significant shortcomings of performing PCA.**\n- PCA is a linear method, however, in some situations, non-linear methods can produce better results. PCA also requires the data to be highly correlated for it to create reasonable results. Then, PCA produces components which are orthogonal and uncorrelated, but sometimes, correlated components can be the better choice. PCA assumes that lower types of variances aren't useful. Hence it may lead to loss of valuable classes or variables in supervised learning procedures.\n\n**Explain some practical considerations before conducting PCA.**\n* Most software packages use SVD to compute the components and assume that the data is scaled and centred, so it is important to do standardisation\/normalisation\n* PCA is a linear transformation method and works well in tandem with linear models such as linear regression, logistic regression etc., though it can be used for computational efficiency with non-linear models as well\n* It should not be used forcefully to reduce dimensionality (when the features are not correlated)\n\n**Elaborate on the necessity of PCA with respect to a logistic regression model and basic EDA.**\n- A logistic regression setting where you have a lot of correlated variables (high multicollinearity) is modelled by doing a variable selection (stepwise\/forward\/backward). But everytime a variable is dropped, you might be losing important information. Similarly, when performing EDA on a dataset, we need to observe a lot of plots as the number of variables increase. PCA intelligently resolves both these issues by not only reducing the number of dimensions required for analysis but also retaining the maximum variance information.\n\n**While doing a PCA analysis on a dataset containing only 2 variables, the first principal component comes out to be -1i + 2j. Then the second principal component must be**\n- 2i + j","d41cbf94":"### Basis Vectors\n\nSo if you actually have a set of vectors where no matter what other vector you pick, it can always be represented as a linear combination of that set, then it is known as the basis vectors for that data-space or dimension. For example the vectors (2,3) and (3,4) can represent any other vector in 2-D as a linear combination of themeselves and hence they're a set of basis vectors for the 2-D space.","34939583":"### **The steps involved in the eigendecomposition algorithm are as follows:**\n\n1. From the original matrix that you have, you compute its covariance matrix C. (You can read about the covariance matrix)\n    - https:\/\/www.itl.nist.gov\/div898\/handbook\/pmc\/section5\/pmc541.htm\n2. After computing the covariance matrix, you do the eigendecomposition and find its eigenvalues and eigenvectors\n3. Sort the eigenvectors on the basis of the eigenvalues.\n4. These eigenvectors are the principal components of the original matrix.\n5. The eigenvalues denote the amount of variance explained by the eigenvectors. Higher the eigenvalues, higher is the variance explained by the corresponding eigenvector.\n6. These eigenvectors are orthonormal,i.e. they're unit vectors and are perpendicular to each other.","da52f853":"### Definition of PCA: Principal Component Analysis\n\nPCA is the statistical procedure to correct observations of possible highly correlated varibales into `Principal Components` that are:\n1. They're weighted linear combinations of the original variables.\n2. They're perpendicular \/ Independent to each other. \n3. They capture maximum variance of the data and are ordered.","b92f880f":"## 4. Implementing PCA using numpy on Digit Recognizer","caf31436":"## 5. Implementing PCA using scikit-learn on Digit Recognizer"}}