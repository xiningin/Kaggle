{"cell_type":{"cbbbb3c2":"code","09b8f695":"code","feb45b6b":"code","21719f68":"code","c8a89935":"code","e48daf45":"code","864caac3":"code","45b4b9d8":"code","97bcb097":"code","191cf92c":"code","15e0628e":"code","112977f4":"code","a5fd696a":"code","dbc9bdd3":"code","0e909a72":"code","e4efec11":"code","695ed586":"code","a77b13e9":"code","265da9ac":"code","f9022637":"code","fdfcf4d5":"code","f7764a27":"code","5c30974a":"code","2abde3fd":"code","26c11e3c":"code","165f9dfd":"code","f6a6d087":"code","dc2e9df4":"code","9f23794d":"code","8bc7292c":"code","25537ebd":"code","edfdeaec":"code","0bee5c90":"code","fb277f4f":"code","9f023f18":"code","3dcdea2b":"code","e5fd302f":"code","ed469c9f":"code","a76ece93":"code","bd9af260":"code","923e629a":"code","c364c8aa":"code","d7962b9a":"code","d96c7cc8":"markdown","2c4a6cb9":"markdown","d4be1c0e":"markdown","c6cf8615":"markdown","28a261ce":"markdown","3aa2a232":"markdown","1535df3a":"markdown","be515183":"markdown","af004fd1":"markdown","1864e3f1":"markdown","1ec7733e":"markdown","0b353fad":"markdown","af0bb57e":"markdown","d7f5da45":"markdown","5a96283c":"markdown","eb2c01c0":"markdown","6c2b2790":"markdown","183e7b58":"markdown","fe11985d":"markdown","3f60dad8":"markdown","278c4958":"markdown","f9b57140":"markdown","1bcecdb5":"markdown","b427906d":"markdown","91ef614b":"markdown","e01eda2e":"markdown","91fead6b":"markdown"},"source":{"cbbbb3c2":"# Import fastai to use their ULMFiT implementation\nfrom fastai.text import * \n\n# fastai needs us to specify a path sometimes\nfrom pathlib import Path\n\n# Import usual data science libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score","09b8f695":"path = Path('..\/input\/twitter-airline-sentiment\/')\nfile_name = 'Tweets.csv'","feb45b6b":"file_path = path \/ file_name\ndf_full = pd.read_csv(file_path)\ndf_full.size","21719f68":"df_full.sample(10, random_state=0)","c8a89935":"pd.set_option('display.max_colwidth', 0) # tweets aren't too long so let's just print it all","e48daf45":"df = df_full[['airline_sentiment', 'text']]\ndf.sample(10)","864caac3":"df[['airline_sentiment', 'text']].isna().sum()","45b4b9d8":"df['airline_sentiment'].value_counts()","97bcb097":"df['airline_sentiment'].value_counts(normalize=True)","191cf92c":"sns.countplot(y='airline', hue='airline_sentiment', data=df_full)","15e0628e":"import re\nregex = r\"@(VirginAmerica|united|SouthwestAir|Delta|USAirways|AmericanAir)\"\ndef text_replace(s):\n    return re.sub(regex, '@airline', s, flags=re.IGNORECASE)","112977f4":"df['text'] = df['text'].apply(text_replace)","a5fd696a":"df['text'].sample(5)","dbc9bdd3":"train, valid = train_test_split(df, test_size=0.2)","0e909a72":"moms = (0.8,0.7)\nwd = 0.1","e4efec11":"working_path = Path('.\/').resolve() # fastai needs a working path","695ed586":"data_lm = TextLMDataBunch.from_df(working_path, train, valid) # form the data bunch","a77b13e9":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3) # this fetches the wiki103 model\nlearn.freeze()","265da9ac":"learn.lr_find()\nlearn.recorder.plot()","f9022637":"learn.fit_one_cycle(1, 5.0E-02, moms=moms, wd=wd) # 5.0E-02 is LR with the steepest slope above","fdfcf4d5":"learn.unfreeze()","f7764a27":"learn.fit_one_cycle(3, 5.0E-03, moms=moms, wd=wd)","5c30974a":"learn.predict('My flight is great!', n_words=20)","2abde3fd":"learn.save_encoder('ft_enc')","26c11e3c":"train_valid, test = train_test_split(df, test_size=0.2)\ntrain, valid = train_test_split(train_valid, test_size=0.2)","165f9dfd":"data_clas = TextClasDataBunch.from_df(working_path, train, valid, test_df=test, vocab=data_lm.train_ds.vocab, text_cols='text', label_cols='airline_sentiment', bs=32)","f6a6d087":"data_clas.show_batch()","dc2e9df4":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\nlearn.load_encoder('ft_enc')\nlearn.freeze()","9f23794d":"learn.lr_find()\nlearn.recorder.plot()","8bc7292c":"lr = 3.0E-02\nlearn.fit_one_cycle(1, lr, moms=moms, wd=wd)","25537ebd":"learn.freeze_to(-2)\nlr \/= 2\nlearn.fit_one_cycle(1, slice(lr\/(2.6**4), lr), moms=moms, wd=wd)","edfdeaec":"learn.freeze_to(-3)\nlr \/= 2\nlearn.fit_one_cycle(1, slice(lr\/(2.6**4), lr), moms=moms, wd=wd)","0bee5c90":"learn.unfreeze()\nlr \/= 5\nlearn.fit_one_cycle(3, slice(lr\/(2.6**4), lr), moms=moms, wd=wd)","fb277f4f":"learn.predict('I love flying')","9f023f18":"learn.predict('My flight was delayed')","3dcdea2b":"learn.predict(\"Safe flight!\")","e5fd302f":"interp = TextClassificationInterpretation.from_learner(learn)\nacc = accuracy(interp.preds, interp.y_true)\nprint('Accuracy: {0:.3f}'.format(acc))","ed469c9f":"interp.plot_confusion_matrix()","a76ece93":"scores = pd.DataFrame(interp.preds)\nplt.figure(figsize=(12, 12))\nfpr = dict()\ntpr = dict()\nthresh = dict()\nfor i, cls in zip(range(3), ['negative', 'neutral', 'positive']):\n    score = scores[i].apply(lambda x: x.item())\n    y_true = [x.item() == i for x in interp.y_true]\n    fpr[i], tpr[i], thresh[i] = roc_curve(y_true, score, pos_label=True)\n    auc = roc_auc_score(y_true, score)\n    leg = \"AUC: {0:.3f} -- {1}\".format(float(auc), cls)\n    plt.plot(fpr[i], tpr[i], label=leg)\n    \nplt.legend(loc=\"lower right\", prop={'size': 28})\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')","bd9af260":"vc = df['airline_sentiment'].value_counts()\nT = sum(vc) # number of total\nP = vc[0] # number of Positive class\nN = T - P # number of Negative class\n\n# The following were computed from the ROC section. zip(Nfpr, Ntpr) gives a list of coordinates to the negative class ROC curve and Nthresh gives the corresponding thresholds.\nNfpr = fpr[0]\nNtpr = tpr[0]\nNthresh = thresh[0]\n\nnum_pts = len(Nfpr)","923e629a":"# This computes the cost as defined above where the FPR and TPR is given by Nfpr[i] and Ntpr[i]\ndef cost(C, i):\n    return N*Nfpr[i] + C*P*(1-Ntpr[i]), Nthresh[i]","c364c8aa":"min(cost(1, i) for i in range(num_pts))[1]","d7962b9a":"min(cost(2, i) for i in range(num_pts))[1]","d96c7cc8":"## 1.1. Problem Statement","2c4a6cb9":"## 3.3. Fine-tune language model","d4be1c0e":"## 4.2 Confusion matrix","c6cf8615":"Suppose the cost of $FP$ is the same as $FN$, then this next computation shows that the threshold to pick is about $.5$.","28a261ce":"## 4.4 Cost-informed threshold choosing\n\nAs we saw in the last section, there is a trade-off between $TPR$ and $FPR$. In this section, we'll consider a hypothetical business suitation that provides a cost function and we will choose the best threshold given the cost function.\n\nSuppose an airline is interested in possibly taking action upon seeing a negative sentiment tweet to preserve their company image. We will assign negative sentiment tweets the Positive class and positive and neutral sentiment tweets the Negative class. In this suitation a False Positive will cost wasting a small amount of an employee's time and a False Negative will cost them a company image hit. Let $C_{FP}$ and $C_{FN}$ be the costs associated to each False Positives and False Negatives, respectively. Then the cost as a function of $FPR$ and $TPR$ is proportional to \n\n$$C_{FP} \\cdot FPR \\cdot N + C_{FN} \\cdot (1-TPR) \\cdot P, $$\n\nwhere $P, N$ is the number Positive and Negative class elements, respectively. Let $C = C_{FN}\/C_{FP}$, then we see that the cost function is proportional to\n\n$$FPR \\cdot N + C \\cdot (1-TPR) \\cdot P$$\n\n**Caveats:** \n1. If we were actually only interesting in whether something is negative sentiment or not, it would make sense to relabel our data accordingly and retrain the model.\n\n2. The ROC computation done above was computed with plotting in mind so we only have a sampling of ~500 points though our training data has many more points. If this analysis was high priority, it would be worthwhile to investigate computing the ROC from scratch.\n","3aa2a232":"# Twitter Airline Sentiment using ULMFiT","1535df3a":"### 1.2.2 Model training\n\nWe follow the ULMFiT approach of Howard and Ruder found here: https:\/\/arxiv.org\/pdf\/1801.06146.pdf. We will also make extensive use of the `fastai` package as the methods describe in the paper are implemented in this package. The paper discuss applying ULMFiT to a IMDB sentiment problem and provides an example notebook (https:\/\/github.com\/fastai\/fastai\/blob\/master\/examples\/ULMFit.ipynb). That notebook was followed in the creation of this notebook.\n\nFigure 1 of the Howard and Ruder paper outlines the 3 steps of language model transfer learning.\n\n1. **LM pre-training**: This step was done in 3.2 simply by using the builtin `language_model_learner` of `fastai`. This fetches a language model built using the `wiki103`  dataset (derived from Wikipedia articles )by Howard and Ruder. This step had a large computational cost. The goal of trasfer learning and the ULMFiT approach is to take this pre-trained model and fine-tune it to our problem.\n\n2. **LM fine-tuning**: The langugage of Wikipedia is different from that of Twitter so we need to fine-tune the language model to the dataset we're interested in. This was done in section 3.3.\n\n3. **Classifer fine-tuning**: A language model predicts the next word given the beginning of a sentence. This is not what we want. So we replace the last layers with some layers for sentiment classification. This was done in section 3.4.\n\nThe above descrive a fairly generic overview of language model transfer learning. If applied naively, overfitting on the smaller dataset is proned to happen and ''castastrophpic forgetting'' occurs, as described in the paper. The Howard and Ruder paper proposes method to avoid this. They propose 'discriminative fine-tuning' (Discr),  'slanted triangular learning rate' (STLR), and gradual unfreezing. All of these techniques have been implemented and somewhat abstrated away in the `fastai` package. We only have to provide a few parameters.","be515183":"## 3.1. Training - validation split","af004fd1":"## 3.2 Get pre-trained model\n\nThis is done in 3 steps as seen in Figure 1 here: https:\/\/arxiv.org\/pdf\/1801.06146.pdf","1864e3f1":"## 1.2. Our Approach","1ec7733e":"# 3. Model training","0b353fad":"The Twitter US Airlines Sentiment has been provided by Kaggle was originally sourced here: https:\/\/www.figure-eight.com\/data-for-everyone\/. The description reads: \n\n> A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \u201clate flight\u201d or \u201crude service\u201d).","af0bb57e":"## 2.2 Data counts\n\nWe see that the tweet sentiments are heavily skewed towards negative sentiments.","d7f5da45":"Suppose the cost of $FP$ is the same as $FN$, then this next computation shows that the threshold to pick is about $.2$.","5a96283c":"## 4.3 ROC and AUC\n\nNow we plot the one vs. rest ROC curves and give the corresponding AUC values. The one vs. rest ROC curves are a generalization where we choose one class to be the Positive class and combine the rest.\n\nFor each of exposition, we'll consider the ROC curve where the Positive class is the neutral sentiment and the Negative class the is the combination of the positive and negative sentiment. The ROC curve gives True Positive Rate (TPR) as a function of the True Negative Rate (TNR). We are able to vary TNR by choosing the different thresholds for a neutral sentiment to be classified as Positive. In this way, we have a family of classifiers deciding between Positive (neutral) and Negative (positive+negative) given by varying the thresholds. In this next section, we'll consider assign some hypothetical costs that'll inform which classifer out of this family is the ''best''.","eb2c01c0":"## 1.3. Results\n\nThis is a summary of section 4.\n\nWe were able to obtain a final accuracy of 83.7%. But since the data was not balanced, we consider other performance metrics. \n\nIn 4.3, we treat our model as a scorer rather than a classifier which enables us to compute ROC curves by considering different thresholds. We plot the one vs. rest ROC curves and also compute the ROC-AUC. We obtain values of .941, .911, .960 when negative, neutral, and positive were taken as the one in one vs. rest, respectively.\n\nMoreover, in 4.4, we consider adapting our model to a binary classifier where the Positive class is the negative sentiment and the Negative class is the combination of the positive and neutral sentiment. We do this by taking the Positive class whenever a threshold is surpassed. We will determine the optimal threshold given costs assigned to False Positives and False Negatives.","6c2b2790":"### 1.2.1 Data exploration and processing\n\nAfter doing some data exploration, we see that the sentiments are heavily biased towards negative and that the distribution of airline sentiments depends on the specific airline. For example, tweets about Virgin America tend to be more positive than the others.\n\nThe imbalanced data suggests that accuracy should not be our only performance metric. To this end, we will also perform a ROC and AUC analysis on the resulting model.\n\nSince the distribution of airline sentiments depend on the specific airline and each tweet contains `@{airline}`, we will do a bit of pre-processing. We will substitute `@{airline}` with `@airline`, for example, `@united` becomes `@airline`. The goal of the substition is to avoid training the model to determine sentiment using the additional data of which airline the tweet was sent towards.","183e7b58":"## 2. Data Exploration and Preparation","fe11985d":"We will be tackling the **ULMFiT Sentiment** problem from Fellowship.ai which states:\n\n> Apply a supervised or semi-supervised ULMFiT model to Twitter US Airlines Sentiment\n\nThe Twitter US Airlines Sentiment is this dataset: https:\/\/www.kaggle.com\/crowdflower\/twitter-airline-sentiment#Tweets.csv and the ULMFiT model was introduced here: http:\/\/nlp.fast.ai\/classification\/2018\/05\/15\/introducing-ulmfit.html.","3f60dad8":"### 2.3.2 Text substitution\n\nIn light of the dependence of sentiment with airlines, we will do  substitute each instance of `@{airline}` with `@airline`. For example, we will replace `@united` with `@airline`. This does not remove all hints about the airlines from the text (for example, any tweet talking about purple lights is probably Virgin America), but it's still a good first step.","278c4958":"## 2.1. Load and sample data\n\nWe will load the entire csv into `df_full`. We'll reserve `df` for when we drop all columns besides `airline_sentiment` and `text`.","f9b57140":"## 3.4. Classifier fine-tuning","1bcecdb5":"## 4.1 Accuracy","b427906d":"## 2.3. Data preprocessing","91ef614b":"# 1. Introduction","e01eda2e":"# 4. Summary of results","91fead6b":"### 2.3.1. Sentiment by airline\n\nAs we see in the sample, `@{airline}` often appears in the text. If the sentiments depend on the airlines (for example, if everyone just loves Virgin America), then we should consider doing some preprocessing of the text so that the output of our model is indepedent from the airlines."}}