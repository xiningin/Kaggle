{"cell_type":{"89d0e79a":"code","3b4338d6":"code","ae5a1c73":"code","be41a796":"code","b265bc8c":"code","60091f17":"code","6c31703e":"code","bcdf3bb2":"code","d519896d":"code","2de9df6c":"code","4253515a":"code","bbcf57f9":"code","ed4f6b13":"code","957afe7a":"code","6243142f":"code","c82876c1":"code","85bf7ac0":"code","952702f0":"code","580a6cd4":"code","3326b561":"code","ad26dfad":"code","e5ae4fcb":"code","4796d73b":"code","7ad097e0":"code","06f43230":"code","5c7103c2":"code","fb405288":"code","8e85df79":"code","37b1a3c1":"markdown","e1a8cf6c":"markdown","8f48daf3":"markdown","947c011b":"markdown","19ee73c8":"markdown","91aee23a":"markdown","5f0af530":"markdown","40476512":"markdown","47a2a8eb":"markdown","ace1e17d":"markdown","33b7dc35":"markdown","e5fbb9c9":"markdown","002ac298":"markdown","e6e25ee7":"markdown","462ffa52":"markdown","0194dd09":"markdown","e8ecb9e1":"markdown","cd4b8b86":"markdown","efdb5bc2":"markdown"},"source":{"89d0e79a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b4338d6":"from __future__ import print_function\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report\nimport sklearn.metrics as mt\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder","ae5a1c73":"df = pd.read_csv(\"..\/input\/crop-recommender-dataset-with-soil-nutrients\/dataset.csv\")","be41a796":"df.head()","b265bc8c":"df.tail()","60091f17":"df.info()","6c31703e":"labels = df[\"label\"].unique()\ndf[\"label\"].value_counts().plot(kind=\"bar\")\nplt.xlabel('Categories')\nplt.ylabel('No of Samples Each')\nplt.show()","bcdf3bb2":"all_columns = df.columns[:-1]\n\nplt.figure(figsize=(15,13))\ni = 1\nfor column in all_columns[:-1]:\n    plt.subplot(4,3,i)\n    sns.histplot(df[column])\n    i+=1\nplt.show()\n\nsns.histplot(df[all_columns[-1]])\nplt.show()","d519896d":"for column in all_columns:\n    plt.figure(figsize=(19,7))\n    sns.barplot(x = \"label\", y = column, data = df)\n    plt.xticks(rotation=90)\n    plt.title(f\"{column} vs Crop Type\")\n    plt.show()","2de9df6c":"plt.figure(figsize=(100,80))\nsns.pairplot(df, hue = \"label\")\nplt.show()","4253515a":"plt.figure(figsize = (20,15))\nsns.heatmap(df.corr(), center = 0, annot = True)\nplt.show()","bbcf57f9":"label_encoder = LabelEncoder()\nX = df[all_columns]\ny = label_encoder.fit_transform(df[\"label\"])\nprint(X.shape, y.shape)","ed4f6b13":"label_dict = {}\nfor i in range(6):\n    label_dict[i] = label_encoder.inverse_transform([i])[0]\nlabel_dict","957afe7a":"from sklearn.model_selection import train_test_split","6243142f":"X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size = 0.2, random_state = 0)\nprint(f\"Train Data: {X_train.shape}, {y_train.shape}\")\nprint(f\"Train Data: {X_test.shape}, {y_test.shape}\")","c82876c1":"from sklearn.neighbors import  KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport pickle\nfrom sklearn.pipeline import make_pipeline","85bf7ac0":"acc = []\nacc_test = []\nmodel = []\nf1scores = []","952702f0":"from sklearn.linear_model import LogisticRegression\n\nlr_pipeline = make_pipeline(StandardScaler(), LogisticRegression(random_state=2))\nlr_pipeline.fit(X_train, y_train)\n\n# Accuray On Test Data\npredictions = lr_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nacc_test.append(accuracy*100)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y_test, predictions), annot = True)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Accuray On Whole Data\npredictions = lr_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nacc.append(accuracy*100)\nmodel.append('Logistic Regression')\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y, predictions), annot = True)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","580a6cd4":"error_rate = []\nfor i in range(1, 50):\n    pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors = i))\n    pipeline.fit(X_train, y_train)\n    predictions = pipeline.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Accuracy at k = {i} is {accuracy}\")\n    error_rate.append(np.mean(predictions != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(\"Minimum error:-\",min(error_rate),\"at K =\",error_rate.index(min(error_rate))+1)","3326b561":"knn_pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors = 3))\nknn_pipeline.fit(X_train, y_train)\n\n# Test Data Metrics\npredictions = knn_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nacc_test.append(accuracy*100)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y_test, predictions), annot = True)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Whole Data Metrics\npredictions = knn_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nacc.append(accuracy*100)\nmodel.append('K Neighbor Classifier')\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y, predictions), annot = True)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","ad26dfad":"rf_pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(random_state = 18))\nrf_pipeline.fit(X_train, y_train)\n\n# Accuray On Test Data\npredictions = rf_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nacc_test.append(accuracy*100)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y_test, predictions), annot = True)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Accuray On Whole Data\npredictions = rf_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nacc.append(accuracy*100)\nmodel.append('Random Forest Classifier')\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y, predictions), annot = True)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","e5ae4fcb":"import xgboost\nfrom xgboost import XGBClassifier","4796d73b":"xgb_pipeline = make_pipeline(StandardScaler(), XGBClassifier(random_state = 18))\nxgb_pipeline.fit(X_train, y_train)\n\n# Accuray On Test Data\npredictions = xgb_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nacc_test.append(accuracy*100)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nplt.figure(figsize = (30,20))\nsns.heatmap(confusion_matrix(y_test, predictions), annot = True)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Accuray On Whole Data\npredictions = xgb_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nacc.append(accuracy*100)\nmodel.append('XGBoost Classifier')\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nplt.figure(figsize = (30,20))\nsns.heatmap(confusion_matrix(y, predictions), annot = True)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","7ad097e0":"from sklearn.tree import DecisionTreeClassifier\n\ndt_pipeline = make_pipeline(StandardScaler(), DecisionTreeClassifier(criterion=\"entropy\",random_state=2,max_depth=5))\ndt_pipeline.fit(X_train, y_train)\n\n# Accuray On Test Data\npredictions = dt_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nacc_test.append(accuracy*100)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y_test, predictions), annot = True)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Accuray On Whole Data\npredictions = dt_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nacc.append(accuracy*100)\nmodel.append('Decision Tree Classifier')\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y, predictions), annot = True)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","06f43230":"from sklearn.naive_bayes import GaussianNB\n\nnb_pipeline = make_pipeline(StandardScaler(), GaussianNB())\nnb_pipeline.fit(X_train, y_train)\n\n# Accuray On Test Data\npredictions = nb_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nacc_test.append(accuracy*100)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y_test, predictions), annot = True)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Accuray On Whole Data\npredictions = nb_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nacc.append(accuracy*100)\nmodel.append('Naive Bayes Classifier')\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y, predictions), annot = True)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","5c7103c2":"plt.figure(figsize=[12,8],dpi = 100)\nplt.title('Accuracy on Whole Data Comparison')\nplt.xlabel('Accuracy')\nplt.ylabel('Algorithm')\nsns.barplot(x = acc,y = model,palette='dark')\nfor index, value in enumerate(acc):\n    plt.text(value, index, str(round(value,2)))","fb405288":"plt.figure(figsize=[12,8],dpi = 100)\nplt.title('Accuracy on Test Data Comparison')\nplt.xlabel('Accuracy')\nplt.ylabel('Algorithm')\nsns.barplot(x = acc_test,y = model,palette='dark')\nfor index, value in enumerate(acc_test):\n    plt.text(value, index, str(round(value,2)))","8e85df79":"pickle.dump(lr_pipeline, open(\"nb_pipeline.pkl\", \"wb\"))\npickle.dump(knn_pipeline, open(\"knn_pipeline.pkl\", \"wb\"))\npickle.dump(rf_pipeline, open(\"rf_pipeline.pkl\", \"wb\"))\npickle.dump(xgb_pipeline, open(\"xgb_pipeline.pkl\", \"wb\"))\npickle.dump(dt_pipeline, open(\"dt_pipeline.pkl\", \"wb\"))\npickle.dump(nb_pipeline, open(\"nb_pipeline.pkl\", \"wb\"))\n\npickle.dump(label_dict, open(\"label_dictionary.pkl\", \"wb\"))\nprint(\"Saved All Models\")","37b1a3c1":"## Import Libraries","e1a8cf6c":"## Comparison on Accuracy of Test Data for The Models Trained","8f48daf3":"## Label Encoding","947c011b":"## Logistic Regression","19ee73c8":"## XGBoost Classifier","91aee23a":"## Correlation Matrix Using Heat Map","5f0af530":"## No of Categories VS No of Samples","40476512":"## Comparison on Accuracy of Entire Data for The Models Trained","47a2a8eb":"## Decision Tree Classifier","ace1e17d":"## Splitting the Dataset","33b7dc35":"## Pairplot Showing Distribution Graph of Parameter Values for Crop Types","e5fbb9c9":"## Random Forest Classifier","002ac298":"## Range of Values Vs Count for Each Parameter","e6e25ee7":"## K Neighbors Classifier","462ffa52":"## Parameter Values vs Crop Type","0194dd09":"## Saving Models","e8ecb9e1":"## Gaussian Naive Bayes (GaussianNB)","cd4b8b86":"## Importing the Essential Models for Training","efdb5bc2":"## Reading the Dataset"}}