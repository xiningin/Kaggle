{"cell_type":{"dbc4ee57":"code","ed26da39":"code","289d28c6":"code","571841a6":"code","665b69b3":"code","346f0edc":"code","58809784":"code","2df5db89":"code","56489183":"code","87948102":"markdown","bd2cfea8":"markdown","c4e553c7":"markdown","17d8a23e":"markdown","f04a6c89":"markdown","2dcfabbc":"markdown","168852cc":"markdown","0a4cffab":"markdown"},"source":{"dbc4ee57":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier","ed26da39":"x = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv', index_col=1)\ny = x.pop('target')\nx = StandardScaler().fit_transform(x)","289d28c6":"train_x, val_x, train_y, val_y = train_test_split(x, y)\nest = LogisticRegression()\nest.fit(train_x, train_y)\nval_pred = est.predict_proba(val_x)[:,1]\nlogreg_pred = est.predict_proba(x)[:,1]\nroc_auc_score(val_y, val_pred)","571841a6":"train_x, val_x, train_y, val_y = train_test_split(x, y) # BAD - DON'T SPLIT AGAIN - DATA LEAKING\nest = LinearSVC(dual=False)\nest.fit(train_x, train_y) # 2nd-model can see the 1st-model's validation-set, due to 'split again'\nval_pred = est.decision_function(val_x)\nlsvc_pred = est.decision_function(x)\nroc_auc_score(val_y, val_pred)","665b69b3":"train_x, val_x, train_y, val_y = train_test_split(x, y) # BAD - DON'T SPLIT AGAIN - DATA LEAKING\nxgb_model = XGBClassifier(use_label_encoder=False, tree_method='hist')\nxgb_model.fit(train_x, train_y) # 3rd-model can see the 2nd-model's validation-set, due to 'split again'\nval_pred = xgb_model.predict_proba(val_x)[:, 1]\nxgb_pred = xgb_model.predict_proba(x)[:, 1]\nroc_auc_score(val_y, val_pred)","346f0edc":"x_new = pd.DataFrame(x)\nx_new['logreg'] = logreg_pred # 1st-model saw some-part of 2nd+3rd validation-set\nx_new['lsvc'] = lsvc_pred # 2nd-model saw some-part of 1st+3rd validation-set\nx_new['xgb'] = xgb_pred # 3rd-model saw some-part of 1st+2rd validation-set\nx_new.shape","58809784":"train_x, val_x, train_y, val_y = train_test_split(x_new, y) # BAD - DON'T SPLIT AGAIN\nfinal_model = XGBClassifier(use_label_encoder=False, tree_method='hist')\nfinal_model.fit(train_x, train_y)\nval_pred = final_model.predict_proba(val_x)[:, 1]\nroc_auc_score(val_y, val_pred)","2df5db89":"train_x, val_x, train_y, val_y = train_test_split(x, y) # only once at the beginning\n\nest = LogisticRegression()\nest.fit(train_x, train_y)\nlogreg_train_pred = est.predict_proba(train_x)[:,1]\nlogreg_val_pred = est.predict_proba(val_x)[:,1]\nprint('lr', roc_auc_score(val_y, logreg_val_pred))\n\nest = LinearSVC(dual=False)\nest.fit(train_x, train_y)\nlsvc_train_pred = est.decision_function(train_x)\nlsvc_val_pred = est.decision_function(val_x)\nprint('lsvc', roc_auc_score(val_y, lsvc_val_pred))\n\nxgb_model = XGBClassifier(use_label_encoder=False, tree_method='hist')\nxgb_model.fit(train_x, train_y)\nxgb_train_pred = xgb_model.predict_proba(train_x)[:, 1]\nxgb_val_pred = xgb_model.predict_proba(val_x)[:, 1]\nprint('xgb', roc_auc_score(val_y, xgb_val_pred))\n\ntrain_x_new = pd.DataFrame(train_x)\ntrain_x_new['logreg'] = logreg_train_pred\ntrain_x_new['lsvc'] = lsvc_train_pred\ntrain_x_new['xgb'] = xgb_train_pred\nval_x_new = pd.DataFrame(val_x)\nval_x_new['logreg'] = logreg_val_pred\nval_x_new['lsvc'] = lsvc_val_pred\nval_x_new['xgb'] = xgb_val_pred\n\nfinal_model = XGBClassifier(use_label_encoder=False, tree_method='hist')\nfinal_model.fit(train_x_new, train_y)\nfinal_val_pred = final_model.predict_proba(val_x_new)[:, 1]\nprint('final', roc_auc_score(val_y, final_val_pred))","56489183":"roc_auc_score(val_y, final_val_pred)","87948102":"### 3rd model","bd2cfea8":"### 1st model","c4e553c7":"I create this notebook in response to https:\/\/www.kaggle.com\/zhangcheche\/work-well-on-trainset-bad-on-testset\n\nI think reason why his validation-score (0.82) is **far higher** that the LB-score (0.74) is because his data is leaking between models.\n\nTo summarize, if you want to split the training-set apart from validation-set, make sure you only doing it **once** at the beginning. Ensure all of your models are being trained\/fit on the same training-set, and being validated on the same validation-set.\n\nCalling `train_test_split` each time you want to train the base models is a bad idea, because `train_test_split` will shuffle the data by default. Validation-set for the 1st model may become training-set for 2nd model, etc, hence data-leak occurs.\n\nI put a lot of comment in the code here, hence make sure to read the code too.","17d8a23e":"### 2nd model","f04a6c89":"# How to Fix\n\nAll model should be trained on the same training-set, and being validated on the same validation-set. It should be easier to do this by calling `train_test_split` only once at the beginning of your notebook\/kernel.","2dcfabbc":"Boom, spot the high validation-score from the final-model.","168852cc":"Validation-score from the `final` model seems more make sense now :-)","0a4cffab":"### Stacking\/Ensembling - Feeding the Prediction from 1st+2nd+3rd Models into the 4th Final Model\n\nRemember that the 2nd-model could see the 1st-model's validation-set,\n\nthe 3rd-model could see the 1st+2nd model's validation-set, etc.\n\nThe final-model will be able to see what 1st+2nd+3rd model saw in the training-set.\nDue to leak, Hence the final-model **almost can see everything** in the whole complete-data, including the label\/answer from its validation-set."}}