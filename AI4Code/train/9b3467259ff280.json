{"cell_type":{"85f125a5":"code","796d186f":"code","d2607340":"code","6bf07932":"code","e072f77d":"code","51e2cca2":"code","846eddee":"code","263e277b":"code","17867b3d":"code","925f898d":"code","2ac314f9":"code","e274188b":"code","a9583426":"code","7ca7f0bc":"code","93ce0a83":"code","6266fe5a":"code","9ec54390":"code","0f9538d6":"code","feef016b":"code","1dafe9cc":"code","2657eb7d":"code","93e95ace":"code","35a187df":"code","02fd8726":"code","b5ac28bf":"code","3fda1412":"code","43e03fb0":"code","67636350":"code","a59eed7f":"code","75829efd":"code","24fbe5a1":"code","271504a2":"code","a8e73171":"code","463a1cdc":"code","f9ea3bc0":"markdown","010072dd":"markdown","3a4173a7":"markdown","fb0024ef":"markdown","3361f2cf":"markdown","30b8760f":"markdown","b7220738":"markdown","80e38a41":"markdown","271568da":"markdown"},"source":{"85f125a5":"!pip install tensorflow==1.14","796d186f":"import pandas as pd\npd.set_option(\"display.max_columns\", 200)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style='darkgrid')\n\nimport warnings\nfrom keras import optimizers\nfrom keras.utils import plot_model\n\nimport keras.backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers import Conv2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout, Activation\nfrom keras.layers import LeakyReLU\n\nimport tensorflow as tf\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n","d2607340":"train_df = pd.read_csv('..\/input\/womenintheloop-data-science-hackathon\/train.csv')\ntest_df = pd.read_csv('..\/input\/womenintheloop-data-science-hackathon\/test_QkPvNLx.csv')","6bf07932":"print(\"Train shape\",train_df.shape)\nprint(\"Test shape\",test_df.shape)","e072f77d":"train_df.head(3)","51e2cca2":"train_df.info()","846eddee":"train_df.Competition_Metric = train_df.Competition_Metric.fillna(0)\ntest_df.Competition_Metric = test_df.Competition_Metric.fillna(0)","263e277b":"train_df['Course'] = \"course_\" + train_df['Course_ID'].astype(str)\ntest_df['Course'] = \"course_\" + test_df['Course_ID'].astype(str)","17867b3d":"def create_lag_features(df, sales_cols, columns_list, lag_days):\n    temp = df.copy()\n    for i in range(lag_days, 0, -1):\n        temp = pd.concat([temp[columns_list],df[sales_cols].shift(i)], axis=1)\n        columns_list = columns_list +[sales_col+'_t_'+str(i) for sales_col in sales_cols]\n        temp.columns = columns_list\n    return temp","925f898d":"original_column_list = ['ID', 'Day_No', 'Course_ID', 'Course_Domain', 'Course_Type',\n                'Short_Promotion', 'Public_Holiday', 'Long_Promotion', 'Competition_Metric']\nlag_cols = ['Short_Promotion', 'Long_Promotion', 'Public_Holiday', 'Sales', 'User_Traffic']\nlag_days = 60\ntrain_lag_df = pd.DataFrame()\nfor course_id in train_df.Course_ID.unique():\n    column_list = original_column_list.copy()\n    temp_df = create_lag_features(train_df.loc[train_df.Course_ID ==course_id], lag_cols, column_list, lag_days)\n    train_lag_df = train_lag_df.append(temp_df)\n    print(\"Finished creating lag for course : \" + str(course_id))","2ac314f9":"train_lag_df = train_lag_df.dropna()\ntrain_lag_df['User_Traffic'] = train_df['User_Traffic']\ntrain_lag_df['Sales_Today'] = train_df['Sales']","e274188b":"derived_test_df = pd.DataFrame()\nactual_training_df = pd.DataFrame()\ntrain_target_columns = ['Short_Promotion', 'Public_Holiday', 'Long_Promotion', 'Competition_Metric', 'Sales']\ntrain_target_append_columns = [col+'_t_+60' for col in train_target_columns if 'Sales' not in col]\nfor course_id in train_df.Course_ID.unique():\n    \n    train_lag_course_df = train_lag_df.loc[train_lag_df.Course_ID==course_id]\n    train_course_df = train_df[train_df.Course_ID==course_id]\n#     print(\"Created df of shape \" + str(train_lag_course_df.shape))\n    train_target_df = train_course_df[train_target_columns].shift(-60)\n    train_target_df.columns = train_target_append_columns + ['Sales']\n    temp_actual_training_df = pd.concat([train_lag_course_df, train_target_df], axis=1)\n    derived_test_df = derived_test_df.append(temp_actual_training_df[temp_actual_training_df['Sales'].isna()],\n                                            verify_integrity=True)\n    actual_training_df = actual_training_df.append(temp_actual_training_df.dropna(), verify_integrity=True)\n    del temp_actual_training_df\n    del train_target_df\n    del train_course_df\n    del train_lag_course_df","a9583426":"# Checking whether the derived test Course_ID is same as test Course_ID\n(derived_test_df.sort_values(by=['Course_ID','Day_No'])['Course_ID'].reset_index(drop=True)==test_df.sort_values(by=['Course_ID','Day_No'])['Course_ID'].reset_index(drop=True)).value_counts()","7ca7f0bc":"model_train_df = actual_training_df.reset_index(drop = True)\nmodel_test_df = derived_test_df.reset_index(drop= True)","93ce0a83":"def overall_preprocessing(df, is_test=False):\n    df.Competition_Metric = df.Competition_Metric.fillna(0)\n    df['Competition_Metric_t_+60'] = df['Competition_Metric_t_+60'].fillna(0)\n    course_type = pd.get_dummies(df['Course_Type'])\n    course_domain = pd.get_dummies(df['Course_Domain'])\n    \n    user_traffic_columns = [col for col in df.columns if 'User_Traffic' in col]\n    \n    df[user_traffic_columns] = df[user_traffic_columns]\/100\n    df_processed = pd.concat([df, course_type, course_domain], axis=1)\n    df_processed['Day_No'] = df_processed['Day_No'].mod(365)\n    df_processed = df_processed.drop(columns = ['ID','Course_Type','Course_Domain'])\n    if is_test:\n        del df_processed['Sales']\n        print(\"Test shape: \" + str(df_processed.shape))\n        return df_processed\n    else:\n        target = df_processed[['Sales']]\n        del df_processed['Sales'] \n        print(\"Train shape: \"+str(df_processed.shape))\n        return df_processed, target","6266fe5a":"model_encoded_train_df, model_target_df = overall_preprocessing(model_train_df)\nmodel_encoded_test_df = overall_preprocessing(model_test_df, True)","9ec54390":"X_train, X_valid, Y_train, Y_valid = train_test_split(model_encoded_train_df, model_target_df.values, test_size=0.3, random_state=45)\nprint('Train set shape', X_train.shape)\nprint('Validation set shape', X_valid.shape)","0f9538d6":"X_train.isnull().any().value_counts()","feef016b":"batch = 64\nlr = 0.0003\nadam = optimizers.Adam(lr)\nepochs = 5\nleaky_relu_alpha =0.05","1dafe9cc":"model_mlp = Sequential()\nmodel_mlp.add(Dense(512, input_dim=X_train.shape[1]))\nmodel_mlp.add(LeakyReLU(alpha=leaky_relu_alpha))\nmodel_mlp.add(Dense(128, kernel_initializer='normal'))\nmodel_mlp.add(LeakyReLU(alpha=leaky_relu_alpha))\nmodel_mlp.add(Dense(32, kernel_initializer='normal'))\nmodel_mlp.add(LeakyReLU(alpha=leaky_relu_alpha))\nmodel_mlp.add(Dense(1))\nmodel_mlp.compile(loss='mse', optimizer=adam, metrics=['msle'])\nmodel_mlp.summary()","2657eb7d":"mlp_history = model_mlp.fit(X_train.values, Y_train,\n                            validation_data=(X_valid.values, Y_valid),\n                            epochs=epochs, verbose=1, batch_size=batch)","93e95ace":"def save_submission(df_pred_ID, prediction, filename):\n    result = pd.concat([df_pred_ID,pd.DataFrame({'Sales':list(prediction)})],axis=1)\n    #result.to_csv('submissions\/' + filename + '.csv', index=False)\n    return result","35a187df":"test_df['ID'].shape","02fd8726":"df_pred_ID = test_df['ID']\nresult = save_submission(df_pred_ID, model_mlp.predict(model_encoded_test_df).flatten(),'MLP_Time_series')","b5ac28bf":"epochs = 3","3fda1412":"X_train['Padding'] = 0\nX_valid['Padding'] = 0\nmodel_encoded_test_df['Padding'] = 0 ","43e03fb0":"subsequences = 5\ntimesteps = X_train.shape[1]\/\/subsequences\nX_train_series = X_train.values.reshape((X_train.shape[0], timesteps, subsequences))\nX_valid_series = X_valid.values.reshape((X_valid.shape[0], timesteps, subsequences))\nX_test_series = model_encoded_test_df.values.reshape((model_encoded_test_df.shape[0], timesteps, subsequences))\nprint('Train series shape', X_train_series.shape)\nprint('Validation series shape', X_valid_series.shape)\nprint('Test series shape', X_test_series.shape)","67636350":"model_cnn = Sequential()\nmodel_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\nmodel_cnn.add(MaxPooling1D(pool_size=2))\nmodel_cnn.add(Flatten())\nmodel_cnn.add(Dense(64, kernel_initializer='normal', activation='relu'))\nmodel_cnn.add(Dense(32, kernel_initializer='normal', activation='relu'))\nmodel_cnn.add(Dense(1))\nmodel_cnn.compile(loss='mse', optimizer=adam, metrics=['msle'])\nmodel_cnn.summary()","a59eed7f":"cnn_history = model_cnn.fit(X_train_series, Y_train,\n                            validation_data=(X_valid_series, Y_valid),\n                            epochs=epochs, verbose=1, batch_size=batch)","75829efd":"result_cnn = save_submission(df_pred_ID, model_cnn.predict(X_test_series).flatten(),'CNN_Time_series')","24fbe5a1":"epochs = 1\nbatch = 128\nlr = 0.0076\nadam = optimizers.Adam(lr)\nleaky_relu_alpha =0.05","271504a2":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(64, input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\nmodel_lstm.add(LeakyReLU(alpha=leaky_relu_alpha))\nmodel_lstm.add(Dense(32, kernel_initializer='normal'))\nmodel_lstm.add(LeakyReLU(alpha=leaky_relu_alpha))\nmodel_lstm.add(Dense(1))\nmodel_lstm.compile(loss='mse', optimizer=adam, metrics=['msle'])\nmodel_lstm.summary()","a8e73171":"lstm_history = model_lstm.fit(X_train_series, Y_train,\n                            validation_data=(X_valid_series, Y_valid),\n                            epochs=epochs, verbose=1, batch_size=batch)","463a1cdc":"result_lstm = save_submission(df_pred_ID, model_lstm.predict(X_test_series).flatten(),'LSTM_Time_series')","f9ea3bc0":"We have to drop the rows which doesn't have all lag featues","010072dd":"> ## MLP for Time Series Forecasting\n\n* First we will use a Multilayer Perceptron model or MLP model, here our model will have input features equal to the window size.\n* The thing with MLP models is that the model don't take the input as sequenced data, so for the model, it is just receiving inputs and don't treat them as sequenced data, that may be a problem since the model won't see the data with the sequence patter that it has.\n* Input shape **[samples, timesteps]**.","3a4173a7":"> ## LSTM for Time Series Forecasting\n\n* Now the LSTM model actually sees the input data as a sequence, so it's able to learn patterns from sequenced data (assuming it exists) better than the other ones, especially patterns from long sequences.\n* Input shape **[samples, timesteps, features]**.","fb0024ef":"Let's import essential libraries (Here I am using tensorflow 1.14)","3361f2cf":"## Sales Forecasting using MLP, CNN and LSTM\n\nQuick foreword: This is the first notebook I am publishing! I've been learning ML, DL and Python over 2 years, I have been participating in few hackathons lately. So any feedback is more than welcome, thanks!\n\nThis Neural network is built by creating lag features for the hackathon https:\/\/datahack.analyticsvidhya.com\/contest\/women-in-the-loop-a-data-science-hackathon-by-bain\/. The problem statement is to forecast 60 days sales of 600 courses from LearnX which belongs to four domains (Development, Software marketing, Finance & Accounting and Business) given ~882 days of sales of each course. I got private leaderboard score of 126.2250155814\n\nThis Notebook is inspired by https:\/\/www.kaggle.com\/dimitreoliveira\/deep-learning-for-time-series-forecasting. I tried to implement similar strategy for this course sales forecasting. Will explain the approach in the following\n","30b8760f":"> ## CNN for Time Series Forecasting\n\n* For the CNN model we will use one convolutional hidden layer followed by a max pooling layer. The filter maps are then flattened before being interpreted by a Dense layer and outputting a prediction.\n* The convolutional layer should be able to identify patterns between the timesteps.\n* Input shape **[samples, timesteps, features]**.\n\n#### Data preprocess\n* Reshape from [samples, timesteps] into [samples, timesteps, features].\n* This same reshaped data will be used on the CNN and the LSTM model.","b7220738":"Replacing *Competition_Metric* nan values to 0","80e38a41":"> ## Feature generation and Preprocessing\n\n   In this example I am considering only selecting *Sales* for creating lag feature. I am only considering 5 days lag features. The whole idea is for each Course take 5 days lag features, current day feature and predict for 5th day from now.\n\nFor this competition, I have considred 60 days of lag features and current day's feature and predicting for 60th day in the future for each course ID","271568da":"*lag_cols* is the list of columns to create lag features"}}