{"cell_type":{"8d2fd854":"code","f7dcdbd3":"code","23e3f3f1":"code","7c1d47b0":"code","b90c35e5":"code","a87d9585":"code","24d3eac8":"code","2a771233":"code","41c2c1d2":"code","4ecc9424":"code","57b3ba0a":"code","141d467f":"code","13780530":"code","f9f09f49":"code","70e3441e":"code","2718e640":"code","a23912d7":"code","8a95cce6":"code","7fdcb73e":"code","19b026a5":"code","64caf677":"code","41a412e7":"code","2e5224d8":"code","e545f5df":"code","37ca638b":"code","ce3890ed":"code","9e139ba5":"code","b58dee86":"code","4ad50196":"code","88da5c49":"code","f559a45e":"code","c98e7989":"code","88513890":"code","bc4fdb70":"code","07fee964":"code","d22a3cba":"code","cbe410ce":"code","ac528493":"code","f9762b42":"code","d9f4be4e":"code","8ca9a9dd":"markdown","a3b4027f":"markdown","5254067a":"markdown","ede87f75":"markdown","72c614b1":"markdown","cd28439b":"markdown","888e9c35":"markdown","10204f88":"markdown","2db618e2":"markdown","4a2bc96b":"markdown","8eeaa0fd":"markdown","5e42bd7d":"markdown","0f989408":"markdown","8e4bbb26":"markdown"},"source":{"8d2fd854":"!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n!pip install tensorflow-addons\n!pip install --upgrade --ignore-installed tensorflow","f7dcdbd3":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport logging\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport time\nimport random\nimport sys\n\nimport matplotlib.pyplot as plt\nfrom IPython import display\nfrom IPython.display import clear_output\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","23e3f3f1":"seed=123\ntf.compat.v1.set_random_seed(seed)\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)\nlogging.disable(sys.maxsize)","7c1d47b0":"BUFFER_SIZE = 10\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nOUTPUT_CHANNELS = 3\nEPOCHS = 1705\nLAMBDA = 10\nGAMMA = 10\nnoise_dim = 100","b90c35e5":"train_selfie_path = '..\/input\/selfie2anime\/testB'","a87d9585":"train_selfie_imgs = np.array(sorted(os.listdir(train_selfie_path)))","24d3eac8":"train_selfie1_imgs = list(train_selfie_imgs[[0,52,19,54,39,71]])\ntrain_selfie2_imgs = list(train_selfie_imgs[[4,43,55,45,31,23]])","2a771233":"ImageSet_1 = [(train_selfie_path)+'\/%s'%(x) for x in train_selfie1_imgs]\nImageSet_2 = [(train_selfie_path)+'\/%s'%(x) for x in train_selfie2_imgs]","41c2c1d2":"def load(image_file):\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image)\n\n    image_ = tf.cast(image, tf.float32)\n    return image_","4ecc9424":"def resize(input_, height, width):\n    img_ = tf.image.resize(input_, [height, width],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)    \n    return img_","57b3ba0a":"# normalizing the images to [-1, 1]\ndef normalize(input_):\n    image = tf.cast(input_, tf.float32)\n    img_ = (image \/ 127.5) - 1\n   \n    return img_","141d467f":"@tf.function()\ndef random_jitter(input_):\n    img_ = resize(input_,IMG_HEIGHT, IMG_WIDTH)\n\n    if tf.random.uniform(()) > 0.5:\n        img_ = tf.image.flip_left_right(img_)\n\n    return img_","13780530":"def load_image_train(file_):\n    input_ = load(file_)\n    img_ = random_jitter(input_)\n    img_ = normalize(img_)\n\n    return img_","f9f09f49":"train_source = (\n    tf.data.Dataset\n    .from_tensor_slices((ImageSet_1))\n    .map(load_image_train, num_parallel_calls=AUTOTUNE)\n    .shuffle(7)\n    .batch(BATCH_SIZE)\n)\ntrain_dest = (\n    tf.data.Dataset\n    .from_tensor_slices((ImageSet_2))\n    .map(load_image_train, num_parallel_calls=AUTOTUNE)\n    .shuffle(7)\n    .batch(BATCH_SIZE)\n)","70e3441e":"def noise_(num, filter, noise):\n    noise_layer = tf.keras.layers.Dense(num*num*filter)(noise)\n    #noise_layer = tfa.layers.InstanceNormalization()(noise_layer)\n    #noise_layer = tf.keras.layers.ReLU()(noise_layer)\n\n    reshape_noise = tf.keras.layers.Reshape((num, num, filter))(noise_layer)\n\n    return reshape_noise","2718e640":"def downsample(filters, size, stride, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(\n              tf.keras.layers.Conv2D(\n                  filters, \n                  size, \n                  strides=stride, \n                  padding='same',\n                  kernel_initializer=initializer, \n                  use_bias=False)\n    )\n    if apply_batchnorm:\n        result.add(tfa.layers.InstanceNormalization())\n\n    result.add(tf.keras.layers.LeakyReLU(0.4))\n    \n    return result","a23912d7":"def upsample(filters, size, stride):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    layer = tf.keras.layers.Conv2DTranspose(\n            filters, \n            size, \n            strides=stride,\n            padding='same',\n            kernel_initializer=initializer,\n            use_bias=False)\n    \n    result.add(\n        layer\n    )\n    return result","8a95cce6":"def Generator():\n    input_encoder = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH,3])\n    input_generator = tf.keras.layers.Input(shape=[noise_dim, ])\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n    down_stack = [\n        #downsample(32, 3, 2, apply_batchnorm=False),\n        downsample(64, 3, 2,False),\n        downsample(128, 3, 2),\n        downsample(256, 3, 2,False),\n        downsample(512, 3, 2,),\n        downsample(512, 3, 2,),\n        downsample(512, 3, 2,False),\n     ]\n    x = input_encoder\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n  \n    skips = reversed(skips[:-1])\n    gen_layer = tf.keras.layers.Dense(4*4*512)(input_generator)\n    gen_layer = tf.keras.layers.BatchNormalization()(gen_layer)\n    gen_layer = tf.keras.layers.LeakyReLU(0.4)(gen_layer)\n\n    reshape = tf.keras.layers.Reshape((4, 4, 512))(gen_layer)\n    x = reshape\n    up_stack = [\n              upsample(512, 3, 2, ),\n              upsample(512, 3, 2, ),\n              upsample(256, 3, 2, ),\n              upsample(128, 3, 2, ),\n              upsample(64, 3, 2, ), \n      \n   ]\n\n    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 3,\n                                      strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      activation='tanh') \n    filters = [512,512,256,128,64]\n    ndim = [8,16,32,64,128]\n    for up, skip, dim, filt in zip(up_stack, skips, ndim, filters):\n        x = up(x)\n        n = noise_(dim, filt, input_generator)\n        x = tf.keras.layers.concatenate([n, x])\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.LeakyReLU(0.4)(x)\n\n        x = tf.keras.layers.concatenate([x, skip])\n    \n    x = last(x)\n    return tf.keras.Model(inputs=[input_encoder,input_generator], outputs=x)","7fdcb73e":"generator = Generator()\ngenerator.summary()","19b026a5":"tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)","64caf677":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    input_ = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 3], name='input_image')\n    down1 = downsample(64, 3, 2,False)(input_)\n    down2 = downsample(128, 3, 2)(down1)\n\n    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down2)\n    conv = tf.keras.layers.Conv2D(256, 3, strides=1,\n                                kernel_initializer=initializer,\n                                use_bias=False)(zero_pad1)\n    batchnorm1 = tf.keras.layers.BatchNormalization()(conv) #Itfa.layers.InstanceNormalization()\n\n    leaky_relu = tf.keras.layers.LeakyReLU(0.4)(batchnorm1)\n\n    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)\n    \n    last = tf.keras.layers.Conv2D(1, 3, strides=1,\n                                kernel_initializer=initializer,\n                                )(zero_pad2)\n    return tf.keras.Model(inputs=input_, outputs=last)","41a412e7":"discriminator = Discriminator()\ntf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)","2e5224d8":"LAMBDA = 10\nloss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)","e545f5df":"def discriminator_loss(real, generated):\n    real_loss = loss_obj(tf.ones_like(real), real)\n\n    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n    total_disc_loss = real_loss + (generated_loss)*LAMBDA*0.5\n    loss_ = (generated_loss)*LAMBDA*0.5\n    \n    return total_disc_loss, loss_","37ca638b":"def generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = loss_obj(tf.ones_like(disc_generated_output), disc_generated_output)\n\n    # mean absolute error\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n    total_gen_loss = gan_loss + (LAMBDA *(l1_loss)*0.5)\n\n    return total_gen_loss, gan_loss, l1_loss","ce3890ed":"def reconstruction(recon_x, x):\n    return tf.reduce_mean(tf.abs(recon_x - x))","9e139ba5":"generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.99,epsilon=1e-08)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.99,epsilon=1e-08)","b58dee86":"def generate_images(model, input_1, input_2, noise):\n    prediction = model([input_1, noise], training=True)\n    plt.figure(figsize=(15,15))\n\n    display_list = [input_1[0], input_2[0], prediction[0]]\n    title = ['Source Image', 'Destination Image', 'Predicted Image']\n\n    for i in range(3):\n        plt.subplot(1, 4, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()","4ad50196":"example_input_1 = next(iter(train_source))\nexample_input_2 = next(iter(train_dest))\nnoise = tf.random.normal([BATCH_SIZE, noise_dim])\ngenerate_images(generator, example_input_1, example_input_2,noise)","88da5c49":"@tf.function\ndef train_step(real_x, real_y, noise):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator([real_x,noise], training=True)\n\n        real_output = discriminator(real_y, training=True)\n        fake_output = discriminator(generated_images, training=True)\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(fake_output, generated_images, real_y)\n\n        recon_loss = reconstruction(generated_images, real_x)\n        disc_loss, _ = discriminator_loss(real_output, fake_output)\n    gradients_of_generator = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))  ","f559a45e":"result=[]\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    n = 0\n    for image_x, image_y in tf.data.Dataset.zip((train_source, train_dest)):\n        noise = tf.random.normal([BATCH_SIZE, noise_dim])\n        train_step(image_x, image_y, noise)\n        if n % 20 == 0:\n            print ('.', end='')\n        n+=1\n\n    clear_output(wait=True)\n    generate_images(generator, image_x, image_y, noise)\n    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                      time.time()-start))\n    if epoch>=1600:\n        pred = generator([image_x, noise], training=True)\n        result.append(pred[0])","c98e7989":"source = []\ndest = []\nfor x,y in zip(ImageSet_1,ImageSet_2):\n    source.append(load_image_train(x))\n    dest.append(load_image_train(y))\nfig, axis = plt.subplots(2, 3, figsize=(18, 10))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(source[i] * 0.5 + 0.5)\n    ax.set(title = f\"Source Set\")\n    ax.axis('off')","88513890":"fig, axis = plt.subplots(2, 3, figsize=(18, 10))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(dest[i] * 0.5 + 0.5)\n    ax.set(title = f\"Destination Set\")\n    ax.axis('off')","bc4fdb70":"res_1 = result[:15]\nfig, axis = plt.subplots(5, 3, figsize=(25, 40))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(res_1[i] * 0.5 + 0.5)\n    ax.axis('off')","07fee964":"res_2 = result[15:30]\nfig, axis = plt.subplots(5, 3, figsize=(25, 40))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(res_2[i] * 0.5 + 0.5)\n    ax.axis('off')","d22a3cba":"res_3 = result[30:45]\nfig, axis = plt.subplots(5, 3, figsize=(25, 40))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(res_3[i] * 0.5 + 0.5)\n    ax.axis('off')","cbe410ce":"res_4 = result[45:60]\nfig, axis = plt.subplots(5, 3, figsize=(25, 40))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(res_4[i] * 0.5 + 0.5)\n    ax.axis('off')","ac528493":"res_5 = result[60:75]\nfig, axis = plt.subplots(5, 3, figsize=(25, 40))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(res_5[i] * 0.5 + 0.5)\n    ax.axis('off')","f9762b42":"res_6 = result[75:90]\nfig, axis = plt.subplots(5, 3, figsize=(25, 40))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(res_6[i] * 0.5 + 0.5)\n    ax.axis('off')","d9f4be4e":"res_7 = result[90:105]\nfig, axis = plt.subplots(5, 3, figsize=(25, 40))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(res_7[i] * 0.5 + 0.5)\n    ax.axis('off')","8ca9a9dd":"## Future Scopes\n[Convolutional Variational Autoencoder](https:\/\/www.tensorflow.org\/tutorials\/generative\/cvae), this is one of the concepts that I came across later on and I think could be used with this model for better results. Using 512x512 resolution image instead of 256x256 could have provided us with better results, as you could observe that generation of lips in generated anime face is not clear enough, so small but necessary details could have solved through it.","a3b4027f":"## Destination Set","5254067a":"## Source Set","ede87f75":"## Thank you\n![thank you](https:\/\/media.tenor.com\/images\/ef0682c31ec867636791918172e35f8d\/tenor.gif)\n\n### Please <span style=\"color:red\">Up-Vote<\/span> and <span style=\"color:red\">Share<\/span> this notebook if you like it or find the content  informative. Also, let me know your opinions and suggestions in the comment section below.","72c614b1":"This notebook is the continuation of my previous kernel [Anime with StyleGAN](https:\/\/www.kaggle.com\/basu369victor\/anime-with-stylegan). The difference that I have made in this notebook is in the learning rate of Adam optimizer while training and updated the instance normalization layer in the encoder layers of the autoencoder. The reason I created this notebook because after changing the parameters and layers mentioned above, I saw a vast change in results generated by the GAN, and I felt like I should share the results by creating a second part rather than replacing the first part. So, scroll down gently and enjoy this notebook. <br> \nAnd also, please go through the first part, it's a very interesting kernel I hope you would love it and if you find it informative please <span style=\"color:red\">Up-Vote<\/span> and <span style=\"color:red\">Share<\/span> the notebook.\n<br>\n<br>\nThank you.","cd28439b":"## Introduction\nThis work is especially inspired by the research paper [Adversarial Latent Autoencoders](https:\/\/arxiv.org\/pdf\/2004.04467.pdf). What I tried was to build a similar model as explained in this research paper but, I could only bring up only some part of it. To be truthful the model I have build in this notebook is a modified version of the original Adversarial Latent Autoencoder. The code for Adversarial Latent Autoencoders is also explained in PyTorch by the original authors. I know how to deploy a deep learning program in PyTorch but in my current state I am not much fluent in constructing GAN with PyTorch, so in this notebook, I tried to decode the original code with TensorFlow. Even if I would have known how to implement GAN in PyTorch, the code written and explained by the original authors was so difficult to understand that.....<br>","888e9c35":"## What is a Autoencoder?\nAutoencoder is a network that use unsupervised approaches aiming at combining generative and representational properties by learning simulteniously an encoder-generator map.\n## What is ALAE?\nALAE introduces an Autoencoder architecture that is general, and has generative power comparable to GANs while learning a less entangled representation. It basically works on the latent space rathan autoencoding the entire data-space.\n![ALAE](https:\/\/miro.medium.com\/max\/1400\/1*RDcDG8G3p6StcNAlc9AYpw.png)\n<br>\nA more detailed analysis and explaination of ALAE cound be found in the link given below:<br>\n* [Original research paper](https:\/\/arxiv.org\/pdf\/2004.04467.pdf)\n* [Towards Data Science](https:\/\/towardsdatascience.com\/adversarial-latent-autoencoders-4ce12c0abbdd)","10204f88":"## Conclusion\n\n### Hey! the results are actually not bad at all...<br>\n![funny2](https:\/\/static0.cbrimages.com\/wordpress\/wp-content\/uploads\/2019\/09\/Featured-Image-9.jpg)\n<br><br>\nYes, I got to learn a lot while working on this project. Truly I never knew before that a GAN generator could be built in such a manner to reach our desired result. I did everything under my knowledge although I would accept the fact that there are much more concepts to learn to solve this problem in a better way some of them I would mention in my future scope section.<br>\n","2db618e2":"## Implementation\n\nIn this project, I have used usual 256x256 anime faces, instead of using 1204x1024 HD celebrity\/human faces. First of all, it needs a lot of computation power and time to iterate over such high-resolution images, and secondly, I decided to choose a new theme fo this project which is not discussed in the research paper, so I chose anime faces over human faces.<br>\nAnd actually, the task was not so easy as I thought. while blending anime faces, generating the lip part in the generated anime face was the most difficult part, not only that coordinating the eyes in the generated face was also a tedious job. It involved a proper selection of data and hyperparameter tuning. A slight change in adjustment of learning rate or epsilon of Adam optimizer changed the entire quality of the generated image.","4a2bc96b":"## Anime with StyleGAN (Part-II)","8eeaa0fd":"## What is Style ALAE?\nStyleALAE uses the StyleGAN based generator along with the Adversarial Latent Autoencoder.StyleALAE can not only generate 1024\u00d71024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real im-ages.  This makes ALAE the first autoencoder able to compare with,  and go beyond the capabilities of a generator-only type of architecture.\n![style alae](https:\/\/miro.medium.com\/max\/1400\/1*N6z6OKYf3S3NVbwf7pR4LQ.png)","5e42bd7d":"![funny1](https:\/\/i0.wp.com\/drunkenanimeblog.com\/wp-content\/uploads\/2019\/07\/Demon-Slayer-Ep14-24.png?fit=1600%2C900&ssl=1)","0f989408":"## Generated Images","8e4bbb26":"![hesder](https:\/\/pbs.twimg.com\/media\/D42jr1bUEAAzlGe.jpg)"}}