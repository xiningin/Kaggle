{"cell_type":{"f4d28727":"code","00de1c6c":"code","026d82af":"code","dc478219":"code","093e24ea":"code","d692f0fa":"code","9db0fd20":"code","56842249":"code","cc688b71":"code","2d3f22d9":"code","e6938dda":"code","b223f1d3":"code","ec8ecacb":"code","5134d0ee":"code","729ff27c":"code","f43cbce8":"code","6a884752":"code","dcf7cd60":"code","9a9a8171":"code","0a679784":"code","0ac5d6e3":"code","275a4424":"code","6bde3089":"code","78a4dfd5":"code","eb5ad08a":"code","3f59ba53":"code","886c9771":"code","399781ea":"code","8d35efb7":"code","faf7b897":"code","11d0ce55":"code","fb45a1e0":"code","0f4673f6":"code","bd1ca0ce":"code","beb858ba":"code","46d2e396":"code","803952d8":"code","53f88f2c":"code","23d6f02f":"code","cbcf9047":"code","b748594f":"code","4d407d8a":"code","3bd25241":"code","f6563db2":"code","c01121ce":"code","447a6c6f":"code","57fe8e4e":"code","ba36e710":"code","de59a780":"code","ea2d86ca":"code","de4524d6":"code","fe5fab2c":"code","e6f9265c":"code","05a892ff":"code","2c5430c3":"code","c6b8d5d8":"code","0f986f6f":"code","9e9c7173":"code","88fa672b":"code","41c7d420":"code","478e6246":"code","0bf8fc01":"code","d5446eff":"code","ecdc2855":"code","8f7971ea":"code","4bb0c96a":"code","e9f0ecb8":"code","be22c06f":"code","700ee582":"code","9040f6df":"code","af8c10a6":"code","27122c7b":"code","2369c0dd":"code","753bdd1d":"code","beaff7fb":"code","9590d3c6":"code","0d1e00ba":"code","0abfa7d2":"code","7685319e":"code","1472e3d0":"code","95524e48":"code","ad38681d":"code","81d81957":"code","1e2ab7bf":"code","173af34e":"code","5b02725c":"code","23812080":"code","71b2d8e2":"code","6c84cc42":"code","561eed97":"code","a9487420":"code","7d0bfa34":"code","2a317727":"code","a2014ea6":"code","98c75866":"code","d9ce6ca1":"code","f9d12dab":"code","958127bd":"code","dc23cd93":"code","1cf89ae7":"code","fc2b5e4b":"code","ce7c3dee":"code","db260fd5":"code","76f4d063":"code","44162c91":"code","afd99f87":"code","64a4412e":"code","cd7f57f0":"code","1f62a543":"code","23ceb1a7":"code","0e30acac":"code","dcf795ab":"code","7b25b258":"code","c57dc15b":"code","a917860c":"code","ffd7df7d":"code","00fa7b5d":"code","7efe306a":"code","7722c188":"code","3927bf36":"code","6444c3a9":"code","20324a87":"code","83cc614b":"code","31d44b8b":"code","5a840406":"code","452c0bb6":"code","580a0216":"code","3feb257a":"code","244e7428":"code","78d6baeb":"code","ea2fdca3":"code","e1a04416":"code","777c9c4e":"code","d53d842f":"code","b499eab2":"code","5a367787":"code","662c0b21":"code","cc531a77":"code","1273a087":"code","920ccd19":"code","fb50b551":"code","21d80dd8":"code","f32b009d":"code","57dd4965":"code","ac272a68":"code","88e34c4c":"code","ebe7f579":"code","54fa5e86":"code","7d773489":"code","93f7792c":"code","dcc136a3":"code","295a470e":"code","950e2316":"code","0580770a":"code","d1f32585":"code","17d322c8":"code","b8454df3":"code","87311ad9":"code","d93002c5":"code","e44179e3":"code","4986750d":"code","277c6d01":"markdown","1b7e17fd":"markdown","dbbbc4c8":"markdown","5556747c":"markdown","a05bf6f7":"markdown","2a77bf4a":"markdown","05392d39":"markdown","5c1fb723":"markdown","7ddeeda9":"markdown","646f9d52":"markdown","469c3566":"markdown","5d8a437b":"markdown","e2c22438":"markdown","8eef6760":"markdown","1ac92e3f":"markdown","bf4eac65":"markdown","204380ee":"markdown","5ec1ec2f":"markdown","b2378f52":"markdown","afb6ddff":"markdown","d9a4f9ed":"markdown","a4a70117":"markdown","10249b0a":"markdown","9503e8b4":"markdown","77d55a3a":"markdown","5ee12d5e":"markdown","e50b135b":"markdown","8ee71e6f":"markdown","2e5985ea":"markdown","d4075c8a":"markdown","bda8a1f8":"markdown","b6388c62":"markdown","59a92e7e":"markdown"},"source":{"f4d28727":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# Importing Pandas and NumPy\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt, seaborn as sns","00de1c6c":"#function definitons\n\n#Function to print null values in all columns\ndef nulls(df):\n    return (100*round(df.isnull().sum()\/len(df),4).sort_values(ascending=False))\n\n#Function to get the VIFs for all the variables in a dataframe\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef getvif(df):\n    if 'const' in list(df.columns):\n        df1=df.drop('const', axis=1) \n    else:\n        df1 = df.copy()\n    vif=pd.DataFrame()\n    vif['Features'] = df1.columns\n    vif['VIF'] = [variance_inflation_factor(df1.values, i) for i in range(df1.shape[1])]\n    vif['VIF'] = round(vif.VIF,2)\n    vif = vif.sort_values(by = 'VIF', ascending = False)\n    return vif","026d82af":"#importing dataset\ndf = pd.read_csv('..\/input\/leadscore\/Leads.csv')\ndf.head()","dc478219":"df.shape","093e24ea":"#Let's see what columns we have\ndf.info()","d692f0fa":"#Description of numerical columns\ndf.describe()","9db0fd20":"#There are a lot of columns with a lot of null values\nnulls(df)","56842249":"#Lets see if there are any duplicates rows in entirety\ndf.duplicated(keep='first').sum()","cc688b71":"#Lets see if there are any duplicates, this time using only the prospect ID as the identifier\ndf.duplicated(keep='first',subset='Prospect ID').sum()","2d3f22d9":"#There are some values that have been mentioned as \"Select\". As per the data dictionary, these are default values selected when the user does not make any other selection.\ndf.Specialization.value_counts(normalize=True)*100","e6938dda":"#For our data to make more sense, we will be replacing the \"Select\" values with NaNs.\ndf=df.replace('Select',np.nan)","b223f1d3":"#Now, we check for null value percentages again \nnulls_list=nulls(df)\nprint(nulls_list)","ec8ecacb":"#Let's drop the columns with more than 50% of null values. these would be of no use to our model building process.\ndf.drop(list(nulls_list.loc[nulls_list>50].index),axis=1,inplace=True)","5134d0ee":"#checking nulls again\nnulls(df)","729ff27c":"#We will try and see what these scores and indices contain\nfor i,each in enumerate(list(nulls(df).index)[:4]):\n    print(df[each].describe())","f43cbce8":"#The scores are numerical, using box plots\nplt.figure(figsize=(20,12))\nplt.subplot(221)\nsns.boxplot(df['Asymmetrique Profile Score'])\nplt.subplot(222)\nsns.boxplot(df['Asymmetrique Activity Score'])\n#Indices are categorical, using countplot\nplt.subplot(223)\nsns.countplot(df['Asymmetrique Profile Index'])\nplt.subplot(224)\nsns.countplot(df['Asymmetrique Activity Index'])","6a884752":"#We cannot see a substantial variance in these features across the data set. Thus, since a large chunk of these variables is missing, we can choose to drop it, since no vital information would be lost.\ndf.drop(list(nulls(df).index)[:4],axis=1,inplace=True)\nnulls(df)","dcf7cd60":"#inspecting city\ndf.City.value_counts(normalize=True)*100","9a9a8171":"#57% of our data points are from Mumbai. We can choose to impute the nulls in city column with Mumbai.\ndf['City'] = df['City'].replace(np.nan, 'Mumbai')\nnulls(df)","0a679784":"#inspecting specialization \ndf['Specialization'].value_counts(normalize=True).sort_values(ascending=False)*100","0ac5d6e3":"#Here a null might mean that either the customer has a specialization that does not exist in this list, or no specialization. We can use 'Others' as a new category here. \ndf['Specialization'] = df['Specialization'].replace(np.nan,'Others')\nnulls(df)","275a4424":"#inspecing tags\ndf.Tags.value_counts(normalize=True).sort_values(ascending=False)*100","6bde3089":"#We'll remove tags since it is a score variable\ndf.drop('Tags', axis=1, inplace=True)","78a4dfd5":"#inspecting What matters most to you in choosing a course\ndf['What matters most to you in choosing a course'].value_counts(normalize=True).sort_values(ascending=False)*100","eb5ad08a":"#Imputing nulls with mode\ndf['What matters most to you in choosing a course'] = df['What matters most to you in choosing a course'].replace(np.nan,'Better Career Prospects')\nnulls(df)","3f59ba53":"#inspecing occupation feature\ndf['What is your current occupation'].value_counts(normalize=True).sort_values(ascending=False)*100","886c9771":"#imputing nulls with mode\ndf['What is your current occupation']=df['What is your current occupation'].replace(np.nan,'Unemployed')\nnulls(df)","399781ea":"#inspecting country\ndf['Country'].value_counts(normalize=True).sort_values(ascending=False)*100","8d35efb7":"#imputing mode\ndf['Country'] = df['Country'].replace(np.nan, 'India')\nnulls(df)","faf7b897":"#the rest of the features contain less than 2% null values. We can safely drop these rows.\ndf.dropna(inplace=True)\nnulls(df)","11d0ce55":"#Percentage of rows retained is pretty good.\n(df.shape[0]\/9240)*100","fb45a1e0":"#We will be inspecting all the cateogrical columns, looking for highly skewed features, and features with less prominent valus that can be clubbed\n\nplt.figure(figsize=(20,8*13))\nfor i,each in enumerate(list(set(df.drop('Prospect ID',axis=1).columns) - set(df._get_numeric_data().columns))):\n    plt.subplot(13,2,i+1)\n    sns.countplot(y=df[each])","0f4673f6":"#We can clearly see the following highly skewed variables, we will use value counts to confirm our suspicion\nfor each in ['Digital Advertisement','Through Recommendations','Magazine','Do Not Call','Search','Newspaper Article',\n        'Update me on Supply Chain Content','Receive More Updates About Our Courses','I agree to pay the amount through cheque',\n        'What matters most to you in choosing a course','Do Not Email','X Education Forums','Newspaper','Country',\n        'Get updates on DM Content']:\n    print('\\n')\n    print(df[each].value_counts(normalize=True)*100)","bd1ca0ce":"#From the plots and value counts above, we can identify some highly skewed variables. These variables will not be of much value to the model, and don't contain valuable information. \n#We can remove all these redundant columns\ndf.drop(['Digital Advertisement','Through Recommendations','Magazine','Do Not Call','Search','Newspaper Article',\n        'Update me on Supply Chain Content','Receive More Updates About Our Courses','I agree to pay the amount through cheque',\n        'What matters most to you in choosing a course','Do Not Email','X Education Forums','Newspaper','Country',\n        'Get updates on DM Content'],axis=1,inplace=True)\ndf.shape","beb858ba":"df.info()","46d2e396":"#Checking correlation between remaining categorical variables\ndf.corr()","803952d8":"df.head()","53f88f2c":"#Lets change column names to more readable ones\ndf=df.rename(columns={'Total Time Spent on Website':'Time Spent','Page Views Per Visit':'Views','What is your current occupation':'Occupation','A free copy of Mastering The Interview':'Free Copy'})","23d6f02f":"#We will now work towards reducing the number of possible values each variable can take.\n#let's start with Last Activity\ndf['Last Activity'].value_counts(normalize=True).sort_values(ascending=False)*100","cbcf9047":"#List of features that have less than 1% frequency\nto_combine = list(df['Last Activity'].value_counts(normalize=True).sort_values(ascending=False).loc[(df['Last Activity'].value_counts(normalize=True).sort_values(ascending=False)<0.01).values].index)\nto_combine","b748594f":"#We'll club the less frequent features into a single category \"Others\"\ndf['Last Activity'] = df['Last Activity'].replace(to_combine,'Others')\ndf['Last Activity'].value_counts()","4d407d8a":"#Inspecting lead source\ndf['Lead Source'].value_counts(normalize=True).sort_values(ascending=False)*100","3bd25241":"#We'll club the less frequent features into a single category \"Others\"\ndf['Lead Source'] = df['Lead Source'].replace(list(df['Lead Source'].value_counts(normalize=True).sort_values(ascending=False).loc[(df['Lead Source'].value_counts(normalize=True).sort_values(ascending=False)<0.01).values].index)\n,'Others')\ndf['Lead Source'].value_counts()","f6563db2":"#inspecting specialization\ndf['Specialization'].value_counts(normalize=True).sort_values(ascending=False)*100","c01121ce":"#inspecting occupation\ndf['Occupation'].value_counts(normalize=True).sort_values(ascending=False)*100","447a6c6f":"#inspecting last notable activity\ndf['Last Notable Activity'].value_counts(normalize=True).sort_values(ascending=False)*100","57fe8e4e":"#We'll club the less frequent features into a single category \"Others\"\ndf['Last Notable Activity'] = df['Last Notable Activity'].replace(list(df['Last Notable Activity'].value_counts(normalize=True).sort_values(ascending=False).loc[(df['Last Notable Activity'].value_counts(normalize=True).sort_values(ascending=False)<0.01).values].index)\n,'Others')\ndf['Last Notable Activity'].value_counts()","ba36e710":"#We can also covert the Free Copy column to numeric (one-hot encoding)\ndf['Free Copy'] = df['Free Copy'].map({'No':0,'Yes':1})\ndf['Free Copy'].describe()","de59a780":"#Let's check data imbalance\n100*df['Converted'].sum()\/len(df)","ea2d86ca":"df.head()","de4524d6":"plt.figure(figsize=(16,8))\nsns.countplot(x='Lead Source',hue='Converted',data=df)","fe5fab2c":"num_vars=['TotalVisits','Time Spent','Views']\nplt.figure(figsize=(8,6))\nfor i,each in enumerate(num_vars):\n    plt.subplot(1,3,i+1)\n    sns.boxplot(y=each,x='Converted',data=df)\n    plt.tight_layout()","e6f9265c":"#Let's analyze all the categorical variables against the target variable\ncats=['Lead Origin','Specialization','Occupation','City','Last Notable Activity', 'Last Activity']\nplt.figure(figsize=(16,25))\nfor i,each in enumerate(cats):\n    plt.subplot(3,2,i+1)\n    sns.countplot(y=each,data=df,hue='Converted')\nplt.tight_layout()","05a892ff":"#Lets see the correaltion heatmap\nsns.heatmap(df.corr(), cmap=\"RdYlGn\",annot=True)","2c5430c3":"#We can use prospect ID for identificatipon, dropping Lead Number\ndf.drop('Lead Number',inplace=True, axis=1)","c6b8d5d8":"df.columns","0f986f6f":"#creating dummy variables\ndummy = pd.get_dummies(df[['Lead Origin','Lead Source','Last Activity','Specialization',\n                           'Occupation','City','Last Notable Activity']], drop_first=True)\ndummy.head()\n","9e9c7173":"#Merging dummies into dataset\ndf=pd.concat([df,dummy],axis=1)\ndf.head()","88fa672b":"#dropping dummified variables \ndf.drop(['Lead Origin','Lead Source','Last Activity','Specialization',\n                           'Occupation','City','Last Notable Activity'],inplace=True,axis=1)","41c7d420":"df.head()","478e6246":"from sklearn.model_selection import train_test_split","0bf8fc01":"x=df.drop(['Converted','Prospect ID'],axis=1)\ny=df[['Converted']]\nx_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.7,test_size=0.3,random_state=1)","d5446eff":"# y_train.index = x_train['Prospect ID']\n# x_train.index = x_train['Prospect ID']\n# x_train.drop('Prospect ID',axis=1,inplace=True)","ecdc2855":"x_train.shape","8f7971ea":"y_train.shape","4bb0c96a":"# y_test.index = x_test['Prospect ID']\n# x_test.index=x_test['Prospect ID']\n# x_test.drop('Prospect ID',axis=1,inplace=True)","e9f0ecb8":"x_test.shape","be22c06f":"y_test.shape","700ee582":"from sklearn.preprocessing import StandardScaler","9040f6df":"scaler=StandardScaler()","af8c10a6":"x_train[['TotalVisits','Time Spent','Views']].describe()","27122c7b":"x_train[['TotalVisits','Time Spent','Views']] = scaler.fit_transform(x_train[['TotalVisits','Time Spent','Views']])\nx_train[['TotalVisits','Time Spent','Views']].describe()","2369c0dd":"import statsmodels.api as sm","753bdd1d":"#Logistic Regression Model\nm1 = sm.GLM(y_train,sm.add_constant(x_train), family = sm.families.Binomial())\nm1.fit().summary()","beaff7fb":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 20)             # running RFE with 20 variables as output\nrfe = rfe.fit(x_train, y_train)\nrfe.support_","9590d3c6":"list(zip(x_train.columns, rfe.support_, rfe.ranking_))","0d1e00ba":"#columns chosen upon running RFE\ncols=x_train.columns[rfe.support_]\ncols","0abfa7d2":"x_train_sm = sm.add_constant(x_train[cols])\nm2=sm.GLM(y_train,x_train_sm,family=sm.families.Binomial())\nm2.fit().summary()","7685319e":"#checking VIF\n\ngetvif(x_train_sm)","1472e3d0":"#let's drop Occupation_Housewife since it shows to be less signficant in the model (relatively higher p value)\nx_train_sm.drop('Occupation_Housewife',axis=1,inplace=True)","95524e48":"#Rebuilding the model\nx_train_sm = sm.add_constant(x_train_sm)\nm3 = sm.GLM(y_train,x_train_sm,family=sm.families.Binomial())\nprint(m3.fit().summary())\n#checking vifs\ngetvif(x_train_sm)","ad38681d":"#Dropping Lead Source_Reference since it is least signficant, and has a high VIF\nx_train_sm = sm.add_constant(x_train_sm.drop('Lead Source_Reference',axis=1))\n#Rebuilding the model\nm4 = sm.GLM(y_train,x_train_sm,family=sm.families.Binomial())\nprint(m4.fit().summary())\n#checking vifs\ngetvif(x_train_sm)","81d81957":"#Dropping Occupation_Unemployed since it has a high correlation with other features\nx_train_sm = sm.add_constant(x_train_sm.drop('Occupation_Unemployed',axis=1))\n#Rebuilding the model\nm5 = sm.GLM(y_train,x_train_sm,family=sm.families.Binomial())\nprint(m5.fit().summary())\n#checking vifs\ngetvif(x_train_sm)","1e2ab7bf":"#Dropping Last Activity_Others since it is coming out to be relatively less significant\nx_train_sm = sm.add_constant(x_train_sm.drop('Last Activity_Others',axis=1))\n#Rebuilding the model\nm6 = sm.GLM(y_train,x_train_sm,family=sm.families.Binomial())\nprint(m6.fit().summary())\n#checking vifs\ngetvif(x_train_sm)","173af34e":"#Dropping Specialization_Hospitality Management since it is coming out to be relatively less significant\nx_train_sm = sm.add_constant(x_train_sm.drop('Specialization_Hospitality Management',axis=1))\n#Rebuilding the model\nm7 = sm.GLM(y_train,x_train_sm,family=sm.families.Binomial())\nprint(m7.fit().summary())\n#checking vifs\ngetvif(x_train_sm)","5b02725c":"#We are getting a high VIF for last activity and last notable activity \"SMS Sent\"\n#We can see that these two are highly correlated, thus we can drop one of them\nx_train_sm[['Last Activity_SMS Sent','Last Notable Activity_SMS Sent']].corr()","23812080":"#We'll drop Last Activity_SMS Sent due to the high correlation\nx_train_sm = sm.add_constant(x_train_sm.drop('Last Activity_SMS Sent',axis=1))\n#Rebuilding the model\nm8 = sm.GLM(y_train,x_train_sm,family=sm.families.Binomial())\nprint(m8.fit().summary())\n#checking VIFs too\ngetvif(x_train_sm)","71b2d8e2":"#Last Activity_Email Link Clicked is highly insignificant in the model with a high p value\n#we will drop it \nx_train_sm = sm.add_constant(x_train_sm.drop('Last Activity_Email Link Clicked',axis=1))\n#Rebuilding the model\nm9 = sm.GLM(y_train,x_train_sm,family=sm.families.Binomial())\nprint(m9.fit().summary())\n#checking VIFs too\ngetvif(x_train_sm)","6c84cc42":"#Let's move forward with this model\nres = m9.fit()","561eed97":"#getting predicted values on the train set\ny_train_pred = res.predict(x_train_sm)","a9487420":"y_train_pred.shape","7d0bfa34":"y_train_pred[:10]","2a317727":"#changing predictions to array\ny_train_pred=y_train_pred.values.reshape(-1)\ny_train_pred[:10]","a2014ea6":"#creating new df for predictions\ny_train_pred_final = pd.DataFrame({'Converted':y_train.values.reshape(-1),'Prob':y_train_pred})\ny_train_pred_final['ID'] = y_train.index\ny_train_pred_final.head()","98c75866":"y_train_pred_final['Predicted'] = y_train_pred_final.Prob.map(lambda x: 1 if x>0.5 else 0)\ny_train_pred_final[:10]","d9ce6ca1":"from sklearn import metrics","f9d12dab":"#confusion matrix\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nconfusion","958127bd":"#We now have our confusion matrix. \ntn = confusion[0][0] #true neatives\ntp = confusion[1][1] #true positives\nfp = confusion[0][1] #false positives\nfn = confusion[1][0] #false negatives","dc23cd93":"#Let's check overall accuracy\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted)","1cf89ae7":"#Sensitivity - this will need to be maximized since the business objective is to identify the hottest leads. \n#We would not want to miss any of the positives in this scenario.\ntp \/ float (tp+fn)","fc2b5e4b":"#specificity - this is a measure of how well the model can tell if a lead is not worth following\ntn \/ float(tn+fp)","ce7c3dee":"#False positive rate - from all the neagtives, how many were falsely predicted as positive? This should be minimized.\nfp\/float(tn+fp)","db260fd5":"#True positive rate - from all the positives, how many were correctly predicted as positive? This should be maximized.\n#This is same as sensitivity\ntp\/float(tp+fn)","76f4d063":"#Positive predictive value\ntp\/float(tp+fp)","44162c91":"#negative predictive value \ntn\/float(tn+fn)","afd99f87":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","64a4412e":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Prob)","cd7f57f0":"# Let's create columns with different probability cutoffs \nnumbers = [(round(i\/100,2)) for i in range(0,101,5)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","1f62a543":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensitivity','specificity'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [(round(i\/100,2)) for i in range(0,101,5)]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n#     accuracy = metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final[i])\n    specificity = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensitivity = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensitivity,specificity]\nprint(cutoff_df)","23ceb1a7":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensitivity','specificity'])\nplt.show()","0e30acac":"y_train_pred_final.head()","dcf795ab":"#Trying cutoff 0.35\ncutoff=0.35","7b25b258":"y_train_pred_final['final_predicted'] = y_train_pred_final.Prob.map( lambda x: 1 if x > cutoff else 0)\n\n#Also adding Lead Score in line with the business objective\ny_train_pred_final['Lead Score'] = y_train_pred_final['Prob'].apply(lambda x: int(round(x*100,0)))\n\n#We can remove the rest of the columns now\ny_train_pred_final = y_train_pred_final[['Converted','Prob','ID','final_predicted','Lead Score']]\n\ny_train_pred_final.head()","c57dc15b":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nTP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives\nconfusion2","a917860c":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","ffd7df7d":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","00fa7b5d":"# Let us calculate specificity\nTN \/ float(TN+FP)","7efe306a":"# Calculate false postive rate - predicting conversion when the customer would not convert\nprint(FP\/ float(TN+FP))","7722c188":"#Setting cutoff 0.3\ncutoff=0.3","3927bf36":"y_train_pred_final['final_predicted'] = y_train_pred_final.Prob.map( lambda x: 1 if x > cutoff else 0)\n\n#Also adding Lead Score in line with the business objective\ny_train_pred_final['Lead Score'] = y_train_pred_final['Prob'].apply(lambda x: int(round(x*100,0)))\n\n#We can remove the rest of the columns now\ny_train_pred_final = y_train_pred_final[['Converted','Prob','ID','final_predicted','Lead Score']]\n\ny_train_pred_final.head()","6444c3a9":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","20324a87":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","83cc614b":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","31d44b8b":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","5a840406":"# Let us calculate specificity\nTN \/ float(TN+FP)","452c0bb6":"# Calculate false postive rate - predicting conversion when the customer would not convert\nprint(FP\/ float(TN+FP))","580a0216":"# Positive predictive value \nprint (TP \/ float(TP+FP))","3feb257a":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","244e7428":"#We have the confusion matrix as \nconfusion2","78d6baeb":"#Precision \nTP\/(TP+FP)","ea2fdca3":"#recall\nTP\/(TP+FN)","e1a04416":"#We can also get the precision and recall values usking sklearn\nfrom sklearn.metrics import precision_score, recall_score","777c9c4e":"precision_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","d53d842f":"recall_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","b499eab2":"from sklearn.metrics import precision_recall_curve","5a367787":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Prob)","662c0b21":"fig,ax=plt.subplots()\nax.plot(thresholds, p[:-1], \"g-\", label='Precision') #plotting precision as green line\nax.plot(thresholds, r[:-1], \"r-\", label = 'Recall') #plotting recall as red line\nplt.xlabel('Probability Threshold')\nlegend = ax.legend(loc='best', shadow=True)\nplt.ylabel('Precision\/Recall')\nplt.title('Precision - Recall Curve')\nplt.show()","cc531a77":"#We will scale numerical features just like we did in train set. This time, we fit don't fit the scaler, we directly transform the data.\nx_test[['TotalVisits','Time Spent','Views']] = scaler.transform(x_test[['TotalVisits','Time Spent','Views']])\nx_test.head()","1273a087":"#retaining only the features that we used in our final model\nx_test = x_test[list(x_train_sm.drop('const',axis=1).columns)]","920ccd19":"x_test_sm = sm.add_constant(x_test)","fb50b551":"x_test_sm","21d80dd8":"#Making predictions\ny_test_pred = res.predict(x_test_sm)","f32b009d":"#creating new df for predictions\ny_test_pred_final = pd.DataFrame({'Converted':y_test.values.reshape(-1),'Prob':y_test_pred})\ny_test_pred_final['ID'] = y_test.index\ny_test_pred_final.head()","57dd4965":"#Setting cutoff to 0.35 to see parameters\ncutoff = 0.35","ac272a68":"y_test_pred_final['Predicted'] = y_test_pred_final.Prob.map(lambda x: 1 if x>cutoff else 0)\n#We will also all add a \"Lead Score\" column, in line with the business objective.\ny_test_pred_final['Lead Score'] = y_test_pred_final['Prob'].apply(lambda x: int(round(x*100,0)))\ny_test_pred_final[:10]","88e34c4c":"#Confusion Matrix for predictions on test data\nconfusion3 = metrics.confusion_matrix(y_test_pred_final.Converted, y_test_pred_final.Predicted)\nTP = confusion3[1,1] # true positive \nTN = confusion3[0,0] # true negatives\nFP = confusion3[0,1] # false positives\nFN = confusion3[1,0] # false negatives# Let's see the sensitivity of our logistic regression model\nconfusion3","ebe7f579":"#Accuracy on test data\nmetrics.accuracy_score(y_test_pred_final.Converted, y_test_pred_final.Predicted)","54fa5e86":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","7d773489":"# Let us calculate specificity\nTN \/ float(TN+FP)","93f7792c":"# Calculate false postive rate \nprint(FP\/ float(TN+FP))","dcc136a3":"#Setting cutoff to 0.3\ncutoff=0.3","295a470e":"y_test_pred_final['Predicted'] = y_test_pred_final.Prob.map(lambda x: 1 if x>cutoff else 0)\n#We will also all add a \"Lead Score\" column, in line with the business objective.\ny_test_pred_final['Lead Score'] = y_test_pred_final['Prob'].apply(lambda x: int(round(x*100,0)))\ny_test_pred_final[:10]","950e2316":"#Accuracy on test data\nmetrics.accuracy_score(y_test_pred_final.Converted, y_test_pred_final.Predicted)","0580770a":"#Confusion Matrix for predictions on test data\nconfusion3 = metrics.confusion_matrix(y_test_pred_final.Converted, y_test_pred_final.Predicted)\nTP = confusion3[1,1] # true positive \nTN = confusion3[0,0] # true negatives\nFP = confusion3[0,1] # false positives\nFN = confusion3[1,0] # false negatives# Let's see the sensitivity of our logistic regression model\nconfusion3","d1f32585":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","17d322c8":"# Let us calculate specificity\nTN \/ float(TN+FP)","b8454df3":"# Calculate false postive rate \nprint(FP\/ float(TN+FP))","87311ad9":"# Positive predictive value \nprint (TP \/ float(TP+FP))","d93002c5":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","e44179e3":"#Recommendations\nres.summary()","4986750d":"#We will now look at the most important features identified in our model\n\nfinal_model = pd.DataFrame(res.params) #getting model parameters (features and coefficients)\n\nfinal_model['Feature']=final_model.index \nfinal_model.index = range(len(final_model))\n\nfinal_model = final_model.rename(columns = {0:'Coefficient'})[['Feature','Coefficient']] #renaming columns for better understanding\nfinal_model.sort_values(by='Coefficient', ascending=False, ignore_index = True) #sorting by coefficient","277c6d01":"## Precision and Recall","1b7e17fd":"**Problem Statement**\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as \u2018Hot Leads\u2019. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone.\n\nX Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.","dbbbc4c8":"All the metrics calculated above are in an acceptable range, but we can work on improving it further and tuning the model to better align with business objective","5556747c":"We'll be leaving specializations as-is.","a05bf6f7":"We will now plot the ROC curve.\nAn ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","2a77bf4a":"Now we shall work towards refining this model","05392d39":"## Precision and Recall Trade-off","5c1fb723":"#### Inference\nFrom the plot above, it is clear that a value of around 0.3 would be the optimum cut-off for the lead score. Thus, a lead score above 0.3 would qualify as a hot lead and should be pursued by the company, and would have a much better chance of getting converted.\n\nSince the business objective is to get a target of 80% lead conversion rate, we have to keep this in mind while setting the threshold as well.","7ddeeda9":"## Visualizations on final dataset","646f9d52":"#### Inference\n* We obtain an accuracy of 79% on the training data, while maintaing a sensitivity of 84%. \n* We can proceed with these results since they are in line with business objective","469c3566":"## Feature selection using RFE","5d8a437b":"## Inferences\n* The most important colums from the dataset can be identified as below\n    * Lead Origin\n    * Lead Source\n    * Time Spent\n    * Occupation\n    * Last Activity\n    * Last Notable Activity\n* The most important features (dummy variables) used in the model can be identified as below: \n    * Lead Origin_Lead Add Form\n    * Lead Source_Welingak Website\n    * Occupation_Working Professional\n    * Last Activity_Email Bounced\n    * Last Notable Activity_SMS Sent\n* Recommendations to business can be made on the basis of the lead score. Since we chose the probability cut off at 0.3, this would translate to a score of 30. This can be tweaked as and when the needs of the business change   ","e2c22438":"#### Inferences\n* Leads with last notable activity as SMS sent have a high chance of conversion\n* Working professionals have the highest conversion ratio\n* Leads originating from Lead Add Form have high coversion ratio","8eef6760":"#### Inference\nSince the ROC curve sticking close to the edge and resembling a right angle triangle, we have a good operating chacteristic","1ac92e3f":"## Data Preparation \nWe will be converting all categorical features into dummy variables, by implmeneting one-hot encoding. ","bf4eac65":"## Making predictions on the test set","204380ee":"We do not see any alarmingly high levels of correlation in the data","5ec1ec2f":"## Scaling of numerical features","b2378f52":"We will now look at some more metrics, to see how the model is really performing on the train data, and how relevant it will be to meet the business objective.","afb6ddff":"## Building the model","d9a4f9ed":"## Rebuilding the model and assessing using SM","a4a70117":"## Data Cleaning and EDA","10249b0a":"#### Inference\n'Reference' and 'Welingak Website' have great conversion rates","9503e8b4":"#### Inference\nTime spent on the website has a strong correlation with the coversion rate","77d55a3a":"## Finding optimal cut-off point\nWe initially chose the cut-off point for the model as 0.5. The lead score itself would serve the purpose of the model, but for sake on analysis, we will try to find the optimal cutoff point for prediction, and it can be included as a recommendation to the business. ","5ee12d5e":"## Model Finalized","e50b135b":"Now we have the cleaned dataset ready. We will be using this for our analysis.","8ee71e6f":"## Splitting the data into train and test set","2e5985ea":"Around 37% of the data corresponds to the leads which have been converted. Thus, the data is sufficiently balanced and we can continue with building our model here.","d4075c8a":"#### Inference\n* We have obtained an accuracy of 79.25% on the test data, while maintaining a sensitivity of 85.3%. Thus, we can conclude that our model is performing well and can be rolled out to meet the business objective.\n* Our model evaluation parameters have not changed and remained about the same when runnning them on test data. Hence, we can conclude that the model is quite stable.","bda8a1f8":"A value close to 0.3 seems to be optimal.","b6388c62":"#### Note\nWe have obtained quite good accuracy of 81.6%","59a92e7e":"Now we have trained our model and have the predictions on the training set. We will now see some metrics on the predictions made on the training set."}}