{"cell_type":{"488b97f5":"code","d45f6642":"code","58605d35":"code","ed24a30d":"code","a4c157e2":"code","a058aeda":"code","3cfa4c84":"code","79a77f57":"code","a3b6c287":"code","5a646e00":"code","577f3659":"code","1ea1bcc8":"code","ea012068":"code","e1d4cf85":"code","86d5e8cc":"code","8e4de6fd":"code","deb2e6fc":"code","69f6f7fe":"code","2f9378f3":"code","197a3255":"code","3872c7df":"code","121a2c06":"code","f9e089af":"code","2bfa51af":"code","fd070b83":"code","1f0c2366":"code","515f2164":"code","ead5f49e":"code","5098e5b5":"markdown","495a712b":"markdown","fe1b07f6":"markdown","60ad0d17":"markdown","c67248cb":"markdown","205f4b38":"markdown","44ff4610":"markdown","d46fbb5b":"markdown","1361c7a4":"markdown","b92cd97f":"markdown","2beff08d":"markdown","dd8bd5a6":"markdown","93c3c6b4":"markdown"},"source":{"488b97f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport nltk\nimport seaborn as sns\nnltk.download(['vader_lexicon', 'stopwords'])\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfiles = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        files.append(filename)\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d45f6642":"cols = ['location', 'month', 'year', 'text']\ndf = pd.DataFrame(columns=cols, index=range(35))","58605d35":"import os\ni = 0\nfor filename in os.listdir(\"..\/input\/donald-trumps-rallies\"):\n    path = \"..\/input\/donald-trumps-rallies\/\"+filename\n    with open(path) as f:\n        fileText = f.read()\n    \n    filename = filename.rsplit('.',1)[0] #remove .txt\n    fileYear = filename[-4:]\n    filename = filename.rsplit('_', 1)[0] #remove ''\n    filename = ''.join([i for i in filename if not i.isdigit()])\n    fileMonth = filename[-3:]\n    fileLoc = filename[:-3]\n    #print(fileLoc, fileMonth, fileYear, fileText)\n    df.append([fileLoc, fileMonth, fileYear, fileText])\n    df.loc[i].location = fileLoc\n    df.loc[i].month = fileMonth\n    df.loc[i].year = fileYear\n    df.loc[i].text = fileText\n    i+=1\n\ndf.head()","ed24a30d":"df.tail()","a4c157e2":"text = df.text.str.lower()\nsentences = text.str.split('.')","a058aeda":"def getAverageSentenceLength(sentences):\n    totalLength = len(sentences)\n    running_length = 0\n    for sentence in sentences:\n        tokenized = sentence.split()\n        running_length += len(tokenized)\n    \n    return running_length \/ totalLength","3cfa4c84":"averageSentenceLength = []\nfor sentence in sentences:\n    averageSentenceLength.append(getAverageSentenceLength(sentence))","79a77f57":"plt.figure(figsize=(10, 10))\nplt.title('Average Sentence Length')\nplt.xticks(rotation=90)\nplt.bar(x=df.location, height=averageSentenceLength)\nplt.show","a3b6c287":"from wordcloud import WordCloud, STOPWORDS\nstopwordsNLTK = nltk.corpus.stopwords.words(\"english\")\ndef show_wordcloud(data, title=\"\", mask=None, color=\"white\"):\n    text = \" \".join(t for t in data.dropna())\n    stopwords = set(STOPWORDS)\n    stopwords.update(stopwordsNLTK)\n    wordcloud = WordCloud(stopwords=stopwords, scale=4, max_font_size=50, max_words=500,mask=mask,background_color=color).generate(text)\n    fig = plt.figure(1, figsize=(16,16))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=20)\n    fig.subplots_adjust(top=1.0)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.show()\n    \nshow_wordcloud(df.text.str.lower(), 'Rally')","5a646e00":"#print(sentences[0])   # first text split into sentences","577f3659":"example_text = ' '.join(df.text)","1ea1bcc8":"sid = SentimentIntensityAnalyzer()\nupdated_words = {}\nsid.lexicon.update(updated_words)\nstopwordsNLTK = nltk.corpus.stopwords.words(\"english\")\nstopwordsNLTK.append('.')\nstopwordsNLTK.append(',')\nstopwordsNLTK.append('\\'s')\nstopwordsNLTK.append('\\'re')\nstopwordsNLTK.append('?')\nstopwordsNLTK.append('n\\'t')\nstopwordsNLTK.append('\\'ve')\nstopwordsNLTK.append('``')\nstopwordsNLTK.append('\\'\\'')\nstopwordsNLTK.append('\\'m')\nstopwordsNLTK.append('\\'ll')\ntokenized_example_text = nltk.word_tokenize(example_text)\nwords = [w for w in tokenized_example_text if w.lower() not in stopwordsNLTK]","ea012068":"frequency = nltk.FreqDist(words)\nfrequency.most_common(20)","e1d4cf85":"all_sentences = []\nfor texts in sentences:\n    for sentence in texts:\n        all_sentences.append(sentence)","86d5e8cc":"#print(sid.polarity_scores(sentences[0][0]))\nscores = []\nnegative_sentences = []\npositive_sentences = []\nreally_negative_sentences = []\nreally_positive_sentences = []\nfor sentence in all_sentences:\n    score = sid.polarity_scores(sentence)['compound']\n    scores.append(score)\n    if score < 0:\n        negative_sentences.append(sentence)\n    if score > 0:\n        positive_sentences.append(sentence)\n    if score < -0.8:\n        really_negative_sentences.append(sentence)\n    if score > 0.8:\n        really_positive_sentences.append(sentence)\n\nprint('Examples of really negative sentences: ')\nprint(really_negative_sentences[1:10])","8e4de6fd":"#print(scores)\n#print(negative_sentences)\n\ntokenized_negative_sentences = []\ntokenized_positive_sentences = []\nfor sentence in really_negative_sentences:\n    tokenized_negative_sentences.append(nltk.word_tokenize(sentence))\n\ntokenized_neg_sentences = []\nfor sentence in tokenized_negative_sentences:\n    for word in sentence:\n        tokenized_neg_sentences.append(word)\n\nfor sentence in really_positive_sentences:\n    tokenized_positive_sentences.append(nltk.word_tokenize(sentence))\n\ntokenized_pos_sentences = []\nfor sentence in tokenized_positive_sentences:\n    for word in sentence:\n        tokenized_pos_sentences.append(word)\n","deb2e6fc":"# analyzing negative sentences by frequency\n\nnegative_words = [w for w in tokenized_neg_sentences if w.lower() not in stopwordsNLTK]\nnegative_frequency = nltk.FreqDist(negative_words)\nnegative_frequency.most_common(50)","69f6f7fe":"# analyzing frequency of words in positive sentences\n\npositive_words = [w for w in tokenized_pos_sentences if w.lower() not in stopwordsNLTK]\npositive_frequency = nltk.FreqDist(positive_words)\npositive_frequency.most_common(50)","2f9378f3":"def analyzeSentimentScore(word):\n    sentiment = []\n    sentences = []\n    i = 0\n    score = 0\n    for sentence in all_sentences:\n        if word in sentence:\n            sentiment_score = sid.polarity_scores(sentence)['compound']\n            if sentiment_score != 0:\n                sentiment.append(sentiment_score)\n            sentences.append(sentence)\n            score += sentiment_score\n            i += 1\n    total_score = score \/ i\n    return sentiment, sentences","197a3255":"keywords = ['biden', 'democrats', 'republicans', 'kamala','economy', 'america', 'left', 'socialism', 'nation', 'suburban', 'urban', 'bernie']\nkeyword_dataframe = pd.DataFrame()\nfor i in range(len(keywords)):\n    keyword_dataframe[keywords[i]] = pd.Series(analyzeSentimentScore(keywords[i])[0])\n    \nprint(len(keyword_dataframe))","3872c7df":"keyword_dataframe.head()","121a2c06":"def getScores(keywords):\n    scores = []\n    for word in keywords:\n        scores.append(analyzeSentimentScore(word))\n    return scores","f9e089af":"# concordance\ntext = nltk.Text(tokenized_example_text)\ntext.concordance('radical', width=150)","2bfa51af":"plt.figure(figsize = (20, 10))\nplt.title('Sentiment Score of Individual Sentences by Keyword')\nmy_order = keyword_dataframe.mean()\nmy_order = list(my_order.sort_values().index)\nax = sns.violinplot(data=keyword_dataframe, order=my_order, palette='Set2', bw=0.4).set(title='Sentiment Score of Individual Sentences by Keyword')","fd070b83":"plt.figure(figsize = (20, 10))\nsns.set_style('dark')\nax = sns.swarmplot(data=keyword_dataframe, palette='dark', order=my_order)","1f0c2366":"def get_n_grams(text: str, n:int):\n    n_grams = list()\n    text_tokens = text.split(' ')\n    for index, token in enumerate(text_tokens):\n        if index+n < len(text_tokens):\n            n_grams.append(tuple(text_tokens[index:index+n]))\n    return n_grams","515f2164":"# creating bi grams\ntext = ' '.join(words)\nbi_grams = get_n_grams(text, 2)\nbi_grams_freq = nltk.FreqDist(bi_grams)\nbi_grams_sorted = sorted(bi_grams_freq , key = bi_grams_freq.__getitem__, reverse = True)\nbi_grams_sorted = bi_grams_sorted[:20]\n\n# plotting the bi grams\nbi_frequencies = []\nbi_words = []\nfor item in bi_grams_sorted:\n    bi_frequencies.append(bi_grams_freq[item])\n    bi_words.append(' '.join(item))\n    \n\nplt.figure(figsize=(15, 10))\nplt.barh(y=bi_words, width=bi_frequencies)\nplt.gca().invert_yaxis()\nplt.title(\"Most Common Bi Grams\")","ead5f49e":"tri_grams = get_n_grams(text, 3)\ntri_grams_freq = nltk.FreqDist(tri_grams)\ntri_grams_sorted = sorted(tri_grams_freq , key = tri_grams_freq.__getitem__, reverse = True)\ntri_grams_sorted = tri_grams_sorted[:20]\ntri_frequencies = []\ntri_words = []\nfor item in tri_grams_sorted:\n    tri_frequencies.append(tri_grams_freq[item])\n    tri_words.append(' '.join(item))\n\nplt.figure(figsize=(15, 10))\nplt.barh(y=tri_words, width=tri_frequencies)\nplt.gca().invert_yaxis()\nplt.title(\"Most Common Tri Grams\")","5098e5b5":"# **Most common words pulled from sentences with > 0.8 sentiment score**","495a712b":"# **Getting most common tri grams**","fe1b07f6":"# **Getting the data in a dataframe with location, month, year, and text**","60ad0d17":"**Concordance checker:**","c67248cb":"# **Finding average sentiment score based on keyword (you can modify this list to see how he feels about certain topics)**","205f4b38":"# **Most used words in all rallies**","44ff4610":"# **Getting most common bi grams**","d46fbb5b":"# **Average sentence length in each rally**","1361c7a4":"**List of most common words:**","b92cd97f":"# **Segmenting sentences based on their sentiment analysis score: **","2beff08d":"# **Most common words pulled from sentences with sentiment score < -0.8**","dd8bd5a6":"# **Plotting sentiment by keyword (lowest to highest):**","93c3c6b4":"# **Initializing sentiment analysis and removing stop words**"}}