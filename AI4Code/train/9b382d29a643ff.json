{"cell_type":{"f07c1cc8":"code","e2f6259a":"code","7b08c53a":"code","eca620ee":"code","fa6c8df9":"code","30ff3a0c":"code","9c7fd0a8":"code","3619e87a":"code","229e3b99":"code","760c4ab1":"code","8db17498":"code","da4a9f23":"code","efaf7c42":"code","ead2d32b":"code","69923708":"markdown","3eb5fe3d":"markdown","aba21125":"markdown","0230553f":"markdown","bfa85705":"markdown","3201ade0":"markdown","c411c443":"markdown","47cabfbf":"markdown"},"source":{"f07c1cc8":"# installing the gdown package\n! conda install -y gdown","e2f6259a":"import gdown\n\n!gdown  https:\/\/drive.google.com\/uc?id=1-4cbmjy9LmUiR8-V3905oZquU0mIGuOZ\n!gdown  https:\/\/drive.google.com\/uc?id=1-5ZP_djvEH9-WGp1jdk-KBAi8yMnbQjt\n!gdown  https:\/\/drive.google.com\/uc?id=1-7jMARlD5EkPWiKgXsqeN8b76NlHD2EL\n\n!gdown  https:\/\/drive.google.com\/uc?id=1-Dp8LamSxKhBGrC1ObWq9zwBMb2jKxXI\n!gdown  https:\/\/drive.google.com\/uc?id=1-TDB9Adm8PDLUcHh2dvu9bdy8wf081N6\n!gdown  https:\/\/drive.google.com\/uc?id=1-WN5SCMeyftsxHqgeqp1Bf8ReTUo6mEG","7b08c53a":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\n\nimages_train = np.load(\".\/images_train.npy\") \/ 255\nimages_valid = np.load(\".\/images_valid.npy\") \/ 255\nimages_test = np.load(\".\/images_test.npy\") \/ 255\nlabels_train = np.load(\".\/labels_train.npy\")\nlabels_valid = np.load(\".\/labels_valid.npy\")\nlabels_test= np.load(\".\/labels_test.npy\")\n\nprint(\"{} training data examples\".format(images_train.shape[0]))\nprint(\"{} validation data examples\".format(images_valid.shape[0]))\nprint(\"{} test data examples\".format(images_test.shape[0]))","eca620ee":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D , MaxPool2D\ndef get_compiled_model(compile=True):\n  ''' prepare and compile the model '''\n  model = Sequential()\n  model.add(Conv2D(32, (3,3), activation='relu', padding='SAME', input_shape=(160,160,3)))\n  model.add(Conv2D(32, (3,3), activation='relu', padding='SAME'))\n  model.add(MaxPool2D(2,2))\n  model.add(Conv2D(64, (3,3), activation='relu', padding='SAME'))\n  model.add(Conv2D(64, (3,3), activation='relu', padding='SAME'))\n  model.add(MaxPool2D(2,2))\n  model.add(Flatten())\n  model.add(Dense(128, activation='relu'))\n  model.add(Dense(1, activation='sigmoid'))\n  \n  if compile is True:\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              loss=tf.keras.losses.BinaryCrossentropy(), \n              metrics=[tf.keras.metrics.BinaryAccuracy(name='acc')])\n  return model\n\nmodel = get_compiled_model()\n\n# inspecting the model architecture\nmodel.summary()","fa6c8df9":"import datetime\nfrom tensorflow.keras.callbacks import Callback\nclass CustomCallback(Callback):\n  def on_train_begin(self,logs=None):\n    print(\"Training is started, at time {}\".format(datetime.datetime.now().time()))\n  def on_train_end(self, logs=None):\n    print(\"Training is ended at {}\".format(datetime.datetime.now().time()))\n  def on_train_batch_begin(self, batch, logs=None):\n    print('Training: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\n  def on_train_batch_end(self, batch, logs=None):\n    print('Training: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\n\ncustom_callback = CustomCallback()\n\nmodel = get_compiled_model()\nmodel.fit(images_train, labels_train, validation_data=(images_valid, labels_valid), \n          epochs=1, callbacks=[custom_callback])","30ff3a0c":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', \n                               min_delta=0.001, \n                               patience=2, \n                               verbose=0, \n                               mode='min', \n                               baseline=None, \n                               restore_best_weights=False)\n\nmodel = get_compiled_model()\nmodel.fit(images_train, labels_train, \n          validation_data=(images_valid, labels_valid), \n          epochs=80, \n          callbacks=[early_stopping])","9c7fd0a8":"from tensorflow.keras.callbacks import ReduceLROnPlateau\ncallback  = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.1, \n                              patience=10, \n                              verbose=0, \n                              mode='auto', \n                              min_delta=0.002, \n                              cooldown=0, \n                              min_lr=0)\n\nmodel = get_compiled_model()\nmodel.fit(images_train, labels_train, \n          validation_data=(images_valid, labels_valid), \n          epochs=20, \n          callbacks=[callback])","3619e87a":"from tensorflow.keras.callbacks import ModelCheckpoint\nmodel_checkpoint_callback = ModelCheckpoint(filepath= \"model.h5\", \n                                            monitor='val_loss', \n                                            verbose=0, \n                                            save_best_only=False, \n                                            save_weights_only=False, \n                                            mode='min', \n                                            save_freq='epoch')\nmodel = get_compiled_model()\nmodel.fit(images_train, labels_train, \n          validation_data=(images_valid, labels_valid), \n          epochs=20, \n          callbacks=[model_checkpoint_callback])\n\n# loading the model from the disk.\nmodel.load_weights(\"model.h5\")","229e3b99":"# inspecting the architecture .\nmodel.summary()","760c4ab1":"# evaluating the model on the test set.\nmodel.evaluate(images_test, labels_test)","8db17498":"from tensorflow.keras.callbacks import LearningRateScheduler\n\n\ndef lr_function(epoch, lr):\n    if epoch % 2 == 0:\n        return lr\n    else:\n        return lr + epoch\/1000\n\nlearning_rate_schedular_callback = LearningRateScheduler(schedule= lr_function ,\n                                                         verbose=1)\n\nmodel = get_compiled_model()\n\nmodel.fit(images_train, labels_train, \n          validation_data=(images_valid, labels_valid), \n          epochs=10, \n          callbacks=[learning_rate_schedular_callback] )\n","da4a9f23":"# making a custom callback for the \nclass CustomCallback(tf.keras.callbacks.Callback):\n\n    def __init__(self, metrics_dict, num_epochs='?', log_frequency=1,\n                 metric_string_template='\\033[1m[[name]]\\033[0m = \\033[94m{[[value]]:5.3f}\\033[0m'):\n        super().__init__()\n\n        self.metrics_dict = collections.OrderedDict(metrics_dict)\n        self.num_epochs = num_epochs\n        self.log_frequency = log_frequency\n\n        log_string_template = 'Epoch {0:2}\/{1}: '\n        separator = '; '\n\n        i = 2\n        for metric_name in self.metrics_dict:\n            templ = metric_string_template.replace('[[name]]', metric_name).replace('[[value]]', str(i))\n            log_string_template += templ + separator\n            i += 1\n\n        log_string_template = log_string_template[:-len(separator)]\n        self.log_string_template = log_string_template\n\n    def on_train_begin(self, logs=None):\n        print(\"Training: \\033[92mstart\\033[0m.\")\n\n    def on_train_end(self, logs=None):\n        print(\"Training: \\033[91mend\\033[0m.\")\n\n    def on_epoch_end(self, epoch, logs={}):\n        if (epoch - 1) % self.log_frequency == 0 or epoch == self.num_epochs:\n            values = [logs[self.metrics_dict[metric_name]] for metric_name in self.metrics_dict]\n            print(self.log_string_template.format(epoch, self.num_epochs, *values))\n","efaf7c42":"import collections\nimport functools\n\n \nmetrics_to_print = collections.OrderedDict([(\"loss\", \"loss\"), \n                                             (\"acc\", \"acc\"),\n                                            (\"v-loss\", \"val_loss\"),\n                                            (\"v-acc\", \"val_acc\"),\n                                            ])\n\n\ncustom_callback = CustomCallback(metrics_to_print, num_epochs=5)","ead2d32b":"model.fit(images_train, labels_train, validation_data=(images_valid, labels_valid), epochs=5, \n          callbacks=[custom_callback], verbose=False )","69923708":"## 3. ReduceLROnPlateau.\n\nThis callback is used to reduce the learning rate if there is \nnot any improvement in the loss\/metric.\n\nThe arguments are:\n\n* \u201cmonitor\u201d it\u2019s set to that loss\/metric as a string\n of which we are reducing the learning if it\u2019ll not improve.\n\n* \u201cfactor\u201d You can pass an integer in this argument,\nand say your current learning rate is LR, then if\nthere is not any improvement seen in the monitored loss\/metric,\nthen the learning is going to decrease by that \u201cfactor.\u201d\ni.e new learning rate = lr * factor\n\n* \u201cVerbose\u201d\nYou can set verbose =1 to see the learning rate at every epoch.\nOr verbose = 0 to disable it.\n\nThe argument min_delta and mode are the same as explained in the arguments of EarlyStopping Callback.","3eb5fe3d":"\n## 2. EarlyStopping Callback.\n\nSuppose we don\u2019t know about the callbacks, and you want to prevent the overfitting of the model caused by training our model into extra epochs(we\u2019re not god so that we know at how many epochs our model is going to converge). So, we plot the val_loss vs. epochs graph and examine\nhow many epochs it\u2019s started overfitting the data. Then we\u2019ll re-train our model in less than that epoch number.\nWhat if I\u2019ll tell you don\u2019t have to do this thing manually.\nYes, you can do this by using EarlyStopping Callback.\nSo, let\u2019s see how one can use this callback.\n\nFirst, import the callback, and then create the instance of the\nEarlyStopping callback and pass the arguments as per our needs.\nLemme explain these arguements .\n\n* \u201cmonitor\u201d you can pass the loss or the metric.\nGenerally, we pass val_loss and monitor it.\n\n* \u201cmin_delta\u201d you can pass an integer in this argument.\nIn simple words, you\u2019re telling the callback that the model\nis not improving if it\u2019s not decreasing more\/less than the loss\/metrics.\n\n* \u201cpatience,\u201d it means about how many epochs to wait.\nAnd after that, if there is no improvement seen in the\nmodel performance according to the value of \u201cmin delta,\u201d then stop the training.\n\n* \u201cmode\u201d\nBy default it\u2019s set to \u2018auto\u2019 this comes handy when\nyou\u2019re dealing with the custom loss\/metric. So, you can \ntell the callback whether the model is improving when\nits custom loss\/metric is decreasing then set it to \u201cmin\u201d \nor increasing then set it to \u201cmax.\u201d","aba21125":"## 1. Custom callbacks by subclassing callback class.\n\nThese callbacks come under the base class \u201ctf.keras.callbacks.\u201d\nBy subclassing these callbacks, we can perform certain functions when the training\/batch\/epochs have started or ended.\nFor this, we can override the function of callback classes.\nThe name of these functions is self explain their behavior.\nFor example def on_train_begin(), this means what to do when\ntraining will begin.\nLet\u2019s see below how to override these functions. We can\nalso, monitor logs and perform certain actions, generally at \nthe starting or the ending of the training\/batch\/epochs.","0230553f":"###  <span style=\"color:red\">If you like this kernel then please upvote. It'll motivate me for making more kernels like this.<\/span>","bfa85705":"## 4. ModelCheckpoint\n\nLet\u2019s imagine you\u2019re training a heavy model like Bert in colab,\nand it requires a lot of time for training. So, you started the model training and went for sleep. And then the next morning\nyou wake up, and you open your colab.\nBut you\u2019ll see the \u201cRuntime Disconnect\u201d message on your screen.\nSounds like a nightmare tough?\nFor this problem, ModelCheckpoint comes as a savior in our life. We can save the checkpoints at the end of every epoch.\nSo, that we can load the weights or resume the training if \nsomething terrible happens while training.\n\nSo, let\u2019s see how we can use this callback. We can save\nthe model checkpoint in Keras h5\/hd5 format or TensorFlow pb\nformat. If you pass the argument \u201cfilepath= model.h5\u201d(.h5 extension)\nit\u2019ll be saved in the Keras format or \u201cfilepath= model.p\u201d(.pb extension)\nfor saving in the TensorFlow model format.\n\nAlso, there are two options to save the checkpoint either you can save the entire architecture+weights or just the weights. You can do this by setting \u201csave_only_weights=True\u201d or \u201csave_only_weights=False\u201d","3201ade0":"## 5. LearningRateScheduler\n>  The simplest way to schedule the learning is to decrease the learning rate \nlinearly from a large initial value to a small value. \nThis allows large weight changes at the beginning of the \nthe learning process and small changes or fine-tuning towards\nthe end of the learning process.\n\nLet\u2019s see how to schedule the learning rate. For this, we have to\ndefine an auxiliary function that contains the rules for\nalternating the learning rate. \nAnd then we can simply pass the name of this auxiliary function\nto the argument of the object of the LearningRateScheduler class.","c411c443":"## TensorFlow Callbacks in Action\n![](https:\/\/cdn-images-1.medium.com\/max\/4522\/1*c5mBC2KTs0oQ_SRGnbtuLA.jpeg)\n\nIn layman terms, if I want to introduce callbacks, then it\u2019s the controller by which you can control your plane. Without these controllers, you\u2019re not having any control over the plane, and you\u2019ll crash.\n\nCallbacks: from keras.io, a callback is an object that can perform actions at various stages of training (e.g., at the start or end of an epoch, before or after a single batch, etc.).\n\nIt means that callbacks are the functions by which you can perform a particular task during the training\nprocess of your model. \nSo, what can you do with these callbacks?\n1. You can perform a particular task after the starting and ending of the training\/batch\/ epochs.\n2. You can periodically save the model states in the disk.\n3. You can schedule the learning rate as per your task.\n4. You can automatically stop the training when a particular condition becomes True.\n5. And you can do anything during the training process by subclassing these callbacks.\n\nFor example, you can make your training output clean and colorful like this, pretty awesome, right?\n\n![](https:\/\/cdn-images-1.medium.com\/max\/2974\/1*RnOcxRCIhX7gtX7nv9RdGQ.png)\n\nTensorflow provides a wide range of callbacks under the base class \u201ctf.keras.callbacks. \u201cFor the full list of callbacks please visit [TensorFlow\u2019s website](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/Callback).\n\nIn this article, we\u2019re going to cover some of the essential TensorFlow callbacks and how to use them to have full control over the training.\n\nThe context of this article are:-\n1. custom callbacks by subclassing callback class.\n2. Early stopping callback.\n3. Model checkpoint callback.\n4. ReduceOnPlateu callback.\n5. Learning rate Scheduler.\n6. Bonus package for making the output clean and colorful, as shown above.\n\nBut let\u2019s first load the cats_vs_dogs dataset, I\u2019ve been using the very small subclass of the original dataset. And then, let\u2019s define our model architecture using sequential API. Throughout this article, I\u2019m using this dataset and this model architecture.","47cabfbf":"#### Lastly here is the custom callback for making training output neat and colorful.\nps:- You can even use the below script as a utility file. Get this utility file from my github. \\\n\n### [Repository on Github](https:\/\/github.com\/abhinavsp0730\/callback_blog)"}}