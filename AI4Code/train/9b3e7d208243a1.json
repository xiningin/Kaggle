{"cell_type":{"11dca2c6":"code","27f165c9":"code","87925abf":"code","2be70b62":"code","1217c6df":"code","c155da4a":"code","d8d42f1f":"code","10a1304d":"code","049587be":"code","a5f704ae":"code","0e18064c":"code","785c85a0":"code","6b890f73":"code","61beb4f1":"code","8510db4b":"code","f04fa060":"code","4ce981ec":"code","e9c47a90":"code","4a7bb522":"code","151965a0":"code","31626dc0":"code","c34f4b93":"code","cc262194":"code","e7484c8f":"code","5bddf4c5":"code","e628e5ee":"code","9b717375":"code","c2919d41":"code","97aefd29":"code","5322d356":"code","a046d275":"code","c3becea4":"markdown","8dce9e21":"markdown","3ae02acd":"markdown","40cf551d":"markdown","f41a0079":"markdown","e36ccabe":"markdown","85b31681":"markdown","3490f068":"markdown","216c80a8":"markdown","b4e6467a":"markdown","ab55c9c2":"markdown","455e43c9":"markdown","87fabb14":"markdown","317dcaed":"markdown","244a8603":"markdown","96f00448":"markdown","1ce26986":"markdown","cb3715a7":"markdown","60c0bc79":"markdown","7fd45012":"markdown","c2c9fba6":"markdown","56c62b32":"markdown","6c4f4df6":"markdown","2d7867fb":"markdown","199c75ac":"markdown","638c4b34":"markdown","ef751b12":"markdown","5132213d":"markdown","5506669f":"markdown","384b9020":"markdown"},"source":{"11dca2c6":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Visualization\n!pip install dataprep | grep -v 'already satisfied'\nfrom dataprep.eda import plot, plot_diff, plot_correlation, create_report\n\n# Preprocessing and Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras tuner\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n# Warning\nimport warnings\nwarnings.filterwarnings('ignore')","27f165c9":"train_full = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_full = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()\/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()\/2**20))","87925abf":"plot(train_full)","2be70b62":"create_report(train_full)","1217c6df":"plot(train_full, 'text')","c155da4a":"create_report(train_full.text)","d8d42f1f":"create_report(train_full.target)","10a1304d":"plot(train_full, \"text\", \"target\")","049587be":"df1 = train_full.text[train_full.target == 0]\ndf2 = train_full.text[train_full.target == 1]\nplot_diff([df1, df2])","a5f704ae":"# free some space\ndel train_full, test_full\n\n# Read commited-dataset\ndf_train = pd.read_csv(\"\/kaggle\/input\/disastertweet-prepared2\/train_prepared.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/disastertweet-prepared2\/test_prepared.csv\")","0e18064c":"# Instantiate the Vectorizer\nvect = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0, max_df=0.9, max_features=100)\ndf_dtm = vect.fit_transform(df_train)\ndf_dtm.toarray()[0]","785c85a0":"tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0, max_df=0.98, max_features=100)\ndf_ifidf= tfidf_vect.fit_transform(df_train)\ndf_ifidf.toarray()[0]","6b890f73":"BATCH_SIZE=64","61beb4f1":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","8510db4b":"max_len = 0\n# Find the longest sentence \nfor sentence in pd.concat([df_train.text, df_test.text]):\n    if len(sentence) > max_len: # number of word in a sentence tokenizer is greater max_len\n        max_len = len(sentence)\nmax_len","f04fa060":"# Using for Fine_tuning\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(df_train.text, \n                                                                    df_train.target, \n                                                                    test_size=0.2, \n                                                                    random_state=42)\n# Use padding with max_len to get train\/val\/test with same dimension\ntrain_encodings = tokenizer(train_texts.tolist(), truncation=True, max_length=max_len, padding=\"max_length\", return_tensors='tf')\nval_encodings = tokenizer(val_texts.tolist(), truncation=True, max_length=max_len, padding=\"max_length\", return_tensors='tf')\ntest_encodings = tokenizer(df_test.text.fillna('').tolist(), truncation=True, max_length=max_len, padding=\"max_length\", return_tensors='tf')\n\nprint(train_encodings)","4ce981ec":"# Using for Keras-tuner\ntrain_encodings_keras = tokenizer(df_train.text.tolist(), truncation=True, max_length=max_len, padding=\"max_length\", return_tensors=\"tf\")\ntrain_encodings_keras","e9c47a90":"train_encodings_keras['input_ids']","4a7bb522":"# Encode Training Data\nX_train_ids = train_encodings['input_ids'].numpy()\nX_train_attention = train_encodings['attention_mask'].numpy\n# Encode Validating Data\nX_val_ids = val_encodings['input_ids'].numpy()\nX_val_attention = val_encodings['attention_mask'].numpy()","151965a0":"train_tf_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\ntrain_tf_dataset = train_tf_dataset.shuffle(len(train_encodings)).batch(BATCH_SIZE)\n\neval_tf_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels))\neval_tf_dataset = eval_tf_dataset.batch(BATCH_SIZE)","31626dc0":"from transformers import TFDistilBertModel, DistilBertConfig\n\nBERT_DROPOUT = 0.2\nBERT_ATT_DROPOUT = 0.2\n \n# Configure DistilBERT's initialization\nconfig = DistilBertConfig(dropout=BERT_DROPOUT, \n                          attention_dropout=BERT_ATT_DROPOUT, \n                          output_hidden_states=True)","c34f4b93":"# Model function\ndef create_model(transformer):\n    \n    # Make Transformer layers untrainable\n    for layer in transformer.layers:\n        layer.trainable = False\n    # Input layers\n    input_ids_layer = keras.Input(shape =(max_len,), \n                           dtype=tf.int32, \n                           name='input_ids') \n    input_attention_layer = keras.Input(shape=(max_len,),\n                                    dtype=tf.int32, \n                                    name='attention_mask')  \n    \n    # DistilBERT outputs a tuple where the first element at index 0\n    # represents the hidden-state at the output of the model's last layer.\n    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n    \n    # We only care about DistilBERT's output for the [CLS] token, \n    # which is located at index 0 of every encoded sequence.  \n    # Splicing out the [CLS] tokens gives us 2D data.\n    cls_token = last_hidden_state[:, 0, :]\n    # Hidden layers\n    output = keras.layers.Dense(256,\n                                kernel_initializer=keras.initializers.GlorotUniform(seed=1),  \n                                kernel_constraint=None,\n                                bias_initializer='zeros',\n                                activation='relu')(cls_token)\n    output = keras.layers.Dropout(0.2)(output)\n    output = keras.layers.Dense(64, activation = 'relu')(output)\n    # Output layer\n    output = keras.layers.Dense(1, activation='sigmoid')(output)\n    # Define the model \n    model = keras.Model([input_ids_layer, input_attention_layer],\n                       output)\n    model.summary()\n    keras.utils.plot_model(model)\n    \n    return model","cc262194":"def distilBERT_NN_tuner(hp):\n\n    # The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n    # and without any specific head on top.\n    distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n    model = create_model(distilBERT)\n    # Using learning_rate is recommendated from paper BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding\n    hp_learning_rate = hp.Choice('learning_rate', values=[5e-5, 4e-5 , 3e-5, 2e-5])\n    optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n    # Compile the model\n    model.compile(optimizer, \n                  loss=\"binary_crossentropy\",\n                  metrics=['accuracy'])\n    return model","e7484c8f":"tuner = kt.RandomSearch(distilBERT_NN_tuner,\n                objective='val_accuracy')\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n                                           patience=4, \n                                           restore_best_weights=True)\ntuner.search(train_tf_dataset,\n                epochs=25,\n                batch_size=BATCH_SIZE,\n                validation_data=eval_tf_dataset,\n                callbacks = [early_stop],\n                verbose=2)\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"\"\"The hyperparameter search is complete. The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\"\"\")","5bddf4c5":"# Running with specific number\nDistilBERTmodel = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\nmodel = create_model(DistilBERTmodel)\n# Compile the model\nmodel.compile(keras.optimizers.Adam(lr=best_hps.get('learning_rate')), \n              loss=\"binary_crossentropy\",\n              metrics=['accuracy'])\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=4, restore_best_weights=True)\n\ntrain_history1 = model.fit(train_tf_dataset,\n                           epochs=25,\n                           batch_size=BATCH_SIZE,\n                           validation_data=eval_tf_dataset,\n                           callbacks = [early_stop],\n                           verbose=2)","e628e5ee":"for layer in DistilBERTmodel.layers:\n    layer.Trainable = True","9b717375":"model.compile(keras.optimizers.Adam(lr=1e-5), \n              loss=\"binary_crossentropy\",\n              metrics=['accuracy'])","c2919d41":"early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=2, restore_best_weights=True)\ntrain_history2 = model.fit(train_tf_dataset,\n                               epochs=25,\n                               batch_size=BATCH_SIZE,\n                               validation_data=eval_tf_dataset,\n                               callbacks = [early_stop],\n                               verbose=2 )","97aefd29":"def submission(model, test):\n    sample_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n    predictions =  model.predict(test.data, batch_size=BATCH_SIZE, verbose =1)\n    y_preds = [ int(i) for i in np.rint(predictions)]\n    sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_preds})\n    sub.to_csv('submission.csv', index=False)","5322d356":"submission(model, test_encodings)","a046d275":"pd.read_csv('submission.csv')","c3becea4":"<a id=5.1 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.1 Tokenizing Text<\/p>\n\n[Content](#0)","8dce9e21":"# Main technics I used in this data\n    * [3.1] Remove 92 duplicated rows\n    * [3.2] Cleaning text\n    * [3.3] Spelling Checker\n    * [3.4] Remove Stemming\n #### Step 3.3 spends a lot time (around 4000s in 4536s in total). \n #### So, I splits Data Preprocessing into [another kernel](https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepareddata). \n #### And the prepared data to save in to [new dataset](https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepared2)\n #### I am so appreciate to you for using\/upvoting it.\n","3ae02acd":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">0. Introduction and update <\/p>\n# Introduction: \nIn this kernel, beside the general steps working with text data as EDA, preprocessing. The workflow in Modelling can divided into 3 main stages:\n1. Defining a Model Architecture.\n2. Training Classification Layer Weights.\n3. Fine-tuning DistilBert and Tranining All Weights.\n\n# Update: \nCurrent Version\n1. Use Keras-tuner to find the optimized learning rate for main model.\n\n[Content](#0)","40cf551d":"<a id=4.2 ><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4.2 TF-IDF<\/p>\nReference: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n\n[Content](#0)","f41a0079":"<a id=6 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">6. Make a Submission<\/p>\n\n[Content](#0)","e36ccabe":"## Recompile model after unfreezing\n\nThe lower learning-rate is chosen because of preventing the major update to pre-trained weights.","85b31681":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Pre-processing <\/p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.\n\n\n\n\n[Content](#0)","3490f068":"<img src=\"https:\/\/i.ibb.co\/4tzyG1P\/Bert-Classification.png\" alt=\"Bert-Classification\" border=\"0\">","216c80a8":"### Unfreeze all layer weights in distilBERT and make available for training","b4e6467a":"# If you like this kernel, please upvote and tell me your thought. Thank you @@","ab55c9c2":"### Dataset is balanced","455e43c9":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data \ud83d\udc8e<\/p>\n\nJust load the dataset and global variables for colors and so on.\n\n[Content](#0)","87fabb14":"## Training the model again","317dcaed":"<a id=7 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">7. References<\/p>\n\n[Content](#0)","244a8603":"BERT(*Bi-directional Encoder Representations from Transformers*)\n\n    - GLUE Score to 80.5%\n    - MultiNLI accuracy to 86.7%\n    - SQuAD v1.1 question answering Test F1 to 93.3\n    - SQuAD v2.0 Test F1 to 83.1","96f00448":"<a id=0><\/a>\n## <p style=\"background-color:lightblue; font-family:newtimeroman; font-size:120%; text-align:left; border-radius: 15px 50px;\">Table of Content<\/p>\n* [0. Introduction and updates](#0)\n* [1. Loading Data \ud83d\udc8e](#1)\n* [2. EDA \ud83d\udcca](#2)\n* [3. Data Preprocessing](#3)\n* [4. Vectorization](#4)\n    * [4.1 Common Vectorizer Usage](#4.1)\n    * [4.2 If-Idf Term Weightings](#4.2)\n* [5. Transfer Learning with Hugging Face](#5)\n    * [5.1 Tokenization](#5.1)\n    * [5.2 Defining a Model Architecture](#5.2)\n    * [5.3 Training Classification Layer Weights](#5.3)\n    * [5.4 Fine-tuning DistilBert and Training All Weights](#5.4)\n* [6. Make a Submission](#6)\n* [7. References](#7)","1ce26986":"<a id=5.4 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.4 Fine-tune DistilBERT model and Training all Weights<\/p>\n\n    1. Unfrezzing layer weights in DistilBERT model\n    2. Using lower learning rate to prevent large update to pre-trained weights\n    3. Recompile model again\n[Content](#0)","cb3715a7":"<a id=4.1 ><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4.1 Common Vectorizer Usage<\/p>\nReference: https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#common-vectorizer-usage\n\n[Content](#0)","60c0bc79":"# Initialize the Base Model","7fd45012":"# Add a Classification Head","c2c9fba6":"[Hugging Face Transformers Fine-Tunning DistilBert for Binary Classification Tasks](https:\/\/towardsdatascience.com\/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379)\n\n[Keras Tuner](https:\/\/keras.io\/keras_tuner)\n\n[Distil Bert](https:\/\/huggingface.co\/transformers\/model_doc\/distilbert.html)\n","56c62b32":"# Running model with the best Learning Rate","6c4f4df6":"<a id=5.2 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.2 Define a model based in DistilBERT<\/p>\n\nIn this part, I try a lighter model than BERT: \n\n[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https:\/\/arxiv.org\/abs\/1910.01108)\n\n[Content](#0)\n","2d7867fb":"<a id=4 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4. Vectorization<\/p>\n\nThree steps using the Bag-of-words (BOW) model:\n1. Term frequency : count occurrences of word in sentence\n2. Inverse document frequency: \n3. L2 Norm\nReference : https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction\n\n[Content](#0)","199c75ac":"### RandomSearch","638c4b34":"<a id=5 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">5. Transfer Learning with Hugging Face<\/p>\n\n[Content](#0)","ef751b12":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA \ud83d\udcca<\/p>\n\n\n[Content](#0)","5132213d":"<a id=5.3 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.3 Training Classification Layer Weights<\/p>\n\n[Content](#0)","5506669f":"### Range from 120 to 140 characters is the most common in tweet.","384b9020":"# Using Keras-Tuner to find the best Learning-rate"}}