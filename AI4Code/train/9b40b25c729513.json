{"cell_type":{"0691bcc0":"code","e0280da2":"code","164258cc":"code","62657389":"code","516da086":"code","4f8d6b34":"code","7a2ef62d":"code","cbca4839":"code","ea841fc2":"code","90c21497":"code","b80a2182":"code","45fe016c":"code","4392c555":"code","8306e3ec":"code","efae7755":"markdown","54e6aed2":"markdown","dcabf845":"markdown","53cf589c":"markdown","87db1eb6":"markdown","ce158e19":"markdown","851cd644":"markdown"},"source":{"0691bcc0":"import os\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = ['svg']\n%matplotlib inline","e0280da2":"DATA_DIR = '..\/input\/predictive-maintenance'\nFILE = 'ai4i2020.csv'\ndf = pd.read_csv(os.path.join(DATA_DIR, FILE))","164258cc":"df.info()","62657389":"df.head()","516da086":"df['Machine failure'].value_counts().plot(kind='pie', autopct='%1.1f%%')\nplt.show()","4f8d6b34":"df[df['Machine failure'] == 1][['TWF', 'HDF', 'PWF', 'OSF', 'RNF']].apply(pd.value_counts)","7a2ef62d":"def plot_pair():\n    sns.pairplot(data=df.drop(['UDI', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'], axis=1).sample(1000).select_dtypes(include='number'),\n                 hue='Machine failure',\n                 plot_kws={'s':6},\n                 corner=True)\n    plt.show()\n\nplot_pair()","cbca4839":"import os\nimport time\nimport joblib\nfrom functools import partial, wraps\n\nimport numpy as np\nimport pandas as pd\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\n\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.metrics import roc_auc_score, plot_roc_curve, plot_precision_recall_curve\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ea841fc2":"def read_data(data_dir, file, target, cols):\n    \"\"\"\n    Read the tabular data and split it into input (x), output (y) components.\n    \"\"\"\n    df = pd.read_csv(os.path.join(data_dir, file), usecols=cols)\n    y = df[target]\n    x = df.drop([target], axis=1)\n    return x, y\n\n\ndef get_train_test_data():\n    \"\"\"\n    Hard code some of the variables as they won't change for different experiments.\n    \"\"\"\n    DATA_DIR = '..\/input\/predictive-maintenance'\n    TRAIN_FILE = 'train.csv'\n    TEST_FILE = 'test.csv'\n    COLS = ['Type', 'Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]',\n            'Tool wear [min]', 'Machine failure']\n    CATS = ['Type']\n    NUMS = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]',\n            'Tool wear [min]']\n    TARGET = 'Machine failure'\n\n    x_train, y_train = read_data(DATA_DIR, TRAIN_FILE, TARGET, COLS)\n    x_test, y_test = read_data(DATA_DIR, TEST_FILE, TARGET, COLS)\n    return x_train, y_train, x_test, y_test, NUMS, CATS\n\n\ndef get_preprocessor(est_name, nums, cats):\n    \"\"\"\n    We will need the transformers defined below for parameter tuning and then to obtain the best-fit model.\n    So we need this function to help us call the transformers.\n    \"\"\"\n    if est_name == 'cb':\n        preprocessor = ColumnTransformer(transformers=[('num', StandardScaler(), nums)],\n                                         remainder='passthrough')\n    elif est_name == 'lgb':\n        preprocessor = ColumnTransformer(transformers=[('num', StandardScaler(), nums),\n                                                       ('cat', OrdinalEncoder(), cats)],\n                                         remainder='passthrough')\n    elif est_name == 'xgb':\n        preprocessor = ColumnTransformer(transformers=[('num', StandardScaler(), nums),\n                                                       ('cat', OneHotEncoder(), cats)],\n                                         remainder='passthrough')\n    return preprocessor\n\n\ndef get_estimator(est_name, params):\n    \"\"\"\n    Estimators have to be instantiated inside the cross validation loops. This will help us to do that.\n    Alternatively one can instantiate an estimator and then clone it.\n    \"\"\"\n    if est_name == 'xgb':\n        estimator = xgb.XGBClassifier(**params)\n    elif est_name == 'lgb':\n        estimator = lgb.LGBMClassifier(**params)\n    elif est_name == 'cb':\n        estimator = cb.CatBoostClassifier(**params)\n    return estimator\n\n\ndef cross_validate(est_name, x, y, params, cv=None, method='predict_proba', return_model=False):\n    \"\"\"\n    When tuning the parameters of GBDTs, we will use early stopping. Sklearn's cross validation functions do not allow early stopping.\n    So we need to use this custom cross validator.\n    \"\"\"\n    if cv is None:\n        cv = StratifiedKFold(n_splits=5, random_state=84, shuffle=True)\n\n    oof_preds = np.zeros(len(y))\n    models = []\n\n    for fold, (train_index, validation_index) in enumerate(cv.split(x, y)):\n        x_train = x[train_index]\n        y_train = y[train_index]\n\n        x_validation = x[validation_index]\n        y_validation = y[validation_index]\n\n        estimator = get_estimator(est_name, params)\n        estimator.fit(x_train, y_train, eval_set=[(x_validation, y_validation)], early_stopping_rounds=10,\n                      verbose=False)\n\n        if method == 'predict_proba':\n            validation_pred = estimator.predict_proba(x_validation)\n        elif method == 'predict':\n            validation_pred = estimator.predict(x_validation)\n\n        oof_preds[validation_index] = validation_pred[:, 1]\n\n        if return_model:\n            models.append(estimator)\n\n    if return_model:\n        return {'oof_preds': oof_preds, 'models': models}\n    else:\n        return {'oof_preds': oof_preds}\n\n    \ndef define_objective(trial, x, y, est_name):\n    \"\"\"\n    This will define the objective function that optuna needs.\n    The parameters and their distributions are hard-coded here.\n    \"\"\"\n    if est_name == 'xgb':\n        params = {\n            'eval_metric': trial.suggest_categorical('eval_metric', ['auc']),\n            'n_estimators': trial.suggest_int('n_estimators', 300, 300),\n            'num_parallel_tree': trial.suggest_int('num_parallel_tree', 1, 5),\n            'max_depth': trial.suggest_int('max_depth', 2, 32),\n            'reg_alpha': trial.suggest_float('reg_alpha', 0, 20),\n            'reg_lambda': trial.suggest_float('reg_lambda', 0, 20),\n            'min_child_weight': trial.suggest_float('min_child_weight', 0, 5),\n            'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n            'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01),\n            'colsample_bynode': trial.suggest_discrete_uniform('colsample_bynode', 0.1, 1, 0.01),\n            'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.1, 1, 0.01),\n            'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 1, 0.05)}\n\n    elif est_name == 'lgb':\n        params = {\n            'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n            'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt']),\n            'metric': trial.suggest_categorical('metric', ['auc']),\n            'feature_pre_filter': trial.suggest_categorical('feature_pre_filter', [False]),\n            'reg_alpha': trial.suggest_float('reg_alpha', 0, 20),\n            'reg_lambda': trial.suggest_float('reg_lambda', 0, 20),\n            'num_leaves': trial.suggest_int('num_leaves', 2, 32),\n            'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.01),\n            'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 1, 0.01),\n            'subsample_freq': trial.suggest_int('subsample_freq', 7, 7),\n            'min_child_samples': trial.suggest_int('min_child_samples', 1, 30),\n            'early_stopping_round': trial.suggest_int('early_stopping_round', 10, 10),\n            'n_estimators': trial.suggest_int('n_estimators', 100, 100),\n            'verbosity': trial.suggest_categorical('verbosity', [-1])}\n\n    elif est_name == 'cb':\n        params = {\n            'loss_function': trial.suggest_categorical('loss_function', ['Logloss']),\n            'eval_metric': trial.suggest_categorical('eval_metric', ['AUC']),\n            'iterations': trial.suggest_int('iterations', 50, 50),\n            'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n            'depth': trial.suggest_int('depth', 6, 12),\n            'verbose': trial.suggest_categorical('verbose', [False]),\n            'early_stopping_rounds': trial.suggest_categorical('early_stopping_rounds', [10]),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0, 100),\n            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.8, 1),\n            'cat_features': trial.suggest_categorical('cat_features', [[5]])}\n\n    results = cross_validate(est_name, x, y, params, method='predict_proba')\n\n    return roc_auc_score(y_train, results['oof_preds'])\n\n\ndef tune_parameters(x_train, y_train, est_name, n_trials, nums, cats):\n    \"\"\"\n    This will tune the parameters of a given model and store the parameters in a file.\n    \"\"\"\n    preprocessor = get_preprocessor(est_name, nums, cats)\n    x_train = preprocessor.fit_transform(x_train)\n\n    study_name = 'study_' + est_name + '.pkl'\n    if os.path.exists(study_name):\n        study = joblib.load(study_name)\n    else:\n        sampler = optuna.samplers.TPESampler()\n        study = optuna.create_study(sampler=sampler, direction='maximize')\n\n    objective = partial(define_objective, x=x_train, y=y_train, est_name=est_name)\n    study.optimize(objective, n_trials=n_trials, gc_after_trial=True)\n    joblib.dump(study, study_name)","90c21497":"# Tune all the models with N_TRIALS.\nEST_NAMES = ['xgb', 'lgb', 'cb']\nN_TRIALS = 50\n\n# Read data\nx_train, y_train, x_test, y_test, nums, cats = get_train_test_data()\n\n# Tune hyper-parameters\nfor EST_NAME in EST_NAMES:\n    print('Tuning ' + EST_NAME + ' parameters...')\n    tune_parameters(x_train, y_train, est_name=EST_NAME, n_trials=N_TRIALS, nums=nums, cats=cats)\nprint('Done')","b80a2182":"def get_best_params(est_name='xgb'):\n    study_name = 'study_' + est_name + '.pkl'\n    study = joblib.load(study_name)\n    params = study.best_params\n    return params\n\nclass MeanClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, transformer, est_name, params):\n        self.transformer = transformer\n        self.est_name = est_name\n        self.params = params\n\n    def fit(self, X, y=None):\n        self.classes_ = np.unique(y)\n\n        X = self.transformer.fit_transform(X)\n        self.models_ = cross_validate(self.est_name, X, y, self.params, cv=None, method='predict_proba',\n                                      return_model=True)['models']\n        return self\n\n    def predict_proba(self, X, y=None):\n        X = self.transformer.transform(X)\n        y_pred = np.zeros(len(X))\n        for model in self.models_:\n            y_pred += model.predict_proba(X)[:, 1] \/ len(self.models_)\n        return y_pred\n\n    def predict(self, X, y=None, threshold=0.5):\n        y_pred = self.predict_proba(X)\n        return np.where(y_pred < threshold, 0, 1)\n\n    def score(self, x_true, y_true):\n        y_pred = self.predict_proba(x_true)\n        return roc_auc_score(y_true, y_pred)","45fe016c":"# Best parameters\nlgbparams = get_best_params('lgb')\nxgbparams = get_best_params('xgb')\ncbparams = get_best_params('cb')\n\n# Transformers\ntransformer_lgb = get_preprocessor('lgb', nums, cats)\ntransformer_xgb = get_preprocessor('xgb', nums, cats)\ntransformer_cb = get_preprocessor('cb', nums, cats)\n\n# Classifiers\nclf_lgb = MeanClassifier(transformer_lgb, 'lgb', lgbparams)\nclf_xgb = MeanClassifier(transformer_xgb, 'xgb', xgbparams)\nclf_cb = MeanClassifier(transformer_cb, 'cb', cbparams)","4392c555":"clfs = [clf_lgb, clf_xgb, clf_cb]\nfor clf in clfs:\n    clf.fit(x_train, y_train)\n    print('%s Test Score (AUC): %f' % (clf.models_[0].__class__.__name__, clf.score(x_test, y_test)))","8306e3ec":"fig, ax = plt.subplots()\nax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n       title=\"Receiver operating characteristic\")\nfor clf in clfs:\n    viz = plot_roc_curve(clf, x_test, y_test, ax=ax, name=clf.est_name)\nplt.show()","efae7755":"We have 5 models in total, one for each of the five folds. Next, we will write a sklearn classifier that will 1) retrain the 5 models with the best-fit parameters, 2) predict the probabilities by averaging the ouputs of 5 models.","54e6aed2":"### Subgroups of the Machine Failure","dcabf845":"# Predictive maintanence\n\nPredictive maintanence is the maintanence of machines at a predicted future time before the machine failure. This allows scheduled maintanence of the machines, reducing the unplanned downtime costs.\n\nIn this notebook, we will build a deployable end-to-end classification model to predict whether a machine failure will occur or not. We will train state-of-the-art gradient boosted decision tree (GBDT) algorithms, and compare their performances.\n\n## Data\n\nWe will use a simulated dataset taken from [Matzka (2020)](https:\/\/archive.ics.uci.edu\/ml\/datasets\/AI4I+2020+Predictive+Maintenance+Dataset). It consists of 10,000 data points stored as rows with features like product type, air temperature, process temperature, rotational speed, torque wear, machine failure. The machine failures are grouped into 5 subcategories. For simplicity, we will predict the machine failure feature. A concise summary of the data, pair plots, and the distribution of the target variable are given below.","53cf589c":"## Model Fitting and Test Results\n\nThe classifiers were written in the sklearn classifier format. Now, we can easily fit the models and obtain the test scores. Note that the classifiers defined above include the data preprocessors (column transformers); hence, they are ready to be saved and deployed right away.","87db1eb6":"### Pairplots","ce158e19":"# Modeling\n\nWe will use the sklearn api and sklearn api of the xgboost, lightgbm, and catboost modeling. For parameter tuning, we will use the optuna optimization library. The performance of the models will be measured with the AUC metric.\n\nThe modeling consists of two parts. In the first part, we will write a column transformer to have a clean and reproducable data preprocessing, and then tune the hyper-parameters of each model in a 5-Fold cross validation scheme with early stopping. In the second part we will retrain the models with the best-fit parameters over the very same cross validation splits we used in the hyper-parameter tuning. The early stopping will give us 5 models that come from the 5-Fold cross validation and then we will obtain the predictions by averaging the results of 5 models for each of the GBDT algorithms.","851cd644":"### Target Distribution"}}