{"cell_type":{"231f3c7f":"code","7af5e2a0":"code","6daae723":"code","5ba9c529":"code","3f150e17":"code","98700f07":"code","53d6483d":"code","758b221e":"code","cdc32eb4":"code","4886587a":"code","f27cf29f":"code","883fca5b":"code","f6b607a9":"markdown"},"source":{"231f3c7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7af5e2a0":"# Import Libraries\nnp.random.seed(0) \nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","6daae723":"train = pd.read_csv(r\"..\/input\/fashionmnist\/fashion-mnist_train.csv\",dtype = np.float32)\ntest =  pd.read_csv(r\"..\/input\/fashionmnist\/fashion-mnist_test.csv\",dtype = np.float32)","5ba9c529":"#set the device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","3f150e17":"y_train = train['label']\nX_train = train.drop(labels = ['label'], axis = 1)\/255 \nX_test = test.drop(labels = ['label'], axis = 1)\/255","98700f07":"features_train, features_test, targets_train, targets_test = train_test_split(X_train,\n                                                                             y_train,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\n\n# put the data to gpu(the device we set)\nfeaturesTrain = torch.from_numpy(features_train.values).to(device)\ntargetsTrain = torch.from_numpy(targets_train.values).type(torch.LongTensor).to(device) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test.values).to(device)\ntargetsTest = torch.from_numpy(targets_test.values).type(torch.LongTensor).to(device) # data type is long","53d6483d":"X_Test = torch.from_numpy(X_test.values)","758b221e":"batch_size = 100\nn_iters = 14000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\nprint(num_epochs)","cdc32eb4":"train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader: set shuffle = true\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = True)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = True)","4886587a":"class ANNModel(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        \n        # Linear function 1: 784 --> 150\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear function 2: 150 --> 150\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear function 3: 150 --> 150\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear function 4 (readout): 150 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n        \n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.tanh2(out)\n        \n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.elu3(out)\n        \n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n\ninput_dim = 28*28\nhidden_dim = 28*28*2 + 1\noutput_dim = 10\n\n# put the model to gpu\nmodel = ANNModel(input_dim, hidden_dim, output_dim).to(device)\n \nerror = nn.CrossEntropyLoss()\n\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","f27cf29f":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        #Variable --> cuda\n        train = Variable(images.view(-1, 28*28)).cuda()\n        labels = Variable(labels).cuda()\n        \n        optimizer.zero_grad()\n        \n        outputs = model(train)\n        \n        loss = error(outputs, labels)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            \n            correct = 0\n            total = 0\n            \n            for images, labels in test_loader:\n\n                test = Variable(images.view(-1, 28*28))\n                \n                outputs = model(test)\n                \n                predicted = torch.max(outputs.data, 1)[1]\n                \n                total += len(labels)\n\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","883fca5b":"output = model(test)\n\npredicted = torch.max(outputs.data, 1)[1]\n\npredicted_numpy = predicted.data.cpu().numpy()\n\noutput = pd.Series(predicted_numpy, name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,10001), name = \"ImageId\"), output], axis = 1)\n\nsubmission.to_csv(\"ANN_submission_hyperparameters.csv\", index=False)","f6b607a9":"While learning Pytorch, I set a ANN(4 layers) whose hidden layers have (28*28*2 + 1) inter nodes. It takes more than 1 hr to train this 4-Layer NN, in this notebook, I wrote how to use gpu in kaggle.\n\n# Put everything on gpu\n\nFirst, turn gpu accelerator on."}}