{"cell_type":{"8bfd417c":"code","ccc874b6":"code","8d551b14":"code","4fee89a3":"code","59059087":"code","c55dd0c2":"code","7b6cf27c":"code","89c75fe0":"code","7712d0d9":"code","2160dc32":"code","78fa68cf":"code","6142cc1e":"code","d75a1e4d":"code","527cfd57":"code","ca5144eb":"code","dd6256a5":"code","cdb0c00c":"code","9faaf854":"code","3fd2ecf8":"code","85f13530":"code","526eb505":"code","71ef7444":"code","bda251e9":"code","091b2d71":"code","b0b42810":"code","928a3aa9":"code","61525fdb":"code","81d68cb2":"code","68f20286":"code","ae270a1d":"code","2629223b":"code","623761ff":"code","581ea10f":"code","deec11ea":"code","ded47b1a":"code","8e8dca09":"code","735e0fd3":"code","a934cd7e":"code","da5ac322":"code","53e8723a":"code","ba5608ee":"code","8dcccc87":"code","f3ab87d2":"code","0f50292d":"code","a144b9c7":"code","66180ef0":"code","344b0c1c":"code","2a8f943e":"code","e07bff24":"markdown","0dca72fe":"markdown","0ae1e190":"markdown","faa1ef15":"markdown","5cf25dae":"markdown","0a068473":"markdown","7eb69254":"markdown","bafec7ea":"markdown","e9c5ccd9":"markdown","846e3897":"markdown","1039d59b":"markdown","36ad9728":"markdown","0ec627dc":"markdown","53b2c7bd":"markdown","0d996c48":"markdown","145da0ed":"markdown","3f79fa76":"markdown","a72afd73":"markdown","2ba5e800":"markdown","0952ce46":"markdown","1ec330c2":"markdown","fb66faaa":"markdown","21ba8a59":"markdown","e86eb3cd":"markdown","86745a69":"markdown","8bac7724":"markdown","2def2552":"markdown"},"source":{"8bfd417c":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","ccc874b6":"# load data\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ncats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\n# set index to ID to avoid droping it later\ntest  = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')","8d551b14":"train.head()","4fee89a3":"print('train size, item in train, shop in train', train.shape[0], train.item_id.nunique(), train.shop_id.nunique())\nprint('train size, item in train, shop in train', test.shape[0], test.item_id.nunique(),test.shop_id.nunique())\nprint('new items:', len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test))","59059087":"train.isnull().sum()","c55dd0c2":"sale_by_month = train.groupby('date_block_num')['item_cnt_day'].sum()\nsale_by_month.plot()","7b6cf27c":"block_item_shop_sale = train.groupby(['date_block_num','item_id','shop_id'])['item_cnt_day'].sum()\nblock_item_shop_sale.clip(0,20).plot.hist(bins=20)","89c75fe0":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)\n\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","7712d0d9":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","2160dc32":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","78fa68cf":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","6142cc1e":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","d75a1e4d":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","527cfd57":"\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float32))\n","ca5144eb":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","dd6256a5":"\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\n","cdb0c00c":"\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n","9faaf854":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","3fd2ecf8":"\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')","85f13530":"def add_group_stats(matrix_, groupby_feats, target, enc_feat, last_periods):\n    if not 'date_block_num' in groupby_feats:\n        print ('date_block_num must in groupby_feats')\n        return matrix_\n    \n    group = matrix_.groupby(groupby_feats)[target].sum().reset_index()\n    max_lags = np.max(last_periods)\n    for i in range(1,max_lags+1):\n        shifted = group[groupby_feats+[target]].copy(deep=True)\n        shifted['date_block_num'] += i\n        shifted.rename({target:target+'_lag_'+str(i)},axis=1,inplace=True)\n        group = group.merge(shifted, on=groupby_feats, how='left')\n    group.fillna(0,inplace=True)\n    for period in last_periods:\n        lag_feats = [target+'_lag_'+str(lag) for lag in np.arange(1,period+1)]\n        # we do not use mean and svd directly because we want to include months with sales = 0\n        mean = group[lag_feats].sum(axis=1)\/float(period)\n        mean2 = (group[lag_feats]**2).sum(axis=1)\/float(period)\n        group[enc_feat+'_avg_sale_last_'+str(period)] = mean\n        group[enc_feat+'_std_sale_last_'+str(period)] = (mean2 - mean**2).apply(np.sqrt)\n        group[enc_feat+'_std_sale_last_'+str(period)].replace(np.inf,0,inplace=True)\n        # divide by mean, this scales the features for NN\n        group[enc_feat+'_avg_sale_last_'+str(period)] \/= group[enc_feat+'_avg_sale_last_'+str(period)].mean()\n        group[enc_feat+'_std_sale_last_'+str(period)] \/= group[enc_feat+'_std_sale_last_'+str(period)].mean()\n    cols = groupby_feats + [f_ for f_ in group.columns.values if f_.find('_sale_last_')>=0]\n    matrix = matrix_.merge(group[cols], on=groupby_feats, how='left')\n    return matrix","526eb505":"ts = time.time()\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'item', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'shop', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'category', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'city', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'type_code'], 'item_cnt_month', 'type', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'subtype_code'], 'item_cnt_month', 'subtype', [12])\ntime.time() - ts","71ef7444":"#first use target encoding each group, then shift month to creat lag features\ndef target_encoding(matrix_, groupby_feats, target, enc_feat, lags):\n    print ('target encoding for',groupby_feats)\n    group = matrix_.groupby(groupby_feats).agg({target:'mean'})\n    group.columns = [enc_feat]\n    group.reset_index(inplace=True)\n    matrix = matrix_.merge(group, on=groupby_feats, how='left')\n    matrix[enc_feat] = matrix[enc_feat].astype(np.float16)\n    matrix = lag_feature(matrix, lags, enc_feat)\n    matrix.drop(enc_feat, axis=1, inplace=True)\n    return matrix","bda251e9":"ts = time.time()\nmatrix = target_encoding(matrix, ['date_block_num'], 'item_cnt_month', 'date_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'date_item_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'date_shop_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'date_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id', 'item_category_id'], 'item_cnt_month', 'date_shop_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'date_city_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id', 'city_code'], 'item_cnt_month', 'date_item_city_avg_item_cnt', [1])\ntime.time() - ts","091b2d71":"ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n# https:\/\/stackoverflow.com\/questions\/31828240\/first-non-null-value-per-row-from-a-list-of-pandas-columns\/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts","b0b42810":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","928a3aa9":"matrix['month'] = matrix['date_block_num'] % 12\nmatrix['year'] = (matrix['date_block_num'] \/ 12).astype(np.int8)","61525fdb":"#Month since last sale for each shop\/item pair.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby(['item_id','shop_id'])['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.get_level_values(0).values,\n                       'shop_id': last_month.index.get_level_values(1).values,\n                       'item_shop_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id','shop_id'], how='left')\ntime.time() - ts","81d68cb2":"#Month since last sale for each item.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby('item_id')['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.values,\n                       'item_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id'], how='left')\ntime.time() - ts","68f20286":"# Months since the first sale for each shop\/item pair and for item only.\nts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","ae270a1d":"matrix = matrix[matrix.date_block_num > 11]\nmatrix.columns","2629223b":"#matrix.to_pickle('..\/output\/kaggle\/working\/data.pkl')\nmatrix.to_pickle('data.pkl')\ndel matrix\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","623761ff":"data = pd.read_pickle('.\/data.pkl')\ndata.head()","581ea10f":"data = data[[\n    'date_block_num',\n    'shop_id',\n    #'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code','subtype_code',\n    'item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_6','item_cnt_month_lag_12',\n    'item_avg_sale_last_6', 'item_std_sale_last_6',\n    'item_avg_sale_last_12', 'item_std_sale_last_12',\n    'shop_avg_sale_last_6', 'shop_std_sale_last_6',\n    'shop_avg_sale_last_12', 'shop_std_sale_last_12',\n    'category_avg_sale_last_12', 'category_std_sale_last_12',\n    'city_avg_sale_last_12', 'city_std_sale_last_12',\n    'type_avg_sale_last_12', 'type_std_sale_last_12',\n    'subtype_avg_sale_last_12', 'subtype_std_sale_last_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1','date_item_avg_item_cnt_lag_2','date_item_avg_item_cnt_lag_3','date_item_avg_item_cnt_lag_6','date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1','date_shop_avg_item_cnt_lag_2','date_shop_avg_item_cnt_lag_3','date_shop_avg_item_cnt_lag_6','date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month','year',\n    'item_shop_last_sale','item_last_sale',\n    'item_shop_first_sale','item_first_sale',\n]]\n\ncat_feats = ['shop_id','city_code','item_category_id','type_code','subtype_code']","deec11ea":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ndel data\ngc.collect();\n","ded47b1a":"import numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","8e8dca09":"ts = time.time()\n\nmodel = LGBMRegressor(\n    max_depth = 8,\n    n_estimators = 500,\n    colsample_bytree=0.7,\n    min_child_weight = 300,\n    reg_alpha = 0.1,\n    reg_lambda = 1,\n    random_state = 42,\n)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    categorical_feature = cat_feats) # use LGBM's build-in categroical features.\n\ntime.time() - ts","735e0fd3":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nX_train_level2 = pd.DataFrame({\n    \"ID\": np.arange(Y_pred.shape[0]), \n    \"item_cnt_month\": Y_pred\n})\nX_train_level2.to_csv('lgb_valid.csv', index=False)\n\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(Y_test.shape[0]), \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('lgb_submission.csv', index=False)","a934cd7e":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.regularizers import l2, l1\nfrom keras.optimizers import RMSprop, Adam\n#from tensorflow import set_random_seed\nimport tensorflow as tf    \n\nnp.random.seed(233333)","da5ac322":"data = pd.read_pickle('data.pkl')\n# do not use ID features\ndata = data[[\n    'date_block_num',\n    #'shop_id',\n    #'item_id',\n    'item_cnt_month',\n    #'city_code',\n    #'item_category_id',\n    #'type_code','subtype_code',\n    'item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_6','item_cnt_month_lag_12',\n    'item_avg_sale_last_6', 'item_std_sale_last_6',\n    'item_avg_sale_last_12', 'item_std_sale_last_12',\n    'shop_avg_sale_last_6', 'shop_std_sale_last_6',\n    'shop_avg_sale_last_12', 'shop_std_sale_last_12',\n    'category_avg_sale_last_12', 'category_std_sale_last_12',\n    'city_avg_sale_last_12', 'city_std_sale_last_12',\n    'type_avg_sale_last_12', 'type_std_sale_last_12',\n    'subtype_avg_sale_last_12', 'subtype_std_sale_last_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1','date_item_avg_item_cnt_lag_2','date_item_avg_item_cnt_lag_3','date_item_avg_item_cnt_lag_6','date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1','date_shop_avg_item_cnt_lag_2','date_shop_avg_item_cnt_lag_3','date_shop_avg_item_cnt_lag_6','date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month','year',\n    'item_shop_last_sale','item_last_sale',\n    'item_shop_first_sale','item_first_sale',\n]]","53e8723a":"# define model\ndef Sales_prediction_model(input_shape):\n    in_layer = Input(input_shape)\n    x = Dense(16,kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(in_layer)\n    x = Dense(8, kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(x)\n    x = Dense(1, kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(x)\n    \n    model = Model(inputs = in_layer, outputs = x, name='Sales_prediction_model')\n    return model\n\n# NN cannot take missing values, fill NaN with 0.\nX_train.fillna(0,inplace=True)\nX_valid.fillna(0,inplace=True)\nX_test.fillna(0,inplace=True)\n\n# We do no feature scaling here. \n# Some features like 'item_avg_sale_last_6' are already scaled in feature engineering part.\n\ninput_shape = [X_train.shape[1]]\nmodel = Sales_prediction_model(input_shape)\nmodel.compile(optimizer = Adam(lr=0.0005) , loss = [\"mse\"], metrics=['mse'])\nmodel.fit(X_train, Y_train, validation_data = (X_valid, Y_valid), batch_size = 10000, epochs=5)\n","ba5608ee":"Y_pred = model.predict(X_valid).clip(0, 20)[:,0]\nY_test = model.predict(X_test).clip(0, 20)[:,0]\n\nX_train_level2 = pd.DataFrame({\n    \"ID\": np.arange(Y_pred.shape[0]), \n    \"item_cnt_month\": Y_pred\n})\nX_train_level2.to_csv('nn_valid.csv', index=False)\n\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(Y_test.shape[0]), \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('nn_submission.csv', index=False)","8dcccc87":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBRegressor\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","f3ab87d2":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=7,\n    n_estimators=1000,\n    min_child_weight=300,   \n    colsample_bytree=0.8, \n    subsample=0.8, \n    gamma = 0.005,\n    eta=0.1,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    )\n\ntime.time() - ts","0f50292d":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nX_train_level2 = pd.DataFrame({\n    \"ID\": np.arange(Y_pred.shape[0]), \n    \"item_cnt_month\": Y_pred\n})\nX_train_level2.to_csv('xgb_valid.csv', index=False)\n\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(Y_test.shape[0]), \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","a144b9c7":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, LinearRegression\nimport gc","66180ef0":"\ndata = pd.read_pickle('data.pkl')\nY_train_level2 = data[data.date_block_num == 33]['item_cnt_month']\ndel data\ngc.collect()","344b0c1c":"X_train_level2 = pd.DataFrame()\ndf = pd.read_csv('.\/lgb_valid.csv')\nX_train_level2['lgb'] = df['item_cnt_month']\ndf = pd.read_csv('.\/xgb_valid.csv')\nX_train_level2['xgb'] = df['item_cnt_month'] \ndf = pd.read_csv('.\/nn_valid.csv')\nX_train_level2['nn'] = df['item_cnt_month'] \n\nX_test_level2 = pd.DataFrame()\ndf = pd.read_csv('.\/lgb_submission.csv')\nX_test_level2['lgb'] = df['item_cnt_month']\ndf = pd.read_csv('.\/xgb_submission.csv')\nX_test_level2['xgb'] = df['item_cnt_month'] \ndf = pd.read_csv('.\/nn_submission.csv')\nX_test_level2['nn'] = df['item_cnt_month']\n","2a8f943e":"# simple weighted average\n# find best linear combination coefficient to weight \nbest_alpha = 1;\nbest_rmse = 100;\nfor alpha in np.arange(0,1,0.02):\n    Y_pred_level2 = alpha*X_train_level2['lgb'] + (1-alpha)*X_train_level2['xgb']\n    rmse = np.sqrt(mean_squared_error(Y_train_level2, Y_pred_level2))\n    if (rmse<best_rmse):\n        best_rmse = rmse\n        best_alpha = alpha\n\nY_test_level2 = best_alpha*X_test_level2['lgb'] + (1-best_alpha)*X_test_level2['xgb']\nprint('best alpha:', best_alpha)\nprint('weighted average of lgb and xgb validation rmse: ',best_rmse)\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(Y_test_level2.shape[0]), \n    \"item_cnt_month\": Y_test_level2\n})\nsubmission.to_csv('.\/blended_submission1.csv', index=False)\n\n# Linear regression\nmodel = LinearRegression()\nmodel.fit(X_train_level2, Y_train_level2)\nY_pred_level2 = model.predict(X_train_level2)\nY_test_level2 = model.predict(X_test_level2)\nrmse = np.sqrt(mean_squared_error(Y_train_level2, Y_pred_level2))\nprint('Linear regression validation rmse: ',rmse)\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(Y_test_level2.shape[0]), \n    \"item_cnt_month\": Y_test_level2\n})\nsubmission.to_csv('.\/blended_submission2.csv', index=False)","e07bff24":"Test set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. ","0dca72fe":"## Lag features","0ae1e190":"# Exploratory Data Analysis","faa1ef15":"The distribution of sale grouped by month, item and shop, we see most item-shop pairs have small monthly sale.","5cf25dae":"Last month shop revenue trend","0a068473":"This notebook does exploratory data analysis and feature engineering. The results are dumped to file for modeling.\n\n#### Exploratary Data Analysis\n* load data\n* trend of sales\n* distribution of target\n\n#### Data Cleaning & Feature Engineering\n* heal data and remove outliers\n* work with shops\/items\/cats objects and features\n* expand training set to include all item-shop pairs\n* clip item_cnt_month by (0,20)\n* append test to the matrix, fill 34 month nans with zeros\n* merge shops\/items\/cats dataframe to training set.\n* add group sale stats in recent months\n* add lag features\n* add trend features\n* add month and year\n* add months since last sale\/months since first sale features\n* cut first year and drop columns which can not be calculated for the test set","7eb69254":"## Test set\nTo use time tricks append test pairs to the matrix.","bafec7ea":"## Shops\/Items\/Cats features","e9c5ccd9":"Several shops are duplicates of each other (according to its name). Fix train and test set.","846e3897":"Check for missing values.","1039d59b":"# **Set up validation strategy**\n\nValidation strategy is 34 month for the test set, 33 month for the validation set and 13-32 months for the train.","36ad9728":"#### remove outliers\nRemove outliers with very large item_cnt_day and item_price.","0ec627dc":"There is no missing value in the training set.\n\nPlot the total sale of each month, we see a clear trend and seasonality. The overall sale is decreasing with time, and there are peaks in November.","53b2c7bd":"## Traget lags","0d996c48":"\n# Feature engineering and data cleaning","145da0ed":"## Add month since the last and first sale\nThe code has been simplified to reduce run time, though still may not be optimal -- ideally we don't need to compute max for each month.","3f79fa76":"Price trend for the last six months.","a72afd73":"## Add month and year","2ba5e800":"## Final preparations\nBecause of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).\nLightgbm and XGBboost can deal with missing values, so we will leave the NaNs as it is. Later for neural network, we will fill na with 0.","0952ce46":"## Monthly sales\nMost of the items in the test set target value should be zero, while train set contains only pairs which were sold or returned in the past. So we expand the train set to include those item-shop pairs with zero monthly sales. This way train data will be similar to test data.","1ec330c2":"#### Shops\/Cats\/Items preprocessing\nObservations:\n* Each shop_name starts with the city name.\n* Each category contains type and subtype in its name.","fb66faaa":"# XGBoost","21ba8a59":"# LightGBM","e86eb3cd":"## Group sale stats in recent\ncreate stats (mean\/var) of sales of certain groups during the past 12 months","86745a69":"## Trend features","8bac7724":"There is one item with price below zero. Fill it with median.","2def2552":"Aggregate train set by shop\/item pairs to calculate target aggreagates, then <b>clip(0,20)<\/b> target value. This way train target will be similar to the test predictions.\n\nDowncast item_cnt_month to float32 -- float16 was too small to perform sum operation."}}