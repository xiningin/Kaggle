{"cell_type":{"8e788476":"code","b6ab0b90":"code","47c47be6":"code","6df86a07":"code","8f8798ab":"code","06c8af89":"code","0bb0ace1":"code","78d8a59d":"code","59d33436":"code","625f4641":"code","076fc2d4":"code","ea28e279":"code","532e21aa":"code","d0222050":"code","8a031308":"code","dc99c7b7":"code","97558078":"code","ad60213a":"code","ac82c82a":"code","d5b8f1ac":"code","0e687c9d":"code","6b0bc7ec":"code","470a1c97":"code","e51b4e5e":"code","c5c7f135":"code","6ad37679":"code","7844d9b4":"code","236c50a7":"code","bbe02198":"code","74fab8ed":"code","efce024a":"code","fd304dfd":"code","2689bc66":"code","fc4e5806":"code","2c46e4e6":"code","89407e1c":"code","d1c0a72b":"code","20297305":"code","ca3034a4":"code","a827c446":"code","9688a8dd":"code","877aca00":"code","7f4e1e9d":"code","f5edf716":"code","306b924d":"code","b8730e9c":"code","649b2075":"code","78cec1f9":"code","81d3c6b8":"code","2d66235c":"code","e053e8f3":"code","7ff6c0d9":"code","8e56f95a":"code","8406ece2":"code","68283134":"code","a7fbee8d":"code","278952d8":"code","99a2f496":"code","86d49583":"code","e643b832":"code","0b82b90e":"code","b530020f":"code","9dfdd2e9":"code","48f6e437":"code","2ba84b2f":"code","abbf1b81":"code","1d4da256":"code","3aa736e3":"code","0599625c":"code","6c3639b3":"code","bfe50cc5":"code","7470f465":"code","eac5e90d":"code","a25caf45":"code","6e668ba6":"code","5831e1ba":"code","10344828":"code","f332958c":"code","f73374c2":"code","8c14574e":"code","086a812d":"code","c7c1f463":"code","c6c8e484":"code","c59b9e6b":"code","b2b80e7c":"code","d84bea60":"code","0a3872fc":"code","3fad9e53":"code","72a7a757":"code","509a5a04":"code","380e710d":"code","6771f8aa":"code","2101baca":"code","cd3f1048":"code","573ae9ad":"markdown","5ce84560":"markdown","6c7a0e56":"markdown","2142dfc2":"markdown","91b5b7b8":"markdown","c7b4e43e":"markdown","bcac5f1a":"markdown","a8b86a8d":"markdown","e81665a0":"markdown","3c71443a":"markdown","25b87d72":"markdown","6f4040b6":"markdown","5ad3b8db":"markdown","17aa8e71":"markdown","3999556b":"markdown","c1874139":"markdown","b40f17b4":"markdown","197f1a50":"markdown","ef873a9b":"markdown","0ee8a620":"markdown","8fdc2238":"markdown","88f83329":"markdown","a6df42f3":"markdown","333bd88e":"markdown","bc41eb40":"markdown","ed5febf9":"markdown","42fb9c13":"markdown","77bf1ce5":"markdown"},"source":{"8e788476":"!pip install pyspark","b6ab0b90":"import pyspark","47c47be6":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[4]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\nflightData2015 = spark\\\n                 .read\\\n                 .option(\"inferSchema\",\"true\")\\\n                 .option(\"header\",\"true\")\\\n                 .csv(\"..\/input\/dataset\/data\/flight-data\/csv\/2015-summary.csv\")\n                ","6df86a07":"flightData2015.take(3)","8f8798ab":"flightData2015.sort(\"count\").take(3) #sort is wide and read is narrow transformation actions","06c8af89":"flightData2015.sort(\"count\").explain()","0bb0ace1":"spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")\nflightData2015.sort(\"count\").take(2)","78d8a59d":"flightData2015.createOrReplaceTempView(\"flight_data_2015\")","59d33436":"sql_code = spark.sql(\"SELECT DEST_COUNTRY_NAME,count(1) from flight_data_2015 GROUP BY DEST_COUNTRY_NAME\")","625f4641":"sql_code.show(5)","076fc2d4":"#same thing in python\npython_code = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()\npython_code.show(5)","ea28e279":"sql_code.explain() == python_code.explain()","532e21aa":"spark.sql(\"select max(count) from flight_data_2015\").take(1) #sql","d0222050":"from pyspark.sql.functions import *\nflightData2015.select(max(\"count\")).take(1) #pyhton","8a031308":"#any sql query\nmaxsql = spark.sql(\"\"\"\n                   SELECT DEST_COUNTRY_NAME, sum(count) destinamtion_total from flight_data_2015\n                   group by DEST_COUNTRY_NAME order by sum(count) desc limit 5 \"\"\")\nmaxsql.show()","dc99c7b7":"maxpython = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\",\"destinamtion_total\")\\\n            .sort(desc(\"destinamtion_total\")).limit(5).show()","97558078":"flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\",\"destinamtion_total\")\\\n            .sort(desc(\"destinamtion_total\")).limit(5).explain()","ad60213a":"#.\/bin\/spark-submit -- master local .\/examples\/src\/main\/python\/pi.py 10","ac82c82a":"staticDataframe = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\")\\\n                  .load(\"..\/input\/dataset\/data\/retail-data\/by-day\/*.csv\")\nstaticDataframe.createOrReplaceTempView(\"retail_data\")\nstaticschema = staticDataframe.schema","d5b8f1ac":"staticschema","0e687c9d":"sql_select = spark.sql(\"select * from retail_data limit 3\").show()","6b0bc7ec":"#python\ntotal_cust_per_session = staticDataframe.selectExpr(\"CustomerID\",\"(Quantity*UnitPrice) as total_cost\",\"InvoiceDate\")\\\n                         .groupby(col(\"customerID\"),window(col(\"InvoiceDate\"),\"1 day\"))\\\n                         .sum(\"total_cost\")\\\n                         .show(10)","470a1c97":"streamingDataFrame = spark.readStream.schema(staticschema).option(\"maxFilesPerTrigger\",1)\\\n                     .format(\"csv\")\\\n                     .option(\"header\",\"true\").load(\"..\/input\/dataset\/data\/retail-data\/by-day\/*.csv\")","e51b4e5e":"streamingDataFrame.isStreaming","c5c7f135":"#writting same code again\ntotal_cust_per_day = streamingDataFrame.selectExpr(\"CustomerID\",\"(Quantity*UnitPrice) as total_cost\",\"InvoiceDate\")\\\n                         .groupby(col(\"customerID\"),window(col(\"InvoiceDate\"),\"1 day\"))\\\n                         .sum(\"total_cost\")\n                    \n#it is in  lazy execution mode \ntotal_cust_per_day.writeStream.format(\"memory\").queryName(\"customer_purchase\").outputMode(\"complete\").start()","6ad37679":"#Now query the table\nspark.sql(\"\"\"select * from customer_purchase order by `sum(total_cost)` desc\"\"\").show(5)","7844d9b4":"#Now query the table\nspark.sql(\"\"\"select * from customer_purchase order by `sum(total_cost)` desc\"\"\").take(2)","236c50a7":"total_cust_per_day.writeStream.format(\"console\").queryName(\"customer_purchase_2\").outputMode(\"complete\").start()","bbe02198":"#create df\nspark.range(100)","74fab8ed":"df = spark.range(100).toDF(\"Number\")\ndf.select(df[\"Number\"] + 3).show(2)","efce024a":"df = spark.read.format(\"json\").load(\"..\/input\/dataset\/data\/flight-data\/json\/2015-summary.json\") #dataframe ","fd304dfd":"df.printSchema()","2689bc66":"df.schema","fc4e5806":"#creating schema manually\nfrom pyspark.sql.types import *\nmyschema = StructType([\n                      StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n                      StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n                      StructField(\"count\",LongType(),False)\n])\n\ndf = spark.read.format(\"json\").schema(myschema).load(\"..\/input\/dataset\/data\/flight-data\/json\/2015-summary.json\")","2c46e4e6":"#accessing columns\nspark.read.format(\"json\").load(\"..\/input\/dataset\/data\/flight-data\/json\/2015-summary.json\").columns","89407e1c":"#see the first row\ndf.first()","d1c0a72b":"#filtering col\ndf.filter(col(\"count\")<2).take(3)","20297305":"df.filter(col(\"count\")<2).where(col(\"DEST_COUNTRY_NAME\")!=\"United States\").take(2)","ca3034a4":"#unique value\ndf.select(\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\").distinct().count()","a827c446":"#doing random sampleing\nseed = 2\nfraction = 0.7\nwithReplacement = False\ndf.sample(withReplacement,fraction,seed).count()\n","9688a8dd":"dataframe = df.randomSplit([0.5,0.5],seed)\ndataframe[0] == dataframe[1]\ndataframe[0].count() == dataframe[1].count()","877aca00":"#union of rows\nfrom pyspark.sql import Row \nschema = df.schema\nnewRow = [\n    Row(\"New Country\",\"Some Country\", 3),\n    Row(\"New Country2\",\"Some Country2\",13),\n    Row(\"New Country3\",\"Some Country3\",30)\n    \n]\nparallelizedRows = spark.sparkContext.parallelize(newRow)\nnewdf = spark.createDataFrame(parallelizedRows,schema)\n\n#union\ndf.union(newdf).where(\"count=1\").where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\").show()","7f4e1e9d":"#sorting\ndf.sort(\"count\").show()","f5edf716":"df.orderBy(col(\"count\"),col(\"ORIGIN_COUNTRY_NAME\")).show()","306b924d":"#partitions\ndf.rdd.getNumPartitions()","b8730e9c":"#repartition\ndf.repartition(5)","649b2075":"df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"..\/input\/dataset\/data\/retail-data\/by-day\/2010-12-01.csv\")","78cec1f9":"df.printSchema()","81d3c6b8":"df.createOrReplaceTempView(\"dfTable\")","2d66235c":"df.show(5)","e053e8f3":"#some filtering boolean operator\ndf.where(col(\"InvoiceNo\") != 536365).select(col(\"Description\")).show(3)","7ff6c0d9":"df.where(col(\"InvoiceNo\") != 536365).show(5,False)","8e56f95a":"#filtering with variable\npricefilter = col(\"UnitPrice\") > 2.5\ndf.where(df.StockCode.isin(\"DOT\")).where(pricefilter).show()","8406ece2":"#addition of column\npricefilter = col(\"UnitPrice\") > 600\nDOTfilter = col(\"StockCode\") == \"DOT\"\nDescfilter = instr(col(\"Description\"),\"POSTAGE\") >= 1\ndf.withColumn(\"isExpensive\",DOTfilter & (pricefilter | Descfilter)).where(\"isExpensive\").select(\"InvoiceNo\",\"isExpensive\",\"UnitPrice\").show(5)","68283134":"artificalquantity = pow(col(\"Quantity\")*col(\"UnitPrice\"),2) + 7\ndf.select(expr(\"CustomerId\"),artificalquantity.alias(\"Quantity\")).show(3)","a7fbee8d":"df.selectExpr(\"CustomerId\",\"(POWER((Quantity*UnitPrice),2)+5) as realquantity\").show(2)","278952d8":"df.select(corr(\"Quantity\",\"UnitPrice\")).show()","99a2f496":"df.describe().show()","86d49583":"df.stat.crosstab(\"StockCode\",\"Quantity\").show()","e643b832":"df.stat.freqItems([\"StockCode\",\"Quantity\"]).show()","0b82b90e":"df.select(monotonically_increasing_id()).show(3)","b530020f":"#strings\ndf.select(initcap(col(\"Description\"))).show()","9dfdd2e9":"spark.sql(\"SELECT initcap(Description) from dfTable \").show(5)","48f6e437":"df.select(col(\"Description\"),upper(col(\"Description\")),lower(upper(col(\"Description\")))).show(3)","2ba84b2f":"#sql\nspark.sql(\"SELECT Description,lower(Description),upper(Description) from dfTable\").show(3)","abbf1b81":"df.select(ltrim(lit(\"   Hello   \")).alias(\"ltrim\")).show(5)","1d4da256":"df.select(lpad(lit(\"Hello   \"),3,\" \").alias(\"ltrim\")).show(5)","3aa736e3":"regexp_string = \"BLACK|GREEN|RED|BLUE|WHITE\"\ndf.select(regexp_replace(col(\"Description\"),regexp_string,\"COLOR\")).alias(\"color_clean\").show(3)","0599625c":"#translate\ndf.select(translate(col(\"Description\"),\"LEET\",\"1337\"),col(\"Description\")).show(2)\n","6c3639b3":"regexp_string = \"(BLACK|GREEN|RED|BLUE|WHITE)\"\ndf.select(regexp_extract(col(\"Description\"),regexp_string,1).alias(\"color_clean\"),col(\"Description\")).show(3)","bfe50cc5":"df.printSchema()","7470f465":"dateDF = spark.range(10).withColumn(\"today\",current_date()).withColumn(\"now\",current_timestamp())\ndateDF.createOrReplaceTempView(\"dateTable\")","eac5e90d":"dateDF.printSchema()","a25caf45":"dateDF.select(date_sub(col(\"today\"),5),date_add(col(\"today\"),5)).show(1)","6e668ba6":"#coalesce\ndf.select(coalesce(col(\"Description\"),col(\"CustomerId\"))).show(1)","5831e1ba":"df.select(count(\"StockCode\")).show()","10344828":"df.select(approx_count_distinct(\"StockCode\",0.15)).show()","f332958c":"!pip install tensorframes h5py pyspark","f73374c2":"# Import libraries\nfrom pyspark.sql import SparkSession\n\n# Creating SparkSession\nspark = (SparkSession\n            .builder\n            .config('spark.jars.packages', 'databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11')\n            .getOrCreate()\n)\n\n# Import Spar-Deep-Learning-Pipelines\nimport sparkdl","8c14574e":"# Import Spar-Deep-Learning-Pipelines\nimport sparkdl\n\n\nimg_dir = \"..\/input\/dataset\/data\/deep-learning-images\"\nimage_df = sparkdl.image.imageIO.Image.open(\"..\/input\/dataset\/data\/deep-learning-images\/daisy\/100080576_f52e8ee070_n.jpg\")","086a812d":"image_df","c7c1f463":"image_df = spark.read.format(\"image\").load(img_dir)","c6c8e484":"image_df.printSchema()","c59b9e6b":"from pyspark.sql.functions import lit\ntulip_dfs = spark.read.format(\"image\").load(img_dir + \"\/tulips\").withColumn(\"label\",lit(1))\ndaisy_dfs = spark.read.format(\"image\").load(img_dir + \"\/daisy\").withColumn(\"label\",lit(0))\ntulip_train,tulip_test = tulip_dfs.randomSplit([0.6,0.4])\ndaisy_train,daisy_test = daisy_dfs.randomSplit([0.6,0.4])\ntrain_df = tulip_train.unionAll(daisy_train)\ntest_df = tulip_test.unionAll(daisy_test)","b2b80e7c":"# train logistic regression on features generated by InceptionV3:\nfeaturizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n# Build logistic regression transform\nlr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n# Build ML pipeline\np = Pipeline(stages=[featurizer, lr])\n# Build our model\np_model = p.fit(train_df)\n# Run our model against test dataset\ntested_df = p_model.transform(test_df)\n# Evaluate our model\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(tested_df.select(\"prediction\", \"label\"))))","d84bea60":"from sparkdl import DeepImagePredictor\npredictor = DeepImagePredictor(inputCol=\"image\",outputCol=\"predicted_labels\",modelName=\"InceptionV3\",\n                              topK=10)\nprediction_df = predictor.transform(image_df)","0a3872fc":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/dataset\/data\/flight-data\/csv\/2010-summary.csv\")\nsparkDf = spark.createDataFrame(df)\nsparkDf.show(3)","3fad9e53":"pandas_df=sparkDf.toPandas()\npandas_df.head()","72a7a757":"bikestation = spark.read.option(\"header\",\"true\").csv(\"..\/input\/dataset\/data\/bike-data\/201508_station_data.csv\")","509a5a04":"tripData = spark.read.option(\"header\",\"true\").csv(\"..\/input\/dataset\/data\/bike-data\/201508_trip_data.csv\")","380e710d":"#creating vertice and edge\nstationVertice = bikestation.withColumnRenamed(\"name\",\"id\").distinct()\ntripEdges = tripData.withColumnRenamed(\"Start Station\",\"src\").withColumnRenamed(\"End Station\",\"dst\")\n","6771f8aa":"!pip install graphframes","2101baca":"from graphframes import GraphFrame","cd3f1048":"stationGraph = GraphFrame(stationVertice,tripEdges)\nstationGraph.cache()","573ae9ad":"# Aggregation","5ce84560":"# Working With DateTime","6c7a0e56":"# doing some actions in sql as well as in python","2142dfc2":"# A simple Action","91b5b7b8":"How spark Executes user job:\n<p><h3>\n<b>User code > unresolved logical plan >  catalog(Analysis) > Resolved Logical plan > logical optimizer > optimized logical plan > physical plans > cost model > Best physical plan > Execution of code in the cluster<\/b><\/h3><\/p>","c7b4e43e":"# Tools in Spark","bcac5f1a":"# Creating A spark Dataframe","a8b86a8d":"## See How Spark Execute plan for running sort ","e81665a0":"# Structured APIS ","3c71443a":"see it really doesnot matter wheather you write code in SQL or python or R as underneth spark executes same physical plan","25b87d72":"examine wheather actually streaming","6f4040b6":"# From pandas to spark","5ad3b8db":"## Finding the total cost per customer","17aa8e71":"creating a dataframe(optimized data structure in spark) with which we can work ","3999556b":"## Creation of static Dataframe","c1874139":"## Submitting into python ","b40f17b4":"How spark actually works under the hood:\n**Structured API > Catalyst Optimizer > Physical Plan**","197f1a50":"# Structured API in Spark","ef873a9b":"# Transformation Wide and Narrow","0ee8a620":"# looking into streaming code","8fdc2238":"# Regular Expression\nspark uses java regular expression","88f83329":"# Codes Are the Examples Followed in spark the definete guide book with much clear elaboration","a6df42f3":"# Deep learning Using Spark and choice of framework","333bd88e":"# Datatypes","bc41eb40":"# Execute sql statement   ","ed5febf9":"What are possibilities with spark\n* Running production applications \n* Machine Learning\n* type safe structured dataframe with dataset\n* stremming\n* low level RDD\n* SparkR\n* Third party integrations","42fb9c13":"# Creating Table From Dataframe","77bf1ce5":"Thats it for this chapter."}}