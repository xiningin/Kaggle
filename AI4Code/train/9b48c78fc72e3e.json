{"cell_type":{"83e2a860":"code","cb00921f":"code","5fac78b7":"code","6246efb1":"code","c763b11b":"code","4176f08c":"code","d8a7d1cd":"code","6195ce9d":"code","43e64276":"code","4b847046":"code","39a89bae":"code","44ea8069":"code","7eb4ba5c":"code","2eb7c1ef":"markdown","6b2d15e5":"markdown"},"source":{"83e2a860":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb00921f":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization,Dense\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\nimport cv2\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt","5fac78b7":"labels = ['PNEUMONIA','NORMAL']\nimg_size = 150\n\ndef get_training_data(data_dir):\n    \n    data = []\n    \n    for label in labels:\n        \n        path = os.path.join(data_dir , label)\n        class_num = labels.index(label)\n        \n        for img in os.listdir(path):\n            \n            try:\n                img_arr = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr , (img_size , img_size))\n                \n                data.append([resized_arr , class_num])\n                \n            except Exception as e:\n                print(e)\n                \n    return np.array(data)           \n                \n        ","6246efb1":"train = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train')\ntest = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test')\nval = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val')","c763b11b":"plt.figure(figsize = (7,7))\nplt.imshow(train[0][0],cmap = 'gray')\nplt.title(labels[train[0][1]])\n\nplt.figure(figsize = (7,7))\nplt.imshow(train[-1][0],cmap = 'gray')\nplt.title(labels[train[-1][1]])","4176f08c":"x_train = []\ny_train = []\n\nx_val = []\ny_val = []\n\nx_test = []\ny_test = []\n\nfor feature , label in train:\n    x_train.append(feature)\n    y_train.append(label)\n\n    \nfor feature,label in test:\n    x_test.append(feature)\n    y_test.append(label)\n    \nfor feature,label in val:\n    x_val.append(feature)\n    y_val.append(label)\n    ","d8a7d1cd":"#normalize the data for faster learning\n\nx_train = np.array(x_train)\/ 255\nx_test = np.array(x_test)\/ 255\nx_val = np.array(x_val) \/255","6195ce9d":"# resize the data for (150,150) to (150,150,1)\nx_train = x_train.reshape(-1,img_size,img_size,1)\ny_train = np.array(y_train)\n\nx_val = x_val.reshape(-1,img_size,img_size,1)\ny_val = np.array(y_val)\n\nx_test = x_test.reshape(-1,img_size,img_size,1)\ny_test = np.array(y_test)","43e64276":"#image data augmentation\n\ndatagen = ImageDataGenerator(\n                            horizontal_flip = True,\n                            zoom_range = 0.2,\n                            shear_range = 0.2)\n\ndatagen.fit(x_train)","4b847046":"model = Sequential()\n\nmodel.add(Conv2D(32, (3,3), activation = 'relu' ,input_shape=(150,150,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64 ,(3,3), activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(128 ,(3,3), activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(256 ,(3,3), activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1,activation = 'sigmoid'))\n\n\nmodel.compile(optimizer = 'rmsprop' ,loss = 'binary_crossentropy',metrics = ['accuracy'])\n\nmodel.summary()\n","39a89bae":"from keras.callbacks import ReduceLROnPlateau\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)","44ea8069":"model.fit(datagen.flow(x_train,y_train,batch_size=32),epochs = 20, validation_data = datagen.flow(x_val,y_val),callbacks = [learning_rate_reduction])","7eb4ba5c":"model.evaluate(x_test,y_test)","2eb7c1ef":"**91.5 % accuracy of the model on given test set**\n","6b2d15e5":"**0.48 loss of model on given test set **\n"}}