{"cell_type":{"f490b141":"code","ad340034":"code","78ea392b":"code","8daf61e9":"code","245661c7":"code","b7f30efd":"code","4af81085":"code","d8b617c4":"code","e709f469":"code","cb976fc4":"code","81790a58":"code","2d0b8e2f":"code","ef2e1ec1":"code","bc87b2e4":"code","b06db9cd":"code","b8e3251f":"code","d8a8ebe5":"code","0c8192d0":"code","d7d74209":"code","7f55ac12":"code","a5c702fd":"code","fd87fdbb":"code","c8bade7b":"code","115d7eae":"code","e16f1e19":"code","93b9c879":"code","f704bf3c":"code","38454ff5":"code","280226d4":"code","2190919f":"code","59b647f4":"code","fd92474f":"code","c240c16b":"code","daeefbe5":"code","330de94e":"code","e88612cd":"code","b0a76d64":"code","1368f597":"code","bbf0405b":"code","927c85c7":"code","a3b2a481":"code","efc99f43":"code","850e4422":"code","2cca977b":"code","fe144e57":"code","3790c16b":"code","7edd81e9":"code","7de389df":"code","a80fb0ce":"code","b474772b":"code","b61dbaff":"code","d8804e0e":"code","628f159a":"code","15b83ee4":"code","7a6e88d0":"code","50075187":"code","66552ad8":"code","50820cac":"code","c00f2c0b":"code","06c0ad4a":"code","1280a8f3":"code","65c38e36":"code","916e082c":"code","042435c9":"code","07957bed":"code","dedb88b6":"code","80fba50a":"code","eccbf6bc":"code","bd1fd2dc":"code","30d14720":"code","9cbf4e5a":"code","7096e1f8":"code","6b1f61a3":"code","f88fc0d5":"code","be5367d9":"code","fdcd9285":"code","5278c4b0":"code","313dfbf5":"code","f8cef32c":"code","f7a92446":"code","ec5ed1f4":"code","ef2ffdd1":"code","54f2cf24":"code","d4ccc3c4":"code","84147c8e":"code","640a69c8":"code","325eafaa":"code","958eea91":"code","a67ef180":"code","010a0616":"code","7b9c206a":"code","3e48c106":"code","044f748b":"markdown","d1077377":"markdown","d3bf36e0":"markdown","2fc8f270":"markdown","3d042547":"markdown","a718b740":"markdown","ad5b2388":"markdown","5a779603":"markdown","f7a9d339":"markdown","63e3cb80":"markdown","6b38f5be":"markdown","7218d106":"markdown","c516c733":"markdown","64528ee0":"markdown","66f3222b":"markdown","a21a0ea9":"markdown","0b97929a":"markdown","c2f523ce":"markdown","adb28e1a":"markdown","d8b47f66":"markdown","516e452e":"markdown","e769e182":"markdown","899f027b":"markdown","525ae420":"markdown","85043900":"markdown","dd1bc499":"markdown","7137a481":"markdown","def39782":"markdown","c4cf8f81":"markdown","97161b78":"markdown","05edd283":"markdown","dd509e48":"markdown","1d1c8bfc":"markdown","71201100":"markdown","ebd72021":"markdown","543118c7":"markdown","bb79eef8":"markdown","77c0566f":"markdown"},"source":{"f490b141":"! pip install --upgrade rasterio\n! pip install -U git+https:\/\/github.com\/qubvel\/segmentation_models.pytorch\n","ad340034":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # image plotting\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 300 #increase plot resolution\n\n#Raster data handling\nfrom PIL import Image\nimport skimage\nfrom skimage import io, transform \nimport rasterio as rio # geo-oriented plotting library\nfrom rasterio import features\nimport cv2\n\n#PyTorch\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Sampler # custom dataset handling\nimport torch.autograd.profiler as profiler # to track model inference and detect leaks\nfrom torchvision import transforms, utils\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch.nn.modules.padding import ReplicationPad2d\nimport torchvision.models as models\nfrom torch import optim\nfrom collections import OrderedDict\nimport segmentation_models_pytorch as smp #semantic segmentation models and utils\nfrom torch.cuda.amp import GradScaler\nfrom torch.cuda.amp import autocast\n\n#Augmentations\nimport albumentations as alb\nfrom albumentations.pytorch import ToTensorV2\n\n#Logging errors and progress, sending them to tg-bot\nimport requests\nimport traceback\n\n#Other\nfrom pathlib import Path # to have path strings as PosixPath objexts\nimport pathlib \nfrom pyproj import CRS\nimport geopandas as gpd # to make dataframes out of geojson files\nimport itertools\nimport re\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport os\nimport gc\nfrom timeit import default_timer as time\nimport copy\nimport json\nimport logging\nimport datetime\n\n\n","78ea392b":"import warnings\nwarnings.filterwarnings(\"ignore\")","8daf61e9":"# Input\ntrain_dir = Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train')\ntest_dir = Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_test_public')\nsample_dir = Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train_sample')\n\n# Output\noutput_path = Path.cwd()\noutput_csv_path = output_path\/'output_csvs\/'\nPath(output_csv_path).mkdir(parents=True, exist_ok=True)\n\n# \u0422\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435: \u0440\u0430\u0441\u0442\u0440\u044b, geojson \u0438 geodataframe \u0441 \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b\u043e\u043c \u0432 24 \u043c\u0435\u0441\u044f\u0446\u0430\ntest_raster_path = Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train_sample\/sample\/L15-0506E-1204N_2027_3374_13\/images_masked\/global_monthly_2018_01_mosaic_L15-0506E-1204N_2027_3374_13.tif')\ntest_raster_path_24 = Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train_sample\/sample\/L15-0506E-1204N_2027_3374_13\/images_masked\/global_monthly_2019_12_mosaic_L15-0506E-1204N_2027_3374_13.tif')\ntest_geojson_path = Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train_sample\/sample\/L15-0506E-1204N_2027_3374_13\/labels_match_pix\/global_monthly_2018_01_mosaic_L15-0506E-1204N_2027_3374_13_Buildings.geojson')\ntest_geojson_path_24 = Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train_sample\/sample\/L15-0506E-1204N_2027_3374_13\/labels_match_pix\/global_monthly_2019_12_mosaic_L15-0506E-1204N_2027_3374_13_Buildings.geojson')\ntest_gdf = gpd.read_file(test_geojson_path)\ntest_gdf_24 = gpd.read_file(test_geojson_path_24)\n\n# \u0414\u0435\u043b\u0430\u0435\u043c \u0438\u0437 Id \u0438\u043d\u0434\u0435\u043a\u0441\u044b \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430 \u0438 \u0441\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u043f\u043e \u0438\u043d\u0434\u0435\u043a\u0441\u0443\ntest_gdf.set_index('Id',inplace=True)\ntest_gdf_24.set_index('Id',inplace=True)\n\ntest_gdf.sort_index(inplace=True)\ntest_gdf_24.sort_index(inplace=True)\n\n","245661c7":"def unparse_path(string):\n    pattern = r'\/(train|test_public|sample)\/(L.+)\/(\\w+)\/(.+_(\\d+)_(\\d+)_m.+_\\d+_\\d+_\\d+)(?:_(\\w+))?.(\\w+)'\n    match = re.findall(pattern=pattern,string=string)\n    return match[0]","b7f30efd":"sample_string = '..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train\/train\/L15-0331E-1257N_1327_3160_13\/labels_match_pix\/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson'\nunparse_path(sample_string)","4af81085":"def get_paths(path):\n    return [str(pth) for pth in Path.glob(path,pattern = '**\/*.*')]\n\npathlist_train = get_paths(train_dir)\npathlist_test = get_paths(test_dir)\npathlist_sample = get_paths(sample_dir)\n","d8b617c4":"print(f'train_files: {len(pathlist_train)} \\n test_files: {len(pathlist_test)} \\n sample_files: {len(pathlist_sample)}')","e709f469":"def get_meta_from_pathlist(pathlist):\n    keys = ['parent', 'location_dir', 'sub_dir', 'filename', 'year', 'month', 'file_type', 'extension', 'source']\n    temp_dir = {key: [] for key in keys}\n    for path in pathlist:\n        temp_dir['source'].append(path)\n        meta = unparse_path(path)\n        for i,data in enumerate(meta):\n            temp_dir[keys[i]].append(data)\n    return pd.DataFrame(temp_dir)","cb976fc4":"train_meta_df = get_meta_from_pathlist(pathlist_train)\ntest_meta_df = get_meta_from_pathlist(pathlist_test)\nsample_meta_df = get_meta_from_pathlist(pathlist_sample)\nconcatenated_df = pd.concat([train_meta_df, test_meta_df, sample_meta_df]).reset_index()","81790a58":"train_meta_df.head(5)","2d0b8e2f":"test_meta_df.head(5)","ef2e1ec1":"sample_meta_df","bc87b2e4":"concatenated_df.file_type.value_counts()","b06db9cd":"concatenated_df[concatenated_df.file_type == ''].extension.unique()","b8e3251f":"train_meta_df.loc[train_meta_df['file_type'] == '', 'file_type'] = 'sat_picture'\ntest_meta_df.loc[test_meta_df['file_type'] == '', 'file_type'] = 'sat_picture'\nsample_meta_df.loc[sample_meta_df['file_type'] == '', 'file_type'] = 'sat_picture'\nconcatenated_df.loc[concatenated_df['file_type'] == '', 'file_type'] = 'sat_picture'","d8a8ebe5":"concatenated_df.file_type.value_counts()","0c8192d0":"def get_metadata(path):\n    pathlist = get_paths(path)\n    meta_df = get_meta_from_pathlist(pathlist)\n    meta_df.loc[meta_df['file_type'] == '', 'file_type'] = 'Sat_picture'\n    \n    # Choosing udm masks only and taking the indices\n    condition = (meta_df['sub_dir'] == 'UDM_masks')\n    udm_indices = meta_df.loc[condition].index\n    \n    # Get list of unique file names that have UDMs and taking their list\n    udm_fnames = list(meta_df.loc[udm_indices,'filename'])\n    udm_mask = meta_df['filename'].progress_map(lambda x: x in udm_fnames)\n    \n    # Initialize has_udm feature as False\n    meta_df['has_udm'] = False\n    # Changing has_udm feature for files mentioned in list above\n    meta_df.loc[udm_mask,'has_udm'] = True\n\n    return meta_df","d7d74209":"train_meta_df = get_metadata(train_dir)\ntest_meta_df = get_metadata(test_dir)\nsample_meta_df = get_metadata(sample_dir)\nconcatenated_meta_df = pd.concat([train_meta_df, test_meta_df, sample_meta_df]).reset_index()","7f55ac12":"concatenated_meta_df.has_udm.value_counts()","a5c702fd":"train_meta_df.to_csv(output_csv_path\/'meta_train.csv')\ntest_meta_df.to_csv(output_csv_path\/'meta_test.csv')\nsample_meta_df.to_csv(output_csv_path\/'meta_sample.csv')\nconcatenated_meta_df.to_csv(output_csv_path\/'meta_concat.csv')","fd87fdbb":"concatenated_meta_df[concatenated_meta_df.has_udm == True].filename.iloc[0]","c8bade7b":"def make_untidy(ser):\n    \"\"\"Function for making untidy data out of tidy dataframe\"\"\"\n    part = ser['parent']\n    aoi = ser['location_dir']\n    year = ser['year']\n    month = ser['month']\n    filename = ser['filename']\n    has_udm = ser['has_udm']\n    \n    #making images_masked adresses\n    images_masked =  aoi + '\/images_masked\/' + filename + '.tif'\n    \n    # test public has images_masked only\n    if part == 'test_public':\n        images = None\n        labels = None\n        labels_match = None\n        labels_match_pix = None\n        UDM_masks = None\n    else:\n        images = aoi + '\/images\/' + filename + '.tif'\n        labels = aoi + '\/labels\/' + filename + '.geojson'\n        labels_match = None\n        labels_match_pix = None\n        \n        if has_udm == True:\n            UDM_masks = aoi + '\/UDM_masks\/' + filename + '_UDM.tif'\n        else: UDM_masks = None\n        \n        images = aoi + '\/images\/' + filename + '.tif'\n        labels_buildings = aoi + '\/labels\/' + filename + '_Buildings.geojson'\n        labels_udm = aoi + '\/labels\/' + filename + '_UDM.geojson'\n        labels_match = aoi + '\/labels_match\/' + filename + '_Buildings.geojson'\n        labels_match_pix = aoi + '\/labels_match_pix\/' + filename + '_Buildings.geojson'\n        \n    keys = ['part', 'aoi', 'filename', 'year', 'month', 'has_udm', 'UDM_masks', 'images', 'images_masked', 'labels', 'labels_match', 'labels_match_pix'] \n    values = [part, aoi, filename, year, month, has_udm, UDM_masks, images, images_masked, labels, labels_match, labels_match_pix]\n    temp_dict = {k:v for k, v in zip(keys,values)}\n    return pd.Series(temp_dict)\n\ndef make_untidy_dataframe(df):\n    return df.progress_apply(lambda x: make_untidy(x), axis = 1).drop_duplicates()","115d7eae":"train_untidy_df = make_untidy_dataframe(train_meta_df)\ntest_untidy_df = make_untidy_dataframe(test_meta_df)\nsample_untidy_df = make_untidy_dataframe(sample_meta_df)\nconcatenated_untidy_df = pd.concat([train_untidy_df, test_untidy_df, sample_untidy_df]).reset_index()","e16f1e19":"train_untidy_df.to_csv(output_csv_path\/'untidy_train.csv')\ntest_untidy_df.to_csv(output_csv_path\/'untidy_test.csv')\nsample_untidy_df.to_csv(output_csv_path\/'untidy_sample.csv')\nconcatenated_untidy_df.to_csv(output_csv_path\/'untidy_concat.csv')","93b9c879":"label_csv_path = Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train_csvs\/csvs\/sn7_train_ground_truth_pix.csv')\ndf = pd.read_csv(label_csv_path)","f704bf3c":"df.sample(5)","38454ff5":"print(df.filename.unique())\nprint(len('L15-0506E-1204N_2027_3374_13'))","280226d4":"def extract_fname_feats(string):\n    \"\"\"Function that extract metadata from filenames by regular expressions\"\"\"\n    date_pattern = r'\\d+'\n    id_pattern = r'_(L.+)'\n    extract = lambda x, y: re.findall(pattern=x,string=y)\n    date = tuple(extract(date_pattern, string)[0:2])\n    aoi = extract(id_pattern, string)[0]\n    \n    feat_dict = {'date':date,\n                'aoi': aoi.replace('.tif','')}\n    return feat_dict\n    ","2190919f":"test_str = 'global_monthly_2019_12_mosaic_L15-0506E-1204N_2027_3374_13.tif'\nfeats = extract_fname_feats(test_str)","59b647f4":"df['date'], df['aoi'] = zip(*df['filename'].progress_apply(lambda x: list(extract_fname_feats(x).values())))\ndf['year'], df['month'] = zip(*df['date'].progress_map(lambda x: x[0:2]))\ndf['date'] = df['date'].progress_apply(lambda x: datetime.date(int(x[0]), int(x[1]), 1))","fd92474f":"df.aoi.value_counts()\n","c240c16b":"df.year.value_counts()\n","daeefbe5":"print(f'Date range: {df.date.min()} - {df.date.max()}')","330de94e":"test_gdf.head(5)","e88612cd":"raster = rio.open(test_raster_path)\nprint(raster.meta)\nprint(raster.crs.wkt)\nr = raster.read()\nprint(r.shape)","b0a76d64":"def plot_polygons(gdf, fill = False, l_wd = 0.2, axs = None, color = 'yellow'):\n    \"\"\"Function to plot polions of structures\n    gdf: geodataframe with polygons geometry\n    fill: boolean, defines either polygons filled or not\n    l_wd: float, linewidth of the polygon contour\n    axs: ax, variable to specify already existing ax to plot, for example: to overlay a sattelite picture\"\"\"\n    \n    if axs == None:\n        _, ax = plt.subplots(1, figsize = (3,3))\n    \n    for geom in gdf.geometry:\n        if fill:\n            # for highliting above the raster\n            ax = axs\n            patch = PolygonPatch(geom,color=color, linewidth = l_wd)\n            ax.add_patch(patch)\n        else:\n            ax.plot(*geom.exterior.xy,linewidth=l_wd)\n    return(ax)","1368f597":"plot_polygons(test_gdf)","bbf0405b":"def plot_satellite(path, gdf = None, fill = False, l_wd = 0.2):\n    \"\"\"Function to plot sattelite image with ability to overlay gdf polygons\n    path: string, path to raster\n    gdf: geodataframe that we are going to use to plot polygons\n    fill: boolean, either polygons are filled or not\n    l_wd: float, linewidth\"\"\"\n    \n    fig, ax = plt.subplots(1, figsize = (3,3))\n    fig.tight_layout() #to adjust subplots layout\n    \n    sat = rio.open(path)\n    sat = sat.read()\n    sat = sat.transpose((1,2,0,))\n    ax.imshow(sat)\n    \n    if gdf is not None:\n        plot_polygons(gdf,fill=fill,axs=ax,l_wd=l_wd)\n        \n    return(ax)\n\n    ","927c85c7":"plot_satellite(test_raster_path)","a3b2a481":"plot_satellite(test_raster_path, test_gdf, fill = True)","efc99f43":"def make_mask(source, raster):\n    return features.rasterize(((polygon, 255) for polygon in source['geometry']),out_shape=raster.shape)\n    \n\ndef rasterize_mask(source, raster_path, ftype = 'gdf'):\n    \n    \"\"\"Function that allows us to make rasterized mask\n    ftype: string, 'gdf' or 'geojson', defines what is used for source of geometry: dataframe or json\n    source: string or pandas dataframe object, path to geojson or dataframe vatiable\n    raster_path: path to raster which is used as reference to mask shape\n    \"\"\"\n    \n    if ftype == 'gdf':\n        with rio.open(raster_path) as raster:\n            r = raster.read(1)\n            mask = make_mask(source, r)\n        return mask\n            \n    elif ftype == 'geojson':\n        gdf = gpd.read_file(source)\n        with rio.open(raster_path) as raster:\n            r = raster.read(1)\n            mask = make_mask(gdf, r)\n        return mask\n    \n    else:\n        raise ValueError('ftype is incorrect, it can be either \"gdf\" or \"json\"')\n        ","850e4422":"test_mask = rasterize_mask(test_geojson_path, test_raster_path, ftype = 'geojson')\ntest_mask_24 = rasterize_mask(test_geojson_path_24, test_raster_path_24, ftype = 'geojson')\nmasks = [test_mask, test_mask_24]\nmonths = ['month_1', 'month_24']","2cca977b":"print(f'Mask data type {type(test_mask)}, unique values in mask - {np.unique(test_mask)}, mask shape - {test_mask.shape}')","fe144e57":"_, axs = plt.subplots(1,2, figsize = (10,10))\n_.tight_layout()\nfor i, ax in enumerate(axs):\n    ax.set_title(months[i])\n    ax.imshow(masks[i])","3790c16b":"def gdf_difference(gdf1, gdf2):\n    \"\"\"Function that leaves only structures which make difference\n    between two versions of the geodataframe \n    \n    Args:\n    gdf1: first geodataframe with polygons (earlier one)\n    gdf2: second geodataframe with polygons (later one)\n    Return:\n    gdf with polygons that make difference\n    \"\"\"\n    try:\n        gdf1.reset_index(inplace=True,drop=True)\n    except:\n        pass\n    try:\n        gdf2.reset_index(inplace=True,drop=True)\n    except:\n        pass\n    \n    \n    len_1 = len(gdf1)\n    len_2 = len(gdf2)\n    \n    len_diff = abs(len_2-len_1)\n    \n    if len_2 > len_1:\n        start_index = len_2-len_diff\n        diff_gdf = gdf2[start_index:].copy()\n    else:\n        start_index = len_1-len_diff\n        diff_gdf = gdf1[start_index:].copy()\n\n    diff_gdf.reset_index(inplace=True,drop=True)\n        \n    return diff_gdf","7edd81e9":"def get_difference(gdf1, gdf2):\n    \n    \"\"\"Function that leaves only new structures between two versions of the geodataframe \n    and classifies either structure is built or demolished\n    \n    Args:\n    gdf1: first geodataframe with polygons (earlier one)\n    gdf2: second geodataframe with polygons (later one)\n    Return:\n    gdf with polygons that make difference\n    \"\"\"\n    \n    df1 = gdf1.copy()\n    df2 = gdf2.copy()\n    df1.drop(['area','image_fname','iou_score'], inplace = True, axis = 1, errors = 'ignore')\n    df2.drop(['area','image_fname','iou_score'], inplace = True, axis = 1, errors = 'ignore')\n    df1['time'] = 'before'\n    df2['time'] = 'after'\n    \n    difference = df1.merge(df2, how = 'outer', on = 'geometry')\n    difference = difference[(difference.time_x.isna() == True) | (difference.time_y.isna() == True)]\n    difference['mark'] = difference['time_x'].apply(lambda x: 'before' if x == 'before' else 'after')\n    difference.drop(['time_x', 'time_y'], axis = 1, inplace = True)\n    return difference","7de389df":"diff = get_difference(test_gdf, test_gdf_24)","a80fb0ce":"builded = diff[diff['mark'] == 'after']\ndestroyed = diff[diff['mark'] == 'before']","b474772b":"bld_diff = rasterize_mask(builded,test_raster_path)\ndst_diff = rasterize_mask(destroyed,test_raster_path)","b61dbaff":"_,axs = plt.subplots(1,4,figsize=(10,10))\n_.tight_layout()\n\nmasks = [test_mask,test_mask_24, bld_diff, dst_diff]\ntitles = ['month 1', 'month 24', 'builded', 'destroyed']\n\nfor i,ax in enumerate(axs):\n    ax.set_title(titles[i])\n    ax.imshow(masks[i]);","d8804e0e":"class ChipCreator():\n    \"\"\"Class that allows us to split bigger satellite picture and mask into smaller pieces\"\"\"\n    \n    def __init__(self, dimension, is_raster = False):\n        self.dimension = dimension\n        self.raster = is_raster\n        \n    def __read_image(self,image):\n        # checks whether image is a path or array\n        if isinstance(image,(pathlib.PurePath,str)):\n                with Image.open(image) as img:\n                    # converts image into np array\n                    np_array = np.array(img)\n                return np_array\n            \n        elif isinstance(image,np.ndarray):\n            return image\n        else:\n            raise ValueError(f\"Expected Path or Numpy array received: {type(image)}\")\n        \n    def make_chips(self, image):\n        \n        #getting image and converting to np.array if necessary\n        np_array = self.__read_image(image)\n        \n        # then get numbers of chips per row and column\n        n_rows = (np_array.shape[0] - 1) \/\/ self.dimension + 1\n        n_cols = (np_array.shape[1] - 1) \/\/ self.dimension + 1\n        \n        chip_list = [] #\n        for r in range(n_rows):\n            for c in range(n_cols):\n                #starting row and column\n                start_r_idx = r*self.dimension\n                end_r_idx = start_r_idx + self.dimension\n                #ending row and column\n                start_c_idx = c*self.dimension\n                end_c_idx = start_c_idx + self.dimension\n                #cutting fragment by indexes\n                chip = np_array[start_r_idx:end_r_idx,start_c_idx:end_c_idx]\n                \n                if self.raster:\n                    # if raster is True then format is (channels, rows, columns)\n                    # else (rows, columns, channels)\n                    chip = np.moveaxis(chip,-1,0)\n\n                chip_list.append(chip)\n\n        return np.array(chip_list)\n    def __call__(self, image):\n        # slightly different verison of make_chips\n        np_array = self.__read_image(image)\n        n_rows = (np_array.shape[1] - 1) \/\/ self.dimension + 1\n        n_cols = (np_array.shape[2] - 1) \/\/ self.dimension + 1\n        chip_dict = {'chip':[],'x':[],'y':[], 'blank':[]}\n        for r in range(n_rows):\n            for c in range(n_cols):\n                start_r_idx = r*self.dimension\n                end_r_idx = start_r_idx + self.dimension\n\n                start_c_idx = c*self.dimension\n                end_c_idx = start_c_idx + self.dimension\n                chip = np_array[:,start_r_idx:end_r_idx,start_c_idx:end_c_idx]\n\n                chip_dict['chip'].append(chip)\n                chip_dict['x'].append(start_r_idx)\n                chip_dict['y'].append(start_c_idx)\n                if chip.mean() == 0 and chip.sum() == 0:\n                    chip_dict['blank'].append('_blank')\n                else:\n                    chip_dict['blank'].append('')\n        return chip_dict\n    \ndef plot_many(pictures, ncols = 4, dpi = 300, is_raster = False):\n    matplotlib.rcParams['figure.dpi'] = dpi\n    nrows = (len(pictures) - 1) \/\/ ncols + 1\n    \n    fig,axs = plt.subplots(nrows,ncols,figsize=(10,10))\n    fig.tight_layout()\n    \n     \n    for r,ax in enumerate(axs):\n        for c,row in enumerate(ax):\n            # i is current index in array of axes\n            i = r*ncols + c\n            ax[c].set_title(i)\n            image = pictures[i]\n            # unmaking raster format if necessary\n            if is_raster:\n                image = np.moveaxis(image,0,-1)\n            ax[c].imshow(image);\n\n    ","628f159a":"\u0441hips_256 = ChipCreator(256, is_raster = True)\nplot_many(\u0441hips_256.make_chips(test_raster_path), is_raster = True)","15b83ee4":"plot_many(\u0441hips_256.make_chips(test_mask), is_raster = True)","7a6e88d0":"root_sample_folder =  Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train_sample\/sample')\ncsv_sample_path = Path('.\/output_csvs\/untidy_sample.csv')\nroot_train_folder =  Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train\/train')\ncsv_train_path = Path('.\/output_csvs\/untidy_train.csv')","50075187":"class CustomSatelliteDataset():\n    \"\"\"SpaceNet7 dataset imagery holder\"\"\"\n    \n    def __init__(self, csv, root_folder, udm = False, transform = None, chip_dim = None):\n        \"\"\"\n        csv: string, path to the csv file with untidy dataframe with all elements adresses\n        root_folder: string, path where imagery is stored\n        udm_use: boolean, condition which specify using of udm masks\n        transform: np.array, transformation matrix if needed\n        chip_dim: int, pixel dimension of the chip size\"\"\"\n        \n        self.csv = pd.read_csv(csv)\n        self.root_folder = root_folder\n        self.udm_use = udm\n        self.transform = transform\n        self.chip_dim = chip_dim\n        \n        \n        if chip_dim is not None:\n            self.\u0441hip_gen = self.__ChipGenerator(dimension = self.chip_dim)\n            # \u0432 \u044d\u0442\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0432\u0441\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 1024\u04251024\n            self.n_chips = ((1024 - 1) \/\/ self.chip_dim + 1)**2\n        \n        self.idx_combinations = self.__get_all_idx_combinations()\n        self.max = self.__len__()\n            \n        \n    def __len__(self):\n        \"\"\"Function that returns the dataset length including chip division\"\"\"\n        if self.chip_dim is not None:\n            return len(self.idx_combinations)*self.n_chips\n        else:\n            return len(self.idx_combinations)\n    \n    def __getitem__(self, idx):\n        \"\"\"Function for read images as required, but not to store them all in memory\"\"\"\n        #checking chip division\n        if self.chip_dim is not None:\n            img_idx = idx\/\/self.n_chips\n            chip_idx = idx%self.n_chips\n        else:\n            img_idx = idx\n        \n        #checking is idx numer or tensor\n        if torch.is_tensor(img_idx):\n            img_index = img_index.to_list()\n        # getting two images    \n        idx1,idx2 = self.idx_combinations[img_idx]\n        # and their path\n        img1_path = self.root_folder\/self.csv.loc[idx1,'images_masked']\n        img2_path = self.root_folder\/self.csv.loc[idx2,'images_masked']\n        # getting building polygons\n        labels1_path = self.root_folder\/self.csv.loc[idx1,'labels_match_pix']\n        labels2_path = self.root_folder\/self.csv.loc[idx2,'labels_match_pix']\n        # reading rasters from the paths\n        with rio.open(img1_path) as r1, rio.open(img2_path) as r2:\n            raster1 = r1.read()[0:3]  \n            raster2 = r2.read()[0:3]\n            raster_bounds = r1.bounds\n            rio_transform = r1.transform\n        # to get difference we pass two concatenated images\n        raster_diff = np.concatenate((raster1,raster2),axis=0)\n        # getting  their dates \n        date1 = tuple(self.csv.loc[idx1,['month','year']])\n        date2 = tuple(self.csv.loc[idx2,['month','year']])\n        # read geojson files fwith polygons\n        gdf1 = gpd.read_file(labels1_path).set_index('Id').sort_index()\n        gdf2 = gpd.read_file(labels2_path).set_index('Id').sort_index()\n        \n        # \u044d\u0442\u0438\u0445 \u043f\u043e\u043a\u0430 \u043d\u0435\u0442 (\u0434\u043e\u043f\u0438\u0441\u0430\u043b)\n        # get the change between the 2 satellite images by comparing their polygons\n        gdf_diff = self.__geo_difference(labels1_path,labels2_path)\n        # get the corresponding rasterized image of the geodataframes\n        mask_diff = self.__rasterize_gdf(gdf_diff,out_shape=raster1.shape[1:3])   \n        \n        sample = {'raster1':raster1,'raster2':raster2,'raster_diff':raster_diff,'raster_bounds':raster_bounds,'rio_transform':rio_transform,\n                  'date1':date1,'date2':date2,'mask_diff':mask_diff,'fname':img1_path.parent.parent.stem}\n        \n        if self.transform:\n            sample = self.transform(sample)\n            \n        return sample\n        \n        \n    \n    def  __get_all_idx_combinations(self):\n        \n        all_combinations = []\n        # group by area of interest\n        aoi_groups = self.csv.groupby('aoi')\n        # loop through the groups and get the different index combinations\n        for i,aoi in enumerate(aoi_groups):\n            # get the dataframe in the group\n            loc_frame = aoi[1]\n            # excluding unidentified masks\n            condition = (loc_frame['has_udm'] == False)\n            # return a list of the indices in the location dataframe\n            l = list(loc_frame[condition].index)\n            # use itertools to get all the different combinations between 2 in the list\n            combinations = list(itertools.combinations(l,2))\n            all_combinations.extend(combinations)\n        return all_combinations\n    \n    def __geo_difference(self,geojson1,geojson2):\n        # read geojson into geodataframes\n        gdf1 = gpd.read_file(geojson1).set_index('Id').sort_index()\n        gdf2 = gpd.read_file(geojson2).set_index('Id').sort_index()\n\n        # get geodataframe lengths\n        len_1 = len(gdf1)\n        len_2 = len(gdf2)\n        # check which gdf is longer\n        len_diff = abs(len_2-len_1)\n\n        if len_2 > len_1:\n            start_index = len_2-len_diff\n            diff_gdf = gdf2.iloc[start_index:].copy()\n        else:\n            start_index = len_1-len_diff\n            diff_gdf = gdf1.iloc[start_index:].copy()\n\n        # reset the index\n        diff_gdf.reset_index(inplace=True,drop=True)\n\n        return diff_gdf\n\n    \n    def __rasterize_gdf(self,gdf,out_shape):\n        # if geodataframe is empty return empty mask\n        if len(gdf)==0:\n            return np.zeros((1,*out_shape))\n            \n        mask = features.rasterize(((polygon, 255) for polygon in gdf['geometry']),out_shape=out_shape)\n        \n        return np.expand_dims(mask,axis=0)\n    \n    class __ChipCreator():\n        \"\"\"Class that allows us to split bigger satellite picture and mask into smaller pieces\"\"\"\n        # \u043f\u0440\u0438 \u043f\u043e\u043f\u044b\u0442\u043a\u0435 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u044e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u0444\u043e\u0440\u043c\u0443 \u043c\u0430\u0441\u0441\u0438\u0432\u0430, 4 \u043a\u0430\u043d\u0430\u043b\u0430 \u0432 \u043d\u0430\u0447\u0430\u043b\u0435 \u0438\u043b\u0438 \u0432 \u043a\u043e\u043d\u0446\u0435 \u0438 \u043d\u0443\u0436\u043d\u043e \u043b\u0438 \u0434\u0432\u0438\u0433\u0430\u0442\u044c \u043e\u0441\u044c    \n        def __init__(self, dimension):\n            self.dimension = dimension\n            self.chip_dict = {'chip':[],'x':[],'y':[], 'blank':[]}\n        \n        def __read_image(self,image):\n            # checks whether image is a path or array\n            if isinstance(image,(pathlib.PurePath,str)):\n                with Image.open(image) as img:\n                    # converts image into np array\n                    np_array = np.array(img)\n                    return np_array\n            \n            elif isinstance(image,np.ndarray):\n                return image\n            else:\n                raise ValueError(f\"Expected Path or Numpy array received: {type(image)}\")\n        \n        def __call(self, image):\n        \n            #getting image and converting to np.array if necessary\n            np_array = self.__read_image(image)\n            chip_dict = {'chip':[],'x':[],'y':[], 'blank':[]}\n        \n            # then get numbers of chips per row and column\n            n_rows = (np_array.shape[0] - 1) \/\/ self.dimension + 1\n            n_cols = (np_array.shape[1] - 1) \/\/ self.dimension + 1\n        \n            for r in range(n_rows):\n                for c in range(n_cols):\n                    #starting row and column\n                    start_r_idx = r*self.dimension\n                    end_r_idx = start_r_idx + self.dimension\n                    #ending row and column\n                    start_c_idx = c*self.dimension\n                    end_c_idx = start_c_idx + self.dimension\n                    #cutting fragment by indexes\n                    chip = np_array[:, start_r_idx:end_r_idx,start_c_idx:end_c_idx]\n                    #filling dictionary\n                    chip_dict['chip'].append(chip)\n                    chip_dict['x'].append(start_r_idx)\n                    chip_dict['y'].append(start_c_idx)\n                    \n                    # Marking blank chips\n                    if chip.mean() == 0 and chip.sum() == 0:\n                        chip_dict['blank'].append(1)\n                    else:\n                        chip_dict['blank'].append(0)\n\n            return chip_dict","66552ad8":"class DatasetCreator():\n    def __init__(self,chip_dimension=256):\n        self.chip_dimension = chip_dimension\n    \n    def __call__(self,dataset):\n        for d in tqdm(dataset):\n            raster_diff = d['raster_diff']\n            mask_diff = d['mask_diff']\n            \n            self.__fname = d['fname']\n            self.__date1= d['date1']\n            self.__date2 = d['date2']\n            self.__raster_bounds = d['raster_bounds']\n            self.__transform = d['rio_transform']\n            self.__raster_shape = raster_diff.shape[1:3]\n            \n            \n            \n            self.__save_chips(image=raster_diff,subdir_name='chips')\n            self.__save_chips(image=mask_diff,subdir_name='masks')\n            \n            \n    def __save_chips(self,image,subdir_name='chips'):\n        \n        month1,year1 = self.__date1\n        month2,year2 = self.__date2\n        \n        chip_generator = ChipCreator(dimension=self.chip_dimension)\n        chip_dict = chip_generator(image)\n        \n        x_coords = chip_dict['x']\n        y_coords = chip_dict['y']\n        chips = chip_dict['chip']\n        blanks = chip_dict['blank'] \n        \n        im_dir = Path('chip_dataset\/change_detection')\/Path(self.__fname)\/Path(subdir_name)\/Path(f'{year1}_{month1}_{year2}_{month2}')\n        im_dir.mkdir(parents=True, exist_ok=True)\n\n        for chip,x,y,blank in zip(chips,x_coords,y_coords,blanks):\n            im_name = f'global_monthly_{year1}_{month1}_{year2}_{month2}_chip_x{x}_y{y}_{self.__fname}{blank}.tif'\n            im_path = im_dir\/im_name\n            \n            if subdir_name == 'chips':\n                count = 6\n            else:\n                count = 1\n            \n            # Calculate the new bounds for the raster chips\n            transform = self.__get_geo_transform(x,y)\n            \n            profile = {'driver':'GTiff', 'width':self.chip_dimension,'height':self.chip_dimension,'crs':CRS.from_wkt('LOCAL_CS[\"WGS 84 \/ Pseudo-Mercator\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3857\"]]'),\n                       'count':count,'dtype':rio.uint8, 'compress':'zip','transform': transform}\n            \n            with rio.open(im_path, 'w',**profile) as dst:\n                dst.write(chip.astype(rio.uint8))\n                \n    \n    def __get_geo_transform(self,x,y):\n        top = self.__raster_bounds[3]\n        bottom = self.__raster_bounds[1]\n        left = self.__raster_bounds[0]\n        right = self.__raster_bounds[2]\n        \n        raster_height = self.__raster_shape[0]\n        raster_width = self.__raster_shape[1]\n        \n        chip_height = (top-bottom)\/(raster_height\/\/self.chip_dimension)\n        chip_width = (left-right)\/(raster_width\/\/self.chip_dimension)\n        \n        pixel_height = (top-bottom)\/raster_height\n        pixel_width = (left-right)\/raster_width\n        \n        chip_top = top + y * pixel_height\n        chip_bottom = chip_top + chip_height\n        \n        chip_left = left + x * pixel_width\n        chip_right = chip_left + chip_width\n        \n        bounds = {'left': chip_left, 'bottom': chip_bottom, 'right': chip_right, 'top': chip_top}\n        \n        return rio.Affine(self.__transform[0], self.__transform[1], chip_left,self.__transform[3], self.__transform[4], chip_top)","50820cac":"#dataset = CustomSatelliteDataset(root_folder=root_train_folder,csv=csv_train_path)","c00f2c0b":"#dataset_creator = DatasetCreator(chip_dimension=64)","06c0ad4a":"#dataset_creator(dataset=dataset)","1280a8f3":"gc.collect()","65c38e36":"root_folder = '..\/input\/spacenet-7-change-detection-chips-and-masks\/chip_dataset\/chip_dataset\/change_detection\/'\ncsv_path = '..\/input\/spacenet-7-change-detection-chips-and-masks\/annotations.csv'\n\nBATCH_SIZE = 64\nNUM_WORKERS = 8","916e082c":"df = pd.read_csv(csv_path)\ndf.sample(5)","042435c9":"df.target.mean()","07957bed":"# \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0447\u0442\u043e \u0442\u0430\u0440\u0433\u0435\u0442 = 0 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0443\u0441\u0442\u044b\u0435\ndf[df.target == 0].is_blank.value_counts()","dedb88b6":"aoi = df['im_name'].unique()\nlen(aoi)","80fba50a":"train_aoi = aoi[:40]\ntest_aoi = aoi[-20:-10]\nvalid_aoi = aoi[-10:]","eccbf6bc":"def choose_aoi(df, names):\n    mask = df['im_name'].map(lambda x: x in names)\n    return df[mask].reset_index(drop=True)\n\ndf_dict = {'train' : choose_aoi(df, train_aoi),\n          'test' : choose_aoi(df, test_aoi),\n          'valid' : choose_aoi(df, valid_aoi)\n          }\n\ndel(df)","bd1fd2dc":"class TorchDataset(Dataset):\n    \"\"\"Dataset class\n    Args:\n        root_folder: Path object, root directory of picture dataset\n        csv: pandas.DataFrame, untidy df with all data relationships\n        aug: albumentations dictionary\n        preproc: callable, preprocessing function related to specific encoder\n        grayscale: boolean, preprocessing condition to grayscale colored rasters\n    Return:\n        image, mask tensors\"\"\"\n    \n    def __init__(self, root_folder, df, aug = None, preproc = None, grayscale = True):\n        self.root_folder = root_folder\n        self.csv = df\n        self.aug = aug\n        self.preproc = preproc\n        self.grayscale = grayscale\n    \n    def __len__(self):\n        return len(self.csv)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        chip_path = self.root_folder + self.csv.loc[idx,'chip_path']\n        # read chip into numpy array\n        chip = skimage.io.imread(root_folder + self.csv.loc[idx,'chip_path']).astype('float32')\n        if self.grayscale:\n            gray1 = np.dot(chip[:,:,0:3], [0.2989, 0.5870, 0.1140])\n            gray2 = np.dot(chip[:,:,3:], [0.2989, 0.5870, 0.1140])\n            chip = np.divide(np.stack((gray1, gray2),axis = 2),255).astype('float32')\n        # get target for corresponding chip\n        mask = np.abs(np.divide(skimage.io.imread(root_folder + self.csv.loc[idx,'mask_path']),255)).astype('float32')\n        # apply augmentations\n        if self.aug:\n            sample = self.aug(image=chip, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            mask = mask.unsqueeze(0)\n            if self.grayscale:\n                sample = {'I1':image[0,:,:].unsqueeze(0),'I2':image[1,:,:].unsqueeze(0), 'label':mask}\n            else: \n                sample = {'I1':image[0:3,:,:],'I2':image[3:,:,:], 'label':mask}\n            del(image,mask,chip,gray1,gray2)\n            return sample\n        else:\n            image = torch.Tensor(np.moveaxis(chip, 2, 0))\n            mask = torch.Tensor(mask).unsqueeze(0)\n            if self.grayscale:\n                sample = {'I1':image[0,:,:].unsqueeze(0),'I2':image[1,:,:].unsqueeze(0), 'label':mask}\n            else: \n                sample = {'I1':image[0:3,:,:],'I2':image[3:,:,:], 'label':mask}\n            del(mask,chip,gray1,gray2)\n            return sample\n    \n\n    \n    \n\n\nclass BalancedSampler(Sampler):\n    \"\"\"Balancer for torch.DataLoader to adjust chips loading\"\"\"\n    \n    def __init__(self, dataset, percentage = 0.5):\n        \"\"\"\n        dataset: custom torch dataset\n        percentage: float number between 0 and 1, percentage of change containing pictures in batch\n        \"\"\"\n        assert 0 <= percentage <= 1,'percentage must be a value between 0 and 1'\n        \n        self.dataset = dataset\n        self.pct = percentage\n        self.len_ = len(dataset)\n    \n    def __len__(self):\n        return self.len_\n    \n    def __iter__(self):\n        # get indices for chips containing change and blank ones\n        change_chip_idxs = np.where(self.dataset.csv['target'] == 1)[0]\n        blank_chip_idxs = np.where(self.dataset.csv['target'] == 0)[0]\n        # randomly sample from the incides of each class according to percentage value\n        change_chip_idxs = np.random.choice(change_chip_idxs,int(self.len_ * self.pct), replace=True)\n        blank_chip_idxs = np.random.choice(blank_chip_idxs,int(self.len_ * (1 - self.pct))+1, replace=False)\n        # stack and shuffle of sampled indices\n        all_idxs = np.hstack([change_chip_idxs,blank_chip_idxs])\n        np.random.shuffle(all_idxs)\n        return iter(all_idxs)","30d14720":"chip_dimension = 64\naugs = {\n    'train': alb.Compose([\n        alb.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        alb.HorizontalFlip(p=0.5),\n        alb.VerticalFlip(p=0.5),\n        ToTensorV2() #apparently doesn't work properly with smp Unet, included in get_preprocessing function\n    ]),\n    'test': alb.Compose([\n        alb.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        ToTensorV2()\n    ]), \n    'valid': alb.Compose([\n        alb.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        ToTensorV2()\n    ]),\n}","9cbf4e5a":"augs['train']","7096e1f8":"import ast\nfile = open(\"..\/input\/logging-utils\/credentials.txt\", \"r\")\ncontents = file.read()\ntoken = ast.literal_eval(contents)\nfile.close()\n\ndef telegram_bot_sendtext(bot_message):\n    send_text = 'https:\/\/api.telegram.org\/bot' + token['bot_token'] + '\/sendMessage?chat_id=' + token['bot_chatID'] + '&parse_mode=Markdown&text=' + bot_message\n\n    response = requests.get(send_text)\n\n    return response.json()\n\ndef telegram_send_file (file_address):\n    url = f'https:\/\/api.telegram.org\/bot' + token['bot_token'] + '\/sendVoice'\n    #response = requests.post(url, data=data)\n    post_data = {'chat_id': token['bot_chatID']}\n    with open(file_address, 'r+b') as infile:\n        post_file = {'document': infile}\n        r = requests.post(f'https:\/\/api.telegram.org\/bot' +token['bot_token'] + '\/sendDocument', data=post_data, files=post_file)\n        print(r.text)\n\n\ntest = telegram_bot_sendtext(\"Testing Telegram bot\")\nprint(test)\n\ndef log_traceback(ex, ex_traceback=None):\n    if ex_traceback is None:\n        ex_traceback = ex.__traceback__\n    tb_lines = [ line.rstrip('\\n') for line in\n                 traceback.format_exception(ex.__class__, ex, ex_traceback)]\n    return tb_lines","6b1f61a3":"dic = {'a':'b'}\nwith open('d.json', 'w+') as f:\n    json.dump(dic, f, indent=4) \ntelegram_send_file('.\/d.json')","f88fc0d5":"from sklearn.metrics import confusion_matrix\n\nclass IoULoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoULoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        #inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)\/(union + smooth)\n                \n        return 1 - IoU\n\ndef iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n    \"\"\"Fast enough iou calculation function\"\"\"\n    SMOOTH = 1e-6\n    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n    #outputs = outputs.detach()\n    #labels = labels.detach()\n    \n    intersection = (outputs & labels).float().sum((1, 2))\n    union = (outputs | labels).float().sum((1, 2))\n    \n    iou = (intersection + SMOOTH) \/ (union + SMOOTH)  # We smooth our devision to avoid 0\/0\n    \n    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() \/ 10  # This is equal to comparing with thresolds\n    \n    return thresholded.mean() # to get a batch average\n\ndef segmentation_report(running_preds, running_labels):\n    \"\"\"Function to get a closer look to a confusion metrics and related metrics\"\"\"\n    rp = running_preds.flatten()\n    rl = running_labels.flatten()\n    tn, fp, fn, tp = confusion_matrix(rl, rp, labels=[0,1]).ravel()\n    px_accuracy = (tp+tn) \/ (tp+fp+tn+fn)\n    precision = tp \/ (tp+fp)\n    recall = tp \/ (tp+fn)\n    #calculating intersection over union\n    intersection = np.logical_and(rl, rp)\n    union = np.logical_or(rl, rp)\n    iou_score = np.sum(intersection) \/ np.sum(union)\n    fmeasure = 2 * precision * recall \/ (precision + recall)\n    #making report\n    report = { 'tp\/tn\/fp\/fn' : (tp,tn,fp,fn),\n              'px_accuracy': px_accuracy,\n              'precision': precision,\n              'recall': recall,\n              'iou_score': iou_score,\n              'fmeasure':fmeasure\n             }\n    return report\n\n\ndef log_batch_statistics(batch_number,batch_labels,batch_preds,phase,loss,since,num_batches,period=500):\n    if batch_number % period == 0:\n        iou_score = segmentation_report(batch_preds,batch_labels)\n        time_elapsed = time.time() - since\n\n        if phase == 'train':\n            telegram_bot_sendtext('TRAINING BATCH')\n        else:\n            telegram_bot_sendtext('VALIDATION BATCH')\n            \n        telegram_bot_sendtext('-'*50)\n        telegram_bot_sendtext(f'\\n{batch_number}\/{num_batches-1}:')\n        telegram_bot_sendtext(f'Total Time Elapsed: {time_elapsed\/60:.2f} mins')\n        telegram_bot_sendtext(f'Batch Loss: {loss.item():.4f}\\n')\n        telegram_bot_sendtext(f\"``` IoU_score:{iou_score}\\n ```\")\n        telegram_bot_sendtext('-'*50)\n\ndef break_time_limit(start_time,time_limit=28080):\n    time_elapsed = time()-start_time\n    if time_elapsed > time_limit:\n        sys.exit()","be5367d9":"#learning policy params\ngrayscale = True\nif grayscale == True:\n    in_channels = 2\nelse: in_channels = 6\n\nN_EPOCHS = 25\n\n# turning on GPU if possible\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n# cleaning GPU\ngc.collect() \ntorch.cuda.empty_cache()\ntorch.backends.cudnn.benchmark = True\n\n# mean percentage of positives is 6.5% from the frame, median is 3.4%, so weights for bce loss are required.\nweights = torch.Tensor([28]).to(device) \n\n#criterion = torch.nn.BCEWithLogitsLoss(pos_weight = weights) \ncriterion = IoULoss()\n","fdcd9285":"import torch.nn.functional as F\n\nclass Unet(nn.Module):\n    \"\"\"EF segmentation network.\"\"\"\n\n    def __init__(self, input_nbr, label_nbr):\n        super(Unet, self).__init__()\n\n        self.input_nbr = input_nbr\n\n        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n        self.bn11 = nn.BatchNorm2d(16)\n        self.do11 = nn.Dropout2d(p=0.2)\n        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n        self.bn12 = nn.BatchNorm2d(16)\n        self.do12 = nn.Dropout2d(p=0.2)\n\n        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.bn21 = nn.BatchNorm2d(32)\n        self.do21 = nn.Dropout2d(p=0.2)\n        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n        self.bn22 = nn.BatchNorm2d(32)\n        self.do22 = nn.Dropout2d(p=0.2)\n\n        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn31 = nn.BatchNorm2d(64)\n        self.do31 = nn.Dropout2d(p=0.2)\n        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn32 = nn.BatchNorm2d(64)\n        self.do32 = nn.Dropout2d(p=0.2)\n        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn33 = nn.BatchNorm2d(64)\n        self.do33 = nn.Dropout2d(p=0.2)\n\n        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn41 = nn.BatchNorm2d(128)\n        self.do41 = nn.Dropout2d(p=0.2)\n        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn42 = nn.BatchNorm2d(128)\n        self.do42 = nn.Dropout2d(p=0.2)\n        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn43 = nn.BatchNorm2d(128)\n        self.do43 = nn.Dropout2d(p=0.2)\n\n\n        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n        self.bn43d = nn.BatchNorm2d(128)\n        self.do43d = nn.Dropout2d(p=0.2)\n        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n        self.bn42d = nn.BatchNorm2d(128)\n        self.do42d = nn.Dropout2d(p=0.2)\n        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn41d = nn.BatchNorm2d(64)\n        self.do41d = nn.Dropout2d(p=0.2)\n\n        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn33d = nn.BatchNorm2d(64)\n        self.do33d = nn.Dropout2d(p=0.2)\n        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n        self.bn32d = nn.BatchNorm2d(64)\n        self.do32d = nn.Dropout2d(p=0.2)\n        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn31d = nn.BatchNorm2d(32)\n        self.do31d = nn.Dropout2d(p=0.2)\n\n        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn22d = nn.BatchNorm2d(32)\n        self.do22d = nn.Dropout2d(p=0.2)\n        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn21d = nn.BatchNorm2d(16)\n        self.do21d = nn.Dropout2d(p=0.2)\n\n        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn12d = nn.BatchNorm2d(16)\n        self.do12d = nn.Dropout2d(p=0.2)\n        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n\n        self.sm = nn.Sigmoid()\n\n    def forward(self, x1, x2):\n\n        x = torch.cat((x1, x2), 1)\n\n        \"\"\"Forward method.\"\"\"\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\n        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n\n\n        # Stage 4d\n        x4d = self.upconv4(x4p)\n        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\n        x4d = torch.cat((pad4(x4d), x43), 1)\n        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n\n        # Stage 3d\n        x3d = self.upconv3(x41d)\n        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\n        x3d = torch.cat((pad3(x3d), x33), 1)\n        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n\n        # Stage 2d\n        x2d = self.upconv2(x31d)\n        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\n        x2d = torch.cat((pad2(x2d), x22), 1)\n        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n\n        # Stage 1d\n        x1d = self.upconv1(x21d)\n        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\n        x1d = torch.cat((pad1(x1d), x12), 1)\n        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n        x11d = self.conv11d(x12d)\n\n        return self.sm(x11d)\n\nclass SiamUnet_diff(nn.Module):\n    \"\"\"SiamUnet_diff segmentation network.\"\"\"\n\n    def __init__(self, input_nbr, label_nbr):\n        super(SiamUnet_diff, self).__init__()\n\n        self.input_nbr = input_nbr\n\n        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n        self.bn11 = nn.BatchNorm2d(16)\n        self.do11 = nn.Dropout2d(p=0.2)\n        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n        self.bn12 = nn.BatchNorm2d(16)\n        self.do12 = nn.Dropout2d(p=0.2)\n\n        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.bn21 = nn.BatchNorm2d(32)\n        self.do21 = nn.Dropout2d(p=0.2)\n        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n        self.bn22 = nn.BatchNorm2d(32)\n        self.do22 = nn.Dropout2d(p=0.2)\n\n        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn31 = nn.BatchNorm2d(64)\n        self.do31 = nn.Dropout2d(p=0.2)\n        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn32 = nn.BatchNorm2d(64)\n        self.do32 = nn.Dropout2d(p=0.2)\n        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn33 = nn.BatchNorm2d(64)\n        self.do33 = nn.Dropout2d(p=0.2)\n\n        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn41 = nn.BatchNorm2d(128)\n        self.do41 = nn.Dropout2d(p=0.2)\n        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn42 = nn.BatchNorm2d(128)\n        self.do42 = nn.Dropout2d(p=0.2)\n        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn43 = nn.BatchNorm2d(128)\n        self.do43 = nn.Dropout2d(p=0.2)\n\n        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n        self.bn43d = nn.BatchNorm2d(128)\n        self.do43d = nn.Dropout2d(p=0.2)\n        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n        self.bn42d = nn.BatchNorm2d(128)\n        self.do42d = nn.Dropout2d(p=0.2)\n        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn41d = nn.BatchNorm2d(64)\n        self.do41d = nn.Dropout2d(p=0.2)\n\n        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn33d = nn.BatchNorm2d(64)\n        self.do33d = nn.Dropout2d(p=0.2)\n        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n        self.bn32d = nn.BatchNorm2d(64)\n        self.do32d = nn.Dropout2d(p=0.2)\n        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn31d = nn.BatchNorm2d(32)\n        self.do31d = nn.Dropout2d(p=0.2)\n\n        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn22d = nn.BatchNorm2d(32)\n        self.do22d = nn.Dropout2d(p=0.2)\n        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn21d = nn.BatchNorm2d(16)\n        self.do21d = nn.Dropout2d(p=0.2)\n\n        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn12d = nn.BatchNorm2d(16)\n        self.do12d = nn.Dropout2d(p=0.2)\n        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n\n        self.sm = nn.Sigmoid()\n\n    def forward(self, x1, x2):\n\n\n        \"\"\"Forward method.\"\"\"\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n\n        ####################################################\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n\n\n\n        # Stage 4d\n        x4d = self.upconv4(x4p)\n        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)\n        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n\n        # Stage 3d\n        x3d = self.upconv3(x41d)\n        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)\n        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n\n        # Stage 2d\n        x2d = self.upconv2(x31d)\n        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)\n        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n\n        # Stage 1d\n        x1d = self.upconv1(x21d)\n        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)\n        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n        x11d = self.conv11d(x12d)\n\n        return self.sm(x11d)\n","5278c4b0":"!pip install segmentation_models_pytorch\nimport segmentation_models_pytorch as smp","313dfbf5":"if grayscale == True:\n    net, net_name = smp.Unet('resnet34', in_channels = 2, activation='sigmoid'), 'FC-EF'\n    #net, net_name = SiamUnet_diff(1,1), 'FC-Siam-diff'\n    #net, net_name = SiamUnet_conc(1,1), 'FC-Siam-conc'\n    #net, net_name = Unet(2,1), 'FC-EF'\n    \nelse:\n    net, net_name = smp.Unet('resnet34', in_channels = 6, activation='sigmoid'), 'FC-EF'\n    #net, net_name = SiamUnet_diff(3, 1), 'FC-Siam-diff'\n    #net, net_name = SiamUnet_conc(3,1), 'FC-Siam-conc'\n    #net, net_name = Unet(3*2,1), 'FC-EF'\nnet = net.to(device)","f8cef32c":"#defining datasets, samplers and dataloaders\ndatasets = {x:TorchDataset(root_folder = root_folder,df = df_dict[x],aug = None, preproc = None, grayscale = grayscale) for x in ['train','test','valid']}\n\nsamplers = {'train':BalancedSampler(datasets['train'], percentage = 1), \n            'test':BalancedSampler(datasets['test'], percentage = 1),\n            'valid':BalancedSampler(datasets['valid'], percentage = 1),\n            'sanity':BalancedSampler(datasets['train'], percentage = 1)}\n\ndataloaders = {x: DataLoader(dataset=datasets[x],\n                             batch_size=BATCH_SIZE,\n                             sampler=samplers[x],\n                             num_workers=16) for x in ['train','test','valid']}\n\ndataset_sizes = {x: len(datasets[x]) for x in ['train', 'test', 'valid']}\n\n","f7a92446":"# net = torch.load('..\/input\/satellite-models\/netFC-Siam-diff-best_epoch-1_iou_score-0.023448613236376363.tar')","ec5ed1f4":"def train(net, n_epochs = 25):\n    print('epoch,train_loss,train_iou,test_loss,test_iou',file=open('loss_log.txt', 'a'))\n    start_time = time()\n    # telegram_bot_sendtext(f'Training started')\n    scaler = GradScaler()\n        \n    iou = -1\n    best_iou = -1\n    \n    lss = 100000000\n    best_lss = 100000000\n    \n    train_epoch_iou = 0\n    test_epoch_iou = 0\n    \n    \n    #defining optimizer and scheduler\n    optimizer = torch.optim.Adam(net.parameters(), lr = 0.005)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n    \n\n    for epoch_index in tqdm(range(n_epochs)):\n        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(n_epochs))\n        train_running_loss = 0.0\n        train_running_iou = 0.0\n        test_running_loss = 0.0\n        test_running_iou = 0.0\n        \n        for phase in ['train','test']:\n            if phase == 'train':\n                net.train()  # Set model to training mode\n            else:\n                net.eval()   # Set model to evaluate mode\n\n            num_batches = len(dataloaders[phase])\n            for batch_index, batch in enumerate(tqdm(dataloaders[phase])):\n                torch.cuda.empty_cache()\n                I1 = Variable(batch['I1'].float().to(device))\n                I2 = Variable(batch['I2'].float().to(device))\n                labels = Variable(batch['label'].float().to(device))\n\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n                    with autocast():\n                        # outputs = net(I1, I2)\n                        outputs = net(torch.stack((I1, I2), axis = 1).squeeze()) # fusion for smp.Unet\n                        loss = criterion(outputs, labels)\n                    _, preds = torch.max(outputs.data, 1)\n                    del(_,outputs)\n                    if phase == 'train':\n                        scaler.scale(loss).backward()\n                        scaler.step(optimizer)\n                        optimizer.step()\n                        scaler.update()\n                    del(I1, I2)\n#                     except Exception as e:\n#                             msg = log_traceback(e)\n#                             telegram_bot_sendtext(f'failed at batch {batch_index}, with message: {msg}')\n#                             raise e\n#                             break\n#                     log_batch_statistics(batch_index,labels.data.long(),outputs.data.long(),phase,loss=loss,num_batches=num_batches,since=start_time)\n                if phase == 'train':\n                    train_running_loss += loss.item()\n                    # train_running_iou += iou_pytorch(preds.int().to('cpu'), labels.int().to('cpu')).item()\n                else:\n                    test_running_loss += loss.item()\n                    # test_running_iou += iou_pytorch(preds.int().to('cpu'), labels.int().to('cpu')).item()\n                                \n            if phase == 'train':\n                scheduler.step()\n                train_epoch_loss = train_running_loss \/ dataset_sizes[phase] * BATCH_SIZE\n                # train_epoch_iou = train_running_iou \/ dataset_sizes[phase] * BATCH_SIZE\n            else:\n                test_epoch_loss = test_running_loss \/ dataset_sizes[phase] * BATCH_SIZE\n                # test_epoch_iou = test_running_iou \/ dataset_sizes[phase] * BATCH_SIZE\n            print(f'{epoch_index},{train_epoch_loss}, {train_epoch_iou},{test_epoch_loss},{test_epoch_iou}', file=open('loss_log.txt', 'a'))\n#             print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=20), file=open(\"profiler.txt\", \"a\"))\n#             telegram_send_file('.\/profiler.txt')\n            \n#             if epoch_iou > best_iou:\n#                 best_iou = epoch_iou\n#                 save_str = f'.\/net{net_name}-best_epoch-' + str(epoch_index + 1) + '_iou_score-' + str(best_iou) + '.pth.tar'\n#                 torch.save(net, save_str)\n#                 telegram_send_file(save_str)\n#                 print('Model saved!')\n        \n            if (phase  == 'train') and (train_epoch_loss < best_lss):\n                best_lss = train_epoch_loss\n                save_str = f'.\/net{net_name}-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(best_lss) + '.pth.tar'\n                torch.save(net, save_str)\n                telegram_send_file(save_str)\n                print('Model saved!')\n            \n            if (phase == 'test') and (test_epoch_loss < best_lss):\n                best_lss = test_epoch_loss\n                save_str = f'.\/net{net_name}-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(best_lss) + '.pth.tar'\n                torch.save(net, save_str)\n                telegram_send_file(save_str)\n                print('Model saved!')\n        \n    \n    model_str = '.\/model.pth.tar'\n    torch.save(net, model_str)\n    telegram_send_file(model_str)\n    print('Model saved!')\n    time_elapsed = time() - start_time\n    print()\n    print(f'Training complete in {time_elapsed \/\/ 60:.0f}m {time_elapsed % 60:.0f}s')\n\n    print(f'Training complete in {time_elapsed \/\/ 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val change_accuracy: {best_fm:4f}')\n    \n    return net\n        ","ef2ffdd1":"model = train(net,2)  ","54f2cf24":"dataset_sizes['train']","d4ccc3c4":"dataset_sizes['valid']","84147c8e":"model_path = '..\/input\/satellite-models\/netFC-Siam-diff-best_epoch-1_iou_score-0.825063088258398.pth.tar'\nmodel = torch.load(model_path)","640a69c8":"model = model.cuda()","325eafaa":"model.eval()","958eea91":"def val_model(model):\n    iou_list = []\n    for batch_index, batch in enumerate(tqdm(dataloaders['valid'])):\n        I1 = Variable(batch['I1'].float().to(device))\n        I2 = Variable(batch['I2'].float().to(device))\n        labels = Variable(batch['label'].int().to(device))\n        outputs = model(I1,I2)\n        iou_list.append(iou_pytorch(outputs.int().to('cpu'), labels.to('cpu')))\n    print(np.mean(iou_list)) ","a67ef180":"val_model(model)","010a0616":"batch = next(iter(dataloaders['valid']))\nn = np.random.randint(BATCH_SIZE)\n_, preds = torch.max(model(batch['I1'].cuda(),batch['I2'].cuda()).data,1)","7b9c206a":"fig, axs = plt.subplots(1,4)\nfig.suptitle('Random chip prediction')\nfig.tight_layout() \naxs[0].imshow(batch['I1'][n].squeeze())\naxs[1].imshow(batch['I2'][n].squeeze())\naxs[2].imshow(batch['label'][n].squeeze())\naxs[3].imshow(preds[n].cpu())\n","3e48c106":"torch.stack((datasets['train'][0]['I1'],datasets['train'][0]['I1']),axis = 1).squeeze().shape","044f748b":"There are some missing values in file_type. With the elimination method and common sense, we understand that these are rasters.","d1077377":"global_monthly_YYYY_MM_mosaic_AOI_ID.tif\n\n\u0433\u0434\u0435:\n* YYYY - 4 digits number of the year\n* MM - 2 digits number of the month\n* AOI_ID - area of interest, location id (28 digits), upper register letters.\n\nLets make helpers to be able to extract these features from filenames as well.","d3bf36e0":"# Extracting meta-data from file names\nAs we mentioned above, filenames have similar structure","2fc8f270":"Now we can to get a closer look to the dataset.","3d042547":"# Train function","a718b740":"Now we need to combine untidy dataframe with all existing filepaths to the rasters and masks for different areas of interest. It will help us to form custom torch dataset for model training and validation. Lets recollect how the data is organized:\n\n* there is only images_masked dir inside the test_public\n* train and sample consist of: images, images_masked, labels, labels_match, labels_match_pix dirs\n* next part of the path is the same for all dataset parts\n* despite the paths as adresses we need to mention separately: parent directory as dataset part(train, test, sample), AOI ids, date of the shot, filename \n* there are two kinds of labels: with _Buildings and _UDM postfixes. _Buildings postfix is clear. UDM - unidentified mask and most probably contain glitches related to clouds, aerosols and so on","ad5b2388":"# Model setup","5a779603":"# Setup","f7a9d339":"\u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u043f\u0440\u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u0438 \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0435 \u0435\u0433\u043e \u0432 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u043a\u043b\u044e\u0447\u0438:\nparent, location_dir, sub_dir, filename, year, month, file_type, extension","63e3cb80":"Geodataframe includes polygons represented in WKT format (well-known-text), these are primitives to draw bulding masks. df columns are district, raster path, polygon geometry.\n\nLets inspect the rasters' size, number of channels, georeferncing, projection, affine transformations","6b38f5be":"Generating dataframes with paths metadata","7218d106":"# SpaceNet7 Change Detection\nProject task: train neural network for semantic segmentation of urban development change","c516c733":"I decided to relate on a following articale: Daudt, R.C., Le Saux, B. and Boulch, A., 2018, October. Fully convolutional siamese networks for change detection. In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.https:\/\/rcdaudt.github.io\/files\/2018icip-fully-convolutional.pdf .  \nIt desctibes neural networks architectures for CD specifically, which authors consider as most effective in cases of training from zero, without transfer learning or fine tuning of pretrained models.  \nThese are:\n* FC-EF (Fully Convolutional - Early Fusion)\n* FC_Siam_conc\n* FC_Siam_diff  \n\n\nThe first one involves feeding two of the concatenated rasters and their joint processing, the second and third involves separate treatment of the raster inputs at the beginning, during the convolutional stage, with identical branches with the shared weights and bias. Both outputs of individual branches are concatenated and fed to the input of a single network, ending with a layer of softmax or sigmoid transformation. Siam_conc and Siam_diff differ from each other in the mechanism of implementation of the feature skipping, in the first case it is more intuitive - intermediate features from the two branches of the conv stage are concatenated with the output at the respective upconv stages. In the case of Siam_diff, instead of two sets, the absolute difference between the intermediate features is concatenated. In the conclusion to the article it is proposed to use FC_Siam_diff as the most productive approach (although it is noted that all the above-mentioned methods are superior to the use of, for example, Transfer Learning) the next most productive one is Early Fusion.","64528ee0":"## Raster metadata","66f3222b":"only 17% contains change, we need at least 50%, but probably less or no blank masks.","a21a0ea9":"\u0421\u0432\u0435\u0434\u0435\u043c \u0432 \u043a\u043b\u0430\u0441\u0441 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0435\u0449\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0435 \u0432\u044b\u0448\u0435 (\u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0447\u0438\u043f\u0441\u043e\u0432, \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043d\u0438\u0446\u044b \u0432 \u043c\u0430\u0441\u043a\u0430\u0445)","0b97929a":"60 unique locations, mentioned as im_name in this df. Lets divide into train\/test parts by whole locations to avoid leakage.\n40\/10\/10","c2f523ce":"Now we need to figure out connections between the rasters and masks, udm absence or presence.","adb28e1a":"Now we now which buildings are appeared and which are demolished. There are much fewer demolished buildings, and for the sake of simplicity I will neglect them.","d8b47f66":"We see some difference with naked eye already. Lets make helper and try to see if there is any need to track which of the buildings are demolished through time.","516e452e":"# Import","e769e182":"## Train\/Test\/Valid split","899f027b":"Dividing the dataset into chips, according to tqdm's calculations, takes 194 hours. Anyway, after I have learned and reproduced the preparation process, for speed purposes I will take the dataset which is already split into chips the same way. Fortunately it exists and is open on the kaggle.","525ae420":"Now we need to make chipmaker to inflate dataset and be able to train deeper model.","85043900":"## Metrics Evaluation","dd1bc499":"# Helpers\n\nRoughly speaking, we need to be able to:\n* plot poligons (separatly and above rasters)\n* make rasters out of vector polygons\n* subtract masks to get change masks\n* make smaller chips out of original pictures","7137a481":"# Deep Learning preparation\nNow we operate the change detection dataset we prepared above.\nIt is necessary to:\n* redefine paths\n* divide dataset into train\/test\/valid parts and avoid leakage\n* augment the data without distortion. rotation, reflection, padding, normalisation, convertation to tensor format.\n* make torch dataset with __len__ and __getitem__ attributes, to be able to lazyload and not to store nothing.\n* consider unchanged chips and may be avoid them, make balancing sampler.\n* choose model architecture, learning rate policy, loss and metrics. considering specificity of the task -  Jaccard loss and Intersection over Union (IoU) is most relevant.\n* maintain logging and messaging to tg bot.\n\n\n","def39782":"0 - background, 255 - mask(building) needs to be normalised (divided by 255)\nLets see the very first and the very last sample masks (2 years interval)","c4cf8f81":"## Augmentations","97161b78":"# Sandbox for debuging","05edd283":"# Torch Custom Dataset","dd509e48":"# Metrics evaluation and logging\n## Tg-bot setup","1d1c8bfc":"# Metadata Extraction\n\n## Paths metadata\nFirst of all lets collect all existing paths and parse their elements to extract any metadata and significant features.\n\nTypical path looks like this:\n..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train\/train\/L15-1200E-0847N_4802_4803_13\/UDM_masks\/global_monthly_2018_05_mosaic_L15-1200E-0847N_4802_4803_13_UDM.tif\n\nand can be divided into following parts:\n* parent directory: ..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train\/train\/, which can be reduced as just train, test or sample\n* location directory with unique id: L15-1200E-0847N_4802_4803_13 which always has 28 digits\n* directory with name that indicates file content: UDM_masks, images, images_masked, labels, labels_match, labels_match_pix\n* filename which includes year, month, location id and file content \n* file extension (.tif, .csv and so on)","71201100":"### Change detection FC model","ebd72021":"* Raster size: 1024 x 1024 px, 3-4 channels: RGB\/alpha-RGB\n* Projection: WGS 84 \/ Pseudo-Mercator (EPSG:3857)\n* Affine: affine transformation matrix, 6 numbers: a,b,c,d,e,f which make this matrix\n\n| x' |   | a b c | | x |  \n| y' | = | d e f | | y |  \n| 1  |   | 0 0 1 | | 1 |  \n\n x,y - pixel coordinates, x',y' - world coordinates","543118c7":"# Results visualization","bb79eef8":"# Model Setup","77c0566f":"Lets check how it works."}}