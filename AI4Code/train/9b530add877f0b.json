{"cell_type":{"b33df025":"code","71a7d15b":"code","dbc06de4":"code","19228052":"code","6327e3c3":"code","7af37dc4":"code","790443da":"code","a8fd76ce":"code","5bd19844":"code","457ff2c0":"code","4de2c1ed":"code","e2de5e29":"code","0744d21d":"code","42f46154":"code","af1ff998":"code","fe2f0745":"code","2b773137":"code","42a3397b":"code","62d657dd":"code","22c12def":"code","4bee8159":"code","70949db7":"markdown","7f26907e":"markdown","cf4020e7":"markdown","0d38a911":"markdown"},"source":{"b33df025":"!git clone https:\/\/github.com\/Edresson\/VoiceSplit","71a7d15b":"!apt install ffmpeg","dbc06de4":"!pip install -U tqdm numpy librosa mir_eval matplotlib Pillow  tensorboardX pandas torchaudio PyYAML pysoundfile ffmpeg-normalize","19228052":"# Check gpu \nimport torch\ntorch.cuda.current_device()\ntorch.cuda.device_count()\ntorch.cuda.is_available()","6327e3c3":"# clone GE2E Embedder\n!git clone https:\/\/github.com\/Edresson\/GE2E-Speaker-Encoder.git\n#Install Requeriments\n!python -m pip install umap-learn visdom webrtcvad librosa>=0.5.1 matplotlib>=2.0.2 numpy>=1.14.0  scipy>=1.0.0  tqdm sounddevice Unidecode inflect multiprocess numba\n\n\n#Download encoder Checkpoint\n!wget https:\/\/github.com\/Edresson\/Real-Time-Voice-Cloning\/releases\/download\/checkpoints\/pretrained.zip\n!unzip pretrained.zip","7af37dc4":"import sys \nsys.path.insert(0, \".\/VoiceSplit\/\")\nsys.path.insert(0, \".\/GE2E-Speaker-Encoder\/\")","790443da":"# Imports from GE2E\nfrom encoder import inference as encoder\nfrom encoder.params_model import model_embedding_size as speaker_embedding_size\nfrom pathlib import Path\n\n\n# Imports from VoiceSplit model\nfrom utils.audio_processor import WrapperAudioProcessor as AudioProcessor \nfrom utils.generic_utils import load_config\nimport librosa\nimport os\nimport numpy as np\nimport torch\nfrom glob import glob\nfrom tqdm import tqdm\nimport torch\n\nfrom models.voicefilter.model import VoiceFilter\nfrom models.voicesplit.model import VoiceSplit\n\nfrom utils.generic_utils import load_config, load_config_from_str","a8fd76ce":"#Load and test GE2E model\nprint(\"Preparing the encoder, the synthesizer and the vocoder...\")\nencoder.load_model(Path('encoder\/saved_models\/pretrained.pt'))\nprint(\"Testing your configuration with small inputs.\")\n# Forward an audio waveform of zeroes that lasts 1 second. Notice how we can get the encoder's\n# sampling rate, which may differ.\n# If you're unfamiliar with digital audio, know that it is encoded as an array of floats \n# (or sometimes integers, but mostly floats in this projects) ranging from -1 to 1.\n# The sampling rate is the number of values (samples) recorded per second, it is set to\n# 16000 for the encoder. Creating an array of length <sampling_rate> will always correspond \n# to an audio of 1 second.\nprint(\"\\tTesting the encoder...\")\n\nwav = np.zeros(encoder.sampling_rate)    \nembed = encoder.embed_utterance(wav)\nprint(embed.shape)","5bd19844":"def get_embedding(encoder, ap, wave_file_path):\n  preprocessed_wav = encoder.preprocess_wav(wave_file_path)\n  file_embedding = encoder.embed_utterance(preprocessed_wav)\n  return torch.from_numpy(file_embedding.reshape(-1))","457ff2c0":"# Download VoiceSplit checkpoint\n!wget https:\/\/github.com\/Edresson\/VoiceSplit\/releases\/download\/checkpoints\/voiceSplit-trained-with-Si-SRN-GE2E-CorintinJ-best_checkpoint.pt -O best_checkpoint.pt","4de2c1ed":"# Paths\ncheckpoint_path = 'best_checkpoint.pt'\n# load checkpoint \ncheckpoint = torch.load(checkpoint_path, map_location='cpu')\n#load config from checkpoint\nc = load_config_from_str(checkpoint['config_str'])","e2de5e29":"ap = AudioProcessor(c.audio) # create AudioProcessor for model\nmodel_name = c.model_name\ncuda = True","0744d21d":"# load model\nif(model_name == 'voicefilter'):\n    print('inicializado com voicefilter')\n    model = VoiceFilter(c)\nelif(model_name == 'voicesplit'):\n    model = VoiceSplit(c)\nelse:\n    raise Exception(\" The model '\"+model_name+\"' is not suported\")\n\nif c.train_config['optimizer'] == 'adam':\n    optimizer = torch.optim.Adam(model.parameters(),\n                                  lr=c.train_config['learning_rate'])\nelse:\n    raise Exception(\"The %s  not is a optimizer supported\" % c.train['optimizer'])\n\n      \nmodel.load_state_dict(checkpoint['model'])\n\n\noptimizer.load_state_dict(checkpoint['optimizer'])\nstep = checkpoint['step']\n\nprint(\"load model form Step:\", step)\n# convert model from cuda\nif cuda:\n    model = model.cuda()","42f46154":"# utils for plot spectrogram\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pylab as plt\nimport numpy as np\nimport imageio\ndef fig2np(fig):\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    return data\n\ndef plot_spectrogram_to_numpy(spectrogram):\n    fig, ax = plt.subplots(figsize=(12, 3))\n    im = ax.imshow(spectrogram, aspect='auto', origin='lower',\n                   interpolation='none')\n    plt.colorbar(im, ax=ax)\n    plt.xlabel('Frames')\n    plt.ylabel('Channels')\n    plt.tight_layout()\n\n    fig.canvas.draw()\n    data = fig2np(fig)\n    plt.close()\n    return data\n\ndef save_spec(path, spec):\n  data = plot_spectrogram_to_numpy(spec)\n  imageio.imwrite(path, data)","af1ff998":"# utils for calculate SNR and SDR\n# this code is adpated from https:\/\/github.com\/JusperLee\/Calculate-SNR-SDR\/\nimport torch\nfrom mir_eval.separation import bss_eval_sources\nfrom itertools import permutations\nimport soundfile as sf\n\ndef SI_SNR(_s, s, mix, zero_mean=True):\n    '''\n         Calculate the SNR indicator between the two audios. \n         The larger the value, the better the separation.\n         input:\n               _s: Generated audio\n               s:  Ground Truth audio\n         output:\n               SNR value \n    '''\n    length = _s.shape[0]\n    _s = _s[:length]\n    s =s[:length]\n    mix = mix[:length]\n    if zero_mean:\n        _s = _s - torch.mean(_s)\n        s = s - torch.mean(s)\n        mix = mix - torch.mean(mix)\n    s_target = sum(torch.mul(_s, s))*s\/(torch.pow(torch.norm(s, p=2), 2)+1e-8)\n    e_noise = _s - s_target\n    # mix ---------------------------\n    mix_target = sum(torch.mul(mix, s))*s\/(torch.pow(torch.norm(s, p=2), 2)+1e-8)\n    mix_noise = mix - mix_target \n    return 20*torch.log10(torch.norm(s_target, p=2)\/(torch.norm(e_noise, p=2)+1e-8)) - 20*torch.log10(torch.norm(mix_target, p=2)\/(torch.norm(mix_noise, p=2)+1e-8))\n\n\ndef permute_SI_SNR(_s_lists, s_lists, mix):\n    '''\n        Calculate all possible SNRs according to \n        the permutation combination and \n        then find the maximum value.\n        input:\n               _s_lists: Generated audio list\n               s_lists: Ground truth audio list\n        output:\n               max of SI-SNR\n    '''\n    length = len(_s_lists)\n    results = []\n    per = []\n    for p in permutations(range(length)):\n        s_list = [s_lists[n] for n in p]\n        result = sum([SI_SNR(_s, s, mix, zero_mean=True) for _s, s in zip(_s_lists, s_list)])\/length\n        results.append(result)\n        per.append(p)\n    return max(results), per[results.index(max(results))]\n\n\ndef SDR(est, egs, mix):\n    '''\n        calculate SDR\n        est: Network generated audio\n        egs: Ground Truth\n    '''\n    length = est.numpy().shape[0]\n    sdr, _, _, _ = bss_eval_sources(egs.numpy()[:length], est.numpy()[:length])\n    mix_sdr, _, _, _ = bss_eval_sources(egs.numpy()[:length], mix.numpy()[:length])\n    return float(sdr-mix_sdr)\n\n\ndef permutation_sdr(est_list, egs_list, mix, per):\n    n = len(est_list)\n    result = sum([SDR(est_list[a], egs_list[b], mix)\n                      for a, b in enumerate(per)])\/n\n    return result","fe2f0745":"# extract caracteristics\ndef normalise_and_extract_features(encoder, ap, mixed_path, target_path, target_path2, emb_ref_path):\n  mixed_path_norm = mixed_path.replace('.wav','-norm.wav') \n  target_path_norm = target_path.replace('.wav','-norm.wav')\n  target_path_norm2 = target_path2.replace('.wav','-norm.wav')\n  emb_ref_path_norm = emb_ref_path.replace('.wav','-norm.wav')\n  \n  # normalise wavs\n  ! ffmpeg-normalize $mixed_path -ar 16000 -o $mixed_path_norm -f\n  ! ffmpeg-normalize  $target_path -ar 16000 -o $target_path_norm -f \n  ! ffmpeg-normalize  $target_path2 -ar 16000 -o $target_path_norm2 -f \n  ! ffmpeg-normalize  $emb_ref_path -ar 16000 -o $emb_ref_path_norm -f\n\n  # load wavs\n  target_wav = ap.load_wav(target_path_norm)\n  target_wav2 = ap.load_wav(target_path_norm2)\n  mixed_wav = ap.load_wav(mixed_path_norm)\n  emb_wav = ap.load_wav(emb_ref_path_norm)\n  \n  # trim initial and end  wave file silence using librosa\n  # target_wav, _ = librosa.effects.trim(target_wav, top_db=20)\n  # mixed_wav, _ = librosa.effects.trim(mixed_wav, top_db=20)\n  # emb_wav, _ = librosa.effects.trim(emb_wav, top_db=20)\n\n  # normalise wavs\n  norm_factor = np.max(np.abs(mixed_wav)) * 1.1\n  mixed_wav = mixed_wav\/norm_factor\n  emb_wav = emb_wav\/norm_factor\n  target_wav = target_wav\/norm_factor\n  target_wav2 = target_wav2\/norm_factor\n\n  # save embedding ref \n  #librosa.output.write_wav(emb_ref_path_norm, emb_wav, 16000)\n  sf.write(emb_ref_path_norm, emb_wav, 16000, 'PCM_24')  \n  # save this is necessary for demo\n  #librosa.output.write_wav(mixed_path_norm, mixed_wav, 16000)\n  #librosa.output.write_wav(target_path_norm, target_wav, 16000)\n  #librosa.output.write_wav(target_path_norm2, target_wav2, 16000)\n  sf.write(mixed_path_norm, mixed_wav, 16000, 'PCM_24')\n  sf.write(target_path_norm, target_wav, 16000, 'PCM_24')\n  sf.write(target_path_norm2, target_wav2, 16000, 'PCM_24')\n    \n  embedding = get_embedding(encoder, ap, emb_ref_path_norm)\n  mixed_spec, mixed_phase = ap.get_spec_from_audio(mixed_wav, return_phase=True)\n  return embedding, mixed_spec, mixed_phase, target_wav, target_wav2, mixed_wav, emb_wav","2b773137":"def predict(encoder, ap, mixed_path, target_path, target_path2, emb_ref_path, outpath='predict.wav', save_img=False):\n  embedding, mixed_spec, mixed_phase, target_wav, target_wav2, mixed_wav, emb_wav = normalise_and_extract_features(encoder, ap, mixed_path, target_path, target_path2,  emb_ref_path)\n  # use the model\n  mixed_spec = torch.from_numpy(mixed_spec).float()\n\n  # append 1 dimension on mixed, its need because the model spected batch\n  mixed_spec = mixed_spec.unsqueeze(0)\n  embedding = embedding.unsqueeze(0)\n\n  if cuda:\n    embedding = embedding.cuda()\n    mixed_spec = mixed_spec.cuda()\n\n  mask = model(mixed_spec, embedding)\n  output = mixed_spec * mask\n\n  # inverse spectogram to wav\n  est_mag = output[0].cpu().detach().numpy()\n  mixed_spec = mixed_spec[0].cpu().detach().numpy()\n  # use phase from mixed wav for reconstruct the wave\n  est_wav = ap.inv_spectrogram(est_mag, phase=mixed_phase)\n\n\n  #librosa.output.write_wav(outpath, est_wav, 16000)\n  sf.write(outpath, est_wav, 16000, 'PCM_24')\n  if save_img:\n      img_path = outpath.replace('predict', 'images').replace(' ', '').replace('.wav','-est.png')\n      save_spec(img_path, est_mag)\n      target_mag = ap.get_spec_from_audio(target_wav, return_phase=False)\n      img_path = outpath.replace('predict', 'images').replace(' ', '').replace('.wav','-target.png')\n      save_spec(img_path, target_mag)\n      img_path = outpath.replace('predict', 'images').replace(' ', '').replace('.wav','-mixed.png')\n      save_spec(img_path, mixed_spec)\n      \n\n  return est_wav, target_wav, target_wav2, mixed_wav, emb_wav","42a3397b":"import pandas as pd\nfrom IPython.display import Audio, display\nfrom mir_eval.separation import bss_eval_sources\nimport numpy as np\n\n# create output path\nos.makedirs('VoiceSplit\/datasets\/LibriSpeech\/audios_demo\/2_speakers\/predict\/',exist_ok=True)\nos.makedirs('VoiceSplit\/datasets\/LibriSpeech\/audios_demo\/2_speakers\/images\/',exist_ok=True)\n\ntest_csv = pd.read_csv('VoiceSplit\/datasets\/LibriSpeech\/test_demo.csv', sep=',').values\n\nsdrs_before = []\nsdrs_after = []\nsnrs_before = []\nsnrs_after = []\nfor noise_utterance,emb_utterance, clean_utterance, clean_utterance2 in test_csv:\n  noise_utterance = os.path.join('VoiceSplit',noise_utterance).replace(' ', '')\n  emb_utterance = os.path.join('VoiceSplit',emb_utterance).replace(' ', '')\n  clean_utterance = os.path.join('VoiceSplit',clean_utterance).replace(' ', '')\n  clean_utterance2 = os.path.join('VoiceSplit',clean_utterance2).replace(' ', '')\n  output_path = noise_utterance.replace('noisy', 'predict').replace(' ', '')\n  est_wav, target_wav, target_wav2, mixed_wav, emb_wav = predict(encoder, ap, noise_utterance, clean_utterance, clean_utterance2, emb_utterance, outpath=output_path, save_img=True)\n\n  len_est = len(est_wav)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest \n    est_wav = est_wav[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    est_wav = np.pad(est_wav, (0, len(mixed_wav)-len(est_wav)), 'constant', constant_values=(0, 0))\n\n  # get wav for second voice, its need for SDR calculation\n  est_wav2 = mixed_wav-est_wav\n\n  len_est = len(est_wav2)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest\n    est_wav2 = est_wav2[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    est_wav2 = np.pad(est_wav2, (0, len(mixed_wav)-len(est_wav2)), 'constant', constant_values=(0, 0))\n\n  len_est = len(target_wav)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest\n    target_wav = target_wav[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    target_wav = np.pad(target_wav, (0, len(mixed_wav)-len(target_wav)), 'constant', constant_values=(0, 0))\n\n  # get target_wav for second voice, its recomended because google dont provide clean_utterance2 in your demo i need get in LibreSpeech Dataset, but i dont know if they normalised this file..\n  target_wav2 = mixed_wav - target_wav\n  '''len_est = len(target_wav2)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest\n    target_wav2 = target_wav2[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    target_wav2 = np.pad(target_wav2, (0, len(mixed_wav)-len(target_wav2)), 'constant', constant_values=(0, 0))'''\n\n  # calculate snr and sdr before model\n  ests = [torch.from_numpy(mixed_wav), torch.from_numpy(mixed_wav)] # the same voices is mixed_wav\n  egs = [torch.from_numpy(target_wav), torch.from_numpy(target_wav2)]\n  mix = torch.from_numpy(mixed_wav)\n  _snr, per = permute_SI_SNR(ests, egs, mix)\n  _sdr = permutation_sdr(ests, egs, mix, per)\n  snrs_before.append(_snr)\n  sdrs_before.append(_sdr)\n\n  # calculate snr and sdr after model\n  ests = [torch.from_numpy(est_wav), torch.from_numpy(est_wav2)]\n  egs = [torch.from_numpy(target_wav), torch.from_numpy(target_wav2)]\n  mix = torch.from_numpy(mixed_wav)\n  _snr, per = permute_SI_SNR(ests, egs, mix)\n  _sdr = permutation_sdr(ests, egs, mix, per)\n  snrs_after.append(_snr)\n  sdrs_after.append(_sdr)\n\n  # show in notebook results\n  print('-'*100)\n  print('-'*30,os.path.basename(noise_utterance),'-'*30)\n  print(\"Input\/Noise Audio\")\n  display(Audio(mixed_wav,rate=16000))\n  print('Predicted Audio')\n  display(Audio(est_wav,rate=16000))\n  print('Target Audio')\n  display(Audio(target_wav,rate=16000))\n  print('Predicted2 Audio')\n  display(Audio(est_wav2,rate=16000))\n  print('Target2 Audio')\n  display(Audio(target_wav2,rate=16000))\n  print('-'*100)\n  del target_wav, est_wav, mixed_wav\n\n\nprint('='*20,\"Before Model\",'='*20)\nprint('\\nAverage SNRi: {:.5f}'.format(np.array(snrs_before).mean()))\nprint('Average SDRi: {:.5f}'.format(np.array(sdrs_before).mean()))\n\nprint('='*20,\"After Model\",'='*20)\nprint('\\nAverage SNRi: {:.5f}'.format(np.array(snrs_after).mean()))\nprint('Average SDRi: {:.5f}'.format(np.array(sdrs_after).mean()))","62d657dd":"!zip -r audios_demo_single_best_model.zip VoiceSplit\/datasets\/LibriSpeech\/audios_demo\/","22c12def":"# Apply VoiceFilter on clean audio (single speaker)\nimport pandas as pd\nfrom IPython.display import Audio, display\nfrom mir_eval.separation import bss_eval_sources\nimport numpy as np\n# create output path\nos.makedirs('VoiceSplit\/datasets\/LibriSpeech\/audios_demo\/single_speaker\/predict\/',exist_ok=True)\nos.makedirs('VoiceSplit\/datasets\/LibriSpeech\/audios_demo\/single_speaker\/images\/',exist_ok=True)\ntest_csv = pd.read_csv('VoiceSplit\/datasets\/LibriSpeech\/test_demo.csv', sep=',').values\n\nsdrs_before = []\nsdrs_after = []\nsnrs_before = []\nsnrs_after = []\nfor _ ,emb_utterance, clean_utterance, clean_utterance2 in test_csv:\n  emb_utterance = os.path.join('VoiceSplit',emb_utterance).replace(' ', '')\n  clean_utterance = os.path.join('VoiceSplit',clean_utterance).replace(' ', '')\n  clean_utterance2 = os.path.join('VoiceSplit',clean_utterance2).replace(' ', '')\n  output_path = clean_utterance.replace('\/clean\/', '\/single_speaker\/predict\/').replace(' ', '')\n\n  #  input = clean uterrance\n  est_wav, target_wav, target_wav2, mixed_wav, emb_wav = predict(encoder, ap, clean_utterance, clean_utterance, clean_utterance2, emb_utterance, outpath=output_path, save_img=True)\n\n  len_est = len(est_wav)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest \n    est_wav = est_wav[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    est_wav = np.pad(est_wav, (0, len(mixed_wav)-len(est_wav)), 'constant', constant_values=(0, 0))\n\n  # get wav for second voice, its need for SDR calculation\n  est_wav2 = mixed_wav-est_wav\n\n  len_est = len(est_wav2)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest\n    est_wav2 = est_wav2[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    est_wav2 = np.pad(est_wav2, (0, len(mixed_wav)-len(est_wav2)), 'constant', constant_values=(0, 0))\n\n  len_est = len(target_wav)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest\n    target_wav = target_wav[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    target_wav = np.pad(target_wav, (0, len(mixed_wav)-len(target_wav)), 'constant', constant_values=(0, 0))\n\n  # show in notebook results\n  print('-'*100)\n  print('-'*30,os.path.basename(noise_utterance),'-'*30)\n  print(\"Input\/Clean Audio\")\n  display(Audio(mixed_wav,rate=16000))\n  print('Predicted Audio')\n  display(Audio(est_wav,rate=16000))\n  print('-'*100)\n  del target_wav, est_wav, mixed_wav","4bee8159":"# in google paper dont is reported SNRi, and not is clean for my when we calculate SNR, for this reason i calculate this\n# NOTE: its use other speaker encoder, and other normalization on wavs, for this reason its not directly comparable.\nimport pandas as pd\nfrom IPython.display import Audio, display\nfrom mir_eval.separation import bss_eval_sources\nimport numpy as np\n# SDR from google paper for this instances\ntest_csv = pd.read_csv('VoiceSplit\/datasets\/LibriSpeech\/test_demo.csv', sep=',').values\nsdrs_before = []\nsdrs_after = []\nsnrs_after = []\nsnrs_before = []\nfor noise_utterance, emb_utterance, clean_utterance, clean_utterance2  in test_csv:\n  noise_utterance = os.path.join('VoiceSplit',noise_utterance).replace(' ', '')\n  emb_utterance = os.path.join('VoiceSplit',emb_utterance).replace(' ', '')\n  clean_utterance = os.path.join('VoiceSplit',clean_utterance).replace(' ', '')\n  clean_utterance2 = os.path.join('VoiceSplit',clean_utterance2).replace(' ', '')\n  est_utterance = noise_utterance.replace('noisy', 'enhanced').replace(' ', '')\n\n  target_wav, _ = librosa.load(clean_utterance, sr=16000)\n  target_wav2, _ = librosa.load(clean_utterance2, sr=16000)\n  est_wav, _ = librosa.load(est_utterance, sr=16000)\n  mixed_wav, _ = librosa.load(noise_utterance, sr=16000)\n\n  len_est = len(est_wav)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest\n    est_wav = est_wav[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    est_wav = np.pad(est_wav, (0, len(mixed_wav)-len(est_wav)), 'constant', constant_values=(0, 0))\n\n  # get wav for second voice, its need for SDR calculation\n  est_wav2 = mixed_wav-est_wav\n\n  len_est = len(est_wav2)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest\n    est_wav2 = est_wav2[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    est_wav2 = np.pad(est_wav2, (0, len(mixed_wav)-len(est_wav2)), 'constant', constant_values=(0, 0))\n\n  len_est = len(target_wav)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest\n    target_wav = target_wav[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    target_wav = np.pad(target_wav, (0, len(mixed_wav)-len(target_wav)), 'constant', constant_values=(0, 0))\n\n  # get target_wav for second voice, its recomended because google dont provide clean_utterance2 in your demo i need get in LibreSpeech Dataset, but i dont know if they normalised this file..\n  target_wav2 = mixed_wav - target_wav\n  '''len_est = len(target_wav2)\n  len_mixed = len(mixed_wav)\n  if len_est > len_mixed:\n    # mixed need is biggest\n    target_wav2 = target_wav2[:len_mixed]\n  else:\n    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n    target_wav2 = np.pad(target_wav2, (0, len(mixed_wav)-len(target_wav2)), 'constant', constant_values=(0, 0))'''\n\n\n  # calculate snr and sdr before model\n  ests = [torch.from_numpy(mixed_wav), torch.from_numpy(mixed_wav)] # the same voices is mixed_wav\n  egs = [torch.from_numpy(target_wav), torch.from_numpy(target_wav2)]\n  mix = torch.from_numpy(mixed_wav)\n  _snr, per = permute_SI_SNR(ests, egs, mix)\n  _sdr = permutation_sdr(ests, egs, mix, per)\n  snrs_before.append(_snr)\n  sdrs_before.append(_sdr)\n\n  # calculate snr and sdr after model\n  ests = [torch.from_numpy(est_wav), torch.from_numpy(est_wav2)]\n  egs = [torch.from_numpy(target_wav), torch.from_numpy(target_wav2)]\n  mix = torch.from_numpy(mixed_wav)\n  _snr, per = permute_SI_SNR(ests, egs, mix)\n  _sdr = permutation_sdr(ests, egs, mix, per)\n  snrs_after.append(_snr)\n  sdrs_after.append(_sdr)\n\n  # show in notebook results\n  print('-'*100)\n  print('-'*30,os.path.basename(noise_utterance),'-'*30)\n  print(\"Input\/Noise Audio\")\n  display(Audio(mixed_wav,rate=16000))\n  print('Predicted Audio')\n  display(Audio(est_wav,rate=16000))\n  print('Target Audio')\n  display(Audio(target_wav,rate=16000))\n  print('Predicted2 Audio')\n  display(Audio(est_wav2,rate=16000))\n  print('Target2 Audio')\n  display(Audio(target_wav2,rate=16000))\n  print('-'*100)\n  del target_wav, est_wav, mixed_wav\n\n\nprint('='*20,\"Before Model\",'='*20)\nprint('\\nAverage SNRi: {:.5f}'.format(np.array(snrs_before).mean()))\nprint('Average SDRi: {:.5f}'.format(np.array(sdrs_before).mean()))\n\nprint('='*20,\"After Model\",'='*20)\nprint('\\nAverage SNRi: {:.5f}'.format(np.array(snrs_after).mean()))\nprint('Average SDRi: {:.5f}'.format(np.array(sdrs_after).mean()))","70949db7":"## Paper: [VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking](https:\/\/arxiv.org\/abs\/1810.04826)","7f26907e":"## Architecture:\n![image.png](attachment:3d9b211b-5a7d-4d8b-9495-8e9b40fda667.png)","cf4020e7":"## Repro [Github](https:\/\/github.com\/Edresson\/VoiceSplit)","0d38a911":"# VoiceSplit"}}