{"cell_type":{"4bb878af":"code","db08c8c0":"code","a242e967":"code","488b1cde":"code","f536ed94":"code","bc129b73":"code","0f39a3da":"code","1d57f1ea":"code","d8231e7c":"code","ba90e66b":"code","3d83c483":"code","de178d98":"code","2e48800e":"code","23e70c23":"code","2974c927":"code","08e94383":"code","cb9d0e25":"code","4c75d84a":"code","ea8b50f5":"code","bf79ac6d":"code","40f80abc":"code","89778e6e":"code","fc77bee0":"code","40ebc77e":"code","7a8c9738":"code","48499e39":"code","897f3f48":"code","714c573f":"code","1f366170":"code","6045ccc0":"code","f4108151":"code","69c2e594":"code","9071edde":"code","066a8444":"code","7b4478dc":"markdown","7c1cb75b":"markdown","ce673286":"markdown","13530128":"markdown","c1cd7f21":"markdown","08084c6b":"markdown","e96df465":"markdown","dbf5f4b8":"markdown","935961e7":"markdown","7a4eb4f2":"markdown","888bd878":"markdown","3753e51a":"markdown","5a0192f3":"markdown","63129637":"markdown","a44305df":"markdown","24a3666f":"markdown","fbf6ca56":"markdown","87cef4be":"markdown","2c859124":"markdown","97952ab1":"markdown","4e0680d0":"markdown","1548b756":"markdown","682b0c6b":"markdown","d247cab7":"markdown","c007db35":"markdown","46076bb3":"markdown","899ea3ff":"markdown","33e0602e":"markdown","d9493ea5":"markdown","ae8afd84":"markdown","98aefe16":"markdown","29159c45":"markdown","bbe1aba5":"markdown","46d03592":"markdown","87357879":"markdown","ae12cc5c":"markdown","8a212d3e":"markdown","cc7252e4":"markdown","68a5b2d1":"markdown","036255f9":"markdown","921cc126":"markdown","4f486b0d":"markdown","29dbd4ef":"markdown","b391f724":"markdown","78a2b428":"markdown","cdd47ccb":"markdown","a81e65e3":"markdown","ad3d44c6":"markdown","36492095":"markdown","2efa260c":"markdown","a7c6d197":"markdown","0ceacdf3":"markdown","0a278070":"markdown","fe18bb3d":"markdown","55264d03":"markdown","2b0b24c8":"markdown","77cb6411":"markdown","8c821afe":"markdown","b15e49de":"markdown","d61d717e":"markdown","0f04615a":"markdown"},"source":{"4bb878af":"# import libraries for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# import libraries for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.gofplots import ProbPlot\n\n# import libraries for building linear regression model\nfrom statsmodels.formula.api import ols\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\n\n# import library for preparing data\nfrom sklearn.model_selection import train_test_split\n\n# import library for data preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","db08c8c0":"df = pd.read_csv(\"..\/input\/bostonhouseprices\/Boston.csv\")\ndf.head()","a242e967":"df.info()","488b1cde":"#write your code here\ndf.describe().T","f536ed94":"# let's plot all the columns to look at their distributions\nfor i in df.columns:\n    plt.figure(figsize=(7, 4))\n    sns.histplot(data=df, x=i, kde = True)\n    plt.show()","bc129b73":"df['MEDV_log'] = np.log(df['MEDV'])","0f39a3da":"sns.histplot(data=df, x='MEDV_log', kde = True)","1d57f1ea":"plt.figure(figsize=(12,8))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(df.corr(),annot=True,fmt='.2f',cmap=cmap ) #write your code here\nplt.show()","d8231e7c":"# scatterplot to visualize the relationship between NOX and INDUS\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x='NOX', y='INDUS', data=df); #write you code here\nplt.show()","ba90e66b":"# scatterplot to visualize the relationship between AGE and NOX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x='AGE', y='NOX', data=df); #Write your code here\nplt.show()","3d83c483":"# scatterplot to visualize the relationship between DIS and NOX.\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x='DIS', y='NOX', data=df); #Write your code here\nplt.show()","de178d98":"# scatterplot to visualize the relationship between AGE and DIS\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'AGE', y = 'DIS', data = df)\nplt.show()","2e48800e":"# scatterplot to visualize the relationship between AGE and INDUS\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'AGE', y = 'INDUS', data = df)\nplt.show()","23e70c23":"# scatterplot to visulaize the relationship between RAD and TAX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'RAD', y = 'TAX', data = df)\nplt.show()","2974c927":"# remove the data corresponding to high tax rate\ndf1 = df[df['TAX'] < 600]\n# import the required function\nfrom scipy.stats import pearsonr\n# calculate the correlation\nprint('The correlation between TAX and RAD is', pearsonr(df1['TAX'], df1['RAD'])[0])","08e94383":"# scatterplot to visualize the relationship between INDUS and TAX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'INDUS', y = 'TAX', data = df)\nplt.show()","cb9d0e25":"# scatterplot to visulaize the relationship between RM and MEDV\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'RM', y = 'MEDV', data = df)\nplt.show()","4c75d84a":"# scatterplot to visulaize the relationship between LSTAT and MEDV\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'LSTAT', y = 'MEDV', data = df)\nplt.show()","ea8b50f5":"# separate the dependent and indepedent variable\nY = df['MEDV_log']\nX = df.drop(columns = {'MEDV', 'MEDV_log'})\n\n# add the intercept term\nX = sm.add_constant(X)","bf79ac6d":"# splitting the data in 70:30 ratio of train to test data\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30 , random_state=1)","40f80abc":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# function to check VIF\ndef checking_vif(train):\n    vif = pd.DataFrame()\n    vif[\"feature\"] = train.columns\n\n    # calculating VIF for each feature\n    vif[\"VIF\"] = [\n        variance_inflation_factor(train.values, i) for i in range(len(train.columns))\n    ]\n    return vif\n\n\nprint(checking_vif(X_train))","89778e6e":"# create the model after dropping TAX\nX_train = X_train.drop(columns={'TAX'})#Write your code here\n\n# check for VIF\nprint(checking_vif(X_train))","fc77bee0":"# create the model\nmodel1 = sm.OLS(y_train, X_train).fit()#write your code here\n\n# get the model summary\nmodel1.summary()","40ebc77e":"# create the model after dropping TAX\nY = df['MEDV_log']\nX = X.drop(columns = {'ZN', 'AGE', 'INDUS', 'TAX'}) #write your code here\nX = sm.add_constant(X)\n\n#splitting the data in 70:30 ratio of train to test data\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30 , random_state=1)\n\n# create the model\nmodel2 = sm.OLS(y_train, X_train).fit() #write your code here\n# get the model summary\nmodel2.summary()","7a8c9738":"residuals = model2.resid\nresiduals.mean()\n# Write your code here","48499e39":"from statsmodels.stats.diagnostic import het_white\nfrom statsmodels.compat import lzip\nimport statsmodels.stats.api as sms","897f3f48":"##import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\n\nname = [\"F statistic\", \"p-value\"]\ntest = sms.het_goldfeldquandt(y_train, X_train)\nlzip(name, test)","714c573f":"# predicted values\nfitted = model2.fittedvalues\n\n# sns.set_style(\"whitegrid\")\nsns.residplot(x = fitted, y = residuals, color=\"lightblue\", lowess=True) #write your code here\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residual\")\nplt.title(\"Residual PLOT\")\nplt.show()","1f366170":"# Plot histogram of residuals\n#write your code here\nsns.histplot(residuals, kde=True)","6045ccc0":"# Plot q-q plot of residuals\nimport pylab\nimport scipy.stats as stats\n\nstats.probplot(residuals, dist=\"norm\", plot=pylab)\nplt.show()","f4108151":"# RMSE\ndef rmse(predictions, targets):\n    return np.sqrt(((targets - predictions) ** 2).mean())\n\n\n# MAPE\ndef mape(predictions, targets):\n    return np.mean(np.abs((targets - predictions)) \/ targets) * 100\n\n\n# MAE\ndef mae(predictions, targets):\n    return np.mean(np.abs((targets - predictions)))\n\n\n# Model Performance on test and train data\ndef model_pref(olsmodel, x_train, x_test):\n\n    # Insample Prediction\n    y_pred_train = olsmodel.predict(x_train)\n    y_observed_train = y_train\n\n    # Prediction on test data\n    y_pred_test = olsmodel.predict(x_test)\n    y_observed_test = y_test\n\n    print(\n        pd.DataFrame(\n            {\n                \"Data\": [\"Train\", \"Test\"],\n                \"RMSE\": [\n                    rmse(y_pred_train, y_observed_train),\n                    rmse(y_pred_test, y_observed_test),\n                ],\n                \"MAE\": [\n                    mae(y_pred_train, y_observed_train),\n                    mae(y_pred_test, y_observed_test),\n                ],\n                \"MAPE\": [\n                    mape(y_pred_train, y_observed_train),\n                    mape(y_pred_test, y_observed_test),\n                ],\n            }\n        )\n    )\n\n\n# Checking model performance\nmodel_pref(model2, X_train, X_test)  ","69c2e594":"# import the required function\n\nfrom sklearn.model_selection import cross_val_score\n\n# build the regression model and \nlinearregression = LinearRegression()                                    \n\ncv_Score11 = cross_val_score(linearregression, X_train, y_train, cv = 10)#write your code here\ncv_Score12 = cross_val_score(linearregression, X_train, y_train, cv = 10, scoring = 'neg_mean_squared_error') #write your code here                                \n\n\nprint(\"RSquared: %0.3f (+\/- %0.3f)\" % (cv_Score11.mean(), cv_Score11.std() * 2))\nprint(\"Mean Squared Error: %0.3f (+\/- %0.3f)\" % (-1*cv_Score12.mean(), cv_Score12.std() * 2))","9071edde":"\ncoef =model2.params\n#write your code here","066a8444":"# Let us write the equation of the fit\nEquation = \"log (Price) =\"\nprint(Equation, end='\\t')\nfor i in range(len(coef)):\n    print('(', coef[i], ') * ', coef.index[i], '+', end = ' ')","7b4478dc":"- Droping the column 'TAX' from the training data to check if multicollinearity is removed","7c1cb75b":"* **The variables CRIM and ZN are positively skewed.** This suggests that most of the areas have lower crime rates and most residential plots are under the area of 25,000 sq. ft.\n* **The variable CHAS, with only 2 possible values 0 and 1, follows a binomial distribution**, and the majority of the houses are away from Charles river (CHAS = 0).\n* The distribution of the variable AGE suggests that many of the owner-occupied houses were built before 1940. \n* **The variable DIS** (average distances to five Boston employment centers) **has a nearly exponential distribution**, which indicates that most of the houses are closer to these employment centers.\n* **The variables TAX and RAD have a bimodal distribution.**, indicating that the tax rate is possibly higher for some properties which have a high index of accessibility to radial highways.  \n* The dependent variable MEDV seems to be slightly right skewed.","ce673286":"### Visualizing the relationship between the features having significant correlations (> 0.7) ","13530128":"### Check the below linear regression assumptions\n\n1. **Mean of residuals should be 0**\n2. **No Heteroscedasticity**\n3. **Linearity of variables**\n4. **Normality of error terms**","c1cd7f21":"**Observations:____**\n- Average per capita crime rate by town is about 3.6%. It has a range, from 0.006% to 89%, and atleast 75% of house neighbouhoods has crime rate of 3.7%.\n- proportion of residential land zoned for lots over 25,000 sq.ft is around 11.4%. It has a range, from 0 to 100.\n- proportion of non-retail business acres per town is 11%. It has a high range of values from 0.5-27%.\n- On average, nitric oxides concentration (parts per 10 million) percentage is about 0.6%. \n- average number of rooms per dwelling averging around 6. 75% of houses has around 7.\n- average proportion of owner-occupied units built prior to 1940 is around 66 and it ranges between 3 and 100.\n- avearage weighted distances to five Boston employment centres is 3.8 and it ranges between 1 and 12.\n- average index of accessibility to radial highwaysis around 10 and maximum is 24.\n- average full-value property-tax rate per 10,000 dollars is 408 and it ranges between 187 and 711. 75% of the houses has 666.\n- average pupil-teacher ratio by town is around 18 and it ranges between 12-22.\n- average %lower status of the population is 12.7% and it ranges between 1.7-37.93%.\n-  average Median value of owner-occupied homes in 1000 dollars is around 22 and it ranges between 5-50. at least 75% of the houses have median of 25  owner-occupied homes in 1000 dollars.","08084c6b":"## Recommendations\n- House prices tend to be higher mainly when number of rooms getting increased.\n- House prices also getting slightly increased if houses tract bound river.\n- House prices are decresed mainly when nitric oxide concentration is higher in the area.\n- House prices also getting decresed by the features like lower pupil-teacher ratio,  higher percentage of lower status people availability and when have longer distance to 5 bostom employment centers.","e96df465":"* No trend between the two variables is visible in the above plot.","dbf5f4b8":"#### Check for mean residuals","935961e7":"**Observations:____**\n- We can see that when NOX is in the range of 0-06, it has positive correlation. when NOX increases INDUS also increases and INDUS seems to have maintain constant 17-18 value afterwards while having few outliers. It is possible that since areas with higher NO concentration has not much houses , they have higher non retail business acres per town. ","7a4eb4f2":"Now, we will check the linear regression assumptions.","888bd878":"* The price of the house indicated by the variable MEDV is the target variable and the rest are the independent variables based on which we will predict house price.","3753e51a":"---------------------------\n## Univariate Analysis\n---------------------------","5a0192f3":"**Observations:____**\nAs we can see from the above test the p-value is greater than 0.05, so we fail to reject the null-hypothesis. That means - residuals are homoscedastic.","63129637":"### Check the distribution of the variables","a44305df":"**Observations:______**\n- RM seems to have a higher positive correlation with MEDV and MEDV_log.\n- LSTAT(%lower status of population) is seems like highly negatively correlated with MEDV and MEDV_lg.\n- PTRATIO, tax, NOX and INDUS also have a negative correlation with both MEDV and MEDV_log.\n- CRIM, AGE, RAD also are positively correlated with MEDV_log and MEDV. MEDV_log seems to have higher positive correlation with those independant variables comparaed to MEDV.\n- CRIM seems to have positive correlation with RAD and TAX.","24a3666f":"* The price of the house seems to increase as the value of RM increases. This is expected as the price is generally higher for more rooms.\n\n* There are a few outliers in a horizotal line as the MEDV value seems to be capped at 50.","fbf6ca56":"\nThe problem on hand is to predict the housing prices of a town or a suburb based on the features of the locality provided to us. In the process, we need to identify the most important features in the dataset. We need to employ techniques of data preprocessing and build a linear regression model that predicts the prices for us. \n\n----------------------------\n## Data Information\n---------------------------\n\nEach record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. Detailed attribute information can be found below-\n\nAttribute Information (in order):\n- **CRIM:**     per capita crime rate by town\n- **ZN:**       proportion of residential land zoned for lots over 25,000 sq.ft.\n- **INDUS:**    proportion of non-retail business acres per town\n- **CHAS:**     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n- **NOX:**      nitric oxides concentration (parts per 10 million)\n- **RM:**       average number of rooms per dwelling\n- **AGE:**     proportion of owner-occupied units built prior to 1940\n- **DIS:**      weighted distances to five Boston employment centres\n- **RAD:**      index of accessibility to radial highways\n- **TAX:**      full-value property-tax rate per 10,000 dollars\n- **PTRATIO:**  pupil-teacher ratio by town\n- **LSTAT:**    %lower status of the population\n- **MEDV:**     Median value of owner-occupied homes in 1000 dollars.\n\n--------------------------------------------\nWishing you all the best!","87cef4be":"The log-transformed variable (**MEDV_log**) appears to have a **nearly normal distribution without skew**, and hence we can proceeed.","2c859124":"#### Check for homoscedasticity","97952ab1":"\n\n* Homoscedasticity - If the residuals are symmetrically distributed across the regression line, then the data is said to homoscedastic.\n\n* Heteroscedasticity- - If the residuals are not symmetrically distributed across the regression line, then the data is said to be heteroscedastic. In this case, the residuals can form a funnel shape or any other non-symmetrical shape.\n\n* We'll use `Goldfeldquandt Test` to test the following hypothesis with alpha = 0.05:\n\n    - Null hypothesis: Residuals are homoscedastic\n    - Alternate hypothesis: Residuals have heteroscedasticity","4e0680d0":"#### Apply cross validation to improve the model and evaluate it using different evaluation metrics","1548b756":"### Get model Coefficients in a pandas dataframe with column 'Feature' having all the features and column 'Coefs' with all the corresponding Coefs. Write the regression equation.","682b0c6b":"## Let's now check the summary statistics of this dataset","d247cab7":"Before performing the modeling, it is important to check the univariate distribution of the variables.","c007db35":"So the high correlation between TAX and RAD is due to the outliers. The tax rate for some properties might be higher due to some other reason.","46076bb3":"Before creating the linear regression model, it is important to check the bivariate relationship between the variables. Let's check the same using the heatmap and scatterplot.","899ea3ff":"**Observations:____**\n-The residual value is negative which means the actual value is about -4.05827286402529e-15 lesser than predicted vaues. Also it is closer to o hence, the corresponding assumption is satisfied.","33e0602e":"#### Normality of error terms\nThe residuals should be normally distributed.","d9493ea5":"The correlation between RAD and TAX is very high. But, no trend is visible between the two variables. \nThis might be due to outliers. ","ae8afd84":"### Let us start by importing the required libraries","98aefe16":"### Split the dataset\nLet's split the data into the dependent and independent variables and further split it into train and test set in a ratio of 70:30 for train and test set.","29159c45":"#### Linearity of variables\n\nIt states that the predictor variables must have a linear relation with the dependent variable.\n\nTo test the assumption, we'll plot residuals and fitted values on a plot and ensure that residuals do not form a strong pattern. They should be randomly and uniformly scattered on the x-axis.","bbe1aba5":"### Read the dataset","46d03592":"* There are two variables with a high VIF - RAD and TAX. Let's remove TAX as it has the highest VIF values and check the multicollinearity again.","87357879":"**Observatioins: - We can see that there is some pattern in fitted values and residuals which means the residuals are not randomly distributed. We need to fix this in order to have a proper model.**","ae12cc5c":"**Observations:____**\n- The RMSE on the test data set is 0.195504 which is almost similar to the RMSE on the training dataset.\n- And the MAE on test data set is 0.143686 which is almost similar to the MAE on the training dataset.\n- And the MAPE on test data set is 4.981813 which is almost similar to the MAPE on the training dataset.","8a212d3e":"**Observations:___**\n\n`DIS` and `NOX`, it is clear that there is a negative correlation between them with few outliers. It is possible that housing neighbourhoods closer to Boston employment centers are developed in areas with less nitric oxides concentration. This means less houses and buiseness centers are built in areas with higher nitric concentration. This must be sue to the ehalth risks.","cc7252e4":"**Observations:_____**\n- adj. R-squared is 0.762 which is closer to 1 which means the model ahs a better fit.\n- p-value of CRIM, CHAS, NOX, RM, DIS, RAD, PTRATIO, LSTAT variables are less than the significance level 0.05, that means we will reject the null hypothesis in favor of alternate hypothesis for those features. In other words, we have enough statistical evidence that there is relationship exists between each of those independant variables and `MEDV_log`.","68a5b2d1":"As the dependent variable is sightly skewed, we will apply a **log transformation on the 'MEDV' column** and check the distribution of the transformed column.","036255f9":"* The distance of the houses to the Boston employment centers appears to decrease moderately as the the proportion of the old houses increase in the town. It is possible that the Boston employment centers are located in the established towns where proportion of owner-occupied units built prior to 1940 is comparatively high.","921cc126":"* The tax rate appears to increase with an increase in the proportion of non-retail business acres per town. This might be due to the reason that the variables TAX and INDUS are related with a third variable.","4f486b0d":"**Observations:____**\n`AGE` and `NOX`, it is clear that there is a positive correlation between them with few outliers. It is possible that owner-occupied units built prior to 1940 is comparatively high in areas with higher Nitric Oxide. This might be becasue, older houses did not consider NO concentration during the construction.","29dbd4ef":"---------------------------\n## Bivariate Analysis\n---------------------------","b391f724":"# Project Linear Regression: Boston House Price Prediction","78a2b428":"Next, we will check the multicollinearity in the train dataset.","cdd47ccb":"Now, we will create the linear regression model as the VIF is less than 5 for all the independent variables, and we can assume that multicollinearity has been removed between the variables.","a81e65e3":"### Check for Multicollinearity\n\nWe will use the Variance Inflation Factor (VIF), to check if there is multicollinearity in the data.\n\nFeatures having a VIF score > 5 will be dropped\/treated till all the features have a VIF score < 5","ad3d44c6":"### Get information about the dataset using the info() method","36492095":"### Examining the significance of the model\n\nIt is not enough to fit a multiple regression model to the data, it is necessary to check whether all the regression coefficients are significant or not. Significance here means whether the population regression parameters are significantly different from zero. \n\nFrom the above it may be noted that the regression coefficients corresponding to ZN, AGE, and INDUS are not statistically significant at level \u03b1 = 0.05. In other words, the regression coefficients corresponding to these three are not significantly different from 0 in the population. Hence, we will eliminate the three features and create a new model.","2efa260c":"- The R-squared on the cross validation is 0.729, whereas on the training dataset it was 0.769\n- And the MSE on cross validation is 0.041, whereas on the training dataset it was 0.038","a7c6d197":"### Check the performance of the model on the train and test data set","0ceacdf3":"**Observations:_____**\n- In the histrogram we can see residuals are normaly distrbuted so it maintains Normality.\n- In QQ Plot, we can see a straightline plot for Residuals following normal distribution. The plot deviates from straigh line after theorotical quantity value=2.","0a278070":"#### Let's check the correlation using the heatmap ","fe18bb3d":"Let's check the correlation after removing the outliers.","55264d03":"We may want to reiterate the model building process again with new features or better feature engineering to increase the R-squared and decrease the MSE on cross validation.","2b0b24c8":"* There are a total of 506 non-null observations in each of the columns. This indicates that there are no missing values in the data.\n\n* Every column in this dataset is numeric in nature.","77cb6411":"* We can see that the **R-squared value has decreased by 0.002**, since we have removed variables from the model, whereas the **adjusted R-squared value has increased by 0.001**, since we removed statistically insignificant variables only.","8c821afe":"## Conclusions\n- From this above equation, we can interpret that - with one unit change in the variable `CHAS` the outcome variable log of `MEDV` increases by 0.1198 units. \\n\",\n- Also, with one unit change in the variable `RM` the outcome variable log of `MEDV` increases by 0.0589 units. This means house prices getting higherby 0,0589 when room numbers are increasing by a unit.\\n\",\n- On an average, the log MEVD of houses decresed by 0.0125 when crime rate is incresed by 1 unit.\\n\",\n- On an average, the log MEVD of houses decresed by 1.056 when nitric oxide concentration rate is incresed by 1 unit.\\n\",\n- On an average, the log MEVD of houses decresed by 0.0079 when weighted distance to 5 boston employment centers when crime rate is incresed by 1 unit.\\n\",\n- On an average, the log MEVD of houses decresed by 0.0485 when pupil-teacher ratio is incresed by 1 unit.\\n\",\n- On an average, the log MEVD of houses decresed by 0.0293 when lower status of population percentage is incresed by 1 unit.\"","b15e49de":"We have seen that the variables LSTAT and RM have a linear relationship with the dependent variable MEDV. Also, there are significant relationships among a few independent variables, which is not desirable for a linear regression model. Let's first split the dataset.","d61d717e":"* The price of the house tends to decrease with an increase in LSTAT. This is also possible as the house price is lower in areas where lower status people live.\n\n* There are few outliers and the data seems to be capped at 50.","0f04615a":"Now, we will visualize the relationship between the pairs of features having significant correlations."}}