{"cell_type":{"89591a23":"code","9d6c8dec":"code","a9fbfd17":"code","84efe03d":"code","d217e42b":"code","a6b50475":"code","0204f1b2":"markdown","7fe9e3ec":"markdown","534f4c96":"markdown","68a9e6b8":"markdown","ebb3d83c":"markdown","f76390dc":"markdown","f2f9349d":"markdown","e735be64":"markdown","88daf717":"markdown","6d326456":"markdown","ff22d410":"markdown","d4a06ddb":"markdown","cbd8a055":"markdown","f8ea8e5a":"markdown","be43b41f":"markdown","ed999bb0":"markdown","d2928888":"markdown","13936563":"markdown","e62100de":"markdown","b3b77b22":"markdown","8e3b6a52":"markdown","7c1c0bcf":"markdown","82bbfd1a":"markdown","b2f2cba7":"markdown","26bb9b35":"markdown","bbe5d456":"markdown","4e2565ff":"markdown","64624c89":"markdown","1e34b6ce":"markdown","5dac9511":"markdown","1113f792":"markdown","bd33169f":"markdown","147de413":"markdown","00401ecf":"markdown","2a2acb8c":"markdown","ea35bd47":"markdown","46840e59":"markdown","25433d42":"markdown","0e704189":"markdown","8bcdcc54":"markdown","bcb1c211":"markdown"},"source":{"89591a23":"import tensorflow.keras as keras\nimport numpy as np\n\npath = keras.utils.get_file(\n    'nietzsche.txt',\n    origin='https:\/\/s3.amazonaws.com\/text-datasets\/nietzsche.txt')\ntext = open(path).read().lower()\nprint('Corpus length:', len(text))","9d6c8dec":"# Length of extracted character sequences\nmaxlen = 60\n\n# We sample a new sequence every `step` characters\nstep = 3\n\n# This holds our extracted sequences\nsentences = []\n\n# This holds the targets (the follow-up characters)\nnext_chars = []\n\nfor i in range(0, len(text) - maxlen, step):\n    sentences.append(text[i: i + maxlen])\n    next_chars.append(text[i + maxlen])\nprint('Number of sequences:', len(sentences))\n\n# List of unique characters in the corpus\nchars = sorted(list(set(text)))\nprint('Unique characters:', len(chars))\n# Dictionary mapping unique characters to their index in `chars`\nchar_indices = dict((char, chars.index(char)) for char in chars)\n\n# Next, one-hot encode the characters into binary arrays.\nprint('Vectorization...')\nx = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        x[i, t, char_indices[char]] = 1\n    y[i, char_indices[next_chars[i]]] = 1","a9fbfd17":"from tensorflow.keras import layers\n\nmodel = keras.models.Sequential()\nmodel.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(layers.Dense(len(chars), activation='softmax'))","84efe03d":"optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)","d217e42b":"def sample(preds, temperature=1.0):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)","a6b50475":"import random\nimport sys\n\nfor epoch in range(1, 60):\n    print('epoch', epoch)\n    # Fit the model for 1 epoch on the available training data\n    model.fit(x, y,\n              batch_size=128,\n              epochs=1)\n\n    # Select a text seed at random\n    start_index = random.randint(0, len(text) - maxlen - 1)\n    generated_text = text[start_index: start_index + maxlen]\n    print('--- Generating with seed: \"' + generated_text + '\"')\n\n    for temperature in [0.2, 0.5, 1.0, 1.2]:\n        print('------ temperature:', temperature)\n        sys.stdout.write(generated_text)\n\n        # We generate 400 characters\n        for i in range(400):\n            sampled = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(generated_text):\n                sampled[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(sampled, verbose=0)[0]\n            next_index = sample(preds, temperature)\n            next_char = chars[next_index]\n\n            generated_text += next_char\n            generated_text = generated_text[1:]\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()","0204f1b2":"Possible to sample from the softmax output which we know produces a 'probability' distribution\n\nBut - uncontrollable","7fe9e3ec":"0.5 is the most interesting temperature in this case\n\nA bigger model, trained for longer and on more data, would achieve more coherent and \nrealistic text","534f4c96":"But don't expect meaning!\n\nThe network is merely sampling a statistical model of which characters follow other characters","68a9e6b8":"The paramaterised softmax distribution is computed like this:\n\n1. Take logs: $\\frac{\\log(p_i)}{T} = \\frac{x_i}{T} - \\frac{N}{T}$\n2. Re-exponentiate: $e^{\\frac{\\log(p_i)}{T}} = c(T) e^{\\frac{x_i}{T}} \\text{ where }c\\text{ is a temperature dependent constant}$\n3. Find new normalisation: $N' = \\sum c(T) e^{\\frac{x_i}{T}}$\n4. Temperatured softmax: $q_i = \\frac{1}{N'}e^{\\frac{x_i}{T}} = \\frac{ e^{\\frac{x_i}{T}} }{\\sum e^{\\frac{x_i}{T}}} $","ebb3d83c":"## 8.140 Implementing character-level LSTM text generation\n\nA large training set is required for a good language model\n\nAny large text file such as Lord of the Rings, or even set of texts such as Wikipedia\n\nWe will model the writings of a late C19 German philosopher \n\n","f76390dc":"----","f2f9349d":"------ temperature: 0.\no wield the sceptre.\nthe christian pessimists of practice, he is the same same sense of the fact of the other conduct.\n\n","e735be64":"------ temperature: 1.2\nring of person one more tiral soul more contemption of the\nevloughtiitions difficed and english them to\ncould have is every bood higher objection is and  thever keen to the e?fings","88daf717":"# My university of London Coursework","6d326456":"------ temperature: 1.0\necisely with the assumerance, which the old pated between fortem last exercice necessaried--yes original foregnoth and over-pride of shame methally forehated truves of an act oftiness sas.\n","ff22d410":"Low temperature: extremely repetitive and predictable text, with realistic local structure - almost all words (= a local pattern of characters) are real English words \n\nIntermediate temperatures: more interesting, surprising, even creative text - sometimes completely new but plausible words are invented \n","d4a06ddb":"Limiting cases\n\n1. $T \\rightarrow 0$ \n\n$\\max\\left( \\frac{1}{N}e^{\\frac{x_i}{T}}\\right)$ dominates - greedy sampling","cbd8a055":"2015: Google's DeepDream produced psychedelic images\n\n2016: Prisma turns photos into 'paintings'\n\n2016: [Sunspring](https:\/\/www.youtube.com\/watch?v=LY7x2Ihqjmc), an experimental film with an LSTM generated script\n\nhttps:\/\/www.youtube.com\/watch?v=LY7x2Ihqjmc\n\n2000's: Neural network generated music","f8ea8e5a":"Not human replacement, but augmented intelligence\n\nA different kind of intelligence","be43b41f":"##### Training and sampling the language model\n\nThe sampling function:\n","ed999bb0":"##### Preparing the data\n\nDownloading the corpus and converting to lowercase","d2928888":"\nTrain an ANN to predict the next token, or tokens, in a sequence using the previous tokens as input\n\nText = words or characters\n\nAny text-trained model is known as a *language model*","13936563":"----","e62100de":"###### Softmax temperature\n\nRemember, the softmax output is \n\n$\np_i = \\frac{1}{N}e^{x_i}\n$\n\nwhere $N = \\sum e^{x_i}$ ","b3b77b22":"High temperature: local structure breaks down and most words look random","8e3b6a52":"*Uniform sampling* Each token has the same probability - maximum randomness  ","7c1c0bcf":"----","82bbfd1a":"----","b2f2cba7":"2. $T \\rightarrow \\infty$ \n\n$e^{\\frac{x_i}{T}} \\rightarrow 1$ so $q_i \\rightarrow \\frac{1}{M}$ where $M$ is the number of softmax outputs - a uniform distribution","26bb9b35":"DL algorithms can learn a statistical *latent space*\n\nSampling from the latent space 'creates' new artworks similar to the training data\n\nThe algorithm attaches no meaning to the process and the product - but we might\n\nPotentially eliminates technical skill\n\n=> enables free expression; separates art from craft","bbe5d456":"*Intermediate randomness* controlled by the softmax temperature - this is where we expect to find the more interesting, creative outputs","4e2565ff":"----","64624c89":"## 8.130 The sampling strategy\n\n1. *Greedy sampling.* Select the most probable token - repetitive and unrealistic\n\n2. *Stochastic sampling.* Sample from the probability distribution of the next character","1e34b6ce":"------ temperature: 0.5\npirit and the are soul and also its conscience of the provessy in the form is be an incertain promasing who has not been lack and formul.\n\n","5dac9511":"The parameterised softmax distribution is \n\n$\nq_i = \\frac{1}{N}e^\\frac{x_i}{T}\n$\n\nwhere $N = \\sum e^\\frac{x_i}{T}$ ","1113f792":"###### Building the network\n\nA single `LSTM` layer followed by a `Dense` classifier and softmax over all possible characters\n\n(1D convnets as an alternative)","bd33169f":"- speech synthesis\n\n- chatbox dialogue\n\n- Google's Smart Reply (2016) - automatic generation of short replies to emails and text messages.","147de413":"## 8.120 Generating sequence data\n\nRNNs can generate new sequence data\n\n- musical notes\n\n- brushstrokes recorded on an iPad\n\n- handwriting","00401ecf":"----","2a2acb8c":"##### The idea\n\nArtistic creation involves pattern recognition and technical skill - tedious work that can be mechanised\n\nPerceptual modalities, language, artwork and music all have statistical structure and statistical structure can be learned by DL algorithms","ea35bd47":"Extract partially-overlapping sequences of length `maxlen`, one-hot encodes and pack in a 3D Numpy array `x` of shape `(sequences, maxlen, unique_characters)`\n\nPrepare an array `y` containing the corresponding targets: the one-hot encoded characters that come right after each extracted sequence","46840e59":"Train and generate text using a range of different temperatures at each epoch end\n\n=> monitor convergence and the impact of temperature","25433d42":"## 8.100 Text generation\n\nWe have seen how DL can analyse data - but can it also create?","0e704189":"Targets are one-hot encoded => use `categorical_crossentropy` loss:","8bcdcc54":"## import tensorflow as tf\ntf.__version__","bcb1c211":"A language model captures the latent space of language i.e. its statistical structure\n\n1. Train model\n2. Present an initial *conditioning* text string\n3. Model predicts the next token(s)\n4. Add the generated text to the input text\n5. Go back to step 3."}}