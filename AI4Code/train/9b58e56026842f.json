{"cell_type":{"e5fe1c74":"code","7dfb93c5":"code","e9e90ece":"code","834aad63":"code","c2c1a662":"code","943ba4b3":"code","12eb61e4":"code","6d68e16e":"code","06c3c5de":"code","02548ef7":"code","77ddb51f":"code","5af4001d":"code","0df1bbb9":"code","475f8ab3":"code","09a7d658":"code","9c3d8e88":"code","851f2e91":"code","33912c3d":"code","36e6d993":"code","a8538d23":"code","68ffad96":"code","576e7494":"code","e7cde759":"code","5b4204e2":"code","6ed543b8":"code","ce3269fb":"code","47c4372f":"code","46f25624":"markdown","963e3bef":"markdown","566a9ab3":"markdown","cf06fd67":"markdown","e828915f":"markdown","b0f2db28":"markdown","9f29193c":"markdown","f40fa0b2":"markdown","cc5c5e56":"markdown","79cf473a":"markdown","ce2b0c94":"markdown","ca4cebb4":"markdown"},"source":{"e5fe1c74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7dfb93c5":"train_df = pd.read_csv(\"\/kaggle\/input\/fall-2021-2022-cs412-project\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/fall-2021-2022-cs412-project\/test.csv\")\n\n\ntrain_df.head(10)","e9e90ece":"# Try to guess null brands from their models\ntemp = train_df['brand'].isnull().values\n\nindex = np.where(temp == True)\n\nfor i in index[0]:\n    car_model = train_df.loc[i, 'model']\n    car_brand = train_df.loc[train_df[\"model\"] == car_model].iloc[0]['brand']\n    train_df.loc[i, 'brand'] = car_brand\n    \ntemp = test_df['brand'].isnull().values\n\nindex = np.where(temp == True)\n\nfor i in index[0]:\n    car_model = test_df.loc[i, 'model']\n    car_brand = test_df.loc[test_df[\"model\"] == car_model].iloc[0]['brand']\n    test_df.loc[i, 'brand'] = car_brand","834aad63":"# Commented out since not improving the accuracy\n\n\n# Try to guess null models from their brands most used models\n\"\"\"groupModel = test_df.groupby('brand')\n\nbrands = train_df.brand.unique()\n\nfor brand in brands:\n    temp = groupModel.get_group(brand)\n    \n    nullVal = train_df['model'].isnull().values\n    index = np.where(nullVal == True)\n    \n    arr = temp['model'].value_counts()\n    model = arr.index[0]\n    \n    for i in index[0]:\n        if train_df.loc[i, 'brand'] == brand:\n            train_df.loc[i, 'model'] = model\n\nbrands = test_df.brand.unique()\n\nfor brand in brands:\n    temp = groupModel.get_group(brand)\n    \n    nullVal = test_df['model'].isnull().values\n    index = np.where(nullVal == True)\n    \n    arr = temp['model'].value_counts()\n    model = arr.index[0]\n    \n    for i in index[0]:\n        if test_df.loc[i, 'brand'] == brand:\n            test_df.loc[i, 'model'] = model\"\"\"","c2c1a662":"# Transform categorical columns into one hot encoding \ndataset = pd.concat(objs=[train_df, test_df], axis=0)\ndataset_preprocessed = pd.get_dummies(dataset)\ntrain_objs_num = len(train_df)\ntrain_df = dataset_preprocessed[:train_objs_num]\ntest_df = dataset_preprocessed[train_objs_num:]","943ba4b3":"temp = train_df[train_df['tax'].isna()].index.tolist()\ntrain_df.loc[temp, 'tax'] = 0\n\n\ntemp = test_df[test_df['tax'].isna()].index.tolist()\ntest_df.loc[temp, 'tax'] = 0","12eb61e4":"temp = train_df[train_df['tax(\u00a3)'].isna()].index.tolist()\ntrain_df.loc[temp, 'tax(\u00a3)'] = 0\n\ntemp = test_df[test_df['tax(\u00a3)'].isna()].index.tolist()\ntest_df.loc[temp, 'tax(\u00a3)'] = 0","6d68e16e":"tax_df = train_df['tax'] + train_df['tax(\u00a3)']\ntrain_df['tax'] = tax_df\n\n\ntax_df = test_df['tax'] + test_df['tax(\u00a3)']\ntest_df['tax'] = tax_df","06c3c5de":"del train_df['tax(\u00a3)']\ndel test_df['tax(\u00a3)']\ndel test_df['price']","02548ef7":"test_df.head()","77ddb51f":"train_df = train_df.dropna()\n\ntest_df = test_df.fillna(test_df.mean())","5af4001d":"# Find the correlations\n\ncor = train_df.corr()\nprice_cor = cor['price']\npd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\ndf = pd.DataFrame(data=price_cor).head(100)\nsorted = df.sort_values(by='price')\nprint(sorted[-10:])\nprint(sorted[:10])","0df1bbb9":"from sklearn.preprocessing import StandardScaler\n\ncol_list = ['year', 'mileage', 'mpg', 'engineSize', 'tax', 'price']\ncol_list_test = ['year', 'mileage', 'mpg', 'engineSize', 'tax']\n\n\nscaler = StandardScaler()\n# scale the data\ntrain_df[col_list_test] = scaler.fit_transform(train_df[col_list_test])\n\ntest_df[col_list_test] = scaler.transform(test_df[col_list_test])","475f8ab3":"X = train_df.drop(\"price\", axis = 1).values\ny = train_df[['price']].values","09a7d658":"from sklearn.model_selection import train_test_split\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0001, random_state=42)\nX_train, y_train = X, y","9c3d8e88":"# run once to get the best params\n\n\"\"\"from sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\npenalty = ['l2', 'l1', 'elasticnet']\nalpha = [0.0001, 0.001, 0.01, 0.1, 1, 2]\nfit_intercept = [True, False]\nmax_iter = [50, 100, 1000, 2000]\nlearning_rate = ['invscaling', 'adaptive', 'optimal']\n\nrandom_grid = {\n    'penalty': penalty,\n    'alpha' : alpha,\n    'fit_intercept' : fit_intercept,\n    'max_iter' : max_iter,\n    'learning_rate' : learning_rate\n}\n\n\nsgdParamTuning = SGDRegressor()\n\nsgd_random = RandomizedSearchCV(estimator = sgdParamTuning, param_distributions = random_grid, n_iter = 50, cv = 3, verbose = 2, random_state= 42, n_jobs = -1)\n\nsgd_random.fit(X_train[:7500], y_train[:7500])\n\nprint(sgd_random.best_params_)\"\"\"","851f2e91":"# run once to get the best params\n\n\"\"\"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neural_network import MLPRegressor\n    \nhidden_layer_sizes = [(50,50,50), (50,100,50), (100,)]\nactivation = ['identity', 'logistic', 'tanh', 'relu']\nalpha = [0.0001, 0.001, 0.01, 0.1, 1, 2]\nlearning_rate = ['invscaling', 'adaptive', 'optimal']\n\nrandom_grid = {\n    'hidden_layer_sizes' : hidden_layer_sizes,\n    'activation': activation,\n    'alpha' : alpha,\n    'learning_rate' : learning_rate\n}\n\n\nmlpParamTuning = MLPRegressor()\n\nmlp_random = RandomizedSearchCV(estimator = mlpParamTuning, param_distributions = random_grid, n_iter = 50, cv = 3, verbose = 2, random_state= 42, n_jobs = -1)\nmlp_random.fit(X_train[:7500], y_train[:7500])\n\nprint(mlp_random.best_params_)\"\"\"","33912c3d":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score\n\n# used only for quick accuracy adjustments\n#\u00a0est = GradientBoostingRegressor(n_estimators=150, learning_rate=1, max_depth=2, random_state=0).fit(X_train, y_train)\n# print(r2_score(y_true=y_test, y_pred=est.predict(X_test)))","36e6d993":"# Used once to check the results before submitting to competition\n\"\"\"# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model_cv(model, X_train, y_train.ravel())\n    results.append(scores)\n    names.append(name)\n    print('>%s: accuracy= %.3f, std= (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\"\"\"","a8538d23":"# make a prediction with a stacking ensemble\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR, LinearSVR\n\nfrom sklearn.ensemble import StackingRegressor, GradientBoostingRegressor, BaggingRegressor, RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import explained_variance_score\n\n\n# define the base models\nlevel0 = list()\nlevel0.append(('ridge', Ridge(normalize=False, max_iter=5000, fit_intercept=True, alpha=0.5)))\nlevel0.append(('rf', RandomForestRegressor(random_state=42, n_estimators=500, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', max_depth=70, bootstrap=False)))\nlevel0.append(('gbr', GradientBoostingRegressor(n_estimators=700, max_depth=4, min_samples_split=8, min_samples_leaf=1, max_features='auto', learning_rate=0.1)))\nlevel0.append(('SGD', SGDRegressor(penalty='l2', max_iter=1000, learning_rate='optimal', fit_intercept=False, alpha=2)))\nlevel0.append(('mlp', MLPRegressor(learning_rate='invscaling', hidden_layer_sizes=(50, 100, 50), alpha=1, activation='relu')))\nlevel0.append(('xgb', xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                   colsample_bynode=1, colsample_bytree=1, gamma=0,\n                   importance_type='gain', learning_rate=0.08, max_delta_step=0,\n                   max_depth=7, min_child_weight=1, missing=1, n_estimators=100,\n                   n_jobs=1, nthread=None, random_state=0,\n                   reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                   silent=None, subsample=0.75, verbosity=1, objective='reg:squarederror')))\nseed = np.random.randint(0,1000)\n\n\n# define the stacking ensemble\nlevel1 = LinearRegression()\n# define the stacking ensemble\nmodel = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\nmodel.fit(X_train, y_train.ravel())\n\n# pred_v = model.predict(X_test)\n# print(r2_score(y_true=y_test, y_pred=pred_v))","68ffad96":"y_train","576e7494":"pred_t = model.predict(test_df)","e7cde759":"pred_df = pd.DataFrame(pred_t, columns = ['price'])","5b4204e2":"IDS = test_df['ID']\npred_df = pred_df.set_index(IDS)\n\npred_df.head()","6ed543b8":"prediction = pd.DataFrame(pred_df).to_csv('predictions.csv')","ce3269fb":"\"\"\"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nwidth = 12\nheight = 10\nplt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(y_test, hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(pred_v, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n\nplt.title('Actual vs Fitted Values for Price')\nplt.xlabel('Price (in dollars)')\nplt.ylabel('Proportion of Cars')\n\nplt.show()\nplt.close()\"\"\"","47c4372f":"\"\"\"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nX_reduced = X_train[:5000]\ny_reduced = y_train[:5000]\n\n\npoly_reg=PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(X_train)\npol_reg = LinearRegression()\npol_reg.fit(X_poly, y_train)\"\"\"","46f25624":"Takes ~1 hour and 10 minutes","963e3bef":"# Ensemble (Hybrid) Learning","566a9ab3":"# Data Exploration","cf06fd67":"# Hyperparameter Tuning","e828915f":"Best Ridge Parameters\n\n\n{'normalize': False, 'max_iter': 5000, 'fit_intercept': True, 'alpha': 0.5}","b0f2db28":"# Visualization\n#### Not used when submitting predictions","9f29193c":"# Ensemble Prediction","f40fa0b2":"# Polynomial Regression (deprecated)\n\nUnable to compute since the memory usage exceeds the available ram","cc5c5e56":"#### Merge 2 tax cols to one column","79cf473a":"## Crossvalidation \n(takes too much time ~1.5 hours)","ce2b0c94":"# Data Preprocessing","ca4cebb4":"# Gradient Boosting Regressor\n### Not used!\n\nMixes up 150 estimators, takes ~40 seconds"}}