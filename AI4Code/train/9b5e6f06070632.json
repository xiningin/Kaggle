{"cell_type":{"78ca8d0b":"code","16bc7c0b":"code","6643778b":"code","4659bfca":"code","3aa6d284":"code","f5641f9c":"code","05406936":"code","00845086":"code","da042ca8":"code","8c318e37":"code","960c4f74":"code","86fad76d":"code","50e3fbc4":"code","912513a9":"code","b06739b9":"code","a2c9420f":"code","bb0e9f4a":"code","5be28f4c":"code","f025e581":"code","9b5336c6":"code","b0fceea8":"code","13b61820":"code","9fedff79":"code","18d245cc":"code","f8619672":"code","84a95634":"code","972b0351":"markdown","01b5b11e":"markdown","a8c0cc28":"markdown","a2b3e14d":"markdown","f2826027":"markdown","123a92a2":"markdown","fc0c9c25":"markdown","2c0622a7":"markdown","8516473f":"markdown","cc5c9fde":"markdown","523d1fa1":"markdown","fc6de954":"markdown","56d54f7f":"markdown","df2c404d":"markdown","00ddbf36":"markdown","0d04663b":"markdown","332ed88f":"markdown","9e7d636d":"markdown","8b667199":"markdown","ba6cfe0c":"markdown","c4d9b0f8":"markdown","d220e0d5":"markdown"},"source":{"78ca8d0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16bc7c0b":"pip install pyLDAvis\npip install gensim","6643778b":"# Importing the required libraries\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nnltk.download('punkt')\nnltk.download('wordnet')\nimport gensim\nfrom gensim import corpora\nfrom gensim import models\nfrom gensim.models import LdaModel\nfrom gensim.models import TfidfModel\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport re\nimport nltk\nimport spacy\nimport string\nimport matplotlib.pyplot as plt \nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport scattertext as st\nfrom collections import Counter\nimport re, io\nimport pyLDAvis\nimport pyLDAvis.gensim \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom pprint import pprint\nfrom scipy.stats import rankdata, hmean, norm\nimport spacy\nimport scattertext as st\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nimport os, pkgutil, json, urllib\nfrom urllib.request import urlopen\nfrom IPython.display import IFrame\nfrom IPython.core.display import display, HTML\nfrom scattertext import CorpusFromPandas, produce_scattertext_explorer","4659bfca":"# Reading the data set\ncustomer = pd.read_csv(\"..\/input\/customer-support-on-twitter\/twcs\/twcs.csv\", nrows = 5000)\n\n# Storing the text of the tweets in a variable\ntweet_text = customer[[\"text\"]]","3aa6d284":"# Visualizing the first few rows of the data set\ncustomer.head()","f5641f9c":"# Let us clean the tweet text for better visualization\ncustomer.drop('tweet_id', axis = 1, inplace = True)\ncustomer.drop('created_at', axis = 1, inplace = True)\ncustomer = customer[~customer['text'].isnull()]\n\ndef preprocess(CleanText):\n    CleanText = CleanText.str.replace(\"(<br\/>)\", \"\")\n    CleanText = CleanText.str.replace('(<a).*(>).*(<\/a>)', '')\n    CleanText = CleanText.str.replace('(&amp)', '')\n    CleanText = CleanText.str.replace('(&gt)', '')\n    CleanText = CleanText.str.replace('(&lt)', '')\n    CleanText = CleanText.str.replace('(\\xa0)', ' ')  \n    return CleanText\n\ncustomer['text'] = preprocess(customer['text'])","05406936":"customer['word_count'] = customer['text'].apply(lambda x: len(str(x).split(\" \")))\ncustomer[['text','word_count']].head(5)","00845086":"# Let us take a look at distribution of words in each tweet\ncustomer.text.str.split().\\\n    map(lambda x: len(x)).\\\n    hist()","da042ca8":"# Finding the number of characters in each tweet incuding the spaces\ncustomer['char_count'] = customer['text'].str.len() \ncustomer[['text','char_count']].head()","8c318e37":"# Plotting the character count for the tweets\ncustomer.text.str.len().hist()","960c4f74":"# Let us take a look at the number of stopwords in the tweets\nstopword = stopwords.words('english')\n\ncustomer['stopwords'] = customer['text'].apply(lambda x: len([x for x in x.split() if x in stopword]))\ncustomer[['text','stopwords']].head()","86fad76d":"# Distribution of stopwords\ncustomer.text.map(lambda x: len([x for x in x.split() if x in stopword])).hist()","50e3fbc4":"# Removing stopwords and punctuations from the tweets\ncustomer['text'] = customer['text'].str.replace('[^\\w\\s]','')\ncustomer['text'].head()\ncustomer['text'] = customer['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopword))\ncustomer['text'].head()","912513a9":"# Let us take a look at the most frequently used word in the tweets\nword_cloud = WordCloud(width = 1000,\n                       height = 800,\n                       colormap = 'Blues', \n                       margin = 0,\n                       max_words = 200,  \n                       min_word_length = 4,\n                       max_font_size = 120, min_font_size = 15,  \n                       background_color = \"white\").generate(\" \".join(customer['text']))\n\nplt.figure(figsize = (10, 15))\nplt.imshow(word_cloud, interpolation = \"gaussian\")\nplt.axis(\"off\")\nplt.show()","b06739b9":"print(\"Word Count of tweets for each Twitter Account\")\ncustomer.groupby('author_id').apply(lambda x: x.text.apply(lambda x: len(x.split())).sum())","a2c9420f":"print(\"Count of tweets for each Twitter Account\")\nprint(customer.groupby('author_id')['text'].count())","bb0e9f4a":"# Defining a function to visualise n-grams\ndef get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) \n                  for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:10]","5be28f4c":"# Visualising the most frequent unigrams in the tweets\ntop_unigrams = get_top_ngram(customer['text'],1)[:10]\nx,y = map(list,zip(*top_unigrams))\nsns.barplot(x = y,y = x)","f025e581":"# Visualising the most frequent bigrams in the tweets\ntop_bigrams = get_top_ngram(customer['text'],2)[:10]\nx,y = map(list,zip(*top_bigrams))\nsns.barplot(x = y,y = x)","9b5336c6":"# Visualising the most frequent trigrams in the tweets\ntop_trigrams = get_top_ngram(customer['text'],3)[:10]\nx,y = map(list,zip(*top_trigrams))\nsns.barplot(x = y,y = x)","b0fceea8":"reindexed_data = customer['text']\ntfidf_vectorizer = TfidfVectorizer(stop_words = 'english', use_idf=True, smooth_idf=True)\nreindexed_data = reindexed_data.values\ndocument_term_matrix = tfidf_vectorizer.fit_transform(reindexed_data)\nn_topics = 6\nlsa_model = TruncatedSVD(n_components=n_topics)\nlsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)\n\n## Function to return an integer list of predicted topic categories for the topic matrix\ndef get_keys(topic_matrix):\n    keys = topic_matrix.argmax(axis = 1).tolist()\n    return keys\n\n## Function to return a tuple of topic categories \ndef keys_to_counts(keys):\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)\n    \nlsa_keys = get_keys(lsa_topic_matrix)\nlsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n\n## Function to return a list of n topics\ndef get_top_n_words(n, keys, document_term_matrix, tfidf_vectorizer):\n    top_word_indices = []\n    for topic in range(n_topics):\n        temp_vector_sum = 0\n        for i in range(len(keys)):\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        temp_vector_sum = temp_vector_sum.toarray()\n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)   \n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            the_word = tfidf_vectorizer.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n        top_words.append(\" \".join(topic_words))         \n    return top_words","13b61820":"# Printing the most common topics \ntop_n_words_lsa = get_top_n_words(3, lsa_keys, document_term_matrix, tfidf_vectorizer)\n\nfor i in range(len(top_n_words_lsa)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])","9fedff79":"# Pre processing the tweets by performing stemming and lemmatization using nltk\ndef preprocess_tweets(customer):\n    corpus = []\n    stem = PorterStemmer()\n    lem = WordNetLemmatizer()\n    for tweets in customer['text']:\n        words = [w for w in word_tokenize(tweets) if (w not in stopword)]\n        \n        words = [lem.lemmatize(w) for w in words if len(w)>2]\n        \n        corpus.append(words)\n    return corpus\n\ncorpus = preprocess_tweets(customer)","18d245cc":"# Creating bag of words model using gensim\ndic = gensim.corpora.Dictionary(corpus)\nbow_corpus = [dic.doc2bow(doc) for doc in corpus]","f8619672":"# Creating the LDA model\nlda_model = gensim.models.LdaMulticore(bow_corpus, \n                                   num_topics = 5, \n                                   id2word = dic,                                    \n                                   passes = 10,\n                                   workers = 2)\nlda_model.show_topics()","84a95634":"# Visualizing the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\nvis","972b0351":"### Distribution of character count of the tweets","01b5b11e":"This notebook is a work in progress. I will try to add different types of visualization techniques which can be done on textual data. Any suggestions or feedback is welcome.","a8c0cc28":"We can see here that most of the tweets contain 5 to 25 words while there are a few tweets having a word count aobve 25.","a2b3e14d":"### Distribution of stopwords","f2826027":"### Visualizing the first few rows of the data set","123a92a2":"### Distribution of word count for the tweets","fc0c9c25":"### Visualizing the number of total words used in the tweets by each account","2c0622a7":"We can see here that most of the tweets have a range of 50 to 150 characters, while very few tweets have 200 or more characters.","8516473f":"### Visualizing the mostly used trigrams","cc5c9fde":"### Most commonly used words in the tweets","523d1fa1":"### Removing stopwords from the tweets","fc6de954":"### Word count for the tweets","56d54f7f":"### Stopwords in the tweets","df2c404d":"In the field of NLP, one of the important tasks is to visually represent the textual content of a data set, it can be in the form of reviews, tweets, articles and documents. In this notebook, I have tried to visualize the textual data which might be helpful for analysis. ","00ddbf36":"### Visualizing the number of tweets by each Id","0d04663b":"### Visualizing the mostly used unigrams","332ed88f":"Most of the tweets have a range of 0 to 8 stopwords.","9e7d636d":"### Visualizing the different topics in the tweets","8b667199":"### Visualizing the topics","ba6cfe0c":"### Visualizing the mostly used bigrams","c4d9b0f8":"### Number of characters in the tweets","d220e0d5":"![](https:\/\/i.pinimg.com\/originals\/2e\/ae\/ea\/2eaeea861a76fcbe74ea535c8860f0a6.png)"}}