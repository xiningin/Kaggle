{"cell_type":{"c7914d8b":"code","dc3e8a9d":"code","2c36f68d":"code","112f160a":"code","8160583e":"code","4d2845d5":"code","e8770fe2":"code","74e3386b":"code","621737c6":"code","1d881964":"code","7f90d600":"code","91b80db7":"code","82615f15":"code","bb6a075f":"code","9cba23e6":"code","f6afdf2c":"markdown","e6b852a5":"markdown","f69e7899":"markdown","543de8ab":"markdown","380551c0":"markdown","72ef5d04":"markdown","397feac5":"markdown","53ee6199":"markdown","10b5807f":"markdown","0970b911":"markdown","f380bca5":"markdown","65f96c6d":"markdown","2fdafd61":"markdown"},"source":{"c7914d8b":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, scale\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, roc_curve, auc\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.feature_selection import SelectFromModel\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns; sns.set()\n%matplotlib inline","dc3e8a9d":"#Files\n!ls","2c36f68d":"#Eplore the files\ncancer_targets = pd.read_csv(\"..\/input\/gene-expression\/actual.csv\") #targets\ncancer_targets['patient'] = cancer_targets['patient'].astype('int')\n\nprint(cancer_targets[\"cancer\"].value_counts())\nprint(\"\\nNumber of samples;\", cancer_targets.shape)","112f160a":"cancer_train = pd.read_csv(\"..\/input\/gene-expression\/data_set_ALL_AML_train.csv\")\ncancer_test = pd.read_csv(\"..\/input\/gene-expression\/data_set_ALL_AML_independent.csv\")\n\nprint(\"Train shape:\",cancer_train.shape)\nprint(\"Test shape:\",cancer_test.shape)\n\n#There are 72 patients according to the target dataframe, but shapes are 78 and 70 in train and test?\ncancer_train.head(4)","8160583e":"def rename_columns(df):\n    \"\"\"Get's the correct patient ID for the call columns\"\"\"\n    for col in df.columns:\n        if \"call\" in col:\n            loc = df.columns.get_loc(col)\n            patient = df.columns[loc-1]\n            df.rename(columns={col: f'Call_{patient}'}, inplace=True)\n            \n            \nrename_columns(df=cancer_train)\nrename_columns(df=cancer_test)\n\n#check for duplicate columns\n#print(cancer_test.groupby([\"Gene Description\"]).size().value_counts(),\n#      cancer_train.groupby([\"Gene Description\"]).size().value_counts())\n\n#Gene description and Gene accesion should be kept together, otherwise there will be duplicates.\ncancer_train[\"Gene\"] = cancer_train[\"Gene Description\"] + '_' + cancer_train[\"Gene Accession Number\"]\ncancer_test[\"Gene\"] = cancer_test[\"Gene Description\"] + '_' +  cancer_test[\"Gene Accession Number\"]\n\n#Transpose the dataset and fix the columns + label train and test set with new column\ncancer_train = cancer_train.T\ncancer_train.columns = cancer_train.iloc[-1]\ncancer_train = cancer_train[2:-1]\ncancer_train['dataset'] = 'train'\n\ncancer_test = cancer_test.T\ncancer_test.columns = cancer_test.iloc[-1]\ncancer_test = cancer_test[2:-1]\ncancer_test['dataset'] = 'test'\n\n\ndf = pd.concat([cancer_train, cancer_test], axis=0,join='inner', sort=False)\ndf.shape","4d2845d5":"#Genes with only A calls are not of any use:\ncall_rows = [row for row in df.index if \"Call\" in row]\nconditional = df.filter(call_rows, axis=0).apply(lambda x: x == 'A', axis=1).all()\nprint(conditional.value_counts()) #True will be dropped.\ndf = df.loc[:, ~conditional]\n\n\n#Next we can delete the Call rows and add the cancer labels\n#del df.columns.name\ndf.drop(call_rows, axis = 0, inplace=True)\ndf['patient'] = df.index\ndf['patient'] = df['patient'].astype('int') #Have same dtype for the columns to merge on\ndf.reset_index(drop=True)\n#df = pd.concat([df, cancer_targets], axis=1, join='inner')\n\n#Merge\n\ndf = pd.merge(left=df, right=cancer_targets, left_on='patient', right_on='patient')\n\nprint(df.shape)\n\ndf.head(5)","e8770fe2":"#Assign train and test sets\ntrain = df[df['dataset'] == 'train'].iloc[:,1:-3]\ntrain_target = df[df['dataset'] == 'train'].iloc[:,-1]\ntest = df[df['dataset'] == 'test'].iloc[:,1:-3]\ntest_target = df[df['dataset'] == 'test'].iloc[:,-1]\n\nprint(train.shape, train_target.shape)\nprint(test.shape, test_target.shape)","74e3386b":"#Scaling\n#Initialize a scaler later to be used on test set as well.\nscaler = StandardScaler().fit(train)\ntrain_scaled = pd.DataFrame(scaler.transform(train), columns=train.columns)\ntest_scaled = pd.DataFrame(scaler.transform(test), columns=test.columns)\n\n#Awkward distribution!\nfig, ax = plt.subplots(ncols=2, figsize=(15,5))\nsns.distplot(np.concatenate(train.values), ax=ax[0])\nsns.distplot(np.concatenate(train_scaled.values), ax=ax[1])\nplt.tight_layout\nplt.show()","621737c6":"#Feature selection using SelectFromModel and Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\nprint(\"Original Shape:\", train_scaled.shape)\nlogistic_regression = LogisticRegression(penalty=\"l1\", solver='saga').fit(train_scaled, train_target) #l1 for sparsity\nlog_coefficients = logistic_regression.coef_\nselector_log = SelectFromModel(logistic_regression, prefit=True)\ntrain_scaled_logreg = selector_log.transform(train_scaled)\nprint(\"Features after selection using Logistic Regression:\", train_scaled_logreg.shape) \n\n\n#Alternatively\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nclf = ExtraTreesClassifier(n_estimators=200)\nclf = clf.fit(train_scaled, train_target) \nselector_tree = SelectFromModel(clf, prefit=True)\ntrain_scaled_tree = selector_tree.transform(train_scaled)\ntest_scaled_tree = selector_tree.transform(test_scaled)\nprint(\"Features after selection using ExtraTreesClassifier:\", train_scaled_tree.shape)\n\n\n\n#Absolute values for coefficients in Logistic Regression \"=~importance\"\nlog_coefficients_abs = abs(log_coefficients)\nlog_coefficients_abs_sort = np.sort(log_coefficients_abs).flatten()\nsortedidx = log_coefficients_abs.argsort()\nlog_labels = train_scaled.columns[sortedidx].flatten()\nsns.barplot(x=log_coefficients_abs_sort[-20:], y=log_labels[-20:]);","1d881964":"extratree_feature_importances_sort = np.sort(clf.feature_importances_ ).flatten()\nsortedidx = clf.feature_importances_.argsort()\ntree_labels = train_scaled.columns[sortedidx].values.flatten()\nsns.barplot(x=extratree_feature_importances_sort[-20:], y=tree_labels[-20:]);","7f90d600":"[gene for gene in tree_labels[-20:] if gene in log_labels[-20:]]","91b80db7":"#Let's do a pca first \npca = PCA(n_components=3)\npca.fit_transform(train_scaled_tree)\nprint(pca.explained_variance_ratio_) # Small variance explained\n\nPCA_df = pd.DataFrame(data = pca.fit_transform(train_scaled_tree), \n                           columns = ['pc1', 'pc2', 'pc3'])","82615f15":"PCA_df = pd.concat([PCA_df, train_target], axis = 1)\n\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111, projection='3d')\ncolors = {'ALL':'red', 'AML':'blue'}\nax.scatter(PCA_df.pc1, PCA_df.pc2, PCA_df.pc3, \n           c=train_target.apply(lambda x: colors[x]))\nplt.title('First 3 Principal Components after PCA')\nax.set_xlabel('PC1')\nax.set_ylabel('PC2')\nax.set_zlabel('PC3')\nax.view_init(20, 80)\nplt.tight_layout\nplt.show()","bb6a075f":"# Model Selection\nclassifiers = [LogisticRegression(),\n               KNeighborsClassifier(n_neighbors=5),\n               SVC(probability=True),\n               RandomForestClassifier(n_estimators=100, min_samples_leaf=4),\n               MLPClassifier(hidden_layer_sizes=250),\n               GaussianNB()\n              ]\nnames = ['Logistic Regression',\n         'K Nearest Neighbours', \n         'Support Vector Machine', \n         'Random Forest', \n         'Multi Layer Perceptron', \n         'Gaussian Naive Bayes'\n        ]\n\nROC_results = dict()\nPR_results = dict()\n\nfor name, classifier in zip(names,classifiers):\n    \n    #fit\n    clf = classifier.fit(train_scaled_tree, train_target)\n    \n    #predictions on test set\n    pred_targets = clf.predict(test_scaled_tree)\n    probas = clf.predict_proba(test_scaled_tree)[:,1]\n    \n    #ROC and AUC\n    tpr, fpr, _ = roc_curve(test_target, probas, pos_label='AML')\n    ROCAUC = roc_auc_score(test_target, probas)\n    ROC_results[name] = (tpr, fpr, ROCAUC)\n    \n    #PR\n    precision, recall, _ = precision_recall_curve(test_target, probas, pos_label='AML')\n    AUCPR = auc(recall, precision)\n    PR_results[name] = (precision, recall, AUCPR)\n    \n    #print results\n    print(f'\\nResults for {name}:\\n')\n    print(classification_report(y_true=test_target, y_pred=pred_targets))\n    print(confusion_matrix(y_true=test_target, y_pred=pred_targets))","9cba23e6":"fig, (roc,pr) = plt.subplots(nrows=1, ncols=2, sharex=True, figsize=(15, 7))\n\n#ROC\nfor clf in ROC_results.keys():\n    roc.plot(ROC_results[clf][0], ROC_results[clf][1], label=f'{clf} (ROCAUC = {round(ROC_results[clf][2],3)})');\n\nroc.set_xlabel('False Positive Rate')\nroc.set_ylabel('True Positive Rate')\nroc.set_title('ROC curve')\nroc.legend();\n\n#PR\nfor clf in PR_results.keys():\n    pr.plot(PR_results[clf][1], PR_results[clf][0], label=f'{clf} (AUC = {round(PR_results[clf][2],3)})')\n\npr.set_xlabel('Recall')\npr.set_ylabel('Precision')\npr.set_title('PR curve')\npr.legend();\n\n","f6afdf2c":"- acute myeloid leukemia (AML)\n- lymphoblastic leukemia (ALL)","e6b852a5":"### MERGE TRAIN AND TEST DATASETS\n\nI decided to merge the train and test csv into one big dataframe...","f69e7899":"The call columns contain: Present, Absent, and Marginal. This is a detection call by the manufacturer of the DNA Microarray used in the paper. \n(These values are related to p-values of intensity calls compared to a cutoff noise frequency it seems...). Therefore it seems best to exclude rows where all values are Absent(A) calls, i.e. untrustworthy calls. As a matter of fact it maybe better to completely zero out all A calls. We will see if this is appropriate later. This article has analysed the effect of filtering: https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC1409797\/","543de8ab":"The above plot shows relatively  separation of the two cancer types along the first 3 components as defined by PCA using. Even using only PC1 and PC2 would already separate them.","380551c0":"Using sklearn's SelectFromModel with a logistic regression and L1 norm, only 1254 genes are considered relevant to classify the two cancer types (i.e. their coefficients did not converge to zero). Using the ExtraTreesClassifier classifier on the other hand results in only 455 genes (this number is random everytime since the decision trees of the classifier select random subsets) that are necessary to classifiy the cancers.\n\nAmong the 20 highest ranked genes in both methods, around 8 are the same (changes every run):","72ef5d04":"# AUROC\/PR CURVES","397feac5":"### CLEAN UP","53ee6199":"# IMPORTS","10b5807f":"# PREPROCESSING AND FEATURE SELECTION","0970b911":"# READ ME\n\nA notebook using the kaggle microarray dataset : https:\/\/www.kaggle.com\/crawford\/gene-expression to classify two types of cancer. \n\nNOTE :There are most likely microarray specific processing approaches (For example in R's Bioconductor), but I will consider this dataset as a numerical feature matrix.  ","f380bca5":"# MODEL SELECTION","65f96c6d":"# PREPARE THE DATASET","2fdafd61":"# PRINCIPAL COMPONENT ANALYSIS\n### Using the dataset retrieved from the feature selection method using the ExtraTreesClassifier"}}