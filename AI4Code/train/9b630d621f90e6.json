{"cell_type":{"8e469236":"code","529d4247":"code","3e4a00ee":"code","a648f9c6":"code","78ac8651":"code","37983351":"code","b14a2228":"code","d610c257":"code","fb8685eb":"code","fb7ce30d":"code","2c0a0810":"code","b19de946":"code","1a0c5312":"code","682fc8e5":"code","6a89db12":"code","554fdba4":"code","752400b9":"code","876b5dd3":"code","2adb9e18":"code","a28b953f":"code","099dd216":"code","7f032c3e":"code","00c16da8":"code","b96debe4":"code","297ceb97":"code","fa90a781":"code","ba1822ee":"code","c69e40f3":"code","795ac440":"code","a6221c06":"code","6586872d":"code","a7f3a1d2":"code","6034ebbf":"code","219b5f39":"code","c747bc0f":"markdown","55460773":"markdown","3925fb9e":"markdown","a5a709b7":"markdown","578b3756":"markdown","b88aa540":"markdown","ba0c7952":"markdown","1807321e":"markdown","db24cc24":"markdown","034d56e6":"markdown","35bdb34a":"markdown","2c16aabc":"markdown","eb391728":"markdown","46b772ce":"markdown","0a69dc8e":"markdown","0efe5c0c":"markdown","13503278":"markdown","19b575f4":"markdown","06faec71":"markdown","b9ed40e8":"markdown","1a7e6ee7":"markdown","f2ddd27c":"markdown","d93a40a6":"markdown","f6721732":"markdown","5d1fc775":"markdown","d2c35448":"markdown","80c4bea7":"markdown","51d6495d":"markdown","ff492f77":"markdown","8e6425b1":"markdown","d7035869":"markdown","b9cf577d":"markdown"},"source":{"8e469236":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport missingno as msno \nimport scipy\nfrom scipy.sparse import hstack\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","529d4247":"data = pd.read_csv('\/kaggle\/input\/bu-cs506-spring-2020-midterm\/train.csv')","3e4a00ee":"data.head()","a648f9c6":"plt.title('10 Most Active Users')\ndata['UserId'].value_counts(sort=True).nlargest(10).plot.bar()","78ac8651":"print('There are', str(data.shape[0]), 'records in total.')","37983351":"plt.title('10 Most Rated Products')\ndata['ProductId'].value_counts(sort=True).nlargest(10).plot.bar()","b14a2228":"HelpfulnessNumerator0 = data[data['HelpfulnessNumerator'] == 0]['HelpfulnessNumerator'].value_counts()\nHelpfulnessNumerator1 = data[data['HelpfulnessNumerator'] == 1]['HelpfulnessNumerator'].value_counts()\nHelpfulnessNumerator2 = data[data['HelpfulnessNumerator'] == 2]['HelpfulnessNumerator'].value_counts()\nHelpfulnessNumerator3 = data[data['HelpfulnessNumerator'] == 3]['HelpfulnessNumerator'].value_counts()\nHelpfulnessNumeratorMoreThan3 = data[data['HelpfulnessNumerator'] > 3]['HelpfulnessNumerator'].value_counts()\n\nlabels = '0', '1', '2', '3', 'more than 3'\nsizes = [HelpfulnessNumerator0.values.item(), HelpfulnessNumerator1.values.item(), HelpfulnessNumerator2.values.item(), \n         HelpfulnessNumerator3.values.item(), HelpfulnessNumeratorMoreThan3.values.sum()]\nexplode = (0, 0, 0, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig, ax = plt.subplots()\nax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(' Portions of Amount of Helpfulness Labels')\nplt.show()","d610c257":"HelpfulnessDenominator0 = data[data['HelpfulnessDenominator'] == 0]['HelpfulnessDenominator'].value_counts()\nHelpfulnessDenominator1 = data[data['HelpfulnessDenominator'] == 1]['HelpfulnessDenominator'].value_counts()\nHelpfulnessDenominator2 = data[data['HelpfulnessDenominator'] == 2]['HelpfulnessDenominator'].value_counts()\nHelpfulnessDenominator3 = data[data['HelpfulnessDenominator'] == 3]['HelpfulnessDenominator'].value_counts()\nHelpfulnessDenominatorMoreThan3 = data[data['HelpfulnessDenominator'] > 3]['HelpfulnessDenominator'].value_counts()\n\nlabels = '0', '1', '2', '3', 'more than 3'\nsizes = [HelpfulnessDenominator0.values.item(), HelpfulnessDenominator1.values.item(), HelpfulnessDenominator2.values.item(), \n         HelpfulnessDenominator3.values.item(), HelpfulnessDenominatorMoreThan3.values.sum()]\nexplode = (0, 0, 0, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig, ax = plt.subplots()\nax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(' Portions of Amount of Comments Watched')\nplt.show()","fb8685eb":"plt.title('Scores')\ndata['Score'].value_counts().plot.bar()","fb7ce30d":"fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(121)\ntext = data.Summary.values\nwordcloud = WordCloud(\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nplt.title('Summary Keywords')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\n\nax = fig.add_subplot(122)\ntext = data.Text.values\nwordcloud = WordCloud(\n    background_color = 'white',\n    stopwords = STOPWORDS).generate(str(text))\nplt.title('Text Keywords')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","2c0a0810":"msno.matrix(data) ","b19de946":"msno.heatmap(data) ","1a0c5312":"# fives = data.loc[data['Score'] == 5]\n# fives = fives.sample(frac=0.5)\n# data = pd.concat([data.loc[data['Score'] != 5], fives])","682fc8e5":"OHE = OneHotEncoder(sparse=True)\nID_fitter = OHE.fit(data[['ProductId', 'UserId']])\nIDs = ID_fitter.transform(data[['ProductId', 'UserId']])","6a89db12":"# # collect the model to reuse later\n# from joblib import dump, load\n# dump(ID_fitter, 'OHE.joblib') ","554fdba4":"data['Text'].loc[data['Text'].isna()] = ''\ndata['Summary'].loc[data['Summary'].isna()] = ''","752400b9":"data['Helpful'] = data['HelpfulnessNumerator']\ndata['Unhelpful'] = data['HelpfulnessDenominator'] - data['HelpfulnessNumerator']\nscaler = StandardScaler()\nscalerFitter = scaler.fit(data[['Helpful', 'Unhelpful', 'Time']])\ndata[['Helpful', 'Unhelpful', 'Time']] = scalerFitter.transform(data[['Helpful', 'Unhelpful', 'Time']])\ndata = data.drop(['HelpfulnessDenominator','HelpfulnessNumerator'], axis=1)","876b5dd3":"# dump(scalerFitter, 'scaler.joblib')","2adb9e18":"text_vectorizer = TfidfVectorizer(input='content', analyzer='word', stop_words='english')\nsummary_vectorizer = TfidfVectorizer(input='content', analyzer='word', stop_words='english')\ntext_fitter = text_vectorizer.fit(data['Text'])\ntext_matrix = text_fitter.transform(data['Text'])\nsummary_fitter = summary_vectorizer.fit(data['Summary'])\nsummary_matrix = summary_fitter.transform(data['Summary'])","a28b953f":"# dump(text_fitter, 'text.joblib')\n# dump(summary_fitter, 'summary.joblib')","099dd216":"text_matrix, summary_matrix","7f032c3e":"numerical = scipy.sparse.csr_matrix(data[['Helpful', 'Unhelpful', 'Time']].values)","00c16da8":"X = hstack([text_matrix, summary_matrix, numerical, IDs])","b96debe4":"mask = data[\"Score\"].isnull()\n\nind_test = mask.to_numpy().nonzero()[0]\nind_train = (~ mask).to_numpy().nonzero()[0]\n\ntrain_X = scipy.sparse.csr_matrix(X)[ind_train]\ntest_X = scipy.sparse.csr_matrix(X)[ind_test]","297ceb97":"# plt.spy(train_X)","fa90a781":"train_Y = data['Score'].loc[data['Score'].isna() == False]\ntest_Y = data['Score'].loc[data['Score'].isna()]\n\ntrain_Y = train_Y.reset_index()['Score']\ntest_Y = test_Y.reset_index()['Score']","ba1822ee":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=42)\ntrain_X, train_Y = ros.fit_resample(train_X, train_Y)","c69e40f3":"plt.title('Scores')\ntrain_Y.value_counts().plot.bar()","795ac440":"X_train, X_valid, y_train, y_valid = train_test_split(train_X, train_Y, test_size=0.2, random_state=42)","a6221c06":"def CVKFold(k, X, y, model):\n    np.random.seed(1)\n    #reproducibility\n    \n    highest_accuracy = float('inf')\n    best_model = None\n\n    kf = KFold(n_splits = k,shuffle =True)\n    #CV loop\n    \n    for train_index,test_index in kf.split(X):#generation of the sets\n    #generate the sets    \n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        #model fitting\n        model.fit(X_train,y_train)\n        y_test_pred = model.predict(X_test)\n    \n        test_accuracy = mean_squared_error(y_test_pred, y_test)\n        print(\"The accuracy is \" + str(test_accuracy))\n        if test_accuracy < highest_accuracy:\n          best_model = model\n          highest_accuracy = test_accuracy\n\n    print(\"The highest accuracy is \" + str(highest_accuracy))\n    return best_model, highest_accuracy","6586872d":"# model = LogisticRegression(random_state = 0)\n# model = model.fit(train_X, train_Y)\n# dump(model, 'model.joblib')","a7f3a1d2":"# Logistics Regression\nmodel = LogisticRegression(random_state = 0)\nmodel = model.fit(train_X, train_Y)\n# clf_Log, accuracy_Log = CVKFold(5, train_X, train_Y, model)\n# Decision Tree\n# model = DecisionTreeClassifier(random_state = 0, max_depth=20)\n# clf_DTree, accuracy_DTree = CVKFold(5, train_X, train_Y, model)\n# # Random Forest\n# model = RandomForestClassifier(random_state = 0, max_depth=20)\n# clf_RF, accuracy_RF = CVKFold(5, train_X, train_Y, model)","6034ebbf":"# accuracies = {accuracy_Log: clf_Log, accuracy_DTree: clf_DTree, accuracy_RF: clf_RF}\n# clf = accuracies[min([accuracy_Log, accuracy_DTree, accuracy_RF])]","219b5f39":"sample = pd.read_csv('\/kaggle\/input\/bu-cs506-spring-2020-midterm\/sample.csv')\npredict_df = pd.DataFrame(sample)\n\npredict_df['Score'] = model.predict(test_X)\npredict_df.to_csv(r'sample.csv',index=False)","c747bc0f":"#### output","55460773":"## Standardize the numerical features\n-----\nStandardization helps to reduce the influence of outliers and converge faster.","3925fb9e":"## Import libraries","a5a709b7":"### HelpfulnessDenominator","578b3756":"# **Amazon Movie Rating Prediction**\n-------\nThis project examines the realtionship between the star rating score between user reviews from Amazon Movie Reviews using the available features. The features include unique identifier for the product\/user, the number of users who found the review helpful, the number of users who indicated whether they found the review helpful, the timestamp for the review, the brief summary of the review and the text of the review. In the rest of this notebook, I'm going to utilize these features on the prediction of the star rating score. This project, if successful, is beneficial to estimate any unlisted movie's popularity or reputation, which could be further used in the recommendation system in this field. For any further detail about this competition, please refer [here](http:\/\/www.kaggle.com\/c\/bu-cs506-spring-2020-midterm).","b88aa540":"## Import train data","ba0c7952":"### Users","1807321e":"Commenters incline to rate a movie 5.0\/5.0, so unlike previous features, a rating of 5.0\/5.0 might cause a side effect when implementing machine learning that the classifier tends to predict an un labeled movie 5\/0\/5.0. I will avoid this happen in the following chapters.","db24cc24":"### Score","034d56e6":"## Calculate TF-IDF and Vectorize\n-----\nThis is the baseline step. By vectorizing the text values, we can access to more information about this dataset so that a higher accuracy is obtained.","35bdb34a":"## EDA\n-------\nI am going to discover some significant properties among the data by carrying out an exploratory data analysis.","2c16aabc":"### Summary and Text\n-----\nIn this step I plotted out the word cloud images of both \"Summary\" and \"Text\", in search of any valuable information.","eb391728":"There is not too much overlapping between the two word clouds, basically because summary is more consise than its text and so that it contains less information. However, since they have few in common, it is save to asusme there does not exist a strong correlation between the two. Thus I am going to use both of them as my factors of the predictor.","46b772ce":"### HelpfulnessNumerator","0a69dc8e":"# Data Preprocessing","0efe5c0c":"## predict the output results on the test set\n","13503278":"### Separate a ratio of train set for the use of validation","19b575f4":"Except \"Score\" (our projection of this competition which contains test data), it seems that there almost no missing values insides other properties. According to the following heatmap, the missing values in other columns only exist in the property \"Summary\" and \"Text\", while neither of each is strongly correlated to one another. This makes the problem simple because I can just apply a simple imputation on them without considering other situations.","06faec71":"### split out the train set and test set\n\n#### input columns","b9ed40e8":"This is a visualization of my sparse matrix. However, even if it is called \"Sparse\", it is not sparse at all from the perspective of maths because the elements within the matrix are barely zero.","1a7e6ee7":"## Concatenate all the features\n\n### make dense series to a sparse matrix","f2ddd27c":"## One-hot Encoding to Categorical Features\n------\nEncoding is a method to digitalize the categorical features. The reason I chose one-hot encoding over label encoding is due to the fact that label encoding labels each unique identifier a different on a basis of \"first appear, first encode\". In other words, the identifier which appears earlier gets a smaller value. This would conflict with the fact that identifier cannot be measureable feature. The only disvantage of one-hot encoding, on the other hand, is that it consumes too many memories. I will show how to handle this situation.","d93a40a6":"## Impute the missing values by making blanks as void strings\n-----\nThe easiest imputation.","f6721732":"## Missing Value Detection\n-----\nAs the overview suggests, there are missing values inside our data set. So it is necessary to research their properties and make sure no distortions caused by them.","5d1fc775":"## Implement the classifier\n---\nI chose Logistics regression as my model because of its efficiency and am going to check its accuracy with K-fold cross validation","d2c35448":"I found the most active user among the dataset has no more than 3000 records in it. Compared to the total amount of over 1 million records, this is not a very significant portion. Therefore we have to worry about whether a certain user is able to make too big an influence to the entire prediction. This is the same in terms of the column \"ProductId\" because its most frequently seen entry has no more than 3000 records either.","80c4bea7":"In \"HelpfulnessNumerator\" and \"HelpfulnessDenominator\", none of a single value makes up more than 50%, so I can safely put that none of the value is too overwhelming to affect the performance of the predictor.","51d6495d":"## Random Oversampling","ff492f77":"### Products","8e6425b1":"The original data set contains 9 columns, and column \"Id\" serves as the unique identifier of each row entry which in other words, is irrelevant to the modeling.","d7035869":"## Downsize the data whose score is 5.0\/5.0\n------\nLike I showed before, there are too many score of 5.0\/5.0 insides. To avoid side effect of it (a predictor tends to set the score of an unknown record as the most commonly seen one), I am going to cut off the scale of the dataset whose score is 5.0\/","b9cf577d":"### stack all the sparse matrices above acording to their row indices\n----\nSparse matrix helps solve the memory consuming trouble of the one-hot encoding."}}