{"cell_type":{"1198c518":"code","f49d4a9b":"code","3fb700a1":"code","f2d1faf8":"code","aef08c01":"code","9c092a6f":"code","03ea081d":"code","d203c361":"code","4e2039a9":"code","8aa20162":"code","5d72f863":"code","f0cfca82":"code","eb84422d":"code","50302958":"code","4aa0755f":"code","9c65b132":"code","6e19e836":"code","d58efecb":"code","840ace67":"code","e7f2a426":"code","6bb169d9":"code","578208f5":"code","f66b7560":"code","6df3701b":"code","d90edc28":"code","acdc2a48":"code","3ddd9407":"code","b8da4f67":"code","b48ccc1b":"code","a0bd597c":"code","7e0e4ae3":"code","1f8e6a52":"code","12093ba8":"code","b54a506c":"code","119cf457":"code","2eea8f48":"code","b55ecc8a":"code","b3e2768a":"code","c2cdcfbd":"code","cf3e33b1":"code","5abfc57d":"code","143c86cd":"code","2f94401b":"code","3a1d71a6":"code","c2435b72":"code","d7712dd7":"code","937ef3c3":"code","52038c99":"code","2e4b7b90":"code","1c13b101":"code","da5135e3":"code","325d735e":"code","a3b19d50":"markdown","aed0d07d":"markdown","4d1528fc":"markdown","467eba5e":"markdown","9d0f7351":"markdown","fa865425":"markdown","1f0a8714":"markdown","6bffcac6":"markdown","9199a573":"markdown","26f15634":"markdown","a3c8598c":"markdown","a1f26466":"markdown","0f53f1f0":"markdown","51aa6b65":"markdown","402c265b":"markdown","ca257831":"markdown","3cb6b566":"markdown","abab6e90":"markdown","76056f38":"markdown","32fd9983":"markdown","a7ee8490":"markdown","f3fdd197":"markdown","f16b733b":"markdown","2e6b0ba8":"markdown","1c3ce697":"markdown","3e03ac4d":"markdown","a5effc17":"markdown","933af422":"markdown","e9fb82a1":"markdown","f6bc757b":"markdown","c8ce9a58":"markdown","f0677fd4":"markdown","51a795b9":"markdown","fc454e95":"markdown","0ca8fb5b":"markdown","5916f46c":"markdown","c1a70d5b":"markdown","e896b8ec":"markdown","ea85f273":"markdown","086d0a69":"markdown","c2c4cb7b":"markdown","d8500066":"markdown","4ae913cc":"markdown","e7e9c7e7":"markdown","b2c093db":"markdown","9a4ab140":"markdown","bcab25c3":"markdown"},"source":{"1198c518":"!pip install ftfy\n!pip install gensim\n!pip install nltk","f49d4a9b":"!pip install skorch\n!pip install scikit-learn==0.23.0\n\nfrom numpy.ma import MaskedArray\nimport sklearn.utils.fixes\n\nsklearn.utils.fixes.MaskedArray = MaskedArray","3fb700a1":"#Para o uso geral\nimport random\nimport numpy as np\nimport pandas as pd\nimport copy \nimport time\nfrom scipy.stats import uniform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport requests\nimport io\n\n#Para o processamento de textos\nfrom ftfy import fix_text\nimport string\nimport re\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\n#Para Machine Learning e NLP\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score","f2d1faf8":"treino=pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_train.csv\")","aef08c01":"treino.head(15)","9c092a6f":"treino.shape","03ea081d":"total = treino.isnull().sum().sort_values(ascending = False)\npercentual = ((treino.isnull().sum()\/treino.isnull().count())*100).sort_values(ascending = False)\nmissing_data = pd.concat([total, percentual], axis = 1, keys = ['Total', '%'])\nmissing_data.head()","d203c361":"treino.loc[treino.duplicated(subset='review', keep=False)==True].sort_values(by='review').head(14)","4e2039a9":"treino=treino.drop_duplicates(subset='review', keep='first')\ntreino.shape","8aa20162":"print(\"Podemos ver que apenas das reviews\", (treino.shape[0]*100\/x),\"% ser\u00e3o \u00fateis\")","5d72f863":"treino.loc[:,'positive'].value_counts()","f0cfca82":"X_train = treino['review']\nY_train = treino['positive']","eb84422d":"X_train[0]","50302958":"Y_train[0]","4aa0755f":"def clean(text):\n    txt=text.replace(\"<br \/>\",\" \") #retirando tags\n    txt=fix_text(txt) #consertando Mojibakes\n    txt=txt.lower() #passando tudo para min\u00fasculo\n    txt=txt.translate(str.maketrans('', '', string.punctuation)) #retirando toda pontua\u00e7\u00e3o\n    txt=txt.replace(\" \u2014 \", \" \") #retirando h\u00edfens\n    txt=re.sub(\"\\d+\", ' <number> ', txt) #colocando um token especial para os n\u00fameros\n    txt=re.sub(' +', ' ', txt) #deletando espa\u00e7os extras\n    return txt\n\n    ","9c65b132":"X_train = X_train.apply(clean)","6e19e836":"X_train = X_train.apply(lambda x: x.split())","d58efecb":"X_train.head()","840ace67":"d2v = Doc2Vec.load(\"..\/input\/sentiment-analysis-pmr3508\/doc2vec\") ","e7f2a426":"def emb(txt, model, normalize=False): \n    model.random.seed(42)\n    x=model.infer_vector(txt, steps=20)\n    \n    if normalize: return(x\/np.sqrt(x@x))\n    else: return(x)","6bb169d9":"X_train = X_train.to_list()\nX_train = [emb(x, d2v) for x in X_train] \nX_train = np.array(X_train)\nX_train.shape","578208f5":"X_train.shape, Y_train.shape","f66b7560":"valid = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test1.csv\")","6df3701b":"X_valid = valid['review'].tolist()\nY_valid = valid['positive']","d90edc28":"X_valid = [clean(x).split() for x in X_valid]\nX_valid = [emb(x, d2v) for x in X_valid] \nX_valid = np.array(X_valid)","acdc2a48":"# Modelo de Regress\u00e3o Log\u00edstica\nlogreg = LogisticRegression(solver='liblinear',random_state=42)\n\nhyperparams = dict(C=np.linspace(0,10,100), \n                     penalty=['l2', 'l1'])\n# Busca de Hiperpar\u00e2metros\nlogreg_clf = RandomizedSearchCV(logreg, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_logreg = logreg_clf.fit(X_train,Y_train)","3ddd9407":"search_logreg.best_params_, search_logreg.best_score_","b8da4f67":"logreg = LogisticRegression(C=search_logreg.best_params_['C'], \n                            penalty=search_logreg.best_params_['penalty'],\n                            solver='liblinear', random_state=42)\n\nlogreg.fit(X_train,Y_train)","b48ccc1b":"from scipy.stats import loguniform as sp_loguniform","a0bd597c":"# Modelo de Rede Neural de 1 camada escondida\nmlp_1l = MLPClassifier(random_state=42, early_stopping=True)\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'hidden_layer_sizes': [(2 ** i) for i in np.arange(6,12)],\n               'alpha': sp_loguniform(0.000001, 0.1),\n               'learning_rate': ['constant','adaptive']}\n\n# Busca de Hiperpar\u00e2metros\nmlp_clf_1l = RandomizedSearchCV(mlp_1l, hyperparams, scoring='roc_auc', n_iter=25, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_mlp = mlp_clf_1l.fit(X_train,Y_train)","7e0e4ae3":"search_mlp.best_params_, search_mlp.best_score_","1f8e6a52":"mlp_1_auc=search_mlp.best_score_","12093ba8":"# Modelo de Rede Neural de 2 camadas escondidas\nmlp_2l = MLPClassifier(random_state=42, early_stopping=True)\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'hidden_layer_sizes': [(2 ** i, 2 ** j) for j in np.arange(6,10) for i in np.arange(6,10)],\n               'alpha': sp_loguniform(0.000001, 0.1),\n               'learning_rate': ['constant','adaptive']}\n\n# Busca de Hiperpar\u00e2metros\nmlp_clf_2l = RandomizedSearchCV(mlp_2l, hyperparams, scoring='roc_auc', n_iter=25, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_mlp = mlp_clf_2l.fit(X_train,Y_train)","b54a506c":"search_mlp.best_params_, search_mlp.best_score_","119cf457":"mlp_2_auc=search_mlp.best_score_","2eea8f48":"import torch.nn as nn\nimport torch.nn.functional as F\n\n# Nossa rede neural\nclass MLPNet(nn.Module):\n    def __init__(self, hidden1_dim=512, hidden2_dim=256, p=0.25):\n        super().__init__()\n        self.fc1 = nn.Linear(50, hidden1_dim)           # Primeira camada escondida de tamanho 'hidden1_dim'\n        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)  # Segunda camada escondida de tamanho 'hidden2_dim'\n        self.fc3 = nn.Linear(hidden2_dim, 2)            # Camada de output de tamanho 2 (para as duas classes de target)\n        self.dropout = nn.Dropout(p)                 # Camada de Dropout para regulariza\u00e7\u00e3o\n        \n    def forward(self, X, **kwargs):\n        fc_out = F.relu(self.fc1(X))                    # Aplicando a primeira camada escondida\n        fc_out = self.dropout(fc_out)                   # Aplicando o Dropout\n        \n        fc_out = F.relu(self.fc2(fc_out))               # Aplicando a segunda camada escondida\n        fc_out = self.dropout(fc_out)                   # Aplicando o Dropout\n        \n        fc_out = self.fc3(fc_out)                       # Aplicando a camada de output\n        soft_out = F.softmax(fc_out, dim=-1)            # Obtendo as probabilidades de cada classe com um softmax\n        \n        return soft_out","b55ecc8a":"import torch\n\n# Instanciando a nossa rede\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmlp_net = MLPNet().to(device)","b3e2768a":"import skorch\nfrom torch import optim\nfrom skorch import NeuralNetClassifier\n\nskorch_net = NeuralNetClassifier(mlp_net,\n                                 max_epochs=20,\n                                 lr=1e-4,\n                                 optimizer=optim.Adam,\n                                 optimizer__weight_decay=1e-4,\n                                 train_split=False,\n                                 verbose=0,\n                                 iterator_train__shuffle=True,\n                                 )\n","c2cdcfbd":"from skopt import BayesSearchCV\nfrom skopt.space import Real, Integer\n\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'module__hidden1_dim': Integer(256, 2048),\n               'module__hidden2_dim': Integer(64, 1024),\n               'module__p': Real(0.1, 0.75, prior='uniform'),\n               'optimizer__weight_decay': Real(1e-10, 1e-2, prior='log-uniform')}\n\n# Busca de Hiperpar\u00e2metros\nskorch_clf = BayesSearchCV(skorch_net, hyperparams, scoring='roc_auc', n_iter=30, cv=2, n_jobs=-1, random_state=42, verbose=0)\nsearch_skorch = skorch_clf.fit(X_train.astype(np.float32), Y_train.astype(np.int64))","cf3e33b1":"search_skorch.best_params_, search_skorch.best_score_","5abfc57d":"pt_nn_2_auc=search_skorch.best_score_","143c86cd":"scores = [[\"Rede Neural com 1 Camada Oculta (Scikit-Learn)\", mlp_1_auc], \n          [\"Rede Neural com 2 Camadas Ocultas (Scikit-Learn)\", mlp_2_auc],\n          [\"Rede Neural com 2 Camadas Ocultas (Pytorch)\", pt_nn_2_auc]]\n\nscores_df = pd.DataFrame(scores, columns=[\"Modelo\", \"AUC\"])\nscores_df","2f94401b":"from sklearn.metrics import roc_auc_score\n\n# Calculando a AUC da regress\u00e3o log\u00edstica\nlogreg_roc_auc = roc_auc_score(Y_valid, logreg_clf.predict_proba(X_valid)[:,1])\n\nprint('AUCs --- Log. Reg.: {:.4f}'.format(logreg_roc_auc))","3a1d71a6":"from sklearn.metrics import roc_auc_score\n\n# Calculando a AUC da rede neural de 1 camada do sklearn\nmlp_1l_roc_auc = roc_auc_score(Y_valid, mlp_clf_1l.predict_proba(X_valid)[:,1])\n\nprint('AUCs --- MLP (1 camada): {:.4f}'.format(mlp_1l_roc_auc))","c2435b72":"from sklearn.metrics import roc_auc_score\n\n# Calculando a AUC da rede neural de 2 camadas do sklearn\nmlp_2l_roc_auc = roc_auc_score(Y_valid, mlp_clf_2l.predict_proba(X_valid)[:,1])\n\nprint('AUCs --- MLP (2 camadas): {:.4f}'.format(mlp_2l_roc_auc))","d7712dd7":"from sklearn.metrics import roc_auc_score\n\n# Calculando a AUC da rede neural do Skorch\nskorch_roc_auc = roc_auc_score(Y_valid, search_skorch.predict_proba(X_valid)[:,1])\n\nprint('AUCs --- MLP (Skorch): {:.4f}'.format(skorch_roc_auc))","937ef3c3":"test = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test2_X.csv\")","52038c99":"test.head()","2e4b7b90":"X_test = test['review'].tolist()\nX_test = [clean(x).split() for x in X_test]\nX_test = [emb(x, d2v) for x in X_test] \nX_test = np.array(X_test)","1c13b101":"prediction = search_skorch.predict_proba(X_test)[:,1]","da5135e3":"output = {'positive': prediction}\noutput = pd.DataFrame(output)\n\noutput.shape\noutput.head()","325d735e":"output.to_csv(\"submission.csv\", index = True, index_label = 'Id')","a3b19d50":"\"Um neur\u00f4nio de uma rede neural \u00e9 um componente que calcula a soma ponderada de v\u00e1rios inputs, aplica uma fun\u00e7\u00e3o e passa o resultado adiante. Quando utilizamos v\u00e1rios neur\u00f4nios em paralelo temos uma rede neural. N\u00f3s podemos pensar em cada neur\u00f4nio como recebendo sinais das vari\u00e1veis dos inputs e passando adiante uma vers\u00e3o ponderada e tratada desse sinal. Esses neur\u00f4nios em paralelo formam uma **camada oculta da rede neural**. N\u00f3s podemos tratar o output de cada neur\u00f4nio como uma vari\u00e1vel do input de uma outra camada oculta. Assim, podemos empilhar camadas ocultas e produzir uma rede neural profunda.\"\n\n[Introdu\u00e7\u00e3o \u00e0s Redes Neurais Artificiais](https:\/\/matheusfacure.github.io\/2017\/03\/05\/ann-intro\/)","aed0d07d":"# 3. Limpeza do c\u00f3digo \ud83e\uddf9\ud83e\uddec","4d1528fc":"# Compara\u00e7\u00e3o \ud83d\udc41\u200d\ud83d\udde8","467eba5e":"Como h\u00e1 a presen\u00e7a de reviews repetidas e isso n\u00e3o ajudar\u00e1 na nossa an\u00e1lise, removeremos estes dados:","9d0f7351":"Outra contendo a indica\u00e7\u00e3o de que a cr\u00edtica \u00e9 positiva ou n\u00e3o:","fa865425":"#PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n# An\u00e1lise de Sentimentos com Redes Neurais \ud83e\udde0\ud83d\udc97 \n\nAluno: J\u00falia Mello de Almeida\n\n---\n\n","1f0a8714":"# 6. Os melhores hiperpar\u00e2metros","6bffcac6":"$Resultados$\n\nE a seguir uma tabela com os resultados obtidos com redes neurais:","9199a573":"E podemos conferir como ficou:","26f15634":"Esse notebook visa o uso de Redes Neurais para a classifica\u00e7\u00e3o de textos de acordo com o sentimento que cada um transmite (positivo ou negativo)","a3c8598c":"Tamb\u00e9m nos ser\u00e1 \u00fatil ver a distribui\u00e7\u00e3o de avalia\u00e7\u00f5es positive==0 e positive==1","a1f26466":"# 5. Utilizando Doc2Vec e modelos supervisionados para An\u00e1lise de Sentimentos","0f53f1f0":"Podemos ver que cada entrada do nosso X \u00e9 uma representa\u00e7\u00e3o do nosso texto em um vetor de 50 n\u00fameros","51aa6b65":"Utilizaremos agora a regress\u00e3o log\u00edstica, que \u00e9 uma t\u00e9cnica estat\u00edstica que tem como objetivo produzir um modelo que permita a predi\u00e7\u00e3o de valores tomados por uma vari\u00e1vel categ\u00f3rica, frequentemente bin\u00e1ria, a partir de uma s\u00e9rie de vari\u00e1veis.\n\nUsaremos como recurso o Randomized Search para escolher os melhores hiperpar\u00e2metros para o nosso modelo de classifica\u00e7\u00e3o Regress\u00e3o Log\u00edstica:","402c265b":"$Regress\u00e3o$ $Log\u00edstica$","ca257831":"Como na pr\u00f3xima etapa teremos que trabalhar com vetores compostos por palavras, utilizamos a fun\u00e7\u00e3o split para obter o resultado que queremos:","3cb6b566":"Analisando o *shape* do nosso dataset podemos ver que temos 24984 reviews cadastradas separadas em dois tipo de opni\u00e3o, de quem gostou e quem n\u00e3o gostou dos filmes.","abab6e90":"# 7. Redes Neurais \ud83e\udde0\n\n\n\n---\n\nUma rede neural \u00e9 um modelo computacional inspirado pelo sistema nervoso central e \u00e9 capaz de realizar o aprendizado de m\u00e1quina bem como o reconhecimento de padr\u00f5es.\n","76056f38":"Como havia sido dito anteriormente, analizaremos avalia\u00e7\u00f5es dispon\u00edveis no IMDb, tamb\u00e9m conhecida como Internet Movie Database. Trata-se de uma base de dados online de informa\u00e7\u00e3o sobre entretenimento, e neste caso analizaremos avalia\u00e7\u00f5es de filmes. \n\nUma critica de filme pode apresentar diversas caracter\u00edsticas comuns, como compara\u00e7\u00e3o entre filmes, uma contextualiza\u00e7\u00e3o do roteiro ou da vida do ator\/diretor, mas o que ela sempre cont\u00e9m \u00e9 uma opni\u00e3o. Esses coment\u00e1rios (reviews) a seguir foram classificadas como positivas (positive==1) ou negativas (positive==0). Podemos analisar isso bem observando o *head* do nosso dataset de treino.\n","32fd9983":"# Predi\u00e7\u00e3o e Submiss\u00e3o","a7ee8490":"Nosso melhro resultado continua com a rede neural do PyTorch! Agora, com mais certeza, podemos utiliz\u00e1-la para fazer a predi\u00e7\u00e3o.","f3fdd197":"O **doc2vec** \u00e9 uma extens\u00e3o do word2vec, que aprende representa\u00e7\u00f5es de frases em um documento, em um esquema de aprendizagem profunda supervisionada para\na classifica\u00e7\u00e3o autom\u00e1tica. Ou seja, ele ser\u00e1 essencial para fazer nossa m\u00e1quina aprender.","f16b733b":"Em seguida, podemos transformar nossos dados de treino:","2e6b0ba8":"Vamos ent\u00e3o analizar nossas redes com 1 ou 2 camadas:","1c3ce697":"No momento do treinamento do Doc2Vec, geramos representa\u00e7\u00f5es somente para os textos que foram utilizados para aquele fim. Ent\u00e3o teremos que inferir as representa\u00e7\u00f5es para os novos textos, que utilizaremos agora. \n\nVamos definir uma fun\u00e7\u00e3o para obtermos as representa\u00e7\u00f5es vetoriais dos textos:","3e03ac4d":"Para nos ajudar futuramente podemos criar duas listas, uma $X$, que cont\u00e9m os textos, e outra $Y$, que cont\u00e9m os marcadores (0 e 1)","a5effc17":"O textos que utilizaremos como dados s\u00e3o avalia\u00e7\u00f5es de filmes (IMDb).","933af422":"**Rede neural (2 camadas)**","e9fb82a1":"Vamos agora garimpar nosso c\u00f3digo e procurar se algum de nossos dados est\u00e1 faltando:","f6bc757b":"Vamos agora abrir o Doc2Vec pr\u00e9-treinado:","c8ce9a58":"# 2. Conhecendo nossos dados \ud83d\udd0d\ud83c\udfb2","f0677fd4":"Faremos algumas limpezas nos textos (review) para que possam ser homogeneizados para facilitar o entendimento do computador.\n\n**Conserto de Mojibakes:** Mojibake \u00e9 o mais comum dos problemas com codifica\u00e7\u00e3o de texto. Ele pode acontecer na visualiza\u00e7\u00e3o de informa\u00e7\u00f5es textuais ao serem transferidas de um sistema para outro. Consiste na apresenta\u00e7\u00e3o de um simbolo gr\u00e1fico que n\u00e3o corresponde ao desejado.\n\n**Retirar a pontua\u00e7\u00e3o:** Retirar a pontua\u00e7\u00e3o \u00e9 algo importante pois se n\u00e3o, estes s\u00edmbolos de pontua\u00e7\u00e3o ir\u00e3o fazer parte das palavras; ou seja, uma string como *machine,* tamb\u00e9m ser\u00e1 considerada uma palavra, pois n\u00e3o h\u00e1 nenhum espa\u00e7o entre investors e a v\u00edrgula que a segue. Conseqeentemente, corremos o risco de o programa n\u00e3o reconhecer *machine* quando n\u00e3o estiver acompanhada da v\u00edrgula ao final.","51a795b9":"Verificamos agora com quantos dados e features estamos lidando:","fc454e95":"$Rede$ $Neural$ - 1 $camada$ $ocultas$","0ca8fb5b":"Anteriormente, obtivemos o valor da AUC de cada classificador somente com a base de treino, agora iremos avali\u00e1-los com a dataset de valida\u00e7\u00e3o para termos resultados n\u00e3o enviesados.","5916f46c":"Uma contendo o texto:","c1a70d5b":"**Rede Neural (1 camada)**","e896b8ec":"Agora vamos procurar valores (no caso reviews) repetidas:","ea85f273":"$Rede$ $Neural$ - 2 $camadas$ $ocultas$ - $PyTorch$","086d0a69":"# 1. Importando biblioteca e dados \ud83d\udcda\ud83c\udfb2","c2c4cb7b":"Agora faremos a limpeza nos textos:","d8500066":"$Pytorch$","4ae913cc":"Como ser\u00e1 que o computador lida com as palavras?","e7e9c7e7":"# 4. Limpeza do texto \ud83e\uddf9\ud83d\udcc4","b2c093db":"$Rede$ $Neural$ - 2 $camadas$ $ocultas$","9a4ab140":"Agora precisamos fazer o mesmo processo de limpeza, transforma\u00e7\u00e3o em lista e vetoriza\u00e7\u00e3o, para nossos dados para o teste de valida\u00e7\u00e3o","bcab25c3":"Usaremos o pytorch para tentarmos ter resultados melhores. Ele \u00e9 uma biblioteca open source para criar redes neurais, um framework usado no meio acad\u00eamico para construir redes neurais."}}