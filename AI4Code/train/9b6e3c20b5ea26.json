{"cell_type":{"90afafd5":"code","27926a4f":"code","22348e38":"code","b9607f67":"code","46710103":"code","63a99c68":"code","464b7891":"code","69fba16a":"code","62ce0e3b":"code","11611f9b":"code","1b1498ab":"code","a3db52e0":"code","0e8a207f":"code","a038bdd8":"code","149dc853":"code","b5287f2e":"code","5d616bf8":"code","0b44aa08":"code","3a338c86":"code","8cb14ed3":"code","c6e478ca":"code","5b1879b9":"code","ca85ff4d":"code","e9d48851":"code","43efa9f0":"code","adcad44c":"code","a22242fc":"code","3b6714c4":"code","8a306444":"code","c079ec99":"code","ca401a7c":"code","fbda4487":"code","ddeda994":"code","0bcc391e":"code","20aac531":"code","8ef8f97d":"markdown","2cb3ec64":"markdown","66557c6f":"markdown","c71dcf7c":"markdown","73a4c320":"markdown","3cb625f0":"markdown","261d4c93":"markdown","fe73f73c":"markdown","9559ff84":"markdown","dbc3089a":"markdown","5171f783":"markdown","1a06b92d":"markdown","0894e266":"markdown","48f3864b":"markdown","8b10699e":"markdown","9adc897a":"markdown","d8f1e7d1":"markdown","36406821":"markdown","325baef6":"markdown","3c3e73b1":"markdown","59d749ec":"markdown","ab0ea3ae":"markdown","79012c44":"markdown","a9a79204":"markdown","f425b85b":"markdown","c67a68b9":"markdown","ed8be4d2":"markdown","f4bb3a7b":"markdown","1900cab4":"markdown","a321778b":"markdown","2b2b575f":"markdown","34894196":"markdown"},"source":{"90afafd5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","27926a4f":"train = pd.read_csv(\"..\/input\/insurance-churn-prediction-weekend-hackathon\/Insurance_Churn_ParticipantsData\/Train.csv\")\ntrain.head()","22348e38":"train.shape","b9607f67":"train.columns","46710103":"train.isnull().sum()","63a99c68":"train.dtypes","464b7891":"train.describe()","69fba16a":"train.columns","62ce0e3b":"plt.figure(figsize=(16,6))\ntrain.boxplot(column=['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n                       'feature_5', 'feature_6', 'feature_7'])","11611f9b":"plt.figure(figsize=(12,6))\ntrain.boxplot(column=['feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', \n                                   'feature_13', 'feature_14', 'feature_15'])","1b1498ab":"plt.figure(figsize=(14,10))\nclr=['red','blue','green','pink','lime','orange','yellow','violet','indigo','teal','red','blue','green','pink','lime','orange']\nfor i,j in zip(range(1,17),train.columns[:-1]):\n    plt.subplot(4,4,i)\n    train[j].hist(color = clr[i-1], label=j)\n    plt.legend()\n    ","a3db52e0":"train[['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n       'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n       'feature_15']].plot(kind='density', subplots=True, \n                                                    layout=(4,4), sharex=False,\n                                                    sharey=False, figsize=(14,6))\nplt.show()","0e8a207f":"train.labels.value_counts().plot(kind='bar', colors=['green', 'orange'])","a038bdd8":"plt.figure(figsize=(20,8))\ncorr = train.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","149dc853":"import seaborn as sns\nsns.set(style=\"ticks\")\n\nsns.pairplot(train)","b5287f2e":"train.reset_index(drop=True, inplace=True)","5d616bf8":"train.head()","0b44aa08":"x = train.drop(['labels'], axis=1)\ny = train['labels']","3a338c86":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)","8cb14ed3":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# feature extraction\nmodel = LogisticRegression(solver='lbfgs')\nrfe = RFE(model, 3)\nfit = rfe.fit(x, y)\n\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)\n\n\ndf_feat = pd.DataFrame(fit.ranking_, x.columns)\ndf_feat.rename(columns = {0:\"Feature_Ranking\"}, inplace=True)\n","c6e478ca":"df_feat.sort_values(by=\"Feature_Ranking\").plot(kind='bar', figsize=(18,7))","5b1879b9":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.metrics import classification_report\n","ca85ff4d":"from sklearn.tree import DecisionTreeClassifier\n\n#making the instance\nmodel= DecisionTreeClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11],\n          'random_state':[123]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_dt = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_dt.predict(X_test)\n\nprint(\"*******************ACCURACY***************************************************************\")\n#Check Prediction Score\nprint(\"Accuracy of Decision Trees: \",accuracy_score(y_test, predictions))\n\nprint(\"*******************CLASSIFICATION - REPORT***************************************************************\")\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n\n","e9d48851":"from sklearn.ensemble import RandomForestClassifier\n\n#making the instance\nmodel= RandomForestClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_rf = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_rf.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of Random Forest: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n","43efa9f0":"import xgboost as xgb\n\nxgbmodel=xgb.XGBClassifier(learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n\n# Fit on data\nbest_clf_xgb = xgbmodel.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_xgb.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of XGBoost: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n\n","adcad44c":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(20, 3), max_iter=150, alpha=1e-4,\n                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n                    learning_rate_init=.1)\n\nmlp.fit(X_train,y_train)\nprint(\"Training set score: %f\" % mlp.score(X_train,y_train))\n","a22242fc":"#Predict\npredictions = mlp.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of MLP: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","3b6714c4":"from lightgbm import LGBMClassifier\n\nlgbm_c = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                        learning_rate=0.5, max_depth=7, min_child_samples=20,\n                        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n                        n_jobs=-1, num_leaves=500, objective='binary', random_state=None,\n                        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n                        subsample_for_bin=200000, subsample_freq=0)\n\n# Fit on data\nbest_clf_lgbm = lgbm_c.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_lgbm.predict(X_test)\n\nprint(\"*******************ACCURACY***************************************************************\")\n#Check Prediction Score\nprint(\"Accuracy of LGBM: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","8a306444":"test = pd.read_csv('..\/input\/insurance-churn-prediction-weekend-hackathon\/Insurance_Churn_ParticipantsData\/Test.csv')\n\ntest.shape","c079ec99":"test.head(5)","ca401a7c":"test_for_prediction = test[['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n       'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n       'feature_15']]","fbda4487":"\nprediction_from_dt  = best_clf_dt.predict(test_for_prediction)\ndf_prediction_from_dt = pd.DataFrame({'labels': prediction_from_dt})\ndf_prediction_from_dt.to_excel(\"Final_output_prediction_from_dt.xlsx\")\n\nprediction_from_rf  = best_clf_rf.predict(test_for_prediction)\ndf_prediction_from_rf = pd.DataFrame({'labels': prediction_from_rf})\ndf_prediction_from_rf.to_excel(\"Final_output_prediction_from_rf.xlsx\")\n\nprediction_from_xgb  = best_clf_rf.predict(test_for_prediction)\nprediction_from_xgb = pd.DataFrame({'labels': prediction_from_xgb})\nprediction_from_xgb.to_excel(\"Final_output_prediction_from_xgb.xlsx\")\n\n","ddeda994":"def generate_prediction(model_name, model, test_file):    \n    prediction_file_name = \"Final_output_prediction_from_\" + model_name +\".xlsx\"\n    prediction_from_model  = model.predict(test_file)\n    prediction_from_model = pd.DataFrame({'labels': prediction_from_model})\n    prediction_from_model.to_excel(prediction_file_name)\n","0bcc391e":"generate_prediction(\"lgbm\", best_clf_lgbm, test_for_prediction)","20aac531":"generate_prediction(\"mlp\", mlp, test_for_prediction)","8ef8f97d":"## 4.1 Re-setting Index Before Splitting","2cb3ec64":"# Phase1: Model Building On Training Data","66557c6f":"## 6.2 Importing and Model Fitting","c71dcf7c":"### 2.3.5 Target Variable Plot","73a4c320":"### 6.2.2 Random Forest","3cb625f0":"### 6.2.3 XGBoost","261d4c93":"### 6.2.1 Decision Trees","fe73f73c":"# Step4: Separating X and Y","9559ff84":"### Light GBM Classifier","dbc3089a":"## 2.2 Data Type Analysis ","5171f783":"# Step2: Exploratory Data Analysis","1a06b92d":"# Step1: Read Data","0894e266":"# Problem Statement","48f3864b":"# Step6: Model Building","8b10699e":"### 2.3.4 Density Plots Of Continuous Variables ","9adc897a":"### 2.3.2 Plot for Continuous variables","d8f1e7d1":"### 2.3.3 Histogram Plots Of Continuous Variables ","36406821":"Below are the steps involved to understand, clean and prepare your data for building your predictive model:\n\n1. Variable Identification\n2. Univariate Analysis\n3. Bi-variate Analysis\n4. Missing values treatment\n5. Outlier treatment\n6. Variable transformation\n7. Variable creation","325baef6":"# 2.4 Bi-variate Analysis","3c3e73b1":"## 2.3 Univariate Analysis\n\nAt this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous. Let\u2019s look at these methods and statistical measures for categorical and continuous variables individually:\n\n<b> Continuous Variables:- <\/b> In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics such as Histogram and Bar plots: ","59d749ec":"### 2.4.1 Correlation Matrix Plot","ab0ea3ae":"## 4.2 Split Data","79012c44":"## 2.1 Missing Data Analysis ","a9a79204":"<b> From the plots we can see that, there are lots of outliers in each varibale. <\/b>","f425b85b":"# Phase2: Applying Model On Test Data","c67a68b9":"### Neural Network","ed8be4d2":"# Step5: Creating Train and Test Set In Ratio 80:20","f4bb3a7b":"## 6.1 Identification Of Best Features","1900cab4":"Bi-variate Analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.","a321778b":"### 2.4.2 Scatterplot Matrix","2b2b575f":"### 2.3.1 Box Plot of CONTINUOUS variables ","34894196":"Insurance companies around the world operate in a very competitive environment. With various aspects of data collected from millions of customers, it is painstakingly hard to analyze and understand the reason for a customer\u2019s decision to switch to a different insurance provider.\n\nFor an industry where customer acquisition and retention are equally important, and the former being a more expensive process, insurance companies rely on data to understand customer behavior to prevent retention. Thus knowing whether a customer is possibly going to switch beforehand gives Insurance companies an opportunity to come up with strategies to prevent it from actually happening.\n\nGiven are 16 distinguishing factors that can help in understanding the customer churn, your objective as a data scientist is to build a Machine Learning model that can predict whether the insurance company will lose a customer or not using these factors.\n\nYou are provided with 16 anonymized factors (feature_0 to feature 15) that influence the churn of customers in the insurance industry\n"}}