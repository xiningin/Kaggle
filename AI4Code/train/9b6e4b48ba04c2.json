{"cell_type":{"a50df5b1":"code","3147db55":"code","352f067f":"code","92522671":"code","0280894d":"code","63674807":"code","7f36dbec":"code","b7abdc82":"code","48ca20e5":"code","11ace007":"code","a9ac843d":"code","b808577d":"code","97860f65":"code","d8d78cbc":"code","16ed2de1":"code","a36bd1bd":"code","539a4438":"code","dbf0dbd8":"code","27775420":"code","0628a56f":"code","7d13006e":"code","ecf3c4ca":"code","2e7c2b27":"code","2f4713ee":"code","d6db01bb":"code","efa40099":"code","751b46d3":"markdown","54dcf0c6":"markdown","f89eba4a":"markdown","2f01335b":"markdown","5e0a5bf9":"markdown","117e7f42":"markdown","13da682c":"markdown","5de30910":"markdown"},"source":{"a50df5b1":"from numpy import array\nfrom pickle import dump\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom pickle import load\nfrom keras.models import load_model\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nimport random\nimport string\nimport os\nfrom keras.callbacks import EarlyStopping","3147db55":"os.listdir('..\/input\/reddit-usernames')","352f067f":"\nnames =  pd.read_csv('..\/input\/reddit-usernames\/users.csv', error_bad_lines = False, encoding='latin-1')","92522671":"names.head()","0280894d":"names = names.drop(['n'], axis=1)","63674807":"data = names.iloc[0][0]\nfor i in range(1,10000):\n    x = names.iloc[i][0]\n    data = data + ' ' + x","7f36dbec":"del names","b7abdc82":"# organize into sequences of characters\nlength = 5\nsequence = list()\nfor i in range(length, len(data)):\n    # select sequence of tokens\n    seq = data[i-length:i+1]\n    # store\n    sequence.append(seq)\nprint('Total Sequences: %d' % len(sequence))","48ca20e5":"print(sequence[7])\nprint(sequence[8])\nprint(sequence[9])\nprint(sequence[10])\nprint(sequence[11])\nprint(sequence[12])","11ace007":"# unique list of letters in vocabulary\nchars = sorted(list(set(data)))\n# mapping each character to a unique integer\nmapping = dict((c, i) for i, c in enumerate(chars))","a9ac843d":"# vocabulary size\nvocab_size = len(mapping)\nprint('Vocabulary Size: %d' % vocab_size)","b808577d":"sequences = list()\nfor line in sequence:\n    # integer encode line\n    encoded_seq = [mapping[char] for char in line]\n    # store\n    sequences.append(encoded_seq)","97860f65":"print(sequences[0])","d8d78cbc":"del data\ndel sequence","16ed2de1":"sequences = array(sequences)\nX, y = sequences[:,:-1], sequences[:,-1]","a36bd1bd":"sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\nX = array(sequences)\ny = to_categorical(y, num_classes=vocab_size)","539a4438":"#splitting data into train and test sets. 3\/4 train, 1\/4 test.\nx_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.25, shuffle=False, random_state=69)\ndel X\ndel y\ndel sequences","dbf0dbd8":"# define model\nmodel = Sequential()\nmodel.add(LSTM(64, input_shape=(x_train.shape[1], x_train.shape[2]),return_sequences=True))\nmodel.add(LSTM(64))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())","27775420":"# Configure the checkpoint :\ncheckpoint = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='auto', restore_best_weights=True)\ncallbacks_list = [checkpoint]","0628a56f":"# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n# fit model\nhistory = model.fit(x_train, y_train, epochs=100, batch_size=2500, verbose=1,validation_data=(x_test, y_test),callbacks=callbacks_list)","7d13006e":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Test', 'Validation'], loc='upper right')\nplt.show()","ecf3c4ca":"# generate a sequence of characters with a language model\ndef generate_seq(model, mapping, seq_length, seed_text, n_chars):\n    result = ''\n    in_text = seed_text\n    # generate a fixed number of characters\n    for _ in range(n_chars):\n        # encode the characters as integers\n        encoded = [mapping[char] for char in in_text]\n        # truncate sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n        # one hot encode\n        encoded = to_categorical(encoded, num_classes=len(mapping))\n        # predict character\n        yhat = model.predict_classes(encoded, verbose=0)\n        # reverse map integer to character\n        out_char = ''\n        for char, index in mapping.items():\n            if index == yhat:\n                out_char = char\n                break\n        # append to input\n        in_text += char\n        result += char\n    return result","2e7c2b27":"#Generate random string as primer for char RNN\n\ndef randomString(stringLength):\n    letters = string.ascii_letters\n    return ''.join(random.choice(letters) for i in range(stringLength))","2f4713ee":"#result\nfor i in range(2):\n    result = generate_seq(model, mapping, 5, randomString(4) + ' ', 10).split()\n    for username in result:\n        print(username)","d6db01bb":"#result\nfor i in range(2):\n    result = generate_seq(model, mapping, 5, randomString(4) + ' ', 10).split()\n    for username in result:\n        print(username)","efa40099":"#result\nfor i in range(2):\n    result = generate_seq(model, mapping, 5, randomString(4) + ' ', 10).split()\n    for username in result:\n        print(username)","751b46d3":"* Splitting data into X and y\n* X is 5 characters and y is the next character","54dcf0c6":"# Results","f89eba4a":"# Inference","2f01335b":"One-hot encoding sequences","5e0a5bf9":"# Data Preprocessing","117e7f42":"Sliding window of length 5","13da682c":"# Character-Based RNN\nWork based on [Machine Learning Mastery Tutorial](https:\/\/machinelearningmastery.com\/develop-character-based-neural-language-model-keras\/)","5de30910":"Concatenating 10,000 usernames for easy preprocessing"}}