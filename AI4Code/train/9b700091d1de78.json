{"cell_type":{"6458bbb3":"code","deefc227":"code","95c8397b":"code","b4b6f7bd":"code","a86e5628":"code","d34ba729":"code","0bd76e5b":"code","c6c9d302":"code","7e03bb3b":"code","9af078fe":"code","96c37705":"code","a577f55c":"code","887ff967":"code","83e79589":"code","621cb1f4":"code","e2d63c0e":"code","6b55232b":"code","2148bfc8":"code","568b61d5":"code","7a4e539c":"code","c2dee232":"code","45c9e8be":"code","f655d411":"code","1a179ba9":"code","7b8ad2bc":"code","01d9ec8d":"code","665b6ba9":"code","04001cd5":"code","8101684a":"code","bd3d9a39":"code","34e3c76d":"code","de0f3082":"code","e0a5e215":"code","87d02e0e":"code","7f3fb276":"code","2e53c0e4":"code","fca34bd8":"code","99fb9f70":"code","185383ea":"code","dd7ee42e":"code","e19281e3":"code","51778eb2":"code","7d8933a4":"code","65e17f6a":"code","ec31ee7e":"code","b65e0f92":"code","48c01037":"code","3de85943":"code","7bc09f3d":"code","6036304a":"code","21fd8253":"code","877757de":"code","f82b1b68":"code","2455c854":"code","63b16d76":"code","7a529122":"code","57f54852":"code","261c4e59":"code","14c899bd":"code","88f244a4":"code","4e6557e0":"markdown","2f19f6da":"markdown","0041234f":"markdown","e7f7d6bf":"markdown","26c5d727":"markdown","aba118b9":"markdown","50dee55d":"markdown","9c26a5fb":"markdown","0f202627":"markdown","a769efed":"markdown","9819e5fe":"markdown","584b5986":"markdown","af0bbedb":"markdown","3d35647d":"markdown","28eb8c79":"markdown","135b3186":"markdown","7591b385":"markdown","7b71925f":"markdown"},"source":{"6458bbb3":"import numpy as np \nimport pandas as pd\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nimport plotly\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nfrom sklearn.manifold import TSNE\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nfrom numpy.random import seed\nseed(0)\n\ntf.random.set_seed(0)\n","deefc227":"df= pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","95c8397b":"df.head()","b4b6f7bd":"df.shape","a86e5628":"df.info()","d34ba729":"df.isna().sum()","0bd76e5b":"df.describe()","c6c9d302":"df.drop([\"Time\"], axis=1, inplace=True)","7e03bb3b":"count_classes = df['Class'].value_counts(normalize=True)\n\nfig= go.Figure(go.Bar(x=count_classes.index,\n                      y=count_classes.values,\n                     width=0.6,\n                     text= count_classes.values.round(decimals=4),\n                     textposition='outside',\n                     textfont=dict(color='black',\n                                  size=15)))\n\ncolors=['green','red']\n\nfig.update_layout(title_text='Fraud class histogram',\n                 xaxis_title='Class',\n                 yaxis_title='Percentage',\n                 width=650,\n                 height=600,\n                titlefont=dict(size=22, color='black')\n                  \n                 )\n\nfig.update_traces(marker_line_width=1.5,\n                 marker_line_color='black',\n                 marker_color=colors)\n","9af078fe":"# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","96c37705":"f, (ax1,ax2)= plt.subplots(2,1,figsize=(24,20))\n\nsns.heatmap(df.corr(), cmap=sns.color_palette(\"RdBu_r\", 10), annot_kws={'size':20}, ax=ax1)\nax1.set_title('Imbalanced Correlation Heatmap', fontdict=dict(fontsize=20))\n\nsns.heatmap(new_df.corr(), cmap=sns.color_palette(\"RdBu_r\", 10), annot_kws={'size':20}, ax=ax2)\nax2.set_title('Balanced Correlation Heatmap', fontdict=dict(fontsize=20))\n\nplt.show()","a577f55c":"non_fraudulent_df= df[df.Class==0]\nfraudulent_df= df[df.Class==1]","887ff967":"params = {'axes.titlesize':'45'}\nmatplotlib.rcParams.update(params)\n\ndf.hist(bins=50, figsize=(80,60))\nplt.suptitle('Imbalanced dataset histogram graphs', fontsize=60)\nplt.show()","83e79589":"params = {'axes.titlesize':'45'}\nmatplotlib.rcParams.update(params)\n\nnew_df.hist(bins=50, figsize=(80,60))\nplt.suptitle('Balanced dataset histogram graphs', fontsize=60)\n\nplt.show()","621cb1f4":"columns= df.columns.tolist()\n\ncolumns=[c for c in columns if c not in ['Class']]\n\ntarget= 'Class'\n\nX= df[columns]\nY= df[target]\n\n# Whole dataset\nX_train, X_test, y_train, y_test = train_test_split(X, Y,test_size = 0.3, random_state=42)","e2d63c0e":"X_train.loc[:,'Amount']= RobustScaler().fit_transform(X_train.loc[:,'Amount'].values.reshape(-1, 1))\nX_test.loc[:,'Amount']= RobustScaler().fit_transform(X_test.loc[:,'Amount'].values.reshape(-1, 1))","6b55232b":"X_train_normal = X_train[y_train==0]\nX_train_fraud = X_train[y_train==1]","2148bfc8":"normal_df_sample= non_fraudulent_df.sample(n=3000, random_state=42)\nnormal_df_sample","568b61d5":"df1= fraudulent_df.append(normal_df_sample)","7a4e539c":"x1 = df1.drop(['Class'], axis = 1).values\ny1 = df1[\"Class\"].values","c2dee232":"def tsne_plot(x1, y1, name=\"graph.png\"):\n    tsne = TSNE(n_components=2, random_state=42)\n    X_t = tsne.fit_transform(x1)\n\n    plt.figure(figsize=(12, 8))\n    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Non Fraud')\n    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='r', linewidth='1', alpha=0.8, label='Fraud')\n\n    plt.legend(loc='best');\n    plt.savefig(name);\n    plt.show();\n    \ntsne_plot(x1, y1, \"original.png\")","45c9e8be":"input_layer = Input(shape=(X.shape[1], ))\nencoded1 = Dense(29,activation='relu', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n\nlatent_view= Dense(15,activation='relu')(encoded1)\n\ndecoded1 = Dense(29,activation='relu')(latent_view)\n\noutput_layer = Dense(X.shape[1])(decoded1)","f655d411":"autoencoder= Model(input_layer, output_layer)\n\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')","1a179ba9":"autoencoder.fit(X_train_normal,X_train_normal, epochs=20, batch_size=256,\n                shuffle = True, validation_split = 0.20)","7b8ad2bc":"encoder_all= Sequential([autoencoder.layers[0], autoencoder.layers[1], autoencoder.layers[2]])\nenc_X_train= encoder_all.predict(X_train)","01d9ec8d":"norm_hid_rep =encoder_all.predict(X_train_normal[:3000])\nfraud_hid_rep= encoder_all.predict(X_train_fraud)","665b6ba9":"rep_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)\ny_n = np.zeros(norm_hid_rep.shape[0])\ny_f = np.ones(fraud_hid_rep.shape[0])\nrep_y = np.append(y_n, y_f)\ntsne_plot(rep_x, rep_y, \"latent_representation.png\")","04001cd5":"knn_model = KNeighborsClassifier(n_neighbors=3)\nknn_model.fit(enc_X_train,y_train)","8101684a":"knn_predicted= knn_model.predict(encoder_all.predict(X_test))","bd3d9a39":"pd.Series(knn_predicted).value_counts()","34e3c76d":"params = {'figure.figsize': (15, 10),\n         'axes.titlesize':20}\n\nmatplotlib.rcParams.update(params)","de0f3082":"conf_matrix = confusion_matrix(y_test,knn_predicted)\n\nax=plt.subplot()\n\nsns.heatmap(conf_matrix,annot=True,ax=ax,fmt='g')#annot=True to annotate cells, fmt='g' numbers not scientific form\nax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels')\nax.set_title('KNN Confusion Matrix'); \nax.xaxis.set_ticklabels(['Normal', 'Fraud']); ax.yaxis.set_ticklabels(['Normal', 'Fraud']);\nax.set(yticks=[0, 2], \n       xticks=[0.5, 1.5])\n\n\nax.yaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0.5))","e0a5e215":"print(classification_report(y_test, knn_predicted))","87d02e0e":"accuracy= round(accuracy_score(y_test,knn_predicted)*100, 2)\nprint(f'Accuracy Score for KNN: {accuracy}%')","7f3fb276":"input_layer = Input(shape=(enc_X_train.shape[1], ))\nlayer2 = Dense(8,activation='relu')(input_layer)\nlayer3 = Dense(4,activation='relu')(layer2)\nlayer4 = Dense(1,activation='sigmoid')(layer3)\n\nmodel= Model(input_layer, layer4)","2e53c0e4":"model.compile(optimizer=Adam(learning_rate= 0.005), loss='binary_crossentropy', metrics='Recall')","fca34bd8":"early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=1e-5) #stop training if loss does not decrease with at least 0.00001\nreduce_lr = ReduceLROnPlateau(monitor='loss', patience=5, min_delta=1e-5, factor=0.2) #reduce learning rate (divide it by 5 = multiply it by 0.2) if loss does not decrease with at least 0.00001\n\ncallbacks = [early_stopping, reduce_lr]\n\nmodel.fit(enc_X_train,y_train,batch_size=128, epochs=30, validation_data=[encoder_all.predict(X_test), y_test])","99fb9f70":"neural_network_prediction_train= model.predict(encoder_all.predict(X_train))","185383ea":"mse = np.mean(np.power(y_train.values.reshape(-1,1) - neural_network_prediction_train, 2), axis=1)\nerror_df = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y_train})\nerror_df.groupby('true_class').describe()","dd7ee42e":"normal_mean= error_df.groupby('true_class').describe().loc[0,'reconstruction_error']['mean']\nnormal_std= error_df.groupby('true_class').describe().loc[0,'reconstruction_error']['std']","e19281e3":"test_predictions= model.predict(encoder_all.predict(X_test))\nmse = np.mean(np.power(y_test.values.reshape(-1,1) - test_predictions, 2), axis=1)\ny_pred=[(lambda er: 1 if er>=(normal_mean+ 3*normal_std)  else 0)(er) for er in mse]","51778eb2":"pd.Series(y_pred).value_counts()","7d8933a4":"pd.Series(y_test).value_counts()","65e17f6a":"conf_matrix_nn = confusion_matrix(y_test,y_pred)\n\nax=plt.subplot()\nsns.heatmap(conf_matrix_nn,annot=True,ax=ax,fmt='g')#annot=True to annotate cells, fmt='g' numbers not scientific form\nax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels')\nax.set_title('Neural Network Confusion Matrix'); \nax.xaxis.set_ticklabels(['Normal', 'Fraud']); ax.yaxis.set_ticklabels(['Normal', 'Fraud']);\nax.set(yticks=[0, 2], \n       xticks=[0.5, 1.5])\nax.yaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0.5))","ec31ee7e":"print(classification_report(y_test,y_pred))","b65e0f92":"accuracy= round(accuracy_score(y_test,y_pred)*100, 2)\nprint(f'Accuracy Score for Simple Neural Network: {accuracy}%')","48c01037":"clf= XGBClassifier(random_state=42)","3de85943":"clf= XGBClassifier()\nclf.fit(enc_X_train, y_train)","7bc09f3d":"XGB_prediction= clf.predict(encoder_all.predict(X_test))","6036304a":"pd.Series(XGB_prediction).value_counts()","21fd8253":"conf_matrix_nn = confusion_matrix(y_test,XGB_prediction)\n\nax=plt.subplot()\nsns.heatmap(conf_matrix_nn,annot=True,ax=ax,fmt='g')#annot=True to annotate cells, fmt='g' numbers not scientific form\nax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels')\nax.set_title('XGB Confusion Matrix'); \nax.xaxis.set_ticklabels(['Normal', 'Fraud']); ax.yaxis.set_ticklabels(['Normal', 'Fraud']);\nax.set(yticks=[0, 2], \n       xticks=[0.5, 1.5])\nax.yaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0.5))","877757de":"print(classification_report(y_test,XGB_prediction))","f82b1b68":"accuracy= round(accuracy_score(y_test,XGB_prediction)*100, 2)\nprint(f'Accuracy Score for XGBClassifier: {accuracy}%')","2455c854":"X_oversampled_train, Y_oversampled_train= SMOTE(sampling_strategy='minority').fit_sample(enc_X_train,y_train)","63b16d76":"clf= XGBClassifier()\nclf.fit(X_oversampled_train, Y_oversampled_train)","7a529122":"X_oversampled_train.shape","57f54852":"XGB_over_prediction= clf.predict(encoder_all.predict(X_test))","261c4e59":"pd.Series(XGB_over_prediction).value_counts()","14c899bd":"conf_matrix_nn = confusion_matrix(y_test,XGB_over_prediction)\n\nax=plt.subplot()\nsns.heatmap(conf_matrix_nn,annot=True,ax=ax,fmt='g')#annot=True to annotate cells, fmt='g' numbers not scientific form\nax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels')\nax.set_title('Oversampled XGB Confusion Matrix'); \nax.xaxis.set_ticklabels(['Normal', 'Fraud']); ax.yaxis.set_ticklabels(['Normal', 'Fraud']);\nax.set(yticks=[0, 2], \n       xticks=[0.5, 1.5])\nax.yaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0.5))","88f244a4":"print(classification_report(y_test,XGB_over_prediction))","4e6557e0":"Although the fraudulent transactions are not perfectly distinguished from the normal transactions, it seems like there is slight improvement to distinguish them from normal transactions.","2f19f6da":"![fraud.jpg](attachment:fraud.jpg)","0041234f":"Although there are some clear indication for some outliers, but most of the fraudulent transactions **are actually quite difficult** to identify out of the normal transactions.","e7f7d6bf":"Apply Robust Scaler to the 'Amount' column.","26c5d727":"Seems like a simple strategy like KNN or XGBClassifier can yield a even better result. However, I noticed that changed in random state do have a drastic change in the results (Neural network might performed better in some state).","aba118b9":"<font color='purple' size=+2.5><b> Correlation between variables<\/b><\/font>\n\n<font size=+0.5>We are going to make a correlation heatmap for **balanced** and **imbalanced** datasets.<\/font>","50dee55d":"<font color='purple' size=+2.5> <b> XGBOOST <\/b><\/font>","9c26a5fb":"<font color='purple' size=+2.5><b> Preprocessing <\/b><\/font>","0f202627":"<font size=+0.5>Detecting Credit Card Fraud is a difficult process due to its <font color='red'> **highly imbalanced proportion** <\/font> (normal to fraudulent transactions).\n\nAny misclassified normal or fraudulent transcations can lead to a financial loss to the company.\n\nAccuracy is not a good measure for highly imbalanced datasets. You would still get more than 99% accuracy even though your model predicted all outputs as normal transactions. Hence, **measures like recall or f1 score** are a better measure for imbalanced dataset. <\/font>","a769efed":"<font color='purple' size=+2.5><b> KNN<\/b><\/font>\n\nLet's perform some models to distinguish fraudulent transcations.","9819e5fe":"<font color='purple' size=+2.5><b> Simple Neural Network <\/b><\/font>\n\nA plain neural network is used.","584b5986":"Clearly, imbalanced datasets might be **highly skewed and misinformed** of its true relationship.","af0bbedb":"<font color='purple' size=+2.5> <b> With Oversampling method and Autoencoder (XGB Classifier) <\/b><\/font>\n\nLet's try with SMOTE oversampling method for output after autoencoder to test it on a XGB Classifier.","3d35647d":"We have created a dataset with 50\/50 proportion of fraudulent and non-fraudulent datasets. (**Balanced Datasets**)","28eb8c79":"We have just 492 of fraudelent transactions out of 284807 transactions. (0.17 %)\n\nHence, this is a highly imbalanced dataset.","135b3186":"Unlike KNN that produced binary outputs (0 or 1), neural network that used binary crossentropy as a loss function won't produce a output in binary form.\n\nHence, we would calculate the mean-sqaured errors (MSE) between the predicted valeus and actual values.\n\nA transcation would be considered as fraudulent if the mean-squared error >= mean of MSE of normal transcations + 3 * standard deviation of MSE of normal transcations.","7591b385":"<font color='purple' size=+2.5><b> Autoencoders <\/b><\/font>\n\n<font color='blue'> Let's compute a simple autoencoder to determine its usefulness to distinguish fraudulent transaction from **non-fraudulent** transaction. <\/font>","7b71925f":"Very well-defined. No missing values at all!"}}