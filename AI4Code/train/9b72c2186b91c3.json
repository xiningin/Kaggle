{"cell_type":{"b07da1de":"code","d44704f8":"code","5c0c2241":"code","1ac9210a":"code","7618ea33":"code","d26cafcf":"code","7cce7c57":"code","9c815bc6":"code","9a0ab017":"code","999e2e4b":"code","f043cda7":"code","6ee4c47c":"code","552e64eb":"code","c07f9db8":"code","4f8b5473":"code","6978af11":"code","6fabface":"code","20b1ec6a":"code","c9a93946":"code","f9b60a49":"code","b4bb939b":"code","0e19968b":"code","503ce9d9":"code","e0d9f7b1":"code","1227ebda":"code","23f84719":"markdown","b34e1547":"markdown","440c8c54":"markdown","30b16869":"markdown","7cc9c65b":"markdown","c3ba2709":"markdown"},"source":{"b07da1de":"pip install --upgrade scikit-learn","d44704f8":"import sklearn\nprint(sklearn.__version__)","5c0c2241":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option(\"display.max_columns\", 80)\npd.set_option(\"display.max_rows\", 100)","1ac9210a":"data = pd.read_csv('\/kaggle\/input\/predicting-profitable-customer-segments\/customerTargeting.csv')\ndata.head()","7618ea33":"data.info() #no data missing and all values are already integers\/floats","d26cafcf":"data.describe()","7cce7c57":"corr_df = data.corr()\ncorr_df #unhide the output to see the full set of corrolation coeficients","9c815bc6":"import matplotlib\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(12, 12))\n\nsns.heatmap(corr_df)","9a0ab017":"sns.countplot(data['target'])","999e2e4b":"data['target'].value_counts()","f043cda7":"def enc_0(num):\n    if num == 0:\n        return 1\n    else:\n        return 0\n\ndef enc_1(num):\n    if num == 1:\n        return 1\n    else:\n        return 0\n\ndef enc_2(num):\n    if num == 2:\n        return 1\n    else:\n        return 0\n\ndata['target_0'] = pd.Series([enc_0(x) for x in data.target], index=data.index)\ndata['target_1'] = pd.Series([enc_1(x) for x in data.target], index=data.index)\ndata['target_2'] = pd.Series([enc_2(x) for x in data.target], index=data.index)\n\n## One hot encoding the outcomes didn't help any of the models I tried\n## But there is one possibility with this I left unexplored, see my final thoughts for more details","6ee4c47c":"corr_df = data.corr()\ncorr_df = corr_df.sort_values(by=['target'])\npd.DataFrame(corr_df['target'])","552e64eb":"# There are other methods of generating this in less code\n# I just want my mistakes to be a little easier to spot\n# the d_x refers to the delta between g1_x and g2_x\ndata['d_1'] = data['g1_1'] - data['g2_1']\ndata['d_2'] = data['g1_2'] - data['g2_2']\ndata['d_3'] = data['g1_3'] - data['g2_3']\ndata['d_4'] = data['g1_4'] - data['g2_4']\ndata['d_5'] = data['g1_5'] - data['g2_5']\ndata['d_6'] = data['g1_6'] - data['g2_6']\ndata['d_7'] = data['g1_7'] - data['g2_7']\ndata['d_8'] = data['g1_8'] - data['g2_8']\ndata['d_9'] = data['g1_9'] - data['g2_9']\ndata['d_10'] = data['g1_10'] - data['g2_10']\ndata['d_11'] = data['g1_11'] - data['g2_11']\ndata['d_12'] = data['g1_12'] - data['g2_12']\ndata['d_13'] = data['g1_13'] - data['g2_13']\ndata['d_14'] = data['g1_14'] - data['g2_14']\ndata['d_15'] = data['g1_15'] - data['g2_15']\ndata['d_16'] = data['g1_16'] - data['g2_16']\ndata['d_17'] = data['g1_17'] - data['g2_17']\ndata['d_18'] = data['g1_18'] - data['g2_18']\ndata['d_19'] = data['g1_19'] - data['g2_19']\ndata['d_20'] = data['g1_20'] - data['g2_20']\ndata['d_21'] = data['g1_21'] - data['g2_21']","c07f9db8":"features = ['c_1', 'c_2', 'c_3', 'c_4', 'c_5', 'c_6','c_7', 'c_8', 'c_9', 'c_10', 'c_11', 'c_12', 'c_13',\n            'c_14', 'c_15', 'c_16', 'c_17', 'c_18', 'c_19', 'c_20', 'c_21', 'c_22', 'c_23', 'c_24', 'c_25',\n            'c_26', 'c_27', 'c_28', 'd_1', 'd_2', 'd_3', 'd_4', 'd_5', 'd_6', 'd_7', 'd_8', 'd_9', 'd_10',\n            'd_11', 'd_12', 'd_13', 'd_14', 'd_15', 'd_16', 'd_17', 'd_18', 'd_19', 'd_20', 'd_21']#,\n            #'g1_1', 'g1_5', 'g2_12', 'g2_21', 'g2_1', 'g2_13', 'g2_11']\n","4f8b5473":"X = data[features]\ny = data['target'] #[['target_0', 'target_1', 'target_2']] # one hot encoding did not improve performance","6978af11":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n","6fabface":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1, test_size=.2)","20b1ec6a":"hgb = HistGradientBoostingClassifier()\nhgb.fit(train_X, train_y)\nls_preds = hgb.predict(val_X)\nprint('Score of Histogram-based Gradient Boosting Regression Tree: ', accuracy_score(val_y, ls_preds))","c9a93946":"from sklearn.metrics import confusion_matrix\n\nconfuse = confusion_matrix(val_y, ls_preds)\nconfuse = pd.DataFrame(confuse)\nfigure(num=None, figsize=(7, 7))\nsns.heatmap(confuse, linewidths=1,annot=True, fmt='.5g', annot_kws={\"size\": 12},cmap=\"YlGnBu\", \n            yticklabels=['0 True','1 True','2 True'], xticklabels=['0 Predicted','1 Predicted','2 Predicted'])","f9b60a49":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier(n_estimators=90, random_state=0)\netc.fit(train_X, train_y)\nls_preds = etc.predict(val_X)\nprint('Score of Etremely Random Forest: ', accuracy_score(val_y, ls_preds))","b4bb939b":"from sklearn.metrics import confusion_matrix\n\nconfuse = confusion_matrix(val_y, ls_preds)\nconfuse = pd.DataFrame(confuse)\nfigure(num=None, figsize=(7, 7))\nsns.heatmap(confuse, linewidths=1,annot=True, fmt='.5g', annot_kws={\"size\": 12},cmap=\"YlGnBu\", \n            yticklabels=['0 True','1 True','2 True'], xticklabels=['0 Predicted','1 Predicted','2 Predicted'])\n","0e19968b":"from sklearn.ensemble import BaggingClassifier\nbaggin = BaggingClassifier(n_estimators=90)\nbaggin.fit(train_X, train_y)\nls_preds = baggin.predict(val_X)\nprint('Score of Histogram-based Gradient Boosting Regression Tree: ', accuracy_score(val_y, ls_preds))","503ce9d9":"confuse = confusion_matrix(val_y, ls_preds)\nconfuse = pd.DataFrame(confuse)\nfigure(num=None, figsize=(7, 7))\nsns.heatmap(confuse, linewidths=1,annot=True, fmt='.5g', annot_kws={\"size\": 12},cmap=\"YlGnBu\", \n            yticklabels=['0 True','1 True','2 True'], xticklabels=['0 Predicted','1 Predicted','2 Predicted'])","e0d9f7b1":"import sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nestimators = [\n    ('rf', ExtraTreesClassifier(n_estimators=90, random_state=0)),\n    ('hgb', RandomForestClassifier(n_estimators=40)),\n    ('ada', HistGradientBoostingClassifier()),\n    ('bag', BaggingClassifier(n_estimators=70))\n    ]\nclf = StackingClassifier(estimators=estimators, final_estimator=AdaBoostClassifier(n_estimators=40))\nclf.fit(train_X, train_y)\nls_preds = clf.predict(val_X)\nprint('Score of stacked: ', accuracy_score(val_y, ls_preds))","1227ebda":"confuse = confusion_matrix(val_y, ls_preds)\nconfuse = pd.DataFrame(confuse)\nfigure(num=None, figsize=(7, 7))\nsns.heatmap(confuse, linewidths=1,annot=True, fmt='.5g', annot_kws={\"size\": 12},cmap=\"YlGnBu\", \n            yticklabels=['0 True','1 True','2 True'], xticklabels=['0 Predicted','1 Predicted','2 Predicted'])","23f84719":"I had expirimented with many different features to include, ultimately the delta variable set proved to add the most value. I commented out some of the variables that had boosted the models accuracy but had an unintended side affect (described below).","b34e1547":"I had experimented with removing c_15, c_9, and c_7, because of their really low corrolation coeficients. After testing I discovered they had a not insignifigant effect of the capture rate of 0 for the target.","440c8c54":"# Task: Predict which customer group is worth targeting\n\nThe goal of this task is to determine which group of customers are worth targeting given the following dataset. The data is highly ananomized so there isn't a good possibility to apply domain expertise. My intial goal for this task were to try out the stacked classifier from SK learn (new in version 0.22.1).\n\nThanks to:\n* Thanisis https:\/\/www.kaggle.com\/tsiaras for the dataset https:\/\/www.kaggle.com\/tsiaras\/predicting-profitable-customer-segments and also for the task","30b16869":"# Data:\nThe columns starting with 'g1_' represent info about the first group\nthe columns starting with 'g2_' represent infor about the second group\nthe columns with 'c_' are features representing a comparison of the two groups\n\n'target' is the value we are trying to predict. A value of 0 indicates that neither group was profitable after the marketing campaign, a value of  1 indicates group 1 was more profitable and a value of 2 indicates that a value of 2 was more profitable","7cc9c65b":"The above model is my final model. I had some other models that were more accurate but this model did a better job of catching True negitives. through feature engineering, hyperparameter tuning, and feature selection I was able to get slightly higher accuracy in other models but each of them wouldn't predict a 0 state (which remember meant that neither group was profitable to market to).\n\n# Accuracy\nThis model is only 56% accurate. The model had to detect one of 3 distinct outcomes, if each of those outcomes were equally likely then deciding at random would give us an accuracy of 33%. Since group one being more profitable is the most common outcome, a model can be 46% accurate just by always predicting target to be 1. \n\nBelow there are several other models hidden, which had decent enough performance to be worth a look, feel free to unhide them if your curious. The other visible model below is the one I initially set forth to test out, the Stacked Classifier, but I'm not hugely thrilled with its performance.","c3ba2709":"# Final Thoughts:\n\nI set out to try out the StackingClassifier and I can say I was successful with that. There is still room for improvement in the model, and I imagine more signal to tease out of the dataset. I look forward to see what others try with this task, I'm really curious what I may not have found. \n\nEach of the models that I created did rather poorly at predicting an outcome of 0 (where neither group was profitable). One possibility that I did not explore was the option of training a seperate model (probably a decision tree) and tune it to just detect 0 or not 0 outcome. Including such model in the stack could have been useful.\n\nI'll also acknowledge that I'm not overly experienced with model stacking\/blending so I may have overlooked some better model designs. \n\nCurrently I'm challenging myself to complete one Kaggle Task per week. If you have any constructive feedback or want to collaborate with me on a future task feel free to message me or leave a comment on this notebook.\n\nAs always upvotes are appreciated."}}