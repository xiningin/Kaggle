{"cell_type":{"a926dbe3":"code","d15cd232":"code","d5d14399":"code","8a552c90":"code","6d9d87e6":"code","73619b56":"code","5f63f4f6":"code","f037839a":"code","2eeed4e8":"code","b9fee407":"code","8a6ee1ec":"code","1a6d728b":"code","93e76bef":"code","5dc7c7c1":"code","60a10aa3":"code","415a11ee":"code","ce47eb5f":"code","a6c0e951":"markdown","f752df4d":"markdown","fcafb021":"markdown","21777f6b":"markdown","7959cd56":"markdown","1c3ca2e0":"markdown","0c01e896":"markdown","2fd96aa7":"markdown","3f0ce362":"markdown","a7455fa3":"markdown","31c7d237":"markdown","3717e290":"markdown","7c0f707e":"markdown","1904549d":"markdown","ec4fa68e":"markdown"},"source":{"a926dbe3":"import numpy as np\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","d15cd232":"from keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LeakyReLU, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras import initializers\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d5d14399":"(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# We only concerned with 'X_train' data\n# It can be rewritten as (X_train,_),(_,_) = mnist.load_data()\n\nprev = X_train.shape","8a552c90":"X_train = X_train.reshape(60000, 28*28)\n\nprint(prev)\nprint(X_train.shape)","6d9d87e6":"# normalizing the inputs (-1, 1)\n\nX_train = (X_train.astype('float32') \/ 255 - 0.5) * 2\n# we have pixels from 0-255, dividing by 255 leads to normalize them in range 0-1\n#(-0.5) * 2 shift it to (-1,1) for tanh activation","73619b56":"latent_dim = 100\n# Latent dimensions are dimensions which we do not directly observe, but which we assume to exist (Hidden)\n# We use this in reference of generator, it create images from latent dimension whichwe  do not directly observe, but assume to exist\n\n# image dimension 28x28\nimg_dim = 784\n\ninit = initializers.RandomNormal(stddev=0.02)\n# stddev = Standard deviation of the random values to generate\n\n# The neural network needs to start with some weights and then iteratively update them to better values. \n# kernel_initializer is term for which statistical distribution or function to use for initialising the weights.\n# Ref - https:\/\/datascience.stackexchange.com\/questions\/37378\/what-are-kernel-initializers-and-what-is-their-significance\n\n# Generator network\n\n# sequential model simply allows us to stitch layers together\ngenerator = Sequential()\n\n# Input layer and hidden layer 1\ngenerator.add(Dense(128, input_shape=(latent_dim,), kernel_initializer=init))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization(momentum=0.8))\n\n# A dense layer is simply a fully connected layer of neurons.\n\n# The LeakyReLU remove problem of \"dying ReLU\" and alpha is negative slope constant\n# Deep dive - https:\/\/towardsdatascience.com\/complete-guide-of-activation-functions-34076e95d044\n\n# batchnormalization layer will transform inputs so they will have a mean of zero and a standard deviation of one.\n# \u201cmomentum\u201d in batch norm allows you to control how much of the statistics from the previous mini batch to include when the update is calculated\n# Deep dive - https:\/\/machinelearningmastery.com\/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization\/\n\n# Hidden layer 2\ngenerator.add(Dense(256))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization(momentum=0.8))\n\n# Hidden layer 3\ngenerator.add(Dense(512))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization(momentum=0.8))\n\n# Output layer \ngenerator.add(Dense(img_dim, activation='tanh'))","5f63f4f6":"generator.summary()","f037839a":"discriminator = Sequential()\n\n# Input layer and hidden layer 1\ndiscriminator.add(Dense(512, input_shape=(img_dim,), kernel_initializer=init))\ndiscriminator.add(LeakyReLU(alpha=0.2))\n\n# Hidden layer 2\ndiscriminator.add(Dense(256))\ndiscriminator.add(LeakyReLU(alpha=0.2))\n\n# Hidden layer 3\ndiscriminator.add(Dense(128))\ndiscriminator.add(LeakyReLU(alpha=0.2))\n\n# Output layer\ndiscriminator.add(Dense(1, activation='sigmoid'))","2eeed4e8":"discriminator.summary()","b9fee407":"optimizer = Adam(lr=0.0002, beta_1=0.5)\ndiscriminator.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Since we have to predict either fake or real (i.e. two classes)","8a6ee1ec":"discriminator.trainable = False\n\nd_g = Sequential()\nd_g.add(generator)\nd_g.add(discriminator)\nd_g.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])","1a6d728b":"d_g.summary()","93e76bef":"epochs = 100\nbatch_size = 64","5dc7c7c1":"real = np.ones(shape=(batch_size, 1)) #craete a \"real\" array with values = 1 and size = 100 \nfake = np.zeros(shape=(batch_size, 1)) # craete a \"fake\" array with values = 0 and size = 100","60a10aa3":"d_loss = [] #discriminator loss\nd_g_loss = [] #adversarial loss\/generator loss","415a11ee":"for e in range(epochs + 1):\n    for i in range(len(X_train) \/\/ batch_size):\n        \n        # Train Discriminator weights\n        discriminator.trainable = True\n        \n        # Real samples\n                \n        X_batch = X_train[i*batch_size:(i+1)*batch_size]\n        # Defining size of batches per 64 in one batch\n        \n        d_loss_real = discriminator.train_on_batch(x=X_batch, y=real * (0.9))\n        # train_on_batch (predefined keras function) - Runs a single gradient update on a single batch of data.\n        # Pre train discriminator on  fake and real data  before starting the gan.\n        # helps to check if our compiled models run fine on our real data as well as the noised data.\n        \n       \n        # Fake Samples      \n                \n        z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n        # generate random noise as an input to initialize generator\n        \n        X_fake = generator.predict_on_batch(z)\n        # Generate fake MNIST images from noised input\n        \n        d_loss_fake = discriminator.train_on_batch(x=X_fake, y=fake)\n        # train discriminator on fake images generated by generator and fake data (array of 0 values)\n         \n        # Discriminator loss.... well what's this ?\n        # we only grabbed half the number of images that we specified with the real loss, \n        # we're take other half images from our generator for the other half of the batch:\n\n        \n        \n        d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])\n        \n        # Train Generator weights\n        \n        discriminator.trainable = False\n        # When we train the GAN we need to freeze the weights of the Discriminator. \n        # GAN is trained by alternating training of Discriminator and then training chained GAN model with Discriminator weights frozen\n        \n        # during training of gan weights of discriminator should be fixed We can enforce that by setting the trainable flag\n        \n        d_g_loss_batch = d_g.train_on_batch(x=z, y=real)\n        # training the GAN by alternating training of Discriminator \n        # training the chained GAN model with Discriminator\u2019s weights freezed\n        \n        # We'll now train the GAN with mislabeled generator outputs ([z=Noise] with [real i.e. 1]). \n        # That means we will generate images from noise and assign a label to one of them while training with the GAN\n        \n        # But Why ?\n        \n        # we are using the newly trained discriminator to improve generated output\n        # GAN loss is going to describe the confusion of discriminator from generated outputs.\n\n # Rest is for visualization   \n        print('epoch = %d\/%d, batch = %d\/%d, d_loss=%.3f, g_loss=%.3f' % \n            (e + 1, epochs, i, len(X_train) \/\/ batch_size, d_loss_batch, d_g_loss_batch[0]),100*' ',end='\\r')\n    \n    d_loss.append(d_loss_batch)\n    d_g_loss.append(d_g_loss_batch[0])\n    print('epoch = %d\/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], d_g_loss[-1]), 100*' ')\n\n    if e % 10 == 0:\n        samples = 10\n        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, latent_dim)))\n\n        for k in range(samples):\n            plt.subplot(2, 5, k+1)\n            plt.imshow(x_fake[k].reshape(28, 28), cmap='gray')\n            plt.xticks([])\n            plt.yticks([])\n\n        plt.tight_layout()\n        plt.show()","ce47eb5f":"plt.plot(d_loss)\nplt.plot(d_g_loss)\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Discriminator', 'Adversarial'], loc='center right')\nplt.show()","a6c0e951":"The first model is called a Generator and it aims to generate new data similar to the expected one.","f752df4d":"It's like Two player game: The Generator (forger) needs to learn how to create data in such a way that the Discriminator isn\u2019t able to distinguish it as fake anymore. The competition between these two teams is what improves their knowledge, until the Generator succeeds in creating realistic data.","fcafb021":" GAN = minimax game which G(generator) wants to minimize V while D(Discriminator) wants to maximize it.","21777f6b":"* Discriminator is a classifier trained using the supervised learning. \n* It classifies whether an image is real (1) or is fake (0).","7959cd56":"* Input to the generator is a series of randomly generated numbers called latent sample. \n* It tries to produce data that come from some probability distribution. \n* The generator network takes random noise as input, then runs that noise through a differentiable function to transform the noise and reshape it to have recognizable structure. \n* The output of the generator network ia a realistic image. Without training, the generator produces garbage images only.","1c3ca2e0":"## Training","0c01e896":"* we use both generaor and discriminator model here\n* Sets its trainability to False, meaning that during the adversarial training, it will not be training\n* Reason behind set trainable=False, Generator is consistently getting better, but discriminator will remain the same.","2fd96aa7":"## Generator","3f0ce362":"* When we feed a latent sample to the GAN, the generator internally produces a digit image which is then passed to the discriminator for classification. If the generator does a good job, the discriminator returns a value close to 1\n* However, the generator initially produces garbage images, and the loss value is high. So, the back-propagation updates the generator\u2019s weights to produce more realistic images as the training continues. ","a7455fa3":"### Workflow","31c7d237":"#### Basic Idea","3717e290":"**Generator try to fool by generating real-looking images**","7c0f707e":"**Discriminator try to distinguish between real and fake images**","1904549d":"1) Set the discriminator trainable\n\n2) Train the discriminator with the real MNIST digit images and the images generated by the generator to classify the real and fake images.\n\n3) Set the discriminator non-trainable\n\n4) Train the generator as part of the GAN. We feed latent samples into the GAN and let the generator to produce digit images and use the discriminator to classify the image","ec4fa68e":"## Descriminator"}}