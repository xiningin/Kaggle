{"cell_type":{"a0360e40":"code","01df9b90":"code","a9069de2":"code","dce83140":"code","4322c812":"code","194724e2":"code","3850ab62":"code","27cb4ccc":"code","cf5ae5f5":"code","20f402d0":"code","faf5f638":"code","b675b64a":"code","5fed4625":"code","7b5d1d45":"code","280a2839":"code","2837f8ad":"code","a567080d":"code","eb4e9dc3":"code","b9f0e87e":"code","b5df394a":"code","7c1e1a3d":"code","c334ffff":"code","eeadebd4":"code","3b6e93ea":"code","96b559ce":"code","a9b44ee4":"code","29658898":"code","e1b8e4f2":"markdown","9565c463":"markdown","1ca5cea5":"markdown","850a7656":"markdown","19000376":"markdown","89db8367":"markdown","6f8ea25e":"markdown","aac1a3f2":"markdown","33d41395":"markdown","6026ff9f":"markdown","52804aaa":"markdown","518806a7":"markdown","74d826b1":"markdown","8f5c0558":"markdown","3682544c":"markdown","e06d3119":"markdown","176c0e5b":"markdown","9e61312d":"markdown","dead1a63":"markdown","244c268d":"markdown","e77d32f3":"markdown","8b6a8c39":"markdown","d11a23ee":"markdown","a203fbef":"markdown","6b476c5b":"markdown","846f32e4":"markdown","ff305dea":"markdown","657b0555":"markdown","e2fd3181":"markdown"},"source":{"a0360e40":"!pip install transformers","01df9b90":"from transformers import pipeline, set_seed\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a9069de2":"generator = pipeline('text-generation', model='gpt2')\nset_seed(42)\ngenerator(\"Hello, I like to play cricket,\", max_length=60, num_return_sequences=7)","dce83140":"generator(\"The Indian man worked as a\", max_length=10, num_return_sequences=5)","4322c812":"generator(\"Machine learning is evolving technology\", max_length=10, num_return_sequences=5)","194724e2":"# Allocate a pipeline for sentiment-analysis\n#classifier = pipeline('sentiment-analysis')\nclassifier('The secret of getting ahead is getting started.')","3850ab62":"# Allocate a pipeline for question-answering\nquestion_answerer = pipeline('question-answering')\nquestion_answerer({\n    'question': 'What is the Newtons third law of motion?',\n    'context': 'Newton\u2019s third law of motion states that, \"For every action there is equal and opposite reaction\"'})","27cb4ccc":"#nlp = pipeline(\"question-answering\")\n\ncontext = r\"\"\"\nMicorsoft was founded by Bill gates and Paul allen in the year 1975.\nThe property of being prime (or not) is called primality.\nA simple but slow method of verifying the primality of a given number n is known as trial division.\nIt consists of testing whether n is a multiple of any integer between 2 and itself.\nAlgorithms much more efficient than trial division have been devised to test the primality of large numbers.\nThese include the Miller\u2013Rabin primality test, which is fast but has a small probability of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical.\nParticularly fast methods are available for numbers of special forms, such as Mersenne numbers.\nAs of January 2016, the largest known prime number has 22,338,618 decimal digits.\n\"\"\"\n\n#Question 1\nresult = nlp(question=\"What is a simple method to verify primality?\", context=context)\n\nprint(f\"Answer 1: '{result['answer']}'\")\n\n#Question 2\nresult = nlp(question=\"When did Bill gates founded Microsoft?\", context=context)\n\nprint(f\"Answer 2: '{result['answer']}'\")","cf5ae5f5":"unmasker = pipeline('fill-mask', model='bert-base-cased')\nunmasker(\"Hello, My name is [MASK].\")","20f402d0":"from transformers import pipeline\n","faf5f638":"#Summarization is currently supported by Bart and T5.\n\nsummarizer = pipeline(\"summarization\")\n\nARTICLE = \"\"\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.\nFirst conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,\nApollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. \nProject Mercury was followed by the two-man ProjectGemini (1962\u201366). \nThe first manned flight of Apollo was in 1968.\nApollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966. \nGemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions.\nApollo used Saturn family rockets as launch vehicles. \nApollo\/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973\u201374, and the Apollo\u2013Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\n \"\"\"\n\nsummary=summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)[0]\n\nprint(summary['summary_text'])","b675b64a":"# English to German\ntranslator_ger = pipeline(\"translation_en_to_de\")\nprint(\"German: \",translator_ger(\"Joe Biden became the 46th president of U.S.A.\", max_length=40)[0]['translation_text'])\n\n# English to French\ntranslator_fr = pipeline('translation_en_to_fr')\nprint(\"French: \",translator_fr(\"Joe Biden became the 46th president of U.S.A\",  max_length=40)[0]['translation_text'])","5fed4625":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft\/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft\/DialoGPT-medium\")\n\n# Let's chat for 5 lines\nfor step in range(5):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n\n    # pretty print last ouput tokens from bot\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))","7b5d1d45":"nlp_token_class = pipeline('ner') \nnlp_token_class('Ronaldo was born in 1985, he plays for Juventus and Portugal. ')","280a2839":"#from transformers import pipeline, set_seed\n\nclassifier_zsl = pipeline(\"zero-shot-classification\")\n\nsequence_to_classify = \"Bill gates founded a company called Microsoft in the year 1975\"\ncandidate_labels = [\"Europe\", \"Sports\",'Leadership','business', \"politics\",\"startup\"]\nclassifier_zsl(sequence_to_classify, candidate_labels)","2837f8ad":"import numpy as np\nnlp_features = pipeline('feature-extraction')\noutput = nlp_features('Deep learning is a branch of Machine learning')\nnp.array(output).shape   # output: (Samples, Tokens, Vector Size)","a567080d":"import ipywidgets as widgets\n\nnlp_qaA = pipeline('question-answering')\n\ncontext = widgets.Textarea(\n    value='Einstein is famous for the general theory of relativity',\n    placeholder='Enter something',\n    description='Context:',\n    disabled=False\n)\n\nquery = widgets.Text(\n    value='Why is Einstein famous for ?',\n    placeholder='Enter something',\n    description='Question:',\n    disabled=False\n)\n\ndef forward(_):\n    if len(context.value) > 0 and len(query.value) > 0: \n        output = nlp_qaA(question=query.value, context=context.value)            \n        print(output)\n\nquery.on_submit(forward)\ndisplay(context, query)","eb4e9dc3":"import tensorflow as tf\nfrom tensorflow.keras import activations, optimizers, losses\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\nimport pickle","b9f0e87e":" x = [\n     'Great customer service! The food was delicious! Definitely a come again.',\n     'The VEGAN options are super fire!!! And the plates come in big portions. Very pleased with this spot, I\\'ll definitely be ordering again',\n     'Come on, this place is family owned and operated, they are super friendly, the tacos are bomb.',\n     'This is such a great restaurant. Multiple times during days that we don\\'t want to cook, we\\'ve done takeout here and it\\'s been amazing. It\\'s fast and delicious.',\n     'Staff is really nice. Food is way better than average. Good cost benefit.',\n     'pricing for this, while relatively inexpensive for a Las Vegas attraction, is completely over the top.',\n     'At such a *fine* institution, I find the lack of knowledge and respect for the art appalling',\n     'If I could give one star I would...I walked out before my food arrived the customer service was horrible!',\n     'Wow the slowest drive thru I\\'ve ever been at WOWWWW. Horrible I won\\'t be coming back here ever again',\n     'Service: 1 out of 5 stars. They will mess up your order, not have it ready after 30 mins calling them before. Worst ran family business Ive ever seen.'\n]\n\ny = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]","b5df394a":"MODEL_NAME = 'distilbert-base-uncased'\nMAX_LEN = 20\n\nreview = x[0]\n\ntkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n\ninputs = tkzr(review, max_length=MAX_LEN, truncation=True, padding=True)\n\nprint(f'review: \\'{review}\\'')\nprint(f'input ids: {inputs[\"input_ids\"]}')\nprint(f'attention mask: {inputs[\"attention_mask\"]}')","7c1e1a3d":"def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n    return tkzr(x, max_length=max_len, truncation=trucation, padding=padding)\n    \nencodings = construct_encodings(x, tkzr, max_len=MAX_LEN)\n\n#The first stage of preprocessing is done! The second stage is converting our encodings and y (which holds the classes of the reviews) into a Tensorflow Dataset object. Below is a function to do this:\ndef construct_tfdataset(encodings, y=None):\n    if y:\n        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n    else:\n        # this case is used when making predictions on unseen samples after training\n        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n    \ntfdataset = construct_tfdataset(encodings, y)","c334ffff":"TEST_SPLIT = 0.2\nBATCH_SIZE = 2\n\ntrain_size = int(len(x) * (1-TEST_SPLIT))\n\ntfdataset = tfdataset.shuffle(len(x))\ntfdataset_train = tfdataset.take(train_size)\ntfdataset_test = tfdataset.skip(train_size)\n\ntfdataset_train = tfdataset_train.batch(BATCH_SIZE)\ntfdataset_test = tfdataset_test.batch(BATCH_SIZE)","eeadebd4":"N_EPOCHS = 2\n\nmodel = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\noptimizer = optimizers.Adam(learning_rate=3e-5)\nloss = losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\nmodel.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)","3b6e93ea":"benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\nprint(benchmarks)","96b559ce":"def create_predictor(model, model_name, max_len):\n    tkzr = DistilBertTokenizer.from_pretrained(model_name)\n    def predict_proba(text):\n        x = [text]\n\n        encodings = construct_encodings(x, tkzr, max_len=max_len)\n        tfdataset = construct_tfdataset(encodings)\n        tfdataset = tfdataset.batch(1)\n\n        preds = model.predict(tfdataset)\n        preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n        return preds[0][0]\n    \n    return predict_proba\n\nclf = create_predictor(model, MODEL_NAME, MAX_LEN)\nprint(clf('this restaurant has horrible food'))","a9b44ee4":"model.save_pretrained('.\/model\/clf')\nwith open('.\/model\/info.pkl', 'wb') as f:\n    pickle.dump((MODEL_NAME, MAX_LEN), f)","29658898":"new_model = TFDistilBertForSequenceClassification.from_pretrained('.\/model\/clf')\nmodel_name, max_len = pickle.load(open('.\/model\/info.pkl', 'rb'))\n\nclf = create_predictor(new_model, model_name, max_len)\nprint('Sentiment [pos, neg]: ',clf('this restaurant has poor ambiance.'))","e1b8e4f2":"### 1. Preprocessing the data","9565c463":"### Chatbot","1ca5cea5":"#### The transformers library allows you to benefits from large, pretrained language models without requiring a huge and costly computational infrastructure. Most of the State-of-the-Art models are provided directly by their author and made available in the library in PyTorch and TensorFlow in a transparent and interchangeable way.","850a7656":"### 5. Saving and loading the model for future use","19000376":"### 4. Using the fine-tuned model to predict new samples","89db8367":"### Zero-shot Learning\nZero-Shot learning method aims to solve a task without receiving any example of that task at training phase. The task of recognizing an object from a given image where there weren't any example images of that object during training phase can be considered as an example of Zero-Shot Learning task.","6f8ea25e":"### 2. Fine-tuning the model","aac1a3f2":"### Install Transformer","33d41395":"The third and final preprocessing step is to create training and test sets:","6026ff9f":"### Transformer\n\nIn \u201cAttention Is All You Need\u201d, Google introduce the Transformer, a novel neural network architecture based on a self-attention mechanism that we believe to be particularly well suited for language understanding.\n\nA Transformer network applies self-attention mechanism which scans through every word and appends attention scores(weights) to the words. The Transformer was introduced as a simple network architecture, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. ","52804aaa":"### Getting started on a task with a\u00a0pipeline\n\nThe easiest way to use a pre-trained model on a given task is to use pipeline(). \ud83e\udd17 Transformers provides the following tasks out of the box:\nSentiment analysis: is a text positive or negative?\n\n1. Text generation (in English): provide a prompt and the model will generate what follows.\n2. Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)\n3. Question answering: provide the model with some context and a question, extract the answer from the context.\n4. Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.\n5. Summarization: generate a summary of a long text.\n6. Language Translation: translate a text into another language.\n7. Feature extraction: return a tensor representation of the text.","518806a7":"### Text generation","74d826b1":"### Classifying text with DistilBERT and Tensorflow","8f5c0558":"### Text prediction","3682544c":"# This kernel demonstrates how to use Hugging Face's transformers package","e06d3119":"### Named Entity Recognition","176c0e5b":"### Text Summarization","9e61312d":"### 3. Testing the model\n\nNow we can use our test set to evaluate the performance of the model. ","dead1a63":"### English to German translation","244c268d":"### Features Extraction","e77d32f3":"### Problem statement\n\nLets consider a small corpus of 10 Yelp reviews: 5 positive (class 1) and 5 negative (class 0). BERT (and its variants like DistilBERT) can be a great tool to use when you have a shortage of training data. that said, don't expect great results with just 10 reviews! Interchanging x and y with your own dataset is recommended \ud83d\ude42","8b6a8c39":"### Using transformers in Widgets","d11a23ee":"### BERT\n\nThe BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\u2019s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\nThe abstract from the paper is the following:\n\n> We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","a203fbef":"Tasks:\n\n1. Preprocessing the data\n2. Fine-tuning the model\n3. Testing the model\n4. Using the fine-tuned model to predict new samples\n5. Saving and loading the model for future use","6b476c5b":"### Question Answering","846f32e4":"### GPT2\n\n#### Model description\n\n**GPT-2** is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.\n\n","ff305dea":"Apply this transformation to each review in our corpus. To do this we define a function construct_encodings, which maps the tokenizer to each review and aggregates them in encodings:","657b0555":"### Sentiment analysis","e2fd3181":"![Screenshot-2019-09-30-17.43.59.png](attachment:Screenshot-2019-09-30-17.43.59.png)"}}