{"cell_type":{"0f98d1a1":"code","4ed49e80":"code","93494b51":"code","e7b2a4ab":"code","56fafe89":"code","c985f2d5":"code","cbce37d6":"code","40b48d16":"code","b1b873cd":"code","4114f3f8":"code","41eeeba6":"code","a65eecef":"code","cf7de434":"code","009f759c":"code","685d9b3a":"code","8cfe3ba8":"code","9ad017bf":"code","880ee8b2":"markdown","a28bf686":"markdown","113a462a":"markdown","82eef49b":"markdown","d13ed6fb":"markdown","edea321c":"markdown"},"source":{"0f98d1a1":"%%time\n\nimport os\nimport logging\nimport sys\nimport time\nfrom datetime import timedelta\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport numpy as np\nimport pandas as pd\n\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB","4ed49e80":"%%time\n\n# Refer to https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro\n\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\". \n              format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","93494b51":"%%time\n\ndata_dir = \"..\/input\/tabular-playground-series-oct-2021\/\"\n\ntrain  = reduce_memory_usage(pd.read_csv(data_dir  + \"train.csv\"))\ntest = reduce_memory_usage(pd.read_csv(data_dir + \"test.csv\"))\nsubmission = reduce_memory_usage(pd.read_csv(data_dir + \"sample_submission.csv\"))","e7b2a4ab":"%%time\n\nprint(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape, end=\"\\n\\n\")","56fafe89":"def merge_preds_df(df, preds_files):\n    for preds_file in preds_files:\n        df_tmp = pd.read_csv(preds_dir + preds_file)\n        df = df.merge(df_tmp, on=\"id\", how=\"left\")\n    return df\n\ndef build_preds_file_names(preds):\n    test_files = list()\n    train_files = list()\n    \n    for pred in preds:\n        test_files.append(f\"{pred}_test.csv\")\n        train_files.append(f\"{pred}_train.csv\")\n        \n    return (test_files, train_files)","c985f2d5":"%%time\n\npreds_dir = \"..\/input\/tps-october-2021-predictions\/\"\n\nTARGET = \"target\"\n\nfeatures = [\n    #\"cb1\", \"hgb1\", \"lgb1\", \"vote1\",\n    \n    \"cb2\", \"hgb2\", \"lgb2\",\n    \n    #\"cb3\", \"hgb3\", \"lgb3\",\n    \n    #\"cb4\", \"hgb4\", \"lgb4\",\n    \n    #\"cb5\", \"hgb5\", \"lgb5\",\n    \n    \"cb6\", \"hgb6\", \"lgb6\",\n    \n    #\"cb7\", \"hgb7\", \"lgb7\",\n    \n    \"cb8\", \"hgb8\", \"lgb8\",\n]\n\npreds_test_files, preds_train_files = build_preds_file_names(features)\n\ntest = merge_preds_df(test, preds_test_files)\ntrain = merge_preds_df(train, preds_train_files)","cbce37d6":"train[features].head()","40b48d16":"test[features].head()","b1b873cd":"%%time\n\ndef load_vis_data(preds_files):\n    # Read test preds files\n    df_list = []\n    for preds_file in preds_files:\n        df_tmp = pd.read_csv(f\"{preds_dir}\/{preds_file}\")\n        # rename preds column \n        df_tmp.columns = [\"id\", TARGET]\n        df_list.append((preds_file, df_tmp))\n        \n    hist_data = []\n    for i in range(len(df_list)):\n        _, df = df_list[i]\n        hist_data.append(df[TARGET])\n        \n    return hist_data","4114f3f8":"%%time\n\nvis_data = load_vis_data(preds_test_files)","41eeeba6":"%%time\n\nfig1= ff.create_distplot(vis_data, preds_test_files, bin_size=0.3, show_hist=False, show_rug=False)\nfig1.show()","a65eecef":"%%time\n\nfig2 = px.imshow(np.corrcoef(vis_data), x=preds_test_files, y=preds_test_files)\nfig2.show()","cf7de434":"%%time\n\ny = train.pop(TARGET)\nX = train[[\"id\"] + features].copy()\nX_test = test[features].copy()\n\ndel train\ndel test","009f759c":"%%time\n\nlr1_params = {\n    \"random_state\": 42, \n    \"solver\": \"saga\"\n}\n\nlr2_params = {\n    \"fit_intercept\": True, \n    \"random_state\": 42, \n    \"solver\": \"sag\", \n}\n\ngnb1_params = {}\n\n# Model name must be unique\nmodels = [\n    (\"lr1\", LogisticRegression(**lr1_params)),\n    (\"lr2\", LogisticRegression(**lr2_params)),\n    (\"gnb1\", GaussianNB(**gnb1_params)),\n]","685d9b3a":"%%time\n\ndef predict_with_model(model, simple_fit=False, splits=5):\n    test_preds = []\n    valid_preds = {}\n    scores = []\n    selected_cols = []\n    \n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n    for fold, (idx_train, idx_valid) in enumerate(skf.split(X, y)):\n        start_time = time.monotonic()\n        \n        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n        \n        valid_ids = X_valid.id.values.tolist()\n\n        X_train = X_train[features]\n        X_valid = X_valid[features]\n\n        if simple_fit:\n            model.fit(X_train, y_train)\n        else:\n            model.fit(\n                X_train, y_train,\n                eval_set=[(X_valid, y_valid)],\n                early_stopping_rounds=180,\n                verbose=1000\n            )\n        \n        valid_pred = model.predict_proba(X_valid)[:, 1]\n        test_pred = model.predict_proba(X_test)[:, 1]\n        \n        test_preds.append(test_pred)\n        valid_preds.update(dict(zip(valid_ids, valid_pred)))\n\n        score = roc_auc_score(y_valid, valid_pred)\n        \n        end_time = time.monotonic()\n        dur = timedelta(seconds=end_time - start_time)\n        print(f\"Fold {fold} | AUC: {score} | Took: {dur}\")\n        scores.append(score)\n    \n    test_preds = np.mean(np.column_stack(test_preds), axis=1)\n    valid_preds = pd.DataFrame.from_dict(valid_preds, orient=\"index\").reset_index()\n    \n    return test_preds, valid_preds, scores","8cfe3ba8":"%%time\n\ndef predict_with_models(models):\n    print(f\"Predicting with {len(models)} models...\", end=\"\\n\\n\")\n    for model_name, model in models:\n        start_time = time.monotonic()\n        \n        print(\"-\" * 50)\n        print(f\"Using {model_name} model...\")\n        test_preds, valid_preds, scores = predict_with_model(model, simple_fit=True)\n        print(f\"Score: {np.mean(scores)}, Std: {np.std(scores)}\", end=\"\\n\\n\")\n\n        print(\"Saving predictions...\")\n        valid_preds.columns = [\"id\", model_name]\n        valid_preds.to_csv(f\"{model_name}_train.csv\", index=False)\n\n        test_preds_df = pd.DataFrame({\"id\": submission.id, model_name: test_preds})\n        test_preds_df.to_csv(f\"{model_name}_test.csv\", index=False)\n\n        sub = pd.DataFrame({\"id\": submission.id, TARGET: test_preds})\n        sub.to_csv(f\"{model_name}_submission.csv\", index=False)\n        \n        end_time = time.monotonic()\n        dur = timedelta(seconds=end_time - start_time)\n        print(f\"Took: {dur}\")","9ad017bf":"%%time\n\npredict_with_models(models)","880ee8b2":"## Merge predictions to main data frames","a28bf686":"## Predict","113a462a":"## Load datasets","82eef49b":"## Visualize correlations","d13ed6fb":"## Import libraries","edea321c":"## \ud83c\uddf5\ud83c\udded TPS Oct. 2021 - Pinoy blends \ud83d\udd25"}}