{"cell_type":{"8c79c8e6":"code","e7679933":"code","5e3f62e0":"code","23a3d394":"code","52984dae":"code","494f618c":"code","254c600e":"code","8796db71":"code","e543b479":"code","964cfbca":"code","74e1be4e":"code","469357d6":"code","054fd890":"code","e9f20560":"code","20529181":"code","62147cdf":"code","fc64bee6":"code","495333f9":"code","edc2e653":"code","c4151c8a":"code","50e93138":"code","a9ac14f5":"code","8364bde8":"code","ba80bc06":"code","787c7164":"code","300aa564":"code","a66b38d2":"code","f8468a38":"code","f65625e4":"code","6bbd79ae":"code","ff567be5":"code","ef4c2e18":"code","86b54c50":"code","6faec560":"code","0625e7cc":"code","770e44a9":"code","f834759b":"code","d37a42b3":"code","03724aea":"code","2127d801":"code","c8cd7d3c":"code","e0e83c94":"code","a9e3b24c":"code","00925686":"code","d5cf390d":"code","146cf1a0":"code","6501a5bd":"code","5364b805":"code","89179423":"code","0045d4ed":"code","c3a0d9ee":"code","fff2cad1":"code","f18b0508":"code","ca680b13":"code","59918e89":"code","c3bb6f58":"code","3de1fa8a":"code","5bcaddf4":"code","bc037c0e":"code","6f357b3b":"code","5f5b6859":"code","f5e0f327":"code","57b9cd9f":"markdown","670ec859":"markdown","0b54e8bb":"markdown","40710ad6":"markdown","4525db7d":"markdown","47ae4d86":"markdown","e3b5b058":"markdown","0352376a":"markdown","c8abc64c":"markdown","b92b53f7":"markdown","fa40c57d":"markdown","e6a8dea7":"markdown"},"source":{"8c79c8e6":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for data cleaning\nimport string\nfrom wordcloud import WordCloud,STOPWORDS\nfrom matplotlib.lines import Line2D\n# for stopwords Removal\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n# for calculating Polarity and Subjectivity\nfrom textblob import TextBlob\nfrom nltk.stem.porter import PorterStemmer\n# for regular expressions\n# for calculating Polarity and Subjectivity\nfrom textblob import TextBlob\n\n# for data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\npalette = [\"#9bf6ff\",\"#ffadad\"]\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})","e7679933":"df_vall = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ndf_vall.head()","5e3f62e0":"df_vall.shape","23a3d394":"less_toxic = pd.DataFrame()\nless_toxic['text'] = df_vall['less_toxic'].tolist()\nless_toxic['label'] = \"Less Toxic\"\n\nmore_toxic = pd.DataFrame()\nmore_toxic['text'] = df_vall['more_toxic'].tolist()\nmore_toxic['label'] = \"More Toxic\"\n\ntoxicity_text = pd.concat([less_toxic, more_toxic], ignore_index=True)\ntoxicity_text.head()","52984dae":"# First lets remove Punctuations from the Reviews\ndef punctuation_removal(messy_str):\n    clean_list = [char for char in messy_str if char not in string.punctuation]\n    clean_str = ''.join(clean_list)\n    return clean_str\n\ntoxicity_text['text'] = toxicity_text['text'].apply(punctuation_removal)","494f618c":"# lets make a function to remove Numbers from the reviews\nimport re\ndef drop_numbers(list_text):\n    list_text_new = []\n    for i in list_text:\n        if not re.search('\\d', i):\n            list_text_new.append(i)\n    return ''.join(list_text_new)\n\ntoxicity_text['text'] = toxicity_text['text'].apply(drop_numbers)","254c600e":"toxicity_text['text'].head(10) ","8796db71":"df_com = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndf_com.head()","e543b479":"submission_csv = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\")\nsubmission_csv.head()\n","964cfbca":"# lets check the Descriptive Summary of the Dataset\ntoxicity_text.describe()","74e1be4e":"# lets check the summary of Date, Variation and Reviews\ntoxicity_text.describe(include = 'object')","469357d6":"# lets check the Value Counts for Variation \ntoxicity_text['text'].value_counts()","054fd890":"# Lets calculate the length of the Reviews\ntoxicity_text['length'] = toxicity_text['text'].apply(len)","e9f20560":"# Lets calculate the Polarity of the Reviews\ndef get_polarity(text):\n    textblob = TextBlob(str(text.encode('utf-8')))\n    pol = textblob.sentiment.polarity\n    return pol\n\n# lets apply the function\ntoxicity_text['polarity'] = toxicity_text['text'].apply(get_polarity)","20529181":"# Lets calculate the Subjectvity of the Reviews\ndef get_subjectivity(text):\n    textblob = TextBlob(str(text.encode('utf-8')))\n    subj = textblob.sentiment.subjectivity\n    return subj\n\n# lets apply the Function\ntoxicity_text['subjectivity'] = toxicity_text['text'].apply(get_subjectivity)","62147cdf":"## lets summarize the Newly Created Features\ntoxicity_text[['length','polarity','subjectivity']].describe()","fc64bee6":"# Lets calculate the Polarity of the Reviews\ndef get_polarity(text):\n    textblob = TextBlob(str(text.encode('utf-8')))\n    pol = textblob.sentiment.polarity\n    return pol\n\n# lets apply the function\ntoxicity_text['polarity'] = toxicity_text['text'].apply(get_polarity)","495333f9":"# Lets calculate the Subjectvity of the Reviews\ndef get_subjectivity(text):\n    textblob = TextBlob(str(text.encode('utf-8')))\n    subj = textblob.sentiment.subjectivity\n    return subj\n\n# lets apply the Function\ntoxicity_text['subjectivity'] = toxicity_text['text'].apply(get_subjectivity)","edc2e653":"## Visualizing Polarity and Subjectivity\n\nplt.rcParams['figure.figsize'] = (10, 4)\n\nplt.subplot(1, 2, 1)\nsns.distplot(toxicity_text['polarity'])\n\nplt.subplot(1, 2, 2)\nsns.distplot(toxicity_text['subjectivity'])\n\nplt.suptitle('Distribution of Polarity and Subjectivity')\nplt.show()","c4151c8a":"# lets check relation between Polarity and Subjectivity\n\nsns.scatterplot(toxicity_text['polarity'], toxicity_text['subjectivity'])\nplt.title('Polarity vs Subjectivity')\nplt.show()","50e93138":"## Visualizing the Most Frequent Words\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ncv = CountVectorizer(stop_words = 'english')\nwords = cv.fit_transform(toxicity_text['text'])\nsum_words = words.sum(axis=0)\n\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\nplt.style.use('fivethirtyeight')\ncolor = plt.cm.ocean(np.linspace(0, 1, 20))\nfrequency.head(20).plot(x='word', y='freq', kind='bar', figsize=(15, 6), color = color)\nplt.title(\"Most Frequently Occuring Words - Top 20\")\nplt.show()","a9ac14f5":"## Visualizing the Least Frequent Words\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ncv = CountVectorizer(stop_words = 'english')\nwords = cv.fit_transform(toxicity_text['text'])\nsum_words = words.sum(axis=0)\n\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\nplt.style.use('fivethirtyeight')\ncolor = plt.cm.ocean(np.linspace(0, 1, 20))\nfrequency.tail(20).plot(x='word', y='freq', kind='bar', figsize=(15, 6), color = color)\nplt.title(\"Least Frequently Occuring Words - Top 20\")\nplt.show()","8364bde8":"# Visualizing the BiGrams\n #load in all the modules we're going to need\nimport nltk\nimport collections\n\n# function for making ngrams\nfrom nltk.util import ngrams \ntext = str(toxicity_text['text'])\ntokenized = text.split()\n\n# and get a list of all the bi-grams\nesBigrams = ngrams(tokenized, 2)\n\n# get the frequency of each bigram in our corpus\nesBigramFreq = collections.Counter(esBigrams)\n\n# what are the ten most popular ngrams in this Spanish corpus?\nesBigramFreq.most_common(10)","ba80bc06":"# lets plot the Wordscloud\n\ncv = CountVectorizer(stop_words = 'english')\nwords = cv.fit_transform(toxicity_text['text'])\nsum_words = words.sum(axis=0)\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n\nwordcloud = WordCloud(background_color = 'black', width = 2000, height = 2000).generate_from_frequencies(dict(words_freq))\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.title(\"Vocabulary from Reviews\", fontsize = 20)\nplt.show()","787c7164":"# lets check the Distribution of Rating and Useful Count\n\nplt.rcParams['figure.figsize'] = (15, 4)\n\nplt.subplot(1, 2, 1)\nsns.distplot(toxicity_text['polarity'])\n\nplt.subplot(1, 2, 2)\nsns.distplot(toxicity_text['subjectivity'])\n\nplt.suptitle('Distribution of Rating and Useful Count \\n ', fontsize = 20)\nplt.show()\n\n","300aa564":"# lets check the Impact of Ratings on Usefulness\n\nplt.rcParams['figure.figsize'] = (15, 4)\nsns.barplot(toxicity_text['polarity'], toxicity_text['subjectivity'], palette = 'hot')\nplt.grid()\nplt.xlabel('\\n Ratings')\nplt.ylabel('Count\\n', fontsize = 20)\nplt.title('\\n Rating vs Usefulness \\n', fontsize = 20)\nplt.show()","a66b38d2":"## Cleaning the Data\n\ncorpus = []\n\nfor i in range(0, 3150):\n    review = re.sub('[^a-zA-Z]', ' ', toxicity_text['text'][i])  ## Removing all Unecessary items\n    review = review.lower()                                         ## Converting into Lower Case\n    review = review.split()\n    ps = PorterStemmer()                                            ## Stemming\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]  ## Removing Stopwords\n    review = ' '.join(review)\n    corpus.append(review)","f8468a38":"# as it is clear that the reviews have so many unnecassry things such as Stopwords, Punctuations, numbers etc\n\n# First lets remove Punctuations from the Reviews\ndef punctuation_removal(messy_str):\n    clean_list = [char for char in messy_str if char not in string.punctuation]\n    clean_str = ''.join(clean_list)\n    return clean_str\n\ntoxicity_text['text'] = toxicity_text['text'].apply(punctuation_removal)","f65625e4":"# Now lets Remove the Stopwords also\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nstop = stopwords.words('english')\nstop.append(\"i'm\")\n\nstop_words = []\n\nfor item in stop: \n    new_item = punctuation_removal(item)\n    stop_words.append(new_item) \n\ndef stopwords_removal(messy_str):\n    messy_str = word_tokenize(messy_str)\n    return [word.lower() for word in messy_str \n            if word.lower() not in stop_words ]\n\ntoxicity_text['text'] = toxicity_text['text'].apply(stopwords_removal)","6bbd79ae":"# lets remove the Numbers also\n\nimport re\ndef drop_numbers(list_text):\n    list_text_new = []\n    for i in list_text:\n        if not re.search('\\d', i):\n            list_text_new.append(i)\n    return ' '.join(list_text_new)\n\ntoxicity_text['text'] = toxicity_text['text'].apply(drop_numbers)","ff567be5":"# for using Sentiment Analyzer we will have to dowload the Vader Lexicon from NLTK\n\nimport nltk\nnltk.download('vader_lexicon')","ef4c2e18":"# lets calculate the Sentiment from Reviews\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\ntrain_sentiments = []\n\nfor i in toxicity_text['text'] :\n    train_sentiments.append(sid.polarity_scores(i).get('compound'))\n    \ntrain_sentiments = np.asarray(train_sentiments)\ntoxicity_text['sentiment'] = pd.Series(data=train_sentiments)\ntoxicity_text['sentiment'] ","86b54c50":"from nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef lemmatise(text):\n    text_tokens = word_tokenize(text)\n    text_lemm = [lemmatizer.lemmatize(word) for word in text_tokens]\n    return ' '.join(text_lemm)\n\ntoxicity_text['text']  = toxicity_text['text'] .apply(lemmatise)","6faec560":"toxicity_text['text'] .values","0625e7cc":"# Lets calculate the Polarity of the Reviews\ndef get_polarity(text):\n    textblob = TextBlob(str(text))\n    pol = textblob.sentiment.polarity\n    if(pol==0):\n        return \"Neutral\"\n    elif(pol>0 and pol<=0.3):\n        return \"Weakly Positive\"\n    elif(pol>0.3 and pol<=0.6):\n        return \"Positive\"\n    elif(pol>0.6 and pol<=1):\n        return \"Strongly Positive\"\n    elif(pol>-0.3 and pol<=0):\n        return \"Weakly Negative\"\n    elif(pol>-0.6 and pol<=-0.3):\n        return \"Negative\"\n    elif(pol>-1 and pol<=-0.6):\n        return \"Strongly Negative\"\n    \ntoxicity_text['polarity'] = toxicity_text['text'] .apply(get_polarity)","770e44a9":"toxicity_text['polarity'].value_counts()","f834759b":"# color function for the wordcloud\ndef color_wc(word=None,font_size=None,position=None, orientation=None,font_path=None, random_state=None):\n    h = int(360.0 * 255.0 \/ 255.0)\n    s = int(190.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(40, 80)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\n\nfig = plt.gcf()\nfig.set_size_inches(16, 8)\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"black\", contour_width=2, contour_color='orange',width=1500, height=750,color_func=color_wc ,max_words=150, max_font_size=256,random_state=42)\nwc.generate(' '.join(less_toxic['text']))\nfig = plt.imshow(wc, interpolation=\"bilinear\")\nfig = plt.axis('off')","d37a42b3":"fig = plt.gcf()\nfig.set_size_inches(16, 8)\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"black\", contour_width=2, contour_color='read',width=1500, height=750,color_func=color_wc,max_words=150, max_font_size=256,random_state=42)\nwc.generate(' '.join(more_toxic['text']))\nfig = plt.imshow(wc, interpolation=\"bilinear\")\nfig = plt.axis('off')","03724aea":"import os\nimport warnings\nwarnings.filterwarnings(\"ignore\")                     #Ignoring unnecessory warnings\n\nimport numpy as np                                  #for large and multi-dimensional arrays\nimport pandas as pd                                 #for data manipulation and analysis\nimport nltk                                         #Natural language processing tool-kit\n\nfrom nltk.corpus import stopwords                   #Stopwords corpus\nfrom nltk.stem import PorterStemmer                 # Stemmer\n\nfrom sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\nfrom sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\nfrom gensim.models import Word2Vec                                   #For Word2Vec\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense","2127d801":"train_file = \"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\"\ntest_file = \"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\"\ncleaned_text = \"..\/input\/cleaned-toxic-comments\/train_preprocessed.csv\"\nsubmission = \"..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\"","c8cd7d3c":"cleaned_text_df = pd.read_csv(cleaned_text)\ncleaned_text_df","e0e83c94":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\n\ncombined_comments =  pd.read_csv(cleaned_text).comment_text.tolist()","a9e3b24c":"for i in range(500,510):\n    print(combined_comments[i])\n    print('--------------------------------------------------------------------------------')","00925686":"# df_x = combined_comments\nvoc_size=5000\nonehot_repr=[one_hot(words,voc_size)for words in combined_comments] \ntype(onehot_repr)\nsent_length=400\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)","d5cf390d":"## Creating model\nembedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nprint(model.summary())","146cf1a0":"from sklearn.preprocessing import LabelEncoder\nencode = LabelEncoder()\ndf_y2 = encode.fit_transform(pd.read_csv(cleaned_text)['toxicity'])\ntype(df_y2)","6501a5bd":"import numpy as np\nX_final=np.array(embedded_docs)\ny_final=np.array(df_y2)","5364b805":"print(X_final.shape)\nprint(y_final.shape)","89179423":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)","0045d4ed":"#we are feeding the \nmodel.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=5,batch_size=64)","c3a0d9ee":"score = model.evaluate(X_test,Y_test)\nscore","fff2cad1":"y_pred_model = model.predict(X_test)\ny_pred_model","f18b0508":"# print(\"Accuracy: %.2f%%\" % (score[1]*100))\n# diff = Y_test - y_pred_model\nmae = np.mean(abs(Y_test - model.predict(X_test)))\nmse = np.mean((Y_test - model.predict(X_test))**2)\nrmse = np.sqrt(mse)\nprint(mae)\nprint(mse)\nprint(rmse)","ca680b13":"from keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n\nlstm_cnn=Sequential()\nlstm_cnn.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nlstm_cnn.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nlstm_cnn.add(MaxPooling1D(pool_size=2))\nlstm_cnn.add(LSTM(100))\nlstm_cnn.add(Dense(1))\nlstm_cnn.compile(loss='mean_squared_error', optimizer='adam')\nprint(lstm_cnn.summary())","59918e89":"lstm_cnn.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=5,batch_size=64)","c3bb6f58":"score = model.evaluate(X_test,Y_test)\nscore","3de1fa8a":"y_pred_model = model.predict(X_test)\ny_pred_model","5bcaddf4":"# print(\"Accuracy: %.2f%%\" % (score[1]*100))\n# diff = Y_test - y_pred_model\nmae = np.mean(abs(Y_test - lstm_cnn.predict(X_test)))\nmse = np.mean((Y_test - lstm_cnn.predict(X_test))**2)\nrmse = np.sqrt(mse)\nprint(mae)\nprint(mse)\nprint(rmse)","bc037c0e":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport numpy as np\n\ntrain_df = pd.read_csv(train_file)\ncomments=[str(x) for x in train_df['less_toxic'].tolist()+train_df['more_toxic'].tolist()]\ndf_t = pd.DataFrame({'comments':comments})\n\n\nsnow = nltk.stem.SnowballStemmer('english')\n\ncorpus = []\nfor i in range(0, len(df_t)):\n    review = re.sub('[^a-zA-Z]', ' ', df_t['comments'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [snow.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)\n    \n# df_x = combined_comments\nvoc_size=5000\nonehot_repr=[one_hot(words,voc_size)for words in corpus] \ntype(onehot_repr)\n\nsent_length=400\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)\n\nsub_test =np.array(embedded_docs)","6f357b3b":"sub_pred = model.predict(sub_test)","5f5b6859":"sub = pd.read_csv(submission)\nsub['score'] = sub['score'].rank(method='first')\nsub.to_csv('submission.csv', index=False)","f5e0f327":"sub.head(10)","57b9cd9f":"## Cleaning the Text","670ec859":"## Creating model","0b54e8bb":"## 4. Feature Extraction","40710ad6":"## Show Less Toxic Comments","4525db7d":"## Modeling","47ae4d86":"### Text Polarity &  Text Subjectivity","e3b5b058":"### Calculating the Sentiment from Reviews","0352376a":"# Lets calculate the Polarity of the Reviews","c8abc64c":"## Visualizing the Most Frequent Words","b92b53f7":"##  Feature Engineering","fa40c57d":"## Visualizing the Least Frequent Words","e6a8dea7":"## Cleaning Data"}}