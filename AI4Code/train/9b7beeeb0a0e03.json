{"cell_type":{"466006f3":"code","798d36ea":"code","5acb287b":"code","cbf31b13":"code","43bbcb3c":"code","9b4e7cca":"code","8bc5266d":"code","befde4a2":"code","cdf4c874":"code","7d85ff0c":"code","59d3b66a":"code","cce66457":"code","be533e04":"code","77e734bf":"code","f1bd1365":"code","33a76c9a":"code","fff5b6f3":"code","a8557e93":"code","b50bd1c1":"code","8babcfb0":"code","81b6a4da":"code","e289c25b":"code","136543a2":"code","4a210dd6":"code","fbc79c65":"code","b33c7843":"code","599bf827":"code","de65452a":"code","9655f05a":"code","cf9ddc75":"code","2f217fb6":"code","0f41ab8e":"code","d9aec0c4":"code","cda1e300":"code","dd36aef0":"code","9c27aebf":"code","36c7c5f0":"code","1ad0c12b":"code","1e8c37e7":"code","6b160391":"code","094dd60e":"code","94878a80":"code","60c6ae38":"code","866fe655":"code","ba0655ee":"code","4a7e9f6c":"code","23dde99a":"code","fb7749af":"code","54c29f4c":"code","8a87be65":"code","023a20c2":"code","b7c8d4d7":"code","06110da5":"code","ac0c0e62":"code","444af055":"code","323937f2":"code","1a45434e":"code","65380d9e":"code","1a3a53ad":"code","77b18004":"code","8a13674d":"code","7d567fff":"code","2812a02f":"code","e45fca0a":"code","bef7c393":"code","5ffab7b4":"code","4fc3387f":"markdown","75774670":"markdown","3c51b697":"markdown","fe4d66ea":"markdown","6eec128a":"markdown","c8c5f716":"markdown","8849d73e":"markdown","02b026a9":"markdown","d54ad186":"markdown","4eded1ca":"markdown","2a5d6c60":"markdown","eea446ea":"markdown","0906c3e7":"markdown","7be1884f":"markdown","0b32edc0":"markdown","3a856074":"markdown","8f3f7f7d":"markdown","1236d122":"markdown","4d56eda3":"markdown","34835b3f":"markdown","d4998d8c":"markdown","e5432073":"markdown","f0142da0":"markdown","1704231d":"markdown","efb381e6":"markdown","24a4cac6":"markdown","1f9629ed":"markdown","30438c62":"markdown","2e8bb675":"markdown","d421c3cc":"markdown","3046c450":"markdown","7a7baebc":"markdown","a611f75d":"markdown","282340f7":"markdown","d7bcc45a":"markdown","5ad19b1e":"markdown","e5d67ef5":"markdown","49ffb4f6":"markdown","05a53adf":"markdown","f7fc398e":"markdown","cd8bbe8b":"markdown","27efadd7":"markdown","4ffbf542":"markdown","bc150cf0":"markdown"},"source":{"466006f3":"\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport re\n\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n\nfrom collections import Counter\n","798d36ea":"train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data  = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","5acb287b":"print('There are {} rows and {} columns in train'.format(train_data.shape[0],train_data.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test_data.shape[0],test_data.shape[1]))","cbf31b13":"train_data.columns","43bbcb3c":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',str(text))\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n\ndef remove_punctuation(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","9b4e7cca":"#Removing the URL\ntrain_data['text'] = train_data['text'].apply(lambda x : remove_URL(x))\ntest_data['text'] = test_data['text'].apply(lambda x : remove_URL(x))\n\n\n#Removing the emoji\ntrain_data['text'] = train_data['text'].apply(lambda x : remove_emoji(x))\ntest_data['text'] = test_data['text'].apply(lambda x : remove_emoji(x))\n\n#Removing the html_tags\ntrain_data['text'] = train_data['text'].apply(lambda x : remove_html(x))\ntest_data['text'] = test_data['text'].apply(lambda x : remove_html(x))\n\n#Removing the puntuations\ntrain_data['text'] = train_data['text'].apply(lambda x : remove_punctuation(x))\ntest_data['text'] = test_data['text'].apply(lambda x : remove_punctuation(x))\n","8bc5266d":"#Creating a test data for checking the model performance on unseen data\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_data, train_data['target'], test_size=0.10, random_state=42)\n","befde4a2":"vectorizer = TfidfVectorizer(max_features=500)  ## we are only considering the words that occurs more than 10 times my setting df=10\ntfidf = vectorizer.fit(x_train['text'])  ## Note that the fit has to happens only in the train data\n\nx_train_tfidf = tfidf.transform(x_train['text'])\nx_test_tfidf = tfidf.transform(x_test['text'])\n\ntest_tfidf  = tfidf.transform(test_data['text'])\n\n\nprint(\"Shape of the train embedded vector is {}\".format(x_train_tfidf.shape))\nprint(\"Shape of the test embedded vector is {}\".format(x_test_tfidf.shape))\n\nprint(type(y_train))","cdf4c874":"parameters = {'kernel':('rbf',), 'C':[1,2,5,10]}\n\nsvc = SVC()\nclf = GridSearchCV(svc, parameters, cv = 4, scoring = 'f1')\nclf.fit(x_train_tfidf, y_train)\n","7d85ff0c":"print(\"Average of the best f1-score in every fold during cross validation = \",clf.best_score_)\nprint(\"The best parameters found during k-fold cross validation is = \",clf.best_params_)","59d3b66a":"y_pred = clf.predict(x_test_tfidf)","cce66457":"#Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud for this function\n\ndef plot_cm(y_true, y_pred, title, figsize=(7,7)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","be533e04":"#Ploting the confusion matrix\nplot_cm(y_test, y_pred, \"Confusion Matrix on unseen data using TF-IDf + SVM\")\n\nfrom sklearn.metrics import f1_score\nprint(\"The F1-Score on the test data is = \",f1_score(y_test, y_pred))","77e734bf":"## Lets build our hyperparameter search grid\n## since we have a large space of hyperparamter we will use randomisedSearchCV\n\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 400, num = 5)]\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 20, num = 5)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\n\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 5, cv = 4,scoring = 'f1')\nrf_random.fit(x_train_tfidf, y_train)","f1bd1365":"print(\"Average of the best f1-score in various folds during cross validation = \",rf_random.best_score_)\nprint(\"The best parameters found during k-fold cross validation is = \",rf_random.best_params_)","33a76c9a":"y_pred = rf_random.predict(x_test_tfidf)","fff5b6f3":"#Ploting the confusion matrix\nplot_cm(y_test, y_pred, \"Confusion Matrix on unseen data using TF-IDf + RandomForest\")\n\nfrom sklearn.metrics import f1_score\nprint(\"The F1-Score on the test data is = \",f1_score(y_test, y_pred))","a8557e93":"xgb_model = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600)\n\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,10,20,40]\n        }\n\nclf = RandomizedSearchCV(xgb_model, param_distributions=params, n_iter=10, scoring='f1', cv=4)\nclf.fit(x_train_tfidf, y_train)\n","b50bd1c1":"print(\"Average of the best f1-score in various folds during cross validation = \",clf.best_score_)\nprint(\"The best parameters found during k-fold cross validation is = \",clf.best_params_)","8babcfb0":"y_pred = clf.predict(x_test_tfidf)","81b6a4da":"#Ploting the confusion matrix\nplot_cm(y_test, y_pred, \"Confusion Matrix on unseen data using TF-IDf + XgBoost\")\n\nfrom sklearn.metrics import f1_score\nprint(\"The F1-Score on the test data is = \",f1_score(y_test, y_pred))","e289c25b":"model = keras.Sequential()\n\nmodel.add(layers.LSTM(256, activation='relu', kernel_initializer='he_normal', input_shape=(1,500), return_sequences=True))\n\nmodel.add(layers.LSTM(100, activation='relu', kernel_initializer='he_normal', return_sequences=True))\n\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary() ","136543a2":"model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])","4a210dd6":"model_input = x_train_tfidf.toarray()\nmodel_input = model_input[:, None, :]\nmodel_label =  y_train[:, None]\nprint(model_input.shape)\nhistory = model.fit(model_input, y_train,\n                    batch_size=64,\n                    epochs=3,\n                    validation_split=0.2)","fbc79c65":"test_input = x_test_tfidf.toarray()\ntest_input = test_input[:, None, :]\ny_pred = model.predict(test_input)\ny_pred = y_pred.round().astype(int)","b33c7843":"#Ploting the confusion matrix\nplot_cm(y_test, y_pred[:, 0, 0], \"Confusion Matrix on unseen data using TF-IDf + Sequential Model With LSTM\")\n\nfrom sklearn.metrics import f1_score\nprint(\"The F1-Score on the test data is = \",f1_score(y_test, y_pred[:, 0, 0]))","599bf827":"## Making a dictionary of the words and their vector representation\n\nembeddings_index = {}\nf = open('\/kaggle\/input\/glove840b300dtxt\/glove.840B.300d.txt')\nfor line in f:\n    values = line.split(' ')\n    word = values[0] ## The first entry is the word\n    coefs = np.asarray(values[1:], dtype='float32') ## These are the vectors representing the embedding for the word\n    embeddings_index[word] = coefs\nf.close()\n\n\nprint('GloVe data loaded')","de65452a":"glove_words =  set(embeddings_index.keys())\n\n'''\nBelow is a uliity function that takes sentenes as a input and return the vector representation of the same\nMethod adopted is similar to average word2vec. Where i am summing up all the vector representation of the words from the glove and \nthen taking the average by dividing with the number of words involved\n'''\n\ndef convert_sen_to_vec(text):\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\n    for word in sentence.split():\n        if word in glove_words:\n            vector += embeddings_index[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    return vector\n\n","9655f05a":"glove_train = []  ## this will be of size num_of_train_points*300\nglove_test = []   ## this will be of size num_of_test_points*300\n\nfor sentence in x_train['text']:\n    sen_vec = convert_sen_to_vec(sentence)\n    glove_train.append(sen_vec)\n    \nfor sentence in x_test['text']:\n    sen_vec = convert_sen_to_vec(sentence)\n    glove_test.append(sen_vec)\n\nprint(len(glove_train))\nprint(len(glove_train[0]))","cf9ddc75":"## setting up the hyperparamter space\nparameters = {'kernel':('rbf',), 'C':[1,2,5,10]}\n\nsvc = SVC()\nclf = GridSearchCV(svc, parameters, cv = 4, scoring = 'f1')\nclf.fit(glove_train, y_train)","2f217fb6":"print(\"Average of the best f1-score in every fold during cross validation = \",clf.best_score_)\nprint(\"The best parameters found during k-fold cross validation is = \",clf.best_params_)","0f41ab8e":"y_pred = clf.predict(glove_test)","d9aec0c4":"#Ploting the confusion matrix\nplot_cm(y_test, y_pred, \"Confusion Matrix on unseen data using GloVe + SVM\")\n\nfrom sklearn.metrics import f1_score\nprint(\"The F1-Score on the test data is = \",f1_score(y_test, y_pred))","cda1e300":"## Lets build our hyperparameter search grid\n## since we have a large space of hyperparamter we will use randomisedSearchCV\n\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 400, num = 5)]\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 20, num = 5)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\n\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 5, cv = 4,scoring = 'f1')\nrf_random.fit(glove_train, y_train)","dd36aef0":"print(\"Average of the best f1-score in various folds during cross validation = \",rf_random.best_score_)\nprint(\"The best parameters found during k-fold cross validation is = \",rf_random.best_params_)","9c27aebf":"y_pred = rf_random.predict(glove_test)","36c7c5f0":"#Ploting the confusion matrix\nplot_cm(y_test, y_pred, \"Confusion Matrix on unseen data using GloVe + Random Forest\")\n\nfrom sklearn.metrics import f1_score\nprint(\"The F1-Score on the test data is = \",f1_score(y_test, y_pred))","1ad0c12b":"xgb_model = xgb.XGBClassifier(learning_rate=0.02, n_estimators=150)\n\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4,10,20]\n        }\n\nclf = RandomizedSearchCV(xgb_model, param_distributions=params, n_iter=5, scoring='f1', cv=4)\nclf.fit(np.array(glove_train), y_train)\n","1e8c37e7":"print(\"Average of the best f1-score in various folds during cross validation = \",clf.best_score_)\nprint(\"The best parameters found during k-fold cross validation is = \",clf.best_params_)","6b160391":"y_pred = clf.predict(np.array(glove_test))","094dd60e":"#Ploting the confusion matrix\nplot_cm(y_test, y_pred, \"Confusion Matrix on unseen data using GloVe + XgBoost\")\n\nfrom sklearn.metrics import f1_score\nprint(\"The F1-Score on the test data is = \",f1_score(y_test, y_pred))","94878a80":"model = keras.Sequential()\n\nmodel.add(layers.LSTM(256, activation='relu', kernel_initializer='he_normal', input_shape=(1,300), return_sequences=True))\n\nmodel.add(layers.LSTM(100, activation='relu', kernel_initializer='he_normal', return_sequences=True))\n\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary() ","60c6ae38":"model.compile(Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])","866fe655":"model_input = np.array(glove_train)\nmodel_input = model_input[:, None, :]\nmodel_label =  y_train[:, None]\nprint(model_input.shape)\nhistory = model.fit(model_input, y_train,\n                    batch_size=64,\n                    epochs=10,\n                    validation_split=0.1)","ba0655ee":"test_input = np.array(glove_test)\ntest_input = test_input[:, None, :]\ny_pred = model.predict(test_input)\ny_pred = y_pred.round().astype(int)","4a7e9f6c":"#Ploting the confusion matrix\nplot_cm(y_test, y_pred[:, 0, 0], \"Confusion Matrix on unseen data using GloVe + Sequential Model With LSTM\")\n\nfrom sklearn.metrics import f1_score\nprint(\"The F1-Score on the test data is = \",f1_score(y_test, y_pred[:, 0, 0]))","23dde99a":"# We will use the official tokenization script created by the Google team\n# If you are running this on kaggle make sure you turn on the internet of the kaggle notebook (Click on arrow at the top right corner and you will find it)\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","fb7749af":"import tokenization","54c29f4c":"'''\nWe are preparing out data in such a way that bert model can understand it\n\nWe have to give three sequences as input to the BERT\n\nall_tokens : It basically performs the tokenization of the input sentences\nall_masks  : This is done to make every input of the same length. We choose the maximum length of the vector and pad other vectors accordingly. We padd them \n             with the help of '0' which tells tells the model not to give attension to this token\nsegment Ids: This is used when we are giving multiple sentences as the input. Since we are only giving one sentence as the input we set the value of the \n             segment ids as 0 for all the tokens.\n             \nFor more detailed you can visit this terrific blog : jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\n'''\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n","8a87be65":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","023a20c2":"#Setting up the tokenizer\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","b7c8d4d7":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","06110da5":"train_input = bert_encode(x_train['text'], tokenizer, max_len=160)\ntest_input = bert_encode(x_test['text'], tokenizer, max_len=160)\nsubmission_test_input = bert_encode(test_data['text'], tokenizer, max_len=160) ","ac0c0e62":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","444af055":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, y_train,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","323937f2":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","1a45434e":"y_pred = test_pred.round().astype(int)\ny_pred.shape","65380d9e":"#Ploting the confusion matrix\nplot_cm(y_test, y_pred[:, 0], \"Confusion Matrix on unseen data using Fine Tuned BERT + Simple Classifier\")\n\nfrom sklearn.metrics import f1_score\nprint(\"The F1-Score on the test data is = \",f1_score(y_test, y_pred[:, 0]))","1a3a53ad":"#get output of slice layer from model above\ncls_layer_model = Model(model.input, outputs=model.get_layer('tf_op_layer_strided_slice').output)","77b18004":"X_train = cls_layer_model.predict(train_input)\nX_test = cls_layer_model.predict(test_input)","8a13674d":"print(X_train.shape)","7d567fff":"parameters = {'kernel':('rbf',), 'C':[2,5,10]}\n\nsvc = SVC()\nclf = GridSearchCV(svc, parameters, cv = 4, scoring = 'f1')\nclf.fit(X_train, y_train)","2812a02f":"print(\"Average of the best f1-score in every fold during cross validation = \",clf.best_score_)\nprint(\"The best parameters found during k-fold cross validation is = \",clf.best_params_)","e45fca0a":"y_pred = clf.predict(X_test)","bef7c393":"#Ploting the confusion matrix\nplot_cm(y_test, y_pred, \"Confusion Matrix on unseen data using Fine Tuned BERT + SVM\")\n\nfrom sklearn.metrics import f1_score\nprint(\"The F1-Score on the test data is = \",f1_score(y_test, y_pred))","5ffab7b4":"test_input_for_submission = cls_layer_model.predict(submission_test_input)\ny_pred = clf.predict(test_input_for_submission)\n\nsubmission['target'] = y_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","4fc3387f":"#### Now we have found the best hyperparameter for the model. So lets see how our model performs on unseen data. Also we will be building our confusion mastrix using this test only","75774670":"#### Now we have found the best hyperparameter for the model. So lets see how our model performs on unseen data. Also we will be building our confusion mastrix using this test only","3c51b697":"### 5.4 GloVe + XgBoost","fe4d66ea":"#### Ever wondered why we are using .apply() instead of transforming with the help of loop. Not just because it shortens the code but because it is way faster than traditional for loop. Check [this](https:\/\/engineering.upside.com\/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6) out.","6eec128a":"#### Conclusion: Even simple Sequential Model on top of GloVe outperforms SVM and XgBoost.","c8c5f716":"### 5.5 GloVe + Sequential Model with LSTM","8849d73e":"## 2. Loading The Dataset","02b026a9":"\n## **0.1 Notebook objective**\n   *  Compare different models with different embeddings by using only the **tweet** text. We will look at the following combinations:\n     \n     Tf-IDF embedding\n      * Tf-IDF embedding + SVM\n      * Tf-IDF embedding + RandomForest\n      * Tf-IDF embedding + XgBoost\n      * Tf-IDF embedding + Sequential model with LSTM\n      \n     GloVe embedding:\n      * GloVe embedding + SVM\n      * GloVe embedding + RandomForest\n      * GloVe embedding + XgBoost\n      * GloVe embedding + Sequential model with LSTM    \n      \n     BERT\n      * Fine Tuned BERT + Simple Classifier \n      * SVM on top of Fine Tuned BERT","d54ad186":"### 6.2 SVM on top of Fine Tuned BERT","4eded1ca":"#### Conclusion: As we can see from the confusion matrix that our model is predecting class \"0\" with good confidence but when it comes to class \"1\" our model is a little confused in predicting this class.","2a5d6c60":"## 0.2 References\n\n#### I took a lot of help from the following kernels. They are very helpful\n\n* [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)\n* [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n* [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert)","eea446ea":"### 5.2 GloVe + SVM","0906c3e7":"## 4. Building Model Using Tf-IDF Embedding","7be1884f":"#### I am going to use SVM on top of the Fine Tuned BERT as the model for the prediction","0b32edc0":"#### We are getting rid of the Embedding layer of the LSTM(We can build the model using the Embedding layer too with trainable = false). Rather we are giving the input after converting our training example into vector format","3a856074":"#### Conclusion: The F1 score of XgBoost roughly matches with SVM ","8f3f7f7d":"## 6. BERT","1236d122":"### 5.3 GloVe + Random Forest","4d56eda3":"#### Now we have found the best hyperparameter for the model. So lets see how our model performs on unseen data. Also we will be building our confusion mastrix using this test only","34835b3f":"## 3. Jumping Straight Into Data Processing","d4998d8c":"#### Conclusion : XgBoost model slightly performs better than RandomForest","e5432073":"## 7. Performing submission","f0142da0":"### 4.0 Building TF-IDf Embeddding","1704231d":" ### 3.1 Definning Ulitity Functions","efb381e6":"### 6.1 Fine Tuned BERT","24a4cac6":"#### Conclusion : We can see a good jump in F1 score by using GloVe embedding.","1f9629ed":"### 4.1 TF-IDf + SVM","30438c62":"## 1. **Importing Libraries **","2e8bb675":"### 5.1 Building the Glove Embedding for text","d421c3cc":"### 4.2 TF-IDf + Random Forest","3046c450":"## 5. Building Model Using GloVe Embedding","7a7baebc":"#### Conclusion : The performance on unseen data is not good, as it is clear from the f1 score of the test data. ","a611f75d":"Let's have a high level overview how tf-idf embedding is calculated.\n\ntf -- term frequency(how many times a word occurs in a sentences)\nidf - - inverse document frequency(how many times the word occurs in whole of the training data)\n\nDuring training phase tf-idf learns the vocabulary and IDF from the training data. And then the following embedding is calculated for each sentence:\n\n![autodraw%205_23_2020.png](attachment:autodraw%205_23_2020.png)","282340f7":"## If you found the notebook useful then an upvote is highy appreciated. ","d7bcc45a":"#### Now we have found the best hyperparameter for the model. So lets see how our model performs on unseen data. Also we will be building our confusion mastrix using this test only","5ad19b1e":"#### Lets print the confusion matrix on the test data and see how the model performed on the unseen data","e5d67ef5":"### 4.4 Tf-IDF + Sequential model with LSTM","49ffb4f6":"### 3.2 Applying the Ultility Functions","05a53adf":"#### F1 score on this model is good and we may get a better result by tuning the hyperparameters but since we have limited data so it is not advised to train deep networks.","f7fc398e":"#### Now we have found the best hyperparameter for the model. So lets see how our model performs on unseen data. Also we will be building our confusion mastrix using this test only","cd8bbe8b":"#### Now we have found the best hyperparameter for the model. So lets see how our model performs on unseen data. Also we will be building our confusion mastrix using this test only","27efadd7":"#### Conclusion : Although the F1 score is good as compared to TF-IDf + Random Forest, but the F1 score is less than GloVe + SVM model","4ffbf542":"#### Lets print the confusion matrix on the test data and see how the model performed on the unseen data","bc150cf0":"### 4.3 TF-IDf + XgBoost"}}