{"cell_type":{"5c6772ba":"code","accda10f":"code","67413287":"code","18959105":"code","94303440":"code","3335365f":"code","24bcf672":"code","822fa666":"code","6d25190e":"code","aa686b9a":"code","0bb5493f":"code","b3790ec6":"code","5b8d7dcd":"code","3ded51b8":"code","eee09d17":"code","3d8d0cae":"code","004a46aa":"code","8fda80f6":"code","7689d9d0":"code","637e3845":"code","99639aa8":"code","97a01d54":"code","1209529f":"code","bd5d895d":"markdown"},"source":{"5c6772ba":"#pip install pycaret","accda10f":"pip install xfeat","67413287":"import re\nfrom functools import partial\nfrom glob import glob\n\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom xfeat import (ArithmeticCombinations, ConcatCombination,\n                   GBDTFeatureExplorer, GBDTFeatureSelector, LabelEncoder,\n                   LambdaEncoder, Pipeline, SelectCategorical, SelectNumerical,\n                   TargetEncoder, aggregation)\n\nsns.set()\n\n\n# \u30c7\u30fc\u30bf\u524d\u51e6\u7406\u7528\u95a2\u6570\u3092\u4f5c\u6210\n\ndef normalize_moyori(moyori):\n    if moyori == moyori:\n        if moyori == '30\u5206?60\u5206':\n            moyori = 45\n        elif moyori == '1H?1H30':\n            moyori = 75\n        elif moyori == '1H30?2H':\n            moyori = 105\n        elif moyori == '2H?':\n            moyori = 120\n        moyori = int(moyori)\n    return moyori\n\n\ndef normalize_area(area):\n    if area == area:\n        area = int(re.sub('m\\^2\u672a\u6e80|\u33a1\u4ee5\u4e0a', '', str(area)))\n    return area\n\n\ndef convert_wareki_to_seireki(wareki):\n    if wareki == wareki:\n        if wareki == '\u6226\u524d':\n            wareki = '\u662d\u548c20\u5e74'\n        value = wareki[2:-1]\n        if value == '\u5143':\n            value = 1\n        else:\n            value = int(value)\n        if '\u662d\u548c' in wareki:\n            seireki = 1925+value\n        elif '\u5e73\u6210' in wareki:\n            seireki = 1988+value\n        elif '\u4ee4\u548c' in wareki:\n            seireki = 2018+value\n    else:\n        seireki = wareki\n    return seireki\n\n\n# \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\n\npaths = glob('..\/input\/secondhandhouse\/train\/train\/*')\ntrain_dfs = []\nfor path in paths:\n    train_df = pd.read_csv(path)\n    train_dfs.append(train_df)\ntrain_df = pd.concat(train_dfs)\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df = pd.read_csv('..\/input\/secondhandhouse\/test.csv')\n\nsub_df = pd.read_csv('..\/input\/secondhandhouse\/sample_submission.csv')\n\n","18959105":"pd.options.display.max_columns = None\npd.options.display.max_rows = None\n\nnp.set_printoptions(threshold=np.inf)\ntrain_df.head()","94303440":"#ID\t\u7a2e\u985e\t\u5730\u57df\t\u5e02\u533a\u753a\u6751\u30b3\u30fc\u30c9\t\u90fd\u9053\u5e9c\u770c\u540d\t\u5e02\u533a\u753a\u6751\u540d\t\u5730\u533a\u540d\t\u6700\u5bc4\u99c5\uff1a\u540d\u79f0\t\u6700\u5bc4\u99c5\uff1a\u8ddd\u96e2\uff08\u5206\uff09\t\u9593\u53d6\u308a\t\u9762\u7a4d\uff08\u33a1\uff09\t\u571f\u5730\u306e\u5f62\u72b6\t\u9593\u53e3\t\u5ef6\u5e8a\u9762\u7a4d\uff08\u33a1\uff09\t\u5efa\u7bc9\u5e74\t\u5efa\u7269\u306e\u69cb\u9020\t\u7528\u9014\t\u4eca\u5f8c\u306e\u5229\u7528\u76ee\u7684\t\u524d\u9762\u9053\u8def\uff1a\u65b9\u4f4d\t\u524d\u9762\u9053\u8def\uff1a\u7a2e\u985e\t\u524d\u9762\u9053\u8def\uff1a\u5e45\u54e1\uff08\uff4d\uff09\t\u90fd\u5e02\u8a08\u753b\t\u5efa\u307a\u3044\u7387\uff08\uff05\uff09\t\u5bb9\u7a4d\u7387\uff08\uff05\uff09\t\u53d6\u5f15\u6642\u70b9\t\u6539\u88c5\t\u53d6\u5f15\u306e\u4e8b\u60c5\u7b49\t\u53d6\u5f15\u4fa1\u683c\uff08\u7dcf\u984d\uff09_lo\ntrain_df.columns = ['ID','type','area','city_code','prefectures','city_name','area_name','near_station_name','near_station_min','floor','size','shape','frontage',\n                    'floor_area','build_year','build_type','usage','future_use','front_loc','front_type','front_m','city_plan',\n                   'build_ratio','area_ratio','trade_time','change','event','price']\ntest_df.columns = ['ID','type','area','city_code','prefectures','city_name','area_name','near_station_name','near_station_min','floor','size','shape','frontage',\n                    'floor_area','build_year','build_type','usage','future_use','front_loc','front_type','front_m','city_plan',\n                   'build_ratio','area_ratio','trade_time','change','event']","3335365f":"train_df.head()","24bcf672":"ID = 'ID'\nTARGET = 'price'\nrm_cols = []\n\ntest_df[TARGET] = np.nan\ndf = pd.concat([train_df, test_df])\n\nrm_cols += ['city_code']\n\nfor i, v in df.nunique().iteritems():\n    if v <= 1:\n        rm_cols.append(i)\n\ntrain_df.drop(rm_cols, axis=1, inplace=True)\ntest_df.drop(rm_cols, axis=1, inplace=True)\ndf = pd.concat([train_df, test_df])\n","822fa666":"df.head()","6d25190e":"# \u7279\u5fb4\u91cf\u751f\u6210\n\nenc_dic = {}\nfor i, e in enumerate(sorted(list(set(df['trade_time'].values)))):\n    enc_dic[e] = i\ndf['trade_time_enc'] = df['trade_time'].map(enc_dic)\n\n# Target Encoding\n# \u3042\u308b\u884c\u306e\u7279\u5fb4\u91cf\u3068\u3057\u3066\u5e73\u5747\u5024\u3092\u8a08\u7b97\u3059\u308b\u306e\u306b\u3001\u305d\u306e\u6642\u70b9\u3067\u904e\u53bb\u306b\u767b\u5834\u3057\u305f\u30c7\u30fc\u30bf\u306e\u96c6\u8a08\u3092\u7528\u3044\u308b\n# \u4ee5\u4e0b\u4f8b\u3067\u306f\u3001\u90fd\u9053\u5e9c\u770c\u3054\u3068\u306b\u5404\u53d6\u5f15\u6642\u70b9\u3088\u308a\u904e\u53bb\u306e\u5024\u306e\u5e73\u5747\u5024\u3092\u7528\u3044\u308b\nte_dic = {}\ntime_col = 'trade_time_enc'\ngroup_col = 'prefectures'\n\nfor i in set(df[time_col].values):\n    tmp_df = df[df[time_col] < i]\n    te_dic[i] = tmp_df.groupby(group_col)[TARGET].agg('mean').to_dict()\n\n\ndef calc_te(row):\n    if row[time_col] in te_dic and row[group_col] in te_dic[row[time_col]]:\n        return te_dic[row[time_col]][row[group_col]]\n    else:\n        return 0\n\n\ndf[group_col+'_te'] = df.apply(calc_te, axis=1)\n\ndf['trade_time_ago'] = df['trade_time'].apply(lambda x: 2020-int(x[:4]))\ndf.drop(['trade_time'], axis=1, inplace=True)\ndf['build_year'] = df['build_year'].apply(lambda x: convert_wareki_to_seireki(x))\ndf['size'] = df['size'].apply(lambda x: normalize_area(x))\ndf['near_station_min'] = df['near_station_min'].apply(lambda x: normalize_moyori(x))\ndf['year_area_ratio']=df['size']\/(2021-df['build_year'])\ndf['taken_area_multi']=(df['size']*df['build_ratio'])\/df['area_ratio']\ndf['area_min_ratio']=df['size']\/df['near_station_min']\ndf['time_area_min_multi']=df['year_area_ratio']*df['near_station_min']\ndf['size_min_area_bulid_fuck']=((df['size']*2)\/(df['area_ratio']\/100))*(1\/df['near_station_min'])\n# \u6570\u5024\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\u3057\u3066\u304a\u304f\nnum_df = SelectNumerical().fit_transform(df)\n\n# \u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\nencoder = Pipeline([\n    SelectCategorical(),\n    LabelEncoder(output_suffix=\"\"),\n])\n\nle_df = encoder.fit_transform(df)\n\n# \u6570\u5024\u30c7\u30fc\u30bf\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u7279\u5fb4\u91cf\u751f\u6210\nencoder = Pipeline(\n    [\n        SelectNumerical(),\n        ArithmeticCombinations(\n            input_cols=[\"size\", \"area_ratio\"],\n            drop_origin=True,\n            operator=\"*\",\n            r=2,\n        ),\n    ]\n)\n\nnum_comb_df = encoder.fit_transform(df)\/100\n\n# \u96c6\u7d04\u7279\u5fb4\u91cf\u751f\u6210\nagg_dfs = []\n\n\ndef get_agg_df(df, group_col):\n\n    agg_df, agg_cols = aggregation(df,\n                                   group_key=group_col,\n                                   group_values=['near_station_min','year_area_ratio','taken_area_multi',\n                                                 'size', 'build_ratio', 'area_ratio','time_area_min_multi','area_min_ratio','size_min_area_bulid_fuck'],\n                                   agg_methods=['count', 'mean', 'min', 'max','std'],\n                                   )\n\n    return agg_df[agg_cols]\n\n\ngroup_col = 'city_name'\nagg_dfs.append(get_agg_df(df, group_col))\n\ngroup_col = 'prefectures'\nagg_dfs.append(get_agg_df(df, group_col))\n\n# group_col = 'build_type'\n# agg_dfs.append(get_agg_df(df, group_col))\n\n# group_col = 'city_plan'\n# agg_dfs.append(get_agg_df(df, group_col))\n\n\n\n# \u751f\u6210\u3057\u305f\u7279\u5fb4\u91cf\u3092\u7d50\u5408\nfeat_df = pd.concat([num_df, le_df, num_comb_df]+agg_dfs, axis=1)\n\n","aa686b9a":"from scipy import stats\nfrom scipy.special import inv_boxcox\nxt, fitted_lambda = stats.boxcox(feat_df[TARGET])\nfeat_df[TARGET] = xt","0bb5493f":"# \u30e2\u30c7\u30eb\u69cb\u7bc9\ntrain_df = feat_df[feat_df['trade_time_ago'] > 1]\nval_df = feat_df[feat_df['trade_time_ago'] == 1]\ntest_df = feat_df[feat_df['trade_time_ago'] == 0]\n\nfeat_cols = [col for col in train_df.columns if col not in rm_cols+[ID, TARGET]]\n\ncat_cols = list(le_df.columns) + ['trade_time_enc']\ntrain_x = train_df[feat_cols]\ntrain_y = train_df[TARGET]\nval_x = val_df[feat_cols]\nval_y = val_df[TARGET]\ntest_x = test_df[feat_cols]\ntest_y = test_df[TARGET]","b3790ec6":"train_df.head()","5b8d7dcd":"# from pycaret.regression import *\n# exp_name = setup(data = feat_df,  target = 'price',ignore_features=['ID'],train_size=0.9)\n#                # imputation_type='iterative',iterative_imputation_iters=2)","3ded51b8":"# cat = create_model('catboost',fold=3,round=2)#0.0816 ###0.0854","eee09d17":"# pred=predict_model(cat,test_df)","3d8d0cae":"# sub_df['\u53d6\u5f15\u4fa1\u683c\uff08\u7dcf\u984d\uff09_log']=pred['Label']","004a46aa":"# sub_df.to_csv('submission_cat2.csv', index=False)","8fda80f6":"# from sklearn import metrics\n# from sklearn.model_selection import GridSearchCV\n# lg = lgb.LGBMRegressor(silent=False,'objective': 'regression','metric': 'mae',\"early_stopping_rounds\": 100,\"feature_fraction\": 0.85)\n# param_dist = {\"max_depth\": [4,5,7,8],\n#               \"learning_rate\" : [0.01,0.05,0.1],\n#               \"num_leaves\": [31,47,67,127]\n#              }\n# grid_search = GridSearchCV(lg, n_jobs=-1, param_grid=param_dist, cv = 5, scoring=\"neg_mean_absolute_error\", verbose=50)\n# grid_search.fit(train_x,train_y)\n# grid_search.best_estimator_, grid_search.best_score_\n\n\n# (LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n#                importance_type='split', learning_rate=0.1, max_depth=8,\n#                min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n#                n_estimators=100, n_jobs=-1, num_leaves=127, objective=None,\n#                random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n#                subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n#  -0.11008801757583823)","7689d9d0":"import lightgbm as lgb\nSEED = 1998\n\nparams = {\n#     'objective': 'regression',\n#     'metric': 'mae','boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0,\n#     'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 12,\n#                'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0,\n#                'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 627, 'objective': None,\n#                'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': False,\n#                'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0,\n#     \"early_stopping_rounds\": 100\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_leaves': 127,\n    'max_depth': 8,\n    \"feature_fraction\": 0.7,\n    'subsample_freq': 1,\n    \"bagging_fraction\": 0.9,\n    'min_data_in_leaf': 10,\n    'learning_rate': 0.1,\n    \"boosting\": \"gbdt\",\n    \"verbosity\": -1,\n    \"random_state\": 998,\n    \"num_boost_round\": 50000,\n    \"early_stopping_rounds\": 100\n}\n\ntrain_data = lgb.Dataset(train_x, label=train_y)\nval_data = lgb.Dataset(val_x, label=val_y)\n\nmodel = lgb.train(\n    params,\n    train_data,\n    categorical_feature=cat_cols,\n    valid_names=['train', 'valid'],\n    valid_sets=[train_data, val_data],\n    verbose_eval=500,\n)\n\nval_pred = model.predict(val_x, num_iteration=model.best_iteration)\nscore = mean_absolute_error(val_y, val_pred)\n\npred_df = pd.DataFrame(sorted(zip(val_x.index, val_pred, val_y)), columns=[\n                       'index', 'predict', 'actual'])\n\nfeature_imp = pd.DataFrame(sorted(zip(model.feature_importance(\n), train_x.columns)), columns=['importance', 'feature'])\n\nprint(f'box-cox MAE: {score:.4f}')\n\nval_pred = inv_boxcox( np.array( val_pred ), fitted_lambda)\nval_y = inv_boxcox( np.array( val_y ), fitted_lambda)\nscore = mean_absolute_error(val_y, val_pred)\nprint(f'normal MAE: {score:.4f}')\n# \u7279\u5fb4\u91cf\u306e\u91cd\u8981\u5ea6\u53ef\u8996\u5316\nlgb.plot_importance(model, figsize=(\n    12, 8), max_num_features=50, importance_type='gain')\nplt.tight_layout()\nplt.show()\n\ntest_pred = model.predict(test_x, num_iteration=model.best_iteration)","637e3845":"import catboost as cat\nfrom sklearn.model_selection import GridSearchCV\ncat_grid = {'depth':[8]\n            , 'bootstrap_type':['Bernoulli']\n            , 'od_type':['Iter']\n            , 'l2_leaf_reg':[5]#range(1,40,1)\n            , 'learning_rate': [0.1]\n            , 'allow_writing_files':[False]\n            , 'silent':[True]\n            , 'od_type':['Iter']\n           }\n\ncatgrid = GridSearchCV(cat.CatBoostRegressor(), param_grid=cat_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose = 50)\ncatgrid.fit( train_x, train_y )\n#print best params\nprint( catgrid.best_params_, catgrid.best_score_ )","99639aa8":"#predict\ncat_preds = catgrid.predict(val_x )\n#boxcox\u9006\u53d8\u6362\n# preds_inv = inv_boxcox( np.array( cat_preds ), fitted_lambda)\n# print( preds_inv.min(), preds_inv.max() )","97a01d54":"cat_pred = catgrid.predict(test)\n# cat_pred = inv_boxcox( np.array( cat_pred  ), fitted_lambda)\nsub_df['\u53d6\u5f15\u4fa1\u683c\uff08\u7dcf\u984d\uff09_log']=cat_pred\nsub_df.head()","1209529f":"sub_df.to_csv('submission_cat_2_2.csv', index=False)","bd5d895d":"# \u6570\u636e\u96c6\u5212\u5206"}}