{"cell_type":{"c99b8e6d":"code","c352690e":"code","2429eade":"code","1c34e485":"code","12913af8":"code","0416ebb9":"code","28666bcd":"code","33152129":"code","de9519b7":"code","2e28aef7":"code","5fae5200":"code","1a547f62":"code","1afddaa8":"code","0e8bb4c2":"code","ae8b6d1c":"code","49a90d15":"code","fa3ded7d":"markdown","0b1aee34":"markdown","3a503c29":"markdown","12ff9eaa":"markdown","65d8bbaa":"markdown","a48ac4ee":"markdown","0456d19d":"markdown","8e4384b7":"markdown","dc9e350d":"markdown","8f994648":"markdown","c1afcf33":"markdown","46becc97":"markdown","08fcd35b":"markdown","9530e261":"markdown","df8ba348":"markdown","6f13f217":"markdown","f3cff2f2":"markdown","347ccfee":"markdown","57508284":"markdown","b4920e60":"markdown","531bbdb0":"markdown","ee9125bb":"markdown","dbbb9dc2":"markdown"},"source":{"c99b8e6d":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","c352690e":"# !wget https:\/\/raw.githubusercontent.com\/alexeygrigorev\/mlbookcamp-code\/master\/chapter-06-trees\/CreditScoring.csv","2429eade":"df = pd.read_csv('\/kaggle\/input\/creditscoring\/CreditScoring.csv')\ndf.columns = df.columns.str.lower()\ndf","1c34e485":"status_values = {\n    1: 'ok',\n    2: 'default',\n    0: 'unk'\n}\n\ndf.status = df.status.map(status_values)\n\n\nhome_values = {\n    1: 'rent',\n    2: 'owner',\n    3: 'private',\n    4: 'ignore',\n    5: 'parents',\n    6: 'other',\n    0: 'unk'\n}\n\ndf.home = df.home.map(home_values)\n\nmarital_values = {\n    1: 'single',\n    2: 'married',\n    3: 'widow',\n    4: 'separated',\n    5: 'divorced',\n    0: 'unk'\n}\n\ndf.marital = df.marital.map(marital_values)\n\nrecords_values = {\n    1: 'no',\n    2: 'yes',\n    0: 'unk'\n}\n\ndf.records = df.records.map(records_values)\n\njob_values = {\n    1: 'fixed',\n    2: 'partime',\n    3: 'freelance',\n    4: 'others',\n    0: 'unk'\n}\n\ndf.job = df.job.map(job_values)\ndf","12913af8":"for c in ['income', 'assets', 'debt']:\n    df[c] = df[c].replace(to_replace=99999999, value=0)","0416ebb9":"df = df[df.status != 'unk'].reset_index(drop=True)","28666bcd":"df['default'] = (df.status == 'default').astype(int)\ndel df['status']\ndf","33152129":"numerical = [\"seniority\",\"time\",\"age\",\"expenses\",\"income\",\"assets\",\"debt\",\"amount\",\"price\",\"default\"]\ncategorical = [\"home\", \"marital\", \"records\", \"job\"]","de9519b7":"# Setup validation framework\nfrom sklearn.model_selection import train_test_split\n\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n\ndf_train = df_train.reset_index(drop=\"true\")\ndf_val = df_val.reset_index(drop=\"true\")\ndf_test = df_test.reset_index(drop=\"true\")\n\n# y_full_train = df_full_train.churn.values\n# y_train = df_train.churn.values\n# y_val = df_val.churn.values\n# y_test = df_test.churn.values\n\n# del df_test[\"churn\"]\n# del df_val[\"churn\"]\n# del df_train[\"churn\"]\n# df_train.columns\n\nprint(\"train: %.2f, val: %.2f, test: %.2f\" % (len(df_train)\/len(df), len(df_val)\/len(df), len(df_test)\/len(df)))","2e28aef7":"# Q1 (not following question) (see Q1b)\n\nimport warnings\nwarnings.filterwarnings(action='once')\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.model_selection import KFold\nfrom tqdm.auto import tqdm\n\ndef train(dataFrame, y):\n    dicts = dataFrame.to_dict(orient=\"records\")\n    dv = DictVectorizer(sparse=False)\n    X = dv.fit_transform(dicts)\n\n    model = linear_model.LogisticRegression()\n    model.fit(X, y)\n    return dv, model\n\ndef predict(dataFrame, dv, model):\n    dicts = dataFrame.to_dict(orient=\"records\")\n    X = dv.transform(dicts)\n    y_pred = model.predict_proba(X)[:,1]\n    return y_pred\n\nfields = [\n    {\"field\": \"seniority\", \"correlation\": 1},\n    {\"field\": \"time\", \"correlation\": 1},\n    {\"field\": \"age\", \"correlation\": 1},\n    {\"field\": \"expenses\", \"correlation\": 1},\n    {\"field\": \"income\", \"correlation\": 1},\n    {\"field\": \"assets\", \"correlation\": 1},\n    {\"field\": \"debt\", \"correlation\": 1},\n    {\"field\": \"amount\", \"correlation\": 1},\n    {\"field\": \"price\", \"correlation\": -1},\n    {\"field\": \"default\", \"correlation\": 1}\n]\n\nfor f in fields:\n    df_train_selected = df_train[[f[\"field\"]]]\n    df_val_selected = df_val[[f[\"field\"]]]\n\n    y_train = f[\"correlation\"] * df_train[\"default\"].values\n    y_val = df_val[\"default\"].values\n\n    dv, model = train(df_train_selected, y_train)\n    y_pred = predict(df_val_selected, dv, model)\n\n#     display(y_val, y_pred)\n    auc = metrics.roc_auc_score(y_val, y_pred)\n    print(\"field:\", f[\"field\"], \"auc:\",round(auc,3))","5fae5200":"# Q1b (following question, no model training)\n\nimport warnings\nwarnings.filterwarnings(action='once')\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.model_selection import KFold\nfrom tqdm.auto import tqdm\n\nfields = [\n    {\"field\": \"seniority\", \"correlation\": -1},\n    {\"field\": \"time\", \"correlation\": 1},\n    {\"field\": \"age\", \"correlation\": -1},\n    {\"field\": \"expenses\", \"correlation\": -1},\n    {\"field\": \"income\", \"correlation\": -1},\n    {\"field\": \"assets\", \"correlation\": -1},\n    {\"field\": \"debt\", \"correlation\": -1},\n    {\"field\": \"amount\", \"correlation\": 1},\n    {\"field\": \"price\", \"correlation\": 1},\n    {\"field\": \"default\", \"correlation\": 1}\n]\n\n# df_train[fields[0][\"field\"]].values\n\n\n# metrics.roc_auc_score([1,0,1], [0.3, 0.2, 0.4])\n\nfor f in fields:\n    train_selected = f[\"correlation\"] * df_train[f[\"field\"]].values\n#     df_val_selected = df_val[[f[\"field\"]]]\n\n#     y_train = f[\"correlation\"] * df_train[\"default\"].values\n    y = df_train[\"default\"].values\n\n#     dv, model = train(df_train_selected, y_train)\n#     y_pred = predict(df_val_selected, dv, model)\n\n    auc = metrics.roc_auc_score(y, train_selected)\n    print(\"field:\", f[\"field\"], \"auc:\",round(auc,3))","1a547f62":"# Q2\nselected_fields = ['seniority', 'income', 'assets', 'records', 'job', 'home']\n\ndef train(dataFrame, y):\n    dicts = dataFrame[selected_fields].to_dict(orient=\"records\")\n    dv = DictVectorizer(sparse=False)\n    X = dv.fit_transform(dicts)\n\n    model = linear_model.LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n    model.fit(X, y)\n    return dv, model\n\ndef predict(dataFrame, dv, model):\n    dicts = dataFrame.to_dict(orient=\"records\")\n    X = dv.transform(dicts)\n    y_pred = model.predict_proba(X)[:,1]\n    return y_pred\n\ny_train = df_train[\"default\"].values\ny_val = df_val[\"default\"].values\n\ndv, model = train(df_train, y_train)\ny_pred = predict(df_val, dv, model)\n\nauc = metrics.roc_auc_score(y_val, y_pred)\nprint(\"auc:\",round(auc,3))\n","1afddaa8":"import matplotlib.pyplot as plt\n\nprecissions = []\nrecalls = []\n\n# default prediction thrashold:\nthrashholds = np.linspace(0,1, 100)\n\nfor t in thrashholds:\n    predict_positive = (y_pred >= t)\n    predict_negative = (y_pred < t)\n\n    actual_positive = (y_val >= t)\n    actual_negative = (y_val < t)\n\n    true_positive = (predict_positive & actual_positive).sum()\n    true_negative = (predict_negative & actual_negative).sum()\n    false_positive = (predict_positive & actual_negative).sum()\n    false_negative = (predict_negative & actual_positive).sum()\n\n    precission = true_positive \/ (true_positive + false_positive)\n#     print(\"t=\",t, \"precission=\",precission)\n    precissions.append(precission)\n\n    recall = true_positive \/ (true_positive + false_negative)\n#     print(\"t=\",t, \"recall=\",recall)\n    recalls.append(recall)\n\n# precissions, recalls\n\nplt.plot(thrashholds, precissions)\nplt.plot(thrashholds, recalls)","0e8bb4c2":"thrashholds_precissions_recalls = zip(thrashholds, precissions, recalls)\n# set(thrashholds_precissions_recalls)\n\nthrashholds_F1s = []\n\nfor t_p_r in thrashholds_precissions_recalls:\n    F1 = (2*t_p_r[1]*t_p_r[2])\/(t_p_r[1]+t_p_r[2])\n    thrashholds_F1s.append((t_p_r[0], F1))\n\nsorted_thrashholds_F1s = sorted(thrashholds_F1s, key=lambda elem: elem[1], reverse=True)\nrounded_sorted_thrashholds_F1s = map(lambda t: (round(t[0],2),round(t[1],2)), sorted_thrashholds_F1s)\nlist(rounded_sorted_thrashholds_F1s)\n\n#threashold #F1\n\n","ae8b6d1c":"# !pip install tqdm\nimport warnings\nwarnings.filterwarnings(action='once')\nwarnings.filterwarnings('ignore')\n\n#Cross Validation\nfrom sklearn.model_selection import KFold\nfrom tqdm.auto import tqdm\n\ndef train(dataFrame, y):\n    dicts = dataFrame[selected_fields].to_dict(orient=\"records\")\n    dv = DictVectorizer(sparse=False)\n    X = dv.fit_transform(dicts)\n\n    model = linear_model.LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n    model.fit(X, y)\n    return dv, model\n\ndef predict(dataFrame, dv, model):\n    dicts = dataFrame.to_dict(orient=\"records\")\n    X = dv.transform(dicts)\n    y_pred = model.predict_proba(X)[:,1]\n    return y_pred\n\n# df_full_train_selected1 = df_full_train[selected_fields]\n\nsplits = 5\n\n# for C in tqdm([ 0.001, 0.01, 0.1, 0.5, 1, 5, 10], total=splits):\nkf = KFold(n_splits=splits, shuffle=True, random_state=1)\nauc_scores = []\nfor train_idx, val_idx in kf.split(df_full_train):\n\n    df_train_itter = df_full_train.iloc[train_idx]\n    df_val_itter = df_full_train.iloc[val_idx]\n\n    y_train_iter = df_full_train.iloc[train_idx].default.values\n    y_val_iter = df_full_train.iloc[val_idx].default.values\n\n    dv, model = train(df_train_itter, y_train_iter)\n    y_pred_iter = predict(df_val_itter, dv, model)\n    auc = metrics.roc_auc_score(y_val_iter, y_pred_iter)\n    auc_scores.append(auc)\n\nauc_scores\nprint(\"AUC mean: %.4f, AUC std: +-%.4f\" % (np.mean(auc_scores), np.std(auc_scores)))","49a90d15":"# !pip install tqdm\nimport warnings\nwarnings.filterwarnings(action='once')\nwarnings.filterwarnings('ignore')\n\n#Cross Validation\nimport sklearn\nfrom sklearn.model_selection import KFold\n\ndef train(dataFrame, y, C):\n    dicts = dataFrame[selected_fields].to_dict(orient=\"records\")\n    dv = DictVectorizer(sparse=False)\n    X = dv.fit_transform(dicts)\n\n    model = linear_model.LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n    model.fit(X, y)\n    return dv, model\n\ndef predict(dataFrame, dv, model):\n    dicts = dataFrame.to_dict(orient=\"records\")\n    X = dv.transform(dicts)\n    y_pred = model.predict_proba(X)[:,1]\n    return y_pred\n\n# df_full_train_selected1 = df_full_train[selected_fields]\n\nsplits = 5\n\nfor C in [0.01, 0.1, 1, 10]:\n    kf = KFold(n_splits=splits, shuffle=True, random_state=1)\n    auc_scores = []\n    for train_idx, val_idx in kf.split(df_full_train):\n\n        df_train_itter = df_full_train.iloc[train_idx]\n        df_val_itter = df_full_train.iloc[val_idx]\n\n        y_train_iter = df_full_train.iloc[train_idx].default.values\n        y_val_iter = df_full_train.iloc[val_idx].default.values\n\n        dv, model = train(df_train_itter, y_train_iter,C)\n        y_pred_iter = predict(df_val_itter, dv, model)\n        auc = metrics.roc_auc_score(y_val_iter, y_pred_iter)\n        auc_scores.append(auc)\n\n    print(\"C: %s, AUC mean: %.3f, AUC std: +-%.4f\" % (C, np.mean(auc_scores), np.std(auc_scores)))\nprint(\"sklearn.__version__=\",sklearn.__version__)","fa3ded7d":"Some of the features are encoded as numbers. Use the following code to de-code them:","0b1aee34":"## Submit the results\n\nSubmit your results here: https:\/\/forms.gle\/e497sR5iB36mM9Cs5\n\nIt's possible that your answers won't match exactly. If it's the case, select the closest one.\n\n## Deadline\n\nThe deadline for submitting is 04 October 2021, 17:00 CET. After that, the form will be closed.","3a503c29":"Which C leads to the best mean score?\n\n- 0.01\n- 0.1\n- 1\n- 10\n\nIf you have ties, select the score with the lowest std. If you still have ties, select the smallest C","12ff9eaa":"At which threshold precision and recall curves intersect?\n\n* 0.2\n* 0.4\n* 0.6\n* 0.8","65d8bbaa":"Prepare the numerical variables:","a48ac4ee":"At which threshold F1 is maximal?\n\n- 0.1\n- 0.3\n- 0.5\n- 0.7","0456d19d":"## Question 2\n\nWhat's the AUC of this model on the validation dataset? (round to 3 digits)\n\n- 0.512\n- 0.612\n- 0.712\n- 0.812","8e4384b7":"## Homework 4\n\nUse this notebook as a starter","dc9e350d":"Which numerical variable (among the following 4) has the highest AUC?\n\n- seniority\n- time\n- income\n- debt","8f994648":"Data:\n\n- https:\/\/github.com\/gastonstat\/CreditScoring\n- Also available [here](https:\/\/raw.githubusercontent.com\/alexeygrigorev\/mlbookcamp-code\/master\/chapter-06-trees\/CreditScoring.csv)","c1afcf33":"## Question 1\n\nROC AUC could also be used to evaluate feature importance of numerical variables. \n\nLet's do that\n\n* For each numerical variable, use it as score and compute AUC with the \"default\" variable\n* Use the training dataset for that\n\n\nIf your AUC is < 0.5, invert this variable by putting \"-\" in front\n\n(e.g. `-df_train['expenses']`)\n\nAUC can go below 0.5 if the variable is negatively correlated with the target varialble. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive.","46becc97":"## Your code","08fcd35b":"Create the target variable","9530e261":"## Question 5\n\n\nUse the `KFold` class from Scikit-Learn to evaluate our model on 5 different folds:\n\n```\nKFold(n_splits=5, shuffle=True, random_state=1)\n```\n\n* Iterate over different folds of `df_full_train`\n* Split the data into train and validation\n* Train the model on train with these parameters: `LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)`\n* Use AUC to evaluate the model on validation\n","df8ba348":"## Question 4\n\nPrecision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both\n\nThis is the formula for computing F1:\n\n$$F_1 = 2 \\cdot \\cfrac{P \\cdot R}{P + R}$$\n\nWhere $P$ is precision and $R$ is recall.\n\nLet's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01","6f13f217":"Split the data into 3 parts: train\/validation\/test with 60%\/20%\/20% distribution. Use `train_test_split` funciton for that with `random_state=1`","f3cff2f2":"How large is standard devidation of the scores across different folds?\n\n- 0.001\n- 0.014\n- 0.09\n- 0.14","347ccfee":"## Training the model\n\nFrom now on, use these columns only:\n\n```\n['seniority', 'income', 'assets', 'records', 'job', 'home']\n```\n\nApply one-hot-encoding using `DictVectorizer` and train the logistic regression with these parameters:\n\n```\nLogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n```","57508284":"What are the categorical variables? What are the numerical?","b4920e60":"## Question 6\n\nNow let's use 5-Fold cross-validation to find the best parameter C\n\n* Iterate over the following C values: `[0.01, 0.1, 1, 10]`\n* Use these parametes for the model: `LogisticRegression(solver='liblinear', C=C, max_iter=1000)`\n* Compute the mean score as well as the std","531bbdb0":"Remove clients with unknown default status","ee9125bb":"## Preparation \n\nWe'll talk about this dataset in more details in week 6. But for now, use the following code to get started","dbbb9dc2":"## Question 3\n\nNow let's compute precision and recall for our model.\n\n* Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n* For each threshold, compute precision and recall\n* Plot them"}}