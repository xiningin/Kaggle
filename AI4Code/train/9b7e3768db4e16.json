{"cell_type":{"ead3dad7":"code","bfecd08f":"code","14b20111":"code","532aa6e9":"code","fcf55751":"code","d395af5a":"code","10e4d39b":"code","9531a020":"code","40ed13f9":"code","846a2648":"code","d6141487":"code","a183b7cf":"code","ec36fee6":"code","76e2c05e":"code","b35aa352":"code","69ca227c":"code","ffb13973":"code","c6f87b62":"code","460c6114":"code","1a5c9ac3":"code","5463b649":"code","736e9cc9":"code","0567ad35":"code","763acd85":"code","a80e662d":"code","822bf502":"code","97859fd0":"code","7e889e76":"code","13016a05":"code","bc4ee13a":"code","5b173877":"code","cf883ec1":"code","5b9f695c":"code","1f205e26":"code","799a407c":"code","4db7caf0":"code","0ded697a":"code","9d7cc9a0":"code","04e03417":"code","1aef48b3":"code","58d48df8":"code","2e09d33a":"code","a1d712e3":"code","f5853ca9":"code","238bd9b4":"markdown","da0ca230":"markdown","cd970cb8":"markdown","88dcc82b":"markdown","bc91f4ba":"markdown","59589b17":"markdown","8fef53f9":"markdown","71a3db51":"markdown","ea1ceb72":"markdown","66247130":"markdown","97ab9062":"markdown","a3e57ab4":"markdown","819d1de6":"markdown","b95355da":"markdown","78108a25":"markdown","677e98de":"markdown","f5747f71":"markdown","e00ed7e5":"markdown","121614a0":"markdown","0c4531a9":"markdown","81a178b8":"markdown","d1e1d139":"markdown","e14301e8":"markdown","af43ed75":"markdown","900930b8":"markdown","8bbb47c5":"markdown","752c8465":"markdown","c74d28e4":"markdown","048922c7":"markdown","14da1f8b":"markdown","271791e0":"markdown","a21c3cf6":"markdown","32dca9a5":"markdown","e8edf0d2":"markdown","498bbd62":"markdown","0b6b3d9b":"markdown","819acd4d":"markdown","dc349f95":"markdown"},"source":{"ead3dad7":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nimport librosa\nimport librosa.display\nimport IPython.display as display\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.applications import VGG19, VGG16, ResNet50\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bfecd08f":"path = '\/kaggle\/input\/birdclef-2021\/'\nos.listdir(path)","14b20111":"def read_ogg_file(path, file):\n    \"\"\" Read ogg audio file and return numpay array and samplerate\"\"\"\n    \n    data, samplerate = sf.read(path+file)\n    return data, samplerate\n\n\ndef plot_audio_file(data, samplerate):\n    \"\"\" Plot the audio data\"\"\"\n    \n    sr = samplerate\n    fig = plt.figure(figsize=(8, 4))\n    x = range(len(data))\n    y = data\n    plt.plot(x, y)\n    plt.plot(x, y, color='red')\n    plt.legend(loc='upper center')\n    plt.grid()\n    \n    \ndef plot_spectrogram(data, samplerate):\n    \"\"\" Plot spectrogram with mel scaling \"\"\"\n    \n    sr = samplerate\n    spectrogram = librosa.feature.melspectrogram(data, sr=sr)\n    log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n    librosa.display.specshow(log_spectrogram, sr=sr, x_axis='time', y_axis='mel')","532aa6e9":"train_labels = pd.read_csv(path+'train_soundscape_labels.csv')\ntrain_meta = pd.read_csv(path+'train_metadata.csv')\ntest_data = pd.read_csv(path+'test.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","fcf55751":"print('Number train label samples:', len(train_labels))\nprint('Number train meta samples:', len(train_meta))\nprint('Number train short folder:', len(os.listdir(path+'train_short_audio')))\nprint('Number train audios:', len(os.listdir(path+'train_soundscapes')))\nprint('Number test samples:', len(test_data))","d395af5a":"os.listdir(path+'train_short_audio\/caltow')[:2]","10e4d39b":"train_labels.head()","9531a020":"train_meta.head()","40ed13f9":"row = 0\ntrain_meta.iloc[row]","846a2648":"label = train_meta.loc[row, 'primary_label']\nfilename = train_meta.loc[row, 'filename']\n\n# Check if the file is in the folder\nfilename in os.listdir(path+'train_short_audio\/'+label)","d6141487":"data, samplerate = sf.read(path+'train_short_audio\/'+label+'\/'+filename)\nprint(data[:8])\nprint(samplerate)","a183b7cf":"plot_audio_file(data, samplerate)","ec36fee6":"plot_spectrogram(data, samplerate)","76e2c05e":"display.Audio(path+'train_short_audio\/'+label+'\/'+filename)","b35aa352":"train_labels['audio_id'].unique()","69ca227c":"train_labels.groupby(by=['audio_id']).count()['birds'][:4]","ffb13973":"print('original label:', train_labels.loc[458, 'birds'])\nprint('split into list:', train_labels.loc[458, 'birds'].split(' '))","c6f87b62":"labels = []\nfor row in train_labels.index:\n    labels.extend(train_labels.loc[row, 'birds'].split(' '))\nlabels = list(set(labels))\n\nprint('Number of unique bird labels:', len(labels))","460c6114":"df_labels_train = pd.DataFrame(index=train_labels.index, columns=labels)\nfor row in train_labels.index:\n    birds = train_labels.loc[row, 'birds'].split(' ')\n    for bird in birds:\n        df_labels_train.loc[row, bird] = 1\ndf_labels_train.fillna(0, inplace=True)\n\n# We set a dummy value for the target label in the test data because we will need for the Data Generator\ntest_data['birds'] = 'nocall'\n\ndf_labels_test = pd.DataFrame(index=test_data.index, columns=labels)\nfor row in test_data.index:\n    birds = test_data.loc[row, 'birds'].split(' ')\n    for bird in birds:\n        df_labels_test.loc[row, bird] = 1\ndf_labels_test.fillna(0, inplace=True)","1a5c9ac3":"df_labels_train.sum().sort_values(ascending=False)[:10]","5463b649":"train_labels = pd.concat([train_labels, df_labels_train], axis=1)\ntest_data = pd.concat([test_data, df_labels_test], axis=1)","736e9cc9":"file = os.listdir(path+'train_soundscapes')[0]\nfile","0567ad35":"data, samplerate = read_ogg_file(path+'train_soundscapes\/', file)","763acd85":"audio_id = file.split('_')[0]\nsite = file.split('_')[1]\nprint('audio_id:', audio_id, ', site:', site)","a80e662d":"train_labels[(train_labels['audio_id']==int(audio_id)) & (train_labels['site']==site) & (train_labels['birds']!='nocall')]","822bf502":"sub_data = data[int(455\/5)*160000:int(460\/5)*160000]","97859fd0":"plt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(sub_data, sr=samplerate)\nplt.grid()\nplt.show()","7e889e76":"display.Audio(sub_data, rate=samplerate)","13016a05":"data_lenght = 160000\naudio_lenght = 5\nnum_labels = len(labels)","bc4ee13a":"batch_size = 16","5b173877":"list_IDs_train, list_IDs_val = train_test_split(list(train_labels.index), test_size=0.33, random_state=2021)\nlist_IDs_test = list(samp_subm.index)","cf883ec1":"class DataGenerator(Sequence):\n    def __init__(self, path, list_IDs, data, batch_size):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.data = data\n        self.batch_size = batch_size\n        self.indexes = np.arange(len(self.list_IDs))\n        \n    def __len__(self):\n        len_ = int(len(self.list_IDs)\/self.batch_size)\n        if len_*self.batch_size < len(self.list_IDs):\n            len_ += 1\n        return len_\n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        X = X.reshape((self.batch_size, 100, 1600\/\/2))\n        return X, y\n    \n    def __data_generation(self, list_IDs_temp):\n        X = np.zeros((self.batch_size, data_lenght\/\/2))\n        y = np.zeros((self.batch_size, num_labels))\n        for i, ID in enumerate(list_IDs_temp):\n            prefix = str(self.data.loc[ID, 'audio_id'])+'_'+self.data.loc[ID, 'site']\n            file_list = [s for s in os.listdir(self.path) if prefix in s]\n            if len(file_list) == 0:\n                # Dummy for missing test audio files\n                audio_file_fft = np.zeros((data_lenght\/\/2))\n            else:\n                file = file_list[0]#[s for s in os.listdir(self.path) if prefix in s][0]\n                audio_file, audio_sr = read_ogg_file(self.path, file)\n                audio_file = audio_file[int((self.data.loc[ID, 'seconds']-5)\/audio_lenght)*data_lenght:int(self.data.loc[ID, 'seconds']\/audio_lenght)*data_lenght]\n                audio_file_fft = np.abs(np.fft.fft(audio_file)[: len(audio_file)\/\/2])\n                # scale data\n                audio_file_fft = (audio_file_fft-audio_file_fft.mean())\/audio_file_fft.std()\n            X[i, ] = audio_file_fft\n            y[i, ] = self.data.loc[ID, self.data.columns[5:]].values\n        return X, y","5b9f695c":"train_generator = DataGenerator(path+'train_soundscapes\/', list_IDs_train, train_labels, batch_size)\nval_generator = DataGenerator(path+'train_soundscapes\/', list_IDs_val, train_labels, batch_size)\ntest_generator = DataGenerator(path+'test_soundscapes\/', list_IDs_test, test_data, batch_size)","1f205e26":"epochs = 2\nlernrate = 2e-3","799a407c":"model = Sequential()\nmodel.add(Conv1D(64, input_shape=(100, 1600\/\/2,), kernel_size=5, strides=4, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=(4)))\nmodel.add(Conv1D(64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(num_labels, activation='sigmoid'))","4db7caf0":"model.compile(optimizer = Adam(lr=lernrate),\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])","0ded697a":"model.summary()","9d7cc9a0":"history = model.fit_generator(generator=train_generator, validation_data=val_generator, epochs = epochs, workers=4)","04e03417":"fig, axs = plt.subplots(1, 2, figsize=(16, 4))\nfig.subplots_adjust(hspace = .2, wspace=.2)\naxs = axs.ravel()\nloss = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1, len(loss)+1)\naxs[0].plot(epochs, loss, 'bo', label='loss_train')\naxs[0].plot(epochs, loss_val, 'ro', label='loss_val')\naxs[0].set_title('Value of the loss function')\naxs[0].set_xlabel('epochs')\naxs[0].set_ylabel('value of the loss function')\naxs[0].legend()\naxs[0].grid()\nacc = history.history['binary_accuracy']\nacc_val = history.history['val_binary_accuracy']\naxs[1].plot(epochs, acc, 'bo', label='accuracy_train')\naxs[1].plot(epochs, acc_val, 'ro', label='accuracy_val')\naxs[1].set_title('Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Value of accuracy')\naxs[1].legend()\naxs[1].grid()\nplt.show()","1aef48b3":"y_pred = model.predict_generator(test_generator, verbose=1)","58d48df8":"y_test = np.where(y_pred > 0.5, 1, 0)","2e09d33a":"for row in samp_subm.index:\n    string = ''\n    for col in range(len(y_test[row])):\n        if y_test[row][col] == 1:\n            if string == '':\n                string += labels[col]\n            else:\n                string += ' ' + labels[col]\n    if string == '':\n        string = 'nocall'\n    samp_subm.loc[row, 'birds'] = string","a1d712e3":"output = samp_subm\noutput.to_csv('submission.csv', index=False)","f5853ca9":"output","238bd9b4":"# Train, Val And Test Data","da0ca230":"We extract to features, the primary label which is the name of the folder where the audio file is stored and the filename:","cd970cb8":"Finally we merge the labels with the original data:","88dcc82b":"# Export","bc91f4ba":"Test the Data Generator","59589b17":"# Path","8fef53f9":"Generate target label string:","71a3db51":"# Load Data","ea1ceb72":"Set all values grather than 0.5 to 1:","66247130":"For the Data Generator we want to define in the next step we need additional parameters:","97ab9062":"Plot the audio array:","a3e57ab4":"We extract all label of the train data:","819d1de6":"## Focus On Example","b95355da":"Display the audio of the file:","78108a25":"Listen to the bird:","677e98de":"Each audio file consists of 120 birds with a lenth of 5 seconds.","f5747f71":"# Define Model","e00ed7e5":"# Parameter\nBased on the EDA we define some parameters:","121614a0":"# Audio Data Generator\nWe use a Data Generator to load the data on demand.","0c4531a9":"# Predict Test Data","81a178b8":"# Intro\n\nWelcome to the [BirdCLEF 2021 - Birdcall Identification](https:\/\/www.kaggle.com\/c\/birdclef-2021\/overview) compedition.\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25954\/logos\/header.png)\n\nWe will give you first a short introduction to start with your work. The nex step is to show a short analysis befor definen a model with keras.\n\nWe recommend [this notebook](https:\/\/www.kaggle.com\/drcapa\/recognizesongapp-fromscratch-tutorial) for handling audio data tutorial.\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Feel free to leave a comment above the notebook. Thank you. <\/span>","d1e1d139":"# Overview","e14301e8":"# Libraries","af43ed75":"We focus on an example. The first audio file is named by","900930b8":"# Exploratory Data Analysis\nOur challenge is to identify which birds are calling in **long** recordings.\n\nThere are 20 long audio files in the folder train_soundscapes. And there are also 20 unique audio ids: ","8bbb47c5":"The numpy array has a lenght of 19,200,000. So every sample consists of 160,000 values. These 160,000 values describes 5 seconds of the audio file.\n\nWe split the file name into the audio_id and site:","752c8465":"We encode the labels and write them into a data frame:","c74d28e4":"We load the data and samplerate:","048922c7":"Plot [spectrogram](https:\/\/en.wikipedia.org\/wiki\/Spectrogram) with mel scaling:","14da1f8b":"## Focus On Labels\nThe target label birds is a space delimited list of any bird songs present in the 5 second window. So we have to encode the labels. Therefor we look on an example with 3 different birds:","271791e0":"Load the data and samplerate:","a21c3cf6":"# A Sample File\nWe focus on the sample in the first row of the train meta data.","32dca9a5":"# Functions\nWe define some helper functions.","e8edf0d2":"# Analyse Training","498bbd62":"We want to extract the first example with the id 1771. This bird we can here from 455 seconds to 460 seconds.  ","0b6b3d9b":"This representation of the labels we can use for further analysis. In instance for the distribution of the bird labels. We show the top 10 of the most observations:","819acd4d":"We focus on the samples with the label birds unequal to nocall. There are 4 samples","dc349f95":"So we have to split the long audio into 120 small audio."}}