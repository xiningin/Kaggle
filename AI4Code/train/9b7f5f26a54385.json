{"cell_type":{"61c794fc":"code","8d2d4b89":"code","3430bdc5":"code","a4b4528a":"code","49d01591":"code","86cb7fff":"code","dc522ab8":"code","df90052a":"code","c69185cb":"code","10eff5a3":"code","e934a34c":"code","5faa3675":"code","99d1be5d":"code","423d734d":"markdown","57b40859":"markdown","9d207400":"markdown","56019937":"markdown","e1ffcd40":"markdown","7b6f5896":"markdown"},"source":{"61c794fc":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils \nimport tensorflow as tf\nimport numpy as np \nimport pandas as pd\nimport numpy as np","8d2d4b89":"df = pd.read_csv('\/kaggle\/input\/shakespeare-plays\/Shakespeare_data.csv')\ndf.head(1)","3430bdc5":"import csv\n\ncorpus = []\n\nwith open('\/kaggle\/input\/shakespeare-plays\/Shakespeare_data.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    next(reader)        # to pass first row,header\n    for row in reader:\n        corpus.append(row[5])\n        \nprint(len(corpus))\nprint(corpus[:3])","a4b4528a":"import string\n\ndef text_cleaner(text):\n    text = \"\".join(car for car in text if car not in string.punctuation).lower()\n    text = text.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return text\n\ncorpus = [text_cleaner(line) for line in corpus]\n","49d01591":"# Tokenization is the process of splitting up a text into a list of individual words, or tokens.\n# corpus is too big if you try with all data, you can see this message\n# Your notebook tried to allocate more memory than is available. It has restarted.\ncorpus = corpus[:5000]\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\nword_index = tokenizer.word_index\ntotal_words = len(word_index) + 1\ntotal_words","86cb7fff":"# create input sequences using list of tokens\ninput_sequences =[]\n\nfor sentence in corpus:\n    token_list = tokenizer.texts_to_sequences([sentence])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n        ","dc522ab8":"# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, \n                                         maxlen=max_sequence_len, \n                                         padding='pre')) # as laste value is label.padding method is 'pre'","df90052a":"# create predictors and label\npredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n# create one-hot encoding of the labels\nlabel = tensorflow.keras.utils.to_categorical(label, num_classes=total_words)","c69185cb":"print(label[0])\nprint(label[0].shape)","10eff5a3":"model = Sequential()\nmodel.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(512)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","e934a34c":" history = model.fit(predictors, label, epochs=50,  verbose=1)","5faa3675":"import matplotlib.pyplot as plt\n\nacc = history.history['accuracy']\nloss = history.history['loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training accuracy')\nplt.title('Training accuracy')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.title('Training loss')\nplt.legend()\n\nplt.show()","99d1be5d":"seed_text = \"To be, or not to be\"\nnext_words = 50\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\n    if len(seed_text) % 10 == 0 :\n        seed_text+= '\\n'\nprint(seed_text)","423d734d":"# Import Libraries","57b40859":"# Load Dataset","9d207400":"# Data Cleaning","56019937":"- __Sequences__ and __n_Gram_squences__ for a sentence after __Padding__\n\n- Last value for every line is target, label. \n\n- line1 : 2\n\n- line2 : 66\n\n![n_gram](https:\/\/gurubux.files.wordpress.com\/2019\/09\/input_sequences_padded.jpg?w=641)","e1ffcd40":"<font color='red'> IF YOU FIND THIS NOTEBOOK HELPFUL, PLEASE LEAVE A UPVOTE :) THANKS IN ADVANCE <\/font>","7b6f5896":"![Tokenization](https:\/\/blog.floydhub.com\/content\/images\/2020\/02\/tokenize.png)"}}