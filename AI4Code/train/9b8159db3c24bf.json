{"cell_type":{"00be29d2":"code","2ba28de3":"code","dcc52dda":"code","913d3b24":"code","15875b72":"code","831122f3":"code","b23f6121":"code","1614a7c6":"code","b3a6e1ed":"code","a1e1d255":"code","23369d35":"code","423e084d":"code","1e0554c4":"code","d37a242b":"code","88b08fa6":"code","fc96fc3f":"code","6a86ac85":"code","a31bca5e":"code","94a93e7a":"code","4133415c":"code","989df3b6":"code","ff82e339":"code","3cbeb7c6":"code","dd189f6e":"code","0056a742":"code","edc72b90":"code","eeb864d0":"code","c0f312e4":"code","d4290e84":"code","b2d9b7cd":"code","4292bf7b":"code","13c90504":"code","6639cb08":"code","9763f8ae":"code","f9e29abb":"code","885a0168":"code","64a4a89f":"code","22a40bee":"code","ec2b5fda":"code","fd9090a5":"code","f1f7e011":"code","c468ae1f":"code","4dad431c":"code","de75f1f9":"code","3fae4087":"code","6269c653":"code","881265eb":"code","3449020e":"code","47650daf":"code","b9ec79e5":"code","1fedfb55":"code","7d69c342":"code","c243fad5":"code","53d2ba88":"code","ad174cf2":"markdown","6713a643":"markdown","a6569540":"markdown","7b5dfd2b":"markdown","d6f3ebf2":"markdown","0c20ac40":"markdown","a9388858":"markdown","8c813433":"markdown","57611545":"markdown","728ec1ba":"markdown","4cf79f3e":"markdown","0bf70a5e":"markdown","5051f849":"markdown","79f21202":"markdown","88b54381":"markdown","d4977d8f":"markdown","7a750681":"markdown","e246bd78":"markdown","b183bf49":"markdown","90b74bf2":"markdown","340044fb":"markdown","d171f655":"markdown","af48b870":"markdown","0c3f4476":"markdown","70248c93":"markdown","51a8c4af":"markdown","0f1baaff":"markdown","023fd5c0":"markdown","8e0f1825":"markdown","a59e128f":"markdown","bedffcf5":"markdown","7ac986aa":"markdown","2a61d227":"markdown","6a0d878e":"markdown","0453a525":"markdown","205ba3c2":"markdown","85c9af69":"markdown","1e743a9f":"markdown","62ebf5c9":"markdown","e00c83a7":"markdown","9c55efbf":"markdown","6002b03f":"markdown","3511429d":"markdown","f558c290":"markdown","5b221803":"markdown","1d8c62ba":"markdown","36d500f7":"markdown","9ce4fb69":"markdown","ed33b288":"markdown","0a744cfc":"markdown","a1412006":"markdown","406be879":"markdown","f9e10b73":"markdown","4ef2e8be":"markdown","dd8ef91b":"markdown","90aa6efe":"markdown","57b041e3":"markdown","50d70377":"markdown","8d7e0b5a":"markdown","ac85a459":"markdown","d9f2f673":"markdown","f2002ffa":"markdown","2aef8345":"markdown","1294e499":"markdown","0ff39cd5":"markdown","2e6ed488":"markdown","b864c811":"markdown","36deb5b8":"markdown","a6a73c58":"markdown","1fca64ea":"markdown","61913010":"markdown","ac88e1ec":"markdown","0a77edb3":"markdown","3350b65b":"markdown","3812ebd0":"markdown"},"source":{"00be29d2":"#importing modules\n\nimport warnings \nwarnings.filterwarnings('ignore')\nimport time\nt = time.time()\n\nprint('Importing startred...')\n\n# basic moduele\nimport os\nimport numpy as np\nimport pandas as pd\nimport re\nfrom scipy import stats\nfrom random import randint\n\n# visualization moduels\nimport matplotlib.pyplot as plt\nimport matplotlib \n%matplotlib inline\nimport seaborn as sns\nimport missingno as msno\n\n\n# preprocessing modules\nfrom sklearn.model_selection import (train_test_split, \n                                     KFold, \n                                     StratifiedKFold,\n                                    cross_val_score)\n\nfrom sklearn.preprocessing import (LabelEncoder,\n                                   StandardScaler, \n                                   MinMaxScaler, \n                                   OrdinalEncoder,\n                                   RobustScaler)\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\n# metrics\nfrom sklearn.metrics import (mean_squared_error, \n                             r2_score, \n                             mean_absolute_error)\n\n\n# modeling algos\nfrom sklearn.linear_model import (LogisticRegression,\n                                  Lasso, \n                                  ridge_regression,\n                                  LinearRegression)\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (AdaBoostRegressor, \n                              RandomForestRegressor,\n                              VotingRegressor, \n                              GradientBoostingRegressor)\nfrom xgboost import XGBRegressor\nfrom lightgbm import (LGBMRegressor,\n                      early_stopping)\n\nfrom sklearn.base import clone ## sklearn base models for stacked ensemble model\n\n#Interpretiability of the model\nimport shap\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n\n## miss\nfrom sklearn.pipeline import make_pipeline\n\n\n\nprint('Done,All the required modules are imported. Time elapsed: {}sec'.format(time.time()-t))","2ba28de3":"# loading dataset\nprint(\"Loading data....\", 'blue','on_white')\ndf = pd.read_csv('..\/input\/diamonds\/diamonds.csv', encoding = 'utf-8', delimiter = ',')\nprint(\"Done. Shape of the Dataset is {}\".format(df.shape))\ndf.head().round(1).style.hide_index()","dcc52dda":"# color palette\n\n#black  '#1F0C07',\n#light pink '#FA74BF'\n\ncolors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\",colors)\n\n\nsns.palplot(colors, size = 4)\nplt.text(-0.75,-0.75, 'Diamonds and Dollars: A Good Palette for A Good Story.',{'font':'serif', 'size':24, 'weight':'bold'})\nplt.text(-0.75,-0.65, 'Lets have fun with these colors, and hopefully stick to them throughout presentation.',{'font':'serif', 'size':16},alpha = 0.7)\nfor idx,values in enumerate(colors):\n    plt.text(idx,-0.40, colors[idx],{'font':'serif', 'size':20, 'weight':'bold'}, alpha = 0.5)\nplt.gcf().set_facecolor('#f5f6f6')\nplt.box(None)\nplt.axis('off')\nplt.text(3,0.65,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':9,  'color':'grey'})\nplt.show()","913d3b24":"## information and descriptive statistical dataframes \n\ndf.drop(columns = ['Unnamed: 0'], inplace = True) ### unnecessary coloumn, repeated index\n\ninfo_df = (pd.DataFrame({'Features':df.columns,'Non Null Count':df.count(), 'Null Count': df.isnull().sum(), 'Datatypes':df.dtypes,})\n        .sort_values(by='Datatypes').reset_index(drop = True))\nstats = df.describe(include = 'all').T.reset_index()\n\n\n\ntota = pd.merge( info_df,stats, right_on = 'index',left_on = 'Features',how = 'inner').drop(columns = ['index', 'count']).round(0)\ntota = tota.fillna(0)\n\n\n\ncolors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\ntota.style.bar(subset = ['mean', 'std', 'min', '25%','50%','75%', 'max'],axis = 1 ,color = colors[2])\\\n    .format({'mean':\"{:20,.0f}\", \n             'std':\"{:20,.0f}\", \n             'min':\"{:20,.0f}\", \n             '25%':\"{:20,.0f}\",\n             '50%':\"{:20,.0f}\",\n             '75%':\"{:20,.0f}\", \n             'max':\"{:20,.0f}\"})\\\n    .format({\"Features\": lambda x:  x.upper()},\n           )\\\n    .set_properties(**{'background-color': '#f9f9f9',                                                   \n                                    'color': 'black',                       \n                                    'border-color': 'white'})\\\n    .hide_index()","15875b72":"### Statistics dataframe creation\n\ndes_stats = df.describe(exclude = ['object']).drop(index = ['count'], axis = 0).T\nskew = []\nkurt = []\nnum_cols = df.select_dtypes(exclude = ['object']).columns\nfor col in num_cols:\n    skew.append(df[col].skew().round(1))\n    kurt.append(df[col].kurt().round(1))\n\nstats = pd.DataFrame({'skew':skew,'kurt':kurt}, index = num_cols)\n\nall_stats = pd.merge(left = des_stats,right = stats, left_index = True, right_index = True)","831122f3":"\nfig = plt.figure(figsize =(15,12), dpi = 80)\nfig.patch.set_facecolor('#f6f5f5')\n\ngs = fig.add_gridspec(8,8)\ngs.update(wspace = 0, hspace = 3)\n\nback_ground = '#f6f5f5'\nax1 = fig.add_subplot(gs[0:3,0:3])\nax2 = fig.add_subplot(gs[0:5,3])\nax3 = fig.add_subplot(gs[3:5,1:2])\nax4 = fig.add_subplot(gs[0:3,3])\n\naxes = [ax1,ax2, ax3]\n\nfor ax in axes:\n    for loc in ['left','right','top','bottom']:\n        ax.spines[loc].set_visible(False)\n    ax.set_facecolor('#f6f5f5')\n    \n\n#colormap for visualization\ncolormap  = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\n\n#### statistics of numerical features \nsns.heatmap(all_stats.drop(index = ['price']),\n            annot = True, ax = ax1, \n            vmin = -0, vmax = 120,\n            square = True, linewidths = 0.09,linecolor = '#f6f5f5',\n            cbar = False,\n            cmap =colormap, annot_kws={'font':'serif', 'size':9, 'weight':'normal', 'color':'black'}, alpha = 0.9)\n\n#### statistics of target\n#ax4 = ax2.twinx()\nsns.violinplot(y = df['price'], ax = ax2, palette= [colors[0]],inner = 'box', saturation = 0.9, linewidth = 0.6, **{'linecolor' :'black' })\n\n\n## labeling stats in violin plot\npr = df['price'].describe().drop(index = ['count', 'mean','std'], axis = 0)\n \nfor idx,value in pr.items():\n    ax2.plot([-0.25,0.5], [value,value], **{'linewidth': 0.5, 'linestyle': '--','color':'black' }, alpha = 0.5 )\n    if idx == 'max':\n        idx = 'Max'\n    elif idx == 'min':\n        idx = 'Min'\n    else:\n        idx = idx\n    ax2.text(0.5,value,'{}: {}'.format(idx,value) ,{'font':'serif', 'size':10, 'weight':'bold', 'color':'black'})\n    \n\nax2.axes.get_yaxis().set_visible(False)\n\n### price stats table\nprice_stats = all_stats.T['price'].to_frame().round(1)\nprice_stats = (price_stats.T[['mean', 'std', 'skew', 'kurt']]).T\nbbox = [1, 0, 0.35, 0.45]\nax4.table(cellText = price_stats.values, rowLabels = price_stats.index, \n          bbox=bbox,cellColours = np.array(['#f6f5f5','#f6f5f5','#f6f5f5','#f6f5f5']).reshape(-1,1),\n          rowColours = ['#f6f5f5','#f6f5f5','#f6f5f5','#f6f5f5'])\n\nax4.axis('off')\n\n#### statistics of categorical features\nsx = df.describe(exclude = ['int64', 'float64','float']).drop(index = ['count'], axis = 0).T\nsx['top']= ['4', '4', '6']\nsx = sx.astype(int)\n\nsns.heatmap(sx,annot = True,fmt = '1.0f',vmin = -0, vmax = 20,ax = ax3,\n            square = True, linewidths = 0.09,cbar = False,linecolor = '#f6f5f5',\n            cmap =colormap, annot_kws={'font':'serif', 'size':9, 'weight':'normal', 'color':'black'}, alpha =0.9)\n\n#### axis and labeling\nax1.set_yticklabels(labels = ['Carat', 'Depth %', 'Table %','X', 'Y', 'Z'],rotation = 0,**{'font':'serif', 'size':10, 'weight':'bold', 'color':'black'})\nax1.set_xticklabels(labels = ['Mean','Std','Min','25%','50%','75%', 'Max','Skew','Kurt'],rotation = 0,**{'font':'serif', 'size':10, 'weight':'bold', 'color':'black'})\n\nax3.set_yticklabels(labels = ['Cut', 'Color', 'Clarity'],rotation = 0,**{'font':'serif', 'size':10, 'weight':'bold', 'color':'black'})\nax3.set_xticklabels(labels = ['Unique', 'Top', 'Freq'],rotation = 0,**{'font':'serif', 'size':10, 'weight':'bold', 'color':'black'})\n\n### titles\nax1.text(-1., -0.7 , 'Numerical Features:',{'font':'serif', 'size':12, 'weight':'bold','color':'black'}, alpha = 0.9)\nax1.text(-1., -0.4, 'Standard Statistics of features',{'font':'serif', 'size':8, 'color':'black'}, alpha = 0.7)\n\nax2.text(-0.47, 22250 , 'Target Stats: ',{'font':'serif', 'size':12, 'weight':'bold','color':'black'}, alpha = 0.9)\nax2.text(-0.47, 21750 , 'Distribution of Price Percentiles',{'font':'serif', 'size':8,'color':'black'}, alpha = 0.7)\n\nax3.text(-2.75, -0.7 , 'Categorical Features:',{'font':'serif', 'size':12, 'weight':'bold','color':'black'}, alpha = 0.9)\nax3.text(-2.75, -0.4 , 'Uniques Values and Counts',{'font':'serif', 'size':8,  'color':'black'}, alpha = 0.9)\n\nax1.text(-2,-2.7,'Diamonds and Dollars : A Statistical Overview',{'font':'serif', 'size':20, 'weight':'bold', 'color':'black'})\nax1.text(-2,-1.6,\"Visualization of feature and target statistics to get insights about diamonds and 4C's. \\nIt seems like data have some outliers, in x,y,z, and price. Our skewness and \\nkurtosis values are way beyond acceptable, which need to be addressed.\",{'font':'serif', 'size':11,  'color':'black'}, alpha = 0.7)\n\nax2.text(0,-2500,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8,  'color':'grey'})\n\nfig.show()","b23f6121":"temp_df = df.copy()\ntemp_df = temp_df.replace(0,np.nan)\nfig, ax = plt.subplots(figsize = (7,3), dpi= 100)\nfig.patch.set_facecolor('#f5f6f6')\nax.set_facecolor('#f5f6f6')\ncolor = [ colors[2]  if (temp_df[col].isnull().sum()) > 0 else 'dimgrey' for col in df.columns ]\n\nax.axvspan(xmin = 6.5, xmax = 9.5, color = '#ffeb48',alpha = 0.9)\nmsno.bar(df = temp_df, ax = ax,color=color, log = False)\n\n\nfig.text(0.05,1.25,'Our Invincible Diamonds: Missing Values  ', {'font':'serif', 'size':20, 'weight':'bold'})\nfig.text(0.05,1.15,'We have some data missing, and for humor called them misisng diamonds,\\nand these are found in x,y,z features. Dropping them from Dataset.', {'font':'serif', 'size':10, 'weight':'normal'}, alpha = 0.7)\n\nax.axes.get_yaxis().set_visible(False)\nax.set_xticklabels(ax.axes.get_xticklabels(), rotation = 90, **{'font':'serif','size':14,'weight':'bold'})\n\nax.spines['bottom'].set_visible(True)\nfig.text(0.78,-0.13,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8,  'color':'grey'})\nfig.show()","1614a7c6":"### dataframe free of null values\n\nprint(\"Cleaning data....\")\nprint('Shape of Dataset after without any processing: {}'.format(df.shape))\ndf = df[(df['x'] != 0) | (df['y'] != 0) | (df['z'] != 0)]\n\n\nprint('Shape of Dataset after droping null values: {}'.format(df.shape))\n\n## checking for duplicate values\nif df.duplicated().any:\n    df = df.drop_duplicates()\n    temp3 = df.shape[0]\n    print('Duplicates exist,and Shape of Dataset after dropping duplicates: {}'.format(df.shape))\ndf.reset_index(drop = True, inplace = True)\n\norig_df = df.copy() ## lets keep it for futur comparision","b3a6e1ed":"#### zscore dataframe\n\nzscore_df = df.copy().select_dtypes(exclude = 'object') # this is not yet zscore just a copy of dataframe\nobj_col = zscore_df.columns\n\nfor col in obj_col:\n    zscore_df[col] = np.abs((df[col] - df[col].mean())\/df[col].std())\n    \n\n## outliers \nstd = 3\n    \noutliers = df[(zscore_df['x'] > std)| (zscore_df['y'] > std)| (zscore_df['z'] > std)| (zscore_df['price'] > std)| (zscore_df['depth'] > std)| (zscore_df['table'] > std)]\ndf = df.drop(index = outliers.index).reset_index(drop = True)\n\n\n\nprint('Outliers removing....')\nprint('Number of Outliers in the dataset are: {}'.format(outliers.shape[0]))\nprint('Shape of Dataset after removing Outliers: {}'.format(df.shape[0]))","a1e1d255":"temp = zscore_df.copy()\n\nfor col in temp.columns:\n    temp[col] = temp[col].apply( lambda x: np.nan if x > 3  else x )\n\nnullvalues = temp.isnull().sum()\n\nfig, ax = plt.subplots(figsize = (7,3), dpi = 100)\nfig.patch.set_facecolor(colors[2])\nax.set_facecolor(colors[2])\n\nfor loc in  ['left','right', 'bottom', 'top']:\n    ax.spines[loc].set_visible(False)\n    \nax.axes.get_yaxis().set_visible(False)\n#ax.axes.get_xaxis().set_visible(False)\n\nax.bar(x = nullvalues.index, height = nullvalues.values, width = 0.5, color = colors[1])\n\nfor idx,pa in enumerate(ax.patches):\n    ax.text(pa.get_x() + pa.get_width()\/8, pa.get_height() + 15, pa.get_height(),{'font':'serif', 'size':9, 'weight':'normal', 'color':'white'}, alpha = 1)\n    ax.text(pa.get_x() , -100, nullvalues.index[idx],{'font':'serif', 'size':12, 'weight':'bold', 'color':'white'}, alpha = 1)\n    \n\nfig.text(-0.1,1.11,'Diamonds and Dollars: Outliers Spread with Zscore ', {'font':'serif', 'size':19, 'weight':'bold','color':'white'})\nfig.text(-0.1,0.95,'Looks like our target have many outliers as per 3 standarad deviaiton \\nchoppoing, and remaining outliers are less in number but \\nimpact is severe. Few such are outliers in x,y,x values.', {'font':'serif', 'size':10, 'weight':'normal','color':'white'}, alpha = 0.95)\n\nax.set_xticks(ticks = '')\n\n\nfig.text(0.85,-0.06,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':7,  'color':'white'})\nfig.show()","23369d35":"### Statistics dataframe creation after statistical insights\n\ndes_stats = df.describe(exclude = ['object']).drop(index = ['count'], axis = 0).T\nskew = []\nkurt = []\nnum_cols = df.select_dtypes(exclude = ['object']).columns\nfor col in num_cols:\n    skew.append(df[col].skew().round(1))\n    kurt.append(df[col].kurt().round(1))\n\nstats = pd.DataFrame({'skew':skew,'kurt':kurt}, index = num_cols)\n\nall_stats = pd.merge(left = des_stats,right = stats, left_index = True, right_index = True)","423e084d":"### Stats viualization after statistical insights\n\nfig = plt.figure(figsize =(15,12), dpi = 80)\nfig.patch.set_facecolor('#f6f5f5')\n\ngs = fig.add_gridspec(8,8)\ngs.update(wspace = 0, hspace = 3)\n\nback_ground = '#f6f5f5'\nax1 = fig.add_subplot(gs[0:3,0:3])\nax2 = fig.add_subplot(gs[0:5,3])\nax3 = fig.add_subplot(gs[3:5,1:2])\nax4 = fig.add_subplot(gs[0:3,3])\n\naxes = [ax1,ax2, ax3]\n\nfor ax in axes:\n    for loc in ['left','right','top','bottom']:\n        ax.spines[loc].set_visible(False)\n    ax.set_facecolor('#f6f5f5')\n    \n\n#colormap for visualization\ncolormap  = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\n\n#### statistics of numerical features \nsns.heatmap(all_stats.drop(index = ['price']),\n            annot = True, ax = ax1, \n            vmin = -0, vmax = 120,\n            square = True, linewidths = 0.09,linecolor = '#f6f5f5',\n            cbar = False,\n            cmap =colormap, annot_kws={'font':'serif', 'size':9, 'weight':'normal', 'color':'black'}, alpha = 0.9)\n\n#### statistics of target\n#ax4 = ax2.twinx()\nsns.violinplot(y = df['price'], ax = ax2, palette= [colors[0]],inner = 'box', saturation = 0.9, linewidth = 0.6, **{'linecolor' :'black' })\n\n\n## labeling stats in violin plot\npr = df['price'].describe().drop(index = ['count', 'mean','std'], axis = 0)\n \nfor idx,value in pr.items():\n    ax2.plot([-0.25,0.5], [value,value], **{'linewidth': 0.5, 'linestyle': '--','color':'black' }, alpha = 0.5 )\n    if idx == 'max':\n        idx = 'Max'\n    elif idx == 'min':\n        idx = 'Min'\n    else:\n        idx = idx\n    ax2.text(0.5,value,'{}: {}'.format(idx,value) ,{'font':'serif', 'size':10, 'weight':'bold', 'color':'black'})\n    \n\nax2.axes.get_yaxis().set_visible(False)\n\n### price stats table\nprice_stats = all_stats.T['price'].to_frame().round(1)\nprice_stats = (price_stats.T[['mean', 'std', 'skew', 'kurt']]).T\nbbox = [1, 0, 0.35, 0.45]\nax4.table(cellText = price_stats.values, rowLabels = price_stats.index, \n          bbox=bbox,cellColours = np.array(['#f6f5f5','#f6f5f5','#f6f5f5','#f6f5f5']).reshape(-1,1),\n          rowColours = ['#f6f5f5','#f6f5f5','#f6f5f5','#f6f5f5'])\n\nax4.axis('off')\n\n#### statistics of categorical features\nsx = df.describe(exclude = ['int64', 'float64','float']).drop(index = ['count'], axis = 0).T\nsx['top']= ['4', '4', '6']\nsx = sx.astype(int)\n\nsns.heatmap(sx,annot = True,fmt = '1.0f',vmin = -0, vmax = 20,ax = ax3,\n            square = True, linewidths = 0.09,cbar = False,linecolor = '#f6f5f5',\n            cmap =colormap, annot_kws={'font':'serif', 'size':9, 'weight':'normal', 'color':'black'}, alpha =0.9)\n\n#### axis and labeling\nax1.set_yticklabels(labels = ['Carat', 'Depth %', 'Table %','X', 'Y', 'Z'],rotation = 0,**{'font':'serif', 'size':10, 'weight':'bold', 'color':'black'})\nax1.set_xticklabels(labels = ['Mean','Std','Min','25%','50%','75%', 'Max','Skew','Kurt'],rotation = 0,**{'font':'serif', 'size':10, 'weight':'bold', 'color':'black'})\n\nax3.set_yticklabels(labels = ['Cut', 'Color', 'Clarity'],rotation = 0,**{'font':'serif', 'size':10, 'weight':'bold', 'color':'black'})\nax3.set_xticklabels(labels = ['Unique', 'Top', 'Freq'],rotation = 0,**{'font':'serif', 'size':10, 'weight':'bold', 'color':'black'})\n\n### titles\nax1.text(-1., -0.7 , 'Numerical Features:',{'font':'serif', 'size':12, 'weight':'bold','color':'black'}, alpha =0.9)\nax1.text(-1., -0.4, 'Standard Statistics of features',{'font':'serif', 'size':8, 'color':'black'}, alpha =0.7)\n\nax2.text(-0.47, 18500 , 'Target Stats: ',{'font':'serif', 'size':12, 'weight':'bold','color':'black'}, alpha= 0.9)\nax2.text(-0.47, 17850 , 'Distribution of Price Percentiles',{'font':'serif', 'size':8,'color':'black'}, alpha =0.7)\n\nax3.text(-2.75, -0.7 , 'Categorical Features:',{'font':'serif', 'size':12, 'weight':'bold','color':'black'}, alpha =0.9)\nax3.text(-2.75, -0.4 , 'Uniques Values and Counts',{'font':'serif', 'size':8,  'color':'black'}, alpha =0.7)\n\n\nax1.text(-2,-2.7,'Diamonds and Dollars : After All the Data Correction',{'font':'serif', 'size':20, 'weight':'bold', 'color':'black'})\nax1.text(-2,-1.6,\"With the few tweeks, such as missing values, duplicate value, \\nand outlier removal data looks much better. All the stats \\nare with in the acceptable ranges.\",{'font':'serif', 'size':11,  'color':'black'}, alpha = 0.7)\n\nax2.text(0,-2500,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8,  'color':'grey'})\n\nfig.show()","1e0554c4":"### binnining and feature engineering\n\n\n\ndf_backup = df.copy()\n\n\n####################### Rarity of the diamond\nc1 = [(df['carat'] > 1.5)] ## rare \nc2 = [(df['carat'] < 1) | ((df['cut'] == 'Ideal') | (df['cut'] == 'Premium') | (df['cut'] == 'Very Good') | (df['cut'] == 'Good')) ] ## common\nc5 = [(df['carat'] < 0.5)] ## very common\n\n## rare\nc3 = [(df['carat'] > 0.50) & (df['carat'] <1.0) & ((df['clarity'] == 'VVSI') | (df['clarity'] == 'IF')) & ((df['color'] == 'D'))]\n\n## ultra rare\nc4 = [(df['carat'] > 1) & ( (df['clarity'] == 'VVS1')|(df['clarity'] == 'IF')) & ((df['color'] == 'D') | (df['color'] == 'E')) & ((df['cut'] == 'Ideal')) ]\n\ndf.loc[c2[0], 'rarity'] = 'Common'\ndf.loc[c5[0], 'rarity'] = 'Very Common'\ndf.loc[c1[0], 'rarity'] = 'Rare'\ndf.loc[c3[0], 'rarity'] = 'Rare'\ndf.loc[c4[0], 'rarity'] = 'Ultra Rare'\n\ndf = df.fillna('Common')\n\n\n############# size of the diamond\ndf['diamond_size'] = pd.cut(df['carat'], bins = [0,0.5,1,2,3], labels = ['Tiny','Small','Normal','Big'])\ndf['diamond_size'] = df['diamond_size'].astype('object')\n\n########shape of the diamond\n\ndf['shape'] = (np.abs(df['x'] - df['y'])).apply(lambda x: 'Regualr' if x <=0.03 else 'Fancy')\n\n\n######## Volume of the diamond\n\ndf['Volume'] = df['x']*df['y']*df['z']","d37a242b":"### univariate ananalysis\n\n\nfig =  plt.figure(figsize = (14,8.5), dpi = 85)\nfig.patch.set_facecolor('#f5f6f6')\n\n\n                                                    ##### Note to reader I could have used subplots feature, but going with grid\n                                                           #spec for more controlover plots\ngs = fig.add_gridspec(3,3)\ngs.update(wspace =0.2,hspace = 0.2)\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[1,1])\nax5 = fig.add_subplot(gs[1,2])\n\n\nax6 = fig.add_subplot(gs[2,0])\nax7 = fig.add_subplot(gs[2,1])\nax8 = fig.add_subplot(gs[2,2])\n\n\n\naxes = [ax0,ax1,ax2,ax3, ax4,ax5,ax6,ax7,ax8]\n\n\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')\n    ax.tick_params(axis='x',\n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 1.5)\n    ax.tick_params(axis='y', colors= 'black')\n    ax.axes.get_yaxis().set_visible(False)\n    \n    for loc in ['left', 'right', 'top', 'bottom']:\n        ax.spines[loc].set_visible(False)\n        \ncols = df.select_dtypes(exclude = 'object').columns\n\n### ax0- carat\n\nsns.kdeplot(x = df[cols[0]],fill = True, color = colors[0], alpha = 1, ax = ax0)\nsns.kdeplot(x = df[cols[0]],color = colors[1], alpha = 1, ax = ax0)\nax0.set_xlabel(xlabel = '')\nax0.text((df[cols[0]].max() ), 1.4,'Carat', **{'font':'serif', 'size':18,'weight':'bold'}, alpha = 1)\n\n\n\n\n\n### ax1 - depth\nsns.kdeplot(x = df[cols[1]], color = colors[0], alpha = 1, ax = ax1)\nsns.kdeplot(x = df[cols[1]], fill = True, color = colors[1], alpha = 1, ax = ax1)\nax1.set_xlabel(xlabel = '')\nax1.text((df[cols[1]].max()-2 ), 0.35,'Depth %', **{'font':'serif', 'size':18,'weight':'bold'}, alpha = 1)\n\n\n\n\n### ax2 -- table\nsns.kdeplot(x = df[cols[2]],color = colors[0], alpha = 1, ax = ax2)\nsns.kdeplot(x = df[cols[2]], fill = True, color = colors[1], alpha = 1, ax = ax2)\n\nax2.set_xlabel(xlabel = '')\nax2.text((df[cols[2]].max()), 0.25,'Table %', **{'font':'serif', 'size':18,'weight':'bold'}, alpha = 1)\n\n\n\n\n### ax3 --- price\nsns.kdeplot(x = df[cols[3]],  color = colors[0], alpha = 1, ax = ax3)\nsns.kdeplot(x = df[cols[3]], fill = True, color = colors[1], alpha = 1, ax = ax3)\nax3.set_xlabel(xlabel = '')\nax3.text((df[cols[3]].max()), 0.00025,'Price', **{'font':'serif', 'size':18,'weight':'bold'}, alpha = 1)\n\n\n\n\n\n### ax4 --- free space\n\nax4.axes.get_xaxis().set_visible(False)\nax4.spines['bottom'].set_visible(False)\nax4.text(-0.01,0.25,'X,y,z are physical dimensions of diamond, \\nwith carat as weight prameter. Volume, \\nDepth%, Table% are scaling measurements. \\nPrice is market variable, and target.',\n           **{'font':'serif', 'size':11.5,'weight':'bold'}, alpha = 0.9 )\n\n\n### ax5 --- Volume\nsns.kdeplot(x = df[cols[7]], fill = True, color = colors[0], alpha = 1, ax = ax5)\nsns.kdeplot(x = df[cols[7]],  color = colors[1], alpha = 1, ax = ax5)\nax5.set_xlabel(xlabel = '')\nax5.text((df[cols[7]].max()), 0.008,'Volume', **{'font':'serif', 'size':18,'weight':'bold'}, alpha = 1)\n\n\n### ax6 ---- X\nsns.kdeplot(x = df[cols[4]], fill = True, color = colors[0], alpha = 1, ax = ax6)\nsns.kdeplot(x = df[cols[4]], color = colors[1], alpha = 1, ax = ax6)\nax6.set_xlabel(xlabel = '')\nax6.text((df[cols[4]].max()), 0.4,'X', **{'font':'serif', 'size':18,'weight':'bold'}, alpha = 1)\n\n\n\n### ax7 --- Y\nsns.kdeplot(x = df[cols[5]], fill = True, color = colors[0], alpha = 1, ax = ax7)\nsns.kdeplot(x = df[cols[5]], color = colors[1], alpha = 1, ax = ax7)\nax7.set_xlabel(xlabel = '')\nax7.text((df[cols[5]].max() ), 0.4,'Y', **{'font':'serif', 'size':18,'weight':'bold'}, alpha = 1)\n\n\n\n\n### ax8 ---- z\nsns.kdeplot(x = df[cols[6]], fill = True, color = colors[0], alpha = 1, ax = ax8)\nsns.kdeplot(x = df[cols[6]],  color = colors[1], alpha = 1, ax = ax8)\nax8.set_xlabel(xlabel = '')\nax8.text((df[cols[6]].max()), 0.6,'Z', **{'font':'serif', 'size':18,'weight':'bold'}, alpha = 1)\n\nax8.text(5.,-0.25,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':12, 'weight':'bold',},alpha = 0.8)\n\n\nfig.text(0.07,0.95,'Diamonds and Dollars: An Overview of Univariate Numerical Features' ,**{'font':'serif', 'size':25,'weight':'bold',}, alpha = 0.9)\n\nfig.text(0.07,0.88,'All the numerical features are skewed towards left except for depth and table. A nice log Transformation could\\n be helpful to bring more symmetry to all Features. It can be seen that \\nall the skewed features have similar kind of distribuitons.' ,**{'font':'serif', 'size':12,}, alpha = 0.9)\n\n\nfig.show()\n","88b08fa6":"### univariate ananalysis\n\n\nfig =  plt.figure(figsize = (15,6.5), dpi = 90)\nfig.patch.set_facecolor('#f5f6f6')\n\n\n                                                    ##### Note to reader I could have used subplots feature, but going with grid\n                                                           #spec for more controlover plots\ngs = fig.add_gridspec(2,3)\ngs.update(wspace =0.1,hspace = 0.2)\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[1,1])\nax5 = fig.add_subplot(gs[1,2])\n\n\n\n\naxes = [ax0,ax1,ax2,ax3, ax4,ax5,]\n\n\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')\n    ax.tick_params(axis='x',\n                   labelsize = 1, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 1)\n    ax.tick_params(axis='y', colors= 'black')\n    ax.axes.get_yaxis().set_visible(False)\n    ax.axes.get_xaxis().set_visible(True)\n    \n    for loc in ['left', 'right', 'top', 'bottom']:\n        ax.spines[loc].set_visible(False)\n        \ncols = df.select_dtypes(exclude = ['int64','float64','float']).columns\nlabels = ['Cut', 'Color', 'Clarity', 'Rarity', 'Diamond Size', 'Shape']\n\n\n\n### ax0- carat\ns = 250\n\nfor col,ax,label in zip(cols,axes,labels):\n    ax.bar(x = df[col].value_counts().index, height = df[col].value_counts().values, width = 0.1, color = colors[0] )\n    ax.scatter(x = df[col].value_counts().index, y = df[col].value_counts().values, s = s,color = colors[0] )\n    ht = df[col].value_counts().values.max()\n    ax.text(-1,ht\/2.2 ,label,**{'font':'serif', 'size':12, 'weight':'bold', 'rotation' : 'vertical'}, alpha = 1)\n    ax.set_xticklabels(df[col].value_counts().index , rotation = 0,**{'font':'serif', 'size':7.5, 'weight':'bold'}, alpha = 1)\n    \n    for pa in ax.patches: \n        ax.text((pa.get_x() - 2*pa.get_width() ), pa.get_height()+1600, pa.get_height(), **{'font':'serif', 'size':9, 'weight':'bold',}, alpha = 1)\n    height = [ val - 2000 if (val - 2000) > 0 else 0 for val in df[col].value_counts().values ]\n    ax.bar(x = df[col].value_counts().index, height = height , width = 0.1, color = colors[1] )\n        \n\n\n\nfig.text(0.09,1.06,'Diamonds and Dollars: An Overview of Univariate Categorical Features' ,**{'font':'serif', 'size':18,'weight':'bold', }, alpha = 1)\n\nfig.text(0.09,0.97,'''Cut, Clarity, and color are most important features as per the diamond grading. Remaining three \ncategories are feature generated, and no idea about its importance on price. Most common \nDiamond cut is ideal cut, and this is interesting to note as this is a crutial.''' ,**{'font':'serif', 'size':12,'weight':'normal',}, alpha = 0.75)\n\n\nax0.set_xlim(-1,5)\nax1.set_xlim(-1,7)\nax2.set_xlim(-1,10)\nax3.set_xlim(-1.6,4)\nax4.set_xlim(-2,5)\nax5.set_xlim(-2,5)\n\n\nfig.text(0.75,0.075,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':9, 'weight':'bold',},alpha = 0.75)\n\nfig.show()\n    ","fc96fc3f":"actualdf = df_backup.copy()\n\nfeatdf = df.copy()\n\nfor col in actualdf.select_dtypes(exclude = ['int','float','float64','int64']).columns:\n    le = LabelEncoder()\n    actualdf[col] = le.fit_transform(actualdf[col])\n\nfor col in featdf.select_dtypes(exclude = ['int','float','float64','int64']).columns:\n    le = LabelEncoder()\n    featdf[col] = le.fit_transform(featdf[col])\n\n\n\n\n## actual correlations\nact_corr = actualdf.corr()\nmask1 = np.triu(np.ones_like(act_corr, dtype=np.bool))\nmask1 = mask1[1:, :-1]\nact_corr = act_corr.iloc[1:,:-1].copy()\n\n## featured correlations\nfea_corr = featdf.corr()\nmask2 = np.triu(np.ones_like(fea_corr, dtype=np.bool))\nmask2 = mask2[1:, :-1]\nfea_corr = fea_corr.iloc[1:,:-1].copy()\n\ncust_colors = ['#FB5B68','#FFBDB0','#FFEB48','#2676A1',]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\",cust_colors)\n\nfig, ax = plt.subplots(1,2, figsize = (20,10), dpi = 88)\nfig.patch.set_facecolor('#f5f6f6')\n\naxes = ax.ravel()\n\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')\n\n\n### actual heatmap\nsns.heatmap(data = act_corr, ax = axes[0], vmin= -1,vmax = 1 ,\n            annot = True, fmt = '1.0g', annot_kws={'font':'serif', 'size':9, 'weight':'normal', 'color':'black'}, \n            square = True, linewidth = 1, linecolor = '#f6f5f5',\n             cmap = colormap, mask = mask1, cbar = False,alpha = 1)\n\n\n### featured heatmap\nsns.heatmap(data = fea_corr, ax = axes[1], vmin = -1, vmax = 1,\n            annot = True, fmt = '1.0g',annot_kws={'font':'serif', 'size':9, 'weight':'normal', 'color':'black'},\n            square = True, linewidth = 1, linecolor = '#f6f5f5',\n             cmap = colormap, mask = mask2, cbar = False, alpha = 1)\n\n\nfor ax in axes:\n    ax.set_xticklabels(ax.get_xticklabels(), rotation =90,**{'font':'serif', 'size':12, 'weight':'bold', 'color':'black'})\n    ax.set_yticklabels(ax.get_yticklabels(), rotation =0,**{'font':'serif', 'size':12, 'weight':'bold', 'color':'black'})\n    \n    \naxes[0].text(4, 1, 'Actual Features',{'font':'serif', 'size':18, 'weight':'bold', 'color':'black'})\naxes[1].text(4,1, 'Feature Engineered Features',{'font':'serif', 'size':18, 'weight':'bold', 'color':'black'})\n\n\nfig.text(0.095,0.965, 'Diamonds and Dollars: Correlation Maps for Actual And Augumented Data.',{'font':'serif', 'size':20, 'weight':'bold', 'color':'black'})\nfig.text(0.095,0.88, '''Only x,y,z,volume have a strong correlation with price, So, lets focus on these features\nalong with primary category of 4C's of Diamond, which are carat,cut,color,clarity.\nWith some sort of visualization lets understood the impact \nof highly correlated features on price.''',{'font':'serif', 'size':14, 'weight':'normal', 'color':'black'})\n\n\nfig.text(0.7,0.01,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':9, 'weight':'bold','color':'black'},alpha = 0.75)\n\n\nfig.show()","6a86ac85":"def custom_scatter(x,y,value, size = None,figsize = None, ax = None, marker = None, color = None):\n    \n    x_labels = [val for val in sorted(x.unique())]\n    y_labels  = [val for val in sorted(y.unique())]\n    \n    x_to_num = {val:idx for idx,val in enumerate(x_labels)}\n    y_to_num = {val:idx for idx,val in enumerate(y_labels)}\n    \n    \n    x_values = x.map(x_to_num)\n    y_values = y.map(y_to_num)\n    \n\n    if size == None:\n        size = 1\n    if ax == None:\n        fig,ax = plt.subplots(figsize = figsize)\n    if marker == None:\n        marker = None\n    else:\n        marker = marker\n        \n    ax.scatter(x = x_values,y = y_values, s = value * size, c = color, marker = marker)\n    \n    \n    ax.set_xticks(ticks = x_values.unique())\n    ax.set_yticks(ticks = y_values.unique())\n    \n    ax.set_xticklabels(labels = x_labels,**{'font':'serif', 'size':12,'weight':'bold'}, alpha = 1)\n    ax.set_yticklabels(labels = y_labels,**{'font':'serif', 'size':12,'weight':'bold'}, alpha = 1)\n    \n    ax.set_xlim(x_values.min()-0.5, x_values.max()+0.5)\n    ax.set_ylim(y_values.min()-0.5, y_values.max()+0.5)\n    \n    \n    \n    ax.grid(False, 'major')\n    \n    ax.grid(False, 'minor', )\n    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n    \n    \n    #ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5]) \n    #ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n    \n    return None","a31bca5e":"\ntem = df.copy()\ntem['price_cat'] = pd.cut(df['price'], bins = [0,1500,4000,10000,100000], labels = ['Cheaper','Affordable', 'Expensive', 'Very Expensive'])\ntem['price_cat'] = tem['price_cat'].astype('object')\n\n\ntab = pd.crosstab(tem['diamond_size'],tem['price_cat'])\ntab_val = pd.melt(tab.reset_index(),id_vars = 'diamond_size')\ntab_val.columns = ['x','y','value']\n\n\nfig = plt.figure(figsize = (12,12), dpi = 65)\nfig.patch.set_facecolor('#f5f6f6')\ngs = fig.add_gridspec(20,20)\ngs.update(wspace = 0.6, hspace = 0)\n\nax0 = fig.add_subplot(gs[0:,0:])\nax0.set_facecolor('#f5f6f6')\nax0.axes.get_xaxis().set_visible(False)\nax0.axes.get_yaxis().set_visible(False)\nfor loc in ['left', 'right', 'top', 'bottom']:\n        ax0.spines[loc].set_visible(False)\n\n\n\nax1 = fig.add_subplot(gs[0:4,0:10])\n\nax2 = fig.add_subplot(gs[3:19,12:20])\n\nax3 = fig.add_subplot(gs[5:15,0:10])\n\nax4 = fig.add_subplot(gs[16:20,0:10])\n\naxes = [ax1,ax2,ax3,ax4]\n\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')  \n    ax.tick_params(axis='both',\n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.001)\n    \n    for loc in ['left', 'right', 'top', 'bottom']:\n        ax.spines[loc].set_linewidth(1)\n\n        \n##############################################violin plot\nsns.violinplot(x = -df['carat'],  ax = ax1, color = colors[1], alpha = 1, linecolor = 'black')\n\n\n## violin\nticks = [3.5,3,2.5,2,1.5,1,0.5,0]\nax1.set_xticklabels(labels = ticks, **{'font':'serif','size':12, 'weight':'bold'},alpha = 1)\nax1.set_xlabel('')\nfor loc in ['left', 'right', 'top', 'bottom']:\n    ax1.spines[loc].set_visible(False)\n\n\n############################################### scatter plot and regplot\nsns.regplot(x = 'carat', y = 'price',data = tem, ax = ax2, color = colors[1],)\nsns.scatterplot(x = 'carat', y = 'price',data = tem, ax = ax2, alpha = 0.7,size = 0.5, color = colors[2],**{'linewidth' : 0.1})\n\nax2.legend(labels = [])\n\nax2.set_yticklabels(np.arange(0,18000,2000),**{'font':'serif','size':12, 'weight':'bold'},alpha = 1)\nax2.set_xticklabels('')\nax2.set_ylabel('')\nax2.set_xlabel('')\nfor loc in ['left', 'right', 'top', 'bottom']:\n    ax2.spines[loc].set_visible(False)\n\n\n## generating spans for diamond sizes\n## www.kaggle.com\/subinium\/simple-matplotlib-visualization-tips\/  --- #thanks to @subinium\n\nbin_labels = ['Tiny','Small','Normal','Big']\nsize_bins = [[0, 0.5], [0.5,  1], [1, 2], [2, 3]]\n\nfor idx, label in enumerate(bin_labels):\n        ax2.annotate(label,\n                    xy=(sum(size_bins[idx])\/2 ,17000),\n                    xytext=(0,0), textcoords='offset points',\n                    va=\"center\", ha=\"center\",\n                    **{'font':'serif','size':12, 'weight':'bold','color':'white'},\n                    bbox=dict(boxstyle='round4', pad=0.2, color=colors[idx], alpha=1))\n        ## adding span over region\n        ax2.axvspan(size_bins[idx][0],size_bins[idx][1], ymax = 1, color=colors[idx], alpha=0.2)\n\nax2.set_ylim(0,17000)\n\n####################################################### scattermap\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], size = 0.25, ax = ax3, color = colors[2])\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], size = 0.2, ax = ax3, color = colors[1])\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], size = 0.07, ax = ax3, color = colors[0])\nax3.set_facecolor('#f5f6f6')\n\n\n##################################################### final ratio plot\ndia_size = round(tem.diamond_size.value_counts(normalize = True) * 100,0).astype(int)\n\nax4.barh(y = dia_size.index, width = dia_size.values, height = 0.4, color = colors[2])\n\n\n\nfor pa in ax4.patches: \n        ax4.text((pa.get_width()), pa.get_y(),'{} %'.format(pa.get_width()), **{'font':'serif', 'size':12, 'weight':'bold'}, alpha = 1)\n\nax4.barh(y = dia_size.index, width = dia_size.values, height = 0.3, color = colors[1])\nax4.set_yticklabels(labels = dia_size.index,**{'font':'serif', 'size':12,'weight':'bold'}, alpha = 1)\n\n\n\nax4.barh(y = dia_size.index, width = dia_size.values -00.1, height = 0.2, color = colors[0])\nfor loc in ['left', 'right', 'top', 'bottom']:\n    ax4.spines[loc].set_visible(False)\nax4.axes.get_xaxis().set_visible(False)\n\n\n\n\n### titles and descriptions\n\nfig.text(0,0.98, 'Diamonds and Dollars: How carat influences Price of the Diamond?',{'font':'serif', 'size':20.,'weight':'bold'}, alpha = 1)\n\n\nfig.text(0,0.92,'''Its no surprice to see that Carat have such a strong correlation with the price.\nAs the size of the diamond increases, so does it values. And In same time\nbigger diamonds are rare to find so it price would be higher \nthan regular diamond''',{'font':'serif', 'size':11,'weight':'normal'}, alpha = 0.8)\n\n\nfig.text(0.05, 0.88, 'Diamond Carat vs Size:',{'font':'serif', 'size':16,'weight':'bold'}, alpha = 0.9)\n\n\nfig.text(0.55, 0.88, 'Diamond Price (USD) vs Size:',{'font':'serif', 'size':16,'weight':'bold'}, alpha = 0.9)\n\nfig.text(0.75,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10, 'weight':'bold'},alpha = 0.75)\nfig.show()","94a93e7a":"tem = df.copy()\ntem['price_cat'] = pd.cut(df['price'], bins = [0,1500,4000,10000,100000], labels = ['Cheaper','Affordable', 'Expensive', 'Very Expensive'])\ntem['price_cat'] = tem['price_cat'].astype('object')\n\n\ntab = pd.crosstab(tem['cut'],tem['price_cat'])\ntab_val = pd.melt(tab.reset_index(),id_vars = 'cut')\ntab_val.columns = ['x','y','value']\n\ncut_labels = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\ncut_to_num = { val : idx for idx,val in enumerate(cut_labels)}\n\ntem['cut'] = tem['cut'].map(cut_to_num)\n\n\nfig = plt.figure(figsize = (12,12), dpi = 65)\nfig.patch.set_facecolor('#f6f5f5')\ngs = fig.add_gridspec(20,20)\ngs.update(wspace = 0.6, hspace = 0)\n\nax0 = fig.add_subplot(gs[0:,0:])\nax0.set_facecolor('#f5f6f6')\nax0.axes.get_xaxis().set_visible(False)\nax0.axes.get_yaxis().set_visible(False)\nfor loc in ['left', 'right', 'top', 'bottom']:\n        ax0.spines[loc].set_visible(False)\n\n\n#ax1 = fig.add_subplot(gs[0:4,0:10])\n\nax2 = fig.add_subplot(gs[2:18,13:20])\n\nax3 = fig.add_subplot(gs[2:13,0:11])\n\nax4 = fig.add_subplot(gs[14:19,0:10])\n\naxes = [ax1,ax2,ax3,ax4]\n\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')  \n    ax.tick_params(axis='both', \n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.001)\n    \n    for loc in ['left', 'right', 'top', 'bottom']:\n        ax.spines[loc].set_linewidth(1)\n\n            \n    \n    \n############################################### scatter plot and regplot\nsns.regplot(x = 'cut', y = 'price',data = tem, ax = ax2, color = colors[1])\nsns.stripplot(x = 'cut', y = 'price',data = tem, ax = ax2, alpha = 0.5, color = colors[2], size = 2)\nax2.set_yticklabels(np.arange(0,18000,2000),**{'font':'serif','size':12, 'weight':'bold'},alpha = 1)\nax2.set_xticklabels('')\nax2.set_ylabel('')\nax2.set_xlabel('')\nfor loc in ['left', 'right', 'top', 'bottom']:\n    ax2.spines[loc].set_visible(False)\n\n    \nbin_labels = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\nsize_bins = [[-0.5,0.5],[0.5, 1.5], [1.5,  2.5], [2.5, 3.5], [3.5, 4.5]]\n\ncol_here = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0','dimgrey']\nfor idx, label in enumerate(bin_labels):\n        ax2.annotate(label,\n                    xy=(sum(size_bins[idx])\/2 ,17000),\n                    xytext=(0,0), textcoords='offset points',\n                    va=\"center\", ha=\"center\",rotation = 90,\n                    **{'font':'serif','size':12, 'weight':'bold','color':'white'},\n                    bbox=dict(boxstyle='round4', pad=0.2, color=col_here[idx], alpha=1))\n        ## adding span over region\n        ax2.axvspan(size_bins[idx][0],size_bins[idx][1], ymax = 1, color=col_here[idx], alpha=0.2)\n\nax2.set_ylim(0,17000)\n\n\n####################################################### scattermap\n\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], marker = 'D',size = 0.25, ax = ax3, color = colors[2])\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], marker = 'D',size = 0.2, ax = ax3, color = colors[1])\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], marker = 's',size = 0.05,ax = ax3, color = colors[0])\nax3.set_facecolor('#f5f6f6')\n\n\n##################################################### final ratio plot\n\ncut = round(df.cut.value_counts(normalize = True) * 100,0).astype(int)\n\nax4.barh(y = cut.index, width = cut.values, height = 0.3, color = colors[2])\n\n\n\nfor pa in ax4.patches: \n        ax4.text((pa.get_width()), pa.get_y(),'{} %'.format(pa.get_width()), **{'font':'serif', 'size':12, 'weight':'bold'}, alpha = 1)\n\nax4.set_yticklabels(labels = cut.index,**{'font':'serif', 'size':12,'weight':'bold'}, alpha = 1)\n\n\nax4.barh(y = cut.index, width = cut.values -00.1, height = 0.2, color = colors[1])\nax4.barh(y = cut.index, width = cut.values -00.1, height = 0.1, color = colors[0])\nfor loc in ['left', 'right', 'top', 'bottom']:\n    ax4.spines[loc].set_visible(False)\nax4.axes.get_xaxis().set_visible(False)\n\n\n\n\n### titles and descriptions\n\nfig.text(0,0.98, 'Diamonds and Dollars: Does Cut of the Diamond have Influence on Price ?',{'font':'serif', 'size':20.,'weight':'bold'}, alpha = 1)\n\nfig.text(0,0.92,'''It can be seen that, even cheap price diamond is having an ideal cut. \nMay be because it might increase the value of the diaomond, and from scatter plot \nit is clear that all expensive diamonds are in either category cut of ideal or very good.\nthan regular diamond''',{'font':'serif', 'size':12,'weight':'normal'}, alpha = 0.9)\n\n\nfig.text(0.05, 0.88, 'Diamond Cut and Price Matrix:',{'font':'serif', 'size':16,'weight':'bold'}, alpha = 1)\n\nfig.text(0.55, 0.88, 'Diamond Price (USD) vs Cut:',{'font':'serif', 'size':16,'weight':'bold'}, alpha = 1)\n\nfig.text(0.552, 0.85, 'Ideal Cut is the best of Quality, \\nand Fair Cut inferior Quality Cut. ',{'font':'serif', 'size':12,'weight':'normal'}, alpha = 0.9)\n\nfig.text(0.75,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10, 'weight':'bold'},alpha = 0.75)\nfig.show()","4133415c":"tem = df.copy()\ntem['price_cat'] = pd.cut(df['price'], bins = [0,1500,4000,10000,100000], labels = ['Cheaper','Affordable', 'Expensive', 'Very Expensive'])\ntem['price_cat'] = tem['price_cat'].astype('object')\n\n\ntab = pd.crosstab(tem['color'],tem['price_cat'])\ntab_val = pd.melt(tab.reset_index(),id_vars = 'color')\ntab_val.columns = ['x','y','value']\n\ncolor_labels = [ 'J', 'I', 'H', 'G', 'F', 'E', 'D']\ncolor_to_num = { val : idx for idx,val in enumerate(color_labels)}\n\ntem['color'] = tem['color'].map(color_to_num)\n\n\nfig = plt.figure(figsize = (12,12), dpi = 65)\nfig.patch.set_facecolor('#f5f6f6')\ngs = fig.add_gridspec(20,20)\ngs.update(wspace = 0.6, hspace = 0)\n\n\nax0 = fig.add_subplot(gs[0:,0:])\nax0.set_facecolor('#f5f6f6')\nax0.axes.get_xaxis().set_visible(False)\nax0.axes.get_yaxis().set_visible(False)\nfor loc in ['left', 'right', 'top', 'bottom']:\n        ax0.spines[loc].set_visible(False)\n\n\n#ax1 = fig.add_subplot(gs[0:4,0:10])\n\nax2 = fig.add_subplot(gs[2:18,13:20])\n\nax3 = fig.add_subplot(gs[2:13,0:11])\n\nax4 = fig.add_subplot(gs[14:19,0:10])\n\naxes = [ax1,ax2,ax3,ax4]\n\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')  \n    ax.tick_params(axis='both',\n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.001)\n    \n    for loc in ['left', 'right', 'top', 'bottom']:\n        ax.spines[loc].set_linewidth(1)\n\n\n    \n    \n############################################### scatter plot and regplot\nsns.regplot(x = 'color', y = 'price',data = tem, ax = ax2, color = colors[1])\nsns.stripplot(x = 'color', y = 'price',data = tem, ax = ax2,  color = colors[2], size = 2, alpha = 0.5)\nax2.set_yticklabels(np.arange(0,18000,2000),**{'font':'serif','size':12, 'weight':'bold'},alpha = 1)\nax2.set_xticklabels('')\nax2.set_ylabel('')\nax2.set_xlabel('')\nfor loc in ['left', 'right', 'top', 'bottom']:\n    ax2.spines[loc].set_visible(False)\n\n    \nbin_labels = ['J', 'I', 'H', 'G', 'F', 'E', 'D']\nsize_bins = [[-0.5,0.5],[0.5, 1.5], [1.5,  2.5], [2.5, 3.5], [3.5, 4.5],[4.5,5.5],[5.5,6.5]]\n\ncolor = ['#FB5B68','#FFEB48','#FFBDB0','#2676A1','dimgrey', 'grey','black']\nfor idx, label in enumerate(bin_labels):\n        ax2.annotate(label,\n                    xy=(sum(size_bins[idx])\/2 ,16500),\n                    xytext=(0,0), textcoords='offset points',\n                    va=\"center\", ha=\"center\",rotation = 0,\n                    **{'font':'serif','size':12, 'weight':'bold','color':'white'},\n                    bbox=dict(boxstyle='round4', pad=0.2, color=color[idx], alpha=1))\n        ## adding span over region\n        ax2.axvspan(size_bins[idx][0],size_bins[idx][1], ymax = 1, color=color[idx], alpha=0.2)\n\nax2.set_ylim(0,17000)\n\n\n####################################################### scattermap\n\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], marker = 'o',size = 0.32, ax = ax3, color = colors[2])\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], marker = 'o',size = 0.2, ax = ax3, color = colors[1])\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], size = 0.1,marker = 'h', ax = ax3, color = colors[0])\nax3.set_facecolor('#f5f6f6')\n\n\n##################################################### final ratio plot\n\ncolor = round(df.color.value_counts(normalize = True) * 100,0).astype(int)\n\nax4.barh(y = color.index, width = color.values, height = 0.4, color = colors[2])\n\n\n\nfor pa in ax4.patches: \n        ax4.text((pa.get_width()), pa.get_y(),'{} %'.format(pa.get_width()), **{'font':'serif', 'size':12, 'weight':'bold'}, alpha = 1)\n\nax4.set_yticklabels(labels = color.index,**{'font':'serif', 'size':12,'weight':'bold'}, alpha = 1)\n\n\nax4.barh(y = color.index, width = color.values -00.1, height = 0.25, color = colors[1])\nax4.barh(y = color.index, width = color.values -00.1, height = 0.12, color = colors[0])\nfor loc in ['left', 'right', 'top', 'bottom']:\n    ax4.spines[loc].set_visible(False)\nax4.axes.get_xaxis().set_visible(False)\n\n\n\n\n### titles and descriptions\n\nfig.text(0,0.98, 'Diamonds and Dollars: Does Color of the Diamond have Influence on Price?',{'font':'serif', 'size':20.,'weight':'bold'}, alpha = 1)\n\n\nfig.text(0,0.93,'''Color parameter is a important parameter to determine the price as per diamond traders. \nBut its not reflecting the same with data, for an expensive diamond it is expected to be color less that is D category,\ndata speaks otherwise. Hmm, may be there is not much impact of color on price of the diamond.''',{'font':'serif', 'size':11}, alpha = 0.8)\n\n\nfig.text(0.05, 0.88, 'Diamond Color and Price Matrix:',{'font':'serif', 'size':16,'weight':'bold'}, alpha = 1)\nfig.text(0.55, 0.88, 'Diamond Price (USD) vs Color:',{'font':'serif', 'size':16,'weight':'bold'}, alpha = 1)\n \nfig.text(0.552, 0.85, 'D grade is colorless and very desirable,\\nand J grade is yellowish colored diamond.',{'font':'serif', 'size':12}, alpha = 0.9)   \n\nfig.text(0.75,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10, 'weight':'bold'},alpha = 0.75)\nfig.show()","989df3b6":"tem = df.copy()\ntem['price_cat'] = pd.cut(df['price'], bins = [0,1500,4000,10000,100000], labels = ['Cheaper','Affordable', 'Expensive', 'Very Expensive'])\ntem['price_cat'] = tem['price_cat'].astype('object')\n\n\ntab = pd.crosstab(tem['clarity'],tem['price_cat'])\ntab_val = pd.melt(tab.reset_index(),id_vars = 'clarity')\ntab_val.columns = ['x','y','value']\n\n\n\nclarity_labels = [ 'I1' , 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF' ]\nclarity_to_num = { val : idx for idx,val in enumerate(clarity_labels)}\n\ntem['clarity'] = tem['clarity'].map(clarity_to_num)\n\n\nfig = plt.figure(figsize = (12,12), dpi = 65)\nfig.patch.set_facecolor('#f5f6f6')\ngs = fig.add_gridspec(20,20)\ngs.update(wspace = 0.6, hspace = 0)\n\n\nax0 = fig.add_subplot(gs[0:,0:])\nax0.set_facecolor('#f5f6f6')\nax0.axes.get_xaxis().set_visible(False)\nax0.axes.get_yaxis().set_visible(False)\nfor loc in ['left', 'right', 'top', 'bottom']:\n        ax0.spines[loc].set_visible(False)\n\n#ax1 = fig.add_subplot(gs[0:4,0:10])\n\nax2 = fig.add_subplot(gs[2:18,13:20])\n\nax3 = fig.add_subplot(gs[2:13,0:11])\n\nax4 = fig.add_subplot(gs[14:19,0:10])\n\naxes = [ax1,ax2,ax3,ax4]\n\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')  \n    ax.tick_params(axis='both', \n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.001)\n    \n    for loc in ['left', 'right', 'top', 'bottom']:\n        ax.spines[loc].set_linewidth(1)\n\n\n    \n    \n############################################### scatter plot and regplot\nsns.regplot(x = 'clarity', y = 'price',data = tem, ax = ax2, color = colors[1])\nsns.stripplot(x = 'clarity', y = 'price',data = tem, ax = ax2, alpha = 0.5, color = colors[2],size =2)\nax2.set_yticklabels(np.arange(0,18000,2000),**{'font':'serif','size':12, 'weight':'bold'},alpha = 1)\nax2.set_xticklabels('')\nax2.set_ylabel('')\nax2.set_xlabel('')\nfor loc in ['left', 'right', 'top', 'bottom']:\n    ax2.spines[loc].set_visible(False)\n\n    \nbin_labels = [ 'I1' , 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF' ]\nsize_bins = [[-0.5,0.5],[0.5, 1.5], [1.5,  2.5], [2.5, 3.5], [3.5, 4.5],[4.5,5.5],[5.5,6.5],[6.5,7.5]]\n\ncolor = ['#FB5B68','#FFEB48','#FFBDB0','#2676A1','dimgrey', 'grey','gold','black']\nfor idx, label in enumerate(bin_labels):\n        ax2.annotate(label,\n                    xy=(sum(size_bins[idx])\/2 ,16500),\n                    xytext=(0,0), textcoords='offset points',\n                    va=\"center\", ha=\"center\",rotation = 90,\n                    **{'font':'serif','size':12, 'weight':'bold','color':'white'},\n                    bbox=dict(boxstyle='round4', pad=0.2, color=color[idx], alpha=1))\n        ## adding span over region\n        ax2.axvspan(size_bins[idx][0],size_bins[idx][1], ymax = 1, color=color[idx], alpha=0.2)\n\nax2.set_ylim(0,17000)\n\n\n####################################################### scattermap\n\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], marker = 'D',size = 0.3, ax = ax3, color = colors[2])\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], marker = 'D',size = 0.2, ax = ax3, color = colors[1])\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], size = 0.1,marker = 'o', ax = ax3, color = colors[0])\nax3.set_facecolor('#f5f6f6')\n\n\n##################################################### final ratio plot\n\nclarity = round(df.clarity.value_counts(normalize = True) * 100,0).astype(int)\n\nax4.barh(y = clarity.index, width = clarity.values, height = 0.35, color = colors[2])\n\n\n\nfor pa in ax4.patches: \n        ax4.text((pa.get_width()), pa.get_y(),'{} %'.format(pa.get_width()), **{'font':'serif', 'size':12, 'weight':'bold'}, alpha = 1)\n\nax4.set_yticklabels(labels = clarity.index,**{'font':'serif', 'size':12,'weight':'bold'}, alpha = 1)\n\n\nax4.barh(y = clarity.index, width = clarity.values, height = 0.25, color = colors[1])\nax4.barh(y = clarity.index, width = clarity.values -00.1, height = 0.15, color = colors[0])\nfor loc in ['left', 'right', 'top', 'bottom']:\n    ax4.spines[loc].set_visible(False)\nax4.axes.get_xaxis().set_visible(False)\n\n\n\n\n### titles and descriptions\n\nfig.text(0,0.98, 'Diamonds and Dollars: How much Does Crystal Clear Diamonds Costs?',{'font':'serif', 'size':20.,'weight':'bold'}, alpha = 1)\n\n\nfig.text(0,0.93,'''Clarity is defined as no internal flaws in the dimonds, This includes intrusions, scatches, etc.\nData says that finding IF, VVS1, VVS2 is less likely for expensive dimonds as well. \nMost of the diamods are in the median range for categoris of clarity.''',{'font':'serif', 'size':11}, alpha = 0.9)\n\n\nfig.text(0.05, 0.88, 'Diamond Clarity and Price Matrix:',{'font':'serif', 'size':16,'weight':'bold'}, alpha = 1)\n\nfig.text(0.55, 0.88, 'Diamond Price (USD) vs Clarity:',{'font':'serif', 'size':16,'weight':'bold'}, alpha = 1)\n\nfig.text(0.552, 0.85, 'IF is internally flawless diamonds and very desirable,\\nand I1 is flawed diamond and least desirable.  :',{'font':'serif', 'size':12}, alpha = 0.8)   \n\nfig.text(0.75,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10, 'weight':'bold'},alpha = 0.75)\nfig.show()","ff82e339":"\ntem = df.copy()\ntem['price_cat'] = pd.cut(df['price'], bins = [0,1500,4000,10000,100000], labels = ['Cheaper','Affordable', 'Expensive', 'Very Expensive'])\ntem['price_cat'] = tem['price_cat'].astype('object')\n\n\ntab = pd.crosstab(tem['diamond_size'],tem['price_cat'])\ntab_val = pd.melt(tab.reset_index(),id_vars = 'diamond_size')\ntab_val.columns = ['x','y','value']\n\n\n\n\nshape_labels = [ 'Regular','Fancy']\nshape_to_num = { val : idx for idx,val in enumerate(clarity_labels)}\n\ntem['shape'] = tem['shape'].map(shape_to_num)\n\nfig = plt.figure(figsize = (12,12), dpi = 65)\nfig.patch.set_facecolor('#f5f6f6')\ngs = fig.add_gridspec(25,20)\ngs.update(wspace = 0.2, hspace = 0.2)\n\nax0 = fig.add_subplot(gs[0:,0:])\nax0.set_facecolor('#f5f6f6')\nax0.axes.get_xaxis().set_visible(False)\nax0.axes.get_yaxis().set_visible(False)\nfor loc in ['left', 'right', 'top', 'bottom']:\n        ax0.spines[loc].set_visible(False)\n\nax1 = fig.add_subplot(gs[2:7,12:])\n\nax2 = fig.add_subplot(gs[10:15,12:])\n\nax3 = fig.add_subplot(gs[18:24,12:])\n\n\nax4 = fig.add_subplot(gs[16:22,0:11])\n\nax5 = fig.add_subplot(gs[1:13,0:10])\n\n\naxes = [ax0,ax1, ax2, ax3, ax4]\n\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')  \n    ax.tick_params(axis='both', \n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.02)\n    \n    for loc in ['left', 'right', 'top', 'bottom']:\n        ax.spines[loc].set_visible(False)\n        \n\naxes = [ax1, ax2, ax3,ax4]\n\ncols = ['x','y','z','Volume']\n\nfor ax,col in zip(axes,cols):\n    \n    sns.regplot(y = tem[col], x = 'price',data = tem, ax = ax, color = colors[1])\n    sns.scatterplot(y = tem[col], x = 'price',data = tem, ax = ax, alpha = 0.6,color = colors[2],size = 2,**{'linewidth':0.2})\n    ax.set_xticklabels(np.arange(0,18000,2000),**{'font':'serif','size':12, 'weight':'bold'},alpha = 1)\n    #ax.set_yticklabels('')\n    ax.set_ylabel('')\n    ax.set_xlabel('')\n    \n    \n    bin_labels = ['Cheaper','Affordable', 'Expensive', 'Very Expensive']\n    [0,1500,4000,10000,100000]\n    size_bins = [[0,1500],[1500, 4000], [4000, 10000], [10000,tem.price.max()+500]]\n\n    color = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0']\n    for idx, label in enumerate(bin_labels):\n        ax.annotate(label,\n                    xy=(sum(size_bins[idx])\/2 ,tem[col].max()),\n                    xytext=(0,0), textcoords='offset points',\n                    va=\"center\", ha=\"center\",rotation = 0,\n                    **{'font':'serif','size':6, 'weight':'bold','color':'black'},\n                    bbox=dict(boxstyle='round4', pad=0., color=color[idx], alpha= 1))\n        ## adding span over region\n        ax.axvspan(size_bins[idx][0],size_bins[idx][1], ymax = 1, color=color[idx], alpha=0.2)\n\n        #ax.set_ylim(0,17000)\n    \n\n####################################################### scattermap\n\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], marker = 'o',size = 0.3, ax = ax5, color = colors[2])\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], marker = 'o',size = 0.2, ax = ax5, color = colors[1])\ncustom_scatter(x = tab_val['x'], y = tab_val['y'], value = tab_val['value'], size = 0.1,marker = 'o', ax = ax5, color = colors[0])\nax5.set_facecolor('#f5f6f6')\n\n\nax5.tick_params(axis='both', \n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.02)\n\n    \nfor loc in ['left', 'right', 'top', 'bottom']:\n    ax5.spines[loc].set_linewidth(1)\n\n\n\n### titles and descriptions\n\nfig.text(0,0.98, 'Diamonds and Dollars: Correlation of Diamond dimentions over Price?',{'font':'serif', 'size':20.,'weight':'bold'}, alpha = 1)\n\n\nfig.text(0,0.93,'''There is a strong impact on price due to dimond dimentions.Data says that with increase in one\nquantity drives price of that diamond to good extent. So, it seems there could be \na highest correlation between these features to price.''',{'font':'serif', 'size':11}, alpha = 0.9)\n\n\nfig.text(0.05, 0.88, 'Price of The Diamond with Shape:',{'font':'serif', 'size':16,'weight':'bold'}, alpha = 1)\n\nfig.text(0.55, 0.88, 'Price Vs Measurements of Diamond',{'font':'serif', 'size':16,'weight':'bold'}, alpha = 1)\n\nax1.text(0,10.5,'Length of The Diamond',{'font':'serif', 'size':14,'weight':'bold'}, alpha = 1)\nax2.text(0,10.5,'Breath of The Diamond',{'font':'serif', 'size':14,'weight':'bold'}, alpha = 1)\nax3.text(0,6.,'Height  of The Diamond',{'font':'serif', 'size':14,'weight':'bold'}, alpha = 1)\nax4.text(0,550,'Volume of The Diamond',{'font':'serif', 'size':14,'weight':'bold'}, alpha = 1)\n\n\nfig.text(0.725,0.065,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10, 'weight':'bold'},alpha = 0.75)\nfig.show()\n\n\n","3cbeb7c6":"### two variable comparisions \n\ntem = featdf.copy()\n\n\ntem['Price Category'] = pd.cut(df['price'], bins = [0,1500,4000,10000,100000], labels = ['Cheaper','Affordable', 'Expensive', 'Very Expensive'])\ntem['Price Category'] = tem['Price Category'].astype('object')\n\ncols_here = ['#FFBDB0', '#FFEB48', '#2676A1', '#FB5B68']\norder = ['Cheaper','Affordable', 'Expensive', 'Very Expensive']\n\nfig = plt.gcf();\nfig.set_size_inches(10,10)\n\nfig.set_dpi(50)\n\n\ng = sns.pairplot(data = tem, vars = ['x','y','z','Volume','price'],\n                 hue= 'Price Category', hue_order = order,\n             corner = True, diag_kind='kde', palette = cols_here, \n                plot_kws = {'alpha':0.90, 'size' : tem['price']*2, 'linewidth' : 0.2})\ng._legend.remove()\n\nfor ax in plt.gcf().axes:\n    ax.set_facecolor('#f5f6f6')\n    for loc in ['left','right','top','bottom']:\n        ax.spines[loc].set_visible(False)\n    ax.set_xticks(ticks = [])\n    ax.set_yticks(ticks = [])\n    \n    ax.set_xlabel(xlabel = ax.get_xlabel(), **{'font':'serif', 'size':17,'weight':'bold'}, alpha = 0.9)\n    ax.set_ylabel(ylabel = ax.get_ylabel(), **{'font':'serif', 'size':17,'weight':'bold'}, rotation  = 0,alpha = 0.9)\n\nplt.gcf().patch.set_facecolor('#f5f6f6')\n\n### titles and descriptions\n\nplt.gcf().text(0.,1.0425, 'Diamonds and Dollars: Multivariate Analysis of Highly Positive Correlated Features',{'font':'serif', 'size':20.,'weight':'bold'}, alpha = 1)\n\n\nplt.gcf().text(0,0.99,'''This visualization enables to see the intrafeature correations with respect to price.\nx,y,z,volume have a strong influence over price, and so does the multiple features.\nPrice can clearly clustered for all these features.''',{'font':'serif', 'size':12}, alpha = 0.8)\n\n\n## legend\n\n\n\nplt.gcf().text(0.55 - 0.05 , 0.9 + 0.025, 'Price Tag of The Diamond', {'font':'serif', 'size':18,'weight':'bold', 'color':'black'}) \ni = 0 ## xcontrol\nfor word,color in zip(order,cols_here):\n    j = 0 ##ycontrol\n    \n    for char in list(word):\n        \n        plt.gcf().text(0.55 - 0.025 +i, 0.9  - j , '|', {'font':'serif', 'size':18,'weight':'bold', 'color':'black'}) \n        plt.gcf().text(0.55 + i, 0.9 - j , char , {'font':'serif', 'size':18,'weight':'bold', 'color':color} )\n        plt.gcf().text(0.55 +0.025 +i, 0.9  - j , '|', {'font':'serif', 'size':18,'weight':'bold', 'color':'black'})\n        j += 0.02\n    i += 0.05\n\nplt.gcf().text(0.7,0.,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10, 'weight':'bold'},alpha = 0.75)\n\nplt.show()","dd189f6e":"### two variable comparisions \n\ntem = featdf.copy()\n\n\ntem['Price Category'] = pd.cut(df['price'], bins = [0,1500,4000,10000,100000], labels = ['Cheaper','Affordable', 'Expensive', 'Very Expensive'])\ntem['Price Category'] = tem['Price Category'].astype('object')\n\ncols_here = ['#FFBDB0', '#FFEB48', '#2676A1', '#FB5B68']\norder = ['Cheaper','Affordable', 'Expensive', 'Very Expensive']\n\nfig = plt.gcf();\nfig.set_size_inches(12,12)\n\nfig.set_dpi(70)\n\n\ng = sns.pairplot(data = tem, vars = ['depth','table','carat','price'],\n                 hue= 'Price Category', hue_order = order,\n             corner = True, diag_kind='kde', palette = cols_here, \n                plot_kws = {'alpha':0.90, 'size' : tem['price']*2, 'linewidth' : 0.2})\ng._legend.remove()\n\nfor ax in plt.gcf().axes:\n    ax.set_facecolor('#f5f6f6')\n    for loc in ['left','right','top','bottom']:\n        ax.spines[loc].set_visible(False)\n    ax.set_xticks(ticks = [])\n    ax.set_yticks(ticks = [])\n    \n    ax.set_xlabel(xlabel = ax.get_xlabel(), **{'font':'serif', 'size':17,'weight':'bold'}, alpha = 0.9)\n    ax.set_ylabel(ylabel = ax.get_ylabel(), **{'font':'serif', 'size':17,'weight':'bold'}, rotation  = 0,alpha = 0.9)\n\nplt.gcf().patch.set_facecolor('#f5f6f6')\n\n### titles and descriptions\n\nplt.gcf().text(0.,1.05, 'Diamonds and Dollars: Multivariate analysis of Weight, Ratio Features',{'font':'serif', 'size':20.,'weight':'bold'}, alpha = 1)\n\n\nplt.gcf().text(0,0.99,'''This visualization enables to see dependecy of weight and proportional correations \nwith respect to price.carat have a strong influence over price, \nwhere as other features cannot sepereate price fearure.''',{'font':'serif', 'size':12}, alpha = 0.8)\n\n\n## legend\n\n\n\nplt.gcf().text(0.55 - 0.05 , 0.92 + 0.025, 'Price Tag of The Diamond', {'font':'serif', 'size':18,'weight':'bold', 'color':'black'}) \ni = 0 ## xcontrol\nfor word,color in zip(order,cols_here):\n    j = 0 ##ycontrol\n    \n    for char in list(word):\n        \n        plt.gcf().text(0.55 - 0.025 +i, 0.92  - j , '|', {'font':'serif', 'size':18,'weight':'bold', 'color':'black'}) \n        plt.gcf().text(0.55 + i, 0.92 - j , char , {'font':'serif', 'size':18,'weight':'bold', 'color':color} )\n        plt.gcf().text(0.55 +0.025 +i, 0.92  - j , '|', {'font':'serif', 'size':18,'weight':'bold', 'color':'black'})\n        j += 0.02\n    i += 0.05\n\nplt.gcf().text(0.7,0.,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10, 'weight':'bold'},alpha = 0.75)\n\nplt.show()","0056a742":"### two variable comparisions \n\ntem = featdf.copy()\n\n\ntem['price_cat'] = pd.cut(df['price'], bins = [0,1500,4000,10000,100000], labels = ['Cheaper','Affordable', 'Expensive', 'Very Expensive'])\ntem['price_cat'] = tem['price_cat'].astype('object')\n\ntem['price_cat'] = tem['price_cat'].map({'Cheaper':0,'Affordable':1, 'Expensive':2, 'Very Expensive':3})\n\n\nfrom umap import UMAP\n# Umap -  Uniform Manifold Approximation and Projection\n\n\nX_temp = tem.drop(columns = ['price','price_cat'])\ny_temp = tem['price_cat']\n\numap = UMAP(random_state=2021)\nstroke_umap = umap.fit_transform(X_temp, y_temp)\n\n\n\nfig,ax0 = plt.subplots(figsize=(12,12),dpi = 60)\n\n\n# Change background color\nbackground_color = \"#f5f6f6\"\nfig.patch.set_facecolor(background_color) \nax0.set_facecolor(background_color)\n\ncols_here = ['#FFBDB0', '#FFEB48', '#2676A1', '#FB5B68']\n\nax0.scatter(stroke_umap[tem['price_cat'] == 0][:,0], stroke_umap[tem['price_cat'] == 0][:,1], c= cols_here[0], alpha=0.8,s=25)\nax0.scatter(stroke_umap[tem['price_cat'] == 1][:,0], stroke_umap[tem['price_cat'] == 1][:,1], c= cols_here[1], alpha=0.8,s=50)\nax0.scatter(stroke_umap[tem['price_cat'] == 2][:,0], stroke_umap[tem['price_cat'] == 2][:,1], c= cols_here[2], alpha=0.9,s=75)\nax0.scatter(stroke_umap[tem['price_cat'] == 3][:,0], stroke_umap[tem['price_cat'] == 3][:,1], c= cols_here[3], alpha=1,s=100)\n\n\nfor s in [\"top\",\"right\",\"left\",\"bottom\"]:\n    ax0.spines[s].set_visible(False)\n    \nax0.set_xticks([])\nax0.set_yticks([])\n\n\n\n### titles and descriptions\n\nplt.gcf().text(0.1,1.05, 'Diamonds and Dollars: Diamond Price Clustering ',{'font':'serif', 'size':20.,'weight':'bold'}, alpha = 1)\n\n\nplt.gcf().text(0.1,0.98,'''This is just an attempt to visualize the culstering of price \nof the diamond as a classification problem. Happy to see there are clear , \ndistinguished clusters. Just for sake of visualizaiton converted \ncontinous target to categorical target without data leakage.''',{'font':'serif', 'size':12}, alpha = 0.8)\n\n\n## legend\n\n\nplt.gcf().text(0.66 - 0.05 , 0.95 + 0.025, 'Price Tag of The Diamond', {'font':'serif', 'size':14,'weight':'bold', 'color':'black'}) \ni = 0 ## xcontrol\nfor word,color in zip(order,cols_here):\n    j = 0 ##ycontrol\n    \n    for char in list(word):\n        \n        plt.gcf().text(0.66 - 0.025 +i, 0.95  - j , '|', {'font':'serif', 'size':16,'weight':'bold', 'color':'black'}) \n        plt.gcf().text(0.66 + i, 0.95 - j , char , {'font':'serif', 'size':14,'weight':'bold', 'color':color} )\n        plt.gcf().text(0.66 +0.025 +i, 0.95  - j , '|', {'font':'serif', 'size':16,'weight':'bold', 'color':'black'})\n        j += 0.02\n    i += 0.05\n\nplt.gcf().text(0.7,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8, 'weight':'bold'},alpha = 0.75)\n\nplt.show()\n\n\n\n\nplt.show()","edc72b90":"print('final preprocessing of variables......')\n#log transform to all the variables to make less skewed\nfeatdf[\"price\"] = np.log1p(featdf[\"price\"])\n\n#log transform skewed numeric features:\nnumeric_feats = featdf.dtypes[featdf.dtypes != \"object\"].index\n\nskewed_feats = featdf[numeric_feats].dropna().skew() #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nfeatdf[skewed_feats] = np.log1p(featdf[skewed_feats])\n\n# split data into X and y\nX = featdf.drop(columns = ['price'])\ny = featdf['price']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 2021)\n\n#X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size = 0.1, random_state = 2021)\n\nprint('All set! Good to go!')","eeb864d0":"def feature_importance(X_val,y_val, importance = None):\n    if importance == None:\n        importance = 'weight'\n    else:\n        importance = importance\n        \n    model = XGBRegressor(random_state = 2021)\n    model.fit(X_val,y_val)\n    imp = model.get_booster().get_score(importance_type= importance)\n    imp_results = (pd.DataFrame({'Features':imp.keys(), 'Importance':imp.values()})\n            .sort_values(by = 'Importance'))\n    return imp_results\n\ndef plot_imp(data, ax = None):\n    \n    \n    if ax == None:\n        fig,ax = plt.subplots(figsize = (10,6), dpi = 100)\n        fig.patch.set_facecolor(colors[2])\n        ax.set_facecolor(colors[2])\n        \n        ax.tick_params(axis='both', \n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.0001)\n    \n    data['shades'] = data['Features'].apply(lambda x: '#FB5B68' if x == 'Random' else colors[1])\n    \n    ax.barh(y = data.Features, width = data.Importance,\n            height = 0.5, color = data['shades'] )\n    \n    for loc in ['left','right','bottom','top']:\n        ax.spines[loc].set_visible(False)\n    ax.axes.get_xaxis().set_visible(False)\n    #ax.axes.get_yaxis().set_visible(False)\n    \n    ax.set_yticklabels(data.Features, {'font':'serif','size':12,'weight':'bold','color':'white'})\n    \n    \n    ","c0f312e4":"plot_imp(feature_importance(X_train,y_train))\nplt.gcf().text(-0.02,1,'Diamonds and Dollors: Actual Feature Importance',{'font': 'serif', 'size':18,'weight':'bold','color':'white'})\nplt.gcf().text(-0.02, 0.91,'''It makes sence, carat and volume does have a best correlation, x,z does not have much of \nimportance though. Diamond is a costly thing and good investment,so, \nare these features really important? or act of randomness''',{'font': 'serif', 'size':10,'weight':'normal','color':'white'}, alpha = 1)\nplt.gcf().text(0.7,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8, 'weight':'bold','color':'white'},alpha = 0.8)\nplt.show()","d4290e84":"X_train_rand = X_train.copy()\nX_train_rand['Random'] = np.random.random(size=len(X_train_rand))\nplot_imp(feature_importance(X_train_rand,y_train))\nplt.gcf().text(-0.02,1,'Diamonds and Dollors: Random Feature Test',{'font': 'serif', 'size':18,'weight':'bold','color':'white'})\nplt.gcf().text(-0.02, 0.91,'''In this experiment a random feature is added to features, and fitted with same model and prameters\nas actual feature importance, now things got sketchy!! random feature out perfoms other dominating features?\nThis nullify the credibility of feature importance here. What now? Solution could be Permution importance.''',{'font': 'serif', 'size':10,'weight':'normal','color':'white'}, alpha = 1)\nplt.gcf().text(0.7,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8, 'weight':'bold','color':'white'},alpha = 0.8)\nplt.show()","b2d9b7cd":"### permutation importance do not retrain over all the features.\n\n\ndef Permutation_importances(feat,targ):\n    \n    ## spliting data into validation set or hold out for generality of the importances\n    X_train,X_val,y_train,y_val = train_test_split(feat,targ, test_size = 0.2, random_state = 2021)\n    \n    \n    model = XGBRegressor()\n    model.fit(X_train,y_train)\n    ypreds = model.predict(X_val)\n    baseline_r2 = r2_score(y_val,ypreds)\n    \n    imp = []\n    \n    for col in X_val.columns:\n        save = X_val[col].copy()\n        X_val[col] = np.random.permutation(X_val[col])   ## Making a random feature by fixing all other feature undisturbed\n        rand_ypreds = model.predict(X_val)\n        rand_r2    = r2_score(y_val,rand_ypreds)\n        \n        X_val[col] = save\n        \n        imp.append(baseline_r2 - rand_r2)\n    results =  (pd.DataFrame({'Features':X_val.columns, 'Importance':imp})\n                .sort_values(by = 'Importance'))\n    #results['Featrue_ranking'] = [(len(X_val.columns) - idx) for idx,val in enumerate(results['Importance'])]\n    return results","4292bf7b":"per_imp = Permutation_importances(X_train,y_train)","13c90504":"plot_imp(per_imp)\nplt.gcf().text(-0.02,1,'Diamonds and Dollors: A Simple Permutation Feature Importance Experiment',{'font': 'serif', 'size':18,'weight':'bold','color':'white'})\nplt.gcf().text(-0.02, 0.91,'''From the random feature test, it is clear that default feature selection techninque utterly fails to randomly introduced Feature.\nTo overcome this issue, a simple experiment results are demonsted. Idea is simple, for a trained model if each feature column shuffled \nwithout touching other features, then how much does that effect our scoring parameter? Each shuffled feature is a Random Feature now.'''\n               ,{'font': 'serif', 'size':11,'weight':'normal','color':'white'}, alpha = 1)\nplt.gcf().text(0.85,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8, 'weight':'bold','color':'white'},alpha = 0.8)\nplt.show()","6639cb08":"np.random.seed(2021)\nX_train,X_val,y_train,y_val = train_test_split(X_train,y_train, test_size = 0.2, random_state = 2021)\n\nmodel = XGBRegressor().fit(X_train,y_train)\n\n\nperm = PermutationImportance(model,scoring='r2' ).fit(X_val,y_val)\neli5_feature_importance = (pd.DataFrame({'Features':X_train.columns.tolist(),'Importance':perm.feature_importances_})\n                           .sort_values(by = 'Importance'))\n\nprint('Feature impact weights and importance for the given model:')\neli5.show_weights(perm, feature_names = X_val.columns.tolist())","9763f8ae":"fig, ax = plt.subplots(1,2, figsize = (7,10), dpi = 70)\nfig.patch.set_facecolor(colors[2])\n\naxes = ax.ravel()\n\nfor ax in axes:\n    ax.set_facecolor(colors[2])\n    ax.tick_params(axis='both', \n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.0001)\n\nplot_imp(eli5_feature_importance, ax = axes[0])\nplot_imp(per_imp, ax = axes[1])\nplt.gcf().text(-0.35,0.95,'Diamonds and Dollors: Naive Implementation VS Eli5 Permutation Importance',{'font': 'serif', 'size':18,'weight':'bold','color':'white'})\nplt.gcf().text(-0.35, 0.91,'''With a simple algoritham, we got out permuated feature importances from our naive implementation \nBut Question is are the accurate? To see that we implement Permutation Importance from well know Eli5.'''\n               ,{'font': 'serif', 'size':12,'weight':'normal','color':'white'}, alpha = 1)\nplt.gcf().text(0.25, 0.88,'Naive',{'font': 'serif', 'size':15,'weight':'bold','color':'white'}, alpha = 1)\nplt.gcf().text(0.7, 0.88,'Eli5',{'font': 'serif', 'size':15,'weight':'bold','color':'white'}, alpha = 1)\nplt.gcf().text(0.86,0.15,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8, 'weight':'bold','color':'white'},alpha = 0.8)\nplt.show()","f9e29abb":"## feature selection based on permutation importance\nfeature_import = (eli5_feature_importance.sort_values(by = 'Importance', ascending = False)\n                  .reset_index(drop = True).Features.tolist()[0:6]) ## selecting top 6 features with help of permutation importance\n\n\n# split data into X and y\nX = featdf.copy().drop(columns = ['price'])\ny = featdf['price'].copy()\n\nX_train,X_test,y_train,y_test = train_test_split(X[feature_import],y,test_size = 0.2,random_state = 2021)\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)\n","885a0168":"## appending all the regressors\n\nregressors = []\n\nregressors.append(LinearRegression())\nregressors.append(Lasso(alpha = 0.0005,max_iter = 1000))\nregressors.append(RandomForestRegressor(n_estimators = 200,max_depth=15))\nregressors.append(AdaBoostRegressor())\nregressors.append(GradientBoostingRegressor())\nregressors.append(XGBRegressor(n_estimators = 200,importance_type = 'gain'))\nregressors.append(LGBMRegressor(n_estimators = 200, objective ='regression',\n                                importance_type = 'gain'))","64a4a89f":"import time\n\nprint('Working on base models and fitting begains.....')\n\nkfolds = KFold(n_splits = 5, shuffle = False)\n\nbaseline_models= clone(regressors)\nalgo = ['LinearRegression','Lasso', 'RandomForestRegressor' ,'AdaBoostRegressor',\n                                          'GradientBoostingRegressor', 'XGBRegressor','LGBMRegressor']\n\ncv_results = list()\nfor idx, reg in enumerate(baseline_models):\n    t = time.time()\n    print('Fitting of {} Model'.format(algo[idx]))\n    print('Parameters of the model are: {}'. format(reg.get_params()))\n    \n    cv_results.append(cross_val_score(estimator=reg,X =X_train,y= y_train, cv = kfolds, n_jobs = -1, scoring= 'r2'))\n\n    print('time elapsed is : {} sec'.format(round((time.time() - t),2)))\n    print('\\n\\n\\n*****************************\\n\\n\\n')\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \nbaseline_cv_res = pd.DataFrame({'CrossValMeans':cv_means,\"CrossValerrors\":\n                      cv_std,\"Algorithm\":['LinearRegression','Lasso', 'RandomForestRegressor' ,'AdaBoostRegressor',\n                                          'GradientBoostingRegressor', 'XGBRegressor','LGBMRegressor']})\n    ","22a40bee":"fig,ax = plt.subplots(figsize = (12,5), dpi = 100)\nfig.patch.set_facecolor(colors[2])\nax.set_facecolor(colors[2])\nax.tick_params(axis='both', \n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.001)\n\nax.barh(y = baseline_cv_res['Algorithm'], width = baseline_cv_res['CrossValMeans'] , height = 0.5,color = colors[1])\nax.set_yticklabels(baseline_cv_res['Algorithm'], {'font':'serif','size':16,'weight':'bold', 'color':'white'}, )\n\n#ax.errorbar(x = , b, xerr=c, fmt=\"o\", color=\"r\")\n\nfor pa in ax.patches: \n        ax.text((pa.get_width())+0.01, pa.get_y()+0.05,'{}'.format(round(pa.get_width(),3)), **{'font':'serif', 'size':12, 'weight':'bold','color':'white'}, alpha = 1)\n        \nax.axes.get_xaxis().set_visible(False)\n\nfor loc in ['left', 'right', 'top','bottom']:\n    ax.spines[loc].set_visible(False)\n    \nax.text(-0.4,8,'Diamonds and Dollars: Basemodels Comparision with R2 Score Metric',{'font':'serif','size':24,'weight':'bold', 'color':'white'})\nax.text(-0.4,6.7,''' Result seems good for a base models without any hyperparameter tunning. Boosting models are showing \na great promise, so, for stacked ensemble model, lets use xgbregressor as meta model.\n''',{'font':'serif','size':14,'weight':'normal', 'color':'white'})\n\nplt.gcf().text(0.75,0.05,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8, 'weight':'bold','color':'white'},alpha = 0.9)\nfig.show()","ec2b5fda":"class stackedRegression():\n    \"\"\" X: features as a dataframe \n        Y: target as a dataframe\n         \n         methods are\n         .fit(X,y) -- fitts on regressor\n         .predict(X)  --- final predictions of the model\"\"\"\n    \n    ## initialization of stackedRegression class with required parameters\n    \n    \n    def __init__(self,base_models,meta_model,n_jobs,cv = 5):\n        \n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.cv = cv\n        self.n_jobs = n_jobs\n        \n\n    ## defining a fit method for stackedRegression\n    \n    def fit(self, X,y):\n        \n        ## create an empty list with embedded list to store the model states for all the crossvalidation folds \n        ## and models -------    shaped would be like this [[cv * cv - in lower triangular array] * number of models]\n        \n        self.base_model_states = [[] for model in self.base_models]\n        \n        ## creating a copy of a meta model (last model in the stack, analogous to fully connected layer in cnn)\n        self.meta_model_clone = clone(self.meta_model)\n        \n        # creating folds \n        kfolds = KFold(n_splits = self.cv,shuffle = True, random_state = 2021)\n        \n        #### Creating an array of meta features to strore the predictions from all the models and folds of cross validation\n        meta_features_ = np.zeros((X.shape[0], len(self.base_models)))\n        \n        \n        #looping over the base models to fit on folds and extract fold validation predictions\n        for idx_of_model, base_model in enumerate(self.base_models):\n            \n            for idx, (train_idx,val_idx) in enumerate(kfolds.split(X,y)):\n                \n                fold_X_train, fold_y_train = X.iloc[train_idx], y.iloc[train_idx]\n                fold_X_val, fold_y_val     = X.iloc[val_idx], y.iloc[val_idx]\n                \n                ## cloning base model to keep actual base models unfit and unmodified state\n                cloned_base = clone(base_model)\n                \n                cloned_base.fit(fold_X_train,fold_y_train)\n                \n                self.base_model_states[idx_of_model].append(cloned_base) ## stroing all the fold states to initially created list\n                                                                          ## base model states list embedding\n                \n                \n                fold_predictions = cloned_base.predict(fold_X_val)  ## fold predictions = Meta features\n                \n                \n                ## storing all the fold predictions to meta_features array\n                meta_features_[val_idx,idx_of_model] = fold_predictions\n                \n        ## training meta model with meta_features( nothing but fold validation predictions stacked in columns for all base models)\n        self.meta_model_clone.fit(meta_features_,y)\n        \n        return self ## returning the self to return final fit with inheritance\n    \n    \n\n    def predict(self, X):\n        meta_features = np.column_stack([np.column_stack([model.predict(X) for model in base_models]).mean(axis=1) for base_models in self.base_model_states])\n        return self.meta_model_clone.predict(meta_features)","fd9090a5":"X_train_reframe = pd.DataFrame(X_train, columns = X[feature_import].columns.values)\nX_test_reframe = pd.DataFrame(X_test,columns = X[feature_import].columns.values)\n\nstacked = stackedRegression(base_models = regressors ,meta_model = XGBRegressor(),n_jobs =None, cv = 5)\n\n\nstacked.fit(X = X_train_reframe, y =y_train)\nfinalpreds = stacked.predict(X_test_reframe)\n\nstacked_r2 = r2_score(y_test, finalpreds)\n\nprint('R2 score of the stacked model: {}'.format(stacked_r2))","f1f7e011":"fig,ax = plt.subplots(figsize = (12,8), dpi = 100)\nfig.patch.set_facecolor(colors[2])\nax.set_facecolor(colors[2])\nax.tick_params(axis='both', \n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.001)\n\nylabels = ['LinearRegression', 'Lasso',\n 'RandomForestRegressor', 'AdaBoostRegressor', 'GradientBoostingRegressor', 'XGBRegressor', 'LGBMRegressor','Stacked Regressor']\n\nax.barh(y = baseline_cv_res['Algorithm'], width = baseline_cv_res['CrossValMeans'] *1000, height = 0.5,color = colors[1])\nax.barh(y = 'Stacked Regressor', width = stacked_r2*1000, height = 0.5,color = colors[0])\nax.set_yticklabels(ylabels, {'font':'serif','size':16,'weight':'bold', 'color':'white'}, )\n\n#ax.errorbar(x = , b, xerr=c, fmt=\"o\", color=\"r\")\n\nfor pa in ax.patches: \n        ax.text((pa.get_width())+0.01, pa.get_y()+0.05,'{}'.format(round(pa.get_width(),3)), **{'font':'serif', 'size':12, 'weight':'bold','color':'white'}, alpha = 1)\n        \nax.axes.get_xaxis().set_visible(False)\n\nfor loc in ['left', 'right', 'top','bottom']:\n    ax.spines[loc].set_visible(False)\n    \nfig.text(-0.15,0.99,'Diamonds and Dollars: Stacked Regression and Baseline models with R2 Score Metric',{'font':'serif','size':24,'weight':'bold', 'color':'white'})\nfig.text(-0.15,0.92,''' With implementation of stacking strategy surely gave some improvement in r2 score.\nIts pretty decent score for a model with out parameter tuning.''',{'font':'serif','size':16,'weight':'normal', 'color':'white'})\nplt.gcf().text(0.85,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8, 'weight':'bold','color':'white'},alpha = 0.8)\nfig.show()","c468ae1f":"X_train, X_test, Y_train, Y_test = train_test_split(X[feature_import], y, test_size = 0.2)\n\nmodel = XGBRegressor(max_depth=6, random_state=0, n_estimators=200); ### for xgboost no need of scaling\nmodel.fit(X_train, Y_train);\nshap_values = shap.Explainer(model).shap_values(X_test)","4dad431c":"fig,ax =  plt.subplots(figsize =(10,6), dpi = 80)\nfig.patch.set_facecolor('#f5f6f6')\n\ncmap1 = matplotlib.colors.LinearSegmentedColormap.from_list(\"\",colors = [colors[2],colors[1],colors[0]])\n\nshap.dependence_plot('carat', shap_values, X_test, show = False, ax =ax,cmap = cmap1)\n    \nax.set_facecolor('#f5f6f6')\nax.set_xlabel(xlabel = ax.get_xlabel(), **{'font':'serif', 'color':'black','size':18,'weight':'bold'},alpha = 1)\nax.set_ylabel(ylabel = ax.get_ylabel(), **{'font':'serif', 'color':'black','size':18,'weight':'bold'},alpha = 1)\n#ax.set_yticklabels(ax.get_yticklabels(), **{'font':'serif', 'color':'black','size':16,'weight':'bold'},alpha = 1)\n\n    \nplt.tight_layout()\n\nfig.text(-0.1,1.26, 'Diamonds and Dollars: Carat Feature Global Interpretation',\n              **{'font':'serif', 'size':20, 'weight':'bold', 'color': 'Black'})\nfig.text(-0.1,0.99,'''In this dependence plot, on X-axis actual feature values are given, Y-axis corresponding\nSHAP Values are given, and on colorbar a most interacting feature with respect to carat feature\nis given which is clarity. Fist, from X-y scatter, it is observed that feature, and SHAP have a\nlinear realtion, i.e, increase in feature results in contribution to wards prediction.\nand color map enables to viualize the interplay between three parameters, SHAP value, and \nother two actual feature values. Instance, Higher valued carat with high value calrity \nhave a high SHAP values, which are indirectly expensive diamonds.\n''',**{'font':'serif', 'size':12, 'weight':'normal', 'color': 'Black'}, alpha = 0.95)\n\n\nfig.text(0.7,0.,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10, 'weight':'bold',},alpha = 0.9)\nfig.show()","de75f1f9":"fig,ax =  plt.subplots(3,2, figsize =(15,10))\nfig.patch.set_facecolor('#f5f6f6')\n\naxes = ax.ravel()\ncmap1 = matplotlib.colors.LinearSegmentedColormap.from_list(\"\",colors = [colors[2],colors[1],colors[0]])\nfor ax,col in zip(axes,X_test.columns):\n    shap.dependence_plot(col, shap_values, X_test, show = False, ax =ax,cmap = cmap1)\n    \n    ax.set_facecolor('#f5f6f6')\n    ax.set_xlabel(xlabel = ax.get_xlabel(), **{'font':'serif', 'color':'black','size':18,'weight':'bold'},alpha = 1)\n    ax.set_ylabel(ylabel = ax.get_ylabel(), **{'font':'serif', 'color':'black','size':18,'weight':'bold'},alpha = 1)\n    #ax.set_yticklabels(ax.get_yticklabels(), **{'font':'serif', 'color':'black','size':16,'weight':'bold'},alpha = 1)\n\n    \nplt.tight_layout()\n\nfig.text(-0.,1.13, 'Diamonds and Dollars: Global Interpretation of the Model',\n              **{'font':'serif', 'size':24, 'weight':'bold', 'color': 'Black'})\nfig.text(-0.,0.99,'''These are the dependence plots for all the features with respect to highly interacting unique feature \nfrom the whole other than self, and impacts the target value greatly. From these plots, It is \nclearly observed the relationship(linear, complex or monotonic) of feature and corresponding SHAP values. \nIn addition, strongly interacted feature dependece wrt to given features can be interpreted.\n\n''',**{'font':'serif', 'size':14, 'weight':'normal', 'color': 'Black'}, alpha = 0.95)\n\n\nfig.text(0.8,0.,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10, 'weight':'bold',},alpha = 0.9)\nfig.show()","3fae4087":"shap.summary_plot(shap_values, X_test, plot_type=\"bar\", color= colors[1], axis_color = 'white',show = False)\n\nplt.gcf().set_size_inches(9,6)\nplt.gcf().set_dpi(100)\nplt.gcf().set_facecolor(colors[2])\nplt.gca().set_facecolor(colors[2])\n#plt.gca().set_xlabel(xlabel = plt.gca().get_xlabel(), **{'font':'serif', 'color':'white','size':16,'weight':'bold'},alpha = 1)\nplt.gca().set_yticklabels(plt.gca().get_yticklabels(), **{'font':'serif', 'color':'white','size':16,'weight':'bold'},alpha = 1)\n\nfor pa in plt.gca().patches: \n        plt.gca().text((pa.get_width())+0.001, pa.get_y()+0.25,'{}'.format(round(pa.get_width(),3)), \n                       **{'font':'serif', 'size':12, 'weight':'bold', 'color': 'White'}, alpha = 0.95)\n        \nplt.gca().get_xaxis().set_visible(False)\nplt.gca().spines['bottom'].set_visible(False)\n\nplt.gcf().text(-0.1,1, 'Diamonds and Dollars: Mean-SHAP Values for Avg Impact on Target',\n              **{'font':'serif', 'size':20, 'weight':'bold', 'color': 'White'})\nplt.gcf().text(-0.1,0.87,''' This Plot shows the mean (Modulus of) SHAP values and arranged in order of how much impact\neach feature have over the target values. Value of y dimension have highest impact over the price,it could in \npositive or negative direction. For which need to check Actual Shap values.\n''',**{'font':'serif', 'size':12, 'weight':'normal', 'color': 'White'}, alpha = 0.95)\nplt.gcf().text(0.8,0.05,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8, 'weight':'bold','color':'white'},alpha = 0.9)\nplt.gcf().show()","6269c653":"cmap1 = matplotlib.colors.LinearSegmentedColormap.from_list(\"\",colors = [colors[1],colors[0]])\nshap.summary_plot(shap_values, X_test, show = False,cmap = cmap1,axis_color = 'white')\n\nplt.gca().tick_params(axis = 'both',colors = 'white', labelsize = '16')\n\n\nplt.gcf().set_size_inches(9,6)\nplt.gcf().set_dpi(100)\nplt.gcf().set_facecolor(colors[2])\nplt.gca().set_facecolor(colors[2])\nplt.gca().set_xlabel(xlabel = plt.gca().get_xlabel(), **{'font':'serif', 'color':'white','size':16,'weight':'bold'},alpha = 1)\n#plt.gca().set_xticklabels(plt.gca().get_xticklabels(), **{'font':'serif', 'color':'white','size':16,'weight':'bold'},alpha = 1)\nplt.gca().set_yticklabels(plt.gca().get_yticklabels(), **{'font':'serif', 'color':'white','size':16,'weight':'bold'},alpha = 1)\n\nfor pa in plt.gca().patches: \n        plt.gca().text((pa.get_width())+0.001, pa.get_y()+0.25,'{}'.format(round(pa.get_width(),3)), \n                       **{'font':'serif', 'size':12, 'weight':'bold', 'color': 'White'}, alpha = 0.95)  \n        \n        \n#plt.gca().get_xaxis().set_visible(False)\nplt.gca().spines['bottom'].set_color('white')\n\nplt.gcf().text(-0.1,1.12, 'Diamonds and Dollars: SHAP Values and Its Impact on Target',\n              **{'font':'serif', 'size':20, 'weight':'bold', 'color': 'White'})\nplt.gcf().text(-0.1,0.92,''' Features with Positve SHAP values push towards the target, likewise feature negaitve SHAP values \npull away from the target. This plot shows relation between Feature Values (numerical value) and\nthe direction of prediction value moves. For instance, feature Y - Higher feature values leads \nto higher SHAP values there by increase its contribution towards prediction. And lower feature \nlead to lower SHAP values and decreasing its contribution. For feature color its reverse.\n''',**{'font':'serif', 'size':12, 'weight':'normal', 'color': 'White'}, alpha = 0.95)\n\n\nplt.gcf().text(0.7,0.,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8, 'weight':'bold','color':'white'},alpha = 0.9)\nplt.gcf().show()","881265eb":"\nexplainerModel = shap.TreeExplainer(model)\nshap_values_Model = explainerModel.shap_values(X_test)\n\n# Initialize Jupyter notebook with initjs()\nshap.initjs();\n\n# Write in a function\ndef shap_plot(j):\n    \n    p = shap.force_plot(explainerModel.expected_value, shap_values_Model[j],feature_names= ['carat', 'Volume', 'y', 'x', 'clarity', 'color'])\n    return(p)","3449020e":"shap_plot(10)","47650daf":"shap_plot(1)","b9ec79e5":"shap_plot(100)","1fedfb55":"shap_plot(1000)","7d69c342":"shap_plot(10000)","c243fad5":"shap_plot(500)","53d2ba88":"shap_plot(8000)","ad174cf2":"<div style = \"font-family: serif; font-size: 18px; font-weight:;\"> Comparision of the Naive implementation and eli5 implementation. <\/div>","6713a643":"<div>\n <img style=\"float:center; border:3px solid #2676A1; height:; width:50%\" align= center src = \"https:\/\/miro.medium.com\/proxy\/1*vMV_FdR1yPAc-ktQrkH_hA.jpeg\"> \n <\/div>\n \n [Imge Source](https:\/\/medium.com\/analytics-vidhya\/why-should-i-trust-your-model-bdda6be94c6f)","a6569540":"<div>\n <img style=\"float:center; border:3px solid #2676A1; height:; width:50%\" align= center src = \"http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor_files\/stacking_cv_regressor_overview.png\"> \n <\/div>\n\n\n[Image Source: stacked regressor](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor\/)","7b5dfd2b":"<div style = \"font-family:serif; font-size: 17px;color:black\">Histories were made,and kingdoms fell for claiming precious things since the stone age. One such precious thing for us is Diamonds. As per one survey of luxury goods, Diamonds are most desirable things for 90 out of 100 people, why u ask? \nPeople say because of it's rarity and stunning beauty. Though, diamonds are diamonds, each have its own unique characteristics to consider for gauging its value. Coming to this dataset, we are given few such features along with there market price value in USD. Lets see these stats from statistical stand point and get some insights about diamonds.\n<\/div> <br> \n\n<div style = \"font-family:serif; font-size: 17px;color:black\"> \ud83d\udce3\ud83d\udce3\ud83d\udce2, lets continue our story. At the end of the day our \ud83e\udd35 need to find his perfect diamond. To Meet make it happen fast, guy have decided to analyize data and remove outliers\ud83d\udca3 from dataset. He decided to  approched guy friend of him named \ud83d\udc68\u200d\ud83c\udfeb(lol statistics) to help him his perfect data to make a \ud83c\udfaf model to get his perfect \ud83d\udc8e.\n<br>\n            \ud83d\udc68\u200d\ud83c\udfeb idea is simple, see basic stats and determine which paremeters are odd from statistical standpoint and \u2702\u2702 to off. \n<\/div><br> ","d6f3ebf2":"<a id = '3.2'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'>3.2 Better way to look at Statistics: Visualization <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#3.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#3.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","0c20ac40":"<a id = '5.1.3'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 3. Permutaion Importance In a Nut Shell <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.1.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5.1.3.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","a9388858":"<div style = \"font-family: serif; font-size: 18px; font-weight:;\">Finally model interpretabilty at the prediction level.\nHere we can visualize how much each feature contributing towards the predictions. And here on each feature we will see the marginaral contributions of the individual features. \n\n<br><br>\n\nFew things to know before are base values. Base values are the predictions when made with no feature data i.e, simple average of y_test data. Based on these values, and how poistively and negitively impacting on predictions dectate the contribution of each feature.\n<br>\n<br>\nIf a feature contribution is more than marginal contributions then its push towards target, else it pull away from the target.\nBut, What is marginal contributions? its an average of all possible permutations of contribuitons of each feature towards target. for instance, ABC are three friends and done a work in 10 hours, now if we take each person contribution as permutations such as (1,1,8),(1,2,7), (10,0,0),(3,7,0) like these set and compute average for each person, that would give  marginal contribution of that person. So, it feature contribution > marginal contributionn its +ve impact else -ve impact.\n\n<br><br>\n    \n    \nlets visualize the individual feature contribution toward target.\n    \n<br>\n<\/div>","8c813433":"<div style = \"font-family: serif; font-size: 18px; font-weight:;\">With all the analysis, visualizaitons, Modeling and interpretation of the model, now I am sure this model will give the best diamond for our guy. \n<br>\nWell its time to help our guy, with our little project, he got his prefect diamond, and lived happily ever after. \n<br><br>\n\nLets revist What have we done so far?\n\n<br>\n<ul><li>First we have exposed ourself to dataset terminology, and dataset.<\/li>\n    <li>Viewed data from statistical stand point.<\/li>\n    <li>Find few patterns from data<\/li>\n    <li>Fitted data to models<\/li>\n    <li>Made sure our mode is trust worthy.<\/li>\n   \n<\/ul>\n<br>\n\nWhat else can we do in futur?\nmay be tunning base models and traing the stacked regressor with those base models and intrepreting stacked model if possible.\n\n<br>\n    \n    \n   \n<p style = \"font-family: Garamond; font-size: 35px; font-weight:\"> <strong>Thanks for reading all the way here.............Please Up vote if you find my work useful.<\/strong><\/p>\n<\/div>","57611545":"<br><br>\n\n<a id ='2'><\/a>\n\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48;padding:2px\" align = 'center'>2. Libraries And Utilities<\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#1.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#2.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","728ec1ba":"<div style = \"font-family: serif; font-size: 18px; font-weight:;\">From the above figure, basevalue is mean of X_test, and red color is +ve and adds value to basevalue,and blue is -ve and substracts value from the basevalue. In this tug of war, if values contribute to increase overall value from basevalue is poistively impacted feature and if feature is contributed to reduction in overall values from basevalue it is negatively impacted feature. \n    <br>\n    <br>\n    Simply red features are contributing towards predcition of the feature, were as blue features dampen the effect.\n    <br>\n    <br>\n    \nLets visualize few more samples from the data.\n    <br><br>\n <\/div>\n    ","4cf79f3e":"<p style = \"font-family: garamond; font-size: 25px; font-weight:bold;  background-color: #2676A1 ; color: #FFEB48 ; padding :10px\">My Other Notebooks, Please Do Check Out!! Thanks.<\/p>\n<div style = \"font-family: Serif; font-size: 17px; font-weight:normal; color: \">\n<ul><br>\n    <li> Netflix is Awsome. Why? See here. \n        <a href = 'https:\/\/www.kaggle.com\/bhuvanchennoju\/netflix-is-awsome-why-see-here'>Click Here<\/a>\n    <\/li>\n    <li> Data storytelling with AUC Focus on Strokes. \n       <a href = 'https:\/\/www.kaggle.com\/bhuvanchennoju\/data-stroytelling-auc-focus-on-strokes'>Click Here<\/a>\n    <\/li> \n<\/ul> \n<\/div>\n","0bf70a5e":"<a id = '6'><\/a>\n<h1 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 6. Summary and Conclusions<\/h1>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.4.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","5051f849":"<a id = '7'><\/a>\n<h1 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 7. References<\/h1>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#6' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>","79f21202":"<div style = \"font-family: serif; font-size: 18px; font-weight:;\"> From the premutation feature selection, selecing top 6 featrues for the modeling. And lets hope this could give the better results.\n<br>\n<br>\nFor modeling part few linear models, tree based models, and boosting models are appended to a list of regressors and implementing a crossvalidtion of  10 folds to know the best model.\n   <br>\n    <br>\n<\/div>","88b54381":"<br><br><div style = \"font-family: serif; font-size: 18px; font-weight:;\"> With this little visualizaion attempt, we can clearly distingush the pricy diamonds from cheaper diamonds. This visualization is to see how closely related datapoints are we handling.<\/div><br><br>","d4977d8f":"<p style = \"font-family: cursive; font-size: 17px; font-weight:noramal; color:#2676A1 ;border-radius:5px ; background-color: #ffeb48; padding : 15px\"> After though \ud83d\udcc8 of statistics of the data, our \ud83e\udd35 found there are hidden \ud83d\udc8es in the data, and relatively high skewed <strong>(acceptable in range of |3|)<\/strong> datapoints along with high kurotisis values (acceptable in range of |10|, as noramal distribution has 3) for y,z which indicated fat tail or high outliers . Our \ud83e\udd35 decided to drop \ud83d\uddd1 Those missing values and <strong>apply 3 Standard Deviation<\/strong> to \u2702\u2702 outliers. <\/p>","7a750681":"<br><br>\n***\n***","e246bd78":"<a id = '5.1.3.2'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 3.2. Explain like I'm 5 (Eli5) -  Implementation of Permutation Importance <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.1.3.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","b183bf49":"<div style = \"font-family: serif; font-size: 18px; font-weight:;\"> All the data tweeks are completed from statistical stand poit and lets construct few features for visualization purpose. For binning of the diamonds following articles were refered. <\/div>\n    \n    \n*  [how to read diamond grading report](https:\/\/4cs.gia.edu\/files\/GIA-how-to-read-a-diamond-grading-report-EN.pdf)\n*  [What makes a diamond rare?](https:\/\/www.jewelry-secrets.com\/Blog\/diamond-rarity-is-a-factor-in-price\/)","90b74bf2":"<br>\n<h2 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #2676A1 ; color : #FFEB48 ; text-align: center; border-radius: 100px 100px; padding: 5px\"> Diamonds and Dollars: Story, Stats, Stacked, Sense<\/h2>\n<br>\n<br>\n<br>\n<div class = 'image'> <img style=\"float:center; border:2px solid black; width:60%\" align=center src = https:\/\/im.rediff.com\/money\/2018\/feb\/23diamond1.jpg?w=670&h=900> \n<\/div>\n<br>\n<br>\n<a href =\"https:\/\/im.rediff.com\/money\/2018\/feb\/23diamond1.jpg?w=670&h=900\" title = \"Diamond trade continues to be money-laundering conduit\" style = \"font-size:20px,color: dimgrey, text-align:left,font-family:serif\">Image Source: Diamond trade continues to be money-laundering conduit<\/a>\n<br>\n<br>\n\n\n***\n***","340044fb":"<div style = \"font-family: serif; font-size: 18px; font-weight:;\">  Stacked regression is a very old Idea which was introduced back in 1996 by <a href = 'http:\/\/scholar.google.co.in\/scholar_url?url=https:\/\/link.springer.com\/content\/pdf\/10.1007\/BF00117832.pdf&hl=en&sa=X&ei=J0SJYMPgL-eM6rQPtcud-Ac&scisig=AAGBfm1o75rkQ4uCRgr0NU-VKCTJ4P6kew&nossl=1&oi=scholarr'>Breiman<\/a>  \n<br><br>\n    In a nutshell  its a way to linearly combine a bunch of good linear or non linear predictors to acheive an imporved accuracy. Whole idea relays on cross validation and hold on predictions from crossvalidation set for all single predictors.  Key idea is when we train our training data on few selected base models with a cross validation implentation, and stacking the predictions from each fold of cross validaion of the basemodels to feed them as features to one final model (at level 1, which is last but one level) would improve the accuracy of predictions due to low redisudual error. \n \n<br>\n \nA detailed way of algoritham explination was available in this <a href = 'https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard'> kernel<\/a>    \n    \n<br>\n \nA general Schematic is provide in the above picture. Its a 5 step process\n<br>\n<ul>    \n    <li>step 1: split the train data into crossvalidtion folds, and select basemodels and meta model.\n            basemodel are the ones we lineraly combine to generate (meta) features for our meta-model.<\/li> <br>\n    <li>step 2: for each basemodel fit the (n-1) folds of cross validation data, and for 1 validation set make predictions \n            and strore in an array for meta features. <br><br>\n            Itereate through all the models in the basemodels and store validation fold predictions in a row stacks for \n            all the basemodels. Along with the predictions, store  the state of the predictiors for throught.<\/li><br>\n   <li> step 3: Once iteration over basemodels in done, Feed the row wise stacked hold-on-validation fold predictions as \n            features to meta model along with ground truth as true labels.<\/li><br>\n   <li>step 4: Now training of stacked regressor is done, and for final prediction, we need the test data to State stored\n            models and column stack the predictions. Take a mean of predictions in rowwise and feed that to trained\n            meta model.<\/li><br>\n   <li>step 5: This should give the final prediction with implementation of .predict method.<\/li> <br>\n\n<\/ul>  \n    \n<br>    \n    \n<\/div>","d171f655":"<div style = \"font-family: serif; font-size: 18px; font-weight:;\">\nFinally, We have our predictions. What next? can we reccomend required diamond to our guy.\n\n\nNot so fast, though we have our answers, how did we get these answers from our model is still kind of a black art!!\n\nCan we able to unravel the mystery of model and look how each feature impacting the model?, luckily yes. Before trusting our model for sure, lets depend on SHAP Values Concepts. <\/div>\n\n<div class = \"alert alert-block alert-info\" \"font-family: cursive; font-size: 25px; font-weight:noramal; color:#2676A1 ;border-radius:10px ; background-color: #ffeb48; padding : 15px\"><strong> Shap value generaliazation with an example <\/strong>In layman terms, SHAP Values tell us how much does each feature contributing to reach our final predictions and How they really impact the target prediction. Let me put things clear with an example. Everyone wants to be fit over fat right?, for being fit, \nneed exercise, healthy diet, good routine, and balanced healthcare. Following them in a right proportions and order could make one fit, and maintaining these in habits in utter commitment could push us towards being fit. On converse, poor commitment proportions of this pull towards \nfat. Exactly similar thing SHAP values does, tell us how things impacting our objective, which way are we going in overalll, and which feature is push towards target?, which feature in pulling away from the target?<\/div>\n","af48b870":"<a id = '5.3'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 5.3 Stacked Regression and Results<\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5.4' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","0c3f4476":"<a id = '4'><\/a>\n<h2 style = \"font-family: Garamond; font-size: 35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 4. Finding a Story in Data: Pattrens and Plots<\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#3.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","70248c93":"<div style = \"font-family: serif; font-size: 17px; font-weight:normal; color: \">Wikipedia says lot about diamonds. Cutting all those jargons and things that are out of scope of this notebook. Lets say, <strong>Diamond is a chunk of carbon<\/strong>, which been through hell on earth at extreme pressures and temperatures. Diamonds are rare, fancy, desiarable, and worth keeping. This a data story of a hopeless guy in search for his perfect diamond. <br><br>\n    \n<p style = \"font-family: Garamond; font-size: 35px; font-weight:bold; color: #2676A1\"> <s>An Analysis<\/s> A Story of A Perfect Stone<\/p><br>\nLets say there is \ud83e\udd35 with zero knowledge on \ud83d\udc8e and went to purchase a beautiful \ud83d\udc8d for his fiancee\ud83d\udc70. He just had a chat with \ud83d\udc74,and jeweller keeps on going with details, specs and all the boring stuff with price tags of shock. Poor guy just \ud83d\ude35 to the core and looking for desperate help in making a fine choice with trade off between \ud83d\udcb2\ud83d\udcb0 tag and \ud83e\udd29 of the diamond. Luckily our guy is a \ud83d\udcc8\ud83d\udcca\ud83d\udcc9 guy, so he fired up his \ud83d\udcbb and got a kaggle dataset of diamonds, started making a data dance to help him find this perfect diamond.\n\n<br> Our guy is likely to answer few of following questions to himself by end of his analysis\n<ul>\n <li>How much should one pay \ud83d\udcb0 for a \ud83d\udc8e?<\/li>\n <li>There are range of diamonds out there which deserve all his savings\ud83e\udd1e, Which one the worth spending fortune\ud83d\udcb8\ud83d\udcb8?<\/li> \n <li>Have to many technical and physical parameters \ud83d\udea7 of diamonds, which things have a best correlation\ud83d\udcc8\ud83d\udcca to Price tag?<\/li>\n    <li>Ultimately can he predict a price \ud83d\udcb2 for his ideal diamond based \ud83c\udfaf\ud83d\udcca\u2714?<\/li>\n<\/ul> \n\n <p style = \"font-family: cursive; font-size: 17px; font-weight:noramal; color:#2676A1 ;border-radius:5px ; background-color: #ffeb48; padding : 15px\"> Okay he got his questions \u2705 and knows whats he is \ud83d\udd0d for. As this is an \u2753area for him, He just want to know few details of  about diamonds specs, so that he can make few assumptions and make a better model. When he googled \ud83d\udd0e for requried information,he came to know about <strong>4C's of \ud83d\udc8e, and Diamond components<\/strong> are key parameters that decide the \ud83d\udcb2tag. <\/p>\n <\/div>\n ","51a8c4af":"<a id = '4.4.3'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 3. Color of The Diamond <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.4.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.4.4' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","0f1baaff":"<br>\n\n<br>\n\n***\n***","023fd5c0":"<a id = '4.4.1'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 1. Carat of The Diamond <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.4.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","8e0f1825":"\n<br><br>\n<a id = '4.6'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 4.6 Diamond Clustering Visualization wrt Price Tag <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.5'style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","a59e128f":"<br>\n<a id = '2.1'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48;padding:2px\" align = center>2.1 Dataloading, Colorpalette for visulaization <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#2' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#3' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","bedffcf5":"\n\n<div style = \"font-family:serif; font-size: 17px;color:black\">  \n\nFrom Stastical overview \ud83d\udc68\u200d\ud83c\udfeb deduce following insights\n\n<ol>  \n <li> <strong>Cut:<\/strong> Ideal cut is common with highest frequency. Cut determines \u2702\u2702 of the jeweal as it determines the amount of light refract within the diamond and brings out that shining beauty, so, assuming this one goes  hand in hand with \ud83d\udcb2.<br>\n    <p style = \"font-family: Garamond; font-size: 22px;color:#2676a1; font-weight: bold\">Cut Categories: Fair, Good, Very Good, Premium, Ideal (Ordered from worst to best)<\/p><\/li>\n    \n <li><strong>Clarity:<\/strong>SI1 grade is common with highest frequency. Clarity of the diamond is nothing but free of any intrutions or flaws. Who does not love \n    flawless things? atleast me :) So, lets assume this is super important. <br> <p style = \"font-family: Garamond; font-size: 22px;color:#2676a1; font-weight: bold\">Clarity Categories:  I1 , SI2, SI1, VS2, VS1, VVS2, VVS1, IF (Ordered from worst to best) <\/p><\/li>\n    \n <li> <strong>Color:<\/strong> G colorscale is common kind of diamond. Surpricingly diamond with no color are extremely rare and regard as highly valuable.       <br><p style = \"font-family: Garamond; font-size: 22px;color:#2676a1; font-weight: bold\">Color Categories: J, I, H, G, F, E, D  (Ordered from colored to less color, same is order of rarity)<\/p><\/li>\n          \n <li> <strong>Carat:<\/strong> 1 carat diamonds are common, and beyond 1 carat are rare. As caret is a \u2696 parameter, finding a dinmond so heavy is very difficult and rare. It makes some sence. More the carat value, rare the diamond.<\/li>\n                    \n <li> <strong>Depth%:<\/strong> It a ratio of the total depth (height of the diamond) to the table of the diamond, so, it is an empirical parmaeter, and it determinds the light prenetrations in to the diamond. As per wikipedia, as the depth increases diamond looks dull and it's value reduces. So, need furthur study. Based on cut shapes, ideal depth % should be in between (55-70)%<\/li>\n                  \n <li><strong>Table%:<\/strong> It is a ratio of the truncated table on top of the diamond to whole diameter avereage. This measure the size of the diamond, does deals with the light beam penetrations as well. Based on cut shapes, ideal Table % should be in between (52-60)%<\/li>\n                  \n                  \n <li> <strong>X,Y,Z :<\/strong> These are dimensions in mm, Interestingly we have values of 0 as minimum, as these are table and depth parameters, its not practically possible. Based on these three dimensions Depth and Table perentages could be caclulated. There are severe outlier in these 3 columns.<\/li>\n          \n<li><strong>Price :<\/strong> Most of the dimonds are under 5k USD and outliers are there upto 18k USD. This is target, which is a continous distribution, and lots of paremeters are effecting this price value.<\/li>\n<\/ol>\n   \n<\/div>\n\n\n<div class = \"alert alert-block alert-info\" style = \"font-family:cursive ;font-size:24px, font-color: black\"> \ud83d\udce2 Missing values alert!!!!! Data have missing values hidden in X,Y,Z  features as zero values, if anyvalue is zero, then physically diamond don't exit. Looks like he found invincible diamonds in our data. \ud83d\udef8 <\/div>","7ac986aa":"<br>\n<br>\n<div style = \"font-family: serif; font-size: 18px; font-weight:;\"> From the implementation of the Stacked regressionn surely increased the r2 score a little. May be hyperparemeter tunning of individual base models could change the situation. With optimized parameters may be higher r2 score could be reached. <\/div>\n<br>\n<br>","2a61d227":"<br><br>\n<a id = '5.2'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 5.2 Base models and Results <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.1.3.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>\n<br><br>","6a0d878e":"<a id = '5.1.2'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 2. Feature Importance with Introduction of Random Feature <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.1.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5.1.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","0453a525":"<p style = \"font-family:serif; font-size: 17px; font-weight:normal;border-radius: ; background-color: ; color: ; padding:\">Lets see which modules are used by him, color schemes for the visualizations.<\/p>","205ba3c2":"<a id = '5.1.3.1'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 3.1. A Naive Implementation of Permutation Importance <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.1.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5.1.3.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","85c9af69":"<br><br>\n<a id = '3'><\/a>\n<h2 style = \"font-family: Garamond; font-size: 35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'>3. Statistical Insights about Dataset <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#2.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#3.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","1e743a9f":"<div style = \"font-family:Garamond; font-size: 25px; font-weight:noramal; color:#2676A1 ;border-radius:5px ; background-color: #ffeb48; padding : 15px\"> \n    Why Model Interpretation is need of the Hour?\n<ul>\n<li>How much does each feature contributing towards final predictions?<\/li>\n<li>How (positively or negatively) each feature impact the final prediction?<\/li>\n<li>Whats impact of overall features towards final prediction?<\/li>\n<\/ul>\n    This whole questions will be answered with game theory starategy and SHAP values.\n<\/div>\n<br><br>","62ebf5c9":"<br><br>\n<a id = '5.4.2'><\/a>\n<div style = \"font-family: Garamond; font-size: 35px; font-weight:bold; background-color: #2676A1 ; color: #FFEB48 ; padding :10px  \">Local Intepretability of the Model:<\/div>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.4.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#6' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>\n<br>\n<br>\n<br>\n\n<div style = \"font-family: serif; font-size: 18px; font-weight:;\">With the local interpretability one can get a bigger picture in feature level,like Which direction does this features taking the target? How values of feature impacting the features? Which feature is highly contributing towards target? Which featrue in not important? <br>\n<br>\n    \nLocal Interpretation in noting but understanding of model at feature level. Thanks to SHAP functionality we can visualize how each feature contribute to final prediction can be seen clearly. As per my understanding we can put these interactions into these three level\n    \n<ul><li> With Mean SHAP values, understanding of Average Impact of features over the predictions  <\/li>\n        <li> With actual SHAP values, understanding the direction feature move the prediction based on features numerical value.<\/li>\n        <li> How much does each feature move from base values (average values) to contribute to prediction based on positive and negative impact of the feature over prediction.<\/li>\n<\/ul>\n\nTo Answer this question lets examine the predictions of model with xgboost regressor, as it is giving the best score after the stacked regressor. And use SHAP module to find the shap values and visualize the results.\n\n<br>\n    \n  Lets visualize the Mean SHAP valeus and impact order over the features. These are similar to feature importance given in an order of highly impacting feature to low. With these Averaged SHAP, we know that feature has impact on target, but not clear inforamtion about in sense of positive and negative impact can't be iterepted for this plot.\n <br>\n <br>\n     \n<\/div>\n\n\n    \n    \n   ","e00c83a7":"<a id = '5'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 5. Modeling and Results<\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.6' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","9c55efbf":"<a id ='1.0'><\/a>\n\n<h2 style = \"font-family: Garamond; font-size:40px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48 ; padding : 2px\" align = 'center'>1. Introduction<\/h2>\n\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#1.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","6002b03f":"<a id ='1.1'><\/a>\n<h2 style = \"font-family: Garamond; font-size: 35px; font-weight:bold;border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = center> 1.1 Diamond 101: 4C's Diamond - Carat, Color, Clarity, Cut<\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#1.0' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#1.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>\n<br><br>\n\n<div>\n <img style=\"float:center; border:3px solid #2676A1; height:; width:50%\" align= center src = \"https:\/\/raw.githubusercontent.com\/bhuvan454\/AI_workspace\/master\/EDA\/Diamonds\/4cs.png\"> \n <\/div>\n    <a href = \"https:\/\/www.youtube.com\/watch?v=qJs6P7ChGd8\"> Image Source: Diamond Education-Learning Beyond 4C's<\/a>\n \n <br><br>\n\n <div style = \"font-family: serif; font-size: 18px; font-weight:normal; color: \"> Our guy understood that diamond quality and price are gauged by four important parameters called  4 C's, which are Carat, Clartiy, Color, Cut. Infographic gives what and what about 4 C's in a fun way. Lets see the elobrate these 4C's.\n    <ol>\n      <li><a style = \"font-family: serif; font-size: 20px; font-weight:bold;color:#FB5B68\">Carat:<\/a>\n          Carat is the weight of the diamond. Bigger the diamond, rare to find them.<\/li>\n      <li><a style = \"font-family: serif; font-size: 20px; font-weight:bold;color:#FB5B68\">Clarity:<\/a>\n          Clarity of the diamond is intrusions in the diamond, which could be internal flaws, external defects.<\/li> \n      <li><a style = \"font-family: serif; font-size: 20px; font-weight:bold;color:#FB5B68\">Color:<\/a> \n          Color of the diamond is self explinary, and on scale of AtoZ, A is best and colorless, where as Z is worst and yellowish color. The colorless diamonds are rare and costly.\n      <li><a style = \"font-family: serif; font-size: 20px; font-weight:bold;color:#FB5B68\">Cut:<\/a> \n          Cut of the diamond is about physical dimensions of the diamond. It determine the light amount enter into the diamond. Anotomy of the diamond is an important parameter. <\/li>\n    <\/ol>\n   <\/div>    \n\n","3511429d":"<br>\n<br>\n<div style = \"font-family: serif; font-size: 18px; font-weight:;\"> Though feature importance showing expected features on top of the list, its better to perform a random feature test to eliminate the doubt of randomness in the system.<\/div><br><br>","f558c290":"<div style = \"font-family: serif; font-size: 18px; font-weight:;\"> Implementing Premutation Importance for El5 module to validate our litte experiment with premutation importance. Weights of each feature importance shows the similar trend as our experiment, and thus corroborating that our method is valid as eli5 implementation. For feature selection lets use these importances. <br><br> <\/div>","5b221803":"<a id = '3.1'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'>3.1 Data Summary: Descriptive Statistics & Datatypes<\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#3' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#3.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","1d8c62ba":"* [simple-matplotlib-visualization-tips](https:\/\/www.kaggle.com\/subinium\/simple-matplotlib-visualization-tips\/comments)\n* [top-50-matplotlib-visualizations-the-master-plots-python](https:\/\/www.machinelearningplus.com\/plots\/top-50-matplotlib-visualizations-the-master-plots-python\/)\n* [simple-trick-to-select-variables-for-model](https:\/\/www.kaggle.com\/michau96\/simple-trick-to-select-variables-for-model)\n* [feature-selection-with-null-importances](https:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances)\n* [Beware of Random Forests](https:\/\/explained.ai\/rf-importance\/index.html)\n* [better-heatmaps-and-correlation-matrix-plots-in-python](https:\/\/towardsdatascience.com\/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec)\n* [just-keep-stacking-implement-stacking-regression-in-python-using-mlxtend](https:\/\/towardsdatascience.com\/just-keep-stacking-implement-stacking-regression-in-python-using-mlxtend-3250ff327ee5)\n* [stacked-regressions-top-4-on-leaderboard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n* [why-should-i-trust-your-model](https:\/\/medium.com\/analytics-vidhya\/why-should-i-trust-your-model-bdda6be94c6f)\n* [feature-selection-with-permutation-importance](https:\/\/www.kaggle.com\/paultimothymooney\/feature-selection-with-permutation-importance)\n* [create-beautiful-notebooks-formatting-tutorial](https:\/\/www.kaggle.com\/shubhamksingh\/create-beautiful-notebooks-formatting-tutorial)\n* [explain-your-model-with-the-shap-values](https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d)\n* [A Unified Approach to Interpreting Model Predictions](https:\/\/arxiv.org\/abs\/1705.07874#:~:text=Understanding%20why%20a%20model%20makes,prediction's%20accuracy%20in%20many%20applications.&text=SHAP%20assigns%20each%20feature%20an%20importance%20value%20for%20a%20particular%20prediction.)\n* [shap-explained-the-way-i-wish-someone-explained-it-to-me](https:\/\/towardsdatascience.com\/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30#:~:text=In%20a%20nutshell%2C%20SHAP%20values,answer%20the%20%E2%80%9Chow%20much%E2%80%9D.)\n* [shap github source code](https:\/\/github.com\/slundberg\/shap\/blob\/master\/shap\/plots\/_beeswarm.py)","36d500f7":"<a id = '4.4.4'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 4. Clarity of The Diamond <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.4.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.4.5' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","9ce4fb69":"<div style = \"font-family: garamond; font-size: 35px; border-radius:5px; font-weight:bold; background-color: #2676A1 ; color: #FFEB48 ; padding :10px \">  If you find this work useful, Please Up Vote!, Kindly don't ignore... Happy reading... <\/div>\n\n\n<br>\n<br>\n\n***\n*** ","ed33b288":"\n<br><div style = \"font-family: serif; font-size: 18px; font-weight:;\">In this dependence plot, on X-axis actual feature values are given, Y-axis corresponding SHAP Values are given, and on colorbar a most interacting feature with respect to carat feature\nis given which is clarity. Fist, from X-y scatter, it is observed that feature, and SHAP have a linear realtion, i.e, increase in feature results in contribution to wards prediction. and color map enables to viualize the interplay between three parameters, SHAP value, and other two actual feature values. Instance, Higher valued carat with high value calrity \nhave a high SHAP values, which are indirectly expensive diamonds.<br><br>\nLets visulaize overall features global interpretation with respect to other highly interactive features.\n    \n<br><br>\n\n<\/div>","0a744cfc":"<br>\n<br>\n<div style = \"font-family: serif; font-size: 18px; font-weight:;\"> With introduction of a random feature into the training data, is  shows very interesting results. Our randomized feature is outperforming the actal features, and this may be valid but, now raise the doubt out trust worthyness of our features for random act. To over come this doubt, lets perfom a permutation feature importance and decide which feature is matter. <\/div><br>","a1412006":"<a id = '4.4.5'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 5. Dimensions of The Diamond <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.4.4' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.5' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","406be879":"<a id ='1.2'><\/a>\n<h2 style = \"font-family: Garamond; font-size: 35px; font-weight:bold;border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = center>1.2 Diamond Components and Cuts: Price Proportional Parameters<\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#1.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#2' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>\n\n\n<br>\n<table>\n    \n<tr>\n   <br><br>\n<td> <img style=\"float:center; border:5px solid #2676A1 ;height:400px; width:900px\"  src = \"https:\/\/raw.githubusercontent.com\/bhuvan454\/AI_workspace\/master\/EDA\/Diamonds\/componentsof%20diamond.png\">\n    \n<a href = 'https:\/\/www.brilliance.com\/infographs\/anatomy-of-a-diamond'>Image Source:Anatomy of a Diamond Infograph<\/a>\n<\/td>\n    \n<td> <img style=\"float:center; border:5px solid #2676A1; height:400px; width:1000px\"  src = \"https:\/\/raw.githubusercontent.com\/bhuvan454\/AI_workspace\/master\/EDA\/Diamonds\/diamond%20cut.png\">\n<a href = 'https:\/\/www.brilliance.com\/infographs\/anatomy-of-a-diamond'>Image Source:Anatomy of a Diamond Infograph<\/a>\n <\/td>\n    \n<\/tr>\n<\/table>\n\n\n\n\n<div style = \"font-family:serif; font-size: 17px; font-weight:normal;border-radius: ; background-color: ; color: ; padding:\"> \n    <blockquote>There are several things to consider to see in a diamond. Few such is table%, depth% and cuts. Diamond anotomy is given it the cut of the diamond infographic in the above image.\n    <ul>\n        <li>All the basic information about the diamond cut are givn above.<\/li>\n        <li>Side view of the diamond show the terminology and meaning of his features.<\/li>\n        <li>Thus table length, table width, diamond depth, depth%, and table% are very important features to  determine the price  of a diamond. <\/li>\n    <\/ul>\n    \n   <\/blockquote>\n        <\/div>\n        \n        \n  \n  <p style = \"font-family: cursive; font-size: 17px; font-weight:noramal; color:#2676A1 ;border-radius:5px ; background-color: #ffeb48; padding : 15px\"> Hmm, thanks to google, \ud83e\udd35 got all the information about \ud83d\udc8e, and basic terminology of the \ud83d\udc8e. Now, what? As part of routine, he want to \ud83d\udcca\ud83d\udcc8 diamonds to see which feature are \ud83d\udd25  and which are \ud83d\udea8! So, next <strong>Let the Data Tell A Story \ud83d\udcad\ud83d\uddbc <\/strong>for itself about \ud83d\udcb5\ud83d\udcb5. <\/p>\n \n <br>\n <br>\n\n***\n***","f9e10b73":"<br>\n<br>\n<div style = \"font-family: serif; font-size: 18px; font-weight:;\"> With the visualization part our guy, understood that price, has strong correaltionss with the dimensions of the diamond and 4C's of the diamond. \n<br><br>\nNow, lets build a model to predict accurately as required for our guy. \n<br><br>\nHere method is simple, find featrue importance, select features, build some base models, and stack them high to make a final predictions. In the end lets see how can we interpert our model to our guy and help him find his ring.\n<br>\n<br>\nlets do few last minute tweeks to data before jump into modeling. <\/div>\n<br> <br>","4ef2e8be":"<div style = \"font-family: Garamond; font-size: 30px; font-weight:bold; background-color: #2676A1 ; color: #FFEB48 ; padding :10px \">Anything Interesting here?<\/div>\n<br>\n<div style = \"font-family: serif; font-size: 17px; font-weight:normal; color: \">Yes. Whole content can be dived into four parts. Few things about Data, Statistics, Visualization, and Modeling.\n\n<ul>\n    <li> Visualization of Statistics of The Data.<\/li>\n    <li> How much does people pay for a diamond for it weight, color, flaws and cuts<\/li>\n    <li> Fun Experiments with <strong>Default feature importances<\/strong> <\/li> \n    <li> Implementation of Permutation Feature Importance With a <strong>Very Basic Idea and ELI5 Comparission.<\/strong><\/li>\n    <li> Thanks to <a href ='https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard'>serigne  <\/a>\n        , Adopted Stacking Regression Idea and Attempted here. You can read about math \n        <a href = 'http:\/\/scholar.google.co.in\/scholar_url?url=https:\/\/link.springer.com\/content\/pdf\/10.1007\/BF00117832.pdf&hl=en&sa=X&ei=iRGIYP2jJcyWywTK4I3gDw&scisig=AAGBfm1o75rkQ4uCRgr0NU-VKCTJ4P6kew&nossl=1&oi=scholarr'> here.<\/a> <\/li>\n    <li>Finally, Interpretation of model predictions with <strong>SHAP Values<\/strong>.<\/li>\n        \n<\/ul>  \nThanks to <a href = 'https:\/\/www.kaggle.com\/shivam2503'>shivam<\/a> for this wonderful dataset.\n<\/div>","dd8ef91b":"<a id = '4.4'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'>4.4 4C's and Dimensions of Diamond: Price Comparions<\/h2>\n\n\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.4.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","90aa6efe":"<a id = '0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #2676A1; color : #FFEB48; border-radius: 300px 300px; text-align:center; font-weight: bold\" >Table of Contents<\/h2>\n\n\n* [1. Introduction ](#1.0)\n    * [1.1 Diamond 101: 4C's Diamond - Carat, Color, Clarity, Cut](#1.1)\n    * [1.2 Diamond Components and Cuts: Price Proportional Parameters](#1.2)\n* [2. Libraries And Utilities](#2)\n    * [2.1 Dataloading, Colorpalette for visulaization](#2.1)\n* [3. Statistical Insights about Dataset](#3)\n    * [3.1 Data Summary: Descriptive Statistics & Datatypes](#3.1)\n    * [3.2 Better way to look at Statistics: Visualization](#3.2)\n    * [3.3 Setting up  Stage for Data Viz: Missing Diamonds, Duplicate Diamonds, And Odd diamonds](#3.3)\n* [4. Finding a Story in Data: Pattrens and Plots](#4)\n    * [4.1 Univariate Numerical Feature Distributions](#4.1)\n    * [4.2 Univariate Categorical Feature Distributions](#4.2)\n    * [4.3 Overall Correlationship Maps For Features](#4.3)\n    * [4.4 4C's and Dimensions of Diamond: Price Comparions](#4.4) \n       * [1. Carat of The Diamond](#4.4.1)\n       * [2. Cut of The Diamond ](#4.4.2)\n       * [3. Color of The Diamond](#4.4.3)\n       * [4. Clarity of The Diamond](#4.4.4)\n    * [4.5 Multi-variate Analysis of Diamond](#4.5)\n    * [4.6 Diamond Clustering Visualization wrt Price Tag](#4.6)\n* [5.Modeling and Resutls](#5)\n    * [5.1 Feature Importance and Experiments ](#5.1)\n       * [1. Feature Importance for Actual Features](#5.1.1)\n       * [2. Feature Importance with Introduction of Random Feature](#5.1.2)\n       * [3. Permutaion Importance In a Nut Shell](#5.1.3)\n          * [3.1. A Naive Implementation of Permutation Importance](#5.1.3.1)\n          * [3.2. Explain like I'm 5 (Eli5) -  Implementation of Permutation Importance](#5.1.3.2)\n          \n    * [5.2 Basemodels and Results](#5.2)\n    * [5.3 Stacked Regression and Results](#5.3)\n    * [5.4 Interpretability of Model with Shap Values](#5.4)\n\n* [6. Summary and Conclusions](#6)\n* [7. References](#7)\n\n<br>\n<br>\n\n***\n***    \n","57b041e3":"<br>\n<a id = '4.2'><\/a>\n<h2 style = \"font-family: Garamond; font-size: 35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 4.2 Univariate Categorical Feature Distributions <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","50d70377":"<div style = \"font-family: Garamond; font-size: 30px; font-weight:bold; background-color: #2676A1 ; color: #FFEB48 ; padding :10px  \">What's This Notebook About?<\/div>\n\n\n<br>\n<div style = \"font-family: serif; font-size: 17px; font-weight:normal; color: \">Its a story of guy in search for his perfect diamond for his Dday, and we are attempting to help him find it with a \nGood visualization, Statistics, and  Good machine learning model.<\/div>","8d7e0b5a":"\n<br>\n<br>\n<a id = '4.5'><\/a>\n<h1 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 4.5 Multi-variate Analysis of Diamond<\/h1>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.4.5' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.6' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","ac85a459":"<a id = '5.1'><\/a>\n<h1 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 5.1 Feature Importance and Experiments <\/h1>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5.1.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","d9f2f673":"<br>\n\n*** \n\n***","f2002ffa":"<br>\n<br>\n<div style = \"font-family: serif; font-size: 18px; font-weight:;\">Lets visualise the actual SHAP values vs features plot, where \nwe can get the sense of the directionality factor for the prediction. \n<br><br>\n<p style = \"font-family: serif; font-size:20px; font-weight:;\"><strong>Explination of the Beeswarm Summary plot:<\/strong><p>\n   \n<ul><li><strong>Feature importance:<\/strong> Features are ranked in descending order.<\/li><br>\n        <li><strong>Impact on the prediction:<\/strong> Position of scatter dots for a feature informs the SHAP values corresponding from Xaxis. Positive X direciton means high impact, negative X direction means low imapct .<\/li><br>\n        <li><strong>Original value:<\/strong> Color shows whether that feature numerical value is high (in red) or low (in yellow) for that observation.<\/li><br>\n        <li><strong>Correlation:<\/strong> Features with Positve SHAP values push towards the target, likewise feature negaitve SHAP values \npull away from the target. For instance, feature Y - Higher feature values leads \nto higher SHAP values there by increase its contribution towards prediction. And lower feature \nlead to lower SHAP values and decreasing its contribution. For feature color its reverse<\/li>\n<\/ul>\n\n<\/div>\n<br>\n<br>","2aef8345":"There are missing data in X,y,z and these are very less in compared to dataset, so let's drop them.","1294e499":"<div style = \"font-family:serif ; font-size: 17px;color:black\">  \n Mr.statistics, \ud83d\udc68\u200d\ud83c\udfeb found that data have 3\ufe0f\u20e3 categorical features, which Cut, Clarity, Color, along with 4\ufe0f\u20e3 Numerical features are depth%, table%, x,y,and z. \ud83c\udfaf is \ud83d\udcb2. Now \ud83e\udd35 told his friend \ud83d\udc68\u200d\ud83c\udfeb about 4C's he learned from his research. So, \ud83d\udc68\u200d\ud83c\udfeb decided that they are clearly dealing with Ordinal categorical features, So, it is Safe to assume they have some kind of ordered \u2696\u2696 on \ud83d\udcb2.\n<br>\nWith initial understandig of stats, we can make these observations,\n<ul>\n<li> There are no  <strong>Explicit Null Values<\/strong> Present in the dataset.<\/li>\n<li>   Price, depth,y, and z have many outliers , which can be be infered for percentailes and standad deviation.<\/li>\n<li>   Data normailzation could be needed for price.<\/li>\n<li>   They have three kinds of datatypes, int, float, and object.<\/li>\n<\/ul>\n <\/div>","0ff39cd5":"<a id = '4.1'><\/a>\n<h2 style = \"font-family: Garamond; font-size: 35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 4.1 Univariate Numerical Feature Distributions <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","2e6ed488":"<br>\n<br>\n<div style = \"font-family: serif; font-size: 18px; font-weight:;\"> Lets code an simple permutaion feature selection helper.\nIdea behind is discussed in an article named Beware of Random Forests, and here we will see which feature is actually important by measureing the change in metric with randomization of the one feature at a time.\n<br>\n<br>\n<ul> Its a four step process and it goes as follows.<br>\n    <br>\n    <li> Split the training data into training data,and hold on data(validataion data).<\/li><br>\n    <li> Fit the model with training data, and get a baseline metric with validation data.<\/li><br>\n    <li> Key part is shuffle one feature at time in validation case and find the metric with shuffle validaiton data on pretrained model.<\/li><br>\n    <li> For each modified metric calculte the residual between baseline metric and modified metric<\/li><br>\n    <li> Highly important feature could result in higher in residual, which imploys its importance<\/li><br>\n    \n<\/ul>\n<br>\n\n<\/div>","b864c811":"<div style = \"font-family: serif; font-size: 18px; font-weight:;\">\n<br><br>\n\nResults look pretty good and all the boosting models are performing way better. \n<br><br><\/div>","36deb5b8":"<br><br>\n<a id = '5.4.1'><\/a>\n<div style = \"font-family: Garamond; font-size: 35px; font-weight:bold; background-color: #2676A1 ; color: #FFEB48 ; padding :5px  \">Gloabal Intepretability of the Model:<\/div>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.4' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5.4.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>\n<br>\n<br>\n<br>\n\n\n<div style = \"font-family: serif; font-size: 18px; font-weight:;\">With the global interpretability one can get the overall how each feature is effecting the model and prediction. This SHAP values enable us to understand how this complex models work in a a global scale for all features. \n<br>\n<br>\nHere questions, we deal are about the feature contriution with respect to highly interacting feature to the predictions. Global interpretations can be visualized with the help of the dependence plots and scatter plot. What we are interested are like how each feature realted to coressponding SHAP values, and how highly interacting feature effecting this SHAP values.\n<br>\n<br>\nTo Answer this question lets examine the predictions of model with xgboost regressor, as it is giving the best score after the stacked regressor. And use SHAP module to find the shap values and visualize the results.\n\n\n<br><br>\nLets see the one particular features global interpretation with the dependency plot.\n<br>\n<\/div>","a6a73c58":"<a id = 5.1.1><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding: 2px\" align = 'center'> 1. Feature Importance for Actual Features <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#5.1.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","1fca64ea":"<a id = '4.4.2'><\/a>\n<h2 style = \"font-family: Garamond; font-size: 35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 2. Cut of The Diamond <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.4.1' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.4.3'style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","61913010":"<br>\n<a id = '4.3'><\/a>\n<h2 style = \"font-family: Garamond; font-size: 35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 4.3 Overall Correlationship Maps For Features <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#4.2' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#4.4' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","ac88e1ec":"<br>\n<a id = '3.3'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'>3.3 Setting up  Stage for Data Viz: Missing Diamonds, Duplicate Diamonds, And Odd diamonds <\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#3.2'style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a> \n<a href = '#4' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>","0a77edb3":"<br>\n<br>\n<div style = \"font-family: serif; font-size: 18px; font-weight:;\">Why do I need to good features? because I need a better prediction. But can I trust my prediction if I am giving a garbage into my model. No right!, \n \n To avoid the garbage in and garbage out situations, need a solid preperation in data and model. In plethora of options to avoid such issues, One strategy is this premutaion importance, It ensures to keep only real impacting features in the training data. \n    \nWhat exactly we do in premutation feature importance? we shuffle internally one feature at a time and measure its impact on the given metric and we rank the features according to residue of metric loss. <\/div>\n\n<br>\n<br>","3350b65b":"<br>\n<br>\n<a id = '5.4'><\/a>\n<h2 style = \"font-family: Garamond; font-size:35px; font-weight: bold; border-radius: 300px 300px; background-color: #2676A1; color : #FFEB48; padding:2px\" align = 'center'> 5.4 Interpretability of Model with Shap Values<\/h2>\n<a href = '#0' style = 'font-size:15px' align = 'right'>\u23eb<\/a>\n<a href = '#5.3' style = 'font-size:15px' align = 'right'>\ud83d\udd3c<\/a>\n<a href = '#6' style = 'font-size:15px' align = 'right'>\ud83d\udd3d<\/a>\n<a href = '#7' style = 'font-size:15px' align = 'right'>\u23ec<\/a>\n<br>\n<br>","3812ebd0":"<br>\n<br>\n<div style = \"font-family: serif; font-size: 18px; font-weight:;\">\nEvery feature cannot be of same importance in prediction as most of the data includes of redundant data with in. Moreover an effective prunnin of data, could save lots of trianing time as well. So, to know which feature is to keep in the dataset, lets explolit the inbuilt functionality of the XGBoost regressors feature importance. Few helper functions are defined below to met the objective. <\/div>"}}