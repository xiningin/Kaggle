{"cell_type":{"d4cfc0ca":"code","0b7a57f9":"code","d03e0c1f":"code","e8ce3e8e":"markdown","d470109c":"markdown","7d54cd6d":"markdown","00e0c2f5":"markdown","0fe9ba3e":"markdown"},"source":{"d4cfc0ca":"import pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tqdm import tqdm","0b7a57f9":"def format_prediction_string(image_id, result):\n    prediction_strings = []\n\n    for i in range(len(result['detection_scores'])):\n        class_name = result['detection_class_names'][i].decode(\"utf-8\")\n        # Coordinates of the bounding box in the correct order\n        corrected_coordinates = [result['detection_boxes'][i][1], result['detection_boxes'][i][0], result['detection_boxes'][i][3], result['detection_boxes'][i][2]]\n        boxes = corrected_coordinates\n        score = result['detection_scores'][i]\n\n        prediction_strings.append(\n            f\"{class_name} {score} \" + \" \".join(map(str, boxes))\n        )\n\n    prediction_string = \" \".join(prediction_strings)\n\n    return {\n        \"ImageID\": image_id,\n        \"PredictionString\": prediction_string\n    }","d03e0c1f":"sample_submission_df = pd.read_csv('..\/input\/sample_submission.csv')\nimage_ids = sample_submission_df['ImageId']\npredictions = []\n\n# Create session\nwith tf.Graph().as_default():\n    # Create our inference graph\n    image_string_placeholder = tf.placeholder(tf.string)\n    decoded_image = tf.image.decode_jpeg(image_string_placeholder)\n    decoded_image_float = tf.image.convert_image_dtype(image=decoded_image, dtype=tf.float32)\n    # Expanding image from (height, width, 3) to (1, height, width, 3)\n    image_tensor = tf.expand_dims(decoded_image_float, 0)\n    # Load the model from tfhub.dev, and create a detector_output tensor\n    model_url = \"https:\/\/tfhub.dev\/google\/openimages_v4\/ssd\/mobilenet_v2\/1\"\n    detector = hub.Module(model_url)\n    detector_output = detector(image_tensor, as_dict=True)\n    # Initialize the Session\n    init_ops = [tf.global_variables_initializer(), tf.tables_initializer()]\n    sess = tf.Session()\n    sess.run(init_ops)\n\n# Make prediction on test set\nfor image_id in tqdm(image_ids):\n    # Load the image string\n    image_path = f'..\/input\/test\/{image_id}.jpg'\n    with tf.gfile.Open(image_path, \"rb\") as binfile:\n        image_string = binfile.read()\n\n    # Run our session\n    result_out = sess.run(\n        detector_output,\n        feed_dict={image_string_placeholder: image_string}\n    )\n    predictions.append(format_prediction_string(image_id, result_out))\n\nsess.close()\n\npred_df = pd.DataFrame(predictions)\npred_df.to_csv('submission.csv', index=False)","e8ce3e8e":"## Inference","d470109c":"# TF Hub - Bounding boxes coordinates corrected","7d54cd6d":"## Create prediction string into kaggle format, making correction for BB coordinates","00e0c2f5":"### This kernel is highly inspired by [**this code**](https:\/\/www.kaggle.com\/xhlulu\/intro-to-tf-hub-for-object-detection), please do not hesitate to upvote this kernel and the original one if it helps you.\n### According to [**this discussion**](https:\/\/www.kaggle.com\/c\/open-images-2019-object-detection\/discussion\/98205), it seems that the order of coordinates for bounding boxes is different between kaggle and tensorflow. Putting the coordinates back in the correct order may give a much higher score using TF Hub as it is shown in the [**original kernel**](https:\/\/www.kaggle.com\/xhlulu\/intro-to-tf-hub-for-object-detection). In this kernel, we will implement this small correction.","0fe9ba3e":"## Imports"}}