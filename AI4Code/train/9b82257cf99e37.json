{"cell_type":{"ee7e29df":"code","a3468dd1":"code","c583f6e2":"code","309b6595":"code","723cf5e0":"code","c3f68adb":"code","1e3ae2e0":"code","317cb6ed":"code","e69583c6":"code","644c6917":"code","634de665":"code","980c0492":"code","14cf6c48":"code","08ea323b":"code","1b23c5a4":"code","9fcdbc9d":"code","7e8b53e4":"code","7d7a7115":"code","f4916ac0":"code","0395834a":"code","481d3c64":"code","6a594f16":"code","fc8e55c7":"code","a926fd0d":"code","4785af0b":"code","a6e32fe0":"code","84b51102":"code","205ddf3d":"code","3b2b6274":"code","52dad6af":"code","d6c033a5":"code","757af34a":"code","52a2adf9":"code","0e6439e4":"code","a869f73b":"code","32c1b656":"code","2a5867bc":"code","64a57612":"code","41306b3d":"code","71b0d2a6":"code","42331c4c":"code","eab5313d":"code","3d5d85d3":"code","14950a99":"code","7ccb6640":"code","fa66e4d7":"code","8743c1a1":"code","63ce132d":"code","64252313":"code","4a021420":"code","a48197f8":"code","da3df5f9":"code","86670ca3":"code","921e8c30":"code","496bcecc":"code","5f21776a":"code","33ed3b5f":"code","b15ee30a":"code","298a9171":"code","c895168e":"code","3b8a002e":"code","d5dd0e21":"code","d4f8549a":"code","e319461a":"code","e787e34b":"code","b0f764ed":"code","efbaf9d9":"code","bc22a970":"code","79c60430":"code","0e684b6e":"code","54cf6f2a":"code","06732d00":"code","2e31220c":"code","fe8010f4":"code","7bb32eb4":"code","bd5e573f":"code","f7b76c6a":"code","9bd47a16":"code","943b9c0d":"code","8f8c45e8":"code","b754fe3b":"code","11c2742e":"code","dfd76e54":"code","623b73d9":"code","5208d547":"code","27885069":"code","31d1da82":"code","c4c99ded":"code","8923d4bb":"code","a0988e3d":"code","d5901607":"code","f32bd8b3":"markdown","c407fe5e":"markdown","ae320d43":"markdown","5f4f64a4":"markdown","8a770396":"markdown","da073f5c":"markdown","b1662d2a":"markdown","d1b23737":"markdown","2a4c4498":"markdown","f817cade":"markdown","1c04761b":"markdown","2d7c8224":"markdown","c79e33cb":"markdown","db855e3d":"markdown","bf762b69":"markdown","8637f6e3":"markdown","ff906043":"markdown","60eb94ba":"markdown","a9b843a1":"markdown","0f440994":"markdown","3b5ac3ce":"markdown","b655b403":"markdown","925409f2":"markdown","ba8fb07f":"markdown","511f7cf6":"markdown","2258cec1":"markdown","ee44b75e":"markdown","55d3728d":"markdown","f40c301c":"markdown","542c8b86":"markdown","1a30a278":"markdown","862d167c":"markdown","84585abd":"markdown","598561e7":"markdown","6677a806":"markdown","227e506f":"markdown","cf85a0bb":"markdown","8e8290f0":"markdown","190d5007":"markdown","b2435899":"markdown","8db2223b":"markdown","6f31ada3":"markdown","07a96f15":"markdown","8d3cc835":"markdown","648d887f":"markdown","3cf124d6":"markdown","d357e170":"markdown","2001c385":"markdown","f8397781":"markdown","14b156ab":"markdown","40e8bf43":"markdown","a7e07b98":"markdown","4f4b0b63":"markdown","7b968917":"markdown","1b22c178":"markdown","80ac324f":"markdown","b65d1829":"markdown","06079506":"markdown","09f05bcd":"markdown","a3c6864f":"markdown","4d50e62d":"markdown","46cff52c":"markdown","c2429e1d":"markdown","a6aa57d3":"markdown","d86c5c8b":"markdown","29f13dc2":"markdown","938e9a34":"markdown","30a8fd3d":"markdown","1f03265e":"markdown","65a7a046":"markdown","37ce831b":"markdown","9ef3c013":"markdown","16e86ec3":"markdown","8f95f364":"markdown","bf3aa092":"markdown","75eb4037":"markdown","a517a880":"markdown","faf3a12c":"markdown","c1c3fcb0":"markdown","d44748ad":"markdown","ea7a1a35":"markdown","a53f5300":"markdown","e77a7b64":"markdown"},"source":{"ee7e29df":"%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n!pip install wordcloud\n!pip install transformers==3.0.2\n!pip install simpletransformers\n!pip install sklearn\n!pip install nltk\n!pip install unidecode\n!pip install normalise\n!pip install contractions\nimport os\nimport string\nimport re\nimport sys\nsys.path.insert(1, '\/kaggle\/input\/pymodules4\/') # link modules to be accessible from this Kaggle kernel\nos.system('python3 -m spacy download en')# it doesnt work when running directly on terminal\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport spacy\nimport nltk\nimport unidecode\nfrom normalise import normalise\nimport contractions\nfrom nltk.corpus import stopwords\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport dill\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils.multiclass import unique_labels\nimport logging\nfrom simpletransformers.classification import ClassificationModel\nfrom spacy_text_classifier import SpacyClassifier\ntrain_df = pd.read_csv(\"\/kaggle\/input\/data-baby2\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/data-baby2\/test.csv\")\nmsk = np.random.rand(len(train_df)) < 0.3\ndev_df = train_df[msk]\ntrain_df = train_df[~msk]\ndev_df.reset_index(inplace=True)\ntrain_df.reset_index(inplace=True)","a3468dd1":"train_df[train_df[\"target\"] == 0][\"text\"].values[1]","c583f6e2":"train_df[train_df[\"target\"] == 1][\"text\"].values[1]","309b6595":"train_df.head()","723cf5e0":"train_df.describe()","c3f68adb":"train_df.info()","1e3ae2e0":"test_df.info()","317cb6ed":"train_has_keyword_df = train_df[train_df.keyword.notnull()]\ntrain_has_location_df = train_df[train_df.location.notnull()]\ntrain_has_keyword_df.head()","e69583c6":"text = \" \".join(keyword for keyword in train_has_keyword_df.keyword)\nprint (\"There are {} words in the combination of all keywords.\".format(len(text)))","644c6917":"import seaborn as sns\nax = sns.countplot(train_df['target'])","634de665":"def renderWordcloud(text):\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud().generate(text)\n\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    F = plt.gcf()\n    Size = F.get_size_inches()\n    F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\n    plt.show()","980c0492":"text = \" \".join(keyword for keyword in train_has_keyword_df.keyword)\nrenderWordcloud(text)","14cf6c48":"text = \" \".join(loc for loc in train_has_location_df.location)\nrenderWordcloud(text)","08ea323b":"len(train_has_keyword_df.keyword.unique()), len(train_has_location_df.location.unique())","1b23c5a4":"group_keyword_sum_target = train_has_keyword_df.groupby(\"keyword\").sum().sort_values(\"target\")\ngroup_keyword_len = train_has_keyword_df.groupby(\"keyword\").count()\ngroup_keyword_sum_target#['true\/all'] = group_keyword_sum_target.target \/ group_keyword_len.target","9fcdbc9d":"pd.set_option('display.max_rows', None)\ngroup_keyword_sum_target","7e8b53e4":"df_target_1 = train_has_keyword_df['target']==1\ndf_target_1.head()","7d7a7115":"pd.crosstab(train_has_keyword_df['keyword'], [df_target_1], rownames=['keyword'], colnames=['target'])","f4916ac0":"group_location_sum_target = train_has_location_df.groupby(\"location\").sum().sort_values(\"target\")\ngroup_location_len = train_has_location_df.groupby(\"location\").count()\ngroup_location_sum_target['true\/all'] = group_location_sum_target.target \/ group_location_len.target","0395834a":"group_location_sum_target.sort_values(by=['target'], ascending=False)[:50]","481d3c64":"pd.set_option('display.max_rows', 20)","6a594f16":"import re\npattern = re.compile(\"[0-9]\")\nnumbers_df = train_df[train_df['text'].str.contains('[0-9]', regex= True, na=False)]\nnumber_texts = [keyword if pattern.search(keyword) else None for keyword in numbers_df.text]\ntext = \" \".join(number_texts)\nrenderWordcloud(text)\n\n\n","fc8e55c7":"count_vectorizer = feature_extraction.text.CountVectorizer()\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])","a926fd0d":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","4785af0b":"example_train_vectors[4]","a6e32fe0":"train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ndev_vectors = count_vectorizer.transform(dev_df[\"text\"])","84b51102":"## Our vectors are really big, so we want to push our model's weights\n## toward 0 without completely discounting different words - ridge regression \n## is a good way to do this.\nclf = linear_model.RidgeClassifier()","205ddf3d":"%%time\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n","3b2b6274":"count_vectorizer = feature_extraction.text.CountVectorizer()\ntrain_replaced_na_keyword = train_df.copy()\ntrain_replaced_na_keyword['keyword'] = train_df['keyword'].fillna(' ')\ntrain_vectors = count_vectorizer.fit_transform(train_replaced_na_keyword[\"keyword\"])\n\ndev_replaced_na_keyword = dev_df.copy()\ndev_replaced_na_keyword['keyword'] = dev_df['keyword'].fillna(' ')\ndev_vectors = count_vectorizer.transform(dev_replaced_na_keyword[\"keyword\"])\ntrain_replaced_na_keyword","52dad6af":"train_vectors[1].todense()","d6c033a5":"clf = linear_model.RidgeClassifier()\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n","757af34a":"count_vectorizer = feature_extraction.text.CountVectorizer()\ntrain_textandkeyword_df = train_df.copy()\ntrain_textandkeyword_df['keyword'] = train_df['keyword'].fillna(' ')\ntrain_textandkeyword_df['textandkeyword'] = train_textandkeyword_df['text'] + \" \/\/ \" + train_textandkeyword_df['keyword']\ntrain_vectors = count_vectorizer.fit_transform(train_textandkeyword_df[\"textandkeyword\"])\n\n\n\ndev_textandkeyword_df = dev_df.copy()\ndev_textandkeyword_df['keyword'] = dev_df['keyword'].fillna(' ')\ndev_textandkeyword_df['textandkeyword'] = dev_textandkeyword_df['text'] + \" \/\/ \" + dev_textandkeyword_df['keyword']\ndev_vectors = count_vectorizer.transform(dev_textandkeyword_df[\"textandkeyword\"])\n\n","52a2adf9":"clf = linear_model.RidgeClassifier()\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n\n","0e6439e4":"clf = linear_model.RidgeClassifier()\ntrain_vectors = count_vectorizer.fit_transform(train_textandkeyword_df[\"text\"])\ndev_vectors = count_vectorizer.transform(dev_textandkeyword_df[\"text\"])\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n\n","a869f73b":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words='english')\nclf = linear_model.RidgeClassifier()\ntrain_vectors = vectorizer.fit_transform(train_df[\"text\"])\ndev_vectors = vectorizer.transform(dev_df[\"text\"])\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n\n\n","32c1b656":"\nvectorizer = TfidfVectorizer()\nclf = linear_model.RidgeClassifier()\ntrain_vectors = vectorizer.fit_transform(train_df[\"text\"])\ndev_vectors = vectorizer.transform(dev_df[\"text\"])\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n","2a5867bc":"clf = linear_model.RidgeClassifier()\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n\n\n","64a57612":"from sklearn.base import BaseEstimator\nclass ClfSwitcher(BaseEstimator):\n    def __init__(self, estimator = linear_model.RidgeClassifier()):\n        \"\"\"\n        A Custom BaseEstimator that can switch between classifiers.\n        :param estimator: sklearn object - The classifier\n        \"\"\" \n        self.estimator = estimator\n\n\n    def fit(self, X, y=None, **kwargs):\n        self.estimator.fit(X, y)\n        return self\n\n\n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n\n\n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)\n\n\n    def score(self, X, y):\n        return self.estimator.score(X, y)","41306b3d":"%%time\n\n\n\n\nclf = linear_model.RidgeClassifier()\npipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')), ('clf', ClfSwitcher())])\n\n\n\nparameters = [\n    {\n        'clf__estimator': [SVC()],\n    },\n    {\n        'clf__estimator': [SGDClassifier()],\n    },\n    {\n        'clf__estimator': [MultinomialNB()],\n    },\n    {\n        'clf__estimator': [linear_model.RidgeClassifier()],\n    },\n    {\n        'clf__estimator': [MLPClassifier(random_state=1, max_iter=200, early_stopping=True)],\n    },\n    {\n        'clf__estimator': [RandomForestClassifier()]\n    }\n]\n\n\nos.write(1, b\"Starting grid search of models\\n\")\ngscv = GridSearchCV(pipeline, parameters, cv=3, n_jobs=12, return_train_score=True, verbose=3, scoring='f1')\ngscv.fit(train_df[\"text\"], train_df[\"target\"])","71b0d2a6":"df = pd.DataFrame(gscv.cv_results_)\ndf","42331c4c":"%%time\nclf = MultinomialNB()\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n\n\n\n","eab5313d":"%%time\nclf = MLPClassifier(random_state=1, max_iter=200, early_stopping=True)\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n\n","3d5d85d3":"pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])\n\n\n\nparameters = [\n    {\n        'clf__alpha': [1, 0, 0.5],\n        'clf__fit_prior': [True, False]\n    },\n]\n\nos.write(1, b\"Starting grid search of alpha & fit_prior\\n\")\ngscv = GridSearchCV(pipeline, parameters, cv=3, n_jobs=12, return_train_score=True, verbose=3, scoring='f1')\ngscv.fit(train_df[\"text\"], train_df[\"target\"])","14950a99":"df = pd.DataFrame(gscv.cv_results_)\ndf","7ccb6640":"clf = MultinomialNB(fit_prior=False)\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","fa66e4d7":"%%time\n!pip install gensim\n\n\nfrom typing import Callable, List, Optional, Tuple\n\nimport pandas as pd\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport gensim\nfrom nltk import ngrams\nfrom gensim.models.keyedvectors import FastTextKeyedVectors\nimport random\n#api = gensim.downloader\n#api.BASE_DIR = \".\"\n#fastText_model = api.load(\"fasttext-wiki-news-subwords-300\")  \nfrom gensim.models import FastText\nfastText_model = FastText.load('\/kaggle\/input\/english-wikipedia-articles-20170820-models\/enwiki_2017_08_20_fasttext.model')\n\ndef randvec(w, n=50, lower=-1.0, upper=1.0):\n    \"\"\"Returns a random vector of length `n`. `w` is ignored.\"\"\"\n    return np.array([random.uniform(lower, upper) for i in range(n)])\n\ndef get_oov_fasttext(w):\n    twograms = ngrams(w, 2)\n    vectors = []\n    for gram in twograms:\n        word_2gram = gram[0] + gram[1]\n        if word_2gram in fastText_model:\n            vectors.append(fastText_model[word_2gram])\n    if(len(vectors) > 0):\n        return np.sum(vectors, axis=0)\n    else:\n        return randvec(w, n=300)\ndef fasttext_vec(w):    \n    \"\"\"Return `w`'s fastext representation if available, else return \n    a random vector.\"\"\"\n    if(w in fastText_model):\n        return fastText_model[w]\n    else:\n        return get_oov_fasttext(w)\n    \nclass FastTextTransformer(BaseEstimator, TransformerMixin):\n    def __init__( self, combine_strategy=\"concatenate\", max_sentence_length=30):\n        assert (combine_strategy==\"concatenate\" or combine_strategy=='mean')\n        self.combine_strategy = combine_strategy\n        self.max_sentence_length = max_sentence_length\n        self.empty_word_token = \"EOF\"\n        \n    def transform(self, text_list):\n        texts = text_list.tolist()\n        result = [];\n        for text in texts:\n            vectors = [];\n            words = text.split()\n            if(self.combine_strategy == 'concatenate'):\n                max_index = self.max_sentence_length\n            else:\n                max_index = len(words) \n            for index in range(max_index):\n                if(len(words) > index):\n                    word = words[index]\n                else:\n                    word = self.empty_word_token\n                vectors.append(fasttext_vec(word))\n            if(self.combine_strategy == 'concatenate'):\n                result.append(np.concatenate(vectors))\n            elif(self.combine_strategy == 'mean'):\n                result.append(np.mean(vectors, axis=0))\n        return result;\n\n    def fit(self, X, y=None):\n        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n        return self","8743c1a1":"os.write(1, b\"Starting fasttext experiments\\n\")\nclf = SGDClassifier()\npipe = Pipeline([('vectorizer', FastTextTransformer(combine_strategy='concatenate')), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","63ce132d":"clf = SGDClassifier()\npipe = Pipeline([('vectorizer', FastTextTransformer(combine_strategy='mean')), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","64252313":"clf = SGDClassifier()\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","4a021420":"# normalise has several nltk data dependencies. Install these by running the following python commands:\n\nimport nltk\nfor dependency in (\"brown\", \"names\", \"wordnet\", \"averaged_perceptron_tagger\", \"universal_tagset\"):\n    nltk.download(dependency)","a48197f8":"nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\nnltk.download('stopwords')\nstops = stopwords.words(\"english\")\n\n\n\ndef remove_accented_chars(text):\n    \"\"\"remove accented characters from text, e.g. caf\u00e9\"\"\"\n    text = unidecode.unidecode(text)\n    return text\n\ndef expand_contractions(text):\n    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n    return contractions.fix(text);\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',str(text))\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n\ndef remove_punctuation(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n#FIXME: for lemmatizing, normalizing and removing stop words we tokenize the text and then join all the tokens using Python String.join, when doing this info like some punctuation marks are dropped which reduces model's accuracy\n#If you find a way to over come this problem please let me know in the comments!\ndef text_normalizer(comment, lemmatize, lowercase, remove_stopwords, remove_accents, normalize_contractions, normalize_URL, normalize_emoji, normalize_html, normalize_punctuation):\n    if lowercase:\n        comment = comment.lower()\n    if(remove_accents):\n        comment = remove_accented_chars(comment)\n    if(normalize_contractions):\n        comment = expand_contractions(comment)\n    if(normalize_URL):\n        comment = remove_URL(comment)\n    if(normalize_emoji):\n        comment = remove_emoji(comment)   \n    if(normalize_html):\n        comment = remove_html(comment)   \n    if(normalize_punctuation):\n        comment = remove_punctuation(comment)   \n    if(remove_stopwords):\n        comment = nlp(comment)\n        words = [];\n        for token in comment:\n            if not remove_stopwords or (remove_stopwords and token.text not in stops):\n                    words.append(token.text)\n        comment = \" \".join(words)\n    if(lemmatize):\n        comment = nlp(comment)\n        comment = \" \".join(word.lemma_.strip() for word in comment)\n    return comment\n\n\nclass PrePropTextTransformer(BaseEstimator, TransformerMixin):\n    def __init__( self, lemmatize=False, lowercase=False, remove_stopwords=False, remove_accents=False, normalize_contractions=False, normalize_URL=False, normalize_emoji=False, normalize_html=False, normalize_punctuation=False):\n        self.lemmatize=lemmatize\n        self.lowercase=lowercase\n        self.remove_stopwords=remove_stopwords\n        self.remove_accents=remove_accents\n        self.normalize_contractions=normalize_contractions\n        self.normalize_URL=normalize_URL\n        self.normalize_emoji=normalize_emoji\n        self.normalize_html=normalize_html\n        self.normalize_punctuation=normalize_punctuation\n        \n    def transform(self, text_list):\n        texts = text_list.tolist()\n        result = [];\n        for text in texts:\n            result.append(text_normalizer(text, self.lemmatize, self.lowercase, self.remove_stopwords, self.remove_accents, self.normalize_contractions, self.normalize_URL, self.normalize_emoji, self.normalize_html, self.normalize_punctuation))\n        return pd.Series(result)\n\n    def fit(self, X, y=None):\n        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n        return self\n\n","da3df5f9":"%%time\nclf = MultinomialNB(fit_prior=False)\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","86670ca3":"%%time\nos.write(1, b\"Starting experimetns w text preprop\\n\")\nclf = MultinomialNB()\npipe = Pipeline([('preprop', PrePropTextTransformer()), ('vectorizer', TfidfVectorizer()), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n\n","921e8c30":"%%time\nclf = MultinomialNB(fit_prior=False)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=True,\n                                                    lowercase=True,\n                                                    remove_stopwords=True,\n                                                    remove_accents=True, \n                                                    normalize_contractions=True,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=True\n                                                   )), ('vectorizer', TfidfVectorizer()), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","496bcecc":" %%time\npipeline = Pipeline([('preprop', PrePropTextTransformer()),\n                     ('vectorizer', TfidfVectorizer()),\n                     ('predictor', MultinomialNB(fit_prior=False))])\n\n\n\nparameters = [\n        {'preprop__lemmatize': [True, False]},\n        {'preprop__lowercase': [True, False]},\n        {'preprop__remove_stopwords': [True, False]},\n        {'preprop__remove_accents': [True, False]},\n        {'preprop__normalize_contractions': [True, False]},\n        {'preprop__normalize_URL': [True, False]},\n        {'preprop__normalize_emoji': [True, False]},\n        {'preprop__normalize_html': [True, False]},\n        {'preprop__normalize_punctuation': [True, False]}\n    ]\n\ntry:\n    grid_search_pd = pd.read_csv(\"\/kaggle\/input\/precomputedgridsearches2\/text_preprop_gs_results.csv\")\nexcept:\n    # FIXME: n_jobs has to be 1 or it crashes\n    gscv = GridSearchCV(pipeline, parameters, cv=3, n_jobs=1, return_train_score=True, verbose=3, scoring='f1')\n    gscv.fit(train_df[\"text\"], train_df[\"target\"])\n    grid_search_pd = pd.DataFrame(gscv.cv_results_);\n    grid_search_pd.to_csv(\"\/kaggle\/input\/precomputedgridsearches\/text_preprop_gs_results.csv\")\ngrid_search_pd\n","5f21776a":"%%time\nclf = MultinomialNB(fit_prior=False)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', TfidfVectorizer()), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","33ed3b5f":"%%time\nfrom nltk.tokenize import TweetTokenizer\nclf = MultinomialNB(fit_prior=False)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', TfidfVectorizer(tokenizer=TweetTokenizer().tokenize)), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","b15ee30a":"from typing import Callable, List, Optional, Tuple\n\nimport pandas as pd\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom spacy.util import minibatch\nimport torch\n!pip install transformers\nfrom transformers import BertModel, BertTokenizer\n\n\n\ndef mean_across_all_tokens(hidden_states):\n    return torch.mean(hidden_states[-1], dim=1)\n\ndef sum_all_tokens(hidden_states):\n    return torch.sum(hidden_states[-1], dim=1)\n\ndef concat_all_tokens(hidden_states):\n    batch_size, max_tokens, emb_dim = hidden_states[-1].shape\n    return torch.reshape(hidden_states[-1], (batch_size, max_tokens * emb_dim))\n\ndef CLS_token_embedding(hidden_states):\n    return hidden_states[-1][:, 0, :]\n\nclass BertTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n            self,\n            max_length: int = 60,\n            tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n            model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True),\n            embedding_func = mean_across_all_tokens,\n            combine_sentence_tokens=True\n    ):\n        self.tokenizer = tokenizer;\n        self.combine_sentence_tokens = combine_sentence_tokens;\n        self.embedding_func = embedding_func;\n        self.model = model\n        self.model.eval()\n        self.max_length = max_length\n\n    def _tokenize(self, text_list: List[str]) -> Tuple[torch.tensor, torch.tensor]:\n        # Tokenize the text with the provided tokenizer\n        input_ids = self.tokenizer.batch_encode_plus(text_list,\n                                                    add_special_tokens=True,\n                                                    max_length=self.max_length,\n                                                    pad_to_max_length=True\n                                                    )[\"input_ids\"]\n\n        return torch.LongTensor(input_ids)\n         \n\n    def _tokenize_and_predict(self, text_list: List[str]) -> torch.tensor:\n        input_ids_tensor = self._tokenize(text_list)\n        out = self.model(input_ids=input_ids_tensor)\n        hidden_states = out[2]\n        if(self.combine_sentence_tokens):\n            return self.embedding_func(hidden_states)\n        else:\n            return hidden_states[-1]\n    \n    def transform(self, text_list: List[str], batch_size=32):\n        if isinstance(text_list, pd.Series):\n            text_list = text_list.tolist()\n        batches = minibatch(text_list, size=batch_size)\n        predictions = []\n        for batch in batches:\n            with torch.no_grad():\n                 batch_predictions = self._tokenize_and_predict(batch)\n            predictions.append(batch_predictions)\n        return torch.cat(predictions, dim=0)\n\n    def fit(self, X, y=None):\n        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n        return self","298a9171":"bertTransformer = BertTransformer(combine_sentence_tokens=False)\nbertTransformer.transform([\"pablo\", \"I love Pablo\"]).shape","c895168e":"%%time\nos.write(1, b\"Starting experiments with BERT\\n\")\nclf = linear_model.RidgeClassifier()\npipe = Pipeline([('vectorizer', BertTransformer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","3b8a002e":"%%time\nclf = linear_model.RidgeClassifier()\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer()), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","d5dd0e21":"%%time\nclf = MLPClassifier(random_state=1, max_iter=200, early_stopping=True)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer()), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","d4f8549a":"%%time\nclf = MLPClassifier(random_state=1, max_iter=100, early_stopping=True)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer()), ('clf', clf)])\n\nparameters = [\n        {'clf__hidden_layer_sizes': [(100, 100), (100,), (100, 50), (50, 50)]}\n    ]\n\ntry:\n    grid_search_pd = pd.read_csv(\"\/kaggle\/input\/precomputedgridsearches2\/mlp_clf_gs_results.csv\")\nexcept:\n    # FIXME: n_jobs has to be 1 or it crashes\n    gscv = GridSearchCV(pipe, parameters, cv=2, n_jobs=1, return_train_score=True, verbose=3, scoring='f1')\n    gscv.fit(train_df[\"text\"], train_df[\"target\"])\n    grid_search_pd = pd.DataFrame(gscv.cv_results_);\n    grid_search_pd.to_csv(\"\/kaggle\/input\/precomputedgridsearches2\/mlp_clf_gs_results.csv\")\ngrid_search_pd\n","e319461a":"%%time\nclf = MLPClassifier(random_state=1, max_iter=100, early_stopping=True)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer()), ('clf', clf)])\n\nparameters = [\n        {'clf__hidden_layer_sizes': [(100, 50, 50), (100,100, 50)]}\n    ]\n\ntry:\n    grid_search_pd = pd.read_csv(\"\/kaggle\/input\/precomputedgridsearches2\/mlp_clf2_gs_results.csv\")\nexcept:\n    # FIXME: n_jobs has to be 1 or it crashes\n    gscv = GridSearchCV(pipe, parameters, cv=2, n_jobs=1, return_train_score=True, verbose=3, scoring='f1')\n    gscv.fit(train_df[\"text\"], train_df[\"target\"])\n    grid_search_pd = pd.DataFrame(gscv.cv_results_);\n    grid_search_pd.to_csv(\"\/kaggle\/input\/precomputedgridsearches2\/mlp_clf2_gs_results.csv\")\ngrid_search_pd\n","e787e34b":"bertTransformer = BertTransformer()\nbertTransformer.transform([\"granola bars\"]).shape","b0f764ed":"%%time\nclf = MLPClassifier(random_state=1, max_iter=100, early_stopping=True)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer()), ('predictor', clf)])\n\n\nparameters = [\n        {'vectorizer__embedding_func': [sum_all_tokens, mean_across_all_tokens, concat_all_tokens, CLS_token_embedding]}\n    ]\n\ntry:\n    grid_search_pd = pd.read_csv(\"\/kaggle\/input\/precomputedgridsearches2\/emb_funcs_gs_results.csv\")\nexcept:\n    # FIXME: n_jobs has to be 1 or it crashes\n    gscv = GridSearchCV(pipe, parameters, cv=2, n_jobs=1, return_train_score=True, verbose=3, scoring='f1')\n    gscv.fit(train_df[\"text\"], train_df[\"target\"])\n    grid_search_pd = pd.DataFrame(gscv.cv_results_);\n    grid_search_pd.to_csv(\"\/kaggle\/input\/precomputedgridsearches2\/emb_funcs_gs_results.csv\")\ngrid_search_pd\n\n","efbaf9d9":"%%time \nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom xgboost import XGBClassifier\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\nclf = XGBClassifier(n_estimators=300)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer(model=model, tokenizer=tokenizer)), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","bc22a970":"%%time\nos.write(1, b\"Starting experiments with sequential models\\n\")\nbertTransformer = BertTransformer(combine_sentence_tokens=False)\nX_train = bertTransformer.transform(train_df[\"text\"])\nX_dev = bertTransformer.transform(dev_df[\"text\"])\ny_train = train_df[\"target\"].tolist()\ny_dev = dev_df[\"target\"].tolist()","79c60430":"%%time\nfrom torch_rnn_classifier_attn import TorchRNNClassifier\ntorch_rnn = TorchRNNClassifier(\n        vocab=[],\n        use_embedding=False,\n        bidirectional=False,\n        hidden_dim=50,\n        max_iter=50,\n        eta=0.05) \n_ = torch_rnn.fit(X_train, y_train)","0e684b6e":"%%time\nfrom sklearn.metrics import classification_report\npredictions = torch_rnn.predict(X_dev)\nprint(classification_report(y_dev, predictions))","54cf6f2a":"%%time\nfrom torch_rnn_classifier import TorchRNNClassifier\n\ntorch_bi_lstm = TorchRNNClassifier(\n        vocab=[],\n        use_embedding=False,\n        bidirectional=True,\n        hidden_dim=50,\n        max_iter=50,\n        eta=0.05) \n_ = torch_bi_lstm.fit(X_train, y_train)","06732d00":"%%time\npredictions = torch_bi_lstm.predict(X_dev)\nprint(classification_report(y_dev, predictions))\n","2e31220c":"%%time\nfrom torch_rnn_classifier_attn import TorchRNNClassifier\ntorch_bilstm_attn_manning2015 = TorchRNNClassifier(\n        vocab=[],\n        use_embedding=False,\n        attention=\"GlobalAttnManning2015\",\n        bidirectional=True,\n        hidden_dim=50,\n        max_iter=30,\n        eta=0.05) \n_ = torch_bilstm_attn_manning2015.fit(X_train, y_train)","fe8010f4":"%%time\npredictions = torch_bilstm_attn_manning2015.predict(X_dev)\nprint(classification_report(y_dev, predictions))\n","7bb32eb4":"%%time\ntorch_bilstm_attn_ShouPeng2016 = TorchRNNClassifier(\n        vocab=[],\n        use_embedding=False,\n        attention=\"AttnShouPeng2016\",\n        bidirectional=True,\n        hidden_dim=50,\n        max_iter=30,\n        eta=0.05) \n_ = torch_bilstm_attn_ShouPeng2016.fit(X_train, y_train)","bd5e573f":"%%time\npredictions = torch_bilstm_attn_ShouPeng2016.predict(X_dev)\nprint(classification_report(y_dev, predictions))\n","f7b76c6a":"class BertClassifierPredictor(BaseEstimator, ClassifierMixin):\n    def __init__(self, model=ClassificationModel('roberta', 'roberta-base', args={\"overwrite_output_dir\": True},\n                                                 use_cuda=False)):\n        self.model = model\n        logging.basicConfig(level=logging.INFO)\n        transformers_logger = logging.getLogger(\"transformers\")\n        transformers_logger.setLevel(logging.WARNING)\n\n\n    def fit(self, X, y):\n        X = X.tolist()\n        y = y.tolist()\n        self.classes_ = unique_labels(y)\n        self.X_ = X\n        self.y_ = y\n        d = {'text': X, 'labels': y}\n        df = pd.DataFrame(data=d)\n        self.model.train_model(df)\n        return self\n\n    def predict(self, X):\n        return self.model.predict(X.tolist())[0]","9bd47a16":"%%time\nos.write(1, b\"Starting experiments w BERT finetuning\\n\")\nmodel = ClassificationModel('bert', '\/kaggle\/input\/simpletransformer-outputs\/checkpoint-bert-epoch-1', args={\"overwrite_output_dir\": True}, use_cuda=False)\n\nclf = BertClassifierPredictor(model=model)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('predictor', clf)])\n\n#model already trained, uncomment below line to continue trasining\n#pipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","943b9c0d":"%%time\nfrom simpletransformers.classification import ClassificationModel\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n# Create a TransformerModel\nmodel = ClassificationModel('roberta', '\/kaggle\/input\/simpletransformer-outputs\/checkpoint-roberta-epoch-1', args={\"overwrite_output_dir\": True}, use_cuda=False)\n\nclf = BertClassifierPredictor(model=model)\npipe_roberta_finetuned = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('predictor', clf)])\n\n#model already trained, uncomment below line to continue trasining\n#pipe_roberta_finetuned.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe_roberta_finetuned.predict(dev_df['text'])\nprint(classification_report(dev_df['target'], predictions))\n","8f8c45e8":"%%time\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\nclf = MLPClassifier(random_state=1, max_iter=200, early_stopping=True)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer(model=model, tokenizer=tokenizer)), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","b754fe3b":"%%time\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\nbertTransformer = BertTransformer(model=model, tokenizer=tokenizer, combine_sentence_tokens=False)\nX_train = bertTransformer.transform(train_df[\"text\"])\nX_dev = bertTransformer.transform(dev_df[\"text\"])\ny_train = train_df[\"target\"].tolist()\ny_dev = dev_df[\"target\"].tolist()\n","11c2742e":"%%time\nfrom torch_rnn_classifier_attn import TorchRNNClassifier\ntorch_bilstm_attention_roberta_exp = TorchRNNClassifier(\n        vocab=[],\n        use_embedding=False,\n        attention=\"AttnShouPeng2016\",\n        bidirectional=True,\n        hidden_dim=50,\n        max_iter=30,\n        eta=0.05) \n_ = torch_bilstm_attention_roberta_exp.fit(X_train, y_train)","dfd76e54":"%%time\npredictions = torch_bilstm_attention_roberta_exp.predict(X_dev)\nprint(classification_report(y_dev, predictions))\n","623b73d9":"%%time\nos.write(1, b\"Starting experiments with spacyclassifier\\n\")\nspacy_classifier = SpacyClassifier(n_iter=50)\n\n\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('predictor', spacy_classifier)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])","5208d547":"%%time\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","27885069":"%%time\nfrom GPT2_classifier import GPT2Classifier\ncheckpoint_path = \"\/kaggle\/input\/models3\/GPT2_CLS_CHECKPOINT_ephoc20.pth.tar\"\ngpt2_classifier = GPT2Classifier(max_iter=20, finetune_GPT2=False, batch_size=32, checkpoint_path=checkpoint_path,\n                 base_dir=\".\", classes=[0,1])\n\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('predictor', gpt2_classifier)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\n\n\n","31d1da82":"%%time\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","c4c99ded":"entire_train_df = pd.concat([train_df, dev_df])","8923d4bb":"%%time\n# retrain best model on  all available data\nmodel = ClassificationModel('roberta', '\/kaggle\/input\/simpletransformer-outputs\/checkpoint-roberta_submission-epoch-2', args={\"overwrite_output_dir\": True, \"num_train_epochs\": 2}, use_cuda=False)\n\nclf = BertClassifierPredictor(model=model)\npipe_roberta_finetuned_submit = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('predictor', clf)])\n#model already trained, uncomment below line to continue trasining\n#pipe_roberta_finetuned_submit.fit(entire_train_df[\"text\"], entire_train_df[\"target\"])\n","a0988e3d":"test_vectors = test_df['text']\nsample_submission = pd.read_csv(\"\/kaggle\/input\/data-baby2\/sample_submission.csv\")\nsample_submission[\"target\"] = pipe_roberta_finetuned_submit.predict(test_vectors)\nsample_submission.head()","d5901607":"sample_submission.to_csv(\"submission.csv\", index=False)","f32bd8b3":"### Does using a tweet tokenizer improve it?","c407fe5e":"- Train word embeddings and LSTMs w attention together, so word vectors get fine tuned, this will definitely yield better results than when we don't finetune the word vectors and only train the LSTM classifier\n- Can we place a NN on top of the LSTM and have it behave as an attention layer? if no why not?\n- Oversample the minority class\n- Use other types of attentions when working with RNNs Manning 2015 suggests a better type called local attention\n- Explore bigger versions of GPT2\n- Explore other ways of generating sentence embeddings(eg: add a CLS token and use its hidden state as sentence representation) using GPT2\n- Finetune GPT2 model, my model only trains the classificaiton layer, tried to also finetune GPT2 and even the current code is supposed to be able to finetune it, but for some reason doesn't work","ae320d43":"### Wordcloud of keywords","5f4f64a4":"compare best two classifier: MultinomialNB & MLPClassifier","8a770396":"## <a id=\"epa\">Exploratory Analysis<\/a>","da073f5c":"Train the final model on all available labelled data, the below df contains the examples used for training and dev sets during the notebooka","b1662d2a":"####  BI-LSTM classifier","d1b23737":"Select top 10 words from every tweet using TF-IDF and feed them to a classifier","2a4c4498":"hyperparameter search for MultinomialNB","f817cade":"Compare same model using TF-IDF rather than fast-text for encoding","1c04761b":"Even though we already tested a similar architecture in this notebook: [BERT embeddings feeded TO A SK-learn feed forward NN](#bert-NN), the below model has an advantage that makes it more promising: both the transformer used to generating the word embeddings and the classification layer weights are adjusted(https:\/\/arxiv.org\/abs\/2004.14448) ","2d7c8224":"### relationship between keyword and target","c79e33cb":"Increasing FFN network layers doesn't make a significant improvement","db855e3d":"Adding keyword to text reduces the score","bf762b69":"### A quick look at our data\n\nLet's look at our data... first, an example of what is NOT a disaster tweet.","8637f6e3":"### <a id=\"lstms\">LSTM classifiers<\/a>","ff906043":"## hyper param search MLP classifier","60eb94ba":"**Note**: Because Kaggle kernels can't run for more than 9 hours, I had to train the biggest models on my local computer and this notebook loads them from a checkpoint. Also for grid search the notebook loads the cached results from csv to save compute time.  \nAll the checkpoints and grid search results loaded in this notebook where generated using solely the code in this notebook.","a9b843a1":"#### Vanilla LSTM","0f440994":"Frozen(not finetuned during training) distill-GPT2 language model used to generate word embeddings + feed forward classification layer on top","3b5ac3ce":"## Roberta word vectors(not finetuned) + feed forward NN","b655b403":"### use only text but only over tweets that have keywords","925409f2":"## XGBoost model","ba8fb07f":"## Spacy classifier(Architecture=Enseble)","511f7cf6":"text preprocessing, BERT encoding, RidgeClassifier classifier","2258cec1":"# <a id=\"bert_e2e\">BERT (finetuned)<\/a>","ee44b75e":"So far we've combined the word embeddings in some way and trained feed-fordward, shadow models, let's see what happens when our word embeddings are evaluated using a sequential model!","55d3728d":"### Finetune BERT models for classification task","f40c301c":"## Unique words","542c8b86":"## Relationship between categorical vars & target","1a30a278":"### use preprocess_text to try improve baseline","862d167c":"# Feature engineering","84585abd":"#### Use Mean function for combining word vectors","598561e7":"### Building vectors\n\nThe theory behind this model is pretty simple: the words contained in each tweet are a good indicator of whether they're about a real disaster or not (this is not entirely correct, but it's a great place to start).\n\nWe'll use scikit-learn's `CountVectorizer` to count the words in each tweet and turn them into data our machine learning model can process.\n\nNote: a `vector` is, in this context, a set of numbers that a machine learning model can work with. We'll look at one in just a second.","6677a806":"#### Manning 2015 Global Attention ","227e506f":"# Index","cf85a0bb":"## Roberta word vectors used in best model so far (biLSTM with attention)","8e8290f0":"# More advanced Vector representation techniques","190d5007":"# <a id=\"text_preprop\">pre-processing techniques<\/a>","b2435899":"# Submit best model predictions to Kaggle","8db2223b":"## <a id=\"fast_text\">Fast-text<\/a>","6f31ada3":"# From shallow learning to 2020 SOTA(GPT-2, ROBERTA) ","07a96f15":"* The above scores aren't terrible! It looks like our assumption will score roughly 0.8 on the leaderboard. There are lots of ways to potentially improve on this (TFIDF, LSA, LSTM \/ RNNs, the list is long!) - give any of them a shot!\n","8d3cc835":"### BERT compare different ways of generating sentence embeddings for classification: concatenate VS avg VS [CLS] token embedding","648d887f":"#### contigency table","3cf124d6":"# Follow up work","d357e170":"From comparing all the models tested on this notebook I observe that the best performing model for the task of tweet classification is: [Roberta classifier(Finetuned)](#winner)\n\nOther interesting findings:\n- using TF-IDF vectors + shallow learning(eg: ridge classifier) yields results that are hard to beat by most deep learning models [Link](#best_shallow)\n- Adding attention to an LSTM classifier increases its accuracy noticeably, particularly when using shoug peng 2016 attention [Link](#shou_peng_attn)","2001c385":"# Models","f8397781":"In this notebook I explore a variety of Machine learning models ranging from good old shallow learning(Naive naives, TF-IDF, SVMs) to the state of the art in NLP(GPT2, ROBERTA) with the goal of finding the best possible model and preprocessing steps for the task of tweet classification posted on this Kaggle competition: https:\/\/www.kaggle.com\/c\/nlp-getting-started  \nAs a by-product of this experimentation we also obtain a clear comparison across a number of popular NLP algorithms.","14b156ab":"### relationship between location and target: percentage of true targets per location","40e8bf43":"## best result so far using text preprocessing:","a7e07b98":"### Our model\n\nAs we mentioned above, we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n\nWhat we're assuming here is a _linear_ connection. So let's build a linear model and see!","4f4b0b63":"### correlation between keyword and Target","7b968917":"### use keywords for prediction","1b22c178":"> # <a id=\"Conclusions\">Conclusions<\/a>","80ac324f":"### <a id=\"best_shallow\">Best model so far: ridge classifier using TF-IDF for text encoding<\/a>","b65d1829":"### BERT","06079506":"<a id='bert-NN'><\/a>\nBERT embeddings feeded to feed forward NN","09f05bcd":"# <a id=\"shallow_learning\">Evaluate multiple classifier models<\/a>","a3c6864f":"## wordclouds","4d50e62d":"Increased training ephocs from 1 to 2 for submission","46cff52c":"I'm interested in this cause to see wether numbers hold a correlation to words\/meaning of the tweet","c2429e1d":"#  Vector representations","a6aa57d3":"BERT model with an added single linear layer on top for classification. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.","d86c5c8b":"Encoding text with TF-IDF vectorizer proves better than fast-text, which makes sense because TF-IDF helps our model pay attention to important words, while fast-text doesn't. This means that a model that has attention mechanism may deliver promising results, later in this notebook I'll experiment with one.","29f13dc2":"From the doc(https:\/\/spacy.io\/api\/textcategorizer#architectures):  \nStacked ensemble of a bag-of-words model and a neural network model. The neural network uses a CNN with mean pooling and attention","938e9a34":"# Abstract","30a8fd3d":"reduce num of ephocs and set cv=2 to reduce training time","1f03265e":"- [Exploratory analysis](#epa)\n- [Shallow Learning](#shallow_learning)\n- [Fast text](#fast_text)\n- [Text preprocessing](#text_preprop)\n- [BERT & ROBERTA](#bert_e2e)\n- [LSTMs](#Conclusions)\n- [GPT2](#gpt2)\n- [Conclusions](#Conclusions)","65a7a046":"not removing stop words is better:","37ce831b":"# Sequential models","9ef3c013":"The above tells us that:\n1. There are 54 unique words (or \"tokens\") in the first five tweets.\n2. The first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet.\n\nNow let's create vectors for all of our tweets.","16e86ec3":"### Wordcloud of locations","8f95f364":"#### BI-LSTM with attention","bf3aa092":"# More advanced word embeddings","75eb4037":"### <a id=\"winner\">Roberta(Finetuned)<\/a>","a517a880":"#### baseline:","faf3a12c":"#### <a id=\"shou_peng_attn\">Shou, Peng, et al. 2016 Attention<\/a>","c1c3fcb0":"# <a id=\"gpt2\">GPT-2<\/a>","d44748ad":"### How are numbers formatted in tweets? Wordcloud of words from tweets that contain numbers","ea7a1a35":"### Use TF-IDF to highlight important words in the text","a53f5300":"### use tweet text in combination with keyword","e77a7b64":"Let's test our model and see how well it does on the training data. For this we'll use `cross-validation` - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n\nThe metric for this competition is F1, so let's use that here."}}