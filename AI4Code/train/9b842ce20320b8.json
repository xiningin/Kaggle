{"cell_type":{"2d9a1e8a":"code","f110b395":"code","7ca23779":"code","b7524259":"code","d5728900":"code","b13acfe6":"code","bc3ee4b5":"code","79364280":"code","537b5794":"code","3c8a635e":"code","84d33030":"code","4538fe7c":"code","84b9c79a":"code","6ff3b9ed":"code","b1c0a1a9":"code","509ab36f":"code","de834266":"code","c5ee2d88":"code","d92603e5":"code","af839d37":"code","da2082fb":"code","cfe468d8":"code","de58daaa":"code","0977d5c5":"code","7686541d":"code","123b8a89":"code","58af8369":"code","68ce00d3":"code","1d89595a":"code","a9e4e821":"code","3ab169dd":"code","d80a080d":"code","e019e6c7":"code","2afaa953":"code","b49d9449":"code","f69c7676":"code","9676ec29":"code","a4616abe":"code","4500e720":"code","25c38fea":"code","b034bdd9":"code","ab8952f3":"code","2f504854":"code","9a1a022b":"code","3f3edba2":"code","544b306c":"code","11254965":"code","08a15489":"code","d1f0f941":"code","d03c8888":"code","2f6fbd4b":"code","04308a38":"code","feeb828a":"code","0cbbfc4f":"code","9320880e":"markdown"},"source":{"2d9a1e8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f110b395":"df = pd.read_csv('..\/input\/wikihow-summarization\/wikihowAll.csv', delimiter = ',')\ndf.head()","7ca23779":"import numpy as np  \nimport pandas as pd \n\nimport re\nimport nltk\n\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords   \nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n\n#embedding : we use an embedding layer to compress the input feature space into a smaller one.\n#The Embedding layer is used to create word vectors for incoming words.\n#A dense layer is just a regular layer of neurons in a neural network.\n\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm, tqdm_notebook\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\n#import warnings\n#pd.set_option(\"display.max_colwidth\", 200)\n#warnings.filterwarnings(\"ignore\")","b7524259":"df.shape","d5728900":"DF = df.dropna()","b13acfe6":"import nltk\nnltk.download('stopwords')\nSTOP_WORDS = set(stopwords.words('english')) ","bc3ee4b5":"DF.shape","79364280":"DF.head()","537b5794":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","3c8a635e":"DF.shape","84d33030":"\ndef CLEAN_TEXT(text):\n\n    TEXT = text.lower()\n    TEXT = re.sub(r'\\([^)]*\\)', '', TEXT)\n    TEXT = re.sub('\"','', TEXT)\n\n    TEXT = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in TEXT.split(\" \")])    \n\n    TEXT = re.sub(r\"'s\\b\",\"\", TEXT)\n    TEXT = re.sub(\"[^a-zA-Z]\", \" \", TEXT) \n    tokens = [w for w in TEXT.split() if not w in STOP_WORDS]\n    #tokens = TEXT.split()\n\n    long_words = []\n    \n    for i in tokens:\n        if len(i) >= 3:                \n            long_words.append(i)   \n    return (\" \".join(long_words)).strip()\n    \n    #return TEXT\n\ncleaned_text = []\n\nfor t in DF['text']:\n    cleaned_text.append(CLEAN_TEXT(t))","4538fe7c":"def CLEAN_SUMMARY(text):\n\n    TEXT = re.sub('\"','', text)\n\n    TEXT = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in TEXT.split(\" \")])  \n\n    TEXT = re.sub(r\"'s\\b\",\"\",TEXT)\n    TEXT = re.sub(\"[^a-zA-Z]\", \" \", TEXT)\n    TEXT = TEXT.lower()\n    #return TEXT\n    tokens = TEXT.split()\n\n    corpus = ''\n\n    for i in tokens:\n        if len(i)>1:                                 \n            corpus = corpus + i + ' '  \n    return corpus\n\ncleaned_summary = []\n\nfor t in DF['headline']:\n    cleaned_summary.append(CLEAN_SUMMARY(t))\n\nDF['cleaned_text'] = cleaned_text\nDF['cleaned_summary'] = cleaned_summary\nDF['cleaned_summary'].replace('', np.nan, inplace = True)\n\nDF.dropna(axis = 0, inplace = True)","84b9c79a":"for i in range(5):\n    print(\"TEXT : \",DF['cleaned_text'][i])\n    print(\"SUMMARY : \",DF['cleaned_summary'][i])\n    print(\"\\n\")","6ff3b9ed":"text_word_count = []\nsummary_word_count = []\n\n# populate the lists with sentence lengths\nfor i in DF['cleaned_text']:\n      text_word_count.append(len(i.split()))\n\nfor i in DF['cleaned_summary']:\n      summary_word_count.append(len(i.split()))\n\nlength_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})","b1c0a1a9":"length_df.hist(bins = 40, figsize = (15, 5))\nplt.show()","509ab36f":"max_text_len = 70\nmax_summary_len = 20","de834266":"DF['cleaned_summary'] = DF['cleaned_summary'].apply(lambda x : 'sostok '+ x + ' eostok')","c5ee2d88":"DF['cleaned_summary']","d92603e5":"from sklearn.model_selection import train_test_split\n\nx_tr, x_val, y_tr, y_val = train_test_split(np.array(DF['cleaned_text']), np.array(DF['cleaned_summary']),\n                                            test_size = 0.1,\n                                            random_state = 0,\n                                            shuffle = True)","af839d37":"import pickle\n\nX_TRAIN = open('x_tr_metrics.pickle', 'wb')\npickle.dump(x_tr, X_TRAIN)\nX_TRAIN.close()\n\nY_TRAIN = open('y_tr_metrics.pickle', 'wb')\npickle.dump(y_tr, Y_TRAIN)\nY_TRAIN.close()\n\nY_VAL = open('y_val_metrics.pickle', 'wb')\npickle.dump(y_val, Y_VAL)\nY_VAL.close()\n\nX_VAL = open('x_val_metrics.pickle', 'wb')\npickle.dump(x_val, X_VAL)\nX_VAL.close()","da2082fb":"from keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\n\n#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer() \nx_tokenizer.fit_on_texts(list(x_tr))","cfe468d8":"thresh = 4\n\ncnt = 0\ntot_cnt = 0\nfreq = 0\ntot_freq = 0\n\nfor key,value in x_tokenizer.word_counts.items():\n    tot_cnt = tot_cnt + 1\n    tot_freq = tot_freq + value\n    if(value < thresh):\n        cnt = cnt + 1\n        freq = freq + value\n    \nprint(\"% of rare words in vocabulary:\", (cnt\/tot_cnt) * 100)\nprint(\"Total Coverage of rare words:\", (freq\/tot_freq) * 100)","de58daaa":"x_tokenizer = Tokenizer(num_words = tot_cnt-cnt) \nx_tokenizer.fit_on_texts(list(x_tr))\n\n#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\nx_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \nx_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n\n#padding zero upto maximum length\nx_tr    =   pad_sequences(x_tr_seq,  maxlen = max_text_len, padding = 'post') #pr\u00e9server la dimension des features\nx_val   =   pad_sequences(x_val_seq, maxlen = max_text_len, padding = 'post')\n\n#size of vocabulary ( +1 for padding token)\nx_voc   =  x_tokenizer.num_words + 1\n\nprint(\"Size of vocabulary in X = {}\".format(x_voc))","0977d5c5":"#prepare a tokenizer for reviews on training data\n\ny_tokenizer = Tokenizer()   \ny_tokenizer.fit_on_texts(list(y_tr))","7686541d":"cnt = 0\ntot_cnt = 0\nfreq = 0\ntot_freq = 0\n\nfor key,value in y_tokenizer.word_counts.items():\n    tot_cnt = tot_cnt + 1\n    tot_freq = tot_freq + value\n    \n    if(value < thresh):\n        cnt = cnt + 1\n        freq = freq + value\n    \nprint(\"% of rare words in vocabulary:\",(cnt\/tot_cnt) * 100)\nprint(\"Total Coverage of rare words:\",(freq\/tot_freq) * 100)","123b8a89":"#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \ny_tokenizer.fit_on_texts(list(y_tr))\n\n#convert text sequences into integer sequences (i.e one hot encode the text in Y)\ny_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \ny_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n\n#padding zero upto maximum length\ny_tr    =   pad_sequences(y_tr_seq, maxlen = max_summary_len, padding = 'post')\ny_val   =   pad_sequences(y_val_seq, maxlen = max_summary_len, padding = 'post')\n\n#size of vocabulary\ny_voc  =   y_tokenizer.num_words + 1\nprint(\"Size of vocabulary in Y = {}\".format(y_voc))","58af8369":"ind = []\nfor i in range(len(y_tr)):\n    cnt = 0\n    for j in y_tr[i]:\n        if j != 0:\n            cnt = cnt + 1\n    if(cnt == 2):\n        ind.append(i)\n\ny_tr = np.delete(y_tr, ind, axis = 0)\nx_tr = np.delete(x_tr, ind, axis = 0)","68ce00d3":"ind=[]\nfor i in range(len(y_val)):\n    cnt=0\n    for j in y_val[i]:\n        if j != 0:\n            cnt = cnt + 1\n    if(cnt==2):\n        ind.append(i)\n\ny_val = np.delete(y_val, ind, axis = 0)\nx_val = np.delete(x_val, ind, axis = 0)","1d89595a":"latent_dim = 300\nembedding_dim = 200\n\n# Encoder\nencoder_inputs = Input(shape=(max_text_len,))\n\n#embedding layer\nenc_emb =  Embedding(x_voc, embedding_dim, trainable = True)(encoder_inputs)\n\n#encoder lstm 1\nencoder_lstm1 = LSTM(latent_dim, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout = 0.4)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n\n#encoder lstm 2\nencoder_lstm2 = LSTM(latent_dim,return_sequences = True,return_state = True, dropout = 0.4, recurrent_dropout = 0.4)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n\n#encoder lstm 3\nencoder_lstm3 = LSTM(latent_dim, return_state = True, return_sequences = True, dropout = 0.4, recurrent_dropout = 0.4)\nencoder_output3, state_h3, state_c3 = encoder_lstm3(encoder_output2)\n\n#encoder lstm 4\n#encoder_lstm4 = LSTM(latent_dim, return_state = True, return_sequences = True, dropout = 0.4, recurrent_dropout = 0.4)\n\n#encoder_output4, state_h4, state_c4 = encoder_lstm4(encoder_output3)\n\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output3)\n#encoder_outputs, state_h, state_c= encoder_lstm2(encoder_output1)\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape = (None,))\n\n#embedding layer\ndec_emb_layer = Embedding(y_voc, embedding_dim, trainable = True)\ndec_emb = dec_emb_layer(decoder_inputs)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state = True, dropout = 0.4, recurrent_dropout = 0.2)\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n\n#dense layer\ndecoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model \nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n\nmodel.summary()","a9e4e821":"#from keras.callbacks import ModelCheckPoint","3ab169dd":"model.compile(optimizer = 'rmsprop',\n              loss = 'sparse_categorical_crossentropy')\n\nes = EarlyStopping(monitor = 'val_loss',\n                   mode = 'min',\n                   verbose = 1,\n                   patience = 2)\n\nmc = ModelCheckpoint('ALL_ATTENTION_best_model_with_no_stopwords_METRICS.h5',\n                    monitor = 'val_loss',\n                    mode = 'min',\n                    verbose = 1,\n                    save_best_only = True)\n\nhistory = model.fit([x_tr, y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:,1:],\n                    epochs = 50,\n                    callbacks = [es, mc],\n                    batch_size = 128,\n                    validation_data = ([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:,1:]))","d80a080d":"#haha","e019e6c7":"#de 9 \u00e0 20\n\n#from keras.models import load_model\n#from tensorflow.keras.models import load_model\n\n#new_model = tf.keras.models.load_model('..\/input\/models\/ALL_best_model.h5')\n\n#new_model.summary()\n\n#new_model.compile(optimizer = \"rmsprop\", loss = 'sparse_categorical_crossentropy')\n#es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\n#checkpoint = ModelCheckpoint('ALL_best_model_7_ 50_epochs.h5', monitor = 'val_loss', verbose = 1, save_best_only = True, mode = 'min', period = 1)\n#history = new_model.fit([x_tr, y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:,1:],\n#                    epochs = 50,\n#                    callbacks = [es, checkpoint],\n#                    batch_size = 128,\n#                    validation_data = ([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:,1:]))","2afaa953":"from matplotlib import pyplot\npyplot.figure(figsize = (11,8))\npyplot.plot(history.history['loss'], label = 'train', color = 'black')\npyplot.plot(history.history['val_loss'], label = 'test', color = 'purple')\npyplot.legend()\npyplot.show()\npyplot.savefig('plot.jpeg')","b49d9449":"reverse_target_word_index = y_tokenizer.index_word\nreverse_source_word_index = x_tokenizer.index_word\ntarget_word_index = y_tokenizer.word_index\n\n# Encode the input sequence to get the feature vector\n\nencoder_model = Model(inputs = encoder_inputs, outputs = [encoder_outputs, state_h, state_c])\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\n\ndecoder_state_input_h = Input(shape = (latent_dim,))\ndecoder_state_input_c = Input(shape = (latent_dim,))\ndecoder_hidden_state_input = Input(shape = (max_text_len, latent_dim))\n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs) \n\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = [decoder_state_input_h, decoder_state_input_c])","f69c7676":"# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_outputs2) \n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n    [decoder_outputs2] + [state_h2, state_c2])","9676ec29":"import pickle\n#from tf.keras.model import save_model\n#from tf.keras.models import load_model\n\n#model.save_model('standard_model.h5')\n#m = open('Attention_model.pickle', 'wb')\n#pickle.dump(model, m)\n#m.close()\n\nencoder_model.save('encoder_metrics.h5')\ndecoder_model.save('decoder_metrics.h5')\n\nx = open('x_tokenizer_metrics.pickle', 'wb')\npickle.dump(x_tokenizer, x)\nx.close()\n\ny = open('y_tokenizer_metrics.pickle', 'wb')\npickle.dump(y_tokenizer, y)\ny.close()","a4616abe":"#save that model, honey boo~\nfrom keras.models import save_model","4500e720":"#model = save_model('..\/input\/output\/MODEL.h5')\nmodel.save_weights('ATTENTION_MODEL_WEIGHTS.h5')","25c38fea":"#BROUILLON\n\n#del model  # deletes the existing model\n\n# returns a compiled model\n# identical to the previous one\n#model = load_model('my_model.h5')\n\n#model = load_model('Attention_model.h5')\n#model.save('Attention_model.h5')\n\n#encoder_model.save('Attention_encoder.h5')\n\n#decoder_model.save('Attention_decoder.h5')\n\n#x = open('Attention_x_tokenizer.pickle', 'wb')\n#pickle.dump(x_tokenizer, x)\n#x.close()\n\n#y = open('Attention_y_tokenizer.pickle', 'wb')\n#pickle.dump(y_tokenizer, y)\n#y.close()","b034bdd9":"#m = open('Attention_model.pickle', 'wb')#\n#pickle.dump(model, m)\n#m.close()\n\n#encoder = open('Attention_encoder_model.pickle', 'wb')\n#pickle.dump(encoder_model, encoder)\n#encoder.close()\n\n#decoder = open('Attention_decoder_model.pickle', 'wb')\n#pickle.dump(decoder_model, decoder)\n#decoder.close()\n\n#x = open('Attention_x_tokenizer.pickle', 'wb')\n#pickle.dump(x_tokenizer, x)\n#x.close()\n\n#y = open('Attention_y_tokenizer.pickle', 'wb')\n#pickle.dump(y_tokenizer, y)\n#y.close()\n\n#decode_sequence = open('Attention_decode_sequence.pickle', 'wb')\n#pickle.dump(y_tokenizer, decode_sequence)\n#decode_sequence.close()","ab8952f3":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index['sostok']\n\n    stop_condition = False\n    decoded_sentence = ''\n    \n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='eostok'):\n            decoded_sentence += ' '+sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","2f504854":"def seq2summary(input_seq):\n    newString = ''\n    for i in input_seq:\n        if((i != 0 and i != target_word_index['sostok']) and i != target_word_index['eostok']):\n            newString = newString + reverse_target_word_index[i] + ' '\n    return newString\n\ndef seq2text(input_seq):\n    newString = ''\n    for i in input_seq:\n        if(i != 0):\n            newString = newString + reverse_source_word_index[i]+' '\n    return newString","9a1a022b":"for i in range(0, 100):\n    #print(DF['text'][i])\n    print(\"TEXT : \", seq2text(x_tr[i]))\n    print(\"ORIGINAL SUMMARY :\", seq2summary(y_tr[i]))\n    print(\"PREDICTED SUMMARY :\", decode_sequence(x_tr[i].reshape(1, max_text_len)))\n    print(\"\\n\")","3f3edba2":"DF.shape","544b306c":"#liste des r\u00e9sum\u00e9s de r\u00e9f\u00e9rences\ngoldstandard = []\n#liste des r\u00e9sum\u00e9s g\u00e9n\u00e9r\u00e9s\nsumm = []\n\n#on en met 100 dans 2 listes\nfor i in range(0, 100000):\n    goldstandard.append(seq2summary(y_tr[i]))\n\nfor i in range(0, 100000):\n    summ.append(decode_sequence(x_tr[i].reshape(1, max_text_len)))\n#on split les phrases pour cr\u00e9er une liste de liste de mots\n\nGOLD = []\nfor i in goldstandard:\n    GOLD.append(i.split())\n\nSUMM = []\nfor i in summ:\n    SUMM.append(i.split())\n\n# on \u00e9value la moyenne des scores\nnltk.translate.bleu_score.corpus_bleu(GOLD, SUMM)","11254965":"from nltk.translate.bleu_score import sentence_bleu","08a15489":"goldstandard = []\nsumm = []\n\nfor i in range(0, 100000):\n    goldstandard.append(seq2summary(y_tr[i]))\n\nfor i in range(0, 100000):\n    summ.append(decode_sequence(x_tr[i].reshape(1, max_text_len)))","d1f0f941":"GOLD = []\nfor i in goldstandard:\n    GOLD.append(i.split())\n\nSUMM = []\nfor i in summ:\n    SUMM.append(i.split())","d03c8888":"nltk.translate.bleu_score.corpus_bleu(GOLD, SUMM)","2f6fbd4b":"pip install rouge","04308a38":"from rouge import Rouge \nrouge = Rouge()\nfor i, j in zip(goldstandard, summ):\n    print(\"Original summary:\",i)\n    print(\"Predicted summary:\",j)\n    scores = rouge.get_scores(i, j)\n    print(scores)\n    print('\\\\n')","feeb828a":"hypothesis = \"talk to your friends about your feelings talk to therapist \"\nreference = \"problem solve ask for support prioritize self care take break engage in therapy help others\"","0cbbfc4f":"rouge = Rouge()\nscores = rouge.get_scores(hypothesis, reference)\nprint(scores)","9320880e":"**Evaluations :**"}}