{"cell_type":{"27cfa504":"code","d1fad599":"code","1edbfcb6":"code","2a403639":"code","c07333bb":"code","e47a960e":"code","9732b6bf":"code","ab12e3a9":"code","0df207f7":"code","ece6163b":"code","a9e15ee4":"code","41d2ce47":"code","8ab3d007":"code","e33dc045":"code","e79b1080":"code","48157adb":"code","9098a8db":"code","7a247557":"code","75e385a8":"code","6ebe3fd3":"code","34634efa":"code","dde98ec2":"code","df118bdf":"code","ff89c598":"code","525b6bfb":"code","c04b6db1":"code","bfc44d06":"code","163e7b13":"code","aaac32c2":"code","ca4e6d63":"code","ef2bda96":"code","3a3111dc":"code","55839d24":"markdown"},"source":{"27cfa504":"# Let's start with standard libraries\n\n# The seed setting is an attempt to get consistent results from Keras. \nfrom numpy.random import seed\nseed(20)\nfrom tensorflow import set_random_seed\nset_random_seed(40)\n\nimport keras\nfrom keras import backend as K\n\nimport tensorflow as tf\nimport os\n\n#import glob\n#import datetime \n\nprint(os.listdir(\"..\/input\"))","d1fad599":"# We will set some configuration variables here\npreTrainedModel = False\nloadOnlyWeights = False\nuseBestModel = True\n\nEPOCH = 32\n#EPOCH = 40\n\nBATCH_SIZE = 1024\n\nVALIDATION_SIZE = 500000\n#TRAINING_SIZE = 10000000 #6M\nTRAINING_SIZE = 6000000 #6M\n#TRAINING_SIZE = 60000\n\n#INTEL_AI_DEV_CLOUD = True\nINTEL_AI_DEV_CLOUD = False\n","1edbfcb6":"# This code is added to enable the current notebook to work seamlessly on Intel AI DevCloud. \n# But it turns out that my MacBook is much better than Intel AI DevCloud resources.\n\nif INTEL_AI_DEV_CLOUD:\n    \n    # Set environment variables to fine tune the nodes for better performance\n    config = tf.ConfigProto(intra_op_parallelism_threads = 6, \n                            inter_op_parallelism_threads = 2, \n                            allow_soft_placement = True, \n                            device_count = {'CPU': 6 })\n\n    session = tf.Session(config = config)\n\n    K.set_session(session) \n    os.environ[\"OMP_NUM_THREADS\"] = \"6\" \n    os.environ[\"KMP_BLOCKTIME\"] = \"30\" \n    os.environ[\"KMP_SETTINGS\"] = \"1\" \n    os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n    \n# end of if statement","2a403639":"# Define the necessary libraries required to define Keras Regression Netowrk\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras import regularizers\n\nfrom keras.models import load_model\nfrom keras.callbacks import ModelCheckpoint\n\n# import our good friend numpy\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nprint(keras.__version__)\n","c07333bb":"# Many thanks to Albert Van Breemen for inspiring the idea of fixed frame of reference\n\n# JFK airport & NYC coordinates, see https:\/\/www.travelmath.com\/airport\/JFK\n# Latitude: 40.6441666667 \/ Longitude: -73.7822222222\n# Latitude: 40.7141667 \/ Longitude: -74.0063889\n\nJFK = (-73.7822222222, 40.6441666667)\nNYC = (-74.0063889, 40.7141667)\n\n# We will use EWR and LGR only if required - too many features are also a problem\nEWR = (-74.175, 40.6897222222)\nLGA = (-73.8739659, 40.7769271)","e47a960e":"# Many thanks to Will Koehrsen for this wonderful function\n# x1 = pickup_longitude \/ x2 = dropoff_longitude \/ y1 = pickup_latitude \/ y2 = dropoff_latitude\n# if p = 1, then this is the Manhattan distance and if p = 2 this is the Euclidean distance.\n\ndef minkowski_distance(x1, x2, y1, y2, p):\n    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 \/ p)        ","9732b6bf":"# if p = 1, then this is the Manhattan distance and if p = 2 this is the Euclidean distance.\n# Inspired by 3 dimensional representation of latitude and longitude - \n# please see the details below regarding converting latitude and longitude to three dimensional coordinates.\ndef minkowski_distance_3D(x1, x2, y1, y2, z1, z2, p):\n    return ((abs(x2 - x1) ** p) + (abs(y2 - y1) ** p) + (abs(z2 - z1) ** p)) ** (1 \/ p)","ab12e3a9":"# MANY MANY THANKS TO Albert Van Breeman for this useful function!\n# direction of a trip, from 180 to -180 degrees. Horizontal axes = 0 degrees.\ndef calculate_direction(d_lon, d_lat):\n    \n    result = np.zeros(len(d_lon))\n    \n    l = np.sqrt(d_lon**2 + d_lat**2)\n    result[d_lon>0] = (180\/np.pi)*np.arcsin(d_lat[d_lon>0]\/l[d_lon>0])\n    idx = (d_lon<0) & (d_lat>0)\n    result[idx] = 180 - (180\/np.pi)*np.arcsin(d_lat[idx]\/l[idx])\n    idx = (d_lon<0) & (d_lat<0)\n    result[idx] = -180 - (180\/np.pi)*np.arcsin(d_lat[idx]\/l[idx])\n    \n    return result\n\n#df_train['direction'] = calculate_direction(df_train.delta_lon, df_train.delta_lat)","0df207f7":"# Another function ispired by Albert Van Breeman's Analysis of NY Taxi Data\ndef time_type_of_the_day(hour):\n    \n    time_of_the_day = 2\n    \n    if (hour >= 4 and hour < 7):\n        time_of_the_day = 1 # Early Morning\n    elif (hour >= 7 and hour < 12):\n        time_of_the_day = 2 # Morning\n    elif (hour >= 12 and hour < 16):\n        time_of_the_day = 3 # Noon\n    elif (hour >= 16 and hour < 20):\n        time_of_the_day = 4 # Evening\n    elif (hour >= 20 and hour < 23):\n        time_of_the_day = 5 # Night\n    elif (hour >= 23 and hour < 4):\n        time_of_the_day = 6 # Late Night\n    else:\n        time_of_the_day = 2 # Morning Default\n\n    return time_of_the_day\n\n# end of the function!","ece6163b":"# The day of the week with Monday=0, Sunday=6\n# 0.5 of additional surcharge between 8PM - 6AM.\n# Peak hour weekday surcharge of $1 Monday-Friday between 4PM-8PM.\ndef peak_hours_of_the_day(hour, weekday):\n    \n    peak_hours = 0\n    \n    if (0 <= weekday <= 4) and (16 <= hour <= 20):\n        peak_hours = 1\n        \n    return peak_hours\n\n# end of the function!","a9e15ee4":"# The day of the week with Monday=0, Sunday=6\n# 0.5 of additional surcharge between 8PM - 6AM.\n# Peak hour weekday surcharge of $1 Monday-Friday between 4PM-8PM.\ndef surcharge_hours_of_the_day(hour):\n    \n    surcharge_hours = 0\n    \n    if (hour >= 20 or hour <= 6):\n        surcharge_hours = 1\n        \n    return surcharge_hours\n\n# end of the function!","41d2ce47":"def add_travel_vector_features(df):\n    \n    df['delta_longitude'] = df.pickup_longitude - df.dropoff_longitude\n    df['delta_latitude'] = df.pickup_latitude - df.dropoff_latitude\n\n    # The credit for the next features goes to Jan van der Vegt @datascience.stackexchange.com\n    # https:\/\/datascience.stackexchange.com\/users\/14904\/jan-van-der-vegt\n    \n    # Lat long coordinates have a problem that they are 2 features that represent a three dimensional space. \n    # This means that the long coordinate goes all around, which means the two most extreme values are actually very close together. \n    # I've dealt with this problem a few times and what I do in this case is map them to x, y and z coordinates. \n    # This means close points in these 3 dimensions are also close in reality. \n    # Depending on the use case you can disregard the changes in height and map them to a perfect sphere. \n    # These features can then be standardized properly.\n\n    # To clarify (summarised from the comments):\n\n    # x = cos(lat) * cos(lon)\n    # y = cos(lat) * sin(lon), \n    # z = sin(lat) \n    \n    # The mapping of pickup and dropoff to three dimensional space helped to improve my ranking by 80 places. It is very similar \n    # to two dimensional representation of cyclical values.\n    \n    df['pickup_x'] = np.cos(df.pickup_latitude) * np.cos(df.pickup_longitude)\n    df['pickup_y'] = np.cos(df.pickup_latitude) * np.sin(df.pickup_longitude)\n    df['pickup_z'] = np.sin(df.pickup_latitude)\n    \n    df['dropoff_x'] = np.cos(df.dropoff_latitude) * np.cos(df.dropoff_longitude)\n    df['dropoff_y'] = np.cos(df.dropoff_latitude) * np.sin(df.dropoff_longitude)\n    df['dropoff_z'] = np.sin(df.dropoff_latitude)\n\n# end of function add_travel_vector_features\n","8ab3d007":"def parse_and_extract_date_time_components(df):\n        \n    df['pickup_datetime'] =  pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z')\n      \n    df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n    df['month'] = df['pickup_datetime'].apply(lambda x: x.month)\n    df['day'] = df['pickup_datetime'].apply(lambda x: x.day)\n    df['hour'] = df['pickup_datetime'].apply(lambda x: x.hour)\n    df['minute'] = df['pickup_datetime'].apply(lambda x: x.minute)\n    df['weekday'] = df['pickup_datetime'].apply(lambda x: x.dayofweek)\n    df['weekofyear'] = df['pickup_datetime'].apply(lambda x: x.weekofyear)\n    df['dayofyear'] = df['pickup_datetime'].apply(lambda x: x.dayofyear)\n    df['quarterofyear'] = df['pickup_datetime'].apply(lambda x: x.quarter)\n    df['days_in_month'] = df['pickup_datetime'].apply(lambda x: x.days_in_month)\n    df['linear_time'] = df['pickup_datetime'].apply(lambda x: x.value \/\/ 10 ** 9)    \n    df['time_type_of_the_day'] = df['pickup_datetime'].apply (lambda x: time_type_of_the_day(x.hour)) \n    df['peak_hours'] = df['pickup_datetime'].apply(lambda x: peak_hours_of_the_day(x.hour, x.dayofweek))\n    df['surcharge'] = df['pickup_datetime'].apply(lambda x: surcharge_hours_of_the_day(x.hour))\n    \n    # Process cyclical values using sin and cosine transformations\n    # These are better than one-hot encoding and helped me a lot to keep features compact.\n    \n    df['hour_sin'] = np.sin(df.hour * (2. * np.pi \/ 24))\n    df['hour_cos'] = np.cos(df.hour * (2. * np.pi \/ 24))\n \n    df['minute_sin'] = np.sin(df.minute * (2. * np.pi \/ 60))\n    df['minute_cos'] = np.cos(df.minute * (2. * np.pi \/ 60))\n    \n    df['time_type_of_day_sin'] = np.sin(df.time_type_of_the_day * (2. * np.pi \/ 6))\n    df['time_type_of_day_cos'] = np.cos(df.time_type_of_the_day * (2. * np.pi \/ 6))\n    \n    df['month_sin'] = np.sin((df.month - 1) * (2. * np.pi \/ 12))\n    df['month_cos'] = np.cos((df.month - 1) * (2. * np.pi \/ 12))\n\n    df['day_sin'] = np.sin((df.day - 1) * (2. * np.pi \/ df.days_in_month))\n    df['day_cos'] = np.cos((df.day - 1) * (2. * np.pi \/ df.days_in_month))\n    \n    df['weekday_sin'] = np.sin((df.weekday - 1) * (2. * np.pi \/ 7))\n    df['weekday_cos'] = np.cos((df.weekday - 1) * (2. * np.pi \/ 7))\n    \n    df['weekofyear_sin'] = np.sin((df.weekofyear - 1) * (2. * np.pi \/ 52))\n    df['weekofyear_cos'] = np.cos((df.weekofyear - 1) * (2. * np.pi \/ 52))\n  \n    df['dayofyear_sin'] = np.sin((df.dayofyear - 1) * (2. * np.pi \/ 365))\n    df['dayofyear_cos'] = np.cos((df.dayofyear - 1) * (2. * np.pi \/ 365))\n\n    df['quarterofyear_sin'] = np.sin((df.quarterofyear - 1) * (2. * np.pi \/ 4))\n    df['quarterofyear_cos'] = np.cos((df.quarterofyear - 1) * (2. * np.pi \/ 4))\n\n    # Clean Up Extra Features which may not be needed\n    del df['hour']\n    del df['minute']\n    del df['month']\n    del df['day']\n    del df['weekday']\n    del df['days_in_month']\n    del df['time_type_of_the_day']\n    del df['weekofyear']\n    del df['quarterofyear']    \n    \n# end of the function to extract date components","e33dc045":"# Calculate NY Manhattan Distance using longitude and lattitude\n# I need to add credit to the author of this fucntion from StackOverFlow. \ndef haversine_np(lon1, lat1, lon2, lat2):\n    \n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n\n    \"\"\"\n    \n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat\/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon\/2.0)**2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    \n    miles = 3959 * c\n    \n    return miles","e79b1080":"# Now read the training set\n\n# To process the entire data set\n#train_df =  pd.read_csv('train.csv')\n\n# To play with only subset of the data\nfname = 'train.csv'\ntrain_df =  pd.read_csv('..\/input\/train.csv', nrows = TRAINING_SIZE)\n\n# Let's find out if there are some null values\nprint(train_df.isnull().sum())\n\n# Let's drop the null values\nprint(fname + ' Old size: %d' % len(train_df))\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')\nprint(fname + ' New size: %d' % len(train_df))","48157adb":"# The next couple of sections focuses on data cleaning and feature extractions which is kind of excessive for Deep Learning\n# probably that's the diference between theoritical claim and practical reality...\n\n# Important feature here two dimensional latitude and longitudes to three dimesnional coordinate systems.\nadd_travel_vector_features(train_df)\n\n# Extract various date components.\nparse_and_extract_date_time_components(train_df)\n\n# NEED TO CHECK THIS VALUE - PROBABLY FIND OUT THE MAXIMUM RANGE OF NEW YORK CITY LIMITS\n\nprint(fname + ' Before dropping Zero pickup longitude and latitude - Old size: %d' % len(train_df))\ntrain_df = train_df[(train_df.pickup_longitude != 0.) & (train_df.pickup_latitude != 0.)]\nprint(fname + ' After dropping Zero pickup longitude and latitude - New size: %d' % len(train_df))\n\nprint(fname + ' Before dropping Zero dropoff longitude and latitude - Old size: %d' % len(train_df))\ntrain_df = train_df[(train_df.dropoff_longitude != 0.) & (train_df.dropoff_latitude != 0.)]\nprint(fname + ' After dropping Zero dropoff longitude and latitude - New size: %d' % len(train_df))\n\nprint(fname + ' Before dropping really large lat and lon Outliers - Old size: %d' % len(train_df))\ntrain_df = train_df[(train_df.delta_longitude.abs() < 5.0) & (train_df.delta_latitude.abs() < 5.0)]\nprint(fname + ' After dropping really large lat and lon Outliers - New size: %d' % len(train_df))\n\nprint(fname + ' Before dropping zero delta lat and long values - Old size: %d' % len(train_df))\ntrain_df = train_df[(train_df.delta_longitude != 0.) & (train_df.delta_latitude != 0.)]\nprint(fname + ' After dropping delta lat and long values - New size: %d' % len(train_df))\n\n# ADD CODE TO CHECK FOR NEGATIVE FARES\nprint(fname + ' Before dropping NEGATIVE FARE VALUES - Old size: %d' % len(train_df))\ntrain_df = train_df[(train_df.fare_amount > 0.)]\nprint(fname + ' After dropping NEGATIVE FARE VALUES - New size: %d' % len(train_df))\n\n# ADD CODE TO CHECK FOR ZERO PASSENGER COUNTS\nprint(fname + ' Before dropping Zero Passenger Counts - Old size: %d' % len(train_df))\ntrain_df = train_df[(train_df.passenger_count > 0) & (train_df.passenger_count < 7)]\nprint(fname + ' After dropping Zero Passenger Counts - New size: %d' % len(train_df))\n","9098a8db":"# Extract more features for the model\ntrain_df['distance_in_miles'] = haversine_np(train_df.dropoff_longitude, train_df.dropoff_latitude, train_df.pickup_longitude, train_df.pickup_latitude)\n\ntrain_df['direction'] = calculate_direction(train_df.delta_longitude, train_df.delta_latitude)\n\n# JFK AS THE FRAME OF REFERENCE\ntrain_df['pickup_to_JFK_distance'] = haversine_np(train_df.pickup_longitude, train_df.pickup_latitude, JFK[0], JFK[1]) \ntrain_df['dropoff_to_JFK_distance'] = haversine_np(train_df.dropoff_longitude, train_df.dropoff_latitude, JFK[0], JFK[1]) \n\n# ALSO USE EWR AS ANOTHER FRAME OF REFERENCE IF REQUIRED\ntrain_df['pickup_to_EWR_distance'] = haversine_np(train_df.pickup_longitude, train_df.pickup_latitude, EWR[0], EWR[1]) \ntrain_df['dropoff_to_EWR_distance'] = haversine_np(train_df.dropoff_longitude, train_df.dropoff_latitude, EWR[0], EWR[1]) \n\n# ALSO USE LGA AS ANOTHER FRAME OF REFERENCE IF REQUIRED\ntrain_df['pickup_to_LGA_distance'] = haversine_np(train_df.pickup_longitude, train_df.pickup_latitude, LGA[0], LGA[1]) \ntrain_df['dropoff_to_LGA_distance'] = haversine_np(train_df.dropoff_longitude, train_df.dropoff_latitude, LGA[0], LGA[1]) \n\n# ALSO USE NYC AS ANOTHER FRAME OF REFERENCE IF REQUIRED\ntrain_df['pickup_to_NYC_distance'] = haversine_np(train_df.pickup_longitude, train_df.pickup_latitude, NYC[0], NYC[1]) \ntrain_df['dropoff_to_NYC_distance'] = haversine_np(train_df.dropoff_longitude, train_df.dropoff_latitude, NYC[0], NYC[1]) \n\n# For some reason my NN model didn't find Euclidean and Manhattan distances that important.\n\n# Calculate Manhattan Distance between two sets of GPS coordinates\ntrain_df['manhattan'] = minkowski_distance(train_df.pickup_longitude, train_df.dropoff_longitude, \n                                           train_df.pickup_latitude, train_df.dropoff_latitude, 1)\n\n# Calcualte Euclidean Distance between two sets of GPS Coordinates\ntrain_df['euclidean'] = minkowski_distance(train_df.pickup_longitude, train_df.dropoff_longitude, \n                                           train_df.pickup_latitude, train_df.dropoff_latitude, 2)\n\n# Calcualte 3D Euclidean Distance between two sets 3D GPS Coordinates\ntrain_df['euclidean_3D'] = minkowski_distance_3D(train_df.pickup_x, train_df.dropoff_x, \n                                                 train_df.pickup_y, train_df.dropoff_y, \n                                                 train_df.pickup_z, train_df.dropoff_z, 2)\n\n# Calcualte 3D Manhattan Distance between two sets 3D GPS Coordinates\ntrain_df['manhattan_3D'] = minkowski_distance_3D(train_df.pickup_x, train_df.dropoff_x, \n                                                 train_df.pickup_y, train_df.dropoff_y, \n                                                 train_df.pickup_z, train_df.dropoff_z, 1)\n\n\"\"\"\n# JFK AS THE EUCLIDEAN FRAME OF REFERENCE\ntrain_df['euclidean_pickup_to_JFK'] = minkowski_distance(train_df.pickup_longitude, JFK[0], train_df.pickup_latitude,  JFK[1], 2) \ntrain_df['euclidean_dropoff_to_JFK'] = minkowski_distance(train_df.dropoff_longitude, JFK[0], train_df.dropoff_latitude, JFK[1], 2) \n\n# NYC AS THE EUCLIDEAN FRAME OF REFERENCE\ntrain_df['euclidean_pickup_to_NYC'] = minkowski_distance(train_df.pickup_longitude, NYC[0], train_df.pickup_latitude,  NYC[1], 2) \ntrain_df['euclidean_dropoff_to_NYC'] = minkowski_distance(train_df.dropoff_longitude, NYC[0], train_df.dropoff_latitude, NYC[1], 2) \n\n# EWR AS THE EUCLIDEAN FRAME OF REFERENCE\ntrain_df['euclidean_pickup_to_EWR'] = minkowski_distance(train_df.pickup_longitude, EWR[0], train_df.pickup_latitude,  EWR[1], 2) \ntrain_df['euclidean_dropoff_to_EWR'] = minkowski_distance(train_df.dropoff_longitude, EWR[0], train_df.dropoff_latitude, EWR[1], 2) \n\"\"\"\n\n# ADD CODE TO CHECK FOR REALLY HIGH FARE AMOUNTS\nprint(fname + ' Before dropping HIGH FARE AMOUNTS - Old size: %d' % len(train_df))\ntrain_df = train_df[(train_df.fare_amount > 1.) & (train_df.fare_amount < 251.)]\nprint(fname + ' After dropping HIGH FARE AMOUNTS - New size: %d' % len(train_df))\n","7a247557":"# Extract the training features and standardize the values\ntraining_set = train_df.loc[:, [\n                                'passenger_count',\n                                'distance_in_miles',\n                                'pickup_x',\n                                'pickup_y',\n                                'pickup_z',\n                                'dropoff_x',\n                                'dropoff_y',\n                                'dropoff_z',\n                                'pickup_to_JFK_distance',\n                                'dropoff_to_JFK_distance',\n                                'pickup_to_NYC_distance',\n                                'dropoff_to_NYC_distance',\n                                'delta_longitude',\n                                'delta_latitude',\n                                'direction',\n                                'year',\n                                'linear_time',\n                                'peak_hours',\n                                'surcharge',\n                                'hour_sin',\n                                'hour_cos',\n                                'minute_sin',\n                                'minute_cos',\n                                'time_type_of_day_sin',\n                                'time_type_of_day_cos',\n                                'month_sin',\n                                'month_cos',\n                                'day_sin',\n                                'day_cos',\n                                'weekday_sin',\n                                'weekday_cos',\n                                'weekofyear_sin',\n                                'weekofyear_cos',\n                                'quarterofyear_sin',\n                                'quarterofyear_cos'\n                                ]].values\n\n# Extract the training label\ntraining_label = train_df.loc[:, 'fare_amount'].values\n\n# Standardize the training data\nmean = training_set.mean(axis = 0)\n\ntraining_set -= mean\n\nstd = training_set.std(axis = 0)\n\ntraining_set \/= std # this corresponds to sci kit fit & transform\n    \n# Code to drop the columns if required - please use the second set of indexes\n# training_set = np.delete(training_set, [21, 22, 23, 24, 27, 28], axis = 1)\n","75e385a8":"# ADD CODE TO GENERATE VALIDATION SET\n# define records to be considered for validation\nnum_validation_samples = VALIDATION_SIZE\n\n# Check if sometimes Validation Set is larger than training set:\nif num_validation_samples >= len(training_set):\n    # Set the validation data set to 20% of training set\n    num_validation_samples = int(len(training_set) * 0.2)\n# end of if\n\n# Validation Data for input\nvalidation_data = training_set[:num_validation_samples]\ntraining_set = training_set[num_validation_samples:]\n\n# Validation Data for target\nvalidation_targets = training_label[:num_validation_samples]\ntraining_label = training_label[num_validation_samples:]\n","6ebe3fd3":"# BEGIN MODEL DEFINITION\n\n# Code snippet to load pretrained model and save weights\n#model = load_model('.\/GoodModelRepo\/NYCTFP_Model_4.h5')\n#model.save_weights('.\/GoodModelRepo\/NYCTFP_weights_4.h5')\n#del model\n\n#kernel_regularizer = regularizers.l2(0.001)\n#kernel_regularizer = regularizers.l2(0.0001)\n#kernel_regularizer = regularizers.l1_l2(l1=0.001, l2=0.001)\n#kernel_regularizer = regularizers.l1_l2(l1=0.0001, l2=0.0001)\n\nNN_INPUT_SIZE = training_set.shape[1]\n\nif preTrainedModel == True:\n    \n    # Load from the previous best model\n    model = load_model('.\/GoodModelRepo\/0_NYCTFP_Model_8.h5')\n    \nelse:\n    \n    # Intialize the model\n    model = models.Sequential()\n    \n    #model.add(layers.Dense(128, kernel_regularizer = regularizers.l1_l2(l1=0.001, l2=0.001), activation = 'relu', input_shape = (NN_INPUT_SIZE,)))\n    model.add(layers.Dense(128, kernel_initializer = 'uniform', activation = 'relu', input_shape = (NN_INPUT_SIZE,)))\n    #model.add(layers.BatchNormalization())\n    #model.add(layers.Dropout(0.2))\n    \n    # Add a second layer\n    #model.add(layers.Dense(128, kernel_regularizer = regularizers.l1_l2(l1=0.001, l2=0.001), activation = 'relu'))\n    model.add(layers.Dense(128, kernel_initializer = 'uniform', activation = 'relu'))\n    #model.add(layers.BatchNormalization())\n    #model.add(layers.Dropout(0.2))\n    \n    # Add a third layer\n    #model.add(layers.Dense(64, kernel_regularizer = regularizers.l1_l2(l1=0.001, l2=0.001), activation = 'relu'))\n    model.add(layers.Dense(64, kernel_initializer = 'uniform', activation = 'relu'))\n    #model.add(layers.BatchNormalization())\n    #model.add(layers.Dropout(0.2))\n    \n    # Add a fourth layer\n    #model.add(layers.Dense(32, kernel_regularizer = regularizers.l1_l2(l1=0.001, l2=0.001), activation = 'relu'))\n    model.add(layers.Dense(32, kernel_initializer = 'uniform', activation = 'relu'))\n    #model.add(layers.BatchNormalization())\n    #model.add(layers.Dropout(0.2))\n    \n    \"\"\"\n    # Add a fifth layer\n    #model.add(layers.Dense(8, kernel_regularizer = regularizers.l1_l2(l1=0.001, l2=0.001), activation = 'relu'))\n    model.add(layers.Dense(16, kernel_initializer = 'uniform', activation = 'relu'))\n    #model.add(layers.BatchNormalization())\n    #model.add(layers.Dropout(0.2))\n    \n    # Add a fifth layer\n    #model.add(layers.Dense(8, kernel_regularizer = regularizers.l1_l2(l1=0.001, l2=0.001), activation = 'relu'))\n    model.add(layers.Dense(8, kernel_initializer = 'uniform', activation = 'relu'))\n    #model.add(layers.BatchNormalization())\n    #model.add(layers.Dropout(0.2))\n    \n    \"\"\"\n    \n    # Add a output layer without an activation function\n    model.add(layers.Dense(1))\n    \n    #model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])\n    #model.compile(optimizer = optimizers.Adam(lr=0.01), loss = 'mse', metrics = ['mae'])\n    \n    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n    #model.compile(optimizer = optimizers.RMSprop(lr=0.01), loss = 'mse', metrics = ['mae'])\n    #model.compile(optimizer = optimizers.RMSprop(lr=0.0006), loss = 'mse', metrics = ['mae'])\n    \n    #model.compile(optimizer = 'sgd', loss = 'mse', metrics = ['mae'])\n    \n    # Load the good weights that we got from previous best score\n    # These weights are good for 64\/64\/1 architecture\n    # FOR NEW ARCHITECTURES THE FOLLOWING LINE SHOULD BE COMMENTED\n    if loadOnlyWeights == True:    \n        model.load_weights('.\/GoodModelRepo\/0_NYCTFP_Weights_8.h5')\n\n# END OF MODEL DEFINITION\n","34634efa":"# DEFINE A CALLBACK TO SAVE THE BEST MODEL WITH MINIMUM VALIDATION MAE VALUE\n# keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\ncheckpointer = ModelCheckpoint(filepath = 'NYCTFP_Ref_Model.h5', monitor = 'val_mean_absolute_error', verbose = 1, save_best_only = True, mode = 'min')\n\n# Train the model\ntest_history = model.fit(training_set, training_label, shuffle=True,\n                         validation_data = (validation_data, validation_targets), \n                         epochs = EPOCH, batch_size = BATCH_SIZE,\n                         callbacks = [checkpointer])\n","dde98ec2":"print(os.listdir(\"\/kaggle\/working\")) #print(os.getcwd()) #print(os.listdir(\".\")) # \/kaggle\/working is the current directory","df118bdf":"# PLOT THE GRAPH HERE TO TRACE MAE AND LOSS.\n\n# Code to Evaluate the model on the validation data - if required\n#val_mse, val_mae = model.evaluate(validation_data, validation_targets)\n#print(val_mse, val_mae)\n\nif INTEL_AI_DEV_CLOUD == False:\n    \n    # Draw the graphs on the MacBook\n    \n    # forms set of Y axis values\n    mae = test_history.history['mean_absolute_error']\n    val_mae = test_history.history['val_mean_absolute_error']\n    loss = test_history.history['loss']\n    val_loss = test_history.history['val_loss']\n\n    # forms X axis values\n    epochs = range(1, len(mae) + 1)\n\n    # Process Training and Validation Accuracy\n    plt.plot(epochs, mae, 'bo', label = 'New York Taxi Fare Training MAE')\n    plt.plot(epochs, val_mae, 'r', label = 'New York Taxi Fare Validation MAE')\n    plt.title('New York Taxi Fare Training and Validation MAE Comparison')\n    plt.legend()\n    plt.show()\n\n    # Process Training and Validation Losses\n    plt.plot(epochs, loss, 'ro', label = 'New York Taxi Fare Training Loss')\n    plt.plot(epochs, val_loss, 'g', label = 'New York Taxi Fare Validation Loss')\n    plt.title('New York Taxi Fare Training and Validation Loss Comparison')\n    plt.legend()\n    plt.show()\n\n# end of if statement\n","ff89c598":"# Always save the model after training - we could reuse it as the sophistication increases.\nmodel.save('NYCTFP_Model_12.h5')\nmodel.save_weights('NYCTFP_Weights_12.h5')\npd.DataFrame(test_history.history).to_csv(\"epoch_testing_history.csv\", header = True, index = False)\n\n# Release the model created for training. We will create a new one for testing.\n# del model\n","525b6bfb":"print(os.listdir(\"\/kaggle\/working\")) #print(os.getcwd()) #print(os.listdir(\".\")) # \/kaggle\/working is the current directory","c04b6db1":"# Read the test file\ntest_df = pd.read_csv('..\/input\/test.csv')\n\nprint(test_df.isnull().sum())\n\n# Let's drop the null values - THERE SHOULDN'T BE ANY...\nprint('test.csv Old size: %d' % len(test_df))\ntest_df = test_df.dropna(how = 'any', axis = 'rows')\nprint('test.csv New size: %d' % len(test_df))\n","bfc44d06":"# Generate the necessary features for test data file\nadd_travel_vector_features(test_df)\n\nparse_and_extract_date_time_components(test_df)\n","163e7b13":"# Generate more features - we need to match the input layout provided to train the neural network\n\n# Calculate the distance in miles\ntest_df['distance_in_miles'] = haversine_np(test_df.dropoff_longitude, test_df.dropoff_latitude, test_df.pickup_longitude, test_df.pickup_latitude)\n\ntest_df['direction'] = calculate_direction(test_df.delta_longitude, test_df.delta_latitude)\n\n# JFK AS THE FRAME OF REFERENCE\ntest_df['pickup_to_JFK_distance'] = haversine_np(test_df.pickup_longitude, test_df.pickup_latitude, JFK[0], JFK[1]) \ntest_df['dropoff_to_JFK_distance'] = haversine_np(test_df.dropoff_longitude, test_df.dropoff_latitude, JFK[0], JFK[1]) \n\n# ALSO USE EWR AS ANOTHER FRAME OF REFERENCE IF REQUIRED\ntest_df['pickup_to_EWR_distance'] = haversine_np(test_df.pickup_longitude, test_df.pickup_latitude, EWR[0], EWR[1]) \ntest_df['dropoff_to_EWR_distance'] = haversine_np(test_df.dropoff_longitude, test_df.dropoff_latitude, EWR[0], EWR[1]) \n\n# ALSO USE LGA AS ANOTHER FRAME OF REFERENCE IF REQUIRED\ntest_df['pickup_to_LGA_distance'] = haversine_np(test_df.pickup_longitude, test_df.pickup_latitude, LGA[0], LGA[1]) \ntest_df['dropoff_to_LGA_distance'] = haversine_np(test_df.dropoff_longitude, test_df.dropoff_latitude, LGA[0], LGA[1]) \n\n# ALSO USE NYC AS ANOTHER FRAME OF REFERENCE IF REQUIRED\ntest_df['pickup_to_NYC_distance'] = haversine_np(test_df.pickup_longitude, test_df.pickup_latitude, NYC[0], NYC[1]) \ntest_df['dropoff_to_NYC_distance'] = haversine_np(test_df.dropoff_longitude, test_df.dropoff_latitude, NYC[0], NYC[1]) \n\n# Calculate Manhattan Distance between two sets of GPS coordinates\ntest_df['manhattan'] = minkowski_distance(test_df.pickup_longitude, test_df.dropoff_longitude, \n                                          test_df.pickup_latitude, test_df.dropoff_latitude, 1)\n\n# Calcualte Euclidean Distance between two sets of GPS Coordinates\ntest_df['euclidean'] = minkowski_distance(test_df.pickup_longitude, test_df.dropoff_longitude, \n                                          test_df.pickup_latitude, test_df.dropoff_latitude, 2)\n\n# Calcualte 3D Euclidean Distance between two sets 3D GPS Coordinates\ntest_df['euclidean_3D'] = minkowski_distance_3D(test_df.pickup_x, test_df.dropoff_x, \n                                                 test_df.pickup_y, test_df.dropoff_y, \n                                                 test_df.pickup_z, test_df.dropoff_z, 2)\n\n# Calcualte 3D Manhattan Distance between two sets 3D GPS Coordinates\ntest_df['manhattan_3D'] = minkowski_distance_3D(test_df.pickup_x, test_df.dropoff_x, \n                                                 test_df.pickup_y, test_df.dropoff_y, \n                                                 test_df.pickup_z, test_df.dropoff_z, 1)\n\"\"\"\n# JFK AS THE EUCLIDEAN FRAME OF REFERENCE\ntest_df['euclidean_pickup_to_JFK'] = minkowski_distance(test_df.pickup_longitude, JFK[0], test_df.pickup_latitude,  JFK[1], 2) \ntest_df['euclidean_dropoff_to_JFK'] = minkowski_distance(test_df.dropoff_longitude, JFK[0], test_df.dropoff_latitude, JFK[1], 2) \n\n# NYC AS THE EUCLIDEAN FRAME OF REFERENCE\ntest_df['euclidean_pickup_to_NYC'] = minkowski_distance(test_df.pickup_longitude, NYC[0], test_df.pickup_latitude,  NYC[1], 2) \ntest_df['euclidean_dropoff_to_NYC'] = minkowski_distance(test_df.dropoff_longitude, NYC[0], test_df.dropoff_latitude, NYC[1], 2) \n\n# EWR AS THE EUCLIDEAN FRAME OF REFERENCE\ntest_df['euclidean_pickup_to_EWR'] = minkowski_distance(test_df.pickup_longitude, EWR[0], test_df.pickup_latitude,  EWR[1], 2) \ntest_df['euclidean_dropoff_to_EWR'] = minkowski_distance(test_df.dropoff_longitude, EWR[0], test_df.dropoff_latitude, EWR[1], 2) \n\n\"\"\"\nprint('Done with Test Data File Feature Generations')","aaac32c2":"# Extract the testing features to be used for extraction - it needs to match with features used for training.\n\ntesting_set = test_df.loc[:, [\n                                'passenger_count',\n                                'distance_in_miles',\n                                'pickup_x',\n                                'pickup_y',\n                                'pickup_z',\n                                'dropoff_x',\n                                'dropoff_y',\n                                'dropoff_z',\n                                'pickup_to_JFK_distance',\n                                'dropoff_to_JFK_distance',\n                                'pickup_to_NYC_distance',\n                                'dropoff_to_NYC_distance',\n                                'delta_longitude',\n                                'delta_latitude',\n                                'direction',\n                                'year',\n                                'linear_time',\n                                'peak_hours',\n                                'surcharge',\n                                'hour_sin',\n                                'hour_cos',\n                                'minute_sin',\n                                'minute_cos',\n                                'time_type_of_day_sin',\n                                'time_type_of_day_cos',\n                                'month_sin',\n                                'month_cos',\n                                'day_sin',\n                                'day_cos',\n                                'weekday_sin',\n                                'weekday_cos',\n                                'weekofyear_sin',\n                                'weekofyear_cos',\n                                'quarterofyear_sin',\n                                'quarterofyear_cos'\n                                ]].values\n\n# Now standardize the test data using mean and standard deviation calculated using training data\ntesting_set -= mean\ntesting_set \/= std # this corresponds to sci kit transform\n\n# Code to drop the columns if required - please use the second set of indexes\n# testing_set = np.delete(testing_set, [21, 22, 23, 24, 27, 28], axis = 1)\n","ca4e6d63":"# Create the NN Model for prediction\nif useBestModel == True:\n    \n    # Then use the best model to score test data\n    del model\n    model = load_model('NYCTFP_Ref_Model.h5')\n    \n    # testModel = load_model('NYCTFP_Ref_Model.h5')\n# end of if useBest Model\n\n# TEST PREDICTION HERE AND GENERATE OUTPUT FILE\npredictions = model.predict(testing_set) #, batch_size = BATCH_SIZE)\n\noutputKeys = np.reshape(test_df.iloc[:, 0].values, (len(test_df), 1))\n\noutputdata = np.concatenate((outputKeys, predictions), axis = 1)\n\n# Write the predictions to a CSV file which we can submit to the competition.\nsubmission = pd.DataFrame(outputdata, columns = ['key', 'fare_amount']) #, index = outputdata[:, 0])\n# submission = pd.DataFrame({'key': outputKeys, 'fare_amount': predictions}, columns = ['key', 'fare_amount'], index = test_df.iloc[:, 0] )\n\nsubmission.to_csv('submission.csv', index = False)\n\n#del model","ef2bda96":"print(os.listdir(\"\/kaggle\/working\")) #print(os.getcwd()) #print(os.listdir(\".\")) # \/kaggle\/working is the current directory","3a3111dc":"#import subprocess\n#subprocess.run('kaggle competitions submit -c new-york-city-taxi-fare-prediction -f \/kaggle\/working\/submission.csv -m \"Submitted From Kaggle Notebook\"')\n#kaggle competitions submit -c new-york-city-taxi-fare-prediction -f submission.csv -m \"Submitted From Kaggle Notebook\"","55839d24":"The code in this notebook generated the RMSE score of 3.099. The progress has been slow ever since. It appears that in general band of brothers and sisters around Keras and TensorFlow are suffering. \n\nBest Val MAE of 1.50873. 500K\/6M-128\/128\/64\/32\/16\/8\/1 - 32\/1024 - 35 Features - loss: 11.0058 - mean_absolute_error: 1.5223 - val_loss: 11.7594 - val_mean_absolute_error: 1.5686  - MacBook Processing - Modified code to add quarter of the year  - the ranking increased by 14.  Used Best Model Approach!\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output."}}