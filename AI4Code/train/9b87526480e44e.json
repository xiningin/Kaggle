{"cell_type":{"355ba6ae":"code","c00dbd56":"code","371e9aff":"code","170d8049":"code","93f3dd3e":"code","f3b47a24":"code","6b5f0094":"code","904ff49a":"code","0415d9cb":"code","5baca63c":"code","831e4bdf":"code","0f046f06":"code","0a8f5518":"code","ffa1ee44":"code","b5acf783":"code","d85b9086":"code","5c7ee5e9":"code","a492e23a":"code","fb1cb9e4":"code","46ffc329":"code","6e48cb75":"code","6ffb0c52":"code","892c284b":"code","6f503273":"code","3c2357fa":"code","1c6312d9":"code","0b613c69":"code","4c88f49a":"code","34f5dfc6":"code","8c6cdd6c":"code","9fe5c0a1":"code","29639fab":"code","e48724a7":"markdown","12014484":"markdown","9fc4666e":"markdown","00df479d":"markdown","f29b9451":"markdown","cdd2ec9c":"markdown","b439433e":"markdown","a867c5cf":"markdown","71b4e688":"markdown","b68e45ea":"markdown","08af2715":"markdown","15670f95":"markdown","8a76fade":"markdown","33130cdb":"markdown","037f9ba8":"markdown","08d20b9b":"markdown","f27d9f61":"markdown","34e46f6e":"markdown","e6af744c":"markdown","2a8e04d5":"markdown"},"source":{"355ba6ae":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import KFold \nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom sklearn.metrics import r2_score\n\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","c00dbd56":"# load datasets \nhappy_2015 = pd.read_csv(\"..\/input\/world-happiness-report\/2015.csv\")\nhappy_2016 = pd.read_csv(\"..\/input\/world-happiness-report\/2016.csv\")\nhappy_2017 = pd.read_csv(\"..\/input\/world-happiness-report\/2017.csv\")\nhappy_2018 = pd.read_csv(\"..\/input\/world-happiness-report\/2018.csv\")\nhappy_2019 = pd.read_csv(\"..\/input\/world-happiness-report\/2019.csv\")","371e9aff":"happy_2015.head()","170d8049":"# get column names \nfor col in happy_2015.columns: \n    print(col) ","93f3dd3e":"# check for missing values\nhappy_2015.isna().sum(axis=0)","f3b47a24":"happy_2016.head()","6b5f0094":"# get column names \nfor col in happy_2016.columns: \n    print(col) ","904ff49a":"# check for missing values\nhappy_2019.isna().sum(axis=0)","0415d9cb":"happy_2017.head()","5baca63c":"# get column names \nfor col in happy_2017.columns: \n    print(col) ","831e4bdf":"# check for missing values\nhappy_2017.isna().sum(axis=0)","0f046f06":"happy_2018.head()","0a8f5518":"# get column names \nfor col in happy_2018.columns: \n    print(col) ","ffa1ee44":"# check for missing values\nhappy_2018.isna().sum(axis=0)","b5acf783":"# drop row with missing value\nhappy_2018 = happy_2018.dropna()","d85b9086":"happy_2019.head()","5c7ee5e9":"print(happy_2018.shape)","a492e23a":"# get column names \nfor col in happy_2019.columns: \n    print(col) ","fb1cb9e4":"# check for missing values\nhappy_2018.isna().sum(axis=0)","46ffc329":"# create data frames with only top 20 countries \ntop_20_2015 = happy_2015.iloc[:20,:]\ntop_20_2016 = happy_2016.iloc[:20,:]\ntop_20_2017 = happy_2017.iloc[:20,:]\ntop_20_2018 = happy_2018.iloc[:20,:]\ntop_20_2019 = happy_2019.iloc[:20,:]","6e48cb75":"# creating trace1\ntrace1 =go.Scatter(\n                    x = top_20_2015['Country'],\n                    y = top_20_2015['Happiness Score'],\n                    mode = \"markers\",\n                    name = \"2015\",\n                    marker = dict(color = 'red'),\n                    text= top_20_2015.Country)\n# creating trace2\ntrace2 =go.Scatter(\n                    x = top_20_2015['Country'],\n                    y = top_20_2016['Happiness Score'],\n                    mode = \"markers\",\n                    name = \"2016\",\n                    marker = dict(color = 'green'),\n                    text= top_20_2015.Country)\n# creating trace3\ntrace3 =go.Scatter(\n                    x = top_20_2015['Country'],\n                    y = top_20_2017['Happiness.Score'],\n                    mode = \"markers\",\n                    name = \"2017\",\n                    marker = dict(color = 'blue'),\n                    text= top_20_2015.Country)\n\n# creating trace4\ntrace4 =go.Scatter(\n                    x = top_20_2015['Country'],\n                    y = top_20_2018['Score'],\n                    mode = \"markers\",\n                    name = \"2018\",\n                    marker = dict(color = 'black'),\n                    text= top_20_2015.Country)\n\n# creating trace5\ntrace5 =go.Scatter(\n                    x = top_20_2015['Country'],\n                    y = top_20_2019['Score'],\n                    mode = \"markers\",\n                    name = \"2019\",\n                    marker = dict(color = 'pink'),\n                    text= top_20_2015.Country)\n\n\ndata = [trace1, trace2, trace3, trace4, trace5]\nlayout = dict(title = 'Happiness Rate Changing 2015 to 2019 for Top 20 Countries',\n              xaxis= dict(title= 'Country',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Happiness',ticklen= 5,zeroline= False),\n              hovermode=\"x unified\"\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","6ffb0c52":"# pairs plot \nplt.figure(figsize = [16, 16])\nfor i, r in enumerate(happy_2019.columns[2:]): \n    for j, c in enumerate(happy_2019.columns[2:]): \n        plt.subplot(7, 7, 7*i+j+1)\n        if i == j: \n            plt.hist(happy_2019[c], bins = 20, edgecolor = 'k', color = 'cornflowerblue')\n        else:\n            plt.scatter(happy_2019[c], happy_2019[r], s = 5, alpha = 0.5, color = 'cornflowerblue')\n        plt.xlabel(c)\n        plt.ylabel(r)\nplt.tight_layout()\nplt.savefig('pairs_plot.png')","892c284b":"data = dict(\n        type = 'choropleth',\n        colorscale = 'Viridis',\n        marker_line_width=1,\n        locations = happy_2019['Country or region'],\n        locationmode = \"country names\",\n        z = happy_2019['Score'],\n        text = happy_2019['Country or region'],\n        colorbar = {'title' : 'Score'},\n        \n      )\nlayout = dict(title = 'Happiness Map 2019',\n              geo = dict(projection = {'type':'mercator'}, showocean = False, showlakes = True, showrivers = True, )\n             )\nchoromap = go.Figure(data = [data],layout = layout)\n\nchoromap.update_layout(autosize = False, \n                       width = 600,\n                       height = 600)\n\niplot(choromap, validate=False)","6f503273":"data = [\n    {\n        'y': happy_2019['GDP per capita'],\n        'x': happy_2019['Healthy life expectancy'],\n        'mode': 'markers',\n        'marker': {\n            'color': happy_2019['Freedom to make life choices'],\n            'size': happy_2019['Score'],\n            'showscale': True\n        },\n        \"text\" :  happy_2019['Country or region']    \n    }\n]\nlayout = go.Layout(barmode='overlay', hovermode=\"x\",\n                   title='Bubble Chart: x = Healthy life expectancy, y = GDP per capita, size = Happiness Score, Color = Freedom to make life choices, year = 2019',\n                   xaxis=dict(title='Healthy life expectancy'),\n                   yaxis=dict(title='GDP per capita'),\n                  \n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","3c2357fa":"# shuffle the dataframe\nhappy_2019 = happy_2019.sample(frac=1).reset_index(drop=True)\n\nhappy_2019.head()","1c6312d9":"# create 2D array of features \nX = happy_2019.loc[:,['GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']].values\n\n# create array for labels \ny = happy_2019.loc[:,['Score']]","0b613c69":"# scale variables \nscaler = preprocessing.MinMaxScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)","4c88f49a":"# set random seed \nnp.random.seed(1)\n\n#linear model\nlr_model = LinearRegression()\nlr_model.fit(X, y)\nprint(lr_model.coef_)\n \nlm_result = cross_val_score(lr_model , X, y, cv = 10, n_jobs = -1)\n \nprint(np.mean(lm_result))","34f5dfc6":"from sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet ","8c6cdd6c":"%%time\n\nelasticnet_model = ElasticNet(normalize = True)\nparameters = {\n    'l1_ratio' : [0.0, 0.5, 1.0],\n    'alpha' : [0.003, 0.005, 0.0075, 0.01]\n}\n\nen_grid = GridSearchCV(elasticnet_model, parameters, cv = 10, refit = True, \n                       n_jobs = -1, verbose = 0, scoring = 'r2')\nen_grid.fit(X, y)","9fe5c0a1":"grid = en_grid\nen_model = grid.best_estimator_\n\nprint('Best Parameters:', grid.best_params_)\nprint('Best CV Score:  ', grid.best_score_)\nprint('Training r2:    ', grid.best_estimator_.score(X, y))\n\ny1 = y\ny2 = grid.best_estimator_.predict(X)\nprint('Training MSE:   ', r2_score(y1, y2))\n\nprint()\ncv_res = grid.cv_results_\nfor params, score in zip(cv_res['params'], cv_res['mean_test_score']): \n    print(f'{str(params):<40} {score:.8f}')","29639fab":"# compare coefficients from unregularized model to regularized model \nprint(lr_model.coef_)\nprint(en_model.coef_)","e48724a7":"### 2019","12014484":"### 2018","9fc4666e":"### 2016","00df479d":"### 2017","f29b9451":"## Predictive Models\nLooking at 2019 only. <br> <br> \nBecause this is a particularly small data set, it doesn't lend itself to predictive modeling. However, we can use both Linear and Elastic Net models to understand the importance of factors. By looking at the coefficients, particularly on the Elstic Net model, we can determine which factors are the least and most important in predicting the Happiness Score. While this data set is already small, this can also be useful for dimensionality reduction.  \n","cdd2ec9c":"## Bubble Plots\nLooking at 2019 only. ","b439433e":"### Comparing Happiness Scores for each country and year","a867c5cf":"## Map Plots\nLooking at 2019 only. \n","71b4e688":"### Load Data","b68e45ea":"## Overview\n\nThis project explores the happiness scores, as well as factors used to explain the score, for 153 countries from 2015 through 2019. \nThe score and rankings are based on answer to the Gallup World Survey. ","08af2715":"### Exploring the relationships between variables\nLooking at 2019 only. \n","15670f95":"## Scatter Plots","8a76fade":"### Linear Regression","33130cdb":"## Preparing the Data","037f9ba8":"### Import Packages","08d20b9b":"Here, we can see that the coefficient for \"Genorosity\" is the lowest. According to this model, this variable is least important in predicting the Happiness Score. While this data set only has 6 predictor variables, if we wanted to reduce dimensionality, we might remove this column from our model. \n\nConversely, \"Freedom to make life choices\" and \"Healthy life expectancy\" appear to be the most important variables in predicting Happiness Score.","f27d9f61":"# World Happiness Report \n### Exploratory Data Analysis\nMadison Agatstein","34e46f6e":"## What factors help to determine the Happiness Score? \n* GDP per capita\n* Social support\n* Healthy life expectancy\n* Freedom to make life choices\n* Generosity\n* Perceptions of corruption\n\nThese factors come from the year 2019. There were additional factors in previous years, but these factors were common for every year. \n","e6af744c":"### 2015","2a8e04d5":"### Elastic Net"}}