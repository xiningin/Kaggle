{"cell_type":{"4084a22c":"code","f173732c":"code","df7ca4e9":"code","f8fc2477":"code","84f64bf3":"code","b8a49d92":"code","7d05f1ce":"code","8a729443":"code","90f0aa48":"code","5d666d3f":"code","e815a8c6":"code","16a5e895":"code","143f4520":"code","6ed62a16":"code","a920151f":"code","efdf134e":"code","7b46da8d":"code","20b3fc72":"code","4f1ed3f0":"code","2e326c8f":"code","563465fe":"code","b751d4c2":"code","3518e7dd":"code","3e931a63":"code","89b47962":"code","6bfd1d57":"code","3f47e16e":"code","ab853bd0":"code","ff82c5c6":"code","d7f74da6":"code","388da76c":"code","b7316dbc":"code","016114e2":"code","858ee19a":"code","037f3570":"code","8558322b":"code","cc54e11f":"code","400d8c69":"code","4cce66c7":"code","c6041d17":"code","58287f2b":"code","39a8d914":"code","ed31c925":"code","5735d727":"code","80f87797":"code","57a128fd":"code","5adc2863":"code","6d6623b4":"code","1af7562e":"code","05d37733":"code","8ed7373c":"code","99a0e3e9":"code","f2225272":"code","bdb15aad":"code","33c98a57":"code","f3364404":"code","6e0d5871":"code","782aad71":"code","8a4ade86":"code","6f9483ec":"code","f3bb07e6":"code","d445fd50":"code","350e11de":"markdown","b3afeef4":"markdown","909aa392":"markdown","e8432236":"markdown","cd56a06e":"markdown","cb4c3373":"markdown","7abd8e1a":"markdown","613af87e":"markdown","66398a68":"markdown","644ae848":"markdown","3158f4e0":"markdown","ec8f0fa0":"markdown","8d2ef152":"markdown","1fab6551":"markdown","4f810c23":"markdown","aa9f82e1":"markdown","45e9a9c8":"markdown","f2e1c811":"markdown","7f6760f7":"markdown","493e79d6":"markdown","d4901c8c":"markdown","96a00f83":"markdown","ec4bd231":"markdown","04d25483":"markdown","d41a43fd":"markdown","ee74ba2c":"markdown"},"source":{"4084a22c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f173732c":"import pandas as pd\ndf_diamonds = pd.read_csv(\"..\/input\/diamonds\/diamonds.csv\")","df7ca4e9":"import IPython, graphviz\nfrom IPython.display import display\nfrom sklearn import metrics\nimport pandas as pd\nimport numpy as np\nimport re\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nimport os\nimport math\nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import forest\nfrom IPython.display import Image\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier","f8fc2477":"## Chechking dataset summary\ndf_diamonds.head()","84f64bf3":"## Here we're checking dimenstions of the dataset\ndf_diamonds.shape","b8a49d92":"## Row max or columns max view \npd.options.display.max_columns = None\npd.options.display.max_rows = None","7d05f1ce":"# To check the data type which category each variables lies\ndf_diamonds.dtypes","8a729443":"## Droping unnamed variable, which is actually not relavent for our further calculations.\ndf_diamonds.drop('Unnamed: 0', axis=1, inplace=True)","90f0aa48":"## after droppping the serial number variable, we're checking the dataset head again.\ndf_diamonds.head()","5d666d3f":"df_diamonds.describe()","e815a8c6":"import matplotlib.pyplot as plt        # For plotting graphs \n%matplotlib inline ","16a5e895":"## Here we're checking that is there any null or NA or missing values in the dataset or else dataset is clean.\ndf_diamonds.isnull().sum()","143f4520":"## Here we're  checking one by one each variable distribution. start with 'Cut'.\ndf_diamonds['cut'].value_counts()\/df_diamonds['cut'].value_counts().sum()*100","6ed62a16":"df_diamonds['cut'].value_counts().plot.barh()","a920151f":"## Secondly, now we are taking color variable and checking its distribution.\ndf_diamonds['color'].value_counts()","efdf134e":"df_diamonds['color'].value_counts().plot.bar()","7b46da8d":"## Now we are taking 'Clarity' variable and checking how it is distributed.\ndf_diamonds['clarity'].value_counts()","20b3fc72":"df_diamonds['clarity'].value_counts().plot.barh()","4f1ed3f0":"import seaborn as sns","2e326c8f":"# Setting the plot size\nfig, axis = plt.subplots(figsize=(7,7))\n# We use kde=True to plot the gaussian kernal density estimate\nsns.distplot(df_diamonds['price'], bins=50, kde=True)","563465fe":"## Continuous variable distribution, start with depth, then table, then carat.\ndf_diamonds['depth'].plot.box()","b751d4c2":"# Setting the plot size\nfig, axis = plt.subplots(figsize=(7,7))\n# We use kde=True to plot the gaussian kernal density estimate\nsns.distplot(df_diamonds['depth'], bins=20, kde=True)","3518e7dd":"## Table - Distribution\ndf_diamonds['table'].plot.box()","3e931a63":"# Setting the plot size\nfig, axis = plt.subplots(figsize=(7,7))\n# We use kde=True to plot the gaussian kernal density estimate\nsns.distplot(df_diamonds['table'], bins=30, kde=True)","89b47962":"## Now Carat\ndf_diamonds['carat'].plot.box()","6bfd1d57":"# Convert target variable to categorical like low,medium,high and prime price and analyse: \n\ndf_diamonds.loc[df_diamonds['price'] <= 5000,'Price_Cat'] = 0\ndf_diamonds.loc[(df_diamonds['price'] > 5000) & (df_diamonds.price <=10000),'Price_Cat'] = 1\ndf_diamonds.loc[(df_diamonds['price'] > 10000) & (df_diamonds.price <=15000),'Price_Cat'] = 2\ndf_diamonds.loc[df_diamonds['price'] > 15000,'Price_Cat'] = 3\ndf_diamonds.head(5)","3f47e16e":"sns.countplot('Price_Cat',data=df_diamonds,color='blue')\nplt.show()","ab853bd0":"## checking price effect on 'Cut' variable:\nfig, axes = plt.subplots(1,2,figsize=(18,5))\nsns.countplot('cut',data=df_diamonds,ax=axes[0],color='blue')\nsns.countplot('cut',hue='Price_Cat',data=df_diamonds,ax=axes[1])\nplt.show()","ff82c5c6":"fig, axes = plt.subplots(1,2,figsize=(18,5))\nsns.countplot('clarity',data=df_diamonds,ax=axes[0])\nsns.countplot('clarity',hue='Price_Cat',data=df_diamonds,ax=axes[1])\nplt.show()","d7f74da6":"### Now here we are droping our new variable which was created to see the distribution more precisely \"Price-Cat\"\ndf_diamonds.drop('Price_Cat', axis=1, inplace=True)","388da76c":"### Checking variable x, whcih is highly spread on the curve\n# Setting the plot size\nfig, axis = plt.subplots(figsize=(7,7))\n# We use kde=True to plot the gaussian kernal density estimate\nsns.distplot(df_diamonds['x'], bins=2, kde=True)","b7316dbc":"# Setting the plot size\nfig, axis = plt.subplots(figsize=(7,7))\n# We use kde=True to plot the gaussian kernal density estimate\nsns.distplot(df_diamonds['y'], bins=1, kde=True)","016114e2":"# Now, we're tring to see the correlation between the variables and try to check the multicollinearity of the variables.\nCorrelation = df_diamonds.corr(method='pearson')\nCorrelation","858ee19a":"# Generate a mask for the upper trinagle\n# np.zeros_like_Return an array of zeros with the same shape\n# and type as per given array\n# In this case we pass the correlation matrix\n# we create a varibale \"mask\" which is a 14 x 14 numpy array\nmask = np.zeros_like(Correlation, dtype=np.bool)\n\n# we create a tuple with triu_indices_from() passing the \"mask\" array\n# k is used to offset diagonal\n# with k=0, we offset all diagonal \n# if we put k=13, means we offset 14-13=1 diagonal\n\n# triu_indices_from() return the indices for the upper-trinagle of err.\nmask[np.triu_indices_from(mask, k=0)]=True\n\n#Setting the plot size\nfig, axis = plt.subplots(figsize=(11,11))\n# cbar_kwn=(\"shrink\":0.5) is shrinking the legend color bar\nsns.heatmap(Correlation, mask=mask, cmap=\"YlGnBu\", vmax=.4, center=0, square=True, linewidths=.1, cbar_kws={\"shrink\":0.5})","037f3570":"links = Correlation.stack().reset_index()\nlinks.columns = ['var1', 'var2','value']","8558322b":"links_filtered=links.loc[ (links['value'] > 0.95) & (links['var1'] != links['var2']) ]\nlinks_filtered","cc54e11f":"# Creating a network graph to see the correlation\nimport networkx as nx\nG=nx.from_pandas_edgelist(links_filtered, 'var1', 'var2')\nplt.subplots(figsize=(8,6))\nnx.draw(G, with_labels=True, node_color='green', node_size=500, edge_color='black', linewidths=6, font_size=12)","400d8c69":"#FUNCTIONS TAB\ndef set_plot_sizes(sml, med, big):\n    plt.rc('font', size=sml)          # controls default text sizes\n    plt.rc('axes', titlesize=sml)     # fontsize of the axes title\n    plt.rc('axes', labelsize=med)    # fontsize of the x and y labels\n    plt.rc('xtick', labelsize=sml)    # fontsize of the tick labels\n    plt.rc('ytick', labelsize=sml)    # fontsize of the tick labels\n    plt.rc('legend', fontsize=sml)    # legend fontsize\n    plt.rc('figure', titlesize=big)  # fontsize of the figure title\ndef add_datepart(df, fldnames, drop=True, time=False, errors=\"raise\"):\t\n    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n    the information from the date. This applies changes inplace.\n    Parameters:\n    -----------\n    df: A pandas data frame. df gain several new columns.\n    fldname: A string or list of strings that is the name of the date column you wish to expand.\n        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n    drop: If true then the original date column will be removed.\n    time: If true time features: Hour, Minute, Second will be added.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3\/11\/2000', '3\/12\/2000', '3\/13\/2000'], infer_datetime_format=False) })\n    >>> df\n        A\n    0   2000-03-11\n    1   2000-03-12\n    2   2000-03-13\n    >>> add_datepart(df, 'A')\n    >>> df\n        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n    >>>df2 = pd.DataFrame({'start_date' : pd.to_datetime(['3\/11\/2000','3\/13\/2000','3\/15\/2000']),\n                            'end_date':pd.to_datetime(['3\/17\/2000','3\/18\/2000','4\/1\/2000'],infer_datetime_format=True)})\n    >>>df2\n        start_date\tend_date    \n    0\t2000-03-11\t2000-03-17\n    1\t2000-03-13\t2000-03-18\n    2\t2000-03-15\t2000-04-01\n    >>>add_datepart(df2,['start_date','end_date'])\n    >>>df2\n    \tstart_Year\tstart_Month\tstart_Week\tstart_Day\tstart_Dayofweek\tstart_Dayofyear\tstart_Is_month_end\tstart_Is_month_start\tstart_Is_quarter_end\tstart_Is_quarter_start\tstart_Is_year_end\tstart_Is_year_start\tstart_Elapsed\tend_Year\tend_Month\tend_Week\tend_Day\tend_Dayofweek\tend_Dayofyear\tend_Is_month_end\tend_Is_month_start\tend_Is_quarter_end\tend_Is_quarter_start\tend_Is_year_end\tend_Is_year_start\tend_Elapsed\n    0\t2000\t    3\t        10\t        11\t        5\t            71\t            False\t            False\t                False\t                False\t                False\t            False\t            952732800\t    2000\t    3\t        11\t        17\t    4\t            77\t            False\t            False\t            False\t            False\t                False\t        False\t            953251200\n    1\t2000\t    3\t        11\t        13\t        0\t            73\t            False\t            False\t                False\t                False               \tFalse           \tFalse           \t952905600     \t2000       \t3\t        11      \t18  \t5           \t78          \tFalse\t            False           \tFalse           \tFalse               \tFalse          \tFalse           \t953337600\n    2\t2000\t    3\t        11\t        15\t        2           \t75          \tFalse           \tFalse               \tFalse               \tFalse               \tFalse               False           \t953078400      \t2000    \t4          \t13      \t1   \t5           \t92          \tFalse           \tTrue            \tFalse           \tTrue                \tFalse          \tFalse           \t954547200\n    \"\"\"\n    if isinstance(fldnames,str): \n        fldnames = [fldnames]\n    for fldname in fldnames:\n        fld = df[fldname]\n        fld_dtype = fld.dtype\n        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n            fld_dtype = np.datetime64\n\n        if not np.issubdtype(fld_dtype, np.datetime64):\n            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n        targ_pre = re.sub('[Dd]ate$', '', fldname)\n        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n        if time: attr = attr + ['Hour', 'Minute', 'Second']\n        for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) \/\/ 10 ** 9\n        if drop: df.drop(fldname, axis=1, inplace=True)\n            \n            \n            \n            \ndef train_cats(df):\n    \"\"\"Change any columns of strings in a panda's dataframe to a column of\n    categorical values. This applies the changes inplace.\n    Parameters:\n    -----------\n    df: A pandas dataframe. Any columns of strings will be changed to\n        categorical values.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category\n    \"\"\"\n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\n\n\ndef proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n  \n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res\n\ndef fix_missing(df, col, name, na_dict):\n    \n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict\n\ndef numericalize(df, col, name, max_n_cat):\n\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = pd.Categorical(col).codes+1\n        \ndef get_sample(df,n):\n    \"\"\" Gets a random sample of n rows from df, without replacement.\n    Parameters:\n    -----------\n    df: A pandas data frame, that you wish to sample from.\n    n: The number of rows you wish to sample.\n    Returns:\n    --------\n    return value: A random sample of n rows of df.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    >>> get_sample(df, 2)\n       col1 col2\n    1     2    b\n    2     3    a\n    \"\"\"\n    idxs = sorted(np.random.permutation(len(df))[:n])\n    return df.iloc[idxs].copy()\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\" Draws a representation of a random forest in IPython.\n    Parameters:\n    -----------\n    t: The tree you wish to draw\n    df: The data used to train the tree. This is used to get the names of the features.\n    \"\"\"\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))\n\ndef set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n))\n    \ndef reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n_samples))\n    \ndef parallel_trees(m, fn, n_jobs=8):\n        return list(ProcessPoolExecutor(n_jobs).map(fn, m.estimators_))\n","4cce66c7":"df_diamonds.price = np.log(df_diamonds.price)","c6041d17":"df, y, nas = proc_df(df_diamonds, 'price')","58287f2b":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(df, y)\nm.score(df,y)","39a8d914":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 10800  # 80:20 as train and validation dataset distribution\nn_trn = len(df)-n_valid\nraw_train, raw_valid = split_vals(df_diamonds, n_trn)\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","ed31c925":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","5735d727":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","80f87797":"M = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nM.fit(X_train, y_train)\nprint_score(M)","57a128fd":"fi = rf_feat_importance(m, df); fi[:10]","5adc2863":"fi.plot('cols', 'imp', figsize=(10,6), legend=False);","6d6623b4":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","1af7562e":"plot_fi(fi[:20]);","05d37733":"### here we are keeping feature importance more than .005 and then check what our distrubtion says !!!\nto_keep = fi[fi.imp>0.005].cols; len(to_keep)","8ed7373c":"## Now we're keeping only these 5 variables into account for our next Random forest model.\ndf_keep = df[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","99a0e3e9":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5,\n                          n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","f2225272":"fi = rf_feat_importance(m, df_keep)\nplot_fi(fi);","bdb15aad":"import scipy\nfrom scipy.cluster import hierarchy as hc","33c98a57":"corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","f3364404":"S = RandomForestRegressor(n_jobs=-1, random_state=75, n_estimators=80, min_samples_leaf=2, \n                          max_features='auto')\n%time S.fit(X_train, y_train)\nprint_score(S)","6e0d5871":"P = RandomForestRegressor(n_jobs=-1, random_state=75, n_estimators=80, min_samples_leaf=4, max_features='auto')\n%time P.fit(X_train, y_train)\nprint_score(P)","782aad71":"Q = RandomForestRegressor(n_jobs=-1, random_state=75, n_estimators=75, min_samples_leaf=3, max_features=.75)\nQ.fit(X_train, y_train)\nprint_score(Q)","8a4ade86":"from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor","6f9483ec":"M = ExtraTreesRegressor(n_jobs=-1, random_state=75, n_estimators=75, min_samples_leaf=3, max_features=.75)\nM.fit(X_train, y_train)\nprint_score(M)","f3bb07e6":"M.score(X_valid, y_valid)","d445fd50":"results = pd.DataFrame({'Models by Shubham': ['RandomForestRegressor-S', 'RandomForestRegressor-P', 'RandomForestRegressor-Q','ExtraTreeRegressor'],\n                        'Score': [S.score(X_valid, y_valid), P.score(X_valid, y_valid), Q.score(X_valid, y_valid), \n                                  M.score(X_valid, y_valid)]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df","350e11de":"## Pre Processing: ","b3afeef4":"### Breakpoints of 5000, 10000, 150000, and above 15000 is considered to be low, mid, high, and prime which is also clearly seen from the above histogram plot of Price.","909aa392":"#### Same as previously explained, G - Green color category got the highest weigtage in amount of numbers counting.","e8432236":"#### Fair enough, informative but still not found any conclusive statement.","cd56a06e":"### From the top 4 models, we can see that Regressor Q [no of trees =75, min_samples_leaf = 3 and max features used = .75] provides the highest accuracy of 73.22% in validation dataset.\n### However, we have also considered ExtraTrees Regressor model for the same, where no of trees = 75, min samples leaf = 3 and max features used = .75, we can see the highest accuracy of 81.71% in validation dataset.","cb4c3373":"#### we can see that almost 40% of the data of cut is related to Ideal cut and rest 25 25 under premium & Very Good as per the distribution of the variable we can see that least important is Fair. but still we're not sure yet. let's go ahead...","7abd8e1a":"## Random Forests\n## Base model - Let's try our model again, this time with separate training and validation sets.\n","613af87e":"#### Again, SI1 is on peak and got highest number of counts","66398a68":"### Now here comes to the very interesting part of the dataset, here we can see that these 4 variables are higly correlated with each other and with the excat correlation, strange but true, now we can also see the multicollinearity between the variables. we'll see the feature importance further.","644ae848":"### We're using random forest regression because our target variable is continous:","3158f4e0":"\n### Removing redundant features\u00b6\n### One thing that makes this harder to interpret is that there seem to be some variables with very similar meanings. Let's try to remove redundent features.","ec8f0fa0":"### Now here we're defining our Target Variable which is \"Price\", and try to observe its distribution and spread on the normal distributioncurve.\n\n### Target Variable - Price","8d2ef152":"#### Again good enough to checking the distribution of clarity on the basis of price but still not so informative.m","1fab6551":"#### Depth - range of 55 to 65 cover large amount of data distribution.","4f810c23":"## Interesting observation in variable X and Clarity , previously where x is less important than clarity now it get more importance and in carat too previously it was soo far from y now it is quite close to Y.","aa9f82e1":"### Shubham Paliwal\n### Machine Learning on Diamonds Dataset","45e9a9c8":"#### Table - more than 90% of table distribution is under the range of 50 to 70. fair enough, we'll move ahead.","f2e1c811":"### Split data into 80:20 as train and validation sets.","7f6760f7":"# Thank You","493e79d6":"### It becomes hard to comment on single values of a variable which is continuous in nature and can having too many values, that too analysing it in uni-variate fashion and commenting on a trend or general behaviour becomes tough. So, here categorising the Price(s) comes to rescue and will help us understand the behaviour of Price in a much convinient way.","d4901c8c":"### Variables Understanding","96a00f83":"### Now here we're using Extratrees classifier to check our model performance on more accurate scale.","ec4bd231":"### Here we can clearly see that there is nerly 37500 customers have their price in range 5000 to 10000. which is low range.","04d25483":"### Here we're come up with the decent amount of information that data is highly right skewed and almost 40% of the prices of the given dataset variables is under the bin range of 0 to 5000.\n### Here we need to work on it.","d41a43fd":"### Here we can see that this is the best optimal point at which RMSE is .1911 and accuracy of validation dataset is 81.71%.","ee74ba2c":"## here now we come to the important information which is feature importance and we can see that Y is having highest weightage with .82 followed by carat which is 0.11. \n### Let's plot it."}}