{"cell_type":{"a49e8abe":"code","9eab584d":"code","eb0df14c":"code","1e0e3c88":"code","ad9f2f2f":"code","e62bf49c":"code","42a63698":"code","8f6a8488":"code","fdf418e1":"code","613354ac":"markdown","9a5f75b5":"markdown","38286795":"markdown","df107572":"markdown","b609b82b":"markdown","fa502827":"markdown","acd3cc68":"markdown","aec98d93":"markdown","fc2f06da":"markdown","c1990107":"markdown","a7558ffd":"markdown","f3ef4e0e":"markdown","f837d99e":"markdown","cb0d3176":"markdown"},"source":{"a49e8abe":"\n# For data creation and other tasks\nimport numpy as np\nimport random\nimport scipy\nfrom scipy.stats import norm\nimport pandas as pd\n\n## Mertics to evaluate the models \nfrom sklearn.metrics import accuracy_score # for Logistic Regression\n\n# For plotting the graphs.. \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns \n\n# For spliting the data into 80:20 ratio \nfrom sklearn.model_selection import train_test_split","9eab584d":"def generate_data(n,m,theta):\n    X = []\n    # m = 3  #change to user defined \n    # n = 100   #change to user defined \n    theta = int(n*(theta))  #change to user defined \n    for i in range(0,n):\n        X_i = scipy.stats.norm.rvs(0,1,m)\n        X.append(X_i)\n    \n    beta = scipy.stats.norm.rvs(0,1,m)\n    # for simplicity I am not adding '1' to either beta or X rather directly adding it to the 'odds' that will\n    ## be used as y1 which will be passed through cost fn with theta which will define whether it will be \n    ### '1' or '0' (bernolli distribution)\n    odds =  (np.exp(1+ np.matmul(X,beta)) \/ ( 1 + np.exp(1+ np.matmul(X,beta)) )) \n    y1 = []\n    for i in odds:\n        if(i >= 0.5):\n            y1.append(1)\n        else:\n            y1.append(0)\n    df1 = pd.DataFrame(X)\n    df2 = pd.DataFrame(y1)\n    df1['Y'] = df2[0]\n    #df1.head()\n    #df1.tail()\n\n    ## Adding noise using theta \n    change = df1.sample(theta).index\n    \n    for i in change:\n        if(df1.loc[i,'Y'] == 0):\n            df1.loc[i,'Y'] = 1 \n        else:\n            df1.loc[i,'Y'] = 0 \n    return df1","eb0df14c":"def weightInitialization(n_features):\n    w = np.zeros((1,n_features))\n    b = 0\n    return w,b","1e0e3c88":"def sigmoid_activation(result):\n    final_result = 1\/(1+np.exp(-result))\n    return final_result","ad9f2f2f":"def predict(final_pred, m):\n    y_pred = np.zeros((1,m))\n    for i in range(final_pred.shape[1]):\n        if final_pred[0][i] > 0.5:\n            y_pred[0][i] = 1\n    return y_pred","e62bf49c":"lam = 0.1\ndef model_optimize_for_L1(w, b, X, Y):\n    m = X.shape[0]\n    \n    #Prediction\n    final_result = sigmoid_activation(np.dot(w,X.T)+b)\n    Y_T = Y.T\n    cost = (-1\/m)*(np.sum((Y_T*np.log(final_result)) + ((1-Y_T)*(np.log(1-final_result)))))  + (lam * (np.sum(w)))\n\n    #\n    \n    #Gradient calculation\n    dw = (1\/m)*(np.dot(X.T, (final_result-Y.T).T)) + lam\n    db = (1\/m)*(np.sum(final_result-Y.T))\n    \n    grads = {\"dw\": dw, \"db\": db}\n    \n    return grads, cost\ndef model_predict_l1(w, b, X, Y, learning_rate, no_iterations):\n    costs = []\n    for i in range(no_iterations):\n        #\n        grads, cost = model_optimize_for_L1(w,b,X,Y)\n        #\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        #weight update\n        w = w - (learning_rate * (dw.T))\n        b = b - (learning_rate * db)\n        #\n        \n        if (i % 100 == 0):\n            costs.append(cost)\n            #print(\"Cost after %i iteration is %f\" %(i, cost))\n    \n    #final parameters\n    coeff = {\"w\": w, \"b\": b}\n    gradient = {\"dw\": dw, \"db\": db}\n    \n    return coeff, gradient, costs","42a63698":"lam = 0.1\ndef model_optimize_for_L2(w, b, X, Y):\n    m = X.shape[0]\n    \n    #Prediction\n    final_result = sigmoid_activation(np.dot(w,X.T)+b)\n    Y_T = Y.T\n    cost = (-1\/m)*(np.sum((Y_T*np.log(final_result)) + ((1-Y_T)*(np.log(1-final_result)))))  + (lam * (np.sum(np.square(w))))\n\n    #\n    \n    #Gradient calculation\n    dw = (1\/m)*(np.dot(X.T, (final_result-Y.T).T)) + lam * w\n    db = (1\/m)*(np.sum(final_result-Y.T))\n    \n    grads = {\"dw\": dw, \"db\": db}\n    \n    return grads, cost\ndef model_predict_l2(w, b, X, Y, learning_rate, no_iterations):\n    costs = []\n    for i in range(no_iterations):\n        #\n        grads, cost = model_optimize_for_L2(w,b,X,Y)\n        #\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        #weight update\n        w = w - (learning_rate * (dw.T))\n        b = b - (learning_rate * db)\n        #\n        \n        if (i % 100 == 0):\n            costs.append(cost)\n            #print(\"Cost after %i iteration is %f\" %(i, cost))\n    \n    #final parameters\n    coeff = {\"w\": w, \"b\": b}\n    gradient = {\"dw\": dw, \"db\": db}\n    \n    return coeff, gradient, costs","8f6a8488":"def Log_reg_L1_L2(n,m,theta,learning_rate,no_iterations):\n    df1 = generate_data(n,m,theta)\n        \n    X1 = df1.iloc[:,0:m].values\n    y1 = df1.iloc[:,m].values\n        \n    X_train,X_test,Y_train,Y_test = train_test_split(X1,y1,test_size = 0.2)\n    n_features = X_train.shape[1]\n    print('Number of Features', n_features)\n    w, b = weightInitialization(n_features)\n    #Gradient Descent for L1 reguralisation\n    coeff1, gradient1, costs1 = model_predict_l1(w, b, X_train, Y_train,learning_rate=0.0001,no_iterations=45000)\n     \n    #Final prediction\n    w1 = coeff1[\"w\"]\n    b1 = coeff1[\"b\"]\n    print('Optimized weights - Beta', w1)\n    print('Optimized intercept',b1)\n    #\n    final_train_pred = sigmoid_activation(np.dot(w1,X_train.T)+b1)\n    final_test_pred = sigmoid_activation(np.dot(w1,X_test.T)+b1)\n    #\n    m_tr =  X_train.shape[0]\n    m_ts =  X_test.shape[0]\n       #\n    y_tr_pred = predict(final_train_pred, m_tr)\n    print('Training Accuracy',accuracy_score(y_tr_pred.T, Y_train))\n    #\n    y_ts_pred = predict(final_test_pred, m_ts)\n    print('Test Accuracy',accuracy_score(y_ts_pred.T, Y_test))\n    \n    plt.plot(costs1)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title('Cost reduction over time using L1 Regularisation')\n    plt.show()\n    \n    \n    #Gradient Descent for L2 reguralisation\n    coeff2, gradient2, costs2 = model_predict_l2(w, b, X_train, Y_train,learning_rate=0.0001,no_iterations=45000)\n    #Final prediction\n    w2 = coeff2[\"w\"]\n    b2 = coeff2[\"b\"]\n    print('Optimized weights - Beta', w2)\n    print('Optimized intercept',b2)\n    #\n    final_train_pred = sigmoid_activation(np.dot(w2,X_train.T)+b2)\n    final_test_pred = sigmoid_activation(np.dot(w2,X_test.T)+b2)\n    #\n    m_tr =  X_train.shape[0]\n    m_ts =  X_test.shape[0]\n       #\n    y_tr_pred = predict(final_train_pred, m_tr)\n    print('Training Accuracy',accuracy_score(y_tr_pred.T, Y_train))\n    #\n    y_ts_pred = predict(final_test_pred, m_ts)\n    print('Test Accuracy',accuracy_score(y_ts_pred.T, Y_test))\n\n    plt.plot(costs2)\n    plt.ylabel('cost2')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title('Cost reduction over time using L2 Regularisation')\n    plt.show()","fdf418e1":"## when using L1 reg and L2 reg\nLog_reg_L1_L2(200,3,0.01,learning_rate=0.0001,no_iterations=5000)","613354ac":"# <font color='darkorange'> Why Do we need L1 L2 Regularisation ??","9a5f75b5":"## Though both L1 reg. and L2 reg. should have the same effects on the training and test accuracies . The affect on beta\/weights have been that they have been shrinked a little. The effects were not that visible as the data is between [0,1] and thus weights\/beta are also between the range [0,1] and near 0.0.","38286795":"## Main Call function","df107572":"## For L1 Regularisation:- ","b609b82b":"## <font color='green'> Firstly why do we need Regularisation ??\n    \n<font size='4'>The biggest reasons for regularization are <br> 1) to avoid overfitting by not generating high coefficients for predictors that are sparse.<br> 2) to stabilize the estimates especially when there's collinearity in the data.\n\n<li>For Point No.1 - is inherent in the regularization framework. Since there are two forces pulling each other in the objective function, if there's no meaningful loss reduction, the increased penalty from the regularization term wouldn't improve the overall objective function. This is a great property since a lot of noise would be automatically filtered out from the model.\n<br>\n<li>To give you an example for 2), if you have two predictors that have same values, if you just run a regression algorithm on it since the data matrix is singular, your beta coefficients will be Inf if you try to do a straight matrix inversion. But if you add a very small regularization lambda to it, you will get stable beta coefficients with the coefficient values evenly divided between the equivalent two variables.\n    \n\n","fa502827":"## Prediction function","acd3cc68":"# <font color='torques blue'>Do Comment and UpVote the kernel if it was informative. Thank You\ud83d\ude4f\ud83d\ude03  <br> Do Read Part 1: Of this notebook --- https:\/\/www.kaggle.com\/achintyatripathi\/logistic-reg-data-gen-to-modelling-from-scratch","aec98d93":"## Sigmoid function","fc2f06da":"## <font color='green'> When Do we need L1 and L2 regularisation?\n    \n![](https:\/\/miro.medium.com\/max\/550\/1*-LydhQEDyg-4yy5hGEj5wA.png)\n<font size='4'>\nThe main intuitive difference between the L1 and L2 regularization is that L1 regularization tries to estimate the median of the data while the L2 regularization tries to estimate the mean of the data to avoid overfitting.\n<br><br>    \nAnother difference between them is that L1 regularization helps in feature selection by eliminating the features that are not important. This is helpful when the number of feature points are large in number.\n    \nNote: You can read more <a herf=\"https:\/\/medium.com\/analytics-vidhya\/l1-vs-l2-regularization-which-is-better-d01068e6658c#:~:text=As%20we%20can%20see%20from,Wj)%20in%20the%20cost%20function.\">Here<\/a> ","c1990107":"## L2 Regularisation","a7558ffd":"## Weight Initializations","f3ef4e0e":"## Now let's Start with the coding. ","f837d99e":"## 1. Data Generation","cb0d3176":"## Hope this kernel was helpfull and helped you all in some or the other way. \n## Do Comment and UpVote the kernel. Thank You\ud83d\ude03\ud83d\ude4f\n## Do Read Part 1: Of this notebook. https:\/\/www.kaggle.com\/achintyatripathi\/logistic-reg-data-gen-to-modelling-from-scratch"}}