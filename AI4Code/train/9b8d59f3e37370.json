{"cell_type":{"66beb57e":"code","5b553232":"code","663d3c5e":"code","710200f1":"code","cf991ede":"code","41beb484":"code","1e247929":"code","b9f1f8c2":"code","3c8c33f6":"code","d7d5f297":"code","3d67bd2d":"code","baa12f97":"code","0d3253b1":"code","578e6257":"code","0316b225":"code","d1cff129":"code","ad49bee2":"markdown","41d237fb":"markdown","5561a2a5":"markdown","e213b236":"markdown","e41cbced":"markdown","c834e743":"markdown","c9f5d7dd":"markdown","75f59c41":"markdown","b668bc04":"markdown","f13c5832":"markdown","e0de6f3d":"markdown","0eb9075e":"markdown","a2ecf2cf":"markdown","09aaccae":"markdown","669f6565":"markdown","4d95de7f":"markdown","5db57d40":"markdown","a37cb8be":"markdown"},"source":{"66beb57e":"#The standard imports Kaggle give you when you start a kernel\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5b553232":"import warnings\nfrom matplotlib import pyplot as plt\nimport copy\nimport scipy.stats as ss\n#from astropy.time import Time\n#import fbprophet as fbp\n#import FATS\n#import cesium\n","663d3c5e":"print('Getting Base (meta) dataFrame')\n#bdf=pd.read_csv('..\/input\/test_set_metadata.csv')\nbdf=pd.read_csv('..\/input\/training_set_metadata.csv')\n\nprint(bdf.shape)\n#print('Postponing getting raw (lightcurve) dataFrame')\nrdf=pd.read_csv('..\/input\/training_set.csv')\n#print(rdf.shape)\nprint(rdf.shape)","710200f1":"lep=2\nhep=5\n# originally planned on 2, 4 based on DDF data\n# based on all data going with 2, 5\n\ndef filterRawDf(ordf, lowEngPb=lep, highEngPb=hep):\n    #frdf=rdf[rdf['passband'] in [2,5]]\n    rdf=copy.deepcopy(ordf)\n    filterLow = rdf.loc[:,'passband']==lowEngPb\n    filterHigh = rdf.loc[:,'passband']==highEngPb\n    filterPb = filterLow | filterHigh\n    frdf=rdf.loc[filterPb,:]\n    return frdf\n\nfrdf = filterRawDf(rdf)\nfrdf.shape\n\n#t=Time(frdf.loc[:,'mjd'])\n#print(t.isot)\n#frdf=frdf.rename(columns={'mjd':'ds', 'flux':'y'})","cf991ede":"#get light curve data for one object_id from the raw data\n#this can be used with either rdf or filtered raw data (frdf)\ndef getLCDF(ordf, objid, show=False):\n    rdf=copy.deepcopy(ordf)\n    lcdf=rdf[rdf['object_id']==objid]\n    if show:\n        plt.plot(lcdf.loc[:,'mjd'],lcdf.loc[:,'flux'])\n        plt.show()\n\n        \n    return lcdf\n        \nelcdf=getLCDF(frdf,615, show=True)\nprint(elcdf.shape)\n#print(frdf.shape)\nelcdf=getLCDF(frdf, 1019335, show=True)\nprint(elcdf.shape)","41beb484":"def divideLcdf(elcdf, ddf, lep=2, hep=5):\n    lcdf=copy.deepcopy(elcdf)\n    #this simple date cutting works on the ddf objects\n    if ddf:\n        minDate=np.min(elcdf.loc[:,'mjd'])\n        maxDate=np.max(elcdf.loc[:,'mjd'])\n\n        halfPoint=np.average([minDate, maxDate])\n        firstCut=np.average([minDate, halfPoint])\n        secondCut=np.average([halfPoint, maxDate])\n        minDate=np.min(elcdf.loc[:,'mjd'])\n        maxDate=np.max(elcdf.loc[:,'mjd'])\n\n        halfPoint=np.average([minDate, maxDate])\n        firstCut=np.average([minDate, halfPoint])\n        secondCut=np.average([halfPoint, maxDate])\n\n        #early\n        efilter=elcdf.loc[:,'mjd']<=firstCut\n        #late\n        lfilter=elcdf.loc[:,'mjd']>=secondCut\n        #mid\n        mfilter=(efilter | lfilter)==False\n    \n        edf=elcdf.loc[efilter]\n        mdf=elcdf.loc[mfilter]\n        ldf=elcdf.loc[lfilter]\n        \n        ledf = edf[edf['passband']==lep]\n        hedf = edf[edf['passband']==hep]\n        lmdf = mdf[mdf['passband']==lep]\n        hmdf = mdf[mdf['passband']==hep]\n        lldf = ldf[ldf['passband']==lep]\n        hldf = ldf[ldf['passband']==hep]\n    \n    #using the datecutting method often leads to zero population sizes with non-ddf objects\n    else:\n        \n        lowdf=elcdf[elcdf['passband']==lep]\n        highdf=elcdf[elcdf['passband']==hep]\n        lenLow=lowdf.shape[0]\n        lenHigh=highdf.shape[0]\n        \n        minSizeLow = int(lenLow \/ 3)\n        minSizeHigh = int(lenHigh \/ 3)\n        \n        lldf=lowdf.nlargest(minSizeLow, 'mjd')\n        hldf=highdf.nlargest(minSizeHigh, 'mjd')\n        ledf=lowdf.nsmallest(minSizeLow, 'mjd')\n        hedf=highdf.nsmallest(minSizeHigh, 'mjd')\n        lmdf=lowdf.nlargest(lenLow-minSizeLow, 'mjd').nsmallest(lenLow-2*minSizeLow, 'mjd')\n        hmdf=highdf.nlargest(lenHigh-minSizeHigh, 'mjd').nsmallest(lenHigh-2*minSizeHigh, 'mjd')\n    \n    return ledf, hedf, lmdf, hmdf, lldf, hldf\n\nledf, hedf, lmdf, hmdf, lldf, hldf=divideLcdf(elcdf, 0)    \nprint(ledf.shape)\nprint(lmdf.shape)\nprint(lldf.shape)\nprint(hedf.shape)\nprint(hmdf.shape)\nprint(hldf.shape)","1e247929":"def getSubPopFeats(pbdf, outSig=3.0):\n    \n    average=np.average(pbdf.loc[:,'flux'])\n    median=np.median(pbdf.loc[:,'flux'])\n    stdev=np.std(pbdf.loc[:,'flux'])\n    maxflux=np.max(pbdf.loc[:,'flux'])\n    minflux=np.min(pbdf.loc[:,'flux'])\n    stdflerr=np.std(pbdf.loc[:,'flux_err'])\n    medflerr=np.median(pbdf.loc[:,'flux_err'])\n    \n    #We want a means to extract the rate of decay or rise from minima or maxima\n    #This is grabbing within the population\n    #We also will look between populations\n    maxmjd=np.max(pbdf[pbdf['flux']==maxflux].loc[:,'mjd'])\n    minmjd=np.max(pbdf[pbdf['flux']==minflux].loc[:,'mjd'])\n    \n    #at what date does the max occur?\n    aftmaxdf=pbdf[pbdf['mjd']>maxmjd]\n    \n    #if there are data points after the max, what is the value and date of the lowest?\n    if aftmaxdf.shape[0]>0:\n        minaft=np.min(aftmaxdf.loc[:,'flux'])\n        aftminmjd=np.min(aftmaxdf[aftmaxdf['flux']==minaft].loc[:,'mjd'])\n        #(val at t0 - val at t1) \/ (t0 - t1) sb neg\n        decaySlope=(maxflux-minaft)\/(maxmjd-aftminmjd)\n    \n    else:\n        decaySlope=0\n        \n    aftmindf=pbdf[pbdf['mjd']<minmjd]\n    if aftmindf.shape[0]>0:\n        maxaft=np.max(aftmindf.loc[:,'flux'])\n        aftmaxmjd=np.max(aftmindf[aftmindf['flux']==maxaft].loc[:,'mjd'])\n        #(val at t0 - val at t1) \/ (t0 - t1) sb pos\n        riseSlope=(minflux - maxaft)\/(aftmaxmjd-minmjd)\n    \n    else:\n        riseSlope=0\n        \n    return average, stdev, median, medflerr, stdflerr, maxflux, \\\n            maxmjd, decaySlope, minflux, minmjd, riseSlope\n\na,b,c,d,e,f,g, h,i,j,k=getSubPopFeats(hmdf)\nprint(a)\nprint(b)\nprint(c)\nprint(d)\nprint(e)\nprint(f)\nprint(g)\nprint(h)\nprint(i)\nprint(j)\nprint(k)\n","b9f1f8c2":"def processLc(objid, elcdf, ddf, lep=2, hep=5):\n    \n    lcdf=copy.deepcopy(elcdf)\n    \n    #feature borrowed from Grzegorz Sionkowski (..\/sionek)\n    #dt[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n    #detectMjds=elcdf[elcdf['detected']==1].loc[:,'mjd']\n    #deltaDetect=np.max(detectMjds) - np.min(detectMjds)\n    \n    #divide the incoming light curve to 6 subpopulations\n    ledf, hedf, lmdf, hmdf, lldf, hldf=divideLcdf(lcdf, ddf,lep=lep, hep=hep)\n    #return average, stdev, median, medflerr, stdflerr, maxflux, \\\n    #        maxmjd, decayslope, minflux, minmjd, riseSlope\n    \n    leavg, lestd, lemed, lemfl, lesfl, lemax, lemxd, ledsl, lemin, lemnd, lersl=getSubPopFeats(ledf)\n    heavg, hestd, hemed, hemfl, hesfl, hemax, hemxd, hedsl, hemin, hemnd, hersl=getSubPopFeats(hedf)\n    lmavg, lmstd, lmmed, lmmfl, lmsfl, lmmax, lmmxd, lmdsl, lmmin, lmmnd, lmrsl=getSubPopFeats(lmdf)\n    hmavg, hmstd, hmmed, hmmfl, hmsfl, hmmax, hmmxd, hmdsl, hmmin, hmmnd, hmrsl=getSubPopFeats(hmdf)\n    llavg, llstd, llmed, llmfl, llsfl, llmax, llmxd, lldsl, llmin, llmnd, llrsl=getSubPopFeats(lldf)\n    hlavg, hlstd, hlmed, hlmfl, hlsfl, hlmax, hlmxd, hldsl, hlmin, hlmnd, hlrsl=getSubPopFeats(hldf)\n    \n    \n    feats= [objid, leavg, lestd, lemed, lemfl, lesfl, lemax, \n            lemxd, ledsl, lemin, lemnd, lersl,\n            heavg, hestd, hemed, hemfl, hesfl, hemax, hemxd,\n            hedsl, hemin, hemnd, hersl,\n            lmavg, lmstd, lmmed, lmmfl, lmsfl, lmmax, lmmxd,\n            lmdsl, lmmin, lmmnd, lmrsl,\n            hmavg, hmstd, hmmed, hmmfl, hmsfl, hmmax, hmmxd, \n            hmdsl, hmmin, hmmnd, hmrsl,\n            llavg, llstd, llmed, llmfl, llsfl, llmax, llmxd, \n            lldsl, llmin, llmnd, llrsl,\n            hlavg, hlstd, hlmed, hlmfl, hlsfl, hlmax, hlmxd, \n            hldsl, hlmin, hlmnd, hlrsl]\n    \n    return feats\n\nfeats=processLc(1019335, elcdf, 0)\n\nprint(feats)\n    ","3c8c33f6":"from io import StringIO\nfrom csv import writer \nimport time\n\ndef writeAChunk(firstRecord, lastRecord, bdf, frdf, statusFreq=500):\n    output = StringIO()\n    csv_writer = writer(output)\n\n    fdf=pd.DataFrame(columns=['objid', 'leavg', 'lestd', 'lemed', 'lemfl', 'lesfl', 'lemax', \n                'lemxd', 'ledsl', 'lemin', 'lemnd', 'lersl',\n                'heavg', 'hestd', 'hemed', 'hemfl', 'hesfl', 'hemax', 'hemxd',\n                'hedsl', 'hemin', 'hemnd', 'hersl',\n                'lmavg', 'lmstd', 'lmmed', 'lmmfl', 'lmsfl', 'lmmax', 'lmmxd',\n                'lmdsl', 'lmmin', 'lmmnd', 'lmrsl',\n                'hmavg', 'hmstd', 'hmmed', 'hmmfl', 'hmsfl', 'hmmax', 'hmmxd', \n                'hmdsl', 'hmmin', 'hmmnd', 'hmrsl',\n                'llavg', 'llstd', 'llmed', 'llmfl', 'llsfl', 'llmax', 'llmxd', \n                'lldsl', 'llmin', 'llmnd', 'llrsl',\n                'hlavg', 'hlstd', 'hlmed', 'hlmfl', 'hlsfl', 'hlmax', 'hlmxd', \n                'hldsl', 'hlmin', 'hlmnd', 'hlrsl'])\n\n    theColumns=fdf.columns\n    \n    csv_writer.writerow(theColumns)\n    started=time.time()\n    for rindex in range(firstRecord, lastRecord):\n        #if you want to monitor progress\n        #ddf 18 sec per 100 on my macAir\n        #non ddf 25 sec per 100 on my macAir\n        if rindex%statusFreq==(statusFreq-1):\n            print(rindex)\n            print(\"Processing took {:6.4f} secs, records = {}\".format((time.time() - started), statusFreq))\n            started=time.time()\n            #fdf=pd.merge(fdf, tdf, on='key')\n        objid = bdf.loc[rindex,'object_id']\n        ddf=bdf.loc[rindex,'ddf']==1\n        #ig=bdf.loc[rindex,'hostgal_specz']==0\n        lcdf=getLCDF(frdf, objid)\n        feats=processLc(objid, lcdf, ddf)\n        #fdf.loc[rindex,:]=feats\n        csv_writer.writerow(feats)\n\n    output.seek(0) # we need to get back to the start of the BytesIO\n    chdf = pd.read_csv(output)\n    chdf.columns=theColumns\n    \n    return chdf\n\ntheColumns=['objid', 'leavg', 'lestd', 'lemed', 'lemfl', 'lesfl', 'lemax', \n                'lemxd', 'ledsl', 'lemin', 'lemnd', 'lersl',\n                'heavg', 'hestd', 'hemed', 'hemfl', 'hesfl', 'hemax', 'hemxd',\n                'hedsl', 'hemin', 'hemnd', 'hersl',\n                'lmavg', 'lmstd', 'lmmed', 'lmmfl', 'lmsfl', 'lmmax', 'lmmxd',\n                'lmdsl', 'lmmin', 'lmmnd', 'lmrsl',\n                'hmavg', 'hmstd', 'hmmed', 'hmmfl', 'hmsfl', 'hmmax', 'hmmxd', \n                'hmdsl', 'hmmin', 'hmmnd', 'hmrsl',\n                'llavg', 'llstd', 'llmed', 'llmfl', 'llsfl', 'llmax', 'llmxd', \n                'lldsl', 'llmin', 'llmnd', 'llrsl',\n                'hlavg', 'hlstd', 'hlmed', 'hlmfl', 'hlsfl', 'hlmax', 'hlmxd', \n                'hldsl', 'hlmin', 'hlmnd', 'hlrsl']\n\nfdf=pd.DataFrame(columns=theColumns)\nchunksize=2616\nfirstLoop=0\nlastLoop=3\nloops=lastLoop-firstLoop\nveryFirstRow=firstLoop*chunksize\nveryLastRow=lastLoop*chunksize-1\nfor i in range(firstLoop, lastLoop):\n    startRow=i*chunksize\n    stopRow=(i+1)*chunksize\n    chdf=writeAChunk(startRow, stopRow, bdf, frdf, statusFreq=int(chunksize\/2))\n    fdf= pd.concat([fdf, chdf])\n    print(fdf.shape)\n","d7d5f297":"\nfdf=fdf.rename({'objid':'object_id'},axis=1)\nbdf.loc[:,'object_id']=bdf.loc[:,'object_id'].astype(str)\nfdf.loc[:,'object_id']=fdf.loc[:,'object_id'].astype(str)\n#DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)[source]\u00b6\nmdf=bdf.merge(fdf, sort=False)\nprint(mdf.shape)\nmdf.head()","3d67bd2d":"def testForOutlier(bdf, energy='high', sigmas=1.0):\n    \n    if energy=='high':\n        valCols=['heavg', 'hmavg', 'hlavg']\n        sigCols=['hestd', 'hmstd', 'hlstd']\n    else:\n        valCols=['leavg', 'lmavg', 'llavg']\n        sigCols=['lestd', 'lmstd', 'llstd']\n    \n    fdf=copy.deepcopy(bdf)\n    \n\n    \n    fdf.loc[:,energy + 'Energy_transitory_' + str(round(sigmas,1)) + '_TF']=False\n    for i in range(len(valCols)):\n        fdf.loc[:,'min' + str(valCols[i])] = fdf.loc[:,valCols[i]] - sigmas*fdf.loc[:,sigCols[i]]\n        \n        fdf.loc[:,'max' + str(valCols[i])] = fdf.loc[:,valCols[i]] + sigmas*fdf.loc[:,sigCols[i]]\n    \n    for i in range(len(valCols)):\n        #fdf.loc[:,'earlySet']=range(fdf.loc[:,'minX100' + str(valCols[0])],fdf.loc[:, 'maxX100' + str(valCols[0])])\n        #earlyMaxLessThanMedMin\n        for j in range(len(valCols)):\n            if j!=i:\n                \n                maxFailsOverlap=fdf.loc[:,'max' + str(valCols[i])]<fdf.loc[:,'min' + str(valCols[j])]\n                minFailsOverlap=fdf.loc[:,'min' + str(valCols[i])]>fdf.loc[:,'max' + str(valCols[j])]\n                theValue= (fdf.loc[:,energy + 'Energy_transitory_' + str(round(sigmas,1)) + '_TF'] | minFailsOverlap | maxFailsOverlap)\n                #theValue=theValue.astype(str)\n                fdf.loc[:,energy + 'Energy_transitory_' + str(round(sigmas,1)) + '_TF']=theValue\n                #fdf.loc[:,energy + '_' + str(valCols[i]) + '_' + str(valCols[j])] = str(theValue) + \\\n                #+ '_' + str(maxFailsOverlap) + '_'+ str(minFailsOverlap)\n    for i in range(len(valCols)):\n        fdf=fdf.drop('min' + str(valCols[i]), axis=1)\n        fdf=fdf.drop('max' + str(valCols[i]), axis=1)\n        \n    return fdf\n\nenergy='high'\nsigmas=1.0\nfdf=testForOutlier(mdf)\nfdf.shape\nprint(fdf.loc[:,energy + 'Energy_transitory_' + str(round(sigmas,1)) + '_TF'].sum())\n\nsigmas=1.5\nfdf=testForOutlier(fdf, energy=energy, sigmas=sigmas)\nprint(fdf.loc[:,energy + 'Energy_transitory_' + str(round(sigmas,1)) + '_TF'].sum())\n\nenergy='low'\nsigmas=1.0\nfdf=testForOutlier(fdf, energy=energy, sigmas=sigmas)\nprint(fdf.loc[:,energy + 'Energy_transitory_' + str(round(sigmas,1)) + '_TF'].sum())\n\nsigmas=1.5\nfdf=testForOutlier(fdf, energy=energy, sigmas=sigmas)\nprint(fdf.loc[:,energy + 'Energy_transitory_' + str(round(sigmas,1)) + '_TF'].sum())\n\nprint(fdf.shape)\nfdf.head()","baa12f97":"fdf.loc[:,'outlierString']=fdf.loc[:,'highEnergy_transitory_1.5_TF'].astype(str) + \\\n                             fdf.loc[:,'highEnergy_transitory_1.0_TF'].astype(str) + \\\n                             fdf.loc[:,'lowEnergy_transitory_1.5_TF'].astype(str) + \\\n                             fdf.loc[:,'lowEnergy_transitory_1.0_TF'].astype(str)\n\n\ndef getOutlierScore(row):\n    tdict={'TrueTrueTrueTrue':8, 'FalseTrueTrueTrue':7, 'TrueTrueFalseTrue':7,\n       'FalseTrueFalseTrue':6, 'FalseFalseTrueTrue':3, 'TrueTrueFalseFalse':3,\n       'FalseTrueFalseFalse':3, 'FalseFalseFalseTrue':3, 'FalseFalseFalseFalse':0}\n    return tdict[row['outlierString']]\n\nfdf['outlierScore']=fdf.apply(getOutlierScore, axis=1)\n    \nfdf=fdf.drop('outlierString', axis=1)\n\n#fdf.to_csv('fastestFeatureTableWithTransitoryFlags.csv')\nprint(fdf.shape)\nprint(fdf.columns)\nprint(np.average(fdf.loc[:,'outlierScore']))\nprint(np.min(fdf.loc[:,'outlierScore']))\nprint(np.max(fdf.loc[:,'outlierScore']))\nprint(np.median(fdf.loc[:,'outlierScore']))","0d3253b1":"fdf['hipd']=0\nfdf['hipr']=0\nfdf['htpd']=0\nfdf['htpr']=0\n\nfdf['lipd']=0\nfdf['lipr']=0\nfdf['ltpd']=0\nfdf['ltpr']=0\n\noutlierFilter=(fdf['outlierScore']>0)\nprint(outlierFilter.sum())\n\nhipdFilter = (fdf['hmmax']>fdf['hemax']) & (fdf['hmmax']>fdf['hlmax']) & outlierFilter\nhtpdFilter = (fdf['hemax']>fdf['hmmax']) & (fdf['hemax']>fdf['hlmax']) & outlierFilter\nlipdFilter = (fdf['lmmax']>fdf['lemax']) & (fdf['lmmax']>fdf['llmax']) & outlierFilter\nltpdFilter = (fdf['lemax']>fdf['lmmax']) & (fdf['lemax']>fdf['llmax']) & outlierFilter\n\nprint(hipdFilter.sum())\nprint(htpdFilter.sum())\nprint(lipdFilter.sum())\nprint(ltpdFilter.sum())\n\n","578e6257":"#peak to peak\n#these are light curves where the peak was in the middle\nfdf.loc[hipdFilter,'hipd']=(fdf.loc[hipdFilter,'hmmax']-fdf.loc[hipdFilter,'hlmax']) \/ \\\n     (fdf.loc[hipdFilter,'hmmxd']-fdf.loc[hipdFilter,'hlmxd'])\nfdf.loc[lipdFilter,'lipd']=(fdf.loc[lipdFilter,'lmmax']-fdf.loc[lipdFilter,'llmax']) \/ \\\n     (fdf.loc[lipdFilter,'lmmxd']-fdf.loc[lipdFilter,'llmxd'])\n\n#these are light curves where the peak was in the beginning\nfdf.loc[htpdFilter,'hipd']=(fdf.loc[htpdFilter,'hemax']-fdf.loc[htpdFilter,'hmmax']) \/ \\\n     (fdf.loc[htpdFilter,'hemxd']-fdf.loc[htpdFilter,'hmmxd'])\nfdf.loc[ltpdFilter,'lipd']=(fdf.loc[ltpdFilter,'lemax']-fdf.loc[ltpdFilter,'lmmax']) \/ \\\n     (fdf.loc[ltpdFilter,'lemxd']-fdf.loc[ltpdFilter,'lmmxd'])\nfdf.loc[htpdFilter,'htpd']=(fdf.loc[htpdFilter,'hmmax']-fdf.loc[htpdFilter,'hlmax']) \/ \\\n     (fdf.loc[htpdFilter,'hmmxd']-fdf.loc[htpdFilter,'hlmxd'])\nfdf.loc[ltpdFilter,'ltpd']=(fdf.loc[ltpdFilter,'lmmax']-fdf.loc[ltpdFilter,'llmax']) \/ \\\n     (fdf.loc[ltpdFilter,'lmmxd']-fdf.loc[ltpdFilter,'llmxd'])\n\n#print(fdf.loc[lipdFilter,'lipd'])\nfdf[outlierFilter].head()","0316b225":"hiprFilter = (fdf['hmmin']<fdf['hemin']) & (fdf['hmmin']<fdf['hlmin']) & outlierFilter\nhtprFilter = (fdf['hemin']<fdf['hmmin']) & (fdf['hemin']<fdf['hlmin']) & outlierFilter\nliprFilter = (fdf['lmmin']<fdf['lemin']) & (fdf['lmmin']<fdf['llmin']) & outlierFilter\nltprFilter = (fdf['lemin']<fdf['lmmin']) & (fdf['lemin']<fdf['llmin']) & outlierFilter\n\n#these are light curves where the peak was in the middle\nfdf.loc[hipdFilter,'hipr']=(fdf.loc[hipdFilter,'hmmin']-fdf.loc[hipdFilter,'hlmin']) \/ \\\n     (fdf.loc[hipdFilter,'hmmnd']-fdf.loc[hipdFilter,'hlmnd'])\nfdf.loc[lipdFilter,'lipr']=(fdf.loc[lipdFilter,'lmmin']-fdf.loc[lipdFilter,'llmin']) \/ \\\n     (fdf.loc[lipdFilter,'lmmnd']-fdf.loc[lipdFilter,'llmnd'])\n\n#these are light curves where the peak was in the beginning\nfdf.loc[htpdFilter,'hipr']=(fdf.loc[htpdFilter,'hemin']-fdf.loc[htpdFilter,'hmmin']) \/ \\\n     (fdf.loc[htpdFilter,'hemnd']-fdf.loc[htpdFilter,'hmmnd'])\nfdf.loc[ltpdFilter,'lipr']=(fdf.loc[ltpdFilter,'lemin']-fdf.loc[ltpdFilter,'lmmin']) \/ \\\n     (fdf.loc[ltpdFilter,'lemnd']-fdf.loc[ltpdFilter,'lmmnd'])\nfdf.loc[htpdFilter,'htpr']=(fdf.loc[htpdFilter,'hmmin']-fdf.loc[htpdFilter,'hlmin']) \/ \\\n     (fdf.loc[htpdFilter,'hmmnd']-fdf.loc[htpdFilter,'hlmnd'])\nfdf.loc[ltpdFilter,'ltpr']=(fdf.loc[ltpdFilter,'lmmin']-fdf.loc[ltpdFilter,'llmin']) \/ \\\n     (fdf.loc[ltpdFilter,'lmmnd']-fdf.loc[ltpdFilter,'llmnd'])\n\nfdf[outlierFilter].head()","d1cff129":"fdf.to_csv('newTrainFeatureOutputUnprocessed.csv')\nprint(fdf.shape)","ad49bee2":"## Use the filters to set decay values without ifs and loops","41d237fb":"### Test for transitory objects\n- what I'm doing here is a psuedo-box-plot without needing the entire distributions\n- I'm looking for populations that don't overlap one another\n- transitory events could be a downslope, and upslope, or and updown (and all be the same type of event)\n- I'll add this processing at a later revision","5561a2a5":"### Append the features to the base dataFrame\n- I know there's a more pythonic way to do it, but this was quick and I couldn't get it to work","e213b236":"### We need to divide the light curve\n- we want to see if beginning, middle, end are all the same (permanent objects)\n- or different (transitory events)\n- we note the ddf=1 and ddf=0 curves have different sampling methodologies\n","e41cbced":"**Output the csv for further analysis**","c834e743":"## After spending some time looking at other people's work, I note\n- The initial label for this approach, 'Fast Feature Extractor,' was a misnomer\n- Other people's approaches work better (inital score training GBM with these features was 1.958)\n\n### HOWEVER\n- That doesn't mean there's nothing of value here\n- Most of the public kernels take very similar approaches to one another\n- Reading an outsider's approach may trigger some new ideas for subject matter experts","c9f5d7dd":"## This method won't be used with the test data, but it's conventient here\n- the test data will use the 'fast_test_set_reading' method","75f59c41":"## Repeat decay stats for rise\n- Not sure if this will have significance but not ruling it out\n- Many of the rises will have negative slopes - which is counterintuitive at first but is because most outliers are flux spikes so even min to min is a decay\n- If an outlier is actually a trough then the 'decay' could have a positive slope","b668bc04":"**I need a few more libraries**","f13c5832":"\n# NOTE - [THIS METHOD](https:\/\/www.kaggle.com\/jimpsull\/fast-test-set-reading-merged-with-fast-extractor) is MUCH faster\n- but here I'm just going to use the loaded training set\n- will need to merge fast test set reading with this extractor","e0de6f3d":"### There is more processing to do, but it should be done array-style\n- should be able to induce amplitude and frequency from perHalfSigma, percOneSigma, percTwoSigma\n- should know if outliers are high or low based on median average comparison\n- etc","0eb9075e":"## This won't be used for the test data\n- but the principle will\n- 2 and 5 were chosen because they are far apart on the spectrum and they have a lot of data\n- 0 and 1 have significantly less data in the Test data set","a2ecf2cf":"### This method will get features on subpopulations\n- When the subpopulations for both low energy and high energy bands overlap each other, we have a steady state object, more on this later\n- This has evolved from the first draft (eliminating percent within different sigma bands)","09aaccae":"### The processing isn't done, but we will do as much as possible array style later\n- TBD how much slowdown this change causes.  I am now returning 12 instead of 7\n- Want to get decay rates within subsection in case peak at last subsection","669f6565":"## Consolidate four booleans into one score - arbitrary scoring choice\u00b6\n- The scoring dictionary is a potential tuning opportunity if this feature shows some power\n- 6,7,8 are all situations where both passbands show some outlier tendencies\n- 3 is where one passband shows outlier tendencies. Reasoning explained more in comments below","4d95de7f":"## Pseudo box-plot\n- I'm trying to see if the populations overlap\n- I'm saying if each population has a one sigma cushion around its average and it doesn't overlap either of the other - population cushions then its an outlier","5db57d40":"## Rate of decay\n- If outlierScore is zero we only care about the intraPop slopes and its amplitude related\n- If it is non-zero we want to see ","a37cb8be":"**Let's grab the training data**\n- If we can't get a feature table for the training data in ~15 minutes we'll be in trouble with the test data\n- There are roughly 500 times more objects in the test set"}}