{"cell_type":{"a4d81463":"code","9066d974":"code","6e04dd13":"code","bf35d626":"code","a09614d5":"code","0898f06f":"code","86db3961":"code","ab2bc37c":"code","0e64e888":"code","64e16803":"code","a8c20ae9":"code","28b179e6":"code","bbad1df4":"code","72b31256":"code","26bcc1b5":"code","3f8292eb":"code","3ff7d70d":"code","a12911e9":"code","17d8020d":"code","dcc942a6":"code","3e29e7ef":"code","b9926fe4":"code","e103b2af":"code","1eb7c54e":"code","41e8c804":"code","8fe9ea5e":"code","6bd0d6d3":"code","416db11d":"code","5e629bbd":"code","367f3e00":"code","9be02a6c":"code","78736502":"code","f74b4023":"code","4e18a147":"code","c78b5c66":"code","de5e591e":"code","48da37ce":"code","cd5b5308":"code","3b5a24c0":"code","f7d12131":"code","a93f66ac":"code","6b790102":"code","0e01fd0c":"code","55833329":"code","e4339d6a":"code","af99b6b8":"code","26e791ab":"code","f39c177f":"code","e3b81c77":"code","8349d1c8":"code","b1140ad1":"code","c91e19fd":"code","c7b5ddab":"code","d3136fe7":"code","c1958957":"code","1b11c005":"code","ceaba5d9":"code","6f6d4114":"markdown","64549ae9":"markdown","932509ce":"markdown"},"source":{"a4d81463":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9066d974":"import albumentations as A\nimport matplotlib.pyplot as plt\nimport cv2","6e04dd13":"import glob\nimgdir = \"\/kaggle\/input\/954471-bp\/\"\nimg_names = sorted(glob.glob(imgdir + \"*.jpg\"))\nlabel_names = sorted(glob.glob(imgdir+ \"*.txt\"))","bf35d626":"img_names","a09614d5":"BOX_COLOR = (255, 0, 0) # Red\nTEXT_COLOR = (255, 255, 255) # White\ndef read_label(labelfile):\n    content = open(labelfile).readlines()\n    list_with_all_boxes = []\n    list_names = []\n    for item in content:\n        classname, xcen, ycen, w, h = item.rstrip().split()\n        list_with_single_boxes = [float(xcen),float(ycen),float(w),float(h)]\n        list_with_all_boxes.append(list_with_single_boxes)\n        list_names.append(int(classname))\n    return list_names, list_with_all_boxes\n\ndef visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n    \"\"\"Visualizes a single bounding box on the image\"\"\"\n    imgSize = np.shape(img)\n    xcen, ycen, w, h = bbox\n    x_min = int(max(xcen - w\/2, 0)*imgSize[1])\n    x_max = int(min(xcen + w\/2, 1)*imgSize[1])\n    y_min = int(max(ycen - h\/2, 0)*imgSize[0])\n    y_max = int(min(ycen + h\/2, 1)*imgSize[0])\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n\n    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n    cv2.putText(\n        img,\n        text=class_name,\n        org=(x_min, y_min - int(0.3 * text_height)),\n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=0.35, \n        color=TEXT_COLOR, \n        lineType=cv2.LINE_AA,\n    )\n    return img\n\n\ndef visualize(image, bboxes, category_ids, category_id_to_name):\n    img = image.copy()\n    for bbox, category_id in zip(bboxes, category_ids):\n        class_name = category_id_to_name[category_id]\n        img = visualize_bbox(img, bbox, class_name)\n    plt.figure(figsize=(12, 12))\n    plt.axis('off')\n    plt.imshow(img)","0898f06f":"from PIL import Image\ncategory_id_to_name = {0: \"Rose\"}\nimage = np.array(Image.open(img_names[5]))\nclass_labels, bboxes = read_label(label_names[5])\nvisualize(image, bboxes, class_labels, category_id_to_name)","86db3961":"#Function for Albumentation (Augamentation )\nimg_size = 416\ntrain_transform = A.Compose([\n    A.Resize(width=img_size, height=img_size), #resize to square size\n    A.HorizontalFlip(p=0.5), #random horizontal flip \n    A.VerticalFlip(p=0.5), #random verizontal flip \n    A.ShiftScaleRotate(shift_limit=0.25, scale_limit=0.25, rotate_limit=45, p=0.5), #\u0e1a\u0e34\u0e14\u0e41\u0e25\u0e30\u0e22\u0e48\u0e2d\u0e02\u0e22\u0e32\u0e22 0.25 \u0e2b\u0e21\u0e38\u0e19\u0e44\u0e14\u0e49 45 \u0e2d\u0e07\u0e28\u0e32 \u0e04\u0e27\u0e32\u0e21\u0e19\u0e48\u0e32\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19 0.5\n    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5), #color\n    A.RandomBrightnessContrast(p=0.5), #brighth\n    A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.3), #color \n], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.2))","ab2bc37c":"#try train_transform function\ntransformed = train_transform(image=image, bboxes=bboxes, class_labels=class_labels)\ntransformed_image = transformed['image']\ntransformed_bboxes = transformed['bboxes']\ntransformed_class_labels = transformed['class_labels']\nvisualize(transformed_image, transformed_bboxes, transformed_class_labels, category_id_to_name) #show","0e64e888":"!mkdir ROSE_TRAIN #directory for keep image that already transform (train images)","64e16803":"#keep image that already transform (train images) to ROSE directory \noutdir = \".\/ROSE_TRAIN\/\"\nfor i in range(len(img_names)-8): #last 8 images spilt to be validation data and test data\n    image = np.array(Image.open(img_names[i]))\n    class_labels, bboxes = read_label(label_names[i])\n    onlyname = img_names[i].split('\/')[-1].split('.')[0]\n    for ii in range(20): #for 1 images -> agamentation 20 : 12 train images -> 240 train images\n        transformed = train_transform(image=image, bboxes=bboxes, class_labels=class_labels)\n        transformed_image = transformed['image']\n        transformed_bboxes = transformed['bboxes']\n        transformed_class_labels = transformed['class_labels']\n        transformed_name = onlyname+'_'+str(ii)\n        cv2.imwrite(outdir + transformed_name +'.jpg', cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)) \n        out_file = open(outdir + transformed_name +'.txt', 'w')\n        for iii in range(len(transformed_bboxes)):\n            bIn  =  A.augmentations.bbox_utils.convert_bbox_to_albumentations(transformed_bboxes[iii], 'yolo', img_size, img_size, check_validity=True)\n            bOut = A.augmentations.bbox_utils.convert_bbox_from_albumentations(bIn, 'yolo', img_size, img_size, check_validity=True)\n            out_file.write(str(0) + \" \" + \" \".join([str(b) for b in bOut]) + '\\n')\n        out_file.close()","a8c20ae9":"!ls .\/ROSE_TRAIN\/* | wc -l #numbers of train images = 480 (images 240 + labels 240)","28b179e6":"!ls .\/ROSE_TRAIN\/*","bbad1df4":"!mkdir ROSE_VAL #directory for keep validation image ","72b31256":"#Augumentation validation images \nval_transform = A.Compose([\n    A.Resize(width=img_size, height=img_size), #just resize to validation in model\n], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.2))","26bcc1b5":"#keep validation image to ROSE_VAL directory \noutdir = \".\/ROSE_VAL\/\"\nfor i in range(12, len(img_names)-2): #validation 6 images \/ Test 2 images\n    image = cv2.imread(img_names[i])\n    class_labels, bboxes = read_label(label_names[i])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    onlyname = img_names[i].split('\/')[-1].split('.')[0]\n\n    transformed = val_transform(image=image, bboxes=bboxes, class_labels=class_labels)\n    transformed_image = transformed['image']\n    transformed_bboxes = transformed['bboxes']\n    transformed_class_labels = transformed['class_labels']\n    transformed_name = onlyname\n    cv2.imwrite(outdir + transformed_name +'.jpg', cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)) \n    out_file = open(outdir + transformed_name +'.txt', 'w')\n    for iii in range(len(transformed_bboxes)):\n        bIn  =  A.augmentations.bbox_utils.convert_bbox_to_albumentations(transformed_bboxes[iii], 'yolo', img_size, img_size, check_validity=True)\n        bOut = A.augmentations.bbox_utils.convert_bbox_from_albumentations(bIn, 'yolo', img_size, img_size, check_validity=True)\n        out_file.write(str(0) + \" \" + \" \".join([str(b) for b in bOut]) + '\\n')\n    out_file.close()","3f8292eb":"!ls .\/ROSE_VAL\/* | wc -l #(images 6 + labels 6)","3ff7d70d":"!ls .\/ROSE_VAL\/*","a12911e9":"!mkdir ROSE_TEST #directory for keep test image ","17d8020d":"#keep test image to ROSE_TEST directory \noutdir = \".\/ROSE_TEST\/\"\nfor i in range(18, len(img_names)): #Test 2 images\n    image = cv2.imread(img_names[i])\n    class_labels, bboxes = read_label(label_names[i])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    onlyname = img_names[i].split('\/')[-1].split('.')[0]\n\n    transformed = val_transform(image=image, bboxes=bboxes, class_labels=class_labels)\n    transformed_image = transformed['image']\n    transformed_bboxes = transformed['bboxes']\n    transformed_class_labels = transformed['class_labels']\n    transformed_name = onlyname\n    cv2.imwrite(outdir + transformed_name +'.jpg', cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)) \n    out_file = open(outdir + transformed_name +'.txt', 'w')\n    for iii in range(len(transformed_bboxes)):\n        bIn  =  A.augmentations.bbox_utils.convert_bbox_to_albumentations(transformed_bboxes[iii], 'yolo', img_size, img_size, check_validity=True)\n        bOut = A.augmentations.bbox_utils.convert_bbox_from_albumentations(bIn, 'yolo', img_size, img_size, check_validity=True)\n        out_file.write(str(0) + \" \" + \" \".join([str(b) for b in bOut]) + '\\n')\n    out_file.close()","dcc942a6":"!ls .\/ROSE_TEST\/* | wc -l ","3e29e7ef":"!ls .\/ROSE_TEST\/*","b9926fe4":"#import YOLO by pythoch (Framework)\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nprint(torch.__version__, torchvision.__version__)\nos.getcwd()","e103b2af":"!git clone https:\/\/github.com\/ultralytics\/yolov5.git #git clone command for download program","1eb7c54e":"os.chdir(\"\/kaggle\/working\/yolov5\") #run at yolov5 directory\n!pip install -r .\/requirements.txt","41e8c804":"os.chdir(\"\/kaggle\/working\/yolov5\") #run at yolov5 directory\n#import train images from ROSE directory to train.txt\nimport glob\nfilename = glob.glob('\/kaggle\/working\/ROSE_TRAIN\/*.jpg')\nfilename.sort()\nf = open(\".\/train.txt\", \"w\")\nfor name in filename:\n    f.write(name+\"\\n\")\nf.close()\n\n#import validation images from ROSE_VAL directory to val.txt\nfilename = glob.glob('\/kaggle\/working\/ROSE_VAL\/*.jpg')\nfilename.sort()\nf = open(\".\/val.txt\", \"w\")\nfor name in filename:\n    f.write(name+\"\\n\")\nf.close()","8fe9ea5e":"#create train directory\nos.chdir(\"\/kaggle\/working\/yolov5\")\nf = open(\".\/train.yaml\", \"w\")\nf.write('train: .\/train.txt'+\"\\n\")\nf.write('val: .\/val.txt'+\"\\n\")\nf.write('nc: 1'+\"\\n\")\nf.write('names: [ \\'ROSE\\' ]'+\"\\n\")\nf.close()","6bd0d6d3":"!ls .\/yolov5\/*","416db11d":"!wandb disabled","5e629bbd":"os.chdir(\"\/kaggle\/working\/yolov5\")\n!python train.py --img 416 --batch 8 --epochs 100 --data train.yaml --cfg .\/models\/yolov5s.yaml --name ROSERUN #keep result at ROSERUN directory","367f3e00":"#Train rate Graph \nos.chdir(\"\/kaggle\/working\/yolov5\")\nfrom utils.plots import plot_results \nplot_results('runs\/train\/ROSERUN\/results.csv')\nimage = np.array(Image.open('runs\/train\/ROSERUN\/results.png'))\nplt.figure(figsize=(20, 20))\nplt.imshow(image)","9be02a6c":"os.chdir(\"\/kaggle\/working\/yolov5\")\nfrom models.experimental import attempt_load\nfrom utils.torch_utils import select_device\nweights = '.\/runs\/train\/ROSERUN\/weights\/best.pt' #select best model\ndevice = select_device('cpu')\nmodel = attempt_load(weights, map_location=device)  # load FP32 model\nstride = int(model.stride.max())  # model stride\nnames = model.module.names if hasattr(model, 'module') else model.names  # get class names","78736502":"!ls \/kaggle\/working\/ROSE_VAL\/*","f74b4023":"#code for keep precision result \nos.chdir(\"\/kaggle\/working\/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('\/kaggle\/working\/ROSE_VAL\/blackpink_2.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img \/ 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) \/ 2 \/ w;\n                ycen = (ymin + ymax) \/ 2 \/ h;\n                ww = (xmax - xmin) \/ w;\n                hh = (ymax - ymin) \/ h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","4e18a147":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","c78b5c66":"confs","de5e591e":"#code for keep precision result \nos.chdir(\"\/kaggle\/working\/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('\/kaggle\/working\/ROSE_VAL\/blackpink_3.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img \/ 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) \/ 2 \/ w;\n                ycen = (ymin + ymax) \/ 2 \/ h;\n                ww = (xmax - xmin) \/ w;\n                hh = (ymax - ymin) \/ h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","48da37ce":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","cd5b5308":"confs","3b5a24c0":"#code for keep precision result \nos.chdir(\"\/kaggle\/working\/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('\/kaggle\/working\/ROSE_VAL\/blackpink_4.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img \/ 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) \/ 2 \/ w;\n                ycen = (ymin + ymax) \/ 2 \/ h;\n                ww = (xmax - xmin) \/ w;\n                hh = (ymax - ymin) \/ h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","f7d12131":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","a93f66ac":"confs","6b790102":"#code for keep precision result \nos.chdir(\"\/kaggle\/working\/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('\/kaggle\/working\/ROSE_VAL\/blackpink_5.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img \/ 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) \/ 2 \/ w;\n                ycen = (ymin + ymax) \/ 2 \/ h;\n                ww = (xmax - xmin) \/ w;\n                hh = (ymax - ymin) \/ h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","0e01fd0c":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","55833329":"confs","e4339d6a":"#code for keep precision result \nos.chdir(\"\/kaggle\/working\/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('\/kaggle\/working\/ROSE_VAL\/blackpink_6.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img \/ 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) \/ 2 \/ w;\n                ycen = (ymin + ymax) \/ 2 \/ h;\n                ww = (xmax - xmin) \/ w;\n                hh = (ymax - ymin) \/ h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","af99b6b8":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","26e791ab":"confs","f39c177f":"#code for keep precision result \nos.chdir(\"\/kaggle\/working\/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('\/kaggle\/working\/ROSE_VAL\/blackpink_7.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img \/ 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) \/ 2 \/ w;\n                ycen = (ymin + ymax) \/ 2 \/ h;\n                ww = (xmax - xmin) \/ w;\n                hh = (ymax - ymin) \/ h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","e3b81c77":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","8349d1c8":"confs","b1140ad1":"!ls .\/ROSE_TEST\/*","c91e19fd":"os.chdir(\"\/kaggle\/working\/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('\/kaggle\/working\/ROSE_TEST\/blackpink_8.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img \/ 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) \/ 2 \/ w;\n                ycen = (ymin + ymax) \/ 2 \/ h;\n                ww = (xmax - xmin) \/ w;\n                hh = (ymax - ymin) \/ h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","c7b5ddab":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","d3136fe7":"confs","c1958957":"os.chdir(\"\/kaggle\/working\/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('\/kaggle\/working\/ROSE_TEST\/blackpink_9.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img \/ 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) \/ 2 \/ w;\n                ycen = (ymin + ymax) \/ 2 \/ h;\n                ww = (xmax - xmin) \/ w;\n                hh = (ymax - ymin) \/ h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","1b11c005":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","ceaba5d9":"confs","6f6d4114":"# **Train**","64549ae9":"<p> mAP@0.5 = mean average precision \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e2a\u0e19\u0e43\u0e08 IOU >50% \u0e02\u0e36\u0e49\u0e19\u0e44\u0e1b \n<p> mAP@ = \u0e04\u0e33\u0e19\u0e27\u0e13\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e48\u0e32 confident \u0e2a\u0e39\u0e07\u0e40\u0e01\u0e34\u0e19 50% \u0e02\u0e36\u0e49\u0e19\u0e44\u0e1b","932509ce":"# **TEST**"}}