{"cell_type":{"30e30b51":"code","3c29bf5e":"code","67690e92":"code","53c1f532":"code","fc4fffdc":"code","d7c487f6":"code","599411b1":"code","2d0c2be7":"code","1c504a3f":"code","83440fb6":"code","f6fcfeda":"code","395c7773":"code","445553c0":"code","ecc086be":"code","8b75da5f":"code","f807b3c8":"code","30afba3e":"code","9c953855":"code","a0145b30":"code","adbfc938":"code","00eeef4e":"code","1355b4a0":"code","f4214ecf":"code","39a65fcc":"code","3727723e":"code","e54f0e81":"code","6097ddf5":"code","6a1fa898":"code","b0626b2a":"code","12ec760d":"code","20a46a27":"code","ae1eb4ce":"code","4cabdf06":"code","88c55a7d":"code","bb9fa61f":"code","fea581dd":"code","50a26f4b":"code","afc9e12e":"code","92b4e0e3":"code","f24c2138":"code","fa2a9bed":"code","6f7a4287":"code","f24f1173":"code","e9cd0b74":"code","1eb54e9c":"code","0688dcaa":"code","0dfec9db":"code","205bf885":"code","3137fc3a":"code","a581f25f":"code","fdcedc92":"code","de915fc3":"code","869f0daf":"code","f649b6a4":"code","520be67f":"code","fd1f8423":"code","4e33e5df":"code","f7dcb49a":"code","80b66e91":"code","e4caf202":"code","43740294":"code","a901ac8d":"code","90436c40":"code","c75921ae":"code","c1f0609d":"code","ee5871da":"code","988103a1":"code","6864a9e9":"code","df0cbcf4":"code","0da90a5d":"code","323aa452":"code","2d405f04":"code","c71915c5":"code","dea6182f":"code","77b3611c":"code","44950c91":"code","a401e150":"code","b380be4e":"code","012c9a8a":"code","fb50def0":"code","3a0f367d":"code","638de440":"code","8562ad70":"code","33845218":"code","30fec1a9":"code","523a9e06":"code","a39929a6":"code","b4aae10d":"code","3c89cc07":"code","15078a55":"code","69de9d27":"code","370cb277":"code","ea91fd06":"code","379ef8f8":"code","99519442":"code","a2387d2f":"markdown","48059fbe":"markdown","ac4a5a81":"markdown","31eeb2d0":"markdown","8b9af1e0":"markdown","c5ad60b7":"markdown","99504289":"markdown","2f7d2219":"markdown","53707d79":"markdown","224c19e8":"markdown","5a76bab2":"markdown","07e8fc21":"markdown","dee0d1f2":"markdown","5a3a0cee":"markdown","5f812f8b":"markdown","18477338":"markdown","b4ba919c":"markdown","da6ca08d":"markdown","5c9b4449":"markdown","e1b96516":"markdown","276abf73":"markdown","9a89d6bf":"markdown","e16f8c54":"markdown","da7d7501":"markdown","ad765a5c":"markdown","3cb473bb":"markdown","93968603":"markdown","239ba41a":"markdown","a831ded5":"markdown","47812e28":"markdown","b43d8cef":"markdown","20ec6a60":"markdown","b78fed99":"markdown","2096a1d1":"markdown","4cbcb82c":"markdown","79399e28":"markdown","0fa54c65":"markdown","a44b0f3f":"markdown","3c134e61":"markdown","15ff875c":"markdown","5a51fd74":"markdown","a289275f":"markdown","a8e47934":"markdown","aafcbbbe":"markdown","09b92520":"markdown","43429b69":"markdown","4623d082":"markdown","9527ee93":"markdown","b8a2e019":"markdown","da40f14b":"markdown","74010139":"markdown","ed012abd":"markdown","2a0573c1":"markdown","963010b3":"markdown","46ef52b8":"markdown","165a7b86":"markdown","5d8f207b":"markdown","b5922a4a":"markdown","f5d98b57":"markdown","2f323b31":"markdown","aefd41b9":"markdown","ec4d5edd":"markdown","fe2d237d":"markdown","dded9c7c":"markdown","c52ef58a":"markdown","ff89c656":"markdown","680b4b07":"markdown","4a19a3f6":"markdown","415f5a45":"markdown","289ff5d5":"markdown","f6090492":"markdown","1c6f3fb7":"markdown","a5c21007":"markdown","a6e929af":"markdown","ecd0181a":"markdown","28b6fa2e":"markdown","3848d7f0":"markdown","53afdf5a":"markdown","d582f133":"markdown","0428764d":"markdown","a3766297":"markdown","7adeeac3":"markdown","1d2e4767":"markdown","a1109923":"markdown","4a24143f":"markdown","395c60e6":"markdown","aa36e186":"markdown","c8767857":"markdown","96554ebc":"markdown","cefe1067":"markdown","df06f3b6":"markdown","bae5aa1d":"markdown","a0c8a9ca":"markdown","4d99b657":"markdown","bf4e87ac":"markdown","de41b7b2":"markdown","5615562b":"markdown","a0ee5de6":"markdown","fb2fa4d8":"markdown","2c771aaa":"markdown","c47fc700":"markdown","3859ddd6":"markdown","fa3bf9dd":"markdown"},"source":{"30e30b51":"! pip install WordCloud\n! pip install advertools\n#! pip install comet_ml","3c29bf5e":"# import comet_ml \n#from comet_ml import Experiment\n    \n# experiment details\n# experiment = Experiment(api_key=\"kA4vrDTbXeNqCz9ThC0POryU4\",\n                        #project_name=\"tweet-classifier-team-5-dbn\",\n                        #workspace=\"ivenocarolus\")\n\n# general imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nimport itertools\n%matplotlib inline\n\n# text analysis & nlp imports\nimport re\nimport nltk\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom advertools.emoji import extract_emoji\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import FreqDist\n\n# machine learning imports\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer","67690e92":"train_raw = pd.read_csv('..\/input\/climate-change-belief-analysis\/train.csv')\ntest_raw = pd.read_csv('..\/input\/climate-change-belief-analysis\/test.csv')","53c1f532":"train_raw.head()","fc4fffdc":"test_raw.head()","d7c487f6":"# create sentiment dataframe\nclass_dict = {2: 'News: tweet links to factual news about climate change',\n              1: 'Pro: tweet supports the belief of man-made climate change',\n              0: 'Neutral: the tweet neither supports nor refutes the belief of man-made climate change',\n              -1: 'Anti: the tweet does not believe in man-made climate change'\n              }\nclass_names = ['News', 'Pro', 'Neutral', 'Anti']\n\nsentiment_df = pd.DataFrame(class_dict.items(),\n                            columns=['Class', 'Detailed Description'])\nsentiment_df['Description'] = class_names\nsentiment_df.head()                                       ","599411b1":"# Check for missing values\nprint(train_raw.isnull().sum())\nprint(test_raw.isnull().sum())","2d0c2be7":"# initialise working dataframes for train & test\ntrain = train_raw.copy()\ntest = test_raw.copy()","1c504a3f":"def url_hash_mention_extractor(df):\n    \"\"\"\n    This function extracts url, hashtag and user mentions\n    from tweets.\n    \n    Arguments:\n    dataframe containing tweets in 'message' column.\n    \n    Returns:\n    a modified dataframe with url, hashtag and mention columns.\n    \"\"\"\n    # initialise lists & strings\n    urls = []\n    tags = []\n    ment = []\n    m_str = '@(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n    u = 'http[s]?:\/\/(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n    t_str = '#(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n    # extract & append information \n    for tweet in df['message'].str.lower():\n        ment.append(re.findall(m_str, tweet))\n        urls.append(re.findall(u, tweet))\n        tags.append(re.findall(t_str, tweet))\n    # add extracted information to df\n    df['urls'] = urls\n    df['hashtags'] = tags\n    df['mentions'] = ment\n    return df","83440fb6":"def tweet_get_emoji(df):\n    \"\"\"\n    This function extracts emoji from tweets.\n    \n    Arguments:\n    dataframe containing tweets in 'message' column.\n    \n    Returns:\n    a modified dataframe with url, hashtag and mention columns.\n    \"\"\"\n    #extract emojis\n    emoji_2d_list = df['message'].values\n    emoji_2d_list = extract_emoji(emoji_2d_list)\n    df['emoji'] = emoji_2d_list['emoji']\n    return df","f6fcfeda":"# extract twitter information\ntrain = url_hash_mention_extractor(train)\n# extract emojis\ntrain = tweet_get_emoji(train)","395c7773":"train.head()","445553c0":"# initialise common contractions as dictionary\ncontractions = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n                \"can't've\": \"cannot have\", \"'cause\": \"because\",\n                \"could've\": \"could have\", \"couldn't\": \"could not\",\n                \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n                \"doesn't\": \"does not\", \"don't\": \"do not\",\n                \"everyone's\": \"everyone is\", \"finna\": \"going to\",\n                \"gimme\": \"give me\", \"gonna\": \"going to\", \"gotta\": \"got to\",\n                \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n                \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n                \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n                \"he'll've\": \"he he will have\", \"he's\": \"he is\",\n                \"how'd\": \"how did\", \"howdy\": \"how do you do\",\n                \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\",\n                \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n                \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n                \"i'm'a\": \"I am about to\", \"i've\": \"i have\",\n                \"innit\": \"is it not\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n                \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n                \"mayn't\": \"may not\", \"might've\": \"might have\",\n                \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n                \"must've\": \"must have\", \"mustn't\": \"must not\",\n                \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                \"needn't've\": \"need not have\",\n                \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n                \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n                \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n                \"she's\": \"she is\", \"should've\": \"should have\",\n                \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n                \"so've\": \"so have\", \"so's\": \"so as\", \"that'd\": \"that would\",\n                \"that'd've\": \"that would have\", \"that's\": \"that is\",\n                \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n                \"there's\": \"there is\", \"they'd\": \"they would\",\n                \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n                \"they'll've\": \"they will have\", \"they're\": \"they are\",\n                \"they've\": \"they have\", \"to've\": \"to have\",\n                \"wasn't\": \"was not\", \"we'd\": \"we would\",\n                \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n                \"we'll've\": \"we will have\", \"we're\": \"we are\",\n                \"we've\": \"we have\", \"weren't\": \"were not\",\n                \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n                \"what're\": \"what are\", \"what's\": \"what is\",\n                \"what've\": \"what have\", \"when's\": \"when is\",\n                \"when've\": \"when have\", \"where'd\": \"where did\",\n                \"where's\": \"where is\", \"where've\": \"where have\",\n                \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n                \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n                \"why've\": \"why have\", \"will've\": \"will have\",\n                \"won't\": \"will not\", \"won't've\": \"will not have\",\n                \"would've\": \"would have\", \"wouldn't\": \"would not\",\n                \"wouldn't've\": \"would not have\",\n                \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\",\n                \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n                \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n                \"you'll've\": \"you will have\", \"you're\": \"you are\",\n                \"you've\": \"you have\"\n                }","ecc086be":"# initialise common text & social media abbreviations as dictionary\ntext_abb = {\"aka\": \"also known as\", \"btw\": \"by the way\",\n            \"b\/c\": \"because\", \"fyi\": \"for your information\",\n            \"idk\": \"i do not know\", \"lol\": \"laughing out loud\",\n            \"lmao\": \"laughing my ass off\", \"lmfao\": \"laughing\",\n            \"omg\": \"oh my god\", \"otoh\": \"on the other hand\",\n            \"wth\": \"what the hell\", \"wtf\": \"what the fuck\",\n            \"icymi\": \"in case you missed it\",\n            \"rofl\": \"rolling on the floor laughing\",\n            \"stfu\": \"shut the fuck up\", \"nvm\": \"nevermind\",\n            \"luv\": \"love\", \"luvs\": \"loves\"\n            }","8b75da5f":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords","f807b3c8":"# create stop words list\nall_stopwords = stopwords.words('english')\n# update stop words list for specific project\nall_stopwords.extend(['amp']) \nall_stopwords.remove('not')","30afba3e":"def clean_text(df, cont_dict, abbr_dict, all_stopwords):\n    \"\"\"\n    This function conducts basic cleaning of tweet data\n    including lowercasing, expansion of contractions &\n    common text abbrieviations, and removal of punctuation\n    and stopwords.\n    \n    Arguments:\n    df: original dataframe column containing tweets\n    cont_dict: dictionary of contractions to be expanded\n    abbr_dict: dictionary of text abbreviations to be expanded\n    all_stopwords: list of stopwords to be removed\n    \n    Returns:\n    Modified dataframe of cleaned tweets\n    \"\"\"\n    # convert to lowercase\n    clean_df = df.str.lower()\n    # expand contractions & abbreviations\n    clean_df = clean_df.replace(contractions, regex=True)\n    clean_df = clean_df.replace(abbr_dict, regex=True)\n    # remove urls\n    pattern_url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n    clean_df = clean_df.str.replace(pattern_url, \" \", regex=True)\n    clean_df = clean_df.str.replace(\"[^http[s]\\S+]\", \" \", regex=True)\n    # remove punctuation & noise\n    clean_df = clean_df.str.replace(\"[^a-z]\", \" \")\n    clean_df = clean_df.apply(lambda x: ' '.join\n                              ([word for word in x.split() if len(word) > 2]))\n    # remove stop words\n    corpus = []\n    n = len(df)\n    for i in range(0, n):\n        clean_text = clean_df[i]\n        clean_text = [word for word in clean_text.split()\n                      if word not in set(all_stopwords)]\n        clean_text = ' '.join(clean_text)\n        corpus.append(clean_text)\n    return corpus","9c953855":"train['clean_tweet'] = clean_text(train['message'],\n                                  contractions,\n                                  text_abb,\n                                  all_stopwords)\ntest['clean_tweet'] = clean_text(test['message'],\n                                 contractions,\n                                 text_abb,\n                                 all_stopwords)","a0145b30":"train.head(5)","adbfc938":"# create barplot of tweet distribution by class\nsns.set(font_scale=1.3)\nplt.figure(figsize=(10, 5))\ncount = sns.countplot(x=train['sentiment'], data=train, palette=\"husl\")\nplt.title('Tweet distribution by Sentiment Class: Train data')\n\ntotal = len(train)\nfor p in count.patches:\n    height = p.get_height()\n    count.text(p.get_x()+p.get_width()\/2.,\n               height + 3,\n               '{:.0%}'.format(height\/total),\n               ha=\"center\")","00eeef4e":"# Create tweet features to explore tweet lengths\ntrain['no_of_characters'] = train['message'].apply(len)\ntrain['word_count'] = train.message.str.split().apply(len)\ntrain['av_word_length'] = train['no_of_characters']\/train['word_count']","1355b4a0":"# create boxplot of characters per tweet by sentiment class\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=train['sentiment'],\n            y=train['no_of_characters'],\n            data=train,\n            palette=\"husl\")\n\nplt.title('Characters per Tweet by Sentiment Class')\nplt.xlabel('Sentiment Class')\nplt.ylabel('Tweet Length (characters)');","f4214ecf":"# create boxplot of number of words by sentiment class\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=train['sentiment'],\n            y=train['word_count'],\n            data=train,\n            palette=\"husl\")\n\nplt.title('No of Words per Tweet by Sentiment Class')\nplt.xlabel('Sentiment Class')\nplt.ylabel('Word Count per Tweet');","39a65fcc":"# create boxplot of average word length by sentiment class\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=train['sentiment'],\n            y=train['av_word_length'],\n            data=train,\n            palette=\"husl\")\nplt.ylim(3, 15)\nplt.title('Average Word length by Sentiment Class')\nplt.xlabel('Sentiment Class')\nplt.ylabel('Average Word Length (characters per word)');","3727723e":"# create scatter plot of tweet length characteristics\nplt.figure(figsize=(15, 10))\nsns.scatterplot(y=\"no_of_characters\",\n                x=\"word_count\",\n                data=train,\n                hue='sentiment',\n                size='av_word_length',\n                sizes=(10, 500),\n                palette='coolwarm')\n\nplt.title('Overview of Tweet Length Features')\nplt.xlabel('word_count')\nplt.ylabel('Tweet Length (characters)');","e54f0e81":"# generate word cloud for train tweets\ntrain_text = \" \".join(tweet for tweet in train.clean_tweet)\ntrain_wordcloud = WordCloud(max_font_size=300,\n                            background_color=\"black\",\n                            width=1600,\n                            height=800,\n                            collocations=False,\n                            colormap='Paired').generate(train_text)\nplt.figure(figsize=(15, 10))\n#plt.title('Word Cloud of Train Tweets', fontsize=25)\nplt.imshow(train_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","6097ddf5":"# filter dataframe by sentiment\nanti = train[(train['sentiment']) == -1].reset_index(drop=True)\nneutral = train[(train['sentiment']) == 0].reset_index(drop=True)\npro = train[(train['sentiment']) == 1].reset_index(drop=True)\nnews = train[(train['sentiment']) == 2].reset_index(drop=True)","6a1fa898":"# determine most common words\nmost_common_anti = list(itertools.chain\n                        (*[tweet.split() for tweet in\n                           anti['clean_tweet'].values]))\nfreq = FreqDist(most_common_anti)\ny_label = [x[0] for x in freq.most_common(n=20)]\nx_freq = [x[1] for x in freq.most_common(n=20)]\nfreq.most_common(n=20)\n\n# create plot of most common words for anti sentiment\nplt.figure(figsize=(15, 8))\nsns.barplot(data={'x': x_freq, 'y': y_label}, x='x', y='y')\nplt.xlabel('Frequency')\nplt.ylabel('Word')\nplt.title('Top 20 Words: Anti Sentiment');","b0626b2a":"# determine most common words\nmost_common_neut = list(itertools.chain(*[tweet.split() for tweet in\n                                          neutral['clean_tweet'].values]))\nfreq = FreqDist(most_common_neut)\ny_label = [x[0] for x in freq.most_common(n=20)]\nx_freq = [x[1] for x in freq.most_common(n=20)]\nfreq.most_common(n=20)\n\n# create plot of most common words for neutral sentiment\nplt.figure(figsize=(15, 8))\nsns.barplot(data={'x': x_freq, 'y': y_label}, x='x', y='y')\nplt.xlabel('Frequency')\nplt.ylabel('Word')\nplt.title('Top 20 Words: Neutral Sentiment');","12ec760d":"# determine most common words\nmost_common_pro = list(itertools.chain(*[tweet.split() for tweet in\n                                         pro['clean_tweet'].values]))\nfreq = FreqDist(most_common_pro)\ny_label = [x[0] for x in freq.most_common(n=20)]\nx_freq = [x[1] for x in freq.most_common(n=20)]\nfreq.most_common(n=20)\n\n# create plot of most common words for pro sentiment\nplt.figure(figsize=(15, 8))\nsns.barplot(data={'x': x_freq, 'y': y_label}, x='x', y='y')\nplt.xlabel('Frequency')\nplt.ylabel('Word')\nplt.title('Top 20 Words: Pro Sentiment');","20a46a27":"# determine most common words\nmost_common_news = list(itertools.chain(*[tweet.split() for tweet in\n                                          news['clean_tweet'].values]))\nfreq = FreqDist(most_common_news)\ny_label = [x[0] for x in freq.most_common(n=20)]\nx_freq = [x[1] for x in freq.most_common(n=20)]\nfreq.most_common(n=20)\n\n# create plot of most common words for news sentiment\nplt.figure(figsize=(15, 8))\nsns.barplot(data={'x': x_freq, 'y': y_label}, x='x', y='y')\nplt.xlabel('Frequency')\nplt.ylabel('Word')\nplt.title('Top 20 Words: News Sentiment');","ae1eb4ce":"def total_words(df):\n    \"\"\"\n    Calculates the total words used in a dataframe of tweets.\n    \n    Arguments:\n    dataframe of tweets.\n    \n    Returns:\n    number of words used.\n\n    \"\"\"\n    df_words = (\" \".join(df['clean_tweet']))\n    df_words = df_words.split()\n    return len(df_words)","4cabdf06":"def unique_words(df):\n    \"\"\"\n    Calculates the unique words used in a dataframe of tweets.\n    \n    Arguments:\n    dataframe of tweets.\n    \n    Returns:\n    number of unique words used.\n\n    \"\"\"\n    df_unique = (\" \".join(df['clean_tweet']))\n    df_unique = set(df_unique.split())\n    return len(df_unique)","88c55a7d":"# create barplot of total unique words by class\nsentiment_cat = ['anti', 'neutral', 'pro', 'news']\nunique_list = [unique_words(anti),\n               unique_words(neutral),\n               unique_words(pro),\n               unique_words(news)]\n\nplt.figure(figsize=(10, 5))\ncount = sns.barplot(x=sentiment_cat, y=unique_list, palette=\"husl\")\nplt.xlabel('Sentiment Class')\nplt.ylabel('No of unique words')\nplt.title('Unique Words used by Sentiment Class');","bb9fa61f":"# create barplot of unique word ratio by class\nsentiment_cat = ['anti', 'neutral', 'pro', 'news']\nratio = [unique_words(anti)\/total_words(anti),\n         unique_words(neutral)\/total_words(neutral),\n         unique_words(pro)\/total_words(pro),\n         unique_words(news)\/total_words(news)]\nplt.figure(figsize=(10, 5))\ncount = sns.barplot(x=sentiment_cat, y=ratio, palette=\"husl\")\nplt.xlabel('Sentiment Class')\nplt.ylabel('Ratio')\nplt.title('Ratio of Unique to Total Words used by Sentiment Class');","fea581dd":"# calculate tweet count\nprint(train.sentiment.value_counts())\ntweets_list = list(train.sentiment.value_counts())","50a26f4b":"# calculate retweet count\nprint((train[train.message.str.contains('RT')]).sentiment.value_counts())\nretweets_list = list((train[train.message.str.contains('RT')])\n                     .sentiment.value_counts())","afc9e12e":"# create dataframe of tweet & retweet counts\ntweet_class_order = ['Pro', 'News', 'Neutral', 'Anti']\ntotal_tweets = pd.DataFrame({'Sentiment class': tweet_class_order,\n                             'Count': tweets_list})\ntotal_retweets = pd.DataFrame({'Sentiment class': tweet_class_order,\n                               'Count': retweets_list})\ncomb_count = pd.concat([total_tweets, total_retweets], keys=['total tweets',\n                                                             'total retweets'])\ncomb_count.reset_index(inplace=True)\ncomb_count.columns = comb_count.columns.str.replace('level_0', 'Type')","92b4e0e3":"plt.figure(figsize=(10, 5))\ncount = sns.barplot(x='Sentiment class', y='Count', data=comb_count,\n                    palette=\"Paired\", hue='Type')\nplt.title('Tweet Count by Sentiment Class');","f24c2138":"def common_hashtags(df, n):\n    \"\"\"\n    Returns a dataframe with the most common 'n' hashtags\n    in tweet data and the frequency of said hashtags.\n    \n    Arguments:\n    df: dataframe containing 'hashtag' column of extracted\n    hashtags.\n    \n    n: integer number of common words\n    \n    Returns:\n    dataframe of common hashtags and frequency of use.\n    \n    \"\"\"\n    hashtag_list = []\n    for i in range(0, len(df)):  # extract hashtags\n        hashtags = df['hashtags'][i]\n        hashtags = [word for word in hashtags]\n        hashtags = ''.join(hashtags)\n        hashtag_list.append(hashtags)\n    top_hashtags = Counter(hashtag_list).most_common(n+1)\n    comm_hashtag = pd.DataFrame(np.array(top_hashtags),  # create new df\n                                columns=['hashtag', 'frequency'])\n    comm_hashtag['frequency'] = comm_hashtag['frequency'].astype(int)\n    return comm_hashtag[1:]","fa2a9bed":"# create plot of top 10 hashtags overall\nplt.figure(figsize=(10, 5))\nsns.barplot(data=common_hashtags(train, 10),\n            x='frequency',\n            y='hashtag',\n            palette=\"husl\")\nplt.title('Top 10 Hashtags Overall');","6f7a4287":"# initialise dataframes\nanti_htags = common_hashtags(anti, 10)\nneutral_htags = common_hashtags(neutral, 10)\npro_htags = common_hashtags(pro, 10)\nnews_htags = common_hashtags(news, 10)\ncombine_hashtag = pd.concat([anti_htags, neutral_htags, pro_htags, news_htags],\n                            keys=[\"Anti\", \"Neutral\", \"Pro\", \"News\"])\ncombine_hashtag.reset_index(0, inplace=True)\ncombine_hashtag.columns = ['sentiment', 'hashtag', 'frequency']","f24f1173":"# plot comparison clustered bar chart\nplt.figure(figsize=(12, 10))\nsns.barplot(x='frequency',\n            y='hashtag',\n            data=combine_hashtag,\n            hue='sentiment',\n            palette=\"husl\")\nplt.title('Hashtag Comparison by Sentiment');","e9cd0b74":"def common_mentions(df, n):\n    \"\"\"\n    Returns a dataframe with the most common 'n' user mentions\n    in tweet data and the frequency of said user mentions.\n    \n    Arguments:\n    df: dataframe containing 'mentions' column of username\n    mentions.\n    \n    n: integer number of common username mentions\n    \n    Returns:\n    dataframe of common username mentions and frequency of use.\n    \n    \"\"\"\n    mentions_list = []\n    for i in range(0, len(df)):  # extract mentions\n        mentions = df['mentions'][i]\n        mentions = [word for word in mentions]\n        mentions = ''.join(mentions)\n        mentions_list.append(mentions)\n    top_mentions = Counter(mentions_list).most_common(n+1)\n    common_mentions = pd.DataFrame(np.array(top_mentions),  # create new df\n                                   columns=['mention', 'frequency'])\n    common_mentions['frequency'] = common_mentions['frequency'].astype(int)\n    return common_mentions[1:]","1eb54e9c":"# create plot of top 10 usernames\nplt.figure(figsize=(10, 5))\nsns.barplot(data=common_mentions(train, 10),\n            x='frequency',\n            y='mention',\n            palette=\"husl\")\nplt.title('Top 10 Mentions Overall');","0688dcaa":"# initialise dataframes\nanti_mentions = common_mentions(anti, 10)\nneutral_mentions = common_mentions(neutral, 10)\npro_mentions = common_mentions(pro, 10)\nnews_mentions = common_mentions(news, 10)\ncombined_mentions = pd.concat([anti_mentions,\n                               neutral_mentions,\n                               pro_mentions,\n                               news_mentions],\n                              keys=[\"Anti\", \"Neutral\", \"Pro\", \"News\"])\ncombined_mentions.reset_index(0, inplace=True)\ncombined_mentions.columns = ['sentiment', 'username', 'frequency']","0dfec9db":"# create username comparison plot\nplt.figure(figsize=(12, 10))\nsns.barplot(x='frequency',\n            y='username',\n            data=combined_mentions,\n            hue='sentiment',\n            palette=\"husl\")\nplt.title('Username Mention Comparison by Sentiment');","205bf885":"# determine no of tweets containing urls per class\nurl_appearance = train[train.astype(str)['urls'] != '[]']\nurl_appearance.sentiment.value_counts()","3137fc3a":"# create plot of tweets containing urls per class\nurl_classes = ['Pro', 'News', 'Neutral', 'Anti']\nurl_class_freq = list(url_appearance.sentiment.value_counts())\nplt.figure(figsize=(10, 5))\nsns.barplot(x=url_classes,\n            y=url_class_freq,\n            palette=\"husl\")\nplt.xlabel('Sentiment Class')\nplt.ylabel('Tweets with URLs')\nplt.title('Url count per class');","a581f25f":"# create plot of URL frequency per class\nplt.figure(figsize=(10, 5))\nurl_fpt_by_class = [x\/y for x, y in zip(url_class_freq, tweets_list)]\nsns.barplot(x=url_classes,\n            y=url_fpt_by_class,\n            palette=\"husl\")\nplt.xlabel('Sentiment Class')\nplt.ylabel('Ratio of Total Tweets to Tweets with URLs')\nplt.title('Url Appearance per No of Tweets by Sentiment Class');","fdcedc92":"def common_urls(df):\n    \"\"\"\n    Returns a dataframe with the most common 'n' urls\n    in tweet data and the frequency of said urls.\n    \n    Arguments:\n    df: dataframe containing 'urls' column of url links\n    \n    n: integer number of common urls\n    \n    Returns:\n    new dataframe of common username urls and frequency of use.\n    \n    \"\"\"\n    urls_list = []\n    for i in range(0, len(df)):  # extract urls\n        urls = df['urls'][i]\n        urls = [word for word in urls]\n        urls = ''.join(urls)\n        urls_list.append(urls)\n    top_urls = Counter(urls_list).most_common()\n    common_urls = pd.DataFrame(np.array(top_urls),  # create url df\n                               columns=['urls', 'frequency'])\n    common_urls['frequency'] = common_urls['frequency'].astype(int)\n    common_urls.iloc[0,0] = 'no url'\n    return common_urls","de915fc3":"# create plot of top 10 urls\nplt.figure(figsize=(10, 5))\nsns.barplot(data=common_urls(train).head(10),\n            x='frequency',\n            y='urls',\n            palette=\"husl\")\nplt.title('Top 10 Most Common URLS Overall');","869f0daf":"# create url df\nurl_df = common_urls(train)\nurl_df.head()","f649b6a4":"# calculate no. of unique urls\ntotal_unique_urls = len(url_df) - 1\nprint(total_unique_urls)","520be67f":"# calculate % tweets without urls\nno_tweets_without_urls = url_df.iloc[0, 1]\ntotal_no_of_tweets = len(train)\nperc_tweets_without_urls = no_tweets_without_urls\/total_no_of_tweets\nprint(\"{:.2%}\".format(perc_tweets_without_urls))","fd1f8423":"# calculate % url tweets containing t.co domain\nt_co_frequency = url_df.urls.str.contains('t.co').sum()\nprint(\"{:0.2%}\". format(t_co_frequency\/total_unique_urls))","4e33e5df":"emoji = list(itertools.chain(*train['emoji'].values))\nfreq = FreqDist(emoji)\nfreq.most_common(n=15)","f7dcb49a":"# determine no of tweets containing emojis per class\nemoji_appearance = train[train.astype(str)['emoji'] != '[]']\nemoji_appearance.sentiment.value_counts()","80b66e91":"# create plot of tweets containing emojis per class\nemoji_classes = ['Pro', 'Neutral', 'Anti', 'News']\nemoji_class_freq = list(emoji_appearance.sentiment.value_counts())\nplt.figure(figsize=(10, 5))\nsns.barplot(x=emoji_classes,\n            y=emoji_class_freq,\n            palette=\"husl\")\nplt.xlabel('Sentiment Class')\nplt.ylabel('Tweets with Emojis')\nplt.title('Emojis per Sentiment Class');","e4caf202":"# create plot of emoji frequency per class\nemoji_tweet_order = [len(pro), len(neutral), len(anti), len(news)]\nplt.figure(figsize=(10, 5))\nemoji_fpt_by_class = [x\/y for x, y in zip(emoji_class_freq, emoji_tweet_order)]\nsns.barplot(x=url_classes,\n            y=emoji_fpt_by_class,\n            palette=\"husl\")\nplt.xlabel('Sentiment Class')\nplt.ylabel('Ratio of Total Tweets to Tweets with Emojis')\nplt.title('Emoji Appearance per No of Tweets by Sentiment Class');","43740294":"# update text abbreviations\ntext_abb['qanda'] = 'question and answer'\ntext_abb['maga'] = 'make america great again'\ntext_abb['nodapl'] = 'no dakota access pipeline'","a901ac8d":"def tweet_preprocessing(df, cont_dict, abbr_dict, all_stopwords):\n    \"\"\"\n    This function conducts preprocessing of tweet data for a\n    machine learning model including lowercasing, expansion of\n    contractions & common text abbrieviations, and removal of\n    punctuation and stopwords and stemming.\n    \n    Arguments:\n    df: original dataframe column containing tweets\n    cont_dict: dictionary of contractions to be expanded\n    abbr_dict: dictionary of text abbreviations to be expanded\n    all_stopwords: list of stopwords to be removed\n    \n    Returns:\n    modified dataframe of cleaned tweets\n    \"\"\"\n    # convert to lower case\n    clean_df = df.str.lower()\n    # expand contractions & abbreviations\n    clean_df = clean_df.replace(contractions, regex=True)\n    clean_df = clean_df.replace(text_abb, regex=True)\n    # replace urls\n    pattern_url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n    clean_df = clean_df.str.replace(pattern_url, \"weburl\", regex=True)\n    # remove punctuation & noise\n    clean_df = clean_df.str.replace(\"[^a-z@#!?]\", \" \")\n    clean_df = clean_df.apply(lambda x: ' '.join([word for word in x.split()\n                                                  if len(word) > 2]))\n    # remove stop words\n    clean_df = clean_df.apply(lambda x: ' '.join\n                             ([word for word in x.split()\n                               if word not in set(all_stopwords)]))\n    # reduce word to root form using lemmatization\n    tweet_corpus = []\n    for i in range(0, len(df)):\n        tweet = clean_df[i]\n        lemmatizer = WordNetLemmatizer()\n        tweet = [lemmatizer.lemmatize(word) for word in tweet.split()]\n        tweet = ' '.join(tweet)\n        tweet_corpus.append(tweet)\n    return tweet_corpus","90436c40":"# process the train and test tweets\ntrain_corpus = tweet_preprocessing(train_raw.message, contractions,\n                                   text_abb, all_stopwords)\ntest_corpus = tweet_preprocessing(test_raw.message, contractions,\n                                  text_abb, all_stopwords)","c75921ae":"# create document matrix for train & test\ntfv = TfidfVectorizer()\nX = tfv.fit_transform(train_corpus).toarray()\nX_test = tfv.transform(test_corpus).toarray()","c1f0609d":"X.shape","ee5871da":"# initialise training y: sentiment class\ny = np.array(train_raw['sentiment'])","988103a1":"# create the  object with the desired sampling strategy.\nsmote = SMOTE(sampling_strategy='minority')\n\n# fit object to training data\nX_train_smote, y_train_smote = smote.fit_sample(X, y)","6864a9e9":"X_train_smote.shape","df0cbcf4":"# split data into train validation test sets\nX_train, X_vt, y_train, y_vt = train_test_split(X_train_smote,\n                                                y_train_smote,\n                                                test_size=0.2,\n                                                random_state=101)","0da90a5d":"# create svc model object & train model\ncl_svc = LinearSVC(random_state=101)\nmodel_svc = cl_svc.fit(X_train, y_train)\nprint(classification_report(y_vt, model_svc.predict(X_vt),\n                            target_names=['Anti', 'Neutral', 'Pro', 'News']))","323aa452":"# create logistic regression model object & train model\ncl_lr = LogisticRegression(C=20, max_iter=500)\nmodel_lr = cl_lr.fit(X_train, y_train)\nprint(classification_report(y_vt, model_lr.predict(X_vt),\n                            target_names=['Anti', 'Neutral', 'Pro', 'News']))","2d405f04":"# create naive bayes model object & train model\ncl_nb = GaussianNB()\nmodel_nb = cl_nb.fit(X_train, y_train)\nprint(classification_report(y_vt, model_nb.predict(X_vt),\n                            target_names=['Anti', 'Neutral', 'Pro', 'News']))","c71915c5":"# create decision tree model object & train model\ncl_dt = DecisionTreeClassifier(random_state=101)\nmodel_dt = cl_dt.fit(X_train, y_train)\nprint(classification_report(y_vt, model_dt.predict(X_vt),\n                            target_names=['Anti', 'Neutral', 'Pro', 'News']))\n","dea6182f":"# create random forest model object & train model\ncl_rf = RandomForestClassifier(n_estimators=10, random_state=101)\nmodel_rf = cl_rf.fit(X_train, y_train)\nprint(classification_report(y_vt, model_rf.predict(X_vt),\n                            target_names=['Anti', 'Neutral', 'Pro', 'News']))","77b3611c":"# make predictions using trained models\ny_pred_lr = model_lr.predict(X_vt)\ny_pred_svc = model_svc.predict(X_vt)\ny_pred_nb = model_nb.predict(X_vt)\ny_pred_rf = model_rf.predict(X_vt)\ny_pred_dt = model_dt.predict(X_vt)","44950c91":"# calculate f1 scores for validation test set\nlr_f1 = f1_score(y_vt, y_pred_lr, average='macro')\nsvc_f1 = f1_score(y_vt, y_pred_svc, average='macro')\nrf_f1 = f1_score(y_vt, y_pred_rf, average='macro')\ndt_f1 = f1_score(y_vt, y_pred_dt, average='macro')\nnb_f1 = f1_score(y_vt, y_pred_nb, average='macro')\n\n# calculate precision scores for validation test set\nlr_prec = precision_score(y_vt, y_pred_lr, average='macro')\nsvc_prec = precision_score(y_vt, y_pred_svc, average='macro')\nrf_prec = precision_score(y_vt, y_pred_rf, average='macro')\ndt_prec = precision_score(y_vt, y_pred_dt, average='macro')\nnb_prec = precision_score(y_vt, y_pred_nb, average='macro')\n\n# calculate recall scores for validation test set\nlr_rec = recall_score(y_vt, y_pred_lr, average='macro')\nsvc_rec = recall_score(y_vt, y_pred_svc, average='macro')\nrf_rec = recall_score(y_vt, y_pred_rf, average='macro')\ndt_rec = recall_score(y_vt, y_pred_dt, average='macro')\nnb_rec = recall_score(y_vt, y_pred_nb, average='macro')","a401e150":"# initialise columns for model evaluation df\nmodels = ['Support Vector', 'Logistic Regression', 'Decision Tree',\n          'Random Forest', 'Naive Bayes']\nf1_score_validation = [svc_f1, lr_f1, dt_f1, rf_f1, nb_f1]\nf1_score_kaggle = [0.72190, 0.72392, 0.62470, 0.68172, 0.50640] \nprec_validation = [svc_prec, lr_prec, dt_prec, rf_prec, nb_prec]\nrecall_score_validation = [svc_rec, lr_rec, dt_rec, rf_rec, nb_rec]\n\n# create model evaluation df\nmetrics_df = pd.DataFrame({'model': models,\n                           'precision': prec_validation,\n                           'recall': recall_score_validation,\n                           'f1-score_validation': f1_score_validation,\n                           'f1-score_kaggle': f1_score_kaggle})\nmetrics_df.head()","b380be4e":"# initialise feature & target variables for train and test\nX_u = train_raw['message']\ny_u = train_raw['sentiment']\nX_test_u = test_raw['message']","012c9a8a":"# create document matrix for train & test\ntfv = TfidfVectorizer()\nX_u = tfv.fit_transform(X_u).toarray()\nX_test_u = tfv.transform(X_test_u).toarray()\n#del tfv","fb50def0":"# split data into train validation test sets\nX_train_u, X_vt_u, y_train_u, y_vt_u = train_test_split(X_u,\n                                                        y_u,\n                                                        test_size=0.2,\n                                                        random_state=101)\n#del X_u, y_u","3a0f367d":"# create svc model object & train model on unprocessed data\ncl_svc_u = LinearSVC()\nmodel_svc_u = cl_svc_u.fit(X_train_u, y_train_u)\nprint(classification_report(y_vt_u, model_svc_u.predict(X_vt_u),\n                            target_names=['Anti', 'Neutral', 'Pro', 'News']))","638de440":"# create logistic regression model object & train model\ncl_lr_u = LogisticRegression(C=20, max_iter=500)\nmodel_lr_u = cl_lr_u.fit(X_train_u, y_train_u)\nprint(classification_report(y_vt_u, model_lr_u.predict(X_vt_u),\n                            target_names=['Anti', 'Neutral', 'Pro', 'News']))","8562ad70":"# initialise columns for model evaluation on raw data\nmodels_u = ['Support Vector', 'Logistic Regression']\n# update scores with kaggle f1 score output\nf1_kaggle_u = [0.74359, 0.74063]\n# precision\nprec_u = [precision_score(y_vt_u,\n                          model_lr_u.predict(X_vt_u),\n                          average='macro'),\n          precision_score(y_vt_u,\n                          model_svc_u.predict(X_vt_u),\n                          average='macro')]\n# recall\nrecall_u = [recall_score(y_vt_u,\n                         model_lr_u.predict(X_vt_u),\n                         average='macro'),\n            recall_score(y_vt_u,\n                         model_svc_u.predict(X_vt_u),\n                         average='macro')]\n# validation f1 score\nf1_score_valid_u = [f1_score(y_vt_u,\n                             model_lr_u.predict(X_vt_u),\n                             average='macro'),\n                    f1_score(y_vt_u,\n                             model_svc_u.predict(X_vt_u),\n                             average='macro')]\n# create model evaluation df\nmetrics_u = pd.DataFrame({'model': models_u,\n                          'precision': prec_u,\n                          'recall': recall_u,\n                          'f1-score_validation': f1_score_valid_u,\n                          'f1-score_kaggle': f1_kaggle_u})","33845218":"metrics_df.iloc[0:2,:]","30fec1a9":"metrics_u.head()","523a9e06":"labels = ['-1: ANTI', '0: NEUTRAL', '1: PRO', '2: NEWS']\npd.DataFrame(data=confusion_matrix(y_vt, y_pred_svc),\n             index=labels, columns=labels)","a39929a6":"nltk.download('vader_lexicon')","b4aae10d":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()","3c89cc07":"# VADER analysis\ndfv = neutral.copy()\ndfv['score'] = train['message'].apply(lambda x: sid.polarity_scores(x))\ndfv['compound'] = dfv['score'].apply(lambda score_dict: score_dict['compound'])\ndfv['comp_score'] = dfv['compound'].apply(lambda c: 'pos' if c > 0\n                                          else ('neg' if c < 0 else 'neutral'))","15078a55":"# calculate percantage of neg, pos & neutral scored tweets from VADER analysis\nround((dfv['comp_score'].value_counts()*100\/len(dfv)), 2)","69de9d27":"# create parameter grid\n#param_grid = {'C': [0.1, 1.0, 10.0, 100]}\n\n#f1 = make_scorer(f1_score, average='macro')\n# defining the grid search object\n#grid = GridSearchCV(estimator=LinearSVC(max_iter=5000),\n                    param_grid=param_grid,\n                    scoring=f1,\n                    cv=5)\n\n# fitting train sets and displaying best score and best parameters for model\n#grid_result = grid.fit(X_u, y_u)\n#print('Best Score: ', grid_result.best_score_)\n#print('Best Params: ', grid_result.best_params_)","370cb277":"# logging the paramters to comet\n#for cv_res_index in range(len(grid.cv_results_['params'])):\n    #for key, value in grid.cv_results_.items():\n        #if key == \"params\":\n            #exp.log_parameters(value[cv_res_index])\n        #else:\n            #exp.log_metric(key, value[cv_res_index])\n#end experiment session\n#exp.end()","ea91fd06":"# create svc model object & retrain model\ncl_svc_f = LinearSVC()\nmodel_svc_f = cl_svc_f.fit(X_u, y_u)","379ef8f8":"y_pred = model_svc_f.predict(X_test_u)","99519442":"submission = pd.DataFrame({'tweetid': test_raw['tweetid'],\n                           'sentiment': y_pred})\nsubmission.to_csv('svc_final.csv', index=False)\nsubmission.head()","a2387d2f":"# Table of Contents <a name=\"toc\"><\/a>\n1. [Imports](#imports)\n2. [Data Description](#dd)\n3. [Data Preparation](#dp)\n4. [Exploratory Data Analysis](#eda)\n5. [Data Preprocessing](#dpre)\n6. [Classification Models](#models)\n7. [Model Performance Evaluation](#perf)\n8. [Final Model Selection & Tuning](#select&tune)\n9. [Conclusion & Recommendations](#c&r)","48059fbe":"### **Analysis of Results**\n\nFrom the above results we can conclude that the best score was achieved by SVC using uncleaned, unbalanced tweet data, in comparison to cleaned data that uses NLP techniques. The F1 scores on the actual test set for both methods improved using this approach. Analysis and reasoning as to why these models performed better follows.\n\n#### **Use of raw tweets:**\nAs suspected, using unprocessed tweets yielded better classification performance. Possible rationale for why implementing Data cleaning & NLP techniques produces a worse score could be attributed to the inherent nature of tweets.\n\nTweets are generally short (most tweets containing around 10 to 20 words). Removing key tokens could alter or change the meaning\/sentiment for example, key words such as 'not' are considered stop words but actually add meaning to text. Classifying the sentiment of such short text would benefit from the preservation of words in their original form of the tweet. \n\nFrom the EDA we also noticed that many of a sentiment class's most common words overlapped with other classes and perhaps removing stop words confuses the model and results in misclassification.\n\nRemoving stop words and reducing words to root form should therefore be carefully considered depending on the specific application of Text Classification. Larger data sets with longer text length per observation may benefit from these techniques to improve model processing performance. It is also up to the modeler to decide if speed\/processing performance outweighs the slight increase in accuracy of predictions, and decide on the way forward with respect to processing or not processing text.\n\n#### **Unbalanced Data:**\nBoth models performed better using unbalanced data. This initially seemed counterintuitive as initial testing on the base model resulted in scores ranging between 0.73 & 0.75 (depending on classification type, and level of cleaning. It was then then theorized that balancing would improve model performance because of the imbalance in class training data.\n\nOnce every classification method, parameter, NLP technique and balancing\/resampling method was tried and the F1 scores remained the same or worse, it was theorized that perhaps the imbalance in the data reflected the actual distribution of sentiment classes in tweets in reality.\n\nBalanced data is ideal when training models and limiting bias. However after the balancing of that data the underlying frequencies of the sentiment distribution was lost. It seems as though the probability of a climate change related tweet being Pro, is in reality higher than other sentiments. Increasing the number of observations therefore implied that the probablity of each sentiment was equal, which may not necessarily be the case, causing the model to make slightly worse predicitons.\n\nFurther research into the debate of balanced vs unbalanced data showed that inherently, some data sets will always have a minority class, for example, fraud detection. Again, the decision to balance data or leave as is it is dependent on the type of classification problem at hand and the nature of the data sets involved. ","ac4a5a81":"**Observations:**\n- A high ratio (closer to 1) denotes a high URL frequency. \n- From the above visualisation, we note that URLs appear most frequently in the News class (almost one URL every tweet). This is intuitive as News sources would be tweeting links to articles on news sites, interview links on Youtube etc.\n- URLs appear less frequently in Pro & Anti sentiment classes (one URL approximately every 2 to 3 tweets).\n- URLs appear least frequently in the Neutral sentiment class (one URL approximately every 3 tweets).","31eeb2d0":"#### Observations: \n- Aside from the obvious common words, the news tweets appear to reflect the expected factual nature of the tweets reporting news, with words like 'says' 'world', 'news', 'energy', 'china', 'paris' (referring to the Paris Agreement) & 'pruitt' (Former EPA Administrator noted for rescinding the EPAs climate change policy & mitigation measures in 2017.\n- Donald Trump is mentioned 1355 times in total in News tweets as 'trump, 'donald, or 'president'. This is the equivalent of roughly once every 4 tweets, the highest frequency of any class. ","8b9af1e0":"# Data Preprocessing <a class=\"anchor\" id=\"dpre\"><\/a>\n[Back to Table of Contents](#toc)\n\nNow that we've gained some formative insights from the EDA process, we may begin processing the data to train the machine learning models. This will comprise the following steps:\n\n- Data Cleaning with updates\n- Prepare Data for Machine Learning\n    - Balance Data\n    - Train & Validation Test Split","c5ad60b7":"# Data Description <a class=\"anchor\" id=\"dd\"><\/a>\n[Back to Table of Contents](#toc)","99504289":"EDA is critical in any machine learning application and exploration could yield insights, structure and patterns to the data that may inform further cleaning or transformation of the data for machine learning\n\nThe focus of the EDA will centre around exploring tweet characteristics and answering key questions that includes the following:\n\n- **Data Balance:** What is the distribution of tweets by sentiment class?\n- **Tweet Lengths:** Exploring differences in tweet lengths by class\n- **Word Frequency:** Exploring common words by sentiment class\n- **Vocabulary:** What are the differences in vocabulary used among the different classes?\n- **Retweets** Are users retweeting tweets or creating original content and how do retweets vary among the classes?\n- **Hashtag Analysis:** What are the most common hashtags and how do they relate to each class?\n- **Username Analysis:** What are the most common username mentions and how do they relate to each class?\n- **URL analysis:** Are URLs relevant to the classification of sentiment?\n- **Emoji Analysis** What are the most common emojis and how do they relate to each class?","2f7d2219":"### Define Stop Words\nThe stop words list was adapted from the NLTK English stop words package. The word 'amp' which is common in extracted tweets and signifies the '&' sign were added and the word 'not' was removed from the list. \n\nFrom initial research we found that the word 'not' was commonly removed from most generic stop word lists used in sentiment analysis applications as this could alter the meaning of a sentence. \n\nFor example, \"Leo's not wrong about the severity of climate change\" (Pro sentiment) would become \"Leo's wrong about the severity of climate change\" (Anti sentiment) - two polar opposite sentiment classes from the removal of a single word. ","53707d79":"#### Metrics: Models using Clean Processed Tweets","224c19e8":"*Source: Wikipedia*","5a76bab2":"### Logistic Regression\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable. It may also be applied in multiclass aplications using the one versus rest (ovr) approach.","07e8fc21":"#### Observations:\n- The total number of tweets that are or contain retweets is 9686 which is equivalent to 61% of the train tweets.\n- The Pro sentiment has the greatest number of Retweets. Almost 67% of Pro tweets comprise retweets. This could be a contributing factor to the lower unique word ratio, as if a tweet is retweeted several times, this brings down the uniqueness.","dee0d1f2":"### Average word length per Tweet by Class","5a3a0cee":"# Model Performance Evaluation <a class=\"anchor\" id=\"perf\"><\/a>\n[Back to Table of Contents](#toc)","5f812f8b":"# Final Model Selection & Tuning <a class=\"anchor\" id=\"select&tune\"><\/a>\n[Back to Table of Contents](#toc)\n\nFrom critical analysis and testing the model with the best f1-score on both the Validation test & Test sets was:\n\n- SVC trained on raw tweets with no balancing techniques.\n\n\n## Hyperparameter Tuning & Comet Implementation ","18477338":"#### Observations:\n- The average word length for News tweets is marginally higher (almost 8 characters per word) than other classes (approximately 7 characters per word).","b4ba919c":"### Naive Bayes\n\nA classification method based on Bayes\u2019 Theorem that assumes independence among predictors and generally is used for large data sets.","da6ca08d":"### Number of Words per Tweet by Sentiment Class","5c9b4449":"#### Observations: \n- From the above visualisations we observe that although tweets falling into the Pro sentiment class use the most unique words, in proportion to the total words used, the uniqueness of the words used is approximately 15%, and the lowest proportion of any class. This indicates that words are repeated more frequently than any other class of sentiment.\n- Tweets in the neutral sentiment class use the highest proportion of unique words (approximately 33%) followed by anti and news sentiment classes.","e1b96516":"### Top 10 URLs","276abf73":"## Data Cleaning\n### Define Contractions\nContractions need to be expanded into full text for standardization, for example, \"you're\" becomes \"you are\". This also prevents the loss of words and meanings when punctuation is removed. ","9a89d6bf":"![title](https:\/\/github.com\/rohinijagath\/Images\/blob\/master\/Banner2.png?raw=true)","e16f8c54":"## Pitfalls: Swiss Tweets (Neutral Class Tweets)\n\nFrom the classification reports, we noted that the one class that every model performs poorly on (regardless how the data it is trained on it cleaned or balanced) is the Neutral Class, bringing down the overall macro f1-score in every case even after rebalancing of the data.\n\nIn general there were too many false negatives (low recall - between 0.25 & 0.45) for observations that were of neutral sentiment in the validation test sets indicating that the models misclassified neutral tweets. This brought down the f1-score considerably for the neutral class, with the highest achieved by SVC of 0.47. Almost half of the neutral sentiment tweets were misclassified with approximately 35% misclassified as Pro sentiment. In turn 8% of Pro tweets were misclassified as Neutral.\n\nThis could be due to an inherent problem with the Neutral class, as Tweets in text form do lack context in that hyperlinks, photos, memes and media are not preserved. During EDA, one of the key insights was that neutral tweets either had no defining characteristics or contained a lot of words that were common to both the Pro & Anti Classes.\n\nTo confirm our suspicion we conducted a VADER (Valence Aware Dictionary for Sentiment Reasoning) analysis to assess the polarity of neutral tweets","da7d7501":"### Observations\n- The visualisations indicate that the training data set is unbalanced, with approximately 54% of tweets classified as 'Pro' sentiment. Only 8% of the tweets are considered 'Anti' climate change. \n- Bias toward classes with more observations is an inherent issue machine learning models, as the model would tend to overfit the training data and perform poorly on unseen data and disregard the underlying features of other classes during training. \n- The issue of unbalanced data will be revisited in Data Processing and Model Analysis.","ad765a5c":"## Import Packages","3cb473bb":"## Emoji Analysis\nAn emoji is a digital icon used in text & various social media platforms such as Twitter. Emojis vary from facial expressions to objects reflecting places, gestures, natural elements or even types of weather for example a snowflake. Emojis could potentially provide an indication of sentiment and further analysis is warranted.","93968603":"#### Observations:\n- From the above visualisation it is clear that there are specific hashtags that are associated with each class, for example:\n    - Various News & media outlets with News sentiment\n    - Public & political figures active in advocation for Climate Change mitigation and Pro Sentiment\n- There is very little overlap in usernames among the groups, with some exceptions, for example, '@realdonaldtrump' (mentioned in pro, anti & neutral sentiment tweets) & '@cnn' (mentioned in news & pro tweets)","239ba41a":"# Conclusion and Recommendations <a class=\"anchor\" id=\"c&r\"><\/a>\n[Back to Table of Contents](#toc)","a831ded5":"## URL Analysis\nA Uniform Resource Loacator or a URL, is basically a web address or link. Are URLs at all relevant to the analysis of tweet sentiment? ","47812e28":"### Test Set Prediction Submission","b43d8cef":"### Train New Models","20ec6a60":"### Basic Cleaning for EDA\nA custom text cleaning function was defined incorporating all the above-mentioned text cleaning in one step. Additional cleaning was also included to focus EDA on the most common and meaningful words used. The EDA text cleaning function executes the following steps:\n\n- lowercase all words to standardize text\n- expand words to full form\n- remove noise such as URLs and punctuation\n- remove frequently occurring words (stop words) to focus in on more meaningful words that indicate sentiment during EDA.","b78fed99":"### News Sentiment ","2096a1d1":"### Top 10 Username Mentions Overall","4cbcb82c":"## Train (Validation) Test Split\nThe train data is split into train and validation test sets to gauge model performance.","79399e28":"### Model Analysis (Comparing Classification Methods)\n\nSupport Vector Classification (SVC) performed consistently on the validation test set and on unseen data. \n\nLogistic Regression achieved the best F1 & Recall scores on the validation test set, as well as the highest score on Kaggle (0.72392). This indicates that the model was able to generalise slightly better on unseen data in comparison to SVC.\n\nBoth Decision Tree and Naive Bayes performed poorly on the validation and test data sets. Naive Bayes is generally better on smaller training sets as it is a high bias, low variance classifier. Random Forest performed better than Decision Tree, as expected, but still fell short in terms of classification in comparison to SVC & Logistic Regression.\n\nWe decided to evaluate and test both the SVC and Logistic Regression models further as they demonstrated the best performance in both test sets.","0fa54c65":"### Evaluation of Models: Raw vs Clean Tweets","a44b0f3f":"## The Effect of Text Cleaning, NLP Techniques & Balancing Data\nMost NLP literary sources recommend that text data is cleaned, stop words be removed and words be reduced to their root form either by stemming or lemmatization, however, when testing base models on the raw tweet data we found the opposite to be true. These models also used the data as is, with no balancing techniques implemented.\n\nIt was deemed prudent to test the top two best performing models using the unbalanced, uncleaned, untransformed tweets to assess the impact of the recommended cleaning, NLP and balancing techniques on model performance. The suffix u denotes unprocessed\/uncleaned\/unbalanced data.\n\n### Vectorize & Split Data into Train & Validation Test","3c134e61":"# Imports <a class=\"anchor\" id=\"imports\"><\/a>\n[Back to Table of Contents](#toc)","15ff875c":"#### Observations: \n- Aside from the obvious common words, the neutral tweets contain a mix of climate change related words found in other classes of tweets. This overlap could be an issue in classification of tweets by the model.\n- The words 'penguin' and 'club' are a nod to the popular meme 'If global warming isn't real why did club penguin shut down?', further echoing the informal nature of neutral sentiment tweets.","5a51fd74":"# Challenge Introduction & Rationale\n\nClimate change has long been acknowledged by the scientific community as a critical issue facing mankind. As the impacts of Climate Change become more pronounced, many Companies are incorporating Corporate Environmental Responsibility (CER) practices and measures in an effort to reduce environmental impacts and lessen carbon footprint. However, even with scientific consensus around Climate Change, public opinion on the subject may vary. \n\nThe goal of this challenge is to build a Classification Machine Learning model that will determine whether a person believes in Climate Change using tweet data. This model will provide insights of public opinion of Climate Change & consumer sentiment to companies looking to market their new or improved products or services to consumers, in response to CER.\n\nAs the demand for sustainable, eco-friendly products and services by consumers increases, a sentiment classification model that identifies these potential customers is key and could be used any business or organisation committed to carbon neutrality & wanting to inform marketing strategies. This includes, but is not limited to companies in the retail, automotive, government, agriculture & food, pharmaceutical spheres. The model could also be used by sectors in government wanting to identify the various belief sentiments in order to better direct environmental awareness and education campaigns in alignment with their legislative directives and climate change response plans.","a289275f":"### Overall Insights on Tweet Length\n\nOverall, with the exception of outliers:\n\n- Tweets in the Pro Climate Change belief sentiment have more characters and words than other classes (cluster of peach hued points, top right).\n- News Tweets use fewer but longer words than other classes (larger red points clustered together)\n- Significantly fewer Anti and Neutral tweets (dark and light blue hued points)\n\n","a8e47934":"## Objective \nPredict an individual\u2019s belief in climate change based on historical tweet data & provide insights into the various climate change sentiment classes.","aafcbbbe":"#### Observations:\n- As expected, topical words surrounding the issue of Climate Change are the most prevalent. These include 'climate', 'change', 'global', 'warming'. \n- Social & Political figures and themes are also significantly common: 'trump', 'president', 'people', 'government', 'obama', 'donald', & 'human'.\n- Words such as 'scientist', 'science', 'believe', 'hoax', 'fight', 'action', 'real', 'think', 'denier' & 'right' represent the polarity of views on climate change.\n\nLet's explore the most common words by sentiment class further.","09b92520":"From the above visualisation we observe that all the urls have the t.co domain name, and almost 50% of tweets do not have urls at all. Let's explore this further using pandas dataframe methods.\n\n### Exploring Domain names\nURLs comprise domain names, which could provide an indication of the type of site visited or themes\/sentiment reflected in the tweet.","43429b69":"## Evaluating the Trained Base Models\n### Model Metrics\nMacro average F1, precision and recall scores will be evaluated for model selection. The macro average F1 score is the ultimate measure of performance used on the test data on Kaggle.","4623d082":"The data has been sufficiently cleaned and is ready for further analysis and visualisation.","9527ee93":"## Summary of Key Insights from the EDA\n### Overall Insights\n1. The train data is unbalanced, with Pro sentiment accounting for over 50% of observations. Rebalancing techniques should be employed to limit bias toward the Pro Class.\n2. Some Usernames and Hashtags are more prevalent or unique in some cases to specific classes eg. 'maga' in Anti Sentiment Tweets. A few hashtags could be expanded into meaningful words.\n3. 99% of URLs are from the t.co domain and are therefore not meaningful to classifying sentiment classes. \n4. Emojis although more common in some classes appear infrequently overall (occur in <2% of all tweets).\n5. Tweet lengths overall (all classes) range between 14 and 208 characters.\n\n### Insights by Class\n\n- Tweets in the Pro & Anti Sentiment classes could be characterised as having longer length, using the most words overall.\n- Tweets in the Pro Sentiment use the greatest number of words per tweet.\n- The Tweets are laden with political figures and themes reflecting how politically charged the issue is.\n- Many words reflect the polarity in opinions on climate change and may be found in the Anti & Pro Classes.\n- Pro sentiment tweets comprise 67% Retweets, thus contributing to the low Unique Word Ratio.\n- Neutral Tweets contain the most unique words, perhaps reflecting the spectrum of neutral topics and opinions. They also contain more emojis and references to memes, reflecting informality or casualness. \n- News Tweets have fewer, but longer words. They contain the fewest emojis and the most URLs thus reinforcing the formal and factual nature of tweets from news outlets.\n- There seem to be distinguishing features or characteristics for each class with the exception of Neutral Tweets, so Neutral sentiment may be difficult to predict.","b8a2e019":"### URLs by Sentiment Class","da40f14b":"## Model Predictions\nThe trained models are now used to predict the sentiment values of the validation test set.","74010139":"## Exploring the Twitter Lexicon by Sentiment Class\n\n### Unique Words used\nWe begin by defining functions that will return the no of words used and total unique words used:","ed012abd":"## Data Balance\n### Tweet Distribution by Sentiment Class","2a0573c1":"## Exploring Tweet Length\nThe length of tweets in their original form is explored further and compared among sentiment classes. We begin by creating additional tweet features for tweet length. ","963010b3":"**Observations:**\n- A total of 2598 hashtags are present in the train corpus.\n- A total of 1706 unique hashtags are present in the train corpus.\n- The most common hashtags are centred around climate change & political themes.\n- #quanda is actually a text abbreviation and could be expanded to 'question and answer'.\n\nNotable hashtags are:\n- #beforetheflood: Reference to Climate Change documentary with Leonardo DiCaprio \n- #cop22: Reference to 2016 United Nations Climate Change Conference \n- #parisagreement: Reference to the Paris Agreement within the United Nations Framework Convention on Climate Change signed in 2016.\n","46ef52b8":"## Balance Data\n\nBalancing is required in order to address the data imbalance (pro sentiment comprises 54% of data) such that all classes are weighted evenly in order to improve model performance on unseen data.\n\nSeveral methods were tried, including random oversampling and undersampling. The method that yielded the best performance from the various models was Synthetic Minority Oversampling Technique (SMOTE).\n\nThe SMOTE technique entails creating 'synthetic' samples from the minority class rather than duplicating samples (oversampling) or undersampling the majority class.","165a7b86":"#### Observations: \n- Aside from the obvious common words, the anti sentiment tweets are politically charged with mentions of 'trump' & 'obama'  referring to Donald Trump & Barack Obama.\n- Contrasting words such as 'science', 'real', 'data' and 'fake', 'hoax' & 'scam' are prevalent, and also indicative of the anti sentiment.\n- Donald Trump's personal twitter handle is also in the top ten common anti sentiment words, potentially indicating the presence of retweets of his tweets.","5d8f207b":"#### Observations:\n- There are 8295 tweets containing URLs in the train corpus. Of these, 7234 are unique urls\n- Basically all urls contain the domain t.co\n- From further research, t.co are shortened links used on the Twitter Platform. Anytime a user composes a tweet within Twitter that includes a link, that link is converted to a t.co domain.\n-  We can therefore infer that the url in a tweet would probably not be an indication of sentiment as the url's have the same domain names. The urls could either be discarded completely or substituted as a string 'web-url'.\n- The most frequent url appears 307 times, which is the equivalent of 1.9% of tweets.","b5922a4a":"#### Observations: \n- Aside from the obvious common words, the pro tweets also contain political undertones. The words 'president' and (Melania Trump's) 'husband' again refer to Donald Trump. ","f5d98b57":"## Installations\n*(uncomment the libraries\/packages needed for installation)*","2f323b31":"## Retrain Final Model with Best Parameters","aefd41b9":"## Exploring Word Frequency\nThe frequency of words used could give us an indication of the pertinent keywords, characteristics or themes relating to the data as a whole and by sentiment class. Let's explore word frequency visually!\n\n### Most Common Words used Overall\n\nA word cloud is a useful visualisation to assess the most common words present in the corpus in a single glance. The size of the word is a measure of the frequency of its occurrence. \n","ec4d5edd":"**Observations:**\nPro & News Tweets contain the most URL appearances. This could be biased as Pro sentiment class contains the greatest number of tweets regardless. Let's explore the frequency of URLs in proportion to the number of tweets per class.","fe2d237d":"#### Observations:\n\n- There are 11808 username mentions in total in the train corpus.\n- There are over 7141 unique username mentions in the train corpus.\n- There are 6140 usernames only mentioned once. These usernames could be considered as adding noise to the data and could potentially be removed.\n- The most common username mentions include:\n    - Political figures such as Bernie Sanders, Donald Trump & public figures active in advocation for Climate Change mitigation such as Leornado DiCaprio & Seth Macfarlane.\n    - Media outlets such as CNN, National Geographic & The New York times","dded9c7c":"## Data Cleaning with updates\n\nFrom our analysis the processing will comprise the same cleaning as conducted previously but with the following additions:\n\n- Handling Twitter words & characters:\n    - Keep '@' to distinguish between a usernames and the regular words\n    - Keep Hashtags to distinguish between a hashtag and the regular words\n    - Replace urls with 'web-url' to denote presence of a url\n- Update text abbreviation dictionary with additional abbreviations found in the EDA.\n- Lemmatizing\/Stemming: reduce words to root form. We chose Lemmatization after comparing initial results on a base model. Lemmatization out performed both Porter and Snowball Stemmers.","c52ef58a":"*Source: Personal knowledge as Millenials & Gen Z's who text & tweet & Webopedia*","ff89c656":"### URL Frequency per Tweet by Sentiment Class","680b4b07":"The following parameters were selected for hyperparameter tuning of the best model using Grid Search, which is an exhaustive search through a specified subset of hyperparameters:\n<br>\n* **C** - Regularization parameter\n<br>\nThe Grid Search is conducted using the whole training set and includes cross validation to determine the best parameters for the specified scoring metric.","4a19a3f6":"## Hashtag Analysis\nA hashtag is a word preceded by the '#' symbol. Hashtags are used to identify text on social media platforms by themes or topics and serve to make these posts easily accessible or followable by users. Hashtags could therefore be an indicator of sentiment if they are unique or used more frequently in specific classes of tweets. We begin by defining a function that returns the most common hashtags.","415f5a45":"### Analysis of Common Username Mentions & Sentiment","289ff5d5":"### Analysis of Common Hashtags & Sentiment\n\nLet's compare the top ten hashtags of each sentiment class with eachother:","f6090492":"1. Scikit-learn USer Guide: https:\/\/scikit-learn.org\/stable\/index.html\n2. Twitter Help Centre: https:\/\/help.twitter.com\/en\n3. Wikipedia Contractions List: https:\/\/en.wikipedia.org\/wiki\/Contraction_(grammar)\n4. An Introduction to Statistical Learning by Gareth James, Daniele Witten, Trevo Hastie & Robert Tibshirani\n5. EMojipedia: https:\/\/emojipedia.org\/\n6. NLTK Documentation: https:\/\/www.nltk.org\/\n7. Webopedia: https:\/\/www.webopedia.com\/quick_ref\/textmessageabbreviations.asp","1c6f3fb7":"### References","a5c21007":"## Feature Extraction\n\nFeature extraction from text may be done in two ways: Bag-Of-Words and TF-IDF techniques. The Bag of Words(or BOW) counts occurrences of unique words, whereas Term Frequency-Inverse Document Frequency(TF-IDF) uses statistics to weight the relative importance of each word in document. Both methods were tested on base models and TF-IDF performed consistently better.\n\nThe cleaned train and test data is now converted into a matrix of TF-IDF features for the machine learning model.","a6e929af":"**Observations:**\n- The most common emoji is the 'Face with Tears of Joy' emoji, one of the most common emoji between 2014-2018 and is used to show something as funny or pleasing. This could hint at sarcasm, humour or meme culture present in tweets.\n- 'Thinking Face' emoji (third most common) represents thinking or questioning belief.\n- Fire and Snowflake emojis make the top ten and perfectly illustrate the extreme impacts of global warmaing on weather and climate change.\n- 'Face with rolling Eyes' appears 11 times, and conveys disdain or disapproval, perhaps indicating frustration over climate change issues, regardless of opinion\/belief.\n- The Skull emoji commonly represents figurative death.\n- The black arrow is the play emoji perhaps used in tweets with video media attached.\n\n*Source: emojipedia*","ecd0181a":"### Define Text Abbreviations\nTexting language on social media platforms frequently uses text abbreviations especially when character limits are enforced as on a platform like Twitter. Similar to contractions these common text abbreviations need to be expanded to convey full meaning \/ sentiment in tweets.","28b6fa2e":"## Overview of Challenge Approach \n\nThe approach to this analysis comprises of four key areas:\n- Preparation, cleaning and transformation of Data\n- Exploratory Data Analysis to reveal key insights of classes and features.\n- Preparation of data for Machine Learning\n- Development of several classification models\n- Evaluation & Comparison of model performance using the specified metrics\n","3848d7f0":"### Pro Sentiment","53afdf5a":"### Decision Tree\nDecision Tree is a non-parametric supervised learning method that may be used for classification. The resulting model predicts the target variable by partitioning or splitting data features into subsets & decision\/rule based learning thereby growing a 'tree'.","d582f133":"### Top 10 Common Hashtags Overall","0428764d":"## Data Completeness\nBoth test and train data sets are complete, there are no null values present.","a3766297":"#### Neutral Sentiment","7adeeac3":"**Obervations:**\n- Emojis seem to appear a lot more in the Neutral tweets and very little in the News Tweets, showing that there's formality to the News Tweets and informality to Neutral Tweets.\n- The most frequent occurrence is for the Neutral Class (approximately 1 emoji every 38 tweets).\n- There are approximately 180 emojis in the train data overall, not a significant amount of emoji data to be impactful, thus emojis could be removed from the Tweet data.\n- The lack of emojis could possibly attributed to social media platform as twitter limits characters in Tweets, and as these tweets are centred around a serious & divisive environmental issue, emojis are used less frequently.","1d2e4767":"#### Anti Sentiment","a1109923":"#### Observations:\nFrom the above visualisation, it's clear that there are specific hashtags that are associated with each class, for example:\n- '#beforetheflood', '#imvotingbecause', '#ivotedbecause' and Pro\n- '#maga', '#fakenews' and Anti\n- '#cnn', '#news' and News\n\nHashtags such as '#maga' could be expanded to 'make america great again'.","4a24143f":"\n# Classification Models <a class=\"anchor\" id=\"models\"><\/a>\n[Back to Table of Contents](#toc)\n\nSeveral classification models using different methods were trained and the performance of each on the validation test set was compared.\n\n## Training Models\n\n### Support Vector Classifier (Linear-SVC)\nThe support Vector Classifier is a generalisation of the simple maximal margin classifier and makes use of 'support vectors' on the edge or within a soft margin to classify observations. ","395c60e6":"The train dataset contains tweets that have been classified into four classes (Anti, Neutral, Pro and News) as shown in the sentiment dataframe.","aa36e186":"## Load Data\nThe collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. ","c8767857":"### Most Common Words by Sentiment Class\nExamining the most common words by sentiment may provide insights into the distinguishing features of each class, as well as the overlap or similarities between tweets.","96554ebc":"## Data Structure & Types\n\nThe test & train datasets comprise aggregated tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. The data is structured as follows:\n- **sentiment:** the sentiment classification of the tweet (int)\n- **message:** the tweet body (object)\n- **tweetid:** unique id of the tweet (int)\n\nThe sentiment for the test data is the target to be predicted and as such is excluded from this set.","cefe1067":"# Exploratory Data Analysis (EDA) <a class=\"anchor\" id=\"eda\"><\/a>\n[Back to Table of Contents](#toc)","df06f3b6":"# Data Preparation <a class=\"anchor\" id=\"dp\"><\/a>\n[Back to Table of Contents](#toc)","bae5aa1d":"## Retweets by Sentiment\n\nAs defined by Twitter: \"A retweet is a re-posting of a tweet. Twitter's Retweet feature helps you and others quickly share that tweet with all of your followers\".\n\nLet's take a look at retweet distribution among the sentiment classes.","a0c8a9ca":"The best value for the Regularisation Parameter as determined by the Gridsearch was C=1.0. The output of the Gridsearch was:\n\n- Best Score:  0.6665654677010584\n- Best Params:  {'C': 1.0}\n\nThe code has been commented out along with the comet implementation for the kaggle notebook due to runtime & RAM issues.","4d99b657":"#### Observations: \n- Tweet lengths overall (all classes) range between 14 and 208 characters including outliers. Twitter's character limit per tweet changed from 140 to 280 characters in 2017, which accounts for this variability, as the tweets in the train data range between the years 2015-2018.\n- The greatest mean tweet length in terms of character usage is observed for the sentiment class \"Pro\" (127 characters per tweet). \n- The lowest mean tweet length (characters) is observed for the sentiment class \"Neutral\" (110 characters per tweet).\n- The greatest variation in tweet lengths is observed for the sentiment class \"Neutral\".","bf4e87ac":"## Username Mentions Analysis\nAs defined by Twitter, a mention is a Tweet that contains another user's username anywhere in the body of the Tweet. Username mentions could be an indicator of sentiment if they are unique, or used more frequently in specific classes of Tweets. We begin by defining a function that will return the most common usernames from the tweet corpus.","de41b7b2":"#### Observations\n- Higher mean word count per tweet is observed for both Pro & Anti Sentiment classes (approximately 19 words per tweet), with Pro sentiment edging out Anti sentiment marginally overall. \n- News & Neutral classes have a mean no of words of 15 & 18 words per tweet respectively. The lower word count could potentially indicate that News tweets use fewer, but longer words. ","5615562b":"From the VADER sentiment analysis we observe that under 25% of neutral tweets reflect neutral sentiment. This illustrates the overlap that neutral tweets have with Pro and Anti sentiment classes. The train set initially had four duplicate tweets that were classified as belonging to two different classes, which could potentially indicate discrepancies or errors in the classification of the train tweets.\n\nSignificant testing and tuning was conducted to try to improve overall model performance but the neutral class consistently underperformed due to the 'grey areas' that the neutral sentiment class contains. ","a0ee5de6":"Text data such as tweet data  is typically considered unstructured or semi-structured data. In order to derive meaning and insights from the data during EDA, some degree of data cleaning and preprocessing is required in order to standardize the text before analysis. From examining the data, the following steps are deemed necessary before further exploration:\n\n1. Extract Twitter Information: hashtags, mentions, emojis and URLs for analysis.\n2. Basic Cleaning: Lowercase, Expanding contractions & abbreviations, removing stop words & more. \n    \nThe level of data cleaning and preparation for the model will be evaluated again after insights from the EDA as certain steps may or may not be required.\n\n## Extract Twitter Information\nWe begin by defining functions to extract the tweet information (URLs, hashtags, user mentions & emojis) for analysis.","fb2fa4d8":"#### Observations: \n\n- Tweets classified as Pro Climate change contain the most unique words\n- Tweets classified as Anti Climate change contain the least unique words (approximately four times less than the Pro sentiment class)\n\nThis could be considered as biased as the Pro climate change sentiment accounts for over 50% of the observations regardless. Let's explore the unique words used in tweets as a proportion of the total words used in the tweet corpus for each class.\n\n### Unique Word Ratio\n\nUnique word ratio is the Unique Words used in tweets of a per sentiment class divided by Total Words of that specific sentiment class.","2c771aaa":"#### Metrics: Models using Raw Unprocessed Tweets","c47fc700":"- Several sentiment classification machine learning models were trained and tested on tweet data. The tweets were cleaned, pre-processed and the data was balanced using the SMOTE algorithm.\n- Logistic Regression & SVC were the top performing model choices, with SVC producing the best f1-score on Kaggle. \n- Training models and making Predictions using raw data was also tested against processed data. It was found that both models performed better using tweets in original form. This is attributed to the inherent nature of tweets, which are short and processing the text basically leaves the model with fewer features to train on.\n- Theoretically, it would be best practice to clean data & use NLP techniques for large data sets, comprising longer texts, however for the purpose of this challenge, the models trained on raw tweets performed better & one of these, Logistic Regression, was selected as the final submission.\n- The models also performed better when trained on unbalanced data, which we believe to be because the actual probabilities of a climate change related tweet belonging to a particular class is reflected in the (unbalanced) distribution of the data itself.\n- Testing on a base model is recommended. This allows for adding or peeling back layers of cleaning, processing, and balancing techniques and logging results of each experiment using Comet is ideal when determining how best to treat the data. \n- The final model predicts the sentiment of a tweet as one of four different classes (Anti, Neutral, Pro & News) with a macro F1-score on Kaggle of 0.75260 on unseen data.\n- It was theorised, after EDA on the tweets, that there may be problems with misclassification of neutral tweets as there was overlap in characteristics with other Classes.\n- After training and model evaluation it was discovered that the Neutral class did indeed have lower recall than other classes regardless of classification method, balancing or data processing. A subsequent Vader Analysis showed that the majority of neutral tweets echoed Pro or Anti Sentiment Classes. \n- The model could be used to inform marketing strategies of companies trying to reach out to Pro \/ Neutral Belief users on Twitter, in order to market their 'green' and climate change mitigating products or services.\n- To this end, as a large number of neutral tweets are misclassified as Pro sentiment, if the potential target market includes both Pro & Neutral twitter users, the model largely achieves the objective of the classification process.\n- It is recommended that the Sentiment classes are readjusted to absorb neutral tweets into the Pro and Anti Sentiment Classes, that is reclassify tweets as either Anti, Pro, or News.\n- Alternatively, more data could be obtained and the models retrained. Having more observations could assist in obtaining better recall on neutral tweets and other classes in general. Deep Learning methods could also be employed to improve results.","3859ddd6":"### Random Forest\nRandom Forest is an ensemble learning method based of Decision Trees that is less prone to overfitting. Random forest instead considers a different random subset each time a tree is 'grown'. The final prediction of the target variable comprises an average of all predicted values from the various estimators.","fa3bf9dd":"### Characters per Tweet by Sentiment Class"}}