{"cell_type":{"19044f27":"code","2115821f":"code","435c4794":"code","ff16eb87":"code","76c6b6c7":"code","b01d4482":"code","bec21cd8":"code","b187de16":"code","59b514f3":"code","8bec6d9a":"code","6d81aece":"code","c414753f":"code","b804e3c8":"code","44f7476b":"code","64a78ba5":"code","a04f1103":"code","8e045145":"code","296cbf63":"code","28f1ca4b":"code","80f1440a":"markdown","c7fd3f63":"markdown"},"source":{"19044f27":"!pip install spacy\n!python -m download en_core_web_sm","2115821f":"# import libraries \nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom string import punctuation\nfrom heapq import nlargest","435c4794":"text = \"\"\"\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\nChallenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation.\nSince the so-called \"statistical revolution\" in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.\nMany different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\nSome of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\nSince the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.\nA major drawback of statistical methods is that they require elaborate feature engineering. Since the early 2010s, the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). Latest works tend to use non-technical structure of a given task to build proper neural network.\n\"\"\"","ff16eb87":"stopwords = list(STOP_WORDS)\n# length of stopwords in spacy\nlen(stopwords)","76c6b6c7":"nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)","b01d4482":"# word tokenization\ntokens = [token.text for token in doc]\ntokens","bec21cd8":"# word frequency\nword_frequency = {}\nfor word in doc:\n    if word.text.lower() not in stopwords:\n        if word.text.lower() not in punctuation:\n            if word.text not in word_frequency.keys():\n                word_frequency[word.text] = 1\n            else:\n                word_frequency[word.text] += 1","b187de16":"word_frequency","59b514f3":"# normalized frequency\nmax_frequency = max(word_frequency.values())\nfor word in word_frequency.keys():\n    word_frequency[word] = word_frequency[word]\/max_frequency","8bec6d9a":"word_frequency","6d81aece":"# sentence tokens\nsentence_tokens = [sent for sent in doc.sents]\nlen(sentence_tokens)","c414753f":"sentence_scores = {}\nfor sent in sentence_tokens:\n    for word in sent:\n        if word.text.lower() in word_frequency.keys():\n            if sent not in sentence_scores.keys():\n                sentence_scores[sent] = word_frequency[word.text.lower()]\n            else:\n                sentence_scores[sent] +=word_frequency[word.text.lower()]","b804e3c8":"sentence_scores","44f7476b":"len(sentence_tokens)","64a78ba5":"select_length = int(len(sentence_tokens)*0.3)\nselect_length ","a04f1103":"summary = nlargest(select_length,sentence_scores,key=sentence_scores.get)","8e045145":"summary","296cbf63":"final_summary = [word.text for word in summary]\nfinal_summary = ''.join(final_summary)","28f1ca4b":"final_summary","80f1440a":"# Text Summarization\nText summarizsation is the technique of shortening long pieces of text. The intention is to create a coherent and fluent summary having only the main points outlined in the document.It can be done by two approaches:\n- **Extractive Summarization** : We identify the important sentences or phrases from the original text and extract that from the text\n- **Absractive Summarization** : This is the amazimg technique in which a new sentence is generated from the original text","c7fd3f63":"In this notebook we are going to use Extractive Summarization technique.First we will going to clean the text,then we will create tokens (sentence and word both) after that we will going to create word-frequency dictionary by that we came to know about the most important sentences and then take 30% of the text based on scores"}}