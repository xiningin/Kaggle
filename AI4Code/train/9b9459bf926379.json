{"cell_type":{"407cec83":"code","f2f55eac":"code","825c126d":"code","b7ca79ae":"code","883e09d7":"code","770e0077":"code","9b8f7bff":"code","2224997c":"code","11493128":"code","b5d95739":"code","1b6580d2":"code","e6cb4d85":"code","88072984":"code","729e9b05":"code","56e3e72e":"code","f5c55189":"code","687f630b":"code","267a3cca":"code","5b110557":"code","7a057714":"markdown","e87a1c49":"markdown","1d258d0d":"markdown","623ef03d":"markdown","527f58d3":"markdown","fe83fcfc":"markdown","c9e7276b":"markdown","89c115bf":"markdown","1936a3d9":"markdown","7ff4a0fb":"markdown","a3c37636":"markdown","c5333079":"markdown","9aa562cf":"markdown"},"source":{"407cec83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2f55eac":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_confusion_matrix","825c126d":"# Prepare your file\nparent_dir: str = os.path.join('\/kaggle', 'input', 'heart-failure-clinical-data')\ndataset_name: str = \"heart_failure_clinical_records_dataset.csv\"\ndataset_path: str = os.path.join(parent_dir, dataset_name)\nprint(f\"Dataset directory: {dataset_path}\")","b7ca79ae":"# Load data\nheart_failure_df: pd.DataFrame = pd.read_csv(dataset_path)\nheart_failure_df.head()","883e09d7":"def plot_survival_vs_binary_variable(data: pd.DataFrame, predicted_col: str, response_col: str, unique_labels: list):\n    # Preprocess the data\n    positive = data[data[predicted_col]==1]\n    negative = data[data[predicted_col]==0]\n    \n    # Extract values and labels\n    data_values = zip([positive, negative, positive, negative.copy()],\n                      [0,0,1,1])\n    values = [i[i[response_col]==j].shape[0] for i, j in data_values]\n    \n    # Extract labels\n    labels = [f\"{label} - Survived\" for label in unique_labels] + [f\"{label} - not Survived\" for label in unique_labels]\n    \n    # Plot Figure\n    fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n    fig.update_layout(\n        title_text=f\"Analysis on Survival - {predicted_col.title()}\")\n    fig.show()","770e0077":"# By Sex\npredicted_col: str = \"sex\"\nresponse_col: str = \"DEATH_EVENT\"\nunique_labels: list = [\"Male\", \"Female\"]\n\n# Call Function\nplot_survival_vs_binary_variable(data=heart_failure_df, \n                                 predicted_col=predicted_col,\n                                 response_col=response_col,\n                                 unique_labels=unique_labels)","9b8f7bff":"# By Diabetes\npredicted_col: str = \"diabetes\"\nresponse_col: str = \"DEATH_EVENT\"\nunique_labels: list = [\"Yes\", \"No\"]\n\n# Call Function\nplot_survival_vs_binary_variable(data=heart_failure_df, \n                                 predicted_col=predicted_col,\n                                 response_col=response_col,\n                                 unique_labels=unique_labels)","2224997c":"# By anaemia\npredicted_col: str = \"anaemia\"\nresponse_col: str = \"DEATH_EVENT\"\nunique_labels: list = [\"Yes\", \"No\"]\n\n# Call Function\nplot_survival_vs_binary_variable(data=heart_failure_df, \n                                 predicted_col=predicted_col,\n                                 response_col=response_col,\n                                 unique_labels=unique_labels)","11493128":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score","b5d95739":"response_variable: str = \"DEATH_EVENT\"\n\n# Get Predictive columns\npredictive_variables: list = list(heart_failure_df.columns)\npredictive_variables.remove(response_variable)\nprint(predictive_variables)\n","1b6580d2":"# Prepare data (Cross-validation purposes)\nX: pd.DataFrame = heart_failure_df[predictive_variables]\ny: pd.Series = heart_failure_df[[response_variable]]\ntest_size: float = 0.25\n\n# Split data into separate sets (Training purposes)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n\nprint(f\"X_train shape: {X_train.shape}\\n\")\nprint(f\"X_test shape: {X_test.shape}\\n\")\nprint(f\"y_train shape: {y_train.shape}\\n\")\nprint(f\"y_test shape: {y_test.shape}\\n\")","e6cb4d85":"# Scaler\nscaler: RobustScaler = RobustScaler()\nn_features: int = int(X.shape[1])\n\n# Numerical transformer\nnumerical_transformer:Pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy='median',\n                                                                            fill_value=-99)),\n                                                 ('scaler', scaler)\n                                                 ])\n# Preprocessor Transformer\npreprocessor_transformer: ColumnTransformer = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, predictive_variables)])\n","88072984":"# Ensemble model (stacking)\n\nrandom_state=123\nestimators: list = [('RF', RandomForestClassifier(max_features=\"sqrt\",\n                                                  n_estimators=150,\n                                                  random_state=random_state)),\n                    ('GB', GradientBoostingClassifier(max_depth=5,\n                                                     random_state=random_state)),\n                    ('SVM', SVC(C=2))]\nfinal_estimator: LogisticRegression = LogisticRegression()\n\nensemble_model: StackingClassifier = StackingClassifier(estimators=estimators,\n                                                        final_estimator=final_estimator\n                                                        )\nprint(ensemble_model)","729e9b05":"# Complete pipeline\nsteps: list = [(\"preprocessor\", preprocessor_transformer),\n               ('classifier', ensemble_model)]\nml_model: Pipeline = Pipeline(steps=steps)\nprint(ml_model)","56e3e72e":"from sklearn.model_selection import cross_validate\ny_np: np.array = np.array(y).ravel()\ncv: int = 5\nscoring: tuple = ('balanced_accuracy', 'f1', 'precision', 'recall', 'roc_auc')\nscores = cross_validate(ml_model, X, y_np, cv=cv, scoring=scoring, return_train_score=True)\nfor metric_name, score in scores.items():\n    print(f\"{metric_name} mean: {np.mean(score)}, {metric_name} std: {np.std(score)}\")","f5c55189":"# Train the model\nml_model.fit(X=X_train, y=np.array(y_train).ravel())","687f630b":"y_pred=ml_model.predict(X_test)\nprint(f\"Ensemble Model score: {ml_model.score(X_test, y_test)}\")","267a3cca":"# Prepare Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure()\nplot_confusion_matrix(cm, figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues)\nplt.title(\"Ensemble Model - Confusion Matrix\")\nplt.xticks(range(2), [\"Heart Not Failed\",\"Heart Fail\"], fontsize=16)\nplt.yticks(range(2), [\"Heart Not Failed\",\"Heart Fail\"], fontsize=16)\nplt.show()","5b110557":"# Compute \nTP: float = cm[0,0]\nFP: float = cm[1,0]\nFN: float = cm[0,1]\nTN: float = cm[1,1]\n\nsensitivity = (TP \/ (TP + FN))\nprint(f\"Sensitivity: {sensitivity}\")\nspecificity = ( TN \/ (TN + FP))\nprint(f\"Specificity: {specificity}\")","7a057714":"### 2.2 Design and implement An Ensemble model","e87a1c49":"## 2. Build the Pipeline\n### 2.1 Preprocessor\n\nIn this section we are going to build the first part of the Pipeline which is going to be the preprocessor. It will include a dummy simpleInputer which will fill any empty value with the median of the feature and it will scale the input features using a scaler from Sklearn. ","1d258d0d":"# Train the model with Cross-validation","623ef03d":"### 2.3 Organise the final pipeline","527f58d3":"### Analyse Survival By Gender\n\nLet's see whether the gender of the person is relevant when predicting a heart failure","fe83fcfc":"# Final Training and testing of the model","c9e7276b":"Now let's plot our confusion matrix using the predictions we got from the test samples","89c115bf":"# Explore Dataset","1936a3d9":"# Additional Information\n\nAccording to the description of the dataset, some of the aforementioned columns have been binarized for data analysis purposes including:\n* **Sex** - Gender of patient Male = 1, Female =0\n* **Diabetes** - 0 = No, 1 = Yes\n* **Anaemia** - 0 = No, 1 = Yes\n* **High_blood_pressure** - 0 = No, 1 = Yes\n* **Smoking** - 0 = No, 1 = Yes\n* **DEATH_EVENT** - 0 = No, 1 = Yes","7ff4a0fb":"# Setting up the requirements","a3c37636":"# Process Data via Pipelines\n\nLet's preprocess a little bit our dataset in order to build a robust pipeline to perform the final classification task:","c5333079":"# Visualizing the Confusion Matrix","9aa562cf":"## 1. Prepare the data\n\nNow let's prepare our training and validation sets including X and y values"}}