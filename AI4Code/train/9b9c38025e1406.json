{"cell_type":{"f2d2a92d":"code","741a9e72":"code","c15b7390":"code","47dd3e29":"code","4854b317":"code","42384d73":"code","03e9becf":"code","28180b7a":"code","f59c2255":"code","4f229b68":"code","1ddddec5":"code","a8366c5a":"code","2ed9bb06":"code","cba701e3":"code","44655a81":"code","912678ea":"markdown","bb63f877":"markdown","94950e21":"markdown","288fa4ac":"markdown","fe2e2fb4":"markdown"},"source":{"f2d2a92d":"#import all the packages...\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom dateutil import relativedelta\nfrom scipy.stats import boxcox\nfrom scipy.special import inv_boxcox\nimport lightgbm as lgb\nimport datetime","741a9e72":"df = pd.read_excel('..\/input\/timeseries-data\/Data.xlsx')","c15b7390":"df=df[['Date','Key','Volume','avg_T','precipitation']]","47dd3e29":"df.columns","4854b317":"df.Date = pd.to_datetime(df.Date,format='%d-%m-%Y')\nBrand_list = df.Key.unique()","42384d73":"df.dropna(inplace=True)","03e9becf":"final = pd.DataFrame()\n\nfor brand_name in Brand_list:\n    print(brand_name)\n    brand_df = df.loc[df.Key == brand_name]\n    #brand_df = bgt_transformation(brand_df)\n    brand_df.set_index('Date',inplace=True)\n    tmp = []\n    forecast = pd.DataFrame()\n    Actuals = pd.DataFrame()\n    p = pd.DataFrame()\n    k = pd.DataFrame()\n#     brand_df = brand_df[:'2019-10-01']\n    if len(brand_df)>12:\n        train_start = datetime.date(2019, 3, 1)\n        train_till = datetime.date(2019, 12, 1)\n        Actuals_end = datetime.date(2019, 12, 1)\n        train_date = train_start\n        while train_date < train_till:\n            test_date = train_date + relativedelta.relativedelta(months=1)\n            dependent_colume = 'Volume'\n            x = brand_df.drop(columns=[dependent_colume,'Key'])\n            y = brand_df[[dependent_colume]]\n            train_x = x[:train_date]\n            train_y = y[:train_date][[dependent_colume]]\n            test_x = x[test_date:]\n            test_y = y[test_date:][[dependent_colume]]\n            train_date = train_date + relativedelta.relativedelta(months=1)\n\n            params = {\n                'boosting_type': 'gbdt',\n                'objective': 'regression',\n                'metric': {'l2', 'l1'},\n                'learning_rate': 0.05,\n                'feature_fraction': 0.9,\n                'bagging_fraction': 0.8,\n                'bagging_freq': 5,\n                'verbose': 0\n            }\n            \n            try:\n\n                lgb_train = lgb.Dataset(train_x, train_y)\n                gbm = lgb.train(params,lgb_train,num_boost_round=500)\n\n        #         #forecast for next forecast period....\n                if test_date > Actuals_end:\n                    p = pd.DataFrame()\n                    print(list(gbm.predict(test_x[test_date:], num_iteration=gbm.best_iteration)))\n                    p[\"Forecast_values\"] = list(gbm.predict(test_x[test_date:], num_iteration=gbm.best_iteration))\n                    p.index = test_y[test_date:].index\n                    p[\"Brand\"] = str(brand_name) +str(\"_\")+ p.index.month.astype(str) +str(\"_\")+p.index.year.astype(str)\n            \n                    \n                    k = pd.DataFrame()\n                    k = test_y[test_date:]\n                    k.columns = ['Actual_values']\n                    k.index = test_y[test_date:].index\n                    k[\"Brand\"] = str(brand_name) +str(\"_\")+ k.index.month.astype(str) +str(\"_\")+k.index.year.astype(str)\n\n                    break\n                    \n                forecast[str(brand_name)+str('_')+str(test_date.month)+str(\"_\")+str(test_date.year)] = gbm.predict(test_x[test_date:test_date], num_iteration=gbm.best_iteration).reshape(1,)\n                Actuals[str(brand_name)+str('_')+str(test_date.month)+str(\"_\")+str(test_date.year)] = test_y[test_date:test_date].values[0]\n            \n            except:\n                continue\n                \n        if (len(forecast)>0 & len(Actuals>0)):\n            forecast=forecast.T.reset_index()\n            forecast.columns=[\"Brand\",\"Forecast_values\"]\n            if(len(p)>0):\n                forecast= pd.concat([forecast,p],axis=0)\n            Actuals=Actuals.T.reset_index()\n            Actuals.columns=[\"Brand\",\"Actual_values\"]\n            if(len(k)>0):\n                Actuals= pd.concat([Actuals,k],axis=0)\n            brand_wise_merge = forecast.merge(Actuals,on=\"Brand\",how=\"left\")\n            final = final.append(brand_wise_merge,ignore_index=True)\n        else:\n            print(\"doesn't match with LGBM\")\n    else:\n        print(\"length does not match\")","28180b7a":"final.head(25)","f59c2255":"plt.figure(figsize=(15,8))\nplt.plot(final.Actual_values,label='Actual value')\nplt.plot(final.Forecast_values,label='Forecast Value')\nplt.legend()\nplt.show()","4f229b68":"final['Error']=np.abs(final['Actual_values']-final['Forecast_values'])\nfinal[['Brandname','leMonth','leYear']] = final.Brand.str.split(\"_\",expand=True)","1ddddec5":"Agg_accuracy = final.groupby(by=['leMonth']).sum()[['Error','Actual_values']]\nAgg_accuracy['Accuracy'] = np.round((1-(Agg_accuracy['Error']\/Agg_accuracy['Actual_values']))*100,2)","a8366c5a":"Agg_accuracy_brand = final.groupby(by=['Brandname']).sum()[['Error','Actual_values']]\nAgg_accuracy_brand['Accuracy'] = np.round((1-(Agg_accuracy_brand['Error']\/Agg_accuracy_brand['Actual_values']))*100,2)","2ed9bb06":"Agg_accuracy","cba701e3":"Agg_accuracy_brand","44655a81":"final.to_excel('LGBM_Forecast.xlsx')","912678ea":"Convert date column in datetime format as we have to create index as datetime format only. be careful while giving format of date as it might change later operations. get the unique brand list as we have to iterate over all the brands separately.","bb63f877":"LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n1. Faster training speed and higher efficiency.\n2. Lower memory usage.\n3. Better accuracy.\n4. Support of parallel and GPU learning.\n5. Capable of handling large-scale data","94950e21":"**Actual vs Predicted value Plot**","288fa4ac":"**For modelling we follow differnt steps:**\n\n1. iterate over all brands and take out single brand each time.\n2. provide train and test date based on requirement.\n3. divide train and test data as le cycle works for whole year. ex apr is start so (3+9,4+8 etc..) each 4. time upto december we have to complete, start date could be any.\n5. Select all those indepent regressor\n6. Set the hyperparameter\n7. save the result and iterate till end.","fe2e2fb4":"# *Light Gradient Boosting Machine*"}}