{"cell_type":{"bf2f26cf":"code","439da72b":"code","b93c19bf":"code","1b182c02":"code","736f968b":"code","927190a0":"code","8082f38e":"code","4fd29465":"code","34db51b5":"code","9b5d1b8e":"code","f1c07e98":"code","e130ed45":"code","5ffbddb2":"code","3e355f4f":"code","ffa3f85c":"code","f82a2a72":"code","46e03e34":"markdown","ec5af04a":"markdown","539931ed":"markdown","85d034e7":"markdown","128796e0":"markdown","a10bbfe6":"markdown","194288b5":"markdown","513c610d":"markdown"},"source":{"bf2f26cf":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os \nimport time\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport warnings\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport pickle\nfrom pandas.api.types import is_numeric_dtype\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport itertools\n%matplotlib inline","439da72b":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n","b93c19bf":"# sanity check \nprint(\"Shape of train and test are {} and {} respectively\".format(train.shape, test.shape))\nprint(\"Number of nulls in train and test are {} and {} respectively\".format(train.isnull().sum().sum(), test.isnull().sum().sum()))\n# we actually won't worry as XGB handles missing values well.","1b182c02":"# distribution of target\nstart = time.time()\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(1, 1, figsize=(11, 7), sharex=True)\nsns.despine(left=True)\nsns.distplot(train.Target.values, axlabel = \"target_values\", label = 'frequency', color=\"r\", kde = False)\nplt.setp(axes, yticks=[])\nplt.tight_layout()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\nplt.show()","736f968b":"# Checking columns which are not numeric and convertiong them to factors \nprint(\"Number of columns in train file is {}\".format(train.shape[1]))\n# checking columns that has values other than numbers \nnon_numeric_cols = []\nfor col in train.columns:\n    if np.issubdtype(train[col].dtype, np.number) == False:\n        non_numeric_cols.append(col)\nprint(\"Number of non numeric columns {}\".format(non_numeric_cols.__len__()))\nprint(non_numeric_cols)  \n","927190a0":"# we has to remove id and has to convert all the other variables into factors \ncols_to_factor = ['idhogar', 'dependency', 'edjefe', 'edjefa']\ndef Label_for_cat_var(df, col):\n    \"\"\"Function to define labels for categorical columns\"\"\"\n    le = preprocessing.LabelEncoder()\n    le.fit(df[col])\n    df[col] = le.transform(df[col])\n    del le\n    return(df)\n\nt0 = time.time()\nfor col in cols_to_factor:\n    train_num = Label_for_cat_var(train, col)\nt1 = time.time()\nprint(\"Taken taken in converting categorical variables to numberic in train is \"+ str(t1-t0))\n\n# for test set \nt0 = time.time()\nfor col in cols_to_factor:\n    test_num = Label_for_cat_var(test, col)\nt1 = time.time()\nprint(\"Taken taken in converting categorical variables to numberic in test is \"+ str(t1-t0))\n","8082f38e":"def train_best_model(target_stats, train, target_var_xgboost, features_name):\n    \"\"\"Function to train and save best model for given target variable list\n        Input -\n                target_stats - a dict performance for each target files\n                train - train dataframe contaning all the variables that are needed for xgb\n                target_var_xgboost - List containing all the target variables for xgb\n        Output - \n    \"\"\"\n    for target in target_var_xgboost:\n        index_max = target_stats[target]['score'].index(max(target_stats[target]['score']))\n        parameter_for_max = target_stats[target]['parameters'][index_max]\n        y = train[target].values\n        Xtr, Xv, ytr, yv = train_test_split(train[features_name].values, y, test_size=0.2, random_state=1987)\n        dtrain = xgb.DMatrix(Xtr, label=ytr)\n        dvalid = xgb.DMatrix(Xv, label=yv)\n        #dtest = xgb.DMatrix(test[temp].values)\n        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n        xgb_par = parameter_for_max\n        model = xgb.train(xgb_par, dtrain, 2000, watchlist, early_stopping_rounds=50,\n                          maximize=False, verbose_eval=100)\n        \n        return(model)","4fd29465":"train_, test_ = train_test_split(train_num, test_size=0.2, random_state=1987)\ntrain_.shape\ntarget_var_xgboost = ['Target']\ncols = train_num.columns\nfeatures_name = [x for x in cols if x !='Id']\nfeatures_name = [x for x in features_name if x !='Target']\nprint(\"number of features {}\".format(features_name.__len__()))","34db51b5":"target_stats = {'Target': {'parameters': [{'booster': 'gbtree',\n                'colsample_bytree': 0.3,\n                'eta': 0.1,\n                'eval_metric': 'mlogloss',\n                'lambda': 2.0,\n                'max_depth': 6,\n                'min_child_weight': 10,\n                'nthread': -1,\n                'num_class': 15,\n                'objective': 'multi:softmax',\n                'silent': 1,\n                'subsample': 0.9}],\n              'score': [0.362916]}}","9b5d1b8e":"model = train_best_model(target_stats, train_, target_var_xgboost, features_name)","f1c07e98":"def variable_importance(model, features_name):\n    \"\"\"Function to calculate the variable importance for model\n        Input - \n                model - xgb model \n        Output -\n                var_imp_dict - dict of variable importance \n    \"\"\"\n    feature_importance_dict = model.get_fscore()\n    fs = ['f%i' % i for i in range(len(features_name))]\n    new_feat_number = [x[1:] for x in feature_importance_dict.keys()]\n    \n    f1 = pd.DataFrame({'f': list(feature_importance_dict.keys()),\n                       'feature_names': [features_name[int(x)] for x in new_feat_number],\n                       'importance': list(feature_importance_dict.values())}).sort_values(by='importance', ascending=False)\n\n\n    return(f1)\n\n\n\nfeature_importance = variable_importance(model, features_name)\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(1, 1, figsize=(17, 7), sharex=True)\nsns.despine(left=True)\nsns.barplot(x = feature_importance['feature_names'].head(10), y =feature_importance['importance'].head(10) ) \nplt.show()","e130ed45":"# Checking confusion matrix on validation datasets \n# predictions and checking the performance \nstart = time.time()\ndtest = xgb.DMatrix(test_[features_name].values)\n#yvalid = model.predict(dvalid)\nytest = model.predict(dtest)\nend = time.time()\nprint(\"Time taken in prediction is {}.\".format(end - start))","5ffbddb2":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    fig = plt.figure(figsize = (11,11))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","3e355f4f":"ygt = test_.Target.values\nlabel = list(set(ygt))\ncnf_matrix = confusion_matrix(ygt, ytest, label)\n\n\nplot_confusion_matrix(cnf_matrix, label,\n                      normalize=False,\n                      title='Confusion matrix',\n                      cmap=plt.cm.Blues)","ffa3f85c":"# predictions and checking the performance \nstart = time.time()\ndtest = xgb.DMatrix(test_num[features_name].values)\n#yvalid = model.predict(dvalid)\nytest = model.predict(dtest)\nend = time.time()\nprint(\"Time taken in prediction is {}.\".format(end - start))","f82a2a72":"ytest = model.predict(dtest)\nprint('Test shape OK.') if test_num.shape[0] == ytest.shape[0] else print('Oops')\ntest_num['Target'] = ytest\ntest_num['Target'] = list(map(int, test_num['Target']))\ntest_num[['Id', 'Target']].to_csv('BuryBury_xgb_submission.csv', index=False)","46e03e34":"# Training XGBoost model ","ec5af04a":"# Reading the data ","539931ed":"# Prediction using XGB model ","85d034e7":"The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It\u2019s especially tricky when a program focuses on the poorest segment of the population. The world\u2019s poorest typically can\u2019t provide the necessary income and expense records to prove that they qualify\nIn Latin America, one popular method uses an algorithm to verify income qualification. It\u2019s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family\u2019s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n\nWhile this is an improvement, accuracy remains a problem as the region\u2019s population grows and poverty declines. And This challenge ask kagglers to develop a model to improve the accuracy so that this can be used in other countries as well.","128796e0":"# Upvote if you find it useful","a10bbfe6":"# Submission ","194288b5":"# Features importance ","513c610d":"# Beating the baseline"}}