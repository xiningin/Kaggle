{"cell_type":{"80930784":"code","5a58589a":"code","0d11090d":"code","cdc2e6c9":"code","70436f46":"code","5a8c56ee":"code","8f2a754f":"code","11609b93":"code","4c02f006":"code","8cab2601":"code","da6998ab":"code","8074ca9a":"code","e2ae8fdd":"code","d842e38b":"code","0aeab940":"code","c12ad50b":"code","6d4321ed":"markdown","4cf8d2d4":"markdown"},"source":{"80930784":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import MinMaxScaler","5a58589a":"torch.manual_seed(1)\n\nif torch.cuda.is_available():\n    device = 'cuda'\n    torch.cuda.manual_seed_all(1)\nelse:\n    device = 'cpu'","0d11090d":"train = pd.read_csv('..\/input\/nbaprediction\/train.csv')\ntest = pd.read_csv('..\/input\/nbaprediction\/test.csv')","cdc2e6c9":"train.shape, test.shape","70436f46":"train.head()","5a8c56ee":"# Null \uac12 \ud655\uc778\ntrain.isnull().sum()","8f2a754f":"# \ud574\ub2f9 \ud589 \uc81c\uac70\ntrain.dropna(inplace=True)","11609b93":"# \ud559\uc2b5\uc744 \uc704\ud55c \ub370\uc774\ud130 \uc900\ube44\nX_train = train.drop(['ID', 'Win'], axis=1)\nX_test = test.drop('ID', axis=1)\ny_train = train['Win']\n\n\n# Scaling\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n# Tensor\ub85c \ubcc0\ud658\nX_train = torch.FloatTensor(X_train).to(device)\nX_test = torch.FloatTensor(X_test).to(device)\n\ny_train = torch.LongTensor(y_train.values).to(device)","4c02f006":"# Hyperparameter \uc124\uc815\nlearning_rate = 0.1\nn_epochs = 500\ndrop_prob = 0.3\n\n\n# Model \uc124\uc815\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.fc1 = nn.Linear(X_train.shape[1], 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 256)\n        self.fc4 = nn.Linear(256, 256)\n        self.fc5 = nn.Linear(256, 256)\n        self.fc6 = nn.Linear(256, 2)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight.data)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc3(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc4(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc5(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc6(out)\n        return out","8cab2601":"model = Net().to(device)\n\n\n# optimizer\uc640 Loss Function \uc124\uc815\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\nloss_fn = nn.CrossEntropyLoss()\n\n\n# \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud574 Learning rate Scheduler \uc0ac\uc6a9\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.5)","da6998ab":"## \ud559\uc2b5 ##\n\nfor epoch in range(1, n_epochs+1):\n    model.train()\n    H = model(X_train)\n    loss = loss_fn(H, y_train)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    accuracy = (torch.argmax(H, dim=1) == y_train).float().mean()\n    \n    scheduler.step()\n    \n    if epoch % 20 == 0:\n        print('Epoch {:4d} \/ {}, Loss : {:.4f}, Accuracy : {:.2f} %'.format(\n            epoch, n_epochs, loss.item(), accuracy*100))","8074ca9a":"with torch.no_grad():\n    model.eval()\n    pred = model(X_test)","e2ae8fdd":"submit = pd.read_csv('..\/input\/nbaprediction\/sample_submit.csv')","d842e38b":"submit['Win'] = torch.argmax(pred, dim=1).cpu()","0aeab940":"submit.head()","c12ad50b":"submit.to_csv('submission.csv', index=False)","6d4321ed":"## Training","4cf8d2d4":"## Data Preparation"}}