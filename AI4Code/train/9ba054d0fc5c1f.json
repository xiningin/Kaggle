{"cell_type":{"f1deae2d":"code","2ea7d478":"code","c3fd2758":"code","50f0ebd0":"code","34d180b5":"code","85c4958f":"code","37943a94":"code","9ef9994d":"code","39dfed35":"code","828f0a31":"code","b141e413":"code","3fae8e01":"code","f69991f7":"code","7e842dd6":"code","715f03ad":"code","c5013d74":"code","db3aa948":"code","1a5cb974":"code","6a1ab472":"code","0c955d08":"code","941a79e5":"code","91c161a9":"code","0a4b75e6":"code","936c0355":"code","5e6447a4":"code","c2cddbba":"code","ba00f66e":"code","58f9ee68":"code","7cfbe4cd":"code","cfca305f":"code","2538c191":"code","b9672985":"code","9de4049b":"code","b8ebd7bc":"code","3dcbc382":"code","7ef5382e":"code","b6b1ebe1":"code","b01f083e":"code","1cd1fe3d":"code","e0a1fd48":"code","1332c00b":"code","bf71e43f":"code","50df83b5":"code","677ca847":"code","11ae5a23":"code","2f49e7b7":"code","f79c1011":"code","19ea4b46":"code","c29b0103":"code","0bc74063":"code","9351d204":"code","3a57a33f":"code","2b55c28b":"code","d479b40b":"code","b964fbbf":"code","fbfea714":"code","6bdc87be":"code","4c14159f":"code","3eac38c3":"code","f92fcf45":"code","4e4a94d9":"code","ea099f2f":"code","64bb1cea":"code","290dddde":"code","7781a7e5":"code","5fb21013":"code","449ca374":"code","c9b60bbc":"code","10688c5d":"markdown","6005f280":"markdown","9957d9c3":"markdown","ef0d204d":"markdown","b382db96":"markdown","51e8980a":"markdown","61df16d6":"markdown","547ee663":"markdown","206e5058":"markdown","488dbb97":"markdown","85de7e84":"markdown","951b894d":"markdown","f045a9c9":"markdown","db7a0e8e":"markdown","cfe03300":"markdown","6c256ff2":"markdown","29a9791a":"markdown","cff4a30a":"markdown","a9140e4f":"markdown","b3e68cd8":"markdown","f5e43938":"markdown","8432f75a":"markdown","5dc4ec63":"markdown","922210bd":"markdown","16e14a6e":"markdown","06474775":"markdown","8ffa0bc0":"markdown","b4e6b8c9":"markdown","5f768b5e":"markdown","2edbd079":"markdown","74b34efb":"markdown","b9361cfa":"markdown","72c737e3":"markdown","70d0d23b":"markdown"},"source":{"f1deae2d":"# DataFrame\nimport pandas as pd\n\n# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM, SpatialDropout1D\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport tensorflow as tf\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\n\n# Set log\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","2ea7d478":"nltk.download('stopwords')\n!pip install TurkishStemmer\nfrom TurkishStemmer import TurkishStemmer\nstemmer = TurkishStemmer()","c3fd2758":"TEST_SIZE = 0.1\n\n# TEXT CLENAING\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^a-zA-Z0-9\u011f\u00fc\u015f\u00f6\u00e7\u0131\u0130\u011e\u00dc\u015e\u00d6\u00c7]+\"\n\n# WORD2VEC \nW2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 16\nW2V_MIN_COUNT = 10\n\n# KERAS\nSEQUENCE_LENGTH = 300\nEPOCHS = 3\nBATCH_SIZE = 256\n\n# SENTIMENT\nPOSITIVE = \"POSITIVE\"\nNEGATIVE = \"NEGATIVE\"\nNEUTRAL = \"NEUTRAL\"\nSENTIMENT_THRESHOLDS = (0.4, 0.7)\n\n# EXPORT\nKERAS_MODEL = \"model.h5\"\nWORD2VEC_MODEL = \"model.w2v\"\nTOKENIZER_MODEL = \"tokenizer.pkl\"\nENCODER_MODEL = \"encoder.pkl\"","50f0ebd0":"df= pd.read_csv(\"..\/input\/yorumsepeti\/yorumsepeti.csv\", sep=';')","34d180b5":"print(\"Dataset size:\", len(df))","85c4958f":"df.head(5)","37943a94":"df['Character Count'] = df['review'].apply(lambda x: len(str(x)))\ndf['Character Count'].max(axis = 0, skipna = True)","9ef9994d":"pd.isna(df['review']).sum()","39dfed35":"df = df.dropna(subset=['review'], axis=0)","828f0a31":"# We will use the mean of 3 point scoring system\n# so we will change the dataset to target-text\n\ndf['speed'] = df['speed'].replace(['-'], np.nan)\ndf['service'] = df['service'].replace(['-'], np.nan)\ndf['flavour'] = df['flavour'].replace(['-'], np.nan)\n\ndf['speed'] = df['speed'].astype(float)\ndf['service'] = df['service'].astype(float)\ndf['flavour'] = df['flavour'].astype(float)\ndf['review'] = df['review'].astype(str)\n","b141e413":"df = df.assign(target=df.loc[:, ['speed', 'service', 'flavour']].mean(axis=1))\ndf['target'] = round(df['target'])\n\ndf = df.dropna(subset=['target'], axis=0)\ndf['target'] = df['target'].astype(int)\n\ndf","3fae8e01":"decode_map = { 1:\"NEGATIVE\", 2:\"NEGATIVE\" ,3: \"NEGATIVE\", 4:\"NEGATIVE\", 5:\"NEGATIVE\",6: \"POSITIVE\",\n              7: \"POSITIVE\", 8: \"POSITIVE\", 9: \"POSITIVE\", 10: \"POSITIVE\"}\ndef decode_sentiment(label):\n    return decode_map[int(label)]","f69991f7":"%%time\ndf.target = df.target.apply(lambda x: decode_sentiment(x))","7e842dd6":"df","715f03ad":"target_cnt = Counter(df.target)\n\nplt.figure(figsize=(8,4))\nplt.bar(target_cnt.keys(), target_cnt.values())\nplt.title(\"Dataset labels distribuition\")","c5013d74":"stop_words = stopwords.words(\"turkish\")","db3aa948":"def preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","1a5cb974":"%%time\ndf.review = df.review.apply(lambda x: preprocess(x))","6a1ab472":"df","0c955d08":"df_train, df_test = train_test_split(df, test_size=TEST_SIZE, random_state=42)\nprint(\"TRAIN size:\", len(df_train))\nprint(\"TEST size:\", len(df_test))","941a79e5":"%%time\ndocuments = [_text.split() for _text in df_train.review] ","91c161a9":"w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE, \n                                            window=W2V_WINDOW, \n                                            min_count=W2V_MIN_COUNT, \n                                            workers=8)","0a4b75e6":"w2v_model.build_vocab(documents)","936c0355":"vocab_len = len(w2v_model.wv)\nprint(\"Vocab size\", vocab_len)","5e6447a4":"%%time\nw2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)","c2cddbba":"w2v_model.wv.most_similar(\"tatl\u0131\")","ba00f66e":"all_sims = w2v_model.wv.most_similar(negative=\"tatl\u0131\")\nall_sims","58f9ee68":"%%time\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train.review)\n\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Total words\", vocab_size)","7cfbe4cd":"%%time\nx_train = pad_sequences(tokenizer.texts_to_sequences(df_train.review), maxlen=SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(df_test.review), maxlen=SEQUENCE_LENGTH)","cfca305f":"labels = df_train.target.unique().tolist()\nlabels","2538c191":"encoder = LabelEncoder()\nencoder.fit(df_train.target.tolist())\n\ny_train = encoder.transform(df_train.target.tolist())\ny_test = encoder.transform(df_test.target.tolist())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"x_train\", x_train.shape)\nprint(\"y_train\", y_train.shape)\nprint()\nprint(\"x_test\", x_test.shape)\nprint(\"y_test\", y_test.shape)","b9672985":"from imblearn.over_sampling import SMOTE\n\ndef smote(x, y):\n    smote = SMOTE(sampling_strategy= 1, random_state=42)\n    x, y = smote.fit_resample(x, y)\n    return x, y","9de4049b":"print(x_train.shape)\nprint(y_train.shape)","b8ebd7bc":"# CREATING DICTIONARY TO SEE CLASS DIST\ndf_c = pd.DataFrame(y_train)\nvalue_counts = df_c.value_counts()\ndictionary = dict()\nfor (i,), j in value_counts.items():\n    dictionary[i] = j\n\ndictionary","3dcbc382":"x_train, y_train = smote(x_train, y_train)\nprint(x_train.shape)\nprint(y_train.shape)","7ef5382e":"## CONTROL THE AUGMENTED DISTRIBUTION\ndf_c = pd.DataFrame(y_train)\nvalue_counts = df_c.value_counts()\ndictionary = dict()\nfor (i,), j in value_counts.items():\n    dictionary[i] = j\n\ndictionary","b6b1ebe1":"embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\nfor word, i in tokenizer.word_index.items():\n    if word in w2v_model.wv:\n        embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)","b01f083e":"embedding_layer = tf.keras.layers.Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], \n                                            input_length=SEQUENCE_LENGTH, trainable=False)\n\nsequence_input = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = tf.keras.layers.SpatialDropout1D(0.2)(embedding_sequences)\nx = tf.keras.layers.Conv1D(64, 3, activation='relu')(x)\nx = tf.keras.layers.Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)\n\nmodel.summary()","1cd1fe3d":"model.compile(loss='binary_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])","e0a1fd48":"callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]","1332c00b":"%%time\nhistory = model.fit(x_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=3,\n                    validation_split=0.1,\n                    callbacks=callbacks)","bf71e43f":"tf.keras.utils.plot_model(model, show_shapes=True)","50df83b5":"%%time\nscore = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\nprint()\nprint(\"ACCURACY:\",score[1])\nprint(\"LOSS:\",score[0])","677ca847":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","11ae5a23":"def decode_sentiment(score, include_neutral=False):\n    if include_neutral:        \n        label = NEUTRAL\n        if score <= SENTIMENT_THRESHOLDS[0]:\n            label = NEGATIVE\n        elif score >= SENTIMENT_THRESHOLDS[1]:\n            label = POSITIVE\n\n        return label\n    else:\n        return NEGATIVE if score < 0.5 else POSITIVE","2f49e7b7":"def predict(text, include_neutral=True):\n    start_at = time.time()\n    # Tokenize text\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n    # Predict\n    score = model.predict([x_test])[0]\n    # Decode sentiment\n    label = decode_sentiment(score, include_neutral=include_neutral)\n\n    return {\"label\": label, \"score\": float(score),\n       \"elapsed_time\": time.time()-start_at}  ","f79c1011":"predict(\"L\u00fctfen \u015fu yeme\u011fi d\u00fczg\u00fcn pi\u015firin\")","19ea4b46":"predict(\"\u0130\u015fini a\u015fkla yapan bir mekan daha sarrrrd\u0131\")","c29b0103":"predict(\"Kuryeniz \u00e7ok sayg\u0131s\u0131z\")","0bc74063":"predict(\"lezzet s\u0131f\u0131r kalite s\u0131f\u0131r hizmet s\u0131f\u0131r ekmek aras\u0131 ekmek de g\u00f6ndermezsiniz.\")","9351d204":"predict(\"EFFFFSANEYD\u0130 BEEE\")","3a57a33f":"predict(\"Ellerinize sa\u011fl\u0131k \u00e7ok g\u00fczeldi\")","2b55c28b":"predict(\"Bi tantuni yiyelim dedik kusacakt\u0131k reziller sizi\")","d479b40b":"predict(\"Hep buradan al\u0131yoruz yine memnunuz te\u015fekk\u00fcrler\")","b964fbbf":"predict(\"bir daha burdan almay\u0131n paran\u0131za yaz\u0131k\")","fbfea714":"predict(\"Bug\u00fcn g\u00fcnlerden a\u00e7l\u0131k g\u00fcn\u00fc zannetmi\u015ftim ama varya biliyosunuz bu i\u015fi.\")","6bdc87be":"%%time\ny_pred_1d = []\ny_test_1d = list(df_test.target)\nscores = model.predict(x_test, verbose=1, batch_size=8000)\ny_pred_1d = [decode_sentiment(score) for score in scores]","4c14159f":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=16)\n    plt.yticks(tick_marks, classes, fontsize=16)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=12)\n    plt.xlabel('Predicted label', fontsize=12)","3eac38c3":"%%time\n\ncnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix, classes=df_train.target.unique(), title=\"Confusion matrix\")\nplt.show()","f92fcf45":"print(classification_report(y_test_1d, y_pred_1d))","4e4a94d9":"accuracy_score(y_test_1d, y_pred_1d)","ea099f2f":"model.save(KERAS_MODEL)\nw2v_model.save(WORD2VEC_MODEL)\npickle.dump(tokenizer, open(TOKENIZER_MODEL, \"wb\"), protocol=0)\npickle.dump(encoder, open(ENCODER_MODEL, \"wb\"), protocol=0)","64bb1cea":"with open('..\/input\/models\/tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)\n\nwith open('..\/input\/models\/encoder.pkl', 'rb') as f:\n    encoder = pickle.load(f)\n\nw2v_model = gensim.models.word2vec.Word2Vec.load(\"..\/input\/models\/model.w2v\")\nmodel = tf.keras.models.load_model('..\/input\/model\/model.h5')","290dddde":"!pip install twint\nimport twint\nimport nest_asyncio\nnest_asyncio.apply()\n!pip install --user --upgrade git+https:\/\/github.com\/twintproject\/twint.git@origin\/master#egg=twint","7781a7e5":"c = twint.Config()\nc.Search = \"getirYemek\" #keyword for search\nc.Limit = 100 #limit of the number of tweets which will be extracted\nc.Store_csv = True \nc.Output = 'GetirYemek_tweet_data.csv'\ntwint.run.Search(c)","5fb21013":"crawled_data = pd.read_csv(\"GetirYemek_tweet_data.csv\")\n#crawled_data = pd.read_json(\"yemeksepeti_v1.json\", lines=True)\ncrawled_data.head()","449ca374":"pd.options.display.max_columns=36\ncrawled_data[\"tweet\"][0]","c9b60bbc":"# prediction of the first 10 extracted tweets\nfor i in range(10):\n    print(crawled_data[\"tweet\"][i])\n    print(predict(crawled_data[\"tweet\"][i], include_neutral = True))\n    print(\"\\n\")","10688c5d":"## Compile model","6005f280":"## We stored the related tweets in the .csv\/.json file which is really fast and cool\n\nSo how we will read from csv\/json file to use for our purpose ? ","9957d9c3":"# Evaluate","ef0d204d":"## Callbacks","b382db96":"## Read Dataset","51e8980a":"# Train","61df16d6":"# Pre-Process dataset","547ee663":"# Fetching data from twitter\nTo get started,\n\n* Import the twint package as follows.","206e5058":"# Save model","488dbb97":"## Settings\n\nDataset is big enough to take only %10 of the train set as test set so we will use our most of the data as our training set. Most of the parameters are chosen by experience and lots of tries. However, you can improve them as you wish.","85de7e84":"# Build Model\nWord2Vec-CNN-BiLSTM-FC learning pipeline, which consists of four sequential module.\nRecently, Word2Vec model still produces competitive results by using as embedding matrix of\ndeep learning models.<br \/><br \/>\nThe combination of CNN and BiLSTM models requires a particular design, since each model\nhas a specific architecture and its own strengths:<br \/>\n\u2022 BERT is utilized to transform word tokens from the raw Tweet messages to contextual word\nembeddings.<br \/>\n\u2022 CNN is known for its ability to extract as many features as possible from the text.<br \/>\n\u2022 BiLSTM keeps the chronological order between words in a document, thus it has the ability\nto ignore unnecessary words using the delete gate.<br \/>\n\u2022 Fully Connected Layers give robustness to decrease unsteadiness of results in hard cases.<br \/>\nThe purpose of combining these two models is to create a model that takes advantage of the\nstrengths of CNN and BiLSTM, so that it captures the features extracted using CNN, and uses\nthem as an LSTM input. Therefore, I develop a model that meets this objective, such that the\nvectors built in the word embedding part are used as convolutional neural network input.","951b894d":"## Embedding layer","f045a9c9":"### Predict","db7a0e8e":"____________________________________","cfe03300":"## Classification Report","6c256ff2":"### If you want to know more about twint, you can checkout this Github link:\nhttps:\/\/github.com\/twintproject\/twint","29a9791a":"### Recap: We have mean scores in target column. We map it as, 1-5 points correspond to negative, 6-10 points correspond to positive. We will use 0.4-0.7 as thresholds for neutral addition in the last section.","cff4a30a":"_____\n# After you've prepared your model, do not need to train everytime.","a9140e4f":"## Confusion Matrix","b3e68cd8":"_____\nAs you can see above we have lots of features which extracted by twint. However, we only need the \"tweet\" feature which includes the text data of tweets for our purpose.","f5e43938":"## Word2Vec ","8432f75a":"# Results and Observations\nI also conducted experiments to evaluate a set of models, and present a performance comparison\nof all evaluated models in table 1. The set of models CNN, BiLSTM, CNN-BiLSTM, and CNNBiLSTM-\nDense forms an ablation study, from which we can evaluate the performance of each\nindividual module and the combined versions. It can be seen that the pure CNN model performs\nthe worst since a single-layer CNN cannot learn any contextual information. Therefore, could\nnot obtain any results. BiLSTM present an obvious improvement. My final model, BiLSTM-CNNDense\ntops every other model, showing its power to combine the strength of each individual building\nblock. Another observation is that even though the its accuracy is high, I see pure BiLSTM model\ndoes not perform robust results in terms of hard cases.<br\/>\n\n                                            Table 1: A performance comparison of models.\n\n| Model | Precision | Recall | Accuracy |\n| --- | --- | --- | --- |\n| BiLSTM | 90.01 | 86.32 | 86.41 |\n| CNN-BiLSTM | 90.23 | 85.74 | 85.03 |\n| CNN-BiLSTM-Dense | 91.06 | 86.64 | 86.23 |\n","5dc4ec63":"## Creating a target column to be able to make binary classification depending on mean of 3 types of scoring","922210bd":"SMOTE [1](https:\/\/arxiv.org\/abs\/1106.1813) (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem. SMOTE synthesises new minority instances\nbetween existing minority instances. It generates the virtual training records by linear interpolation\nfor the minority class. These synthetic training records are generated by randomly selecting\none or more of the k-nearest neighbors for each example in the minority class. After the oversampling\nprocess, the data is reconstructed and several classification models can be applied for the\nprocessed data. We had 1: 46502, 0: 7683 distribution in training set. After SMOTE it becomes\nto 0: 46502, 1: 46502.","16e14a6e":"# Introduction\nMobile Apps has been increasingly popular for people to share instant feelings, emotions, opinions,\nstories, and so on. As a leading food delivery platform, Yemeksepeti has gained tremendous popularity\nsince its inception. People comment on the comments section of the restaurants which they\norder meal to spread goodness or badness of the restaurant. Therefore, enough data is generated\nfor sentiment analysis to give an intuition to people who are seeking delicious food. The comments\nare not clean. That means we will start with the data cleaning, and continue with the changing\ndataset in order to make it useful for our purpose. Word2Vec, CNN and Bidirectional LSTM will\nbe used and explained later.","06474775":"## Split train and test","8ffa0bc0":"As explained before dataset includes 3 features labelled as speed, service and flavour out of 10\npoints. For sentiment analysis, it is difficult to reach state-of-art results by using 10 classes for\nevery point due to lack of size of dataset. Therefore, as pre-processing step, the \u2019-\u2019 character is\ndeleted from the dataset and inserted NaN instead. This is done because, for every row, we will\ntake the mean of 3 features and create new column as target to hold rounded mean scores(int).\nMeanwhile, dataset has NaN values for reviews which mean that we cannot use those rows for any\nsentiment analysis. Therefore, we will drop NULL review rows. In raw dataset, 33 rows has no\nreviews, and the latest dataset size is 60206 rows.","b4e6b8c9":"## Accuracy Score","5f768b5e":"Choosing language dependent stemmer is important before cleaning the text.","2edbd079":"## Tokenize Text","74b34efb":"_________\n### In next step, we will use our prepared model to predict yemeksepeti comments on twitter using Twint.","b9361cfa":"#### *I hope this notebook will give you an intuition about data handling by purpose and creating different type of training models* \n\n* I tried to keep it as simple as possible. Have a nice ml day.\n> If you find my work useful please don't forget to *Upvote!* so it can reach more people.","72c737e3":"### Dataset details\nThe dataset was created by Do\u011fukan Arslan from Turkish restaurant reviews taken from the\nfood ordering site [yemeksepeti.com](https:\/\/https:\/\/www.yemeksepeti.com). The dataset also includes scores in speed, service and flavour categories. In this respect, it is expected to be used in aspect-based sentiment analysis studies. Therefore, for sentiment extraction, dataset named [Yorumsepeti](https:\/\/www.kaggle.com\/dgknrsln\/yorumsepeti) choice is reasonable and challenging.\n* **speed**: speed point\n* **service**: service point\n* **flavour**: flavour point\n* **review**: the text of the comments (Superdi.)\n* **target**: the polarity of the tweet (1-5 = negative, 6-10 = positive) -> custom feature, we created target feature to be able to make binary classification | negative will be added as threshold |\n","70d0d23b":"# Imbalanced distribution handling"}}