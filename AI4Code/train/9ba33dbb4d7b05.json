{"cell_type":{"1f28a9db":"code","b1126a8c":"code","d2dd3b81":"code","1ea67b05":"code","18a57193":"code","92262bcd":"code","d65a6f0a":"code","bfe8d3cf":"code","1125a763":"code","687bd4f6":"code","0a9a7f03":"code","5eeb71a6":"code","b396faac":"code","74074ca1":"code","4128d00b":"code","fcda7301":"code","25e9e6b0":"code","4d0362f0":"code","ed97f26d":"code","a6652d89":"code","94602574":"code","6eadc690":"code","e1aac307":"code","55cf6da7":"code","e4ac2b05":"code","9fb7637d":"code","21818039":"code","d639f648":"code","2b6f2840":"code","34c99673":"code","b7ac44c3":"code","73d23aa7":"code","2808b390":"code","adfd3763":"code","a53274cf":"code","0e330275":"code","abb1a11c":"code","a2d61a02":"code","30cbf01a":"code","d919cb6b":"code","2eb1a769":"code","45fb89e4":"markdown","9d3da8b8":"markdown","f5ab5df3":"markdown","42db34fe":"markdown","149678f1":"markdown","2c60140d":"markdown"},"source":{"1f28a9db":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nnp.random.seed(42)","b1126a8c":"!ls ..\/input\/arabic-poetry-dataset-478-2017\n\n","d2dd3b81":"data = pd.read_csv('..\/input\/arabic-poetry-dataset-478-2017\/all_poems.csv')","1ea67b05":"data.info()","18a57193":"data.head()","92262bcd":"data['poet_name'].value_counts()","d65a6f0a":"import seaborn as sns","bfe8d3cf":"sns.countplot(x='poet_name', data=data)\n","1125a763":"data[data['poet_name'] == '\u0646\u0632\u0627\u0631 \u0642\u0628\u0627\u0646\u064a']","687bd4f6":"data[data['poet_name']== '\u0623\u0628\u0648 \u0646\u0648\u0627\u0633']","0a9a7f03":"poems_count = 200","5eeb71a6":"ghazal = data[(data['poet_name'] == '\u0646\u0632\u0627\u0631 \u0642\u0628\u0627\u0646\u064a') | (data['poet_name']== '\u0623\u0628\u0648 \u0646\u0648\u0627\u0633')].sample(frac=1)[:poems_count]","b396faac":"ghazal.head()","74074ca1":"only_poems = np.array(ghazal['poem_text'].values.tolist())","4128d00b":"only_poems.shape","fcda7301":"all(isinstance(item, str) for item in only_poems)","25e9e6b0":"poems_list = np.array([item if isinstance(item,str) else ' ' for item in only_poems[:poems_count]])","4d0362f0":"del only_poems","ed97f26d":"# all_poems_to_text = ' '.join(poems_list)","a6652d89":"import re\nclean_poems = []\nfor p in poems_list:\n    clean_poems.append(re.sub('([@A-Za-z0-9_\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640]+)|[^\\w\\s]|#|http\\S+', ' ', p))\n","94602574":"len(clean_poems)","6eadc690":"# all_words = clean_poems.split(' ')","e1aac307":"import tensorflow as tf\ntf.random.set_seed(42)\n\n","55cf6da7":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils as ku ","e4ac2b05":"tokenizer = Tokenizer()\n\ncorpus = clean_poems\n\n\ntokenizer.fit_on_texts(corpus)\ntotal_words = len(tokenizer.word_index) + 1\n","9fb7637d":"print(total_words)","21818039":"\ninput_sequences = []\nfor line in corpus:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n\n\n","d639f648":"# pad sequences \npad_seq = [len(x) for x in input_sequences]\nmax_sequence_len = max(pad_seq)\nprint(max_sequence_len)\n\n","2b6f2840":"input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n\n# create predictors and label\npredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n\nlabel = ku.to_categorical(label, num_classes=total_words)","34c99673":"# print(tokenizer.word_index['\u062e\u0628\u0632'])\nprint(tokenizer.word_index)\n","b7ac44c3":"model = Sequential()\nmodel.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","73d23aa7":"history = model.fit(predictors, label, epochs=80, verbose=1, batch_size=128, shuffle=True)\n","2808b390":"model.summary()","adfd3763":"model.save('ghazal_train.h5')","a53274cf":"from IPython.display import FileLink\nFileLink(r'ghazal_train.h5')","0e330275":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.show()","abb1a11c":"plot_graphs(history, 'accuracy')\n","a2d61a02":"seed_text = \"\u064a\u0635\u064a\u0631 \u0627\u0644\u0639\u064a\u0646\u0627\u0646\"\nnext_words = 200\ntoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')","30cbf01a":"token_list = tokenizer.texts_to_sequences([seed_text])\n    \ntoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\npredicted = model.predict(token_list)\nprint(np.argmax(predicted))","d919cb6b":"words_list = tokenizer.word_index.items()","2eb1a769":"for k in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = np.argmax(model.predict(token_list), axis=-1)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","45fb89e4":"### Saving the Model","9d3da8b8":"## Evaluation","f5ab5df3":"### Training","42db34fe":"## Modeling","149678f1":"## Generating Poems","2c60140d":"We will create a input sequences using the list of tokens we have."}}