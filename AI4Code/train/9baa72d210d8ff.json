{"cell_type":{"55bdfa9d":"code","24fbbf41":"code","0ee2185c":"code","59d5b3b9":"code","e9a7307c":"code","d9653547":"code","529209bd":"code","c745dffb":"code","da34d107":"code","f32c9aca":"code","34db2b79":"code","a08f86cc":"code","f4d35052":"code","9f81ca34":"code","1a4a3fde":"code","e1065377":"code","f611f05f":"code","352ccfc5":"code","3c1a4237":"code","a18479a6":"code","aec0d141":"code","40a038b8":"code","744f1bfd":"code","8b1ef297":"code","de7e4765":"code","b4fc7ef5":"code","c88c42da":"code","63080f97":"code","a26883e7":"code","84045aeb":"code","10eb8b90":"code","b2e73f26":"code","33670aba":"code","c3756763":"code","c02b2cb5":"code","55da20e6":"code","9f515f91":"code","a6998340":"code","c0b6d721":"code","13d3c032":"code","968fffc7":"code","707a3ec4":"markdown","c5261745":"markdown","5a5f8529":"markdown","571ca5e8":"markdown","05d2d76c":"markdown","fec310d4":"markdown","29b04fa5":"markdown","1c284078":"markdown","07c03e25":"markdown","eb17cab1":"markdown","6214ce6d":"markdown","756b2178":"markdown","1b458fb7":"markdown","3b7fb054":"markdown","ceb268e4":"markdown","fb0ca79d":"markdown","c36523ba":"markdown","9172022d":"markdown","4e09d494":"markdown","b93b6523":"markdown","e28d0a65":"markdown"},"source":{"55bdfa9d":"### General libraries ###\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n##################################\n\n### ML Models ###\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree\nfrom sklearn.tree.export import export_text\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\n##################################\n\n### Metrics ###\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score","24fbbf41":"# Read the data from the auto-mpg_data-original.csv file.\nds=pd.read_csv(\"..\/input\/auto-mpg_data-original.csv\")","0ee2185c":"# Examine the data types and the number of non-null items.\nds.info()","59d5b3b9":"# Display the shape of the data.\nprint(\"The data frame has {} rows and {} columns.\".format(ds.shape[0],ds.shape[1]))","e9a7307c":"# Statistics for the data set.\nds.describe().transpose()","d9653547":"# Check for duplicate rows.\nprint(f\"There are {ds.duplicated().sum()} duplicate rows in the data set.\")","529209bd":"# Number of null values for each attribute.\nds.isnull().sum()","c745dffb":"# Rows with null value at the \"mpg\" column.\nds[ds['mpg'].isnull()]","da34d107":"# Store rows with a missing value at \"mpg\" attribute as a prediction set for future use.\nprediction_set=ds[ds['mpg'].isnull()].drop('mpg',axis=1)","f32c9aca":"# Rows with null value at the \"horsepower\" column.\nds[ds['horsepower'].isnull()]","34db2b79":"# Fill the null values of \"horsepower\" with the mean of \"horsepower\".\nds['horsepower']=ds['horsepower'].fillna(ds['horsepower'].mean())","a08f86cc":"# Remove rows from the data set with a null values.\nnew_ds=ds.dropna()","f4d35052":"# Information about the new \"clean\" data set.\nnew_ds.info()\n\n# Display the shape of the data.\nprint(\"\\nThe new data frame has {} rows and {} columns.\".format(new_ds.shape[0],new_ds.shape[1]))","9f81ca34":"# Statistics for the new data set.\nnew_ds.describe().transpose()","1a4a3fde":"# Plotting a pairplot for the new data set.\nplt.figure(figsize=(20,20))\nsns.set(font_scale=1.1)\nsns.pairplot(data=new_ds, diag_kind='kde', hue='origin')\nplt.show();","e1065377":"# Plotting a correlation heatmap for the new data set.\ncorr = new_ds.corr()\nplt.figure(figsize=(8,8))\nsns.set(font_scale=1.1)\nsns.heatmap(data=corr,annot=True,cmap='rainbow',linewidth=0.5)\nplt.title('Correlation Matrix')\nplt.show();","f611f05f":"# Distinguish attribute columns and target column.\nX=new_ds[new_ds.columns[1:-1]]\ny=new_ds['mpg']","352ccfc5":"# Split to train and test sets. \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)","3c1a4237":"# Standardization\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","a18479a6":"# Initialize a Logistic Regression estimator.\nlinreg=LinearRegression(n_jobs=-1)\n\n# Train the estimator.\nlinreg.fit(X_train,y_train)","aec0d141":"# Make predictions.\nlin_pred=linreg.predict(X_test)\n\n# Calculate CV score.\ncv_lin_reg=cross_val_score(linreg, X_train, y_train, cv=10).mean()","40a038b8":"# Mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.2f\" % mean_squared_error(y_test, lin_pred))\n\n# Mean absolute error (average error).\nprint(\"Mean absolute error (MAE): %.2f\" % mean_absolute_error(y_test, lin_pred))\n\n# Cross-Validation accuracy\nprint('Cross-validation accuracy: %0.1f' % (cv_lin_reg*100),'%')\n\n# Accuracy score: 1 is perfect prediction.\nprint('Accuracy: %.1f' % (linreg.score(X_test, y_test)*100),'%')","744f1bfd":"# Hyperparameters to be checked.\nparameters = {'normalize':[True, False],\n              'fit_intercept':[True, False]\n             }\n\n# Linear Regression estimator.\ndefault_linreg=LinearRegression(n_jobs=-1)\n\n# GridSearchCV estimator.\ngs_linreg = GridSearchCV(default_linreg, parameters, cv=10, n_jobs=-1, verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_linreg.fit(X_train,y_train)","8b1ef297":"# Make predictions with the best parameters.\ngs_linreg_pred=gs_linreg.predict(X_test)","de7e4765":"# Best parameters.\nprint(\"Best Linear Regression Parameters: {}\".format(gs_linreg.best_params_))\n\n# Mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.2f\" % mean_squared_error(y_test, gs_linreg_pred))\n\n# Mean absolute error (average error).\nprint(\"Mean absolute error (MAE): %.2f\" % mean_absolute_error(y_test, gs_linreg_pred))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.1f' % (gs_linreg.best_score_*100),'%')\n\n# Accuracy score: 1 is perfect prediction.\nprint('Accuracy: %.1f' % (gs_linreg.score(X_test, y_test)*100),'%')","b4fc7ef5":"# Initialize a decision tree estimator.\ntr = tree.DecisionTreeRegressor(max_depth=3, random_state=25)\n\n# Train the estimator.\ntr.fit(X_train, y_train)","c88c42da":"# Plot the tree.\nfig=plt.figure(figsize=(23,15))\ntree.plot_tree(tr.fit(X_train, y_train),feature_names=X.columns,filled=True,rounded=True,fontsize=16);\nplt.title('Decision Tree');","63080f97":"# Print the tree in a simplified version.\nr = export_text(tr, feature_names=X.columns.tolist())\nprint(r)","a26883e7":"# Make predictions.\ntr_pred=tr.predict(X_test)\n\n# Calculate CV score.\ncv_tr_reg=cross_val_score(tr, X_train, y_train, cv=10).mean()","84045aeb":"# Mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.2f\" % mean_squared_error(y_test, tr_pred))\n\n# Mean absolute error (average error).\nprint(\"Mean absolute error (MAE): %.2f\" % mean_absolute_error(y_test, tr_pred))\n\n# Cross-Validation accuracy.\nprint('Cross-validation accuracy: %0.1f' % (cv_tr_reg*100),'%')\n\n# Accuracy score: 1 is perfect prediction.\nprint('Accuracy: %.1f' % (tr.score(X_test, y_test)*100),'%')","10eb8b90":"# Hyperparameters to be checked.\nparameters = {'criterion':['mse','friedman_mse','mae'],\n              'splitter':['best','random'],\n              'max_depth':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n              'min_samples_leaf':[2,3,5,10,20]\n             }\n\n# MLP estimator.\ndefault_tr = tree.DecisionTreeRegressor(random_state=25)\n\n# GridSearchCV estimator.\ngs_tree = GridSearchCV(default_tr, parameters, cv=10, n_jobs=-1,verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_tree.fit(X_train,y_train)","b2e73f26":"# Make predictions with the best parameters.\ngs_tree_pred=gs_tree.predict(X_test)","33670aba":"# Best parameters.\nprint(\"Best Decision tree Parameters: {}\".format(gs_tree.best_params_))\n\n# Mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.2f\" % mean_squared_error(y_test, gs_tree_pred))\n\n# Mean absolute error (average error).\nprint(\"Mean absolute error (MAE): %.2f\" % mean_absolute_error(y_test, gs_tree_pred))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.1f' % (gs_tree.best_score_*100),'%')\n\n# Accuracy score: 1 is perfect prediction.\nprint('Accuracy: %.1f' % (gs_tree.score(X_test, y_test)*100),'%')","c3756763":"# Initialize a Multi-layer Perceptron classifier.\nmlp = MLPRegressor(max_iter=1000, random_state=25,shuffle=True, verbose=False)\n\n# Train the classifier.\nmlp.fit(X_train, y_train)","c02b2cb5":"# Make predictions.\nmlp_pred = mlp.predict(X_test)\n\n# Calculate CV score.\ncv_mlp_reg=cross_val_score(mlp, X_train, y_train, cv=10).mean()","55da20e6":"# Mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.2f\" % mean_squared_error(y_test, mlp_pred))\n\n# Mean absolute error (average error).\nprint(\"Mean absolute error (MAE): %.2f\" % mean_absolute_error(y_test, mlp_pred))\n\n# Cross-Validation accuracy\nprint('Cross-validation accuracy: %0.1f' % (cv_mlp_reg*100),'%')\n\n# Accuracy score: 1 is perfect prediction.\nprint('Accuracy: %.1f' % (mlp.score(X_test, y_test)*100),'%')","9f515f91":"# Hyperparameters to be checked.\nparameters = {'activation':['logistic','tanh','relu'],\n              'solver': ['lbfgs','adam','sgd'],\n              'alpha': 10.0 ** -np.arange(1,3),\n              'learning_rate': ['constant', 'invscaling', 'adaptive'],\n              'hidden_layer_sizes':[(7),(6),(14),(3),(7,3),(6,3),(14,7),(3,1)]}\n\n# Decision tree estimator.\ndefault_mlp = MLPRegressor(max_iter=1000, random_state=25,shuffle=True, verbose=False)\n\n# GridSearchCV estimator.\ngs_mlp = GridSearchCV(default_mlp, parameters, cv=10, n_jobs=-1,verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_mlp.fit(X_train,y_train)","a6998340":"# Make predictions with the best parameters.\ngs_mlp_pred=gs_mlp.predict(X_test)","c0b6d721":"# Best parameters.\nprint(\"Best MLP Regression Parameters: {}\".format(gs_mlp.best_params_))\n\n# Mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.2f\" % mean_squared_error(y_test, gs_mlp_pred))\n\n# Mean absolute error (average error).\nprint(\"Mean absolute error (MAE): %.2f\" % mean_absolute_error(y_test, gs_mlp_pred))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.1f' % (gs_mlp.best_score_*100),'%')\n\n# Accuracy score: 1 is perfect prediction.\nprint('Accuracy: %.1f' % (gs_mlp.score(X_test, y_test)*100),'%')","13d3c032":"metrics=['MSE','MAE','CV accuracy','Accuracy']\n\n# Plot metrics.\nfig = go.Figure(data=[\n    go.Bar(name='Linear Regression', x=metrics, y=[mean_squared_error(y_test, lin_pred),mean_absolute_error(y_test, lin_pred),cv_lin_reg,linreg.score(X_test, y_test)]),\n    go.Bar(name='Decision tree', x=metrics, y=[mean_squared_error(y_test, tr_pred),mean_absolute_error(y_test, tr_pred),cv_tr_reg,tr.score(X_test, y_test)]),\n    go.Bar(name='Neural Network', x=metrics, y=[mean_squared_error(y_test, mlp_pred),mean_absolute_error(y_test, mlp_pred),cv_mlp_reg,mlp.score(X_test, y_test)]),\n    go.Bar(name='GridSearch+Linear Regression', x=metrics, y=[mean_squared_error(y_test, gs_linreg_pred),mean_absolute_error(y_test, gs_linreg_pred),gs_linreg.best_score_,gs_linreg.score(X_test, y_test)]),\n    go.Bar(name='GridSearch+Decision tree', x=metrics, y=[mean_squared_error(y_test, gs_tree_pred),mean_absolute_error(y_test, gs_tree_pred),gs_tree.best_score_,gs_tree.score(X_test, y_test)]),\n    go.Bar(name='GridSearch+Neural Network', x=metrics, y=[mean_squared_error(y_test, gs_mlp_pred),mean_absolute_error(y_test, gs_mlp_pred),gs_mlp.best_score_,gs_mlp.score(X_test, y_test)])\n])\n\nfig.update_layout(title_text='Results',\n                  barmode='group',xaxis_tickangle=-45,bargroupgap=0.05)\nfig.show()","968fffc7":"d={\n'': ['Linear Regression','GridSearchCV + Linear Regression','Decision Tree','GridSearchCV + Decision Tree','Neural Network (MLP)','GridSearchCV + Neural Network (MLP)'],\n    'MSE': [mean_squared_error(y_test, lin_pred), mean_squared_error(y_test, gs_linreg_pred),mean_squared_error(y_test, tr_pred),mean_squared_error(y_test, gs_tree_pred),mean_squared_error(y_test, mlp_pred),mean_squared_error(y_test, gs_mlp_pred)],\n    'MAE': [mean_absolute_error(y_test, lin_pred), mean_absolute_error(y_test, gs_linreg_pred),mean_absolute_error(y_test, tr_pred),mean_absolute_error(y_test, gs_tree_pred),mean_absolute_error(y_test, mlp_pred),mean_absolute_error(y_test, gs_mlp_pred)],\n    'CV Accuracy': [cv_lin_reg, gs_linreg.best_score_, cv_tr_reg,gs_tree.best_score_,cv_mlp_reg,gs_mlp.best_score_],\n    'Accuracy': [linreg.score(X_test, y_test), gs_linreg.score(X_test,y_test),tr.score(X_test, y_test),gs_tree.score(X_test,y_test),mlp.score(X_test, y_test),gs_mlp.score(X_test, y_test)]\n}\n\nresults=pd.DataFrame(data=d).round(3).set_index('')\nresults","707a3ec4":"The rows with a null value from the `mpg` attribute can be seen below:","c5261745":"## Decision tree Metrics","5a5f8529":"## Grid search Metrics for Decision tree","571ca5e8":"The rows with null values in the `horsepower` column can be seen below:","05d2d76c":"## Grid search for Linear Regression","fec310d4":"## Grid search for Decision tree","29b04fa5":"## Decision tree","1c284078":"## Grid search MLP metrics","07c03e25":"## Grid search for Neural network (MLP)","eb17cab1":"The above rows provide no usefull information because the class attribute value is missing. Hence, we cannot use them as a training nor a test set for our model, so they will be removed them from the dataset. For now, we store them for a potential future use.","6214ce6d":"## Part 1: Load and clean the data","756b2178":"## Part 2: Preproccesing\nIn this part we prepare the data for our models. This means that we choose the columns that will be our independed variables and which column the target that we want to predict. Once we are done with that, we split our data into train and test sets and perfom a standardization upon them.","1b458fb7":"## Part 3: Modeling\nIn this section we build and try 3 models:\n- Linear Regression\n- Decision tree\n- Neural network\n\nEach model will be trained and make a prediction for the test set. Accuracy, precision, recall, f1-score, confusion matrix and ROC will be calculated for each model. Then we will use the GridSearchCV module to tune our models and search for the best hyperparameters in order to increase the accuracy of each model.","3b7fb054":"## Linear Regression Metrics","ceb268e4":"## Neural network (MLP) metrics","fb0ca79d":"## Neural network (MLP)","c36523ba":"## Import libraries\/packages","9172022d":"## Grid search Metrics for Linear Regression","4e09d494":" ## Introduction\nThis notebook was created for analysis and prediction making of the Auto MPG data set from UCI Machine Learning Library. The data set can be accessed separately from the UCI Machine Learning Repository page, [here](http:\/\/archive.ics.uci.edu\/ml\/datasets\/Auto+MPG).\n\n## Description\nThe following description can be found on the UCI Repository page for the data set.\n\n- \"This dataset is a slightly modified version of the dataset provided in the StatLib library. In line with the use by Ross Quinlan (1993) in predicting the attribute \"mpg\", 8 of the original instances were removed because they had unknown values for the \"mpg\" attribute. The original data set is available in the file \"auto-mpg.data-original\"\".\n\n- \"The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\" (Quinlan, 1993)\n\n\n## Relevant Papers\n\nIn his paper \"Combining Instance-Based and Model-Based Learning\", (Quinlan,R., 1993), which can be found [here](http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.34.6358&rep=rep1&type=pdf), Quinlan tries to predict the city cycle consumption in miles per galon, using three methods: Linear Regression, Model trees and Neural Networks.\n\n## Models\n\nWe will create 3 models in order to make predictions and compare them with the original paper. These models are:\n- Linear Regression\n- Decision tree\n- Neural Network\n\nAfter the initial predictions, each model will be \"optimized\" by `GridSearchCV` estimator, which will search for the best set of hyperparameters for every model.  \n\n## Goal\n\nUsing the models above, we will try to predict the class value of `y` column with better scores, than the scores presented in the original paper. For our model, we will use the original full data set (**auto-mpg_data-original.csv**).","b93b6523":"## Results","e28d0a65":"## Linear Regression"}}