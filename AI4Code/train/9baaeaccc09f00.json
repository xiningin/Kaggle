{"cell_type":{"2164d9ac":"code","139adbf8":"code","670c8e08":"code","13b72a05":"code","d7a6c82a":"code","9fa26cf3":"code","c12a56f7":"code","d7e05f89":"code","29f05a67":"code","a1c188c2":"code","fc959814":"code","8ff9610c":"code","3fed6eeb":"markdown","b8829662":"markdown","83954654":"markdown","00c2348d":"markdown","5e6058b6":"markdown"},"source":{"2164d9ac":"import pandas as pd\nimport numpy as np\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\niris = datasets.load_iris()\nX = pd.DataFrame(iris.data) #Dataframe: 150 rows x 4 columns\ny = iris.target #numpy.ndarray. class= 0,1,2","139adbf8":"# logtistic regression \n#ref for one-vs-rest logtistic regression: https:\/\/chrisalbon.com\/machine_learning\/logistic_regression\/one-vs-rest_logistic_regression\/\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score\n\ncv=5\nmodel=LogisticRegression(solver='newton-cg',multi_class='ovr',penalty='l2')\n\ngrid_search = GridSearchCV(model, param_grid={},cv=cv,scoring='accuracy') \ngrid_search.fit(X, y) \nprint('Accuracy= ',grid_search.best_score_)","670c8e08":"#Decision tree\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import precision_score\nimport matplotlib.pyplot as plt\n\ncv=5\nmodel=DecisionTreeClassifier()\n\ngrid_search = GridSearchCV(model, param_grid={},cv=cv,scoring='accuracy') \ngrid_search.fit(X, y) \nprint('Accuracy= ',grid_search.best_score_)\n\nfig=plot_tree(grid_search.best_estimator_)\n\n\nfrom matplotlib.pylab import rcParams\n##set up the parameters\nrcParams['figure.figsize'] = 10,10\nplt.show","13b72a05":"#function for models that need hyper-parameter tuning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef my_inner_cv(X,y,model,cv,param_grid,test_size,random_state):\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=random_state,stratify=y)\n\n  grid_search = GridSearchCV(model, param_grid=param_grid,cv=cv,iid=False) \n  grid_search.fit(X_train, y_train)\n\n  accuracy=accuracy_score(y_test,grid_search.best_estimator_.predict(X_test))\n  \n  return([grid_search.best_estimator_,grid_search.best_params_,accuracy])","d7a6c82a":"#Random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_state=101\ntest_size=0.2\nmax_depth = list(range(1,11))\ncriterion=['entropy','gini']\nn_estimators=list(range(1,10)) #this can possibly be set to a much higher value but it would take lots of time\nparam_grid={'criterion':criterion,'max_depth':max_depth,'n_estimators':n_estimators}\ncv=5\n\nmodel=RandomForestClassifier()\nresult=my_inner_cv(X,y,model,cv,param_grid,test_size,random_state)\n\nprint('Accuracy= ',result[2])","9fa26cf3":"#Boosting (gradient boosting)\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nrandom_state=101\ntest_size=0.2\nn_estimators = [5,10,15,20,50,100,200,400]\nmax_iter=1000\n\nparam_grid={'n_estimators':n_estimators}\ncv=5\nmodel=GradientBoostingClassifier()\n\nresult=my_inner_cv(X,y,model,cv,param_grid,test_size,random_state)\nprint('Accuracy= ',result[2])","c12a56f7":"#SVM\nfrom sklearn.svm import SVC\n\n#==============\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) #avoid the \"future warning\" that I think is not important\"\n#==============\n\nrandom_state=101\ntest_size=0.2\nds = [1,2,3,4]\nCs = [0.001, 0.01, 0.1, 1, 10,100,1000] #ref: https:\/\/medium.com\/@aneesha\/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a0\nkernels=['linear', 'poly', 'rbf', 'sigmoid'] # I removed 'precomputed' kernel because it only accepts data that looke like: (n_samples, n_samples). Ref: https:\/\/stackoverflow.com\/questions\/36306555\/scikit-learn-grid-search-with-svm-regression\/36309526\nparam_grid={'degree':ds,'C':Cs, 'kernel':kernels}\ncv=5\nmodel=SVC()\n\nresult=my_inner_cv(X,y,model,cv,param_grid,test_size,random_state)\nprint('Accuracy= ',result[2])","d7e05f89":"#Neuro net \nfrom keras.layers import Dense \nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\ntest_size = 0.2\nrandom_state=101\nepochs=100\noptimizer='sgd'\nloss='categorical_crossentropy'\nmetrics=['accuracy']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=random_state,stratify=y) #not sure if stratify=y works when there are more than 2 classes for the labels\n\n#The no. of layers and no. of nodes for each layer can be freely customized\nmodel = Sequential()\nmodel.add(Dense(10,activation='relu',input_dim=X.shape[1])) #X.shape[1] should equal the number of features\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(len(np.unique(y_train)),activation='softmax')) #the number of the output layer should equal the number of unique outcomes (response variables)\n        \nmodel.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n\nmodel.fit(X_train, to_categorical(y_train),epochs=epochs,verbose=0)\n\ny_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred,axis=1)\n\naccuracy=accuracy_score(y_test,y_pred)\nprint('Accuracy= ',accuracy)","29f05a67":"#Hierarchical clustering\nimport scipy.cluster.hierarchy as shc\n\nplt.figure(figsize=(20, 10))\nplt.title(\"Iris Dendograms\")\ndend = shc.dendrogram(shc.linkage(X, method='ward'),labels=y)","a1c188c2":"#PCA\n##ref: https:\/\/towardsdatascience.com\/pca-using-python-scikit-learn-e653f8989e60\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as mplcm\nimport matplotlib.colors as colors\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents)\n\nfinalDf = pd.concat([principalDf, pd.DataFrame(y)], axis = 1)\nfinalDf.columns = ['principal component 1', 'principal component 2','target']\n\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\n\nNUM_COLORS = len(np.unique(np.array(y)))\ncm = plt.get_cmap('gist_rainbow')\ncNorm  = colors.Normalize(vmin=0, vmax=NUM_COLORS-1)\nscalarMap = mplcm.ScalarMappable(norm=cNorm, cmap=cm)\n\n\ntargets = np.unique(y) \n\nfor target in targets:\n  indicesToKeep = (finalDf['target'] == target)\n  ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1'], finalDf.loc[indicesToKeep, 'principal component 2'], s = 50, alpha=0.3)\nax.legend(targets)\nax.grid()","fc959814":"# t-SNE\n##ref: https:\/\/medium.com\/@sourajit16.02.93\/tsne-t-distributed-stochastic-neighborhood-embedding-state-of-the-art-c2b4b875b7da\nfrom sklearn.manifold import TSNE \n\n\n# Module for standardization\nfrom sklearn.preprocessing import StandardScaler\n#Get the standardized data\nstandardized_data = StandardScaler().fit_transform(X)\n\nmodel = TSNE(n_components=2) #n_components means the lower dimension\n\nlow_dim_data = pd.DataFrame(model.fit_transform(standardized_data))\n\n\nfinalDf = pd.concat([low_dim_data, pd.DataFrame(y)], axis = 1)\nfinalDf.columns = ['Dim 1', 'Dim 2','target']\n\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Dim 1', fontsize = 15)\nax.set_ylabel('Dim 2', fontsize = 15)\nax.set_title('2 component tSNE', fontsize = 20)\ntargets = np.unique(y)\n\nfrom itertools import cycle\ncycol = cycle('bgrcmk')\n\nfor target in targets:\n  indicesToKeep = (finalDf['target'] == target)\n  ax.scatter(finalDf.loc[indicesToKeep, 'Dim 1'], finalDf.loc[indicesToKeep, 'Dim 2'], s = 50, color=next(cycol), alpha=0.3)\nax.legend(targets)\nax.grid()","8ff9610c":"!pip install minisom\n# self-organizing map\n##ref: https:\/\/rubikscode.net\/2018\/08\/27\/implementing-self-organizing-maps-with-python-and-tensorflow\/\n##ref: https:\/\/pypi.org\/project\/MiniSom\/\n##ref: https:\/\/github.com\/JustGlowing\/minisom\n##ref: https:\/\/glowingpython.blogspot.com\/2013\/09\/self-organizing-maps.html\n#!pip install minisom\nfrom minisom import MiniSom\nfrom numpy import genfromtxt,array,linalg,zeros,apply_along_axis\n\ndata = X\n# normalization to unity of each pattern in the data\ndata = apply_along_axis(lambda x: x\/linalg.norm(x),1,data)\n\nfrom minisom import MiniSom\n### Initialization and training ###\nsom = MiniSom(7,7,X.shape[1],sigma=1.0,learning_rate=0.5)\nsom.random_weights_init(data)\nsom.train_random(data,100) # training with 100 iterations\n\nfrom pylab import plot,axis,show,pcolor,colorbar,bone\nbone()\npcolor(som.distance_map().T) # distance map as background\ncolorbar()\n\n# use different colors and markers for each label\nmarkers = ['o','s','D']\ncolors = ['r','g','b']\nfor cnt,xx in enumerate(data):\n w = som.winner(xx) # getting the winner\n # palce a marker on the winning position for the sample xx\n plot(w[0]+.5,w[1]+.5,markers[y[cnt]],markerfacecolor='None',\n   markeredgecolor=colors[y[cnt]],markersize=12,markeredgewidth=2)\nshow() # show the figure","3fed6eeb":"## Data preparation[](http:\/\/)","b8829662":"Iris is a good old dataset many people use for teaching\/learning. Here I summarize basic yet useful supervised\/ unsupervised learning techniques on the Iris dataset. The idea is that given any input X (features) as a pandas dataframe and y (target variable) as a numpy array, all the implementations below can be easily used by copying-pasting. Welcome to use my code and let me know if anything's lacking ~","83954654":"## Unsupervised learning","00c2348d":"# Introduction","5e6058b6":"## Supervised learning"}}