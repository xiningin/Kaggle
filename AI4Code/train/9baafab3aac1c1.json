{"cell_type":{"4002982f":"code","262bb3c9":"code","e003b36f":"code","d48288c0":"code","bb794ec5":"code","03cc1e61":"code","16703952":"code","36944e9f":"code","667414ab":"code","ef792334":"code","55291c38":"code","c6c927ba":"code","2dc83a10":"code","d2d5075f":"code","1096938e":"code","4ee951e1":"code","15630ef9":"code","79ab41f3":"markdown","4fbf86a5":"markdown","ed206741":"markdown"},"source":{"4002982f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","262bb3c9":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom scipy import ndimage\n\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Input, concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.regularizers import l2\n\nnp.random.seed(7)\n\n\n%matplotlib inline","e003b36f":"# Load the data from the input CSV files. Use relative path.\ntrain_path = '..\/\/input\/\/train.csv'\ntest_path = '..\/\/input\/\/test.csv'\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\n\ntrain_df.info()\ntest_df.info()","d48288c0":"# Doing some basic checks\npd.options.display.max_rows = 1000\nprint (train_df.isnull().sum())","bb794ec5":"print (test_df.isnull().sum())","03cc1e61":"'''\nTraining data has column \"label\" which is the Y value. So, assign all columns apart from \"label\" to X_train and assign the\n\"label\" column value to Y_train\n'''\nX_train=train_df.drop(\"label\",axis=1).values\nY_train=train_df[\"label\"].values\n\nprint ('Shape of X_train >>',X_train.shape)\nprint ('Shape of Y_train >>',Y_train.shape)\n\n'''\nTest data are not labeled. So, assigning all to X_test\n'''\nX_test=test_df.values\n\nprint ('Shape of X_test >>',X_test.shape)","16703952":"#prepare data e.g. one hot conversion\nnum_classes = len(np.unique(Y_train))\nnum_classes\n\nX_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\nX_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\nY_train = keras.utils.to_categorical(Y_train, num_classes)\n\nprint(X_train.shape, 'train samples')\nprint(X_test.shape, 'test samples')\nprint(Y_train.shape, 'target train samples')","36944e9f":"# convert int to float. This helps avoid rounding of parameter values\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\n# normalise\nX_train \/= 255\nX_test \/= 255","667414ab":"# Visualize a sample of the training data\n\nindex  = 120\nk = X_train[index,:]\nk = k.reshape((28, 28))\nplt.title('Label is {label}'.format(label= np.argmax(Y_train[index])))\nplt.imshow(k, cmap='gray')","ef792334":"# Inception block\ndef inception_block(inputs):\n    tower_one = MaxPooling2D((3,3), strides=(1,1), padding='same')(inputs)\n    tower_one = Conv2D(6, (1,1), activation='relu', border_mode='same')(tower_one)\n\n    tower_two = Conv2D(6, (1,1), activation='relu', border_mode='same')(inputs)\n    tower_two = Conv2D(6, (3,3), activation='relu', border_mode='same')(tower_two)\n\n    tower_three = Conv2D(6, (1,1), activation='relu', border_mode='same')(inputs)\n    tower_three = Conv2D(6, (5,5), activation='relu', border_mode='same')(tower_three)\n    x = concatenate([tower_one, tower_two, tower_three], axis=3)\n    return x\n\n# Creating model with the inception block\ndef inception_model(x_train):\n\n    inputs = Input(x_train.shape[1:])\n\n    x = inception_block(inputs)\n        \n    x = Dropout(0.25)(x)\n\n    x = Conv2D(32, (3, 3), padding='same')(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(32, (3, 3), padding='same')(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Dropout(0.25)(x)\n    \n    x = Flatten()(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(10, activation='softmax')(x)\n\n    model = Model(input=inputs, output=predictions)\n\n    model.compile(loss='categorical_crossentropy',\n                 optimizer='adam',\n                 metrics=['accuracy'])\n    return model","55291c38":"model = inception_model(X_train)\nmodel.summary()","c6c927ba":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","2dc83a10":"# Training the model\nhistory = model.fit(X_train, Y_train,\n              batch_size=100,\n              epochs=100,\n              validation_split=0.1,\n              shuffle=True)","d2d5075f":"# check accuracy on complete training data set\nscores_train = model.evaluate(X_train, Y_train)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))","1096938e":"# Making prediction on the test data set\npredictions = model.predict(X_test)\npredictions = np.argmax(predictions, axis = 1)\npredictions","4ee951e1":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","15630ef9":"# write the output data file\nresult=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\"Label\": predictions})\nresult.to_csv(\"mnist_cnn_only_v1.csv\", index=False, header=True)","79ab41f3":"Before we run the model on the training datasets, we compile the model in which we define various things like the loss function, the optimizer and the evaluation metric.","4fbf86a5":"Now, to fit the model on the training input and training target dataset, we run the following command using a minibatch of size 10 and 10 epochs.","ed206741":"Now, let's make predictions on the test dataset."}}