{"cell_type":{"72b29e3d":"code","929a8a7e":"code","ca2451c4":"code","3fd63293":"code","26177a06":"code","e2cd90c8":"code","35425fbb":"code","f4c30100":"code","b30aecd2":"code","bd20805e":"code","0fa39bb8":"code","e8a9692f":"code","791a49a7":"code","f174b874":"code","cfef09d7":"code","5c19f304":"code","103e3113":"code","993ff246":"code","f4f3bb2e":"code","4680cb44":"code","2e7aa378":"code","dfea1845":"code","f76f1ea4":"code","0424b433":"code","098347fc":"code","a1ebbd2f":"code","89014184":"code","321be3b5":"code","537df3c1":"code","dc0bcaf7":"code","8cacf2c5":"code","7b95a21a":"code","a97aed01":"code","49e44364":"code","24b35bc5":"code","3708eacb":"markdown","768b1ceb":"markdown","bacc0645":"markdown","6e76e5ff":"markdown","0c6702ac":"markdown","0f94d2f1":"markdown","d71fbeb5":"markdown","763d480f":"markdown","93298000":"markdown","7075d7fa":"markdown","205c107e":"markdown","8954ac74":"markdown","d32d6efc":"markdown","cb72cae5":"markdown","ff9100ff":"markdown","9088d42c":"markdown","5a8bb22b":"markdown","754b61b2":"markdown","d681586e":"markdown","5df835f6":"markdown","440c34b6":"markdown","8df01c72":"markdown","c58846bf":"markdown","ca411648":"markdown","2f54213d":"markdown","c0d9cf09":"markdown","b45b02c9":"markdown","93cfe94d":"markdown","72c5cf8f":"markdown","0af6b5d1":"markdown","0986e7d7":"markdown","e806c29f":"markdown","d8d7f5b9":"markdown","ea20ff7e":"markdown","058a590c":"markdown","c99bfce9":"markdown","22ed2ad1":"markdown"},"source":{"72b29e3d":"#Import libraries\nimport numpy as np\nimport pandas as pd\nimport os,time\nimport pickle,gzip","929a8a7e":"#Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor=sns.color_palette()\nimport matplotlib as mpl\n%matplotlib inline","ca2451c4":"#Data prep and Model Evaluation\nfrom sklearn import preprocessing as pp\nfrom scipy.stats import pearsonr\nfrom numpy.testing import assert_array_almost_equal\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import precision_recall_curve,average_precision_score\nfrom sklearn.metrics import roc_curve,auc,roc_auc_score\nfrom sklearn.metrics import confusion_matrix,classification_report","3fd63293":"#Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb","26177a06":"import pandas as pd\n\ntrain_set, validation_set, test_set = pd.read_pickle(r'..\/input\/d\/towhidultonmoy\/mnist-pickle-dataset\/mnist.pkl')","e2cd90c8":"X_train,y_train=train_set[0],train_set[1]\nX_validation,y_validation=validation_set[0],validation_set[1]\nX_test,y_test=test_set[0],test_set[1]","35425fbb":"print(\"Shape of X_train: \",X_train.shape)\nprint(\"Shape of y_train: \",y_train.shape)\nprint(\"Shape of X_validation: \", X_validation.shape)\nprint(\"Shape of y_validation: \", y_validation.shape)\nprint(\"Shape of X_test: \", X_test.shape)\nprint(\"Shape of y_test: \", y_test.shape)","f4c30100":"#Creat Pandas DataFrames from the datasets\ntrain_index=range(0,len(X_train))\nvalidation_index=range(len(X_train),len(X_train)+len(X_validation))\ntest_index=range(len(X_train)+len(X_validation),len(X_train)+len(X_validation)+len(X_test))\nX_train=pd.DataFrame(data=X_train,index=train_index)\ny_train=pd.Series(data=y_train,index=train_index)\n\nX_validation=pd.DataFrame(data=X_validation,index=validation_index)\ny_validation=pd.Series(data=y_validation,index=validation_index)\n\nX_test=pd.DataFrame(data=X_test,index=test_index)\ny_test=pd.Series(data=y_test,index=test_index)","b30aecd2":"X_train","bd20805e":"#Exploring the data\nX_train.describe()","0fa39bb8":"y_train.head()","e8a9692f":"#Displaying the image\ndef view_digit(example):\n    label = y_train.loc[example]\n    image = X_train.loc[example,:].values.reshape([28,28])\n    plt.title('Example: %d Label: %d' % (example, label))\n    plt.imshow(image, cmap=plt.get_cmap('gray'))\n    plt.show()\n","791a49a7":"view_digit(2)","f174b874":"#Applying PCA\nfrom sklearn.decomposition import PCA\n\nn_components=784\nwhiten= False\nrandom_state=2018\n\npca=PCA(n_components=n_components,whiten=whiten,random_state=random_state)","cfef09d7":"X_train_PCA=pca.fit_transform(X_train)\nX_train_PCA=pd.DataFrame(data=X_train_PCA,index=train_index)\nX_train_PCA","5c19f304":"#Percenatage of Variance Captured by 784 principal components\nprint(\"Variance Explained by all 784 principle components: \",sum(pca.explained_variance_ratio_))","103e3113":"#Percenatage of Variance Captured by X principal components\nimportanceOfPrincipalComponents= pd.DataFrame(data=pca.explained_variance_ratio_)\nimportanceOfPrincipalComponents=importanceOfPrincipalComponents.T\nimportanceOfPrincipalComponents","993ff246":"print(\"Variance Captured by First 10 Principal Components: \",importanceOfPrincipalComponents.loc[:,0:9].sum(axis=1).values)\nprint(\"Variance Captured by First 20 Principal Components: \",importanceOfPrincipalComponents.loc[:,0:19].sum(axis=1).values)\nprint(\"Variance Captured by First 50 Principal Components: \",importanceOfPrincipalComponents.loc[:,0:49].sum(axis=1).values)\nprint(\"Variance Captured by First 100 Principal Components: \",importanceOfPrincipalComponents.loc[:,0:99].sum(axis=1).values)\nprint(\"Variance Captured by First 200 Principal Components: \",importanceOfPrincipalComponents.loc[:,0:199].sum(axis=1).values)\nprint(\"Variance Captured by First 300 Principal Components: \",importanceOfPrincipalComponents.loc[:,0:299].sum(axis=1).values)","f4f3bb2e":"# View percentage captured by first X components\nsns.set(rc={'figure.figsize':(10,10)})\nsns.barplot(data=importanceOfPrincipalComponents.loc[:,0:10],color='k')","4680cb44":"X_train_PCA","2e7aa378":"def scatterPlot(xDF,yDF,algoName):\n    tempDF=pd.DataFrame(data=xDF.loc[:,0:1],index=xDF.index)\n    tempDF=pd.concat((tempDF,yDF),axis=1,join=\"inner\")\n    tempDF.columns=[\"First Vector\",\"Second Vector\",\"Label\"]\n    sns.lmplot(x=\"First Vector\",y=\"Second Vector\",hue=\"Label\",data=tempDF,fit_reg=False)\n    ax=plt.gca()\n    ax.set_title(\"Separation of Observations using \" +algoName)\nscatterPlot(X_train_PCA,y_train,\"PCA\")","dfea1845":"# View two random dimensions\nX_train_scatter = pd.DataFrame(data=X_train.loc[:,[350,406]], index=X_train.index)\nX_train_scatter = pd.concat((X_train_scatter,y_train), axis=1, join=\"inner\")\nX_train_scatter.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\nsns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", data=X_train_scatter, fit_reg=False)\nax = plt.gca()\nax.set_title(\"Separation of Observations Using Original Feature Set\")","f76f1ea4":"from sklearn.decomposition import IncrementalPCA\n\nn_components=784\nbatch_size=None\n\nincrementalPCA=IncrementalPCA(n_components=n_components,batch_size=batch_size)\n\nX_train_incrementalPCA=incrementalPCA.fit_transform(X_train)\nX_train_incrementalPCA=pd.DataFrame(data=X_train_incrementalPCA,index=train_index)\n\nX_validation_incrementalPCA=incrementalPCA.fit_transform(X_validation)\nX_validation_incrementalPCA=pd.DataFrame(data=X_validation_incrementalPCA,index=validation_index)\n\nscatterPlot(X_train_incrementalPCA,y_train,\"Incremental PCA\")","0424b433":"from sklearn.decomposition import SparsePCA\nn_components=100\nalpha=0.0001\nrandom_state=2018\nn_jobs=-1\n\nsparsePCA=SparsePCA(n_components=n_components,alpha=alpha,random_state=random_state,n_jobs=n_jobs)\n\nsparsePCA.fit(X_train.loc[:10000,:])  #Because this algorithm trains a bit more slowly than normal PCA, we will train on just the first 10,000 examples in our training set\nX_train_sparsePCA = sparsePCA.transform(X_train)\nX_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=train_index)\nX_validation_sparsePCA = sparsePCA.transform(X_validation)\nX_validation_sparsePCA = \\\npd.DataFrame(data=X_validation_sparsePCA, index=validation_index)\nscatterPlot(X_train_sparsePCA, y_train, \"Sparse PCA\")\n","098347fc":"from sklearn.decomposition import KernelPCA\nn_components=100\nkernel=\"rbf\"\ngamma=None\nrandom_state=2018\nn_jobs=1\n\nkernelPCA=KernelPCA(n_components=n_components, kernel=kernel, \\\ngamma=gamma, n_jobs=n_jobs, random_state=random_state)","a1ebbd2f":"kernelPCA.fit(X_train.loc[:10000,:])  #Because this algorithm trains a bit more slowly than normal PCA, we will train on just the first 10,000 examples in our training set\nX_train_sparsePCA = sparsePCA.transform(X_train)\nX_train_kernelPCA = kernelPCA.transform(X_train)\nX_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA,index=train_index)\nX_validation_kernelPCA = kernelPCA.transform(X_validation)\nX_validation_kernelPCA = \\\npd.DataFrame(data=X_validation_kernelPCA, index=validation_index)\nscatterPlot(X_train_kernelPCA, y_train, \"Kernel PCA\")","89014184":"from sklearn.decomposition import TruncatedSVD\nn_components=200\nalgorithm=\"randomized\"\nn_iter=5\nrandom_state=2018\n\nsvd = TruncatedSVD(n_components=n_components, algorithm=algorithm, \\\nn_iter=n_iter, random_state=random_state)\nX_train_svd = svd.fit_transform(X_train)\nX_train_svd = pd.DataFrame(data=X_train_svd, index=train_index)\nX_validation_svd = svd.transform(X_validation)\nX_validation_svd = pd.DataFrame(data=X_validation_svd, index=validation_index)\nscatterPlot(X_train_svd, y_train, \"Singular Value Decomposition\")","321be3b5":"#Gaussian Random Projection\nfrom sklearn.random_projection import GaussianRandomProjection\n\nn_components=\"auto\"\neps=0.5\nrandom_state=2018\n\nGRP= GaussianRandomProjection(n_components=n_components, eps=eps, \\\nrandom_state=random_state)\n\nX_train_GRP = GRP.fit_transform(X_train)\nX_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)\nX_validation_GRP = GRP.transform(X_validation)\nX_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)\nscatterPlot(X_train_GRP, y_train, \"Gaussian Random Projection\")","537df3c1":"from sklearn.random_projection import SparseRandomProjection\nn_components = 'auto'\ndensity = 'auto'\neps = 0.5\ndense_output = False\nrandom_state = 2018\nSRP = SparseRandomProjection(n_components=n_components, \\\ndensity=density, eps=eps, dense_output=dense_output, \\\nrandom_state=random_state)\n\nX_train_SRP = SRP.fit_transform(X_train)\nX_train_SRP = pd.DataFrame(data=X_train_SRP, index=train_index)\nX_validation_SRP = SRP.transform(X_validation)\nX_validation_SRP = pd.DataFrame(data=X_validation_SRP, index=validation_index)\nscatterPlot(X_train_SRP, y_train, \"Sparse Random Projection\")","dc0bcaf7":"# Isomap\nfrom sklearn.manifold import Isomap\nn_neighbors = 5\nn_components = 10\nn_jobs = 4\nisomap = Isomap(n_neighbors=n_neighbors, \\\nn_components=n_components, n_jobs=n_jobs)\nisomap.fit(X_train.loc[0:5000,:])\nX_train_isomap = isomap.transform(X_train)\nX_train_isomap = pd.DataFrame(data=X_train_isomap, index=train_index)\nX_validation_isomap = isomap.transform(X_validation)\nX_validation_isomap = pd.DataFrame(data=X_validation_isomap, \\\nindex=validation_index)\nscatterPlot(X_train_isomap, y_train, \"Isomap\")","8cacf2c5":"# Multidimensional Scaling\nfrom sklearn.manifold import MDS\nn_components = 2\nn_init = 12\nmax_iter = 1200\nmetric = True\nn_jobs = 4\nrandom_state = 2018\nmds = MDS(n_components=n_components, n_init=n_init, max_iter=max_iter, \\\nmetric=metric, n_jobs=n_jobs, random_state=random_state)\nX_train_mds = mds.fit_transform(X_train.loc[0:1000,:])\nX_train_mds = pd.DataFrame(data=X_train_mds, index=train_index[0:1001])\nscatterPlot(X_train_mds, y_train, \"Multidimensional Scaling\")\n","7b95a21a":"# Locally Linear Embedding (LLE)\nfrom sklearn.manifold import LocallyLinearEmbedding\nn_neighbors = 10\nn_components = 2\nmethod = 'modified'\nn_jobs = 4\nrandom_state = 2018\nlle = LocallyLinearEmbedding(n_neighbors=n_neighbors, \\\nn_components=n_components, method=method, \\\nrandom_state=random_state, n_jobs=n_jobs)\nlle.fit(X_train.loc[0:5000,:])\nX_train_lle = lle.transform(X_train)\nX_train_lle = pd.DataFrame(data=X_train_lle, index=train_index)\nX_validation_lle = lle.transform(X_validation)\nX_validation_lle = pd.DataFrame(data=X_validation_lle, index=validation_index)\nscatterPlot(X_train_lle, y_train, \"Locally Linear Embedding\")","a97aed01":"from sklearn.manifold import TSNE\nn_components=2\nlearning_rate=300\nperplexity=30\nearly_exaggeration=12\ninit=\"random\"\nrandom_state=2018\n\ntSNE=TSNE(n_components=n_components,learning_rate=learning_rate,\n         perplexity=perplexity,early_exaggeration=early_exaggeration,\n         init=init,random_state=random_state)\n\nX_train_tSNE=tSNE.fit_transform(X_train_PCA.loc[:5000,:9])\nX_train_tSNE=pd.DataFrame(data=X_train_tSNE,index=train_index[:5001])\n\nscatterPlot(X_train_tSNE,y_train,\"t-SNE\")","49e44364":"from sklearn.decomposition import MiniBatchDictionaryLearning\n\nn_components=50\nalpha=1\nbatch_size=200\nn_iter=25\nrandom_state=2018\n\nminiBatchDictLearning=MiniBatchDictionaryLearning( \\\nn_components=n_components, alpha=alpha, \\\nbatch_size=batch_size, n_iter=n_iter, \\\nrandom_state=random_state)\n\nminiBatchDictLearning.fit(X_train.loc[:,:10000])\nX_train_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_train)\nX_train_miniBatchDictLearning = pd.DataFrame( \\\ndata=X_train_miniBatchDictLearning, index=train_index)\nX_validation_miniBatchDictLearning = \\\nminiBatchDictLearning.transform(X_validation)\nX_validation_miniBatchDictLearning = \\\npd.DataFrame(data=X_validation_miniBatchDictLearning, \\\nindex=validation_index)\nscatterPlot(X_train_miniBatchDictLearning, y_train, \\\n\"Mini-batch Dictionary Learning\")","24b35bc5":"# Independent Component Analysis\nfrom sklearn.decomposition import FastICA\nn_components = 25\nalgorithm = 'parallel'\nwhiten = True\nmax_iter = 100\nrandom_state = 2018\nfastICA = FastICA(n_components=n_components, algorithm=algorithm, \\\nwhiten=whiten, max_iter=max_iter, random_state=random_state)\nX_train_fastICA = fastICA.fit_transform(X_train)\nX_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=train_index)\nX_validation_fastICA = fastICA.transform(X_validation)\nX_validation_fastICA = pd.DataFrame(data=X_validation_fastICA, \\\nindex=validation_index)\nscatterPlot(X_train_fastICA, y_train, \"Independent Component Analysis\")","3708eacb":"## We will be using the MNIST dataset.\n\n**Regrading the dataset:**\n\n* Divided into three sets\u2014a training set with 50,000 examples, a validation set with 10,000 examples, and a test set with 10,000 examples. We have labels for all the examples. \n* This dataset consists of 28x28 pixel images of handwritten digits. To make this simpler, we can flatten each array into a 28x28, or 784, dimensional vector. \n* Each component of the vector is a float between zero and one\u2014representing the intensity of each pixel in the image. Zero stands for black; one stands for white. The labels are numbers between zero and nine, and indicate which digit the image represents.\n","768b1ceb":"## **PCA**","bacc0645":"### Dictionary learning learns the **sparse representation** of the original data. The resulting matrix is known as the dictionary, and the vectors in the dictionary are known as atoms. These atoms are simple, binary vectors, populated by zeros and ones. Each instance in the original data can be reconstructed as a weighted sum of these atoms.","6e76e5ff":"### In PCA, the algorithm finds a **low-dimensional representation of the data while retaining as much of the variation (information) as possible**. \n\n* PCA does this by addressing the correlation among features. \n* If the correlation is very high among a subset of the features, PCA will attempt to combine the highly correlated features and represent this data with a smaller number of linearly uncorrelated features. \n* The algorithm keeps performing this correlation eduction, finding the directions of maximum variance in the original high dimensional data and projecting them onto a smaller dimensional space.  **These newly derived components are known as principal components.**","0c6702ac":"### Another popular **nonlinear dimensionality reduction** method is called locally linear embedding (LLE). This method preserves distances within local neighborhoods as it projects the data from the original feature space to a reduced space. LLE discovers the nonlinear structure in the original, high-dimensional data by segmenting the data into smaller components and modeling each component as a linear embedding.\n","0f94d2f1":"### t-SNE has a nonconvex cost function, which means that different initializations of the algorithm will generate different results. There is no stable solution.","d71fbeb5":"# **Random projection**","763d480f":"### One common problem with unlabeled data is that there are many independent signals embedded together into the features we are given. Using independent component analysis (ICA), we can separate these blended signals into their individual components. After the separation is complete, we can reconstruct any of the original features by adding together some combination of the individual components we generate. ICA is commonly used in signal processing tasks (for example, to identify the individual voices in an audio clip of a busy coffeehouse)","93298000":"### Another approach to learning the underlying structure of the data is to **reduce the rank of the original matrix** of features to a smaller rank such that the original matrix can be recreated using a linear combination of some of the vectors in the smaller rank matrix. This is known as singular value decomposition (SVD).\n\n### **The maximum number of its linearly independent columns (or rows ) of a matrix is called the rank of a matrix. The rank of a matrix cannot exceed the number of its rows or columns**\n","7075d7fa":"## **Sparse Random Projection**\n","205c107e":"## In this figure, we have eight samples of data, each with **four independent features or** **dimensions**. Depending upon the problem being solved, or the origin of this dataset, we may want to reduce the number of dimensions per sample without losing the provided information. This is where dimensionality reduction can be helpful","8954ac74":"### t-distributed stochastic neighbor embedding (t-SNE) is a **nonlinear dimensionality reduction** technique. It does this by constructing **two probability distributions**, one over pairs of points in the high-dimensional space and another over pairs of points in the low-dimensional space such that similar points have a high probability and dissimilar points have a lower probability. Specifically, t-SNE minimizes the Kullback\u2013Leibler divergence between the two probability distributions.\n\nThe **Kullback-Leibler Divergence score** , or KL divergence score, quantifies how much one probability distribution differs from another probability distribution.","d32d6efc":"# **Isomap**","cb72cae5":"## **Gaussian Random Projection**","ff9100ff":"### **Normal PCA, incremental PCA, and sparse PCA linearly project the original data onto a lower dimensional space**, but there is also a **nonlinear form of PCA** known as kernel PCA, which runs a similarity function over pairs of original data points in order to perform nonlinear dimensionality reduction.\n\n### For the kernel PCA algorithm, we need to set the number of components we desire, the type of kernel, and the kernel coefficient, which is known as the **gamma**\n\n* Kernel is used as a measure of similarity. In particular, the kernel function k(x,.) defines the distribution of similarities of points around a given point x. k(x,y) denotes the similarity of point x with another given point y.\n\n* In machine learning \"kernel\" is usually used to refer to the kernel trick, a method of using a linear classifier to solve a non-linear problem \"by mapping the original non-linear observations into a higher-dimensional space\".\n","9088d42c":"### Just as there is a sparse version of PCA, there is a sparse version of random projection known as sparse random projection","5a8bb22b":"# **Independent Component Analysis**\n","754b61b2":"# **Kernel PCA**","d681586e":"# First, clarify what is **sparsity**,\n\n## A matrix in which most entries are **0** is called a **sparse matrix**. These matrices can be stored more efficiently and certain computations can be carried out more efficiently on them provided the matrix is sufficiently large and sparse. Neural networks can leverage the efficiency gained from sparsity by assuming most connection weights are equal to 0.\n\n## **The goal is to reduce the mounds of matrix multiplication deep learning requires**, shortening the time to good results. \n\n\n### The normal PCA algorithm \n* searches for linear combinations in all the input variables, reducing the original feature space as densely as possible. \n* But for some machine learning problems, some degree of **sparsity** may be preferred. \n* A version of PCA that retains some degree of sparsity\u2014controlled by a hyperparameter called alpha\u2014is known as sparse PCA. \n* The sparse PCA algorithm searches for linear combinations in just some of the input variables, reducing the original feature space to some degree but not as compactly as normal PCA.","5df835f6":"### In real-world applications of t-SNE, it is best to use another dimensionality reduction technique (such as PCA, as we do here) to reduce the number of dimensions before applying t-SNE. By applying another form of dimensionality reduction first, we reduce the noise in the features that are fed into t-SNE and speed up the computation of the algorithm","440c34b6":"# **Multidimensional Scaling**","8df01c72":"# **t-SNE**","c58846bf":"![table](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200710135837\/Screenshot-9461.png)","ca411648":"# **Dimensionality reduction** helps counteract \n# \u2014the curse of dimensionality\u2014in which algorithms cannot effectively and efficiently train on the data because of the sheer size of the feature space\n\n## **Dimensionality reduction algorithms project high-dimensional data to a low dimensional space, retaining as much of the salient information as possible while removing redundant information**\n\n### **Once the data is in the low-dimensional space, machine learning algorithms are able to identify interesting patterns more effectively and efficiently because a lot of the noise has been reduced**\n\n\n\nThere are various branches of dimensionality reduction. \n\n# **Linear projection**: \n\n## Linearly projecting data from a high dimensional space to a low-dimensional space\n\nTechniques include:\n* ## Principal component analysis\n * Incremental PCA\n * Sparse PCA \n\n* ## Singular value decomposition \n* ## Random projection\n\n * Gaussian random projection\n * sparse random projection\n\n# **Manifold learning**:\n\n## It is referred to as nonlinear dimensionality reduction. This involves techniques such as \n\nTechniques include:\n\n* ## Kernel PCA \n* ## Isomap\n* ## Multidimensional scaling (MDS)\n* ## Locally linear embedding (LLE)\n* ## t-distributed stochastic neighbor embedding (t-SNE)\n\n# **Other Dimensionality Reduction Methods**:\n\n## Do not rely on any sort of geometry or distance metric.\n\nTechniques include:\n\n* ## Dictionary learning\n* ## Independent component analysis.\n","2f54213d":"### Another linear dimensionality reduction technique is random projection, which relies on the **Johnson\u2013Lindenstrauss lemma**. According to the **Johnson\u2013Lindenstrauss lemma**, points in a high-dimensional space can be embedded into a much lower-dimensional space so that distances between the points are nearly preserved. ","c0d9cf09":"### It is essential to **perform feature scaling before running PCA**. PCA is very sensitive to the relative ranges of the original features.\n\n#### However, for our MNIST digits dataset, the features are already scaled to a range of zero to one, so we can skip this step.","b45b02c9":"# **Locally Linear Embedding (LLE)**","93cfe94d":"## **To put it simply**, \n## **Dimensionality** is the number of dimensions, features, or variables associated with a sample of data. Often, this can be thought of as a number of columns in a dataframe,where each sample is on a new row, and each column describes some attribute of the sample.\n","72c5cf8f":"# **Incremental PCA**","0af6b5d1":"### For **datasets that are very large and cannot fit in memory** , we can perform **PCA incrementally in small batches** , where each batch is able to fit in memory. The batch size can be either set manually or determined automatically. This batch\u0002based form of PCA is known as incremental PCA. The resulting principal components of PCA and incremental PCA are generally pretty similar","0986e7d7":"# **Singular Value Decomposition**","e806c29f":"### For Gaussian random projection, we can either specify the number of components we would like to have in the reduced feature space, or we can set the hyperparameter **eps**. The **eps** controls the quality of the embedding according to the Johnson\u2013Lindenstrauss lemma, where smaller values generate a higher number of dimensions. ","d8d7f5b9":"### Multidimensional scaling (MDS) is a form of **nonlinear dimensionality reduction** that learns the similarity of points in the original dataset and, using this similarity learning, models this in a lower dimensional space","ea20ff7e":"# **Mini-batch dictionary learning**","058a590c":"# References:\n* Ankur A. Patel - Hands-On Unsupervised Learning Using Python_ How to Build Applied Machine Learning Solutions from Unlabeled Data-O'Reilly Media (2019)\n* Rowel Atienza - Advanced Deep Learning with TensorFlow 2 and Keras_ Apply DL, GANs, VAEs, deep RL, unsupervised learning, object detection and segmentation, and more, 2nd Edition\n* Johnston, Benjamin_Jones, Aaron_Kruger, Christopher - Applied Unsupervised Learning with Python-Packt Publishing (2019)\n* https:\/\/machinelearningmastery.com\/divergence-between-probability-distributions\/\n* https:\/\/www.analyticsvidhya.com\/blog\/2018\/08\/dimensionality-reduction-techniques-python\/\n* https:\/\/byjus.com\/jee\/rank-of-a-matrix-and-special-matrices\/\n* https:\/\/stats.stackexchange.com\/questions\/2499\/what-is-a-kernel-in-plain-english\n* https:\/\/stackoverflow.com\/questions\/41325673\/what-does-sparse-mean-in-the-context-of-neural-nets\n\n","c99bfce9":"# **Sparse PCA**","22ed2ad1":"### It learns a new, low-dimensional embedding of the original feature set by calculating the pairwise distances of all the points, where distance is curved or geodesic distance rather than Euclidean distance. "}}