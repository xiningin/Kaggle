{"cell_type":{"248299c4":"code","926565cc":"code","5f04bedc":"code","8d69c87a":"code","b4de6405":"code","87f86da2":"code","fd6596f9":"code","54abec70":"code","727c6db1":"code","8d61c7cb":"code","65315a20":"code","a536c5ae":"code","ef8c7e25":"code","3658db5b":"code","0343d815":"code","22d2ec07":"code","0682fb61":"code","0f7eceeb":"code","f1c79f54":"code","bd871648":"code","442ee5ec":"code","18d9aab6":"code","4ba0d0a4":"code","25ba2cae":"code","3a3a2fc2":"code","cc073786":"code","90d335e4":"code","5a3a1914":"markdown","4da61d57":"markdown","d3b55fbb":"markdown","be2ad794":"markdown","17fa2dee":"markdown","7e21ffaf":"markdown","949f9731":"markdown","ea690a70":"markdown","89f84aae":"markdown"},"source":{"248299c4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport warnings\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\n\nwarnings.simplefilter(\"ignore\")","926565cc":"def plot_metric(clf, testX, testY, name):\n    \"\"\"\n    Small function to plot ROC-AUC values and confusion matrix\n    \"\"\"\n    styles = ['bmh', 'classic', 'fivethirtyeight', 'ggplot']\n\n    plt.style.use(random.choice(styles))\n    plot_confusion_matrix(clf, testX, testY)\n    plt.title(f\"Confusion Matrix [{name}]\")","5f04bedc":"data = pd.read_csv(\"..\/input\/60k-stack-overflow-questions-with-quality-rate\/train.csv\")\ndata2 = pd.read_csv(\"..\/input\/60k-stack-overflow-questions-with-quality-rate\/valid.csv\")\ndata.head()","8d69c87a":"data = data.drop(['Id', 'Tags', 'CreationDate'], axis=1)\ndata['Y'] = data['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\n\ndata2 = data2.drop(['Id', 'Tags', 'CreationDate'], axis=1)\ndata2['Y'] = data2['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\n\ndata.head()","b4de6405":"labels = ['Open Questions', 'Low Quality Question - Close', 'Low Quality Question - Edit']\nvalues = [len(data[data['Y'] == 2]), len(data[data['Y'] == 0]), len(data[data['Y'] == 1])]\nplt.style.use('classic')\nplt.figure(figsize=(16, 9))\nplt.pie(x=values, labels=labels, autopct=\"%1.1f%%\")\nplt.title(\"Target Value Distribution\")\nplt.show()","87f86da2":"data['text'] = data['Title'] + ' ' + data['Body']\ndata = data.drop(['Title', 'Body'], axis=1)\n\ndata2['text'] = data2['Title'] + ' ' + data2['Body']\ndata2 = data2.drop(['Title', 'Body'], axis=1)\n\n\ndata.head()","fd6596f9":"# Clean the data\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text\ndata['text'] = data['text'].apply(clean_text)\ndata2['text'] = data2['text'].apply(clean_text)","54abec70":"# Training Sets\ntrain = data\ntrainX = train['text']\ntrainY = train['Y'].values\n\n# Validation Sets\nvalid = data2\nvalidX = valid['text']\nvalidY = valid['Y'].values\n\nassert trainX.shape == trainY.shape\nassert validX.shape == validY.shape\n\nprint(f\"Training Data Shape: {trainX.shape}\\nValidation Data Shape: {validX.shape}\")","727c6db1":"# Load the vectorizer, fit on training set, transform on validation set\nvectorizer = TfidfVectorizer()\ntrainX = vectorizer.fit_transform(trainX)\nvalidX = vectorizer.transform(validX)","8d61c7cb":"# Define and fit the classifier on the data\nlr_classifier = LogisticRegression(C=1.)\nlr_classifier.fit(trainX, trainY)","65315a20":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Logsitic Regression Classifier is: {(lr_classifier.score(validX, validY))*100:.2f}%\")","a536c5ae":"# Also plot the metric\nplot_metric(lr_classifier, validX, validY, \"Logistic Regression\")","ef8c7e25":"# Define and fit the classifier on the data\nnb_classifier = MultinomialNB()\nnb_classifier.fit(trainX, trainY)","3658db5b":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Naive Bayes Classifier is: {(nb_classifier.score(validX, validY))*100:.2f}%\")","0343d815":"# Also plot the metric\nplot_metric(nb_classifier, validX, validY, \"Naive Bayes\")","22d2ec07":"# Define and fit the classifier on the data\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(trainX, trainY)","0682fb61":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Random Forest Classifier is: {(rf_classifier.score(validX, validY))*100:.2f}%\")","0f7eceeb":"# Also plot the metric\nplot_metric(nb_classifier, validX, validY, \"Random Forest\")","f1c79f54":"# Define and fit the classifier on the data\ndt_classifier = DecisionTreeClassifier()\ndt_classifier.fit(trainX, trainY)","bd871648":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Decision Tree Clf. is: {(dt_classifier.score(validX, validY))*100:.2f}%\")","442ee5ec":"# Also plot the metric\nplot_metric(dt_classifier, validX, validY, \"Decision Tree Classifier\")","18d9aab6":"# Define and fit the classifier on the data\nkn_classifier = KNeighborsClassifier()\nkn_classifier.fit(trainX, trainY)","4ba0d0a4":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of KNN Clf. is: {(kn_classifier.score(validX, validY))*100:.2f}%\")","25ba2cae":"# Also plot the metric\nplot_metric(dt_classifier, validX, validY, \"Decision Tree Classifier\")","3a3a2fc2":"# Define and fit the classifier on the data\nxg_classifier = XGBClassifier()\nxg_classifier.fit(trainX, trainY)","cc073786":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of XGBoost Clf. is: {(xg_classifier.score(validX, validY))*100:.2f}%\")","90d335e4":"# Also plot the metric\nplot_metric(xg_classifier, validX, validY, \"XGBoost Classifier\")","5a3a1914":"# We will try to classify into three classes:\n\n1. must be closed\n\n2. must be edited\n\n3. high-quality\n\n\n### Based on results of several baseline models, we achieve 87% accuracy.","4da61d57":"# Modelling\nLet's start with different non-deep learning approaches for this task.","d3b55fbb":"## 5. KNN Classifier\nWe now are going to use KNN Classifier for this task.","be2ad794":"## 4. Decision Tree Classifier\nLet's now take some decisions using the Decision Tree Classifer","17fa2dee":"# 1. EDA","7e21ffaf":"## 2. Multinomial Naive Bayes\nLet's now switch to the naive the bayes, the NAIVE BAYES!","949f9731":"## 1. Logistic Regression\nLet's first start with our good old, Logistic Regression!","ea690a70":"## 6. XGBoost\nFinally, let's use the XGBoost Classifier and then we'll compare all the different classifiers so far","89f84aae":"## 3. Random Forest Classifier\nLet's now enter the forest with the Random Forest Classifier and see where it takes us!"}}