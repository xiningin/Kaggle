{"cell_type":{"ce9b8568":"code","b3f80fd2":"code","0bee8296":"code","6fb32768":"code","5829a1d6":"code","89dbf3f3":"code","7b5bba68":"code","ab0a9037":"code","835996b5":"code","d0381995":"code","39df18f5":"code","5d8a0aba":"code","5c094e94":"code","210797d9":"code","f558556e":"code","ba8cc6ac":"code","1bc38ad9":"code","eabb8684":"code","744a4f19":"code","18abf331":"markdown","5b11fd87":"markdown","a636e257":"markdown","ff0d8382":"markdown","0ffdc379":"markdown","9f662c73":"markdown"},"source":{"ce9b8568":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.preprocessing import (\n    QuantileTransformer, OneHotEncoder, FunctionTransformer, MinMaxScaler\n)\nfrom sklearn.metrics import make_scorer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import TweedieRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn import set_config\n\nfrom plotnine import *\nfrom plotnine.labels import labs\nfrom plotnine.themes import theme_classic\nfrom plotnine.scales import scale_x_continuous\n\nset_config(display='diagram')\nnp.random.seed(0)","b3f80fd2":"!pip install scikit-misc","0bee8296":"data_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndata_train.head()","6fb32768":"treat_numeric = ['OverallQual', 'YearBuilt', 'GarageArea', 'YearRemodAdd', \n                 'GarageYrBlt', 'TotRmsAbvGrd', 'GrLivArea', 'LotFrontage']\n\ntreat_categ = ['Neighborhood', 'KitchenQual', 'ExterQual','BsmtQual', 'GarageCars', \n               'Alley', 'FullBath', 'GarageFinish', 'Foundation', 'MSSubClass', \n               'GarageType', 'HeatingQC', 'Fireplaces', 'Exterior1st', 'BsmtFinType1', \n               'Exterior2nd']\nsubset_feature = [*treat_numeric, *treat_categ]\n\nx_train = data_train[subset_feature]\n\n# Create features for numeric columns\n# Features are normal distributed at this point, so the z-values\n# For percentiles can be hard coded\ncreate_features = FeatureUnion([\n    ('idenity', FunctionTransformer(lambda x: x)),\n    ('squared-polynomial', FunctionTransformer(lambda x: x ** 2)),\n    ('cubed-polynomial', FunctionTransformer(lambda x: x ** 3)), \n    ('piecewise-0-25-percentiles', FunctionTransformer(lambda x: np.clip(x, -np.Inf, -0.68))),  \n    ('piecewise-25-50-percentiles', FunctionTransformer(lambda x: np.clip(x, -0.68, 0))), \n    ('piecewise-50-75-percentiles', FunctionTransformer(lambda x: np.clip(x, 0, 0.68))),  \n    ('piecewise-75-100-percentiles', FunctionTransformer(lambda x: np.clip(x, 0.68, np.Inf))), \n])\n\n# Numeric columns get imputed, scaled, then feature created\nnumeric_transformer = Pipeline(\n    steps=[\n        ('imputer', SimpleImputer(add_indicator = True, strategy = 'median')),         \n        ('scaler', QuantileTransformer(output_distribution = 'normal')),\n        ('feature_creator', create_features),\n        ('scale_min_max', MinMaxScaler()) # scale 0 to 1\n    ])\n\n# Categorical columns get one-hot encoded\ncategorical_transformer = Pipeline(\n    steps=[\n        ('missing_ind', SimpleImputer(strategy = 'constant')),  \n        ('cat-onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n\n# Select columns into one of those groups\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('numeric', numeric_transformer, treat_numeric),\n        ('cat', categorical_transformer, treat_categ)\n    ])\n","5829a1d6":"def log_rmse(y_true, y_pred):\n    \"\"\"Calculates RMSE the same way the competition does\"\"\"\n    pred = np.log(y_pred)\n    truth = np.log(y_true)\n    return np.sqrt(np.mean(np.square(truth-pred)\/truth))\n\nscore = make_scorer(log_rmse, greater_is_better = False)","89dbf3f3":"param_grid = {\n    'Regression__alpha': [0.1**(x) for x in range(2, 6)], #l2 penalty\n    'Regression__power': [0, 1.7, 1.8, 1.9, 2],\n    'Regression__link': ['identity', 'log']\n}\n\nmy_pipe = Pipeline(steps=[\n    ('Preprocessor', preprocessor),\n    ('Regression', TweedieRegressor(max_iter = 100000, warm_start = True))\n])\n\ngrid_search = GridSearchCV(my_pipe, param_grid, cv=10, n_jobs = -1, scoring = score)\n\ngrid_search.fit(X = x_train, y = data_train['SalePrice'])","7b5bba68":"def pretty_up_table(df):\n    new_columns = df.params.apply(pd.Series)\n    old_columns = df.drop(['params'], axis = 1)\n    return pd.concat([new_columns, old_columns], axis = 1)\n                          \nresult = (\n    pd.DataFrame(grid_search.cv_results_)\n    [['params', 'mean_test_score', 'std_test_score']]\n    .pipe(pretty_up_table)\n    .sort_values('mean_test_score', ascending = False)\n)\n\nresult","ab0a9037":"data_train['predicted_SalePrice'] = grid_search.predict(data_train[subset_feature])\n\n# Model Result\ntrain_score = log_rmse(data_train['SalePrice'], data_train['predicted_SalePrice'])\n\n# Model that Predicts Median\nnaive_score = log_rmse(data_train['SalePrice'], data_train['SalePrice'].mean())\n\nprint(f\"Score: {train_score}, Naive Score: {naive_score}\")","835996b5":"(ggplot(data_train)\n + geom_smooth(aes(x='SalePrice', y='SalePrice', color = \"'Ground Truth'\"), \n               method = 'lm', \n               se=False)\n + geom_smooth(aes(x='SalePrice', y='predicted_SalePrice', color = \"'Prediction'\"), \n               method = 'loess')\n + labs(x = 'Observed Home Price', \n        y = 'Predicted', \n        color = 'Result Type',\n        title = 'Comparison of Prediction vs Observed Sale Price')\n + theme_classic()\n + theme(axis_text_x=element_text(angle=45, hjust=1))\n + scale_x_log10()\n + scale_y_log10()\n)","d0381995":"(ggplot(data_train)\n + geom_smooth(aes(x='GrLivArea', y='SalePrice', color = \"'Observed'\"),\n               method = 'loess', \n               se=False)\n + geom_smooth(aes(x='GrLivArea', y='predicted_SalePrice', color = \"'Predicted'\"), \n               method = 'loess', \n               se=False)\n + labs(x = 'Ground Level Area (Square Feet)', \n        y = 'Home Sale Price', \n        color = 'Result Type',\n        title = 'Comparison of Predicted vs Observed Sale Price\\nby Ground Level Area')\n + theme_classic()\n + theme(axis_text_x=element_text(angle=45, hjust=1))\n + scale_y_log10()\n)","39df18f5":"(ggplot(data_train)\n + geom_smooth(aes(x='OverallQual', y='SalePrice', color = \"'Observed'\"), \n               method = 'loess', \n               se=False)\n + geom_smooth(aes(x='OverallQual', y='predicted_SalePrice', color = \"'Predicted'\"), \n               method = 'loess', \n               se=False)\n + labs(x = 'House Quality', \n        y = 'Home Sale Price', \n        color = 'Result Type',\n        title = 'Comparison of Predicted vs Observed Sale Price\\nby House Quality')\n + theme_classic()\n + theme(axis_text_x=element_text(angle=45, hjust=1))\n + scale_y_log10()\n)","5d8a0aba":"temp_data = data_train\\\n    .groupby(['Neighborhood']) \\\n    [['SalePrice', 'predicted_SalePrice']] \\\n    .agg('mean') \\\n    .assign(error = lambda x: x['predicted_SalePrice']\/x['SalePrice'] - 1) \\\n    .reset_index()\n\n(ggplot(temp_data)\n + geom_bar(aes(x='Neighborhood', y='error'), \n            stat = 'identity', \n            fill = 'lightblue')\n + labs(x = 'Neighborhood', \n        y = '% Error on Train Data', \n        color = 'Result Type',\n        title = 'Model Error Percentage by Neighborhood')\n + theme_classic()\n + theme(axis_text_x=element_text(angle=45, hjust=1))\n)","5c094e94":"import shap\nimport functools\n\ndef score_model(x):\n    df = pd.DataFrame(x, columns = subset_feature)\n    return grid_search.predict(df)\n\ntemp_data = data_train[subset_feature].head(250)\nshap_kernel_explainer = shap.KernelExplainer(score_model, \n                                             temp_data, \n                                             masker = shap.maskers.Independent)\nshap_vals = shap_kernel_explainer.shap_values(temp_data, nsamples = 100)\n\nshap.summary_plot(shap_vals, temp_data)","210797d9":"shap.dependence_plot(ind = 'GrLivArea', \n                     shap_values = shap_vals, \n                     features = temp_data, \n                     interaction_index = None, \n                     alpha = 0.5)","f558556e":"shap.dependence_plot(ind = 'OverallQual', \n                     shap_values = shap_vals, \n                     features = temp_data, \n                     interaction_index = None, \n                     alpha = 0.5)","ba8cc6ac":"shap.dependence_plot(ind = 'GarageArea', \n                     shap_values = shap_vals, \n                     features = temp_data, \n                     interaction_index = None, \n                     alpha = 0.5)","1bc38ad9":"shap.dependence_plot(ind = 'YearRemodAdd', \n                     shap_values = shap_vals, \n                     features = temp_data, \n                     interaction_index = None, \n                     alpha = 0.5)","eabb8684":"data_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndata_test['SalePrice'] = grid_search.predict(data_test[subset_feature])\ndata_test.head()","744a4f19":"data_test[['Id', 'SalePrice']].to_csv('submission.csv', index = False)","18abf331":"A hyperparameter search over the alpha parameter (an l2 regularizer) was conducted. \n\nThis also included a search over the tweedie power, and link function. Surprisingly, a slightly lower parameter than Gamma (p = 2) was selected.","5b11fd87":"## Feature Prep\n\nBased upon EDA conducted in prior week, selected the top 25 predictors on a univariate basis to use further\n+ Numeric features were median imputed, quantile scaled to a normal distribution, passed through polynomial transformations as well as piecewise transformations, and then finally scalled between 0 & 1.\n+ Categorical features had missing values treated as a seperate attribute, and then one-hot encoded","a636e257":"# Model Interpretation\n\nThe SHAP package was used to get some high level inference about the predictive power and direction of the of the input variables.\n\nPink values mean the input is \"high\", and blue values mean the input is \"low\" in the range.\n\nThis visual includes the full model pipeline including all transformations and the actual model prediction.","ff0d8382":"## Score On Test\n\nProduces a prediction on the test data which was then submitted to the competition.","0ffdc379":"# Model Fit","9f662c73":"## Checkout Result\n\nA check of the overall model predictions as well as predictions for some of the major variables was conducted.\n\nEverything seemed to be within a range of reasonable."}}