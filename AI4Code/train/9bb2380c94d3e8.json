{"cell_type":{"fb5755e6":"code","a2cae6bd":"code","d2c8c981":"code","1ca7da10":"code","d2282c2e":"code","435e408c":"code","8518d515":"code","4bac6918":"code","e33a116a":"code","c82b6811":"code","1fddc4c7":"code","9fbf297a":"code","326846e8":"code","c7f4da7a":"code","dea87d79":"code","5c3a8f95":"code","1b7d1053":"code","39f51e60":"code","1c979334":"code","053743e3":"code","fe6ca521":"code","383a2956":"code","badfbeef":"code","faa69706":"code","a24762e7":"code","90c9108b":"code","13f7a8a6":"code","cb61bd3f":"code","c1fcfe97":"code","e9bd546d":"code","02ace0ba":"markdown","feeaf938":"markdown","23b45229":"markdown","1b1cc479":"markdown","88ada8b9":"markdown"},"source":{"fb5755e6":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","a2cae6bd":"#Show all the rows & Columns of data\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","d2c8c981":"#Read in the data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","1ca7da10":"train.head(3)","d2282c2e":"test.head(3)","435e408c":"#Checking shape of train & test data\nprint(train.shape, test.shape)","8518d515":"#Checking missing values\nprint('Is the training data contains any missing values? ' + str(train.isnull().any().any()) + '\\n'\n     + 'Is the testing data contains any missing values? ' + str(test.isnull().any().any()))","4bac6918":"#Checking column types\ntrain.info(verbose = 1)","e33a116a":"train.describe()","c82b6811":"#Visualizing the response variable\nsns.countplot(train['target'])","1fddc4c7":"#Take out the response variable and sample 3 dataset\ntarget = train.iloc[:, train.columns == 'target']\nsample1 = pd.concat([train.iloc[:, 2:10], target], axis = 1)\nsample2 = pd.concat([train.iloc[:, 25:35], target], axis = 1)","9fbf297a":"sample1.head(3)","326846e8":"cor1 = sample1.corr()\ncor2 = sample2.corr()\nf, ax = plt.subplots(1, 2, figsize = (12, 8))\nsns.heatmap(cor1, vmax = 0.9, annot = True, square = True, fmt = '.2f', ax=ax[0])\nsns.heatmap(cor2, vmax = 0.9, annot = True, square = True, fmt = '.2f', ax=ax[1])","c7f4da7a":"#Checking correlation between features\n#referening to : https:\/\/www.kaggle.com\/allunia\/santander-customer-transaction-eda\ntrain_correlations = train.drop([\"target\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","dea87d79":"#Take out the data without id & response variable\ntrain_tran = train.iloc[:, 2:]","5c3a8f95":"#Doing PCA for our model\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\npca = PCA()\npca.fit(train_tran)\npca.data = pca.transform(train_tran)\n\n\n#Percentage variance of each pca component stands for\nper_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n#Create labels for the scree plot\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n\n#Plot the data\nplt.figure(figsize = (12, 10))\nplt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label = labels)\nplt.ylabel('percentage of Explained Variance')\nplt.xlabel('Principle Component')\nplt.title('Scree plot')\nplt.show()","1b7d1053":"print('PC1 + PC150 add up to ' +  str(sum(per_var[:150])) + ' % of the variance')","39f51e60":"#Extract the top 150 pc information\npc_columns = []\nfor i in range(150):\n    pc_columns.append('PC' + str(i + 1))\n\nPC_train = pd.DataFrame(data=pca.data[:,:150], columns = pc_columns)","1c979334":"PC_train.head(3)","053743e3":"PC_train.shape","fe6ca521":"#Setting the seed for calculation\nseed = 2019","383a2956":"#train test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(PC_train, target,\n                                                   test_size = 0.25, random_state = seed)","badfbeef":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)  # Don't cheat - fit only on training data\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # apply same transformation to test data","faa69706":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nrf_model = RandomForestClassifier(n_estimators=300, max_depth=10,\n                                  oob_score=True,\n                              random_state=seed)\nrf_model.fit(X_train, y_train.values.ravel())","a24762e7":"from sklearn import metrics\n\nrf_pred = rf_model.predict(X_test)\nprint(metrics.accuracy_score(y_test, rf_pred))","90c9108b":"#Get a validation dataset\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                                   test_size = 0.25, random_state = seed)","13f7a8a6":"import lightgbm as lgb\n\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)","cb61bd3f":"#Tunning in hyperparameters\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'max_depth': 6,\n    'learning_rate': 0.06,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 5,\n    'max_bin': 255,\n    'verbose': 1\n}","c1fcfe97":"num_round = 2000\nlgbm_model = lgb.train(params, lgb_train, num_round, valid_sets = lgb_eval, early_stopping_rounds = 10)","e9bd546d":"lgbm_pred = lgbm_model.predict(X_test, num_iteration = lgbm_model.best_iteration)\nprint(metrics.accuracy_score(y_test, lgbm_pred.round()))","02ace0ba":"__Random Forest__","feeaf938":"- It seems that most of these variables are not pretty independent of each other\n- It also seems that most of these variales are weakly corrlated with the response variables\n- We will try to PCA to reduce the dimensionality of our model while retaining a good amount of information avaliable","23b45229":"- Since we have lots of variables, it would be hard to examing the relations between them, we sample some data out to see\nif we need to reduce our dimensionality\n- Check the relations using heatmap","1b1cc479":"__LightGBM__","88ada8b9":"- These 150 PCs should be sufficient for us to predict model, it preserve roughly 99.5% percent of our variance with 24.75% of \n    variables\n- Now we prepare our data for modeling"}}