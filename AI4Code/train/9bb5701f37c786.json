{"cell_type":{"f0f7e1be":"code","71a28309":"code","0c5bb02b":"code","0e69f125":"code","f7edd36d":"code","62434958":"code","8a78043f":"code","a29167d3":"code","936e61b1":"code","43e73536":"code","e179be64":"code","b550b527":"code","f837ead4":"code","de535cea":"code","5928d47b":"code","055293ec":"code","dc69c7f4":"code","4254fcf7":"code","a22e7850":"code","f7e56f86":"code","bc45271c":"code","bc600af4":"code","9bd20b13":"code","65312a4a":"code","363777ea":"code","d8e80d69":"code","caed4db8":"code","4b85b99a":"code","c6c0a859":"code","932e03e8":"code","11441452":"code","aecec925":"code","2e952844":"code","fca123df":"code","14cfe975":"code","a8af0b87":"code","9a5348a9":"code","419a5272":"code","776c90d1":"code","4fc69ee0":"code","b91bc16f":"code","ad537fba":"code","fd19c78b":"code","07aa71a3":"code","e7123638":"code","b2b4fa2d":"code","195f9dee":"code","15bc5431":"code","6415c744":"code","6c3b8a0d":"code","1ff942b6":"code","39ec2185":"code","96647e9f":"markdown","e4d1602b":"markdown","5823cdd0":"markdown","10b0b3cd":"markdown","1784de9f":"markdown","b96f79f3":"markdown","4508c5fc":"markdown","dd08b0c9":"markdown","4ea52096":"markdown","b737d01f":"markdown","3c0744a3":"markdown","c3d69efb":"markdown","b58f07f1":"markdown","ede3f914":"markdown","ec335dbe":"markdown","885f7fc3":"markdown","a0311ec3":"markdown","fd59300c":"markdown","cf2fbf07":"markdown","938fcd20":"markdown","880752b1":"markdown","c6f2bfc4":"markdown","70bb09ce":"markdown","4a128e02":"markdown","9cac50a1":"markdown","90270a7d":"markdown","cca7f369":"markdown","ba657ebb":"markdown","cc9ba3dc":"markdown","95bed9b5":"markdown","50f5963d":"markdown","46ecf689":"markdown","9036af30":"markdown","0ffb8ff9":"markdown","0c6520e4":"markdown","2a9c3c07":"markdown","f906d263":"markdown","d2d99b08":"markdown","e5e910af":"markdown","b05252ce":"markdown","22b63270":"markdown"},"source":{"f0f7e1be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","71a28309":"import os\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input","0c5bb02b":"PATH = os.getcwd()","0e69f125":"os.chdir(PATH)","f7edd36d":"data = pd.read_csv(\"..\/input\/BackOrders.csv\",header=0)","62434958":"data.shape","8a78043f":"data.columns","a29167d3":"data.index","936e61b1":"data.head(3)","43e73536":"data.describe(include='all')","e179be64":"data.dtypes","b550b527":"for col in ['sku', 'potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']:\n    data[col] = data[col].astype('category')","f837ead4":"data.dtypes","de535cea":"np.size(np.unique(data.sku, return_counts=True)[0])","5928d47b":"data.drop('sku', axis=1, inplace=True)","055293ec":"data.isnull().sum()","dc69c7f4":"print (data.shape)","4254fcf7":"data = data.dropna(axis=0)","a22e7850":"data.isnull().sum()\nprint(data.shape)","f7e56f86":"print (data.columns)","bc45271c":"categorical_Attributes = data.select_dtypes(include=['category']).columns","bc600af4":"data = pd.get_dummies(columns=categorical_Attributes, data=data, prefix=categorical_Attributes, prefix_sep=\"_\",drop_first=True)","9bd20b13":"print (data.columns, data.shape)","65312a4a":"pd.value_counts(data['went_on_backorder_Yes'].values)","363777ea":"#Performing train test split on the data\nX, y = data.loc[:,data.columns!='went_on_backorder_Yes'].values, data.loc[:,'went_on_backorder_Yes'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n","d8e80d69":"#To get the distribution in the target in train and test\nprint(pd.value_counts(y_train))\nprint(pd.value_counts(y_test))","caed4db8":"perceptron_model = Sequential()\n\nperceptron_model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid', kernel_initializer='normal'))","4b85b99a":"perceptron_model.compile(loss='binary_crossentropy', optimizer='adam')","c6c0a859":"perceptron_model.fit(X_train, y_train, epochs=100)","932e03e8":"test_pred=perceptron_model.predict_classes(X_test)\ntrain_pred=perceptron_model.predict_classes(X_train)","11441452":"confusion_matrix_test = confusion_matrix(y_test, test_pred)\nconfusion_matrix_train = confusion_matrix(y_train, train_pred)\n\nprint(confusion_matrix_train)\nprint(confusion_matrix_test)","aecec925":"Accuracy_Train = (confusion_matrix_train[0,0]+confusion_matrix_train[1,1])\/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\nTNR_Train = confusion_matrix_train[0,0]\/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\nTPR_Train = confusion_matrix_train[1,1]\/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n\nprint(\"Train TNR: \",TNR_Train)\nprint(\"Train TPR: \",TPR_Train)\nprint(\"Train Accuracy: \",Accuracy_Train)","2e952844":"Accuracy_Test = (confusion_matrix_test[0,0]+confusion_matrix_test[1,1])\/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\nTNR_Test = confusion_matrix_test[0,0]\/(confusion_matrix_test[0,0] +confusion_matrix_test[0,1])\nTPR_Test = confusion_matrix_test[1,1]\/(confusion_matrix_test[1,0] +confusion_matrix_test[1,1])\n\nprint(\"Test TNR: \",TNR_Test)\nprint(\"Test TPR: \",TPR_Test)\nprint(\"Test Accuracy: \",Accuracy_Test)","fca123df":"# The size of encoded and actual representations\nencoding_dim = 18\nactual_dim = X_train.shape[1]","14cfe975":"X_train.shape","a8af0b87":"# Input placeholder\ninput_attrs = Input(shape=(actual_dim,))\n\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu')(input_attrs)\n\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(actual_dim, activation='sigmoid')(encoded)","9a5348a9":"# this model maps an input to its reconstruction\nautoencoder = Model(input_attrs, decoded)","419a5272":"print(autoencoder.summary())","776c90d1":"autoencoder.compile(optimizer='adam', loss='binary_crossentropy')","4fc69ee0":"autoencoder.fit(X_train, X_train, epochs=10)","b91bc16f":"# this model maps an input to its encoded representation\nencoder = Model(input_attrs, encoded)","ad537fba":"print(encoder.summary())","fd19c78b":"X_train_nonLinear_features = encoder.predict(X_train)\nX_test_nonLinear_features = encoder.predict(X_test)","07aa71a3":"X_train = np.concatenate((X_train, X_train_nonLinear_features), axis=1)\nX_test = np.concatenate((X_test, X_test_nonLinear_features), axis=1)","e7123638":"X_test.shape","b2b4fa2d":"perceptron_model = Sequential()\n\nperceptron_model.add(Dense(1, input_dim=X_train_nonLinear_features.shape[1], activation='sigmoid'))","195f9dee":"perceptron_model.compile(loss='binary_crossentropy', optimizer='adam')","15bc5431":"%time perceptron_model.fit(X_train, y_train, epochs=30)","6415c744":"test_pred=perceptron_model.predict_classes(X_test_nonLinear_features)\ntrain_pred=perceptron_model.predict_classes(X_train_nonLinear_features)","6c3b8a0d":"confusion_matrix_test = confusion_matrix(y_test, test_pred)\nconfusion_matrix_train = confusion_matrix(y_train, train_pred)\n\nprint(confusion_matrix_train)\nprint(confusion_matrix_test)","1ff942b6":"Accuracy_Train=(confusion_matrix_train[0,0]+confusion_matrix_train[1,1])\/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\nTNR_Train= confusion_matrix_train[0,0]\/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\nTPR_Train= confusion_matrix_train[1,1]\/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n\nprint(\"Train TNR: \",TNR_Train)\nprint(\"Train TPR: \",TPR_Train)\nprint(\"Train Accuracy: \",Accuracy_Train)","39ec2185":"Accuracy_Test=(confusion_matrix_test[0,0]+confusion_matrix_test[1,1])\/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\nTNR_Test= confusion_matrix_test[0,0]\/(confusion_matrix_test[0,0] +confusion_matrix_test[0,1])\nTPR_Test= confusion_matrix_test[1,1]\/(confusion_matrix_test[1,0] +confusion_matrix_test[1,1])\n\nprint(\"Test TNR: \",TNR_Test)\nprint(\"Test TPR: \",TPR_Test)\nprint(\"Test Accuracy: \",Accuracy_Test)","96647e9f":"#### Getting evaluation metrics and evaluating model performance","e4d1602b":"14 dimensions Auto Encoder\n\n\nTest TNR:  0.9351943462897526\nTest TPR:  0.6657592256503327\nTest Accuracy:  0.8841659028414299","5823cdd0":"#### Calculate Accuracy, True Positive Rate and True Negative Rates","10b0b3cd":"See the No. row and columns","1784de9f":"Observing the number of records before and after missing value records removal","b96f79f3":"#### Split the data in to train and test\n\nsklearn.model_selection.train_test_split\n\n    Split arrays or matrices into random train and test subsets","4508c5fc":"Display the columns","dd08b0c9":"#### Missing Data\n\n    Missing value analysis and dropping the records with missing values","4ea52096":"Creating dummy variables.\n\n    If we have k levels in a category, then we create k-1 dummy variables as the last one would be redundant. So we use the parameter drop_first in pd.get_dummies function that drops the first level in each of the category\n","b737d01f":"#### Perceptron Model Building with both actual and non-linear features","3c0744a3":"Display data type of each variable","c3d69efb":"#### Target attribute distribution","b58f07f1":"Since the number of missing values is about 5%. For initial analysis we ignore all these records","ede3f914":"#### Combining new non-linear features to X_train and X_test respectively","ec335dbe":"#### Derive new non-linear features using autoencoder","885f7fc3":"##  Binary Classification using Perception, MLP and Autoencoder","a0311ec3":"#### Perceptron Model Building","fd59300c":"#### Understand the Data","cf2fbf07":"#### derive new non-linear features","938fcd20":"#### Calculate Accuracy, True Positive Rate and True Negative Rates","880752b1":"#### Predictions","c6f2bfc4":"Create a separate encoder model","70bb09ce":"#### Delete sku attribute","4a128e02":"See the top rows of the data","9cac50a1":"Display data type of each variable","90270a7d":"#### Observations\n\n    sku is Categorical but is interpreted as int64 \n    potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, and went_on_backorder are also categorical but is interpreted as object. ","cca7f369":"#### Predictions","ba657ebb":"Test TNR:  0.9496819787985866\nTest TPR:  0.5187537810042347\nTest Accuracy:  0.8680682859761687","cc9ba3dc":"Data type conversion\n\n    Using astype('category') to convert potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, and went_on_backorder attributes to categorical attributes.\n","95bed9b5":"Shows a quick statistic summary of your data using describe","50f5963d":"#### Identify Right Error Metrics\n\n    Based on the businees have to identify right error metrics.","46ecf689":"Display the index","9036af30":"#### Getting evaluation metrics and evaluating model performance","0ffb8ff9":"18 dimensions Auto Encoder\n\nTest TNR:  0.9385159010600707\nTest TPR:  0.6454930429522081\nTest Accuracy:  0.8830201649862511","0c6520e4":"#### Data\n\nData file contains the historical data for the 8 weeks prior to the week we are trying to predict. The data was taken as weekly snapshots at the start of each week. Columns are defined as follows:\n\n    sku - Random ID for the product\n\n    national_inv - Current inventory level for the part\n\n    lead_time - Transit time for product (if available)\n\n    in_transit_qty - Amount of product in transit from source\n\n    forecast_3_month - Forecast sales for the next 3 months\n\n    forecast_6_month - Forecast sales for the next 6 months\n\n    forecast_9_month - Forecast sales for the next 9 months\n\n    sales_1_month - Sales quantity for the prior 1 month time period\n\n    sales_3_month - Sales quantity for the prior 3 month time period\n\n    sales_6_month - Sales quantity for the prior 6 month time period\n\n    sales_9_month - Sales quantity for the prior 9 month time period\n\n    min_bank - Minimum recommend amount to stock\n\n    potential_issue - Source issue for part identified\n\n    pieces_past_due - Parts overdue from source\n\n    perf_6_month_avg - Source performance for prior 6 month period\n\n    perf_12_month_avg - Source performance for prior 12 month period\n\n    local_bo_qty - Amount of stock orders overdue\n\n    deck_risk - Part risk flag\n\n    oe_constraint - Part risk flag\n\n    ppap_risk - Part risk flag\n\n    stop_auto_buy - Part risk flag\n\n    rev_stop - Part risk flag\n\n    went_on_backorder - Product actually went on backorder. This is the target value.","2a9c3c07":"#### Loading the required libraries","f906d263":"#### Convert all the attributes to appropriate type","d2d99b08":"#### Loading the data","e5e910af":"#### Converting Categorical to Numeric\n\nFor some of the models all the independent attribute should be of type numeric and Linear Regression model is one among them.\nBut this data set has some categorial attributes.\n\n'pandas.get_dummies' To convert convert categorical variable into dummy\/indicator variables\n","b05252ce":"Test TNR:  0.9836749116607774\nTest TPR:  0.2958257713248639\nTest Accuracy:  0.8534028414298809","22b63270":"#### Problem\n\n    Is to identify products at risk of backorder before the event occurs so the business has time to react. "}}