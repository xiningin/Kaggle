{"cell_type":{"2091a039":"code","8b2990c8":"code","6b4cda1e":"code","62ebe933":"code","6b43b88d":"code","c06b3f8a":"code","6afc06b9":"code","14c2ee83":"code","33b5e41a":"code","167fc0f6":"code","0390fff7":"code","4d6e4eaa":"code","12331082":"code","08493fc2":"code","6f16c9a1":"code","35d99692":"code","b672964a":"code","48aefed1":"code","71ee7523":"code","ae906d1d":"code","accb545c":"code","7e0c60c5":"code","55dd4b29":"code","028aa588":"code","48fafe80":"code","7184d461":"code","93fcc739":"markdown","1d399b50":"markdown","947af77e":"markdown","a80316b0":"markdown","47e0856c":"markdown","0c009bde":"markdown","9bf70259":"markdown","47009946":"markdown","2404ac94":"markdown","f29ede54":"markdown","2a8622d0":"markdown","418c1582":"markdown","2a0e2f09":"markdown","a4b74f89":"markdown","ed54b2c1":"markdown","2de85aef":"markdown","e025d64c":"markdown","1d1e0ec5":"markdown","7225dad0":"markdown","d824e710":"markdown","be14d0a9":"markdown","e47147f5":"markdown","4d8b6fa4":"markdown","450610b3":"markdown","8876caf4":"markdown","762d07de":"markdown","aa1886ce":"markdown","7180cb6d":"markdown","295ee6e3":"markdown","ab6ddec0":"markdown","19551e43":"markdown","8fc5d5be":"markdown","15877174":"markdown","3013145d":"markdown","a6f0fe62":"markdown","e727d0ad":"markdown"},"source":{"2091a039":"# Make sure to have the library versions below for interactive plotting\nfrom IPython.display import clear_output\n!pip install cufflinks\nclear_output() # Clears out huge shell output from cufflinks installation...","8b2990c8":"# Data Wrangling\nimport os\nimport re\nimport sys\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport random as rnd\n\n# Visualizations - Regular plotting\nimport seaborn as sns\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (12,6)\n%matplotlib inline\n\n# Visualizations - Interactive plotting\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \ninit_notebook_mode(connected=False)\nimport cufflinks as cf\ncf.go_offline()\n\n# Feature Engineering\nimport scipy.stats as st\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler\n\n# Machine Learning Models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Model Evaluation\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\n# Warning Handling\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nprint(f'Python environment: {sys.version}')","6b4cda1e":"# Load from .csv\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv', index_col='PassengerId')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv', index_col='PassengerId')\n\n# Extract survival information from train_df and combine the  rest\nhas_survided = train_df['Survived']\ntrain_df.drop('Survived', axis=1, inplace=True)\ndf = pd.concat([test_df, train_df])\n\n# Let's not forget to save train and test index for splitting it later\ntrain_index, test_index = train_df.index, test_df.index \n\n# As we no longer need these dataframes, let's clear some memory\ndel train_df, test_df\n\n# And them let's have a look at what we've got\ndf.head()","62ebe933":"df.info()","6b43b88d":"# Let's create some features that might be insightful and easier to grasp during our exploratory analysis\ndf['FamilySize'] = 1 + df.SibSp + df.Parch\ndf['NameLength'] = df.Name.apply(len)\ndf['TravelsAlone'] = df.FamilySize.apply(lambda x: 1 if x == 1 else 0)\n\n# I noticed every name has a prefixed title, maybe that's predictive in some way... Let's extract it and find it out!\ndf['Title'] = df.Name.str.extract('([A-Za-z]+)\\.')\ndf.Title.value_counts(dropna=False).iplot('bar', title='Captured Titles from Names')","c06b3f8a":"title_dict = {\n    'Mrs': 'Mrs', 'Lady': 'Mrs', 'Countess': 'Mrs',\n    'Jonkheer': 'Other', 'Col': 'Other', 'Rev': 'Other',\n    'Miss': 'Miss', 'Mlle': 'Miss', 'Mme': 'Miss', 'Ms': 'Miss', 'Dona': 'Miss',\n    'Mr': 'Mr', 'Dr': 'Mr', 'Major': 'Mr', 'Capt': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Master': 'Mr'\n}\n\ndf.Title = df.Title.map(title_dict)\nprint('Title count')\ndf.Title.value_counts(dropna=False)","6afc06b9":"# Maybe de distribuition of age might help us\ndf[['Title', 'Age']].pivot(columns='Title', values='Age').iplot(kind='box', title='Age Distribution across Titles')","14c2ee83":"# What about Fares and Pclass\ndf[['Pclass', 'Fare']].pivot(columns='Pclass', values='Fare').iplot(kind='box', title='Fare Distribution across Ticket Classes')","33b5e41a":"# Average Age per Title is the best we can get in a quick-fix\nfor title in df.Title.unique():\n    df.loc[(df.Age.isnull())&(df.Title==title), 'Age'] = df.Age[df.Title==title].mean()\n\n# Average Fare per Pclass as well... The boxplots were indeed insightful\nfor pclass in df.Pclass.unique():\n    df.loc[(df.Fare.isnull())&(df.Pclass==pclass), 'Fare'] = df.Fare[df.Pclass==pclass].mean()\n\n# Let's just take the most common value (mode) to fill Embarked... It was just one missing piece after all\ndf['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode().iloc[0])","167fc0f6":"# Now, let's turn our categorical features into numerical ones so we can plug them into our Machine Learning Models\ndf['Sex'], int2class_sex           = pd.factorize(df.Sex)\ndf['Title'], int2class_title       = pd.factorize(df.Title)\ndf['Embarked'], int2class_embarked = pd.factorize(df.Embarked)\n\n# To easily navigate from numeric to categorical variables, let's re-write those lists as dictionaries\nint2class_sex      = {key: value for key, value in enumerate(int2class_sex)}\nint2class_title    = {key: value for key, value in enumerate(int2class_title)}\nint2class_embarked = {key: value for key, value in enumerate(int2class_embarked)}\n\n# These features are no longer necessary, let's drop them\ndf.drop(['Ticket', 'Cabin', 'Name'], axis=1, inplace=True)\n\n# Let's store the categorial and continuous features in lists is good practice, it might come in handy in the future\ncategorical_features = [\"Pclass\",\"Sex\",\"TravelsAlone\",\"Title\", \"Embarked\"]\ncontinuous_features = ['Fare','Age','NameLength']\n\n# Let's check if we forgot to fill up any missing value\nprint('Missing value percentage')\n(df.isnull().sum() * 100 \/ len(df)).sort_values(ascending=False)","0390fff7":"_ = pd.concat([df, has_survided], axis=1).hist()\nplt.tight_layout(pad=1)","4d6e4eaa":"int2class_embarked","12331082":"# Feature distributions prior to StandardScaler\n_ = df[continuous_features].hist()\nplt.tight_layout(pad=1)","08493fc2":"# Applying StandardScaler\nfor col in continuous_features:\n    transf = df[col].values.reshape(-1,1)\n    scaler = StandardScaler().fit(transf)\n    df[col] = scaler.transform(transf)","6f16c9a1":"# Feature distributions prior to StandardScaler\n_ = df[continuous_features].hist()\nplt.tight_layout(pad=1)","35d99692":"_, ax = plt.subplots(figsize=(12,8))\n_ = sns.heatmap(pd.concat([df, has_survided], axis=1).corr(), annot=True, fmt=\".1f\", cbar_kws={'label': 'Percentage %'}, cmap=\"coolwarm\", ax=ax)\n_ = ax.set_title(\"Feature Correlation Matrix\")\n\n# Try out the interactive plot as alternative below! \n# pd.concat([df, has_survided], axis=1).corr().iplot(kind='heatmap', colorscale=\"RdBu\", title=\"Feature Correlation Matrix\") ","b672964a":"train_df = df.loc[train_index, :]\ntrain_df['Survived'] = has_survided\ntest_df = df.loc[test_index, :]\n\ndel df","48aefed1":"X = train_df.drop(['Survived'], axis=1)\ny = train_df['Survived']","71ee7523":"print(\"Target Variable Distribution - Survived\")\nprint(y.value_counts(normalize=True))","ae906d1d":"# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","accb545c":"def eval_model(model):\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    output_dict =  classification_report(y_test, pred, output_dict=True)\n    fpr, tpr, _ = metrics.roc_curve(y_test, model.predict_proba(X_test)[:,1])\n    output_dict['auc'] = skplt.metrics.auc(fpr, tpr)\n    output_dict['classifier'] = model\n    return output_dict","7e0c60c5":"# I usually store every model and corresponding metric in dictionaries for quick later access\nmodels = {}\nmodels['KNN']                = eval_model(KNeighborsClassifier())\nmodels['LogisticRegression'] = eval_model(LogisticRegression())\nmodels['RandomForest']       = eval_model(RandomForestClassifier())\n\nmodels = pd.DataFrame.from_dict(models)\nfor metric in ['precision', 'recall', 'f1-score']:\n    models = models.append(\n        models.loc['macro avg'].apply(\n            lambda x: dict(x)[metric]).rename(f'{metric} avg'))","55dd4b29":"models.loc[['accuracy', 'auc', 'precision avg', 'recall avg', 'f1-score avg']].T\\\n.sort_values(by='accuracy', ascending=False)\\\n.iplot('bar', title='Model Performance Comparison - Four Metrics', yrange=[.7,.85])","028aa588":"model = models['LogisticRegression']['classifier']\nskplt.metrics.plot_confusion_matrix(y_test, model.predict(X_test), title='Logistic Regression Confusion Matrix')","48fafe80":"model = models['LogisticRegression']['classifier']\nskplt.metrics.plot_roc_curve(y_test, model.predict_proba(X_test))","7184d461":"model = models['RandomForest']['classifier']\nfeats = {feature: importance for feature, importance in zip(X.columns, model.feature_importances_)}\npd.DataFrame.from_dict(feats,orient='index', columns=['importance']).sort_values('importance').iplot('barh', title='Feature Importances - RandomForest')","93fcc739":"Let's have a look at how our target variable is distributed","1d399b50":"Finally let's have a look at feature importance. It means which features have the largest impact on the models predictions.","947af77e":"Here are some insights:\n* Title and Sex are strongly correlated as expected. After all, they are linearly correlated by definition.\n* Same thing goes for FamiliSize and SibSp, Parch and Travels alone. \n* Our target feature, Survived, shows stronger correlation with Sex, Title Fare and strangely NameLength. Let's dig in deeper...","a80316b0":"Let's visualize it so it's easier to grasp which model has the best performance...","47e0856c":"A core concept of the ROC Curve is that the diagonal line represents random guessing and the ideal model (which would always predict correctly) being at True and False Positive Rates axis. We can measure this objectively via AUC (Area Under Curve) which represents here 85%, not bad for starting...","0c009bde":"# Titanic Disaster Survival: A Supervised Learning Classification Problem","9bf70259":"### Feature engineering","47009946":"Much better now! Let's move back to the missing values now. But what could be good criteria for each feature? Let's have a look...","2404ac94":"### Data Toolkit","f29ede54":"Now things start getting fun... Let's train some Machine Learning models to make predictions for us! <br>\nLet's start splitting our dataset into test and train. Then, declaring our target (y) and independent (X) variables.","2a8622d0":"### Dataset","418c1582":"The proposal is to show a straight-forward approach for this Classification Problem, giving special attention to insight creation, explanation and further hypothesis discussions. Main concepts handled:\n\n* Exploratory Analysis\n* Feature Engineering\n* Data Visualization\n* Predictive Model Training and Evaluation \n\nWith no further ado, let's dig into it!","2a0e2f09":"We were able to tackle just a tiny fraction of the Data Science pipeline with this Classification Challenge, but nevertheless it was a lot! Through Exploratory analysis, Feature Engineering, Model Training and Performance Evaluation we put into practice some of the core concepts of Machine Learning and Data Science in general. There's still much to do though, like Hyperparameter Tunning via  Grid Search Cross Validation for instance, that's an assisted method to check several model presets and pick the best performer from them. Or check for better performing algorithms such as Neural Netwokrs of Support Vectors. <br><br>\nThe methods and techniques applied here are far from optimal, but put into perspective how to handle a classification problem. Specially due to a still high False Negative Rate such a model would not yet be ready for production as many improvements listed above are recommended.","a4b74f89":"Well indeed it looks like title is a good proxy for age, we are going to use it to fill the missing ages...","ed54b2c1":"Apparently there are too many titles with the same meaning, let's simplify things...","2de85aef":"It's slightly imbalanced, which may lead to biased classification towards the majoritary class... There are some techniques to handle this issue such as data augmentation, resampling, or stratifing train and test sets. For the purposes of this notebook, such techniques will be left as future work.<br><br>\nNow, let's split the dataset into train and test for training...","e025d64c":"So, the old and traditional Titanic Competition once again... If you are not familiar with it no worries, I'll give you a quick explanation about it and mention the core concepts tackled in this notebook. \n* This is a very popular Data Science challange with a dataset inspired on the sinking of RMS Titanic on April 14 1912. From the 2,224 passenger onboard, there are registers for 1,237 of them including a collection of personal and demographical information such as passenger's names, ticket class, fares and most importantly where the passanger has survived or not to this disaster. This last piece of information is only available for a fraction of the dataset though, 891 people more specifically. What happens to the rest of them is our job to predict!","1d1e0ec5":"Awesome, the distribuition silhouette hasn't changed, only the mean and standard deviation values... Normalization properly accomplished!","7225dad0":"### Model Evaluation","d824e710":"Let's apply it in three different classifiers for demonstration purposes. In a real project I would recommend picking one of these and work on hyperparameter tunning which usually yields better results than trial and error with a bunch of poorly parametrized classifiers.<br><br>\nI chose the following algorithms:\n* K Nearest Neighbors (KNN)\n* Logistic Regression\n* Random Forest","be14d0a9":"Everything look right... Some intermidiate insights:\n* The majority of passengers were between 25 and 50 years old\n* 60% of them were travelling alone\n* 40\/60 female-male ratio\n* 50% travelling third class\n* 80% Embarked at 1... Wait, what was that once again?","e47147f5":"Prior to moving to our Machine Learning Models, let's have a loot at how the features correlate with each other... Seaborn and Pandas have great implementations for that.","4d8b6fa4":"Here we can se that NameLength, Fare, Age and Sex were the most important features to predict wether a person would survive or not to the Titatic sinking. That's a great way to understand why your model performs like it does, and thus find room for improvement. NameLength is for me an unexpected feature to be on the top of this list. That investigation is work for next steps!","450610b3":"Analysing the Confusion Matrix we can notice the false negatives are proporcionally higher than the rest - see the 29% missclassified as negative at bottom left quadrant. That's a issue we can leave for improvement on next steps.<br>\nNow let's have a look at the ROC Curve.","8876caf4":"Everything's looking good so far. Let's take a closer look to each variable, or feature...","762d07de":"### Predictive Modelling","aa1886ce":"Let's start importing the necessary packages.","7180cb6d":"### Conclusions","295ee6e3":"So far the LogistRegression excels in all metrics, being the accuracy a good proxy in this case... Let's take a closer look to some other metrics.","ab6ddec0":"It looks like there are some missing values... Age, Fare, Cabin and Embarked. We're gonna handle it in a while!","19551e43":"Yeap, dictionaries are also awesome for quick peeks like this... So, 1 refers to 'S' which it's supposed to be a harbor or something.","8fc5d5be":"Let's load our train and hold-out (test) datasets and combine them. <br>\nThat's a good practice because we can preprocess the whole dataset at once.","15877174":"Moving on, a good practice with numerical features is to use StandardScaler, a built in feature fron scikit-learn. It standartizes all values from a given feature, so that its mean and standard deviation turn to 0 and 1, respectively.","3013145d":"And Pclass stacks fare groups fairly well... So let's start filling it all up!","a6f0fe62":"Great! Our dataset is looking much tidier now... <br> \nNow that all features were turned into numeric values, we can plot some histograms. <br>\nTha's a good practice so we can get a feeling of the distributions for each feature. <br>\nIt helps also to spot possible mistakes we might have made with so much data wrangling...","e727d0ad":"In order to facilite performance comparison among models, let's wrap it into a method. It basically fits the model to the data and return a dictionary containing classification report statistics, such as accuracy and f1-score."}}