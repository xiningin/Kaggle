{"cell_type":{"fabf674e":"code","533c3205":"code","99938f52":"code","d0f7c1ab":"code","80e55437":"code","147eeb77":"code","3a379bb0":"code","6a1b8186":"code","9ca88da0":"code","2d4b8db5":"code","6a068bff":"code","f68b5ca5":"code","c0cbf865":"code","d5dc546d":"code","f682b29b":"code","7aeb3810":"code","8e3acbac":"code","24066420":"code","d0521a0b":"code","a179ce4d":"code","125c9059":"code","13db016b":"code","2c64d406":"code","084fd397":"code","199f5416":"code","f1cdbc61":"code","26468cb8":"code","2614cc5c":"code","c792e403":"code","36a8a59a":"code","85864fb2":"code","4ef5e990":"code","d75cd10a":"code","944bda2b":"code","aea91b99":"markdown","240cf07f":"markdown","047fb15b":"markdown","62f44f0b":"markdown","0f49f741":"markdown","5d51e22a":"markdown","a8dea75f":"markdown","fd870cb1":"markdown","24a750ed":"markdown","3242e80a":"markdown","c9ac6d4b":"markdown","0a68b675":"markdown","5ea0fa09":"markdown","464c4bf3":"markdown","95f4893a":"markdown","4922205c":"markdown","97354c84":"markdown","bdf39927":"markdown","2507d75b":"markdown","0f7e839f":"markdown","5ff66665":"markdown"},"source":{"fabf674e":"!pip install simple-colors","533c3205":"import numpy as np\nimport pandas as pd\nfrom simple_colors import *\nfrom termcolor import colored\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\nfrom scipy.stats import normaltest\nfrom scipy import stats\n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","99938f52":"#Setting up options\n\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\npd.options.display.float_format = \"{:,.3f}\".format","d0f7c1ab":"# Load the data\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","80e55437":"def data_desc(df):\n    \n    \"\"\"\n    This function helps us with simple data analysis.\n    We may explore the common information about the dataset, missing values, features distribution and duplicated rows\n    \"\"\"\n    \n    # applying info() method\n    print('*******************')\n    print(cyan('General information of this dataset', 'bold'))\n    print('*******************\\n')\n    print(df.info())\n    \n    print('\\n*******************')\n    print(cyan('Number of rows and columns', 'bold'))\n    print('*******************\\n')\n    print(\"Number of rows:\", colored(df.shape[0], 'green', attrs=['bold']))\n    print(\"Number of columns:\", colored(df.shape[1], 'green', attrs=['bold']))\n    \n    # missing values\n    print('\\n*******************')\n    print(cyan('Missing value checking', 'bold'))\n    print('*******************\\n')\n    if df.isna().sum().sum() == 0:\n        print(colored('There are no missing values', 'green', attrs=['bold']))\n        print('*******************')\n    else:\n        print(colored('Missing value detected!', 'green', attrs=['bold']))\n        print(\"\\nTotal number of missing values:\", colored(sum(df.isna().sum()), 'green', attrs=['bold']))\n        \n        print('\\n*******************')\n        print(cyan('Missing values of features', 'bold'))\n        print('*******************\\n')\n        display(df.isna().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('Purples', axis = None))\n        print('\\n*******************')\n        print(cyan('Percentage of missing values of features', 'bold'))\n        print('*******************\\n')\n        display(round((df.isnull().sum() \/ (len(df.index)) * 100) , 3).sort_values(ascending = False).to_frame().rename({0:'%'}, axis = 1).T.style.background_gradient('PuBuGn', axis = None))\n\n        \n    # applying describe() method for categorical features\n    cat_feats = [col for col in df.columns if 'int' in str(df[col].dtype) and col not in ('id', 'target')]\n    print('\\n*******************')\n    print(cyan('Categorical columns', 'bold'))\n    print('*******************\\n')\n    if len(cat_feats) == 0:\n        print(colored(\"There is no categorical features in this data set!\", 'green', attrs=['bold']))\n    else:\n        print(\"Total categorical (binary) features:\", colored(len(cat_feats), 'green', attrs=['bold']))\n    display(df.describe())\n        \n        \n    # describe() for numerical features\n    cont_feats = [col for col in df.columns if 'float' in str(df[col].dtype) and col not in ('id', 'target')]\n    print('\\n*******************')\n    print(cyan('Numerical columns', 'bold'))\n    print('*******************\\n')\n    print(\"Total numerical features:\", colored(len(cont_feats), 'green', attrs=['bold']))\n    df = df[df.columns.difference(['id', 'claim'], sort = False)]\n    display(df.describe())\n    \n    # Checking for duplicated rows -if any-\n    if df.duplicated().sum() == 0:\n        print('\\n*******************')\n        print(colored('There are no duplicates!', 'green', attrs=['bold']))\n        print('*******************')\n    else:\n        print('\\n*******************')\n        print(colored('Duplicates found!', 'green', attrs=['bold']))\n        print('*******************')\n        display(df[df.duplicated()])\n\n    print('\\n*******************')\n    print(cyan('Preview of the data - Top 10 rows', 'bold'))\n    print('*******************\\n')\n    display(df.head(10))\n    print('*******************\\n')\n    \n    print('\\n*******************')\n    print(cyan('End of the report', 'bold'))","147eeb77":"data_desc(train)","3a379bb0":"data_desc(test)","6a1b8186":"plt.figure(figsize=(10, 7))\nax = sns.countplot(y=train[\"target\"], palette='muted', zorder=3, linewidth=5, orient='h', saturation=1, alpha=1)\nax.set_title('Distribution of Target', fontname = 'Times New Roman', fontsize = 30, color = '#8c49e7', x = 0.5, y = 1.05)\nbackground_color = \"#8c49e7\"\nsns.set_palette(['#ffd514']*120)\n\nfor a in ax.patches:\n    value = f'Amount and percentage of values: {a.get_width():,.0f} | {(a.get_width()\/train.shape[0]):,.3%}'\n    x = a.get_x() + a.get_width() \/ 2 - 140000\n    y = a.get_y() + a.get_height() \/ 2 \n    ax.text(x, y, value, ha='left', va='center', fontsize=18, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round4', linewidth=0.7))\n\n\n# ax.margins(-0.12, -0.12)\nax.grid(axis=\"x\")\n\nsns.despine(right=True)\nsns.despine(offset=15, trim=True)","9ca88da0":"numerical_features =[]\n\nfor col in train.columns:\n    if train[col].dtype != 'int64' and col not in ('id', 'target'):\n        numerical_features.append(col)\nprint('Numerical features: ', numerical_features)","2d4b8db5":"def box_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 150))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last = remove_last - 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.01, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        v0 = sns.color_palette(palette = \"pastel\").as_hex()[2]\n        ax = sns.boxplot(x = data[feature], color=v0, saturation=.75)  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('Values', fontsize=14, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'navy')\n    plt.show()","6a068bff":"box_plot(train, numerical_features, 'Box Plot of Numerical Columns of Train Dataset')","f68b5ca5":"box_plot(test, numerical_features, 'Box Plot of Numerical Columns of Train Dataset')","c0cbf865":"def kde_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 150))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(data[feature], color=\"m\", shade=True, label=\"%.3f\"%(data[feature].skew()))  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('Density', fontsize=14, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'navy')\n    \n    plt.show()","d5dc546d":"kde_plot(train, numerical_features, titleText = 'KDE Plot of Numerical Features of Train Dataset', hue = None)","f682b29b":"kde_plot(test, numerical_features, titleText = 'KDE Plot of Numerical Features of Test Dataset', hue = None)","7aeb3810":"def correlation_matrix(data, features):\n    \n    fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n    plt.title('Pearson Correlation Matrix', fontweight='bold', fontsize=25)\n    fig.set_facecolor('#d0d0d0') \n    corr = data[features].corr()\n\n    # Mask to hide upper-right part of plot as it is a duplicate\n    mask = np.triu(np.ones_like(corr, dtype = bool))\n    sns.heatmap(corr, annot = False, center = 0, cmap = 'jet', mask = mask, linewidths = .5, square = True, cbar_kws = {\"shrink\": .70})\n    ax.set_xticklabels(ax.get_xticklabels(), fontfamily = 'sans', rotation = 90, fontsize = 12)\n    ax.set_yticklabels(ax.get_yticklabels(), fontfamily = 'sans', rotation = 0, fontsize = 12)\n    plt.tight_layout()\n    plt.show()","8e3acbac":"correlation_matrix(train, numerical_features)","24066420":"correlation_matrix(test, numerical_features)","d0521a0b":"def hist_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 150))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.histplot(data[feature], edgecolor=\"black\", color=\"darkseagreen\", alpha=0.7)  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Frequency', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","a179ce4d":"train_frac = train.sample(frac = 0.25).reset_index(drop = True)\n\nhist_plot(train_frac, numerical_features, titleText = 'Histogram of Numerical Features of Train Dataset', hue = None)","125c9059":"test_frac = test.sample(frac = 0.25).reset_index(drop = True)\n\nhist_plot(test_frac, numerical_features, titleText = 'Histogram of Numerical Features of Test Dataset', hue = None)","13db016b":"def qqplot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 150))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n        \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)   \n        stats.probplot(data[feature],plot=plt)\n        plt.title('\\nQ-Q Plot')\n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Sample Quantile', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","2c64d406":"qqplot(train_frac, numerical_features, 'Q-Q Plot of Numerical Features of Train Dataset', hue=None)","084fd397":"qqplot(test_frac, numerical_features, 'Q-Q Plot of Numerical Features of Train Dataset', hue=None)","199f5416":"# D'Agostino and Pearson's Test\n\ndef normality_check(data):\n  for i in numerical_features:\n    # normality test\n    stat, p = normaltest(data[[i]])\n    print('Statistics=%.3f, p=%.3f' % (stat, p))\n    # interpret results\n    alpha = 1e-2\n    if p > alpha:\n        print(f'{i} looks Gaussian (fail to reject H0)\\n')\n    else:\n        print(f'{i} does not look Gaussian (reject H0)\\n')","f1cdbc61":"normality_check(train)","26468cb8":"normality_check(test)","2614cc5c":"def detect_outliers(x, c = 1.5):\n    \"\"\"\n    Function to detect outliers.\n    \"\"\"\n    q1, q3 = np.percentile(x, [25,75])\n    iqr = (q3 - q1)\n    lob = q1 - (iqr * c)\n    uob = q3 + (iqr * c)\n\n    # Generate outliers\n\n    indicies = np.where((x > uob) | (x < lob))\n\n    return indicies","c792e403":"# Detect all Outliers \noutliers = detect_outliers(train['target'])\nprint(\"Total Outliers count for claim : \", len(outliers[0]))\n\nprint(\"\\nShape before removing outliers : \",train.shape)\n\n# Remove outliers\n#train.drop(outliers[0],inplace=True, errors = 'ignore')\nprint(\"Shape after removing outliers : \",train.shape)","36a8a59a":"train_iqr = pd.DataFrame()\ntrain_iqr.reindex(columns=[*train_iqr.columns.tolist(), \"-3 IQR\", \"-1.5 IQR\", \"1.5 IQR\", \"3 IQR\"], fill_value = 0)","85864fb2":"from scipy.stats import iqr\n\ndata = []\n\nk = 0\ncolumns = [\"-3 IQR\", \"-1.5 IQR\", \"1.5 IQR\", \"3 IQR\"]\n\nfor i in numerical_features:\n\n    q1 = train[i].quantile(0.25)\n    q3 = train[i].quantile(0.75)\n    \n    iqr = (q3 - q1)\n    lob_1 = q1 - (iqr * 1.5)\n    uob_1 = q3 + (iqr * 1.5)\n    lob_3 = q1 - (iqr * 3)\n    uob_3 = q3 + (iqr * 3)\n    \n    number_uob_1 = f'{round(sum(train[numerical_features[k]] > uob_1) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_lob_1 = f'{round(sum(train[numerical_features[k]] < lob_1) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_uob_3 = f'{round(sum(train[numerical_features[k]] > uob_3) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_lob_3 = f'{round(sum(train[numerical_features[k]] < lob_3) \/ len(train[numerical_features[k]]), 5):,.3%}'\n\n    values = [number_lob_3, number_lob_1, number_uob_1, number_uob_3]\n    zipped = zip(columns, values)\n    a_dictionary = dict(zipped)\n    print(a_dictionary)\n    data.append(a_dictionary)\n    \n    k = k + 1","4ef5e990":"train_iqr = train_iqr.append(data, True)\ntrain_iqr.set_axis([numerical_features], axis='index')","d75cd10a":"def colour(value):\n\n    if float(value.strip('%')) > 10:\n      color = 'red'\n    elif float(value.strip('%')) > 5:\n        color = 'darkorange'   \n    else:\n      color = 'green'\n\n    return 'color: %s' % color\n\n# train_iqr = train_iqr.set_axis([numerical_features], axis='index')\ntrain_iqr = train_iqr.style.applymap(colour)","944bda2b":"train_iqr","aea91b99":"## <p style=\"background-color:#3a2c57; font-family:newtimeroman; margin-bottom:2px; font-size:32px; color: white; text-align:center\">Table of Content<\/p>  \n\n<a id=\"table-of-contents\"><\/a>\n1. [Preperation](#preperation)\n    * 1.1. [Loading Packages and Importing Libraries](#load_packages_import_libraries)\n    * 1.2. [Data Description](#data_description)\n2. [Exploratory Data Analysis (EDA)](#eda)\n    * 2.1. [Numerical Variables](#numerical_variables)\n        * 2.1.1. [Box Plot of Numerical Variables](#box_numerical_variables)\n        * 2.1.2. [KDE Plot of Numerical Variables](#kde_numerical_variables)\n        * 2.1.3. [Correlation Matrix of Numerical Variables](#corr_numerical_variables)\n        * 2.1.4. [Histogram Plot of Numerical Variables](#hist_numerical_variables)\n        * 2.1.5. [Q-Q Plot of Numerical Variables](#qq_numerical_variables)\n    * 2.2. [Normality Check and Outlier Detection](#norm_check_outlier_detect)\n       * 2.2.1. [Mild and Extreme Outlier Detection](#mild_extreme_outlier)","240cf07f":"<a id=\"norm_check_outlier_detect\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">2.2. Normality Check and Outlier Detection<\/p>","047fb15b":"* **Obviously, since the target variable is 0-1 (binary), there is no outlier value for this variable. There are many outliers for other features, but no direct data dropping is done in order not to lose an enormous number of rows.** ","62f44f0b":"* **KDE plot provide an initial insight into the distribution of attributes in the data set.**","0f49f741":"<a id=\"corr_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.3. Correlation Matrix of Numerical Variables<\/p>","5d51e22a":"* **This dataframe about how to manage outlier values during the feature engineering section while developing the model will be very helpful.** ","a8dea75f":"* **It is very obvious that some features contain significant amount of outlier value in both data sets. This has to be handled.**","fd870cb1":"<a id=\"qq_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.5. Q-Q Plot of Numerical Variables<\/p>","24a750ed":"<a id=\"box_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.1. Box Plot of Numerical Variables<\/p>","3242e80a":"[back to top](#table-of-contents)\n<a id=\"eda\"><\/a>\n# <p style=\"background-color:#3a2c57; font-family:newtimeroman; font-size:150%; text-align:center\">2. Exploratory Data Analysis (EDA)<\/p>\n\n* **All numerical variables will be explored in this section.**","c9ac6d4b":"<a id=\"data_description\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">1.2. Data Description<\/p>\n\n* **First of all, some setting up options were made. It is aimed to show all rows and columns in order to improve the general view of data sets. Next, I will load the train and test data sets and display train and test data sets as well.**","0a68b675":"<a id=\"hist_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.4. Histogram Plot of Numerical Variables<\/p>","5ea0fa09":"* **The target variable is in balance in terms of classes. This highlights that there is no need to use methods (such as SMOTE) related to the handling of the imbalanced data set.**","464c4bf3":"[back to top](#table-of-contents)\n<a id=\"preperation\"><\/a>\n# <p style=\"background-color:#3a2c57; font-family:newtimeroman; font-size:150%; text-align:center\">1. Preperation<\/p>\n\n\n<a id=\"load_packages_import_libraries\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">1.1. Loading Packages and Importing Libraries<\/p>\n\n* **Loading packages and importing some helpful libraries.**","95f4893a":"* **Since Q-Q plots are processed in a long time, plots were created on 25% of the data sets.**\n* **The Q-Q plot with clues to the normal distribution also shows tremendously that the data is not normally distributed.**","4922205c":"* **Since histogram plots are processed in a long time, plots were created on 25% of the data sets.**\n* **The logic in the KDE plots is also executed in the histogram plots. Supporting the KDE graphic, it seems that some features are stuck in certain bins as if they were categorical variables. In fact, this gives some clues in terms of feature engineering.**","97354c84":"<a id=\"numerical_variables\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">2.1. Numerical Variables<\/p>","bdf39927":"* **Supporting the box plot, it can be seen from this chart that there are various outliers in some features.**","2507d75b":"* **There is no significant correlation between numerical variables in both train and test data set.**","0f7e839f":"<a id=\"mild_extreme_outlier\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.1. Mild and Extreme Outlier Detection<\/p>","5ff66665":"<a id=\"kde_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.2. KDE Plot of Numerical Variables<\/p>"}}