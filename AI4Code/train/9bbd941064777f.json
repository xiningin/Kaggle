{"cell_type":{"011ec45f":"code","35684271":"code","7dd8f6aa":"code","c1eb6d16":"code","33ea5901":"code","780fdb1b":"code","a4217958":"code","a36ae841":"code","00b89464":"code","137b4894":"code","3332c292":"code","4fe76e5c":"code","5e5644a4":"code","2f872bca":"code","383acc23":"code","a326db4f":"code","3e04c365":"code","f0cca0bf":"code","eb5947a1":"code","99e985a9":"code","a6c31bfd":"code","1a7181ac":"code","6c14d4b9":"code","a0cf8ff1":"code","45b82b71":"code","85eefa18":"code","659d381d":"code","43e856b6":"code","27b7fc7d":"code","960f7f6e":"code","7ad87757":"code","8d689e3d":"code","6d8745f1":"code","2cb98105":"code","1bf379bd":"code","aaa6be9b":"code","f4280215":"code","461466a0":"code","c87a6660":"code","ead2b472":"code","cdf93739":"code","a7ed6c02":"code","fb9fbf07":"code","7f0f6479":"code","c0f505b6":"code","14b3e69f":"code","d0c36d84":"code","9ca37725":"markdown","c198ed71":"markdown","25282cc1":"markdown","1b337693":"markdown","ea74628f":"markdown","041f09f2":"markdown","b1f07de7":"markdown","441686e9":"markdown","690a6b39":"markdown","223d2875":"markdown","0ab8c2b4":"markdown","40628beb":"markdown","5c6bf698":"markdown","79e2268e":"markdown","f6cce7f4":"markdown","fecbcbb9":"markdown","80ece553":"markdown"},"source":{"011ec45f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import probplot\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression,Ridge, Lasso, ElasticNet, RidgeCV, LassoCV,ElasticNetCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error","35684271":"# Download data from here\n# https:\/\/www.kaggle.com\/ahmettezcantekin\/beginner-datasets\n\ndf = pd.read_csv(\"..\/input\/beginner-datasets\/beginner_datasets\/diamond.csv\")","7dd8f6aa":"df.head()","c1eb6d16":"df.info()\n# looks like no null values","33ea5901":"df.describe()","780fdb1b":"for col in df.columns:\n    if df[col].dtype == 'object':\n        print(col,\": \",df[col].unique())","a4217958":"df.groupby('Cut').describe()\n\n# i think cut is a ordinal variable, cut matters","a36ae841":"df.groupby('Color').describe()\n# color might be nominal... there is a variation but its not huge","00b89464":"df.groupby('Clarity').describe()\n\n# to be frank, for now i don't know if i can properly interpret this way so i'll use some plots which will make it easy\n\n# we can also just use target guided ordinal encoding so we'll look into that too... (replace with means)","137b4894":"# it is jittered i guess\n\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        sns.set_theme(style=\"whitegrid\")\n        ax = sns.stripplot(x=col, y=\"Price\", data=df);\n        plt.show()\n\n# i feel like my eda is getting better...\n# practice more is the value to go!","3332c292":"# boxplots\n\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        sns.set_theme(style=\"whitegrid\")\n        ax = sns.boxplot(df[col], df['Price']);\n        plt.show()","4fe76e5c":"# violin plots\n\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        sns.set_theme(style=\"whitegrid\")\n        ax = sns.violinplot(df['Price'], df[col],oreint='h');\n        plt.show()\n        \n# from seaborn:-\n# A violin plot plays a similar role as a box and whisker plot. It shows the distribution of quantitative data across several \n# levels of one (or more) categorical variables such that those distributions can be compared. Unlike a box plot, in which all \n# of the plot components correspond to actual datapoints, the violin plot features a kernel density estimation of the\n# underlying distribution.","5e5644a4":"#ok we'll try target guided ordinal encoding first\n\n# we'll use the describe method to get the values... i was gonna write it down but i realized i can make a dictionary directly\n\ncut_vals = df.groupby('Cut').describe()['Price']['mean'].to_dict()\ncolor_vals = df.groupby('Color').describe()['Price']['mean'].to_dict()\nclarity_vals = df.groupby('Clarity').describe()['Price']['mean'].to_dict()\npolish_vals = df.groupby('Polish').describe()['Price']['mean'].to_dict()\nsymmetry_vals = df.groupby('Symmetry').describe()['Price']['mean'].to_dict()\nreport_vals = df.groupby('Report').describe()['Price']['mean'].to_dict()","2f872bca":"# replace all... i used as int type because floats take more space and i dont think that will make a difference\n\ndf['tg_'+'cut'] = df['Cut'].replace(cut_vals).astype('int')\ndf['tg_'+'color'] = df['Color'].replace(color_vals).astype('int')\ndf['tg_'+'clarity'] = df['Clarity'].replace(clarity_vals).astype('int')\ndf['tg_'+'polish'] = df['Polish'].replace(polish_vals).astype('int')\ndf['tg_'+'symmetry'] = df['Symmetry'].replace(symmetry_vals).astype('int')\ndf['tg_'+'report'] = df['Report'].replace(report_vals).astype('int')","383acc23":"# lets use a heat map to see the correlation\n\ncor = np.round(df.corr(),2)\nplt.figure(figsize=(6,6))\nsns.heatmap(cor, annot=True, cmap='Greens');","a326db4f":"# checking others out of curiosity... \n\n# for i in ['min','max','50%','25%','75%']:\n#     cut_vals = df.groupby('Cut').describe()['Price'][i].to_dict()\n#     color_vals = df.groupby('Color').describe()['Price'][i].to_dict()\n#     clarity_vals = df.groupby('Clarity').describe()['Price'][i].to_dict()\n#     polish_vals = df.groupby('Polish').describe()['Price'][i].to_dict()\n#     symmetry_vals = df.groupby('Symmetry').describe()['Price'][i].to_dict()\n#     report_vals = df.groupby('Report').describe()['Price'][i].to_dict()\n\n#     df['tg_'+'cut'] = df['Cut'].replace(cut_vals).astype('int')\n#     df['tg_'+'color'] = df['Color'].replace(color_vals).astype('int')\n#     df['tg_'+'clarity'] = df['Clarity'].replace(clarity_vals).astype('int')\n#     df['tg_'+'polish'] = df['Polish'].replace(polish_vals).astype('int')\n#     df['tg_'+'symmetry'] = df['Symmetry'].replace(symmetry_vals).astype('int')\n#     df['tg_'+'report'] = df['Report'].replace(report_vals).astype('int')\n\n#     cor = np.round(df.corr(),2)\n#     plt.figure(figsize=(12,12))\n#     sns.heatmap(cor, annot=True, cmap='Greens');\n#     plt.show();","3e04c365":"# tried to get these from the graphs, but they didnt work out that well though similar to mean\n\n# cut_p  = {'Ideal':5, 'Very Good':4, 'Fair':1, 'Good':3, 'Signature-Ideal':2}\n# color_p =  {'H':2, 'E':5, 'G':3,'D':6,'F':4, 'I':1}\n# clarity_p =  {'SI1':1, 'VS1':3 ,'VS2':2, 'VVS2':5 ,'VVS1':4 ,'IF':7 ,'FL':6}\n# polish_p =  {'VG':3 ,'ID':1, 'EX':4, 'G':2}\n# symmetry_p =  {'EX':4, 'ID':1, 'VG':3, 'G':2}\n# report_p =  {'GIA':2, 'AGSL':1}\n\n# df['p_'+'cut'] = df['Cut'].replace(cut_p).astype('int')\n# df['p_'+'color'] = df['Color'].replace(color_p).astype('int')\n# df['p_'+'clarity'] = df['Clarity'].replace(clarity_p).astype('int')\n# df['p_'+'polish'] = df['Polish'].replace(polish_p).astype('int')\n# df['p_'+'symmetry'] = df['Symmetry'].replace(symmetry_p).astype('int')\n# df['p_'+'report'] = df['Report'].replace(report_p).astype('int')\n\n# cor = np.round(df.corr(),2)\n# plt.figure(figsize=(10,10))\n# sns.heatmap(cor, annot=True, cmap='Greens');","f0cca0bf":"# ax1 = plt.subplot(221)\nprobplot(df['Carat Weight'], plot=plt);\n\n#### WOW!\n\n## lets fix that\n\n## tried a few transformations but they don't make a desirable difference","eb5947a1":"probplot(np.power(df['Carat Weight'],3), plot=plt);","99e985a9":"sns.distplot(df['Carat Weight']);","a6c31bfd":"# maybe linear regression won't do so well here...","1a7181ac":"# lets use standard scaler\n\nX = df.drop(['Price','Cut','Color','Clarity','Polish','Symmetry','Report'], axis=1)\ny = df.Price\n\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","6c14d4b9":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvariables = X_scaled\n\n# we create a new data frame which will include all the VIFs\n# note that each variable has its own variance inflation factor as this measure is variable specific (not model specific)\n# we do not include categorical values for mulitcollinearity as they do not provide much information as numerical ones do\nvif = pd.DataFrame()\n\n# here we make use of the variance_inflation_factor, which will basically output the respective VIFs \nvif[\"VIF\"] = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n# Finally, I like to include names so it is easier to explore the result\nvif[\"Features\"] = X.columns\nvif","a0cf8ff1":"X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=42)","45b82b71":"model = LinearRegression()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nmodel.score(X_test,y_test)","85eefa18":"print(cross_val_score(model, X, y , cv=5))\nnp.mean(cross_val_score(model, X, y , cv=5))","659d381d":"ridge = Ridge()\nparameters = {'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,90,100,110]}\nridge_regressor = GridSearchCV(ridge,parameters,scoring='neg_mean_squared_error', cv=5)\nridge_regressor.fit(X,y)","43e856b6":"print(ridge_regressor.best_params_)\nprint(ridge_regressor.best_score_)","27b7fc7d":"ridge = Ridge(alpha=0.01)\nridge.fit(X_train,y_train)\nridge.score(X_test,y_test)","960f7f6e":"lasso = Lasso()\nparameters = {'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,0.1,1,5,10,20,30,35,40,45,50]}\nlasso_regressor = GridSearchCV(lasso,parameters,scoring='neg_mean_squared_error', cv=5)\nlasso_regressor.fit(X,y)\n                               \n    \nprint(lasso_regressor.best_params_)\nprint(lasso_regressor.best_score_)","7ad87757":"lasso = Lasso(alpha=0.1)\nlasso.fit(X_train,y_train)\nlasso.score(X_test,y_test)","8d689e3d":"prediction_lasso = lasso.predict(X_test)\nprediction_ridge = ridge.predict(X_test)","6d8745f1":"sns.distplot(y_test-prediction_lasso);   # right skewed","2cb98105":"sns.distplot(y_test-prediction_ridge);   # right skewed","1bf379bd":"# Lasso Regularization\n# LassoCV will return best alpha and coefficients after performing 10 cross validations\nlasscv = LassoCV(alphas = None,cv =10, max_iter = 100000, normalize = True)\nlasscv.fit(X_train, y_train)\n\n# best alpha parameter\nalpha = lasscv.alpha_\nalpha","aaa6be9b":"lasso_reg = Lasso(alpha)\nlasso_reg.fit(X_train, y_train)\nlasso_reg.score(X_test, y_test)","f4280215":"# Using Ridge regression model\n# RidgeCV will return best alpha and coefficients after performing 10 cross validations. \n# We will pass an array of random numbers for ridgeCV to select best alpha from them\n\nalphas = np.random.uniform(low=0, high=10, size=(50,))\nridgecv = RidgeCV(alphas = alphas,cv=10,normalize = True)\nridgecv.fit(X_train, y_train)\n\nridgecv.alpha_","461466a0":"ridge_model = Ridge(alpha=ridgecv.alpha_)\nridge_model.fit(X_train, y_train)","c87a6660":"ridge_model.score(X_test, y_test)","ead2b472":"elasticCV = ElasticNetCV(alphas = None, cv =10)\n\nelasticCV.fit(X_train, y_train)","cdf93739":"elasticCV.alpha_","a7ed6c02":"# l1_ration gives how close the model is to L1 regularization, below value indicates we are giving equal\n#preference to L1 and L2\nelasticCV.l1_ratio","fb9fbf07":"elasticnet_reg = ElasticNet(alpha = elasticCV.alpha_,l1_ratio=0.5)\nelasticnet_reg.fit(X_train, y_train)\nelasticnet_reg.score(X_test, y_test)","7f0f6479":"model = LinearRegression()\nmse = cross_val_score(model,X,y,scoring='neg_mean_squared_error', cv=5)\nmean_mse = np.mean(mse)\nprint(mse)\nprint(mean_mse)","c0f505b6":"mean_absolute_error(y_test, y_pred)","14b3e69f":"mean_squared_error(y_test, y_pred)","d0c36d84":"# root mean squared error\nnp.sqrt(mean_squared_error(y_test, y_pred))","9ca37725":"# Thank you and don't forget to give a like\/star if it was worth your time","c198ed71":"### i'm gonna use this\n\nhttps:\/\/towardsdatascience.com\/an-easy-tool-to-correctly-transform-non-linear-data-for-linear-regression-5fbe7f7bfe2f\n\n### wait... we can't, its for bulging data which we dont have but worth noting... check above link for more info","25282cc1":"## Feature selection","1b337693":"### Now, lets use this to encode our categorical data","ea74628f":"### Now, their is a thing that when it comes to linear regression we assume the error rate is normally distributed...","041f09f2":"#### lets try to find a relationship between the independent categorical variables and price","b1f07de7":"### Good! this is actually helpful, but keep in mind in the real world such relationships can be hard to get... like if you have less data it will be hard to understand where it falls (eg: FL in clarity column). \n\n### The way data is collected is also important... not all samples are good predictor of the population distribution, some can be really bad ","441686e9":"## EDA","690a6b39":"## Performance metrics","223d2875":"### we'll go with carat weight, cut, color, clarity (target encoded)","0ab8c2b4":"## This is a regression problem, im thinking of using linear regression for this dataset, ill try to get a little in depth more... maybe use ridge,lasso and elastic as well","40628beb":"### Not bad...","5c6bf698":"## Feature engineering","79e2268e":"### Imports","f6cce7f4":"### Internet says:\n#### 1.boxplots\n#### 2.Jittered plots (no idea what it is)\n#### 3.violin plots ( finally!, a usecase)","fecbcbb9":"#### These metrics would be good for comparing models but simple linear regression itself is performing the best in our case so we'll compare it with elastic net","80ece553":"### looks like carat weight, cut, color, clarity matter are top most important"}}