{"cell_type":{"e7b9e3c2":"code","c9fece72":"code","183dcd7d":"code","67e8c805":"code","da96984c":"code","93b3aa42":"code","f14c93a4":"code","affe3d46":"code","cba68d56":"code","78fc4a27":"code","3be12615":"code","4d886618":"code","4a28a539":"code","bf727ef0":"code","2bbfa1aa":"code","8eb964bf":"code","1c58cecd":"code","0a645f6c":"code","6d920282":"code","17118ef6":"code","33a46cc3":"code","6f3db888":"markdown","0d0ee6ba":"markdown","bac78ea0":"markdown","38b5b9ba":"markdown","debbafca":"markdown"},"source":{"e7b9e3c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport torch\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nimport matplotlib.patches as patches\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9fece72":"\ndef generate_box(obj):\n    \n    xmin = int(obj.find('xmin').text)\n    ymin = int(obj.find('ymin').text)\n    xmax = int(obj.find('xmax').text)\n    ymax = int(obj.find('ymax').text)\n    \n    return [xmin, ymin, xmax, ymax]\n\ndef generate_label(obj):\n    if obj.find('name').text == \"with_mask\":\n        return 1\n    elif obj.find('name').text == \"mask_weared_incorrect\":\n        return 2\n    return 0\n\ndef generate_target(image_id, file): \n    with open(file) as f:\n        data = f.read()\n        soup = BeautifulSoup(data, 'xml')\n        objects = soup.find_all('object')\n\n        num_objs = len(objects)\n\n        # Bounding boxes for objects\n        # In coco format, bbox = [xmin, ymin, width, height]\n        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n        boxes = []\n        labels = []\n        for i in objects:\n            boxes.append(generate_box(i))\n            labels.append(generate_label(i))\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # Labels (In my case, I only one class: target class or background)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        # Tensorise img_id\n        img_id = torch.tensor([image_id])\n        # Annotation is in dictionary format\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = img_id\n        \n        return target\n","183dcd7d":"imgs = list(sorted(os.listdir(\"\/kaggle\/input\/face-mask-detection\/images\/\")))","67e8c805":"labels = list(sorted(os.listdir(\"\/kaggle\/input\/face-mask-detection\/annotations\/\")))","da96984c":"class MaskDataset(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(\"\/kaggle\/input\/face-mask-detection\/images\/\")))\n#         self.labels = list(sorted(os.listdir(\"\/kaggle\/input\/face-mask-detection\/annotations\/\")))\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        file_image = 'maksssksksss'+ str(idx) + '.png'\n        file_label = 'maksssksksss'+ str(idx) + '.xml'\n        img_path = os.path.join(\"\/kaggle\/input\/face-mask-detection\/images\/\", file_image)\n        label_path = os.path.join(\"\/kaggle\/input\/face-mask-detection\/annotations\/\", file_label)\n        img = Image.open(img_path).convert(\"RGB\")\n        #Generate Label\n        target = generate_target(idx, label_path)\n        \n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","93b3aa42":"data_transform = transforms.Compose([\n        transforms.ToTensor(), \n    ])","f14c93a4":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndataset = MaskDataset(data_transform)\ndata_loader = torch.utils.data.DataLoader(\n dataset, batch_size=4, collate_fn=collate_fn)","affe3d46":"torch.cuda.is_available()","cba68d56":"def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","78fc4a27":"model = get_model_instance_segmentation(3)","3be12615":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nfor imgs, annotations in data_loader:\n    imgs = list(img.to(device) for img in imgs)\n    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n    print(annotations)\n    break","4d886618":"\nnum_epochs = 25\nmodel.to(device)\n    \n# parameters\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n\nlen_dataloader = len(data_loader)\n\nfor epoch in range(num_epochs):\n    model.train()\n    i = 0    \n    epoch_loss = 0\n    for imgs, annotations in data_loader:\n        i += 1\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        loss_dict = model([imgs[0]], [annotations[0]])\n        losses = sum(loss for loss in loss_dict.values())        \n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step() \n#         print(f'Iteration: {i}\/{len_dataloader}, Loss: {losses}')\n        epoch_loss += losses\n    print(epoch_loss)\n","4a28a539":"for imgs, annotations in data_loader:\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        break","bf727ef0":"model.eval()\npreds = model(imgs)\npreds","2bbfa1aa":"def plot_image(img_tensor, annotation):\n    \n    fig,ax = plt.subplots(1)\n    img = img_tensor.cpu().data\n\n    # Display the image\n    ax.imshow(img.permute(1, 2, 0))\n    \n    for box in annotation[\"boxes\"]:\n        xmin, ymin, xmax, ymax = box\n\n        # Create a Rectangle patch\n        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n\n    plt.show()","8eb964bf":"print(\"Prediction\")\nplot_image(imgs[2], preds[2])\nprint(\"Target\")\nplot_image(imgs[2], annotations[2])","1c58cecd":"torch.save(model.state_dict(),'model.pt')","0a645f6c":"model2 = get_model_instance_segmentation(3)","6d920282":"model2.load_state_dict(torch.load('model.pt'))\nmodel2.eval()\nmodel2.to(device)","17118ef6":"pred2 = model2(imgs)","33a46cc3":"print(\"Predict with loaded model\")\nplot_image(imgs[3], pred2[3])","6f3db888":"# Function to plot image","0d0ee6ba":"# Save Model","bac78ea0":"# Load Model","38b5b9ba":"# Train Model","debbafca":"# Model"}}