{"cell_type":{"d1eda00a":"code","3f7efa51":"code","30e7d672":"code","51875a3a":"code","37645b0f":"code","6ab83691":"code","c21392a3":"code","b3385d95":"code","de55e1c8":"code","841fab2e":"code","9e5f40d6":"code","7a8fe492":"code","32d07455":"code","03ff65c1":"code","e2dc34d4":"code","c497ed15":"markdown","9553b085":"markdown","1dd742a0":"markdown","ccd39f9b":"markdown","a7a99f2f":"markdown","a68e8c3c":"markdown"},"source":{"d1eda00a":"import numpy as np\nimport pandas as pd\nimport pickle\nfrom torch.utils.data import Dataset\nimport time\nimport torch\nfrom fastai.text.all import *\nfrom sklearn.preprocessing import MinMaxScaler","3f7efa51":"def load_pickle(filename):\n    infile = open(filename,'rb')\n    obj = pickle.load(infile)\n    infile.close()\n    return obj\n\ndef save_pickle(obj, filename):\n    outfile = open(filename,'wb')\n    pickle.dump(obj, outfile)\n    outfile.close()\n    \n    \n    \ndef comp_metric(xhat, yhat, fhat, x, y, f):\n    intermediate = torch.sqrt(torch.pow(xhat-x, 2) + torch.pow(yhat-y, 2)) + 15 * torch.abs(fhat-f)\n#     intermediate = intermediate * distrib\n    return intermediate.sum()\/xhat.shape[0]\/xhat.shape[1]\n\ndef comp_metric2(xhat, yhat, fhat, x, y, f):\n    intermediate = torch.sqrt(torch.pow(xhat-x, 2) + torch.pow(yhat-y, 2)) + 15 * torch.abs(fhat-f)\n    return intermediate.sum()\/xhat.shape[0]\n\n\ndef loss_fn(outputs, labels):\n    xhat = outputs[:, :, 1]\n    yhat = outputs[:, :, 2]\n    fhat = outputs[:, :, 0]\n    \n    x = labels[:, :, 1]\n    y = labels[:, :, 2]\n    f = labels[:, :, 0]\n\n    return comp_metric(xhat, yhat, fhat, x, y, f)\n\ndef metric_fn(outputs, labels):\n    xhat = outputs[:, -1, 1]\n    yhat = outputs[:, -1, 2]\n    fhat = outputs[:, -1, 0]\n\n    \n    x = labels[:, -1, 1]\n    y = labels[:, -1, 2]\n    f = labels[:, -1, 0]\n\n    return comp_metric2(xhat, yhat, fhat, x, y, f)\n\n\n\nclass StopAt(Callback):\n    \"\"\"stops training after epoch {stop}. when stop is 1, it will train for two cycles (0 and 1)\"\"\"\n    def __init__(self, stop):\n        self.stop = stop\n        super().__init__()\n\n    def before_epoch(self):\n        if self.epoch == self.stop + 1:\n          raise CancelFitException()","30e7d672":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using {} device\".format(device))","51875a3a":"%%time\ndata = load_pickle('..\/input\/indoor-location-rnn-data-v2\/data.pickle')\ntest_data = load_pickle('..\/input\/indoor-location-rnn-test-data-v2\/test-data.pickle')\npath_data = load_pickle('..\/input\/indoor-location-rnn-data-v2\/path_data.pickle')","37645b0f":"%%time\n# normalize X\nrssi_list = []\nfor key in data:\n    rssis = data[key][0][:, -1]\n    rssi_list.append(rssis)\nrssi_list = np.hstack(rssi_list).reshape(-1, 1)\n\nscaler = MinMaxScaler()\nscaler.fit(rssi_list)\n\nfor key in data:\n    X = data[key][0].astype('float32')\n    X[:, -1:] = scaler.transform(X[:, -1:])\n    data[key] = (X, data[key][1].astype('float32'))\n    \nfor key in test_data:\n    X = test_data[key][0].astype('float32')\n    X[:, -1:] = scaler.transform(X[:, -1:])\n    test_data[key] = (X, test_data[key][1].astype('float32'))","6ab83691":"# seq_lens = []\n# for key in train_data:\n#     seq_lens.append(train_data[key][0].shape[0])\n# pd.Series(seq_lens).describe()\nseq_len = 174","c21392a3":"# bssids = set()\n# for key in train_data:\n#     X = train_data[key][0]\n#     for wifi in X:\n#         bssids.add(wifi[1])\n# max(bssids)\nn_bssids = 63114","b3385d95":"def train_test_split(state=0, mod=10):\n    assert state < mod\n    train_data = {}\n    val_data = {}\n    for path in path_data:\n        if path % mod == state:\n            for key in path_data[path]:\n                val_data[len(val_data)] = data[key]\n        else:\n            for key in path_data[path]:\n                train_data[len(train_data)] = data[key]\n    return train_data, val_data","de55e1c8":"# sample_data = {} # data of first building\n# for key in data:\n#     if key == 26507:\n#         break\n#     sample_data[key] = data[key]\n    \n# train_data, val_data = train_test_split(sample_data)","841fab2e":"class MyDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.padding_len = seq_len\n        self.pad = np.array([[0, n_bssids, -100]] * self.padding_len, dtype='float32')\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        X, Y = self.data[idx]\n        original_len = X.shape[0]\n        if original_len == self.padding_len:\n            pass\n        elif original_len > self.padding_len:\n            X = X[:self.padding_len]\n        else:\n            X = np.vstack([X, self.pad[original_len:]])\n#         X = np.pad(X, ((0, self.padding_len - original_len), (0, 0)), constant_values=((0, self.padding_value), (0, 0)))\n#         X[-1][-1] = original_len\n        X = X[:, 1:]\n        Y = np.repeat(Y.reshape(1,3), self.padding_len, axis=0)\n#         Y = np.array([Y] * self.padding_len)\n#         X = torch.tensor(X, device=device)\n#         Y = torch.tensor(Y, device=device)\n        return (X, Y)\n    \n    \n    \nclass EmptyDataset(Dataset):\n    def __init__(self):\n        pass\n\n    def __len__(self):\n        return 0\n\n    def __getitem__(self, idx):\n        return None","9e5f40d6":"# dset = MyDataset(data)\n# dl = DataLoader(dset, batch_size=64, shuffle=True, device=device)","7a8fe492":"# %%time\n# for i in range(len(dset)):\n#     dset[i]","32d07455":"# pure 391ms\n# padding 44.3s\n# padding with values 58.8s\n# padding by stacking 7.9s\n# padding both X and Y 12.8s","03ff65c1":"class Model(Module):\n    def __init__(self, vocab_sz, embed_dim, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, embed_dim)\n        self.rnn = nn.LSTM(embed_dim + 1, n_hidden, n_layers, batch_first=True, dropout=p)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, 3)\n\n#     def forward(self, x):\n#         raw, _ = self.rnn(torch.cat((self.i_h(x[:, :, 0].long()), x[:, :, 1].unsqueeze(2)), 2))\n#         out = self.drop(raw)\n#         return self.h_o(out)#,raw,out\n    \n    def forward(self, x):\n#         max_size = int(x[:, -1, -1].max())\n#         print(max_size)\n#         x = x[:, :max_size]\n        raw, _ = self.rnn(torch.cat((self.drop(self.i_h(x[:, :, 0].long())), x[:, :, 1].unsqueeze(2)), 2))\n        return self.h_o(self.drop(raw))","e2dc34d4":"%%time\n# hyperparameters\nbatch_size = 256\nembed_dim =  128\nhidden_size = (embed_dim + 1 + 3) \/\/ 2\nn_layers = 2\np = 0\nlr = 0.015625\ncycle = 32\nstop = 27\n\nfor i in range(10):\n    # dataloader\n    train, val = train_test_split(i)\n    dset_train = MyDataset(train)\n    dl_train = DataLoader(dset_train, batch_size=batch_size, shuffle=True)\n    dset_val = MyDataset(val)\n    dl_val = DataLoader(dset_val, batch_size=batch_size)\n    dls = DataLoaders(dl_train, dl_val, device=device)\n\n    # fit\n    print('batch_size, embed_dim, hidden_size, n_layers, p, lr, cycle, stop')\n    print(batch_size, embed_dim, hidden_size, n_layers, p, lr, cycle, stop)\n    learn = Learner(dls, Model(n_bssids + 1, embed_dim, hidden_size, n_layers, p).to(device), loss_func=loss_fn, metrics=metric_fn)\n    learn.fit_one_cycle(cycle, lr, cbs=StopAt(stop))\n    \n    # predict\n    dset = MyDataset(test_data)\n    test_dl = DataLoader(dset, batch_size=batch_size, device=device)\n\n    preds, _ = learn.get_preds(dl=test_dl)\n    pred_df = pd.DataFrame(preds[:, -1])\n    pred_df.to_csv(f'predictions_fold{i}.csv', index=False)","c497ed15":"# Model","9553b085":"# train and predict","1dd742a0":"# useful infos","ccd39f9b":"# ------------------------------------------------","a7a99f2f":"# data","a68e8c3c":"# functions"}}