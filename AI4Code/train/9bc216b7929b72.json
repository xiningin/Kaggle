{"cell_type":{"b09966a8":"code","0a300629":"code","6f62117f":"code","aaed4989":"code","e60b62d0":"code","61ee3a06":"code","f413bc0c":"code","1fc56bc4":"code","5a01ed0f":"code","5ae12997":"code","04a2d907":"code","017fa5ba":"code","3db9f41b":"code","7beea312":"code","e96ee60a":"code","5aeb157b":"code","ad82d18d":"code","0e27bf5c":"code","d5448693":"code","5d5d2a02":"code","e7d4079b":"code","8750cb53":"code","2a284e84":"code","a6f070f4":"code","fb6d4b16":"code","045d98ba":"code","ed4a910f":"code","792ae5ff":"code","532f6656":"code","f2e97ace":"code","216d9f12":"code","1ad40c24":"code","2df87d9f":"markdown","2e60c4d1":"markdown","5093ce1c":"markdown","19146b33":"markdown","4275904d":"markdown","b53712f2":"markdown","07cce501":"markdown","3eae461c":"markdown","0de0eeee":"markdown","25f7227d":"markdown","010a8ee3":"markdown","9ba5fedf":"markdown","a75bc6e5":"markdown","e36ed5be":"markdown","9befcfc3":"markdown","40914156":"markdown","4a02e629":"markdown","b4b9cc6a":"markdown","71d37c18":"markdown","b21fa3d2":"markdown"},"source":{"b09966a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n!pip3 install catboost\n!pip3 install xgboost \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a300629":"df = pd.read_csv('\/kaggle\/input\/hackerearth-employee-attrition\/Train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/hackerearth-employee-attrition\/Test.csv')","6f62117f":"df_test.head()","aaed4989":"import seaborn as sns\nsns.distplot(df['Attrition_rate'])","e60b62d0":"print('skew',df['Attrition_rate'].skew())\nprint('kurtosis',df['Attrition_rate'].kurtosis())","61ee3a06":"import plotly.express as px\nfig = px.pie(values=df['Gender'].value_counts(), names=df['Gender'].value_counts().index, title='Gernes')\nfig.show()","f413bc0c":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=1, cols=5, specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}]])\n\ndf_aux = df[['Relationship_Status', 'Hometown', 'Unit', 'Decision_skill_possess', 'Compensation_and_Benefits']]\nk = 1\nfor column in df_aux.columns: \n    fig.add_bar(y=list(df_aux[column].value_counts()), \n                            x=df_aux[column].value_counts().index, name=column, row=1, col=k)\n    k+=1\nfig.show()","1fc56bc4":"import plotly.figure_factory as ff\nfig = make_subplots(rows=1, cols=5)\ndf_num = df[['Time_since_promotion', 'growth_rate', 'Travel_Rate', 'Post_Level', 'Education_Level']]\n\nfig1 = ff.create_distplot([df_num['Time_since_promotion']], ['Time_since_promotion'])\nfig2 = ff.create_distplot([df_num['growth_rate']], ['growth_rate'])\nfig3 =  ff.create_distplot([df_num['Travel_Rate']], ['Travel_Rate'])\nfig4 =  ff.create_distplot([df_num['Post_Level']], ['Post_Level'])\nfig5 =  ff.create_distplot([df_num['Education_Level']], ['Education_Level'])\n\nfig.add_trace(go.Histogram(fig1['data'][0], marker_color='blue'), row=1, col=1)\nfig.add_trace(go.Histogram(fig2['data'][0],marker_color='red'), row=1, col=2)\nfig.add_trace(go.Histogram(fig3['data'][0], marker_color='green'), row=1, col=3)\nfig.add_trace(go.Histogram(fig4['data'][0],marker_color='yellow'), row=1, col=4)\nfig.add_trace(go.Histogram(fig5['data'][0],marker_color='purple'), row=1, col=5)\n\n\nfig.show()","5a01ed0f":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 20.7,5.27\ndf_aux = df[['Relationship_Status', 'Hometown', 'Unit', 'Decision_skill_possess', 'Compensation_and_Benefits', 'Attrition_rate']]\nf, axes = plt.subplots(1, 5)\nk = 0\nfor column in df_aux.columns[:-1]:\n    g = sns.boxplot(x=column, y='Attrition_rate',\n                    data=df_aux, ax=axes[k])\n    g.set_xticklabels(labels=g.get_xticklabels(),rotation=90)\n    k +=1 \ng","5ae12997":"fig = px.bar(x=df.isna().sum().index, y=df.isna().sum())\nfig.show()","04a2d907":"ax = sns.heatmap(df.corr(), annot=True, fmt=\".4f\")","017fa5ba":"import math\nfrom scipy.interpolate import interp1d\ndf_age = df[~df['Time_of_service'].isna()]\ndf_age_ = df_age[~df_age['Age'].isna()]\ndf_age_ = df_age_.sort_values('Time_of_service',  ascending=False)\ninterpolate_poly = interp1d(kind='linear', x=list(df_age_['Time_of_service']), y=list(df_age_['Age']))\nages =[]\nfor age, time_service in zip(df_age['Age'], df_age['Time_of_service']):\n    if math.isnan(float(age)):\n        age_interpolated = interpolate_poly(time_service)\n        ages.append(age_interpolated)\n    else:\n        ages.append(int(age))\ndf_age['new_age'] = ages","3db9f41b":"df_age = df_age.sort_values('Time_of_service', ascending=False)\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(df_age['Time_of_service']), \n                         y=list(df_age['Age']), mode='markers', name='Original Age'))\n\ndf_age2 = df_age[df_age['Age'].isna()]\nfig.add_trace(go.Scatter(x=list(df_age2['Time_of_service']), \n                         y=list(df_age2['new_age']), mode='markers', marker_color='red', name='Interpolated Age'))\n\nfig.show()","7beea312":"pay = []\nwork = []\npp = df_age['Pay_Scale'].mode()\nww = df_age['Work_Life_balance'].mode()\nfor p, w in zip(df_age['Pay_Scale'], df_age['Work_Life_balance']):\n    if math.isnan(float(p)):\n        pay.append(pp)\n    else:\n        pay.append(p)\n    if math.isnan(float(w)):\n        work.append(ww)\n    else:\n        work.append(w)\n\ndf_age['Pay_Scale'] = pay\ndf_age['Work_Life_balance'] = work","e96ee60a":"df_age.info()","5aeb157b":"rel_status = pd.get_dummies(df_age['Relationship_Status'])\nhometown = pd.get_dummies(df_age['Hometown'])\nunit = pd.get_dummies(df_age['Unit'])\ndecision = pd.get_dummies(df_age['Decision_skill_possess'])\ncompenssion = pd.get_dummies(df_age['Compensation_and_Benefits'])\nto_work = df_age[['Education_Level', 'Time_of_service', 'Time_since_promotion', 'growth_rate', 'Travel_Rate', 'Post_Level', 'Pay_Scale', 'Work_Life_balance', 'VAR1', 'VAR3', 'VAR5', 'VAR6','VAR7', 'Attrition_rate', 'new_age']]\ndf_to_modelling = pd.concat([to_work, compenssion, decision, unit, hometown, rel_status], axis=1)","ad82d18d":"y = df_to_modelling['Attrition_rate']\ndf_to_modelling = df_to_modelling.drop(['Attrition_rate'], axis=1)\ndf_to_modelling['Pay_Scale'] = df_to_modelling['Pay_Scale'].astype(float)\ndf_to_modelling['new_age'] = df_to_modelling['new_age'].astype(int)\ndf_to_modelling['Work_Life_balance'] = df_to_modelling['Work_Life_balance'].astype(float)","0e27bf5c":"def plot_predict(pred, true):\n    indexs = []\n    for i in range(len(pred)):\n        indexs.append(i)\n        \n\n    fig = go.Figure()\n\n    fig.add_trace(go.Line(\n        x=indexs,\n        y=pred,\n        name=\"Predict\"\n    ))\n\n    fig.add_trace(go.Line(\n        x=indexs,\n        y=true,\n        name=\"Test\"\n    ))\n\n    fig.show()","d5448693":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df_to_modelling, y, random_state=42\n)\n","5d5d2a02":"param_random_tree = {\"max_depth\": [None],\n              \"max_features\": [10,15, 20, 30, 43],\n              \"min_samples_split\": [2, 3, 10,15],\n              \"min_samples_leaf\": [1, 3, 10,15],\n              \"n_estimators\" :[50,100,200,300,500]}\n\nrandom = RandomForestRegressor(random_state=42)\nclf = GridSearchCV(random, param_random_tree, cv=5,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf.fit(X_train, y_train)\nprint(clf.best_estimator_)\nprint(clf.best_score_)\n# (max_features=10, min_samples_leaf=15, n_estimators=500, random_state=42)\n","e7d4079b":"scores = {}\nrandom = RandomForestRegressor(max_features=10, min_samples_leaf=15, n_estimators=500, random_state=42)\nmodel = random.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['RF'] = score","8750cb53":"import xgboost\nxgboost_params = {'max_features': [10,15, 20, 30],\n                  'n_estimators' :[25,50,100],\n                   'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n                  'gamma':[0.5, 0.1, 1, 10],\n                  'max_depth':[5, 10, 15]}\n\nxgb = xgboost.XGBRegressor(random_state=42)\nclf_xgb = GridSearchCV(xgb, xgboost_params, cv=5,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf_xgb.fit(df_to_modelling, y)\nprint(clf_xgb.best_estimator_)\nprint(clf_xgb.best_score_)\n\"\"\"\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=1, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=5, max_features=10,\n             min_child_weight=1, missing=nan, monotone_constraints='()',\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=42,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\"\"\"\n\n","2a284e84":"import xgboost\nxgb = xgboost.XGBRegressor(gamma=1, random_state=42, max_depth=5, max_features=10,learning_rate=0.1, n_estimators=100)\nmodel = xgb.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['XGB'] = score","a6f070f4":"import lightgbm as lgb\nlightgbm_params ={'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1],\n                  'n_estimators':[10,20, 50, 100],\n                 'max_depth':[4, 6, 10, 15, 20, 50]}\ngbm = lgb.LGBMRegressor(random_state = 42)\nclf_gbm = GridSearchCV(gbm, lightgbm_params, cv=5,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf_gbm.fit(df_to_modelling, y)\nprint(clf_gbm.best_estimator_)\nprint(clf_gbm.best_score_)\n# (learning_rate=0.001, max_depth=6, n_estimators=50, random_state=42)","fb6d4b16":"gbm = lgb.LGBMRegressor(random_state = 42, learning_rate=0.001, max_depth=6, n_estimators=50)\nmodel = gbm.fit(df_to_modelling, y)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['LGBM'] = score","045d98ba":"from sklearn.ensemble import AdaBoostRegressor\nadam_boosting_params = {'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1,1],\n                        'n_estimators':[10,20, 50, 100]}\nada = AdaBoostRegressor(random_state=42)\nclf_ada = GridSearchCV(ada, adam_boosting_params, cv=5,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf_ada.fit(df_to_modelling, y)\nprint(clf_ada.best_estimator_)\nprint(clf_ada.best_score_)\n# (learning_rate=0.0001, n_estimators=100, random_state=42)","ed4a910f":"ada = AdaBoostRegressor(random_state=42, learning_rate=0.0001, n_estimators=100)\nmodel = ada.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['ADA'] = score","792ae5ff":"from sklearn.svm import LinearSVR\n\nsvr_params = {'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100]}\nsvr = LinearSVR(random_state=42)\nclf_svr = GridSearchCV(svr, svr_params, cv=5, scoring='neg_mean_squared_error', n_jobs=4, verbose=1)\nclf_svr.fit(df_to_modelling, y)\nprint(clf_svr.best_estimator_)\nprint(clf_svr.best_score_)\n# (C=0.001, random_state=42)","532f6656":"lvr = LinearSVR(C=0.001, random_state=42)\nmodel = svr.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['SVR'] = score","f2e97ace":"estimators  =  [\n    ('rf', RandomForestRegressor(max_features=10, min_samples_leaf=15, n_estimators=500, random_state=42)),\n    ('svr', LinearSVR(C=0.001, random_state=42)),\n    ('ada', AdaBoostRegressor(random_state=42, learning_rate=0.0001, n_estimators=100)),\n    ('lgb', lgb.LGBMRegressor(random_state = 42, learning_rate=0.001, max_depth=6, n_estimators=50)),\n    \n]\nclf = StackingRegressor(\n    estimators=estimators, final_estimator=xgboost.XGBRegressor(gamma=1, random_state=42, max_depth=5, max_features=10,learning_rate=0.1, n_estimators=100)\n)\nmodel = clf.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['STACK'] = score","216d9f12":"from sklearn.ensemble import VotingRegressor\nestimators  =  [\n    ('rf', RandomForestRegressor(max_features=10, min_samples_leaf=15, n_estimators=500, random_state=42)),\n    ('svr', LinearSVR(C=0.001, random_state=42)),\n    ('ada', AdaBoostRegressor(random_state=42, learning_rate=0.0001, n_estimators=100)),\n    ('lgb', lgb.LGBMRegressor(random_state = 42, learning_rate=0.001, max_depth=6, n_estimators=50)),\n    ('xgb', xgboost.XGBRegressor(gamma=1, random_state=42, max_depth=5, max_features=10,learning_rate=0.1, n_estimators=100))\n]\nclf = VotingRegressor(\n    estimators=estimators\n)\nmodel = clf.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['VOLTING'] = score","1ad40c24":"result = pd.DataFrame([])\nresult['model'] = list(scores.keys())\nresult['score'] = list(scores.values())\nresult = result.sort_values(['score'], ascending=False)\nresult.head(10)","2df87d9f":"* So outlier maybe is a feature of data, upper limit in all the above charts is a value next to 0.5 at the moment nothing will be done about handling this data.","2e60c4d1":"## LinearSVR","5093ce1c":"<font size=\"+3\" color=\"black\"><b>2 - EDA<\/b><\/font><br><a id=\"2\"><\/a>\n\n* To start understanding our data will start plot the target variable distribution","19146b33":"<font size=\"+3\" color=\"black\"><b>5 - Handling missing data<\/b><\/font><br><a id=\"5\"><\/a>","4275904d":"* Give your feedback to improve this kernel :)","b53712f2":"## Random Forest","07cce501":"* In chart above is possible to see handling over missing ages, linear interpolation apparently keep the structure of data not adding no value that is clearly a bias\n\n* So know whats means the features var2 and var4 i chose not to insert in the models for now\n\n* Pay Scale and Work Life balance will replace by mode","3eae461c":"* As matrix showed, there are high correlation among time_of_service and age, so maybe apply some interpolation can be help to impute the missing values.","0de0eeee":"<font size=\"+3\" color=\"black\"><b>4 - Outlier Detection<\/b><\/font><br><a id=\"4\"><\/a>\n","25f7227d":"## LightGBM","010a8ee3":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">&nbsp;Summary Table:<\/h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Introduction<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"profile\">2. EDA<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"profile\">3. Univariante Analysis<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"profile\">4. Outlier Detection<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"profile\">5. Handling Missing Data<span class=\"badge badge-primary badge-pill\">5<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"profile\">6. Modelling<span class=\"badge badge-primary badge-pill\">6<\/span><\/a>\n<\/div>","9ba5fedf":"<font size=\"+3\" color=\"black\"><b>3 - Univariante Analysis<\/b><\/font><br><a id=\"3\"><\/a>\n\n* To better undestanding about data we will explore some of features presents in the data set","a75bc6e5":"* The most part of the people in data set are women","e36ed5be":"<font size=\"+3\" color=\"black\"><b>1 - Introduction<\/b><\/font><br><a id=\"1\"><\/a>\n\n* This kernel will present a simple EDA over data, and in the end usign some models to predict de employeer attrintion rate","9befcfc3":"* Var2 and Var4 are the features with the most missing values, and in the dataset description does not have detailed information on what these variables can be\n\n* Age has a considerable value of missing values. Time of service too\n\n* to better undestanding the relationship over each feature we will plot the correlation matrix \n\n* First to impute the data we will disregard the samples that contains missing data in time service feature","40914156":"<font size=\"+3\" color=\"black\"><b>6 - Modelling<\/b><\/font><br><a id=\"6\"><\/a>","4a02e629":"## Xgboost","b4b9cc6a":" * Exploring some of categorical features:","71d37c18":"## AdaBoost","b21fa3d2":"* post level looklike normal distribution"}}