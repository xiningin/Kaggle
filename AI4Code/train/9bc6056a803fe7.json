{"cell_type":{"ecdc30ce":"code","4e1b50e7":"code","6cfb3b9b":"code","509bba1d":"code","ed6b1a06":"code","d24c9d8a":"code","c54837bf":"code","a2204df8":"code","ae696c7f":"code","9da53bc1":"code","b5a735fa":"code","225f40e4":"code","491af532":"code","08821f61":"code","f1d7c317":"code","1a8215c5":"code","ef312c61":"code","2244bf55":"code","7468039d":"code","176ac063":"code","65b121fa":"code","b4aa6acf":"code","9a482055":"code","7ba40f99":"code","44e933c5":"code","1c74a1f4":"code","7553fd0c":"code","0fab5ba9":"code","213f07b6":"code","d5b3374a":"code","b8b64421":"code","9020264d":"code","c47ffc23":"markdown","30718310":"markdown","8ee503c5":"markdown","c6811777":"markdown","22afd617":"markdown","b0e14f5b":"markdown","492d909c":"markdown","c9174629":"markdown","2fd9af2b":"markdown","6905300e":"markdown","44c1c300":"markdown","92944d05":"markdown","e3663eba":"markdown","faa2235f":"markdown","fa9102eb":"markdown","4896d60c":"markdown","9813f8d3":"markdown","267512d6":"markdown","8c8d978c":"markdown","3a74cf03":"markdown","35438c9e":"markdown","43a08b9a":"markdown","c07ac7b5":"markdown","397a3333":"markdown","abe1511e":"markdown","2619a0b6":"markdown","0629a437":"markdown","cb8c10e4":"markdown","dc81ad61":"markdown","63daafb5":"markdown","920ecd77":"markdown","40488229":"markdown","45250403":"markdown","e4b99fa2":"markdown","4a93114f":"markdown","b5dd2102":"markdown","f1c7b3ff":"markdown","0c3e6bb8":"markdown"},"source":{"ecdc30ce":"# import libraries\nimport pandas as pd\nimport numpy as np\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nimport re\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport math\n\nimport warnings\nwarnings.filterwarnings('ignore')","4e1b50e7":"# Read in the train_rev1 datafile downloaded from kaggle\ndf = pd.read_csv('..\/input\/Train_rev1.csv')\ndf.head(2)","6cfb3b9b":"print(\"Data Shape:\", df.shape)","509bba1d":"# randomly sample 10000 rows from the data\nimport random\nrandom.seed(1)\nindices = df.index.values.tolist()\n\nrandom_10000 = random.sample(indices, 10000)\n\nrandom_10000[:5]","ed6b1a06":"# subset the imported data on the selected 2500 indices\ntrain = df.loc[random_10000, :]\ntrain = train.reset_index(drop = True)\ntrain.head(2)","d24c9d8a":"train.columns.values","c54837bf":"# some problems with the way FullDescription has been encoded\ndef convert_utf8(s):\n    return str(s)\n\ntrain['FullDescription'] = train['FullDescription'].map(convert_utf8)","a2204df8":"train.loc[2, 'FullDescription']","ae696c7f":"# Remove the urls first - Anything that has .com, .co.uk or www. is a url!\ndef remove_urls(s):\n    s = re.sub('[^\\s]*.com[^\\s]*', \"\", s)\n    s = re.sub('[^\\s]*www.[^\\s]*', \"\", s)\n    s = re.sub('[^\\s]*.co.uk[^\\s]*', \"\", s)\n    return s\n\ntrain['Clean_Full_Descriptions'] = train['FullDescription'].map(remove_urls)","9da53bc1":"# Remove the star_words\ndef remove_star_words(s):\n    return re.sub('[^\\s]*[\\*]+[^\\s]*', \"\", s)\n\ntrain['Clean_Full_Descriptions'] = train['Clean_Full_Descriptions'].map(remove_star_words)","b5a735fa":"def remove_nums(s):\n    return re.sub('[^\\s]*[0-9]+[^\\s]*', \"\", s)\n\ntrain['Clean_Full_Descriptions'] = train['Clean_Full_Descriptions'].map(remove_nums)","225f40e4":"# Remove the punctuations\nfrom string import punctuation\n\ndef remove_punctuation(s):\n    global punctuation\n    for p in punctuation:\n        s = s.replace(p, '')\n    return s\n\ntrain['Clean_Full_Descriptions'] = train['Clean_Full_Descriptions'].map(remove_punctuation)","491af532":"# Convert to lower case\ntrain['Clean_Full_Descriptions'] = train['Clean_Full_Descriptions'].map(lambda x: x.lower())","08821f61":"print(nltk.pos_tag(word_tokenize(\"I love riding my motorcycle\")))","f1d7c317":"# define a function for parts of speech tagging\n# make a corpus of all the words in the job description\ncorpus = \" \".join(train['Clean_Full_Descriptions'].tolist())\n\n# This is the NLTK function that breaks a string down to its tokens\ntokens = word_tokenize(corpus)\n\n# Get the parts of speech tag for all words\nanswer = nltk.pos_tag(tokens)\nanswer_pos = [a[1] for a in answer]\n\n# print a value count for the parts of speech\nall_pos = pd.Series(answer_pos)\nall_pos.value_counts().head()","1a8215c5":"# store english stopwords in a list\nfrom nltk.corpus import stopwords\nen_stopwords = stopwords.words('english')\n\n# define a function to remove stopwords from descriptions\ndef remove_stopwords(s):\n    global en_stopwords\n    s = word_tokenize(s)\n    s = \" \".join([w for w in s if w not in en_stopwords])\n    return s\n\n# Create a new column of descriptions with no stopwords\ntrain['Clean_Full_Descriptions_no_stop'] = train['Clean_Full_Descriptions'].map(remove_stopwords)\n\n# make a corpus of all the words in the job description\ncorpus = \" \".join(train['Clean_Full_Descriptions_no_stop'].tolist())\n\n# This is the NLTK function that breaks a string down to its tokens\ntokens = word_tokenize(corpus)\n\nanswer = nltk.pos_tag(tokens)\nanswer_pos = [a[1] for a in answer]\n\nall_pos = pd.Series(answer_pos)\nall_pos.value_counts().head()","ef312c61":"# prepare corpus from the descriptions that still have stopwords\ncorpus = \" \".join(train['Clean_Full_Descriptions'].tolist())\n\n#tokenize words\ntokenized_corpus = nltk.word_tokenize(corpus)\nfd = nltk.FreqDist(tokenized_corpus)\n\n# get the top words\ntop_words = []\nfor key, value in fd.items():\n    top_words.append((key, value))\n\n# sort the list by the top frequencies\ntop_words = sorted(top_words, key = lambda x:x[1], reverse = True)\n\n# keep top 100 words only\ntop_words = top_words[:100]\n\n# Keep the frequencies only from the top word series\ntop_word_series = pd.Series([w for (v,w) in top_words])\ntop_word_series[:5]\n\n# get actual ranks of these words - wherever we see same frequencies, we give same rank\nword_ranks = top_word_series.rank(method = 'min', ascending = False)","2244bf55":"# Get the value of the denominator n*x_n\ndenominator = max(word_ranks)*min(top_word_series)\n\n# Y variable is the log of word ranks and X is the word frequency divided by the denominator\n# above\nY = np.array(np.log(word_ranks))\nX = np.array(np.log(top_word_series\/denominator))\n\n# fit a linear regression to these, we dont need the intercept!\nfrom sklearn import linear_model\nreg_model = linear_model.LinearRegression(fit_intercept = False)\nreg_model.fit(Y.reshape(-1,1), X)\nprint(\"The value of theta obtained is:\",reg_model.coef_)\n\n# make a plot of actual rank obtained vs theoretical rank expected\nplt.figure(figsize = (8,5))\nplt.scatter(Y, X, label = \"Actual Rank vs Frequency\")\nplt.title('Log(Rank) vs Log(Frequency\/nx(n))')\nplt.xlabel('Log Rank')\nplt.ylabel('Log(Frequency\/nx(n))')\n\nplt.plot(reg_model.predict(X.reshape(-1,1)), X, color = 'red', label = \"Zipf's law\")\nplt.legend()","7468039d":"# import the necessary functions from the nltk library\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlmtzr = WordNetLemmatizer()\n\n# prepare corpus from the descriptions that dont have stopwords\ncorpus = \" \".join(train['Clean_Full_Descriptions_no_stop'].tolist())\n\n#tokenize words\ntokenized_corpus = nltk.word_tokenize(corpus)\n\n# lemmatize these tokens\nlemmatized_tokens = [lmtzr.lemmatize(token) for token in tokenized_corpus]\n\n# word frequencies for the lemmatized tokens\nfd = nltk.FreqDist(lemmatized_tokens)\n\n# get the top words\ntop_words = []\nfor key, value in fd.items():\n    top_words.append((key, value))\n\n# sort the list by the top frequencies\ntop_words = sorted(top_words, key = lambda x:x[1], reverse = True)\n\n# keep top 10 words only\ntop_words = top_words[:10]\n\ntop_words","176ac063":"# get the 75th percentile value of salary!\nsal_perc_75 = np.percentile(train['SalaryNormalized'], 75)\n\n# make a new target variable that captures whether salary is high (1) or low (0)\ntrain['Salary_Target'] = np.where(train['SalaryNormalized'] >= sal_perc_75, 1, 0)","65b121fa":"train.dtypes.value_counts()","b4aa6acf":"train.isnull().sum()[train.isnull().sum()>0]","9a482055":"# this gives us the categorical variables and the number of unique entries in them!\ntrain.select_dtypes('object').nunique(dropna = False)","7ba40f99":"exp_cities = ['London', 'Oxford', 'Brighton', 'Cambridge', 'Bristol', 'Portsmouth', \n              'Reading', 'Edinburgh', 'Leicester', 'York', 'Exeter']","44e933c5":"def check_city(s):\n    '''Given a Normalized Location this tells us if it is an expensive city'''\n    global exp_cities\n    answer = pd.Series(exp_cities).map(lambda x: x in s)\n    answer = min(np.sum(answer),1)\n    return answer\n\n# add the indicator as a column in the dataframe\ntrain['Exp_Location'] = train['LocationNormalized'].map(check_city)","1c74a1f4":"train['Salary_Target'].value_counts()\/len(train)","7553fd0c":"# Subset the columns required\ncolumns_required = ['ContractType', 'ContractTime', 'Company', 'Category', 'SourceName', 'Exp_Location', 'Salary_Target']\ntrain_b1 = train.loc[:, columns_required]\n\n# Convert the categorical variables to dummy variables\ntrain_b1 = pd.get_dummies(train_b1)\n\n# Lets separate the predictors from the target variable\ncolumns_selected = train_b1.columns.values.tolist()\ntarget_variable = ['Salary_Target']\n\n# predictors are all variables except for the target variable\npredictors = list(set(columns_selected) - set(target_variable))\n\n# setup the model\nfrom sklearn.naive_bayes import BernoulliNB\n\nX = np.array(train_b1.loc[:,predictors])\ny = np.array(train_b1.loc[:,target_variable[0]])\n\n# create test train splits \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)\n\nmodel = BernoulliNB()\n\n# Fit the model and predict the output on the test data\nmodel.fit(X_train, y_train)\n\n# Predicted output\npredicted = model.predict(X_test)\n\n# Accuracy\nfrom sklearn import metrics\n\nprint(\"Model Accuracy is:\", metrics.accuracy_score(y_test, predicted))\nprint(\"Area under the ROC curve:\", metrics.roc_auc_score(y_test, predicted))\nprint(\"Confusion Matrix:\\n\",metrics.confusion_matrix(y_test, predicted))","0fab5ba9":"# Lets lemmatize the job descriptions before we run the model\ndef text_lemmatizer(s):\n    '''Given a description, this lemmatizes it'''\n    tokenized_corpus = nltk.word_tokenize(s)\n    \n    # lemmatize\n    s = \" \".join([lmtzr.lemmatize(token) for token in tokenized_corpus])\n    return s\n\n# lemmatize the descriptions\ntrain['Clean_Full_Descriptions_no_stop_lemm'] = train['Clean_Full_Descriptions_no_stop'].map(text_lemmatizer)\n\n# make the X and y matrices for model fitting\nX = np.array(train.loc[:, 'Clean_Full_Descriptions_no_stop_lemm'])\ny = np.array(train.loc[:, 'Salary_Target'])\n\n# split into test and train data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)\n\n# Convert the arrays into a presence\/absence matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer()\nX_train_counts = count_vectorizer.fit_transform(X_train)\nX_test_counts = count_vectorizer.transform(X_test)\n\nfrom sklearn.naive_bayes import MultinomialNB\nnb_mult_model = MultinomialNB().fit(X_train_counts, y_train)\npredicted = nb_mult_model.predict(X_test_counts)\n\nprint(\"Model Accuracy:\", metrics.accuracy_score(y_test, predicted))\nprint(\"Area under the ROC curve:\", metrics.roc_auc_score(y_test, predicted))\nprint(\"Model Confusion Matrix:\\n\", metrics.confusion_matrix(y_test, predicted))","213f07b6":"# Calculate the frequencies of words using the TfidfTransformer\nX_train_bern = np.where(X_train_counts.todense() > 0 , 1, 0)\nX_test_bern = np.where(X_test_counts.todense() > 0, 1, 0)\n\n# Fit the model\nfrom sklearn.naive_bayes import BernoulliNB\nnb_bern_model = BernoulliNB().fit(X_train_bern, y_train)\npredicted = nb_bern_model.predict(X_test_bern)\n\n# print the accuracies\nprint(\"Model Accuracy:\", metrics.accuracy_score(y_test, predicted))\nprint(\"Area under the ROC curve:\", metrics.roc_auc_score(y_test, predicted))\nprint(\"Model Confusion Matrix:\\n\", metrics.confusion_matrix(y_test, predicted))","d5b3374a":"# extract the column names for the columns in our training dataset.\ncolumn_names = [x for (x,y) in sorted(count_vectorizer.vocabulary_.items(), key = lambda x:x[1])]\n\n# probability of high salary\np_1 = np.mean(y_train)\n\n# probability of low salary\np_0 = 1 - p_1\n\n# create an array of feature vectors\nfeature_vectors = np.array(X_train_bern)\n\n# probability of word appearance\nword_probabilities = np.mean(feature_vectors, axis = 0)\n\n# probability of seeing these words for class= 1 and class = 0 respectively\np_x_1 = np.mean(feature_vectors[y_train==1, :], axis = 0)\np_x_0 = np.mean(feature_vectors[y_train==0, :], axis = 0)\n\n# words that are good indicators of high salary (class = 1)\nhigh_indicators = p_x_1 * (np.log2(p_x_1) - np.log2(word_probabilities) - np.log2(p_1))\n\nhigh_indicators_series = pd.Series(high_indicators, index = column_names)\n\n# words that are good indicators of low salary (class = 0)\nlow_indicators = p_x_0 * (np.log2(p_x_0) - np.log2(word_probabilities) - np.log2(p_0))\n\nlow_indicators_series = pd.Series(low_indicators, index = column_names)","b8b64421":"low_indicators_series[[i for i in low_indicators_series.index if i not in en_stopwords]].sort_values(ascending = False)[:10].index","9020264d":"high_indicators_series[[i for i in high_indicators_series.index if i not in en_stopwords]].sort_values(ascending = False)[:10].index","c47ffc23":"**Id** - A unique identifier for each job ad\n\n**Title** - A freetext field supplied by the job advertiser as the Title of the job ad.  Normally this is a summary of the job title or role.\n\n**FullDescription** - The full text of the job ad as provided by the job advertiser.  Whenever we see ***s, these are values stripped from the description in order to ensure that no salary information appears within the descriptions. \n\n**LocationRaw** - The freetext location as provided by the job advertiser.\n\n**LocationNormalized** - Normalized location of the job location.\n\n**ContractType** - full_time or part_time.\n\n**ContractTime** - permanent or contract.\n\n**Company** - the name of the employer as supplied by the job advertiser.\n\n**Category** - which of 30 standard job categories this ad fits into.\n\n**SalaryRaw** - the freetext salary field in the job advert from the advertiser.\n\n**SalaryNormalised** - the annualised salary interpreted by Adzuna from the raw salary. We convert this value to a categorical variable denoting 'High salary' or 'Low Salary' and try to predict those.\n\n**SourceName** - the name of the website or advertiser where the job advert is posted. ","30718310":"### Job Descriptions\nLet's look at one job description to see how these are - ","8ee503c5":"## Introducing the Naive Bayes Algorithm\nFor prediction problems using text data, Naive Bayes algorithm is one of the better algorithms out there.  They have been demonstrated to be fast, reliable and accurate in a number of applications of NLP. The basic fundamentals for this algorithm derive from the Naive Bayes Theorem which states (mathematically) - \n$$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$$\n\nFor our problem, we have to predict a class given a vector of predictors. The above formula becomes - \n\n$$P(Class|Vector) = \\frac{P(Vector|Class)*P(Class)}{P(Vector)}$$\n\nThe final choice of the class between two classes (in our case, high or low salary), is based on which probability is higher - \n\n$$P(Class = High Salary|Vector)$$  vs $$P(Class = Low Salary|Vector)$$\n\nSince the probability of the Vector $P(Vector)$ remains the same for both calculations, we ignore it. So, in essence we have to find the RHS of the equation -  \n$$P(Class|Vector) \\propto P(Vector|Class)*P(Class)$$\n\nThe $P(Class)$ is just the probability that a class appears, given by - \n$$\\frac{Number of rows with that Class}{Total Rows}$$\n\nSometimes, because the product $$P(Vector|Class)*P(Class)$$ can get really small, we take the log of the expression to calculate - \n$$log(P(Vector|Class)*P(Class))$$\n\nand then compare these values for both classes for a document (or job description)","c6811777":"Based on the result of the regression coefficient and the supporting graph, we find that the top 100 words in the data do support Zipf's law.","22afd617":"#### Get a list of expensive cities in England","b0e14f5b":"### Get words indicative of high salary\nThe numbers against the terms show the mutual information of these words with the low salary output","492d909c":"For our problem, since we split salaries using the 75th percentile as the threshold, **we have high salaries (1) 25% of the time and low salaries (0) 75% of the time!** This gives us an imbalanced class problem. Therefore, in addition to reporting accuracy we would also report the [AUC-ROC score](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc)!","c9174629":"### Multinomial Naive Bayes using job descriptions","2fd9af2b":"### Bernoulli Naive Bayes using Job Descriptions","6905300e":"So, the train data now has 10000 rows and some columns. Lets see what these columns are","44c1c300":"where **PRP** - Preposition, **VBP** - Verb present participle, **VBG**- Verb\/Gerund, **PRP$** - possessive pronoun and **NN** - Noun.","92944d05":"## Conclusion - \nWe saw that using just the job descriptions gives us an accuracy comparable and slightly better as compared to using the other predictors in this case. There is a lot more that can be done with text and I have just scratched the surface here for text analytics. I hope you liked this. Share your thoughts! :)","e3663eba":"# A tutorial in text analytics\n### Salary Prediction using job descriptions\n\nThe data comes from an old competition posted by Adzuna to predict salaries for jobs in the UK based on a variety of attributes, the most important of which is the job description (as we will see shortly). This is a good example to show that even unstructured data such as text can be used to make pretty solid predictions - something that goes against intuition somehow!","faa2235f":"### Multinomial Naive Bayes - \nFor multinomial naive bayes, our representation of the data changes and now we use the frequency of appearance of a term rather than just a 1 or 0 depending on the occurence. For this algorithm, the \n$$P(Vector|Class) = P(Class) * \\Pi_{t=1}^{V}P(w_{t}|Class)^{d_{jt}}$$\n\nwhere $w_{t}$ are the words in the vocabulary and $d_{jt}$ are the elements in row j and column t. In this case, the  bag-of-words representation changes slightly and we each element $d_{jt}$ is the frequency of occurence of a term in the document. Suppose that we have the following documents - \n\n| Text                         | Class      |\n|------------------------------|------------|\n| A great game                 | Sports     |\n| The election was over        | Not Sports |\n| Very clean match             | Sports     |\n| A clean but forgettable game | Sports     |\n| It was a close election      | Not Sports |\n\nThe bag-of-words representation of this data is as follows - \n\n| Text                         | a | great | game | the | election | was | over | very | clean | match | but | forgettable | it | close | Class      |\n|------------------------------|---|-------|------|-----|----------|-----|------|------|-------|-------|-----|-------------|----|-------|------------|\n| A great game                 | 1 | 1     | 1    | 0   | 0        | 0   | 0    | 0    | 0     | 0     | 0   | 0           | 0  | 0     | Sports     |\n| The election was over        | 0 | 0     | 0    | 1   | 1        | 1   | 1    | 0    | 0     | 0     | 0   | 0           | 0  | 0     | Not Sports |\n| Very clean match             | 0 | 0     | 0    | 0   | 0        | 0   | 0    | 1    | 1     | 1     | 0   | 0           | 0  | 0     | Sports     |\n| A clean but forgettable game | 1 | 0     | 1    | 0   | 0        | 0   | 0    | 0    | 1     | 0     | 1   | 1           | 0  | 0     | Sports     |\n| It was a close election      | 1 | 0     | 0    | 0   | 1        | 1   | 0    | 0    | 0     | 0     | 0   | 0           | 1  | 1     | Not Sports |\n\n\nFor multinomial Naive Bayes, we look at the probability of a word appearing in each class $P(w_{t}|Class)$, which for this example is given by the sum of frequencies in all documents of a class where the word appears (sum of column values for that word) divided by the total number of times any word appears (sum of all terms in the bag-of-words representation). Therefore we get (for some words) - \n\n| Word  | P(Word,Sports) | P(Word,Not Sports) |\n|-------|----------------|--------------------|\n| a     | (2\/11)         | (1\/9)              |\n| close | (0\/11)         | (1\/9)              |\n| game  | (2\/11)         | (0\/9)              |\n| very | (1\/11)         | (0\/9)             |\n\nNote - I use comma in the column names above because the table gives me a weird error. It actually means $P(Word|Sports)$ and $P(Word|Not Sports)$. ","fa9102eb":"## How does the frequency change if we exclude stopwords?\n**Stopwords** are words such as* I, me, myself, they, and*, etc. that are useful for meaningful sentence structure but **do not really add value to the prediction task at hand. This is because they appear with equal probability in both high salary and low salary job descriptions.** To make computations faster, we remove these stopwords before we go into the prediction algorithm!\n\nApproach -\n1. Make a list of stopwords from the nltk library.\n2. Remove stopwords from each of the descriptions.\n3. Apply the pos_tagger again.","4896d60c":"NLTK is the key library here! It has a host of functions that help us quickly make sense of a large amount of text data! Some of the basic terminology associated with the NLTK library is as follows - \n \n1. **Corpus** - This is just the text string. It could be one line long or a 100 lines long.\n2. **Tokens** - Tokens are just the words appearing in a text string. For instance, in the string \"I like New York\" we have 4 tokens - \"I\", \"like\", \"New\" and \"York\". Although this can be done using a simple split, NLTK has functions that tokenize rapidly especially if we have a lot of text in a huge dataframe!\n3. **Vocabulary** - This is the entire collection of all tokens found in a corpus!\n4. **Parts of Speech** - Going back to basic English classes, every sentence has words that belong to some part of speech. There are verbs, nouns, proper nouns, pronouns, etc. NLTK has the `pos_tagger` function that allows us to quickly tag parts of speech to a string. You might be thinking if its any good at it and I can tell you that it is!\n5. **Stop Words** - Stop words are english words such as \"i\", \"me\", \"myself\" which although add sense to a sentence, contribute nothing towards making it special enough to be of significance in a prediction problem.","9813f8d3":"### Concept - Mutual Information\nJust like the concept of feature importance in predictive models such as regression or random forests, we have the concept of mutual information while performing text analytics. For our example, mutual Information would tell us which words are the most indicative of high or low salary. The formula for mutual information looks like - \n$$MI(x,y) = p(x, y) * log_2(p(x,y)\/(p(x) * p(y)))$$\n\nwhere $x$ is the word in the vocabulary and $y$ is a class. The values in the expression are found as follows - \n$p(x) = \\frac{n_{x}}{N}$, $p(y) = \\frac{n_{y}}{N}$ and $p(x,y) = \\frac{n_{x,y}}{N}$,\n\nwhere $n_{x}$ is count of documents with word 'x', $n_{y}$ is count of documents with class 'y', $n_{x,y}$ is count of documents with class 'y' and word 'x' and 'N' is the total documents.\n\nThe interpretation of this is simple, large positive values of MI mean that the presence of a certain words are strongly indicative that the document belongs to the class. Negative values means that the presence is negatively associated with the class. \n\nThe term inside the log can be thought of as a 'Lift' - How much do we expect this word and class to appear together. If the word and class are independent $p(x,y)$ equals $p(x)*p(y)$ and MI goes to 0.","267512d6":"## Two types of Naive Bayes Algorithms\nWe understood that essentially a naive bayes problem boils down to calculating the following probability - \n$$P(Vector|Class)$$\n\nThere are two approaches to get here and this gives rise to bernoulli naive bayes and multinomial naive bayes. The difference is subtle and has to do with how we represent our data. \n\n### Bernoulli Naive Bayes -\nFor Bernoulli Naive Bayes each column in the dataset has to be a 1 or 0 type variable. If we speak about only job description data, we can say that for each term in a job description (where each term is a unique column of my dataframe), I will have either a 0 or a 1 depending on whether a word is present in my document or not. Lets say I have the following training data, categorising documents as belonging to either the 'Sport' class or the 'Informatics' class.\n\n| Document ID | goal | tutor | variance | speed | drink | defence | performance | field | Class       |\n|-------------|------|-------|----------|-------|-------|---------|-------------|-------|-------------|\n| 1           | 1    | 0     | 0        | 0     | 1     | 1       | 1           | 1     | Sport       |\n| 2           | 0    | 0     | 1        | 0     | 1     | 1       | 0           | 0     | Sport       |\n| 3           | 0    | 1     | 0        | 1     | 0     | 1       | 1           | 0     | Sport       |\n| 4           | 1    | 0     | 0        | 1     | 0     | 1       | 0           | 1     | Sport       |\n| 5           | 1    | 0     | 0        | 0     | 1     | 0       | 1           | 1     | Sport       |\n| 6           | 0    | 0     | 1        | 1     | 0     | 0       | 1           | 1     | Sport       |\n| 7           | 0    | 1     | 1        | 0     | 0     | 0       | 1           | 0     | Informatics |\n| 8           | 1    | 1     | 0        | 1     | 0     | 0       | 1           | 1     | Informatics |\n| 9           | 0    | 1     | 1        | 0     | 0     | 1       | 0           | 0     | Informatics |\n| 10          | 0    | 0     | 0        | 0     | 0     | 0       | 0           | 0     | Informatics |\n| 11          | 0    | 0     | 1        | 0     | 1     | 0       | 1           | 0     | Informatics |\n\nand I want to classify these two documents - \n\n| Document ID | goal | tutor | variance | speed | drink | defence | performance | field | Class |\n|-------------|------|-------|----------|-------|-------|---------|-------------|-------|-------|\n| 12          | 1    | 0     | 0        | 1     | 1     | 1       | 0           | 1     | ?     |\n| 13          | 0    | 1     | 1        | 0     | 1     | 0       | 1           | 0     | ?     |\n<br> \n\n\nFor the above example we get - \n$P(Sport) = \\frac{6}{11}$, $P(Informatics) = \\frac{5}{11}$. The probabilities for the words for each class are - \n\n$P(Goal|Sport) = \\frac{1}{2}$, $P(tutor|Sport) = \\frac{1}{6}$, $P(Goal|Informatics) = \\frac{1}{5}$, etc.\n\n**Naive bayes is called naive bayes because of the assumption that all words occur independently**, which means - \n\n$$P(Vector|Class) = \\Pi_{t = 1}^{V}(d_{jt}*P(w_{t}|Class) + (1 - d_{jt})*(1 - P(w_{t}|Class))$$\n\nwhere $w_{t}$ are the words in the dataframe and $d_{jt}$ are the elements in $j$ row and $t$ column.\n\nEssentially, we multiply the probability of occurence or non-occurence of a word in a class times an indicator (1 for occurence and 0 for non-occurence). Intuitively, this says that a description can be of a particular class both by what words it has and also by what words it doesnt have! So for document 12 the probability that it belongs to class Sport is - \n$$P(Sport|Doc12) = \\frac{6}{11}*(\\frac{1}{2}*(1 - \\frac{1}{6})*(1 - \\frac{1}{3})*\\frac{1}{2}*\\frac{1}{2}*\\frac{2}{3}*(1 - \\frac{2}{3})*\\frac{2}{3}) = 5.6 * 10^{-3}$$","8c8d978c":"The `Clean_Full_Descriptions` now has the full descriptions without punctuations, numbers, star words or urls!","3a74cf03":"# On to Predictions - \nHaving understood the basics of text analytics, let's move on to predicting salary using the data that we have and see if using job descriptions gives us a good model to use to predict job salaries! To see the power of text data, we will compare two models - \n1. Using job descriptions (without the stopwords) as the only predictor\n2. Using all the other variables as the predictors except for Job ID.\n\nWe will convert this to a classification problem - <br>\n*Predict high (>75th percentile) or low salary (<75th percentile) using the data provided*. ","35438c9e":"Let's do the customary checks - data types of variables and missing values in the data.","43a08b9a":"The top 10 words make a lot of intuitive sense - words like **experience, role, work, business and skill** are essential parts of any job description!","c07ac7b5":"### What's the baseline accuracy for this problem?\nBaseline accuracy is defined as the accuracy that we would get if we dont fit a model, but just keep predicting the majority class irrespective of the data. How are our two classes represented in this data?","397a3333":"*LocationNormalized* has the job location information! This has a large number of cities which would give us a huge number of columns - we DONT want that! Lets instead make a list of expensive cities and make an indicator variable denoting whether a city is expensive or not!","abe1511e":"## Does this data support Zipf's law?\n[**Zipf's law**](https:\/\/en.wikipedia.org\/wiki\/Zipf's_law) states that for any word (including stopwords) in a given corpus of natural language utterances, its frequency of occurence is inversely proportional to its rank in the frequency table! This can also be stated as 'Frequency of the word in a corpus times its rank is a constant'! Mathematically, this can be expressed as  - \n\n$$R*x_{r} = c$$\n\n$$R = \\frac{c}{x_{r}}$$\n\n$$\\log(R) = -1\\log(\\frac{x_{r}}{c})$$\n\nSince $R*x_{r}$ is a constant, we approximate it by $N*x_{n}$ where $N$ is the largest rank and $x_{n}$ is the frequency associated with the largest rank. Thereby, we get - \n$$\\log(R) = -1\\log(\\frac{x_{r}}{n*x_{n}})$$\n\nwhere $R$ is the rank of the word and $x_{r}$ is the frequency of occurence!\n\nThis last equation can be modelled as a linear regression and we can check if Zipf's law holds if the coefficient of the variable comes out to be -1!\n\nApproach - \n1. Create a corpus of the cleaned descriptions.\n2. Tokenize the words in the corpus.\n3. Take a count of these words and associate ranks with these words! Take the top 100 words\n4. Create a linear regression model as given in the last equation above and check the value of the coefficient!","2619a0b6":"### Cleaning up the descriptions\nA look at the description above shows us that these descriptions contain - numbers, urls and certain strings as '*' which I believe are either phone numbers or salary figures that have been removed so that these do not affect our predictions! We will have to remove these strings before we try out any analytics!\n\nApproach - We will use the substitute feature to find and substitute these anomalous strings in our job descriptions","0629a437":"What do we find? Removing stopwords totally removed **Pronouns and Determiners** from the top 5. In their place we now have **Verb\/Gerund (VBG) and Verb\/Present Participle (VBP)**.","cb8c10e4":"### Bernoulli Naive Bayes using everything but descriptions\nNow that we have the basics in place, lets apply these models. Just as we showed for document-vocabulary dataframe above, bernoulli naive bayes can be applied to any data which has variables with the domain 0 or 1. So, for our data, since we have all categorical variables, we can get dummy variables for those and then apply bernoulli naive bayes!","dc81ad61":"As expected - **Nouns (NN), plural nouns (NNS), adjectives (JJ), Pronouns (IN) and Determiners (DT)** are the top 5 parts of speech tagged in the corpus. Mostly these will be the top 5 parts of speech for any corpus!","63daafb5":"### Words that indicate high\/low salary","920ecd77":"The data that we have has around 240k rows and 12 columns. For the sake of simplicity and faster processing, lets just pick a random sample of 10000 rows from it and proceed further.","40488229":"We see a problem in the above table - For words where the frequency of appearance in training data is 0 (for example for the word 'close' in Sports documents), we would also get a 0 probability if that word appears in the testing data even though that test document might belong to 'Sports'. To avoid this error, we add a very small frequency to the probability of appearance of each term in the corpus and the formula becomes - \n$$P(w_{t}|Class) = \\frac{(1 + \\text{sum of column where word appears})}{(\\text{count of vocabulary} + \\text{sum of all terms in the bag of words representation})}$$.\n\nSo our table of probabilities changes to - \n\n| Word  | P(Word,Sports)  | P(Word,Not Sports) |\n|-------|-----------------|--------------------|\n| a     | (1 + 2\/11 + 14) | (1 + 1\/9 + 14)     |\n| close | (1 + 0\/11 + 14) | (1 + 1\/9 + 14)     |\n| game  | (1 + 2\/11 + 14) | (1 + 0\/9 + 14)     |\n| very | (1 + 1\/11 + 14) | (1 + 0\/9  + 14)    |\n\nFor a test document - \"A very close game\", we get the probabilities as - \n$$P(document|Sports) = \\frac{3}{25}^{1} * \\frac{2}{25}^{1} * \\frac{1}{25}^{1} * \\frac{3}{25}^{1} * \\frac{3}{5}$$\n$$P(document|Sports) = 4.61 * 10^{-5}$$\n\n$$P(document|Not Sports) = \\frac{2}{23}^{1} * \\frac{1}{23}^{1} * \\frac{2}{23}^{1} * \\frac{1}{23}^{1} * \\frac{2}{5}$$\n$$P(document|Not Sports) = 1.43 * 10^{-5}$$\n\nSo, this document belongs to the Sports class!","45250403":"## What are the top 5 parts of speech in the job description? How frequently do they appear? \nNLTK has a parts of speech tagger that looks at a sentence and assigns parts of speech to different words in it such as - nouns, plural nouns, determiners, pronouns, verbs, adverbs, etc. This tagging is done based on the word (token) itself.\n\nFor example, if I have a sentence such as \"I love riding my motorcycle\", NLTK assigns the following parts of speech to it - \n","e4b99fa2":"## What are the 10 most common words in the job description data?\nWe calculated the frequent words list above, but this also had stopwords in it and most probably stopwords appear very frequently in the top 100 words. **Let's see what words (other than stopwords) appear in this corpus of job descriptions!**\n\n### Concept 1 - Lemmatization\n'Lemma' is a latin word meaning *root*. What lemmatization does is to **take similar words - 'experience', 'experiences', 'experiencing' and reduces them to their root word *experience*.** This is helpful for us since it brings out the most important words in the corpus and doesnt treat each form of the word separately! \n\n### Concept 2 - Stemming\nStemming is a heuristic process that trims the ends of similar looking words to end up with the same words - thereby reducing the vocabulary of words we have! **This is not that intuitive however, since sometimes it would take 'operate', 'operates' and 'operating' and trim them all to 'oper' which is not even a word!** So while this can be useful for purely prediction purposes, this is not meaningful as an output! Check out this [link](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html) for an excellent explanation on Stemming and Lemmatization!","4a93114f":"### Get words indicative of low salary\nThe numbers against the terms show the mutual information of these words with the low salary output","b5dd2102":"To get the top 5 parts of speech in the job descriptions, we will - \nApproach - \n1. Tokenize each description under the clean_full_description column.\n2. NLTK pos_tag function returns the word followed by its part of speech tagging in a tuple. We need to extract the second element from these tuples.\n3. For each description get the parts of speech tagging for each string in the full description","f1c7b3ff":"**Observations** -\n1. Most values in our dataframe are of the 'Object' or 'String' data type. This means that we will have to convert these to dummy variables to proceed! \n2. There are missing values in the variables as shown above! These are all 'character' variables so when we create dummies, there will be a new column for the 'NA' values.\n\nBefore we create dummies, lets see which columns are categorical in nature!","0c3e6bb8":"1. The prediction accuracy achieved is **78.12%** using just the numerical variables!\n2. The area under the ROC curve (AUC-ROC score) is **66.6%**. \n\n3. **The confusion matrix has the actual y labels as the rows and predicted labels as the columns.** \n    So the first cell in the matrix is read as - *The number of times the actual salary was low (0) and our model also predicted it as low (0)*"}}