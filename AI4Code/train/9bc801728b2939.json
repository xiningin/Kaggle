{"cell_type":{"51adf698":"code","300c7700":"code","63454000":"code","704e8ab7":"code","8da6f5f9":"code","8bd6b86b":"code","b48bd606":"code","e7e5d8c2":"code","7142df75":"code","61abad71":"code","7c341307":"code","a3da785b":"code","81e74900":"code","dac0b934":"code","1446a701":"code","98e20996":"code","fd5892bf":"code","5175a5f4":"code","856fe577":"code","ba98a899":"code","38f054dd":"code","f1eee794":"code","6c454d19":"code","f40b9167":"markdown","6873ccab":"markdown","986e3615":"markdown","f08b30b1":"markdown","b211b6ef":"markdown","e7974522":"markdown","f85aa0a4":"markdown","e3b6fd86":"markdown","bfc430fc":"markdown","b91c294d":"markdown","94a9079c":"markdown","d02956ac":"markdown","e10ed168":"markdown","25eeeb9c":"markdown","1ba80e98":"markdown","f72af3de":"markdown","3e2f57d8":"markdown","3825699d":"markdown","a23dd171":"markdown","ca30f0fb":"markdown","55e40a68":"markdown"},"source":{"51adf698":"import pandas as pd\nimport numpy as np  \nimport matplotlib.pyplot as plt  \n#import seaborn as seabornInstance \n#from sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.metrics as metrics\n%matplotlib inline","300c7700":"#flightdata = pd.read_csv(\"https:\/\/introtomlsampledata.blob.core.windows.net\/data\/flightdelays\/flightdelays.csv\")\nflightdata = pd.read_csv(r\"..\/input\/flightdelays.csv\")","63454000":"print(flightdata.shape)\nflightdata.columns","704e8ab7":"flightdata.dtypes","8da6f5f9":"flightdata.head(10)","8bd6b86b":"#flightdata.describe()\ncarrierlist = list(flightdata.Carrier.unique())\ncarrierlist.sort()","b48bd606":"carrierdict = {carrierlist[i]: list(range(len(carrierlist)))[i] for i in range(len(carrierlist))} \nflightdata[\"Carrier\"] = flightdata[\"Carrier\"].replace(carrierdict) \nflightdata.Carrier.unique()","e7e5d8c2":"train = flightdata[flightdata[\"Month\"] < 10]\ntest = flightdata[flightdata[\"Month\"] >= 10]\nprint(train.shape, test.shape)","7142df75":"train = train.drop(\n    [\"Month\", \"Year\", \"Year_R\", \"Timezone\", \"Timezone_R\"], axis=1)\ntest = test.drop([\"Month\", \"Year\", \"Year_R\", \"Timezone\", \"Timezone_R\"], axis=1)\nprint(train.shape, test.shape)","61abad71":"trainX = train.drop([\"ArrDel15\"],axis = 1)\ntrainy = train[\"ArrDel15\"]\nprint(trainX.shape,trainy.shape)","7c341307":"testX = test.drop([\"ArrDel15\"],axis = 1)\ntesty = test[\"ArrDel15\"]\nprint(testX.shape,testy.shape)","a3da785b":"model = LogisticRegression(solver=\"liblinear\")\nmodel.fit(trainX, trainy)","81e74900":"predicted_classes = model.predict(testX)\naccuracy = metrics.accuracy_score(testy,predicted_classes)\nparameters = model.coef_","dac0b934":"accuracy","1446a701":"confusion = metrics.confusion_matrix(testy, model.predict(testX))\nprint(confusion)\n#[row, column]\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]","98e20996":"# use float to perform true division, not integer division\nprint((TP + TN) \/ float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(testy, model.predict(testX)))","fd5892bf":"classification_error = (FP + FN) \/ float(TP + TN + FP + FN)\n\nprint(classification_error)\nprint(1 - metrics.accuracy_score(testy, model.predict(testX)))","5175a5f4":"sensitivity = TP \/ float(FN + TP)\n\nprint(sensitivity)\nprint(metrics.recall_score(testy, model.predict(testX)))","856fe577":"specificity = TN \/ (TN + FP)\n\nprint(specificity)","ba98a899":"false_positive_rate = FP \/ float(TN + FP)\n\nprint(false_positive_rate)\nprint(1 - specificity)","38f054dd":"precision = TP \/ float(TP + FP)\n\nprint(precision)\nprint(metrics.precision_score(testy, model.predict(testX)))","f1eee794":"# calculate the fpr and tpr for all thresholds of the classification\nprobs = model.predict_proba(testX)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(testy, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","6c454d19":"# IMPORTANT: first argument is true values, second argument is predicted probabilities\nprint(metrics.roc_auc_score(testy, preds))","f40b9167":"Calculating the roc_auc_score\n","6873ccab":"Using a predownloaded dataset instead of the link as the dataset is huge and there has been some download issues","986e3615":"Creating trainX, trainy, textX and testy.","f08b30b1":"Calculating the confusion matrix.","b211b6ef":"Splitting into test and train dataset.","e7974522":"Initialising our classifier and fitting.","f85aa0a4":"Calculating the classification error using the confusion matrix.","e3b6fd86":"Calculating the false positive rate using the confusion matrix.","bfc430fc":"Creating a dictionary using dictionary comprehension.\n\nParsing the dictionary to replace() method to find and replace \"keys\" with \"values\".\n\nMore on Dictionary Comprehensions: https:\/\/www.programiz.com\/python-programming\/dictionary-comprehension","b91c294d":"Here, we are encoding the \"Carrier\" categorical column.\n\nunique() is used to find out the unique values in a DataFrame column.\n\nsort() is used to sort the list.","94a9079c":"<h1>Lab5: Train and evaluate a model<\/h1>\n\nIn this lab, we will be using the Flight Delays data set that is enhanced with the weather data. Based on the enriched dataset, we will learn to use the Python and Jupyter Notebook to process data, build, train, score, and evaluate a classification model to predict if a particular flight will be delayed by 15 minutes or more.\n","d02956ac":"Dropping the \"Month\", \"Year\", \"Year_R\", \"Timezone\", \"Timezone_R\" columns from the test and train datasets.\n","e10ed168":"Plotting the ROC curve","25eeeb9c":"Predicting, calculating the Accuracy and taking a look at the model coefficients.","1ba80e98":"Calculating the accuracy using the confusion matrix.","f72af3de":"Calculating the sensitivity using the confusion matrix.","3e2f57d8":"Calculating the precision using the confusion matrix.","3825699d":"Looking at the top 10 records in the dataset.","a23dd171":"Calculating the specificity using the confusion matrix.","ca30f0fb":"Importing the required libraries:\n\n\n* pandas: For reading and manipulating our dataset\n* numpy: Used for working on arrays\n* matplotlib: For plotting\n* sklearn.linear_model: For importing our Logistic Regression Classifier\n* sklearn.metrics: Importing this will enable us to use different metrics for evaluating our model","55e40a68":"Having a first glance at the data. shape, columns and dtypes attributes are used."}}