{"cell_type":{"f4016280":"code","d6c71d3b":"code","be76fc91":"code","7a437bba":"code","d21d01cc":"code","0fab1dcd":"code","bafba6b6":"code","f21644bd":"code","069a1d92":"code","92136610":"code","9dd3cff1":"code","f422a88e":"code","ec831c1b":"code","eb856db1":"code","884e9a32":"code","0d1ef3fc":"code","f6651929":"code","9ec4dbd6":"code","0ac3591d":"code","a441bbf0":"code","b8f03782":"code","41f9afe5":"code","de0db2e2":"code","1022371e":"code","82ac1b80":"code","e9251026":"code","0a472002":"code","d776955e":"code","f422edec":"code","ad0f5875":"code","980ae5ee":"code","e1473835":"code","802c215c":"code","681cd664":"code","99361b03":"code","7d95d656":"code","0bdd4bd8":"code","33c46a38":"code","d134d0e8":"code","9069d14c":"code","1aaed4f2":"code","b251e6c4":"code","0dc079e9":"code","4f87b4d0":"code","a51f2419":"code","7da27d66":"code","298777fd":"code","0482781b":"code","da9cb66f":"code","07ab19ff":"code","59b0684f":"code","931d758b":"code","025c2ce8":"code","23e33cd9":"code","628babd9":"code","08da55b0":"code","906eb873":"code","b53d697d":"code","ec2cd4ac":"code","512580ff":"code","f4653665":"code","e2f9757d":"code","498fa6c3":"code","1959ba4b":"code","2714425e":"code","9241946f":"code","762fe059":"code","f18505d8":"code","b94d223c":"code","8d380d1b":"code","672032c4":"code","c3ba9bdd":"code","ba14186c":"code","22009b45":"code","06c76202":"code","2cdac83a":"code","d8a6a72a":"code","5bad6c78":"markdown","ac05802a":"markdown","6e77a816":"markdown","55fd24de":"markdown","243a8653":"markdown","844c3cfb":"markdown","a2a55a4d":"markdown","a7f74ace":"markdown","8b8c1387":"markdown","28e74482":"markdown","222ad547":"markdown","3869c719":"markdown","69d28300":"markdown","534e8999":"markdown","e828a21a":"markdown","249d2e57":"markdown","566a04a8":"markdown","62f92c11":"markdown","c19b2bc1":"markdown","823012b6":"markdown","cdab3846":"markdown","f65eb3fd":"markdown","499a4871":"markdown","19e7144d":"markdown","05ef3418":"markdown","9e684803":"markdown","b1fe7dba":"markdown","458b6357":"markdown","e6712c94":"markdown","c319fe04":"markdown","62bd7d4a":"markdown","15795539":"markdown","3dbd2273":"markdown","767f4961":"markdown","94c61138":"markdown","bd0d3214":"markdown","13afeebb":"markdown","78f92e65":"markdown","bb896f2b":"markdown","8678551b":"markdown","d73701be":"markdown","dadafdae":"markdown","f9d1fe98":"markdown","8108284d":"markdown","5d44dbf7":"markdown","b6c61297":"markdown","e603265f":"markdown","3ded6c16":"markdown","dcfda891":"markdown","3aa9a44d":"markdown","1e55bb0b":"markdown","12ad4e2f":"markdown","13d118ac":"markdown","096dc8a3":"markdown","f5ad8358":"markdown","2a80adc4":"markdown","e7cbcf38":"markdown","ad48e3d3":"markdown","337acccf":"markdown","c48ad190":"markdown","552f475a":"markdown","0999c395":"markdown","0e3e066b":"markdown","336f32f2":"markdown","9bc417ff":"markdown","f5bd00ca":"markdown","f510dbaa":"markdown","e7f8f3cd":"markdown","899c28b8":"markdown","21f152ce":"markdown","4dff7a5c":"markdown","6b527987":"markdown","529a1ac6":"markdown","a3fa6650":"markdown","9bf05683":"markdown","e41deb34":"markdown","5862260f":"markdown","6a519820":"markdown","8e388b3e":"markdown","bc6ca1a3":"markdown","8697d309":"markdown"},"source":{"f4016280":"import numpy as np\nimport pandas as pd","d6c71d3b":"!pip install scikit-learn==0.23.0\n\nfrom numpy.ma import MaskedArray\nimport sklearn.utils.fixes\n\nsklearn.utils.fixes.MaskedArray = MaskedArray","be76fc91":"df = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\", index_col=['Id'], na_values=\"?\")","7a437bba":"df.head()","d21d01cc":"df.info()","0fab1dcd":"df.describe()","bafba6b6":"import matplotlib.pyplot as plt\nimport seaborn as sns","f21644bd":"# Copia \"df\" para \"df_analysis\"\ndf_analysis = df.copy()","069a1d92":"# Importando o LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Instanciando o LabelEncoder\nle = LabelEncoder()\n\n# Modificando o nosso dataframe, transformando a vari\u00e1vel de classe em 0s e 1s\ndf_analysis['income'] = le.fit_transform(df_analysis['income'])","92136610":"df_analysis['income']","9dd3cff1":"mask = np.triu(np.ones_like(df_analysis.corr(), dtype=np.bool))\n\nplt.figure(figsize=(10,10))\n\nsns.heatmap(df_analysis.corr(), mask=mask, square = True, annot=True, vmin=-1, vmax=1, cmap='autumn')\nplt.show()","f422a88e":"sns.distplot(df_analysis['age']);","ec831c1b":"sns.catplot(x=\"income\", y=\"hours.per.week\", data=df_analysis);","eb856db1":"sns.catplot(x=\"income\", y=\"hours.per.week\", kind=\"boxen\", data=df_analysis);","884e9a32":"sns.catplot(x=\"income\", y=\"education.num\", kind=\"boxen\", data=df_analysis);","0d1ef3fc":"sns.catplot(x=\"income\", y=\"age\", kind=\"boxen\", data=df_analysis);","f6651929":"sns.catplot(x=\"income\", y=\"capital.gain\", kind=\"boxen\", data=df_analysis);","9ec4dbd6":"sns.catplot(x=\"income\", y=\"capital.loss\", kind=\"boxen\", data=df_analysis);","0ac3591d":"sns.catplot(x=\"income\", y=\"capital.gain\", data=df_analysis);","a441bbf0":"sns.catplot(x=\"income\", y=\"capital.loss\", data=df_analysis);","b8f03782":"df_analysis.describe()","41f9afe5":"sns.catplot(y=\"sex\", x=\"income\", kind=\"bar\", data=df_analysis);","de0db2e2":"sns.catplot(y=\"race\", x=\"income\", kind=\"bar\", data=df_analysis);","1022371e":"sns.catplot(y=\"workclass\", x=\"income\", kind=\"bar\", data=df_analysis);","82ac1b80":"sns.catplot(y=\"marital.status\", x=\"income\", kind=\"bar\", data=df_analysis);","e9251026":"sns.catplot(y=\"occupation\", x=\"income\", kind=\"bar\", data=df_analysis);","0a472002":"sns.catplot(y=\"native.country\", x=\"income\", kind=\"bar\", data=df_analysis);","d776955e":"df_analysis[\"native.country\"].value_counts()","f422edec":"df.drop_duplicates(keep='first', inplace=True)","ad0f5875":"df = df.drop(['fnlwgt', 'native.country', 'education'], axis=1)","980ae5ee":"df.head()","e1473835":"# Removendo a nossa vari\u00e1vel de classe\ny = df.pop('income')\n\nX = df","802c215c":"X.head()","681cd664":"from sklearn.model_selection import train_test_split\nX_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=0.25, random_state=42)","99361b03":"# Seleciona as vari\u00e1veis num\u00e9ricas\nnumerical_cols = list(X_train.select_dtypes(include=[np.number]).columns.values)\n\n# Remove as vari\u00e1veis num\u00e9ricas esparsas\nnumerical_cols.remove('capital.gain')\nnumerical_cols.remove('capital.loss')\n\n# Seleciona as vari\u00e1veis num\u00e9ricas esparsas\nsparse_cols = ['capital.gain', 'capital.loss']\n\n# Seleciona as vari\u00e1veis categ\u00f3ricas\ncategorical_cols = list(X_train.select_dtypes(exclude=[np.number]).columns.values)\n\n# Mostrando as diferentes sele\u00e7\u00f5es\nprint(\"Colunas num\u00e9ricas: \", numerical_cols)\nprint(\"Colunas esparsas: \", sparse_cols)\nprint(\"Colunas categ\u00f3ricas: \", categorical_cols)","7d95d656":"from sklearn.impute import SimpleImputer\n\n# Inicializa nosso Imputer\nsimple_imputer = SimpleImputer(strategy='most_frequent')","0bdd4bd8":"# Cria um array com um dado faltante\narray = np.array([[\"Female\"],\n         [\"Male\"],\n         [np.nan],\n         [\"Female\"]], dtype=object)\n\n# Preenche o dado faltante com o Imputer\nnew_array = simple_imputer.fit_transform(array)\n\nprint(new_array)","33c46a38":"from sklearn.preprocessing import OneHotEncoder\n\n# Inicializa nosso Encoder\none_hot = OneHotEncoder(sparse=False)","d134d0e8":"# Cria um array com dados categ\u00f3ricos\narray = np.array([[\"Female\"],\n         [\"Male\"],\n         [\"Female\"],\n         [\"Female\"]], dtype=object)\n\n# Transforma o nosso array\nnew_array = one_hot.fit_transform(array)\n\nnew_array","9069d14c":"from sklearn.pipeline import Pipeline\n\n# Cria a nossa pipeline categ\u00f3rica\ncategorical_pipeline = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('onehot', OneHotEncoder(drop='if_binary'))\n])","1aaed4f2":"# Cria um array com dados categ\u00f3ricos\narray = np.array([[\"Female\"],\n         [\"Male\"],\n         [np.nan],\n         [\"Female\"]], dtype=object)\n\n# Transforma o nosso array\nnew_array = categorical_pipeline.fit_transform(array)","b251e6c4":"from sklearn.impute import KNNImputer\n\n# Cria o nosso KNNImputer com 5 vizinhos\nknn_imputer = KNNImputer(n_neighbors=5)","0dc079e9":"# Cria o nosso array com dados faltantes\narray = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n\n# Preenche os dados faltantes\nnew_array = knn_imputer.fit_transform(array)\n\nnew_array","4f87b4d0":"from sklearn.preprocessing import StandardScaler\n\n# Cria o nosso StandardScaler\nscaler = StandardScaler()","a51f2419":"# Cria um array num\u00e9rico\narray = [[-3, 0], [0, 0], [3, 1], [0, 1]]\n\n# Normaliza nosso array\nnew_array = scaler.fit_transform(array)\n\nnew_array","7da27d66":"# Cria a nossa pipeline num\u00e9rica\nnumerical_pipeline = Pipeline(steps = [\n    ('imputer', KNNImputer(n_neighbors=10, weights=\"uniform\")),\n    ('scaler', StandardScaler())\n])","298777fd":"from sklearn.preprocessing import RobustScaler\n\nsparse_pipeline = Pipeline(steps = [\n    ('imputer', KNNImputer(n_neighbors=10, weights=\"uniform\")),\n    ('scaler', RobustScaler())\n])","0482781b":"from sklearn.compose import ColumnTransformer\n\n# Cria o nosso Pr\u00e9-Processador\n\n# Cada pipeline est\u00e1 associada a suas respectivas colunas no datast\npreprocessor = ColumnTransformer(transformers = [\n    ('num', numerical_pipeline, numerical_cols),\n    ('spr', sparse_pipeline, sparse_cols),\n    ('cat', categorical_pipeline, categorical_cols)\n])","da9cb66f":"X_train = preprocessor.fit_transform(X_train)","07ab19ff":"from sklearn.svm import SVC\n\n# Instancia nosso classificador\nsvc = SVC(random_state=42, probability=True)","59b0684f":"from sklearn.model_selection import cross_val_score\n\nscore = cross_val_score(svc, X_train, Y_train, cv = 4, scoring=\"accuracy\")\nprint(\"Acur\u00e1cia com cross validation:\", score.mean())","931d758b":"# Importa o Bayes Search:\nfrom skopt import BayesSearchCV\n\n# Importa o espa\u00e7o de busca inteiro\nfrom skopt.space import Integer, Real\n\n# Cria o Bayes Search:\nsvc_search_cv = BayesSearchCV(estimator = svc,\n                              search_spaces = {'C': Real(1e-2, 20),\n                                               'gamma': ['scale', 'auto'],},\n                              cv = 2,\n                              n_iter = 15, n_jobs=-1, random_state=42)\n\n# Realizando a otimiza\u00e7\u00e3o por BayesSearch:\n%timeit -n 1 -r 1 svc_search_cv.fit(X_train, Y_train)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(svc_search_cv.best_params_))\nprint('Desempenho do melhor modelo: {}'.format(round(svc_search_cv.best_score_,5)))","025c2ce8":"from sklearn.ensemble import RandomForestClassifier\n\n# Instancia nosso classificador\nrfc = RandomForestClassifier(random_state=42)","23e33cd9":"# Cria o Bayes Search:\nrfc_search_cv = BayesSearchCV(estimator = rfc,\n                              search_spaces = {'n_estimators': Integer(100, 500),\n                                               'criterion': ['gini', 'entropy'],\n                                               'max_depth': Integer(1, 50),},\n                              cv = 5,\n                              n_iter = 20, n_jobs=-1, random_state=42)\n\n# Realizando a otimiza\u00e7\u00e3o por BayesSearch:\n%timeit -n 1 -r 1 rfc_search_cv.fit(X_train, Y_train)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(rfc_search_cv.best_params_))\nprint('Desempenho do melhor modelo: {}'.format(round(rfc_search_cv.best_score_,5)))","628babd9":"from xgboost import XGBClassifier\n\n# Instancia nosso classificador\nxgb = XGBClassifier(random_state=42)","08da55b0":"# Cria o Bayes Search:\nxgb_search_cv = BayesSearchCV(estimator = xgb,\n                              search_spaces = {'n_estimators': Integer(10, 500),\n                                               'learning_rate': Real(1e-3, 1),\n                                               'max_depth': Integer(1, 20),\n                                               'reg_alpha': Real(1e-14, 1e1, prior = 'log-uniform'),\n                                               'reg_lambda': Real(1e-14, 1e1, prior = 'log-uniform'),},\n                              cv = 5,\n                              n_iter = 75, n_jobs=-1, random_state=42)\n\n# Realizando a otimiza\u00e7\u00e3o por BayesSearch:\n%timeit -n 1 -r 1 xgb_search_cv.fit(X_train, Y_train)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(xgb_search_cv.best_params_))\nprint('Desempenho do melhor modelo: {}'.format(round(xgb_search_cv.best_score_,5)))","906eb873":"from sklearn.neural_network import MLPClassifier\n\n# Instancia nosso classificador\nmlp = MLPClassifier(random_state=42, early_stopping=True)","b53d697d":"from scipy.stats import loguniform as sp_loguniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'hidden_layer_sizes': [(2 ** i, 2 ** j) for j in np.arange(5, 8) for i in np.arange(4, 7)],\n               'alpha': sp_loguniform(1e-10, 1e-1),\n               'learning_rate': ['constant','adaptive']}\n\n# Busca de Hiperpar\u00e2metros\nmlp_search_cv = RandomizedSearchCV(mlp, hyperparams, scoring='accuracy', n_iter=25, cv=3, n_jobs=-1, random_state=42)\n%timeit -n 1 -r 1 mlp_search_cv.fit(X_train, Y_train)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(mlp_search_cv.best_params_))\nprint('Desempenho do melhor modelo: {}'.format(round(mlp_search_cv.best_score_,5)))","ec2cd4ac":"X_valid = preprocessor.transform(X_valid)","512580ff":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","f4653665":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\n# Calculando a AUC do SVC\nsvc_roc_auc = roc_auc_score(le.fit_transform(Y_valid), svc_search_cv.predict_proba(X_valid)[:,1])\n\n# Calculando a acur\u00e1cia do SVC\nsvc_acc = accuracy_score(Y_valid, svc_search_cv.predict(X_valid))\n\nprint('AUC -------- SVC: {:.4f}'.format(svc_roc_auc))\nprint('Acur\u00e1cia --- SVC: {:.4f}'.format(svc_acc))","e2f9757d":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_roc_curve\n\nplot_roc_curve(svc_search_cv, X_valid, Y_valid)\nplt.show()","498fa6c3":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\n# Calculando a AUC da Floresta Aleat\u00f3ria\nrfc_roc_auc = roc_auc_score(le.fit_transform(Y_valid), rfc_search_cv.predict_proba(X_valid)[:,1])\n\n# Calculando a acur\u00e1cia da Floresta Aleat\u00f3ria\nrfc_acc = accuracy_score(Y_valid, rfc_search_cv.predict(X_valid))\n\nprint('AUC -------- Random Forest: {:.4f}'.format(rfc_roc_auc))\nprint('Acur\u00e1cia --- Random Forest: {:.4f}'.format(rfc_acc))","1959ba4b":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_roc_curve\n\nplot_roc_curve(rfc_search_cv, X_valid, Y_valid)\nplt.show()","2714425e":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\n# Calculando a AUC do XGBoost\nxgb_roc_auc = roc_auc_score(le.fit_transform(Y_valid), xgb_search_cv.predict_proba(X_valid)[:,1])\n\n# Calculando a acur\u00e1cia do XGBoost\nxgb_acc = accuracy_score(Y_valid, xgb_search_cv.predict(X_valid))\n\nprint('AUC -------- XGBoost: {:.4f}'.format(xgb_roc_auc))\nprint('Acur\u00e1cia --- XGBoost: {:.4f}'.format(xgb_acc))","9241946f":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_roc_curve\n\nplot_roc_curve(xgb_search_cv, X_valid, Y_valid)\nplt.show()","762fe059":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\n# Calculando a AUC da Rede Neural\nmlp_roc_auc = roc_auc_score(le.fit_transform(Y_valid), mlp_search_cv.predict_proba(X_valid)[:,1])\n\n# Calculando a acur\u00e1cia da Rede Neural\nmlp_acc = accuracy_score(Y_valid, mlp_search_cv.predict(X_valid))\n\nprint('AUC -------- Rede Neural: {:.4f}'.format(mlp_roc_auc))\nprint('Acur\u00e1cia --- Rede Neural: {:.4f}'.format(mlp_acc))","f18505d8":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_roc_curve\n\nplot_roc_curve(mlp_search_cv, X_valid, Y_valid)\nplt.show()","b94d223c":"test_data = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\", index_col=['Id'], na_values=\"?\")","8d380d1b":"X_test = test_data.drop(['fnlwgt', 'native.country', 'education'], axis=1)","672032c4":"X_test = preprocessor.transform(X_test)","c3ba9bdd":"predictions = xgb_search_cv.predict(X_test)","ba14186c":"predictions","22009b45":"submission = pd.DataFrame()","06c76202":"submission[0] = test_data.index\nsubmission[1] = predictions\nsubmission.columns = ['Id','income']","2cdac83a":"submission.head()","d8a6a72a":"submission.to_csv('submission.csv',index = False)","5bad6c78":"<a id=\"rfc\"><\/a>\n## \ud83c\udf32 Random Forest Classifier\n\nCaso voc\u00ea n\u00e3o esteja familiarizado com esse modelo, recomendo a leitura do seguinte texto:\n\n**[Leitura Recomendada - Random Forest](https:\/\/medium.com\/turing-talks\/turing-talks-18-modelos-de-predi%C3%A7%C3%A3o-random-forest-cfc91cd8e524)**\n\nPara utilizar esse modelo, vamos instanciar a classe `RandomForestClassifier` do **scikit-learn**:","ac05802a":"Nossa acur\u00e1cia \u00e9 bem alta! Em seguida, podemos plotar a curva ROC do modelo:","6e77a816":"E prediremos a nossa vari\u00e1vel de classe:","55fd24de":"#### Dados Num\u00e9ricos\n\nPara os nossos dados num\u00e9ricos, tamb\u00e9m come\u00e7aremos com o preenchimento de dados faltantes. No entanto, nesse caso vamos utilizar um outro tipo de imputer, o `KNNImputer`.\n\nEsse Imputer preenche os dados faltantes aplicando o pr\u00f3prio k-Nearest Neighbors, utilizando a coluna com os dados faltantes como vari\u00e1vel de classe.","243a8653":"Agora, podemos ver como ficou o nosso dataframe:","844c3cfb":"Quando usamos o m\u00e9todo `describe` do DataFrame no in\u00edcio do notebook, percebemos que essas duas vari\u00e1veis pareciam bem esparsas e com alguns outliers. Com esses gr\u00e1ficos, essa distribui\u00e7\u00e3o fica ainda mais clara. \u00c9 importante manter esse fato em mente, depois vamos utiliz\u00e1-lo para fazer o pr\u00e9-processamento dos nossos dados.","a2a55a4d":"### Bayes Search\n\nPara escolher a melhor quantidade de vizinhos, tamb\u00e9m podemos utilizar outras t\u00e9cnicas de otimiza\u00e7\u00e3o de hiperpar\u00e2metros, como o **Bayes Search**! Se quiser ler mais sobre o assunto, recomendo o seguinte texto:\n\n[Leitura Recomendada - Otimiza\u00e7\u00e3o de Hiperpar\u00e2metros](https:\/\/medium.com\/turing-talks\/modelos-de-predi%C3%A7%C3%A3o-otimiza%C3%A7%C3%A3o-de-hiperpar%C3%A2metros-em-python-3436fc55016e)\n\n![Bayes Search](https:\/\/miro.medium.com\/max\/875\/1*sJ3B_jLUzuGkBbKNwZUGtQ.gif)","a7f74ace":"Depois vamos preench\u00ea-lo com o \u00edndice de cada dado e seu respectivo r\u00f3tulo:","8b8c1387":"Esses dois \u00faltimos gr\u00e1ficos, de **'capital.gain'** e **'capital.loss'**, certamente parecem diferir bastante dos anteriores. Quando tra\u00e7amos novamente os scatter plots, o motivo fica bem mais claro:","28e74482":"Em seguida, vamos otimizar os hiperpar\u00e2metros da rede neural:","222ad547":"Por fim, podemos utilizar o m\u00e9todo `describe` para obter algumas estat\u00edsticas mais simples sobre os nossos dados:","3869c719":"Nosso array foi completado com o dado **'Female'**, a moda da nossa vari\u00e1vel!\n\nEm seguida, precisaremos transformar os dados categ\u00f3ricos em num\u00e9ricos para que o nosso modelo consiga interpret\u00e1-lo. Para isto, vamos utilizar o `OneHotEncoder`.\n\nEsse encoder transforma a nossa vari\u00e1vel com N classes em N vari\u00e1veis bin\u00e1rias, indicando se o nosso dado pertence \u00e0quela classe. Com o seguinte exemplo essa ideia fica mais clara:","69d28300":"Com a otimiza\u00e7\u00e3o de hiperpar\u00e2metros, conseguimos uma acur\u00e1cia de **87%**! Agora, podemos testar outros modelos de classifica\u00e7\u00e3o.","534e8999":"<a id=\"svc\"><\/a>\n## \ud83d\udcc9 Support Vector Classifier\n\nCaso voc\u00ea n\u00e3o esteja familiarizado com esse modelo, recomendo a leitura do seguinte texto:\n\n**[Leitura Recomendada - SVM](https:\/\/medium.com\/turing-talks\/turing-talks-12-classifica%C3%A7%C3%A3o-por-svm-f4598094a3f1)**\n\nPara utilizar esse modelo, vamos instanciar a classe `SVC` do **scikit-learn**:","e828a21a":"Assim fica o nosso novo dataframe sem a vari\u00e1vel de classe:","249d2e57":"E plotaremos sua curva ROC:","566a04a8":"Em seguida, vamos otimizar os hiperpar\u00e2metros do XGBoost:","62f92c11":"Como voc\u00ea consegue ver, temos **14** vari\u00e1veis diferentes: treze independentes e uma de classe. Essa vari\u00e1vel de classe \u00e9 o que queremos prever, a renda anual de cada pessoa (ou 'income').\n\nEssas 14 vari\u00e1veis est\u00e3o divididas entre vari\u00e1veis ***num\u00e9ricas*** e vari\u00e1veis ***categ\u00f3ricas***. As ***num\u00e9ricas***, como a quantidade de horas trabalhadas por semana (ou 'hours.per.week'), s\u00e3o representadas por n\u00fameros, o que faz com que elas sejam facilmente compreendidas pelo nosso computador. J\u00e1 as ***categ\u00f3ricas***, como o sexo de cada indiv\u00edduo (ou 'sex'), est\u00e3o divididas em categorias ('Male' e 'Female') e portanto s\u00e3o um pouco mais complicadas de se trabalhar inicialmente. Na limpeza de dados, n\u00f3s deveremos de alguma maneira transformar elas em n\u00fameros para que o nosso estimador consiga fazer c\u00e1lculos.","c19b2bc1":"Em seguida, vamos remover as colunas que ignoramos anteriormente:","823012b6":"Podemos agora separar os nossos dados em X e Y, sendo X as nossas vari\u00e1veis independentes e Y a nossa vari\u00e1vel de classe:","cdab3846":"Agora, podemos usar essa Pipeline para aplicar os dois passos anteriores com apenas um `fit_transform`:","f65eb3fd":"Dando uma olhada inicial nos nossos dados num\u00e9ricos, j\u00e1 podemos perceber algumas coisas interessantes. As vari\u00e1veis **'capital-gain'** e **'capital.loss'** parecem ser bem esparsas, com a maioria de seus dados sendo iguais a 0. Al\u00e9m disso, o **'capital.gain'** especialmente parece ter alguns outliers, em fun\u00e7\u00e3o de seu grande desvio padr\u00e3o e de seu m\u00e1ximo de **99999.0**.","499a4871":"Aplicaremos o preprocessamento:","19e7144d":"Finalmente! S\u00f3 basta exportar nosso DataFrame:","05ef3418":"Entretanto, vamos focar em analisar a distribui\u00e7\u00e3o de cada vari\u00e1vel em rela\u00e7\u00e3o \u00e0 vari\u00e1vel de classe **'income'**, o que pode ser feito por meio do m\u00e9todo `catplot`.\n\nCom o catplot, podemos tra\u00e7ar um scatter plot simples das nossas vari\u00e1veis:","9e684803":"<a id=\"subsection-three-one\"><\/a>\n## \u2697\ufe0f Pr\u00e9-Processamento\n\nPara processar nossos dados, vamos divid\u00ed-los em tr\u00eas partes diferentes: os dados **categ\u00f3ricos**, os dados **esparsos** e os dados **num\u00e9ricos**. Dessa forma, poderemos trabalhar com eles de maneira diferente.","b1fe7dba":"A primeira limpeza que faremos \u00e9 remover os dados duplicados. Para isto, podemos utilizar o m\u00e9todo `drop_duplicates` do nosso dataframe:","458b6357":"<a id=\"section-four\"><\/a>\n# \ud83d\udd2e Predi\u00e7\u00e3o\n\nTerminada a limpeza e o pr\u00e9-processamento dos dados, podemos enfim aplicar os nossos modelos de predi\u00e7\u00e3o! Anteriormente, hav\u00edamos utilizado um **classificador KNN**, enquanto agora deveremos comparar outros quatro classificadores diferentes. Nesse caso, vamos utilizar um **Support Vector Classifier**, um **Random Forest Classifier**, um **Extreme Gradient Boosting Classifier** e um **Neural Network Classifier**.","e6712c94":"## \u274e Extreme Gradient Boosting\n\nTamb\u00e9m calcularemos a **AUC** e a **acur\u00e1cia** do XGBoost nos dados de valida\u00e7\u00e3o:","c319fe04":"Olhando por cima, todas as nossas vari\u00e1veis parecem estar ligadas de alguma forma com a renda dos indiv\u00edduos.\n\nPrincipalmente nesse \u00faltimo gr\u00e1fico, voc\u00ea deve ter percebido as linhas pretas em cima de cada uma das barras. Essas linhas representam a varia\u00e7\u00e3o de renda dentro de cada grupo. A maioria dos pa\u00edses possui uma varia\u00e7\u00e3o muito grande, t\u00e3o grande que n\u00e3o podemos ter muita certeza de seu valor real.\n\nQuando analisamos a quantidade de indiv\u00edduos de cada na\u00e7\u00e3o no nosso dataset, essa varia\u00e7\u00e3o come\u00e7a a fazer mais sentido:","62bd7d4a":"Como \u00e9 poss\u00edvel perceber, esse tipo de visualiza\u00e7\u00e3o acaba n\u00e3o sendo t\u00e3o \u00fatil quando temos muitos dados. Apesar disso, podemos ver que indiv\u00edduos que trabalham poucas horas por semana tendem a ganhar menos de 50k por ano.\n\nPara visualizar melhor nossas distribui\u00e7\u00f5es, vamos utilizar os 'boxen plots', como apresentado a seguir:","15795539":"<a id=\"comparacao\"><\/a>\n# \u2696\ufe0f Compara\u00e7\u00e3o\n\nFinalmente, podemos comparar os nossos 4 modelos para descobrir qual obteve um resultado melhor. Neste caso, compararemos suas respectivas **acur\u00e1cias** e **AUCs**.","3dbd2273":"Em seguida, vamos testar o **SVC** com os hiperpar\u00e2metros padr\u00f5es:","767f4961":"Em seguida, vamos remover algumas colunas que n\u00e3o consideramos t\u00e3o importantes:","94c61138":"Em seguida, vamos otimizar os hiperpar\u00e2metros da Floresta Aleat\u00f3ria:","bd0d3214":"Agora a nossa coluna **income** cont\u00e9m somente valores num\u00e9ricos, com os quais poderemos trabalhar muito mais facilmente.","13afeebb":"Antes de qualquer coisa, vamos importar duas bibliotecas bem importantes para trabalhar com os nossos dados: *numpy* e *pandas*. Caso voc\u00ea n\u00e3o esteja muito familiarizado com essas bibliotecas, recomendo a leitura do seguinte texto:\n\n[Leitura Recomendada - Bibliotecas de Data Science](https:\/\/medium.com\/turing-talks\/turing-talks-6-data-science-libraries-6c2599838b3e)","78f92e65":"Nosso resultado \u00e9 esse:","bb896f2b":"<a id=\"section-one\"><\/a>\n# \ud83c\udfb2 Importando os Dados\n\nAntes de come\u00e7ar a fazer qualquer an\u00e1lise, precisamos antes importar os nossos dados. Para isto, utilizamos o m\u00e9todo `read_csv` da biblioteca **pandas** com o local do nosso arquivo. Tamb\u00e9m especificaremos que a coluna 'Id' s\u00e3o os \u00edndices da nossa tabela, e que os elementos com '?' ser\u00e3o considerados valores faltantes.","8678551b":"Ao final da transforma\u00e7\u00e3o, possu\u00edmos duas colunas num\u00e9ricas diferentes! A primeira indica se o nosso indiv\u00edduo \u00e9 do sexo feminino enquanto a segunda indica se ele \u00e9 do sexo masculino. Note que apenas uma dessas colunas pode ser igual a **1** para cada linha do nosso array.","d73701be":"Em seguida, podemos dar uma primeira olhada nos nossos dados com o m\u00e9todo `head` do nosso dataframe.","dadafdae":"Nosso modelo de **Extreme Gradient Boosting** conseguiu os melhores resultados, tanto para **AUC** quanto para **acur\u00e1cia** (a m\u00e9trica utilizada na competi\u00e7\u00e3o). Dessa forma, vamos utiliz\u00e1-lo para predizer os dados de teste.","f9d1fe98":"Em seguida, optei por copiar nosso dataframe para um *df_analysis*, j\u00e1 que ser\u00e1 preciso fazer uma pequena mudan\u00e7a para melhor visualizar os dados.","8108284d":"Entretanto, antes de qualquer coisa, precisamos processar nossos dados de valida\u00e7\u00e3o:","5d44dbf7":"Em seguida, vamos utilizar o `StandardScaler` para normalizar nossas vari\u00e1veis. \n\nEsse transformador padroniza as nossas features alterando sua m\u00e9dia para 0 e seu desvio padr\u00e3o para 1. Dessa forma, todas as vari\u00e1veis estar\u00e3o em uma mesma **'escala'**.","b6c61297":"<a id=\"section-three\"><\/a>\n# \ud83e\uddf9 Limpeza de Dados\n\nAgora que temos uma boa no\u00e7\u00e3o dos nossos dados, podemos come\u00e7ar a prepara\u00e7\u00e3o e a limpeza deles para facilitar o trabalho do nosso modelo. Caso queria ler um pouco mais sobre esse processo, voc\u00ea pode dar uma lida nesse texto:\n\n[Leitura Recomendada](https:\/\/medium.com\/turing-talks\/como-fazer-uma-limpeza-de-dados-completa-em-python-7abc9dfc19b8)","e603265f":"Finalmente, podemos aplicar todos esses passos no nosso dataset com apenas uma linha de c\u00f3digo:","3ded6c16":"<a id=\"section-two\"><\/a>\n# \ud83d\udcc8 Visualiza\u00e7\u00e3o de Dados\n\nAp\u00f3s a importa\u00e7\u00e3o, podemos come\u00e7ar o processo de **Visualiza\u00e7\u00e3o de Dados**, no qual tentaremos entender melhor a disposi\u00e7\u00e3o de nossos dados e definir como vamos trabalhar com eles. \n\n**[Leitura Recomendada - Visualiza\u00e7\u00e3o e An\u00e1lise de Dados](https:\/\/medium.com\/turing-talks\/como-visualizar-e-analisar-dados-com-python-f209bfbae68e)**\n\nNosso primeiro passo ser\u00e1 importar as bibliotecas *matplotlib* e *seaborn*, muito \u00fateis para a constru\u00e7\u00e3o de gr\u00e1ficos.","dcfda891":"**\u00c9 uma diferen\u00e7a bem grande!** \n\nEnquanto **30%** dos homens ganham mais que 50k por ano, somente **10%** das mulheres ultrapassam esse total.","3aa9a44d":"## \ud83c\udf32 Random Forest Classifier\n\nTamb\u00e9m calcularemos a **AUC** e a **acur\u00e1cia** da Floresta Aleat\u00f3ria nos dados de valida\u00e7\u00e3o:","1e55bb0b":"# \u2696\ufe0f Compara\u00e7\u00e3o de Classificadores - Dataset Adult\n\n### Bernardo Coutinho - PMR3508-2020-151\n\nGrande parte desse notebook \u00e9 id\u00eantica \u00e0 vers\u00e3o do trabalho de An\u00e1lise e Predi\u00e7\u00e3o do Dataset Adult com KNN. Dessa forma, caro queira ver somente os novos classificadores, basta pular para a se\u00e7\u00e3o **\ud83d\udd2e Predi\u00e7\u00e3o**.\n\n### \u00cdndice\n- [\ud83c\udfb2 Importando os Dados](#section-one)\n- [\ud83d\udcc8 Visualiza\u00e7\u00e3o de Dados](#section-two)\n  - [\ud83d\udd22 Visualizando Vari\u00e1veis Num\u00e9ricas](#subsection-two-one)\n  - [\ud83d\udcca Visualizando Vari\u00e1veis Categ\u00f3ricas](#subsection-two-two)\n- [\ud83e\uddf9 Limpeza de Dados](#section-three)\n  - [\u2697\ufe0f Pr\u00e9-Processamento](#subsection-three-one)\n- [\ud83d\udd2e Predi\u00e7\u00e3o](#section-four)\n  - [\ud83d\udcc9 Support Vector Classifier](#svc)\n  - [\ud83c\udf32 Random Forest Classifier](#rfc)\n  - [\u274e Extreme Gradient Boosting](#xgb)\n  - [\ud83e\udde0 Rede Neural](#NN)\n- [\u2696\ufe0f Compara\u00e7\u00e3o](#comparacao)\n- [\ud83d\udcc4 Submiss\u00e3o](#submissao)","12ad4e2f":"![](https:\/\/i.pinimg.com\/originals\/4e\/9e\/6f\/4e9e6f979347906a426adcbe57fd3259.gif)\n\nMuito obrigado por acompanhar esse notebook at\u00e9 o final!\n\nV\u00e1rios conte\u00fados aplicados na an\u00e1lise n\u00e3o seria poss\u00edvel sem [o material do Grupo Turing](https:\/\/medium.com\/turing-talks), que recomendo muito.\n\nSe quiser acompanhar nossos projetos, tamb\u00e9m temos um [GitHub](https:\/\/github.com\/orgs\/GrupoTuring\/dashboard) e um [site](http:\/\/www.grupoturing.com.br\/)!","13d118ac":"#### Juntando tudo\n\nCom todas as nossas Pipelines definidas, podemos junt\u00e1-las em apenas um transformador, que utilizaremos para pr\u00e9processar o dataset. Para tal, usaremos o `ColumnTransformer`, que aplicar\u00e1 as diferentes pipelines em suas respectivas colunas:","096dc8a3":"Temos um caso bem extremo de dados desbalanceados, a **grande maioria** dos nossos indiv\u00edduos \u00e9 dos Estados Unidos, 90% da quantidade total. Para efeito de compara\u00e7\u00e3o, o M\u00e9xico representa somente 2% do nosso dataset.\n\nApesar de existirem maneiras de lidar com esse tipo de problema, para simplificar optei por remover essa coluna.","f5ad8358":"<a id=\"subsection-two-one\"><\/a>\n## \ud83d\udd22 Visualizando Vari\u00e1veis Num\u00e9ricas\n\nComo primeiro gr\u00e1fico, vamos construir um **heatmap** representando a correla\u00e7\u00e3o entre as vari\u00e1veis num\u00e9ricas do nosso DataFrame:","2a80adc4":"Vamos fazer o mesmo para as outras vari\u00e1veis categ\u00f3ricas do nosso dataset:","e7cbcf38":"A desigualdade tamb\u00e9m est\u00e1 presente nesse caso. A etnia de cada indiv\u00edduo parece ter uma rela\u00e7\u00e3o bem grande com sua probabilidade de ganhar mais de 50k.","ad48e3d3":"<a id=\"xgb\"><\/a>\n## \u274e Extreme Gradient Boosting\n\nCaso voc\u00ea n\u00e3o esteja familiarizado com esse modelo, recomendo a leitura do seguinte texto:\n\n**[Leitura Recomendada - Ensemble Learning](https:\/\/medium.com\/turing-talks\/turing-talks-24-modelos-de-predi%C3%A7%C3%A3o-ensemble-learning-aa02ce01afda)**\n\nPara utilizar esse modelo, vamos instanciar a classe `XGBClassifier` da biblioteca **xgboost**:","337acccf":"## \ud83e\udde0 Rede Neural\n\nTamb\u00e9m calcularemos a **AUC** e a **acur\u00e1cia** da Rede Neural nos dados de valida\u00e7\u00e3o:","c48ad190":"<a id=\"NN\"><\/a>\n## \ud83e\udde0 Rede Neural\n\nPor fim, podemos testar uma rede neural do **scikit-learn** no nosso dataset.\n\nCaso voc\u00ea n\u00e3o esteja familiarizado com elas, recomendo a leitura do seguinte texto:\n\n**[Leitura Recomendada - Redes Neurais](https:\/\/medium.com\/turing-talks\/turing-talks-19-modelos-de-predi%C3%A7%C3%A3o-redes-neurais-1f165583a927)**\n\nEm seguida, vamos instanciar a classe `MLPClassifier` do **scikit-learn**:","552f475a":"#### Dados Categ\u00f3ricos\n\nO primeiro passo que tomaremos \u00e9 completar os dados faltantes do nosso dataset, para o qual usaremos a classe `SimpleImputer` do **scikit-learn**.\n\nO `SimpleImputer` \u00e9 um **transformador** que preenche os dados faltantes de cada vari\u00e1vel de acordo com uma estrat\u00e9gia que podemos escolher:\n\n - `mean` - Preenche os dados faltantes com a m\u00e9dia da coluna\n - `median` - Preenche os dados faltantes com a mediana da coluna\n - `most_frequent` - Preenche os dados faltantes com a moda da coluna\n \nPara os nossos dados categ\u00f3ricos, escolheremos a estrat\u00e9gia `most_frequent`!\n\nA seguir, temos uma explica\u00e7\u00e3o do funcionamento desse Imputer:","0999c395":"Conseguimos obter nosso resultado!","0e3e066b":"Cada quadrado do heatmap representa o coeficiente de correla\u00e7\u00e3o entre as vari\u00e1veis correspondentes nos eixos X e Y, e quanto mais pr\u00f3xima de amarelo for sua cor, maior a correla\u00e7\u00e3o positiva. Por exemplo, a correla\u00e7\u00e3o entre o **\"education.num\"** e a **\"income\"** \u00e9 de **0.34**, por isto sua cor \u00e9 mais clara que as outras.\n\nNo gr\u00e1fico, podemos observar que todas as nossas vari\u00e1veis possuem algum grau de correla\u00e7\u00e3o com a **\"income\"**, o valor que queremos predizer, exceto pela **\"fnlwgt\"**, que n\u00e3o parece estar relacionada com nenhuma outra vari\u00e1vel. Em fun\u00e7\u00e3o disto, parece n\u00e3o existir muito valor em utilizar essa vari\u00e1vel como feature, j\u00e1 que ela n\u00e3o ajudar\u00e1 a diferenciar os nossos dados.","336f32f2":"Vamos utilizar a classe **LabelEncoder** do **scikit-learn** para transformar nossa vari\u00e1vel de classe **income** em num\u00e9rica, mapeando todos os valores \"<=50K\" e \">50K\" em 0s e 1s.","9bc417ff":"Em seguida, dividiremos nosso dataset em dados de **treino** e de **valida\u00e7\u00e3o**:","f5bd00ca":"Em seguida, vamos reverter nosso **scikit-learn** para a vers\u00e3o 0.23.0 e realizar um pequeno bugfix. Esse passo s\u00f3 \u00e9 necess\u00e1rio porque a vers\u00e3o atual do sklearn conflita com uma biblioteca que utilizaremos durante a predi\u00e7\u00e3o, a **scikit-optimize**.","f510dbaa":"E plotaremos sua curva ROC:","e7f8f3cd":"Por fim, agora podemos criar a nossa **Pipeline** num\u00e9rica:","899c28b8":"Para analisar a distribui\u00e7\u00e3o de cada vari\u00e1vel, podemos utilizar o m\u00e9todo `distplot`:","21f152ce":"Agora conseguimos perceber melhor a distribui\u00e7\u00e3o das horas semanais bem como sua rela\u00e7\u00e3o com a renda de cada pessoa. Vamos tamb\u00e9m tra\u00e7ar esse gr\u00e1fico para nossas outras vari\u00e1veis:","4dff7a5c":"E plotaremos sua curva ROC:","6b527987":"<a id=\"submissao\"><\/a>\n# \ud83d\udcc4 Submiss\u00e3o\n\nFinalmente, agora podemos predizer os nossos dados teste e enviar uma submiss\u00e3o para o Kaggle.\n\nPrimeiramente, vamos importar os nossos dados:","529a1ac6":"#### Dados Num\u00e9ricos Esparsos\n\nPor fim, temos as colunas 'capital.gain' e 'capital.loss', que se diferem muito dos outros dados num\u00e9ricos por serem bem mais esparsas e possu\u00edrem mais outliers.\n\nPara pr\u00e9process\u00e1-las, vamos adotar os mesmos passos da Pipeline num\u00e9rica, apenas trocando o `StandardScaler` por um `RobustScaler`:","a3fa6650":"<a id=\"subsection-two-two\"><\/a>\n## \ud83d\udcca Visualizando Vari\u00e1veis Categ\u00f3ricas\n\nPara visualizar as vari\u00e1veis categ\u00f3ricas, precisaremos de outros tipos de gr\u00e1ficos para representar claramente nossas diferentes categorias. Nesse caso, vamos recorrer aos gr\u00e1ficos de barras para comparar a renda de v\u00e1rios grupos.\n\nO m\u00e9todo `catplot` da biblioteca *seaborn* \u00e9 uma \u00f3tima op\u00e7\u00e3o para tra\u00e7ar os gr\u00e1ficos com vari\u00e1veis categ\u00f3ricas, e s\u00f3 precisamos especificar o par\u00e2metro `kind` para obter v\u00e1rios tipos de gr\u00e1ficos diferentes.\n\nPrimeiramente, vamos representar o gr\u00e1fico do **sexo** de cada pessoa pela probabilidade de cada uma ganhar mais de 50k por ano:","9bf05683":"Para submeter nossa predi\u00e7\u00e3o, vamos export\u00e1-la no formato de `.csv`. Para isso, primeiro vamos criar um DataFrame:","e41deb34":"Tamb\u00e9m instanciaremos um **LabelEncoder** para transformar as classes em inteiros (para o c\u00e1lculo da AUC).","5862260f":"Agora que entendemos os nossos transformadores categ\u00f3ricos, podemos construir a nossa **Pipeline** categ\u00f3rica. \n\nUma Pipeline nada mais \u00e9 que uma sequ\u00eancia de transformadores do scikit-learn. Dessa forma, na c\u00e9lula a seguir estaremos criando uma Pipeline com os dois passos detalhados acima:\n\n - Um SimpleImputer\n - Um OneHotEncoder","6a519820":"Sem otimiza\u00e7\u00e3o de hiperpar\u00e2metros, nosso modelo conseguiu uma acur\u00e1cia de **80,1%**.\n\nAgora, vamos testar outros hiperpar\u00e2metros, e descobrir qual nos d\u00e1 um melhor resultado.","8e388b3e":"Em seguida, podemos tra\u00e7ar o mesmo gr\u00e1fico para as diferentes etnias do nosso dataset:","bc6ca1a3":"## \ud83d\udcc9 Support Vector Classifier\n\nPrimeiro, vamos calcular a **AUC** e a **acur\u00e1cia** do SVC nos dados de valida\u00e7\u00e3o:","8697d309":"Com o m\u00e9todo `info`, podemos observar o tipo de cada vari\u00e1vel da nossa tabela, bem como quantos valores n\u00e3o faltantes n\u00f3s temos."}}