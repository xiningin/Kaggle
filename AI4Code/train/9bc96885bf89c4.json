{"cell_type":{"fa16995f":"code","f2b752e6":"code","0dcdc4ea":"code","3d0584e6":"code","a2fb5341":"code","12ea47a4":"code","f9e1cd5a":"code","92554d5b":"code","2b30a70f":"code","c45f3296":"code","a40eabd8":"code","f93fd425":"code","7e2156fd":"code","e701614f":"code","6e7d4f72":"code","ccf7c19b":"code","af9baf28":"code","23b12299":"code","e40a7d3f":"code","d124c971":"code","23fb83ed":"code","0ef5cbc3":"code","a2823fde":"code","b39deb92":"code","0f00ddd8":"code","abba7e8e":"code","d66355d1":"code","6817efc9":"code","d23077ff":"code","9ca373fb":"code","a692f803":"code","4eaeed06":"code","63d1061d":"code","5dcdbfa8":"code","cfa83f4a":"code","aaabaae1":"code","fea963a9":"code","9fbbe614":"code","f7d04bba":"code","8ec6dd75":"code","3fa2cb42":"code","4ed79580":"code","6af9f7e8":"code","be1bb81a":"code","24cfdece":"code","852a33cb":"code","0aa44308":"code","88cc655c":"markdown","a1678da4":"markdown","55c48806":"markdown","92ed3c34":"markdown","87ff8204":"markdown","b2de1afe":"markdown","12caaca4":"markdown","b65cd20e":"markdown","f6d7d69f":"markdown","bcd2021d":"markdown","fb2fd084":"markdown","4a6f538e":"markdown","1b7d1872":"markdown","bccc57bb":"markdown","2bdacd7d":"markdown","9aeef3f3":"markdown","a8541443":"markdown","af68e38c":"markdown","8cf71ef7":"markdown","395ff277":"markdown","2eb7dc06":"markdown","1524013d":"markdown"},"source":{"fa16995f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import text_to_word_sequence, Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras import layers, Input, Model, models, optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.random import set_seed\n#pip install keras-tuner --upgrade\nimport keras_tuner as kt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\n\nfrom sklearn import svm\nfrom sklearn.metrics import *\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n","f2b752e6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        ","0dcdc4ea":"data = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding=\"ISO-8859-1\")","3d0584e6":"data.head()","a2fb5341":"data = data.iloc[:,:2]","12ea47a4":"data.info()","f9e1cd5a":"data['v1'].value_counts()\/data.shape[0]","92554d5b":"data['v1'].value_counts()","2b30a70f":"# let transform our labels to binary values\nY = LabelEncoder().fit_transform(data['v1']).reshape(-1,1)\nY.shape","c45f3296":"X = data['v2']\nX.shape","a40eabd8":"# Let check how many words we have in our dataset\n\n# tokenize the document\nwords = []\n\nfor i in range(0, X.shape[0]):\n    result = text_to_word_sequence(X[i])\n    words.extend(result)\n    \nlen(set(words))","f93fd425":"# Let see top 10 words, whether there are some useless words or single characters\npd.DataFrame(words).value_counts().head(10)","7e2156fd":"# Our stopwords which we'll remove are presented below\n#nltk.download('stopwords')\nprint(stopwords.words('english'))","e701614f":"# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\ndf = pd.DataFrame(X)\nstop_words = set(stopwords.words('english'))\n\n\ndf['clean'] = df['v2'].apply(lambda x: ' '.join([word for word in x.split() if not word.lower() in (stop_words)]))\nprint(df)","6e7d4f72":"# Let check again if there are still any useless words not included in stopwords\n\n# tokenize the document\nwords = []\ndf = df['clean']\n\nfor i in range(0, df.shape[0]):\n    result = text_to_word_sequence(df[i])\n    words.extend(result)\n    \nlen(set(words))\n\n    \npd.DataFrame(words).value_counts().head(20)\n#there are still some stopwords","ccf7c19b":"# let remove another stopwords, not defined before\n\nmore_stopwords = set(['u','4','2',\"i'm\",\"i'll\",'r','ur','n'])\nupdated_step_words = stop_words | more_stopwords\n\ndf1 = pd.DataFrame(df)\n\ndf1['clean_1'] = df1['clean'].apply(lambda x: ' '.join([word for word in x.split() if not word.lower() in (updated_step_words)]))\nprint(df1)","af9baf28":"#let steem words in our dataset\nps = PorterStemmer()\n\ndef porter(text):\n    text = nltk.word_tokenize(text) # split sentence to words\n    text_set = []                   # empty list\n    for i in text:                  # for each word do...\n        text_set.append(ps.stem(i)) # steem and append to list\n\n    return \" \".join(text_set)       # join every word in list back to sentence\n\ndf1['text_steemed'] =df1['clean_1'].apply(porter)\ndf1","23b12299":"# Let split our dataset on train and test\nX_train, X_test,Y_train, Y_test = train_test_split(df1['text_steemed'],Y, test_size= 0.2, random_state=1)","e40a7d3f":"# let check the longest sms (number of words) in our dataset\nnum_words = df1['text_steemed'].apply(lambda x: len(nltk.word_tokenize(x)))\nnum_words.max()","d124c971":"#How looks the range of words per messege\n_= plt.hist(num_words, bins = 100)\nprint('number of words represents 99% percentile is ', round(np.quantile(num_words,0.99),2))","23fb83ed":"#let create tokenizer to apply values for each word\nmax_words = 800\nmax_len = 42\n\ntok = Tokenizer(max_words,lower= True)\ntok.fit_on_texts(X_train) #token trainging on X_train\n\nsequences = tok.texts_to_sequences(X_train) #conversion from text to vectors\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len) #matrix with padding has been created\n\ntest_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","0ef5cbc3":"#let check records where after tokenizing we have only zeros\ntest_0 = pd.DataFrame({\"X\" : test_sequences_matrix.argmax(axis=1)})\ntest_0_filtred = test_0[test_0[\"X\"]<1]\n\ntest_to_remove = test_0_filtred.index\n\ntrain_0 = pd.DataFrame({\"X\" : sequences_matrix.argmax(axis=1)})\ntrain_0_filtred = train_0[train_0[\"X\"]<1]\n\ntrain_to_remove = train_0_filtred.index","a2823fde":"# let remove those zeros tokens from our dataset \n\n#X_train \nsequences_matrix = np.delete(sequences_matrix, train_to_remove,axis = 0 )\n\n#X_test\ntest_sequences_matrix =  np.delete(test_sequences_matrix, test_to_remove,axis = 0 )\n\n#Y_train\nY_train = np.delete(Y_train, train_to_remove, axis = 0)\n\n#Y_test\nY_test = np.delete(Y_test, test_to_remove, axis = 0)","b39deb92":"# to make reproduced results.\nnp.random.seed(400)","0f00ddd8":"#simple model as benchmark\n\nmodel1_svm = make_pipeline(MinMaxScaler(),svm.SVC())\nmodel1_svm.fit(sequences_matrix, Y_train.flatten())\n\ny_pred1_svm = model1_svm.predict(test_sequences_matrix)\n\n# evaluation\nprint(classification_report(Y_test.flatten(),y_pred1_svm))","abba7e8e":"# another simple model\n\nmodel2_svm = make_pipeline(MinMaxScaler(),svm.SVC(kernel = 'poly'))\nmodel2_svm.fit(sequences_matrix, Y_train.flatten())\n\ny_pred2_svm = model2_svm.predict(test_sequences_matrix)\n\n\n# evaluation\nprint(classification_report(Y_test.flatten(),y_pred2_svm))","d66355d1":"# another simple model\n\nmodel3_svm = make_pipeline(MinMaxScaler(),svm.SVC(kernel = 'linear'))\nmodel3_svm.fit(sequences_matrix, Y_train.flatten())\n\ny_pred3_svm = model3_svm.predict(test_sequences_matrix)\n\n\n# evaluation\nprint(classification_report(Y_test.flatten(),y_pred3_svm))","6817efc9":"# simple model as benchmark\n\nmodel1_knn = make_pipeline(MinMaxScaler(),KNeighborsClassifier())\nmodel1_knn.fit(sequences_matrix, Y_train.flatten())\n\ny_pred1_knn = model1_knn.predict(test_sequences_matrix)\n\n# evaluation\nprint(classification_report(Y_test.flatten(),y_pred1_knn))","d23077ff":"# simple model as benchmark\n\nmodel2_knn = make_pipeline(MinMaxScaler(),KNeighborsClassifier(weights = 'distance'))\nmodel2_knn.fit(sequences_matrix, Y_train.flatten())\n\ny_pred2_knn = model2_knn.predict(test_sequences_matrix)\n\n# evaluation\nprint(classification_report(Y_test.flatten(),y_pred2_knn))","9ca373fb":"# simple model as benchmark\n\nmodel3_knn = make_pipeline(StandardScaler(),KNeighborsClassifier(weights = 'distance'))\nmodel3_knn.fit(sequences_matrix, Y_train.flatten())\n\ny_pred3_knn = model3_knn.predict(test_sequences_matrix)\n\n# evaluation\nprint(classification_report(Y_test.flatten(),y_pred3_knn))","a692f803":"#simple model as benchmark\nmodel1_rf = RandomForestClassifier(random_state=1)\nmodel1_rf.fit(sequences_matrix, Y_train.flatten())\n\ny_pred1_rf = model1_rf.predict(test_sequences_matrix)\n\n# evaluation\nprint(classification_report(Y_test.flatten(),y_pred1_rf))","4eaeed06":"# first we have to prepare Y_train and Y_test to correct shape\nY_train_encoded = OneHotEncoder(sparse= False).fit_transform(Y_train)\nY_test_encoded = OneHotEncoder(sparse= False).fit_transform(Y_test)","63d1061d":"#set result repeatable\n\nset_seed(2)","5dcdbfa8":"# first DL model\ninput_layer = Input(shape = sequences_matrix.shape[1])\n\nx1 = layers.Dense(128, activation='relu')(input_layer)\nx1 = layers.Dense(64, activation='relu')(x1)\n\nout = layers.Dense(2)(x1)\n\nout = layers.Softmax()(out)\n\nmodel_dl1 = Model(inputs = input_layer, outputs = out)\nmodel_dl1.compile(optimizer= 'Adam', loss = 'binary_crossentropy', metrics = 'AUC')\n\nmodel_dl1.fit(sequences_matrix, Y_train_encoded, epochs = 20, validation_split=0.1, callbacks=EarlyStopping(patience=3, monitor='val_loss', min_delta = 0.02))\n\ny_pred_dl1 = model_dl1.predict(test_sequences_matrix).argmax(axis = 1) \n\nprint(classification_report(Y_test.flatten(),y_pred_dl1))","cfa83f4a":"# our model definition\ndef model_rnn(X_train,Y_train, units_rnn, embedd = 64, opt = 'adam', metrics = 'AUC', drop = 0, batch = 64, epochs = 30, verbose = True, display = 2):\n    \n    '''\n    X_train = training data without labels, in matrix format, prepared earlier as a sequences_matrix,\n    Y_train = labels for training data as an array(None, 2),\n    units_rnn = list of units per layer, same units per layer are recommended\n    embedd = dimension of embedding, default 64\n    opt = optimizer as a string,  default = 'adam'\n    drop = dropout size, default = 0\n    metric = metric to evaluate model results, default = 'auc'\n    batch = batch size, default 64, NOTE than batch size should be divided by 8\n    epochs = number of epochs, iteration of model, default = 30\n    verbose = whether print model summary or not, default = true (print)\n    display = whether print during fit progres bar (1), oneline summary per epoch (2) or nothing (0), default 2\n    \n    max_words = number of words used in model, parameter defined in preprocessing stage,\n    max_len = maximal lenght of one record, number of words in one record, parameter defined in preprocessing stage,\n    \n    '''\n    \n    # model instance\n    model = models.Sequential()\n    \n    # input layer\n    model.add(\n        layers.InputLayer(\n                    name = 'intro', \n                    input_shape = max_len\n        )\n    )\n    \n    # text embedding\n    model.add(\n        layers.Embedding(\n            input_dim = max_words,\n            output_dim=embedd\n        )\n    )\n    \n    # model with LSTM layers, and dense as an output\n    for i, j in enumerate(units_rnn):\n        if i == len(units_rnn)-1 and i == 0:\n            model.add(\n                layers.LSTM(\n                    units = j, \n                    name = 'rnn' + str(i)\n                )\n            )\n        \n        elif i == len(units_rnn)-1:\n             model.add(\n                layers.LSTM(\n                    units = j, \n                    name = 'rnn' + str(i)\n                )\n            )\n        \n        \n        else:\n            model.add(\n                layers.LSTM(\n                    units = j, \n                    return_sequences = True,\n                    name = 'rnn' + str(i)\n                )\n            )\n            \n    model.add(layers.Dropout(rate = drop))\n            \n    model.add(\n        layers.Dense(\n            units=2,\n            name = 'out'\n            )\n        )  \n    \n    \n    model.add(layers.Softmax())\n        \n    if verbose == True:\n        model.summary()\n    \n    \n    # model compile\n    model.compile(\n            loss = 'binary_crossentropy',\n            optimizer = opt,\n            metrics = metrics\n    )\n    \n    # model fit\n    model.fit(\n        x = X_train,\n        y = Y_train,\n        batch_size = batch,\n        epochs = epochs,\n        validation_split = 0.2,\n        verbose = display,\n        callbacks = [EarlyStopping(\n                            monitor ='val_loss',\n                            patience = 5,\n                            min_delta = 0.01\n                            )]\n    )\n    \n    \n    return model","aaabaae1":"model_rnn1 = model_rnn(sequences_matrix,Y_train_encoded,[64], display = 2)","fea963a9":"# model evaluation results\nres = model_rnn1.evaluate(test_sequences_matrix,Y_test_encoded, verbose=0)\nprint('Test set\\n  Loss: {:0.2f}\\n  AUC: {:0.2f}'.format(res[0],res[1]))","9fbbe614":"# classification report\ny_pred = model_rnn1.predict(test_sequences_matrix).argmax(axis = 1)\nprint(classification_report(Y_test, y_pred))","f7d04bba":"#precision score for class 'spam'\nprecision_score(Y_test, y_pred,pos_label=1)","8ec6dd75":"parameters = { \"units_rnn\" : [[64,64],[128],[128,128],[32],[32,32]]}    \n\nresults = pd.DataFrame({\"units_rnn\" : [], \"recall\" : []})\n\nlenght = len(parameters[\"units_rnn\"])\n\nfor i in range(lenght):\n    param = parameters.get(\"units_rnn\")[i]\n    model = model_rnn(sequences_matrix,Y_train_encoded,param, verbose = False)\n    y_pred = model.predict(test_sequences_matrix).argmax(axis = 1)\n    print('model' + str(i+1), \"\\n\")\n    \n    recall = recall_score(Y_test, y_pred,pos_label=1)\n    results = results.append({\"units_rnn\": param,\"recall\" : recall}, ignore_index = True)\n    \nprint(results)","3fa2cb42":"# let try with some droupout\nmodel_rnn2 = model_rnn(sequences_matrix,Y_train_encoded,[32], drop = 0.2)\ny_pred2 = model_rnn2.predict(test_sequences_matrix).argmax(axis = 1)\nprecision_score(Y_test, y_pred2,pos_label=1)","4ed79580":"print(classification_report(Y_test, y_pred2))","6af9f7e8":"# let try improve our model with keras tuner\n# first create model instance\n\ndef build_model(hp):\n        model = models.Sequential()\n        \n        model.add(\n        layers.InputLayer(input_shape = max_len)\n        )\n        \n        # text embedding\n        model.add(\n            layers.Embedding(\n                input_dim = max_words,\n                output_dim=hp.Int(\n                'embedding_size', min_value = 15, max_value = 100, step = 5, default = 50)\n            )\n        )\n        \n        model.add(\n            layers.LSTM(\n                hp.Choice('units', [8,16,32,64,128]),\n                        name = 'rnn1'\n            )\n        )\n          \n        model.add(\n            layers.Dense(\n                units=2,\n                name = 'out'\n            )\n        )     \n        \n        model.add(layers.Softmax())  \n\n        model.compile(\n                loss = 'binary_crossentropy',\n                optimizer = optimizers.Adam(\n                    hp.Float(\n                        'learning_rate',\n                        min_value=1e-4,\n                        max_value=1e-2,\n                        sampling='LOG',\n                        default=1e-3\n                    )),\n                metrics = 'AUC'\n        )\n              \n        return model","be1bb81a":"# let use Bayesian Optimization definition and start to search the best parameters set\ntuner = kt.BayesianOptimization(build_model, objective = 'val_loss',max_trials = 30,overwrite = True)\n\ntuner.search(sequences_matrix,Y_train_encoded,\n        validation_split = 0.2,\n        callbacks = [EarlyStopping(\n                            monitor ='val_loss',\n                            patience = 5,\n                            min_delta = 0.01\n                            )]\n    )","24cfdece":"# best model create,\nbest_model = tuner.get_best_models()[0]\n\ny_pred_best = best_model.predict(test_sequences_matrix).argmax(axis = 1)\nrecall_score(Y_test, y_pred_best,pos_label=1)","852a33cb":"#Our best model structure\nbest_model.summary()\n\n#and set of parameters\nprint(tuner.get_best_hyperparameters(1)[0].values)","0aa44308":"print(classification_report(Y_test, y_pred_best))","88cc655c":"How we can see, simple DL model hasn't improve our result.  \nModel with only dense layer can't predict spam class,\nlet improve it, by adding recurential layers.","a1678da4":"2. Data cleaning and preprocessing","55c48806":"c) RandomForrest","92ed3c34":"b) KNN","87ff8204":"How we can see there is no impact on results with dropout layer\n","b2de1afe":"How we can see our data set is unbalanced, 86.5% to 13.5% percent, in favor of not spam.","12caaca4":"How we can see the best results we achieved with less amount of layers, the best with 128 and 32 units and one layer","b65cd20e":"How we can see first, basic KNN model didn't improve our previous results, but with only one change in weight parameters, we beat out previous best result. \nFinally with StandardScaler instead of MinMaxScaller our results was a little bit better.  \nLet try with more another option.","f6d7d69f":"e) rnn networks","bcd2021d":"3. Text tokenize","fb2fd084":"How we can see, many of our popular words are stopwords, let remove them.","4a6f538e":"As we can see, model predict not-spam class very well, but reach only 58% f1_score for spam class.  \n\nWe are going to improve this model a little bit, by changing default parameters.","1b7d1872":"a) SVM","bccc57bb":"4. Models implementation","2bdacd7d":"f) Keras tuner","9aeef3f3":"Three basic models with default parameters, only with different kernel don't reach sufficient results.","a8541443":"After first try, in default model we reached the highest f1 score on this dataset.\nLet check more sofisticated model form keras library.","af68e38c":"In this case we had to detect spam in text messages.  \n\nThe best results we achived using neural network with:\n- 1 recurential layer and 128 Units,\n- emedding layer with size 100,\n- Dense layers with 2 units and softmax activation layers as an output \n- binary_crossentropy loss function,\n- Adam Optimizer with learning rate = 0.01\n\n\nOur model results is:\n- 99% accuracy in general, \n- 99% precision for both classes,\n- 91% recall in Spam class.\n\nOur results tell us that final model can detect 99% of spam, classifing correctly certain message as a spam with 91% probability (recall).","8cf71ef7":"d) simple neural network","395ff277":"5. Summary","2eb7dc06":"How we can see our model, have almost perfect match, it makes only some mistakes, and not spam messeges classify as a spam.\nLet's go to optimize recall_score for 'spam' class.","1524013d":"1. Data import"}}