{"cell_type":{"fbec859b":"code","4ffde266":"code","1452db2e":"code","871e93ce":"code","b5d32244":"code","ff9d73d7":"code","0f840801":"code","6f549661":"code","eb4ad7f1":"code","57669599":"code","a883b0a3":"code","4c520ea3":"code","52c754aa":"code","319ae43d":"code","fa9d9913":"code","e6e266a9":"code","6d62a0ee":"code","d0ac606d":"code","d7a60cfd":"code","75914757":"code","9172839d":"code","2ffef880":"code","4c8b34da":"code","363fa8d4":"code","6a391d49":"code","2581fae1":"code","c0926df6":"code","45a28dc3":"code","55b3ddf1":"code","373f0e3c":"markdown","8bbff7f3":"markdown","1b8c3956":"markdown","c2c12d20":"markdown"},"source":{"fbec859b":"import os\nimport cv2\nimport math\nimport numpy as np\nimport pandas as pd\n\nimport scikitplot\nimport seaborn as sns\nfrom matplotlib import pyplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Flatten, Dense, GlobalAvgPool2D, GlobalMaxPool2D\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.utils import plot_model\n\nfrom keras.utils import np_utils","4ffde266":"INPUT_PATH = \"..\/input\/fer13-cleaned-dataset\/\"","1452db2e":"total_images = 0\nfor dir_ in os.listdir(INPUT_PATH):\n    count = 0\n    for f in os.listdir(INPUT_PATH + dir_ + \"\/\"):\n        count += 1\n        total_images += 1\n    print(f\"{dir_} has {count} number of images\")\n    \nprint(f\"\\ntotal images are {total_images}\")","871e93ce":"TOP_EMOTIONS = [\"fear\", \"Happy\", \"Neutral\", \"Angry\"]\ntotal_images -= 380\ntotal_images","b5d32244":"img_arr = np.empty(shape=(total_images,48,48,3))\nimg_label = np.empty(shape=(total_images))\nlabel_to_text = {}\n\ni = 0\ne = 0\nfor dir_ in os.listdir(INPUT_PATH):\n    if dir_ in TOP_EMOTIONS:\n        label_to_text[e] = dir_\n        for f in os.listdir(INPUT_PATH + dir_ + \"\/\"):\n            img_arr[i] = cv2.imread(INPUT_PATH + dir_ + \"\/\" + f)\n            img_label[i] = e\n            i += 1\n        print(f\"loaded all {dir_} images to numpy arrays\")\n        e += 1\n\nimg_arr.shape, img_label","ff9d73d7":"label_to_text","0f840801":"fig = pyplot.figure(1, (8,8))\n\nidx = 0\nfor k in label_to_text:\n    sample_indices = np.random.choice(np.where(img_label==k)[0], size=4, replace=False)\n    sample_images = img_arr[sample_indices]\n    for img in sample_images:\n        idx += 1\n        ax = pyplot.subplot(4,4,idx)\n        ax.imshow(img[:,:,0], cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(label_to_text[k])\n        pyplot.tight_layout()","6f549661":"img_label = np_utils.to_categorical(img_label)\nimg_label.shape","eb4ad7f1":"img_arr = img_arr \/ 255.","57669599":"X_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=0.9, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","a883b0a3":"del img_arr\ndel img_label","4c520ea3":"img_width = X_train.shape[1]\nimg_height = X_train.shape[2]\nimg_depth = X_train.shape[3]\nnum_classes = y_train.shape[1]","52c754aa":"mobile_net = MobileNet(\n    input_shape = (img_width, img_height, img_depth),\n    include_top = False,\n    weights = \"imagenet\",\n    classes = num_classes\n)\n\nx = mobile_net.layers[-14].output\nglobal_pool = GlobalMaxPool2D(name=\"global_pool\")(x)\nout = Dense(num_classes, activation=\"softmax\", name=\"out_layer\")(global_pool)\n\nmodel = Model(inputs=mobile_net.input, outputs=out)","319ae43d":"plot_model(model, show_shapes=True, show_layer_names=True, expand_nested=True, dpi=50, to_file='mobilenet.png')","fa9d9913":"for layer in model.layers[:15]:\n    layer.trainable = False","e6e266a9":"train_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.15,\n    height_shift_range=0.15,\n    shear_range=0.15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n    zca_whitening=False,\n)\ntrain_datagen.fit(X_train)","6d62a0ee":"\"\"\"\nI used two callbacks one is `early stopping` for avoiding overfitting training data\nand other `ReduceLROnPlateau` for learning rate.\n\"\"\"\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.00008,\n    patience=11,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    min_delta=0.0001,\n    factor=0.25,\n    patience=4,\n    min_lr=1e-7,\n    verbose=1,\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]","d0ac606d":"batch_size = 25\nepochs = 40\n\noptims = [\n    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n    optimizers.Adam(0.01),\n]\n\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer=optims[1],\n        metrics=['accuracy']\n)\n\nhistory = model.fit_generator(\n    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n    validation_data=(X_test, y_test),\n    steps_per_epoch=len(X_train) \/ batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","d7a60cfd":"model_yaml = model.to_yaml()\nwith open(\"model_mobelnet.yaml\", \"w\") as yaml_file:\n    yaml_file.write(model_yaml)\n    \nmodel.save(\"model_moblenet.h5\")","75914757":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_mobilenet.png')\npyplot.show()","9172839d":"label_to_text","2ffef880":"text_to_label = dict((v,k) for k,v in label_to_text.items())\ntext_to_label","4c8b34da":"yhat_test = np.argmax(model.predict(X_test), axis=1)\nytest_ = np.argmax(y_test, axis=1)\n\nscikitplot.metrics.plot_confusion_matrix(ytest_, yhat_test, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_mobilenet.png\")\n\ntest_accu = np.sum(ytest_ == yhat_test) \/ len(ytest_) * 100\nprint(f\"test accuracy: {round(test_accu, 4)} %\\n\\n\")\n\nprint(classification_report(ytest_, yhat_test))","363fa8d4":"np.random.seed(42)\nfear_imgs = np.random.choice(np.where(y_test[:, text_to_label[\"fear\"]]==1)[0], size=9, replace=False)\nangry_imgs = np.random.choice(np.where(y_test[:, text_to_label[\"Angry\"]]==1)[0], size=9, replace=False)\n\nfig = pyplot.figure(1, (18, 4))\n\nfor i, (fear_idx, angry_idx) in enumerate(zip(fear_imgs, angry_imgs)):\n        sample_img = X_test[fear_idx,:,:,:]\n        sample_img = sample_img.reshape(1,*sample_img.shape)\n        pred = label_to_text[np.argmax(model.predict(sample_img), axis=1)[0]]\n\n        ax = pyplot.subplot(2, 9, i+1)\n        ax.imshow(sample_img[0,:,:,0], cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"t:fear, p:{pred}\")\n\n        sample_img = X_test[angry_idx,:,:,:]\n        sample_img = sample_img.reshape(1,*sample_img.shape)\n        pred = label_to_text[np.argmax(model.predict(sample_img), axis=1)[0]]\n\n        ax = pyplot.subplot(2, 9, i+10)\n        ax.imshow(sample_img[0,:,:,0], cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"t:angry, p:{pred}\")\n\n        pyplot.tight_layout()","6a391d49":"def plot_miss_classified(emotion):\n    miss_happy_indices = np.where((ytest_ != yhat_test) & (ytest_==text_to_label[emotion]))[0]\n    print(f\"total {len(miss_happy_indices)} miss labels out of {len(np.where(ytest_==text_to_label[emotion])[0])} for emotion {emotion}\")\n\n    cols = 15\n    rows = math.ceil(len(miss_happy_indices) \/ cols)\n    fig = pyplot.figure(1, (20, rows * 2))\n\n    for i,idx in enumerate(miss_happy_indices):\n        sample_img = X_test[idx,:,:,:]\n        sample_img = sample_img.reshape(1,*sample_img.shape)\n        pred = label_to_text[np.argmax(model.predict(sample_img), axis=1)[0]]\n\n        ax = pyplot.subplot(rows,cols,i+1)\n        ax.imshow(sample_img[0,:,:,0], cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"p:{pred}\")    ","2581fae1":"plot_miss_classified(emotion=\"Happy\")","c0926df6":"plot_miss_classified(emotion=\"fear\")","45a28dc3":"plot_miss_classified(emotion=\"Angry\")","55b3ddf1":"plot_miss_classified(emotion=\"Neutral\")","373f0e3c":"The confusion matrix clearly shows that our model is doing good job on the class `happy` but it's performance is not that good on other classes. One of the reason for this could be the fact that these classes have less data as compared to `happy`.","8bbff7f3":"One of the reason for such low accuracy is the data quality. Below I have shown all the miss-classified images. Many of them doesn't belong to their true class but are actually looking more likely to the predicted class.","1b8c3956":"As we see most of these are classified as `fear` the reason for that is that most of the fear mouths are opened as we see earlier and many of these mouths are opened like that.","c2c12d20":"`Splitting the data into training and validation set.`"}}