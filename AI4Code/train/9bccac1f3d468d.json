{"cell_type":{"859a7e97":"code","93290f0e":"code","72e61947":"code","4083b7e2":"code","c3b4ddb4":"code","a5997390":"code","be4eb227":"code","13bf5bb3":"code","aa1a1101":"code","6402154d":"code","4d8e2693":"code","f2b39304":"code","407ded71":"code","959dbe5d":"code","b585cd77":"code","d62aabe8":"code","0639cb7f":"code","37b72661":"code","b4bfb614":"code","eebff294":"code","ca75dfec":"code","6c2c8deb":"code","2e544d9e":"code","df2d51da":"code","a859f5c9":"code","2f4726bc":"code","54653e3c":"code","60685ac1":"code","c231ae07":"code","bdabd84b":"code","460938ac":"code","62a55865":"code","7bfe175e":"code","87dbe1db":"code","d7ef1dd1":"code","38a7a2f6":"code","070c1b96":"code","7cd28679":"code","7c6cfd2f":"code","cb48b43b":"code","cdda08ed":"code","dcbeb60c":"code","96641070":"code","cb9e6c49":"code","162c8e84":"code","4d8a5ce7":"code","2e764354":"code","501ac0dc":"code","296be41a":"code","7d8c9045":"code","8eda7a46":"code","bed6002e":"code","55bd5b8d":"code","7c23102c":"markdown","9b97582f":"markdown","25bc65dc":"markdown","147a7f82":"markdown","55ded9e8":"markdown","b8afb4f6":"markdown","615e069e":"markdown","edf9788b":"markdown","c762cc7a":"markdown","e63f61bd":"markdown","cc783f16":"markdown","3ab9d632":"markdown","14ea4f4d":"markdown","57364431":"markdown","dd296ad8":"markdown","14c4bec2":"markdown","7cfc6e7b":"markdown","dad8ce71":"markdown","704ffc7f":"markdown","727939b9":"markdown","b77f0317":"markdown","831145c5":"markdown","c0ef2ece":"markdown","b6f1bcb6":"markdown","fe84f084":"markdown","e01c4b82":"markdown","8be72b48":"markdown","c262e234":"markdown","57c88b5a":"markdown","3697141d":"markdown","8ffabd17":"markdown","ac8fe15b":"markdown","a14a2103":"markdown","bb7c4abb":"markdown","96f697bd":"markdown","820c6236":"markdown","e365863a":"markdown","19aa35ec":"markdown","b43b7c75":"markdown","b3cfb0c7":"markdown","d6649ebc":"markdown","3d13a3c6":"markdown","2b4aca26":"markdown","ba2c7241":"markdown"},"source":{"859a7e97":"#Importing all the Libraries needed\n#Data Preprocessing\nfrom ipywidgets import interact\nimport unidecode\nimport pandas as pd\nimport random\nimport json\nfrom collections import Counter\nfrom itertools import chain\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport re\n#https:\/\/pypi.org\/project\/tqdm\/ information on tqdm\nfrom tqdm import tqdm\ntqdm.pandas()\n\n#Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \ninit_notebook_mode(connected=True)\nimport plotly.offline as offline\nimport plotly.graph_objs as go\nimport plotly.express as px\n\n# Data Modeling\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom gensim.models import FastText, Word2Vec\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold, cross_validate, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import model_selection \nfrom nltk.stem import WordNetLemmatizer\nimport warnings\nwarnings.filterwarnings('ignore')","93290f0e":"def random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","72e61947":"train = pd.read_json(\"\/kaggle\/input\/whats-cooking-kernels-only\/train.json\")\ntest = pd.read_json(\"\/kaggle\/input\/whats-cooking-kernels-only\/test.json\")","4083b7e2":"train.info()","c3b4ddb4":"test.info()","a5997390":"train.head()","be4eb227":"train['cuisine'].unique()","13bf5bb3":"train['cuisine'].value_counts().plot.bar(color=random_colours(20),figsize=(16,6))","aa1a1101":"# Taking Out all the ingredients in the dataset and storing in a list\nraw_ingredients = [ing for ingredients in train['ingredients'] for ing in ingredients]","6402154d":"print('Maximum Number of Ingredients in a Dish: ',train['ingredients'].str.len().max())\nprint('Minimum Number of Ingredients in a Dish: ',train['ingredients'].str.len().min())","4d8e2693":"#no of Ingredients\ntrain['num_ing'] = train['ingredients'].str.len()","f2b39304":"plt.figure(figsize=(16,6))\nsns.distplot(train['num_ing'],kde =False ,bins=60)","407ded71":"plt.figure(figsize=(16,6))\nsns.countplot(x='num_ing',data=train)","959dbe5d":"longrecip = train[train['num_ing'] > 30]\nprint(len(longrecip))","b585cd77":"longrecip['cuisine'].value_counts()","d62aabe8":"print(longrecip[longrecip['num_ing'] == 65]['ingredients'].values)\nprint('Cuisine :-',longrecip[longrecip['num_ing'] == 65]['cuisine'].values)","0639cb7f":"shortrecip = train[train['num_ing']<=2]\nprint(len(shortrecip))","37b72661":"shortrecip['cuisine'].value_counts()","b4bfb614":"train[train['num_ing'] <= 1]","eebff294":"[ingredient for ingredient in raw_ingredients if len(ingredient) <= 2]","ca75dfec":"plt.figure(figsize=(20,8))\nsns.boxplot(x='cuisine',y='num_ing',data=train)","6c2c8deb":"' '.join(sorted([char for char in set(' '.join(raw_ingredients)) if re.findall('[^A-Za-z]', char)]))","2e544d9e":"list(set([ingredient for ingredient in raw_ingredients if re.findall('[A-Z]+', ingredient)]))[:5]","df2d51da":"list(set([ingredient for ingredient in raw_ingredients if '\u2019' in ingredient]))","a859f5c9":"list(set([ingredient for ingredient in raw_ingredients if re.findall('-', ingredient)]))[:5]","2f4726bc":"temp_ing = list(set([ingredient for ingredient in raw_ingredients if re.findall('[0-9]', ingredient)]))\ntemp_ing[:6]","54653e3c":"len(temp_ing)","60685ac1":"units = ['inch', 'oz', 'lb', 'ounc', '%'] # ounc is a misspelling of ounce?\n\n@interact(unit=units)\ndef f(unit):\n    ingredients_df = pd.DataFrame([ingredient for ingredient in raw_ingredients if unit in ingredient], columns=['ingredient'])\n    return ingredients_df.groupby(['ingredient']).size().reset_index(name='count').sort_values(['count'], ascending=False)","c231ae07":"keywords = ['american', 'greek', 'filipino', 'indian', 'jamaican', 'spanish', 'italian', 'mexican', 'chinese', 'thai',\n    'vietnamese', 'cajun', 'creole', 'french', 'japanese', 'irish', 'korean', 'moroccan', 'russian',\n]\nd ={}\nfor k in keywords:\n    temp = [ingredient for ingredient in raw_ingredients if k in ingredient]\n    d[k] = temp","bdabd84b":"d['american']","460938ac":"top = Counter([item for sublist in train['ingredients'] for item in sublist])\nprint(top.most_common(20))","62a55865":"temp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['ingredients','total_count']\nplt.figure(figsize=(7,9))\nsns.barplot(x='total_count',y='ingredients',data=temp)","7bfe175e":"labels = ['greek', 'southern_us', 'filipino', 'indian', 'jamaican',\n       'spanish', 'italian', 'mexican', 'chinese', 'british', 'thai',\n       'vietnamese', 'cajun_creole', 'brazilian', 'french', 'japanese',\n       'irish', 'korean', 'moroccan', 'russian']\ntemplist=[]\nfor cus in labels:\n    lisofing=[]\n    for lis in train[train['cuisine'] == cus]['ingredients']:\n        for ing in lis:\n            lisofing.append(ing)\n    templist.append([cus,len(list(set(lisofing)))])\nUnique_ing = pd.DataFrame(templist,columns=['cuisine','unique_ing']).sort_values(by='unique_ing',ascending=False)","87dbe1db":"plt.figure(figsize=(20,8))\nsns.barplot(x='cuisine',y='unique_ing',data=Unique_ing)","d7ef1dd1":"def cuisine_unique(cuisine,numingr,raw_ingredients):\n    '''\n    Input:\n        cuisine - cuisine category (ex. 'brazilian');\n        numingr - how many specific ingredients do you want to see in the final result; \n        allingredients - list  for item in train_data[train_data.cuisine == cuisine]['ingredients']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.cuisine != cuisine]['ingredients']:\n        for ingr in item:\n            allother .append(ingr)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_ingredients if x not in allother]\n    \n    mycounter = Counter()\n     \n    for item in train[train.cuisine == cuisine]['ingredients']:\n        for ingr in item:\n            mycounter[ingr] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    cuisinespec = pd.DataFrame(mycounter.most_common(numingr), columns = ['ingredient','count'])\n    \n    return cuisinespec","38a7a2f6":"cuisinespec= cuisine_unique('mexican', 10, raw_ingredients)\nprint(\"The top 10 unique ingredients in Mexican cuisine are:\")\ncuisinespec","070c1b96":"#Removing Outliers Values that were irrevelant to model\ntrain = train[train['num_ing'] > 1]","7cd28679":"train = train[train['num_ing']<60]","7c6cfd2f":"train.shape","cb48b43b":"lemmatizer = WordNetLemmatizer()\ndef preprocess(ingredients):\n    ingredients_text = ' '.join(ingredients)\n    ingredients_text = ingredients_text.lower() #Lower - Casing\n    ingredients_text = ingredients_text.replace('-', ' ') # Removing Hyphen\n    words = []\n    for word in ingredients_text.split():\n        word = re.sub(\"[0-9]\",\" \",word) #removing numbers,punctuations and special characters\n        word = re.sub((r'\\b(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\b'), ' ', word) # Removing Units\n        if len(word) <= 2: continue # Removing words with less than two characters\n        word = unidecode.unidecode(word) #Removing accents\n        word = lemmatizer.lemmatize(word) #Lemmatize\n        if len(word) > 0: words.append(word)\n    return ' '.join(words)","cdda08ed":"#Checking if our function works\nfor ingredient, expected in [\n    ('Eggs', 'egg'),\n    ('all-purpose flour', 'all purpose flour'),\n    ('pur\u00e9e', 'puree'),\n    ('1% low-fat milk', 'low fat milk'),\n    ('half & half', 'half half'),\n    ('safetida (powder)', 'safetida (powder)')\n]:\n    actual = preprocess([ingredient])\n    assert actual == expected, f'\"{expected}\" is excpected but got \"{actual}\"'","dcbeb60c":"train['x'] = train['ingredients'].progress_apply(preprocess)\ntest['x'] = test['ingredients'].progress_apply(preprocess)\ntrain.head()","96641070":"def apply_word2vec(sentences):\n    vectorizer = Word2Vec(\n        sentences,\n        size=500,\n        window=20,\n        min_count=3,\n        sg=1,\n        iter=20\n    )\n\n    def to_vector(sentence):\n        words = [word for word in sentence if word in vectorizer.wv.vocab]\n        if words:\n            return np.mean(vectorizer[words], axis=0)\n        else:\n            return np.zeros(500)\n\n    return np.array([to_vector(sentence) for sentence in sentences])\n\ndef apply_fasttext(sentences):\n    vectorizer = FastText(\n        size=500,\n        window=20,\n        min_count=3,\n        sg=1,\n        iter=20\n        )\n    vectorizer.build_vocab(sentences)\n    vectorizer.train(sentences, total_examples=vectorizer.corpus_count, epochs=vectorizer.iter)\n\n    def to_vector(sentence):\n        words = [word for word in sentence if word in vectorizer.wv.vocab]\n        if words:\n            return np.mean(vectorizer.wv[words], axis=0)\n        else:\n            return np.zeros(500)\n\n    return np.array([to_vector(sentence) for sentence in sentences])\n\ndef train_model(x, y, n_splits=3):\n    model = LogisticRegression(C=10, solver='sag', multi_class='multinomial', max_iter=300, n_jobs=-1)\n    i = 0\n    accuracies = []\n    kfold = KFold(n_splits)\n    for train_index, test_index in kfold.split(x):\n        classifier = LogisticRegression(C=10, solver='sag', multi_class='multinomial', max_iter=300, n_jobs=-1)\n        classifier.fit(x[train_index], y[train_index])\n        predictions = classifier.predict(x[test_index])\n        accuracies.append(accuracy_score(predictions, y[test_index]))\n        i += 1\n    average_accuracy = sum(accuracies) \/ len(accuracies)\n    return average_accuracy\n\ndef run_experiment(preprocessor):\n    train = json.load(open('\/kaggle\/input\/whats-cooking-kernels-only\/train.json'))\n\n    target = [doc['cuisine'] for doc in train]\n    lb = LabelEncoder()\n    y = lb.fit_transform(target)\n\n    x = preprocessor.fit_transform(train)\n\n    return train_model(x, y)\n","cb9e6c49":"import time\nresults = []\nfor (name, preprocessor) in [\n    ('TfidfVectorizer()', make_pipeline(\n        FunctionTransformer(lambda x: [\" \".join(doc['ingredients']).lower() for doc in x], validate=False),\n        TfidfVectorizer(),\n    )),\n    ('TfidfVectorizer(binary=True)', make_pipeline(\n        FunctionTransformer(lambda x: [\" \".join(doc['ingredients']).lower() for doc in x], validate=False),\n        TfidfVectorizer(binary=True),\n    )),\n    ('TfidfVectorizer(min_df=3)', make_pipeline(\n        FunctionTransformer(lambda x: [\" \".join(doc['ingredients']).lower() for doc in x], validate=False),\n        TfidfVectorizer(min_df=3),\n    )),\n    ('TfidfVectorizer(min_df=5)', make_pipeline(\n        FunctionTransformer(lambda x: [\" \".join(doc['ingredients']).lower() for doc in x], validate=False),\n        TfidfVectorizer(min_df=5),\n    )),\n    ('TfidfVectorizer(max_df=0.95)', make_pipeline(\n        FunctionTransformer(lambda x: [\" \".join(doc['ingredients']).lower() for doc in x], validate=False),\n        TfidfVectorizer(max_df=0.95),\n    )),\n     ('TfidfVectorizer(max_df=0.9)', make_pipeline(\n        FunctionTransformer(lambda x: [\" \".join(doc['ingredients']).lower() for doc in x], validate=False),\n        TfidfVectorizer(max_df=0.9),\n    )),\n    ('TfidfVectorizer(sublinear_tf=True)', make_pipeline(\n        FunctionTransformer(lambda x: [\" \".join(doc['ingredients']).lower() for doc in x], validate=False),\n        TfidfVectorizer(sublinear_tf=True),\n    )),\n    ('TfidfVectorizer(strip_accents=unicode)', make_pipeline(\n        FunctionTransformer(lambda x: [\" \".join(doc['ingredients']).lower() for doc in x], validate=False),\n        TfidfVectorizer(strip_accents='unicode'),\n    )),\n]:\n    start = time.time()\n    accuracy = run_experiment(preprocessor)\n    execution_time = time.time() - start\n    results.append({\n        'name': name,\n        'accuracy': accuracy,\n        'execution time': f'{round(execution_time, 2)}s'\n    })\npd.DataFrame(results, columns=['name', 'accuracy', 'execution time']).sort_values(by='accuracy', ascending=False)","162c8e84":"vectorizer = TfidfVectorizer(sublinear_tf=True)","4d8a5ce7":"X_train = vectorizer.fit_transform(train['x'].values)\nX_train.sort_indices()\nX_test = vectorizer.transform(test['x'].values)","2e764354":"label_encoder = LabelEncoder()\nY_train = label_encoder.fit_transform(train['cuisine'].values)","501ac0dc":"classifier = SVC(C=100, # penalty parameter\n\t \t\t\t kernel='rbf', # kernel type, rbf working fine here\n\t \t\t\t degree=3, # default value\n\t \t\t\t gamma=1, # kernel coefficient\n\t \t\t\t coef0=1, # change to 1 from default value of 0.0\n\t \t\t\t shrinking=True, # using shrinking heuristics\n\t \t\t\t tol=0.001, # stopping criterion tolerance \n\t      \t\t probability=False, # no need to enable probability estimates\n\t      \t\t cache_size=200, # 200 MB cache size\n\t      \t\t class_weight=None, # all classes are treated equally \n\t      \t\t verbose=False, # print the logs \n\t      \t\t max_iter=-1, # no limit, let it run\n          \t\t decision_function_shape=None, # will use one vs rest explicitly \n          \t\t random_state=None)","296be41a":"model = OneVsRestClassifier(classifier, n_jobs=4)\nmodel.fit(X_train, Y_train)","7d8c9045":"print (\"Predict on test data ... \")\nY_test = model.predict(X_test)\nY_pred = label_encoder.inverse_transform(Y_test)","8eda7a46":"Y_pred[:20]","bed6002e":"test_id = test['id']\nsub = pd.DataFrame({'id': test_id, 'cuisine': Y_pred}, columns=['id', 'cuisine'])\nsub.to_csv('submission.csv', index=False)","55bd5b8d":"sub.head()","7c23102c":"Apparently Not all the Words make sense , We will keep this in mind","9b97582f":"### Let's Take a closer look at the extremes","25bc65dc":"# Chapter 1 :- A Date with Data  \nAs the Experts always Say : Always Know your Data\n<br>I wouldn't say that knowing your data is the most difficult thing in data science, but it is time-consuming. Therefore, it's easy to overlook this initial step and jump too soon into the water.\nSo I tried to learn how to swim before jumping into the water\n\n<br>In this section we will go on a date 'Data'. We will explore the data thoroughly , what its like, how it is distributed , what is the target , what affects the targets most ,etc","147a7f82":"From the plot of label distribution, we observe that the most common category in our sample is the Italian cuisine, followed by the Mexican. The least represented cuisines are the Irish, Jamaican, Russian and Brazilian - counting for only 6% of our training sample of recipes.","55ded9e8":"### Units","b8afb4f6":"Explore recipe length distribution in each cuisine ","615e069e":"### Regions\nWe will explore whether or not the name of the region is present in the name of ingredients , For Eg:- Greek Yogurt. If found this will help the model and an unigram model will perform better","edf9788b":"# Chapter :- Exploring the Unknowns","c762cc7a":"We will remove all the units from the ingredients as it will not be helpful for the model ","e63f61bd":"It seems that 40 recipes consist of more than 30 ingredients!\n","cc783f16":"From the above bar chart, we can see that it is not necessary that cuisines with more instances in the training sample should be associated with more ingredients representing them. It turns out that the French cuisine which is 6,65% of the training sample has more variability in ingredients than the Indian cuisine (this observation is unexpected since Indians use many spices in their recipes","3ab9d632":"**It seems that 215 recipes consist of less than or equal to 2 ingredients**","14ea4f4d":"### Exploring Most Common Ingredients in the whole dataset\nWe make use of counter function from the collections module here . Help on this function can be found from here :\nhttps:\/\/www.youtube.com\/watch?v=cDw3ppRKAck ","57364431":"In this section we will :-\n* Remove outliers\n* convert to lowercase\n* remove hyphen\n* remove numbers\n* remove words which consist of less than 2 characters\n* remove units\n* remove accents\n* lemmatize","dd296ad8":"The Special Characters have come from ingredients from a specific company or from ingredients which come in packaging:\n- \"Bertolli\u00ae Alfredo Sauce\"\n- \"Progresso\u2122 Chicken Broth\"\n- \"green bell pepper, slice\"\n- \"half & half\"\n- \"asafetida (powder)\"\n- \"Spring! Water\"","14c4bec2":"Now That we have completed cleaning and changed our data for the better its time to chose the method which we will use to tranform text into numbers","7cfc6e7b":"# Chapter 4:- The Final Decision","dad8ce71":" ### Apostrophes","704ffc7f":"The distribution of recipe length is right-skewed as we can see from the histogram above.","727939b9":"# Chapter 2 :- Experiencing the Change","b77f0317":"**Time To Look At The Ingredients and their Distribution among different Cuisines**","831145c5":"The longest recipe in our sample is part of the Italian cuisine. However, Italian cuisine is often associated with simple recipes. Also 65 ingredients can be considered too many for any recipe.So, My guess is that the case is more about wrong data in our sample rather than anything else","c0ef2ece":"The test and train data provided in this Kaggle competition are in json format. We have imported the data as a data frame object and the above lines of code show us the initial look of both samples.\n\n### So What can we Expect ?\n- There are 39774 unique recipies belonging to different cusines in the train set\n- There are 9944 unique recipies in the test set\n- Cuisine is the target variable\n- The Ingredients for every recipe is given as a list\n- Their are recipies from 20 different Cuisines. **This means that the problem at hand is a multi-class classification**","b6f1bcb6":"### Exploring how many different ingredients can be found in each ","fe84f084":"We need to convert ingredeints to numeric values so that computers can do mathematical operations. My question was what the best representation is for this dataset.\n\nI have compared representations below.\n\n* CountVectorizer\n* TfidfVectorizer\n* TfidfVectorizer + SVD\n* Word2Vec\n* fastText","e01c4b82":"**Creating A Feature that counts the Number Of Ingredients used in a given Recipe**","8be72b48":"I tried LogisticRegression, GaussianProcessClassifier, GradientBoostingClassifier, MLPClassifier, LGBMClassifier, SGDClassifier, Keras but SVC was working the best,So I choose SVC to build our final model","c262e234":"### Hyphens ( - )\nWe will Replace (-)  with null string (\"\") later on if found","57c88b5a":"# What's Cooking:- Building a Text Classifier\n> ![image.png](attachment:image.png)\n\n<br>This Kaggle competition asks us to predict the category of a dish's cuisine given a list of its ingredients.<b>This Notebook explores and analyses this dataset to build a Text Classifier<\/b>. It can also serve as a great starting point for learning how to explore, manipulate, transform and learn from textual data . \n<br><br> This Notebook is divided into the following sections:-\n* **A Date with Data**  : As they say, to know someone better you must meet him\/her in person. In this section we will go on a date with data to explore and visualize it to gain insights \n* **Experiencing the Change** : As you and data fall in love, data experiences positive changes. In this section data is cleaned and pre-processed (which is a positive change) for model development.\n* **Exploring the Unknowns**: As the relationship between you and data goes on, you tend to explore and dig into what suits data the most. In this section we will explore which Feature Engineering technique for representing and converting text data gives the best results.\n* **The Final Decision** : Its time to build the Model\n\n<br> Let's Get the Party Started","3697141d":"From the box plots of recipe length distributions, we can make several observations:\n\n- The Moroccan cuisine seems to have the longest recipes on average compared to all the rest cuisines in our sample\n- We observe the opposite phenomenon for the Irish, British, French and Southern_us cuisine\n- There exist outliers in all cuisines\n- Recipes part of the European cuisine tend to be with average length or shorter compared to the rest of the sample.\n","8ffabd17":"It seems that salt is the most commonly used ingredient which is not surprising at all! We also find water, onions, garlic and olive oil - not so surprising also .\n**Salt is such a common ingredient that we expect it to have poor predictive power in recognizing the type of cuisine**\nWe will remove salt and some other common ingredients from the mix to make our model do better","ac8fe15b":"So we can see there are region names present , we will keep this in our mind","a14a2103":"We Can See recipes with only one ingredient which is very small and recipes with more than 50 ingredients which is insanely High. These are outliers and could get our model confused , unfortunately there some examples with single ingredient and high ingredients in test set as well","bb7c4abb":"But, are all ingredients valid? For example, do ingredients consisting of less than 2 characters make sense?","96f697bd":"**Explore the ingredients in the longest recipe in our training set**","820c6236":"Creating a plot to See the Distribution of Different type of Cuisines in the Train Data","e365863a":"### Numbers","19aa35ec":"### Special Characters","b43b7c75":"It's Time to build the model , first I will convert text data into numerical data using tfidf and then use SVC to build a model","b3cfb0c7":"### Upper Cases\nSince Company names and Region Names are used in ingredients it may have been written in uppercase so checking for that","d6649ebc":"https:\/\/pypi.org\/project\/Unidecode\/\nInformation about unidecode can be read from here","3d13a3c6":"This is My first kernel on kaggle. I have learnt a lot from this competition ,from the kaggle community and some exceptionally well written kernels.\n<br>The kernel By Rejasupotaro https:\/\/www.kaggle.com\/rejasupotaro\/representations-for-ingredients , helped me a lot and taught me how to compare all the Feature Engineering Techniques and choose the best one.\n<br>I wrote this kernel so that anyone starting with machine learning and natural language processing can understand it.\n### If you find my kernel useful and like it, please don't forget to upvote. Thank you!","2b4aca26":"So, We can see Tf-Idf is the Method suitable for our Data","ba2c7241":"**Numbers show quantity or density. Quantities can be a factor of identifying the cuisine , but the number of ingredients containing numbers is just 40 so it will be better to remove the numbers**"}}