{"cell_type":{"ecdb2a57":"code","0869c3cc":"code","9a8f49f7":"code","02e35977":"code","431da44a":"code","c96ab20f":"code","0e5c2d7c":"code","a62ad614":"code","70e24b0c":"code","c2347d0b":"code","493e2727":"code","d29e85bb":"code","2cfa2c03":"code","ff506997":"code","b645e464":"code","05a4cbbb":"code","a6db3e19":"code","67bbf297":"code","7275a2fa":"code","ddc39416":"code","3200a1f1":"code","9aeab913":"code","74de4234":"code","296f08a3":"code","f7634bae":"code","e0062eb0":"code","5e973d5f":"code","8a770bff":"code","62b26695":"code","d7f12fea":"code","8cf58ada":"code","916636ca":"code","3f417b66":"code","5e4d6587":"code","64af52df":"code","441c879b":"code","3873b299":"code","f1727e84":"code","75a5105c":"code","1249bfa8":"code","b4c3f079":"code","7aff2366":"code","53a73fd4":"code","93aaab81":"code","5a6ba778":"code","f2d382de":"code","84c59bc8":"code","49fd9b1f":"code","ce0f6c2c":"code","540d1084":"code","cd191f9f":"code","8bf61984":"code","bb3380b4":"code","4097f061":"code","e864c0ed":"code","40e0a464":"code","57314346":"code","5cb7734a":"code","568c4b2b":"code","eaa8df6c":"code","cf6af753":"code","f8598233":"code","4108ec69":"code","ef5eae71":"code","bd61900a":"code","6e1f5fdb":"code","3f2fe39d":"code","42e2a2dc":"code","c8f67cfe":"code","14d62d8d":"code","cd49b775":"code","4cd703f9":"code","263f1659":"code","e5f0d605":"code","06c059dd":"code","0ad81192":"code","0ce75e0a":"code","488141bf":"code","b326a124":"code","d7c918db":"code","fdd6b1b9":"code","95efc6d8":"code","bd11e7d9":"code","a7dafbcc":"code","190e71eb":"code","25ee2466":"code","dfa15d06":"code","8e6ebace":"code","bb936c8d":"code","97e8ab9f":"code","8687d648":"code","86af68c7":"code","5f8bf5a8":"code","16c9dbfb":"code","576c4f50":"code","496051c0":"code","6cdfcdb9":"code","6f5152b2":"code","583e4f48":"code","8b33b925":"code","5cb3927a":"code","20f7c93c":"code","118f6aad":"code","93e538af":"code","b53998c0":"code","3bc57cfe":"markdown","7b754c4a":"markdown","a7868eda":"markdown","9e3188cb":"markdown","50c0ad1a":"markdown","f69a9fdb":"markdown","6b49e088":"markdown","e3643b10":"markdown","23c3b7ae":"markdown","af788a4a":"markdown","58516741":"markdown","6bab0871":"markdown","27242e81":"markdown","92a7a958":"markdown","c2293f9f":"markdown","ac28db85":"markdown","deb28d67":"markdown","b57f87d7":"markdown","bc508ba8":"markdown","dad66782":"markdown","2da3eb51":"markdown","6a05c354":"markdown","6e104f81":"markdown","8a9e7259":"markdown","04d4e33f":"markdown","be8867af":"markdown","f52e57c1":"markdown","29c98158":"markdown","0b0b12af":"markdown","56ab250f":"markdown","5dafc65f":"markdown","19e4ad78":"markdown","3f8086f0":"markdown","49937cf3":"markdown","13226020":"markdown","70b69300":"markdown","157c0922":"markdown","7b1579c5":"markdown","36d73cce":"markdown","dd8b5a60":"markdown","966e3d24":"markdown","e798eb12":"markdown","8b3d5e7e":"markdown","c30a1a75":"markdown","d90bbc37":"markdown","a7433a08":"markdown","c8e3c80a":"markdown","818d570b":"markdown","3cc0ca2c":"markdown","a8cab17e":"markdown","1a2d4ef5":"markdown","f57358c3":"markdown","064012a2":"markdown"},"source":{"ecdb2a57":"import numpy as np \nimport pandas as pd \nimport os\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, RidgeCV\nimport gc\nfrom catboost import CatBoostRegressor\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 500)\n\n# import workalendar\n# from workalendar.america import Brazil","0869c3cc":"%%time\ntrain = pd.read_csv('..\/input\/train.csv', parse_dates=['first_active_month'])\ntest = pd.read_csv('..\/input\/test.csv', parse_dates=['first_active_month'])\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')","9a8f49f7":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","02e35977":"e = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='train')\ne","431da44a":"train['feature_1'] = train['feature_1'].astype('category')\ntrain['feature_2'] = train['feature_2'].astype('category')\ntrain['feature_3'] = train['feature_3'].astype('category')\ntrain.head()","c96ab20f":"train.info()","0e5c2d7c":"fig, ax = plt.subplots(1, 3, figsize = (16, 6))\nplt.suptitle('Violineplots for features and target');\nsns.violinplot(x=\"feature_1\", y=\"target\", data=train, ax=ax[0], title='feature_1');\nsns.violinplot(x=\"feature_2\", y=\"target\", data=train, ax=ax[1], title='feature_2');\nsns.violinplot(x=\"feature_3\", y=\"target\", data=train, ax=ax[2], title='feature_3');","a62ad614":"fig, ax = plt.subplots(1, 3, figsize = (16, 6));\ntrain['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='feature_1');\ntrain['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='feature_2');\ntrain['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='feature_3');\nplt.suptitle('Counts of categiories for features');","70e24b0c":"test['feature_1'] = test['feature_1'].astype('category')\ntest['feature_2'] = test['feature_2'].astype('category')\ntest['feature_3'] = test['feature_3'].astype('category')","c2347d0b":"d1 = train['first_active_month'].value_counts().sort_index()\nd2 = test['first_active_month'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Counts of first active\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","493e2727":"test.loc[test['first_active_month'].isna(), 'first_active_month'] = test.loc[(test['feature_1'] == 5) & (test['feature_2'] == 2) & (test['feature_3'] == 1), 'first_active_month'].min()","d29e85bb":"plt.hist(train['target']);\nplt.title('Target distribution');","2cfa2c03":"print('There are {0} samples with target lower than -20.'.format(train.loc[train.target < -20].shape[0]))","ff506997":"max_date = train['first_active_month'].dt.date.max()\ndef process_main(df):\n    date_parts = [\"year\", \"weekday\", \"month\"]\n    for part in date_parts:\n        part_col = 'first_active_month' + \"_\" + part\n        df[part_col] = getattr(df['first_active_month'].dt, part).astype(int)\n            \n    df['elapsed_time'] = (max_date - df['first_active_month'].dt.date).dt.days\n    \n    return df","b645e464":"train = process_main(train)\ntest = process_main(test)","05a4cbbb":"historical_transactions = pd.read_csv('..\/input\/historical_transactions.csv')\ne = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='history')\ne","a6db3e19":"print(f'{historical_transactions.shape[0]} samples in data')\nhistorical_transactions.head()","67bbf297":"# let's convert the authorized_flag to a binary value.\nhistorical_transactions['authorized_flag'] = historical_transactions['authorized_flag'].apply(lambda x: 1 if x == 'Y' else 0)","7275a2fa":"print(f\"At average {historical_transactions['authorized_flag'].mean() * 100:.4f}% transactions are authorized\")\nhistorical_transactions['authorized_flag'].value_counts().plot(kind='barh', title='authorized_flag value counts');","ddc39416":"autorized_card_rate = historical_transactions.groupby(['card_id'])['authorized_flag'].mean().sort_values()\nautorized_card_rate.head()","3200a1f1":"autorized_card_rate.tail()","9aeab913":"historical_transactions['installments'].value_counts()","74de4234":"historical_transactions.groupby(['installments'])['authorized_flag'].mean()","296f08a3":"historical_transactions['installments'] = historical_transactions['installments'].astype('category')","f7634bae":"historical_transactions['purchase_date'] = pd.to_datetime(historical_transactions['purchase_date'])","e0062eb0":"plt.title('Purchase amount distribution.');\nhistorical_transactions['purchase_amount'].plot(kind='hist');","5e973d5f":"for i in [-1, 0]:\n    n = historical_transactions.loc[historical_transactions['purchase_amount'] < i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount less than {i}.\")\nfor i in [0, 10, 100]:\n    n = historical_transactions.loc[historical_transactions['purchase_amount'] > i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount more than {i}.\")","8a770bff":"plt.title('Purchase amount distribution for negative values.');\nhistorical_transactions.loc[historical_transactions['purchase_amount'] < 0, 'purchase_amount'].plot(kind='hist');","62b26695":"map_dict = {'Y': 0, 'N': 1}\nhistorical_transactions['category_1'] = historical_transactions['category_1'].apply(lambda x: map_dict[x])\nhistorical_transactions.groupby(['category_1']).agg({'purchase_amount': ['mean', 'std', 'count'], 'authorized_flag': ['mean', 'std']})","d7f12fea":"historical_transactions.groupby(['category_2']).agg({'purchase_amount': ['mean', 'std', 'count'], 'authorized_flag': ['mean', 'std']})","8cf58ada":"map_dict = {'A': 0, 'B': 1, 'C': 2, 'nan': 3}\nhistorical_transactions['category_3'] = historical_transactions['category_3'].apply(lambda x: map_dict[str(x)])\nhistorical_transactions.groupby(['category_3']).agg({'purchase_amount': ['mean', 'std', 'count'], 'authorized_flag': ['mean', 'std']})","916636ca":"for col in ['city_id', 'merchant_category_id', 'merchant_id', 'state_id', 'subsector_id']:\n    print(f\"There are {historical_transactions[col].nunique()} unique values in {col}.\")","3f417b66":"def aggregate_historical_transactions(trans, prefix):\n    # more features from this kernel: https:\/\/www.kaggle.com\/chauhuynh\/my-first-kernel-3-699\n    trans['purchase_month'] = trans['purchase_date'].dt.month\n#     trans['year'] = trans['purchase_date'].dt.year\n#     trans['weekofyear'] = trans['purchase_date'].dt.weekofyear\n#     trans['month'] = trans['purchase_date'].dt.month\n#     trans['dayofweek'] = trans['purchase_date'].dt.dayofweek\n#     trans['weekend'] = (trans.purchase_date.dt.weekday >=5).astype(int)\n#     trans['hour'] = trans['purchase_date'].dt.hour\n    trans['month_diff'] = ((datetime.datetime.today() - trans['purchase_date']).dt.days)\/\/30\n    trans['month_diff'] += trans['month_lag']\n    trans['installments'] = trans['installments'].astype(int)\n\n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']). \\\n                                        astype(np.int64) * 1e-9\n    trans = pd.get_dummies(trans, columns=['category_2', 'category_3'])\n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['sum', 'mean'],\n        'category_2_1.0': ['mean', 'sum'],\n        'category_2_2.0': ['mean', 'sum'],\n        'category_2_3.0': ['mean', 'sum'],\n        'category_2_4.0': ['mean', 'sum'],\n        'category_2_5.0': ['mean', 'sum'],\n        'category_3_1': ['sum', 'mean'],\n        'category_3_2': ['sum', 'mean'],\n        'category_3_3': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_month': ['mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp, 'max', 'min'],\n        'month_lag': ['min', 'max'],\n        'merchant_category_id': ['nunique'],\n        'state_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'city_id': ['nunique'],\n        'month_diff': ['min', 'max', 'mean']\n    }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n\n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n\n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n\n    return agg_trans","5e4d6587":"def aggregate_per_month(history):\n    grouped = history.groupby(['card_id', 'month_lag'])\n    history['installments'] = history['installments'].astype(int)\n    agg_func = {\n            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            }\n\n    intermediate_group = grouped.agg(agg_func)\n    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n    intermediate_group.reset_index(inplace=True)\n\n    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n    final_group.reset_index(inplace=True)\n    \n    return final_group\n\nfinal_group = aggregate_per_month(historical_transactions) ","64af52df":"%%time\ndel d1, d2, autorized_card_rate\ngc.collect()\nhistorical_transactions = reduce_mem_usage(historical_transactions)\nhistory = aggregate_historical_transactions(historical_transactions, prefix='hist_')\nhistory = reduce_mem_usage(history)\ngc.collect()\n","441c879b":"train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\ndel history","3873b299":"del historical_transactions\ngc.collect()","f1727e84":"new_merchant_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv')\ne = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='new_merchant_period')\ne","75a5105c":"print(f'{new_merchant_transactions.shape[0]} samples in data')\nnew_merchant_transactions.head()","1249bfa8":"# let's convert the authorized_flag to a binary value.\nnew_merchant_transactions['authorized_flag'] = new_merchant_transactions['authorized_flag'].apply(lambda x: 1 if x == 'Y' else 0)","b4c3f079":"print(f\"At average {new_merchant_transactions['authorized_flag'].mean() * 100:.4f}% transactions are authorized\")\nnew_merchant_transactions['authorized_flag'].value_counts().plot(kind='barh', title='authorized_flag value counts');","7aff2366":"card_total_purchase = new_merchant_transactions.groupby(['card_id'])['purchase_amount'].sum().sort_values()\ncard_total_purchase.head()","53a73fd4":"card_total_purchase.tail()","93aaab81":"new_merchant_transactions['installments'].value_counts()","5a6ba778":"new_merchant_transactions.groupby(['installments'])['purchase_amount'].sum()","f2d382de":"new_merchant_transactions['installments'] = new_merchant_transactions['installments'].astype('category')","84c59bc8":"plt.title('Purchase amount distribution.');\nnew_merchant_transactions['purchase_amount'].plot(kind='hist');","49fd9b1f":"for i in [-1, 0]:\n    n = new_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] < i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount less than {i}.\")\nfor i in [0, 10, 100]:\n    n = new_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] > i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount more than {i}.\")","ce0f6c2c":"plt.title('Purchase amount distribution for negative values.');\nnew_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] < 0, 'purchase_amount'].plot(kind='hist');","540d1084":"map_dict = {'Y': 0, 'N': 1}\nnew_merchant_transactions['category_1'] = new_merchant_transactions['category_1'].apply(lambda x: map_dict[x])\nnew_merchant_transactions.groupby(['category_1']).agg({'purchase_amount': ['mean', 'std', 'count']})","cd191f9f":"new_merchant_transactions.groupby(['category_2']).agg({'purchase_amount': ['mean', 'std', 'count']})","8bf61984":"map_dict = {'A': 0, 'B': 1, 'C': 2, 'nan': 3}\nnew_merchant_transactions['category_3'] = new_merchant_transactions['category_3'].apply(lambda x: map_dict[str(x)])\nnew_merchant_transactions.groupby(['category_3']).agg({'purchase_amount': ['mean', 'std', 'count']})","bb3380b4":"for col in ['city_id', 'merchant_category_id', 'merchant_id', 'state_id', 'subsector_id']:\n    print(f\"There are {new_merchant_transactions[col].nunique()} unique values in {col}.\")","4097f061":"new_merchant_transactions['purchase_date'] = pd.to_datetime(new_merchant_transactions['purchase_date'])","e864c0ed":"def aggregate_historical_transactions(trans, prefix):\n    # more features from this kernel: https:\/\/www.kaggle.com\/chauhuynh\/my-first-kernel-3-699\n    trans['purchase_month'] = trans['purchase_date'].dt.month\n    trans['year'] = trans['purchase_date'].dt.year\n    trans['weekofyear'] = trans['purchase_date'].dt.weekofyear\n    trans['month'] = trans['purchase_date'].dt.month\n    trans['dayofweek'] = trans['purchase_date'].dt.dayofweek\n    trans['weekend'] = (trans.purchase_date.dt.weekday >=5).astype(int)\n    trans['hour'] = trans['purchase_date'].dt.hour\n    trans['installments'] = trans['installments'].astype(int)\n    trans['month_diff'] = ((datetime.datetime.today() - trans['purchase_date']).dt.days)\/\/30\n    trans['month_diff'] += trans['month_lag']\n\n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']).astype(np.int64) * 1e-9\n    trans['installments'] = trans['installments'].astype(int)\n    trans = pd.get_dummies(trans, columns=['category_2', 'category_3'])\n    agg_func = {\n        'category_1': ['sum', 'mean'],\n        'category_2_1.0': ['mean', 'sum'],\n        'category_2_2.0': ['mean', 'sum'],\n        'category_2_3.0': ['mean', 'sum'],\n        'category_2_4.0': ['mean', 'sum'],\n        'category_2_5.0': ['mean', 'sum'],\n        'category_3_1': ['sum', 'mean'],\n        'category_3_2': ['sum', 'mean'],\n        'category_3_3': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_month': ['mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp, 'max', 'min'],\n        'month_lag': ['min', 'max'],\n        'merchant_category_id': ['nunique'],\n        'state_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'city_id': ['nunique'],\n    }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n\n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n\n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n\n    return agg_trans","40e0a464":"%%time\ngc.collect()\nnew_transactions = reduce_mem_usage(new_merchant_transactions)\nhistory = aggregate_historical_transactions(new_merchant_transactions, prefix='new')\nhistory = reduce_mem_usage(history)\ndel new_merchant_transactions\ngc.collect()\ntrain = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\ndel history\ngc.collect()","57314346":"train = pd.merge(train, final_group, on='card_id')\ntest = pd.merge(test, final_group, on='card_id')\ngc.collect()\ndel final_group","5cb7734a":"merchants = pd.read_csv('..\/input\/merchants.csv')\ne = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='merchant')\ne","568c4b2b":"print(f'{merchants.shape[0]} merchants in data')\nmerchants.head()","eaa8df6c":"# encoding categories.\nmap_dict = {'Y': 0, 'N': 1}\nmerchants['category_1'] = merchants['category_1'].apply(lambda x: map_dict[x])\nmerchants.loc[merchants['category_2'].isnull(), 'category_2'] = 0\nmerchants['category_4'] = merchants['category_4'].apply(lambda x: map_dict[x])","cf6af753":"merchants['merchant_category_id'].nunique(), merchants['merchant_group_id'].nunique()","f8598233":"plt.hist(merchants['numerical_1']);\nplt.title('Distribution of numerical_1');","4108ec69":"np.percentile(merchants['numerical_1'], 95)","ef5eae71":"plt.hist(merchants.loc[merchants['numerical_1'] < 0.1, 'numerical_1']);\nplt.title('Distribution of numerical_1 less than 0.1');","bd61900a":"min_n1 = merchants['numerical_1'].min()\n_ = sum(merchants['numerical_1'] == min_n1) \/ merchants['numerical_1'].shape[0]\nprint(f'{_ * 100:.4f}% of values in numerical_1 are equal to {min_n1}')","6e1f5fdb":"plt.hist(merchants['numerical_2']);\nplt.title('Distribution of numerical_2');","3f2fe39d":"plt.hist(merchants.loc[merchants['numerical_2'] < 0.1, 'numerical_2']);\nplt.title('Distribution of numerical_2 less than 0.1');\nmin_n1 = merchants['numerical_1'].min()\n_ = sum(merchants['numerical_1'] == min_n1) \/ merchants['numerical_1'].shape[0]\nprint(f'{_ * 100:.4f}% of values in numerical_1 are equal to {min_n1}')","42e2a2dc":"(merchants['numerical_1'] != merchants['numerical_2']).sum() \/ merchants.shape[0]","c8f67cfe":"merchants['most_recent_sales_range'].value_counts().plot('bar');","14d62d8d":"d = merchants['most_recent_sales_range'].value_counts().sort_index()\ne = merchants.loc[merchants['numerical_2'] < 0.1].groupby('most_recent_sales_range')['numerical_1'].mean()\ndata = [go.Bar(x=d.index, y=d.values, name='counts'), go.Scatter(x=e.index, y=e.values, name='mean numerical_1', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Counts of values in categories of most_recent_sales_range\",\n                        xaxis = dict(title = 'most_recent_sales_range'),\n                        yaxis = dict(title = 'Counts'),\n                        yaxis2=dict(title='mean numerical_1', overlaying='y', side='right')),\n                   legend=dict(orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","cd49b775":"d = merchants['most_recent_purchases_range'].value_counts().sort_index()\ne = merchants.loc[merchants['numerical_2'] < 0.1].groupby('most_recent_purchases_range')['numerical_1'].mean()\ndata = [go.Bar(x=d.index, y=d.values, name='counts'), go.Scatter(x=e.index, y=e.values, name='mean numerical_1', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Counts of values in categories of most_recent_purchases_range\",\n                        xaxis = dict(title = 'most_recent_purchases_range'),\n                        yaxis = dict(title = 'Counts'),\n                        yaxis2=dict(title='mean numerical_1', overlaying='y', side='right')),\n                   legend=dict(orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","4cd703f9":"plt.hist(merchants['avg_sales_lag3'].fillna(0));\nplt.hist(merchants['avg_sales_lag6'].fillna(0));\nplt.hist(merchants['avg_sales_lag12'].fillna(0));","263f1659":"for col in ['avg_sales_lag3', 'avg_sales_lag6', 'avg_sales_lag12']:\n    print(f'Max value of {col} is {merchants[col].max()}')\n    print(f'Min value of {col} is {merchants[col].min()}')","e5f0d605":"plt.hist(merchants.loc[(merchants['avg_sales_lag12'] < 3) & (merchants['avg_sales_lag12'] > -10), 'avg_sales_lag12'].fillna(0), label='avg_sales_lag12');\nplt.hist(merchants.loc[(merchants['avg_sales_lag6'] < 3) & (merchants['avg_sales_lag6'] > -10), 'avg_sales_lag6'].fillna(0), label='avg_sales_lag6');\nplt.hist(merchants.loc[(merchants['avg_sales_lag3'] < 3) & (merchants['avg_sales_lag3'] > -10), 'avg_sales_lag3'].fillna(0), label='avg_sales_lag3');\nplt.legend();","06c059dd":"merchants['avg_purchases_lag3'].nlargest()","0ad81192":"merchants.loc[merchants['avg_purchases_lag3'] == np.inf, 'avg_purchases_lag3'] = 6000\nmerchants.loc[merchants['avg_purchases_lag6'] == np.inf, 'avg_purchases_lag6'] = 6000\nmerchants.loc[merchants['avg_purchases_lag12'] == np.inf, 'avg_purchases_lag12'] = 6000","0ce75e0a":"plt.hist(merchants['avg_purchases_lag3'].fillna(0));\nplt.hist(merchants['avg_purchases_lag6'].fillna(0));\nplt.hist(merchants['avg_purchases_lag12'].fillna(0));","488141bf":"plt.hist(merchants.loc[(merchants['avg_purchases_lag12'] < 4), 'avg_purchases_lag12'].fillna(0), label='avg_purchases_lag12');\nplt.hist(merchants.loc[(merchants['avg_purchases_lag6'] < 4), 'avg_purchases_lag6'].fillna(0), label='avg_purchases_lag6');\nplt.hist(merchants.loc[(merchants['avg_purchases_lag3'] < 4), 'avg_purchases_lag3'].fillna(0), label='avg_purchases_lag3');\nplt.legend();","b326a124":"train.head()","d7c918db":"for col in train.columns:\n    if train[col].isna().any():\n        train[col] = train[col].fillna(0)","fdd6b1b9":"for col in test.columns:\n    if test[col].isna().any():\n        test[col] = test[col].fillna(0)","95efc6d8":"y = train['target']","bd11e7d9":"col_to_drop = ['first_active_month', 'card_id', 'target']","a7dafbcc":"for col in col_to_drop:\n    if col in train.columns:\n        train.drop([col], axis=1, inplace=True)\n    if col in test.columns:\n        test.drop([col], axis=1, inplace=True)","190e71eb":"train['feature_3'] = train['feature_3'].astype(int)\ntest['feature_3'] = test['feature_3'].astype(int)","25ee2466":"categorical_feats = ['feature_1', 'feature_2']\n\nfor col in categorical_feats:\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))","dfa15d06":"train.head()","8e6ebace":"for col in ['newpurchase_amount_max', 'newpurchase_date_max', 'purchase_amount_max_mean']:\n    train[col + '_to_mean'] = train[col] \/ train[col].mean()\n    test[col + '_to_mean'] = test[col] \/ test[col].mean()","bb936c8d":"X = train\nX_test = test","97e8ab9f":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n# folds = RepeatedKFold(n_splits=n_fold, n_repeats=2, random_state=11)","8687d648":"def train_model(X=X, X_test=X_test, y=y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False):\n\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n                    verbose=1000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit)\n            \n        if model_type == 'rcv':\n            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=3)\n            model.fit(X_train, y_train)\n            print(model.alpha_)\n\n            y_pred_valid = model.predict(X_valid)\n            score = mean_squared_error(y_valid, y_pred_valid) ** 0.5\n            print(f'Fold {fold_n}. RMSE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n        \n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","86af68c7":"params = {'num_leaves': 54,\n         'min_data_in_leaf': 79,\n         'objective': 'regression',\n         'max_depth': 18,\n         'learning_rate': 0.018545526395058548,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.8354507676881442,\n         \"bagging_freq\": 5,\n         \"bagging_fraction\": 0.8126672064208567,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         'min_child_weight': 5.343384366323818,\n         'reg_alpha': 1.1302650970728192,\n         'reg_lambda': 0.3603427518866501,\n         'subsample': 0.8767547959893627,}","5f8bf5a8":"oof_lgb, prediction_lgb, feature_importance = train_model(params=params, model_type='lgb', plot_feature_importance=True)","16c9dbfb":"submission['target'] = prediction_lgb\nsubmission.to_csv('lgb.csv', index=False)","576c4f50":"xgb_params = {'eta': 0.01, 'max_depth': 11, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': 4}\noof_xgb, prediction_xgb = train_model(params=xgb_params, model_type='xgb')","496051c0":"submission['target'] = prediction_xgb\nsubmission.to_csv('xgb.csv', index=False)","6cdfcdb9":"oof_rcv, prediction_rcv = train_model(params=None, model_type='rcv')","6f5152b2":"submission['target'] = prediction_rcv\nsubmission.to_csv('rcv.csv', index=False)","583e4f48":"cat_params = {'learning_rate': 0.02,\n              'depth': 13,\n              'l2_leaf_reg': 10,\n              'bootstrap_type': 'Bernoulli',\n              #'metric_period': 500,\n              'od_type': 'Iter',\n              'od_wait': 50,\n              'random_seed': 11,\n              'allow_writing_files': False}\noof_cat, prediction_cat = train_model(params=cat_params, model_type='cat')","8b33b925":"submission['target'] = (prediction_lgb + prediction_xgb + prediction_rcv + prediction_cat) \/ 4\nsubmission.to_csv('blend.csv', index=False)","5cb3927a":"train_stack = np.vstack([oof_lgb, oof_xgb, oof_rcv, oof_cat]).transpose()\ntrain_stack = pd.DataFrame(train_stack)\ntest_stack = np.vstack([prediction_lgb, prediction_xgb, prediction_rcv, prediction_cat]).transpose()\ntest_stack = pd.DataFrame(test_stack)","20f7c93c":"oof_lgb_stack, prediction_lgb_stack = train_model(X=train_stack, X_test=test_stack, params=params, model_type='lgb')","118f6aad":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission['target'] = prediction_lgb_stack\nsample_submission.to_csv('stacker_lgb.csv', index=False)","93e538af":"oof_rcv_stack, prediction_rcv_stack = train_model(X=train_stack, X_test=test_stack, params=None, model_type='rcv')","b53998c0":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission['target'] = prediction_rcv_stack\nsample_submission.to_csv('stacker_rcv.csv', index=False)","3bc57cfe":"Well, 95% of values are less than 0.1, we'll need to deal with outliers.","7b754c4a":"### avg_sales_lag","a7868eda":"### Numerical_2","9e3188cb":"These two variables seem to be quite similar.","50c0ad1a":"Also there is one line with a missing data in test. I'll fill in with the first data, having the same values of features.","f69a9fdb":"### target","6b49e088":"### most_recent_purchases_range","e3643b10":"It seems that there are some cards, for which most of transactions were declined. Were this fraud transactions?","23c3b7ae":"### purchase_amount\nSadly purchase_amount is normalized. Let's have a look at it nevertheless.","af788a4a":"### Basic LGB model","58516741":"### avg_purchases_lag","6bab0871":"Trends of counts for train and test data are similar, and this is great.\nWhy there is such a sharp decline at the end of the period? I think it was on purpose. Or maybe new cards are taken into account only after fulfilling some conditions. ","27242e81":"For now I won't use merchants data in models.","92a7a958":"### Categories","c2293f9f":"## new_merchant_transactions \nTwo months' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data.","ac28db85":"It seems that there are some cards, for which most of transactions were declined. Were this fraud transactions?","deb28d67":"### Feature engineering","b57f87d7":"## General information\n\nThis kernel is dedicated to EDA of Elo Merchant Category Recommendation competition as well as feature engineering.\n\nIn this dataset we can see clients who use Elo and their transactions. We need to predict the loyalty score for each card_id.\n\nWork in progress.\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/10445\/logos\/thumb76_76.png?t=2018-10-24-17-14-05)","bc508ba8":"### Categories","dad66782":"## Main data exploration\nLet's have a look at data","2da3eb51":"#### Cards with lowest and highest total purchase amount","6a05c354":"### Feature engineering","6e104f81":"## merchants\nAggregate information for each merchant_id","8a9e7259":"### numerical_1","04d4e33f":"### installments","be8867af":"#### Code for training models","f52e57c1":"### Processing data for modelling","29c98158":"All categories are quite different","0b0b12af":"All categories are quite different","56ab250f":"It seems that almost all transactions have purchase amount in range (-1, 0). Quite a strong normalization and high outliers, which will need to be processed.","5dafc65f":"In fact more than a half values are equal to minimum value. A very skewered distribution.","19e4ad78":"## historical_transactions\nUp to 3 months' worth of historical transactions for each card_id","3f8086f0":"### date","49937cf3":"### Feature engineering","13226020":"This looks really strange!","70b69300":"We have a date column, three anonymized categorical columns and target.","157c0922":"Interesting. Most common number of installments are 0 and 1 which is expected. But -1 and 999 are strange. I think that these values were used to fill in missing values.","7b1579c5":"### Features 1, 2, 3","36d73cce":"### installments","dd8b5a60":"### purchase_amount\nSadly purchase_amount is normalized. Let's have a look at it nevertheless.","966e3d24":"We even have infinite values...","e798eb12":"In contrast with historical data, **all** transactions here were authorized!","8b3d5e7e":"We can see that these ranges have different counts and different mean value of numerical_1 even after removing outliers.","c30a1a75":"#### Cards with lowest and highest percentage of authorized transactions","d90bbc37":"These two plots show an important idea: while different categories of these features could have various counts, the distribution of target is almost the same. This could mean, that these features aren't really good at predicting target - we'll need other features and feature engineering.\nAlso it is worth noticing that mean target values of each catogory of these features is near zero. This could mean that data was sampled from normal distribution.","a7433a08":"And they have 1 unique value: -33.21928095.\nThis seems to be a special case. Maybe it would be reasonable to simply exclude these samples. We'll try later.","c8e3c80a":"It seems that almost all transactions have purchase amount in range (-1, 0). Quite a strong normalization and high outliers, which will need to be processed.","818d570b":"Interesting. Most common number of installments are 0 and 1 which is expected. But -1 and 999 are strange. I think that these values were used to fill in missing values.","3cc0ca2c":"These two variables are very similar. In fact for 90% merchants they are the same.","a8cab17e":"Distribution of these values is quite similar and most values are between 0 and 2.","1a2d4ef5":"On the other hand it seems that `999` could mean fraud transactions, considering only 3% of these transactions were approved. One more interesting thing is that the higher the number of installments is, the lower is the approval rate.","f57358c3":"> most_recent_sales_range \tmost_recent_purchases_range \tavg_sales_lag3 \tavg_purchases_lag3 \tactive_months_lag3 \tavg_sales_lag6 \tavg_purchases_lag6 \tactive_months_lag6 \tavg_sales_lag12 \tavg_purchases_lag12 \tactive_months_lag12","064012a2":"### most_recent_sales_range"}}