{"cell_type":{"9092db40":"code","5499ec85":"code","aaa7e170":"code","6becfae1":"code","38580629":"code","42b15db4":"code","9003dca3":"code","6443363d":"code","dbb75560":"code","cb55d983":"code","6e309c29":"code","d3dddd9c":"code","d9d7d1cf":"code","c6eda100":"code","e6f12788":"code","b456663c":"code","0e266ff5":"code","2b955239":"code","b02e0f5a":"code","a80267b5":"code","c4455974":"code","3d883ffa":"code","c08cd2b9":"code","a3adf30b":"markdown","2790c231":"markdown","28162738":"markdown","1c23b080":"markdown","f7f6fe5d":"markdown","1f6f40e7":"markdown","1e9a1288":"markdown","e301dae5":"markdown","fecd85f0":"markdown","1432ed4d":"markdown","a311a3db":"markdown","433e14cf":"markdown","402e7d2f":"markdown","494bddfe":"markdown","637c212f":"markdown","f6e6cc2d":"markdown","fdb9c217":"markdown","5412c52a":"markdown","28b60b00":"markdown","ba52149b":"markdown"},"source":{"9092db40":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom pandas import Series\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation,Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow.keras.backend as K\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import BatchNormalization\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom tensorflow.keras import initializers\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.models import load_model\nfrom keras.utils.generic_utils import get_custom_objects\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5499ec85":"train=pd.read_csv(\"..\/input\/cap-4611-2021-fall-assignment-3\/train.csv\")\ntest = pd.read_csv(\"..\/input\/cap-4611-2021-fall-assignment-3\/eval.csv\")\n\ntrain = shuffle(train)\ntest = shuffle(test)","aaa7e170":"\ntrain = train.drop(['pubchem_id', 'id'], axis = 1)\nids = test[\"id\"]\ntest = test.drop(['pubchem_id', 'id'],axis = 1)\nX = train.drop('Eat', axis = 1)\nY = train['Eat']\n\n\n","6becfae1":"f = plt.figure(figsize=(10,10))\nplt.matshow(train.drop(['Eat'], axis = 1).corr(),fignum=f.number)\ncb = plt.colorbar()\nplt.title('Correlation Matrix')","38580629":"sns.distplot(Y)","42b15db4":"def root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square((y_pred - y_true))))","9003dca3":"from sklearn import preprocessing\n\n\nscaler = preprocessing.StandardScaler().fit(X)\noutputscaler = preprocessing.StandardScaler().fit(Y.values.reshape(-1, 1))\n\nX = scaler.transform(X)\ntest = scaler.transform(test)\nY = outputscaler.transform(Y.values.reshape(-1, 1))\n\n\n\n\n","6443363d":"X=np.asarray(X)\nY=np.asarray(Y)\n\n\n\n","dbb75560":"X_train, X_test, y_train, y_test = train_test_split(\n    X,Y, test_size=0.1)","cb55d983":"def create_model():\n  \n        model = Sequential()\n        model.add(Dense(512,input_dim=X_train.shape[1],activation = 'relu'))\n        model.add(Dense(256, activation='relu'))\n        model.add(Dense(128, activation='relu'))\n        model.add(Dense(64, activation='relu'))\n        model.add(Dense(32, activation='relu'))\n        model.add(Dense(1))\n\n\n        model.add(Dense(1))\n\n        model.compile(optimizer = Adam(learning_rate = 0.0001), loss = root_mean_squared_error)\n        return model","6e309c29":"def create_model1():\n  \n    model = Sequential()\n    model.add(Dense(64,input_dim=X_train.shape[1],activation = 'relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(1))\n\n\n    model.add(Dense(1))\n\n    model.compile(optimizer = Adam(learning_rate = 0.0001), loss = root_mean_squared_error)\n    return model","d3dddd9c":"def create_model2():\n  \n    model = Sequential()\n    model.add(Dense(32,input_dim=X_train.shape[1],activation = 'relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(8, activation='relu'))\n    model.add(Dense(1))\n\n\n    model.add(Dense(1))\n\n    model.compile(optimizer = Adam(learning_rate = 0.0001), loss = root_mean_squared_error)\n    return model","d9d7d1cf":"def get_model_name(k):\n    return 'model_'+str(k) + '.h5'","c6eda100":"best_best = \"\"\nbest_rmse = 9999\nfold_var = 1","e6f12788":"\nsave_dir = '\/saved_models2\/'\nfold_var = 1\n\nkfold= RepeatedKFold(n_splits=10, n_repeats=5)\nestimator2 = create_model2()\n\n\n\nval_scores2 = []\n\nfor train, tester in kfold.split(X_train, y_train):\n    early_stopping = EarlyStopping(monitor = 'loss', patience = 5)\n    model_save = ModelCheckpoint(save_dir+get_model_name(fold_var), save_best_only = True)\n    h_callback=  estimator2.fit(X_train[train],y_train[train], validation_data = (X_train[tester], y_train[tester])  , callbacks = [early_stopping, model_save], epochs = 999, verbose = 2, batch_size = 32)\n    scores = estimator2.evaluate(X_train[tester], y_train[tester])\n    val_scores2.append(scores)\n    estimator2.load_weights(\"\/saved_models2\/model_\"+str(fold_var)+\".h5\")\n    prediction = estimator2.predict(X_test)\n    i_prediction1 = outputscaler.inverse_transform(prediction)\n    y_test1 = outputscaler.inverse_transform(y_test)\n    rmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n    if (rmse < best_rmse) :\n        best_best = \"\/saved_models2\/model_\"+str(fold_var)+\".h5\"\n        best_rmse = rmse\n    print(\"Fold Number \" + str(fold_var) + \" RMSE: \")\n    print(rmse)\n    fold_var += 1\n   \n\nval_scores2 = pd.DataFrame (val_scores2, columns = ['rmse'])\nprint(val_scores2.describe())\nval_scores2.plot.hist(title = \"model 1 rmse distribution\")\n\n","b456663c":"\nsave_dir = '\/saved_models1\/'\nfold_var = 1\n\nkfold= RepeatedKFold(n_splits=10, n_repeats=5)\nestimator1 = create_model1()\n\n\n\nval_scores1 = []\n\nfor train, tester in kfold.split(X_train, y_train):\n    early_stopping = EarlyStopping(monitor = 'loss', patience = 5)\n    model_save = ModelCheckpoint(save_dir+get_model_name(fold_var), save_best_only = True)\n    h_callback=  estimator1.fit(X_train[train],y_train[train], validation_data = (X_train[tester], y_train[tester])  , callbacks = [early_stopping, model_save], epochs = 999, verbose = 2, batch_size = 32)\n    scores = estimator1.evaluate(X_train[tester], y_train[tester])\n    val_scores1.append(scores)\n    estimator1.load_weights(\"\/saved_models1\/model_\"+str(fold_var)+\".h5\")\n    prediction = estimator1.predict(X_test)\n    i_prediction1 = outputscaler.inverse_transform(prediction)\n    y_test1 = outputscaler.inverse_transform(y_test)\n    rmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n    if (rmse < best_rmse) :\n        best_best = \"\/saved_models1\/model_\"+str(fold_var)+\".h5\"\n        best_rmse = rmse\n    print(\"Fold Number \" + str(fold_var) + \" RMSE: \")\n    print(rmse)\n    fold_var += 1\n   \n\nval_scores1 = pd.DataFrame (val_scores1, columns = ['rmse'])\nprint(val_scores1.describe())\nval_scores1.plot.hist(title = \"model 2 rmse distribution\")\n\n\n\n\n\n","0e266ff5":"\nsave_dir = '\/saved_models\/'\nfold_var = 1\n\nkfold= RepeatedKFold(n_splits=10, n_repeats=5)\nestimator = create_model()\n\n\n\nval_scores = []\n\nfor train, tester in kfold.split(X_train, y_train):\n    early_stopping = EarlyStopping(monitor = 'loss', patience = 5)\n    model_save = ModelCheckpoint(save_dir+get_model_name(fold_var), save_best_only = True)\n    h_callback=  estimator.fit(X_train[train],y_train[train], validation_data = (X_train[tester], y_train[tester])  , callbacks = [early_stopping, model_save], epochs =999, verbose = 2, batch_size = 32)\n    scores = estimator.evaluate(X_train[tester], y_train[tester])\n    val_scores.append(scores)\n    estimator.load_weights(\"\/saved_models\/model_\"+str(fold_var)+\".h5\")\n    prediction = estimator.predict(X_test)\n    i_prediction1 = outputscaler.inverse_transform(prediction)\n    y_test1 = outputscaler.inverse_transform(y_test)\n    rmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n    if (rmse < best_rmse) :\n        best_best = \"\/saved_models\/model_\"+str(fold_var)+\".h5\"\n        best_rmse = rmse\n    print(\"Fold Number \" + str(fold_var) + \" RMSE: \")\n    print(rmse)\n    fold_var += 1\n   \n\nval_scores = pd.DataFrame (val_scores, columns = ['rmse'])\nprint(val_scores.describe())\nval_scores.plot.hist(title = \"model 3 rmse distribution\")\n\n","2b955239":"def create_model_grid(lr = 0.001):\n  \n    model = Sequential()\n    model.add(Dense(512,input_dim=X_train.shape[1],activation = 'relu'))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1))\n\n\n    model.add(Dense(1))\n\n    model.compile(optimizer = Adam(learning_rate = lr), loss = root_mean_squared_error)\n    return model","b02e0f5a":"\nestimator_Batch16 = create_model()\n\nmodel_save = ModelCheckpoint('bestmodel_searched.h5', save_best_only = True)\nh_callback=  estimator_Batch16.fit(X_train,y_train, validation_split = 0.15  , callbacks = [early_stopping, model_save], epochs =999, verbose = 2, batch_size = 16)\nprediction = estimator_Batch16.predict(X_test)\ni_prediction1 = outputscaler.inverse_transform(prediction)\ny_test1 = outputscaler.inverse_transform(y_test)\nrmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n\nprint(rmse)\n\nestimator_Batch64 = create_model()\n\nmodel_save = ModelCheckpoint('bestmodel_searched.h5', save_best_only = True)\nh_callback=  estimator_Batch64.fit(X_train,y_train, validation_split = 0.15  , callbacks = [early_stopping, model_save], epochs =999, verbose = 2, batch_size = 32)\nprediction = estimator_Batch64.predict(X_test)\ni_prediction1 = outputscaler.inverse_transform(prediction)\ny_test1 = outputscaler.inverse_transform(y_test)\nrmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n\nprint(rmse)\n\nestimator_Batch64 = create_model()\n\nmodel_save = ModelCheckpoint('bestmodel_searched.h5', save_best_only = True)\nh_callback=  estimator_Batch64.fit(X_train,y_train, validation_split = 0.15  , callbacks = [early_stopping, model_save], epochs =999, verbose = 2, batch_size = 64)\nprediction = estimator_Batch64.predict(X_test)\ni_prediction1 = outputscaler.inverse_transform(prediction)\ny_test1 = outputscaler.inverse_transform(y_test)\nrmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n\nprint(rmse)\n\nestimator_Batch128 = create_model()\n\nmodel_save = ModelCheckpoint('bestmodel_searched.h5', save_best_only = True)\nh_callback=  estimator_Batch128.fit(X_train,y_train, validation_split = 0.15  , callbacks = [early_stopping, model_save], epochs =999, verbose = 2, batch_size = 128)\nprediction = estimator_Batch128.predict(X_test)\ni_prediction1 = outputscaler.inverse_transform(prediction)\ny_test1 = outputscaler.inverse_transform(y_test)\nrmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n\nprint(rmse)","a80267b5":"\nestimator_01 = create_model_grid(.01)\n\nmodel_save = ModelCheckpoint('bestmodel_searched.h5', save_best_only = True)\nh_callback=  estimator_01.fit(X_train,y_train, validation_split = 0.15  , callbacks = [early_stopping, model_save], epochs =999, verbose = 2, batch_size = 64)\nprediction = estimator_01.predict(X_test)\ni_prediction1 = outputscaler.inverse_transform(prediction)\ny_test1 = outputscaler.inverse_transform(y_test)\nrmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n\nprint(rmse)\n\nestimator_001 = create_model_grid(.001)\n\nmodel_save = ModelCheckpoint('bestmodel_searched.h5', save_best_only = True)\nh_callback=  estimator_001.fit(X_train,y_train, validation_split = 0.15  , callbacks = [early_stopping, model_save], epochs =999, verbose = 2, batch_size = 64)\nprediction = estimator_001.predict(X_test)\ni_prediction1 = outputscaler.inverse_transform(prediction)\ny_test1 = outputscaler.inverse_transform(y_test)\nrmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n\nprint(rmse)\n\nestimator_0001 = create_model_grid(.0001)\n\nmodel_save = ModelCheckpoint('bestmodel_searched.h5', save_best_only = True)\nh_callback=  estimator_0001.fit(X_train,y_train, validation_split = 0.15  , callbacks = [early_stopping, model_save], epochs =999, verbose = 2, batch_size = 64)\nprediction = estimator_0001.predict(X_test)\ni_prediction1 = outputscaler.inverse_transform(prediction)\ny_test1 = outputscaler.inverse_transform(y_test)\nrmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n\nprint(rmse)\n\nestimator_0005 = create_model_grid(.0001)\n\nmodel_save = ModelCheckpoint('bestmodel_searched.h5', save_best_only = True)\nh_callback=  estimator_0005.fit(X_train,y_train, validation_split = 0.15  , callbacks = [early_stopping, model_save], epochs =999, verbose = 2, batch_size = 64)\nprediction = estimator_0005.predict(X_test)\ni_prediction1 = outputscaler.inverse_transform(prediction)\ny_test1 = outputscaler.inverse_transform(y_test)\nrmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n\nprint(rmse)\n","c4455974":"save_dir = '\/saved_modelsf\/'\nfold_var = 1\nkfold= RepeatedKFold(n_splits=10, n_repeats=5)\nestimator = create_model()\nval_scores = []\n\nfor train, tester in kfold.split(X_train, y_train):\n    early_stopping = EarlyStopping(monitor = 'loss', patience = 5)\n    model_save = ModelCheckpoint(save_dir+get_model_name(fold_var), save_best_only = True)\n    h_callback=  estimator.fit(X_train[train],y_train[train], validation_data = (X_train[tester], y_train[tester])  , callbacks = [early_stopping, model_save], epochs =999, verbose = 2, batch_size = 64)\n    scores = estimator.evaluate(X_train[tester], y_train[tester])\n    val_scores.append(scores)\n    estimator.load_weights(\"\/saved_modelsf\/model_\"+str(fold_var)+ '.h5')\n    prediction = estimator.predict(X_test)\n    i_prediction1 = outputscaler.inverse_transform(prediction)        \n    y_test1 = outputscaler.inverse_transform(y_test)\n    rmse = mean_squared_error(i_prediction1, y_test1, squared= False)\n    if (rmse < best_rmse) :\n        best_best = \"\/saved_modelsf\/model_\"+str(fold_var)+ \".h5\"\n        best_rmse = rmse\n\n    print(\"Fold Number \" + str(fold_var) + \" RMSE: \")\n    print(rmse)\n    print(best_rmse)\n    print(best_best)\n\n    fold_var += 1\n   \n\n        \n    \n\n    \n\n    ","3d883ffa":"print(best_best)\nestimator.load_weights(best_best)\nprediction = estimator.predict(X_test)\ni_prediction = outputscaler.inverse_transform(prediction)\ny_test = outputscaler.inverse_transform(y_test)\nrmse = mean_squared_error(i_prediction, y_test, squared= False)\nprint(rmse)","c08cd2b9":"pre = estimator.predict(test)\nsub = outputscaler.inverse_transform(pre)\n\n\npre_final = []\n\nfor x in sub :\n    for y in x:\n        pre_final.append(y)\n    \n\n\noutput = pd.DataFrame({ 'id' : ids, 'Eat': pre_final})\noutput.to_csv(\"submission.csv\",index = False)\nprint(output.to_string())\n","a3adf30b":"Here we test the model with diffrent batch sizes","2790c231":"After testing, the model defined by the \"create_model\" function was the best so we will choose this model and it's archetichture to tune","28162738":"first model","1c23b080":"We will also reserve 10% of our data for testing","f7f6fe5d":"This function helps save the best model","1f6f40e7":"# Defining RMSE\nThis is a function to use later on that will calculate RMSE","1e9a1288":"# Preprocessing\n\nTo help train our NN, we scale the outpunt and input data to normalize the ranges within the data.  We also need to convert the pandas to arrays for inputs into keras. \n","e301dae5":"second model","fecd85f0":"# EDA","1432ed4d":"This functions is the same as our best model but it allows a modification to the learning rate","a311a3db":"# Creating Models\n\nHere we define 3 diffrnet models with variantion in their archetichture ","433e14cf":"Since there seems to be a wide range of Y values, it may be worth scaling both the input and output data","402e7d2f":"# Load The Data In\n\nHere we load the data in and shuffle it","494bddfe":"So our best scoring model used a bacth size of 64 and learning rate 0.0001. Now we will build our final model with the parameters.","637c212f":"testing learning rate","f6e6cc2d":"# Comparing Models","fdb9c217":"# Column Dropping\n\nHere there are two identificaiton columns so we can drop both. Also we can distiguish the predictors and target vectors and label them as x and y","5412c52a":"third model","28b60b00":"# Tuning","ba52149b":"There seems to be some level of correlations between our diffrent features, it may be worth doing a PCA"}}