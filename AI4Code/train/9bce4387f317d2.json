{"cell_type":{"3dfe1fc4":"code","1848bce0":"code","82381bbc":"code","517843a4":"code","8d435e71":"code","b67936a2":"code","f486ccbf":"code","75231f1f":"code","076e3513":"code","34263f8b":"code","9dd518bf":"code","2ab5b4bf":"code","3f40ea04":"code","e27d97c8":"code","99f5ab64":"code","4880773c":"code","c430a68d":"code","91cc23e4":"code","d72f1b1d":"code","2d78166d":"code","96e6911b":"code","54d88225":"code","764c920c":"code","ee09081e":"code","4d0ff27a":"code","3e3de32f":"code","4d50a1c5":"code","4bee7c30":"code","a1d75378":"code","044ccfc6":"code","43456064":"code","14f82ed7":"code","6c1ad10d":"code","40dc6fcc":"code","f202af7a":"code","8da25127":"markdown","359613b8":"markdown","f7beece0":"markdown","e4e2bfbe":"markdown","3e95d9dd":"markdown","ea9a22e2":"markdown","d8250814":"markdown","f9ce115f":"markdown","eb3e832e":"markdown","9051f519":"markdown","7bd51d2e":"markdown","7490e947":"markdown"},"source":{"3dfe1fc4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1848bce0":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport random\nimport time\nfrom pandas import DataFrame, Series\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold,KFold,GroupKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_absolute_error\n\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom category_encoders.woe import WOEEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.sum_coding import SumEncoder\nfrom category_encoders.one_hot import OneHotEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV,Lasso,Ridge,ElasticNet\nfrom sklearn.metrics import mean_squared_error, make_scorer,mean_squared_log_error\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import quantile_transform\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook as tqdm","82381bbc":"train_x = pd.read_csv('\/kaggle\/input\/exam-for-students20200527\/train.csv')\ntest_x = pd.read_csv('\/kaggle\/input\/exam-for-students20200527\/test.csv')\nsubmit = pd.read_csv('\/kaggle\/input\/exam-for-students20200527\/sample_submission.csv')\ndict_d = pd.read_csv('\/kaggle\/input\/exam-for-students20200527\/data_dictionary.csv')","517843a4":"def count_mis(df):\n    df['missing_count'] = df.isnull().sum(axis=1)\n    return df\n\ndef mis_pattern(df):\n    m = df.isnull().sum()\n    cols_with_missing = list(m[m != 0].index)\n\n    df['missing_pattern'] = 0\n    for col in cols_with_missing:\n        df['missing_pattern'] *= 2\n        df.loc[df[col].isnull(), 'missing_pattern'] += 1\n    \n    df['missing_pattern'] *= 1e-16\n    return df\n\ndef outlier_removing(df,n_sigma):\n\n    for i in range(len(df.columns)):\n\n        # \u5217\u3092\u62bd\u51fa\u3059\u308b\n        col = df.iloc[:,i]\n\n        # \u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\n        average = np.mean(col)\n        sd = np.std(col)\n\n        # \u5916\u308c\u5024\u306e\u57fa\u6e96\u70b9\n        outlier_min = average - (sd) * n_sigma\n        outlier_max = average + (sd) * n_sigma\n\n        # \u7bc4\u56f2\u304b\u3089\u5916\u308c\u3066\u3044\u308b\u5024\u3092\u9664\u304f\n        col[col < outlier_min] = None\n        col[col > outlier_max] = None\n\n    return df","8d435e71":"train_x.shape,test_x.shape","b67936a2":"train_x.info()","f486ccbf":"test_x.info()","75231f1f":"train_x['Area']=np.log1p(train_x['Area'])\ntrain_x['Area'].hist()","076e3513":"train_x['TotalFloorArea']=np.log1p(train_x['TotalFloorArea'])\ntest_x['TotalFloorArea']=np.log1p(test_x['TotalFloorArea'])","34263f8b":"test_x['Area']=np.log1p(test_x['Area'])\ntest_x['Area'].hist()","9dd518bf":"target='TradePrice'\ngroups = train_x.Prefecture.values\ntrain_x[target]=np.log1p(train_x[target])\ntrain_y=train_x[target]\ntest_x=test_x.drop(['id','DistrictName','Prefecture','TotalFloorAreaIsGreaterFlag','FrontageIsGreaterFlag'],axis=1)\ntrain_x=train_x.drop(['id','TradePrice','DistrictName','Prefecture','TotalFloorAreaIsGreaterFlag','FrontageIsGreaterFlag'],axis=1)","2ab5b4bf":"train_y.hist()","3f40ea04":"cats = []\nfor col in train_x.columns:\n    if train_x[col].dtype == 'object':\n        cats.append(col)      \n        print(col, train_x[col].nunique())","e27d97c8":"num_cols = []\nfor col in train_x.columns:\n    if train_x[col].dtype != 'object':\n        num_cols.append(col)\n        print(col, train_x[col].mean(),train_x[col].std())","99f5ab64":"train_x.isnull().sum().plot(kind='bar',figsize=(20,5))","4880773c":"test_x.isnull().sum().plot(kind='bar',figsize=(20,5))","c430a68d":"train_x = count_mis(train_x)\ntest_x = count_mis(test_x)\ntrain_x = mis_pattern(train_x)\ntest_x = mis_pattern(test_x)","91cc23e4":"train_x['TimeToNearestStation']=train_x['TimeToNearestStation'].replace('1H-1H30',75).replace('30-60minutes',45).replace('2H-',120).replace('1H30-2H',105)\ntest_x['TimeToNearestStation']=test_x['TimeToNearestStation'].replace('1H-1H30',75).replace('30-60minutes',45).replace('2H-',120).replace('1H30-2H',105)\ntrain_x['TimeToNearestStation']=train_x['TimeToNearestStation'].fillna(train_x['TimeToNearestStation'].median())\ntest_x['TimeToNearestStation']=test_x['TimeToNearestStation'].fillna(test_x['TimeToNearestStation'].median())\ntrain_x[cats]=train_x[cats].fillna('miss')\ntest_x[cats]=test_x[cats].fillna('miss')","d72f1b1d":"# # ordinal enc\n# oe = OrdinalEncoder(cols=cats, return_df=False)\n# train_x[cats] = oe.fit_transform(train_x[cats])\n# test_x[cats] = oe.transform(test_x[cats])\n\n# # target enc\nte = TargetEncoder(cols=cats)\ntrain_x[cats] = te.fit_transform(train_x[cats],train_y)\ntest_x[cats] = te.transform(test_x[cats])\n\n# # CatBoost enc\n# cae = CatBoostEncoder(cols=cats)\n# train_x[cats] = cae.fit_transform(train_x[cats],train_y)\n# test_x[cats] = cae.transform(test_x[cats])","2d78166d":"# \u6b20\u640d\u5024\u306f\u4e2d\u592e\u5024\ntrain_x = train_x.fillna(-1)\ntest_x = test_x.fillna(-1)\n# Standardize numerical features\nstd = StandardScaler()\ntrain_x[num_cols]=std.fit_transform(train_x[num_cols])\ntest_x[num_cols]=std.fit_transform(test_x[num_cols])\n\n# #\u30e9\u30f3\u30af\u30ac\u30a6\u30b9\u3067\u6b63\u898f\u5316\n# x_all = pd.concat([train_x, test_x], axis=0)\n# x_all[num_cols] = quantile_transform(x_all[num_cols],n_quantiles=100, random_state=0, output_distribution='normal')\n\n# train_x = x_all.iloc[:train_x.shape[0], :]\n# test_x = x_all.iloc[train_x.shape[0]:, :]","96e6911b":"train_x[num_cols]","54d88225":"params = {\"objective\": 'regression',\n          \"boosting_type\": \"gbdt\",\n          \"learning_rate\": 0.05,\n          \"num_leaves\": 60,\n          \"verbosity\": 0,\n          \"drop_rate\": 0.1,\n          \"is_unbalance\": False,\n          \"max_drop\": 50,\n          \"min_child_samples\": 20,\n          \"min_child_weight\": 1e-3,\n          \"min_split_gain\": 0.0,\n          \"subsample\": 1.0,\n          \"metric\": \"rmse\",\n    }","764c920c":"predict_data=submit","ee09081e":"#Group K Fold\nscores = []\ntotal_time=0\nr_seed=10\n\npredictions = pd.DataFrame()\npredictions = predictions.fillna(0)\nprint(predictions)\n\ngkf = GroupKFold(n_splits=5)\nscores = []\n\nfor i, (train_ix, test_ix) in tqdm(enumerate(gkf.split(train_x, train_y,groups))):\n    t0 = time.time()\n    X_train_, y_train_,groups_train_ = train_x.values[train_ix], train_y.values[train_ix],groups[train_ix]\n    X_val, y_val,groups_val_ = train_x.values[test_ix], train_y.values[test_ix],groups[test_ix]\n    \n    print('Train Groups', np.unique(groups_train_))\n    print('Val Groups', np.unique(groups_val_))    \n    \n    ##define model\n    lr=LGBMRegressor(n_estimators=9999, random_state=r_seed, **params)\n    \n    ##modeling\n    lr.fit(X_train_, y_train_, early_stopping_rounds=100, eval_metric='rmse', eval_set=[(X_val, y_val)], verbose=100)\n    ##predict test data\n    y_pred_val = lr.predict(X_val)\n    \n    #\n    score = mean_squared_log_error(y_val, y_pred_val)**0.5\n    scores.append(score)\n    \n    y_pred=lr.predict(test_x)\n    predictions = pd.concat([predictions,pd.DataFrame(y_pred,columns=['pred'+str(i)])],axis=1)\n    print('\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u6642\u9593{}\u79d2'.format(time.time() - t0))\n    \n    total_time += time.time()-t0\n    print('CV Score of Fold_%d is %f' % (i, score))\n    \n    r_seed=random.randint(1,100)\n\nprint('total:{} sec'.format(total_time))","4d0ff27a":"predictions","3e3de32f":"predict_data=predictions.mean(axis=1)","4d50a1c5":"predict_data = np.expm1(predict_data)","4bee7c30":"predict_data","a1d75378":"submit['TradePrice']=predict_data","044ccfc6":"submit","43456064":"submit['TradePrice'] = submit['TradePrice'].round(-4)","14f82ed7":"submit.to_csv('submission.csv',index=False)","6c1ad10d":"imp = DataFrame(lr.booster_.feature_importance(importance_type='gain'), index = train_x.columns, columns=['importance']).sort_values(['importance'], ascending=False)\nprint(imp[:20].index)","40dc6fcc":"#\u7279\u5fb4\u91cf\u53ef\u8996\u5316\nimp.plot.bar(figsize=(10,5))","f202af7a":"submit","8da25127":"# \u30c7\u30fc\u30bf\u5206\u5272\uff0b\u5bfe\u6570\u5909\u63db","359613b8":"# \u30ab\u30c6\u30b4\u30ea\u578b\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","f7beece0":"# LGBM\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a","e4e2bfbe":"# \u30c7\u30fc\u30bf\u306e\u5f62\u3092\u78ba\u8a8d\u3057\u305f\u3068\u3053\u308d\u4f4f\u5b85\u4fa1\u683c\u306f\u4e07\u5186\u5358\u4f4d\u3067\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u305d\u3046\u306a\u305f\u3081\u3001\u4e0b4\u6841\u3092\u4e38\u3081\u308b\u3002","3e95d9dd":"# submit\u7528\u30c7\u30fc\u30bf\u306b\u683c\u7d0d","ea9a22e2":"# \u30ab\u30c6\u30b4\u30ea\u578b\u62bd\u51fa","d8250814":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u30a4\u30f3\u30dd\u30fc\u30c8","f9ce115f":"# \u95a2\u6570\u5b9a\u7fa9\uff08\u6b20\u640d\u5024\u30ab\u30a6\u30f3\u30c8\uff0b\u7570\u5e38\u5024\u9664\u5916\u7528\u95a2\u6570\uff09","eb3e832e":"# \u6b20\u640d\u5024\u51e6\u7406","9051f519":"# CV+\u30e2\u30c7\u30ea\u30f3\u30b0","7bd51d2e":"# \u4e88\u6e2c\u7d50\u679c\u5f8c\u51e6\u7406","7490e947":"# \u6570\u5024\u578b\uff08\u6b63\u898f\u5316\uff09"}}