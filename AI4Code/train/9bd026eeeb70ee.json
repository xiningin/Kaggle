{"cell_type":{"d4dbd540":"code","77e2f0d6":"code","8ff8c2f1":"code","f1868033":"code","f5788e01":"code","67de07e3":"code","d98598f7":"code","f34dff94":"code","3eaa56f1":"code","553b1e39":"code","9177b733":"code","7bd3727a":"code","2bf7d759":"code","9c2b146f":"code","d1128356":"code","13cc49ce":"code","3aa9b1d3":"code","2c56afff":"code","46604eb4":"code","f9282e45":"code","1be8483f":"code","7ab4c6cf":"code","be8eb723":"code","3d31ea1a":"code","326b0815":"code","437cd954":"code","2300062f":"code","c3fc92f0":"code","a2c5b43c":"code","7a1121bb":"code","04d5b3f9":"code","b8bed23a":"code","b02690ee":"code","5074d7f0":"code","808c2b09":"code","72d67546":"code","d8d3a8b3":"code","4de04e93":"code","54aaca16":"code","c7ed7237":"code","7053bd01":"code","f4cc5889":"code","c0c293d6":"code","961cd4d2":"code","d3dd1b0b":"code","841be1eb":"code","c73c151b":"code","3f00da30":"markdown","c5c187eb":"markdown","51dd6170":"markdown","728001be":"markdown","a69fb399":"markdown","2abaf5f3":"markdown","74f4a391":"markdown","1ec30548":"markdown","907f18cc":"markdown","e6e307d3":"markdown","010e8f1f":"markdown","8710d17d":"markdown","d8413dfa":"markdown","afe9bfc9":"markdown","f51d6cab":"markdown","67da93d7":"markdown","cbd0afb4":"markdown","938e0979":"markdown","19c7f8ef":"markdown","497eb0d0":"markdown","0e0cec00":"markdown","dfc5104e":"markdown","a1fb8e43":"markdown","5d087d3b":"markdown","87b092e4":"markdown","7df9d68f":"markdown","a1dfbcda":"markdown"},"source":{"d4dbd540":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns #importing seaborn module \nimport warnings\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler \nfrom sklearn import metrics\nfrom pandas import set_option\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score # to split the data\nfrom sklearn.metrics import explained_variance_score, median_absolute_error, r2_score, mean_squared_error, accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split # Model evaluation\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler # Preprocessing\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, RANSACRegressor, SGDRegressor, HuberRegressor, BayesianRidge # Linear models\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor  # Ensemble methods\nfrom xgboost import XGBRegressor, plot_importance # XGBoost\nfrom sklearn.svm import SVR, SVC, LinearSVC  # Support Vector Regression\nfrom sklearn.tree import DecisionTreeRegressor # Decision Tree Regression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import Pipeline # Streaming pipelines\nfrom sklearn.decomposition import KernelPCA, PCA # Dimensionality reduction\nfrom sklearn.feature_selection import SelectFromModel # Dimensionality reduction\nfrom sklearn.model_selection import learning_curve, validation_curve, GridSearchCV # Model evaluation\nfrom sklearn.base import clone # Clone estimator\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nimport xgboost as xgb\nwarnings.filterwarnings('ignore')","77e2f0d6":"#Load train and test files \ndata = pd.read_csv('..\/input\/big-mart-sales-prediction\/Train.csv')\ntest = pd.read_csv('..\/input\/big-mart-sales-prediction\/Test.csv')","8ff8c2f1":"#creating dataframe for the required output for submission file\n\nsubmission = pd.DataFrame()\nsubmission['Item_Identifier'] = test['Item_Identifier']\nsubmission['Outlet_Identifier'] = test['Outlet_Identifier']","f1868033":"\n#First look at train\ndata.sample(5)","f5788e01":"#First look at test\ntest.sample(5)","67de07e3":"#Shape of train and test\nprint('There are {} rows and {} columns in train'.format(data.shape[0],data.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","d98598f7":"#Check Missing values in train\ndata.isna().sum()","f34dff94":"#Check Missing values in test\ntest.isna().sum()","3eaa56f1":"#check datatypes in train\ndata.info()","553b1e39":"#check datatypes in test\ntest.info()","9177b733":"#Lets describe train\ndata.describe()","7bd3727a":"#Lets describe test\ntest.describe()","2bf7d759":"#Lets concatenate train & test\ndf=pd.concat([data,test])\ndf.shape ","9c2b146f":"#Lets convert Year from numerical to object type \ndf['Outlet_Establishment_Year']  = df['Outlet_Establishment_Year'].astype('object')","d1128356":"\ndf.Item_Identifier.nunique()\n#df['Item_Identifier'].value_counts().sample(10)","13cc49ce":"#Identify categorical columns \ndf_cat=df.select_dtypes(include='object')\ndf_cat.drop(['Item_Identifier'],axis=1,inplace=True)\ndf_cat.columns","3aa9b1d3":"#Identify numerical columns\ndf_num=df.select_dtypes(include=['int64','float64'])\nlist(df_num.columns)","2c56afff":"#Explore categorical variables - 'Item_Fat_Content', 'Outlet_Size' \ncat_col_1 = ['Item_Fat_Content', 'Outlet_Size']\ncount = 1\nfor cols in cat_col_1:\n    plt.subplot(2, 2, count)\n    df[cols].value_counts().plot.pie(shadow=True,autopct='%1.1f%%',radius=1.1,textprops={'fontsize': 9} )\n    count +=1\n    plt.subplot(2, 2, count)\n    plt.tight_layout()\n    plt.style.use('fivethirtyeight')\n    df[cols].value_counts().plot.bar()\n    fig=plt.gcf()\n    fig.set_size_inches(12,7)\n    plt.title('{0}'.format(cols))\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=5)\n    plt.xticks(rotation=30)\n    count+=1","46604eb4":"#Explore categorical variables - \ncat_col_1 = [ 'Item_Type', 'Outlet_Identifier']\ncount = 1\nfor cols in cat_col_1:\n    plt.subplot(2, 2, count)\n    df[cols].value_counts().plot.pie(shadow=True,autopct='%1.1f%%',radius=1.2,textprops={'fontsize': 8} )\n    count +=1\n    plt.subplot(2, 2, count)\n    plt.tight_layout()\n    plt.style.use('fivethirtyeight')\n    df[cols].value_counts().plot.bar()\n    fig=plt.gcf()\n    fig.set_size_inches(18,9)\n    plt.title('{0}'.format(cols))\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=9)\n    plt.xticks(rotation=30)\n    count+=1","f9282e45":"#Explore categorical variables - \ncat_col_1 = [ 'Outlet_Location_Type', 'Outlet_Type']\ncount = 1\nfor cols in cat_col_1:\n    plt.subplot(2, 2, count)\n    df[cols].value_counts().plot.pie(shadow=True,autopct='%1.1f%%',radius=1.0,textprops={'fontsize': 9} )\n    count +=1\n    plt.subplot(2, 2, count)\n    plt.tight_layout()\n    plt.style.use('fivethirtyeight')\n    df[cols].value_counts().plot.bar()\n    fig=plt.gcf()\n    fig.set_size_inches(13,8)\n    plt.title('{0}'.format(cols))\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=9)\n    plt.xticks(rotation=30)\n    count+=1","1be8483f":"#Explore categorical variables - \ncat_col_1 = [ 'Outlet_Establishment_Year']\ncount = 1\nfor cols in cat_col_1:\n    plt.subplot(1, 2, count)\n    df[cols].value_counts().plot.pie(shadow=True,autopct='%1.1f%%',radius=1.2,textprops={'fontsize': 10} )\n    count +=1\n    plt.subplot(1, 2, count)\n    plt.tight_layout()\n    plt.style.use('fivethirtyeight')\n    df[cols].value_counts().plot.bar()\n    fig=plt.gcf()\n    fig.set_size_inches(12,4)\n    plt.title('{0}'.format(cols))\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=9)\n    plt.xticks(rotation=30)\n    count+=1","7ab4c6cf":"sns.set(style='whitegrid', palette=\"plasma\", font_scale=1.1, rc={\"figure.figsize\": [8, 5]})\ndf[list(df_num.columns)].hist(bins=15, figsize=(15, 6), layout=(2, 4));","be8eb723":"#Numerical Data ('Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Outlet_Sales' Vs Outlet_Type)\nplt.figure(figsize=(10, 14))\ncount = 1\nfor cols in df_num:\n    sns.set(style='darkgrid', palette=\"Set1\", font_scale=1.1, rc={\"figure.figsize\": [8, 6]})\n    plt.subplot(4, 2, count)\n    plt.tight_layout()\n    #sns.boxenplot(x='Outlet_Type', y= cols, data=data)\n    sorted_nb = df.groupby(['Outlet_Type'])[cols].median().sort_values()\n    sns.boxenplot(x=df['Outlet_Type'], y=df[cols], order=list(sorted_nb.index))\n    plt.xticks(rotation=30)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=9)\n    \n    count+=1","3d31ea1a":"#Numerical Data ('Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Outlet_Sales' Vs Item_Fat_Content)\nplt.figure(figsize=(10, 14))\ncount = 1\nfor cols in df_num:\n    sns.set(style='whitegrid', palette=\"rocket\", font_scale=1.1, rc={\"figure.figsize\": [8, 6]})\n    plt.subplot(4, 2, count)\n    plt.tight_layout()\n    #sns.boxenplot(x='Outlet_Type', y= cols, data=data)\n    sorted_nb = df.groupby(['Item_Fat_Content'])[cols].median().sort_values()\n    sns.boxenplot(x=df['Item_Fat_Content'], y=df[cols], order=list(sorted_nb.index))\n    plt.xticks(rotation=30)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=9)\n    \n    count+=1","326b0815":"#Numerical Data ('Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Outlet_Sales' Vs Item_Type)\nplt.figure(figsize=(14, 12))\ncount = 1\nfor cols in df_num:\n    sns.set(style='darkgrid', palette=\"ocean\", font_scale=1.1)\n    plt.subplot(4, 2, count)\n    plt.tight_layout()\n    #sns.boxenplot(x='Outlet_Type', y= cols, data=data)\n    sorted_nb = df.groupby(['Item_Type'])[cols].median().sort_values()\n    sns.boxenplot(x=df['Item_Type'], y=df[cols], order=list(sorted_nb.index))\n    plt.xticks(rotation=60)\n    plt.xticks(fontsize=9)\n    plt.yticks(fontsize=9)\n    \n    count+=1","437cd954":"#Numerical Data ('Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Outlet_Sales' Vs Outlet_Location_Type)\nplt.figure(figsize=(14, 12))\ncount = 1\nfor cols in df_num:\n    sns.set(style='ticks', palette=\"tab10\", font_scale=1.1)\n    plt.subplot(4, 2, count)\n    plt.tight_layout()\n    #sns.boxenplot(x='Outlet_Type', y= cols, data=data)\n    sorted_nb = df.groupby(['Outlet_Location_Type'])[cols].median().sort_values()\n    sns.boxenplot(x=df['Outlet_Location_Type'], y=df[cols], order=list(sorted_nb.index))\n    plt.xticks(rotation=60)\n    plt.xticks(fontsize=9)\n    plt.yticks(fontsize=9)\n    \n    count+=1","2300062f":"#Numerical Data ('Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Outlet_Sales' Vs Outlet_Size)\nplt.figure(figsize=(14, 12))\ncount = 1\nfor cols in df_num:\n    sns.set(style='dark', palette=\"rocket\", font_scale=1.1)\n    plt.subplot(4, 2, count)\n    plt.tight_layout()\n    #sns.boxenplot(x='Outlet_Type', y= cols, data=data)\n    sorted_nb = df.groupby(['Outlet_Size'])[cols].median().sort_values()\n    sns.boxplot(x=df['Outlet_Size'], y=df[cols], order=list(sorted_nb.index))\n    plt.xticks(rotation=60)\n    plt.xticks(fontsize=9)\n    plt.yticks(fontsize=9)\n    \n    count+=1","c3fc92f0":"#Numerical Data ('Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Outlet_Sales' Vs Outlet_Establishment_Year)\nplt.figure(figsize=(14, 12))\ncount = 1\nfor cols in df_num:\n    sns.set(style='whitegrid', palette=\"rocket\", font_scale=1.1)\n    plt.subplot(4, 2, count)\n    plt.tight_layout()\n    #sns.boxenplot(x='Outlet_Type', y= cols, data=data)\n    sorted_nb = df.groupby(['Outlet_Establishment_Year'])[cols].median().sort_values()\n    sns.boxenplot(x=df['Outlet_Establishment_Year'], y=df[cols], order=list(sorted_nb.index))\n    plt.xticks(rotation=60)\n    plt.xticks(fontsize=9)\n    plt.yticks(fontsize=9)\n    \n    count+=1","a2c5b43c":"#Variable - Outlet_Type Vs Item_Outlet_Sales\n#Catplot\nsns.catplot(x='Outlet_Location_Type',y='Item_Outlet_Sales',kind='boxen',data=df,col='Outlet_Type',hue='Outlet_Size',palette=\"Set1\")","7a1121bb":"#Item Weight - Item weight should depend upon the item type and Item_Fat_Content a per above numerical analysis with boxenplots\nItem_Weight_group = df.groupby([\"Item_Type\",\"Item_Fat_Content\"])[\"Item_Weight\"]\n\n#printing the variabe that we created by median\nprint(Item_Weight_group.median())\n\ndf = df.reset_index()","04d5b3f9":"# using the groupby to transform this variables\ndf.loc[df.Item_Weight.isnull(), 'Item_Weight'] = df.groupby(['Item_Type','Item_Fat_Content']).Item_Weight.transform('median')","b8bed23a":"# printing the total of nulls in Item_Weight Feature\nprint(df[\"Item_Weight\"].isnull().sum())","b02690ee":"#Identify outlet type & Outlet_Location_Type for missing Outlet_Type\nprint(df[df.Outlet_Type.isnull()].Outlet_Size.value_counts())","5074d7f0":"df.loc[df.Outlet_Size.isnull(),['Outlet_Type','Outlet_Location_Type']].sample(5)","808c2b09":"df['Outlet_Size'].fillna('Small', inplace=True)\n","72d67546":"df.isnull().sum()","d8d3a8b3":"#Reset index\ndf.drop('index', axis = 1, inplace = True)\n#Drop non-imp columns\ndf.drop('Item_Identifier', axis = 1, inplace = True)","4de04e93":"#Converting categorocal variables to dummy variables \ndf=pd.get_dummies(df,drop_first=True)\n\n# Segregating train and test from df\ntrain=df[:data.shape[0]]\ntest1=df[data.shape[0]:]","54aaca16":"#Shape of train and test\nprint('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test1.shape[0],test1.shape[1]))\n\ntrain.drop('Item_Outlet_Sales', axis = 1, inplace = True)\ntest1.drop('Item_Outlet_Sales', axis = 1, inplace = True)","c7ed7237":"validation_size = 0.20\nseed = 7","7053bd01":"# Split-out validation dataset\nX_train, X_validation, Y_train, Y_validation = train_test_split(train.values, data['Item_Outlet_Sales'].values, test_size=validation_size, random_state=seed)","f4cc5889":"#Standard scaling\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=np.array([80,82,84,85,90,91,92,94,95,150,200]))","c0c293d6":"#Use GridSearchCV to find best estimator & RMS (root mean square)to calculate score\nnum_folds = 10\nRMS = 'neg_mean_squared_error'\nmodel = GradientBoostingRegressor(random_state=seed)\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)","961cd4d2":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","d3dd1b0b":"#Make predictions on validation dataset using best estimstor = 80\n# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(alpha=0.9,learning_rate=0.05, max_depth=5, min_samples_leaf=3, min_samples_split=2, n_estimators=80, random_state=30)\nmodel.fit(rescaledX, Y_train)\n# transform the validation dataset\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(\"RMSE = \", mean_squared_error(Y_validation, predictions))","841be1eb":"#Stanrdard scaling of the test dataset\nrescaled_test1 = scaler.transform(test1)","c73c151b":"#Predict on the test dataset\npredicted_prices = model.predict(rescaled_test1)\n\n#PRepare submission file\nsubmission['Item_Outlet_Sales'] = predicted_prices\nsubmission.to_csv('.\/submission_rahulpednekar.csv',index=False)\nsubmission.head()","3f00da30":"* Item weight :  same across all \n* Item_Visibility : Tier 2 < 3 & 1\n* Item_MRP : same across all\n* Item_Outlet_Sales : Tier 1 < 3 < 2 (i.e. Tier 2 generates highest sales median value)","c5c187eb":"* Item weight : Its same across diff outlets. Item weight is missing for super mkt 3 and grocery as per below stats\n* Item_Visibility : Grocery stores have highest visibility, rest all super markets have same visibility\n* Item_MRP : MRP is same across all outlets which is obvious\n* Item_Outlet_Sales : Median Sales volume are highest at super mkt type 3 > super mkt 1 >  super mkt 2 > grocery","51dd6170":"* 1985: Maximum count of 2300+ can be seen\n* 1998 : Minimum count of 900 can be seen\n* All other years have same count which is surprising","728001be":"> > **If you have liked my Kernel then please UPVOTE**","a69fb399":"Sales Prediction for Big Mart Outlets\nThe data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and predict the sales of each product at a particular outlet.\n\nUsing this model, BigMart will try to understand the properties of products and outlets which play a key role in increasing sales.\n\nPlease note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly. \n\nData Dictionary\nWe have train (8523) and test (5681) data set, train data set has both input and output variable(s). You need to predict the sales for test data set.","2abaf5f3":"* Outlet_Location_Type : Max count for Tier3 (39.3%) and Tier 1 is having lowest count\n* Outlet_Type : Supermarket Type1 has mx count (65.4%)","74f4a391":"* Imputing missing values for Outlet Size :\n* There are three types of outlet sizes : small\/medium\/large\n* The mussing values exist for the below combination of outlet type & outlet location type\n    * a. Grocery & Tier 3\n    * b. Super market 1 & Tier 2\n\n* As per the above boxen plot we can see that for\n    * a. outlet type = grocery generally the outlet size is small \n    * b. outlet type = super market generally the outlet size is small & medium.\n* Hence I decided to imput small value for all missing values of Outlet Size","1ec30548":"Multivariate Analysis","907f18cc":"In train & test - Item_Weight & Outlet_Size have missing values","e6e307d3":"After trying out many models, I decided to use GBM to predict, as it was showing lowest RMSE","010e8f1f":"Prepare the submission file","8710d17d":"* Item fat content : Low Fat is having maximum (59.7%) count\n* outlet size      : Medium size outlet (45.7%) is having max count","d8413dfa":"> ****This prediction achieved 106th position (RMSE : 1144) in Analytics Vidhya Hackathon on Big Mart Sales  ","afe9bfc9":"* Item weight :  \"low fat\" is having min median item weight < LF < Regular < reg < Low Fat\n* Item_Visibility : same across all categories\n* Item_MRP : Lf < reg & low fat < Low Fat < Regular\n* Item_Outlet_Sales : Median Sales volume reg < low fat < Low Fat < LF < Regular (But not much variance among diff categories)","f51d6cab":"* Item weight :  hard drinks is having min median item weight & others having max median value\n* Item_Visibility : meat is lowest & breakfast is highest in median values\n* Item_MRP : baking goods is lowest and starchy foods is highest in median values \n* Item_Outlet_Sales : soft drinks have min median Vs sea food is having maximum median sales value ","67da93d7":"All null values are imputed and data is ready for modelling","cbd0afb4":"There are 1559 unique values of Item_Identifier with max frequency of 10 and min is 7","938e0979":"Imputing missing values","19c7f8ef":"Scale the test dataset","497eb0d0":"* Item weight :  same across all \n* Item_Visibility : medium  < high & small\n* Item_MRP : same across all\n* Item_Outlet_Sales : small < high < medium (i.e. medium generates highest sales median value)","0e0cec00":"* Item type: Fruits and vegetables has max count of 2k and seafood is min\n* outlet identifier : outl027 is having max countand out019 is having min count","dfc5104e":"#Step 1 : Explore train and test datasets","a1fb8e43":"Imputing missing values : Outlet_Size","5d087d3b":"* Item Weight     : We can see maximum frequency  between 6-20\n* Sales           : Max occurances for sales value 0-2k, few sales can be seen around 9k as well \n* Item_MRP        : Price frequency is max between 100-140 & 150-190\n* Item_Visibility : Maximum frequency between 0.01-.05","87b092e4":"* Item weight :  same across all (1985 has misising data)\n* Item_Visibility : 1999 had lowest Vs 1998 had highest\n* Item_MRP : almost same across all\n* Item_Outlet_Sales : 1998 had lowest sales Vs 1987&2004 had highest (i.e. 1987 & 2004 generates highest sales median value)","7df9d68f":"Tier 3 in type super mkt 3 is having highest median sales","a1dfbcda":"Step 2 : EDA"}}