{"cell_type":{"baab27f4":"code","c100d242":"code","58600bbe":"code","a3f78f16":"code","c4b6385e":"code","7ca920d2":"code","4ce13b4a":"code","f156b31d":"code","2bb08be6":"code","c8bf751b":"code","0a21790e":"code","f2fb5fd3":"code","8a2bb433":"code","ed60182b":"code","4030130e":"code","ea20024c":"code","e1ba3ab5":"code","c12ebae6":"code","0883cf2a":"code","e6fcf69c":"code","6a8a0c7e":"code","1019d375":"code","2316e6f8":"code","3c1d4cc5":"code","a01d6441":"code","e873f1a0":"code","c07b388f":"code","449c6b60":"markdown","aca9728c":"markdown","45f5f51e":"markdown","1922acf7":"markdown","ab3f327f":"markdown","75040c9a":"markdown","cdf17b41":"markdown","61aa25b4":"markdown","e7a01b0e":"markdown","cd07d638":"markdown","03dfa9d1":"markdown","9c9198a3":"markdown","5b332178":"markdown","9c3b5f1b":"markdown","796e0b49":"markdown","5d9dde7b":"markdown","20d3a038":"markdown","f6b241f6":"markdown","b12eec3a":"markdown","d2bb95d8":"markdown","2b59538e":"markdown"},"source":{"baab27f4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport time\nimport optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.impute import SimpleImputer\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)","c100d242":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","58600bbe":"train.head()","a3f78f16":"print(f'Number of Rows in Training Dataset: {train.shape[0]}  \\nNumber of Columns in Training Dataset: {train.shape[1]} \\nTotal Number of missing values in Training Dataset: {sum(train.isna().sum())}')","c4b6385e":"train.describe()","7ca920d2":"test.head()","4ce13b4a":"print(f'Number of Rows in Testing Dataset: {test.shape[0]}  \\nNumber of Columns in Testing Dataset: {test.shape[1]} \\nTotal Number of missing values in Testing Dataset: {sum(train.isna().sum())}')","f156b31d":"test.describe()","2bb08be6":"corr = train.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","c8bf751b":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"orange\", \"lightblue\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\\n\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","0a21790e":"sns.set(font_scale=1.4)\ntrain['claim'].value_counts().plot(kind='bar',figsize=(7, 6), rot=0)\nplt.xlabel(\"Claim (target) value\", labelpad=14)\nplt.ylabel(\"Values\", labelpad=14)\nplt.title(\"Claim Value Distribution\", y=1.02);","f2fb5fd3":"missing_train_df = pd.DataFrame(train.isna().sum())\nmissing_train_df = missing_train_df.drop(['id', 'claim']).reset_index()\nmissing_train_df.columns = ['feature', 'count']\nmissing_train_df['percentage'] = (missing_train_df['count']\/train.shape[0])*100 \nmissing_train_df.head()","8a2bb433":"fig, ax = plt.subplots(figsize=(20, 10))\n\nbars = ax.bar(missing_train_df['feature'],\n              missing_train_df['count'],\n              color=\"lightskyblue\",\n              edgecolor=\"black\",\n              width=0.5\n             )\nax.set_title(\"Missing feature values distribution in the train dataset\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Feature\", fontsize=14, labelpad=15)\nax.tick_params(axis=\"x\", rotation=90, labelsize=8)\nax.margins(0.005, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","ed60182b":"missing_test_df = pd.DataFrame(test.isna().sum())\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'count']\nmissing_test_df['percentage'] = (missing_test_df['count']\/train.shape[0])*100 \nmissing_test_df.head()","4030130e":"fig, ax = plt.subplots(figsize=(20, 10))\n\nbars = ax.bar(missing_test_df['feature'],\n              missing_test_df['count'],\n              color=\"lightskyblue\",\n              edgecolor=\"black\",\n              width=0.5\n             )\nax.set_title(\"Missing feature values distribution in the test dataset\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Feature\", fontsize=14, labelpad=15)\nax.tick_params(axis=\"x\", rotation=90, labelsize=8)\nax.margins(0.005, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","ea20024c":"features = [feature for feature in train.columns if feature not in ('id', 'claim')]","e1ba3ab5":"train['num_missing'] = train[features].isna().sum(axis=1)\ntrain['std_dev'] = train[features].isna().std(axis=1)\n\ntest['num_missing'] = test[features].isna().sum(axis=1)\ntest['std_dev'] = test[features].isna().std(axis=1)\n\nfeatures += ['num_missing', 'std_dev']","c12ebae6":"train[features] = train[features].fillna(train[features].mean())\ntest[features] = test[features].fillna(test[features].mean())","0883cf2a":"scaler = StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","e6fcf69c":"train.shape, test.shape","6a8a0c7e":"X = train.drop([\"id\", \"claim\"], axis=1)\nX_test = test.drop(\"id\", axis=1)\ny = train[\"claim\"]","1019d375":"def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n    preds=0\n    lgbm_params = {\n        \"objective\": trial.suggest_categorical(\"objective\", ['binary']),\n        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.001, 0.005]),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [20000])\n    }\n    \n    model = LGBMClassifier(**lgbm_params, device='gpu')\n    model.fit(X_train, y_train,\n              eval_set = [(X_valid, y_valid)],\n              eval_metric='auc',\n              early_stopping_rounds=100,\n              verbose=False\n             )\n    \n    print(f\"Number of boosting rounds: {model.best_iteration_}\")\n    oof = model.predict_proba(X_valid)[:, 1]\n    return roc_auc_score(y_valid, oof)","2316e6f8":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=2021, stratify=y)\ntime_limit = 3600 * 4\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid, y_train, y_valid),\n               n_trials=1,\n               timeout=time_limit\n              )\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","3c1d4cc5":"lgb_params = {\n    'objective': 'binary',\n    'n_estimators': 20000,\n    'random_state': 2021,\n    'learning_rate': 5e-3,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'importance_type': 'gain',\n    'device': 'gpu'\n}","a01d6441":"lgb_oof = np.zeros(train.shape[0])\nlgb_pred = np.zeros(test.shape[0])\nlgb_importances = pd.DataFrame()\n\nkf = KFold(n_splits=5, shuffle=True, random_state=2021)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train)):\n    print(f\"****** Fold {fold} ******\")\n    X_train = train[features].iloc[train_idx]\n    y_train = y.iloc[train_idx]\n    X_valid = train[features].iloc[val_idx]\n    y_valid = y.iloc[val_idx]\n    X_test = test[features]\n    \n    start = time.time()\n    model = LGBMClassifier(**lgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_valid, y_valid)],\n              eval_metric='auc',\n              early_stopping_rounds=200,\n              verbose=1000\n    )\n    \n    fi_temp = pd.DataFrame()\n    fi_temp['feature'] = model.feature_name_\n    fi_temp['importance'] = model.feature_importances_\n    fi_temp['fold'] = fold\n    fi_temp['seed'] = 2021\n    lgb_importances = lgb_importances.append(fi_temp)\n    \n    lgb_oof[val_idx] = model.predict_proba(X_valid)[:, -1]\n    lgb_pred += model.predict_proba(X_test)[:, -1] \/ 5\n    \n    elapsed = time.time() - start\n    auc = roc_auc_score(y_valid, lgb_oof[val_idx])\n    print(f\"fold {fold} - lgb auc: {auc:.6f}, elapsed time: {elapsed:.2f}sec\\n\")","e873f1a0":"print(f\"oof lgb roc = {roc_auc_score(y, lgb_oof)}\")","c07b388f":"output = pd.DataFrame({'id': test.id,\n                       'claim': lgb_pred})\noutput.to_csv('submission_lgbm_hyper.csv', index=False)","449c6b60":"# **5. HYPERPARAMETER OPTIMIZATION USING OPTUNA**\n\n#### Partial code for Hyperparameter Optimization using Optuna. This can further be extended to find the optimal hyperparameter value for training the Light GBM model.","aca9728c":"# **3. EXPLORATORY DATA ANALYSIS**\n\n### Correlation Plot of the Dataset\n\n### We observed that majority of the correlation values are near to 0 which states that there are no highly dependent features.","45f5f51e":"#### Imputing the null values with the mean of the respective column","1922acf7":"# **6. DATA MODELING**\n\n## **LIGHTGBM CLASSIFIER WITH 5 FOLDS CV**","ab3f327f":"### Visualizing the distribution of data into Training and Testing Set","75040c9a":"### Printing the basic statistics for each variable in Training Dataset which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum.","cdf17b41":"### Printing the information about actual and missing values in Training Data","61aa25b4":"# **4. DATASET PREPROCESSING**\n\n#### Creating a set of required features by dropping the id and claim column","e7a01b0e":"#### Analysing the features and their corresponding missing values in the Testing Dataset","cd07d638":"### Printing the information about actual and missing values in Testing Data","03dfa9d1":"## Reading the dataset files","9c9198a3":"#### Missing feature values distribution in the Test dataset","5b332178":"# **2. DATASET OVERVIEW**\n\n## Train Dataset","9c3b5f1b":"#### Analysing the features and their corresponding missing values in the Training Dataset","796e0b49":"### Printing the basic statistics for each variable in Testing Dataset which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum.","5d9dde7b":"#### Calculating the sum of missing values in each features and standard deviation of each feature. Adding those new calculated columns in our original dataset to use them in the modeling phase","20d3a038":"# **1. DATASET PREPARATION**\n\n## Importing Necessary Libraries","f6b241f6":"### Visualizing the distribution of Claim (Target) in the Training Set\n\n### Here we can observe that the target class is well balanced. This helps us to proceed with applying suitable techniques on the data during the data modeling phase. ","b12eec3a":"#### Missing feature values distribution in the Train dataset","d2bb95d8":"#### Scaling the training and testing features","2b59538e":"## Test Dataset"}}