{"cell_type":{"99ef4768":"code","6ec1fa89":"code","6d98f6aa":"code","87a8f8bb":"code","81d9cbab":"code","d8250705":"code","af67c60e":"code","20e21ae1":"code","30caf332":"code","2f083f8e":"code","7a958d24":"code","9e3930d5":"code","67df40d0":"code","de6a10fd":"code","d1c44027":"code","531d5009":"code","23a19b45":"code","2ec97824":"code","354f83ac":"code","4d9bab6d":"code","a80ff840":"code","bb73ec39":"code","ad25f164":"code","f632ed45":"code","9a064af1":"code","3ade4e86":"code","31c23c4d":"code","5dc10d1a":"code","53921829":"markdown","3509f4c7":"markdown","45693867":"markdown","df438116":"markdown","2bdf8fcd":"markdown","74cef412":"markdown","23c2f54a":"markdown","f923e3dd":"markdown","a556f106":"markdown","d88dc26a":"markdown","6453eb4a":"markdown","cd05bde8":"markdown","b096dd7b":"markdown","614d423f":"markdown","0faa26fe":"markdown","415bd099":"markdown","30ca6a0f":"markdown","9345a7b1":"markdown","4fd4c3ba":"markdown","36fe94ab":"markdown","101b9ae7":"markdown"},"source":{"99ef4768":"# Helper function, used these for debugging purposes\n# detector2 build only succeeds if CUDA version is correct\n\n#!nvidia-smi\n#!nvcc --version\n\n#import torch\n#torch.__version__\n#import torchvision\n#torchvision.__version__\n\n!pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu102\/torch1.7\/index.html\n!pip install fastai","6ec1fa89":"# Base setup:\n# detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.structures import BoxMode","6d98f6aa":"# loading of data\n# challenge1_path = \n\ntraining_path = \"\/kaggle\/input\/d\/ykviki\/dsta-object-detection-til-2021-dataset\/merged\/merged\"\ntrain_annotation = os.path.join(training_path, \"annotations\/train.json\")\nval_annotation = os.path.join(training_path, \"annotations\/val.json\")\nimage_path = os.path.join(training_path,\"images\")","87a8f8bb":"os.path.exists(train_annotation)","81d9cbab":"from detectron2.structures import BoxMode\n# if your dataset is in COCO format, this cell can be replaced by the following three lines:\nfrom detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"train_data\", {}, train_annotation, image_path)\nregister_coco_instances(\"val_data\", {}, val_annotation, image_path)","d8250705":"#visualize training data\nmy_dataset_train_metadata = MetadataCatalog.get(\"train_data\")\ndataset_dicts = DatasetCatalog.get(\"train_data\")\n\nmy_dataset_val_metadata = MetadataCatalog.get(\"val_data\")\nval_dicts = DatasetCatalog.get(\"val_data\")\n\nimport random\nfrom detectron2.utils.visualizer import Visualizer\nimport cv2\nimport matplotlib.pyplot as plt\n\nfor d in random.sample(dataset_dicts, 3):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)\n    vis = visualizer.draw_dataset_dict(d)\n    plt.imshow(vis.get_image()[:, :, ::-1])","af67c60e":"# # DATA AUG\n\n# from detectron2.data import transforms as T\n# # Define a sequence of augmentations:\n# augs = T.AugmentationList([\n#     T.RandomBrightness(0.9, 1.1),\n#     T.RandomFlip(prob=0.5),\n#     T.RandomCrop(\"absolute\", (640, 640))\n# ])  # type: T.Augmentation\n\n# # Define the augmentation input (\"image\" required, others optional):\n# input = T.AugInput(image, boxes=boxes, sem_seg=sem_seg)\n\n# # Apply the augmentation:\n# transform = augs(input)  # type: T.Transform\n# image_transformed = input.image  # new image\n# sem_seg_transformed = input.sem_seg  # new semantic segmentation\n\n# # For any extra data that needs to be augmented together, use transform, e.g.:\n# image2_transformed = transform.apply_image(image2)\n# polygons_transformed = transform.apply_polygons(polygons)","20e21ae1":"# # Run training\n\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.evaluation import COCOEvaluator\n\n\nclass CocoTrainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            os.makedirs(\"coco_eval\", exist_ok=True)\n            output_folder = \"coco_eval\"\n        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n    \n","30caf332":"# LOADING PREV FORMAT\n\n# from detectron2.config.config import CfgNode as CN\n\n# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml\"))\n# cfg.DATASETS.TRAIN = (\"train_data\",)\n# cfg.DATASETS.TEST = (\"val_data\",)\n# cfg.DATALOADER.NUM_WORKERS = 4\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml\")  # Let training initialize from model zoo\n# cfg.SOLVER.IMS_PER_BATCH = 4\n# cfg.SOLVER.BASE_LR = 0.001\n# cfg.SOLVER.WARMUP_ITERS = 1000\n# cfg.SOLVER.MAX_ITER = 20000 #adjust up if val mAP is still rising, adjust down if overfit\n# cfg.SOLVER.STEPS = [1000,8000,16000]\n# cfg.SOLVER.GAMMA = 0.05\n# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 7\n# # cfg.TEST.EVAL_PERIOD = 1000\n\n\n# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n# trainer = CocoTrainer(cfg) \n# # trainer.resume_or_load(resume=False)\n\n# trainer.train()","2f083f8e":"# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/retinanet_R_50_FPN_1x.yaml\"))\n# cfg.DATASETS.TRAIN = (\"train_data\",)\n# cfg.DATASETS.TEST = (\"val_data\",)\n# cfg.DATALOADER.NUM_WORKERS = 4\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/retinanet_R_50_FPN_1x.yaml\")  # Let training initialize from model zoo\n# cfg.SOLVER.IMS_PER_BATCH = 4\n# cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n# cfg.SOLVER.MAX_ITER = 300    # 300 iterations enough for this dataset; Train longer for a practical dataset\n# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, enough for this dataset (default: 512)\n# # cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5  # classes for RCNN\n# cfg.MODEL.RETINANET.NUM_CLASSES = 5 # Classes for Retina\n# cfg.TEST.EVAL_PERIOD = 500\n\n\n# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n# trainer = DefaultTrainer(cfg) \n# trainer.resume_or_load(resume=False)\n# trainer.train()","7a958d24":"# RELOADING MODEL\nfrom detectron2.config.config import CfgNode as CN\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml\"))\ncfg.MODEL.WEIGHTS = os.path.join(\"\/kaggle\/input\/detectron-dsta-model\", \"model_final (3).pth\")  # path to the model we trained\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\ncfg.DATASETS.TRAIN = (\"train_data\",)\ncfg.DATASETS.TEST = (\"val_data\",)\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.SOLVER.BASE_LR = 0.005\ncfg.SOLVER.WARMUP_ITERS = 1000\ncfg.SOLVER.MAX_ITER = 9000 #adjust up if val mAP is still rising, adjust down if overfit\ncfg.SOLVER.STEPS = [1000,3000,6000]\ncfg.SOLVER.GAMMA = 0.05\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 7\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a testing threshold\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = CocoTrainer(cfg) \ntrainer.resume_or_load(resume=False)\n\ntrainer.train()","9e3930d5":"predictor = DefaultPredictor(cfg)","67df40d0":"from detectron2.utils.visualizer import ColorMode\nval_dict = DatasetCatalog.get(\"val_data\")\n\nfor d in random.sample(val_dict, 3):    \n    im = cv2.imread(d[\"file_name\"])\n    outputs = predictor(im) \n    v = Visualizer(im[:, :, ::-1],\n                   metadata=my_dataset_train_metadata, \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. Only available for segmentation models\n    )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    plt.figure(figsize=(15,7))\n    plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","de6a10fd":"from detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\nfrom detectron2.modeling import build_model\n\n\nevaluator = COCOEvaluator(\"val_data\", None, False, output_dir=\".\/output\/\")\n# evaluator = COCOEvaluator(\"val_data\", (\"bbox\", \"segm\"), False, output_dir=\".\/output\/\")\n\n# Loading model\nmodel_uploaded = build_model(cfg)\n\nval_loader = build_detection_test_loader(cfg, \"val_data\")\n# print(inference_on_dataset(trainer.model, val_loader, evaluator))\nprint(inference_on_dataset(model_uploaded, val_loader, evaluator))","d1c44027":"# cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n# cfg.MODEL.WEIGHTS = os.path.join(\"\/kaggle\/input\/objectron-retinanetv1\/model_final.pth\")\n\n\nc3_test_path = \"\/kaggle\/input\/d\/ykviki\/dsta-object-detection-til-2021-dataset\/challenge_3_test_dataset\/challenge_3_test_dataset\"\n\ncfg.DATASETS.TEST = (\"my_dataset_test\", )\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\ntest_metadata = MetadataCatalog.get(\"my_dataset_test\")\n\nfrom detectron2.utils.visualizer import ColorMode\nimport glob\n\nou_test = []\nfor imageName in glob.glob(os.path.join(c3_test_path,\"images\/*.jpg\")):\n  im = cv2.imread(imageName)\n  outputs = predictor(im)\n  ou_test.append(outputs)\n  v = Visualizer(im[:, :, ::-1],\n                metadata=my_dataset_train_metadata, \n                scale=0.8\n                 )\n  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n  plt.imshow(out.get_image()[:, :, ::-1])\n    \n\n","531d5009":"ou_test","23a19b45":"#\nim = cv2.imread(os.path.join(c3_test_path,\"images\/44.jpg\"))\noutputs = predictor(im)\nv = Visualizer(im[:, :, ::-1],\n            metadata=my_dataset_train_metadata, \n            scale=0.8\n             )\nout = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\nplt.imshow(out.get_image()[:, :, ::-1])","2ec97824":"# generate detections on the folder of test images (this will be used for submission)\nfrom PIL import Image, ImageDraw\nfrom torchvision import transforms\nfrom torchvision.ops import batched_nms\nfrom torchvision.transforms import functional as F\nimport torch\n\ndetections = []\n\nfor imageName in glob.glob(os.path.join(c3_test_path,\"images\/*.jpg\")):\n\n        im = cv2.imread(imageName)\n        outputs = predictor(im)\n        classes = outputs[\"instances\"].pred_classes.tolist()\n        box_round = outputs[\"instances\"].pred_boxes.tensor.tolist()\n        score_output = outputs[\"instances\"].scores.tolist()\n        head, tail = os.path.split(imageName)\n        img_id = int(tail.split('.')[0])\n\n        for i in range(len(box_round)):\n\n            x1, y1, x2, y2 = box_round[i]\n            label = int(classes[i]) + 1\n            score = float(score_output[i])\n\n            left = int(x1)\n            top = int(y1)\n            width = int(x2 - x1)\n            height = int(y2 - y1)\n\n            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})\n\ntest_pred_json = os.path.join(\"\/kaggle\/working\", \"test_pred_2.json\")\nwith open(test_pred_json, 'w') as f:\n    json.dump(detections, f)","354f83ac":"# Check \nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nsample_json_path = os.path.join(c3_test_path,\"c2_test_sample.json\")\n\ncoco_gt = COCO(sample_json_path)\ncoco_dt = coco_gt.loadRes(test_pred_json)\ncocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\ncocoEval.evaluate()\ncocoEval.accumulate()\ncocoEval.summarize()","4d9bab6d":"# !wget http:\/\/images.cocodataset.org\/val2017\/000000282037.jpg -q -O input.jpg\n# im = cv2.imread(\".\/input.jpg\")","a80ff840":"# cfg = get_cfg()   # fresh config\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints\/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints\/keypoint_rcnn_R_50_FPN_3x.yaml\")\n# predictor = DefaultPredictor(cfg)\n# outputs = predictor(im)\n# v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n# out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n# plt.figure(figsize=(15,7))\n# plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","bb73ec39":"# !wget http:\/\/images.cocodataset.org\/val2017\/000000282037.jpg -q -O input.jpg\n# im = cv2.imread(\".\/input.jpg\")","ad25f164":"# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x.yaml\"))\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x.yaml\")\n# predictor = DefaultPredictor(cfg)\n# panoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\n# v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n# out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n# plt.figure(figsize=(25,15))\n# plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","f632ed45":"# from IPython.display import YouTubeVideo, display, Video # for viewing the video\n# !pip install youtube-dl # for downloading the video","9a064af1":"# #video = YouTubeVideo(\"ll8TgCZ0plk\", width=500)#7HaJArMDKgI\n# video = YouTubeVideo(\"7HaJArMDKgI\", width=750, height= 450)#\n# display(video)","3ade4e86":"# !youtube-dl https:\/\/www.youtube.com\/watch?v=7HaJArMDKgI -f 22 -o video.mp4\n# !ffmpeg -i video.mp4 -t 00:00:10 -c:v copy video-clip.mp4 ","31c23c4d":"# !git clone https:\/\/github.com\/facebookresearch\/detectron2\n# !python detectron2\/demo\/demo.py --config-file detectron2\/configs\/COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output 1video-output.mkv \\\n#   --opts MODEL.WEIGHTS detectron2:\/\/COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x\/139514519\/model_final_cafdb1.pkl","5dc10d1a":"# !git clone https:\/\/github.com\/vandeveldemaarten\/tempdetector2video.git\n# Video(\".\/tempdetector2video\/myvideo.mkv\")","53921829":"Above we can see that our models performs pretty well! Let's now evaluate our custom model with [Evaluators](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/engine.html?highlight=DefaultPredictor#detectron2.engine.defaults.DefaultPredictor). Two evaluators can be used:\n* [**COCOEvaluator**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/evaluation.html#detectron2.evaluation.COCOEvaluator) can evaluate AP (Average Precision) for box detection, instance segmentation and keypoint detection.\n* [**SemSegEvaluator**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/evaluation.html#detectron2.evaluation.SemSegEvaluator) can evaluate semantic segmentation metrics.\n\nAfterwards we'll use the [**build_detection_test_loader**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/data.html?highlight=build_detection_test_loader#detectron2.data.build_detection_test_loader) which returns a torch DataLoader, that loads the given detection dataset.\n\nAt last we'll use the model, evaluated and dataloader within the [inference_on_dataset](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/evaluation.html#detectron2.evaluation.inference_on_dataset) function. It runs the model on the dataloader and evaluates the metric with the evaluator.","3509f4c7":"Reload the data.","45693867":"Underneed you'll find the extra libraries we'll use in this notebook. More libraries will be added througout the notebook when needed.","df438116":"Downloading the video and cropping 6 seconds for processing\n","2bdf8fcd":"Notice that by using the [**ColorMode.IMAGE_BW**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/utils.html?highlight=ColorMode#module-detectron2.utils.visualizer) we we're capable of removing the colors from objects which aren't detected!","74cef412":"<a id=\"modelevaluation\" ><\/a>\n## 4.4. Model evaluation\nLet's check out the performance of our model!\n\nFirst of all let's make some predictions! We're going to use the [**DefaultPredictor**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/engine.html?highlight=DefaultPredictor#detectron2.engine.defaults.DefaultPredictor) class. Ofcourse we'll use the same cfg that we used during training. We'll change two parameters for our inferencing.","23c2f54a":"#### RetinaNet","f923e3dd":"<a id=\"panoptic\" ><\/a>\n## 5.2. Panoptic segmentation","a556f106":"# 4. Training","d88dc26a":"### Loading model","6453eb4a":"Running the model","cd05bde8":"<a id=\"semantic\" ><\/a>\n## 5.3. Semantic, Densepose, ...\n\nWill be added in a future version! Stay tuned!","b096dd7b":"<a id=\"thevideo\" ><\/a>\n## 6.2. The video","614d423f":"<a id=traincustom> <\/a>\n## 3. Data Preparation\nLet's first check our training data! Ofcourse we'll use the **Visualizer** class again.","0faa26fe":"<a id=\"othermodels\" ><\/a>\n# 5. Other models\n\nIt's possible to use other high-end object detection models aswell. Let's check it out!\n\n<a id=\"keypoint\" ><\/a>\n## 5.1. Keypoint detection","415bd099":"<a id=\"video\" ><\/a>\n# 6. Video\n\nSo up until now we've been working with images only. Can we quickly use the models for videos? The answer is YES!\n\n<a id=\"videolib\" ><\/a>\n## 6.1. Libraries\nAs you can see we actually don't need many other libraries. Lets import a library to handle the video.","30ca6a0f":"Let's check the result! \n\n*I've ran into some trouble with video encoding opencv and ffmpeg (fix in future version of this notebook).*","9345a7b1":"# That's all for now!\n\nThank you for reading this notebook! If you enjoyed it, please upvote!\n\n*More coming soon!*","4fd4c3ba":"#### FASTERCNN\n","36fe94ab":"## Test","101b9ae7":"<a id=\"videoinference\" ><\/a>\n## 6.3. Inference on the video\nLet's now run an panoptic model over the video above.\n\n*note: For now I'll be using some [demo](https:\/\/github.com\/facebookresearch\/detectron2\/tree\/master\/demo) files, I'll later add the code implementations to this notebook.*"}}