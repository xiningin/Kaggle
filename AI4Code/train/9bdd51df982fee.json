{"cell_type":{"662252e2":"code","237c831b":"code","fb5bfaf8":"code","c09b7546":"code","33a14a21":"code","4ed0884d":"code","2de00436":"code","433cfaa1":"code","ca121d00":"code","097384b5":"code","52ca71cd":"code","30382e37":"code","7483334d":"code","609d5079":"code","117284f3":"code","934f5a55":"code","09d2befc":"code","576d179f":"code","d405ecdd":"code","38f9463a":"code","3572f8d1":"code","c9c04ed0":"code","128d19f1":"code","1857adfe":"code","16116407":"code","a1748814":"code","a37bd545":"code","0c64b141":"code","fbe0658c":"code","82651b58":"code","72b2d2e0":"code","a03c1a12":"code","4fa7357e":"code","dd2a940e":"code","bde2d802":"code","0a94e50d":"code","00688063":"code","1e9eb744":"code","039a1684":"code","920d1801":"code","2aad1737":"code","16ee3d13":"code","563cd690":"code","155190f9":"code","7aebbc5b":"code","27ed5899":"code","e6ff4b7e":"code","904ea82f":"code","d7471ced":"code","b84e920d":"code","8bb16fd2":"code","888a3d60":"code","a96c6f87":"code","1f296b59":"code","01eb2a72":"code","62f36a84":"code","af58d757":"code","39258140":"code","9c8fbff9":"code","ae750658":"code","b5ec6033":"code","6da4326c":"code","af73e538":"code","f2ee2fac":"code","a2386b8c":"code","6f32b767":"code","dae2fa5c":"code","f60267ed":"code","49ac0f47":"code","9c322bae":"code","9b64e710":"code","5f60ca4d":"code","19e96c9f":"code","af869daa":"code","b3be85e1":"code","bfa8f5f3":"code","e6ed6578":"code","60d34e36":"code","ef16b829":"code","fef5ba90":"code","4996836e":"code","5635b1af":"code","9d9fbecb":"code","c9c0e388":"code","b912dd7e":"code","dc473b9e":"code","799fca87":"code","2b7053b3":"code","62a5b924":"code","ad810912":"code","f9e09731":"code","0ac5fc1d":"code","4fed38ae":"code","a12deb63":"code","f81ef57b":"code","0bfb55a1":"code","20756ad2":"code","09e9b83b":"code","6698f1c8":"code","9eef2b94":"code","fa431bf2":"code","c3c9553c":"code","aafe74f2":"code","33ba24c6":"code","cba78d05":"code","dd34ebf2":"code","bd7f1755":"code","a392ee0a":"markdown","f2f2e444":"markdown","20d4ea1f":"markdown","c200ee51":"markdown","2fadf420":"markdown","fd2ef84d":"markdown","aeb4584f":"markdown","7f26ab24":"markdown","c05d278c":"markdown","d7378afa":"markdown","d862fd43":"markdown","682bff43":"markdown","d239f8b5":"markdown","e593ff5d":"markdown","e4ee4b06":"markdown","59090506":"markdown","78ef1fd1":"markdown","264ca100":"markdown","4ca9ed46":"markdown","8a808970":"markdown","33afd66b":"markdown","816b2b13":"markdown","1161e499":"markdown","42eab3bb":"markdown","603b1800":"markdown","51d70b6d":"markdown","c619f09c":"markdown","5bcaa277":"markdown","21a9ea68":"markdown","912013ba":"markdown","349603fe":"markdown","f83b52f1":"markdown","72c3af68":"markdown","e486b4fd":"markdown","88a24fd7":"markdown","fb18af2d":"markdown","528c545c":"markdown","869d40e9":"markdown","abc93faa":"markdown","d384e350":"markdown","b41c2202":"markdown","c6d01b64":"markdown","312f93d1":"markdown","114aa2c6":"markdown","b39674dc":"markdown","2ee9ff2c":"markdown","84d23981":"markdown","66d29697":"markdown","5369fcb0":"markdown","32e05c70":"markdown","27736303":"markdown","f08bc150":"markdown","85620474":"markdown","809326d2":"markdown","58271f45":"markdown","4b621c6d":"markdown","e7ab9839":"markdown","13791da4":"markdown","462e0677":"markdown","b3cbadca":"markdown","1ef2c16f":"markdown","0f443de1":"markdown","64acc83f":"markdown","7beee393":"markdown","320788bc":"markdown","c4293c59":"markdown","b89f7494":"markdown"},"source":{"662252e2":"#Import Libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport missingno as msno\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import base\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\n!pip install category_encoders\n!pip install rfpimp\nfrom category_encoders import BinaryEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score,classification_report,confusion_matrix\nfrom IPython.display import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")","237c831b":"#Read The Dataset\n\nurl = \"https:\/\/drive.google.com\/uc?id=1TiEhIjpjB6KUxvqqgVeps9SkKTff1hvY\"\n\ndata = pd.read_csv(url)","fb5bfaf8":"data.head()","c09b7546":"data.shape","33a14a21":"data.info()","4ed0884d":"#Inserting KEY\n\n#In this data, we define a column with a unique value of 65188 as a key.\n\ndata.insert(0,\"KEY\",data.CLNHGVS)","2de00436":"data['KEY'].nunique()==len(data)","433cfaa1":"data.describe()","ca121d00":"%matplotlib inline\n# Histogram of the target categories\ndef histogram(df,feature):\n    ncount = len(df)\n    ax = sns.countplot(x = feature, data=df ,palette=\"hls\")\n    sns.set(font_scale=1)\n    ax.set_xlabel('Target Segments')\n    plt.xticks(rotation=90)\n    ax.set_ylabel('Number of Observations')\n    fig = plt.gcf()\n    fig.set_size_inches(12,5)\n    # Make twin axis\n    ax2=ax.twinx()\n    # Switch so count axis is on right, frequency on left\n    ax2.yaxis.tick_left()\n    ax.yaxis.tick_right()\n    # Also switch the labels over\n    ax.yaxis.set_label_position('right')\n    ax2.yaxis.set_label_position('left')\n    ax2.set_ylabel('Frequency [%]')\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.2f}%'.format(100.*y\/ncount), (x.mean(), y), \n                ha='center', va='bottom') # set the alignment of the text\n    # Use a LinearLocator to ensure the correct number of ticks\n    ax.yaxis.set_major_locator(ticker.LinearLocator(11))\n    # Fix the frequency range to 0-100\n    ax2.set_ylim(0,100)\n    ax.set_ylim(0,ncount)\n    # And use a MultipleLocator to ensure a tick spacing of 10\n    ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n    # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\n    ax2.grid(None)\n    plt.title('Histogram of Binary Target Categories', fontsize=20, y=1.08)\n    plt.show()\n    plt.savefig('target_histogram.png')\n    del ncount, x, y","097384b5":"histogram(data,\"CLASS\")","52ca71cd":"def MissingUniqueStatistics(df):\n  \n  total_entry_list = []\n  total_missing_value_list = []\n  missing_value_ratio_list = []\n  data_type_list = []\n  unique_values_list = []\n  number_of_unique_values_list = []\n  variable_name_list = []\n  \n  for col in df.columns:\n\n    variable_name_list.append(col)\n    missing_value_ratio = round((df[col].isna().sum()\/len(df[col])),4)\n    total_entry_list.append(df[col].shape[0] - df[col].isna().sum())\n    total_missing_value_list.append(df[col].isna().sum())\n    missing_value_ratio_list.append(missing_value_ratio)\n    data_type_list.append(df[col].dtype)\n    unique_values_list.append(list(df[col].unique()))\n    number_of_unique_values_list.append(len(df[col].unique()))\n\n  data_info_df = pd.DataFrame({'Variable':variable_name_list,'#_Total_Entry':total_entry_list,\\\n                           '#_Missing_Value':total_missing_value_list,'%_Missing_Value':missing_value_ratio_list,\\\n                           'Data_Type':data_type_list,'Unique_Values':unique_values_list,\\\n                           '#_Uniques_Values':number_of_unique_values_list})\n  \n  return data_info_df.sort_values(by=\"#_Missing_Value\",ascending=False)","30382e37":"data_info = MissingUniqueStatistics(data)\ndata_info = data_info.set_index(\"Variable\")\ndata_info","7483334d":"drop_list = list(data_info[data_info[\"%_Missing_Value\"] >= 0.99].index)\n\ndata.drop(drop_list,axis = 1,inplace = True) # --> MAIN DF CHANGED","609d5079":"data[\"Protein_position\"].unique() , data[\"CDS_position\"].unique()  , data[\"cDNA_position\"].unique()","117284f3":"def value_correction(df,columns):\n  \n  for col in columns:\n\n    value_correction = pd.DataFrame(df[col][df[col].notnull()].str.split(\"-\").tolist(),columns=[\"X\",\"Y\"])\n    value_correction[\"X\"][value_correction[\"X\"]==\"?\"] = value_correction[\"Y\"]\n    key = df[[col,\"KEY\"]][df[col].notnull()][\"KEY\"]\n\n    counter = 0\n\n    for i in key.index:\n\n      df[col][i] = value_correction[\"X\"][counter]\n      counter += 1\n\n    df[col] = df[col].astype(float)\n  return df","934f5a55":"data = value_correction(data,[\"CDS_position\",\"cDNA_position\",\"Protein_position\"]) # --> MAIN DF CHANGED","09d2befc":"f, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(data.corr(), annot=True)\nplt.show()","576d179f":"data.drop([\"CDS_position\",\"cDNA_position\"],axis = 1, inplace = True) # --> MAIN DF CHANGED","d405ecdd":"data[[\"EXON\",\"INTRON\"]][data[\"INTRON\"].notnull() & data[\"EXON\"].notnull()].head(10)","38f9463a":"def convert_to_float(df,columns):\n  \n  for col in columns:\n       \n    convert_to_float = pd.DataFrame(df[col][df[col].notnull()].str.split(\"\/\").tolist(),columns=[\"Numerator\",\"Denominator\"])\n    convert_to_float = convert_to_float.astype(\"float\")\n    convert_to_float[\"Result\"] = convert_to_float[\"Numerator\"] \/ convert_to_float[\"Denominator\"]\n    key =df[[col,\"KEY\"]][df[col].notnull()][\"KEY\"]\n\n    counter = 0\n    for i in key.index:\n\n      df[col][i] = convert_to_float[\"Result\"][counter]\n      counter += 1\n    df[col] = df[col].astype(float)\n\n  return df","3572f8d1":"data = convert_to_float(data,[\"INTRON\",\"EXON\"]) # --> MAIN DF CHANGED","c9c04ed0":"MissingUniqueStatistics(data)","128d19f1":"msno.bar(data,color='#79ccb3',sort='descending')\nplt.show()","1857adfe":"msno.matrix(data,color=(0.45,0.45,0.64),figsize=(27, 10), width_ratios=(10, 0))\nplt.show()","16116407":"msno.dendrogram(data);","a1748814":"msno.heatmap(data)\nplt.show()","a37bd545":"data[\"EXON\"][data[\"EXON\"].isnull()]","0c64b141":"data[\"INTRON\"][data[\"INTRON\"].notnull()]","fbe0658c":"data[\"EXON\"][data[\"EXON\"].isnull()] = data[\"INTRON\"][data[\"INTRON\"].notnull()] # --> MAIN DF CHANGED","82651b58":"data.drop([\"INTRON\"], axis = 1, inplace = True) # --> MAIN DF CHANGED","72b2d2e0":"data_info = MissingUniqueStatistics(data)\ndata_info = data_info.set_index(\"Variable\")\ndata_info","a03c1a12":"#Creating new column for add of variable type\ndata_info[\"Variable_Type\"] = [\"Ordinal\",\"Ordinal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Continuous\",\"Nominal\",\n                              \"Continuous\",\"Continuous\",\"Continuous\",\"Continuous\",\"Continuous\",\"Nominal\",\"Nominal\",\"Nominal\",\n                              \"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Ordinal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\n                              \"Continuous\",\"Continuous\",\"Continuous\",\"Nominal\",\"Nominal\",\"Cardinal\",\"Ordinal\"]\ndata_info","4fa7357e":"# 1- Row Uniqueness (Drop Duplicates) \nlen(data_info.index) == data_info.shape[0]","dd2a940e":"# 2- Column Uniqueness (Drop Singletons)\nnumerical_columns = list(data_info.loc[(data_info.loc[:,\"Variable_Type\"]==\"Cardinal\") |\n                                       (data_info.loc[:,\"Variable_Type\"]==\"Continuous\")].index)\nlen(numerical_columns), numerical_columns","bde2d802":"categorical_columns = list(data_info.loc[(data_info.loc[:,\"Variable_Type\"]==\"Nominal\") |\n                                       (data_info.loc[:,\"Variable_Type\"]==\"Ordinal\")].index)\nlen(categorical_columns), categorical_columns","0a94e50d":"def ZeroVarianceFinder(df, numerical_columns):\n  \n  import pandas as pd\n  import numpy as np\n\n  zerovariance_numerical_features=[]\n  for col in numerical_columns:\n      try:\n          if pd.DataFrame(df[col]).describe().loc['std'][0] == 0.00 or \\\n          np.isnan(pd.DataFrame(df[col]).describe().loc['std'][0]):\n              zerovariance_numerical_features.append(col)\n      except:\n          print(\"Error:\",col)\n  return zerovariance_numerical_features","00688063":"zerovariance_numerical_features = ZeroVarianceFinder(data,numerical_columns)\nzerovariance_numerical_features","1e9eb744":"singleton_categorical_features=[]\nfor col in categorical_columns:\n    if len(data[col].unique()) <= 1:\n        singleton_categorical_features.append(col)\nlen(singleton_categorical_features), singleton_categorical_features","039a1684":"y = data.loc[:,\"CLASS\"]","920d1801":"x1 = data.iloc[:,1:15]\nx2 = data.iloc[:,16:]\nx = pd.concat([x1,x2],axis = 1)","2aad1737":"X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.33,random_state=42)\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape","16ee3d13":"histogram(X_train,Y_train)","563cd690":"histogram(X_test,Y_test)","155190f9":"df_train = data.copy()\nnumerical_columns_remains = numerical_columns\n        \nsparse_columns = []\nfor col in numerical_columns_remains:\n    if (df_train[col].quantile(0.01)==df_train[col].quantile(0.25)==df_train[col].mode()[0]):\n        sparse_columns.append(col)\n\nsparse_columns_2 = []\nfor col in numerical_columns_remains:\n    if (df_train[col].quantile(0.01)==df_train[col].quantile(0.25)):\n        sparse_columns_2.append(col)\n\nlen(numerical_columns_remains), len(sparse_columns), len(sparse_columns_2)","7aebbc5b":"from pylab import rcParams\n\ndef box_plot(x,y,data):\n\n  rcParams['figure.figsize'] = 20, 10\n  fig, axs = plt.subplots(2,5)\n  plt.tight_layout()\n  fig.subplots_adjust(top=0.7)\n  sns.set(style=\"ticks\", palette=\"rainbow\")\n\n  j = 0\n  k = 0\n  for i in range(len(y)):\n    sns.boxplot(x=x, y=y[i], data=data,ax=axs[j,k])\n    if(k==4):\n      k = 0\n      j += 1\n    else:\n      k += 1\n\n  plt.tight_layout()\n  plt.show()\n\nbox_plot(Y_train,numerical_columns,X_train)","27ed5899":"\"\"\"\nAlgorithm 'HER(Hard-Edges Method)' applies induction to the elements of a value line which are:\n\n    - lower than the 1th quantile to that quantile and\n    - upper than the 99th quantile to that quantile.\n    \nMain aim is to diminish negative effects of outlier values on analytical operations being performed.\n\"\"\"\n\ndef HardEdgeReduction(df,numerical_columns,sparse_columns,upper_quantile=0.99,lower_quantile=0.01):\n    \n    import pandas as pd\n\n    import psutil, os, gc, time\n    print(\"HardEdgeReduction process has began:\\n\")\n    proc = psutil.Process(os.getpid())\n    gc.collect()\n    mem_0 = proc.memory_info().rss\n    start_time = time.time()\n    \n    # Do outlier cleaning in only one loop\n    epsilon = 0.0001 # for zero divisions\n\n    # Define boundaries that we will use for Reduction operation\n\n    df_outlier_cleaned = df.copy()\n\n\n    print(\"Detected outliers will be replaced with edged quantiles\/percentiles: 1% and 99%!\\n\")\n    print(\"Total number of rows is: %s\\n\"%df_outlier_cleaned.shape[0])\n\n    outlier_boundries_dict={}\n\n    for col in numerical_columns:\n\n        if col in sparse_columns:\n\n            # First ignore the 'sparse' data points:\n            nonsparse_data = pd.DataFrame(df_outlier_cleaned[df_outlier_cleaned[col] !=\\\n                                                             df_outlier_cleaned[col].mode()[0]][col]) \n            \n            #we used only mode to catch sparse points, since we know\/proved it is enough to do that.\n\n            # Find Outlier Thresholds:\n            # Note: All columns are right-skewed\n            # For lower threshold (left-hand-side)\n            if nonsparse_data[col].quantile(lower_quantile) < df_outlier_cleaned[col].mode()[0]: #Unexpected case\n                lower_bound_sparse = nonsparse_data[col].quantile(lower_quantile)\n            else:\n                lower_bound_sparse = df_outlier_cleaned[col].mode()[0]\n            \n            # For upper threshold (right-hand-side)\n            if nonsparse_data[col].quantile(upper_quantile) < df_outlier_cleaned[col].mode()[0]: #Unexpected case\n                upper_bound_sparse = df_outlier_cleaned[col].mode()[0]\n            else:\n                upper_bound_sparse = nonsparse_data[col].quantile(upper_quantile)\n\n            outlier_boundries_dict[col]=(lower_bound_sparse,upper_bound_sparse)\n\n            # Inform user about the cardinality of Outlier existence:\n            number_of_outliers = len(df_outlier_cleaned[(df_outlier_cleaned[col] < lower_bound_sparse) |\\\n                                                        (df_outlier_cleaned[col] > upper_bound_sparse)][col])\n            print(\"Sparse: Outlier number in {} is equal to: \".format(col),round(number_of_outliers\/(nonsparse_data.shape[0] -\n                                                                                       nonsparse_data.isnull().sum()),2))\n\n            # Replace Outliers with Edges --> 1% and 99%:\n            if number_of_outliers > 0:\n\n                # Replace 'left-hand-side' outliers with its 1% quantile value\n                df_outlier_cleaned.loc[df_outlier_cleaned[col] < lower_bound_sparse,col] = lower_bound_sparse - epsilon # --> MAIN DF CHANGED\n\n                # Replace 'right-hand-side' outliers with its 99% quantile value\n                df_outlier_cleaned.loc[df_outlier_cleaned[col] > upper_bound_sparse,col] = upper_bound_sparse + epsilon # --> MAIN DF CHANGED\n\n        else:\n            # Find Edges:\n            number_of_outliers = len(df_outlier_cleaned[(df_outlier_cleaned[col] < \\\n                                                         df_outlier_cleaned[col].quantile(lower_quantile))|\\\n                                                        (df_outlier_cleaned[col] > \\\n                                                         df_outlier_cleaned[col].quantile(upper_quantile))]\\\n                                     [col])\n            print(\"Other: Outlier number in {} is equal to: \".format(col),round(number_of_outliers\/(df[col].shape[0] -\n                                                                                       df[col].isnull().sum()),2)) \n\n            # Replace 'Standard' outliers:\n            if number_of_outliers > 0:\n                # Replace all outliers with its %99 quartile\n                lower_bound_sparse = df_outlier_cleaned[col].quantile(lower_quantile)\n                df_outlier_cleaned.loc[df_outlier_cleaned[col] < \\\n                                       lower_bound_sparse,col] \\\n                = lower_bound_sparse  - epsilon\n\n                upper_bound_sparse = df_outlier_cleaned[col].quantile(upper_quantile)\n                df_outlier_cleaned.loc[df_outlier_cleaned[col] > \\\n                                       upper_bound_sparse,col] \\\n                = upper_bound_sparse  + epsilon\n\n            outlier_boundries_dict[col]=(lower_bound_sparse,upper_bound_sparse)\n\n\n    print('HardEdgeReduction process has been completed!')\n    print(\"--- in %s minutes ---\" % ((time.time() - start_time)\/60))\n\n    return df_outlier_cleaned, outlier_boundries_dict\n\n","e6ff4b7e":"X_train, outlier_boundries_dict = HardEdgeReduction(X_train,numerical_columns,sparse_columns)","904ea82f":"outlier_boundries_dict","d7471ced":"# Do outlier cleaning in only one loop\nepsilon = 0.0001 # for zero divisions\n\n# Define boundaries that we will use for Reduction operation\nupper_quantile = 0.99\nlower_quantile = 0.01\n\ndf_test_outlier_cleaned = X_test.copy()\n\nprint(\"Detected outliers will be replaced with edged quantiles\/percentiles: 1% and 99%!\\n\")\nprint(\"Total number of rows is: %s\\n\"%df_test_outlier_cleaned.shape[0])\n\nfor col in numerical_columns_remains:\n\n      lower_bound = outlier_boundries_dict[col][0]\n      upper_bound = outlier_boundries_dict[col][1]\n        \n      # Inform user about the cardinality of Outlier existence:\n      number_of_outliers = len(df_test_outlier_cleaned[(df_test_outlier_cleaned[col] < lower_bound) |\\\n                                                        (df_test_outlier_cleaned[col] > upper_bound)][col])\n      print(\"Outlier number in {} is equal to: \".format(col), round(number_of_outliers\/\n            (df_test_outlier_cleaned[col].shape[0] - df_test_outlier_cleaned[col].isnull().sum()),2))\n\n      # Replace Outliers with Edges --> 1% and 99%:\n      if number_of_outliers > 0:\n\n          # Replace 'left-hand-side' outliers with its 1% quantile value\n          df_test_outlier_cleaned.loc[df_test_outlier_cleaned[col] < lower_bound,col] = lower_bound  - epsilon # --> MAIN DF CHANGED\n          \n          # Replace 'right-hand-side' outliers with its 99% quantile value\n          df_test_outlier_cleaned.loc[df_test_outlier_cleaned[col] > upper_bound,col] = upper_bound  + epsilon # --> MAIN DF CHANGED\n        \n","b84e920d":"box_plot(Y_train,numerical_columns,X_train)","8bb16fd2":"X_test = df_test_outlier_cleaned","888a3d60":"Zero_MR_variables_list = list(data_info[data_info['%_Missing_Value']==0].index)\nLow_MR_variables_list = list(data_info[(data_info['%_Missing_Value']>0)&\n                                       (data_info['%_Missing_Value']<=0.05)].index)\nModerate_MR_variables_list = list(data_info[(data_info['%_Missing_Value']>0.05)&\\\n                                                      (data_info['%_Missing_Value']<=0.25)].index)\nHigh_MR_variables_list = list(data_info[(data_info['%_Missing_Value']>0.25)&\\\n                                                  (data_info['%_Missing_Value']<=0.50)].index)\nExtreme_MR_variables_list = list(data_info[(data_info['%_Missing_Value']>0.50)&\n                                           (data_info['%_Missing_Value']<=0.95)].index)\nDrop_MR_variables_list = list(data_info[data_info['%_Missing_Value']>0.95].index)\n\nlen(Zero_MR_variables_list),len(Low_MR_variables_list),len(Moderate_MR_variables_list),len(High_MR_variables_list),\\\nlen(Extreme_MR_variables_list),\\\nlen(Zero_MR_variables_list)+len(Low_MR_variables_list)+len(Moderate_MR_variables_list)+len(High_MR_variables_list)+\\\nlen(Extreme_MR_variables_list) == len(data_info)","a96c6f87":"Low_MR_variables_list","1f296b59":"def SimpleImputer(df,data_info,variable_list):\n  for col in variable_list:\n    \n    if(col in numerical_columns):\n      \n      print(\"Total null values: {}\".format(df[[str(col)]].isnull().sum()))\n\n      average = float(df[col].mean())\n      std = float(df[col].std())\n      count_nan = int(df[col].isnull().sum())\n      rand = np.random.normal(loc=average,scale=std,size =count_nan)\n      slice_col = pd.Series(df[col].copy())\n      slice_col[pd.isnull(slice_col)] = rand\n      df[col] = slice_col\n\n      print(\"Numerical variable {} have been imputed.\".format(col))\n\n    else:\n\n      print(\"Total null values: {}\".format(df[[str(col)]].isnull().sum()))\n      df.loc[df.loc[:,col].isnull(),col] = np.random.choice(sorted(list(df.loc[:,col].dropna().unique())),\n                                                            size=int(df.loc[df.loc[:,col].isnull(),col].shape[0]),\n                                                            p=[pd.Series(df.groupby(col).size()\/df.loc[:,col].dropna().shape[0]).iloc[i] for i in \n                                                               np.arange(0,len(df.loc[:,col].dropna().unique()))])\n      \n      print(\"Categorical variable {} have been imputed.\".format(col))","01eb2a72":"SimpleImputer(X_train, data_info, Low_MR_variables_list)","62f36a84":"SimpleImputer(X_test,data_info,Low_MR_variables_list)","af58d757":"MissingUniqueStatistics(X_train.loc[:,Low_MR_variables_list])","39258140":"MissingUniqueStatistics(X_test.loc[:,Low_MR_variables_list])","9c8fbff9":"class KFoldTargetEncoderTrain(base.BaseEstimator,\n                               base.TransformerMixin):\n    def __init__(self,colnames,targetName,\n                  n_fold=5, verbosity=True,\n                  discardOriginal_col=False):\n        self.colnames = colnames\n        self.targetName = targetName\n        self.n_fold = n_fold\n        self.verbosity = verbosity\n        self.discardOriginal_col = discardOriginal_col\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self,X):\n        assert(type(self.targetName) == str)\n        assert(type(self.colnames) == str)\n        assert(self.colnames in X.columns)\n        assert(self.targetName in X.columns)\n        \n        mean_of_target = X[self.targetName].mean()\n        kf = KFold(n_splits = self.n_fold,\n                   shuffle = False, random_state=2020)\n        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n        X[col_mean_name] = np.nan\n        for tr_ind, val_ind in kf.split(X):\n            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n            X.loc[X.index[val_ind], col_mean_name] = \\\n            X_val[self.colnames].map(X_tr.groupby(self.colnames)\n                                     [self.targetName].mean())\n            X[col_mean_name].fillna(mean_of_target, inplace = True)\n        if self.verbosity:\n            encoded_feature = X[col_mean_name].values\n            print('Correlation between the new feature, {} and, {} is {}.'\\\n                  .format(col_mean_name,self.targetName,\n                          np.corrcoef(X[self.targetName].values,\n                                      encoded_feature)[0][1]))\n        if self.discardOriginal_col:\n            X = X.drop(self.targetName, axis=1)\n        return X","ae750658":"def StringConverterTrain(df,target_name,variable_list):\n    for col in variable_list:\n      targetc = KFoldTargetEncoderTrain(col,target_name,n_fold=4)\n      new_train = targetc.fit_transform(df)\n    return new_train","b5ec6033":"nominal_variable = list(data_info[data_info[\"Variable_Type\"]==\"Nominal\"].index)\nnominal_lst = [item for item in Moderate_MR_variables_list+High_MR_variables_list+Extreme_MR_variables_list if item in nominal_variable]\nnominal_lst","6da4326c":"df_trial = pd.concat([X_train,Y_train],axis=1).copy()\ndf_output_train = StringConverterTrain(df=df_trial,target_name=\"CLASS\",variable_list=nominal_lst)","af73e538":"for item in nominal_lst:\n  print(df_output_train.loc[:,[item+\"_Kfold_Target_Enc\"]].isnull().sum())","f2ee2fac":"for item in nominal_lst:\n  X_train[item] = df_output_train[item+\"_Kfold_Target_Enc\"]","a2386b8c":"nominal_encoding_variable_lst =[\"Consequence\",\"REF\",\"ALT\",\"CLNDISDB\",\"CLNDN\",\"Allele\",\"Feature\",\"SYMBOL\"]\n\ndf_encoding = pd.concat([X_train,Y_train],axis=1).copy()\ndf_encoding_train = StringConverterTrain(df=df_encoding,target_name=\"CLASS\",variable_list=nominal_encoding_variable_lst)","6f32b767":"for item in nominal_encoding_variable_lst:\n  print(df_encoding_train.loc[:,[item+\"_Kfold_Target_Enc\"]].isnull().sum())","dae2fa5c":"for item in nominal_encoding_variable_lst:\n  X_train[item] = df_encoding_train[item+\"_Kfold_Target_Enc\"]","f60267ed":"MissingUniqueStatistics(X_train)","49ac0f47":"df_output_test = X_test.copy()\nmean_of_target = df_output_train['CLASS'].copy().mean()\ntarget_mean_list = nominal_lst                                                 \nfor col in target_mean_list:\n    df_output_test[col] = df_output_test[col].map(df_output_train.groupby(col)[col+'_Kfold_Target_Enc'].mean())\n    df_output_test[col].fillna(mean_of_target, inplace = True)","9c322bae":"for item in nominal_lst:\n  print(df_output_test.loc[:,[item]].isnull().sum())","9b64e710":"X_test[nominal_lst] = df_output_test[nominal_lst]","5f60ca4d":"X_test_encoder = X_test.copy()\nmean_of_target = df_encoding_train['CLASS'].copy().mean()\ntarget_mean_list = nominal_encoding_variable_lst                                                 \nfor col in target_mean_list:\n    X_test_encoder[col+'_Kfold_Target_Enc'] = X_test_encoder[col].map(df_encoding_train.groupby(col)[col+'_Kfold_Target_Enc'].mean())\n    X_test_encoder[col+'_Kfold_Target_Enc'].fillna(mean_of_target, inplace = True)","19e96c9f":"for item in nominal_encoding_variable_lst:\n  X_test[item] = X_test_encoder[item+\"_Kfold_Target_Enc\"]\n  \nMissingUniqueStatistics(X_test[nominal_encoding_variable_lst])","af869daa":"X_train.drop(\"CLNHGVS\", axis = 1, inplace = True)\nX_test.drop(\"CLNHGVS\", axis = 1, inplace = True)","b3be85e1":"def MBI(df,columns,train_or_test,lst_numerical):\n\n  data_binary_encoded=df.copy()\n  le=LabelEncoder()\n\n  for col in columns:\n    \n    if(train_or_test == \"test\"):\n\n      le.fit(X_train[col].copy().astype(str))\n      data_binary_encoded[col]=le.transform(df[col].copy().astype(str))\n\n    else:\n\n      data_binary_encoded[col] = le.fit_transform(df[col].copy().astype(str))\n\n  data_scaled=data_binary_encoded.copy()\n\n  for col in numerical_columns:\n\n    scaler = StandardScaler()\n\n    if(train_or_test == \"test\"):\n\n      scaler.fit(np.array(X_train.loc[:,col]).reshape(-1,1))\n      data_scaled.loc[:,col] = scaler.transform(np.array(data_scaled.loc[:,col]).reshape(-1,1))\n\n    else:\n      data_scaled.loc[:,col] = scaler.fit_transform(np.array(data_scaled.loc[:,col]).reshape(-1,1))\n\n  for col in lst_numerical:\n\n    target_dropped_fullcases = data_scaled.drop(col,axis=1).loc[:,list(set(Zero_MR_variables_list+Low_MR_variables_list)-\n                                                                                  set([\"CLASS\",\"KEY\",\"CLNHGVS\"]))].copy()\n    \n    target = data_scaled.loc[:,col]\n    null_mask = target.isna()\n    print(col)\n\n    if(col in numerical_columns):\n      \n      mlp = MLPRegressor(hidden_layer_sizes=(100,10,),\n                        activation='tanh',\n                        solver='adam',\n                        learning_rate='adaptive',\n                        max_iter=1000,\n                        learning_rate_init=0.01,\n                        alpha=0.01,\n                        early_stopping = False)\n    else:\n      mlp = MLPClassifier(hidden_layer_sizes=(100,10,),\n                        activation='tanh',\n                        solver='adam',\n                        learning_rate='adaptive',\n                        max_iter=1000,\n                        learning_rate_init=0.01,\n                        alpha=0.01,\n                        early_stopping = False)\n    \n    mlp.fit(target_dropped_fullcases[~null_mask],target[~null_mask])\n    data_scaled.loc[null_mask,col] = mlp.predict(target_dropped_fullcases[null_mask])\n\n  print(data_scaled.loc[:,lst_numerical].isnull().sum());\n  return data_scaled","bfa8f5f3":"lst_numerical = [item for item in Moderate_MR_variables_list if item in numerical_columns]\nlst_numerical.append(\"SIFT\")\nlst_numerical.append(\"PolyPhen\")\nlst_numerical","e6ed6578":"encoding_col_list =[\"CHROM\",\"CLNVC\",\"Feature_type\",\"BIOTYPE\",\"IMPACT\"]\n","60d34e36":"X_train_scaled = MBI(X_train,encoding_col_list,\"train\",lst_numerical)","ef16b829":"X_train_scaled","fef5ba90":"X_test_scaled = MBI(X_test,encoding_col_list,\"test\",lst_numerical)","4996836e":"def Label_Encoder(df,columns,train_or_test):\n  le = LabelEncoder()\n  for col in columns:\n    if(train_or_test == \"test\"):\n\n      le.fit(X_train_scaled[col].copy().astype(str))\n      df[col] = le.transform(df[col].copy().astype(str))\n\n    else:\n      df[col] = le.fit_transform(df[col].copy().astype(str))\n\n  return df","5635b1af":"X_test_scaled = Label_Encoder(X_test_scaled,[\"SIFT\",\"PolyPhen\"],\"test\")","9d9fbecb":"X_train_scaled = Label_Encoder(X_train_scaled,[\"SIFT\",\"PolyPhen\"],\"train\")","c9c0e388":"rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train_scaled, Y_train)\n\nfeatures = X_train_scaled.columns\nimportances = rnd_clf.feature_importances_\nindices = np.argsort(importances)","b912dd7e":"plt.figure(figsize=(20,10))\nfeat_importances = pd.Series(importances, index=features)\nfeat_importances.nlargest(len(indices)).plot(kind='bar',color = '#79CCB3');","dc473b9e":"# Creating an empty Dataframe with Scores\ndf_accur_roc_score = pd.DataFrame(columns=['Roc_Auc_Score'])","799fca87":"import matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0)\nclf.fit(X_train_scaled, Y_train)\n\ny_preds = clf.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['Logistic_regression'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='Logistic R. AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","2b7053b3":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(n_estimators=150,random_state=0,learning_rate=0.25,eta=0.4,booster=\"gbtree\",base_score=0.8,colsample_bylevel=0.9009229642844634,gamma=0.49967765132613584,\n                        max_depth=6,min_child_weight=7,reg_lambda=0.27611902459972926,subsample=0.9300916052594785)\n\nxgb_model.fit(X_train_scaled, Y_train)\n\ny_preds = xgb_model.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['XGBoost_Classifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='XGBoost Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()\n","62a5b924":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train_scaled,Y_train)\ny_preds = knn.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['KNeighborsClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='KNeighbors Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","ad810912":"from sklearn.tree import DecisionTreeClassifier\n\nreg_dtr = DecisionTreeClassifier(random_state=0)\nreg_dtr.fit(X_train_scaled,Y_train)\n\ny_preds = reg_dtr.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['DecisionTreeClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='DecisionTree Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","f9e09731":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(\n        max_depth=6,\n        n_estimators=100,random_state=0,learning_rate=0.1,eta=0.4,base_score=0.8,colsample_bylevel=0.9009229642844634,gamma=0.49967765132613584,\n                        min_child_weight=9,reg_lambda=0.27611902459972926,subsample=0.9300916052594785,min_samples_split=2,min_samples_leaf=0.1)\n\nlgbm.fit(X_train_scaled, Y_train)\n\ny_preds = lgbm.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['LGBMClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='LGBM Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","0ac5fc1d":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boosting_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n                                                   max_depth=7, random_state=0).fit(X_train_scaled, Y_train)\n\ny_preds = gradient_boosting_clf.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['GradientBoostingClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='GradientBoosting Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","4fed38ae":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\n\nhist_gradient_boosting_clf = HistGradientBoostingClassifier(learning_rate=0.25,\n                                                   max_depth=4, random_state=0).fit(X_train_scaled, Y_train)\n\ny_preds = hist_gradient_boosting_clf.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['HistGradientBoostingClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.plot(fpr, tpr, label='HistGradientBoosting Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\nplt.title('ROC Curve')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()\n","a12deb63":"df_accur_roc_score\n","f81ef57b":"df_accur_roc_score.sort_values(by=['Roc_Auc_Score'],ascending=False).plot(kind='bar', y='Roc_Auc_Score',figsize=(20,8),color='#79ccb3', rot=0,title=\"Model outputs by roc score before feature importance\");","0bfb55a1":"lst_importance_drop = []\n\nfor item in range(0,feat_importances.shape[0]):\n  \n  if(feat_importances[item] < 0.004):\n    lst_importance_drop.append(features[item])\n\nX_train_importance = X_train_scaled.drop(lst_importance_drop,axis=1)\nX_test_importance = X_test_scaled.drop(lst_importance_drop,axis=1)\n\nlst_importance_drop","20756ad2":"# Creating an empty Dataframe with Scores\ndf_accur_roc_score_importance = pd.DataFrame(columns=['Roc_Auc_Score'])","09e9b83b":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=0)\nclf = logreg.fit(X_train_importance, Y_train)\n\ny_preds = clf.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['Logistic_regression'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='Logistic Regression AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","6698f1c8":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(n_estimators=150,random_state=0,learning_rate=0.1,eta=0.4,booster=\"gbtree\",base_score=0.8,colsample_bylevel=0.9009229642844634,gamma=0.49967765132613584,\n                        max_depth=6,min_child_weight=7,reg_lambda=0.27611902459972926,subsample=0.9300916052594785)\n\nxgb_model.fit(X_train_importance, Y_train)\ny_preds = xgb_model.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['XGBoost_Classifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='XGBoost Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","9eef2b94":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train_importance,Y_train)\ny_preds = knn.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['KNeighborsClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='KNeighbors Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","fa431bf2":"from sklearn.tree import DecisionTreeClassifier\n\nreg_dtr = DecisionTreeClassifier(random_state=0)\nreg_dtr.fit(X_train_importance,Y_train)\n\ny_preds = reg_dtr.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['DecisionTreeClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='DecisionTree Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","c3c9553c":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(\n        max_depth=6,\n        n_estimators=100,random_state=0,learning_rate=0.25,eta=0.4,base_score=0.8,colsample_bylevel=0.9009229642844634,gamma=0.49967765132613584,\n                        min_child_weight=7,reg_lambda=0.27611902459972926,subsample=0.9300916052594785,min_sample_split=2)\n\nlgbm.fit(X_train_importance, Y_train)\n\ny_preds = lgbm.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['LGBMClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='LGBM Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","aafe74f2":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boosting_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n                                                   max_depth=5, random_state=0).fit(X_train_importance, Y_train)\n\n\ny_preds = gradient_boosting_clf.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['GradientBoostingClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='GradientBoosting Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","33ba24c6":"from sklearn.ensemble import HistGradientBoostingClassifier\n\nhist_gradient_boosting_clf = HistGradientBoostingClassifier(learning_rate=0.1,\n                                                   max_depth=7, random_state=0).fit(X_train_importance, Y_train)\n\ny_preds = hist_gradient_boosting_clf.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['HistGradientBoostingClassifier'] = [auc_score]\nplt.subplots(figsize=(8, 6))\nplt.plot(fpr, tpr, label='HistGradientBoosting Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\nplt.title('ROC Curve')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","cba78d05":"df_accur_roc_score_importance.sort_values(by=['Roc_Auc_Score'],ascending=False).plot(kind='bar', y='Roc_Auc_Score',figsize=(20,8),color='#79ccb3', rot=0,title=\"Model outputs by roc score before feature importance\");","dd34ebf2":"f,ax = plt.subplots(figsize = (9,10))\nsns.barplot(x=df_accur_roc_score_importance.Roc_Auc_Score,y=df_accur_roc_score_importance.index,color='red',alpha = 0.5,label='After Feature Importance' )\nsns.barplot(x=df_accur_roc_score.Roc_Auc_Score,y=df_accur_roc_score.index,color='blue',alpha = 0.7,label='Before Feature Importance')\n\nax.legend(frameon = True)\nax.set(xlabel='Scores', ylabel='Models',title = \"Auc Score \")\nplt.show()","bd7f1755":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[\n    go.Bar(name='Before Feature Importance', y=df_accur_roc_score.Roc_Auc_Score, x=df_accur_roc_score.index,text=round(df_accur_roc_score.Roc_Auc_Score,3),textposition='auto'),\n    go.Bar(name='After Feature Importance', y=df_accur_roc_score_importance.Roc_Auc_Score, x=df_accur_roc_score_importance.index,text=round(df_accur_roc_score_importance.Roc_Auc_Score,3),textposition='auto',)\n    \n])\nfig.update_layout(barmode='group')\nfig.show()","a392ee0a":"![alt text](https:\/\/cdn.discordapp.com\/attachments\/693138332166914077\/698141755178221629\/target_histogram.png)","f2f2e444":"# **Simple Imputer for Low Missing Values**","20d4ea1f":"#### **Histogram of Binary Target Categories for Test**\n","c200ee51":"Clinic Variant is a public resource containing annotations about human genetic variants. These variants are (usually manually) classified by clinical laboratories on a categorical spectrum ranging from **benign, likely benign, uncertain significance, likely pathogenic, and pathogenic.** Variants that have conflicting classifications (from laboratory to laboratory) can cause confusion when clinicians or researchers try to interpret whether the variant has an impact on the disease of a given patient.","2fadf420":"**The CLASS distribution is skewed a bit to the 0 class, meaning there are fewer variants with conflicting submissions.**\n\n","fd2ef84d":"Categorizes empty cells and converts them into numerical variables by using mean encoding.","aeb4584f":"We have a heapmap showing the relationship of missing values \u200b\u200bwith each other. On this map, we see that the \"EXON\" and \"INTRON\" columns are inversely related to each other. Therefore, we fill in the missing values \u200b\u200bof the column \"EXON\" with the values \u200b\u200bof the column \"INTRON\".","7f26ab24":"Then we drop the \"INTRON\" column.","c05d278c":"### **LightGBM Classifier**","d7378afa":"### When we analyze the missing value statistics, we see that our 9 columns are larger than 99%. These columns do not contain information. That's why we drop these columns.","d862fd43":"**The CLASS distribution is skewed a bit to the 0 class, meaning there are fewer variants with conflicting submissions.**","682bff43":"## **KNeighbors Classifier**","d239f8b5":"## **Cleaning Outliers for Train Dataset**\n\n","e593ff5d":"<img src=\"https:\/\/cdn.discordapp.com\/attachments\/693138332166914077\/698142092865830922\/Missing_Value.png\">","e4ee4b06":"## **Specifying a range for missing values**","59090506":"# Feature Importance","78ef1fd1":"## **XGBoost Classifier**","264ca100":"# Modal Based Imputation","4ca9ed46":"![alt text](https:\/\/cdn.discordapp.com\/attachments\/693138332166914077\/698141746902597662\/conflicting_variant.png)","8a808970":"# Mean Encoding for nominal variables(non missing value)","33afd66b":"### Finding sparse columns","816b2b13":"Filling in the null values \u200b\u200bof columns with a missing value statistic less than 0.05\n\nIt does this according to the frequency of the data.","1161e499":"## MEAN ENCODING","42eab3bb":"### **Hist Gradient Boosting Classifier**","603b1800":"## **Data Controlling**","51d70b6d":"## **Cleaning Outliers for Test Dataset**","c619f09c":"### When we continue to analyze the missing data statistics, we see that there are 3 different columns with numerical data, but the data type is an object.","5bcaa277":"## **Hist Gradient Boosting Classifier**","21a9ea68":"### **Logistic Regression**","912013ba":"## **Decision Tree Classifier**","349603fe":"# **Models Histogram**","f83b52f1":"# **String Converter for Test Dataset**","72c3af68":"### **Decision Tree Classifier**","e486b4fd":"### We see that some data is separated by \"-\". And these numbers are very close to each other. Based on this information, we can only use the first number.","88a24fd7":"# MODELLING","fb18af2d":"## **Logistic Regression**","528c545c":"![alt text](https:\/\/cdn.discordapp.com\/attachments\/693138332166914077\/698275109416206486\/bootcamp.png)","869d40e9":"The objective is to predict whether a Clinic Variant  will have conflicting classifications. This is presented here as a binary classification problem, where each record in the dataset is a genetic variant.\n","abc93faa":"# **Distribution of Target**","d384e350":"![alt text](https:\/\/cdn.discordapp.com\/attachments\/693138332166914077\/698141745719935036\/concordant_variant.png)","b41c2202":"# GENETIC VARIANT CLASSIFICATION","c6d01b64":"#### **Histogram of Binary Target Categories for Train**","312f93d1":"# **Scaling**","114aa2c6":"Since our target variable is binary (categorical), we will use both old and new models. These can be listed as follows:\n\n\n\n*   Logistic Regression\n*   XGBoost Classifier\n*   KNeighbors Classifier\n*   Decision Tree Classifier\n*   LigthGBM Classifier\n*   Gradient Boosting Classifier\n*   Hist Gradient Boosting Classifier\n\n","b39674dc":"When we continue to analyze the table of missing values \u200b\u200bstatistics, we see that we have two columns with fractional numbers but the data type is an object.These are \"EXON\" and \"INTRON\" columns.\n\nWe make the data type of these columns \"float\".\n\n","2ee9ff2c":"# Showing missing values on the bar graphic","84d23981":"![alt text](https:\/\/cdn.discordapp.com\/attachments\/693138332166914077\/698263762855002164\/Ekran_Resmi_2020-04-10_23.08.13.png)","66d29697":"## **Histogram of Binary Target Categories**","5369fcb0":"## **Visualization After Cleaning Outlier**","32e05c70":"## **Outlier Detection**","27736303":"## **Visualization Before Cleaning Outlier**","f08bc150":"### **Gradient Boosting Classifier**","85620474":"## **STRING CONVERSION**","809326d2":"When we analyze this map, we see that the correlation of cDNA_position, CDS_position, Protein_position columns with each other is 1.And more than 85% of the CDS_position values \u200b\u200bare filled with 3 times the Protein_position value.Also, the value of cDNA_position is 50 numbers larger than CDS_position.\n\nTherefore, we dropped the cDNA_position and CDS_position columns.","58271f45":"Our column \"CLNHGVS\" in the table has 65188 values \u200b\u200band all values \u200b\u200bare unique. Therefore, we drop this column as it does not carry any information.","4b621c6d":"### **XGBoost Classifier**","e7ab9839":"# **Imputation for Missing Values**","13791da4":"##  **LightGBM Classifier**","462e0677":"# Showing the correlation map of the dataset","b3cbadca":"After the separation of the train and test data we made, we see that the target distribution does not deteriorate.","1ef2c16f":"## Gradient Boosting Classifier","0f443de1":"### **KNeighbors Classifier**","64acc83f":"### **Visualization for After Feature Importance and Before Feature Importance**","7beee393":"# **Changing Dataset using Feature Importance** ","320788bc":"# **Visualization Model Outputs**","c4293c59":"### **Information function about missing values**","b89f7494":"# **Train\/Test Split**"}}