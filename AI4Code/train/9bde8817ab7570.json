{"cell_type":{"2e3ddd58":"code","f2d8b988":"code","aa67954b":"code","d0a3c8ea":"code","26ccdd6d":"code","cd757b11":"code","f934a083":"code","7d3e8404":"code","e3c0d113":"code","2ebce5d8":"code","e6525a5d":"code","4954b504":"code","fa5d770d":"code","4a29feb0":"code","57a13c24":"code","715deb72":"code","6b913eee":"code","f98b3b37":"code","1ab27c2a":"code","e3eca1ff":"code","a9776d1c":"code","ef24e765":"code","dd2915bf":"code","7ce6cdae":"code","c4c3b51d":"code","2bccad97":"code","30f78647":"code","e8d1c9fc":"code","56239a8d":"markdown","80f4886e":"markdown","c0593fd5":"markdown","49875d57":"markdown","475514c1":"markdown","bfdb1a7d":"markdown","1907406d":"markdown","e343ee8c":"markdown","a2aa4a43":"markdown","ac789671":"markdown","0fb526f9":"markdown","8fb9b5ca":"markdown","2fec7fee":"markdown","4cba83cb":"markdown","b63a9d71":"markdown","6703a2ee":"markdown","805011e3":"markdown","2361297e":"markdown","506f6e43":"markdown","e82271bd":"markdown","edf61077":"markdown","e36f8512":"markdown","d675fd9b":"markdown","b221cbee":"markdown","1a88567b":"markdown","c55d6fa5":"markdown","0a10ac5a":"markdown"},"source":{"2e3ddd58":"!mkdir -p \/tmp\/pip\/cache\/\n!cp ..\/input\/segmentationmodelspytorch\/segmentation_models\/efficientnet_pytorch-0.6.3.xyz \/tmp\/pip\/cache\/efficientnet_pytorch-0.6.3.tar.gz\n!cp ..\/input\/segmentationmodelspytorch\/segmentation_models\/pretrainedmodels-0.7.4.xyz \/tmp\/pip\/cache\/pretrainedmodels-0.7.4.tar.gz\n!cp ..\/input\/segmentationmodelspytorch\/segmentation_models\/segmentation-models-pytorch-0.1.2.xyz \/tmp\/pip\/cache\/segmentation_models_pytorch-0.1.2.tar.gz\n!cp ..\/input\/segmentationmodelspytorch\/segmentation_models\/timm-0.1.20-py3-none-any.whl \/tmp\/pip\/cache\/\n!cp ..\/input\/segmentationmodelspytorch\/segmentation_models\/timm-0.2.1-py3-none-any.whl \/tmp\/pip\/cache\/\n!pip install --no-index --find-links \/tmp\/pip\/cache\/ efficientnet-pytorch\n!pip install --no-index --find-links \/tmp\/pip\/cache\/ segmentation-models-pytorch","f2d8b988":"# Imports\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport rasterio\nimport tempfile\nimport cv2\nimport os\nimport shutil\nimport gc\nfrom tqdm.notebook import tqdm\nimport time\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.resnet import ResNet, Bottleneck\n\nfrom fastai.vision.all import PixelShuffle_ICNR, ConvLayer # TODO: remove\nimport segmentation_models_pytorch as smp\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","aa67954b":"# PARAMETERS\n\n# Printing parameters\nVERBOSE = True\n\n# Data processing\nDATA_DIR = '..\/input\/hubmap-kidney-segmentation\/test' # Input data directory\nREDUCTION = 3 # Reduce the original images by x times\nTILE_SZ = 512 # check 1024 # Size of tiles on which inference is done\n\n# #ver2\n# 256 x 256 on all tiles\nMEAN = np.array([0.63482309,0.47376275,0.67814029])\nSTD = np.array([0.17405236,0.23305763,0.1585981])\n\n\n# Models \n# # This separation worked VERY well on the first competition iteration before the dataset update\n# MODELS_FRESH_FROZEN = [f'..\/input\/models-256-cutmixup-color-nostoch\/model_{i}.pth' for i in range(4)]\n# MODELS_FFPE = [f'..\/input\/models-256-cutmixup-color-nostoch\/model_{i}.pth' for i in range(4)]\n# MODELS_PATHS = [MODELS_FRESH_FROZEN, MODELS_FFPE]\n\nMODELS_FRESH_FROZEN = [f'..\/input\/ret-r101-multi3468-lf\/model_{i}.pth' for i in [0,2]] + \\\n                        [f'..\/input\/ens-red345\/model_effb7_{i}.pth' for i in [1,2,3]] + \\\n                        [f'..\/input\/ens-red345\/model_effb5_{i}.pth' for i in [0,1]]\n\nMODELS_PATHS = [MODELS_FRESH_FROZEN, MODELS_FRESH_FROZEN]\n\n# Tiles selection\n# https:\/\/www.kaggle.com\/iafoss\/256x256-images\nS_TH = 40 # Saturation blancking threshold\nP_TH = 200*TILE_SZ\/\/256 # Threshold for the minimum number of pixels\n\n# Inference\nPUBLIC_ONLY = False # Make predictions only on public LB\nX_OVERLAP = [0., 0.5] # Overlap between tiles during prediction (X axis)\nY_OVERLAP = [0., 0.5] # Overlap between tiles during prediction (Y axis)\nCUSTOM_REDS = [1, 1] # Reduction for two types of models\nTHS = [0.3, 0.3] # Threshold for positive predictions\nN_BINS = 255 # Number of bins when saving mask tiles\nBATCH_SIZE = 8\nNUM_WORKERS = 4\nHALF_PRECISION = False\nTTA_FLIPS = [[-1], [-2], [-2, -1]]\nROT_TTA_FLIPS = [0]\n\n# Final prediction\nMASK_SZ = 4096 # Size of saved mask tiles","d0a3c8ea":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif os.path.exists('tmp'):\n    if VERBOSE:\n        print(\"Removing 'tmp' directory\")\n    shutil.rmtree('tmp')","26ccdd6d":"class FPN(nn.Module):\n    def __init__(self, input_channels:list, output_channels:list):\n        super().__init__()\n        self.convs = nn.ModuleList(\n            [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=1),\n             nn.ReLU(inplace=True), nn.BatchNorm2d(out_ch*2),\n             nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n            for in_ch, out_ch in zip(input_channels, output_channels)])\n        \n    def forward(self, xs:list, last_layer):\n        hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear') \n               for i,(c,x) in enumerate(zip(self.convs, xs))]\n        hcs.append(last_layer)\n        return torch.cat(hcs, dim=1)\n\nclass UnetBlock(nn.Module):\n    def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n                 self_attention:bool=False, **kwargs):\n        super().__init__()\n        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c\/\/2, blur=blur, **kwargs)\n        self.bn = nn.BatchNorm2d(x_in_c)\n        ni = up_in_c\/\/2 + x_in_c\n        nf = nf if nf is not None else max(up_in_c\/\/2,32)\n        self.conv1 = ConvLayer(ni, nf, norm_type=None, **kwargs)\n        self.conv2 = ConvLayer(nf, nf, norm_type=None,\n            xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, up_in:torch.Tensor, left_in:torch.Tensor) -> torch.Tensor:\n        s = left_in\n        up_out = self.shuf(up_in)\n        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n        return self.conv2(self.conv1(cat_x))\n        \nclass _ASPPModule(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size, padding, dilation, groups=1):\n        super().__init__()\n        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                stride=1, padding=padding, dilation=dilation, bias=False, groups=groups)\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_conv(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\nclass ASPP(nn.Module):\n    def __init__(self, inplanes=512, mid_c=256, dilations=[6, 12, 18, 24], out_c=None):\n        super().__init__()\n        self.aspps = [_ASPPModule(inplanes, mid_c, 1, padding=0, dilation=1)] + \\\n            [_ASPPModule(inplanes, mid_c, 3, padding=d, dilation=d,groups=4) for d in dilations]\n        self.aspps = nn.ModuleList(self.aspps)\n        self.global_pool = nn.Sequential(nn.AdaptiveMaxPool2d((1, 1)),\n                        nn.Conv2d(inplanes, mid_c, 1, stride=1, bias=False),\n                        nn.BatchNorm2d(mid_c), nn.ReLU())\n        out_c = out_c if out_c is not None else mid_c\n        self.out_conv = nn.Sequential(nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False),\n                                    nn.BatchNorm2d(out_c), nn.ReLU(inplace=True))\n        self.conv1 = nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False)\n        self._init_weight()\n\n    def forward(self, x):\n        x0 = self.global_pool(x)\n        xs = [aspp(x) for aspp in self.aspps]\n        x0 = F.interpolate(x0, size=xs[0].size()[2:], mode='bilinear', align_corners=True)\n        x = torch.cat([x0] + xs, dim=1)\n        return self.out_conv(x)\n    \n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()","cd757b11":"class UneXt50(nn.Module):\n    def __init__(self, stride=1, **kwargs):\n        super().__init__()\n        #encoder\n        m = ResNet(Bottleneck, [3, 4, 6, 3], groups=32, width_per_group=4)\n        #m = torch.hub.load('facebookresearch\/semi-supervised-ImageNet1K-models',\n        #                   'resnext50_32x4d_ssl')\n        self.enc0 = nn.Sequential(m.conv1, m.bn1, nn.ReLU(inplace=True))\n        self.enc1 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1),\n                            m.layer1) #256\n        self.enc2 = m.layer2 #512\n        self.enc3 = m.layer3 #1024\n        self.enc4 = m.layer4 #2048\n        #aspp with customized dilatations\n        self.aspp = ASPP(2048,256,out_c=512,dilations=[stride*1,stride*2,stride*3,stride*4])\n        self.drop_aspp = nn.Dropout2d(0.5)\n        #decoder\n        self.dec4 = UnetBlock(512,1024,256)\n        self.dec3 = UnetBlock(256,512,128)\n        self.dec2 = UnetBlock(128,256,64)\n        self.dec1 = UnetBlock(64,64,32)\n        self.fpn = FPN([512,256,128,64],[16]*4)\n        self.drop = nn.Dropout2d(0.1)\n        self.final_conv = ConvLayer(32+16*4, 1, ks=1, norm_type=None, act_cls=None)\n        \n    def forward(self, x):\n        enc0 = self.enc0(x)\n        enc1 = self.enc1(enc0)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n        enc5 = self.aspp(enc4)\n        dec3 = self.dec4(self.drop_aspp(enc5),enc3)\n        dec2 = self.dec3(dec3,enc2)\n        dec1 = self.dec2(dec2,enc1)\n        dec0 = self.dec1(dec1,enc0)\n        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n        x = self.final_conv(self.drop(x))\n        x = F.interpolate(x,scale_factor=2,mode='bilinear')\n        return x\n    \n    \n    \nclass UneXt101(nn.Module):\n    def __init__(self, stride=1, **kwargs):\n        super().__init__()\n        #encoder\n        m = ResNet(Bottleneck, [3, 4, 23, 3], groups=32, width_per_group=16)\n\n        self.enc0 = nn.Sequential(m.conv1, m.bn1, nn.ReLU(inplace=True))\n        self.enc1 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1),\n                            m.layer1) #256\n        self.enc2 = m.layer2 #512\n        self.enc3 = m.layer3 #1024\n        self.enc4 = m.layer4 #2048\n        #aspp with customized dilatations\n        self.aspp = ASPP(2048,256,out_c=512,dilations=[stride*1,stride*2,stride*3,stride*4])\n        self.drop_aspp = nn.Dropout2d(0.5)\n        #decoder\n        self.dec4 = UnetBlock(512,1024,256)\n        self.dec3 = UnetBlock(256,512,128)\n        self.dec2 = UnetBlock(128,256,64)\n        self.dec1 = UnetBlock(64,64,32)\n        self.fpn = FPN([512,256,128,64],[16]*4)\n        self.drop = nn.Dropout2d(0.1)\n        self.final_conv = ConvLayer(32+16*4, 1, ks=1, norm_type=None, act_cls=None)\n        \n    def forward(self, x):\n        enc0 = self.enc0(x)\n        enc1 = self.enc1(enc0)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n        enc5 = self.aspp(enc4)\n        dec3 = self.dec4(self.drop_aspp(enc5),enc3)\n        dec2 = self.dec3(dec3,enc2)\n        dec1 = self.dec2(dec2,enc1)\n        dec0 = self.dec1(dec1,enc0)\n        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n        x = self.final_conv(self.drop(x))\n        x = F.interpolate(x,scale_factor=2,mode='bilinear')\n        return x\n    \nclass Unet50(nn.Module):\n    def __init__(self, stride=1, **kwargs):\n        super().__init__()\n        #encoder\n#         m = torch.hub.load('facebookresearch\/semi-supervised-ImageNet1K-models',\n#                            'resnet50_swsl')\n        m = ResNet(Bottleneck, [3, 4, 6, 3])\n        self.enc0 = nn.Sequential(m.conv1, m.bn1, nn.ReLU(inplace=True))\n        self.enc1 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1),\n                            m.layer1) #256\n        self.enc2 = m.layer2 #512\n        self.enc3 = m.layer3 #1024\n        self.enc4 = m.layer4 #2048\n        #aspp with customized dilatations\n        self.aspp = ASPP(2048,256,out_c=512,dilations=[stride*1,stride*2,stride*3,stride*4])\n        self.drop_aspp = nn.Dropout2d(0.5)\n        #decoder\n        self.dec4 = UnetBlock(512,1024,256)\n        self.dec3 = UnetBlock(256,512,128)\n        self.dec2 = UnetBlock(128,256,64)\n        self.dec1 = UnetBlock(64,64,32)\n        self.fpn = FPN([512,256,128,64],[16]*4)\n        self.drop = nn.Dropout2d(0.1)\n        self.final_conv = ConvLayer(32+16*4, 1, ks=1, norm_type=None, act_cls=None)\n        \n    def forward(self, x):\n        enc0 = self.enc0(x)\n        enc1 = self.enc1(enc0)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n        enc5 = self.aspp(enc4)\n        dec3 = self.dec4(self.drop_aspp(enc5),enc3)\n        dec2 = self.dec3(dec3,enc2)\n        dec1 = self.dec2(dec2,enc1)\n        dec0 = self.dec1(dec1,enc0)\n        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n        x = self.final_conv(self.drop(x))\n        x = F.interpolate(x,scale_factor=2,mode='bilinear')\n        return x","f934a083":"# Import models\n\nMODELS = []\nfor models_list in MODELS_PATHS:\n    models_i = []\n    for ij,path in enumerate(models_list):\n        state_dict = torch.load(path,map_location=torch.device('cpu'))\n        if ij < 2:\n            model = UneXt101()\n        elif ij < 5:\n            model = smp.Unet(encoder_name='efficientnet-b7', classes=1, activation=None, encoder_weights=None)\n        else:\n            model = smp.Unet(encoder_name='efficientnet-b5', classes=1, activation=None, encoder_weights=None)\n        model.load_state_dict(state_dict)\n        model.float()\n        model.eval()\n        model.to(device)\n        models_i.append(model)\n    del state_dict\n    MODELS.append(models_i)","7d3e8404":"def read_tiff(filename):\n    img = rasterio.open(filename)\n    W, H = img.shape\n    tmp = np.memmap(tempfile.TemporaryFile(), shape=(W, H, 3),\n                          dtype=np.uint8)\n    if len(img.subdatasets) == 3:\n        for i in range(3):\n            tmp[:,:,i] = rasterio.open(img.subdatasets[i]).read(1)\n    else:\n        for i in range(3):\n            tmp[:,:,i] = img.read(i+1)\n    return tmp\n\ndef load_image(filename):\n    img_id = filename.split(\"\/\")[-1].split(\".\")[0]\n    img = read_tiff(filename)\n    if VERBOSE:\n        print(\"Initial size of %s:\" %(img_id,), img.shape)\n    return img","e3c0d113":"def _tile_resize_save(img, img_id, tile_sz, reduce=1):\n    \"\"\"\n    Divide WSI into small tiles, resize them and save them locally.\n    \"\"\"\n    x = 0\n    while x < img.shape[0]:\n        y = 0\n        while y < img.shape[1]:\n            # Get tile\n            img_tile = img[x:x+tile_sz,y:y+tile_sz]\n\n            # Reduce if needed\n            if reduce > 1:\n                new_dim = (img_tile.shape[1]\/\/reduce,img_tile.shape[0]\/\/reduce)\n                img_tile = cv2.resize(img_tile, new_dim, interpolation = cv2.INTER_AREA)\n\n            # Save tile\n            save_path = \"%s_%d_%d.png\" %(img_id, x\/\/reduce, y\/\/reduce)\n            Image.fromarray(img_tile).save(save_path)\n            y += tile_sz\n        x += tile_sz\n\n    # Return dimension after reduction\n    final_x = ((x-tile_sz)\/\/tile_sz)*(tile_sz\/\/reduce) + img_tile.shape[0]\n    final_y = ((y-tile_sz)\/\/tile_sz)*(tile_sz\/\/reduce) + img_tile.shape[1]\n    return (final_x, final_y, 3)","2ebce5d8":"def _reconstruct_img(img_id, tile_sz, shape):\n    \"\"\"\n    Reconstruct image from reduced tiles.\n    \"\"\"\n\n    img = np.zeros(shape, dtype=np.uint8)\n    if VERBOSE:\n        print(\"Reconstructed image:\", shape)\n    x = 0\n    while x < shape[0]:\n        y = 0\n        while y < shape[1]:\n            tile_path = \"%s_%d_%d.png\" %(img_id, x, y)\n            img_tile = np.asarray(Image.open(tile_path))\n            img[x:x+tile_sz,y:y+tile_sz] = img_tile\n            os.remove(tile_path) # Tiles are deleted when read\n            y += tile_sz\n        x += tile_sz\n    return img","e6525a5d":"def load_resize(idx, reduce):\n    \"\"\"\n    Memory efficient WSI loading and resampling.\n    Return resampled image and initial shape.\n    \"\"\"\n    img = load_image(os.path.join(DATA_DIR,idx+'.tiff'))\n    init_shape = img.shape\n    shape = _tile_resize_save(img, idx, (MASK_SZ*REDUCTION), reduce=REDUCTION)\n    img = _reconstruct_img(idx, (MASK_SZ*REDUCTION)\/\/REDUCTION, shape)\n    return img, init_shape","4954b504":"def _get_group(filename):\n    return 0 ###\n    with tiff.TiffFile(filename) as f:\n        description = f.pages[0].description\n    if int(\"PhysicalSizeY=\\\"0.65\\\"\" in description):\n        return 1\n    elif int(\"PhysicalSizeY=\\\"0.5\\\"\" in description):\n        return 0\n    else:\n        return \"ERROR\"","fa5d770d":"def _get_nored_pads(initW, initH, upW, upH, xa, xb, ya, yb):\n    \"\"\"\n    Get padding to remove in final mask.\n    \"\"\"\n    px = xa\/(xa+xb)\n    py = ya\/(ya+yb)\n    padx = upW - initW\n    pady = upH - initH\n    assert padx > 0\n    assert pady > 0\n    xa = int(px*padx)\n    xb = padx - xa\n    ya = int(py*pady)\n    yb = pady - ya\n    return xa, xb, ya, yb\n\ndef _add_padding(img, init_sz, img_shape, p0, p1):\n    \"\"\"\n    Add padding to make the image dividable into tiles.\n    \"\"\"\n    start = time.time()\n    if VERBOSE:\n        print(\"  > Adding padding to make the image dividable into tiles...\")\n    if VERBOSE:\n        print(\"  > Before reduction:\", img_shape)\n        print(\"  > After reduction:\", img.shape)\n\n    # X overlap padding\n    pad0_ = TILE_SZ - img.shape[0]%TILE_SZ\n    x_pad = int(TILE_SZ*p0)\n    xa = (pad0_\/\/2 + x_pad)\n    xb = pad0_+TILE_SZ-(pad0_\/\/2 + x_pad)\n    pad0_lr = [xa, xb]\n\n    # Y overlap padding\n    pad1_ = TILE_SZ - img.shape[1]%TILE_SZ\n    y_pad = int(TILE_SZ*p1)\n    ya = (pad1_\/\/2 + y_pad)\n    yb = pad1_+TILE_SZ-(pad1_\/\/2 + y_pad)\n    pad1_lr = [ya, yb]\n\n    img = np.pad(img,[pad0_lr, pad1_lr,[0,0]],constant_values=0)\n    if VERBOSE:\n        print(\"  > After padding:\", img.shape, \"Time =\", time.time() - start, \"s\")\n    xa, xb, ya, yb = _get_nored_pads(img_shape[0], img_shape[1],\n                                 REDUCTION*img.shape[0], REDUCTION*img.shape[1],\n                                 xa, xb, ya, yb)\n    return img, xa, xb, ya, yb, img.shape","4a29feb0":"def _split_image(img):\n    \"\"\"\n    Split image into tiles using the reshape+transpose trick.\n    Final shape = [nb_x*nb_y, TILE_SZ, TILE_SZ, 3].\n    \"\"\"\n    start = time.time()\n    if VERBOSE:\n        print(\"  > Splitting image into tiles...\")\n    assert not img.shape[0]%TILE_SZ # Check that width is OK\n    assert not img.shape[1]%TILE_SZ # Check that height is OK\n    img = img.reshape(img.shape[0]\/\/TILE_SZ,\n                      TILE_SZ,\n                      img.shape[1]\/\/TILE_SZ,\n                      TILE_SZ,\n                      3)\n    img = img.transpose(0,2,1,3,4).reshape(-1,TILE_SZ,TILE_SZ,3)\n    if VERBOSE:\n        print(\"  > Splitting done! Time =\", time.time() - start)\n    return img","57a13c24":"def _select_tiles(img):\n    \"\"\"\n    Select tiles for running the model.\n    \"\"\"\n    start = time.time()\n    if VERBOSE:\n        print(\"  > Selecting tiles...\")\n    if not os.path.exists('tmp'):\n        # Generate tmp directory if needed\n        os.makedirs('tmp')\n    idxs = []\n    for i, im in enumerate(img):\n        # Remove black or gray images based on saturation check\n        hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n        h, s, v = cv2.split(hsv)\n        if (s>S_TH).sum() <= P_TH or im.sum() <= P_TH: continue \n        cv2.imwrite(\"tmp\/%d.png\" %(i,), im)\n        idxs.append(i)\n    if VERBOSE:\n        print(\"  > Tiles selected! Time =\", time.time() - start)\n    return idxs","715deb72":"def img2tensor(img, dtype:np.dtype=np.float32):\n    if img.ndim==2: img = np.expand_dims(img,2)\n    img = np.transpose(img,(2,0,1))\n    return torch.from_numpy(img.astype(dtype, copy=False))\n\nclass HuBMAPTestDataset(Dataset):\n    def __init__(self, idxs):\n        self.fnames = idxs\n        \n    def __len__(self):\n        return len(self.fnames)\n    \n    def __getitem__(self, idx):\n        im = cv2.imread(\"tmp\/%d.png\" %(self.fnames[idx],))\n        return img2tensor((im\/255.0 - MEAN)\/STD)","6b913eee":"def _make_tiles_dataloader(idxs):\n    \"\"\"\n    Make tiles dataset.\n    \"\"\"\n    start = time.time()\n    ds = HuBMAPTestDataset(idxs)\n    dl = DataLoader(ds, BATCH_SIZE,\n                    num_workers=NUM_WORKERS,\n                    shuffle=False,\n                    pin_memory=True)\n    if VERBOSE:\n        print(\"  > Tiles dataset created! Time =\", time.time() - start)\n    return dl","f98b3b37":"def _generate_masks(dl, idxs, n_tiles, init_sz, group):\n    \"\"\"\n    Generate masks.\n    \"\"\"\n    start = time.time()\n    if VERBOSE:\n        print(\"  > Generating masks...\")\n    red = CUSTOM_REDS[group]\n    mp = Model_pred(MODELS[group], dl, red)\n    mask = torch.zeros(n_tiles,\n                       init_sz,\n                       init_sz,\n                       dtype=torch.uint8)\n    for i, p in zip(idxs,iter(mp)): mask[i] = p.squeeze(-1)\n    if VERBOSE:\n        print(\"  > Masks generated! Time =\", time.time() - start)\n    return mask","1ab27c2a":"# Iterator-like wrapper that returns predicted masks\nclass Model_pred:\n    def __init__(self, models, dl, red, half:bool=False):\n        self.models = models # List of models\n        self.dl = dl # Dataloader\n        self.half = half # Half precision\n        self.red = red # Reduction on reduced image\n        \n    def __iter__(self):\n        with torch.no_grad():\n            for x in iter(self.dl):\n                # Prepare input\n                x = x.to(device)\n                x = F.interpolate(x, scale_factor=1\/self.red, mode='bilinear')\n                if self.half: x = x.half()\n\n                # Make predictions\n                py = 0.\n                for rot_flip in ROT_TTA_FLIPS: #[0,1]\n                    for model in self.models:\n                        xr = torch.rot90(x, rot_flip, [-2, -1])\n                        p = model(xr)\n                        p = torch.rot90(p, -rot_flip, [-2, -1])\n                        p = torch.sigmoid(p).detach()\n                        py += p\n                    for f in TTA_FLIPS:\n                        xf = torch.rot90(x, rot_flip, [-2, -1])\n                        xf = torch.flip(xf,f)\n                        for model in self.models:\n                            p = model(xf)\n                            p = torch.flip(p,f)\n                            p = torch.rot90(p, -rot_flip, [-2, -1])\n                            py += torch.sigmoid(p).detach()\n                        \n                py \/= (1+len(TTA_FLIPS))*len(ROT_TTA_FLIPS)       \n                py \/= len(self.models)\n\n                # Upsample to initial shape\n                py = F.upsample(py, scale_factor=REDUCTION*self.red, mode=\"bilinear\")\n                py = py.permute(0,2,3,1).float().cpu()\n\n                # Quantize probablities to save memory\n                py = (N_BINS*py).int()\n\n                # Output predictions\n                batch_size = len(py)\n                for i in range(batch_size):\n                    yield py[i]\n                    \n    def __len__(self):\n        return len(self.dl.dataset)","e3eca1ff":"def _reshape_depad_mask(mask, init_shape, init_sz, p0, p1, xa, xb, ya, yb):\n    \"\"\"\n    Reshape tiled masks into a single mask and crop padding.\n    \"\"\"\n    start = time.time()\n    if VERBOSE:\n        print(\"  > Merge tiled masks into one mask and crop padding...\")\n    mask = mask.view(init_shape[0]\/\/TILE_SZ,\n                     init_shape[1]\/\/TILE_SZ,\n                     init_sz,\n                     init_sz).\\\n                permute(0,2,1,3).reshape(init_shape[0]*REDUCTION,\n                                         init_shape[1]*REDUCTION)\n    mask = mask[xa:-xb,ya:-yb]\n    if VERBOSE:\n        print(\"  > Mask created! Shape =\", mask.shape,\"Time =\", time.time() - start)\n    return mask","a9776d1c":"def _save_mask_tiles(mask, idx, p0, p1):\n    start = time.time()\n    if VERBOSE:\n        print(\"  > Saving tiles in HDD memory...\")\n    x = 0\n    while x < mask.shape[0]:\n        y = 0\n        while y < mask.shape[1]:\n            mask_tile = mask[x:x+MASK_SZ,y:y+MASK_SZ].numpy()\n            save_path = \"%s_%d_%d_%s_%s.png\" %(idx, x, y, str(p0), str(p1))\n            Image.fromarray(mask_tile).save(save_path)\n            y += MASK_SZ\n        x += MASK_SZ\n    if VERBOSE:\n        print(\"Tiles saved! Time =\", time.time() - start)","ef24e765":"def make_one_prediction(img, group, idx, img_shape, p0, p1):\n    \"\"\"\n    Predict a mask for one given image.\n    \"\"\"\n\n    init_sz = TILE_SZ*REDUCTION\n\n    # Add padding to make the image dividable into tiles\n    img, xa, xb, ya, yb, img_shape_p = _add_padding(img, init_sz, img_shape,\n                                                    p0, p1)\n\n    # Split image into tiles using the reshape+transpose trick\n    # Final shape = [nb_x*nb_y, TILE_SZ, TILE_SZ, 3]\n    img = _split_image(img)\n    n_tiles = img.shape[0]\n\n    # Select tiles for running the model\n    idxs = _select_tiles(img)\n\n    # Make tiles dataset\n    dl = _make_tiles_dataloader(idxs)\n\n    # Generate masks\n    mask = _generate_masks(dl, idxs, n_tiles, init_sz, group)\n\n    # Reshape tiled masks into a single mask and crop padding\n    mask = _reshape_depad_mask(mask, img_shape_p, init_sz,\n                               p0, p1, xa, xb, ya, yb)\n\n    # A little bit of cleaning...\n    gc.collect()\n    shutil.rmtree('tmp')\n\n    # Save tiles in HDD memory\n    _save_mask_tiles(mask, idx, p0, p1)","dd2915bf":"def get_mask_tiles(idx, p0_list, p1_list):\n    \"\"\"\n    Load a WSI and generate mask tiles.\n    Return initial shape of WSI and binarization threshold.\n    \"\"\"\n    group = _get_group(os.path.join(DATA_DIR,idx+'.tiff'))\n    TH = THS[group]\n    img, init_shape = load_resize(idx, REDUCTION)\n    for p0 in p0_list:\n        for p1 in p1_list:\n            make_one_prediction(img, group, idx, init_shape, p0, p1)\n    return init_shape, TH","7ce6cdae":"def make_predictions(idx):\n    \"\"\"\n    Generate RLE prediction for idx.\n    \"\"\"\n\n    # First generate mask tiles\n    init_shape, TH = get_mask_tiles(idx, X_OVERLAP, Y_OVERLAP)\n\n    # Then reconstruct mask from tiles\n    mask = torch.zeros(*init_shape[:2], dtype=torch.uint8)\n    x = 0\n    while x < init_shape[0]:\n        y = 0\n        while y < init_shape[1]:\n            mask_tile = 0.\n            for p0 in X_OVERLAP:\n                for p1 in Y_OVERLAP:\n                    tile_path = \"%s_%d_%d_%s_%s.png\" %(idx, x, y, str(p0), str(p1))\n                    mask_tile += torch.tensor(np.asarray(Image.open(tile_path), dtype=int))\n                    os.remove(tile_path)\n            NEW_TH = int(N_BINS*len(X_OVERLAP)*len(Y_OVERLAP)*TH)\n            mask[x:x+MASK_SZ,y:y+MASK_SZ] = mask_tile>NEW_TH\n            y += MASK_SZ\n        x += MASK_SZ\n\n    # Eventually convert to rle\n    # https:\/\/www.kaggle.com\/bguberfain\/memory-aware-rle-encoding\n    if VERBOSE:\n        print(\"  > Converting to RLE...\")\n    rle = rle_encode_less_memory(mask.numpy())\n    del mask\n    return rle","c4c3b51d":"# https:\/\/www.kaggle.com\/bguberfain\/memory-aware-rle-encoding\ndef rle_encode_less_memory(img):\n    # watch out for the bug\n    pixels = img.T.flatten()\n    # This simplified method requires first and last pixel to be zero\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","2bccad97":"df_sample = pd.read_csv('..\/input\/hubmap-kidney-segmentation\/sample_submission.csv')\nnames,preds = [],[]\nif PUBLIC_ONLY:\n    samples = ['d488c759a', 'aa05346ff','57512b7f1','3589adb90','2ec3f1bb9']\n    samples_n = [id for id in df_sample.id if id not in samples]\n\n    for x in samples_n:\n        names += [x]\n    preds += [np.NaN]*len(samples_n)\n    df_sample = df_sample.loc[df_sample.id.isin(samples)]\n","30f78647":"for idx,row in tqdm(df_sample.iterrows(),total=len(df_sample)):\n    idx = row['id']\n    print(\"Computing predictions for image\", idx)\n    rle = make_predictions(idx)\n    names.append(idx)\n    preds.append(rle)","e8d1c9fc":"df = pd.DataFrame({'id': names, 'predicted': preds})\ndf.to_csv('submission.csv',index=False)","56239a8d":"### Saving the Segmentation Mask\n\nThe segmentation mask is then saved as tiles containing scores (it is not binarized yet).","80f4886e":"# Insights\n\nThanks to our predictions, we could compute some metrics about glomeruli. In particular we were interested in the median area per patient, the median perimeter per patient and the median ratio area\/(perimeter\u00b2) per patient. Here are the values we obtained based on the GT for images in the training set and based on our predictions for images in the test set.\n\n| id        | median area (\u00b5m\u00b2) | median perimeter (\u00b5m) | median area\/perimeter\u00b2 |\n|-----------|-------------------|-----------------------|------------------------|\n| 0486052bb | 5610              | 309                   | 0.059                  |\n| 2f6ecfcdf | 34158             | 708                   | 0.068                  |\n| 8242609fa | 32320             | 683                   | 0.069                  |\n| aaa6a05cc | 13466             | 436                   | 0.071                  |\n| b2dc8411c | 24430             | 594                   | 0.069                  |\n| b9a3865fc | 15859             | 483                   | 0.068                  |\n| cb2d976f4 | 13378             | 435                   | 0.071                  |\n| 095bf7a1f | 29992             | 651                   | 0.071                  |\n| 1e2425f28 | 8768              | 356                   | 0.069                  |\n| 26dc41664 | 28316             | 649                   | 0.067                  |\n| 4ef6695ce | 22636             | 574                   | 0.069                  |\n| 54f2eec69 | 32392             | 684                   | 0.069                  |\n| afa5e8098 | 21632             | 577                   | 0.065                  |\n| c68fe75ea | 34348             | 720                   | 0.067                  |\n| e79de561c | 33504             | 704                   | 0.068                  |\n| 2ec3f1bb9 | 33462             | 692                   | 0.070                  |\n| 3589adb90 | 27831             | 624                   | 0.072                  |\n| 57512b7f1 | 11356             | 400                   | 0.071                  |\n| aa05346ff | 48216             | 1064                  | 0.046                  |\n| d488c759a | 9560              | 388                   | 0.063                  |\n\nWe found that the median area of glomeruli was negatively correlated to the weight (correlation coefficient of -0.42), the height (-0.25) and the BMI (-0.28). The median perimeter was similarly correlated (respectively -0.48, -0.29 and -0.33). The median area\/(perimeter\u00b2), describing how complex the shape of the glomeruli were, has been found to be correlated to the weight (0.21), the height (0.20) and the BMI (0.14).\n\nHowever these findings must be confirmed on more patients, as they are based on a very small sample of people that is not necessarily representative of the general population.","c0593fd5":"### Generating the Segmentation Mask\n\nThe segmentation mask is then generated. It is not yet binarized. The `_generate_masks` function outputs a zero-padded mask. At that moment, the mask is already upsampled to match the original image.","49875d57":"Dealing with such big images is not convenient, we found that we should downsample data for better performance. After loading the image using the `load_image` function above, we apply the `_tile_resize_save` function below.","475514c1":"# Training Tricks\n\n## Disclamer\n\nWe used NO hand-labelling and manual annotation since in our vision it is not consistent with the initial goal and objective of the competition.\n\n## Color spaces augmentation trick:\n\nFor the training we found and decided to use various Color Space Augmentations in combination with custom stochastic kernel - this is one of the most important things that allowed our models to stay robust and segment glomeruli despite the data source, type of the images (FFPE\/fresh-frozen\/even others) and \"color-related\" variations. \n\nOn the images below one could see how the naturally looked tiles are transformed being augmented with this method. You can see normal tiles with the cutmix augs, color spaced without cutmix, and color spaced with cutmix.\n\n\n**Tiles with CutMix augmentation:**\n\n![image.png](attachment:9408f479-08ca-46fa-8d25-2f090fdbeadf.png)\n\n**Tiles with Color Space augmentation:**\n\n![image.png](attachment:0e0ed513-8f0e-459a-8f93-61152ceeafac.png)\n\n**Tiles with both augmentations:**\n\n![image.png](attachment:a124c589-2fe5-46c4-93e2-d2f5257806d8.png)\n\nThis methodology demonstrated its effectiveness and power during both stages of the competition, but mostly with the first set of the data (maybe due to the different test sets, since we dont know yet results on the private test data).\n\nWe also share the code as we implemented this idea.\n\n\n```\nimport cv2\n\ncspaces = [cv2.COLOR_BGR2HLS,\n\n            cv2.COLOR_BGR2HSV,\n\n            cv2.COLOR_BGR2LAB,\n\n            cv2.COLOR_BGR2LUV,\n\n            cv2.COLOR_BGR2Lab,\n\n            cv2.COLOR_BGR2Luv,\n\n            cv2.COLOR_BGR2RGB,\n\n            cv2.COLOR_BGR2XYZ,\n\n            cv2.COLOR_BGR2YUV,\n\n            cv2.COLOR_RGB2HLS,\n\n            cv2.COLOR_RGB2HSV,\n\n            cv2.COLOR_RGB2LAB,\n\n            cv2.COLOR_RGB2LUV,\n\n            cv2.COLOR_RGB2Lab,\n\n            cv2.COLOR_RGB2Luv,\n\n            cv2.COLOR_RGB2BGR,\n\n            cv2.COLOR_RGB2XYZ,\n\n            cv2.COLOR_RGB2YUV]\n```\n\n\nIn the batch sampling part:\n \n\n```\nif self.train and random.random() > 1\/len(cspaces):\n\n    cspace = random.choice(cspaces)\n\n    img = cv2.cvtColor(img, cspace)\n\nif self.train:\n\n    stoch = np.random.rand(3,3)\n\n    K = stoch.sum(0, keepdims=True)\n\n    if random.random() > 0.7:\n\n        K=np.exp(K)\/K\n\n    elif random.random() > 0.25:\n\n        K=np.exp(K)\/(np.sqrt(K))\n\n    stoch = stoch\/K\n\n    img = np.einsum(\"ijk,kl->ijl\", img, stoch)\n```\n\nThis method also allowed to obtain much more focused predictions - we observed very high \u201csoft dice\u201d metrics even without proper binarization threshold. For example improvement is from 0.6-0.65 in the standard pipeline up to 0.87-0.89 Dice, depending on the fold.\n\nAdditionally we observed interesting behaviour such as robust segmentation even of some suspicious glomeruli in the d48*** sample from public test set.\n\n## Multi-scale input tiles\n\nWe used multiple image reduction rates (3, 4, 5, 6 and 8) in order to create the tiles from original images. Additionally we used only \u201cnon-empty\u201d tiles of the bigger reduction rates to balance more the training dataset. Such approach naturally increases the training dataset and enriches variation of glomeruli size and its proportion in the considered tile of fixed size.\n\nThis also gave us significant boost both in CV and LB.\n\n## Pseudo-labelling\n\nPublic test set pseudo-labeled with own models and included in the training procedure\n\n## Short training pipeline\n\nUnet with resnext50_32x4d\/resnext101_32x16d\/efficientnet-b7\/b5 backbones, ASPP and FPN modules + complex loss (combination of FocalLoss + DiceLoss + SymmetricLovasz) + OneCycleLR + Heavy Augmentations & CutMix & ColorSpaced augs + 5-fold patient-based CV.","bfdb1a7d":"# Detailed Description of our Inference Approach\n\nIn this section, we describe our approach (version 1). The second version, using a two-pass segmentation, is described in a separate notebook.\n\n## Data Preprocessing\n\nData preprocessing consists in downsampling images. We also initially wanted to find to which group (fresh-frozen vs FFPE) they belonged to, but as we said before, we finally did not follow that approach. We copy our group-finding code for information.\n\n### Loading and Downsampling Whole-Slide Images\n\nThe main issue with WSI is that they are very big. Here is how we load an image, using NumPy MemMap.","1907406d":"### Separate Slices between FFPE and Fresh-Frozen\n\nAs both types of slices have different properties, we found beneficial to apply a different model for FFPE slices and Fresh-Frozen slices. The following `_get_group` function allowed to get the group of a given image: 0 if the slice has been fresh-frozen and and 1 if it has been formalin-fixed paraffin embedded. We found indeed that fresh-frozen tissue images had a physical spacing of 0.65 \u03bcm, whereas FFPE tissue images had a physical spacing of 0.5 \u03bcm. We obtained that information from the metadata of TIFF images. The first line (with ``###``) needs to be removed for that function to work.","e343ee8c":"We import them as follows.","a2aa4a43":"The aforementioned DataLoader is generated using the following `_make_tiles_dataloader` function.","ac789671":"Segmentation mask tiles are generated for all possible sets of padding parameters using on the following `get_mask_tiles` function.","0fb526f9":"## Generating Final Predictions\n\nFinal predictions are generated as follows. First, the `make_predictions` function generates mask tiles using the aforementioned `get_mask_tiles` function. Then, the mask is reconstructed based on these tiles computed with different padding parameters. Eventually, the obtained mask is binarized and converted into RLE format.","8fb9b5ca":"### Dividing the Image into Tiles\n\nThe following ``_split_image`` function allows us to divide images into tiles.","2fec7fee":"Then we generate masks for each image in the test set.","4cba83cb":"# Introduction\n\nThis kernel contains the inference code of the submission_experts team for the 2021 HuBMAP competition (version 1). Some additional insights are provided at the end of the notebook.\n\nThe goal of the competition was to provide with automated glomeruli segmentation models for WSI of human kidneys. In this competition, these WSI can be divided into two categories: those containing fresh-frozen (FFPE) tissues and those containing formalin-fixed paraffin-embedded tissues. Obvious visual differences can be noticed in both types of WSI: the structure of FFPE tissues is better preserved than fresh-frozen ones. Therefore, we initially tried to apply different models with respect to the type of WSI that we were processing. That approach was giving us very good results on the public leaderboard and on cross-validation before the change of test data, but unfortunately for some reason, we could not see that improvement afterwards. After discussion, we decided not to follow that approach in our final submissions.","b63a9d71":"And finally we generate a submission.csv file containing all our predictions.","6703a2ee":"All these functions are combined in `make_one_prediction`, that aims at saving segmentation mask tiles based on an image and a given set of padding parameters.","805011e3":"## Models\n\nThe models that we can use for inference are defined below.","2361297e":"All put together, the result is the following `load_resize` function.","506f6e43":"Then, we must reconstruct the image from resampled tiles that we saved. For that purpose, we apply the following `_reconstruct_img` function.","e82271bd":"## Making Predictions\n\nIn this section, we describe how we make predictions.\n\n### Padding\n\nAs images are very big, we must divide them into tiles. We added zero-padding to make the dimensions of the image dividable by the tile size. Ensembling predictions with different padding dimensions allows us to avoid bad predictions due to glomeruli located on edges of the tiles.","edf61077":"Eventually, we need a DataLoader to feed data to our model. This DataLoader is based on the `HuBMAPTestDataset` class below.","e36f8512":"Then we select tiles on which we will make predictions based on their color saturations.","d675fd9b":"The RLE conversion is done using the following `rle_encode_less_memory` function, obtained from https:\/\/www.kaggle.com\/bguberfain\/memory-aware-rle-encoding.","b221cbee":"# Submission\n\nHere is the final part of this kernel, dealing with the actual submission of a CSV file containing generated masks.\n\nImage names from public and private test sets are retrieved from the sample_submission.csv file that is provided. If `PUBLIC_ONLY = True`, then we only make the prediction for public test data.","1a88567b":"The `_generate_masks` function is based on the `Model_pred` iterator-like class that is used to generate predictions. These predictions are upsampled to match the original image. We use a TTA approach with four transformations from D4 (we finally did not use rotations, even though our code allows us to do it).","c55d6fa5":"The next step is logically to remove that padding.","0a10ac5a":"# Inference\n\nIn this section, we describe how we make predictions.\n\n## Inference in a nutshell\n\n- We make an inference on the bigger patch size than was used for training. Before the data update the best one was of 1024 and after - 512 while the training is performed on 256x256.\n\n- We used overlapping step of 0.5xtile_size in both X and Y directions to cut the patches from test images. Locally we used smaller step of 0.33 that wasn't used because of the inference time constraints.\n\n- We also used classical TTA based on transformations of the D4 group, but we were limited by the kernel inference time.\n\n- In order to handle the WSI images size we used memmap to load the images, and also used temporary saving of processed patches to perform the ensembling.\n\n- Very important point that we wanted to highlight, that we were able to split all test images into two groups with respect to the ffpe\/ff type using the \"PhysicalSizeY\" from the tiff.pages.description. This information was crucial before the data update and allowed us to get different models that worked for each of the groups. However, by some reasons (we did not discover which) this stopped working after the data update (at least for the public LB). But it still demonstrated higher CV when models were trained independently for each group.\n\n- Final submissions are ensembles of models trained with and without Color Space Augmenatations and Cutmix in order to increase the robustness.\n\n## Parameters\n\nHere are the parameters that we use for inference."}}