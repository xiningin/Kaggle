{"cell_type":{"853f0de5":"code","9deee1de":"code","91d70727":"code","4555733d":"code","ac02108e":"code","2479af47":"code","5d10016f":"code","d82b6cd3":"code","473b3733":"code","b187f968":"code","e7dba34a":"code","7705e784":"code","f0e1530f":"code","7a2c89fc":"code","eae6b6a7":"code","5dc45817":"code","c01abc05":"code","944b7128":"code","4ab1f42c":"code","1f572d6b":"code","828b7c43":"code","742489cb":"code","8f96a511":"code","576ce6b5":"code","1e1ea204":"code","8697a181":"code","f1898c80":"code","54bd5d7b":"code","6ab6e8b1":"code","60fc5520":"code","f82cbf0d":"code","b76a0a20":"markdown","1874c32c":"markdown","babd0034":"markdown","0c12f5e8":"markdown","3c2ad65a":"markdown","00c49ffc":"markdown","583a69d8":"markdown","66c0916c":"markdown","6ab7e272":"markdown","484bce65":"markdown","63bbced5":"markdown","57d41629":"markdown","ff8ce5af":"markdown","dd2a9948":"markdown","bc5005dd":"markdown","e1ae789d":"markdown","818aa3e4":"markdown","7ad3b492":"markdown","fa315717":"markdown","1be70d50":"markdown","e1b13fff":"markdown","695dd37a":"markdown"},"source":{"853f0de5":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\n\nplt.rcParams.update({'font.size': 14})\n\n# Load data\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsub_sample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nprint (train.shape, test.shape, sub_sample.shape)","9deee1de":"train.head()","91d70727":"train.duplicated().sum()","4555733d":"train = train.drop_duplicates().reset_index(drop=True)","ac02108e":"# Class balance\n# train.target.value_counts()\nsns.countplot(y=train.target);","2479af47":"# NA data\ntrain.isnull().sum()","5d10016f":"test.isnull().sum()","d82b6cd3":"# Check number of unique keywords, and whether they are the same for train and test sets\nprint (train.keyword.nunique(), test.keyword.nunique())\nprint (set(train.keyword.unique()) - set(test.keyword.unique()))","473b3733":"# Most common keywords\nplt.figure(figsize=(9,6))\nsns.countplot(y=train.keyword, order = train.keyword.value_counts().iloc[:15].index)\nplt.title('Top 15 keywords')\nplt.show()\n# train.keyword.value_counts().head(10)","b187f968":"kw_d = train[train.target==1].keyword.value_counts().head(10)\nkw_nd = train[train.target==0].keyword.value_counts().head(10)\n\nplt.figure(figsize=(13,5))\nplt.subplot(121)\nsns.barplot(kw_d, kw_d.index, color='c')\nplt.title('Top keywords for disaster tweets')\nplt.subplot(122)\nsns.barplot(kw_nd, kw_nd.index, color='y')\nplt.title('Top keywords for non-disaster tweets')\nplt.show()","e7dba34a":"top_d = train.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\ntop_nd = train.groupby('keyword').mean()['target'].sort_values().head(10)\n\nplt.figure(figsize=(13,5))\nplt.subplot(121)\nsns.barplot(top_d, top_d.index, color='pink')\nplt.title('Keywords with highest % of disaster tweets')\nplt.subplot(122)\nsns.barplot(top_nd, top_nd.index, color='yellow')\nplt.title('Keywords with lowest % of disaster tweets')\nplt.show()","7705e784":"# Check number of unique keywords and locations\nprint (train.location.nunique(), test.location.nunique())","f0e1530f":"# Most common locations\nplt.figure(figsize=(9,6))\nsns.countplot(y=train.location, order = train.location.value_counts().iloc[:15].index)\nplt.title('Top 15 locations')\nplt.show()","7a2c89fc":"raw_loc = train.location.value_counts()\ntop_loc = list(raw_loc[raw_loc>=10].index)\ntop_only = train[train.location.isin(top_loc)]\n\ntop_l = top_only.groupby('location').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l.index, y=top_l)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()","eae6b6a7":"# Fill NA values\nfor col in ['keyword','location']:\n    train[col] = train[col].fillna('None')\n    test[col] = test[col].fillna('None')\n\ndef clean_loc(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_loc:\n        return x\n    else: return 'Others'\n    \ntrain['location_clean'] = train['location'].apply(lambda x: clean_loc(str(x)))\ntest['location_clean'] = test['location'].apply(lambda x: clean_loc(str(x)))","5dc45817":"top_l2 = train.groupby('location_clean').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l2.index, y=top_l2)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()","c01abc05":"import pandas as pd\n#submit_lr = pd.read_csv(\"..\/input\/socialmediadisastertweetsdfecsv\/submit_lr.csv\")\n\n\nleak = pd.read_csv(\"..\/input\/disasters-on-social-media\/socialmedia-disaster-tweets-DFE.csv\")\nleak['target'] = (leak['choose_one']=='Relevant').astype(int)\nleak['id'] = leak.index\nleak = leak[['id', 'target','text']]\nmerged_df = pd.merge(test, leak, on='id')\nsub1 = merged_df[['id', 'target']]\nsub1.to_csv('submit_1.csv', index=False)","944b7128":"import re\n\ntest_str = train.loc[417, 'text']\n\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\n\nprint(\"Original text: \" + test_str)\nprint(\"Cleaned text: \" + clean_text(test_str))","4ab1f42c":"def find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n\ndef find_mentions(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n\ndef find_links(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"https?:\/\/\\S+\", tweet)]) or 'no'\n\ndef process_text(df):\n    \n    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n    df['links'] = df['text'].apply(lambda x: find_links(x))\n    # df['hashtags'].fillna(value='no', inplace=True)\n    # df['mentions'].fillna(value='no', inplace=True)\n    \n    return df\n    \ntrain = process_text(train)\ntest = process_text(test)","1f572d6b":"from wordcloud import STOPWORDS\n\ndef create_stat(df):\n    # Tweet length\n    df['text_len'] = df['text_clean'].apply(len)\n    # Word count\n    df['word_count'] = df[\"text_clean\"].apply(lambda x: len(str(x).split()))\n    # Stopword count\n    df['stop_word_count'] = df['text_clean'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n    # Punctuation count\n    df['punctuation_count'] = df['text_clean'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    # Count of hashtags (#)\n    df['hashtag_count'] = df['hashtags'].apply(lambda x: len(str(x).split()))\n    # Count of mentions (@)\n    df['mention_count'] = df['mentions'].apply(lambda x: len(str(x).split()))\n    # Count of links\n    df['link_count'] = df['links'].apply(lambda x: len(str(x).split()))\n    # Count of uppercase letters\n    df['caps_count'] = df['text_clean'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n    # Ratio of uppercase letters\n    df['caps_ratio'] = df['caps_count'] \/ df['text_len']\n    return df\n\ntrain = create_stat(train)\ntest = create_stat(test)\n\nprint(train.shape, test.shape)","828b7c43":"train.corr()['target'].drop('target').sort_values()","742489cb":"from nltk import FreqDist, word_tokenize\n\n# Make a set of stop words\nstopwords = set(STOPWORDS)\n# more_stopwords = {'https', 'amp'}\n# stopwords = stopwords.union(more_stopwords)","8f96a511":"# Unigrams\nword_freq = FreqDist(w for w in word_tokenize(' '.join(train['text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_word_freq = pd.DataFrame.from_dict(word_freq, orient='index', columns=['count'])\ntop20w = df_word_freq.sort_values('count',ascending=False).head(20)\n\nplt.figure(figsize=(8,6))\nsns.barplot(top20w['count'], top20w.index)\nplt.title('Top 20 words')\nplt.show()","576ce6b5":"plt.figure(figsize=(16,7))\nplt.subplot(121)\nfreq_d = FreqDist(w for w in word_tokenize(' '.join(train.loc[train.target==1, 'text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_d = pd.DataFrame.from_dict(freq_d, orient='index', columns=['count'])\ntop20_d = df_d.sort_values('count',ascending=False).head(20)\nsns.barplot(top20_d['count'], top20_d.index, color='c')\nplt.title('Top words in disaster tweets')\nplt.subplot(122)\nfreq_nd = FreqDist(w for w in word_tokenize(' '.join(train.loc[train.target==0, 'text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_nd = pd.DataFrame.from_dict(freq_nd, orient='index', columns=['count'])\ntop20_nd = df_nd.sort_values('count',ascending=False).head(20)\nsns.barplot(top20_nd['count'], top20_nd.index, color='y')\nplt.title('Top words in non-disaster tweets')\nplt.show()","1e1ea204":"# Bigrams\n\nfrom nltk import bigrams\n\nplt.figure(figsize=(16,7))\nplt.subplot(121)\nbigram_d = list(bigrams([w for w in word_tokenize(' '.join(train.loc[train.target==1, 'text_clean']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nd_fq = FreqDist(bg for bg in bigram_d)\nbgdf_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\nbgdf_d.index = bgdf_d.index.map(lambda x: ' '.join(x))\nbgdf_d = bgdf_d.sort_values('count',ascending=False)\nsns.barplot(bgdf_d.head(20)['count'], bgdf_d.index[:20], color='pink')\nplt.title('Top bigrams in disaster tweets')\nplt.subplot(122)\nbigram_nd = list(bigrams([w for w in word_tokenize(' '.join(train.loc[train.target==0, 'text_clean']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nnd_fq = FreqDist(bg for bg in bigram_nd)\nbgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\nbgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\nbgdf_nd = bgdf_nd.sort_values('count',ascending=False)\nsns.barplot(bgdf_nd.head(20)['count'], bgdf_nd.index[:20], color='yellow')\nplt.title('Top bigrams in non-disaster tweets')\nplt.show()","8697a181":"import category_encoders as ce\n\nfeatures = ['keyword', 'location_clean']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train[features],train['target'])\n\ntrain = train.join(encoder.transform(train[features]).add_suffix('_target'))\ntest = test.join(encoder.transform(test[features]).add_suffix('_target'))","f1898c80":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nfeatures_to_drop = ['id', 'keyword','location','text','location_clean','text_clean', 'hashtags', 'mentions','links']\n\nX_train = train.drop(columns = features_to_drop + ['target'])\nX_test = test.drop(columns = features_to_drop)\ny_train = train.target\n\nlr = LogisticRegression(solver = 'liblinear')\nlr.fit(X_train, y_train)\ncv5 = cross_val_score(lr, X_train, y_train, cv=5, scoring='f1')\nprint(cv5)\nprint(np.mean(cv5))","54bd5d7b":"pd.Series(np.transpose(lr.coef_[0]), index=X_train.columns).sort_values().plot.barh(grid=True)","6ab6e8b1":"y_test = lr.predict(X_test)\n\nsubmit = sub_sample.copy()\nsubmit.target = y_test\nsubmit.to_csv('submit_lr.csv',index=False)","60fc5520":"import pandas as pd\nsubmit_lr = pd.read_csv(\"..\/input\/socialmediadisastertweetsdfecsv\/submit_lr.csv\")","f82cbf0d":"import pandas as pd\nsocialmedia_disaster_tweets-DFE = pd.read_csv(\"..\/input\/disasters-on-social-media\/socialmedia-disaster-tweets-DFE.csv\")","b76a0a20":"## 6. Create statistics from texts","1874c32c":"Mumbai and Nigeria are still on the top. Other than the strange 'ss', London and New York made the bottom of % of disaster tweets.","babd0034":"The top 3 locations with highest % of disaster tweets are **Mumbai, Inida, and Nigeria**. As the location data is not clean, we see some interesting cases, such as **'London, UK' saw a higher-than-average % of disaster tweets, but 'London' is below average**. We try to clean up the location and see if there is any difference:","0c12f5e8":"## 9. Base Model: Logistic Regression\n\nWe try the simplest model with logistic regression, based on keyword, location and statistics we created.\n\nAs the competition uses F-1 score, our cv also use this scoring.\n\nReference: [scoring parameters](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter)","3c2ad65a":"## 3. Locations","00c49ffc":"## 2. Keywords","583a69d8":"There are 52 duplicated rows. The duplicates will be removed.","66c0916c":"Train and test have the same set of keywords","6ab7e272":"Text is all non-null. Only a small percentage of tweets have no keyword. Location has much more null values.","484bce65":"## 4. Spoiler Alert: You Can Get Perfect Score in (public) Leader Board\n\nYou will see in the public leaderboard, many participants got a perfect score. It is because the whole dataset with label is available online (one copy available [on Kaggle](https:\/\/www.kaggle.com\/jannesklaas\/disasters-on-social-media)). You can find the correct label of our test set so you can achieve perfect score.\n\nIn such case, the ranking on public leaderboard is meaningless. The good news is, you can now focus on learning NLP and modelling skills with this dataset, instead of fighting for higher position on the leaderboard!\n\n(Reference: [szelee's notebook](https:\/\/www.kaggle.com\/szelee\/a-real-disaster-leaked-label\/notebook))","63bbced5":"There is no common top 10 keywords between disaster and non-disaster tweets.","57d41629":"All of the statistics have very low correlation with the target variable","ff8ce5af":"Findings:\n- Top two words in disaster tweets: 'fire' and 'news', don't make the top 20 on unreal disaster tweets.\n- Words are more specific for real disaster tweets (e.g. 'califonia', 'hiroshima', 'fire', 'police', 'suicide', 'bomb').","dd2a9948":"Vectorization: TBC","bc5005dd":"## 5. Clean up Text Column\n\nHere we clean up the text column by:\n- Making a 'clean' text column, removing links and unnecessary white spaces\n- Creating separate columns containing lists of hashtags, mentions, and links","e1ae789d":"## 7. Most frequent words and bigrams\n\nWhat are the most common unigrams (single word) and bigrams (two-word sequence)?","818aa3e4":"That's it for now. Stay tuned for more analysis!","7ad3b492":"We try to distinguish disaster and non-disaster tweets:","fa315717":"Findings:\n- Most top bigrams in disaster tweets show certain kinds of catestrophe (e.g. suicide bomber, oil spill, california wildfire); for non-disaster tweets, only 'burning buildings' as top bigram look like a disaster;\n- Top bigrams in disaster tweets have a more casual tone;\n- 'youtube' appears in three of the twenty bigrams for non-disaster tweets; none in disaster tweets","1be70d50":"As location is free text, the data is not clean, you can see both 'USA' and 'United States' in top locations. We than have a look at % of disaster tweets for common locations.","e1b13fff":"## 1. Basic Exploration","695dd37a":"## 8. Encoding and Vectorizers\n\nAs part of feature generation, we will:\n- Apply target encoding to keyword and location (cleaned)\n- Count Vectorize cleaned text, links, hashtags and mentions columns"}}