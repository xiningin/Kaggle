{"cell_type":{"578ab14a":"code","6db8d39d":"code","278d9b5a":"code","ef989e06":"code","f2c65f2a":"code","8714f82a":"code","19b7aaa8":"code","9b9b93a9":"code","4938be09":"code","1d798aa5":"code","ee636eaa":"code","7a3531c8":"code","33c7efb7":"code","88fae07b":"code","3ee7ee39":"code","5af78fc8":"code","e0332f17":"code","dbc8560c":"code","f5d82fde":"code","931e90e8":"code","c6b4f208":"code","d40b0cca":"code","4a769024":"code","c0a9d931":"code","00bc9369":"code","4fd94200":"code","8719012c":"code","076df9d3":"code","d8d2be0f":"code","264e39cb":"code","18b43e6e":"code","ad8de884":"code","c834f01c":"code","b4a71c4c":"code","14b0e81c":"code","d02dd37c":"code","1ad11a13":"markdown","62e6f75e":"markdown","7b4c6565":"markdown","7d0459f5":"markdown","980bad8d":"markdown","f7af436c":"markdown","ecd1f184":"markdown","9f85457e":"markdown","97ce2377":"markdown","135b9173":"markdown","c592b3f2":"markdown","4d5e8383":"markdown","faa6dd90":"markdown","4e893b54":"markdown","e69c5172":"markdown","56e3c217":"markdown","efec1b5e":"markdown","f61480af":"markdown","769c4978":"markdown","1ed7cd2a":"markdown","2bdd8b7a":"markdown","2f330c19":"markdown","08979575":"markdown","03f28552":"markdown","94703011":"markdown","19c8d758":"markdown","48997a02":"markdown","ebc42579":"markdown","029c110b":"markdown","c610e668":"markdown","217626ec":"markdown","ea7d4368":"markdown","649860e4":"markdown","18ef92c1":"markdown","6fa86573":"markdown","911cd349":"markdown","4454e347":"markdown","141fa58f":"markdown"},"source":{"578ab14a":"from sklearn import metrics\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport warnings\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","6db8d39d":"#Importing Train dataset into Colab\ndata=pd.read_csv('..\/input\/predicting-employee-status\/employee_data (1).csv')","278d9b5a":"#First few rows of the dataframe\ndata.head()","ef989e06":"# Get all types of colums\ndata.dtypes","f2c65f2a":"# Can Statistic data on each column to understand the data better\ndata.describe(include='all')","8714f82a":"# View all dublicate row\ndata.duplicated().sum()\ndata[data.duplicated()]","19b7aaa8":"# Finding number of null values in individual column\ndata.isnull().sum()","9b9b93a9":"nullTable=round((data.isnull().sum()\/data.shape[0])*100,2)\nnullValueCols=pd.DataFrame(nullTable,columns=['Missing Value %'])\nnullValueCols.reset_index(inplace=True)\nnullValueCols.rename(columns={'index': 'Column Name'},inplace=True)\nnullValueCols[nullValueCols['Missing Value %']!=0]\nprint(nullTable)\n\nsns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='plasma')\nplt.title(\"Heat map plotting the missing values in the columns\")\nplt.show()","4938be09":"plt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'department', data = data)\nplt.title(\"Countplot for department Column\")\nplt.show()","1d798aa5":"# filling all null values with new department type 'other'\ndata['department'] = data['department'].fillna('other')","ee636eaa":"plt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'filed_complaint', data = data)\nplt.title(\"Countplot for filed_complaint Column\")\nplt.show()\n\nplt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'recently_promoted', data = data)\nplt.title(\"Countplot for recently_promoted Column\")\nplt.show()","7a3531c8":"# filling all null values of filed_complaint and recently_promoted with 0\ndata['filed_complaint'] = data['filed_complaint'].fillna(0)\ndata['recently_promoted'] = data['recently_promoted'].fillna(0)","33c7efb7":"plt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'last_evaluation', data = data.sample(n=1000))\nplt.title(\"Countplot for last_evaluation Column\")\nplt.show()\n\nplt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'satisfaction', data = data.sample(n=1000))\nplt.title(\"Countplot for satisfaction Column\")\nplt.show()\n\nplt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'tenure', data = data.sample(n=1000))\nplt.title(\"Countplot for tenure Column\")\nplt.show()","88fae07b":"data['last_evaluation'] = data['last_evaluation'].fillna(data['last_evaluation'].mean())\ndata['satisfaction'] = data['satisfaction'].fillna(data['satisfaction'].mean())\ndata['tenure'] = data['tenure'].fillna(data['tenure'].mean())","3ee7ee39":"sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='plasma')\nplt.title(\"Heat map plotting the missing values in the columns\")\nplt.show()","5af78fc8":"NewNumeric=data[['last_evaluation','n_projects','satisfaction','tenure']]\nNewNumericMelt=NewNumeric.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp=sns.boxplot(x='variable',y='value',data=NewNumericMelt)\nbp.set_xticklabels(bp.get_xticklabels(),rotation=90)\nplt.show()","e0332f17":"100 * (data[\"tenure\"] > 6).sum() \/ data.shape[0]","dbc8560c":"above6years = data[data['tenure'] > 6]\n(above6years['status'] == 'Left').sum()","f5d82fde":"# saving all the people who are working only less then 6 years\ndata = data[data['tenure'] < 6]","931e90e8":"NewNumeric=data[['last_evaluation','n_projects','satisfaction','tenure']]\nNewNumericMelt=NewNumeric.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp=sns.boxplot(x='variable',y='value',data=NewNumericMelt)\nbp.set_xticklabels(bp.get_xticklabels(),rotation=90)\nplt.show()","c6b4f208":"plt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'status', data = data)\nplt.title(\"Countplot for status Column\")\nplt.show()","d40b0cca":"# Convert 'Left' for 1 and 'Employed' for 0\nY = np.where(data['status'].values == 'Left', 1, 0)\n\n# dropping the target column and create the matrix of features\nX = data.drop(['status'], axis=1)","4a769024":"# view the number of unique values each column\nfeatures=data.columns\nfor i in features:\n    uniqueValues=data[i].nunique()\n    print(i,uniqueValues)\n    \ndata.dtypes","c0a9d931":"# make all categorial columns to separeted columns\nX = pd.get_dummies(X, drop_first=True)\nX.head()","00bc9369":"num_cols=['avg_monthly_hrs','last_evaluation','n_projects','satisfaction','tenure']\nscaler = StandardScaler()\nX[num_cols] = scaler.fit_transform(X[num_cols])\nX.head()","4fd94200":"# train test split\n\n\ny_left = Y[Y == 1]\nx_left = X[Y == 1]\ny_stay = Y[Y == 0]\nx_stay = X[Y == 0]\n\nx_train_left, x_test_left, y_train_left, y_test_left = train_test_split(x_left,y_left , test_size = .25, random_state=45)\nx_train_stay, x_test_stay, y_train_stay, y_test_stay = train_test_split(x_stay,y_stay , test_size = .25, random_state=45)\nx_train = np.concatenate((x_train_left, x_train_stay), axis=0)\ny_train = np.concatenate((y_train_left, y_train_stay), axis=0)\nx_test = np.concatenate((x_test_left, x_test_stay), axis=0)\ny_test = np.concatenate((y_test_left, y_test_stay), axis=0)","8719012c":"#creating a random search for some hyper parameters given in param_grid_1\ndt=DecisionTreeClassifier()\nparam_grid={\n    'criterion':['gini','entropy'],\n    'max_depth':np.arange(4,20,1),\n    'min_samples_split':np.arange(0.001,0.1,0.01),\n    'max_features':['log2','sqrt','auto'],\n    'min_weight_fraction_leaf':np.arange(0.001,0.25,0.05)\n}\nr_search=RandomizedSearchCV(dt,param_distributions=param_grid,n_iter=10,verbose=1)\nr_search.fit(x_train,y_train)","076df9d3":"#getting best performing hyper parameters from random search \nr_search.best_params_","d8d2be0f":"param_grid = {'min_weight_fraction_leaf': np.arange(0.001,0.01,0.001),\n 'min_samples_split': np.arange(0.07,0.12,0.01),\n 'max_features': ['auto'],\n 'max_depth': np.arange(10,20,1),\n 'criterion': ['entropy']}\ndt=DecisionTreeClassifier()\ngrid_search=GridSearchCV(estimator=dt,param_grid = param_grid,cv=5,verbose=1,n_jobs=-1)\ngrid_search.fit(x_train,y_train)\ngrid_search.best_params_\ny_predictions = grid_search.best_estimator_.predict(x_test)\nprint(classification_report(y_test,y_predictions))","264e39cb":"dt=DecisionTreeClassifier(criterion= 'entropy',max_depth= 17,max_features= 'auto',min_samples_split= 0.07,min_weight_fraction_leaf= 0.006)\ndt.fit(x_train,y_train)\ny_predictions = dt.predict(x_test)\nprint(classification_report(y_test,y_predictions))","18b43e6e":"dt=RandomForestClassifier()\nparam_grid={\n    'criterion':['gini','entropy'],\n    'max_depth':np.arange(4,20,1),\n    'min_samples_split':np.arange(0.001,0.1,0.01),\n    'max_features':['log2','sqrt','auto'],\n    'min_weight_fraction_leaf':np.arange(0.001,0.25,0.05),\n    'n_estimators': np.arange(50,500,50)\n}\nr_search=RandomizedSearchCV(dt,param_distributions=param_grid,n_iter=50,verbose=1)\nr_search.fit(x_train,y_train)\nr_search.best_params_\n","ad8de884":"param_grid = {'min_weight_fraction_leaf': np.arange(0.001,0.005,0.001),\n 'min_samples_split': np.arange(0.01,0.06,0.01),\n 'max_features': ['auto'],\n 'max_depth': np.arange(15,20,1),\n 'criterion': ['entropy','gini'],\n 'n_estimators': [100]}\ndt=RandomForestClassifier()\ngrid_search=GridSearchCV(estimator=dt,param_grid = param_grid,cv=5,verbose=1,n_jobs=-1, scoring='recall')\ngrid_search.fit(x_train,y_train)\ngrid_search.best_params_#getting best parameters of grid search\nm_best = grid_search.best_estimator_\nrf_predictions_val_y=m_best.predict(x_test)\nprint(classification_report(y_test,rf_predictions_val_y))\n","c834f01c":"clf = RandomForestClassifier(max_depth=18, random_state=45)\nclf.fit(x_train, y_train)\nrf_predictions_val_y=clf.predict(x_test)\nprint(classification_report(y_test,rf_predictions_val_y))","b4a71c4c":"param_grid = {'learning_rate': np.arange(0.1,1,0.05),\n 'max_depth': np.arange(1,15,2),\n 'max_features': ['auto', 'sqrt', 'log2'],\n 'max_depth': np.arange(15,20,1),\n 'n_estimators': np.arange(80,150,20)}\ngradient_boosting=GradientBoostingClassifier()\ngrid_search=GridSearchCV(estimator=gradient_boosting,param_grid = param_grid,cv=5,verbose=1,n_jobs=-1, scoring='recall')\ngrid_search.fit(x_train,y_train)\ngrid_search.best_params_#getting best parameters of grid search\nm_best = grid_search.best_estimator_\nrf_predictions_val_y=m_best.predict(x_test)\nprint(classification_report(y_test,rf_predictions_val_y))","14b0e81c":"gradient_boosting=GradientBoostingClassifier(learning_rate= 0.15,max_depth= 8,max_features= 'log2',n_estimators= 100)\ngradient_boosting.fit(x_train, y_train)\nrf_predictions_val_y=gradient_boosting.predict(x_test)\nprint(classification_report(y_test,rf_predictions_val_y))","d02dd37c":"from sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier(n_estimators=10, random_state=0, learning_rate= 1)\nclf.fit(x_train, y_train)\nrf_predictions_val_y=gradient_boosting.predict(x_test)\nprint(classification_report(y_test,rf_predictions_val_y))","1ad11a13":"For every model I am using, I am running first RandomGridSearch to find the general rage of the optimize parameters from wide range parameters. Then I will run GridSearchCV to find the best parameters from small range of parameters by running each one. ","62e6f75e":"# **Random Forest**","7b4c6565":"# **Handle Missing Values**\n\nLet's check if we have missings cells in our dataset.","7d0459f5":"# **DecisionTree**","980bad8d":"**Observation:**\n\nAs we can in the plot all those feilds are numerical and distrubuted well. We will fill when with their mean.","f7af436c":"**Fill Missing Values in filed_complaint and recently_promoted:**\n\nLet's check this column values in order to decide what values we can fill the the missing spots","ecd1f184":"# **Train Test Split**\n\nWe will split our data for train and test so we can verify our performance. Since only 30% of our data refers to people who left the company we want our train and test data to be balanced with this samples.","9f85457e":"# **Outliers**\n\nLet's plot our numerical colums to see if we have some outliers","97ce2377":"**Observation:**\n\nWe can see that most of the dublicated rows caused due has Null values, so that are not really dublicated. So we wont remove them.","135b9173":"**Observation:**\n\nWe have some outliers in tenure column. Most the people stays in the company for 3-4 years. But some people are more than 6 years or even 10(maybe the founders). Let's check how much from the dataset this outliers represents:","c592b3f2":"**Goal:**\nOur main goal is to predict the highest percentege of the people who are going to leave the company. It will be better to find more of them and mistake in some. Meaning the recall is more important for us then precision. ","4d5e8383":"**Fill Missing Values in last_evaluation & satisfaction & tenure:**\n\nLet's check this column values in order to decide what values we can fill the the missing spots","faa6dd90":"Now let's run GridSearchCV for specific parameters to get the best values","4e893b54":"# **AdaBoost**","e69c5172":"# **Dublicate Rows**\n\nBefore we will start working on the data, let's make sure there is no duplicate data in our dataset.","56e3c217":"Now let's make sure we dont have any outliers now","efec1b5e":"# **Categorial Analysis**","f61480af":"As we can see the outliers are 3.76% of out data. Let's see how many people who stays more that 6 years left the company.","769c4978":"first let's run Random Grid Search to find the general rage of the optimize parameters","1ed7cd2a":"# **Importing Dataset and Performing Descriptive Statistics**","2bdd8b7a":"**Observation:**\n\nOur categorial colums are:\n1. department\n2. filed_complaint(Aready 0 and 1)\n3. recently_promoted(Aready 0 and 1)\n4. salary\n\nAll those feilds are already objects and all others are numbers(float). We can run get_dummies.","2f330c19":"The department column is a categorial column and has diffrent types of departments. Some of the workers in the company does not have any department value, maybe they are not in specific department so we will create for them a new department type called 'other'.","08979575":"**Fill Missing Values in depertment:**\n\nLet's check this column values in order to decide what values we can fill the the missing spots\n","03f28552":"filed_complaint is saying if the employee has filed a formal complaint in the last 3 years, as we can see in the plot 1 is when the employee field a complaint. All missing values are when the employee didnt file it. As for this , the missing values will be 0. Also recently_promoted is the same, only the one who recently promoted are mask with 1 , all the missing ones should be 0.","94703011":"first let's run Random Grid Search to find the general rage of the optimize parameters","19c8d758":"**Observation:**\n\nGood for us ,we dont have imbalance data. third of the rows are decribing emplyees who left the comapny. We are good to go.","48997a02":"**Observation:**\n\nOur numeric colums are:\n1. avg_monthly_hrs\n2. last_evaluation\n3. n_projects\n4. satisfaction\n5. tenure\n\nWe will Scale those columns with the StandardScaler(x-std\/mean)","ebc42579":"# Importing Python Libraries","029c110b":"**Verify That There is no Missing Values:**","c610e668":"**Overall Observations Missing Value Analysis:**\n\nBelow are colums with the  missing values in our dataset:\n\n1. departmen\n2. filed_complaint\n3. last_evaluation\n4. recently_promoted\n5. satisfaction\n6. tenure\n\nLet's plot the missing values.","217626ec":"Now let's run GridSearchCV for specific parameters to get the best values","ea7d4368":"# **Model Building**","649860e4":"# **Numeric Analysis**","18ef92c1":"# **Gradient Boosting**","6fa86573":"## **Employee Attrition Problem:**\n\nThe key to success in any organization is attracting and retaining top talent who stays and works together. One of the key tasks is to prevent an employee from leaving the company. We will use machine learning to predict the employees which are going to leave the company and try to prevent it from happening.\n\nDescription of few variables:\n\nData Description:\n\n1. **status** \u2013 Current employment status (Employed \/ Left)\n1\n2. **department** \u2013 Department employees belong(ed) to\n3. **salary** \u2013 Salary level relative to rest of their department\n4. **tenure** \u2013 Number of years at the company\n5. **recently_promoted** \u2013 Was the employee promoted in the last 3 years?\n6. **n_projects** \u2013 Number of projects employee is staffed on\n7. **avg_monthly_hrs** \u2013 Average number of hours worked per month\n8. **satisfaction** \u2013 Score for employee\u2019s satisfaction with the company (higher is better)\n9. **last_evaluation** \u2013 Score for most recent evaluation of employee (higher is better)\n10. **filed_complaint** \u2013 Has the employee filed a formal complaint in the last 3 years?\n","911cd349":"# **Imbalance Analysis**\n\nBefore we will start working on the data, let's make sure the data is balanced and we have enought cases of people who Left the company. If not , we will use SMOTE to create more data.\n\n\n","4454e347":"There are no people at the level that left the comapny. Since our goal is to find those people we will remove this outlier data becouse it wont have us quite.","141fa58f":"# **Create Feature and Targets Matrixes**"}}