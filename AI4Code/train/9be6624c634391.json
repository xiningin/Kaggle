{"cell_type":{"9f8fbb07":"code","eb6626cb":"code","b2fc52bb":"code","71f8f0df":"code","e2179442":"code","737c55b5":"code","901b062a":"code","72f99ed0":"code","a43fbf7e":"code","358fb886":"code","7be8f5b9":"code","5960d959":"code","6e3c8b4a":"markdown","ba689425":"markdown","c3e5ba44":"markdown","fb450d13":"markdown","20e17878":"markdown","415698c5":"markdown","31c54aa1":"markdown","e270f7f3":"markdown","73aad2e5":"markdown","cc4ca76a":"markdown","9ba982ed":"markdown","e394b07d":"markdown"},"source":{"9f8fbb07":"#Code Block 1\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score\n\ncols = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-rate', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\ncars = pd.read_csv('..\/input\/ucmachinelearning\/imports-85.data', names=cols) #df lacks headers\npd.set_option('max_columns', cars.shape[1]) #display all columns\ncars","eb6626cb":"#Code Block 2\n#exclude price and exclude symboling from numeric_col\nnumeric_col = ['normalized-losses', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-size', 'bore', 'stroke', 'compression-rate', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg']\ndisplay(len(numeric_col))\n\ndf = cars[numeric_col].copy() #features only\ndf['price'] = cars['price'] #target column\ndf","b2fc52bb":"#Code Block 3\ndf = df.replace('?',np.nan) #get rid of objects\ndf = df.astype(float) #change all to float\ndisplay(df.isnull().sum()) #too many nulls, cannot delete rows\n\ndf = df.fillna(df.mean()) #replace nulls with mean values of each column\ndisplay(df)\ndisplay(df.isnull().sum())","71f8f0df":"#Code Block 4\ndf[numeric_col] = (df[numeric_col]-df[numeric_col].min())\/(df[numeric_col].max()-df[numeric_col].min())\ndf","e2179442":"#Code Block 5\ndef knn_train_test(train_col, target_col, df, k = 5):\n    \n    np.random.seed(1)\n    df = df.iloc[np.random.permutation(len(df))]  #randomize rows of df\n    \n    sample_size = len(df)\/\/2  #train half, test half\n    train = df[:sample_size]\n    test = df[sample_size:]\n    \n    # train and predict, default 5 neighbors\n    knn = KNeighborsRegressor(n_neighbors = k)\n    knn.fit(train[[train_col]], train[target_col])\n    predictions = knn.predict(test[[train_col]])\n    rmse = (mean_squared_error(test[target_col], predictions))**0.5\n    return rmse\n\nneighbor5_univariate_rmse_dic = {} #empty dictionary to store rmse values\nfor col in numeric_col:\n    rmse = knn_train_test(col, 'price', df)\n    neighbor5_univariate_rmse_dic[col] = rmse\ndisplay(neighbor5_univariate_rmse_dic)\n\n#sort values ascending\nsorted(neighbor5_univariate_rmse_dic.items(), key = lambda item: item[1])","737c55b5":"#Code Block 6\nvary_k_univariate_rmse_list_dic = {}\nvary_k_univariate_avg_rmse_dic = {}\nk_list = [1,3,5,7,9]\nplt.figure(figsize=(10,8))\n\nfor col in numeric_col:\n    temp_list = []\n    for k in k_list:\n        rmse = knn_train_test(col, 'price', df, k)\n        temp_list.append(rmse)\n    vary_k_univariate_rmse_list_dic[col] = temp_list\n    vary_k_univariate_avg_rmse_dic[col] = np.mean(temp_list)\n    plt.plot(k_list, temp_list, linewidth=3)\n\nplt.title('Univariate model feature selection', fontsize=20)\nplt.xlabel('Number of neighbors', fontsize=16)\nplt.ylabel('RMSE', fontsize=16)\nplt.xticks(k_list, fontsize=12)\nplt.yticks(fontsize=12)\nplt.ylim(3900,5000) #zooming in here to focus on low RMSE values\nplt.legend(numeric_col, bbox_to_anchor=(1.25, 0.8), fontsize=12)\nplt.show()\n\ndisplay(sorted(vary_k_univariate_avg_rmse_dic.items(), key = lambda item: item[1]))","901b062a":"#Code Block 7\ndef knn_train_test(train_list, target_col, df, k_list = [7]): #start with 7 neighbors\n    np.random.seed(1)\n    df = df.iloc[np.random.permutation(len(df))]  #randomize rows of df\n    \n    sample_size = len(df)\/\/2  #train half, test half\n    train = df[:sample_size]\n    test = df[sample_size:]\n    \n    k_list_multivar_rmse_dic = {}  #generalize for multiple k later\n    for k in k_list:\n        knn = KNeighborsRegressor(n_neighbors = k) #train and predict\n        knn.fit(train[train_list], train[target_col])\n        predictions = knn.predict(test[train_list])\n        rmse = (mean_squared_error(test[target_col], predictions))**0.5\n        k_list_multivar_rmse_dic[k] = rmse\n    return k_list_multivar_rmse_dic\n\ntop_6_features = ['engine-size','highway-mpg','curb-weight','horsepower', 'width', 'city-mpg']\nlist_of_dics = []\nfor i in range(2,7):\n    train_list = top_6_features[:i]\n    rmse_dic = knn_train_test(train_list,'price',df)\n    list_of_dics.append(rmse_dic)\ndisplay(list_of_dics)","72f99ed0":"#Code Block 8\ndic_of_dics = {}\nfor i in range(2,7):\n    train_list = top_6_features[:i]\n    rmse_dic = knn_train_test(train_list,'price',df,k_list = [i for i in range(1,27)])\n    dic_of_dics[i] = rmse_dic\n    print(train_list)\n    min_error = min(rmse_dic.values())\n    display(min_error, list(filter(lambda x: rmse_dic[x] == min_error, rmse_dic.keys())))","a43fbf7e":"#Code Block 9\nplt.figure(figsize = (10,8))\nfor number_of_features, dic in dic_of_dics.items(): #dic is the dictionary where key is k and value is error\n    x = list(dic.keys())\n    y = list(dic.values())\n    string = str(number_of_features) + \" features\"\n    plt.plot(x,y, label = string)\n    \nplt.xlabel('number of neighbors')\nplt.ylabel('RMSE')\nplt.xlim(0,10)  #zooming in to focus on low RMSE\nplt.ylim(3250,3600)\nplt.legend()\nplt.show()","358fb886":"#Code Block 10\ntop_5_features = ['engine-size','highway-mpg','curb-weight', 'width', 'city-mpg']\ndic_of_dics = {}\nfor i in range(2,6):\n    train_list = top_5_features[:i]\n    rmse_dic = knn_train_test(train_list,'price',df,k_list = [i for i in range(1,27)])\n    dic_of_dics[i] = rmse_dic\n    print(train_list)\n    min_error = min(rmse_dic.values())\n    display(min_error, list(filter(lambda x: rmse_dic[x] == min_error, rmse_dic.keys())))\n    \nplt.figure(figsize = (10,8))\nfor number_of_features, dic in dic_of_dics.items(): #dic is the dictionary where key is k and value is error\n    x = list(dic.keys())\n    y = list(dic.values())\n    string = str(number_of_features) + \" features\"\n    plt.plot(x,y, label = string)\n    \nplt.xlabel('number of neighbors')\nplt.ylabel('RMSE')\nplt.xlim(0,10)  #zooming in to focus on low RMSE\nplt.ylim(3200,3600)\nplt.legend()\nplt.show()","7be8f5b9":"#Code Block 11\ndef multifold_univar_knn(fold_list, train_list, target_col, df, k_list = [5]): #start with 5 neighbors\n    np.random.seed(1)\n    df = df.iloc[np.random.permutation(len(df))]  #randomize rows of df\n    multifold_univar_rmse_dic = {}\n    for n_splits in fold_list:\n        for k in k_list:\n            kf = KFold(n_splits, shuffle = True, random_state = 1)\n            knn = KNeighborsRegressor(n_neighbors = k) #train and predict\n            mses = cross_val_score(knn, df[train_list], df['price'], scoring = 'neg_mean_squared_error', cv = kf)\n            rmses = (abs(mses))**0.5\n            avg_rmse = np.mean(rmses)\n            std_rmse = np.std(rmses)\n            multifold_univar_rmse_dic[(n_splits,k)] = (avg_rmse, std_rmse)\n    return multifold_univar_rmse_dic\n\ntop_6_features = ['engine-size','highway-mpg','curb-weight','horsepower', 'width', 'city-mpg']\nfold_list = [x for x in range(2,27)]\nplt.figure(figsize=(10,6))\nlist_of_dics = []\n\nfor i in range(6):\n    train_list = [top_6_features[i]]\n    rmse_dic = multifold_univar_knn(fold_list, train_list,'price',df)\n    list_of_dics.append(rmse_dic)\n    x = [list(rmse_dic.keys())[i][0] for i in range(25)]\n    y = [list(rmse_dic.values())[j][0] for j in range(25)]\n    z = [list(rmse_dic.values())[j][1] for j in range(25)]\n    plt.plot(x,y, linewidth = 3)\nplt.legend(top_6_features, frameon=False, bbox_to_anchor=(1.25, 0.8), fontsize=12)\nplt.title('Multifold Univariate model, 5 neighbors', fontsize=30)\nplt.xlabel('Number of folds', fontsize=22)\nplt.ylabel('RMSE average', fontsize=22)\nplt.show()\nprint('Please wait 1 minute for second graph for bivariate model to load.')\n#     plt.figure(figsize=(10,6))\n#     plt.plot(x,z)\n#     plt.legend(train_list, frameon=False, bbox_to_anchor=(1.25, 0.8), fontsize=12)\n#     plt.title('Multifold Univariate model, 5 neighbors', fontsize=30)\n#     plt.xlabel('Number of folds', fontsize=22)\n#     plt.ylabel('RMSE standard deviation', fontsize=22)\n#     plt.show()\n    \n\n#display(list_of_dics)\n\n#bivariate model\nlist_bivariate = []\nfor i in range(6):\n    for j in range(i+1,6):\n        train_list = [top_6_features[i],top_6_features[j]]\n        list_bivariate.append(train_list)\n        rmse_dic = multifold_univar_knn(fold_list, train_list,'price',df)\n        list_of_dics.append(rmse_dic)\n\nplt.figure(figsize = (10,6))\n \nfor m in range(len(list_of_dics)):\n    x = [list(list_of_dics[m].keys())[i][0] for i in range(25)]\n    y = [list(list_of_dics[m].values())[j][0] for j in range(25)]\n    z = [list(list_of_dics[m].values())[j][1] for j in range(25)]\n    plt.plot(x,y, linewidth = 3)\nplt.legend(list_bivariate, frameon=False, bbox_to_anchor=(1.5, 1), fontsize=12)\nplt.title('Multifold Bivariate model, 5 neighbors', fontsize=30)\nplt.xlabel('Number of folds', fontsize=22)\nplt.ylabel('RMSE average', fontsize=22)\nplt.show()\n\ndisplay(list_of_dics)\n\n#         plt.figure(figsize=(10,6))\n#         plt.plot(x,z)\n#         plt.legend(train_list, frameon=False, bbox_to_anchor=(1.25, 0.8), fontsize=12)\n#         plt.title('Multifold Bivariate model, 5 neighbors', fontsize=30)\n#         plt.xlabel('Number of folds', fontsize=22)\n#         plt.ylabel('RMSE standard deviation', fontsize=22)\n#         plt.show()","5960d959":"#Code Block 12\ntop_5_features = ['engine-size','highway-mpg','curb-weight', 'width', 'city-mpg']\ndic_of_dics = {}\nfold_list = [x for x in range(3,20,1)]  #folds\nk_list = [i for i in range(3,11,1)]  #neighbors\n\nlist_multi_feature = []\nfor i in range(2,6):\n    train_list = top_5_features[:i]\n    list_multi_feature.append(train_list)\n#print(len(list_multi_feature))\n    \nplt.figure(figsize=(15,40))\n\nfor k in k_list:\n    plt.subplot(10, 2, k-2)\n    for thelist in list_multi_feature:\n        final_list = []\n        for n in fold_list:\n            rmse = multifold_univar_knn([n], thelist, 'price', df, k_list = [k])\n            rmse = rmse[(n,k)][0]\n            final_list.append(rmse)\n        plt.plot(fold_list, final_list, linewidth=3) #18 sets of values\n        plt.text(11, 3800, f'number of neighbors = {k}', fontsize=14)\n        plt.ylim(2800,3900)\n        if k==10:\n            plt.legend(list_multi_feature, frameon=False, bbox_to_anchor=(0,-0.2), fontsize=13)\n    plt.xticks(fold_list, fontsize=10)\nplt.show()\n","6e3c8b4a":"## Data Cleaning","ba689425":"From graph, best k value with 2 or 3 features is k = 5, while best k value with 4 to 6 features is k = 2. Best RMSE is 3308, with top 6 features and 2 neighbors. It bothers me that horsepower should probably not be in the top features. I will re-run the code with just the top 5 features, excluding horsepower.","c3e5ba44":"The error has improved. The top 3 models are (1) 4 features with 2 neighbors, (2) 5 features with 2 neighbors, (3) 2 features with 5 neighbors. I'm pretty happy with these results.","fb450d13":"## Fix 5 neighbors, test\/train validation, univariate model, find best predictive features","20e17878":"Univariate model with lowest RMSE 3324 is engine-size univariate model, 5 neighbors and 25 folds.\n\nBivariate model with lowest RMSE 2891 is city-mpg and highway-mpg, 5 neighbors and 25 folds.\n\nI don't want too many folds as it can lead to overfitting and I want a model that generalizes well. \n\nIdeally the final model has around 5 folds and around 5 neighbors.","415698c5":"With varied number of neighbors, average rmse is lowest for engine-size, highway-mpg, curb-weight, horsepower, width and city-mpg.\nFrom the graph, we can see that 3 neighbors or 7 neighbors give low RMSE in general. The line graph for horsepower can barely be seen. So we will test the Top 6 features in the upcoming multivariate model. The graph suggests that 7 neighbors is the best for the top predicting features. So I will choose 7 neighbors for the following multivariate model.","31c54aa1":"## Testing Different Number of Folds on univariate model with 5 neighbors","e270f7f3":"It seems like, the fewer the features, the better the results. Let's iterate over more values of k.","73aad2e5":"3 or 4 neighbors is the best. 8 or 9 folds look good. bivariate model with 2 features is the best, engine-size and highway-mpg. I will choose 9 folds, 3 neighbors, bivariate model with engine-size and highway-mpg as predictive features.","cc4ca76a":"Top 6 features in univariate model are engine-size, highway-mpg, curb-weight, width, city-mpg, horsepower. \n## Let number of neighbors vary for each of the 14 features","9ba982ed":"## Objective:\n1. Clean data for machine learning.\n2. Employ k-nearest neighbors models. First fix number of neighbors and find the best feature(s) using 2-fold.\n3. Then select the best feature(s) and vary the number of neighbors to find the best combination(s).\n4. Next select best combinations and find the best number of folds. \n5. Conclusion: The best predictive multivariate model uses f features, n neighbors and k folds.","e394b07d":"## Fix 7 neighbors, test what number of features\/variables is the best"}}