{"cell_type":{"0b73793f":"code","858e9226":"code","a0a7b908":"code","4ec6d05b":"code","cbb35adb":"code","15dd3657":"code","ccdf16aa":"code","4f6fd507":"code","37657c31":"code","f5be0f8c":"code","6fa0ab76":"code","243fe8f8":"code","2042ef2d":"code","33f39351":"code","f3c7af73":"code","f10d76f1":"code","d7faf103":"code","40dcd316":"code","403d694e":"code","52778b9c":"code","b47d21f0":"code","050597b4":"code","a9d601c7":"code","3005b8ac":"code","21e38968":"code","8362b102":"code","e2b305ed":"code","9100efcc":"code","a8b61e40":"code","dc1c6a5e":"code","4e3d550f":"code","1964dfd4":"code","7f4c6238":"code","7a971951":"code","8cba4cfb":"code","bfba7d2c":"code","06e73ff6":"code","f6e76557":"code","65a0117e":"code","005c21d4":"code","3cf9909a":"code","c44d1a25":"code","443c3fbe":"code","8498beab":"code","c807b0e5":"code","45b92016":"code","03b8b879":"code","749aea5e":"code","8640ce68":"code","f5583e41":"code","3469579d":"code","610c6c9b":"code","fec6e9dc":"code","b9ee1165":"code","aae8e789":"code","5e12587f":"code","1278c65a":"code","d101d9cb":"code","1523fa86":"code","0a32fdbc":"code","6698da46":"code","c50ae646":"code","7fd513df":"code","e393312c":"code","da4e1ad9":"code","01a863d0":"code","83fb9011":"code","7bf63adf":"markdown","525b72d9":"markdown","f770f083":"markdown","b6854241":"markdown","4bc5f1ee":"markdown","23ab8ad5":"markdown","14f47709":"markdown","3c44590b":"markdown","52b88465":"markdown","9a043ab2":"markdown","f71e48db":"markdown","b8374a0a":"markdown","f0861008":"markdown","b86db853":"markdown","7bf8ddd9":"markdown","5263b781":"markdown","13a22160":"markdown","a41ed9ea":"markdown","fd9fd4e8":"markdown","f8648c9c":"markdown","50d115ae":"markdown","455672c1":"markdown","8947f5f3":"markdown","1ce9a349":"markdown","fa565014":"markdown","a3af86f8":"markdown","6b941820":"markdown","89c3fab3":"markdown","5e1195de":"markdown"},"source":{"0b73793f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","858e9226":"raw_data = pd.read_csv('\/kaggle\/input\/1.04. Real-life example.csv')\nraw_data.head()","a0a7b908":"raw_data.describe(include='all')","4ec6d05b":"data = raw_data.drop('Model', axis=1)\ndata.describe(include='all')","cbb35adb":"data.isnull().sum()","15dd3657":"data_no_mv = data.dropna(axis=0)","ccdf16aa":"data_no_mv.describe(include='all')","4f6fd507":"sns.distplot(data_no_mv['Price'])","37657c31":"sns.distplot(data_no_mv['Mileage'])","f5be0f8c":"sns.distplot(data_no_mv['EngineV'])","6fa0ab76":"sns.distplot(data_no_mv['Year'])","243fe8f8":"q = data_no_mv['Price'].quantile(0.99)\ndata_1 = data_no_mv[data_no_mv['Price'] < q]\ndata_1.describe(include='all')","2042ef2d":"sns.distplot(data_1['Price'])","33f39351":"q = data_1['Mileage'].quantile(0.99)\ndata_2 = data_1[data_1['Mileage'] < q]","f3c7af73":"sns.distplot(data_2['Mileage'])","f10d76f1":"data_3 = data_2[data_2['EngineV'] < 6.5]","d7faf103":"sns.distplot(data_3['EngineV'])","40dcd316":"q = data_3['Year'].quantile(0.01)\ndata_4 = data_3[data_3['Year'] > q]","403d694e":"sns.distplot(data_4['Year'])","52778b9c":"data_cleaned = data_4.reset_index(drop=True)","b47d21f0":"data_cleaned.describe(include='all')","050597b4":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n\nax1.scatter(data_cleaned['Year'], data_cleaned['Price'])\nax1.set_title('Price and Year')\n\nax2.scatter(data_cleaned['EngineV'], data_cleaned['Price'])\nax2.set_title('Price and EngineV')\n\nax3.scatter(data_cleaned['Mileage'], data_cleaned['Price'])\nax3.set_title('Price and Mileage')\n\nplt.show()","a9d601c7":"#The above patterns is not linear, may be because of Price column in not normally distributed\nsns.distplot(data_cleaned['Price'])","3005b8ac":"log_price = np.log(data_cleaned['Price'])\ndata_cleaned['log_price'] = log_price\ndata_cleaned.describe(include='all')","21e38968":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n\nax1.scatter(data_cleaned['Year'], data_cleaned['log_price'])\nax1.set_title('Log Price and Year')\n\nax2.scatter(data_cleaned['EngineV'], data_cleaned['log_price'])\nax2.set_title('Log Price and EngineV')\n\nax3.scatter(data_cleaned['Mileage'], data_cleaned['log_price'])\nax3.set_title('Log Price and Mileage')\n\nplt.show()","8362b102":"data_cleaned = data_cleaned.drop(['Price'], axis=1)","e2b305ed":"from statsmodels.stats.outliers_influence import variance_inflation_factor","9100efcc":"variables = data_cleaned[['Mileage', 'EngineV', 'Year']]\nvif = pd.DataFrame()","a8b61e40":"vif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]","dc1c6a5e":"vif['Features'] = variables.columns","4e3d550f":"vif","1964dfd4":"data_no_multicollinearity = data_cleaned.drop('Year', axis=1)","7f4c6238":"data_with_dummies = pd.get_dummies(data_no_multicollinearity, drop_first=True)","7a971951":"data_with_dummies.head()","8cba4cfb":"data_with_dummies.columns.values","bfba7d2c":"cols = ['log_price', 'Mileage', 'EngineV', 'Brand_BMW',\n       'Brand_Mercedes-Benz', 'Brand_Mitsubishi', 'Brand_Renault',\n       'Brand_Toyota', 'Brand_Volkswagen', 'Body_hatch', 'Body_other',\n       'Body_sedan', 'Body_vagon', 'Body_van', 'Engine Type_Gas',\n       'Engine Type_Other', 'Engine Type_Petrol', 'Registration_yes']","06e73ff6":"data_preprocessed = data_with_dummies[cols]","f6e76557":"data_preprocessed.head()","65a0117e":"variables = data_preprocessed\nvariables.head()","005c21d4":"vif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif['Features'] = variables.columns.values\nvif","3cf9909a":"variables = data_preprocessed.drop('log_price', axis=1)\nvariables.head()","c44d1a25":"vif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif['Features'] = variables.columns.values\nvif","443c3fbe":"targets = data_preprocessed['log_price']\ninputs = data_preprocessed.drop('log_price', axis=1)","8498beab":"scaler = StandardScaler()\nscaler.fit(inputs)","c807b0e5":"inputs_scaled = scaler.transform(inputs)","45b92016":"x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=42)","03b8b879":"reg = LinearRegression()\nreg.fit(x_train, y_train)","749aea5e":"y_hat = reg.predict(x_train)","8640ce68":"plt.scatter(y_train, y_hat)\nplt.xlabel('Targets (y_train)', size=18)\nplt.ylabel('Predictions (y_hat)', size=18)\nplt.xlim(6, 13)\nplt.ylim(6, 13)\nplt.show()","f5583e41":"sns.distplot(y_train - y_hat)\nplt.title('Residual PDF', size=18)","3469579d":"reg.score(x_train, y_train)","610c6c9b":"reg.intercept_","fec6e9dc":"reg.coef_","b9ee1165":"reg_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])\nreg_summary['Weights'] = reg.coef_","aae8e789":"reg_summary","5e12587f":"y_hat_test = reg.predict(x_test)","1278c65a":"plt.scatter(y_test, y_hat_test, alpha=0.2)\nplt.xlabel('Targets (y_test)', size=18)\nplt.ylabel('Predictions (y_hat_test)', size=18)\nplt.xlim(6, 13)\nplt.ylim(6, 13)\nplt.show()","d101d9cb":"df_pf = pd.DataFrame(np.exp(y_hat_test), columns=['Prediction'])","1523fa86":"df_pf.head()","0a32fdbc":"y_test = y_test.reset_index(drop=True)","6698da46":"df_pf['Target'] = np.exp(y_test)\ndf_pf.head()","c50ae646":"df_pf['Residuals'] = df_pf['Target'] - df_pf['Prediction']","7fd513df":"df_pf.head()","e393312c":"df_pf['Difference%'] = np.absolute(df_pf['Residuals'] \/ df_pf['Target'] * 100)","da4e1ad9":"df_pf.head()","01a863d0":"df_pf.describe()","83fb9011":"df_pf.sort_values(by=['Difference%'])","7bf63adf":"#### Relaxing the assumptions","525b72d9":"#### Exploring PDFs","f770f083":"From above, we can say that error is normally distributed","b6854241":"From above, we can say that Price column is not normally distributed, so we need to remove some outliers from data","4bc5f1ee":"#### Multicollinearity","23ab8ad5":"### Create the regression","14f47709":"### Create Dummy Variables","3c44590b":"Obviously, 'log_price' has a very high VIF. This implies it is most definitely linearly correlated with all the other variables. And this is no surprise! We are using a linear regression to determine 'log_price' given values of the independent variables! This is exactly what we expect - a linear relationship!\n\nHowever, to actually assess multicollinearity for the predictors, we have to drop 'log_price'. The multicollinearity assumption refers only to the idea that the independent variables shoud not be collinear.","52b88465":"### Preprocessing","9a043ab2":"#### Determining the variables of intereset","f71e48db":"#### Scale the data","b8374a0a":"#### Declare the inputs and targets","f0861008":"#### Train Test Split","b86db853":"### Checking the OLS assumptions","7bf8ddd9":"#### Linearity","5263b781":"#### Dealing with missing values","13a22160":"#### Rearrange a bit","a41ed9ea":"Since Year has the highest VIF, I will remove it from the model\n\nThis will drive the VIF of other variables down\n\nSo even if EngineV seems with a high VIF, too, once 'Year' is gone that will no longer be the case","fd9fd4e8":"For no endogeneity (OLS second assumption), we'll have the opportunity to discuss them after regression is created \n\nFor Normality and homoscedasticity (OLS third assumption), normality is assumed for big sample, following central limit theorem, the zero mean of the distribution of errors is accomplished due to inclusion of intercept in the regression, homoscedasticity assumption is generally hold, as we can see  in the above graphs, it is handled due to log transformation of target variable, which is the most common fix for heteroscedasticity\n\nFor No autocorrelation (OLS fourth assumption), as the given data is not time series data, and each row comes from a different customer who is willing to sale their car through the platform. Logically, there is no reason for the observations to be dependent on each other, so we are safe","f8648c9c":"#### Dealing with outliers","50d115ae":"### Testing","455672c1":"### Load the raw data","8947f5f3":"### Linear Regression Model","1ce9a349":"#### Exploring the descriptive statistics of variables","fa565014":"From above plot, we can say that relationship is not linear in any of the case, so for now we cannot apply linear regression, first of all, we have to do some changes in the dataset","a3af86f8":"From above, we can see that Model column is categorical variable and having 312 unique values, which implies, after converting it to dummy, it will add 312 new columns to the dataframe, so we will drop this column","6b941820":"After transformation, we can say that we got linear patterns in almost all plots now","89c3fab3":"#### Finding the weights and bias","5e1195de":"### Importing the relevant libraries"}}