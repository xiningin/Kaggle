{"cell_type":{"a3d3e893":"code","360aeb9e":"code","87cd750c":"code","c223f124":"code","f0dd89fc":"code","20eeec84":"code","42a36857":"code","423794c4":"code","df29c370":"code","8a88f3ea":"code","a94e1ba7":"code","89a21cd9":"code","b2dace38":"code","ad7d7742":"code","b73525bf":"code","68cadbfa":"code","89914be8":"code","699d0c32":"code","b0087fa8":"code","53fce502":"code","a7e7d761":"code","4a899d0b":"code","0c3e91ad":"code","cda488ef":"code","1bc71832":"code","90c2c742":"code","fbdd81fe":"code","a45aeabc":"code","833bbd2d":"code","90ef3c15":"code","a2ccf178":"code","386a8e3c":"code","98e1372c":"code","9ec4c2d7":"code","e9be2f7b":"code","d256f969":"code","94f03f6a":"code","6a2da9f2":"code","3e19708d":"code","af9058e3":"markdown","85693542":"markdown","d0b3b120":"markdown","aef61e2e":"markdown","a30997b2":"markdown","7c4a3a08":"markdown","5d1b702f":"markdown","d4205049":"markdown","4ba4a008":"markdown","ed6e6afd":"markdown","d44148be":"markdown","3c696a1a":"markdown","25036f09":"markdown","21ec156a":"markdown","570f5394":"markdown","563a7457":"markdown","7899aaf6":"markdown","55f63a15":"markdown"},"source":{"a3d3e893":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","360aeb9e":"!ls ..\/input\/","87cd750c":"hist_trans=pd.read_csv('..\/input\/historical_transactions.csv')\nmerchants=pd.read_csv('..\/input\/merchants.csv')\nnew_merchant_transactions=pd.read_csv('..\/input\/new_merchant_transactions.csv')\n","c223f124":"train_df=pd.read_csv('..\/input\/train.csv',parse_dates=[\"first_active_month\"])\ntest_df=pd.read_csv('..\/input\/test.csv',parse_dates=['first_active_month'])\n\nprint('Number of rows and columns : ',train_df.shape)\nprint('Number of rows and columns : ',test_df.shape)","f0dd89fc":"train_df.head()","20eeec84":"target_col = \"target\"\n\nplt.scatter(range(train_df.shape[0]),np.sort(train_df.target.values))\n\nplt.xlabel(\"Index\",fontsize=12)\nplt.ylabel(\"LoyalityScore\",fontsize=12)\n\nplt.show()\n","42a36857":"plt.figure(figsize=(8,8))\nplt.hist(train_df.target.values,bins=50,color='red')\n\nplt.title(\"Histogram of Loyalty score\")\nplt.xlabel('Loyalty score', fontsize=12)\nplt.show()","423794c4":"(train_df.target.values < -30).sum()","df29c370":"cnt_srs = train_df['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in train set\")\nplt.show()\n\ncnt_srs = test_df['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in test set\")\nplt.show()\n\n\n\n","8a88f3ea":"sns.violinplot(x='feature_1',y=train_df.target.values,data=train_df)\nplt.xlabel('Feature 1', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 1 distribution\")\nplt.show()\n\nsns.violinplot(x='feature_2',y=train_df.target.values,data=train_df)\nplt.xlabel('Feature 2', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 2 distribution\")\nplt.show()\n\nsns.violinplot(x='feature_3',y=train_df.target.values,data=train_df)\nplt.xlabel('Feature 3', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 3 distribution\")\nplt.show()\n","a94e1ba7":"#Train Missing Values\ntotal=train_df.isnull().sum().sort_values(ascending = False)\npercentage=(train_df.isnull().sum()\/train_df.isnull().count()*100).sort_values(ascending=False)\n\nmissing_data=pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\nmissing_data.head(10)","89a21cd9":"#Test missing values\ntotal=test_df.isnull().sum().sort_values(ascending = False)\npercentage=(test_df.isnull().sum()\/train_df.isnull().count()*100).sort_values(ascending=False)\n\nmissing_data=pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\nmissing_data.head(10)","b2dace38":"import datetime","ad7d7742":"import datetime\n\nfor df in [train_df,test_df]:\n    df['first_active_month']=pd.to_datetime(df['first_active_month'])\n    df['year']=df['first_active_month'].dt.year\n    df['month']=df['first_active_month'].dt.month\n    \n    df['elapsed_time']=(datetime.date(2018,2,1)-df['first_active_month'].dt.date).dt.days\n\ntarget=train_df[target_col]\ndel train_df['target']\n","b73525bf":"train_df.head()","68cadbfa":"hist_trans.head()\n","89914be8":"print(hist_trans.shape)","699d0c32":"hist_trans['authorized_flag']=hist_trans['authorized_flag'].map({'Y':1,'N':0})\n\n","b0087fa8":"hist_trans.loc[:,'purchase_date']=pd.DatetimeIndex(hist_trans['purchase_date']).astype(np.int64)*1e-9\n\n# aggregate function\nagg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\nagg_histroy=hist_trans.groupby(['card_id']).agg(agg_func)\nagg_histroy.columns=['hist_' + '-'.join(col).strip() for col in agg_histroy.columns.values]\nagg_histroy.reset_index(inplace=True)    ","53fce502":"df=agg_histroy.groupby(['card_id']).size().reset_index(name='hist_transactions_count')\nagg_histroy=pd.merge(df,agg_histroy,on='card_id',how='left')\n\nagg_histroy.head(10)","a7e7d761":"train=pd.merge(train_df,agg_histroy,on='card_id',how='left')\ntest=pd.merge(train_df,agg_histroy,on='card_id',how='left')","4a899d0b":"train.head(10)","0c3e91ad":"new_merchant_transactions.head(5)","cda488ef":"new_merchant_transactions.shape","1bc71832":"new_merchant_transactions['authorized_flag']=new_merchant_transactions['authorized_flag'].map({'Y':1,'N':0})","90c2c742":"# Finding missing values\n\ntotal=new_merchant_transactions.isnull().sum().sort_values(ascending=False)\npercentage=(new_merchant_transactions.isnull().sum()\/new_merchant_transactions.isnull().count() * 100).sort_values(ascending=False)\n\nmissing_data=pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n\nmissing_data","fbdd81fe":"# Making aggregations on New Merchant transactions\n\n# new_merchant_transactions.loc[:,'purchase_date']=pd.DatetimeIndex(new_merchant_transactions['purchase_date']).astype(np.int64)*1e-9\n\n# aggregate function\nnew_agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max']\n        }\nnew_agg_histroy=new_merchant_transactions.groupby(['card_id']).agg(new_agg_func)\n# new_agg_histroy.head(10)\nnew_agg_histroy.columns=['new_' + '-'.join(col1).strip() for col1 in new_agg_histroy.columns.values]\nnew_agg_histroy.reset_index(inplace=True)\nnew_agg_histroy.head(10)\nnew_df=(new_agg_histroy.groupby(['card_id']).size().reset_index(name='new_transactions_count'))\n\nnew_agg_histroy=pd.merge(new_df,new_agg_histroy,on='card_id',how='left')\n","a45aeabc":"new_agg_histroy.head(10)","833bbd2d":"train=pd.merge(train,new_agg_histroy,on='card_id',how='left')\ntest=pd.merge(test,new_agg_histroy,on='card_id',how='left')\n","90ef3c15":"train.shape,test.shape","a2ccf178":"use_col=[col for col in train.columns if col not in ['card_id', 'first_active_month']]","386a8e3c":"use_col=[col for col in train.columns if col not in ['card_id', 'first_active_month']]\n\n# using below train & test data for our prediction\n\ntrain=train[use_col]\ntest=train[use_col]\n\nfeatures=list(train.columns)\ncategorical_features=[featr for featr in features if 'feature_' in featr]\n\nfor category_col in categorical_features:\n    print(category_col,'********',train[category_col].value_counts().shape[0],'categories')\n    ","98e1372c":"from sklearn.preprocessing import LabelEncoder\nfor cat in categorical_features:\n    print(cat)\n    lbe=LabelEncoder()\n    lbe.fit(list(train[cat].values.astype('str')) + list(test[cat].values.astype('str')))\n    train[cat]=lbe.transform(list(train[cat].values.astype('str')))\n    test[cat]=lbe.transform(list(test[cat].values.astype('str')))","9ec4c2d7":"train.shape","e9be2f7b":"df_all=pd.concat([train,test])\ndf_all=pd.get_dummies(df_all,columns=categorical_features)\n# df_all.head(20)\n\n\n","d256f969":"len_train=train.shape[0]\nprint(len_train)\ntrain=df_all[:len_train]\nprint(train.shape)\ntest=df_all[len_train:]\nprint(test.shape)","94f03f6a":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\nlgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 7, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.005, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 50)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","6a2da9f2":"cols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","3e19708d":"sub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = pd.Series(predictions_lgb)\nsub_df.to_csv(\"submission_lgb.csv\", index=False)\n","af9058e3":"**Objective:**\n\nBuild a model which will improve RMSE error in the predictions\n\n","85693542":"Lets understand how target column for given dataset","d0b3b120":"Not much difference in there distribution of traing and testing data.","aef61e2e":"Lets look at how ","a30997b2":"**Simple Exploration: New_merchants**","7c4a3a08":"**Base Model**\n\nkernel : https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you","5d1b702f":"**What am I predicting?**\n\nYou are predicting a loyalty score for each card_id represented in test.csv and sample_submission.csv.","d4205049":"**Simple Exploration: Historical Transactions**","4ba4a008":"We can notice some extreme outlier compare to actual Loyality score, its close to 1% of acutal data . ","ed6e6afd":"**More to Come !!!!**","d44148be":"* card_id : Card identifier\n* month_lag : month lag to reference date\n* purchase_date : Purchase date\n* authorized_flag : Y' if approved, 'N' if denied\n* category_3 : anonymized category\n* installments : number of installments of purchase\n* category_1 : anonymized category\n* merchant_category_id : Merchant category identifier (anonymized )\n* subsector_id : Merchant category group identifier (anonymized )\n* merchant_id : Merchant identifier (anonymized)\n* purchase_amount : Normalized purchase amount\n* city_id : City identifier (anonymized )\n* state_id : State identifier (anonymized )\n* category_2 : anonymized category","3c696a1a":"We have all featured ready , lets go head and try with base model","25036f09":"Label Encoding for categorical features","21ec156a":"* card_id : Card identifier\n* month_lag : month lag to reference date\n* purchase_date : Purchase date\n* authorized_flag : Y' if approved, 'N' if denied\n* category_3 : anonymized category\n* installments : number of installments of purchase\n* category_1 : anonymized category\n* merchant_category_id : Merchant category identifier (anonymized )\n* subsector_id : Merchant category group identifier (anonymized )\n* merchant_id : Merchant identifier (anonymized)\n* purchase_amount : Normalized purchase amount\n* city_id : City identifier (anonymized )\n* state_id : State identifier (anonymized )\n* category_2 : anonymized category","570f5394":"In this Notebook try to explore the data given for **Elo Merchant Category Recommendation** , let us compile about the compitation ","563a7457":"**Elo**\n\nOne of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders.","7899aaf6":"**Data Exploration**","55f63a15":"**Finding missing values from Train and Test**"}}