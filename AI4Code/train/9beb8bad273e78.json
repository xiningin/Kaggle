{"cell_type":{"3817f4ce":"code","1de71a0f":"code","7b1994a1":"code","c4df63ff":"code","b8caad8a":"code","8495f8cb":"code","939b1559":"code","f847071f":"code","7e222a24":"code","1e6af06c":"code","ee24922e":"code","a917d0db":"code","49b24d4b":"code","98b51248":"code","30c88da2":"markdown","181a11e9":"markdown","afe06fe0":"markdown","c3758a4e":"markdown"},"source":{"3817f4ce":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model\nfrom keras.layers import Activation, Input, Conv2D, Add, Flatten, AveragePooling2D, Dense, Dropout, BatchNormalization\nfrom keras.utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nimport h5py","1de71a0f":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nsample = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")","7b1994a1":"train.describe()","c4df63ff":"X = train.drop(\"label\", axis = 1).values.reshape(-1, 28, 28, 1)\ny = train[\"label\"].values\n\ntest = test.values.reshape(-1, 28, 28, 1)\n\ndef normalize(data):\n    return (data\/255)\n\nX = normalize(X)\ntest = normalize(test) ","b8caad8a":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex = True, sharey = True)\naxs = [ax1, ax2, ax3, ax4]\nfor i in range(len(axs)):\n    axs[i].imshow(X[i][:, :, 0])","8495f8cb":"def blocks(x, strides, filters):\n    y = x\n    \n    x = Conv2D(filters, kernel_size = 3, padding = \"same\", strides = strides)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    \n    x = Conv2D(filters, kernel_size = 3, padding = \"same\", strides = (1, 1))(x)\n    x = BatchNormalization()(x)\n    \n    \n    if strides[0] > 1:\n        y = Conv2D(filters, kernel_size = 3, padding = \"same\", strides = strides)(y)\n        y = BatchNormalization()(y)\n        \n    x = Add()([x, y])\n    return x","939b1559":"def residual_net(input_shape, num_classes, filters, blks):\n    inputs = Input(input_shape)\n    x = Conv2D(filters, kernel_size = 7, padding = \"same\", strides = (1, 1))(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    \n    # Adding residual blocks\n    for block in blks:\n        x = blocks(x, strides = (2, 2), filters = filters)\n        for i in range(block - 1):\n            x = blocks(x, strides = (1, 1), filters = filters)\n        filters *= 2\n        \n    x = AveragePooling2D(4)(x)\n    x = Flatten()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(int(filters\/4), activation='relu')(x)\n    outputs = Dense(10, activation='softmax')(x)\n    \n    # Instantiate model\n    model = Model(inputs=inputs, outputs=outputs)\n    return model       \n        ","f847071f":"model = residual_net(\n    input_shape=X[0].shape, \n    num_classes=np.unique(y).shape[-1], \n    filters=64, \n    blks=[2,2,2]\n)\nresnet_architecture = plot_model(model, show_shapes=True, show_layer_names=False)\nresnet_architecture.width = 600\nresnet_architecture","7e222a24":"def train(epochs, batch_size, i):\n    model = residual_net(\n    input_shape=X[0].shape, \n    num_classes=np.unique(y).shape[-1], \n    filters=64, \n    blks=[3,3,3])\n    \n    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n    \n    checkpoint = ModelCheckpoint(\n    filepath=f'resnet-{i}.hdf5',\n    monitor='loss',\n    save_best_only=True\n    )\n    def scheduler(epoch):\n        return 0.001 * 0.8**epoch\n    schedule = LearningRateScheduler(scheduler)\n    callbacks = [checkpoint, schedule]\n    \n    train_datagen = ImageDataGenerator(  \n        rotation_range=10,  \n        zoom_range=0.1, \n        width_shift_range=0.1, \n        height_shift_range=0.1)\n    \n    train_datagen.fit(X)\n\n    # Fit model\n    history = model.fit_generator(\n        train_datagen.flow(X, y, batch_size=batch_size),\n        epochs=epochs, \n        verbose=2, \n        callbacks=callbacks)\n    \n    return history, model\n    \n    \n    ","1e6af06c":"models = []\nall_history = []\nfor i in range(5):\n    print(\"#\"*20)\n    print(f\"Model number {i + 1}\")\n    print(\"#\"*20)\n    \n    history, model = train(\n        epochs=12,\n        batch_size=128, i = i)\n    \n    models.append(model)\n    all_history.append(history)","ee24922e":"preds = []\nfor model in models:\n    preds.append(model.predict(test))\n    ","a917d0db":"f_pred = np.zeros(preds[0].shape)\nfor p in preds:\n    f_pred = f_pred + p\n    \npreds_mean = (f_pred)\/5","49b24d4b":"predictions = np.argmax(preds_mean, axis=1)","98b51248":"result = pd.Series(predictions, name = \"Label\")\nsample[\"Label\"] = result\nsample.set_index(\"ImageId\", inplace = True)\nsample.to_csv(\"submission_resnet.csv\")","30c88da2":"Resnets (Residual Networks) are used when we need to train very deep neural networks. Because we know weights updation during back propagation follows chain rule, if netweek is very deep then there is a chance of vanishing gradient which makes weight updation slow which makes learning very slow. To avoid this we use residual network. Below is the architecture of Resnet.\n\nHere is a [link](https:\/\/en.wikipedia.org\/wiki\/Vanishing_gradient_problem) for Vanishing Gradient","181a11e9":"# Building our own Resnet","afe06fe0":"# Resnet Architecture","c3758a4e":"![](https:\/\/miro.medium.com\/max\/1226\/1*zS2ChIMwAqC5DQbL5yD9iQ.png)"}}