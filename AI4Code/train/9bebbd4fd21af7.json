{"cell_type":{"213d8d9b":"code","6441f201":"code","b3e5cff1":"code","e13827f0":"code","ed80c978":"code","3d673f0b":"code","f6222a8e":"code","17634da1":"code","eb77d2b1":"code","044a836c":"code","241f334f":"code","63e7df0b":"code","b78c0091":"code","f0042491":"code","ebdf4af3":"code","9b9643fb":"code","fae76ae2":"code","68ef86e4":"code","516a491d":"code","6a4e568c":"code","f2515f87":"code","7bd476a0":"code","ff5eeb7d":"code","891cf733":"code","9efb8651":"code","c92ae8bc":"code","49fba25b":"markdown","ba52c121":"markdown","ff3253d0":"markdown","bdce679f":"markdown","de1c9d9b":"markdown","7db80518":"markdown","dca84f0f":"markdown","70188207":"markdown","3c910e0d":"markdown","6c847a1f":"markdown","28de08b9":"markdown","0471a0c2":"markdown","1f25cfd7":"markdown","0f064b21":"markdown","d20dc472":"markdown","b3096f9c":"markdown","2ddb370b":"markdown"},"source":{"213d8d9b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","6441f201":"data = pd.read_csv('..\/input\/mlcourse\/telecom_churn.csv')\ndata.head()","b3e5cff1":"data['International plan'] = data['International plan'].map({'Yes': 1, 'No': 0})\ndata['Voice mail plan'] = data['Voice mail plan'].map({'Yes': 1, 'No': 0})\ndata.head()","e13827f0":"data['Churn'].value_counts(normalize=True)","ed80c978":"# Keep State\nStates = data.pop('State')","3d673f0b":"X, y = data.drop('Churn', axis= 1), data['Churn']","f6222a8e":"X.shape, y.shape","17634da1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state= 17)","eb77d2b1":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","044a836c":"tree = DecisionTreeClassifier(random_state= 17)\ntree.fit(X_train, y_train)\ny_pred = tree.predict(X_test)\nacc = accuracy_score(y_pred, y_test)\nacc","241f334f":"from sklearn.model_selection import cross_val_score, StratifiedKFold","63e7df0b":"skf = StratifiedKFold(n_splits=5, shuffle= True, random_state= 17)","b78c0091":"val_scores = cross_val_score(estimator= tree,  X= X_train, y= y_train, cv= skf)\nval_scores","f0042491":"val_scores.mean()","ebdf4af3":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","9b9643fb":"params = {'max_depth': np.arange(2, 11),\n         'min_samples_leaf': np.arange(2, 11)}\n","fae76ae2":"best_tree = GridSearchCV(estimator= tree, param_grid= params, \n                         cv= skf, verbose= True, n_jobs= -1)","68ef86e4":"%%time\nbest_tree.fit(X_train, y_train)","516a491d":"best_tree.best_params_","6a4e568c":"best_tree.best_score_","f2515f87":"best_y_pred = best_tree.predict(X_test)\nbest_acc = accuracy_score(y_test, best_y_pred)\nbest_acc","7bd476a0":"params = {'max_depth':  np.arange(2, 11), \n         'min_samples_leaf': np.arange(2, 11),\n          'max_features': np.arange(2, 11),\n          'criterion' : ['gini', 'entropy']}","ff5eeb7d":"best_tree = RandomizedSearchCV(estimator= tree, param_distributions=params,\n                              cv= skf, verbose= True, n_jobs= -1)","891cf733":"%%time\n\nbest_tree.fit(X_train, y_train)","9efb8651":"best_tree.best_params_","c92ae8bc":"best_tree.best_score_","49fba25b":"### Grid Search \nWe try all possible combinations of the parameters of interest and find the best ones","ba52c121":"![stritifaied.png](attachment:399e07a3-f289-42ed-8c84-e60fa64c2d8c.png)","ff3253d0":"### Train_Test Split approach.\nIn this approach we `randomly split` the complete data into training and test sets. Then Perform the model training on the training set and use the test set for validation purpose","bdce679f":"## Hyperparameters Tunning","de1c9d9b":"There is always a need to validate the stability of your machine learning model. We can\u2019t fit the model to our training data and hope it would accurately work for the real data it has never seen before. **So we need some kind of assurance that your model has got most of the patterns from the data correct**, and its not picking up too much on the noise.\n\n\n","7db80518":"### Random Search\nRandomizedSearchCV is very useful when we have many parameters to try and the training time is very long, and we will use random values from the range of each parameter.","dca84f0f":"### 3. Stratified K-Fold Cross Validation approach.\n* Stratified K Fold **used when just random shuffling and splitting the data is not sufficient**, and we want to have correct distribution of data in each fold.\n\n* In some cases, there may be a **large imbalance in the response variables**. For example, in dataset concerning price of houses, there might be large number of houses having high price. Or in case of classification, there might be several times more negative samples than positive samples.\n\n* In case of regression problem folds are selected so that the mean response value is approximately equal in all the folds. In case of classification problem folds are selected to have same proportion of class labels.","70188207":"* In cross-validation, we run our modeling process on different subsets of the data `to get multiple measures of model quality`\n* if your dataset is smaller, you should run cross-validation.\n* To tackle overfit using **Cross Validation**","3c910e0d":"### K-Fold Cross Validation approach.\n\n* The data is divided into `k subsets`. Suppose we have divided data into 5 folds i.e. K=5. Now we have 5 sets of data to train and test our model. So the model will get trained and tested 5 times, `but for every iteration we will use one fold as test data and rest all as training data`.\n\n* As a general rule and empirical evidence, `K = 5 or 10` is generally preferred, but nothing\u2019s fixed and it can take any value.\n","6c847a1f":"### Cross-validation & Stratified K-Fold ","28de08b9":"**Because the data was imbalanced, we should use `Stratified K-Fold Cross` to make it a correct distribution**","0471a0c2":"### Model base line","1f25cfd7":"#### Spliting data by `train_test_split` method","0f064b21":"### Code\n","d20dc472":"![kfold.png](attachment:e97cd44e-086d-4ccc-ad12-c4eb88376a7e.png)","b3096f9c":"### Conclusion\nGrid Search is good when we work with a small number of hyperparameters. However, if the number of parameters to consider is particularly high and the magnitudes of influence are imbalanced, the better choice is to use the Random Search.","2ddb370b":"To achieve high performance for these algorithms, you often need to **tune model hyperparameters**. Hyperparameters are the parameters of a model which `are not updated during training and are used to configure the model`.\n\n\nNatively, Scikit-Learn provides two techniques to address hyperparameter tuning: **Grid Search** (GridSearchCV) and **Random Search** (RandomizedSearchCV). Though effective, both techniques are brute-force approaches `to finding the right hyperparameter configurations`, which is an expensive and time-consuming process!\n"}}