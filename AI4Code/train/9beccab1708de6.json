{"cell_type":{"fcabe4a9":"code","2fd2fa81":"code","6ea8fc6b":"code","c7988c5b":"code","77dae971":"code","88c8fb26":"code","787870b1":"code","a5b8a1f7":"code","17478fc9":"code","c2097518":"code","785c217c":"code","03eb4f33":"code","17b5c36f":"code","0438c029":"code","f50e9bd3":"code","3f5f3046":"code","580e4f56":"code","638797cd":"code","ec457850":"code","aa9e67d3":"code","cc57575e":"code","e293b8f5":"code","5f872891":"code","f75b63a8":"code","7dc6efde":"code","91197c6c":"code","0bb2a57c":"code","93271a31":"code","e0b7be97":"code","e735678a":"code","9d16504d":"code","7c7ea2c2":"code","5b2fc7b9":"code","feeaea20":"code","77005a10":"code","90ac50ff":"code","b51ef936":"code","59aaace4":"code","ef64d197":"code","43b4b149":"code","41035570":"code","b9a43951":"code","2bb75d7c":"code","9d72079f":"code","37337ca3":"code","9383ef40":"code","913c3af6":"code","52f6ce23":"code","7b62dfc5":"code","28a406f4":"markdown","f37ac216":"markdown","e6d960f0":"markdown","31c9a479":"markdown","d8a0cf7f":"markdown","8805912d":"markdown","0f46be9b":"markdown","1e3536ec":"markdown","8d152a53":"markdown","5ae09564":"markdown","4184605a":"markdown","cd43d523":"markdown","39dde15d":"markdown","ae54ae37":"markdown","3816f554":"markdown","4914c9dc":"markdown","5f32d067":"markdown","00a3b477":"markdown","11309964":"markdown","398e2627":"markdown","6265c40a":"markdown","f832cec3":"markdown","43f09a88":"markdown","16d9e725":"markdown","8b331958":"markdown","67d3f7e8":"markdown","fdd36be9":"markdown","b1739de0":"markdown","2acc1fc6":"markdown","897e7285":"markdown","7bc0ce09":"markdown","e2b58a78":"markdown","da88a55a":"markdown","221ebbe2":"markdown","df433669":"markdown","d58393e9":"markdown","05959453":"markdown","b557562d":"markdown","7a9a704d":"markdown","085c7932":"markdown","679060d6":"markdown","6d97b0cb":"markdown","e718cdfb":"markdown"},"source":{"fcabe4a9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2fd2fa81":"import matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sb","6ea8fc6b":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain.head()","c7988c5b":"combine = [train,test]","77dae971":"for c in train.columns:\n    print(c, str(100*train[c].isnull().sum()\/len(train)))\n","88c8fb26":"train['Age'] = train['Age'].fillna(train['Age'].mean())","787870b1":"for c in train.columns:\n    print(c, str(100*train[c].isnull().sum()\/len(train)))","a5b8a1f7":"dependencies_sex = train[['Sex', 'Survived']].groupby(['Sex'],as_index=False).mean()\ndependencies_Pclass = train[['Pclass', 'Survived']].groupby(['Pclass'],as_index=False).mean()\n\nprint(dependencies_Pclass)\nprint(dependencies_sex)","17478fc9":"sb.set_style('whitegrid')\nsb.countplot(x='Survived',data=train,palette='YlOrBr')\n","c2097518":"sb.set_style('whitegrid')\nsb.countplot(x='Survived',hue='Sex',data=train,palette='rocket')\n","785c217c":"sb.set_style('whitegrid')\nsb.countplot(x='Survived',hue='Pclass',data=train,palette='icefire')\n","03eb4f33":"g = sb.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)\n","17b5c36f":"for dats in combine:\n    dats['Title'] = dats.Name.str.extract('([A-Za-z]+)\\.',expand=False)","0438c029":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","f50e9bd3":"train.head()","3f5f3046":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","580e4f56":"train.head()","638797cd":"title_dependencies=train[['Title','Survived','Sex']].groupby(['Title','Sex'],as_index=False).mean()\n\ntitle_dependencies","ec457850":"train = train.drop(['Name', 'PassengerId', 'Cabin', 'Embarked','Ticket'], axis=1)\ntest = test.drop(['Name', 'PassengerId', 'Cabin', 'Embarked','Ticket'], axis=1)\n\ncombine=[train,test]\n\nprint(train.head())\n\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0}).astype(int)\n\nprint(train.head())\nprint (test.head())\n","aa9e67d3":"from sklearn.model_selection import train_test_split\nX_train, X_test , Y_train, Y_test = train_test_split(train.drop(['Survived'],axis=1),train['Survived'],test_size=0.10,random_state=None)","cc57575e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","e293b8f5":"modelLR= LogisticRegression(solver='liblinear',C=0.21,random_state=1)\nmodelLR.fit(X_train,Y_train)\n","5f872891":"Y_pred_log=modelLR.predict(X_test)\nacc_LR = metrics.accuracy_score(Y_test, Y_pred_log)","f75b63a8":"print(Y_pred_log)\nprint(\"We see that Logistic regression gives an Accuracy of \",acc_LR*100,\"% on the traing set.\")\n","7dc6efde":"from sklearn.tree import DecisionTreeClassifier","91197c6c":"dtree = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n                       max_features=7, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0,\n                       random_state=None, splitter='best')\ndtree.fit(X_train, Y_train)\ny_pred_tree = dtree.predict(X_test)","0bb2a57c":"acc_DT = metrics.accuracy_score(y_pred_tree, Y_test)\nprint(y_pred_tree)\nprint(\"We see that Decision Tree gives an Accuracy of \",acc_DT*100,\"% on the traing set.\")","93271a31":"from sklearn.ensemble import RandomForestClassifier","e0b7be97":"rforest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features=7, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n            oob_score=False, random_state=42, verbose=0, warm_start=False)\nrforest.fit(X_train, Y_train)\ny_pred_forest = rforest.predict(X_test)","e735678a":"acc_RF = metrics.accuracy_score(y_pred_forest, Y_test)\nprint(y_pred_forest)\nprint(\"We see that Random Forest classifier gives an Accuracy of \",acc_RF*100,\"% on the traing set.\")","9d16504d":"from sklearn.svm import SVC","7c7ea2c2":"svc = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)\nsvc.fit(X_train, Y_train)\ny_pred_svc = svc.predict(X_test)","5b2fc7b9":"acc_SVC = metrics.accuracy_score(y_pred_svc, Y_test)\nprint(y_pred_svc)\nprint(\"We see that SVC classifier gives an Accuracy of \",acc_SVC*100,\"% on the traing set.\")","feeaea20":"from xgboost import XGBClassifier","77005a10":"xgb=XGBClassifier(learning_rate=0.05, n_estimators=500)\nxgb.fit(X_train,Y_train)\ny_pred_xgb=xgb.predict(X_test)","90ac50ff":"acc_XGB= metrics.accuracy_score(y_pred_xgb,Y_test)\nprint(y_pred_xgb)\nprint(\"We see that XGB classifier gives an Accuracy of \",acc_XGB*100,\"% on the traing set.\")                               ","b51ef936":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.metrics import classification_report","59aaace4":"nn  = Sequential()\nnn.add(Dense(units= 14, activation = 'relu', input_dim=7, kernel_initializer=\"uniform\"))\nnn.add(Dense(units= 14, activation = 'relu',kernel_initializer=\"uniform\"))\nnn.add(Dense(units= 1, activation = 'sigmoid',kernel_initializer=\"uniform\"))\nnn.compile(optimizer='adam',\n              loss='mean_squared_error',\n              metrics=['accuracy'])\n","ef64d197":"nn.fit(X_train,Y_train, batch_size=32,epochs=50,verbose= 0)\n\nnn_pred = nn.predict(X_test)\nnn_pred = [ 1 if y>=0.5 else 0 for y in nn_pred]\nprint(nn_pred)\n\nacc_NN = metrics.accuracy_score(Y_test, nn_pred)","43b4b149":"print(\"We can see that the neural network gives an Accuracy of \",acc_NN*100 , \"% on the training set.\")","41035570":"models = pd.DataFrame({\n    'Model': ['Logistic Regression','Decision Tree','Random Forest','Support Vector Machines',\n              'Gradient Boosting Classifier','Artificial Neural Network'],\n    'Score': [acc_LR, acc_DT, acc_RF, \n              acc_SVC, acc_XGB, acc_NN]})\nbest_model=models.sort_values(by='Score', ascending=False)\nprint(best_model)","b9a43951":"best=best_model['Model'].iloc[0]\n\nprint(str(best))","2bb75d7c":"test.head()\nfor c in test.columns:\n    print(c, str(100*test[c].isnull().sum()\/len(test)))\nprint(\"..............Before\")    \n\ntest['Age'] = test['Age'].fillna(test['Age'].mean())\ntest['Fare'] = test['Fare'].fillna(test['Fare'].mean())\nprint(\"\\n\")\nfor c in test.columns:\n    print(c, str(100*test[c].isnull().sum()\/len(test)))\nprint(\"..............After\")","9d72079f":"\nif best == 'Logistic Regression':\n    modelLR.fit(train.drop(['Survived'],axis=1),train['Survived'])\n    test_pred=modelLR.predict(test)\n    print(test_pred)\n    print('Logistic Regression')\n    \nif best == 'Decision Tree':\n    dtree.fit(train.drop(['Survived'],axis=1),train['Survived'])\n    test_pred=modelLR.predict(test)\n    print(test_pred)\n    print('Decision Tree')\n    \nif best =='Random Forest':\n    rforest.fit(train.drop(['Survived'],axis=1),train['Survived'])\n    test_pred=rforest.predict(test)\n    print(test_pred)\n    print('Random Forest')\n    \nif best == 'Support Vector Machine':\n    svc.fit(train.drop(['Survived'],axis=1),train['Survived'])\n    test_pred=svc.predict(test)\n    print(test_pred)\n    print('Support Vector Machines')\n\nif best == 'Gradient Boosting Classifier':\n    xgb.fit(train.drop(['Survived'],axis=1),train['Survived'])\n    test_pred=xgb.predict(test)\n    print(test_pred)\n    print('Gradient Boosting Classifier')\n    \nif best == 'Artificial Neural Network':\n    nn.fit(train.drop(['Survived'],axis=1),train['Survived'], epochs=500 , verbose=0)\n    test_pred=nn.predict(test)\n    test_pred = [ 1 if y>=0.5 else 0 for y in test_pred]\n    print(test_pred)\n    print('Artificial Neural Network')\n    ","37337ca3":"test_data=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ntest_data = test_data.drop(['PassengerId'], axis=1)\n\ntest_data.head()","9383ef40":"test_data.values.tolist()\ntest_acc = metrics.accuracy_score(test_data, test_pred)\n\nprint(\"Here we see that the test data has an Accuracy of \",test_acc*100,\"% \")","913c3af6":"test_predict = pd.DataFrame(test_pred, columns= ['Survived'])\ntest_new= pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nnew_test = pd.concat([test_new, test_predict], axis=1, join='inner')","52f6ce23":"new_test.head()","7b62dfc5":"submit=new_test[['PassengerId','Survived']]\nsubmit.to_csv('predictions.csv',index=False)","28a406f4":"# Applying Artificial Neural Network:","f37ac216":"# Applying Gradient Boosting:","e6d960f0":"* Fetching the best Model","31c9a479":"<img src=\"https:\/\/cdn.britannica.com\/79\/4679-050-BC127236\/Titanic.jpg\">","d8a0cf7f":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/9\/9c\/Titanic_wreck_bow.jpg\/1200px-Titanic_wreck_bow.jpg\">","8805912d":"* Submitting the predictions","0f46be9b":"* Adding the predictions to the Test set","1e3536ec":"* Survival Count based on Age","8d152a53":"# Graphs that support my feature selection based on the movie.","5ae09564":"* Checking the error or missing value percentages of each feature.","4184605a":"# Selecting the final features !!","cd43d523":"* Splitting Titles from Names ","39dde15d":"# Fitting the test set on the Best Classifier:","ae54ae37":"# Playing with data :\n\nAs the title suggests , I have no experience of how things are in Kaggle , thus the \"playing\" is sort of a , playing with borrowed toys.","3816f554":"* Combining the train and test datasets.","4914c9dc":"* Grouping various Titles under Rare Category.","5f32d067":"#                         *The End*                                                   ","00a3b477":"Here , I have replaced the missing values of age with the average of all the ages of the people who boarded the Titanic.\n","11309964":"**Fetching the Survival Data of the test set from the *gender_submission.csv* file .**","398e2627":"***Selecting the Best Model and fitting it to the Test data :***","6265c40a":"According to the movie , the Children and the Women , obviously in the decrasing order of their class , were allowed to board the life boats first. Remember the scene were the crew were shouthing , *\" Women and Children First !!\"*. Thus the selection of the *Sex* , *Age* and the *Pclass* as a valid feature for the prediction. As we have clearly seen that people of high social stature had a cent percent chance of survival , thus the *Title* column is also a valid feature. \n\nNow , the *Parch* and *SibSp* feature , which represents parents and children, and siblings and spouse , is also a valid feature , because parents and children were not to be separated , and siblings and spouse too. Thus , those passengers who were travelling alone were left behind. \n\nThe reason behind dropping , *Cabin* , *Embarked* and *Ticket* feature is , that in the data *Ticket* , there are too many mixed data type , and that doesnt help us figure out the survival of the passengers ; in the *Cabin* data , there were too many missing values , although it might have been useful , because the cabin position had an inportant role in the survival as the lower cabins were filled with water first ; the *Embarked* data has nothing to do with the survival , because the port of embarkation did not determine the fate of the passengers, although , upon speculations one may find patterns in the data. \n\nWhat we can see is , that the prejudice of people of certain social backgrounds determined the survival of the passengers.  ","f832cec3":"* Checking how many people from differnt class and sex had survived.","43f09a88":"# Applying SVC :\n","16d9e725":"# Applying Logistic Regression :","8b331958":"* Here we see how the survival of the passengers depend on the *Sex* and *Title*. ( There are 2 rows with Title as 5 , which represents the Rare category , and it implies that females of high classes were favoured over all )","67d3f7e8":"***Bon Voyage !!***","fdd36be9":"<img src=\"https:\/\/64.media.tumblr.com\/37e6f5cd93269829406ccb5bbccf34e0\/tumblr_pezn7j41oN1qzs7uio4_r1_400.gifv\">","b1739de0":"* Loading the datasets","2acc1fc6":"* Label Encoding Titles. ","897e7285":"Now we can see that under the Column Title , instead of *Mr.* , *Mrs.* , *Miss* , etc. , there are numbers 1-5 , representing the 5 different Titles.","7bc0ce09":"# Getting the best Classifier:","e2b58a78":"The large difference in the numbers shows that these two features - *Pclass* and *Sex* , have influence on the survival of the boarded passengers. ","da88a55a":"Using sequential model to stack two ***14*** unit hidden layer , with ***1*** output layer.","221ebbe2":"We see there are missing values in *Age* , *Cabin* and *Embarked* data. As the values of Cabin are in string , we are going to disregard it. According to the movie and common human sentiments , we will use the age values.","df433669":"* Starting off with the survivor count\n","d58393e9":"# Applying Decision Tree:","05959453":"The new Test set with Survived Column","b557562d":"# Applying Ramndom Forest :","7a9a704d":"Now we can see that there are no missing values in Age column.","085c7932":"* Survivor Count based on Sex ","679060d6":"* Survivor Count based on Class of Passenger","6d97b0cb":"# Introduction :An interpretation of the data with respect to the Movie.\n                                                                 \n                                         \nIn this notebook , the titanic dataset is explored with relevance to the perspectives that were been projected in the famous James Cameron movie Titanic based on the tragic story of the RMS Titanic , in the cold Atlantic Ocean. Now , lets not get deep in the story , which might lead us nowhere near to the solution. But if one may question , how the story has thrown light on this dataset , my answer is , the certain facts that were shown in the movie , which will be supported by the graphs as we go through the notebook ,helped me in Feature selection. \n\n","e718cdfb":"# Splitting Training data "}}