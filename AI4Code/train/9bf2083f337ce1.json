{"cell_type":{"c10a8c47":"code","df4baa78":"code","f63b6ec6":"code","a65b8b7c":"code","2c9794b3":"code","5f98ca35":"code","a7f8908d":"code","13c2e326":"code","3c0b3ec2":"code","570a4d3f":"code","1f560324":"code","fdea4057":"code","c1470d03":"code","92ff23fb":"code","647d6454":"code","00670004":"code","54590d5a":"code","8d92c22a":"code","9c344d7d":"markdown"},"source":{"c10a8c47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df4baa78":"# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Model Selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\n# Linear Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# Non-Linear Models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n# Ensembles\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","f63b6ec6":"#Load training and test data\ntrain=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")","a65b8b7c":"# Create list containing train and test\nboth=[train,test]","2c9794b3":"# View shape of the data\ntrain.shape,test.shape","5f98ca35":"# View first few records\ntrain.head()","a7f8908d":"# View percentage of nulls\nfor df in both:\n    print(df.isnull().sum()\/len(df)*100)","13c2e326":"# PassengerID and Name are considered unnecessary and dropped. \n# Cabin has too many missing values and will be dropped.  \n\n# Create list of numeric features\nnumeric_features = ['Age', 'Fare']\n\n# Create pipeline to impute missing values and scale numeric features\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\n# Create list of categorical features\ncategorical_features = ['Embarked', 'Sex', 'Pclass','SibSp','Parch']\n\n# Create pipeline to impute missing values and encode categorical features\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder',OneHotEncoder(handle_unknown='ignore'))])\n\n# Create pipeline to combine numeric and categorical transformers\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)],sparse_threshold=0)\n","3c0b3ec2":"# Create a list of classifiers\nclf = []\nclf.append(('LR',LogisticRegression(solver='liblinear'))) \nclf.append(('LDA', LinearDiscriminantAnalysis()))\nclf.append(('KNN', KNeighborsClassifier()))\nclf.append(('CART', DecisionTreeClassifier()))\nclf.append(('NB', GaussianNB()))\nclf.append(('SVM', SVC()))","570a4d3f":"# Create feature and target datasets\nX=train.drop(\"Survived\",axis=1)\ny=train.Survived","1f560324":"# Within each iteration of the for loop, the model contains the preprocessing steps... \n# ...along with a different algorithm\n\nresults = []\nnames = []\nfor name, classifier in clf:\n    model = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', classifier)])\n    kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n    cv_results = cross_val_score(model, X, y, cv=kfold)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)  ","fdea4057":"# Compare algorighms\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","c1470d03":"# KNN algorithm tuning\nk_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\nparam_grid = dict(classifier__n_neighbors=k_values)\npipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', KNeighborsClassifier())])\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\ngrid = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=kfold)\ngrid_result = grid.fit(X, y)","92ff23fb":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","647d6454":"# Create a list of ensembles\nens = []\nens.append(('AB',AdaBoostClassifier())) \nens.append(('GB', GradientBoostingClassifier()))\nens.append(('RF', RandomForestClassifier()))\nens.append(('ET',ExtraTreesClassifier()))","00670004":"# Within each iteration of the for loop, the model contains the preprocessing steps... \n# ...along with a different ensemble\n\nresults = []\nnames = []\nfor name, ensemble in ens:\n    model = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('ensemble', ensemble)])\n    kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n    cv_results = cross_val_score(model, X, y, cv=kfold)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)  ","54590d5a":"# Generate predictions using the Random Forest Classifier\nmodel=Pipeline(steps=[('preprocessor', preprocessor),\n                      ('ensemble', RandomForestClassifier())])\nmodel.fit(X,y)\nprediction=model.predict(test)","8d92c22a":"# Create submission as CSV\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": prediction\n    })\n\nsubmission.to_csv('submission.csv', index=False)","9c344d7d":"# Summary\n1. Identify relevant features\n2. Identify numeric and categorical features\n3. Create pipelines to impute missing values, scale numeric features, and encode categorical features\n4. Evaluate the performance of several linear and non-linear classifiers\n5. Tune the best classifier\n6. Evaluate ensembles\n7. Create submission"}}