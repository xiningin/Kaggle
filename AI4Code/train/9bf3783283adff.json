{"cell_type":{"936bdc91":"code","8c65986c":"code","a0e6ef44":"code","76b8cf14":"code","245fdeac":"code","2bb1d14b":"code","20cc1765":"code","69f343e3":"code","584cacd6":"code","744beb67":"code","5d253cfc":"code","09493af9":"code","b5fc265f":"code","5cb8dd54":"markdown","94f773b0":"markdown","fb78ad9b":"markdown","716841ef":"markdown","4f3fc763":"markdown","f86de51e":"markdown","0602dcf8":"markdown","ff092dfb":"markdown","4dd80eea":"markdown","5ac6fe6e":"markdown"},"source":{"936bdc91":"from PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport tensorflow as tf\nimport datetime\nfrom IPython.display import clear_output","8c65986c":"X = []\nY = []\n\nc = 0;\ndata_size = 1000\ndirname = \"..\/input\/mscoco\/mscoco_resized\/train2014\"\nfor filename in tqdm(os.listdir(dirname)):\n    im  = Image.open(os.path.join(dirname, filename))\n    Y.append(np.array(im))\n    im = im.resize((128,128))\n    X.append(np.array(im))\n    \n    c+=1\n    if(c == data_size+20):\n        break\n    \nX = np.array(X, dtype = 'float32')\nY = np.array(Y, dtype = 'float32')\n\nX = (X\/127.5)-1\nY = (Y\/127.5)-1\n\nX_train = X[0:data_size].reshape(-1,1,128,128,3)\nY_train = Y[0:data_size].reshape(-1,1,256,256,3)\n\nX_test = X[data_size:].reshape(-1,1,128,128,3)\nY_test = Y[data_size:].reshape(-1,1,256,256,3)\n\nprint(X_train.shape)\nprint(Y_train.shape)\n\nprint(X_test.shape)\nprint(Y_test.shape)\n","a0e6ef44":"#downsampling block\n#Structure : Conv2D -> BatchNorm -> LeakyReLU\n\ndef downsample(filters, size, apply_batchnorm = True):\n    initializer = tf.random_normal_initializer(0. , 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n    \n    if apply_batchnorm:\n        result.add(tf.keras.layers.BatchNormalization())\n        \n    result.add(tf.keras.layers.LeakyReLU())\n    \n    return result\n\n#upsampling block\n#Structure : Conv2DTranspose -> BatchNorm -> Dropout -> ReLU\n\ndef upsample(filters, size, apply_dropout = True):\n    initializer = tf.random_normal_initializer(0. , 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n    result.add(tf.keras.layers.BatchNormalization())\n    \n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n    \n    result.add(tf.keras.layers.ReLU())\n    \n    return result\n    ","76b8cf14":"def Generator():\n    inputs = tf.keras.layers.Input(shape = [128,128,3])\n    \n    down_stack = [\n        downsample(64, 4, apply_batchnorm = False), #(bs,64,64,64)\n        downsample(128, 4), #(bs,32,32,128)\n        downsample(256, 4), #(bs,16,16,256)\n        downsample(512, 4), #(bs,8,8,512)\n        downsample(512, 4), #(bs,4,4,512)\n        downsample(512, 4), #(bs,2,2,512)\n        downsample(512, 4), #(bs,1,1,512)\n    ] \n    #each downsampling reduces size by 2 because of stride = 2,\n    #bs = batch size, 4th value is number of filters\n    \n    up_stack = [\n        upsample(512, 4, apply_dropout = True), #(bs,2,2,1024)\n        upsample(512, 4, apply_dropout = True), #(bs,4,4,1024)\n        upsample(512, 4, apply_dropout = True), #(bs,8,8,1024)\n        upsample(512, 4), #(bs,16,16,1024)\n        upsample(256, 4), #(bs,32,32,512)\n        upsample(128, 4), #(bs,64,64,256)\n        \n    ]\n    \n    initializer = tf.random_normal_initializer(0. , 0.02)\n    last = tf.keras.layers.Conv2DTranspose(3,4,strides=4,padding='same',kernel_initializer = initializer,activation = 'tanh')#(bs,256,256,3)                                      \n    \n    x = inputs\n    \n    \n    skips = []\n    \n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    \n    skips = reversed(skips[:-1])\n    \n    for up,skip in zip(up_stack, skips):\n        x = up(x)\n        x = tf.keras.layers.Concatenate()([x,skip])\n    \n    x = last(x)\n    \n    return tf.keras.Model(inputs = inputs, outputs = x)","245fdeac":"generator = Generator()\ntf.keras.utils.plot_model(generator, show_shapes = True)","2bb1d14b":"LAMBDA = 500\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n\ndef generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n    \n    l1_loss = tf.reduce_mean(tf.abs(target-gen_output))\n    \n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n    \n    return total_gen_loss","20cc1765":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    inp = tf.keras.layers.Input(shape=[128,128,3], name='input_image')\n    tar = tf.keras.layers.Input(shape=[256,256,3], name='target_image')\n    \n    inp_resized = tf.image.resize(inp, (256,256), method = 'bicubic')\n    \n    x = tf.keras.layers.concatenate([inp_resized,tar]) #(bs,256,256,6)\n    x = downsample(64,4,False)(x) #(bs,128,128,64)\n    x = downsample(128,4)(x) #(bs,64,64,128)\n    x = downsample(256,4)(x) #(bs,32,32,256)\n    \n    x = tf.keras.layers.ZeroPadding2D()(x) #(bs,34,34,256)\n    x = tf.keras.layers.Conv2D(512,4,strides=1,kernel_initializer=initializer,use_bias=False)(x) #(bs,31,31,512)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU()(x)\n    x = tf.keras.layers.ZeroPadding2D()(x) #(bs,33,33,512)\n    x = tf.keras.layers.Conv2D(1,4,strides=1,kernel_initializer=initializer)(x) #(bs,30,30,1)\n    \n    return tf.keras.Model(inputs = [inp,tar], outputs = x)","69f343e3":"discriminator = Discriminator()\ntf.keras.utils.plot_model(discriminator, show_shapes=True)","584cacd6":"def discriminator_loss(disc_real_output, disc_gen_output):\n    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n    \n    gen_loss = loss_object(tf.zeros_like(disc_gen_output), disc_gen_output)\n    \n    return real_loss + gen_loss","744beb67":"generator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)\n\ncheckpoint_dir = \".\/training_checkpoints\"\ncheckpoint_prefix = os.path.join(checkpoint_dir,\"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer,\n                                discriminator_optimizer = discriminator_optimizer,\n                                generator = generator,\n                                discriminator = discriminator)\n","5d253cfc":"def generate_images(model, inp, tar):\n    inp_normalized = (inp\/127.5)-1\n    pred = model(inp_normalized, training = True)\n    pred = (pred+1)*127.5\n    \n    pred = Image.fromarray(tar[0].astype('uint8'),'RGB')\n    pred = pred.resize((200,200))\n    pred = np.array(pred).reshape((1,200,200,3))\n    display_list = [np.array(inp[0], dtype='int'),np.array(pred[0], dtype='int'),np.array(tar[0], dtype='int')]\n    title_list = ['input','prediction','target']\n    plt.figure(figsize = (20,20))\n    \n    for i in range(3):\n        plt.subplot(1,3,i+1)\n        plt.title(title_list[i])\n        plt.imshow(display_list[i])\n        plt.axis('off')\n    \n    plt.show()\n\n\n","09493af9":"EPOCHS = 10\n@tf.function\ndef train_step(inp, tar, epoch):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = generator(inp, training = True)\n        \n        disc_real_output = discriminator([inp,tar], training = True)\n        disc_gen_output = discriminator([inp,gen_output], training = True)\n        \n        gen_loss = generator_loss(disc_gen_output, gen_output, tar)\n        disc_loss = discriminator_loss(disc_real_output, disc_gen_output)\n    \n    gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n\ndef fit(X_train, Y_train, X_test, Y_test, epochs):\n    for epoch in range(epochs):\n        clear_output(wait = True)\n        \n        #generate_images(generator, (X_test[0]+1)*127.5, (Y_test[0]+1)*127.5)\n\n        print(\"Epoch : \", epoch)\n        \n        for inp,tar in tqdm(zip(X_train,Y_train)):\n            train_step(inp,tar,epoch)\n        \n        if (epoch+1)%10 == 0:\n            checkpoint.save(file_prefix=checkpoint_prefix)\n        \n    checkpoint.save(file_prefix=checkpoint_prefix)\n\nfit(X_train, Y_train, X_test, Y_test, EPOCHS)","b5fc265f":"generate_images(generator, (X_test[1]+1)*127.5, (Y_test[1]+1)*127.5)","5cb8dd54":"**CREATING GENERATOR MODEL**\n\n* GAN uses two models, a generator model that generates output and a discriminator model that classifies wether the output is generated by the generator model(fake) or taken from the dataset(real)\n\n* The generator model is a U-Net. It is a neural network used for image to image tasks. It has three major components : downsampling blocks, upsampling blocks and skip connections.\n\n* Downsampling blocks convert image input to tensors of lower dimesions until it becomes a 1D tensor. Upsampling blocks convert output of downsampling blocks back to image output. Skip connections provide connections between downsampling and upsampling blocks at each level.\n\n* Generator and discriminator compete against each other.","94f773b0":"**CREATING DISCRIMINATOR MODEL**\n\n* Discriminator model is a PatchGAN\n* In a PatchGAN, the output is a 3D vector referring to similarity between patches of input and target images\n* Model consists of downsampling blocks : Conv->BatchNorm->LeakyReLU\n* It receives two inputs : Input image and generated image which is classified as fake and input image and target image which is classified as real\n","fb78ad9b":"**OPTIMIZERS AND CHECKPOINT SAVER**","716841ef":"**FUNCTION TO GENERATE IMAGES**","4f3fc763":"**LOADING DATASET**\n\n* using 256x256 images from MSCOCO dataset\n* X is array containing input images generated by resizing original images to 128x128 size\n* Y is array containing original images of 256x256 size\n* Only 1000 images are used out of 80K images for faster computation","f86de51e":"**IMPORTING LIBRARIES AND PACKAGES**","0602dcf8":"**DISCRIMINATOR LOSS**\n\n* It takes two inputs : discriminator output for real images and generated images and it has two components : real loss and generated loss\n* real loss is sigmoid cross entropy loss of real image output and array of ones\n* generated loss is sigmoid cross entropy loss of generated image output and array of zeros\n* total loss is sum of real loss and generated loss","ff092dfb":"**GENERATOR LOSS**\n\n* Generator loss consists of two components\n* L1 loss which is mean absolute error between the generated image and target image to make generated images structurally similar to target images\n* GAN loss which is binary crossentropy loss of discriminator's output on generated images and array of ones.\n* Total loss = GAN loss + (LAMBDA * L1 loss)","4dd80eea":"**TRAINING**","5ac6fe6e":"**Using GAN to upscale images from 128x128 to 256x256**\n\nreferences : \n\nhttps:\/\/www.tensorflow.org\/tutorials\/generative\/pix2pix\n\nhttps:\/\/arxiv.org\/abs\/1609.04802"}}