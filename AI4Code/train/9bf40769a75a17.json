{"cell_type":{"a432b1dc":"code","02000272":"code","0bdd9721":"code","36926912":"code","ea00aee4":"code","47bb3a6a":"code","583f2fa0":"code","b7e2f8b1":"code","0516d1ba":"code","be5516fb":"code","cf24d854":"code","f90fbf18":"code","15637cd5":"code","f20e3f40":"code","80d74697":"code","acbe8726":"code","a9f5b585":"code","b6117ecd":"code","4eb75f3e":"code","ed0471c0":"code","effa28a2":"code","c4f078a4":"code","cb603b9e":"code","c6996347":"code","af11c64e":"code","81caf3d5":"code","30d7ebe1":"code","956c53f4":"code","4381a2ce":"code","87b65211":"code","a7a81dcf":"code","11fb5f14":"code","25a5ff64":"code","9ddf8c56":"code","0f512bbc":"code","c646822f":"code","b85fa10a":"code","7d87f530":"code","46291ebc":"code","f3c99d04":"code","f46d8083":"code","7b09af6c":"code","467a5467":"code","59906273":"code","590f166e":"code","7a7beca6":"code","790456cc":"code","7f3fe6ee":"code","57ee227b":"code","90a7040b":"code","a440b3c5":"code","c1b7aafc":"code","f3f93319":"code","1cee60d4":"code","02bf722f":"code","bab722df":"code","d793a632":"code","2b4d7d96":"code","5015ef4f":"code","b6d939db":"code","0d8b8778":"code","fb06df6a":"markdown","73a69310":"markdown","1713fdde":"markdown","4f8c847b":"markdown","2028bab4":"markdown","e366b32b":"markdown","f194a1a0":"markdown","1c2f7e71":"markdown","d932ff83":"markdown","1d46d415":"markdown","0e3a1cdd":"markdown","26678c88":"markdown","a519ed07":"markdown","2bdd1aa1":"markdown","c3c5c167":"markdown","d3647c8a":"markdown","57ffa309":"markdown"},"source":{"a432b1dc":"import numpy as np #scientific computing \nimport pandas as pd #data processing\n\n#display max width and depth of the data\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","02000272":"raw_csv_data = pd.read_csv('..\/input\/absenteeism-at-work-data-set\/Absenteeism_data.csv')","0bdd9721":"raw_csv_data.head()","36926912":"raw_csv_data.shape","ea00aee4":"# always make a copy of your data, do not ever overwrite the original data\ndf = raw_csv_data.copy()","47bb3a6a":"df.head() ","583f2fa0":"# summeray info about the dataframe\ndf.info()","b7e2f8b1":"#drop ID\ndf = df.drop(\"ID\", axis = 1)","0516d1ba":"df.head()","be5516fb":"# num of duplicated rows\ndf.duplicated().sum()","cf24d854":"# display of duplicates \ndf[df.duplicated() == True]","f90fbf18":"raw_csv_data.duplicated().sum()","15637cd5":"raw_csv_data[raw_csv_data.duplicated() == True]","f20e3f40":"raw_csv_data[(raw_csv_data.index == 126)|(raw_csv_data.index == 127)]","80d74697":"# Unique values of this feature  \n# Each number reprsent a reason that you can find the data description\ndf['Reason for Absence'].unique()","acbe8726":"df['Reason for Absence'].nunique()","a9f5b585":"df['Reason for Absence'].value_counts()","b6117ecd":"# Converting it to dummy variables \nreason_columns = pd.get_dummies(df['Reason for Absence'])","4eb75f3e":"reason_columns.head()","ed0471c0":"# check that the conversion is free of mistakes\nreason_columns['check'] = reason_columns.sum(axis = 1)","effa28a2":"reason_columns.head()","c4f078a4":"# the output should be 1, otherwise we have a mistake\nreason_columns['check'].unique()","cb603b9e":"reason_columns = pd.get_dummies(df['Reason for Absence'], drop_first = True)","c6996347":"reason_columns.head()","af11c64e":"# drop the original variable\ndf = df.drop(['Reason for Absence'], axis = 1)","81caf3d5":"reason_columns.head()","30d7ebe1":"# groupthe values based on the reson of the abscense\nreason_columns.loc[:, 1:14].max(axis = 1).head()","956c53f4":"# groupthe values based on the reson of the abscense (see the data discription)\nreason_type_1 = reason_columns.loc[:, 1:14].max(axis = 1) \nreason_type_2 = reason_columns.loc[:, 15:17].max(axis = 1) \nreason_type_3 = reason_columns.loc[:, 18:21].max(axis = 1)\nreason_type_4 = reason_columns.loc[:, 22:].max(axis = 1)","4381a2ce":"# add the new features to our dataframe\ndf = pd.concat([df, reason_type_1, reason_type_2, reason_type_3, reason_type_4], axis = 1)\ndf.head()","87b65211":"df.columns.values","a7a81dcf":"# Change the names of the last 4 features \ncol_names = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',\n       'Daily Work Load Average', 'Body Mass Index', 'Education',\n       'Children', 'Pets', 'Absenteeism Time in Hours', 'Reason_1', 'Reason_2', 'Reason_3', 'Reason_4']","11fb5f14":"# Change the names of the last 4 features \ndf.columns = col_names","25a5ff64":"#See the result\ndf.columns.values","9ddf8c56":"# re-arrange the order of the features\n# we want the reasons in the begining again\ncol_names_ordered = [ 'Reason_1',\n       'Reason_2', 'Reason_3', 'Reason_4', 'Date', 'Transportation Expense', 'Distance to Work', 'Age',\n       'Daily Work Load Average', 'Body Mass Index', 'Education',\n       'Children', 'Pets', 'Absenteeism Time in Hours']","0f512bbc":"# re-arrange the order of the features\n# we want the reasons in the begining again\ndf = df[col_names_ordered]","c646822f":"df.head()","b85fa10a":"# make a copy of the dataframe at this stage\n# if any errors happened, we can start from here\n# we do not need to rerun the code from the beginning   \ndf_reason_mod = df.copy()","7d87f530":"df_reason_mod.head()","46291ebc":"# Date type\ntype(df_reason_mod[\"Date\"])","f3c99d04":"# Date type\ntype(df_reason_mod[\"Date\"][0])","f46d8083":"# convert it to a date-time\ndf_reason_mod[\"Date\"] = pd.to_datetime(df_reason_mod[\"Date\"], format = '%d\/%m\/%Y') \n# it you do not correctly specify the format it confuse the day with the month","7b09af6c":"# Date type\ntype(df_reason_mod[\"Date\"][0])","467a5467":"# display the first date\ndf_reason_mod[\"Date\"][0]","59906273":"# extracting the month out of the date \ndf_reason_mod[\"Date\"][0].month","590f166e":"# create an impty list to store the months in\nlist_months = []","7a7beca6":"# loop through the dataframe and extract the month out of the date \nfor i in range(df_reason_mod.shape[0]):\n    list_months.append(df_reason_mod[\"Date\"][i].month)","790456cc":"# add it to the dataframe\ndf_reason_mod['month_value'] = list_months","7f3fe6ee":"df_reason_mod.head()","57ee227b":"# another way\n#df_reason_mod['month_value_another_way'] = df_reason_mod[\"Date\"].apply(lambda x: x.month)\n#np.sum(df_reason_mod['month_value_another_way'] != df_reason_mod['month_value'])","90a7040b":"df_reason_mod[\"Date\"][1]","a440b3c5":"# extract weekday\ndf_reason_mod[\"Date\"][1].weekday()","c1b7aafc":"# fuction to extract weekday\ndef date_to_weekday(date_value):\n    return date_value.weekday()","f3f93319":"# apply the function to the date columns and store the result into a new feature\ndf_reason_mod['day_of_week'] = df_reason_mod[\"Date\"].apply(date_to_weekday)","1cee60d4":"df_reason_mod.head()","02bf722f":"# drop the Date, we do not need it any more\ndf_reason_mod = df_reason_mod.drop(['Date'], axis = 1)","bab722df":"# value counts\ndf_reason_mod['Education'].value_counts()","d793a632":"# regrouping \ndf_reason_mod['Education'] = df_reason_mod['Education'].map({1:0, 2:1, 3:1, 4:1})","2b4d7d96":"df_reason_mod['Education'].value_counts()","5015ef4f":"df_processed = df_reason_mod.copy()","b6d939db":"df_processed.head()","0d8b8778":"df_processed.to_csv(\"processed.csv\")","fb06df6a":"We finished the data cleaning stage, we will now start the machine learning part of the project. We will do that in a [part 2](https:\/\/www.kaggle.com\/ahmedmohameddawoud\/absenteeism-analysis-part-2-machine-learning?scriptVersionId=77820916)","73a69310":"## Copy of the original data","1713fdde":"## Basic quality checks","4f8c847b":"## Reason for Absence","2028bab4":"Before dropping these rows lets check if the original dataframe (before dropping ID) has duplicates or not ","e366b32b":"It seems that these data is not duplicates, they represent the same employee being abscent for the same reson by in different days. We will not drop them.","f194a1a0":"## Create Check Points","1c2f7e71":"## Required Packages ","d932ff83":"## Month","1d46d415":"- There is no missing values \n- ID has no value for our analysis, therefore it will be dropped\n- Reason for Absence and Date are in an incorrect data type, needs to be fixed\n- Reason for Absence and Date also needs to be featured to extract the most useful info out of them\n","0e3a1cdd":"## Any Duplicates?","26678c88":"## Read the data","a519ed07":"## Education ","2bdd1aa1":"### Check point ","c3c5c167":"This notebook is part of a bigger project. The project is consisted of 3 stages:\n\n1. Data cleaning in python\n2. Model building, evaluation and prediction\n3. Storing the cleaned data in SQL database\n4. Connceting the database to tableau and creating a detailed visual analysis\n\nIn the data cleaning stage we will process the data to make sure that:\n- Data types are consistent\n- Feature engineering (creating new feature from existing ones)","d3647c8a":"## Date","57ffa309":"## Day of the week"}}