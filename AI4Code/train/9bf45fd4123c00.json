{"cell_type":{"b1d4c6cc":"code","cf7caf6c":"code","707a13bc":"code","8c00978c":"code","b9e94b64":"code","b2f35f88":"code","31b2c3ae":"code","ee319b19":"code","b6354d26":"code","0be3df45":"code","1787e019":"code","75e21c63":"code","247c13a1":"code","8fbfb561":"code","945d832f":"code","bf7f37b0":"code","14936773":"code","805f4c51":"code","c477e1a6":"code","377cbe55":"markdown","7256fede":"markdown","91cb76ac":"markdown","d7970aff":"markdown","97396259":"markdown","7a26c6b0":"markdown","5d3c5b2c":"markdown","473d60c5":"markdown","2c7486f2":"markdown","878ea23d":"markdown","dd167e3e":"markdown","2a5383d5":"markdown","372ffbed":"markdown","c808e2f4":"markdown","57b18bce":"markdown","3a132f35":"markdown","a9e45b86":"markdown","a17c8a04":"markdown","ff2516f5":"markdown","ddd3f505":"markdown","c88b8eb1":"markdown","45239aa5":"markdown","a1f614d3":"markdown","f2862f83":"markdown"},"source":{"b1d4c6cc":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder\nfrom datetime import datetime","cf7caf6c":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","707a13bc":"categorical = len(train.select_dtypes(include=['object']).columns)\nnumerical = len(train.select_dtypes(include=['int64','float64']).columns)\nprint('Total number of variables= ', categorical, 'Categorical', '+',\n      numerical, 'Numerical', '=>', categorical+numerical, 'variables')\ntrain.head(10)","8c00978c":"train.describe()","b9e94b64":"sns.barplot(x=train['Sex'], y=train['Survived']) \n\n\nsns.catplot(x=\"Age\", y=\"Sex\", \n            hue=\"Survived\", kind=\"violin\", figsize=(25, 25),\n            split=True, inner=\"stick\",\n            palette={0: \"b\", 1: \"g\"},\n            bw=.15, cut=0,\n            data=train)\n","b2f35f88":"#The graphs above contain some useful information. First, men are much more likely to die than women. Second, young men are more likely to survive. To be specific, chances of survival for men under the age of 15 are very high, but not for women. Third, men between the ages of 18 and 30 are more likely to die. Fourth, age in general does not appear to have a direct impact on women's survival. These results show that the principle of priority for women and children in war and disaster was applied to the Titanic.\n\ncols = ['Embarked', 'Pclass', 'SibSp', 'Parch']\nn_rows = 2\nn_cols = 2\n\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*7,n_rows*7))\n\nfor row in range(0,n_rows):\n    for column in range(0,n_cols):  \n        i = row*n_cols+ column       \n        ax = axs[row][column]\n        sns.countplot(train[cols[i]], hue=train[\"Survived\"], ax=ax)\n        ax.set_title(cols[i])\n        ax.legend(title=\"Survived\", loc='upper right') \n        \nplt.tight_layout() ","31b2c3ae":"grid = sns.catplot(x=\"Fare\", y=\"Survived\", row=\"Pclass\",\n                kind=\"box\", orient=\"h\", height=1.5, aspect=4,\n                data=train.query(\"Fare > 0\"))","ee319b19":"sns.heatmap(train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","b6354d26":"comb = train.append(test)\ncomb.shape\n\ncomb.isnull().sum()\/comb.isnull().count()*100 ","0be3df45":"comb['IsFemale'] = np.where(comb['Sex']=='female', 1, 0 )\nsns.barplot(x='IsFemale', y='Survived', data=comb) ","1787e019":"Titles = set()\nfor name in comb['Name']:\n    Titles.add(name.split(',')[1].split('.')[0].strip())\nDict = {\"Capt\": \"Special\", \"Col\": \"Special\", \"Major\": \"Special\", \"Jonkheer\": \"Special\",\"Don\": \"Special\",\"Dona\": \"Special\",\n    \"Sir\" : \"Special\", \"Dr\": \"Special\", \"Rev\": \"Special\", \"the Countess\":\"Special\", \"Mme\": \"Mrs\", \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\", \"Mr\" : \"Mr\", \"Mrs\" : \"Mrs\", \"Miss\" : \"Miss\", \"Master\" : \"Master\", \"Lady\" : \"Special\"}\ndef Titles():\n    comb['Title'] = comb['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    comb['Title'] = comb.Title.map(Dict)\n    return comb\ncomb = Titles()\n\nsns.barplot(x='Title', y='Survived', data=comb) \ncomb['IsMr'] = np.where(comb['Title']=='Mr', 1,0)\nsns.barplot(x='IsMr', y='Survived', data=comb) ","75e21c63":"#Extract ticket type from Ticket\nimport string\nTicketType = []\nfor i in range(len(comb.Ticket)):\n    ticket = comb.Ticket.iloc[i]\n    for c in string.punctuation:\n                ticket = ticket.replace(c,\"\")\n                splited_ticket = ticket.split(\" \")   \n    if len(splited_ticket) == 1:\n                TicketType.append('NO')\n    else: \n                TicketType.append(splited_ticket[0])\ncomb['TicketType'] = TicketType\ncomb['TicketType'] = np.where((comb.TicketType!='NO') & (comb.TicketType!='PC') & \n                                (comb.TicketType!='CA') & (comb.TicketType!='A5') & \n                                (comb.TicketType!='SOTONOQ'),'OT',comb.TicketType)\n\n\n#Extract Crew from Ticket.\ncomb['Crew'] = np.where(comb.Fare==0, 1, 0)\ncomb.Crew.value_counts()\n\n#sns.catplot(x='Crew', y='Survived', kind='bar', data=comb)\n#Extract LastName from Name\ncomb['LastName'] = comb['Name'].str.extract('([A-Za-z]+),', expand=False)\n\n#Create a new variable by cencatinatinig TicketType, Embarked, LastName \ncomb['Group'] = comb['TicketType'].astype(str) + comb['Embarked'].astype(str) + comb['LastName'].astype(str) \nle = LabelEncoder()\ncomb['GroupID']  = le.fit_transform(comb['Group'])","247c13a1":"comb['TicketFreq'] = comb.groupby('Ticket')['Ticket'].transform('count')","8fbfb561":"cols = ['IsMr', 'IsFemale', 'Pclass', 'TicketFreq']\n\nn_rows = 2\nn_cols = 2\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*7,n_rows*7))\n\nfor row in range(0,n_rows):\n    for column in range(0,n_cols):  \n        i = row*n_cols+ column      \n        ax = axs[row][column] \n        sns.countplot(comb[cols[i]], hue=comb[\"Survived\"], ax=ax)\n        ax.set_title(cols[i])\n        ax.legend(title=\"Survived\", loc='upper right') \n        \nplt.tight_layout() ","945d832f":"# Drop unwanted features\ncomb.drop(['Ticket', 'Sex', 'Title', 'Name', 'Fare', 'Age', 'TicketType', 'LastName', 'Parch', 'SibSp', 'Crew', \n           'Cabin', 'Embarked', 'Group'], axis=1, inplace=True) \ncomb.head(5)","bf7f37b0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score, cross_val_predict\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score, confusion_matrix\n\n#Before we run the models we need to split the combined dataset into training and testing datasets.\nn=len(train)\ntrain_df = comb.iloc[:n] \ntest_df = comb.iloc[n:]\n\ny_train = train_df['Survived'].astype(int)\nX_train = train_df.drop(['Survived', 'PassengerId'], axis=1) \nX_test = test_df.drop(['Survived', 'PassengerId'], axis=1)","14936773":"\ndef stats(models, X_train, y_train):\n    stats = {}\n    for name, inst in models.items():\n        mscores = []\n        model_pipe = make_pipeline(StandardScaler(), inst)\n        model_pipe.fit(X_train, y_train)\n        acc=round(model_pipe.score(X_train, y_train)* 100, 2)\n        mscores.append(acc)\n        scores = cross_val_score(model_pipe, X_train, y_train, cv=10, scoring='accuracy')\n        acccv=round(scores.mean()* 100, 2)\n        mscores.append(acccv)\n        mscores.append(scores.std())\n        y_train_cv_pred = cross_val_predict(model_pipe, X_train, y_train, cv=10)\n        p = round(precision_score(y_train, y_train_cv_pred)* 100, 2)\n        s = round(recall_score(y_train, y_train_cv_pred)* 100, 2)\n        mscores.append(p)\n        mscores.append(s)\n        f1 = 2*((p*s)\/(p+s))\n        mscores.append(f1)\n        roc_auc_cvs = round(cross_val_score(model_pipe, X_train, y_train, cv=10, scoring='roc_auc').mean()* 100, 2)\n        mscores.append(roc_auc_cvs)\n        stats[name] = mscores\n    colnames = ['Accuracy','AccuracyCv','StandardDeviation', 'Precision','Sensitivity','F1Score', 'RocAucCv']\n    df_ms = pd.DataFrame.from_dict(stats, orient='index', columns=colnames)\n    df_msr = df_ms.sort_values(by='RocAucCv', ascending=False)\n    return df_msr\n\nmodels = {'AdaBoostClassifier': AdaBoostClassifier(),\n          'GradientBoostingClassifier': GradientBoostingClassifier(),\n          'LogisticRegression': LogisticRegression(),\n          'SupportVectorMachines': SVC(random_state=0),\n          'RandomForestClassifier': RandomForestClassifier(),\n          'KNeighborsClassifier': KNeighborsClassifier(),\n          'DecisionTreeClassifier': DecisionTreeClassifier()}\ndf_all_models = stats(models, X_train,y_train)\n\ndf_all_models","805f4c51":"vote = VotingClassifier(estimators=[\n    ('AdaBoostClassifier', AdaBoostClassifier()),\n    ('GradientBoostingClassifier', GradientBoostingClassifier()),\n    ('LogisticRegression', LogisticRegression()),\n    ('SupportVectorMachines', SVC()),\n    ('RandomForest', RandomForestClassifier()),\n    ('KNN', KNeighborsClassifier())\n],\n          voting='hard', n_jobs=15)\nvote.fit(X_train,y_train)\nprediction_vote=vote.predict(X_test)\nprint ('Voting Classifier Accuracy Score = {:.2f}%'.format(round(vote.score(X_train,y_train, sample_weight=None)* 100, 2)), 'on', datetime.now())","c477e1a6":"output= pd.DataFrame (pd.DataFrame({\n    \"PassengerId\": test_df[\"PassengerId\"],\n    \"Survived\": prediction_vote}))\noutput.head()\noutput.to_csv('FinalSubmission.csv', index=False)","377cbe55":"# **Ticket Frequency**","7256fede":"We can see a couple of things in the table above. First, many features are not numeric, so we want to change those features to numeric later. Second, we can see that the features have a wide range of levels that we need to change to the same scale. There are also some features that include missing values (NAN).\n\nThe following table describes the numerical features of the training dataset. Generally, 38% of the passengers survived on the Titanic. We also found that the average age of the passengers was around 30 years and ranged from 0.4 to 80 years. We can also see that the fare for passengers is very different, that is, the minimum is 79 and the maximum is 512. The count variable indicates that some features are missing values and we want to fix them. Let's take a look at the relationship between each variable and Survival, and how they relate to one another. \n","91cb76ac":"Before we start cleaning the data, let us see the correlation among continious variables. This information is so vital for missng value imputation. \n\nFrom the below graph we can observe that that all features are correlated with Survived. However, the correlation between Pclass and Survived is the highest, -0.34. Since all other features are correlated with Pclass it would be fare to assume that they would not add much value. So, it is better to drop SibSp, Parch, Fare and Age.\n","d7970aff":"The Titanic was on a ship bound for New York City on April 15, 1912, when it collided with an iceberg and caused many deaths. An estimated 2,224 passengers and crew were on board. More than 1,500 of them died. When the Titanic entered service, it was the largest ship. \n\nThe main purpose of this notebook is to show how machine learning models can help us predict passenger survival. You can use this notebook to learn the basics of machine learning in Python.","97396259":"To apply the same changes on both the training and testing datasets at the same time, we shall combine them together.","7a26c6b0":"From the graphs above, it can be seen that Pclass plays a role in determining the chances of survival. Overall, the higher the Pclass, the lower the chance of survival. For male passengers on P2 and P3, however, their chances of survival seem to be the same. In addition, the ticket price correlates with the Pclass. It seems that the port of entry also has something to do with survival. People who embarked in Port C have a better chance of survival. This pattern applies to both sexes. But, if you think about it, it doesn't make sense, right? How could where you board on the ship affect your servival? I think, Pclass can may have played a mediating role here. As can be seen above, the impact of Port of entery on survival depends on Pclass. Therefore we will consider removing the \"Embarked\" attribute. \n\nNext, we'll see how Sibps (i.e. the number of siblings or spouses on board the Titanic) and Parch (the number of parents or children on board the Titanic) are related to the chances of survival. That said, Sibps and Parch are features that indicate that a person has relatives on the Titanic. But, there reports of inconsistencies on these variables. \n","5d3c5b2c":"In this part we will create a statistical model based on the features developed in the previous part. Since the target variable is binary in nature, we will use classification modals. There is a wide variety of models to choose from. We are going to use 7 of them i.e. Logistic Regression, Decision Trees, Random Forests, Gradient Boost, AdaBoost, Support Vector Machines and KNeighbors.","473d60c5":"# **Final Data Visualisation**","2c7486f2":"From our previous analysis, we know that men are less likely to survive. The above graph adds some additional information. Well, not all men are not equally affected. Those with the title Mr are highly likely to perish. It is wise to create a new feature called IsMr. ","878ea23d":"The following function helps us evaluate our models by returning a data frame that contains useful information based on a 10-fold cross-validation.","dd167e3e":"People not only travel with close family members such as siblings and spouses, but also with distant relatives, friends and nannies. These people are not considered a family, but they use the same ticket number. Creating a new group function based on Parch and SibSp may be an option, but I didn't choose to follow this path.\n\nI've decided to create a group as follows I created a new feature that combines the ticket type, embarkation port, and family name. That is, people with the same family name, the same passengers and the same type of ticket are grouped.","2a5383d5":"# **Introduction**","372ffbed":"# **Model Building**","c808e2f4":"# **Group Formation**","57b18bce":"# **Data Exploration**","3a132f35":"Let us get the training and testing datasets. ","a9e45b86":"Decision Forest and Random Forest seem to get high accuracy (99.66%), but when it comes to cross-validation accuracy, they perform poorly.\n\n#Instead of choosing one single model, we opt for VotingClassifier. VotingClassifier is a useful method when we have multiple models with different cross-validation scores. It basically combines different machine learning models and applies majority decision (i.e. hard vote) or average predicted probabilities (i.e. soft vote) for the prediction. In our case, we will use a hard vote where the majority is selected. By using hard vote VotingClassifier we will compensate for the weaknesses of some models. ","a17c8a04":"# **Import Libraries**","ff2516f5":"# **IsFemale**","ddd3f505":"Now in order to study the structure of our data set, let us perform some data exploration.","c88b8eb1":"# **Submission**","45239aa5":"Ticket Frequency is a ticket-based feature that includes people who have the same ticket number. This feature can serve as group size as it puts people who travel with the same ticket number together, whether they are related or not. ","a1f614d3":"# **IsMr**","f2862f83":"# **Feature Engneering**"}}