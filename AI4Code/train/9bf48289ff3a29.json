{"cell_type":{"575c2913":"code","c0cac403":"code","5b9162ec":"code","67e309b6":"code","04f6ca88":"code","6e679c3c":"code","1f918a1b":"code","3b971009":"code","e9bdd2d5":"code","7882c9ed":"code","3ea59dd5":"code","b78bde0f":"code","38121b6c":"code","07b43259":"code","c927d599":"code","95b2bc29":"code","1b670166":"code","d5c90572":"code","377b61d4":"code","3d4a59b0":"code","41c01bad":"code","d1ed1a08":"code","26bbb720":"code","c67ba930":"code","cba49efa":"code","e04de096":"code","e815c9ac":"code","58afe642":"code","b07b6386":"code","7a9f176f":"code","efcc48c7":"markdown","db49b463":"markdown","0f1077bb":"markdown","5187ef9b":"markdown","5c5eb47c":"markdown","6de232ac":"markdown","5d6422eb":"markdown"},"source":{"575c2913":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0cac403":"import matplotlib.pyplot as plt\nimport seaborn as sns","5b9162ec":"train = pd.read_csv('\/kaggle\/input\/fake-news\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/fake-news\/test.csv')\n","67e309b6":"train.head()","04f6ca88":"test.head()","6e679c3c":"print(train.shape)\nprint(test.shape)","1f918a1b":"#creating a dataframe and storing the null values in it\n\nnull_vals = pd.DataFrame(columns= {'train_data','test_data'})\nnull_vals['train'] = train.isnull().sum()\nnull_vals['test'] = test.isnull().sum()\n\nprint(null_vals)","3b971009":"#filling the nan values with empty strings from our dataframe if nan exists\ntrain = train.fillna('missing')\ntest = test.fillna('missing')\n\nprint(train.shape)\nprint(test.shape)\n","e9bdd2d5":"target = train['label']\n\nsns.set_style('whitegrid')\nsns.countplot(target)","7882c9ed":"# train dataframe authors\nl = train['author'].to_list()\nprint('there is', str(len(set(l))),'different authors')\n","3ea59dd5":"train['author'].value_counts().head(10)","b78bde0f":"#train dataset\n\n# Plotting a bar graph of the number of tweets in each location, for the first ten locations listed\n# in the column 'location'\nauthor_count  = train['author'].value_counts()\nauthor_count = author_count[:10,]\nplt.figure(figsize=(20,15))\nsns.barplot(author_count.index, author_count.values, alpha=1)\nplt.title('Top 10 authors')\nplt.ylabel('Number of Occurrences', fontsize= 25)\nplt.xlabel('author', fontsize=25)\nplt.show()","38121b6c":"# test dataset\n\nauthor_count  = test['author'].value_counts()\nauthor_count = author_count[:10,]\nplt.figure(figsize=(20,15))\nsns.barplot(author_count.index, author_count.values, alpha=1)\nplt.title('Top 10 authors')\nplt.ylabel('Number of Occurrences', fontsize= 25)\nplt.xlabel('author', fontsize=25)\nplt.show()","07b43259":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer_tr = WordNetLemmatizer()\n\ndef process_text(text):\n    \n    #result = text.replace('\/','').replace('\\n','')\n    result = re.sub('[^a-zA-Z]',\" \", text)\n    result = result.lower()\n    result = result.split()\n    result = [lemmatizer_tr.lemmatize(word) for word in result if word not in stopwords.words('english') ]\n    \n    result = \" \".join(result)\n    return result","c927d599":"# for train dataset\nimport nltk\n\n# progress bar\nfrom tqdm import tqdm_notebook,tqdm\n\n# instantiate\ntqdm.pandas(tqdm_notebook)\n\ntrain['processed_headline'] = train['text'].progress_apply(process_text)\n","95b2bc29":"print(train.shape)\ntrain.head()","1b670166":"# for test dataset\n\ntest['processed_headline'] = test['text'].progress_apply(process_text)","d5c90572":"print(test.shape)\ntest.head()\n","377b61d4":"from sklearn.feature_extraction.text import TfidfVectorizer","3d4a59b0":"#applying the TfidfVectorizer for bag of TF-IDF\n\n\ntfidf = TfidfVectorizer(max_features = 20000,ngram_range=(1,3))\n# taking the most frequent 5000 words and take combination of 1 word as a feature and then apply TF-IDF\n\nX_train = tfidf.fit_transform(train['processed_headline'])\n\nX_test = tfidf.transform(test['processed_headline'])","41c01bad":"print(X_train.shape)\nprint(X_test.shape)","d1ed1a08":"tfidf.get_feature_names()[:20]","26bbb720":"tfidf.get_params()","c67ba930":"Y_train = train['label']\n\nprint(Y_train.shape)","cba49efa":"import numpy as np\nimport itertools\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier","e04de096":"\"\"\"\ntechnique 1\n....................\n.......................\n\n\nmodel1 = RandomForestClassifier(n_estimators=300,criterion='entropy')\nmodel1.fit(X_train,Y_train)\n\nmodel2=DecisionTreeClassifier()\nmodel2.fit(X_train,Y_train)\n\nmodel3=SVC()\nmodel3.fit(X_train,Y_train)\n\nmodel4=LogisticRegression()\nmodel4.fit(X_train,Y_train)\n\nmodel5=GaussianNB()\nmodel5.fit(x_train.toarray(),y_train)\n\"\"\"","e815c9ac":"\"\"\"\npred1 = model1.predict(X_test)\npred2 = model2.predict(X_test)\npred3 = model3.predict(X_test)\npred4 = model4.predict(X_test)\npred5 = model5.predict(X_test)\n\nstacked_predictions = np.column_stack((pred1,pred2,pred3,pred4,pred5))\n\nmeta_model = LogisticRegression()\n\n# fit meta model on stacked prediction\nmeta_model.fit(stacked_predictions,Y_test)\n\"\"\"","58afe642":"\"\"\"\ntechnique2 \n..................................\n..................................\n\nlevel0 = list()\nlevel0.append(('model1',RandomForestClassifier(n_estimators=300,criterion='entropy')))\nlevel0.append(('model2',DecisionTreeClassifier()))\nlevel0.append(('model3',SVC()))\nlevel0.append(('model4',LogisticRegression()))\nlevel0.append(('model5',GaussianNB()))\n\nlevel1 = LogisticRegression()\n\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n\nmodel.fit(X_train,Y_train)\nY_pred = model.predict(X_test)","b07b6386":"test.id","7a9f176f":"my_submission = pd.DataFrame({'id': test.id, 'label': Y_pred })\n\nmy_submission.to_csv('submission.csv', index=False)","efcc48c7":"**Submission file**(with 2 col id and y_pred from test dataset)","db49b463":"** Training the machine learning model**","0f1077bb":"**Text preprocessing by removing stopwords and using stemming\/lemmatization**","5187ef9b":"Data cleaning","5c5eb47c":"Exploratory data analysis","6de232ac":"* **Using stacking technique**","5d6422eb":"both the datasets are almost same"}}