{"cell_type":{"6c6219de":"code","291a61af":"code","6c940465":"code","4120f6f5":"code","a156b784":"code","effe29a8":"code","4523e989":"code","a26ac5cc":"code","72a4956d":"code","4339bc08":"code","25f23968":"code","9ee37709":"code","72c42389":"code","1cf73ae2":"code","062de84c":"code","131d4eff":"code","bdb42a90":"code","fa93f1ad":"code","3080af2b":"code","09a5f2d9":"code","26be327c":"code","df4da4cd":"code","75159171":"code","9a201075":"code","b2cb1d84":"markdown","ac23a088":"markdown","151ef8fb":"markdown","ab62aa24":"markdown","1b711fd7":"markdown"},"source":{"6c6219de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n","291a61af":"from keras.applications import VGG16\n\nconv_base = VGG16(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(150,150,3))","6c940465":"import os\nimport numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator","4120f6f5":"# location of dataset in 2 different folders\noriginal_dataset_cat = '..\/input\/originalcatvsdog\/PetImages\/Cat'\noriginal_dataset_Dog = '..\/input\/originalcatvsdog\/PetImages\/Dog'","a156b784":"import shutil\n\nbase_dir = '\/kaggle\/working\/base_dir'\nos.mkdir(base_dir)\n\ntest_dir = os.path.join(base_dir,'test')\nos.mkdir(test_dir)\n\ntrain_dir = os.path.join(base_dir,'train')\nos.mkdir(train_dir)\n\nvalid_dir = os.path.join(base_dir,'valid')\nos.mkdir(valid_dir)","effe29a8":"test_dir_cat = os.path.join(test_dir,'cat')\nos.mkdir(test_dir_cat)\n\ntest_dir_dog = os.path.join(test_dir,'dog')\nos.mkdir(test_dir_dog)\n\ntrain_dir_cat = os.path.join(train_dir,'cat')\nos.mkdir(train_dir_cat)\n\ntrain_dir_dog = os.path.join(train_dir,'dog')\nos.mkdir(train_dir_dog)\n\nvalid_dir_cat = os.path.join(valid_dir,'cat')\nos.mkdir(valid_dir_cat)\n\nvalid_dir_dog = os.path.join(valid_dir,'dog')\nos.mkdir(valid_dir_dog)\n\nfnames =['{}.jpg'.format(i) for i in range(1001)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_cat, fname)\n    dst = os.path.join(train_dir_cat,fname)\n    shutil.copyfile(src,dst)\n\nfnames = ['{}.jpg'.format(i) for i in range(1001,1501)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_cat, fname)\n    dst = os.path.join(valid_dir_cat,fname)\n    shutil.copyfile(src,dst)\n    \nfnames = ['{}.jpg'.format(i) for i in range(1501,2001)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_cat, fname)\n    dst = os.path.join(test_dir_cat,fname)\n    shutil.copyfile(src,dst)  \n    \nfnames =['{}.jpg'.format(i) for i in range(1001)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_Dog, fname)\n    dst = os.path.join(train_dir_dog,fname)\n    shutil.copyfile(src,dst)\n\nfnames = ['{}.jpg'.format(i) for i in range(1000,1500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_Dog, fname)\n    dst = os.path.join(valid_dir_dog,fname)\n    shutil.copyfile(src,dst)\n    \nfnames = ['{}.jpg'.format(i) for i in range(1500,2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_Dog, fname)\n    dst = os.path.join(test_dir_dog,fname)\n    shutil.copyfile(src,dst)    ","4523e989":"datagen = ImageDataGenerator(rescale = 1.\/255)\nbatch_size = 20\n\ndef extract_features(directory,sample_count):\n    features = np.zeros(shape = (sample_count,4,4,512))\n    labels = np.zeros(shape=(sample_count))\n    generator = datagen.flow_from_directory(\n    directory,\n        target_size =(150,150),\n        batch_size=batch_size,\n        class_mode='binary')\n    i = 0\n    for input_batch,labels_batch in generator:\n        features_batch = conv_base.predict(input_batch)\n        features[ i* batch_size: (i+1) * batch_size] = features_batch\n        labels[ i* batch_size :(i+1)* batch_size] = labels_batch\n        i +=1\n        if i * batch_size >= sample_count:\n            break\n    return features,labels        \n    ","a26ac5cc":"train_features, train_labels = extract_features(train_dir, 2000)","72a4956d":"os.remove('\/kaggle\/working\/base_dir\/train\/cat\/666.jpg')","4339bc08":"train_features, train_labels = extract_features(train_dir, 2001)\ntest_features, test_labels = extract_features(test_dir, 1000)\nvalid_features, valid_labels = extract_features(valid_dir, 2000)","25f23968":"train_features = np.reshape(train_features,(2001,4*4*512))\nvalid_features = np.reshape(valid_features,(2000,4*4*512))\ntest_features = np.reshape(test_features,(1000,4*4*512))","9ee37709":"# defining densly connevcted layer\n\nfrom keras.layers import Dense,Dropout\n\nfrom keras.models import Sequential\nfrom keras import optimizers\n\nmodel = Sequential()\nmodel.add(Dense(256,activation='relu',input_dim= 4*4*512))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(\noptimizer= optimizers.RMSprop(lr=1e-4),\n    loss='binary_crossentropy',\n    metrics =['acc'])\n\nhistory =model.fit(train_features,train_labels,\n                  epochs=30,\n                  batch_size=20,\n                  validation_data=(valid_features,valid_labels))\n\n\n","72c42389":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss =history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc)+1)\n\nplt.plot(epochs,acc,'bo',label='Training accuracy')\nplt.plot(epochs,val_acc,'b',label= 'Validating Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs,loss,'ro',label='Training Loss')\nplt.plot(epochs,val_loss,'r',label='Vaalidation Loss')\nplt.legend()\nplt.show()","1cf73ae2":"from keras.models import Sequential\n\nfrom keras.layers import Flatten,Dense\n\nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))","062de84c":"model.summary()","131d4eff":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\n\ntrain_datagen = ImageDataGenerator(\n                rescale =1.\/255,\n                rotation_range=40,\n                width_shift_range=0.2,\n                height_shift_range=0.2,\n                shear_range=0.2,\n                zoom_range=0.2,\n                horizontal_flip=True,\n                fill_mode='nearest')\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n                  train_dir,\n                  target_size=(150,150),\n                  batch_size=20,\n                  class_mode='binary')\n\nvalid_generator = train_datagen.flow_from_directory(\n                  valid_dir,\n                  target_size=(150,150),\n                  batch_size=20,\n                  class_mode='binary')\n\nmodel.compile(loss='binary_crossentropy',\n             optimizer = optimizers.RMSprop(lr=1e-4),\n             metrics=['acc'])\n\nhistory = model.fit_generator(\ntrain_generator,\nsteps_per_epoch=100,\nepochs =30,\nvalidation_data = valid_generator,\nvalidation_steps=50)","bdb42a90":"acc = history.history['acc']\nloss = history.history['loss']\nval_acc = history.history['val_acc']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc)+1)\nplt.plot(epochs,acc,'bo',label='Training accuracy')\nplt.plot(epochs,val_acc,'b',label='validating accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs,loss,'ro',label='Training loss')\nplt.plot(epochs,val_loss,'r',label='validating loss')\nplt.legend()\nplt.show()","fa93f1ad":"conv_base.trainable=True\nset_trainable = False\nfor layer in conv_base.layers:\n    if layer.name=='block_conv1':\n        set_trainable=True\n    if set_trainable:\n        layer.trainale=True\n    else:\n        layer.trainable=False","3080af2b":"model.compile(loss='binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-5),\n             metrics =['acc'])","09a5f2d9":"history = model.fit_generator( train_generator, \n                              steps_per_epoch=100,\n                              epochs=100,\n                              validation_data=valid_generator, \n                              validation_steps=50)\n\n","26be327c":"acc = history.history['acc']\nloss = history.history['loss']\nval_acc = history.history['val_acc']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc)+1)\nplt.plot(epochs,acc,'bo',label='Training accuracy')\nplt.plot(epochs,val_acc,'b',label=' validating accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs,loss,'ro',label=' Training loss')\nplt.plot(epochs,val_loss,'r',label=' validating loss')\nplt.legend()\nplt.show()","df4da4cd":"def smooth_curve(points, factor=0.8): \n    smoothed_points = [] \n    for point in points: \n        if smoothed_points: \n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor)) \n        else: smoothed_points.append(point)\n    return smoothed_points\n","75159171":"acc = history.history['acc']\nloss = history.history['loss']\nval_acc = history.history['val_acc']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc)+1)\nplt.plot(epochs,smooth_curve(acc),'bo',label='Smooth Training accuracy')\nplt.plot(epochs,smooth_curve(val_acc),'b',label='Smooth validating accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs,smooth_curve(loss),'ro',label='Smooth Training loss')\nplt.plot(epochs,smooth_curve(val_loss),'r',label='Smooth validating loss')\nplt.legend()\nplt.show()","9a201075":"test_generator = test_datagen.flow_from_directory( \n    test_dir, target_size=(150, 150), batch_size=20, class_mode='binary')\ntest_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\nprint('test acc:', test_acc)\n","b2cb1d84":" After inspection it is found that image address '\/kaggle\/working\/base_dir\/train\/cat\/666.jpg' is corrupt so we are going to delete it.","ac23a088":"function to smooth the lines in the plot","151ef8fb":"> There is more advnace way as well known as fine tuning,in which we Unfreeze the model and join the trained mmodel and defreeze model's some layers. we will start with block 5.\n","ab62aa24":"> From Previous Version","1b711fd7":"# feature extraction with data augmentation\n"}}