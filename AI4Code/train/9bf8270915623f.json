{"cell_type":{"6fd64f25":"code","5936d3a1":"code","e3000e8f":"code","e99a7857":"code","9427d0db":"code","3e423e41":"markdown","873edb20":"markdown","2f2d5dd6":"markdown","37d43813":"markdown","a9965448":"markdown"},"source":{"6fd64f25":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy import interp\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5936d3a1":"def run(x_c0, x_c1):\n    # Create Target Variable\n    y = np.array([1 if v < x_c0.size else 0 for v in range(x_c0.size + x_c1.size)])\n\n    fig, axs = plt.subplots(1, 2, figsize = (16,8))\n\n    # Plot the distributions\n    ax = sns.kdeplot(x_c0, ax=axs[0])\n    ax = sns.kdeplot(x_c1, ax=axs[0])\n\n    # Prepare classification\n    X = np.append(x_c0, x_c1)\n    X = X.reshape(-1,1)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n\n    # Run a simple linear classifier    \n    clf = LogisticRegression(solver='lbfgs')\n    clf.fit(X_train, y_train)\n    y_preds = clf.predict_proba(X_test)\n\n    # Calculate AUC\n    fpr, tpr, thresholds = roc_curve(y_test, y_preds[:,1])    \n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC\n    axs[1].plot(fpr, tpr, lw=1, label='(AUC = %0.2f)' % (roc_auc))\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")","e3000e8f":"run(np.random.normal(-7, 2, 100), np.random.normal(7, 2, 100))","e99a7857":"run(np.random.normal(-7, 5, 100), np.random.normal(7, 5, 100))","9427d0db":"run(np.random.normal(-7, 14, 100), np.random.normal(7, 14, 100))","3e423e41":"I hope the kernel helps one or the other to better understand the AUC score.","873edb20":"\n### Creating normal distributions that overlaps a little bit\n\nThe more the distributions overlap, the worse the AUC score will be.","2f2d5dd6":"### Creating normal distributions that clearly separates the target variable\n\nIn the following cell you can see that the AUC score is = 1 if the distributions per class are clearly separable.","37d43813":"The AUC score can be used to validate a binary classification. With this kernel I want to explain it visually to better understand it.","a9965448":"### Creating normal distributions that overlaps a little bit\n\nNow you can see that the AUC score decreases if the distributions per class overlapping a little bit."}}