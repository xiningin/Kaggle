{"cell_type":{"5ba3458a":"code","cf5f7b67":"code","3a032790":"code","14205ba3":"code","3db0670d":"code","da368d8d":"code","137bfc8c":"code","c4770def":"code","c377c900":"code","b3e6a63c":"code","4092013e":"code","96caa0ca":"code","2b121466":"code","fbf17442":"code","2e4b5a4c":"code","af1b788a":"code","7c7cf2f1":"code","a8fe18e7":"code","27f9e867":"code","9283ed6a":"code","c429a903":"code","7067006b":"code","7ade9a64":"code","1f609288":"code","0e8d20ea":"code","cbec232b":"code","f7c980e4":"code","d6c51551":"code","c6a8da0f":"code","c178859c":"code","4a8454b4":"code","417ac093":"code","0252caee":"code","95db86c9":"code","4833d3a1":"code","a7e5e26d":"code","adc2d0a2":"code","4829deaa":"markdown","7b26772b":"markdown","ddbf0ef4":"markdown","b1f45130":"markdown","70d6742b":"markdown","0166b6f3":"markdown","3375b8ee":"markdown","e8395666":"markdown","06dd8cd9":"markdown","c8c2a9a7":"markdown","78fc776d":"markdown","4a133fda":"markdown","22816ee8":"markdown","31e68711":"markdown","2d5db721":"markdown","854e7c93":"markdown","53fec822":"markdown","9939e4ac":"markdown","ba796ccf":"markdown"},"source":{"5ba3458a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n### import packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\n \nfrom sklearn.model_selection import train_test_split, ShuffleSplit, learning_curve, GridSearchCV, KFold, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.metrics import roc_curve, accuracy_score, confusion_matrix, classification_report, roc_auc_score, make_scorer, precision_recall_curve, average_precision_score \nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn import metrics\n\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n%matplotlib inline\nplt.style.use('ggplot')\n\nfrom catboost import CatBoostClassifier # Or CatBoostRegressor\nfrom sklearn.model_selection import KFold\nfrom itertools import product,chain\n\nimport shap\nimport catboost\nfrom catboost import *\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import recall_score, accuracy_score, confusion_matrix, classification_report\nimport random\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nnp.random.seed(0)\nrandom.seed(0)\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cf5f7b67":"from sortedcontainers import SortedList\nimport copy\nimport collections\nimport numpy as np\nfrom itertools import product,chain\nimport pandas\nfrom sklearn.model_selection import KFold\nimport catboost as cb\n\n''' a class for doing grid search on a set of parameters provided in a dict. 'pdict' should be a dictionary like the following:\npdict = {'depth':[1,2], 'iterations':[250,100,500], 'thread_count':4}\n\nwhen grid_search is called it will return an iterator that provides samples from the dictionary e.g.\n{'depth':1, 'iterations':250, 'thread_count':4}\n{'depth':2, 'iterations':250, 'thread_count':4}\n{'depth':1, 'iterations':100, 'thread_count':4}\netc.\nafter calling an iteration of grid_search, you need to test the classifier and run 'register_result'\nThis will update the internal list of results, so that the next call to grid_search will use the best\nparameters for all the parameters not currently being updated.\n\ngrid_search can be provided a list e.g. grid_search(['depth']) this will use the current best parameters for all\nthe other arguments and only search over 'depth'. You can then call e.g. grid_search(['iterations']) and it will use\nthe best depth found previously and cycle through all the 'iterations'. Searching incrementally can be much faster\nthan doing a full grid search, but may miss the global optimum. '''\nclass paramsearch:\n    def __init__(self,pdict):    \n        self.pdict = {}\n        # if something is not passed in as a sequence, make it a sequence with 1 element\n        #   don't treat strings as sequences\n        for a,b in pdict.items():\n            if isinstance(b, collections.Sequence) and not isinstance(b, str): self.pdict[a] = b\n            else: self.pdict[a] = [b]\n        # our results are a sorted list, so the best score is always the final element\n        self.results = SortedList()       \n                    \n    def grid_search(self,keys=None):\n        # do grid search on only the keys listed. If none provided, do all\n        if keys==None: keylist = self.pdict.keys()\n        else: keylist = keys\n \n        listoflists = [] # this will be list of lists of key,value pairs\n        for key in keylist: listoflists.append([(key,i) for i in self.pdict[key]])\n        for p in product(*listoflists):\n            # do any changes to the current best parameter set\n            if len(self.results)>0: template = self.results[-1][1]\n            else: template = {a:b[0] for a,b in self.pdict.items()}\n            # if our updates are the same as current best, don't bother\n            if self.equaldict(dict(p),template): continue\n            # take the current best and update just the ones to change\n            yield self.overwritedict(dict(p),template)\n                              \n    def equaldict(self,a,b):\n        for key in a.keys(): \n            if a[key] != b[key]: return False\n        return True            \n                              \n    def overwritedict(self,new,old):\n        old = copy.deepcopy(old)\n        for key in new.keys(): old[key] = new[key]\n        return old            \n    \n    # save a (score,params) pair to results. Since 'results' is a sorted list,\n    #   the best score is always the final element. A small amount of noise is added\n    #   because sorted lists don't like it when two scores are exactly the same    \n    def register_result(self,result,params):\n        self.results.add((result+np.random.randn()*1e-10,params))    \n        \n    def bestscore(self):\n        return self.results[-1][0]\n        \n    def bestparam(self):\n        return self.results[-1][1]\n        ","3a032790":"# Function to load the inputs dataset\ndef read_data(tp = \"Train\", N = 1542865627584):\n    target = pd.read_csv(\"\/kaggle\/input\/healthcare-provider-fraud-detection-analysis\/{}-{}.csv\".format(tp.title(), N))\n    pt = pd.read_csv(\"\/kaggle\/input\/healthcare-provider-fraud-detection-analysis\/{}_Beneficiarydata-{}.csv\".format(tp.title(), N))\n    in_pt = pd.read_csv(\"\/kaggle\/input\/healthcare-provider-fraud-detection-analysis\/{}_Inpatientdata-{}.csv\".format(tp.title(), N))\n    out_pt = pd.read_csv(\"\/kaggle\/input\/healthcare-provider-fraud-detection-analysis\/{}_Outpatientdata-{}.csv\".format(tp.title(), N))\n    return (in_pt, out_pt, pt, target)","14205ba3":"# This function helps us find the unique values in a row\ndef N_unique_values(df):\n    return np.array([len(set([i for i in x[~pd.isnull(x)]])) for x in df.values])","3db0670d":"### Load Train data\nin_pt, out_pt, ben, target = read_data()","da368d8d":"# Replace values with a binary annotation\nben = ben.replace({'ChronicCond_Alzheimer': 2, 'ChronicCond_Heartfailure': 2, 'ChronicCond_KidneyDisease': 2,\n                   'ChronicCond_Cancer': 2, 'ChronicCond_ObstrPulmonary': 2, 'ChronicCond_Depression': 2,\n                   'ChronicCond_Diabetes': 2, 'ChronicCond_IschemicHeart': 2, 'ChronicCond_Osteoporasis': 2,\n                   'ChronicCond_rheumatoidarthritis': 2, 'ChronicCond_stroke': 2, 'Gender': 2 }, \n                  0)\nben = ben.replace({'RenalDiseaseIndicator': 'Y'}, 1).astype({'RenalDiseaseIndicator': 'int64'})\n\n# Change target variable to binary\ntarget[\"target\"] = np.where(target.PotentialFraud == \"Yes\", 1, 0) \ntarget.drop('PotentialFraud', axis=1, inplace=True)","137bfc8c":"# Merge in_pt, out_pt and ben df into a single patient dataset\ndata = pd.merge(in_pt, out_pt,\n                    left_on = [ idx for idx in out_pt.columns if idx in in_pt.columns],\n                    right_on = [ idx for idx in out_pt.columns if idx in in_pt.columns],\n                    how = 'outer').\\\n          merge(ben,left_on='BeneID',right_on='BeneID',how='inner')","c4770def":"patient_merge_id = [i for i in out_pt.columns if i in in_pt.columns]\n\n# Merge in_pt, out_pt and ben df into a single patient dataset\ndata = pd.merge(in_pt, out_pt,\n                    left_on = patient_merge_id,\n                    right_on = patient_merge_id,\n                    how = 'outer').\\\n          merge(ben,left_on='BeneID',right_on='BeneID',how='inner')\n","c377c900":"# We find the number of unique physicians \ndata['N_unique_Physicians'] = N_unique_values(data[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']]) \n\n# We separate the types of physicians into numeric values\ndata[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']] = np.where(data[['AttendingPhysician','OperatingPhysician',\n                                                                                      'OtherPhysician']].isnull(), 0, 1)\n\n# We count the number of types of physicians that attend the patient\ndata['N_Types_Physicians'] = data['AttendingPhysician'] +  data['OperatingPhysician'] + data['OtherPhysician']\n\n# Now we create a variable to check if there is a single doctor on a patient that was attended by more than 1 type of doctor\n# This helps us finds those cases that are only looked at by 1 physicians\ndata['Same_Physician'] = data.apply(lambda x: 1 if (x['N_unique_Physicians'] == 1 and x['N_Types_Physicians'] > 1) else 0,axis=1)\n\n# Similar to Same_Physician, we create a variable to see if 1 physicians has had multiple roles, but has not been alone reviewing the case\ndata['Same_Physician2'] = data.apply(lambda x: 1 if (x['N_unique_Physicians'] == 2 and x['N_Types_Physicians'] > 2) else 0,axis=1)\n\n# We check our new variables\ndata[['N_unique_Physicians','N_Types_Physicians','Same_Physician','Same_Physician2']].head()","b3e6a63c":"# We count the number of procedures for each claim, we drop the initial variables\nClmProcedure_vars = ['ClmProcedureCode_{}'.format(x) for x in range(1,7)]\ndata['N_Procedure'] = N_unique_values(data[ClmProcedure_vars])\ndata = data.drop(ClmProcedure_vars, axis = 1)\n\n# We count the number of claims, we also separate this by unique claims and extra claims, we drop the initial variables\nClmDiagnosisCode_vars =['ClmAdmitDiagnosisCode'] + ['ClmDiagnosisCode_{}'.format(x) for x in range(1, 11)]\n\ndata['N_Unique_Claims'] = N_unique_values(data[ClmDiagnosisCode_vars])\ndata['N_Total_Claims'] = data[ClmDiagnosisCode_vars].notnull().to_numpy().sum(axis = 1)\ndata['N_Extra_Claims'] = data['N_Total_Claims'] - data['N_Unique_Claims']\n\nClmDiagnosisCode_vars.append('N_Total_Claims')\ndata = data.drop(ClmDiagnosisCode_vars, axis = 1)","4092013e":"#  Transform string columns of date into type date\ndata['AdmissionDt'] = pd.to_datetime(data['AdmissionDt'] , format = '%Y-%m-%d')\ndata['DischargeDt'] = pd.to_datetime(data['DischargeDt'],format = '%Y-%m-%d')\n\ndata['ClaimStartDt'] = pd.to_datetime(data['ClaimStartDt'] , format = '%Y-%m-%d')\ndata['ClaimEndDt'] = pd.to_datetime(data['ClaimEndDt'],format = '%Y-%m-%d')\n\ndata['DOB'] = pd.to_datetime(data['DOB'] , format = '%Y-%m-%d')\ndata['DOD'] = pd.to_datetime(data['DOD'],format = '%Y-%m-%d')\n\n# Number of days\ndata['Admission_Days'] = ((data['DischargeDt'] - data['AdmissionDt']).dt.days) + 1\n\n# Number of claim days \ndata['Claim_Days'] = ((data['ClaimEndDt'] - data['ClaimStartDt']).dt.days) + 1\n\n# Age at the time of claim\ndata['Age'] = round(((data['ClaimStartDt'] - data['DOB']).dt.days + 1)\/365.25)","96caa0ca":"# We create a Hospitalization flag \ndata['Hospt'] = np.where(data.DiagnosisGroupCode.notnull(), 1, 0)\ndata = data.drop(['DiagnosisGroupCode'], axis = 1)\n\n# Variable if patient is dead\ndata['Dead']= 0\ndata.loc[data.DOD.notna(),'Dead'] = 1","2b121466":"# We find which variables hold missing data\nna = data.isnull().sum()\nna[na != 0]","fbf17442":"## We know that missing admission days come from missing admission and discharge date, and those cases come from the out patients data set\n\n\n# We also see that there are some cases of missing deductible amount paid, so we also want to keep an eye on that\ndata['Missing_Deductible_Amount_Paid'] = 0\ndata.loc[data['DeductibleAmtPaid'].isnull(), 'Missing_Deductible_Amount_Paid'] = 1 \n\n# After identifying the missing values, we fill the missing values with 0\ndata = data.fillna(0).copy()","2e4b5a4c":"### Sum all numeric variables\n_sum = data.groupby(['Provider'], as_index = False)[['InscClaimAmtReimbursed', 'DeductibleAmtPaid', 'RenalDiseaseIndicator', \n                                                    'ChronicCond_Alzheimer', 'AttendingPhysician', 'OperatingPhysician', \n                                                    'OtherPhysician', 'N_unique_Physicians', 'ChronicCond_Heartfailure', \n                                                    'N_Types_Physicians', 'Same_Physician',\n                                                    'ChronicCond_KidneyDisease', 'ChronicCond_Cancer', \n                                                    'ChronicCond_ObstrPulmonary', 'ChronicCond_Depression', \n                                                    'ChronicCond_Diabetes', 'ChronicCond_IschemicHeart', \n                                                    'ChronicCond_Osteoporasis', 'ChronicCond_rheumatoidarthritis',\n                                                    'ChronicCond_stroke', 'Dead', \n                                                    'N_Procedure','N_Unique_Claims', 'N_Extra_Claims', 'Admission_Days',\n                                                    'Claim_Days', 'Hospt', 'Missing_Deductible_Amount_Paid']].sum()\n\n# To separate our variables, we shall add '_sum' at the end of their names\n_sum = _sum.add_suffix('_sum')\n\n\n### Count number of records\n_count = data[['BeneID', 'ClaimID']].groupby(data['Provider']).nunique().reset_index()\n_count.rename(columns={'BeneID':'BeneID_count','ClaimID':'ClaimID_count'},inplace=True)\n\n\n### Calculate mean for all numeric variables\n_mean = data.groupby(['Provider'], as_index = False)[['NoOfMonths_PartACov', 'NoOfMonths_PartBCov',\n                                                      'IPAnnualReimbursementAmt', 'IPAnnualDeductibleAmt',\n                                                      'OPAnnualReimbursementAmt', 'OPAnnualDeductibleAmt', 'Age',\n                                                      'AttendingPhysician', 'OperatingPhysician','OtherPhysician',\n                                                      'N_unique_Physicians', 'ChronicCond_Heartfailure', \n                                                      'N_Types_Physicians', 'Same_Physician',\n                                                      'ChronicCond_KidneyDisease', 'ChronicCond_Cancer', \n                                                      'ChronicCond_ObstrPulmonary', 'ChronicCond_Depression', \n                                                      'ChronicCond_Diabetes', 'ChronicCond_IschemicHeart', \n                                                      'ChronicCond_Osteoporasis', 'ChronicCond_rheumatoidarthritis',\n                                                      'ChronicCond_stroke', 'Dead', 'N_Procedure','N_Unique_Claims', \n                                                      'N_Extra_Claims', 'Admission_Days','Claim_Days', 'Hospt',\n                                                      'Missing_Deductible_Amount_Paid'\n                                                   ]].mean()\n\n# To separate our variables, we shall add '_mean' at the end of their names\n_mean = _mean.add_suffix('_mean')\n\n# We create a dataset that holds all the variables\n_total = _count.merge(_sum, how='left',left_on='Provider',right_on='Provider_sum').\\\n                merge(_mean, how='left',left_on='Provider',right_on='Provider_mean').\\\n                drop(['Provider_sum','Provider_mean'], axis=1).\\\n                merge(target, on='Provider', how='left')","af1b788a":"_total.groupby( [\"target\"] ).target.count().plot(kind = \"bar\", figsize = (10,6));","7c7cf2f1":"def plot_corr(df_corr):\n    corrMatrix = df_corr.corr()\n    plt.subplots(figsize=(20,15))\n    sns.heatmap(corrMatrix, annot=False)\n    plt.show()","a8fe18e7":"# We plot the correlations in the _sum dataset\nplot_corr(_sum)","27f9e867":"# We drop correlated values \nsum_corr = _sum.drop(['Hospt_sum','AttendingPhysician_sum','OperatingPhysician_sum','Admission_Days_sum',\n                      'OtherPhysician_sum','ChronicCond_ObstrPulmonary_sum', 'ChronicCond_Depression_sum',\n                      'ChronicCond_Diabetes_sum','ChronicCond_IschemicHeart_sum','ChronicCond_KidneyDisease_sum', \n                      'ChronicCond_Cancer_sum','ChronicCond_Osteoporasis_sum','RenalDiseaseIndicator_sum',\n                      'ChronicCond_rheumatoidarthritis_sum','ChronicCond_Heartfailure_sum','N_Unique_Claims_sum',\n                      'ChronicCond_Alzheimer_sum','ChronicCond_stroke_sum','N_Procedure_sum','N_unique_Physicians_sum',\n                      'N_Types_Physicians_sum','DeductibleAmtPaid_sum'],axis=1) \n\n# And plot again to check\nplot_corr(sum_corr)","9283ed6a":"# We plot the correlations in the _mean dataset\nplot_corr(_mean)","c429a903":"# We drop correlated values \nmean_corr = _mean.drop(['Hospt_mean','Admission_Days_mean','N_unique_Physicians_mean','NoOfMonths_PartBCov_mean',\n                        'NoOfMonths_PartACov_mean','IPAnnualReimbursementAmt_mean','N_Procedure_mean',\n                        'OPAnnualDeductibleAmt_mean', 'N_Unique_Claims_mean'], axis=1) \n\n# And plot again to check\nplot_corr(mean_corr)","7067006b":"# this function does 3-fold crossvalidation with catboostclassifier          \ndef cross_val_test(params,train_set,train_label,cat_dims,n_splits=3):\n    Skf = StratifiedKFold(n_splits=n_splits,shuffle=True, random_state = 0) \n    res = []\n    for train_index, test_index in Skf.split(train_set,train_label):\n        train = train_set.iloc[train_index,:]\n        test = train_set.iloc[test_index,:]\n        \n        labels = train_label.iloc[train_index]\n        test_labels = train_label.iloc[test_index]\n\n        clf = CatBoostClassifier(**params)\n        clf.fit(train, np.ravel(labels), cat_features=cat_dims,verbose=False)\n\n        # We use the recall score to search for the best parameters\n        res.append(recall_score(test_labels, clf.predict(test)))\n    return np.mean(res)","7ade9a64":"# this function runs grid search on several parameters\ndef catboost_param_tune(params,train_set,train_label,cat_dims=None,n_splits=3):\n    ps = paramsearch(params)\n    # search 'border_count', 'l2_leaf_reg' etc. individually \n    #   but 'iterations','learning_rate' together\n    for prms in chain(ps.grid_search(['border_count']),\n                      ps.grid_search(['l2_leaf_reg']),\n                      ps.grid_search(['iterations','learning_rate']),\n                      ps.grid_search(['depth'])):\n        res = cross_val_test(prms,train_set,train_label,cat_dims,n_splits)\n        # save the crossvalidation result so that future iterations can reuse the best parameters\n        ps.register_result(res,prms)\n        print(ps.bestscore())\n    return ps.bestparam()\n","1f609288":"# This function runs a catboost model on the data we define\ndef fcatboost(train, test):\n    \n    X_train, X_test, y_train, y_test = train_test_split(train, test, test_size=0.30, random_state=1)\n    \n    # convert categorical columns to integers\n    category_cols = []\n    cat_dims = [train_set.columns.get_loc(i) for i in category_cols[:-1]] \n    \n    column_names = X_train.columns\n    \n    X_train = pd.DataFrame(StandardScaler().fit_transform(X_train))\n    X_test = pd.DataFrame(StandardScaler().fit_transform(X_test))\n    \n    X_train.columns = column_names\n    X_test.columns = column_names\n\n    params = {'depth':[3,1,2,6,4,5,7,8,9,10],\n              'iterations':[250,100,500,1000],\n              'learning_rate':[0.03,0.001,0.01,0.1,0.2,0.3], \n              'l2_leaf_reg':[3,1,5,10,100],\n              'border_count':[32,5,10,20,50,100,200],\n              'thread_count':4,\n              'random_seed':2}\n\n    bestparams = catboost_param_tune(params,X_train,y_train,[])\n\n    model = CatBoostClassifier(**bestparams)\n    model.fit(X_train,y_train,verbose=False)\n    print(model.random_seed_)\n    \n    return model, X_train, X_test, y_train, y_test","0e8d20ea":"# This function plots the shap values from the model\ndef catboost_shap_plot(model, X_train, y_train):\n    \n    shap.initjs()\n\n    shap_values = model.get_feature_importance(Pool(X_train,y_train), type='ShapValues')\n\n    expected_value = shap_values[0,-1]\n    shap_values = shap_values[:,:-1]\n    print('\\nShap features importance:')\n\n    # summarize the effects of all the features\n    shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n    shap.summary_plot(shap_values, X_train)\n    ","cbec232b":"\n\nsum_corr = sum_corr.merge(target, left_on='Provider_sum', right_on='Provider', how='left')\n\nmodel, X_train, X_test, y_train, y_test = fcatboost(sum_corr.drop(['Provider','target','Provider_sum'], axis = 1),\n                                                    sum_corr.target)","f7c980e4":"catboost_shap_plot(model, X_train, y_train)","d6c51551":"mean_corr = mean_corr.merge(target, left_on='Provider_mean', right_on='Provider', how='left')\n\nmodel, X_train, X_test, y_train, y_test = fcatboost(mean_corr.drop(['Provider','target','Provider_mean'], axis = 1),\n                                                    mean_corr.target)","c6a8da0f":"catboost_shap_plot(model, X_train, y_train)","c178859c":"df = _total[['InscClaimAmtReimbursed_sum','N_Extra_Claims_sum','Claim_Days_sum',\n             'AttendingPhysician_mean','Missing_Deductible_Amount_Paid_mean','Dead_mean','Claim_Days_mean','N_Extra_Claims_mean',\n             'BeneID_count','ClaimID_count',\n             'Provider','target']]","4a8454b4":"def cm_Score(model, df):\n    recall = np.mean(cross_val_score(model, df.drop(['Provider','target'], axis = 1), df.target, cv=3, scoring='recall'))\n    accuracy = np.mean(cross_val_score(model, df.drop(['Provider','target'], axis = 1), df.target, cv=3, scoring='accuracy'))\n    \n    print('Accuracy Score: {}'.format(accuracy))\n    print('Recall Score: {}'.format(recall))\n   ","417ac093":"model, X_train, X_test, y_train, y_test = fcatboost(df.drop(['Provider','target'], axis = 1), df.target)","0252caee":"def flogistic(df, penalty):\n    X_train, X_test, y_train, y_test = train_test_split(df.drop(['Provider','target'], axis = 1),df.target, \n                                                  test_size=0.30, random_state=1)\n\n    X_train = pd.DataFrame(StandardScaler().fit_transform(X_train))\n    X_test = pd.DataFrame(StandardScaler().fit_transform(X_test))\n    \n    #Liblinear for small datasets of binary classes\n    logreg = LogisticRegression(penalty= penalty,solver= 'liblinear',class_weight='balanced', random_state = 5 , C = 0.001)\n    \n    cm_Score(logreg, df)\n    \n    ","95db86c9":"flogistic(df, 'l1')","4833d3a1":"flogistic(df, 'l2')","a7e5e26d":"def Gaus_bayes(df):\n    X_train, X_test, y_train, y_test = train_test_split(df.drop(['Provider','target'], axis = 1),df.target, \n                                                  test_size=0.30, random_state=1)\n\n\n    X_train = pd.DataFrame(StandardScaler().fit_transform(X_train))\n    X_test = pd.DataFrame(StandardScaler().fit_transform(X_test))\n\n    gnb = GaussianNB()\n    \n    cm_Score(gnb, df)","adc2d0a2":"Gaus_bayes(df)","4829deaa":">     With this graph, we decide on the variables InscClaimAmtReimbursed_sum and Claim_Days_sum\n","7b26772b":" # Modeling - Catboost","ddbf0ef4":"acc .58\nrec 0.97","b1f45130":"# Handling missing data","70d6742b":"    The naive bayes model has a recall of 0.50","0166b6f3":"# Feature Selection - Correlation","3375b8ee":"# Feature engineering ","e8395666":"# Features Selection - Final draft\n\nBased on the previous shap plots we can obtain the variables of interest from the sum and mean datasets to test our models, we also add the variables from the count dataset and the provider and target.","06dd8cd9":"# Modeling - Naive Bayes","c8c2a9a7":"Thanks to user Dima for giving context of variables in his code, since there is no data dictionary available. Some part of his code were used for data cleaning since they were optimal.","78fc776d":"# Modeling - Logistic Regression","4a133fda":"acc .8\nrec 0.91","22816ee8":"    The best catboost model has recall of 0.53","31e68711":"# Data Cleaning","2d5db721":"    Looking at the graph of SHAP values, we decide to use the variables: AttendingPhysician_mean,Missing_Deductible_Amount_Paid_mean,Dead_mean,Claim_Days_mean and N_Extra_Claims_mean","854e7c93":"We run the catboost model on the mean dataset ","53fec822":"We run the catboost model on the sum dataset first","9939e4ac":"# Create final Datasets","ba796ccf":"# Feature Selection - SHAP values\n    We shall use shap plots from a catboost model to find the best parameters"}}