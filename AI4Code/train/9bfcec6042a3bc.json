{"cell_type":{"65771453":"code","4f0384c1":"code","87e47569":"code","aaa4c684":"code","271eec3b":"code","bd3eed85":"code","97fa6527":"code","d35c2509":"code","4cfe2fc7":"code","444695da":"code","7064c41d":"code","80569e01":"code","a85d6e11":"code","b6c53dc4":"code","181facf5":"code","392d6708":"code","698acd6b":"code","55b7b9d6":"code","4d3bdb57":"code","10cb68bc":"code","776805d4":"code","52f6bd69":"code","90b8f1e8":"code","b61a78fe":"code","4be902be":"code","b80784df":"code","b4c1f15e":"code","40c8b736":"code","901f00a4":"code","7918630b":"code","aa45d96c":"code","82540b48":"code","3424939a":"code","d7ff0031":"code","62b26f98":"code","059202ae":"code","684c4acb":"code","15e9e305":"code","9f32b3eb":"code","4698f3ef":"code","91dbbfe8":"code","c8af6909":"code","c4f0d747":"code","bb2eec12":"code","09af0181":"code","b69bfa40":"code","a98ca1c0":"code","f9d70c88":"markdown","6b97d3a0":"markdown","2fc3ba60":"markdown","6404147d":"markdown","3d5053f0":"markdown","7c7d349d":"markdown","34c0b657":"markdown","c5ea8845":"markdown","af5d99d9":"markdown","8f337fe0":"markdown","97ecc181":"markdown","80432957":"markdown","36057c59":"markdown","5dfb35bc":"markdown","a0d6f017":"markdown","25776550":"markdown","4715144f":"markdown","cd87c362":"markdown","7439257f":"markdown","d9c8de6e":"markdown","f6695fbe":"markdown","4c6df5cd":"markdown","a284b149":"markdown","883dc63c":"markdown","ac5af9a5":"markdown","cbf55907":"markdown","812c7a18":"markdown"},"source":{"65771453":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom fastai.imports import *\nfrom fastai.structured import *\n\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\n\nfrom sklearn import metrics\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4f0384c1":"data = pd.read_csv('..\/input\/train\/Train.csv',low_memory=False, parse_dates=[\"saledate\"])","87e47569":"data.saledate","aaa4c684":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","271eec3b":"display_all(data.sample(10).T)","bd3eed85":"display_all(data.isnull().sum().sort_values(ascending=False)\/len(data))","97fa6527":"data_needed = data[['SalesID',                    \n'state'  ,                 \n'fiProductClassDesc',          \n'fiBaseModel',         \n'fiModelDesc' ,        \n'ProductGroup' ,       \n'saledate',      \n'datasource',     \n'ModelID' ,    \n'MachineID',   \n'SalePrice' ,\n'YearMade',   \n'ProductGroupDesc',  \n'Enclosure', \n'auctioneerID' ,\n'Hydraulics',\n'fiSecondaryDesc'  ,\n'Coupler' ,\n'Forks',\n'ProductSize'  ,\n'Transmission']]","d35c2509":"data_needed.SalePrice = np.log(data_needed.SalePrice)","4cfe2fc7":"data_needed.head()","444695da":"add_datepart(data_needed, 'saledate',drop=False)","7064c41d":"data_needed.sort_values('saledate',inplace=True)","80569e01":"data_needed.head(20)","a85d6e11":"data_needed.drop('saledate',axis=1,inplace=True)","b6c53dc4":"display_all(data_needed.head())","181facf5":"train_cats(data_needed)","392d6708":"data_needed.dtypes","698acd6b":"#let's see our data with the changes we have made so far\ndisplay_all(data_needed.head(100))","55b7b9d6":"data_needed.state.cat.categories","4d3bdb57":"data_needed.state.cat.codes.sort_index()","10cb68bc":"data_needed.isnull().sum().sort_values(ascending=False)\/len(data_needed)*100","776805d4":"#let see in transmission column and try to make a better intution about why is data missing in this column\ndata_needed.Transmission","52f6bd69":"data_needed.drop('Transmission',axis=1,inplace=True)","90b8f1e8":"#let's see the next column\ndata_needed.ProductSize       ","b61a78fe":"#it's the sizes of the product so i think we can play around so will fill in values with the perivous instant as x[1] which\n#is NaN will be filled with x[0]\ndata_needed.ProductSize.fillna(method = 'bfill', axis=0)","4be902be":"#coupler\ndata_needed.Coupler                 ","b80784df":"#as before fill in the next value to the NaN\ndata_needed.Coupler.fillna(method = 'bfill', axis=0)","b4c1f15e":"data_needed.fiSecondaryDesc         ","40c8b736":"data_needed.fiSecondaryDesc.fillna(method = 'bfill', axis=0)","901f00a4":"data_needed.Hydraulics.fillna(method = 'bfill', axis=0)            ","7918630b":"data_needed.auctioneerID             ","aa45d96c":"#Since we finally have a numerical type data we will fill with the median of the column\ndata_needed.auctioneerID = data_needed.auctioneerID.fillna(data_needed.auctioneerID.median())","82540b48":"#small number of NaN so will just drop it\ndata_needed.Enclosure.dropna()              ","3424939a":"df, y, nas = proc_df(data_needed, 'SalePrice')","d7ff0031":"#see the source code of the proc fn\n??proc_df","62b26f98":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(df, y)\nm.score(df,y)","059202ae":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 12000  # same as Kaggle's test set size\nn_trn = len(df)-n_valid\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","684c4acb":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","15e9e305":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","9f32b3eb":"m = RandomForestRegressor(n_estimators=40,n_jobs=-1,oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","4698f3ef":"preds = np.stack([t.predict(X_valid) for t in m.estimators_])","91dbbfe8":"preds[:,0], np.mean(preds[:,0]), y_valid[0]","c8af6909":"set_rf_samples(80000)","c4f0d747":"m = RandomForestRegressor(n_estimators=80, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","bb2eec12":"#reset_rf_samples()","09af0181":"m = RandomForestRegressor(n_estimators=80, n_jobs=-1, min_samples_leaf=3, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","b69bfa40":"m = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","a98ca1c0":"m = RandomForestRegressor(n_estimators=80, min_samples_leaf=4, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","f9d70c88":"\nWe revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods.","6b97d3a0":"Let's make use of the date here split it into useful information like day of year , day of month , it will be useful as there are factors like is it was working day or a holiday so that factor could be affecting the purchasing rate of any product","2fc3ba60":"Took quiet a time so we are taking just a subset from the whole data and cut the training data leaving the old validation data as it is","6404147d":"**No changes!!!!**\ndon't worry it works behind the scene trust me but if you don't :D you can run this\ndata_needed.UsageBand = data_needed.UsageBand.cat.codes\nand see in by your own eyes","3d5053f0":"We need to build fn that allows us to see the full rows\/columns in data when we hit .head() or .tail()","7c7d349d":"Seems that it's some types of machines so i will not be able to guess about it or fill in some values so i will drop that column","34c0b657":"https:\/\/youtu.be\/zvUOpbgtW3c Here's a video that demonstrates regression tree from there you have the basic idea about what is going on so what is the difference , Random forest are just more trees represented by parameter n_estimators that we will be using later and it outputs the average","c5ea8845":"# Take a first look at the data\n________\nThe first thing we'll need to do is load in the libraries and datasets we'll be using. For today, I'll be using a dataset of events that occured at auction based on its usage in Fast Iron store and predict the price of their products.\n\n> **Important!** Make sure you run this cell yourself or the rest of your code won't work!","af5d99d9":"fns below are representing the score function our model will be based on","8f337fe0":"**We have solved all our problems with the data so far but! **\nhow could we train our model with all these string data so we should figure out something that could help us to convert these data types to a numerical values, so next line of code is just we iterate through the data column name(n)\/values(c), and we intialize some categorical values for example 0 for high ,1 for low and so on to the string data after we convert it to a category type.","97ecc181":"We are going to see in a sample of our data to check for missing values, i added .T for transpose since we are having too many columns so to make it easier for me to see the instane for each column ","80432957":"If you go to overview then Evaluation, you will find how kaggle is going to measure the performance of your model so\nThe evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\n\nSample submission files can be downloaded from the data page. Submission files should be formatted as follows:\n\n    Have a header: \"SalesID,SalePrice\"\n    Contain two columns\n        SalesID: SalesID for the validation set in sorted order\n        SalePrice: Your predicted price of the sale\nso we need to build our score fn that upon it we will detect the performance first will change our sale price to log representation","36057c59":"Now let's split the data to training data and labels as x and y respectively ","5dfb35bc":"So now we will split our data to training\/validation splits in order to test the model we will build","a0d6f017":"Last we will apply proc_df fn which is function introduced by Fastai it just handle the NaN values we couldnt handle and convert the types of all columns across our dataset to numeric value so that we can use it in learning process","25776550":"Trying a different approach instead of just training on a subsample of the data we can actually let each tree train on different subsample of the whole data so in this case we can see the whole data and at the same time not taking much time and mitigating overfitting","4715144f":"**Handling missing values**","cd87c362":"Incresing the number of trees so we are able to learn more from data we have plus our new variable oob_score  which create different validation data from the subsample we cut from the dataset  so   whether to use out-of-bag samples to estimate the R^2 on unseen data.\n","7439257f":"Stack the prediction of each tree in this case 40 and print the mean of them and the actual value","d9c8de6e":"Well it seems that there are a bunch of data that has  90% missing values, so we are throwing away columns that have more than 60 percent missing data","f6695fbe":"max_features : int, float, string or None, optional (default=\u201dauto\u201d)\n\n   The number of features to consider when looking for the best split:\n        If int, then consider max_features features at each split.\n        If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n        If \u201cauto\u201d, then max_features=n_features.\n        If \u201csqrt\u201d, then max_features=sqrt(n_features).\n        If \u201clog2\u201d, then max_features=log2(n_features).\n        If None, then max_features=n_features.\n\n    Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n","4c6df5cd":"\n\nAnother way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with min_samples_leaf) that we require some minimum number of rows in every leaf node. This has two benefits:\n\n   There are less decision rules for each leaf node; simpler models should generalize better\n   The predictions are made by averaging more rows in the leaf node, resulting in less volatility\n\n","a284b149":"Because we are experimenting on just one data so accuarcy percentage will go up high but this might be prone to overfitting, please see the image below to understand what i am after\nhttps:\/\/raw.githubusercontent.com\/fastai\/fastai\/6ccb0f4e6c7ad88279dcf678da2b605e8e32aea8\/courses\/ml1\/images\/overfitting2.png\n","883dc63c":"Now i need to see the categories of the most intuitive column lets say ","ac5af9a5":"Code\nused for every state","cbf55907":"The first thing I do when I get a new dataset is take a look at some of it. This lets me see that it all read in correctly and get an idea of what's going on with the data. In this case, I'm looking to see if I see any missing values, which will be reprsented with `NaN` or `None`.","812c7a18":"Sorting the data according to date so later we can train on an earlier purchasing events then we can predict on later events"}}