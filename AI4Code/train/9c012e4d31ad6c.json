{"cell_type":{"4dc0bb22":"code","d959d4ff":"code","73950409":"code","db7c5b32":"code","68968102":"code","08673f6e":"code","cb4ec7e2":"code","4cafd6e8":"code","a92f15de":"markdown","8327eb81":"markdown","068620eb":"markdown","5baad384":"markdown"},"source":{"4dc0bb22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d959d4ff":"# to read cvs faster (uses GPU)\nimport cudf\n\n# to split dataset\nfrom sklearn.model_selection import train_test_split\n# model\nfrom xgboost import XGBClassifier\n# to split dataset in folds for cross-validation preserving the percentage of samples for each class\nfrom sklearn.model_selection import StratifiedKFold\n# to perform a randomized search for cross-validation\nfrom sklearn.model_selection import RandomizedSearchCV\n# to calculate the score\nfrom sklearn.metrics import roc_auc_score\n\n# garbage collector: to free-up memory when needed\nimport gc\n\n# to keep track of time\nimport time","73950409":"%%time\n# Loading data sets using cudf (faster) and coverts to pandas (DataFrame)\ntrain = cudf.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv', index_col = 'id').to_pandas()\nX_test = cudf.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv', index_col = 'id').to_pandas()\n\n\ntrain","db7c5b32":"# Check how many entries are missing per column, how many unique entries per column\ntrain.isnull().sum().values, train.dtypes.values, train.nunique().values","68968102":"# First let us separate X from y and divide the sets into search set - to perform a randomized search -\n# and opt set - to be fitted by the model with the best \"optimized hyperparameter set\" from the search.\nX = train.drop(columns='target')\ny = train.target\nX_search, X_opt, y_search, y_opt = train_test_split(X, y, train_size = 0.3, random_state=21)\n\n# since the datasets below won't be used here, we free up memory space by removing them\ndel train\ndel X\ndel y\ndel X_opt\ndel y_opt\ngc.collect()\n\n# Defining the model: 'gpu_hist' is important to run it faster with GPU\nmodel = XGBClassifier(tree_method='gpu_hist', use_label_encoder=False, eval_metric='auc', random_state=7)","08673f6e":"# Let us vary thorough the XGBoost paramenters to see which setup gives the best result (score)\n\nstart = time.time()\n\n# define the hyperparameters and the ranges to perform the scan\n# params_rnd = {'n_estimators':np.arange(100, 1000, 100),'learning_rate':np.arange(0.01, 0.31, 0.01),\n#           'max_depth':np.arange(3, 12, 1), 'subsample':np.arange(0.1, 1, 0.1), 'colsample_bytree':np.arange(0.1, 1.1, 0.1),\n#          'colsample_bylevel':np.arange(0.1, 1.1, 0.1), 'min_child_weight':np.arange(0,10,1), 'reg_alpha':np.arange(0,15,1), \n#               'reg_lambda':np.arange(0,30,1)}\n\n\nparams_rnd = {'n_estimators':np.arange(400, 1500, 50),'learning_rate':np.arange(0.006, 0.21, 0.001),\n          'max_depth':np.arange(3, 12, 1)}\n\n# for cross validation with 5 splits, using StratifiedKFold to keep the same percentage of sample per each class\nskfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n\n# defining the random scan, n_iter=7 (picks 7 random scenarios from params_rnd), using 'roc_auc' and the scoring method\nrnd_search = RandomizedSearchCV(model, params_rnd, n_iter=30, scoring='roc_auc', cv=skfold, random_state=7)\n\n\n# the model should fit the search set\nrnd_result = rnd_search.fit(X_search, y_search)\n\n# print the best score in the search and the corresponding best parameters\nprint(\"Best: %f using %s\" % (rnd_result.best_score_, rnd_result.best_params_))\n\n# print the total time...\nelapsed = time.time() - start\nprint(\"Seconds to run the scan: %f\" % (elapsed))\n\n# Search datasets won't be used anymore, so we remove them from memory\ndel X_search\ndel y_search\ngc.collect()","cb4ec7e2":"# now, using the tuned hyperparameters, we fit and test the model on the \"opt\" set \n# Defining the model\nmodel_opt = XGBClassifier(**rnd_result.best_params_, tree_method='gpu_hist', use_label_encoder=False, \n                          eval_metric='auc', random_state=7)\n\ntrain = cudf.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv', index_col = 'id').to_pandas()\nX = train.drop(columns='target')\ny = train.target\nX_search, X_opt, y_search, y_opt = train_test_split(X, y, train_size = 0.3, random_state=21)\n\n# Search datasets won't be used anymore, so we remove them from memory\ndel X\ndel y\ndel X_search\ndel y_search\ngc.collect()\n\nmodel_opt.fit(X_opt, y_opt)\n\ndel X_opt\ndel y_opt\ngc.collect()","4cafd6e8":"### We calculate and store the probability of the positive prediction\npred_test = model_opt.predict_proba(X_test)[:,1]\n\n\noutput = pd.DataFrame({'id': X_test.index,\n                       'target': pred_test})\noutput.to_csv('submission_TPSOct21.csv', index=False)","a92f15de":"****With the model determined, we use it to make predictions based on the X_test set and save them in a csv file to submit for the competition****","8327eb81":"****Having found the best parameter set in the scan, we use these hyperparameters to fit the remaining dataset, i.e. X_opt and y_opt****","068620eb":"****Now, we proceed to perform a ramdomized scan within \"interesting\" hyperparameter regions****","5baad384":"****From the results above, we see that there are no missing values and no categorical entries.\nThere are features with only 2 unique entries.****\n\n****For now, let us not do any preprocessing on the data to see how the model performs****"}}