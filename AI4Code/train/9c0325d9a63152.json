{"cell_type":{"41739a05":"code","433f96ce":"code","dadc5bbf":"code","a9f13cb6":"code","7892818a":"code","9b9dd4fd":"code","32daf675":"code","44064d20":"code","737ec5e7":"code","8f476330":"code","8d016a29":"code","0b95d9f3":"code","4a71c54e":"code","ce620fae":"code","4626e4c0":"code","1abe4eb5":"code","86de1b7c":"code","87376711":"code","3ed39f7b":"code","d1167371":"code","492e8106":"code","6aeefa8e":"code","75022a6e":"code","2e230d68":"code","ab308924":"code","eae2d59a":"code","3e1f2678":"code","dd3b006c":"code","0b834cc5":"code","e79f392d":"code","943d03b7":"code","6b79765c":"code","c5942d0d":"code","ba783c86":"code","cb60dfd1":"code","35301452":"markdown","c6d9bf05":"markdown","e8a95fca":"markdown","9780bf3d":"markdown","8f24bbce":"markdown","d2c57fb8":"markdown","2a478507":"markdown","1437de0f":"markdown","86937d2f":"markdown","adc29be5":"markdown","6fedd204":"markdown","47c83c77":"markdown","bbf4d30f":"markdown","3d22549f":"markdown","cecb44bc":"markdown","894cb1bd":"markdown","f08734b1":"markdown","1c2bd3f3":"markdown","dc36101e":"markdown","cad67393":"markdown","61097f47":"markdown","5dfead0b":"markdown","c33200e2":"markdown"},"source":{"41739a05":"# load the file with the top results\nimport numpy as np\nimport pandas as pd\ntop20 = pd.read_csv('..\/input\/ethics\/ethic_paper.csv')\npd.set_option('max_colwidth', 150)\ntop20[['cord_uid','title']].head(20)","433f96ce":"# load all the required libraries\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\n\n# load the raw data\npaper = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')","dadc5bbf":"length_of_df = len(paper)\nprint(f'Before cleaning, there are {length_of_df} entries in the metadata.csv file.')","a9f13cb6":"# cleaning pipline\n''' 1. keeping the columns: cord_uid, title, abstract\n    2. remove duplication in the title column, because they may have different cord_uid, \n       and not being recognized as duplicated entry\n    3. remove NA in the abstract column, because the analysis will use the words from the \n       abstract column\n    4. reset the index of the dataframe'''\n\npaper.drop(columns=['sha','source_x','doi','pmcid','pubmed_id','license', 'publish_time',\n                    'authors', 'journal','Microsoft Academic Paper ID',\n                    'WHO #Covidence', 'has_pdf_parse','has_pmc_xml_parse',\n                    'full_text_file', 'url'], \n           inplace = True)\n\npaper.drop_duplicates(subset=['title'], keep = 'first', inplace = True)\n\npaper.dropna(subset = ['abstract'], inplace = True)\n\npaper.reset_index(drop=True)","7892818a":"length_of_df = len(paper)\nprint(f'After cleaning, there are {length_of_df} entries in dataframe, \"paper\".')","9b9dd4fd":"# need to install en_core_sci_lg, as it is not pre-installed on Kaggle.\n\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","32daf675":"# import the natural language processing library and PhraseMatcher\nimport spacy\nimport en_core_sci_lg\nfrom spacy.matcher import PhraseMatcher\n\n# load large science tokenizer, tagger, parser NER and word vectors\nnlp = en_core_sci_lg.load()\n\n# define the matcher\nmatcher = PhraseMatcher(nlp.vocab)","44064d20":"# define the phrases for each sub-topics\nmoral_list = ['health care providers',\n              'health care workers',\n              'ethical principles',\n              'professional moral duty',\n              'duty to care',\n              'obligation', \n              'moral',\n              'duty',\n              'decisions',\n              'dilemmas']\n\neducation_list = ['education',\n                  'access',\n                  'capacity building',\n                  'ethics']\n               \nmeasure_list = ['qualitative assessment',\n                'public health measures',\n                'secondary impacts',\n                'prevention',\n                'surgical masks',\n                'social distancing',\n                'school closures']\n        \npsy_list = ['psychological health',\n            'immediate needs',\n            'fear',\n            'anxiety']\n\nmisinfo_list = ['stigma',\n                'misinformation',\n                'rumor',\n                'social media']\n\ncovid_list = ['COVID-19',\n              'novel coronavirus 2019',\n              '2019 novel coronavirus',\n              'new coronavirus pneumonia',\n              'SARS-CoV-2',\n              'coronavirus disease 2019'\n             ]\n\n# convert the phrase to a Doc object\nphrase_pattern_moral = [nlp(text) for text in moral_list]\nphrase_pattern_education = [nlp(text) for text in education_list]\nphrase_pattern_measure = [nlp(text) for text in measure_list]\nphrase_pattern_psy = [nlp(text) for text in psy_list]\nphrase_pattern_misinfo = [nlp(text) for text in misinfo_list]\nphrase_pattern_covid = [nlp(text) for text in covid_list]\n\n# pass the phrase_pattern to the matcher\nmatcher.add('Moral', None, *phrase_pattern_moral)\nmatcher.add('Education', None, *phrase_pattern_education)\nmatcher.add('Measure', None, *phrase_pattern_measure)\nmatcher.add('Psychological', None, *phrase_pattern_psy)\nmatcher.add('Misinformation', None, *phrase_pattern_misinfo)\nmatcher.add('COVID', None, *phrase_pattern_covid)","737ec5e7":"def matchID_to_matchString(match_output):\n    '''this function coverts the result of Phrasematcher, match ID into the match string.\n       \n       input: Phrasematcher output, list of number with match ID, the location of the match\n       string in the text. The function will use the first entry of each item in the list, \n       and convert it back to the string.\n       \n       output: a list of unqiue match string, which are the sub-topics (repeated matches are removed)\n    '''\n    \n    match_string_list = list()\n    \n    for i in range(0, len(match_output)):\n        match_string = nlp.vocab.strings[match_output[i][0]]\n        \n        if match_string not in match_string_list:\n            match_string_list.append(match_string)\n            \n    return match_string_list\n\n\ndef match_all_keywords(abstract_nlp, match_ouput):\n    '''this function covert the match output to individual keywords.\n       \n       input: tokenized abstract and the match output\n       output: a list of keywords found in the abstract\n    '''\n    \n    keywords = list()\n    \n    for i in range(0, len(match_ouput)):\n        start_id, start_end = match_ouput[i][1], match_ouput[i][2]\n        keywords.append(abstract_nlp[start_id:start_end].text)\n        \n    return keywords\n\n\ndef find_unique_keywords(all_keywords):\n    '''this function find the unique keywords from the all_keywords list\n    \n       input: a list of all keywords\n       output: a list of unique keywords\n    '''\n    \n    unique_list = list()\n    \n    for words in all_keywords:\n        if words not in unique_list:\n            unique_list.append(words)\n            \n    return unique_list\n\ndef one_hot_encoding(df, col_name, topics):\n    '''this function is a one_hot_encoding process to convert a list of match_string or unique_keywords into\n       columns with 0\/1 with the column name representing the sub-topic or keywords.\n       \n       input: \n             df:the dataframe that the new columns will be added\n             col_name: it can be the columns match_string if sub-topic is intented to be unpacked; or \n                       unique_keywords if indivdiual keywords is intented to be unpacked\n             word_list:  a list of sub-topics in match_string or a list of unique_keywords,\n                        it should be each entry of the column: 'match_string' or 'unique_keywords'\n       ouput: the dataframe with new column\n       \n    '''\n    def unpack_topic(word_list, topic):\n        \n        is_topic = list()\n        \n        for item in word_list:\n            is_topic.append(topic in item)\n        return sum(is_topic)\n\n    for topic in topics:\n        df[topic] = df[col_name].apply(lambda x: unpack_topic(x, topic))\n    \n    return df","8f476330":"# additional functions for analysis on individual keywords level\n\ndef count_keywords(unique_keywords):\n    '''this function count the unique keywords of the whole dataframe'''\n    \n    keyword_count = dict()\n    keywords_list = list()\n    \n    for row in unique_keywords:\n        keywords_list.extend(row)\n        \n    for word in keywords_list:\n        if word in keyword_count:\n            keyword_count[word] += 1\n        else:\n            keyword_count[word] = 1\n\n    return keyword_count\n\ndef remove_covid(count_list):\n    for word in covid_list:\n        count_list.pop(word)\n    return count_list","8d016a29":"# tokenizer the text in abstract\npaper['abstract_nlp'] = paper['abstract'].apply(lambda x: nlp(x))","0b95d9f3":"# match the phrases to the tokenized abstract\npaper['match_output'] = paper['abstract_nlp'].apply(lambda x: matcher(x))\n\n# convert the phrasematcher output into strings\npaper['match_string'] = paper['match_output'].apply(matchID_to_matchString)\n\n# find all the keywords\/phrase in the string output of the phrasematcher\npaper['all_keywords'] = paper.apply(lambda x: match_all_keywords(x['abstract_nlp'],x['match_output']), axis = 'columns')\n\n# find the unique keywords, ie. remove duplications\npaper['unique_keywords'] = paper['all_keywords'].apply(find_unique_keywords)\n\n# count the number of topics and the number of unique keywords\npaper['no_topics'] = paper['match_string'].apply(len)\npaper['no_unique_keywords'] = paper['unique_keywords'].apply(len)\n\n# remove unwanted columns\npaper.drop(columns = ['abstract_nlp','match_output'], inplace = True)","4a71c54e":"# one-hot-encoding to covert the data in list to individual columns\nsub_topics = ['Moral', 'Education', 'Measure', 'Psychological', 'Misinformation', 'COVID']\none_hot_encoding(paper, 'match_string', sub_topics)","ce620fae":"# subset the papers related to COVID-19\ncovid_paper = paper.loc[paper['COVID'] > 0].copy()","4626e4c0":"# count number of unique keywords that is present (not counting keywords related to COVID-19)\ncovid_ethics_list = count_keywords(covid_paper['unique_keywords'])\ncovid_ethics_list = remove_covid(covid_ethics_list)\nkeywords_stat = pd.DataFrame.from_dict(covid_ethics_list, orient = 'index', columns = ['counts'])\nkeywords_stat = keywords_stat.sort_values(by = ['counts'], ascending = False)","1abe4eb5":"# adding each keywords to the dataframe\nkeywords_list = keywords_stat.index\none_hot_encoding(covid_paper, 'unique_keywords', keywords_list)","86de1b7c":"# add a column with the number of ethic topics\ncovid_paper['ethic_topics'] = covid_paper['no_topics'].apply(lambda x: x-1)\n\n# add a column with the number of ethic topics\ncovid_paper['no_ethics_keywords'] = covid_paper.apply(lambda x: sum(x[keywords_list]), axis = 1)\n\n# sort the dataframe with the number of topic, and the number of unique keywords\ncovid_paper.sort_values(by = ['ethic_topics', 'no_ethics_keywords'], ascending = False, inplace = True)\n\n# output the COVID-19 papers \ncovid_paper.to_csv('covid_paper.csv')","87376711":"# output to csv paper with COVID-19 and at least 1 ethic search phrase\/keywords\nethic_paper = covid_paper.loc[covid_paper['no_ethics_keywords']> 1].copy()\nethic_paper.to_csv('ethic_paper.csv')","3ed39f7b":"length_of_covid = len(covid_paper)\nprint(f'There are {length_of_covid} research papers related to COVID-19.')","d1167371":"length_of_ethic = sum(covid_paper['ethic_topics'] > 0)\nprint(f'There are {length_of_ethic} research papers related to COVID-19, and contain ethic-related keywords.')","492e8106":"covid_summary = covid_paper.groupby(by = ['COVID']).sum()\n\nmoral_no = covid_summary['Moral'][1]\nedu_no = covid_summary['Education'][1]\nmeasure_no = covid_summary['Measure'][1]\npsychological_no = covid_summary['Psychological'][1]\nmisinfo_no = covid_summary['Misinformation'][1]\n\nprint(f'Within those, {moral_no} papers contain keywords in sub-topic: moral;')\nprint(f'              {edu_no} papers contain keywords in sub-topic: education;')\nprint(f'              {measure_no} papers contain keywords in sub-topic: measure;')\nprint(f'              {psychological_no} papers contain keywords in sub-topic: psychological health;')\nprint(f'              {misinfo_no} papers contain keywords in sub-topic: misinformation.')","6aeefa8e":"# examine how many paper contain the search phrases\/keywords\nplt.rcParams['figure.figsize'] = [10, 8]\nplt.figure()\nplot1 = paper['no_topics'].hist(bins= 10)\nplot1.set_xlabel('Number of topics')\nplot1.set_ylabel('Number of research papers')","75022a6e":"# examine how many paper has the increased topic and with COVID-19 \n\nplt.rcParams['figure.figsize'] = [10, 8]\nplt.figure()\n\npaper_summary = paper.groupby(by = ['COVID']).sum()\n\nplot2 = paper_summary[['Moral', 'Education', 'Measure', \n               'Misinformation','Psychological']].plot(kind = 'barh')\nplot2.set_xlabel('Number of research papers')\nplot2.set_ylabel('Contain COVID-19')","2e230d68":"# examine how many paper has ethic topics within the COVID-19 papers\nplt.rcParams['figure.figsize'] = [10, 8]\nplt.figure()\n\ncovid_sum = covid_paper.groupby(['ethic_topics']).sum()\nplot3 = covid_sum[['Moral', 'Education', 'Measure', \n           'Misinformation','Psychological']].plot(kind = 'bar')\n\nplot3.set_xlabel('number of sub-topics')\nplot3.set_ylabel('Number of research papers')","ab308924":"plt.rcParams['figure.figsize'] = [10, 8]\nplt.figure()\n\nplot4 = keywords_stat.plot(kind = 'barh',legend=False)\nplot4.set_xlabel('Number of research papers')\nplot4.set_ylabel('search phrases\/keywords')","eae2d59a":"# output the results\ndef output_title(df, keyword):\n    new_df = df.loc[df[keyword] == 1].copy()\n    new_df.sort_values(by = ['no_ethics_keywords'], ascending = False, inplace = True)\n    return new_df.filter(items = ['cord_uid', 'title','unique_keywords'])","3e1f2678":"pd.set_option('max_colwidth', 150)\nbroad_ethic_papers = covid_paper.loc[covid_paper['ethic_topics']> 1].copy()","dd3b006c":"print(f'There are {len(broad_ethic_papers)} papers contain serach phrases\/keywords of more than 2 sub-topics.')\nprint('Within those papers are under the topics Moral, Education and Psychological health')","0b834cc5":"output_title(broad_ethic_papers, 'Moral')","e79f392d":"output_title(broad_ethic_papers, 'Education')","943d03b7":"output_title(broad_ethic_papers, 'Psychological')","6b79765c":"# subset for papers with more than 1 unqiue keywords and only 1 sub-topic\nethic_subtopic = covid_paper.loc[(covid_paper['no_ethics_keywords']> 1) & (covid_paper['ethic_topics']<2)].copy()","c5942d0d":"output_title(ethic_subtopic, \"Psychological\")","ba783c86":"output_title(ethic_subtopic, \"Measure\")","cb60dfd1":"output_title(ethic_subtopic, \"Misinformation\")","35301452":"<a id=\"education\"><\/a>\n### Research papers on education\n* Efforts to support sustained education, access, and capacity building in the area of ethics","c6d9bf05":"After looking at the papers with more than one sub-topic, we can look at papers that have more than one keywords in only one sub-topic.","e8a95fca":"<a id=\"visualizations\"><\/a>\n### Visualizations","9780bf3d":"<a id=\"discussion\"><\/a>\n## Discussion","8f24bbce":"<a id=\"top57\"><\/a>\n### Research papers with more than one sub-topics related to ethics","d2c57fb8":"<a id=\"data_processing\"><\/a>\n## Data processing","2a478507":"<a id=\"summary\"><\/a>\n## Summary:\n\nUsing PhraseMatcher from the NLP library Spacy, with customized search phrases and keywords, out of 41848 of research papers with abstract available (accoring cleaned data, version 7), 2214 papers are related to COVID-19.\n\nWithin those, 63 papers contain keywords in sub-topic: moral;\n              83 papers contain keywords in sub-topic: education;\n              334 papers contain keywords in sub-topic: measure;\n              51 papers contain keywords in sub-topic: psychological health;\n              20 papers contain keywords in sub-topic: misinformation.","1437de0f":"Figure 2: Bar chart showing the distribution of sub-topics found in the research paper.(1: COVID-19 papers, 0: non-COVID-19 papers)\nWithin those 5000 papers that contains our search phrases, only a small subset of papers is related to COVID-19.","86937d2f":"<a id=\"psychology\"><\/a>\n### Research papers on psychological health\n* Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.","adc29be5":"### Top 20 research papers on ethics","6fedd204":"Figure 3. Bar chart showing the number of ethic topics found within the COVID-19 paper subset.\nMost of the papers only contain 1 sub-topics, with the sub-topic 'measure' being the top. ","47c83c77":"<a id=\"results\"><\/a>\n## Results","bbf4d30f":"<a id=\"measure\"><\/a>\n### Research papers on measure\n* Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)","3d22549f":"<a id=\"strategy\"><\/a>\n## Strategy","cecb44bc":"This study takes advandage of the PhraseMatcher from the NLP library Spacy, not only individual keywords, but also tokenized phrases can be use to find matched within a tokenized text. \n\nIn this study, the abstract of the reseachers papers will be tokenized to be searched with custom defined keywords and search phrases on the topic ethics. This is because the abstract usually contain the important keywords and main messages of the whole research paper while the full text contain more information that might not be directly related to the questions that we wanted to address. \n\nAccording to the task details, the following search phrases\/keywords are used for each sub-topic. Notice that some of the search phrases and keywords are not provided in the task, but rather by the common-sense and antipiation of what keywords would show up in the research papers that answer the question. I defined \"Moral\", \"Education\", \"Measure\", \"Psychological health\" and \"Misinformation\" sub-topics, while ethic is our main topic.\n\n* Moral: 'health care providers','health care workers', 'ethical principles', 'professional moral duty', 'duty to care', 'obligation', 'moral', 'duty', 'decisions','dilemmas'\n\n* Education: 'education', 'access', 'capacity building', 'ethics'\n               \n* Measure: 'qualitative assessment', 'public health measures', 'secondary impacts', 'prevention', 'surgical masks', 'social distancing', 'school closures'\n        \n* Psychologycal health: 'psychological health', 'immediate needs', 'fear', 'anxiety'\n\n* Misinformation: 'stigma', 'misinformation', 'rumor', 'social media'\n\n* COVID-19: 'COVID-19', 'novel coronavirus 2019', '2019 novel coronavirus', 'new coronavirus pneumonia','SARS-CoV-2', 'coronavirus disease 2019'\n            \nAs we are interested in the research papers related to the current COVID-19 outbreak, the papers with COVID-19 related will be first selected. The papers are ranked by the number of unique keywords and the the number of sub-topics found in the papers. \n\nIt is possible that a paper can contain keywords\/phrases in multiple sub-topics, which means that this paper covers a broad range of topic of our interest. On the other hand, it is possible that a paper contain multiple keywords or phrases within a particular sub-topics. The higher the number of unique keywords\/phrases, the more likely that paper is relavant to the questions we are interested in. ","894cb1bd":"# Comprehensive COVID-19 research papers analysis on the topic ethics","f08734b1":"## Outline:\n\n* [Summary](#summary)\n* [Strategy](#strategy)\n* [Data processing](#data_processing)\n* [Results](#results)\n    - [Visualizations](#visualizations)\n    - [Research papers with more than one sub-topics related to ethics](#top57)\n    - [research papers on moral](#moral)\n    - [research papers on education](#education)\n    - [research papers on measure](#measure)\n    - [research papers on psychological health](#psychology)\n    - [research papers on misinformation](#misinformation)\n       \n* [Discussion](#discussion)","1c2bd3f3":"<a id=\"misinformation\"><\/a>\n### Research papars on misinformation\n* Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.","dc36101e":"![Screenshot%202020-04-09%20at%2020.32.41.png](attachment:Screenshot%202020-04-09%20at%2020.32.41.png)","cad67393":"#### Pros:\n* Using Spacy and the PhraseMatcher are relative simply and easy, no specific or in-depth maths is needed.\n* The whole study didn't require specific hardware or computation power, which can be run on a laptop. The computation time is also considered as short.\n* Customized search phrases and keywords generate better and more specific results, especifically for topics that are under-represented in the dataset. Compare to other methods which depends on the frequency of the keywords on a particular topic to be discovered, such as topic-modeling with Latent dirichlet allocation, topics like ethics that are not frequently presented in the dataset are likely to be missed.\n\n#### Cons:\n* The quality of the results depends on the quality of the search phrases and keywords, which means that subject-matter experts may be needed to provide useful search phrases and keywords in order to have the optimal results.\n* Some of the keywords can have different meanings in different context, and thus generate some off-topic results, such as 'access', 'prevention', 'decisions' are considered to be too general and give search results that are not related to the ethic topic. However, co-exisitence with other keywords helps define ethic-related papers.\n* The use of sub-topic to help narrow down the focus may not be the best approach. For example, the paper 'An \"Infodemic\": Leveraging High-Volume Twitter Data to Understand Public Sentiment for the COVID-19 Outbreak' didn't have any serach phrase\/keywords from the sub-topic 'misinformation', however, it is being picked up because the present of other serach phrase\/keywords, in this case,'prevention', 'fear'. Therefore, human being is still needed to validate the serach results.\n\n#### Suggested further studies:\n* New search phrases and keywords can be generated from the verified research papers from this study, and use for another round of PhraseMatcher search. \n* From the research papers found in this study, some of the papers focus on the author's own country. It would be interesting to compare the response and on ethical issues across different geographical locations.\n* Upon examination of the results, there may still be duplications in the titles which is not found because of the special charachers or the name of the journal , more thoroughful data cleaning process maybe needed. \n* The PhraseMatcher method can be used for other tasks in this challenge.\n\n#### Acknowledgement\nThank you for reading.\n\nThanks all the developers for the libraries and infrastructure, and thanks all Kagglers for their contributions which inspired this study. ","61097f47":"Figure 4. Bar chart showing the number of individual keywords found within the COVID-19 papers. \n'prevention' and 'social distancing' are the top two most frequently found keywords.","5dfead0b":"Figure 1. Histogram showing how many research papers contains the search phrases\/keywords. \nIn total, close to 5000 research papers contain the search phrases (including both ethic-related and COVID-19).","c33200e2":"<a id=\"moral\"><\/a>\n### Research papers on moral\nSpecific tasks:\n* Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\n* Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\""}}