{"cell_type":{"37dd8d7d":"code","c459f977":"code","8c9716b4":"code","32c24fd3":"code","f07b14af":"code","99ba2ee0":"code","44c6c8e8":"code","31599134":"code","4819c039":"code","693267f7":"code","309092bd":"code","a685f1fd":"code","cd4290c4":"code","33a06501":"code","9442c8db":"code","f99a8279":"code","db7101b1":"code","463754e9":"code","c4214bcb":"code","5b195b4d":"code","6ab502f1":"code","8d2b4a1d":"code","1704be00":"code","844fe3f7":"code","cdac9d97":"code","52a83d5a":"code","af46af04":"code","6b4a9010":"code","ebe4b4bf":"code","3ef4b896":"code","9ac4e645":"code","6a97a6d9":"code","0a1494b9":"code","e6b58241":"code","4ad70460":"code","928a9582":"code","734bbec9":"code","5106dfcf":"code","66728fef":"code","2e718366":"code","7e2652d0":"code","d1bc8c9d":"code","41026ce2":"code","7cb60db1":"code","b78b8c56":"code","062e92cf":"code","2463c7bf":"code","50305013":"code","ee602eb9":"code","a4fa07b9":"code","564dcd39":"code","21f89b8a":"code","5639e933":"code","a72b5f9b":"code","3252162c":"code","4af272e8":"code","b2c0d2b5":"code","f86bcfce":"code","cfbfffe7":"code","a05b2d16":"code","93f14219":"code","cadbbbe2":"code","383230d6":"code","98bc6651":"code","f68cf171":"markdown","a9515e49":"markdown","d48342ff":"markdown","79da9912":"markdown","2a7bc2eb":"markdown","1db9f350":"markdown","9b7966e6":"markdown","e2900be0":"markdown","6d981179":"markdown","bb71e973":"markdown","d4392b2f":"markdown","02d61b0c":"markdown","aa6b5fcf":"markdown","06a99af3":"markdown","2a3d5f02":"markdown","a2073353":"markdown","ba08d623":"markdown","24751c8f":"markdown","b87c63d0":"markdown","dda558b1":"markdown","72af9112":"markdown","988b98e4":"markdown","597e96a1":"markdown","5d22a361":"markdown","f83cd03d":"markdown","24dee7a3":"markdown","94e69930":"markdown","9f9cd053":"markdown","1dd147e6":"markdown","c87cdef5":"markdown","fc41a908":"markdown","7358cbe3":"markdown","798417f6":"markdown","c13ca73e":"markdown","dc290ab7":"markdown","681950d5":"markdown","82186f61":"markdown","b6967382":"markdown","49bfe8e6":"markdown","8984f400":"markdown","9e2416d3":"markdown","24d02c20":"markdown","0d1369c2":"markdown","202a7b10":"markdown","aba17f60":"markdown","7c7a9890":"markdown","c9b39a76":"markdown","dde17d29":"markdown"},"source":{"37dd8d7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import roc_auc_score\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\nfrom sklearn import preprocessing\n\nimport gc, datetime, random\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c459f977":"def prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\n    vega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workarounds = f\"\"\"    requirejs.config({{\n        baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workarounds\n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"<\/script>\",\n)))","8c9716b4":"sample_sub = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')\nsample_sub.head(10)","32c24fd3":"del sample_sub","f07b14af":"train_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')","99ba2ee0":"train_identity.head(5)","44c6c8e8":"train_transaction.head(5)","31599134":"train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","4819c039":"print(f'Train dataset: {train.shape[0]} rows & {train.shape[1]} columns')\nprint(f'Test dataset: {test.shape[0]} rows & {test.shape[1]} columns')","693267f7":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","309092bd":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","a685f1fd":"del train_identity, train_transaction, test_identity, test_transaction","cd4290c4":"cat_cols = list(train.select_dtypes(include=['object']).columns)\ncat_cols","33a06501":"data_null = train.isnull().sum()\/len(train) * 100\ndata_null = data_null.drop(data_null[data_null == 0].index).sort_values(ascending=False)[:500]\nmissing_data = pd.DataFrame({'Missing Ratio': data_null})\nmissing_data.head()","9442c8db":"missing_data.shape","f99a8279":"def get_too_many_null_attr(data):\n    many_null_cols = [col for col in data.columns if data[col].isnull().sum() \/ data.shape[0] > 0.9]\n    return many_null_cols","db7101b1":"def get_too_many_repeated_val(data):\n    big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n    return big_top_value_cols","463754e9":"def get_useless_columns(data):\n    too_many_null = get_too_many_null_attr(data)\n    print(\"More than 90% null: \" + str(len(too_many_null)))\n    too_many_repeated = get_too_many_repeated_val(data)\n    print(\"More than 90% repeated value: \" + str(len(too_many_repeated)))\n    cols_to_drop = list(set(too_many_null + too_many_repeated))\n    cols_to_drop.remove('isFraud')\n    return cols_to_drop","c4214bcb":"cols_to_drop = get_useless_columns(train)","5b195b4d":"plt.subplots(figsize=(40,10))\nplt.xticks(rotation='90')\nsns.barplot(data_null.index, data_null)\nplt.xlabel('Features', fontsize=20)\nplt.ylabel('Missing rate', fontsize=20)","6ab502f1":"train['id_03'].value_counts(dropna=False, normalize=True).head()","8d2b4a1d":"train['id_11'].value_counts(dropna=False, normalize=True).head()","1704be00":"list(train.columns)","844fe3f7":"for i in range(1, 10):\n    print(train['id_0' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')\n    \nfor i in range(10, 39):\n    print(train['id_' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","cdac9d97":"charts = {}\ninfo = []\nfor i in range(12, 39):\n    info.append('id_' + str(i))\nfor i in info:\n    width_len = 400\n    if i in ['id_30', 'id_31', 'id_33']:\n        width_len = 600\n    feature_count = train[i].value_counts(dropna=False).reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                y=alt.Y(f\"{i}:N\", axis=alt.Axis(title=i)),\n                x=alt.X('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=width_len)\n    charts[i] = chart                         \n\n\nfor i in ['id_30', 'id_31', 'id_33']:\n    feature_count = train[i].value_counts(dropna=False)[:40].reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n                y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=800)\n    charts[i] = chart\n    \nrender((charts['id_12'] | charts['id_15']) & \n       (charts['id_16'] | charts['id_23']) & \n       (charts['id_27'] | charts['id_28']) & \n       (charts['id_29'] | charts['id_34']) & \n       (charts['id_35'] | charts['id_36']) &\n       (charts['id_37'] | charts['id_38']))\n","52a83d5a":"render(charts['id_30'] & charts['id_31'] & charts['id_33'])","af46af04":"for i in ['DeviceType', 'DeviceInfo']:\n    feature_count = train[i].value_counts(dropna=False)[:40].reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n                y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=800)\n    charts[i] = chart\n    \nrender(charts['DeviceType'] & charts['DeviceType'])","6b4a9010":"plt.hist(train['TransactionDT'], label='train');\nplt.hist(test['TransactionDT'], label='test');\nplt.legend();\nplt.title('Transaction dates');","ebe4b4bf":"for i in range(1, 7):\n    print(train['card' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","3ef4b896":"for i in range(1, 15):\n    print(train['C' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","9ac4e645":"for i in range(1, 16):\n    print(train['D' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","6a97a6d9":"for i in range(1, 10):\n    print(train['M' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","0a1494b9":"charts = {}\ninfo = []\nfor i in range(1, 10):\n    info.append('M' + str(i))\nfor i in info:\n    feature_count = train[i].value_counts(dropna=False).reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                y=alt.Y(f\"{i}:N\", axis=alt.Axis(title=i)),\n                x=alt.X('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=400)\n    charts[i] = chart                         \n    \nrender((charts['M1'] | charts['M2'] | charts['M3']) & (charts['M4'] | charts['M5'] | charts['M6']) & (charts['M7'] | charts['M8'] | charts['M9']))","e6b58241":"del charts","4ad70460":"def seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    ","928a9582":"SEED = 42\nseed_everything(SEED)\nTARGET = 'isFraud'\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","734bbec9":"def addNewFeatures(data): \n    data['uid'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n\n    data['uid2'] = data['uid'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n\n    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n    \n    return data","5106dfcf":"train = addNewFeatures(train)\ntest = addNewFeatures(test)","66728fef":"i_cols = ['card1','card2','card3','card5','uid','uid2','uid3']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n\n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n\n        train[new_col_name] = train[col].map(temp_df)\n        test[new_col_name]  = test[col].map(temp_df)","2e718366":"train = train.replace(np.inf,999)\ntest = test.replace(np.inf,999)","7e2652d0":"train['TransactionAmt'] = np.log1p(train['TransactionAmt'])\ntest['TransactionAmt'] = np.log1p(test['TransactionAmt'])","d1bc8c9d":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","41026ce2":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nuknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].fillna(uknown)\n    df[r] = df[r].fillna(uknown)\n    \n    # Check if P_emaildomain matches R_emaildomain\n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \ntrain=setDomain(train)\ntest=setDomain(test)","7cb60db1":"def setTime(df):\n    df['TransactionDT'] = df['TransactionDT'].fillna(df['TransactionDT'].median())\n    # Temporary\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n    \n    df['DT_hour'] = df['DT'].dt.hour\n    df['DT_day_week'] = df['DT'].dt.dayofweek\n    df['DT_day'] = df['DT'].dt.day\n    \n    # Lets transform D8 and D9 column\n    # As we almost sure it has connection with hours\n    df['D9_not_na'] = np.where(df['D9'].isna(),0,1)\n    df['D8_not_same_day'] = np.where(df['D8']>=1,1,0)\n    df['D8_D9_decimal_dist'] = df['D8'].fillna(0)-df['D8'].fillna(0).astype(int)\n    df['D8_D9_decimal_dist'] = ((df['D8_D9_decimal_dist']-df['D9'])**2)**0.5\n    df['D8'] = df['D8'].fillna(-1).astype(int)\n\n    return df\n    \ntrain=setTime(train)\ntest=setTime(test)","b78b8c56":"train[\"lastest_browser\"] = np.zeros(train.shape[0])\ntest[\"lastest_browser\"] = np.zeros(test.shape[0])\n\ndef setBrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\ntrain=setBrowser(train)\ntest=setBrowser(test)","062e92cf":"def setDevice(df):\n    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n    \n    df['device_name'] = df['DeviceInfo'].str.split('\/', expand=True)[0]\n\n    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    df['had_id'] = 1\n    gc.collect()\n    \n    return df\n\ntrain=setDevice(train)\ntest=setDevice(test)","2463c7bf":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo','device_name',\n          'id_30','id_33',\n          'uid','uid2','uid3',\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n    train[col+'_fq_enc'] = train[col].map(fq_encode)\n    test[col+'_fq_enc']  = test[col].map(fq_encode)\n\n\nfor col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train[col+'_total'] = train[col].map(fq_encode)\n    test[col+'_total']  = test[col].map(fq_encode)\n\nperiods = ['DT_M','DT_W','DT_D']\ni_cols = ['uid']\nfor period in periods:\n    for col in i_cols:\n        new_column = col + '_' + period\n            \n        temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n        fq_encode = temp_df[new_column].value_counts().to_dict()\n            \n        train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n        test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n        \n        train[new_column] \/= train[period+'_total']\n        test[new_column]  \/= test[period+'_total']","50305013":"train = train.drop(cols_to_drop, axis=1)","ee602eb9":"class ModifiedLabelEncoder(LabelEncoder):\n    def fit_transform(self, y, *args, **kwargs):\n        return super().fit_transform(y).reshape(-1, 1)\n\n    def transform(self, y, *args, **kwargs):\n        return super().transform(y).reshape(-1, 1)","a4fa07b9":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attr):\n        self.attributes = attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attributes].values","564dcd39":"noisy_cols = [\n    'TransactionID','TransactionDT',                      # Not target in features))\n    'uid','uid2','uid3',                                 \n    'DT','DT_M','DT_W','DT_D',       \n    'DT_hour','DT_day_week','DT_day',\n    'DT_D_total','DT_W_total','DT_M_total',\n    'id_30','id_31','id_33',\n    'D1', 'D2', 'D9',\n]\n\nnoisy_cat_cols = list(train[noisy_cols].select_dtypes(include=['object']).columns) \nnoisy_num_cold = list(train[noisy_cols].select_dtypes(exclude=['object']).columns)","21f89b8a":"cat_attr = list(train.select_dtypes(include=['object']).columns)\nnum_attr = list(train.select_dtypes(exclude=['object']).columns)\nnum_attr.remove('isFraud')\n\nfor col in noisy_cat_cols:\n    if col in cat_attr:\n        print(\"Deleting \" + col)\n        cat_attr.remove(col)\nfor col in noisy_num_cold:\n    if col in num_attr:\n        print(\"Deleting \" + col)\n        num_attr.remove(col)","5639e933":"num_pipeline = Pipeline([\n        ('selector', DataFrameSelector(num_attr)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('scaler', StandardScaler()),\n    ]) \n\ncat_pipeline = Pipeline([\n        ('selector', DataFrameSelector(cat_attr)),\n        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n    ])\n\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline', num_pipeline),\n    ('cat_pipeline', cat_pipeline),\n])","a72b5f9b":"def encodeCategorical(df_train, df_test):\n    for f in df_train.drop('isFraud', axis=1).columns:\n        if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n            df_train[f] = lbl.transform(list(df_train[f].values))\n            df_test[f] = lbl.transform(list(df_test[f].values))\n    return df_train, df_test","3252162c":"y_train = train['isFraud']\ntrain, test = encodeCategorical(train, test)","4af272e8":"X_train = pd.DataFrame(full_pipeline.fit_transform(train))\ngc.collect()","b2c0d2b5":"del train","f86bcfce":"test = test.drop(cols_to_drop, axis=1)\ntest = pd.DataFrame(full_pipeline.transform(test))","cfbfffe7":"def makePredictions(tr_df, tt_df, target, lgb_params, NFOLDS=2):\n    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n\n    X,y = tr_df, y_train    \n    P = tt_df\n\n    predictions = np.zeros(len(tt_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n\n        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p\/NFOLDS\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    tt_df['prediction'] = predictions\n    \n    return tt_df","a05b2d16":"lgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.064,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':800,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                } ","93f14219":"lgb_params['learning_rate'] = 0.005\nlgb_params['n_estimators'] = 1800\nlgb_params['early_stopping_rounds'] = 100    \ntest_predictions = makePredictions(X_train, test, TARGET, lgb_params, NFOLDS=8)","cadbbbe2":"lgb_submission = pd.DataFrame({\n    \"isFraud\": test_predictions['prediction'],\n})","383230d6":"lgb_submission.insert(0, \"TransactionID\", np.arange(3663549, 3663549 + 506691))\nlgb_submission.to_csv('prediction.csv', index=False)","98bc6651":"lgb_submission.head()","f68cf171":"The functions used for visualization are below.","a9515e49":"# Feature Engineering","d48342ff":"### C","79da9912":"The kernels below helped me in writing this kernel. Thanks!\n\nAndrew Lukyanenko: https:\/\/www.kaggle.com\/artgor\/eda-and-models\n\nLeonardo Ferreira: https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\n\nKonstantin Yakovlev: https:\/\/www.kaggle.com\/kyakovlev\/ieee-gb-2-make-amount-useful-again\n\nKonstantin Yakovlev: https:\/\/www.kaggle.com\/kyakovlev\/ieee-simple-lgbm","2a7bc2eb":"We prepare the test data with the pipeline we had so it will be ready to be used for prediction.","1db9f350":"### ID","9b7966e6":"## Models","e2900be0":"### Handle Device Type","6d981179":"![](https:\/\/www.xenonstack.com\/wp-content\/uploads\/xenonstack-credit-card-fraud-detection.png)","bb71e973":"### Set Frequency","d4392b2f":"This data seems quite huge and hard to understand. TransactionID is the common column in both transaction data and identity data and the two tables can be joined using this common column.","02d61b0c":"First let's check the sample submission.","aa6b5fcf":"IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they\u2019re partnering with the world\u2019s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.","06a99af3":"### Date","2a3d5f02":"## Knowing the Data","a2073353":"Now we need to deal with categorical data.","ba08d623":"We have merged the train_transaction and train_identity into a single table called train (similarly for test data). So we can delete the extra info.","24751c8f":"In a real dataset, it is common to have many null attributes.","b87c63d0":"As you may have noticed, for some companies there are several email addresses. For a better analysis we will consider them same as each other.","dda558b1":"### Card","72af9112":"### D","988b98e4":"Now let's recognize the categorical data and numerical data.","597e96a1":"There are 414 attributes containing null values.","5d22a361":"Anyways, we need to analyze the data to find out which fields are useful and which are not.","f83cd03d":"Above, it is shown that the dates of Train and Test data have an empty intersection.","24dee7a3":"### Handle P Email Domain and R Email Domain","94e69930":"Let's visualize the categorical ones which are:\n\n['id_12',\n 'id_15',\n 'id_16',\n 'id_23',\n 'id_27',\n 'id_28',\n 'id_29',\n 'id_30',\n 'id_31',\n 'id_33',\n 'id_34',\n 'id_35',\n 'id_36',\n 'id_37',\n 'id_38']","9f9cd053":"## Recognize categorical and numerical attributes","1dd147e6":"As seen above,\n\n       ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1',\n       'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15',\n       'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_33',\n       'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType',\n       'DeviceInfo']\n       \nare categorical and the rest of features are numerical.","c87cdef5":"Here we see that 76% of data is missing and more that 22% is equal to 100. This does not seem useful either.","fc41a908":"### Handle Browser Version","7358cbe3":"We need to find the best model and train it.","798417f6":"Now, we will load the train and test data.","c13ca73e":"## Prepare Data for Train","dc290ab7":"## Add New Features","681950d5":"### Set Time","82186f61":"We are supposed to output the probability of a transaction being fraudulant. ","b6967382":"## Reduce Mamory","49bfe8e6":"### M","8984f400":"Now we will delete attributes with more than 90 percent missing value.","9e2416d3":"### DeviceType and DeviceInfo","24d02c20":"### LGB","0d1369c2":"# Fraud Detection Competition","202a7b10":"Let's visualize it.","aba17f60":"We have the same issue with devices too.","7c7a9890":"# Recognize missing data","c9b39a76":"### Handle Email Domains","dde17d29":"As you can see, 88% of values are missing and 10% of them are equal to 0. So, 98% of data is either missing or 0. This attribute does not seem to be helpful."}}