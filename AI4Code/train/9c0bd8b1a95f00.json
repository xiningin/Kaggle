{"cell_type":{"9fe33b8a":"code","744ef13f":"code","4737fac5":"code","3f845e41":"code","52c3934c":"code","c665cd88":"code","478b512a":"code","c6734921":"code","80a0a664":"markdown","d8b594d4":"markdown","ad9e9089":"markdown","0fcb0f6f":"markdown","c5b17c42":"markdown","26a4a8fc":"markdown","8d589106":"markdown","f521d135":"markdown","c0f2c6da":"markdown"},"source":{"9fe33b8a":"# First some \"import\" housekeeping\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport os\nimport time\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom math import sqrt","744ef13f":"# Here is the data \nprint(os.listdir(\"..\/input\"))\ntrain_df = pd.read_csv('..\/input\/train.csv')\nprint('Rows: ',train_df.shape[0],'Columns: ',train_df.shape[1])\ntrain_df.info()","4737fac5":"train_df.head()","3f845e41":"# The usual split of the data into an X and a y array\nX = train_df.drop(['ID_code','target'],axis=1)\ny = train_df['target']","52c3934c":"split = train_df['target'].value_counts()\nprint(\"target value counts \")\nprint( split)\ntot = split[0] + split[1]\np = split[1]\/(split[0] + split[1])\nq = 1 - p\nsigma = sqrt((p * q) \/ tot)\n# If np \u2265 5 and nq \u2265 5 for a binomial distribution, then the sampling can have distribution for p-hat that is approximately normal\n# and since p is smaller than q, we have\nmin_sample = int((5 \/ p)+1)\nprint(\"Probability p of target being 1: \", p )\nprint(\"Probability q of target being 0: \", q )\nprint(\"Minimum number of samples of test set for an approximately normal distribution is: \", min_sample )\nprint('Expected standard deviation if minimum number of samples taken ', sigma)\nsns.countplot(train_df['target'])\nsns.set_style('whitegrid')","c665cd88":"n_fold = 50 # number of sets of samples to randomly take from the training set\nn = tot \/ n_fold\nall_p_hats = np.zeros(n_fold)\nall_std_errors = np.zeros(n_fold)\n\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state = 12345)\nfor fold_n, (remainder, this_fold) in enumerate(folds.split(X,y)):\n    y_this_fold = y.iloc[this_fold]\n    n = y_this_fold.count()\n    p_hat = y_this_fold.sum() \/ n\n    std_error = sqrt((p_hat * (1 - p_hat)) \/ n)\n#    print(\"Fold k = \", fold_n, \"has a probability p of target being 1: \", p_hat  )\n    all_p_hats[fold_n] = p_hat\n    all_std_errors[fold_n] = std_error\n        \nprint('Mean of distribution ', all_p_hats.mean(), 'as compared to the predicted ', p)\nprint('Standard Deviation of distribution', all_p_hats.std(), 'as compared to the predicted  ', sigma)\n# Use Freedman\u2013Diaconis rule to pick a good number of bins \nbin_width = 2.0 * ((np.percentile(all_p_hats, q=75) - np.percentile(all_p_hats, q=25)) \/ np.cbrt(n_fold))\nbins = int ( (all_p_hats.max() - all_p_hats.min() ) \/ bin_width )\nsns.distplot(all_p_hats, bins=bins)","478b512a":"all_zs = np.zeros(n_fold)\nfor j in range (n_fold) :\n    p_hat = all_p_hats[j]\n    std_error = all_std_errors[j]\n    z = (p_hat - p) \/ std_error\n#    print(\"Fold k = \", j, \"has a z value of \", z  )\n    all_zs[j] = z\nbin_width = 2.0 * ((np.percentile(all_zs, q=75) - np.percentile(all_zs, q=25)) \/ np.cbrt(n_fold))\nbins = int ( (all_zs.max() - all_zs.min() ) \/ bin_width )\nsns.distplot(all_zs, bins=bins)","c6734921":"# Choose a few Alphas\nalphas = (.5, .4, .2, 0.1, 0.05, 0.01, 0.001, 0.0001)\nz0s = (.675, .84, 1.28, 1.645, 1.960, 2.576, 3.290, 3.891)\nnum_alphas = len(alphas)\nrej_count = np.zeros(num_alphas, dtype = int)\nexpected_rej_count = np.zeros(num_alphas)\nfor j in range(num_alphas) :\n    expected_rej_count[j] = int(alphas[j] * n_fold)\n\nfor i in range (n_fold):\n    this_z = all_zs[i]\n    for j in range(num_alphas) :\n        alpha = alphas[j]\n        z0 = z0s[j] \n        if this_z > z0 :\n            rej_count[j] += 1\n        if this_z < -z0 :\n            rej_count[j] += 1\n\nprint (\"Alpha values we will test\")\nprint(alphas)\nprint (\"Expected number (alpha pecent out of \",n_fold,\") that will fail the z-test\")\nprint(expected_rej_count)\nprint (\"Actual number that failed the z-test\")\nprint(rej_count)\nprint(\"Confidence level = 1 - Alpha\")\nfor j in range(num_alphas) :\n    if (expected_rej_count[j] < rej_count[j]) :\n        print (\"For confidence level of\", ((1 - alphas[j])*100), \"percent, the hypothesis is proven FALSE\")\n    else : \n        print (\"For confidence level of\", ((1 - alphas[j])*100), \"percent, the hypothesis may still be true\")\n","80a0a664":"This kernel is intended to examine a statistical hypothesis about the population which contest entries sometimes assume but may not always test. \n\nThis formal statistical testing may help you validate your contest winning straegy!\n\nI noticed several entries in this contest look at the TRAINING set and count the percentage of targets being equal to \"1,\" These entries go on to hypothesize that the percentage of targets equal to \"1\" in the TEST set is the same. By this hypothesis, the use of stratified k-fold segmentation, and other probability-assuming techniques are used. Are these techniques really justified, and how can we test this hypothesis?\n\nStatistical analysis offers us a straightforward way to test this hypothesis, as follows:\n\n* We begin with a Hypothesis H-0 and assume it is correct. We also identify the opposite (or alternate) hypothesis H-a.\n* We then choose a level of significance we are targeting, alpha, which equals the probability that we find H-0 false when it really is true. Common values for alpha are 5%, 10%, or even as low as 1%. Note, this also provides us with a confidence level c = 1 - alpha\n* Next, we take random samples (of sufficient size) from the population, and test if any of them prove that H-0 is false. We only expect that very few (alpha) will be statistically significant.\n* Finally we will extend our findings to further useful statistical inferences about the data useful for this competition.\n\nNote: we can never prove out hypothesis, we can only disprove it, or fail to disprove it!","d8b594d4":"Here are the first few examples from the training set","ad9e9089":"NEXT STEPS:\n* Try additional numbers of folds, and\/or different random selection menthods (remember, using the above K-folkds still makes sure every training sample is selected once and only once - rather than re-scrambling every fold)\n* Once you have a good feel for your best confidence level, re-examine assumptions.\n\nASSUMPTIONS TO RE-EXAMINE:\n* StratifiedKFold is teaching the classification algorithm an assumed value of p with a high confidence level, but is that a good assumption?\n* Alternatively, randomly selecting from the training set (KFold) let's the p_hat dominate the training for that fold, which varies as we can see above.\n* Finally, we may choose a different training \/ validation split methodology altogether. For example...\n* How close are the training set samples to the test set samples (classification notwithstanding)? \n* Perhaps an adversarial validation scheme is warranted.\n\nI hope this was helpful to you!","0fcb0f6f":"So what this means is that if we take a sample of data from our training set, and calculate the p-hat in that sample, over time, it should form a normal distribution with a mean of p, and a standard deviation of sigma. \n\n* With 200,000 samples in our training set, we can perform at most 4,000 tests since each test must contain 50 samples.\n* Let's try that with 50 tests (folds) taking 4000 random samples each.\n* NOTE: we are not using \"StratifiedKFold\" and instead we use simply \"KFold\"  because we purposely do not want to assume the probability \"P_hat\"","c5b17c42":"Initial version forked from SCTP-working(LGB) https:\/\/www.kaggle.com\/dromosys\/sctp-working-lgb courtesy of Dromosys","26a4a8fc":"CONCLUSION:\n* Whenever the hypothesis is proven FALSE for a particular confidence level, we can no longer have that level of confidence that p and sigma will hold true for the population if p-hat and standard error are measured for a sample from that population. \n* If we prove our hypothesis FALSE at a certain confidence level, but it may still be true at GREATER confidence levels, then we might want to attribute that to the random nature of the selection (and may want to run more tests) - but in either case, we would stick with the HIGHEST CONFIDENCE LEVEL LOWER THAN A PROVEN FALSE LEVEL.","8d589106":"Beside a hypothesis null and alternate values, we also have to have a \"significance\"\n* If we choose a level of significance alpha such as 5%, it would equivalently give us a confidence level c = 95%\n* We now examine our z values again, except we are now comparing them to the rejection regions, otherwise known as the Z-test\n\nFor each alpha, count how many z values exceed the + or - z0 rejection regions for our given confidence level.","f521d135":"You can try different values for \"n_fold\" and thereby change the number of experiments, but be sure not to exceed 4,000 because then our assumptions of approximate normal distribution are violated.\n\nSo from the above, it looks like the mean is spot-on, whereas the standard deviation is a bit bigger than expected (a bit wide). What does this mean to our hypothesis?\n\nIn order to formally test a hypothesis, we need to formally state the hypothesis.\n* H-0, our null hypothesis, states that the probability of the target being equal to \"1\" in the population is p = 10.049 %\n* We choose a level of significance alpha such as 5%, which would equivalently give us a confidence level c = 95%\n* We repeat our tests (as per the above) and test if any of them prove that H-0 is false. We only expect that very few (alpha) will be statistically significant.\n\nFirst, shift and squeeze the distribution to be \"standard\" (zero mean, variance = 1) normal distribution, this will give us a \"Z-value\" for each of our samples (folds).\n* The graph should look roughly the same, just shifted and squeezed.","c0f2c6da":"Here are the target value counts. It is from these target value counts that we will form our hypothesis. \n\nIn statistics terminology, the Santander problem is a Bernoulli trial of the random variable X. \n* The probability of having a \"1\" for the target is p, and so the expected value of X is p, and the variance is p * (1 - p)\n* if we take n samples from the Santander population, and we add up all the ones that had a target of \"1\", the total T will be a binomial distribution with an integer value anywhere from 0 to n.\n* The binomial distribution is such that the probability of having k targets = 1 is P(T = k) = (n choose k) * p^k \/ (1 - p)^k\n* This makes the expected value of T = np, the variance of T = n * p * (1 - p), and the standard deviation of T = sqrt(n * p * (1 - p)) or equivalently, (n * p * (1 - p)) ^ 0.5\n* Since we cannot sample the entire Santander population, we can only make estimates. We can estimate p as a value we call p_hat\n* Using the above sample from the population, p_hat is by definition T \/ n, so the variance of p_hat is p * (1 - p) \/ n, and the standard deviation (sometimes called standard error because we're only talking about a sample) is the square root of the variance.\n* We fully expect that p_hat approahes p, as n approaches infinity.\n\n**How confident are we ** in the use of p_hat (or \"p from the training set\") as the value p in our Bayes theorem, and therefore the use of \"StratifiedKFold\" etc.?\n\n* First we see the **probability p that the target is \"1\"** in the training set as being simply the number of \"1\" targets divided by the whole set; where the probability of \"not-p\" is q = 1 - p.\n* Then we calculate the **minimum number of samples** we need to take from the training set, such that within those samples, p-hat, the number of \"1\" targets, is approximately normally distributed. This is true if np \u2265 5 and nq \u2265 5; where n is the number of items in our samples.\n* If the above is true, and we take repeated n samples from the training set, then the mean value of p_hat for those samples will be mu(p_hat) = p, and the standard deviation sigma(p_hat) = ( (p * q) \/ n ) ^ 0.5\n* We can create what is called a \"two-tailed\" hypothesis stating that 95% of p_hat values will fall within the range of p plus or minus 1.96 sigma(p)\n\n* We can then find confidence intervals, after we shift and stretch the distribution until it has a standard normal distribution (mean = 0, standard deviation = 1), by using Z-Tables for the standard normal distribution\n"}}