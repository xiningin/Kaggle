{"cell_type":{"a046c292":"code","0daee350":"code","27521f80":"code","0985c440":"code","d8150aed":"code","18e28993":"code","8ec9c71e":"code","ce63dba7":"code","3139e170":"code","57f573cf":"code","cda4560f":"code","6559baba":"code","9e27e0c0":"code","7d7183e2":"code","8362f78c":"code","ce3ea554":"code","b5081b68":"code","1dfd35e5":"code","5a323020":"markdown","aa808b19":"markdown"},"source":{"a046c292":"%%time\n# INSTALL RAPIDS OFFLINE (FROM KAGGLE DATASET). TAKES 1 MINUTE :-)\nimport sys\n!cp ..\/input\/rapids\/rapids.0.13.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","0daee350":"import cudf, cuml\nimport cupy as cp\nimport numpy as np\nimport pandas as pd\nimport os\nfrom cuml.manifold import TSNE, UMAP\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import ylim, xlim\n%matplotlib inline\n","27521f80":"fnc_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/fnc.csv\")\nloading_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/loading.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\n\n\nlabels_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/train_scores.csv\")\nlabels_df[\"is_train\"] = True\n\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\n\ndf.shape, test_df.shape","0985c440":"# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1\/600\n\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE\n\nfeatures = loading_features + fnc_features","d8150aed":"train_test = np.vstack([df[features], test_df[features]])\ntrain_test.shape","18e28993":"%%time\ntsne = TSNE(n_components=2)\ntrain_test_2D = tsne.fit_transform(train_test)","8ec9c71e":"plt.scatter(train_test_2D[:,0], train_test_2D[:,1], s = 0.5)\n","ce63dba7":"\n%%time\ntsne = TSNE(n_components=2)\n\ntrain_2D = tsne.fit_transform(df[features].values)","3139e170":"plt.scatter(train_2D[:,0], train_2D[:,1], s = 0.5)","57f573cf":"df['age'].values","cda4560f":"plt.scatter(train_2D[:,0], train_2D[:,1], c=df['age'].values, s = 0.5)","6559baba":"plt.scatter(train_2D[:,0], train_2D[:,1], c=df['domain1_var1'].values, s = 0.5)","9e27e0c0":"plt.scatter(train_2D[:,0], train_2D[:,1], c=df['domain1_var2'].values, s = 0.5)","7d7183e2":"%%time\numap = UMAP(n_components=2)\ntrain_test_2D = umap.fit_transform(train_test)","8362f78c":"plt.scatter(train_test_2D[:,0], train_test_2D[:,1], s = 0.5)\n","ce3ea554":"\n%%time\numap = UMAP(n_components=2)\n\ntrain_2D = umap.fit_transform(df[features].values)","b5081b68":"plt.scatter(train_2D[:,0], train_2D[:,1], s = 0.5)","1dfd35e5":"plt.scatter(train_2D[:,0], train_2D[:,1], c=df['age'].values, s = 0.5)","5a323020":"There is some structure there, with \"central\" denser reagion, and more dispersed periphery, but other than that it's hard to see any distinct groupings. ","aa808b19":"[Rapids](https:\/\/rapids.ai) is an open-source GPU accelerated Data Science and Machine Learning library, developed and mainatained by [Nvidia](https:\/\/www.nvidia.com). It is designed to be compatible with many existing CPU tools, such as Pandas, scikit-learn, numpy, etc. It enables **massive** acceleration of many data-science and machine learning tasks, oftentimes by a factor fo 100X, or even more. \n\nRapids is still undergoing developemnt, and as of right now it's not availabel in the Kaggle Docker environment. If you are interested in installing and riunning Rapids locally on your own machine, then you shoudl [refer to the followong instructions](https:\/\/rapids.ai\/start.html).\n\n\nThe purpose of this kernel is to take alook at dimensionality reduction that one gets with t-SNE and UMAP algorithms. TReNDS data is very high-dimensional, and it may not be easy to understand what is going on. The set of features that we are using are the ones that have proven useful in some of the top-scoring public kernels."}}