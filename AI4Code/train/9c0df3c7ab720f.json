{"cell_type":{"cf3c0a69":"code","c5ff06ea":"code","c349b86f":"code","5a2777bf":"code","7044efe4":"code","3d4ba6d7":"code","b245a810":"code","ce4cd20e":"code","33893e14":"code","e51d4808":"code","57767b1c":"code","01b5bfa3":"code","5001152b":"markdown","6120043b":"markdown","c072e9e7":"markdown","f549ef0b":"markdown","837a1a6f":"markdown","b538f01c":"markdown","ebac6716":"markdown","f5bb29ec":"markdown","71e74269":"markdown","ed56ab46":"markdown"},"source":{"cf3c0a69":"# data analysis and wrangling\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Visualize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","c5ff06ea":"data = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndata.head()\n#data.tail()","c349b86f":"data.info()\ndata.isna().sum()","5a2777bf":"y = data.iloc[:,-1].values\nX = data.iloc[:,:-1].values","7044efe4":"# Split the data into training and test sets with a ratio of 7:3. \n# Set a random seed in order to get fixed results for the purpose of exploring.\n\nfrom sklearn.model_selection import train_test_split,RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 123)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","3d4ba6d7":"# Using linear regression to predict model\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\ny_pred=regressor.predict(X_test).round(0).astype(int)\n\naccuracy_score(y_pred,y_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_pred,y_test)\n","b245a810":"# Use K-Neighbor classifier to predict model.\n\nfrom sklearn.neighbors import KNeighborsClassifier as knn\nks = []\nfor i in range(1,300):\n    knn_regressor = knn(n_neighbors = i,weights = 'distance')\n    knn_regressor.fit(X_train,y_train)\n    y_pred=knn_regressor.predict(X_test).round(0).astype(int)\n\n    ks.append(accuracy_score(y_test, y_pred))\nplt.plot(ks)\n\nmax_percent = max(ks)\nindex = ks.index(max_percent)+1\nprint(max_percent,index)\n","ce4cd20e":"# With optimal neighbors\n\nclassifier = knn(n_neighbors = 23,weights='distance')\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_pred,y_test)","33893e14":"from sklearn.ensemble import RandomForestClassifier\nrfc = []\nfor i in range(1,100):\n    classifier = RandomForestClassifier(n_estimators = i, criterion = 'entropy', random_state = 123)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    rfc.append(accuracy_score(y_test, y_pred))\n\nmax_percent = max(rfc)\nindex = rfc.index(max_percent)+1\nprint(max_percent,index)\nplt.plot(rfc)","e51d4808":"classifier = RandomForestClassifier(n_estimators = 17, criterion = 'entropy', random_state = 123)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","57767b1c":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 123)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","01b5bfa3":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","5001152b":"# K-Neighbor Classification\nWhen using k-nearest neigbor classification, we have the option of choosing how much neighbors to classify the quality. Using a for loop to iterate in finding the optimal number of neighbors. After 100 iterations, as the graph above shows, 23 neighbors seem ideal as it presents a 69.583% accuracy.","6120043b":"# Preprocess Data\n\n\n### Split Training\/Test set\nIn order to create and test whether our model is accurate, we need to divide the data set into a training set and test set. We will set the training set 70% of the original data and the remaining will be the test set.\n\n### Scale Data\nWhen viewing the data, we can notice each column have different magnitudes of data. For example, **alcohol** has figures around 10 whereas **chlorides** data are at $10^{-2}$. This makes some columns more influential than others based on the scale. Therefore we will scale the data to match each column.","c072e9e7":"# Create Model to Predict Wine Ratings\n","f549ef0b":"## Seperate Dependent and Independent Data\n\nWe are trying to predicts quality given other predictors such as fixed acidity, volatile acidity, and etc. So, set quality(*dependent data*) to `y` and the remaining predictors(*independent data*) as `X`.","837a1a6f":"## Check data\n\nWe should also make sure there aren't any missing data. Also by checking the data type we might need to encode categorcial data.\nBased on checking the data, we none of the columns are missing and are all float data types. Hence, there is no cleaning needed.","b538f01c":"# Linear Regression\n\nUsing linear regression for the wine rating dataset, the predicted model's accuracy is 53.125%. \n\n","ebac6716":"# Conclusion\n\nWe have used 4 different methods to classify red wine based on fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol.\n* Linear Regression\n* K-Neighbor Classification\n* Random Forest Classification\n* Support Vector Machine\n\nFor this dataset, k-neighbor classification had the best results with a 69.583% accuracy. Linear regression on the other hand had the lowest with a little over 50%. However, this does not mean k-neighbor is always the best to use and this may differ on many factors. ","f5bb29ec":"## Import data\n\nFirst we will acquire the data and take a look at what columns exist. Using the `head` and `tail` function, we can get a general idea of the data as well as know the size of it. As seen, the data has 12 columns and 1598 rows.","71e74269":"# Random Forest Classifier\n\nUsing a random forest classifier, we alter the number of trees used to find the optimal number to use as our classifier. After iterating for 100 times, we notice the accuracy tends to converge between 66% and 68% along with some noise. Hence, we can choose the number of trees within this range and not have to check for numbers greater than 100. Among the best predictor is when `n` is 17. With this value the accuracy for predicting the wine quality is 67.5%.","ed56ab46":"# Support Vector Machine\nApplying the support vector machine to our data, the predicted data accuracy is 60.83%. "}}