{"cell_type":{"e2a191b3":"code","ce4d34b9":"code","93cbf29e":"code","09c01f13":"code","0038dd1c":"code","227220c6":"code","741d8181":"code","bc081d6d":"code","9fee8979":"code","e3a1da04":"code","c692c844":"code","48d07739":"code","7980803e":"code","50859b0a":"code","f4a8795a":"code","44252564":"code","38ac55d6":"code","6dc27354":"code","23ec4738":"code","32160390":"code","30b95f96":"code","8c8b6ae8":"code","92ef60ab":"code","52948ef2":"code","ec7feb05":"code","99228660":"code","8bef5e78":"code","b78df0a8":"code","61703e99":"code","857d6abc":"code","83ca4387":"code","6936b30c":"code","87f9e0b8":"code","628de8d3":"code","0594d45d":"code","1c141fd8":"code","458e1528":"code","bcb003ee":"code","d795ed50":"code","7d02a0c6":"code","520c6a82":"code","3bdb3347":"code","1e3ddf7c":"code","d4352dce":"code","ff99d495":"code","07a10f48":"code","2d20893c":"code","2d240bbc":"code","16d35434":"code","75662141":"code","8a265740":"code","8fca9fa0":"code","a52702f6":"code","fa768e69":"code","b1b1d0e3":"code","34116d75":"code","087d10b1":"code","e9833059":"code","7a18d688":"code","56a153ca":"code","aecbbc31":"code","56211e93":"code","fd828f04":"code","4f54a0b8":"code","9732d88a":"code","06cb0e7a":"code","93dd044f":"code","7094358c":"code","ce43145b":"code","78c2ba57":"code","31c70860":"code","8aaf5cdf":"code","0a0c84f2":"code","644783c3":"code","758322b8":"code","4ad93359":"code","f626cb31":"code","f40f4501":"code","4099d16f":"code","afbb946f":"code","4898f3b2":"code","d69bed79":"code","9dafc288":"code","069e17b6":"code","42ba11ce":"code","6fb5db51":"code","824a3088":"code","2bdd1589":"code","5f6185d9":"code","383104c9":"code","fd9ca39e":"code","d8f0750a":"code","e5ca075b":"code","1ca279b6":"code","48f48004":"code","a28e80cc":"code","572a4c53":"code","bd698e79":"code","33eb6292":"code","dd9a3f1c":"code","5e9e3cf9":"code","d8760f61":"code","7f3d7378":"markdown","e2a25a97":"markdown","b0b52d4b":"markdown","045f5c51":"markdown","beda3542":"markdown","8f5a0292":"markdown","99a289b6":"markdown","0549f1e1":"markdown","5ba2d867":"markdown","5788ec98":"markdown","2e33d1aa":"markdown","c30ba738":"markdown","e118586e":"markdown","7ae14c47":"markdown","06b1f049":"markdown","90ceb008":"markdown","70b64065":"markdown","016312fe":"markdown","20f5ff2c":"markdown","4617f51e":"markdown","65c46800":"markdown","a222ad5a":"markdown","bba34b36":"markdown","50c360da":"markdown","46e40544":"markdown","f3235282":"markdown","7300c7e6":"markdown","ec192ccb":"markdown","be07d861":"markdown","88351338":"markdown","9727841c":"markdown","0ba21d83":"markdown","825cd969":"markdown","fd090e81":"markdown","87fce534":"markdown","33886b13":"markdown","407bd65e":"markdown","0dab57d0":"markdown","4eb23ad0":"markdown","5f2a179d":"markdown","86c8709c":"markdown","3444717e":"markdown","21e97ff3":"markdown","b668e4ef":"markdown","18e6391e":"markdown","8881dbcd":"markdown","3853e827":"markdown","57bbcac3":"markdown","1c96b513":"markdown","bbe27b6f":"markdown","80b1ee10":"markdown"},"source":{"e2a191b3":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport category_encoders as ce\n\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.naive_bayes import CategoricalNB, BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.preprocessing import label_binarize\n\n\nimport plotly.graph_objects as go\nfrom nltk.corpus import stopwords","ce4d34b9":"df = pd.read_csv(\"..\/input\/us-accidents\/US_Accidents_June20.csv\")\ndf.head()","93cbf29e":"state_counts = df[\"State\"].value_counts()\nfig = go.Figure(data=go.Choropleth(locations=state_counts.index, z=state_counts.values.astype(float), locationmode=\"USA-states\", colorscale=\"turbo\"))\nfig.update_layout(title_text=\"Number of US Accidents for each State\", geo_scope=\"usa\")\nfig.show()","09c01f13":"plt.figure(figsize=(20, 8))\nplt.title(\"Top 10 states with the highest number of accidents\")\nsns.barplot(state_counts[:10].values, state_counts[:10].index, orient=\"h\")\nplt.xlabel(\"Number of accident\")\nplt.ylabel(\"State\")\nplt.show()","0038dd1c":"stop = stopwords.words(\"english\") + [\"-\"]\n\ndf_s4_desc = df[df[\"Severity\"] == 4][\"Description\"]\n# Split the description\ndf_words = df_s4_desc.str.lower().str.split(expand=True).stack()\n\n# If the word is not in the stopwords list\ncounts = df_words[~df_words.isin(stop)].value_counts()[:10]\n\nplt.figure(figsize=(18, 8))\nplt.title(\"Top 10 words used to describe an accident with severity 4\")\nsns.barplot(counts.values, counts.index, orient=\"h\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Word\")\nplt.show()","227220c6":"road_features = [\"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\", \"Junction\", \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\", \"Traffic_Calming\", \"Traffic_Signal\", \"Turning_Loop\"]\ndata = df[road_features].sum().sort_values(ascending=False)\n\nplt.figure(figsize=(18, 8))\nplt.title(\"Most frequent road features\")\nsns.barplot(data.values, data.index, orient=\"h\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Road feature\")\nplt.show()","741d8181":"severity_distance = df.groupby(\"Severity\").mean()[\"Distance(mi)\"].sort_values(ascending=False)\n\nplt.figure(figsize=(18, 8))\nplt.title(\"Medium distance by severity\")\nsns.barplot(severity_distance.values, severity_distance.index, orient=\"h\", order=severity_distance.index)\nplt.xlabel(\"Distance (mi)\")\nplt.show()","bc081d6d":"counts = df[\"Weather_Condition\"].value_counts()[:15]\nplt.figure(figsize=(20, 8))\nplt.title(\"Histogram distribution of the top 15 weather conditions\")\nsns.barplot(counts.index, counts.values)\nplt.xlabel(\"Weather Condition\")\nplt.ylabel(\"Value\")\nplt.show()","9fee8979":"counts = pd.to_datetime(df['Start_Time']).dt.day_name().value_counts()\nweekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\nplt.figure(figsize=(20, 8))\nplt.title(\"Number of accidents for each weekday\")\nsns.barplot(counts.index, counts.values, order=weekdays)\nplt.xlabel(\"Weekday\")\nplt.ylabel(\"Value\")\nplt.show()","e3a1da04":"X = df\nX.head()","c692c844":"# Cast Start_Time to datetime\nX[\"Start_Time\"] = pd.to_datetime(X[\"Start_Time\"])\n\n# Extract year, month, weekday and day\nX[\"Year\"] = X[\"Start_Time\"].dt.year\nX[\"Month\"] = X[\"Start_Time\"].dt.month\nX[\"Weekday\"] = X[\"Start_Time\"].dt.weekday\nX[\"Day\"] = X[\"Start_Time\"].dt.day\n\n# Extract hour and minute\nX[\"Hour\"] = X[\"Start_Time\"].dt.hour\nX[\"Minute\"] = X[\"Start_Time\"].dt.minute\n\nX.head()","48d07739":"corr_matrix = X.corr()\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr_matrix, vmin=-1, vmax=1, cmap=\"seismic\")\nplt.gca().patch.set(hatch=\"X\", edgecolor=\"#666\")\nplt.show()","7980803e":"features_to_drop = [\"ID\", \"Source\", \"TMC\", \"Start_Time\", \"End_Time\", \"End_Lat\", \"End_Lng\", \"Description\", \"Number\", \"Street\", \"County\", \"State\", \"Zipcode\", \"Country\", \"Timezone\", \"Airport_Code\", \"Weather_Timestamp\", \"Wind_Chill(F)\", \"Turning_Loop\", \"Sunrise_Sunset\", \"Nautical_Twilight\", \"Astronomical_Twilight\"]\nX = X.drop(features_to_drop, axis=1)\nX.head()","50859b0a":"print(\"Number of rows:\", len(X.index))\nX.drop_duplicates(inplace=True)\nprint(\"Number of rows after drop of duplicates:\", len(X.index))","f4a8795a":"X[\"Side\"].value_counts()","44252564":"X = X[X[\"Side\"] != \" \"]\nX[\"Side\"].value_counts()","38ac55d6":"X[[\"Pressure(in)\", \"Visibility(mi)\"]].describe().round(2)","6dc27354":"X = X[X[\"Pressure(in)\"] != 0]\nX = X[X[\"Visibility(mi)\"] != 0]\nX[[\"Pressure(in)\", \"Visibility(mi)\"]].describe().round(2)","23ec4738":"unique_weather = X[\"Weather_Condition\"].unique()\n\nprint(len(unique_weather))\nprint(unique_weather)","32160390":"X.loc[X[\"Weather_Condition\"].str.contains(\"Thunder|T-Storm\", na=False), \"Weather_Condition\"] = \"Thunderstorm\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Snow|Sleet|Wintry\", na=False), \"Weather_Condition\"] = \"Snow\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Rain|Drizzle|Shower\", na=False), \"Weather_Condition\"] = \"Rain\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Wind|Squalls\", na=False), \"Weather_Condition\"] = \"Windy\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Hail|Pellets\", na=False), \"Weather_Condition\"] = \"Hail\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Fair\", na=False), \"Weather_Condition\"] = \"Clear\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Cloud|Overcast\", na=False), \"Weather_Condition\"] = \"Cloudy\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Mist|Haze|Fog\", na=False), \"Weather_Condition\"] = \"Fog\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Sand|Dust\", na=False), \"Weather_Condition\"] = \"Sand\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Smoke|Volcanic Ash\", na=False), \"Weather_Condition\"] = \"Smoke\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"N\/A Precipitation\", na=False), \"Weather_Condition\"] = np.nan\n\nprint(X[\"Weather_Condition\"].unique())","30b95f96":"X[\"Wind_Direction\"].unique()","8c8b6ae8":"X.loc[X[\"Wind_Direction\"] == \"CALM\", \"Wind_Direction\"] = \"Calm\"\nX.loc[X[\"Wind_Direction\"] == \"VAR\", \"Wind_Direction\"] = \"Variable\"\nX.loc[X[\"Wind_Direction\"] == \"East\", \"Wind_Direction\"] = \"E\"\nX.loc[X[\"Wind_Direction\"] == \"North\", \"Wind_Direction\"] = \"N\"\nX.loc[X[\"Wind_Direction\"] == \"South\", \"Wind_Direction\"] = \"S\"\nX.loc[X[\"Wind_Direction\"] == \"West\", \"Wind_Direction\"] = \"W\"\n\nX[\"Wind_Direction\"] = X[\"Wind_Direction\"].map(lambda x : x if len(x) != 3 else x[1:], na_action=\"ignore\")\n\nX[\"Wind_Direction\"].unique()","92ef60ab":"X.isna().sum()","52948ef2":"features_to_fill = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\", \"Precipitation(in)\"]\nX[features_to_fill] = X[features_to_fill].fillna(X[features_to_fill].mean())\n\nX.dropna(inplace=True)\n\nX.isna().sum()","ec7feb05":"X.describe().round(2)","99228660":"severity_counts = X[\"Severity\"].value_counts()\n\nplt.figure(figsize=(10, 8))\nplt.title(\"Histogram for the severity\")\nsns.barplot(severity_counts.index, severity_counts.values)\nplt.xlabel(\"Severity\")\nplt.ylabel(\"Value\")\nplt.show()","8bef5e78":"size = len(X[X[\"Severity\"]==1].index)\ndf = pd.DataFrame()\nfor i in range(1,5):\n    S = X[X[\"Severity\"]==i]\n    df = df.append(S.sample(size, random_state=42))\nX = df","b78df0a8":"severity_counts = X[\"Severity\"].value_counts()\nplt.figure(figsize=(10, 8))\nplt.title(\"Histogram for the severity\")\nsns.barplot(severity_counts.index, severity_counts.values)\nplt.xlabel(\"Severity\")\nplt.ylabel(\"Value\")\nplt.show()","61703e99":"scaler = MinMaxScaler()\nfeatures = ['Temperature(F)','Distance(mi)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)','Precipitation(in)','Start_Lng','Start_Lat','Year', 'Month','Weekday','Day','Hour','Minute']\nX[features] = scaler.fit_transform(X[features])\nX.head()","857d6abc":"categorical_features = set([\"Side\", \"City\", \"Wind_Direction\", \"Weather_Condition\", \"Civil_Twilight\"])\n\nfor cat in categorical_features:\n    X[cat] = X[cat].astype(\"category\")\n\nX.info()","83ca4387":"print(\"Unique classes for each categorical feature:\")\nfor cat in categorical_features:\n    print(\"{:15s}\".format(cat), \"\\t\", len(X[cat].unique()))","6936b30c":"X = X.replace([True, False], [1, 0])\n\nX.head()","87f9e0b8":"# Remove city because it will be encoded later\nonehot_cols = categorical_features - set([\"City\"])\n\nX = pd.get_dummies(X, columns=onehot_cols, drop_first=True)\n\nX.head()","628de8d3":"binary_encoder = ce.binary.BinaryEncoder()\n\ncity_binary_enc = binary_encoder.fit_transform(X[\"City\"])\ncity_binary_enc","0594d45d":"X = pd.concat([X, city_binary_enc], axis=1).drop(\"City\", axis=1)\n\nX.head()","1c141fd8":"# Metrics dictionary\naccuracy = dict()\nprecision = dict()\nrecall = dict()\nf1 = dict()\nfpr = dict()\ntpr = dict()","458e1528":"# Train\/Validation - Test split\nX, X_test = train_test_split(X, test_size=.2, random_state=42)\nprint(X.shape, X_test.shape)","bcb003ee":"sample = X\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\n\nX_train, X_validate, y_train, y_validate = train_test_split(X_sample, y_sample, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_validate.shape, y_validate.shape)","d795ed50":"lr = LogisticRegression(random_state=42, n_jobs=-1)\nparams = {\"solver\": [\"newton-cg\", \"sag\", \"saga\"]}\ngrid = GridSearchCV(lr, params, n_jobs=-1, verbose=5)\ngrid.fit(X_train, y_train)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nprint(\"Train score:\", grid.score(X_train, y_train))\nprint(\"Validation score:\", grid.score(X_validate, y_validate))","7d02a0c6":"print(\"Default scores:\")\nlr.fit(X_train, y_train)\nprint(\"Train score:\", lr.score(X_train, y_train))\nprint(\"Validation score:\", lr.score(X_validate, y_validate))","520c6a82":"pd.DataFrame(grid.cv_results_)","3bdb3347":"y_pred = lr.predict(X_validate)\n\naccuracy[\"Logistic Regression\"] = accuracy_score(y_validate, y_pred)\nf1[\"Logistic Regression\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, lr.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","1e3ddf7c":"y_pred = lr.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()","d4352dce":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = lr.predict_proba(X_validate)\n\nprecision[\"Logistic Regression\"], recall[\"Logistic Regression\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Logistic Regression\"], tpr[\"Logistic Regression\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Logistic Regression\"], precision[\"Logistic Regression\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Logisitc Regression\")\nplt.show()","ff99d495":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"Logistic Regression\"], tpr[\"Logistic Regression\"], where=\"post\")\n\nplt.title(\"ROC curve - Logistic Regression\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","07a10f48":"parameters = [{\"kernel\": [\"linear\", \"rbf\", \"sigmoid\"], \"C\": [.2, .5, .8, 1.]}, {\"kernel\": [\"poly\"], \"C\": [.2, .5, .8, 1.], \"degree\": [2, 3, 4]}]\nsvc = svm.SVC(verbose=5, random_state=42)\ngrid = GridSearchCV(svc, parameters, verbose=5, n_jobs=-1)\n\nsample = X.sample(5_000, random_state=42)\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\ngrid.fit(X_sample, y_sample)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nprint(\"Train score:\", grid.score(X_sample, y_sample))","2d20893c":"print(\"Default scores:\")\nsvc.fit(X_sample, y_sample)\nprint(\"Train score:\", svc.score(X_sample, y_sample))","2d240bbc":"pd.DataFrame(grid.cv_results_).sort_values(by=\"rank_test_score\")","16d35434":"sample = X.sample(10_000, random_state=42)\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\n\nX_train, X_validate, y_train, y_validate = train_test_split(X_sample, y_sample, test_size=.2, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_validate.shape, y_validate.shape)","75662141":"svc = svm.SVC(**grid.best_params_, random_state=42)\nsvc.fit(X_train, y_train)\n\nprint(\"Train score:\", svc.score(X_train, y_train))\nprint(\"Validation score:\", svc.score(X_validate, y_validate))","8a265740":"y_pred = svc.predict(X_validate)\n\naccuracy[\"SVM\"] = accuracy_score(y_validate, y_pred)\nf1[\"SVM\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, svc.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","8fca9fa0":"y_pred = svc.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Support Vector Machine\")\nplt.show()","a52702f6":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = svc.decision_function(X_validate)\n\nprecision[\"SVM\"], recall[\"SVM\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"SVM\"], tpr[\"SVM\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"SVM\"], precision[\"SVM\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Support Vector Machine\")\nplt.show()","fa768e69":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"SVM\"], tpr[\"SVM\"], where=\"post\")\n\nplt.title(\"ROC curve - Support Vector Machine\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","b1b1d0e3":"sample = X\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\n\nX_train, X_validate, y_train, y_validate = train_test_split(X_sample, y_sample, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_validate.shape, y_validate.shape)","34116d75":"dtc = DecisionTreeClassifier(random_state=42)\nparameters = [{\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [5, 10, 15, 30]}]\ngrid = GridSearchCV(dtc, parameters, verbose=5, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nprint(\"Train score:\", grid.score(X_train, y_train))\nprint(\"Validation score:\", grid.score(X_validate, y_validate))","087d10b1":"print(\"Default scores:\")\ndtc.fit(X_train, y_train)\nprint(\"Train score:\", dtc.score(X_train, y_train))\nprint(\"Validation score:\", dtc.score(X_validate, y_validate))","e9833059":"pd.DataFrame(grid.cv_results_).sort_values(by=\"rank_test_score\")","7a18d688":"y_pred = dtc.predict(X_validate)\n\naccuracy[\"Decision Tree\"] = accuracy_score(y_validate, y_pred)\nf1[\"Decision Tree\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, dtc.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","56a153ca":"y_pred = dtc.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Decision Tree\")\nplt.show()","aecbbc31":"importances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=[\"importance\"], index=X_train.columns)\n\nimportances.iloc[:,0] = dtc.feature_importances_\n\nimportances = importances.sort_values(by=\"importance\", ascending=False)[:30]\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x=\"importance\", y=importances.index, data=importances)\nplt.show()","56211e93":"fig, ax = plt.subplots(figsize=(20, 10))\nplot_tree(dtc, max_depth=4, fontsize=10, feature_names=X_train.columns.to_list(), class_names = True, filled=True)\nplt.show()","fd828f04":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = dtc.predict_proba(X_validate)\n\nprecision[\"Decision Tree\"], recall[\"Decision Tree\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Decision Tree\"], tpr[\"Decision Tree\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Decision Tree\"], precision[\"Decision Tree\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Decision Tree\")\nplt.show()","4f54a0b8":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"Decision Tree\"], tpr[\"Decision Tree\"], where=\"post\")\n\nplt.title(\"ROC curve - Decision Tree\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","9732d88a":"sample = X\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\n\nX_train, X_validate, y_train, y_validate = train_test_split(X_sample, y_sample, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_validate.shape, y_validate.shape)","06cb0e7a":"rfc = RandomForestClassifier(n_jobs=-1, random_state=42)\nparameters = [{\"n_estimators\": [50, 100, 200, 500], \"max_depth\": [5, 10, 15, 30]}]\ngrid = GridSearchCV(rfc, parameters, verbose=5, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nprint(\"Train score:\", grid.score(X_train, y_train))\nprint(\"Validation score:\", grid.score(X_validate, y_validate))","93dd044f":"print(\"Default scores:\")\nrfc.fit(X_train, y_train)\nprint(\"Train score:\", rfc.score(X_train, y_train))\nprint(\"Validation score:\", rfc.score(X_validate, y_validate))","7094358c":"pd.DataFrame(grid.cv_results_).sort_values(by=\"rank_test_score\")","ce43145b":"y_pred = rfc.predict(X_validate)\n\naccuracy[\"Random Forest\"] = accuracy_score(y_validate, y_pred)\nf1[\"Random Forest\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, rfc.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","78c2ba57":"y_pred = rfc.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Random Forest\")\nplt.show()","31c70860":"importances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=[\"importance\"], index=X_train.columns)\n\nimportances.iloc[:,0] = rfc.feature_importances_\n\nimportances = importances.sort_values(by=\"importance\", ascending=False)[:30]\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x=\"importance\", y=importances.index, data=importances)\nplt.show()","8aaf5cdf":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = rfc.predict_proba(X_validate)\n\nprecision[\"Random Forest\"], recall[\"Random Forest\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Random Forest\"], tpr[\"Random Forest\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Random Forest\"], precision[\"Random Forest\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Random Forest\")\nplt.show()","0a0c84f2":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"Random Forest\"], tpr[\"Random Forest\"], where=\"post\")\n\nplt.title(\"ROC curve - Random Forest\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","644783c3":"sample = X\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\n\nX_train, X_validate, y_train, y_validate = train_test_split(X_sample, y_sample, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_validate.shape, y_validate.shape)","758322b8":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\nprint(\"Train score:\", gnb.score(X_train, y_train))\nprint(\"Validation score:\", gnb.score(X_validate, y_validate))","4ad93359":"y_pred = gnb.predict(X_validate)\n\naccuracy[\"Gaussian Naive Bayes\"] = accuracy_score(y_validate, y_pred)\nf1[\"Gaussian Naive Bayes\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, gnb.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","f626cb31":"y_pred = gnb.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Gaussian Naive Bayes\")\nplt.show()","f40f4501":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = gnb.predict_proba(X_validate)\n\nprecision[\"Gaussian Naive Bayes\"], recall[\"Gaussian Naive Bayes\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Gaussian Naive Bayes\"], tpr[\"Gaussian Naive Bayes\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Gaussian Naive Bayes\"], precision[\"Gaussian Naive Bayes\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Gaussian Naive Bayes\")\nplt.show()","4099d16f":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"Gaussian Naive Bayes\"], tpr[\"Gaussian Naive Bayes\"], where=\"post\")\n\nplt.title(\"ROC curve - Gaussian Naive Bayes\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","afbb946f":"mnb = MultinomialNB()\nmnb.fit(X_train, y_train)\n\nprint(\"Train score:\", mnb.score(X_train, y_train))\nprint(\"Validation score:\", mnb.score(X_validate, y_validate))","4898f3b2":"y_pred = mnb.predict(X_validate)\n\naccuracy[\"Multinomial Naive Bayes\"] = accuracy_score(y_validate, y_pred)\nf1[\"Multinomial Naive Bayes\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, mnb.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","d69bed79":"y_pred = mnb.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Multinomial Naive Bayes\")\nplt.show()","9dafc288":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = mnb.predict_proba(X_validate)\n\nprecision[\"Multinomial Naive Bayes\"], recall[\"Multinomial Naive Bayes\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Multinomial Naive Bayes\"], tpr[\"Multinomial Naive Bayes\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Multinomial Naive Bayes\"], precision[\"Multinomial Naive Bayes\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Multinomial Naive Bayes\")\nplt.show()","069e17b6":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"Multinomial Naive Bayes\"], tpr[\"Multinomial Naive Bayes\"], where=\"post\")\n\nplt.title(\"ROC curve - Multinomial Naive Bayes\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","42ba11ce":"bnb = BernoulliNB()\nbnb.fit(X_train, y_train)\n\nprint(\"Train score:\", bnb.score(X_train, y_train))\nprint(\"Validation score:\", bnb.score(X_validate, y_validate))","6fb5db51":"y_pred = bnb.predict(X_validate)\n\naccuracy[\"Bernoulli Naive Bayes\"] = accuracy_score(y_validate, y_pred)\nf1[\"Bernoulli Naive Bayes\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, mnb.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","824a3088":"y_pred = bnb.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Bernoulli Naive Bayes\")\nplt.show()","2bdd1589":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = bnb.predict_proba(X_validate)\n\nprecision[\"Bernoulli Naive Bayes\"], recall[\"Bernoulli Naive Bayes\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Bernoulli Naive Bayes\"], tpr[\"Bernoulli Naive Bayes\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Bernoulli Naive Bayes\"], precision[\"Bernoulli Naive Bayes\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Bernoulli Naive Bayes\")\nplt.show()","5f6185d9":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"Bernoulli Naive Bayes\"], tpr[\"Bernoulli Naive Bayes\"], where=\"post\")\n\nplt.title(\"ROC curve - Bernoulli Naive Bayes\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","383104c9":"sample = X\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\n\nX_train, X_validate, y_train, y_validate = train_test_split(X_sample, y_sample, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_validate.shape, y_validate.shape)","fd9ca39e":"mlp = MLPClassifier(random_state=42, verbose=False)\nparameters = [{\"hidden_layer_sizes\": [(64, 32), (32, 64, 32)], \"max_iter\": [200], \"solver\": [\"sgd\", \"adam\"], \"activation\": [\"tanh\", \"relu\"]}]\ngrid = GridSearchCV(mlp, parameters, verbose=5, n_jobs=-1)\n\nsample = X.sample(10_000, random_state=42)\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\ngrid.fit(X_sample, y_sample)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nprint(\"Train score:\", grid.score(X_train, y_train))\nprint(\"Validation score:\", grid.score(X_validate, y_validate))","d8f0750a":"print(\"Default scores:\")\nmlp.fit(X_train, y_train)\nprint(\"Train score:\", mlp.score(X_train, y_train))\nprint(\"Validation score:\", mlp.score(X_validate, y_validate))","e5ca075b":"pd.DataFrame(grid.cv_results_).sort_values(by=\"rank_test_score\")","1ca279b6":"y_pred = mlp.predict(X_validate)\n\naccuracy[\"Multi Layer Perceptron\"] = accuracy_score(y_validate, y_pred)\nf1[\"Multi Layer Perceptron\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, mlp.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","48f48004":"y_pred = mlp.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Multi Layer Perceptron\")\nplt.show()","a28e80cc":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = mlp.predict_proba(X_validate)\n\nprecision[\"Multi Layer Perceptron\"], recall[\"Multi Layer Perceptron\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Multi Layer Perceptron\"], tpr[\"Multi Layer Perceptron\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Multi Layer Perceptron\"], precision[\"Multi Layer Perceptron\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Multi Layer Perceptron\")\nplt.show()","572a4c53":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"Multi Layer Perceptron\"], tpr[\"Multi Layer Perceptron\"], where=\"post\")\n\nplt.title(\"ROC curve - Multi Layer Perceptron\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","bd698e79":"plt.figure(figsize=(20, 8))\nplt.title(\"Accuracy on Validation set for each model\")\nsns.barplot(list(range(len(accuracy))), list(accuracy.values()))\nplt.xticks(range(len(accuracy)), labels=accuracy.keys())\nplt.show()","33eb6292":"plt.figure(figsize=(20, 8))\nplt.title(\"F1 Score on Validation set for each model\")\nsns.barplot(list(range(len(f1))), list(f1.values()))\nplt.xticks(range(len(f1)), labels=f1.keys())\nplt.show()","dd9a3f1c":"plt.figure(figsize=(15, 8))\nfor key in f1.keys():\n    plt.step(recall[key], precision[key], where=\"post\", label=key)\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR curve\")\nplt.legend()\nplt.show()","5e9e3cf9":"plt.figure(figsize=(15, 8))\nfor key in f1.keys():\n    plt.step(fpr[key], tpr[key], where=\"post\", label=key)\n\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"ROC curve\")\nplt.legend()\nplt.show()","d8760f61":"sample = X_test\ny_test_sample = sample[\"Severity\"]\nX_test_sample = sample.drop(\"Severity\", axis=1)\n\ny_pred = rfc.predict(X_test_sample)\n\nprint(classification_report(y_test_sample, y_pred))\n\nconfmat = confusion_matrix(y_true=y_test_sample, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.show()","7f3d7378":" We can see that the minimum value is 0, meaning that some records are missing them and replaced them by putting zeros.\n For this reason, we are going to drop the records with missing values for these two columns.","e2a25a97":"In most frequent cases the weather condition is clear.","b0b52d4b":"## Logistic Regression","045f5c51":"Let's instead analyze Pressure and Visibility:","beda3542":"Now, remains only to encode the *City* feature. In order to, reduce the usage of memory and the number of features we used the `BinaryEncoder` included in the library category_encoders.","8f5a0292":"## Number of Accidents per State\n\nWith the following code we are going to create a map of each state of the US with a color based on the number of accidents present in the dataset for that state.","99a289b6":"Finally, we can merge the two dataframes and obtain the final dataframe X with the categorical features encoded.","0549f1e1":"# Results","5ba2d867":"## Handle erroneous and missing values\n\nHere we are going to clean the dataset from erroneous or missing values.\n\nLet's start looking to the Side column:","5788ec98":"## Medium distance by severity\n\nLet's now analyze the medium distance of an accident based on its severity.","2e33d1aa":"Let's first encode the boolean values in a numerical form.","c30ba738":"As we can see, we can group the values like we did with Weather_Condition:","e118586e":"We can see that there is one record without side, so we can drop it.","7ae14c47":"## Feature scaling\n\nIn this section we are going to scale and normalize the features.\n\nTo improve the performance of our models, we normalized the values of the continuous features.","06b1f049":"From the matrix we can see that the start and end GPS coordinates of the accidents are highly correlated. \n\nIn fact, from the medium distance shown before, the end of the accident is usually close to the start, so we can consider just one of them for the machine learning models.\n\nMoreover, the wind chill (temperature) is directly proportional to the temperature, so we can also drop one of them.\n\nWe can also see that the presence of a traffic signal is slightly correlated to the severity of an accident meaning that maybe traffic lights can help the traffic flow when an accident occurs.\n\nFrom the matrix we can also note that we couldn't compute the covariance with Turning_Loop, and that's because it's always False.","90ceb008":"Since a lot of records do not have informations about Precipitation, we are going to drop the feature.\n\nFor numerical features we are going to fill the missing features with the mean, while for categorical features like City, Wind_Direction, Weather_Condition and Civil_Twilight, we are going to delete the records with missing informations.","70b64065":"Next, let's analyze the missing values:","016312fe":"## Drop duplicates\n\nIn this section we are going to check if there are some duplicates in the dataset.","20f5ff2c":"# Dataset import\n\nIn the first place we are going to import the dataset using pandas.","4617f51e":"## Check correlation between features\n\n\nIn the next block is presented the correlation matrix between all the possible features, in the form of an heatmap. \n\nWith that we can observe the correlation between the different features of the dataset, in order to check if some features are highly correlated and remove one of them.","65c46800":"## Weather condition histogram\n\nWith the following code we can plot the histogram of the weather condition column.","a222ad5a":"To do so, we are going to replace them with a more generic description:","bba34b36":"As we can see from the plot above, the days with the most accidents are working days, while in the weekend we have a frequency of at least 2\/3 less. This may be due to the fact that during the weekend there are fewer cars on the road.","50c360da":"## Number of accidents for weekday\n\nHere is a plot of the number of accidents appened in each weekday.","46e40544":"As we can see from the map and the plot above California is the state with the highest number of accidents, then we have Texas and Florida.","f3235282":"As we can see, most of the accidents occured near a traffic signal, expecially where a junction or a crossing was present. \n\nThe fourth most common road feature, instead, was the presence of a nearby station, probably because of the high presence of vehicles.","7300c7e6":"## Multi Layer Perceptron","ec192ccb":"## Most frequent road features\n\nNow we are going to analyze which are the most frequent road features in accidents.","be07d861":"## Most frequent words in the description of an accident with severity 4\n\nWe are going to compute the most frequent words in the description column of the accidents with a value of severity equal to 4, using some stopwords from the english language.","88351338":"We can see that the most used word in the description is *closed*. Subsequent words are *accident*, *due* and *road*.","9727841c":"# Foundation of Data Science Project 2020\/2021\n\nIn our project we are going to predict the severity of an accident based on a set of features (weather, road features, time) about some accidents in US.","0ba21d83":"The severity attribute as we can see from the previous plot is highly unbalanced, the number of accident with the severity 1 is very small instead the number of accident with severity 2 is much higher.\n\nSo, in order to balance the data we are going to undersample all the categories to the number of records of the minority category, in this case the severity 1.\nWe thought this was a good choice since this leaves us with a good amount of records for each category, which is ~25k records","825cd969":"# Exploratory Data Analysis (EDA)\n\nIn this first part of this notebook we are going to do an EDA of the dataset.","fd090e81":"Even though Precipitation and Pressure have a small variance, there is no need to drop them since they usually have small increments.","87fce534":"## Random Forest","33886b13":"## Feature addition\n\nWe decided to decompose the Start_Time feature in year, month, day, weekday, hour and minute, in order to feed them to the models.","407bd65e":"## Handle unbalanced data","0dab57d0":"Now we can encode the categorical features using the method `get_dummies()` which converts the features with the one-hot encoding.","4eb23ad0":"In this graph we can see that the distance of the accident is more or less proportional to the severity, and in fact accidents with severity 4 have the longest distance.","5f2a179d":"## Feature encoding\n\nFinally, in this section we are going to encode the categorical features.","86c8709c":"Let's check also the Wind_Direction field:","3444717e":"First of all, we show the number of unique classes for each categorical feature.","21e97ff3":"## Support Vector Machine","b668e4ef":"## Naive Bayes","18e6391e":"If we analyze the weather conditions, we can see that there are lots of them, so it's better to reduce the number of unique conditions.","8881dbcd":"## Feature selection\n\nHere is the process of feature selection, in order to select the best features from which our models can learn.\n\nFrom the observations made with the correlation matrix, we are going to drop the following features:\n- End_Lat and End_Lng\n- Wind Chill\n\nMoreover, we are going to drop the following features:\n- ID, Source: since they don't carry any information for the severity\n- TMC: because it could already contains information about the accident severity\n- Start_Time: because it was decomposed by the time features added before (day, month, weekday)\n- End_Time: beause we cannot know in advance when the traffic flow will become regular again\n- Description: most description only report the name of the road of the accident, and so we decided to omit this feature for semplicity\n- Number, Street, County, State, Zipcode, Country: because we just focus on the City where the accident happened\n- Timezone, Airport_Code, Weather_Timestamp: because they are not useful for our task\n- Turning_Loop: since it's always False\n- Sunrise_Sunset, Nautical_Twilight, Astronomical_Twilight: because they are redundant","3853e827":"# Model\n\nIn this section we will test sevaral models to identify the best one for this task.\n\nIn particular, we will use the following models:\n- Logistic Regression;\n- Support Vector Machine;\n- Decision Tree\n- Random Forest\n- Naive Bayes\n- Multi Layer Perceptron\n\nMoreover, we are going to seach the best hyperparameters to produce the best results for each model. In the end, we will show the performance of each model using different metrics: precision, recall, accuracy.","57bbcac3":"# Data preprocessing\n\nIn this phase we are going to process the dataset in order to make it usable for the machine learning models.","1c96b513":"## Decision Tree","bbe27b6f":"# Required libraries","80b1ee10":"## Check features variance\n\nIn this section we are going to check the variance for each feature in order to remove features with a very low variance beacuse they can't help to discriminate instances."}}