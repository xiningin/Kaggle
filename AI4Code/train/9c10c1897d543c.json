{"cell_type":{"faa85969":"code","eed9cdce":"code","fdb784e4":"code","c0b0dc22":"code","2ef56aed":"code","de58fed1":"code","457bc571":"code","1c72ca6b":"code","f79e2a28":"code","8c4a7bec":"code","31db3eaa":"code","3035963f":"code","f47b4226":"code","a5f4204d":"code","a7692b16":"code","c3cf4106":"code","546c1540":"code","31255994":"code","2d915bac":"code","d8046943":"code","1e1c4368":"code","c923d40a":"code","1fbf94d9":"code","1915754c":"code","70cf8fa9":"code","7c1452d0":"code","14c79e4f":"code","9760d113":"code","60c5f171":"code","ce86f163":"code","c68ca994":"code","8c435a86":"code","5fc7e036":"code","94e80729":"code","e9aa8707":"code","6b2ebd02":"code","41b13435":"code","e42b46ec":"code","911e3bea":"code","3d115672":"code","6c4957e9":"code","b826e6bc":"code","75e6c488":"code","065ba429":"code","9de9e999":"code","eb10c818":"code","3da1e76e":"code","bd29b597":"code","dd8fb68f":"code","6fe56f39":"code","c9e0bacf":"code","311ac30e":"code","b117d261":"code","e3dc2a91":"code","868625fa":"code","627ab6ed":"code","4fbac237":"code","1f0b62db":"code","e68aa69e":"code","d98a2638":"code","c9138efc":"code","2370fda1":"code","17432d4f":"code","2aff0f07":"code","4df50eab":"code","32466516":"code","ebae165f":"code","227656cd":"code","9254919c":"code","19917a27":"code","acedca2e":"markdown","f5f83434":"markdown","af37549c":"markdown","6561cc27":"markdown","74903af7":"markdown","66e91829":"markdown","293c72c8":"markdown","b0cf4af5":"markdown","812dbb9e":"markdown","6352ab4e":"markdown","5abc3fa7":"markdown","85c6925e":"markdown","a2a48f5a":"markdown","73b67bcf":"markdown","24da9f91":"markdown","c345cfa6":"markdown","5d443b21":"markdown","770d596e":"markdown","ca740e33":"markdown","afc009f5":"markdown","229c87bb":"markdown","cf527f8d":"markdown","24a364e4":"markdown","b4fbe59d":"markdown","9da5cb9f":"markdown","706ed91c":"markdown","a1f5c747":"markdown","026a0c9e":"markdown","2d0d10cb":"markdown","28c488ed":"markdown","4abd0703":"markdown","c37160f6":"markdown","27dac644":"markdown","bee479c9":"markdown","63656ebc":"markdown","ab14fb6b":"markdown","69ea1792":"markdown","19cb32e4":"markdown","3daee58c":"markdown","7e46e642":"markdown","037f9761":"markdown","ee785ea6":"markdown","5e2f47fc":"markdown","d0d3420a":"markdown","e11bff18":"markdown","bad7878b":"markdown","a47a6c6e":"markdown","6abfb923":"markdown","052000f9":"markdown","20fb1fd6":"markdown","65fbcb30":"markdown","33ddf630":"markdown","fe366f24":"markdown","ee2df755":"markdown","8e48cc98":"markdown","0e2f30fc":"markdown","16c8de08":"markdown","542eb244":"markdown","cd5ff0bd":"markdown","99eb8e7f":"markdown","ffcce670":"markdown","d06400e9":"markdown","f729f019":"markdown","0ff0cf6b":"markdown","d3b3c316":"markdown","1374c6f2":"markdown","000616a8":"markdown","a634322b":"markdown"},"source":{"faa85969":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier","eed9cdce":"data = pd.read_csv('\/kaggle\/input\/Churn_Modelling.csv')\ndata.head()","fdb784e4":"data.shape","c0b0dc22":"data.info()","2ef56aed":"data.describe()","de58fed1":"for column in data.columns : \n    print('Number of unique data for {0} is {1}'.format(column , len(data[column].unique())))\n    print('unique data for {0} is {1}'.format(column , data[column].unique()))\n    print('=====================================')","457bc571":"data.drop(['RowNumber', 'CustomerId', 'Surname' ], axis=1, inplace=True)","1c72ca6b":"data.head()","f79e2a28":"for column in data.columns : \n    print('Number of unique data for {0} is {1}'.format(column , len(data[column].unique())))\n    print('unique data for {0} is {1}'.format(column , data[column].unique()))\n    print('=====================================')","8c4a7bec":"def make_pie(feature) : \n    plt.pie(data[feature].value_counts(),labels=list(data[feature].value_counts().index),\n        autopct ='%1.2f%%' , labeldistance = 1.1,explode = [0.05 for i in range(len(data[feature].value_counts()))] )\n    plt.show()","31db3eaa":"def make_countplot(feature) :\n    sns.countplot(x=feature, data=data,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"prism\", 3)) ","3035963f":"def make_kdeplot(feature) : \n    sns.kdeplot(data[feature], shade=True)","f47b4226":"def divide_feature(feature,n):\n    return round((data[feature]- data[feature].min())\/n)","a5f4204d":"def make_label_encoder(original_feature , new_feature) : \n    enc  = LabelEncoder()\n    enc.fit(data[original_feature])\n    data[new_feature] = enc.transform(data[original_feature])\n    data.drop([original_feature],axis=1, inplace=True)","a7692b16":"def make_standardization(feature) : \n    data[feature] =  (data[feature] - data[feature].mean()) \/ (data[feature].max() - data[feature].min())","c3cf4106":"def make_report() : \n    print(classification_report(y_test,y_pred))\n    print('************************************')\n    CM = confusion_matrix(y_test, y_pred)\n    print('Confusion Matrix is : \\n', CM)\n    print('************************************')\n    sns.heatmap(CM, center = True)\n    plt.show()","546c1540":"make_countplot(\"Exited\")","31255994":"len(data['CreditScore'].unique())","2d915bac":"data['temp'] = divide_feature('CreditScore',100)","d8046943":"data.head()","1e1c4368":"make_countplot('temp')","c923d40a":"data.drop([\"temp\" ], axis=1, inplace=True)","1fbf94d9":"make_countplot(\"Geography\")","1915754c":"make_pie('Geography')","70cf8fa9":"make_countplot(\"Gender\")","7c1452d0":"make_pie(\"Age\")","14c79e4f":"data['temp'] = divide_feature('Age',10)","9760d113":"make_pie('temp')","60c5f171":"make_kdeplot('Age')","ce86f163":"data.drop([\"temp\" ], axis=1, inplace=True)","c68ca994":"make_countplot(\"Tenure\")","8c435a86":"make_kdeplot('Balance')","5fc7e036":"data['temp'] = divide_feature('Balance',10000)\nprint('Number of Sectors are {}'.format(len(data['temp'].unique())))","94e80729":"make_pie('temp')","e9aa8707":"data.drop([\"temp\" ], axis=1, inplace=True)","6b2ebd02":"make_pie('NumOfProducts')","41b13435":"make_countplot('HasCrCard')","e42b46ec":"make_pie('IsActiveMember')","911e3bea":"len(data['EstimatedSalary'].unique())","3d115672":"data['temp'] = divide_feature('EstimatedSalary',10000)\nprint('Number of Sectors are {}'.format(len(data['temp'].unique())))","6c4957e9":"make_pie('temp')","b826e6bc":"make_kdeplot('temp')","75e6c488":"data.drop([\"temp\"], axis=1, inplace=True)","065ba429":"data.head()","9de9e999":"make_label_encoder('Geography' , 'Geography Code')","eb10c818":"data.head()","3da1e76e":"make_label_encoder('Gender' , 'Gender Code')\ndata.head()","bd29b597":"for column in data.columns  : \n    if not column  =='Exited' :\n        make_standardization(column)","dd8fb68f":"data.head()","6fe56f39":"X = data.drop(['Exited'], axis=1, inplace=False)\ny = data['Exited']","c9e0bacf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44, shuffle =True)\n\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","311ac30e":"LogisticRegressionModel = LogisticRegression(penalty='l2',solver='sag',C=1.0,random_state=33)\nLogisticRegressionModel.fit(X_train, y_train)","b117d261":"print('LogisticRegressionModel Train Score is : ' , LogisticRegressionModel.score(X_train, y_train))\nprint('LogisticRegressionModel Test Score is : ' , LogisticRegressionModel.score(X_test, y_test))\nprint('LogisticRegressionModel Classes are : ' , LogisticRegressionModel.classes_)\nprint('LogisticRegressionModel No. of iteratios is : ' , LogisticRegressionModel.n_iter_)\nprint('----------------------------------------------------')","e3dc2a91":"y_pred = LogisticRegressionModel.predict(X_test)\ny_pred_prob = LogisticRegressionModel.predict_proba(X_test)\nmake_report()","868625fa":"GaussianNBModel = GaussianNB()\nGaussianNBModel.fit(X_train, y_train)\n\nprint('GaussianNBModel Train Score is : ' , GaussianNBModel.score(X_train, y_train))\nprint('GaussianNBModel Test Score is : ' , GaussianNBModel.score(X_test, y_test))","627ab6ed":"y_pred = GaussianNBModel.predict(X_test)\ny_pred_prob = GaussianNBModel.predict_proba(X_test)\nmake_report()","4fbac237":"DecisionTreeClassifierModel = DecisionTreeClassifier(criterion='gini',max_depth=3,random_state=33) #criterion can be entropy\nDecisionTreeClassifierModel.fit(X_train, y_train)\n\nprint('DecisionTreeClassifierModel Train Score is : ' , DecisionTreeClassifierModel.score(X_train, y_train))\nprint('DecisionTreeClassifierModel Test Score is : ' , DecisionTreeClassifierModel.score(X_test, y_test))\nprint('DecisionTreeClassifierModel Classes are : ' , DecisionTreeClassifierModel.classes_)\nprint('DecisionTreeClassifierModel feature importances are : ' , DecisionTreeClassifierModel.feature_importances_)","1f0b62db":"y_pred = DecisionTreeClassifierModel.predict(X_test)\ny_pred_prob = DecisionTreeClassifierModel.predict_proba(X_test)\n\nmake_report()","e68aa69e":"SVCModel = SVC(kernel= 'sigmoid',# it can be also linear,poly,sigmoid,precomputed\n               max_iter=1000,C=0.5,gamma='auto')\nSVCModel.fit(X_train, y_train)\n\nprint('SVCModel Train Score is : ' , SVCModel.score(X_train, y_train))\nprint('SVCModel Test Score is : ' , SVCModel.score(X_test, y_test))","d98a2638":"RandomForestClassifierModel = RandomForestClassifier(criterion = 'gini',n_estimators=1000,max_depth=2,random_state=33) #criterion can be also : entropy \nRandomForestClassifierModel.fit(X_train, y_train)\n\nprint('RandomForestClassifierModel Train Score is : ' , RandomForestClassifierModel.score(X_train, y_train))\nprint('RandomForestClassifierModel Test Score is : ' , RandomForestClassifierModel.score(X_test, y_test))\nprint('RandomForestClassifierModel features importances are : ' , RandomForestClassifierModel.feature_importances_)\n","c9138efc":"GBCModel = GradientBoostingClassifier(n_estimators=100,max_depth=5,random_state=33) \nGBCModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('GBCModel Train Score is : ' , GBCModel.score(X_train, y_train))\nprint('GBCModel Test Score is : ' , GBCModel.score(X_test, y_test))","2370fda1":"y_pred = GBCModel.predict(X_test)\ny_pred_prob = GBCModel.predict_proba(X_test)\n\nprint('Predicted Value for GBCModel is : ' , y_pred[:10])\nprint('Prediction Probabilities Value for GBCModel is : ' , y_pred_prob[:10])","17432d4f":"make_report()","2aff0f07":"SelectedModel = GradientBoostingClassifier()\nSelectedParameters = {'loss':('deviance', 'exponential'), 'max_depth':[1,2,3,4,5] , 'n_estimators':[50,75,100]}\n\nGridSearchModel = GridSearchCV(SelectedModel,SelectedParameters,cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train, y_train)\nsorted(GridSearchModel.cv_results_.keys())\nGridSearchResults = pd.DataFrame(GridSearchModel.cv_results_)[['mean_test_score', 'std_test_score', 'params' , 'rank_test_score' , 'mean_fit_time']]","4df50eab":"print('All Results are :\\n', GridSearchResults )\nprint('Best Score is :', GridSearchModel.best_score_)\nprint('Best Parameters are :', GridSearchModel.best_params_)\nprint('Best Estimator is :', GridSearchModel.best_estimator_)","32466516":"GBCModel = GridSearchModel.best_estimator_\nGBCModel.fit(X_train, y_train)","ebae165f":"print('GBCModel Train Score is : ' , GBCModel.score(X_train, y_train))\nprint('GBCModel Test Score is : ' , GBCModel.score(X_test, y_test))\nprint('GBCModel features importances are : ' , GBCModel.feature_importances_)","227656cd":"y_pred = GBCModel.predict(X_test)\ny_pred_prob = GBCModel.predict_proba(X_test)\n\nprint('Predicted Value for GBCModel is : ' , y_pred)\nprint('Prediction Probabilities Value for GBCModel is : ' , y_pred_prob)","9254919c":"X_test.insert(10,'Predicted Valued',y_pred)","19917a27":"X_test.head(30)","acedca2e":"also , we'll need to make standardization for each feature here","f5f83434":"what is the dimension ","af37549c":"now let's drop it ","6561cc27":"then a function for making countplot using seaborn","74903af7":"ok , we now finished data processing , so we can move to get dummies step\n\n_____\n\n\n# Get Dummies\n\nfirst let's now what categorical features needed to convert it ","66e91829":"not exactly equally divided but they are pretty close to each other \n\n____\n\n\nnow let's have a look to the Age distribution ","293c72c8":"which refer to us that almost 75% of people are from first 2 segmentaions , ok how about kdeplot , for the Age feature itself ","b0cf4af5":"____\n\nnow we can have a look to Tenure feature","812dbb9e":"kinda equally distributed , let's have a look to the kdeplot","6352ab4e":"great . now if he is an active member or now","5abc3fa7":"majority of it either 1 or 2 , so this data will affect in a bad way finding any product number 3 or 4 , but there is nothing to do here\n\n___\n\nok, how about either he had a card or not","85c6925e":"and at last to make a classification report using its class from sklearn","a2a48f5a":"now let's use it to predict the test value","73b67bcf":"it's a big a mount which will not enable us to plot it easily , since it's numerical values so we can use them in training , but we need to divide it now into segmentaion to have a look to it , let's use this function ","24da9f91":"almost same result , majority of people from 30 to 50 , ok , let's drop the temp feature","c345cfa6":"ok , majority of them in 2nd , 3rd & 4th segmentaion , now we can drop it since we'll not use it in training","5d443b21":"_____\n\nhow about the original country , let's have a look at it","770d596e":"____\n\nboth features Geography & Gender  , so let's convert both of them into new features & frop the original features ","ca740e33":"____\n\nok better accuracy , let's use the best model to fit & predict the data","afc009f5":"also for making dummies for categorical features , using LabelEncoder from sklearn","229c87bb":"majority of people either have zero balance , or between 10 & 15 thousand . \n\nto have an accurate look , let's divide it ","cf527f8d":"Ok , better accuracy so we can focus in the model , let's check the ","24a364e4":"almost equally distributed  . . \n\nhow about balane , let's graph it","b4fbe59d":"\n____\n\n\nhow about Gradient Boosting Classifier, will it be better ? ","9da5cb9f":"now the gender","706ed91c":"ow about Nomber of Products ? ","a1f5c747":"____\n\nlooks clean data , also alkl features even numerical or categorical , so we don't have a mized data which need conversion \n\nok , how about the details of it ? ","026a0c9e":"now we can know the countplot for each segmentation","2d0d10cb":"let's have a look","28c488ed":"___\n\nnow let's check SVC","4abd0703":"a slightly better","c37160f6":"Ok , how about the accuracy ? ","27dac644":"___\n\nhow about Gaussian NB , will it helps ? ","bee479c9":"now how data looks like ? ","63656ebc":"almost equl numbers \n\n___\n\n\nhow about the estimated salary , let's know its unique values","ab14fb6b":"ok , now we can plot it easily ","69ea1792":"how about Decision Tree ? ","19cb32e4":"____\n\n\n# Needed Functions\n\nso before we handle the dummies values for categrocial features , let's first build an important functions that we'll need , to know the relationship & the correlations between features & each other\n\n_____\n\nfirst a function to make pie chart depend on the the value counts & their index\n\n","3daee58c":"kinda more clear , ok drop it ","7e46e642":"a little better , but we might catch something best","037f9761":"another one for kdeplot also using seaborn","ee785ea6":"25 segmentations are fine , now let's make pie chart about it ","5e2f47fc":"& see the final Result","d0d3420a":"also we'll need this function to divide some features into few segmentations","e11bff18":"\n_____\n\n# Data Standardization\n\nok , lets use the defined function above , to startdardize all features , except the output","bad7878b":"no we can insert the predicted value to X_test","a47a6c6e":"____\n\n# Data Correlations\n\nnow let's use these functions . to show everything we need in features \n\nlet's start with countplotting the output , to know how many people are exited & how many are not","6abfb923":"let's see it in the data","052000f9":"how it looks now ? ","20fb1fd6":"how about the score ? ","65fbcb30":"____\n\n# Data Splitting\n\nnow we are ready to define X , y data\n","33ddf630":"almost 50% of people are from france & the rest are equally divided between spain & germany  , ok will pie chart graph whelp us in something ? ","fe366f24":"oh , very far from calling it suitable , how about Random Forest ? ","ee2df755":"ok , about 20 % , which is enough data for training both types\n\nnow when we move to the creditscore features , how many unique values it contain ? ","8e48cc98":"_____\n\n# Reading Data\n\nthen read the data file","0e2f30fc":"\n____\n\n# Using GridSearch\n\nsince we see the GBClassifier is the most suitable model , let's use GridSearch tool to look for the best parameters for it","16c8de08":"_____\n\ngreat , what is the type of values , & is there any nulls ? ","542eb244":"we need to have another look to the unique values of features ","cd5ff0bd":"now split it using sklearn","99eb8e7f":"since it's a big amount of unique values , we have to divide it","ffcce670":"____\n\n\n# Unique Values \n\nok we can notice that row number is just a series of number , so it'll not be helpful for training , so we'll drop it later \n\nalso we can notice a binary values (just ones & zeros) at HasCrCard , IsActiveMember , and ofcourse the output Exited\n\nwe need to look to the unique values of each feature , and this can be easily done here ","d06400e9":"_____\n\n# Building the Model\n\nthere are several classifier models , let's start with LogisticRegression","f729f019":"now we can make the pie again","0ff0cf6b":"ok it gave us the same idea \n\n_____\n\nhow about the Gender distribution ? ","d3b3c316":"# Churn Classification\nby : Hesham Asem\n\nthis is a data set which contain several features , & we need to apply classification model , to be able to detect if the person exited or not\n\nhere is the data : \n\nhttps:\/\/www.kaggle.com\/mrtechnical011\/classification-dataset\n\n\n\nso let's start importing needed libraries","1374c6f2":"____\n\nso it's clear that we'll not use ('RowNumber', 'CustomerId', 'Surname') , since they will help us with nothing in trainging the model , let's drop them \n","000616a8":"not very good accuracy , let's see classification report for it","a634322b":"Oh , since the age unique values are so much , so we'll need to divide them into segmentaions  "}}