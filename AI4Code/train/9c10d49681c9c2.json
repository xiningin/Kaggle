{"cell_type":{"b181fceb":"code","41a281fd":"code","f338abb3":"code","730b4339":"code","5f9e9176":"code","89fda0ed":"code","612c7cd0":"code","de34ddf1":"code","c3918769":"code","3573dff1":"code","ef5f109d":"code","8a1cdfee":"code","ed114b4b":"code","e0a3c76e":"code","c8a6e715":"code","974efbd9":"code","d38c3867":"code","40822191":"code","6f39b063":"code","5c77afca":"code","fa06c200":"code","4aef8d64":"code","207f5f5d":"code","91e9299c":"code","2f8d7a4a":"code","cd8c9b38":"code","7e09203d":"code","2b722fb1":"code","52ad1fb7":"code","40e20f5c":"code","c0f4e9c4":"code","c8fd2d2c":"code","229f1fec":"code","98d04512":"code","2652b5a0":"code","53bf4ee6":"code","9b89df09":"code","d3d72f98":"code","620cd671":"code","e02fa40d":"code","0c4acaae":"code","5cf6cfde":"code","24547a8c":"code","2e741a3a":"code","eb5e428b":"code","c2055c61":"code","6964f6c0":"code","e3d8f2c1":"code","e0c99867":"code","5b8a4d55":"code","256363a5":"code","58f4b953":"code","852d21f6":"code","35e929d3":"code","efb23123":"code","a4c2e693":"code","f6f0df05":"code","022b98f9":"code","97650728":"code","f3fb4a15":"code","a482de83":"code","0a8d7086":"code","4b0b6bf6":"code","b386c2c7":"code","25de0e73":"code","b4a0865d":"code","75c6ee57":"code","f3816d3b":"code","92b92491":"code","98ef6f63":"code","21c95d6f":"code","687c698c":"code","8a24757a":"code","b24732ef":"code","43db2b14":"code","2eae9b65":"code","b7fa52e8":"code","69b62f09":"code","769adc08":"code","f30d62ad":"code","c209d433":"code","2f7ab12f":"code","a6e98a23":"code","b4fa127e":"code","713189a3":"code","ac380b14":"code","d2450a7e":"code","226e5b1a":"code","9436c4fa":"code","57b28a91":"code","28606a3a":"code","6c98f3a9":"code","55fa7d25":"code","7993f6bb":"code","a874496a":"code","f02d7c83":"code","1567d6c7":"code","038f6158":"code","08c69b9a":"code","0bd362a7":"code","4e229f45":"code","4526c068":"code","afa51854":"code","a2a4b2c9":"code","75ff6177":"code","5dc12b85":"code","b7819f82":"code","3b239f3e":"code","f21a5c71":"code","2a0a9aed":"code","e5f04a9b":"code","45f96a06":"code","aab7f3e5":"code","c4a7042a":"code","d31d463e":"code","c60af772":"code","40622ebc":"code","db94546a":"code","35c37547":"code","c48c67d1":"code","a7bd7f37":"code","7972eefc":"code","e5d8bc1c":"code","9c2adaf2":"code","92bac343":"code","110bc15b":"code","840842d8":"code","5a7287df":"code","99a4d7d4":"code","2e21433d":"code","c96f6611":"code","25f82b6a":"code","d11d5b71":"code","60d8d283":"code","f44d080a":"code","91c636ee":"code","bbd05f30":"code","c163a272":"code","876022fe":"code","77183c0a":"code","9a259e06":"code","e20c38e7":"code","f0011e1c":"code","94caa3e8":"code","405dcdd0":"code","df6865ce":"code","e60691fe":"code","4deb71ad":"code","83f9bcc0":"code","dda5379c":"code","42af4d6e":"code","7aa7b08c":"code","71109ff4":"code","e41c2153":"code","42de5090":"code","1d439130":"code","aef582c8":"code","f2b46cbc":"code","b1dd381a":"code","22f2320a":"code","1f14b4ce":"code","d741a12b":"code","8c355a64":"code","0b9b21bb":"code","9f8df765":"code","fa9dccde":"code","6cae781b":"code","9ffa691e":"code","357c2fb9":"code","460e6c78":"code","a7bfe2ae":"code","775ec1bf":"code","da209b68":"code","b65d3b30":"code","33906433":"code","6ba37bf3":"code","784fa807":"code","e9cb479e":"code","a9192b45":"code","845ae064":"code","2db4e219":"code","c6c97fd9":"markdown","a993fd7a":"markdown","e4f6436f":"markdown","45115cfa":"markdown","f1561e07":"markdown","88a08e07":"markdown","718f6435":"markdown","980d7b34":"markdown","830f1f5b":"markdown","edd01f4d":"markdown","ce7ff446":"markdown","cd2b3245":"markdown","84b7c751":"markdown","166ed67a":"markdown","ca8d9c38":"markdown","5413b500":"markdown","43f93453":"markdown","36864226":"markdown","bfcdf4a1":"markdown","6e251ebd":"markdown","e6f42873":"markdown","ff841044":"markdown","19fc9d28":"markdown","7ca5b172":"markdown","5f1583f4":"markdown","c6355c81":"markdown","a77c59b9":"markdown","ff41a747":"markdown","ee3f01de":"markdown","578de879":"markdown","39869aff":"markdown","2254405d":"markdown","f9477d57":"markdown","6ace03e8":"markdown","9cedb30f":"markdown","405351a6":"markdown","50d21fcf":"markdown","0da350d7":"markdown","761703bd":"markdown","ac8536fd":"markdown","d82d8f7a":"markdown","0ae1bd41":"markdown","c37e96e3":"markdown","62bfdc1a":"markdown","48dd7548":"markdown","8af8cf0d":"markdown","4e0a6c4b":"markdown","f04d32f1":"markdown","e4b9c867":"markdown","f0297913":"markdown","42447610":"markdown","af964553":"markdown","61ca72e4":"markdown","016b8cfb":"markdown"},"source":{"b181fceb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\npd.set_option('display.max_columns', 1000)\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nimport datetime\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nsns.set()\n%matplotlib inline","41a281fd":"train_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv', index_col = 'TransactionID')\nprint('Successfully loaded train_identity')\n\ntrain_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv', index_col = 'TransactionID')\nprint('Successfully loaded train_transaction')\n\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv', index_col = 'TransactionID')\nprint('Successfully loaded test_identity')\n\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv', index_col = 'TransactionID')\nprint('Successfully loaded test_transaction')\n\nsub = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')\nprint('Successfully loaded sample_submisssion')\n\nprint('Data was successfully loades!')\n\nprint('Merging data....')\ntrain = train_transaction.merge(train_identity, how = 'left', left_index = True, right_index = True)\ntest = test_transaction.merge(test_identity, how = 'left', left_index = True, right_index = True)\n\nprint('Data was successfully merged!')\n\ndel train_identity, train_transaction, test_identity, test_transaction\n\nprint('Train dataset has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\nprint('Test dataset has {} rows and {} columns'.format(test.shape[0], test.shape[1]))","f338abb3":"# target variable\n\ntrain['TransactionAmt'] = train['TransactionAmt'].astype(float)\ntotal = len(train)\ntotal_amt = train.groupby(['isFraud'])['TransactionAmt'].sum().sum()\nplt.figure(figsize=(12,5))\n\nplt.subplot(121)\nplot_tr = sns.countplot(x='isFraud', data=train)\nplot_tr.set_title(\"Fraud Transactions Distribution \\n 0: No Fraud | 1: Fraud\", fontsize=18)\nplot_tr.set_xlabel(\"Is fraud?\", fontsize=16)\nplot_tr.set_ylabel('Count', fontsize=16)\nfor p in plot_tr.patches:\n    height = p.get_height()\n    plot_tr.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=15) \n    \npercent_amt = (train.groupby(['isFraud'])['TransactionAmt'].sum())\npercent_amt = percent_amt.reset_index()\nplt.subplot(122)\nplot_tr_2 = sns.barplot(x='isFraud', y='TransactionAmt',  dodge=True, data=percent_amt)\nplot_tr_2.set_title(\"% Total Amount in Transaction Amt \\n 0: No Fraud | 1: Fraud\", fontsize=18)\nplot_tr_2.set_xlabel(\"Is fraud?\", fontsize=16)\nplot_tr_2.set_ylabel('Total Transaction Amount Scalar', fontsize=16)\nfor p in plot_tr_2.patches:\n    height = p.get_height()\n    plot_tr_2.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total_amt * 100),\n            ha=\"center\", fontsize=15)","730b4339":"def missing_values(df):\n    df1 = pd.DataFrame(df.isnull().sum()).reset_index()\n    df1.columns = ['features', 'freq']\n    df1['percentage'] = df1['freq']\/df.shape[0]\n    df1.sort_values('percentage', ascending = False, inplace = True)\n    return df1\n\nmissing_train = missing_values(train)\nmissing_train.columns = ['features', 'freq_tr', 'percentage_tr']\nmissing_train","5f9e9176":"missing_test = missing_values(test)\nmissing_test.columns = ['features', 'freq_te', 'percentage_te']\nmissing_test","89fda0ed":"missing = missing_train.merge(missing_test, on = 'features')\nmissing.head(10)","612c7cd0":"train['id_24'].value_counts(normalize = True, dropna = False)","de34ddf1":"test['id_24'].value_counts(normalize = True, dropna = False)","c3918769":"drop_features = []\ndrop_features.append('id_24')","3573dff1":"for i in ['id_24', 'id_25', 'id_08', 'id_07', 'id_21', 'id_26', 'id_27', 'id_23', 'id_22']:\n    drop_features.append(i)\ndrop_features","ef5f109d":"train['dist2'].value_counts(normalize = True, dropna = False).head()","8a1cdfee":"test['dist2'].value_counts(normalize = True, dropna = False).head()","ed114b4b":"def plot_feature(train, test, feature, log = False):\n    df1_0 = train[train['isFraud']==0]\n    df1_1 = train[train['isFraud']==1]\n    fig, (ax1, ax2) = plt.subplots(2,1, figsize=(13,9))\n    if log == True:\n        sns.kdeplot(np.log(df1_0[feature]), shade = True, label = 'Not Fraud', ax = ax1)\n        sns.kdeplot(np.log(df1_1[feature]), shade = True, label = 'Fraud', ax = ax1)\n    else:\n        sns.kdeplot(df1_0[feature], shade = True, label = 'Not Fraud', ax = ax1)\n        sns.kdeplot(df1_1[feature], shade = True, label = 'Fraud', ax = ax1)\n        \n    \n    if log == True:\n        sns.kdeplot(np.log(train[feature]), shade = True, label = 'Train', ax = ax2)\n        sns.kdeplot(np.log(test[feature]), shade = True, label = 'Test', ax = ax2)\n    else:\n        sns.kdeplot(train[feature], shade = True, label = 'Train', ax = ax2)\n        sns.kdeplot(test[feature], shade = True, label = 'Test', ax = ax2)\n        \nplot_feature(train, test, 'dist2', True)","e0a3c76e":"check_features = ['dist2']","c8a6e715":"missing[missing['features']=='D7']","974efbd9":"plot_feature(train, test, 'D7', True)","d38c3867":"check_features.append('D7')","40822191":"missing[missing['features']=='id_18']","6f39b063":"plot_feature(train, test, 'id_18', True)","5c77afca":"drop_features.append('id_18')","fa06c200":"missing[missing['features']=='D13']","4aef8d64":"plot_feature(train, test, 'D13', True)","207f5f5d":"missing[missing['features']=='D14']","91e9299c":"plot_feature(train, test, 'D14', True)","2f8d7a4a":"missing[missing['features']=='D12']","cd8c9b38":"plot_feature(train, test, 'D12', True)","7e09203d":"missing[missing['features']=='id_04']","2b722fb1":"def plot_c_feature(train, feature):\n    tmp = pd.crosstab(train[feature], train['isFraud'], normalize = 'index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    plt.figure(figsize=(13,9))\n    plot_1 = sns.countplot(x = feature, hue = 'isFraud', data = train)\n    plt.legend(title = 'Fraud', loc = 'best', labels = ['No', 'Yes'])\n    plot_1_1 = plot_1.twinx()\n    plot_1_1 = sns.pointplot(x = feature, y = 'Fraud', data = tmp, color = 'black', \n                         order = list(tmp[feature].values), legend = False)\n    plot_1_1.set_ylabel('% of Fraud Transactions', fontsize = 16)\n    plot_1.set_ylabel(\"Count\", fontsize=16)\n    \n    \nplot_c_feature(train, 'id_04')","52ad1fb7":"missing[missing['features']=='id_03']","40e20f5c":"plot_c_feature(train, 'id_03')","c0f4e9c4":"missing[missing['features']=='D6']","c8fd2d2c":"plot_feature(train, test, 'D6', True)","229f1fec":"missing[missing['features']=='id_33']","98d04512":"plot_c_feature(train, 'id_33')","2652b5a0":"missing[missing['features']=='id_09']","53bf4ee6":"plot_c_feature(train, 'id_09')","9b89df09":"missing[missing['features']=='id_10']","d3d72f98":"plot_c_feature(train, 'id_10')","620cd671":"check_features.append('id_10')","e02fa40d":"missing[missing['features']=='D9']","0c4acaae":"plot_feature(train, test, 'D9', True)","5cf6cfde":"missing[missing['features']=='D8']","24547a8c":"plot_feature(train, test, 'D8', True)","2e741a3a":"missing[missing['features']=='id_30']","eb5e428b":"plot_c_feature(train, 'id_30')","c2055c61":"missing[missing['features']=='id_32']","6964f6c0":"plot_c_feature(train, 'id_32')","e3d8f2c1":"missing[missing['features']=='id_34']","e0c99867":"plot_c_feature(train, 'id_34')","5b8a4d55":"drop_features.append('id_34')","256363a5":"missing[missing['features']=='id_14']","58f4b953":"plot_c_feature(train, 'id_14')","852d21f6":"missing[missing['features']=='V141']","35e929d3":"plot_feature(train, test, 'V141', True)","efb23123":"drop_features.append('V141')","a4c2e693":"missing[missing['features']=='V157']","f6f0df05":"plot_feature(train, test, 'V157', True)","022b98f9":"check_features.append('V157')","97650728":"missing[missing['features']=='V162']","f3fb4a15":"plot_feature(train, test, 'V162', True)","a482de83":"missing[missing['features']=='V161']","0a8d7086":"plot_feature(train, test, 'V161', True)","4b0b6bf6":"missing[missing['features']=='V158']","b386c2c7":"plot_feature(train, test, 'V158', True)","25de0e73":"check_features.append('V158')","b4a0865d":"missing[missing['features']=='V156']","75c6ee57":"plot_feature(train, test, 'V156', True)","f3816d3b":"check_features.append('V156')","92b92491":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,5,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,5,i)\n        sns.kdeplot(np.log(df1[feature]), bw=0.5,label=label1)\n        sns.kdeplot(np.log(df2[feature]), bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","98ef6f63":"V = ['V142', 'V155', 'V154', 'V140', 'V149', 'V148', 'V147', 'V146', 'V153', 'V163', 'V139', 'V138', 'V151', 'V152', \n     'V145','V144', 'V143', 'V160', 'V159', 'V164', 'V165', 'V166', 'V150', 'V337', 'V333', 'V336', 'V335', 'V334', 'V338', \n     'V339', 'V325', 'V332', 'V324', 'V330', 'V329', 'V328', 'V327', 'V326', 'V322', 'V323', 'V331', 'V278', 'V277', 'V252', \n     'V253', 'V254', 'V257', 'V258', 'V260', 'V243', 'V262', 'V263', 'V264', 'V249', 'V266', 'V267', 'V268', 'V269', 'V273', \n     'V274', 'V275', 'V276', 'V265', 'V261', 'V247', 'V246', 'V241', 'V240', 'V237', 'V236', 'V235', 'V233', 'V232', \n     'V231', 'V230', 'V229', 'V228', 'V226', 'V225', 'V224', 'V223', 'V219', 'V218', 'V217', 'V244', 'V248', 'V242', 'V211', \n     'V214', 'V213', 'V212', 'V196', 'V205', 'V183', 'V216', 'V206', 'V186', 'V187', 'V192', 'V207', 'V215', 'V182', 'V191',\n     'V181', 'V167', 'V168', 'V199', 'V193', 'V172', 'V173', 'V202', 'V203', 'V176', 'V177', 'V178', 'V179', 'V204', 'V190',\n     'V194', 'V201', 'V189', 'V188', 'V185', 'V184', 'V180', 'V175', 'V174', 'V171', 'V170', 'V169', 'V195', 'V200', 'V197', \n     'V198', 'V208', 'V210', 'V209', 'V272', 'V234', 'V222', 'V238', 'V239', 'V227', 'V251', 'V250', 'V271', 'V270', 'V221',\n     'V220', 'V255', 'V256', 'V259', 'V245', 'V3', 'V1', 'V2', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V46', 'V42',\n     'V44', 'V43', 'V47', 'V41', 'V40', 'V39', 'V38', 'V37', 'V36', 'V35', 'V52', 'V51', 'V50', 'V49', 'V48', 'V45', 'V88', \n     'V93', 'V85', 'V87', 'V84', 'V83', 'V82', 'V81', 'V90', 'V91', 'V92', 'V94', 'V80', 'V79', 'V78', 'V77', 'V76',\n     'V75', 'V86', 'V53', 'V74', 'V73', 'V72', 'V66', 'V54', 'V67', 'V64', 'V63', 'V62', 'V61', 'V71', 'V69', 'V55',\n     'V60', 'V59', 'V58', 'V57', 'V65', 'V56', 'V70', 'V22', 'V23', 'V24', 'V34', 'V33', 'V32', 'V31', 'V30', 'V29',\n     'V26', 'V25', 'V15', 'V21', 'V14', 'V16', 'V17', 'V18', 'V19', 'V12', 'V20', 'V13', 'V282', 'V301', 'V300', 'V296',\n     'V289', 'V288', 'V283', 'V281', 'V315', 'V314', 'V313', 'V114', 'V110', 'V105', 'V104', 'V103', 'V102', 'V101', 'V100',\n     'V95', 'V99', 'V98', 'V97', 'V107', 'V111', 'V96', 'V112', 'V106', 'V113', 'V137', 'V136', 'V108', 'V135', 'V134', 'V133', \n     'V132', 'V131', 'V130', 'V129', 'V128', 'V127', 'V126', 'V125', 'V124', 'V123', 'V122', 'V121', 'V120', 'V119', 'V118',\n     'V117', 'V116', 'V115', 'V109', 'V312', 'V321', 'V294', 'V306', 'V305', 'V304', 'V303', 'V302', 'V299', 'V298', 'V297', \n     'V295', 'V293', 'V308', 'V292', 'V291', 'V290', 'V287', 'V286', 'V285', 'V284', 'V280', 'V279', 'V320', 'V307', 'V309',\n     'V316', 'V310', 'V318', 'V317', 'V319', 'V311']","21c95d6f":"t0 = train[train['isFraud']==0]\nt1 = train[train['isFraud']==1]\nfirst = V[0:50]\nplot_feature_distribution(t0, t1, '0', '1', first)","687c698c":"V1drop = ['V142', 'V146', 'V138', 'V151', 'V152', 'V333', 'V338', 'V339', 'V325', 'V332', 'V324', 'V330', 'V329', 'V322', \n          'V323', 'V278', 'V277', 'V252', 'V253', 'V254', 'V260']\n\nfor i in V1drop:\n    drop_features.append(i)","8a24757a":"second = V[50:100]\nplot_feature_distribution(t0, t1, '0', '1', second)","b24732ef":"V2drop = ['V263', 'V249', 'V266', 'V267', 'V268', 'V273', 'V276', 'V275', 'V247', 'V241', 'V240', 'V237', 'V235', 'V225', \n          'V224', 'V224', 'V248', 'V211', 'V213', 'V196', 'V205', 'V183', 'V206', 'V192']\nfor i in V2drop:\n    drop_features.append(i)","43db2b14":"third = V[100:150]\nplot_feature_distribution(t0, t1, '0', '1', third)","2eae9b65":"V3drop = ['V191', 'V181', 'V193', 'V172', 'V173', 'V202', 'V203', 'V177', 'V179', 'V194', 'V185', 'V184', 'V175', 'V174', \n          'V195', 'V197', 'V198', 'V208', 'V210', 'V227', 'V251', 'V250', 'V271', 'V270', 'V225']\nfor i in V3drop:\n    drop_features.append(i)","b7fa52e8":"forth = V[150:200]\nplot_feature_distribution(t0, t1, '0', '1', forth)","69b62f09":"V4drop = ['V89', 'V256', 'V3', 'V1', 'V2', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V46', 'V42', 'V43', 'V47', \n         'V41', 'V39', 'V36', 'V35', 'V51', 'V50', 'V49', 'V48', 'V88', 'V93', 'V85', 'V84', 'V83', 'V81', 'V90', 'V91', \n         'V92', 'V94', 'V80', 'V79', 'V75', 'V75']\nfor i in V4drop:\n    drop_features.append(i)","769adc08":"fifth = V[200:250]\nplot_feature_distribution(t0, t1, '0', '1', fifth)","f30d62ad":"V5drop = ['V68', 'V27', 'V28', 'V53', 'V74', 'V73', 'V72', 'V66', 'V54', 'V67', 'V64', 'V63', 'V62', 'V61', 'V71', 'V69', \n          'V55', 'V60', 'V59', 'V58', 'V57', 'V65', 'V56', 'V70', 'V22', 'V23', 'V24', 'V34', 'V33', 'V32', 'V31', 'V30', \n          'V29', 'V26', 'V25', 'V15', 'V21', 'V14', 'V16', 'V17', 'V18', 'V19', 'V12', 'V20', 'V13', 'V301', 'V300', 'V296', \n          'V289', 'V288']\nfor i in V5drop:\n    drop_features.append(i)","c209d433":"sixth = V[250:300]\nplot_feature_distribution(t0, t1, '0', '1', sixth)","2f7ab12f":"V6drop = ['V114', 'V110', 'V105', 'V104', 'V103', 'V102', 'V101', 'V100', 'V95', 'V99', 'V98', 'V107', 'V111', 'V112', 'V106',\n         'V113', 'V108', 'V134', 'V133', 'V135', 'V132', 'V131', 'V130', 'V129', 'V126', 'V125', 'V124', 'V123', 'V122', \n          'V121', 'V120', 'V119', 'V118', 'V117', 'V116', 'V115', 'V109', 'V294']\nfor i in V6drop:\n    drop_features.append(i)","a6e98a23":"seventh = V[300:350]\nplot_feature_distribution(t0, t1, '0', '1', seventh)","b4fa127e":"V7drop = ['V305', 'V304', 'V303', 'V302', 'V299', 'V298', 'V297', 'V295', 'V293', 'V292', 'V291', 'V290', 'V287', 'V286',\n         'V285', 'V289', 'V279', 'V309', 'V316', 'V318', 'V319']\nfor i in V7drop:\n    drop_features.append(i)","713189a3":"D_done = ['D7', 'D13', 'D14', 'D12', 'D6', 'D9', 'D8']\nD_not_done = missing['features'].apply(lambda x: x if x[0]=='D' else 0)\nD_not_done = pd.DataFrame(D_not_done)\nD_not_done = D_not_done[D_not_done['features']!=0]\nD_not_done = D_not_done[~D_not_done['features'].isin(D_done)]\nD_not_done = D_not_done[~D_not_done['features'].isin(['DeviceInfo', 'DeviceType'])]\nD = list(D_not_done['features'])\nD","ac380b14":"plot_feature(train, test, 'D5', True)","d2450a7e":"plot_feature(train, test, 'D2', True)","226e5b1a":"plot_feature(train, test, 'D11', True)","9436c4fa":"plot_feature(train, test, 'D3', True)","57b28a91":"plot_feature(train, test, 'D4', True)","28606a3a":"plot_feature(train, test, 'D15', True)","6c98f3a9":"plot_feature(train, test, 'D10', True)","55fa7d25":"plot_feature(train, test, 'D1', True)","7993f6bb":"C_not_done = missing['features'].apply(lambda x: x if x[0]=='C' else 0)\nC_not_done = pd.DataFrame(C_not_done)\nC_not_done = C_not_done[C_not_done['features']!=0]\nC_not_done = list(C_not_done['features'])\nC_not_done","a874496a":"plot_feature(train, test, 'C1', True)","f02d7c83":"plot_feature(train, test, 'C2', True)","1567d6c7":"plot_feature(train, test, 'C3', False)","038f6158":"drop_features.append('C3')","08c69b9a":"plot_feature(train, test, 'C4', True)","0bd362a7":"plot_feature(train, test, 'C5', True)","4e229f45":"plot_feature(train, test, 'C6', True)","4526c068":"plot_feature(train, test, 'C7', True)","afa51854":"plot_feature(train, test, 'C8', True)","a2a4b2c9":"plot_feature(train, test, 'C9', True)","75ff6177":"plot_feature(train, test, 'C10', True)","5dc12b85":"plot_feature(train, test, 'C11', True)","b7819f82":"plot_feature(train, test, 'C12', True)","3b239f3e":"plot_feature(train, test, 'C13', True)","f21a5c71":"plot_feature(train, test, 'C14', True)","2a0a9aed":"id_done = ['id_24', 'id_25', 'id_08', 'id_07', 'id_21', 'id_26', 'id_27', 'id_23', 'id_22', 'id_18', 'id_04', 'id_03', \n           'id_33', 'id_09', 'id_30', 'id_32', 'id_32', 'id_34', 'id_14']\nid_not_done = missing['features'].apply(lambda x: x if x[0]=='i' else 0)\nid_not_done = pd.DataFrame(id_not_done)\nid_not_done = id_not_done[id_not_done['features']!=0]\nid_not_done = id_not_done[~id_not_done['features'].isin(id_done)]\nid_not_done","e5f04a9b":"plot_c_feature(train, 'id_10')","45f96a06":"check_features.append('id_10')","aab7f3e5":"plot_c_feature(train, 'id_13')","c4a7042a":"plot_c_feature(train, 'id_16')","d31d463e":"check_features.append('id_16')","c60af772":"plot_c_feature(train, 'id_05')","40622ebc":"plot_c_feature(train, 'id_06')","db94546a":"plot_c_feature(train, 'id_20')","35c37547":"plot_c_feature(train, 'id_19')","c48c67d1":"plot_c_feature(train, 'id_17')","a7bd7f37":"plot_c_feature(train, 'id_31')","7972eefc":"plot_feature(train, test, 'id_02', False)","e5d8bc1c":"plot_c_feature(train, 'id_11')","9c2adaf2":"plot_c_feature(train, 'id_28')","92bac343":"check_features.append('id_28')","110bc15b":"plot_c_feature(train, 'id_29')","840842d8":"check_features.append('id_29')","5a7287df":"plot_c_feature(train, 'id_38')","99a4d7d4":"check_features.append('id_38')","2e21433d":"plot_c_feature(train, 'id_37')","c96f6611":"check_features.append('id_37')","25f82b6a":"plot_c_feature(train, 'id_36')","d11d5b71":"check_features.append('id_36')","60d8d283":"plot_c_feature(train, 'id_35')","f44d080a":"check_features.append('id_35')","91c636ee":"plot_c_feature(train, 'id_15')","bbd05f30":"plot_feature(train, test, 'id_01', False)","c163a272":"plot_c_feature(train, 'id_12')","876022fe":"c_not_done = missing['features'].apply(lambda x: x if x[0]=='c' else 0)\nc_not_done = pd.DataFrame(c_not_done)\nc_not_done = c_not_done[c_not_done['features']!=0]\nlist(c_not_done['features'])","77183c0a":"plot_feature(train, test, 'card1', True)","9a259e06":"plot_feature(train, test, 'card2', False)","e20c38e7":"plot_feature(train, test, 'card3', True)","f0011e1c":"plot_c_feature(train, 'card4')","94caa3e8":"check_features.append('card4')","405dcdd0":"plot_feature(train, test, 'card5', True)","df6865ce":"plot_c_feature(train, 'card6')","e60691fe":"check_features.append('card6')","4deb71ad":"M_not_done = missing['features'].apply(lambda x: x if x[0]=='M' else 0)\nM_not_done = pd.DataFrame(M_not_done)\nM_not_done = M_not_done[M_not_done['features']!=0]\nlist(M_not_done['features'])","83f9bcc0":"plot_c_feature(train, 'M1')","dda5379c":"drop_features.append('M1')","42af4d6e":"plot_c_feature(train, 'M2')","7aa7b08c":"check_features.append('M2')","71109ff4":"plot_c_feature(train, 'M3')","e41c2153":"check_features.append('M3')","42de5090":"plot_c_feature(train, 'M4')","1d439130":"plot_c_feature(train, 'M5')","aef582c8":"check_features.append('M5')","f2b46cbc":"plot_c_feature(train, 'M6')","b1dd381a":"check_features.append('M6')","22f2320a":"train_copy = train.copy()\ntest_copy = test.copy()","1f14b4ce":"def id_split(dataframe):\n    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('\/', expand=True)[0]\n    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('\/', expand=True)[1]\n\n    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n\n    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n\n    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n\n    dataframe['id_34'] = dataframe['id_34'].str.split(':', expand=True)[1]\n    dataframe['id_23'] = dataframe['id_23'].str.split(':', expand=True)[1]\n\n    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n    \n    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = 'Others'\n    gc.collect()\n    return dataframe","d741a12b":"# split some features and replace values\ntrain = id_split(train)\ntest = id_split(test)","8c355a64":"# filter usefull features with the e.d.a\nusefull_features = [col for col in train.columns if col not in drop_features]\ntrain = train[usefull_features]\nusefull_features.remove('isFraud')\ntest = test[usefull_features]","0b9b21bb":"# New feature - log of transaction amount. ()\ntrain['TransactionAmt_Log'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt_Log'] = np.log(test['TransactionAmt'])\n\n# New feature - decimal part of the transaction amount.\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# Some arbitrary features interaction\nfor feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))","9f8df765":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', \n          'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', \n          'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', \n          'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', \n          'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', \n          'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', \n          'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', \n          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', \n          'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', \n          'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', \n          'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', \n          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', \n          'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', \n          'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', \n          'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', \n          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', \n          'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', \n          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']","fa9dccde":"for c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","6cae781b":"START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\ndef setTime(df):\n    df['TransactionDT'] = df['TransactionDT'].fillna(df['TransactionDT'].median())\n    # Temporary\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n    \n    df['DT_hour'] = df['DT'].dt.hour\n    df['DT_day_week'] = df['DT'].dt.dayofweek\n    df['DT_day'] = df['DT'].dt.day\n    \n    # Lets transform D8 and D9 column\n    # As we almost sure it has connection with hours\n    df['D9_not_na'] = np.where(df['D9'].isna(),0,1)\n    df['D8_not_same_day'] = np.where(df['D8']>=1,1,0)\n    df['D8_D9_decimal_dist'] = df['D8'].fillna(0)-df['D8'].fillna(0).astype(int)\n    df['D8_D9_decimal_dist'] = ((df['D8_D9_decimal_dist']-df['D9'])**2)**0.5\n    df['D8'] = df['D8'].fillna(-1).astype(int)\n\n    return df\n    \ntrain=setTime(train)\ntest=setTime(test)","9ffa691e":"def addNewFeatures(data): \n    data['uid'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n\n    data['uid2'] = data['uid'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n\n    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n    \n    return data","357c2fb9":"train = addNewFeatures(train)\ntest = addNewFeatures(test)","460e6c78":"i_cols = ['card2','card3','card5','uid','uid2','uid3']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n\n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n\n        train[new_col_name] = train[col].map(temp_df)\n        test[new_col_name]  = test[col].map(temp_df)","a7bfe2ae":"train = train.replace(np.inf,999)\ntest = test.replace(np.inf,999)","775ec1bf":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo','device_name',\n          'id_30','id_33',\n          'uid','uid2','uid3',\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n    train[col+'_fq_enc'] = train[col].map(fq_encode)\n    test[col+'_fq_enc']  = test[col].map(fq_encode)\n\n\nfor col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train[col+'_total'] = train[col].map(fq_encode)\n    test[col+'_total']  = test[col].map(fq_encode)\n\nperiods = ['DT_M','DT_W','DT_D']\ni_cols = ['uid']\nfor period in periods:\n    for col in i_cols:\n        new_column = col + '_' + period\n            \n        temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n        fq_encode = temp_df[new_column].value_counts().to_dict()\n            \n        train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n        test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n        \n        train[new_column] \/= train[period+'_total']\n        test[new_column]  \/= test[period+'_total']","da209b68":"# drop noisy columns    \ntrain.drop(['TransactionDT', 'uid','uid2','uid3', 'DT','DT_M','DT_W','DT_D', 'DT_hour','DT_day_week','DT_day',\n            'DT_D_total','DT_W_total','DT_M_total', 'id_30','id_31','id_33', 'D1', 'D2', 'D9'], axis = 1, inplace = True)\ntest.drop(['TransactionDT', 'uid','uid2','uid3', 'DT','DT_M','DT_W','DT_D', 'DT_hour','DT_day_week','DT_day',\n            'DT_D_total','DT_W_total','DT_M_total', 'id_30','id_31','id_33', 'D1', 'D2', 'D9'], axis = 1, inplace = True)","b65d3b30":"for col in train.columns:\n    if train[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","33906433":"def agg_features(df):\n    columns_a = ['TransactionAmt', 'id_02', 'D15']\n    columns_b = ['card1', 'card4', 'addr1']\n    for col_a in columns_a:\n        for col_b in columns_b:\n            df[f'{col_a}_to_mean_{col_b}'] = df[col_a] \/ df.groupby([col_b])[col_a].transform('mean')\n            df[f'{col_a}_to_std_{col_b}'] = df[col_a] \/ df.groupby([col_b])[col_a].transform('std')\n    return df\n\ntest = agg_features(test)\ntrain = agg_features(train)","6ba37bf3":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","784fa807":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","e9cb479e":"X = train.drop(['isFraud'], axis = 1)\ny = train['isFraud']\n\nprint('Our train set have {} columns'.format(train.shape[1]))\nprint('Our test set have {} columns'.format(test.shape[1]))\n\ngc.collect()","a9192b45":"params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.005,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':100000,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'random_state': 47,\n                    'early_stopping_rounds':100, \n                }","845ae064":"NFOLDS = 10\nfolds = KFold(n_splits=NFOLDS)\n\n\nsplits = folds.split(X, y)\ny_preds = np.zeros(test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label = y_train)\n    dvalid = lgb.Dataset(X_valid, label = y_valid)\n    \n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], \n                    verbose_eval = 200, early_stopping_rounds = 500)\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) \/ NFOLDS\n    y_preds += clf.predict(test) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(\"Mean AUC: \", score)\nprint(\"Out of folds AUC: \", roc_auc_score(y, y_oof))","2db4e219":"sub['isFraud'] = y_preds\nsub.to_csv('submission_v1.csv', index = False)","c6c97fd9":"# id_18","a993fd7a":"Helpfull","e4f6436f":"# V156","45115cfa":"# M","f1561e07":"* Distribution are different, maybee this feature can be usefull so it's not a good idea to drop it\n* The distribution between train and test are little different.\n* Let's store it in a list of features that we need to experiment with","88a08e07":"Some value have a lot of isFraud, maybee this feature is helpfull.","718f6435":"I believe all the features starting with D are usefull, the only one that i am not sure is D7. ","980d7b34":"# Model and Feature Engineering","830f1f5b":"# D9","edd01f4d":"# C","ce7ff446":"Getting tired :), let's make some multiplots. We already analyze the features that have a lot of NaN values so we are good to go","cd2b3245":"Same as D7, more missing values in the train train set\n\nThis is a time series problem so maybee with time, this feature have less NaN's","84b7c751":"# Others V","166ed67a":"Have a lot of missing values and the distribution are not that different, let's drop it","ca8d9c38":"# id_33","5413b500":"The same for 'id_24', 'id_25', 'id_08', 'id_07', 'id_21', 'id_26', 'id_27', 'id_23', 'id_22'","43f93453":"# D13","36864226":"# id_09","bfcdf4a1":"This feature have more missing values in the train set. Very difficult to impute it.","6e251ebd":"# Dist 2","e6f42873":"# V158","ff841044":"# id_24 and others","19fc9d28":"Let's analyze them and complete the list","7ca5b172":"Usefull","5f1583f4":"Helpfull, a lot of categories have 100% prob to be Fraud","c6355c81":"# D14","a77c59b9":"Usefull","ff41a747":"The ideas and a lot of code was copied from other scripts. It's a mix!!","ee3f01de":"# id_03","578de879":"Different distributions, helpfull","39869aff":"# id_04","2254405d":"# id_30, id_32, id_34, id_14","f9477d57":"Same as id_04","6ace03e8":"# Other id","9cedb30f":"# V161","405351a6":"# V157","50d21fcf":"* Good feature\n* The distribution of the training and the test set are different","0da350d7":"All the c features are usefull except C3. I am not totally sure because non linear models behave in misterious ways so we need to experiment","761703bd":"# D8","ac8536fd":"# D6","d82d8f7a":"So we have a lot of missing values in our train set. Let's check our test set and compare them.","0ae1bd41":"We have a very unbalance dataset.","c37e96e3":"We have a lot of features, let's start exploring our dataset.","62bfdc1a":"# c","48dd7548":"# V162","8af8cf0d":"# id_10","4e0a6c4b":"Almost all the values are NaN","f04d32f1":"# D12","e4b9c867":"# Exploratory Data Analysis","f0297913":"This feature can help us, let's store it in the check list","42447610":"# Read Data and Merge","af964553":"# Other D","61ca72e4":"# D7","016b8cfb":"# V141"}}