{"cell_type":{"4e150530":"code","4a87148d":"code","2df5fb59":"code","3f51061c":"code","badc0b15":"code","83f85daa":"code","2d0e5298":"code","8f5d2db6":"code","9a7446c6":"code","ac0e0765":"code","b4e94fd4":"code","cce647fe":"code","ac832719":"code","64240023":"markdown","b441b9ab":"markdown","a72cb1b9":"markdown","cc5f4fad":"markdown","ff6c4b22":"markdown","5c6fcc9c":"markdown","7dcfc734":"markdown","57067813":"markdown","cc1df8b3":"markdown"},"source":{"4e150530":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a87148d":"file='..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt'","2df5fb59":"#Install the library\n!pip install MiniClassifier","3f51061c":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport MiniClassifier.MiniClassifier as mc\n\ntrain_df=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntrain_df.columns","badc0b15":"train_df.head()","83f85daa":"##Using it in Kaggle \n## Without pretrained embeddings\n## Evaluating it on a small dataset just for checking\n##Load and tokenize the dataset.\n\nif __name__=='__main__':\n    df=train_df\n    df=df[:1000]\n    df.head()\n    X=(df['text'])\n    li=df['target']\n    #Extract the targets\n    Y=(li)\n    print(Y.shape)\n    Y_unique=list(set(li))\n    \n    '''Hyperparameter (maxwords, maxlen) for Maximum words in Embedding and Maximum length for the sentence after which\n    padding is done'''\n    \n    maxwords=1000\n    maxlen=500\n    '''Creating the tokenizer object and creating padded tokens from the text corpus (X).\n    This calls the SimpleTokenizer Class.'''\n    token=mc.Simple_tokenizer(maxwords,maxlen,X)\n    tokeni=token.create_tokenizer()\n    encoded_token=token.encode_tokenizer(X)\n    pad_token,vocab_size=token.padded_tokenizer()\n    print(\"padded token\".format(),pad_token)\n    print(\"vocab_size\".format(),vocab_size)\n    print('Padded Token Shape'.format(),pad_token.shape)\n    #print(Y_unique)\n    '''Label Encode the Y or target labels'''\n    labels,encoder=token.labelencode_labels(Y)\n    print('Labels shape'.format(),labels.shape)\n    '''Split the dataset into test and train using sklearn'''\n    X_train,X_test,Y_train,Y_test= train_test_split(pad_token,labels,test_size=0.2)\n    print('X_train shape'.format(),X_train.shape)\n    print('Y_train shape'.format(),Y_train.shape)\n    tokenizer=tokeni\n    \n\n    \n    ","2d0e5298":"##Specify the hyperparameters\n'''Set the hyperparameters for the models.Names are self-explanatory'''\nprint(\"==============================\")\noutput_samples=labels.shape[-1]\nembedding_dim=512\ndense_units=64\nlstm_units=64\nbilstm_units=64\nactivation='relu'\nfinal_activation='sigmoid'\noptimizer='adam'\ntraining_epochs=5\ntraining_batch_size=150\nval_epochs=5\nval_batch_size=150\nfilter_size=128\nkernel_size=5\n#Specify path to the Pretrained Embedding file\npath='..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt'\n#If no pretrained embedding is required.For this the second last arguement in parameters method should be False\npath1=''\n","8f5d2db6":"## Using Without Glove pretrained embedding\n## Bilstm Model\n\n'''Demonstration for the Bilstm model.The parameters are self explanatory and used from the hyperameters set.'''\nprint(\"Bilstm model for Evaluation\")\nbilstm=mc.BiLSTM_Cell()\n#if no pretrained embedding\nbilstm.parameters(activation,final_activation,embedding_dim,dense_units,bilstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,False,path1)\n#Use pretrained glove embdding\n#bilstm.parameters(activation,final_activation,embedding_dim,dense_units,bilstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,True,path)\nbilstm.build_bilstm_neuron(maxwords,maxlen)\nprint('bilstm X_train shape'.format(),bilstm.X_train.shape)\nprint('bilstm Y_train shape'.format(),bilstm.Y_train.shape)\nprint(\"Evaluating the Model-Training\")\nbilstm.fit_model(training_epochs,training_batch_size)\nprint(\"Evaluating the Model-Validation\")\nbilstm.evaluate(val_epochs,val_batch_size)\nprint(\"Prediction of Labels\")\n#Predictor class for predicting \npredictor=mc.Predictor()\npred_list=predictor.predict(bilstm.model,bilstm.X_test,X,Y,encoder)\nprint(\"=====================================================\")\n   \n\n","9a7446c6":"##Dense Network without pretrained embedding\n    \n    \n'''Demonstration for the Dense model.The parameters are self explanatory and used from the hyperameters set.'''\nprint(\"Dense model for Evaluation\")\ndense=mc.Dense_Cell()\ndense.parameters(activation,final_activation,embedding_dim,dense_units,lstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,False,path1)   \n#dense.parameters(activation,final_activation,embedding_dim,dense_units,lstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,True,path)\ndense.build_dense_neuron(maxwords,maxlen)\nprint('Dense X_train shape'.format(),dense.X_train.shape)\nprint('Dense Y_train shape'.format(),dense.Y_train.shape)\nprint(\"Evaluating the Model-Training\")\ndense.fit_model(training_epochs,training_batch_size)\nprint(\"Evaluating the Model-Validation\")\ndense.evaluate(val_epochs,val_batch_size)\n#Predictor class for predicting\npredictor=mc.Predictor()\npred_list=predictor.predict(dense.model,dense.X_test,X,Y,encoder)\nprint(\"=====================================================\")\n","ac0e0765":"##Convolution Network without pretrained embedding\n\n'''Demonstration for the Convolution model.The parameters are self explanatory and used from the hyperameters set.'''\nconv=mc.Convolution_Cell()\nconv.parameters(activation,final_activation,embedding_dim,filter_size,kernel_size,dense_units,lstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,False,path1)\n#conv.parameters(activation,final_activation,embedding_dim,filter_size,kernel_size,dense_units,lstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,True,path)\nconv.build_conv1d_neuron(maxwords,maxlen)\nprint('Convolution X_train shape'.format(),conv.X_train.shape)\nprint('Convolution Y_train shape'.format(),conv.Y_train.shape)\nprint(\"Evaluating the Model-Training\")\nconv.fit_model(training_epochs,training_batch_size)\nprint(\"Evualating the Model-Validation\")\nconv.evaluate(val_epochs,val_batch_size)\n#Predictor class for predicting\npredictor=mc.Predictor()\npred_list=predictor.predict(conv.model,conv.X_test,X,Y,encoder)\nprint(\"=====================================================\")\n","b4e94fd4":"##Now we will be using pretrained Glove embeddings for our models and re-run the evaluation\n#bilstm cell\n## Using Without Glove pretrained embedding\n## Bilstm Model\n\n'''Demonstration for the Bilstm model.The parameters are self explanatory and used from the hyperameters set.'''\nprint(\"Bilstm model for Evaluation\")\nbilstm=mc.BiLSTM_Cell()\n#if no pretrained embedding\n#bilstm.parameters(activation,final_activation,embedding_dim,dense_units,bilstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,False,path1)\n#Use pretrained glove embdding\nbilstm.parameters(activation,final_activation,embedding_dim,dense_units,bilstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,True,path)\nbilstm.build_bilstm_neuron(maxwords,maxlen)\nprint('bilstm X_train shape'.format(),bilstm.X_train.shape)\nprint('bilstm Y_train shape'.format(),bilstm.Y_train.shape)\nprint(\"Evaluating the Model-Training\")\nbilstm.fit_model(training_epochs,training_batch_size)\nprint(\"Evaluating the Model-Validation\")\nbilstm.evaluate(val_epochs,val_batch_size)\nprint(\"Prediction of Labels\")\n#Predictor class for predicting \npredictor=mc.Predictor()\npred_list=predictor.predict(bilstm.model,bilstm.X_test,X,Y,encoder)\nprint(\"=====================================================\")\n   \n\n\n","cce647fe":"##Dense Network with pretrained embedding\n    \n    \n'''Demonstration for the Dense model.The parameters are self explanatory and used from the hyperameters set.'''\nprint(\"Dense model for Evaluation\")\ndense=mc.Dense_Cell()\n#dense.parameters(activation,final_activation,embedding_dim,dense_units,lstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,False,path1)   \ndense.parameters(activation,final_activation,embedding_dim,dense_units,lstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,True,path)\ndense.build_dense_neuron(maxwords,maxlen)\nprint('Dense X_train shape'.format(),dense.X_train.shape)\nprint('Dense Y_train shape'.format(),dense.Y_train.shape)\nprint(\"Evaluating the Model-Training\")\ndense.fit_model(training_epochs,training_batch_size)\nprint(\"Evaluating the Model-Validation\")\ndense.evaluate(val_epochs,val_batch_size)\n#Predictor class for predicting\npredictor=mc.Predictor()\npred_list=predictor.predict(dense.model,dense.X_test,X,Y,encoder)\nprint(\"=====================================================\")\n","ac832719":"##Convolution Network with pretrained embedding\n\n'''Demonstration for the Convolution model.The parameters are self explanatory and used from the hyperameters set.'''\nconv=mc.Convolution_Cell()\n#conv.parameters(activation,final_activation,embedding_dim,filter_size,kernel_size,dense_units,lstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,False,path1)\nconv.parameters(activation,final_activation,embedding_dim,filter_size,kernel_size,dense_units,lstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,True,path)\nconv.build_conv1d_neuron(maxwords,maxlen)\nprint('Convolution X_train shape'.format(),conv.X_train.shape)\nprint('Convolution Y_train shape'.format(),conv.Y_train.shape)\nprint(\"Evaluating the Model-Training\")\nconv.fit_model(training_epochs,training_batch_size)\nprint(\"Evualating the Model-Validation\")\nconv.evaluate(val_epochs,val_batch_size)\n#Predictor class for predicting\npredictor=mc.Predictor()\npred_list=predictor.predict(conv.model,conv.X_test,X,Y,encoder)\nprint(\"=====================================================\")\n","64240023":"## Using MiniClassifier Deep Learning Library\n\n\nThe example [script](https:\/\/github.com\/abhilash1910\/MiniClassifier\/blob\/master\/MiniClassifier-TwitterBinaryClassification.ipynb) provides an overview of it. For using it in any classification tasks, the following changes are needed to be made:\n\n1. Load the valid dataset: The dataset should be loaded with pandas. Segregate the X input features,i.e. the Text and the Y labels\/targets i.e the Target.For different datasets, the path will have to be changed,along with the column headings (in this case- text and target).This is shown in the starting lines\n\n\n```python\n    df=pd.read_csv(\"D:\\\\Twitter\\\\Twitter.csv\")\n    df=df[:1000]\n    df.head()\n    X=(df['text'])\n    li=df['target']\n    #Extract the targets\n    Y=(li)\n    print(Y.shape)\n    Y_unique=list(set(li))\n```\n\n2. Specify the hyperparameters: The hyperparameters can be tweaked as per requirements as provided below.Recommended to use sigmoid for binary classification and softmax for multiclass\/categorical classification, in the final_activation variable.Attention must be provided to the path and path1 variables. While using pretrained embeddings, and running locally, we have to specify the relative path to the embedding file. Else no pretrained embedding is required. \n\n\n```python\n    '''Set the hyperparameters for the models.Names are self-explanatory'''\n    print(\"==============================\")\n    output_samples=labels.shape[-1]\n    embedding_dim=512\n    dense_units=64\n    lstm_units=64\n    bilstm_units=64\n    activation='relu'\n    final_activation='sigmoid'\n    optimizer='adam'\n    training_epochs=5\n    training_batch_size=150\n    val_epochs=5\n    val_batch_size=150\n    filter_size=128\n    kernel_size=5\n    #Specify path to the Pretrained Embedding file\n    path='D:\\\\glove.6B.200d\\\\glove.6B.200d.txt'\n    #If no pretrained embedding is required.For this the second last arguement in parameters method should be False\n    path1=''\n```\n\n3. Choose a model: From the 3 different models provided, Bilstm\/Dense\/Con1D, we can choose any one. The general workflow should remain the same.\n\nBilstm Model:-\n\n```python\n'''Demonstration for the Bilstm model.The parameters are self explanatory and used from the hyperameters set.'''\n    print(\"Bilstm model for Evaluation\")\n    bilstm=mc.BiLSTM_Cell()\n    #if no pretrained embedding\n                      #bilstm.parameters(activation,final_activation,embedding_dim,dense_units,bilstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,False,path1)\n    #Use pretrained glove embdding\n    bilstm.parameters(activation,final_activation,embedding_dim,dense_units,bilstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,True,path)\n    bilstm.build_bilstm_neuron(maxwords,maxlen)\n    print('bilstm X_train shape'.format(),bilstm.X_train.shape)\n    print('bilstm Y_train shape'.format(),bilstm.Y_train.shape)\n    print(\"Evaluating the Model-Training\")\n    bilstm.fit_model(training_epochs,training_batch_size)\n    print(\"Evaluating the Model-Validation\")\n    bilstm.evaluate(val_epochs,val_batch_size)\n    print(\"Prediction of Labels\")\n    #Predictor class for predicting \n    predictor=mc.Predictor()\n    pred_list=predictor.predict(bilstm.model,bilstm.X_test,X,Y,encoder)\n    print(\"=====================================================\")\n    \n    \n```\n\nLSTM + Dense Simple Model:-\n\n```python\n    '''Demonstration for the Dense model.The parameters are self explanatory and used from the hyperameters set.'''\n    print(\"Dense model for Evaluation\")\n    dense=mc.Dense_Cell()\n    dense.parameters(activation,final_activation,embedding_dim,dense_units,lstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,True,path)\n    dense.build_dense_neuron(maxwords,maxlen)\n    print('Dense X_train shape'.format(),dense.X_train.shape)\n    print('Dense Y_train shape'.format(),dense.Y_train.shape)\n    print(\"Evaluating the Model-Training\")\n    dense.fit_model(training_epochs,training_batch_size)\n    print(\"Evaluating the Model-Validation\")\n    dense.evaluate(val_epochs,val_batch_size)\n    #Predictor class for predicting\n    predictor=mc.Predictor()\n    pred_list=predictor.predict(dense.model,dense.X_test,X,Y,encoder)\n    print(\"=====================================================\")\n```\n\nConvolution 1D model:-\n\n```python\n    '''Demonstration for the Convolution model.The parameters are self explanatory and used from the hyperameters set.'''\n    conv=mc.Convolution_Cell()\n    conv.parameters(activation,final_activation,embedding_dim,filter_size,kernel_size,dense_units,lstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,False,path)\n    conv.build_conv1d_neuron(maxwords,maxlen)\n    print('Convolution X_train shape'.format(),conv.X_train.shape)\n    print('Convolution Y_train shape'.format(),conv.Y_train.shape)\n    print(\"Evaluating the Model-Training\")\n    conv.fit_model(training_epochs,training_batch_size)\n    print(\"Evualating the Model-Validation\")\n    conv.evaluate(val_epochs,val_batch_size)\n    #Predictor class for predicting\n    predictor=mc.Predictor()\n    pred_list=predictor.predict(conv.model,conv.X_test,X,Y,encoder)\n    print(\"=====================================================\")\n```\n\nIntermediate print statements can be removed as per requirements.\n\n4. Pre-trained embeddings: Care should be taken in using pretrained embeddings, the parameters method should contain True and the path to the embedding file as the last 2 arguements. If pretrained embeddings are not requried, then arguements can be False, and any path. This is mentioned above.\n\n","b441b9ab":"## Predictor Class\n\nThe Predictor class shows the predictions of the model on the testing dataset. This can be commented as per requirements. The example shows this:\n\n```python\n    predictor=mc.Predictor()\n    pred_list=predictor.predict(conv.model,conv.X_test,X,Y,encoder)\n    print(\"=====================================================\")\n```\n\n\n","a72cb1b9":"## Better Performance\n\nFor better performance, it is recommended to switch to GPU based kernel in Kaggle for training the neural networks. This will speed up the neural computation.\n\nThis was a small demonstration of the capabilities of the framework. Experiments can be made with different embedding libraries.\nAlso this layout should be the same for any textual classification tasks (whether labels are numeric or textual). For textual labels having spaces , the spaces will be removed before label encoding them. The sample [script](https:\/\/github.com\/abhilash1910\/MiniClassifier\/blob\/master\/CategoricalClassificationTest.py) on BBC-News classification is provided for multiclass classification on textual labels.\n\nFor now Glove support is provided, newer embedding file support will be provided soon.","cc5f4fad":"## Using With Pretrained Embeddings\n\n\nThe librrary allows the flexibility of using pre-trained embeddings. In the [example](https:\/\/github.com\/abhilash1910\/MiniClassifier\/blob\/master\/MiniClassifier-TwitterBinaryClassification.ipynb) shown here, inside the parameters method, the last 2 arguements specify whether to use pretrained embeddings or not. For using it, second last arguement should be kept True, and the path to the embedding file should be provided in the last arguement.\n\n```python\npath='D:\\\\glove.6B.200d\\\\glove.6B.200d.txt'\nbilstm.parameters(activation,final_activation,embedding_dim,dense_units,bilstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,True,path)\n```\n\n\nIf no prtrained embeddings are to be used, then the arguements can be False, and any path (can be blank as well). This allows the framework to use tensorflow.keras.Embedding Layer instead (no pretrained embeddings.)\n\n```python\npath1=\"\"\nbilstm.parameters(activation,final_activation,embedding_dim,dense_units,bilstm_units,output_samples,optimizer,X_train,Y_train,X_test,Y_test,tokenizer,False,path1)\n```\nA preview of the [Glove embeddings](https:\/\/nlp.stanford.edu\/projects\/glove\/) can be found here:\n\n\n<img src=\"https:\/\/nlp.stanford.edu\/projects\/glove\/images\/man_woman.jpg\"><\/img>\n","ff6c4b22":"# MiniClassifier Deep Learnining Library\n\nThis Notebook is a demonstration of MiniClassifier Library built on Tensorflow framework. More details on this library can be found [here](https:\/\/github.com\/abhilash1910\/MiniClassifier). Installation can be found in [Pypi](https:\/\/pypi.org\/project\/MiniClassifier\/)\n\nThe 3 different architectures involved includes:\n\n1. [Bidirectional LSTM model](https:\/\/paperswithcode.com\/method\/bilstm)\n\n<img src=\"https:\/\/ai2-s2-public.s3.amazonaws.com\/figures\/2017-08-08\/f7bdb849dafe17c952bfd88b879e01f74cf59d78\/4-Figure3-1.png\"><\/img>\n\n\n2. [Simple Dense + LSTM model](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Xuan_Hien_Le2\/publication\/334268507\/figure\/fig8\/AS:788364231987201@1564972088814\/The-structure-of-the-Long-Short-Term-Memory-LSTM-neural-network-Reproduced-from-Yan.png\"><\/img>\n\n3. [Convolution 1D model](https:\/\/machinelearningmastery.com\/cnn-models-for-human-activity-recognition-time-series-classification\/)\n\n<img src=\"https:\/\/www.researchgate.net\/publication\/334609713\/figure\/fig2\/AS:783455927406594@1563801857139\/a-Simple-scheme-of-a-one-dimension-1D-convolutional-operation-b-Full.jpg\"><\/img>\n\nAny of the three models can be chosen from the framework or can be used together for a comparative accuracy analysis.\n","5c6fcc9c":"## Library Future Roadmap\n\nPull requests for bugs and features and welcome. Future roadmap is to add more support for complex architectures and integration with Transformers, including increasing the embedding support.\n\n","7dcfc734":"## Loaded the Glove embedding from Kaggle\n\nThe path variable contains the loaded Glove embedding relative path.","57067813":"## Use the Framework as is\n\nThe example provided can be used as is, with only changes in the paths of the files (datasets, and embeddings -if any) and the X,Y values(texts and labels).Hyperparameters can be tuned as well.","cc1df8b3":"## Tutorial For Using MiniClassifier Deep Learning Library\n\nMiniClassifier is a deep learning classification library for textual\/semantic classification.It is built on top of tensorflow\/keras framework and contains 3 different architectures- Bilstm, Dense with LSTM,Convolution. \n\nThe main implication of this library is to provide a benchmark deep learning performance estimation.There are scopes for using pre-trained embeddings (like Glove) which aids in the training and evaluation phases.\n\nThe library can be found in [Pypi](https:\/\/pypi.org\/project\/MiniClassifier\/)\n\nThe source code and examples for this library can be found in the github account: [abhilash1910](https:\/\/github.com\/abhilash1910\/MiniClassifier)\n\n"}}