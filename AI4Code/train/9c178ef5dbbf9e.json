{"cell_type":{"5650c361":"code","2c86105e":"code","4bc1c0d5":"code","7697d85b":"code","42b87aa5":"code","d40a3600":"code","9702045a":"code","f847b095":"code","fd930130":"code","0d82ca20":"code","11a01854":"code","d8213667":"code","92f73d7c":"code","aba05758":"code","39be4f1a":"markdown","d75b2fa5":"markdown","ed2ed47d":"markdown","ac62938b":"markdown","1d571aa5":"markdown"},"source":{"5650c361":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfile_list=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n        file_list.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c86105e":"len(file_list)","4bc1c0d5":"path1='\/kaggle\/input\/structural-defects-network-concrete-crack-images\/'\n","7697d85b":"from  PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib import image\nimgs=[]\nfile_list_vgg=[]\n#im_resize=(32,32)  # for LeNet5\nim_resize=(224,224) # for VGG16\nfile_list_vgg[0:599]=file_list[0:599]\nfile_list_vgg[600:799]=file_list[11000:11199]\nfile_list_vgg[800:999]=file_list[14000:14199]\nfile_list_vgg[1000:1199]=file_list[37000:37199]\nfile_list_vgg[1200:1399]=file_list[42000:42199]\nfile_list_vgg[1400:1599]=file_list[55000:55199]\n\n#for fname in file_list:\nfor fname in file_list_vgg[0:1599]:\n    img= Image.open(fname)\n    img=img.resize(im_resize)\n    imgs.append(np.array(img)\/255)\n#   imgs.append(np.array(img))\n    del img\n    ","42b87aa5":"print(np.array(imgs).shape)","d40a3600":"print(file_list[0])","9702045a":"print(file_list[0].find(\"Non-Cracked\"))","f847b095":"class_list = []\n\n\n\n#for fname in file_list[0:1500]:                    # take  few files to avoid filling up of memory.\nfor fname in file_list_vgg[0:1599]:  \n    if (fname.find('Decks') != -1):\n        if (fname.find('Non-cracked') != -1):\n            class_list.append(0)\n        if (fname.find('Cracked') != -1):\n            class_list.append(1)\n    if (fname.find('Pavements') != -1):\n        if (fname.find('Non-cracked') != -1):\n            class_list.append(2)\n        if (fname.find('Cracked') != -1):\n            class_list.append(3)        \n    if (fname.find('Walls') != -1):\n        if (fname.find('Non-cracked') != -1):\n            class_list.append(4)\n        if (fname.find('Cracked') != -1):\n            class_list.append(5)","fd930130":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n#clistdf=pd.DataFrame(class_list)\n#imgsdf = imgsdf.T\n#clistdf=clistdf.T\n#print(imgsdf.shape)\n#print(clistdf.shape)\n\n\nimgs_train,imgs_test, tgt_train,tgt_test=train_test_split(imgs,class_list,test_size=0.1,random_state=42)","0d82ca20":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Dropout, Flatten, Dense\n\n#from tensorflow.keras.layers import BatchNormalization\n\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', input_shape=(32,32,3)))\nmodel.add(AveragePooling2D())\n\nmodel.add(Conv2D(filters=16, kernel_size=(5, 5), activation='tanh'))\nmodel.add(AveragePooling2D())\n\nmodel.add(Conv2D(filters=128, kernel_size=(5,5),activation='tanh'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units=84, activation='tanh'))\n\nmodel.add(Dense(units=6, activation = 'softmax'))\n\nmodel.summary()\n\n","11a01854":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Dropout, Flatten, Dense\n\nfrom tensorflow.keras.layers import BatchNormalization\n\n\nmodel = Sequential()\n#model.add(Conv2D(input_shape=(224,224,3),filters=1,kernel_size=(1,1),padding=\"valid\", activation=\"relu\"))\nmodel.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\nmodel.add(Dense(units=4096,activation=\"relu\"))\nmodel.add(Dense(units=4096,activation=\"relu\"))\nmodel.add(Dense(units=6, activation=\"softmax\"))\n\nmodel.summary()","d8213667":"model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n\nmodel.fit(np.array(imgs_train), np.array(tgt_train), epochs=50)\n\n#imgs_train,imgs_test, tgt_train,tgt_test","92f73d7c":"#from sklearn.metrics import accuracy_score\ntgt_pred = model.evaluate(np.array(imgs_test),np.array(tgt_test))\nprint(tgt_pred)\n#print(imgs_test.shape)\n#accuracy_score = accuracy_score(tgt_test,tgt_pred)\n#print('Test Accuracy:',accuracy_score)","aba05758":"import matplotlib.pyplot as plt\n\n#xpic= Image.open('..\/input\/structural-defects-network-concrete-crack-images\/Pavements\/Cracked\/002-102.jpg')\nxpic= Image.open('..\/input\/structural-defects-network-concrete-crack-images\/Walls\/Non-cracked\/7069-248.jpg')\n#xpic= Image.open('..\/input\/structural-defects-network-concrete-crack-images\/Walls\/Cracked\/7069-17.jpg')\nplt.imshow(xpic)\n\nxpic=xpic.resize(im_resize)\nxpicarr= np.array(xpic)\/255\nxpicarr=xpicarr.reshape(1,224,224,3)\nprint(xpicarr.shape)\ntgt_pred1 = model.predict_classes(xpicarr)\nprint(tgt_pred1)\n\n","39be4f1a":"# CNN Assignment-II\nDataset: https:\/\/www.kaggle.com\/aniruddhsharma\/structural-defects-network-concrete-crack-images\nRead the description of dataset. \nBuild a CNN model to predict the label for given image. \nFollow the instructions given in notebook.","d75b2fa5":"**Build LeNet-5 Model**","ed2ed47d":"* Split the data with 10% test set.\n* Build the model to find whether there is a crack or not for all types of images.\n* Include any tuning layers like Dropout\/Batchnormalization\/Learningparameters in optimizers\/Kernal-regularization etc. as per requirement.\n* Build the model for individual types if it is not possible for all parts of bridge. \n* Expected Accuracy: 65% on test data","ac62938b":"**VGG-16 Neural Network**","1d571aa5":"Hints:\n* if memory is not sufficient for loading entire dataset, feel free to use sampling techniques. \n* try with LeNET-5 and VGG-16 "}}