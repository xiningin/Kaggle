{"cell_type":{"0225836f":"code","3bb51b45":"code","065336bd":"code","ca350268":"code","d80fc83e":"markdown","097df5f8":"markdown","df887d98":"markdown"},"source":{"0225836f":"import gensim\nfrom gensim.models import Word2Vec \nfrom gensim.models import KeyedVectors\nimport pandas as pd\nfrom nltk.tokenize import RegexpTokenizer","3bb51b45":"# read in data with kaggle forum posts\nforum_posts = pd.read_csv(\"..\/input\/meta-kaggle\/ForumMessages.csv\")\n\n# grab forum forum posts\nsentences = forum_posts.Message.astype('str').tolist()\n\n# tokenize\ntokenizer = RegexpTokenizer(r'\\w+')\nsentences_tokenized = [w.lower() for w in sentences]\nsentences_tokenized = [tokenizer.tokenize(i) for i in sentences_tokenized]","065336bd":"# update existing embedding w\/ kaggle data\nmodel_2 = Word2Vec(size=300, min_count=1)\nmodel_2.build_vocab(sentences_tokenized)\ntotal_examples = model_2.corpus_count\nmodel_2.intersect_word2vec_format(\"..\/input\/word2vec-google\/GoogleNews-vectors-negative300.bin\", binary=True, lockf=1.0)\nmodel_2.train(sentences_tokenized, total_examples=total_examples, epochs=5)","ca350268":"# gensim flavored word2vec model (smaller)\nmodel_2.save(\"kaggle_word2vec_gensim.model\")\n\n# generic word2vec model\nmodel_2.wv.save_word2vec_format(\"kaggle_word2vec.model\")","d80fc83e":"## Preprocessing","097df5f8":"## Save tuned model","df887d98":"## Fine tuning"}}