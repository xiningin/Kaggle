{"cell_type":{"565584c8":"code","07e263eb":"code","a4be7996":"code","55d6dff3":"code","67c4c7ef":"code","29e3fbf0":"code","424a5eb7":"code","362e41f4":"code","238435ab":"code","2c4c896d":"code","89466dd4":"code","9805acef":"code","2920ee9f":"code","1f5d84fe":"code","652871a3":"code","5fd648b8":"code","ad057af7":"markdown","8799e32e":"markdown","6e1a4a2d":"markdown","d9e0c108":"markdown","c7054230":"markdown","f3bb2622":"markdown","8971fb05":"markdown","9eed7637":"markdown","76264a0c":"markdown","7f182d66":"markdown","8fc427c1":"markdown","5998533f":"markdown","40ed5666":"markdown","b22e6fff":"markdown","1266c976":"markdown","a8478269":"markdown","b0abe35a":"markdown","03c6143c":"markdown","9b86f6f2":"markdown","b12c3d67":"markdown","3cfd46af":"markdown","f61a3c3f":"markdown","aafa4636":"markdown","bd6dd4df":"markdown","1c25f2bf":"markdown","70be867c":"markdown"},"source":{"565584c8":"from PIL import Image\nImage.open('..\/input\/jelimages\/Case1.jpg')","07e263eb":"Image.open('..\/input\/jelimages\/Case2.jpg')","a4be7996":"Image.open('..\/input\/jelimages\/Case3.jpg')","55d6dff3":"import numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Dense, Concatenate\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras import models\ntf.compat.v1.disable_eager_execution()\n\ndef jaccard_expectation_loss(y_true, y_pred):\n    start_true, end_true = y_true[:, :MAX_LEN], y_true[:, MAX_LEN:]\n    start_pred, end_pred = y_pred[:, :MAX_LEN], y_pred[:, MAX_LEN:]\n    \n    # for true labels we can use argmax() function, cause labels don't involve in SGD\n    x_start = K.cast(K.argmax(start_true, axis=1), dtype=tf.float32)\n    x_end   = K.cast(K.argmax(end_true  , axis=1), dtype=tf.float32)\n    l = x_end - x_start + 1\n    \n    # some magic for getting indices matrix like this: [[0, 1, 2, 3], [0, 1, 2, 3]] \n    batch_size = K.shape(x_start)[0]\n    ind_row = tf.range(0, MAX_LEN, dtype=tf.float32)\n    ones_matrix = tf.ones([batch_size, MAX_LEN], dtype=tf.float32)\n    ind_matrix = ind_row * ones_matrix\n    \n    # expectations for x_start^* (x_start_pred) and x_end^* (x_end_pred)\n    x_start_pred = K.sum(start_pred * ind_matrix, axis=1)\n    x_end_pred   = K.sum(end_pred   * ind_matrix, axis=1)\n    \n    relu11 = K.relu(x_start_pred - x_start)\n    relu12 = K.relu(x_end   - x_end_pred  )\n    relu21 = K.relu(x_start - x_start_pred)\n    relu22 = K.relu(x_end_pred   - x_end  )\n    \n    intersection = l - relu11 - relu12\n    union = l + relu21 + relu22\n    jel = intersection \/ union\n    \n    return 1 - jel","67c4c7ef":"MAX_LEN = 10\nN_BATCH = 4\n\nstarts = np.array([np.eye(MAX_LEN)[1]] * N_BATCH).astype(float)\nends   = np.array([np.eye(MAX_LEN)[3]] * N_BATCH).astype(float)\ny_true = np.concatenate([starts, ends], axis=1)\n\ny_pred = np.random.rand(N_BATCH, 2 * MAX_LEN)","29e3fbf0":"y_pred_inp = tf.compat.v1.placeholder(tf.float32, shape=[None, 2 * MAX_LEN])\ny_true_inp = tf.compat.v1.placeholder(tf.float32, shape=[None, 2 * MAX_LEN])\n\njel = jaccard_expectation_loss(y_pred_inp, y_true_inp)\n\nsess = tf.compat.v1.Session()\njel = sess.run(jel, feed_dict={y_pred_inp: y_pred, y_true_inp: y_true})\njel","424a5eb7":"def get_rand_discrete_output():\n    output = np.zeros(MAX_LEN).astype(int)\n    ind = np.random.randint(0, MAX_LEN - 1)\n    output[ind] = 1\n    return output\n\ndef get_discrete_batch(n_batch):\n    starts_true, ends_true = [], []\n    starts_pred, ends_pred = [], []\n\n    batch_ind = 0\n    while batch_ind < N_BATCH:\n        starts_true_, ends_true_ = get_rand_discrete_output(), get_rand_discrete_output()\n        starts_pred_, ends_pred_ = get_rand_discrete_output(), get_rand_discrete_output()\n        if starts_true_.argmax() <= ends_true_.argmax():\n            batch_ind += 1\n            starts_true += [starts_true_]\n            ends_true   += [ends_true_  ]\n            starts_pred += [starts_pred_]\n            ends_pred   += [ends_pred_  ]\n    return np.array(starts_true), np.array(ends_true), np.array(starts_pred), np.array(ends_pred)","362e41f4":"N_BATCH = 100\n\nstarts_true, ends_true, starts_pred, ends_pred = get_discrete_batch(N_BATCH)\ny_true = np.concatenate([starts_true, ends_true], axis=1)\ny_pred = np.concatenate([starts_pred, ends_pred], axis=1)","238435ab":"sess = tf.compat.v1.Session()\njel_arr = sess.run(\n    jaccard_expectation_loss(y_pred_inp, y_true_inp),\n    feed_dict={y_pred_inp: y_pred, y_true_inp: y_true}\n)\nlen(jel_arr)","2c4c896d":"def jaccard(str1, str2):\n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ndef get_str_output(start, end):\n    output = ((start.cumsum() - end[::-1].cumsum()[::-1]) == 0).astype(int)\n    output = output * np.arange(1, len(start) + 1)\n    return \" \".join(output[output > 0].astype(str))","89466dd4":"get_str_output(np.array([1, 0, 0, 0]), np.array([0, 0, 1, 0]))","9805acef":"def checking_formula(eps=0.001):\n    for batch_ind in range(N_BATCH):\n        x_start0, x_end0 = starts_true[batch_ind].argmax(), ends_true[batch_ind].argmax()\n        x_start1, x_end1 = starts_pred[batch_ind].argmax(), ends_pred[batch_ind].argmax()    \n\n        str1 = get_str_output(starts_true[batch_ind], ends_true[batch_ind])\n        if x_start1 <= x_end1:\n            str2 = get_str_output(starts_pred[batch_ind], ends_pred[batch_ind])\n        else:\n            str2 = get_str_output(ends_pred[batch_ind], starts_pred[batch_ind])\n\n        out1 = jaccard(str1, str2)\n        out2 = 1 - jel_arr[batch_ind]\n\n        if (x_start1 > x_end1):\n            case = \"incorrrect pred \"\n            assert out2 <= 0, (str1, str2, out1, out2)\n        else:\n            if (x_start1 > x_end0) or (x_start0 > x_end1):\n                case = \"intersection = 0\"\n                assert out2 <= 0, (str1, str2, out1, out2)\n            else:\n                case = \"intersection > 0\"\n                assert abs(out1 - out2) < eps, (str1, str2, out1, out2)\n\n        if case != \"intersection > 0\":\n            show_str1 = str1.replace(\" \", \"\").ljust(MAX_LEN)\n            show_str2 = str2.replace(\" \", \"\").ljust(MAX_LEN)\n            print(f'[{case}]: true: {show_str1}, pred: {show_str2}, out1: {out1:.3f}, out2: {out2:.3f}') ","2920ee9f":"checking_formula(eps=0.001)","1f5d84fe":"x1 = Input((MAX_LEN,), dtype=tf.int32)\nx2 = Input((MAX_LEN,), dtype=tf.int32)\n\ny1 = Dense(MAX_LEN)(x1)\ny2 = Dense(MAX_LEN)(x2)\n\ny = Concatenate(axis=1)([y1, y2])\n\nmodel = models.Model(inputs=[x1, x2], outputs=y)\nmodel.compile(loss=jaccard_expectation_loss, optimizer=SGD())","652871a3":"model.summary()","5fd648b8":"x1 = np.random.rand(N_BATCH, MAX_LEN)\nx2 = np.random.rand(N_BATCH, MAX_LEN)\n\nmodel.fit([x1, x2], y_true, epochs=10, verbose=1)","ad057af7":"It works! Now check correctness.","8799e32e":"## Implementation\n\nNow implement JEL with this formula:\n\n$$\nJaccard^*\n= \\frac{l - Relu(x_{start}^* - x_{start}  ) - Relu(x_{end}   - x_{end}^*)}\n       {l + Relu(x_{start}   - x_{start}^*) + Relu(x_{end}^* - x_{end})}. \n$$\n\n**Note**: We concatenate two output vectors with start and end probabilities in one row, so $y\\_true$ and $y\\_pred$ have 2 * MAX_LEN dimensionality.","6e1a4a2d":"It looks good! Assert works even with some comments for continuos cases, when strict proof can go wrong.","d9e0c108":"## Major Theory\n\nAt this section we will try transform Jaccard score fucntion:\n$$Jaccard = \\frac{length(Intersection)}{length(Union)}$$\n\nas a differentiable loss with as similar behavior as possible. So, our aim is to get some differentiable function $Jaccard^*$:\n\n$$Jaccard^* \\approx Jaccard$$\n\nand then denote $JEL = 1 - Jaccard^*$ and use this loss for model fitting.","c7054230":"Now we can check the formula. Cause of predictions ar arbitrary, there ar three cases:\n- [**incorrect prediction**]: $x_{start}^* > x_{end}^*$ . We can check $Jaccard^* \\leq 0$.\n- [**intersection > 0**]: $x_{start}^* \\leq x_{end}^*$, $[x_{start}^*, x_{end}^*] \\cap [x_{start}^*, x_{end}^*] = \\varnothing$ . Correct prediction, but $Jaccard^* \\leq 0$ (but $Jaccard = 0$)\n- [**intersection = 0**]: $x_{start}^* \\leq x_{end}^*$, $[x_{start}^*, x_{end}^*] \\cap [x_{start}^*, x_{end}^*] \\neq \\varnothing$ . Everything is correct : $Jaccard^* > 0$ (and $Jaccard > 0$)","f3bb2622":"### Case $x_{start}^* \\leq x_{end}^*$.\n\n#### Subcase $[x_{start}, x_{end}] \\cap [x_{start}^*, x_{end}^*] \\neq \\varnothing  $.\n\nCase images for better understanding:","8971fb05":"#### Subcase $[x_{start}, x_{end}] \\cap [x_{start}^*, x_{end}^*] = \\varnothing  $.\n\nThere is only two cases (now $Union$ of segments has two different segements):","9eed7637":"Lets prove this case in three steps:\n\n1. If $x_{start}^*$ is in zone 3 (except board), then proof is over, cause of $Relu(x_{start}^* - x_{start}) > l$. Similary if $x_{end}^*$ is in zone 1 (except board), proof is over again cause of $Relu(x_{end}- x_{end}^*) > l$. Now lets say, that $x_{start}^*$ is not in zone 1, and $x_{end}^*$ is not in zone 1.\n<br><br>\n2. Case with $x_{start}^*$ in zone 1 imposible, cause it means, that $x_{end}^*$ in zone 1 too cause of $x_{end}^* \\leq x_{start}^*$. Similary case with $x_{end}^*$ in zone 3 imposible too.\n<br><br>\n3. Now we have only one case , when $x_{start}^*$ and $x_{end}^*$ are in zone 2. In this case all relu functions is positive and finally we get (using inequality $x_{start}^* \\neq x_{end}^*$):\n<br><br>\n$$\nRelu(x_{start}^* - x_{start}) + Relu(x_{end}   - x_{end}^*) = (x_{start}^* - x_{start}) + (x_{end}   - x_{end}^*) = (x_{start}^* - x_{end}^*) + (x_{end} - x_{start}) \\geq 1 + (l - 1) = l\n$$\n\n(another comment: in continuos case with small possibility $0 < x_{start}^* - x_{end}^* < 1$)","76264a0c":"### Checking Being Loss\n\nNow check, that our custom loss can be used as Keras loss with some tiny model. It can be wrong if we used some non differentiable functions, or cause of other technical problems. And we also can check decreasing loss during training, that a necessary point, independent of size model.","7f182d66":"Apply fresh JEL:","8fc427c1":"# Conclusion\n\nThis notebook introduce two new loss function: Weighted BCE (without implementation) and Jaccard Expectation Loss (with implementation). It needs for eliminate some BCE defects, decribed above, and just for using competition metric as a loss. That's not so easy, cause the metric is sophisticated enough and we need to programm it in differentiable manner. It was hard in sone degree, but we do it! (advantages describing in **Theoretical Conclusion** part).\n<br><br>\nHope, this metric will help you to achive good score according to your aim. I don't use it yet, but going to do that this night. Please, let me know if this metric really can help you to get better score, than you already have (or don't helps whatsoever :) ). Good luck!","5998533f":"Now find original Jaccard scores for the same data. Cause we don't have real texts, we just transform vectors to string, and then apply Jaccard score in next manner: \n    - inp start: [1, 0, 0, 0]\n    - inp end  : [0, 0, 1, 0]\n    - out str  : \" 1 2 3\"\n(operating only 0 and 1 signs would be incorrect cause of set() using in Jaccard score formula)","40ed5666":"# Why BCE can works wrong?\n\nLets consider some of disadvantages of BCE in TSE task.\n\n## Penalty untill ideal prediction\n\nThat's what i mean: if at some stage of SGD we have prediction like this\n    - true: [0  , 0  , 1  , 0  , 0  , 0  ] (start label or end label)\n    - pred: [0.1, 0.1, 0.4, 0.1, 0.2, 0.1]\nthen this prediction is alredy fine, but obviously $BCE(true, pred) \\neq 0$. Moreover, $BCE(true, pred^*) \\neq 0$ for each $pred^* \\neq true$, that seems way too unnesesarily penalty. It will be good to find some loss $L$, that can get $L(true, pred) \\approx 0$ for this example.\n\n##  Order Independence\n\nThat's in my opinion more important disadvantage. Examples again:\n    - true : [0  , 0  , 1  , 0  , 0  , 0  ]\n    - pred1: [0. , 0. , 0.1, 0. , 0. , 0.9]\n    - pred2: [0. , 0. , 0.1, 0. , 0.9, 0. ]\n    - pred3: [0. , 0. , 0.1, 0.9, 0. , 0. ]\n    \nBecause of index order independence of $BCE$, we get $BCE(true, pred_1) = BCE(true, pred_2) = BCE(true, pred_3)$. But obviously $pred_3$ is the best prediction, cause corresponding Jaccard score is higher then for another predictions. That's bad cause of two reasons:\n\n- BCE loss don't repeat Jaccard score behavior\n- This bad BCE property can create some area in model parameters space with constant loss, therefore with null gradient. It means, that in this space there was a lot of local minima (or maxima, or saddle point), that will stop your SGD optimization.\n\n##  Weighted BCE\n\nThat's a WBCE comes out time. Lets just spread some potential from true label for making $pred_3$ prediction the best:\n\n    - true      : [0  , 0  , 1, 0  , 0  , 0  ]\n    - potential : [0.2, 0.1, 0, 0.1, 0.2, 0.3]\n    \nand then just weigh BCE vector by this potential vector. We also can tune potential construction:\n- different potential factor (0.1 in example - we also can multyply by 0.1 instead of adding)\n- biased potential (in example we can try to penalize left errors tougher, then right ones or vice versa)\n\nfor getting closer to Jaccard score behavior. But apparenly even optimized WBCE loss will have some fundamental difference from Jaccard score that never vanish.","b22e6fff":"Now $Union$ of segments has two different segements, separate with some gap with length $\\xi > 0$. <br>\n<br>\nLets consider case $x^*_{start} \\leq x^*_{end} \\leq x_{start} \\leq x_{end}$. In this case gap is $\\xi = x_{start} - x^*_{end} > 0$. Compute Relu functions:\n\n$$Relu(x_{start}^* - x_{start}) = Relu(x_{end}^* - x_{end}) = 0$$\n$$Relu(x_{start} - x_{start}^*) = x_{start} - x_{start}^*$$\n$$Relu(x_{end} - x_{end}^*) = x_{end} - x_{end}^*$$\n\nAnd put into Jaccard score formula:\n\n$$\nJaccard^*\n= \\frac{l - 0 - (x_{end} - x_{end}^*)}\n       {l + (x_{start}   - x_{start}^*) + 0}\n= \\frac{l + x_{end}^* - x_{end}    }\n       {l + x_{start} - x_{start}^*} \n= \\frac{l + x_{end}^* - x_{start} + x_{start} - x_{end}}\n       {l + x_{start} - x_{start}^*}\n= \\frac{l - \\xi  - (l - 1)}\n       {l + x_{start} - x_{start}^*}\n= \\frac{1 - \\xi}\n       {l + x_{start} - x_{start}^*}\n$$\n\nThat's really attractive result, and thats why:\n- if $x_{end}^*$ is integer, so in corner case we have $\\xi = 1$ and $Jaccard^*=Jaccard=0$. In case of continuos $x_{end}^*$ near to $x_{start}$ we can get some small positive numbers, when $\\xi \\approx 0$, but there is small possibility for this continuos case.\n- in cases with with gap $\\xi > 1$ we get something better, then $Jaccard=0$, we get .. negative $Jaccard^*$! That's relly smart benavior, cause now we can separate \"bad cases\" from \"so so bad cases\". And more importantly, we get correct derivative for our loss, so we can make a step from \"so so bad case\" to \"bad case\" in SGD process (as describe in sections above, BCE can't do that even in $Jaccard > 0$ cases).","1266c976":"## Checking Correctness","a8478269":"# Abstract\n\nSome days ago **Y. O.** present a post about new approach for Tween Segment Extraction (TSE) : https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/142125. The was intresting idea about using Jaccard metric not only as metric, but as loss too: https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/142125#820299. Using Jaccard as loss seems really attractive for me, cause it allows to prevent loss-metric bias (for instance, using BCE only has some disadvantages, explained in next sections). But this idea works only in case of segment prediction (one label \"00111100\" instead of start_label \"00100000\" and end label \"00000100\"), and even in this case we have a problem with continuity property: how to determine predicted segment from arbitrary prediction?<br>\n   <br>\n    In this notebooks i present two new losses, that seems to be usefull in TSE task:\n   - **W**eighted **BCE** (**WBCE**): BCE with taking into account order of items in prediction\n   - **J**accard **E**xpectation **L**oss (**JEL**): Using Jaccard metric as loss in \"traditional\" task formulations with two heads for start and end of text span prediction (with some trick, that i also explain below)\n   \nHope you're enjoyng mathematical proofs and improve your results with one of this losses.\n\n**Assumption**: In my opinion, we have strange behavior with competition Jaccard metric, cause of using set() in computation. It means, that, for instance:\n\n$$Jaccard(i \\text{ } am \\text{ } data \\text{ } scientist) = Jaccard(data \\text{ } i \\text{ } scientist \\text{ } am) = 1$$\n\nIt looks strange, but that happends rare, cause of $text$ and $selected\\_text$ are ordered simbol sequence from the same corpora. So, lets take into account simbol orders:\n<br><br>\n$$\nJaccard(i \\text{ } am \\text{ } data \\text{ } scientist) = Jaccard(data \\text{ } i \\text{ } scientist \\text{ } am) = 0\n$$\n$$\nJaccard(i \\text{ } am \\text{ } data \\text{ } scientist) = Jaccard(i \\text{ } am \\text{ } scientist \\text{ } data) = 0.5\n$$\n$$\nJaccard(i \\text{ } am \\text{ } data \\text{ } scientist) = Jaccard(i \\text{ } data \\text{ } scientist \\text{ } am) = 0.25\n$$\n\nThis score, i think, have very high correlation with competition Jaccard score, but make our discussion easier.","b0abe35a":"# Jaccard Expectation Loss\n\n## Denotes\n\nNow i introduce JEL. Lets say we consider one sample and try to predict start and end indexes of text span. Also introduce some natural notation:\n- $start\\_pred$, $end\\_pred$ - text span start\/end prediction vectors\n- $start$, $end$ - start\/end true one-encoding vectors\n- $x_{start}$, $x_{end}$ - inex of 1 in true start\/end vectors\n- $l = length([x_{start}, x_{end}]) = x_{end} - x_{start} + 1 > 0$ - length of true text span\n\n(+1, cause in terns of this competition, length of segement with $x\\_start=x\\_end$ equals 1)\n\nThat OK with four argument dependences for JEL: $$JEL=JEL(start, end, start\\_pred, end\\_pred)$$ cause \n- $[start\\_pred, end\\_pred]$ - model output\n- $[start, end]$ - true label we can prepare.\n\nExample for understanding:\n    - start     : [0  , 1  , 0  , 0  , 0  , 0  ] (x_start = 1)\n    - end       : [0  , 0  , 0  , 0  , 1  , 0  ] (x_end = 4)\n    - start_pred: [0.1, 0.4, 0.2, 0.1, 0. , 0. ]\n    - end_pred  : [0. , 0. , 0.1, 0. , 0.6, 0.2]\n    - indexes   : [0  , 1  , 2  , 3  , 4  , 5  ]\n    \n## Why Expectation?\n\nFirst problem : we want to get prediction $x_{start}^*$, $x_{end}^*$ for $x_{start}$, $x_{end}$ with vectors $start\\_pred$ and $end\\_pred$ in way like that\n\n$$x_{start}^* = argmax(start\\_pred)$$\n$$x_{end}^* = argmax(end\\_pred)$$\n\nfor getting JEL counting more clear, but $argmax(\\cdot)$ function don't differentiable. But insead of tough $argmax(\\cdot)$ function we can use methematical expectation:\n\n$$x_{start}^* = E(x_{start}|start\\_pred) = <start\\_pred, indexes>$$\n$$x_{end}^* = E(x_{end}|end\\_pred) = <end\\_pred, indexes>$$\n\n($indexes$ just a range from 0 till MAX_LEN). Dot product with constant vecor is differentiable, so problem is solved. Some observation:\n- For prevent training-inference bias we have to use similar approach for getting prediction from probability vector at inference time.\n- If $start\\_pred$ or $end\\_pred$ near to delta-function (one item equal 1, other equal 0), then expectation equals argmax function. At first glance that's the case of most samples of  $start\\_pred$ or $end\\_pred$ in practice.\n\nNow it ehough to find JEL function with next dependences:\n$$JEL=JEL(x_{start}, x_{end}, x_{start}^*, x_{end}^*)$$\n\n","03c6143c":"Loss really decrease!","9b86f6f2":"Generate some correct true labels and arbitrary prediction lables:","b12c3d67":"### Checking Formula\n\nGenerate some batch of data with some restrictions:\n- True labels are correct (starts smaller, than ends)\n- Prediction labels are arbitrary, nut only one-encoding vectors (for being able to compare with original Jaccard score)","3cfd46af":"In this case $Union$ and $Intersection$ for Jaccard score is a segments (as opposed to the case $[x_{start}, x_{end}] \\cap [x_{start}^*, x_{end}^*] = \\varnothing $, when union is a tuple of segments). So, formula in terms of current denotes absolutely clear:\n\n$$length(Intersection) = min(x_{end}, x^*_{end}) - max(x_{start}, x^*_{start}) + 1$$\n$$length(Union) = max(x_{end}, x^*_{end}) - min(x_{start}, x^*_{start}) + 1$$\n\nexcept the fact, that $max(\\cdot)$ and $min(\\cdot)$ functions don't differentiable and therefore we cannot use it. But we can use next trick for achieve differentiability:\n\n$$max(a, b) = a + Relu(b - a)$$\n$$min(a, b) = a - Relu(a - b)$$\n\nThis trick helps to make Jaccard score differentiable too:\n\n$$\nJaccard\n= \\frac{length(Intersection)}{length(Union)}\n= \\frac{min(x_{end}, x^*_{end}) - max(x_{start}, x^*_{start}) + 1}\n       {max(x_{end}, x^*_{end}) - min(x_{start}, x^*_{start}) + 1}\n= \\frac{[x_{end} - Relu(x_{end} - x^*_{end})] - [x_{start} + Relu(x^*_{start} - x_{start})] + 1}\n       {[x_{end} + Relu(x^*_{end} - x_{end})] - [x_{start} - Relu(x_{start} - x^*_{start})] + 1}\n$$\n\nWith $l = x_{end} - x_{start} + 1$ we get:\n\n$$\nJaccard^*\n= \\frac{l - Relu(x_{start}^* - x_{start}  ) - Relu(x_{end}   - x_{end}^*)}\n       {l + Relu(x_{start}   - x_{start}^*) + Relu(x_{end}^* - x_{end})}. \n$$","f61a3c3f":"### Case $x_{start}^*$ > $x_{end}^*$.\n\nLet proof, that in this case $Jaccard^* \\leq 0$ again, but now in geometric manner. It enough to proove, tnat numerator is negative (cause deniminator is positive in each cases): \n\n$$\nl - Relu(x_{start}^* - x_{start}) - Relu(x_{end} - x_{end}^*) \\leq 0\n$$\nor\n<br>\n$$\nRelu(x_{start}^* - x_{start}) + Relu(x_{end} - x_{end}^*) \\geq l\n$$\n\nLet's go. True label split all space into 3 zones:","aafa4636":"### Theoretical Conclusion\n\nProof is over. Finnally we get one formula for all cases:\n<br>\n$$\nJaccard^*\n= \\frac{l - Relu(x_{start}^* - x_{start}  ) - Relu(x_{end}   - x_{end}^*)}\n       {l + Relu(x_{start}   - x_{start}^*) + Relu(x_{end}^* - x_{end})}. \n$$\nwich has a lot of advantages:\n- Absolutely equal competition Jaccard score, when it's possible to compute.\n- When $Jaccard = 0$, $Jaccard^* <= 0$ and push us into correct area in SGD.\n- When prediction is incorrect (left boarder prediction bigger then right one) $Jaccard <= 0$ again! So, maybe we finally get good submission even without postprocessing.\n- Denominator for this formula is always positive, so we can compute it for every sample without some lousy NaN's during training. Moreover denominator $ \\geq l \\geq 1$, so we get computationally robust formula even in theory area, so we haven't add some litle numbers in denimunator in practice.","bd6dd4df":"Case $x_{start} \\leq x_{end} \\leq x_{start}^* \\leq x_{end}^*$ with $\\xi = x^*_{start} - x_{end} > 0$ exactly the same:\n<br>\n\n$$Relu(x_{start} - x_{start}^*) = Relu(x_{end} - x_{end}^*) = 0$$\n$$Relu(x_{start}^* - x_{start}) = x_{start}^* - x_{start}$$\n$$Relu(x_{end}^* - x_{end}) = x_{end}^* - x_{end}$$\n<br>\n\n$$\nJaccard\n= \\frac{l - (x_{start}^* - x_{start}) - 0}\n       {l + 0 + (x_{end}^* - x_{end})} \n= \\frac{l - (x_{start}^* - x_{start})}\n       {l + (x_{end  }^* - x_{end  })} \n= \\frac{l - (x_{start}^* - x_{end} + x_{end} - x_{start})}\n       {l + (x_{end  }^* - x_{end  })} \n= \\frac{l - (\\xi + (l - 1))}\n       {l + (x_{end  }^* - x_{end  })} \n= \\frac{1 - \\xi}\n       {l + (x_{end  }^* - x_{end  })} \n$$","1c25f2bf":"And apply the formula:","70be867c":"Check on some random input data:"}}