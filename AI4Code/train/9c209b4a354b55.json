{"cell_type":{"58cc5eea":"code","4f3ca8be":"code","6631ee8d":"code","a934ce40":"code","ceb1ef9e":"code","aaaca53e":"code","f3084772":"code","0f8e414d":"code","765d9f73":"code","3524c647":"code","f040ce4d":"code","3dfa56c9":"code","f2eac81a":"code","46b51509":"code","6e530992":"code","34b4631a":"code","373db219":"code","a1dd40b4":"code","b43a7890":"code","67db8d70":"code","ecf67c1e":"code","ac53fec4":"code","d40602fd":"code","69beb0c1":"code","040bb111":"code","82e2ab8d":"code","22a026aa":"code","43577b3a":"code","304df18a":"code","bf573112":"code","b137dbdf":"code","ee53c994":"code","a6d1793e":"code","d9a63bfd":"code","95170294":"code","89e54a2b":"code","f79c690b":"code","3755d0e8":"code","e69fe0db":"code","9da2de80":"code","6b1c4f16":"code","7727533a":"markdown","a3f9e363":"markdown","52709ace":"markdown","61bf6462":"markdown","beeeffa0":"markdown","7d89b9a9":"markdown","d38531a6":"markdown","5a8c9524":"markdown","b72682d6":"markdown","741facf8":"markdown","d6aacd78":"markdown","b26a29dc":"markdown","3115d7bc":"markdown","c3a98e5c":"markdown","af11bbae":"markdown","6ec9b3a0":"markdown","49c87ec9":"markdown","5e1ddcc4":"markdown"},"source":{"58cc5eea":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport cv2\nfrom skimage.transform import pyramid_reduce, resize\nfrom sklearn.model_selection import train_test_split\nimport os\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout\nfrom keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf","4f3ca8be":"np.random.seed(123)","6631ee8d":"# determine extension of images\n## covid positive images\npath = '..\/input\/covidct\/CT_COVID'\next_set = set()\nfor child in Path(path).iterdir():\n    ext = Path(child).suffix\n    ext_set.add(ext)\nprint(f'positive image extensions: {ext_set}')\n\n## covid negative images\npath = '..\/input\/covidct\/CT_NonCOVID'\next_set = set()\nfor child in Path(path).iterdir():\n    ext = Path(child).suffix\n    ext_set.add(ext)\nprint(f'negative image extensions: {ext_set}')","a934ce40":"# obtain list of images\n## postive\npath = '..\/input\/covidct\/CT_COVID'\npos_li = list(Path(path).glob('*.png'))\n\n## negative\npath = '..\/input\/covidct\/CT_NonCOVID'\nneg_li = list(Path(path).glob('*.png'))\nneg_li.extend(list(Path(path).glob('*.jpg')))\n\n# display number of images\nprint(f'Postive images: {len(pos_li)}\\nNegative images: {len(neg_li)}')","ceb1ef9e":"# create numpy array placeholder for pixels with 1 channel (grey scale)\nIMG_SIZE = 128\npos_data = np.empty((len(pos_li), IMG_SIZE, IMG_SIZE, 1), dtype=np.float32)\nneg_data = np.empty((len(neg_li), IMG_SIZE, IMG_SIZE, 1), dtype=np.float32)\n# ^ float data type must be used to save precise pixel values","aaaca53e":"# convert images to numpy arrays\n## positive\nfor i, img_path in enumerate(sorted(pos_li)):\n    # load image\n    img = cv2.imread(str(img_path))\n    # convert BGR to RGB (since CV2 reads in BGR)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    # resize image with 1 channel\n    img = resize(img, output_shape=(IMG_SIZE, IMG_SIZE, 1), preserve_range=True)\n    # save to x_data\n    pos_data[i] = img\n## negative\nfor i, img_path in enumerate(sorted(neg_li)):\n    # load image\n    img = cv2.imread(str(img_path))\n    # convert BGR to RGB (since CV2 reads in BGR)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    # resize image with 1 channel\n    img = resize(img, output_shape=(IMG_SIZE, IMG_SIZE, 1), preserve_range=True)\n    # save to x_data\n    neg_data[i] = img","f3084772":"# scale image arrays\npos_data \/= 255\nneg_data \/= 255","0f8e414d":"# define function to perform image segmentation with k-means clustering\ndef k_means(img_array_list, K, criteria, attempts):\n    new_img_array_list = []\n    for array in img_array_list:\n        # flatten array into 2D\n        img = array.reshape(-1,1) # reshape into new dimensions; -1 refers to unknown dimension and will depend on others\n                                  # (-1,1) will result in 2D with 1 column and n rows where 1 column x n rows is equal to  \n                                  # the original number of elements. ex) (10,10) = (5,20) > both with 100 elements\n                                  # 1 column is used since it's gray-scale image (3 used for RGB)\n        ret, label, center = cv2.kmeans(img, K, None, criteria, attempts, cv2.KMEANS_PP_CENTERS)\n#         center = np.uint8(center)\n        res = center[label.flatten()]\n        result_image = res.reshape(128,128,1)\n        new_img_array_list.append(result_image)\n    return new_img_array_list","765d9f73":"# perform image segmentation\n## define hyperparameters\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\nK = 2\nattempts=10\n## positive\npos_data_seg = k_means(pos_data, K, criteria, attempts)\n## negative\nneg_data_seg = k_means(neg_data, K, criteria, attempts)","3524c647":"# show results for positive scans\nfig, ax = plt.subplots(5, 2, figsize=(10, 10))\nfor i, seg in enumerate(pos_data_seg):\n    if i == 5:\n        break\n    ax[i, 0].imshow(pos_data[i].squeeze(), cmap='gray')\n    ax[i, 1].imshow(seg.squeeze(), cmap='gray')\nfig.suptitle('Image Segmentation of COVID Postive Images\\nOriginal(Left) Segmented(Right)', fontsize=16)\nplt.show()","f040ce4d":"# show results for negtive \nfig, ax = plt.subplots(5, 2, figsize=(10, 10))\nfor i, seg in enumerate(neg_data_seg):\n    if i == 5:\n        break\n    ax[i, 0].imshow(neg_data[i].squeeze(), cmap='gray')\n    ax[i, 1].imshow(seg.squeeze(), cmap='gray')\nfig.suptitle('Image Segmentation of COVID Negative Images\\nOriginal(Left) Segmented(Right)', fontsize=16)\nplt.show()","3dfa56c9":"# split data into train-validation datasets with 20% validation proportion\nx_data = pos_data_seg + neg_data_seg  # segmented images\nx_data = np.array(x_data, dtype='float32')\n\ny_data = [1]*len(pos_data_seg) + [0]*len(neg_data_seg)\ny_data = np.array(y_data, dtype='float32')\n\nx_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2)","f2eac81a":"# callback options\n\"\"\"\nAnd EarlyStopping will stop the training if validation accuracy doesn't improve in 15 epochs.\n\"\"\"\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=15, verbose=1, mode='auto')","46b51509":"# define input resolution size\nimg_height = 128\nimg_width = 128\n\n# define function to build VGG-16 model\ndef build_vgg():\n    model = Sequential()\n    model.add(Conv2D(input_shape=(img_height,img_width,1),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Flatten())\n    model.add(Dense(units=4096,activation=\"relu\"))\n    model.add(Dense(units=4096,activation=\"relu\"))\n    model.add(Dense(units=1, activation=\"sigmoid\"))\n    opt = Adam(lr=0.001)\n    model.compile(optimizer=opt, loss=keras.losses.binary_crossentropy, metrics=['accuracy'])\n    \n    return model\n\n# build model\nmodel_vgg_seg = build_vgg()","6e530992":"# model summary\nmodel_vgg_seg.summary()","34b4631a":"# train model\nbatch_size = 16\nepochs = 20\nhistory_vgg_seg = model_vgg_seg.fit(x_train, y_train, validation_data=(x_val, y_val), \n                                    epochs=epochs, batch_size=batch_size, callbacks=[early])","373db219":"# evaluate model\nfig, ax = plt.subplots(1, 2, figsize=(10, 7))\n\n# loss\nax[0].set_title('model loss')\nax[0].plot(history_vgg_seg.history['loss'], 'b')\nax[0].plot(history_vgg_seg.history['val_loss'], 'r')\nax[0].legend(['train', 'test'], loc='upper right')\nax[0].set_ylabel('loss')\nax[0].set_xlabel('epoch')\n\n# accuracy \nax[1].set_title('model accuracy')\nax[1].plot(history_vgg_seg.history['accuracy'], 'b')\nax[1].plot(history_vgg_seg.history['val_accuracy'], 'r')\nax[1].legend(['train', 'test'], loc='lower right')\nax[1].set_ylabel('accuracy')\nax[1].set_xlabel('epoch')\n\nplt.tight_layout()\nplt.show()","a1dd40b4":"# generate train and validation datasets from directories\n\"\"\"\nhttps:\/\/stackoverflow.com\/questions\/42443936\/keras-split-train-test-set-when-using-imagedatagenerator\nhttps:\/\/www.kaggle.com\/dergel\/cnn-on-covid-19-ct-lungs-scans\n\"\"\"\nDIR = '..\/input\/covidct'\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    horizontal_flip=True,\n    rotation_range=5,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    shear_range=0.05,\n    zoom_range=0.05,\n    validation_split=0.2) \n\ntrain_generator = train_datagen.flow_from_directory(\n    DIR,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    color_mode=\"grayscale\",\n    subset='training') \n\nvalidation_generator = train_datagen.flow_from_directory(\n    DIR, \n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    color_mode=\"grayscale\",\n    subset='validation') ","b43a7890":"# rebuild model\nmodel_vgg_raw = build_vgg()","67db8d70":"# train model\nhistory_vgg_raw = model_vgg_raw.fit_generator(train_generator, steps_per_epoch = train_generator.samples \/\/ batch_size,\n        validation_data = validation_generator, validation_steps = validation_generator.samples \/\/ batch_size,\n        epochs=epochs, callbacks=[early])","ecf67c1e":"# evaluate model\nfig, ax = plt.subplots(1, 2, figsize=(10, 7))\n\n# loss\nax[0].set_title('model loss')\nax[0].plot(history_vgg_raw.history['loss'], 'b')\nax[0].plot(history_vgg_raw.history['val_loss'], 'r')\nax[0].legend(['train', 'test'], loc='upper right')\nax[0].set_ylabel('loss')\nax[0].set_xlabel('epoch')\n\n# accuracy \nax[1].set_title('model accuracy')\nax[1].plot(history_vgg_raw.history['accuracy'], 'b')\nax[1].plot(history_vgg_raw.history['val_accuracy'], 'r')\nax[1].legend(['train', 'test'], loc='lower right')\nax[1].set_ylabel('accuracy')\nax[1].set_xlabel('epoch')\n\nplt.tight_layout()\nplt.show()","ac53fec4":"# define function to build 3-layer CNN model\ndef build_cnn():\n    model = Sequential()\n    model.add(Conv2D(32, 3, padding='same', activation='relu',input_shape=(img_height, img_width, 1))) \n    model.add(MaxPool2D()) \n    model.add(Conv2D(64, 5, padding='same', activation='relu'))\n    model.add(MaxPool2D())\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    opt = Adam(lr=0.001)\n    model.compile(optimizer=opt, loss=keras.losses.binary_crossentropy, metrics=['accuracy'])\n    \n    return model\n\n# build 3-layer CNN model\nmodel_cnn_seg = build_cnn()","d40602fd":"model_cnn_seg.summary()","69beb0c1":"# train model\nhistory_cnn_seg = model_cnn_seg.fit(x_train, y_train, validation_data=(x_val, y_val), \n                                    epochs=epochs, batch_size=batch_size, callbacks=[early])","040bb111":"# evaluate model\nfig, ax = plt.subplots(1, 2, figsize=(10, 7))\n\n# loss\nax[0].set_title('model loss')\nax[0].plot(history_cnn_seg.history['loss'], 'b')\nax[0].plot(history_cnn_seg.history['val_loss'], 'r')\nax[0].legend(['train', 'test'], loc='upper right')\nax[0].set_ylabel('loss')\nax[0].set_xlabel('epoch')\n\n# accuracy \nax[1].set_title('model accuracy')\nax[1].plot(history_cnn_seg.history['accuracy'], 'b')\nax[1].plot(history_cnn_seg.history['val_accuracy'], 'r')\nax[1].legend(['train', 'test'], loc='lower right')\nax[1].set_ylabel('accuracy')\nax[1].set_xlabel('epoch')\n\nplt.tight_layout()\nplt.show()","82e2ab8d":"# build model\nmodel_cnn_raw = build_cnn()","22a026aa":"history_cnn_raw = model_cnn_raw.fit_generator(train_generator, steps_per_epoch = train_generator.samples \/\/ batch_size,\n        validation_data = validation_generator, validation_steps = validation_generator.samples \/\/ batch_size,\n        epochs=epochs, callbacks=[early])","43577b3a":"# evaluate model\nfig, ax = plt.subplots(1, 2, figsize=(10, 7))\n\n# loss\nax[0].set_title('model loss')\nax[0].plot(history_cnn_raw.history['loss'], 'b')\nax[0].plot(history_cnn_raw.history['val_loss'], 'r')\nax[0].legend(['train', 'test'], loc='upper right')\nax[0].set_ylabel('loss')\nax[0].set_xlabel('epoch')\n\n# accuracy \nax[1].set_title('model accuracy')\nax[1].plot(history_cnn_raw.history['accuracy'], 'b')\nax[1].plot(history_cnn_raw.history['val_accuracy'], 'r')\nax[1].legend(['train', 'test'], loc='lower right')\nax[1].set_ylabel('accuracy')\nax[1].set_xlabel('epoch')\n\nplt.tight_layout()\nplt.show()","304df18a":"# evaluate model\nfig, ax = plt.subplots(4, 2, figsize=(10, 10))\n\n# loss\nax[0,0].set_title('Segmented VGG-16 loss')\nax[0,0].plot(history_vgg_seg.history['loss'], 'b')\nax[0,0].plot(history_vgg_seg.history['val_loss'], 'r')\nax[0,0].legend(['train', 'test'], loc='lower right')\nax[0,0].set_ylabel('loss')\nax[0,0].set_xlabel('epoch')\nax[1,0].set_title('Raw VGG-16 loss')\nax[1,0].plot(history_vgg_raw.history['loss'], 'b')\nax[1,0].plot(history_vgg_raw.history['val_loss'], 'r')\nax[1,0].legend(['train', 'test'], loc='lower right')\nax[1,0].set_ylabel('loss')\nax[1,0].set_xlabel('epoch')\nax[2,0].set_title('Segmented 3-layer CNN loss')\nax[2,0].plot(history_cnn_seg.history['loss'], 'b')\nax[2,0].plot(history_cnn_seg.history['val_loss'], 'r')\nax[2,0].legend(['train', 'test'], loc='lower right')\nax[2,0].set_ylabel('loss')\nax[2,0].set_xlabel('epoch')\nax[3,0].set_title('Raw 3-layer CNN loss')\nax[3,0].plot(history_cnn_raw.history['loss'], 'b')\nax[3,0].plot(history_cnn_raw.history['val_loss'], 'r')\nax[3,0].legend(['train', 'test'], loc='lower right')\nax[3,0].set_ylabel('loss')\nax[3,0].set_xlabel('epoch')\n\n# accuracy \nax[0,1].set_title('Segmented VGG-16 accuracy')\nax[0,1].plot(history_vgg_seg.history['accuracy'], 'b')\nax[0,1].plot(history_vgg_seg.history['val_accuracy'], 'r')\nax[0,1].legend(['train', 'test'], loc='lower right')\nax[0,1].set_ylabel('accuracy')\nax[0,1].set_xlabel('epoch')\nax[1,1].set_title('Raw VGG-16 accuracy')\nax[1,1].plot(history_vgg_raw.history['accuracy'], 'b')\nax[1,1].plot(history_vgg_raw.history['val_accuracy'], 'r')\nax[1,1].legend(['train', 'test'], loc='lower right')\nax[1,1].set_ylabel('accuracy')\nax[1,1].set_xlabel('epoch')\nax[2,1].set_title('Segmented 3-layer accuracy')\nax[2,1].plot(history_cnn_seg.history['accuracy'], 'b')\nax[2,1].plot(history_cnn_seg.history['val_accuracy'], 'r')\nax[2,1].legend(['train', 'test'], loc='lower right')\nax[2,1].set_ylabel('accuracy')\nax[2,1].set_xlabel('epoch')\nax[3,1].set_title('Raw 3-layer accuracy')\nax[3,1].plot(history_cnn_raw.history['accuracy'], 'b')\nax[3,1].plot(history_cnn_raw.history['val_accuracy'], 'r')\nax[3,1].legend(['train', 'test'], loc='lower right')\nax[3,1].set_ylabel('accuracy')\nax[3,1].set_xlabel('epoch')\n\nplt.tight_layout()\nplt.show()","bf573112":"# build vgg-16 model\nmodel_vgg_raw_2 = build_vgg()","b137dbdf":"# train model\nbatch_size = 16\nepochs = 200\nhistory_vgg_raw_2 = model_vgg_raw_2.fit_generator(train_generator, steps_per_epoch = train_generator.samples \/\/ batch_size,\n        validation_data = validation_generator, validation_steps = validation_generator.samples \/\/ batch_size,\n        epochs=epochs, callbacks=[early])","ee53c994":"# build model with SGD instead of ADAM\ndef build_vgg_sgd():\n    model = Sequential()\n    model.add(Conv2D(input_shape=(img_height,img_width,1),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Flatten())\n    model.add(Dense(units=4096,activation=\"relu\"))\n    model.add(Dense(units=4096,activation=\"relu\"))\n    model.add(Dense(units=1, activation=\"sigmoid\"))\n    opt = SGD(lr=0.001)\n    model.compile(optimizer=opt, loss=keras.losses.binary_crossentropy, metrics=['accuracy'])\n    return model\n\nmodel_vgg_sgd = build_vgg_sgd()","a6d1793e":"# train model\nbatch_size = 16\nepochs = 50\nhistory_vgg_sgd = model_vgg_sgd.fit_generator(train_generator, steps_per_epoch = train_generator.samples \/\/ batch_size,\n        validation_data = validation_generator, validation_steps = validation_generator.samples \/\/ batch_size,\n        epochs=epochs, callbacks=[early])","d9a63bfd":"# build model with SGD reduced learning rate\ndef build_vgg_sgd2():\n    model = Sequential()\n    model.add(Conv2D(input_shape=(img_height,img_width,1),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Flatten())\n    model.add(Dense(units=4096,activation=\"relu\"))\n    model.add(Dense(units=4096,activation=\"relu\"))\n    model.add(Dense(units=1, activation=\"sigmoid\"))\n    opt = SGD(lr=1e-5)\n    model.compile(optimizer=opt, loss=keras.losses.binary_crossentropy, metrics=['accuracy'])\n    return model\n\nmodel_vgg_sgd2 = build_vgg_sgd2()","95170294":"# train model\nbatch_size = 16\nepochs = 200\nhistory_vgg_sgd2 = model_vgg_sgd2.fit_generator(train_generator, steps_per_epoch = train_generator.samples \/\/ batch_size,\n        validation_data = validation_generator, validation_steps = validation_generator.samples \/\/ batch_size,\n        epochs=epochs, callbacks=[early])","89e54a2b":"# evaluate model\nfig, ax = plt.subplots(3, 2, figsize=(10, 10))\n\n# loss\nax[0,0].set_title('Raw VGG-16 with 20 epochs loss')\nax[0,0].plot(history_vgg_raw.history['loss'], 'b')\nax[0,0].plot(history_vgg_raw.history['val_loss'], 'r')\nax[0,0].legend(['train', 'test'], loc='lower right')\nax[0,0].set_ylabel('loss')\nax[0,0].set_xlabel('epoch')\nax[1,0].set_title('Raw VGG-16 with 200 epochs loss')\nax[1,0].plot(history_vgg_raw_2.history['loss'], 'b')\nax[1,0].plot(history_vgg_raw_2.history['val_loss'], 'r')\nax[1,0].legend(['train', 'test'], loc='lower right')\nax[1,0].set_ylabel('loss')\nax[1,0].set_xlabel('epoch')\nax[2,0].set_title('Raw VGG-16 with SGD & reduced lr epochs loss')\nax[2,0].plot(history_vgg_sgd2.history['loss'], 'b')\nax[2,0].plot(history_vgg_sgd2.history['val_loss'], 'r')\nax[2,0].legend(['train', 'test'], loc='lower right')\nax[2,0].set_ylabel('loss')\nax[2,0].set_xlabel('epoch')\n\n# accuracy \nax[0,1].set_title('Raw VGG-16 with 20 epochs accuracy')\nax[0,1].plot(history_vgg_raw.history['accuracy'], 'b')\nax[0,1].plot(history_vgg_raw.history['val_accuracy'], 'r')\nax[0,1].legend(['train', 'test'], loc='lower right')\nax[0,1].set_ylabel('accuracy')\nax[0,1].set_xlabel('epoch')\nax[1,1].set_title('Raw VGG-16 with 200 epochs accuracy')\nax[1,1].plot(history_vgg_raw_2.history['accuracy'], 'b')\nax[1,1].plot(history_vgg_raw_2.history['val_accuracy'], 'r')\nax[1,1].legend(['train', 'test'], loc='lower right')\nax[1,1].set_ylabel('accuracy')\nax[1,1].set_xlabel('epoch')\nax[2,1].set_title('Raw VGG-16 with SGD & reduced lr accuracy')\nax[2,1].plot(history_vgg_sgd2.history['accuracy'], 'b')\nax[2,1].plot(history_vgg_sgd2.history['val_accuracy'], 'r')\nax[2,1].legend(['train', 'test'], loc='lower right')\nax[2,1].set_ylabel('accuracy')\nax[2,1].set_xlabel('epoch')\n\nplt.tight_layout()\nplt.show()","f79c690b":"# load pre-trained model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\n\n# load model without classifier layers\nvgg = VGG16(include_top=False, input_shape=(128, 128, 3), weights='imagenet', pooling='avg')\n# make only last 2 conv layers trainable\nfor layer in vgg.layers[:-4]:\n    layer.trainable = False\n# add output layer \nout_layer = Dense(1, activation='sigmoid')(vgg.layers[-1].output)\nmodel_pre_vgg = Model(vgg.input, out_layer)\n# compile model\nopt = SGD(lr=1e-5)\nmodel_pre_vgg.compile(optimizer=opt, loss=keras.losses.binary_crossentropy, metrics=['accuracy'])","3755d0e8":"# model summary\nmodel_pre_vgg.summary()","e69fe0db":"# load images in RGB-scale without normalisation\ntrain_datagen_pre_vgg = ImageDataGenerator(\n    horizontal_flip=True,\n    rotation_range=5,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    shear_range=0.05,\n    zoom_range=0.05,\n    validation_split=0.2) \n\ntrain_generator_pre_vgg = train_datagen_pre_vgg.flow_from_directory(\n    DIR,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    color_mode=\"rgb\",\n    subset='training') \n\nvalidation_generator_pre_vgg = train_datagen_pre_vgg.flow_from_directory(\n    DIR, \n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    color_mode=\"rgb\",\n    subset='validation') ","9da2de80":"# train model\nbatch_size = 16\nepochs = 200\nhistory_pre_vgg = model_pre_vgg.fit_generator(train_generator_pre_vgg, steps_per_epoch = train_generator_pre_vgg.samples \/\/ batch_size,\n        validation_data = validation_generator_pre_vgg, validation_steps = validation_generator_pre_vgg.samples \/\/ batch_size,\n        epochs=epochs, callbacks=[early])","6b1c4f16":"# evaluate model\nfig, ax = plt.subplots(3, 2, figsize=(10, 10))\n\n# loss\nax[0,0].set_title('Non pre-trained VGG-16 loss')\nax[0,0].plot(history_vgg_sgd2.history['loss'], 'b')\nax[0,0].plot(history_vgg_sgd2.history['val_loss'], 'r')\nax[0,0].legend(['train', 'test'], loc='lower right')\nax[0,0].set_ylabel('loss')\nax[0,0].set_xlabel('epoch')\nax[1,0].set_title('Pre-trained VGG-16 loss')\nax[1,0].plot(history_pre_vgg.history['loss'], 'b')\nax[1,0].plot(history_pre_vgg.history['val_loss'], 'r')\nax[1,0].legend(['train', 'test'], loc='lower right')\nax[1,0].set_ylabel('loss')\nax[1,0].set_xlabel('epoch')\nax[2,0].set_title('3-layer CNN loss')\nax[2,0].plot(history_cnn_raw.history['loss'], 'b')\nax[2,0].plot(history_cnn_raw.history['val_loss'], 'r')\nax[2,0].legend(['train', 'test'], loc='lower right')\nax[2,0].set_ylabel('loss')\nax[2,0].set_xlabel('epoch')\n\n\n# accuracy \nax[0,1].set_title('Non pre-trained VGG-16 accuracy')\nax[0,1].plot(history_vgg_sgd2.history['accuracy'], 'b')\nax[0,1].plot(history_vgg_sgd2.history['val_accuracy'], 'r')\nax[0,1].legend(['train', 'test'], loc='lower right')\nax[0,1].set_ylabel('accuracy')\nax[0,1].set_xlabel('epoch')\nax[1,1].set_title('Pre-trained VGG-16 accuracy')\nax[1,1].plot(history_pre_vgg.history['accuracy'], 'b')\nax[1,1].plot(history_pre_vgg.history['val_accuracy'], 'r')\nax[1,1].legend(['train', 'test'], loc='lower right')\nax[1,1].set_ylabel('accuracy')\nax[1,1].set_xlabel('epoch')\nax[2,1].set_title('3-layer CNN accuracy')\nax[2,1].plot(history_cnn_raw.history['accuracy'], 'b')\nax[2,1].plot(history_cnn_raw.history['val_accuracy'], 'r')\nax[2,1].legend(['train', 'test'], loc='lower right')\nax[2,1].set_ylabel('accuracy')\nax[2,1].set_xlabel('epoch')\n\n\nplt.tight_layout()\nplt.show()","7727533a":"## 8. Pre-trained VGG-16 ","a3f9e363":"### 5.2. Classification with raw images","52709ace":"### 7.3. VGG-16 with reduced learning rate","61bf6462":"## 3. Image segmentation with k-means clustering","beeeffa0":"# COVID-19 Lung CT Segmentation & Classification\nThis notebook aims to build image classifiers to determine whether a paitient is tested positive or negative for COVID-19 based on lung CT scan images. In doing so, a VGG-16 model and a 3-layer CNN model are used for classification.\n\nPrior to the classification, the images are firstly segmented using K-means Clustering to enhance classification performance. Then, the VGG-16 model and the 3-layer CNN model are implemented on the raw and segmented images. The effect of the image segmentation is discussed and two models are compared. To improve the performance of the VGG-16 model, various tuning methods including increasing epochs, changing optimiser and reducing learning rate are performed and evaluated. In addition, pre-trained weights of the VGG-16 model are implemented to enhance the model. \n\nThe dataset is sourced from 'COVID-19 Lung CT Scans' in Kaggle.\nhttps:\/\/www.kaggle.com\/luisblanche\/covidct\n\n","7d89b9a9":"## 1. Import libraries","d38531a6":"## 7. VGG-16 tuning\n","5a8c9524":"## 5. Classification using simple 3-layer CNN model","b72682d6":"### 5.1. Classification with segmented images","741facf8":"## 2. Pre-processing","d6aacd78":"### 7.2. VGG-16 with different optimiser","b26a29dc":"## 4. Classification using VGG-16 model","3115d7bc":"### 7.1. VGG-16 with increased epochs","c3a98e5c":"## 6. Comparison\n\n### Effect of image segmentation\nBy looking at the performances of the two models on both segmented images and raw images, the image segmentation allows smoother learning for both models reducing variance. And for the 3-layer CNN model, it has reached overfitting quicker with the segmented\nimages. However, the average validation accuracy for the segmented images is higher than the raw images. This is due to the reduction in image features during image segmentation. Additionally, a stright horizontal line for the validation accuracy of the VGG-16 model on the segmented images indicates that the model predicts the same classes for all iterations. This may be due to a lack of training or incorrect settings for weight initialisation or optimisation. \n\n### Comparison between deep and simple CNN models\nThe 3-layer CNN model on both types of images has reached overfitting during training. However, the VGG-16 model did not reach \noverfitting during training. This is likely due to gradient vanishing caused by its deeper architecture. Certainly 20 epochs are\nnot enough for the gradients to impact the first convolutions of the model. \n\nHence, additional epochs and different hyper-parameters are given to the VGG-16 model to enhance its learning. \n\n","af11bbae":"### 7.4. Evaluations\nVGG-16 model is tuned by increasing epochs, chaning optimiser and reducing learning rate. Purely increasing epochs to 200 did not show any effect on the model's learning. The oscillation in both loss and accuracy indicates that the model still cannot learn the image feauters. Thus, different approaches are used. Chaning the optimiser from ADAM to SDG and reducing the learning rate showed enhancement in the model's learning implied by the continuously decreasing training and testing loss. However, the reduction rate is marginal so that the accuracy is still fluctuating. Considering the size of the available dataset with about 600 images, training a deep architectural neural network such as VGG-16 may be limited. Thus, pre-trained VGG-16 model is loaded to benefit from the pre-trained weights and to examine its ability in learning the image features. ","6ec9b3a0":"### 8.1. Evaluations\n\nImplementing the pre-trained VGG-16 model significantly improved the learning ability of the model. It reaches overfitting at around 25 epochs and its accuracy ranges between 0.7 to 0.8. This is because the pre-trained model has weights that are already trained on large set of images. Only the last two convolutional layers of the model are trained in order to learn the features of the lung CT scan images. By using the pre-trained weights, the model overcame the limitation of its deep architecture and the small datasets. \n\nIn comparison to the 3-layer CNN model, the testing accuracies are similar ranging between 0.7 and 0.8. This may be due to the limitation of the learnable features from the small datasets. Hence, using a simple model may be a good decision in this case for the sake of computation time and resources. For future application, increasing dataset size by data augmentation can be looked into to fully achieve the potential of the deep architectural neural networks. ","49c87ec9":"### 4.1. Classification with segmented images","5e1ddcc4":"### 4.2. Classification with raw images"}}