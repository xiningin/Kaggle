{"cell_type":{"fb4de6b8":"code","23d8f14a":"code","238b84b1":"code","0fea1f41":"code","dc6834ff":"code","d12955f9":"code","4307bbd6":"code","b723f41d":"code","9c64b721":"code","4ca92804":"code","077edee6":"code","6849e268":"code","6423fc34":"code","aacb3628":"code","0b30d8d1":"code","d9e5b4ce":"code","aa3b3108":"code","d113ba49":"code","bbb5dab9":"code","0d48348d":"code","286f8ad2":"code","fb610c06":"code","6d8cef23":"code","25b58df0":"code","bf4fd7f7":"code","f3d4bc27":"code","fa736568":"markdown","714ccd3c":"markdown","880e3694":"markdown","7a498d98":"markdown","70af72d3":"markdown","114662d7":"markdown","1d623c6d":"markdown","f35f2afe":"markdown","c339717c":"markdown","5d8936b7":"markdown","9804cca3":"markdown","32cbcdb8":"markdown","85cf77b7":"markdown","78d15644":"markdown","dd53f9dd":"markdown","dc2200ee":"markdown","d2344aa9":"markdown","44d4fa91":"markdown","cff0d592":"markdown","94530da2":"markdown","8458c092":"markdown","d278d41e":"markdown","df069ecc":"markdown","f713f5d5":"markdown","fe270c17":"markdown","43eb4eec":"markdown","070f2f72":"markdown","0c1e2e8e":"markdown","f3d3ea48":"markdown","8ad4d497":"markdown","50b95a08":"markdown","e09a332d":"markdown","e78ed8df":"markdown","27091604":"markdown","d5e4ab58":"markdown","0d2be52b":"markdown","48547d02":"markdown"},"source":{"fb4de6b8":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport math, re, os\nimport numpy as np\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport json\nimport cv2\nimport os\nimport pandas as pd\nimport gc \nimport tifffile\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input\nfrom keras.layers.core import Dropout, Lambda\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, LayerNormalization\nfrom tensorflow.keras.layers import MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import layers\nfrom keras.engine.topology import Layer\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.utils.generic_utils import get_custom_objects\n\nclass LayerNormalization(Layer):\n\n    def call(self, x, mask=None, training=None):\n        axis = list(range(1, len(x.shape)))\n        x \/= K.std(x, axis=axis, keepdims=True) + K.epsilon()\n        x -= K.mean(x, axis=axis, keepdims=True)\n        return x\n\n    def compute_output_shape(self, input_shape):\n        return input_shape","23d8f14a":"def dice_coeff(y_true, y_pred):\n    # this formula adds epsilon to the numerator and denomincator to avoid a divide by 0 error \n    # in case a slice has no pixels set; the relative values are important, so this addition\n    # does not effect the coefficient\n    _epsilon = 10 ** -7\n    intersections = tf.reduce_sum(y_true * y_pred)\n    unions = tf.reduce_sum(y_true + y_pred)\n    dice_scores = (2.0 * intersections + _epsilon) \/ (unions + _epsilon)\n    return dice_scores\n\n\ndef dice_loss(y_true, y_pred):\n    #defined as 1 minues the dice coefficient\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\n\nget_custom_objects().update({\"dice\": dice_loss})","238b84b1":"file_list = tf.io.gfile.glob('..\/hubmap-kidney-segmentation\/train\/*.tiff')        ","0fea1f41":"def normalize(input_image):\n    image = tifffile.imread(input_image)\n    image = tf.squeeze(image)\n    print(image.shape)\n    if image.shape[0]==3:\n        image = tf.transpose(image, [2, 1, 0])\n       #image = tf.cast(image, tf.float32) \/ 255.0\n    return image, image.shape[0], image.shape[1]\n    gc.collect()\n#plt.figure(figsize=(20,10))\n#plt.imshow(image)","dc6834ff":"def convert_rle_to_image(rle_file, image_shape):\n    image_shape = (image_shape[1], image_shape[0])\n    file_string = rle_file.split()\n    # Convert strings to integers and subtract 1 because of python's 0-indexing\n    start_pixel = np.array(file_string[0::2], dtype=int) - 1\n    length = np.array(file_string[1::2], dtype=int)\n    end_pixel = start_pixel + length\n    mask = np.zeros(image_shape[0] * image_shape[1], dtype=np.uint8)\n\n    for start, end in zip(start_pixel, end_pixel):\n        mask[start:end] = 1\n\n    return mask.reshape(image_shape).T","d12955f9":"def RLE_ENCODER(mask):\n    # add 0 to start and end so that first change will be 1s \n    #mask = np.concatenate([[0],mask,[0]])\n    mask = np.append(np.insert(mask,[0],0),0)\n    #print(mask)\n    start_of_ones = (np.where(mask[1:] != mask[:-1])[0] +1)[::2] #This gives us all changes in sequence we add 1 to get index of 1s, we want every second change i.e. 0s to 1s\n    print(start_of_ones)\n    end_of_ones = (np.where(mask[1:] != mask[:-1])[0])[1::2]\n    print(end_of_ones)\n    length_of_encoding = end_of_ones - start_of_ones +1\n    return ' '.join([(str(item1)+\" \"+str(item2)) for item1,item2 in zip(start_of_ones,length_of_encoding)])\n    ","4307bbd6":"def _float_feature(value):\n  \"\"\"Returns a float_list from a float \/ double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n# For the mask\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef serialiaze_images(image_id, image, mask, tile_no, start_row, start_col, image_dist0, image_dist1):\n    \"\"\"\n    Creates a tf.train.Example message ready to be written to a file.\n    \"\"\"\n    image = image.numpy()\n    image_bytes = image.tobytes()\n    mask_bytes = mask.tobytes()\n    feature_dict = {\n        'image': _bytes_feature(image_bytes),\n        'mask': _bytes_feature(mask_bytes),\n        'tile_No': _int64_feature(tile_no),\n        'image_id': _bytes_feature(image_id),\n        'start_row_pixel': _int64_feature(start_row),\n        'start_col_pixel': _int64_feature(start_col),\n        'image_distribution0': _bytes_feature(image_dist0),\n        'image_distribution1': _bytes_feature(image_dist1)\n\n    }\n\n    # Create a Features message using tf.train.Example.\n    message_feature = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n    return message_feature.SerializeToString()","b723f41d":"def create_tf_record(image_id,image,mask, tile_size,tf_record_filename):\n    tile_no = 0\n    \n    # Num_tile_cols \n    num_tile_rows =  image.shape[0] \/\/ tile_size\n    num_tile_cols =  image.shape[1] \/\/ tile_size\n    \n    compress = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n    with tf.io.TFRecordWriter(tf_record_filename, compress) as writer:\n    \n        for row in range(num_tile_rows):\n            for col in range(num_tile_cols):\n                #print(tile_no)\n                start_row = row*tile_size\n                start_col = col*tile_size\n                image_tile = image[start_row:start_row+tile_size, start_col:start_col+tile_size]\n                image_dist = np.histogram(image_tile)\n                image_dist0 = image_dist[0].tobytes()\n                image_dist1 = image_dist[1].tobytes()\n                mask_tile = mask[start_row:start_row+tile_size, start_col:start_col+tile_size]\n            \n                message = serialiaze_images(image_id, image_tile, mask_tile, tile_no, start_row, start_col, image_dist0, image_dist1)\n                writer.write(message)\n                tile_no = tile_no + 1\n    writer.close()\n    tile_count = tile_no\n    return tile_count","9c64b721":"def generate_tf_records(tile_size, file_list):\n    image_list = []\n    for file_name in file_list:\n        image, shape0, shape1 = normalize(file_name)\n        image_name = Path(file_name).stem\n        image_id = bytes(image_name, 'utf8')\n        tf_record_filename = 'train+'+image_name+'_'+str(tile_size)+'.tfrecords'\n        mask = convert_rle_to_image(df_train[df_train[\"id\"] == image_name][\"encoding\"].values[0], (shape0,shape1))\n        tile_count = create_tf_record(image_id, image, mask, tile_size, tf_record_filename)\n        image_list.append((image_id,tile_count))\n        gc.collect()\n    return image_list","4ca92804":"file_list = tf.io.gfile.glob('..\/hubmap-kidney-segmentation\/train\/*.tiff')        \n\ndf_image_details = pd.DataFrame(generate_tf_records(512,file_list),columns=[\"Image_id\", \"Tile_count\"])","077edee6":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)","6849e268":"print(\"Tensorflow version \" + tf.__version__)\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","6423fc34":"filenames = tf.io.gfile.glob(\"gs:\/\/dsi_module_4_mm\/*.tfrecords\")\nfilenames","aacb3628":"train_data = np.array(filenames[0:6])\nvalidation_data = np.array(filenames[6:9])\ntrain_data","0b30d8d1":"validation_data","d9e5b4ce":"image_feature_description = {\n      'image': tf.io.FixedLenFeature([], tf.string),\n      'mask': tf.io.FixedLenFeature([], tf.string),\n      'tile_No': tf.io.FixedLenFeature([], tf.int64),\n      'image_id': tf.io.FixedLenFeature([], tf.string),\n      'start_row_pixel': tf.io.FixedLenFeature([], tf.int64),\n      'start_col_pixel': tf.io.FixedLenFeature([], tf.int64),\n      'image_distribution0': tf.io.FixedLenFeature([], tf.string),\n      'image_distribution1': tf.io.FixedLenFeature([], tf.string)\n      \n  }\n\ndef _parse_image_function2(example_proto):\n  # Parse the input tf.Example proto using the dictionary above.\n    single_example = tf.io.parse_single_example(example_proto, image_feature_description)\n    \n    image_id = single_example['image_id']\n    start_row_pixel = single_example['start_row_pixel']\n    start_col_pixel = single_example['start_col_pixel']\n    \n    num_channels = 3\n    tile_size = 512\n        \n    \n    image =  tf.io.decode_raw(single_example['image'],out_type='uint8')\n   \n    #img_array = tf.reshape( image, ( 1, tile_size, tile_size, num_channels))\n    img_array = tf.reshape( image, (  tile_size, tile_size, num_channels))\n    \n    img_array = tf.cast(img_array, tf.float32) \/ 255.0\n   \n    mask =  tf.io.decode_raw(single_example['mask'],out_type='bool')\n    \n    mask = tf.reshape(mask, (tile_size,tile_size))\n    \n    mask = tf.cast(mask,tf.float32)\n    \n    image_distribution0 = tf.io.decode_raw(single_example['image_distribution0'], out_type = 'int64')\n    image_distribution1 = tf.io.decode_raw(single_example['image_distribution1'], out_type = 'int64')\n    \n    image_distribution = (image_distribution0, image_distribution1)\n    \n    mtd = dict()\n    mtd['img_index'] = single_example['image_id']\n    mtd['tile_id'] = single_example['tile_No']\n    mtd['start_col_pixel'] = single_example['start_col_pixel']\n    mtd['start_row_pixel'] = single_example['start_row_pixel']\n    struct = {\n        'img_array': img_array,\n        'mask': mask,\n        'mtd': mtd,\n        'image_distribution': image_distribution,\n    } \n    return img_array, mask","aa3b3108":"### A second version of the function that manipulates the image dimensions\ndef _parse_image_function2(example_proto):\n    # Parse the input tf.Example protocol. The image_feature_description dictionary must be loaded!\n    single_example = tf.io.parse_single_example(example_proto, image_feature_description)\n    \n    #the keys ultimtaley refer to the image_feature_description dictionary\n    image_id = single_example['image_id']\n    start_row_pixel = single_example['start_row_pixel']\n    start_col_pixel = single_example['start_col_pixel']\n\n    num_channels = 3\n    tile_size = 512\n\n    image = tf.io.decode_raw(single_example['image'], out_type='uint8')\n\n    img_array = tf.reshape(image, (tile_size, tile_size, num_channels))\n    img_array = img_array[None,:,:,:] #needs to be 4-dimensional for the model\n    img_array = tf.cast(img_array, tf.float32) \/ 255.0\n\n    mask = tf.io.decode_raw(single_example['mask'], out_type='bool')\n    mask = tf.reshape(mask, (tile_size, tile_size))\n    mask = tf.cast(mask, tf.float32) #the model cannot take in bool, so must cast to float\n\n    image_distribution0 = tf.io.decode_raw(single_example['image_distribution0'], out_type='int64')\n    image_distribution1 = tf.io.decode_raw(single_example['image_distribution1'], out_type='int64')\n\n    image_distribution = (image_distribution0, image_distribution1)\n\n    mtd = dict()\n    mtd['img_index'] = single_example['image_id']\n    mtd['tile_id'] = single_example['tile_No']\n    mtd['start_col_pixel'] = single_example['start_col_pixel']\n    mtd['start_row_pixel'] = single_example['start_row_pixel']\n    struct = {\n        'img_array': img_array,\n        'mask': mask,\n        'mtd': mtd,s\n        'image_distribution': image_distribution,\n    }\n    return img_array, mask #, image_distribution0","d113ba49":"def read_tf_dataset2(storage_file_path):\n    encoded_image_dataset = tf.data.TFRecordDataset(storage_file_path, compression_type=\"GZIP\")\n    parsed_image_dataset = encoded_image_dataset.map(_parse_image_function2)\n    return parsed_image_dataset","bbb5dab9":"import tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","0d48348d":"with strategy.scope():\n    def unet_model(OUTPUT_CHANNELS=1, tile_size=512, strides=1):\n        initializer = 'he_normal'\n        #keras.initializers.HeNormal()\n\n        inputs = layers.Input(shape=[tile_size, tile_size, 3])\n\n        ###LEVEL1\n        d_conv1 = layers.Conv2D(filters=64, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(inputs)\n        norm1 = layers.BatchNormalization()(d_conv1)                         \n        d_conv2 = layers.Conv2D(filters=64, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(norm1)                             \n        d_pool1 = layers.MaxPool2D(pool_size=2, strides=2, padding ='same')(d_conv2)\n        ###LEVEL2\n        d_conv3 = layers.Conv2D(filters=128, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_pool1)\n        norm2 = layers.BatchNormalization()(d_conv3)                             \n        d_conv4 = layers.Conv2D(filters=128, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(norm2)\n        d_pool2 = layers.MaxPool2D(pool_size=2, strides=2, padding='same')(d_conv4)\n        ##LEVEL3\n        d_conv5 = layers.Conv2D(filters=256, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_pool2)\n        d_conv6 = layers.Conv2D(filters=256, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_conv5)\n        d_pool3 = layers.MaxPool2D(pool_size=2, strides=2, padding='same')(d_conv6)\n        ###LEVEL4\n        d_conv7 = layers.Conv2D(filters=512, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_pool3)\n        d_conv8 = layers.Conv2D(filters=512, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_conv7)\n        d_pool4 = layers.MaxPool2D(pool_size=2, strides=2, padding='same')(d_conv8)\n        ###LEVEL5\n        d_conv9 = layers.Conv2D(filters=1024, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_pool4)\n        d_conv10 = layers.Conv2D(filters=1024, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(d_conv9)    \n\n        ###Upsampling\n        ###LEVEL1\n        u_sample1 = layers.UpSampling2D(size=2)(d_conv10)\n        u_conv11 = layers.Conv2D(filters=512, kernel_size=2, strides=strides, padding='same',\n                                 kernel_initializer=initializer)(u_sample1)\n        u_copy1 = layers.Concatenate(axis = 3)([d_conv8, u_conv11])\n\n        u_conv12 = layers.Conv2D(filters=512, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_copy1)\n        u_conv13 = layers.Conv2D(filters=512, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_conv12)\n        ###Level2\n\n        u_sample2 = layers.UpSampling2D(size=2)(u_conv13)\n        u_conv14 = layers.Conv2D(filters=256, kernel_size=2, strides=strides, padding='same',\n                                 kernel_initializer=initializer)(u_sample2)\n        u_copy2 = layers.Concatenate(axis = 3)([d_conv6, u_conv14])\n        u_conv15 = layers.Conv2D(filters=256, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_copy2)\n        u_conv16 = layers.Conv2D(filters=256, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_conv15)\n\n\n        ###Level3\n\n        u_sample3 = layers.UpSampling2D(size=2)(u_conv16)\n        u_conv17 = layers.Conv2D(filters=128, kernel_size=2, strides=strides, padding='same',\n                                 kernel_initializer=initializer)(u_sample3)\n        u_copy3 = layers.Concatenate(axis = 3)([d_conv4, u_conv17])\n        u_conv18 = layers.Conv2D(filters=128, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_copy3)\n\n        u_conv19 = layers.Conv2D(filters=128, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_conv18)\n\n        ###Level4\n\n        u_sample4 = layers.UpSampling2D(size=2)(u_conv19)\n        u_conv20 = layers.Conv2D(filters=64, kernel_size=2, strides=strides, padding='same',\n                                 kernel_initializer=initializer)(u_sample4)\n        u_copy4 = layers.Concatenate(axis = 3)([d_conv2, u_conv20])\n        u_conv21 = layers.Conv2D(filters=64, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_copy4)\n        u_conv22 = layers.Conv2D(filters=64, kernel_size=3, strides=strides, padding='same',\n                                 kernel_initializer=initializer, activation ='relu')(u_conv21)\n        ###Last Level\n        u_conv23 = layers.Conv2D(filters =1, kernel_size = OUTPUT_CHANNELS, activation='sigmoid')(u_conv22)\n\n        return keras.Model(inputs=inputs, outputs=u_conv23)","286f8ad2":"with strategy.scope():   \n    model = unet_model()\n    model.summary()\n    model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=0.001),\n              loss=dice_loss,\n              metrics=[dice_coeff])","fb610c06":"with strategy.scope():\n    ACCELERATOR_TYPE = 'TPU'\n    if ACCELERATOR_TYPE == 'TPU':\n        batch_size = 128\n        train_dataset = read_tf_dataset2(train_data) \n        train_dataset = train_dataset.batch(batch_size, drop_remainder=True).cache().prefetch(2)\n        validation_dataset = read_tf_dataset2(validation_data)\n        validation_dataset = validation_dataset.batch(batch_size, drop_remainder=True).prefetch(2)\n        steps_per_epoch = 100\n        checkpointer = ModelCheckpoint('\/kaggle\/working\/unet-tpu.h5', verbose=1)\n        history = model.fit(train_dataset,batch_size=batch_size,validation_data=validation_dataset, epochs=5,callbacks=[checkpointer])\n        model.save_weights(\"\/kaggle\/working\/hubmap-tpu-unetf.h5\")\n        model.save(\"\/kaggle\/working\/hubmap-tpu-unet_model.h5\")\n    ","6d8cef23":"history","25b58df0":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['dice_coeff', 'val_dice_coeff']].plot();","bf4fd7f7":"history_frame","f3d4bc27":"history_frame.to_csv('model_performance')","fa736568":"**RLE decoder function**: ","714ccd3c":"This plots  a graph comparing training to validation","880e3694":"We now need to read in the TF records.\n\nSteps:\n\n1. Create a dictionary mapping the features of the messages in the TFrecord to a label and specifying the data type\n2. Creat a function to decode the messages into images and masks\n3. Uncompress the TFrecord and pass it to the decode function ","7a498d98":"The code below is used to package all  steps into a single function which will generate a separate TFrecord file for.","70af72d3":"## Introduction\n\n**Image segmentation** is the process of separating a digital image into specific partitions by creating a pixel-wise mask for an object(s) of interest. Pixels are labelled if they share a certain shared characteristic. Segmentation creates a representation of an image that is easier and more meaningful to analyse.     \n\n**Problem example**: An example of a image segmentation task is a recent [Kaggle challenge](https:\/\/www.kaggle.com\/c\/hubmap-kidney-segmentation) where a structure in the kidney called the glomeruli, cells and capillaries that facilitate the filtration of waste products (100-350 \u03bcm in diameter; spherical shape), needed to be identified in unlablled images (on a pixel level). The functional tissue unit (FTU) that needs to be identified is a block of cells around a capillary (a 3D sphere).    \n\n**Objective of Notebook**: This notebook will walk you through a pipeline using **TF records** and the convolutional neural network **U-Net** architecture for biomedical image segmentation (also see this [academic article](https:\/\/arxiv.org\/pdf\/1505.04597.pdf)) using Python with tensorflow and keras.   \n","114662d7":"The first step is to have a look at the image arrays to get an idea of:\n1. The image shapes\n2. The image formats\n\nSteps:\n1. Look for all the tiff files in the training set.\n2. Read the images into arrays using the tifffile library.\n3. Print out the image shapes to see if there is consistency.\n4. Normalise\/Standardise the images so that they are in the same format.\n5. Convert the image masks into image arrays (0s and 1s) of the same size as their associated images.\n6. Pick an image size to subsample the image (The image tile chosen must have equal dimensions and be a multiple of the number of filters in the convolutional kernals).\n7. Convert the images into a TFrecord (steps outlined later)\n","1d623c6d":"# We tackle this problem into two general steps\n \n1. Preparing the images for processinng (Normalising --> Tiling --> Serialising)\n2. Building a CNN and reading the serialized images (Model Definition --> Model Compilation --> Model Fitting-Model Tuning)","f35f2afe":"This code is used to read all the tiff image file paths in the training set and put it in a list","c339717c":"We then fit the model to the data set using a batch size of 128 and 30 epochs. ","5d8936b7":"We now define the model within a with strategy.scope(): block to ensure the TPU is used. The model designed is very similar to the one in the [paper](https:\/\/arxiv.org\/pdf\/1505.04597.pdf). The differences being that a few normalisation layers were added to the downsampling block and we used strides of 1 for the convolution layers and different size input images.","9804cca3":"This function is used to encode an image into its run length","32cbcdb8":"We now compile the model","85cf77b7":"## Evaluation Metric: Dice Coefficient\nThe dice coefficient is used to compare the pixel-wise agreement between a predicted segmentation and the true value (ground truth) and is defined as 2 times the area of overlap between the predicted and actual value divided by the total number of pixels in both images ([see more](https:\/\/towardsdatascience.com\/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2)).    \nHere is the formula: $\\Large \\frac{2*|X \u2229 Y|}{|X|+|Y|}$    \nAnd here is its visualisation: ![Dice](https:\/\/miro.medium.com\/max\/858\/1*yUd5ckecHjWZf6hGrdlwzA.png)    ","78d15644":"We noticed that the colour channel sometimes occurred in the first column and sometimes in the third. Some images allso had leading dimensions of size 1 which we remove using the squeeze function. We run the garbage collector to free up memory space after calling the function","dd53f9dd":"## Generating a TFRecord\n \nA Tensorflow record file consists of serialised messages which is a dictionary of a feature label and its associated value.\nTo convert images into TFrecord files we utilize the protocol tensorflow.train.Example\n\nSteps 1:\n1. Break down the image into smaller images (tiling)\n2. Create helper functions to cast datatypes into 1 of the type lists (integer,float and bytes)\n3. Create a feature dictionary which will be the contents of message. This is how we associate the image to the mask\n4. Convert the features into to bytes, a process called serialization\n5. Add the features to a message\n6. Create a tfrecord file and write the messages (image and its associated features) to it\n","dc2200ee":"## Acknowlegements\nThis workflow is based on the below sources, mainly by reading other Kaggle Kernals, in particular the Notebooks of the Hacking the Kidney coach Marcos Novaes' notebook series.\n- https:\/\/www.kaggle.com\/marcosnovaes\/hubmap-3-unet-models-with-keras-cpu-gpu\/   \n- https:\/\/www.kaggle.com\/marcosnovaes\/hubmap-unet-keras-model-fit-with-tpu","d2344aa9":"## Libraries to import","44d4fa91":"**Creating the TF record**:","cff0d592":"You can now add a TPU by setting the accelerator to TPU v#-#\n![image.png](attachment:image.png)","94530da2":"## Data\nThe [data set](https:\/\/www.kaggle.com\/c\/hubmap-kidney-segmentation\/data) contains 8 training and 5 test images as TIFF files. The images are stained (with Periodic acid Schiff stain) histology tissue sections of the kidney. The training images come with an associated mask that identifies the areas of interest that can be accessed as in both an unencoded JSON form and as a run-length encoded (RLE) form from a CSV file, which stores a sequence of data in a single value. Additional information is also available for each image such as demographic information.    \n\n### Encoding\nRun Length Encoding (RLE) is a lossless compression format. Here pixels are numbered in 1D first from top to bottom (row-wise) then from left to right.  Here the pixel locations (label) of an object are represented by two numbers. The first number refers to the starting pixel (in 1D) and the second number is the number of successive pixels that object is present on. So for example if an image has an object with pixels labelled at (789, 790, 791,900, 901, 904, 906). THe run length encoding would be 789 3 900 2 904 1 906 1.\n\nAs can be seen from the above example the RLE will only be smaller than the original image if the object of interest is dense (i.e. connected consecutively).\n\nFor this problem there is only 1 object for detection so glomeruli pixels so binary classification is used where 1 represents the presence of a glomeruli and 0 not a glomeruli.\n\n### Images\n\nThe image files are large (1-2GB) and difficult to train (computationally intensive). As a general rule of thumb many smaller images are faster to train and process than one large image of the same disk size. So the strategy is to first sub-sample the large images into smaller images before passing them on to a CNN. \n\n","8458c092":"We now call the above function which will generate tfrecord files.\nWe define a pandas DataFrame to store the image names along with the number of tiles, this is helpful in getting an idea of how many images can be trained on. ","d278d41e":"A TPU is a Tensor Processing Unit it, it has a built in distribution strategy with 8 cores. In order to use a TPU, the datasets need to be stored on Google Cloud Storage; Google Colab also has TPUs but TFrecords are not supported. In order to use the TPU functionality on Kaggle, the Tfrecords need to be uploaded to Google Cloud Storage.\n\nSteps\n1. Create a bucket on Google Cloud Storage (GCS)\n2. Link GCS drive to Kaggle Notebook (Add-ons --> Google Cloud SDK)\n3. Set the Accelarator to TPU on the Notebook\n4. Use the Secret Keys\n5. Reference the GCS bucket","df069ecc":"We first ran the training model without a validation dataset, to get an idea of the training times. The below information summarises this:","f713f5d5":"Run the code below to see ther tensorflow version as well as if the TPU is infact running","fe270c17":"## Abstract\n\nWe are presented with images of kidney tissue. These images are supplemented with a CSV file where for each image the pixel location of glomeruli cells (spherical capsules) are indicated. The task is to take the image files as well as the pixel location of the glomeruli (referred to as masks) and train a machine learning model to find the pixel locations of glomeruli on an unlabelled data set of kidney images. \n\nThis problem is an image segmentation type problem and can be solved using an encoder-decoder architecture, where a CNN is first downsampled to extract features and then subsequently symmetrically upsampled to reproduce the image and identify the pixels containing glomeruli.\n\n### In this document we show:\n1. How to sub-sample a large image into smaller images, a process referred to as tiling.\n2. How to associate pixel lables (masks) to the image\n3. How to convert images into TensorFlow record files (TFrecord) to save on storage space as well as faster processing\n4. How to read a TFrecord\n5. How to construct a class of encoder-decoder models (UNet type model)\n6. How to use a TPU for faster machine learning\n7. How to train a UNet type model","43eb4eec":"We now split the training data into a training set and validation set. We did not use all the images as we the number of files was quite large and we wanted to first generate a training model and see what the performance is like before loading all the images.\n\n![image.png](attachment:image.png)","070f2f72":"## Data Processing: TF Records\nThe Tensorflow record is a format to store a sequence of binary record from large datasets. A major advantage of this format is the datasets that are too large to be stored fully in memory can be loaded in batches from the disk and processed ([see more](https:\/\/towardsdatascience.com\/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2)).  ","0c1e2e8e":"Reading the TF record:","f3d3ea48":"## Using TPU from Kaggle","8ad4d497":"Epoch 1\/30\n    119\/Unknown - 167s 1s\/step - dice_coeff: 0.2725 - loss: 0.7275\nEpoch 00001: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 169s 1s\/step - dice_coeff: 0.2725 - loss: 0.7275\nEpoch 2\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00002: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 886ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 3\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00003: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 4\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00004: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 5\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00005: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 6\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00006: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 7\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00007: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 8\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00008: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 9\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00009: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 106s 888ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 10\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00010: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 106s 887ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 11\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00011: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 12\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00012: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 13\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00013: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 14\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00014: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 15\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00015: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 16\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00016: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 17\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00017: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 18\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00018: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 19\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00019: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 20\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00020: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 21\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00021: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 22\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00022: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 23\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00023: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 24\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00024: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 25\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00025: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 26\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00026: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 27\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00027: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 105s 885ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 28\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00028: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 106s 888ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 29\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00029: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 106s 887ms\/step - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 30\/30\n119\/119 [==============================] - ETA: 0s - dice_coeff: 0.2920 - loss: 0.7080\nEpoch 00030: saving model to \/kaggle\/working\/unet-tpu.h5\n119\/119 [==============================] - 106s 888ms\/step - dice_coeff: 0.2920 - loss: 0.7080\n","50b95a08":"# **Image segmentation using Unet architecture and TFrecords**\n### Authors: Muhammad Valiallah and Martin Page","e09a332d":"We now have a function that reads a TFrecord and can be used to generate images and masks that can be passed to a CNN","e78ed8df":"**Normalisation function:** ","27091604":"1. Link Notebook to GCS\n![image.png](attachment:image.png)\n2. Login to your GCS account\n![image.png](attachment:image.png)\n3. You will then be provided with the code snippet below\n","d5e4ab58":"## U-Net Model\n\nThe UNet architecture was proposed in 2015. It was based off a similar architecture called FCN, but it has no dense layer.\n\nThe U-Net architecture uses a contracting path and an expansive path (encoder-decoder), which thus give the U-shape. The contracting path is a typical convolutional network with repeated application of convolutions with ReLU and max pooling activations (downsampling operations). The contracting pathways reduce the spatial information while increasing feature information, thus the resolution of the output is increased (at each step the number of features is doubled). The expansive pathway combines the feature and spatial information through a sequence of up-convolutions (down sampling that halves the number of feature channels) and concatenations with the high-resolution features from the contracting pathway ([see more](https:\/\/www.kaggle.com\/prvnkmr\/unet-architecture-breakdown)).   \nThe basic architecture is:  ![UNet](https:\/\/www.researchgate.net\/profile\/Alan_Jackson9\/publication\/323597886\/figure\/fig2\/AS:601386504957959@1520393124691\/Convolutional-neural-network-CNN-architecture-based-on-UNET-Ronneberger-et-al.png)\n\nTo summarize the architecture:\n1. 23 Convolution operations\n2. Downsampling operations (Conv(3x3)-->Relu-->Conv(3x3)-->Relu-->MaxPooling\n3. Upsampling operations (Upsampling-->Conv(2x2)-->Concatenation-->Conv(3x3)-->Relu-->Conv(3x3)--Relu)\n4. Last Level (Conv(1x1)-->2D softmax function which is the sigmoid function)\n5. The authors of the paper recommended initailising the kernal weights to have a Gaussian Normal Distribution due to the repeated Convultions and Pooling layers\n6. No padding in the layers\n\n","0d2be52b":"**Serialisation function and its helpers:**","48547d02":"We created tfrecords for all of the training image files. These records can be accessed by referring to the global storage path"}}