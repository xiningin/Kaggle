{"cell_type":{"3a726b31":"code","e9d98265":"code","216ef6e5":"code","d1bc5d80":"code","fee1615d":"code","50637140":"code","d6d39204":"code","ade2eb3f":"code","485202d4":"code","94b4e42e":"code","d02e819a":"code","53310fe8":"code","4552f108":"markdown","9c0f69fd":"markdown","1bb1d33f":"markdown","3db022f2":"markdown","0f001f3f":"markdown","e69d29c5":"markdown","4f29aca2":"markdown","b5afa248":"markdown","00367048":"markdown","78a262c5":"markdown","a4f062c9":"markdown"},"source":{"3a726b31":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9d98265":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict","216ef6e5":"class DatasetMNIST(torch.utils.data.Dataset):\n    \n    def __init__(self, file_path, transform=None):\n        self.data = pd.read_csv(file_path)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        image = self.data.iloc[index, 1:].values.astype(np.uint8).reshape((28, 28, 1))\n        label = self.data.iloc[index, 0]\n        \n        if self.transform is not None:\n            #print(\"---\")\n            image = self.transform(image)\n            \n        return image, label","d1bc5d80":"lRate   = 1.0\nlGamma  = 0.7\nlEpochs = 1#5\nlog_interval = 100","fee1615d":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.net = nn.Sequential( OrderedDict([\n            ('conv1', nn.Conv2d( 1,  8, 3, 2) ),\n            ('relu1', nn.ReLU() ),\n            ('conv2', nn.Conv2d( 8,  16, 3, 2) ),\n            ('relu2', nn.ReLU() ),\n            ('conv3', nn.Conv2d( 16,  32, 3, 2) ),\n            ('relu3', nn.ReLU() ),\n            #('maxpl', nn.MaxPool2d( 2 ) ),\n            ('flatt', nn.Flatten() ),\n            ('fc3'  , nn.Linear( 128, 10) ),\n            #('relu3', nn.ReLU() ),\n            #('fc4'  , nn.Linear( 64, 10) ),\n            #('relu4', nn.ReLU() ),\n        ]))\n        self.firstRun= 1;\n    \n    def forward(self, x):\n        for layer in self.net:\n            if self.firstRun == 1:\n                print( x.shape )\n            x = layer(x)\n        output = F.log_softmax(x, dim=1)\n        self.firstRun= 0;\n        return output","50637140":"device = torch.device(\"cpu\")\nthe_kwargs = {'num_workers': 1, 'shuffle': True, }\nthe_transform = transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,)),\n                       ])\ntrain_loader = torch.utils.data.DataLoader(\n        DatasetMNIST('..\/input\/mnist-in-csv\/mnist_train.csv', the_transform),\n        batch_size=64, **the_kwargs)\n\n\ntest_set = DatasetMNIST('..\/input\/mnist-in-csv\/mnist_test.csv', the_transform)\n\ntest_loader = torch.utils.data.DataLoader(\n        test_set, batch_size=1000, **the_kwargs)","d6d39204":"model = Net().to(device)\noptimizer = optim.Adadelta(model.parameters(), lr=lRate)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=lGamma )","ade2eb3f":"def train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx \/ len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss \/= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct \/ len(test_loader.dataset)))","485202d4":"for epoch in range(1, lEpochs + 1):\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)\n    scheduler.step()","94b4e42e":"(smplStd,ansStd) = next(iter(test_loader))\nlst  = F.nll_loss( model(smplStd), ansStd, reduction='none' )\nbads = smplStd[ torch.nonzero( lst>1 ) ]\nbads = bads.reshape( (-1, 1, 28, 28) )\nbedA = model(bads.reshape(-1,1,28,28))\nabc = torch.argmax(bedA, dim=1)","d02e819a":"for j in range (2):\n    for i in range (8):\n      if bads.size()[0] > i +j*8 :\n        plt.subplot(2,8, i +j*8+1)\n        plt.imshow( bads[i +j*8].reshape(28,28), cmap=\"gray_r\")\n        bedaE = enumerate( bedA[i +j*8] )\n        print( ansStd[ torch.nonzero( lst>1 ) ][i +j*8], abc[i +j*8] ) ","53310fe8":"test= test_set.data\ntotal_samples = test.shape[0]\nprint(test.shape)\n#test.head()\ntest = test.values.reshape(-1,(28*28 +1)) [:,1:].reshape(-1,1,28,28)\nmodel.eval()\ny_pred_test = model( torch.Tensor(test))\nprediction = np.argmax(y_pred_test.detach().numpy(), axis = 1)\n\n# create submission DataFrame\nsubmission = pd.DataFrame({'ImageId' : range(1, total_samples+1), 'Label' : list(prediction)})\nsubmission.head(10)\nprint (submission.shape)\nsubmission.to_csv(\"submission.csv\",index=False)","4552f108":"# Show Some Mistakes","9c0f69fd":"# Basic Imports","1bb1d33f":"# Result Export\nfrom https:\/\/www.kaggle.com\/furkanuysl\/digit-recognizer-cnn-99","3db022f2":"# Sample Mistakes","0f001f3f":"# Loaders","e69d29c5":"# Learning","4f29aca2":"# Prepare Learning","b5afa248":"# Net","00367048":"Setup key parameters","78a262c5":"# Main Scripts","a4f062c9":"# Dataset class\nFrom https:\/\/www.kaggle.com\/pinocookie\/pytorch-dataset-and-dataloader\n\nload image as ndarray type (Height * Width * Channels)\nbe carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\nin this example, i don't use ToTensor() method of torchvision.transforms\nso you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)"}}