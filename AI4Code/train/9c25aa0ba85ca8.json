{"cell_type":{"bc6207d2":"code","6d756fa1":"code","1e0d11b6":"code","57f51e88":"code","224ccc9e":"code","fd6f7af8":"code","9337c96a":"code","c91fc6eb":"code","d6981b79":"code","75fe5e39":"code","c0303d2a":"code","32b8d129":"markdown","ed6307e3":"markdown"},"source":{"bc6207d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6d756fa1":"import os\nimport pprint\nimport numpy as np\nimport pandas as pd\nfrom collections import OrderedDict\nimport random\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import weight_dict","1e0d11b6":"#make a word list of common adjectives in data-set\n\nwords_dict = {\"Uncurable\":-1,\"love\":1,\"good\":1,\"awesome\":1,\"nice\":1,\"good quality\":1,\"classic\":1,\"pretty\":1,\"seasoned\":1,\"lovely\":1,\"privileged\":1,\"attentive\":1,\"friendly\":1,\"modern\":1,\"exceptional\":1,\"enthusiastic\":1,\"famous\":1,\"prompt\":1,\"special\":1,\"unbelievable\":1,\"courteous\":1,\"delightful\":1,\"efficient\":1,\"inexpensive\":1,\"great\":1,\"pleasant\":1,\"fresh\":1,\"cool\":1,\"refresh\":1,\"positive\":1,\"beautiful\":1,\"wonderful\":1,\"perfect\":1,\"best\":1,\"amazing\":1,\"excellent\":1,\"impressive\":1,\"impressed\":1,\"pleased\":1,\"overwhelmed\":1,\"negative\":-1,\"mean\":-1,\"bad\":-1,\"sad\":-1,\"poor\":-1,\"frustrated\":-1,\"low\":-1,\"worse\":-1,\"worst\":-1,\"horrible\":-1,\"cheap\":-1,\"ridiculous\":-1,\"overpriced\":-1,\"costly\":-1,\"pneumatic\":-1,\"strange\":-1,\"unprofessional\":-1,\"nasty\":-1,\"late\":-1,\"low quality\":-1,\"bad quality\":-1,\"disappointed\":-1,\"disappointing\":-1,\"angry\":-1}\n\n","57f51e88":"#extract common adjectives from the different data sets \n\ndef get_adjectives(text):\n    blob = TextBlob(text)\n    adjectives = list()\n    for word, tag in blob.tags:\n        if tag == 'JJ':\n            adjectives.append(word.lower())\n    return set(adjectives)","224ccc9e":"#get the unique attribute from is dataset and map them based on similarity and buid a bipartite sentiment graph\n\ndef get_unique_attributes(file):\n    result = {}\n    unique_attr = {}\n\n    with open(file,'r') as file:\n\n        for i, line in enumerate(file):\n\n            result[i] = set()\n            words = line.split()\n\n            for w, word  in enumerate (map (lambda word :  word.lower().replace(\".\",\"\"), words)):\n                if word in words_dict:\n                    if 'no' == words[w-1].lower() or 'not' == words[w-1].lower():\n                        if 'not ' + word in unique_attr:\n                            unique_attr['not ' + word] +=  -1* words_dict[word]\n                        else:\n                            unique_attr['not ' + word] =  -1* words_dict[word]\n                    else:\n                        if word in unique_attr:\n                            unique_attr[word]+= words_dict[word]\n                        else:\n                            unique_attr[word]= words_dict[word]\n    return unique_attr","fd6f7af8":"def PercentageMissin(Dataset):\n    #\"\"\"this function will return the percentage of missing values in a dataset \"\"\"\n    if isinstance(Dataset,pd.DataFrame):\n        adict={} #a dictionary conatin keys columns names and values percentage of missin value in the columns\n        for col in Dataset.columns:\n            adict[col]=(np.count_nonzero(Dataset[col].isnull())*100)\/len(Dataset[col])\n\n        return pd.DataFrame(adict,index=['% of missing'],columns=adict.keys())\n    else:\n        raise TypeError(\"can only be used with panda dataframe\")\n\nif __name__ == \"__main__\":\n\n    file1 = \"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/25621281691205eb015383cbac839182b838514f.json\"\n    file2 = \"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/7db22f7f81977109d493a0edf8ed75562648e839.json\"\n    file3 = \"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/6c3e1a43f0e199876d4bd9ff787e1911fd5cfaa6.json\"\n    file4 = \"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/2ce201c2ba233a562ee605a9aa12d2719cfa2beb.json\"\n    file5 = \"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/b460e5b511b4e2c3233f9476cd4e0616d6f405ac.json\"\n\n    files = [file1,file2,file3,file4,file5]\n    fnames = []\n\n    file_results = OrderedDict()\n    all_unique_attributes = set()\n\n    for f in files:\n        unique_attributes = get_unique_attributes(f)\n        file_results[os.path.basename(f)] = unique_attributes","9337c96a":"print('##################################### File wise attributes - start #################')\npp = pprint.PrettyPrinter()\npp.pprint(file_results)\n\n","c91fc6eb":"print('##################################### File wise attributes - end #################')\n\nfor res in file_results.values():\n    all_unique_attributes.update(res)\nprint('')\n\n\n\n\n","d6981b79":"print('##################################### Unique attributes amongst all reviews #################')\npp.pprint(all_unique_attributes)\n\n    ","75fe5e39":"B = nx.Graph()\nB.add_nodes_from(file_results.keys(), bipartite=0)\nB.add_nodes_from(all_unique_attributes, bipartite=1)\n\nall_edges = []\nfor file, weight_dict in file_results.items():\n    for attribute, weight in weight_dict.items():\n        all_edges.append((file,attribute,weight))\n        B.add_weighted_edges_from(all_edges)\n\n    print(B.edges(data=True))\n\n   ","c0303d2a":"pos = {node:[0, i] for i,node in enumerate(file_results.keys())}\npos.update({node:[1, i] for i,node in enumerate(all_unique_attributes)})\nnx.draw(B, pos, with_labels=False)\nfor p in pos:  # raise text positions\n    pos[p][1] += 0.25\n    nx.draw_networkx_labels(B, pos)\n\n    plt.show()","32b8d129":"**#The cardinality of words used in different papers are shown in above graph.\n#'efficient and positive, low and negative words are most popular among these five papers, however considering all dataset will bring a different picture.'****","ed6307e3":"**In this code, I have taken only top five json of covid file to find the relationship among the adjectives used in different paper of COVID19 research. ABSG method is a novel visualization approach to find relationship of similarity of different papers.**"}}