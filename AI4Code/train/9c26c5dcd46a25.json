{"cell_type":{"682a853d":"code","4741634f":"code","da5228a5":"code","bd92c09e":"code","66d6b379":"code","88ef9b1e":"code","83e1b52d":"code","68834424":"code","821208b8":"code","29f5799b":"code","4a3ab69c":"code","b0e45d8d":"code","56fbd985":"code","fdf4be87":"code","4085f591":"code","3ee94c94":"code","2e061cb8":"code","6be457d7":"code","e3cee3ff":"code","be856aa0":"code","bd91b8fb":"code","535495c6":"code","11e76afe":"code","ad03f1a0":"code","948c1f59":"code","7e5a00fc":"code","85c5b65c":"code","8b801032":"code","7370046b":"code","4bdd0686":"code","c4f31160":"code","bcf7fc2c":"code","dbde926c":"code","5560cc83":"code","e4b05d2d":"code","db101900":"code","02d35b8a":"code","f918e86c":"code","dac34e39":"code","a08d4ffe":"code","c73ac3fa":"code","711a0210":"code","2308bf8c":"code","ef2a434f":"code","3463443d":"code","2b2f14a4":"code","6e8b9643":"markdown","62e346ae":"markdown","824f73f1":"markdown","7056c838":"markdown","c3266610":"markdown","c722963d":"markdown","c14e7a91":"markdown","ddd2348d":"markdown","a5e02940":"markdown","61b73646":"markdown","ae67decd":"markdown","09b37d46":"markdown","4b648ad0":"markdown","a39eadb5":"markdown","7dffbb83":"markdown","ef7cef2c":"markdown","ad95c8ad":"markdown","cd5560e8":"markdown","3c17fba6":"markdown","cafe1359":"markdown","08f69ef3":"markdown","3d417227":"markdown","74c9aac3":"markdown","120384c4":"markdown","1438bb98":"markdown","75592e4e":"markdown","a6f81711":"markdown","b9ba3371":"markdown","35b13eb1":"markdown","d8af10b8":"markdown","a36f0fee":"markdown","06fdfd3e":"markdown","d906db0b":"markdown","417b27e0":"markdown","74fbd4f3":"markdown","91419369":"markdown"},"source":{"682a853d":"# Importation des librairies\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Lecture et importation du dossier input des datas\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4741634f":"datas = pd.read_csv('..\/input\/enopenfoodfactsorgproducts\/cleaned_openfoodfacts.csv', \n                    sep='\\t',\n                    index_col=False,\n                    parse_dates=[3,4],\n                    low_memory=False)\ndatas.head()","da5228a5":"datas.info()","bd92c09e":"datas.describe()","66d6b379":"add_per_year = datas['code'].groupby(by=datas['created_datetime'].dt.year).nunique()\nmodified_per_year = datas['code'].groupby(by=datas['last_modified_datetime'].dt.year).nunique()\n\nfig=plt.figure(figsize=(12,8))\n\nfont_title = {'family': 'serif',\n              'color':  '#114b98',\n              'weight': 'bold',\n              'size': 18,\n             }\n\nsns.set_style(\"whitegrid\")\nplt.plot(add_per_year, \n         color=\"#114b98\", \n         label=\"Ajouts\")\nplt.plot(modified_per_year, \n         color=\"#00afe6\", \n         label=\"Modifications\")\nplt.title(\"Evolution des cr\u00e9ations et modifications de produits par ann\u00e9e\", \n          fontdict=font_title)\nplt.xlabel(\"Ann\u00e9e\")\nplt.ylabel(\"Nombre de produits\")\nplt.legend()\nplt.show()","88ef9b1e":"modified_products = round((datas[datas['last_modified_datetime'] > datas['created_datetime']].shape[0] \/ datas.shape[0])*100,2)\nprint(\"{:02}% des produits ont \u00e9t\u00e9 modifi\u00e9s au moins 1 fois depuis leur cr\u00e9ation.\".format(modified_products))","83e1b52d":"creators = datas.groupby(by='creator')['code'].nunique().sort_values(ascending=False)","68834424":"def plot_contributor(n=5):\n    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(aspect=\"equal\"))\n\n    explodes = np.zeros(n)\n    explodes[0] = .1\n\n    def pct_tot(pct):\n        tot = round(pct*creators[:n].sum(),0)\n        tot_pct = tot\/creators.sum()\n        return \"{:.1f}%\\n({:.0f})\".format(tot_pct,(tot\/100))\n\n    plt.pie(creators[:n], labels=creators[:n].index, \n            startangle=45, \n            shadow=True,\n            explode=explodes,\n            autopct=lambda pct: pct_tot(pct),\n            textprops=dict(color=\"black\",size=12, weight=\"bold\"))\n    plt.title(\"Qui sont les {:d} meilleurs contributeurs ?\".format(n), \n              fontdict=font_title)\n    plt.show()","821208b8":"plot_contributor(n=5)","29f5799b":"nutrigrades = datas.groupby(by='nutriscore_grade')['code'].nunique().sort_values(ascending=False)\nfig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(aspect=\"equal\"))\n\nexplodes = np.zeros(5)\nexplodes[0] = .1\n\nplt.pie(nutrigrades, labels=nutrigrades.index, \n        startangle=0, \n        shadow=True,\n        explode=explodes,\n        autopct='%1.1f%%',\n        textprops=dict(color=\"black\",size=12, weight=\"bold\"))\nplt.title(\"R\u00e9partition des Nutrigrades\", fontdict=font_title)\nplt.show()","4a3ab69c":"#On s'occupe ici uniquement des nutriscores et nutrigrades compl\u00e9t\u00e9s\ndatas_nutri = datas[(datas['nutriscore_grade'].isnull()==False) & \n                   (datas['nutriscore_score'].isnull()==False)]\n\nng_per_year = datas_nutri[['code','nutriscore_grade']].groupby(by=['nutriscore_grade',datas_nutri['created_datetime'].dt.year]).nunique().reset_index()\ncum_per_year = datas_nutri[['code']].groupby(by=datas_nutri['created_datetime'].dt.year).nunique().reset_index()\nng_per_year = pd.merge(ng_per_year, cum_per_year, how=\"left\", left_on=\"created_datetime\", right_on=\"created_datetime\")\nng_per_year = ng_per_year.rename(columns={'created_datetime':'year', 'code_x':'nb_nutrigrade', 'code_y':'total_grade'})\nng_per_year['nutrigrade_rate'] = (ng_per_year['nb_nutrigrade'] \/ ng_per_year['total_grade'])*100\n\nfig =plt.figure(figsize=(12,8))\nax = sns.lineplot(x='year', y='nutrigrade_rate', hue='nutriscore_grade', data=ng_per_year)\nplt.xlabel(\"Ann\u00e9e\")\nplt.ylabel(\"Taux du Nutrigrade\")\nplt.title(\"Evolution de la r\u00e9partition des Nutrigrades au fil des ann\u00e9es\", fontdict=font_title)\nplt.show()","b0e45d8d":"sns.pairplot(datas_nutri.sample(frac=0.05), hue=\"nutriscore_grade\")","56fbd985":"fig, axes = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(21,8))\nfig.suptitle(r\"R\u00e9partition des scores Nutriscore et de leurs grades\" \"\\n\", fontdict=font_title, fontsize=22)\n\nsns.histplot(data=datas_nutri.sort_values(\"nutriscore_grade\"), x=\"nutriscore_grade\", hue=\"nutriscore_grade\", ax=axes[0])\naxes[0].set_title('Grades de Nutriscores')\naxes[0].set_xlabel(\"Grade Nutriscore\")\naxes[0].set_ylabel(\"Nombre de produits\")\n\nsns.histplot(data=datas_nutri.sort_values(\"nutriscore_grade\"), x=\"nutriscore_score\", hue=\"nutriscore_grade\", ax=axes[1])\naxes[1].set_title('Scores de Nutriscores')\naxes[1].set_xlabel(\"Score Nutriscore\")\naxes[1].set_ylabel(\"Nombre de produits\")\n\nplt.show()","fdf4be87":"n_sub = len(datas_nutri['pnns_groups_1'].unique())\nn_col = 2\nfig, axes = plt.subplots(int(n_sub\/n_col), n_col, sharex=False, sharey=False, figsize=(21,int(3*n_sub)))\naxes = np.array(axes)\nfig.suptitle(\"Distribution des scores Nutriscore par cat\u00e9gorie\", fontdict=font_title, fontsize=22)\ni=0\nfor ax in axes.reshape(-1):\n    cat = datas_nutri['pnns_groups_1'].unique()[i]\n    subset = datas_nutri[datas_nutri['pnns_groups_1']==cat].sort_values(\"nutriscore_grade\")\n    sns.histplot(data=subset, x=\"nutriscore_score\", hue=\"nutriscore_grade\", ax=ax)\n    ax.set_title('Cat\u00e9gorie : {}'.format(cat))\n    ax.set_xlabel(\"Score Nutriscore\")\n    ax.set_ylabel(\"Nombre de produits\")\n    i+=1\nplt.show()","4085f591":"fig = plt.figure(figsize=(12,8))\nax = sns.boxplot(data=datas_nutri, x='pnns_groups_1', y='nutriscore_score')\nplt.setp(ax.get_xticklabels(), rotation=45)\nplt.title(\"Influence de la cat\u00e9gorie de produit sur le Nutriscore\", fontdict=font_title)\nplt.show()","3ee94c94":"import statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nanova_category = smf.ols('nutriscore_score~pnns_groups_1', data=datas_nutri).fit()\nprint(anova_category.summary())","2e061cb8":"#Tableau d'analyse de variance\nsm.stats.anova_lm(anova_category, typ=2)","6be457d7":"#Matrice des corr\u00e9lations\ncorr_mat = datas_nutri.corr()\n#heatmap\nfig = plt.figure(figsize=(10,10))\nsns.heatmap(corr_mat, square=True, linewidths=0.01, annot=True, cmap='coolwarm')\nplt.title(\"Coefficients de corr\u00e9lation de Pearson\", fontdict=font_title)\nplt.show()","e3cee3ff":"fig = plt.figure(figsize=(10,10))\ncols = corr_mat.nlargest(11,'nutriscore_score')['nutriscore_score'].index\ncorr_nutriscore = corr_mat.loc[cols, cols]\nsns.heatmap(corr_nutriscore, cbar=True, annot=True,\n            fmt='.2f', annot_kws={'size': 9},\n            square=True, linewidths=0.01, cmap='coolwarm')\nplt.title(\"Corr\u00e9lations de Pearson avec le Nutriscore\", fontdict=font_title)\nplt.show()","be856aa0":"# Nous partons du dataset dont les nutriscores sont compl\u00e9t\u00e9s.\nnumerical_features = datas_nutri.select_dtypes(include=['float64','int64'])\n\ny = numerical_features['nutriscore_score'].values\nX = numerical_features.drop('nutriscore_score', axis=1)","bd91b8fb":"X.drop(['sodium_100g','carbohydrates_100g','fat_100g'], axis=1, inplace=True)","535495c6":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)","11e76afe":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)","ad03f1a0":"from sklearn.dummy import DummyRegressor\ndummy_reg = DummyRegressor(strategy=\"mean\")\ndummy_reg.fit(X_train, y_train)\ndummy_pred = dummy_reg.predict(X_test)","948c1f59":"def metrics_model(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    diff = y_true - y_pred\n    mae = np.mean(abs(diff))\n    mse = np.mean(diff**2)\n    rmse = np.sqrt(mse)\n    r2 = 1-(sum(diff**2)\/sum((y_true-np.mean(y_true))**2))\n    dict_metrics = {\"M\u00e9trique\":[\"MAE\", \"MSE\", \"RMSE\", \"R\u00b2\"], \"R\u00e9sultats\":[mae, mse, rmse, r2]}\n    df_metrics = pd.DataFrame(dict_metrics)\n    return df_metrics","7e5a00fc":"dummy_metrics = metrics_model(y_test, dummy_pred).rename(columns={'R\u00e9sultats':'Baseline'})\ndummy_metrics","85c5b65c":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)","8b801032":"lr_metrics = metrics_model(y_test, lr_pred).rename(columns={'R\u00e9sultats':'LinearRegression'})\nlr_metrics = pd.concat([dummy_metrics,lr_metrics['LinearRegression']], axis=1)\nlr_metrics","7370046b":"def plot_pred_true(y_true, y_pred):\n    X_plot = [y_true.min(), y_true.max()]\n    fig = plt.figure(figsize=(12,8))\n    plt.scatter(y_true, y_pred)\n    plt.plot(X_plot, X_plot, color='r')\n    plt.xlabel(\"Valeurs r\u00e9\u00e9lles\")\n    plt.ylabel(\"Valeurs pr\u00e9dites\")\n    plt.title(\"Projection des valeurs pr\u00e9dites en fonction des valeurs r\u00e9\u00e9lles\", fontdict=font_title, fontsize=18)\n    plt.show()","4bdd0686":"plot_pred_true(y_true=y_test, y_pred=lr_pred)","c4f31160":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nnumerical_features = list(X.columns)\nnumerical_transformer = StandardScaler()\n\ncategorical_features = list(['pnns_groups_1', 'pnns_groups_2'])\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features),\n        ('cat', categorical_transformer, categorical_features)])\n\npipeline_lr = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('regressor', LinearRegression())])\n\nX = datas_nutri[numerical_features + categorical_features]\ny = datas_nutri['nutriscore_score']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3, \n                                                    random_state=42)\n\npipeline_lr.fit(X_train, y_train)\nplr_pred = pipeline_lr.predict(X_test)\n\nplr_metrics = metrics_model(y_test, plr_pred).rename(columns={'R\u00e9sultats':'LinearRegression cat'})\nplr_metrics = pd.concat([lr_metrics,plr_metrics['LinearRegression cat']], axis=1)\nplr_metrics","bcf7fc2c":"from sklearn import set_config\n\nset_config(display='diagram')\npipeline_lr","dbde926c":"from sklearn.decomposition import PCA\n\n#Centrage et r\u00e9duction\nX = datas_nutri.select_dtypes(include=['float64','int64'])\nX_scaled = scaler.fit_transform(X)\n\n#Instanciation de l'ACP\npca = PCA(svd_solver='full').fit(X_scaled)\nX_projected = pca.transform(X_scaled)","5560cc83":"#Variances expliqu\u00e9es\nvarexpl = pca.explained_variance_ratio_*100\n\n#Projection de l'\u00e9boulis des valeurs propres\nplt.figure(figsize=(12,8))\nplt.bar(np.arange(len(varexpl))+1, varexpl)\nplt.plot(np.arange(len(varexpl))+1, varexpl.cumsum(),c=\"red\",marker='o')\nplt.xlabel(\"rang de l'axe d'inertie\")\nplt.ylabel(\"pourcentage d'inertie\")\nplt.title(\"Eboulis des valeurs propres\", fontdict=font_title)\nplt.show(block=False)","e4b05d2d":"print(\"Le premier plan factoriel couvrira une inertie de {:.2f}% et le second plan : {:.2f}%.\".format(varexpl[0:2].sum(),\n                                                                                                     varexpl[0:4].sum()))","db101900":"#Espace des composantes principales\npcs = pca.components_\n\n#Matrice des corr\u00e9lations variables x facteurs\np = X.shape[1]\nsqrt_valprop = np.sqrt(pca.explained_variance_)\ncorvar = np.zeros((p, p))\nfor dim in range(p):\n    corvar[:,dim] = pcs[dim,:] * sqrt_valprop[dim]\n\n#on affiche pour les deux premiers plans factoriels \ncorr_matrix = pd.DataFrame({'feature':X.columns,'CORR_F1':corvar[:,0],'CORR_F2':corvar[:,1], \n              'CORR_F3':corvar[:,2], 'CORR_F4':corvar[:,3]})\ncorr_matrix","02d35b8a":"#Variable Illustrative\nivNutrigrade = datas_nutri['nutriscore_grade'].values\n\n#Encodage des grades\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nivNutrigrade = encoder.fit_transform(ivNutrigrade)\nivNutrigrade = ivNutrigrade.reshape((ivNutrigrade.shape[0],1))\n\n#Corr\u00e9lation de la variable illustrative avec les axes factoriels \ncorrIv = np.zeros((ivNutrigrade.shape[1],p))\nfor j in range(p): \n    for k in range(ivNutrigrade.shape[1]): \n        corrIv[k,j] = np.corrcoef(ivNutrigrade[:,k],X_projected[:,j])[0,1]","f918e86c":"def cerle_corr(pcs, n_comp, pca, axis_ranks, \n               labels=None, label_rotation=0, \n               illustrative_var_label=None, illustrative_var_corr=None):\n    for d1, d2 in axis_ranks:\n        if d2 < n_comp:\n            \n            # initialisation de la figure\n            fig=plt.figure(figsize=(10,10))\n            fig.subplots_adjust(left=0.1,right=0.9,bottom=0.1,top=0.9)\n            ax=fig.add_subplot(111)\n            ax.set_aspect('equal', adjustable='box') \n\n            #d\u00e9termination des limites du graphique\n            ax.set_xlim(-1,1) \n            ax.set_ylim(-1,1) \n\n            #affichage des fl\u00e8ches \n            plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n                       pcs[d1,:],pcs[d2,:], \n                       angles='xy', scale_units='xy', scale=1, \n                       color=\"grey\", alpha=0.5)\n            # et noms de variables\n            for i,(x,y) in enumerate(pcs[[d1,d2]].T):\n                plt.annotate(labels[i],(x,y),\n                             ha='center', va='center',\n                             fontsize='14',color=\"#17aafa\", alpha=0.8) \n\n            #variable illustrative\n            if illustrative_var_label is not None :\n                plt.annotate(illustrative_var_label,\n                             (illustrative_var_corr[0,d1],illustrative_var_corr[0,d2]),\n                             color='g')\n                plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n                                   illustrative_var_corr[0,d1],illustrative_var_corr[0,d2], \n                                   angles='xy', scale_units='xy', scale=1, color=\"g\", alpha=0.5)\n\n            #ajouter les axes \n            plt.plot([-1,1],[0,0],linewidth=1, color='grey', ls='--') \n            plt.plot([0,0],[-1,1],linewidth=1, color='grey', ls='--')\n\n            #ajouter un cercle \n            cercle = plt.Circle((0,0),1,color='#17aafa',fill=False) \n            ax.add_artist(cercle) \n\n            # nom des axes, avec le pourcentage d'inertie expliqu\u00e9\n            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n\n            plt.title(\"Cercle des corr\u00e9lations (F{} et F{})\".format(d1+1, d2+1), fontdict=font_title)\n            plt.show(block=False)","dac34e39":"cerle_corr(pcs, 4, pca, [(0,1),(2,3)], labels = np.array(X.columns), \n           illustrative_var_label=\"Nutriscore_grade\", illustrative_var_corr = corrIv)","a08d4ffe":"def plot_plans_factoriels(X_projected, n_comp, pca, axis_ranks, labels=None, alpha=1, illustrative_var=None):\n    for d1,d2 in axis_ranks:\n        if d2 < n_comp:\n \n            # initialisation de la figure       \n            fig = plt.figure(figsize=(12,8))\n        \n            # affichage des points\n            if illustrative_var is None:\n                plt.scatter(X_projected[:, d1], X_projected[:, d2], alpha=alpha)\n            else:\n                illustrative_var = np.array(illustrative_var)\n                for value in np.unique(illustrative_var):\n                    selected = np.where(illustrative_var == value)\n                    plt.scatter(X_projected[selected, d1], X_projected[selected, d2], alpha=alpha, label=value)\n                plt.legend()\n\n            # affichage des labels des points\n            if labels is not None:\n                for i,(x,y) in enumerate(X_projected[:,[d1,d2]]):\n                    plt.text(x, y, labels[i],\n                              fontsize='14', ha='center',va='center') \n            \n            # d\u00e9termination des limites du graphique\n            boundary = np.max(np.abs(X_projected[:, [d1,d2]])) * 1.1\n            plt.xlim([-boundary,boundary])\n            plt.ylim([-boundary,boundary])\n        \n            # affichage des lignes horizontales et verticales\n            plt.plot([-100, 100], [0, 0], color='grey', ls='--')\n            plt.plot([0, 0], [-100, 100], color='grey', ls='--')\n\n            # nom des axes, avec le pourcentage d'inertie expliqu\u00e9\n            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n\n            plt.title(\"Projection des {} individus sur F{} et F{}\".format(X_projected.shape[0], d1+1, d2+1), fontdict=font_title)\n            plt.show(block=False)","c73ac3fa":"plot_plans_factoriels(X_projected, 4, pca, [(0,1),(2,3)], illustrative_var = ivNutrigrade)","711a0210":"#Calcul des COS\u00b2\ncos2var = corvar**2\ndf_cos2var = pd.DataFrame({'feature':X.columns,'COS2_F1':cos2var[:,0],\n                           'COS2_F2':cos2var[:,1], 'COS2_F3':cos2var[:,2],\n                           'COS2_F4':cos2var[:,3]}).set_index('feature')\n\n#affichage dans un heatmap seaborn\nfig = plt.figure(figsize=(12,10))\nsns.heatmap(df_cos2var, annot=True, cmap=\"YlGnBu\")\nplt.title(\"Qualit\u00e9 de repr\u00e9sentation des variables (COS\u00b2)\", fontdict=font_title)\nplt.show()","2308bf8c":"#contributions \nctrvar = cos2var \nfor k in range(p): \n    ctrvar[:,k] = ctrvar[:,k]\/pca.explained_variance_[k] \n\ndf_ctrvar = pd.DataFrame({'feature':X.columns,'CTR_F1':ctrvar[:,0],'CTR_F2':ctrvar[:,1],\n                         'CTR_F3':ctrvar[:,2], 'CTR_F4':ctrvar[:,3]}).set_index('feature')\n\n#affichage dans un heatmap seaborn\nfig = plt.figure(figsize=(12,8))\nsns.heatmap(df_ctrvar, annot=True, cmap=\"YlGnBu\")\nplt.title(\"Contribution des variables aux axes (CTR)\", fontsize=22)\nplt.show()","ef2a434f":"df_syn_var = pd.DataFrame(X_projected[:,:4], index=datas_nutri.index, \n                          columns=[\"F\"+str(i+1) for i in range(4)])\ndatas_extend = pd.concat([datas_nutri, df_syn_var], axis=1)\ndatas_extend.head()","3463443d":"#Les variables synth\u00e9tiques sont d\u00e9j\u00e0 standardis\u00e9es\nnumerical_features = ['F1','F2','F3','F4']\n\ncategorical_features = list(['pnns_groups_1', 'pnns_groups_2'])\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features)])\n\npipeline_lr_pca = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('regressor', LinearRegression())])\n\nX = datas_extend[numerical_features + categorical_features]\ny = datas_extend['nutriscore_score']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3, \n                                                    random_state=42)\n\npipeline_lr_pca.fit(X_train, y_train)\nplr_pca_pred = pipeline_lr_pca.predict(X_test)\n\nplr_pca_metrics = metrics_model(y_test, plr_pca_pred).rename(columns={'R\u00e9sultats':'LinearRegression PCA'})\nplr_pca_metrics = pd.concat([plr_metrics,plr_pca_metrics['LinearRegression PCA']], axis=1)\nplr_pca_metrics","2b2f14a4":"plot_pred_true(y_true=y_test, y_pred=plr_pca_pred)","6e8b9643":"On peut \u00e9galement visualiser l'**\u00e9volution des diff\u00e9rents nutrigrades dans le temps** :","62e346ae":"Nous allons devoir **standardiser nos donn\u00e9es** afin de les placer sur la m\u00eame ech\u00e8le *(moyenne = 0 et \u00e9cart-type = 1)*","824f73f1":"Les m\u00e9triques de ce premier mod\u00e8le sont bien meilleurs que la baseline. Le R\u00b2 (coefficient de d\u00e9termination) qui est le carr\u00e9 du coefficient de corr\u00e9lation lin\u00e9aire, est de 0.64 ce qui en fait une base correcte de pr\u00e9diction. Nous pouvons d'ailleurs projeter les erreurs de pr\u00e9dictions :","7056c838":"Afin de **tester une derni\u00e8re mod\u00e9lisation \u00e0 partir des variables synth\u00e9tiques** $F_1$ \u00e0 $F_4$, nous allons les ajouter \u00e0 notre dataset initial :","c3266610":"L'analyse de ces pairplots montre d\u00e9j\u00e0 des relations lin\u00e9aires entre certaines variables. On remarque \u00e9galement le regroupement par Nutrigrade sur la plupart des features. Regardons \u00e0 pr\u00e9sent la r\u00e9partition de ces Nutri-score et Nutri-grades","c722963d":"#### <font color=\"#114b98\" id=\"section_2_3\">2.3. R\u00e9gression lin\u00e9aire avec cat\u00e9gorie produits<\/font>\n\nAjoutons \u00e0 pr\u00e9sent dans le mod\u00e8le les 2 principales variables de cat\u00e9gorisation de produits : `pnns_groups_1`, `pnns_groups_2`. Ces variables cat\u00e9gorielles vont devoir \u00eatre encod\u00e9es pour \u00eatre incorpor\u00e9es au mod\u00e8le. Pour cette mod\u00e9lisattion, nous allons r\u00e9aliser un OneHotEncoder.","c14e7a91":"Nous allons calculer plusieurs m\u00e9triques pour cette baseline :","ddd2348d":"#### <font color=\"#114b98\" id=\"section_1_4\">1.4. Analyse des corr\u00e9lations lin\u00e9aires<\/font>\nPour analyser les corr\u00e9lations lin\u00e9aires entre nos variables quantitatives, nous allons r\u00e9aliser un **test de corr\u00e9lation de Pearson** et afficher ses r\u00e9sultats dans un heatmap :","a5e02940":"### <font color=\"#ea1c60\" id=\"section_1\">1. Analyses univari\u00e9es<\/font>\n\nCommen\u00e7ons par charger le **jeu de donn\u00e9es nettoy\u00e9es** et regardons une rapide description : ","61b73646":"On remarque ici une tr\u00e8s grosse perte de pr\u00e9cision si l'on base la mod\u00e9lisation sur les variables synth\u00e9tiques de la PCA. **La r\u00e9duction dimensionnelle, \u00e0 ce stade de pr\u00e9paration des donn\u00e9es n'est pas pertinante pour notre application**. En effet, le nettoyage initial a d\u00e9j\u00e0 permis de r\u00e9duire consid\u00e9rablement le nombre de variables du dataset.","ae67decd":"#### <font color=\"#114b98\" id=\"section_3_3\">3.3. Projection des produits sur les plans factoriels<\/font>\n\nOn peut \u00e0 pr\u00e9sent visualiser la projection des individus sur ces premiers plans factoriels et donc en 2D :","09b37d46":"On remarque ici ques les cat\u00e9gories semblent assez diff\u00e9rentes, m\u00eame si l'ordre de grandeur des \u00e9cart est relativement faible. Dans les histogrammes ci-dessus, nous voyons cependant que les distributions ne semblent pas suivre la loi normale.      \nLa question sera \u00e0 pr\u00e9sent de savoir si ces \u00e9carts sont significatifs ou pas via l'analyse de variance :","4b648ad0":"**Les 2 premiers plans factoriels couvrent une inertie d'un peu plus de 77%**. Une analyse sur $F_1$ et $F_2$ semble donc coh\u00e9rente.\n\nProjetons \u00e0 pr\u00e9sent le **cercle des corr\u00e9lations** :\n\n#### <font color=\"#114b98\" id=\"section_3_2\">3.2. Cercle des corr\u00e9lations<\/font>","a39eadb5":"### <font color=\"#ea1c60\" id=\"section_2\">2. Analyses multivari\u00e9es : R\u00e9gression lin\u00e9aire multivari\u00e9e.<\/font>\n\nQuand une variable cible est le fruit de la corr\u00e9lation de plusieurs variables pr\u00e9dictives, on parle de **Multivariate Regression** pour faire des pr\u00e9dictions. Toutes ces variables pr\u00e9dictives seront utilis\u00e9es dans notre mod\u00e8le de r\u00e9gression lin\u00e9aire multivari\u00e9e pour trouver une **fonction pr\u00e9dictive** du type :\n\n$$\\large F(X) = \\epsilon + \\alpha x_1 + \\beta x_2 + \\gamma x_3 + ... + \\omega x_n$$\n\no\u00f9 :\n- $\\large \\epsilon$ est une constante,\n- $\\large \\alpha , \\beta , \\gamma$ repr\u00e9sentent les coefficients de notre fonction pr\u00e9dictive $\\large F(X)$,\n- $\\large X$ est un vecteur de variables pr\u00e9dictives.\n\nDans un premier temps, nous allons consid\u00e9rer comme **variables pr\u00e9dictives uniquement l'ensemble des variables num\u00e9riques**. Nous \u00e9tenderons par la suite le mod\u00e8le aux variables cat\u00e9gorielles. ","7dffbb83":"Afin de v\u00e9rifier si la cat\u00e9gorie `pnns_groups_1` ou `pnns_groups_2` influence r\u00e9\u00e9llement le Nutriscore, nous pouvons r\u00e9aliser **une ANOVA (Analyse de la variance)**. Le choix de ce test est d\u00fb au fait que nous \u00e9tudions 1 variable qualitative comparativement \u00e0 une variable quantitative.\n\nLes hypoth\u00e8ses pos\u00e9es seront donc les suivantes :\n- ***H0*** : La distribution des \u00e9chantillons est similaire *(et donc la cat\u00e9gorie n'a aucune influence sur le Nutriscore)*.\n- ***H1*** : Une ou plusieurs distributions sont in\u00e9gales.\n\nPour commencer, nous pouvons projeter les boxplots de la r\u00e9partition des nutriscores par cat\u00e9gorie `pnns_groups_1` pour \u00e9galement v\u00e9rifier les hypoth\u00e8ses de d\u00e9part li\u00e9es \u00e0 l'ANOVA \u00e0 savoir :\n- Les observations dans chaque \u00e9chantillon sont ind\u00e9pendantes et distribu\u00e9es de mani\u00e8re identique (iid). \n- Les observations dans chaque \u00e9chantillon ont la m\u00eame variance.\n- Les observations de chaque \u00e9chantillon sont normalement distribu\u00e9es. ","ef7cef2c":"La r\u00e9partition des Nutriscores est quasi \u00e9quitable avec tout de m\u00eame une pr\u00e9pond\u00e9rance pour la classe D. On peut \u00e9galement projeter la **r\u00e9partition des scores nutriscore par cat\u00e9gorie de produits** (`pnns_groups_1`) ","ad95c8ad":"On remarque que les ajouts dans la base OpenFoodFacts se sont acc\u00e9l\u00e9r\u00e9s \u00e0 partir de 2016. Les modifications de produits quant \u00e0 elles se sont intensifi\u00e9es nettement \u00e0 partir de 2019 avec un pic pour le moment en 2020.","cd5560e8":"#### <font color=\"#114b98\" id=\"section_1_3\">1.3. R\u00e9partition des Nutriscores et ANOVA<\/font>\n\nNous avons tent\u00e9 de calculer simplement les nutriscores et nutrigrades dans le Notebook *PSant\u00e9_01_nettoyage*. Cependant, les erreurs constat\u00e9es \u00e9tant sup\u00e9rieur \u00e0 50%, nous n'avons pas imput\u00e9 les valeurs manquantes \u00e0 cette \u00e9tape. Regardons \u00e0 pr\u00e9sent la r\u00e9partition des nutriscores d\u00e9j\u00e0 compl\u00e9t\u00e9s dans le dataset initial :","3c17fba6":"On voit ici clairement que la mise en place de la Loi de Sant\u00e9 2017 et du calcul du Nutri-score par l'\u00e9quipe du Pr. Serge Hercberg a fait chuter la part de produits consid\u00e9r\u00e9s Nutri-score A au profit des produits typ\u00e9s D et E.\n\nRegardons \u00e9galement les relations par paires de variables :","cafe1359":"On remarque ici clairement, avec le cercle et le COS\u00b2, les corr\u00e9lations importantes entre l'energie et le caract\u00e8re \"gras\" des produits. L'axe  $F_1$  va donc parfaitement repr\u00e9senter le facteur \"\u00e9nerg\u00e9tique\" et l'axe  $F_2$  quant \u00e0 lui repr\u00e9sentera bien les qualit\u00e9s \"sucr\u00e9 \/ sal\u00e9\".\n\nRegardons \u00e0 pr\u00e9sent la **contribution des variables aux axes *(CTR)***, elle aussi \u00e9galement bas\u00e9e sur le carr\u00e9 de la corr\u00e9lation, mais relativis\u00e9e par l\u2019importance de l\u2019axe :","08f69ef3":"### <font color=\"#ea1c60\" id=\"section_3\">3. R\u00e9duction dimensionnelle.<\/font>\n\nPour cette r\u00e9duction du nombre de dimensions, nous allons r\u00e9aliser une **Analyse en Composantes Principale *(PCA)***, l'une des m\u00e9thodes d'analyse de donn\u00e9es multivari\u00e9es les plus utilis\u00e9es. Elle permet d'explorer des jeux de donn\u00e9es multidimensionnels constitu\u00e9s de variables quantitatives.\n\nPour cela, nous utiliserons la m\u00e9thode `PCA` du module `decomposition` Sklearn sur les variables num\u00e9riques centr\u00e9es et r\u00e9duite. Nous prendrons en variable illustrative de cette ACP le grade Nutriscore du produit.","3d417227":"Suite au nettoyage effectu\u00e9 dans le Notebook pr\u00e9c\u00e9dent, les valeurs m\u00e9dianes, \u00e9carts-type et valeus max semblent \u00eatre coh\u00e9rentes compte tenu du volume de donn\u00e9es. \n\n#### <font color=\"#114b98\" id=\"section_1_1\">1.1. Analyse des dates de cr\u00e9ation et modification de produits<\/font> ","74c9aac3":"#### <font color=\"#114b98\" id=\"section_3_4\">3.4. Qualit\u00e9 de repr\u00e9sentation de la r\u00e9duction de dimension<\/font>\n\nAfin d'analyser la performance de notre r\u00e9duction de dimension via PCA, nous allons regarder le ***COS\u00b2*** et la **CTR***.\n\nOn peut calculer la **qualit\u00e9 de repr\u00e9sentation des variables *(COS\u00b2)*** en \u00e9levant la corr\u00e9lation au carr\u00e9 :","120384c4":"Logiquement, les variables utilis\u00e9es dans le calcul des nutriscore sont bien corr\u00e9l\u00e9es lin\u00e9airement \u00e0 ce dernier. **<font color=\"green\">Notre concept d'application pourrait donc \u00eatre d'estimer le grade Nutriscore et le score correspondant gr\u00e2ce \u00e0 une r\u00e9gression lin\u00e9aire multiple sur les variables les plus corr\u00e9l\u00e9es, tout en utilisant la cat\u00e9gorie du produit<font>**. \n    \nNous allons v\u00e9rifier si cette th\u00e9orie fonctionne gr\u00e2ce \u00e0 la **r\u00e9gression lin\u00e9aire multivari\u00e9e**.","1438bb98":"# <font color=\"#114b98\">Conception d'une application au service de la sant\u00e9 publique<\/font>\n![Sante-publique-France-logo.png](attachment:3eae8bc6-bfb7-4fb7-8b97-95939f0b030b.png)\n\n## <font color=\"#00afe6\">Analyse exploratoire des donn\u00e9es<\/font>\n\nApr\u00e8s avoir nettoy\u00e9 les donn\u00e9es issues de la base OpenFoodFacts dans le notebook PSant\u00e9_01_nettoyage, nous avons export\u00e9 un nouveau dataset clean\u00e9 que nous allons ici pouvoir analyser. \n\nNous r\u00e9aliserons ici des analyses univari\u00e9es, bivari\u00e9es et multivari\u00e9es ainsi qu'une r\u00e9duction dimensionnelle.","75592e4e":"Les m\u00e9triques pour ce second mod\u00e8le sont meilleures que le premier mod\u00e8le simple de r\u00e9gression lin\u00e9aire. En effet, nous avons vu pr\u00e9alablement que l'hypoth\u00e8se selon laquelle la cat\u00e9gorie influen\u00e7ait le Nutriscore \u00e9tait v\u00e9rifi\u00e9e.\n\nPour am\u00e9liorer encore les performances, nous pourrions traiter d'autres types d'algorithmes comme la r\u00e9gression Ridge par exemple ou encore la r\u00e9gression lasso.\n\nNous allons \u00e0 pr\u00e9sent faire une r\u00e9duction de dimensions afin de trouver les meilleurs plans factoriels du jeu de donn\u00e9es.","a6f81711":"#### <font color=\"#114b98\" id=\"section_3_1\">3.1. Eboulis des valeurs propres<\/font>\n\nAfin d'avoir un aper\u00e7u du nombre de composantes n\u00e9cessaire \u00e0 l'analyse, nous allons projeter l'**\u00e9boulis des valeurs propres** :","b9ba3371":"Nous devons ici **supprimer les variables fortement corr\u00e9l\u00e9es** \u00e0 savoir : `sodium_100g`, `carbohydrates_100g` et `fat_100g`","35b13eb1":"#### <font color=\"#114b98\" id=\"section_2_1\">2.1. Baseline<\/font>\n\nNous avons donc \u00e0 notre disposition un jeu d'entrainement et un jeu de test pour nos pr\u00e9dictions du Nutriscore. Comme dans toute approche de mod\u00e9lisation, nous allons devoir v\u00e9rifier que notre mod\u00e8le est r\u00e9ellement performant. La r\u00e9gression lin\u00e9aire \u00e9tant un mod\u00e8le relativement simple, nous allons devoir **r\u00e9aliser une baseline**. Nous utiliserons la m\u00e9thode `DummyRegressor` de la librairie Sklearn :","d8af10b8":"Les r\u00e9sultats du **test de Fisher** nous indiquent ici une p-value de 0 pour l'ensemble des cat\u00e9gories, donc inferieur au niveau de test de 5%. Nous rejettons donc l'hypoth\u00e8se *H0* selon laquelle les ditributions sont identiques.        \n**La cat\u00e9gorie de produit a donc bien une influence sur le Nutriscore**.","a36f0fee":"## <font color=\"#00afe6\">Sommaire<\/font>\n\n1. [Analyses univari\u00e9es](#section_1)    \n    1.1. [Analyse des dates de cr\u00e9ation et modification de produits](#section_1_1)   \n    1.2. [Les contributeurs \u00e0 la base OpenFoodFacts](#section_1_2)     \n    1.3. [R\u00e9partition des Nutriscores et ANOVA](#section_1_3)     \n    1.4. [Analyse des corr\u00e9lations lin\u00e9aires](#section_1_4)     \n2. [Analyses multivari\u00e9es : R\u00e9gression lin\u00e9aire multivari\u00e9e](#section_2)     \n    2.1. [Baseline](#section_2_1)     \n    2.2. [Premi\u00e8re r\u00e9gression lin\u00e9aire](#section_2_2)     \n    2.3. [R\u00e9gression lin\u00e9aire avec cat\u00e9gorie produits](#section_2_3)          \n3. [R\u00e9duction dimensionnelle](#section_3)     \n    3.1. [Eboulis des valeurs propres](#section_3_1)     \n    3.2. [Cercle des corr\u00e9lations](#section_3_2)     \n    3.3. [Projection des produits sur les plans factoriels](#section_3_3)     \n    3.4. [Qualit\u00e9 de repr\u00e9sentation de la r\u00e9duction de dimension](#section_3_4)     ","06fdfd3e":"#### <font color=\"#114b98\" id=\"section_1_2\">1.2. Les contributeurs \u00e0 la base OpenFoodFacts<\/font> \n\nRegardons \u00e0 pr\u00e9sent la r\u00e9partition des entr\u00e9es par contributeurs :","d906db0b":"Puis nous allons spliter nos donn\u00e9es en 1 jeu d'entrainement et un jeu de test *(30%)* :","417b27e0":"Cette matrice des corr\u00e9lations ne nous apporte pas r\u00e9\u00e9llement d'informations mais confirme math\u00e9matiquement des \u00e9l\u00e9ments logiques : `salt_100g` est tr\u00e8s fortement corr\u00e9l\u00e9 avec `sodium_100g`, `fat_100g` avec `satured-fat_100g`... Il faudra cependant tenir compte de ces fortes corr\u00e9lation dans nos mod\u00e8les, la **colin\u00e9arit\u00e9** d\u00e9gradant les performances.\n\n\nOn remarque \u00e9galement que des **corr\u00e9lations lin\u00e9aires existent entre le nutriscore et certaines variables** plus que d'autres :","74fbd4f3":"#### <font color=\"#114b98\" id=\"section_2_2\">2.2. premi\u00e8re r\u00e9gression lin\u00e9aire<\/font>\nNous avons donc nos m\u00e9triques de Baseline et nous allons \u00e0 pr\u00e9sent pouvoir r\u00e9aliser notre premier mod\u00e8le de r\u00e9gression lin\u00e9aire multivari\u00e9e et le comparer aux m\u00e9triques de base obtenues :","91419369":"On remarque une dispersion encore importante des pr\u00e9dictions aux valeurs test. Ce premier mod\u00e8le peut \u00eatre am\u00e9lior\u00e9, par exemple en ajoutant la cat\u00e9gorie du produit.  "}}