{"cell_type":{"ddc1d98e":"code","e444051a":"code","997a5c42":"code","e9d7f127":"code","4bac1b59":"code","2b921b80":"code","fa870ba4":"code","425ef793":"code","4c413a05":"code","be20255b":"code","979d6dfc":"code","60bf1f52":"code","199971b1":"code","cc74bfd2":"code","dd39c729":"code","8c9b1ed8":"code","a4917928":"code","1e19c619":"code","cc98fc38":"code","f4ea2403":"code","5eeecc5c":"code","67d5d433":"code","5f1c05de":"code","c63cc15f":"code","5f7aa05c":"code","ad6fc8ab":"code","4c96634c":"code","d10d1c96":"markdown","33e5f482":"markdown","9b643433":"markdown","0040bf76":"markdown","bc4bcda5":"markdown","43478cf2":"markdown","6b2b64a8":"markdown","cddfb205":"markdown","3698828a":"markdown","53bf463f":"markdown"},"source":{"ddc1d98e":"import sys\nfrom pathlib import Path\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2 as cv\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.utils import class_weight\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.experimental import CosineDecay\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomCrop,CenterCrop, RandomRotation","e444051a":"dataset_df = pd.read_csv(\"..\/input\/nfl-player-numbers\/train_player_numbers.csv\")\ndataset_df","997a5c42":"dataset_df = dataset_df.sample(frac=1).reset_index(drop=True)\ndataset_df[\"filepath\"] = [\"..\/input\/nfl-player-numbers\/\"+row.filepath for idx, row in dataset_df.iterrows()]\ndataset_df = dataset_df[dataset_df.video_frame.str.contains(\"Endzone\")]","e9d7f127":"training_percentage = 0.8\ntraining_item_count = int(len(dataset_df)*training_percentage)\nvalidation_item_count = len(dataset_df)-int(len(dataset_df)*training_percentage)\ntraining_df = dataset_df[:training_item_count]\nvalidation_df = dataset_df[training_item_count:]","4bac1b59":"batch_size = 64\nimage_size = 64\ninput_shape = (image_size, image_size, 3)\ndropout_rate = 0.4\nclasses_to_predict = sorted(training_df.label.unique())\nclass_weights = class_weight.compute_class_weight(\"balanced\", classes_to_predict, training_df.label.values)\nclass_weights_dict = {i : class_weights[i] for i,label in enumerate(classes_to_predict)}\n\ntraining_data = tf.data.Dataset.from_tensor_slices((training_df.filepath.values, training_df.label.values))\nvalidation_data = tf.data.Dataset.from_tensor_slices((validation_df.filepath.values, validation_df.label.values))","2b921b80":"def load_image_and_label_from_path(image_path, label):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    return img, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntraining_data = training_data.map(load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\nvalidation_data = validation_data.map(load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\n\ntraining_data_batches = training_data.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=AUTOTUNE)\nvalidation_data_batches = validation_data.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=AUTOTUNE)","fa870ba4":"data_augmentation_layers = tf.keras.Sequential(\n    [\n        layers.experimental.preprocessing.RandomRotation(0.25),\n        layers.experimental.preprocessing.RandomZoom((-0.2, 0)),\n        layers.experimental.preprocessing.RandomContrast((0.2,0.2))\n    ]\n)","425ef793":"image = Image.open(training_df.filepath.values[1])\nplt.imshow(image)\nplt.show()","4c413a05":"image = tf.expand_dims(np.array(image), 0)\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n  augmented_image = data_augmentation_layers(image)\n  ax = plt.subplot(3, 3, i + 1)\n  plt.imshow(augmented_image[0])\n  plt.axis(\"off\")","be20255b":"efficientnet = EfficientNetB0(weights=\"..\/input\/efficientnet-b0-for-keras-no-top\/efficientnetb0_notop.h5\", \n                              include_top=False, \n                              input_shape=input_shape, \n                              drop_connect_rate=dropout_rate)\n\ninputs = Input(shape=input_shape)\naugmented = data_augmentation_layers(inputs)\nefficientnet = efficientnet(augmented)\npooling = layers.GlobalAveragePooling2D()(efficientnet)\ndropout = layers.Dropout(dropout_rate)(pooling)\noutputs = Dense(len(classes_to_predict), activation=\"softmax\")(dropout)\nmodel = Model(inputs=inputs, outputs=outputs)\n    \nmodel.summary()","979d6dfc":"epochs = 25\ndecay_steps = int(round(len(training_df)\/batch_size))*epochs\ncosine_decay = CosineDecay(initial_learning_rate=1e-3, decay_steps=decay_steps, alpha=0.3)\n\ncallbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(cosine_decay), metrics=[\"accuracy\"])","60bf1f52":"history = model.fit(training_data_batches,\n                  epochs = epochs, \n                  validation_data=validation_data_batches,\n                  class_weight=class_weights_dict,\n                  callbacks=callbacks)","199971b1":"model.load_weights(\"best_model.h5\")","cc74bfd2":"def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n    video_path=f\"{video_dir}\/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv.VideoCapture(video_path)\n    if only_with_impact:\n        boxes_all = video_labels.query(\"video == @video_name\")\n        print(video_path, boxes_all[boxes_all.impact == 1.0].shape[0])\n    else:\n        print(video_path)\n    frame = 0\n\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        if only_with_impact:\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            boxes_with_impact = boxes[boxes.impact == 1.0]\n            if boxes_with_impact.shape[0] == 0:\n                continue\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}\/{video_name}'.replace('.mp4',f'_{frame}.png')\n\n        try:\n            _ = cv.imwrite(image_path, img)\n        except:\n            print(img_name+\" \"+image_path)","dd39c729":"test_frames_folder = \"..\/working\/test_frames\"\nos.mkdir(test_frames_folder)\n\nvideo_dir = '..\/input\/nfl-health-and-safety-helmet-assignment\/test'\nvideo_folder = [filename for filename in os.listdir(video_dir)]\nfor video_name in video_folder:\n    print(video_name)\n    mk_images(video_name, pd.DataFrame(), video_dir, test_frames_folder, only_with_impact=False)","8c9b1ed8":"test_helmet_df = pd.read_csv(\"..\/input\/nfl-health-and-safety-helmet-assignment\/test_baseline_helmets.csv\")\ntest_tracking_df = pd.read_csv(\"..\/input\/nfl-health-and-safety-helmet-assignment\/test_player_tracking.csv\")","a4917928":"def find_team(jersey_number, video_frame_name):\n    '''\n    find to which team a player belong based on the jersey number\n    return None if we have 2 players with the same number on the pitch\n    '''\n    \n    game_id = int(video_frame_name.split(\"_\")[0])\n    play_id = int(video_frame_name.split(\"_\")[1])\n    player_list = test_tracking_df.query(\"gameKey==@game_id and playID==@play_id\").player.unique()\n    possible_players = [player_code for player_code in player_list if jersey_number==int(player_code[1:])]\n\n    if len(possible_players)==1:\n        return possible_players[0]\n    else:\n        return None\n\ndef extract_player_jersey(video_frame_name, display=False):\n    '''\n    Get the helmet boxes for a frame and apply the model.\n    If a player is predicted twice, keeps the prediction\n    with the highest confidence score.\n    '''\n    \n    predictions = []\n    img = np.array(Image.open(test_frames_folder+\"\/\"+str(video_frame_name)))\n    frame_df = test_helmet_df[test_helmet_df[\"video_frame\"]==video_frame_name.replace(\".png\",\"\")]\n\n    baseline_boxes = np.array([np.array([row.left, row.top, row.left+row.width, row.top+row.height ])  for idx, row in frame_df.iterrows()])\n    for idx, box in enumerate(baseline_boxes):\n        box_centre = int(box[0]+round((box[2]-box[0])\/2))\n        jersey_box = img[box[3]-24:box[3]+40,box_centre-32:box_centre+32,:]\n        \n        if jersey_box.shape==(64,64,3):\n            result = model.predict(np.array([np.array(jersey_box)]))\n            predicted_jersey_number = np.argmax(result)\n            confidence = result[0][np.argmax(result)] \n            confidence_threshold = 0.90\n\n            if confidence>confidence_threshold:\n                predicted_player_code = find_team(predicted_jersey_number, video_frame_name)\n                \n                if predicted_player_code is not None:\n                    player_already_detected = [(i, item) for i, item in enumerate(predictions) if item[\"label\"] == predicted_player_code]\n                    prediction_data = {\"video_frame\":video_frame_name.replace(\".png\",\"\"), \n                                            \"label\":predicted_player_code,\n                                            \"left\":frame_df.iloc[idx].left,\n                                            \"width\":frame_df.iloc[idx].width,\t\n                                            \"top\":frame_df.iloc[idx].top,\n                                            \"height\":frame_df.iloc[idx].height,\n                                            \"confidence\":confidence}\n                    \n                    if player_already_detected==[]:\n                        predictions.append(prediction_data)\n                    else:\n                        if player_already_detected[0][1]['confidence']<confidence:\n                            dict_index_to_remove = player_already_detected[0][0]\n                            del predictions[dict_index_to_remove]\n                            predictions.append(prediction_data)\n                    \n                    if display:\n                        print(predicted_player_code, confidence)\n                        plt.imshow(jersey_box)\n                        plt.show()\n        \n    return predictions","1e19c619":"frame_list = os.listdir(test_frames_folder)\nframe_list = [x for x in frame_list if \"Endzone\" in x]\nrandom.seed(42)\nframes_to_test = random.sample(frame_list, 6) ","cc98fc38":"extract_player_jersey(frames_to_test[0], display=True)","f4ea2403":"extract_player_jersey(frames_to_test[1], display=True)","5eeecc5c":"extract_player_jersey(frames_to_test[2], display=True)","67d5d433":"extract_player_jersey(frames_to_test[3], display=True)","5f1c05de":"extract_player_jersey(frames_to_test[4], display=True)","c63cc15f":"extract_player_jersey(frames_to_test[5], display=True)","5f7aa05c":"def predict_for_submission(frame_list):\n    prediction_list = []\n    with tqdm(total=len(frame_list)) as pbar:\n        for video_frame in frame_list:\n            prediction = extract_player_jersey(video_frame)\n            prediction_list += prediction\n            pbar.update(1)\n    return pd.DataFrame(prediction_list)","ad6fc8ab":"predict_for_submission(frame_list[:30])","4c96634c":"!rm -rf ..\/working\/test_frames","d10d1c96":"## Thanks for reading this notebook! If you found this notebook helpful, please give it an upvote. It is always greatly appreciated!","33e5f482":"# NFL Jersey Number Recognition to Track Players","9b643433":"This notebook presents a computer vision approach to track NFL players in video frames. Using the helmet boxes provided in the original dataset, a [dataset](https:\/\/www.kaggle.com\/frlemarchand\/nfl-player-numbers) was built by creating larger boxes to train a model to recognise jersey numbers. This notebook shows how to use the dataset, how to train a model and how to predict over video frames from the test set. I am mostly sharing this in order to help people interested in this approach but please bear in mind this is a work in progress and the current results seem to perform below other approaches such as the [simple helmet mapping](https:\/\/www.kaggle.com\/its7171\/nfl-baseline-simple-helmet-mapping) by [tito](https:\/\/www.kaggle.com\/its7171).","0040bf76":"# Model training","bc4bcda5":"# Extract videos frames from test set","43478cf2":"I have created the function to submit the predictions but it's still early days to be spamming the leaderboard yet!","6b2b64a8":"The training process used below is heavily inspired from one of my previous notebooks available [here](https:\/\/www.kaggle.com\/frlemarchand\/efficientnet-aug-tf-keras-for-cassava-diseases). Please note that I should have probably split my dataset based on the playID as some frames within the same play can be extremely similar.","cddfb205":"# Recognise jersey numbers in new video frames","3698828a":"We proceed to a quick shuffle of the dataframe while filtering the images to only keep the ones taken from the endzone. From previous attempts, it appeared that frames taken from the sideline result in jersey numbers being too low resolution to read them.","53bf463f":"For this run, we'll only apply prediction on the frames taken from the endzone as the jerseys are often easier to see."}}