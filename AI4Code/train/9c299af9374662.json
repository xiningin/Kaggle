{"cell_type":{"bd6ede18":"code","8bbf26b8":"code","a90690a1":"code","8e5fd167":"code","9958bb74":"code","2e6c25b7":"code","a990f2f9":"code","684c0e8f":"code","e94eb26c":"code","7a27c805":"code","8e0914d1":"code","0d3c68e9":"code","10bb2505":"code","b87e110f":"code","9377ff6d":"code","2efc1f84":"code","542d1b96":"code","01870562":"code","037c5532":"code","b3f7fc25":"code","9522163d":"code","2c2bbdf5":"code","7f53a2bb":"code","0b17bb06":"code","6575e61b":"code","41db8a52":"code","05e4c57d":"code","3e4a12f3":"code","7d3353c4":"code","93467a01":"code","9ffa77df":"code","eb97f48a":"code","d9acc27d":"code","9fe910b5":"code","96746132":"code","124bff49":"code","7e83c41a":"code","120c3b08":"code","1fa5ed11":"code","d365d637":"code","5039f7ec":"code","82c66619":"code","0b09421b":"code","e870f3d9":"code","6388af9f":"code","8a8970c8":"code","56528b4f":"code","7b26a784":"code","88dbdfbb":"code","e43c802f":"code","e13aa27c":"code","85ceac23":"code","8c323e24":"code","fa230e55":"code","be3e38d6":"code","5ce92bc9":"code","06c70657":"markdown","426bb097":"markdown","a21e795f":"markdown","9c655727":"markdown","0575850f":"markdown","1e561588":"markdown","0ab8aead":"markdown","9a64c56d":"markdown","7d1a42ce":"markdown","3f129be1":"markdown","f128eaf6":"markdown","00baf898":"markdown","e31d3238":"markdown","2da40f53":"markdown","986f5acc":"markdown","a0513b61":"markdown","26bc26d9":"markdown","9ecbec8c":"markdown"},"source":{"bd6ede18":"from IPython.display import Image\nimport os\n#!ls ..\/input\/\nImage(\"..\/input\/baggingimage\/Bagging_Main.PNG\")","8bbf26b8":"# Loading Libraries\nimport pandas as pd # for data analysis\nimport numpy as np # for scientific calculation\nimport seaborn as sns # for statistical plotting\nimport datetime # for working with date fields\nimport matplotlib.pyplot as plt # for plotting\n%matplotlib inline\nimport math # for mathematical calculation","a90690a1":"#Reading Loan Payment given Data Set.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/loan-payment\/Loan_payments_data.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8e5fd167":"# Reading Loan Payment .csv file.\nloanpayment = pd.read_csv(\"..\/input\/loan-payment\/Loan_payments_data.csv\")                             # Reading data using simple Pandas","9958bb74":"# Describe method is used to view some basic statistical details like percentile, mean, std etc. of a data frame of numeric values.\nloanpayment.describe()","2e6c25b7":"#Checking shape of data\nloanpayment.shape","a990f2f9":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n# Verifying top 3 sample records of data.\nloanpayment.head(3)\n# Checking Null Values : We can see there are No Null Values \nloanpayment.isnull().sum()\n# Checking the data information.\n# Observation: No missing values\nloanpayment.info()","684c0e8f":"loanpayment['loan_status'].value_counts()","e94eb26c":"loanpayment['education'].value_counts()","7a27c805":"loanpayment['Gender'].value_counts()","8e0914d1":"loanpayment['terms'].value_counts()","0d3c68e9":"loanpayment.groupby(by=['Gender','education','loan_status'])['loan_status'].count()","10bb2505":"print(np.min(loanpayment.age))\nprint(np.max(loanpayment.age))","b87e110f":"loanpayment['age_bins'] = pd.cut(x=loanpayment['age'], bins=[18, 20, 30, 40, 50, 60])","9377ff6d":"plt.rcParams['figure.figsize'] = (20.0, 10.0)\nplt.rcParams['font.family'] = \"serif\"\nfig, ax =plt.subplots(3,2)\nsns.countplot(loanpayment['Gender'], ax=ax[0,0])\nsns.countplot(loanpayment['education'], ax=ax[0,1])\nsns.countplot(loanpayment['loan_status'], ax=ax[1,0])\nsns.countplot(loanpayment['Principal'], ax=ax[1,1])\nsns.countplot(loanpayment['terms'], ax=ax[2,0])\nsns.countplot(loanpayment['age_bins'], ax=ax[2,1])\nfig.show();","2efc1f84":"import plotly.express as px\n\nfig = px.histogram(loanpayment, x=\"terms\", y=\"Principal\", color = 'Gender',\n                   marginal=\"rug\", # or violin, rug,\n                   hover_data=loanpayment.columns,\n                   color_discrete_sequence=['indianred','lightblue'],\n                   )\n\nfig.update_layout(\n    title=\"Gender wise segregation of loan terms and principal\",\n    xaxis_title=\"Loan Term\",\n    yaxis_title=\"Principal Count\/Gender Segregation\",\n)\nfig.update_yaxes(tickangle=-30, tickfont=dict(size=7.5))\n\nfig.show();","542d1b96":"fig = px.scatter_3d(loanpayment,z=\"age\",x=\"Principal\",y=\"terms\",\n    color = 'Gender', size_max = 18,\n    #color_discrete_sequence=['indianred','lightblue'] \n                    symbol='Gender', opacity=0.7\n    )\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n","01870562":"loanpayment.head(1)","037c5532":"for dataset in [loanpayment]:\n    dataset.loc[dataset['age'] <= 20,'age']=0\n    dataset.loc[(dataset['age']>20) & (dataset['age']<=25),'age']=1\n    dataset.loc[(dataset['age']>25) & (dataset['age']<=30),'age']=2\n    dataset.loc[(dataset['age']>30) & (dataset['age']<=35),'age']=3\n    dataset.loc[(dataset['age']>35) & (dataset['age']<=40),'age']=4\n    dataset.loc[(dataset['age']>40) & (dataset['age']<=45),'age']=5\n    dataset.loc[(dataset['age']>45) & (dataset['age']<=50),'age']=6\n    dataset.loc[(dataset['age']>50) & (dataset['age']<=55),'age']=7","b3f7fc25":"loanpayment.head(1)","9522163d":"# Import label encoder \nfrom sklearn import preprocessing \n  \n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \nloanpayment_fe=loanpayment.copy()\n# Encode labels in column 'education'. \nloanpayment_fe['education_label']= label_encoder.fit_transform(loanpayment_fe['education']) \n#loanpayment_fe['education_label'].unique() \n# Encode labels in column 'loan_status'. \nloanpayment_fe['loan_status_label']= label_encoder.fit_transform(loanpayment_fe['loan_status']) \n#loanpayment_fe['loan_status_label'].unique() \n# Encode labels in column 'Gender'. \nloanpayment_fe['gender_label']= label_encoder.fit_transform(loanpayment_fe['Gender']) \n#loanpayment_fe['gender_label'].unique() ","2c2bbdf5":"loanpayment_fe.head(1)","7f53a2bb":"loanpayment_fe=loanpayment_fe.drop(['Loan_ID','loan_status','education','Gender','age_bins'],axis=1)\nloanpayment_fe.head(1)","0b17bb06":"loanpayment_fe['effective_date']= pd.to_datetime(loanpayment_fe['effective_date']) \nloanpayment_fe['due_date']= pd.to_datetime(loanpayment_fe['due_date']) \nloanpayment_fe['paid_off_time']= pd.to_datetime(loanpayment_fe['paid_off_time']) \nloanpayment_fe.info()","6575e61b":"loanpayment_fe['actual_tenure_days'] = loanpayment_fe['due_date'] - loanpayment_fe['effective_date']\nloanpayment_fe['actual_tenure_days']=loanpayment_fe['actual_tenure_days']\/np.timedelta64(1,'D')","41db8a52":"print(np.min(loanpayment_fe['actual_tenure_days']))\nprint(np.max(loanpayment_fe['actual_tenure_days']))","05e4c57d":"loanpayment_fe['paidoff_tenure_days'] = loanpayment_fe['paid_off_time'] - loanpayment_fe['effective_date']\nloanpayment_fe['paidoff_tenure_days']=loanpayment_fe['paidoff_tenure_days']\/np.timedelta64(1,'D')","3e4a12f3":"print(np.min(loanpayment_fe['paidoff_tenure_days']))\nprint(np.max(loanpayment_fe['paidoff_tenure_days']))","7d3353c4":"loanpayment_fe.isnull().sum()","93467a01":"loanpayment_fe.describe().T","9ffa77df":"loanpayment_fe=loanpayment_fe.drop(['past_due_days'],axis=1)","eb97f48a":"null_data = loanpayment_fe[loanpayment_fe.isnull().any(axis=1)]\nnull_data.head(2)","d9acc27d":"notnull_data = loanpayment_fe[loanpayment_fe.notnull().any(axis=1)]\nnotnull_data.head(2)","9fe910b5":"loanpayment_fe['paidoff_tenure_days'] = loanpayment_fe.groupby(['Principal','terms','age','education_label','gender_label'])['paidoff_tenure_days'].transform(lambda x:x.fillna(x.mean()))\nloanpayment_fe['paidoff_tenure_days'] = loanpayment_fe['paidoff_tenure_days'].fillna(0)","96746132":"\nloanpayment_fe.isnull().sum()","124bff49":"loanpayment_fe=loanpayment_fe.drop(['effective_date','due_date','paid_off_time'],axis=1)\nloanpayment_fe.head(1)","7e83c41a":"loanpayment_fe['paidoff_tenure_days']=loanpayment_fe['paidoff_tenure_days'].round().astype(int)","120c3b08":"loanpayment_fe['paidoff_tenure_days'].value_counts().head(2)","1fa5ed11":"loanpayment_fe.info()","d365d637":"loanpayment_fe.head(2)","5039f7ec":"loanpayment_fe['loan_status_label'].value_counts().plot.bar();","82c66619":"X2=loanpayment_fe.drop(['loan_status_label'],axis=1)\nX1=preprocessing.scale(X2)\nX=pd.DataFrame(X1)\ny=loanpayment_fe['loan_status_label']","0b09421b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,shuffle=True,test_size=0.25,stratify=y)","e870f3d9":"print(len(X_train[X_train==0]), len(X_train[X_train==1]), len(X_train[X_train==2]))\nprint(len(X_test[X_test==0]), len(X_test[X_test==1]), len(X_test[X_test==2]))\nprint(len(y_train[y_train==0]), len(y_train[y_train==1]), len(y_train[y_train==2]))\nprint(len(y_test[y_test==0]), len(y_test[y_test==1]), len(y_test[y_test==2]))\n","6388af9f":"#!pip install imblearn","8a8970c8":"from imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\noversampler = SMOTE(random_state=0)\nX_train ,y_train = oversampler.fit_sample(X_train, y_train)","56528b4f":"print(len(X_train[X_train==0]), len(X_train[X_train==1]), len(X_train[X_train==2]))\nprint(len(X_test[X_test==0]), len(X_test[X_test==1]), len(X_test[X_test==2]))\nprint(len(y_train[y_train==0]), len(y_train[y_train==1]), len(y_train[y_train==2]))\nprint(len(y_test[y_test==0]), len(y_test[y_test==1]), len(y_test[y_test==2]))","7b26a784":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","88dbdfbb":"from IPython.display import Image\nimport os\n#!ls ..\/input\/\nImage(\"..\/input\/baggingimage\/Bagging_Flow.PNG\")","e43c802f":"# Get some classifiers to evaluate\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import classification_report\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\nseed = 123\nkfold = model_selection.KFold(n_splits=5, random_state=seed,shuffle=True)\ndt = DecisionTreeClassifier()\nnum_trees = 100\nmodel_dt = BaggingClassifier(base_estimator=dt, n_estimators=num_trees, random_state=seed).fit(X_train,y_train)\nresults = model_selection.cross_val_score(model_dt, X_train, y_train, cv=kfold,n_jobs=1)\n\npredicted = model_selection.cross_val_predict(model_dt,X_test,y_test,cv=kfold) \ntest_scores = accuracy_score(y_test, predicted)    \nprint(\"Train Accuracy: %0.2f (+\/- %0.2f)\" % (results.mean(), results.std()))\nprint(\"\\nTest Accuracy: %0.2f (+\/- %0.2f)\" % (test_scores.mean(),test_scores.std()))\nprint(\"\\nTest classification_report \\n\", classification_report(y_test, predicted))","e13aa27c":"seed = 123\nnum_trees = 100\nkfold = model_selection.KFold(n_splits=5, random_state=seed,shuffle=True)\nmodel_rf = RandomForestClassifier() \nbagging_clf = BaggingClassifier(model_rf,n_estimators=num_trees, random_state=seed).fit(X_train,y_train) \nresults = model_selection.cross_val_score(bagging_clf, X_train, y_train, cv=kfold,n_jobs=1)\n\npredicted = model_selection.cross_val_predict(bagging_clf,X_test,y_test,cv=kfold) \ntest_scores = accuracy_score(y_test, predicted)    \nprint(\"Train Accuracy: %0.2f (+\/- %0.2f)\" % (results.mean(), results.std()))\nprint(\"\\nTest Accuracy: %0.2f (+\/- %0.2f)\" % (test_scores.mean(),test_scores.std()))\nprint (\"\\nTest classification_report \\n\", classification_report(y_test, predicted))","85ceac23":"seed = 123\nnum_trees = 100\nkfold = model_selection.KFold(n_splits=5, random_state=seed,shuffle=True)\nmodel_etc = ExtraTreesClassifier() \nbagging_clf = BaggingClassifier(model_etc,n_estimators=num_trees, random_state=seed).fit(X_train,y_train) \nresults = model_selection.cross_val_score(model_etc, X_train, y_train, cv=kfold,n_jobs=1)\n\npredicted = model_selection.cross_val_predict(bagging_clf,X_test,y_test,cv=kfold) \ntest_scores = accuracy_score(y_test, predicted)    \nprint(\"Train Accuracy: %0.2f (+\/- %0.2f)\" % (results.mean(), results.std()))\nprint(\"\\nTest Accuracy: %0.2f (+\/- %0.2f)\" % (test_scores.mean(),test_scores.std()))\nprint (\"\\nTest classification_report \\n\", classification_report(y_test, predicted))","8c323e24":"from IPython.display import Image\nimport os\n#!ls ..\/input\/\nImage(\"..\/input\/baggingimages\/Ensemble_Bagging_Final_Flow.PNG\")","fa230e55":"from sklearn.model_selection import cross_val_score\n\nseed = 123\n\n\n# Create classifiers\ndtc = DecisionTreeClassifier()\nrf = RandomForestClassifier()\net = ExtraTreesClassifier()\nknn = KNeighborsClassifier()\nsvc = SVC()\nrg = RidgeClassifier()\n\nclf_array = [dtc, rf, et, knn, svc, rg]\n\nfor clf in clf_array:\n    vanilla_scores = cross_val_score(clf, X_train, y_train, cv=5, n_jobs=-1)\n    bagging_clf = BaggingClassifier(base_estimator=clf,n_estimators=num_trees, random_state=seed).fit(X_train,y_train) \n    bagging_scores = model_selection.cross_val_score(bagging_clf, X_train, y_train, cv=10, n_jobs=1)#, error_score='raise')\n    print (\"Mean of: {1:.3f}, std: (+\/-) {2:.3f} [PlainVanilla {0}]\".format(clf.__class__.__name__,vanilla_scores.mean(), vanilla_scores.std()))\n    print (\"Mean of: {1:.3f}, std: (+\/-) {2:.3f} [Bagging {0}]\\n\".format(clf.__class__.__name__,bagging_scores.mean(), bagging_scores.std()))\n    predicted = model_selection.cross_val_predict(bagging_clf,X_test,y_test,cv=kfold) \n    test_scores = accuracy_score(y_test, predicted)    \n    print(\"Test Accuracy: %0.2f (+\/- %0.2f)\" % (test_scores.mean(),test_scores.std()))\n    print (\"Test classification_report \\n\", classification_report(y_test, predicted))","be3e38d6":"from IPython.display import Image\nimport os\n#!ls ..\/input\/\nImage(\"..\/input\/baggingimage\/Voting.PNG\")","5ce92bc9":"\nclf = [rf, et, knn, svc, rg]\neclf = VotingClassifier(estimators=[('Random Forests', rf), ('Extra Trees', et), ('KNeighbors', knn), ('SVC', svc), ('Ridge Classifier', rg)], voting='hard')\nfor clf, label in zip([rf, et, knn, svc, rg, eclf], ['Random Forest', 'Extra Trees', 'KNeighbors', 'SVC', 'Ridge Classifier', 'Ensemble']):\n    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n","06c70657":"<a id=\"Tableofcontents\"><\/a>\n# Table of Contents\n\n* [Introduction](#Introduction)\n* [EDA](#EDA)\n* [Data Visualizations](#DataVisualizations)\n* [Feature Engineering](#FeatureEngineering)\n* [Train and Test Split](#TrainandTestSplit)\n* [SMOTE to Balance Data](#SMOTE)\n* [Ensemble Introduction](#EnsembleIntroduction)\n* [Bagging Introduction](#BaggingIntroduction)\n* [Boot Strapping Flow](#BootStrappingFlow)\n* [Bagged Decission Trees](#BaggedDecissionTrees)\n* [Random Forest](#RandomForest)\n* [Extra Trees](#ExtraTrees)\n* [Bagging with Multiple Models](#BaggingwithMultipleModels)\n* [Voting Classifier](#VotingClassifier)\n* [References](#References)\n","426bb097":"* [Table of Contents](#Tableofcontents)\n<a id=\"DataVisualizations\"><\/a>\n\n# Data Visualizations","a21e795f":"* [Table of Contents](#Tableofcontents)\n<a id=\"BaggingwithMultipleModels\"><\/a>\n\n# Bagging with multiple models","9c655727":"* [Table of Contents](#Tableofcontents)\n<a id=\"FeatureEngineering\"><\/a>\n# Feature Engineering","0575850f":"The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result. Here\u2019s a question: If you create all the models on the same set of data and combine it, will it be useful? There is a high chance that these models will give the same result since they are getting the same input. So how can we solve this problem? One of the techniques is bootstrapping.\n\n#### Bootstrapping: Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, with replacement. The size of the subsets is the same as the size of the original set.\n\n#### Bagging: Bagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set). The size of subsets created for bagging may be less than the original set.\n\n1. Multiple subsets are created from the original dataset, selecting observations with replacement.\n2. A base model (weak model) is created on each of these subsets.\n3. The models run in parallel and are independent of each other.\n4. The final predictions are determined by combining the predictions from all the models.","1e561588":"* [Table of Contents](#Tableofcontents)\n<a id=\"References\"><\/a>\n\n# References:\n1. https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/\n2. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n3. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html?highlight=bagging#sklearn.ensemble.BaggingClassifier\n4. https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/\n5. https:\/\/machinelearningmastery.com\/ensemble-machine-learning-algorithms-python-scikit-learn\/\n6. https:\/\/machinelearningmastery.com\/bagging-ensemble-with-python\/\n7. https:\/\/towardsdatascience.com\/using-bagging-and-boosting-to-improve-classification-tree-accuracy-6d3bb6c95e5b\n8. https:\/\/medium.com\/@rrfd\/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de\n\n","0ab8aead":"* [Table of Contents](#Tableofcontents)\n<a id=\"SMOTE\"><\/a>\n# SMOTE to Balance Data","9a64c56d":"* [Table of Contents](#Tableofcontents)\n<a id=\"BootStrappingFlow\"><\/a>\n\n# Boot Strapping Flow","7d1a42ce":"* [Table of Contents](#Tableofcontents)\n<a id=\"BaggingIntroduction\"><\/a>\n\n# Bagging Introduction","3f129be1":"* [Table of Contents](#Tableofcontents)\n<a id=\"Introduction\"><\/a>\n# Introduction\n### Domain : Banking\n\n### Dataset: Loan Payment data\n\n### Attributes:\n\n1. Loan_id : A unique loan number assigned to each loan customers\n2. Loan_status: Whether a loan is paid off, in collection, new customer yet to payoff, or paid off after the collection efforts\n3. Principal: Basic principal loan amount at the origination terms, could be weekly (7 days), biweekly, and monthly payoff schedule\n4. effective_date: When the loan got originated and took effects\n5. due_date: Since it\u2019s one-time payoff schedule, each loan has one single due date\n6. paidoff_time: The actual time a customer pays off the loan\n7. pastdue_days: How many days a loan has been past due age, education, Gender: A customer\u2019s basic demographic information","f128eaf6":"* [Table of Contents](#Tableofcontents)\n<a id=\"ExtraTrees\"><\/a>\n\n# Extra Trees","00baf898":"### Main intension of developing this project is to understand the Ensemble Technique Bagging concepts. Demonstrated Bagging techniques for individual models, mulitple models and used Voting Classifer to understand the accuracy for the multi classification problem with Loan Payment Dataset.\n\n#### Note: Up to Train-Test Split used part 1 article which has published in my previous notebook.\n#### Part 1: Stacking & Blending: https:\/\/www.kaggle.com\/rameshbabugonegandla\/ensemble-tec-stacking-blending-loanpayment-project\n#### Part 2: Bagging: current notebook\/kernel\n\n#### Happy Learning!!!","e31d3238":"* [Table of Contents](#Tableofcontents)\n<a id=\"BaggedDecissionTrees\"><\/a>\n\n# Bagged Decision Trees ","2da40f53":"* [Table of Contents](#Tableofcontents)\n<a id=\"VotingClassifier\"><\/a>\n\n# Voting Classifier","986f5acc":"* [Table of Contents](#Tableofcontents)\n<a id=\"RandomForest\"><\/a>\n\n# Random Forest","a0513b61":"* [Table of Contents](#Tableofcontents)\n<a id=\"TrainandTestSplit\"><\/a>\n# Train and Test Split","26bc26d9":"* [Table of Contents](#Tableofcontents)\n<a id=\"EnsembleIntroduction\"><\/a>\n# Ensemble Introduction\n\n## What is Ensemble Learning\n**Wikipedia Defination:** Ensemble learning is a type of machine learning that studies algorithms and architectures that build collections, or ensembles, of statistical classifiers that are more accurate than a single classifier.\n                                                            (or)\nIn statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n\nLet\u2019s understand the concept of ensemble learning with an example. Suppose you are a movie director and you have created a short movie on a very important and interesting topic. Now, you want to take preliminary feedback (ratings) on the movie before making it public. What are the possible ways by which you can do that?\n\nA: You may ask one of your friends to rate the movie for you.\n\nNow it\u2019s entirely possible that the person you have chosen loves you very much and doesn\u2019t want to break your heart by providing a 1-star rating to the horrible work you have created.\n\nB: Another way could be by asking 5 colleagues of yours to rate the movie.\n\nThis should provide a better idea of the movie. This method may provide honest ratings for your movie. But a problem still exists. These 5 people may not be \u201cSubject Matter Experts\u201d on the topic of your movie. Sure, they might understand the cinematography, the shots, or the audio, but at the same time may not be the best judges of dark humour.\n\nC: How about asking 50 people to rate the movie?\n\nSome of which can be your friends, some of them can be your colleagues and some may even be total strangers.\n\nThe responses, in this case, would be more generalized and diversified since now you have people with different sets of skills. And as it turns out \u2013 this is a better approach to get honest ratings than the previous cases we saw.\n\nWith these examples, you can infer that a diverse group of people are likely to make better decisions as compared to individuals. Similar is true for a diverse set of models in comparison to single models. This diversification in Machine Learning is achieved by a technique called Ensemble Learning.","9ecbec8c":"* [Table of Contents](#Tableofcontents)\n<a id=\"EDA\"><\/a>\n\n# EDA"}}