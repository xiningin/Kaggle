{"cell_type":{"d666db34":"code","e7ccc11a":"code","a1e88f16":"code","d69ae5e2":"code","d8eaf7ce":"code","204c7620":"code","d6fea32a":"code","edb4c7cc":"code","579b445a":"code","7aca859f":"code","9d9dbfc1":"code","fea85998":"code","d9ac8fca":"code","5d0005f1":"code","17a35038":"code","14c710e9":"code","a903fe2a":"code","d92f3dc9":"code","549be57b":"code","d274188b":"code","dd89de64":"code","85cb6d29":"code","65790866":"code","e2af7ae0":"code","cb517758":"markdown","939f9afa":"markdown","16d4e5d5":"markdown","bd803fae":"markdown","855c9e62":"markdown","2b12ce2a":"markdown","218d5ab1":"markdown","02227166":"markdown"},"source":{"d666db34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e7ccc11a":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\n\ndata = pd.concat([train, test], sort=False)","a1e88f16":"from sklearn.preprocessing import LabelEncoder\n#data = pd.concat([train, test], sort=False)\ntrain['train_or_test'] = 'train'\ntest['train_or_test'] = 'test'\ndata = pd.concat(\n    [\n        train,\n        test\n    ],\n    sort=False\n).reset_index(drop=True)\n\n\n#data['Sex'].replace(['male', 'female'], [0, 1], inplace=True)\ndata['Embarked'].fillna(data.Embarked.mode()[0], inplace=True)\n#data['Embarked'] = data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2}).astype(int)\ndata['Fare'].fillna(data.Fare.median(), inplace=True)\n#data['Age'].fillna(data['Age'].median(), inplace=True)\n#\u6b7b\u3093\u3067\u308b\u304b\u3089\u6b20\u640d\u3057\u3066\u308b\u53ef\u80fd\u6027\u3082\u3042\u308b\u306e\u3067\u6bd4\u8f03\u7684\u306b\u4f53\u529b\u7684\u306b\u751f\u304d\u6b8b\u308a\u306b\u304f\u305d\u3046\u306a\u6700\u5e74\u9577\u306e\u5e74\u9f62\u306b\u3057\u3066\u307f\u305f\u3002\n#data['Age'].fillna(data['Age'].max(), inplace=True)\n#\u3068\u3044\u3046\u304b\u3001Age\u3092\u57cb\u3081\u308b\u3053\u3068\u81ea\u4f53\u4e3b\u89b3\u3067\u30ce\u30a4\u30ba\u3092\u5165\u308c\u308b\u304b\u3089Age\u3092\u4f7f\u308f\u306a\u3044\u3002\n\ndata['FamilySize'] = data['Parch'] + data['SibSp'] + 1\n\n#data['IsAlone'] = 0\n\n#'FamilySize'\u304c\u30011\u306e\u3082\u306e\u306e\"\u884c\"\u306b\u5bfe\u3057\u3066\u3001\u306e'IsAlone'\"\u5217\"\u306b\u5bfe\u3057\u3066\u30011\u3092\u4ee3\u5165\u3057\u3066\u3044\u308b\u3002\n#\u3061\u306a\u307f\u306b\u300cloc\u300d\u306f\u884c\u3068\u5217\u306e\u30e9\u30d9\u30eb\u306b\u5bfe\u3057\u3066\u306e\u64cd\u4f5c\u304c\u884c\u3048\u308b\u3001\n#\u300ciloc\u300d\u3068\u3044\u3046\u3082\u306e\u3082\u3042\u308a\u3001\u3053\u308c\u306f\u884c\u5217\u306e\u756a\u53f7\u306b\u5bfe\u3057\u3066\u3001\u64cd\u4f5c\u3092\u884c\u3048\u308b\u3002\n#\u300cix\u300d\u306f\u3069\u3063\u3061\u3082\u3067\u304d\u308b\n\n#\u304a\u307e\u3051\u300cat\u300d\u300ciat\u300d\u306f\u5358\u72ec\u8981\u7d20\u306b\u5bfe\u3057\u3066\n#data.loc[data['FamilySize'] == 1, 'IsAlone'] = 1\n\n\ndata['FamilySize_bin'] = 'big'\ndata.loc[data['FamilySize'] == 1, 'FamilySize_bin'] = 'alone'\ndata.loc[(data['FamilySize'] >= 2) & (data['FamilySize']<=4), 'FamilySize_bin'] = 'small'\ndata.loc[(data['FamilySize'] >= 5) & (data['FamilySize']<=7), 'FamilySize_bin'] = 'mediam'\n\n\n\n\n\ndata.loc[:, 'TicketFreq'] = data.groupby(['Ticket'])['PassengerId'].transform('count')\n\ndata['Cabin_ini'] = data['Cabin'].map(lambda x:str(x)[0])\ndata['Cabin_ini'].replace(['G', 'T'], 'Rare', inplace=True)\n\ndata.Pclass = data.Pclass.astype('str')\n\ndata['honorific'] = data['Name'].map(lambda x: x.split(',') [1].split('.')[0])\ndata['honorific'].replace(['Col', 'Dr', 'Rev'], 'Rare', inplace=True)\ndata['honorific'].replace('Mlle', 'Miss', inplace=True)\ndata['honorific'].replace('Ms', 'Miss', inplace=True)\n\n\ndata.loc[:, 'Fare_bin'] = pd.qcut(data.Fare, 14)\n\nle_target_col = ['Sex','Fare_bin']\nle = LabelEncoder()\nfor col in le_target_col:\n    data.loc[:, col] = le.fit_transform(data[col])\n\ncat_col = ['Embarked','FamilySize_bin', 'Pclass','Cabin_ini', 'honorific', 'Fare_bin']\ndata=pd.get_dummies(data, drop_first=True, columns=cat_col)","d69ae5e2":"data.head()","d8eaf7ce":"#delete_columns = ['Name', 'PassengerId', 'Ticket', 'Cabin']\n#\u3053\u3053\u304c\u3081\u3063\u3061\u3083\u5927\u4e8b\u3001\u3053\u3053\u3067\u30ab\u30e9\u30e0\u3092\u4f7f\u3046\u305f\u3081\u306b\u4f7f\u3063\u305fParch,SibSp\u3068\u304b\u306e\u305d\u306e\u30ab\u30e9\u30e0\u5358\u4f53\u3067\u5f79\u306b\u7acb\u305f\u306a\u3044\u3082\u306e\u3092\u6368\u3066\u308b\u3002\n#\u307e\u308b\u3067\u30ed\u30b8\u30ab\u30eb\u30d7\u30ec\u30bc\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u306eMECE\u306e\u3088\u3046\u3060\u2026\uff01\u3082\u308c\u306a\u304f\u30c0\u30d6\u308a\u306a\u304f\u306e\u5927\u5207\u3055\u2026\n#\u4eca\u5f8c\u306f\u3053\u306e\u524d\u51e6\u7406\u3092\u4f7f\u3063\u3066\u3001\u6b8b\u308a\u306e\u624b\u6cd5\u3082\u4f7f\u3044\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u5b66\u7fd2\u3059\u308b\u3002\ndelete_columns = ['Name', 'PassengerId', 'Ticket', 'Cabin','Age', 'Fare','Parch','SibSp','FamilySize']\ndata.drop(delete_columns, axis=1, inplace=True)\n\n#train = data[:len(train)]\n#test = data[len(train):]\n\ntrain = data.query('train_or_test == \"train\"')\ntest = data.query('train_or_test == \"test\"')\n\ntrain.drop('train_or_test', axis=1, inplace=True)\ntest.drop('train_or_test', axis=1, inplace=True)\n\n\ny_train = train['Survived']\nX_train = train.drop('Survived', axis=1)\nX_test = test.drop('Survived', axis=1)","204c7620":"X_train.head()","d6fea32a":"y_train.head()","edb4c7cc":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngs1 = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n                  param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}],\n                  scoring='accuracy',\n                  cv=2)\nscores = cross_val_score(gs1, X_train, y_train, \n                         scoring='accuracy', cv=5)\nprint('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), \n                                      np.std(scores)))","579b445a":"'''\nfrom sklearn.ensemble import RandomForestClassifier\n\ngs2 = GridSearchCV(estimator=RandomForestClassifier(random_state=0),\n                  param_grid=[{ \"max_depth\": [6,7,8,9,10,11,12,13,14],\n                               \"min_samples_leaf\": [1,2,3,4,5],\n                               \"max_features\": [4,5,6,7,8,9]}],\n                  scoring='accuracy',\n                  cv=2)\nscores = cross_val_score(gs2, X_train, y_train, \n                         scoring='accuracy', cv=5)\nprint('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), \n                                      np.std(scores)))\n'''","7aca859f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npipe_svc  = make_pipeline(StandardScaler(),LogisticRegression(random_state=0))\ngs3 = GridSearchCV(estimator=pipe_svc,\n                  param_grid=[{ \"logisticregression__penalty\": ['l2'],\n                               \"logisticregression__C\": [0.001,0.01,0.1,1,10]},\n                              {\"logisticregression__penalty\":['l1'],\n                               \"logisticregression__C\":[0.001,0.01,0.1,1,10],\n                               \"logisticregression__solver\":['liblinear']}],\n                  scoring='accuracy',\n                  cv=2)\nscores = cross_val_score(gs3, X_train, y_train, \n                         scoring='accuracy', cv=5)\nprint('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), \n                                      np.std(scores)))","9d9dbfc1":"'''\ngs = gs2.fit(X_train,y_train)\nprint(gs.best_params_)\n'''","fea85998":"'''\nbest_clf = RandomForestClassifier(max_depth=7,max_features=6,min_samples_leaf=1,random_state=0)\nbest_clf.fit(X_train, y_train)\nbest_y_pred = best_clf.predict(X_test)\n'''","d9ac8fca":"'''\nbest_y_pred[:10]\n'''","5d0005f1":"'''\nsub = pd.DataFrame(pd.read_csv('..\/input\/titanic\/test.csv')['PassengerId'])\nsub['Survived'] = list(map(int, best_y_pred))\nsub.to_csv('sub_randomforest_10.csv', index=False)\n'''","17a35038":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=0, stratify=y_train)","14c710e9":"import optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import log_loss\n\n'''\ndef objective(trial):\n    params = {\n        'objective': 'binary',\n        'max_bin': trial.suggest_int('max_bin', 255, 500),\n        'learning_rate': 0.01,\n        'num_leaves': trial.suggest_int('num_leaves', 32, 128)\n    }\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n    model = lgb.train(\n        params, lgb_train,\n        valid_sets=[lgb_train, lgb_eval],\n        verbose_eval=10,\n        num_boost_round=1000,\n        early_stopping_rounds=10\n    )\n\n    y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n    score = log_loss(y_valid, y_pred_valid)\n    return score\n'''","a903fe2a":"'''\nstudy = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\nstudy.optimize(objective, n_trials=100)\n'''","d92f3dc9":"'''\nparams = {\n    'objective': 'binary',\n    'max_bin': study.best_params['max_bin'],\n    'learning_rate': 0.01,\n    'num_leaves': study.best_params['num_leaves']\n}\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\nmodel = lgb.train(\n    params, lgb_train,\n    valid_sets=[lgb_train, lgb_eval],\n    verbose_eval=10,\n    num_boost_round=1000,\n    early_stopping_rounds=10\n)\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n'''","549be57b":"'''\nsub = gender_submission\ny_pred = (y_pred > 0.5).astype(int)\nsub['Survived'] = y_pred\nsub.to_csv(\"submission_lightgbm_optuna.csv\", index=False)\n\nsub.head()\n'''","d274188b":"'''\nsub = pd.DataFrame(pd.read_csv('..\/input\/titanic\/test.csv')['PassengerId'])\nsub['Survived'] = list(map(int, y_pred))\nsub.to_csv('submission.csv', index=False)\n'''","dd89de64":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport xgboost as xgb\n\n#Objective\u95a2\u6570\u306e\u8a2d\u5b9a\ndef objective(trial):\n    params = {\n        'objective': 'binary:logistic',\n        'max_depth': trial.suggest_int('max_depth', 1, 9),\n        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n        'learning_rate': 0.01\n\n    }\n    model = xgb.XGBClassifier(**params)\n    model.fit(X_train, y_train)\n\n    pred = model.predict(X_valid)\n\n    accuracy = accuracy_score(y_valid, pred)\n    return (1-accuracy)\n\nif __name__ == '__main__':\n\n    study = optuna.create_study()\n    study.optimize(objective, n_trials=100)\n\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)","85cb6d29":"    params = {\n        'objective': 'binary:logistic',\n        'max_depth': 2,\n        'n_estimators': 706,\n        'learning_rate': 0.01\n\n    }\n    model = xgb.XGBClassifier(**params)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)","65790866":"sub = gender_submission\ny_pred = (y_pred > 0.5).astype(int)\nsub['Survived'] = y_pred\nsub.to_csv(\"submission_xgboost_optuna.csv\", index=False)\n\nsub.head()","e2af7ae0":"sub = pd.DataFrame(pd.read_csv('..\/input\/titanic\/test.csv')['PassengerId'])\nsub['Survived'] = list(map(int, y_pred))\nsub.to_csv('submission.csv', index=False)","cb517758":"\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30","939f9afa":"\u4e00\u756aaccuracy\u304c\u9ad8\u3044\u306e\u304c\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3060\u3063\u305f\u306e\u3067\u5229\u7528\u3057\u3066\u307f\u308b","16d4e5d5":"\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8","bd803fae":"![](http:\/\/)\u3053\u3053\u307e\u3067\u306f\u95a2\u5ca1\u541b\u3068\u540c\u3058\n\n\u3053\u308c\u4ee5\u964d\u306f\u4f7f\u3046\u6a5f\u68b0\u5b66\u7fd2\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u9078\u629e\u3059\u308b","855c9e62":"\u4eca\u5ea6\u306fLightGBM\u3068XGBoost\u3092Optuna\u3067\u6700\u9069\u5316\u3057\u3066\u6027\u80fd\u306e\u9055\u3044\u3092\u898b\u3066\u307f\u308b","2b12ce2a":"\u307e\u305a\u306f\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\u3092\u7528\u3044\u305f\u5165\u308c\u5b50\u5f0f\u4ea4\u5dee\u691c\u8a3c\u3092\u7528\u3044\u3066\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u9078\u629e\u3057\u3066\u307f\u308b(\u7dd1\u306e\u672cP198)","218d5ab1":"\u6c7a\u5b9a\u6728","02227166":"\u51e6\u7406\u306b\u3081\u3063\u3061\u3083\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u306b\u3057\u3066\u304a\u304f\n\u3061\u306a\u307f\u306baccuracy\u306f0.83\u3060\u3063\u305f"}}