{"cell_type":{"7af487d5":"code","9d32243e":"code","404c7a44":"code","40545a2b":"code","dafce88f":"code","d7a332f3":"code","9e9ad3d9":"code","deb77bfe":"code","bc76e88d":"code","8673ac71":"code","1fa18d29":"code","2a0c393a":"code","341b7d83":"code","71dee741":"code","1ffb8272":"code","c5a41280":"code","9bf9b9ed":"code","d81b24b7":"code","08c35eb3":"code","54423b0e":"code","a893d27d":"code","d9249c50":"code","7ea57163":"code","ebd359e5":"code","95f0c8b3":"code","824662ce":"code","dea46a62":"code","89b5cc99":"code","ed5b37ed":"code","00c7ae5e":"code","42aea67d":"code","f32b4a9d":"code","1523d4d1":"code","778e764c":"code","f8975d75":"code","7717c8f9":"code","522a7498":"code","eec7d41a":"code","9627793f":"code","a0556f0c":"code","4615f355":"code","40c10a3f":"code","ce278060":"code","92603b91":"code","1cfb6254":"code","2d2d350b":"code","d0c37850":"code","4313f9b1":"code","a5e616b8":"code","2843b788":"code","9bb328b1":"code","067c664f":"code","fafdd887":"code","fdb92b25":"code","b724f45b":"code","296380df":"code","4515bbbf":"code","836b1d8f":"code","6091fa83":"code","b2b9d7bf":"code","84ae1d38":"code","7b416cf3":"code","35834add":"code","58483b9e":"code","61125bff":"code","7859e4ca":"code","eb025042":"code","dfddd8b7":"code","c1707f1b":"code","8b5cca90":"code","d6b8016d":"code","da3d2991":"code","4dbf875b":"code","e8560181":"code","4af2eafc":"code","caf1dba2":"code","730b9abd":"code","57017a8b":"code","1c61f16e":"code","d613902a":"code","6bc0b596":"code","db5bed32":"code","22bc06c3":"markdown","e96e00e4":"markdown","37fdb0ef":"markdown","e28ad03e":"markdown","97358fc6":"markdown","c12653ad":"markdown","13452181":"markdown","1cda267c":"markdown","b953b820":"markdown","13e71b31":"markdown","a6a37ebf":"markdown","14d2828a":"markdown","d88db75c":"markdown","b3c7ba81":"markdown","c4fd69c4":"markdown","93dc165c":"markdown","4bb81a98":"markdown","083818f4":"markdown","edf86f27":"markdown","4a85f8af":"markdown","e4e18dc0":"markdown","1b213752":"markdown","62b84bc3":"markdown","481dc3fc":"markdown","3bc3ae11":"markdown","d5ffa2b4":"markdown","e75fa59e":"markdown","639c50aa":"markdown","fe4b5b23":"markdown","e90e82ab":"markdown","2d42172b":"markdown","dbe6ce8c":"markdown","6d3e4d2b":"markdown","8aa0d347":"markdown","6b38fec6":"markdown","c782b9ed":"markdown","03c23966":"markdown","8cdcbc8f":"markdown","1344d5fa":"markdown","1771e725":"markdown","4b6c9c24":"markdown","02ccd4f4":"markdown","13f82a3b":"markdown","cc4b1164":"markdown","97201a5a":"markdown","3a080615":"markdown","26fa2639":"markdown","2fd98c7e":"markdown","6abc35ab":"markdown","6ed8550a":"markdown","66b340f7":"markdown","d18d56db":"markdown","de5f9ef5":"markdown","bb751a24":"markdown","72c6d1cc":"markdown","36ebe0c3":"markdown","8579f08f":"markdown","5e53f6dc":"markdown","94029ee4":"markdown","33c30496":"markdown","08a6cedf":"markdown","9c6cf676":"markdown","ab15a0d0":"markdown","73ece37f":"markdown","83dd0278":"markdown","056e1e95":"markdown","45e25167":"markdown","6518bec5":"markdown","199a1d84":"markdown","6cdd3a09":"markdown"},"source":{"7af487d5":"import os\nos.listdir('..\/input\/people-interactive-assignment')","9d32243e":"! pip install --upgrade pip\n# ! pip install autoplotter\n! pip install git+git:\/\/github.com\/thelittlebug\/autoplotter\n! pip install sweetviz dataprep nbconvert featuretools matplotlib","404c7a44":"import numpy as np\nimport pandas as pd\n# from autoplotter import run_app\nimport sweetviz as sv\nfrom dataprep.eda import plot, plot_correlation, plot_missing, create_report\nfrom sklearn.preprocessing import OneHotEncoder\nimport warnings\nwarnings.filterwarnings('ignore')","40545a2b":"df = pd.read_csv('..\/input\/people-interactive-assignment\/Dataset.txt', sep=\"\\t\", header=0, index_col='Index')\ndf.head()","dafce88f":"test_df = pd.read_csv('..\/input\/people-interactive-assignment\/Dataset_test.txt', sep=\"\\t\", header=0, index_col='Index')\ntest_df.head()","d7a332f3":"# Stats for the given data\ndf.describe()","9e9ad3d9":"# Checking for NULL Values in train data\ndf.info()","deb77bfe":"# Checking for NULL Values in train data\ntest_df.info()","bc76e88d":"# Converting column F15 and F16 to date column since they are present as string type\ndf[[\"F15\", \"F16\"]] = df[[\"F15\", \"F16\"]].apply(pd.to_datetime)\ntest_df[[\"F15\", \"F16\"]] = test_df[[\"F15\", \"F16\"]].apply(pd.to_datetime)","8673ac71":"df.head()","1fa18d29":"# Number of Unique Elements in each column for train data\ndf.T.apply(lambda x: x.nunique(), axis=1)","2a0c393a":"# Number of Unique Elements in each column for test data\ntest_df.T.apply(lambda x: x.nunique(), axis=1)","341b7d83":"# Converting F17, F18, F21, and F22 to string type to get counts of it and for treating them as categorical column\ndf[[\"F17\", \"F18\", \"F21\", \"F22\"]] = df[[\"F17\", \"F18\", \"F21\", \"F22\"]].astype('category')\ntest_df[[\"F17\", \"F18\", \"F21\", \"F22\"]] = test_df[[\"F17\", \"F18\", \"F21\", \"F22\"]].astype('category')","71dee741":"# Summary Report for train data\ntrain_report = sv.analyze(df, target_feat='C')\ntrain_report.show_html() # Default arguments will generate to \"SWEETVIZ_REPORT.html\"","1ffb8272":"# Summary Report for train-test data\ntrain_test_report = sv.compare([df.iloc[:,:-1], \"Training Data\"], [test_df, \"Test Data\"], feat_cfg= sv.FeatureConfig(skip=['F15','F16']))\ntrain_test_report.show_html() # Default arguments will generate to \"SWEETVIZ_REPORT.html\"","c5a41280":"# Dataset stats and distribution\nplot(df)","9bf9b9ed":"column_list = list(df.columns)","d81b24b7":"plot(df, column_list[0])","08c35eb3":"plot(df, column_list[1])","54423b0e":"plot(df, column_list[2])","a893d27d":"plot(df, column_list[3])","d9249c50":"plot(df, column_list[4])","7ea57163":"plot(df, column_list[5])","ebd359e5":"plot(df, column_list[6])","95f0c8b3":"plot(df, column_list[7])","824662ce":"plot(df, column_list[8])","dea46a62":"plot(df, column_list[9])","89b5cc99":"plot(df, column_list[10])","ed5b37ed":"plot(df, column_list[11])","00c7ae5e":"plot(df, column_list[12])","42aea67d":"plot(df, column_list[13])","f32b4a9d":"plot(df, column_list[16])","1523d4d1":"plot(df, column_list[17])","778e764c":"plot(df, column_list[18])","f8975d75":"plot(df, column_list[19])","7717c8f9":"plot(df, column_list[20])","522a7498":"plot(df, column_list[21])","eec7d41a":"# Calculating Correlation for the given dataset\ndf.corr()","9627793f":"plot_correlation(df)","a0556f0c":"create_report(df)","4615f355":"numerical_columns = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11', 'F12', 'F13', 'F14', 'F19', 'F20']\ndf[numerical_columns].head()","40c10a3f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf[numerical_columns]=scaler.fit_transform(df[numerical_columns])\ntest_df[numerical_columns]=scaler.transform(test_df[numerical_columns])\ndf[numerical_columns].head()","ce278060":"df.info()","92603b91":"# Converting 'F17', 'F18', 'F21', 'F22' to one-hot encoded form\nprocessed_df = pd.get_dummies(df, columns=['F17', 'F18', 'F21', 'F22'], drop_first=True)\nprocessed_test_df = pd.get_dummies(test_df, columns=['F17', 'F18', 'F21', 'F22'], drop_first=True)","1cfb6254":"processed_df.head()","2d2d350b":"# Drop Date Columns\nprocessed_df=processed_df.drop(['F15','F16'], axis=1)\nprocessed_test_df=processed_test_df.drop(['F15','F16'], axis=1)","d0c37850":"from sklearn.model_selection import StratifiedShuffleSplit\n# Here we split the data into training and test sets and implement a stratified shuffle split.\nstratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=40)\n\nfor train_set, validation_set in stratified.split(processed_df, processed_df[\"C\"]):\n    stratified_train = processed_df.reindex(index = train_set)\n    stratified_validation = processed_df.reindex(index = validation_set)\n    \nstratified_train[\"C\"].value_counts()\/len(stratified_train)","4313f9b1":"stratified_validation[\"C\"].value_counts()\/len(stratified_validation)","a5e616b8":"train_data = stratified_train.copy() # Make a copy of the stratified training set.\ntest_data = stratified_validation.copy()\nprint(train_data.shape)\nprint(test_data.shape)","2843b788":"train_data['C'].value_counts()","9bb328b1":"train_data[:] = np.nan_to_num(train_data)\ntest_data[:] = np.nan_to_num(test_data)","067c664f":"X_train=train_data.drop(['C'], axis=1)\ny_train=train_data.C\nX_test=test_data.drop('C', axis=1)\ny_test=test_data.C","fafdd887":"from sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nbaselog_model = LogisticRegression()\nbaselog_model.fit(X_train,y_train)\ny_pred = baselog_model.predict(X_test)\nprint(accuracy_score(y_pred,y_test))","fdb92b25":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\ndict_of_algos={'LR':LogisticRegression(),'svc':SVC(),'KNC':KNeighborsClassifier(),'DT':tree.DecisionTreeClassifier(),'MLPc':MLPClassifier(),\n               'GRBC':GradientBoostingClassifier(),'RFC':RandomForestClassifier(),'GNB':GaussianNB()}","b724f45b":"def accuracy_of_algos(dict_of_algos):\n    dict_of_accuracy={}\n    for k,v in dict_of_algos.items():\n        v.fit(X_train,y_train)\n        y_pred = v.predict(X_test)\n        dict_of_accuracy[k] = accuracy_score(y_pred,y_test)\n        y=v.score(X_train,y_train)\n    return dict_of_accuracy\n\nprint(accuracy_of_algos(dict_of_algos)) ","296380df":"import time\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\ndict_classifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Nearest Neighbors\": KNeighborsClassifier(),\n    \"Linear SVM\": SVC(),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n    \"Decision Tree\": tree.DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(n_estimators=18),\n    \"Naive Bayes\": GaussianNB()\n}","4515bbbf":"no_classifiers = len(dict_classifiers.keys())\n\ndef batch_classify(X_train, Y_train,X_test, verbose = True):\n    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,4)), columns = ['classifier', 'train_score', 'training_time','test_score'])\n    count = 0\n    for key, classifier in dict_classifiers.items():\n        t_start = time.clock()\n        classifier.fit(X_train, Y_train)\n        t_end = time.clock()\n        t_diff = t_end - t_start\n        train_score = classifier.score(X_train, Y_train)\n        \n        y_pred=classifier.predict(X_test)\n        test_score=accuracy_score(y_test,y_pred)\n        \n        df_results.loc[count,'classifier'] = key\n        df_results.loc[count,'train_score'] = train_score\n        df_results.loc[count,'training_time'] = t_diff\n        df_results.loc[count,'test_score']=test_score\n        if verbose:\n            print(\"trained {c} in {f:.2f} s\".format(c=key, f=t_diff))\n        count+=1\n    return df_results","836b1d8f":"df_results = batch_classify(X_train, y_train,X_test)\nprint(df_results.sort_values(by='train_score', ascending=False))","6091fa83":"# Use Cross-validation\nfrom sklearn.model_selection import cross_val_score\n\n# Logistic Regression\nlog_reg = LogisticRegression()\nlog_scores = cross_val_score(log_reg, X_train, y_train, cv=3)\nlog_reg_mean = log_scores.mean()","b2b9d7bf":"# KNearestNeighbors\nknn_clf = KNeighborsClassifier()\nknn_scores = cross_val_score(knn_clf, X_train, y_train, cv=3)\nknn_mean = knn_scores.mean()","84ae1d38":"# Decision Tree\ntree_clf = tree.DecisionTreeClassifier()\ntree_scores = cross_val_score(tree_clf, X_train, y_train, cv=3)\ntree_mean = tree_scores.mean()","7b416cf3":"# Gradient Boosting Classifier\ngrad_clf = GradientBoostingClassifier()\ngrad_scores = cross_val_score(grad_clf, X_train, y_train, cv=3)\ngrad_mean = grad_scores.mean()","35834add":"# Random Forest Classifier\nrand_clf = RandomForestClassifier(n_estimators=18)\nrand_scores = cross_val_score(rand_clf, X_train, y_train, cv=3)\nrand_mean = rand_scores.mean()","58483b9e":"# Naives Bayes\nnav_clf = GaussianNB()\nnav_scores = cross_val_score(nav_clf, X_train, y_train, cv=3)\nnav_mean = nav_scores.mean()","61125bff":"# Create a Dataframe with the results.\nd = {'Classifiers': ['Logistic Reg.', 'KNN', 'Dec Tree', 'Grad B CLF', 'Rand FC', 'Naives Bayes'], \n    'Crossval Mean Scores': [log_reg_mean, knn_mean, tree_mean, grad_mean, rand_mean, nav_mean]}\n\nresult_df = pd.DataFrame(data=d)","7859e4ca":"# All our models perform well but I will go with GradientBoosting.\nresult_df = result_df.sort_values(by=['Crossval Mean Scores'], ascending=False)\nresult_df","eb025042":"from sklearn.metrics import accuracy_score\n\ngrad_clf = GradientBoostingClassifier()\ngrad_clf.fit(X_train, y_train)","dfddd8b7":"y_pred=grad_clf.predict(X_test)\nprint (\"Gradient Boost Classifier Train accuracy is %2.2f\" % accuracy_score(y_test, y_pred))","c1707f1b":"y_test.value_counts()","8b5cca90":"from matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n# 4697: no's, 4232: yes\nconf_matrix = confusion_matrix(y_test, y_pred)\nf, ax = plt.subplots(figsize=(10, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', linewidths=.5, ax=ax)\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.subplots_adjust(left=0.15, right=0.99, bottom=0.15, top=0.99)\nax.set_yticks(np.arange(conf_matrix.shape[0]) + 0.5, minor=False)\nax.set_xticklabels([\"Predicted False\",'Predicted True'])\nax.set_yticklabels(['Actual False', 'Actual True'], fontsize=16, rotation=360)\nplt.show()","d6b8016d":"from sklearn.metrics import precision_score, recall_score\n\n# The model is only retaining 60% of clients that agree to suscribe a term deposit.\nprint('The model is {c} % sure that the potential client will buy'.format(c=np.round(precision_score(y_test, y_pred),2)*100))\nprint('The model is retaining {c} % of clients that agree to buy'.format(c=np.round(recall_score(y_test, y_pred),2)*100))","da3d2991":"from sklearn.metrics import f1_score\n\nf1_score(y_test, y_pred)*100","4dbf875b":"from sklearn.metrics import precision_recall_curve\nplt.figure(figsize=(14,8))\ny_prob=grad_clf.predict_proba(X_test)[:,1]\nprecisions, recalls, threshold = precision_recall_curve(y_test, y_prob)\nplt.plot(threshold,recalls[:-1],marker='.',label='recall')\nplt.plot(threshold,precisions[:-1],marker='.',label='precision')\nplt.legend(frameon=True,fontsize=20)\nplt.axvline(x=0.354,c='black')\n\nplt.show()","e8560181":"from sklearn.metrics import roc_curve\ngrd_fpr, grd_tpr, threshold = roc_curve(y_test, y_prob)\n","4af2eafc":"def graph_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.figure(figsize=(10,6))\n    plt.title('ROC Curve \\n Gradient Boosting Classifier', fontsize=18)\n    plt.plot(false_positive_rate, true_positive_rate, label=label)\n    plt.plot([0, 1], [0, 1], '#0C8EE0')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('ROC Score of 71.54% \\n ', xy=(0.43, 0.74), xytext=(0.50, 0.67),\n            arrowprops=dict(facecolor='#F75118', shrink=0.05),\n            )\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#F75118', shrink=0.05),\n                )\n    \n    \ngraph_roc_curve(grd_fpr, grd_tpr, threshold)\nplt.show()","caf1dba2":"from sklearn.metrics import roc_auc_score\n\nprint('Gradient Boost Classifier Score: ', roc_auc_score(y_test, y_prob))","730b9abd":"from sklearn.model_selection import GridSearchCV\n\nparam_test3 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1000,2000), 'min_samples_leaf':range(30,71,10),'n_estimators':range(20,81,10)}\ngsearch = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1,\n                                                               max_features='sqrt', subsample=0.8, random_state=10),\n                        param_grid = param_test3, scoring='accuracy',n_jobs=4,iid=False, cv=5)\n\ngsearch.fit(X_train,y_train)\ngsearch.best_params_, gsearch.best_score_","57017a8b":"processed_df['predicted_C'] = gsearch.best_estimator_.predict(processed_df.drop(['C'], axis=1))","1c61f16e":"accuracy_score(processed_df['predicted_C'], processed_df['C'])","d613902a":"processed_test_df['predicted_C'] = gsearch.best_estimator_.predict(processed_test_df)","6bc0b596":"processed_test_df.head()","db5bed32":"processed_df.to_csv(\"train_dataset_pred.csv\", sep=\"\\t\")\nprocessed_test_df.to_csv(\"test_dataset_pred.csv\", sep=\"\\t\")","22bc06c3":"#### F2","e96e00e4":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 3.36*10^7\n2. Histogram:\n    - Noramlly distributed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed. We can observe a peak when F2 value is around -1500\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","37fdb0ef":"#### F10","e28ad03e":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 3.33*10^19\n2. Histogram:\n    - Noramlly distributed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed.\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","97358fc6":"#### F12","c12653ad":"Here, we can observe there are no null values in the train data","13452181":"### Installing Packages","1cda267c":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 0.08\n2. Histogram:\n    - Noramlized values between 0 and 1\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","b953b820":"From this correlation plot we can observe:\n- `F17` and `F19` are positively correlated with each other\n- `F18` and `F20` are also positively correlated with each other\n- Columns `F17`, `F18`, `F19`, and `F20` are negatively correlated with target column `C`\n- `F4` is slightly positively correlated with target column `C`\n- `F2` is slightly negatively correlated with target column `C`","13e71b31":"###### ROC Curve (Receiver Operating Characteristic):\nThe ROC curve tells us how well our classifier is classifying between True Positives and True Negatives. The X-axis is represented by False positive rates (Specificity) and the Y-axis is represented by the True Positive Rate (Sensitivity.) As the line moves the threshold of the classification changes giving us different values. The closer is the line to our top left corner the better is our model separating both classes.","a6a37ebf":"##### Let's Create A baseline model for Benchmarking","14d2828a":"#### F6","d88db75c":"1. Stats:\n    - No missing values\n    - 5 Unique Values\n2. Bar Chart:\n    - Count of `1` is highest followed by `2` with count of `4` being the lowest\n3. Pie Chart:\n    - Distribution of `1` is approx `80%` where other values comprises of remaining `20%`\n    \nNote- Ignore Word Cloud, Word Frequencies and Word Length in this case","b3c7ba81":"#### Avoiding Overfitting:\nBrief Description of Overfitting\nThis is an error in the modeling algorithm that takes into consideration random noise in the fitting process rather than the pattern itself. You can see that this occurs when the model gets an awsome score in the training set but when we use the test set (Unknown data for the model) we get an awful score. This is likely to happen because of overfitting of the data (taking into consideration random noise in our pattern). What we want our model to do is to take the overall pattern of the data in order to correctly classify.\n\nHow can we avoid Overfitting?\nThe best alternative to avoid overfitting is to use cross validation. Taking the training test and splitting it. For instance, if we split it by 3, 2\/3 of the data or 66% will be used for training and 1\/3 33% will be used or testing and we will do the testing process three times. This algorithm will iterate through all the training and test sets and the main purpose of this is to grab the overall pattern of the data.","c4fd69c4":"### Dataset Summary Report using SWEETVIZ","93dc165c":"#### F18","4bb81a98":"1. Stats:\n    - No missing values\n    - 21 Unique Values\n2. Bar Chart:\n    - Count of `1` is highest followed by `2` \n3. Pie Chart:\n    - Distribution of `1` is approx `41.08%` where other values comprises of remaining `49%`\n    \nNote- Ignore Word Cloud, Word Frequencies and Word Length in this case","083818f4":"#### F8","edf86f27":"#### F1","4a85f8af":"#### F17","e4e18dc0":"#### F20","1b213752":"##### In order to maintain the Same Ratio in our Train and Validation set we will use stratified Sampling technique","62b84bc3":"1. Stats:\n    - No missing values\n    - 5 Unique Values\n2. Bar Chart:\n    - Count of `1` is highest followed by `2` with count of `4` being the lowest\n3. Pie Chart:\n    - Distribution of `1` is approx `80%` where other values comprises of remaining `20%`\n    \nNote- Ignore Word Cloud, Word Frequencies and Word Length in this case","481dc3fc":"#### F11","3bc3ae11":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - High Variance in this column is 3.343*10^7\n2. Histogram:\n    - Normally distributed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth. We can observe a peak when F2 value is around -2850\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","d5ffa2b4":"The detailed report you can find in `Train_Data_REPORT.html` which can be opened in chrome and can be observed for data understanding of each column stats and distribution graph. We can even check for association between each of these features in that report.","e75fa59e":"#### F4","639c50aa":"##### The Probablity threshold for Balanced value of Precision vs Recall is 0.354","fe4b5b23":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 55667.48\n2. Histogram:\n    - Noramlly distributed but slightly right skewed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed.\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","e90e82ab":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 0.08\n2. Histogram:\n    - Noramlized values between 0 and 1\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth. We can observe a peak when F2 value is 0.7\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","2d42172b":"Here we can observe, column `F17`, `F18`, `F21` and `F22` are categorical for which we will be doing one-hot encoding before training the model anf before prediction.","dbe6ce8c":"### Dataset Complete Report\nThis report includes following details for each of the features given in the dataset:\n- Stats\n    1. Quantile Statistics\n    2. Descriptive Statistics\n- Plots\n    1. KDE Plot\n    2. Normal Q-Q Plot\n    3. Box Plot\n    \nWe can also select two variables and see the association between them.","6d3e4d2b":"##### Using advanced Classification Algos for prediction and Comparing their Accuracy","8aa0d347":"##### Making predictions on train and test data and writing them to output files","6b38fec6":"### Scaling Numerical features to same scale","c782b9ed":"## Data Quality Check","03c23966":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 0.08\n2. Histogram:\n    - Noramlized values between 0 and 1\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth. We can observe a peak when F2 value is 0.38\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","8cdcbc8f":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 3.33*10^19\n2. Histogram:\n    - Noramlly distributed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed.\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","1344d5fa":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 3.32*10^7\n2. Histogram:\n    - Noramlly distributed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed. We can observe a peak when F2 value is around -4950\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","1771e725":"### Converting Categorical Columns to One-Hot Encoded Form","4b6c9c24":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 3.32*10^19\n2. Histogram:\n    - Noramlly distributed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed.\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outlier in this column","02ccd4f4":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 3.33*10^7\n2. Histogram:\n    - Noramlly distributed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed.\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","13f82a3b":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 55401.05\n2. Histogram:\n    - Noramlly distributed with slightly right skewed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed.\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","cc4b1164":"### Dataset Summary Report using DataPrep","97201a5a":"#### F3","3a080615":"### Loading Packages","26fa2639":"1. Stats:\n    - No missing values\n    - 21 Unique Values\n2. Bar Chart:\n    - Count of `1` is highest followed by `2` \n3. Pie Chart:\n    - Distribution of `1` is approx `41.93%` where other values comprises of remaining `49%` \n    \nNote- Ignore Word Cloud, Word Frequencies and Word Length in this case","2fd98c7e":"#### F14","6abc35ab":"### Loading Data","6ed8550a":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 3.34*10^7\n2. Histogram:\n    - Noramlly distributed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed.\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","66b340f7":"#### F22","d18d56db":"## Modeling and Prediction","de5f9ef5":"#### F9","bb751a24":"Here, we can observe there are no null values in the test data","72c6d1cc":"Here from the above feature plots we can observe:\n1. Columns `F15` and `F16` are date columns which are normally distributed\n2. Columns `F17`, `F18`, `F21` and `F22` are rightly skewed\n3. Target Column `C` has binary values and number of users in class `0` is larger than the users in class `1`\n4. Other columns are having the same distribution frequency","36ebe0c3":"#### F13","8579f08f":"#### F21","5e53f6dc":"##### Parameter Optimization for Gradient Boosting Classifier","94029ee4":"#### Confusion Matrix: \n\n<img src=\"https:\/\/computersciencesource.files.wordpress.com\/2010\/01\/conmat.png\">\n\n##### Insights of a Confusion Matrix: \nThe main purpose of a confusion matrix is to see how our model is performing when it comes to classifying potential customers that are likely to buy an item. We will see in the confusion matrix four terms the True Positives, False Positives, True Negatives and False Negatives.<br><br>\n\n**Positive\/Negative:** Type of Class (label) [\"No\", \"Yes\"]\n**True\/False:** Correctly or Incorrectly classified by the model.<br><br>\n\n**True Negatives (Top-Left Square):** This is the number of **correctly** classifications of the \"No\" class<br><br>\n\n**False Negatives (Top-Right Square):** This is the number of **incorrectly** classifications of the \"No\" class<br><br>\n\n**False Positives (Bottom-Left Square):** This is the number of **incorrectly** classifications of the \"Yes\" class<br><br>\n\n**True Positives (Bottom-Right Square):** This is the number of **correctly** classifications of the \"Yes\" class","33c30496":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 0.08\n2. Histogram:\n    - Noramlized values between 0 and 1\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth. We can observe a peak when F2 value is 0.5 and 0.8\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","08a6cedf":"#### So we are able to predict values in test Data with `76%` Accuracy","9c6cf676":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 3.34*10^19\n2. Histogram:\n    - Noramlly distributed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed.\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","ab15a0d0":"1. Stats:\n    - No missing values\n    - Normally Distributed\n    - Variance of this column is 3.34*10^19\n2. Histogram:\n    - Noramlly distributed\n3. KDE Plot\n    - Because of lower variance in data, density curve is smooth with some peak observed.\n4. Normal Q-Q Plot\n    - From Q-Q plot we can observe this column is lightly tailed\n5. Box Plot\n    - From box plot, we can oberve there are no outliers in this column","73ece37f":"#### F5","83dd0278":"##### Separate the labels(y) and the features(X)","056e1e95":"### Correlation Plot","45e25167":"#### F19","6518bec5":"##### Final Model using Gradient boosting Algorithm","199a1d84":"#### F7","6cdd3a09":"### Lets explore each of the features individually and get insights out of it"}}