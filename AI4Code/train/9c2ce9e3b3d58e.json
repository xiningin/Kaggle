{"cell_type":{"c7bbbc98":"code","9dc0ef78":"code","f771040d":"code","9c052e09":"code","156b861c":"code","2396ecf1":"code","1b84e77c":"code","55b3d460":"code","efdd6f92":"code","d8a5c419":"code","542eb630":"code","1502ffaa":"code","68100a1a":"code","5f555e63":"code","1921ff24":"markdown","e0ba23b9":"markdown"},"source":{"c7bbbc98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9dc0ef78":"# install autokeras module which will work for your architecture\/parameter  serach you will need to enable \n# Internet option in notebok environment \n# for it to work uncomment below line to install\n! pip install autokeras ","f771040d":"import numpy as np\nimport random\nimport keras\n\n## now we import the autokeras clssfiers\nimport autokeras as ak\n\nimport matplotlib.pyplot as plt \nimport pandas as pd\n\nfrom datetime import datetime\nnow = datetime.now()\ncurrent_time = now.strftime(\"%H:%M:%S\")\nprint(current_time)\n","9c052e09":"# load the dataset into train and test sets\ndf_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ndf_train.head()","156b861c":"df_test.head()","2396ecf1":"## we remove the label from train data  in seperate columns and data in seperate test data has no labels \ny_train = df_train.label.to_numpy()\nx_train = df_train.drop('label',axis=1).to_numpy()\nprint(x_train.shape,y_train.shape)\n\nx_test = df_test.to_numpy()\nprint(x_test.shape)","1b84e77c":"## ## after removing the label we will resize the image into 28*28 size from 784  \n##   from train and test data  in seperate columns and data in seperate \n## normalization i done by dividing by 255 as simple way \n\n\nx_train = x_train.reshape(x_train.shape[0],28,28)\nx_test = x_test.reshape(x_test.shape[0],28,28)\nprint(x_train.shape,x_test.shape)\n\n# convert int to float for all data and normalize using divide by 255.0 : \nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\n# normalise\nx_train \/= 255.0\nx_test \/= 255.0\n\nprint(x_train.shape,x_test.shape)","55b3d460":"# convert class labels (from digits) to one-hot encoded vectors : ensure to not run it multiple times as to_categorical \n##  will keep on adding dimensions \n# number of classes in label\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\nprint(y_train.shape)","efdd6f92":"## here we set max_trial = 1 , this is very important, since ImageClassifier class is a  wrapper on automodel,\n# it start going through all the models and training , some like resnet , which takes lot of time\n# and in current imlementation there is no way to use transfer learning \n# we will use an alternate implementation usig automodel later in notebook a more detailed control.\n# we can use the name parameter to pass in name of automodel to use, but its kind of redundant imo as \n# automodel api extends fit and predict methods to be used directly \n\nm_model = ak.ImageClassifier(overwrite=False, max_trials =1,num_classes=10,multi_label=True,loss = 'categorical_crossentropy',\n                             metrics= 'accuracy' ,  objective  = 'val_accuracy' )","d8a5c419":"## at this point we have our image calssifier, we have set max_trials as 1 to just run 1 architecture search , we can increase the \n# no. of trial sand have it loop over \n\n# Feed the image classifier with training data.\nm_model.fit(x_train, y_train, epochs=40,validation_split =0.2,verbose=2)","542eb630":"## predict categories\n\ny_pred = np.argmax(m_model.predict(x_test), axis=1)\nprint(y_pred)\n## here we get matrix 10 labels, we wil use argmax for final label prediction \n\n","1502ffaa":"## lets test 1 image and lable of 0th position \nplt.imshow(x_test[0].reshape(28,28), cmap=plt.get_cmap('gray'), vmin=0, vmax=1)\n# print the predicted label of the above image\ny_pred[0]\n","68100a1a":"## looks good, ","5f555e63":"## looks good \n# create csv dataset for redictions and save for submission\nsubmissions = pd.DataFrame()\nsubmissions[\"ImageId\"] = [i for i in range(1, y_pred.shape[0]+1)]\nsubmissions[\"Label\"] = y_pred\n\nsubmissions.to_csv(\"submissions.csv\", index=False)","1921ff24":"## Limitations of AutoML \nas per current limitations of keras we cannot completly specify all parameters limiting their ability , however it does takes the guess work . I personally found it more suitable to use as a baseline search, afer which you can create a similar modles and try to fine tune it, which in my experience frees up the Human cost for more analytical work . the downside , you will need lots of GPU power .\n","e0ba23b9":"# The future of complex AI\/ML modelling:Building a Basic AutoML: The MNIST Dataset\n\nIn this notebook, we will build a Network Architecture Search, using Keras AutoML API  to classify the 10 digits (0-9) of the MNIST dataset. The objective of this notebook is to solve one of the most time-consuimng process in any NN design designing the structre and the parameter tuning, with the new AutoML api released by Keras ( and Google and Microsoft ) among others , AI is trully going in the direction of general purpose usage where a person does not needs a degree in complex ML\/AI maths to use the available information, because lets face it companies care about output , less about how insightfully it is achieved.\nAs a person with 10+years exp. in Software industry i see the beginning of these tools as a good sign,and to move AI\/ML modelling from the research domain to more in the domain of software engineering.\n\nRefer this paper on NAS ( Network architecture search ) \"https:\/\/arxiv.org\/pdf\/1611.01578.pdf\",\nthe code here is referenced from the official examples from Automl website \"https:\/\/autokeras.com\/\"\nif you have enough computing power offload the AI job to the AI (isnt that ironic)\n\n\n1. Importing libraries and the dataset\n2. Data preparation: Train-test split, specifying the shape of the input data etc.\n3. Building and understanding the AutoML Api \n4. Fitting and evaluating the model\n"}}