{"cell_type":{"459eb3a8":"code","0b1c543c":"code","26175dfd":"code","1435537a":"code","7146542c":"code","78f8bf23":"code","20d6e460":"code","57a4f83d":"code","cb44a0b2":"code","93b8efd1":"code","fdbee49d":"code","c8730c34":"code","87bf6d0c":"code","03030af7":"code","bb896ad2":"code","be925062":"code","baa0d9ea":"code","25748c02":"code","10ee5248":"code","8968e3cc":"code","893a81ae":"code","360e4150":"code","6fa653c2":"code","2fa90724":"code","591a960f":"code","ba598d23":"code","c40a772b":"code","b63eb588":"code","05c0c3e5":"code","2c6dd59f":"code","1aeb1e56":"code","90ae168e":"code","d3990171":"code","199d44c6":"code","18d7aa42":"code","d27336c4":"code","5c7ff9bd":"code","dd945c64":"code","1ce4577d":"code","e6cd48c4":"code","62b7f3ef":"code","0b6906ae":"code","4ac7ca51":"code","da18c109":"code","5204d86d":"code","c3370da1":"code","bed5b3f0":"code","9d939790":"code","9db86491":"code","f4f4efdd":"code","c440f615":"code","6e1dea3b":"code","c119d465":"code","a137733b":"code","c130a476":"code","20ebcf1b":"code","20f8873d":"code","37c62f0b":"code","f7d55020":"code","10f9eb58":"code","9915e4cb":"code","0ce511bb":"code","ac12d312":"code","adf308d0":"code","30ab54ed":"code","f3b9859f":"code","0e6c8392":"markdown","dcb741ed":"markdown","9658ed78":"markdown","35890aac":"markdown","8d7b723d":"markdown","473f2f45":"markdown","7a2a619e":"markdown","ea5f43e1":"markdown","248a042b":"markdown","4326046c":"markdown","f1988ada":"markdown","de23648f":"markdown","87ec94bb":"markdown","e5c04187":"markdown","473d8f72":"markdown","8495d066":"markdown","9d6f2585":"markdown","560b8af5":"markdown","abbbd375":"markdown","1a6525af":"markdown","edbe3e0b":"markdown","7cf0fad3":"markdown","7678244d":"markdown"},"source":{"459eb3a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","0b1c543c":"import pandas as pd\nimport numpy\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport featuretools as ft\n","26175dfd":"def __rolling_window(data, window_size):\n    \"\"\"\n    Rolling window: take window with definite size through the array\n\n    :param data: array-like\n    :param window_size: size\n    :return: the sequence of windows\n\n    Example: data = array(1, 2, 3, 4, 5, 6), window_size = 4\n        Then this function return array(array(1, 2, 3, 4), array(2, 3, 4, 5), array(3, 4, 5, 6))\n    \"\"\"\n    shape = data.shape[:-1] + (data.shape[-1] - window_size + 1, window_size)\n    strides = data.strides + (data.strides[-1],)\n    return numpy.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n\n\ndef __cvm(subindices, total_events):\n    \"\"\"\n    Compute Cramer-von Mises metric.\n    Compared two distributions, where first is subset of second one.\n    Assuming that second is ordered by ascending\n\n    :param subindices: indices of events which will be associated with the first distribution\n    :param total_events: count of events in the second distribution\n    :return: cvm metric\n    \"\"\"\n    target_distribution = numpy.arange(1, total_events + 1, dtype='float') \/ total_events\n    subarray_distribution = numpy.cumsum(numpy.bincount(subindices, minlength=total_events), dtype='float')\n    subarray_distribution \/= 1.0 * subarray_distribution[-1]\n    return numpy.mean((target_distribution - subarray_distribution) ** 2)\n\n\ndef compute_cvm(predictions, masses, n_neighbours=200, step=50):\n    \"\"\"\n    Computing Cramer-von Mises (cvm) metric on background events: take average of cvms calculated for each mass bin.\n    In each mass bin global prediction's cdf is compared to prediction's cdf in mass bin.\n\n    :param predictions: array-like, predictions\n    :param masses: array-like, in case of Kaggle tau23mu this is reconstructed mass\n    :param n_neighbours: count of neighbours for event to define mass bin\n    :param step: step through sorted mass-array to define next center of bin\n    :return: average cvm value\n    \"\"\"\n    predictions = numpy.array(predictions)\n    masses = numpy.array(masses)\n    assert len(predictions) == len(masses)\n\n    # First, reorder by masses\n    predictions = predictions[numpy.argsort(masses)]\n\n    # Second, replace probabilities with order of probability among other events\n    predictions = numpy.argsort(numpy.argsort(predictions, kind='mergesort'), kind='mergesort')\n\n    # Now, each window forms a group, and we can compute contribution of each group to CvM\n    cvms = []\n    for window in __rolling_window(predictions, window_size=n_neighbours)[::step]:\n        cvms.append(__cvm(subindices=window, total_events=len(predictions)))\n    return numpy.mean(cvms)\n\n\ndef __roc_curve_splitted(data_zero, data_one, sample_weights_zero, sample_weights_one):\n    \"\"\"\n    Compute roc curve\n\n    :param data_zero: 0-labeled data\n    :param data_one:  1-labeled data\n    :param sample_weights_zero: weights for 0-labeled data\n    :param sample_weights_one:  weights for 1-labeled data\n    :return: roc curve\n    \"\"\"\n    labels = [0] * len(data_zero) + [1] * len(data_one)\n    weights = numpy.concatenate([sample_weights_zero, sample_weights_one])\n    data_all = numpy.concatenate([data_zero, data_one])\n    fpr, tpr, _ = roc_curve(labels, data_all, sample_weight=weights)\n    return fpr, tpr\n\n\ndef compute_ks(data_prediction, mc_prediction, weights_data, weights_mc):\n    \"\"\"\n    Compute Kolmogorov-Smirnov (ks) distance between real data predictions cdf and Monte Carlo one.\n\n    :param data_prediction: array-like, real data predictions\n    :param mc_prediction: array-like, Monte Carlo data predictions\n    :param weights_data: array-like, real data weights\n    :param weights_mc: array-like, Monte Carlo weights\n    :return: ks value\n    \"\"\"\n    assert len(data_prediction) == len(weights_data), 'Data length and weight one must be the same'\n    assert len(mc_prediction) == len(weights_mc), 'Data length and weight one must be the same'\n\n    data_prediction, mc_prediction = numpy.array(data_prediction), numpy.array(mc_prediction)\n    weights_data, weights_mc = numpy.array(weights_data), numpy.array(weights_mc)\n\n    assert numpy.all(data_prediction >= 0.) and numpy.all(data_prediction <= 1.), 'Data predictions are out of range [0, 1]'\n    assert numpy.all(mc_prediction >= 0.) and numpy.all(mc_prediction <= 1.), 'MC predictions are out of range [0, 1]'\n\n    weights_data \/= numpy.sum(weights_data)\n    weights_mc \/= numpy.sum(weights_mc)\n\n    fpr, tpr = __roc_curve_splitted(data_prediction, mc_prediction, weights_data, weights_mc)\n\n    Dnm = numpy.max(numpy.abs(fpr - tpr))\n    return Dnm\n\n\ndef roc_auc_truncated(labels, predictions, tpr_thresholds=(0.2, 0.4, 0.6, 0.8),\n                      roc_weights=(4, 3, 2, 1, 0)):\n    \"\"\"\n    Compute weighted area under ROC curve.\n\n    :param labels: array-like, true labels\n    :param predictions: array-like, predictions\n    :param tpr_thresholds: array-like, true positive rate thresholds delimiting the ROC segments\n    :param roc_weights: array-like, weights for true positive rate segments\n    :return: weighted AUC\n    \"\"\"\n    assert numpy.all(predictions >= 0.) and numpy.all(predictions <= 1.), 'Data predictions are out of range [0, 1]'\n    assert len(tpr_thresholds) + 1 == len(roc_weights), 'Incompatible lengths of thresholds and weights'\n    fpr, tpr, _ = roc_curve(labels, predictions)\n    area = 0.\n    tpr_thresholds = [0.] + list(tpr_thresholds) + [1.]\n    for index in range(1, len(tpr_thresholds)):\n        tpr_cut = numpy.minimum(tpr, tpr_thresholds[index])\n        tpr_previous = numpy.minimum(tpr, tpr_thresholds[index - 1])\n        area += roc_weights[index - 1] * (auc(fpr, tpr_cut, reorder=True) - auc(fpr, tpr_previous, reorder=True))\n    tpr_thresholds = numpy.array(tpr_thresholds)\n    # roc auc normalization to be 1 for an ideal classifier\n    area \/= numpy.sum((tpr_thresholds[1:] - tpr_thresholds[:-1]) * numpy.array(roc_weights))\n    return area\n","1435537a":"folder = \"..\/input\/\"\ntrain = pd.read_csv(folder+'training.csv', index_col='id')\n","7146542c":"train.head()","78f8bf23":"test = pd.read_csv(folder + 'test.csv', index_col='id')\ntest.shape","20d6e460":"# visualize the relationship between the features and the response using scatterplots\nsns.pairplot(train, x_vars=['LifeTime','dira','FlightDistance','FlightDistanceError'], y_vars='signal', size=7, aspect=0.7)","57a4f83d":"sns.pairplot(train, x_vars=['LifeTime','dira','FlightDistance'], y_vars='FlightDistanceError', size=7, aspect=0.7)","cb44a0b2":"removeFeatures = ('signal','production','mass','min_ANNmuon')\ntrTarget = train.loc[:,'signal']\ntrTarget.head()","93b8efd1":"trainNew = train.drop(columns=['signal','production','mass','min_ANNmuon'])\ntrainNew.head()","fdbee49d":"trainNew.shape","c8730c34":"trainNew.tail(1)\n","87bf6d0c":"test.head(1)","03030af7":"sns.boxplot( y=trainNew[\"pt\"] )\n","bb896ad2":"trainNew['pt'].describe()","be925062":"combi = pd.concat([trainNew, test])\ncombi.shape","baa0d9ea":"def convertNum2Bin(x):\n    if 0 < x <= 5028:\n        return 0\n    return 1\n\ncombi['ptBin'] = combi['pt'].apply(convertNum2Bin)","25748c02":"trainNew['LifeTime'].describe()","10ee5248":"def convertLT2Bin(x):\n    if 0 < x <= 0.001255:\n        return 0\n    return 1\n\ncombi['LifeTimeBin'] = combi['LifeTime'].apply(convertLT2Bin)","8968e3cc":"trainNew['FlightDistance'].describe()","893a81ae":"def convertFD2Bin(x):\n    if 0 < x <= 15.154:\n        return 0\n    return 1\n\ncombi['FDBin'] = combi['FlightDistance'].apply(convertFD2Bin)","360e4150":"trainNew['FlightDistanceError'].describe()","6fa653c2":"def convertFDE2Bin(x):\n    if 0 < x <= 0.5018:\n        return 0\n    return 1\n\ncombi['FDEBin'] = combi['FlightDistanceError'].apply(convertFDE2Bin)","2fa90724":"combi.shape","591a960f":"\n# Make an entityset and add the entity\n#es = ft.EntitySet(id = 'particleData')\n#es.entity_from_dataframe(entity_id = 'TrainAndTest', dataframe = combi, \n#                         make_index = True, index = 'index')\n\n# Run deep feature synthesis with transformation primitives\n#feature_matrix, feature_defs = ft.dfs(entityset = es, target_entity = 'TrainAndTest', max_depth = 2, verbose = 1,\n                                      agg_primitives = ['count','mean'])\n#                                      trans_primitives = ['add', 'multiply'])\n\n#feature_matrix.head()","ba598d23":"#feature_matrix.columns\n#feature_matrix.shape","c40a772b":"#combi.shape","b63eb588":"#combi['id']=combi.reset_index().index\n#combi.iloc[67552:67554,:]\ntrainNew = combi.iloc[:67553,:]    # or should we use feature_matrix instead of combi?\ntest= combi.iloc[67553:,:]\ntrainNew.shape","05c0c3e5":"test.shape","2c6dd59f":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrainScaled = scaler.fit_transform(trainNew)\ntrainScaled[0:2,]\n","1aeb1e56":"testScaled = scaler.transform(test)\ntestScaled[0:2,]\n","90ae168e":"from sklearn.decomposition import PCA\n\npca = PCA().fit(trainScaled)\ntrain_pca = pca.transform(trainScaled)\ntest_pca = pca.transform(testScaled)","d3990171":"def pca_summary(pca, standardized_data, out=True):\n    names = [\"PC\"+str(i) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n    a = list(np.std(pca.transform(standardized_data), axis=0))\n    b = list(pca.explained_variance_ratio_)\n    c = [np.sum(pca.explained_variance_ratio_[:i]) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n    columns = pd.MultiIndex.from_tuples([(\"sdev\", \"Standard deviation\"), (\"varprop\", \"Proportion of Variance\"), (\"cumprop\", \"Cumulative Proportion\")])\n    summary = pd.DataFrame(list(zip(a, b, c)), index=names, columns=columns)\n    \n    if out:\n        print(\"Importance of components:\")\n        display(summary)\n    return summary","199d44c6":"summary = pca_summary(pca, train_pca)","18d7aa42":"import matplotlib.pyplot as plt\n\ndef screeplot(pca, standardized_values):\n    y = np.std(pca.transform(standardized_values), axis=0)**2\n    x = np.arange(len(y)) + 1\n    plt.plot(x, y, \"o-\")\n    plt.xticks(x, [\"Comp.\"+str(i) for i in x], rotation=60)\n    plt.ylabel(\"Variance\")\n    plt.show()\n\nscreeplot(pca, trainScaled)","d27336c4":"pcatrain = pd.DataFrame(train_pca[:,0:31])\npcatrain.head()","5c7ff9bd":"pcatest = pd.DataFrame(test_pca[:,0:31])\npcatest.head()","dd945c64":"baseline = GradientBoostingClassifier(n_estimators=40, learning_rate=0.01, subsample=0.7,\n                                      min_samples_leaf=10, max_depth=7, random_state=11)\nbaseline.fit(pcatrain, trTarget)\n#baseline.fit(train[variables], train['signal'])","1ce4577d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n#pcatrain, pcatest, trTarget - already available\n\n# Create the parameter grid \nparam_grid = {\n    'bootstrap': [False],\n#    'max_depth': [100, 200],       ------ The RF Classifier failed the Agreement Test based on these parameters. So, leaving this step out for faster re-run of code.\n    'max_features': ['sqrt'],\n#    'min_samples_leaf': [1, 3],\n#    'n_estimators': [200, 400]\n}\n# Create a RF Classifier model\nrf = RandomForestClassifier(random_state = 42)\n# Instantiate the grid search model\n#grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 2, n_jobs = -1, verbose = 2) --- CV works but takes time so doing it without CV\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, n_jobs = -1, verbose = 2)\n# Fit the grid search to the data\ngrid_search.fit(pcatrain, trTarget)\ngrid_search.best_params_\n","e6cd48c4":"best_gridRFCModel = grid_search.best_estimator_","62b7f3ef":"import xgboost as xgb\n\n\nclfXGB = xgb.XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=5,random_state = 42)\nclfXGB.fit(pcatrain, trTarget)\n","0b6906ae":"clfXGB2 = xgb.XGBClassifier(n_estimators=40, learning_rate=0.01, subsample=0.7,min_samples_leaf=10, max_depth=7, random_state=11)\nclfXGB2.fit(pcatrain, trTarget)","4ac7ca51":"clfXGB3 = xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11)\nclfXGB3.fit(pcatrain, trTarget)","da18c109":"check_agreement = pd.read_csv(folder + 'check_agreement.csv', index_col='id')\ncheck_agreement.shape","5204d86d":"check_agreement.head(2)","c3370da1":"check_agreement_sig_wt = check_agreement.loc[:,['signal','weight']]\ncheck_agreement_sig_wt.head(2)\n\ncheck_agreement_var = check_agreement.drop(columns=['signal','weight']) \ncheck_agreement_var.head(2)","bed5b3f0":"check_agreement_var['ptBin'] = check_agreement_var['pt'].apply(convertNum2Bin)\ncheck_agreement_var['LifeTimeBin'] = check_agreement_var['LifeTime'].apply(convertLT2Bin)\ncheck_agreement_var['FDBin'] = check_agreement_var['FlightDistance'].apply(convertFD2Bin)\ncheck_agreement_var['FDEBin'] = check_agreement_var['FlightDistanceError'].apply(convertFDE2Bin)\n\nagreementScaled = scaler.transform(check_agreement_var)\nagreement_pca = pca.transform(agreementScaled)\npcaAgreement = pd.DataFrame(agreement_pca[:,0:31])\n\npcaAgreement.head(2)","9d939790":"#agreement_probs = baseline.predict_proba(check_agreement[variables])[:, 1]\nagreement_probs = baseline.predict_proba(pcaAgreement)[:, 1]\n\nks = compute_ks(\n    agreement_probs[check_agreement_sig_wt['signal'].values == 0],\n    agreement_probs[check_agreement_sig_wt['signal'].values == 1],\n    check_agreement[check_agreement_sig_wt['signal'] == 0]['weight'].values,\n    check_agreement[check_agreement_sig_wt['signal'] == 1]['weight'].values)\nprint ('KS metric', ks, ks < 0.09)","9db86491":"agreement_probs = best_gridRFCModel.predict_proba(pcaAgreement)[:, 1]\n\nks = compute_ks(\n    agreement_probs[check_agreement_sig_wt['signal'].values == 0],\n    agreement_probs[check_agreement_sig_wt['signal'].values == 1],\n    check_agreement[check_agreement_sig_wt['signal'] == 0]['weight'].values,\n    check_agreement[check_agreement_sig_wt['signal'] == 1]['weight'].values)\nprint ('KS metric', ks, ks < 0.09)","f4f4efdd":"agreement_probs = clfXGB.predict_proba(pcaAgreement)[:, 1]\n\nks = compute_ks(\n    agreement_probs[check_agreement_sig_wt['signal'].values == 0],\n    agreement_probs[check_agreement_sig_wt['signal'].values == 1],\n    check_agreement[check_agreement_sig_wt['signal'] == 0]['weight'].values,\n    check_agreement[check_agreement_sig_wt['signal'] == 1]['weight'].values)\nprint ('KS metric', ks, ks < 0.09)","c440f615":"agreement_probs = clfXGB2.predict_proba(pcaAgreement)[:, 1]\n\nks = compute_ks(\n    agreement_probs[check_agreement_sig_wt['signal'].values == 0],\n    agreement_probs[check_agreement_sig_wt['signal'].values == 1],\n    check_agreement[check_agreement_sig_wt['signal'] == 0]['weight'].values,\n    check_agreement[check_agreement_sig_wt['signal'] == 1]['weight'].values)\nprint ('KS metric', ks, ks < 0.09)","6e1dea3b":"agreement_probs = clfXGB3.predict_proba(pcaAgreement)[:, 1]\n\nks = compute_ks(\n    agreement_probs[check_agreement_sig_wt['signal'].values == 0],\n    agreement_probs[check_agreement_sig_wt['signal'].values == 1],\n    check_agreement[check_agreement_sig_wt['signal'] == 0]['weight'].values,\n    check_agreement[check_agreement_sig_wt['signal'] == 1]['weight'].values)\nprint ('KS metric', ks, ks < 0.09)","c119d465":"check_correlation = pd.read_csv(folder + 'check_correlation.csv', index_col='id')\ncheck_correlation.shape\ncheck_correlation.head(1)","a137733b":"check_correlation_mass = check_correlation.loc[:,'mass']\ncheck_correlation_mass.head(2)\n\ncheck_correlation_var = check_correlation.drop('mass',axis = 1) \ncheck_correlation_var.head(2)","c130a476":"check_correlation_var.shape","20ebcf1b":"check_correlation_var['ptBin'] = check_correlation_var['pt'].apply(convertNum2Bin)\ncheck_correlation_var['LifeTimeBin'] = check_correlation_var['LifeTime'].apply(convertLT2Bin)\ncheck_correlation_var['FDBin'] = check_correlation_var['FlightDistance'].apply(convertFD2Bin)\ncheck_correlation_var['FDEBin'] = check_correlation_var['FlightDistanceError'].apply(convertFDE2Bin)\n\ncorrelationScaled = scaler.transform(check_correlation_var)\ncorrelation_pca = pca.transform(correlationScaled)\npcaCorrelation = pd.DataFrame(correlation_pca[:,0:31])\n\npcaCorrelation.head(2)","20f8873d":"correlation_probs = baseline.predict_proba(pcaCorrelation)[:, 1]\ncvm = compute_cvm(correlation_probs, check_correlation_mass)\nprint ('CvM metric', cvm, cvm < 0.002)","37c62f0b":"correlation_probs = best_gridRFCModel.predict_proba(pcaCorrelation)[:, 1]\ncvm = compute_cvm(correlation_probs, check_correlation_mass)\nprint ('CvM metric', cvm, cvm < 0.002)","f7d55020":"correlation_probs = clfXGB2.predict_proba(pcaCorrelation)[:, 1]\ncvm = compute_cvm(correlation_probs, check_correlation_mass)\nprint ('CvM metric', cvm, cvm < 0.002)","10f9eb58":"correlation_probs = clfXGB3.predict_proba(pcaCorrelation)[:, 1]\ncvm = compute_cvm(correlation_probs, check_correlation_mass)\nprint ('CvM metric', cvm, cvm < 0.002)","9915e4cb":"train_eval = train[train['min_ANNmuon'] > 0.4]\ntrainEvalSignal = train_eval['signal']\ntrainEvalTruncated = train_eval.drop(columns=['signal','production','mass','min_ANNmuon'])\n\ntrainEvalTruncated['ptBin'] = trainEvalTruncated['pt'].apply(convertNum2Bin)\ntrainEvalTruncated['LifeTimeBin'] = trainEvalTruncated['LifeTime'].apply(convertLT2Bin)\ntrainEvalTruncated['FDBin'] = trainEvalTruncated['FlightDistance'].apply(convertFD2Bin)\ntrainEvalTruncated['FDEBin'] = trainEvalTruncated['FlightDistanceError'].apply(convertFDE2Bin)\n\ntrainEvalScaled = scaler.transform(trainEvalTruncated)\ntrainEval_pca = pca.transform(trainEvalScaled)\npcaTrainEval = pd.DataFrame(trainEval_pca[:,0:31])\n\npcaTrainEval.head(2)","0ce511bb":"train_probs = baseline.predict_proba(pcaTrainEval)[:, 1]\nAUC = roc_auc_truncated(trainEvalSignal, train_probs)\nprint ('AUC', AUC)","ac12d312":"train_probs = clfXGB2.predict_proba(pcaTrainEval)[:, 1]\nAUC = roc_auc_truncated(trainEvalSignal, train_probs)\nprint ('AUC', AUC)","adf308d0":"train_probs = clfXGB3.predict_proba(pcaTrainEval)[:, 1]\nAUC = roc_auc_truncated(trainEvalSignal, train_probs)\nprint ('AUC', AUC)","30ab54ed":"result = pd.DataFrame({'id': test.index})\nresult['prediction'] = clfXGB3.predict_proba(pcatest)[:, 1]","f3b9859f":"sub = result.to_csv('Achal_baseline_5.csv', index=False, sep=',')","0e6c8392":"# Check agreement test","dcb741ed":"**XGB Classifier (Second set of parameters)**","9658ed78":" **XGB Classifier (First set of parameters)**","35890aac":"Out of the above 5 models, Baseline model (Gradient Boosting), Random Forest Classifier, XGB (with second set of parameters) and XGB (with third set of parameters) passed the Agreement Test. We will proceed with these 4 models for the next steps.","8d7b723d":"Before we proceed, let's Scale the data so that all features contribute equally to the prediction. We do not want some features to have more weight compared to others just becasue of a difference in the units of measurement.","473f2f45":"**Random Forest Classifier (with Parameter Tuning)**","7a2a619e":"# Predict test, create file for kaggle","ea5f43e1":"It might be a good idea to do a Principal Component Analysis of the data to avoid features that are highly correlated. Also, we will go for features that represent 95% of the variance to avoid overfitting of the data.","248a042b":"Check Agreement Dataframe has weight and signal in addition to the 50 attributes given in Train dataset.","4326046c":"# Compute weighted AUC on the training data with min_ANNmuon > 0.4\n","f1988ada":"Let's do the Agreement Test on Random Forest Classifier too.","de23648f":"Let's try to add some features to the dataset. \n\nWe can add features by introducing some categorical variable (if binary, we do not need OneHotEncoding). \nWe can also use FeatureTools library to automatically generate features.","87ec94bb":"Let's try to visualise the data - it always helps to see if there is some apparent relationship or anomaly in the dataset.","e5c04187":"Random Forest Classifier test passed the test. Let's do it on XGB Classifier for 3 different sets of parameters.","473d8f72":"# Check correlation test\n","8495d066":"**Baseline Training (Gradient Boosting Classifier)**","9d6f2585":"AUC Metric is best for XGB with the third set of parameters. Let's create file for Kaggle.","560b8af5":" **XGB Classifier (Third set of parameters)**","abbbd375":"So, only 3 of the models - passed the Correlation Test.","1a6525af":"First 31 PCA attributes give more than 95% variance. Let's go ahead with 31 PCA Attributes for our analysis.","edbe3e0b":"We will try various models - Gradient Boosting Classifier, Random Forest Classifier and Extreme Gradient Boosting (XGB) Classifier. \n\nWe can try Parameter Tuning using Grid Cross Validation to fine tune the model. ","7cf0fad3":"Since the test dataset does not have 4 features, it is prudent to remove these from the training dataset for the analysis.","7678244d":"**Flavors of Physics**\n\nThis is a problem that is not easy to understand for someone who is not into Quantum Physics. Though a bit intriguing, the problem seems to be very interesting. \n\nIn simple terms, as I could undertand, the scientists are trying to observe the decay of tau to three muons - a phenomenon that is very rare.  The null hypothesis is that this decay does not happen and the idea here is to see that the decay happens more than what is currently known.\n\nThe dataset consists of real and simulated events. Real events (or Background events or those with Signal value = 0) and Simulated Events (or Signal Events or those with Signal value = 1) are part of the dataset. The analysis is to be done on the entire dataset to classify the data based on various attributes. \n\nBefore the algorithm is evaluated (based on AUC metric), the algorithm needs to pass through Agreement Test and Correlation Test. The code for these tests is provided along with the dataset."}}