{"cell_type":{"0ca8e82f":"code","4e0c22b7":"code","28ae98e8":"code","c14ea694":"code","0b20c4cc":"code","ce3b5760":"code","c4f85b93":"code","6e9f2b5d":"code","94ab880f":"code","9f0deb62":"code","6aa6dade":"code","69a105cf":"code","d1286c6f":"code","e3cf70f7":"code","d0c2bf02":"code","dd01c2dc":"code","2050e1c5":"code","ef34b53d":"code","689e87e7":"code","77ea401f":"code","bd6dbe44":"code","4cf94593":"code","ace8e540":"code","d847d53d":"code","b8b0a316":"code","a9b00852":"code","bc7b8ebd":"code","1193f6f9":"markdown","71b57bfb":"markdown","57955ae4":"markdown","07474dc9":"markdown","0715c6fd":"markdown","959ce45c":"markdown","48b86afb":"markdown","85bf78e2":"markdown","113b37a3":"markdown","240c4be8":"markdown","6050cf75":"markdown","ef4a894b":"markdown","4117bb36":"markdown","346e80d9":"markdown","d7bb2b2c":"markdown","f79d47ca":"markdown","832d0d47":"markdown","5f379062":"markdown","1c9a7456":"markdown","2bd18369":"markdown","b1b9156a":"markdown","8db4a51f":"markdown","f808b17b":"markdown","8fd801d9":"markdown","2586f9d1":"markdown","fd1079d3":"markdown","a5b77f06":"markdown","6aa6b8e4":"markdown"},"source":{"0ca8e82f":"import os\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom IPython import display","4e0c22b7":"def decode_audio(audio_binary):\n    audio, _ = tf.audio.decode_wav(audio_binary)\n    return tf.squeeze(audio, axis=-1)\n\ndef get_label(file_path):\n    parts = tf.strings.split(file_path, os.path.sep)\n\n    # Note: You'll use indexing here instead of tuple unpacking to enable this \n    # to work in a TensorFlow graph.\n    return parts[-2]\n\ndef get_waveform_and_label(file_path):\n    label = get_label(file_path)\n    audio_binary = tf.io.read_file(file_path)\n    waveform = decode_audio(audio_binary)\n    return waveform, label\n\ndef get_spectrogram(waveform):\n    # Padding for files with less than 16000 samples\n    zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n\n    # Concatenate audio with padding so that all audio clips will be of the \n    # same length\n    waveform = tf.cast(waveform, tf.float32)\n    equal_length = tf.concat([waveform, zero_padding], 0)\n    spectrogram = tf.signal.stft(\n      equal_length, frame_length=255, frame_step=128)\n\n    spectrogram = tf.abs(spectrogram)\n\n    return spectrogram\n\ndef plot_spectrogram(spectrogram, ax):\n    # Convert to frequencies to log scale and transpose so that the time is\n    # represented in the x-axis (columns). An epsilon is added to avoid log of zero.\n    log_spec = np.log(spectrogram.T+np.finfo(float).eps)\n    height = log_spec.shape[0]\n    width = log_spec.shape[1]\n    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n    Y = range(height)\n    ax.pcolormesh(X, Y, log_spec)\n    \ndef get_spectrogram_and_label_id(audio, label):\n    spectrogram = get_spectrogram(audio)\n    spectrogram = tf.expand_dims(spectrogram, -1)\n    label_id = tf.argmax(label == commands)\n    return spectrogram, label_id\n\ndef preprocess_dataset(files):\n    files_ds = tf.data.Dataset.from_tensor_slices(files)\n    output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n    output_ds = output_ds.map(\n      get_spectrogram_and_label_id,  num_parallel_calls=AUTOTUNE)\n    return output_ds","28ae98e8":"SEED = 42\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nDATA_DIR = 'data\/mini_speech_commands'\nFILE_NAME =  'mini_speech_commands.zip'\nORIGIN = \"http:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/mini_speech_commands.zip\"\nCACHE_SUBDIR = 'data'\n\nTRAIN_DATA_SIZE = 6400\nVALIDATATION_DATA_SIZE = 800\nENCODING = 'utf-8'\nBATCH_SIZE = 64\nAUTOTUNE = tf.data.AUTOTUNE\nEPOCHS = 10\nPATIENCE = 2\nPREDICTION = 'Prediction'\nLABEL = 'Label'\nLOSS = 'loss'\nVAL_LOSS = 'val_loss'\nWAVEFORM = 'Waveform'\nSPECTROGRAM = 'Spectrogram'\nMETRICS_ACCUARACY = 'accuracy'","c14ea694":"data_dir = pathlib.Path(DATA_DIR)\nif not data_dir.exists():\n    tf.keras.utils.get_file(\n     FILE_NAME,\n      origin=ORIGIN,\n      extract=True,\n      cache_dir='.', cache_subdir=CACHE_SUBDIR)","0b20c4cc":"commands = np.array(tf.io.gfile.listdir(str(data_dir)))\ncommands","ce3b5760":"commands = commands[commands != 'README.md']\nprint('Commands:', commands)","c4f85b93":"filenames = tf.io.gfile.glob(str(data_dir) + '\/*\/*')\nfilenames = tf.random.shuffle(filenames)\nnum_samples = len(filenames)\nprint('Number of total examples:', num_samples)\nprint('Number of examples per label:',\n      len(tf.io.gfile.listdir(str(data_dir\/commands[0]))))\nprint('Example file tensor:', filenames[0])","6e9f2b5d":"train_files = filenames[:TRAIN_DATA_SIZE]\nval_files = filenames[TRAIN_DATA_SIZE: TRAIN_DATA_SIZE + VALIDATATION_DATA_SIZE]\ntest_files = filenames[-VALIDATATION_DATA_SIZE:]\n\nprint('Training set size', len(train_files))\nprint('Validation set size', len(val_files))\nprint('Test set size', len(test_files))","94ab880f":"files_ds = tf.data.Dataset.from_tensor_slices(train_files)\nwaveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)","9f0deb62":"rows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 12))\nfor i, (audio, label) in enumerate(waveform_ds.take(n)):\n    r = i \/\/ cols\n    c = i % cols\n    ax = axes[r][c]\n    ax.plot(audio.numpy())\n    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n    label = label.numpy().decode(ENCODING)\n    ax.set_title(label)\n\nplt.show()","6aa6dade":"for waveform, label in waveform_ds.take(1):\n    label = label.numpy().decode(ENCODING)\n    spectrogram = get_spectrogram(waveform)\n\nprint('Label:', label)\nprint('Waveform shape:', waveform.shape)\nprint('Spectrogram shape:', spectrogram.shape)\nprint('Audio playback')\ndisplay.display(display.Audio(waveform, rate=16000))","69a105cf":"fig, axes = plt.subplots(2, figsize=(12, 8))\ntimescale = np.arange(waveform.shape[0])\naxes[0].plot(timescale, waveform.numpy())\naxes[0].set_title(WAVEFORM)\naxes[0].set_xlim([0, 16000])\nplot_spectrogram(spectrogram.numpy(), axes[1])\naxes[1].set_title(SPECTROGRAM)\nplt.show()","d1286c6f":"spectrogram_ds = waveform_ds.map(\n    get_spectrogram_and_label_id, num_parallel_calls=AUTOTUNE)","e3cf70f7":"rows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 10))\nfor i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n    r = i \/\/ cols\n    c = i % cols\n    ax = axes[r][c]\n    plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)\n    ax.set_title(commands[label_id.numpy()])\n    ax.axis('off')\n\nplt.show()","d0c2bf02":"train_ds = spectrogram_ds\nval_ds = preprocess_dataset(val_files)\ntest_ds = preprocess_dataset(test_files)","dd01c2dc":"batch_size = BATCH_SIZE\ntrain_ds = train_ds.batch(batch_size)\nval_ds = val_ds.batch(batch_size)","2050e1c5":"train_ds = train_ds.cache().prefetch(AUTOTUNE)\nval_ds = val_ds.cache().prefetch(AUTOTUNE)","ef34b53d":"for spectrogram, _ in spectrogram_ds.take(1):\n    input_shape = spectrogram.shape\nprint('Input shape:', input_shape)\nnum_labels = len(commands)\n\nnorm_layer = preprocessing.Normalization()\nnorm_layer.adapt(spectrogram_ds.map(lambda x, _: x))\n\nmodel = models.Sequential([\n    layers.Input(shape=input_shape),\n    preprocessing.Resizing(32, 32), \n    norm_layer,\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_labels),\n])\n\nmodel.summary()","689e87e7":"tf.keras.utils.plot_model(model)","77ea401f":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[METRICS_ACCUARACY],\n)","bd6dbe44":"history = model.fit(\n    train_ds, \n    validation_data=val_ds,  \n    epochs=EPOCHS,\n    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=PATIENCE),\n)","4cf94593":"metrics = history.history\nplt.plot(history.epoch, metrics[LOSS], metrics[VAL_LOSS])\nplt.legend([LOSS, VAL_LOSS])\nplt.show()","ace8e540":"test_audio = []\ntest_labels = []\n\nfor audio, label in test_ds:\n    test_audio.append(audio.numpy())\n    test_labels.append(label.numpy())\n\ntest_audio = np.array(test_audio)\ntest_labels = np.array(test_labels)","d847d53d":"y_pred = np.argmax(model.predict(test_audio), axis=1)\ny_true = test_labels","b8b0a316":"test_acc = sum(y_pred == y_true) \/ len(y_true)\nprint(f'Test set accuracy: {test_acc:.0%}')","a9b00852":"confusion_mtx = tf.math.confusion_matrix(y_true, y_pred) \nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mtx, xticklabels=commands, yticklabels=commands, \n            annot=True, fmt='g')\nplt.xlabel(PREDICTION)\nplt.ylabel(LABEL)\nplt.show()","bc7b8ebd":"sample_file = data_dir\/'no\/01bb6a2a_nohash_0.wav'\n\nsample_ds = preprocess_dataset([str(sample_file)])\n\nfor spectrogram, label in sample_ds.batch(1):\n    prediction = model(spectrogram)\n    plt.bar(commands, tf.nn.softmax(prediction[0]))\n    plt.title(f'Predictions for \"{commands[label[0]]}\"')\n    plt.show()","1193f6f9":"# convert file data to waveform dataset ","71b57bfb":"# draw waveform dataset ","57955ae4":"# get label and spectrogram ","07474dc9":"## exclude README.md ","0715c6fd":"# global variables","959ce45c":"# get filenames and data count","48b86afb":"# get test set accuaracy","85bf78e2":"# test model using sample file ","113b37a3":"this sample data label => 'no'","240c4be8":"# train model","6050cf75":"# get commands from data directory ","ef4a894b":"# make model","4117bb36":"# draw confusion matrix ","346e80d9":"# convert test dataset to numpy array ","d7bb2b2c":"# split train set, validation set, test set ","f79d47ca":"Computes the crossentropy loss between the labels and predictions.","832d0d47":"# draw spectorgram and label","5f379062":"# fetch data from orgin to data directory","1c9a7456":"# import libraries","2bd18369":"# draw train result (loss and val_loss)","b1b9156a":"# common functions","8db4a51f":"# model setting","f808b17b":"# make train set ,validation set, test set ","8fd801d9":"# predict test dataset ","2586f9d1":"# draw waveform and spectogram","fd1079d3":"# prefetch autotune ","a5b77f06":"# make spectogram dataset ","6aa6b8e4":"# reduce train dataset and validation dataset "}}