{"cell_type":{"5b9be1e6":"code","8d30fd7b":"code","21fc3ee7":"code","9851a5ca":"code","4a94c653":"code","c506ec1f":"code","f4cea4ba":"code","1e2cfa65":"code","aa235a13":"code","4d5ab672":"code","1c5d247c":"code","764b4a28":"code","656ca09b":"code","c849025c":"code","20ef8dc1":"code","5589cc90":"code","a7bc2137":"code","3e32256f":"code","39139940":"code","3ac0e29f":"code","4b7da982":"code","f21fec83":"code","37362c0a":"code","ecce6dd8":"code","72f5e1f1":"code","ee67b340":"code","54cc5557":"code","8906572c":"code","c86e760e":"code","e3de6f4d":"code","b5ecdd34":"code","5704a133":"code","8eb6ab1b":"code","ac0514d6":"code","feac1e61":"code","7a7cc1d1":"code","9ba99e84":"code","741614cf":"code","6e3e2112":"code","f7123152":"code","fc009477":"code","ff5523bc":"code","e1ad7243":"code","6859045d":"code","23cc3f27":"code","40f14284":"code","77e3dbd5":"code","314e0d8e":"code","b77687e1":"code","011340af":"code","cd6c5db7":"code","2cd02ab0":"code","32ef5ad3":"code","8bf89769":"code","0e562a10":"code","7c788679":"code","50341e10":"code","6548250c":"code","313aa596":"code","56334bc4":"markdown","e4ac6758":"markdown","fcecd6f7":"markdown","d09c7336":"markdown","04757bf9":"markdown","626a1ead":"markdown","bac14f51":"markdown","dde38308":"markdown","fe66a2d5":"markdown","4800fb81":"markdown","dd272c02":"markdown","22bc3d59":"markdown","def0f46c":"markdown","796dafca":"markdown","327aa8ca":"markdown","f22cfce1":"markdown"},"source":{"5b9be1e6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport xgboost\nfrom xgboost import XGBClassifier\n\nimport keras \nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import layers, models, optimizers\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8d30fd7b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","21fc3ee7":"data_df =  pd.read_csv('..\/input\/ufcdata\/data.csv')","9851a5ca":"data_df.info()","4a94c653":"df = data_df.dropna()","c506ec1f":"df.info()","f4cea4ba":"columns=df.select_dtypes(include='object').columns","1e2cfa65":"columns","aa235a13":"df.drop(columns=['R_fighter', 'B_fighter', 'Referee', 'date', 'location','weight_class'], inplace=True)","4d5ab672":"df.select_dtypes(include='object')","1c5d247c":"map_stance = {'Orthodox': 0, 'Switch': 1, 'Southpaw': 2, 'Open Stance': 3}\ndf['B_Stance'] = df['B_Stance'].replace(map_stance)\ndf['R_Stance'] = df['R_Stance'].replace(map_stance)\n\nmap_winner = {'Red': 0, 'Blue': 1, 'Draw': 2}\ndf['Winner'] = df['Winner'].replace(map_winner)","764b4a28":"df.drop(columns=df.select_dtypes(include='bool').columns, inplace=True)\ndf.info()","656ca09b":"df['Winner'].unique()","c849025c":"df = df[df['Winner'] != 2]","20ef8dc1":"df.info()","5589cc90":"X = df.drop(columns=['Winner'])\nY = df['Winner']","a7bc2137":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)","3e32256f":"print(\"Training size = \" + str(X_train.shape[0]))\nprint(\"Testing size = \" + str(X_test.shape[0]))","39139940":"seed = 404\nnp.random.seed(seed)","3ac0e29f":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(gnb, X_train, y_train.values.ravel(), cv=kfold)\ngnb_score = cv_score.mean()\nprint('Gaussian Naive Bayes K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Gaussian Naive Bayes Average Score:')\nprint(gnb_score)\nprint()","4b7da982":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter = 10000)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(lr, X_train, y_train.values.ravel(), cv=kfold)\nlr_score = cv_score.mean()\nprint('Logistic Regression K-fold Scores (training):')\nprint(cv_score)\nprint()\nprint('Logistic Regression Average Score:')\nprint(lr_score)","f21fec83":"from sklearn import tree\n\ndt = tree.DecisionTreeClassifier(random_state = 1)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(dt, X_train, y_train.values.ravel(), cv=kfold)\ndt_score = cv_score.mean()\nprint('Decision Tree K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Decision Tree Average Score:')\nprint(dt_score)","37362c0a":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(knn, X_train, y_train.values.ravel(), cv=kfold)\nknn_score = cv_score.mean()\nprint('KNN K-fold Scores):')\nprint(cv_score)\nprint()\nprint('KNN Average Score:')\nprint(knn_score)","ecce6dd8":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state = 1)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(rf, X_train, y_train.values.ravel(), cv=kfold)\nrf_score = cv_score.mean()\nprint('Random Forest K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Random Forest Average Score:')\nprint(rf_score)","72f5e1f1":"from sklearn.svm import SVC\n\nsvc = SVC(probability = True)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(svc, X_train, y_train.values.ravel(), cv=kfold)\nsvc_score = cv_score.mean()\nprint('Support Vector Classification K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Support Vector Classification Average Score:')\nprint(svc_score)","ee67b340":"import xgboost\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(objective='binary:logistic',random_state =1, use_label_encoder=False)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(xgb, X_train, y_train.values.ravel(), cv=kfold)\nxgb_score = cv_score.mean()\nprint('XGBoost Classifier K-fold Scores:')\nprint(cv_score)\nprint()\nprint('XGBoost Classifier Average Score:')\nprint(xgb_score)","54cc5557":"from keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\nencoder.fit(y_train)\nencoded_Y = encoder.transform(y_train)\ny_Train = np_utils.to_categorical(encoded_Y)\n\nencoder = LabelEncoder()\nencoder.fit(y_test)\ny_Test = encoder.transform(y_test)","8906572c":"import keras \nfrom keras.models import Sequential\nfrom keras.layers import Dense\n# from keras import layers, models, optimizers\n\n\ndef create_model():\n    model = Sequential()\n    \n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu'))\n    model.add(Dense(64, activation='tanh'))\n    model.add(Dense(128, activation='tanh'))\n    model.add(Dense(128, activation='tanh'))    \n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(2, activation='sigmoid'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","c86e760e":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import KFold\n\nseed = 7\nnp.random.seed(seed)\n\nmodel = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n\n\nkfold = KFold(n_splits=10, shuffle=True)\nresults = cross_val_score(model, X_train, y_Train, cv=kfold)\nnn_score = cv_score.mean()\nprint('Neural Network K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Neural Network Average Score:')\nprint(nn_score)","e3de6f4d":"scores = [['Gaussian Naive Bayes', gnb_score],\n ['Logistic Regression', lr_score],\n ['Random Forest', rf_score],\n ['Decision Tree', dt_score],\n ['K-Nearest Neighbor', knn_score],\n ['Support Vector Classifier', svc_score],\n ['XGBoost', xgb_score],\n ['Neural Network', nn_score]]\n\ndf_scores = pd.DataFrame(scores,\n                         columns = ['Model', 'Score Average']\n                        )\ndf_scores","b5ecdd34":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nGNB = GaussianNB()\nGNB_model = GNB.fit(X_train, y_train.values.ravel())\ny_pred = GNB_model.predict(X_test)\n\ndisp = plot_confusion_matrix(GNB_model, X_test, y_test)\ndisp.ax_.set_title('Gaussian Naive Bayes Confusion Matrix')\n\nplt.show()\nprint('Gaussian Naive Bayes Model Accuracy (on testing set): ')\nprint(accuracy_score(y_test, y_pred))","5704a133":"lr = LogisticRegression(max_iter = 10000)\nlr_model = lr.fit(X_train, y_train.values.ravel())\ny_pred = lr_model.predict(X_test)\n\ndisp = plot_confusion_matrix(lr_model, X_test, y_test)\ndisp.ax_.set_title('Logistic Regression Confusion Matrix')\n\nplt.show()\nprint('Logistic Regression Model Accuracy (on testing set): ')\nprint(accuracy_score(y_test, y_pred))","8eb6ab1b":"rf = RandomForestClassifier(random_state = 1)\nrf_model = rf.fit(X_train, y_train.values.ravel())\ny_pred = rf_model.predict(X_test)\ndisp = plot_confusion_matrix(rf_model, X_test, y_test)\ndisp.ax_.set_title('Random Forest Confusion Matrix')\n\nplt.show()\n\nprint('Random Forest Model Accuracy (on testing set): ')\nprint(accuracy_score(y_test, y_pred))","ac0514d6":"svc = SVC(probability = True)\nsvc_model = svc.fit(X_train, y_train.values.ravel())\ny_pred = svc_model.predict(X_test)\ndisp = plot_confusion_matrix(svc_model, X_test, y_test)\ndisp.ax_.set_title('Support Vector Classifier Confusion Matrix')\n\nplt.show()\n\nprint('SVC Model Accuracy (on testing set): ')\nprint(accuracy_score(y_test, y_pred))","feac1e61":"columns = X.columns","7a7cc1d1":"fight1 = [5, 0, 4, 0, 8, 27, 0, 188, 198, 205, 0, 20, 0, 0, 20, 1, 193, 293, 193]\nfight2 = [5, 0, 11, 0, 4, 20, 0, 173, 165, 145, 0, 2, 0, 4, 11, 0, 183, 183, 145]\nfight3 = [5, 0, 10, 0, 1, 15, 1, 170, 170, 135, 0, 5, 0, 3, 19, 0, 170, 180, 135]\nfight4 = [3, 0, 6, 0, 1, 18, 0, 178, 178, 155, 0, 3, 0, 9, 23, 2, 173, 178, 155]\nfight5 = [3, 2, 0, 0, 8, 21, 0, 188, 193, 205, 0, 1, 0, 2, 13, 0, 193, 198, 205]\n\ndf1 = pd.DataFrame(np.array([fight1, fight2, fight3, fight4, fight5]), columns = columns)","9ba99e84":"svc_model.predict(df1)","741614cf":"rf_model.predict(df1)","6e3e2112":"lr_model.predict(df1)","f7123152":"df['Winner'].value_counts()","fc009477":"df = df[df['Winner'] != 'Draw']","ff5523bc":"df = df.drop(columns=['R_fighter', 'B_fighter', 'Referee', 'date', 'location', 'title_bout', 'weight_class', 'B_draw', 'R_draw'])","e1ad7243":"mapping = {'Orthodox': 0, 'Switch': 1, 'Southpaw': 2, 'Open Stance': 3}\ndf['B_Stance'] = df['B_Stance'].replace(mapping)\ndf['R_Stance'] = df['R_Stance'].replace(mapping)","6859045d":"X = df.drop(columns=['Winner'])\nY = df['Winner']\nmapping = {'Red': 0, 'Blue': 1}\nY = Y.replace(mapping)","23cc3f27":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)","40f14284":"seed = 404\nnp.random.seed(seed)","77e3dbd5":"gnb = GaussianNB()\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(gnb, X_train, y_train.values.ravel(), cv=kfold)\nprint('Gaussian Naive Bayes K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Gaussian Naive Bayes Average Score:')\nprint(cv_score.mean())\nprint()","314e0d8e":"lr = LogisticRegression(max_iter = 10000)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(lr, X_train, y_train.values.ravel(), cv=kfold)\nprint('Logistic Regression K-fold Scores (training):')\nprint(cv_score)\nprint()\nprint('Logistic Regression Average Score:')\nprint(cv_score.mean())","b77687e1":"dt = tree.DecisionTreeClassifier(random_state = 1)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(dt, X_train, y_train.values.ravel(), cv=kfold)\nprint('Decision Tree K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Decision Tree Average Score:')\nprint(cv_score.mean())","011340af":"knn = KNeighborsClassifier()\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(knn, X_train, y_train.values.ravel(), cv=kfold)\nprint('KNN K-fold Scores):')\nprint(cv_score)\nprint()\nprint('KNN Average Score:')\nprint(cv_score.mean())","cd6c5db7":"rf = RandomForestClassifier(random_state = 1)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(rf, X_train, y_train.values.ravel(), cv=kfold)\nprint('Random Forest K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Random Forest Average Score:')\nprint(cv_score.mean())","2cd02ab0":"svc = SVC(probability = True)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(svc, X_train, y_train.values.ravel(), cv=kfold)\nprint('Support Vector Classification K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Support Vector Classification Average Score:')\nprint(cv_score.mean())","32ef5ad3":"xgb = XGBClassifier(objective='binary:logistic',random_state =1, use_label_encoder=False)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(xgb, X_train, y_train.values.ravel(), cv=kfold)\nprint('XGBoost Classifier K-fold Scores:')\nprint(cv_score)\nprint()\nprint('XGBoost Classifier Average Score:')\nprint(cv_score.mean())","8bf89769":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\n\nencoder = LabelEncoder()\nencoder.fit(y_train)\nencoded_Y = encoder.transform(y_train)\ny_Train = np_utils.to_categorical(encoded_Y)\n\nencoder = LabelEncoder()\nencoder.fit(y_test)\ny_Test = encoder.transform(y_test)\n\n\ndef create_model():\n    model = Sequential()\n    \n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu'))\n    model.add(Dense(X_train.shape[1]*2, activation='tanh'))\n    model.add(Dense(X_train.shape[1]*4, activation='tanh'))\n    model.add(Dense(X_train.shape[1]*2, activation='tanh'))    \n    model.add(Dense(X_train.shape[1], activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","0e562a10":"seed = 7\nnp.random.seed(seed)\n\nmodel = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n\n\nkfold = KFold(n_splits=10, shuffle=True)\nresults = cross_val_score(model, X_train, y_Train, cv=kfold)\nprint('Neural Network K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Neural Network Average Score:')\nprint(cv_score.mean())","7c788679":"lr = LogisticRegression(max_iter = 2000)\nlr_model = lr.fit(X_train, y_train.values.ravel())\ny_pred = lr_model.predict(X_test)\n\ndisp = plot_confusion_matrix(lr_model, X_test, y_test)\ndisp.ax_.set_title('Logistic Regression Confusion Matrix')\n\nplt.show()\nprint('Logistic Regression Model Accuracy (on testing set): ')\nprint(accuracy_score(y_test, y_pred))","50341e10":"rf = RandomForestClassifier(random_state = 1)\nrf_model = rf.fit(X_train, y_train.values.ravel())\ny_pred = rf_model.predict(X_test)\ndisp = plot_confusion_matrix(rf_model, X_test, y_test)\ndisp.ax_.set_title('Random Forest Confusion Matrix')\n\nplt.show()\n\nprint('Random Forest Model Accuracy (on testing set): ')\nprint(accuracy_score(y_test, y_pred))","6548250c":"svc = SVC(probability = True)\nsvc_model = svc.fit(X_train, y_train.values.ravel())\ny_pred = svc_model.predict(X_test)\ndisp = plot_confusion_matrix(svc_model, X_test, y_test)\ndisp.ax_.set_title('Support Vector Classifier Confusion Matrix')\n\nplt.show()\n\nprint('SVC Model Accuracy (on testing set): ')\nprint(accuracy_score(y_test, y_pred))","313aa596":"xgb = XGBClassifier(objective='binary:logistic',random_state =1, use_label_encoder=False)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(xgb, X_train, y_train.values.ravel(), cv=kfold)\nprint('XGBoost Classifier K-fold Scores:')\nprint(cv_score)\nprint()\nprint('XGBoost Classifier Average Score:')\nprint(cv_score.mean())","56334bc4":"# Final Thoughts\n\nIf we look at the current under\/over odds in the betting world, most agree with the model predictions for Fight 1 and Fight 5, but only the Random Forest is in line with the odds for Fight 4. Fight 3 has the odds at -110 to -110, so Vegas seems to be split evenly on this fight.\n\nI hope to expand the models to take in more features, and as a fan I can't help but think strike %, take down %, take down defence %, and a number of the various other features definitely come into play when accessing winner outcome.\n\nI wouldn't take these models straight to the bookies, but it was a fun exercise to run and explore the data\/models.\n\nOn a personal note, I would never bet against Amanda Nunes or Islam Makhachev.\n\nI hope you enjoyed this!","e4ac6758":"## Support Vector Classifier:","fcecd6f7":"# Model Predictions: (0 indicates Red Fighter Wins, 1 indicates Blue Fighter Wins)","d09c7336":"# Mixed Martial Arts and the UFC\n\n\nThe UFC is the largest MMA promotion company in the world and features some of the highest-level fighters in the sport. As of 2020 the UFC has held over 500 events features fighters in 12 different weight divisions. The data set is a collection of over 5000 fights from the years 1993 to 2019.\n\nBeing a huge fan of MMA, I wanted to design some Machine Learning Models to experiment with the avaiable data. The goal is to make a model to predict fight outcomes, and see if it has any usefulness in real world application.\n\nIn this particular notebook I reduce the data down to (what I felt was) core stats, so despite this dataset having over 145 features, I reduce it down to height, weight, reach, win streak, lose streak, total wins, total losses, and total draws. In the future I will apply more features to see if the model accuracy improves at all.\n\nIn this notebook I use the following algorithms for model building:\n* Gaussian Naive Bayes\n* Logistic Regression\n* Decision Tree\n* KNN\n* Random Forest\n* Support Vector Classifier\n* XGBoost\n* Artificial Neural Network\n\nThe models with the highest accuracy score (Using k-fold cross-validation) on the training data are then accessed on the testing data.\n\nFinally the models that performed well are then applied to the upcoming event (March 6th 2021), to make predictions on fight winners.","04757bf9":"# Full Feature Modeling\nWe'll reduce the problem to a binary classification problem, especially with the new scoring system draws don't occur in the UFC anymore.","626a1ead":"## Model Training and Evaluation on Data using k-fold cross validation","bac14f51":"## Random Forest:","dde38308":"### Import and clean data for use in models","fe66a2d5":"### Predicted Winners:\n* Fight 1 - Israel Adesanya\n* Fight 2 - Megan Anderson\n* Fight 3 - Aljamain Sterling\n* Fight 4 - Drew Dober\n* Fight 5 - Aleksandar Rakic","4800fb81":"## Best performing models\n\nWith the training accuracy in mind, we will grab the top 3 models and evaluate them on the testing set","dd272c02":"### Predicted Winners:\n* Fight 1 - Israel Adesanya\n* Fight 2 - Megan Anderson\n* Fight 3 - Aljamain Sterling\n* Fight 4 - Islam Makhachev\n* Fight 5 - Aleksandar Rakic","22bc3d59":"## Logistic Regression:","def0f46c":"# Prediction Time","796dafca":"# UFC 259: Blachowicz vs Adesanya\n\n### Title fights (5 rounds):\nJan Blachowicz vs Israel Adesanya\n\nAmanda Nunes vs Megan Anderson\n\nPetr Yan vs Aljamain Sterling\n\n### 3 round fights:\nIslam Makhachev vs Drew Dober\n\nThiago Santos vs Aleksandar Rakic\n\n\n## The Stats: ([https:\/\/www.espn.co.uk\/mma\/fightcenter\/_\/id\/600001860\/league\/ufc](http:\/\/))\n### Fight 1\n#### Jan Blachowicz (Blue):\n* Current Lose Streak: 0\n* Current Win Streak: 4\n* Draws: 0\n* Losses: 8\n* Wins: 27\n* Stance: Orthodox\n* Height: 188\n* Reach: 198\n* Weight: 205\n\n#### Israel Adesanya (Red):\n* Current Lose Streak: 0\n* Current Win Streak: 20\n* Draws: 0\n* Losses: 0\n* Wins: 20\n* Stance: Switch\n* Height: 193\n* Reach: 203\n* Weight: 193 (speculation based on interview, weigh-ins to come)\n\n\n### Fight 2\n#### Amanda Nunes (Blue):\n* Current Lose Streak: 0\n* Current Win Streak: 11\n* Draws: 0\n* Losses: 4\n* Wins: 20\n* Stance: Orthodox\n* Height: 173\n* Reach: 165\n* Weight: 145\n\n#### Megan Anderson (Red):\n* Current Lose Streak: 0\n* Current Win Streak: 2\n* Draws: 0\n* Losses: 4\n* Wins: 11\n* Stance: Orthodox\n* Height: 183\n* Reach: 183\n* Weight: 145\n\n### Fight 3\n#### Petr Yan (Blue):\n* Current Lose Streak: 0\n* Current Win Streak: 10\n* Draws: 0\n* Losses: 1\n* Wins: 15\n* Stance: Switch\n* Height: 170\n* Reach: 170\n* Weight: 135\n\n#### Aljamain Sterling (Red):\n* Current Lose Streak: 0\n* Current Win Streak: 5\n* Draws: 0\n* Losses: 3\n* Wins: 19\n* Stance: Orthodox\n* Height: 170\n* Reach: 180\n* Weight: 135\n\n### Fight 4\n#### Islam Makhachev (Blue):\n* Current Lose Streak: 0\n* Current Win Streak: 6\n* Draws: 0\n* Losses: 1\n* Wins: 18\n* Stance: Orthodox\n* Height: 178\n* Reach: 178\n* Weight: 155\n\n#### Drew Dober (Red):\n* Current Lose Streak: 0\n* Current Win Streak: 3\n* Draws: 0\n* Losses: 9\n* Wins: 23\n* Stance: Southpaw\n* Height: 173\n* Reach: 178\n* Weight: 155\n\n### Fight 5\n#### Thiago Santos (Blue):\n* Current Lose Streak: 2\n* Current Win Streak: 0\n* Draws: 0\n* Losses: 8\n* Wins: 21\n* Stance: Orthodox\n* Height: 188\n* Reach: 193\n* Weight: 205\n\n#### Aleksandar Rakic (Red):\n* Current Lose Streak: 0\n* Current Win Streak: 1\n* Draws: 0\n* Losses: 2\n* Wins: 13\n* Stance: Orthodox\n* Height: 193\n* Reach: 198\n* Weight: 205","327aa8ca":"### Predicted Winners:\n* Fight 1 - Israel Adesanya\n* Fight 2 - Megan Anderson\n* Fight 3 - Aljamain Sterling\n* Fight 4 - Drew Dober\n* Fight 5 - Aleksandar Rakic","f22cfce1":"With all the stats available to us, it can create a data frame to feed into our models and get predictions"}}