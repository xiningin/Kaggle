{"cell_type":{"ebb1a44b":"code","a5de1092":"code","edb43479":"code","445fdb2f":"code","9e793222":"code","6500fffd":"code","b1b3bc49":"code","03783108":"code","7f3b4ee5":"code","8a6ff1f6":"code","a4ae4df1":"code","8bfa1eac":"code","1c31c29a":"code","c80e187b":"code","051ad3aa":"code","08409924":"code","44293cf3":"code","e9918f31":"code","21dee166":"code","5a33f07a":"code","0dd1cbe3":"code","7745e47e":"code","67af8c5d":"code","5d09c0e3":"code","aa327f3e":"code","037035f7":"code","292197a1":"code","95d7b1c9":"code","f5adb62d":"markdown","c4d0fbe0":"markdown","0470da4a":"markdown","05902317":"markdown","2d506e3b":"markdown","aaec2c65":"markdown","2061c401":"markdown","b5418331":"markdown"},"source":{"ebb1a44b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport matplotlib.pyplot as plt\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport nltk\nfrom wordcloud import WordCloud,STOPWORDS\nimport string\nimport re\nimport tensorflow\nfrom scipy import stats\nfrom heapq import nlargest\nimport spacy as sp\nnlps = sp.load('en')\nfiles = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        with open((os.path.join(dirname, filename))) as w: files.append(w.read())\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize","a5de1092":"f_names = [f.replace('.txt','') for f in filenames]\nmonths = ['Jan','Feb','Mar','Apr','May','Jul','Jun','Aug','Sep','Oct','Nov','Dec']\ncity,r_month,date = [],[],[]\nfor name in f_names:\n    index= -1\n    for month in months:\n        index =name.find(month)\n        if index != -1:\n            r_month.append(month)\n            break\n    city.append(name[:index])\n    date.append(name[index+3:])\n\n    \n    \nrallies_df = pd.DataFrame({'Month':r_month,'Year':date,'City':city,'Speech':files})\nrallies_df['Day'] = rallies_df['Year'].apply(lambda x: x.split('_')[0])\nrallies_df['Year'] = rallies_df['Year'].apply(lambda x: x.split('_')[1])\nrallies_df = rallies_df[['Day','Month','Year','City','Speech']]\nrallies_df['City'] = rallies_df['City'].apply(lambda x:  ' '.join(re.sub( r\"([A-Z])\", r\" \\1\", x).split()))\nrallies_df['Speech'] = rallies_df['Speech'].apply(lambda x: x.strip().lower())\n\n","edb43479":"rallies_df.head(5)","445fdb2f":"rallies_df['# Of Words'] = rallies_df['Speech'].apply(lambda x: len(x.split(' ')))\nrallies_df['# Of StopWords'] = rallies_df['Speech'].apply(lambda x: len([word for word in x.split(' ') if word in list(STOPWORDS)]))\nrallies_df['# Of Sentences'] = rallies_df['Speech'].apply(lambda x: len(re.findall('\\.',x)))\nrallies_df['Average Word Length'] = rallies_df['Speech'].apply(lambda x: np.mean(np.array([len(va) for va in x.split(' ') if va not in list(STOPWORDS)])))\nrallies_df['Average Sentence Length'] = rallies_df['Speech'].apply(lambda x: np.mean(np.array([len(va) for va in x.split('.')])))\n\nrallies_df['Speech'] = rallies_df['Speech'].apply(lambda x: re.sub(r\"[,.;@#?!&$]+\", ' ', x))\n","9e793222":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()\nrallies_df['sentiments'] = rallies_df['Speech'].apply(lambda x: sid.polarity_scores(x))\nrallies_df['Positive Sentiment'] = rallies_df['sentiments'].apply(lambda x: x['pos']) \nrallies_df['Neutral Sentiment'] = rallies_df['sentiments'].apply(lambda x: x['neu'])\nrallies_df['Negative Sentiment'] = rallies_df['sentiments'].apply(lambda x: x['neg'])\nrallies_df.drop(columns=['sentiments'],inplace=True)","6500fffd":"rallies_df['State'] = ['Mississippi','Minnesota','Ohio','Michigan','New Hampshire','Nevada','New Jersey','Texas','Iowa','North Carolina','Michigan','Kentucky','Wisconsin','Arizona',\n                       'Oklahoma','Minnesota','New Hampshire','Pennsylvania','Colorado','Pennsylvania','Ohio','South Carolina','North Carolina','Nevada','North Carolina','New Hampshire',\n                       'North Carolina','Ohio','Texas','Wisconsin','Nevada','South Carolina','New Mexico','Arizona','Pennsylvania']","b1b3bc49":"rallies_df['# Of Different Countries Mentioned'] = rallies_df['Speech'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'GPE' ]))\nrallies_df['# Of Times Money Was Mentioned'] = rallies_df['Speech'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'MONEY' ]))\nrallies_df['# Of Different People Mentioned'] = rallies_df['Speech'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'PERSON' ]))","03783108":"rallies_df.head(5)","7f3b4ee5":"tr1 = go.Bar(x=rallies_df.Month.unique(),y=rallies_df.Month.value_counts().values)\nlayout = dict(title='Distribution Of Rallies Over Different Months',yaxis_title='Number Of Rallies Made')\ngo.Figure(data=[tr1],layout=layout)","8a6ff1f6":"ex.pie(rallies_df,'Year',title='Years Of Rallies In Our Dataset')","a4ae4df1":"plt.figure(figsize=(20,11))\nax = sns.kdeplot(rallies_df['Positive Sentiment'],lw=3)\nax = sns.kdeplot(rallies_df['Neutral Sentiment'],lw=3)\nax = sns.kdeplot(rallies_df['Negative Sentiment'],lw=3)\nax.set_xlabel('Sentiment Value',fontsize=20)\nax.set_ylabel('Density',fontsize=20)\nax.set_title('Trumps Speech Sentiments Distribution',fontsize=20,fontweight='bold')\nplt.legend(prop=dict(size=20))\nplt.show()","8bfa1eac":"slope_neg, intercept_neg, r_value_neg, p_value_neg, std_err_neg = stats.linregress(rallies_df['# Of Sentences'],(rallies_df['Negative Sentiment']-rallies_df['Negative Sentiment'].mean())\/rallies_df['Negative Sentiment'].std())\nslope_pos, intercept_pos, r_value_pos, p_value_pos, std_err_pos = stats.linregress(rallies_df['# Of Sentences'],(rallies_df['Positive Sentiment']-rallies_df['Positive Sentiment'].mean())\/rallies_df['Positive Sentiment'].std())\n\n\ntr1= go.Scatter(x = rallies_df['# Of Sentences'],y=(rallies_df['Negative Sentiment']-rallies_df['Negative Sentiment'].mean())\/rallies_df['Negative Sentiment'].std(),mode='markers',name='Negative Sentiment')\ntr15= go.Scatter(x = rallies_df['# Of Sentences'],y=slope_neg*rallies_df['# Of Sentences']+intercept_neg,name='Negative Trend')\ntr2= go.Scatter(x = rallies_df['# Of Sentences'],y=(rallies_df['Positive Sentiment']-rallies_df['Positive Sentiment'].mean())\/rallies_df['Positive Sentiment'].std(),mode='markers',name='Positive Sentiment')\ntr25= go.Scatter(x = rallies_df['# Of Sentences'],y=slope_pos*rallies_df['# Of Sentences']+intercept_pos,name='Positive Trend')\n\nfig = go.Figure(data=[tr1,tr2,tr15,tr25],layout=dict(title='Spread Of Sentiment vs Number Of Sentences In Trumps Speech',yaxis_title='Z Score',xaxis_title='Number Of Sentences'))\nfig.show()","1c31c29a":"state_counts = rallies_df.State.value_counts()\nstates = [\"AL - Alabama\", \"AK - Alaska\", \"AZ - Arizona\", \"AR - Arkansas\", \"CA - California\", \"CO - Colorado\",\n\"CT - Connecticut\", \"DC - Washington DC\", \"DE - Deleware\", \"FL - Florida\", \"GA - Georgia\",\n\"HI - Hawaii\", \"ID - Idaho\", \"IL - Illinios\", \"IN - Indiana\", \"IA - Iowa\",\n\"KS - Kansas\", \"KY - Kentucky\", \"LA - Louisiana\", \"ME - Maine\", \"MD - Maryland\",\n\"MA - Massachusetts\", \"MI - Michigan\", \"MN - Minnesota\", \"MS - Mississippi\",\n\"MO - Missouri\", \"MT - Montana\", \"NE - Nebraska\", \"NV - Nevada\", \"NH - New Hampshire\",\n\"NJ - New Jersey\", \"NM - New Mexico\", \"NY - New York\", \"NC - North Carolina\",\n\"ND - North Dakota\", \"OH - Ohio\", \"OK - Oklahoma\", \"OR - Oregon\", \"PA - Pennsylvania\",\n\"RI - Rhode Island\", \"SC - South Carolina\", \"SD - South Dakota\", \"TN - Tennessee\",\n\"TX - Texas\", \"UT - Utah\", \"VT - Vermont\", \"VA - Virgina\", \"WA - Washington\", \"WV - West Virginia\",\n\"WI - Wisconsin\", \"WY - Wyoming\"]\nstates = {s.split(' - ')[1]:s.split(' - ')[0] for s in states}\nlos = pd.Series(state_counts.index.values).apply(lambda x: states[x])\nfig = ex.choropleth(locations=los, locationmode=\"USA-states\",color = state_counts.values, scope=\"usa\",color_continuous_scale=ex.colors.diverging.Portland)\n\nfig.update_layout(title = \"Number Of Rallies At Each State\")\nfig.show()","c80e187b":"r_df = rallies_df.copy()\nr_df['State'] = r_df['State'].apply(lambda x: states[x])\nr_df = r_df.groupby(by='State').mean()\nr_df = r_df.reset_index()\n\nfig = ex.choropleth(r_df,locations ='State', locationmode=\"USA-states\",color ='Positive Sentiment' , scope=\"usa\",color_continuous_scale=ex.colors.diverging.Portland)\n\nfig.update_layout(title = \"How Positive Was The Speech At Each State\")\nfig.show()","051ad3aa":"fig = ex.choropleth(r_df,locations ='State', locationmode=\"USA-states\",color ='Negative Sentiment' , scope=\"usa\",color_continuous_scale=ex.colors.diverging.Portland)\n\nfig.update_layout(title = \"How Negative Was The Speech At Each State\")\nfig.show()","08409924":"fig = ex.choropleth(r_df,locations ='State', locationmode=\"USA-states\",color ='# Of Sentences' , scope=\"usa\",color_continuous_scale=ex.colors.diverging.Portland)\n\nfig.update_layout(title = \"Number Of Sentences Said At Each State\")\nfig.show()","44293cf3":"fig = ex.choropleth(r_df,locations ='State', locationmode=\"USA-states\",color ='Average Sentence Length' , scope=\"usa\",color_continuous_scale=ex.colors.diverging.Portland)\n\nfig.update_layout(title = \"Average Sentence Length At Each State\")\nfig.show()","e9918f31":"fig = ex.choropleth(r_df,locations ='State', locationmode=\"USA-states\",color ='# Of Different Countries Mentioned' , scope=\"usa\",color_continuous_scale=ex.colors.diverging.Portland)\n\nfig.update_layout(title = \"Number Of Different Countries Mentioned During The Speech At Each State\")\nfig.show()","21dee166":"fig = ex.choropleth(r_df,locations ='State', locationmode=\"USA-states\",color ='# Of Times Money Was Mentioned' , scope=\"usa\",color_continuous_scale=ex.colors.diverging.Portland)\n\nfig.update_layout(title = \"Number Of Times Money Was Mentioned During The Speech At Each State\")\nfig.show()","5a33f07a":"fig = ex.choropleth(r_df,locations ='State', locationmode=\"USA-states\",color ='# Of Different People Mentioned' , scope=\"usa\",color_continuous_scale=ex.colors.diverging.Portland)\n\nfig.update_layout(title = \"Number Of Different People Names Mentioned During The Speech At Each State\")\nfig.show()","0dd1cbe3":"fig,axes = plt.subplots(8,4)\nfig.set_figwidth(25)\nfig.set_figheight(25)\nr,c=0,0\n\nfor cit in rallies_df.City:\n    text = rallies_df.query('City == \"{}\"'.format(cit))\n    text = ' '.join(text.Speech.values)\n    axes[r][c].imshow(WordCloud(stopwords=STOPWORDS).generate(text))\n    axes[r][c].axis('off')\n    axes[r][c].set_title('Most Frequent Words Spoken In {}'.format(cit),fontsize=13,fontweight='bold')\n    c+=1\n    if c == 4:\n        c = 0\n        r+=1\n    if r == 8:\n        break\nplt.tight_layout()\n        ","7745e47e":"fig,axes = plt.subplots(8,4)\nfig.set_figwidth(25)\nfig.set_figheight(25)\nr,c=0,0\n\nfor city in rallies_df.City:\n    neg_word_list = []\n    text = rallies_df.query('City == \"{}\"'.format(cit))\n    text = ' '.join(text.Speech.values)\n    tokenized_sentence = text.split(' ')\n    tokenized_sentence = [w for w in tokenized_sentence if w not in list(STOPWORDS)]\n    for word in tokenized_sentence:\n        if (sid.polarity_scores(word)['compound']) <= -0.1:\n            neg_word_list.append(word)\n    axes[r][c].imshow(WordCloud(stopwords=STOPWORDS).generate(' '.join(neg_word_list)))\n    axes[r][c].axis('off')\n    axes[r][c].set_title('Most Negative Words Spoken In {}'.format(city),fontsize=12,fontweight='bold')\n    c+=1\n    if c == 4:\n        c = 0\n        r+=1\n    if r == 8:\n        break\n        \n        ","67af8c5d":"fig,axes = plt.subplots(8,4)\nfig.set_figwidth(25)\nfig.set_figheight(25)\nr,c=0,0\n\nfor city in rallies_df.City:\n    pos_word_list = []\n    text = rallies_df.query('City == \"{}\"'.format(cit))\n    text = ' '.join(text.Speech.values)\n    tokenized_sentence = text.split(' ')\n    tokenized_sentence = [w for w in tokenized_sentence if w not in list(STOPWORDS)]\n    for word in tokenized_sentence:\n        if (sid.polarity_scores(word)['compound']) >= 0.1:\n            pos_word_list.append(word)\n    axes[r][c].imshow(WordCloud(stopwords=STOPWORDS).generate(' '.join(pos_word_list)))\n    axes[r][c].axis('off')\n    axes[r][c].set_title('Most Positive Words Spoken In {}'.format(city),fontsize=12,fontweight='bold')\n    c+=1\n    if c == 4:\n        c = 0\n        r+=1\n    if r == 8:\n        break\n        \n        ","5d09c0e3":"pos_word_list = []\ntext = ' '.join(rallies_df.Speech.values)\ntokenized_sentence = text.split(' ')\ntokenized_sentence = [w for w in tokenized_sentence if w not in list(STOPWORDS)]\nfor word in tokenized_sentence:\n    if (sid.polarity_scores(word)['compound']) >= 0.1:\n        pos_word_list.append(word)\npos_word_list\npos_word_dict = {word:len(re.findall(word,text)) for word in pos_word_list}       ","aa327f3e":"neg_word_list = []\ntext = ' '.join(rallies_df.Speech.values)\ntokenized_sentence = text.split(' ')\ntokenized_sentence = [w for w in tokenized_sentence if w not in list(STOPWORDS)]\nfor word in tokenized_sentence:\n    if (sid.polarity_scores(word)['compound']) <= -0.1:\n        neg_word_list.append(word)\nneg_word_dict = {word:len(re.findall(word,text)) for word in neg_word_list}       \n\ntop_10_neg = nlargest(10, neg_word_dict, key=neg_word_dict.get)\ntop_10_pos = nlargest(10, pos_word_dict, key=pos_word_dict.get)\n\n\n\n","037035f7":"plt.figure(figsize=(20,15), facecolor = None)\nplt.imshow(WordCloud(width = 800, height = 800,background_color='white').generate(' '.join(nlargest(30, pos_word_dict, key=pos_word_dict.get))))\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.title('Top 30 Positive Words',fontsize=23,fontweight='bold')\nplt.show()","292197a1":"plt.figure(figsize=(20,15), facecolor = None)\nplt.imshow(WordCloud(width = 800, height = 800,background_color='white').generate(' '.join(nlargest(30, neg_word_dict, key=neg_word_dict.get))))\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.title('Top 30 Negative Words',fontsize=23,fontweight='bold')\nplt.show()","95d7b1c9":"l_t = ' '.join(rallies_df.Speech) \n\ntoken=nltk.word_tokenize(l_t)\ntrigram =ngrams(token,3)\ntrigram = [k for k in trigram if k[0] in top_10_pos]\n\ntoken=nltk.word_tokenize(l_t)\nbigram=ngrams(token,2)\nbigram_dict = dict()\nfor i in bigram:\n    bigram_dict[i] = bigram_dict.get(i,0)+1\n    \ntrigram_dict = dict()\nfor i in trigram:\n    trigram_dict[i] = trigram_dict.get(i,0)+1\n\ntoken=nltk.word_tokenize(l_t)\nbigram=ngrams(token,2)\nbigram_dict = dict()\nfor i in bigram:\n    bigram_dict[i] = bigram_dict.get(i,0)+1\n    \ntrigram_dict = dict()\nfor i in trigram:\n    trigram_dict[i] = trigram_dict.get(i,0)+1\n    \n    \n    \ntri_gram =pd.DataFrame(list(trigram_dict.keys())[:15],columns=['One Of Top 10 Words','Second Word','Third Word'])\n\ndef get_prob(sir):\n    key = (sir['One Of Top 10 Words'],sir['Second Word'],sir['Third Word'])\n    w3 = trigram_dict[key]\n    w2 = bigram_dict[(sir['One Of Top 10 Words'],sir['Second Word'])]\n    return w3\/w2\n\ntri_gram['Probabilty Of Sentence'] = tri_gram.apply(get_prob,axis=1)\n\ntri_gram.style.background_gradient(cmap='coolwarm')    ","f5adb62d":"### Most of Trump's rallies were in 2020","c4d0fbe0":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h1>\n","0470da4a":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing<\/h1>\n","05902317":"### We see that the distributions of negative and positive sentiments are distributed around 10 and 20 % and the neutral sentiment represent 70% of the sentiment score at each speech.","2d506e3b":"### We can see two trends, The first one shows that when Trump is using more sentences in his speech the less positive is the overall sentiment of the speech, and the second one is when he uses more sentences the overall negative sentiment get higher.","aaec2c65":"### We can see that most of trumps rallies were in August and December","2061c401":"## $\\text{State Base Analysis}$","b5418331":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Feature Engineering<\/h1>\n"}}