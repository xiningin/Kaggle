{"cell_type":{"fe0690c6":"code","1bb65932":"code","c013bc79":"code","f56509fd":"code","1643f9eb":"code","5674840e":"code","fae25438":"code","d1d9ef36":"code","8045bb9b":"code","dc372667":"code","62e9ebe8":"code","4e9890cb":"code","682dd181":"code","ecf78303":"code","7779d44b":"code","050217a3":"code","42c2e2d3":"code","931b9ae6":"code","595690ae":"code","aa3398cd":"code","f46b2ee2":"code","491a3cb5":"code","c7af90e4":"code","9d6876fb":"code","1fbdd26e":"markdown","0c345f16":"markdown","bbde14dd":"markdown","48194bbb":"markdown"},"source":{"fe0690c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\nimport gensim\nprint(os.listdir(\"..\/input\/embeddings\/GoogleNews-vectors-negative300\/\"))     \n### ^^^^***** under 'embeddings' -> GoogleNews, wiki-news... these folders to use for extracting files (.bin)\n\n# Any results you write to the current directory are saved as output.","1bb65932":"url = \"..\/input\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin\"\n\nembeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary = True)","c013bc79":"embeddings['sabermetrics'] ## presence of word in news","f56509fd":"embeddings['ghuiya'] ## NOT present in news; hence, ERROR","1643f9eb":"embeddings.most_similar('camera', topn = 10)   ## based on Cosine Similarity , to find similar terms","5674840e":"embeddings.doesnt_match(['king', 'woman', 'gandhi', 'sonia'])  ## odd one out \n\n## king - man = queen","fae25438":"embeddings.most_similar(positive = ['king', 'woman'], negative = ['man'], topn = 10) ","d1d9ef36":"url = 'https:\/\/bit.ly\/2S2yXEd'  ## from the Internet, downloading a link for text-classification\nimdb = pd.read_csv(url)\nimdb.shape","8045bb9b":"imdb.loc[0, 'review']","dc372667":"embeddings['A'] \n\n### ^^^^^^ checking the embedding value, for which the Gradient Descent formula is used to calculate weights ","62e9ebe8":"''' ## without stopwords ##\nimport nltk \n\ndocs_vectors = pd.DataFrame()\n\n## in below... all lowercase shall help in covering all the words, instead of adding \"\"A-Z\"\" in RegEx which may not provide suitable outputs\nfor doc in imdb['review'].str.lower().str.replace('[^a-z ]', ''):\n    temp = pd.DataFrame()   ## initially empty, and empty on every iteration\n    for word in nltk.word_tokenize(doc):  ## choose one word at a time from the doc from above\n        try:\n            word_vec = embeddings[word]  ## if present, the following code applies\n            temp = temp.append(pd.Series(word_vec), ignore_index = True)  ## .Series to make it easier to append \"without\" index labels\n        except:\n            pass\n    doc_vector = temp.mean()\n    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) ## added to the empty data frame\ndocs_vectors.shape ## 300 columns is a lot lesser'''","4e9890cb":"## with stopwords ##\n\nimport nltk\n\ndocs_vectors = pd.DataFrame()\nstopwords = nltk.corpus.stopwords.words('english')   ## !! added later\n\n## in below... all lowercase shall help in covering all the words, instead of adding \"\"A-Z\"\" in RegEx which may not provide suitable outputs\nfor doc in imdb['review'].str.lower().str.replace('[^a-z ]', ''):\n    temp = pd.DataFrame()   ## initially empty, and empty on every iteration\n    for word in doc.split(' '):  ## !!\n        if word not in stopwords: \n            try:\n                word_vec = embeddings[word]  ## if present, the following code applies\n                temp = temp.append(pd.Series(word_vec), ignore_index = True)  ## .Series to make it easier to append \"without\" index labels\n            except:\n                pass\n    doc_vector = temp.mean()\n    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) ## added to the empty data frame\ndocs_vectors.shape ## 300 columns is a lot lesser","682dd181":"docs_vectors.head()","ecf78303":"pd.isnull(docs_vectors).sum().sum() ## 600 nulls, when all in lowercase","7779d44b":"## adding a column in docs_vector of \"sentiment\"  + dropping the null values\ndocs_vectors['sentiment'] = imdb['sentiment']\ndocs_vectors = docs_vectors.dropna()","050217a3":"from sklearn.model_selection import train_test_split ## here vectorization (vectorizer) again shall not come, since we are calculated weights \nfrom sklearn.ensemble import AdaBoostClassifier\n\ntrain_x, test_x, train_y, test_y = train_test_split(docs_vectors.drop('sentiment', axis = 1),\n                                                   docs_vectors['sentiment'],\n                                                   test_size = 0.2,\n                                                   random_state = 1)\ntrain_x.shape, test_x.shape, train_y.shape, test_y.shape","42c2e2d3":"model = AdaBoostClassifier(n_estimators = 800, random_state = 1)\nmodel.fit(train_x, train_y)\ntest_pred = model.predict(test_x)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(test_y, test_pred)  \n\n## 74% accuracy without stopwords \n\n## 75.33% accuracy with stopwords","931b9ae6":"'''model = AdaBoostClassifier(n_estimators = 2000, random_state = 5)\nmodel.fit(train_x, train_y)\ntest_pred = model.predict(test_x)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(test_y, test_pred)  \n\n## 74% accuracy without stopwords \n\n## 75.33% accuracy with stopwords''' ## accuracy fell to 74.6% with stopwords","595690ae":"from nltk.sentiment import SentimentIntensityAnalyzer\n\nsentiment = SentimentIntensityAnalyzer()","aa3398cd":"reviews = imdb['review'].str.lower().str.replace('[^a-z ]', '')\nreviews","f46b2ee2":"imdb['sentiment'].value_counts()","491a3cb5":"def get_sentiment(text):\n    sentiment = SentimentIntensityAnalyzer() #### calling Intensity Analyzer\n    compound = sentiment.polarity_scores(text)['compound']  ### calling the 'compound' score for the \"text\" entered\n    if compound > 0:\n        return 1  ## positive\n    else:\n        return 0 ## negative\n    #else:\n        #return \"Neutral\"     \n    return compound\n\nimdb['sentiment_vader'] = imdb['review'].apply(get_sentiment) ### in the columns of \"imdb\"\nimdb['sentiment_vader'] ","c7af90e4":"get_sentiment(\"YES\")","9d6876fb":"from sklearn.metrics import accuracy_score\n\naccuracy_score(imdb['sentiment'], imdb['sentiment_vader']) ## == 79.011% of accuracy score","1fbdd26e":"## VADER package :\n\nValence Aware Dictionary and sEntiment Reasoner","0c345f16":"### Calculating Accuracy Score using VADER","bbde14dd":"NOTE:\n'''presence of punctuations, capitals make impact on the individual \/ overall score... so DO NOT clean data or change anything in the text to avoid distorting the score\n\nAlso, works well for shorter documents.\n\nSTOPWORDS shall be HEEDED!\n\nSingle letter words are IGNORED! '''","48194bbb":"#### FORMULA:\ncompound score = [score \/ sqrt{(score^2)+alpha}]****"}}