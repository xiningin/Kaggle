{"cell_type":{"5499147d":"code","db189337":"code","8bd36a38":"code","5e23f58b":"code","1643dd86":"code","5501ad9e":"code","dcc80bcb":"code","28d94b26":"code","240ec7a3":"code","a5403ae5":"code","dad502fd":"code","31be083f":"code","3c18fad7":"code","85b248f1":"code","123f4580":"code","cff7553c":"code","0c0db3e3":"code","accb04ed":"code","8e18430f":"code","296ee3f7":"code","ebb38fc3":"code","526aa045":"code","c9ec6607":"code","bcbe143c":"code","0eeed001":"code","c69ad8e2":"markdown","61809af3":"markdown","02430613":"markdown","b0865357":"markdown","dfdf5ffc":"markdown","a0594dd0":"markdown","76e45643":"markdown","c8c5542b":"markdown","33e2020f":"markdown","3035677d":"markdown","2522a122":"markdown","21d8bc38":"markdown"},"source":{"5499147d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns","db189337":"data_train=pd.read_csv('..\/input\/PL_XSELL.csv')","8bd36a38":"data_train.shape","5e23f58b":"data_train.head().T","1643dd86":"data_train=data_train.drop(['random','CUST_ID'],axis=1)","5501ad9e":"import matplotlib.pyplot as plt","dcc80bcb":"plt.pie(data_train['GENDER'].value_counts(),labels=data_train['GENDER'].value_counts().index,autopct='%1.1f%%')","28d94b26":"plt.subplots(figsize=(16,8))\nsns.heatmap(data_train.corr()[data_train.corr().abs()>0.1],annot=True)","240ec7a3":"# convert factors to labels\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata_train['GENDER'] = le.fit_transform(data_train['GENDER'])\ndata_train['AGE_BKT'] = le.fit_transform(data_train['AGE_BKT'])\ndata_train['OCCUPATION'] = le.fit_transform(data_train['OCCUPATION'])\ndata_train['ACC_TYPE'] = le.fit_transform(data_train['ACC_TYPE'])\ndata_train['OCCUPATION'] = le.fit_transform(data_train['OCCUPATION'])","a5403ae5":"data_train.info()","dad502fd":"data_train['ACC_OP_DATE']=pd.to_datetime(data_train['ACC_OP_DATE'])","31be083f":"data_train['ACC_OP_YR']=data_train['ACC_OP_DATE'].dt.year","3c18fad7":"from sklearn.preprocessing import StandardScaler\nx=data_train.drop(['ACC_OP_DATE'],axis=1)\nscaler=StandardScaler().fit(x)\ny=pd.DataFrame(scaler.transform(x),columns=x.columns)\ny.boxplot(vert=False,figsize=(15,10))","85b248f1":"data_train.describe().T","123f4580":"# Load libraries\nfrom matplotlib import pyplot\nfrom matplotlib import pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","cff7553c":"X=data_train.drop(['AGE','TARGET','ACC_OP_DATE'],axis=1)\nY=data_train[['TARGET']]","0c0db3e3":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nx_features = list(X)\n\nx_features.remove('TOT_NO_OF_L_TXNS')\nx_features.remove('ACC_OP_YR')\n\n\ndata_mat = X[x_features].as_matrix()\ndata_mat.shape\nvif = [ variance_inflation_factor( data_mat,i) for i in range(data_mat.shape[1]) ]\nvif_factors = pd.DataFrame()\nvif_factors['column'] = list(x_features)\nvif_factors['vif'] = vif\nprint(vif_factors)","accb04ed":"X=data_train[x_features]\nvalidation_size = 0.30\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)","8e18430f":"# Test options and evaluation metric\nnum_folds = 10\nseed = 7\nscoring = 'accuracy'\n# Spot-Check Algorithms\nmodels = []\nmodels.append(('Logistic', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('GB',GradientBoostingClassifier()))","296ee3f7":"# evaluate each model in turn\nresults = []\nnames = []\nmodel_comp=pd.DataFrame(columns=['Model','Test Accuracy','Std.Dev'])\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    model_comp=model_comp.append([{'Model':name, 'Test Accuracy':cv_results.mean(), 'Std.Dev':cv_results.std()}],ignore_index=True)\n    \nmodel_comp","ebb38fc3":"model=DecisionTreeClassifier(max_depth=15)\nmodel=model.fit(X_train,Y_train)\nmodel.score(X_train,Y_train)","526aa045":"model.score(X_validation,Y_validation)","c9ec6607":"model=DecisionTreeClassifier(max_depth=5)\nmodel=model.fit(X_train,Y_train)\nmodel.score(X_train,Y_train)","bcbe143c":"model.score(X_validation,Y_validation)","0eeed001":"from IPython.display import Image  \nfrom sklearn import tree\nfrom os import system\n\ntrain_char_label = ['yes', 'no']\nLoan_campaign_File = open('Loan_campaign_tree.dot','w')\ndot_data = tree.export_graphviz(model, out_file=Loan_campaign_File, feature_names = list(X), class_names = list(train_char_label))\n\nLoan_campaign_File.close()\n\n\n# importance of features in the tree building ( The importance of a feature is computed as the \n#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint (pd.DataFrame(model.feature_importances_, columns = [\"Imp\"], index = X.columns).sort_values(by='Imp',ascending=False))","c69ad8e2":"- the model is underfit,so regularization is done to become best model","61809af3":"- Because I have given max_depth=5 only five levels are there(exclude the root)\n- Feature selection used for Decision Tree is based on the model importance variables","02430613":"- Above chart shows male account holders are high when compared to others","b0865357":"- Many outliers are there except some column","dfdf5ffc":"- To Create tree diagram,use this link https:\/\/dreampuf.github.io\/GraphvizOnline\/\n- Open .dot file generated and copy it in this url, you will get the tree diagram","a0594dd0":"- since customer id and random number has no meaning we can drop those column","76e45643":"# EDA","c8c5542b":"- Feature Selection based on VIF","33e2020f":"- From the above you can understand there is very small amount of correlation for target coumn with independent variable.\n- Multi-collinearity exists ","3035677d":"- age and age_bucket express same data","2522a122":"- To view the tree diagram use the below link\n- https:\/\/www.kaggle.com\/dineshmk594\/loan-campaign#download.png","21d8bc38":"# BUSINESS PROBLEM STATEMENT\n- Assume you are working with MyBank. The Bank executed a campaign to cross-sell Personal Loans. As part of their Pilot Campaign, 20000 customers were sent campaigns through email, sms, and direct mail.\n- They were given an offer of Personal Loan at an attractive interest rate of 12% and processing fee waived off if they respond within 1 Month. \n- 2512 customer expressed their interest and are marked as Target = 1\n- Many Demographics and Behavioural variables provided. \n- You have to build a Model using Supervised Learning Technique to finds profitable segments to target for cross-selling personal loans. Make necessary assumptions where required.\n"}}