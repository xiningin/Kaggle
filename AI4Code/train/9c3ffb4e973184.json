{"cell_type":{"574d5ad1":"code","91f27703":"code","e8a4146a":"code","b47bae19":"code","c20d2738":"code","acf2d7ea":"code","831288e8":"code","9c51331a":"code","f294e306":"code","36eddffe":"code","5f45a51a":"markdown","b3a5a423":"markdown","a769c7e4":"markdown","c5c1dd7b":"markdown","ac87568e":"markdown","ad59e187":"markdown","f21b3bf2":"markdown"},"source":{"574d5ad1":"import os\nimport random\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torch\n\n# train \/ eval on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","91f27703":"class FoodDataset(Dataset):\n    \n    def __init__(self, path, is_test=False):\n        \"\"\"\n        Initialize the food dataset\n        \n        path - the path to the root directory of the dataset with the subfolders train and test\n       \n        Returns a tuple (img, target)\n        \"\"\"\n        self.path = path\n        self.is_test = is_test\n        \n        self.imgs = list(sorted(os.listdir(os.path.join(path, \"images\"))))\n        self.labels = list(sorted(os.listdir(os.path.join(path, \"labels\"))))\n        \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, idx):      \n        \"\"\"Get an item from the dataset\"\"\"\n        \n        # prepare image and label\/box paths\n        img_path = os.path.join(self.path, \"images\", self.imgs[idx])\n        label_path = os.path.join(self.path, \"labels\", self.labels[idx])\n        \n        # load image as PIL image\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        # load corresponding bbox\n        boxes = []\n        with open(label_path, \"r\") as f:\n            boxes = [list(map(float, line.split())) for line in f]\n            \n        # convert everything into a torch.Tensor\n        labels = torch.ones((len(boxes),), dtype=torch.int64) # class is always 1 -> food\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n\n        # image ID is simply the current index\n        image_id = torch.tensor([idx])\n        \n        # calculate area of box\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        # construct the target dict\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n\n        img = transforms.ToTensor()(img)\n\n        return (img, target)\n    \n","e8a4146a":"dataset = FoodDataset(\"\/kaggle\/input\/few-shot-object-detection\/train\")\nprint(len(dataset))\n\n# 70 \/ 30 percent train \/ test split\nlengths = round(len(dataset) * 0.7), round(len(dataset) * 0.3)\ndataset_train, dataset_val = torch.utils.data.random_split(dataset, lengths, generator=torch.Generator().manual_seed(42))\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset_train, batch_size=2, shuffle=True, num_workers=4, collate_fn=lambda batch: tuple(zip(*batch))\n)\ndata_loader_validation = torch.utils.data.DataLoader(\n    dataset_val, batch_size=2, shuffle=True, num_workers=4, collate_fn=lambda batch: tuple(zip(*batch))\n)\n\ndataset[0]","b47bae19":"\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 2  # 1 class (food) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)","c20d2738":"def train_one_epoch(model, optimizer, data_loader, device, epoch):\n    model.train()\n    print(f\"Epoch: {epoch}\")\n    \n    itr = 1 # iteration counter\n\n    for images, targets in data_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        \n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n\n        itr += 1","acf2d7ea":"\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\n\n# let's train it for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch)\n    # update the learning rate\n    lr_scheduler.step()\n\nprint(\"That's it!\")\n\n# Save the model\ntorch.save(model, \"\/kaggle\/working\/model-checkpoint.pt\")","831288e8":"df = pd.DataFrame(columns=[\"Id\", \"Expected\"])\n\n#model = torch.load(\"..\/input\/j-rg-s-food-detector-playground\/model-checkpoint.pt\")\nmodel.eval()\n\nfor img_path in os.listdir(\"\/kaggle\/input\/few-shot-object-detection\/test\/images\"):\n    img = Image.open(os.path.join(\"\/kaggle\/input\/few-shot-object-detection\/test\/images\", img_path)).convert(\"RGB\")\n    img = transforms.ToTensor()(img)\n    img = img.to(device)\n\n    predictions = model([img])\n\n    list_item = []\n    for p in predictions:\n        zipped = list(zip(p[\"boxes\"], p[\"scores\"]))\n        for box, score in zipped:\n            if score.item() > 0.5:\n                list_item.append([*list(box.cpu().detach().numpy().astype(int)), round(score.item() * 100.) \/ 100.])\n\n    \n    df.loc[len(df)] = [img_path, sorted(list_item)]\n\ndf.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)\ndf","9c51331a":"import matplotlib.pyplot as plt\nimport cv2\n\nimgs = []\nsample = df.sample(16)\n\nfor row in sample.itertuples():\n    img = cv2.imread(os.path.join(\"\/kaggle\/input\/few-shot-object-detection\/test\/images\", row.Id))\n    for pred in row.Expected:\n        x1, y1, x2, y2, score = pred\n        if score > 0.5:\n            cv2.rectangle(img, (int(x1),int(y1)), (int(x2),int(y2)), (255,0,0), 4)  \n    img = img[...,::-1].copy()\n    imgs.append(img)\n    \n_, axs = plt.subplots(4, 4, figsize=(12,8))\naxs = axs.flatten()\nfor img, ax in zip(imgs, axs):\n    ax.imshow(img)\n    ","f294e306":"# Source of the metrics snippet: https:\/\/www.kaggle.com\/chenyc15\/mean-average-precision-metric\n\n# helper function to calculate IoU\ndef iou(box1, box2):\n    x11, y11, x12, y12 = box1\n    x21, y21, x22, y22 = box2\n    w1 = abs(x12-x11)\n    h1 = abs(y12-y11)\n    w2 = abs(x22-x21)\n    h2 = abs(y22-y21)\n\n    area1, area2 = w1 * h1, w2 * h2\n    xi1, yi1, xi2, yi2 = max([x11, x21]), max([y11, y21]), min([x12, x22]), min([y12, y22])\n    \n    if xi2 <= xi1 or yi2 <= yi1:\n        return 0\n    else:\n        intersect = (xi2-xi1) * (yi2-yi1)\n        union = area1 + area2 - intersect\n        return intersect \/ union\n    \ndef map_iou(boxes_true, boxes_pred, scores, thresholds = np.arange(0.5, 1, 0.05)):\n    \"\"\"\n    Mean average precision at differnet intersection over union (IoU) threshold\n    \n    input:\n        boxes_true: Mx4 numpy array of ground true bounding boxes of one image. \n                    bbox format: (x1, y1, w, h)\n        boxes_pred: Nx4 numpy array of predicted bounding boxes of one image. \n                    bbox format: (x1, y1, w, h)\n        scores:     length N numpy array of scores associated with predicted bboxes\n        thresholds: IoU shresholds to evaluate mean average precision on\n    output: \n        map: mean average precision of the image\n    \"\"\"\n    \n    # According to the introduction, images with no ground truth bboxes will not be \n    # included in the map score unless there is a false positive detection (?)\n        \n    # return None if both are empty, don't count the image in final evaluation (?)\n    if len(boxes_true) == 0 and len(boxes_pred) == 0:\n        return None\n    \n    assert boxes_true.shape[1] == 4 or boxes_pred.shape[1] == 4, \"boxes should be 2D arrays with shape[1]=4\"\n    if len(boxes_pred):\n        assert len(scores) == len(boxes_pred), \"boxes_pred and scores should be same length\"\n        # sort boxes_pred by scores in decreasing order\n        boxes_pred = boxes_pred[np.argsort(scores)[::-1], :]\n    \n    map_total = 0\n    \n    # loop over thresholds\n    for t in thresholds:\n        matched_bt = set()\n        tp, fn = 0, 0\n        for i, bt in enumerate(boxes_true):\n            matched = False\n            for j, bp in enumerate(boxes_pred):\n                miou = iou(bt, bp)\n                if miou >= t and not matched and j not in matched_bt:\n                    matched = True\n                    tp += 1 # bt is matched for the first time, count as TP\n                    matched_bt.add(j)\n            if not matched:\n                fn += 1 # bt has no match, count as FN\n                \n        fp = len(boxes_pred) - len(matched_bt) # FP is the bp that not matched to any bt\n        m = tp \/ (tp + fn + fp)\n        map_total += m\n    \n    return map_total \/ len(thresholds)\n","36eddffe":"#model = torch.load(\"..\/input\/j-rg-s-food-detector-playground\/model-checkpoint.pt\")\nmodel.eval()\n\nfrom sklearn.metrics import jaccard_score\n\nm_aps = []\n\nfor (images, targets) in data_loader_validation:\n\n    images = [image.to(device) for image in images]\n        \n    predictions = model(images)\n    \n    for i in range(len(predictions)):\n        pred = predictions[i][\"boxes\"].detach().cpu().numpy().astype(int)\n        target = targets[i][\"boxes\"].detach().cpu().numpy().astype(int)\n        scores = predictions[i][\"scores\"].detach().cpu().numpy()\n        \n        m_ap = map_iou(target, pred, scores)\n        m_aps.append(m_ap)\n        \nprint(sum(m_aps) \/ len(m_aps))\n","5f45a51a":"### Test our dataset class\n\nTo make sure that our _FoodDataset_ works as intended, we instantiate it with the root path of our train data and print the first item:","b3a5a423":"## Loading and structuring our data\n\nRight now, our training and test data is floating around in some JPG and TXT files on our filesystem.\nIn order to get started to train a model, we need to bring that data into a structured form that can be fed to PyTorch.\n\nThe following _FoodDataset_ class helps us to ...\n\n* ... read the **images** into PIL objects\n* ... read the **bounding boxes** from the corresponding files in the _labels_ folder\n* ... set the corresponding class **labels** (in our case always 1 for _food_)\n* ... convert the input data into torch **tensors**\n* ... prepare the expected output (the **target** dictionary) for training","a769c7e4":"### Show some results\n\nThe next code block randomly picks 4 images from the test set and displays the predicted bounding boxes if confidence > 0.5.","c5c1dd7b":"## Evaluation\nFinally, get our predictions on the test data and add the results to a Pandas _DataFrame_. From there we can later export the result to a CSV file for submission to the competition.","ac87568e":"# J\u00f6rg's Food Detector Playground\n","ad59e187":"## Our model\nNow that we are able to read in our data, it is time to build a model that we can train.\n\nWe will use a ResNet model pre-trained on the COCO dataset.\n\n**ResNet-50** (Residual Neural Network with 50 layers) is a convolutional neural network architecture proposed in 2015 by researchers at Microsoft (original code and paper at https:\/\/github.com\/KaimingHe\/deep-residual-networks)\n\n**COCO** is a large image dataset that can be used to train models for many computer vision tasks, such as **object detection**, classification, and segementation. More info on COCO at https:\/\/cocodataset.org\/#home \n","f21b3bf2":"### Evaluate the mAP on the test set"}}