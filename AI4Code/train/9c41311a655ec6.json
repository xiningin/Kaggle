{"cell_type":{"1625a475":"code","55cc24ff":"code","ca296433":"code","677c280a":"code","f4af25de":"code","96e19227":"code","de8c948d":"code","73efb5f9":"code","d6c6445d":"code","358ebb1c":"code","a0337afa":"code","d9c70ace":"code","2f45209d":"code","121f8207":"code","ea9f2bee":"code","6d619a1c":"code","4f8982ea":"code","47a2a52b":"code","8c6c6d8f":"code","dd450771":"code","3b3fa127":"code","c178cc0e":"code","86aafb22":"code","55e272f1":"code","90a41e0b":"code","7bba9403":"code","b4351974":"code","902764b2":"code","2ba432fc":"code","f08403d6":"code","3b9b0644":"code","00ded701":"code","7477796e":"code","79d1e603":"code","80a72c77":"code","5f233bc3":"code","a5275c5b":"code","5fbd14e1":"code","c022d47e":"code","d70d5863":"code","dd361624":"code","de18a2e7":"code","6cbd3117":"code","839e6f6f":"code","f5fc8e74":"code","76259d14":"code","7e1de601":"code","8c57d495":"code","ccebed05":"code","6cf6679d":"code","1e8bbc83":"code","863fc6d7":"code","e12ff0e5":"code","e203c8e6":"code","72ad3c96":"code","eb9f88ff":"code","95663183":"code","26302995":"code","c406a9f6":"code","078e4a3a":"code","c4ee05eb":"code","f861b759":"code","3d526022":"code","64363bb7":"code","fce14a7f":"code","0f149582":"code","2cf47623":"code","ffb120fa":"code","9c71d401":"code","e6a550df":"code","00f1ac30":"code","3813543a":"code","934b4cfb":"code","44eef6fd":"code","fc9f4a7c":"code","354611be":"code","d3f87538":"code","735e531a":"code","27f0e436":"code","3fee9d7a":"markdown","be0d8e20":"markdown","917a77a8":"markdown","48218f2c":"markdown","2ce4c98b":"markdown","659326c9":"markdown","5767788a":"markdown","fbe8eb4c":"markdown","db8a0f0a":"markdown","33dd721b":"markdown","f77064f6":"markdown","b0aa2523":"markdown","039642e8":"markdown","65488f89":"markdown","58bc48fa":"markdown","f687d12c":"markdown","7a8618a7":"markdown","b89b066e":"markdown","9332c508":"markdown","4a14b30a":"markdown","86157ac6":"markdown","1f95549f":"markdown","1de6181a":"markdown","11b80152":"markdown","f721fb2c":"markdown","db1c48a3":"markdown","59ba9764":"markdown","a2215b5e":"markdown","88a109f7":"markdown","36a4e197":"markdown","7291186f":"markdown","b8f8836e":"markdown","082e7371":"markdown","098b1302":"markdown","30c6aa98":"markdown","b01c4265":"markdown","34328b9b":"markdown","c0b8809b":"markdown","20d6520d":"markdown","60e9c3f9":"markdown","06c89cec":"markdown","31d5adfb":"markdown","3fcd10e0":"markdown","1e667a81":"markdown","1bc0c40d":"markdown","e341936e":"markdown"},"source":{"1625a475":"import time\nfrom datetime import datetime\n\n#measure notebook running time\nstart_time = time.time()\n\n%matplotlib inline\n\n# backbone\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# DNN\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping\n\n# SVC\nfrom sklearn.svm import SVC\n\n# xgboost\nimport xgboost as xgb\n\n# ensemble\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n\n# logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\n#linear\nfrom sklearn.linear_model import SGDClassifier \n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# model selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n# preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\n# voting\nfrom sklearn.ensemble import VotingClassifier\n\nsns.set(style='white', context='notebook', palette='deep')\nprint(\"loaded ...\")","55cc24ff":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# ticket left out for the moment\ntrain_data.drop(['Ticket'], inplace = True, axis = 1)\ntest_data.drop(['Ticket'], inplace = True, axis = 1)\n\n# check status\nprint(\"Missing values in train:\\n\",train_data.isna().sum() \/ len(train_data))\nprint(\"\\nMissing values in test:\\n\",test_data.isna().sum() \/ len(test_data))\ntrain_data.head(10)","ca296433":"# this have actually decreased the score, initially\n\ndef name_to_title(name):\n    temp1 = name.split(\",\")\n    temp2 = temp1[1].split('.')\n    return temp2[0].strip(' ')\n\ndef rare_title(title):\n    common_titles = ['Mr', 'Miss', 'Mrs', 'Master']\n    if title in common_titles:\n        return title\n    else:\n        return 'Rare'\n\ntrain_data['Title'] = train_data.Name.apply(name_to_title)\ntest_data['Title'] = test_data.Name.apply(name_to_title)\n\n# not all in test, keep common, rest to rare\ntrain_data['Title'] = train_data.Title.apply(rare_title)\ntest_data['Title'] = test_data.Title.apply(rare_title)\n\n\nprint(train_data.Title.value_counts(), len(train_data.Title.value_counts()))\nprint(test_data.Title.value_counts(), len(test_data.Title.value_counts()))\n\ntrain_data.head()","677c280a":"g = sns.catplot(x=\"Title\",y=\"Survived\",data=train_data,kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability - Title\")","f4af25de":"# sum of parents and children, siblings, +1 person who is observed\ntrain_data['FamilySize'] = train_data.SibSp + train_data.Parch + 1\ntest_data['FamilySize'] = test_data.SibSp + test_data.Parch + 1\n_FamilySize = train_data['FamilySize']","96e19227":"g = sns.catplot(x=\"FamilySize\",y=\"Survived\",data=train_data,kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability - FamilySize\")","de8c948d":"def familySize_to_cat(size):\n    if size == 1: return \"alone\"\n    if size >= 2 and size <= 4: return 'small'\n    if size >= 5 and size <= 7: return 'medium'\n    if size > 7 : return \"large\"\n\ntrain_data['Fsize_cat'] = train_data.FamilySize.apply(familySize_to_cat)\ntest_data['Fsize_cat'] = test_data.FamilySize.apply(familySize_to_cat)","73efb5f9":"g = sns.catplot(x=\"Fsize_cat\",y=\"Survived\",data=train_data,kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability - Fsize_cat\")","d6c6445d":"g = sns.FacetGrid(train_data, col=\"Pclass\")\ng = g.map(sns.histplot, \"Age\", kde=True)\ng.fig.subplots_adjust(top=0.8)\ng.fig.suptitle(\"Age distribution over PClass\");","358ebb1c":"g = sns.FacetGrid(train_data,col=\"Survived\")\ng = g.map(sns.histplot, \"Age\", kde=True)\ng.fig.subplots_adjust(top=0.8)\ng.fig.suptitle(\"Age distribution over Survived\");","a0337afa":"g = sns.catplot(y=\"Age\",x=\"Parch\", data=train_data,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"SibSp\", data=train_data,kind=\"box\")","d9c70ace":"#filling with median, too simplistic\n#train_data.Age.fillna(train_data.Age.median(), inplace=True)\n#test_data.Age.fillna(test_data.Age.median(), inplace=True)","2f45209d":"#train\nindex_NaN_age = list(train_data[\"Age\"][train_data[\"Age\"].isnull()].index)\nmed_age = train_data.Age.median()\nfor i in index_NaN_age:\n    pred_age = train_data[\"Age\"][((train_data.SibSp == train_data.iloc[i]['SibSp']) & (train_data.Parch == train_data.iloc[i]['Parch']))].median()\n    if np.isnan(pred_age):\n        train_data.loc[train_data.index[i],'Age'] = med_age\n    else:\n        train_data.loc[train_data.index[i],'Age'] = pred_age","121f8207":"#test\nindex_NaN_age_test = list(test_data[\"Age\"][test_data[\"Age\"].isnull()].index)\nmed_age = test_data.Age.median()\nfor i in index_NaN_age_test:\n    pred_age = test_data[\"Age\"][((test_data.SibSp == test_data.iloc[i]['SibSp']) & (test_data.Parch == test_data.iloc[i]['Parch']))].median()\n    if np.isnan(pred_age):\n        test_data.loc[test_data.index[i],'Age'] = med_age\n    else:\n        test_data.loc[test_data.index[i],'Age'] = pred_age","ea9f2bee":"def cut_age(age):\n    if age <= 15:\n        return 'child'\n    if age >= 60:\n        return 'senior'\n    return 'adult'\n\ntrain_data['Age_Cat'] = train_data.Age.apply(cut_age) \ntest_data['Age_Cat'] = test_data.Age.apply(cut_age) ","6d619a1c":"g = sns.catplot(x=\"Age_Cat\",y=\"Survived\",data=train_data,kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability - Age_Cat\")","4f8982ea":"#train\ntrain_data['RealFare'] = train_data.Fare \/ train_data.FamilySize\n\nindex_zero_train = list(train_data['RealFare'][train_data['RealFare'] == 0].index)\nfor i in index_zero_train:\n    med_fare = train_data['RealFare'][(train_data.Pclass == train_data.iloc[i]['Pclass'])].median()\n    train_data.loc[train_data.index[i],'RealFare'] = med_fare","47a2a52b":"#test\ntest_data['RealFare'] = test_data.Fare \/ test_data.FamilySize\n\nindex_zero_train = list(test_data['RealFare'][test_data['RealFare'] == 0].index)\nfor i in index_zero_train:\n    med_fare = test_data['RealFare'][(test_data.Pclass == test_data.iloc[i]['Pclass'])].median()\n    test_data.loc[test_data.index[i],'RealFare'] = med_fare","8c6c6d8f":"sns.histplot(data = train_data, x='RealFare', stat='percent', hue='Pclass', kde=True,log_scale=True);","dd450771":"sns.histplot(data = train_data, x='RealFare', stat='percent', hue='Survived', kde=True,log_scale=True);","3b3fa127":"# skewed, I will apply log\ntrain_data['RealFare'] = train_data['RealFare'].apply(lambda row: np.log(row) if row > 0 else 0)\ntest_data['RealFare'] = test_data['RealFare'].apply(lambda row: np.log(row) if row > 0 else 0)","c178cc0e":"#numeric_features = ['RealFare', 'FamilySize']\nnumeric_features = ['RealFare']\nnum_to_check = [*numeric_features,'Survived']\ng = sns.heatmap(train_data[[*numeric_features,'Survived']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","86aafb22":"# MM works better in this case as StandardScaler\nmm = MinMaxScaler()\n#mm = StandardScaler()\n\n#train\nnumeric_part_train = train_data[numeric_features]\ntransformed = mm.fit_transform(numeric_part_train)\n\nfor i,F in enumerate(numeric_features):\n    train_data[F + '_scaled'] = transformed[:,i]\n\n#test\nnumeric_part_test = test_data[numeric_features]\ntransformed = mm.fit_transform(numeric_part_test)\n\nfor i,F in enumerate(numeric_features):\n    test_data[F + '_scaled'] = transformed[:,i]\n    \ntrain_data.head()","55e272f1":"# commented out\n# train_data.Cabin = train_data.Cabin.apply(lambda row: 0 if row is np.nan else 1)\n# test_data.Cabin = test_data.Cabin.apply(lambda row: 0 if row is np.nan else 1)","90a41e0b":"def get_deck(cabin):\n    if cabin is np.nan or cabin.startswith('T'):\n        return 'X'\n    else:\n        return cabin[0]\n    \ntrain_data.Cabin = train_data.Cabin.apply(get_deck)\ntest_data.Cabin = test_data.Cabin.apply(get_deck)\nprint(train_data.Cabin.value_counts(), len(train_data.Cabin.value_counts()))\nprint(test_data.Cabin.value_counts(), len(test_data.Cabin.value_counts()))","7bba9403":"g = sns.catplot(x=\"Cabin\",y=\"Survived\",data=train_data,kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability - Cabin\")","b4351974":"# store untransformed data\n_Sex = train_data.Sex","902764b2":"ax = sns.countplot(data = train_data, x = 'Sex', hue = \"Survived\");\nax.set_title(\"Sex\");\ntrain_data.Sex = train_data.Sex.apply(lambda row: 0 if row == \"male\" else 1)\ntest_data.Sex = test_data.Sex.apply(lambda row: 0 if row == \"male\" else 1)","2ba432fc":"g = sns.catplot(x=\"Pclass\",y=\"Survived\",data=train_data,kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","f08403d6":"ax = sns.countplot(data = train_data, x = 'Embarked', hue = \"Survived\");\nax.set_title(\"Embarked\");","3b9b0644":"cat_features = ['Sex', 'Pclass']\nnum_to_check = [*cat_features,'Survived']\ng = sns.heatmap(train_data[[*cat_features,'Survived']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","00ded701":"_Embarked = train_data.Embarked\n_PClass = train_data.Pclass\n_Cabin = train_data.Cabin\n_Fare = train_data.RealFare\n_Title = train_data.Title\n_Age = train_data.Age_Cat\n_Family = train_data.Fsize_cat","7477796e":"dummy_cols = [\"Pclass\", \"Embarked\", \"Cabin\", \"Title\", \"Age_Cat\",'Fsize_cat']\nprefixes = [\"PC\", \"EM\", \"CAB\", \"TT\", \"AG\", \"FAM\"]\n\ntrain_data = pd.get_dummies(train_data, columns = dummy_cols, prefix=prefixes)\ntest_data = pd.get_dummies(test_data, columns = dummy_cols, prefix=prefixes)","79d1e603":"redundant_features = ['SibSp','Parch', \"Name\", \"Fare\", \"Age\", \"FamilySize\"]\ntrain_data.drop(redundant_features, inplace = True, axis = 1)\ntest_data.drop(redundant_features, inplace = True, axis = 1)\n#drop unscaled\ntrain_data.drop(numeric_features, inplace = True, axis=1)\ntest_data.drop(numeric_features, inplace = True, axis=1)","80a72c77":"train_data.head()","5f233bc3":"test_data.head()","a5275c5b":"fig, ax = plt.subplots(figsize=(10,10))     \ng = sns.heatmap(train_data.corr(),annot=False, cmap = \"coolwarm\")","5fbd14e1":"y = train_data.Survived\nX = train_data.drop(['Survived','PassengerId'], axis = 1)\nTEST = test_data.drop('PassengerId', axis=1)\n\n# reduced features we worse ...\n#filter main features test\n# main_features = ['TT_Mr', 'Sex', 'RealFare_scaled', 'PC_3', 'TT_Miss', 'FamilySize_scaled', 'TT_Mrs', 'CAB_X']\n# X = X[main_features]\n# TEST = TEST[main_features]\n#end filter\n\nprint(\"X.shape\",X.shape)\nprint(\"y.shape\",y.shape)\nprint(\"TEST.shape\", TEST.shape)","c022d47e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 13, stratify=y)","d70d5863":"dnn_model = Sequential()\nn_cols = X.shape[1]\ndnn_model.add(Dense(128, activation=\"relu\", input_shape = (n_cols,))) \ndnn_model.add(Dense(64, activation=\"relu\")) \ndnn_model.add(Dense(32, activation=\"relu\")) \ndnn_model.add(Dense(8, activation=\"relu\")) \ndnn_model.add(Dense(1, activation=\"sigmoid\"))              \ndnn_model.summary()","dd361624":"def plot_loss(loss,val_loss):\n    plt.figure()\n    plt.plot(loss)\n    plt.plot(val_loss)\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper right')\n    plt.show()\n\ndef plot_accuracy(acc,val_acc):\n    plt.figure()\n    plt.plot(acc)\n    plt.plot(val_acc)\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()  ","de18a2e7":"dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nearly_stopping_monitor = EarlyStopping(patience=4, monitor='val_accuracy')\n#dnn_model.fit(X_train,y_train, validation_split = 0.3, callbacks=[early_stopping_monitor], epochs=200,batch_size=20, verbose=0)\ndnn_model.fit(X_train,y_train, validation_data=(X_test,y_test), callbacks=[early_stopping_monitor], epochs=200,batch_size=50, verbose=0)\n\nplot_loss(dnn_model.history.history['loss'], dnn_model.history.history['val_loss'])\nplot_accuracy(dnn_model.history.history['accuracy'], dnn_model.history.history['val_accuracy'])\n\n_, train_dnn_accuracy = dnn_model.evaluate(X_train, y_train)\n_, dnn_accuracy = dnn_model.evaluate(X_test, y_test)\nprint('Train accuracy: {:.2f} %'.format(train_dnn_accuracy*100))\nprint('Accuracy: {:.2f} %'.format(dnn_accuracy*100))\nprint('Overfit: {:.2f} % '.format((train_dnn_accuracy - dnn_accuracy)*100))","6cbd3117":"# clf = RandomForestClassifier()\n# param_grid = {'n_estimators': [50,75,100,200,300],'max_depth': [5,6,7,8,10], 'max_features':['auto','log2']}\n# rf_grid_clf = GridSearchCV(clf, param_grid, cv=4, scoring= \"accuracy\")\n# rf_grid_clf.fit(X, y)\n# print(rf_grid_clf.best_estimator_)\n# print(rf_grid_clf.best_params_)\n\n# rf_accuracy = rf_grid_clf.best_score_\n# print(rf_accuracy)","839e6f6f":"#rf_model = RandomForestClassifier(max_depth=5, n_estimators= 75)\n#rf_model = RandomForestClassifier(max_depth=6, max_features='log2', n_estimators=300)\nrf_model = RandomForestClassifier(max_depth=4, n_estimators= 75, max_features='auto')\nrf_model.fit(X_train, y_train)\nrf_train_score = rf_model.score(X_train, y_train)\nrf_accuracy = rf_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(rf_train_score * 100))\nprint(\"Test: {:.2f} %\".format(rf_accuracy*100))\nprint('Overfit: {:.2f} %'.format((rf_train_score-rf_accuracy)*100))","f5fc8e74":"features = {}\nfor feature, importance in zip(X_train.columns, rf_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"RF\":features})\nimportances.sort_values(\"RF\", ascending = False, inplace=True)\nRF_best_features = list(importances[importances.RF > 0.03].index)\n\nimportances.plot.bar()\n\nprint(\"RF_best_features:\",RF_best_features, len(RF_best_features))","76259d14":"# xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n# gbm_param_grid = {'learning_rate': [0.05, 0.1],'n_estimators': [200, 500],'subsample': [0.5, 0.75, 0.9], \"max_depth\": [4, 6], 'reg_lambda':[0.1, 1, 10]}\n# xgb_grid = GridSearchCV(estimator=xgb_clf, param_grid = gbm_param_grid, cv=4, scoring= \"accuracy\")\n# xgb_grid.fit(X,y)\n\n# print(xgb_grid.best_params_)\n# xgb_accuracy = xgb_grid.best_score_\n# print(xgb_accuracy)","7e1de601":"#xgb_model = xgb.XGBClassifier(learning_rate= 0.2, max_depth=4, n_estimators=75, reg_lambda=0.5, subsample= 0.5, use_label_encoder=False, eval_metric='logloss') #0,843\nxgb_model = xgb.XGBClassifier(learning_rate= 0.01, max_depth= 4, n_estimators= 400, reg_lambda= 0.1, subsample= 0.8, use_label_encoder=False, eval_metric='logloss')\nxgb_model.fit(X_train, y_train)\nxgb_train_score = xgb_model.score(X_train, y_train)\nxgb_accuracy = xgb_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(xgb_train_score*100))\nprint(\"Test: {:.2f} %\".format(xgb_accuracy*100))\nprint('Overfit: {:.2f} %'.format((xgb_train_score-xgb_accuracy)*100))","8c57d495":"weights = xgb_model.get_booster().get_score(importance_type=\"gain\")\nweights = [(weights[w],w) for w in sorted(weights, key=weights.get, reverse=True)]\nXGB_features = [w[1] for w in weights]\nprint(\"XGB best features\", XGB_features)","ccebed05":"# param_grid = {'C': [1, 100, 1000, 10000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001],'kernel': ['rbf']}\n# SVM_grid = GridSearchCV(estimator = SVC(),param_grid=param_grid, cv=5, scoring= \"accuracy\")\n# SVM_grid.fit(X,y)\n# print(SVM_grid.best_params_)\n# SVM_accuracy = SVM_grid.best_score_\n# print(SVM_accuracy)","6cf6679d":"#SVM_model = SVC(C = 1000, gamma= 0.010, kernel='rbf')\nSVM_model = SVC(C = 100, gamma= 0.01, kernel='rbf')\nSVM_model.fit(X_train, y_train)\nsvm_train_score = SVM_model.score(X_train, y_train)\nSVM_accuracy = SVM_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(svm_train_score*100))\nprint(\"Test: {:.2f} %\".format(SVM_accuracy*100))\nprint('Overfit: {:.2f} %'.format((svm_train_score-SVM_accuracy)*100))","1e8bbc83":"features = {}\nfor feature, importance in zip(X_train.columns, rf_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"SVM\":features})\nimportances.sort_values(\"SVM\", ascending = False, inplace=True)\nimportances\nSVM_best_features = list(importances[importances.SVM > 0.03].index)\n\nimportances.plot.bar()\n\nprint(\"SVM_best_features:\",SVM_best_features, len(SVM_best_features))","863fc6d7":"# param_grid = {'C': np.logspace(-4, 4, 10), 'penalty': ['l2','l1']}\n# LR_grid = GridSearchCV(estimator = LogisticRegression(solver='liblinear'), param_grid=param_grid, cv=5, scoring= \"accuracy\")\n# LR_grid.fit(X,y)\n# print(LR_grid.best_params_)\n# LR_accuracy = LR_grid.best_score_\n# print(LR_accuracy)","e12ff0e5":"#LR_model = LogisticRegression(solver='liblinear', C=2.78, penalty='l2')\nLR_model = LogisticRegression(solver='liblinear', C=2.8, penalty='l2')\nLR_model.fit(X_train, y_train)\nLR_train_score = LR_model.score(X_train, y_train)\nLR_accuracy = LR_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(LR_train_score*100))\nprint(\"Test: {:.2f} %\".format(LR_accuracy*100))\nprint('Overfit: {:.2f} %'.format((LR_train_score-LR_accuracy)*100))","e203c8e6":"features = {}\nfor feature, importance in zip(X_train.columns, rf_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"LR\":features})\nimportances.sort_values(\"LR\", ascending = False, inplace=True)\nimportances\nLR_best_features = list(importances[importances.LR > 0.03].index)\n\nimportances.plot.bar()\n\nprint(\"LR_best_features:\",LR_best_features, len(LR_best_features))","72ad3c96":"# grid_params = {'n_neighbors': [3,5,11,19], 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan', 'minkowski']}\n# KNN_grid = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = grid_params, cv=5, scoring= \"accuracy\")\n# KNN_grid.fit(X,y)\n# print(KNN_grid.best_params_)\n# KNN_accuracy = KNN_grid.best_score_\n# print(KNN_accuracy)","eb9f88ff":"KNN_model = KNeighborsClassifier(n_neighbors=10,metric='manhattan',weights='uniform')\n#KNN_model = KNeighborsClassifier(n_neighbors=6,metric='euclidean',weights='uniform')\nKNN_model.fit(X_train, y_train)\nKNN_train_score = KNN_model.score(X_train, y_train)\nKNN_accuracy = KNN_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(KNN_train_score*100))\nprint(\"Test: {:.2f} %\".format(KNN_accuracy*100))\nprint('Overfit: {:.2f} %'.format((KNN_train_score-KNN_accuracy)*100))","95663183":"features = {}\nfor feature, importance in zip(X_train.columns, rf_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"KNN\":features})\nimportances.sort_values(\"KNN\", ascending = False, inplace=True)\nimportances\nKNN_best_features = list(importances[importances.KNN > 0.03].index)\n\nimportances.plot.bar()\n\nprint(\"KNN_best_features:\",KNN_best_features, len(KNN_best_features))","26302995":"# abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n\n# parameters = {'base_estimator__max_depth':[2, 4, 6],\n#               'base_estimator__min_samples_leaf':[5,10],\n#               'n_estimators':[250,500],\n#               'learning_rate':[0.01,0.1]}\n# ADA_grid = GridSearchCV(estimator = abc, param_grid = parameters, cv=5, scoring= \"accuracy\")\n# ADA_grid.fit(X,y)\n# print(ADA_grid.best_params_)\n# ADA_accuracy = ADA_grid.best_score_\n# print(ADA_accuracy)","c406a9f6":"ADA_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3, min_samples_leaf=20), n_estimators=200, learning_rate = 0.01) #0,84\n#ADA_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_leaf=10), n_estimators=500, learning_rate = 0.1) #0,817\nADA_model.fit(X_train,y_train)\nADA_train_score = ADA_model.score(X_train, y_train)\nADA_accuracy = ADA_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(ADA_train_score*100))\nprint(\"Test: {:.2f} %\".format(ADA_accuracy*100))\nprint('Overfit: {:.2f} %'.format((ADA_train_score - ADA_accuracy)*100))","078e4a3a":"features = {}\nfor feature, importance in zip(X_train.columns, rf_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"ADA\":features})\nimportances.sort_values(\"ADA\", ascending = False, inplace=True)\nimportances\nADA_best_features = list(importances[importances.ADA > 0.03].index)\n\nimportances.plot.bar()\n\nprint(\"ADA_best_features:\",ADA_best_features, len(ADA_best_features))","c4ee05eb":"# ExtC = ExtraTreesClassifier()\n# ex_param_grid = {\"max_depth\": [None],\n#               \"max_features\": [1, 3, 10],\n#               \"min_samples_split\": [2, 3, 10],\n#               \"min_samples_leaf\": [1, 3, 10],\n#               \"bootstrap\": [False],\n#               \"n_estimators\" :[100,300],\n#               \"criterion\": [\"gini\"]}\n\n\n# gsExtC = GridSearchCV(estimator= ExtC,param_grid = ex_param_grid, cv=5, scoring=\"accuracy\")\n# gsExtC.fit(X,y)\n\n# print(gsExtC.best_estimator_)\n# ExtC_accuracy = gsExtC.best_score_\n# print(ExtC_accuracy)","f861b759":"#ETC_model = ExtraTreesClassifier(max_features=5, min_samples_leaf=3, n_estimators=300)\nETC_model = ExtraTreesClassifier(max_features=3, min_samples_leaf=3, n_estimators=300)\nETC_model.fit(X_train, y_train)\nETC_train_score = ETC_model.score(X_train, y_train)\nETC_accuracy = ETC_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(ETC_train_score*100))\nprint(\"Test: {:.2f} %\".format(ETC_accuracy*100))\nprint('Overfit: {:.2f} %'.format((ETC_train_score-ETC_accuracy)*100))","3d526022":"features = {}\nfor feature, importance in zip(X_train.columns, rf_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"ETC\":features})\nimportances.sort_values(\"ETC\", ascending = False, inplace=True)\nimportances\nETC_best_features = list(importances[importances.ETC > 0.03].index)\n\nimportances.plot.bar()\n\nprint(\"ETC_best_features:\",ETC_best_features, len(ETC_best_features))","64363bb7":"# GBC = GradientBoostingClassifier()\n# gb_param_grid = {'loss' : [\"deviance\"],\n#               'n_estimators' : [100,200,300],\n#               'learning_rate': [0.1, 0.05, 0.01],\n#               'max_depth': [4, 8],\n#               'min_samples_leaf': [100,150],\n#               'max_features': [0.3, 0.1] \n#               }\n\n# gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=5, scoring=\"accuracy\")\n# gsGBC.fit(X,y)\n\n# print(gsGBC.best_estimator_)\n# gbc_accuracy = gsGBC.best_score_\n# print(gbc_accuracy)","fce14a7f":"GBC_model = GradientBoostingClassifier(max_depth=4, max_features=0.3, min_samples_leaf=100,n_estimators=300)\nGBC_model.fit(X_train, y_train)\nGBC_train_score = GBC_model.score(X_train, y_train)\nGBC_accuracy = GBC_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(GBC_train_score*100))\nprint(\"Test: {:.2f} %\".format(GBC_accuracy*100))\nprint('Overfit: {:.2f} %'.format((GBC_train_score-GBC_accuracy)*100))","0f149582":"features = {}\nfor feature, importance in zip(X_train.columns, rf_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"GBC\":features})\nimportances.sort_values(\"GBC\", ascending = False, inplace=True)\nGBC_best_features = list(importances[importances.GBC > 0.03].index)\nimportances.plot.bar()\n\nprint(\"GBC_best_features:\",GBC_best_features, len(GBC_best_features))","2cf47623":"# params = {\n#     \"loss\" : [\"hinge\", \"modified_huber\"],\n#     \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n#     \"penalty\": ['l2', 'l1', 'elasticnet']\n# }\n\n# sgd_grid = GridSearchCV(estimator =  SGDClassifier(max_iter=1000), param_grid=params, cv=4, scoring=\"accuracy\")\n# sgd_grid.fit(X,y)\n\n# print(sgd_grid.best_estimator_)\n# sgd_accuracy = sgd_grid.best_score_\n# print(sgd_accuracy)","ffb120fa":"SGD_model = SGDClassifier(alpha=0.01, penalty='l1', loss='modified_huber')\nSGD_model.fit(X_train, y_train)\nSGD_train_score = SGD_model.score(X_train, y_train)\nSGD_accuracy = SGD_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(SGD_train_score*100))\nprint(\"Test: {:.2f} %\".format(SGD_accuracy*100))\nprint('Overfit: {:.2f} %'.format((SGD_train_score-SGD_accuracy)*100))","9c71d401":"features = {}\nfor feature, importance in zip(X_train.columns, rf_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"SGD\":features})\nimportances.sort_values(\"SGD\", ascending = False, inplace=True)\nSGD_best_features = list(importances[importances.SGD > 0.03].index)\nimportances.plot.bar()\n\nprint(\"SGD_best_features:\",SGD_best_features, len(SGD_best_features))","e6a550df":"L = min(len(RF_best_features), len(XGB_features), len(ADA_best_features), len(KNN_best_features), len(LR_best_features), len(SVM_best_features), \n        len(ETC_best_features), len(GBC_best_features), len(SGD_best_features))\n\nTF = pd.DataFrame({\"ADA\":ADA_best_features[:L], \"KNN\": KNN_best_features[:L], \"LR\": LR_best_features[:L],\n                  \"SVM\":SVM_best_features[:L], \"XGB\":XGB_features[:L], \"RF\":RF_best_features[:L],\n                  \"ETC\":ETC_best_features[:L], \"GBC\":GBC_best_features[:L], \"SGD\":SGD_best_features[:L]} )\nTF","00f1ac30":"print(\"Accuracy Scores:\")\nprint(\"==================\")\nprint(\"DNN: {:.3f}\".format(dnn_accuracy))\nprint(\"RandomForest: {:.3f}\".format(rf_accuracy))\nprint(\"XGBoost classifier: {:.3f}\".format(xgb_accuracy))\nprint(\"SVM classifier: {:.3f}\".format(SVM_accuracy))\nprint(\"LR classifier: {:.3f}\".format(LR_accuracy))\nprint(\"KNN classifier: {:.3f}\".format(KNN_accuracy))\nprint(\"ADA Boost classifier: {:.3f}\".format(ADA_accuracy))\nprint(\"Extra Tree classifier: {:.3f}\".format(ETC_accuracy))\nprint(\"Gradient Boosting classifier: {:.3f}\".format(GBC_accuracy))\nprint(\"Stochastic Gradient descent: {:.3f}\".format(SGD_accuracy))\nprint(\"==================\")","3813543a":"PassengerIds = test_data.PassengerId.to_list()\n\n# DNN\npredicted = dnn_model.predict(TEST)\ndnn_predictions = [int(np.rint(p)) for p in predicted]\n\n# the rest\nxgb_predictions = xgb_model.predict(TEST) \nrf_predictions = rf_model.predict(TEST) \nsvm_predictions = SVM_model.predict(TEST) \nlr_predictions = LR_model.predict(TEST) \nknn_predictions = KNN_model.predict(TEST)\nada_predictions = ADA_model.predict(TEST)\netc_predictions = ETC_model.predict(TEST)\ngbc_predictions = GBC_model.predict(TEST)\nsgd_predictions = SGD_model.predict(TEST)","934b4cfb":"def vote(votes):\n    weight_dict = {'DNN':1,'XGB':1,'RF':1,'LR':1,\"SVM\":1, \"KNN\":1, \"ADA\":1, \"ETC\":1, \"GBC\": 1, \"SGD\":1}\n    weights = np.array(list(weight_dict.values()))\n    sw = weights.sum()\n    v = [v * weights[i] for i,v in enumerate(votes)]\n    return sum(v)\/ sw\n\nALL_PREDICTIONS = pd.DataFrame({'PassengerId': PassengerIds,\n                                'DNN': dnn_predictions, 'XGB': xgb_predictions, 'RF':rf_predictions,'LR': lr_predictions, \n                                \"SVM\":svm_predictions, \"KNN\":knn_predictions, \"ADA\":ada_predictions, \"ETC\":etc_predictions, \"GBC\":gbc_predictions,\n                               \"SGD\":sgd_predictions})\nclfs = ['DNN','XGB','RF','LR',\"SVM\", \"KNN\", \"ADA\", \"ETC\", \"GBC\", \"SGD\"]\nALL_PREDICTIONS['Vote'] = ALL_PREDICTIONS[clfs].apply(lambda row: vote(row), axis = 1)\nALL_PREDICTIONS['Predict'] = ALL_PREDICTIONS.Vote.apply(lambda row: int(np.rint(row)))\nvc_predictions = ALL_PREDICTIONS.Predict\nALL_PREDICTIONS.head(10)\n","44eef6fd":"# predict train\n# DNN\npredicted_dnn = dnn_model.predict(X)\ndnn_train = [int(np.rint(p)) for p in predicted_dnn]\n#rest\nxgb_train = xgb_model.predict(X) \nrf_train = rf_model.predict(X) \nsvm_train = SVM_model.predict(X) \nlr_train = LR_model.predict(X) \nknn_train = KNN_model.predict(X) \nada_train = ADA_model.predict(X)\netc_train = ETC_model.predict(X)\ngbc_train = GBC_model.predict(X)\nsgd_train = SGD_model.predict(X)\n\n#check where I have failed\n\nTRAIN_PREDICTIONS = pd.DataFrame({'Survived':train_data.Survived,'Sex':_Sex,'Age': _Age,'Fare':_Fare,\n                                  \"FamilySize\":_Family, \"Cabin\":_Cabin,\"Title\": _Title,\n                                  \"PClass\": _PClass, \n                                  'DNN': dnn_train, 'XGB': xgb_train, 'RF':rf_train,'LR': lr_train, \n                                  \"SVM\":svm_train, \"KNN\":knn_train, \"ADA\":ada_train,\n                                 \"ETC\":etc_train, \"GBC\":gbc_train, \"SGD\":sgd_train})\nTRAIN_PREDICTIONS['Vote'] = TRAIN_PREDICTIONS[clfs].apply(lambda row: vote(row), axis = 1)\nTRAIN_PREDICTIONS['VC'] = TRAIN_PREDICTIONS.Vote.apply(lambda row: int(np.rint(row)))\n\nwrong = TRAIN_PREDICTIONS[TRAIN_PREDICTIONS.Survived != TRAIN_PREDICTIONS.VC]\nprint(len(wrong))\nwrong.head(20)","fc9f4a7c":"train_scores = dict()\nfor clf in [*clfs, 'VC']:\n    train_scores[clf] = [len(TRAIN_PREDICTIONS[TRAIN_PREDICTIONS.Survived == TRAIN_PREDICTIONS[clf]]) \/ TRAIN_PREDICTIONS.shape[0]]\n\nTRAIN_SCORES = pd.DataFrame(train_scores)\nTRAIN_SCORES","354611be":"TRAIN_SCORES.plot.bar();","d3f87538":"# don't forget to enter best predictions ...\n#output = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': xgb_predictions})\n#output = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': ada_predictions})\n# output = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': dnn_predictions})\noutput = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': vc_predictions})\n#output = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': rf_predictions})\n\noutput.head(20)","735e531a":"#output\noutput.to_csv('submission.csv', index=False)\nprint(\"Submission was successfully saved!\")","27f0e436":"end_time = time.time()\nprint(\"Notebook run time: {:.1f} seconds. Finished at {}\".format(end_time - start_time, datetime.now()) )","3fee9d7a":"# Titanic ...","be0d8e20":"## SVM","917a77a8":"## Numeric features","48218f2c":"## Gradient Boost (GBC)","2ce4c98b":"...<br>\n version 14: (84)<br>\n  &nbsp; &nbsp; EXT, GBC added, VC used -> submission score: 0.78708 (best)<br>\n  &nbsp; &nbsp; XGB used -> submission score: 0.76315 <br>\n  <br>\n  Familty to cat, VC -> 0.78468<br>\n version 15: (95)<br>\n &nbsp; &nbsp;some hyperparameter tuning, VC used -> 0.77272<br>\n\n\n","659326c9":"## Features so far","5767788a":"### Designate features and label","fbe8eb4c":"## DNN","db8a0f0a":"### Categorical, correlation","33dd721b":"# Predictions","f77064f6":"## ExtraTrees","b0aa2523":"## KNN","039642e8":"### Family size to categorical","65488f89":"# Imports & config","58bc48fa":"### To dummies","f687d12c":"## XGBoost","7a8618a7":"# Load and check data","b89b066e":"### Age to categorical","9332c508":"## SGDClassifier","4a14b30a":"### Fare\n","86157ac6":"## Checking Train Scores","1f95549f":"## ADA Boost","1de6181a":"#### second approach: get deck letter from Cabin name","11b80152":"## Categorical data","f721fb2c":"## Scale\/Normalize Numeric features","db1c48a3":"### New Feature: Family size from SibSp, Parch","59ba9764":"### PClass","a2215b5e":"#### Random Forest, best model","88a109f7":"## Train \/ Test split","36a4e197":"## Random Forest","7291186f":"### Fill with median of the group with same number of childred an siblings","b8f8836e":"### Cabin\nChanging to boolean, 0 if None else 1\nIt seems you had better chance with a cabin (simplistic first approach)","082e7371":"### Sex","098b1302":"### Get info from name -> Title","30c6aa98":"#### Drop features","b01c4265":"# Models:","34328b9b":"#### XGB, best model","c0b8809b":"### Age, check and fill missing values\nChecking age distribution","20d6520d":"## Scores","60e9c3f9":"### Numeric features, correlation","06c89cec":"### Check correlation of all","31d5adfb":"### Embarked","3fcd10e0":"### Top X features","1e667a81":"### My Own Voting Classifier, inclusion of DNN","1bc0c40d":"### Save for review","e341936e":"## Logistic Regression"}}