{"cell_type":{"fa2e4c24":"code","d118377d":"code","04c5b4b6":"code","43353c13":"code","d9b5d6e9":"code","57cb30c1":"code","3ebb6448":"code","f40b9763":"code","fc15c029":"code","e3d77826":"code","cab43e7a":"code","d5877f47":"code","d345187f":"code","6a4bd2da":"code","c57278d5":"code","bf1ae0ae":"code","90b22256":"code","2c0c9434":"code","2c20075b":"code","82ff44e1":"code","2bc06ee5":"code","f40ec05c":"code","903dfa96":"code","c67bdec4":"code","a94b761a":"code","7143391b":"code","0927ef4a":"code","b01e635b":"code","f2361177":"code","5aec0f7a":"code","e11c5509":"code","59fac074":"code","3b9a9c9b":"code","704a282f":"code","6208c8c4":"code","66cbfe11":"code","260b11ce":"code","6c8ba1a2":"code","4cc88e97":"markdown","3f340599":"markdown","d2fc6fb6":"markdown","672beeee":"markdown","84477567":"markdown","748e35b1":"markdown","01a53187":"markdown","af0e30b3":"markdown","c6621960":"markdown","e25d8a6c":"markdown","4d32a557":"markdown","964f1962":"markdown","3b395f55":"markdown","7bb58251":"markdown","efe50993":"markdown","ab9bca43":"markdown","81fd898a":"markdown","a8f5ff44":"markdown","8b87cc6d":"markdown","4f2a6184":"markdown","4f73088b":"markdown","4f24b8fc":"markdown","1af76d14":"markdown","26354b45":"markdown","110063e1":"markdown","01ccb2e7":"markdown","c0f2181f":"markdown","a815610f":"markdown","c9edf719":"markdown","2745d067":"markdown","f0bfa3fe":"markdown","7c1ee501":"markdown","4c0a7d38":"markdown","74733079":"markdown","d81b777a":"markdown","24edeee7":"markdown","45f7a3b6":"markdown","6d4e2d31":"markdown","63cabb74":"markdown","58994cd9":"markdown","35dc5b62":"markdown","3b5ffc52":"markdown","f4f01707":"markdown"},"source":{"fa2e4c24":"# to get no error executing this kernel, it is neccessary to update catboost to version 0.14.2 +\n!pip install catboost==0.14.2","d118377d":"import catboost\nprint(catboost.__version__)","04c5b4b6":"from catboost import CatBoostClassifier","43353c13":"from catboost import datasets\n\ntrain_df, test_df = datasets.amazon() # nice datasets with categorical features only :D\ntrain_df.shape, test_df.shape","d9b5d6e9":"train_df.head()","57cb30c1":"test_df.head()","3ebb6448":"y = train_df['ACTION']\nX = train_df.drop(columns='ACTION') # or X = train_df.drop('ACTION', axis=1)","f40b9763":"X_test = test_df.drop(columns='id')","fc15c029":"SEED = 1","e3d77826":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=SEED)","cab43e7a":"%%time\n\nparams = {'loss_function':'Logloss', # objective function\n          'eval_metric':'AUC', # metric\n          'verbose': 200, # output to stdout info about training process every 200 iterations\n          'random_seed': SEED\n         }\ncbc_1 = CatBoostClassifier(**params)\ncbc_1.fit(X_train, y_train, # data to train on (required parameters, unless we provide X as a pool object, will be shown below)\n          eval_set=(X_valid, y_valid), # data to validate on\n          use_best_model=True, # True if we don't want to save trees created after iteration with the best validation score\n          plot=True # True for visualization of the training process (it is not shown in a published kernel - try executing this code)\n         );","d5877f47":"cat_features = list(range(X.shape[1]))\nprint(cat_features)","d345187f":"cat_features_names = X.columns # here we specify names of categorical features\ncat_features = [X.columns.get_loc(col) for col in cat_features_names]\nprint(cat_features)","6a4bd2da":"condition = True # here we specify what condition should be satisfied only by the names of categorical features\ncat_features_names = [col for col in X.columns if condition]\ncat_features = [X.columns.get_loc(col) for col in cat_features_names]\nprint(cat_features)","c57278d5":"%%time\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'cat_features': cat_features,\n          'verbose': 200,\n          'random_seed': SEED\n         }\ncbc_2 = CatBoostClassifier(**params)\ncbc_2.fit(X_train, y_train,\n          eval_set=(X_valid, y_valid),\n          use_best_model=True,\n          plot=True\n         );","bf1ae0ae":"%%time\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'cat_features': cat_features,\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': SEED\n         }\ncbc_2 = CatBoostClassifier(**params)\ncbc_2.fit(X_train, y_train, \n          eval_set=(X_valid, y_valid), \n          use_best_model=True, \n          plot=True\n         );","90b22256":"%%time\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'cat_features': cat_features,\n          'task_type': 'GPU',\n          'verbose': 200,\n          'random_seed': SEED\n         }\ncbc_3 = CatBoostClassifier(**params)\ncbc_3.fit(X_train, y_train,\n          eval_set=(X_valid, y_valid), \n          use_best_model=True,\n          plot=True\n         );","2c0c9434":"%%time\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'cat_features': cat_features,\n          'task_type': 'GPU',\n          'border_count': 32,\n          'verbose': 200,\n          'random_seed': SEED\n         }\ncbc_4 = CatBoostClassifier(**params)\ncbc_4.fit(X_train, y_train, \n          eval_set=(X_valid, y_valid), \n          use_best_model=True, \n          plot=True\n         );","2c20075b":"import numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","82ff44e1":"np.random.seed(SEED)\nnoise_cols = [f'noise_{i}' for i in range(5)]\nfor col in noise_cols:\n    X_train[col] = y_train * np.random.rand(X_train.shape[0])\n    X_valid[col] = np.random.rand(X_valid.shape[0])","2bc06ee5":"X_train.head()","f40ec05c":"%%time\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'cat_features': cat_features,\n          'verbose': 200,\n          'random_seed': SEED\n         }\ncbc_5 = CatBoostClassifier(**params)\ncbc_5.fit(X_train, y_train, \n          eval_set=(X_valid, y_valid), \n          use_best_model=True, \n          plot=True\n         );","903dfa96":"ignored_features = list(range(X_train.shape[1] - 5, X_train.shape[1]))\nprint(ignored_features)","c67bdec4":"%%time\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'cat_features': cat_features,\n          'ignored_features': ignored_features,\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': SEED\n         }\ncbc_6 = CatBoostClassifier(**params)\ncbc_6.fit(X_train, y_train, \n          eval_set=(X_valid, y_valid), \n          use_best_model=True, \n          plot=True\n         );","a94b761a":"X_train = X_train.drop(columns=noise_cols)\nX_valid = X_valid.drop(columns=noise_cols)","7143391b":"X_train.head()","0927ef4a":"from catboost import Pool\n\ntrain_data = Pool(data=X_train,\n                  label=y_train,\n                  cat_features=cat_features\n                 )\n\nvalid_data = Pool(data=X_valid,\n                  label=y_valid,\n                  cat_features=cat_features\n                 )","b01e635b":"%%time\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n#           'cat_features': cat_features, # we don't need to specify this parameter as \n#                                           pool object contains info about categorical features\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': SEED\n         }\n\ncbc_7 = CatBoostClassifier(**params)\ncbc_7.fit(train_data, # instead of X_train, y_train\n          eval_set=valid_data, # instead of (X_valid, y_valid)\n          use_best_model=True, \n          plot=True\n         );","f2361177":"from catboost import cv","5aec0f7a":"%%time\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'verbose': 200,\n          'random_seed': SEED\n         }\n\nall_train_data = Pool(data=X,\n                      label=y,\n                      cat_features=cat_features\n                     )\n\nscores = cv(pool=all_train_data,\n            params=params, \n            fold_count=4,\n            seed=SEED, \n            shuffle=True,\n            stratified=True, # if True the folds are made by preserving the percentage of samples for each class\n            plot=True\n           )","e11c5509":"cbc_7.get_feature_importance(prettified=True)","59fac074":"import pandas as pd\n\nfeature_importance_df = pd.DataFrame(cbc_7.get_feature_importance(prettified=True), columns=['feature', 'importance'])\nfeature_importance_df","3b9a9c9b":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12, 6));\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df);\nplt.title('CatBoost features importance:');","704a282f":"import shap\nexplainer = shap.TreeExplainer(cbc_7) # insert your model\nshap_values = explainer.shap_values(train_data) # insert your train Pool object\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[:100,:], X_train.iloc[:100,:])","6208c8c4":"shap.summary_plot(shap_values, X_train)","66cbfe11":"%%time\n\nfrom sklearn.model_selection import StratifiedKFold\n\nn_fold = 4 # amount of data folds\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=SEED)\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'verbose': 200,\n          'random_seed': SEED\n         }\n\ntest_data = Pool(data=X_test,\n                 cat_features=cat_features)\n\nscores = []\nprediction = np.zeros(X_test.shape[0])\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index] # train and validation data splits\n    y_train, y_valid = y[train_index], y[valid_index]\n    \n    train_data = Pool(data=X_train, \n                      label=y_train,\n                      cat_features=cat_features)\n    valid_data = Pool(data=X_valid, \n                      label=y_valid,\n                      cat_features=cat_features)\n    \n    model = CatBoostClassifier(**params)\n    model.fit(train_data,\n              eval_set=valid_data, \n              use_best_model=True\n             )\n    \n    score = model.get_best_score()['validation_0']['AUC']\n    scores.append(score)\n\n    y_pred = model.predict_proba(test_data)[:, 1]\n    prediction += y_pred\n\nprediction \/= n_fold\nprint('CV mean: {:.4f}, CV std: {:.4f}'.format(np.mean(scores), np.std(scores)))","260b11ce":"import pandas as pd\n\nsub = pd.read_csv('..\/input\/amazon-employee-access-challenge\/sampleSubmission.csv')\nsub['Action'] = prediction\nsub_name = 'catboost_submission.csv'\nsub.to_csv(sub_name, index=False)\n\nprint(f'Saving submission file as: {sub_name}')","6c8ba1a2":"from catboost import CatBoostRegressor","4cc88e97":"Problem fixed. Good. We obtained the same results as in cbc_2 model, where there were no misleading features. Now let's get rid of them:","3f340599":"Only 0.829? That's not even top 50%. Though to be honest, our model could show better results if we allowed it to train for more iterations (by default, it's 1000). Still, how else can we improve our results? First of all, we should finally specify, what features are categorical. In the above model CatBoost treated categorical features as numerical ones. Thus, the categories were ranked.","d2fc6fb6":"In some cases we may suspect that some features give us misleading information. To experiment with this idea we can either create numerous of slices of our data, or we can just specify in the model HP ***ignored_features=[i1, i2, ... , in]***, list of column numbers we want to ignore.\n\nFirst, let's create columns with data which will puzzle our model:","672beeee":"Catboost introduces the followign algorithmic advances:\n\n**1. Categorical features support:**\n\nFor data with categorical features the accuracy of CatBoost would be better compared to other algorithms. You do not need to preprocess categorical features (like one-hot encoding), just specify some hyperparameters (will be shown below, we will also use **HP** for hyperparameters).\n\n**2. Better overfitting handling:**\n\nCatBoost uses the implementation of ordered boosting, an alternative to the classic boosting algorithm.  \nFor example, the gradient boosting is quickly overfitted on small datasets. In Catboost there is a special modification for such cases, so on small datasets where other algorithms had a problem of overfitting you won\u2019t observe the same problem with Catboost.\n\n**3. Fast and easy-to-use GPU-training:**\n\nThe versions of CatBoost available from pip install (*pip install catboost*) and conda install (*conda install catboost*) have GPU support out-of-the-box. You just need to specify that you want to train your model on GPU in the corresponding HP (will be shown below).\n\n**4. Other useful features:**\n\nMissing value support (***nan_mode*** HP), great visualization.","84477567":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \n### <center> Author: Mikhail Tribunskiy, @MITribunskiy\n    \n## <center> Tutorial\n### <center> \"CatBoost overview\"","748e35b1":"Ok, let's get started:","01a53187":"Or if we know how to distinguish a categorical feature from the numerical one:","af0e30b3":"# Summary\n\nTo make a long story short, CatBoost provides useful tools for easy work with highly categorized data. It shows solid results training on unprocessed categorical features. Calculation on GPU is controlled by only 1 parameter. Moreover, CatBoost has a really comprehensive documentation.\n","c6621960":"Unfortunately, no. However, you can check your model predictions by submitting them [here](https:\/\/www.kaggle.com\/c\/amazon-employee-access-challenge\/overview). That's an old Kaggle competition but you can still make submissions to test yourself. We will also use the same metric to evaluate our model: [area under the ROC curve](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic).","e25d8a6c":"In some datasets training on GPU takes much less time. Training on GPU can be sped up further by specifying HP ***border_count=N***, where N defines the number of splits considered for each feature. CatBoost documentation suggests setting the value of this parameter to **32** if training is performed on GPU. In many cases, this does not affect the quality of the model but significantly speeds up the training. Let's check if that is also our case:","4d32a557":"If not all features are categorical but we have their names, we can specify ***cat_features*** like this:","964f1962":"By default CatBoost uses CPU to make calculations. What will change if we make it conduct the calculations on GPU? To do so, we need to specify HP ***task_type='GPU'***. Let's train our model on GPU (without overfitting HP ***early_stopping_rounds***):","3b395f55":"Now, what shall we do to to estimate feature importance?\n\nFirst of all, let's check result of **.get_feature_importance()** method of the fitted model:","7bb58251":"Let's fix that problem by specifying a HP ***cat_features=[i1, i2, ... , in]*** (list of integers):","efe50993":"Generally speaking, it differs from CatBoostClassifier only by the objective function (*Root Mean Square Error* (**RMSE**) for regression tasks by default) and by final predictions :D Training is done by the same method **.fit()** with the same tuning HP (more parameters [here](https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html)).\n\nIf you are interested in other objectives and metrics, I would advice checking [Objectives and metrics](https:\/\/catboost.ai\/docs\/concepts\/loss-functions.html) section in CatBoost documentation.","ab9bca43":"# Introduction\nCatBoost is an open-source gradient boosting on decision trees library developed by Yandex. \nCatBoost can be used for solving problems, such as: \n- classification (binary, multi-class)\n- regression\n- ranking\n\nThese tasks differ by their objective function, that we are trying to minimize during gradient descend. Moreover, Catboost have pre-build metrics to measure the accuracy of the model.\n\nOn official CatBoost [website](https:\/\/catboost.ai\/#benchmark) you can find the comparison of Catboost with major benchmarks.","81fd898a":"train_df has the same amount of columns as test_df, although it contains label(target) column. Does our test_df dataset contain target values, too?","a8f5ff44":"That's an interactive plot. You can analyse your model by switching parameters for both abscissa and ordinate. Keep in mind that it's made from the slice of the data (first 100 instances).","8b87cc6d":"### Some comments on Regularization tasks\n\nCatBoost also has tools for solving regularization problem:","4f2a6184":"Finally, let us make a prediction:","4f73088b":"What we did here is that we created a list of feature (column) numbers which we want CatBoost to treat as the categorical ones. And in our dataset all features are categorical.","4f24b8fc":"Ah, much better. Moreover, we obtained our best result much faster (iteration 412), though overall training took much more time. We can handle this problem by specifying HP ***early_stopping_rounds=N***, meaning that if the metric result do not improve for N rounds, model should stop training:","1af76d14":"CatBoostClassifier.fit() method can also accept pool object as a train data:","26354b45":"Well, the results didn't change much. Still, they are not the same, so it worth trying. Moreover, some HP can be set only if the model trains on GPU.  \nThese are: \n- tree growing policy (***grow_policy***)\n- the minimum number of training samples in a leaf (***min_data_in_leaf***)\n- the maximum number of leafs in the resulting tree (***max_leaves***)\n\netc.\n\nThese HP may significantly help in model tuning.","110063e1":"Let's use example datasets. ","01ccb2e7":"Look like it matters who is your manager (MGR_ID) :D","c0f2181f":"Or in a more illustrative way:","a815610f":"Let's separate features and label values:","c9edf719":"To make further results reproducible we will use fixed random seed.","2745d067":"# Classification task","f0bfa3fe":"We import model for solving classification tasks:","7c1ee501":"As we can see, we obtained the same results as for cbc_2. Then why should we bother creating Pool objects? Pool object has some nice methods. For example, some parts of our data may be outdated, inaccurate. With ***Pool.set_weight()*** we can specify weights to instances (rows) of our data. Or we can divide data on groups using ***Pool.set_group_id()*** and play with different weights for different groups using ***Pool.set_group_weight()***.  We may have a baseline calculated. Then we will be able to provide initial formula values for all input objects using ***Pool.set_baseline()***. Thus, training will start from these values for all input objects instead of starting from zero.\n\nFinally, Pool object is also a nice way to contain bounded parts of data.","4c0a7d38":"Let us check summary plot:","74733079":"On the above diagram every employee (instance\/row in our dataset) is represented by one dot in each row. The x position of the dot is the impact of that feature on the model\u2019s prediction, and the color of the dot represents the value of that feature for that exact employee. Dots that do not fit on the row pile up to show density. Here we can see that 'ROLE_ROLLUP_1' and 'ROLE_CODE' features have low impact on the model prediction, and for most of employees their impact is almost zero.","d81b777a":"Although datasets contain numerical values, these features are actually codes for different properties of an employee: manager id, company role code and other. Thus, these datasets contain categorical features.","24edeee7":"Wow, didn't expect it to be that low. Let's specify columns which we want to ignore:","45f7a3b6":"For more thorough cross-validation procedure we can use **cv** from catboost:","6d4e2d31":"What time is it? Training time!","63cabb74":"# Resources\n\n1. [CatBoost documentation](https:\/\/catboost.ai\/docs\/)\n2. [CatBoost tutorials repository](https:\/\/github.com\/catboost\/tutorials)\n3. [Introduction to gradient boosting on decision trees with Catboost](https:\/\/towardsdatascience.com\/introduction-to-gradient-boosting-on-decision-trees-with-catboost-d511a9ccbd14)\n4. [Working with categorical data: Catboost](https:\/\/medium.com\/whats-your-data\/working-with-categorical-data-catboost-8b5e11267a37)\n5. [Interpretable Machine Learning with XGBoost](https:\/\/towardsdatascience.com\/interpretable-machine-learning-with-xgboost-9ec80d148d27)","58994cd9":"To estimate the result of the training let's split train data on train and validation parts.","35dc5b62":"Let us submit it [here](https:\/\/www.kaggle.com\/c\/amazon-employee-access-challenge\/submit). 0.90741 private score! That's silver medal! Too bad it closed 6 years ago :D","3b5ffc52":"Indeed, it doesn't affect much the model quality. However, the training process completed almost twice faster. Nice!","f4f01707":"Let's go deeper:"}}