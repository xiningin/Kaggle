{"cell_type":{"ec39362d":"code","427f7dfd":"code","02674c53":"code","3e467960":"code","a9be761e":"code","e0eb317d":"code","3aabd6ae":"code","a03e1339":"code","68c2d51d":"code","9aeb8907":"code","601cd89f":"code","aa7f77c3":"code","9c6f3a1a":"code","ef3c1547":"code","23ec34d7":"code","c297e83b":"code","6645ac7a":"code","883cac10":"code","e2cbe441":"code","b0d1297f":"code","03145d9b":"code","a5855406":"code","5160053c":"code","f5887cf7":"code","f116c588":"code","30333fbb":"code","2ee843eb":"code","730538de":"code","731ab102":"code","54bf2c70":"markdown","6eff7a2f":"markdown","f0c31b3d":"markdown","5eb5c264":"markdown","d8f82cec":"markdown","a8b78a17":"markdown","9f6435fd":"markdown","4a241429":"markdown","29a9cbc2":"markdown","a184ebff":"markdown"},"source":{"ec39362d":"# Getting the directories\nimport glob \n\nBASE_DIR = '\/kaggle\/input\/optiver-realized-volatility-prediction\/'\n\n# Paths to book and trade data\nTRAIN_BOOK_PATHS  = glob.glob(f'{BASE_DIR}book_train.parquet\/*')\nTEST_BOOK_PATHS   = glob.glob(f'{BASE_DIR}book_test.parquet\/*')\nTRAIN_TRADE_PATHS = glob.glob(f'{BASE_DIR}trade_train.parquet\/*')\nTEST_TRADE_PATHS  = glob.glob(f'{BASE_DIR}trade_test.parquet\/*')\n\n# Plotting\nimport matplotlib.pyplot as plt \n\n# Basic Data Wrangling utilites\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error as mae, r2_score as r2\nfrom sklearn.model_selection import KFold\n\n# Xgboost\nimport xgboost as xgb\n\n# LightGBM\nfrom lightgbm import LGBMRegressor, plot_tree, plot_importance, plot_metric, plot_split_value_histogram\nimport lightgbm as lgb\n\n# Working with dataframes and sequences\nimport numpy as np\nimport pandas as pd \n\ntrain = pd.read_csv(f'{BASE_DIR}train.csv')\nsub   = pd.read_csv(f'{BASE_DIR}sample_submission.csv')\n\n# Some helper functions\n\ndef submit(prediction):\n    \"\"\" Submition process for the competition. \"\"\"\n    sub.drop(sub.index, inplace=True)                         # Remove values in the sample submission file\n    sub['row_id'] = test_data['row_id']                       # Get the row_id for each test_data \n    sub['target'] = prediction                                # Getting the prediction\n    sub.to_csv('\/kaggle\/working\/submission.csv', index=False) # Writting out the .csv file\n    \ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.nanmean(np.square(((y_true - y_pred) \/ y_true))))\n\ndef validate(model, Return=False):\n    \"\"\"Validates the model for differnt metrics. \"\"\"\n    val_data = dval if type(model) == xgb.core.Booster else X_val\n    y_pred = model.predict(val_data)\n    print(f' MAE: {mae(y_pred, y_val)}, R2: {r2(y_pred, y_val)}, RMSPE: {rmspe(y_val, y_pred)}')\n    if Return: return r2(y_pred, y_val), rmspe(y_val, y_pred)\n    \ndef log_return(stock_prices):\n    return np.log(stock_prices).diff()\n\ndef xgb_importance_plot(xgb_model):\n    xgb.plot_importance(xgb_model, max_num_features=18, importance_type='gain'  , show_values=False)\n    xgb.plot_importance(xgb_model, max_num_features=18, importance_type='cover' , show_values=False)\n    xgb.plot_importance(xgb_model, max_num_features=18, importance_type='weight', show_values=False)","427f7dfd":"class DataManager:\n    \"\"\" Used for processing the input data so the model can be fitted on it. \"\"\"\n    def __init__(self, train=True):\n        self._train = train\n        self._book_file_list = TRAIN_BOOK_PATHS if train else TEST_BOOK_PATHS\n        self._trade_file_list = TRAIN_TRADE_PATHS if train else TEST_TRADE_PATHS\n        self.measures_list = []\n    \n    def _traverse_book(self):\n        \"\"\" Goes through each of the training files. \"\"\"\n        for book_file_path, trade_file_path in zip(self._book_file_list, self._trade_file_list):\n            stock_id = book_file_path.split(\"=\")[1] # Getting the stock_id\n            \n            # Reading the book info and preparing it for aggregation\n            book = pd.read_parquet(book_file_path)\n            \n            book.sort_values(by=['time_id', 'seconds_in_bucket'])\n            book['wap1'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) \/ (book['bid_size1']+ book['ask_size1'])\n            book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n            book = book[~book['log_return1'].isnull()]\n            \n            book['wap2'] = (book['bid_price2'] * book['ask_size2'] + book['ask_price2'] * book['bid_size2']) \/ (book['bid_size2']+ book['ask_size2'])\n            book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n            book = book[~book['log_return2'].isnull()]\n            \n            # Different spreads: Get the max of these for each time_id\n            book['h_spread_l1'] = book['ask_price1'] - book['bid_price1']\n            book['h_spread_l2'] = book['ask_price2'] - book['bid_price2']\n            book['v_spread_b']  = book['bid_price1'] - book['bid_price2']\n            book['v_spread_a']  = book['ask_price1'] - book['bid_price2']\n            \n            book.loc[:, 'bas'] = (book.loc[:, ('ask_price1', 'ask_price2')].min(axis = 1) \/ book.loc[:, ('bid_price1', 'bid_price2')].max(axis = 1) - 1) \n            \n            # Reading the trade info\n            trade = pd.read_parquet(trade_file_path)\n            \n            # Slicing the train data based on stock_id\n            book_stock_slice = train[train['stock_id'] == int(stock_id)]\n            \n            for time_id in book['time_id'].unique():\n                book_slice = book[book['time_id'] == time_id] # Slicing based on time_id\n                # Features\n                dic = {\n                    'row_id': f\"{stock_id}-{time_id}\", # Fixing row-id from here\n                    \n                    'wap1_mean': book_slice['wap1'].mean(),\n                    'wap1_std':book_slice['wap1'].std(),\n                    'wap1_max':book_slice['wap1'].max(),\n                    \n                    'wap2_mean': book_slice['wap2'].mean(),\n                    'wap2_std':book_slice['wap2'].std(),\n                    'wap2_max':book_slice['wap2'].max(),\n\n                    'h_spread_l1_mean': book['h_spread_l1'].mean(),\n                    'h_spread_l1_std': book['h_spread_l1'].std(),\n                    'h_spread_l1_std': book['h_spread_l1'].max(),\n                    \n                    'h_spread_l2_mean': book['h_spread_l2'].mean(),\n                    'h_spread_l2_std': book['h_spread_l2'].std(),\n                    'h_spread_l2_max': book['h_spread_l2'].max(),\n                    \n                    'v_spread_b_mean': book['v_spread_b'].mean(),\n                    'v_spread_b_std': book['v_spread_b'].std(),\n                    'v_spread_b_max': book['v_spread_b'].max(),\n                    \n                    'v_spread_a_mean': book['v_spread_a'].mean(),\n                    'v_spread_a_std': book['v_spread_a'].std(),\n                    'v_spread_a_max': book['v_spread_a'].max(),\n                    \n                    'log_return1_mean': book_slice['log_return1'].mean(),\n                    'log_return1_std':book_slice['log_return1'].std(),\n                    'log_return1_max':book_slice['log_return1'].max(),\n                    \n                    'log_return2_mean': book_slice['log_return2'].mean(),\n                    'log_return2_std':book_slice['log_return2'].std(),\n                    'log_return2_max':book_slice['log_return2'].max(),\n                    \n                    'bas_mean': book_slice['bas'].mean(),\n                    'bas_std': book_slice['bas'].std(),\n                    'bas_max': book_slice['bas'].max(),\n                    \n                    'ask_size_mean': book_slice['ask_size1'].mean(),\n                    'ask_size_std': book_slice['ask_size1'].std(),\n                    \n                    'ask_price_mean': book_slice['ask_price1'].mean(),\n                    'ask_price_std': book_slice['ask_price1'].std(),\n                    \n                    'bid_size_mean': book_slice['bid_size1'].mean(),\n                    'bid_size_std': book_slice['bid_size1'].std(),\n                    \n                    'bid_price_mean': book_slice['bid_price1'].mean(),\n                    'bid_price_std': book_slice['bid_price1'].std(),\n                    \n                    'actual_price_mean': trade['price'].mean(),\n                    'actual_price_std': trade['price'].std(),\n                    'actual_price_max': trade['price'].max(),\n                    \n                    'size_mean': trade['size'].mean(),\n                    'size_std': trade['size'].std(),\n                    \n                    'order_count_mean': trade['order_count'].mean(),\n                    'order_count_std': trade['order_count'].std(),\n                }\n                \n                # Note: When getting the test_data ready, there is no target column.\n                if self._train: dic['target'] = book_stock_slice[book_stock_slice['time_id'] == time_id]['target'].values[0]\n                \n                self.measures_list.append(dic)\n    \n    def get_processed(self):\n        \"\"\" Returns the processed the data. \"\"\"\n        self._traverse_book() \n        \n        return pd.DataFrame(self.measures_list)","02674c53":"# book = DataManager().get_processed()\n# book.to_csv('\/kaggle\/working\/train_v3.csv', index=False)","3e467960":"# Importing trainind data from the input folder, and generating test data with the same schema\ndata = pd.read_csv('\/kaggle\/input\/processedbooktrade\/train_v3.csv')\ntest_data = DataManager(train=False).get_processed()\n\n# Min-Max Scaling the data for better models\nfor col_name in data.columns[1:-1]:\n    test_data[col_name] = (test_data[col_name] - data[col_name].min()) \/ (data[col_name].max() - data[col_name].min())\n    data[col_name] = (data[col_name] - data[col_name].min()) \/ (data[col_name].max() - data[col_name].min())\n\n# Training Data\nX, y = data.iloc[:,1:-1], data['target']\n\n# Test Data\nX_test = test_data.iloc[:,1:]\n\n# Getting training and validations plits to check for overfitting\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","a9be761e":"# train_preds, test_preds = pd.DataFrame(), pd.DataFrame()","e0eb317d":"# dtrain = xgb.DMatrix(X_train, label=y_train)\n# dval   = xgb.DMatrix(X_val, label=y_val)\n# dtest  = xgb.DMatrix(X_test)\n\n# X_xgb  = xgb.DMatrix(X)","3aabd6ae":"# def run_xgb(params, n):\n#     xgb_model = xgb.train(\n#         params, \n#         dtrain, \n#         num_boost_round=1500, \n#         early_stopping_rounds=20, \n#         evals=[(dtrain, 'train'), (dval, 'eval')],\n#     )\n    \n#     r2, rp = validate(xgb_model, True)\n    \n#     xgb_model.save_model(f'xgb_v3_{n}_1.model')\n    \n#     train_preds[f'xgb_{n}'] = xgb_model.predict(X_xgb).tolist()\n#     test_preds[f'xgb_{n}'] =  xgb_model.predict(dtest).tolist()\n    \n# #     return pred, r2, rp","a03e1339":"xgb_param_1 = {\n    'eta': 1e-1,\n    'max_depth': 12,\n    'objective': 'reg:squarederror',\n    'booster': 'gbtree',\n    'colsample_bytree': 0.9,\n    'sampling_method': 'gradient_based',\n    'subsample': 0.6, # Avoiding overfitting\n    'tree_method': 'gpu_hist'\n}\n\n# xgb1, xgb1_r1, xgb1_rp = \n# run_xgb(xgb_param_1, 1)","68c2d51d":"xgb_param_2 = {\n    'eta': 1e-1,\n    'max_depth': 5,\n    'eval_metric': 'mape',\n    'objective': 'reg:squarederror',\n    'booster': 'gbtree',\n    'lambda': 0.9,\n    'colsample_bytree': 0.5,\n    'sampling_method': 'gradient_based',\n    'subsample': 0.9, # Avoiding overfitting\n    'tree_method': 'gpu_hist'\n}\n\n# xgb2, xgb2_r2, xgb2_rp = \n# run_xgb(xgb_param_2, 2)","9aeb8907":"xgb_param_3 = {\n    'eta': 8e-1,\n    'max_depth': 15,\n    'verbosity': 0,\n    'eval_metric': 'mape',\n    'objective': 'reg:squarederror',\n    'booster': 'dart',\n    'tree_method': 'gpu_hist',\n    'sample_type': 'weighted',\n    'rate_drop': 0.4,\n    'max_leaves': 30,\n    'alpha': 9e-4,\n    'seed':10,\n#     'min_child_weight': 1e-3 \n}\n\n# xgb3, xgb2_r3, xgb3_rp =\n# run_xgb(xgb_param_3, 3)","601cd89f":"# f, axs = plt.subplots(1,1, figsize=(100,100))\n\n# xgb.plot_tree(xgb_model, num_trees=75, ax=axs)","aa7f77c3":"# import gc\n\n# gc.get_count()","9c6f3a1a":"# Reference https:\/\/www.kaggle.com\/yus002\/realized-volatility-prediction-lgbm-train\ndef my_metrics(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef lgbm_rmspe(y_true, y_pred):  \n    output = my_metrics(y_true, y_pred)\n    return 'rmspe', output, False\n\ndef run_lgbm(params, n):\n    lgbm_model = LGBMRegressor(**params)\n    \n    lgbm_model.fit(\n        X_train, y_train,\n        eval_set=[(X_train, y_train), (X_val, y_val)],\n        eval_metric = lgbm_rmspe,\n        verbose=300,\n        early_stopping_rounds=100\n    )\n    \n    r2, rp = validate(lgbm_model, True)\n    \n#     lgbm_model.save_model(f'lgbm_v3_{n}_1')\n    train_preds[f'lgbm_{n}'] = lgbm_model.predict(X).tolist()\n    test_preds[f'lgbm_{n}'] = lgbm_model.predict(X_test).tolist()","ef3c1547":"lgbm_param_1 = {\n    'objective':'rmse', \n    'metric': 'rmse',\n    \"boosting_type\": \"gbdt\",\n    'device_type': 'gpu',\n    'num_iterations': 5000,\n    'early_stopping_rounds': 30,\n      'learning_rate': 0.01,\n      'lambda_l1': 1,\n      'lambda_l2': 1,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n    'num_leaves': 50,\n    'max_depth': 5,\n    'seed': 11,\n}\n\n# lgbm_pred_1 = \n# run_lgbm(lgbm_param_1, 1)","23ec34d7":"lgbm_param_2 = {\n    'objective':'mean_squared_error', \n    'metric': 'rmse',\n    'device_type': 'gpu',\n    'num_iterations': 5000,\n    'num_leaves': 100,\n    'learning_rate': 0.1,\n    'max_depth': 8,\n    'colsample_bytree': 0.85,\n    'subsample': 0.8,   \n    'seed': 11,\n    'tree_learner': 'feature'\n}\n\n# lgbm_pred_2 = run_lgbm(lgbm_param_2, 2)","c297e83b":"# gc.collect()","6645ac7a":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import mean_absolute_error as MAE, mean_squared_error as MSE\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau as RLP, EarlyStopping as ES\n\nimport random\nimport copy","883cac10":"def nn_rmspe(y_true, y_pred):\n    return tf.sqrt(tf.experimental.numpy.nanmean(tf.square(((y_true - y_pred) \/ y_true))))\n\ndef nn_seq_model(layers, n):\n    model = Sequential(copy.deepcopy(layers))\n\n    model.compile(\n        optimizer=Adam(2e-3),\n        loss=nn_rmspe,\n    )\n    \n    model.fit(\n        x=X_train, y=y_train, \n        batch_size=256, \n        epochs=500,\n        verbose=False,\n        callbacks=[\n            RLP(monitor='val_loss', factor=0.98, patience=15, verbose=1), \n            ES(monitor='val_loss', patience=50, verbose=1, restore_best_weights=True)\n        ], \n        validation_data=(X_val, y_val),\n        shuffle=True,\n    )\n    \n    path = f'.\/nn_v3_{n}_1.h5'\n    \n    model.save(path)\n    \n    train_preds[f'nn_{n}'] = model.predict(X)\n    test_preds[f'nn_{n}']  = model.predict(X_test)","e2cbe441":"layers_1 = [\n    Dense(32, kernel_initializer=TruncatedNormal(0, 1, 11), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2, 11), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 1, 161), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n     Dense(32, kernel_initializer=TruncatedNormal(0, 1e-1, 11), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2e-1, 11), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 1e-1, 161), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n     Dense(32, kernel_initializer=TruncatedNormal(0, 1e-2, 11), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2e-2, 11), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 1e-2, 161), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n     Dense(32, kernel_initializer=TruncatedNormal(0, 1e-2, 51), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2e-2, 1), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 1e-2, 11), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n     Dense(32, kernel_initializer=TruncatedNormal(0, 1e-3, 11), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2e-3, 11), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 1e-3, 161), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 1e-2, 71), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 2e-2, 51), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(256, kernel_initializer=TruncatedNormal(0, 1e-2, 61), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n    Dense(256, kernel_initializer=TruncatedNormal(0, 1e-3, 11), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2e-3, 11), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(32, kernel_initializer=TruncatedNormal(0, 1e-3, 161), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n    Dense(1, kernel_initializer=TruncatedNormal(0, 1, 11), bias_initializer=TruncatedNormal(0, 1e-7, 32)),\n]\n\n# nn_seq_model(layers_1, 1)","b0d1297f":" model = Sequential(layers_1)\n\nmodel.compile(\n        optimizer=Adam(1e-3),\n        loss=nn_rmspe,\n    )\n    \nhist = model.fit(\n        x=X_train, y=y_train, \n        batch_size=512, \n        epochs=1000,\n        verbose=True,\n        callbacks=[\n            RLP(monitor='val_loss', factor=0.98, patience=15, verbose=1), \n            ES(monitor='val_loss', patience=200, verbose=1, restore_best_weights=True)\n        ], \n        validation_data=(X_val, y_val),\n        shuffle=True,\n    )","03145d9b":"model.save('\/kaggle\/working\/nnv3_03')","a5855406":"layers_2 = [\n    Dense(16, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 11), bias_initializer=TruncatedNormal(1e-1, 1e-3, 11)),\n    Dense(16, activation='relu', kernel_initializer=TruncatedNormal(0, 2e-3, 11), bias_initializer=TruncatedNormal(0, 5e-3, 11)),\n    BatchNormalization(),\n    Dense(16, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 161), bias_initializer=TruncatedNormal(0, 1e-2, 151)),\n    Dense(16, activation='relu', kernel_initializer=TruncatedNormal(0, 1e-1, 61), bias_initializer=TruncatedNormal(0, 1e-2, 151)),\n    BatchNormalization(),\n    Dense(1, activation='sigmoid', kernel_initializer=TruncatedNormal(0, 1e-1, 11), bias_initializer=TruncatedNormal(0, 1e-4, 32)),\n]\n\n# nn_seq_model(layers_2, 2)","5160053c":"layers_3 = [\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 11), bias_initializer=TruncatedNormal(1e-1, 1e-3, 11)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 2, 11), bias_initializer=TruncatedNormal(0, 5e-2, 11)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 161), bias_initializer=TruncatedNormal(0, 1e-3, 151)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1e-1, 61), bias_initializer=TruncatedNormal(0, 1e-4, 11)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 161), bias_initializer=TruncatedNormal(0, 1e-1, 101)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1e-1, 61), bias_initializer=TruncatedNormal(0, 1e-2, 51)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 161), bias_initializer=TruncatedNormal(0, 3e-1, 11)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1e-1, 61), bias_initializer=TruncatedNormal(0, 1, 151)),\n    BatchNormalization(),\n    Dense(1, activation='sigmoid', kernel_initializer=TruncatedNormal(0, 1e-1, 11), bias_initializer=TruncatedNormal(0, 1e-1, 32)),\n]\n\n# nn_seq_model(layers_3, 3)","f5887cf7":"layers_4 = [\n    BatchNormalization(),\n    Dense(32, activation=None, kernel_initializer=TruncatedNormal(0, 1e-1, 161), bias_initializer=TruncatedNormal(0, 1, 151)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1e-1, 61), bias_initializer=TruncatedNormal(0, 1, 151)),\n    BatchNormalization(),\n    Dense(64, activation=None, kernel_initializer=TruncatedNormal(0, 5e-2, 11), bias_initializer=TruncatedNormal(0, 1, 151)),\n    Dense(128, activation='relu', kernel_initializer=TruncatedNormal(0, 5e-1, 161), bias_initializer=TruncatedNormal(0, 1, 151)),\n    Dense(256, activation=None, kernel_initializer=TruncatedNormal(0, 8e-1, 161), bias_initializer=TruncatedNormal(0, 1, 151)),\n    Dense(1, activation=None, kernel_initializer=TruncatedNormal(0, 1e-1, 11), bias_initializer=TruncatedNormal(0, 1e-1, 32)),\n]\n\n# nn_seq_model(layers_4, 4)","f116c588":"# train_preds = pd.DataFrame(train_preds)\n# test_preds = pd.DataFrame(test_preds)","30333fbb":"# X_train, X_val, y_train, y_val = train_test_split(train_preds, y, test_size=0.2, random_state=23)","2ee843eb":"# ens_layers = [\n#     Dense(8, activation=None, kernel_initializer=TruncatedNormal(0, 2, 11), bias_initializer=TruncatedNormal(0, 1, 151)),\n#     Dense(8, activation='relu', kernel_initializer=TruncatedNormal(0, 5e-1, 11), bias_initializer=TruncatedNormal(0, 1, 11)),\n#     Dense(8, activation='relu', kernel_initializer=TruncatedNormal(0, 8e-1, 61),bias_initializer=TruncatedNormal(0, 1, 51)),\n#     BatchNormalization(),\n#     Dense(1, activation='sigmoid', kernel_initializer=TruncatedNormal(0, 1e-1, 11),\n#         bias_initializer=TruncatedNormal(0, 1e-1, 32), kernel_regularizer=None,\n#         bias_regularizer=None, activity_regularizer=None),\n# ]\n\n# model = Sequential(ens_layers)\n\n# model.compile(\n#     optimizer=Adam(5e-3),\n#     loss=nn_rmspe,\n# )\n\n# model.fit(\n#         x=X_train, y=y_train, \n#         batch_size=256, \n#         epochs=1000,\n#         verbose=False,\n#         callbacks=[\n#             RLP(monitor='val_loss', factor=0.98, patience=15, verbose=1), \n#             ES(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n#         ], \n#         validation_data=(X_val, y_val),\n#         shuffle=True,\n#     )","730538de":"submit(model.predict(X_test))","731ab102":"pd.read_csv('\/kaggle\/working\/submission.csv')","54bf2c70":"## Questions\n- Is it a good idea to augment features and then run a Gradient Boosting model to see which features tend to be more important?\n- Should I trim the features in each iteration? Then add new ones and repeat?","6eff7a2f":"## Data Deneration\n\nThis part is only ran when a new data version is being worked on. Then the outpued file is uploaded [here](https:\/\/www.kaggle.com\/damoonshahhosseini\/processedbooktrade).","f0c31b3d":"# Strategy\n\nTrying out different models then aggregating the results:\n- XGBoost\n- LightBGM\n- Neural Network","5eb5c264":"# Neural Network","d8f82cec":"## Reference (s)\nhttps:\/\/www.kaggle.com\/konradb\/we-need-to-go-deeper-and-validate#Model","a8b78a17":"# Importing Data\n- Training data is imported from the preprocessed dataset.\n- Test data is generated here useing DataManager class.\n- Both training and test datasets are being normalized so the metrics in different columns are close to one another.","9f6435fd":"# Ensembling\n\n","4a241429":"# XGBoost","29a9cbc2":"# CatBoost","a184ebff":"# LightGBM"}}