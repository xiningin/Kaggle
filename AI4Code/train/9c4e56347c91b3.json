{"cell_type":{"b011d990":"code","2c5a02a3":"code","80eff296":"code","b2009975":"code","4448724c":"code","9f424236":"code","6ed2b1f7":"code","16146776":"code","06a70a6e":"code","bb7b8b80":"code","c74f1cf2":"code","0091f25c":"code","b2d6b980":"code","d8e310ef":"code","a32acb86":"code","c1856ec2":"code","6ab15c6a":"code","5fb51685":"code","9ac9908c":"code","1eaa1988":"code","3c397fee":"code","180f0f40":"code","2a14e95d":"code","62378868":"code","1c6e67da":"code","8d35e059":"code","2854bb89":"code","eefee36c":"code","421d691f":"code","defb2839":"code","614503d9":"code","6b02da5a":"code","757a117c":"code","7dff8556":"code","428ae17e":"code","2309ef79":"code","9118d7c6":"code","207bb48b":"code","658d665a":"code","74763672":"code","64746dfc":"code","b1993029":"code","5d95855a":"code","3d7d4c8d":"code","37a17a75":"code","1e65eaf7":"code","3a42cbde":"code","23aaa9c6":"code","beca42ab":"code","01dbd1f8":"code","c334f7f8":"code","612760e8":"code","1f12bc70":"code","5ca1893c":"code","30086853":"code","0606d247":"code","4bb5bb1d":"code","7058e940":"code","3ceb119c":"code","b37969f6":"code","c24056dc":"code","7d56dee9":"markdown","275ca11e":"markdown","65b156ca":"markdown","f53bd2b2":"markdown","ca554eaa":"markdown","33525097":"markdown","89f920c9":"markdown","d5e5dd8f":"markdown","697d5bac":"markdown","63b98ac2":"markdown","823f7631":"markdown","7ba6d8e2":"markdown"},"source":{"b011d990":"import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.semi_supervised import label_propagation\nfrom scipy import stats\n\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD, RMSprop, Adam\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras import regularizers\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' ## retina display. \nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\/\")) ","2c5a02a3":"## Importing the datasets\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","80eff296":"## Check Dimensions\nSize_train = train.shape[0]\nSize_test = test.shape[0]\nDimension = train.shape[1] - 1\nprint(\"Training set size:{}, Testing set size:{}, Feature Dimension:{}\".format(Size_train, Size_test, Dimension))","b2009975":"#print out the first 5 rows of training set\ntrain.head()","4448724c":"# define plotting tool to check distribution\ndef plot_distribution_by_label(X, val_str, label_str, label_dict, missing=-20):\n    fig, axes = plt.subplots(2,1,figsize=(8,6))\n    sns.set_style('white')\n    sns.distplot(X[val_str].fillna(missing), rug=True, color='b', ax=axes[0])\n    ax0 = axes[0]\n    ax0.set_title(val_str +' distribution')\n    ax0.set_xlabel('')\n    \n    \n\n    ax1 = axes[1]\n    ax1.set_title(val_str +' survived distribution')\n    k1 = sns.distplot(X[X[label_str]==0][val_str].fillna(missing), hist=False, color='r', ax=ax1, label=label_dict[0])\n    k2 = sns.distplot(X[X[label_str]==1][val_str].fillna(missing), hist=False, color='g', ax=ax1, label=label_dict[1])\n    ax1.set_xlabel('')\n\n    ax1.legend(fontsize=16)","9f424236":"label_str = 'Survived'\nlabel_dict = {0:'dead', 1:'alive'}\n\nval_strs = ['Age','Pclass','SibSp','Parch','Fare']\nfor val_str in val_strs:    \n    plot_distribution_by_label(train, val_str, label_str, label_dict)","6ed2b1f7":"# split the labels out.\n\nX_train = train.drop(['Survived'], axis=1)\nY_train = train[['PassengerId','Survived']]\nX_test = test\n#X_test = test.drop(['Name'], axis=1)\nX_train.head()","16146776":"X_train.info()","06a70a6e":"X_test.info()","bb7b8b80":"X_train.describe()","c74f1cf2":"columns_train = X_train.drop(['PassengerId'], axis=1).columns\nfor column_train in columns_train:\n    print(\"{0}: {1}\".format(column_train, X_train[column_train].unique()))","0091f25c":"# pd.series(xxx).isna() return the boolings if it's nan\/none or not\nX_train[X_train[\"Cabin\"].isna()].head()","b2d6b980":"X_train.isna().sum()","d8e310ef":"# Merge the train\/test temporarily to do the feature engineering. \n# Also the reason for merge is to ensure the distribution between train\/test sets are the same.\nX_temp = pd.concat([X_train, X_test], sort = False)\n","a32acb86":"\n# shows the count of missing value for each column\nmissing_init = -20\nX_temp.isna().sum()\n","c1856ec2":"# Extract title and last name from Name\n# Extract length of len, which is likely correlates to 'survived'\nX_temp_title = X_temp['Name'].str.split(\", \").str.get(1).str.split(\".\").str.get(0)\nX_temp_title.unique()\n#X_temp_last_name = X_temp['Name'].str.split(\", \").str.get(0)\n","6ab15c6a":"X_temp['Name_title'] = X_temp_title\nX_temp['Name_len'] = X_temp['Name'].apply(lambda x: len(x))\n#X_temp['Last_name'] = X_temp['Name'].apply(lambda x: str.split(x, \",\")[0])\nX_temp.head(2)","5fb51685":"# We further convert name title with low frequency into 'rare'\nX_temp['Name_title'] = X_temp['Name_title'].replace(['Mlle','Ms','Mme'], 'Miss')\nX_temp['Name_title'] = X_temp['Name_title'].replace(['Lady'], 'Mrs')\ntitle_groups_tmp = X_temp.groupby(['Name_title'])['PassengerId'].count()\ntitle_groups_tmp = title_groups_tmp.apply(lambda x: 'rare' if x < 10 else 'keep')\nX_temp['Name_title'] = X_temp['Name_title'].apply(lambda x:x if title_groups_tmp[x] is not 'rare' else 'rare')","9ac9908c":"X_temp = X_temp.drop(['Name'], axis=1)\nX_temp[X_temp['Name_title'] == 'rare'].head()","1eaa1988":"#fill age missing value with the median grouped by name title\nAge_missing_guess = X_temp.groupby(['Name_title'])['Age']\nX_temp['Age'] = Age_missing_guess.transform(lambda x: x.fillna(x.median()))\n\n#denote passengers whos age < 16, since children were more likily to be alive.\nX_temp['Is_child'] = (X_temp['Age'] <= 13) * 1","3c397fee":"# Cut the Age into groups\nX_temp['Age_cut'] = pd.cut(X_temp['Age'],5)\nX_temp.head()","180f0f40":"# Get numbers\/zone of Cabin and replace missing values by missing_init\n#X_temp['Cabin_len'] = X_temp.Cabin.str.split(' ').apply(lambda x: len(x) if x is not np.nan else missing_init)\n#X_temp['Cabin_zone'] = X_temp.Cabin.str[0].apply(lambda x: missing_init if x is np.nan else x)\n\nX_temp['Cabin_isNull'] = X_temp['Cabin'].isna()*1\nX_temp['Cabin'] = X_temp.Cabin.fillna('Z')\nX_temp.head()","2a14e95d":"#define fmamily size\nX_temp['FamilySize'] = X_temp.SibSp + X_temp.Parch + 1\nsame_ticket_check = (X_temp.groupby(['Ticket'])['PassengerId'].transform('count') > 1)\nfamily_check = same_ticket_check & (X_temp['FamilySize'] > 1)\nfriend_check = same_ticket_check & (X_temp['FamilySize'] == 1)\nprint('Family: {} people'.format((X_temp['FamilySize'] > 1).sum()))\nprint('Peole who share same ticket: {} peolple'.format(same_ticket_check.sum()))\nprint('We\\'re family - Same ticket and family size>1: {} people'.format(family_check.sum()))\nprint('We\\'re friends - Same ticket and family size=1: {} people'.format(friend_check.sum()))\n\n","62378868":"#create connected survival column\nX_temp['connected_survival'] = 0.5\nX_temp.connected_survival[:Size_train]\n\n\nconnected_survival_guess = (Y_train.Survived) & (same_ticket_check[:Size_train])*1\nX_temp.connected_survival[:Size_train] = connected_survival_guess\nX_temp.head()","1c6e67da":"X_temp.isna().sum()[X_temp.isna().sum() > 0]","8d35e059":"#fill na to median for Fare\nX_temp['Fare'] = X_temp['Fare'].fillna(X_temp['Fare'].median())\nX_temp['Fare'].median()","2854bb89":"#fill na to median for Embarked\n# Note!! Because there maybe more than 1 mode of one column so df.col.mode() returns a series.\n# When padding to missing values with mode, we should code as: df.col.mode()[0] for 1st element of the modes.\nX_temp['Embarked'] = X_temp['Embarked'].fillna(X_temp['Embarked'].mode()[0])\nX_temp.groupby(['Embarked'])['PassengerId'].count()","eefee36c":"# Now the data is clean.\nX_temp.isna().sum()","421d691f":"X_temp.head()","defb2839":"X_temp.nunique()","614503d9":"# Some other finetune:\n\nX_temp['Fare_cut'] = pd.cut(X_temp['Fare'],5)\nX_temp['Name_len_cut'] = pd.cut(X_temp['Name_len'],5)","6b02da5a":"X_temp.head()","757a117c":"# normalize Age, Fare, Name_len\nX_temp['Age'] = (X_temp['Age'] - X_temp['Age'].min())\/(X_temp['Age'].max() - X_temp['Age'].min())\nX_temp['Fare'] = (X_temp['Fare'] - X_temp['Fare'].min())\/(X_temp['Fare'].max() - X_temp['Fare'].min())\nX_temp['Name_len'] = (X_temp['Name_len'] - X_temp['Name_len'].min())\/(X_temp['Name_len'].max() - X_temp['Name_len'].min())","7dff8556":"# Final adjustment\nAll_PassengerId = X_temp['PassengerId']\nX_temp = X_temp.drop(['Age','PassengerId','SibSp','Parch','Ticket','Name_title','Name_len','Name_len_cut',\n                      'Cabin','Embarked'], axis=1)\n\nX_temp.head()","428ae17e":"X_temp_oh = pd.get_dummies(X_temp)\nX_train_oh = X_temp_oh[:Size_train].copy()\nY_train_oh = Y_train.drop(['PassengerId'], axis=1)\nX_test_oh = X_temp_oh[Size_train:].copy()\nprint('Training set: {} data and {} features after encoding'.format(X_train_oh.shape[0], X_train_oh.shape[1]))\nprint('Testing set: {} data and {} features after encoding'.format(X_test_oh.shape[0], X_test_oh.shape[1]))","2309ef79":"X_temp_oh.head()","9118d7c6":"Tmodel = Sequential()\n#initialize W with normal distribution and b with zeros\n# First layer\n#Tmodel.add(Dense(input_dim=X_train_oh.shape[1], units=12,\n#                 kernel_initializer='normal', bias_initializer='zeros', \n#                 kernel_regularizer=regularizers.l2(0.0001)))\nTmodel.add(Dense(input_dim=X_train_oh.shape[1], units=10))\nTmodel.add(Activation('relu'))\n\n# hidden layers\nfor i in range(0, 2):\n    Tmodel.add(Dropout(0.5))\n    #Tmodel.add(Dense(units=8, kernel_initializer='normal', bias_initializer='zeros'))\n    Tmodel.add(Dense(units=4))\n    Tmodel.add(Activation('relu'))\n    \nTmodel.add(Dropout(0.1))\nTmodel.add(Dense(1, activation='sigmoid'))\n\nlr = 0.01\nepochs = 7\n#decay = lr \/ epochs\n#opt = Adam(lr=lr, beta_1=0.9, beta_2=0.995)\nopt = Adam(lr=lr)\n\nTmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nTmodel.summary()","207bb48b":"stop = EarlyStopping(monitor = 'val_loss', patience = epochs, verbose = 1)\n\nTmodel.fit(X_train_oh.values, Y_train_oh.values, batch_size=16, epochs=epochs,\n           validation_split = 0.25, shuffle = True, verbose=1, callbacks = [stop])\n\nloss, accuracy = Tmodel.evaluate(X_train_oh, Y_train_oh)\nprint(loss, accuracy)","658d665a":"plt.plot(Tmodel.history.history['acc'])\nplt.plot(Tmodel.history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n","74763672":"\nplt.plot(Tmodel.history.history['loss'])\nplt.plot(Tmodel.history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","64746dfc":"Y_pred = Tmodel.predict_classes(X_test_oh.values)","b1993029":"submission = pd.DataFrame()\nsubmission['PassengerId'] = All_PassengerId[Size_train:]\nsubmission['Survived'] = Y_pred\nsubmission.shape","5d95855a":"submission.sum()","3d7d4c8d":"submission.to_csv('titanic_keras_mlp.csv', index=False)","37a17a75":"decisiontree = DecisionTreeClassifier(random_state = 42)","1e65eaf7":"cross_val_score(decisiontree, X_train_oh.values, Y_train_oh.values, cv=30).mean()","3a42cbde":"decisiontree.fit(X_train_oh.values, Y_train_oh.values)\nacc_decision_tree = round(decisiontree.score(X_train_oh.values, Y_train_oh.values), 4)\nprint(\"Accuracy: %0.4f\" % (acc_decision_tree))","23aaa9c6":"Y_pred_tree = decisiontree.predict(X_test_oh.values)\n\nsubmission_tree = pd.DataFrame()\nsubmission_tree['PassengerId'] = All_PassengerId[Size_train:]\nsubmission_tree['Survived'] = Y_pred_tree\nsubmission_tree.shape\n\nsubmission_tree.to_csv('titanic_sk_tree.csv', index=False)","beca42ab":"submission_tree.sum()","01dbd1f8":"thresholds = np.linspace(0, 0.5, 50)\n# Set the parameters by cross-validation\nparam_grid = {'min_impurity_split': thresholds, 'max_depth': [6,7,8,9,10]}\n \ndecisiontree_grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=15, verbose=2)\ndecisiontree_grid.fit(X_train_oh.values, Y_train_oh.values)\nprint(\"best param: {},  best score: {}\".format(decisiontree_grid.best_params_, decisiontree_grid.best_score_))\n\ndecisiontree_grid.fit(X_train_oh.values, Y_train_oh.values)\nacc_decision_tree_grid = round(decisiontree_grid.score(X_train_oh.values, Y_train_oh.values), 4)\nprint(\"Accuracy: %0.4f\" % (acc_decision_tree_grid))","c334f7f8":"\nY_pred_tree_gs = decisiontree_grid.predict(X_test_oh.values)\n\nsubmission_tree_gs = pd.DataFrame()\nsubmission_tree_gs['PassengerId'] = All_PassengerId[Size_train:]\nsubmission_tree_gs['Survived'] = Y_pred_tree_gs\nsubmission_tree_gs.shape","612760e8":"submission_tree_gs.sum()","1f12bc70":"submission_tree_gs.to_csv('titanic_sk_tree_gs.csv', index=False)","5ca1893c":"random_forest = RandomForestClassifier(random_state = 40, n_estimators=500, oob_score=False, max_depth=4,\n                                      max_leaf_nodes=7, min_impurity_split=0.001, max_features='log2',\n                                      min_samples_leaf=3,\n                                      min_impurity_decrease=0.00000001,\n                                      min_weight_fraction_leaf=0.06, warm_start=True,\n                                      class_weight={0: 1, 1: 1.3})\nrandom_forest.fit(X_train_oh.values, Y_train_oh.values)","30086853":"#cross_val_score(random_forest, X_train_oh.values, Y_train_oh.values, cv=10).mean()","0606d247":"acc_decision_rf = round(random_forest.score(X_train_oh.values, Y_train_oh.values), 4)\nprint(\"Accuracy: %0.4f\" % (acc_decision_rf))","4bb5bb1d":"Y_pred_rf = random_forest.predict(X_test_oh.values)\n\nsubmission_rf = pd.DataFrame()\nsubmission_rf['PassengerId'] = All_PassengerId[Size_train:]\nsubmission_rf['Survived'] = Y_pred_rf\n\n\n","7058e940":"submission_rf.shape","3ceb119c":"submission_rf.sum()","b37969f6":"submission_rf.to_csv('titanic_sk_rf.csv', index=False)","c24056dc":"### No use in this notebook finally, just for reference.\n# Build multi-columns label encoder by using LabelEncoder in sklearn\nif 1 != 1:\n    class Multi_Column_LabelEncoder:\n        def __init__(self, columns=None):\n            self.columns = columns\n\n        def fit(self, X, y=None):\n            return self\n\n        def transform(self, X):\n            '''\n            Transforms columns of X specified in self.columns using\n            LabelEncoder(). If no columns specified, transforms all\n            columns in X.\n            '''\n            output = X.copy()\n\n            # Because we've fill na with 0, some columns is mix-typed. \n            # LE need the column value same type so we fix it with: output[col_name].astype(str)\n            if self.columns is None:\n                for col_name in output.columns:\n                    output[col_name] = LabelEncoder().fit_transform(output[col_name].astype(str))\n            else:\n                for col_name in self.columns:\n                    output[col_name] = LabelEncoder().fit_transform(output[col_name].astype(str))\n            return output\n\n        def fit_transform(self, X, y=None):\n            return self.fit(X,y).transform(X)\n        \n        \n####################################\n# Encoding and pre-modeling\n####################################                  \n'''\n# dropping useless features\ndata = data.drop(columns = ['Age','Cabin','Embarked','Name','Last_Name',\n                            'Parch', 'SibSp','Ticket', 'Family_Size'])\n\n# Encoding features\ntarget_col = [\"Survived\"]\nid_dataset = [\"Type\"]\ncat_cols   = data.nunique()[data.nunique() < 12].keys().tolist()\ncat_cols   = [x for x in cat_cols ]\n# numerical columns\nnum_cols   = [x for x in data.columns if x not in cat_cols + target_col + id_dataset]\n# Binary columns with 2 values\nbin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n# Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n# Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    data[i] = le.fit_transform(data[i])\n# Duplicating columns for multi value columns\ndata = pd.get_dummies(data = data,columns = multi_cols )\n# Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(data[num_cols])\nscaled = pd.DataFrame(scaled,columns = num_cols)\n# dropping original values merging scaled values for numerical columns\ndf_data_og = data.copy()\ndata = data.drop(columns = num_cols,axis = 1)\ndata = data.merge(scaled,left_index = True,right_index = True,how = \"left\")\ndata = data.drop(columns = ['PassengerId'],axis = 1)\n\n# Target = 1st column\ncols = data.columns.tolist()\ncols.insert(0, cols.pop(cols.index('Survived')))\ndata = data.reindex(columns= cols)\n\n# Cutting train and test\ntrain = data[data['Type'] == 1].drop(columns = ['Type'])\ntest = data[data['Type'] == 0].drop(columns = ['Type'])\n\n# X and Y\nX_train = train.iloc[:, 1:20].as_matrix()\ny_train = train.iloc[:,0].as_matrix()\n''' \n\n","7d56dee9":"**3. Random Forest**","275ca11e":"## Part1: Pre-processing\n#### Step2 - Dealing with missing values & Making some useful new columns\nIn this part we will deal with missing values and create some useful new columns based on some feature analysis. ","65b156ca":"## Part0: Data Exploration","f53bd2b2":"## Part2: Model Building & Training\n\n**1. MLP<br>\nIn this part we build our neural network with Keras.**","ca554eaa":"**5. Fare and Embarked**\n<br>\nSince the missing values of these 2 columns are quite few, we simply fill them[](http:\/\/) with median or mode.","33525097":"**1. Name**","89f920c9":"**4. Ticket and Family size(SibSp + Parch)**\n<br>\nCheck passengers who share the same ticket, whether they are in a family or not.","d5e5dd8f":"**2. Decision Tree**","697d5bac":"**2. Age** <br>\nIn this part we fill missing values of 'Age' by the name title groups we got above.<br>\nAnd then we will create a group of \"Age\" to make this feature sparse","63b98ac2":"**3. Cabin**\n<br>\nIn this part we just simply use missing\/not missing of Cabin.","823f7631":"# Titanic Survival\n\n![Titanic](https:\/\/cdn-images-1.medium.com\/max\/800\/1*fDlc6TbuONh0p-i0t61w8A.jpeg)","7ba6d8e2":"#### step1 - Basic Information"}}