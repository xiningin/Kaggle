{"cell_type":{"83de1202":"code","b52f1d35":"code","19438419":"code","95195a5c":"code","9a2cbae9":"code","53ea8a92":"code","6b49fc29":"code","c5a573b4":"code","613dbf4d":"code","5c23bb39":"code","5937d40e":"code","9527965f":"code","6815eb59":"code","8d35085e":"code","e5a90631":"code","9435a2aa":"code","e1acb2b5":"code","2429dcc0":"code","ce91899f":"code","10a56a68":"code","4efefeff":"code","0633e6bb":"code","e177ef3e":"code","cfd99624":"code","7955fcc2":"markdown","6a4b0b1d":"markdown","5da65350":"markdown","1843982a":"markdown","1bd1c872":"markdown","068d6e6b":"markdown","e26afd16":"markdown","c5374cb3":"markdown","2787688f":"markdown","7241a32d":"markdown","ce04d3c9":"markdown","87ac9832":"markdown","f0d2c2d9":"markdown","c772d7e2":"markdown"},"source":{"83de1202":"import random\nimport re\nimport pandas as pd\nfrom nltk import sent_tokenize\nfrom tqdm import tqdm\nfrom albumentations.core.transforms_interface import DualTransform, BasicTransform","b52f1d35":"class NLPTransform(BasicTransform):\n    \"\"\" Transform for nlp task.\"\"\"\n    LANGS = {\n        'en': 'english',\n        'it': 'italian', \n        'fr': 'french', \n        'es': 'spanish',\n        'tr': 'turkish', \n        'ru': 'russian',\n        'pt': 'portuguese'\n    }\n\n    @property\n    def targets(self):\n        return {\"data\": self.apply}\n    \n    def update_params(self, params, **kwargs):\n        if hasattr(self, \"interpolation\"):\n            params[\"interpolation\"] = self.interpolation\n        if hasattr(self, \"fill_value\"):\n            params[\"fill_value\"] = self.fill_value\n        return params\n\n    def get_sentences(self, text, lang='en'):\n        return sent_tokenize(text, self.LANGS.get(lang, 'english'))\n","19438419":"class ShuffleSentencesTransform(NLPTransform):\n    \"\"\" Do shuffle by sentence \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text, lang = data\n        sentences = self.get_sentences(text, lang)\n        random.shuffle(sentences)\n        return ' '.join(sentences), lang","95195a5c":"transform = ShuffleSentencesTransform(p=1.0)\n\ntext = '<Sentence1>. <Sentence2>. <Sentence3>. <Sentence4>. <Sentence5>. <Sentence6>.'\nlang = 'en'\n\ntransform(data=(text, lang))['data'][0]","9a2cbae9":"class ExcludeDuplicateSentencesTransform(NLPTransform):\n    \"\"\" Exclude equal sentences \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeDuplicateSentencesTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text, lang = data\n        sentences = []\n        for sentence in self.get_sentences(text, lang):\n            sentence = sentence.strip()\n            if sentence not in sentences:\n                sentences.append(sentence)\n        return ' '.join(sentences), lang","53ea8a92":"transform = ExcludeDuplicateSentencesTransform(p=1.0)\n\ntext = '<Sentence1>. <Sentence2>. <Sentence4>. <Sentence4>. <Sentence5>. <Sentence5>.'\nlang = 'en'\n\ntransform(data=(text, lang))['data'][0]","6b49fc29":"class ExcludeNumbersTransform(NLPTransform):\n    \"\"\" exclude any numbers \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text, lang = data\n        text = re.sub(r'[0-9]', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text, lang","c5a573b4":"transform = ExcludeNumbersTransform(p=1.0)\n\ntext = '<Word1> <Word2> <Word3> <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'\nlang = 'en'\n\ntransform(data=(text, lang))['data'][0]","613dbf4d":"class ExcludeHashtagsTransform(NLPTransform):\n    \"\"\" Exclude any hashtags with # \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text, lang = data\n        text = re.sub(r'#[\\S]+\\b', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text, lang","5c23bb39":"transform = ExcludeHashtagsTransform(p=1.0)\n\ntext = '<Word1> <Word2> <Word3> #kaggle <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'\nlang = 'en'\n\ntransform(data=(text, lang))['data'][0]","5937d40e":"class ExcludeUsersMentionedTransform(NLPTransform):\n    \"\"\" Exclude @users \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text, lang = data\n        text = re.sub(r'@[\\S]+\\b', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text, lang","9527965f":"transform = ExcludeUsersMentionedTransform(p=1.0)\n\ntext = '<Word1> <Word2> <Word3> @kaggle <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'\nlang = 'en'\n\ntransform(data=(text, lang))['data'][0]","6815eb59":"class ExcludeUrlsTransform(NLPTransform):\n    \"\"\" Exclude urls \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text, lang = data\n        text = re.sub(r'https?\\S+', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text, lang","8d35085e":"transform = ExcludeUrlsTransform(p=1.0)\n\ntext = '<Word1> <Word2> <Word3> <Word4> https:\/\/www.kaggle.com\/shonenkov\/nlp-albumentations\/ <Word6> <Word7> <Word8> <Word9> <Word10>'\nlang = 'en'\n\ntransform(data=(text, lang))['data'][0]","e5a90631":"class SwapWordsTransform(NLPTransform):\n    \"\"\" Swap words next to each other \"\"\"\n    def __init__(self, swap_distance=1, swap_probability=0.1, always_apply=False, p=0.5):\n        \"\"\"  \n        swap_distance - distance for swapping words\n        swap_probability - probability of swapping for one word\n        \"\"\"\n        super(SwapWordsTransform, self).__init__(always_apply, p)\n        self.swap_distance = swap_distance\n        self.swap_probability = swap_probability\n        self.swap_range_list = list(range(1, swap_distance+1))\n\n    def apply(self, data, **params):\n        text, lang = data\n        words = text.split()\n        words_count = len(words)\n        if words_count <= 1:\n            return text, lang\n\n        new_words = {}\n        for i in range(words_count):\n            if random.random() > self.swap_probability:\n                new_words[i] = words[i]\n                continue\n    \n            if i < self.swap_distance:\n                new_words[i] = words[i]\n                continue\n    \n            swap_idx = i - random.choice(self.swap_range_list)\n            new_words[i] = new_words[swap_idx]\n            new_words[swap_idx] = words[i]\n\n        return ' '.join([v for k, v in sorted(new_words.items(), key=lambda x: x[0])]), lang\n","9435a2aa":"transform = SwapWordsTransform(p=1.0, swap_distance=1, swap_probability=0.2)\n\ntext = '<Word1> <Word2> <Word3> <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'\nlang = 'en'\n\ntransform(data=(text, lang))['data'][0]","e1acb2b5":"class CutOutWordsTransform(NLPTransform):\n    \"\"\" Remove random words \"\"\"\n    def __init__(self, cutout_probability=0.05, always_apply=False, p=0.5):\n        super(CutOutWordsTransform, self).__init__(always_apply, p)\n        self.cutout_probability = cutout_probability\n\n    def apply(self, data, **params):\n        text, lang = data\n        words = text.split()\n        words_count = len(words)\n        if words_count <= 1:\n            return text, lang\n        \n        new_words = []\n        for i in range(words_count):\n            if random.random() < self.cutout_probability:\n                continue\n            new_words.append(words[i])\n\n        if len(new_words) == 0:\n            return words[random.randint(0, words_count-1)], lang\n\n        return ' '.join(new_words), lang","2429dcc0":"transform = CutOutWordsTransform(p=1.0, cutout_probability=0.2)\n\ntext = '<Word1> <Word2> <Word3> <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'\nlang = 'en'\n\ntransform(data=(text, lang))['data'][0]","ce91899f":"class AddNonToxicSentencesTransform(NLPTransform):\n    \"\"\" Add random non toxic statement \"\"\"\n    def __init__(self, non_toxic_sentences, sentence_range=(1, 3), always_apply=False, p=0.5):\n        super(AddNonToxicSentencesTransform, self).__init__(always_apply, p)\n        self.sentence_range = sentence_range\n        self.non_toxic_sentences = non_toxic_sentences\n\n    def apply(self, data, **params):\n        text, lang = data\n\n        sentences = self.get_sentences(text, lang)\n        for i in range(random.randint(*self.sentence_range)):\n            sentences.append(random.choice(self.non_toxic_sentences))\n        \n        random.shuffle(sentences)\n        return ' '.join(sentences), lang","10a56a68":"nlp_transform = NLPTransform()\n\ndf = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv', nrows=1000)\ndf = df[df.toxic == 0]\ndf['lang'] = 'en'\nnon_toxic_sentences = set()\nfor comment_text in tqdm(df['comment_text'], total=df.shape[0]):\n    non_toxic_sentences.update(nlp_transform.get_sentences(comment_text), 'en')\n\ntransform = AddNonToxicSentencesTransform(non_toxic_sentences=list(non_toxic_sentences), p=1.0, sentence_range=(1,2))","4efefeff":"text = '<Sentence1>. <Sentence2>. <Sentence4>. <Sentence4>. <Sentence5>. <Sentence5>.'\nlang = 'en'\n\ntransform(data=(text, lang))['data'][0]","0633e6bb":"import albumentations\n\ndef get_train_transforms():\n    return albumentations.Compose([\n        ExcludeDuplicateSentencesTransform(p=0.9),  # here not p=1.0 because your nets should get some difficulties\n        albumentations.OneOf([\n            AddNonToxicSentencesTransform(non_toxic_sentences=list(non_toxic_sentences), p=0.8, sentence_range=(1,3)),\n            ShuffleSentencesTransform(p=0.8),\n        ]),\n        ExcludeNumbersTransform(p=0.8),\n        ExcludeHashtagsTransform(p=0.5),\n        ExcludeUsersMentionedTransform(p=0.9),\n        ExcludeUrlsTransform(p=0.9),\n        CutOutWordsTransform(p=0.1),\n        SwapWordsTransform(p=0.1),\n    ])","e177ef3e":"from torch.utils.data import Dataset\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, df, train_transforms=None):\n        self.comment_texts = df['comment_text'].values\n        self.langs = df['lang'].values\n        self.train_transforms = train_transforms\n\n    def __len__(self):\n        return self.comment_texts.shape[0]\n\n    def __getitem__(self, idx):\n        text = self.comment_texts[idx]\n        lang = self.langs[idx]\n        if self.train_transforms:\n            text, _ = self.train_transforms(data=(text, lang))['data']\n        return text","cfd99624":"dataset = DatasetRetriever(df, train_transforms=get_train_transforms())\nfor albumentation_text in tqdm(dataset, total=len(dataset)):\n    pass","7955fcc2":"usage example:","6a4b0b1d":"usage example:","5da65350":"# NLP Albumentations\n\nHi everyone!\n\nRecently I have published my [inference kernel](https:\/\/www.kaggle.com\/shonenkov\/tpu-inference-super-fast-xlmroberta)\n\nNow I would like to share with you, my friends, experience in computer vision competition!\n\nCV? Yes, DL\/CV\/NLP are very similar.\n\nI have got good boost when I used this great library [albumentations](https:\/\/github.com\/albumentations-team\/albumentations) \n\n![](https:\/\/camo.githubusercontent.com\/fd2405ab170ab4739c029d7251f5f7b4fac3b41c\/68747470733a2f2f686162726173746f726167652e6f72672f776562742f62642f6e652f72762f62646e6572763563746b75646d73617a6e687734637273646669772e6a706567)","1843982a":"## MAIN IDEA\n\nIn this competitions I needed similar NLP tool for creating nice training pipeline with augmentations for texts.\n\nSo I started searching another lib, but finally I decided create some similar classes for using [albumentations](https:\/\/github.com\/albumentations-team\/albumentations) for text.\n\nSo, let's start!","1bd1c872":"usage example:","068d6e6b":"usage example:","e26afd16":"usage example:","c5374cb3":"## Thank you for reading my kernel!\n\nI have shown great tools for you. \nAnd.. If you like this format of notebooks I would like continue to make kernels with realizations of my ideas.\n\n\nP.S. Method \"get_train_transforms\" is used only as example for you, my friends. You should get own collection augmentations :) ","2787688f":"## Lets I show example for usage these classes for retrieving data using PyTorch Dataset:","7241a32d":"## So let me implement some nlp \"albumentations\" :D","ce04d3c9":"usage example:","87ac9832":"usage example:","f0d2c2d9":"usage example:","c772d7e2":"usage example:"}}