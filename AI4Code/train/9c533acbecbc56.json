{"cell_type":{"9540141e":"code","ae20a347":"code","9a1122cd":"code","1a96c2bb":"code","7459fd1d":"code","43de4c3a":"code","36abefdf":"code","d4ad1ba7":"code","05d01120":"code","7eef3f06":"code","095ef7e7":"code","81a20d49":"code","38e0bb04":"code","485dc7b4":"code","ec6e1197":"code","6a2dddfe":"code","d5308d52":"code","9c8a8d5b":"code","5e97ba7c":"code","e816862e":"code","591d5bfc":"code","aed84070":"code","54cf75a6":"code","3153b2a0":"code","1c5a1557":"code","7b5d3b35":"code","ad814a66":"code","24136557":"code","4d98b798":"code","cf559c61":"code","17da5bd8":"code","68b9cd5f":"code","cdbd0f33":"code","5006ac55":"code","cea5fc4d":"code","3549096e":"code","05b3f280":"code","7b0418c5":"code","e69872de":"code","d62478b3":"code","07cac811":"code","04840f5e":"code","12269af0":"code","a5adb66b":"code","e71eaca9":"code","85282061":"code","028ff92e":"code","6d0c0e9c":"code","400dcd0b":"code","47dd494c":"code","6a78b273":"code","7851531c":"code","e9d80ea3":"code","81c00534":"code","dd8e03d5":"code","f3b8e1f6":"code","aae611c3":"code","6585cf67":"code","f974040f":"markdown","310e8c06":"markdown","0be9211e":"markdown","6198e2ca":"markdown","3ea20f2a":"markdown","e16c4659":"markdown","1a557f1f":"markdown","a7b145be":"markdown","3fc9cb27":"markdown","47d9a351":"markdown","80833953":"markdown","cb33ac9a":"markdown","e2ad2193":"markdown","3682a099":"markdown","47d2e0e5":"markdown","e453dd62":"markdown","423d4b70":"markdown","c6e8d218":"markdown","48fa97a5":"markdown","0a8580c7":"markdown","60abaf3d":"markdown","63233e87":"markdown","0945807b":"markdown","d532796e":"markdown","7079833e":"markdown","946bad4c":"markdown","79bc9859":"markdown"},"source":{"9540141e":"import os\nimport torch\nimport torchvision\nfrom torch.utils.data import random_split\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport joblib","ae20a347":"data_dir  = '..\/input\/indoor-scenes-cvpr-2019\/indoorCVPR_09\/Images'\n\nclasses = os.listdir(data_dir)\nprint(classes)","9a1122cd":"len(classes)","1a96c2bb":"from torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\n\ntransformations = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n\ndataset = ImageFolder(data_dir, transform = transformations)","7459fd1d":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_sample(img, label):\n    print(\"Label:\", dataset.classes[label], \"(Class No: \"+ str(label) + \")\")\n    plt.imshow(img.permute(1, 2, 0))","43de4c3a":"img, label = dataset[50]\nshow_sample(img, label)","36abefdf":"random_seed = 42\ntorch.manual_seed(random_seed)","d4ad1ba7":"len(dataset)","05d01120":"train_ds, val_ds, test_ds = random_split(dataset, [13000, 2000, 620])\nlen(train_ds), len(val_ds), len(test_ds)","7eef3f06":"from torch.utils.data.dataloader import DataLoader\nbatch_size = 25","095ef7e7":"train_dl = DataLoader(train_ds, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers = 4, pin_memory = True)","81a20d49":"from torchvision.utils import make_grid\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow = 16).permute(1, 2, 0))\n        break","38e0bb04":"show_batch(train_dl)","485dc7b4":"show_batch(val_dl)","ec6e1197":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch {}: train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch+1, result['train_loss'], result['val_loss'], result['val_acc']))","6a2dddfe":"class ResNet(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.resnet18(pretrained=True)\n        # Replace last layer\n        num_ftrs = self.network.fc.in_features\n        self.network.fc = nn.Linear(num_ftrs, len(dataset.classes))\n    \n    def forward(self, xb):\n        return torch.sigmoid(self.network(xb))\n\nmodel = ResNet()","d5308d52":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","9c8a8d5b":"device = get_default_device()\ndevice","5e97ba7c":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nto_device(model, device)","e816862e":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","591d5bfc":"model = to_device(ResNet(), device)","aed84070":"evaluate(model, val_dl)","54cf75a6":"num_epochs = 25\nopt_func = torch.optim.Adam\nlr = 6e-5\n\nhistory = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","3153b2a0":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n\nplot_accuracies(history)","1c5a1557":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n\nplot_losses(history)","7b5d3b35":"evaluate(model, val_dl)","ad814a66":"joblib.dump(model, 'rf_model.h5',protocol=4)","24136557":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    prob, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return dataset.classes[preds[0].item()]","4d98b798":"img, label = test_ds[17]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","cf559c61":"img, label = test_ds[23]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","17da5bd8":"img, label = test_ds[51]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","68b9cd5f":"import urllib.request\nurllib.request.urlretrieve(\"https:\/\/images.squarespace-cdn.com\/content\/v1\/5a7497e29f8dcee376b70f7e\/1591630503059-FBBWAYXPWYOK9BTIBMZY\/ke17ZwdGBToddI8pDm48kA_SSaoz4elkj-HsZd8gX3Z7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UWPwZyNcweDIvdeL5kotwkIXjs9g0WibSO_cU-Ijy4Pwg6poS-6WGGnXqDacZer4yQ\/74586587_10157705983079085_1307946016988725248_o+%281%29.jpg?format=2500w\", \"bar.jpg\")\nurllib.request.urlretrieve(\"https:\/\/www.bocadolobo.com\/en\/inspiration-and-ideas\/wp-content\/uploads\/2018\/03\/Discover-the-Ultimate-Master-Bedroom-Styles-and-Inspirations-6_1.jpg\", \"bedroom.jpg\")    \nurllib.request.urlretrieve(\"https:\/\/sika.scene7.com\/is\/image\/sika\/glo-elevator-appliances?wid=1280&crop=0%2C80%2C4615%2C3212\", \"elevator.jpg\") \nurllib.request.urlretrieve(\"https:\/\/i.pinimg.com\/originals\/2b\/15\/9d\/2b159da035e4e3aaa30c03ec8ba7816c.jpg\", \"gameroom.jpg\")\nurllib.request.urlretrieve(\"https:\/\/i.pinimg.com\/originals\/a6\/d9\/d7\/a6d9d743da7017a7bcf4a53e46d22f81.jpg\", \"inside_bus.jpg\")\nurllib.request.urlretrieve(\"https:\/\/s.wsj.net\/public\/resources\/images\/ON-CE927_moviet_B1280_20170714200426.jpg\", \"theatre.jpg\")","cdbd0f33":"loaded_model = model","5006ac55":"from PIL import Image\nfrom pathlib import Path\n\ndef predict_external_image(image_name):\n    image = Image.open(Path('.\/' + image_name))\n\n    example_image = transformations(image)\n    plt.imshow(example_image.permute(1, 2, 0))\n    print(\"The image resembles\", predict_image(example_image, loaded_model) + \".\")","cea5fc4d":"predict_external_image('bar.jpg')","3549096e":"predict_external_image('bedroom.jpg')","05b3f280":"predict_external_image('elevator.jpg')","7b0418c5":"predict_external_image('gameroom.jpg')","e69872de":"predict_external_image('inside_bus.jpg')","d62478b3":"predict_external_image('theatre.jpg')","07cac811":"!wget https:\/\/daniilak.ru\/photoDown\/photos\/0.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/1.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/2.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/3.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/4.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/5.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/6.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/7.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/8.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/9.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/10.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/11.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/12.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/13.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/14.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/15.jpg\n!wget https:\/\/daniilak.ru\/photoDown\/photos\/16.jpg","04840f5e":"predict_external_image('0.jpg')","12269af0":"predict_external_image('1.jpg')","a5adb66b":"predict_external_image('2.jpg')","e71eaca9":"predict_external_image('3.jpg')","85282061":"predict_external_image('4.jpg')","028ff92e":"predict_external_image('5.jpg')","6d0c0e9c":"predict_external_image('6.jpg')","400dcd0b":"predict_external_image('7.jpg')","47dd494c":"predict_external_image('8.jpg')","6a78b273":"predict_external_image('9.jpg')","7851531c":"predict_external_image('10.jpg')","e9d80ea3":"predict_external_image('11.jpg')","81c00534":"predict_external_image('12.jpg')","dd8e03d5":"predict_external_image('13.jpg')","f3b8e1f6":"predict_external_image('14.jpg')","aae611c3":"predict_external_image('15.jpg')","6585cf67":"predict_external_image('16.jpg')","f974040f":"Let us see the classes present in the dataset:","310e8c06":"## Porting to GPU:","0be9211e":"# Model Base:","6198e2ca":"Now, we'll create training and validation dataloaders using `DataLoader`.","3ea20f2a":"Let's create a helper function to see the image and its corresponding label:","e16c4659":"Let us start by importing the libraries:","1a557f1f":"# Conclusion:\n\nOur model is able to classify indoor with **76% accuracy (max)**!\n\nIt's great to see the model's predictions on the test set. It works pretty good on external images too!\n\nYou can try experimenting with more images and see the results!","a7b145be":"### If you liked the kernel, don't forget to show some appreciation :)","3fc9cb27":"## Transformations:","47d9a351":"This is the function for fitting the model.","80833953":"# Loading and Splitting Data:","cb33ac9a":"Let's create the model base:","e2ad2193":"We'll be using ResNet18 for classifying images:","3682a099":"This is a helper function to visualize batches:","47d2e0e5":"Let's start training the model:","e453dd62":"Let us see the model's predictions on the test dataset:","423d4b70":"# Visualizing Predictions:","c6e8d218":"This function takes the image's name and prints the predictions:","48fa97a5":"Let's now test with external images.\n\nI'll use `urllib` for downloading external images.","0a8580c7":"Let us load the model. You can load an external pre-trained model too!","60abaf3d":"We'll split the dataset into training, validation and test sets:","63233e87":"# Indoor Scene Recognition using PyTorch\n\nIndoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g., bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information.\n\n![Garbage Bins](https:\/\/media.springernature.com\/original\/springer-static\/image\/chp%3A10.1007%2F978-3-030-61705-9_64\/MediaObjects\/500677_1_En_64_Fig1_HTML.png)\n\n\nIn this notebook we'll use PyTorch for classifying the indoor scene into various categories like bakery, casino, etc.","0945807b":"GPUs tend to perform faster calculations than CPU. Let's take this advantage and use GPU for computation:","d532796e":"# Predicting External Images:","7079833e":"Now, let's apply transformations to the dataset and import it for use.","946bad4c":"# Training the Model:","79bc9859":"# Database\n\nThe database contains 67 Indoor categories, and a total of 15620 images. The number of images varies across categories, but there are at least 100 images per category. All images are in jpg format. The images provided here are for research purposes only."}}